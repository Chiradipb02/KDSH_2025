Published in Transactions on Machine Learning Research (11/2024)
Expressive Higher-Order Link Prediction through Hyper-
graph Symmetry Breaking
Simon Zhang zhan4125@purdue.edu
Department of Computer Science
Purdue University
Cheng Xin cx122@cs.rutgers.edu
Department of Computer Science
Rutgers University
Tamal K. Dey tamaldey@purdue.edu
Department of Computer Science
Purdue University
Reviewed on OpenReview: https: // openreview. net/ forum? id= oG65SjZNIF
Figure 1: An illustration of a hypergraph of recipes. The nodes are the ingredients and the hyperedges are
the recipes. The task of higher order link prediction is to predict hyperedges in the hypergraph. A negative
hyperedge sample would be the dotted hyperedge. The Asian ingredient nodes ( α) and the European
ingredient nodes ( β) form two separate isomorphism classes. However, GWL-1 cannot distinguish between
these classes and will predict a false positive for the negative sample.
Abstract
A hypergraph consists of a set of nodes along with a collection of subsets of the nodes called
hyperedges. Higher order link prediction is the task of predicting the existence of a miss-
ing hyperedge in a hypergraph. A hyperedge representation learned for higher order link
prediction is fully expressive when it does not lose distinguishing power up to an isomor-
phism. Many existing hypergraph representation learners, are bounded in expressive power
by the Generalized Weisfeiler Lehman-1 (GWL-1) algorithm, a generalization of the Weis-
feiler Lehman-1 (WL-1) algorithm. The WL-1 algorithm can approximately decide whether
two graphs are isomorphic. However, GWL-1 has limited expressive power. In fact, GWL-1
can only view the hypergraph as a collection of trees rooted at each of the nodes in the
hypergraph. Furthermore, message passing on hypergraphs can already be computationally
expensive, particularly with limited GPU device memory. To address these limitations, we
devise a preprocessing algorithm that can identify certain regular subhypergraphs exhibit-
ing symmetry with respect to GWL-1. Our preprocessing algorithm runs once with the
time complexity linear in the size of the input hypergraph. During training, we randomly
1Published in Transactions on Machine Learning Research (11/2024)
drop the hyperedges of the subhypergraphs identifed by the algorithm and add covering
hyperedges to break symmetry. We show that our method improves the expressivity of
GWL-1. Our extensive experiments1also demonstrate the effectiveness of our approach for
higher-order link prediction on both graph and hypergraph datasets with negligible change
in computation.
1 Introduction
Hypergraphs can model complex relationships in real-world networks, extending beyond the pairwise con-
nections captured by traditional graphs. Figure 1 is an example hypergraph consisting of recipes of two
different types of dishes, which are largely determined by the ingredients to be used. In this hypergraph,
the hyperedges are the recipes, and the nodes are the ingredients used in each recipe. The Asian recipes
are presented in the right part of the figure, which consist of combinations of the ingredients of soy sauce,
scallions, rice, peppers, chicken, ginger, curry and onions. The European recipes are presented in the left
part of the figure, which consist of combinations of the ingredients of Parmesan cheese, mozzarella cheese,
salami, butter, eggs, and pasta.
Hypergraphs have found applications in diverse fields such as recommender systems Lü et al. (2012), visual
classification Feng et al. (2019), and social networks Li et al. (2013). Higher-order link prediction is the task
of predicting missing hyperedges in a hypergraph. For this task, when the hypergraph is unattributed, it is
important to respect the hypergraph’s symmetries, or automorphism group. This brings about challenges
to learning an expressive view of the hypergraph.
A hypergraph neural network (hyperGNN) is any neural network that learns on a hypergraph. This is in
analogy to a graph neural network (GNN), which is a neural network that learns on a graph. Many existing
hyperGNN architectures follow a computational message passing model called the Generalized Weisfeiler
Lehman-1 (GWL-1) algorithm Huang & Yang (2021), a hypergraph isomorphism testing approximation
scheme. GWL-1 is a generalization of the message passing algorithm called Weisfeiler Lehman-1 (WL-1)
algorithm Weisfeiler & Leman (1968) used for graph isomorphism testing.
GWL-1, like WL-1 on graphs, is limited in how well it can express its input. Specifically, by viewing a
hypergraph as a collection of rooted trees, GWL-1 loses topological information of its input hypergraph
and thus cannot fully recognize the symmetries, or automorphism group, of the hypergraph. In fact, the
hyperGNN views the hypergraph as having false-positive symmetries. For a task like transductive hyperlink
prediction, this can result in predicting false-positive hyperlinks as shown in Figure 1. Furthermore, such
an issue can become even worse during test time since the automorphism group of the hypergraph might
change. It is thus important to find a way to improve the expressivity of existing hyperGNNs.
Let a hypergraph H= (V,E)denote a pair where Vis a set of nodes and E⊆2V, a collection of subsets of V,
indexes a set of hyperedges. GWL-1 views a hypergraph as a collection of trees rooted at the nodes. These
rooted trees are formed by viewing each node-hyperedge incidence as an edge and recursively expanding
about the nodes and hyperedges alternately. As an example of the GWL-1 algorithm, one step of GWL-1
would output a collection of depth 1trees rooted at each node where leaves are the incident hyperedges of
each node. Two steps of GWL-1 would output a collection of depth 2trees where the depth 1trees of one
step of GWL-1 have their leaves expanded with the incident nodes of the hyperedges the new leaves. This
leaf expansion process can be repeated, alternating nodes and hyperedges. This is the recursive expansion
of the GWL-1 algorithm.
We can recover hyperGNNs by expressing the computation of GWL-1 as a matrix equation. Parameterizing
the expression with learnable weights, GWL-1 becomes a neural network, called a hypergraph neural network
(hyperGNN), similar to graph neural networks (GNN)s. In practice this is implemented through repeated
sparse matrix multiplication.
Computing on a hypergraph can also be very expensive. The subsets e∈Ethat contain a node v∈V
form the neighborhood of v∈V. This means just the neighborhood size of the nodes in hypergraphs can
1https://github.com/simonzhang00/HypergraphSymmetryBreaking
2Published in Transactions on Machine Learning Research (11/2024)
grow exponentially with the number of nodes of the hypergraph. Thus, a computationally more expensive
message passing scheme over GWL-1 based hyperGNNs may bring difficulties.
In order to address the issue of the expressivity of hyperGNNS for the task of hyperlink prediction while also
respecting the computational complexity of computing on a hypergraph, we devise a method that selectively
breaks the symmetry of the hypergraph topology itself coming from the limitations of the hyperGNN archi-
tecture. Our method is designed as an efficient preprocessing algorithm that can improve the expressive
power of GWL-1 for higher order link prediction. Since the preprocessing only runs once with complexity
linear in the input, we do not increase the computational complexity of training.
Similar to a substructure counting algorithm Bouritsas et al. (2022), we identify certain symmetries in in-
duced subhypergraphs. However, unlike in existing work where node attributes are modified, such as random
noise being appended to the node attributes Sato et al. (2021), we directly target and modify the symmetries
in the topology. This limits the space for augmentation, which can prevent extreme perturbations of the
data. The algorithm identifies a cover of the hypergraph by disjoint connected components whose nodes
are indistinguishable by GWL-1. During training, we randomly replace the hyperedges of the identified sym-
metric regular induced subhypergraphs with single hyperedges that cover the nodes of each subhypergraph.
We show that our method of hyperedge hallucination to break symmetry can increase the expressivity of
existing hypergraph neural networks both theoretically and experimentally.
Contributions. In the context of hypergraph representation learning and hyperlink prediction, we have a
methodthatcanbreakthesymmetriesintroducedbyconventionalhypergraphneuralnetworks. Conventional
hypergraph neural networks are based on the GWL-1 algorithm on hypergraphs. However, the GWL-1
algorithm on hypergraphs views the hypergraph as a collection of rooted trees. This introduces false positive
symmetries. We summarize our contributions in this work as follows:
•Provide a formal analysis of GWL-1 on hypergraphs from the perspective of algebraic topol-
ogy. Our analysis offers a novel characterization of the expressive power and limitations of GWL-1.
By leveraging concepts from algebraic topology, we establish a precise connection between GWL-1
and the universal covers of hypergraphs, providing deeper insights into the algorithm’s behavior on
complex hypergraph structures.
•Devise an efficient hypergraph preprocessing algorithm to identify false positive symmetries
of GWL-1. We propose a linear time preprocessing algorithm to identify specific regular subhyper-
graphs that exhibit symmetry with respect to GWL-1, which are potential sources of expressivity
limitations.
•Introduce a data augmentation scheme that leverages the preprocessing algorithm’s output to
improve GWL-1’s expressivity. During training, we randomly modify the hyperedges of the iden-
tified symmetric subhypergraphs, effectively breaking symmetries that GWL-1 cannot distinguish.
This approach enhances the model’s ability to capture fine-grained structural information without
significantly increasing computational complexity.
•Provide formal analysis and performance guarantees for our method. We rigorously prove
how our approach improves the expressivity of GWL-1 under certain conditions. These theoretical
results offer valuable insights into the circumstances under which our method can enhance hyper-
graph representation learning, providing a solid foundation for its practical application.
•Performextensiveexperimentsonreal-worlddatasets todemonstratetheeffectivenessofour
approach. Our comprehensive evaluation spans various hypergraph and graph datasets, showcasing
consistent improvements across different hypergraph neural network architectures for higher-order
link prediction tasks. These empirical results validate the practical utility of our method and its
ability to enhance existing models with minimal computational overhead.
2 Background
The following notation is used throughout the paper:
3Published in Transactions on Machine Learning Research (11/2024)
LetN≜{0,1,...},Z≜{...,−1,0,1,...},Z+≜{1,...}, andRdenote the naturals, integers, positive integers,
and real numbers respectively. Let [n]≜{1,...,n}⊆Z+denote the integers from 1ton∈Z+.
For a setA, let/parenleftbigA
k/parenrightbig
denote the set of all subsets of Aof sizek. Given the set A, a multiset is defined
by˜A≜(A,m),m:A→Z+. A set is also a multiset with m=1. A submultiset ˜B⊆˜Ais defined by
˜B≜(B,m′)withB⊆Aandm′(e)≤m(e),∀e∈B. A multiset with its elements explicitly enumerated with
the double curly brace notation: {{a,a,a,...}}. The cardinality of a multiset ˜A, is defined as|˜A|≜/summationtext
e∈Am(e).
For two multisets ˜A= (A,mA),˜B= (B,mB), we may define their multiset sum by ˜A⊔˜B≜(A∪B,mA∪B=
mA+mB).
For two pairs of multisets ˜A= (˜A1,˜A2),˜B= (˜B1,˜B2), denote their multiset sum by their elementwise
multiset sum: ˜A⊔˜B≜(˜A1⊔˜B1,˜A2⊔˜B2). Similarly, for two pairs of sets A= (A1,A2),B= (B1,B2),
denote their union by their elementwise union: A∪B≜(A1∪B1,A2∪B2).
LetPt≜P(•;t)denote a probability distribution parameterized by t∈R. The distribution Pthas some
domainD, which we denote by dom (Pt). The notation supp (Pt)≜{x∈dom(Pt) :Pt(x)>0}denotes the
support of a distribution Pt.
A Bernoulli distribution Pp≜P(X;p)is a probability distribution parameterized by a p: 0< p < 1with
the following definition:
Pp(X=k) =/braceleftigg
p k = 1
1−p k = 0(1)
The random variable Bernoulli (p)∼Ppis called a Bernoulli random variable.
2.1 Group Theory
To better study hypergraphs, we use some concepts of group theory. Here we give a brief introduction to
some of them. For more details, see Dummit & Foote (2004).
Definition 2.1. AgroupGis a set equipped with a binary operation " ∗" that satisfies the following four
properties:
1.Closure: For every a,b∈G, the result of the operation a∗bis also inG.
2.Associativity : For every a,b,c∈G,(a∗b)∗c=a∗(b∗c).
3.Identity Element : There exists an element e∈Gsuch that for every a∈G,e∗a=a∗e=a.
4.Inverse Element : For eacha∈G, there exists an element b∈Gsuch thata∗b=b∗a=e(such
elementbis unique for aand is often denoted as a−1.)
Permutation Groups Apermutation group is a group where the elements are permutations of a set, and
the group operation is the composition of these permutations. Permutations are bijective functions that
rearrange the elements of a set.
Group Isomorphism and Automorphism Two groups GandHare called isomorphic if there exists
a bijective function ϕ:G→Hsuch that for all a,b∈G,ϕ(a∗b) =ϕ(a)∗ϕ(b). This means that Gand
Hhave the same group structure, even if their elements are different. Such bijective funtions are called
isomorphisms . IfG=H, an isomorphism to a group itself is called an automorphism . The set of all
automorphisms of a group Gforms a group, with group operations given by compositions. Such group is
called the automorphism group ofG, denoted as Aut(G).
Group Action A group action is a formal way in which a group Goperates on a set X. Formally, a group
action is a map G×X→X(denoted (g,x)∝⇕⊣√∫⊔≀→g·x) that satisfies the following two properties:
1. For allx∈X,e·x=xwhereeis the identity element in G.
4Published in Transactions on Machine Learning Research (11/2024)
2. For allg,h∈Gandx∈X,(gh)·x=g·(h·x).
Group actions are useful for studying the symmetry of objects, as they describe how the elements of a group
move or transform the elements of a set.
Stabilizer In the context of group actions, the stabilizer of an element xin a setX(where a group Gacts
onX) is the set of elements in Gthat leavexfixed. Formally, the stabilizer of xinGis given by:
StabG(x) ={g∈G|g·x=x}
2.2 Isomorphisms on Higher Order Structures
In this section, we go over what a hypergraph is and how this structure is represented as tensors. We then
define what a hypergraph isomorphism is.
A hypergraph is a generalization of a graph. Hypergraphs allow for all possible subsets over a set of vertices,
called hyperedges. We can thus formally define a hypergraph as:
Definition 2.2. An undirected hypergraph is a pair H= (V,E)consisting of a set of vertices Vand a set of
hyperedgesE⊆2Vwhere 2Vis the power set of the vertex set V.
For a given hypergraph H= (V,E), a hypergraphG= (V′,E′)is asubhypergraph ofHifV′⊆Vand
E′⊆E.
A subhypergraph induced byW⊆Vis defined as (W,F= 2W∩E). Similarly, for a subset of hyperedges
F⊆E, a subhypergraph induced byFis defined as (/uniontext
e∈Fe,F)
For a given hypergraph H, we also useVHandEHto denote the sets of vertices and hyperedges of H
respectively. According to the definition, a hyperedge is a nonempty subset of the vertices. A hypergraph
withallhyperedgesthesamesize discalledad-uniformhypergraph. A 2-uniformhypergraphisanundirected
graph, or just graph.
Whenviewedcombinatorially, ahypergraphcanincludesomesymmetriesthatarecapturedbyisomorphisms.
These isomorphisms are defined by bijective structure preserving maps.
Definition 2.3. For two hypergraphs HandD, a structure preserving map ρ:H→Dis a pair of maps ρ=
(ρV:VH→VD,ρE:EH→ED)such that∀e∈EH,ρE(e)≜{ρV(vi)|vi∈e}∈ED. A hypergraph isomorphism
is a structure preserving map ρ= (ρV,ρE)such that both ρVandρEare bijective. Two hypergraphs are said
to be isomorphic, denoted as H∼=D, if there exists an isomorphism between them. When H=D, an
isomorphism ρis called an automorphism on H. All the automorphisms form a group, which we denote as
Aut(H).
A graph isomorphism is the special case of a hypergraph isomorphism between 2-uniform hypergraphs
according to Definition 2.3.
Aneighborhood N(v)≜(/uniontext
v∈ee,{e:v∈e})ofanodev∈VofahypergraphH= (V,E)isthesubhypergraph
ofHinduced by the set of all hyperedges incident to v. Thedegreeofvis denoted deg(v) =|EN(v)|. A
degree vector of a node vofHis defined as: degvecH(v) = (|{e:e∋v,|e|=k}|)n
k=1
A simple but very common symmetric hypergraph is of importance to our task, namely the neighborhood-
regular hypergraph, or just regular hypergraph.
Definition 2.4. A neighborhood-regular hypergraph is a hypergraph where all neighborhoods of each node
are isomorphic to each other.
Ad-uniform neighborhood of vis the set of all hyperedges of size din the neighborhood of v. Thus, in a
neighborhood-regular hypergraph, all nodes have their d-uniform neighborhoods of the same cardinality for
alld∈N.
Representing Higher Order Structures as Tensors :There are many data stuctures one can define on
a higher order structure like a hypergraph. An n-order tensor Maron et al. (2018), as a generalization of
5Published in Transactions on Machine Learning Research (11/2024)
an adjacency matrix on graphs can be used to characterize the higher order connectivities. For simplicial
complexes, which are hypergraphs where all subsets of a hyperedge are also hyperedges, a Hasse diagram,
which is a multipartite graph induced by the poset relation of subset amongst hyperedges, or simplices,
differing in exactly one node, is a common data structure Birkhoff (1940). Similarly, the star expansion
matrix Agarwal et al. (2006) can be used to characterize hypergraphs up to isomorphism.
In order to define the star expansion matrix, we define the star expansion bipartite graph.
Definition 2.5 (star expansion bipartite graph) .Given a hypergraph H= (V,E), thestar expansion bipar-
tite graphBV,Eis the bipartite graph with vertices V/unionsqtextEand edges{(v,e)∈V×E|v∈e}.
Definition 2.6. The star expansion incidence matrix Hof a hypergraphH= (V,E)is the|V|× 2|V|0-1
incidence matrix HwhereHv,e= 1iffv∈efor(v,e)∈V×E for some fixed orderings on both Vand2V.
In practice, as data to machine learning algorithms, the matrix His sparsely represented by its nonzero
entries.
Tostudythesymmetriesofagivenhypergraph H= (V,E), weconsiderthepermutationgrouponthevertices
V, denoted as Sym(V), which acts jointly on the rows and columns of star expansion adjacency matrices.
We assume the rows and columns of a star expansion adjacency matrix have some canonical ordering, say
lexicographic ordering, given by some prefixed ordering of the vertices. Therefore, each hypergraph Hhas a
unique canonical matrix representation H.
We define the action of a permutation π∈Sym(V)on a star expansion adjacency matrix H:
(π·H)v,e=(u1,...,v,...,uk)≜Hπ−1(v),π−1(e)=(π−1(u1),...,π−1(v),...,π−1(uk)) (2)
Based on the group action, consider the stabilizer subgroup of Sym(V)on an incidence matrix H:
StabSym (V)(H) ={π∈Sym(V)|π·H=H} (3)
For simplicity we omit the lower index of Sym(V)when the permutation group is clear from context. It can
be checked that Stab(H)⊆Sym(V)is a subgroup. Intuitively, Stab(H)consists of all permutations that fix
H. These are equivalent to hypergraph automorphisms on the original hypergraph H.
Proposition 2.1. Aut(H)∼=Stab(H)are equivalent as isomorphic groups.
We can also define a notion of isomorphism between k-node sets using the stabilizers on H.
Definition 2.7. For a given hypergraph Hwith star expansion matrix H, twok-node setsS,T⊆Vare
calledisomorphic , denoted as S≃T, if∃π∈Stab(H),π(S) =T.
Such isomorphism is an equivalance relation on k-node sets. When k= 1, we have isomorphic nodes, denoted
u∼=Hvforu,v∈V. Node isomorphism is also studied as the so-called structural equivalence in Lorrain &
White (1971). Furthermore, when S≃Twe can then say that there is a matching due to the graph of the
πmap of the form {(s,π(s)) :s∈S}. This matching is between the node sets SandTso that matched
nodes are isomorphic.
2.3 Invariance and Expressivity
For a given hypergraph H= (V,E), we want to do hyperedge prediction on H, which is to predict missing
hyperedgesfrom k-nodesets for k≥2. Let|V|=n,|E|=m, andH∈Zn×2n
2bethe star expansionadjacency
matrix ofH. To do hyperedge prediction, we study k-node representations g:/parenleftbigV
k/parenrightbig
×Zn×2n
2→Rdthat map
k-node sets of hypergraphs to d-dimensional Euclidean space. Ideally, we want a most-expressive k-node
representation for hyperedge prediction, which is intuitively a k-node representation that is injective on k-
node set isomorphism classes from H. We break up the definition of most-expressive k-node representation
into possessing two properties, as follows:
Definition 2.8. Letg:/parenleftbigV
k/parenrightbig
×Zn×2n
2→Rdbe ak-node representation on a hypergraph H. LetH∈Zn×2n
2
be the star expansion adjacency matrix of Hfornnodes. The representation gisk-node most expressive if
∀S,S′⊆V,|S|=|S′|=k, the following two conditions are satisfied:
6Published in Transactions on Machine Learning Research (11/2024)
1.gisk-node invariant :∃π∈Stab(H),π(S) =S′=⇒g(S,H) =g(S′,H)
2.gisk-node expressive ∄π∈Stab(H),π(S) =S′=⇒g(S,H)̸=g(S′,H)
The first condition of a most expressive k-node representation states that the representation must be well
definedonthe knodesuptoisomorphism. Thesecondconditionrequirestheinjectivityofourrepresentation.
These two conditions mean that the representation does not lose any information when doing prediction for
missingk-sized hyperedges on a set of knodes.
We can also define the symmetry group of a k-node representation map gonHas the set of all permutations
onVthat make the representation map g k-node invariant. This is formally defined below:
Definition 2.9. Forg:/parenleftbigV
k/parenrightbig
×Zn×2n
2→Rdak-node representation on a hypergraph H,
Sym(g(H))≜{π∈Sym(V) :∀S,S′∈/parenleftbiggV
k/parenrightbigg
,π(S) =S′⇒g(S,H) =g(S′,H)} (4)
2.4 Generalized Weisfeiler-Lehman-1
We describe a generalized Weisfeiler-Lehman-1 (GWL-1) hypergraph isomorphism test similar to Huang &
Yang (2021); Feng et al. (2023) based on the WL-1 algorithm for graph isomorphism testing. There have
been many parameterized variants of the GWL-1 algorithm implemented as neural networks, see Section 3.
LetHbe the star expansion matrix for a hypergraph H. We define the GWL-1 algorithm as the following
two step procedure on Hat iteration number i≥0.
f0
e←{},h0
v←{}
fi+1
e←{{(fi
e,hi
v)}}v∈e,∀e∈EH(H)
hi+1
v←{{(hi
v,fi+1
e)}}v∈e,∀v∈VH(H)(5)
This is slightly different from the algorithm presented in Huang & Yang (2021) at the fi+1
eupdate step. Our
update step involves an edge representation fi
e, which is not present in their version. Thus our version of
GWL-1 is more expressive than that in Huang & Yang (2021). However, they both possess some of the same
issues that we identify. We denote fi
e(H)andhi
v(H)as the hyperedge and node ith iteration GWL-1, called
i-GWL-1, values on an unattributed hypergraph Hwith star expansion H. If GWL-1 is run to convergence
then we omit the iteration number i. We also mean this when we say i=∞.
For a hypergraph Hwith star expansion matrix H, GWL-1 is strictly more expressive than WL-1 on
A=H·D−1
e·HTwithDe=diag(HT·1n), the node to node adjacency matrix, also called the clique
expansion ofH. This follows since a triangle with its 3-cycle boundary: Tand a 3-cycle C3have exactly the
same clique expansions. Thus WL-1 will give the same node values for both TandC3. GWL-1 on the star
expansions HTandHC3, on the other hand, will identify the triangle as different from its bounding edges.
Letfi(H)≜[fi
e1(H),···,fi
em(H)]andhi(H)≜[hi
v1(H),···,hi
vn(H)]be two vectors whose entries are
ordered by the column and row order of H, respectively.
Proposition 2.2. The update steps fi(H)andhi(H)of GWL-1 are permutation equivariant; For any
π∈Sym(V), let:π·fi(H)≜[fi
π−1(e1)(H),···,fi
π−1(em)(H)]andπ·hi(H)≜[hi
π−1(v1)(H),···,hi
π−1(vn)(H)]:
∀i∈N,π·fi(H) =fi(π·H),π·hi(H) =hi(π·H) (6)
Define the operator AGGas ak-set map to representation space Rd. Define the following representation of
ak-node subset S⊆Vof hypergraphHwith star expansion matrix H:
hi(S,H)≜AGG [{hi
v(H)}v∈S] (7)
wherehi
v(H)is the node value of i-GWL-1 on Hfor nodev. The representation h(S,H)preserves hyperedge
isomorphism classes as shown below:
7Published in Transactions on Machine Learning Research (11/2024)
Proposition 2.3. Lethi(S,H) =AGG [{hi
v(H)}v∈S]with an injective AGG map. The representation
hi(S,H)isk-node invariant but not necessarily k-node expressive for Sa set ofknodes.
It follows that we can guarantee a k-node invariant representation by using GWL-1. For deep learning, we
parameterize AGGas a universal set learner.
The node representations hi
v(H)are also parameterized and rewritten into a message passing hypergraph
neural network with matrix equations Huang & Yang (2021). For example, the HNHN Dong et al. (2020)
hyperGNN architecture can be viewed as a parameterization of GWL-1:
X(l)
E=σ(HTX(l)
VW(l)
E+b(l)
E)
X(l+1)
V =σ(HX(l)
EW(l)
V+b(l)
V)(8)
whereσis a nonlinearity, XV,XEare vector representations of hi(H)andfi(H)respectively, and
WE,WV,bE,bVare learnable weight matrices. Setting b(l)
E= 0,b(l)
V= 0, we maintain permutation equivari-
ance as in Proposition 2.2. Furthermore, the HNHN equations of Equation 8 become in direct analogy to
the steps of GWL-1 from Equation 5.
3 Related Work and Existing Issues
There are many hyperlink prediction methods. Most message passing based methods for hypergraphs are
based on the GWL-1 algorithm. These include Huang & Yang (2021); Yadati et al. (2019); Feng et al.
(2019); Gao et al. (2022); Dong et al. (2020); Srinivasan et al. (2021); Chien et al. (2022); Zhang et al. (2018).
Examples of message passing based approaches that incorporate positional encodings on hypergraphs include
SNALS Wan et al. (2021). The paper Zhang et al. (2019) uses a pair-wise node attention mechanism to do
higher order link prediction. For a survey on hyperlink prediction, see Chen & Liu (2022).
Various methods have been proposed to improve the expressive power of GNNs due to symmetries in graphs.
In Papp & Wattenhofer (2022), substructure labeling is formally analyzed. One of the methods analyzed
includes labeling fixed radius ego-graphs as in You et al. (2021); Zhang & Li (2021). Other methods include
appending random node features Sato et al. (2021), labeling breadth-first and depth-first search trees Li et al.
(2023b) and encoding substructures Zeng et al. (2023); Wijesinghe & Wang (2021). All of the previously
mentioned methods depend on a fixed subgraph radius size. This prevents capturing symmetries that span
long ranges across the graph. Zhang et al. (2023) proposes to add metric information of each node relative
to all other nodes to improve WL-1. This would be very computationally expensive on hypergraphs.
Cycles are a common symmetric substructure. There are many methods that identify this symmetry. Cy2C
Choi et al. is a method that encodes cycles to cliques. It has the issue that if the the cycle-basis algorithm is
not permutation invariant, isomorphic graphs could get different cycle bases and thus get encoded by Cy2C
differently, violating the invariance of WL-1. Similarly, the CW Network Bodnar et al. (2021) is a method
that attaches cells to cycles to improve upon the distinguishing power of WL-1 for graph classification.
However, inflating the input topology with cells as in Bodnar et al. (2021) would not work for link predicting
since it will shift the hyperedge distribution to become much denser. Other works include cell attention
networks Giusti et al. (2022) and cycle basis based methods Zhang et al. (2022). For more related work, see
the Appendix.
Data augmentation is a commonly used approach to improve robustness to distribution shifts Yao et al.
(2022b), recognize symmetries Chen et al. (2020b), and handle data imbalance Chawla et al. (2002). In
the graph domain, a priori knowledge of the data distribution can inform rule-based data augmentations.
In molecular classification, prior knowledge of the physical meaning of the data can be used to augment
graphs Sun et al. (2021). Data augmentations can also be learned through data generation methods. For
example a link prediction neural network GAE Kipf & Welling (2016b) can be used to propose edges to to
improve node classification Zhao et al. (2021). For a survey on graph data augmentation, see Zhao et al.
(2022).
8Published in Transactions on Machine Learning Research (11/2024)
4 A Characterization of GWL-1
A hypergraph can be represented by a bipartite graph BV,EfromVtoEwhere there is an edge (v,e)in
the bipartite graph iff node vis incident to hyperedge e. This bipartite graph is called the star expansion
bipartite graph.
We introduce a more structured version of graph isomorphism called a 2-color isomorphism to characterize
hypergraphs. It is a map on 2-colored graphs, which are graphs that can be colored with two colors so that
no two nodes in any graph with the same color are connected by an edge. We define a 2-colored isomorphism
formally here:
Definition 4.1. A2-colored isomorphism is a graph isomorphism on two 2-colored graphs that preserves
node colors. It is denoted by ∼=c.
A2-colored isomorphism from a graph Gto itself is called a 2-colored automorphism. The set of all 2-colored
automorphisms on Gis denotedAutc(G).
A bipartite graph always has a 2-coloring. In this paper, we canonically fix a 2-coloring on all star expansion
bipartite graphs by assigning red to all the nodes in the node partition and and blue to all the nodes in the
hyperedge partition. See Figure 2(a) as an example. We let BV,BEbe the red and blue colored nodes in
BV,Erespectively.
Proposition 4.1. We have two hypergraphs (V1,E1)∼=(V2,E2)iffBV1,E1∼=cBV2,E2whereBVi,Eiis the star
expansion bipartite graph of (Vi,Ei)
We define a topological object for a graph originally from algebraic topology called a universal cover:
Definition 4.2 (Hatcher (2005)) .Theuniversal covering of a connected graph Gis a (potentially infinite)
graph ˜Gtogether with a map pG:˜G→Gsuch that:
1.∀x∈V(˜G),pG|N(x)is an isomorphism onto N(pG(x)).
2.˜Gis simply connected (a tree)
We call such pGtheuniversal covering map and ˜Gtheuniversal cover ofG. A covering graph is a graph
that satisfies property 1 but not necessarily 2 in Definition 4.2. The universal covering ˜Gis essentially
unique Hatcher (2005) in the sense that it can cover all connected covering graphs of G. Furthermore, define
a rooted isomorphism Gx∼=Hyas an isomorphism between graphs GandHthat mapsxtoyand vice versa.
Let˜Gi
˜xdenote the rooted universal cover ˜G˜xwith every leaf of depth (number of edges) exactly i∈Z+. It
is a known result that:
Theorem 4.2. [Krebs & Verbitsky (2015)] Let GandHbe two connected graphs. Let pG:˜G→G,pH:
˜H→Hbe the universal covering maps of GandHrespectively. For any i∈N, for any two nodes x∈G
andy∈H:˜Gi
˜x∼=˜Gi
˜yiff the WL-1 algorithm assigns the same value to nodes x=pG(˜x)andy=pH(˜y).
We generalize the second result stated above about a topological characterization of WL-1 for GWL-1 for
hypergraphs. In order to do this, we need to generalize the definition of a universal covering to suite the
requirements of a bipartite star expansion graph. To do this, we lift BV,Eto a2-colored tree universal cover
˜BV,Ewhere the red/blue nodes of BV,Eare lifted to red/blue nodes in ˜BV,E. Furthermore, the labels {}are
placed on the blue nodes corresponding to the hyperedges in the lift and the labels {}are placed on all its
corresponding red nodes in the lift. Let (˜Bk
V,E)˜xdenote the k-hop rooted 2-colored subtree ˜BV,Ewith root ˜x
andpBV,E(˜x) =xfor anyx∈V(BV,E).
Theorem 4.3. LetH1= (V1,E1)andH2= (V2,E2)be two connected hypergraphs. Let BV1,E1andBV2,E2
be two canonically colored bipartite graphs for H1andH2(vertices colored red and hyperedges colored blue).
LetpBV1,E1:˜BV1,E1→BV1,E1,pBV2,E2:˜BV2,E2→BV2,E2be the universal coverings of BV1,E1andBV2,E2
respectively. For any i∈Z+, for any of the nodes x1∈BV1,e1∈BE1andx2∈BV2,e2∈BE2:
•(˜B2i−1
V1,E1)˜e1∼=c(˜B2i−1
V2,E2)˜e2ifffi
e1=fi
e2
9Published in Transactions on Machine Learning Research (11/2024)
Figure 2: An illustration of hypergraph symmetry breaking. (c,d) 3-regular hypergraphs C3
4,C3
5with 4and5
nodes respectively and their corresponding universal covers centered at any hyperedge (˜BC3
4)e∗,∗,∗,(˜BC3
5)e∗,∗,∗
with universal covering maps pBC3
4,pBC3
5. (b,e) the hypergraphs ˆC3
4,ˆC3
5, which are C3
4,C3
5with 4,5-sized
hyperedges attached to them and their corresponding universal covers and universal covering maps. (a,f)
are the corresponding bipartite graphs of ˆC3
4,ˆC3
5. (c,d) are indistinguishable by GWL-1 and thus will give
identical node values by Theorem 4.3. On the other hand, (b,e) gives node values which are now sensitive
to the the order of the hypergraphs 4,5, also by Theorem 4.3.
•(˜B2i
V1,E1)˜x1∼=c(˜B2i
V2,E2)˜x2iffhi
x1=hi
x2,
withfi
•,hi
•theith GWL-1 values for the hyperedges and nodes respectively where e1=pBV1,E1(˜e1),x1=
pBV1,E1(˜x1),e2=pBV2,E2(˜e2),x2=pBV2,E2(˜x2).
Theorem 4.3 states that a 2-colored isomorphism is maintained during each step of the GWL-1 algorithm.
Thus we can view the GWL-1 algorithm on a hypergraph as equivalent to computing a universal cover of
the star expansion bipartite graph up to a 2-colored isomorphism. We can thus deduce from Theorems 4.3,
4.2 that GWL-1 reduces to computing WL-1 on the bipartite graph up to the 2-colored isomorphism.
Corollary 1. LetH1= (V1,E1)andH2= (V2,E2)be two connected hypergraphs. Let BV1,E1andBV2,E2be
two canonically colored bipartite graphs for H1andH2(vertices colored red and hyperedges colored blue).
LetpBV1,E1:˜BV1,E1→BV1,E1,pBV2,E2:˜BV2,E2→BV2,E2be the universal coverings of BV1,E1andBV2,E2
respectively. For any i∈Z+,
•(˜B2i−1
V1,E1)˜v1∼=c(˜B2i−1
V2,E2)˜v2iffgi
v1=gi
v2
withgi
v1,gi
v2thei-th WL-1 values for v1,v2∈ V(BV1,E1),V(BV2,E2)respectively where v1=pBV1,E1(˜v1),
v2=pBV2,E2(˜v2).
See Figure 2 for an illustration of the universal covering of the corresponding bipartite graphs for two
3-uniform neighborhood regular hypergraphs.
4.1 A Limitation of GWL-1
Due to the equivalence of GWL-1 to viewing the hypergraph Has a collection of rooted trees, we can show
that the automorphism group of His a subgroup of the automorphism of this collection of rooted trees. This
is stated in the following proposition:
Theorem 4.4. LethL: [V]1×Zn×2n
2→Rdbe theL-GWL-1 representation of nodes for hypergraph Hin
Equation 7, then
Aut(H)∼=Stab(H)⊆Sym(hL(H))∼=Autc(˜B2L
V,E),∀L≥1 (9)
By Proposition 2.1, Aut(H)∼=Stab(H). The subgroup relationship follows by definition of the symmetry
group of a representation map given in Definition 2.9 and the equivariance of L-GWL-1 due to Proposition
10Published in Transactions on Machine Learning Research (11/2024)
2.2. The last group isomorphism follows by the equivalence between L-GWL-1 and the universal cover of
the star expansion bipartite graph BV,Eup to 2L-hops.
Since there are more automorphisms over the GWL-1 view of the hypergraph, many false positive symmetries
might exist. Consider the following example. For two neighborhood-regular hypergraphs C1andC2, the
red/blue colored universal covers ˜BC1,˜BC2of the star expansions of C1andC2are isomorphic, with the
same GWL-1 values on all nodes. However, two neighborhood-regular hypergraphs of different order become
distinguishable if a single hyperedge covering all the nodes of each neighborhood-regular hypergraph is
added. Furthermore, deleting the original hyperedges, does not change the node isomorphism classes of
each hypergraph. Referring to Figure 2, consider the hypergraph C=C3
4⊔C3
5, the hypergraph with two
3-regular hypergraphs C3
4andC3
5acting as two connected components of C. As shown in Figure 2, the node
representations of the two hypergraphs are identical due to Theorem 4.3.
Given a hypergraph H, we define a special induced subhypergraph R⊆Hwhose node set GWL-1 cannot
distinguish from other such special induced subhypergraphs.
Definition 4.3. AL-GWL-1 symmetric induced subhypergraph R⊂HofHis a connected induced subhy-
pergraph determined by VR⊆VH, some subset of nodes that are all indistinguishable amongst each other by
L-GWL-1:
hL
u(H) =hL
v(H),∀u,v∈VR (10)
WhenL=∞, we call suchRa GWL-1 symmetric induced subhypergraph. Furthermore, if R=H, then we
sayHisGWL-1 symmetric .
This definition is similar to that of a symmetric graph from graph theory Godsil & Royle (2001), except that
isomorphic nodes are determined by the GWL-1 approximator instead of an automorphism. The following
observation follows from the definitions.
Observation 1. A hypergraphHis GWL-1 symmetric if and only if it is L-GWL-1 symmetric for all L≥1
if and only ifHis neighborhood regular.
Our goal is to find GWL-1 symmetric induced subhypergraphs in a given hypergraph and break their
symmetry without affecting any other nodes.
5 Method
Our goal is to learn from a training hypergraph and then predict higher order links in a temporally later
hypergraph transductively. This can be formulated as follows:
Problem 1. Hyperlink Transductive Learning: LetVbennodes.
1. Given a training hypergraph sampled at time ttr∈R:
For(V,(Etr)gt)∼P(H;ttr), learn a boolean predictor ˆhon hypergraph input so that:
2. For a testing hypergraph sampled at time tte∈R,tte>ttr:
For(V,Ete)∼P(H;tte)andEte⊆(Ete)gt,
ˆh((V,Ete),e)predicts whether e∈(Ete)gt\Ete,∀e∈2V
We will assume that the unobservable hyperedges are of the same size kso that we only need to predict on
k-node sets. In order to preserve the most information while still respecting topological structure, we aim
to start with an invariant multi-node representation to predict hyperedges and increase its expressiveness,
as defined in Definition 2.8. For input hypergraph Hand its matrix representation H, to do the prediction
of a missing hyperedge on node subsets, we use a multi-node representation h(S,H)forS⊆V(H)as in
Equation 7 due to its simplicity, guaranteed invariance, and improve its expressivity. We aim to not affect
the computational complexity since message passing on hypergraphs is already quite expensive, especially
on GPU memory.
11Published in Transactions on Machine Learning Research (11/2024)
Figure 3: An illustration of Algorithm 1 for 1-GWL-1.
In (a) a hypergraph is shown. Each node is labeled with a pair. The left part of the pair in Greek
alphabet is its isomorphism class. The right part of the pair in Latin alphabet is its 1-GWL-1 class, which
is determined by its neighborhood of hyperedges. In (b) the multi-hypergraph formed by covering the
original hypergraph by hyperedges (light blue boxes) which are determined by the connected components
of1-GWL-1 indistinguishable node sets. The nodes can now be relabeled by 1-GWL-1. All hyperedges can
be assigned learnable weights. For downstream training, the new hyperedges are randomly added and the
existing hyperedges within each new hyperedge are randomly dropped.
Our method is a preprocessing algorithm that operates on the input hypergraph. In order to increase
expressivity, we search for potentially indistinguishable regular induced subhypergraphs so that they can
be replaced with hyperedges that span the subhypergraph to break the symmetries that prevent GWL-1
from being more expressive. We devise an algorithm, which is shown in Algorithm 1. It takes as input a
hypergraphHwith star expansion matrix H. The idea of the algorithm is to identify nodes of the same
GWL-1 value that are maximally connected and use this collection of node subsets to break the symmetry
ofH.
First we introduce some combinatorial definitions for hypergraph data that we will use in our algorithm:
Definition 5.1. A hypergraphH= (V,E)isconnected ifBV,Eis a connected graph.
Aconnected component ofHis a connected induced subhypergraph which is not properly contained in any
connected subhypergraph of H.
Our preprocessing algorithm is explicitly given in Algorithm 1. We describe it in words here:
Algorithm:
1. For a given L∈Z+and anyL-GWL-1 node value cL, the first step is to construct the induced
subhypergraphHcLfrom theL-GWL-1 class of nodes:
VcL≜{v∈V:cL=hL
v(H)}, (11)
wherehL
vdenotes the L-GWL-1 class of node v.
2. We then compute the connected components of HcL. DenoteCcLas the set of all connected compo-
nents ofHcL. IfL=∞, then drop L. Each of these connected components is a subhypergraph of
H, denotedRcL,iwhereRcL,i⊆HcL⊆Hfori= 1,...,|CcL|.
3. Gather the collection of node sets and hyperedge sets formed by RcL,i,i= 1,...,|CcL|.
We call the subhypergraphs in CcLfound by the symmetry finding Algorithm 1 as maximally connected
L-GWL-1 equal valued subhypergraphs .
12Published in Transactions on Machine Learning Research (11/2024)
Algorithm 1: A Symmetry Finding Algorithm
Data:HypergraphH= (V,E), represented by its star expansion matrix H.L∈Z+is the number
of iterations to run GWL-1.
Result: A pair of collections: (RV={VRj},RE=∪j{ERj})whereRjare disconnected
subhypergraphs exhibiting symmetry in Hthat are indistinguishable by L-GWL-1.
1UL←hL
v(H);GL←{UL[v] :∀v∈V}; /*UL[v]is the L-GWL-1 value of node v∈ V. */
2BVH,EH←Bipartite (H)/* Construct the bipartite graph from H. */
3RV←{};RE←{}
4forcL∈GLdo
5VcL←{v∈V:UL[v] =cL},EcL←{e∈E:u∈VcL,∀u∈e}
6CcL←ConnectedComponents (HcL= (VcL,EcL))
7forRcL,i∈CcLdo
8RV←RV∪{VRcL,i};RE←RE∪ERcL,i
9end
10end
11return (RV,RE)
Wewillusetheoutputhyperedgesofthepreprocessingalgorithmto"softly"covertheinputhypergraph. This
will introduce multiplicities to the hyperedges. We formally define such a generalization of a hypergraph,
called a multi-hypergraph here:
Definition 5.2. Amulti-hypergraphH= (V,˜E)is a pair consisting of a set of nodes Vand a multiset of
hyperedges ˜E≜(E,m)whereE⊆2Vandm:E→Z+is a multiplicity function.
The star expansion incidence matrix of a multi-hypergraph Hcan now be defined as:
Definition 5.3. The star expansion incidence matrix Hof a multi-hypergraph H= (V,˜E)is the|V|× (2|V|×
Z+)(infinite) 0-1incidence matrix HwhereHv,e= 1iffv∈efor(v,e)∈V× ˜Efor some fixed orderings
on bothVand2V×Z+.
In practice, of course, the multiplicity function of His bounded and only the nonzeros of this matrix are
kept track of. Thus the star expansion incidence matrix is actually a sparse finite matrix.
On a multi-hypergraph H= (V,E), we can define the degree of a vertex v∈Vin terms of the cardinality
of a multiset: deg(v)≜|{{e:e∋v}}|=/summationtext
e∋vm(e)wherem(e)is the multiplicity, or number of repeated
occurrences, of ein{{e:e∋v}}.
LettingEH(H)be the multiset of hyperedges of HandVH(H)the set of nodes of H, message passing through
incidences is still well defined. We summarize this in the following proposition:
Proposition 5.1. The GWL-1 algorithm of Equation 5 can be computed on a multi-hypergraph H
Thus,hi
v(H)andhi(S,H)are well defined on its star incidence matrix H.
This implication of Proposition 5.1 is that GWL-1 based hyperGNNs can learn on multi-hypergraphs using
the star expansion incidence matrix representations.
Downstream Training: After executing Algorithm 1, we collect its output (RV,RE). During training,
for eachi= 1,...,|CcL|we randomly perturb RcL,ito form a random multi-hypergraph ˆHLby:
•Attaching (with multiplicity) a single hyperedge that covers VRcL,iwith probability qiand not
attaching with probability 1−qi.
•All the hyperedges in RcL,iare dropped or kept with probability pand1−prespectively.
Our method is similar to the concept of adding virtual nodes Hwang et al. (2022) in graph representation
learning. This is due to the equivalence between virtual nodes and hyperedges by Proposition 4.1. For a
13Published in Transactions on Machine Learning Research (11/2024)
guarantee of improving expressivity, see Lemma 5.4 and Theorems 5.5, 5.6. For an illustration of the data
augmentation, see Figure 2.
Alternatively, downstream training using the output of Algorithm 1 can be done. Similar to subgraph NNs,
this is done by applying an ensemble of models Alsentzer et al. (2020); Papp et al. (2021); Tan et al. (2023),
with each model trained on transformations of Hwith its symmetric subhypergraphs randomly replaced.
This, however, is computationally expensive.
Illustration: In Figure 3 an illustration of Algorithm 1 is shown. For the hypergraph shown in Figure
3 (a), two graph cycles are glued at a single node while a separate hypergraph is glued at that same node.
With 1-GWL- 1, there is a large class of nodes labeled "a" that are indistinguishable. Each of the connected
components of these 1-GWL- 1node classes is covered by a single hyperedge (light blue box) to form a multi-
hypergraph. We show in Section 5.1 that in this multihypergraph, the nodes express more of the original
hypergraph. The data augmentation procedure during downstream training can then be applied to the blue
boxes and the original hyperedges within each blue box separately.
5.1 Algorithm Guarantees
We show some guarantees for the output of Algorithm 1. We will assume the following notation to denote
the relevant substructures on a single hypergraph found by Algorithm 1.
Notation:
LetH= (V,E)be a hypergraph with star expansion matrix Has before.
Let(RV,RE)be the output of Algorithm 1 on HforL∈Z+. We call this collection of nodes and hyperedges
as GWL-1 symmetric induced components. Let:
ˆHL≜(V,E⊔RV) (12)
be the multi-hypergraph formed from Hafter adding all the hyperedges from RVand let ˆHLbe the star
expansion matrix of the resulting multi-hypergraph ˆHL. Let:
VcL,s≜{v∈VcL:v∈R,R∈CcL,|VR|=s} (13)
be the set of all nodes of L-GWL-1 class cLbelonging to a connected component in CcLofs≥1nodes in
HcL, the induced subhypergraph of L-GWL-1. Let:
GL≜{hL
v(H) :v∈V} (14)
be the set of all L-GWL-1 values on H. Let:
ScL≜{|VRcL,i|:RcL,i∈CcL} (15)
be the set of node set sizes of the connected components in HcL.
Properties of (RV,RE)from Algorithm 1:
In the following proposition, we show that the subhypergraphs found by Algorithm 1 are GWL-1 symmetric.
This should be evident by line 5 of Algorithm 1 where an induced subgraph of the bipartite graph is formed
from nodes of the same L-GWL-1 values.
Proposition 5.2. IfL=∞, for any GWL-1 node value ccomputed onH, all connected component
subhypergraphsRc,i∈Ccare GWL-1 symmetric as hypergraphs.
Algorithm 1 outputs a partition of the entire hypergraph. This follows from the fact that GWL-1 already
covers the entire hypergraph after a single hop.
Proposition 5.3. IfL≥1, the output (RV,RE)of Algorithm 1 partitions a subgraph of H, meaning:
V=⊔V∈RVVandE⊃⊔E∈REE (16)
14Published in Transactions on Machine Learning Research (11/2024)
The output of Algorithm 1 can additionally be used to guarantee improvement in expressing the hypergraph:
Prediction Guarantees:
In order to guarantee that the GWL-1 symmetric components Rc,ifound by Algorithm 1 carry additional
information, there needs to be a separation between them to prevent an intersection between the rooted
trees computed by GWL-1. We define what it means for two node subsets to be sufficiently separated via
the shortest hyperedge path distance between nodes in Vas follows:
Definition 5.4. Two subsets of nodes U1,U2⊆Varesufficiently L-separated if:
min
v1∈U1,v2∈U2d(v1,v2)>L (17)
whered(v1,v2)≜mine1,...,ek∈E,v1∈e1,v2∈ekkis the shortest hyperedge path distance from v1∈Vtov2∈V.
A collection of node subsets C⊆2Vissufficiently L-separated if all pairs of node subsets are sufficiently
L-separated .
Our definition of sufficiently L-separated is similar in nature to that of well separation between point sets
Callahan & Kosaraju (1995) in Euclidean space. Assuming that the CcLare sufficiently L-separated from
each other, intuitively meaning that no two nodes from two separate VRcL,i∈RVare withinLhyperedges
away, then the cardinality of each component |VRcL,i|is recognizable. This is stated in the following lemma:
Lemma 5.4. IfL∈Z+is small enough so that after running Algorithm 1 on L, for anyL-GWL-1 node
classcLonVthe collection ofCcLissufficiently L-separated ,
then after forming ˆHL, the newL-GWL-1 node classes of VRcL,ifori= 1,...,CcLinˆHLare all the same
classc′
Lbut are distinguishable from cLdepending on|VRcL,i|.
We can then use this lemma to show that under certain conditions for large hypergraphs, augmenting the
hypergraphHto the multi-hypergraph ˆHLwill give a guarantee on the number of pairs of k-node sets that
become distinguished which were indistinguishable as sets of GWL-1 values.
Theorem 5.5. Let|V|=n,L∈Z+andvol(v)≜/summationtext
e∈E:e∋v|e|and assuming that the collection of node
subsetsCcLis sufficiently L-separated.
Ifvol(v) =O(log1−ϵ
4Ln),∀v∈Vfor any constant ϵ >0;|ScL|≤S,∀cL∈CL,Sconstant, and|VcL,s|=
O(nϵ
log1
2k(n)),∀s∈CcL, then fork∈Z+andk-tupleC= (cL,1,...,cL,k),cL,i∈GL,i= 1..kthere exists
ω(n2kϵ)many pairs of k-node setsS1̸≃S2such that (hL
u(H))u∈S1= (hL
v∈S2(H)) =C, as ordered k-tuples,
whileh(S1,ˆHL)̸=h(S2,ˆHL)also byLsteps of GWL-1.
The conditions of Theorem 5.5 assume an arbitrarily large hypergraph that is sparse and for every cL∈GL
hasCcLsufficiently L-separated, has a bounded set of node set sizes from ScLand a controlled growth for
each node set size from ScL. The idea behind the proof of Theorem 5.5 is that under these conditions, there
are enough isomorphic rooted trees computed by L-GWL-1. It can then be shown that over all pairs of
k-sets of nodes with elementwise isomorphic rooted trees, that they can be distinguished by the component
size they belong in. We give a simple example hypergraph that illustrates the condition of Theorem 5.5.
Example: A simple example of a hypergraph that statisfies the conditions of Theorem 5.5 is a union
of many disconnected hypergraphs H=∪iHi= (V,E)with|VHi|≤SwhereS <∞is a small constant
independent of n=|V|≥S. Such a hypergraph could be a social network where the nodes are user instances
and the hyperedges are private groups. The disconnected hypergraphs represent disconnected communities
where a user can only belong to a single community.
We show that our algorithm increases expressivity (Definition 2.8) for h(S,H)of Equation 7.
Theorem 5.6 (Invariance and Expressivity) .IfL=∞, GWL-1 enhanced by Algorithm 1 is still invari-
ant to node isomorphism classes of Hand can be strictly more expressive than GWL-1 to determine node
isomorphism classes.
15Published in Transactions on Machine Learning Research (11/2024)
Proving expressivity from Theorem 5.4 follows from the added information of component sizes viewable by
each node in its vicinity. Proving the invariance from Theorem 5.4 follows by a proof by contradiction, which
uses the maximality of the connected components found in Algorithm 1.
When training on a hypergraph with the data augmentations, every possible symmetry observed from the
random augmentations will be learned. Thus the symmetry group of hLon the random matrix ˆHLis
isomorphic to the intersection of all symmetries over each augmentation sample. This is expressed as follows:
Sym(hL(ˆHL))≜/intersectiondisplay
ˆH′
L∼P(ˆHL)Sym(hL(ˆH′
L)) (18)
We show that the intersection over all symmetries of the estimated multi-hypergraph ˆHLhas fewer sym-
metries than that of the L-GWL-1 view of the hypergraph as a collection of rooted trees. We call this
symmetry breaking .
Proposition 5.7. The multi-hypergraph ˆHLbreaks the symmetry of the L-GWL-1 view of the hypergraph
H:
Sym(hL(ˆHL))⊆Autc(˜B2L
V,E),∀L≥1 (19)
This follows by the fact that the identity augmentation is in the support of the distribution of random
augmentations and that Sym(hL(ˆHL))∼=Autc(˜B2L
V,E(ˆH))by Theorem 4.4.
Since hyperGNNs represent each node v∈Vby message passing through the neighbors in the rooted tree
(˜BV,E)vatv. If probabilities are assigned between nodes, then Tlayers of a hyperGNN can be viewed as
computing the random walk probability of ending on any node starting from some uniformly chosen node.
We define these terms in the following:
Definition5.5 (Chitra&Raphael(2019)) .Arandom walk on a (multi) hypergraph H= (V,E)is a Markov
chain with state space Vwith transition probabilities Pu,v≜/summationtext
e⊃{u,v}:e∈Eω(e)
deg(u)|e|, whereω(e) :E→ [0,1]is
some discrete probability distribution on the hyperedges. When not specified, this is the constant 1function.
AssumingHis connected, let Xt∈Vdenote the state of the Markov chain at step twithP(X0=v) =
1
|V|,∀v∈V. Lettingt→∞, this probability converges to the stationary distribution on the nodes V, which
is independent of the time. This is expressed in the following definition:
Definition 5.6. Astationary distribution π:V→ [0,1]for a Markov chain with transition probabilities
Pu,vis defined by the relationship/summationtext
u∈VPu,vπ(u) =π(v).
For a (multi) hypergraph random walk we have the closed form: π(v) =deg(v)/summationtext
u∈Vdeg(u)forv∈VassumingH
is a connected (multi) hypergraph.
For the downstream training, we show that there are Bernoulli hyperedge drop/attachment probabilities
p,qirespectively for each RcL,iso that the stationary distribution doesn’t change. This shows that our data
augmentation can still preserve the low frequency random walk signal.
Proposition 5.8. For a connected hypergraph H= (V,E), let(RV,RE)be the output of Algorithm 1 on H.
Then there are Bernoulli probabilities p,qifori= 1,...,|RV|for attaching a covering hyperedge so that ˆπis
an unbiased estimator of π.
The intuition for Proposition 5.8 is that if a hyperedge is added to cover a connected subhypergraph RcL,i
containing at least one hyperedge, then allowing any of the hyperedges in RcL,ito drop is enough to keep
the estimated stationary distribution ˆπunbiased.
Time Complexity:
Proposition 5.9 provides the time complexity of our algorithm.
Proposition 5.9 (Complexity) .Algorithm 1 runs in time O(nnz(H)L+ (n+m)), which is order linear
in the size of the input star expansion matrix Hfor hypergraphH= (V,E), ifLis independent of nnz(H),
wheren=|V|,nnz(H) =vol(V)≜/summationtext
v∈Vdeg(v)andm=|E|.
16Published in Transactions on Machine Learning Research (11/2024)
Since Algorithm 1 runs in time linear in the size of the input when Lis constant, in practice it only takes a
small fraction of the training time for hypergraph neural networks.
6 Evaluation
Table 1: Transductive hyperedge prediction PR-AUC scores on six different hypergraph datasets. The
highest scores per hyperGNN architecture (row) is colored. Red text denotes the highest average scoring
method. Orange text denotes a two-way tie and brown text denotes a three-way tie. All datasets involve
predicting hyperedges of size 3.
PR-AUC↑Baseline Ours Baseln.+edrop
HGNN 0.98±0.03 0.99±0.08 0.96±0.02
HGNNP 0.98±0.02 0.98±0.09 0.96±0.10
HNHN 0.98±0.01 0.96±0.07 0.97±0.04
HyperGCN 0.98±0.07 0.98±0.11 0.98±0.03
UniGAT 0.99±0.06 0.99±0.03 0.99±0.07
UniGCN 0.99±0.00 0.99±0.03 0.99±0.08
UniGIN 0.87±0.02 0.86±0.10 0.85±0.08
UniSAGE 0.86±0.04 0.86±0.05 0.84±0.09
(a) cat-edge-DAWNPR-AUC↑Baseline Ours Baseln.+edrop
HGNN 0.90±0.13 1.00±0.00 0.90±0.13
HGNNP 0.90±0.09 1.00±0.07 1.00±0.03
HNHN 0.90±0.09 0.91±0.02 0.90±0.08
HyperGCN 1.00±0.00 1.00±0.03 1.00±0.02
UniGAT 0.90±0.06 1.00±0.03 1.00±0.06
UniGCN 1.00±0.01 0.91±0.01 0.82±0.09
UniGIN 0.90±0.12 0.95±0.06 0.90±0.11
UniSAGE 0.90±0.16 1.00±0.08 0.90±0.17
(b) cat-edge-music-blues-reviewsPR-AUC↑Baseline Ours Baseln.+ edrop
HGNN 0.96±0.10 0.98±0.05 0.96±0.04
HGNNP 0.96±0.05 0.98±0.09 0.97±0.07
HNHN 0.96±0.02 0.97±0.08 0.97±0.06
HyperGCN 0.93±0.05 0.98±0.07 0.96±0.09
UniGAT 0.96±0.01 0.98±0.14 0.97±0.04
UniGCN 0.96±0.04 0.96±0.11 0.96±0.09
UniGIN 0.97±0.03 0.97±0.11 0.96±0.05
UniSAGE 0.96±0.10 0.96±0.10 0.96±0.02
(c) contact-high-school
PR-AUC↑Baseline Ours Baseln.+edrop
HGNN 0.95±0.03 0.96±0.01 0.95±0.03
HGNNP 0.95±0.02 0.96±0.09 0.96±0.07
HNHN 0.94±0.07 0.97±0.10 0.95±0.05
HyperGCN 0.97±0.01 0.97±0.05 0.96±0.08
UniGAT 0.95±0.02 0.98±0.07 0.98±0.02
UniGCN 0.96±0.00 0.97±0.14 0.97±0.10
UniGIN 0.95±0.09 0.97±0.02 0.95±0.05
UniSAGE 0.96±0.08 0.95±0.05 0.96±0.02
(d) contact-primary-schoolPR-AUC↑Baseline Ours Baseln.+edrop
HGNN 0.95±0.07 0.97±0.08 0.96±0.07
HGNNP 0.95±0.07 0.96±0.02 0.96±0.01
HNHN 0.94±0.01 0.97±0.02 0.95±0.06
HyperGCN 0.92±0.01 0.94±0.06 0.94±0.08
UniGAT 0.94±0.08 0.98±0.14 0.97±0.08
UniGCN 0.97±0.08 0.97±0.14 0.97±0.06
UniGIN 0.93±0.07 0.94±0.11 0.93±0.09
UniSAGE 0.93±0.07 0.93±0.08 0.92±0.04
(e) email-EuPR-AUC↑Baseline Ours Baseln.+edrop
HGNN 0.75±0.09 0.85±0.09 0.71±0.14
HGNNP 0.83±0.09 0.85±0.08 0.85±0.04
HNHN 0.72±0.09 0.82±0.03 0.74±0.09
HyperGCN 0.87±0.08 0.83±0.05 1.00±0.07
UniGAT 0.80±0.09 0.83±0.03 0.78±0.05
UniGCN 0.84±0.08 0.89±0.10 0.71±0.07
UniGIN 0.69±0.14 0.76±0.05 0.61±0.11
UniSAGE 0.72±0.11 0.71±0.10 0.64±0.10
(f) cat-edge-madison-restaurants
We evaluate our method on higher order link prediction with many of the standard hypergraph neu-
ral network methods. Due to potential class imbalance, we measure the PR-AUC of higher order
link prediction on the hypergraph datasets. These datasets are: cat-edge-DAWN, cat-edge-music-
blues-reviews, contact-high-school, contact-primary-school, email-Eu, cat-edge-madison-
restaurants . These datasets range from representing social interactions as they develop over time to
collections of reviews to drug combinations before overdose. We also evaluate on the amherst41 dataset,
which is a graph dataset. All of our datasets are unattributed hypergraphs/graphs.
Data Splitting: For the hypergraph datasets, each hyperedge in it is paired with a timestamp (a real
number). These timestamps are a physical time for which a higher order interaction, represented by a
hyperedge, occurs. We form a train-val-test split by letting the train be the hyperedges associated with the
80th percentile of timestamps, the validation be the hyperedges associated with the timestamps in between
the 80th and 85th percentiles. The test hyperedges are the remaining hyperedges. The train validation and
test datasets thus form a partition of the nodes. We do the task of hyperedge prediction for sets of nodes
of size 3, also known as triangle prediction. Half of the size 3hyperedges in each of train, validation and
test are used as positive examples. For each split, we select random subsets of nodes of size 3that do not
form hyperedges for negative sampling. We maintain positive/negative class balance by sampling the same
number of negative samples as positive samples. Since the test distribution comes from later time stamps
than those in training, there is a possibility that certain datasets are out-of-distribution if the hyperedge
distribution changes.
For the graph dataset, the single graph is deterministically split into 80/5/15 for train/val/test. We remove
10%of the edges in training and let them be positive examples Ptrto predict. For validation and test, we
remove 50%of the edges from both validation and test to set as the positive examples Pval,Pteto predict.
For train, validation, and test, we sample |Ptr|,|Pval|,|Pte|negative link samples from the links of train,
validation and test.
17Published in Transactions on Machine Learning Research (11/2024)
Table 2: PR-AUC on graph dataset amherst41 . Each column is a comparison of the baseline PR-AUC
scores against the PR-AUC score for our method (first row) applied to a standard hyperGNN architecture.
The coloring scheme is the same as in Table 1.
PR-AUC↑ HGNN HGNNP HNHN HyperGCN UniGAT UniGCN UniGIN UniSAGE
Ours 0.73±0.10 0.61±0.05 0.64±0.06 0.71±0.09 0.72±0.08 0.70±0.08 0.73±0.03 0.73±0.06
hyperGNN Baseline 0.62±0.09 0.62±0.10 0.63±0.04 0.71±0.07 0.70±0.06 0.69±0.07 0.73±0.06 0.73±0.09
hyperGNN Baseln.+edrop 0.61±0.03 0.61±0.03 0.61±0.09 0.71±0.06 0.71±0.02 0.69±0.05 0.73±0.09 0.73±0.04
APPNP 0.42±0.07 0.42±0.07 0.42±0.07 0.42±0.07 0.42±0.07 0.42±0.07 0.42±0.07 0.42±0.07
APPNP+edrop 0.42±0.03 0.42±0.03 0.42±0.03 0.42±0.03 0.42±0.03 0.42±0.03 0.42±0.03 0.42±0.03
GAT 0.49±0.06 0.49±0.06 0.49±0.06 0.49±0.06 0.49±0.06 0.49±0.06 0.49±0.06 0.49±0.06
GAT+edrop 0.49±0.06 0.49±0.06 0.49±0.06 0.49±0.06 0.49±0.06 0.49±0.06 0.49±0.06 0.49±0.06
GCN2 0.56±0.12 0.56±0.12 0.56±0.12 0.56±0.12 0.56±0.12 0.56±0.12 0.56±0.12 0.56±0.12
GCN2+edrop 0.54±0.02 0.54±0.02 0.54±0.02 0.54±0.02 0.54±0.02 0.54±0.02 0.54±0.02 0.54±0.02
GCN 0.40±0.03 0.40±0.03 0.40±0.03 0.40±0.03 0.40±0.03 0.40±0.03 0.40±0.03 0.40±0.03
GCN+edrop 0.65±0.04 0.65±0.04 0.65±0.04 0.65±0.04 0.65±0.04 0.65±0.04 0.65±0.04 0.65±0.04
GIN 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10
GIN+edrop 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10
GraphSAGE 0.44±0.01 0.44±0.01 0.44±0.01 0.44±0.01 0.44±0.01 0.44±0.01 0.44±0.01 0.44±0.01
GraphSAGE+edrop 0.44±0.10 0.44±0.10 0.44±0.10 0.44±0.10 0.44±0.10 0.44±0.10 0.44±0.10 0.44±0.10
6.1 Architecture and Training
Our algorithm serves as a preprocessing step for selective data augmentation. Given a single training
hypergraphH, the Algorithm 1 is applied and during training, the identified hyperedges of the symmetric
induced subhypergraphs of Hare randomly replaced with single hyperedges that cover all the nodes of each
induced subhypergraph. Each symmetric subhypergraph has a p= 0.5probability of being selected. To get
a large set of symmetric subhypergraphs, we run 2iterations of GWL-1.
We implement h(S,H)from Equation 7 as follows. Upon extracting the node representations from the
hypergraph neural network, we use a multi-layer-perceptron (MLP) on each node representation, sum across
such compositions, then apply a final MLP layer after the aggregation. We use the binary cross entropy loss
on this multi-node representation for training. We always use 5layers of hyperGNN convolutions, a hidden
dimension of 1024, and a learning rate of 0.01.
6.2 Higher Order Link Prediction Results
We show in Table 1 the comparison of PR-AUC scores amongst the baseline methods of HGNN, HGNNP,
HNHN, HyperGCN, UniGIN, UniGAT, UniSAGE, their hyperedge dropped versions, and "Our" method,
which preprocesses the hypergraph to break symmetry during training. For the hyperedge drop baselines,
there is a uniform 50%chance of dropping any hyperedge. We use the Laplacian eigenmap Belkin & Niyogi
(2003) positional encoding on the clique expansion of the input hypergraph. This is common practice in
(hyper)link prediction and required for using a hypergraph neural network on an unattributed hypergraph.
We show in Table 2 the PR-AUC scores on the amhrest41 . Along with hyperGNN architectures we use for
the hypergraph experiments, we also compare with standard GNN architectures: APPNP Gasteiger et al.
(2018), GAT Veličković et al. (2017), GCN2 Chen et al. (2020a), GCN Kipf & Welling (2016a), GIN Xu
et al. (2018), and GraphSAGE Hamilton et al. (2017). For every hyperGNN/GNN architecture, we also
apply drop-edge Rong et al. (2019) to the input graph and use this also as baseline. The number of layers of
each GNN is set to 5and the hidden dimension at 1024. For APPNP and GCN2, one MLP is used on the
initial node positional encodings.
Overall, our method performs well across a diverse range of higher order network datasets. We observe
that our method can often outperform the baseline of not performing any data perturbations as well as
the same baseline with uniformly random hyperedge dropping. Our method has an added advantage of
being explainable since our algorithm works at the data level. There was also not much of a concern for
computational time since our algorithm runs in time O(nnz(H) +n+m), which is optimal since it is the
size of the input.
18Published in Transactions on Machine Learning Research (11/2024)
64 128 256 512 1024 2048
Number of Nodes020406080100Size of Maximally Connected GWL-1 Classes
(a) Boxplot of the sizes of the connected
components with equal GWL-1 node values from
the hy-MMSBM sampling algorithm where there
are three independent communities.
64 128 256 512 1024 2048
Number of Nodes020406080100120140Size of Maximally Connected GWL-1 Classes
(b) Boxplot of the sizes of the connected
components with equal GWL-1 node values from
the hy-MMSBM sampling algorithm where any
two of the three communities can communicate.
Figure 4: Experiment on the relationship between the sizes of connected components of equal GWL-1 node
values and the communication between communities.
6.3 Empirical Observations on the Components Discovered by the Algorithm
According to Proposition B.14, we know that the symmetry finding algorithm always covers the hypergraph
and thus that we can generate on the order of O(2|V|)counterfactual multi-hypergraphs. It is known that
a large set of data augmentations during learning improves learner generalization. The cover by GWL-1
symmetric components follows a distribution depending on the data.
We show in Figure 4 the distributions for the component sizes over all GWL-1 symmetric connected compo-
nents for samples from the Hy-MMSBM model Ruggeri et al. (2023). This is a model to sample hypergraphs
with community structure. In Figure 4a we sample hypergraphs with 3isolated communities, meaning that
there is 0chance of any interconnections between any two communities. In Figure 4b we sample hypergraphs
with 3communities where every node in a community has a weight of 1to stay in its community and a
weight of 0.2to move out to any other community. We plot the boxplots as a function of increasing num-
ber of nodes. We notice that the more communication there is between communities for more nodes there
is more spread in possible connected component sizes. Isolated communities should make for predictable
clusters/connected components.
7 Discussion
Our proposed data augmentation method uses symmetry breaking to handle the symmetries induced by
GWL-1 based hyperGNNs. Symmetry breaking provides some guarantees that the symmetries induced by
the hyperGNN are brought closer to the symmetries of the training hypergraph. These include hyperlink
prediction guarantees. In addition, symmetry breaking prepares the hyperGNN for an automorphism group
different from the symmetries it views during training. This brings about an important question regarding
the invariant automorphism group across training and testing, namely:
What is the relationship between the symmetries of the training and testing hypergraphs?
We answer this question in the context of a temporal shift from training to testing. The term "temporal shift"
is usually used in the context of distribution shifts Yao et al. (2022a). We define it in terms of transductive
hyperlink prediction.
19Published in Transactions on Machine Learning Research (11/2024)
A temporal shift from training to testing means that the testing hyperedges have a temporal future relation-
ship with the training hyperedges.
We formalize a hypergraph in terms of a physical system that can change over time with the following
assumptions.
Assumption 7.1. If we view a hypergraph as a physical system of particles where only the nodes have mass,
then the task of transductive higher order link prediction is on a closed system Landau & Lifshitz (1960).
This means no mass can enter or leave this system.
GivennnodesV, we can view each node v∈Valong with the hypergraph H= (V,E)it belongs in as a
microstate (v,H)of a statistical ensemble.
In Rashevsky (1955) the entropy of a graph is defined through the orbits of the automorphism group. We
generalize this to a node based definition for hypergraphs. Our definition uses a similar probability on mi-
crostates. We define the hypergraph topological entropy of a hypergraphH= (V,E)by:
S≜−/summationdisplay
v∈Vpv(H) log(pv(H));
withpv(H)≜nv(H)/summationtext
v∈Vnv(H)(20)
wherenv(H)≜|{u∈V:u∼=Hv}|is the number of nodes u∈Visomorphic to node v, including vitself,
(See Definition 2.7).
Assumption 7.2. (Second Law of Thermodynamics ) : The second law of thermodynamics Carnot
(1978) states that the entropy of a closed system must increase over time. This is denoted by the following
equation:
∆S >0 (21)
This law can be used in terms of the hypergraph topological entropy as defined in Assumption 7.1.
We also assume the following random model on the hypergraph we predict on. Let •=tr,te represent
temporally ordered training and testing distributions where (E•)gt= (E•)gt\E•are the hyperedges that
completeH•to(H•)gtand are predicted from E•:
Assumption7.3. For each node v∈V, letXvbe independent Bernoulli random variables ofsome probability
qv∈[0,1]. Let|(Ete)gt|be the number of testing hyperedges of H. It is a random variable defined by:
f((Xv)v∈V) =/summationdisplay
e⊆VΠu∈eXu (22)
Assume further that |(Ete)gt|,(V,(Ete)gt)∼P(H;tte)only depends on n.
Let˜Xvbe a Bernoulli random variable with probability qmax. Assume that fsatisfies
P(f(˜Xv̸=u,...,do (˜Xu= 1),...,˜Xv̸=u)−f(˜Xv̸=u,...,do (˜Xu= 0),...,˜Xv̸=u)≤2)≤n−ω(1),∀u∈V(23)
These assumptions show that with high probability there are node isomorphism classes which decrease in
size:
Theorem 7.1. Under Assumptions 7.1, 7.3, and the Second Law of Thermodynamics on the hypergraph
viewed as a closed system:
∃U⊆V,U̸=∅,so that:nv((Htr)gt)>nv((Hte)gt),∀v∈U, with probability 1−O(1√n)(24)
Proof.By the second law of thermodynamics, we must have that between the training hypergraph H=
(V,(Etr)gt)and testing hypergraph (Hte)gt= (V,(Ete)gt)which is temporally later than (Htr)gt,
∆S=−/summationdisplay
v∈Vpv((Hte)gt) log(pv((Hte)gt)) +/summationdisplay
v∈Vpv(H) log(pv((Htr)gt))>0 (25a)
20Published in Transactions on Machine Learning Research (11/2024)
If we take an upper bound on ∆S, we get the following consequence:
0<∆S≤/summationdisplay
v∈Vlog(pv((Htr)gt)
pv((Hte)gt))⇒pv((Htr)gt)>pv((Hte)gt),∀v∈U,for someU⊆V,U̸=∅(26)
Ifnv((Htr)gt)> nv((Hte)gt),∀v∈U ⊆V , we can conclude that some nodes will shrink the size of their
ground truth isomorphism class.
We show that this occurs with high probability by bounding the complementary case.
Nodes rarely increase their isomorphism class size:
For the complementary case, nv((Htr)gt)≤nv((Hte)gt),∀v∈U⊆V , we show that under Assumption 7.3,
node isomorphism class cardinality growth occurs with low probability due to an anti-concentration bound
on multivariate polynomials Fox et al. (2021).
In this complementary case, we must have that the nodes in U⊆Vincreased the number of nodes isomorphic
to them. This implies that each of these nodes v∈Umust have changed their degree vector upon changing
(Htr)gtto(Hte)gt. This change requires that some other node u∈V,u̸=vobtains a degree vector equal to
the degree vector of v.
Let us define this space of all possible testing hyperedge sets with this necessary condition:
DegVeceq(v,H)≜{E∈supp(Pte((Ete)gt)) :∃u∈V,degvec(V,E)(v) =degvec(V,E)(u),u̸=v}(27)
We can express the relationship between the change in nvwith membership in DegVeceq(v,H):
nv(H)≤nv(Hgt)⇒(Ete)gt∈DegVeceq(v,H) (28)
Letting
xmin(v) = min
E∈DegVeceq(v,H)|E|,∀v∈V (29a)
and
xmax(v) = max
E∈DegVeceq(v,H)|E|,∀v∈V (29b)
We can then say that the number of testing hyperedges |(Ete)gt|is in the interval [xmin(v),xmax(v)]:
(Ete)gt∈DegVeceq(v,H)⇒xmin(v)≤f((Xu)u∈V)≤xmax(v) (30)
Let the diameter of the interval [xmin(v),xmax(v)]be given by D:
D≜xmax(v)−xmin(v) (31)
This only depends on nsince the minimizers and maximizers Exmin,Exmaxof Equations 29a and 29b satisfy:
Exmin,Exmax∈DegVeceq(v,H)andthusbelongtothe supp(P(E;tte). Weknowthatany E∈supp(P(E;tte)
has that|E|depends only on nby Assumption 7.3.
Define the median Mon[xmin(v),xmax(v)]by:
M≜xmax(v) +xmin(v)
2(32)
We thus have that:
xmin(v)≤f((Xu)u∈V)≤xmax(v)⇒|f((Xu)u∈V)−M|≤D (33)
Piecing together Equations 28, 30, and 33, we have by monotonicity:
P(nv(H)≤nv(Hgt))≤P(xmin(v)≤f((Xu)u∈V)≤xmax(v)) (34a)
21Published in Transactions on Machine Learning Research (11/2024)
≤P(|f((Xu)u∈V)−M|<D) (34b)
≤P(|M−f((˜Xu)u∈V)|<D)≤O(1√n),∀v∈U⊆V (34c)
Where the last inequality comes from the anti-concentration bound of Theorem 1.2 of Fox et al. (2021),
which states:
P(|f((˜Xv)v∈V)−x|<s)≤O(1√n),∀x∈R (35)
for anys>0which may depend on nand anyx∈R. Settingx:=M,s :=D, gives the last inequality.
Theorem7.1statesthatthegroundtruthnodeisomorphismclassesforthenodesmustshrinkwithprobability
on order 1−O(1√n). Thus, for large n > > 0, we have that with high probability that the nodes v∈U⊆
V,U̸=∅havenv((Htr)gt)>nv((Hte)gt).
We can recognize this property of (Hte)gtby shrinking the automorphism group that the hypergraph encoder
recognizes from the training hypergraph. According to Proposition 5.7, symmetry breaking as given in
Equation 18, does this.
Our method breaks the symmetry of a GWL-1 based hyperGNN. This, of course, is not of importance if the
symmetry group of the GWL-1 based hyperGNN on some training hypergraph is already the trivial group.
Nonetheless, our symmetry breaking method is theoretically beneficial. Our method can also be used within
other downstream learning methods such as feature averaging Lyle et al. (2020), and ensemble methods, as
mentioned in Section 5.
8 Conclusion
Many existing hyperGNN architectures are based on the GWL-1 algorithm, which is a hypergraph isomor-
phism testing algorithm. We have characterized and identified the limitations of GWL-1. GWL-1 views the
hypergraph as a collection of rooted trees. This means that hyperGNNs recognize more symmetries than the
natural automorphisms of the training hypergraph. In fact, maximally connected subsets of nodes that share
the same value of GWL-1, which act like regular hypergraphs, are indistinguishable. To address this issue
while respecting the structure of a hypergraph, we have devised a preprocessing algorithm that identifies
all such connected components. These components cover the hypergraph and allow for downstream data
augmentation of symmetry breaking by training on a random multi-hypergraph. We show that this approach
improves the expressivity of a hyperGNN learner, including in the case of hyperlink prediction. We perform
extensive experiments to evaluate the effectiveness of our approach and make empirical observations about
the output of the algorithm on hypergraph data.
22Published in Transactions on Machine Learning Research (11/2024)
References
Sameer Agarwal, Kristin Branson, and Serge Belongie. Higher order learning with graphs. In Proceedings of
the 23rd international conference on Machine learning , pp. 17–24, 2006.
Emily Alsentzer, Samuel Finlayson, Michelle Li, and Marinka Zitnik. Subgraph neural networks. Advances
in Neural Information Processing Systems , 33:8017–8029, 2020.
Ilya Amburg, Nate Veldt, and Austin R. Benson. Clustering in graphs and hypergraphs with categorical
edge labels. In Proceedings of the Web Conference , 2020a.
Ilya Amburg, Nate Veldt, and Austin R Benson. Fair clustering for diverse and experienced groups.
arXiv:2006.05645 , 2020b.
Devanshu Arya, Deepak K Gupta, Stevan Rudinac, and Marcel Worring. Hypersage: Generalizing inductive
representation learning on hypergraphs. arXiv preprint arXiv:2010.04558 , 2020.
Song Bai, Feihu Zhang, and Philip HS Torr. Hypergraph convolution and hypergraph attention. Pattern
Recognition , 110:107637, 2021.
Pierre Baldi and Peter Sadowski. The dropout learning algorithm. Artificial intelligence , 210:78–122, 2014.
MikhailBelkinandParthaNiyogi. Laplacianeigenmapsfordimensionalityreductionanddatarepresentation.
Neural computation , 15(6):1373–1396, 2003.
Austin R. Benson, Rediet Abebe, Michael T. Schaub, Ali Jadbabaie, and Jon Kleinberg. Simplicial closure
and higher-order link prediction. Proceedings of the National Academy of Sciences , 2018a. ISSN 0027-8424.
doi: 10.1073/pnas.1800683115.
Austin R Benson, Rediet Abebe, Michael T Schaub, Ali Jadbabaie, and Jon Kleinberg. Simplicial closure and
higher-order link prediction. Proceedings of the National Academy of Sciences , 115(48):E11221–E11230,
2018b.
Garrett Birkhoff. Lattice theory , volume 25. American Mathematical Soc., 1940.
Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Lio, Guido F Montufar, and Michael
Bronstein. Weisfeiler and lehman go cellular: Cw networks. Advances in Neural Information Processing
Systems, 34:2625–2640, 2021.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translat-
ing embeddings for modeling multi-relational data. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahra-
mani, and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems , volume 26. Cur-
ran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper_files/paper/2013/file/
1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf .
Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M Bronstein. Improving graph neural
network expressivity via subgraph isomorphism counting. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 45(1):657–668, 2022.
Derun Cai, Moxian Song, Chenxi Sun, Baofeng Zhang, Shenda Hong, and Hongyan Li. Hypergraph structure
learning for hypergraph neural networks. In Proceedings of the Thirty-First International Joint Conference
on Artificial Intelligence, IJCAI-22 , pp. 1923–1929, 2022.
Paul B. Callahan and S. Rao Kosaraju. A decomposition of multidimensional point sets with applications
to k-nearest-neighbors and n-body potential fields. J. ACM, 42(1):67–90, jan 1995. ISSN 0004-5411. doi:
10.1145/200836.200853. URL https://doi.org/10.1145/200836.200853 .
Sadi Carnot. Réflexions sur la puissance motrice du feu . Number 26. Vrin, 1978.
Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority
over-sampling technique. Journal of artificial intelligence research , 16:321–357, 2002.
23Published in Transactions on Machine Learning Research (11/2024)
Can Chen and Yang-Yu Liu. A survey on hyperlink prediction. arXiv preprint arXiv:2207.02911 , 2022.
Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional
networks. In International conference on machine learning , pp. 1725–1735. PMLR, 2020a.
Samantha Chen, Sunhyuk Lim, Facundo Memoli, Zhengchao Wan, and Yusu Wang. Weisfeiler-lehman
meets gromov-Wasserstein. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang
Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning ,
volume 162 of Proceedings of Machine Learning Research , pp. 3371–3416. PMLR, 17–23 Jul 2022. URL
https://proceedings.mlr.press/v162/chen22o.html .
Samantha Chen, Sunhyuk Lim, Facundo Mémoli, Zhengchao Wan, and Yusu Wang. The weisfeiler-lehman
distance: Reinterpretation and connection with gnns. arXiv preprint arXiv:2302.00713 , 2023.
Shuxiao Chen, Edgar Dobriban, and Jane H Lee. A group-theoretic framework for data augmentation.
Journal of Machine Learning Research , 21(245):1–71, 2020b.
Eli Chien, Chao Pan, Jianhao Peng, and Olgica Milenkovic. You are allset: A multiset function framework
for hypergraph neural networks. arXiv preprint arXiv:2106.13264 , 2021.
Eli Chien, Chao Pan, Jianhao Peng, and Olgica Milenkovic. You are allset: A multiset function framework
for hypergraph neural networks. In International Conference on Learning Representations , 2022. URL
https://openreview.net/forum?id=hpBTIv2uy_E .
Uthsav Chitra and Benjamin Raphael. Random walks on hypergraphs with edge-dependent vertex weights.
In KamalikaChaudhuri andRuslan Salakhutdinov (eds.), Proceedings of the 36th International Conference
on Machine Learning , volume 97 of Proceedings of Machine Learning Research , pp. 1172–1181. PMLR,
09–15 Jun 2019. URL https://proceedings.mlr.press/v97/chitra19a.html .
Yun Young Choi, Sun Woo Park, Youngho Woo, and U Jin Choi. Cycle to clique (cy2c) graph neural
network: A sight to see beyond neighborhood aggregation. In The Eleventh International Conference on
Learning Representations .
Nicolas A Crossley, Andrea Mechelli, Petra E Vértes, Toby T Winton-Brown, Ameera X Patel, Cedric E
Ginestet, Philip McGuire, and Edward T Bullmore. Cognitive relevance of the community structure of
the human brain functional coactivation network. Proceedings of the National Academy of Sciences , 110
(28):11583–11588, 2013.
Janez Demšar. Statistical comparisons of classifiers over multiple data sets. The Journal of Machine learning
research, 7:1–30, 2006.
Yihe Dong, Will Sawin, and Yoshua Bengio. Hnhn: Hypergraph networks with hyperedge neurons. arXiv
preprint arXiv:2006.12278 , 2020.
David Steven Dummit and Richard M Foote. Abstract algebra , volume 3. Wiley Hoboken, 2004.
Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph neural networks. In
Proceedings of the AAAI conference on artificial intelligence , volume 33, pp. 3558–3565, 2019.
Yifan Feng, Jiashu Han, Shihui Ying, and Yue Gao. Hypergraph isomorphism computation. arXiv preprint
arXiv:2307.14394 , 2023.
Jacob Fox, Matthew Kwan, and Lisa Sauermann. Combinatorial anti-concentration inequalities, with appli-
cations. In Mathematical Proceedings of the Cambridge Philosophical Society , volume 171, pp. 227–248.
Cambridge University Press, 2021.
Yue Gao, Yifan Feng, Shuyi Ji, and Rongrong Ji. Hgnn+: General hypergraph neural networks. IEEE
Transactions on Pattern Analysis and Machine Intelligence , 2022.
24Published in Transactions on Machine Learning Research (11/2024)
Johannes Gasteiger, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate: Graph
neural networks meet personalized pagerank. arXiv preprint arXiv:1810.05997 , 2018.
LorenzoGiusti, ClaudioBattiloro, LuciaTesta, PaoloDiLorenzo, StefaniaSardellitti, andSergioBarbarossa.
Cell attention networks. arXiv preprint arXiv:2209.08179 , 2022.
Chris Godsil and Gordon F Royle. Algebraic graph theory , volume 207. Springer Science & Business Media,
2001.
Saiping Guan, Xiaolong Jin, Yuanzhuo Wang, and Xueqi Cheng. Link prediction on n-ary relational data.
InProceedings of the 28th International Conference on World Wide Web (WWW’19) , pp. 583–593, 2019.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances
in neural information processing systems , 30, 2017.
Allen Hatcher. Algebraic topology . Web, 2005.
Yang Hu, Xiyuan Wang, Zhouchen Lin, Pan Li, and Muhan Zhang. Two-dimensional weisfeiler-lehman graph
neural networks for link prediction. arXiv preprint arXiv:2206.09567 , 2022.
Jing Huang and Jie Yang. Unignn: a unified framework for graph and hypergraph neural networks. arXiv
preprint arXiv:2105.00956 , 2021.
EunJeong Hwang, Veronika Thost, Shib Sankar Dasgupta, and Tengfei Ma. An analysis of virtual nodes in
graph neural networks for link prediction. In The First Learning on Graphs Conference , 2022.
Jinwoo Kim, Saeyoon Oh, Sungjun Cho, and Seunghoon Hong. Equivariant hypergraph neural networks. In
European Conference on Computer Vision , pp. 86–103. Springer, 2022.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv
preprint arXiv:1609.02907 , 2016a.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308 , 2016b.
Oliver Knill. A brouwer fixed-point theorem for graph endomorphisms. Fixed Point Theory and Applications ,
2013(1):1–24, 2013.
Andreas Krebs and Oleg Verbitsky. Universal covers, color refinement, and two-variable counting logic:
Lower bounds for the depth. In 2015 30th Annual ACM/IEEE Symposium on Logic in Computer Science ,
pp. 689–700. IEEE, 2015.
Lev Davidovich Landau and Evgenii Mikhailovich Lifshitz. Mechanics , volume 1. CUP Archive, 1960.
Dongjin Lee and Kijung Shin. I’m me, we’re us, and i’m us: Tri-directional contrastive learning on hyper-
graphs.arXiv preprint arXiv:2206.04739 , 2022.
Jure Leskovec, Jon Kleinberg, and Christos Faloutsos. Graph evolution: Densification and shrinking diam-
eters.ACM Transactions on Knowledge Discovery from Data , 1(1), 2007. doi: 10.1145/1217299.1217301.
URL https://doi.org/10.1145/1217299.1217301 .
Dong Li, Zhiming Xu, Sheng Li, and Xin Sun. Link prediction in social networks based on hypergraph. In
Proceedings of the 22nd international conference on world wide web , pp. 41–42, 2013.
Mengran Li, Yong Zhang, Xiaoyong Li, Yuchen Zhang, and Baocai Yin. Hypergraph transformer neural
networks. ACM Transactions on Knowledge Discovery from Data , 17(5):1–22, 2023a.
Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance encoding: Design provably more
powerful neural networks for graph representation learning. Advances in Neural Information Processing
Systems, 33:4465–4478, 2020.
Shouheng Li, Dongwoo Kim, and Qing Wang. Local vertex colouring graph neural networks. 2023b.
25Published in Transactions on Machine Learning Research (11/2024)
Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar Bhalerao, and Ser Nam Lim.
Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods. Advances
in Neural Information Processing Systems , 34:20887–20902, 2021.
Francois Lorrain and Harrison C White. Structural equivalence of individuals in social networks. The Journal
of mathematical sociology , 1(1):49–80, 1971.
Linyuan Lü, Matúš Medo, Chi Ho Yeung, Yi-Cheng Zhang, Zi-Ke Zhang, and Tao Zhou. Recommender
systems. Physics reports , 519(1):1–49, 2012.
ClareLyle, MarkvanderWilk, MartaKwiatkowska, YarinGal, andBenjaminBloem-Reddy. Onthebenefits
of invariance in neural networks. arXiv preprint arXiv:2005.00178 , 2020.
HaggaiMaron,HeliBen-Hamu,NadavShamir,andYaronLipman. Invariantandequivariantgraphnetworks.
arXiv preprint arXiv:1812.09902 , 2018.
Rossana Mastrandrea, Julie Fournet, and Alain Barrat. Contact patterns in a high school: A comparison
between data collected using wearable sensors, contact diaries and friendship surveys. PLOS ONE , 10(9):
e0136497, 2015. doi: 10.1371/journal.pone.0136497. URL https://doi.org/10.1371/journal.pone.
0136497.
Pál András Papp and Roger Wattenhofer. A theoretical comparison of graph neural network extensions. In
International Conference on Machine Learning , pp. 17323–17345. PMLR, 2022.
Pál András Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Random dropouts
increase the expressiveness of graph neural networks. Advances in Neural Information Processing Systems ,
34:21997–22009, 2021.
Nicolas Rashevsky. Life, information theory, and topology. The bulletin of mathematical biophysics , 17:
229–235, 1955.
Petar Ristoski and Heiko Paulheim. Rdf2vec: Rdf graph embeddings for data mining. In The Seman-
tic Web–ISWC 2016: 15th International Semantic Web Conference, Kobe, Japan, October 17–21, 2016,
Proceedings, Part I 15 , pp. 498–514. Springer, 2016.
Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional
networks on node classification. arXiv preprint arXiv:1907.10903 , 2019.
Nicolò Ruggeri, Federico Battiston, and Caterina De Bacco. A framework to generate hypergraphs with
community structure. arXiv preprint arXiv:2212.08593 , 22, 2023.
Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random features strengthen graph neural networks.
InProceedings of the 2021 SIAM international conference on data mining (SDM) , pp. 333–341. SIAM,
2021.
Sameer Singh, Amarnag Subramanya, Fernando Pereira, and Andrew McCallum. Wikilinks: A large-scale
cross-document coreference corpus labeled via links to Wikipedia. Technical Report UM-CS-2012-015,
2012.
Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June (Paul) Hsu, and Kuansan Wang. An
overview of microsoft academic service (MAS) and applications. In Proceedings of the 24th International
Conference on World Wide Web . ACM Press, 2015. doi: 10.1145/2740908.2742839. URL https://doi.
org/10.1145/2740908.2742839 .
Balasubramaniam Srinivasan and Bruno Ribeiro. On the equivalence between positional node embeddings
and structural graph representations. arXiv preprint arXiv:1910.00452 , 2019.
Balasubramaniam Srinivasan, Da Zheng, and George Karypis. Learning over families of sets-hypergraph
representation learning for higher order tasks. In Proceedings of the 2021 SIAM International Conference
on Data Mining (SDM) , pp. 756–764. SIAM, 2021.
26Published in Transactions on Machine Learning Research (11/2024)
Juliette Stehlé, Nicolas Voirin, Alain Barrat, Ciro Cattuto, Lorenzo Isella, Jean-François Pinton, Marco
Quaggiotto, Wouter Van den Broeck, Corinne Régis, Bruno Lina, et al. High-resolution measurements of
face-to-face contact patterns in a primary school. PloS one , 6(8):e23176, 2011.
Mengying Sun, Jing Xing, Huijun Wang, Bin Chen, and Jiayu Zhou. Mocl: data-driven molecular fingerprint
via knowledge-aware contrastive learning from molecular graph. In Proceedings of the 27th ACM SIGKDD
conference on knowledge discovery & data mining , pp. 3585–3594, 2021.
QiaoyuTan, XinZhang,NinghaoLiu,DaochenZha, LiLi, RuiChen, Soo-HyunChoi,andXiaHu. Bringyour
own view: Graph neural networks for link prediction with personalized subgraph selection. In Proceedings
of the Sixteenth ACM International Conference on Web Search and Data Mining , pp. 625–633, 2023.
Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio.
Graph attention networks. arXiv preprint arXiv:1710.10903 , 2017.
Changlin Wan, Muhan Zhang, Wei Hao, Sha Cao, Pan Li, and Chi Zhang. Principled hyperedge prediction
with structural spectral features and neural networks. arXiv preprint arXiv:2106.04292 , 2021.
Haorui Wang, Haoteng Yin, Muhan Zhang, and Pan Li. Equivariant and stable positional encoding for more
powerful graph neural networks. arXiv preprint arXiv:2203.00199 , 2022.
Xiyuan Wang, Pan Li, and Muhan Zhang. Improving graph neural networks on multi-node tasks with
labeling tricks. arXiv preprint arXiv:2304.10074 , 2023.
Tianxin Wei, Yuning You, Tianlong Chen, Yang Shen, Jingrui He, and Zhangyang Wang. Augmentations
in hypergraph contrastive learning: Fabricated and generative. arXiv preprint arXiv:2210.03801 , 2022.
Boris Weisfeiler and Andrei Leman. The reduction of a graph to canonical form and the algebra which
appears therein. nti, Series , 2(9):12–16, 1968.
Jianfeng Wen, Jianxin Li, Yongyi Mao, Shini Chen, and Richong Zhang. On the representation and embed-
ding of knowledge bases beyond binary relations. arXiv preprint arXiv:1604.08642 , 2016.
Asiri Wijesinghe and Qing Wang. A new perspective on" how graph neural networks go beyond weisfeiler-
lehman?". In International Conference on Learning Representations , 2021.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive
survey on graph neural networks. IEEE transactions on neural networks and learning systems , 32(1):4–24,
2020.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks?
arXiv preprint arXiv:1810.00826 , 2018.
Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand Louis, and Partha Talukdar.
Hypergcn: A new method for training graph convolutional networks on hypergraphs. Advances in neural
information processing systems , 32, 2019.
Huaxiu Yao, Caroline Choi, Bochuan Cao, Yoonho Lee, Pang Wei W Koh, and Chelsea Finn. Wild-time: A
benchmark of in-the-wild distribution shift over time. Advances in Neural Information Processing Systems ,
35:10309–10324, 2022a.
Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, and Chelsea Finn. Improving out-
of-distribution robustness via selective augmentation. In International Conference on Machine Learning ,
pp. 25407–25437. PMLR, 2022b.
Hao Yin, Austin R. Benson, Jure Leskovec, and David F. Gleich. Local higher-order graph clustering.
InProceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining. ACM Press, 2017. doi: 10.1145/3097983.3098069. URL https://doi.org/10.1145/3097983.
3098069.
27Published in Transactions on Machine Learning Research (11/2024)
Jiaxuan You, Jonathan M Gomes-Selman, Rex Ying, and Jure Leskovec. Identity-aware graph neural net-
works. In Proceedings of the AAAI conference on artificial intelligence , volume 35, pp. 10737–10745,
2021.
Dingyi Zeng, Wanlong Liu, Wenyu Chen, Li Zhou, Malu Zhang, and Hong Qu. Substructure aware graph
neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 37, pp. 11129–
11137, 2023.
Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of gnns via graph
biconnectivity. arXiv preprint arXiv:2301.09505 , 2023.
Muhan Zhang and Yixin Chen. Weisfeiler-lehman neural machine for link prediction. In Proceedings of the
23rd ACM SIGKDD international conference on knowledge discovery and data mining , pp. 575–583, 2017.
Muhan Zhang and Pan Li. Nested graph neural networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S.
Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems , volume 34,
pp. 15734–15747. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/
paper/2021/file/8462a7c229aea03dde69da754c3bbcc4-Paper.pdf .
Muhan Zhang, Zhicheng Cui, Shali Jiang, and Yixin Chen. Beyond link prediction: Predicting hyperlinks
in adjacency space. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 32, 2018.
MuhanZhang, PanLi, YinglongXia, KaiWang, andLongJin. Labelingtrick: Atheoryofusinggraphneural
networks for multi-node representation learning. Advances in Neural Information Processing Systems , 34:
9061–9073, 2021.
Ruochi Zhang, Yuesong Zou, and Jian Ma. Hyper-sagnn: a self-attention based graph neural network for
hypergraphs. arXiv preprint arXiv:1911.02613 , 2019.
Simon Zhang, Soham Mukherjee, and Tamal K Dey. GEFL: Extended filtration learning for graph classifi-
cation. In Learning on Graphs Conference , pp. 16–1. PMLR, 2022.
Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. Data augmentation
for graph neural networks. In Proceedings of the aaai conference on artificial intelligence , volume 35, pp.
11015–11023, 2021.
Tong Zhao, Wei Jin, Yozen Liu, Yingheng Wang, Gang Liu, Stephan Günnemann, Neil Shah, and Meng
Jiang. Graph data augmentation for graph machine learning: A survey. arXiv preprint arXiv:2202.08871 ,
2022.
Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng
Li, and Maosong Sun. Graph neural networks: A review of methods and applications. AI open, 1:57–81,
2020.
28Published in Transactions on Machine Learning Research (11/2024)
Appendix
A More Background
We discuss in this section about the basics of graph representation learning and link prediction. Graphs
are hypergraphs with all hyperedges of size 2. Simplicial complexes and hypergraphs are generalizations of
graphs. We also discuss more related work.
A.1 Graph Neural Networks and Weisfeiler-Lehman 1
The Weisfeiler-Lehman (WL-1) algorithm is an isomorphism testing approximation algorithm. It involves
repeatedly message passing all nodes with their neighbors, a step called node label refinement. The WL-1
algorithm never gives false negatives when predicting whether two graphs are isomorphic. In other words,
two isomorphic graphs are always indistinguishable by WL-1.
The WL-1 algorithm is the following successive vertex relabeling applied until convergence on a graph
G= (X,A)(a pair of the set of node attributes and the graph’s adjacency structure):
h0
v←Xv,∀v∈VG
hi+1
v←{{(hi
v,hi
u)}}u∈NbrA(v),∀v∈VG(36)
The algorithm terminates after the vertex labels converge. For graph isomorphism testing, the concatenation
of the histograms of vertex labels for each iteration is output as the graph representation. Since we are only
concerned with node isomorphism classes, we ignore this step and just consider the node labels hi
vfor every
v∈VC.
The WL-1 isomorphism test can be characterized in terms of rooted tree isomorphisms between the universal
covers for connected graphs Krebs & Verbitsky (2015). There have also been characterizations of WL-1 in
terms of counting homomorphisms Knill (2013) as well as the Wasserstein Distance Chen et al. (2022) and
Markov chains Chen et al. (2023).
A graph neural network (GNN) is a message passing based node representation learner modeled after the
WL-1 algorithm. It has the important inductive bias of being equivariant to node indices. As a neural
model of the WL-1 algorithm, it learns neural weights common across all nodes in order to obtain a vector
representation for each node. A GNN must use some initial node attributes in order to update its neural
weights. There are many variations on GNNs, including those that improve the distinguishing power beyond
WL-1. For two surveys on the GNNs and their applications, see Zhou et al. (2020); Wu et al. (2020).
A.2 Link Prediction
The task of link prediction on graphs involves the prediction of the existence of links. There are two kinds of
link prediction. There is transductive link prediction where the same nodes are used for all of train validation
and testing. There is also inductive link prediction where the test validation and training nodes can all be
disjoint. Some existing works on link prediction include Zhang & Chen (2017). Higher order link prediction
is a generalization of link prediction to hypergraph data.
A common way to do link prediction is to compute a node-based GNN and for a pair of nodes, aggregate,
similar to in graph auto encoders Kipf & Welling (2016b), the node representations in any target pair in
order to obtain a 2-node representation. Such aggregations are of the form:
h(S={u,v}) =σ(hu·hv) (37)
29Published in Transactions on Machine Learning Research (11/2024)
whereSis a pair of nodes. As shown in Proposition B.4, this guaranteems an equivariant 2-node represen-
tation but can often give false predictions even with a fully expressive node-based GNN Wang et al. (2023).
A common remedy for this problem is to introduce positional encodings such as SEAL Wang et al. (2022)
and DistanceEncoding Li et al. (2020). Positional encodings encode the relative distances amongst nodes
via a low distortion embedding for example. In the related work section we have gone over many of these
embeddings. We have also used these in our evaluation since they are common practice and must exist to
compute a hypergraph neural network if there are no ground truth node attributes. According to Srinivasan
& Ribeiro (2019), fully expressive pairwise node representations, as defined by 2-node invariance and expres-
sivity, can be represented by some fully expressive positional embedding, which is a positional embedding
that is injective on the node pair isomorphism classes. It is not clear how one would achieve this in practice,
however. Another remedy is to increase the expressive power of WL-1 to WL-2 for link prediction Hu et al.
(2022).
A.3 More Related Work
The work of Wei et al. (2022) also does a data augmentation scheme. It considers randomly dropping edges
and generating data through a generative model on hypergraphs. The work of Lee & Shin (2022) also
performs data augmentation on a hypergraph so that homophilic relationships are maintained. It does this
through contrastive losses at the node to node, hyperedge to hyperedge and intra hyperedge level. Neither
of these methods provide guarantees for their data augmentations.
Asmentionedinthemaintext, anensembleofneuralnetworkscanbeusedwithadrop-outBaldi&Sadowski
(2014) like method on the output of the Algorithm. Subgraph neural networks Alsentzer et al. (2020); Tan
et al. (2023) are ensembles of models on subgraphs of the input graph.
Some more of the many existing hypergraph neural network architectures include: Kim et al. (2022); Cai
et al. (2022); Chien et al. (2021); Bai et al. (2021); Li et al. (2023a); Arya et al. (2020).
30Published in Transactions on Machine Learning Research (11/2024)
B Proofs
In this section we provide the proofs for all of the results in the main paper along with some additional
theory.
B.1 Hypergraph Isomorphism
We first repeat the definition of a hypergraph and its corresponding matrix representation called the star
expansion matrix::
Definition B.1. An undirected hypergraph is a pair H= (V,E)consisting of a set of vertices Vand a set
of hyperedgesE⊆2Vwhere 2Vis the power set of the vertex set V.
Definition B.2. The star expansion incidence matrix Hof a hypergraphH= (V,E)is the|V|× 2|V|0-1
incidence matrix HwhereHv,e= 1iffv∈efor(v,e)∈V×E for some fixed orderings on both Vand2V.
We recall the definition of an isomorphism between hypergraphs:
Definition B.3. For two hypergraphs HandD, a structure preserving map ρ:H → D is a pair of
mapsρ= (ρV:VH→ VD,ρE:EH→ ED)such that∀e∈ EH,ρE(e)≜{ρV(vi)|vi∈e} ∈ ED. A
hypergraph isomorphism is a structure preserving map ρ= (ρV,ρE)such that both ρVandρEare bijective.
Two hypergraphs are said to be isomorphic, denoted as H∼=D, if there exists an isomorphism between them.
WhenH=D, an isomorphism ρis called an automorphism on H. All the automorphisms form a group,
which we denote as Aut(H).
The action of π∈Sym(V)on the star expansion adjacency matrix His repeated here for convenience:
(π·H)v,e=(u1,...,v,...,uk)≜Hπ−1(v),π−1(e)=(π−1(u1),...,π−1(v),...,π−1(uk)) (38)
Based on the group action, consider the stabilizer subgroup of Sym(V)on the star expansion adjacency
matrixHdefined as follows:
StabSym (V)(H) ={π∈Sym(V)|π·H=H} (39)
For simplicity we omit the lower index when the permutation group is clear from the context. It can be
checked that Stab(H)≤Sym(V)is a subgroup. Intuitively, Stab(H)consists of all permuations that leave
Hfixed.
For a given hypergraph H= (V,E), there is a relationship between the group of hypergraph automorphisms
Aut(H)and the stabilizer group Stab(H)on the star expansion adjacency matrix.
Proposition B.1. Aut(H)∼=Stab(H)are equivalent as isomorphic groups.
Proof.Considerρ∈Aut(H), define the map Φ :ρ∝⇕⊣√∫⊔≀→π:=ρ|V(H). The group element π∈Sym(V)acts as a
stabilizer of Hsince for any entry (v,e)inH,Hπ−1(v),π−1(e)= (π·H)v,e= 1iffπ−1(e)∈EHiffe∈EHiff
Hv,e= 1 =Hπ◦π−1(v),π◦π−1(e). Since (v,e)was arbitrary, πpreserves the positions of the nonzeros.
We can check that Φis a well defined injective homorphism as a restriction map. Furthermore it is surjective
since for any π∈Stab(H), we must have Hv,e= 1iff(π·H)v,e=Hπ−1(v),π−1(e)= 1which is equivalent to
v∈e∈Eiffπ(v)∈π(e)∈Ewhich implies e∈Eiffπ(e)∈E. Thus Φis a group isomorphism from Aut(H)
toStab(H)
In other words, to study the symmetries of a given hypergraph H, we can equivalently study the auto-
morphisms Aut(H)and the stabilizer permutations Stab(H)on its star expansion adjacency matrix H.
Intuitively, the stabilizer group 0≤Stab(H)≤Sym(V)characterizes the symmetries in a graph. When
the graph has rich symmetries, say a complete graph, Stab(H) =Sym(V)can be as large as the whole
permutaion group.
Nontrivial symmetries can be represented by isomorphic node sets which we define as follow:
31Published in Transactions on Machine Learning Research (11/2024)
Definition B.4. For a given hypergraph Hwith star expansion matrix H, twok-node setsS,T⊆Vare
calledisomorphic , denoted as S≃T, if∃π∈Stab(H),π(S) =Tandπ(T) =S.
Whenk= 1, we have isomorphic nodes, denoted u∼=Hvforu,v∈V. Node isomorphism is also studied as
the so-called structural equivalence in Lorrain & White (1971). Furthermore, when S≃Twe can then say
that there is a matching due to the graph of the πmap of the form {(s,π(s)) :s∈S}. This matching is
between the node sets SandTso that matched nodes are isomorphic.
Definition B.5. Ak-node representation hisk-permutation equivariant if:
for allπ∈Sym(V),S∈2Vwith|S|=k:h(π·S,H) =h(S,π·H)
Proposition B.2. Ifk-node representation hisk-permutation equivariant, then hisk-node invariant.
Proof.givenS,S′∈Cwith|S|=|S′|=k,
if there exists a π∈Stab(H)(meaningπ·H=H) andπ(S) =S′then
h(S′,H) =h(S′,π·H)(byπ·H=H)
=h(S,H)(byk-permutation equivariance of handπ(S) =S′)(40)
We revisit the definition of the symmetry group of a k-node representation map on hypergraph H.
Definition B.6. Forh: [V]k×Zn×2n
2→Rdak-node representation on a hypergraph H,
Sym(h)≜{π∈Sym(V) :π(S) =S′⇒h(S,H) =h(S′,H)} (41)
32Published in Transactions on Machine Learning Research (11/2024)
B.2 Properties of GWL-1
Here are the steps of the GWL-1 algorithm on the star expansion matrix His repeated here for convenience:
f0
e←{},h0
v←{}
fi+1
e←{{(fi
e,hi
v)}}v∈e,∀e∈E(H)
hi+1
v←{{(hi
v,fi+1
e)}}v∈e,∀v∈V(H)(42)
WhereE(H)denotes the nonzero columns of HandV(H)denotes the rows of H.
We make the following observations about each of the two steps of the GWL-1 algorithm:
Observation 2.
{{(fi
e,hi
v)}}v∈e={{(f′i
e,h′i
v)}}v∈eiff(fi
e,{{hi
v}}v∈e) = (f′i
e,{{h′i
v}}v∈e)∀e∈E(H)and (43a)
{{(hi
v,fi+1
e}}v∈e={{(h′i
v,f′i+1
e}}v∈eiff(hi
v,{{fi+1
e}}v∈e) = (h′i
v,{{f′i+1
e}}v∈e)∀v∈V(H)(43b)
Proof.Equation 43a follows since
{{(fi
e,hi
v)}}v∈e={{(f′i
e,h′i
v)}}v∈e∀e∈E(H) (44a)
ifffi
e=f′i
eand{{hi
v}}v∈e={{h′i
v}}v∈e∀e∈E(H) (44b)
iff(fi
e,{{hi
v}}v∈e) = (f′i
e,{{h′i
v}}v∈e)∀e∈E(H) (44c)
For Equation 43b, we have:
{{(hi
v,fi+1
e}}v∈e={{(h′i
v,f′i+1
e}}v∈e∀v∈V(H) (45a)
iff{{(hi
v,{{(fi
e,hi
u)}}u∈e)}}v∈e={{(h′i
v,{{(f′i
e,h′i
u)}}u∈e)}}v∈e∀v∈V(H) (45b)
iffhi
v=h′i
vand{{(fi
e,hi
u)}}u∈e,v∈e={{(f′i
e,h′i
u)}}u∈e,v∈e∀v∈V(H) (45c)
iffhi
v=h′i
vand{{fi+1
e}}={{f′i+1
e}}∀v∈V(H) (45d)
These follow by the definition of multiset equality and since there is no loss of information upon factoring
out a constant tuple entry of each pair in the multisets.
Proposition B.3. The update steps of GWL-1: fi(H)≜[fi
e1(H),···,fi
em(H)]andhi(H)≜
[hi
v1(H),···,hi
vn(H)], are permutation equivariant; in other words, For any π∈Sym(V), letπ·fi(H)≜
[fi
π−1(e1)(H),···,fi
π−1(em)(H)]andπ·hi(H)≜[hi
π−1(v1)(H),···,hi
π−1(vn)(H)], we have∀i∈N,π·fi(H) =
fi(π·H)andπ·hi(H) =hi(π·H)
Proof.We prove by induction on i:
Base case,i= 0:
[π·f0(H)]e={v1,...,vk}={}=f0
π−1(e)={π−1(v1),...,π−1(vk)}(H) =f0
e(π·H)since theπcannot affect a list of
empty sets and the definition of the action of πonHas defined in Equation 38.
[π·h0(H)]v= [π·X]v=Xπ−1(v)=h0
π−1(v)(H) =h0
v(π·H)by definition of the group action Sym(V)acting
on the node indices of a node attribute tensor as defined in Equation 38.
Induction Hypothesis:
[π·fi(H)]e=fi
π−1(e)(H) =fi
e(π·H)and[π·hi(H)]v=hi
π−1(v)(H) =hi
v(π·H) (46)
Induction Step:
33Published in Transactions on Machine Learning Research (11/2024)
[π·hi+1(H)]v={{([π·hi(H)]v,[π·fi+1(H)]e)}}v∈e
={{([π·hi(H)]v,{{([π·fi(H)]e,[π·hi(H)]u)}}u∈e)}}v∈e
={{hi
v(π·H),{{(fi
e(π·H),hi
u(π·H))}}u∈e}}v∈e
=hi+1
v(π·H)(47)
[π·fi+1(H)]e={{([π·fi(H)]e,[π·hi(H)]v)}}v∈e
={{(fi
e(π·H),hi
v(π·H)}}v∈e
=fi+1
e(π·H)(48)
Definition B.7. Leth: [V]k×Zn×2n
2→Rdbe ak-node representation on a hypergraph H. LetH∈Zn×2n
2
be the star expansion adjacency matrix of Hfornnodes. The representation hisk-node most expressive if
∀S,S′⊆V,|S|=|S′|=k, the following two conditions are satisfied:
1.hisk-node invariant :∃π∈Stab(H),π(S) =S′=⇒h(S,H) =h(S′,H)
2.hisk-node expressive ∄π∈Stab(H),π(S) =S′=⇒h(S,H)̸=h(S′,H)
LetAGGbe a permutation invariant map from a set of node representations to Rd.
Proposition B.4. Leth(S,H) =AGGv∈S[hi
v(H)]with injective AGG and hi
vpermutation equivariant. The
representation h(S,H)isk-node invariant but not necessarily k-node expressive for Sa set ofknodes.
Proof.∃π∈Stab(H)s.t.π(S) =S′,π·H=H
⇒π(vi) =v′
ifori= 1,...,|S|,π·H=H
⇒hi
π(v)(H) =hi
v(π·H) =hi
v(H)(By permutation equivariance of hi
vandπ·H=H)
⇒AGGv∈S[hi
v(H)] =AGGv′∈S′[hi
v′(H)](By Proposition B.2 and AGG being permutation invariant)
The converse, that h(S,H)isk-node expressive, is not necessarily true since we cannot guarantee h(S,H) =
h(S′,H)implies the existence of a permutation that maps StoS′(see Zhang et al. (2021)).
A hypergraph can be represented by a bipartite graph BV,EfromVtoEwhere there is an edge (v,e)in the
bipartite graph iff node vis incident to hyperedge e. This bipartite graph BV,Eis called the star expansion
bipartite graph.
We introduce a more structured version of graph isomorphism called a 2-color isomorphism to characterize
hypergraphs. It is a map on 2-colored graphs, which are graphs that can be colored with two colors so that
no two nodes in any graph with the same color are connected by an edge. We define a 2-colored isomorphism
formally here:
Definition B.8. A2-colored isomorphism is a graph isomorphism on two 2-colored graphs that preserves
node colors. In particular, between two graphs G1andG2the vertices of one color in G1must map to
vertices of the same color in G2. It is denoted by ∼=c.
A bipartite graph must always have a 2-coloring. In fact, the 2-coloring with all the nodes in the node
bipartition colored red and all the nodes in the hyperedge bipartition colored blue forms a canonical 2-
coloring ofBV,E. Assume that all star expansion bipartite graphs are canonically 2-colored.
Proposition B.5. We have two hypergraphs (V1,E1)∼=(V2,E2)iffBV1,E1∼=cBV2,E2whereBV,Eis the star
expansion bipartite graph of (V,E)
34Published in Transactions on Machine Learning Research (11/2024)
Proof.DenoteL(BVi,Ei)as the left hand (red) bipartition of BVi,Eito represent the nodes Viof(Vi,Ei)and
R(BVi,Ei)as the right hand (blue) bipartition of BVi,Eito represent the hyperedges Eiof(Vi,Ei). We use the
left/right bipartition and Vi/Eiinterchangeably since they are in bijection.
⇒If there is an isomorphism π:V1→V 2, this means
•πis a bijection and
•has the structure preserving property that (u1,...,uk)∈E1iff(π(u1),...,π (uk))∈E2.
We may induce a 2-colored isomorphism π∗:V(BV1,E1)→V(BV1,E1)so thatπ∗|L(BV1,E1)=πwhere equality
here means that π∗|L(BV1,E1)acts onL(BV1,E1)the same way that πdoes onV1. Furthermore π∗has the
property that π∗|R(BV1,E1)(u1,...,uk) = (π(u1),...,π (uk)),∀(u1,...,uk)∈E1, following the structure preserv-
ing property of isomorphism π.
The mapπ∗is a bijection by definition of being an extension of a bijection.
The mapπ∗is also a 2-colored map since it maps L(BV1,E1)toL(BV2,E2)andR(BV1,E1)toR(BV2,E2).
We can also check that the map is structure preserving and thus a 2-colored isomorphism since
(ui,(u1,...,ui,...,uk))∈ E(BV1,E1),∀i= 1,...,kiff (ui∈ V 1and (u1,...,ui,...,uk)∈ E 1) iffπ(ui)∈ V 2
and(π(u1),...,π (ui),...,π (uk))∈E2iff(π∗(ui),(π∗(u1,...,ui,...,uk))∈E(BV2,E2),∀i= 1,...,k. This follows
fromπbeing structure preserving and the definition of π∗.
⇐If there is a 2-colored isomorphism π∗:BV1,E1→BV2,E2then it has the properties that
•π∗is a bijection,
•(is2-colored):π∗|L(BV1,E1) :L(BV1,E1)→L(BV2,E2)andπ∗|R(BV1,E1):R(BV1,E1)→R(BV2,E2)
•(it is structure preserving): (ui,(u1,...,ui,...,uk))∈ E (BV1,E1),∀i= 1,...,kiff
(π∗(ui),π∗(u1,...,ui,...,uk))∈E(BV2,E2),∀i= 1,...,k.
This then means that we may induce a π:V1→V 2so thatπ=π∗|L(BV1,E1).
We can check that πis a bijection since πis the 2-colored bijection π∗restricted to L(BV1,E1), thus remaining
a bijection.
We can also check that πis structure preserving. This means that (u1,...,uk)∈E1iff(ui,(u1,...,ui,...,uk))∈
E(BV1,E1)∀i= 1,...,kiff(π∗(ui),(π∗(u1,...,ui,...,uk)))∈E(BV2,E2)∀i= 1,...,kiff(π∗(u1,...,uk))∈R(BV2,E2)
iff(π(u1),...,π (uk))∈E2
We define a topological object for a graph originally from algebraic topology called a universal cover:
Definition B.9. (Hatcher (2005)) A universal covering of a connected graph Gis a (potentially infinite)
graph ˜G, s.t. there is a map pG:˜G→Gcalled the universal covering map where:
1.∀x∈V(˜G),pG|N(x)is an isomorphism onto N(pG(x)).
2.˜Gis simply connected (a tree)
A covering graph is a graph that satisfies property 1 but not necessarily property 2 in Definition B.9. It is
known that a universal covering ˜Gcovers all the graph covers of the graph G. LetTr
xdenote a tree with root
xwhere every node has depth r. Furthermore, define a rooted isomorphism Gx∼=Hyas an isomorphism
between graphs GandHthat maps xtoyand vice versa. We will use the following result to prove a
characterization of GWL-1:
Lemma B.6 (Krebs & Verbitsky (2015)) .LetTandSbe trees and x∈V(T)andy∈V(S)be their vertices
of the same degree with neighborhoods N(x) ={x1,...,xk}andN(y) ={y1,...,yk}. Letr≥1. Suppose that
Tr−1
x∼=Sr−1
yandTr
xi∼=Sr
yifor alli≤k. ThenTr+1
x∼=Sr+1
y.
35Published in Transactions on Machine Learning Research (11/2024)
A universal cover of a 2-colored bipartite graph is still 2colored. When we lift nodes vand hyperedge nodes
eto their universal cover, we keep their respective red and blue colors.
Define a rooted colored isomorphism Tk
˜e1∼=cTk
˜e2as a colored tree isomorphism where blue/red node ˜e1/˜v1
maps to blue/red node ˜e2/˜v2and vice versa.
In fact, Lemma B.6 holds for 2-colored isomorphisms, which we show below:
Lemma B.7. LetTandSbe2-colored trees and x∈V(T)andy∈V(S)be their vertices of the same degree
with neighborhoods N(x) ={x1,...,xk}andN(y) ={y1,...,yk}. Letr≥1. Suppose that Tr−1
x∼=cSr−1
yand
Tr
xi∼=cSr
yifor alli≤k. ThenTr+1
x∼=cSr+1
y.
Proof.Certainly 2-colored isomorphisms are rooted isomorphisms on 2-colored trees. The converse is true
if the roots match in color since recursively all descendants of the root must match in color.
IfTr−1
x∼=cSr−1
yandTr
xi∼=cSr
yifor alli≤kandN(x) ={x1,...,xk},N(y) ={y1..yk}, the roots xandy
must match in color. The neighborhoods N(x)andN(y)then must both be of the opposing color. Since
rooted colored isomorphisms are rooted isomorphisms, we must have Tr−1
x∼=Sr−1
yandTr
xi∼=Sr
yifor all
i≤k. By Lemma B.6, we have Tr+1
x∼=Sr+1
y. Once the roots match in color, a rooted tree isomorphism is the
same as a rooted 2-colored tree isomorphism. Thus, since xandyshare the same color, Tr+1
x∼=cSr+1
y
Theorem B.8. LetH1= (V1,E1)andH2= (V2,E2)be two connected hypergraphs. Let BV1,E1andBV2,E2
be two canonically colored bipartite graphs for H1andH2(vertices colored red and hyperedges colored blue)
For anyi∈Z+, for any of the nodes x1∈BV1,e1∈BV1,E1andx2∈BV1,e2∈BV2,E2:
•(˜B2i−1
V1,E1)˜e1∼=c(˜B2i−1
V2,E2)˜e2ifffi
e1=fi
e2
•(˜B2i
V1,E1)˜x1∼=c(˜B2i
V2,E2)˜x2iffhi
x1=hi
x2,
withfi
•,hi
•theith GWL-1 values for the hyperedges and nodes respectively where e1=pBV1,E1(˜e1),x1=
pBV1,E1(˜x1),e2=pBV1,E1(˜e2),x2=pBV1,E1(˜x2). The maps pBV1,E1:˜BV1,E1→BV1,E1,pBV2,E2:˜BV2,E2→BV2,E2
are the universal covering maps of BV1,E1andBV2,E2respectively.
Proof.We prove by induction:
LetTk
˜e1:= ( ˜Bk
V1,E1)˜e1where ˜e1is a pullback of a hyperedge, meaning pBV1,E2(˜e1) =e1. Similarly, let
Tk
˜e2:= ( ˜Bk
V2,E2)˜e2,Tk
˜x1:= ( ˜Bk
V1,E1)˜x1,Tk
˜x2:= ( ˜Bk
V2,E2)˜x2,∀k∈N, where ˜e1,˜e2,˜x1,˜x2are the respective
pullbacks of e1,e2,x1,x2.
Define an ( 2-colored) isomorphism of multisets of graphs to mean that there exists a bijection between the
two multisets so that each graph in one multiset is ( 2-colored) isomorphic with exactly one other element in
the other multiset.
By Observation 2 we can rewrite GWL-1 as:
f0
e←{},h0
v←{} (49)
fi+1
e←(fi
e,{{hi
v}}v∈e)∀e∈EH (50)
hi+1
v←(hi
v,{{fi+1
e}}v∈e)∀v∈VH (51)
Base Casei= 1:
T1
˜e1∼=cT1
˜e2iff(T0
˜e1∼=cT0
˜e2and{{T0
˜x1}}˜x1∈N(˜e1)∼=c{{T0
˜x2}}˜x2∈N(˜e2))(By Lemma B.7) (52a)
iff(f0
e1=f0
e2and{{h0
x1}}={{h0
x2}})(By Equation 49) (52b)
ifff1
e1=f1
e2(By Equation 50) (52c)
36Published in Transactions on Machine Learning Research (11/2024)
T2
˜x1∼=cT2
˜x2iff(T0
˜x1∼=cT0
˜x2and{{T1
˜e1}}˜e1∈N(˜x1)∼=c{{T1
˜e2}}˜e2∈N(˜x2))(By Lemma B.7) (53a)
iff(h0
e1=h0
e2and{{f1
x1}}={{f1
x2}})(By Equation 49) (53b)
ifff1
e1=f1
e2(By Equation 51) (53c)
Induction Hypothesis: For i≥1,T2i−1
˜e1∼=cT2i−1
˜e2ifffi
e1=fi
e2andT2i
˜x1∼=cT2i
˜x2iffhi
x1=hi
x2
Induction Step:
T2i+1
˜e1∼=cT2i+1
˜e2iff(T2i−1
˜e1∼=cT2i−1
˜e2and{{T2i
˜x1}}˜x1∈N(˜e1)∼=c{{T2i
˜x2}}˜x2∈N(˜e2))(By Lemma B.7) (54a)
iff(fi
e1=fi
e2and{{hi
x1}}={{hi
x2}})(By Induction Hypothesis) (54b)
ifffi+1
e1=fi+1
e2(By Equation 50) (54c)
T2i
˜x1∼=cT2i
˜x2iff(T2i−2
˜x1∼=cT2i−2
˜x2and{{T2i−1
˜e1}}˜e1∈N(˜x1)∼=c{{T2i−1
˜e2}}˜e2∈N(˜x2))(By Lemma B.7) (55a)
iff(hi
e1=hi
e2and{{fi
x1}}={{fi
x2}})(By Equation 49) (55b)
iffhi
x1=hi
x2(By Equation 51) (55c)
We write here the theorem characterizing the WL-1 algorithm on a graph by the graph’s universal cover.
Theorem B.9. [Krebs & Verbitsky (2015)] Let GandHbe two connected graphs. Let pG:˜G→G,pH:
˜H→Hbe the universal covering maps of GandHrespectively. For any i∈N, for any two nodes x∈G
andy∈H:˜Gi
˜x∼=˜Gi
˜yiff the WL-1 algorithm assigns the same value to nodes x=pG(˜x)andy=pH(˜y).
Itfollows immediately fromTheoremB.9that theWL-1algorithmon coloredstarexpansionbipartitegraphs
of two hypergraphs corresponds to constructing their universal covers.
Corollary 2. LetH1= (V1,E1)andH2= (V2,E2)be two connected hypergraphs. Let BV1,E1andBV2,E2be
two canonically colored bipartite graphs for H1andH2(vertices colored red and hyperedges colored blue).
LetpBV1,E1:˜BV1,E1→BV1,E1,pBV2,E2:˜BV2,E2→BV2,E2be the universal coverings of BV1,E1andBV2,E2
respectively. For any i∈Z+,
•(˜B2i−1
V1,E1)˜v1∼=c(˜B2i−1
V2,E2)˜v2iffgi
v1=gi
v2
withgi
v1,gi
v2thei-th WL-1 values for v1,v2∈ V(BV1,E1),V(BV2,E2)respectively where v1=pBV1,E1(˜v1),
v2=pBV2,E2(˜v2).
Furthermore, for i≥2,
•g2+i
u1=g2+i
u2iffh1+i
u1=h1+i
u2,∀u1∈L(BV1,E1),∀u2∈L(BV2,E2)and
•g2+i
e1=g2+i
e2ifff2+i
e1=h2+i
e2,∀e1∈R(BV1,E1),∀e2∈R(BV2,E2)
Proof.The first equivalence (˜B2i−1
V1,E1)˜v1∼=c(˜B2i−1
V2,E2)˜v2iffgi
v1=gi
v2follows directly by Theorem 2.
The successive two equivalences follow by Theorem B.8 and the first equivalence.
Observation 3. If the node values for nodes xandyfrom GWL-1 for iiterations on two hypergraphs H1
andH2are the same, then for all jwith 0≤j≤i, the node values for GWL-1 for jiterations on xandy
also agree. In particular deg(x) = deg(y).
37Published in Transactions on Machine Learning Research (11/2024)
Proof.There is a 2-color isomorphism on subtrees (˜Bj
V1,E1)˜xand (˜Bj
V2,E2)˜yof thei-hop subtrees of the
universal covers rooted about nodes x∈V 1andy∈V 2for0≤j≤isince (˜Bi
V1,E1)˜x∼=c(˜Bi
V2,E2)˜y. By
Theorem B.8, we have that GWL-1 returns the same value for xandyfor each 0≤j≤i.
Theorem B.10. LethL: [V]1×Zn×2n
2→Rdbe theL-GWL-1 representation of nodes for hypergraph Hin
Equation 7, then
Aut(H)∼=Stab(H)⊆Sym(hL(H))∼=Autc(˜B2L
V,E),∀L≥1 (56)
Proof.1.Aut(H)∼=Stab(H)follows by Proposition B.1.
2.Stab(H)⊆Sym(hL(H))follows by definition of the symmetry group of a representation map given in
Definition B.6 and the equivariance of L-GWL-1 due to Proposition B.3. Let Setdenote the collection of
all finite sets. We know that
hL(S,H)≜AGG ({hL
v}v∈S) (57)
forAGG :Set→Rdan injective set representation map and that Shas cardinality 1. For any π∈
Stab(H)⊆Sym(V), we must have π·H=Hwe check that πsatisfies:
π(u) =v⇒h(u,H) =h(v,H),∀u,v∈V (58)
Ifπ(u) =vandπ·H=H, we can use the equivariance of hLto get right hand necessary condition:
hL(u,H) =hL(u,π(H)) =hL(π(u),H) =hL(v,H)
3. The last group isomorphism follows by the equivalence between L-GWL-1 and the universal cover up to
2L-hops given in Theorem B.8.
For anyπ∈Sym(hL(H)), it must satisfy π(u) =v⇒h(u,H) =h(v,H),∀u,v∈V. We map each πto a
2-colored isomorphism Φ :π∝⇕⊣√∫⊔≀→ϕcwhich is the 2-colored isomorphism determined by the Theorem B.8:
(˜B2L
V,E)˜u∼=c(˜B2L
V,E)/tildewidestπ(u))iffhL
u=hL
π(u)=hL(u,H) =hL(π(u),H),∀u∈V (59)
Certainly the map Φ :π∝⇕⊣√∫⊔≀→ϕcis a homomorphism because:
1.Φmaps the identity to identity:
(˜B2L
V,E)˜u∼=c(˜B2L
V,E)˜u)iffhL
u=hL(u,H),∀u∈V (60)
can have only one 2-colored isomorphism determining (˜B2L
V,E)˜u∼=c(˜B2L
V,E)˜u, which is the identity.
2.Φperserves composition: π2◦π1∝⇕⊣√∫⊔≀→(ϕ2)c◦(ϕ1)c
By definition of π1andπ2:
π1(u) =v⇒h(u,H) =h(v,H)iff(˜B2L
V,E)˜u∼=c(˜B2L
V,E)^π1(u),∀u∈V (61a)
π2(v) =w⇒h(v,H) =h(w,H )iff(˜B2L
V,E)˜v∼=c(˜B2L
V,E)^π1(w),∀v∈V (61b)
Combining, we get:
(˜B2L
V,E)˜u∼=c(˜B2L
V,E)^π1(u)∼=c(˜B2L
V,E)^π2(u)∼=(˜B2L
V,E)^π2◦π1(u)(62)
where the first isomorphism is (ϕ1)con(˜B2L
V,E)˜u, the second isomorphism is from (ϕ2)con(˜B2L
V,E)˜uand the
third isomorphism is (ϕ1)c◦(ϕ1)con(˜B2L
V,E)^π1(u)
Proposition B.11. If GWL-1 cannot distinguish two connected hypergraphs H1andH2then HyperPageR-
ank will not either.
Proof.HyperPageRank is defined on a hypergraph with star expansion matrix Has the following stationary
distribution Π:
lim
n→∞(D−1
v·H·D−1
e·HT)n= Π (63)
38Published in Transactions on Machine Learning Research (11/2024)
IfHis a connected bipartite graph, Πmust be the eigenvector of (D−1
v·H·D−1
e·HT)for eigenvalue 1. In
other words, Πmust satisfy
(D−1
v·H·D−1
e·HT)·Π = Π (64)
By Theorem 1 of Huang & Yang (2021), we know that the UniGCN defined by:
hi+1
e←ϕ2(hi
e,hi
v) =We·HT·hi
v (65a)
hi+1
v←ϕ1(hi
v,hi+1
e) =Wv·H·hi+1
e (65b)
for constant WeandWvweight matrices, is equivalent to GWL-1 provided that ϕ1andϕ2are both injective
as functions. Without injectivity, we can only guarantee that if UniGCN distinguishes H1,H2then GWL-1
distinguishes H1,H2. In fact, each matrix power of order nin Equation 63 corresponds to hn
vso long as we
satisfy the following constraints:
We←D−1
e,Wv←D−1
vandh0
v←I (66)
We show that the matrix powers are UniGCN under the constraints of Equation 66 by induction:
Base Case: n= 0:h0
v=I
Induction Hypothesis: n>0:
(D−1
v·H·D−1
e·HT)n=hn
v (67)
Induction Step:
(D−1
v·H·hn
e) (68a)
= (D−1
v·H·((D−1
e·HT)·hn
v)) (68b)
= (D−1
v·H·D−1
e·HT)·(D−1
v·H·D−1
e·HT)n(68c)
= (D−1
v·H·D−1
e·HT)n+1=hn+1
v (68d)
Since we cannot guarantee that the maps ϕ1andϕ2are injective in Equation 68b, it must be that the output
hn
v, coming from UniGCN with the constraints of Equation 66, is at most as powerful as GWL-1.
In general, injectivity preserves more information. For example, if ϕ1is injective and if ϕ′
1is an arbitrary
map (not guaranteed to be injective) then:
ϕ1(h1) =ϕ1(h2)⇒h1=h2⇒ϕ′
1(h1) =ϕ′
1(h2) (69)
HyperpageRank is exactly as powerful as UniGCN under the constraints of Equation 66. Thus HyperPageR-
ank is at most as powerful as GWL-1 in distinguishing power.
39Published in Transactions on Machine Learning Research (11/2024)
Figure 5: An illustration of hypergraph symmetry breaking. (c,d) 3-regular hypergraphs C3
4,C3
5with 4and5
nodes respectively and their corresponding universal covers centered at any hyperedge (˜BC3
4)e∗,∗,∗,(˜BC3
5)e∗,∗,∗
with universal covering maps pBC3
4,pBC3
5. (b,e) the hypergraphs ˆC3
4,ˆC3
5, which are C3
4,C3
5with 4,5-sized
hyperedges attached to them and their corresponding universal covers and universal covering maps. (a,f)
are the corresponding bipartite graphs of ˆC3
4,ˆC3
5. (c,d) are indistinguishable by GWL-1 and thus will give
identical node values by Theorem B.8. On the other hand, (b,e) gives node values which are now sensitive
to the the order of the hypergraphs 4,5, also by Theorem B.8.
B.3 Method
We repeat here from the main text the symmetry finding algorithm:
Algorithm 2: A Symmetry Finding Algorithm
Data:HypergraphH= (V,E), represented by its star expansion matrix H.L∈Z+is the number
of iterations to run GWL-1.
Result: A pair of collections: (RV={VRj},RE=∪j{ERj})whereRjare disconnected
subhypergraphs exhibiting symmetry in Hthat are indistinguishable by L-GWL-1.
1Edeg←{{deg(v) :v∈e}:∀e∈E}
2UL←hL
v(H);GL←{UL[v] :∀v∈V}; /*UL[v]is the L-GWL-1 value of node v∈ V. */
3BVH,EH←Bipartite (H)/* Construct the bipartite graph from H. */
4RV←{};RE←{}
5forcL∈GLdo
6VcL←{v∈V:UL[v] =cL},EcL←{e∈E:u∈VcL,∀u∈e}
7CcL←ConnectedComponents (HcL= (VcL,EcL))
8forRcL,i∈CcLdo
9RV←RV∪{VRcL,i};RE←RE∪ERcL,i
10end
11end
12return (RV,RE)
We also repeat here for convenience some definitions used in the proofs. Given a hypergraph H= (V,E), let
VcL:={v∈V:cL=hL
v(H)} (70)
be the set of nodes of the same class cLas determined by L-GWL-1. LetHcLbe an induced subgraph of H
byVcL.
Definition B.10. AL-GWL-1 symmetric induced subhypergraph R ⊂ H ofHis a connected induced
subhypergraph determined by VR⊆VH, some subset of nodes that are all indistinguishable amongst each
other byL-GWL-1:
hL
u(H) =hL
v(H),∀u,v∈VR (71)
40Published in Transactions on Machine Learning Research (11/2024)
WhenL=∞, we call suchRa GWL-1 symmetric induced subhypergraph. Furthermore, if R=H, then we
sayHisGWL-1 symmetric .
Definition B.11. A neighborhood-regular hypergraph is a hypergraph where all neighborhoods of each node
are isomorphic to each other.
Observation 4. A hypergraphHis GWL-1 symmetric if and only if it is L-GWL-1 symmetric for all L≥1
if and only ifHis neighborhood regular.
Proof.
1. First if and only if :
By Theorem B.8, GWL-1 symmetric hypergraph H= (V,E)means that for every pair of nodes u,v∈
V,(˜BV,E)˜u∼=c(˜BV,E)˜v. This implies that for any L≥1,(˜B2L
V,E)˜u∼=c(˜B2L
V,E)˜vby restricting the rooted
isomorphism to 2L-hop rooted subtrees, which means that hL
u(H) =hL
v(H). The converse is true since Lis
arbitrary. If there are no cycles, we can just take the isomorphism for the largest. Otherwise, an isomorphism
can be constructed for L=∞by infinite extension.
2. Second if and only if :
LetpBV,Ebe the universal covering map for BV,E. Denote ˜v,˜uby the lift of some nodes v,u∈VbypBV,E.
Let(˜N(˜u))˜ube the rooted bipartite lift of (N(u))u. IfHisL-GWL-1 symmetric for all L≥1then with
L= 1,(˜B2
V,E)˜u∼=c(˜N(u))˜u∼=c(˜N(v))˜v∼=c(˜B2
V,E)˜v, iff(N(u))u∼=(N(v))˜v,∀u,v∈VsinceN(u)andN(v)
are cycle-less for any u,v∈V. For the converse, assume all nodes v∈Vhave (N(v))v∼=(N1)xfor some
1-hop rooted tree (N1)xrooted at node x, independent of any v∈V. We prove by induction that for all
L≥1and for allv∈V,(˜B2L
V,E)˜v∼=c(˜N2L)xfor a 2L-hop tree (˜N2L)xrooted at node x.
Base case: L= 1is by assumption.
Inductive step: If (˜B2L
V,E)˜v∼=c(N2L)x, we can form (˜B2L+2
V,E)˜vby attaching (˜N(˜u))˜uto each node ˜uin the
2L-th layer of (˜B2L
V,E)˜v∼=c(˜N2L)x. Each (˜N(u))˜uis independent of the root ˜vsince every u∈ Vhas
(˜N(˜u))˜u∼=c(˜N2)xiff(N(˜u))˜u∼=(N1)xfor anxindependent of u∈V. This means (˜B2L+2
V,E)˜v∼=c(˜N2L+2)x
for the same root node xwhere (˜N2L+2)xis constructed in the same manner as (˜B2L
V,E)˜v,∀v∈V.
Proposition B.12. LetH= (V,˜E)be a multi-hypergraph.
A multi-hypergraph isomorphism, like for hypergraphs, is defined by a structure preserving map (ρV:VH→
VD,ρ˜E:EH→ED)but whereρ˜Eis a bijection between multisets.
The star expansion bipartite graph of the multi-hypergraph H:BV,˜E, is defined as before as the bipartite graph
with verticesV/unionsqtext˜Eand edges{(v,e)∈V× ˜E|v∈e}.
With these definitions on multi-hypergraphs, Proposition B.5, Theorem B.8, and Corollary 2 also hold for
multi-hypergraphs.
Proof.In the proposition, theorem and corollary, replace the set Ewith the multiset ˜Eand the proofs become
identical.
B.3.1 Algorithm Guarantees
Continuing with the notation, as before, let H= (V,E)be a hypergraph with star expansion matrix H
and let (RV,RE)be the output of Algorithm 1 on HforL∈Z+. DenoteCcLas the set of all connected
components ofHcL:
CcL≜{CcL:conn. comp. CcLofHcL} (72)
IfL=∞, then drop the L. Thus, the hypergraphs represented by (RV,RE)come fromCcLfor eachcL.
Let:
ˆHL≜(V,E∪RV) (73)
41Published in Transactions on Machine Learning Research (11/2024)
beHafter adding all the hyperedges from RVand let ˆHLbe the star expansion matrix of the resulting
multi-hypergraph ˆHL. Let:
GL≜{hL
v(H) :v∈V} (74)
be the set of all L-GWL-1 values on H. Let:
VcL,s≜{v∈VcL:v∈R,R∈CcL,|VR|=s} (75)
be the set of all nodes of L-GWL-1 class cLbelonging to a connected component in CcLofs≥1nodes in
HcL, the induced subhypergraph of L-GWL-1. Let:
ScL≜{|VRcL,i|:RcL,i∈CcL} (76)
be the set of node set sizes of the connected components in HcL.
Proposition B.13. IfL=∞, for any GWL-1 node value cforH, the connected component induced
subhypergraphsRc,i, fori= 1,...,|Cc|are GWL-1 symmetric and neighborhood-regular.
Proof.LetpBV,Ebe the universal covering map for BV,E. Denote ˜v,˜u,˜v′,˜u′by the lift of some nodes
v,u,v′,u′∈VbypBV,E.
LetL=∞and letHc= (Vc,Ec). For any i, sinceu,v∈Vc,(˜BV,E)u∼=c(˜BV,E)vfor allu,v∈VRc,i.
SinceRc,iis maximally connected we know that every neighborhood NHc(u)foru∈Vcinduced byHchas
NHc(u)∼=N(u)∩Hc. SinceL=∞we have that NHc(u)∼=NHc(v),∀u,v∈VRc,isince otherwise WLOG
there areu′,v′∈VRc,iwithNHc(u′)̸∼=NHc(v′)then WLOG there is some hyperedge e∈ENHc(u′)with
somew∈e,w̸=u′whereecannot be in isomorphism with any e′∈ENHc(v′). For two hyperedges to be in
isomorphism means that their constituent nodes can be bijectively mapped to each other by a restriction of
an isomorphism ϕbetweenNHc(u′),NHc(v′)to one of the hyperedges. This means that (˜BV\{u′},E)wis the
rooted universal covering subtree centered about wnot passing through u′that is connected to u′∈(˜BV,E)u′
bye. However, v′has noeand thus cannot have a Txforx∈V(˜N(v′))vsatisfyingTx∼=c(˜BV\{u′},E)wwith
xconnected to v′by a hyperedge e′isomorphic to ein its neighborhood in (˜BV,E)v′. This contradicts that
(˜BV,E)u′∼=c(˜BV,E)v′.
We have thus shown that all nodes in Vchave isomorphic induced neighborhoods. By the Observation 4,
this is equivalent to saying that Rc,iis GWL-1 symmetric and neighborhood regular.
We show the partitioning of Hby Algorithm 12:
Proposition B.14. IfL≥1, the output (RV,RE)of Algorithm 1 partitions a subgraph of H, meaning:
V=⊔V∈RVVandE⊃⊔E∈REE (77)
Proof.1.V=⊔V∈RVV:
For a given subset of nodes U⊆V, let ConnectedComponentsH(U)be the collection of node subsets of U
where each node subset forms a connected component in H.
LethL
v(H)denote the L-GWL-1 node value of v∈V.
LetRV(L)≜/uniontext
v∈VConnectedComponentsH({u∈V:hL
u(H) =hL
v(H)})denote the collection of node sets
of common L-GWL-1 values for a given L.
By the definition of RV, since every connected component is size atleast 1and every node is considered, we
must have/uniontext
v∈VRV=V. Since each connected component of a given GWL-1 value is maximal, meaning
there is no superset of nodes that is connected, no two connected components can intersect through either
nodes or hyperedges. Furthermore, a single node can belong to only one GWL-1 value, thus the values form
a partition ofV. This provesV=⊔V∈RVV.
2.E⊃⊔E∈REE:
This follows since REare the hyperedges of each connected component spanned by RV. These connected
components form disconnected subhypergraphs of H.
42Published in Transactions on Machine Learning Research (11/2024)
Prediction Guarantees:
In order to guarantee that the GWL-1 symmetric components Rc,ifound by Algorithm 12 carry additional
information, there needs to be a separation between them to prevent an intersection between the rooted
trees computed by GWL-1. We redefine from the main paper what it means for two node subsets to be
sufficiently separated via the shortest hyperedge path distance between nodes in Vas follows:
Definition B.12. Two subsets of nodes U1,U2⊆Varesufficiently L-separated if:
min
v1∈U1,v2∈U2d(v1,v2)>L (78)
whered(v1,v2)≜mine1,...,ek∈E,v1∈e1,v2∈ekkis the shortest hyperedge path distance from v1∈Vtov2∈V.
A collection of node subsets C⊆2Vissufficiently L-separated if all pairs of node subsets are sufficiently
L-separated .
Our definition of sufficiently L-separated is similar in nature to that of well separation between point sets
Callahan & Kosaraju (1995) in Euclidean space.
We give another definition that will be useful for the proof of the following lemma:
Definition B.13. A star graph Nxis defined as a tree rooted at xof depth 1. The root xis the only node
that can have degree more than 1.
Assuming that the CcLare sufficiently L-separated from each other, intuitively meaning that no two nodes
from two separate VRcL,i∈RVare within Lhyperedges away, then the cardinality of each component
|VRcL,i|is recognizable.
Lemma B.15. IfL∈Z+is small enough so that after running Algorithm 1 on L, for anyL-GWL-1 node
classcLonVthe collection ofCcLissufficiently L-separated ,
then after forming ˆHL, the newL-GWL-1 node classes of VRcL,ifori= 1,...,CcLinˆHLare all the same
classc′
Lbut are distinguishable from cLdepending on|VRcL,i|.
Proof.After running Algorithm 1 on H= (V,E), let ˆHL= (ˆVL,ˆEL≜E∪/unionsqtext
cL,i{VRcL,i})be the hypergraph
formed by attaching a hyperedge to each VRcL,i.
For anycL, aL-GWL-1 node class, let RcL,i,i= 1,...,|CcL|be a connected component subhypergraph of
HcL. Over all (cL,i)pairs, all theRcL,iare disconnected from each other and for each cLeachRcL,iis
maximally connected on HcL.
Upon covering all the nodes VRcL,iof each induced connected component subhypergraph RcL,iwith a single
hyperedgee=VRcL,iof sizes=|VRcL,i|, we claim that every node of class cLbecomescL,s, aL-GWL-1
node class depending on the original L-GWL-1 node class cLand the size of the hyperedge s.
Consider for each v∈VRcL,ithe2L-hop rooted tree (˜B2L
V,E)˜vforpBV,E(˜v) =v. Also, for each v∈VRcL,i,
define the tree
Te≜(˜B2L−1
ˆV\{v},ˆE)˜e (79)
We do not index the tree Tebyvsince it does not depend on v∈VRcL,i. We prove this in the following.
proof for: Tedoes not depend on v∈VRcL,i:
Let node ˜ebe the lift of eto(˜B2L−1
ˆV,ˆE)˜e. Define the star graph (N(˜e))˜eas the 1-hop neighborhood of ˜ein
(˜B2L−1
ˆV,ˆE)˜e. We must have:
(˜B2L−1
ˆV,ˆE)˜e∼=c((N(˜e))˜e⊔/unionsqdisplay
˜u∈VN(˜e)\{˜e}(˜B2L−2
V,E)˜u)˜e (80)
Define for each node v∈ewith lift ˜v:
(N(˜e,˜v))˜e≜(V(N(˜e))˜e\{˜v},E(N(˜e))˜e\{(˜e,˜v)})˜e (81)
43Published in Transactions on Machine Learning Research (11/2024)
The tree (N(˜e,˜v))˜eis a star graph with the node ˜vdeleted from (N(˜e))˜e. The star graphs (N(˜e,˜v))˜e⊆
(N(˜e))˜edo not depend on ˜vas long as ˜v∈V(N(˜e))˜e. In other words,
(N(˜e,˜v))˜e∼=c(N(˜e,˜v′))˜e,∀˜v,˜v′∈V(N(˜e))˜e\{˜e} (82)
Since the rooted tree (˜B2L−1
ˆV,ˆE)˜e, where ˜eis the lift of eby universal covering map pBV,E, has all pairs of nodes
˜u,˜u′∈˜ein it with (˜B2L
V,E)˜u∼=c(˜B2L
V,E)˜u′, which implies
(˜B2L−2
V,E)˜u∼=c(˜B2L−2
V,E)˜u′,∀˜u,˜u′∈˜e (83)
By Equations 83, 82, we thus have:
(˜B2L−1
ˆV\{v},ˆE)˜e∼=c((N(˜e,˜v))˜e⊔/unionsqdisplay
˜u∈V(N(˜e,˜v))˜e\{˜e}(˜B2L−2
V,E)˜u)˜e (84)
This proves that Tedoes not need to be indexed by v∈VRcL,i.
We continue with the proof that all nodes in VRcL,ibecome the L-GWL-1 node class cL,sfors=|VRcL,i|.
Since every v∈VRcL,ibecomes connected to a hyperedge e=VRcL,iinˆH, we must have:
(˜B2L
ˆV,ˆE)˜v∼=c((˜B2L
V,E)˜v∪(˜v,˜e)Te)˜v,∀v∈VRcL,i (85)
The notation ((˜B2L
V,E)˜v∪(˜v,˜e)Te)˜vdenotes a tree rooted at ˜vthat is the attachment of the tree Terooted at
˜eto the node ˜vby the edge (˜v,˜e). As is usual, we assume ˜v,˜eare the lifts of v∈V,e∈Erespectively. We
only need to consider the single esinceLwas chosen small enough so that the 2L-hop tree (˜B2L
ˆV,ˆE)˜vdoes not
contain a node ˜usatisfyingpBV,E(˜u) =uwithu∈VRcL,jfor allj= 1,...,|CcL|,j̸=i.
SinceTedoes not depend on v∈VRcL,i,
(˜B2L
ˆV,ˆE)˜u∼=c(˜B2L
ˆV,ˆE)˜v,∀u,v∈VRcL,i (86)
This shows that hL
u(ˆH) =hL
v(ˆH),∀u,v∈VRcL,iby Theorem B.8. Furthermore, since each v∈VRcL,i⊆ˆV
inˆHis now incident to a new hyperedge e=VRcL,i, we must have that the L-GWL-1 class cLofVRcL,ion
His now distinguishable by |VRcL,i|.
We will need the following definition to prove the next lemma.
Definition B.14. A partial universal cover of hypergraph H= (V,E)with an unexpanded induced subhy-
pergraphR, denotedU(H,R)V,Eis a graph cover of BV,Ewhere we freezeBVR,ER⊆˜BV,Eas an induced
subgraph.
Al-hop rooted partial universal cover of hypergraph H= (V,E)with an unexpanded induced subhypergraph
R, denoted (Ul(H,R)V,E)˜uforu∈Vor(Ul(H,R)V,E)˜efore∈E, where ˜v,˜eare lifts of v,e, is a rooted
graph cover ofBV,Ewhere we freezeBVR,ER⊆˜BV,Eas an induced subgraph.
Lemma B.16. Assuming the same conditions as Lemma B.15, where H= (V,E)is a hypergraph and
for allL-GWL-1 node classes cLwith connected components RcL,i, as discovered by Algorithm 1, so that
L≥diam (RcL,i). Instead of only adding the hyperedges {VRcL,i}cL,itoEas stated in the main paper, let
ˆH†≜(V,(E\RE)⊔RV), meaningHwith eachRcL,ifori= 1,...,|CcL|having all of its hyperedges dropped
and with a single hyperedge that covers VRcL,iand let ˆH= (V,E⊔RV)then:
The GWL-1 node classes of VRcL,ifori= 1,...,|CcL|inˆHare all the same class c′
Lbut are distinguishable
fromcLdepending on|VRcL,i|.
44Published in Transactions on Machine Learning Research (11/2024)
Proof.For anycL, aL-GWL-1 node class, let RcL,i,i= 1,...,|CcL|be a connected component subhypergraph
ofHcL. These connected components are discovered by the algorithm. Over all (cL,i)pairs, all theRcL,i
are disconnected from each other. Upon arbitrarily deleting all hyperedges in each such induced connected
component subhypergraph RcL,iand adding a single hyperedge of size s=|VRcL,i|, we claim that every
node of class cLbecomescL,s, aL-GWL-1 node class depending on the original L-GWL-1 node class cLand
the size of the hyperedge s.
Define the subhypergraph made up of the disconnected components RcL,ias:
R:=/uniondisplay
c,iRcL,i (87)
SinceL≥diam (RcL,i), we can construct the 2L-hop rooted partial universal cover with unexpanded induced
subhypergraphR, denoted by (U2L(H,R)V,E)˜v,∀v∈VofHas given in Definition B.14.
Denote the hyperedge nodes, or right hand nodes of the bipartite graph by B(VR,ER)byR(B(VR,ER)).
Their corresponding hyperedges are ER⊆E(U(H,R))⊆E. Since eachRcL,iis maximally connected, for
any nodesu,v∈VRwe have:
(U2L(H,R)˜u\R(B(VR,ER)))˜u∼=c(U2L(H,R)˜v\R(B(VR,ER)))˜v (88)
by Proposition B.13, where U2L(H,R)˜v\R(B(VR,ER))denotes removing the nodes R(B(VR,ER))from
U2L(H,R)˜v. This follows since removing R(B(VR,ER))removes an isomorphic neighborhood of hyperedges
from each node in VR. This requires assuming maximal connectedness of each RcL,i. Upon adding the
hyperedge
ecL,i≜VRcL,i (89)
covering all ofVRcL,iafter the deletion of ERcL,ifor every (cL,i)pair, we see that any node u∈VRcL,iis
connected to any other node v∈VRcL,ithroughecL,iin the same way for all nodes u,v∈VRcL,i. In fact,
we claim that all the nodes in VRcL,istill have the same GWL-1 class.
We can write the multi-hypergraph ˆH†equivalently as (V,/unionsqtext
cL,i(E\E(RcL,i)⊔{{ecL,i}})), which is the multi-
hypergraph formed by the algorithm. The replacement operation on Hcan be viewed in the universal
covering space ˜BV,Eas takingU(H,R)and replacing the frozen subgraph BVR,ERwith the star graphs
(NˆH†(˜ecL,i))˜ecL,iof root node ˜ecL,idetermined by hyperedge ecL,ifor each connected component indexed by
(cL,i). Since the star graphs (NˆH†(˜ecL,i))˜ecL,iare cycle-less, we have that:
(U(H,R)\R(B(VR,ER)))∪/uniondisplay
cL,i(NˆH†(˜ecL,i))˜ecL,i∼=c˜BVˆH†,EˆH†(90)
Viewing Equation 90 locally, by our assumptions on L, for anyv∈VRcL,i, we must also have:
(U2L(H,R)˜v\R(B(VR,ER)))/uniondisplay
(NˆH†(˜ec,i))˜ec,i∼=c˜BVˆH†,EˆH†(91)
We thus have (˜B2L
VˆH†,EˆH†)˜u∼=c(˜B2L
VˆH†,EˆH†)˜vfor everyu,v∈VRcL,iwith ˜u,˜vbeing the lifts of u,vbypBV,E,
since (U2L(H,R)˜u\R(B(VR,ER)))˜u∼=c(U2L(H,R)˜v\R(B(VR,ER)))˜vfor everyu,v∈VRcL,ias in Equation
88. These rooted universal covers now depend on a new hyperedge ecL,iand thus depend on its size s.
This proves the claim that all the nodes in VRcL,iretain the same L-GWL-1 node class by changing HtoˆH†
and that this new class is distinguishable by s=|VRcL,i|. In otherwords, the new class can be determined
bycs. Furthermore, cL,son the hyperedge ecL,icannot become the same class as an existing class due to
the algorithm.
Theorem B.17. Let|V|=n,L∈Z+andvol(v)≜/summationtext
e∈E:e∋v|e|and assuming that the collection of node
subsetsCcLis sufficiently L-separated.
Ifvol(v) =O(log1−ϵ
4Ln),∀v∈Vfor any constant ϵ >0;|ScL|≤S,∀cL∈CL,Sconstant, and|VcL,s|=
O(nϵ
log1
2k(n)),∀s∈CcL, then fork∈Z+andk-tupleC= (cL,1,...,cL,k),cL,i∈GL,i= 1..kthere exists
45Published in Transactions on Machine Learning Research (11/2024)
ω(n2kϵ)many pairs of k-node setsS1̸≃S2such that (hL
u(H))u∈S1= (hL
v∈S2(H)) =C, as ordered k-tuples,
whileh(S1,ˆHL)̸=h(S2,ˆHL)also byLsteps of GWL-1.
Proof.
1. Constructing forests from the rooted universal cover trees :
The first part of the proof is similar to the first part of the proof of Theorem 2 of Zhang et al. (2021).
Consider an arbitrary node v∈Vand denote the 2L-hop tree rooted at vfrom the universal cover as (˜B2L
V,E)v
as in Theorem B.8. As each node v∈Vhas volume vol(v) =/summationtext
v∈e|e|=O(log1−ϵ
4Ln), then every edge e∈E
has|e|=O(log1−ϵ
4Ln)and for all v∈Vwe have that deg(v) =O(log1−ϵ
4Ln), we can say that every node in
(˜B2L
V,E)˜vhas degreed=O(log1−ϵ
4Ln). Thus, the number of nodes in (˜B2L
V,E)˜v, denoted by|V((˜B2L
V,E)˜v|, satisfies
|V((˜B2L
V,E)˜v|≤/summationtext2L
i=0di=O(d2L) =O(log1−ϵ
2n). We setK≜maxv∈V|V((˜B2L
V,E)˜v|as the maximum number
of nodes of (˜B2L
V,E)˜vand thusK=O(log1−ϵ
2n). For allv∈V, expand trees (˜B2L
V,E)˜vto(˜B2L
V,E)˜vby adding
K−|V((˜B2L
V,E)˜v|independent nodes. Then, all (˜BL
V,E)˜vhave the same number of nodes, which is K, becoming
forests instead of trees.
2. Counting|GL|:
Next, we consider the number of non-isomorphic forests over Knodes. Actually, the number of non-
isomorphic graphs over K nodes is bounded by 2(K
2)=exp(O(log1−ϵ
2n)) =o(n1−ϵ). Therefore, due to
the pigeonhole principle, there existn
o(n1−ϵ)=ω(nϵ)many nodes vwhose (˜BL
V,E)˜vare isomorphic to each
other. DenoteGLas the set of all L-GWL-1 values. Denote the set of these nodes as VcL, which consist of
nodes whose L-GWL-1 values are all the same value cL∈GLafterLiterations of GWL-1 by Theorem B.8.
For a fixed L, the setsVcLform a partition of V, in other words,/unionsqtext
cL∈GLVcL=V. Next, we focus on looking
atk-sets of nodes that are not equivalent by GWL-1.
For anycL∈GL, there is a partition VcL=/unionsqtext
sVcL,swhereVcL,sis the set of nodes all of which have L-GWL-
1 classcLand that belong to a connected component of size sinHcL. LetScL≜{|VRcL,j|:RcL,j∈CcL}
denote the set of sizes s≥1of connected component node sets of HcL. We know that|ScL|≤SwhereSis
independent of n.
3. Computing the lower bound:
LetYdenote the number of pairs of k-node setsS1̸≃S2such that (hL
u(H))u∈S1= (hL
v(H))v∈S2=C=
(c(L,1),...,c (L,k)), as ordered tuples, from L-steps of GWL-1. Since if any pair of nodes u,vhave the same L-
GWL-1 values cL, then they become distinguishable by the size of the connected component in HcLthat they
belong to. We can lower bound Yby counting over all pairs of ktuples of nodes ((u1,...,uk),(v1,...,vk))∈
(/producttextk
i=1Vc(L,i))×(/producttextk
i=1Vc(L,i))that both have L-GWL-1 values (c(L,1),...,c (L,k))where there is atleast one
i∈{1,..,k}whereuiandvibelong to different sized connected components si,s′
i∈Sc(L,i)withsi̸=s′
i. We
have:
Y≥1
k![/summationdisplay
((si)k
i=1,(s′
i)k
i=1)∈[(/producttextk
i=1SL
c(L,i))]2
:(si)k
i=1̸=(s′
i)k
i=1k/productdisplay
i=1|VL
(c(L,i)),si||VL
(c(L,i)),s′
i|] (92a)
=1
k![k/productdisplay
i=1(/summationdisplay
si∈SLci|VL
(c(L,i)),si|)2−/summationdisplay
(si)k
i=1∈/producttextk
i=1SL
(c(L,i))(k/productdisplay
i=1|VL
(c(L,i)),si|2)] (92b)
Using the fact that for each i∈{1,...,k},|Vc(L,i)|=/summationtext
si∈Sc(L,i)|V(c(L,i)),si|and by assumption |V(c(L,i)),si|=
O(nϵ
log1
2kn)for anysi∈Sc(L,i), thus we have:
Y≥ω(n2kϵ)−O(|S|kn2kϵ
logn)] =ω(n2kϵ) (93)
46Published in Transactions on Machine Learning Research (11/2024)
Example: A simple example of a hypergraph that statisfies the conditions of Theorem B.17 is a union
of many disconnected hypergraphs H=∪iHi= (V,E)with|VHi|≤SwhereS <∞is a small constant
independent of n=|V|≥S. Such a hypergraph could be a social network where the nodes are user instances
and the hyperedges are private groups. The disconnected hypergraphs represent disconnected communities
where a user can only belong to a single community.
Even though Theorem B.17 does not depend on the cardinality of a set of disconnected hypergraphs Hi
indistinguishable by GWL-1, due to the disconnected nature of Hand the small size of its components, there
is a large chance of obtaining a large number of such components. We give a very rough estimate of this in
the following:
Assuming thatH=∪iHihas eachHii.i.d. sampled from a distribution of s-uniformd-regular hypergraphs
ofnnodes, denotedRn,s,d:P(Hi=Rn,s,d). If the parameters (n,s,d )forRn,s,dsatisfynd=|E|swhere
|E|∈Z+, then a well defined hypergraph is formed. This distribution can be factorized as follows:
P(Hi=Rn,s,d) =P(deg(v) =d|r=s,|V|=n,nd mods≡0)P(r=s||V|=n)P(|V|=n)(94)
where:
P(deg(v) =d|r=s,|V|=n,nd mods≡0) =U({d:nd mods≡0})≥1/parenleftbign
s/parenrightbig,∀v∈VHi(95a)
P(r=s||V|=n) =1
n,P(|V|=n) =1
Sforn≤S (95b)
and we have that
P(H1is neighborhood regular )≥P(H1is a cycle graph )≥1
SS+2
(96)
and that:
P(h(Hi,H) =h(H1,H),H1is neighborhood regular ) (97a)
≥P(h(Hi,H) =h(H1,H),H1is a cycle graph of length S) (97b)
≥P(Hiis a cycle graph of length S)P(H1is a cycle graph of length S) (97c)
≥1
S2(S+2)
,∀i>1 (97d)
where a cycle graph is a 2-uniform hypergraph where each node has degree 2.
Since we sample each Hii.i.d., the indicator random variable is a Bernoulli random variable. By Hoeffding’s
inequality on the sum of Bernoulli random variables, we get:
Pr(m/summationdisplay
i=11[Hiis neighborhood regular and h(Hi,H) =h(H1,H)]≥(m
S2S+4+t))≤e−2t2
m (98)
where/summationtextm
i=1|VHi|=|V|. This means that with large number of samples m, or largen, it is possible for the
number of regular hypergraphs Hiequivalent up to GWL-1 to be atleast of order Ω(√m) +m
S2S+4with high
probability. This is one of the simplest examples that demonstrates Theorem B.17.
For the following proof, we will denote ∼=Has a node or hypergraph automorphism with respect to a
hypergraphH.
Theorem B.18 (Invariance and Expressivity) .IfL=∞, GWL-1 enhanced by Algorithm 1 is still invari-
ant to node isomorphism classes of Hand can be strictly more expressive than GWL-1 to determine node
isomorphism classes.
47Published in Transactions on Machine Learning Research (11/2024)
Proof.
1. Expressivity :
LetL∈Z+be arbitrary. We first prove that L-GWL-1 enhanced by Algorithm 1 is strictly more expressive
for node distinguishing than L-GWL-1 on some hypergraph(s). Let C3
4andC3
5be two 3-regular hypergraphs
from Figure 2. Let H=C3
4/unionsqtextC3
5be the disjoint union of the two regular hypergraphs. Literations of GWL-1
will assign the same node class to all of VH. These two subhypergraphs can be distinguished by L-GWL-1
forL≥1after editing the hypergraph Hfrom the output of Algorithm 1 and becoming ˆH=ˆC3
4∪ˆC3
5. This
is all shown in Figure 2. Since Lwas arbitrary, this is true for L=∞.
2. Invariance :
For any hypergraph H, let ˆH= (ˆV,ˆE)beHmodified by the output of Algorithm 1 by adding hyperedges to
VRc,i. GWL-1 remains invariant to node isomorphism classes of HonˆH.
a. Case 1 (nodeu∈Vhas its class cchanged to class cs):
LetL∈Z+be arbitrary. For any node uwithL-GWL-1 class cchanged to csinˆH, ifu∼=Hvfor any
v∈V, then the GWL-1 class of vmust also be cs. In otherwords, both uandvbelong tos-sized connected
components inHcWe prove this by contradiction.
Sayubelong to a L-GWL-1 symmetric induced subhypergraph Swith|VS|=s.
i.Sayvis originally of L-GWL-1 class cand changes to L-GWL-1cs′fors′<sonˆH, WLOG.
If this is the case then vbelongs to a L-GWL-1 symmetric induced subhypergraph S′with|VS′|=s′. Since
there is aπ∈Aut(H)withπ(u) =vand sinces′<s, by the pigeonhole principle some node w∈VSmust
haveπ(w)/∈VS′. SinceSandS′are maximally connected, π(w)cannot share the same L-GWL-1 class as
w. Thus, it must be that (˜B2L
V,E)]π(w)̸∼=c(˜B2L
V,E)˜wwhere ˜w,]π(w)are the lifts of w,π(w)by universal covering
mappBV,E. However wandπ(w)both belong to L-GWL-1 class cinH, meaning (˜B2L
V,E)]π(w)∼=c(˜B2L
V,E)˜w,
contradiction.
ii.Say nodev∈Vhas its class cunchanged.
The argument for when vdoes not change its class cafter the algorithm, follows by noticing that since cis
the GWL-1 node class of u,csis the GWL-1 node class of vandc̸=cs. Thus we must have u̸∼=Hvonce
again by the contrapositive of Theorem B.8. This also gives a contradiciton.
SinceLwas arbitrary, the contradiction must be true for L=∞.
b. Case 2 (nodeu∈Vhas its class cunchanged):
Now assume L=∞. LetpBV,Ebe the universal covering map of BV,E. For all other nodes u′∼=Hv′for
u′,v′∈Vunaffectedbythereplacement, meaningtheydonotbelongtoany Rc,idiscoveredbythealgorithm,
if the rooted universal covering tree rooted at node ˜u′connects to any node ˜winlhops in (˜Bl
V,E)˜u′where
pBV,E(˜u′) =u′,pBV,E( ˜w) =wand where whas any class cinH, then ˜v′must also connect to a node ˜zin
lhops in (˜Bl
V,E)˜u′wherepBV,E(˜z) =zandw∼=Hz. Furthermore, if wbecomes class csinHdue to the
algorithm, then zalso becomes class csinˆH. This will follow by the previous result on isomorphic wandz
both of class cwithwbecoming class csinˆH.
SinceL=∞: For anyw∈Vconnected by some path of hyperedges to u′∈V, consider the smallest lfor
which (˜Bl
V,E)˜u′, thel-hop universal covering tree of Hrooted at ˜u′, the lift of u′, contains the lifted ˜wof
w∈Vwith GWL-1 node class cat layerl. Sinceu′∼=Hv′byπ. We can use πto find some z=π(w).
We claim that ˜zislhops away from ˜v′. Sinceu′∼=Hv′due to some π∈Aut(H)withπ(u′) =v′, using
Proposition B.2 for singleton nodes and by Theorem B.8 we must have (˜Bl
V,E)˜u′∼=c(˜Bl
V,E)˜v′as isomorphic
rooteduniversalcoveringtreesduetoaninducedisomorphism ˜πofπwherewedefineaninducedisomorphism
˜π: (˜BV,E)˜u′→(˜BV,E)˜v′between rooted universal covers (˜BV,E)˜u′and(˜BV,E)˜v′for˜u′,˜v′∈V(˜BV,E)as˜π(˜a) =˜b
ifπ(a) =b∀a,b∈V(BV,E)connected to u′andv′respectively and pBV,E(˜a) =a,pBV,E(˜b) =b. Sincelis the
shortest path distance from ˜u′to˜w, there must exist some shortest (as defined by the path length in BV,E)
48Published in Transactions on Machine Learning Research (11/2024)
pathPof hyperedges from u′towwith no cycles. Using π, we must map Pto another acyclic shortest path
of the same length from v′toz. This path correponds to a llength shortest path from ˜v′to˜zin(˜BV,E)˜v′.
Ifwhas GWL-1 class cinHthat doesn’t become affected by the algorithm, then zalso has GWL-1 class c
inHsincew∼=Hz.
Ifwhas classcand becomes csinˆH, by the previous result, since w∼=Hzwe must have the GWL-1 classes
c′andc′′ofwandzinˆHbe both equal to cs.
The nodewconnected to u′was arbitrary and so both ˜wand the isomorphism induced ˜zarelhops away
from ˜u′and˜v′respectively, with the same GWL-1 class c′inˆH, thus (˜BˆV,ˆE)˜u′∼=c(˜BˆV,ˆE)˜v′.
We have thus shown, if u∼=Hvforu,v∈H, then in ˆHwe havehL
u(ˆH) =hL
v(ˆH)using the duality between
universal covers and GWL-1 from Theorem B.8 and Proposition B.12.
Here we redefine the symmetry group of the L-GWL-1 node representation map as in the main paper:
Sym(hL(ˆHL))≜/intersectiondisplay
ˆH′
L∼P(ˆHL)Sym(hL(ˆH′
L)) (99)
Proposition B.19. The multi-hypergraph ˆHLbreaks the symmetry of the L-GWL-1 view of the hypergraph
H:
Sym(hL(ˆHL))⊆Autc(˜B2L
V,E),∀L≥1 (100)
Proof.By Equation 99 we have that Sym(hL(ˆHL)) =/intersectiontext
ˆH′
L∼P(ˆHL)Sym(hL(ˆH′
L)). Since the original inci-
dence matrix Hbelongs to supp(P(ˆHL)), we must have/intersectiontext
ˆH′
L∼P(ˆHL)Sym(hL(ˆH′
L))⊆Sym(hL(H)). Since
Sym(hL(H))∼=Autc(˜B2L
V,E), we thus have:
Sym(hL(ˆHL)) =/intersectiondisplay
ˆH′
L∼P(ˆHL)Sym(hL(ˆH′
L))⊆Sym(hL(H))∼=Autc(˜B2L
V,E),∀L≥1 (101)
which proves the symmetry breaking statement.
Proposition B.20 (Complexity) .LetHbe the star expansion matrix of H. Algorithm 1 runs in time
O(L·nnz(H)+(n+m)), the size of the input hypergraph when viewing Las constant, where nis the number
of nodes,nnz(H) =vol(V)≜/summationtext
v∈Vdeg(v)andmis the number of hyperedges.
Proof.Computing Edeg, which requires computing the degrees of all the nodes in each hyperedge takes time
O(nnz(H)). The setEdegcan be stored as a hashset datastructure. Constructing this takes O(nnz(H)).
Computing GWL-1 takes O(L·nnz(H))time assuming a constant Lnumber of iterations. Constructing the
bipartite graphs for Htakes time O(nnz(H) +n+m)since it is an information preserving data structure
change. Define for each c∈C,nc:=|Vc|,mc:=|Ec|. Since the classes partition V, we must have:
n=/summationdisplay
c∈Cnc;m=/summationdisplay
c∈Cmc;nnz(H) =/summationdisplay
c∈Cnnz(Hc) (102)
whereHcis the star expansion matrix of Hc. Extracting the subgraphs can be implemented as a masking
operation on the nodes taking time O(nc)to formVcfollowed by searching over the neighbors of Vcin time
O(mc)to constructEc. Computing the connected components for Hcfor a predicted node class ctakes
timeO(nc+mc+nnz(Hc)). Iterating over each connected component for a given cand extracting their
nodes and hyperedges takes time O(nci+mci)wherenc=/summationtext
inci,mc=/summationtext
imci. Checking that a connected
component has size at least 3takesO(1)time. Computing the degree on Hfor all nodes in the connected
component takes time O(nci)since computing degree takes O(1)time. Checking that the set of node degrees
of the connected component doesn’t belong to Edegcan be implemented as a check that the hash of the set
of degrees is not in the hashset datastructure for Edeg.
49Published in Transactions on Machine Learning Research (11/2024)
Adding up all the time complexities, we get the total complexity is:
O(nnz(H)) +O(nnz(H) +n+m) +/summationdisplay
c∈C(O(nc+mc+nnz(Hc)) +/summationdisplay
conn. comp. iofHcO(nci+mci))(103a)
=O(nnz(H) +n+m) +/summationdisplay
c∈C(O(nc+mc+nnz(Hc)) +O(nc+mc)) (103b)
=O(nnz(H) +n+m) (103c)
Proposition B.21. For a connected hypergraph H, let(RV,RE)be the output of Algorithm 1 on H. Then
there are Bernoulli probabilities p,qifori= 1,...,|RV|for attaching a covering hyperedge so that ˆπis an
unbiased estimator of π.
Proof.LetCcL={RcL,i}ibe the maximally connected components induced by the vertices with L-GWL-1
valuescL. The set of vertex sets {V(RcL,i)}and the set of all hyperedges ∪i{E(RcL,i)}over all the connected
componentsRcL,ifori= 1,...,CcLform the pair (RV,RE).
For a hypergraph random walk on connected H= (V,E), its stationary distribution πonVis given by the
closed form:
π(v) =deg(v)/summationtext
u∈Vdeg(u)(104)
forv∈V.
Let ˆH= (V,ˆE)be the random multi-hypergraph as determined by pandqifori= 1,...,|RV|. These
probabilities determine ˆHby the following operations on the hypergraph H:
•Attaching a single hyperedge that covers VRcL,iwith probability qiand not attaching with proba-
bility 1−qi.
•All the hyperedges in RcL,iare dropped/kept with probability pand1−prespectively.
1. Setup:
Letdeg(v)≜|{e:e∋v}|forv∈V(H)anddeg(S)≜/summationtext
u∈V(S)deg(u)forS⊆Ha subhypergraph.
Let
Bernoulli (p)≜/braceleftigg
1prob.p
0prob. 1−p(105)
and
Binom (n,p)≜n/summationdisplay
i=1Bernoulli (p) (106)
Define for each v∈V,C(v)to be the unique RcL,iwherev∈RcL,iThis means that we have the following
independent random variables:
Xe≜Bernoulli (1−p),∀e∈E(i.i.d. across all e∈E) (107a)
XC(v)≜Bernoulli (qi) (107b)
As well as the following constant, depending only on C(v):
mC(v)≜/summationdisplay
u∈V\C(v)deg(u) (108)
whereC(v)⊆V,∀v∈V
50Published in Transactions on Machine Learning Research (11/2024)
Letˆπbe the stationary distribution of ˆH. Its expectation E[ˆπ]can be written as:
ˆπ(v)≜/summationtext
e∋v:e∈EXe+XC(v)
mC(v)+/summationtext
e∋u:u∈C(v),e∈EXe+XC(v)(109)
Letting
Nv≜/summationdisplay
e∋v:e∈EXe=Binom (deg(v),1−p) (110a)
N≜Nv+XC(v) (110b)
D≜mC(v)+/summationdisplay
e∋u:u∈C(v),e∈EXe+XC(v) (110c)
C≜D−(/summationdisplay
e∋v:e∈E|e|)Nv−mC(v)=/summationdisplay
e∋u:v/∈e,u∈C(v),e∈EXe−mC(v)=Binom (Fv,1−p)−mc(v)(110d)
whereFv≜|{e∋u:v /∈e,u∈C(v),e∈E}|
and so we have: ˆπ(v) =N
D
We have the following joint independence Nv⊥XC(v)⊥Cdue to the fact that each random variable
describes disjoint hyperedge sets.
2. Computing the Expectation:
Writing out the expectation with conditioning on the joint distribution P(D,Nv,XC(v)), we have:
E[ˆπ(v)] =1/summationdisplay
b=0deg(v)/summationdisplay
j=0deg(C(v))/summationdisplay
k=mC(v)E[ˆπ(v)|D=k,N =j]P(D=k,Nv=j,XC(v)=b) (111a)
=1/summationdisplay
b=0deg(v)/summationdisplay
j=0deg(C(v))/summationdisplay
k=mC(v)1
kE[N|D=k,Nv=j,XC(v)=b]P(D=k,Nv=j,XC(v)=b)(111b)
=1/summationdisplay
b=0deg(v)/summationdisplay
j=0deg(C(v))/summationdisplay
k=mC(v)j+b
kP(D=k,Nv=j,XC(v)=b) (111c)
=1/summationdisplay
b=0deg(v)/summationdisplay
j=0deg(C(v))/summationdisplay
k=mC(v)j+b
kP(D=k)P(Nv=j)P(XC(v)=b) (111d)
=1/summationdisplay
b=0deg(v)/summationdisplay
j=0deg(C(v))/summationdisplay
k=mC(v)j+b
kP(C=k−deg(v)j−mC(v))P(Nv=j)P(XC(v)=b) (111e)
=1/summationdisplay
b=0deg(v)/summationdisplay
j=0deg(C(v))/summationdisplay
k=mC(v)j+b
kP(Binom (Fv,1−p),1−p) =k−deg(v)j−mC(v))
·P(Binom (deg(v),1−p) =j)·P(Bernoulli (qi) =b)(111f)
=1/summationdisplay
b=0deg(v)/summationdisplay
j=0deg(C(v))/summationdisplay
k=mC(v)j+b
k/parenleftbiggFv
k−deg(v)j−mC(v)/parenrightbigg
(1
2)Fv·/parenleftbiggdeg(v)
j/parenrightbigg
(1
2)deg(v)·P(Bernoulli (qi) =b)(111g)
=1/summationdisplay
b=0deg(v)/summationdisplay
j=0deg(C(v))/summationdisplay
k=mC(v)j+b
k/parenleftbiggFv
k−deg(v)j−mC(v)/parenrightbigg
(1
2)Fv
·/parenleftbiggdeg(v)
j/parenrightbigg
(1
2)deg(v)·P(Bernoulli (qi) =b)(111h)
51Published in Transactions on Machine Learning Research (11/2024)
=deg(v)/summationdisplay
j=0deg(C(v))/summationdisplay
k=mC(v)/parenleftbiggFv
k−deg(v)j−mC(v)/parenrightbigg
(1
2)Fv·/parenleftbiggdeg(v)
j/parenrightbigg
(1
2)deg(v)[(1−qi)j
k+qij+ 1
k](111i)
=deg(v)/summationdisplay
j=0deg(C(v))/summationdisplay
k=mC(v)/parenleftbiggFv
k−deg(v)j−mC(v)/parenrightbigg
(1−p)Fv−(k−deg(v))pk−deg(v)·/parenleftbiggdeg(v)
j/parenrightbigg
(1−p)jpdeg(v)−j[j
k+qi1
k]
(111j)
=C1(p) +qiC2(p) (111k)
3. Pickpandqi:
We want to find pandqiso that E[ˆπ(v)] =C1(p) +qiC2(p) =π(v)
We know that for a given p∈[0,1], we must have:
qi=π(v)−C1(p)
C2(p)(112)
In order for qi∈[0,1], must have π(v)≥C1(p)andπ(v)−C1(p)≤C2(p).
a. Pickpsufficiently large:
Notice that
0≤C1(p)≤O(E[1
Binom (Fv,1−p) +mC(v)]·E[Binom (deg(v),1−p)]) =O(1
mC(v)deg(v)(1−p))(113)
and that
0≤C1(p)≤O(C2(p)) (114)
forp∈[0,1]sufficiently large. This is because
C1(p)≤O(1
mC(v)deg(v)(1−p)) (115)
and
Ω(1
mC(v)deg(v)(1−p))≤C2(p) (116)
Piecing these two inequalities together gets the desired inequality 114.
We can then pick a p∈[0,1]even larger than the previous pso that for the C′>0which gives C1(p)≤
C′
mC(v)deg(v)(1−p), we achieve
C1(p)≤C′
mC(v)deg(v)(1−p)<π(v) =deg(v)
mC(v)+/summationtext
u∈C(v)deg(u)(117)
We then have that there exists a s>1so that
sC1(p) =π(v) (118)
Using this relationship, we can then prove that for a sufficiently large p∈[0,1], we must have a qi∈[0,1]
b.p∈[0,1]sufficiently large implies qi≥0:
We thus have qi≥0since its numerator is nonnegative:
π(v)−C1(p) = (s−1)C1(p)≥0⇒qi≥0 (119)
52Published in Transactions on Machine Learning Research (11/2024)
c.p∈[0,1]sufficiently large implies qi≤1:
π(v)−C1(p) =sC1(p)−C1(p) = (s−1)C1(p)≤C2(p)⇒qi≤1 (120)
53Published in Transactions on Machine Learning Research (11/2024)
C Additional Experiments
0 5000 10000 15000 20000
Num. Nodes0.000.020.040.060.080.10Frac. of Same GWL-1 Valued Conn. Comps. of Size Atleast 3hypergraph datasets
graph datasets
(a) Fraction of components of size atleast 3
selected by Algorithm 1.
0 5000 10000 15000 20000
Num. Nodes34567Average Size of Conn. Comps. with Size Atleast 3hypergraph datasets
graph datasets(b) Average size of components of size atleast
3from Algorithm 1.
Figure 6
As we are primarily concerned with symmetries in a hypergraph, we empirically measure the size and
frequency of the components found by the Algorithm for real-world datasets. For the real-world datasets
listed in Appendix D, in Figure 6a, we plot the fraction of connected components of the same L-GWL-1
value (L= 2) that are atleast 3in cardinality from Algorithm 1 as a function of the number of nodes of the
hypergraph. For these datasets, it is much more common for the connected components to be of sizes 1and
2. On the right, in Figure 6b we show the distribution of the sizes of the connected components found by
Algorithm 1. We see that, on average, the connected components are at least an order of magnitude smaller
compared to the total number of nodes. Common to both plots, the graph datasets appear to have more
nodes and a consistent fraction and size of components, while the hypergraph datasets have higher variance
in the fraction of components, which is expected since there are more possibilities for the connections in a
hypergraph.
54Published in Transactions on Machine Learning Research (11/2024)
We show below the critical difference diagrams Demšar (2006) of the average PR-AUC score ranks (per-
centiles) for two datasets from Table 1 across all the downstream hyperGNNs. Each plot contains the
PR-AUC ranks of the three compared approaches: baseline, 50%hyperedge drop, and Our method. The
bars in each plot represents statistical insignificance between the two approaches according to 4runs of each
experiment.
0.60 0.62 0.64 0.66 0.68 0.70
baseline (0.61)drop (0.69)(0.7) OursHGNNP
0.5 0.6 0.7 0.8 0.9
drop (0.5)baseline (0.61)(0.89) OursHGNN
0.3 0.4 0.5 0.6 0.7 0.8 0.9
baseline (0.33)
drop (0.74)(0.92) OursHNHN
0.5 0.6 0.7 0.8 0.9
Ours (0.44)
baseline (0.68)(0.88) dropHyperGCN
0.55 0.60 0.65 0.70 0.75
drop (0.57)baseline (0.69)(0.74) OursUniGAT
0.4 0.5 0.6 0.7 0.8
drop (0.42)
baseline (0.72)(0.86) OursUniGCN
0.5 0.6 0.7 0.8
drop (0.5)baseline (0.63)(0.87) OursUniGIN
0.55 0.60 0.65 0.70 0.75 0.80 0.85
drop (0.56)Ours (0.61)(0.83) baselineUniSAGECritical difference diagram of average score ranks for cat-edge-madison-restaurant-reviews
Figure 7: Critical difference diagrams of cat-edge-madison-restaurant-reviews
0.600 0.625 0.650 0.675 0.700 0.725
Ours (0.6)drop (0.69)(0.71) baselineHGNNP
0.55 0.60 0.65 0.70 0.75 0.80
baseline (0.57)drop (0.64)(0.79) OursHGNN
0.55 0.60 0.65 0.70 0.75
drop (0.56)baseline (0.69)(0.76) OursHNHN
0.55 0.60 0.65 0.70 0.75 0.80 0.85
Ours (0.54)drop (0.62)(0.83) baselineHyperGCN
0.50 0.55 0.60 0.65 0.70 0.75
baseline (0.5)drop (0.73)(0.77) OursUniGAT
0.5 0.6 0.7 0.8
drop (0.49)baseline (0.7)(0.81) OursUniGCN
0.65 0.66 0.67 0.68 0.69 0.70
Ours (0.66)drop (0.66)(0.69) baselineUniGIN
0.55 0.60 0.65 0.70 0.75
baseline (0.57)drop (0.66)(0.78) OursUniSAGECritical difference diagram of average score ranks for cat-edge-vegas-bars-reviews
Figure 8: Critical difference diagrams of cat-edge-vegas-bars-reviews
55Published in Transactions on Machine Learning Research (11/2024)
C.1 Additional Experiments on Hypergraphs
In Table 3, we show the PR-AUC scores for four additional hypergraph datasets, cat-edge-Brain ,cat-
edge-vegas-bar-reviews ,WikiPeople-0bi , and JF17Kfor predicting size 3hyperedges.
Table 3: PR-AUC on four other hypergraph datasets. The top average scores for each hyperGNN method,
or row, is colored. Red scores denote the top scores in a row. Orange scores denote a two way tie and brown
scores denote a threeway tie.
PR-AUC↑Baseline Ours Baseln.+edrop
HGNN 0.75±0.01 0.79±0.11 0.74±0.09
HGNNP 0.75±0.05 0.78±0.10 0.74±0.12
HNHN 0.74±0.04 0.74±0.02 0.74±0.05
HyperGCN 0.74±0.09 0.50±0.07 0.50±0.12
UniGAT 0.73±0.07 0.81±0.10 0.81±0.09
UniGCN 0.78±0.04 0.81±0.09 0.71±0.08
UniGIN 0.74±0.09 0.74±0.03 0.74±0.07
UniSAGE 0.74±0.03 0.74±0.12 0.74±0.01
(a) cat-edge-BrainPR-AUC↑Baseline Ours Baseln.+edrop
HGNN 0.95±0.10 0.99±0.04 0.96±0.09
HGNNP 0.95±0.06 0.96±0.09 0.96±0.08
HNHN 1.00±0.08 0.99±0.09 0.95±0.10
HyperGCN 0.76±0.03 0.67±0.14 0.68±0.09
UniGAT 0.87±0.07 1.00±0.09 0.99±0.08
UniGCN 0.99±0.07 0.96±0.09 0.92±0.05
UniGIN 0.98±0.06 0.96±0.08 0.95±0.06
UniSAGE 0.94±0.05 0.98±0.07 0.97±0.07
(b) cat-edge-vegas-bar-reviews
PR-AUC↑Baseline Ours Baseln.+edrop
HGNN 0.52±0.01 0.57±0.08 0.54±0.10
HGNNP 0.52±0.03 0.54±0.07 0.54±0.06
HNHN 0.73±0.03 0.73±0.07 0.73±0.00
HyperGCN 0.54±0.05 0.55±0.02 0.49±0.10
UniGAT 0.49±0.09 0.54±0.04 0.53±0.04
UniGCN 0.46±0.08 0.68±0.08 0.51±0.08
UniGIN 0.73±0.09 0.73±0.01 0.73±0.02
UniSAGE 0.73±0.06 0.73±0.02 0.73±0.08
(c) WikiPeople-0biPR-AUC↑Baseline Ours Baseln.+edrop
HGNN 0.59±0.04 0.63±0.04 0.45±0.09
HGNNP 0.71±0.07 0.63±0.07 0.57±0.04
HNHN 0.73±0.04 0.73±0.03 0.73±0.04
HyperGCN 0.59±0.05 0.58±0.09 0.48±0.01
UniGAT 0.61±0.07 0.61±0.04 0.51±0.08
UniGCN 0.58±0.00 0.60±0.03 0.59±0.02
UniGIN 0.80±0.04 0.77±0.08 0.75±0.05
UniSAGE 0.79±0.02 0.77±0.08 0.74±0.01
(d) JF17K
C.2 Experiments on Graph Data
We show in Tables 4, 5, 6 the PR-AUC test scores for link prediction on some nonattributed graph datasets.
The train-val-test splits are predefined for FB15k-237 and for the other graph datasets a single graph is
deterministically split into 80/5/15 for train/val/test. We remove 10%of the edges in training and let
them be positive examples Ptrto predict. For validation and test, we remove 50%of the edges from both
validation and test to set as the positive examples Pval,Pteto predict. For train, validation, and test, we
sample 1.2|Ptr|,1.2|Pval|,1.2|Pte|negative link samples from the links of train, validation and test. Along
with hyperGNN architectures we use for the hypergraph experiments, we also compare with standard
GNN architectures: APPNP Gasteiger et al. (2018), GAT Veličković et al. (2017), GCN2 Chen et al.
(2020a), GCN Kipf & Welling (2016a), GIN Xu et al. (2018), and GraphSAGE Hamilton et al. (2017).
For every hyperGNN/GNN architecture, we also apply drop-edge Rong et al. (2019) to the input graph
and use this also as baseline. The number of layers of each GNN is set to 5and the hidden dimension at
1024. For APPNP and GCN2, one MLP is used on the initial node positional encodings. Since graphs
do not have any hyperedges beyond size 2, graph neural networks fit the inductive bias of the graph data
moreeasilyandthusmayperformbetterthanhypergraphneuralnetworkbaselinesmoreoftenthanexpected.
56Published in Transactions on Machine Learning Research (11/2024)
Table 4: PR-AUC on graph dataset johnshopkins55 . Each column is a comparison of the baseline PR-AUC
scores against the PR-AUC score for our method (first row) applied to a standard hyperGNN architecture.
Red color denotes the highest average score in the column. Orange color denotes a two-way tie in the column,
and brown color denotes a three-or-more-way tie in the column.
PR-AUC↑ HGNN HGNNP HNHN HyperGCN UniGAT UniGCN UniGIN UniSAGE
Ours 0.71±0.04 0.71±0.09 0.69±0.09 0.75±0.14 0.75±0.09 0.74±0.09 0.65±0.08 0.65±0.07
hyperGNN Baseline 0.68±0.00 0.69±0.06 0.67±0.02 0.75±0.04 0.74±0.02 0.74±0.00 0.65±0.05 0.64±0.08
hyperGNN Baseln.+edrop 0.67±0.02 0.70±0.07 0.66±0.00 0.75±0.03 0.73±0.08 0.74±0.05 0.63±0.01 0.64±0.03
APPNP 0.40±0.03 0.40±0.03 0.40±0.03 0.40±0.03 0.40±0.03 0.40±0.03 0.40±0.03 0.40±0.03
APPNP+edrop 0.40±0.13 0.40±0.13 0.40±0.13 0.40±0.13 0.40±0.13 0.40±0.13 0.40±0.13 0.40±0.13
GAT 0.49±0.03 0.49±0.03 0.49±0.03 0.49±0.03 0.49±0.03 0.49±0.03 0.49±0.03 0.49±0.03
GAT+edrop 0.51±0.05 0.51±0.05 0.51±0.05 0.51±0.05 0.51±0.05 0.51±0.05 0.51±0.05 0.51±0.05
GCN2 0.50±0.09 0.50±0.09 0.50±0.09 0.50±0.09 0.50±0.09 0.50±0.09 0.50±0.09 0.50±0.09
GCN2+edrop 0.56±0.07 0.56±0.07 0.56±0.07 0.56±0.07 0.56±0.07 0.56±0.07 0.56±0.07 0.56±0.07
GCN 0.73±0.02 0.73±0.02 0.73±0.02 0.73±0.02 0.73±0.02 0.73±0.02 0.73±0.02 0.73±0.02
GCN+edrop 0.73±0.01 0.73±0.01 0.73±0.01 0.73±0.01 0.73±0.01 0.73±0.01 0.73±0.01 0.73±0.01
GIN 0.73±0.06 0.73±0.06 0.73±0.06 0.73±0.06 0.73±0.06 0.73±0.06 0.73±0.06 0.73±0.06
GIN+edrop 0.73±0.01 0.73±0.01 0.73±0.01 0.73±0.01 0.73±0.01 0.73±0.01 0.73±0.01 0.73±0.01
GraphSAGE 0.73±0.08 0.73±0.08 0.73±0.08 0.73±0.08 0.73±0.08 0.73±0.08 0.73±0.08 0.73±0.08
GraphSAGE+edrop 0.73±0.09 0.73±0.09 0.73±0.09 0.73±0.09 0.73±0.09 0.73±0.09 0.73±0.09 0.73±0.09
Table 5: PR-AUC on graph dataset FB15k-237 . Each column is a comparison of the baseline PR-AUC
scores against the PR-AUC score for our method (first row) applied to a standard hyperGNN architecture.
Red color denotes the highest average score in the column. Orange color denotes a two-way tie in the column,
and brown color denotes a three-or-more-way tie in the column.
PR-AUC↑ HGNN HGNNP HNHN HyperGCN UniGAT UniGCN UniGIN UniSAGE
Ours 0.66±0.06 0.78±0.02 0.63±0.07 0.82±0.10 0.75±0.05 0.74±0.03 0.75±0.03 0.75±0.06
hyperGNN Baseline 0.65±0.06 0.65±0.06 0.65±0.04 0.82±0.09 0.74±0.04 0.74±0.05 0.75±0.03 0.77±0.01
hyperGNN Baseln.+edrop 0.65±0.09 0.65±0.00 0.64±0.05 0.82±0.00 0.72±0.00 0.74±0.07 0.73±0.03 0.72±0.07
APPNP 0.72±0.10 0.72±0.10 0.72±0.10 0.72±0.10 0.72±0.10 0.72±0.10 0.72±0.10 0.72±0.10
APPNP+edrop 0.71±0.05 0.71±0.05 0.71±0.05 0.71±0.05 0.71±0.05 0.71±0.05 0.71±0.05 0.71±0.05
GAT 0.64±0.06 0.64±0.06 0.64±0.06 0.64±0.06 0.64±0.06 0.64±0.06 0.64±0.06 0.64±0.06
GAT+edrop 0.61±0.09 0.61±0.09 0.61±0.09 0.61±0.09 0.61±0.09 0.61±0.09 0.61±0.09 0.61±0.09
GCN2 0.66±0.03 0.66±0.03 0.66±0.03 0.66±0.03 0.66±0.03 0.66±0.03 0.66±0.03 0.66±0.03
GCN2+edrop 0.65±0.10 0.65±0.10 0.65±0.10 0.65±0.10 0.65±0.10 0.65±0.10 0.65±0.10 0.65±0.10
GCN 0.69±0.03 0.69±0.03 0.69±0.03 0.69±0.03 0.69±0.03 0.69±0.03 0.69±0.03 0.69±0.03
GCN+edrop 0.71±0.06 0.71±0.06 0.71±0.06 0.71±0.06 0.71±0.06 0.71±0.06 0.71±0.06 0.71±0.06
GIN 0.73±0.03 0.73±0.03 0.73±0.03 0.73±0.03 0.73±0.03 0.73±0.03 0.73±0.03 0.73±0.03
GIN+edrop 0.56±0.07 0.56±0.07 0.56±0.07 0.56±0.07 0.56±0.07 0.56±0.07 0.56±0.07 0.56±0.07
GraphSAGE 0.46±0.15 0.46±0.15 0.46±0.15 0.46±0.15 0.46±0.15 0.46±0.15 0.46±0.15 0.46±0.15
GraphSAGE+edrop 0.47±0.01 0.47±0.01 0.47±0.01 0.47±0.01 0.47±0.01 0.47±0.01 0.47±0.01 0.47±0.01
Table 6: PR-AUC on graph dataset AIFB. Each column is a comparison of the baseline PR-AUC scores
against the PR-AUC score for our method (first row) applied to a standard hyperGNN architecture. Red
color denotes the highest average score in the column. Orange color denotes a two-way tie in the column,
and brown color denotes a three-or-more-way tie in the column.
PR-AUC↑ HGNN HGNNP HNHN HyperGCN UniGAT UniGCN UniGIN UniSAGE
Ours 0.79±0.11 0.73±0.10 0.73±0.02 0.85±0.07 0.75±0.10 0.84±0.09 0.72±0.03 0.72±0.12
hyperGNN Baseline 0.72±0.07 0.72±0.07 0.72±0.06 0.85±0.05 0.75±0.09 0.84±0.05 0.72±0.07 0.72±0.06
hyperGNN Baseln.+edrop 0.72±0.05 0.72±0.08 0.72±0.06 0.85±0.07 0.73±0.09 0.84±0.06 0.72±0.03 0.72±0.07
APPNP 0.81±0.12 0.81±0.12 0.81±0.12 0.81±0.12 0.81±0.12 0.81±0.12 0.81±0.12 0.81±0.12
APPNP+edrop 0.80±0.05 0.80±0.05 0.80±0.05 0.80±0.05 0.80±0.05 0.80±0.05 0.80±0.05 0.80±0.05
GAT 0.50±0.02 0.50±0.02 0.50±0.02 0.50±0.02 0.50±0.02 0.50±0.02 0.50±0.02 0.50±0.02
GAT+edrop 0.33±0.02 0.33±0.02 0.33±0.02 0.33±0.02 0.33±0.02 0.33±0.02 0.33±0.02 0.33±0.02
GCN2 0.83±0.05 0.83±0.05 0.83±0.05 0.83±0.05 0.83±0.05 0.83±0.05 0.83±0.05 0.83±0.05
GCN2+edrop 0.78±0.04 0.78±0.04 0.78±0.04 0.78±0.04 0.78±0.04 0.78±0.04 0.78±0.04 0.78±0.04
GCN 0.73±0.14 0.73±0.14 0.73±0.14 0.73±0.14 0.73±0.14 0.73±0.14 0.73±0.14 0.73±0.14
GCN+edrop 0.75±0.08 0.75±0.08 0.75±0.08 0.75±0.08 0.75±0.08 0.75±0.08 0.75±0.08 0.75±0.08
GIN 0.73±0.00 0.73±0.00 0.73±0.00 0.73±0.00 0.73±0.00 0.73±0.00 0.73±0.00 0.73±0.00
GIN+edrop 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10
GraphSAGE 0.46±0.15 0.46±0.15 0.46±0.15 0.46±0.15 0.46±0.15 0.46±0.15 0.46±0.15 0.46±0.15
GraphSAGE+edrop 0.47±0.01 0.47±0.01 0.47±0.01 0.47±0.01 0.47±0.01 0.47±0.01 0.47±0.01 0.47±0.01
57Published in Transactions on Machine Learning Research (11/2024)
D Dataset and Hyperparameters
Table 7 lists the datasets and hyperparameters used in our experiments. All datasets are originally from Ben-
son et al. (2018b) or are general hypergraph datasets provided in Sinha et al. (2015); Amburg et al. (2020a).
We list the total number of hyperedges |E|, the total number of vertices |V|, the positive to negative label
ratios for train/val/test, and the percentage of the connected components searched over by our algorithm
that are size atleast 3. A node isomorphism class is determined by our isomorphism testing algorithm. By
Proposition B.2 we can guarantee that if two nodes are in separate isomorphism classes by our isomorphism
tester, then they are actually nonisomorphic.
We use 1024dimensions for all hyperGNN/GNN layer latent spaces, 5layers for all hypergraph/graph neural
networks, and a common learning rate of 0.01. Exactly 2000epochs are used for training.
The hyperGNN architecture baselines are described in the follwoing:
•HGNN Feng et al. (2019) A neural network that generalizes the graph convolution to hypergraphs
where there are hyperedge weights. Its architecture can be described by the following update step
for thel+ 1-layer from the lth layer:
X(l+1)=σ(D−1
2vHWD−1
eHTD−1
2vX(l)W(l)) (121)
whereDv∈Rn×nis the diagonal node degree matrix, De∈Rm×mis the diagonal hyperedge
degree matrix, H∈Rn×mis the star incidence matrix, Wis the diagonal hyperedge weight matrix,
X(l)∈Rn×dis a node signal matrix, W(l)∈Rd×dis a weight matrix, and σis a nonlinear activation.
Following the matrix products, as a message passing neural network, HGNN is GWL-1 based since
the nodes pass to the hyperedges and back.
•HGNNP Feng et al. (2023) is an improved version of HGNN where asymmetry is introduced into the
message passing weightings to distinguish the vertices from the hyperedges. This is also a GWL-1
based message passing neural network. It is described by the following node signal update equation:
X(l+1)=σ(D−1
vHWD−1
eHTX(l)W(l)) (122)
where the matrices are exactly the same as from HGNN.
•HyperGCN Yadati et al. (2019) computes GCN on a clique expansion of a hypergraph. This has an
updateable adjacency matrix defined as follows:
A(l)
i,j=/braceleftigg
1 (i,j)∈E(l)
0 (i,j)/∈E(l)(123)
where
E(l)={(ie,je) =argmaxi,j∈e|X(l)
i−X(l)
j|:e∈E} (124)
X(l+1)
v =σ(/summationdisplay
u∈N(v)([A(l)]v,uX(l)
uW(l))) (125)
TheX(l)∈Rn×dis the node signal matrix at layer l, theW(l)∈Rd×dis the weight matrix at layer
l, andσis some nonlinear activation. This architecture has less expressive power than GWL-1.
•HNHN Dong et al. (2020) This is like HGNN but where the message passing is explicitly broken up
into two hyperedge to node and node to hyperedge layers.
X(l)
E=σ(HTX(l)
VW(l)
E+b(l)
E) (126a)
and
X(l+1)
V =σ(HX(l)
EW(l)
V+b(l)
V) (126b)
whereH∈Rn×mis the star expansion incidence matrix, W(l)
E,W(l)
V∈Rd×d,b(l)
E∈Rm,b(l)
V∈Rn
are weights and biases, X(l)
E,X(l)
Vare the hyperedge and node signal matrices at layer l, andσis a
nonlinear activation function. The bias vectors prevent HNHN from being permutation equivariant.
58Published in Transactions on Machine Learning Research (11/2024)
•UniGNN Huang & Yang (2021) The idea is directly related to generalizing WL-1 GNNs to Hyper-
graphs. Define the following hyperedge representation for hyperedge e∈E:
h(l)
e=1
|e|/summationdisplay
u∈eX(l)
u (127)
–UniGCN: a generalization of GCN to hypergraphs
X(l)
v=1√dv/summationdisplay
e∋v1√
deW(l)h(l)
e (128)
–UniGAT: a generalization of GAT to hypergraphs
αue=σ(aT[X(l)
iW(l);X(l)
jW(l)]) (129a)
˜αue=eαue
/summationtext
v∈eeαve(129b)
X(l+1)
v =/summationdisplay
e∋vαveheW(l)(129c)
–UniGIN: a generalization of GIN to hypergraphs
X(l+1)
v = ((1 +ϵ)X(l)
v+/summationdisplay
e∋vhe) (130)
–UniSAGE: a generalization of GraphSAGE to hypergraphs
X(l+1)
v = (X(l)
v+/summationdisplay
e∋v(he)) (131)
All positional encodings are computed from the training hyperedges before data augmentation. The loss we
use for higher order link prediction is the Binary Cross Entropy Loss for all the positive and negatives sam-
ples. Hypergraphneuralnetworkimplementationsweremostlytakenfrom https://github.com/iMoonLab/
DeepHypergraph , which uses the Apache License 2.0.
Table 7: Dataset statistics and training hyperparameters used for all datasets in scoring all experiments.
Dataset Information
Dataset |E||V|∆+,tr
∆−,tr∆+,val
∆−,val∆+,te
∆−,te% of Conn. Comps. Selected
cat-edge-DAWN 87,104 2,109 8,802/10,547 1,915/2,296 1,867/2,237 0.05%
email-Eu 234,760 998 1,803/2,159 570/681 626/749 0.6%
contact-primary-school 106,879 242 1,620/1,921 461/545 350/415 9.3%
cat-edge-music-blues-reviews 694 1,106 16/19 7/6 3/3 0.14%
cat-edge-vegas-bars-reviews 1,194 1,234 72/86 12/14 11/13 0.7%
contact-high-school 7,818 327 2,646/3,143 176/208 175/205 5.6%
cat-edge-Brain 21,180 638 13,037/13,817 2,793/3,135 2,794/3,020 9.9%
johnshopkins55 298,537 5,163 29,853/35,634 9,329/11,120 27,988/29,853 2.0%
AIFB 46,468 8,083 4,646/5,575 1,452/1,739 4,356/5,222 0.02%
amherst41 145,526 2,234 14,552/17,211 4,547/5,379 16,125/13,643 4.4%
FB15k-237 272,115 14,505 27,211/32,630 8,767/10,509 10,233/12,271 2.1%
WikiPeople-0bi 18,828 43,388 27,211/32,630 10,254/12,301 1,164/1,396 0.05%
JF17K 76,379 28,645 11,907/14,287 1,341/1,608 1,341/1,608 0.6%
We describe here some more information about each dataset we use in our experiments as provided by
Benson et al. (2018b): Here is some information about the hypergraph datasets:
59Published in Transactions on Machine Learning Research (11/2024)
•Amburg et al. (2020a) cat-edge-DAWN : Here nodes are drugs, hyperedges are combinations of
drugs taken by a patient prior to an emergency room visit and edge categories indicate the patient
disposition (e.g., "sent home", "surgery", "released to detox").
•Benson et al. (2018a); Yin et al. (2017); Leskovec et al. (2007) email-Eu: This is a temporal higher-
ordernetworkdataset, whichheremeansasequenceoftimestampedsimplices, orhyperedgeswithall
itsnodesubsetsexistingashyperedges,whereeachsimplexisasetofnodes. Inemailcommunication,
messages can be sent to multiple recipients. In this dataset, nodes are email addresses at a European
research institution. The original data source only contains (sender, receiver, timestamp) tuples,
where timestamps are recorded at 1-second resolution. Simplices consist of a sender and all receivers
such that the email between the two has the same timestamp. We restricted to simplices that consist
of at most 25 nodes.
•Stehlé et al. (2011) contact-primary-school: This is a temporal higher-order network dataset,
which here means a sequence of timestamped simplices where each simplex is a set of nodes. The
dataset is constructed from interactions recorded by wearable sensors by people at a primary school.
The sensors record interactions at a resolution of 20 seconds (recording all interactions from the pre-
vious 20 seconds). Nodes are the people and simplices are maximal cliques of interacting individuals
from an interval.
•Amburg et al. (2020b) cat-edge-vegas-bars-reviews: Hypergraph where nodes are Yelp users
and hyperedges are users who reviewed an establishment of a particular category (different types of
bars in Las Vegas, NV) within a month timeframe.
•Benson et al. (2018a); Mastrandrea et al. (2015) contact-high-school: This is a temporal higher-
order network dataset, which here means a sequence of timestamped simplices where each simplex
is a set of nodes. The dataset is constructed from interactions recorded by wearable sensors by
people at a high school. The sensors record interactions at a resolution of 20 seconds (recording all
interactions from the previous 20 seconds). Nodes are the people and simplices are maximal cliques
of interacting individuals from an interval.
•Crossley et al. (2013) cat-edge-Brain: This is a graph whose edges have categorical edge labels.
Nodes represent brain regions from an MRI scan. There are two edge categories: one for connecting
regions with high fMRI correlation and one for connecting regions with similar activation patterns.
•Lim et al. (2021) johnshopkins55: Non-homophilous graph datasets from the facebook100 dataset.
•Ristoski & Paulheim (2016) AIFB:The AIFB dataset describes the AIFB research institute in terms
of its staff, research groups, and publications. The dataset was first used to predict the affiliation
(i.e., research group) for people in the dataset. The dataset contains 178 members of five research
groups, however, the smallest group contains only four people, which is removed from the dataset,
leaving four classes.
•Lim et al. (2021) amherst41: Non-homophilous graph datasets from the facebook100 dataset.
•Bordes et al. (2013) FB15k-237: A subset of entities that are also present in the Wikilinks database
Singh et al. (2012) and that also have at least 100 mentions in Freebase (for both entities and
relationships). Relationships like ’!/people/person/nationality’ which just reverses the head and tail
compared to the relationship ’/people/person/nationality’ are removed. This resulted in 592,213
triplets with 14,951 entities and 1,345 relationships which were randomly split.
•Guanetal.(2019) WikiPeople-0bi: TheWikidatadumpwasdownloadedandthefactsconcerning
entities of type human were extracted. These facts are denoised. Subsequently, the subsets of
elements which have at least 30 mentions were selected. And the facts related to these elements
were kept. Further, each fact was parsed into a set of its role-value pairs. The remaining facts were
randomly split into training set, validation set and test set by a percentage of 80%:10%:10%. All
binary relations are removed for simplicity. This modifies WikiPeople to WikiPeople-0bi.
60Published in Transactions on Machine Learning Research (11/2024)
•Wen et al. (2016) JF17K: The full Freebase data in RDF format was downloaded. Entities involved
in very few triples and the triples involving String, Enumeration Type and Numbers were removed.
A fact representation was recovered from the remaining triples. Facts from meta-relations having
only a single role were removed. From each meta-relation containing more than 10,000 facts, 10,000
facts were randomly selected.
61Published in Transactions on Machine Learning Research (11/2024)
D.1 Timings
We perform experiments on a cluster of machines equipped with AMD MI100s GPUs and 112 shared AMD
EPYC 7453 28-Core Processors with 2.6 PB shared RAM. We show here the times for computing each
method. The timings may vary heavily for different machines as the memory we used is shared and during
peak usage there is a lot of paging. We notice that although our data preprocessing algorithm involves
seemingly costly steps such as GWL-1, connected connected components etc. The complexity of the entire
preprocessing algorithm is linear in the size of the input as shown in Proposition B.20. Thus these operations
are actually very efficient in practice as shown by Tables 9 and 10 for the hypergraph and graph datasets
respectively. The preprocessing algorithm is run on CPU while the training is run on GPU for 2000 epochs.
Table 8: Timings for our method broken up into the preprocessing phase and training phases ( 2000epochs)
for the hypergraph datasets.
Timings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 2m:45s±108s 35m:9s±13s
HGNNP 1m:52s±0s 35m:16s±0s
HNHN 1m:55s±0s 35m:0s±1s
HyperGCN 1m:50s±0s 58m:17s±79s
UniGAT 1m:54s±0s 1h:19m:34s±0s
UniGCN 1m:50s±2s 35m:19s±2s
UniGIN 1m:50s±1s 35m:12s±1288s
UniSAGE 1m:51s±0s 35m:16s±0s
(a)cat-edge-DAWNTimings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 1.72s±5s 2m:11s±11s
HGNNP 1.42s±0s 2m:10s±0s
HNHN 1.99s±0s 3m:43s±2s
HyperGCN 1.47s±2s 4m:12s±3s
UniGAT 1.85s±0s 3m:54s±287s
UniGCN 2.93s±0s 3m:15s±19s
UniGIN 2.24s±0s 3m:17s±18s
UniSAGE 2.04s±0s 3m:13s±3s
(b)cat-edge-music-blues-reviews
Timings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 4.17s±0s 2m:34s±1954s
HGNNP 4.54s±0s 2m:41s±53s
HNHN 3.06s±0s 2m:27s±15s
HyperGCN 1.81s±1s 2m:27s±0s
UniGAT 1.91s±0s 2m:27s±306s
UniGCN 2.84s±0s 2m:30s±72s
UniGIN 3.20s±0s 2m:27s±1189s
UniSAGE 1.65s±0s 2m:27s±0s
(c)cat-edge-vegas-bars-reviewsTimings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 5.84s±1s 6m:49s±8s
HGNNP 5.82s±0s 9m:8s±19s
HNHN 5.95s±0s 8m:21s±19s
HyperGCN 5.74s±0s 10m:16s±1s
UniGAT 8.80s±0s 2m:31s±282s
UniGCN 6.35s±0s 6m:9s±957s
UniGIN 5.99s±0s 10m:41s±43s
UniSAGE 5.97s±0s 9m:50s±0s
(d)contact-primary-school
Timings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 23.25s±1s 25m:41s±17s
HGNNP 23.25s±0s 19m:52s±49s
HNHN 24.27s±1s 5m:12s±63s
HyperGCN 24.00s±0s 21m:16s±0s
UniGAT 14.27s±0s 5m:13s±243s
UniGCN 25.44s±0s 5m:51s±1019s
UniGIN 13.71s±1s 19m:10s±3972s
UniSAGE 14.08s±2s 36m:29s±5s
(e)email-EuTimings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 4.89s±6s 1m:27s±8s
HGNNP 2.12s±0s 2m:42s±30s
HNHN 2.12s±0s 2m:39s±42s
HyperGCN 2.11s±0s 40.11s±3s
UniGAT 2.13s±0s 3m:18s±8s
UniGCN 2.11s±0s 3m:21s±2s
UniGIN 2.11s±0s 2m:24s±70s
UniSAGE 2.11s±0s 2m:8s±49s
(f)cat-edge-madison-restaurant-reviews
62Published in Transactions on Machine Learning Research (11/2024)
Table 9: Timings for our method broken up into the preprocessing phase and training phases ( 2000epochs)
for the hypergraph datasets.
Timings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 15.11s±4s 4m:59s±1s
HGNNP 12.72s±0s 2m:29s±0s
HNHN 12.17s±0s 3m:6s±0s
HyperGCN 12.47s±0s 49.25s±0s
UniGAT 12.74s±0s 2m:1s±1s
UniGCN 12.50s±0s 2m:29s±3s
UniGIN 12.57s±0s 2m:16s±3s
UniSAGE 12.67s±0s 1m:50s±29s
(a)contact-high-schoolTimings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 11.34s±10s 4m:24s±6s
HGNNP 6.02s±0s 4m:13s±2s
HNHN 6.01s±0s 5m:31s±1s
HyperGCN 6.32s±0s 1m:33s±0s
UniGAT 6.04s±0s 4m:11s±0s
UniGCN 5.79s±0s 4m:12s±0s
UniGIN 6.64s±1s 3m:4s±1s
UniSAGE 5.79s±0s 3m:2s±0s
(b)cat-edge-Brain
Timings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 3m:30s±5s 1h:29m:33s±6s
HGNNP 3m:34s±1s 1h:48m:57s±1s
HNHN 3m:41s±1s 2h:9m:34s±1s
HyperGCN 3m:24s±1s 58m:27s±1s
UniGAT 3m:50s±1s 4h:21m:24s±1s
UniGCN 3m:38s±1s 29m:14s±1s
UniGIN 3m:50s±1s 27m:50s±1s
UniSAGE 3m:41s±1s 27m:22s±1s
(c)WikiPeople-0biTimings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 8m:11s±52s 37m:18s±9s
HGNNP 7m:34s±1s 47m:56s±1s
HNHN 6m:21s±1s 49m:33s±1s
HyperGCN 8m:20s±1s 28m:25s±1s
UniGAT 10m:40s±1s 1h:54m:36s±1s
UniGCN 7m:25s±1s 2h:40m:20s±1s
UniGIN 10m:37s±1s 2h:48m:35s±1s
UniSAGE 6m:58s±1s 2h:35m:4s±1s
(d)JF17K
63Published in Transactions on Machine Learning Research (11/2024)
Table 10: Timings for our method broken up into the preprocessing phase and training phases ( 2000epochs)
for the graph datasets.
Timings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 11m:14s±75s 53m:21s±2845s
HGNNP 11m:10s±21s 1h:34m:25s±35s
HNHN 5m:15s±395s 1h:35m:15s±419s
HyperGCN 33.98s±0s 5m:8s±0s
UniGAT 1m:59s±120s 2h:2m:47s±25s
UniGCN 34.37s±0s 1h:17m:38s±2s
UniGIN 34.05s±0s 1h:16m:38s±7s
UniSAGE 34.36s±0s 1h:16m:34s±3s
(a)johnshopkins55Timings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 17m:9s±164s 12m:38s±549s
HGNNP 15m:34s±61s 20m:26s±124s
HNHN 15m:31s±83s 18m:11s±30s
HyperGCN 15m:46s±32s 4m:17s±80s
UniGAT 1m:27s±6s 16m:30s±0s
UniGCN 15m:57s±24s 18m:42s±170s
UniGIN 16m:14s±73s 16m:22s±39s
UniSAGE 8m:42s±610s 8m:49s±324s
(b)AIFB
Timings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 4m:1s±11s 22m:30s±1177s
HGNNP 3m:53s±4s 39m:30s±3s
HNHN 3m:16s±22s 44m:7s±71s
HyperGCN 3m:35s±23s 5m:22s±25s
UniGAT 11.92s±0s 1h:51m:53s±123s
UniGCN 3m:20s±6s 39m:18s±51s
UniGIN 3m:21s±8s 38m:3s±0s
UniSAGE 11.27s±0s 58m:48s±956s
(c)amherst41Timings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 3m:32s±9s 1h:19m:5s±4684s
HGNNP 3m:26s±10s 2h:19m:44s±3586s
HNHN 3m:27s±0s 1h:55m:48s±22s
HyperGCN 3m:28s±0s 10m:31s±18s
UniGAT 3m:24s±5s 3h:50m:24s±91s
UniGCN 3m:19s±4s 1h:39m:46s±13s
UniGIN 3m:17s±0s 1h:36m:47s±35s
UniSAGE 3m:25s±13s 1h:37m:16s±102s
(d)FB15k-237
64