Under review as submission to TMLR
SE3Set: Harnessing Equivariant Hypergraph Neural Networks
for Molecular Representation Learning
Anonymous authors
Paper under double-blind review
Abstract
In this paper, we develop SE3Set, an SE(3) equivariant hypergraph neural network ar-
chitecture tailored for advanced molecular representation learning. Hypergraphs are not
merely an extension of traditional graphs; they are pivotal for modeling high-order rela-
tionships, a capability that conventional equivariant graph-based methods lack due to their
inherent limitations in representing intricate many-body interactions. To achieve this, we
first construct hypergraphs via proposing a new fragmentation method that considers both
chemical and three-dimensional spatial information of the molecular system. We then design
SE3Set, which incorporates equivariance into the hypergraph neural network. This ensures
that the learned molecular representations are invariant to spatial transformations, thereby
providing robustness essential for accurate prediction of molecular properties. SE3Set has
shown performance on par with state-of-the-art (SOTA) models for small molecule datasets
like QM9 and MD17. It excels on the MD22 dataset, achieving a notable improvement
of approximately 20% in accuracy across all molecules, which highlights the prevalence of
complex many-body interactions in larger molecules. This exceptional performance of SE3Set
across diverse molecular structures underscores its transformative potential in computational
chemistry, offering a route to more accurate and physically nuanced modeling.
1 Introduction
Molecular representation (Mathews & Chaffee, 2012; David et al., 2020; Wigh et al., 2022) is pivotal for
cheminformatics (Fourches et al., 2010), impacting the prediction of molecular properties in drug discovery
and material science. Traditional descriptors like fingerprints capture basic structural and energetic aspects of
molecules by considering mainly one- and two-body interactions. However, they often miss complex electronic
correlations and collective behaviors important for understanding phenomena such as chemical reactivity
and protein folding. To address this, advanced methods that include many-body interactions are crucial
for a more comprehensive molecular characterization. These methods enhance the predictive capabilities
of computational models by more accurately reflecting the intricate dynamics and properties of molecules,
which are essential for a deeper understanding of their functionality and reactivity in cheminformatics.
Graph neural networks (GNNs) (Zhou et al., 2020; Wu et al., 2020) are a foundational tool for representing
structured data in molecular sciences with atoms as nodes and chemical bonds as edges, respectively. GNN
models excel in tasks ranging from property prediction to reaction simulation (Do et al., 2019; Xiong
et al., 2021; Reiser et al., 2022). GNNs can capture higher-order molecular interactions through message
passing (Gilmer et al., 2017) but face overfitting and inefficiency challenges (Godwin et al., 2021; Rusch et al.,
2023). Architectural improvements in GNNs facilitate the modeling of complex interactions, overcoming
some limitations of deep networks (Gasteiger et al., 2019; Schütt et al., 2021; Batzner et al., 2022). Advances
demonstrate the potential of architectural enhancements in GNNs to represent complex interactions (Gasteiger
et al., 2020; 2021; Thölke & De Fabritiis, 2021; Batatia et al., 2022; Musaelian et al., 2023; Wang et al., 2024),
but efficiently integrating many-body interactions into these networks is an ongoing challenge (Wang et al.,
2023).
1Under review as submission to TMLR
To address the complexities of many-body interactions in molecular systems, hypergraphs offer a compelling
alternative to complex GNN architectures. Hypergraphs, with hyperedges connecting multiple vertices, can
naturally represent many-body phenomena like electronic delocalization and collective vibrations. This
allows for a more accurate modeling of molecular intricacies beyond the limitations of traditional graphs.
Integrating hypergraphs with machine learning, particularly through Hypergraph Neural Networks (HGNNs),
is an emerging research area. HGNNs manage the flow of information across hyperedges, capturing complex
multi-atom interactions and enriching molecular representations. This technique promises to balance model
expressiveness with computational efficiency. By innately encoding many-body interactions, HGNNs stand
to significantly advance cheminformatics, offering a new approach to molecular property prediction and
simulation that resonates with the actual behavior of chemical systems.
In this work, we introduce SE3Set, an innovative approach that enhances traditional GNNs by exploiting
hypergraphs for modeling many-body interactions, while ensuring SE(3) equivariant representations that
remain consistent regardless of molecular orientation. Our key contributions are:
•A new fragmentation method for hypergraph construction that seamlessly integrates 2D chemical
and 3D spatial information, enriching the molecular structure representation.
•The deployment of hypergraph neural networks to capture many-body interactions, providing a
deeper insight into molecular behavior that surpasses conventional pairwise modeling.
•The incorporation of SE(3) equivariance within our hypergraph framework, guaranteeing orientation-
independent molecular representations.
•SE3Set underwent a comprehensive benchmarking process, exhibiting comparable outcomes to
state-of-the-art (SOTA) models on small molecule datasets QM9 and MD17. It demonstrated
exceptional performance on the larger molecule dataset MD22, where higher-order interactions are
more evident, surpassing SOTA models with a significant reduction in mean absolute errors (MAEs)
by an average of roughly 20%. This confirms SE3Set’s efficacy in capturing the complexity of
molecular representations.
These advances establish SE3Set as a formidable tool for molecular representation learning, with implications
for computational chemistry and beyond.
2 Related works
2.1 Graph neural networks
Message passing neural networks (MPNNs), a class of graph neural networks, are essential for learning node
features by transmitting information along graph edges, a process crucial for interpreting structured data
like molecules (Gilmer et al., 2017). Equivariant GNNs are especially important for molecular modeling.
They adopt either group representation methods, aligning architectures to symmetry groups for improved
interaction modeling (Thomas et al., 2018; Anderson et al., 2019; Fuchs et al., 2020; Batzner et al., 2022; Liao
& Smidt, 2022; Liao et al., 2023; Musaelian et al., 2023), or direction-based methods that incorporate spatial
information for accurate molecular representations (Schütt et al., 2017; Kindermans & Müller, 2018; Coors
et al., 2018; Gasteiger et al., 2019; 2020; Schütt et al., 2021; Thölke & De Fabritiis, 2021; Gasteiger et al.,
2021; Wang et al., 2022; Du et al., 2024; Aykent & Xia, 2024; Wang et al., 2024) and have been engineered to
handle intricate up to five-body interactions (Wang et al., 2023).
2.2 Hypergraph neural networks
Hypergraph Neural Networks (HGNNs) enhance GNNs by incorporating multi-node hyperedges, better
capturing complexity in data from various domains. They advance GNNs’ implicit many-body interactions
with methods like clique expansion for compatibility with existing algorithms (Agarwal et al., 2005; Zhou et al.,
2006) and employ tensor techniques for improved hypergraph-based feature learning (Li et al., 2013; Pearson
2Under review as submission to TMLR
& Zhang, 2014; Benson et al., 2017; Chien et al., 2021a; Tudisco et al., 2021). While equivariant HGNNs
adeptly handle node permutations, preserving data symmetries (Kim et al., 2021; 2022a), they often miss
3D spatial transformations, crucial for physical system modeling. In computational chemistry, hypergraph
algorithms simulate complex behaviors and optimize molecules through hypergraph grammar (Cui et al., 2023;
Tavakoli et al., 2022; Kajino, 2019), providing multidimensional insights into molecular structures (Nachmani
& Wolf, 2020; Chen et al., 2021; Chen & Schwaller, 2023). Despite their promise, these methods still face
hurdles in integrating spatial information effectively.
2.3 Fragmentation methods
Fragmentation methods break down complex molecules for simpler ab initio QM computations of properties,
later combining these for a holistic view (Gordon et al., 2012; Collins & Bettens, 2015). Leveraging the
localized nature of chemical reactions, these techniques aim for scalable algorithms suitable for large molecule
analysis. While instrumental in computational pretraining (Du et al., 2021; Kim et al., 2022b; Luong &
Singh, 2023), they typically neglect the fusion of 2D structural with 3D spatial data. Hence, we advocate for
a refined fragmentation approach that merges chemical properties with spatial context, potentially advancing
hypergraph-based chemical modeling.
3 Preliminaries
3.1 Equivariance
Consider a function Lthat maps inputs from space Xto outputs in space Y.Lis calledG-equivariant if it
preserves the symmetry of a group Gacross mappings, meaning for each g∈G, we have:
L◦DX(g) =DY(g)◦L, (1)
whereDXrepresents the group G’s action onX. This ensures that the function Lreflects changes made to
inputs byGin its outputs.
3.2 Hypergraph
Hypergraphs elegantly capture the essence of higher-order interactions among multiple entities, making them
an invaluable tool for representing complex relational data. Let G= (V,E)be a hypergraph with Nvertices
andMhyperedges, where Vrepresents a set of nodes and Eis a set of hyperedges. Distinguishing itself from
a traditional graph, a hyperedge can encompass multiple nodes, not limited to two, i.e. each hyperedge e∈E
is a non-empty subset of V.
3.3 AllSet
The AllSet framework (Chien et al., 2021b), an advanced HGNN model, addresses heuristic propagation rule
limitations in HGNNs by integrating Deep Sets (Zaheer et al., 2017) and Set Transformers (Lee et al., 2019)
principles. It uses task-optimized dual multiset functions that maintain permutation invariance, crucial for
hypergraph learning. The update rules in AllSet are:
Z(t+1),v
e,: =fV→E(Ve\v,X(t);Z(t),v
e,:,X(t)
v,:), (2)
X(t+1)
v,:=fE→V(Ev,Z(t+1),v;X(t)
v,:). (3)
Here,fV→EandfE→Vare the key multiset functions mapping node and hyperedge features. For example,
fV→E(S) =MLP/parenleftbig/summationtext
s∈SMLP (s)/parenrightbig
is used in AllDeepSets. The notation Ve,XandEv,Zrepresent multisets
of node and hyperedge features, respectively. The AllSet approach updates nodes and hyperedges in the
hypergraph by leveraging their features in conjunction with those of adjacent hyperedges or nodes, enabling
a rich representation of the hypergraph structure. The method could differentiate node vfrom its multiset,
allowing for sophisticated feature aggregation.
3Under review as submission to TMLR
4 Methods
We introduce the SE3Set model to leverage hypergraph neural networks for capturing complex molecular
interactions, integrating both 2D chemical and 3D spatial structures (Sec. 4.1). It builds upon the AllSet
framework (Chien et al., 2021b) and the Equiformer (Liao & Smidt, 2022). Upcoming sections will delve into
the specifics of molecular fragmentation and the SE3Set architecture.
HNN
NN
H
N
H
N
OOHO
OHOH2N
OHNN
NN
H
N
H
N
OOHO
OHOH2N
O
HNN
NN
H
N
H
N
OOHO
OHOH2N
OHNN
NN H2N
O
HNN
NN
H
N
OH
N
H
N
O
H
N
OOHO
OHOH
N
OOHOHNN
NN
H
N
H
N
OOHO
OHOH2N
O	a
 	b
	c
 	d
Figure 1: Folic acid fragmentation illustrated with CID 135398658 from PubChem. (a) Preprocessing to
identify cleavable bonds for fragmentation. (b) Initial fragments formed using BFS, color-coded by functional
groups (blue), rings (orange), and single atoms (green). (c) Fragments merged to satisfy atom count criteria,
detailed in D. (d) Expansion of fragments shown with directional arrows.
4.1 Fragmentation algorithm
To harness the power of hypergraph neural networks for molecular representations, we need to map molecules
onto hypergraph structures through a refined fragmentation algorithm. Our strategy intertwines molecular
topologyandspatialgeometrytocreatehyperedgesthatcapturegroupsofatoms, reflectingtheirfunctionaland
spatial characteristics. In crafting this fragmentation approach for hypergraph-based molecular representation,
the methodology must adhere to a set of fundamental principles:
1.The design should merge topological chemistry with 3D structural data into a unified hypergraph
representation, ensuring hyperedges accurately embody the molecule’s chemical and spatial properties.
2.Controlling fragment size is vital for the SE3Set model to balance capturing meaningful interactions
and computational efficiency. Optimal fragment sizes are key for model performance and learning
capabilities.
3.The fragmentation could only selectively break single bonds and must maintain functional groups
and ring integrity to preserving key chemical information critical for the molecule’s properties and
behavior.
4Under review as submission to TMLR
4.Fragment overlap is essential to maintain functional group effects on local charge distribution and to
ensure hyperedge interaction within the SE3Set model for improved molecular learning.
Before delving deeper into the specifics of our fragmentation method, it’s important to establish a foundational
understanding through key definitions and concepts,
Definition 4.1. The bond order represents the multiplicity or the number of shared electron pairs that
constitute a covalent bond between two atoms.
The bond order matrix Bis anN×Nrepresentation of bond strength between atoms in a molecule, with
higher bond order values indicating stronger bonds. This symmetric matrix ( Bij=Bji) is crucial for studying
molecular structure and reactivity, capturing bond nuances including delocalized and resonance bonds in
computational chemistry.
Our fragmentation algorithm improves molecular representations by combining bond order, functional groups,
and substructures, including SMARTS-identified smaller rings and merged adjacent groups. Overcoming
the drawbacks of non-overlapping fragmentation, it uses 3D spatial data and allows overlaps, preserving
local effects for precise charge distribution and enhancing hypergraph neural network learning of molecular
interactions.
Definition 4.2. A molecular fragment, denoted as F, is defined as a specific subset of atoms within a molecule,
characterized by being a cohesive assembly of predefined substructures linked in a sequential concatenation.
Our fragmentation method meticulously dissects a given molecule into meaningful subsets of atoms, and this
process unfolds through four steps (corresponding to the pipeline in Fig. 1):
1.Pre-processing by analyzing the molecule’s bond order matrix to mask high-order bonds and those
within functional groups or rings, and merging adjacent functional groups for a streamlined structural
representation.
2.Core substructures are delineated from the remaining bonds using a Breadth-First Search algorithm,
establishing the basic units of the molecular framework.
3.These substructures are then aggregated into larger molecular fragments according to predefined
rules that maintain a minimum atom count within each fragment. This step could be optional.
4.To enhance fragment connectivity, we expand each by incorporating adjacent groups, using interaction
strength metrics based on interatomic distances to guide this process. Here we set the cutoff value
denoted as cwof an interaction strength metrics to intercept the extended fragment.
Furthermore, step 4 leads to a substantial computational overhead for hypergraph neural networks when
processing larger molecular systems. To enhance the efficiency of our model for such expansive molecular
systems, we introduce a revised strategy for step 4:
4*For each atom i, identify the neighboring atoms Nithat fall within a specified radial cutoff rc. A
fragment Fis considered to be adjacent to atom iif there is an overlap of at least one atom between
FandNi. For ease of reference, the set of fragments adjacent to atom iis represented asNF
i, which
implies thatNF
i={F|F∩Ni̸=∅}.
We designate the application of step 4 as an explicitoverlapand the application of step 4* as an implicit
overlap. These approaches introduce nuanced variations in the mathematical expressions of our model, as
reflected in Eq. 8, Eq. 9, and Eq. 16. For the detailed step-by-step methodology, please refer to Appendix B.
4.2 SE3Set
Building upon our aforementioned fragmentation algorithm, we now turn to outline the architecture of SE3Set.
The SE3Set model, influenced by AllSet (Chien et al., 2021b) and built on the Equiformer (Liao & Smidt,
5Under review as submission to TMLR
V2E Module
Layer NormEmbedding
E2V Module
Output HeadFeed Forward(a)
Linear Linear Linear
Linear
RescaleLinear
Linear
Rescale(b){1}(c)
Linear Linear
Linear
Attenion Module
LinearReshape
LeakyReLU Gate
Linear
Linear Softmax
Reshape
(d) 
(e) 
Linear
LinearLinear
Attenion Module(f) Linear Linear Gate
(g) 
Linear
LinearLinearLinear
Reshape
LeakyReLU Gate
Linear Linear
Softmax
Reshape
Feed Forward
Layer NormFeed ForwardLayer Norm Layer Norm
Layer NormLayer Norm
Layer NormFeed Forward
Figure 2: Overall architecture of SE3Set. (a) SE3Set begins with node and hyperedge embeddings, cycles
through V2E and E2V attention modules for iterative updates, and concludes with normalization and a
feed-forward block for output. (b) Embedding. Atomic numbers and position vectors are transformed into
initial embeddings for nodes and hyperedges. (c) Attention Block. Merges feature sets with positional or
hyperedge data for feature processing. (d) Feed-Forward Block. Enhances feature sets through a streamlined
network. (e) V2E Module. Utilizes node features and their relative positions to update hyperedge features. (f)
E2V Module. Employs hyperedge features to refresh node features, using tensor products (left) or summation
(right) for updates. Symbols ⊗,⊕, and⊙in figures denote depth-wise tensor product, summation, and
Hadamard multiplication, respectively. hα
irepresents hyperedge features, xiis for node features, superscript
nindicates the number of updates, and ⃗ rijis the relative position vector between nodes iandj.
6Under review as submission to TMLR
2022), incorporates 3D spatial equivariance (proof refers to Appendix F) in our hypergraph neural network,
improving the capture of many-body interactions for precise molecular structure representation. SE3Set
consists of an embedding layer, attention blocks, and an output head, as shown in Fig. 2 (a). Moreover, from
a simple example, we give a brief discussion at Appendix M to the advantages of the hypergraph structure
we implement when considering message passing efficiency, comparing to the ordinary graph neural networks.
4.2.1 Embedding
As depicted in Fig. 2 (b), the embedding block generates detailed node and hyperedge features reflecting
molecular structures. Node features blend intrinsic properties with degree embeddings from connected
hyperedges, while hyperedge features aggregate node embeddings and the corresponding relative position
vectors depending on the node on which the hyperedge feature is located. SH (Spherical Harmonic) functions
are used to project normalized relative position vectors ⃗ rijbetween node iandjinto the irreducible
representations (irreps) feature space with different order l, i.e.SH(⃗ rij) =Yl/parenleftig
⃗ rij
∥⃗ rij∥/parenrightig
. The features are also
mapped onto the same l-order irreps space for SE3 equivariance and updated separately. Hyperedges capture
nodes’ positional relationships, assigning a distinct feature hα
ito each node iin hyperedge Fα, reinforcing
structural fidelity. Nodes xiintegrate hyperedge information, harmonizing uniqueness with interconnections.
Attention mechanisms then refine node and hyperedge interactions for accurate molecular and structural
representation.
4.2.2 Equivariant hypergraph attention blocks
As presented in Fig. 2 (c)-(f), the attention mechanism comprises two essential components: the Vertex-to-
Edge (V2E) and Edge-to-Vertex (E2V) attention blocks, based on the AllSet framework (Chien et al., 2021b).
The V2E block refines hyperedge features, while the E2V block updates node features, both operating with an
equivariant hypergraph attention mechanism. To improve training and enable deeper network structures, we
incorporate normalization layers and residual connections to prevent gradient issues. The attention module’s
output passes through a feed-forward block (Fig. 2 (d)), enhancing representation complexity. Node and
hyperedge features maintain equivariance to molecular geometry, preserving data symmetries and the integrity
of representations, thus boosting the model’s expressiveness in capturing complex structural interactions.
(The concepts of irreducible representations and tensor products can be referenced in the Appendix G.)
V2E attention The SE3Set model uses geometrically invariant attention weights aij, derived from l= 0
irreps acting as scalars under geometric transformations. These weights are computed from scalar features
fij,l=0using an MLP with LeakyReLU activation and softmax normalization, reflecting node relationships
within the hypergraph. Node and hyperedge features undergo non-linear transformations represented by
tensor products of irreps with quantum number l. The features combine through direct tensor products
(DTP), yielding non-linear values vij(Fig. 2 (c)). Hyperedge features are updated by aggregating features
from connected nodes, utilizing SH and radial basis functions on hyperedge features. The model calculates
initial features fα
ijand V2E attention weights aα
ijvia MLPs, with non-linear values vα
ijemerging from similar
transformations.
tα
ij= (Linear (xi) +Linear (xj)) (4)
fα
ij=Linear (tα
ij⊗DTP
w(∥⃗ rij∥)SH(⃗ rij)) (5)
aα
ij=Softmaxj(a⊤LeakyReLU (fα
ij,l=0)) (6)
vα
ij=Linear (Gate (fα
ij)⊗DTP
w(∥⃗ rij∥)SH(⃗ rij)) (7)
Ultimately, the SE3Set model updates hyperedge features hk
iby accumulating the weighted features of nodes
within the same hyperedge and applying a linear transformation to the aggregated information. For the
explicitoverlap fragmentation method,
∆hα
i=Linear
/summationdisplay
j:ni∈Fα∧nj∈Fαaα
ijvα
ij
 (8)
7Under review as submission to TMLR
wherenidenotes the node with index iandFαdenotes the fragment with index αas each fragment could be
considered as a hyperedge in the hypergraph. Due to the frequent occurrence of a high number of explicitly
overlapping atoms, this scenario commonly results in increased computational complexity. Consequently,
when adopting the implicitoverlap approach, we may opt for an equation of the form:
∆hα
i=Linear
/summationdisplay
j:j∈Fα∧Fα∈NF
iaα
ijvα
ij
 (9)
whereNF
iis delineated in step 4* of the implicitoverlapmethod. This characteristic renders it a more
computationally efficient scheme for Vertex-to-Edge (V2E) attention mechanisms. The detailed architecture
of the V2E attention block is shown in Fig. 2 (e).
E2V attention Following the V2E attention module, the E2V attention module (Fig. 2 (f)) updates node
features by transforming them with a tensor product of the updated hyperedge feature, followed by a linear
layer. Attention weights are then calculated using softmax-applied, LeakyReLU-activated features, ensuring
node features are refined after hyperedge updates.
fα
i=Linear ((Linear (xi)⊗DTPhα
i) (10)
aα
i=Softmaxα(a⊤LeakyReLU (fα
i,l=0)) (11)
These attention weights direct the synthesis of information, culminating in the calculated value:
vα
i=Linear (Gate (fα
i)⊗DTPhα
i). (12)
Furthermore, we propose an alternative method for constructing the E2V attention block as shown in Fig. 2
(g).
fα
i=Linear (Linear (xi) +Linear (hα
i)) (13)
aα
i=Softmaxα(a⊤LeakyReLU (fα
i,l=0)) (14)
vα
i=Linear (Gate (fα
i)) (15)
However, practical experiments reveal that the previous method yields superior results, with detailed findings
presented in Sec. 5.3.
Then the node aggregates all the hyperedge features corresponding to itself to update the node feature,
Explicit overlap: ∆xi=Linear/parenleftigg/summationdisplay
α:i∈Fαaα
ivα
i/parenrightigg
, (16)
Implicit overlap: ∆xi=Linear
/summationdisplay
α:Fα∈NF
iaα
ivα
i
 (17)
4.2.3 Output head
The SE3Set model employs node features to generate predictions, using a feed-forward network to transform
these features into the target label’s irreps dimension. A summation strategy aggregates node features into a
single hypergraph-level representation, which is then processed by a linear layer to output the model’s final
predictions.
5 Results
We tested our equivariant hypergraph neural network on QM9 (Ruddigkeit et al., 2012; Ramakrishnan et al.,
2014), MD17 (Chmiela et al., 2017) (see Appendix H), MD22 (Chmiela et al., 2023), and OE62 (Stuke
8Under review as submission to TMLR
Table 1: A comparative analysis was performed to assess the Mean Absolute Errors (MAEs) on the QM9
dataset when training SE3Set on a configuration comprising 110,000 training samples and 1,000 validation
samples. Bolding shows the best model and underlining shows the second best model and the underlining
tilde shows third best model. All the baseline results are extracted from the origin papers (Schütt et al.,
2017; Gasteiger et al., 2020; Schütt et al., 2021; Coors et al., 2018; Wang et al., 2022; Thölke & De Fabritiis,
2021; Musaelian et al., 2023; Wang et al., 2024; Liao & Smidt, 2022; Wang et al., 2023).
unit SchNet DimeNet++ PaiNN SphereNet ComENet ET Allegro ViSNet QuinNet Equiformer SE3Set
µ D 0.033 0.030 0.012 0.026 0.0245 0.011 - 0.010 0.771 0.011 0.011
α a3
0 0.235 0.044::::0.045 0.046 0.0452 0.059 - 0.041 0.047 0.046:::::0.045
HOMO meV 41 25 20 23 23 20 -:::17.3 20.4 15 15
LUMO meV 34 20 28 18 20 18 -:::14.8 17.6 14 13
gap meV 63 33 46 32 32 36 - 31.7 28.2::30 29
R2a2
0 0.073 0.331::::0.066 0.292 0.259 0.033 - 0.030 0.194 0.251 0.197
ZPVE meV 1.70:::1.21 1.28 1.12 1.20 1.84 - 1.56 1.26 1.26 1.40
U0 meV 14 6 5.85 6 6.59 6.15 4.7 4.23 7.6 6.59:::5.74
U meV 19 6 5.83 7 6.82 6.38 4.4 4.25 8.4 6.74:::5.69
H meV 14 7 5.98 6 6.86 6.16 4.4 4.52 7.8 6.63:::5.70
G meV 14 8 7.35 8 7.98 7.62 5.7 5.86 8.5 7.63:::6.63
Cvkcal
mol·K0.033 0.023 0.024 0.021 0.024 0.026 - 0.023 0.024 0.023 0.025
et al., 2020) (see Appendix I) to assess its molecular representation learning. QM9 and MD17 gauge small
molecule property prediction, while MD22 and OE62 evaluate larger systems with complex many-body
interactions (Wang et al., 2023). An ablation study was also conducted to pinpoint the contributions of
fragmentation and architecture to our method’s performance, offering insights into the network’s efficacy
and areas for enhancement. In the Appendix J, we have also provided a detailed analysis of the model’s
complexity. Additionally, we have evaluated the robustness of our model to different tasks by some summary
statistic metrics in Appendix N.
5.1 QM9
The QM9 dataset (Ruddigkeit et al., 2012; Ramakrishnan et al., 2014) consists of 134k small organic molecules
calculated at the B3LYP/6-31G(2df, p) level. SE3Set, after training on 110k QM9 molecules and validation
on 10k, achieves low mean absolute errors (MAEs) in 12 tasks, performing on par with leading models,
as detailed in Table 1. For completeness, some pre-trained molecular representation learning model are
also selected as baselines to give a comparison (see Appendix L). In small molecular systems, higher-order
many-body interactions are less pronounced, and as a result, SE3Set does not significantly outperform other
state-of-the-art (SOTA) models.
Table 2: A comparison of Mean Absolute Errors (MAEs) across various benchmarked models. SE3Set is
trained on the five molecules of MD22 dataset with specific number of training/validation. Bolding shows the
best model and underlining shows the second best model. The improvements column shows the improvement
of our model over the previous SOTA model in percentage terms. The MAEs reflect the precision of energy
predictions in units of kcal/mol and forces in units of kcal/(mol ·Å). The results of the baseline models refs to
the origin papers (Chmiela et al., 2023; Thölke & De Fabritiis, 2021; Musaelian et al., 2023; Batatia et al.,
2022; Liao & Smidt, 2022; Wang et al., 2024; 2023; Li et al., 2024).
Molecule # Train/Val sGDML TorchMD-NET Allegro MACE Equiformer ViSNet QuinNet Equiformer-LSRM ViSNet-LSRM SE3Set Improvements
Ac-Ala3-NHMe 5500/500Energy 0.3902 0.1121 0.1019 0.0620 0.0828 0.0796 0.084 0.0780 0.0654 0.0499 19.5%
Force 0.7968 0.1879 0.1068 0.0876 0.0804 0.0972 0.0681 0.0877 0.0902 0.0545 20.0%
DHA 7500/500Energy 1.3117 0.1205 0.1153 0.1317 0.1788 0.1526 0.12 0.0878 0.0873 0.0826 5.4%
Force 0.7474 0.1209 0.0732 0.0646 0.0506 0.0668 0.0515 0.0534 0.0598 0.0360 28.9%
Stachyose 7500/500Energy 4.0497 0.1393 0.2485 0.1244 0.1404 0.1283 0.23 0.1252 0.1055 0.0762 27.8%
Force 0.6744 0.1921 0.0971 0.0876 0.0635 0.0869 0.0543 0.0632 0.0767 0.0424 21.9%
AT-AT 2500/500Energy 0.7235 0.1120 0.1428 0.1093 0.1309 0.1688 0.14 0.1007 0.0772 0.0585 24.2%
Force 0.6911 0.2036 0.0952 0.0992 0.0960 0.1070 0.0687 0.0811 0.0781 0.0556 19.1%
AT-AT-CG-CG 1500/500Energy 1.3885 0.2072 0.3933 0.1578 0.1510 0.1995 0.38 0.1335 0.1135 0.1002 11.7%
Force 0.7028 0.3259 0.1280 0.1153 0.1252 0.1563 0.1273 0.1065 0.1063 0.0825 22.4%
9Under review as submission to TMLR
Table 3: The MAE for energy (unit: kcal/mol ) and forces (unit: kcal/mol·Å) on the AT-AT-CG-CG dataset
using 3 layers SE3Set with different cutoff radii ( rc) in the implicit overlap method.
Implicitrc(Å) 4.0 5.0 6.0
Energy 0.2123 0.1153 0.1103
Force 0.1559 0.1019 0.0937
5.2 MD22
Recognizing the prominence of higher-order many-body interactions in larger molecules (Wang et al., 2023),
SE3Set was tested on the comprehensive MD22 dataset (Chmiela et al., 2023). This dataset spans four classes
of biomolecules and supramolecules, from a 42-atom peptide to a 370-atom nanotube, with high-resolution
sampling at 400-500 K using the PBE+MBD (Perdew et al., 1996; Tkatchenko et al., 2012) framework for
energy and force computations. Our fragmentation method, which maintains functional groups and rings,
selectively excludes structures like the Buckyball catcher and Double-walled nanotube from MD22, thus
concentrating on the other five molecular types. We partition the training/test set following QuinNet (Wang
et al., 2023). As Table 2 shows, SE3Set outperforms other SOTA models in these cases, reducing MAEs by
an average of 20%, underscoring its exceptional ability to capture molecular intricacies. Moreover, our results
indicate that incorporating higher-order many-body interactions is crucial for representing the non-local
features of larger molecules within the MD22 dataset.
5.3 Ablation studies
Figure 3: Ablation studies on the QM9 dataset’s HOMO task
(units: meV). The variable cwrepresents the threshold for expan-
sion in the fourth step of fragmentation, guided by the fragment
bond order defined in Eq. 18. The term BRICS denotes another
fragmentation method implemented in RDKits. Additionally, the
E2V summation refers to the architectural framework specified
from Eq. 13 to Eq. 15.TobetterunderstandSE3Set, weconduct
ablation studies focusing on fragmenta-
tion and model architecture. For the
explicit overlap fragmentation method,
we explore how different fragmenta-
tion techniques affect SE3Set’s training
and compare with the non-overlapping
BRICS (Degen et al., 2008; Landrum
et al., 2020) strategy on QM9’s homo
energy task. As Fig. 3 indicates, tests
on QM9’s homo energy task showed
SE3Set’s robustness to cwvariations
in fragmentation method. The results
show that our method surpasses BRICS,
demonstrating the importance of hyper-
edge interaction. Furthermore, we per-
formed ablation studies on the model ar-
chitecture. Among two design variants in
the E2V attention section, the one using
tensor product interactions between nodes and hyperedges proved superior, emphasizing the value of our tensor
product-based mechanism and architecture design in enhancing molecular property predictions. Additionally,
a 6-layer SE3Set model outperformed its 3-layer counterpart.
Unlike the explicit overlap method, the fragment size of the implicit overlap method depends on the choice of
rc. We test the effect of rcon the model training results on the AT-AT-CG-CG molecule of the MD22 dataset
(Table 3). The SE3Set performs better when using higher rcas it will include more fragments to generate
implicit overlaps. The effect of rcon fragment size is more pronounced for the implicit method than for the
explicit method, but it gives good performance for different parameters compared with the baseline models.
10Under review as submission to TMLR
6 Conclusion
In conclusion, this study demonstrates the efficacy of SE3Set, a cutting-edge hypergraph neural network
architecture, in the realm of molecular representation learning. By meticulously crafting a fragmentation
method that coalesces two-dimensional chemical knowledge with three-dimensional spatial information, we
establish a robust foundation for constructing hypergraphs that faithfully capture the complex nature of
molecular structures. The SE3Set architecture, drawing inspiration from the AllSet framework and the
Equiformer, adeptly processes these hypergraphs and preserves the essential invariances and symmetries.
SE3Set demonstrates performance comparable to SOTA models in small molecular systems and significantly
outperforms SOTA models in large molecular systems where higher-order many-body interactions are
pronounced. The results of our research affirm the potential of SE3Set to model high-order many-body
interactions, providing a powerful tool for molecular representation.
References
Sameer Agarwal, Jongwoo Lim, Lihi Zelnik-Manor, Pietro Perona, David Kriegman, and Serge Belongie.
Beyond pairwise clustering. In 2005IEEEComputer SocietyConference onComputer VisionandPattern
Recognition (CVPR’05), volume 2, pp. 838–845. IEEE, 2005.
Brandon Anderson, Truong Son Hy, and Risi Kondor. Cormorant: Covariant molecular neural networks.
Advances inneuralinformation processing systems, 32, 2019.
Sarp Aykent and Tian Xia. Savenet: a scalable vector network for enhanced molecular representation learning.
Advances inNeuralInformation Processing Systems, 36, 2024.
Ilyes Batatia, David P Kovacs, Gregor Simm, Christoph Ortner, and Gábor Csányi. MACE: Higher order
equivariant message passing neural networks for fast and accurate force fields. Advances inNeural
Information Processing Systems, 35:11423–11436, 2022.
Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P Mailoa, Mordechai Kornbluth, Nicola
Molinari, Tess E Smidt, and Boris Kozinsky. E (3)-equivariant graph neural networks for data-efficient and
accurate interatomic potentials. Nat.Commun., 13(1):2453, 2022.
Austin R Benson, David F Gleich, and Lek-Heng Lim. The spacey random walk: A stochastic process for
higher-order data. SIAMRev., 59(2):321–345, 2017.
Adam J. Bridgeman and Christopher J Empson. Bond orders between molecular fragments. Chemistry , 12 8:
2252–62, 2006.
He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, and Yu Li. Instructmol: Multi-modal integration for building a
versatile and reliable molecular assistant in drug discovery. arXivpreprint arXiv:2311.16208, 2023.
Fangying Chen, Junyoung Park, and Jinkyoo Park. A hypergraph convolutional neural network for molecular
properties prediction using functional group. Preprint athttp://arxiv.org/abs/2106.01028 , 2021.
Junwu Chen and Philippe Schwaller. Molecular hypergraph neural networks. Preprint at
http://arxiv.org/abs/2312.13136 , 2023.
Eli Chien, Pan Li, and Olgica Milenkovic. Landing probabilities of random walks for seed-set expansion in
hypergraphs. In 2021IEEEInformation TheoryWorkshop (ITW), pp. 1–6. IEEE, 2021a.
Eli Chien, Chao Pan, Jianhao Peng, and Olgica Milenkovic. You are AllSet: A multiset function framework
for hypergraph neural networks. In International Conference onLearning Representations, 2021b.
Stefan Chmiela, Alexandre Tkatchenko, Huziel E Sauceda, Igor Poltavsky, Kristof T Schütt, and Klaus-Robert
Müller. Machine learning of accurate energy-conserving molecular force fields. Sci.Adv., 3(5):e1603015,
2017.
11Under review as submission to TMLR
Stefan Chmiela, Valentin Vassilev-Galindo, Oliver T Unke, Adil Kabylda, Huziel E Sauceda, Alexandre
Tkatchenko, and Klaus-Robert Müller. Accurate global machine learning force fields for molecules with
hundreds of atoms. ScienceAdvances, 9(2):eadf0873, 2023.
Michael A Collins and Ryan PA Bettens. Energy-based molecular fragmentation methods. Chem.Rev., 115
(12):5607–5642, 2015.
Benjamin Coors, Alexandru Paul Condurache, and Andreas Geiger. SphereNet: Learning spherical rep-
resentations for detection and classification in omnidirectional images. In Proceedings oftheEuropean
conference oncomputer vision(ECCV), pp. 518–533, 2018.
Shicheng Cui, Qianmu Li, Deqiang Li, Zhichao Lian, Jun Hou, et al. Hyper-mol: Molecular representation
learning via fingerprint-based hypergraph. Comput. Intell.Neurosci., 2023, 2023.
Laurianne David, Amol Thakkar, Rocío Mercado, and Ola Engkvist. Molecular representations in AI-driven
drug discovery: a review and practical guide. J.Cheminform., 12(1):1–22, 2020.
Jörg Degen, Christof Wegscheid-Gerlach, Andrea Zaliani, and Matthias Rarey. On the art of compiling and
using’drug-like’chemical fragment spaces. ChemMedChem: Chemistry Enabling DrugDiscovery , 3(10):
1503–1507, 2008.
Kien Do, Truyen Tran, and Svetha Venkatesh. Graph transformation policy network for chemical reaction
prediction. In Proceedings ofthe25thACMSIGKDD international conference onknowledge discovery &
datamining, pp. 750–760, 2019.
Boxin Du, Changhe Yuan, Robert Barton, Tal Neiman, and Hanghang Tong. Hypergraph pre-training with
graph neural networks. Preprint athttp://arxiv.org/abs/2105.10862 , 2021.
Yuanqi Du, Limei Wang, Dieqiao Feng, Guifeng Wang, Shuiwang Ji, Carla P Gomes, Zhi-Ming Ma, et al. A
new perspective on building efficient and expressive 3d equivariant graph neural networks. Advances in
NeuralInformation Processing Systems, 36, 2024.
Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. Preprint at
http://arxiv.org/abs/1903.02428 , 2019.
Denis Fourches, Eugene Muratov, and Alexander Tropsha. Trust, but verify: on the importance of chemical
structure curation in cheminformatics and qsar modeling research. J.Chem.Inf.Model., 50(7):1189, 2010.
Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. SE (3)-transformers: 3D Roto-translation
equivariant attention networks. Advances inNeuralInformation Processing Systems, 33:1970–1981, 2020.
Johannes Gasteiger, Janek Groß, and Stephan Günnemann. Directional message passing for molecular graphs.
InInternational Conference onLearning Representations, 2019.
Johannes Gasteiger, Shankari Giri, Johannes T Margraf, and Stephan Günnemann. Fast
and uncertainty-aware directional message passing for non-equilibrium molecules. Preprint at
http://arxiv.org/abs/2011.14115 , 2020.
Johannes Gasteiger, Florian Becker, and Stephan Günnemann. Gemnet: Universal directional graph neural
networks for molecules. Advances inNeuralInformation Processing Systems, 34:6790–6802, 2021.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message
passing for quantum chemistry. In International conference onmachine learning, pp. 1263–1272. PMLR,
2017.
Jonathan Godwin, Michael Schaarschmidt, Alexander L Gaunt, Alvaro Sanchez-Gonzalez, Yulia Rubanova,
Petar Veličković, James Kirkpatrick, and Peter Battaglia. Simple GNN regularisation for 3D molecular
property prediction and beyond. In International Conference onLearning Representations, 2021.
12Under review as submission to TMLR
Mark S Gordon, Dmitri G Fedorov, Spencer R Pruitt, and Lyudmila V Slipchenko. Fragmentation methods:
A route to accurate calculations on large systems. Chem.Rev., 112(1):632–672, 2012.
Xiaohong Ji, Zhen Wang, Zhifeng Gao, Hang Zheng, Linfeng Zhang, Guolin Ke, et al. Uni-mol2: Exploring
molecular pretraining model at scale. arXivpreprint arXiv:2406.14969, 2024.
HiroshiKajino. Molecularhypergraphgrammarwithitsapplicationtomolecularoptimization. In International
Conference onMachine Learning, pp. 3183–3191. PMLR, 2019.
Jinwoo Kim, Saeyoon Oh, and Seunghoon Hong. Transformers generalize deepsets and can be extended to
graphs & hypergraphs. Advances inNeuralInformation Processing Systems, 34:28016–28028, 2021.
Jinwoo Kim, Saeyoon Oh, Sungjun Cho, and Seunghoon Hong. Equivariant hypergraph neural networks. In
European Conference onComputer Vision, pp. 86–103. Springer, 2022a.
Seojin Kim, Jaehyun Nam, Junsu Kim, Hankook Lee, Sungsoo Ahn, and Jinwoo Shin. Contrastive learning
of molecular representation with fragmented views. 2022b.
P-J Kindermans and K-R Müller. Schnet–a deep learning architecture for molecules and materials. J.Chem.
Phys., 148(24), 2018.
Arthur Kosmala, Johannes Gasteiger, Nicholas Gao, and Stephan Günnemann. Ewald-based long-range
message passing for molecular graphs. In International Conference onMachine Learning (ICML), 2023.
Greg Landrum, Paolo Tosco, Brian Kelley, sriniker, gedeck, NadineSchneider, Riccardo Vianello, Ric, Andrew
Dalke, Brian Cole, AlexanderSavelyev, Matt Swain, Samo Turk, Dan N, Alain Vaucher, Eisuke Kawashima,
Maciej Wójcikowski, Daniel Probst, guillaume godin, David Cosgrove, Axel Pahl, JP, Francois Berenger,
strets123, JLVarjo, Noel O’Boyle, Patrick Fuller, Jan Holst Jensen, Gianluca Sforna, and DoliathGavid.
rdkit/rdkit: 2020_03_1 (q1 2020) release, March 2020. URL https://doi.org/10.5281/zenodo.3732262 .
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer:
A framework for attention-based permutation-invariant neural networks. In International conference on
machine learning, pp. 3744–3753. PMLR, 2019.
György Lendvay. On the correlation of bond order and bond length. J.Mol.Struct., 501:389–393, 2000.
Guoyin Li, Liqun Qi, and Gaohang Yu. The Z-eigenvalues of a symmetric tensor and its application to
spectral hypergraph theory. Numer.LinearAlgebra Appl., 20(6):1001–1029, 2013.
Yunyang Li, Yusong Wang, Lin Huang, Han Yang, Xinran Wei, Jia Zhang, Tong Wang, Zun Wang, Bin
Shao, and Tie-Yan Liu. Long-short-range message-passing: A physics-informed framework to capture
non-local interaction for scalable molecular dynamics simulation. In TheTwelfthInternational Conference
onLearning Representations, 2024. URL https://openreview.net/forum?id=rvDQtdMnOl .
Yi-Lun Liao and Tess Smidt. Equiformer: Equivariant graph attention transformer for 3D atomistic graphs.
InTheEleventh International Conference onLearning Representations, 2022.
Yi-LunLiao, BrandonWood, AbhishekDas, andTessSmidt. EquiformerV2: Improvedequivarianttransformer
for scaling to higher-degree representations. Preprint athttp://arxiv.org/abs/2306.12059 , 2023.
Kha-Dinh Luong and Ambuj Singh. Fragment-based pretraining and finetuning on molecular graphs. Preprint
athttp://arxiv.org/abs/2310.03274 , 2023.
Jonathan P Mathews and Alan L Chaffee. The molecular representations of coal–a review. Fuel, 96:1–14,
2012.
David L Mobley, Caitlin C Bannan, Andrea Rizzi, Christopher I Bayly, John D Chodera, Victoria T Lim,
Nathan M Lim, Kyle A Beauchamp, David R Slochower, Michael R Shirts, et al. Escaping atom types in
force fields using direct chemical perception. J.Chem.TheoryComput., 14(11):6076–6092, 2018.
13Under review as submission to TMLR
Albert Musaelian, Simon Batzner, Anders Johansson, Lixin Sun, Cameron J Owen, Mordechai Kornbluth,
and Boris Kozinsky. Learning local equivariant representations for large-scale atomistic dynamics. Nat.
Commun., 14(1):579, 2023.
Eliya Nachmani and Lior Wolf. Molecule property prediction and classification with graph hypernetworks.
Preprint athttp://arxiv.org/abs/2002.00240 , 2020.
Kelly J Pearson and Tan Zhang. On spectral hypergraph theory of the adjacency tensor. GraphsCombin.,
30:1233–1248, 2014.
John P Perdew, Kieron Burke, and Matthias Ernzerhof. Generalized gradient approximation made simple.
Phys.Rev.Lett., 77(18):3865, 1996.
Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum
chemistry structures and properties of 134 kilo molecules. Sci.Data, 1(1):1–7, 2014.
Patrick Reiser, Marlen Neubert, André Eberhard, Luca Torresi, Chen Zhou, Chen Shao, Houssam Metni,
Clint van Hoesel, Henrik Schopmans, Timo Sommer, et al. Graph neural networks for materials science
and chemistry. Commun. Mater., 3(1):93, 2022.
Lars Ruddigkeit, Ruud Van Deursen, Lorenz C Blum, and Jean-Louis Reymond. Enumeration of 166 billion
organic small molecules in the chemical universe database GDB-17. J.Chem.Inf.Model., 52(11):2864–2875,
2012.
T Konstantin Rusch, Michael M Bronstein, and Siddhartha Mishra. A survey on oversmoothing in graph
neural networks. Preprint athttp://arxiv.org/abs/2303.10993 , 2023.
Kristof Schütt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko,
and Klaus-Robert Müller. Schnet: A continuous-filter convolutional neural network for modeling quantum
interactions. Advances inneuralinformation processing systems, 30, 2017.
Kristof Schütt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the prediction of tensorial
properties and molecular spectra. In International Conference onMachine Learning , pp. 9377–9388. PMLR,
2021.
Annika Stuke, Christian Kunkel, Dorothea Golze, Milica Todorović, Johannes T Margraf, Karsten Reuter,
Patrick Rinke, and Harald Oberhofer. Atomic structures and orbital energies of 61,489 crystal-forming
organic molecules. Scientific data, 7(1):58, 2020.
Mohammadamin Tavakoli, Alexander Shmakov, Francesco Ceccarelli, and Pierre Baldi. Rxn hy-
pergraph: a hypergraph attention model for chemical reaction representation. Preprint at
http://arxiv.org/abs/2201.01196 , 2022.
Philipp Thölke and Gianni De Fabritiis. Equivariant transformers for neural network based molecular
potentials. In International Conference onLearning Representations, 2021.
Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor
field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. Preprint at
http://arxiv.org/abs/1802.08219 , 2018.
Alexandre Tkatchenko, Robert A DiStasio Jr, Roberto Car, and Matthias Scheffler. Accurate and efficient
method for many-body van der Waals interactions. Phys.Rev.Lett., 108(23):236402, 2012.
Francesco Tudisco, Austin R Benson, and Konstantin Prokopchik. Nonlinear higher-order label spreading. In
Proceedings oftheWebConference 2021, pp. 2402–2413, 2021.
Jeff Wagner, Matt Thompson, David L. Mobley, John Chodera, Caitlin Bannan, Andrea Rizzi, trevorgokey,
David L. Dotson, Josh A. Mitchell, jaimergp, Camila, Pavan Behara, Christopher Bayly, JoshHorton, Iván
Pulido, Lily Wang, Victoria Lim, Sukanya Sasmal, SimonBoothroyd, Andrew Dalke, Daniel Smith, Brent
Westbrook, Josh Horton, Lee-Ping Wang, Richard Gowers, Ziyuan Zhao, Connor Davel, and Yutong Zhao.
14Under review as submission to TMLR
openforcefield/openff-toolkit: 0.15.1. Testing updates, January 2024. URL https://doi.org/10.5281/
zenodo.10593535 .
Limei Wang, Yi Liu, Yuchao Lin, Haoran Liu, and Shuiwang Ji. ComENet: Towards complete and efficient
message passing for 3D molecular graphs. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun
Cho (eds.), Advances inNeuralInformation Processing Systems, 2022.
Yusong Wang, Tong Wang, Shaoning Li, Xinheng He, Mingyu Li, Zun Wang, Nanning Zheng, Bin Shao, and
Tie-Yan Liu. Enhancing geometric representations for molecules with equivariant vector-scalar interactive
message passing. Nat.Commun., 15(1):313, 2024.
Zun Wang, Guoqing Liu, Yichi Zhou, Tong Wang, and Bin Shao. Efficiently incorporating quintuple
interactions into geometric deep learning force fields. In Thirty-seventh Conference onNeuralInformation
Processing Systems, 2023.
Daniel S Wigh, Jonathan M Goodman, and Alexei A Lapkin. A review of molecular representation in the
age of machine learning. WileyInterdiscip. Rev.Comput. Mol.Sci., 12(5):e1603, 2022.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive
survey on graph neural networks. IEEETrans.Neural.Netw.Learn.Syst., 32(1):4–24, 2020.
Jiacheng Xiong, Zhaoping Xiong, Kaixian Chen, Hualiang Jiang, and Mingyue Zheng. Graph neural networks
for automated de novo drug design. DrugDiscov.Today, 26(6):1382–1393, 2021.
ManzilZaheer, SatwikKottur, SiamakRavanbakhsh,BarnabasPoczos, RussRSalakhutdinov, andAlexanderJ
Smola. Deep sets. Advances inneuralinformation processing systems, 30, 2017.
Dengyong Zhou, Jiayuan Huang, and Bernhard Schölkopf. Learning with hypergraphs: Clustering, classifica-
tion, and embedding. Advances inneuralinformation processing systems, 19, 2006.
Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and
Guolin Ke. Uni-mol: A universal 3d molecular representation learning framework. In International
Conference onLearning Representations, 2023.
Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng
Li, and Maosong Sun. Graph neural networks: A review of methods and applications. AIopen, 1:57–81,
2020.
15Under review as submission to TMLR
A Appendix
B Details of fragmentation steps
Based on the design principles in Sec. 4.1, the detailed step-by-step methodology of explicitoverlapfragmen-
tation method is shown as follows,
1.The pre-processing step begins by analyzing the given molecule through its bond order matrix,
denoted as B. Identify and mask bonds that are part of functional groups or rings, as well as those
with a bond order of Bij⩾2. Functional groups are then identified using predefined SMARTS
patterns for accurate matching. To achieve a more generalized representation of functional groups,
topologically adjacent functional groups are merged into a single entity. This aggregation allows to
focus on specific subfunctional groups that are of particular interest, simplifying the complexity of
the molecular structure for subsequent analysis.
2.Following the masking of selected bonds, the Breadth-First Search (BFS) algorithm is employed to
reconstruct the substructures, denoted as {S}, from the remaining unmasked bonds. These groups
represent the core structural units of the molecule as discussed at the outset of this section.
3.Consolidate the previously identified groups {S}into larger molecular fragments, applying a set
of predefined rules to guide the merging process. These rules are meticulously designed to ensure
that each resulting fragment, now denoted as {F}, contains at least a minimum specified number of
atoms. For a comprehensive understanding of the merging criteria, one can refer to the detailed rules
outlined in D.
4.Extend each fragment {F}by incorporating adjacent groups from {S}to enrich the connectivity
between molecular fragments, thus intentionally creating regions of overlap among the fragments.
This expansion is controlled by a cutoff threshold, denoted as cw, which is typically a function based
on interatomic distances. the fragment bond order (Lendvay, 2000; Bridgeman & Empson, 2006),
symbolized by Wfs, is used to quantitatively assess the interaction strength between a fragment
Fiand an adjacent substructure Sj. This method reflects the interaction strength based on the
proximity of atoms in different fragments, expressed by the following equation:
Wfs=/summationdisplay
i∈Ff,j∈Ssexp/parenleftbigg
−(dij−de
ij)·de
ij
(0.25Å)2/parenrightbigg
, (18)
wheredijrepresents the interatomic distance between atoms iandj, andde
ijstands for the equilibrium
distance typically expected for such a bond. This equation is utilized to determine which substructures
should be included in the expansion of a fragment, based on the strength of their interactions as
governed by the distance function. Additionally, in alignment with Pauling’s concept of "chemist’s
bond order" (Lendvay, 2000), an alternative method is introduced to calculate the bond order using
a single exponential function,
Wfs=/summationdisplay
i∈Ff,j∈Ssexp/parenleftbig
−(dij−de
ij)/parenrightbig
, (19)
whereWfsencapsulates the bond order between atoms belonging to a fragment Ffand a substructure
Ss. In this context, dijsignifies the actual measured distance between atom iand atomj. The term
de
ijrefers to the theoretical equilibrium covalent bond length, which is estimated by summing the
empirical covalent radii of the two atoms involved, given by:
de
ij=rzi+rzj, (20)
whererziis the empirical covalent radius of an atom with atomic number zi. This function provides
a simplified yet effective representation of bond order, allowing us to gauge the bonding interactions
within the molecular structure with respect to the proximity of the atoms.
Theimplicitoverlapfragmentation method only changes the step 4, the details of the changed fourth step
have been spelled out in 4.1.
16Under review as submission to TMLR
C Functional groups SMARTS
In the initial phase of our fragmentation approach, we identify functional groups using the SMARTS
pattern matching language. In Table 4, we present the complete list of SMARTS patterns utilized, which
have been expanded upon from the default set found within the Open Force Field toolkit (Mobley et al.,
2018; Wagner et al., 2024) (accessible at: https://github.com/openforcefield/openff-fragmenter/
blob/main/openff/fragmenter/data/default-functional-groups.json ).
Table 4: SMARTS patterns for functional groups employed in the preprocessing stage of fragmentation.
Functional Groups Name SMARTS
hydrazine [NX3:1][NX3:2]
hydrazone [NX3:1][NX2:2]
nitric oxide [N:1]-[O:2]
amide [#7:1][#6:2](=[#8:3]), [NX3:1][CX3:2](=[OX1:3])[NX3:4]
amide negative ion [#7:1][#6:2](-[O-:3])
aldehyde [CX3H1:1](=[O:2])[#6:3]
sulfoxide [#16X3:1]=[OX1:2], [#16X3+:1][OX1-:2]
sulfonyl [#16X4:1](=[OX1:2])=[OX1:3]
sulfinic acid [#16X3:1](=[OX1:2])[OX2H,OX1H0-:3]
sulfonic acid [#16X4:1](=[OX1:2])(=[OX1:3])[OX2H,OX1H0-:4]
sulfinamide [#16X4:1](=[OX1:2])(=[OX1:3])([NX3R0:4])
phosphine oxide [PX4:1](=[OX1:2])([#6:3])([#6:4])([#6:5])
phosphonate [P:1](=[OX1:2])([OX2H,OX1-:3])([OX2H,OX1-:4])
phosphate [PX4:1](=[OX1:2])([#8:3])([#8:4])([#8:5])
carboxylic acid [CX3:1](=[O:2])[OX1H0-,OX2H1:3]
nitro [NX3+:1](=[O:2])[O-:3], [NX3:1](=[O:2])=[O:3]
ester [CX3:1](=[O:2])[OX2H0:3]
tri-halide [#6:1]([F,Cl,I,Br:2])([F,Cl,I,Br:3])([F,Cl,I,Br:4])
hydroxyl [#8:1]-[#1:2]
D Merge process of fragmentation
During the third step of our fragmentation method, we introduce a strategy to enlarge substructures, ensuring
that each initial fragment contains at least nminatoms, with nminbeing a predefined integer. To maintain
permutation invariance for a molecule, we incorporate weights, Wfs, to guide the sequence of merging. The
process is outlined in the pseudocode (Algorithm 1). The calculation of Wis based on either Eq. 18 or Eq. 19.
By considering the sum of bond orders to other groups, we assess each group’s centrality. The groups are
then ordered first by the number of atoms they contain, followed by the summation of their bond orders,
ensuring that the fragmentation merge process is permutation invariant when following this specified sequence.
The algorithm then assists smaller groups in merging with others to achieve a size of at least nminatoms.
Initially, we consider topologically adjacent groups with the fewest atoms. If a target group lacks topological
neighbors, we proceed to merge based on the bond order from W. We introduce a threshold cisthat allows a
group to remain isolated if it is significantly distant from others. It should be noted that isolated groups
may not meet the minimum atom number requirement; however, they could be further expanded in the
subsequent fragmentation step, depending on the chosen thresholds for cisandcw(refer to Sec. 4.1). Overall,
this algorithm ensures a permutation invariant merging process.
E Distribution of fragmentation dataset
Different parameters used in the fragmentation process can lead to a variety of hyperedges, which in turn
result in distinct hypergraphs utilized for training our model. To illustrate the variances attributed to different
fragmentation parameters or methods (such as BRICS implemented in RDKit (Degen et al., 2008; Landrum
17Under review as submission to TMLR
Algorithm 1 Pseudo code of fragmentation merge step.
Input:groups{G}, minimum atoms number nmin, maximum atoms number nmax, Topological bond order
matrixB, isolated threshold cis
m=|{G}|
Isolate groups{GI}={}
Calculate fragmentation bond order matrix WGiGj.
Sort{G}in descending order based on the following attributes: number of atoms,/summationtext
G′,G′̸=GWGG′.
repeat
Pop last fragment as Gkfrom{G}
fori=m−1to1do
a=MAX_INT ,merge_idx=−1
if anyBij⩾1,i∈Gij∈Gkand|Gi|<aanda+|Gi|⩽nmaxthen
a=|Gi|,merge_idx=i
end if
end for
ifmerge_idx==−1then
fori=m−1to1do
if anyWGiGk⩾cisand|Gi|<athen
a=|Gi|,merge_idx=−1
end if
end for
end if
ifmerge_idx̸=−1then
Merge GktoGmerge _idx
Resort{G}by the same priority and update W.
else
Add{Gk}to{GI}
end if
until|{Gk}|⩾nmin
{F}={G}∪{GI}
et al., 2020)), we use the QM9 dataset (Ruddigkeit et al., 2012; Ramakrishnan et al., 2014) to demonstrate
how the data distributions attached to hypergraphs may change.
The impact of adjusting fragmentation parameters on the composition of hyperedges can be observed in
Fig. 4. Altering the expansion threshold cwwithin a certain range has a minimal effect on fragment expansion.
However, when utilizing the Lendvay bond order (Eq. 18), fragments tend to comprise fewer atoms compared
to when using the Exponential bond order (Eq. 19). This difference is likely due to the more gradual decline
in the exponential function, which results in a greater cumulative contribution to the weights Wfs.
Our ablation study (Sec. 5.3) also includes a comparison with the BRICS fragmentation method. Fragments
generated by the BRICS method are observed to contain significantly fewer atoms since this approach does
not create overlapping regions between different fragments.
F Proof of SE(3) equivariance
The SE3Set consists of the basic modules including linear, depth-wise tensor product, gate activation and
layer normalization. Here we will prove the SE3 equivariance for these modules. As the inputs of SE3Set are
all invariant to translation in 3D Euclidean space, we only need to prove the SO(3)equivariance.
Letg∈SO(3)and theDdenote the group representation of SO(3). Besides, we denote the irreps feature as
f, andfldenotes the kthfeature vector in irreducible representations space of SO(3)withlorder.
18Under review as submission to TMLR
(a) (b)
(c) (d)
(f) (g)(e)
(h) (i) (j)
Figure 4: Distribution of fragments in QM9 dataset. (a) Fragment Count Distribution. The distribution
remains consistent regardless of the value of cwor the bond order calculation method employed. (b) Molecule
Size vs. Fragment Count Distribution. Generally, the more atoms molecule has, the more fragments will
generate. It is also invariant for cwor bond order calculation scheme. Average Atom Count per Fragment
Distribution (c) cw= 0.1, (d)cw= 0.05, (e)cw= 0.01for Lendvay bond order and (f) cw= 0.4and (g)
cw= 0.2for exponential bond order, respectively. (h) BRICS Fragment Count Distribution. (i) BRICS
Molecule Size vs. Fragment Count Distribution (j) BRICS Average Atom Count per Fragment.
LinearLinear module deploys separated linear operations for each lin the irreps feature. For each l, we
consider the output channel fl
k. Then we have
fl
j=Linear (fl) =/summationdisplay
kwl
kjfl
k (21)
19Under review as submission to TMLR
wherewl
kjdenotes the linear combination weight. When acting D(g)at the input fl, we can find that
(fl
j)′=Linear (D(g)fl) (22)
=/summationdisplay
kwl
kjD(g)fl
k (23)
=D(g)/summationdisplay
kwl
kjfl
k (24)
=D(g)fl
j (25)
Therefore, the linear module is SO(3)equivariant.
Depth-wise Tensor-product Tensor product is an equivariant operation for D(g)
D(g)(fl
k1⊗fl
k2) = (D(g)fl
k1)⊗(D(g)fl
k2) (26)
The depth-wise tensor product differs from the tensor product only in that one order lvector in out irreps
feature depends only on one order l′feature, where l′is equal to or different from l. Hence the SO(3)
equivariance still holds for depth-wise tensor product.
GateAs thel= 0vector is invariant for D(g), we have
Gate (D(g)f0
k) =Activation (D(g)f0
k) (27)
=Activation (f0
k) (28)
=D(g)Activation (f0
k) (29)
=D(g)Gate (f0
k) (30)
We use the non-linear output from the Activation (D(g)f0
k1)(i.e. Sigmoid in SE3Set) as the weight which
multipliesl>0vector to implement the gate function. Thus for l>0, we also have
Gate (D(g)fl
k1) =Activation (D(g)f0
k′)D(g)fl
k1(31)
=Activation (f0
k1)D(g)fl
k1(32)
=D(g)Gate (fl
k1) (33)
So the gate function is SO(3)equivariant for all lvector.
Layer Normalization Forl= 0, as it isSO(3)invariant, the general layer normalization is adapted
LN(f0
k) =/parenleftbiggf0
k−µ
RMSC(∥f0∥)/parenrightbigg
γ+β (34)
whereCdenotes the channel number corresponding and ∥·∥denotes the 2-norm for each channel vector. µ
is the mean value of f0. Learnable weight γand learnable bias βare also deployed. The module is SO(3)
invariant as f0
kisSO(3)invariant. The layer normalization for l>0vector has the following form
LN(fl
k) =/parenleftbiggfl
k
RMSC(∥fl∥)/parenrightbigg
γ (35)
We can prove the module is SO(3)equivariant as
LN(D(g)fl
k) =/parenleftbiggD(g)fl
k
RMSC(∥D(g)fl∥)/parenrightbigg
γ (36)
=/parenleftbiggD(g)fl
k
RMSC(∥fl∥)/parenrightbigg
γ (37)
=D(g)LN(fl
k) (38)
20Under review as submission to TMLR
G Concepts of irreps features and tensor product
Irreps features The SE3Set model utilizes the special orthogonal group SO(3) to capture three-dimensional
rotational symmetries in molecular structures. This approach is similar to Equiformer (Liao & Smidt, 2022;
Liao et al., 2023).It employs irreducible representations (irreps) of SO(3), parameterized by an integer l,
which correspond to spherical harmonics (SH) functions Ym
l. These functions imbue feature vectors with
rotational information, ensuring the model’s equivariance to rotations and enabling consistent geometric
property analysis. This approach is key to the model’s ability to accurately represent and predict molecular
and other rotationally invariant systems.
Tensor product To boost the model’s expressive power, we consider interactions between irrep features of
different angular momenta lthrough the tensor product, which merges two irreps l1andl2into a new irrep
with angular momentum l3. This is achieved using Clebsch-Gordan coefficients in an expansion weighted by
wm1,m2.
fl3
m3= (fl1
m1⊗fl2
m2)m3
=/summationdisplay
m1,m2wm1,m2Cl3,m3
l1,m1, l2,m2fl1
m1fl2
m2.(39)
To reduce complexity, a depth-wise tensor product ⊗DTPis adopted from the Equiformer (Liao & Smidt,
2022; Liao et al., 2023), utilizing internal weights to streamline computations. Input-dependent tensor product
weights are denoted as ⊗DTP
w, ensuring computational efficiency while preserving equivariance for feature
interactions.
H Results of MD17
The MD17 dataset (Chmiela et al., 2017) features a wide variety of molecular configurations simulated
at 500 K, with high-resolution trajectories and labeled with energies and forces from the PBE+vdW-TS
method (Perdew et al., 1996; Tkatchenko et al., 2012). SE3Set’s performance on this dataset is shown in
Table 5. SE3Set outperforms Equiformer in accuracy, highlighting its refined force calculation capabilities. In
small molecular systems, higher-order many-body interactions are less pronounced, and as a result, SE3Set
does not significantly outperform other state-of-the-art (SOTA) models.
I Results of OE62
The OE62 dataset (Stuke et al., 2020) contains about 62k large organic molecules with annotated DFT-
computed energies in the unit of eV. We randomly selected 50000 data points as training set and 6000 data
points as training set. Then we report the MAE of energy as the performance metric on the test dataset. Since
OE62 is considered to have significant long-range interactions Kosmala et al. (2023), we selected model results
without explicit long-range modeling as a baseline. The results (Table 6) shows that SE3Set outperforms
all the baseline models (short-range models), which indicates the potential of the many body interaction
modeling by our model.
J Complexity Analysis
The computational complexity mainly depends on the V2E module and the E2V module.
The V2E module aggregates the information from each atom in one fragment to generate atom-wise hyperedge
features. Considering the system is split into mfragments and each fragment has niatoms, this module
includes/summationtextm
ini(ni−1)pair-wise messages for the attention architecture. Actually, this number depends on
the fragment hyperparameter nminandnmax, in particular for the explicit overlap method, also on cw. For
the explicit overlap method, this module would have higher complexity than the implicit overlap method
because the explicit overlap has a higher average number of atoms in each fragment.
21Under review as submission to TMLR
Table 5: A comparison of Mean Absolute Errors (MAEs) across various benchmarked models. SE3Set is
trained on the MD17 dataset with a configuration of 950 training samples and 50 validation samples. Bolding
shows the best model and underlining shows the second best model and the underlining tilde shows third
best model. The MAEs reflect the precision of energy predictions in units of kcal/mol and forces in units of
kcal/(mol·Å). The results of each baseline come from the corresponding articles (Schütt et al., 2017; Gasteiger
et al., 2019; Schütt et al., 2021; Thölke & De Fabritiis, 2021; Gasteiger et al., 2021; Batzner et al., 2022;
Wang et al., 2024; Liao & Smidt, 2022; Wang et al., 2023).
SchNet DimeNet PaiNN ET GemNet NequIP ( l=3) ViSNet QuinNet Equiformer SE3Set
AspirinEnergy 0.37 0.204 0.167 0.123 - 0.131 0.116 0.119:::::0.122 0.130
Force 1.35 0.499 0.338 0.253 0.217 0.184 0.155 0.145 0.152:::::0.153
EthanolEnergy 0.08 0.064 0.064 0.052 - 0.051 0.051 0.050 0.051 0.054
Force 0.39 0.230 0.224 0.109 0.085 0.071 0.060 0.060 0.067:::::0.062
MalonaldehydeEnergy 0.13 0.104 0.091 0.077 - 0.076 0.075 0.078 0.074 0.074
Force 0.66 0.383 0.319 0.169 0.155 0.129 0.100 0.097 0.125:::::0.103
NaphthaleneEnergy 0.16 0.122 0.116 0.085 - 0.113 0.085::::0.101 0.085 0.113
Force 0.58 0.215 0.077 0.061 0.051 0.039 0.039 0.039 0.046 0.039
Salicylic acidEnergy 0.20 0.134 0.116 0.093 - 0.106 0.092::::0.101 0.099 0.108
Force 0.85 0.374 0.195 0.129 0.125:::::0.090 0.084 0.080:::::0.090:::::0.090
TolueneEnergy 0.12 0.102 0.095 0.074 - 0.092 0.074 0.080:::::0.085 0.093
Force 0.57 0.216 0.094 0.067 0.060:::::0.046 0.039 0.039 0.048:::::0.046
UracilEnergy 0.14 0.115 0.106 0.095 - 0.104 0.095::::0.096 0.099 0.103
Force 0.56 0.301 0.139 0.095 0.097 0.076 0.062 0.062 0.076:::::0.067
Table 6: A comparison of MAEs across some benchmarked models on OE62 dataset. The best model is
bolded. The results of baselines are extracted from (Kosmala et al., 2023).
Model SchNet PaiNN DimeNet++ GemNet-T SE3Set(3 Layers)
MAE (meV) 131.3 63 53.8 53.1 51.7
For each atom, the E2V module aggregates all the hyperedge features that an atom possesses. Thus the
calculated pair-wise message number depends on the number of fragments per atom shared, corresponding to
the introduced overlap degrees. Then the computational complexity of the explicit overlap method in this
module depends on the cwfor the explicit overlap method and rcfor the implicit overlap method.
To give a general approximate complexity, we consider a system with Natoms. When using the explicit
overlap method, if the system has mfragments with an average nexpatoms in one fragment, the V2E
module’s complexity will be O(mn2
exp)according to the previous analysis. For the E2V module, the number
of fragments to which each atom belongs on average can be represented as mnexp/N. Thus, the complexity
will beO(mnexp)when considering Natoms.
Similarly, for the implicit overlap method, the complexity of the V2E module will be O(nFnimpN), wherenF
represents the average number of neighbor fragments within cutoff per atom and nimpdenotes the average
atoms in one fragment. And the complexity of the E2V module is O(nFN).
Compare to the explicit overlap method, the implicit overlap method has lower computational complexity.
As we can approximate that nFnimp≈nexp(overlap degree is similar) and determine mnimp=N(implicit
overlap method has same fragment number mbut without extra count of atoms in nimp). On the other hand
we can determine mnexp⩾Nas the explicit overlap will count atoms more times. Hence the complexity of
the explicit overlap method has higher complexity than implicit overlap on V2E Module according to the
deduction below
mn2
exp⩾Nnexp≈nFnimpN (40)
mnexp≈mnFnimp≈nFN (41)
22Under review as submission to TMLR
Ac-Ala3-NHMe DHA Stachyose AT-AT AT-AT-CG-CG
Molecule05101520253035Inference Speed (it/s)
SE3Set NequIP(cutoff=6.0 Å) NequIP(cutoff=4.0 Å) Equiformer MACE
Figure 5: Inference speeds of different models on the MD22 dataset in units iteration/s. The average inference
speeds were calculated for each molecule dataset. All tests were performed on a single Tesla A100 80G. The
SE3Set includes 3 layers and different cutoff is set (4 Åand 6 Å) for NeuqIP. The other models. The other
models use parameter configurations that corresponded to the results reported in their respective studies.
To better support the analysis, we have tested the inference speed of SE3Set on part of QM9 and MD22
datasets as shown in Table 7. Larger fragments, with smaller cw, reduce speed, more so in larger molecules.
Speed drops with increased fragment size (smaller cwor largerrc), and the explicit overlap method is slower
than the implicit, leading to our preference for the latter in MD22. This highlights the need to optimize
fragment size for a trade-off between detailed interaction capture and speed, particularly in big molecular
systems. Additionally, the explicit overlap method is slower than the implicit overlap method, which confirms
our previous analysis.
Table 7: The inference speed (unit: iterations/s) of SE3Set with different cworrc. The tests on MD22
dataset only use the AT-AT-CG-CG subset.
datasetExplicitcw Implicitrc
0.1 0.05 0.01 4.0 5.0 6.0
Inference speedQM9 11.60 11.54 11.37 - - -
MD22 (AT-AT-CG-CG) 2.62 2.29 1.68 2.92 2.90 2.87
Moreover, we have evaluated the inference time with different baseline models. The results in Fig. 5 show
SE3Set runs slightly slower than the other models. This is reasonable as we have included more pairwise
node information in our hypergraph. The improvement in our model’s performance indicates that our model
requires further optimization to achieve a better balance between efficiency and performance.
We also show the number of parameters of SE3Set comparing with some state-of-the-art baseline models as
showed in Fig. 8. SE3Set achieves better performance using less parameters.
Table 8: Number of parameters with different models.
Model ViSNet QuinNet SE3Set(6 Layers) SE3Set(3 Layers)
No. of Parameters (Million) 10 9 6.3 3.5
23Under review as submission to TMLR
K Training details
This section outlines the training specifics, encompassing the fragmentation parameters, SE3Set hyperparam-
eters, and certain implementation nuances utilized in our experimental setup.
Our dataset construction is founded on PyTorch Geometric (Fey & Lenssen, 2019) augmented with our
fragmentation process (Sec. 4.1). Due to inconsistencies in molecular topology identified through RDKit’s
sanitization routine (Landrum et al., 2020), 1,403 data points were excised from the original dataset. We
designated 110,000 data points for the training set and 10,000 for the validation set, selected at random.
We used an explicitoverlapscheme on the QM9 and MD17 datasets because of their relatively small molecular
systems. We implemented two distinct schemes for calculating fragment bond orders, following either Eq. 18
or Eq. 19. The parameters for fragmentation are detailed below. Given that the MD17 molecules are relatively
small, the merge step in the fragmentation process was not actually utilized. However, we present the
fragmentation parameters here for the sake of completeness.
Besides, we adopt an implicitoverlapscheme on the MD22 dataset and OE62 dataset to reduce computational
resource consumption. The details of cutoff rcintroduced in 4* can be found in Table 11.
On QM9 and MD17 dataset, our model was trained using a single Tesla V100 GPU with 32GB of memory,
except for the 6-layer model employing the exponential bond order on QM9 dataset, which was trained on
two Tesla V100 GPUs with 32GB each. For MD22 dataset and OE62 dataset, our model was trained on a
single Tesla A100 GPU with 80GB of memory.
We selected l= 2for our irreducible representations (irreps) feature, which includes both node and hypergraph
features. For the radial basis function (RBF), we utilized Gaussian basis functions or Bessel basis functions
for the QM9 dataset (Ruddigkeit et al., 2012; Ramakrishnan et al., 2014) and exponential basis functions
for the MD17 dataset, MD22 dataset and OE62 dataset (Chmiela et al., 2017). Details could be found in
Table 9 and Table 10.
Table 9: Hyper-parameters for training SE3Set model. In the context of hyperparameter settings for
dimensions, the symbols eandoare used to denote even and odd parity, respectively.
QM9 MD17 MD22 OE62
Hyper-parameters Value or discriptions Value or discriptions Value or discriptions Value or discriptions
Optimizer nmin AdamW AdamW AdamW AdamW
Learning rate scheduler Cosine Cosine Cosine Cosine
Warm up epochs nmax 5 10 10 10
Minimum learning rate 1.0×10−61.0×10−61.0×10−61.0×10−6
Batch size 32,128 8 8 32
Number of epochs 400 1500 1500 400
Weight decay 5.0×10−31.0×10−61.0×10−61.0×10−6
Dropout rate 0.1 0.0 0.0 0.0
RBF cutoff (Å) 42.0 Max distance of used atom pairs
Basis Type Gaussian or Bessel Exponential Exponential Exponential
Number of Basis 128 or 8 32 32 32
Number of Blocks 3 or 6 3 or 6 6 3
Node embedding dimension [(128, 0 e), (64, 1o), (32, 2e)]
Hyperedge embedding dimension [(128, 0 e), (64, 1o), (32, 2e)]
Attention head dimension [(32, 0 e), (16, 1o), (8, 2e)]
Feed forward dimension [(384, 0 e), (192, 1o), (96, 2e)]
Output feature dimension [(512, 0 e)]
L Comparison with pre-trained baselines
Pre-trained model has become more popular in molecular representation learning recently. For completeness,
we have added the performance comparison with some state-od-the-art pre-trained models in this section.
We choose the reported average MAE on 3 targets (HOMO, LUMO, gap) of QM9 as a metric, our model
performs better than those pre-trained models on this metic as showed in Table 12.
24Under review as submission to TMLR
Table 10: Hyper-parameters for fragmentation. The expand threshold does not work for models training on
MD22 dataset because they adapt implicitoverlap scheme.
Bond Order Methods Bond Order by Lendvay (18) Frgmentation by Exponential (19)
minimum atoms number nmin 2 2
maximum atoms number nmax 6 6
isolated threshold ( cis) 0.1 0.4
expand threshold ( cw) 0.1 0.2, 0.4
Table 11: Hyper-parameters for step 4* of implicitoverlap scheme in MD22 and OE62 experiments.
Dataset MD22 OE62
Ac-Ala3-NHMe DHA Stachyose AT-AT AT-AT-CG-CG
distance cutoff rc(Å) 5.0 4.0 4.0 6.0 6.0 5.0
Table 12: Performance comparing with some pre-trained models on 3 tasks (HOMO, LUMO, GAP) of QM9
dataset. We use the average mean absolute error performance here following the article of baselines. All the
baseline results are extracted from the origin articles (Zhou et al., 2023; Ji et al., 2024; Cao et al., 2023).
Model UniMol UniMol-2 InstructMol SE3Set
Avg. MAE (meV) 127 95 136 19
M Comparison with graph neural networks
The hypergraph could incorporate more information than graph with binary edges theoretically, as the
hyperedge denotes a set of nodes in graphs, which means graph can be treated as a special case of hypergraph
as shown in Fig. 6 (a). To further elaborate on the features and advantages of the hypergraph neural network
architecture we use, we show the difference between our model and the graph neural network in this section,
using 5 linear arranged nodes as an example in Fig. 6 (b)-(d). The black lines indicate the edges in the
origin graph. This structure can correspond to the heavy atoms of the molecule malononitrile ( NC-CH2-CN).
We focus on the message passing from the orange node to the green node in the graph. For a graph neural
network, such message passing requires 4 layers. For the graph neural network with the addition of 3-hop
node virtual edges 2 layers are required. For SE3Set, on the other hand, only 2 layers are needed using the
explicit overlap method, and only 3 layers are needed for the implicit overlap method, even in the case where
the 3-hop nodes are not included inside the hyperedges either. This demonstrates the efficiency of SE3Set in
message passing.
N Summary Statistics
To further evaluate the performance in our different runs on different targets, we defined the summary
statistics std. MAE following Gasteiger et al. (2019) which reflects the average error compared to the standard
deviation of each target by Eq. 42,
std. MAE =1
MM/summationdisplay
m=1/parenleftigg
1
NN/summationdisplay
i=1|ˆyi(m)−y(m)
i|
σm/parenrightigg
(42)
with target index min totalMtargets, dataset size N, the predict value of the model trained by m-th target,
and the corresponding ground truth y(m)
i.
25Under review as submission to TMLR
(a) (b)
 (c)
(d) (e)
Figure 6: Comparison message passing efficiency of with GNN (graph neural networks), and the GNN
adding virtual bodes between 3-hop neighborhoods. The black line between two circles denotes the binay
edge in the graph. We consider the message passing process from the orange node to the green node
and the nodes in shaded area belongs to a same hyperedge. The red dashed arrows indicate the nodes
where the source information reaches after each passing layer. (a) The structural differences between graph
(above) and hypergraph (below). The hypergraph contains hyperedges defined by set of nodes. (b) Simple
GNN architecture. (c) The GNN architecture adding virtual bodes between 3-hop neighborhoods. (d) The
hypergraph in SE3Set builded with explicit overlap method. (e) The hypergraph in SE3Set builded with
implicit overlap method.
Furthermore, to evaluate the robustness of the performance especially the MAE, we further propose std.
MAE std. by Eq. 43, which reflects the degree of data volatility of MAE on different tasks.
std. MAE std. =/radicaltp/radicalvertex/radicalvertex/radicalbt1
MM/summationdisplay
m=1/parenleftigg
1
NN/summationdisplay
i=1|ˆyi(m)−y(m)
i|
σm−std. MAE/parenrightigg
(43)
We compute these metrics on QM9 ( M= 12), MD17 (M= 7) and MD22 ( M= 5) respectively. The OE62 is
ignored as there is only one task in its experiment. For some energy related labels ( U0,U,H,G) in QM9,
the atomic level reference energy is subtracted from the origin value to generate the ground truth. For MD17
and MD22 dataset, as the forces are calculated by the derivation of energy, which means the energy MAE
and force MAE belong to the same trained model, we calculate the metrics of energy and force separately
with different molecules as different targets. All the results are shown in Table 13. To present the results
more clearly, we also show the average value and standard deviation of origin ground truth as well, which
presented in the Table 14, 15, 16.
26Under review as submission to TMLR
Table 13: The std. MAE and std. MAE std. of different datasets calculated across different targets.
dataset QM9MD17 MD22
Energy Force Energy Force
std. MAE (%) 0.67 1.88 0.28 0.64 0.20
std. MAE std. (%). 0.82 0.28 0.13 0.12 0.05
Table 14: The average groud truth and standard deviation for different targets in QM9 dataset.
Target µ α HOMO LUMO gap R2ZPVE U0U H G C v
Unit D a3
0 eV eV eV a2
0 eV eV eV eV eVkcal
mol·K
Avg. 2.7061 75.1917 -6.5300 0.3027 6.8328 1189.5256 4.0415 -75.9233 -76.3839 -76.8206 -70.6561 31.6009
Std. 1.5304 8.1876 0.6022 1.2772 1.2931 279.7507 0.9054 10.3775 10.4693 10.5440 9.5490 4.0624
Table 15: The average groud truth and standard deviation for different molecules in MD17 dataset. Energy
(E) is in unit of kcal /mol and force (F) is in unit of kcal /(mol·Å).
Molecule Aspirin Ethanol Malonaldehyde Naphthalene Salicylic Acid Toluene Uracil
E Avg. -406737.2765 -97195.9314 -167501.8334 -241898.7881 -311033.7587 -170223.8472 -260107.3102
E Std. 5.9468 4.1920 4.1399 5.5867 5.4163 5.0996 4.9230
F Avg. 0.0000 0.0002 0.0000 -0.0001 0.0000 0.0000 0.0000
F Std. 27.9284 26.2570 28.6374 28.6544 28.5897 27.3297 30.0238
Table 16: The average groud truth and standard deviation for different molecules in MD22 dataset. Energy
(E) is in unit of kcal /mol and force (F) is in unit of kcal /(mol·Å).
Molecule Ac-Ala3-NHMe DHA Stachyose AT-AT AT-AT-CG-CG
E Avg. -620662.7117 -631480.1418 -1578838.9203 -1154896.6603 -2329950.4156
E Std. 8.2038 9.5559 13.7595 10.9584 15.7047
F Avg. 0.0000 0.0000 0.0000 0.0000 0.0000
F Std. 26.0387 25.9614 25.6038 27.9251 27.7151
27