Published in Transactions on Machine Learning Research (07/2023)
Finding and Only Finding Diﬀerential Nash Equilibria by
Both Pretending to be a Follower
Xuchan Bao jennybao@cs.toronto.edu
Department of Computer Science
University of Toronto, Vector Institute
Guodong Zhang gdzhang@cs.toronto.edu
Department of Computer Science
University of Toronto, Vector Institute
Reviewed on OpenReview: https: // openreview. net/ forum? id= igdWKxK5RZ
Abstract
Finding Nash equilibria in two-player diﬀerentiable games is a classical problem in game
theory with important relevance in machine learning. We propose double Follow-the-Ridge
(double-FTR), an algorithm that locally converges to and only to diﬀerential Nash equilibria
in general-sum two-player diﬀerentiable games. To our knowledge, double-FTR is the ﬁrst
algorithm with such guarantees for general-sum games. Furthermore, we show that by
varying its preconditioner, double-FTR leads to a broader family of algorithms with the same
convergence guarantee. In addition, double-FTR avoids oscillation near equilibria due to the
real-eigenvalues of its Jacobian at ﬁxed points. Empirically, we validate the double-FTR
algorithm on a range of simple zero-sum and general sum games, as well as simple Generative
Adversarial Network (GAN) tasks.
1 Introduction
Muchoftherecentsuccessindeeplearningcanbeattributedtotheeﬀectivenessofgradient-basedoptimization.
It is well-known that for a minimization problem, with appropriate choice of learning rates, gradient descent
enjoys convergence guarantee to local minima (Lee et al., 2016; 2019). Based on this foundational result,
an array of accelerated and higher-order methods have since been proposed and widely applied in training
neural networks (Duchi et al., 2011; Kingma & Ba, 2014; Reddi et al., 2018; Zhang et al., 2019b).
However, once we leave the realm of minimization problems and consider the multi-agent setting, the
optimization landscape becomes much more complicated. Multi-agent optimization problems arise in diverse
ﬁelds such as robotics, economics and machine learning (Foerster et al., 2016; Von Neumann & Morgenstern,
2007; Goodfellow et al., 2014; Ben-Tal & Nemirovski, 2002; Gemp et al., 2020; Anil et al., 2021).
A classical abstraction that is especially relevant for machine learning is two-player diﬀerentiable games,
where the objective is to ﬁnd global or local Nash equilibria. The equivalent of gradient descent in such a
game would be each agent applying gradient descent to minimize their own objective function. However, in
stark contrast with gradient descent in solving minimization problems, this gradient-descent-style algorithm
may converge to spurious critical points that are not Nash equilibria, and in the general-sum game case, Nash
equilibria might not even be stable critical points for this algorithm (Mazumdar et al., 2020b)!
These negative results have driven a surge of recent interest in developing other gradient-based algorithms for
ﬁnding Nash equilibria in diﬀerentiable games. Among them is Mazumdar et al. (2019), who proposed an
update algorithm whose attracting critical points are only local Nash equilibria in the special case of zero-sum
games. However, to the best of our knowledge, such guarantees have not been extended to general-sum games.
1Published in Transactions on Machine Learning Research (07/2023)
We propose double Follow-the-Ridge (double-FTR), a gradient-based algorithm for general-sum diﬀerentiable
games that locally converges to and only to diﬀerential Nash equilibria.1Double-FTR is closely related to
the Follow-the-Ridge (FTR) algorithm for Stackelberg games (Wang et al., 2019), which converges to and
only to local Stackelberg equilibria (Fiez et al., 2019). Double-FTR can be viewed as its counterpart for
simultaneous games, where each player adopts the “follower” strategy in FTR.
The rest of this paper is organized as follows. In Section 2, we give background on two-player diﬀerentiable
games and equilibrium concepts. We also explain the issues with using gradient-descent-style algorithms
on such games. In Section 3, we present the double-FTR algorithm and prove its local convergence to and
only to diﬀerential Nash equilibria. We also identify a more general class of algorithms that share these
properties. We discuss recent works directly relevant to double-FTR in Section 4 and other related work in
Section 5. In Section 6, we show empirical evidence of double-FTR’s convergence to and only to diﬀerential
Nash equilibria.
2 Background
2.1 Two-player diﬀerentiable games and equilibrium concepts
In a general-sum two-player diﬀerentiable game, player 1 aims to minimize f:Rn+m→Rwith respect
tox∈Rn, whereas player 2 aims to maximize g:Rn+m→Rwith respect to y∈Rm. Following the
notation in Mazumdar et al. (2019), we denote such a game as {(f,−g),Rn+m}. We also make the following
assumption on the twice-diﬀerentiability of fandg.
Assumption 1. ∀x∈Rn,y∈Rm,fandgare twice-diﬀerentiable, and the second derivatives are continuous.
Also,∇2
xxfand∇2
yygare invertible.
For two rational, non-cooperative players, their optimal outcome is to achieve a local Nash equilibrium (Ratliﬀ
et al., 2013).2
Deﬁnition 2.1 (Local Nash equilibrium) .A point (x∗,y∗)is a local Nash equilibrium of {(f,−g),Rn+m}if
there exists open sets Sx⊂Rn,Sy⊂Rmsuch that x∗∈Sx,y∗∈Sy, and
f(x∗,y∗)≤f(x,y∗), g(x∗,y∗)≥g(x∗,y),∀x∈Sx,∀y∈Sy.
For twice-diﬀerentiable fandg, a local Nash equilibrium at (x∗,y∗)implies (Ratliﬀ et al., 2013, Proposition 2):
(First-order condition) ∇xf(x∗,y∗) = 0and∇yg(x∗,y∗) = 0,
(Second-order necessary condition) ∇2
xxf(x∗,y∗)/followsequal0and∇2
yyg(x∗,y∗)/precedesequal0.
A closely related notion of equilibrium is the diﬀerential Nash equilibrium (DNE) (Ratliﬀ et al., 2013), which
satisﬁes a second-order suﬃcient condition for local Nash equilibrium.
Deﬁnition 2.2 (Diﬀerential Nash equilibrium) .(x∗,y∗)is a diﬀerential Nash equilibrium of {(f,−g),Rn+m}
if the following two conditions hold:
• ∇xf(x∗,y∗) = 0and∇yg(x∗,y∗) = 0,
• ∇2
xxf(x∗,y∗)/follows0and∇2
yyg(x∗,y∗)≺0.
The conditions of DNE are slightly stronger than that of local Nash equilibria in that the second-order
conditions are deﬁnite instead of semi-deﬁnite. In this paper, we focus on DNE, as they make up almost
all local Nash equilibria in the mathematical sense, and are well-suited for the analysis of second-order
algorithms.
1Slightly stronger than the local Nash equilibria, discussed in Section 2.1
2Note that local Nash equilibrium is not guaranteed to exist in nonconvex-nonconcave games ((Jin et al., 2020), Proposition
6), although the (non-)existence of local NE is out of the scope of this paper.
2Published in Transactions on Machine Learning Research (07/2023)
Discussion on Assumption 1 We assume twice-diﬀerentiability of fandgsince we are concerned with
ﬁnding DNE. As for the assumption that ∇2
xxfand∇2
yygare invertible, we need it for deriving our main
algorithm in its original form. However, as discussed in Section 3.2, with a practical implementation of our
main algorithm, the assumption on invertibility can be relaxed.
2.2 Issues with gradient-based algorithms
A natural strategy for agents to search for DNE in a diﬀerentiable game is to use gradient-based algorithms.
The simplest gradient-based algorithm is the gradient descent-ascent (GDA) (Ryu & Boyd, 2016; Zhang
et al., 2021b) (Algorithm 1) or its variants (Zhang et al., 2021a; Korpelevich, 1976; Mokhtari et al., 2020).
Algorithm 1 Gradient descent-ascent (GDA)
Require: Number of iterations T, learning rate γ
1:fort= 1,...,Tdo
2:xt+1=xt−γ∇xf(xt,yt)
3:yt+1=yt+γ∇yg(xt,yt)
4:end for
Letz=/bracketleftbiggx
y/bracketrightbigg
andγ >0be the learning rate, a gradient-based update algorithm can be written as:
zt+1=zt−γω(zt). (1)
The Jacobian of ω(z)is deﬁned as J(z) :=∂ω(z)
∂z. In the case of GDA, we have:
ωGDA(z) =/bracketleftbigg
∇xf(x,y)
−∇yg(x,y)/bracketrightbigg
,JGDA =/bracketleftbigg∇2
xxf∇2
xyf
−∇2
yxg−∇2
yyg/bracketrightbigg
.
Using the Jacobian matrix, we characterize the stability around ﬁxed points of equation 1.
Deﬁnition 2.3 ((Strictly) stable ﬁxed point) .z∗is a stable ﬁxed point of the discrete-time dynamical
system in equation 1 if
ω(z∗) =0andρ(I−γJ(z∗))≤1,
whereρ(·)denotes the spectral radius of a matrix. If we additionally have ρ(I−γJ(z∗))<1, then z∗is a
strictly stable ﬁxed point.
Strictly stable ﬁxed points are important for analysis, as they are locally asymptotically convergent (Galor,
2007), i.e. there exists an open set Szsuch that z∗∈Szandlimt→∞zt=z∗∀z0∈Sz.
A closely related concept is the locally asymptotically stable equilibrium (LASE) for the continuous-time
system ˙z=−ω(z).(Ratliﬀ et al., 2013).
Deﬁnition 2.4 (Locally asymptotically stable equilibrium (LASE)) .z∗is a locally asymptotically stable
equilibrium of the continuous-time dynamics ˙z=−ω(z)if
ω(z∗) =0and Re(λ)>0for∀λ∈spec(J(z∗)),
where Re(·)denotes the real part of a complex number, and spec(·)returns the spectrum (i.e. the set of
eigenvalues) of a matrix.
Note that in the limit γ→0, strictly stable ﬁxed points of GDA are equivalent to LASE of ˙z=−ωGDA(z).
In this paper, we prove convergence results in discrete-time (using Deﬁnition 2.3), but we often provide
intuition using continuous-time concepts such as LASE.
Unfortunately, GDA is not guaranteed to converge to DNE, nor are DNE necessarily (strictly) stable ﬁxed
points of the GDA dynamics. The relationship between the LASE of ˙z=−ωGDA(z)and DNE is shown in
the Venn diagrams in Figure 1. Below, we give a few examples to help build intuition.
3Published in Transactions on Machine Learning Research (07/2023)
(a)General-sum
 (b)Zero-sum
Figure 1: Venn diagrams showing the relationship between the set of locally asymptotically stable equilibria (LASE)
of the GDA ﬂow and the set of diﬀerential Nash equilibria (DNE) in two-player diﬀerentiable games. Note that for
general-sum games, there exist DNE that are unstable for GDA ﬂow.
Zero-sum games Even in the special case of zero-sum games ( g=f), GDA dynamics can still have stable
ﬁxed points that are not DNE (Daskalakis & Panageas, 2018; Mazumdar et al., 2020b). In Figure 2, we
demonstrate the failure modes of GDA in zero-sum games. In Figure 2a, GDA converges to a spurious
strictly stable ﬁxed point which is not DNE (corresponding to the pink areas in Figure 1). In 2b, GDA fails
to converge to the unique DNE (Hsieh et al., 2020). Instead, it goes into a limit cycle, due to the strong
rotation introduced by large complex parts in its Jacobian eigenvalues. We stress that these pathologies are
not limited to GDA, but common for many other ﬁrst-order algorithms (Wang et al., 2019).
10.0
 7.5
 5.0
 2.5
 0.0 2.5 5.0 7.5 10.0
x8
6
4
2
02468yNon-Nash LASE
GDA
OGDA
EG
double-FTR
init
(a)f(x,y) =g(x,y) = 2x2+ 5xy+y2
10.0
 7.5
 5.0
 2.5
 0.0 2.5 5.0 7.5 10.0
x8
6
4
2
02468yDNE
GDA
GDA
double-FTR
init(b)f(x,y) =g(x,y) =xy+/epsilon1(x2−y4)
Figure 2: Two examples of GDA failure modes in ﬁnding DNE in zero-sum games. (a) GDA converges to the spurious
strictly stable ﬁxed point (0,0), which is not a DNE. Other ﬁrst-order methods such as the optimistic GDA (OGDA)
and extragradient (EG) converge to the spurious ﬁxed point as well. (b) Instead of the unique DNE at (0,0), GDA
converges to a limit cycle both when initialized “inside” (green) and “outside” (blue). We use /epsilon1= 0.0001,γ= 0.01.
General-sum games In general-sum games, apart from non-DNE spurious ﬁxed points for GDA, a DNE
might not even be a stable ﬁxed point of GDA (the cyan area in Figure 1). For example, let n=m= 1,
f(x,y) =x2+ 2xyandg(x,y) =−3xy−0.5y2. It is easy to conﬁrm that (0,0)is a DNE. However,
JGDA =/bracketleftbigg2 2
3 1/bracketrightbigg
,spec(JGDA) ={4,−1}.
This means that for the GDA dynamics, (0,0)is a saddle point rather than a LASE.
4Published in Transactions on Machine Learning Research (07/2023)
3 Double Follow-the-Ridge
We propose double Follow-the-Ridge (double-FTR), an update rule for general-sum diﬀerential games that
locally converges to and only to diﬀerential Nash equilibria. The double-FTR update is shown in Algorithm 2
(the arguments xt,ytoffandgare dropped to avoid notational clutter).
Algorithm 2 Double Follow-the-Ridge
Require: Learning rate ηxandηy; number of iterations T.
1:fort= 1,...,Tdo
2:xt+1←xt−ηx∇xf−ηy(∇2
xxf)−1∇2
xyg∇yg
3:yt+1←yt+ηy∇yg+ηx(∇2
yyg)−1∇2
yxf∇xf
4:end for
Letz=/bracketleftbiggx
y/bracketrightbigg
,γ=ηxandc=ηy
ηx, we can express Algorithm 2 in vectorized form (equation 2). To simplify
the notation, we drop the subscript tforfandg.
zt+1=zt−γωFTR(zt),ωFTR(zt) =/bracketleftbiggI−(∇2
xxf)−1∇2
xyg
−(∇2
yyg)−1∇2
yxf I/bracketrightbigg/bracketleftbigg∇xf
−c∇yg/bracketrightbigg
.(2)
3.1 Local convergence of double-FTR
In this section, we give our main theoretical result. First, we introduce an additional assumption.
Assumption 2. At ﬁxed points of equation 2, JGDA(z)has full rank.
Assumption 2 ensures that in double-FTR, the additional terms in the update do not exactly cancel out the
GDA terms, which in turn ensures that ﬁxed points with double-FTR are also ﬁxed points with double-GDA.
Note a similar assumption is introduced in Mazumdar et al. (2019) Theorem 4.
Our main theoretical result is stated below.
Theorem 1. Under Assumptions 1 and 2 and with an appropriate choice of learning rate γ,z∗is a strictly
stable ﬁxed point of the double-FTR update (equation 2) if and only if it is a diﬀerential Nash equilibrium
of the game{(f,−g),Rn+m}. Furthermore, at ﬁxed points of equation 2, all eigenvalues of the Jacobian
JFTR:=∂ωFTR
∂zare real.
Intuitively, the ﬁrst part of the theorem classiﬁes the strictly stable ﬁxed points of double-FTR, which leads
to local convergence to and only to DNE (Corollary 1). The second part ensures that there is no rotation
caused by complex eigenvalues in the neighbourhood of the DNEs. This is beneﬁcial, as eigenvalues with
large imaginary parts can often cause instability (such as the oscillation when training GANs) (Mescheder
et al., 2017; Balduzzi et al., 2018). We defer the proof of Theorem 1 to Appendix A.
Corollary 1 (Local convergence) .Letz∗be a DNE of the game {(f,−g),Rn+m}. Under Assumptions 1
and 2 and with an appropriate choice of learning rate γ, there exists an open set Sz⊂Rn+mwhere z∗∈Sz,
such that when following equation 2, ∀z0∈Sz,limt→∞zt→z∗.
Proof.The proof follows naturally by combining Theorem 1 with the local convergence of strictly stable ﬁxed
points (Galor (2007), Proposition 1.9).
To the best of our knowledge, double FTR is the ﬁrst algorithm with such local convergence result for
general-sum games.
3.2 General preconditioners
In the following remark, we show that double-FTR can be generalized to include a whole family of algorithms.
5Published in Transactions on Machine Learning Research (07/2023)
Remark 1. Theorem 1 applies to a more general version of the double FTR algorithm. In particular, we
can generalize equation 2 to allow a broader class of “preconditioners”:
zt+1=zt−γ˜ωFTR(zt),˜ωFTR(z) =/bracketleftbigg
Px0
0−Py/bracketrightbigg
J/latticetop
GDA(zt)/bracketleftbigg
∇xf
−c∇yg/bracketrightbigg
, (3)
where Px,Pyare continuous functions of x,yrespectively, which satisfy Px/follows0⇐⇒∇2
xxf/follows0and
Py≺0⇐⇒∇2
yyg≺0.
Equation 2 corresponds to the special case of Px= (∇2
xxf)−1,Py= (∇2
yyg)−1. The proof for Theorem 1
directly applies to the case of general preconditioners in Remark 1.
Remark 1 provides intuition on the convergence properties of double-FTR. Without the preconditioner Px
andPy, double-FTR reduces to Hamiltonian gradient descent (Mescheder et al., 2017; Balduzzi et al., 2018;
Loizou et al., 2020; Abernethy et al., 2021), which has spurious non-Nash equilibria. It is the introduction of
the preconditioner that enables strictly stable ﬁxed points to satisfy the second-order condition of DNE.
Remark 1 also sheds light on how to derive a more practical algorithm. Naively implementing Algorithm 2
might cause instability when ∇2
xxfand∇2
yygare near singular. In practice, we use (∇2
xxf∇2
xxf+λI)−1∇2
xxf
instead of (∇2
xxf)−1in Algorithm 2 (where a small λ>0is the damping parameter). Note that this also
allows us to drop the assumption on the invertibility of ∇2
xxfand∇2
yygin Assumption 1.
3.3N-player games
Algorithm 2 naturally extends to n-player games. The algorithm and theoretical results for n-player games
are exactly analogous to the two-player setting. The n-player Follow-the-Ridge algorithm and its convergence
properties are shown in Appendix B.
4 Connection with other algorithms
Mazumdar et al. (2019) proposed local symplectic surgery (LSS) – a gradient-based algorithm whose LASE are
exactly DNE in two-player zero-sum games. LSS avoids oscillatory behaviour at DNE, similar to double-FTR.
Compared to LSS, double-FTR appears to have a simpler form and enables a broader family of algorithms
with such local convergence result in general-sum games.
The Follow-the-Ridge (FTR) algorithm (Wang et al., 2019) is closely related to our proposed double-FTR.
FTR was proposed for two-player sequential games and is guaranteed to converge to and only to local minimax
for zero-sum and Stackelberg equilibria for general-sum sequential games. FTR applies a gradient correction
term on the follower in a sequential game, so that the agents approximately follow a ridge in the landscape of
the objective function. The double-FTR can be viewed as a counterpart of FTR for simultaneous games.
The update rule of double-FTR resembles that of FTR, with the gradient modiﬁcation term applied on both
players.
Another related algorithm is the Hamiltonian gradient descent (HGD) (Mescheder et al., 2017; Balduzzi et al.,
2018; Loizou et al., 2020; Abernethy et al., 2021). HGD performs gradient-descent on the Hamiltonian, or
the squared norm of the gradient. HGD is guaranteed to converge, as it is essentially a minimization problem.
However, in general it may have spurious non-Nash equilibria points. Interestingly, our double-FTR can be
viewed as a preconditioned HGD.
5 Related work
Mazumdar et al. (2020b) introduced a general framework for competitive gradient-based learning. They
characterized DNE in terms of the critical points of the gradient algorithms. They showed the lack of
convergence of the gradient algorithm in games, which motivated the development of the double-FTR
algorithm.
6Published in Transactions on Machine Learning Research (07/2023)
Muchworkhasfocusedonimprovingthedynamicsinﬁndingstableﬁxedpoints, whichiscrucialinapplications
such as GANs, where oscillation caused by eigenvalues with zero real parts and large imaginary parts in the
gradient Jacobian can lead to training instability. Mescheder et al. (2017) proposes Consensus Optimization,
which encourages agreement between the two players by introducing a regularization term in the objectives of
both players. The regularization term results in a more negative real-part for the eigenvalues of the gradient
Jacobian, therefore reduces oscillation and allows larger learning rates. Balduzzi et al. (2018); Gemp &
Mahadevan (2018) proposes Symplectic Gradient Adjustment (SGA), which decomposes the gradient Jacobian
into symmetric (potential) and asymmetric (Hamiltonian) parts and adds a gradient adjustment term for
rapid convergence to stable ﬁxed points. Schäfer & Anandkumar (2019) proposes Competitive Gradient
Descent (CGD), whose update is given by the Nash equilibrium of a regularized bilinear approximation
of the original game. Compared to other methods, CGD has the advantage of not needing to adapt step
size when the interaction strength changes between players. Many other methods have been proposed with
diﬀerent strategies for predicting other agents’ moves, such as Learning with Opponent Learning Awareness
(LOLA) (Foerster et al., 2016) and optimistic gradient descent-ascent (OGDA) (Popov, 1980; Rakhlin &
Sridharan, 2013; Daskalakis et al., 2018; Mertikopoulos et al., 2018). However, none of these existing methods
address the problem of spurious (i.e. non-Nash) stable ﬁxed points.
A separate but related line of research is on bilevel optimization. Bilevel optimization involves nested
objectives: the upper-level objective depends on the solution of the lower-level problem. It has wide
applications such as hyperparameter optimization and GAN training (Franceschi et al., 2018; Goodfellow
et al., 2014). One approach to bilevel optimization is based on implicit diﬀerentiation (Ochs et al., 2015;
Pedregosa, 2016; Lorraine et al., 2020). Implicit diﬀerentiation is closely related to the original Follow-the-
Ridge algorithm (Wang et al., 2019), which tackles bilevel optimization from a game-theoretic perspective.
Compared to simultaneous games (this paper), bilevel optimization has diﬀerent notions of equilibrium, such
as the local minimax (Jin et al., 2020) and Stackelberg equilibrium (Von Stackelberg, 2010).
6 Experiments
We conduct simple experiments to demonstrate the implications of our theoretical results. First, through a
2-D toy example, we show that double-FTR converges to DNE and successfully avoids spurious non-DNE
ﬁxed points, as predicted by Theorem 1. Then, we demonstrate that in a general-sum linear quadratic game,
double-FTR is able to converge to DNE that naive policy gradient avoids. Lastly, we show that double-FTR
can be scaled up and applied to train Generative Adversarial Networks (Goodfellow et al., 2014). Unlike
GDA which suﬀers from severe mode collapse, double-FTR recovers all the modes and learns a distribution
that closely matches the target.
6.1 2-D toy example
We consider the zero-sum game {f,−f},R2with the following 2-D function (same as in Mazumdar et al.
(2019)):
f(x,y) =e−0.01(x2+y2)/parenleftbig
(0.3x2+y)2+ (0.5y2+x)2/parenrightbig
.
This function has several strictly stable ﬁxed points for the GDA dynamics, among which some are DNE and
some are not. As shown in Figure 3, while GDA may converge to ﬁxed points that are not DNE, double-FTR
avoids such spurious ﬁxed points. Also, in the neighbourhood of the DNE, GDA exhibits oscillatory behaviour
due to complex eigenvalues of the Jacobian matrix. In contrast, the double-FTR does not have oscillatory
behaviour near the DNE. For reference, we also show the trajectories of the Local Symplectic Surgery (LSS).
In this experiment, LSS has similar convergence properties – it avoids spurious ﬁxed points and does not
have oscillatory behaviour near the DNE.
Observing the optimization trajectory, we note that double-FTR can approach a non-Nash spurious ﬁxed
point, then steer away without converging to it. In this paper, we do not discuss the convergence rate
of double-FTR, because the dynamics of a multi-player game is much more complicated than that of a
7Published in Transactions on Machine Learning Research (07/2023)
20
 10
 0 10 20
x20
10
01020yDNE
Non-Nash LASE
init
GDA
double-FTR
LSS
12
 11
x789y
5
 0
x10
5
0y
Figure 3: Left: evolution of GDA and double-FTR in the 2-D toy example from multiple initial points. Middle:
zoom-in near a diﬀerential DNE point. Right: zoom-in near a non-Nash LASE for the GDA algorithm.
minimization problem, making the convergence rate a less meaningful metric. Instead, we focus on the local
convergence itself and the characterization of the ﬁxed points.
6.2 General-sum linear quadratic game
The linear quadratic (LQ) game is a classic problem in multi-agent learning. It is an extension of the famous
linear quadratic regulator (LQR) problem of optimal control to the multi-agent setting. Just as LQR being a
simple yet important benchmark problem for studying properties of reinforcement learning algorithms, the
LQ game provides valuable insights to multi-agent RL algorithms (Fazel et al., 2018; Zhang et al., 2019a).
Consider the discrete-time linear dynamical system, where z∈Rdzis the state, and two players provide
control inputs u∈Rduandv∈Rdvrespectively.
zt+1=Azt+Buut+Bvvt,z0∼p(z0)
Each player adopts a linear state-feedback policy: ut=−Kuzt,vt=−Kvzt, where the parameters
Ku∈Rdu×dz,Kv∈Rdv×dzare to be determined by optimization. In a general-sum LQ game, each player
aims to ﬁnd their corresponding policy parameters Kthat minimizes their individual quadratic loss function
f(shown in equation 4, fv(Ku,Kv)deﬁned analogously using QvandRv).
fu(Ku,Kv) =Ez0∼p(z0)/bracketleftbigg∞/summationdisplay
t=0z/latticetop
tQuzt+u/latticetop
tRuut/bracketrightbigg
(Qu/follows0,Ru/follows0) (4)
Despite their simplicity, LQ games are challenging to optimize, because even though the loss functions are
quadratic in the states and actions, they are notconvex with respect to the player parameters KuandKv.
One straightforward algorithm choice is GDA, which we refer to as the naive policy gradient method, as the
gradients are computed using the policy gradient algorithm (Williams, 1992). Importantly, Mazumdar et al.
(2020a) show that in general sum LQ games, naive policy gradient almost surely avoids some Nash equilibria.
We demonstrate in general-sum LQ games, double-FTR is able to ﬁnd DNE that are avoided by naive
policy gradient. We use a setting mentioned in Mazumdar et al. (2020a), where dz= 2,du=dv= 1,
Ru=Rv= 0.01, and
A=/bracketleftbigg0.511 0.064
0.533 0.993/bracketrightbigg
,Bu=/bracketleftbigg1
1/bracketrightbigg
,Bv=/bracketleftbigg0
1/bracketrightbigg
,Qu=/bracketleftbigg0.01 0
0 1/bracketrightbigg
,Qv=/bracketleftbigg1 0
0 0.147/bracketrightbigg
.
The initial state z0is set to/bracketleftbig1 1/bracketrightbig/latticetopor/bracketleftbig1 1.1/bracketrightbig/latticetopwith equal probability.
8Published in Transactions on Machine Learning Research (07/2023)
Ku_10.450.50
Ku_20.375
0.400Steps1e4
0.20.40.60.81.0
Kv_10.04
0.06Kv_20.580.60Steps1e4
0.20.40.60.81.0
Figure 4: Evolution of the loss landscape of a general-sum linear quadratic game when optimized by double-FTR. We
visualize two 2D slices (Ku,1,Ku,2)and (Kv,1,Kv,2)and the loss functions fuandfvrespectively. Yellow represents
higher value and purple represents lower value. As seen on the top levels of the illustration, the loss landscape is
“bowl-shaped” at convergence, conﬁrming that double-FTR solution satisﬁes the second-order conditions for DNE.
Ku_10.13
0.14
0.15
Ku_20.025
0.000Steps1e4
0.20.40.60.81.0
Slice 1: along Ku,1 and Ku,2Ku_10.13
0.14
0.15
Kv_20.99
1.00
1.01Steps1e4
0.20.40.60.81.0
Slice 2: along Ku,1 and Kv,2
Kv_10.39
0.40
0.41 Ku_20.025
0.000Steps1e4
0.20.40.60.81.0
Slice 3: along Kv,1 and Ku,2Kv_10.39
0.40
0.41 Kv_20.99
1.00
1.01Steps1e4
0.20.40.60.81.0
Slice 4: along Kv,1 and Kv,2
(a)Double-FTR
Ku_101
Ku_20.25
0.000.25Steps1e4
0.20.40.60.81.0
Slice 1: along Ku,1 and Ku,2Ku_10
1 Kv_20.5
1.0
1.5Steps1e4
0.20.40.60.81.0
Slice 2: along Ku,1 and Kv,2
Kv_10.0
2.5Ku_20.25
0.00
0.25Steps1e4
0.20.40.60.81.0
Slice 3: along Kv,1 and Ku,2Kv_10.0
2.5Kv_20.5
1.0
1.5Steps1e4
0.20.40.60.81.0
Slice 4: along Kv,1 and Kv,2 (b)Naive policy gradient
Figure 5: For the general-sum linear quadratic game, we visualize how the Jacobian evolves during training. At
each step, we visualize (z−zt)/latticetopJGDA,t(z−zt), the quadratic function deﬁned by the current JGDA, centered at the
current weight values zt. Due to the diﬃculty of directly visualizing a 4D function, we plot diﬀerent slices of the
quadratic function instead. Each contour plot shows an axis-aligned 2D slice, with the remaining 2 dimensions ﬁxed
at their corresponding values at zt. Yellow represents higher value and purple represents lower value. In both (a) and
(b), the weights are initialized near the same DNE that is an unstable ﬁxed point for GDA (naive policy gradient)
dynamics, which is shown as saddle points in some 2D slices. (a): using double-FTR, the weights converge to the
DNE. At convergence, the saddle points of (z−zt)/latticetopJGDA,t(z−zt)still remain. (b): the gradient method avoids this
unstable DNE, and converges to a stable, farther away ﬁxed point instead.
Figure 4 and 5 shows an instance where the double-FTR is able to converge to a DNE, but naive policy
gradient fails to. For both algorithms, we use the same initial policy parameters KuandKv. Figure 4
visualizes the loss landscape for fu(Ku,Kv)andfv(Ku,Kv)when optimized by double-FTR. It conﬁrms
that the solution double-FTR converges to is indeed a DNE (the second-order condition in Deﬁnition 2.2).
Figure 5a visualizes the local vector ﬁeld Jacobian (i.e. JGDA) and shows that the Jacobian contains negative
eigenvalues, which makes it a saddle point for the naive policy gradient method. Indeed, naive policy gradient
9Published in Transactions on Machine Learning Research (07/2023)
(shown in Figure 5b) avoids this DNE. Instead, it eventually ﬁnds another DNE that is stable ﬁxed point for
naive policy gradient.
6.3 Generative Adversarial Networks
The Generative Adversarial Network (GAN) (Goodfellow et al., 2014) is a popular deep learning application for
two-player games. The goal is to ﬁnd the DNE where the generator perfectly matches the target distribution,
and the discriminator is completely fooled by the generator.
In this experiment, we use the GAN framework to learn mixture of Gaussians (MoG). We use the original
saturating loss function. Both the generator and the discriminator are multi-layer perceptrons with 2 hidden
layers and 64 hidden units in each layer. With neural networks, directly implementing the Hessian would be
computationally ineﬃcient or infeasible. Instead, we use conjugate gradient to approximate vector products
with the Hessian inverse. Details of the experiments can be found in Appendix C.
As shown in Figure 6 and 7, we apply GDA and double-FTR to learn MoG in 1D and 2D. In both cases,
GDA gets stuck at a spurious equilibrium and suﬀers from mode collapse. In contrast, double-FTR recovers
all the modes, and the generated distribution closely matches the target.
−6 −4 −2 0 2 4 60.000.050.100.150.20Target Density
−6 −4 −2 0 2 4 60.000.050.100.150.200.25Generator Density (GDA)
−6 −4 −2 0 2 4 60.000.050.100.150.20Generator Density (double-FTR)
Figure 6: Mixture of Gaussians in 1D. Left: ground-truth. Middle: generator distribution learned by GDA. Right:
generator distribution learned by double-FTR.
−4 −2 0 2 4−4−2024Target
−4 −2 0 2 4−4−2024Iteration 0
−4 −2 0 2 4−4−2024Iteration 1000
−4 −2 0 2 4−4−2024Iteration 5000
−4 −2 0 2 4−4−2024Iteration 10000
−4 −2 0 2 4−4−2024Iteration 30000
−4 −2 0 2 4−4−2024Iteration 50000
−4 −2 0 2 4−4−2024Target
−4 −2 0 2 4−4−2024Iteration 0
−4 −2 0 2 4−4−2024Iteration 1000
−4 −2 0 2 4−4−2024Iteration 5000
−4 −2 0 2 4−4−2024Iteration 10000
−4 −2 0 2 4−4−2024Iteration 30000
−4 −2 0 2 4−4−2024Iteration 50000
Figure 7: Mixure of Gaussians in 2D. Top: GDA suﬀers from mode collapse. Bottom: the generator distribution
learned by double-FTR recovers all the modes.
7 Conclusion
We propose double Follow-the-Ridge (double-FTR), a gradient-based algorithm for ﬁnding diﬀerential Nash
equilibria in diﬀerentiable games. We prove that under certain regularity assumptions, double-FTR locally
converges to and only to diﬀerential Nash equilibria in the general-sum games, and avoids oscillation in the
neighbourhood of ﬁxed points. Furthermore, we remark that by varying the preconditioner, double-FTR
leads to a broader family of algorithms that share the same convergence guarantee. Finally, we empirically
verify the eﬀectiveness of double-FTR in ﬁnding and only ﬁnding diﬀerential Nash equilibria across a broad
range of problems.
10Published in Transactions on Machine Learning Research (07/2023)
Acknowledgments
The authors thank Roger Grosse and Jakob Foerster for their helpful discussions and valuable feedback.
References
Jacob Abernethy, Kevin A Lai, and Andre Wibisono. Last-iterate convergence rates for min-max optimization:
Convergence of hamiltonian gradient descent and consensus optimization. In Algorithmic Learning Theory ,
pp. 3–47. PMLR, 2021.
Cem Anil, Guodong Zhang, Yuhuai Wu, and Roger Grosse. Learning to give checkable answers with
prover-veriﬁer games. arXiv preprint arXiv:2108.12099 , 2021.
David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Graepel. The
mechanics of n-player diﬀerentiable games. In International Conference on Machine Learning , pp. 354–363.
PMLR, 2018.
Aharon Ben-Tal and Arkadi Nemirovski. Robust optimization–methodology and applications. Mathematical
programming , 92(3):453–480, 2002.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin,
George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable
transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax .
Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in min-max
optimization. Advances in neural information processing systems , 31, 2018.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with optimism.
InInternational Conference on Learning Representations , 2018.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of machine learning research , 12(7), 2011.
Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gradient methods
for the linear quadratic regulator. In International Conference on Machine Learning , pp. 1467–1476. PMLR,
2018.
Tanner Fiez, Benjamin Chasnov, and Lillian J Ratliﬀ. Convergence of learning dynamics in stackelberg
games.arXiv preprint arXiv:1906.01217 , 2019.
JakobFoerster, IoannisAlexandrosAssael, NandoDeFreitas, andShimonWhiteson. Learningtocommunicate
with deep multi-agent reinforcement learning. Advances in neural information processing systems , 29, 2016.
LucaFranceschi, PaoloFrasconi, SaverioSalzo, RiccardoGrazzi, andMassimilianoPontil. Bilevelprogramming
for hyperparameter optimization and meta-learning. In International Conference on Machine Learning , pp.
1568–1577. PMLR, 2018.
Oded Galor. Discrete dynamical systems . Springer Science & Business Media, 2007.
Ian Gemp and Sridhar Mahadevan. Global convergence to the equilibrium of gans using variational inequalities.
arXiv preprint arXiv:1808.01531 , 2018.
Ian Gemp, Brian McWilliams, Claire Vernade, and Thore Graepel. Eigengame: Pca as a nash equilibrium.
InInternational Conference on Learning Representations , 2020.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing
systems, 27, 2014.
11Published in Transactions on Machine Learning Research (07/2023)
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM , 63(11):
139–144, 2020.
Ya-Ping Hsieh, Panayotis Mertikopoulos, and Volkan Cevher. The limits of min-max optimization algorithms:
convergence to spurious non-critical sets. arXiv preprint arXiv:2006.09065 , 2020.
Chi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimality in nonconvex-nonconcave minimax
optimization? In International conference on machine learning , pp. 4880–4889. PMLR, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Galina M Korpelevich. The extragradient method for ﬁnding saddle points and other problems. Matecon, 12:
747–756, 1976.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only converges to
minimizers. In Conference on learning theory , pp. 1246–1257. PMLR, 2016.
Jason D Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael I Jordan, and Benjamin Recht.
First-order methods almost always avoid strict saddle points. Mathematical programming , 176(1):311–337,
2019.
Nicolas Loizou, Hugo Berard, Alexia Jolicoeur-Martineau, Pascal Vincent, Simon Lacoste-Julien, and Ioannis
Mitliagkas. Stochastic hamiltonian gradient methods for smooth games. In International Conference on
Machine Learning , pp. 6370–6381. PMLR, 2020.
Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit
diﬀerentiation. In International Conference on Artiﬁcial Intelligence and Statistics , pp. 1540–1552. PMLR,
2020.
James Martens et al. Deep learning via hessian-free optimization. In ICML, volume 27, pp. 735–742, 2010.
Eric Mazumdar, Lillian J Ratliﬀ, Michael I Jordan, and S Shankar Sastry. Policy-gradient algorithms have
no guarantees of convergence in linear quadratic games. In AAMAS, 2020a.
Eric Mazumdar, Lillian J Ratliﬀ, and S Shankar Sastry. On gradient-based learning in continuous games.
SIAM Journal on Mathematics of Data Science , 2(1):103–131, 2020b.
Eric V Mazumdar, Michael I Jordan, and S Shankar Sastry. On ﬁnding local nash equilibria (and only local
nash equilibria) in zero-sum games. arXiv preprint arXiv:1901.00838 , 2019.
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar, and
Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile.
InInternational Conference on Learning Representations , 2018.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. Advances in neural
information processing systems , 30, 2017.
Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. A uniﬁed analysis of extra-gradient and optimistic
gradient methods for saddle point problems: Proximal point approach. In International Conference on
Artiﬁcial Intelligence and Statistics , pp. 1497–1507. PMLR, 2020.
Peter Ochs, René Ranftl, Thomas Brox, and Thomas Pock. Bilevel optimization with nonsmooth lower level
problems. In Scale Space and Variational Methods in Computer Vision: 5th International Conference,
SSVM 2015, Lège-Cap Ferret, France, May 31-June 4, 2015, Proceedings 5 , pp. 654–665. Springer, 2015.
Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International conference on
machine learning , pp. 737–746. PMLR, 2016.
12Published in Transactions on Machine Learning Research (07/2023)
Leonid Denisovich Popov. A modiﬁcation of the arrow-hurwicz method for search of saddle points. Mathe-
matical notes of the Academy of Sciences of the USSR , 28(5):845–848, 1980.
Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Conference on
Learning Theory , pp. 993–1019. PMLR, 2013.
Lillian J Ratliﬀ, Samuel A Burden, and S Shankar Sastry. Characterization and computation of local nash
equilibria in continuous games. In 2013 51st Annual Allerton Conference on Communication, Control, and
Computing (Allerton) , pp. 917–924. IEEE, 2013.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In International
Conference on Learning Representations , 2018.
Ernest K Ryu and Stephen Boyd. Primer on monotone operator methods. Appl. Comput. Math , 15(1):3–43,
2016.
Florian Schäfer and Anima Anandkumar. Competitive gradient descent. Advances in Neural Information
Processing Systems , 32, 2019.
Tijmen Tieleman, Geoﬀrey Hinton, et al. Lecture 6.5-rmsprop: Divide the gradient by a running average of
its recent magnitude. COURSERA: Neural networks for machine learning , 4(2):26–31, 2012.
John Von Neumann and Oskar Morgenstern. Theory of games and economic behavior . Princeton university
press, 2007.
Heinrich Von Stackelberg. Market structure and equilibrium . Springer Science & Business Media, 2010.
Yuanhao Wang, Guodong Zhang, and Jimmy Ba. On solving minimax optimization locally: A follow-the-ridge
approach. In International Conference on Learning Representations , 2019.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.
Reinforcement learning , pp. 5–32, 1992.
Guodong Zhang, Xuchan Bao, Laurent Lessard, and Roger Grosse. A uniﬁed analysis of ﬁrst-order methods
for smooth games via integral quadratic constraints. Journal of Machine Learning Research , 22:1–39, 2021a.
Guodong Zhang, Yuanhao Wang, Laurent Lessard, and Roger Grosse. Near-optimal local convergence of
alternating gradient descent-ascent for minimax optimization. arXiv preprint arXiv:2102.09468 , 2021b.
Kaiqing Zhang, Zhuoran Yang, and Tamer Basar. Policy optimization provably converges to nash equilibria
in zero-sum linear quadratic games. Advances in Neural Information Processing Systems , 32, 2019a.
Michael Zhang, James Lucas, Jimmy Ba, and Geoﬀrey E Hinton. Lookahead optimizer: k steps forward, 1
step back. Advances in Neural Information Processing Systems , 32, 2019b.
13