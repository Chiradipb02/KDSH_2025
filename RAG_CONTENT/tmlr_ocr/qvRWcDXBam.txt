Published in Transactions on Machine Learning Research (March/2023)
Containing a spread through sequential learning: to exploit
or to explore?
Xingran Chen xingranc@seas.upenn.edu
Electrical and Systems Engineering Department
University of Pennsylvania
Hesam Nikpey hesam@seas.upenn.edu
Computer and Information Science Department
University of Pennsylvania
Jungyeol Kim jungyeol@alumni.upenn.edu
JPMorgan Chase & Co.
Saswati Sarkar swati@seas.upenn.edu
Electrical and Systems Engineering Department
University of Pennsylvania
Shirin Saeedi-Bidokhti saeedi@seas.upenn.edu
Computer and Information Science Department
University of Pennsylvania
Reviewed on OpenReview: https: // openreview. net/ pdf? id= qvRWcDXBam
Abstract
The spread of an undesirable contact process, such as an infectious disease (e.g. COVID-
19), is contained through testing and isolation of infected nodes. The temporal and spatial
evolution of the process (along with containment through isolation) render such detection
as fundamentally diﬀerent from active search detection strategies. In this work, through an
active learning approach, we design testing and isolation strategies to contain the spread and
minimize the cumulative infections under a given test budget. We prove that the objective
can be optimized, with performance guarantees, by greedily selecting the nodes to test.
We further design reward-based methodologies that eﬀectively minimize an upper bound
on the cumulative infections and are computationally more tractable in large networks.
These policies, however, need knowledge about the nodes’ infection probabilities which are
dynamically changing and have to be learned by sequential testing. We develop a message-
passing framework for this purpose and, building on that, show novel tradeoﬀs between
exploitation of knowledge through reward-based heuristics and exploration of the unknown
through a carefully designed probabilistic testing. The tradeoﬀs are fundamentally distinct
fromtheclassicalcounterpartsunderactivesearchormulti-armedbanditproblems(MABs).
We provably show the necessity of exploration in a stylized network and show through
simulations that exploration can outperform exploitation in various synthetic and real-data
networks depending on the parameters of the network and the spread.
1 Introduction
We consider learning and decision making in networked systems for processes that evolve both temporally
and spatially. An important example in this class of processes is COVID-19 infection. It evolves in time (e.g.
through diﬀerent stages of the disease for an infected individual) and over a contact network and its spread
can be contained by testing and isolation. Public health systems need to judiciously decide who should be
1Published in Transactions on Machine Learning Research (March/2023)
tested and isolated in presence of limitations on the number of individuals who can be tested and isolated
on a given day.
Most existing works on this topic have investigated the spread of COVID-19 through dynamic systems
such SIR models and their variants [1, 2, 3, 4, 5, 6]. These models are made more complex to ﬁt the real
data in [7, 8, 9, 10, 11, 12]. Estimation of the model parameters by learning-based methods are considered
and veriﬁed by real data in [13, 14, 15, 16, 17, 18]. Other attributes such as lockdown policy [19], multi-
wave prediction [20], herd immunity threshold [21] are also considered by data-driven experiments. These
works mostly focus on the estimation of model parameters thorough real data, and aim to make a more
accurate prediction of the spread. None of them, however, consider testing and isolation policies. Our work
complements these investigations by designing sequential testing and isolation policies in order to minimize
the cumulative infections. For this purpose, we have assumed full statistical knowledge of the spread model
and the underlying contact network and we are not concerned with prediction and estimation of model
parameters.
Designing optimal testing and control policies in dynamic networked systems often involves computational
challenges. These challenges have been alleviated in control literature by capturing the spread through dif-
ferential equations [22, 23, 24, 25, 26]. The diﬀerential equations rely on classical mean-ﬁeld approximations,
considering neighbors of each node as “socially averaged hypothetical neighbors”. Reﬁnements of the mean-
ﬁeld approximations such as pair approximation [27], degree-based approximation [28], meta-population
approximation [29] etc, all resort to some form of averaging of neighborhoods or more generally groups of
nodes. The averaging does not capture the heterogeneity of a real-world complex social network and in
eﬀect disregards the contact network topology. But, in practice, the contact network topologies are often
partially known, for example, from contact tracing apps that individuals launch on their phones. Thus test-
ing and control strategies must exploit the partial topological information to control the spread. The most
widely deployed testing and control policy, the (forward and backward) contact tracing (and its variants)
[30, 31, 32, 33, 34, 35, 36, 37, 38], relies on partial knowledge of the network topology (ie, the neighbors
of infectious nodes who have been detected), and therefore does not lend itself to mean-ﬁeld analysis. Our
proposed framework considers both the SIR evolution of the disease for each node and the spread of the
disease through a given network.
The following challenges arise in the design of intelligent testing strategies if one seeks to exploit the spatio-
temporal evolution of the disease process and comply with limited testing budget. Observing the state of a
node at time twill provide information about the state of (i) the node in time t+ 1and (ii) the neighbors of
the node at time t,t+ 1,.... This is due to the inherent correlation that exists between states of neighboring
nodes because an infectious disease spreads through contact. Thus, testing has a dual role. It has to both
detect/isolateinfectednodesandlearnthespreadinvariouslocalitiesofthenetwork. Thespreadcanoftenbe
silent: an undetected node (that may not be particularly likely to be infected based on previous observations)
can infect its neighbors. Thus, testing nodes that do not necessarily appear to be infected may lead to timely
discovery of even larger clusters of infected nodes waiting to explode. In other words, there is an intrinsic
tradeoﬀ between exploitation of knowledge vs. exploration of the unknown . Exploration vs. exploitation
tradeoﬀs were originally studied in classical multi-armed bandit (MAB) problems where there is the notion
of a single optimal arm that can be found by repeating a set of ﬁxed actions [39, 40, 41]. MAB testing
strategies have also been designed for exploring partially observable networks [42]. Our problem diﬀers from
what is mainly studied in the MAB literature because (i) the number of arms (potential infected nodes) is
time-variant and actions cannot be repeated; (ii) the exploration vs. exploitation tradeoﬀ in our context
arises due to lack of knowledge about the time-evolving set of infected nodes, rather than lack of knowledge
about the network or the process model and its parameters.
Note that contact tracing policies are in a sense exploitation policies: upon ﬁnding positive nodes, they
exploit that knowledge and trace the contacts. While relatively practical, they have two main shortcomings,
as implemented today: (i) They are not able to prioritize nodes based on their likelihood of being infected
(beyond the coarse notion of contact or lack thereof). For example, consider an infectious node that has
two neighbors, with diﬀerent degrees. Under current contact tracing strategies, both neighbors have the
same status. But in order to contain the spread as soon as possible, the node with a large degree should
be prioritized for testing. A similar drawback becomes apparent if the neighbors themselves have a diﬀerent
2Published in Transactions on Machine Learning Research (March/2023)
number of infectious neighbors; one with a larger number of infectious neighbors should be prioritized for
testing, but current contact tracing strategies accord both the same priority. (ii) Contact tracing strategies
do not incorporate any type of exploration. This may be a fundamental limitation of contact tracing. [38] has
shown that, with high cost, contact tracing policies perform better when they incorporate exploration (active
case ﬁnding). In contrast, our work provides a probabilistic framework to not only allow for exploitation in
a fundamental manner but also to incorporate exploration in order to minimize the number of infections.
Finally, our problem is also related to active search in graphs where the goal is to test/search for a set of
(ﬁxed) target nodes under a set of given (static) similarity values between pairs of nodes [43, 44, 45, 46]. But
the target nodes in these works are assumed ﬁxed, whereas the target is dynamic in our setting because the
infection spreads over time and space (i.e, over the contact network). Thus, a node may need to be tested
multiple times. The importance of exploitation/exploration is also known, implicitly and/or explicitly, in
various reinforcement learning literature [47, 48, 49].
We now distinguish our work from testing strategies that combine exploitation and exploration in some
form [50, 51, 52]. Through a theoretical approach, [50] models the testing problem as a partially observable
Markov decision process (POMDP). An optimal policy can, in principle, be formulated through POMDP, but
such strategies are intractable in their general form (and heuristics are often far from optimal) [53, 54]. [50]
devises tractable approximate algorithms with a signiﬁcant caveat: In the design, analysis, and evaluation
of the proposed algorithms, it is assumed that at each time the process can spread only on a single random
edge of the network. This is a very special case that is hard to justify in practice and it is not clear how one
could go beyond this assumption. On the other hand, [51] proposes a heuristic by implementing classical
learning methods such as Linear support vector machine (SVM) and Polynomial SVM to rank nodes based
on a notion of risk score (constructed by real-data) while reserving a portion of the test budget for random
testing which can be understood as exploration. No spread model or contact network is assumed. [52] and
this work were done concurrently. In [52], a tractable scheme to control dynamical processes on temporal
graphs was proposed, through a POMDP solution with a combination of Graph Neural Networks (GNN) and
Reinforcement Learning (RL) algorithm. Nodes are tested based on some scores obtained by the sequential
learning framework, but no fundamental probabilities of the states of nodes were revealed. Diﬀerent from
[51, 52], our approach is model-based and we observe novel exploration-exploitation tradeoﬀs that arise not
due to a lack of knowledge about the model or network, but rather because the set of infected nodes is
unknown and evolves with time. We can also utilize knowledge about both the model and the contact
network to devise a probabilistic framework for decision making.
We now summarize the contribution of some signiﬁcant works that consider only exploitation and do not
utilize any exploration [36, 37, 38]. [36, 37] have considered a combination of isolation and contact tracing
sequential policies, and [36] has shown that the sequential strategies would reduce transmission more than
mass testing or self-isolation alone, while [37] has shown that the sequential strategies can reduce the amount
of quarantine time served and cost, hence individuals may increase participation in contact tracing, enabling
less frequent and severe lockdown measures. [38] have proposed a novel approach to modeling multi-round
network-based screening/contact tracing under uncertainty.
OurContributions Inthiswork, westudyaspreadprocesssuchasCovid-19anddesignsequentialtesting
and isolation policies to contain the spread. Our contributions are as follows.
•Formulating the spread process through a compartmental model and a given contact network, we
show that the problem of minimizing the total cumulative infections under a given test budget
reduces to minimizing a supermodular function expressed in terms of nodes’ probabilities of infection
and it thus admits a near-optimal greedy policy. We further design reward-based algorithms that
minimize an upper bound on the cumulative infections and are computationally more tractable in
large networks.
•The greedy policy and its reward-based derivatives are applicable if nodes’ probabilities of infec-
tion were known. However, since the set of infected nodes are unknown, these probabilities are
unknown and can only be learned through sequential testing . We provide a message-passing frame-
work for sequential estimation of nodes’ posterior probabilities of infection given the history of test
observations.
3Published in Transactions on Machine Learning Research (March/2023)
Figure 1: Left: Time evolution of the process per individual nodes. Right: A contact network with nodes in
states susceptible (blue), latency (pink), infectious (red), recovered (yellow).
•We argue that testing has a dual role: (i) discovering and isolating the infected nodes to contain the
spread, and (ii) providing more accurate estimates for nodes’ infection probabilities which are used
for decision making. In this sense, exploitation policies in which decision making only targets (i) can
be suboptimal. We prove in a stylized network that when the belief about the probabilities is wrong,
exploitation can be arbitrarily bad, while a policy that combines exploitation with random testing
can contain the spread. This points to novel exploitation-exploration tradeoﬀs that stem from the
lack of knowledge about the location of infected nodes, rather than the network or spread process.
•Following these ﬁndings, we propose exploration policies that test each node probabilistically ac-
cording to its reward. The core idea is to balance exploitation of knowledge (about the nodes’
infection probabilities and the resulting rewards) and exploration of the unknown (to get more ac-
curate estimates of the infection probabilities). Through simulations, we compare the performance
of exploration and exploitation policies in several synthetic and real-data networks. In particular,
we investigate the role of three parameters on when exploration outperforms exploitation: (i) the
unregulated delay , i.e., the time period when the disease spreads without intervention; (ii) the global
clustering coeﬃcient of the network, and (iii) the average shortest path length of the network. We
show that when the above parameters increase, exploration becomes more beneﬁcial as it provides
better estimates of the nodes’ probabilities of infection.
2 Modeling
To describe a spread process, we use a discrete time compartmental model [55]. Over decades, compartmental
models have been key in the study of epidemics and opinion dynamics, albeit often disregarding the network
topology. In this work, we capture the spread on a given contact network. For clarity of presentation, we
focus on a model for the spread of COVID-19. The ideas can naturally be generalized to other applications.
The main notations in the full paper are given in Table 1.
We model the progression of Covid-19 per individual, in time, through four stages or states: Susceptible ( S),
Latent (L),Infectious ( I), andRecovered ( R). Per contact, an infectious individual infects a susceptible
individual with transmission probability β. An infected individual is initially in the latent state L, subse-
quently he becomes infectious (state I), ﬁnally he recovers (state R). Fig. 1 (left) depicts the evolution. The
durations in the latent and infectious states are geometrically distributed, with means 1/λ,1/γrespectively.
We represent the state of node iat timetby random variable σi(t)and its support set X={S,L,I,R}.
We assume that the parameters β,λandγare known to the public health authority. This is a practical
assumption because the parameters can be estimated by the public health authority based on the pandenmic
data collected [56, 57, 58].
LetG(t) = (V(t),E(t))denote the contact network at time t, whereV(t)is the set of nodes/individuals, of
cardinality N(t), andE(t)is the set of edges between the nodes, describing interactions/contacts on day
t. LetV=V(0),E=E(0),G=G(0), andN=N(0). The network is time-dependent not only because
4Published in Transactions on Machine Learning Research (March/2023)
Notations Deﬁnitions
β transmission probability
1/γ mean duration in the latent state
1/λ mean duration in the infectious state
σi(t)state of node iat timet,σi(t)∈{I,S,L,R}
G(t) contact network at time t
V(t) set of nodes at time t
E(t) set of edges at time t
N(t) cardinality ofV(t)
N N=N(0)
∂i(t) neighbors of node iat timet
∂+
i(t) {i}∪∂i(t)
Yi(t) testing result of node iat timet
O(t) set of nodes tested at time t
Y(t) {Yi(t)}{i∈O(t)}
B(t) testing budget at time t
π a testing and isolation policy
Cπ(t) cumulative infections at time t
Kπ(t)set of nodes tested at time t(under policy π)
Kπ(t) Kπ(t) =|Kπ(t)|
T time horizon
vi(t) true probability vector of node i
ui(t) prior probability vector of node i
wi(t) posterior probability vector of node i
ei(t)updated posterior probability vector of node i
ri(t) rewards of selecting node iat timet
ˆri(t) estimated rewards of node iat timet
Ψi(t) Ψi(t) =O(t)∩∂+
i(t−1)
Φi(t) Φi(t) ={j|j∈∂+
k(t−1),k∈Ψi(t)}\{i}
θi(t)θi(t) =σi(t)|{Y(τ)}t−1
τ=1,θi(t)∈{I,S,L,R}
ζi(t)ζi(t) =σi(t)|{Y(τ)}t
τ=1,ζi(t)∈{I,S,L,R}
Table 1: Summary of main notations
5Published in Transactions on Machine Learning Research (March/2023)
interactions change on a daily basis, but also because nodes may be tested and isolated. If a node is tested
positive on any day t, it will be isolated immediately. If a node is isolated on any day t, we assume that it
remains in isolation until he recovers. We assume that a recovered node can not be reinfected again. Thus
a node that is isolated on any day thas no impact on the network from then onwards. Such nodes can be
regarded as “removed”. Therefore, it is removed from the contact network for all subsequent times t,t+1,....
Fig. 1 (right) depicts a contact network at a given time t.We assume that a public health authority knows
the entire contact network and decides who to test based on this information. This assumption has been
made in several other works in this genre eg in [38].
Denote the set of neighbors of node i, in dayt, by∂i(t). The state of each node at time t+ 1depends on the
state of its neighbors ∂i(t), as well as its own state in day t, as given by the following conditional probability:
Pr/parenleftbig
σi(t+ 1)|{σj(t)}j∈∂+
i(t)/parenrightbig
where∂+
i(t) =∂i(t)∪{i}.
Nodeiis tested positive on day tif it is in the infectious state ( I)1. LetYi(t)denote the test result:
Yi(t) =/braceleftbigg1σi(t) =I
0σi(t)∈{S,L,R}.(1)
We do not assume any type of error in testing and Yi(t)is hence a deterministic function of σi(t). LetO(t)
be the set of nodes that have been tested (observed) in day tand denote the network observations at timet
byY(t) ={Yi(t)}i∈O(t).
Our goal in this paper is to design testing and isolation strategies in order to contain the spread and minimize
the cumulative infections. Naturally, testing resources (and hence observations) are often limited and such
constraints make decision making challenging. Let B(t)be the maximum number of tests that could be
performed on day t, called the testing budget .B(t)can evolve based on the system necessities, e.g., in
contact tracing that is widely deployed for COVID-19, the number of tests is chosen based on the history of
observations2. Also, governments often upgrade testing infrastructure as the number of cases increase. Our
framework captures both ﬁxed and time-dependent budget B(t), but we focus on time-dependent B(t)for
simulations.
Deﬁne the cumulative infections on day t, denoted by Cπ(t), as the number of nodes who have been infected
before and including day t, whereπis the testing and isolation policy. Let Kπ(t)denote the set of tests π
performs on day t. Given a large time horizon T, our objective is:
min
πE[Cπ(T)]
s.t.|Kπ(t)|≤B(t),0≤t≤T−1.(2)
Recall that σi(t), the state of node ion dayt, is a random variable and unknown. For each node i, deﬁne a
probability vector vi(t)of size|X|, where each coordinate is the probability of the node being in a particular
state at the end of time t. The coordinates of vi(t)follow the order (I,L,R,S )and we have
vi(t) =/bracketleftbig
v(i)
x(t)/bracketrightbig
x∈X, v(i)
x(t) = Pr/parenleftbig
σi(t) =x/parenrightbig
. (3)
For example, v(i)
S(t)represents the probability of node ibeing in state Sin timet. We now deﬁne Fi(D;t)
to be the conditional probability of node ibeing infected by nodes in D(for the ﬁrst time) at day t, as a
function of the nodes’ states {σi(t)}i∈V(t). We have
Fi(D;t) = 1{σi(t)=S}·/productdisplay
j∈∂i(t)\D/parenleftbig
1−β1{σj(t)=I}/parenrightbig
·/parenleftbig
1−/productdisplay
j∈D∩∂i(t)(1−β1{σj(t)=I})/parenrightbig
.(4)
1We assume that a node in the latent state Lis infected, but not infectious. We further assume that latent nodes test
negative.
2In practical implementations, scheduling constraints do play a role but we disregard that in this work.
6Published in Transactions on Machine Learning Research (March/2023)
Equation (4), captures the impact that the nodes in Dhave on infecting node iat dayt. In this equation
we assume that the infections from diﬀerent nodes are independent. The same assumption has also been
made in several other papers in this genre, eg in [27, 28, 29]. Then, we ﬁnd the expectation (with respect to
{σi(t)}i∈V(t)) of (4) as follows:
E{σi(t)}i∈V(t)[Fi(D;t)] =v(i)
S(t)·/braceleftbig/productdisplay
j∈∂i(t)\D(1−βv(j)
I(t))/bracerightbig
·/braceleftbig
1−/productdisplay
j∈D∩∂i(t)(1−βv(j)
I(t))/bracerightbig
.(5)
It is worth noting that (4) is a probability conditioned on {σi(t)}i∈V(t), while (5) is an unconditional proba-
bility. To obtain (5), we have indeed assumed that the states of the nodes are independent. This assumption
does not hold in general and we only utilize it here to obtain a simple expression in (5) in terms of the
infection probabilities. We do not use this independence assumption in the rest of the paper. Deﬁne
S/parenleftbig
D;t/parenrightbig
=/summationdisplay
i∈V(t)E[Fi(D;t)]. (6)
Here,S/parenleftbig
D;t/parenrightbig
represents the (expected) number of newly infectious nodes incurred by nodes in Dat dayt.
Recall thatKπ(t)be the set of nodes that are tested at time t. We show the following result in Appendix A.
Lemma 1. E/bracketleftbig
Cπ(t+ 1)−Cπ(t)/bracketrightbig
=S/parenleftbig
V(t)\Kπ(t);t/parenrightbig
.
2.1 Supermodularity
It is complex to solve (2) globally, especially if one seeks to ﬁnd solutions that are optimal looking into the
future. We thus simplify the optimization (2) for policies that are myopic in time as follows. First, note that
Cπ(T)can be re-written as follows through a telescopic sum:
Cπ(T) =T−1/summationdisplay
t=0Cπ(t+ 1)−Cπ(t). (7)
Then, we restrict attention to myopic policies that at each time minimize E[Cπ(t+ 1)−Cπ(t)]. We then
show how E[Cπ(t+ 1)−Cπ(t)]can be expressed in terms of a supermodular function.
Using (7) along with Lemma 1, we seek to solve the following optimization sequentially in time for 0≤t≤
T−1:
min
|Kπ(t)|≤B(t)S/parenleftbig
V(t)\Kπ(t);t/parenrightbig
.(8)
We now prove some desired properties for the set function S(Kπ(t);t)(see Appendix B).
Theorem 1. S/parenleftbig
Kπ(t);t/parenrightbig
deﬁned in (6) is a supermodular3and increasing monotone function on Kπ(t).
On dayt, and given the network, the probability vectors of all nodes, and Kπ
1(t)⊂Kπ
2(t), for any node
i /∈Kπ
2(t), nodeiwill incur larger increment of newly infectious nodes under Kπ
2(t)than that underKπ
1(t).
This is because node imay have common neighbors with nodes in Kπ
2(t). So, supermodularity holds in
Theorem 1.
The optimization (8) is NP-hard [59]. However, using the supermodularity of S/parenleftbig
V(t)\Kπ(t);t/parenrightbig
, we propose
Algorithm 1 based on [60, Algorithm A] to greedily optimize (8) in every day t. Denote the optimum solution
of (8) as OPT. As proved in [60], on every day t, Algorithm 1 attains a solution, denoted by ˜Kπ(t), such
that/parenleftbig
V(t)\˜Kπ(t);t/parenrightbig
≤/parenleftbig
1 +/epsilon1(t)/parenrightbig
·OPT, i.e., the solution ˜Kπ(t)is an/epsilon1(t)-approximation of the optimum
solution. Here, on day t, the constant /epsilon1(t), which is the steepness of the set function S(·;t)as described in
[60], can be calculated as follows, /epsilon1(t) =/epsilon1/prime
4(1−/epsilon1/prime)and/epsilon1/prime= maxa∈V(t)S(V(t);t)−S(V(t)\{a};t)−S({a};t)
S(V(t);t)−S(V(t)\{a};t).
In Algorithm 1, on every day t, in every step, we choose the node who provides the minimum increment
onS(·;t)based on the results in the previous step, and then remove the node from the current node set.
Algorithm 1 is stopped when Kπ(t)nodes have been chosen. The complexity of this algorithm is discussed
in Appendix C.
3LetXbe a ﬁnite set. A function f: 2X→Ris supermodular if for any A⊂B⊂X , andx∈X\B,f(A∪{x})−f(A)≤
f(B∪{x})−f(B).
7Published in Transactions on Machine Learning Research (March/2023)
Algorithm 1 Greedy Algorithm
Step 0: On dayt, input{vi(t)}i∈V(t), setA0=V(t).
repeat
Step i: LetAi=Ai−1\{ai}, where
ai= arg min
a∈Ai−1S/parenleftbig
{a}∪{a1,···,ai−1};t/parenrightbig
.
untili=N(t)−|Kπ(t)|, and returnKπ(t) =Ai.
3 Exploitation and Exploration
In Section 2.1, we proposed a near-optimal greedy algorithm to sequentially (in time) select the nodes to
test. However, Algorithm 1 has two shortcomings. (i) The computation is costly when Nand/orTare large
(see Appendix C). (ii) The objective function S/parenleftbig
V(t)\Kπ(t)/parenrightbig
is dependent on{vi(t)}i∈V(t)which is unknown,
even though the network and the process are stochastically fully given (see Section 2). This is because the
set of infected nodes are unknown and time-evolving.
To overcome the ﬁrst shortcoming, we propose a simpler reward maximization policy by minimizing an upper
bound on the objective function in (8). To overcome the second shortcoming, we estimate {vi(t)}i∈V(t)
using the history of test observations {Y(τ)}t
τ=0(as presented in Section 4). we refer to the estimates as
{ui(t)}i∈V(t). Both the greedy policy and its reward-based variant that we will propose in this section thus
need to perform decision making based on the estimates {ui(t)}i∈V(t)and we refer to them as “exploitation”
policies.
It now becomes clear that testing has two roles: to ﬁnd the infected in order to isolate them and contain
the spread, and to provide better estimates of {vi(t)}i∈V(t). This leads to interesting tradeoﬀs between
exploitation and exploration as we will discuss next. Under exploitation policies, we test nodes determinis-
tically based on a function of {vi(t)}i∈V(t), (which is called “reward”, and will be deﬁned later); while under
exploration policies, nodes are tested according to a probabilistic framework (based on rewards of all nodes).
To simplify the decision making into reward maximization, we ﬁrst derive an upper bound on
S/parenleftbig
V(t)\Kπ(t);t/parenrightbig
. Deﬁne
ri(t) =S/parenleftbig
{i};t/parenrightbig
. (9)
Using the supermodularity of the function S(·), we prove the following lemma in Appendix D.
Lemma 2. S/parenleftbig
V(t)\Kπ(t);t/parenrightbig
≤S/parenleftbig
V(t);t/parenrightbig
−/summationtext
i∈Kπ(t)ri(t).
Remark1. Recall thatS(·;t)is a supermodular function, then the amount of newly infectious nodes incurred
by the setKπ(t),S(Kπ(t);t), is larger than the sum of the amount of newly infectious nodes by every
individual node in Kπ(t), i.e.,/summationtext
i∈Kπ(t)ri(t). Thus,S/parenleftbig
V(t)\Kπ(t);t/parenrightbig
is upper bounded by S/parenleftbig
V(t);t/parenrightbig
−/summationtext
i∈Kπ(t)ri(t).
We propose to minimize the upper bound in Lemma 2 instead of S/parenleftbig
V(t)\Kπ(t);t/parenrightbig
. SinceV(t)is known and
S/parenleftbig
V(t);t/parenrightbig
is hence a constant, the problem reduces to solving:
max
|Kπ(t)|≤B(t)/summationdisplay
i∈Kπ(t)ri(t).(10)
Given probabilities {vi(t)}i∈V(t), the solution to (10) is to pick the nodes associated with the B(t)largest
valuesri(t). We thus refer to ri(t)as therewardof selecting node i.
Let{ui(t)}i∈V(t)beanestimatefor {vi(t)}i∈V(t)foundbyestimatingtheconditionalprobabilityofthestateof
nodeigiven the history of observations {Y(τ)}t−1
τ=0. Our proposed reward-based Exploitation (RbEx) policy
follows the same idea of selecting the nodes with the highest rewards. Note that {vi(t)}i∈V(t)is unknown
to all nodes. Instead of using the true probabilities {vi(t)}i∈V(t), we consider the estimates of it which
8Published in Transactions on Machine Learning Research (March/2023)
we sequentially update by computing the prior probabilities {ui(t)}i∈V(t)and the posterior probabilities
{wi(t)}i∈V(t). In particular,{ui(0)}i∈V(0)and{wi(0)}i∈V(0)are the prior probabilities and the posterior
probabilities on the initial day, respectively. Hence, we calculate the estimate of rewards, denoted by ˆri(t),
by replacing{vi(t)}i∈V(t)with{ui(t)}i∈V(t)in (6) and (9).
Algorithm 2 Reward-based Exploitation (RbEx) Policy
Input{wi(0)}i∈V(t),{ui(0)}i∈V(0),Y(0), andt= 0.
Repeat fort= 1,2,···,T−1.
Step 1: Calculate{ˆri(t)}i∈V(t)based on{ui(t)}i∈V(t)and (9).
Step 2: Re-arrange the sequence {ˆri(t)}i∈V(t)in descending order, and test the ﬁrst B(t)nodes. Get the
new observations Y(t).
Step 3: Based on Y(t), update{ui(t+ 1)}i∈V(t+1)by Algorithm 4 (Step 0 ∼Step 2) in Section 4.
The shortcoming of Algorithm 2 is that it targets maximizing the estimated sum rewards, even though the
estimates may be inaccurate. In this case, testing is heavily biased towards the history of testing and it does
not provide opportunities for getting better estimates of the rewards. For example, consider a network with
several clusters. If one positive node is known by Algorithm 2, then it may get stuck in that cluster and fail
to locate more positives in other clusters.
In Section 4.2, we will prove, in a line network, that the exploitation policy described in Algorithm 2 can be
improved by a constant factor (in terms of the resulting cumulative infections) if a simple form of exploration
is incorporated.
We next propose an exploration policy. Our proposed policy is probabilistic in the sense that the nodes
are randomly tested with probabilities that are proportional to their corresponding estimated rewards. This
approach has similarities and diﬀerences to Thompson sampling and more generally posterior sampling. The
similarity lies in the probabilistic nature of testing using posterior probabilities. The diﬀerence is that in our
setting decision making depends on the distributions of decision variables, but not samples of the decision
variables.
More speciﬁcally, at time t, nodeiis tested with probability min{1,B(t)ˆri(t)/summationtext
j∈V(t)ˆrj(t)}, which depends on the
budgetB(t). Note that each node is tested with probability at most 1; so ifB(t)ˆri(t)/summationtext
j∈V(t)ˆrj(t)>1for some node
i, then we would not fully utilize the budget. The unused budget is thus
c(t) =/summationdisplay
i∈V(t)/parenleftbigB(t)ˆri(t)/summationtext
j∈V(t)ˆrj(t)−1/parenrightbig+(11)
and can be used for further testing4. Algorithm 3 outlines our proposed Reward-based Exploitation-
Exploration (REEr) policy.
Algorithm 3 Reward-based Exploitation-Exploration (REEr) Policy
Input{wi(0)}i∈V(t),{ui(0)}i∈V(0),Y(0), andt= 0.
Repeat fort= 1,2,···,T−1
Step 1: Calculate{ˆri(t)}i∈V(t)based on{ui(t)}i∈V(t)and (9).
Step 2: Test node iwith probability min{1,B(t)ˆri(t)/summationtext
j∈V(t)ˆrj(t)}. After that, randomly select c(t)/parenleftbig
deﬁned in
(11)/parenrightbig
further nodes to test (see Footnote 4). Get the new observations Y(t).
Step 3: Based on Y(t), update{ui(t+ 1)}i∈V(t+1)by Algorithm 4 (Step 0 ∼Step 2) in Section 4.
4Note thatc(t)is not always an integer. Instead of c(t), we use Int/parenleftbig
c(t)/parenrightbig
with probability|Int/parenleftbig
c(t)/parenrightbig
−c(t)|where Int(·)∈
{⌊·⌋,⌈·⌉}.
9Published in Transactions on Machine Learning Research (March/2023)
4 Message-Passing Framework
As discussed in Section 3, the probabilities {vi(t)}iare unknown. In this section, we develop a message
passing framework to sequentially estimate {vi(t)}ibased on the network observations and the dynamics of
the spread process. We refer to these estimates as {ui(t)}i.
When node iis tested on day t, an observation Yi(t)is provided about its state. Knowing the state of node
iprovides two types of information: (i) it provides information about the state of the neighboring nodes in
future time slots t+ 1,t+ 2,...(because of the evolution of the spread in time and on the network), and (ii)
it also provides information about the past of the spread, meaning that we can infer about the state of the
(unobserved) nodes at previous time slots. For example, if node iis tested positive in time t, we would know
that (i) its neighbors are more likely to be infected in time t+1and (ii) some of its neighbors must have been
infected in a previous time for node ito be infected now. This forms the basis for our backward-forward
message passing framework.
Given the spread model of Section 2, we ﬁrst describe the forward propagation of belief. Suppose that at
timet, the probability vector vi(t)is given for all i. The probability vector vi(t+ 1)can be computed as
follows (see Appendix E):
vi(t+ 1) =vi(t)×Pi/parenleftbig
{vj(t)}j∈∂+
i(t)/parenrightbig
(12)
where Pi/parenleftbig
{vj(t)}j∈∂+
i(t)/parenrightbig
is a local transition probability matrix given in Appendix E.
Recall that Y(t)denotes the collection of network observations on day t. The history of observations is then
denoted by{Y(τ)}t−1
τ=1. Based on these observations, we wish to ﬁnd an estimate of the probability vector
vi(t)for eachi∈V(t). We denote this estimate by ui(t) = (u(i)
x(t),x∈X)and refer to it, in this section, as
theprior probability of nodeiin timet. We further deﬁne the posterior probability wi(t) = (w(i)
x(t),x∈X)
of nodeiin timet(after obtaining new observations Y(t)). In particular,
u(i)
x(t) = Pr/parenleftbig
σi(t) =x|{Y(τ)}t−1
τ=1/parenrightbig
w(i)
x(t) = Pr/parenleftbig
σi(t) =x|{Y(τ)}t
τ=1/parenrightbig
.
Here, the prior probability is deﬁned at the beginning of every day, and the posterior probability is deﬁned at
the end of every day. Conditioning all probabilities in (12) on {Y(τ)}t
τ=1, we obtain the following forward-
update rule (see Appendix F)
ui(t+ 1) =wi(t)×Pi/parenleftbig
{wj(t)}j∈∂+
i(t)/parenrightbig
. (13)
Remark 2. Following (13), we need to utilize the observations Y(t)and the underlying dependency among
nodes’ states to update the posterior probabilities {wi(t)}i, and consequently update {ui(t+ 1)}ibased on
the forward-update rule (13). This is however non-trivial. A Naive approach would be to locally incorporate
nodei’s observation Yi(t)intowi(t)and obtain ui(t+ 1)using(13). This approach, however, does not fully
exploit the observations and it disregards the dependency among nodes’ states, as caused by the nature of the
spread (An example is provided in Appendix H).
Backward Propagation of Belief To capture the dependency of nodes’ states and thus best utilize the
observations, we proceed as follows. First, denote
ei(t−1) = (e(i)
x(t−1),x∈X)
e(i)
x(t−1) = Pr/parenleftbig
σi(t−1) =x|{Y(τ)}t
τ=1/parenrightbig
.
Vectorei(t−1)is the posterior probability of node iat timet−1, after obtaining the history of observations
up to and including time t. By computing ei(t−1), we are eﬀectively correcting our belief on the state of the
nodes in the previous time slot by inference based on the observations acquired at time t. This constitutes
thebackward step of our framework and we will expand on it shortly. The backward step can be repeated
to correct our belief also in times t−2,t−3, etc. For clarity of presentation and tractability of our analysis
10Published in Transactions on Machine Learning Research (March/2023)
and experiments, we truncate the backward step at time t−1and present assumptions under which this
truncation is theoretically justiﬁable. Considering larger truncation windows is straightforward but out of
the scope of this paper.
Once our belief about nodes’ states is updated in prior time slots (e.g., ei(t−1)is obtained), it is propa-
gated forward in time for prediction and to provide a more accurate estimate of the nodes’ posterior and
prior probabilities. More speciﬁcally, consider (12) written for time t(rather than t+ 1) and condition all
probabilities on{Y(τ)}t
τ=1. We obtain the following update rule (see Appendix F):
wi(t) =ei(t−1)×˜Pi/parenleftbig
{ej(t−1)}j∈∂+
i(t−1)/parenrightbig
(14)
where ˜Pi/parenleftbig
{ej(t−1)}j∈∂+
i(t−1)/parenrightbig
is given in Appendix F. Note that the local transition matrix in (14) is not
the same as (13). This is because “future" observations were available in ˜Pi/parenleftbig
{ej(t−1)}j∈∂+
i(t−1)/parenrightbig
.The
probability vectors {ei(t−1)}iprovide better estimates for {wi(t)}ithrough (14) and the prior probabilities
{ui(t+ 1)}iare then computed using (13) to be used for decision making in time t+ 1. The block diagram
in Fig. 3 depicts the high-level idea of our framework. It is worth noting that Pi/parenleftbig
{wj(t)}j∈∂+
i(t)/parenrightbig
in (13) and
˜Pi/parenleftbig
{ej(t−1)}j∈∂+
i(t−1)/parenrightbig
in (14) both depend on the observations, {Y(τ)}t
τ=1.
We next discuss how ej(t−1)can be computed, starting with some notations. Denote by
ζi(t) =σi(t)|{Y(τ)}t
τ=1, θj(t) =σi(t)|{Y(τ)}t−1
τ=1, (15)
the state of the nodes in the posterior probability spaces conditioned on the observations {Y(τ)}t
τ=1and
{Y(τ)}t−1
τ=1, respectively. We further deﬁne Ψi(t)to be the set of those neighbors of node iat timet−1,
includingnode i,whoareobserved/testedattime t. Thissetconsistsofallnodeswhoseposteriorprobabilities
will be updated at time t−1(given a new observation Yi(t)). The set of all neighbors (except node i) of the
nodes in Ψi(t)then deﬁnes Φi(t). The set Φi(t)consists of all nodes whose posterior probabilities at time t
is updated by the observation Yi(t). More precisely, we have
Ψi(t) =O(t)∩∂+
i(t−1),
Φi(t) ={j|j∈∂+
k(t−1),k∈Ψi(t)}\{i},
Θi(t) ={j|j∈∂+
k(t−1),k∈O(t)}\{i}
whereO(t)is the set of observed nodes at time t(see Figure 2). In Appendix G, we show
e(i)
x(t−1)=Pr/parenleftbig
Y(t)|ζi(t−1)=x/parenrightbig
w(i)
x(t−1)
Pr/parenleftbig
Y(t)/parenrightbig . (16)
It suﬃces to ﬁnd Pr/parenleftbig
Y(t)|ζi(t−1) =x/parenrightbig
. The denominator Pr/parenleftbig
Y(t)/parenrightbig
is then found by normalization of
the enumerator in (16). Let {xj}j∈O(t)be a realization of {θj(t)}j∈O(t)and{yl}l∈Θi(t)be a realization of
{ζl(t−1)}l∈Θi(t). We prove the following in Appendix G under a simplifying truncation assumption (see
Assumption 1 in Appendix G) where the backward step is truncated in time t−1:
Pr/parenleftbig
Y(t)|ζi(t−1) =x/parenrightbig
= Pr/parenleftbig
{Yj(t)}j∈Ψi(t)|ζi(t−1) =x/parenrightbig
=/summationdisplay
{xj}j∈Ψi(t)/productdisplay
j∈Ψi(t)Pr/parenleftbig
Yj(t)|θj(t)/parenrightbig
×/summationdisplay
{yl}l∈Φi(t)/productdisplay
j∈Ψi(t)Pr/parenleftbig
xj|{yl}l∈∂+
j(t−1)\{i},x/parenrightbig
×/productdisplay
l∈{Φi(t)}w(i)
yl(t−1).(17)
We ﬁnally present our Backward-Forward Algorithm to sequentially compute estimates {ui(t)}iin Algorithm
4. The process of Algorithm 4 is given in Fig 3, and we also give a simple example to show the process of
Algorithm 4 in Appendix H.
11Published in Transactions on Machine Learning Research (March/2023)
Figure 2: An example of Ψi(t),Φi(t)andΘi(t). Nodeiis marked in red, and its neighborhood ∂+
i(t−1)is
shown by the red contour. Suppose that the gray nodes are tested on day t−1, then Ψi(t)is the set of nodes
within the green contour, and Φi(t)consists of the nodes in the purple contour. Finally, nodes in Θi(t)are
marked with bold black border
.
Figure 3: The process of Algorithm 4. One example of a complete process is given in unshaded blocks. Recall
thatui(τ),wi(τ), andei(τ), whereτ∈{t−1,t,t+1}, are the prior probabilities, the posterior probabilities,
and the updated posterior probabilities, respectively.
4.1 Necessity of Backward Updating
Now we provide an example which illustrates the necessity of backward updating.
Algorithm 4 Backward-Forward Algorithm
InputY(0),{ei(0)}i∈V(0),{wi(0)}i∈V(0),{ui(0)}i∈V(0).
Repeat fort= 1,2,···,T−1
Step 0: Based on Y (t), getV(t)fromV(t−1).
Step 1: Backward step. Update ei(t−1)by (16), (17), and then compute wi(t)by (14).
Step 2: Forward step. Compute ui(t+ 1)by (13).
12Published in Transactions on Machine Learning Research (March/2023)
Figure 4: The line network in Example 1.
Example 1. Consider a line network with the node set V={1,2,...,N}and the edge setE={(i,i+1),1≤
i≤N−1} (see Figure 4). On the initial day, we assume that each node is infected independently with
probability 1/N. Letβ= 1,λ= 0,γ= 05, andB(t) = 1. We further assume that there is no isolation when
a positive node is tested.
Based on Example 1, we show that the naive approach of Remark 2 (i.e., forward-only updating) will cause
the estimated probabilities to never converge to the true probabilities of infection. Nonetheless, if we use the
Backward-Forward Algorithm 4, the estimated probabilities converge to the true probabilities after a certain
number of steps. Formally, we prove the following result in Appendix J.
Theorem 2. For any testing policy that sequentially computes {ui(t)}ibased on (13) (see Remark 2), with
probability (approximately)1
e, we have/summationtextN
i=1||vi(t)−ui(t)||t→∞→Θ(N), for largeN6. On the other hand,
there exists a testing policy that sequentially updates {ui(t)}ibased on Algorithm 4 and attains/summationtextN
i=1||vi(t)−
ui(t)||= 0, t≥2N.
Roadmap of proof: Consider a simple case where every node is susceptible. Since each node is infected with
probability 1/N, then the case occurs with probability /similarequal1/e.
Under the case above, consider any testing policy based on the algorithm in Remark 2 . If a node is tested on
dayt, then the policy “clears” the tested node. Since the updating rule of the algorithm can not go back to the
information on day t−1, then it can not “clear” any neighbors of the tested node and its probability of infection
updates to a non-zero value in the next day. Furthermore, we show that almost all nodes have an signiﬁcantly
large probability of infection when time horizon is suﬃciently large, hence/summationtextN
i=1||vi(t)−ui(t)||t→∞→Θ(N).
On the other hand, we can propose a speciﬁc testing policy. Note that there is no infection, if Algorithm 4
is used to update probabilities, then it can reveal the states of all nodes under the speciﬁc testing policy after
at most 2Ndays. So we have/summationtextN
i=1||vi(t)−ui(t)||= 0, t≥2N.
In Theorem 2, we illustrate the necessity of backward updating when testing is limited. In essence, we want
to “clear” the graph and conﬁrm that there are no infections. If the number of tests is limited, we have
mathematically shown that no algorithm can correctly estimate the nodes’ infection probabilities if it does
not use the backward (inference) step. On the contrary, there is an algorithm that uses the backward step
along with the forward step and the estimates that it provides for the nodes’ infection probabilities converge
to the true probabilities of the nodes after some ﬁnite steps. Even though the considered graph is simple
but the phenomena it captures is general.
As discussed in Theorem 2, the backward updating is necessary. However, bacward updating can be compu-
tationally expensive in large dense graphs. To trade oﬀ the impact of backward updating and the reduction
of computation complexity, we propose an α-linking backward updating algorithm in Appendix K, where
Algorithm 4 is applied on a random subgraph with fewer edges.
4.2 Necessity of Exploration
Note that in reality we have no information for {vi(t)}i, and only have the estimates {ui(t)}i. One may
wonder if exploitation based on wrong initial estimated probability vectors, i.e., {ui(0)}i, misleads decision
making by providing poorer and poorer estimates of the probabilities of infection. If so, exploration may be
necessary.
5Here,λ= 0implies there is no latent state, and γ= 0implies that nodes never recover.
6Theorem 2 holds for all kinds of noem due to the equivalence of norms. In addition, the convergence is topological
convergence.
13Published in Transactions on Machine Learning Research (March/2023)
Example2. ConsiderV={1,2,...,N}and edgesE={(i,i+1),1≤i≤N−1}(see Figure 4). Let N/greatermuch10,
β= 1,λ= 0,γ= 0, andB(t) = 10. Suppose that on the initial day, node 1is infected and all other nodes are
susceptible. Consider a wrong initial estimate: w(i)
I(0) =u(i)
I(0) = 0ifi≤9N
10, andw(i)
I(0) =u(i)
I(0) =10/epsilon1
N
otherwise, where /epsilon1>0. With this initial belief, we have/summationtextN
i=1||wi(0)−vi(0)||=O(1 +/epsilon1).
Diﬀerent from Example 1, here we consider the isolation of nodes that are tested positive. In Example 2,
suppose that a speciﬁc exploration policy is applied: 1(out of 10) tests is done randomly, and the other
9tests are done following exploitation. Now, in Appendix L, we show that under the RbEx policy, the
cumulative infection is at least aNfor a constant a, while under the exploration policy deﬁned above, the
cumulative infection is at most bNwith very high probability, and the ratio a/bcan be any constant for a
large enough N. More formally, we have the following theorem. Let p0be a large probability and consider a
large time horizon T. Denote the cumulative infections under the RbEx policy by CRbEx(T)and under the
speciﬁc exploration policy deﬁned above by Cexp(T). We prove the necessity of exploration in the following
Theorem.
Theorem 3. With probability p0≥99
100,CRbEx(T)
Cexp(T)≥c(N,p 0), wherec(N,p 0)is a constant only depending
onNandp0.
Roadmap of proof: Under the RbEx policy, we test nodes based on their predicted probabilities. Since the
nodes that are located towards the end of the line (right side in Fig. 4) have non-zero probabilities, they are
tested ﬁrst while the disease spreads on the other end of the network (left side in Fig. 4). Mathematically,
suppose that for the ﬁrst time, an infectious node is tested at day t=aN, then there are at least min{aN,N}
infectious nodes before the spread can be contained.
Under the speciﬁc exploration policy described above, consider the event that, for the ﬁrst time, an infectious
node is explored on day t=b/primeN(b/prime<a). We argue that with probability p0, the exploration policy catches at
least two new infections at each step after t=b/primeN. After 2t, the algorithm catches all the infections, and we
have at most 2b/primeNinfections. Let b= 2b/prime. This is an improvement by a factor of at leasta
bin comparison
to the RbEx strategy. Factora
bdepends on the values of Nandp0.
In Theorem 3, we show the necessity of exploration when our initial belief is slightly wrong, i.e., it is slightly
biased toward the other end of the network (In general, this could be due to a wrong belief, prior test results,
etc). We have formally proved that when the testing capacity is limited, exploration can signiﬁcantly improve
the cumulative infections, i.e., contain the spread. This motivates the design of exploration policies. Even
though the setting is simple, the phenomena it captures is much more general.
5 Simulations
5.1 Overview
In this section, we use simulations to study the performance of the proposed exploitation and exploration
policies for various synthetic and real-data networks. Towards this end, we deﬁne some metrics that quantify
how diﬀerent metrics perform and key network parameters and attributes that determine the values of these
metrics and thereby how exploitation and exploration compare. We also identify benchmark policies which
represent the extreme ends of the tradeoﬀ between exploration and exploitation to compare with the policies
weproposeandassesstheperformanceenhancementsbroughtaboutbyjudiciouscombinationsofexploration
and exploitation. Through our experiments, we aim to answer two main questions for various synthetic and
real-data networks: (i) Can exploration policies do better that exploitation policies and if so, when would
thatbethecase? (ii)Whatparameterswouldaﬀecttheperformanceofexplorationandexploitationpolicies?
These are important questions to shed light on the role of exploration. These questions are particularly raised
by Theorem 3 in which we prove that exploration can signiﬁcantly outperform exploitation in some (stylized)
networks. We design the experiments in order to shed light on the above questions and to understand the
extent of the necessity of exploration in diﬀerent network models and scenarios.
Network parameters We consider the following parameters: (i) The unregulated delay /lscriptwhich is the
time from the initial start of the spread to the ﬁrst time testing and intervention starts; (ii) The (global)
clustering coeﬃcient [61, Chapter 3], denoted by γc, which is deﬁned as a measure of the degree to which
nodes in a graph tend to cluster together; (iii) The path-length , denoted by Lp, which measures the average
14Published in Transactions on Machine Learning Research (March/2023)
shortest distance between every possible pair of nodes. We consider attributes such as the initialization of
the process, and the lack of knowledge about {vi(t)}i.
Performance metrics We consider the expected number of infected nodes in a time horizon [0,T]as
the performance measure for various policies. Let C0(T)be the number of infected nodes if there is no
testing and isolation, CRbEx(T),CREEr(T)be the corresponding numbers respectively for the RbExpolicy
(Algorithm 2) and the REErpolicy (Algorithm 3). We consider a ratio between the expectations of these:
Ratio =E[CRbEx(T)]−E[CREEr(T)]
E[C0(T)]. (18)
We deﬁne the estimation error Errπ(t)towards capturing the impact of the lack of knowledge about {vi(t)}i.
Errπ(t) =1
N(t)/summationdisplay
i∈G(t)||vi(t)−ui(t)||2
2. (19)
We consider the diﬀerence between the estimation errors of RbExandREErpolicies: ∆Err=ErrRbEx (T)−
ErrREEr (T).
Benchmark policies We will compare the proposed policies with 4benchmark policies. (i) (Forward)
ContactTracing: wetestedeverydaythenodeswhohaveinfectiousneighbors(inaforwardmanner), denoted
bycandidate nodes . Only some candidate nodes are selected randomly due to testing resources being limited.
Note that only exploitation is utilized under this benchmark. (ii) Random Testing: Every day, we randomly
select nodes to test. Typical testing policies that could come out of SIR optimal control formulations for our
problem would naturally reduce to random testing as they treat all nodes to be statistically identical and
ignore the impact of network topology. One can interpret that random testing implements exploration to
its full extent. (iii) Contact Tracing with Active Case Finding: A small portion of (for example, 5%) testing
budget is utilized for active case ﬁnding [38]. This portion of the testing budget is used to test nodes by
Random Testing. The remaining budget is utilized for forward contact tracing. (iv) Logistic Regression:
We use ideas presented in [51], where simple classiﬁers were proposed based on the features of real data. In
our setting, we choose the classiﬁer to be based on logistic regression, and we deﬁne the feature of node ias
Xi(t) = [1,ni(t) +/epsilon1]T. Here,ni(t)is the number of quarantined neighbors node ihas contacted before and
including day t, and/epsilon1/negationslash= 0is a superparameter aiming to avoid the case where ni(t) = 0. In simulations, we
set/epsilon1= 0.1. Let the observation Yi(t)be the testing result of node i. In particular, if node iis not tested
on dayt, then we do notcollect the data (Xi(t),Yi(t)). Thus, the probability of node ibeing infectious is
deﬁned as the Sigmoid function
1
1 + exp(−Xi(t)·wT),
wherewis the parameter which should be learned.
Simulation Setting We consider a process as described in Section 2 with n0randomly located initial
infected nodes. The process evolved without any testing/intervention for /lscriptdays and we refer to /lscriptas the
unregulated delay . After that, one of the (initial) infectious nodes, denoted by node i0, is (randomly)
provided to the policies. Subsequently, the initial estimated probability vector is set to ui0(/lscript) = (1,0,0,0),
andui(/lscript) = (0,0,0,1)wheni/negationslash=i0. We consider the budget to be equal to the expected number of infected
nodes at time t, i.e.,B(t) =/summationtextN(t)
j=1v(j)
I(t).
We choose model parameters considering the particular application of COVID-19 spread. In particular,
1) the mean latency period is 1/λ= 1or2days [56]; 2) the mean duration in the infectious state (I) is
1/γ= 7∼14days [56, 57, 58]; 3) we choose the transmission rate βin a speciﬁc network such that after a
long time horizon, if no testing and isolation policies were applied, then around 60∼90percent individuals
are infected. We did not consider the case where 100percent individuals are infected because given the
recovery rate (and the topology), the spread may not reach every node.
We consider both synthetic networks such as Watts-Strogatz (WS) networks [62], Scale-free (SF) networks
[63], Stochastic Block Models (SBM) [64] and a variant of it (V-SBM), as well as real-data networks. De-
scriptions and further results for the synthetic networks and real networks are presented in Appendix M.
15Published in Transactions on Machine Learning Research (March/2023)
Watts-Strogatz Networks. We consider a network WS (N,d,δ )withNnodes, degree d, and rewiring
probability δ. The transmission probability of the spread is set to β= 0.4and the number of initial seed is
n0= 3.
Scale-free Networks. We consider a network SF (N,α)withNnodes, and the fraction of nodes with
degreekfollows a power law k−α, whereα= 2.1,2.3,2.5,2.7,2.9. The transmission probability of the spread
is set toβ= 0.5and the number of initial seeds is n0= 3.
Stochastic Block Models. The SBM is a generative model for random graphs. The graph is divided
into several communities, and subsets of nodes are characterized by being connected with particular edge
densities. The intra-connection probability is p1, and the inter-connection probability is p2. We denote the
SBM as SBM (N,M,p 1,p2)7. The transmission probability of the spread is set to β= 0.04and the number
of initial seed is n0= 3. The construction of SBM is given in Appendix M.1.
A Variant of Stochastic Block Models. Diﬀerent from SBM, we only allow nodes in cluster ito connect
to nodes in successive clusters (the neighbor clusters). Denote a variant of SBM as V-SBM (N,M,p 1,p2).
The transmission probability of the spread is set to β= 0.04and the number of initial seed is n0= 3. The
construction of V-SBM is given in Appendix M.1.
Real-data Network I. We consider a contact network of university students in the Copenhagen Networks
Study [65]. The network is built based on the proximity between participating students recorded by smart-
phones, at 5 minute resolution. According to the deﬁnition of close contact by [58], we only used proximity
events between individuals that lasted more than 15 minutes to construct the daily contact network. The
contact network has 672individuals spanning 28days. To guarantee a long time-horizon, we replicate the
contact network 4times so that the time-horizon is 112days. We set β= 0.05andn0= 5to have a realistic
simulation of the Covid-19 spread. Note that the network is relatively dense, so we choose a relatively small
value ofβto avoid the unrealistic case in which the disease spreads very fast (see Figure 11 (left)).
Real-data Network II. We consider a publicly available dataset on human social interactions collected
speciﬁcally for modeling infectious disease dynamics [66, 67, 68]. The data set consists of pairwise distances
between users of the BBC Pandemic Haslemere app over time. The contact network has 469individuals
spanning 576days. Since the network is very sparse, then we compress contacts among individuals during
4successive days to one day. Then, we have 469individuals spanning 144days. We set β= 0.95and
n0= 30to have a realistic simulation of the Covid-19 spread. Note that the network is relatively sparse, so
we choose a relatively large value of βto avoid the unrealistic case in which the disease spreads very slow
(see Figure 11 (left)).
5.2 Simulation Results in Synthetic networks
In this section, we compare the performances of our proposed policies and the benckmarks (deﬁned in
Section 5.1) in synthetic networks. We start with some speciﬁc networks and parameters for this purpose
(see Figure 5, Figure 6, Figure 7, and Figure 8). The ﬁgures reveal that our proposed policies, i.e., the RbEx
and REEr policies, outperform the benchmarks. In particular, in Figure 5 and Figure 6 (i.e., the WS and
SF networks), the REEr policy outperforms the RbEx policy, and the REEr policy provides a more accurate
estimation for{vi(t)}i. In Figure 7 and Figure 8 (i.e., the SBM and V-SBM networks), the RbEx policy
outperforms the REEr policy, and the RbEx policy provides a more accurate estimation for {vi(t)}i. In
addition, in Figure 5, we show that Algorithm 1 outperforms the RbEx policy but performs worse than the
REEr policy (recall that the compuation time of Algorithm 1 is high, we therefore only plot the performance
of Algorithm 1 in Figure 5 as an example). This implies that without exploration, the exploitation in a
greedy manner can not perform well in WS networks.
From the discussions above, the advantages of exploration in distinct settings (diﬀerent network topologies
with variant parameters) are diﬀerent. To investigate the advantages of exploration in distinct settings,
it suﬃces to show how the main parameters aﬀect the exploration. In this work, we consider three main
parameters which are deﬁned in Section 5.1, i.e., the unregulated delay /lscript, the global clustering coeﬃcient γc,
and the path-length Lp. Detailed discussions are later given in Section 5.2.1.
7Here, we assume that Mis an exact divisor of N.
16Published in Transactions on Machine Learning Research (March/2023)
0 50 100 150
Day t00.20.40.60.811.21.4Estimation Error Err(t)WS(300,4,0.03)
Algorithm 1
RbEx policy
REEr policy
0 50 100 150
Day t050100150200250300Cumulative Infections E[C(t)]WS(300,4,0.03)
No tests
Random Testing
Contact tracing
Logistic Regression
Active Case Finding
RbEx policy
Algorithm 1
REEr policy
Figure 5: Performances and estimation errors of diﬀerent policies in WS(300, 4, 0.03) when /lscript= 3.
0 20 40 60 80 100
Day t00.10.20.30.40.50.60.7Estimation Error Err(t)SF(300, 2.5)
RbEx policy
REEr policy
0 20 40 60 80 100
Day t050100150200250Cumulative Infections E[C(t)]SF(300, 2.5)
No tests
Random Testing
Contact tracing
Active Case Finding
Logistic Regression
RbEx policy
REEr policy
Figure 6: Performances and estimation errors of diﬀerent policies in SF(300, 2.5) when /lscript= 3.
17Published in Transactions on Machine Learning Research (March/2023)
0 50 100 150
Day t00.20.40.60.811.21.4Estimation Error Err(t)SBM(300, 10, .2736, .02)
RbEx policy
REEr policy
0 50 100 150
Day t050100150200250300Cumulative Infections E[C(t)]SBM(300, 10, .2736, .02)
No tests
Random Testing
Contact tracing
Logistic Regression
Active Case Finding
RbEx policy
REEr policy
Figure 7: Performances and estimation errors of diﬀerent policies in SBM(300, 10, .2736, .02) when /lscript= 5.
0 50 100 150
Day t00.20.40.60.811.21.4Estimation Error Err(t)VSBM(300, 10, .4184, .02)
RbEx policy
REEr policy
0 50 100 150
Day t050100150200250300Cumulative Infections E[C(t)]VSBM(300, 10, .4184, .02)
No tests
Random Testing
Contact tracing
Active Case Finding
Logistic Regression
REEr policy
RbEx policy
Figure 8: Performances and estimation errors of diﬀerent policies in VSBM(300, 10, .4184, .02) when /lscript= 5.
18Published in Transactions on Machine Learning Research (March/2023)
5.2.1 Impact of Network Parameters
In this subsection, we consider the impact of network parameters on the tradeoﬀ between exploration and
exploitation.
Impact of /lscript.We ﬁrst investigate the impact of the unregulated delay, /lscript. Speciﬁcally, from Table 2,
Table 3, Table 4, and Table 5, as /lscriptincreases, so does Ratioand∆Err, implying that exploration becomes
more eﬀective. With increase in /lscript, the infection continues in the network for longer, there are greater number
of infectious nodes in the network and they are scattered throughout the network, thus exploration is better
suited to locate them. Thus, the REEr policy can contain the spread of the disease faster.
In particular, the REEr policy is always better in WS networks. This is because exploitation may conﬁne
the tests in neighborhoods of some infected nodes. While in the SBM networks, the RbEx policy always
outperforms the REEr policy. In both the SF and V-SBM networks, the RbEx policy is better when /lscriptis
small, and the REEr policy is better when /lscriptis large. One interesting observation is that in the V-SBM
networks, the REEr policy performs better when /lscriptis large ( = 11,13), but the corresponding estimation
errors are larger than those in the RbEx policy. In this speciﬁc network topology, it appears that smaller
estimation error does not always correspond to better cumulative infections. One potential reason is that
the REEr policy is sensitive to /lscriptin this topology, i.e., we can achieve smaller cumulative infections under
the REEr policy even if the estimation error is larger.
WS,/lscript 3 5 7 9 11
Ratio 0.097 0.128 0.177 0.207 0.297
∆Err 0.553 0.814 1.092 1.197 1.449
Table 2: Role of the unregulated delay /lscriptwhenδ= 0.03.
SF,/lscript 3 5 7 9 11
Ratio−0.0009 0.0026 0.0033 0.0042 0.0059
∆Err−0.0014 0.0237 0.0334 0.0434 0.1212
Table 3: Role of the unregulated delay /lscriptwhenα= 2.1.
SBM,/lscript 5 7 9 11 13
Ratio−0.092−0.079−0.042−0.035−0.025
∆Err−0.026−0.015−0.010−0.009−0.009
Table 4: Role of the unregulated delay /lscriptwhen (p1,p2) = (.274,.02).
V-SBM,/lscript 5 7 9 11 13
Ratio−0.022−0.016−0.007 0.011 0.019
∆Err−0.081−0.066−0.046−0.033−0.025
Table 5: Role of the unregulated delay /lscriptwhen (p1,p2) = (.418,.02).
Impact of γcandLp.Then, we investigate the impact of the global clustering coeﬃcient, i.e., γc, and
the average shortest path-length, i.e., Lp. In Table 6, both γcandLpdecrease as δincreases. In Table 7, γc
decreases as αincreases. For the SF networks, the graphs are often disconnected, so we only calculate γcin
Table 7. In Table 8 and Table 9, both γcandLpdecrease as p2increases.
From these tables, as Lporγcdecreases, the beneﬁts of exploration compared to exploitation decrease as
well. This conﬁrms the intuition that exploration is particularly helpful in clustered networks with larger
path lengths where undetected infection can spread without any intervention as exploitation largely conﬁnes
the tests in neighborhoods of the infections that were previously detected. This is also supported by the
fact that exploration lowers estimation error in such scenarios, as shown in Table 6, Table 7, Table 8, and
Table 9. Furthermore, we investigate the role of γcandLpindividually in Appendix M.2.
19Published in Transactions on Machine Learning Research (March/2023)
WS,δγcLp Ratio ∆Err
0.5 62.876 0.191 1.153
.0075.489 21.264 0.182 1.423
.015.473 14.253 0.174 0.991
.0225.467 12.171 0.126 0.779
.03.456 10.81 0.097 0.554
Table 6: Role of clustering coeﬃcient and path length when /lscript= 3.
SF,αγc Ratio ∆Err
2.1.5017 0.0080 0.0334
2.3.3374 0.0057 0.0253
2.5.2348 0.0032 0.0177
2.7.1496−0.0019 0.0124
2.9.0219−0.0064 0.0081
Table 7: Role of clustering coeﬃcient and path length /lscript= 3.
SBM, (p1,p2)γcLp Ratio ∆Err
(0.274,0.02) 0.111 2.573−0.092−0.026
(0.214,0.026) 0.075 2.518−0.103−0.023
(0.159,0.032) 0.056 2.492−0.113−0.026
(0.102,0.039) 0.048 2.480−0.118−0.023
(0.045,0.045) 0.043 2.455−0.124−0.027
Table 8: Role of clustering coeﬃcient and path length /lscript= 5.
V-SBM, (p1,p2)γcLp Ratio ∆Err
(0.418,0.020) 0.3557 4.4264−0.022−0.081
(0.351,0.052) 0.2365 3.6584−0.091−0.045
(0.284,0.085) 0.1769 3.307−0.104−0.055
(0.217,0.085) 0.1385 3.1562−0.112−0.041
(0.150,0.0150) 0.1170 3.0563−0.123−0.042
Table 9: Role of clustering coeﬃcient and path length /lscript= 5.
Real-data Network I, /lscript 5 8 11
Ratio−0.0559−0.0255 0.009
∆Err−0.061−0.030 0.035
Table 10: Role of the unregulated delay /lscript
Real-data Network II, /lscript 5 8 11
Ratio 0.0808 0.1039 0.1208
∆Err 0.0317 0.0535 0.0615
Table 11: Role of the unregulated delay /lscript
20Published in Transactions on Machine Learning Research (March/2023)
0 20 40 60 80 100 120
Day t00.20.40.60.811.21.4Estimation Error Err(t)Real-data Network I
RbEx policy
REEr policy
0 20 40 60 80 100 120
Day t050100150200250300350400450500Cumulative Infections E[C(t)]Real-data Network I
No tests
Random Testing
Logistic Regression
Contact tracing
Active Case Finding
RbEx policy
REEr policy
Figure 9: Performances and estimation errors of diﬀerent policies in the real-data network I when /lscript= 8.
0 50 100 150
Day t0.20.40.60.811.21.4Estimation Error Err(t)Real-data Network II
RbEx policy
REEr policy
0 50 100 150
Day t050100150200250Cumulative Infections E[C(t)]Real-data Network II
No tests
Contact tracing
Active Case Finding
Random Testing
RbEx policy
Logistic Regression
REEr policy
Figure 10: Performances and estimation errors of diﬀerent policies in the real-data network II when /lscript= 8.
21Published in Transactions on Machine Learning Research (March/2023)
0 50 100 150
Day t02468Average Edges per NodeReal-data Network I
Real-data Network II
0 50 100 150
Day t150200250300350400450500The Number of ComponentsReal-data Network I
Real-data Network II
Figure 11: Left: The average number of edges per node on each day. Right: The number of components on
each day.
5.3 Simulation Results in Real-data Networks
In this section, we verify our proposed policies in real data networks (Real-data Network I and Real-data
Network II). In Figure 9, our proposed policies outperform the baselines, and the RbEx policy outperforms
the RREr policy. In Figure 10, the REEr policy can contain the spread and outperform other baselines and
RbEx, while the Logistic Regression policies outperforms RbEx. Comparing Figure 9 and Figure 10, we ﬁnd
that the RbEx policy performs well in Real-data Network I (better than the REEr policy), but performs
not well in Real-data Network II (much worse than the REEr policy). In Figure 11 (left), we calculate the
average edges per node on every day, and in Figure 11 (right), we calculate the number of components on
every day. From Figure 11 (left), the Real-data Network I is denser than the Real-data Network II. However,
from Figure 11 (right), the Real-data Network II often has more components (subgraphs) than the Real-
data Network I. Thus, exploitation may become conﬁned within some components (subgraphs), and fail to
locate infectious nodes elsewhere, and exploration becomes more eﬀective in presence of a large number of
components. This explains the relative performances of REEr and RBEx in these. Contact tracing policy
employs only exploitation, while active case ﬁnding policy uses most of its test budget for exploitation (and
the small amount of the residual test budget for exploration). From Figure 9 and Figure 10, the contact
tracing and the active case ﬁnding policies perform relatively poorly in the Real-data Network II compared
to that in the Real-data Network I; this may again be attributed to the presence of a large number of
components in the former.
As/lscriptincreases, as we show in Table 10 and Table 11 that the beneﬁt of exploitation decreases. In Table 11,
becauseofalargenumberofcomponents, explorationalwaysoutperformsexploitation. However, inTable10,
we observe that exploration outperforms exploitation only for larger values of /lscript. Our results are thus
consistent with synthetic networks.
6 Conclusions and Future Work
In this paper, we studied the problem of containing a spread process (e.g. an infectious disease such as
COVID-19) through sequential testing and isolation. We modeled the spread process by a compartmental
model that evolves in time and stochastically spreads over a given contact network. Given a daily test
budget, we aimed to minimize the cumulative infections. Under mild conditions, we proved that the problem
can be cast as minimizing a supermodular function expressed in terms of nodes’ probabilities of infection
and proposed a greedy testing policy that attains a constant factor approximation ratio. We subsequently
designed a computationally tractable reward-based policy that preferentially tests nodes that have higher
rewards, where the reward of a node is deﬁned as the expected number of new infections it induces in the next
time slot. We showed that this policy eﬀectively minimizes an upper bound on the cumulative infections.
Thesepolicies,however,needknowledgeaboutnodes’infectionprobabilitieswhichareunknownandevolving.
Thus, they have to be actively learned by testing. We discussed how testing has a dual role in this problem:
22Published in Transactions on Machine Learning Research (March/2023)
(i) identifying the infected nodes and isolating them in order to contain the spread, and (ii) providing better
estimates for the nodes’ infection probabilities. We proved that this dual role of testing makes decision
making more challenging. In particular, we showed that reward based policies that make decisions based
on nodes’ estimated infection probabilities can be arbitrarily sub-optimal while incorporating simple forms
of exploration can boost their performance by a constant factor. Motivated by this ﬁnding, we devised
exploration policies that probabilistically test nodes according to their rewards and numerically showed that
when (i) the unregulated delay, (ii) the global clustering coeﬃcient, or (iii) the average shortest path length
increase, exploration becomes more beneﬁcial as it provides better estimates of the nodes’ probabilities of
infection.
Given the history of observations, computing nodes’ estimated probabilities of infection is itself a core
challengeinourproblem. Wedevelopedamessage-passingframeworktoestimatetheseprobabilitiesutilizing
the observations in form of the test results. This framework passes messages back and forth in time to
iteratively predict the probabilities in future and correct the errors in the estimates in prior time instants.
This framework can also be of independent interest.
We showed novel tradeoﬀs between exploration and exploitation, diﬀerent from the ones commonly observed
in multi-armed bandit settings: (i) in our setting, the number of arms is time-variant and actions cannot be
repeated; (ii) the tradeoﬀs in our setting are not due to lack of knowledge about the network or the process
model, but rather due to lack of knowledge about the time-evolving unknown set of infected nodes.
We now describe directions for future research.
Ourframeworkcanbeextendedtoincorporatedelayand/orerrorintestresultsinarelativelystraightforward
manner (an outline of the extension incorporating a delay is given in Appendix I), but generalizing the
performance guarantees for the proposed policies in these cases forms a direction of future research. This
includes establishing fundamental lower bounds using genie-aided myopic policies.
6.1 Impact Statements
We have made several assumptions for the purpose of analytical and computational tractability which do
not hold in practice: (1) the infections from diﬀerent nodes are independent (2) given the entire history of
testing results the states of nodes on the truncation day are independent (Assumption 1), (3) the symptoms
need not be considered in deciding who should be tested and (4) the public health authority knows the entire
network topology and uses it to determine who should be tested (5) independence of states of nodes (in one
step). The ﬁrst two assumptions were used to derive the message passing framework and to prove that the
objective function is super-modular which in turn led to a myopic testing strategy which is also optimal.
The ﬁrst assumption is reasonable as speciﬁc actions of infected individuals, eg, coughing, touching, spread
the infection, which are undertaken independently.
We now consider the second assumption, ie, Assumption 1, in which we assume that the nodes’ states ζ(t−g)
(in the posterior probability space on day t−g) are independent. Note that gis the truncation time for each
backward step, that is, once we get the observations Y(t), we do the backward step and truncate at time
t−g. This assumption does not impose independence on the state of the nodes, but only in the posterior
space at a speciﬁc time. That is, in the process of propagating information back to time t−g, we are
assuming that there is no further correlation between time t−g−1and timet−gworthwhile to exploit
given observations at time t. Naturally, as ggets larger and larger, our framework and calculations become
more precise, as the impact of the testing results at time tin inferring about the nodes’ probabilities at time
t−gvanishes as ggets large. But increase in gsigniﬁcantly increases the computation time. Therefore,
for computational tractability, of the backward update equations, we use g= 1. In principle the derivations
of the backward update equations can be generalized in a straightforward manner to g >1.But designing
approximation strategies that ensure computational tractability for larger gconstitutes a direction of future
research.
Consider the third assumption. We have not considered symptoms in determining who to test. But for
some infectious diseases, symptoms are a reliable manifestation of the disease (e.g., Ebola). In principle
our testing framework can be generalized in a straightforward manner to consider symptoms by introducing
additional states in the compartmental model for evolution of the disease. But introduction of additional
23Published in Transactions on Machine Learning Research (March/2023)
states signiﬁcantly increases the computation time, for example of the forward and backward updates of the
probabilities that individuals have the disease, which renders implementation of our framework challenging.
Considering symptoms while retaining computational tractability constitutes a direction of future research.
Next, consider the fourth assumption. In practice, public health authorities will not typically know contact
networks in their entirety particularly when they are large, for example, as in large cities. However, small
network topologies, for example, contact networks within a community, may be observed by the public
health authority. As a speciﬁc example, the Government of China fully detected contact networks in many
communities in Wuhan and tracked paths traversed by every individual [69]. This tracking may also generate
concerns about privacy which is beyond the scope of this paper. Nonetheless, the technology for learning
contact networks in their entirety for small communities exists and our framework can be utilized for those.
Generalizing our framework to obtain approximation guarantees when contact networks can only be partially
observed constitutes a direction of future research.
Finally consider the last assumption. Note that it is a strong assumption and clearly does not hold in general
but it has been resorted to for only one step in the entire framework. Speciﬁcally to obtain Equation (5) we
have assumed that the state of the nodes are independent. This allows us to obtain a simple expression in
(5) in terms of the infection probabilities. We do not use this independence assumption in the rest of the
paper.
Acknowledgments
This work was supported by NSF CAREER Award 2047482, NSF Award 1909186, NSF Award 1910594, and
NSF Award 2008284.
References
[1] B. Shulgin, L. Stone and Z. Agur. Pulse vaccination strategy in the SIR epidemic model. Bulletin of
Mathematical Biology , 60:1123 – 1148, 1998.
[2] P. Tapaswi and J. Chattopadhyay. Global stability results of a ”susceptible-infective-immune-
susceptible” (SIRS) epidemic model. Ecological Modelling , 87(223 - 226), 1996.
[3] L.Stone, B.Shulgin and Z.Agur. Theoretical examination of the pulse vaccination policy in the SIR
epidemic model. Mathematical and Computer Modelling , 31:207 – 215, 2000.
[4] Y. Takeuchi, W. Ma and E. Beretta. Global asymptotic properties of a delay SIR epidemic model with
ﬁnite incubation times. Nonlinear Analysis: Theory, Methods & Applications , 42:931 – 947, 2000.
[5] J. Aron. Acquired immunity dependent upon exposure in an SIRS epidemic model. Mathematical
Biosciences , 88:37 – 47, 1988.
[6] L. Allen. Some discrete-time SI, SIR, and SIS epidemic models. Mathematical Biosciences , 124:83 –
105, 1994.
[7] A. M. Ramos, M. R. Ferrandez, M. Vela-Perez and et al. A simple but complex enough θ-SIR type
model to be used with COVID-19 real data. Application to the case of Italy. Physica D , 421-132839,
2021.
[8] A. G. M. Neves and G. Guerrero. Predicting the evolution of the COVID-19 epidemic with the A-SIR
model: Lombardy, Italy and Sao Paulo state, Brazil. Physica D , 413-132693, 2020.
[9] A.Simha, R.PrasadandS.Narayana. AsimpleStochasticSIRmodelforCOVID-19InfectionDynamics
for Karnataka after interventions – Learning from European Trends. arXiv: 2003.11920, 2020.
[10] B. Ndiaye, L. Tendeng and D. Seck. Analysis of the COVID-19 pandemic by SIR model and machine
learning technics for forecasting. arXiv: 2004.01574, 2020.
[11] J. Zhu, P. Ge, C. Jiang and et al. Deep-learning artiﬁcial intelligence analysis of clinical variables
predicts mortality in COVID-19 patients. Journal of the American College of Emergency Physicians
Open, 1(6):1364–1373, 2020.
24Published in Transactions on Machine Learning Research (March/2023)
[12] C. Mahanty, R. Kumar, B. K. Mishra, and et al. Prediction of COVID-19 active cases using exponential
and non-linear growth models. Expert Systems , 39(3), 2020.
[13] E. B. Postnikov. Estimation of COVID-19 dynamics “on a back-of-envelope”: Does the simplest SIR
model provide quantitative parameters and predictions? Chaos, Solitons and Fractals , volume = 135-
109841, 2020.
[14] B. Ndiaye, L. Tendeng and D. Seck. Comparative prediction of conﬁrmed cases with COVID-19 pan-
demic by machine learning, deterministic and stochastic SIR models. arXiv: 2004.13489, 2020.
[15] I. Rahimi, A. H. Gandomi, P. G. Asteris and et al. Analysis and Prediction of COVID-19 Using SIR,
SEIQR, and Machine Learning Models: Australia, Italy, and UK Cases. Information , 12(109), 2021.
[16] G. Hu and J. Geng. Heterogeneity learning for SIRS model: an application to the COVID-19. Statistics
and Its Interface , 14:73 – 81, 2021.
[17] R. Vega, L. Flores and R. Greiner. SIMLR: Machine Learning inside the SIR Model for COVID-19
Forecasting. Forecasting , 4(1):72 – 94, 2022.
[18] H. Bastani, K. Drakopoulos, V. Gupta and et al. Eﬃcient and targeted COVID-19 border testing via
reinforcement learning. Nature, 599:108 – 113, 2021.
[19] S. A. Alanazi, M. M. Kamruzzaman, M. Alruwaili and et al. Measuring and Preventing COVID-19
Using the SIR Model and Machine Learning in Smart Health Care. Journal of Healthcare Engineering ,
2020-8857346, 2020.
[20] G. Perakis, D. Singhvi, O. S. Lami, and et al. COVID-19: A multiwave SIR-based model for learning
waves.Production and Operations Management , (13681), 2022.
[21] S. Chowdhury, S. Roychowdhury and I. Chaudhuri. Universality and herd immunity threshold : Revis-
iting the SIR model for COVID-19. International Journal of Modern Physics C , 3(6), 2021.
[22] W. Choi and E. Shim. Optimal strategies for social distancing and testing to control COVID-19. Journal
of Theoretical Biology , 512(110568), 2021.
[23] D.Acemoglu,A.Fallah,A.Giomettoandetal. Optimaladaptivetestingforepidemiccontrol: combining
molecular and serology tests. arXiv:2101.00773, 2021.
[24] L. Abraham, G. Becigneul and B. Scholkopf. Crackovid: Optimizing Group Testing. arXiv:2005.06413,
2020.
[25] C. Tsay, F. Lejarza, M. Stadtherr and et al. Modeling, state estimation, and optimal control for the US
COVID-19 outbreak. Scientiﬁc reports , 10(10711), 2020.
[26] F. Piguillem and L. Shi. Optimal COVID-19 quarantine and testing policies. Nature Communications ,
12(356), 2021.
[27] M. Tanaka K. Kuga and J. Tanimoto. Pair approximation model for the vaccination game: predicting
the dynamic process of epidemic spread and individual actions against contagion. Proceedings of the
Royal Society A , 477(2246):20200769, 2021.
[28] K. Kuga K. Kabir and J. Tanimoto. The impact of information spreading on epidemic vaccination
game dynamics in a heterogeneous complex network-a theoretical approach. Chaos, Solitons & Fractals ,
132:109548, 2020.
[29] K. Kabir and J. Tanimoto. Evolutionary vaccination game approach in metapopulation migration model
with information spreading on diﬀerent graphs. Chaos, Solitons & Fractals , 120:41–55, 2019.
[30] L. Willem, S. Abrams, P. J. K. Libin and et al. The impact of contact tracing and household bubbles
on deconﬁnement strategies for COVID-19. Nature Communications , 12(1524), 2021.
25Published in Transactions on Machine Learning Research (March/2023)
[31] J. Kim, X. Chen, H. Nikpey and et al. Tracing and testing multiple generations of contacts to COVID-19
cases: cost-beneﬁt tradeoﬀs. Royal Society Open Science , 9(10):1 – 20, 2022.
[32] A. Aleta, D. Martin-Corral, A. Piontti and et al. Modelling the impact of testing, contact tracing and
household quarantine on second waves of COVID-19. Nature Human Behaviour , 4:964–971, 2020.
[33] J. Hellewell, S. Abbott, A. Gimma and et al. Feasibility of controlling COVID-19 outbreaks by isolation
of cases and contacts. The Lancet Global Health , 8(4), 2020.
[34] A. Kucharski, P. Klepac, A. Conlan and et al. Eﬀectiveness of isolation, testing, contact tracing,
and physical distancing on reducing transmission of SARS-CoV-2 in diﬀerent settings: a mathematical
modelling study. The Lancet Infectious Diseases , 20(10), 2020.
[35] S. Kojaku, L. Hebert-Dufresne, E. Mones, and et al. The eﬀectiveness of backward contact tracing in
networks. Nature Physics , 17:652 – 658, 2021.
[36] A. J. Kucharski, A. J. K. Conlan, S. M. Kissler, etc. Eﬀectiveness of isolation, testing, contact tracing,
and physical distancing on reducing transmission of SARS-CoV-2 in diﬀerent settings: a mathematical
modelling study. The Lancet Infectious Diseases , 20(10):1151 – 1160, 2020.
[37] A. Perrault, M. Charpignon, J. Gruber, etc. Designing Eﬃcient Contact Tracing Through Risk-Based
Quarantining. Working Paper, National Bureau of Economic Research, Nov. 2020.
[38] H. Ou, A. Sinha, S. Suen, etc. Who and when to screen: Multi-round active screening for network
recurrent infectious diseases under uncertainty. In Proceedings of 19th International Conference on
Autonomous Agents and Multiagent Systems (AAMAS) , 2020.
[39] P. Auer, N. Cesa-Bianchi and P. Fischer. Finite-time Analysis of the Multiarmed Bandit Problem.
Machine Learning , 47:235–256, 2002.
[40] S. Agrawal and N. Goyal. Analysis of Thompson sampling for the multi-armed bandit problem. In
Proceedings of the 25th Annual Conference on Learning Theory , volume 23, pages 1–26, 2012.
[41] S. Agrawal and N. Goyal. Regret analysis of stochastic and nonstochastic multi-armed bandit problems.
Foundations and Trends in Machine Learning , 5(1):1–122, 2012.
[42] K. Madhama and T. Murata. A multi-armed bandit approch for exploring partially observed networks.
Applied Network Science , 4(26):1–18, 2019.
[43] M. Bilgic, L. Mihalkova and L. Getoor. Active learning for networked data. In Proceedings of the 27th
International Conference on International Conference on Machine Learning , pages 79–86, 2010.
[44] X. Wang and R. Garnett and J. Schneider. Active search on graphs. In Proceedings of the 19th ACM
SIGKDD international conference on Knowledge discovery and data mining , pages 731–738, 2013.
[45] Y. Ma and T. K. Huang and J. Schneider. Active search and bandits on graphs using sigma-optimality.
InProceedings of the Thirty-First Conference on Uncertainty in Artiﬁcial Intelligence , pages 542 – 551,
2015.
[46] R. Garnett, Y. Krishnamurthy, D. Wang and et al. Bayesian optimal active search on graphs. In
Proceedings of the 29th International Coference on International Conference on Machine Learning ,
pages 843–850, 2011.
[47] D. Zhao, J. Liu, R. Wu and et al. Data-Eﬃcient Reinforcement Learning Using Active Exploration
Method. In International Conference on Neural Information Processing , pages 265–276, 2018.
[48] Y. Burda, H. Edwards, A. Storkey and et al. Exploration by random network distillation. In Interna-
tional Conference on Learning Representations , 2019.
26Published in Transactions on Machine Learning Research (March/2023)
[49] M. Bellemare, S.Srinivasan, G. Ostrovski and et al. Unifying count-based exploration and intrinsic
motivation. In Proceedings of the 30th International Conference on Neural Information Processing
Systems, pages 1479–1487, 2016.
[50] R. Singh, F. Liu and N. B. Shroﬀ. A Partially Observable MDP Approach for Sequential Testing for
Infectious Diseases such as COVID-19. arXiv:2007.13023, 2020.
[51] H. Grushka-Cohen, R. Cohen, B. Shapira and et al. A framework for optimizing COVID-19 testing
policy using a Multi Armed Bandit approach. arXiv:2007.14805, 2020.
[52] E. Meirom, H. Maron, S. Mannor, and G. Chechik. Controlling Graph Dynamics with Reinforcement
Learning and Graph Neural Networks. In Proceedings of the 38th International Conference on Machine
Learning , number 139, pages 7565 – 7577, 2021.
[53] L. Kaelbling and M. Littman and A. Cassandra. Planning and acting in partially observable stochastic
domains. Artiﬁcial intelligence , 101(1-2):99–134, 1998.
[54] G. Monahan. State of the Art—A Survey of Partially Observable Markov Decision Processes: Theory,
Models, and Algorithms. Management Science , 28(1):1–16, 1982.
[55] G. Walter and M. Contreras. Compartmental Modeling with Networks . Birkhauser, Boston, MA, 1999.
[56] S. Ma, J. Zhang, M. Zeng and et al. Epidemiological parameters of coronavirus disease 2019: a
pooled analysis of publicly reported individual data of 1155 cases from seven countries. medRxiv:
https://doi.org/10.1101/2020.03.21.20040329, Feb 2020.
[57] A. Byrne, D. McEvoy, A. Collins and et al. Inferred duration of infectious period of SARS-CoV-2: rapid
scoping review and analysis of available evidence for asymptomatic and symptomatic COVID-19 cases.
BMJ Open , 10, 2020.
[58] CDC. The U.S. Centers for Disease Control and Prevention (CDC).
https://www.cdc.gov/coronavirus/2019-ncov/php/contact579tracing/contact-tracing-plan/contact-
tracing.html, Sept 2020.
[59] D. Topkis. Supermodularity and Complementarity . Princeton University Press, 1998.
[60] V. Ilev. An approximation guarantee of the greedy descent algorithm for minimizing a supermodular
set function. Discrete Applied Mathematics , 114:131–146, 2001.
[61] D. L. Hansen, B. Shneiderman, M. A. Smith and et al. Analyzing Social Media Networks with NodeXL
(Second Edition) . Morgan Kaufmann, 2020.
[62] D. J. Watts and S. H. Strogatz. Collective dynamics of small-world networks. Nature, 393(4):440–442,
1998.
[63] A. D. Broido and A. Clauset. Scale-free networks are rare. Nature Communications , 10(1017):1 – 10,
2019.
[64] C. Lee and D. J. Wilkinson. A review of stochastic block models and extensions for graph clustering.
Applied Network Science , 4(122), 2019.
[65] P. Sapiezynski, A. Stopczynski, D. D. Lassen and et al. Interaction data from the Copenhagen Networks
Study.Scientiﬁc Data , 6(1):1–10, 2019.
[66] S. M. Kissler, P. Klepac, M. Tang, etc. Sparking “The BBC Four Pandemic”: Leveraging citizen science
and mobile phones to model the spread of disease. bioRxiv https://doi.org/10.1101/479154., 2018.
[67] P. Klepac, S. Kissler and J. Gog. Contagion! The BBC Four Pandemic – the model behind the
documentary. Epidemics , 24:49 – 59, 2018.
27Published in Transactions on Machine Learning Research (March/2023)
[68] J. A. Firth, J. Hellewell, P. Klepac, etc. Using a real-world network to model localized COVID-19
control strategies. Nature Medicine , 26:1616 – 1622, 2020.
[69] X.YuandN.Li. HowDidChineseGovernmentImplementUnconventionalMeasuresAgainstCOVID-19
Pneumonia. Risk Manag Healthc Policy , 13:491 – 499, 2020.
[70] R. D. Shachter. Bayes-Ball: The Rational Pastime (for Determining Irrelevance and Requisite Informa-
tion in Belief Networks and Inﬂuence Diagrams). arXiv: 1301.7412, 2013.
[71] M. Jordan. An Introduction to Probabilistic Graphical Models. https://people.eecs.berkeley.edu/ jor-
dan/prelims, 2003.
A Proof of Lemma 1
Note that a node is counted in Cπ(t)once it has been infected. Then, on day t+ 1,Cπ(t+ 1)increases
(comparing to Cπ(t)) only because some susceptible nodes are infected by infectious nodes and are in the
latent state for the ﬁrst time.
After testing, positive nodes in Kπ(t)would not infect others because they are quarantined, and negative
nodes would not infect others due to the model assumptions. Hence
Cπ(t+ 1) =Cπ(t) +/summationdisplay
i∈V(t)Fi(V(t)\Kπ(t);t).
Taking the expectation on both sides, we obtain the desired result.
B Proof of Theorem 1
To showS/parenleftbig
Kπ(t);t/parenrightbig
deﬁned in (6) is a supermodular function. It suﬃces to show that for any A⊂B⊂V (t),
and forx∈V(t)\B, we have
S/parenleftbig
A∪{x};t/parenrightbig
−S/parenleftbig
A;t/parenrightbig
≤S/parenleftbig
B∪{x};t/parenrightbig
−S/parenleftbig
B;t/parenrightbig
. (20)
Then, it suﬃces to show for any i∈V(t),
E[Fi/parenleftbig
A∪{x};t/parenrightbig
]−E[Fi/parenleftbig
A;t/parenrightbig
]≤E[Fi/parenleftbig
B∪{x};t/parenrightbig
]−E[Fi/parenleftbig
B;t/parenrightbig
]. (21)
Now, we consider three cases.
Case 1. IfA∩∂i(t) =B∩∂i(t), then from (5), the LHS and RHS in (21) are exactly the same. Hence, (21)
holds.
Case 2. IfA∩∂i(t)⊂B∩∂i(t), andx /∈∂i, then from (5), fi(A∪{x}) =fi(A)andfi(B∪{x}) =fi(B).
Hence (21) holds.
Case 3. IfA∩∂i(t)⊂B∩∂i(t), andx∈∂i(t), letY=/parenleftbig
B∩∂i(t)/parenrightbig
\/parenleftbig
A∩∂i(t)/parenrightbig
. Herex /∈Y. From (5), we
can compute
E[Fi(A∪{x};t)]−E[Fi(A;t)]
=v(i)
S(t)/productdisplay
j∈∂i(t)\(A∪{x})/parenleftbig
1−βv(j)
I(t)/parenrightbig
×/parenleftBig
1−/productdisplay
j∈∂i(t)∩(A∪{x})(1−βv(j)
I(t))−(1−βv(x)
I(t))/parenleftbig
1−/productdisplay
j∈∂i(t)∩A(1−βv(j)
I(t))/parenrightbig/parenrightBig
,
28Published in Transactions on Machine Learning Research (March/2023)
which implies
E[Fi(A∪{x};t)]−E[Fi(A;t)]
=v(i)
S(t)/productdisplay
j∈∂i(t)\(A∪{x})/parenleftbig
1−βv(j)
I(t)/parenrightbig
×/parenleftBig
βv(x)
I(t)−/productdisplay
j∈∂i(t)∩(A∪{x})(1−βv(j)
I(t)) +/productdisplay
j∈∂i(t)∩(A∪{x})(1−βv(j)
I(t))/parenrightbig/parenrightBig
=v(i)
S(t)/productdisplay
j∈∂i(t)\(A∪{x})/parenleftbig
1−βv(j)
I(t)/parenrightbig
βv(x)
I(t).
Similarly, note that/parenleftbig
B∩∂i(t)/parenrightbig
=/parenleftbig
A∩∂i(t)/parenrightbig
∪Y. We have
E[Fi(B∪{x};t)]−E[Fi(B;t)]
=v(i)
S(t)/productdisplay
j∈∂i(t)\(A∪({x}∪Y))/parenleftbig
1−βv(j)
I(t)/parenrightbig
βv(x)
I(t).
Thus,
E[Fi(A∪{x};t)]−E[Fi(A;t)]
E[Fi(B∪{x};t)]−E[Fi(B;t)]=/productdisplay
y∈Y/parenleftbig
1−βv(y)
I(t)/parenrightbig
≤1,
which implies S/parenleftbig
TPπ(t)/parenrightbig
is supmodular.
To showS/parenleftbig
Kπ(t);t/parenrightbig
is an increasing monotone function on Kπ(t), it suﬃces to show E[Fi/parenleftbig
Kπ(t);t/parenrightbig
]is an
increasing monotone function on Kπ(t)for anyi.
ForA⊂B, we have∂i(t)\B⊂∂i(t)\A, andA∩∂i(t)⊂B∩∂i(t). Then
/productdisplay
j∈∂i(t)\B/parenleftbig
1−βv(j)
I(t)/parenrightbig
≥/productdisplay
j∈∂i(t)\A/parenleftbig
1−βv(j)
I(t)/parenrightbig
/productdisplay
j∈B∩∂i(t)(1−βv(j)
I(t))≤/productdisplay
j∈A∩∂i(t)(1−βv(j)
I(t)),
and thus, from (5), we have E[Fi/parenleftbig
A;t/parenrightbig
]≤E[Fi/parenleftbig
B;t/parenrightbig
].
C Complexity of Algorithm 1
First of all, we consider the complexity of (5). Suppose {vi(t)}i∈V(t)is given for every day t. For anyKπ(t),
the complexity of computing (5) is
1 +|∂i(t)\Kπ(t)|−1 + 1 +|∂i(t)\Kπ(t)|+|∂i(t)∩Kπ(t)|−1 +|∂i(t)∩Kπ(t)|= 2|∂i(t)|.
Then, for anyKπ(t), the complexity of computing S/parenleftbig
Kπ(t);t/parenrightbig
is
2/summationdisplay
j∈V(t)|∂j(t)|.
From Algorithm 1, in step i, the complexity is
/parenleftbig
N(t)−i+ 1/parenrightbig
×2/summationdisplay
j∈V(t)|∂j(t)|.
And in total we have/parenleftbig
N(t)−|Kπ(t)|/parenrightbig
steps, therefore, on day tthe complexity of Algorithm 1 is
N(t)−|Kπ(t)|/summationdisplay
i=02/parenleftbig
N(t)−i+ 1/parenrightbig/summationdisplay
j∈V(t)|∂j(t)|.
29Published in Transactions on Machine Learning Research (March/2023)
Recall that the time horizon is T, then the total complexity of Algorithm 1 is
T−1/summationdisplay
t=0N(t)−|Kπ(t)|/summationdisplay
i=02/parenleftbig
N(t)−i+ 1/parenrightbig/summationdisplay
j∈V(t)|∂j(t)|.
Note that
N(t)−|Kπ(t)|/summationdisplay
i=12/parenleftbig
N(t)−i+ 1/parenrightbig
≤O/parenleftbig
N2(t)/parenrightbig
/summationdisplay
i∈V(t)|∂i(t)|≤O/parenleftbig
N2(t)/parenrightbig
.
Then, the total complexity is bounded by
O/parenleftBigT−1/summationdisplay
t=0N4(t)/parenrightBig
.
D Proof of Lemma 2
As deﬁned in [60] (an equivalent deﬁnition of footnote 3), consider a ﬁnite set I,f: 2I→Ris a supermodular
function if for all X,Y⊂I,
f(X∪Y) +f(X∩Y)≥f(X) +f(Y). (22)
Following the supermodularity of function S(·)as shown in Theorem 1, set X=V(t)\Kπ(t)andY=Kπ(t)
in (22), we have
S/parenleftbig
V(t)\Kπ(t);t/parenrightbig
≤S/parenleftbig
V(t);t/parenrightbig
−S/parenleftbig
Kπ(t);t/parenrightbig
. (23)
Again, setX=Kπ(t)\{i}andY={i}in (22), and use (22) repeatedly to obtain:
S/parenleftbig
Kπ(t);t/parenrightbig
≥/summationdisplay
i∈Kπ(t)S/parenleftbig
{i};t/parenrightbig
=/summationdisplay
i∈Kπ(t)ri(t). (24)
Substituting (24) in (23), we obtain
S/parenleftbig
V(t)\Kπ(t);t/parenrightbig
≤S/parenleftbig
V(t);t/parenrightbig
−/summationdisplay
i∈Kπ(t)ri(t).
E Local Transition Equations
In this section, we will describe the local transition matrix Pi/parenleftbig
{vj(t)}j∈∂+
i(t)/parenrightbig
used in (12). The state of each
node evolves as follows: (i) if node iis susceptible on day t, then it might be infected by its neighbors in
∂i(t); (ii) an infectious node remains in the latent state with probability 1−λ, and changes state to the
infectious state ( I) with probability λ; (iv) if node iis in stateI, it will recover after a geometric distribution
with parameter γ. Letξi(t) = 1−/producttext
m∈∂i(t)/parenleftbig
1−v(m)
I(t)β/parenrightbig
. In particular, deﬁne ξi(t) = 0if∂i(t) =?. Then,
the probabilities of nodes being in diﬀerent states evolve in time as follows:
v(i)
I(t+ 1) =v(i)
I(t)(1−γ) +v(i)
L(t)λ (25)
v(i)
L(t+ 1) =v(i)
L(t)(1−λ) +v(i)
S(t)ξi(t) (26)
v(i)
R(t+ 1) =v(i)
R(t) +v(i)
I(t)γ (27)
v(i)
S(t+ 1) =v(i)
S(t)/parenleftbig
1−ξi(t)/parenrightbig
. (28)
30Published in Transactions on Machine Learning Research (March/2023)
Note that row vector vi(t)is deﬁned in (3). Collecting (25) - (28), we deﬁne the local transition probability
matrix as given below:
Pi/parenleftbig
{vj(t)}j∈∂+
i(t)/parenrightbig
=
(1−γ) 0γ 0
λ 1−λ0 0
0 0 1 0
0ξi(t) 0 1−ξi(t)
. (29)
and we obtain (12).
F Proofs of (13) and (14)
First of all, we give the following deﬁnition.
Deﬁnition 1. LetXbe a random variable and Bbe an event. Deﬁne X|Bas the random variable Xgiven
B; i.e.,
Pr/parenleftbig
X|B=x/parenrightbig
= Pr/parenleftbig
X=x|B/parenrightbig
. (30)
For brevity, let us deﬁne
θi(t) =σi(t)|{Y(τ)}t−1
τ=1, ζi(t) =σi(t)|{Y(τ)}t
τ=1.
We thus have
u(i)
x(t) = Pr/parenleftbig
θi(t) =x/parenrightbig
, w(i)
x(t) = Pr/parenleftbig
ζi(t) =x/parenrightbig
.
Recall that
vi(t) =/bracketleftbig
v(i)
x(t)/bracketrightbig
x∈X, v(i)
x(t) = Pr/parenleftbig
σi(t) =x/parenrightbig
.
Then, (12) can be re-written as
Pr/parenleftbig
σi(t+ 1) =x/prime
i/parenrightbig
= Pr/parenleftbig
σi(t) =xi/parenrightbig
Pi/parenleftBig
{σj(t)}j∈∂+
i(t)={xj}j∈∂+
i(t)/parenrightBig
, (31)
wherex/prime
i,{xj}j∈∂+
i(t)∈X. Conditioning both sides of (31) on {Y(τ)}t−1
τ=1, state variables σi(t)andσi(t−1)
in (31) can be replaced by θi(t)andζi(t−1), respectively, to obtain
ui(t) =wi(t−1)×Pi/parenleftbig
{wj(t−1)}j∈∂+
i(t−1)/parenrightbig
, (32)
which gives (13). In addition, deﬁne
φi(t) =σi(t)|{Y(τ)}t+1
τ=1,
and
ei(t−1) = (e(i)
x(t−1),x∈X),
e(i)
x(t−1) = Pr/parenleftbig
φi(t−1) =x/parenrightbig
.(33)
This notation implies
φi(t−1) =θi(t−1)|Y(t). (34)
Similarly, conditioning both sides of (32) on Y(t), we ﬁnd
wi(t) =ei(t−1)×˜Pi/parenleftbig
{ej(t−1)}j∈∂+
i(t−1)/parenrightbig
, (35)
which gives (14). ˜Pi({ej(t−1)}j∈∂+
i(t−1))is obtained in the following subsection.
31Published in Transactions on Machine Learning Research (March/2023)
F.1 Computing the transition probability matrix ˜Pi({ej(t−1)}j∈∂+
i(t−1))
Notethat ˜Pi({ej(t−1)}j∈∂+
i(t−1))isnotthesameas Pi({wj(t)}j∈∂+
i(t)).This is because “future” observations
were available in ˜Pi({ej(t−1)}j∈∂+
i(t−1)). To get ˜Pi/parenleftbig
{ej(t−1)}j∈∂+
i(t−1)/parenrightbig
, we split the nodes V(t)into two
classes of nodes: (i) nodes who do not get new observations and (ii) nodes who get new observations.
˜Pi({ej(t−1)}j∈∂+
i(t−1))is obtained by the following rules. For the ﬁrst class of nodes, the local transition
matrix in (35), i.e., ˜Pi({ej(t−1)}j∈∂+
i(t−1)), is the same as that in (32). However, for the second class of
nodes, the local transition matrices are changed accordingly because of the new observations. Let [A]{i,:}
be theithrow of matrix A, andqibe a 1×4vector with the ithelement being one and the rest zero. For
brevity, denote the local transition matrices in (32) and (35) by Pi(t−1)and˜Pi(t−1), respectively. We have
the following three cases:
(i) If node iis not observed, then node idoes not have new observation and we have
˜Pi(t−1) =Pi(t−1). (36)
(ii) IfYi(t) = 0, then node iis not infectious in day twith probability 1. The local transition matrix is
changed to
[˜Pi(t−1)]{j,:}=

q3 j= 1
q2 j= 2
[Pi(t−1)]{j,:}otherwise. (37)
(iii) IfYi(t) = 1, then node iis infectious in day twith probability 1. The local transition matrix is
changed to
[˜Pi(t−1)]{j,:}=

q1 j= 1
q1 j= 2
[Pi(t−1)]{j,:}otherwise. (38)
G Proofs of (16) and (17)
Using new observations, we aim to move backward in time and update our belief (posterior probability) in
previous time slots. Deﬁne a truncation number gand suppose that {Y(t)}aﬀects the posterior probabilities
from daytto dayt−g. We call day t−gthetruncation day associated with day t. To get accurate posterior
probabilities in every day, we need to set g=ton every day tand track back to the initial time. However,
the inﬂuence weakens as time elapses backwards, and for computation tractability, we continue under the
following assumption where g= 1. Recall that ζi(t) =σi(t)|{Y(τ)}t
τ=1.
Assumption 1. On the truncation day (t−g),{ζi(t−g)}iare independent over i. In the following, the
truncation number is assumed to be g= 1.
Remark 3. In Assumption 1, we assume that the nodes’ states ζ(t−g)(in the posterior probability space
on dayt−g) are independent. This assumption is only used at time tof our probability update in a moving
window kind of way. It provides us with a truncation time for each backward step. In particular, under
Assumption 1, once we get the observations Y(t), we do the backward step and truncate at time t−g. For
example, in the trivial case of g=t, the assumption holds. This assumption does not impose independence
on the state of the nodes, but only in the posterior space at a speciﬁc time. In a sense, in the process of
propagating information back to time t−g, we are assuming that there is no further correlation between time
t−g−1and timet−gworthwhile to exploit given observations at time t. Naturally, as ggets larger and
larger, our framework and calculations become more precise but this comes at a huge computational cost.
The idea behind truncating the backward step lies in the observation that the impact of the testing results
at timetin inferring about the nodes’ probabilities at time t−gvanishes as ggets large. For simplicity of
derivations and to have manageable complexity, we set g= 1. The idea and the derivations can be generalized
in a straightforward manner to larger g.
32Published in Transactions on Machine Learning Research (March/2023)
Note that the posterior probabilities on day t−1,wi(t−1), i∈V(t−1), are assumed known (and are
conditioned on the history of observations {Y(τ)}t−1
τ=1). The probability vector ei(t−1)is the new posterior
probability at time t−1which is updated (from wi(t−1)) based on new observations Y(t). In other words,
we infer about the previous state of the nodes given new observations at present time.
To obtain{wi(t)}i∈V(t), it suﬃces to obtain ei(t−1)and the corresponding local transition matrix ˜Pi/parenleftbig
{ej(t−
1)}j∈∂+
i(t−1)/parenrightbig
, see (14). Note that the posterior probabilities wi(t−1), i∈V(t−1), which are calculated
based onY(t−1), are known. The vector ei(t−1)is the new posterior probability which is updated based
onY(t)andwi(t−1).
Equation (16), which we aim to prove, simply follows from Deﬁnition 1, (33)-(34), and Bayes rule:
e(i)
x(t−1) = Pr/parenleftbig
ζi(t−1) =x|Y(t)/parenrightbig
=Pr/parenleftbig
Y(t)|ζi(t−1) =x/parenrightbig
w(i)
x(t−1)
Pr/parenleftbig
Y(t)/parenrightbig . (39)
To ﬁnd Pr/parenleftbig
Y(t)|ζi(t−1) =x/parenrightbig
, and establish (17), we now proceed as follows. We introduce {θj(t)}j∈O(t)
into (39). In particular, we have
Pr/parenleftbig
Y(t)|ζi(t−1) =x/parenrightbig
=/summationdisplay
θj(t), j∈O(t)Pr/parenleftbig
{θj(t)}j∈O(t),Y(t)|ζi(t−1) =x/parenrightbig
By the chain rule of conditional probability,
Pr/parenleftbig
Y(t)|ζi(t−1) =x/parenrightbig
=/summationdisplay
θj(t), j∈O(t)Pr/parenleftbig
Y(t)|{θj(t)}j∈O(t),ζi(t−1) =x/parenrightbig
×Pr/parenleftbig
{θj(t)}j∈O(t)|ζi(t−1) =x/parenrightbig
.
From (15),{ζj(t)}j∈V(t)and{θj(t)}j∈V(t)are variables deﬁned by {σj(t)}j∈V(t)in posterior spaces of
{Y(τ)}t
τ=1and{Y(τ)}t−1
τ=1, respectively. Since Y(t)is a deterministic function of {σj(t)}j∈O(t), and hence
{θj(t)}j∈O(t), thenY(t)is independent of ζi(t−1)given{θj(t)}j∈O(t). In addition, the testing result Yj(t)
(on dayt) of nodejonly depends on its state, i.e., given θj(t), the testing results are determined. Therefore,
we have
Pr/parenleftbig
Y(t)|{θj(t)}j∈O(t),ζi(t−1) =x/parenrightbig
= Pr/parenleftbig
Y(t)|{θj(t)}j∈O(t)/parenrightbig
=/productdisplay
j∈O(t)Pr/parenleftbig
Yj(t)|θj(t)/parenrightbig
.
The product above is an indicator which takes values on {0,1}. We can thus re-write it as follows:
Pr/parenleftbig
Y(t)|{θj(t)}j∈O(t),ζi(t−1) =x/parenrightbig
,δ({Yj(t),θj(t)}j∈O(t)).
where
δ({Yj(t),θj(t)}j∈O(t)) = 1
if the pairs{Yj(t),θj(t)}j∈O(t)are consistent, and
δ({Yj(t),θj(t)}j∈O(t)) = 0
otherwise.
Next, deﬁne
Θi(t) ={j|j∈∂+
k(t−1),k∈O(t)}\{i}
to represent the neighbors (in day t−1) of nodes inO(t)excluding node i. Then,
Pr/parenleftbig
{θj(t)}j∈O(t)|ζi(t−1) =x/parenrightbig
=/summationdisplay
ζl(t−1), l∈Θi(t)Pr/parenleftbig
{θj(t)}j∈O(t),{ζl(t−1)}l∈Θi(t)|ζi(t−1) =x/parenrightbig
.(40)
33Published in Transactions on Machine Learning Research (March/2023)
By the chain rule of conditional probability,
Pr/parenleftbig
{θj(t)}j∈O(t)|ζi(t−1) =x/parenrightbig
=/summationdisplay
ζl(t−1), l∈Θi(t)Pr/parenleftbig
{θj(t)}j∈O(t)|{ζl(t−1)}l∈Θi(t),ζi(t−1) =x/parenrightbig
×Pr/parenleftbig
{ζl(t−1)}l∈Θi(t)|ζi(t−1) =x/parenrightbig
.
Given{ζl(t−1)}l∈Θi(t)∪{ζi(t−1)},{θj(t)}j∈O(t)are independent. We thus have
Pr/parenleftbig
{θj(t)}j∈O(t)|{ζl(t−1)}l∈Θi(t),ζi(t−1) =x/parenrightbig
=/productdisplay
j∈O(t)Pr/parenleftbig
θj(t)|{ζl(t−1)}l∈Θi(t),ζi(t−1) =x/parenrightbig
=/productdisplay
j∈O(t)Pr/parenleftbig
θj(t)|{ζl(t−1)}l∈∂+
j(t−1)\{i},ζi(t−1) =x/parenrightbig
.
Based on Assumption 1,
Pr/parenleftbig
{ζl(t−1)}l∈Θi|ζi(t−1) =x/parenrightbig
=/productdisplay
l∈{Θi(t)}Pr/parenleftbig
ζl(t−1)/parenrightbig
.
Therefore,
Pr/parenleftbig
Y(t)|ζi(t−1) =x/parenrightbig
=/summationdisplay
θj(t), j∈O(t)δ({Yj(t),θj(t)}j∈O(t))
×/summationdisplay
ζl(t−1)/productdisplay
j∈O(t)Pr/parenleftbig
θj(t)|{ζl(t−1)}l∈∂+
j(t−1)\{i},ζi(t−1) =x/parenrightbig
×/productdisplay
l∈{Θi(t)}Pr/parenleftbig
ζl(t−1)/parenrightbig
.(41)
Denote{xj}j∈O(t)as a realization of {θj(t)}j∈O(t)and{yl}l∈Θi(t)as a realization of {ζl(t−1)}l∈Θi(t). Then,
Pr/parenleftbig
Y(t)|ζi(t−1) =x/parenrightbig
=/summationdisplay
{xj}j∈O(t)δ({Yj(t),xj}j∈O(t))
×/summationdisplay
{yl}l∈Θi(t)/productdisplay
j∈O(t)Pr/parenleftbig
xj|{yl}l∈∂+
j(t−1)\{i},ζi(t−1) =x/parenrightbig
×/productdisplay
l∈{Θi(t)}Pr/parenleftbig
ζl(t−1) =yl/parenrightbig
.
Denote
ρ/parenleftbig
{xj}j∈O(t),x/parenrightbig
=/summationdisplay
{yl}l∈Θi(t)/productdisplay
j∈O(t)Pr/parenleftbig
xj|{yl}l∈∂+
j(t−1)\{i},ζi(t−1) =x/parenrightbig
×/productdisplay
l∈Θi(t)Pr/parenleftbig
ζl(t−1) =yl/parenrightbig
. (42)
Then,
Pr/parenleftbig
Y(t)|ζi(t−1) =x/parenrightbig
=/summationdisplay
xj∈X,j∈O(t)δ({Yj(t),xj}j∈O(t))ρ/parenleftbig
{xj}j∈O(t),x/parenrightbig
.(43)
Based on Assumption 1, we can further simplify (43). Consider node i,Y(t)can be split into Yi,1(t)and
Yi,2(t), whereYi,1(t)is the observations of the set O(t)∩∂+
i(t−1), andYi,2(t)is the observations of the
rest of the nodes. Note that Yi,1(t)∪Yi,2(t) =Y(t)andYi,1(t)∩Yi,2(t) =?.
Lemma 3. Conditioned on Yi,1(t),ζi(t−1)is independent of Yi,2(t).
Proof.To show Lemma 3, we use the structured belief network as deﬁned in [70]. ζj(t)is the random
variable associated with node j. Note that Yj(t)is the test result of ζj(t)on dayt. Now, we consider
j∈/parenleftbig
O(t)\(O(t)∩∂+
i(t−1))/parenrightbig
. By [70, Theorems 1] and Bayes ball algorithm deﬁned in [71, Section 2], we
investigate the following two cases.
34Published in Transactions on Machine Learning Research (March/2023)
(i) For any j∈/parenleftbig
O(t)\(O(t)∩∂+
i(t−1))/parenrightbig
withYj(t) = 1, the corresponding state ζj(t)is determined
(which isI). Then, probabilities conditioning on Yj(t)is equivalent to (equal to) probabilities
conditioning on ζj(t). By Bayes ball algorithm [70, 71], the information (the ball) is blocked at ζj(t)
when the information (the ball) reaches ζj(t), which implies the information (the ball) can not reach
ζi(t−1).
(ii) For any j∈/parenleftbig
O(t)\(O(t)∩∂+
i(t−1))/parenrightbig
withYj(t) = 0,ζj(t)is not determined. By Bayes ball
algorithm [70, 71], when the information (the ball) reaches ζj(t), it can traverse Yj(t)when blocking
Yj(t)(conditioning on Yj(t)). However, by Assumption 1, ζi(t−1)andζj(t−1)are independent, so
any path between ζi(t−1)andζj(t−1)is blocked, including the path ζj(t−1)↔ζj(t)↔Yj(t)↔
ζj(t)↔ζi(t−1). Thus, the information (the ball) can not reach ζi(t−1).
A simple example is given in Figure 12: Let Y1(t) = 0andY2(t) = 1. GivenY1(t)andY2(t),Y3(t)is
independent of ζ1(t−1).
Figure 12: Bayes ball algorithm in the network of 3nodes. The terms on which we have conditioning are
shaded gray and are equivalently blocked.
From Lemma 3,
e(i)
x(t−1) = Pr/parenleftbig
ζi(t−1) =x|Y(t)/parenrightbig
= Pr/parenleftbig
ζi(t−1) =x|Yi,1(t)/parenrightbig
. (44)
We simplify (43) based on Lemma 3 or (44). From (44), denote the observations of nodes in ∂+
i(t−1)as
Y∂+
i(t),Y∂+
i(t)is independent of ζi(t−1). Denote Ψi(t) =O(t)∩∂+
i(t−1). Then, We can replace O(t)by
Ψi(t)in (16). Subsequently, denote Φi(t) ={j|j∈∂+
k(t−1),k∈Ψi(t)}\{i}, and we can replace Θi(t)by
Φi(t)in (40). Thus, from (42) and (43), we respectively have
ρ/parenleftbig
{xj}j∈Ψi(t),x/parenrightbig
=/summationdisplay
{yl}l∈Φi(t)/productdisplay
j∈Ψi(t)Pr/parenleftbig
xj|{yl}l∈∂+
j(t−1)\{i},ζi(t−1) =x/parenrightbig
×/productdisplay
l∈Φi(t)Pr/parenleftbig
ζl(t−1) =yl/parenrightbig
(45)
and
Pr/parenleftbig
{Yj(t)}j∈Ψi(t)|ζi(t−1) =x/parenrightbig
=/summationdisplay
xj∈X,j∈Ψi(t)δ({Yj(t),xj}j∈Ψi(t))ρ/parenleftbig
{xj}j∈Ψi(t),x/parenrightbig
(46)
which give the desired result (17).
35Published in Transactions on Machine Learning Research (March/2023)
Figure 13: The original graph (left). The graphical model of states and observations (right)
H A Simple Example for Algorithm 4
In this section, we give a simple example to illustrate the ideas and steps of Algorithm 4. Besides, we
compare our proposed algorithm (Algorithm 4) with the Naive approach discussed in Remark 2. Consider
a simple network with three nodes. Node 1has an edge with node 2, node 2has an edge with node 3(see
Fig 13). Nodes 1and3are symmetric and statistically identical, and node 2has higher degree.
Consider the following situation: on the initial day (day 0), assume that nodes 1and3are susceptible, and
node 2is infectious. On day 1, let node 2be tested. Recall that we deﬁne the posterior probability vectors
at the end of every day, and the prior probability vectors at the beginning of every day. Nodes’ states change
in the beginning of every day and testing is also done in the beginning of every day. Let the initial belief,
i.e., the posterior probability wi(0)and the prior probability ui(0)on day 0, of the nodes be
wi(0) = [1/3,0,0,2/3], i= 1,2,3
ui(0) = [1/3,0,0,2/3], i= 1,2,3.
Day 0: No tests on day 0, the prior probabilities are updated by the forward step ( Step 2in Algorithm 4).
Day 1: ByStep 2in Algorithm 4, we have
u1(1) = [0.3144,0.0373,0.0633,0.5849]
u2(1) = [0.3559,0.0676,0.0633,0.5131]
u3(1) = [0.3144,0.0373,0.0633,0.5849].
After testing node 2, we know that node 2 is positive. We use the test result to infer about the state of
the nodes in prior times. In particular, we update the posterior probability on day 0(wi(0)). Denoting the
updated posterior probability as ei(0), by Step 1 in Algorithm 4, we ﬁnd
e1(0) = [0.3144,0.0373,0.0633,0.5849]
e2(0) = [0.9615,0.0385,0.0,0.0]
e3(0) = [0.3144,0.0373,0.0633,0.5849].
We can now say that at the end of day 0, node 2 was infectious with probability 0.9615and it was in the
latent state with probability 0.0385. Moreover, we see that the (posterior) infection probabilities of nodes 1
and3on day 0have increased since they may have infected node 2on day 0, i.e., 0.3517 =e(i)
I(0) +e(i)
L(0)>
1/3 =w(i)
I(0) +w(i)
L(0)withi= 1,3. Next, we obtain the posterior probability on day 1. Recall that wi(t)
36Published in Transactions on Machine Learning Research (March/2023)
describes the posterior probability vector of node iat the end of day t. By Step 1 in Algorithm 4,
w1(1) = [0.4008,0.0,0.1268,0.4724]
w2(1) = [0.90,0.0,0.10,0.0]
w3(1) = [0.4008,0.0,0.1268,0.4724].
One may wonder why the posterior probability is [0.9,0,0.1,0]rather than [1,0,0,0]. This is because testing
is done in the beginning of time tand the posterior probabilities are deﬁned at the end of time slots t. The
infected node may have recovered by the end of time t= 1and this is reﬂected in the posterior probabilities
computed.
Day 2: We can get the prior probability vectors on day 2by our forward update (making predictions):
u1(2) = [0.2883,0.0,0.1268,0.5849]
u2(2) = [0.8135,0.0,0.1865,0.0]
u3(2) = [0.2883,0.0,0.1268,0.5849].
On the other hand, if we apply the naive updating rule deﬁned in Remark 2, on day 2, we ﬁnd
u/prime
1(2) = [0.4074,0.0896,0.0948,0.4082]
u/prime
2(2) = [0.81,0.0,0.19,0.0]
u/prime
3(2) = [0.4074,0.0896,0.0948,0.4082].
Recall that we use Assumption 1 in the proposed algorithm (Algorithm 4), and the Naive approach in
Remark 2 does not have the backward step, so both approaches do not capture the correlations among
nodes. By Monte Carlo simulations, the correlations among nodes are captured, and the nodes’ probability
vectors are approximated on day 2as follows:
v1(2) = [0.3235,0.0976,0.0196,0.5593]
v2(2) = [0.7244,0,0.2756,0]
v3(2) = [0.3158,0.1072,0.019,0.558]
which yields the following comparison for the incurred estimation errors:
0.4342 =3/summationdisplay
i=1||ui(2)−vi(2)||<3/summationdisplay
i=1||u/prime
i(2)−vi(2)||= 0.5018.
The left hand side shows the estimation error under our proposed backward-forward update and the right
hand side shows the estimation error under the naive approach.
I Delay of Testing Results
One can extend the framework to a more realistic case where testing results are not able to be obtained on
the same day, but will be obtained after a delay a. In other words, if nodes are tested on day t−a, the test
results are provided on day t. The extended framework is summarized as follows.
On dayt, before getting the test results of day t−a, the algorithm knows the following information: (i) the
network topology from day t−a−1to dayt, i.e.,G(t−a−1),···,G(t)(it is aﬀected by the past actions);
(ii) the posterior probability of nodes on day t−a−1,/braceleftbig
wi(t−a−1)/bracerightbig
i∈G(t−a−1); and (iii) and the prior
probability vectors of nodes from day t−ato dayt, i.e.,/braceleftbig
ui(t−a)/bracerightbig
i∈G(t−a),···,/braceleftbig
ui(t)/bracerightbig
i∈G(t).
After getting the test results on day t−a, we can obtain the updated posterior probability vectors on
dayt−a−1, and the posterior probability vectors on day t−a, i.e.,/braceleftbig
ei(t−a−1)/bracerightbig
i∈G(t−a−1), and/braceleftbig
wi(t−a)/bracerightbig
i∈G(t−a), byStep 1in Algorithm 4.
37Published in Transactions on Machine Learning Research (March/2023)
Based on{w(t−a)}i∈G(t−a), byStep 2in Algorithm 4, we update the prior probability from day t−a+ 1
to dayt, and obtain the prior probability on day t+ 1, i.e.,{ui(t+ 1)}i∈G(t+1).
Repeating the process, we can compute the estimated probability vectors of nodes and apply the exploration
and exploitation policies.
J Proof of Theorem 2
Step 1: Preliminaries.
We divide the distributions of initial infectious nodes into two complementary events:
I1={No node is infectious }
I2=Ic
1.
LetNbe suﬃciently large,
Pr{I1}= (1−1/N)N≈1/e
Pr{I2}≈1−1/e.
In eventI1, since there is no infection on the initial day, then no node is infectious in the future, i.e., the
true probability of nodes v(i)
I(t) = 0for alli∈V(t)andt≥1.
Note that in Example 1, each node can be in one of two states, SandI. The transmission probability β= 1.
So, on day t, the probability of node ibeing in state Iincludes the infection of node ion dayt−1, and the
infection from its neighbors. Then, based on (13), we have
u(i)
I(t) =w(i)
I(t−1) +{1−w(i)
I(t−1)}/braceleftBig
1−/parenleftbig
1−w(i−1)
I(t−1)/parenrightbig/parenleftbig
1−w(i+1)
I(t−1)/parenrightbig/bracerightBig
=1−{1−w(i−1)
I(t−1)}{1−w(i)
I(t−1)}{1−w(i+1)
I(t−1)}.(47)
For convention, we assume that nodes 0andN+ 1are two virtual nodes with no probability of infection,
i.e.,u(0)
I(t) =u(N+1)
I (t) = 0for allt, and no tests are applied to these two nodes all the time.
Sincew(i)
I(t+ 1),w(i−1)
I(t+ 1),w(i+1)
I(t+ 1)∈[0,1], then from (47),
u(i)
I(t)≥1−1×(1−w(i)
I(t−1))×1 =w(i)
I(t−1). (48)
Thus, by symmetry over w(i−1)
I(t−1),w(i)
I(t−1), andw(i+1)
I(t−1)we get the inequality
u(i)
I(t)≥max{w(i−1)
I(t−1),w(i)
I(t−1),w(i+1)
I(t−1)}. (49)
Step 2: Consider the computation of {ui(t)}ibased on (13)/parenleftbig
equivalently (47)/parenrightbig
under eventI1.
Recall that B(t) = 1for allt. On any day t, if nodei0is tested, then the result is negative, and w(i0)
I(t) = 0,
and
w(i)
I(t) =u(i)
I(t)for alli/negationslash=i0. (50)
In (49), at most one of w(i−1)
I(t−1),w(i)
I(t−1), andw(i+1)
I(t−1)is updated to 0. We ﬁrst prove the
following facts.
Fact 1.u(i)
I(t)≥1
Nfor allt. On any day t,w(i)
I(t)≥1
Nwithi/negationslash=i0, wherei0is the index of node tested on
dayt.
Proof. We prove Fact 1by mathematical induction. On the initial day, by model assumption in Example 1,
u(i)
I(0) =1
Nfor alli. Then, if node i0is tested, then as mentioned above, w(i0)
I(0) = 0, and by (50)
w(i)
I(0) =u(i)
I(0) = 1/Nfor alli/negationslash=i0.
38Published in Transactions on Machine Learning Research (March/2023)
SupposeFact 1holds for all τ≤t−1. Now, we consider τ=t. From (49), we have u(i)
I(t)≥max{w(i−1)
I(t−
1),w(i)
I(t−1),w(i+1)
I(t−1)}≥ 1/N. Then, if node i0is tested, we have w(i0)
I(t) = 0, and then by (50),
w(i)
I(t) =u(i)
I(t)≥1/Nfor alli/negationslash=i0.
Fact 2. If nodeihas not been tested up to day t, thenu(i)
I(t)tends to 1ast→∞.
Proof. Since node iis not tested from the initial day to day t, then
w(i)
I(τ) =u(i)
I(τ), τ≤t. (51)
Note that at most one of its neighbors is tested on day t. By (47) and Fact 1,
u(i)
I(t)≥1−(1−1/N)(1−w(i)
I(t−1)) = 1−(1−1/N)(1−u(i)
I(t−1)),,
which implies
(1−1/N)(1−u(i)
I(t−1))≥1−u(i)
I(t),
which implies
1−u(i)
I(t)≤(1−1/N)t(1−u(i)
I(0)) = (1−1/N)t+1.
Lettingt→∞completes the proof.
Fact 3. If nodeiis not tested on day t−1, then
u(i)
I(t)≥w(i)
I(t−1) +1
N(1−w(i)
I(t−1))w(i)
I(t−1).
Proof. ByFact 3, if nodeiis not tested on day t−1, thenw(i)
I(t−1)>0. From (47), by some algebra,
u(i)
I(t) =w(i)
I(t−1) + (1−w(i)
I(t−1))(w(i−1)
I(t−1) +w(i+1)
I(t−1)−w(i−1)
I(t−1)w(i+1)
I(t−1))
=(1 +/epsilon1)w(i)
I(t−1)
where
/epsilon1=1−w(i)
I(t−1)
w(i)
I(t−1)×(w(i−1)
I(t−1) +w(i+1)
I(t−1)−w(i−1)
I(t−1)w(i+1)
I(t−1)).
Note that at most one of the neighbors of node iis tested on day t−1, then
1−w(i)
I(t−1)
w(i)
I(t−1)≥1−w(i)
I(t−1)
w(i−1)
I(t−1) +w(i+1)
I(t−1)−w(i−1)
I(t−1)w(i+1)
I(t−1)≥max{w(i−1)
I(t−1),w(i+1)
I(t−1)}.
FromFact 1,max{w(i−1)
I(t−1),w(i+1)
I(t−1)} ≥ 1/N. Thus,/epsilon1≥(1−w(i)
I(t−1))×1/N. Hence,
u(i)
I(t)≥w(i)
I(t−1) +1
N(1−w(i)
I(t−1))w(i)
I(t−1).
Since we consider all possible sequential testing policies, then we divide all nodes into two sets
S1(t) ={nodes that have not been tested up to day t}
S2(t) =Sc
1(t).
In the following proof, let t→∞. ByFact 2,u(i)
I(t)→1ifi∈S1(t). Next, we focus on the set S2(t).
Denote the index of node which is tested on day t−1asi0(t). ByFact 1,w(i)
I(t−1)≥1/Nfor alli/negationslash=i0(t).
Then, we deﬁne
S21(t) ={i|1/N≤w(i)
I(t−1)<1−1/N}
S22(t) ={i|1−1/N≤w(i)
I(t−1)}.
Thus, we haveS2(t) =S21(t)∪S22(t)∪{i0(t)}. Due to the equivalence of norms, without loss of generality,
we consider L1norm in the rest of the proof.
39Published in Transactions on Machine Learning Research (March/2023)
(i) Ifi∈S1(t), thenu(i)
I(t)→1. Thus||ui(t)−vi(t)||1→2.
(ii) Ifi∈S21(t), then||ui(t)−vi(t)||1≥||ui(t−1)−vi(t−1)||1+2(N−1)
N3. In fact, since i∈S21(t), then
i/negationslash=i0(t), thus by (50) and Fact 3,
u(i)
I(t)≥u(i)
I(t−1) +1
N(1−w(i)
I(t−1))w(i)
I(t−1).
Note thatNis suﬃciently large, so 1/N < 1/2<1−1/N. Ifx∈[1/N,1−1/N), then the fuction
f(x) =x(1−x)has the minimum valueN−1
N2whenx= 1/N. Thus,
u(i)
I(t)≥u(i)
I(t−1) +N−1
N3. (52)
Recall that v(i)
I(t) = 0andv(i)
S(t) = 1for allt, andu(i)
I(t) +u(i)
S(t) = 1, then
||ui(t)−vi(t)||1=|u(i)
I(t)−v(i)
I(t)|+|u(i)
S(t)−v(i)
S(t)|= 2|u(i)
I(t)−v(i)
I(t)|. (53)
From (52),
||ui(t)−vi(t)||1= 2|u(i)
I(t)−v(i)
I(t)|≥2|u(i)
I(t−1) +N−1
N3−v(i)
I(t−1)|
≥2|u(i)
I(t−1)−v(i)
I(t−1)|+2(N−1)
N3=||ui(t−1)−vi(t−1)||1+2(N−1)
N3.
(iii) Ifi∈S22(t), then node iis not tested on day t, thus from (49), u(i)
I(t)≥w(i)
I(t−1) = 1−1/N.
Thus, by (53),||ui(t)−vi(t)||1≥2(N−1)
N.
Since we consider Nsuﬃciently large, then we can prove the following lemma.
Lemma 4. limt→∞S21(t) =?.
Proof. We ﬁrst prove the following Claims.
Claim 1 . If (i)u(i−1)
I(t)≥1−1/Nand nodei−1is not tested on day t, or (ii)u(i+1)
I(t)≥1−1/Nand node
i+ 1is not tested on day t, or (iii)u(i−1)
I(t)≥1−1/Nandu(i+1)
I(t)≥1−1/N, thenu(i)
I(t+ 1)≥1−1/N.
Proof. By (47) and (50), we can derive u(i)
I(t+ 1)≥1−1/Ndirectly.
Claim 2 . No node can stay in S21(t)for successive/ceilingleftbig
N3/(N−1)/ceilingrightbig
days.
Proof. if nodeistays inS21(t)for successive/ceilingleftbig
N3/(N−1)/ceilingrightbig
days, i.e., from day τto dayτ+/ceilingleftbig
N3/(N−1)/ceilingrightbig
,
then by (52), u(i)
I(τ+/ceilingleftbig
N3/(N−1)/ceilingrightbig
)>1, which contradicts with u(i)
I(t)≤1for allt.
Now, we prove the lemma by contradiction. Based on Claim 2 , assume there exists at least one jand an
increasing sequence {ti}∞
i=0with limn→∞tn=∞, such that j∈S21(ti)for all{ti}∞
i=0.
For somei, nodejis inS22(ti−1)on dayti−1, and node jis inS21(ti)on dayti. In other words,
u(j)
I(ti)<1−1/N≤u(j)
I(ti−1). From (47) and Calim 1 ,u(j)
I(ti)<1−1/N≤u(j)
I(ti−1)holds only because
nodejis tested on day ti−1, and all of its neighbors (i.e., nodes j−1,j+ 1) haveu(j−1)
I(ti−1)<1−1/N
andu(j+1)
I(ti−1)<1−1/N. However, since u(j)
I(ti−1)≥1−1/Nand nodejis tested on day ti−1,
then byClaim 1 ,u(j−1)
I(ti)≥1−1/Nandu(j+1)
I(ti)≥1−1/N. Subsequently, by Claim 1 , we have
u(j)
I(ti+1)≥1−1/N. Thus, on day ti+1, at least one of its neighbors, say j−1, hasu(j−1)
I(ti+1)≥1−1/N.
ByClaim 1 , nodejnever fall intoS21(t)fort∈{ti+1,ti+2,···}, which contradicts with the assumption.
From Lemma 4, when t→∞, we have|S1(t)|= Θ(N)or|S22(t)|= Θ(N). Thus,/summationtextN
i=1||ui(t)−vi(t)||1=
Θ(N).
Step 3: Consider the computation of {ui(t)}ibased on Algorithm 4.
40Published in Transactions on Machine Learning Research (March/2023)
In this step, we consider a speciﬁc testing policy: We test node ion dayk, wherek≡i−1(mod M )for all
1≤i≤M.
In eventI2, since the transmission probability β= 1, then all nodes are infected at most Ndays because
there is no recovery. Thus, no node with positive testing result is repeatedly tested. So in at most 2Ndays,
all nodes are infectious, and the algorithm ﬁnds all infected nodes, so ui(t) =vi(t),t≥2N.
In eventI1, whenever a node is tested, it is negative. Node 1is tested on day 0, the result is negative. On
day1, node 2is tested and the result is negative. By backward updating, since β= 1and no recovery, then
nodes 1&3are inferred to be in state Son day 0. Since node 2is in stateSon day 1. Then, node 1is
inferred in state Son days 0and1.
Assume that nodes 1,2,···,k−2are inferred to be in state Sby dayk−1. Now, we day k, wherek≤N.
On dayk, nodek−1is tested negative, hence by backward updating, nodes k−2andkare inferred to be
in stateSon dayk−1. By the testing result of node k−1on dayk, nodes 1,2,···,k−1are inferred in
stateSby dayk. By induction, after Ndays, it clears every node, so ui(t) =vi(t),t≥N.
FromSteps 1∼3, we complete the proof.
Kα-linking Backward Updating
K.1 Complexity Reduction
Let{xj}j∈O(t)be a realization of {θj(t)}j∈O(t)and{yl}l∈Θi(t)be a realization of {ζl(t−1)}l∈Θi(t). Let
nodeihave statexin dayt−1. Consider one node k∈∂+
j(t−1)\{i}and the probability
Pr/parenleftbig
xj|{yl}l∈∂+
j(t−1)\{i},x/parenrightbig
,j∈Ψi(t).
Since node kis not infectious if yk=L,yk=Roryk=S, then the probability above remians the same no
matter whether yk=L,yk=Roryk=S.
Thus, we introduce a new state, denoted by E, to be a replacement of {L,R,S}, and
Pr/parenleftbig
yk=E/parenrightbig
=/summationdisplay
x∈{L,R,S}Pr/parenleftbig
yk=x/parenrightbig
.
Next, denoteX/prime={I,E}. Equation (17) can be re-written as follows:
Pr/parenleftbig
{Yj(t)}j∈Ψi(t)|ζi(t−1) =x/parenrightbig
=/summationdisplay
{xj}j∈Ψi(t)/productdisplay
j∈Ψi(t)Pr/parenleftbig
Yj(t)|θj(t)/parenrightbig
×/summationdisplay
{yl}l∈Θi(t)/productdisplay
j∈Ψi(t)Pj/parenleftbig
xj|{yl}l∈∂+
j(t−1)\{i},x/parenrightbig
×/productdisplay
zl∈X/prime,l∈Θi(t)Pr/parenleftbig
ζl(t−1) =zl/parenrightbig
,(54)
with reduces the computation complexity. Subsequently, ei(t−1)in (16) can be calculated by (54) directly.
K.2α-linking Backward Updating
In the backward step, the computation complexity is large even in (54). To further reduce the complexity
in (54), one way is to update the posterior probability ei(t)in a sparser network. Now, we deﬁne α-linking
Backward Updating as follows:
(i) We generate a subgraph Gα(t)based on the pre-determined graph G(t): Suppose that each edge (in
G(t)) exists with probability α,0≤α≤1. Ifα= 1, thenGα(t) =G(t); ifα= 0, thenGα(t)is a
graph with no edges.
(ii) Backward updating in Gα(t): Similar with ∂i(t),Ψi(t),Φi(t)andΘi(t), we deﬁne ∂i,α(t),Ψi,α(t),
Φi,α(t)andΘi,α(t)on graphGα(t), respectively. Subsequently, replace ∂i(t),Ψi,Φi(t)andΘi(t)by
∂i,α(t),Ψi,α(t),Φi,α(t)andΘi,α(t)in (54), respectively.
41Published in Transactions on Machine Learning Research (March/2023)
L Proof of Theorem 3
Step 1. Preliminaries.
In Example 2, β= 1,λ= 0, andγ= 0, there is no recovery and we assume no latent state. Based on
(9), the expression of rewards ˆri(t)for every node is given as follows. If node ihas two neighbors (without
quarantine)
ˆri(t) =u(i−1)
S(t)(1−u(i−2)
I(t))u(i)
I(t) +u(i+1)
S(t)(1−u(i+2)
I(t))u(i)
I(t). (55)
If nodeionly has one neighbor, then
ˆri(t) =u(i+d)
S(t)/parenleftbig
1−u(i+2d)
I (t)/parenrightbig
u(i)
I(t), d∈{− 1,1}. (56)
For simplicity, we introduce artiﬁcial nodes −1,0,N+ 1,N+ 2withu(−1)
I(t) =u(0)
I(t) =u(N+1)
I (t) =
u(N+2)
I (t) = 0for allt, and these 4nodes are never tested.
Step 2. The RbEx policy.
Under the RbEx policy, the algorithm always tests the nodes with maximum rewards. Let an infectious node
be found, for the ﬁrst time, on day aN, whereais a positive real number. Note that until the ﬁrst infected
node is found, in any application of the RbEx policy, u(i)
I(t)is the same for any given i, and hence ˆri(t)is
also the same. So, ais the same for any application of the RbEx policy. Recall that in Example 2, nodes
that are tested positive will be isolated. The cumulative infections is at least min{aN,N}in the end.
Step 3. Consider the exploration process of the speciﬁc exploration policy.
Recall that from Step 2, an infectious node is found, for the ﬁrst time, by the RbEx policy with budget 10
tests on day aN. Under the speciﬁc deﬁned exploration policy, we can choose a speciﬁc b/primewithb/prime<a, such
that no infectious node is tested by the RbEx policy with budget 9tests before and including day t=b/primeN.
We know that on day τ, nodes 1,2,···,τare infectious since β= 1. Note that one test is applied to
exploration (randomly choice) on every day, so with probability
τ/productdisplay
τ/prime=1(1−τ/prime
N), (57)
no infectious node is explored from the initial day to day τ. Then, with probability
τ−1/productdisplay
τ/prime=1(1−τ/prime
N)·τ
N,
one infectious node is detected on day τ. Thus, with probability
t/summationdisplay
τ=1τ−1/productdisplay
τ/prime=1(1−τ/prime
N)·τ−1
N, (58)
one infectious node is tested by exploration process on day τ(τ≤t), and this node is not the new infectious
one on day τ, i.e., has index τ. The probability deﬁned in (58) increases with twhenNis ﬁxed, and it
can be close to 1whentclose toN. Therefore, We can choose proper parameters b/primeandNsuch that the
probability deﬁned in (58) is larger than or equal to p0. In particular, if Nis large, we can choose a relatively
smallb/prime. In Theorem 3, we set p0≥99/100.
Let the infectious node detected (for the ﬁrst time) by the exploration process have index jon dayt/prime, where
t/prime≤t. As discussed above, node jis not the new infectious node on day t/prime, so we have j <t/prime. In other words,
nodej+ 1must be infecitous on day t/primewith a positive test result, i.e., Yj(t/prime) = 1. By Step 1 in Algorithm 4,
the updated posterior probability of node j
e(j)
I(t/prime−1) = 1, e(j)
S(t/prime−1) = 0. (59)
42Published in Transactions on Machine Learning Research (March/2023)
Again, by Step 1 in Algorithm 4,
w(j−1)
I (t/prime) =w(j+1)
I (t/prime) = 1. (60)
Then, by Step 2 in Algorithm 4,
u(j−2)
I(t/prime+ 1) =u(j−1)
I(t/prime+ 1) =u(j)
I(t/prime+ 1) =u(j+1)
I(t/prime+ 1) =u(j+2)
I(t/prime+ 1) = 1. (61)
Sincejis detected and isolated on day t/prime, then,
ˆrj(t/prime+ 1) = 0. (62)
By (56) and (61),
ˆrj−1(t/prime+ 1) =u(j−2)
S(t/prime+ 1)/parenleftbig
1−u(j−3)
I(t/prime+ 1)/parenrightbig
= 0
ˆrj+1(t/prime+ 1) =u(j+2)
S(t/prime+ 1)/parenleftbig
1−u(j+3)
I(t/prime+ 1)/parenrightbig
= 0.(63)
By (55) and (61),
ˆrj−2(t/prime+ 1) =u(j−3)
S(t/prime+ 1)/parenleftbig
1−u(j−4)
I(t/prime+ 1)/parenrightbig
ˆrj+2(t/prime+ 1) =u(j+3)
S(t/prime+ 1)/parenleftbig
1−u(j+4)
I(t/prime+ 1)/parenrightbig
.(64)
Step 4.The exploitation process of the speciﬁc exploration policy.
We ﬁrst study an extreme case where no tests are applied. In this case, denote the prior probability of node
ion dayτasU(i)
I(τ), which can be calculated by the following recursion:
U(i)
I(τ+ 1) =U(i)
I(τ) +U(i)
S(τ)/parenleftbig
1−(1−U(i−1)
I (τ))(1−U(i+1)
I(τ))/parenrightbig
. (65)
Based on (65), recall that U(i)
I(0) = 0ifi≤9N
10, andU(i)
I(0) =10/epsilon1
Nif9N
10< i≤N, thenU(i)
I(τ)increases
overτand is a function of /epsilon1. Then, given b/prime,Nandt=b/primeN, we can choose a small enough /epsilon1, denoted by
/epsilon1(b/prime,N), such that U(i)
I(2t)<1
2for alli. SinceU(i)
I(τ)increases over τ, thenU(i)
I(τ)<1
2, τ≤2t.
Now, we introduce the exploitation process. Let t=b/primeN < min{9
40,a}N. There are at most 2tinfectious
nodes on day 2t, i.e., nodes 1,2,···,2t. Sincet<min{9
40,a}N, then nodes with index from 9N/10−2ttoN
are in state S, which implies nodes with index from 9N/10−2ttoNcan never be tested positive before day
2t. Thus, on any day τ≤2t, for 9N/10−2t≤i≤N, if nodeiis tested, and the testing result is negative.
Recall that U(i)
I(τ)in (65) is calculated without any negative testing results. Hence, u(i)
I(τ)≤U(i)
I(τ).
Furthermore, with the condition t=b/primeN < min{9
40,a}N, we can ﬁnd a small enough /epsilon1(b/prime,N), such that
under the speciﬁc exploration policy,
u(i)
I(τ)<1
2, τ≤2t,9N/10−2t≤i≤N. (66)
In the rest, we divide the nodes in to 3sets:Q1={i|i≤2t},Q2={i|2t < i < 9N/10−2t}, and
Q3={i|9N/10−2t≤i≤N}.
Fact 1. Fori∈Q 1andτ≤2t,u(i)
I(τ) = 1oru(i)
I(τ) = 0.
Proof. If no test is applied to Q1, thenu(i)
I(τ) = 0for alli∈Q 1.
On some day τ≤2t, if one node with index j∈Q 1is tested positive on day τ−1, then by (61), u(j−2)
I(τ) =
u(j−1)
I(τ) =u(j)
I(τ) =u(j+1)
I(τ) =u(j+2)
I(τ) = 1. In other words, if node jis tested positive on day τ−1,
then nodej, its neighbors and neighbors of neighbors have probability of infection equal to 1on dayτ.
On some day τ, if nodejis not tested positive on day τ−1, and neither of its neighbors and neighbors of
neighbors are is not tested positive, then u(j)
I(τ) = 1only when u(j)
I(τ−1) = 1, oru(j−1)
I(τ−1) = 1or
u(j+1)
I(τ−1) = 1sinceβ= 1. Otherwise u(j)
I(τ) = 0.
43Published in Transactions on Machine Learning Research (March/2023)
Fact 2. Fori∈Q 1andτ≤2t,ˆri(τ) = 1orˆri(τ) = 0.
Proof. Ifu(i)
I(τ) = 0, then ˆri(τ) = 0by (55) and (56).
Now, we consider u(i)
I(τ) = 1in the following cases: (i) If both neighbors of node iare isolated, then
ˆri(τ) = 0. (ii) If one of neighbors of node i(for example, node i−1) is isolated, then by (56), ˆri(τ) = 0
whenu(i+1)
I(τ) = 1, and ˆri(τ) = 1whenu(i+1)
I(τ) = 0. (iii) If both neighbors are not isolated, then
u(i−1)
I(τ−1) = 1oru(i+1)
I(τ−1) = 1, otherwise, u(i)
I(τ) = 0. Since there is no recovery, then u(i−1)
I(τ) = 1
oru(i+1)
I(τ) = 1. ByFact 1,u(j)
I(τ) = 1oru(j)
I(τ) = 0whenj∈Q 1. Ifu(i+1)
I(τ) =u(i−1)
I(τ) = 1, then
ˆri(τ) = 0. Ifu(i+1)
I(τ) = 1, then by (55), ˆri(τ) = 0whenu(i−2)
I(τ) = 1,ˆri(τ) = 1whenu(i−2)
I(τ) = 0. If
u(i−1)
I(τ) = 1, then by (55), ˆri(τ) = 0whenu(i+2)
I(τ) = 1,ˆri(τ) = 1whenu(i+2)
I(τ) = 0.
From (66), for all τ≤2tandi∈Q 3, we have
ˆri(τ)≤2u(i)
I(τ)<1. (67)
Note that only nodes in Q1andQ3may have positive probability of infection. For all τ≤2tandi∈Q 2,
sinceu(i)
I(τ) = 0, then ˆri(τ) = 0. Therefore, a node with reward equal to 1has the largest reward.
Recall that on day t/prime, nodejis tested positive. From (64), nodes j−2andj+ 2have largest rewards ( = 1)
on dayt/prime+ 1, which are exploited on day t/prime+ 1, and all other nodes in Q1have rewards 0. This is because
t/primeis the ﬁrst day when a positive node is found. Since node jis tested positive and isolated on day t/prime, then
all infectious nodes with index less than jcan no longer infect other nodes in the line network. Now, we
consider the nodes with index larger than j. Recall that j < t/prime, so nodej+ 1must be infectious on day
t/prime, and node j+ 2must be infectious on day t/prime+ 1sinceβ= 1. Thus, node j+ 2is tested positive and is
isolated. Since the network is a line, both nodes j+ 1andj+ 2can no longer infect other nodes once node
j+ 2is isolated. Note that nodes in Q3have positive rewards. When Nis suﬃciently large, in the rest of
the exploitation process, nodes in Q3are tested. Recall that we have one test for exploration, and we can
isolate at least 2infectious nodes with index larger than j.
Repeat the process, we exploit nodes j+ 4,j+ 6,···on dayt/prime+ 2,t/prime+ 3,···, respectively. Consider the
direction from node 1to nodeN. On every day, there is at most one new infectious node, but at least two
infectious nodes can be isolated. On some day, denote as day t/prime+x, the exploitation process can progress
beyond the infections (exceeding by one node) for the ﬁrst time. In other words, node j+2xis tested negative
on dayt/prime+x. By Step 2 in Algorithm 4, e(j+2x)
I (t/prime+x−1) = 0. However, since w(j+2x−1)
I (t/prime+x) = 1
becuase node j+2x−2is tested positive on day t/prime+x−1. By Step 1 in Algorithm 4, u(j+2x)
I (t/prime+x+1) = 1,
hence by (64), ˆrj+2x(t/prime+x+ 1) = 1 , which implies node j+ 2xhas the largest reward and is exploited on
dayt/prime+x+ 1, and it will be tested positive. On day t/prime+x+ 1, all infectious nodes are isolated.
Finally, we can calculate the total number of infections to be
j+ 2(t/prime−j) = 2t/prime−j≤2t/prime≤2t= 2b/primeN.
Letb= 2b/prime. This is an improvement by a factor of at leasta
bin comparison to the RbEx strategy, wherea
b
can be as large as desired by increasing the value of Nor decreasing p0.
M Construction of Networks and Further Results
M.1 Constructions of SBM and V-SBM
In this section, we construct SBMs and its variants.
SBMThe SBM is a generative model for random graphs. The graph is divided into several communities,
and subsets of nodes are characterized by being connected with one another with particular edge densities.8
The intra-connection probability is p1, and inter-connection probability is p2. We denote the SBM as
8Here, we assume that Mis an exact divisor of N.
44Published in Transactions on Machine Learning Research (March/2023)
WS, (d,δ)γcLp Ratio
(6,0.05)0.504 4.952.0003
(4,0)0.500 62.876 0.191
(6,0.1)0.456 5.718−0.027
(4,0.03)0.456 10.810 0.097
Table 12: Clustering coeﬃcients of WS networks
WS, (d,δ)γcLp Ratio
(6,.001) 0.59921.188 0.209
(4,.0075) 0.48921.264 0.182
(6,.005) 0.59214.310 0.211
(4,.015) 0.47314.253 0.174
(6,.009) 0.58512.081 0.137
(4,.0225) 0.46712.171 0.125
Table 13: Clustering coeﬃcients of WS networks
SBM (N,M,p 1,p2). Note that the (expected) number of edges, denoted by |E|, is
|E|=p1
2N(N
M−1) +p2
2N2
M(M−1). (68)
Now, we ﬁx|E|, and choose the pair (p1,p2)under a ﬁxed|E|in (68). The aim of ﬁxing |E|is to guarantee
that the transmission of the disease would not be aﬀected by edges.
V-SBM Now, we consider a variant of SBM, denoted by V-SBM. Diﬀerent from SBM, we only allow
nodes in cluster ito connect to nodes in successive clusters (the neighbor clusters). Denote the V-SBM as
V-SBM (N,M,p 1,p2). Similarly, the expected number of edges, denoted by |E|, is
|E|=p1
2N(N
M−1) +p2N2
M. (69)
Now, we ﬁx|E|, and choose the pair (p1,p2)under a ﬁxed|E|in (69). The aim of ﬁxing |E|is to guarantee
that the transmission of the disease would not be aﬀected by edges.
M.2 The impact of γcandLpindividually
In this subsection, we investigate the role of γcandLpindividually, not through the common factor δ. We
consider diﬀerent WS networks with degrees d= 4,6, and then adjust the rewiring probability δ, such that
one of (γc,Lp)is almost constant, and the other is varying. We can see that the trend is similar to what we
observed by varying δin Table 6.
45