Published in Transations on Machine Learning Research (05/2024)
Exploring validation metrics for oﬄine model-based optimi-
sation with diﬀusion models
Christopher Beckham ﬁrst.last@mila.quebec
Mila, Polytechnique Montréal, ServiceNow Research†
Alexandre Piché ﬁrst.last@servicenow.com
Mila, Université de Montréal, ServiceNow Research
David Vazquez ﬁrst.last@servicenow.com
ServiceNow Research
Christopher Pal ﬁrst.last@polymtl.ca
Mila, Polytechnique Montréal, ServiceNow Research, CIFAR AI Chair
Reviewed on OpenReview: https: // openreview. net/ forum? id= wC4ZID0H9a
Abstract
In model-based optimisation (MBO) we are interested in using machine learning to design
candidates that maximise some measure of reward with respect to a black box function
called the (ground truth) oracle, which is expensive to compute since it involves executing
a real world process. In oﬄine MBO we wish to do so without assuming access to such
an oracle during training or validation, with makes evaluation non-straightforward. While
an approximation to the ground oracle can be trained and used in place of it during
model validation to measure the mean reward over generated candidates, the evaluation
is approximate and vulnerable to adversarial examples. Measuring the mean reward of
generated candidates over this approximation is one such ‘validation metric’, whereas we
are interested in a more fundamental question which is ﬁnding which validation metrics
correlate the most with the ground truth. This involves proposing validation metrics and
quantifying them over many datasets for which the ground truth is known, for instance
simulated environments. This is encapsulated under our proposed evaluation framework
which is also designed to measure extrapolation, which is the ultimate goal behind leveraging
generative models for MBO. While our evaluation framework is model agnostic we speciﬁcally
evaluate denoising diﬀusion models due to their state-of-the-art performance, as well as
derive interesting insights such as ranking the most eﬀective validation metrics as well as
discussing important hyperparameters.
1 Introduction
In model-based optimisation (MBO), we wish to learn a model of some unknown objective function f:X→Y
wherefis the ground truth ‘oracle’, x∈Xis some characterisation of an input and y∈R+is thereward.
The larger the reward is, the more desirable xis. In practice, such a function (a real world process) is often
prohibitively expensive to compute because it involves executing a real-world process. For instance if x∈X
is a speciﬁcation of a protein and the reward function is its potency regarding a particular target in a cell,
then synthesising and testing the protein amounts to laborious work in a wet lab. In other cases, synthesising
and testing a candidate may also be dangerous, for instance components for vehicles or aircraft. In MBO,
we want to learn models that can extrapolate – that is, generate inputs whose rewards are beyond that of
what we have seen in our dataset. However, we also need ot be rigorous in how we evaluate our models since
generating the wrong designs can come at a time, safety, or ﬁnancial cost.
†Work done while author was interning at ServiceNow Research.
1Published in Transations on Machine Learning Research (05/2024)
Model evaluation in MBO is not straightforward. Firstly, it does not adhere to a typical train/valid/test
pipeline that one would expect in other problems. Secondly, it involves evaluating samples that are not from
the same distribution as the training set (after all, we want to extrapolate beyond the training set). To
understand these diﬃculties more precisely, we give a quick refresher on a typical training and evaluation
pipeline for generative models. If we assume the setting of empirical risk minimisation (Vapnik, 1991) then
the goal is to ﬁnd parameters θ∗which minimise some training loss /lscripton the training set Dtrain:
θ∗= arg min
θL(Dtrain;θ) =Ex,y∼D train/lscript(x,y;θ), (1)
for any model of interest that is parameterised by θ, e.g. a generative model pθ(x). Since we do not wish to
overﬁt the training set, model selection is performed on a validation set Dvalidand we can write a variant of
Equation 1 where the actual model we wish to retain is the following:
θ∗= arg min
θ∈ΘM(Dvalid;θ) =Ex,y∼D validm(x,y;θ), (2)
whereMis a validation metric and Θ ={θ∗
j}m
j=1comprises a collection of models θj, i.e. each of them is
the result of optimising Equation 1 under a diﬀerent hyperparameter conﬁguration or seed. Whatever the
best model is according to Equation 2 is ﬁnally evaluated on the test set Dtestas an unbiased measure of
performance. Since Dtestalready comes with labels from the ground truth, it simply suﬃces to just evaluate
Equation 2 and report the result. However, in MBO we wish to generate new examples (in particular ones
with high reward), which is akin to generating our own ‘synthetic’ test set. However, we don’t know the
true values of y(the reward) of its examples unless we evaluate the ground truth oracle fon each generated
example, which is very expensive. Secondly, the synthetic test set that we have generated is not intended
to be from the same distribution as the training set, since the goal of MBO is to extrapolate and generate
examples conditioned on larger rewards than what was observed in the original dataset. This means that
the validation set in Equation 2 should notbe assumed to be the same distribution as the training set, and
evaluation should reﬂect this nuance.
To address the ﬁrst issue, the most reliable thing to do is to simply evaluate the ground truth on our synthetic
test set, but this is extremely expensive since each ‘evaluation’ of the ground truth fmeans executing a real
world process (i.e. synthesising a protein). Furthermore, since we focus on oﬄine MBO we cannot assume an
active learning setting during training like Bayesian optimisation where we can construct a feedback loop
between the oracle and the training algorithm. Alternatively, we could simply substitute the ground truth
with an approximate oracle ˜f(x), but it isn’t clear how reliable this is due to the issue of adversarial examples
(see Paragraph 2, Section 1). For instance, ˜fcould overscore examples in our generated test set and lead us
to believe our generated exmples are much better than what they actually are.
Although this is an unavoidable issue in oﬄine MBO, we can still try to alleviate it by ﬁnding validation
metrics (i.e.Min Equation 2) which correlate well with the ground truth over a range of datasets where it
is known and cheap to evaluate, for instance simulated environments. If we can do this over such datasets
then we can those empirical ﬁndings to help determine what validation metric we should use for a real world
oﬄine problem, where the ground truth is not easily accessible (see Figure 1). To address the second issue,
we intentionally construct our train/valid/test pipeline such that the validation set is designed to contain
examples with larger reward than the training set, and this sets our work apart from existing literature
examining generative models in MBO.
We note that this is very similar to empirical research in reinforcement learning, where agents are evaluated
under simulated environments for which the reward function is known and can be computed in silico. However,
these environments are ultimately there to inform a much grander goal, which is to have those agents operate
safely and reliably in the real world under real world reward functions.
2Published in Transations on Machine Learning Research (05/2024)
metric 
measure 
correlation MBO tasks where the ground truth 
oracle is known (this paper) MBO tasks where the ground truth oracle 
is not known 
metric 
real world generation knowledge transfer train validate 
Figure 1: We want to produce designs xthat have high reward according to the ground truth oracle y=f(x),
but this is usually prohibitively expensive to compute since it involves executing a real-world process. If we
instead considered datasets where the ground truth oracle is cheap to compute (for instance simulations), we
can search for cheap-to-compute validation metrics that correlate well with the ground truth. In principle,
this can facilitate faster and more economical generation of novel designs for real-world tasks where the
ground truth oracle is expensive to compute.
1.1 Contributions
Based on these issues we propose a training and evaluation framework which is amenable to ﬁnding good
validation metrics that correlate well with the ground truth. In addition, we also assume that the training
and validation sets are notfrom the same ground truth distribution. We lay out our contributions as follows:1
•We propose a conceptual evaluation framework for generative models in oﬄine MBO, where we would like
to ﬁnd validation metrics that correlate well with the ground truth oracle. We assume these validation
metrics are cheap to compute. While computing said correlations requires the use of datasets where the
ground truth oracle can be evaluated cheaply (i.e. simulations), they can still be useful to inform more real
world MBO tasks where the ground truth is expensive to evaluate. In that case, ﬁnding good validation
metrics can potentially provide large economic savings.
•While our proposed evaluation framework is agnostic to the class of generative model, we speciﬁcally
demonstrate it using the recently-proposed class of denoising diﬀusion probabilistic models (DDPMs) (Ho
et al., 2020). For this class of model, we examine two conditional variants: classiﬁer-based (Dhariwal &
Nichol, 2021) and classiﬁer-free (Ho & Salimans, 2022) guidance. Since DDPMs appear to be relatively
unexplored in MBO, we consider our empirical results on these class of models to be an orthogonal
contribution of our work.
•We explore ﬁve validation metrics in our work against four datasets in the Design Bench (Trabucco et al.,
2022) framework, motivating their use as well as describing their advantages and disadvantages. We run a
large scale study over diﬀerent hyperparameters and rank these validation metrics by their correlation
with the ground truth.
•Lastly, we derive some additional insights such as which hyperparameters are most important to tune. For
instance, we found that the classiﬁer guidance term is extremely important, which appears to underscore
the trade-oﬀ between sample quality and sample diversity, which is a commonly discussed dilemma in
generative modelling. We also contribute some thoughts on how diﬀusion models in oﬄine MBO can be
bridged with online MBO.
1Corresponding code can be found here: https://github.com/christopher-beckham/validation-metrics-offline-mbo
3Published in Transations on Machine Learning Research (05/2024)
2 Motivation and proposed framework
We speciﬁcally consider the subﬁeld of MBO that is data-driven (leverages machine learning) and is oﬄine.
Unlike online, the oﬄine case doesn’t assume an active learning loop where the ground truth can periodically
be queried for more labels. Since we only deal with this particular instantiation of MBO, for the remainder
of this paper we will simply say MBOinstead of oﬄine data-driven MBO . In MBO, very simple approach to
generation is to approximate the ground truth oracle fby training an approximate oracle fθ(x)– a regression
model – from some dataset D, and exploiting it through gradient ascent to generate a high reward candidates:
x∗= arg max
xf(x)≈arg max
xfθ(x), (3)
which can be approximated by iteratively running gradient ascent on the learned regression model for
t∈{1,...,T}:
xt+1←xt+η∇xfθ(x), (4)
andx0is sampled from some prior distribution. The issue here however is that for most problems, this
will produce an input that is either invalid(e.g. not possible to synthesise) or is poor yet receives a large
reward from the approximate oracle ( overestimation ). This is the case when the space of valid inputs lies on
a low-dimensional manifold in a much higher dimension space (Kumar & Levine, 2020). How these problems
are mitigated depends on whether one approaches MBO from a discriminative modelling point of view (Fu &
Levine, 2021; Trabucco et al., 2021; Chen et al., 2022a), or a generative modelling one (Brookes et al., 2019;
Fannjiang & Listgarten, 2020; Kumar & Levine, 2020). For instance, Equation 3 implies a discriminative
approach where fθ(x)is a regression model. However, in Equation 4 it is reinterpreted as an energy model
Welling & Teh (2011). While this complicates the distinction between discriminative and generative, we refer
to the generative approach as one where it is clear that a joint distribution pθ(x,y)is being learned. For
instance, if we assume that pθ(x,y) =pθ(y|x)pθ(x), then the former is a probabilistic form of the regression
model and the latter models the likelihood, and modelling some notion of it will almost certainly mitigate
the adversarial example issue since it is modelling the prior probability of observing such an input. Because
of this, we argue that a generative view of MBO is more appropriate and we will use its associated statistical
language for the remainder of this paper.
Let us assume we have trained a conditional generative model of the form pθ(x|y)on our training set Dtrain.
Ifptrain(y)denotes the empirical distribution over the y’s in the training set, then this can be used to write
the joint distribution as pθ(x,y) =pθ(x|y)ptrain(y). We do not wish to sample from this joint distribution
however, because we ultimately want to generate x’s whosey’s are as large as possible. To do this we could
switch out the prior ptrain(y)for one that reﬂects the range of values we wish to sample from. However, it
wouldn’t be clear if the model has generalised in this regime of y’s; for instance the sampled x’s may be
invalid. What we want to do is be able to measure and select for models (i.e. pθ(x|y)for some good θ)
that are able to extrapolate ; in other words, models which assign small loss to examples that have larger y’s
than those observed in the training set . As an example, one such appropriate loss could be the negative log
likelihood.
We can measure this through careful construction of our training and validation sets without having to leave
the oﬄine setting. Assume the full dataset D={(xi,yi}n
i=1and(x,y)∼p(x,y), the ground truth joint
distribution. Given some threshold γ, we can imagine dealing with two truncated forms of the ground truth
p0,γ(x,y)andpγ(x,y), where:
p0,γ(x,y) ={(x,y)∼p(x,y)|y∈[0,γ]}
pγ(x,y) ={(x,y)∼p(x,y)|y∈(γ,∞]}, (5)
wherepγis the distribution of samples which are not seen during training but nonetheless we would like our
generative model to explain well. Therefore, if we split Dbased onγthen we can think of the left split Dtrain
as a ﬁnite collection of samples from pγ(x,y)and the right split Dvalidfromp0,γ(x,y). We would like to train
models onDtrainand maximise some measure of desirability on Dvalid(via hyperparameter tuning) to ﬁnd
the best model.
4Published in Transations on Machine Learning Research (05/2024)
metric 
train
extrapolate generate validation scoring 
Algorithm 1 Algorithm 2 
Figure 2: A visualisation of our evaluation framework. Here, we assume joint generative models of the form
pθ(x,y). Models are trained on Dtrainas per Section 2.1, and in this paper we assume the use of conditional
denoising diﬀusion probabilistic models (DDPMs). For this class of model the joint distribution pθ(x,y)
decomposes into pθ(x|y)p(y), and the way we condition the model on yis described in Section 2.1.1. In order
to generate samples conditioned on rewards ylarger than what was observed in the training set, we must
switch the prior distribution of the model, which corresponds to ‘extrapolating’ it and is described in Section
2.2. Validation is done periodically during training and the best weights are saved for each validation metric
considered. The precise details of this are described in Algorithm 1. When the best models have been found
we perform a ﬁnal evaluation on the real ground truth oracle, and this process is described in Algorithm 2.
2.1 Training and generation
While our proposed evaluation framework is agnostic to the class of generative model used, in this paper we
speciﬁcally focus on denoising diﬀusion probabilistic models (DDPMs) (Sohl-Dickstein et al., 2015; Ho et al.,
2020). DDPMs are currently state-of-the-art in many generative tasks and do not suﬀer from issues exhibited
from other classes of model. They can be seen as multi-latent generalisations of variational autoencoders,
and just like VAEs they optimise a variational bound on the negative log likelihood of the data2. First, let us
consider an unconditional generative model pθ(x), whose variational bound is:
−Ex0∼p(x0)logpθ(x0)≤Ep(x0,...,xT)/bracketleftBig
logp(x1,...,xT|x0)
pθ(x0,...,xT)/bracketrightBig
(6)
=Ep(x0,...,xT)/bracketleftBig
logp(xT)
p(xT|x0)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
LT+/summationdisplay
t>1logpθ(xt−1|xt)
p(xt−1|xt,x0)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Lt−logpθ(x0|x1)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
L0/bracketrightBig
,(7)
Here, x0∼p(x0)is the real data (we use x0instead of xto be consistent with DDPM notation, but they are
the same), and p(xt|xt−1)fort∈{1,...,T}is a predeﬁned Gaussian distribution which samples a noisy xt
which contains more noise than xt−1, i.e. astgets larger the original data x0progressively becomes more
noised. These conditional distributions collectively deﬁne the forward process . The full joint distribution of
this forward process is3:
p(x0,...,xT) =p(x0)T/productdisplay
t=1p(xt|xt−1), (8)
2Typical DDPM literature uses qto deﬁne the real distribution, but here we use pto be consistent with earlier notation,
though it is not to be confused with pθ, the learned distribution.
3Note we have still used orange to denote that p(x0)is the ground truth data distribution, but we have omitted it for
p(xt|xt−1)to make it clear that these are predeﬁned noise distributions.
5Published in Transations on Machine Learning Research (05/2024)
and each conditional noising distribution has the form:
p(xt|xt−1) =N(xt;/radicalbig
1−βtxt−1,βtI), (9)
whereβtfollows a predeﬁned noise schedule. Since the product of Gaussians are also Gaussian, one can also
deﬁne thet-step noising distribution p(xt|x0)which is more computationally eﬃcient:
p(xt|x0) =N(xt;√¯αtx0+ (1−¯αt)I) (10)
=⇒xt=√¯αtx0+/radicalbig
(1−¯αt)/epsilon1, /epsilon1∼N(0,I) (11)
whereαt= 1−βtand ¯αt=/producttextt
s=1αs. Generally speaking, we wish to learn a neural network pθ(xt−1|xt)to
undo noise in the forward process, and these learned conditionals comprise a joint distribution for the reverse
process:
pθ(xt−1|xt) =N(xt−1;µθ(xt,t);βt−1I), (12)
wherepθis expressed via a neural network µθwhich learns to predict the mean of the conditional distribution
forxt−1given xt, and we wish to ﬁnd parameters θto minimise the expected value of Equation 6 over the
entire dataset. In practice however, if the conditionals for pθandpare assumed to be Gaussian, one can
dramatically simplify Equation 6 and re-write Lt(for anyt) as a noise prediction task. where instead we
parameterise a neural network /epsilon1θ(xt,t)to predict the noisefromxt∼p(xt|x0), as shown in Equation 11 via
the reparamterisation trick. Note that /epsilon1θ(xt,t)andµθ(xt,t)diﬀer by a closed form expression (and likewise
withpθ(xt−1|x)), but for brevity’s sake we defer those details to Ho et al. (2020) and simply state the ﬁnal
loss function to be:
Ex0∼p(x0),t∼U(1,T),/epsilon1∼N(0,I)/bracketleftbig
/bardbl/epsilon1−/epsilon1θ(√¯αtx0+√1−¯αt/epsilon1,t)/bardbl2/bracketrightbig
, (13)
where√¯αtx0+√1−¯αt/epsilon1tis simply just writing out xt∼p(xt|x0)via the reparamterisation trick (Equation
11), and{αt}T
t=1deﬁnes a noising schedule. Since we have deﬁned our training set Dtrainto be samples from
p0,γ(x,y), our training loss is simply:
min
θLDSM(θ) = min
θEx0∼D train,/epsilon1t∼N(0,I)/bracketleftbig
/bardbl/epsilon1t−/epsilon1θ(√¯αtx0+√1−¯αt/epsilon1t,t)/bardbl2/bracketrightbig
. (14)
In order to generate samples, stochastic Langevin dynamics (SGLD) is used in conjunction with the noise
predictor/epsilon1θto construct a Markov chain that by initialising xT∼p(xT) =N(0,I)and running the following
equation for t∈{T−1,..., 0}:
xt−1=1√αt/parenleftBig
xt−1−αt√1−¯αt/epsilon1θ(xt,t)/parenrightBig
+βtz,z∼N(0,I). (15)
Note that while the actual neural network that is trainedis a noise predictor /epsilon1θ(xt,t)which in turn has a
mathematical relationship with the reverse conditional pθ(xt−1|xt)we will generally refer to the diﬀusion
model as simply pθ(x)throughout the paper, since the use of Equation 15 implies a sample from the
distribution pθ(x0), and we have already deﬁned x=x0.
2.1.1 Conditioning
Classiﬁer-based guidance Note that Equation 14 deﬁnes an unconditional modelpθ(x). In order to
be able to condition on the reward y, we can consider two options.4The ﬁrst is classiﬁer-based guidance
(Dhariwal & Nichol, 2021). Here, we train an unconditional diﬀusion model pθ(x), but during generation we
deﬁne aconditional noise predictor which leverages pre-trained classiﬁer pθ(y|xt)which predicts the reward
fromxt:
/epsilon1θ(xt,t,y) =/epsilon1θ(xt,t)−√1−¯αtw∇xtlogpθ(y|xt;t)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
classiﬁer-based guidance(16)
4While it is possible to derive a conditional ELBO from which a DDPM can be derived from, the most popular method for
conditioning in DDPMs appears to be via guiding an unconditional model instead.
6Published in Transations on Machine Learning Research (05/2024)
wherepθ(y|x)is also trained on Dtrainandw∈R+is a hyperparameter which balances sample diversity and
sample quality. Equation 16 is then plugged into the SGLD algorithm (Equation 15) to produce a sample
from the conditional distribution pθ(x|y). Note that since pθ(y|xt;t)is meant to condition on xtfor anyt, it
ideally requires a similar training setup to that of the diﬀusion model, i.e. sample diﬀerent xt’s via Equation
10 train the network to predict y.
Classiﬁer-free guidance Inclassiﬁer-free guidance (Ho & Salimans, 2022), the noise predictor is re-
parameterised to support conditioning on label y, but it is stochastically ‘dropped’ during training with some
probability τ, in which case yis replaced with a null token:
LC-DSM (θ;τ) =E(x0,y)∼D train,t∼U(1,T),λ∼U(0,1)/bracketleftbig
/bardbl/epsilon1−/epsilon1θ(√¯αtx0+√1−¯αt/epsilon1,1λ<τ(y),t)/bardbl2/bracketrightbig
.(17)
where 1λ<τ(y)is the indicator function and returns a null token if λ < τotherwiseyis returned. The
additional signiﬁcance of this is that at generation time, one can choose various tradeoﬀs of (conditional)
sample quality and diversity by using the following noise predictor, for some hyperparameter w:
¯/epsilon1θ(xt,t,y) = (w+ 1)/epsilon1θ(xt,t,y)−w/epsilon1θ(x,t)/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
classiﬁer-free guidance(18)
2.2 Extrapolation
Given one of the two conditioning variants in Section 2.1.1, we can denote our generative model as pθ(x|y),
which has been trained on (x,y)pairs fromDtrain. If we denote the distribution over y’s in the training
set asp0,γ(y)then through Bayes’ rule we can write the joint likelihood as: pθ(x,y) =pθ(x|y)p0,γ(y). This
equation essentially says: to generate a sample (x,y)from the joint distribution deﬁned by the model, we
ﬁrst sample y∼p0,γ(y), then we sample x∼pθ(x|y)via SGLD (Equation 15). Samples from p0,γ(y)can
be approximated by sampling from the empirical distribution of y’s over the training set. Decomposing the
joint distribution into a likelihood and a prior over ymeans that we can change the latter at any time. For
instance, if we wanted to construct an ‘extrapolated’ version of this model, we can simply replace the prior in
this equation with pγ(y), which is the prior distribution over yfor the validation set. We deﬁne this as the
extrapolated model (Figure 2, extrapolate caption):5
pθ,γ(x,y) =pθ(x|y)pγ(y) (19)
and samples pγ(y)can be approximated by sampling from the empirical distribution over yfrom the validation
set. Of course, it is not clear to what extent the extrapolated model would be able to generate high quality
inputs from y’s much larger than what it has observed during training. This is why we need to perform
model selection via the use of some validation metric that characterises the model’s ability to extrapolate.
2.3 Model selection
Suppose we trained multiple diﬀusion models, each model diﬀering by their set of hyperparameters. If we
denote the j’th model’s weights as θj, then model selection amounts to selecting a θ∗∈Θ={θj}m
j=1which
minimises some ‘goodness of ﬁt’ measure on the validation set. We call this a validation metric , which is
computed the held-out validation set Dvalid. One such metric that is ﬁt for a generative model would be the
expected log likelihood over samples in the validation set, assuming it is tractable:
θ∗= arg max
θ∈Θ1
|Dvalid|/summationdisplay
(x,y)∈Dvalidlogpθ,γ(x,y) (20)
wherepθ,γ(x,y) =pθ(x|y)pγ(y)is the extrapolated model as originally described in Equation 19. Since we
have already established that the validation set comprises candidates that are higher scoring than the training
set, this can be thought of as selecting for models which are able to explain(i.e. assign high conditional
5There are other ways to infer an extrapolated model from a ‘base’ model, and we describe some of these approaches in
Section 3.
7Published in Transations on Machine Learning Research (05/2024)
likelihood) to those samples. However, we cannot compute the conditional likelihood since it isn’t tractable,
and nor can we use its evidence lower bound (ELBO). This is because both conditional diﬀusion variants
(Section 2.1.1) assume a model derived from the unconditional ELBO which is a bound on pθ(x), rather than
the conditional ELBO is correspondingly a bound on pθ(x|y). Despite this, if we are training the classiﬁer-free
diﬀusion variant, we can simply use Equation 17 as a proxy for it but computed over the validation set:
MC-DSM (Dvalid;θ) =1
Dvalid/summationdisplay
(x0,y)∼D valid/bardbl/epsilon1t−/epsilon1θ(xt,y,t)/bardbl2, (21)
where xt∼p(xt|x0). Equation 21 can be thought of as selecting for models which are able to perform a
good job of conditionally denoising examples from the validation set. While Equation 21 seems reasonable,
arguably it is not directly measuring what we truly want, which is a model that can generate candidates that
are high rewarding as possible. Therefore, we should also devise a validation metric which favours models
that, when ‘extrapolated’ during generation time (i.e. we sample x|ywherey’s are drawn from pγ(y)), are
likely under an approximation of the ground truth oracle. This approximate oracle is called the validation
oraclefφ, and is trained on both Dtrain∪Dvalid(see Figure 2):
θ∗= arg max
θ∈ΘE˜x,y∼pθ,γ(x,y)fφ(˜x) = arg max
θ∈ΘE˜x∼pθ(x|y),y∼pγ(y)fφ(˜x). (22)
We consider a biased variant of Equation 22 where the expectation is computed over the best128 samples
generated by the model, to be consistent with Trabucco et al. (2022). This can be written as the following
equation:
Mreward (S;θ,φ) =1
KK/summationdisplay
i=1fφ/parenleftbig
sorted (S;fφ)i/parenrightbig
,Si∼pθ,γ(x,y) (23)
where sorted (S,fφ)sortsS={˜xi}iin descending order via the magnitude of prediction, and |S|/greatermuchK.
Lastly, there are two additional validation metrics we propose, and these have been commonly used for
adversarial-based generative models (which do not permit likelihood evaluation). For the sake of space we
defer the reader to Section A.1 and simply summarise them below:
•Fréchet distance (Heusel et al., 2017) (Equation S28): given samples from the validation set and samples
from the extrapolated model, ﬁt multivariate Gaussians to both their embeddings (a hidden layer in fφis
used for this) and measure the distance between them. This can be thought of as a likelihood-free way to
measure the distance between two distributions. We denote this as MFD.
•Density and coverage (Equation S32): Naeem et al. (2020) proposed ‘density’ and ‘coverage’ as improved
versions of precision and recall, respectively (O’Donoghue et al., 2020; Kynkäänniemi et al., 2019). In the
generative modelling literature, precision and recall measure both sample quality and mode coverage, i.e.
the extent to which the generative model can explain samples from the data distribution. We denote this
asMDC, which is a simple sum over the density and coverage metric.
We summarise all validation metrics – as well as detail their pros and cons – in Table 1. We also summarise
all preceding subsections in Algorithm 1, which constitutes our evaluation framework.
2.3.1 Final evaluation
We may now ﬁnally address the core question presented in this paper: what validation metrics are best
correlated with the ground truth? Since we have already deﬁned various validation metrics in Section 2.3,
the only thing left is to deﬁne an additional metric – a test metric – which is a function of the ground truth
oracle. We simply deﬁne this to be an unbiased estimate of Equation 23 which uses the ground truth oracle
fto rank the top 128 candidates:
Mtest-reward (S;θ,φ) =1
KK/summationdisplay
i=1f/parenleftbig
sorted (S;fφ)i/parenrightbig
,Si∼pθ,γ(x,y) (24)
8Published in Transations on Machine Learning Research (05/2024)
Algorithm 1 Training algorithm, with early implicit early stopping. For mpredeﬁned validation metrics,
the best weights for each along with their values and stored and returned. (For consistency of notation, each
validation metric is of the form Mj(X,˜X,θ,φ), though the true arguments that are taken may be a subset
of these. See Table 1.)
Require:
Thresholdγ, s.t.Dtrain∼p0,γ(x,y),Dvalid∼pγ(x,y) ⊿Eqns. 5
Number of training epochs nepochs, validation rate neval
Validation metrics (functions): {Mj}m
j=1,Mjhas argumentsMj(D,˜D,θ,φ)
h∈Hhyperparameters used to initialise θ
Validation oracle fφ(x) ⊿pre-trained on train + val Dtrain∪Dvalidset
1:Xtrain,Ytrain←Dtrain,Xvalid,Yvalid←Dvalid
2:θ←initialise (h)
3:best_weights←{θ}m
j=1 ⊿store best weights so far for each validation metric
4:best_metrics←{∞}m
j=1 ⊿store best (smallest) values seen per metric
5:forepoch in{1,...,nepochs}do
6:sample (x,y)∼Dtrain
7:θ←θ−η∇θLθ(x,y) ⊿L: eqn. 14, with either eqn. 16 or eqn. 18
8:ifepoch %neval= 0then
9:⊿Model selection (Sec. 2.3)
10: ˜D←{ (˜xi,yi)}|Dvalid|
i=1, where ˜xi∼pθ(x|yi)andyi= (Yvalid)i ⊿sample using eqn. 15
11:⊿Evaluate validation metrics (Table 1)
12: forjin{1,...,m}do
13: mj←Mj(Dvalid,˜D,θ,φ)
14: ifmj<best_metrics jthen
15: best_metrics j←mj ⊿found new best metric value for metric j
16: best_weightsj←θ ⊿ save new weights for metric j
17: end if
18: end for
19:end if
20:end for
21:returnbest_weights ,best_metrics
Algorithm 2 Final evaluation algorithm. As per Algorithm 1, each validation metric Mjis associated with
the best weights θjfound through hyperparameter tuning and early stopping. For each θjwe generate a
candidate setSiof examples, retain the Kbest candidates as per the predicted reward from validation oracle,
and then compute the real reward on the ground truth oracle.
Require:
best_weights ={θj} ⊿assumed to be the best weights found for each validation metric Mj
1:test_rewards←{}m
j=1
2:forjin{1,...,m}do
3:θ←best_weightsj ⊿Load in best weights for metric j
4:S←{ ˜xi}N
i=1, where ˜xi∼pθandyi∼Yvalid
5:valid_rewards ={fφ(Si)}N
i=1 ⊿predict reward for each generated candidate
6:π=argsort (valid_scores )1,...,K ⊿get topKexamples wrt to fφpredictions
7:test_rewards j←{f(Sπ(i))}K
i=1 ⊿get unbiased estimate of test rewards
8:end for
9:returntest_rewards
This is the test metric which is used to determine how well correlated our validation metrics are with the
ground truth. For instance, suppose we have trained many diﬀerent generative models via Algorithm 1 –
each model corresponding to a diﬀerent set of hyperparameters – if we run Algorithm 2 on each of these
9Published in Transations on Machine Learning Research (05/2024)
models then we can compute quantitative metrics such as correlation between best_metrics (Alg. 1) and
test_rewards (Alg. 2), and indeed this is what we will be demonstrating in Section 4.
While it seems reasonable to assume that Equation 24 would be most correlated with its validation set
equivalent (Equation 23), this would only be the case in the limit of a suﬃcient number of examples to be
used to train the validation oracle fφ. Otherwise, the less data that is used the more it will be prone to
overscoring ‘adversarial’ examples produced by the generative model. Therefore, it may be wise to consider
validation metrics which put less emphasis on the magnitude of predictions produced by the oracle, e.g.
measuring the distance between distributions.
Table 1: The column ‘distance?’ asks whether the validation metric is measuring some form of distance
between the extrapolated distribution pγ(x,y)and the extrapolated model pθ,γ(x,y).†: RKL = reverse
KL divergence, see Sec. A.2.2 for proof it is an approximation of this divergence; ‡: FKL = forward KL
divergence, since diﬀusion models optimise an evidence lower bound (Eqn. 6); ∗= distances here are measured
not in data space, but in the semantic space deﬁned by one of the hidden layers inside fφ.
eqn.requirefφmodel agnostic? distance?
MC-DSM (Dvalid;θ)21 7 7(approx. FKL)‡3
−Mreward (S;θ,φ)23 3 3 7
MAgr(Dvalid;θ;φ)26 3 3(approx. RKL)†3
MFD(Xvalid,˜X;φ)S28 3 3 Fréchet∗3
−MDC(Xvalid,˜X;φ)S32 3 3∗3
3 Related work
Design Bench Design Bench is an evaluation framework by Trabucco et al. (2022) that facilitates the
training and evaluation of MBO algorithms. Design Bench, as of time of writing, provides four discrete and
four continuous datasets. These datasets can be further categorised based on two attributes: whether a
ground truth oracle exists or not, and whether the input distribution is fully observed (i.e. the combined
train and test splits contain all possible input combinations). In terms of evaluation, Design Bench does
not prescribe a validation set (only a training set and test oracle), which we argue is important in order to
address the core question of our work, which is ﬁnding validation metrics that correlate well with the ground
truth oracle. While Trabucco et al. (2022) does allude to validation sets in the appendix, these do not convey
the same semantic meaning as our validation set since theirs is assumed to be a subsample of the training
set, and therefore come from the training distibution. Lastly, while the same appendix provides examples
for diﬀerent validation metrics per baseline, the overall paper itself is concerned with establishing reliable
benchmarks for comparison, rather than comparing validation metrics directly.
Validation metrics To be best of our knowledge, a rigorous exploration of validation metrics has not
yet been explored in MBO. The choice of validation metric is indeed partly inﬂuenced by the generative
model, since one can simply assign the validation metric to be the same as the training loss but evaluated
on the validation set. For example, if we choose likelihood-based generative models (essentially almost all
generative models apart from GANs), then we can simply evaluate the likelihood on the validation set and
use that as the validation metric (Equation 20). However, it has been well established that likelihood is a
relatively poor measure of sample quality and is more biased towards sample coverage (Huszár, 2015; Theis
et al., 2015; Dosovitskiy & Brox, 2016). While GANs have made it diﬃcult to evaluate likelihoods – they are
non-likelihood-based generative models – it has fortunately given rise to an extensive literature proposing
‘likelihood-free’ evaluation metrics (Borji, 2022), and these are extremely useful to explore for this study for
two reasons. Firstly, likelihood-free metrics are agnostic to the class of generative model used, and secondly
they are able to probe various aspects of generative models that are hard to capture with just likelihood.
As an example, the Fréchet Distance (FID) (Heusel et al., 2017) is commonly used to evaluate the realism
of generated samples with respect to a reference distribution, and correlates well with human judgement of
sample quality. Furthermore, metrics based on precision and recall can be used to quantify sample quality
and sample coverage, respectively (Sajjadi et al., 2018; Kynkäänniemi et al., 2019).
10Published in Transations on Machine Learning Research (05/2024)
train
test (full dataset) 
(a) Design Bench
train
validation 
 (b) Ours (case 1)
train
test (50%) and valid (50%) 
 (c) Ours (case 2)
Figure 3: 3a: Design Bench only prescribes a training split which is determined by a threshold γto only ﬁlter
examples whose y’s are less than or equal to this threshold. The full dataset, while technically accessible, is
not meant to be accessed for model selection as per the intended use of the framework. While the training set
could be subsampled to give an ‘inner‘ training set and validation set, the validation set would still come from
the same distribution as training, which means we cannot eﬀectively measure how well a generative model
extrapolates. To address this, we retain the training set but denote everything else (examples whose rewards
are>γ) to be the validation set (3b), and the validation oracle fφis trained onDtrain∪Dvalid. No test set
needs to be created since the ground truth oracle fis the ‘test set’. However, if the ground truth oracle does
not exist because the MBO dataset is not exact, we need to also prescribe a test set (3c). Since there is no
ground truth oracle f, we must train a ‘test oracle’ ˜fonDtrain∪Dvalid∪Dtest(i.e. the full dataset). Note
that this remains compatible with the test oracles prescribed by Design Bench, since they are also trained on
the full data. Furthermore, our training sets remain identical to theirs.
Use of validation set Compared to other works, the use of a validation set varies and sometimes details
surrounding how the data is split is opaque. For example, in Kumar & Levine (2020) there is no mention of a
training or validation set; rather, we assume that only DtrainandDtestexists, with the generative model being
trained on the former and test oracle on the latter (note that if the test oracle is approximate there is no need
for aDtest). This also appears to be the case for Fannjiang & Listgarten (2020). While Design Bench was
proposed to standardise evaluation, its API does not prescribe a validation set6. While the training set could
in principle be subsetted into a smaller training set and a validation set (such as in Qi et al. (2022)), the latter
would no longer carry the same semantic meaning as our notion of a validation set, which is intentionally
designed to not befrom the same distribution as the training set. Instead, our evaluation framework code
accesses the fulldataset via an internal method call to Design Bench, and we construct our own validation
set from it. We illustrate these diﬀerences in Figure 3.
Bayesian optimisation Bayesian optimisation (‘BayesOpt’) algorithms are typically used in onlineMBO,
where the online setting permits access to the ground truth during training, as a way to generate and
label new candidates. These algorithms can be seen as sequential decision making processes in which a
Bayesian probabilistic model fθ(x)is used to approximate the ground truth oracle f(e.g. a Gaussian process).
Bayesian optimisation alternates between an acquisition function choosing which candidate xto sample next
and updating fθto reﬂect the knowledge gained by having labelled xwith ground truth f. However, these
algorithms require the speciﬁcation of a prior over the data which can be cumbersome, and therefore recent
works have explored the pre-training of such priors on oﬄine data Wang et al. (2021); Hakhamaneshi et al.
(2021); Wistuba & Grabocka (2021).
Learned black box optimisers Black box optimisation comprises a wide range of algorithms, for instance
simulated annealing, genetic algorithms, and Bayesian optimisation (Paragraph 3). Typically these algorithms
assume a ﬁxed dimensionality for the input space, making it diﬃcult for them to adapt to diﬀerent input
dimensionalities (for instance among diﬀerent tasks). Because of this there is interest in learning black box
optimisers, with one such instance being ‘OptFormer’ Chen et al. (2022b) (a transformer-based model).
OptFormer is trained on a large corpus of precollected hyperparameter optimisation trajectories for various
6However, in Trabucco et al. (2022) (their Appendix F) some examples are given as to what validation metrics could be used.
11Published in Transations on Machine Learning Research (05/2024)
black box algorithms and tasks (i.e. diﬀerent policies). In principle this can also be utilised for MBO but it
would require precollected trajectories from existing policies.
While our focus is on oﬄine MBO, in a real world setting MBO is never fullyoﬄine; this is because one
needs to eventually verify that the candidates generated are actually useful and this can only be done with
the ground truth. In other words, oﬄine MBO is a way to ‘bootstrap’ an online MBO pipeline, by leveraging
past oracle evaluations to learn a model which can inform future evaluations. One way we can combine
both styles is to consider factorised generative models. For instance, in the case of diﬀusion models (Section
2.1.1) one can factorise a joint density pθ(x,y)into a prior over the data pθ(x)and a classiﬁer pθ(y|x). If
we assume that classiﬁers are easier to update in online fashion, then we can train pθ(y|x)as a Bayesian
probabilistic model. At generation time, we can hold the diﬀusion model ﬁxed and ﬁnetune the classiﬁer by
performing Bayesian optimisation.
In the more general case however, for generative models that admit latent space encodings of the data, a very
straightforward way to leverage knowledge of the former is to train BayesOpt algorithms on those encodings
(Maus et al., 2022). This can be especially useful if the latent space is of a much smaller dimension than the
data, or if the data space is discrete.
3.1 Modelling approaches
Model inversion networks The use of generative models for MBO was proposed by Kumar & Levine
(2020), under the name model inversion networks (MINs). The name is in reference to the fact that one
can learn the inverseof the oracle fθ−1:Y→X, which is a generative model. In their work, GANs are
chosen for the generative model, whose model we will denote as Gθ(z,y)– that is to say: x∼pθ(x|y)implies
we sample from the prior z∼p(z), then produce a sample x=Gθ(z,y). Atgeneration time the authors
propose the learning of the following prior distribution as a way to extrapolate the generative model7:
pζ(z,y)∗:= arg max
pζ(z,y)Ez,y∼pζ(z,y)y+E(z,y)∼pζ/bracketleftBig
/epsilon11logpθ(y|Gθ(z,y)) +/epsilon12logp(z)/bracketrightBig
, (25)
where/epsilon11and/epsilon12are hyperparameters that weight the agreement and the prior probability of the candidate z.
The agreement is measuring the log likelihood of ˜x=Gθ(z,y)being classiﬁed as yunder the training oracle
fθ(expressed probabilistically as pθ(y|x)), and can be thought of measuring to what extent the classiﬁer and
generative model ‘agree’ that ˜xhas a score of y. The log density p(z)can be thought of as a regulariser to
ensure that the generated candidate zis likely under the latent distribution of the GAN.
We note that agreement can be easily turned into a validation metric by simply substituting pθ(y|x)for the
validation oracle pφ(y|x). Note that if we assume that pφ(y|x)is a Gaussian, then the log density of some
inputyturns into the mean squared error up to some constant terms, so we we can simply write agreement
out as Ey∼pγ(y)/bardblfφ(Gθ(z,y))−y/bardbl2. This leads us to our second validation metric, which we formally deﬁne
as:
MAgr(Dvalid;θ) =1
|Dvalid|/summationdisplay
(x,y)∼D valid/bardblfφ(˜xi)−y/bardbl2,where ˜xi∼pθ(x|y) (26)
We remark that Equation 26 has a mathematical connection to the reverse KL divergence , one of many
divergences used to measure the discrepency between a generative and ground truth distribution. For inclined
readers, we provide a derivation expressing this relationship in the appendix, Section A.2.2.
Discriminative approaches In the introduction we noted that MBO methods can be seen as approaching
the problem from either a discriminative or a generative point of view (though some overlap can also certainly
exist between the two). In the former case, regularising the approximate oracle fθ(x)is key, and it is also the
model that is sampled from (e.g. as per gradient ascent in Equation 3). The key idea is that the approximate
7In Kumar & Levine (2020) this optimisation is expressed for a single (z,y)pair, but here we formalise it as learning a joint
distribution over these two variables. If this optimisation is expressed for a minibatch of (z,y)’s, then it can be seen as learning
an empirical distribution over those variables.
12Published in Transations on Machine Learning Research (05/2024)
oracle should act conservatively or pessimistically in out-of-distribution regions. Some examples include
mining for and penalising adversarial examples (Trabucco et al., 2021), encouraging model smoothness Yu
et al. (2021), conservative statistical estimators such as normalised maximum likelihood (Fu & Levine, 2021),
learning bidirectional mappings (Chen et al., 2022a), and mitigating domain shift (Qi et al., 2022).
Generative approaches Brookes et al. (2019) propose the use of variational inference to learn a sampling
distribution pθ(x|S)given a probabilistic form of the oracle pθ(S|x)as well as a pre-trained prior distribution
over the data pθ(x). Here,Sis some desirable range of y’s, and therefore pθ(x|S)can be thought of as the
‘extrapolated’ generative model. In Section A.2.1 we discuss how such a model can be viewed within our
evaluation framework. Lastly, Fannjiang & Listgarten (2020) proposes MBO training within the context of a
min-max game between the generative model and approximate oracle. Given some target range y∈San
iterative min-max game is performed where the generative model pθ(x)updates its parameters to maximise
the expected conditional probability over samples generated from that range, and the approximate oracle fθ(x)
updates its parameters to minimise the error between the generated predictions and that of the ground truth.
Since the latter isn’t accessible, an approximation of the error is used instead. In relation to our evaluation
framework, the extrapolated model would essentially be the ﬁnal set of weights θ(t)forpθ(x|S)|θ=θ(t)when
the min-max game has reached equilibrium.
For both approaches, there is a notion of leveraging an initial generative model pθ(x)and ﬁne-tuning it with
the oracle so that it generates higher-scoring samples in regions that it was not initially trained on. Both
the min-max and variational inference techniques can be thought of as creating the ‘extrapolated’ model
within the context of our evaluation framework (Figure 2). Therefore, while our evaluation framework does
not preclude these more sophisticated techniques, we have chosen to use the simplest extrapolation technique
possible – which is simply switching out the prior distribution – as explained in Section 2.1.
Diﬀusion models Recently, diﬀusion models (Ho et al., 2020) have attracted signiﬁcant interest due
to their competitive performance and ease of training. They are also very closely related to score-based
generative models (Song & Ermon, 2019; 2020). In diﬀusion, the task is to learn a neural network that can
denoise any xttoxt−1, whereq(x0,...,xT)deﬁnes a joint distribution over increasingly perturbed versions
of the real data q(x0). Assuming that q(xT)≈p(xT)for some prior distribution over xT, to generate a
sample Langevin MCMC is used to progressively denoise a prior sample xTintox0, and the result is a
sample from the distribution pθ(x0).8To the best of our knowledge, we are not aware of any existing works
that evaluate diﬀusion or score-based generative models on MBO datasets provided by Design Bench, and
therefore we consider our exploration into diﬀusion models here as an orthogonal contribution.
4 Experiments and Discussion
Dataset Our codebase is built on top of the Design Bench (Trabucco et al., 2022) framework. We consider
all continuous datasets in Design Bench datasets: Ant Morphology, D’Kitty Morphology, Superconductor,
and Hopper. Continuous datasets are chosen since we are using Gaussian denoising diﬀusion models, though
discrete variants also exist and we leave this to future work.
Both morphology datasets are ones in which the morphology of a robot must be optimised in order to maximise
a reward function. For these datasets, the ground truth oracle is a morphology-conditioned controller. For
Superconductor, the ground truth oracle is not accessible and therefore it is approximate. For Hopper, the
goal is to sample a large ( ≈5000 dimensional) set of weights which are used to parameterise a controller.
Data splits While our framework is built on top of Design Bench, as mentioned in Section 3 the evaluation
diﬀers slightly. In Design Bench, the user is only oﬃcially prescribed Dtrain, and any users intending to perform
validation or model selection should not use anything external to Dtrain. However, this is fundamentally
incompatible with our evaluation framework since we want our validation set to be out-of-distribution, and
subsampling a part of Dtrainto createDvaliddoes not satisfy this. As illustrated in Figure 3, we break this
8The Langevin MCMC procedure is theoretically guaranteed to produce a sample from pθ(x)(Welling & Teh, 2011). As
opposed to Equation 3, where no noise is injected into the procedure and therefore samples are mode seeking.
13Published in Transations on Machine Learning Research (05/2024)
Table 2: Summary of datasets used in this work. †: thresholds shown are all defaults from Design Bench,
with the exception of Hopper 50% which is a subset of the original Hopper dataset (see Paragraph 4 for
justiﬁcation);‡: because no exact oracle exists, the way the dataset is split corresponds to that shown in
Figure 3c.
# features|Dtrain|/|D|γ†fexists?
Ant Morphology 6010004 / 25009 165.33 3
Kitty Morphology 5610004 / 25009 199.36 3
Superconductor 8617014 / 21263 74.0‡7
Hopper 50% 5126 1600 / 3200 434.5 3
convention and deﬁne the validation split to be Dtrain\D, i.e. their set diﬀerence. Note that ifa ground truth
oracle exists, there is no need to deﬁne a Dtest, and this is the case for all datasets except Superconductor
(Figure 3b). Otherwise, for Superconductor a random 50% subsample of (Dtrain\D)is assigned toDvalid
(Figure 3c) and we use the pre-trained test oracle RandomForest-v0 provided with the framework, which was
trained on the full dataset D.
One nuance with the Hopper dataset is that the full dataset Dand the training set Dtrainare equivalent,
presumably because of the scarcity of examples. This means that a validation set cannot be extracted unless
the training set itself is redeﬁned, and this means that the training set in our framework is no longer identical
to that originally proposed by Design Bench. To address this, we compute the median ywith respect to
Dtrain, and take the lower half as Dtrainand the upper half as Dvalid. We call the ﬁnal dataset ‘Hopper 50%’,
to distinguish it from the original dataset.
Oracle pre-training The validation oracle fφis an MLP comprising of four hidden layers, trained on
Dtrain∪Dvalidwith the mean squared error loss function. We do not apply any special regularisation
tricks to the model. Note that in the case of the Superconductor – the only dataset that doesn’t admit
a ground truth oracle f– it is not to be confused with the approximate test oracle , which is trained on
D=Dtrain∪Dvalid∪Dtest. The approximate test oracle we use for Superconductor is RandomForest-v0 ,
which is provided with the framework.
Architecture The architecture that we use is a U-Net derived from HuggingFace’s ‘annotated diﬀusion
model’9, whose convolutional operators have been replaced with fully connected layers for all datasets except
Hopper. For Hopper, we use 1D convolutions because MLPs performed poorly and signiﬁcantly blew up the
number of learnable parameters. Furthermore, since we know that the Hopper dataset is a feature vector of
neural network weights it is useful to exploit locality.
For all experiments we train with the ADAM optimiser (Kingma & Ba, 2014), with a learning rate of 2×10−5,
β= (0.0,0.9), and diﬀusion timesteps T= 200. Experiments are trained for 5000 epochs with single P-100
GPUs. Input data is normalised with the min and max values per feature, with the min and max values
computed over the training set Dtrain. The same is computed for the score variable y, i.e. all examples in the
training set have their scores normalised to be within [0,1].
Experiments For each validation metric and dataset, we run many experiments where each experiment
is a particular instantiation of hyperparameters (see Section A.3.1 for more details), and the experiment
is run as per Algorithm 1. By running this algorithm for all experiments we can derive an m×Nmatrix
Uof validation metric values, where Uijdenotes the best validation metric value found for metric iand
experiment j. Then, if invoke Algorithm 2 on the same experiments this will give us a matrix Vofm×N
of test rewards. By plotting Uiagainst Viwe obtain the scatterplots shown in Figure 5, and the Pearson
correlation can be deﬁned simply as pearson (Ui,Vi)for thei’th validation metric.
9https://huggingface.co/blog/annotated-diffusion
14Published in Transations on Machine Learning Research (05/2024)
Ant (c.f.g.) Kitty (c.f.g.) SD (c.f.g.) Hopper50 (c.f.g.) Ant (c.g.) Kitty (c.g.) SD (c.g.)0.8
0.6
0.4
0.2
0.00.20.4 CDSM
 FD
 DC
 reward
 Agr
Figure 4: The Pearson correlation computed for each dataset / diﬀusion variant. sPearson correlations are
computed as per the description in Paragraph 4. Since each validation metric is desgned to be minimised,
the ideal metric should be highly negatively correlated with the test reward (Equation 24), which is to be
maximised. By counting the best metric per experiment, we obtain the following counts (the more ticks the
better):−Mreward:3,MFD:33,MAgr:333,MC−DSM:3
4.1 Results
Classiﬁer-free guidance In Figure 4 we plot the Pearson correlations achieved for each dataset as well as
each diﬀusion variant, classiﬁer-free guidance (‘cfg’) and classiﬁer-based (‘cg’). Since all validation metrics
are intended to be minimised, we are interested in metrics that correlate the most negatively with the
ﬁnal test reward, i.e. the smaller some validation metric is, the larger the average test reward as per
Equation 24. We ﬁrst consider the ﬁrst four columns of Figure 4, which correspond to just the classiﬁer-free
guidance experiments over all the datasets. The two most promising metrics appear to be MAgrandMreward.
Interestingly,MC-DSMperforms the best for Hopper50, but we also note that this was our worst performing
dataset and no we did not obtain favourable results for any validation metric considered. Since we had to
modify the dataset to permit a training split, it contains very few examples as well as an unusually large
feature to exmaple ratio (i.e. 5126 features for 1600 examples). Since MC-DSMis the only validation metric
which is nota function of the validation oracle, it is possible the validation oracle is too poor for the other
metrics to perform well.
Interestingly,MDCdoes not perform well for any of the datasets. It is unclear why however, since MDCis
a sum of two terms measuring precision and recall which are both useful things to measure for generative
models (see Section A.1). It may require further exploration in the form of weighted sums instead, since
it currently is deﬁned to give equal weighting to precision and recall. Since testing out various weighted
combinations of the two would have required signiﬁcantly more compute, we did not explore it.
Classiﬁer-based guidance The last three groups of barplots in Figure 4 constitute the classiﬁer guidance
experiments. For this set of experiments the two most promising metrics appear to be MFDandMAgr.
Note that while we have also plotted MC-DSMit is not appropriate as a validation metric for classiﬁer-based
guidance – this is because during those experiments τis deterministically set to 1, which means the score
matching loss in Equation 14 is never conditioned on the yvariable, and therefore /epsilon1θnever seesy. (The
barplots also corroborate this, as the correlation is virtually zero for all experiments.)
Summarising both guidance variants If we count the best validation metric per subplot, then the top
three areMAgr(3 wins),MFD(2 wins), andMreward(1 win), respectively. This suggests that if we were
to perform a real world MBO then these metrics should be most considered with respect to their rankings.
However, since this paper only concerns itself with Gaussian DDPMs, we can not conﬁdently extrapolate this
ranking to other classes of generative model or discrete datasets. We leave the testing of discrete diﬀusion
variants to future work.
Sample quality vs diversity We ﬁnd that the guidance hyperparameter wmakes a huge diﬀerence to
sample quality, and this is shown in Figure 5 for all datasets and across each validation metric. (For brevity’s
sake, we only show results for classiﬁer-free guidance, and defer the reader to Section A.4 for the equivalent
set of plots for classiﬁer guidance.) In each subplot individual points represent experiments and these are also
15Published in Transations on Machine Learning Research (05/2024)
0.06 0.07 0.08
CDSM
200300400500testreward
 = -0.1262
1.0 1.5 2.0
FD
200300400500testreward
 = 0.0564
1.5
 1.0
 0.5
DC
200300400500testreward
 = -0.0512
1.4
 1.3
 1.2
reward
200300400500testreward
 = -0.3081
20000 40000 60000
Agr
200300400500testreward
 = -0.2772
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
w
(a) Ant Morphology. Mreward andMAgrare the most negatively correlated with the test reward.
0.060 0.065 0.070
CDSM
200250testreward
 = -0.0046
0.3 0.4 0.5
FD
200250testreward
 = -0.2192
3.0
 2.5
 2.0
 1.5
DC
200250testreward
 = 0.1479
1.04
 1.03
 1.02
reward
200250testreward
 = -0.1611
3000 4000 5000
Agr
200250testreward
 = -0.0255
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
w
(b) Kitty Morphology (c.f.g.). Mreward andMFDare most negatively correlated with the test reward.
0.004 0.005 0.006
CDSM
050100testreward
 = -0.1529
25 50 75 100
FD
050100testreward
 = 0.1828
1.5
 1.0
DC
050100testreward
 = 0.4337
14
 12
 10
reward
050100testreward
 = -0.1714
500 1000
Agr
050100testreward
 = -0.2003
0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
w
(c) Superconductor (c.f.g.). Mreward andMAgrare most negatively correlated with the test reward.
0.120 0.125
CDSM
250
0250testreward
 = -0.5357
150 200 250
FD
250
0250testreward
 = -0.0964
0.3
 0.2
DC
250
0250testreward
 = 0.3893
16
 14
 12
 10
reward
250
0250testreward
 = 0.3143
20000 30000
Agr
250
0250testreward
 = 0.5286
20 40 60 80 100
w
(d) Hopper 50% (c.f.g.). MFDandMC-DSM are the most negatively correlated with the test reward.
Figure 5: Correlation plots for each dataset using the classiﬁer-free guidance (c.f.g.) diﬀusion variant. Each
point is colour-coded by w, which speciﬁes the strength of the ‘implicit’ classiﬁer that is derived (Equation
18). We can see that wmakes a discernible diﬀerence with respect to most of the plots shown. For additional
plots for other datasets, please see Section A.4.
colour-coded with guidance hyperparameter w. We can see that the choice of wmakes a huge diﬀerence with
respect to the test reward, and appears to highlight a well-established ‘dilemma’ in generative modelling,
which is the trade-oﬀ between sample quality andsample diversity (Ho & Salimans, 2022; Brock et al., 2018;
Burgess et al., 2018; Dhariwal & Nichol, 2021). For instance, if sample quality is too heavily weighted, then
sample diversity suﬀers and as a result the candidates we generate – which supposedly comprise high reward
– may actually truly be bad candidates when scored by the ground truth. Conversely, if sample diversity
16Published in Transations on Machine Learning Research (05/2024)
is too heavily weighted then we miss out on modes of the distribution which correspond to high-rewarded
candidates.
In Table 3 we present the 100th percentile test rewards for each combination of dataset and conditioning
variant. For each of these we simply choose the most promising validation metric as per Figure 4, ﬁnd the
best experiment, and then compute its 100th percentile score as is consistent with evaluation in Design Bench:
M100p(S;θ,φ) =max/bracketleftBig
{f/parenleftbig
sorted (S;fφ)i/parenrightbig
}K
i=1/bracketrightBig
,Si∼pθ,γ(x,y) (27)
Note that Equation 27 diﬀers from our test reward equation (Equation 24) in that a max is used as the
aggregator function instead of the mean. Lastly, we present 50th percentile results in Table S5, which
corresponds to a median instead of max.
Ant Morphology D’Kitty Morphology Superconductor Hopper50
Dtrain 0.565 0.884 0.400 0.272
Auto. CbAS 0.882±0.045 0.906±0.006 0.421±0.045
CbAS 0.876±0.031 0.892±0.008 0.503±0.069
BO-qEI 0.819±0.000 0.896±0.000 0.402±0.034
CMA-ES 1.214±0.732 0.724±0.001 0.465±0.024
Grad ascent 0.293±0.023 0.874±0.0220.518±0.024
Grad ascent (min) 0.479±0.064 0.889±0.011 0.506±0.009
Grad ascent (ensemble) 0.445±0.080 0.892±0.011 0.499±0.017
REINFORCE 0.266±0.032 0.562±0.196 0.481±0.013
MINs 0.445±0.080 0.892±0.011 0.499±0.017
COMs 0.944±0.016 0.949±0.015 0.439±0.033
Cond. Diﬀusion (c.f.g.) 0.954±0.025 0.972±0.006 0.645±0.1150.143±0.037
Cond. Diﬀusion (c.g.) 0.929±0.013 0.952±0.0100.664±0.007 –
Table 3: 100th percentile test rewards for methods from Design Bench (Trabucco et al., 2022) as well as our
diﬀusion results shown in the last two rows, with c.f.g standing for classiﬁer-free guidance (Equation 18) and
c.g. standing for classiﬁer-guidance (Equation 16). Each result is an average computed over six diﬀerent runs
(seeds). test rewards are min-max normalised with respect to the smallest and largest oracle scores in the full
dataset, i.e. any scores greater than 1 are greaterthan any score observed in the full dataset. Design Bench
results are shown for illustrative purposes only – while our training sets are equivalent to theirs, we use a
held-out validation set to guide model selection, which makes a direct comparison to Design Bench diﬃcult.
Which conditional variant should be used? From Table 3 both conditioning variants perform roughly
on par with each other, though it appears classiﬁer-free guidance performs slightly better. However, they
are not necessarily equally convenient to use in a real-world MBO setting. A real-world MBO setting would
involve some sort of online learning component since the ground truth oracle needs to be eventually queried
(see Section 3, Paragraph 3). Because of this, we recommend the use of classiﬁer-based guidance, where the
unconditional generative model pθ(x)can be independently trained oﬄine while the classiﬁer pφ(y|x)can be
a Bayesian probabilistic model which is able to be updated on per-example basis (i.e. after each query of the
ground truth).
5 Conclusion
In this work, we asked a fundamental question pertaining to evaluation in oﬄine MBO for diﬀusion-based
generative models: which validation metrics correlate well with the ground truth oracle? The key idea is that
if we can run our presented study at scale for a both a diﬃcult and diverse range of datasets for which the
ground truth is known, insights derived from those ﬁndings (such as what are robust validation metrics) can
be transferred to more real-world oﬄine MBO tasks where the actual ground truth oracle is expensive to
evaluate. To approach this, our evaluation framework is designed to measure how well a generative model
extrapolates: the training and validation sets are seen as coming from diﬀerent γ-truncated distributions,
where examples in the validation set comprise a range of y’s that are not covered by the training set and are
17Published in Transations on Machine Learning Research (05/2024)
largerthan those in the training set. Therefore, from the point of view of the generative model, the validation
set is out-of-distribution. Because model selection involves measuring some notion of desirability on the
validation set (via a validation metric), we are eﬀectively trying to select for models that can extrapolate .
While our proposed evaluation framework is model-agnostic, we presented it in the context of Gaussian
DDPMs on four continuous datasets prescribed by Design Bench, as well as across ﬁve diﬀerent validation
metrics and two forms of label conditioning for diﬀusion models: classiﬁer-free and classiﬁer-based guidance.
The ﬁve validation metrics we chose were inspired by existing MBO works as well as the GAN literature.
After ranking the validation metrics based on their correlation with the test reward, we found that the
best three performing ones in descending order were agreement, Fréchet Distance, and the validation score,
respectively. While we ran a considerable number of experiments in this study, further exploration should be
done in testing on discrete datasets (which would require discrete or latent diﬀusion models) as well as a
deeper exploration into how these models perform under diﬀerent sampling algorithms and design choices
(for instance see Karras et al. (2022)).
Lastly, we derived some interesting insights from our work. Regardless of which conditioning variant is used,
we found that the most important hyperparameter to tune is the classiﬁer guidance value, which controls the
trade-oﬀ between sample quality and sample diversity. Furthermore, we posit that the classiﬁer-based variant
of diﬀusion is likely more convenient in practice since it makes it easier to bridge the gap between oﬄine and
online MBO.
References
Asmussen, Søren and Glynn, Peter W. Stochastic simulation: algorithms and analysis , volume 57. Springer,
2007.
Borji, Ali. Pros and cons of GAN evaluation measures: New developments. Computer Vision and Image
Understanding , 215:103329, 2022.
Brock, Andrew, Donahue, Jeﬀ, and Simonyan, Karen. Large scale gan training for high ﬁdelity natural image
synthesis. arXiv preprint arXiv:1809.11096 , 2018.
Brookes, David, Park, Hahnbeom, and Listgarten, Jennifer. Conditioning by adaptive sampling for robust
design. In International conference on machine learning , pp. 773–782. PMLR, 2019.
Burgess, Christopher P, Higgins, Irina, Pal, Arka, Matthey, Loic, Watters, Nick, Desjardins, Guillaume, and
Lerchner, Alexander. Understanding disentangling in beta-vae. arXiv preprint arXiv:1804.03599 , 2018.
Chen, Can, Zhang, Yingxueﬀ, Fu, Jie, Liu, Xue (Steve), and Coates, Mark. Bidirectional learning for oﬄine
inﬁnite-width model-based optimization. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho,
K., and Oh, A. (eds.), Advances in Neural Information Processing Systems , volume 35, pp. 29454–29467.
Curran Associates, Inc., 2022a. URL https://proceedings.neurips.cc/paper_files/paper/2022/
file/bd391cf5bdc4b63674d6da3edc1bde0d-Paper-Conference.pdf .
Chen, Yutian, Song, Xingyou, Lee, Chansoo, Wang, Zi, Zhang, Richard, Dohan, David, Kawakami, Kazuya,
Kochanski, Greg, Doucet, Arnaud, Ranzato, Marc’aurelio, et al. Towards learning universal hyperparameter
optimizers with transformers. Advances in Neural Information Processing Systems , 35:32053–32068, 2022b.
Dhariwal, Prafulla and Nichol, Alexander. Diﬀusion models beat gans on image synthesis. Advances in
Neural Information Processing Systems , 34:8780–8794, 2021.
Dosovitskiy, Alexey and Brox, Thomas. Generating images with perceptual similarity metrics based on deep
networks. Advances in neural information processing systems , 29, 2016.
Dowson, DC and Landau, BV666017. The Fréchet distance between multivariate normal distributions.
Journal of multivariate analysis , 12(3):450–455, 1982.
Fannjiang, Clara and Listgarten, Jennifer. Autofocused oracles for model-based design. Advances in Neural
Information Processing Systems , 33:12945–12956, 2020.
18Published in Transations on Machine Learning Research (05/2024)
Fu, Justin and Levine, Sergey. Oﬄine model-based optimization via normalized maximum likelihood
estimation. arXiv preprint arXiv:2102.07970 , 2021.
Hakhamaneshi, Kourosh, Abbeel, Pieter, Stojanovic, Vladimir, and Grover, Aditya. Jumbo: Scalable
multi-task bayesian optimization using oﬄine data. arXiv preprint arXiv:2106.00942 , 2021.
Hamidieh, Kam. A data-driven statistical model for predicting the critical temperature of a superconductor.
Computational Materials Science , 154:346–354, 2018.
Heusel, Martin, Ramsauer, Hubert, Unterthiner, Thomas, Nessler, Bernhard, Klambauer, Günter, and
Hochreiter, Sepp. GANs trained by a two time-scale update rule converge to a nash equilibrium. CoRR,
abs/1706.08500, 2017. URL http://arxiv.org/abs/1706.08500 .
Ho, Jonathan and Salimans, Tim. Classiﬁer-free diﬀusion guidance. arXiv preprint arXiv:2207.12598 , 2022.
Ho, Jonathan, Jain, Ajay, and Abbeel, Pieter. Denoising diﬀusion probabilistic models. Advances in Neural
Information Processing Systems , 33:6840–6851, 2020.
Huszár, Ferenc. How (not) to train your generative model: Scheduled sampling, likelihood, adversary? arXiv
preprint arXiv:1511.05101 , 2015.
Karras, Tero, Aittala, Miika, Aila, Timo, and Laine, Samuli. Elucidating the design space of diﬀusion-based
generative models. Advances in Neural Information Processing Systems , 35:26565–26577, 2022.
Kingma, Diederik P and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Kumar, Aviral and Levine, Sergey. Model inversion networks for model-based optimization. Advances in
Neural Information Processing Systems , 33:5126–5137, 2020.
Kynkäänniemi, Tuomas, Karras, Tero, Laine, Samuli, Lehtinen, Jaakko, and Aila, Timo. Improved precision
and recall metric for assessing generative models. Advances in Neural Information Processing Systems , 32,
2019.
Maus, Natalie, Jones, Haydn, Moore, Juston, Kusner, Matt J, Bradshaw, John, and Gardner, Jacob. Local
latent space bayesian optimization over structured inputs. Advances in Neural Information Processing
Systems, 35:34505–34518, 2022.
Naeem, Muhammad Ferjad, Oh, Seong Joon, Uh, Youngjung, Choi, Yunjey, and Yoo, Jaejun. Reliable
ﬁdelity and diversity metrics for generative models. In International Conference on Machine Learning , pp.
7176–7185. PMLR, 2020.
O’Donoghue, Brendan, Osband, Ian, and Ionescu, Catalin. Making sense of reinforcement learning and
probabilistic inference. arXiv preprint arXiv:2001.00805 , 2020.
Piche, Alexandre, Pardinas, Rafael, Vazquez, David, Mordatch, Igor, and Pal, Chris. Implicit oﬄine
reinforcement learning via supervised learning. arXiv preprint arXiv:2210.12272 , 2022.
Qi, Han, Su, Yi, Kumar, Aviral, and Levine, Sergey. Data-driven oﬄine decision-making via invariant
representation learning. arXiv preprint arXiv:2211.11349 , 2022.
Sajjadi, Mehdi SM, Bachem, Olivier, Lucic, Mario, Bousquet, Olivier, and Gelly, Sylvain. Assessing generative
models via precision and recall. Advances in neural information processing systems , 31, 2018.
Sohl-Dickstein, Jascha, Weiss, Eric, Maheswaranathan, Niru, and Ganguli, Surya. Deep unsupervised learning
using nonequilibrium thermodynamics. In International conference on machine learning , pp. 2256–2265.
PMLR, 2015.
Song, Yang and Ermon, Stefano. Generative modeling by estimating gradients of the data distribution.
Advances in Neural Information Processing Systems , 32, 2019.
19Published in Transations on Machine Learning Research (05/2024)
Song, Yang and Ermon, Stefano. Improved techniques for training score-based generative models. Advances
in neural information processing systems , 33:12438–12448, 2020.
Theis, Lucas, Oord, Aäron van den, and Bethge, Matthias. A note on the evaluation of generative models.
arXiv preprint arXiv:1511.01844 , 2015.
Trabucco, Brandon, Kumar, Aviral, Geng, Xinyang, and Levine, Sergey. Conservative objective models
for eﬀective oﬄine model-based optimization. In International Conference on Machine Learning , pp.
10358–10368. PMLR, 2021.
Trabucco, Brandon, Geng, Xinyang, Kumar, Aviral, and Levine, Sergey. Design-bench: Benchmarks for
data-driven oﬄine model-based optimization. In International Conference on Machine Learning , pp.
21658–21676. PMLR, 2022.
Vapnik, Vladimir. Principles of risk minimization for learning theory. Advances in neural information
processing systems , 4, 1991.
Wang, Zi, Dahl, George E, Swersky, Kevin, Lee, Chansoo, Mariet, Zelda, Nado, Zachary, Gilmer, Justin,
Snoek, Jasper, and Ghahramani, Zoubin. Pre-trained gaussian processes for bayesian optimization. arXiv
preprint arXiv:2109.08215 , 2021.
Welling, Max and Teh, Yee W. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings
of the 28th international conference on machine learning (ICML-11) , pp. 681–688, 2011.
Weng, Lilian. What are diﬀusion models? lilianweng.github.io , Jul 2021. URL https://lilianweng.github.
io/posts/2021-07-11-diffusion-models/ .
Wistuba, Martin and Grabocka, Josif. Few-shot bayesian optimization with deep kernel surrogates. In
International Conference on Learning Representations , 2021.
Yu, Sihyun, Ahn, Sungsoo, Song, Le, and Shin, Jinwoo. Roma: Robust model adaptation for oﬄine
model-based optimization. Advances in Neural Information Processing Systems , 34:4619–4631, 2021.
20Published in Transations on Machine Learning Research (05/2024)
A Appendix
A.1 Validation metrics
Frechet Distance ‘Likelihood-free’ metrics are used almost exclusively in the GAN literature because
there is no straightforward way to compute likelihoods for this class of models, i.e. pθ(x|y)cannot be
evaluated. Furthermore, the search for good metrics is still an active topic of research (Borji, 2022). Common
likelihood-free metrics involve measuring some distance between distributions in some predeﬁned feature
space. For instance, for GANs trained on natural image datasets the Fréchet Inception Distance (FID)
(Heusel et al., 2017) is used to ﬁt Gaussians to both distributions with respect to the feature space of an
InceptionNet classiﬁer trained on ImageNet. Since the acronym ‘FID’ speciﬁcally refers to a very speciﬁc
InceptionNet-based model, we will simply call it ‘FD’. If we assume that FD is computed in some latent space
characterised by an arbitrary feature extractor fh:X→H, then FD can be computed in closed form as
follows (Dowson & Landau, 1982):
MFD(X,˜X;fh) =|µ(fh(X))−µ(fh(˜X))|+Tr(Σ(fh(X)) + Σ(fh(˜X))−2Σ(fh(˜X))Σ(fh(˜X))1
2)
(S28a)
whereMFD∈R+and lower FD is better. FD is also known as the 2-Wasserstein distance. Here, X∈RN×p
denotesNsamples coming from a reference distribution (i.e. ground truth) and ˜Xare samples coming from
the generative model. H=fh(X)denotes these inputs mapped to some feature space. Conveniently, we
can simply deﬁne the feature space to be with respect to some hidden layer of the validation oracle . One
caveat of FD is that it may have a stronger bias towards recall (mode coverage) than precision (sample
quality) (Kynkäänniemi et al., 2019) and that it reports a single number, which makes it diﬃcult to tease
apart how well the model contributes to precision and recall. Furthermore, while there exists a canonical
network architecture and set of weights to use for evaluating generative models on natural image datasets (i.e.
a particular Inception-V3 network that gives rise to the Frechet Inception Distance), this is not the case for
other types of datasets. This means that, unless a particular feature extractor is agreed upon, comparing
results between papers is non-trivial.
Density and coverage We also consider ‘density and coverage’ (Naeem et al., 2020), which corresponds
to an improved version of the ‘precision and recall’ metric proposed in Kynkäänniemi et al. (2019). In essence,
these methods estimate the manifold of both the real and fake data distributions in latent space via the
aggregation of hyperspheres centered on each point, and these are used to deﬁne precision and recall: precision
is theproportion of fake data that can be explained by real data (in latent space), and recall is the proportion
of real data that can be explained by fake data (again, in latent space).
Similar to FD (Paragraph A.1), let us denote Hias the example Xiembedded in latent space. Let us also
deﬁneB(Hi,NNDK(Hi)as the hypersphere centered on Hiwhose radius is the k-nearest neighbour, and k
is a user-speciﬁed parameter. ‘Density’ (the improved precision metric) is deﬁned as:
Mdensity (H,˜H;k) =1
kNM/summationdisplay
j=1N/summationdisplay
i=11/braceleftBig
˜Hj∈B(Hi,NNDk(Hi))/bracerightBig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
how many real neighbourhoods
does fake sample ˜xjbelong to?, (S29)
where 1(·)is the indicator function, and large values corresponds to a better density. While coverage (improved
‘recall’) can be similarly deﬁned by switching around the real and fake terms like so, the authors choose
to still leverage a manifold around real samples due to the concern of potentially too many outliers in the
21Published in Transations on Machine Learning Research (05/2024)
generated distribution ˜H. As a result, their coverage is deﬁned as:
Mcoverage (H,˜H;k) =1
NN/summationdisplay
i=1M/uniondisplay
j=11/braceleftBig
˜Hj∈B(Hi,NNDk(Hi))/bracerightBig
(S30)
=1
NN/summationdisplay
i=11/braceleftBig
∃js.t.˜Hj∈B(Hi,NNDk(Hi))/bracerightBig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
is there anyfake sample belonging
toxi’s neighbourhood?, (S31)
where, again, a larger value corresponds to a better coverage. This leads us to the addition of both metrics,
MDC, which is simply:
MDC(X,˜X;fh,k) =Mdensity (fh(X),fh(˜X);k) +Mcoverage (fh(X),fh(˜X);k) (S32a)
Similar to FD, we use the validation oracle fθto project samples into the latent space. We do not tune kand
simply leave it to k= 3, which is a recommended default.
A.2 Related work
A.2.1 Conditioning by adaptive sampling
CbAS (Brookes et al., 2019), like our proposed method, approaches MBO from a generative modelling
perspective. Given some pre-trained ‘prior’ generative model on the input data pθ(x), the authors propose
the derivation of the conditional generative model pθ(x|y)via Bayes’ rule:
pθ(x|y) =p(y|x)pθ(x)
pθ(y)=p(y|x)pθ(x)/integraltext
xp(y|x)pθ(x)dx, (S33)
wherep(y|x)denotes the oracle in probabilistic form, and is not required to be diﬀerentiable. More generally,
the authors use Sto denote some target range of y’s that would be desirable to condition on, for instance if
p(S|x) =/integraltext
yp(y|x)1y∈Sdythen:
pθ(x|S) =p(S|x)pθ(x)
pθ(S)=p(S|x)pθ(x)/integraltext
xp(S|x)pθ(x)dx, (S34)
Due to the intractability of the denominator term, the authors propose the use of variational inference to learn
a sampling network qζ(x)that is as close as possible to pθ(x|S)as measured by the forward KL divergence.
Here, let us use pθ(S|x)in place ofp(S|x), and assume the oracle pθ(S|x)was trained onDtrain:10:
ζ∗= arg min
ζKL/bracketleftBig
pθ(x|S)/bardblqζ(x)/bracketrightBig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
forward KL
= arg min
ζ/summationdisplay
xpθ(x|S) log/parenleftBig
pθ(x|S)−qζ(x)/parenrightBig
= arg min
ζ/summationdisplay
x/bracketleftBig
pθ(x|S) logpθ(x|S)−pθ(x|S) logqζ(x)/bracketrightBig
= arg max
ζH[pθ(x|S)] +/summationdisplay
x/bracketleftBigpθ(S|x)pθ(x)
pθ(S)logqζ(x)/bracketrightBig
= arg max
ζH[pθ(x|S)]/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
const.+1
pθ(S)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
const./summationdisplay
x/bracketleftBig
pθ(S|x)pθ(x) logqζ(x)/bracketrightBig
= arg max
ζEx∼pθ(x)/bracketleftBig
pθ(S|x)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
oraclelogqζ(x)/bracketrightBig
. (S35)
10In their paper the symbol φis used, but here we use ζsince the former is used to denote the validation oracle.
22Published in Transations on Machine Learning Research (05/2024)
The authors mention that in practice importance sampling must be used for Equation S35. This is because
the expectation is over samples in pθ(x), which in turn was trained on only examples with (relatively) small
y. i.e. those inDtrain. Because of this, p(S|x)is likely to be small in magnitude for most samples. For more
details, we defer the reader to the original paper (Brookes et al., 2019).
To relate the training of CbAS to our evaluation framework, we can instead consider Equation S35 as part
of thevalidation part of our evaluation framework. In other words, if we deﬁne S:= [γ,∞]and use the
validation oracle pφin place ofpθ(S|x), then we can optimise for the extrapolated model as the following:
ζ∗= arg min
ζEx∼pθ(x)/bracketleftBig
pφ(S|x)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
oraclelogqζ(x)/bracketrightBig
(S36)
Generally speaking, validation metrics should not be optimised over directly since they are functions of the
validation set, and the purpose of a validation set in turn is to give a less biased measure of generalisation
than the same metric computed on the training set. However, this may not be too big of a deal here since we
are not taking gradients with respect to the oracle.
metric 
extrapolate generation model 
selection train
approximate 
oracle 
Figure S6: The training and evaluation of CbAS Brookes et al. (2019) in the context of our evaluation
framework. The extrapolation equation is described in Equation S35 and involves variational inference to
ﬁne-tunepθ(x)into a search model qζ(x).
A.2.2 Model inversion networks and the reverse KL divergence
It turns out that there is an interesting connection between the agreement and the reverse KL divergence
between a speciﬁc kind of augmented model distribution and the truncated ground truth p0,γ(x,y). To see
this, let us re-consider the generation time optimisation performed in Kumar & Levine (2020) (which we
calledMIN-Opt ), which tries to ﬁnd a good candidate x=Gθ(z,y)via the following optimisation:
y∗,z∗= arg max
y,zy+/epsilon11logpθ(y|Gθ(z,y))/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
agreement+/epsilon12logp(z)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
prior over z(S37)
zandycan be generated by performing gradient ascent with respect to yandz. We can also express
MIN-Opt with respect to a batch of (y,z)’s, and this can be elegantly written if we express it as optimising
over adistribution pζ(z,y). Then we can ﬁnd such a distribution that maximises the expected value of
Equation S37 over samples drawn from pζ(z,y):
pζ(z,y)∗:= arg max
pζ(z,y)Ez,y∼pζ(z,y)/bracketleftBig
y+/epsilon11/parenleftBig
logpθ(y|Gθ(z,y)) +/epsilon12logp(z)/parenrightBig/bracketrightBig
, (S38)
where e.g.ζparameterises the distribution, e.g. a mean and variance if we assume it is Gaussian. Although
MIN-Opt was intended to be used at generation time to optimise for good candidates, we can also treat it as
a validation metric, especially if we replace the training oracle pθ(y|x)with the validation oracle pφ(y|x).
For the sake of convenience, let us also replace /epsilon11and/epsilon12with one hyperparameter η. This hyperparameter
23Published in Transations on Machine Learning Research (05/2024)
can be seen as expressing a trade-oﬀ between selecting for large scores yversus ones with large agreement
and density under the prior distribution. This gives us the following:
pζ(z,y)∗= arg max
pζ(z,y)Ey,z∼pζ(z,y)/bracketleftBig
y+η/parenleftBig
logpφ(y|Gθ(z,y)) + logp(z)/parenrightBig/bracketrightBig
= arg max
pζ(z,y)Ey∼pζ(y)y+ηEy,z∼pζ(z,y)/bracketleftBig
logpφ(y|Gθ(z,y)) + logp(z)/bracketrightBig
(S39)
= arg max
pζ(z,y)Ey∼pζ(y)y+ηEx,y,z∼pθ(x|y,z)pζ(z,y)/bracketleftBig
logpφ(y|x) + logp(z)/bracketrightBig
. (S40)
Note that in the last line we instead use the notation x∼pθ(x|y,z)(a delta distribution) in place of
x=Gθ(z,y), which is a deterministic operation. We can show that Equation S39 has a very close
resemblence to minimising the reverse KL divergence between a speciﬁc kind of augmented model and the
γ-truncated ground truth, with respect to our distribution pζ(z,y). Suppose that instead of the typical
augmented model pθ,γ(x,y) =pθ(x|y)pγ(y)we consider one where zandyare drawn from a learnable
joint distribution pζ(z,y), and we simply denote pθ(x|y,z)to be a delta distribution (since x=Gθ(z,y)is
deterministic). We can write this new augmented model as the following:
pθ,ζ(x,y) =/integraldisplay
zpθ(x|y,z)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
GANpζ(z,y)dz. (S41)
Although this distribution is not tractable, we will only be using it to make the derivations more clear.
Let us work backwards here: if we take Equation S39 but substitute the inner square bracket terms for the
reverse KL divergence between the augmented model of Equation S41 and the ground truth pγ(x,y), we
obtain the following:
pζ(z,y)∗:= arg min
y∼pζ−Epζ(y)y+ηKL/bracketleftBig
pθ,ζ(x,y)/bardblpγ(x,y)/bracketrightBig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
reverse KL
= arg min
pζ−Ey∼pζ(y)y+η/bracketleftBig
Ex,y∼pθ,ζ(x,y)logpθ,ζ(x,y)−Ex,y∼pθ,ζ(x,y)logpγ(x,y)/bracketrightBig
= arg max
pζEy∼pζ(y)y−η/bracketleftBig
Ex,y∼pθ,ζ(x,y)logpθ,ζ(x,y) +Ex,y∼pθ,ζ(x,y)logpγ(x,y)/bracketrightBig
= arg max
pζEy∼pζ(y)y+η/bracketleftBig
H[pθ,ζ] +Ex,y∼pθ,ζ(x,y)logpγ(x,y)/bracketrightBig
= arg max
pζEy∼pζ(y)y+η/bracketleftBig
H[pθ,ζ]/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
entropy+Ex,y,z∼pθ(x|y,z)pζ(z,y)/bracketleftbig
logp(y|x)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
agreement+ logpγ(x)/bracketrightbig/bracketrightBig
≈arg max
pζEy∼pζ(y)y+η/bracketleftBig
H[pθ,ζ]/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
entropy+Ex,y,z∼pθ(x|y,z)pζ(z,y)/bracketleftbig
logpφ(y|x)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
agreement+ logpγ(x)/bracketrightbig/bracketrightBig
.(S42)
The entropy term is not tractable because we cannot evaluate its likelihood. For the remaining two terms
inside the expectation, the agreement can be approximated with the validation oracle pφ(y|x). Howeer, it
would not be practical to estimate logpγ(x)since that would require us to train a separate density pφ(x)to
approximate it.
For clarity, let us repeat Equation S39 here:
pζ(z,y)∗= arg max
pζ(z,y)Ey∼pζ(y)y+ηEx,y,z∼pθ(x|y,z)pζ(z,y)/bracketleftBig
logpφ(y|x) + logp(z)/bracketrightBig
. (S43)
The diﬀerence between the two is that (1) there is no entropy term; and (2) logpγ(x)term is replaced with
logp(z)for MIN-Opt, which is tractable since the know the prior distribution for the GAN. From these
observations, we can conclude that MIN-Opt (S39) comprises an approximation of the reverse KL divergence
where the entropy term is omitted and the log density of the datais replaced with the log density of the prior.
24Published in Transations on Machine Learning Research (05/2024)
A.2.3 Exponentially tilted densities
Once the best model has been found via an appropriate validation metric, one can train the same type of
model on the full dataset Dusing the same hyperparameters as before. Ultimately, we would like to be
able to generate candidates whose y’s exceed that of the entire dataset, and at the same time at plausible
according to our generative model. To control how much we trade-oﬀ high likelihood versus high rewarding
candidates, we can consider the exponentially tilted density (Asmussen & Glynn, 2007; O’Donoghue et al.,
2020; Piche et al., 2022):
pθ,γ(x,y) exp(η−1y−κ(η)), (S44)
whereκ(η)is a normalisation constant, and smaller ηputs larger emphasis on sampling from regions where y
is large. Taking the log of Equation S44, we arrive at:
x∗,y∗= arg max
x,ylogpθ,γ(x,y) +1
ηy, (S45)
In practice, it would not be clear what the best ηshould be, but a reasonable strategy is to consider a range
ofη’s, where larger values encode a higher tolerance for ‘risk’ since these values favour higher rewarding
candidates at the cost of likelihood. Note that for VAEs and diﬀusion models, logpθ(x,y)will need to be
approximated with the ELBO. Interestingly, since diﬀusion models have an extremely close connection to
score-based models, one could ‘convert’ a diﬀusion model to a score-based model (Weng, 2021) and derive
∇x,ylogpθ(x,y), and this would make sampling trivial.
One potential issue however relates to our empirical observation that predicted scores for generated candidates
exhibit very high variance, i.e. the agreement scores are very high (see Figures S7b and S9b). In other words,
when we sample some x,y∼pθ,γ(x,y)(i.e. from the augmented model ) there is signiﬁcant uncertainty as
to whether xreally does have a score of y. One potential remedy is to take inspiration from the MIN-Opt
generation procedure (Section A.2.2) and add the agreement term to Equation S45:
x∗,y∗= arg max
x,ylogpθ,γ(x,y) +1
ηy+αlogpφ(y|x). (S46)
Due to time constraints, we leave additional experimentation here to future work.
A.3 Additional training details
A.3.1 Hyperparameters
The architecture that we use is a convolutional U-Net from HuggingFace’s ‘annotated diﬀusion model’11,
whose convolutional operators have been replaced with fully connected layers (since Ant and Kitty morphology
inputs are ﬂat vectors).
For all experiments we train with the ADAM optimiser (Kingma & Ba, 2014), with a learning rate of 2×10−5,
β= (0.0,0.9), and diﬀusion timesteps T= 200. Experiments are trained for 5000 epochs with single P-100
GPUs. Input data is normalised with the min and max values per feature, with the min and max values
computed over the training set Dtrain. The same is computed for the score variable y, i.e. all examples in the
training set have their scores normalised to be within [0,1].
Here we list hyperparameters that diﬀer between experiments:
•diffusion_kwargs.tau : for classiﬁer-free diﬀusion models, this is the probability of dropping the label
(score)yand replacing it with a null token. For classiﬁer guidance models, this is ﬁxed to τ= 1since this
would correspond to training a completely unconditional model.
•gen_kwargs.dim : channel multiplier for U-Net architecture
•diffusion_kwargs.w_cg : for classiﬁer-based guidance, this is the wthat corresponds to the win Equation
16.
11https://huggingface.co/blog/annotated-diffusion
25Published in Transations on Machine Learning Research (05/2024)
A.3.2 Hyperparameters explored for classiﬁer-free guidance
{
’diffusion_kwargs.tau’: {0.05, 0.1, 0.2, 0.4, 0.5},
’gen_kwargs.dim’: {128, 256}
}
A.3.3 Hyperparameters explored for classiﬁer guidance
{
’diffusion_kwargs.w_cg’: {1.0, 10.0, 100.0},
’epochs’: {5000, 10000},
’gen_kwargs.dim’: {128, 256}
}
A.3.4 Classiﬁer guidance derivation
Let us denote xtas the random variable from the distribution q(xt), denoting noisy input xat timestep t.
Through Bayes’ rule we know that q(xt|y) =q(xt,y)
q(y)=q(y|xt)q(xt)
q(y). Taking the score ∇xtlogq(xt|y)(which
does not depend on q(y)), we get:
∇xtlogq(xt|y) =∇xtlogq(y|xt) +∇xtlogq(xt) (S47)
≈−1√1−¯αt/parenleftBig
/epsilon1θ(xt,t,y)−/epsilon1θ(xt,t)/parenrightBig
, (S48)
where in the last line we make clear the connection between the score function and the noise predictor /epsilon1θ
Weng (2021). Since we would like to derive the conditional score, we can simply re-arrange the equation to
obtain it:
/epsilon1θ(xt,t,y) =/epsilon1θ(xt,t)−√1−¯αt∇xtlogq(y|xt) (S49)
≈/epsilon1θ(xt,t)−√1−¯αt∇xtlogpθ(y|xt), (S50)
where we approximate the classiﬁer q(y|xt)with our (approximate) training oracle pθ(y|xt). In practice, we
can also deﬁne the weighted version as follows, which allows us to balance between conditional sample quality
and sample diversity:
/epsilon1θ(xt,t,y;w) =/epsilon1θ(xt,t)−√1−¯αtw∇xtlogpθ(y|xt), (S51)
Therefore, in order to perform classiﬁer-guided generation, we replace /epsilon1θ(xt,t)in whatever generation
algorithm we use with /epsilon1θ(xt,t,y;w)instead.
A.3.5 Classiﬁer-free guidance
In classiﬁer-free guidance a conditional score estimator /epsilon1θ(xt,y,t)is estimated via the algorithm described in
Ho & Salimans (2022), where the ytoken is dropped during training according to some probability τ. Ifyis
dropped it is replaced with some unconditional token. In other words, the noise predictor (score estimator) is
trained both conditionally and unconditionally, which means we have both /epsilon1θ(xt,t)as well as/epsilon1θ(xt,y,t).
From Bayes’ rule, we know that: p(y|xt) =p(y,xt)
p(xt)=p(xt|y)p(y)
p(xt), and that therefore the score ∇xtlogp(y|xt)
is:
∇xtlogp(y|xt) =∇xtlogp(xt|y)−∇ xtlogp(xt) (S52)
26Published in Transations on Machine Learning Research (05/2024)
We simply plug this into Equation 16 to remove the dependence on pθ(y|xt):
/epsilon1θ(xt,t,y;w) =/epsilon1θ(xt,t)−√1−¯αtw∇xtlogpθ(y|xt) (S53)
=/epsilon1θ(xt,t)−√1−¯αtw/bracketleftBig
∇xtlogpθ(xt|y)−∇ xtlogpθ(xt)/bracketrightBig
(S54)
=/epsilon1θ(xt,t)−√1−¯αtw/bracketleftBig−1√1−¯αt/epsilon1θ(xt,y,t)−−1√1−¯αt/epsilon1θ(xt,t)/bracketrightBig
(S55)
=/epsilon1θ(xt,t) +w/epsilon1θ(xt,y,t)−w/epsilon1θ(xt,t) (S56)
=/epsilon1θ(xt,t) +w/parenleftBig
/epsilon1θ(xt,y,t)−/epsilon1θ(xt,t)/parenrightBig
(S57)
27Published in Transations on Machine Learning Research (05/2024)
A.4 Correlation and agreement plots
Agreement plots We plot the conditioning y∈linspace (ymin,ymax)against the validation/test oracle
predictions for candidates conditionally generated with that y. We call these ‘agreement plots’ since the
sum of squared residuals for each point would constitute the agreement (with a perfect agreement of zero
corresponding to a diagonal dotted line on each graph). Here we demonstrate this amongst the best three
modelswith respect toMAgr, since this is most correlated with the test oracle (see Figure S7a). The shaded
regions denote±1standard deviation from the mean, and the marker symbols denote the max/min score for
eachywith respect to either oracle.
A.4.1 Ant Morphology
0.06 0.07 0.08
CDSM
200300400500testreward
 = -0.1262
1.0 1.5 2.0
FD
200300400500testreward
 = 0.0564
1.5
 1.0
 0.5
DC
200300400500testreward
 = -0.0512
1.4
 1.3
 1.2
reward
200300400500testreward
 = -0.3081
20000 40000 60000
Agr
200300400500testreward
 = -0.2772
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
w
(a) Correlation plot
400
 200
 0 200 400 600
y0200400600oracle scorescore = -1.42
valid f(x)
test f(x)
400
 200
 0 200 400 600
y0200400600oracle scorescore = -1.38
valid f(x)
test f(x)
400
 200
 0 200 400 600
y0200400600oracle scorescore = -1.36
valid f(x)
test f(x)
(b) Agreement plot
Figure S7: Results on Ant Morphology dataset, using the classiﬁer-free guidance variant (Equation 18).
0.053 0.054 0.055
CDSM
0200400testreward
 = -0.0015
1.5 2.0 2.5 3.0
FD
0200400testreward
 = -0.5972
0.40
 0.35
DC
0200400testreward
 = -0.3130
1.10
 1.05
 1.00
 0.95
reward
0200400testreward
 = -0.2952
100000 120000
Agr
0200400testreward
 = 0.4389
200 400 600 800 1000
w
(a) Correlation plot
400
 200
 0 200 400 600
y200
0200400600oracle scoreFD = 1.49
valid f(x)
test f(x)
400
 200
 0 200 400 600
y200
0200400600oracle scoreFD = 1.61
valid f(x)
test f(x)
400
 200
 0 200 400 600
y200
0200400600oracle scoreFD = 1.62
valid f(x)
test f(x)
(b) Agreement plot
Figure S8: Results on Ant Morphology dataset, using the classiﬁer guidance variant (Equation 16).
28Published in Transations on Machine Learning Research (05/2024)
A.4.2 D’Kitty Morphology
See Figures S9 and S10.
0.060 0.065 0.070
CDSM
200250testreward
 = -0.0046
0.3 0.4 0.5
FD
200250testreward
 = -0.2192
3.0
 2.5
 2.0
 1.5
DC
200250testreward
 = 0.1479
1.04
 1.03
 1.02
reward
200250testreward
 = -0.1611
3000 4000 5000
Agr
200250testreward
 = -0.0255
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
w
(a) Correlation plot
0 50 100 150 200 250 300 350
y0100200300oracle scoreFD = 0.26
valid f(x)
test f(x)
0 50 100 150 200 250 300 350
y0100200300oracle scoreFD = 0.26
valid f(x)
test f(x)
0 50 100 150 200 250 300 350
y0100200300oracle scoreFD = 0.27
valid f(x)
test f(x)
(b) Agreement plot
Figure S9: Results on D’Kitty Morphology dataset, using the classiﬁer-free guidance variant (Equation 18).
0.058 0.060 0.062
CDSM
0200testreward
 = -0.0590
0.4 0.6 0.8
FD
0200testreward
 = -0.7758
1.0
 0.8
 0.6
DC
0200testreward
 = -0.6659
1.00
 0.95
reward
0200testreward
 = -0.5141
20000 40000 60000 80000
Agr
0200testreward
 = -0.7998
20 40 60 80 100
w
(a) Correlation plot
800
 600
 400
 200
 0 200 400
y200
0200oracle scoreAgr = 10842.87
valid f(x)
test f(x)
800
 600
 400
 200
 0 200 400
y200
0200oracle scoreAgr = 12533.25
valid f(x)
test f(x)
800
 600
 400
 200
 0 200 400
y200
0200oracle scoreAgr = 12753.73
valid f(x)
test f(x)
(b) Agreement plot
Figure S10: Results on D’Kitty Morphology dataset, using the classiﬁer guidance variant (Equation 16).
29Published in Transations on Machine Learning Research (05/2024)
A.4.3 Superconductor
See Figure S11.
0.004 0.005 0.006
CDSM
050100testreward
 = -0.1529
25 50 75 100
FD
050100testreward
 = 0.1828
1.5
 1.0
DC
050100testreward
 = 0.4337
14
 12
 10
reward
050100testreward
 = -0.1714
500 1000
Agr
050100testreward
 = -0.2003
0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
w
(a) Correlation plot
0 25 50 75 100 125
y050100oracle scoreAgr = 421.91
valid f(x)
test f(x)
0 25 50 75 100 125
y050100oracle scoreAgr = 439.13
valid f(x)
test f(x)
0 25 50 75 100 125
y050100oracle scoreAgr = 643.39
valid f(x)
test f(x)
(b) Agreement plot
Figure S11: Results on Superconductor dataset (Hamidieh, 2018), using the classiﬁer-free guidance variant
(Equation 18).
0.0050 0.0055 0.0060
CDSM
255075testreward
 = -0.0183
40 60 80 100
FD
255075testreward
 = -0.3922
1.5
 1.0
DC
255075testreward
 = 0.1252
10
 8
 6
 4
reward
255075testreward
 = -0.7096
2000 4000
Agr
255075testreward
 = -0.7574
0.2 0.4 0.6 0.8 1.0
w
(a) Correlation plot
0 25 50 75 100 125
y050100oracle scorescore = -10.41
valid f(x)
test f(x)
0 25 50 75 100 125
y050100oracle scorescore = -10.39
valid f(x)
test f(x)
0 25 50 75 100 125
y050100oracle scorescore = -9.68
valid f(x)
test f(x)
(b) Agreement plot
Figure S12: Results on Superconductor dataset (Hamidieh, 2018), using the classiﬁer-free guidance variant
(Equation 18).
30Published in Transations on Machine Learning Research (05/2024)
50th pt. 100th pt.
Dtrain – 0.400
Auto. CbAS 0.131±0.010 0.421±0.045
CbAS 0.111±0.017 0.503±0.069
BO-qEI 0.300±0.015 0.402±0.034
CMA-ES 0.379±0.003 0.465±0.024
Grad. 0.476±0.0220.518±0.024
Grad. Min 0.471±0.016 0.506±0.009
Grad. Mean 0.469±0.022 0.499±0.017
REINFORCE 0.463±0.016 0.481±0.013
MINs 0.336±0.016 0.499±0.017
COMs 0.386±0.018 0.439±0.033
Cond. Diﬀusion (c.f.g.) 0.518±0.0450.636±0.034
Table S4: 100th and 50th percentile test rewards for the Superconductor dataset. Results above our conditional
diﬀusion results (bottom-most row) were extracted from Design Bench Trabucco et al. (2022). For our
experiments, each result is an average computed over three diﬀerent runs (random seeds). test rewards are
min-max normalised with respect to the smallest and largest oracle scores in the fulldataset, i.e. any scores
greater than 1 are greaterthan any score observed in the full dataset. Design Bench results are shown for
illustrative purposes only, and are not directly comparable to our results due to diﬀerences in evaluation
setup.
31Published in Transations on Machine Learning Research (05/2024)
A.4.4 Hopper (50%)
See Figure S13.
0.120 0.125
CDSM
250
0250testreward
 = -0.5357
150 200 250
FD
250
0250testreward
 = -0.0964
0.3
 0.2
DC
250
0250testreward
 = 0.3893
16
 14
 12
 10
reward
250
0250testreward
 = 0.3143
20000 30000
Agr
250
0250testreward
 = 0.5286
20 40 60 80 100
w
(a) Correlation plot
0 250 500 750 1000 1250
y02004006008001000oracle scoreCDSM = 0.12
valid f(x)
test f(x)
0 250 500 750 1000 1250
y02004006008001000oracle scoreCDSM = 0.12
valid f(x)
test f(x)
0 250 500 750 1000 1250
y02004006008001000oracle scoreCDSM = 0.12
valid f(x)
test f(x)
(b) Agreement plot
Figure S13: Results on Hopper 50%, using the classiﬁer-free guidance variant (Equation 18).
32Published in Transations on Machine Learning Research (05/2024)
A.5 Sensitivity to τhyperparameter
In Figure S14 we present the same correlation plots as shown in Section A.4 but colour-coded with respect to
τ, which is the classiﬁer-free guidance hyperparameter that controls the dropout probability for the label y.
For each dataset, for the validation metrics that perform the best (i.e. correlate most negatively with the test
reward), smaller values of τresult in better test rewards. Overall, the results indicate that τis a sensitive
hyperparameter and should be carefully tuned.
0.06 0.07 0.08
CDSM
200300400500testscore
 = -0.1262
1.0 1.5 2.0
FD
200300400500testscore
 = 0.0564
1.5
 1.0
 0.5
DC
200300400500testscore
 = -0.0512
1.4
 1.3
 1.2
score
200300400500testscore
 = -0.3081
20000 40000 60000
Agr
200300400500testscore
 = -0.2772
0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50
(a) Ant Morphology dataset. For each validation metric, we plot each experiment’s smallest-achieved metric versus
the test reward (Equation 24). The colourbar represents values of τ.
0.060 0.065 0.070
CDSM
200250testscore
 = -0.0046
0.3 0.4 0.5
FD
200250testscore
 = -0.2192
3.0
 2.5
 2.0
 1.5
DC
200250testscore
 = 0.1479
1.04
 1.03
 1.02
score
200250testscore
 = -0.1611
3000 4000 5000
Agr
200250testscore
 = -0.0255
0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50
(b) Kitty dataset. For each validation metric, we plot each experiment’s smallest-achieved metric versus the test
reward (Equation 24). The colourbar represents values of τ.
0.004 0.005 0.006
CDSM
050100testscore
 = -0.1421
25 50 75 100
FD
050100testscore
 = 0.1828
1.5
 1.0
DC
050100testscore
 = 0.4705
14
 12
 10
score
050100testscore
 = -0.1714
500 1000
Agr
050100testscore
 = -0.2003
0.10 0.12 0.14 0.16 0.18 0.20
(c) Superconductor dataset. For each validation metric, we plot each experiment’s smallest-achieved metric versus the
test reward (Equation 24). The colourbar represents values of τ.
Figure S14: Results on each dataset, using the classiﬁer-free guidance variant (Equation 18). Coloured points
represent the hyperparameter τ, which represents the dropout probability for classiﬁer-free guidance (cfg).
Across all datasets, smaller values of τcorrespond to better scores. This suggests that experiments are quite
sensitive to the value of this hyperparameter.
33Published in Transations on Machine Learning Research (05/2024)
A.6 Additional results
Ant Morphology D’Kitty Morphology Superconductor
Auto. CbAS 0.364±0.014 0.736±0.025 0.131±0.010
CbAS 0.384±0.016 0.753±0.008 0.017±0.503
BO-qEI 0.567±0.000 0.883±0.000 0.300±0.015
CMA-ES -0.045 ±0.004 0.684±0.016 0.379±0.003
Gradient Ascent 0.134±0.018 0.509±0.2000.476±0.022
Grad. Min 0.185±0.008 0.746±0.034 0.471±0.016
Grad. Mean 0.187±0.009 0.748±0.024 0.469±0.022
MINs 0.618±0.040 0.887±0.004 0.336±0.016
REINFORCE 0.138±0.032 0.356±0.131 0.463±0.016
COMs 0.519±0.026 0.885±0.003 0.386±0.018
Cond. Diﬀusion (c.f.g.) 0.831±0.052 0.930±0.004 0.492±0.112
Cond. Diﬀusion (c.g.) 0.880±0.012 0.935±0.006 –
Table S5: 50th percentile test rewards for methods from Design Bench (Trabucco et al., 2022) as well as our
diﬀusion results shown in the last two rows, with c.f.g standing for classiﬁer-free guidance (Equation 18) and
c.g. standing for classiﬁer-guidance (Equation 16). Each result is an average computed over six diﬀerent runs
(seeds). test rewards are min-max normalised with respect to the smallest and largest oracle scores in the
fulldataset, i.e. any scores greater than 1 are greaterthan any score observed in the full dataset. Design
Bench results are shown for illustrative purposes only, and are not directly comparable to our results due to
diﬀerences in evaluation setup.
34