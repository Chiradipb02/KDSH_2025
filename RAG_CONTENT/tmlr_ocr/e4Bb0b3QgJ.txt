Published in Transactions on Machine Learning Research (11/2022)
Fail-Safe Adversarial Generative Imitation Learning
Philipp Geiger Philipp.W.Geiger@de.bosch.com
Bosch Center for Artiﬁcial Intelligence
Renningen, Germany
Christoph-Nikolas Straehle Christoph-Nikolas.Straehle@de.bosch.com
Bosch Center for Artiﬁcial Intelligence
Renningen, Germany
Reviewed on OpenReview: https: // openreview. net/ forum? id= e4Bb0b3QgJ
Abstract
For ﬂexible yet safe imitation learning (IL), we propose theory and a modular method,
with a safety layer that enables a closed-form probability density/gradient of the safe gen-
erative continuous policy, end-to-end generative adversarial training, and worst-case safety
guarantees. The safety layer maps all actions into a set of safe actions, and uses the change-
of-variables formula plus additivity of measures for the density. The set of safe actions is
inferred by ﬁrst checking safety of a ﬁnite sample of actions via adversarial reachability
analysis of fallback maneuvers, and then concluding on the safety of these actions’ neigh-
borhoods using, e.g., Lipschitz continuity. We provide theoretical analysis showing the
robustness advantage of using the safety layer already during training (imitation error lin-
ear in the horizon) compared to only using it at test time (up to quadratic error). In an
experiment on real-world driver interaction data, we empirically demonstrate tractability,
safety and imitation performance of our approach.
1 Introduction and Related Work
Forseveralproblemsatthecurrentforefrontofagentlearningalgorithms, suchasdecisionmakingofrobotsor
automated vehicles, or modeling/simulation of realistic agents, imitation learning (IL) is gaining momentum
as a method (Suo et al., 2021; Igl et al., 2022; Bhattacharyya et al., 2019; Bansal et al., 2018; Xu et al.,
2020; Zeng et al., 2019; Cao et al., 2020). But safetyandrobustness of IL for such tasks remain a challenge.
(Generative)IL Thebasicideaofimitationlearningisasfollows: wearegivenrecordingsofthesequential
behavior of some demonstrator agent, and then we train the imitator agent algorithm on this data to make
it behave similarly to the demonstrator (Osa et al., 2018). One powerful recent IL method is generative
adversarial imitation learning (GAIL) (Ho and Ermon, 2016; Song et al., 2018). GAIL’s idea is, on the one
hand, to use policy gradient methods borrowed from reinforcement learning (RL; including, implicitly, a form
ofplanning ) to generate an imitator policy that matches the demonstrator’s behavior over whole trajectories.
And on the other hand, to measure the “matching”, GAIL uses a discriminator as ingenerative adversarial
networks (GANs) that seeks to distinguish between demonstrator and imitator distributions. While the
neural net and GAN aspects help the ﬂexibility of the method, the RL aspects stir against compounding
of errors over rollout horizons, which other IL methods like behavior cloning (BC) , which do not have such
planning aspects, suﬀer from. One line of research generalizes classic Gaussian policies to more ﬂexible (incl.
multi-modal) conditional normalizing ﬂows as imitator policies (Ward et al., 2019; Ma et al., 2020) which
nonetheless give exact densities/gradients for training via the change-of-variables formula .
IL’s safety/robustness challenges While these are substantial advances in terms of learning ﬂexibil-
ity/capacity , it remains a challenge to make the IL methods (1) guaranteeably safe, especially for multi-agent
1Published in Transactions on Machine Learning Research (11/2022)
state
stsafe action
set˜Ast
t
safe
action ¯atpre-safe
action ˆatclosed-form
density
πI,θ(¯at|st)
& gradient
(c) safety layer
(constraining
to˜Ast
t)(a) pre-safe gen-
erative policy
(with param.θ)(b) safe set
inference
(planning)
transition
st+1=
f(st,¯at,ot)fail-safe generative imitator πI,θ(¯at|st)rollout, using current θ imitator trajectories
(s1, a1, s2, a2, . . .), . . .
adversarial training step:
updatediscriminator & θ
demonstrator trajectories
(s1, a1, s2, a2, . . .), . . .
Figure 1: Outline of our theoretically grounded method FAGIL: It consists of the fail-safe generative imitator
policy(inner box on l.h.s.), that takes current state stas input, and then it (a) lets a standard pre-safe
generative policy (e.g., Gauss or normalizing ﬂow) generate a pre-safe action ˆat, (b)infers a provably safe
set˜Ast
tof actions, and then (c) uses our safety layer to constrain the pre-safe action ˆatinto ˜Ast
t, yielding safe
action ¯at. It hasend-to-end closed-form density/gradient , so we train it end-to-end via, e.g., GAIL (r.h.s.).
While our approach is for general safe IL tasks, we take driver imitation as running example/experiments.
interactions with humans, and (2) robust, in the sense of certain generalizations “outside the training distri-
bution”, e.g., longer simulations than training trajectories. Both is non-trivial, since safety and long-term
behavior can heavily be aﬀected already by small learning errors (similar as the compounding error of BC).
Safe control/RL ideas we build on To address this, we build on several ideas from safe control/RL:
(1) In reachability analysis/games (Fridovich-Keil and Tomlin, 2020; Bansal et al., 2017), the safety prob-
lem is formulated as ﬁnding controllers that provably keep a dynamical system within a pre-deﬁned set of
safe/collision-free states over some horizon, possibly under adversarial perturbations/other agents. (2) Any
learning-based policy can be made safe by, at each stage, if it failsto produce a safe action, enforcing a safe
action given by some sub-optimal but safe fallback controller (given there is such a controller) (Wabersich
and Zeilinger, 2018). (3) Often it is enough to search over a reasonably small set of candidates of fallback
controllers, such as emergency brakes and simple evasive maneuvers in autonomous driving (Pek and Althoﬀ,
2020). And (4) a simple way to enforce a policy’s output to be safe is by composing it with a safety layer
(as ﬁnal layer) that maps into the safe set, i.e., constrains the action to the safe set, and diﬀerentiable such
safety layers can even be used during training (Donti et al., 2021a; Dalal et al., 2018).
Safe IL, theoretical guarantees, and their limitations While incorporating safety into RLhas re-
ceived signiﬁcant attention in recent years (Wabersich and Zeilinger, 2018; Berkenkamp et al., 2017; Alshiekh
et al., 2018; Chow et al., 2019; Thananjeyan et al., 2021), safety (in the above sense) in IL, and its theoret-
ical analysis , have received comparably little attention. The existing work on such safe IL can roughly be
classiﬁed based on whether safety is incorporated via reward augmentation or viaconstraints/safety layers
encoded into the policy (there also exists a slightly diﬀerent sense of safe IL, with methods that control the
probabilistic risk of high imitation costs (Javed et al., 2021; Brown et al., 2020; Lacotte et al., 2019), but
they do not consider safety constraints separate from the imitation costs, as we do; for additional, broader
related work, see Appendix D.4). In reward augmentation , loss terms are added to the imitation loss that
penalizes undesired state-action pairs (e.g., collisions) (Bhattacharyya et al., 2019; 2020; Bansal et al., 2018;
Suo et al., 2021; Zeng et al., 2020; Cheng et al., 2022). Regarding hard constrains/safety layers (Tu et al.,
2022; Havens and Hu, 2021), one line of research (Chen et al., 2019) is deep IL with safety layers, but only
during test time . There is little work that uses safety layers during training and test time , with the exception
of (Yin et al., 2021) which gives guarantees but is not generative.
Main contributions and paper structure In this sense we are not aware of general approaches for
the problem of safe generative IL that are (1) safe/robust with theoretical guarantees and(2) end-to-end
2Published in Transactions on Machine Learning Research (11/2022)
trainable. In this paper, we contribute theory (Sec. 3), method (Fig. 1, Sec. 4) and experiments (Sec. 5) as
one step towards understanding and addressing these crucial gaps:12
•We propose a simple yet ﬂexible type of diﬀerentiable safety layer that maps a given “pre-safe” action into
a given safe action set (i.e., constrains the action). It allows to have a closed-form probability density/gradient
(Prop. 3; by a non-injective “piecewise” change of variables) of the overall policy, in contrast to common
existing diﬀerentiable safety layers (Donti et al., 2021a; Dalal et al., 2018) without such analytic density.
•We contribute two sample-based safe set inference approaches , which, for a given state, output a provably
safe set of actions – in spite of just checking safety of a ﬁnitesample of actions, using Lipschitz continu-
ity/convexity (Prop. 1 and 2). This advances sample-based approaches (Gillula et al., 2014) to overcome
limitations of exact but restrictive, e.g., linear dynamics, approaches (Rungger and Tabuada, 2017).
•A general question is to what extent it helps to use the safety layer already during training (as we do) ,
compared to just concatenating it, at testtime, to an unsafely trained policy, given the latter may com-
putationally be much easier. We theoretically quantify the imitation performance advantage of the former
over the latter: essentially, the former imitation error scales linearlyin the rollout horizon, the latter up to
quadratically (Sec. 3.3, rem. 1, and thm. 1) – reminiscent of BC. The intuition: only the former method
learns how to properly deal (plan) with the safety layer, while in the latter case, the safety layer may lead to
unvisited states at test time from which we did not learn to recover . (Generally, proofs are in Appendix A.)
•Combining these results, we propose the method fail-safe generative adversarial imitation learner (FAGIL;
Sec. 4). – It is sketched in Fig. 1.
•In experiments on real-world highway data with multiple interacting driver agents (which also serves as
running example throughout the paper), we empirically show tractability and safety of our method, and
show that its imitation/prediction performance comes close to unconstrained GAIL baselines (Sec. 5).
2 Setting and Problem Formulation
Setting and deﬁnitions
•We consider a dynamical system consisting of: statesst∈S, attime stages t∈1:T:={1,...,T};
•anego agent that takes actionat∈A⊂Rn,n∈N, according to its ego policyπt∈Πt(for convenience
we may drop subscript t); we allow πtto be either a conditional (Lebesgue) density, writing at∼πt(a|st),
or deterministic, writing at=πt(st);
•the ego agent can be either the demonstrator , denoted by πD(a priori unknown); or the imitator, denoted
byπI,θ, withθits parameters;
•other agents , also interpretable as noise/perturbations (a priori unknown), with their (joint) action ot
according to others’ policy σt∈Φt(density/deterministic, analogous to ego);
•the system’s transition function (environment) f, s.t.,
st+1=f(st,at,ot). (1)
(fis formally assumed to be known, but uncertainty can simply be encoded into ot.)
•As training data , we are given a set of demonstrator trajectories of the form (s1,a1),(s2,a2),..., (sT,aT)
ofπD’s episodic rollouts in an instance of this dynamical system including other agents/noise.
•We consider a (momentary) imitation cost function c(s,a)which formalizes the similarity between imitator
and demonstrator trajectories (details follow in Sec. 4.2 on the GAIL-based c), and(momentary) safety cost
functiond(s); statessfor whichd(s)≤0are referred to as momentarily safe orcollision-free .
•Notation:P(·)denotes probability; and px(·)the probability’s density of a variable x, if it exists (we may
drop subscript x).
1Note that our approach works with both, demonstrators that are already fully safe, and those that may be unsafe.
2Regarding the task of realistic modeling/simulation: Adding safety constraints can, in principle, lead to unrealistic biases
in case of actually unsafe demonstrations. But the other extreme is pure IL where, e.g., collisions rates can be unrealistically
high (Bansal et al., 2018). So this is always a trade-oﬀ.
3Published in Transactions on Machine Learning Research (11/2022)
Goal formulation The goal is for the imitator to generate trajectories that minimize the expected imita-
tion cost, while satisfying a safety cost constraint, formally:
πI= arg min
πEπ/parenleftBiggT/summationdisplay
t=1c(st,at)/parenrightBigg
−ψ(π)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:vπ,c,ψ, d (st)≤0,for allt, (2)
wherevπ,c,ψis called total imitation cost function (we drop superscript ψif it is zero), ψ(π)is apolicy
regularizer , andEπ(·)denotes the expectation over roll-outs of π.3We purportedly stay fairly abstract here.
We will later instantiate c,ψ(Sec. 4.2) and d(Sec. 5), and specify our safety-relevant modeling/reasoning
about the other agents/perturbations (Sec. 3.1).
3 General Theoretical Tools for Policy and Training of Safe Generative Imitation
Learning
We start by providing, in this section, several general-purpose theoretical foundations for safe generative
IL, as tools to help the design of the safe imitator policy(Sec. 3.1 and 3.2), as well as to understand the
advantages of end-to-end training (Sec. 3.3). We will later, in Sec. 4, build our method on these foundations.
3.1 Sample-based Inference of Safe Action Sets
For the design of safe imitator policies, it is helpful to know, at each stage, a set of safe actions . Roughly
speaking, we consider an individual action as safe, if, after executing this action, at least one safe “best-
case” future ego policy πexists, i.e., keeping safety cost d(st)below 0for allt, underworst-case other
agents/perturbations σ. So, formally, we deﬁne the safe (action) set in statesat timetas
¯As
t:={a∈A:it existsπt+1:T, s.t. for all σt:Tand for allt<t/prime≤T,d(st/prime)≤0holds, given st=s,at=a}(3)
(based on our general setting of Sec. 2). We refer to actions in ¯As
tassafe actions . Such an adversarial/worst-
case uncertainty reasoning resembles common formulations from reachability game analysis (Fridovich-Keil
and Tomlin, 2020; Bansal et al., 2017). Furthermore, deﬁne the total safety cost (to go) as4
wt(s,a) := min
πt+1:Tmax
σt:Tmax
t/prime∈t+1:Td(st/prime),for allt. (4)
Observe the following:
•The total safety cost wtallows us to quantitatively characterize the safe action set ¯As
tassub-zero set5:
¯As
t={a:wt(s,a)≤0}. (5)
•Under the worst-case assumption, for each t,s: an ego policy πtcan be part of a candidate solution of our
basic problem, i.e., satisfy safety constraint in Eq. (2), only ifπt’s outputted actions are all in ¯As
t(because
otherwise there exists no feasible continuation πt+1:T, by deﬁnition of ¯As
t). This fact will be crucial to deﬁne
our safety layer (Sec. 3.2).
Now, regarding inference of the safe action set ¯As
t: while in certain limited scenarios, such as linear dynamics,
it may be possible to eﬃciently calculate it exactly, this is not the case in general (Rungger and Tabuada,
2017). We take an approach to circumvent this by checking wt(s,a)for just a ﬁnite sample ofa’s. And
3As usual, the arg minis well deﬁned under appropriate compactness and continuity assumptions. The “=” may strictly be
a “∈”. We assume an initial (or intermediate) state as given.
4Note: In the scope of safety deﬁnitions, we generally let ego/other policies π,σrange over compact sets Π,Φof deterministic
policies. Then the minima/maxima are well-deﬁned, once we make appropriate continuity assumptions. The ﬁnite horizon can
be extended to the inﬁnite case using terminal safety terms Pek and Althoﬀ (2020).
5Note that requiring momentary safety cost d(st/prime)≤0for each futuret/primein Eq. (3) translates to requiring that d(st/prime)≤0
for themaximum over future t/primein Eq. (4) (and not the sum over t/primeas done in other formulations).
4Published in Transactions on Machine Learning Research (11/2022)
then concluding on the value of wt(s,·)on thesea’s “neighborhoods”. This at least gives a so-called inner
approximation ˜As
tof the safe set ¯As
t, meaning that, while ˜As
tand ¯As
tmay not coincide, we know for sure
that ˜As
t⊂¯As
t, i.e, ˜As
tis safe. The following result gives us Lipschitz continuity and constant to draw
such conclusions.6It builds on the fact that maximization/minimization preserves Lipschitz continuity
(Appendix A.1.1). We assume all spaces are implicitly equipped with norms.
Proposition 1 (Lipschitz constants for Lipschitz-based safe set) .Assume the momentary safety cost dis
α-Lipschitz continuous. Assume that for all (deterministic) ego/other policies πt∈Πt,σt∈Φt,t∈1:T, the
dynamicss/mapsto→f(s,πt(s),σt(s))as well asa/mapsto→f(s,a,σt(s))for ﬁxedsareβ-Lipschitz. Then a/mapsto→wt(s,a)
isαmax{1,βT}-Lipschitz.
Let us additionally give a second approach for sample-based inner approximation of the safe set.7The idea
is that sometimes, safety of a ﬁnite set of corners/extremalities that span a set already implies safety of the
fullset:
Proposition 2 (Extremality-based safe set) .Assume the dynamics fis linear, and the ego/other policy
classes Π1:T,Φ1:Tconsist of open-loop policies, i.e., action trajectories, and that safety cost dis convex.
Thena/mapsto→wt(st,a)is convex. In particular, for any convex polytope B⊂A,wt(s,·)takes its maximum at
one of the (ﬁnitely many) corners of B.
Later we will give one Lipschitz-based method version (FAGIL-L) building on Prop. 1 (not using Prop. 2),
andoneconvexity-basedversion(FAGIL-E)buildingonProp.2. Forseveralbroaderremarksontheelements
of this section, which are not necessary to understand this main part though, see also Appendix D.1.
3.2 Piecewise Diﬀeomorphisms for Flexible Safety Layers with Diﬀerentiable Closed-Form Density
An important tool to enforce safety of a policy’s actions are safety layers. Assume we are given ˜A⊆¯Aas an
(innerapproximationofthe)safeactionset(e.g., fromSec.3.1). A safety layer isaneuralnetlayerthatmaps
A→˜A, i.e., it constrains a “pre-safe” , i.e., potentially unsafe, action ˆa∈Ainto ˜A. In contrast to previous
safety layers (Donti et al., 2021a; Dalal et al., 2018) for deterministic policies, for our generative/probabilistic
approach, we want a safety layer that gives us an overall policy’s closed-form diﬀerentiable density (when
plugging it on top of a closed-form-density policy like a Gaussian or normalizing ﬂow), for end-to-end training
etc. In this section we introduce a new function class and density formula that helps to construct such safety
layers.
Remember the classic change-of-variables formula (Rudin et al., 1964): If y=e(x)for some diﬀeomorphism8
e, andxhas density px(x), then the implied density of yis|det(Je−1(y))|px(e−1(y)).9The construction of
safety layers with exact density/gradient onlybased on change of variables can be diﬃcult or even impossible.
One intuitive reason for this is that diﬀeomorphisms require injectivity , and therefore one cannot simply map
unsafe actions to safe ones and simultaneously leave safe actions where they are (as done, e.g., by certain
projection-base safety layers (Donti et al., 2021a)).
These limitations motivate our simple yet ﬂexible approach of using the following type of function as safety
layers, which relaxes the rigid injectivity requirement that pure diﬀeomorphisms suﬀer from:
Deﬁnition 1 (Piecewise diﬀeomorphism safety layers) .We call a function g:A→˜Aapiecewise
diﬀeomorphism if there exists a countable partition (Ak)kofA, and diﬀeomorphisms (on the interiors)
gk:Ak→˜Ak⊂˜A, such that g|Ak=gk.
Now we can combine the diﬀeomorphisms’ change-of-variables formulas with additivity of measures:
6Once we know that wt(s,·)isγ-Lipschitz, and that wt(s,a)<0, then we also know that wt(s,·)is negative on a ball of
radius|wt(s,a)|
γarounda.
7Note that Gillula et al. (2014) also inner-approximate safe sets from ﬁnite samples of the current action space, by using
theconvex hull argument. But, among other diﬀerences, they do not allow for other agents, as we do in Prop. 2.
8Adiﬀeomorphism eis a continuously diﬀerentiable bijection whose inverse is also cont. diﬀerentiable.
9Jedenotes the Jacobian matrix of e.
5Published in Transactions on Machine Learning Research (11/2022)
Proposition 3 (Closed-form density for piecewise diﬀeomorphism) .Ifgis such a piecewise diﬀeomorphism,
¯a=g(ˆa)andˆa’s density is pˆa(ˆa), then ¯a’s density is
p¯a(¯a) =/summationdisplay
k:¯a∈gk(Ak)|det(Jg−1
k(¯a))|pˆa(g−1
k(¯a)). (6)
Speciﬁc instances of such piecewise diﬀeomorphism safety layers will be given in Sec. 4. Note that one
limitationofpiecewisediﬀeomorphismlayersliesinthembeingdiscontinuous,whichmaycomplicatetraining.
Further details related to diﬀeomorphic layers, which are not necessary to understand this main part though,
are in Appendix D.2.
3.3 Analysis of End-to-End Training with Safety Layers vs. Test-Time-Only Safety Layers
After providing tools for the design of safe policies, now we focus on the following general question regarding
training of such policies: What is the diﬀerence, in terms of imitation performance, between using a safety
layer only during test time, compared to using it during training and test time? (Note: this question/section
is ajustiﬁcation , but is not strictly necessary for understanding the deﬁnition of our method in Sec. 4 below.
So this section may be skipped if mainly interested in the speciﬁc method and experiments.)
The question is important for the following reason: While we in this paper advocate end-to-end training
including safety layer, in principle one could also use the safety layer onlyduring test/production time, by
composing it with an “unsafely trained” policy. This is nonetheless safe, but much easier computationally
in training. The latter is an approach that other safe IL work takes (Chen et al., 2019), but it means that
thetest-time policy diﬀers from the trained one .
While this question is, of course, diﬃcult to answer in its full generality, let us here provide a theoretical
analysis that improves the understanding at least under certain fairly general conditions. Our analysis is
inspired by the study of the compounding errors incurred by behavior cloning (BC) (Ross and Bagnell,
2010; Syed and Schapire, 2010; Xu et al., 2020) For this section, let us make the following assumptions and
deﬁnitions (some familiar from the mentioned BC/IL work):10
•State setSand action set Aare ﬁnite. As usual (Ho and Ermon, 2016), the time-averaged state-action
distribution ofπis deﬁned as ρ(s,a) :=/parenleftBig
1
T/summationtextT
t=1pst(s)/parenrightBig
π(a|s), for alls,a; andρ(s)denotes the marginal.
•We assume some safety layer (i.e., mapping from action set into safe action set) to be given and ﬁxed. As
before,πDis the demonstrator, πItheimitator trained with train-and-test-time safety layer . Additionally
deﬁneπUas a classic, unsafe/unconstrained trained imitator policy11, andπOas thetest-time-only-safety
policyobtained by concatenating πUwith the safety layer at test time. Let ρD,ρI,ρU,ρOdenote the
corresponding time-averaged state-action distributions.
•Asis common(Ross andBagnell, 2010;Xu etal., 2020), we measureperformancedeviations interms ofthe
(unknown) demonstrator cost function, which we denote by c∗(i.e., in the fashion of inverse reinforcement
learning (IRL), where the demonstrator is assumed to be an optimizer of Eq. (2) with c∗asc); and assume
c∗only depends on s, and/bardblc∗/bardbl∞is its maximum value. Let vD:=vπD,c∗,vI:=vπI,c∗,vO:=vπO,c∗be the
corresponding total imitation cost functions (Sec. 2).
•We assume that (1) the demonstrator always acts safely, and (2) the safety layer is the non-identity only
on actions that are never taken by the demonstrator (mapping them to safe ones). Importantly, note that
if we relax these assumptions, in particular, allow the demonstrator to be unsafe, the results in this section
would get even stronger (the diﬀerence of adding a safety layer would be even bigger).
Keep in mind that what we can expect to achieve by GAIL training is a population-level closeness of imitator
to demonstrator state-action distribution of the form D(ρI,ρD)≤ε, for some εdecreasing in the sample
size (due to the usual bias/generalization error; (Xu et al., 2020)), and D, e.g., the Wasserstein distance
(Sec. 4.2). So we make this an assumption in the following results. Here we use D(ρI,ρD) =DTV(ρI,ρD) =
10We believe that most of the simplifying assumptions we make can be relaxed, but the analysis will be much more involved.
11In our case this would essentially mean to take the pre-safe policy, e.g., Gaussian, with pre-safe ˆat, and train it, as πU.
6Published in Transactions on Machine Learning Research (11/2022)
unsafe states/distance , occasionally
visitied by unsafe trained imitator
safe side strip , never visitied by unsafe
imitator, hence not learned to recover
front
vehicle1 2 3
4pU=δ pU=1
pO
=1pU=1
pD=1main road
side strip
s
Figure 2: Staying with our running example of driver imitation, this ﬁgure illustrates the following: if we
use safety layers only at test time and not already during training, the test-time imitator has not learned
to deal with the states that the safety layer may lead to (i.e. to either plan to avoid, or recover from them),
yieldingpoor imitation performance . In a nutshell: The demonstrator is always ( pD= 1) in state 1, meaning
a constant velocity and distance to the front vehicle. At test/deployment time, the unsafely trained imitator
πU, due to learning error, with low probability pU=δ, deviates from state 1 and reaches ﬁrst state 2 and
then (due to, say, inertia) unsafe state 3, from which it has learned to always recover though. The test-time-
only-safe imitator πO, like the unsafely trained imitator it builds upon, with low probability pO=δreaches
state 2. But from there, the safety constraints (red scissors symbol) kick in and the only remaining action is
to go to the safe side strip 4withpO= 1, where there was no data on how to recover (never visited/learned
by unsafeπU), getting trapped there forever. Details are in Appendix A.3.
1
2/summationtext
s∈S,a∈A|ρI(s,a)−ρD(s,a)|, thetotal variation distance (Klenke, 2013), which simpliﬁes parts of the
derivation, though we believe that versions for other Dsuch as Jensen-Shannon are possible.
Now, ﬁrst, observe that for our “train andtest safety layer” approach, we can have a linear, in the horizon
T, imitation performance guarantee. This is because the imitatorπIis explicitly trained to deal with the
safety layer (i.e., to plan to avoid safe but poor states, or to at least recover from them; see Appendix A.3.1
for the derivation):
Remark 1 (Linear error in Tof imitator with train-and-test-time safety layer) .AssumeDTV(ρI,ρD)≤ε.
Then we get
|vI−vD|≤2εT/bardblc∗/bardbl∞.
In contrast, using test-time-only safety, the test-time imitator πOhas not learned to plan with, and recover
from, the states that the safety layer may lead to . This leads, essentially, to a tight quadratic error bound
(when accounting for arbitrary, including “worst”, environments; proof in Appendix A.3):
Theorem 1 (Quadratic error in Tof imitator with test-time-only safety layer) .Lower bound (an “exis-
tence” statement): We can construct an environment12with variable horizon Tand with a demonstrator,
sketched in Fig. 2 and additional details in Appendix A.3.2, a universal constant ι, and, for every ε >0,
an unconstrainedly trained imitator πUwithDTV(ρD,ρU)≤ε, such that for the induced test-time-only-safe
imitatorπOwe have, for all T≥213,
|vO−vD|≥ιmin{εT2,T}/bardblc∗/bardbl∞. (7)
Upper bound (a “for all” statement): Assume DTV(ρD,ρU)≤εand assume ρU(s)has support wherever
ρD(s)has. Then
|vO−vD|≤4ε
νT2/bardblc∗/bardbl∞, (8)
whereνis the minimum mass of ρD(s)within the support of ρD(s).
12In this discrete settings, the environment amounts to a Markov decision process (MDP) (Sutton and Barto, 2018).
13The quadratic bound in Eq. (7) is substantial once we look at small ε, because then the quadratic bound holds even for
largeT, while the train-and-test-time safety layer in Rem. 1 keeps scaling linearly in T. See also Rem. 4 in Appendix A.3.2.
7Published in Transactions on Machine Learning Research (11/2022)
4 Method Fail-Safe Adversarial Generative Imitation Learner
Building on the above theoretical tools and arguments for end-to-end training, we now describe our modular
methodology FAGIL for safe generative IL. Before going into detail, note that, for our general continuous
setting, tractably inferring safe sets and tractably enforcing safety constraints are both commonly known to
be challenging problems, and usually require non-trivial modeling trade-oﬀs (Rungger and Tabuada, 2017;
Gillula et al., 2014; Achiam et al., 2017; Donti et al., 2021b). Therefore, here we choose to provide (1)
one tractable speciﬁc method instance for thelow-dimensional case (where we can eﬃciently partition the
action space via a grid), which we also use in the experiments, alongside (2) an abstract template of ageneral
approach . We discuss policy, safety guarantees (Sec. 4.1), imitation loss and training (Sec. 4.2).
4.1 Fail-Safe Imitator Policy with Invariant Safety Guarantee
First, our method consists of the fail-safe (generative) imitator πI,θ(a|s). Its structure is depicted on the
l.h.s. of Fig. 1, and contains the following modules: at each time tof a rollout, with state stas input,
(a)thepre-safe generative policy module outputs a pre-safe (i.e., potentially unsafe) sample action
ˆat∈Aas well as explicit probability density pˆa|s(ˆa|st)with parameter θand its gradient. Speciﬁc instan-
tiations: This can be, e.g., a usual Gauss policy or conditional normalizing ﬂow.14
(b)Thesafe set inference module outputs an inner-approximation ˜Ast
tof the safe action set, building
on our results form Sec. 3.1. The rough idea is as follows (recall that an action ais safe if its total safety
costwt(st,a)≤0): We check15wt(st,a)for aﬁnite sample ofa’s, and then conclude on the value of wt(st,·)
on thesea’s“neighborhoods” , via Prop. 1 or Prop. 2.16
Speciﬁc instantiations: We partition the action set Ainto boxes (hyper-rectangles) based on a regular grid
(assumingAis a box). Then we go over each box Akand determine its safety by evaluating wt(st,a), with
eitherathe center of Ak, and then using the Lipschitz continuity argument (Prop. 1) to check if wt(st,·)≤0
on the fullAk(FAGIL-L ), oraranging over all corners of box Ak, yielding safety of this box iﬀ wt(st,·)≤0
on all of them ( FAGIL-E ; Prop. 2). Then, as inner-approximated safe set ˜Ast
t, return the union of all safe
boxesAk.
(c)Then, pre-safeaction ˆatplusthesafesetinnerapproximation ˜Ast
tarefedintothe safetylayer . Theidea
is to use a piecewise diﬀeomorphism gfrom Sec. 3.2 for this layer, because the “countable non-injectivity”
gives us quite some ﬂexibility for designing this layer, while nonetheless we get a closed-form density by
summing over the change-of-variable formulas for all diﬀeomorphisms that map to the respective action
(Prop. 3). As output, we get a sample of the safe action ¯at, as well as density πI,θ(¯at|st)(by plugging the
pre-safe policy’s density pˆa|s(ˆa|st)into Prop. 3) and gradient w.r.t. θ.
Speciﬁc instantiations: We use the partition of Ainto boxes (Ak)kfrom above again. The piecewise
diﬀemorphism gis deﬁned as follows, which we believe introduces a helpful inductive bias: (a) On all safe
boxesk, i.e.,Ak⊂¯A,gkis the identity, i.e., we just keep the pre-safe proposal ˆa. (b) On all unsafe boxes
k,gkis the translation and scaling to the most nearby safe box in Euclidean distance. Further safety layer
instances and illustrations are in Appendix D.3.
Adding a safe fallback memory To cope with the fact that it may happen that at some stage an
individual safe action exists, but no fullysafe partAkexists (since the Akare usually sets of more than one
action and not all have to be safe even though some are safe), we propose the following, inspired by (Pek and
Althoﬀ, 2020): the fail-safe imitator has a memory which, at each stage t, contains one purportedly fail-safe
fallback future ego policy πf
t+1:T. Then, if no fully safe part Ak(including a new fail-safe fallback) is found
anymore at t+ 1, execute the fallback πf
t+1, and keepπf
t+2:Tas new fail-safe fallback (see also Appendices B
and D.3.5 on potential issues with this).
14IfAis box-shaped, then we add a squashing layer to get actions into Ausing component-wise sigmoids.
15Clearly, already the “checker” task of evaluating a single action’s safety, Eq. (4), can be non-trivial. But often, roll-outs,
analytic steps, convex optimizations or further inner approximations can be used, or combinations of them, see also Sec. 5.
16Due to modularity, other safety frameworks like Responsibility-Sensitive Safety (RSS) Shalev-Shwartz et al. (2017) can be
plugged into our approach as well. The fundamental safety trade-oﬀ is between being safe (conservativity) versus allowing for
enough freedom to be able to “move at all” (actionability).
8Published in Transactions on Machine Learning Research (11/2022)
Remark 2 (Invariant safety guarantee) .Based on the above, if we know at least one safe action at stage
1, then we invariably over time have at least one safe action plus subsequent fail-safe fallback policy. So we
areguaranteed that the ego will be safe throughout the horizon 1:T.
4.2 Imitation Cost and Training Based on Generative Adversarial Imitation Learning
In principle, various frameworks can be applied to specify imitation cost (which we left abstract in Sec. 2) and
training procedure for our fail-safe generative imitator. Here, we use a version of GAIL (Ho and Ermon, 2016;
Kostrikov et al., 2018) for these things. In this section, we give a rough idea of how this works, while some
more detailed background and comments for the interested reader are in Appendix B. In GAIL, roughly
speaking, the imitation cost c(s,a)is given via a GAN’s discriminator that tries to distinguish between
imitator’s and demonstrator’s average state-action distribution, and thereby measures their dissimilarity. As
policy regularizer ψ(π)in Eq. (2), we take π’s diﬀerential entropy (Cover and Thomas, 2006).
Training As illustrated on the r.h.s. of Fig. 1, essentially, training consists of steps alternating between:
(a) doing rollouts of our fail-safe imitator πI,θ(a|s)under current parameter θto sample imitator state-
action trajectories (the generator ); (b) obtaining new discriminator parameters and thus c, by optimizing
a discriminator loss, based on trajectory samples of both, imitator πIand demonstrator πD; (c) obtaining
new fail-safe imitator policy parameter θ/primeby optimizing an estimated Eq. (2) from the trajectory sample
andc. Note that in (c), the fail-safe imitator’s closed-form density formula (Prop. 3) and its gradient are
used (the precise use depends on the speciﬁc choice of policy gradient (Kostrikov et al., 2018; Sutton and
Barto, 2018)).
5 Experiments
Figure 3: Driving from right to left, our fail-safe imita-
tor (blue) with past trajectory (blue) and future fail-
safe fallback emergency brake (green); other vehicles
(red), with worst-case reachable boundaries indicated
in yellow (rear vehicles are ignored as described).The goal of the following experiments is to demon-
strate tractability of our method, and empirically
compare safety as well as imitation/prediction per-
formancetobaselines. Wepickdriverimitationfrom
real-world data as task because it is relevant for
both,control(of vehicles), as well as modeling (e.g.,
of real “other” traﬃc participants, for validation of
self-driving “ego” algorithms Suo et al. (2021); Igl
et al. (2022)). Furthermore, its characteristics are
quite representative of various challenging safe con-
trol and robust modeling tasks, in particular includ-
ing ﬁxed (road boundary) as well as many moving
obstacles (other drivers), and multi-modal uncer-
tainties in driver behavior.
Data set and preprocessing We use the open highD data set (Krajewski et al., 2018), which consists
of 2-D car trajectories (each ∼20s) recorded by drones ﬂying over highway sections (we select a straight
section with∼1500trajectories). It is increasingly used for benchmarking (Rudenko et al., 2019; Zhang
et al., 2020). We ﬁlter out other vehicles in a roll-out once they are behind the ego vehicle.17Additionally,
we ﬁlter out initially unsafe states (using the same ﬁlter for all methods).
Setting and simulation We do an open-loop simulation where we only replace the ego vehicle by our
method/the baselines, while keeping the others’ trajectories from the original data. The action is 2-D
(longitudinal and lateral acceleration) and as the simulation environment’s dynamics fwe take a component-
wise double integrator. As collision/safety cost d(s), we take minus the minimum l∞distance (which is
17This is substantial ﬁltering, but is, in one way or another, commonly done (Pek and Althoﬀ, 2020; Shalev-Shwartz et al.,
2017). It is based on the idea that the ego is not blamable for other cars crashing into it from behind (and otherwise safety may
become too conservative; most initial states could already be unsafe if we consider adversarial rear vehicles that deliberately
crash from behind).
9Published in Transactions on Machine Learning Research (11/2022)
Table 1: Outcome on real-world driver data, in terms of imitation and safety (i.e., collision) performance.
Method Imitation performance Safety performance
Pre-safe Overall ADE FDE Probability of crash/oﬀ-road
GaussFAGIL-E (ours) 0.59 1.70 0.00
FAGIL-L (ours) 0.60 1.77 0.00
GAIL Ho and Ermon (2016) 0.47 1.32 0.13
RAIL Bhattacharyya et al. (2020) 0.48 1.35 0.22
TTOS (Sec. 3.3) 0.60 1.78 0.00
FlowFAGIL-E (ours) 0.58 1.69 0.00
FAGIL-L (ours) 0.57 1.68 0.00
GAIL Ho and Ermon (2016) 0.44 1.22 0.11
RAIL Bhattacharyya et al. (2020) 0.53 1.50 0.11
TTOS (Sec. 3.3) 0.59 1.72 0.00
particularly easy to compute) of ego to other vehicles (axis-aligned bounding boxes) and road boundary,
which is 2-Lipschitz (from norm /bardbl·/bardbl∞to|·|).
Policy fail-safe imitator We use our FAGIL imitator policy from Sec. 4.1. As learnable state feature ,
the state is ﬁrst rendered into an abstract birds-eye-view image, depicting other agents and road boundaries,
which is then fed into a convolutional net (CNN) for dimensionality reduction, similar as described in (Chai
et al., 2019). Regarding safe set inference: In this setting, where the two action/state dimensions and their
dynamics are separable, the others’ reachable rectangles can simply be computed exactly based on the others’
maximum longitudinal/lateral acceleration/velocity (this can be extended to approximate separability (Pek
and Althoﬀ, 2020)). As ego’s fallback maneuver candidates, we use non-linear shortest-time controllers to
roll out emergency brake and evasive maneuver trajectories (Pek and Althoﬀ, 2020). Then the total safety
costwis calculated by taking the maximum momentary safety cost dbetween ego’s maneuvers and others’
reachable rectangles over time. Note that the safety cost dand dynamics fwith these fallbacks are each
Lipschitz continuous as required by Prop. 1, rigorously implying the safety guarantee for FAGIL-L. (FAGIL-
E does only roughly but not strictly satisfy the conditions of Prop. 2, but we conjecture it can be extended
appropriately.) As safety layer , we use the distance-based instance from Sec. 4.1. Training: Training
happens according to Sec. 4.2, using GAIL with soft actor-critic (SAC) (Kostrikov et al., 2018; Haarnoja
et al., 2018) as policy training (part (c) in Sec. 4.2), based on our closed-form density from Sec. 3.2. Further
details on setup, methods and outcome are in Appendix C.
Baselines, pre-safe policies, ablations, metrics and evaluation Baselines are: classic GAILHo and
Ermon (2016), reward-augmented GAIL (RAIL) (closest comparison from our method class of safe generative
IL,discussedinSec.1)(Bhattacharyyaetal.,2020;2019), andGAILwith test-time-only safety layer (TTOS)
(Sec. 3.3). Note that TTOS can be seen as an ablation study of our method where we drop the safety layer
during train-time, and GAIL an ablation where we drop it during train and test time. For all methods
including ours, as (pre-safe) policy we evaluate both, Gaussian policy andconditional normalizing ﬂow .
We evaluate, on the test set, overall probability (frequency) of collisions over complete roll-outs (including
crashes with other vehicles and going oﬀ-road) as safety performance measure; and average displacement
error (ADE ; i.e., time-averaged distance between trajectories) and ﬁnal displacement error (FDE) between
demonstrator ego and imitator ego over trajectories of 4s length as imitation/prediction performance measure
(Suo et al., 2021).
Outcome and discussion The outcome is in Table 1 (rounded to two decimal points). It empirically
validates our theoretical safetyclaim by reaching 0% collisions of our FAGIL methods (with ﬁxed and moving
10Published in Transactions on Machine Learning Research (11/2022)
obstacles; note that additionally, TTOS validates safety of our safety inference/layer). At the same time
FAGIL comes close in imitation performance to the baselines RAIL/GAIL, which, even if using reward
augmentation to penalize collisions (RAIL), have signiﬁcant collision rates. (The rather high collision rate of
RAIL may also be due to the comparably small data set we use.) FAGIL’s imitation performance is slightly
better than TTOS, which can be seen as a hint towards an average-case conﬁrmation, for this setting, of
our theoretical worst-case analysis (Thm. 1), but the gap is rather narrow. Fig. 3 gives one rollout sample,
including imitator ego’s fail-safe and others’ reachable futures considered by our safe set inference.
6 Conclusion
In this paper, we considered the problem of safe and robust generative imitation learning, which is of
relevance for robust realistic simulations of agents, as well as when bringing learned agents into many real-
world applications from robotics to highly automated driving. We ﬁlled in several gaps that existed so far
towards solving this problem, in particular in terms of sample-based inference of guaranteed adversarially
safe action sets, safety layers with closed-form density/gradient, and the theoretical understanding of end-to-
end generative training with safety layers. Combining these results, we described a general abstract method
for the problem, as well as one speciﬁc tractable instantiation for a low-dimensional setting. The latter
we experimentally evaluated on the task of driver imitation from real-world data, which includes ﬁxed and
moving, uncertain obstacles (the other drivers).
Limitations of our work in particular come from the challenges that safety goals often bring with them, in
terms of tractability and cautiousness: First, while our theoretical results hold more generally, the speciﬁc
grid-basedmethodinstantiationwegiveistractableonlyinthelow-dimensionalcase, andintheexperimental
setting we harnessed tractability via separability of longitudinal and lateral dynamics of the other agents.
Second, worst-case safety can lead to overly cautious actions while human “demonstrators” often achieve
surprisingly good trade-oﬀs in this regard; incorporating more restrictions about other agents, such as the
responsibility-sensitive safety (RSS) framework, would still be covered by our general theory but help to
be less conservative. We also inherit known drawbacks of generative adversarial imitation learning-based
approaches, in particular instabilities in training, and this can further increase by the additional complexity
induced by the safety layer. On the experimental evaluation side, we focused on a ﬁrst study on one
challenging but restricted setting, leaving an extensive purely empirical study to future work.
In this sense, our work constitutes one step on the path of combining ﬂexibility and scalability (i.e., general
capacity for learning with little hand-crafting) of neural net-based probabilistic policies with theoretic worst-
casesafety guarantees , while maintaining end-to-end generative trainability.
Broader Impact Statement
When controlling agents in physical environments that also involve humans, such as robots that interact
with humans, where even small and/or rare mistakes of methods can injur people, “pure” deep imitation
learning approaches can be problematic. This is because often they are “black boxes” for which we do
not understand precisely enough how they behave “outside the training distribution”. Our approach can
ideally have a positive impact on making imitation learning-based methods more safe and robust for such
domains (or making them more applicable for such domains in the ﬁrst place). One limitation of our work
(beyond the technical limitations discussed above) is that in the end, society a has to debate and decide on
the diﬃcult trade-oﬀs in terms of cautiousness versus actionability that safe control often involves (clearly
safety is of highest value, but the question is how much residual risk society wants to take, given there rarely
exists perfect safety). Science can just acknowledge the problem and provide understanding and tools. And
overall, automation via machine learning approaches, including our approach, can have positive and negative
impacts on jobs and the economoy overall, which society needs to consider and make decisions about.
References
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In Interna-
tional conference on machine learning , pages 22–31. PMLR, 2017.
11Published in Transactions on Machine Learning Research (11/2022)
Lukáš Adam, Rostislav Horčík, Tomáš Kasl, and Tomáš Kroupa. Double oracle algorithm for computing
equilibria in continuous games. Proc. of AAAI-21 , 2021.
Anayo K Akametalu, Jaime F Fisac, Jeremy H Gillula, Shahab Kaynama, Melanie N Zeilinger, and Claire J
Tomlin. Reachability-based safe learning with gaussian processes. In 53rd IEEE Conference on Decision
and Control , pages 1424–1431. IEEE, 2014.
Mohammed Alshiekh, Roderick Bloem, Rüdiger Ehlers, Bettina Könighofer, Scott Niekum, and Ufuk Topcu.
Safe reinforcement learning via shielding. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence ,
volume 32, 2018.
Brandon Amos and J Zico Kolter. Optnet: Diﬀerentiable optimization as a layer in neural networks. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70 , pages 136–145. JMLR.
org, 2017.
Brandon Amos, Ivan Jimenez, Jacob Sacks, Byron Boots, and J Zico Kolter. Diﬀerentiable mpc for end-
to-end planning and control. In Advances in Neural Information Processing Systems , pages 8289–8300,
2018.
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In
International conference on machine learning , pages 214–223. PMLR, 2017.
ShaojieBai, J Zico Kolter, and Vladlen Koltun. Deep equilibriummodels. In Advances in Neural Information
Processing Systems , pages 688–699, 2019.
Mayank Bansal, Alex Krizhevsky, and Abhijit Ogale. Chauﬀeurnet: Learning to drive by imitating the best
and synthesizing the worst. arXiv preprint arXiv:1812.03079 , 2018.
Somil Bansal, Mo Chen, Sylvia Herbert, and Claire J Tomlin. Hamilton-jacobi reachability: A brief overview
and recent advances. In 2017 IEEE 56th Annual Conference on Decision and Control (CDC) , pages 2242–
2253. IEEE, 2017.
Nir Baram, Oron Anschel, Itai Caspi, and Shie Mannor. End-to-end diﬀerentiable adversarial imitation
learning. In International Conference on Machine Learning , pages 390–399. PMLR, 2017.
Felix Berkenkamp, Matteo Turchetta, Angela P Schoellig, and Andreas Krause. Safe model-based reinforce-
ment learning with stability guarantees. arXiv preprint arXiv:1705.08551 , 2017.
Dimitri Bertsekas. Dynamic programming and optimal control: Volume I , volume 1. Athena scientiﬁc, 2012.
Michel Besserve and Bernhard Schölkopf. Learning soft interventions in complex equilibrium systems. arXiv
preprint arXiv:2112.05729 , 2021.
Raunak Bhattacharyya, Blake Wulfe, Derek Phillips, Alex Kueﬂer, Jeremy Morton, Ransalu Senanayake,
and Mykel Kochenderfer. Modeling human driving behavior through generative adversarial imitation
learning. arXiv preprint arXiv:2006.06412 , 2020.
Raunak P Bhattacharyya, Derek J Phillips, Changliu Liu, Jayesh K Gupta, Katherine Driggs-Campbell,
and Mykel J Kochenderfer. Simulating emergent properties of human driving behavior using multi-agent
reward augmented imitation learning. In 2019 International Conference on Robotics and Automation
(ICRA), pages 789–795. IEEE, 2019.
Damian Boborzi, Florian Kleinicke, Jens Buchner, and Lars Mikelsons. Learning to drive from observations
while staying safe. 2021. upcoming.
Daniel Brown, Russell Coleman, Ravi Srinivasan, and Scott Niekum. Safe imitation learning via fast bayesian
reward inference from preferences. In International Conference on Machine Learning , pages 1165–1177.
PMLR, 2020.
12Published in Transactions on Machine Learning Research (11/2022)
Zhangjie Cao, Erdem Bıyık, Woodrow Z Wang, Allan Raventos, Adrien Gaidon, Guy Rosman, and Dorsa
Sadigh. Reinforcement learning based control of imitative policies for near-accident driving. arXiv preprint
arXiv:2007.00178 , 2020.
Michael R Caputo. The envelope theorem and comparative statics of nash equilibria. Games and Economic
Behavior , 13(2):201–224, 1996.
Yuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir Anguelov. Multipath: Multiple probabilistic
anchor trajectory hypotheses for behavior prediction. arXiv preprint arXiv:1910.05449 , 2019.
Jianyu Chen, Bodi Yuan, and Masayoshi Tomizuka. Deep imitation learning for autonomous driving in
generic urban scenarios with enhanced safety. arXiv preprint arXiv:1903.00640 , 2019.
Zhihao Cheng, Li Shen, Meng Fang, Liu Liu, and Dacheng Tao. Lagrangian generative adversarial imitation
learning with safety, 2022. URL https://openreview.net/forum?id=11PMuvv3tEO .
Yinlam Chow, Oﬁr Nachum, Aleksandra Faust, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh.
Lyapunov-based safe policy optimization for continuous control. arXiv preprint arXiv:1901.10031 , 2019.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory . Wiley-Interscience, 2006.
Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval Tassa. Safe
exploration in continuous action spaces. arXiv preprint arXiv:1801.08757 , 2018.
Priya L Donti, Melrose Roderick, Mahyar Fazlyab, and J Zico Kolter. Enforcing robust control guarantees
within neural network policies. In International Conference on Learning Representations , 2021a.
Priya L Donti, David Rolnick, and J Zico Kolter. Dc3: A learning method for optimization with hard
constraints. arXiv preprint arXiv:2104.12225 , 2021b.
LaurentElGhaoui, FangdaGu, BertrandTravacca, andArminAskari. Implicitdeeplearning. arXiv preprint
arXiv:1908.06315 , 2019.
David Fridovich-Keil and Claire J Tomlin. Approximate solutions to a class of reachability games. arXiv
preprint arXiv:2011.00601 , 2020.
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement
learning. arXiv preprint arXiv:1710.11248 , 2017.
Philipp Geiger and Christoph-Nikolas Straehle. Learning game-theoretic models of multiagent trajectories
using implicit layers. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 35, pages
4950–4958, 2021.
JeremyHGillula, ShahabKaynama, andClaireJTomlin. Sampling-basedapproximationoftheviabilityker-
nel for high-dimensional linear sampled-data systems. In Proceedings of the 17th international conference
on Hybrid systems: computation and control , pages 173–182, 2014.
Colas Le Guernic and Antoine Girard. Reachability analysis of hybrid systems using support functions. In
International Conference on Computer Aided Veriﬁcation , pages 540–554. Springer, 2009.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Oﬀ-policy maximum
entropy deep reinforcement learning with a stochastic actor. In International conference on machine
learning, pages 1861–1870. PMLR, 2018.
AaronHavensandBinHu. Onimitationlearningoflinearcontrolpolicies: Enforcingstabilityandrobustness
constraints via lmi conditions. In 2021 American Control Conference (ACC) , pages 882–887. IEEE, 2021.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural information
processing systems , 29:4565–4573, 2016.
13Published in Transactions on Machine Learning Research (11/2022)
Jonathan Ho, Jayesh Gupta, and Stefano Ermon. Model-free imitation learning with policy optimization.
InInternational Conference on Machine Learning , pages 2760–2769. PMLR, 2016.
Junning Huang, Sirui Xie, Jiankai Sun, Qiurui Ma, Chunxiao Liu, Jianping Shi, Dahua Lin, and Bolei Zhou.
Learning a decision module by imitating driver’s control behaviors. arXiv preprint arXiv:1912.00191 ,
2019.
MaximilianIgl, DaewooKim, AlexKueﬂer, PaulMougin, PunitShah, KyriacosShiarlis, DragomirAnguelov,
Mark Palatucci, Brandyn White, and Shimon Whiteson. Symphony: Learning realistic and diverse agents
for autonomous driving simulation. arXiv preprint arXiv:2205.03195 , 2022.
Zaynah Javed, Daniel S Brown, Satvik Sharma, Jerry Zhu, Ashwin Balakrishna, Marek Petrik, Anca Dragan,
and Ken Goldberg. Policy gradient bayesian robust optimization for imitation learning. In International
Conference on Machine Learning , pages 4785–4796. PMLR, 2021.
Achim Klenke. Probability theory: a comprehensive course . Springer Science & Business Media, 2013.
Vilmos Komornik. Topology, calculus and approximation . Springer, 2017.
Philip Koopman, Aaron Kane, and Jen Black. Credible autonomy safety argumentation. In 27th Safety-
Critical Systems Symposium , 2019.
Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tompson.
Discriminator-actor-critic: Addressing sample ineﬃciency and reward bias in adversarial imitation learn-
ing.arXiv preprint arXiv:1809.02925 , 2018.
Robert Krajewski, Julian Bock, Laurent Kloeker, and Lutz Eckstein. The highD Dataset: A Drone Dataset
of Naturalistic Vehicle Trajectories on German Highways for Validation of Highly Automated Driving
Systems. In 2018 IEEE 21st International Conference on Intelligent Transportation Systems (ITSC) ,
2018.
Jonathan Lacotte, Mohammad Ghavamzadeh, Yinlam Chow, and Marco Pavone. Risk-sensitive genera-
tive adversarial imitation learning. In The 22nd International Conference on Artiﬁcial Intelligence and
Statistics , pages 2154–2163. PMLR, 2019.
Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Pérolat, David
Silver, and Thore Graepel. A uniﬁed game-theoretic approach to multiagent reinforcement learning. arXiv
preprint arXiv:1711.00832 , 2017.
Chun Kai Ling, Fei Fang, and J Zico Kolter. What game are we playing? End-to-end learning in normal
and extensive form games. In IJCAI-ECAI-18: The 27th International Joint Conference on Artiﬁcial
Intelligence and the 23rd European Conference on Artiﬁcial Intelligence , 2018.
Chun Kai Ling, Fei Fang, and J Zico Kolter. Large scale learning of agent rationality in two-player zero-sum
games. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 33, pages 6104–6111,
2019.
Bjørnar Steinnes Luteberget. Numerical approximation of conformal mappings. Master’s thesis, Institutt
for matematiske fag, 2010.
Xiaobai Ma, Jayesh K Gupta, and Mykel J Kochenderfer. Normalizing ﬂow policies for multi-agent systems.
InInternational Conference on Decision and Game Theory for Security , pages 277–296. Springer, 2020.
Silvia Magdici and Matthias Althoﬀ. Fail-safe motion planning of autonomous vehicles. In 2016 IEEE 19th
International Conference on Intelligent Transportation Systems (ITSC) , pages 452–458. IEEE, 2016.
Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, and Jan Peters. An
algorithmic perspective on imitation learning. arXiv preprint arXiv:1811.06711 , 2018.
14Published in Transactions on Machine Learning Research (11/2022)
Fabian Otto, Philipp Becker, Ngo Anh Vien, Hanna Carolin Ziesche, and Gerhard Neumann. Diﬀerentiable
trust region layers for deep reinforcement learning. arXiv preprint arXiv:2101.09207 , 2021.
Brian Paden, Michal Čáp, Sze Zheng Yong, Dmitry Yershov, and Emilio Frazzoli. A survey of motion
planning and control techniques for self-driving urban vehicles. IEEE Transactions on intelligent vehicles ,
1(1):33–55, 2016.
George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshmi-
narayanan. Normalizing ﬂows for probabilistic modeling and inference. arXiv preprint arXiv:1912.02762 ,
2019.
Christian Pek and Matthias Althoﬀ. Computationally eﬃcient fail-safe trajectory planning for self-driving
vehicles using convex optimization. In 2018 21st International Conference on Intelligent Transportation
Systems (ITSC) , pages 1447–1454. IEEE, 2018.
Christian Pek and Matthias Althoﬀ. Fail-safe motion planning for online veriﬁcation of autonomous vehicles
using convex optimization. IEEE Transactions on Robotics , 37(3):798–814, 2020.
Stéphane Ross and Drew Bagnell. Eﬃcient reductions for imitation learning. In Proceedings of the thirteenth
international conference on artiﬁcial intelligence and statistics , pages 661–668. JMLR Workshop and
Conference Proceedings, 2010.
Andrey Rudenko, Luigi Palmieri, Michael Herman, Kris M Kitani, Dariu M Gavrila, and Kai O Arras.
Human motion trajectory prediction: A survey. arXiv preprint arXiv:1905.06113 , 2019.
Walter Rudin et al. Principles of mathematical analysis , volume 3. McGraw-hill New York, 1964.
Matthias Rungger and Paulo Tabuada. Computing robust controlled invariant sets of linear systems. IEEE
Transactions on Automatic Control , 62(7):3665–3670, 2017.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. On a formal model of safe and scalable
self-driving cars. arXiv preprint arXiv:1708.06374 , 2017.
Chelsea Sidrane, Amir Maleki, Ahmed Irfan, and Mykel J Kochenderfer. Overt: An algorithm for safety
veriﬁcationofneuralnetworkcontrolpoliciesfornonlinearsystems. Journal of Machine Learning Research ,
23(117):1–45, 2022.
Jiaming Song, Hongyu Ren, Dorsa Sadigh, and Stefano Ermon. Multi-agent generative adversarial imitation
learning. arXiv preprint arXiv:1807.09936 , 2018.
Simon Suo, Sebastian Regalado, Sergio Casas, and Raquel Urtasun. Traﬃcsim: Learning to simulate realistic
multi-agent behaviors. arXiv preprint arXiv:2101.06557 , 2021.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 2018.
Umar Syed and Robert E Schapire. A reduction from apprenticeship learning to classiﬁcation. Advances in
neural information processing systems , 23:2253–2261, 2010.
Brijen Thananjeyan, Ashwin Balakrishna, Suraj Nair, Michael Luo, Krishnan Srinivasan, Minho Hwang,
Joseph E Gonzalez, Julian Ibarz, Chelsea Finn, and Ken Goldberg. Recovery rl: Safe reinforcement
learning with learned recovery zones. IEEE Robotics and Automation Letters , 6(3):4915–4922, 2021.
Stephen Tu, Alexander Robey, Tingnan Zhang, and Nikolai Matni. On the sample complexity of stability
constrained imitation learning. In Learning for Dynamics and Control Conference , pages 180–191. PMLR,
2022.
Renée Vidal, Shawn Schaﬀert, John Lygeros, and Shankar Sastry. Controlled invariance of discrete time sys-
tems. In International Workshop on Hybrid Systems: Computation and Control , pages 437–451. Springer,
2000.
15Published in Transactions on Machine Learning Research (11/2022)
Kim P Wabersich and Melanie N Zeilinger. Linear model predictive safety certiﬁcation for learning-based
control. In 2018 IEEE Conference on Decision and Control (CDC) , pages 7130–7135. IEEE, 2018.
Patrick Nadeem Ward, Ariella Smofsky, and Avishek Joey Bose. Improving exploration in soft-actor-critic
with normalizing ﬂows policies. arXiv preprint arXiv:1906.02771 , 2019.
HuangXiao, MichaelHerman, JoergWagner, SebastianZiesche, JalalEtesami, andThaiHongLinh. Wasser-
stein adversarial imitation learning. arXiv preprint arXiv:1906.08113 , 2019.
Tian Xu, Ziniu Li, and Yang Yu. Error bounds of imitating policies and environments. arXiv preprint
arXiv:2010.11876 , 2020.
He Yin, Peter Seiler, Ming Jin, and Murat Arcak. Imitation learning with stability and safety guarantees.
IEEE Control Systems Letters , 2021.
Wenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin Yang, Sergio Casas, and Raquel Urtasun. End-
to-end interpretable neural motion planner. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 8660–8669, 2019.
Wenyuan Zeng, Shenlong Wang, Renjie Liao, Yun Chen, Bin Yang, and Raquel Urtasun. Dsdnet: Deep
structured self-driving network. In European Conference on Computer Vision , pages 156–172. Springer,
2020.
Chengyuan Zhang, Jiacheng Zhu, Wenshuo Wang, and Junqiang Xi. Spatiotemporal learning of multivehicle
interaction patterns in lane-change scenarios. arXiv preprint arXiv:2003.00759 , 2020.
16Published in Transactions on Machine Learning Research (11/2022)
Appendix
A Proofs with remarks
This section contains all proofs for the mathematical results of the main text, as well as some additional
remarks.
Note: In the main text, we did neither explicitly introduce the probaility space and random variables, nor
explicitly distinguish values/events of such variables from the variables themselves, since this was not crucial
and would have hindered the readability. However, in some of the following proofs, whenever it is necessary
to have full rigor, we will make these things explicit.
A.1 Proofs for Sec. 3.1
A.1.1 Proof of Prop. 1
Wewillneedthefollowingknownresult(afactthatliessomewherebetween envelope andmaximum theorem ):
Fact 1. Lethbe a function from X×YtoR, and assume XandRto be equipped with some given norms
(to which Lipschitz continuity refers in the following). Assume h(x,y)is Lipschitz in xwith constant L
uniformly in yand assume that maxima exist. Then the function x/mapsto→maxyh(x,y)is alsoL-Lipschitz.
For the sake of completeness, we provide a proof of Fact 1. Note: We show the statement for the case that
a supremum is always taken, i.e., is a maximum. For the supremum version we refer the reader to related
work.18
Proof of Fact 1. Let/bardbl·/bardbland|·|denote the given norms on X,R, respectively. Let e(x) := arg max yh(x,y)
(or pick one argmax if there are several).
Consider an arbitrary but ﬁxed pair x,x/primeand let, w.l.o.g., maxyh(x,y)≥maxyh(x/prime,y). Then:
|max
yh(x,y)−max
yh(x/prime,y)| (9)
= max
yh(x,y)−max
yh(x/prime,y) (10)
=h(x,e(x))−h(x/prime,e(x/prime)) (11)
≤h(x,e(x))−h(x/prime,e(x))(sinceh(x/prime,e(x/prime))≥h(x/prime,e(x))) (12)
≤L/bardblx−x/prime/bardbl. (13)
Now we can use this fact for the proof of Prop. 1:
Proof for Prop. 1. The idea is to ﬁrst propagate the Lipschitz continuity through the dynamics, and then
iteratively apply Fact 1 to the min/max operations.
Here, for any Lipschitz continuous function e, letLedenote its constant. Furthermore, regarding policies,
we may write π,σas shorthand for πt,σt,t∈1 :T, respectively.
First observe the following, given an arbitrary but ﬁxed tandπ,σ:
Let
g(s,π,σ ) :=f(s,π(s),σ(s)) (14)
18For a proof, see e.g., https://math.stackexchange.com/q/2532116/1060605 .
17Published in Transactions on Machine Learning Research (11/2022)
andh(s,π,σ,k )be deﬁned as the k-times concatenation of g(·,π,σ ), i.e., a roll out; and then deﬁne function
iby also including the initial action, i.e.,
i(s,a,π,σ,k ) :=h(f(s,a,σ (s)),π,σ,k ). (15)
Then, by our assumptions, for π,σ,karbitrary but ﬁxed,
s/mapsto→d(h(s,π,σ,k )) (16)
is Lipschitz continuous in swith constant (uniformly in π,σ)
LdLk
f(·,π(·),σ(·))=αβk, (17)
since it is the composition of d, which isα-Lipschitz, with the k-times concatenation of g, which isβ-Lipschitz
by assumption. This together with our assumption implies that, for any arbitrary but ﬁxed s, also
a/mapsto→d(i(s,a,π,σ,k ))(=wt(s,a)) (18)
is Lipschitz uniformly in π,σ, with constant (for any k)
αβkβ=αβk+1≤αmax{1,βT}. (19)
Now let state sbe arbitrary but ﬁxed. Also let πbe arbitrary but ﬁxed. Then, based on the above together
with Fact 1,
a/mapsto→max
σt:Tmax
t∈t+1:Td(i(s,a,π,σ,t ))(=:e(a,π)) (20)
is Lipschitz with constant αmax{1,βT}. Again applying Fact 1, we see that also
a/mapsto→min
πt+1:Tmax
σt:Tmax
t∈t+1:Td(i(s,a,π,σ,t ))(= min
πe(a,π)) (21)
is Lipschitz with constant αmax{1,βT}.
Remark 3. Alternative versions of this result could be based, e.g., on the “Envelope Theorem for Nash
Equilibria” (Caputo, 1996).
A.1.2 Proof of Prop. 2
Proof for Prop. 2. Let the mapping ibe deﬁned as in Eq. (15). Fix s.
Observe that, based on our assumption that dis convex and the dynamics flinear, for any σ,tﬁxed,
(a,π)/mapsto→d(i(s,a,π,σ,t )) (22)
is convex. But then, also the maximum over σ,t, i.e.,
(a,π)/mapsto→max
σt:Tmax
t∈t+1:Td(i(s,a,π,σ,t )) (23)
is convex, because the element-wise maximum over a family of convex functions – in our case the family
indexed by σ,t– is again convex.
One step further, this means (since convexity is preserved by “minimizing out” one variable over a convex
domain) that also taking the minimum over πpreserves convexity, i.e.,
wt(s,a) = min
πt+1:Tmax
σt:Tmax
t∈t+1:Td(i(s,a,π,σ,t )) (24)
is convex as a function of a.
But a convex function over a (compact) convex set that is spanned by a set of corners, takes its maximum
at (at least one) of the corners.
18Published in Transactions on Machine Learning Research (11/2022)
A.2 Proofs for Sec. 3.2
A.2.1 Proof of Prop. 3
Proof for Prop. 3. Intuitively it is clear that the results follows from combining σ-additivity of measures
with the change-of-variables formula. Nonetheless we give a rigorous derivation here.
Keep in mind the following explicit speciﬁcations that were left implicit in the main part: In the main part
we just stated to assume Lebesgue densities on A⊂Rn, implicitly we assume Rnwith the usual Lebesgue
σ-algebra as underlying measurable space, and that all parts Akare measurable. Furthermore, to be precise,
we assume diﬀerentiability of the gkon theinterior of their respective domain.
Recall that (Ak)kis a countable partition of A, and we assumed diﬀeomorphisms gk:Ak→¯Ak⊂˜A
(diﬀeomorphic on the interior of Ak) and thatg|Ak=gk.
Note that based on our assumptions, either gis already measurable, or, otherwise, we can turn it into a
measurable function by just modifying it on a Lebesgue null set (the boundaries of the Ak), not aﬀecting
the below argument.
LetPAdenote the original measure on outcome space A(restriction from Rn), andP˜Aits push-forward
measure on ˜Ainduced by g.
Then, up to null sets, for any measurable set M⊂˜A,
P˜A(M) (25)
=PA(g−1(M)) (26)
=PA(g−1(M)∩/uniondisplay
kAk) (27)
=PA(/uniondisplay
kg−1(M)∩Ak) (28)
=PA(/uniondisplay
kg−1
k(M)) (29)
=/summationdisplay
kPA(g−1
k(M)) (30)
=/summationdisplay
k/integraldisplay
M|det(Jg−1
k(¯a))|pˆa(g−1
k(¯a))[¯a∈gk(Ak)]d¯a (31)
=/integraldisplay
M/summationdisplay
k:¯a∈gk(Ak)|det(Jg−1
k(¯a))|pˆa(g−1
k(¯a))d¯a (32)
where Eq. (31) is the classic change-of-variables (also called transformation formula/theorem) (Klenke, 2013)
applied to the push-forward measure of PAundergk. Therefore, the19density ofP˜A, i.e.,p¯a(¯a), is given by/summationtext
k:¯a∈gk(Ak)|det(Jg−1
k(ˆa))|pˆa(g−1
k(¯a)).
A.3 Proofs and precise counterexample ﬁgure/MDP/agents for Sec. 3.3
In this section we give all elaborations and proofs for the end-to-end versus test-time-only safety analysis of
Sec. 3.3.
For this section, keep in mind the following notation and remarks:
•In the discrete setting of this section, the imitation cost function cetc. are vectors in the Euklidean
space here; and, for instance, c·PD
Stor justcPD
Ststands for the inner product , the expectation of the
cost variable c.
19Rigorously speaking, it is adensity.
19Published in Transactions on Machine Learning Research (11/2022)
•If not stated otherwise, /bardbl·/bardblrefers to the l1-norm/bardbl·/bardbl 1.
•Keep in mind that for the total variation distance DTVwe have the relation to the l1norm
DTV(p,q) =1
2/bardblp−q/bardbl1.
•Keep in mind that, as in the main text, letters D,I,U,O stand for demonstrator, imitator (our fail-
safe imitator with safety layer already during training time), unsafe/unconstrained trained imitator,
and test-time-only-safety imitator, respectively; and PD,PI,PU,POetc. denote the probabilities
under the respective policies, and same for ρDetc.
A.3.1 Derviation of Rem. 1
The following derivation works along the line of related established derivations, e.g., the one for inﬁnite-
horizon GAIL guarantees (Xu et al., 2020).
Derviation of Rem. 1. Keep in mind that for any discrete distributions ps,a(s,a),qs,a(s,a), when marginal-
izig outa, we get for ps,qs,
/bardblps−qs/bardbl (33)
=/summationdisplay
s|p(s)−q(s)| (34)
=/summationdisplay
s|/summationdisplay
ap(s,a)−q(s,a)| (35)
≤/summationdisplay
s/summationdisplay
a|p(s,a)−q(s,a)|(triangle inequality) (36)
=/bardblps,a−qs,a/bardbl. (37)
Now we have
|vI−vD| (38)
=|/summationdisplay
tcPI
st−/summationdisplay
tcPD
st| (39)
=|cT(/summationdisplay
t1
TPI
st−/summationdisplay
t1
TPD
st)| (40)
=T|c(ρI
s−ρD
s)|(Deﬁnition ρ) (41)
≤T/bardblc/bardbl∞/bardblρI
s−ρD
s/bardbl (42)
≤T/bardblc/bardbl∞/bardblρI
s,a−ρD
s,a/bardbl(Eq. (37)) (43)
=T/bardblc/bardbl∞2DTV(ρI
s,a,ρD
s,a). (44)
A.3.2 Proof of Thm. 1, including elaboration of example (ﬁgure)
Elaboration of the example for the lower bound (Fig. 2) Here, let us ﬁrst give a precise version
– Fig. 4 – of the example in Fig. 2, that is used to get the lower bound in Thm. 1. Instead of the exact
Fig. 2, we give an “isomorphic” version with more detailed/realistic states and actions in terms of relative
position/velocity/accleration/lane.
Specify the Markov decision process (MDP) and agents (policies) as follows, with imitation error pU=δas
paramteter:
MDP:
20Published in Transactions on Machine Learning Research (11/2022)
unsafe states/distance, occasionally
visitied by unsafe trained imitator
safe side strip, never visitied by unsafe
imitator, hence not learned to recover
front
vehicle(0,0,/lscript) (0,1, /lscript) (1,0, /lscript)
(1,0, r)(+1, /lscript)pU=δ
(−1, /lscript)pU=1
(−1, r)pO
=1(−1, /lscript),(+1, /lscript)pU=1
(0, /lscript)pD=1main road
side strip
s
Figure 4: More detailed (but essentially isomorphic) version of the example MDP/agents from Fig. 2.
•States:The states (of the ego) are triplets ( ∆x,∆v, lane), where ∆xis deviation from demonstrator
position, ∆vis deviation from demonstrator velocity, lane is lane ( /lscriptfor left, or rfor right). (The
demonstrator is in a safe state.) There are ﬁve possible states, all depicted in the ﬁgure, except for
(1,−1,/lscript), to keep the ﬁgure simple.
•Actions: The possible actions are depicted as arrows, with the action (acc,lane)written below the
arrow, where acc means the longitudinal acceleration, and lane the target lane.
•Transition: The longitudinal transition is the usual double integrator of adding acc to ∆v, while,
lateral-wise, action lane instantaneously sets the new lane (more realistic but complex examples are
possible of course).
•Costs:The true demonstrator’s cost c∗is 0 on the demonstrator state (0,0,/lscript), and 1 on every other
state.
•Horizon: formt= 1toT, where we consider Ta parameter (so it is strictly speaking a family of
MDPs).
Agents:
•Demonstrator: Yellow car, always ( pD= 1) staying at the depicted state, referred to as sD(=
(0,0,/lscript)), at a more than safe distance to front car (red).
•Unconstrained trained imitator (blue): due to a slight imitation error, it is sometimes (with proba-
bilitypU=δ) accelerating, and if it accelerates, it necessarily reaches an unsafe state (after one more
step, due to the “inertia” of the double integrator), but, as implied by the GAIL loss, always recovers
(dashed arrows) back to true demonstrator state.
•Safety-constrained test-time imitator (same, except for last action in brown): as the unconstrained
imitator, it occasionally ( pO=δ) ends up at rightmost safe state, and then the only remaining safe
action is to change lane to the side strip, where there was no data on how to recover, so getting
trapped there forever .
Initialization – agent-dependent: While the overall agrument works as well for an agent-independent
initialization, we do an agent-speciﬁc initialization, since it simpliﬁes calculations substantially. At t= 1, for
the demonstartor, we let p(s1)have full mass on the demonstrator state sD. For the unconstrained imitator,
asp(s1), we take the stationary distribution under the induced Markov process, which is1
1+3δ(1,δ,δ,δ ),
where the ﬁrst component means the demonstartor state sD.
Proof of Thm. 1 Keep in mind that we will occasionally use upper case letters like St,At, etc., to refer
to the actual random variables of state and action, etc., distinguishing them from the values st,atthat those
random variables can take.
Proof for Thm. 1. Lower bound part:
21Published in Transactions on Machine Learning Research (11/2022)
Keep in mind the counterexample construction above, in particular its deviation parameter δ, to which the
following arguments refer. Let sD,aDdenote the state and action that the demonstrator is constantly in.
Construction of δ; implication DTV(ρD,ρU)≤ε:
By construction, we have ρD(sD,aD) = 1, while for the unconstrained imitator, ρU(sD,aD) =1
1+3δ·(1−δ)
(recall that ρU(s)is the time-average over the state distribution, which we took as the stationary one for
the unconstrained imitator, giving mass1
1+3δonsD, and 1−δis the action probability).
So
|ρD(sD,aD)−ρU(sD,aD)|=|1−1−δ
1 + 3δ|=4δ
1 + 3δ.
So, by symmetry, DTV(ρD,ρU)is one half times twice this,
DTV(ρD,ρU)≤4δ
1 + 3δ.
Additionally, it is easy to see that for 1≥δ≥0,4δ
1+3δ≤4δ, and so
DTV(ρD,ρU)≤4δ. (45)
Therefore choose
δ:=1
4ε, (46)
based on the εgiven by assumption. This implies DTV(ρD,ρU)≤ε.
Preparation of a bound statement we need later:
As preparation which we repeatedly need later, observe that there is some constant κ>0, such that for all
T≥2, we have
κ≤(1 +−1
T)T≤e−1, (47)
whereehere is the Euler number (exponential) as usual, and for κwe can take, e.g.,1
4. To have a ﬁrst hint
why this holds, see that
(1 +−1
T)TT→∞−−−−→e−1.
For thefull argument , observe the following: First, observe that at T= 2, we have (1+−1
T)T=1
4. Second, it
is suﬃcient to show that the expression is monotonically growing, i.e., the derivatived
dT(1 +−1
T)Tis positive
forT≥2. But this is equivalent tod
dTlog((1 +−1
T)T) =1
T−1+ log(T−1
T)being positive, since logis a
monotonic function. But the latter follows from the fact that always x≥log(1 +x)and then plugging in
1
T−1forx, yieldig1
T−1≥log(1 +1
T−1) = log(T
T−1) =−log(T−1
T).
Now, before going into the main proof parts, recall that we want to show
|vO−vD|≥ιmin{εT2,T}/bardblc∗/bardbl∞ (48)
forιsome constant indepentent of ε,T.
We do so by considering the case where it is quadratic separately from where it is linear.20
Proof for the lower bound, case δT≤1:
Consider the case δT≤1.
20Note that our construction is diﬀerent from behavior cloning error analysis (Ross and Bagnell, 2010; Syed and Schapire,
2010) especially in that we do not need one separate starting state.
22Published in Transactions on Machine Learning Research (11/2022)
By construction, at each ﬁxed stage t, the probability that the test-time constrained imitator πOis not in
the demonstrator state is the summed probability of not deviating from sDforktimes,k≤t, and then
deviating once, i.e.,
PO(St/negationslash=sD) =t/summationdisplay
k=1δ(1−δ)k−1. (49)
We want to bound the term (1−δ)k−1from below, to show that the quadratic bound of Eq. (48) holds
under the current case δT≤1.
We have, for any k≤T,
(1−δ)k≥(1−1
T)k≥(1−1
T)T= (1 +−1
T)T≥κ (50)
where the ﬁrst inequality holds because we assumed δ≤1
Tand so 1−δ≥1−1
T, and the second one because
alwaysk≤Tand third is Eq. (47).
Therefore Eq. (49) can be bounded from below by
t/summationdisplay
k=1δκ=tδκ. (51)
Now we get, with the cost vector /vector c(i.e., the safety cost function c, but making explicit it is a vector since
we are in the discrete case) being 0 in the demonstrator state, and /bardbl/vector c/bardbl∞elsewhere,
|vO−vD| (52)
=|/summationdisplay
t(PO
st−PD
st)·/vector c| (53)
=|/summationdisplay
t(PO(St/negationslash=sD)−0)/bardbl/vector c/bardbl∞| (54)
≥/summationdisplay
t(tδκ)·/bardbl/vector c/bardbl∞(Eq. (51) etc.) (55)
≥δκ/bardbl/vector c/bardbl∞/summationdisplay
tt (56)
≥δκ/prime/bardbl/vector c/bardbl∞T2(57)
for someκ/prime(that absorbs both, κand the deviation between T2and/summationtext
tt.
Proof for the lower bound, case δT > 1:
First note that, alternatively to the previous deviation, we can also write PO(St/negationslash=sD)as the complementary
probability of test-time constrained imitator nothaving deviated from the demonstrator state sDso far, i.e.,
PO(St/negationslash=sD) = 1−(1−δ)t≥1−(1−1
T)t, (58)
since in the current case, δT > 1and soδ >1
Tand so 1−δ <1−1
Tand so (1−δ)t<(1−1
T)tand so
1−(1−δ)t>1−(1−1
T)t.
Now, fort≥T
2, we get,
(1−1
T)t≤(1−1
T)T
2= ((1 +−1
T)T)1
2≤(e−1)1
2<1, (59)
where the second last inequality is Eq. (47). So (1−1
T)tis strictly below and bounded away from 1, and
therefore Eq. (58) is uniformly strictly above and bounded away from 0, i.e.,PC(St/negationslash=sD)is greaterκ/prime/prime>0
fort≥T
2.
23Published in Transactions on Machine Learning Research (11/2022)
Then
|vO−vD| (60)
=|T/summationdisplay
t=1(PO
st−PD
st)·/vector c| (61)
=|T/summationdisplay
t=1(PO(St/negationslash=sD)−0)/bardbl/vector c/bardbl∞| (62)
≥T/summationdisplay
t=T
2PO(St/negationslash=sD)/bardbl/vector c/bardbl∞ (63)
≥T/summationdisplay
t=T
2κ/prime/prime/bardbl/vector c/bardbl∞ (64)
=T
2κ/prime/prime/bardbl/vector c/bardbl∞ (65)
=κ/prime/prime/primeT/bardbl/vector c/bardbl∞. (66)
Proof for the lower bound, ﬁnal remarks:
ιis given by taking the minima of the above κ’s and conbining it with the relation Eq. (46).
Upper bound part:
In what follows, Pstand for probability vectors/matrices, slightly overriding notation, and PS/prime|Sfor the
transition probability (corresponding to fplus potential noise).
First observe that generally (here Pstand for probability vectors/matrices, slightly overriding notation),
∆t+1:=/bardblPO
St+1−PD
St+1/bardbl1 (67)
=/bardblPO
S/prime|SPO
St−PD
S/prime|SPD
St/bardbl1 (68)
=/bardblPO
S/prime|SPO
St−PO
S/prime|SPD
St+PO
S/prime|SPD
St−PD
S/prime|SPD
St/bardbl1 (69)
=/bardblPO
S/prime|S(PO
St−PD
St) + (PO
S/prime|S−PD
S/prime|S)PD
St/bardbl1 (70)
≤/bardblPO
S/prime|S(PO
St−PD
St)/bardbl1+/bardbl(PO
S/prime|S−PD
S/prime|S)PD
St/bardbl1 (71)
First consider the ﬁrst term in Eq. (71).We can generally, just since the 1-norm of any stochastic matrix is
1, bound it as follows:
/bardblPO
S/prime|S(PO
St−PD
St)/bardbl1 (72)
≤/bardblPO
S/prime|S/bardbl1∆t= 1∆t (73)
Regarding the second term in Eq. (71),we have the following way to bound it.
Let¯Sdenote the subset of the set Sof states where ρD(s)has support, and, as in the main text, ν >0is
the minimum value of ρD(s)on¯S. Then
24Published in Transactions on Machine Learning Research (11/2022)
ν/summationdisplay
s∈¯S/bardblπD(·|s)−πU(·|s)/bardbl1 (74)
=ν/summationdisplay
s∈¯S,a|πD(a|s)−πU(a|s)| (75)
≤/summationdisplay
s∈¯S,aρD(s)|πD(a|s)−πU(a|s)| (76)
=/summationdisplay
s,aρD(s)|πD(a|s)−πU(a|s)| (77)
=/summationdisplay
s,a|πD(a|s)ρD(s)−πU(a|s)ρD(s)| (78)
(79)
=/summationdisplay
s,a|πD(a|s)ρD(s) +πU(a|s)ρU(s)−πU(a|s)ρU(s)−πU(a|s)ρD(s)| (80)
≤/summationdisplay
s,a|πD(a|s)ρD(s)−πU(a|s)ρU(s)|+|πU(a|s)ρU(s)−πU(a|s)ρD(s)|(triangle inequality) (81)
=/summationdisplay
s,a|πD(a|s)ρD(s)−πU(a|s)ρU(s)|+πU(a|s)|ρU(s)−ρD(s)|? (82)
≤/summationdisplay
s,a|πD(a|s)ρD(s)−πU(a|s)ρU(s)|+|ρU(s)−ρD(s)| (83)
=/bardblρD
s,a−ρU
s,a/bardbl1+/bardblρD
s−ρU
s/bardbl1(recall deﬁnition ρ) (84)
≤2ε+ 2ε(by assumption and Eq. (37)) , (85)
so in particular
max
s∈¯S/bardblπD(·|s)−πU(·|s)/bardbl1≤4ε/ν. (86)
Therefore
/bardblPD
a|s|¯S−PU
a|s|¯S/bardbl1=/bardblπD
a|s|¯S−πU
a|s|¯S/bardbl1= max
s∈¯S/bardblπD(·|s)−πU(·|s)/bardbl1≤4ε
ν, (87)
where the ﬁrst equation is just a rewriting (note that here, πD
a|s|¯Setc. denote matrices (transition matrix),
whileπD(·|s)etc. denotes a vector (probability vector)). Note that this also implies the same bound for the
test-time-constrained imitator, i.e.,
/bardblPD
a|s|¯S−PO
a|s|¯S/bardbl1≤4ε
ν, (88)
for the following reason: only the deviating mass between DandUthat lies on actions where Ddoes not
have support can be redistributed by the safety layer (because we assumed that Dis safe, and hence those
actions are safe and are not aﬀected by adding a safety layer, based on our assumption about the safety
layer). But this deviation mass already lies where it has the maximum eﬀect on the norm (namely: where
Ddoes not have any mass at all). So redistributing it will make the deviation not bigger.
Furthermore we have, just by the deﬁnition of the conditional probability matrix PS,A|S
/bardbl(PO
S,A|S−PD
S,A|S)/bardbl1=/bardbl(PO
A|S−PD
A|S)/bardbl1, (89)
and the same holds when restricting the state to any subset, in particular ¯S.
25Published in Transactions on Machine Learning Research (11/2022)
Now, to ﬁnalize our argument to also bound the second term in Eq. (71), observe
/bardbl(PO
S/prime|S−PD
S/prime|S)PD
St/bardbl1 (90)
=/bardbl(PO
S/prime|S|¯S−PD
S/prime|S|¯S)PD
St|¯S/bardbl1(since ¯Sis the demonstrator support) (91)
≤/bardbl(PO
S/prime|S|¯S−PD
S/prime|S|¯S)/bardbl1/bardblPD
St|¯S/bardbl1 (92)
=/bardblPS/prime|S,A(PO
S,A|S|¯S−PD
S,A|S|¯S)/bardbl1/bardblPD
St|¯S/bardbl1 (93)
≤/bardblPS/prime|S,A/bardbl1/bardblPO
S,A|S|¯S−PD
S,A|S|¯S/bardbl1/bardblPD
St|¯S/bardbl1 (94)
=/bardblPS/prime|S,A/bardbl1/bardblPO
A|S|¯S−PD
A|S|¯S/bardbl1/bardblPD
St|¯S/bardbl1(Eq. (89)) (95)
≤1·4ε
ν·1(Eq. (88), 1-norm of stochastic matrix/vector is 1) . (96)
Coming towards the end, observe that plugging Eq. (71), (73) and (96) together gives
∆t+1−∆t≤4ε
ν.
Therefore
∆t=/summationdisplay
t/prime≤t∆t/prime+1−∆t/prime≤t4ε
ν.
Then (keep in mind that we assumed that the reward depends only on the state)
|vO−vD|=|/summationdisplay
t(PO
st−PD
st)·/vector c| (97)
≤/summationdisplay
t/bardbl(PO
St−PD
St)/bardbl1/bardbl/vector c/bardbl∞ (98)
=/bardbl/vector c/bardbl∞/summationdisplay
t∆t (99)
≤/bardbl/vector c/bardbl∞/summationdisplay
tt4ε
ν(100)
≤/bardbl/vector c/bardbl∞4ε
νT2. (101)
Remark 4 (Remarks regarding Thm. 1 and proof) .Some considerations:
•Why do we have this “min” lower bound in Eq. (7)of Thm. 1, and not just a purely quadratic one?
Note that in the above proof, if the case δ >1
Tholds, then δT > 1, but the maximum deviation
between imitator and demonstator cannot be larger than the imitator constantly being at a state
diﬀerent from the demonstartor with probability 1, i.e., can asymptotically not be larger than T/bardblc/bardbl∞
(which is then smaller than δTT/bardblc/bardbl∞in this case).
•Note that the theorem formulation is about capturing all possible scenarios (within our overall set-
ting), and thus also accounts for the “worst-case” ones. In practice, depending on the speciﬁc
scenario, of course things may be “better behaved” for the test-time-only-safety approach than the
(quadratic) error concluded by the theorem.
B Additional background on GAIL cost and training
Overall we use (Wasserstein) GAIL, an established methodology, for imitation loss and training in our
approach. The rough idea of this was already given in Sec. 4.2.
26Published in Transactions on Machine Learning Research (11/2022)
For the full account on details of (Wasserstein) GAIL, we refer the reader to the original work (Ho and
Ermon, 2016; Xiao et al., 2019). Here let us nonetheless summarize some background on GAIL cost function
and training, and comment on the parts relevant to our approach. (Additionally, some impelementation
details on the version we use can be found in Sec. 5 and appendix C.)
B.1 Imitation loss
Let us elaborate on the GAIL-based (Ho and Ermon, 2016) cost function. We do it more generally here to
also include Wasserstein-GAIL/GAN.
The reason why we consider using the Wasserstein-version Xiao et al. (2019) is the following: Due to safety
constraints, if the demonstrator is unsafe, we may deal with distributions of disjoint support. Then we
want to encourage “geometric closeness” of distributions. But this is not possible with geometry-agnostic
information-theoretic distance measures.
To specify the imitation cost cthat we left abstract in Sec. 2 and 4.2, let
c(s,a) =e2(s,a), (102)
(e1,e2) = arg max EπD(/summationdisplay
te1(st,at)) +EπI(/summationdisplay
te2(st,at)) (103)
where the arg max ranges over pairs (e1,e2)of bounded functions from some underlying space of bounded
functions (Xiao et al., 2019), for which, besides Eq. (103), the following is required (Xiao et al., 2019):
e1(x) +e2(y)≤δ(x,y), (104)
for some underlying distance δ. This condition is referred to as Lipschitz regularity .e1,e2are called Kan-
torovich potentials .
As classic example, when taking e1(s,a) := log(1−D(s,a))ande2(s,a) := log(D(s,a)), for some function
D, then this exactly amounts to the classic GAN/GAIL formulation with discriminator D.
B.2 Remark on Training and the Handling of the State-Dependent Safe Action Set
Regarding the policy optimization step (c) sketched in Sec. 4.2, note that: For each sampled state st,
πI,θ(a|st)implicitly remembers the safe set ˜As
t, such that also after any policy gradient step its mass will
always remain within the safe set. Note that for this to work, it can be a problem if at some point we took
a safe fallback action (Sec. 3.1) and thus only know a singleton safe set (and it is not a function of current
state only). Since this is rare, it should pose little problems to training. Rigorously addressing this is left to
future work.
B.3 Comment on probability measure underlying the expectation
Observe that in Eq. (2) we take the expectation under the measure induced by dynamic system plus pol-
icy, over the summed future imitation cost. Alternatively, sometimes the expectation is taken under the
time-averaged state-action distribution ρ(s,a)(Sec. 3.3), over the imitation cost. As far as we see, both
formulations seem to be equivalent, similar as is discussed in (Ho et al., 2016) for the discount-factor-based
formulation.
C Additional details of experiments and implementation
Here we give some additional details for our experiments and implementation of Sec. 5. These details are
not necessary for the rough idea, but to get the detailed picture of these parts.
Implementation code is available at: https://github.com/boschresearch/fagil .
27Published in Transactions on Machine Learning Research (11/2022)
C.1 Additional architecture, computation and training information
Details of policy architectures:
•For the Gaussian policy (i.e., parameterizing mean and covariance matrix), we take two hidden
layers each 1024 units.
•As pre-safe normalizing ﬂow policy, we take 8 aﬃne coupling layers with each coupling layer con-
sisting of two hidden layers a 128 units.
•As discriminator, we take a neural net with two hidden layers and 128 units each.
•As state feature CNN, we take a ResNet18 applied to the abstract birds-eye-view image representa-
tion of the state (Chai et al., 2019).
For the custom neural nets, we use leaky ReLUs as non-linearities.
The data set (scene) consists of ∼1500tracks (trajectories), of which we use 300 as test set and 100 as
validation, and the rest as training set.
One full safe set computation with some limited vectorized parts takes around 1.5s on a standard CPU. We
believe vectorizing/parallelizing more of the code can further reduce this substantially.
For training, we use GAIL with training based on soft actor-critic (SAC) (Kostrikov et al., 2018; Haarnoja
et al., 2018).
For the discriminator and its loss, we build on parameter clipping from Wasserstein GANs Arjovsky et al.
(2017), and adversarial inverse reinforcement learning (AIRL) (Fu et al., 2017).
C.2 Regarding safety guarantees and momentary safety cost
Regarding the fact that Prop. 2 does not strictly cover FAGIL-E yet: We conjecture that, extending Prop. 2,
a guarantee can be given for the l∞setting even without convexity over all dimensions, simply by the l∞
norm being a minimum over dimensions and then in some way considering each dimension individually.
Regarding Lipschitz continuity of momentary safety cost: Note that, as brieﬂy stated in the main text, the
momentary safety cost din our experiments is Lipschitz. Intuitively, it is some form of negative (minimum)
distance function between ego and other vehicles. Distance/cost d≤0is momentarily safe, while d >0is
a collision. That is, dis a Lipschitz continuous function, but d= 0is the boundary between the set of safe
states and unsafe states. (Similar to the safe action set being the sub-zero (i.e., w≤0) set of the total safety
costw.)
We believe that Lipschitz continuity is a fairly general assumption for safety, since safety is often considered
in the physical world. And in the physical world, many of the underlying dependencies are in fact suﬃciently
continuous.
D Broader supplementary remarks
In this section, we gather several further discussions that are more broadly related to the results of the main
part.
This section is not necessary to describe or understand the main part.
However, given that safe generative IL is a ﬁeld that is rather little explored, we feel it can be particularly
useful to provide some further comments here that may help understanding and may serve as a basis for
next steps.
28Published in Transactions on Machine Learning Research (11/2022)
D.1 Further details on Sec. 3.1
Several broader remarks regarding safe set inference, which are not necessary to understand the core deﬁni-
tions and results:
•A-temporally, Eq. (4) can be seen as a Stackelberg game.
•In Eq. (4), the others’ maxima may often collapse to open-loop trajectories.
•We use the general formulation where policies πtcan depend on t. This allows us to easily treat the
case where we directly plan an open-loop action trajectory a1:Tsimultaneously, by simply πt(st) :=
at, for allt,st.
•The state set corresponding to our action set would be an “invariant safe set” (Bansal et al., 2017).
•Lipschitz/continuity understanding is helpful also for other things: e.g., understanding the safe set’s
topology for Sec. 3.2.
•Our sample-based approach can also interpreted as follows: we can use single-action safety checkers
(in the sense of the “doer-checker” approach (Koopman et al., 2019)) as oracles for action-set safety
inference.
•On Prop. 2: Open-loop is a restriction but note that we do search over alltrajectories in Π,Φ, so it
is similar to trajectory-based planning.
•Regarding inner approximations of the safe set: For as little as possible bias and conservativity, we
need the inner approximation of the safe set to be as large as possible. In principle, one could also
just give a ﬁnite set of points as inner approximation, but this could be very unﬂexible and biased,
and injectivity for change-of-variables Prop. 3 would not be possible.
•Regarding safety between discrete time steps: Note that our setup is discrete time, but it can directly
also imply safety guarantees between time stages: e.g., for our experiments, we know maximum
velocity of the agents, so we know how much they can move at most between time stages, and we
can add this as a margin to the distance-based safety cost.
•Small set of candidate ego policies, to simplify calculations: As an important simpliﬁcation which
preserves guarantees, in the deﬁnition/evaluation of safe set and total safety cost wt(s,a), we let
the ego policies range over some small, sometimes ﬁnite set ¯Πt+1:T⊂Πt+1:Tof reasonable fallback
future continuations of ego policies. In the case of autonomous driving (see experiments), ¯Πt+1:T
consists of emergency brake andevasive maneuvers (additional details are in Appendix D.1).21
•A popular approach for trajectory planning/optimization is model predictive control (MPC) Bert-
sekas (2012), so let us brieﬂy comment on the relation between our trajectory calculations for safety
andMPC:Inasense, MPCcouldsolvethetrajectoryoptimization, i.e., the“minpart”ofEq.(4). So,
to some extent, it is related. However, MPC would need one speciﬁc policy/dynamics for the other
agents, while we just have a set of possible other agents policies (and take the worst-case/adversarial
case over this set). And MPC alone would also not give us a set of safe actions (which we need
since we want to allow as much support for the generative policy density as possible), but only one
action/trajectory.
•Note that the “adversarial” in the title of our work has the double meaning of “worst-case safety”
and “using generative adversarial training”.
21Regarding the ego, note that the search over a small set of future trajectories in the form of the “fallback maneuvers”
(instead of an exhaustive search/optimization over the full planning/trajecotry space) can be seen as a simple form of motion
primitives or graph search methods and related to “sampling-based” planning approaches (Paden et al., 2016). Regarding the
other agents/perturbations, one could also just consider a small subset of possible behaviors, though we do not do this in the
current work since it weakens guarantees of course. This would be somewhat related to sampling-based methods in continuous
game theory (Adam et al., 2021) as well as empirical/oracle-based game theory (Lanctot et al., 2017).
29Published in Transactions on Machine Learning Research (11/2022)
D.2 Further details on Sec. 3.2
Here we go more into detail on the construction of safety layer that are diﬀerentiable and give us an overall
density, inparticular(1)thechallengesand(2)potentialexistingtoolstobuildon. (SeealsoAppendixD.3.2.)
One purpose of this section is to provide further relevant background for future work on safety layers that
allow to have closed-form density/gradient – given this is a rather new yet highly non-trivial topic (from the
mathematics as well as tractability side).
D.2.1 Deﬁnition and remark
Let us introduce the topological notion of “connected” we use below.
Roughly, an open set in Rnis called (path-) connected , if any two of its elements can be connected by
a continuous path within the set; and simply connected if these connecting paths are, up to continuous
deformations within the set, unique.
For formal deﬁnitions we refer to textbooks, such as (Komornik, 2017).
Remark on Def. 1: Usually, parts Akwill be topologically simply connected sets.
D.2.2 Basic remarks on simplest safety layer approaches
Let us elaborate on the issue of designing safety layers, mappings A→¯A, in particular ones that are
diﬀerentiable and give us an overall density.
Besides the more advanced approaches we discuss in the main text and below, let us also mention the two
simplest ones and their limitations.
First, maybe the simplest safetly layer is to map all unsafe actions, i.e., A\¯A, toonesafe action, i.e., one
point. But this would lead to stronger biases and we would lose the property of having an overall (Lebesgue)
density (at least the change of variables would not be applicable due to non-injectivity; and the policy
gradient theorem would also be problematic).
Second, another simple solutions would be move/scale the whole Aintoone safe open set , given the safe set
contains at least one open set (which should generally be the case, e.g., using Prop. 1). This in fact would be
a diﬀeomorphism so we could apply the change-of-variables formula. However, obviously this would be very
rigid: the possible safe actions to take would then usually be a very small part of the full safe set – a strong
and usually poor bias. This is because, be aware that, also the actually safe parts of the action space would
be re-mapped to the tiny safe open set. That is, it would force us to also move actions that are actually safe
to new positions. And this would leave little space of designing more sensible inductive biases, as in the two
instances we propose.
D.2.3 Simple versatility of piecewise diﬀeomorphism safety layers
In contrast to these overly biased simplest solutions from Appendix D.2.2, let us give an example of how
piecewise diﬀeomorphisms give us more room for design. Note that he following is not meant as a safetly
layer that should actually be used, but rather as a simple argument to show the universality:
Remark 5. Piecewise diﬀeomorphic safety layers are versatile: If a given safe set ¯Acontains at least some
area (open set), then we can usually map Ainto ¯A, simply by moving and scaling the unsafe areas into safe
ones, while leaving the safe actions where they are.
Elaboration of Rem. 5. Assume the safe set ¯Ais given as the subzero set of some continuous function h:
A→R, i.e., ¯A=h−1((−∞,0]), which is the case, e.g., under Prop. 1, and that it contains at least some
open set. Let Bbe some (any) ball in this open set.
30Published in Transactions on Machine Learning Research (11/2022)
Here is the simplest construction: Just take A1:=h−1((−∞,0])andA2:=h−1((0,∞)). Letg1(onA1) be
the identity. And let g2(onA2) be a shrinking, such that A2has less diameter than B, and a subsequent
translation of A2intoB.
Another construction, which has a better “topological” bias in that it tries to map unsafe parts into nearby
safe parts works as follows:
If¯Ais the subzero set of a continuous function, then the unsafe set is open. So it can be written as the union
of its connected components, and each connected component is also open. Therefore, in each connected
component k, there is at least one rational number (or, in higher dimensions, the analogous element of Qn),
so there are at most countably many of them.
So deﬁnegkwith domain the component k, and simply being a translation and scaling that moves this
component kinto anearbyopen part of the safe set (this is where it becomes more appealing in terms
“topological” bias), or, as a default, into B.
Letgbe deﬁned by the “union” of these gk, and on the rest of A, i.e., on the safe set ¯A, let it simply be the
identity.
D.2.4 On general existence, construction/biases and tractability of closed-form-density-preserving
diﬀerentiable safety layer
Here we add some notes on the general discussion of safety layers with closed-form densities, beyond the
particular approach we take in this paper.
There could be many ways to construct safety layers with closed-form density, and it is hard, especially
within the scope of just this paper, to get general answers on questions of (1) possibility and (2) tractable
construction of such safety layers. Especially since there may be trivial solutions (see Appendix D.2.2) which
are not satisfactory in terms of rigidity/bias though. So let us narrow down the problem to try to make at
least some basic assertions.
First, asimple example of impossibility when using pure diﬀeomorphisms : already for topological reasons, if
Ais connected but ˜Anot, then no such safety layer can exist that is bijective onto ˜A.
Nonetheless, overall the starting point in this paper is the change of variables, or the piecewise change of
variables we use. This implies that we extensively need diﬀeomorphisms to build our safety layer on. And
this implies that we need homeomorphisms, when relaxing the diﬀerentiability to continuity.
Regarding the shapes of (un-)safe sets, or their (inner/outer) approximations, general answers would be
desirable too, but particularly here we focus on polytopes and hyperrectangles, since for those are also
typical results of reachability analysis etc. (Sidrane et al., 2022).
Here are some basic concepts and results from topology and related areas. We only give an idea, while
detailed formalities are beyond the scope of this paper.
•InR2there is a lot of relevant theory from complex analysis, which studies holomorphic, i.e.,
complex diﬀerentiable functions (another way to state it is that they are conformal, i.e., locally
angle-preserving mappings; for detailed formalities see, e.g., (Luteberget, 2010)). Biholomorphic
(i.e., a holomorphic bijection whose inverse is also holomorphic) implies diﬀeomorphic, so this is
relevant for us.
For instance, the Riemann mapping theorem (Luteberget, 2010) tells us that such a biholomorphic
mapping exists from A⊂R2to¯A,if¯Ais a simply connected open set in R2(assuming per se that
Ais a box, i.e., simply connected as well). While this could be a powerful result for safety layers,
theissue comes with tractability . Often, Riemann mappings are deﬁned just implicitly, based on
some variational problem, or some diﬀerential equation problem, whose solution can be arbitrarily
complex to calculate. More tractable (though it still remains unclear how to make them fully
tractable) concepts and results (Luteberget, 2010) include:
31Published in Transactions on Machine Learning Research (11/2022)
–Schwarz-Christoﬀel mappings – a special case for polytope domains;
–sphere packing-based algorithms for approximating Riemann mappings via combinatorical so-
lutions on a ﬁnite graph of the problem, and then extrapolation to the continuous original
problem.
–Note that it is easy to see (since holomorphic functions are globally uniquely determined from
any local neighborhood, based on them being fully described by their Taylor expansion around
any single point) that holomorphic functions are a quite rigid function class for safety layer
though: for instance, there cannot be a holomorphic function which would be the identity on
the safe set, but the non-indentity outside the safe set.
•Adding on the negative, i.e., non-existence side, often the non-existence of safety layers or parts
of them can be proved using topological arguments (often invariants), e.g., that the domain and
co-domain do have to have the same connectivity property (connected, simply conntected, etc.), or
other based on homology.
•There are also positive, i.e., existence results, from topology, and very general ones. For instance, if
twoopensubsetsof Rnarecontractibleandsimplyconnectedatinﬁnity, thentheyarehomeomorphic
(i.e., continuous bijections whose inverse is also continuous) to each other.22This, even though not
suﬃcient, is a strong necessary requirement for being diﬀeomorphic of course. Also convexity can
be helpful.23
D.3 Details on FAGIL method Sec. 4, in particular its safety layers
D.3.1 Regarding safe set inference module
Note that, in principle, due to modularity, many safety approaches can be plugged as safety modules into our
overall method (Fig. 1). This includes frameworks based on set ﬁxed points (Vidal et al., 2000), Lyapunov
functions (Donti et al., 2021a), Hamilton-Jacobi type equations (continuous time) (Bansal et al., 2017), or
Responsibility-Sensitive Safety (RSS) Shalev-Shwartz et al. (2017).
A fundamental problem in safe control is the trade-oﬀ between being safe (conservative) versus allowing for
enough freedom to be able to “move at all” (actionability). Ours and many safe control frameworks build on
more conservative adversarial/worst-case reasoning. RSS’s idea is to prove safety under the less conservative
(i.e., “less worst-case”) assumption of others sticking to (traﬃc) norms/conventions , and if collisions happen
nonetheless, then at least the ego is “not to blame”.
D.3.2 On the safety layers - an alternative, pre-safe-probability-based safety layer
Besides the safety layer instance we described in Sec. 4.1, which can be termed distance-based , here is another
pre-safe-probability-based instance of a piecewise diﬀeomorphism safety layer g:
(a) On all safe boxes k, i.e.,Ak⊂¯A,gkis the identity, i.e., we just keep the pre-safe proposal ˆa. (b) On
all unsafe boxes k,gkis the translation/scaling to the box where the pre-safe density pˆahas the most mass
(approximately). Alternatively, in (b) a probabilistic version (b’) can be used where gktranslates to safe
partA/lscriptwith (approximate) probability proportional to pˆaonA/lscript.
Intuitively, the version with (b’) amounts to a conditioning on the safe action set , i.e., a renormalization
(when letting the box size go to zero).
Remark: Note that in this work we only considered these two speciﬁc instances of piecewise diﬀeomorphism
safety layers – the pre-safe-probability-based and the distance-based one – while many others are imaginable.
22Seehttps://math.stackexchange.com/q/55114/1060605 .
23See https://math.stackexchange.com/q/165629/1060605 and http://relaunch.hcm.uni-bonn.de/fileadmin/geschke/
papers/ConvexOpen.pdf .
32Published in Transactions on Machine Learning Research (11/2022)
D.3.3 Illustrative examples of piecewise diﬀeomorphisms for safety layers
Let us give some illustrative examples of piecewise diﬀeomorphism safety layers, see Fig. 5. This includes an
example of our distance-based, grid-based safety layer described in Sec. 4. (The probability-based, grid-based
version from Appendix D.3.2 works in a related way, but choosing the targets A/lscriptbased on their pre-safe
probability, instead of their distance to the sources Ak.)
Note that in the grid-based example, here we just need translations, because all parts (rectangles) of the
partition are of the same size, while in other cases, some parts (e.g., if there is a boundary not in line with
the grid) may have diﬀerent sizes, and then we also need scaling.
D.3.4 Computational complexity of the safety layer
We here give a rough analysis of the computation complexity of piecewise diﬀeomorphisms (Sec. 3.2) and
thus the safety layer, as part of the fail-safe imitator policy πI,θ(¯a|s)(Sec. 4), and their gradients. Let us
look at Eq. (6) and recall the structure of the fail-safe imitator with safe action and pre-safe action from
Fig. 1.
To calculate safe density p¯a(¯a)of a safe action ¯a, given the pre-safe density pˆaof the pre-safe policy, this is a
sum (Eq. (6)) over the relevant parts Akof the partition (Ak)k; and then each k-th summand is the change-
of-variables formula applied to the diﬀeomorphism gkand pre-safe density pˆa, where the determination of
gkmay itself require some calculation (based, e.g., on distances or pre-safe densities of the parts).
So, essentially, the complexity is at most the number of parts in the partition (Ak)ktimes the complexity of
calculating the change-of-variables formula (and determination of gkif necessary, see instances below).
Let us give some instances of this:
•When using any sort of general invertible (normalizing ﬂow) neural net (Papamakarios et al., 2019)
as transformations gk, then we inherit their complexity for each summand of Eq. (6).
•For our proposed safety layers, e.g., the grid- and distance-based one we describe in Appendix D.3
the number of parts in the partition is around 100 (based on a 10 times 10 grid). And each gkis a
translation/scaling to a target part, yielding a simple scaling factor as change-of-variables formula
(the determination of the target part, i.e., of gk, requires, for each state, one single run over all pairs
of parts in the partition to get the distances; on this side, the probability-based safety layer version
we describe in Sec. 4 is faster, since we do not have to go over all pairs, just all single ones).
To calculate the gradient ofπI,θ(¯a|s)w.r.t.θ, this is analogous, where we simply can drag the gradient into
the sum of Eq. (6), and the gradients of the summands are given by the gradients of the pre-safe policy that
is being used (i.e., for instance conditional Gaussian or normalizing ﬂow), times the factor |det(Jg−1
k(ˆa))|
which is constant w.r.t. θ(in the our distance-based safety layer). So here the complexity is essentially
number of (at most) all parts in the partition (Ak)k, times complexity of gradient calculation for pre-safe
policy.
D.3.5 Invariant safety on full sets, not just singletons
Recall that for the general method we used singleton safe sets as backup to guarantee invariant safety (end
of Sec. 4.1).
It is important to emphasize though that in stable linear systems it can in fact often be guaranteed that
always a full safe box ¯Akis found. And then the extension of memorizing a safe fallback singleton is not
necessary.
The idea towards proving this is as follows: The hard part is that we have to check safe future trajecotries
not just for individual trajectories, but where at each stage, any action out of thes safe setcan be taken.
But these sets can be modelled as bounded perturbations of individual actions. Then stability arguments can
show us that the error (deviation from a single safe trajectory over time) can be bounded, and then we can
recur to planning individual trajectories plus simple margins.
33Published in Transactions on Machine Learning Research (11/2022)
Setup:Assume the full action set Ais given by the
set enclosed by the gray frame. ...
... And the unsafe actions A\¯Aare given by the red
subset of the full action set.
First example piecewise diﬀeomorphism: The
partition (Ak)khas just two parts (l.h.s.): A1is
the safe set enclosed by the green line, and A2the
unsafe set, encircled by the blue line. The piecewise
diﬀeomorphism gtransforms these parts as follows:
.../mapsto→
... The diﬀeomorphism g1onA1is just the identity.
And the diﬀeomorphism g2maps the unsafe A2
to the blue polygon at the boundary within the
safe set (r.h.s.). Note that such a diﬀeomorphism
exists by the Riemann (or Schwarz-Christoﬀel)
mapping theorem, or, in this simple case, also
by convexity (see our discussion of all these
concepts in Appendix D.2.4). Together, g1andg2
form our piecewise diﬀeomorphism gonA=A1∪A2.
Second example piecewise diﬀeomorphism:
Here, we consider our grid-based partition of the full
action set, i.e., a complete regular tiling of the action
set with rectangles Ak, as partition (Ak)k. We only
illustratedtheindividualrectangles Akfortheunsafe
area, in blue, while for the safe area we only drew the
outer boundary, in green (l.h.s.). The piecewise dif-
feomorphism gtransforms these parts as follows: .../mapsto→
... On all safe Ak, the diﬀeomorphism gkis simply
the identity. And on all unsafe Ak(meaning fully or
partiallyunsafe), gkistheshiftingtothemostnearby
saferectangle A/lscript. Thosesafetargetrectangles A/lscriptare
drawn in blue (r.h.s.). Together, these gkform our
piecewise diﬀeomorphism gon the full A. Note that
this is an example of our distance-based safety layer
described in Appendix D.3.2.
Figure 5: Examples of piecewise diﬀeomorphism safety layers. Top row: setup. Middle and bottom row:
two examples.
34Published in Transactions on Machine Learning Research (11/2022)
D.4 Further related work and remarks
Beyond the closest related work discussed in Sec. 1, let us here discuss some more broadly related work.
Boborzi et al. (2021) similar to us propose a safe version of GAIL, but there are several diﬀerences: they
build on a diﬀerent safety reasoning (RSS), which is speciﬁc for the scope of driving tasks, while our full
method with its worst-case safety reasoning is applicable to general IL domains, and they do not provide
exact densities/gradients of the full safe policy as we do.
Certain safety layers (Donti et al., 2021a; Dalal et al., 2018) are a special case of so-called implicit layers
(Amos and Kolter, 2017; Amos et al., 2018; El Ghaoui et al., 2019; Bai et al., 2019; Ling et al., 2018; 2019),
which have recently been applied also for game-theoretic multi-agent trajectory model learning (Geiger and
Straehle, 2021), including highway driver modeling; robust learning updates in deep RL (Otto et al., 2021);
and learninginterventionsthat optimize certain equilibria in complexsystems Besserve and Schölkopf (2021).
Worth mentioning is also work on safe IL that builds on hierarchical/hybrid approaches that combine imi-
tation learning with classic safe planning/trajectory tracking (Chen et al., 2019; Huang et al., 2019).
Besides GAIL, which only requires samples of rollouts, one could also think about using fully known and
diﬀerentiable dynamics (Baram et al., 2017) for the training of our method.
Also note reachability-based safe learning with Gaussian processes (Akametalu et al., 2014).
At the intersection of safety/reachability and learning, also work on outer approximations based on veriﬁed
neural net function class properties is connected Sidrane et al. (2022).
Beyond the paper mentioned in the main text, there are several papers on fail-safe planning for autonomous
driving (Magdici and Althoﬀ, 2016; Pek and Althoﬀ, 2018; 2020).
Convex sets, various kinds of them, have proved a helpful concept in reachability analysis and calculation,
see., e.g, (Guernic and Girard, 2009) (their Section 1 gives an overview).
Remark 6 (On the meaning of the term “fail-safe”) .The term “fail-safe” seems to be used in varying
ways in the literature. In this work, in line with common uses, we have the following intuitive meaning in
mind: We have an imitation learning component and a worst-case planning component in our system (the
fail-safe imitator). The pure IL component – the pre-safe policy – can in principle always failin the sense
of leading to “failure”, i.e., momentarily unsafe/collision, states, since it does not have strong guarantees.24
Speciﬁcally, this would happen when it would put all mass on an action that leads to a collision.25But having
ourworst-case fallback planner plus the safety layer as additional component in our system, we make sure
that, even if the IL component fails, nonetheless there exists at least one safe action plus future trajectory.
This makes the system safe against a failure of the pure IL component , and this is what we mean by fail-safe.
This is similar to (Magdici and Althoﬀ, 2016) who use “fail-safe” in the sense that if other agents behave
other than their “normal” prediction module predicts (i.e., the prediction module fails in this sense), then
there is nonetheless a safe fallback plan. All this also relates to notions of redundancy .
24There can also be some confusion because “fail” can be used in two senses: in the sense of a restricted failure, that can be
mitigated and still be safe though; or in the sense that “failure” means “collision” or “momentarily unsafe” state.
25Note that for Gauss as well as ﬂow policies, we have the conceptual issue here that they have mass everywhere; and in this
probabilistic setting it is hard to say what precisely “fail” would be; however, one can as well plug in pre-safe policies into our
approach that do not have support everywhere, and then the aforementioned holds rigorously.
35