Under review as submission to TMLR
Gaussian process surrogate models for neural networks
Anonymous authors
Paper under double-blind review
Abstract
Thelackof insightintodeep learning systems hinders theirsystematic design. In science and
engineering, modeling is a methodology used to understand complex systems whose internal
processes are opaque. Modeling replaces a complex system with a simpler surrogate that
is more amenable to interpretation. Drawing inspiration from this, we construct a class of
surrogate models for neural networks using Gaussian processes. Rather than deriving the
kernels for certain limiting cases of neural networks, we learn the kernels of the Gaussian
process empirically from the naturalistic behavior of neural networks. We ﬁrst evaluate our
approach with two case studies inspired by previous theoretical studies of neural network
behavior in which we capture neural network preferences for learning low frequencies and
identify pathological behavior in deep neural networks. In two further practical case studies,
we use the learned kernel to predict the generalization properties of neural networks.
1 Introduction
Deep learning systems are ubiquitous in machine learning but sometimes exhibit unpredictable and often
undesirable behavior when deployed in real-world applications (Geirhos, Jacobsen, et al. 2020; D’Amour et
al. 2020). This gap between idealized and real-world performance stems from a lack of principles guiding
the design of deep learning systems. Instead, deep learning practitioners often rely upon a set of heuristic
design decisions that are inadequately tied to a system’s behavior (Dehghani et al. 2021), driving calls for
explainability, transparency, andinterpretabilityofdeeplearningsystems(Lipton2016; Doshi-VelezandKim
2017; Samek et al. 2017) especially as these systems are more widely applied in everyday life (Bommasani
et al. 2021).
Machine learning is not unique in seeking to understand a complex system whose inputs and outputs are
observable but whose internal processes are opaque—this challenge occurs across the empirical sciences and
engineering. An explanatory tool that is foundational across these disciplines is that of modeling , that is,
representing a complex and opaque system with a simpler one that is more amenable to interpretation.1
Modelingmakesprecise assumptions about how a system may operate while abstractingaway details thatare
irrelevant for a given level of understanding or a given downstream use case. These properties are valuable
for a framework for understanding deep learning as they are in other scientiﬁc and engineering disciplines.
As the popularity of deep learning has grown, a number of proposals have been made for modeling these
systems. Numerous mathematical models of deep learning have been developed (Roberts et al. 2022), and
some surprising phenomena, such as adversarial examples (Szegedy et al. 2014), have been captured with
a mathematical analysis (Ilyas, Santurkar, et al. 2019). However, existing mathematical models, which are
limited to well-understood mathematical tools, are unable to capture the properties of machine learning
systems as applied in practice (Nakkiran 2021). Beyond mathematical models, localized models have aimed
to explain the predictions of machine learning systems on a per-example basis (Ribeiro et al. 2016; Koh and
P. Liang 2017; Zhou et al. 2022; Ilyas, Park, et al. 2022), but these approaches are, by construction, only
partial explanations of the behavior of the end-to-end system.
What might an alternative modeling approach—one that captures salient aspects of applied systems in
a global fashion—look like? We appeal to two domains for inspiration. In engineering design, surrogate
1Though some architectural components of a deep learning system are commonly referred to as a model—as in “neural
network model”—we use modeling to refer to the methodology of idealizing a complex system as a simpler one.
1Under review as submission to TMLR
models(G. G. Wang and Shan 2006) emulate the input-output behavior of a complex physical system,
allowing practitioners to simulate eﬀects that are consequential for design or analysis without relying on
costly or otherwise prohibitive queries from the system itself. In cognitive science, cognitive models (Sun
2008; McClelland 2009) describe how unobservable mental processes such as memory or attention produce
the range of people’s observed behaviors. Both domains abstract away internal details, such as real-world
constraints on a physical system or neural circuitry in the brain, instead treating the target process or system
as ablack box . At the same time, both surrogate and cognitive models are constructed to replicate the end-
to-end behavior of the target system and thus are complete where localized explanations are not.
We explore an analogous approach to investigate deep learning systems by constructing surrogate models for
neural networks . We ﬁrst must choose an appropriate family of surrogate models. Gaussian processes (GPs)
are a natural choice, with appealing theoretical properties speciﬁc to the study of neural networks (NNs);
namely, certain limiting cases of NN architectures are realizable as GPs (Neal 1996; Li and Y. Liang 2018;
Jacot et al. 2018; Allen-Zhu et al. 2019; Du et al. 2019). However, in contrast to these analytic approaches,
we aim to explore the scientiﬁc and practical utility of idealizing NNs with GPs using a data-driven approach
to estimating the kernel functions. Separately, the learned kernel of a GP is often interpretable (Wilson
and Adams 2013); we use this fact to study the prior over functions represented by a GP that accounts for
observed neural network behavior in less-restricted settings. With this approach, we capture a number of
known phenomena, including a bias towards low frequencies and pathological behavior at initialization, in
a cohesive framework. Finally, we demonstrate the practical beneﬁts of this framework by predicting the
generalization behavior of models in an NN family.
2 Background
Insurrogate modeling , we approximate a complex black-box function with a simpler surrogate model that
ismoreamenabletointerpretation. Surrogatemodelshavemanyapplications: Inoptimization,theyareoften
used to approximate queries from expensive-to-evaluate functions (Snoek et al. 2012; Shahriari et al. 2016;
Xue et al. 2020); in other applications, surrogate models have been used to gain insight into large physical
systems, such as the global ﬂuxes of energy and heat over the earth’s surface (Camps-Valls et al. 2015).
Cognitive models have been used by cognitive scientists since the 1950s to gain insight into another black
box—the human mind (Newell et al. 1958). Bayesian models of cognition, in particular, oﬀer a way to
describe the inductive biases of learning systems in the form of a prior distribution (Griﬃths et al. 2010). As
deep NNs have become more prevalent in machine learning, researchers have started to use methodologies
from cognitive science to interrogate otherwise opaque models (Ritter et al. 2017; Geirhos, Rubisch, et al.
2019; Hawkins et al. 2020). The success of these eﬀorts suggests that other methods from cognitive science—
namely, cognitive modeling—may be applicable to machine learning systems.
Gaussian processes (GPs; Carl E. Rasmussen and Williams 2006) are probabilistic models that specify
a distribution over functions. A GP models any ﬁniteset ofNobservations as a multivariate Gaussian
distribution on RD, where the nth point is interpreted as the function value, f(xn), at the input point xn.
GPs are fully characterized by a mean function m(x), usually taken to be degenerate as m(x) =0,∀x, and
a positive-deﬁnite kernel function k(x,x/prime)that gives the covariance between f(x)andf(x/prime)as a function
ofxandx/prime.
Formally, let Xbe a matrix of inputs and ybe a vector of output responses. Due to the marginalization
properties of the Gaussian distribution, the posterior predictive distribution of a GP for a new input x∗,
conditioned on dataset D={X,y}and assuming centered Gaussian observation noise with variance σ2, is
Gaussian with closed-form expressions for the mean and variance:
E[f(x∗)|D] =m(x∗) +k∗T(K+σ2I)−1(y−m(x∗)) (1)
V[f(x∗)|D] =k(x∗,x∗)−k∗T(K+σ2I)−1k∗ (2)
where Kis theN×NGram matrix of pairwise covariances, k(xi,xj), and k∗= [k(x1,x∗),...,k (xN,x∗)]T.
The kernel function kspeciﬁes the prior on what kind of functions might be represented in observed data; for
example, it can express expectations about smoothness or periodicity. Parametric kernels have hyperparam-
2Under review as submission to TMLR
X1→
→g1(X1)
...
XS→
→gR(XS)
Step 1: Collect predictions across models grand
across target functions (datasets) Xs.
θ*Step 2: Fit Gaussian pro-
cess hyperparameters θto
the aggregate predictions via
Objective ( 6).
−1 0 1
Input X−2−1012Output YNN predictions
0 2 4 6
Distance t0.00.20.40.60.8CovarianceLearned kernelStep 3: Analyze the kernel learned from
the aggregate predictions. Here, the
learnedkernelrevealsthequicklyvarying
behavior of particular neural networks.
Figure 1: Outline of the surrogate modeling approach. We learn a Gaussian process surrogate model for a neural
network family applied to a task family by learning kernel hyperparameters from aggregated neural network predictions across
datasets. We interpret the learned kernel to derive insights into the properties of the neural network family; for example, biases
towards particular frequencies (see Section ( 4.1)), or expected generalization behavior on a new dataset (see Section ( 4.4)).
etersθthat aﬀect this prior and thus the posterior predictive. These kernel hyperparameters can be adapted
tothepropertiesofadataset, thusdeﬁningaprioroverfunctionsthatisappropriateforthatcontext. GPker-
nel hyperparameters are typically learned via gradient-based optimization to maximize the GP marginal like-
lihood,p(y|X). Again due to properties of the GP, this marginal likelihood has the closed-form expression:
logp(y|X) =−1
2yT/parenleftbig
Kθ+σ2
nI/parenrightbig−1y−1
2log|Kθ+σ2
nI|−n
2log 2π , (3)
We write the Gram matrix as Kθto indicate that it depends on kernel hyperparameters via a particular
parameterization. In this work, we make use of two kernel parameterizations: the Matérn kernel (MK;
Matérn 1960) and the spectral mixture kernel (SMK; Wilson and Adams 2013). Speciﬁcally, following
Snoek et al. (2012), we use the automatic relevance determination (ARD) 5/2MK, given by:
k(x,x/prime) =θ0/parenleftBig
1 +/radicalbig
5r2(x,x/prime) +5
3r2(x,x/prime)/parenrightBig
exp/braceleftBig
−5/radicalbig
5r2(x,x/prime)/bracerightBig
r2(x,x/prime) =D/summationdisplay
d=1(xd−x/prime
d)2/θ2
d,(4)
where each θdis the lengthscale parameter for dimension d, which captures how smoothly the function varies
along that dimension. The SMK is derived by modeling the spectral density of a kernel as a scale-location
mixture of Gaussians and computing the Fourier transform of the mixture (Wilson and Adams 2013), giving:
k(τ) =Q/summationdisplay
q=1wqcos/parenleftbig
2π2τTµq/parenrightbigP/productdisplay
p=1exp/braceleftBig
−2π2τ2
pv(p)
q/bracerightBig
. (5)
Here,k(τ)gives the covariance between function values f(x)andf(x/prime)whose corresponding input values x
andx/primeare a distance τapart. For a Q-component spectral mixture, w={wi}Q
i=1are scalar mixture weights,
andµi∈RPandvi∈RPare component-wise Gaussian means and variances, respectively. Appendix ( A)
details how the hyperparameters of the MK and the SMK control the respective priors on functions.
3 Learning a Gaussian process surrogate model from neural network predictions
In this section, we detail the goals and approach of the surrogate modeling framework. In brief, our approach
involves collecting neural network predictions across a set of neural network models and across a set of
datasets, and estimating GP kernel hyperparameters from these predictions by maximizing the marginal
likelihood across model-and-dataset pairs; see Fig. ( 1) for a schematic.
3.1 Formal framework
Our goal is to capture shared properties among a family of neural networks models Fas applied to a
family of datasets D. Here, a model family Fis a set of neural networks {g0,...,gR}that share in design
3Under review as submission to TMLR
choices ( e.g.,architecture, training procedure, random initialization scheme) but diﬀer in quantities that
are randomized prior to or during training ( e.g.,parameter initializations).2Similarly, a dataset family D
is a set of datasets {D0,...,DS}that share some underlying structure as in multi-task and meta-learning
settings (Caruana 1997; Hospedales et al. 2020). We consider supervised learning, in which each dataset
consists of inputs and targets, D= (X,y). Importantly, we ﬁt surrogate model parameters θto abehavioral
datasetof the model family evaluated on the dataset family, and not the ground truth datasets themselves.
Data. We construct a component of the surrogate model training dataset as follows: We sample a model
indexrand a dataset index s. The corresponding dataset is split into a training set and an evaluation set,
Ds=Dtrain
s∪Deval
s. Thecorrespondingmodel grisﬁtthetrainingset Dtrain
s= (Xtrain
s,ytrain
s)accordingtothe
trainingprocedurespeciﬁedbythechoiceofmodelfamily F, producing gﬁt
r. Wethencollectthepredictionsof
the trained model on the evaluation set, gﬁt
r(Xeval
s), to produce the component (Xeval
s,gﬁt
r(Xeval
s))consisting
of theground truth inputs paired with the neural network behavioral targets from the evaluation set. We
aggregate the ground truth inputs and the neural network behavioral targets across pairs to produce the
surrogate model training dataset ,/parenleftbig
(Xeval
s1,gﬁt
r1(Xeval
s1)),..., (Xeval
sT,gﬁt
rT(Xeval
sT))/parenrightbig
.
Algorithm 1: Training and evaluation of the
GP surrogate model described in Section ( 3).
hyperparameters: model family F,
dataset family D,
model-dataset count T,
GP parameterization θ
// Step 1 in Fig. ( 1)
fort∈1...Tdo
Sample a model, grt∼Unif(F)
Sample a dataset, Dst∼Unif(D)
Train the model, gﬁt
rt←train (grt,Dtrain
st)
Evaluate gﬁt
rt(Deval
st)
end
// Step 2 in Fig. ( 1)
Optimize Objective ( 6) forθ∗
// Step 3 in Fig. ( 1)
Analyzeθ∗viaPθ∗Surrogate model. We ﬁt the GP using type-II max-
imum likelihood estimation. Let Pθ(gﬁt(Xeval)|Xeval)
be the GP marginal likelihood of the dataset component/parenleftbig
Xeval,gﬁt(Xeval)/parenrightbig
under a GP with kernel hyperparam-
etersθ, as given in Eq. (3). We ﬁt the surrogate model
jointlyacrossmodel-and-taskpairsinthesurrogatemodel
training dataset by maximizing the joint marginal likeli-
hood with respect to θ:
max
θ/productdisplay
(r,s)Pθ(gﬁt
r(Xeval
s)|Xeval
s). (6)
By optimizing Objective ( 6), we encourage the kernel hy-
perparameters θto capture the implicit prior distribution
over functions induced by the models in the family Fas
applied to the datasets in the family D. Algorithm ( 1)
gives the complete surrogate model training and evalua-
tion process.
3.2 Why use (GP) surrogate models for NNs?
Byestimatingaprioroverfunctionsforaneuralnetworkfamilydirectlyfromneuralnetworkbehavior,weaim
to capture shared properties that determine the model family’s behavior on data, i.e.,the model family’s
inductive biases . There is strong evidence that the inductive biases of neural networks ( e.g.,invariances
and equivariances, Markovian assumptions, compositionality) and not just data, play an important role in
their performance (Poggio, Mhaskar, et al. 2017; Tiňo et al. 2004; Lin and Tegmark 2017; Fukushima 2004;
Werbos1988). Moreover, deepNNsarehighlyoverparametrizedmodelsthatcanneverthelessgeneralizewell,
prompting interest in implicit regularization mechanisms that bias NNs towards learning simpler solutions
(Soudry et al. 2018; Poggio, Kawaguchi, et al. 2018; Neyshabur et al. 2017). More broadly, the extrapolation
behavior of any learning machine is underdetermined by data alone and therefore depends on its inductive
biases (Mitchell 1980).
GPs, in particular, oﬀer several advantages as surrogate models of NNs. Firstly, GPs are ﬂexible models
that are also often interpretable in the sense that the learned hyperparameters can provide insights
into properties of the datasets on which they are trained (Wilson and Adams 2013). As an example, many
covariance functions have separate lengthscales for each input dimension. An inverse lengthscale captures
an input dimension’s “importance;” in Section ( 4.4), we demonstrate that we can use these lengthscales
2We consider both untrained and trained neural networks, where an untrained network is a special case of a trained network
with the number of training iterations at 0; we thus describe the framework only for trained networks.
4Under review as submission to TMLR
−202Output YReLU, Depth=16 ReLU, Depth=32
−1 0 1
Input X−202Output Ysin, Depth=16
−1 0 1
Input Xsin, Depth=32NN samples
−202Output YReLU, Depth=16 ReLU, Depth=32
−1 0 1
Input X−202Output Ysin, Depth=16
−1 0 1
Input Xsin, Depth=32GP samples
Figure 2: Demonstration: Comparing learned GP priors with NN priors. Samples from GP prior ( right) with kernel
hyperparameters inferred from the predictions of NN families ( left). GPs are ﬂexible enough to capture properties of each NN
family; for example, the samples from the learned GP prior reﬂect the quickly varying behavior of the 32-layer sinusoidal NNs
and the increasing-decreasing behavior of rectiﬁer NNs.
to predict generalization behavior, suggesting that the GP surrogate representation is practically useful in
automating model selection.
Secondly, the use of GP surrogate models is also motivated by the theoretical connections between
GPs and NNs. Neal (1996) showed that a prior over the parameters of certain single-layer multi-layer
perceptrons (MLPs) converges to a GP as the MLP’s width approaches inﬁnity, and recent works (Lee et al.
2017; Matthews et al. 2018; Novak, Xiao, Bahri, et al. 2019; Garriga-Alonso et al. 2019; Yang 2019) have
extended this correspondence to deep MLPs and more modern NN architectures. Connections between GPs
and NNs can provide insight because they transform the priors implicit in NNs designs into explicit priors
expressed through a GP. However, our strategy to derive such a connection diﬀers from this prior theoretical
work that derives analytic kernels for limiting cases of NNs—we take an empirical approach by learning GP
kernels directly from the predictions of arbitrary classes of ﬁnite NNs.
Lastly,GPs have a tractable marginal likelihood. Probabilistic models allow us to express inductive
biases in the form of an explicit prior distribution, but the marginal likelihood is intractable for most
complicated Bayesian models. In contrast, for GPs, the marginal likelihood has a closed form expression,
which means that we can optimize it directly instead of resorting to approximations.
3.3 Demonstration: Comparing learned GP priors with NN priors
We brieﬂy demonstrate the surrogate modeling framework of Section ( 3.1). As a simple sanity check, we
verify that GP surrogates learned from varying NN families exhibit meaningful variation in behavior. To do
this, we learn GP priors from varying NN families and compare the learned priors with the NN families.
NN hyperparameters. We consider ensembles of 50 randomly initialized NNs with rectiﬁed linear unit
(ReLU) or sine ( sin) activations and 16 or 32 hidden layers of 128 hidden units each. We randomly initialize
the weights about zero with weight variance σ2
w= 1.5and bias variance σ2
b= 0.05.
GP surrogate. For each ensemble, we learn the hyperparameters of a randomly initialized SMK with
Q= 10mixture components by optimizing Objective ( 6) for 350 iterations with batch gradient descent and
the adaptive momentum (Adam) optimizer (Kingma and Ba 2015) with a learning rate η= 0.1. We choose
the kernel hyperparameters with the highest objective value across three random initializations.
Results. We plot NN predictions and samples from the learned GP priors in Fig. ( 2). The learned
GP captures the periodicity of the sinusoidal neural networks (sinusoidal NNs), and partially captures the
increasing-decreasing behavior of rectiﬁer neural networks (rectiﬁer NNs) about a cusp; though, due to the
5Under review as submission to TMLR
SMK parameterization, it cannot capture the discontinuity at the cusp. The GP also captures diﬀerences
in depths for the sinusoidal NNs: The GP prior samples for the 32-layer networks are quickly varying,
indicating shorter lengthscales have been learned. Taken together, the results of this demonstration show
that GP surrogates can capture certain NN behavior.
4 Experiments
WeprovideaseriesofdemonstrationsofthevalueoftheapproachofSection( 3). Eachexperimentaimstoin-
vestigate the properties of one or more neural network families, speciﬁed by neural network (NN) hyper-
parameters , asevaluatedononeormoredatasetfamilies, parameterizedas targetfunctions , byanalyzing
thecorresponding Gaussianprocess(GP)surrogatemodel . InSections( 4.1)and(4.2), wecapturepre-
viously established NN phenomena, while in Sections ( 4.3) and (4.4), we predict NN generalization behavior.
4.1 Reproduction: Capturing spectral bias in NNs
Rahaman et al. (2019) demonstrated that deep rectiﬁer NNs exhibit spectral bias , the preference to learn
lower frequencies in the target function before higher frequencies. To demonstrate this, the authors studied
the Fourier spectrum of rectiﬁer NNs ﬁt to a sum of sinusoidal functions of varying frequencies. In this
section, we take an alternative approach: We learn kernels from NN predictions at various stages of training
and demonstrate that the evolution of these learned kernels captures the spectral bias.
NN hyperparameters. As in Rahaman et al. (2019), we train an NN with 6 hidden layers of 256 units
and ReLU activations using full-batch gradient descent with Adam and a learning rate of η= 3×10−4.
Target function. The target functions are sums of sine functions with frequencies in (5,10,..., 45,50)and
phasesdrawnfrom U(0,2π), evaluatedat 200pointsevenlyspacedbetween [0,1], asinRahamanetal.(2019).
GP surrogate. We learn the parameters of a spectral mixture kernel (SMK) with Q= 10mixture com-
ponents by optimizing Objective ( 6) with Adam for 350iterations with a learning rate of η= 0.1. Since
the marginal likelihood of the SMK is multi-modal in its frequency parameters, we repeat this optimization
for three diﬀerent random initializations of the kernel parameters and choose the hyperparameters with the
largest marginal likelihood value (the value of Objective ( 6)). We randomly initialize the length-scales vi
by sampling from a truncated normal distribution whose variance depends on the maximum distance be-
tween input points. We set the signal variances wto the variance of the target function values divided by
the number of mixture components. The frequency hyperparameters of the SMK are sometimes initialized
by sampling from a uniform distribution whose upper limit is the Nyquist frequency (Wilson and Adams
2013); since this target function’s largest frequency is smaller than the Nyquist frequency, we instead set a
smaller frequency as the upper limit.
Results. Fig. (3) displays the NN predictions and the kernel of the corresponding GP surrogate at diﬀerent
iterations of NN training. The kernel function, which is given in Eq. (5), reﬂects how the similarity between
function values varies with the distance between their input points.3The structure of the learned kernel
reﬂects the properties of the NN family: Initially, the learned kernel only captures low frequencies in the
NN’s predictions—reﬂected in the long period of the kernel—consistent with the spectral bias of Rahaman
et al. (2019). However, as training progresses, the periodicity of the learned kernel reﬂects both low and
high frequencies.
4.2 Reproduction: Depth pathologies in randomly initialized NNs
Hyperparameter selection in NNs is not always theoretically grounded. Many recent studies thus character-
ize how diﬀerent hyperparameter choices ( e.g.,depth, width) aﬀect the properties of NNs at random initial-
ization (Schoenholz et al. 2017; Yang 2019; Xiao et al. 2018). Towards that end, recent work showed that
3Since the SMK is a stationary covariance function, we graph against the distance between input points rather than the
absolute value of the input points themselves.
6Under review as submission to TMLR
−1 0 1−3−113Output YIteration 100
Neural Network
Data
−1 0 1
Input XIteration 1000
−1 0 1Iteration 14999
0 1 2−10123CovarianceLearned kernel
0 1 2
Distance between points t0 1 2
Figure 3: Capturing spectral bias in neural networks. (Top) Neural network predictions as training progresses on the
sum-of-sines target function described in Section ( 4.1). (Bottom ) Spectral mixture kernel ﬁt to neural network predictions
as training progresses. The kernel reveals a spectral bias for this neural network family, with the range of spectral frequencies
expressed in the kernel increasing with the number of iterations of training.
−2.50.02.5Output Ysin, Depth=16 sin, Depth=32 sin, Depth=256 sin, Depth=512
−1 0 1
Input X−2.50.02.5Output YReLU, Depth=16
−1 0 1
Input XReLU, Depth=32
−1 0 1
Input XReLU, Depth=256
−1 0 1
Input XReLU, Depth=512
01Covariancesin, Depth=16
sin kernelsin, Depth=32 sin, Depth=256 sin, Depth=512
0 5
Distance t01CovarianceReLU, Depth=16
ReLU kernel
0 5
Distance tReLU, Depth=32
0 5
Distance tReLU, Depth=256
0 5
Distance tReLU, Depth=512
Figure4: Depthpathologiesinrandomlyinitializedneuralnetworks. Predictionsofneuralnetworks (left)fromneural
network families of diﬀerent activations (rows)and varying depths (columns) ; mean and standard error of the covariance of
the corresponding surrogate model kernels (right). The covariance is aggregated across 10 kernels learned from 10 diﬀerent
50-member neural network ensembles from a given family. Greater depth results in kernels with shorter lengthscales, with this
pathology emerging earlier in rectiﬁer NNs; this result is consistent with prior work on pathologies of deep neural networks.
increasing depth could actually induce pathologies in randomly initialized NNs (Labatie 2019; Duvenaud,
Rippel, et al. 2014). For example, Duvenaud, Rippel, et al. (2014) proved that increasing depth in a certain
class of inﬁnitely wide NNs produces functions with ill-behaved derivatives. As a result, these functions are
quickly varying in the input space.
We empirically study a similar pathology—quick variation in input space—that emerges in randomly initial-
ized, ﬁnite-width, ﬁnite-depth NNs. To do this, we ﬁt GP surrogates to randomly initialized NN ensembles
of varying depths and activation functions and inspect how the learned kernels change with depth. If NNs
exhibit this pathology, the learned covariance will decay sharply with distance.
NN hyperparameters. We consider families of NNs of varying activation functions ( sin (asin(bx+c))
and ReLU ( max(0,x))) and varying depths (from 16 to 512 layers). From each family, we sample an ensemble
of 50 randomly initialized NNs, each with 128 hidden units in each layer. We randomly initialize NN weights
about zero with weight variance σ2
w= 1.5and bias variance σ2
b= 0.05.
GP surrogate. We sample 10 ensembles of 50 randomly initialized NNs, and learn an SMK kernel by op-
timizing Objective ( 6) separately for each ensemble, running Adam (Kingma and Ba 2015) for 750 iterations
with a learning rate of η= 0.1. We choose the kernel hyperparameters with the highest mean marginal like-
lihood among three random initializations. To ensure our results are robust across random ensembles, we
7Under review as submission to TMLR
consider an averaged learned kernel: Suppose we have nkernels,k1(·),...,kn(·), learned from ndiﬀerent
ensembles from the same family. The average learned kernel, ¯k, is deﬁned as ¯k(τ) =1
n/summationtextN
i=1ki(τ).
Results. Fig. (4) plots the average learned kernels for NN families with varying activation functions and
depths, as well as the predictions of those NN families. Across both activation functions, the learned kernels
reveal a pathology: For large depths, the covariance (Fig. ( 4), right) sharply decays towards zero with
distance. The NN predictions (Fig. ( 4), left) explain this property of the learned kernels: At large depths, the
deep NNs vary quickly in the input domain, which causes the SMK to learn short lengthscales. Interestingly,
this pathology emerges at diﬀerent depths for diﬀerent activation functions: We see rectiﬁer NNs exhibit
this pathology with 256 layers while sinusoidal NNs exhibit this pathology with 512 layers.
4.3 Ranking NN generalization with the GP marginal likelihood
In previous sections, we demonstrated that GP surrogate models could yield insight into NN behavior. The
beneﬁtsofGPsextendbeyondthis. SincetheGPmarginallikelihoodhasaclosedformexpression,manyhave
advocated for using the marginal likelihood in model selection and as an indicator of expected generalization
performance (Mackay 1992). In this section, we leverage the learned GP surrogate to rank NNs by their
generalization error with the GP marginal likelihood . In particular, we learn GP surrogates from diﬀerent
NNs at random initialization, and we then study if the marginal likelihood of the surrogates can rank the NNs
by test error after training. In the following experiments with varying classes of NN families, we ﬁnd that we
can indeed predict test error using the marginal likelihood of the training set under the learned surrogate GP.
4.3.1 The idealized case: Large-width NNs
Before we consider arbitrary NN families, we check that the marginal likelihood is predictive in an idealized
setting. In particular, we consider large-width NNs whose inﬁnite-width analogs are equivalent to GPs (Lee
et al. 2017). If the marginal likelihood is not predictive in this case in which the kernel function can be
analytically determined, it is unlikely to be useful in a general setting where the kernel is learned and GPs
approximate NNs priors but are not equivalent.
NN hyperparameters. We consider NNs with sinor Gauss error function ( erf)4activations and 2 hidden
layers of 1024 units each. We randomly initialize the weights about zero with weight variance σ2
w= 1.5and
bias variance σ2
b= 0.05. We train an ensemble of 50 randomly initialized NNs from each family using full-
batch (vanilla) gradient descent with learning rates of η∈{0.01,0.1}.
0 200 400 600 800 1000
Iterations0.00.20.40.60.81.0Test errorh= 0.01
erf, MLL = -2924.45
sin, MLL = 146.01
0 200 400 600 800 1000
IterationsTest errorh= 0.1
erf, MLL = -2924.45
sin, MLL = 146.01
0 200 400 600 800 1000
Iterations0.00.20.40.60.81.0Test errorh= 0.01
erf, MLL = -2924.45
sin, MLL = 146.01
0 200 400 600 800 1000
IterationsTest errorh= 0.1
erf, MLL = -2924.45
sin, MLL = 146.01
Figure 5: Ranking generalization from MLL in large-
width NNs. Mean and standard error of the test MSE of large-
width sinusoidal and erfNNs trained with learning rates η=
0.01(left)andη= 0.1(right) on the target function of Sec-
tion (4.3.1). The MLL of the target function under the surrogate
model corresponding to the limiting kernel for each model family
is shown in the legend. Consistent with expectations, the model
family whose surrogate assigns higher MLL to the target function
achieves lower test error for both values of η.Target function. The target function is
sin(0.5x).
GP surrogate. We do not learn a kernel from
NN predictions as in previous sections. Instead,
we use the kernels corresponding to the inﬁ-
nite width analogs of the NNs using the neural-
tangentspackage(Novak, Xiao, Hron, etal.2020).
Results. Fig. (5) compares the performance of
these NN families along with the marginal like-
lihood of the target function under the surro-
gate model. The performance (mean-squared er-
ror (MSE) on the test set) is averaged across each
ensemble of NNs. The marginal log-likelihood
(MLL) of the target function is higher for the
better-performing NN family.
4Here, erfis deﬁned as aerf(bx) +c, where erf(x) =2√π/integraltextx
0e−t2dt.
8Under review as submission to TMLR
4.3.2 Small width neural networks and learning the kernel
In the previous experiment, we showed that the marginal likelihood could be predictive when we consider
large-width NNs and when we use a corresponding, analytically derived kernel. Is the marginal likelihood
predictive when we consider smaller-width NNs and when we learn the kernel empirically?
NN hyperparameters. We consider ensembles of width 16, depth 4 NNs from two families: NNs with
sinactivations and NNs with ReLU activations. We randomly initialize weights about zero with weight
varianceσ2
w= 1.5and bias variance σ2
b= 0.05. We train an ensemble of 50 randomly initialized NNs from
each family on the target functions using full-batch gradient descent with a learning rate of η= 0.1.
Target function. The target function families mirror the NN model families: We collect predictions from
randomly initialized, width 16, depth 4 NNs with sinor ReLU activations. These target functions are a useful
sanity check, as the inductive biases of the model families are perfectly suited for a target function family.
0 200 400 600 800 1000
Iterations0.000.050.100.150.20Test errorPerformance Compared
sin, MLL = 181.58
ReLU, MLL = 131.30
0 1 2 3 4
Distance−101CovarianceKernels Compared
sin kernel
ReLU surrogate kernel
Data
0 200 400 600 800 1000
Iterations0.000.050.100.15Test errorPerformance Compared
sin, MLL = 90.33
ReLU, MLL = 148.37
0 1 2 3 4
Distance−101CovarianceKernels Compared
sin surrogate kernel
ReLU kernel
Data
Figure 6: Ranking generalization from MLL in small-
width NNs. Mean and standard error of test MSE (left)
of small-width sinusoidal and rectiﬁer NN ensembles on sin
(top)and ReLU (bottom) target function families, with the
target function MLL under the surrogate learned from each
model family in the legend. Covariance (right) of surrogate
kernels alongside data kernels learned from the sin(top)and
ReLU(bottom) target function families. Even in the small-
width regime and when the kernel is learned, the model family
whose surrogate assigns a higher MLL to the target function at-
tains lower error (left); the surrogate kernel learned from the
better-performing model family better matches the data kernel
(right).GP surrogate. For each ensemble, we learn the
hyperparameters of an SMK with Q= 5mixture
components by optimizing Objective ( 6) across the
ensemble. To optimize, we randomly initialize the
kernel hyperparameters and run Adam for 250 iter-
ations with a learning rate of η= 0.1. We initialize
the frequency parameters by sampling from a uni-
form distribution whose upper limit is the Nyquist
frequency. We choose the kernel hyperparameters
with the highest objective value across three ran-
dom initializations.
Results. In Fig. ( 6), we compare the perfor-
mances of the two NN families on the two tar-
get function families. We also display the kernels
learned from NN behavior ( sin surrogate kernel or
ReLU surrogate kernel ) and learned from the target
function family ( data kernel ) directly. Across both
experiments, the MLL averaged across the target
function family of the better-performing NN family
is higher. In general, the structure of a learned ker-
nel reﬂects the properties of the learned GP prior,
and so we can compare kernels to assess similarity
between target function and NN families. We see
that the data kernel provides a better qualitative
match to the kernel of the better-performing model
family.
4.3.3 Systematic study of various learning rates and architectures
In this last experiment on ranking generalization performance, we establish that Gaussian process surrogates
reliably rank performance across a range of learning rates and gradient descent algorithms.
NN hyperparameters. We consider ensembles of randomly initialized NNs with sinor ReLU activations
and 1 or 3 hidden layers with 256 hidden units in each layer. We randomly initialize the weights about zero
with weight variance σ2
w= 1.5and bias variance σ2
b= 0.05. We train 50 randomly initialized NNs from
each family using either vanilla full-batch gradient descent with a constant learning rate of η= 0.01, or
Adam (Kingma and Ba 2015) using learning rates of η∈{0.0003,0.003}.
Target function. We consider a target function of sin(0.5x).
9Under review as submission to TMLR
0 1000 2000 3000 4000 5000
Iterations0.00.51.01.5Test errorgradient descent w/Adam, h= 0.003
ReLU, MLL = -662.81
sin, MLL = 63.28
0 1000 2000 3000 4000 5000
Iterationsgradient descent w/Adam, h= 0.0003
ReLU, MLL = -662.81
sin, MLL = 63.28
0 1000 2000 3000 4000 5000
Iterationsvanilla gradient descent, h= 0.01
ReLU, MLL = -662.81
sin, MLL = 63.28
0 1000 2000 3000 4000 5000
Iterations0.00.10.20.30.4Test errorgradient descent w/Adam, h= 0.003
ReLU, MLL = -1547.43
sin, MLL = -368.57
0 1000 2000 3000 4000 5000
Iterationsgradient descent w/Adam, h= 0.0003
ReLU, MLL = -1547.43
sin, MLL = -368.57
0 1000 2000 3000 4000 5000
Iterationsvanilla gradient descent, h= 0.01
ReLU, MLL = -1547.43
sin, MLL = -368.57
Figure7: RankinggeneralizationperformancefromMLLacrossdiﬀerentlearningalgorithmsandarchitectures.
Each panel displays mean and standard error of test MSE of an NN family trained on the target function sin(0.5x)with noise;
legend displays MLL of the training data under the surrogate for one of two NN families: 1-layer (256 hidden units) sinusoidal
or rectiﬁer NNs (top)); 3-layer (256 hidden units) sinusoidal or rectiﬁer NNs (bottom) . NNs are trained with batch gradient
descent with Adam (learning rates η= 0.003,η= 0.0003) or vanilla batch gradient descent ( η= 0.01). Across architectures
and learning algorithms, the NN family whose surrogate assigns higher MLL to the target function achieves lower test error.
GP surrogate. For each ensemble, we learn the hyperparameters of an SMK with Q= 5mixture com-
ponents by optimizing Objective ( 6) across the ensemble. To optimize, we randomly initialize the kernel
hyperparameters and run Adam for 250 iterations with a learning rate of η= 0.1. We choose the kernel hy-
perparameters with the highest objective value across three random initializations. To randomly initialize
the frequency parameters, we uniformly sample from the real-valued interval (0,25].
Results. In Fig. (7), we ﬁnd that the marginal likelihood of the better-performing NN family is higher.
The marginal likelihood depends on the diagonal noise σ2
nadded to the Gram matrix (Eq. (3)). We ﬁnd that
ourresultarerobustacrossthreelevelsofthisdiagonalnoise( 10−3,10−4,10−5). Theseresultssuggestwecan
rank these NN families when they are not in the asymptotic regime and when we learn the kernel, in contrast
to Section ( 4.3.1), as well as when a priori no model family should perform better, unlike Section ( 4.3.2).
4.4 Predicting the NN generalization gap with the GP marginal likelihood
In the previous section, we predicted generalization using kernels learned from randomly initialized NNs.
However, some design choices do not aﬀect NN properties at random initialization but may still strongly
inﬂuence generalization ( e.g.,learning algorithm). Motivated by this, we characterize trained NN properties
on thevalidation set and compare these properties to the training data. We focus on the validation set
because it is more informative of extrapolation. If the NN extrapolates well, its predictions on the validation
set should be “similar” in some sense to the dataset. On the other hand, signiﬁcant discrepancies could
indicate poor extrapolation. This intuition motivates our analysis.
In particular, we learn a kernel from the training data and a kernel from NN predictions on a validation
set. We then quantitatively compare these kernels by computing a metric we describe in more detail later.
We ﬁnd that a lower similarity between these kernels correlates with a larger generalization gap (i.e.,poorer
extrapolation), deﬁned as the diﬀerence between test error and training error ( e.g.,Jiang et al. 2020).
10Under review as submission to TMLR
feature 0
feature 1
feature 2
feature 3
feature 4
feature 5
feature 6
feature 7
feature 8
feature 9
feature 100.00.51.01.52.0LengthscalesSurrogate lengthscales
feature 0
feature 1
feature 2
feature 3
feature 4
feature 5
feature 6
feature 7
feature 8
feature 9
feature 10Data lengthscaleswine dataset, generalization gap = 0.219
feature 0
feature 1
feature 2
feature 3
feature 4012LengthscalesSurrogate lengthscales
feature 0
feature 1
feature 2
feature 3
feature 4Data lengthscalesairfoil dataset, generalization gap = 0.0177
Figure 8: Qualitative connection between lengthscale proﬁle discrepancy and generalization gap. Each subﬁgure
compares normalized lengthscales learned from neural network predictions on validation set ( i.e.,surrogate lengthscales) after
training and normalized lengthscales learned from training data ( i.e.,data lengthscales). A lengthscale greater than 1 indicates
an “unimportant” feature. The title indicates the UCI dataset and generalization gap deﬁned in Fig. ( 9). Data and surrogate
lengthscales for some features are diﬀerent ( e.g.,features 1, 4, 6), reﬂected in a high generalization gap ( left). Data and
surrogate lengthscales for the same features are generally similar, reﬂected in a low generalization gap ( right). This suggests
a connection between the generalization gap and discrepancy between surrogate and data lengthscales.
0.00.51.0r = -0.758GELU, 1500 steps
r = -0.816ReLU, 1500 steps
r = -0.842SiLU, 1500 steps
r = -0.636tanh, 1500 steps
0.00.51.0r = -0.802GELU, 200 steps
r = -0.787ReLU, 200 steps
r = -0.856SiLU, 200 steps
r = -0.608tanh, 200 steps
0.0 0.5 1.00.00.51.0r = -0.763GELU, 100 steps
0.0 0.5 1.0r = -0.768ReLU, 100 steps
0.0 0.5 1.0r = -0.831SiLU, 100 steps
0.0 0.5 1.0r = -0.528tanh, 100 stepsairfoil
autompg
breastcancer
concrete
energy
fertility
housing
kin40k
pendulum
power plant
protein
pumadyn32nm
solar
song
stock
wine
yacht
0.0 0.2 0.4 0.6 0.8 1.0
Lengthscale correlation0.00.20.40.60.81.0Generalization gap
Figure 9: Inverse relationship between generalization error and lengthscale correlation on UCI datasets. Each
point represents the lengthscale correlation (between surrogate and data lengthscales) and the generalization gap for a neural
network ensemble to which the surrogate model is ﬁt, on a single UCI dataset. Each panel corresponds to a particular neural
family; see Section ( 4.4) for details about hyperparameters of these families, including architectures. Colors correspond to a
particular UCI dataset. Across datasets and architectures, a larger lengthscale correlation ( i.e.,higher similarity between the
data and surrogate representations) corresponds to a lower generalization gap ( i.e.,better extrapolation).
NN hyperparameters. We train ensembles of randomly initialized NNs with sigmoid-weighted linear
unit (SiLU) (Elfwing et al. 2018), Gaussian error linear unit (GELU) (Hendrycks and Gimpel 2016),
ReLU (Fukushima 1975; Nair and Hinton 2010), or hyperbolic tangent ( tanh) activations, and two layers of
128 hidden units. We use the LeCun normal initialization with a scale of 1.5 (LeCun et al. 2012). We train
25 NNs with full-batch gradient descent using Adam with a learning rate of η= 0.003. We want to assess if
our approach can distinguish between NNs with similar training behavior but varying generalization perfor-
mance. We train NNs either for a maximum number of iterations, a hyperparameter, or until training error
reaches zero.
Target functions. We consider a set of naturalistic regression tasks from the UC Irvine Machine Learning
Repository (UCI) dataset (Dua and Graﬀ 2017), spanning a range of dataset sizes and input dimensions.
We split each of the datasets into a 72/8/20 train/validation/test split. Both the data input and output are
standardized by mean-centering and dividing by the standard deviation dimension-wise so that the target
11Under review as submission to TMLR
values and each dimension of the data input have near zero mean and unit variance. We subsample 2,000
datapoints for datasets with more than 2,000 datapoints, as in Simpson et al. (2021) and Liu et al. (2020).
GP surrogate. We learn a data kernel directly from the training dataset. We also learn a surrogate kernel
from NN predictions on the validation set. In both cases, we use the Matérn kernel (MK) since the SMK can
struggle for higher-dimensional inputs. We learn a separate lengthscale for each input dimension ( i.e.,fea-
ture) of the data. We denote the lengthscales for a kernel as its lengthscale proﬁle . We call the data ker-
nel’s lengthscales the data lengthscales and the surrogate kernel’s lengthscales the surrogate lengthscales . To
quantify the mismatch between NN validation predictions and the training data, we consider the correlation
in lengthscale proﬁles across features . This is the correlation between the data and surrogate lengthscales.
Results. Fig. (8) gives intuition for our more general result in Fig. ( 9). For two UCI datasets, we compare
the data lengthscales and the surrogate lengthscales for a two-layer GELU NN. The vertical axis corresponds
to (normalized) learned lengthscales for each input dimension.5When the generalization gap is small, the
data kernel and surrogate kernel are similar; the same features have similar lengthscales (Fig. ( 8), right).
When the generalization gap is large, the data kernel and surrogate kernel have discrepancies. For example,
the surrogate lengthscales for features 1 and 6 are larger than 1, but the data lengthscales for feature 1 and
6 are smaller than 1 (Fig. ( 8), left).
In Fig. (9), we summarize our results across diﬀerent architectures, datasets, and maximum training itera-
tions. We display the generalization gap against the correlation in lengthscale proﬁles across features. The
similarity in lengthscale proﬁles negatively correlates with generalization gap across a range of architectures
and max iterations. The Pearson correlation coeﬃcients range from −0.856 to −0.528. In Appendix ( B),
we additionally demonstrate that these results are insensitive to outlier datasets by performing a dataset-
sensitivity analysis.
4.5 Correlating NN generalization with the GP marginal likelihood and lengthscales
In this section, we extend the analysis in the previous section to a larger hyperparameter sweep of models.
NN hyperparameters. We train ensembles of randomly initialized NNs with four activations
(SiLU (Elfwing et al. 2018), GELU (Hendrycks and Gimpel 2016), ReLU (Fukushima 1975; Nair and Hinton
2010), or tanh), two diﬀerent depths (2 and 4), two diﬀerent numbers of hidden units (32, 64) in each layer,
and two sets of learning rates with Adam (0.03, 0.003); this leads to a total of 32 diﬀerent combinations of
hyperparameters. We use the LeCun normal initialization with a scale of 1.5 (LeCun et al. 2012). We train
50 NNs with full-batch gradient descent using Adam with two varying learning rates. We train NNs for 500
iterations.
Target functions. We consider a set of naturalistic regression tasks from the UCI dataset (Dua and
Graﬀ 2017), spanning a range of dataset sizes and input dimensions. We consider a set of UCI datasets
that satisfy two criteria: the number of unique feature values for each dimension is greater than 2 and
the range in lengthscale correlations between neural network and datasets is greater than 0.025. The ﬁrst
condition ensures that we can learn sensible lengthscales for the dataset. The second condition ensures there
is meaningful variation in neural network behavior within a single dataset.
We split each of the datasets into a 72/8/20 train/validation/test split. Both the data input and output are
standardized by mean-centering and dividing by the standard deviation dimension-wise so that the target
values and each dimension of the data input have near zero mean and unit variance. We subsample 2,000
datapoints for datasets with more than 2,000 datapoints, as in Simpson et al. (2021) and Liu et al. (2020).
GP surrogate. We learn a data kernel directly from the training dataset. We also learn a surrogate kernel
from NN predictions on the validation set. In both cases, we use the Matérn kernel (MK) since the SMK can
5For this visualization, we divide the learned lengthscale for each dimension by the diﬀerence between the maximum feature
value and minimum feature value for each dimension. By doing so, we can interpret a lengthscale that is much greater than 1
as suggesting that the NN predictions do not vary much along that dimension.
12Under review as submission to TMLR
0.86 0.88 0.90 0.920.050.100.150.20Generalization gapkin40k, r= -0.44
0.93 0.94 0.95 0.96 0.970.040.060.080.100.120.14autompg, r= -0.34
0.4 0.5 0.6 0.7 0.80.3000.3250.3500.3750.4000.425pendulum, r= 0.19
0.5 0.6 0.7 0.8 0.90.20.30.40.5protein, r= 0.50
0.94 0.96 0.98
Lengthscale correlation0.010.020.030.04Generalization gapairfoil, r= -0.50
0.5 0.6 0.7
Lengthscale correlation0.60.70.80.91.01.1song, r= -0.88
0.2 0.4 0.6 0.8 1.0
Lengthscale correlation0.50.60.70.80.91.0pumadyn32nm, r= -0.71
Figure 10: Inverse relationship between generalization error and lengthscale correlation on UCI datasets across
hyperparameter sweep of neural networks. Each point represents the lengthscale correlation (between surrogate and data
lengthscales) and the generalization gap for a neural network ensemble to which the surrogate model is ﬁt. Each panel corre-
sponds to a particular UCI dataset; see Section ( 4.5) for details about hyperparameters of these families, including architec-
tures. In 5/7 datasets, a larger lengthscale correlation ( i.e.,higher similarity between the data and surrogate representations)
corresponds to a lower generalization gap ( i.e.,better extrapolation).
−195−194
MLL0.080.100.120.14Test errorautompg, r = -0.72
−455−450−4450.300.350.40pendulum, r = -0.22
−1290−1285−12800.050.100.150.20kin40k, r = -0.48
−836−834−8320.020.030.040.05airfoil, r = 0.41
−2000−1500−1000
LML0.60.81.0Test errorpumadyn32nm, r = -0.27
−1610−1600−1590
LML0.60.81.0Test errorsong, r = -0.81
−1620−1600−1580
LML0.350.400.450.500.55Test errorprotein, r = 0.62
Figure 11: Inverse relationship between test error and marginal likelihood on UCI datasets Each point represents
the marginal likelihood of the training data (using the kernel learned for a neural network family) and the test error for a neural
network ensemble to which the surrogate model is ﬁt. Each panel corresponds to a particular UCI dataset; see Section ( 4.5) for
details about hyperparameters of these families, including architectures. Each color corresponds to a particular neural network
ensemble. In 5/7 datasets, a larger marginal likelihood correlates with lower test error.
struggle for higher-dimensional inputs. We learn a separate lengthscale for each input dimension ( i.e.,fea-
ture) of the data. We denote the lengthscales for a kernel as its lengthscale proﬁle . We call the data kernel’s
lengthscales the data lengthscales and the surrogate kernel’s lengthscales the surrogate lengthscales . To quan-
tify the mismatch between NN validation predictions and the training data, we consider the correlation in
lengthscale proﬁles across features . We also consider the marginal likelihood of the training dataset under the
learned kernel ( i.e.,the Matern Kernel with the surrogate lengthscales) for each family of neural networks.
Results. In Figure 10, we plot the generalization gap against the correlation in lengthscale proﬁles across
features. In contrast to Figure 9, each panel corresponds to a particular dataset and each data point
corresponds to a particular neural network family with a set of hyperparameters described above.
The majority of datasets exhibit negative correlation between correlation the in lengthscale proﬁles and
generalization gap with two exceptions (pendulum, protein). We explain these exceptions. For the pendulum
13Under review as submission to TMLR
dataset, the positive correlation is driven by neural networks with tanh activations (the dots in diﬀerent
shades of red). We think the Matern kernel may struggle to model the tanh networks. This is consistent
with Figure 9 where the correlations were consistently lower for the tanh networks. For the protein dataset,
the positive correlations might be related to challenges in ﬁtting GPs to this dataset; recent work showed
that exact GP regression on a training dataset (without any subsampling) from the protein dataset attains
high test RMSE (K. A. Wang et al. 2019).
In Figure 11, we plot the test error against the marginal likelihood of the training data under the various
learned kernels for each neural network ensemble. That is, each point corresponds to the marginal likelihood
of the training data using the kernel learned from each neural network family. We do this for several datasets,
indicated by the ﬁgure subtitle. The marginal likelihood correlates with test error on several of the datasets.
Consistent with previous work (Lotﬁ et al. 2022), we ﬁnd that this result is sensitive to the jitter (which we
set to 0.5).
5 Discussion
In this paper, we illustrated the potential of modeling neural networks with Gaussian process surrogates. We
empirically characterized phenomena in neural networks by interpreting kernels learned directly from neural
network predictions, capturing the spectral bias of deep rectiﬁer networks (Section ( 4.1)) and pathological
behavior in deep, randomly initialized neural networks (Section ( 4.2)). We further demonstrated that
Gaussian process surrogates could predict neural network generalization by ranking test error performance
bymarginal(Section( 4.3))andbyquantifyingthegeneralizationgapviaasurrogate-datakerneldiscrepancy
(Section ( 4.4)). Taken together, these results suggest that Gaussian process surrogates may be a valuable
empiricaltoolforinvestigatingdeeplearning, andfutureworkcouldaimtousethisframeworktocomplement
existing approaches to interpretability ( e.g.,Ribeiro et al. 2016) and extrapolation ( e.g.,Xu et al. 2021).
We note a couple of limitations of our current study. First, though the framework is in principle applicable
to broader settings, we restricted this ﬁrst exploration to regression tasks and feed-forward neural network
architectures. A broader study of more architectures on more types of tasks would be challenging due to the
need to scale Gaussian processes but potentially rewarding, as characterizing properties of neural networks
as used in practice is a signiﬁcant open problem with far-reaching implications (Sejnowski 2020). Second, we
learn point estimates of kernel hyperparameters (type II maximum likelihood; Gelman et al. 2013). Although
thisisstandard, wecouldinfertheposterioroverhyperparametersusingMarkovchainMonteCarlo(MCMC)
or variational inference (Lalchand and Carl Edward Rasmussen 2020; Murray and Adams 2010; Simpson
et al. 2021) to perform a fully Bayesian analysis. We also could explore a richer set of kernels, such as
compositional kernels (Duvenaud, Lloyd, et al. 2013). These directions are exciting avenues for future work.
References
Allen-Zhu, Zeyuan, Yuanzhi Li, and Zhao Song (2019). “A convergence theory for deep learning via over-
parameterization”. In: Proceedings of the International Conference on Machine Learning , pp. 242–252.
url:https://arxiv.org/abs/1811.03962 .
Bommasani, Rishi et al. (2021). “On the opportunities and risks of foundation models”. In: arXiv: 2108.
07258.url:https://arxiv.org/abs/2108.07258 .
Camps-Valls, Gustau et al. (2015). “Ranking drivers of global carbon and energy ﬂuxes over land”. In:
Proceedings of the IEEE International Geoscience and Remote Sensing Symposium , pp. 4416–4419. url:
https://doi.org/10.1109/IGARSS.2015.7326806 .
Caruana, Rich (1997). “Multitask learning”. In: Machine Learning 28, pp. 41–75. url:https://doi.org/
10.1023/A:1007379606734 .
D’Amour, Alexander et al. (2020). “Underspeciﬁcation presents challenges for credibility in modern machine
learning”. In: arXiv: 2011.03395 .url:https://arxiv.org/abs/2011.03395 .
14Under review as submission to TMLR
Dehghani, Mostafa, Yi Tay, Alexey A Gritsenko, Zhe Zhao, Neil Houlsby, Fernando Diaz, Donald Metzler,
and Oriol Vinyals (2021). “The benchmark lottery”. In: arXiv: 2107.07002 .url:https://arxiv.org/
abs/2107.07002 .
Doshi-Velez, Finale and Been Kim (2017). “Towards a rigorous science of interpretable machine learning”.
In: arXiv: 1702.08608 .url:https://arxiv.org/abs/1702.08608 .
Du, Simon S, Xiyu Zhai, Barnabas Poczos, and Aarti Singh (2019). “Gradient descent provably optimizes
over-parameterized neural networks”. In: Proceedings of the International Conference on Learning Rep-
resentations .url:https://arxiv.org/abs/1810.02054 .
Dua, Dheeru and Casey Graﬀ (2017). UCI Machine Learning Repository .url:http://archive.ics.uci.
edu/ml.
Duvenaud, David, James Lloyd, Roger Grosse, Joshua Tenenbaum, and Ghahramani Zoubin (2013). “Struc-
ture discovery in nonparametric regression through compositional kernel search”. In: Proceedings of
the International Conference on Machine Learning .url:https : / / proceedings . mlr . press / v28 /
duvenaud13.html .
Duvenaud, David, Oren Rippel, Ryan P. Adams, and Zoubin Ghahramani (2014). “Avoiding pathologies
in very deep networks”. In: Proceedings of the International Conference on Artiﬁcial Intelligence and
Statistics .url:https://arxiv.org/abs/1402.5836 .
Elfwing, Stefan, Eiji Uchibe, and Kenji Doya (2018). “Sigmoid-weighted linear units for neural network
function approximation in reinforcement learning”. In: Neural Networks 107, pp. 3–11. url:https://
doi.org/10.1016/j.neunet.2017.12.012 .
Fukushima, Kunihiko (1975). “Cognitron: A self-organizing multilayered neural network”. In: Biological
Cybernetics 20, pp. 121–136. url:https://doi.org/10.1007/BF00342633 .
— (2004). “Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition
unaﬀected by shift in position”. In: Biological Cybernetics 36, pp. 193–202.
Garriga-Alonso, Adrià, Carl Edward Rasmussen, and Laurence Aitchison (2019). “Deep convolutional net-
works as shallow Gaussian processes”. In: Proceedings of the International Conference on Learning Rep-
resentations .url:https://openreview.net/forum?id=Bklfsi0cKm .
Geirhos, Robert, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias
Bethge, and Felix A Wichmann (2020). “Shortcut learning in deep neural networks”. In: Nature Machine
Intelligence 2, pp. 665–673. url:https://doi.org/10.1038/s42256-020-00257-z .
Geirhos, Robert, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland
Brendel (2019). “ImageNet-trained CNNs are biased towards texture; Increasing shape bias improves
accuracy and robustness”. In: Proceedings of the International Conference on Learning Representations .
url:https://arxiv.org/abs/1811.12231 .
Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin (2013).
Bayesian data analysis . Chapman and Hall/CRC. url:http://www.stat.columbia.edu/~gelman/
book/.
Griﬃths, Thomas L, Nick Chater, Charles Kemp, Amy Perfors, and Joshua B Tenenbaum (2010). “Proba-
bilistic models of cognition: Exploring representations and inductive biases”. In: Trends in cognitive sci-
ences14.8, pp. 357–364. url:https://doi.org/10.1016/j.tics.2010.05.004 .
Hawkins, Robert, Takateru Yamakoshi, Thomas L Griﬃths, and Adele Goldberg (2020). “Investigating
representations of verb bias in neural language models”. In: Proceedings of the Conference on Empirical
Methods in Natural Language Processing , pp. 4653–4663. url:https://arxiv.org/abs/2010.02375 .
Hendrycks, Dan and Kevin Gimpel (2016). “Gaussian error linear units (GELUs)”. In: arXiv: 1606.08415 .
url:https://arxiv.org/abs/1606.08415 .
15Under review as submission to TMLR
Hospedales, Timothy M, Antreas Antoniou, Paul Micaelli, and Amos J Storkey (2020). “Meta-learning in
neural networks: A survey”. In: IEEE Transactions on Pattern Analysis and Machine Intelligence .url:
https://arxiv.org/abs/2004.05439 .
Ilyas, Andrew, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry (2022). “Data-
models: Understanding predictions with data and data with predictions”. In: Proceedings of the Interna-
tional Conference on Machine Learning , pp. 9525–9587. url:https://proceedings.mlr.press/v162/
ilyas22a.html .
Ilyas, Andrew, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry
(2019). “Adversarial examples are not bugs, they are features”. In: Advances in Neural Information
Processing Systems .url:https://arxiv.org/abs/1905.02175 .
Jacot, Arthur, Franck Gabriel, and Clément Hongler (2018). “Neural tangent kernel: Convergence and gen-
eralization in neural networks”. In: Advances in Neural Information Processing Systems . Vol. 31. url:
https://arxiv.org/abs/1806.07572 .
Jiang, Yiding et al. (2020). NeurIPS 2020 Competition: Predicting generalization in deep learning .url:
https://arxiv.org/abs/2012.07976 .
Kingma, Diederick P and Jimmy Ba (2015). “Adam: A method for stochastic optimization”. In: Proceedings
of the International Conference on Learning Representations .url:https://arxiv.org/abs/1412.6980 .
Koh, Pang Wei and Percy Liang (2017). “Understanding black-box predictions via inﬂuence functions”. In:
Proceedings of the International Conference on Machine Learning , pp. 1885–1894. url:https://arxiv.
org/abs/1703.04730 .
Labatie,Antoine(2019).“Characterizingwell-behavedvs.pathologicaldeepneuralnetworks”.In: Proceedings
of the International Conference on Machine Learning .url:https://arxiv.org/abs/1811.03087 .
Lalchand, Vidhi and Carl Edward Rasmussen (2020). “Approximate inference for fully Bayesian Gaussian
process regression”. In: Proceedings of the Symposium on Advances in Approximate Bayesian Inference .
Proceedings of Machine Learning Research. Pmlr. url:https://arxiv.org/abs/1912.13440 .
LeCun, Yann, Léon Bottou, Genevieve B. Orr, and Klaus-Robert Müller (2012). “Eﬃcient backprop”. In:
Neural Networks: Tricks of the Trade . Ed. by Grégoire Montavon, Geneviève B. Orr, and Klaus-Robert
Müller. Springer Berlin Heidelberg, pp. 9–48. url:https://doi.org/10.1007/978-3-642-35289-
8%5C%5F3 .
Lee, Jaehoon, Yasaman Bahri, Roman Novak, Samuel Schoenholz, Jeﬀrey Pennington, and Jascha Sohl-
Dickstein (2017). “Deep neural networks as Gaussian processes”. In: Proceedings of the International
Conference on Learning Representations .url:https://arxiv.org/abs/1711.00165 .
Li, Yuanzhi and Yingyu Liang (2018). “Learning overparameterized neural networks via stochastic gradient
descent on structured data”. In: Advances in Neural Information Processing Systems .url:https://
arxiv.org/abs/1808.01204 .
Lin, Henry W. and Max Tegmark (2017). “Why does deep and cheap learning work so well?” In: Journal of
Statistical Physics 168, pp. 1223–1247. url:https://doi.org/10.1007/s10955-017-1836-5 .
Lipton, Zachary C (2016). “The mythos of model interpretability”. In: arXiv: 1606.03490 .url:https://
arxiv.org/abs/1606.03490 .
Liu, Sulin, Xingyuan Sun, Peter J Ramadge, and Ryan P. Adams (2020). “Task-agnostic amortized inference
of Gaussian process hyperparameters”. In: Advances in Neural Information Processing Systems .url:
https://papers.nips.cc/paper/2020/hash/f52db9f7c0ae7017ee41f63c2a7353bc-Abstract.html .
Lotﬁ, Sanae, Pavel Izmailov, Gregory Benton, Micah Goldblum, and Andrew Gordon Wilson
(2022). “Bayesian model selection, the marginal likelihood, and generalization”. In: arXiv preprint
arXiv:2202.11678 .
16Under review as submission to TMLR
Mackay, David J. C. (1992). “A practical Bayesian framework for backpropagation networks”. In: Neural
Computation 4, pp. 448–472. url:https://doi.org/10.1162/neco.1992.4.3.448 .
Matérn, Bertil (1960). Spatial variation . Vol. 36. Springer Science & Business Media. url:https://doi.
org/10.1007/978-1-4615-7892-5 .
Matthews, Alexander G de G, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani (2018).
“Gaussian process behaviour in wide deep neural networks”. In: Proceedings of the International Confer-
ence on Learning Representations .url:https://openreview.net/forum?id=H1-nGgWC- .
McClelland, James L (2009). “The place of modeling in cognitive science”. In: Topics in Cognitive Science
1.1, pp. 11–38. url:https://doi.org/10.1111/j.1756-8765.2008.01003.x .
Mitchell, Tom M (1980). The need for biases in learning generalizations . Tech. rep.
Murray, Iain and Ryan P Adams (2010). “Slice sampling covariance hyperparameters of latent Gaussian
models”. In: Advances in Neural Information Processing Systems .url:https://arxiv.org/abs/1006.
0868.
Nair, Vinod and Geoﬀrey E Hinton (2010). “Rectiﬁed linear units improve restricted Boltzmann ma-
chines”. In: Proceedings of the International Conference on Machine Learning .url:https://icml.cc/
Conferences/2010/papers/432.pdf .
Nakkiran, Preetum (2021). “Towards an empirical theory of deep learning”. PhD thesis. Harvard University.
url:https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:37370110 .
Neal, Radford M (1996). “Priors for inﬁnite networks”. In: Bayesian Learning for Neural Networks . Springer,
pp. 29–53. url:https://doi.org/10.1007/978-1-4612-0745-0%5C%5F2 .
Newell, Allen, John Calman Shaw, and Herbert A Simon (1958). “Elements of a theory of human problem
solving.” In: Psychological Review 65.3, p. 151. url:https : / / psycnet . apa . org / doi / 10 . 1037 /
h0048495 .
Neyshabur, Behnam, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro (2017). “Exploring general-
ization in deep learning”. In: Advances in Neural Information Processing Systems .url:https://arxiv.
org/abs/1706.08947 .
Novak, Roman, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Daniel A. Abolaﬁa, Jeﬀrey Penning-
ton, and Jascha Sohl-dickstein (2019). “Bayesian deep convolutional networks with many channels are
Gaussian processes”. In: Proceedings of the International Conference on Learning Representations .url:
https://openreview.net/forum?id=B1g30j0qF7 .
Novak,Roman,LechaoXiao,JiriHron,JaehoonLee,AlexanderA.Alemi,JaschaSohl-Dickstein,andSamuel
S.Schoenholz(2020).“Neuraltangents:FastandeasyinﬁniteneuralnetworksinPython”.In: Proceedings
of the International Conference on Learning Representations .url:https://arxiv.org/abs/1912.
02803.
Poggio, Tomaso A., Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier Boix, Jack
Hidary, and Hrushikesh N. Mhaskar (2018). “Theory of deep learning III: Explaining the non-overﬁtting
puzzle”. In: arXiv: 1801.00173 .url:http://arxiv.org/abs/1801.00173 .
Poggio, Tomaso A., Hrushikesh Narhar Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao (2017).
“Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review”. In:
International Journal of Automation and Computing 14, pp. 503–519. url:https://arxiv.org/abs/
1611.00740 .
Rahaman, Nasim, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio,
andAaronCourville(2019).“Onthespectralbiasofneuralnetworks”.In: Proceedings of the International
Conference on Machine Learning .url:https://arxiv.org/abs/1806.08734 .
Rasmussen, Carl E. and Christopher K.I. Williams (Jan. 2006). Gaussian processes for machine learning .
Adaptive Computation and Machine Learning. MIT Press. url:https://gaussianprocess.org/gpml/ .
17Under review as submission to TMLR
Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin (2016). ““Why should I trust you?”: Explaining
the predictions of any classiﬁer”. In: Proceedings of the ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining , pp. 1135–1144. url:https://arxiv.org/abs/1602.04938 .
Ritter, Samuel, David G T Barrett, Adam Santoro, and Matt M Botvinick (2017). “Cognitive psychology
for deep neural networks: A shape bias case study”. In: Proceedings of the International Conference on
Machine Learning , pp. 2940–2949. url:https://arxiv.org/abs/1706.08606 .
Roberts, Daniel A., Sho Yaida, and Boris Hanin (2022). The principles of deep learning theory .https://
deeplearningtheory.com . Cambridge University Press. arXiv: 2106.10165 .
Samek, Wojciech, Thomas Wiegand, and Klaus-Robert Müller (2017). “Explainable artiﬁcial intelligence:
Understanding, visualizing and interpreting deep learning models”. In: arXiv: 1708.08296 .url:https:
//arxiv.org/abs/1708.08296 .
Schoenholz, Samuel S., Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein (2017). “Deep information
propagation”. In: Proceedings of the International Conference on Learning Representations .url:https:
//arxiv.org/abs/1611.01232 .
Sejnowski, Terrence J. (2020). “The unreasonable eﬀectiveness of deep learning in artiﬁcial intelligence”. In:
Proceedings of the National Academy of Sciences 117, pp. 30033–30038. url:https://doi.org/10.
1073/pnas.1907373117 .
Shahriari, Bobak, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando de Freitas (2016). “Taking the
human out of the loop: A review of Bayesian optimization”. In: Proceedings of the IEEE 104, pp. 148–
175.url:https://doi.org/10.1109/JPROC.2015.2494218 .
Simpson, Fergus, Ian Davies, Vidhi Lalchand, Alessandro Vullo, Nicolas Durrande, and Carl Edward Ras-
mussen (2021). “Kernel identiﬁcation through transformers”. In: Advances in Neural Information Pro-
cessing Systems .url:https://openreview.net/forum?id=B0rmtp9q6-%5C%5F .
Snoek, Jasper, Hugo Larochelle, and Ryan P. Adams (2012). “Practical Bayesian optimization of machine
learning algorithms”. In: Advances in Neural Information Processing Systems .url:https://arxiv.
org/abs/1206.2944 .
Soudry, Daniel, Elad Hoﬀer, and Nathan Srebro (2018). “The implicit bias of gradient descent on separable
data”. In: Proceedings of the International Conference on Learning Representations .url:https : / /
openreview.net/forum?id=r1q7n9gAb .
Sun, Ron (2008). The Cambridge handbook of computational psychology . Cambridge University Press.
Szegedy,Christian,WojciechZaremba,IlyaSutskever,JoanBruna,DumitruErhan,IanGoodfellow,andRob
Fergus (2014). “Intriguing properties of neural networks”. In: Proceedings of the International Conference
on Learning Representations .url:https://arxiv.org/abs/1312.6199 .
Tiňo, Peter, Michal Cernanský, and Lubica Benusková (2004). “Markovian architectural bias of recurrent
neural networks”. In: IEEE Transactions on Neural Networks 15, pp. 6–15. url:https://doi.org/10.
1109/TNN.2003.820839 .
Wang,GGaryandSongqingShan(May2006).“Reviewofmetamodelingtechniquesinsupportofengineering
design optimization”. In: Journal of Mechanical Design 129.4, pp. 370–380. url:https://doi.org/10.
1115/1.2429697 .
Wang, Ke Alexander, Geoﬀ Pleiss, Jacob R. Gardner, Stephen Tyree, Kilian Q. Weinberger, and Andrew
Gordon Wilson (2019). “Exact gaussian processes on a million data points”. In: NeurIPS .
Werbos, Paul J. (1988). “Generalization of backpropagation with application to a recurrent gas market
model”. In: Neural Networks 1, pp. 339–356. url:https://doi.org/10.1016/0893-6080(88)90007-X .
Wilson, Andrew G and Ryan P Adams (2013). “Gaussian process kernels for pattern discovery and extrap-
olation”. In: Proceedings of the International Conference on Machine Learning .url:https://arxiv.
org/abs/1302.4245 .
18Under review as submission to TMLR
Xiao, Lechao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S. Schoenholz, and Jeﬀrey Pennington (2018).
“Dynamical isometry and a mean ﬁeld theory of CNNs: How to train 10,000-layer vanilla convolutional
neural networks”. In: Proceedings of the International Conference on Machine Learning .url:https://
arxiv.org/abs/1806.05393 .
Xu, Keyulu, Mozhi Zhang, Jingling Li, Simon Shaolei Du, Ken-Ichi Kawarabayashi, and Stefanie Jegelka
(2021). “How neural networks extrapolate: From feedforward to graph neural networks”. In: Proceedings
of the International Conference on Learning Representations .url:https://openreview.net/forum?
id=UH-cmocLJC .
Xue, Tianju, Alex Beatson, Sigrid Adriaenssens, and Ryan P. Adams (2020). “Amortized ﬁnite element anal-
ysis for fast PDE-constrained optimization”. In: Proceedings of the International Conference on Machine
Learning . Vol. 119, pp. 10638–10647. url:https://proceedings.mlr.press/v119/xue20a.html .
Yang, Greg (2019). “Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation”. In: arXiv: 1902.04760 .url:http://
arxiv.org/abs/1902.04760 .
Zhou, Yilun, Marco Tulio Ribeiro, and Julie Shah (2022). “ExSum: From local explanations to model under-
standing”. In: Annual Conference of the North American Chapter of the Association for Computational
Linguistics .url:https://arxiv.org/abs/2205.00130 .
19Under review as submission to TMLR
−2.50.02.5w= 0.1 w= 0.5 w= 2.0
−2.50.02.5t= 0.1 t= 0.5 t= 2.0
−1 0 1−2.50.02.5m= 0.1
−1 0 1m= 1.0
−1 0 1m= 2.5
−2.50.02.5/lscript= 0.1, n= 0.5/lscript= 0.5, n= 0.5/lscript= 2.0, n= 0.5
−2.50.02.5/lscript= 0.1, n= 1.5/lscript= 0.5, n= 1.5/lscript= 2.0, n= 1.5
−1 0 1−2.50.02.5/lscript= 0.1, n= 2.5
−1 0 1/lscript= 0.5, n= 2.5
−1 0 1/lscript= 2.0, n= 2.5
Figure 12: Illustrating the eﬀect of GP kernel hyperparameters on the GP prior. (Left) Samples from a GP prior
withSMKwithvaryingmixtureweights ω,mixturescale τ,andmixturemeans µ. (Right)SamplesfromaGPpriorwithMatern
kernel with varying νand/lscript(lengthscale). GPs are ﬂexible models whose properties can be controlled through hyperparameters.
A Properties of the spectral mixture kernel and the Matérn kernel
We describe how the various hyperparameters of the SMK and MK kernel aﬀect the GP prior. We begin with
the spectral mixture kernel. The mixture weights ware signal variances and control the scale of the function
values. The mixture means ( µ) encode periodic behavior. The variances ( τ) are (inverse) lengthscales, which
control the smoothness. The (ARD) MK kernel has lengthscales θ, which controls the smoothness of the
function with respect to each dimension. νis another hyperparameter that also modulates smoothness, and
the Matern covariance function admits a simple expression when νis a half-integer. ν= 2.5corresponds to
twice diﬀerentiable functions and ν= 1.5corresponds to once diﬀerentiable functions.
In Fig. (12), we vary the hyperparameters of the SMK ( w,µ,τ)and Matern kernels ( ν,θ) and illustrate how
they impact the prior over functions.
20Under review as submission to TMLR
1510GELU,steps=1500 ReLU,steps=1500 SiLU,steps=1500 tanh,steps=1500
1510GELU,steps=200 ReLU,steps=200 SiLU,steps=200 tanh,steps=200
−0.4−0.6−0.8−1.01510GELU,steps=100
−0.4−0.6−0.8−1.0ReLU,steps=100
−0.4−0.6−0.8−1.0SiLU,steps=100
−0.4−0.6−0.8−1.0tanh,steps=100
0.0 0.2 0.4 0.6 0.8 1.0
Correlations (with one dataset removed)0.00.20.40.60.81.0Correlation counts
Figure 13: Sensitivity analysis of generalization gap and lengthscale proﬁle relationship. Each panel a histogram
and mean (red line) of correlations obtained by recomputing the correlation between lengthscale proﬁle correlation and gener-
alization gap after removing each UCI dataset. Across datasets and architectures, even when a single dataset is removed, there
remains an negative correlation between generalization gap and lenthscale proﬁle correlation. Therefore, the inverse relation-
ship between generalization gap and lengthscale proﬁle correlation demonstrated in Fig. ( 9) is robust to outlier datasets.
B Correlation sensitivity
We present some additional results to supplement our analysis from Section ( 4.4) where we demonstrated
that discrepancy in lengthscale proﬁles between data and neural network predicts the generalization gap.
Correlation can be sensitive to outliers. Does any single dataset account for the negative correlations? To
answer this, we characterize how the correlation changes as a result of dropping each dataset. Speciﬁcally,
for each UCI dataset, we remove that dataset and then compute the correlation between lengthscale pro-
ﬁle correlation and generalization gap for the remaining datasets. We plot the resulting distribution of cor-
relations in Fig. ( 13). We ﬁnd there is a tight spread around the correlation computed from all the UCI
datasets. Importantly, when we remove any UCI dataset, we still see moderate to high negative correlations
between lengthscale proﬁle correlation and generalization gap.
21