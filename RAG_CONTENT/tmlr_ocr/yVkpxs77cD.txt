Published in Transactions on Machine Learning Research (07/2022)
Deformation Robust Roto-Scale-Translation Equivariant
CNNs
L. Mars Gao marsgao@uw.edu
Paul G. Allen School of Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
Guang Lin guanglin@purdue.edu
Department of Mathematics and School of Mechanical Engineering
Purdue University
West Lafayette, IN 47907, USA
Wei Zhu weizhu@umass.edu
Department of Mathematics and Statistics
University of Massachusetts Amherst
Amherst, MA 01003, USA
Reviewed on OpenReview: https: // openreview. net/ forum? id= yVkpxs77cD
Abstract
Incorporating group symmetry directly into the learning process has proved to be an eﬀec-
tive guideline for model design. By producing features that are guaranteed to transform
covariantly to the group actions on the inputs, group-equivariant convolutional neural net-
works (G-CNNs) achieve signiﬁcantly improved generalization performance in learning tasks
withintrinsicsymmetry. GeneraltheoryandpracticalimplementationofG-CNNshavebeen
studied for planar images under either rotation or scaling transformation, but only individu-
ally. We present, in this paper, a roto-scale-translation equivariant CNN ( RST-CNN), that
is guaranteed to achieve equivariance jointly over these three groups via coupled group con-
volutions. Moreover, as symmetry transformations in reality are rarely perfect and typically
subject to input deformation, we provide a stability analysis of the equivariance of repre-
sentation to input distortion, which motivates the truncated expansion of the convolutional
ﬁlters under (pre-ﬁxed) low-frequency spatial modes. The resulting model provably achieves
deformation-robust RSTequivariance, i.e., the RSTsymmetry is still “approximately”
preserved when the transformation is “contaminated” by a nuisance data deformation, a
property that is especially important for out-of-distribution generalization. Numerical ex-
periments on MNIST, Fashion-MNIST, and STL-10 demonstrate that the proposed model
yields remarkable gains over prior arts, especially in the small data regime where both
rotation and scaling variations are present within the data.
1 Introduction
Symmetry is ubiquitous in machine learning. For instance, in image classiﬁcation, the class label of an image
remains the same when the image is spatially translated. Convolutional neural networks (CNNs) through
spatial weight sharing achieve built-in translation-equivariance , i.e., a shift of the input leads to a correspond-
ing shift of the output, a property that improves the generalization performance and sample complexity of
the model for computer vision tasks with translation symmetry, such as image classiﬁcation (Krizhevsky
et al., 2012), object detection (Ren et al., 2015), and segmentation (Long et al., 2015; Ronneberger et al.,
2015).
1Published in Transactions on Machine Learning Research (07/2022)
Inspired by the standard CNNs, researchers in recent years have developed both theoretical foundations and
practicalimplementationsof group equivariant CNNs (G-CNNs),i.e.,generalizedCNNmodelsthatguarantee
a desired transformation on layer-wise features under a given input group transformation, for signals deﬁned
on Euclidean spaces (Cohen & Welling, 2016; 2017; Worrall et al., 2017; Weiler et al., 2018b;a; Worrall &
Welling, 2019; Sosnovik et al., 2020; Cheng et al., 2019; Zhu et al., 2019; Weiler & Cesa, 2019; Hoogeboom
et al., 2018; Zhou et al., 2017; Worrall & Brostow, 2018; Kanazawa et al., 2014; Xu et al., 2014; Marcos
et al., 2018), manifolds (Cohen et al., 2018; 2019; Kondor et al., 2018; Deﬀerrard et al., 2020), point clouds
(Thomas et al., 2018; Chen et al., 2021; Zhao et al., 2020), and graphs (Kondor, 2018; Anderson et al., 2019;
Keriven & Peyré, 2019). In particular, equivariant CNNs for eitherrotation (Weiler & Cesa, 2019; Cheng
et al., 2019; Hoogeboom et al., 2018; Worrall et al., 2017; Zhou et al., 2017; Marcos et al., 2017; Weiler et al.,
2018b)orscaling (Kanazawa et al., 2014; Marcos et al., 2018; Xu et al., 2014; Worrall & Welling, 2019;
Sosnovik et al., 2020; Zhu et al., 2019; Sosnovik et al., 2021) transforms on 2D inputs have been well studied
separately , and their advantage has been empirically veriﬁed in settings where the data have rich variance in
either rotation or scale individually .
However, formanyvisiontasks, itisbeneﬁcialforamodelto simultaneously incorporatetranslation, rotation,
and scaling symmetry directly into its representation. For example, a self-driving vehicle is required to
recognizeandlocatepedestrians, objects, androadsignsunderrandomtranslation(e.g., movingpedestrians),
rotation(e.g., tiltedroadsigns), andscaling(e.g., closeanddistantobjects)(Bojarskietal.,2016). Moreover,
in realistic settings, symmetry transformations are rarely perfect; for instance, a tilted stop sign located
faraway can be modeled in reality as if it were transformed through a sequence of translation, rotation and
scaling following a local data deformation , which results from (unavoidable) changing view angle and/or
digitization. It is thus crucial to design roto-scale-translation equivariant CNNs ( RST-CNNs) with provably
robust equivariant representation such that the RSTsymmetry is still “approximately" preserved when
the transformation is “contaminated” by a nuisance data deformation. Such deformation robustness is
especially important for out-of-distribution generalization. However, the design of RST-equivariant CNNs
with theoretically guaranteed deformation robustness is challenging both in theory and in practice due to
the intertwined convolution on the non-compact RSTgroup with inﬁnite Haar measure.
The purpose of this paper is to address both the theoretical and practical aspects of constructing deformation
robustRST-CNNs, which, to the best of our knowledge, have not been jointly studied in the computer vision
community. Speciﬁcally, our contribution is three-fold:
1. We propose roto-scale-translation equivariant CNNs with joint convolutions over the space R2, the
rotation group SO(2), and the scaling group S, which is shown to be suﬃcient and necessary for
equivariance with respect to the regular representation of the group RST.
2. We provide a stability analysis of the proposed model, guaranteeing its ability to achieve equivariant
representations that are robust to nuisance data deformation.
3. Numerical experiments are conducted to demonstrate the superior (both in-distribution and out-of-
distribution) generalization performance of our proposed model for vision tasks with intrinsic RST
symmetry, especially in the small data regime.
2 Related Works
Group-equivariant CNNs (G-CNNs). Since its introduction by Cohen & Welling (2016), a variety of
works on G-CNNs have been conducted that consistently demonstrate the beneﬁts of bringing equivariance
prior into network designs. Based on the idea proposed in (Cohen & Welling, 2016) for discrete symmetry
groups, G-CNNs with group convolutions which achieve equivariance under regular representations of the
grouphavebeenstudiedforthe2D(and3D)roto-translationgroup SE(2)(andSE(3))(Weiler&Cesa,2019;
Cheng et al., 2019; Hoogeboom et al., 2018; Worrall et al., 2017; Zhou et al., 2017; Marcos et al., 2017; Weiler
etal., 2018b; Worrall &Brostow,2018), scaling-translation group (Kanazawaet al.,2014;Marcoset al.,2018;
Xuetal.,2014;Worrall&Welling,2019;Sosnoviketal.,2020;Zhuetal.,2019;Sosnoviketal.,2021), rotation
SO(3)on the sphere (Cohen et al., 2018; Kondor et al., 2018; Deﬀerrard et al., 2020), and permutation on
2Published in Transactions on Machine Learning Research (07/2022)
graphs (Kondor, 2018; Anderson et al., 2019; Keriven & Peyré, 2019). Polar transformer networks (Esteves
et al., 2017) generalizes group-equivariance to rotation and dilation. B-spline CNNs (Bekkers, 2020) and
LieConv (Finzi et al., 2020) generalize group convolutions to arbitrary Lie groups on generic spatial data,
albeit achieving slightly inferior performance compared to G-CNNs specialized for Euclidean inputs (Finzi
et al., 2020). Steerable CNNs further generalize the network design to realize equivariance under induced
representations of the symmetry group (Cohen & Welling, 2017; Weiler et al., 2018a; Weiler & Cesa, 2019;
Cohen et al., 2019), and the general theory has been summarized by Cohen et al. (2019) for homogeneous
spaces.
Representation robustness to input deformations. Input deformations typically introduce noticeable
yet uninformative variability within the data. Models that are robust to data deformation are thus favorable
for many vision applications. The scattering transform network (Bruna & Mallat, 2013; Mallat, 2010; 2012),
a multilayer feature encoder deﬁned by average pooling of wavelet modulus coeﬃcients, has been proved
to be stable to both input noise and nuisance deformation. Using group convolutions, scattering transform
has also been extended in (Oyallon & Mallat, 2015; Sifre & Mallat, 2013) to produce rotation/translation-
invariant features. Despite being a pioneering mathematical model, the scattering network uses pre-ﬁxed
wavelet transforms in the model, and is thus non-adaptive to the data. Stability and invariance have also
been studied in (Bietti & Mairal, 2017; 2019) for convolutional kernel networks (Mairal, 2016; Mairal et al.,
2014). DCFNet (Qiu et al., 2018) combines the regularity of a pre-ﬁxed ﬁlter basis and the trainability of the
expansion coeﬃcients, achieving both representation stability and data adaptivity. The idea is later adopted
in (Cheng et al., 2019) and (Zhu et al., 2019) in building deformation robust models that are equivariant to
either rotation or scaling individually.
Despite the growing body of literature in G-CNNs, to the best of our knowledge, no G-CNNs have been
speciﬁcally designed to simultaneously achieve roto-scale-translation equivariance. More importantly, no
stability analysis has been conducted to quantify and promote robustness of such equivariant model to
nuisance input deformation.
3 Roto-Scale-Translation Equivariant CNNs
We ﬁrst explain, in Section 3.1 , the deﬁnition of groups, group representations, and group-equivariance,
which serves as a background for constructing RST-CNNs in Section 3.2.
3.1 Preliminaries
Group. A group is a set Gequipped with a binary operator ·:G×G→G, satisfying associativity and
the existence of an identity eas well as an inverse element g−1for allg∈G. In this paper, we consider in
particular the roto-scale-translation group RST = (SO(2)×R)nR2={(η,β,v ) :η∈[0,2π],β∈R,v∈R2},
with the group multiplication
(η,β,v )·(θ,α,u ) = (θ+η,α+β,v+Rη2βu), (1)
whereRηuis a counterclockwise rotation (around the origin) by angle ηapplied to a point u∈R2.
Group action and representation. Given a group Gand a setX,Dg:X→Xis called aG-action on
XifDgis invertible for all g∈G, andDg1◦Dg2=Dg1·g2,∀g1,g2∈G, where◦denotes map composition.
AG-actionDgis called aG-representation ifXis further assumed to be a vector space and Dgis linear for
allg∈G. In particular, given an input RGB image x(0)(u,λ)modeled in the continuous setting (i.e., x(0)
is the intensity of the RGB color channel λ∈{1,2,3}at the pixel location u∈R2), a roto-scale-translation
transformation on the image x(0)(u,λ)can be understood as an RST-action (representation) D(0)
g=D(0)
η,β,v
acting on the input x(0):
[D(0)
η,β,vx(0)](u,λ) =x(0)/parenleftbig
R−η2−β(u−v),λ/parenrightbig
, (2)
i.e., the transformed image [Dη,β,vx(0)]is obtained through an ηrotation, 2βscaling, and vtranslation.
3Published in Transactions on Machine Learning Research (07/2022)
Group Equivariance. Letf:X→Ybe a map between XandY, andDX
g,DY
gbeG-actions onXandY
respectively. The map fis said to be G-equivariant if
f(DX
gx) =DY
g(f(x)),∀g∈G, x∈X. (3)
A special case of (3) is G-invariance, when DY
gis set to IdY, the identity map on Y. For vision tasks where
the output y∈Yis known a priori to transform covariantly through DY
gto aDX
gtransformed input, e.g.,
the class label yremains identical for a rotated/rescaled/shifted input x∈X, it is beneﬁcial to consider only
G-equivariant models fto reduce the statistical error of the learning method for improved generalization.
3.2 Equivariant Architecture
Since the composition of equivariant maps remains equivariant, to construct an L-layerRST-CNN, we only
need to specify the RST-actionD(l)
η,β,von each feature space X(l),0≤l≤L, and require the layer-wise
mapping to be equivariant:
x(l)[D(l−1)
η,β,vx(l−1)] =D(l)
η,β,vx(l)[x(l−1)],∀l≥1, (4)
where we slightly abuse the notation x(l)[x(l−1)]to denote the l-th layer output given the (l−1)-th layer
featurex(l−1). In particular, we deﬁne D(0)
η,β,von the input as in (2); for the hidden layers 1≤l≤L, we
letX(l)be the feature space consisting of features in the form of x(l)(u,θ,α,λ ), whereu∈R2is the spatial
position,θ∈[0,2π]is the rotation index, α∈Ris the scale index, λ∈[Ml]:={1,...,Ml}corresponds to
the unstructured channels (similar to the RGB channels of the input), and we deﬁne the action D(l)
η,β,von
X(l)as
[D(l)
η,β,vx(l)](u,θ,α,λ ) =x(l)/parenleftbig
R−η2−β(u−v),θ−η,α−β,λ/parenrightbig
,∀l≥1. (5)
We note that (5) corresponds to the regularrepresentation of RSTonX(l)(Cohen et al., 2019), which is
adopted in this work as its ability to encode any function on the group RSTleads typically to better model
expressiveness and stronger generalization performance (Weiler & Cesa, 2019). The following proposition
outlines the general network architecture to achieve RST-equivariance under the representations D(l)
η,β,v(2)
(5).
Proposition 1. AnL-layer feedforward neural network is RST-equivariant under the representations (2)
(5)if and only if the layer-wise operations are deﬁned as (6)and(7):
x(1)[x(0)](u,θ,α,λ ) =σ/bracketleftbigg/summationdisplay
λ/prime/integraldisplay
R2x(0)(u+u/prime,λ/prime)·2−2αW(1)
λ/prime,λ/parenleftbig
2−αR−θu/prime/parenrightbig
du/prime+b(1)(λ)/bracketrightbigg
, (6)
x(l)[x(l−1)](u,θ,α,λ )
=σ/bracketleftbigg/summationdisplay
λ/prime/integraldisplay
R2/integraldisplay
S1/integraldisplay
Rx(l−1)(u+u/prime,θ+θ/prime,α+α/prime,λ/prime)·2−2αW(l)
λ/prime,λ/parenleftbig
2−αR−θu/prime,θ/prime,α/prime/parenrightbig
dα/primedθ/primedu/prime+b(l)(λ)/bracketrightbigg
,(7)
whereσ:R→Ris a pointwise nonlinearity, W(1)
λ/prime,λ(u)is the spatial convolutional ﬁlter in the ﬁrst layer with
output channel λand input channel λ/prime,W(l)
λ/prime,λ(u,θ,α )is theRSTjoint convolutional ﬁlter for layer l >1,
and/integraltext
S1f(α)dαdenotes the normalized S1integral1
2π/integraltext2π
0f(α)dα.
We note that the joint-convolution (7) is the group convolution over RST(whose subgroup SO(2)×Ris a
non-compact Lie group acting on R2), which is known to achieve equivariance under regular representations
(5) (Cohen et al., 2019). We provide an elementary proof of Proposition 1 in the appendix for completeness.
4 Robust Equivariance to Input Deformation
Proposition 1 details the network architecture to achieve RST-equivariance for images modeled on the
continuous domain R2undergoing a “ perfect"RST-transformation (2). However, in practice, symmetry
4Published in Transactions on Machine Learning Research (07/2022)
Figure 1:RST-transformations in reality, e.g., the blue arrow, are rarely perfect, and they can be modeled
as a composition of input deformation Dτ[cf. (12)] and an exact RST-transformation D(0)
η,β,v.
transformations are rarely perfect, as they typically suﬀer from numerous source of input deformation coming
from, for instance, unstable camera position, change of weather, as well as practical issues such as numerical
discretization and truncation (see, for example, Figure 1.) We explain, in this section, how to quantify and
improve the representation stability of the model such that it stays “approximately" RST-equivariant even
if the input transformation is “contaminated" by minute local distortion (see, for instance, Figure 2.)
4.1 Decomposition of Convolutional Filters
In order to quantify the deformation stability of representation equivariance, motivated by (Qiu et al., 2018;
Cheng et al., 2019; Zhu et al., 2019), we leverage the geometry of the group RSTand decompose the
convolutional ﬁlters W(l)
λ,λ/prime(u,θ,α )under the separable product of three orthogonal function bases, {ψk(u)}k,
{ϕm(θ)}m, and{ξn(α)}n. In particular, we choose {ϕm}mas the Fourier basis on S1, and set{ψk}kand
{ξn}nto be the eigenfunctions of the Dirichlet Laplacian over, respectively, the unit disk D⊂R2and the
intervalIα= [−1,1], i.e.,
/braceleftBigg
∆ψk=−µkψkinD,
ψk= 0on∂D,/braceleftBigg
ξ/prime/prime
n=−νmξninIα
ξn(±1) = 0,(8)
whereµkandψnare the corresponding eigenvalues.
Remark 1. One has ﬂexibility in choosing the spatial function basis {ψk}k. We consider mainly, in this
work, the rotation-steerable Fourier-Bessel (FB) basis (Abramowitz & Stegun, 1965) deﬁned in (8), as the
spatial regularity of its low-frequency modes leads to robust equivariance to input deformation in an RST-
CNN, which will be shown in Theorem 1. One can also choose {ψk}kto be the eigenfunctions of Dirichlet
Laplacian over the cell [−1,1]2(Zhu et al., 2019), i.e., the separable product of the solutions to the 1D
Dirichlet Sturm–Liouville problem, which leads to a similar stability analysis. We denote such basis the
Sturm-Liouvielle (SL) basis, and its eﬃcacy will be compared to FB basis in Section 6.
Since spatial pooling can be modeled as rescaling the convolutional ﬁlters in space, we assume the ﬁlters
W(l)
λ/prime,λare compactly supported on a rescaled domain as follows
W(1)
λ/prime,λ∈Cc(2j1D), W(l)
λ/prime,λ∈Cc(2jlD×S1×Iα), (9)
wherejl<jl+1models a sequence of ﬁlters with decreasing size. Let ψj,k(u) = 2−2jψk(2−ju)be the rescaled
spatial basis function, and we can decompose W(l)
λ/prime,λunder{ψjl,k}k,{ϕm}m,{ξn}ninto
W(1)
λ/prime,λ(u) =/summationdisplay
ka(1)
λ/prime,λ(k)ψj1,k(u), W(l)
λ/prime,λ(u,θ,α ) =/summationdisplay
k,m,na(l)
λ/prime,λ(k,m,n )ψj1,k(u)ϕm(θ)ξn(α),(10)
wherea(l)
λ/prime,λare the expansion coeﬃcients of the ﬁlters. In practice, the ﬁlter expansion (10) are truncated
to only low-frequency components of the separable basis, which reduces the computation and memory cost
of the model. In addition, we will explain in Section 4.2 the eﬀect of the truncated ﬁlter expansion on the
stability of theRST-equivariant representation to input deformation.
5Published in Transactions on Machine Learning Research (07/2022)
4.2 Stability under Input Deformation
First, in order to gauge the distance between diﬀerent inputs and features, we deﬁne the layer-wise feature
norm as
/vextenddouble/vextenddouble/vextenddoublex(0)/vextenddouble/vextenddouble/vextenddouble2
=1
M0M0/summationdisplay
λ=1/integraldisplay
R2|x(0)(u,λ)|2du,/vextenddouble/vextenddouble/vextenddoublex(l)/vextenddouble/vextenddouble/vextenddouble2
= sup
α1
MlMl/summationdisplay
λ=1/integraldisplay/integraldisplay
S1×R2|x(l)(u,θ,α,λ )|2dudθ, (11)
forl≥1, i.e., the norm is a combination of an L2-norm over the roto-translation group SE(2)∼=S1×R2and
anL∞-norm over the scaling group S∼=R. We point out the importance of the L∞-norm inS, as signals
after the coupled convolution (7) generally do not vanish as α→−∞.
We next deﬁne the spatial deformation of an input image. Given a C2vector ﬁeld τ:R2→R2, the spatial
deformation Dτonx(0)is deﬁned as
Dτx(0)(u,λ) =x(0)(ρ(u),λ), (12)
whereρ(u) =u−τ(u). Thusτ(u)is understood as the local image distortion (at pixel location u), andDτ
is the identity map if τ(u)≡0, i.e., not input distortion.
The deformation stability of the equivariant representation can be quantiﬁed in terms of (11) after we make
the following three mild assumptions on the model and the input distortion Dτ:
(A1):The nonlinearity σ:R→Ris non-expansive, i.e., |σ(x)−σ(y)|≤|x−y|,∀x,y∈R. For instance,
the rectiﬁed linear unit (ReLU) satisﬁes this assumption.
(A2):The convolutional ﬁlters are bounded in the following sense: Al≤1,∀l≥1, where
A1:=πmax/braceleftBig
sup
λ/summationdisplay
λ/prime/bardbla(1)
λ/prime,λ/bardblFB,M0
M1sup
λ/prime/summationdisplay
λ/prime/bardbla(1)
λ/prime,λ/bardblFB/bracerightBig
Al:=πmax/braceleftBig
sup
λ/summationdisplay
λ/prime/summationdisplay
n/bardbla(l)
λ/prime,λ(·,n)/bardblFB,2Ml−1
Ml/summationdisplay
nsup
λ/prime/summationdisplay
λ/bardbla(l)
λ/prime,λ(·,n)/bardblFB/bracerightBig
,∀l>1,(13)
in which the FB-norm /bardbl·/bardbl FBof a sequence{a(k)}k≥0and double sequence {b(k,m)}k,m≥0is the weighted
l2-norm deﬁned as
/bardbla/bardbl2
FB=/summationdisplay
kµka(k)2,/bardblb/bardbl2
FB=/summationdisplay
k/summationdisplay
mµkb(k,m)2, (14)
µkbeing the eigenvalues deﬁned in (8). This technical assumption (A2)on the FB-norms of the expansion
coeﬃcients,/bardbla(l)
λ/prime,λ/bardblFB, ensurestheboundednessoftheconvolutionalﬁlters W(l)
λ/prime,λ(u,θ,α )undervariousnorms
(the exact details can be found in Lemma 2 of the appendix), which in turn quantiﬁes the stability of the
equivariant representation to input deformation. We point out that the boundedness of Alcan be facilitated
by truncating the expansion coeﬃcients to low-frequency components (i.e., small µk’s), which is the key idea
of ourRST-CNN (cf. Remark 3).
(A3):The local input distortion is small:
|∇τ|∞:= sup
u/bardbl∇τ(u)/bardbl<1/5, (15)
where/bardbl·/bardblis the operator norm.
Theorem 1 below quantiﬁes the deformation stability of the equivariant representation in an RST-CNN
under the assumptions (A1)-(A3):
Theorem 1. LetD(l)
η,β,v,0≤l≤L, be theRSTgroup actions deﬁned in (2)(5), and letDτbe a small
input deformation deﬁne in (12). If anRST-CNN satisﬁes the assumptions (A1)-(A3), we have, for any
L,
/vextenddouble/vextenddouble/vextenddoublex(L)[D(0)
η,β,v◦Dτx(0)]−D(L)
η,β,vx(L)[x(0)]/vextenddouble/vextenddouble/vextenddouble≤2β+1/parenleftbig
4L|∇τ|∞+ 2−jL|τ|∞/parenrightbig
/bardblx(0)/bardbl. (16)
6Published in Transactions on Machine Learning Research (07/2022)
Figure 2: Approximate RST-equivariance in the presence of nuisance input deformation Dτ.
The proof of Theorem 1 is deferred to the Appendix. An important message from Theorem 1 is that as long
as(A1)-(A3)are satisﬁed, the model stays approximately RST-equivariant, i.e., x(L)[D(0)
η,β,v◦Dτx(0)]≈
D(L)
η,β,vx(L)[x(0)], even with the presence of non-zero (yet small) input deformation Dτ(see, e.g., Figure 2.)
Remark 2. The fact that the right hand side of (16)grows exponentially with βis inevitable, as it comes
naturally from the deﬁnition of the norm (11): if an image is spatially rescaled by 2βwithout correspondingly
scaling the color range (i.e., pixel intensity), its L2-norm is enlarged by 2β.
Remark 3. According to the deﬁnition of the FB-norm (14), the main assumption (A2)can be facilitated by
truncating the ﬁlter expansion (10)to include only low-frequency (small µk) components. The implementation
detail of such truncated ﬁlter expansion will be explained in detail in Section 5.
5 Implementation Details
We next discuss the implementation details of the RST-CNN outlined in Proposition 1.
Discretization. To implementRST-CNN in practice, we ﬁrst need to discretize the features x(l)modeled
originally under the continuous setting. First, the input signal x(0)(u,λ)is discretized on a uniform grid into
a 3D array of shape [M0,H0,W0], whereH0,W0,M0, respectively, are the height, width, and the number
of the unstructured channels of the input (e.g., M0= 3for RGB images.) For l≥1, the rotation group
S1is uniformly discretized into Nrpoints; the scaling group S∼=R1, unlikeS1, is unbounded, and thus
featuresx(l)are computed and stored only on a truncated scale interval I= [−T,T]⊂R, which is uniformly
discretized into Nspoints. The feature x(l)is therefore stored as a 5D array of shape [Ml,Nr,Ns,Hl,Wl].
Filter expansion. The analysis in Section 4 suggests that robust RST-equivariance is achieved if the con-
volutional ﬁlters are expanded with only the ﬁrst Klow-frequency spatial modes {ψk}K
k=1. More speciﬁcally,
the ﬁrstKspatial basis functions as well as their rotated and rescaled versions {2−2αψk(2−αR−θu/prime)}k,θ,α,u/prime
are sampled on a uniform grid of size L×Land stored as an array of size [K,Nr,Ns,L,L ], which is ﬁxed
during training. The expansion coeﬃcients a(l)
λ/prime,λ, on the other hand, are the trainable parameters of the
model, which are used together with the ﬁxed basis to linearly expand the ﬁlters. The resulting ﬁlters
{2−2αW(1)
λ/prime,λ(2−αR−θu/prime)}λ/prime,λ,θ,α,u/primeand{2−2αW(l)
λ/prime,λ(2−αR−θu/prime,θ/prime,α/prime)}λ/prime,λ,θ,θ/prime,α,α/prime,u/primeare stored, respectively,
as tensors of size [M0,M1,Nr,Ns,L,L ]and[Ml−1,Ml,Nr,Lθ,Ns,Lα,L,L ], whereLαis the number of grid
points sampling the interval Iαin (8) (this is typically smaller than Ns, i.e., the number of the grid points
discretizing the scale interval I= [−T,T]), andLθis the number of grid points sampling S1on which the
integral/integraltext
S1dθin (7) is performed.
7Published in Transactions on Machine Learning Research (07/2022)
Remark 4. The number Lαmeasures the support Iα(8)of the convolutional ﬁlters in scale, which cor-
responds to the amount of “inter-scale" information transfer when performing the convolution over scale/integraltext
R(···)dα/primein(7). It is typically chosen to be a small number (e.g., 1 or 2) to avoid the “boundary leakage
eﬀect" (Worrall & Welling, 2019; Sosnovik et al., 2020; Zhu et al., 2019), as one needs to pad unknown
values beyond the truncated scale channel [−T,T]during convolution (7)whenLα>1. The number Lθ,
on the other hand, corresponds to the “inter-rotation" information transfer when performing the convolution
over the rotation group/integraltext
S1(···)dθ/primein(7); it does not have to be small since periodic-padding of known values
is adopted when conducting integrals on S1with no “boundary leakage eﬀect". We only require Lθto divide
Nrsuch that/integraltext
S1(···)dθ/primeis computed on a (potentially) coarser grid (of size Lθ) compared to the ﬁner grid
(of sizeNr) on which we discretize the rotation channel of the feature x(l).
Discrete convolution. After generating, in the previous step, the discrete joint convolutional ﬁlters to-
getherwiththeirrotatedandrescaledversions, thecontinuousconvolutionsinProposition1canbeeﬃciently
implemented using regular 2D discrete convolutions.
Morespeciﬁcally, let x(0)(u,λ)beaninputimageofshape [M0,H0,W0]. Atotalof Nr×Nsdiscrete2Dconvo-
lutionswiththerotatedandrescaledﬁlters {2−2αψk(2−αR−θu/prime)}k,θ,α,u/prime, i.e., replacingthespatialintegralsin
(7) by summations, are conducted to obtain the ﬁrst-layer feature x(1)(u,θ,α,λ )of size [M1,Nr,Ns,H1,W1].
For the subsequent layers, given a feature x(l−1)(u,θ,α,λ )of shape [Ml−1,Nr,Ns,Hl−1,Wl−1]and the joint
ﬁltersF(l)={2−2αW(l)
λ/prime,λ(2−αR−θu/prime,θ/prime,α/prime)}λ/prime,λ,θ,θ/prime,α,α/prime,u/primeof size [Ml−1,Ml,Nr,Lθ,Ns,Lα,L,L ], the next-
layer feature x(l)is computed in the following way: for each lα∈[0,Lα−1]andlθ∈[0,Lθ−1], we shift
the signalx(l−1)in the scale channel by lαand in the rotation channel by lθNr/Lθ, which is then convolved
with the ﬁlter F[:,:,:,lθ,:,lα,:,:](after proper reshaping and combining adjacent dimensions) to produce an
output array of shape [Ml,Nr,Ns,Hl,Wl]. Thel-th layer feature map x(l)(u,θ,α,λ )is then computed as
the sum of the Lθ×Lαtensors obtained by iterating over lθ∈[0,Lθ−1]andlα∈[0,Lα−1].
Group pooling. For learning tasks where the outputs are supposed to remain unchanged to RST-
transformed inputs, e.g., image classiﬁcation, a max-pooling over the entire group RSTis performed on
the last-layer feature x(L)(u,θ,α,λ )of shape [ML,Nr,Ns,HL,ML]to produce anRST-invariant 1D output
of lengthML. We only perform the RSTgroup-pooling in the last layer without explicit mention.
6 Numerical Experiments
We conduct numerical experiments, in this section, to demonstrate:
•The proposed model indeed achieves robust RST-equivariance under realistic settings.
•RST-CNN yields remarkable gains over prior arts in vision tasks with intrinsic RST-symmetry,
especially in the small data regime.
Software implementation of the experiments is included in the supplementary materials.
6.1 Data Sets and Models
We conduct the experiments on the Rotated-and-Scaled MNIST (RS-MNIST), Rotated-and-Scaled Fashion-
MNIST (RS-Fashion), SIM2MNIST (Esteves et al., 2017), as well as the STL-10 data sets (Coates et al.,
2011b).
RS-MNIST and RS-Fashion are constructed through randomly rotating (by an angle uniformly distributed
on[0,2π]) as well as rescaling (by a uniformly random factor from [0.3, 1]) the original MNIST (LeCun
et al., 1998) and Fashion-MNIST (Xiao et al., 2017a) images. The transformed images are zero-padded
back to a size of 28×28. We upsize the image to 56×56for better comparison of the models. In addition,
we also consider the noise-introduced SIM2MNIST data set, obtained from randomly rotating (uniformly on
[0,2π]), rescaling (uniformly on [1, 2.4]), and translating the MNIST images before upsampling to a spatial
dimension of 96×96. The resulting SIM2MNIST data set is split into 10K, 5K, 50K samples for training,
validation, and testing respectively.
8Published in Transactions on Machine Learning Research (07/2022)
(a) Equivariance error, K= 5.
 (b) Equivariance error, K= 10.
Figure 3: Layer-wise equivariance error (17) of the RST-CNN. Convolutional ﬁlters with varying number
Lαof “inter-scale" channels are constructed from Klow-frequency spatial modes {ψk}K
k=1. The equivariance
error is smaller when Kdecreases, verifying our theoretical analysis (Theorem 1) that truncated basis
expansion to low-frequency modes improves deformation robustness of the equivariant representation.
The STL-10 data set has 5,000 training and 8,000 testing RGB images of size 96×96belonging to 10 diﬀerent
classessuchascat, deer, anddog. Weusethisdatasettoevaluatediﬀerentmodelsunderbothin-distribution
(ID) and out-of-distribution (OOD) settings. More speciﬁcally, the training set remains unchanged, while
the testing set is either unaltered for ID testing, or randomly rotated (by an angle uniformly distributed on
[−π/2,π/2]) and rescaled (by a factor uniformly distributed on [0.8,1]) for OOD testing.
We evaluate the performance of the proposed RST-CNN against other models that are equivariant to either
roto-translation ( SE(2)) or scale-translation ( ST) of the inputs. The SE(2)-equivariant models consider in
this section include the Rotation Decomposed Convolutional Filters network (RDCF) (Cheng et al., 2019)
and the Rotation Equivariant Steerable Network (RESN) (Weiler et al., 2018b), which is shown to achieve
best performance among all SE(2)-equivariant CNNs in (Weiler & Cesa, 2019). The ST-equivariant models
includetheScaleEquivariantVectorFieldNetwork(SEVF)(Marcosetal.,2018), ScaleEquivariantSteerable
Network (SESN) (Sosnovik et al., 2020), and Scale Decomposed Convolutional Filters network (SDCF) (Zhu
et al., 2019).
6.2 Equivariance Error
We ﬁrst measure the RST-equivariance error of our proposed model with the presence of discretization
and scale channel truncation. More speciﬁcally, we construct a 5-layer RST-CNN with randomly initialized
expansion coeﬃcients a(l)
λ/prime,λtruncated to K= 5orK= 10low-frequency spatial (FB) modes {ψk}K
k=1. The
scale channel is truncated to [−1,1], which is uniformly discretized into Ns= 9points; the rotation group
S1is sampled on a uniform grid of size Nr= 8. TheRSTequivariance error is computed on random
RS-MNIST images, and measured in a relative L2sense at the scale α= 0and rotation θ= 0, with the
RST-action corresponding to the group element (η,β,v ) = (−π/2,−0.5,0), i.e.,
/vextenddouble/vextenddouble/parenleftbig
x(l)[Dη,β,vx(0)]−Dη,β,vx(l)[x(0)]/parenrightbig
(·,α,θ)/vextenddouble/vextenddouble
L2/vextenddouble/vextenddoubleDη,β,vx(l)[x(0)](·,α,θ)/vextenddouble/vextenddouble
L2(17)
We ﬁxLθ, i.e., the number of the “inter-rotation" channels corresponding to the “coarser" grid of S1for
discreteS1integration, to Lθ= 4, and examine the equivariance error induced by the “boundary leakage
eﬀect" with diﬀerent numbers Lαof the “inter-scale" channels [cf. Remark 4].
Figure 3 displays the equivariance error (17) of the RST-CNN at diﬀerent layers l∈{1,···,5}with varying
Lα∈{1,2,3}. It can be observed that the equivariance error is inevitable due to numerical discretization and
9Published in Transactions on Machine Learning Research (07/2022)
RS-MNIST test accuracy (%) RS-MNIST+ test accuracy (%)
Models Ntr= 2,000Ntr= 5,000Ntr= 2,000Ntr= 5,000
CNN 80.63±0.11 87.41±0.32 89.52±0.21 93.08±0.04
RESN 85.71±0.18 89.69±0.40 90.86±0.20 93.78±0.35
RESN+ 87.96±0.05 92.29±0.10 92.20±0.09 95.32±0.09
RDCF 86.79±0.12 90.46±0.33 90.40±0.31 94.84±0.15
RDCF+ 87.68±0.52 91.74±0.37 92.49±0.28 95.81±0.11
SEVF 85.76±0.38 90.29±0.37 89.97±0.42 93.47±0.18
SESN 84.58±0.29 90.19±0.39 90.33±0.30 93.40±0.29
SDCF 85.62±0.51 90.40±0.09 90.14±0.16 93.47±0.05
RST -CNN(FB) 89.16±0.32 93.19±0.29 92.58±0.35 96.33±0.26
RST -CNN(SL) 88.97±0.17 93.03±0.20 92.30±0.19 96.04±0.11
RST -CNN+(FB) 89.53±0.27 93.40±0.26 93.99±0.07 96.53±0.25
RST -CNN+(SL) 90.26±0.37 93.59±0.06 93.82±0.25 96.76±0.11
Table 1: Classiﬁcation accuracy on the RS-MNIST data set. Models are trained on Ntr=2K or 5K images
with spatial resolution 56×56. A plus sign “+” on the data, i.e., RS-MNIST+, is used to denote the presence
of data augmentation during training. A plus sign “+” on the model, e.g., RDCF+, denotes a larger network
with more “inter-rotation" correlation Lθ= 4[cf. Section 5]. The mean ±std of the test accuracy over ﬁve
independent trials are reported.
truncation as the model goes deeper. However, it can be mitigated by choosing a small Lα, i.e., less “inter-
scale" information transfer, to avoid the “boundary leakage eﬀect", or expanding the ﬁlters with a small
numberKof low-frequency spatial components {ψk}K
k=1, supporting our theoretical analysis Theorem 1.
Due to this ﬁnding, we will consider in the following experiments RST-CNNs with Lα= 1, which has the
additional beneﬁt of better model scalability due to the reduced scale channel convolution.
6.3 Image Classiﬁcation
We next demonstrate the superior performance of the proposed RST-CNN in image classiﬁcation under
settings where a large variation of rotation and scale is present in the test and/or the training data.
6.3.1 RS-MNIST, RS-Fashion and SIM2MNIST
We ﬁrst benchmark the performance of diﬀerent models on the RS-MNIST, and RS-Fashion data sets. We
generate ﬁve independent realizations of the rotated and rescaled data [cf. Secion 6.1], which are split into
Ntr=5,000 or 2,000 images for training, 2,000 images for validation, and 50,000 images for testing.
For fair comparison among diﬀerent models, we use a benchmark CNN with three convolutional and two
fully-connected layers as a baseline. Each hidden layer (i.e., the three convolutional and the ﬁrst fully-
connected layer) is followed by a batch-normalization, and we set the number of output channels of the
hidden layers to [32,63,95,256]. The size of the convolutional ﬁlters is set to 7×7for each layer in the
CNN. All comparing models (those in Table 1-4 without “+" after the name of the model) are built on
the same CNN baseline, and we keep the trainable parameters almost the same ( ∼500K) by modifying the
number of the unstructured channels. For models that are equivariant to rotation (including RESN, RDCF,
andRST-CNN), we set the number Nrof rotation channels to Nr= 8[cf. Section 5]; for scale-equivariant
models (including SESN, SDCF, and RST-CNN), the number Nsof scale channels is set to Ns= 4.
In addition, for rotation-equivariant CNNs, we also construct larger models (with the number of trainable
parameters∼1.6M) after increasing the “inter-rotation" information transfer [cf. Section 5] by setting Lθ= 4;
we attach a “+" symbol to the end of the model name (e.g., RDCF+) to denote such larger models with
more “inter-rotation". A group max-pooling is performed only after the ﬁnal convolutional layer to achieve
group-invariant representations for classiﬁcation. Moreover, for RST-CNN, we consider two diﬀerent spatial
10Published in Transactions on Machine Learning Research (07/2022)
RS-Fashion test accuracy (%) RS-Fashion+ test accuracy (%)
Models Ntr= 2,000Ntr= 5,000Ntr= 2,000Ntr= 5,000
CNN 62.71±0.37 67.91±0.28 67.92±0.12 72.41±0.46
RESN 70.80±0.41 75.80±0.11 74.10±0.46 77.76±0.16
RESN+ 71.59±0.71 76.32±0.26 76.80±0.55 80.89±0.41
RDCF 70.72±0.10 73.96±0.19 73.46±0.10 77.53±0.11
RDCF+ 71.27±0.34 75.94±0.35 76.87±0.57 80.66±0.48
SEVF 66.57±0.32 71.03±0.31 68.83±0.52 73.25±0.22
SESN 66.28±0.14 72.19±0.05 69.43±0.07 75.85±0.26
SDCF 66.29±0.23 72.24±0.23 68.40±0.05 75.11±0.18
RST -CNN(FB) 73.31±0.16 78.64±0.60 76.43±0.59 81.93±0.04
RST -CNN(SL) 72.90±0.34 78.37±0.22 76.06±0.13 80.81±0.29
RST -CNN+(FB) 74.37±0.08 79.19±0.36 80.65±0.31 84.37±0.19
RST -CNN+(SL) 74.68±0.29 79.81±0.06 80.65±0.46 84.09±0.09
Table 2: Classiﬁcation accuracy on the RS-Fashion data set. Models are trained on Ntr=2K or 5K images
with spatial resolution 56×56. A plus sign “+” on the data, i.e., RS-Fashion+, is used to denote the presence
of data augmentation during training. A plus sign “+” on the model, e.g., RDCF+, denotes a larger network
with more “inter-rotation" correlation Lθ= 4[cf. Section 5]. The mean ±std of the test accuracy over ﬁve
independent trials are reported.
Models CNN RDCF RESN SEVF SESN SDCF RST -CNNRST -CNN+
Accuracy (%) 86.6±0.14 89.3±0.37 89.2±0.36 87.31±0.05 87.86±0.31 88.1±0.62 93.34±0.13 94.81±0.12
Table 3: Classiﬁcation accuracy on the SIM2MNIST data set. The mean ±std of the test accuracy over
three independent trials are reported.
function basis for ﬁlter expansion, namely the Fourier-Bessel (FB) basis, and Sturm-Liouville (SL) basis [cf.
Remark 1].
We use the Adam optimizer (Kingma & Ba, 2014) to train all models for 60 epochs with the batch size set
to 128. We set the initial learning rate to 0.01, which is scheduled to decrease tenfold after 30 epochs. We
conduct the experiments in 4 diﬀerent settings, where the number Ntrof training samples is either 2,000 or
5,000, and the models are trained with or without RSTdata augmentation.
We report the mean ±std of the test accuracy after ﬁve independent trials in Table 1 and Table 2, where, for
example, RS-MNIST (or RS-MNIST+) denotes models are trained on the RS-MNIST data set without (or
with) data augmentation. It is clear from Table 1 and Table 2 that RST-CNN has superior generalization
capability compared to other models with approximately the same trainable parameters, especially in the
small data regime. Furthermore, RST-CNN+ with more inter-rotation correlation (i.e., Lθ= 4) has further
improved performance, achieving the best accuracy among all methods. It can also be observed that neither
of the two spatial basis functions (i.e., FB and SL) has signiﬁcant advantage over the other. The preferred
choice of the basis for ﬁlter expansion could depend on experimental settings including sample size, the
number of ﬁlters, original data characteristics and the presence (or lack thereof) of data augmentation.
Similarly, adopting the same hyper-parameter setting as the RS-MNIST experiment, we report the test
accuracy after three independent trails on the SIM2MNIST data set (Esteves et al., 2017) in Table 3. One
can observe that RST-CNN once again outperforms other models with approximately the same number of
parameters. In addition, RST-CNN+ with inter-rotation correlation ( Lθ= 4) further improves RST-CNN,
achieving the best performance among all comparing methods.
11Published in Transactions on Machine Learning Research (07/2022)
Models ID Accuracy (%) OOD Accuracy (%)
ResNet-16 82.66±0.53 37 .63±1.95
RESN 83.84±0.67 51 .28±2.29
RDCF 83.66±0.57 51 .12±4.21
SESN 83.79±0.24 47 .26±0.63
SDCF 83.83±0.41 43 .60±0.87
RST -ResNet 84.08±0.11 58.31 ±3.62
Table 4: Test accuracy on the STL-10 data set for both in-distribution (ID) and out-of-distribution (OOD)
settings.
6.3.2 STL-10
Finally, we use the STL-10 data set as an example to showcase both ID and OOD generalization capacity
of the proposedRST-ResNet. As explained in Section 6.1, the training set remains unaltered, while the
testing set is either (a) unchanged for ID testing, or (b) randomly rotated and rescaled for OOD testing. In
order for all models to be trained on a single GPU, we choose a ResNet (He et al., 2016) with 16 layers as the
baseline—this is diﬀerent from previous works such as (Sosnovik et al., 2020) where a WideResNet baseline
(Zagoruyko & Komodakis, 2016) is adopted—and we keep the number of trainable parameters almost the
same for comparing networks. A group max-pooling is performed after the ﬁnal residual block to achieve
invariant representations.
Similar to the idea of (Sosnovik et al., 2020; Zhu et al., 2019), the data set is augmented ( withoutrotation
and rescaling) during training by randomly cropping a 12 pixel zero-padded image. Furthermore, random
horizontal ﬂipping and Cutout (DeVries & Taylor, 2017) with 32 pixels are applied to the cropped image. We
train all models for 1000 epochs with a batch size of 64, using an SGD optimizer with Nesterov momentum
set to 0.9 and weight decay set to 5×10−4. Learning rate starts at 0.1, and is scheduled to decrease tenfold
after 300, 400, 600, and 800 epochs.
The mean±std after three independent trials of both ID and OOD test accuracy is displayed in Table 4. It
can be observed that the proposed RST-ResNet signiﬁcantly outperforms all comparing models, especially
for OOD generalization, demonstrating further its advantage in computer vision where both rotation and
scale transformations are intrinsic symmetries of the learning task.
7 Conclusion
In this paper, we have proposed the roto-scale-translation equivariant CNN ( RST-CNN), which is able to
achieve equivariance jointly over these three groups. Through truncated expansion of the joint convolutional
ﬁlters under pre-ﬁxed low-frequency spatial modes, which is motivated by a rigorous stability analysis of
the representation, the proposed model provably attains deformation robust equivariance, i.e., the features
stay “approximately" equivariant even if the RSTtransformation is “contaminated" by nuisance input
distortion, a property that is crucial for out-of-distribution model generalization. Experiments on vision
tasks with intrinsic RSTsymmetry are conducted to demonstrate the improved generalization capability of
our proposed model under both in-distribution and out-of-distribution setting, especially in the small data
regime.
One limitation of the current work is that we have considered deformation robust neural networks that are
equivariant to only the regularrepresentation of the group RST. Such models have empirically proved to
exhibit stronger generalization performance because of their ability to encode any function on the group.
However, regular representation requires high dimensional feature spaces, and the memory consumption
(mainly from storing intermediate features) of the proposed RST-CNN isNr×Nstimes that of a regular
CNN with the same number of unstructured channels. For future work, we will extend the idea in this
12Published in Transactions on Machine Learning Research (07/2022)
paper to construct deformation robust steerable RST-CNNs with reduced model size and eﬃcient network
implementation.
References
Milton Abramowitz and Irene A Stegun. Handbook of mathematical functions: with formulas, graphs, and
mathematical tables , volume 55. Courier Corporation, 1965. 5
Brandon Anderson, Truong Son Hy, and Risi Kondor. Cormorant: Covariant molecular neural networks.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d /quotesingle.ts1Alché-Buc, E. Fox, and R. Garnett (eds.), Advances
in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019. URL https://
proceedings.neurips.cc/paper/2019/file/03573b32b2746e6e8ca98b9123f2249b-Paper.pdf . 2, 3
Erik J Bekkers. B-spline cnns on lie groups. In International Conference on Learning Representations , 2020.
URL https://openreview.net/forum?id=H1gBhkBFDH . 3
Alberto Bietti and Julien Mairal. Invariance and stability of deep convolutional representations. In NIPS
2017-31st Conference on Advances in Neural Information Processing Systems , pp. 1622–1632, 2017. 3
Alberto Bietti and Julien Mairal. Group invariance, stability to deformations, and complexity of deep
convolutional representations. The Journal of Machine Learning Research , 20(1):876–924, 2019. 3
Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal,
Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning for self-driving
cars.arXiv preprint arXiv:1604.07316 , 2016. 2
Joan Bruna and Stéphane Mallat. Invariant scattering convolution networks. IEEE transactions on pattern
analysis and machine intelligence , 35(8):1872–1886, 2013. 3
Haiwei Chen, Shichen Liu, Weikai Chen, Hao Li, and Randall Hill. Equivariant point network for 3d point
cloud analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pp. 14514–14523, 2021. 2
Xiuyuan Cheng, Qiang Qiu, Robert Calderbank, and Guillermo Sapiro. RotDCF: Decomposition of convo-
lutional ﬁlters for rotation-equivariant deep networks. In International Conference on Learning Represen-
tations, 2019. 2, 3, 5, 9, 21
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature
learning. In Proceedings of the fourteenth international conference on artiﬁcial intelligence and statistics ,
pp. 215–223. JMLR Workshop and Conference Proceedings, 2011a. 25
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature
learning. In Proceedings of the fourteenth international conference on artiﬁcial intelligence and statistics ,
pp. 215–223. JMLR Workshop and Conference Proceedings, 2011b. 8
Taco Cohen and Max Welling. Group equivariant convolutional networks. In International conference on
machine learning , pp. 2990–2999, 2016. 2
Taco Cohen and Max Welling. Steerable CNNs. In International Conference on Learning Representations ,
2017. 2, 3
Taco S. Cohen, Mario Geiger, Jonas Köhler, and Max Welling. Spherical CNNs. In International Conference
on Learning Representations , 2018. 2
Taco S Cohen, Mario Geiger, and Maurice Weiler. A general theory of equivariant cnns on homogeneous
spaces. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d /quotesingle.ts1Alché-Buc, E. Fox, and R. Garnett (eds.),
Advances in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019. 2, 3, 4
13Published in Transactions on Machine Learning Research (07/2022)
Michaël Deﬀerrard, Martino Milani, Frédérick Gusset, and Nathanaël Perraudin. Deepsphere: a graph-
based spherical cnn. In International Conference on Learning Representations , 2020. URL https://
openreview.net/forum?id=B1e3OlStPB . 2
Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with
cutout.arXiv preprint arXiv:1708.04552 , 2017. 12
CarlosEsteves,ChristineAllen-Blanchette,XiaoweiZhou,andKostasDaniilidis. Polartransformernetworks.
arXiv preprint arXiv:1709.01889 , 2017. 3, 8, 11
Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing convolutional neural
networks for equivariance to lie groups on arbitrary continuous data. In International Conference on
Machine Learning , pp. 3165–3176. PMLR, 2020. 3
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016. 12
Emiel Hoogeboom, Jorn W.T. Peters, Taco S. Cohen, and Max Welling. Hexaconv. In International Con-
ference on Learning Representations , 2018. 2
Angjoo Kanazawa, Abhishek Sharma, and David Jacobs. Locally scale-invariant convolutional neural net-
works.arXiv preprint arXiv:1412.5104 , 2014. 2
Nicolas Keriven and Gabriel Peyré. Universal invariant and equivariant graph neural networks. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d /quotesingle.ts1Alché-Buc, E. Fox, and R. Garnett (eds.), Advances
in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019. URL https:
//proceedings.neurips.cc/paper/2019/file/ea9268cb43f55d1d12380fb6ea5bf572-Paper.pdf . 2, 3
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014. 11
Risi Kondor. N-body networks: a covariant hierarchical neural network architecture for learning atomic
potentials. arXiv preprint arXiv:1803.01588 , 2018. 2, 3
Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch–gordan nets: a fully fourier space spherical con-
volutional neural network. Advances in Neural Information Processing Systems , 31:10117–10126, 2018.
2
Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with deep convolutional
neural networks. In Advances in neural information processing systems , pp. 1097–1105, 2012. 1
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/ , 1998. 25
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haﬀner. Gradient-based learning applied to docu-
ment recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998. 8
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmenta-
tion. InThe IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2015. 1
Julien Mairal. End-to-end kernel learning with supervised convolutional kernel networks. In D. Lee,
M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Process-
ing Systems , volume 29. Curran Associates, Inc., 2016. 3
Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid. Convolutional kernel networks. In
Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural
Information Processing Systems , volume 27. Curran Associates, Inc., 2014. 3
Stéphane Mallat. Recursive interferometric representation. In Proc. of EUSICO conference, Danemark ,
2010. 3
14Published in Transactions on Machine Learning Research (07/2022)
Stéphane Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics , 65(10):
1331–1398, 2012. 3
Diego Marcos, Michele Volpi, Nikos Komodakis, and Devis Tuia. Rotation equivariant vector ﬁeld networks.
InProceedings of the IEEE International Conference on Computer Vision , pp. 5048–5057, 2017. 2
Diego Marcos, Benjamin Kellenberger, Sylvain Lobry, and Devis Tuia. Scale equivariance in CNNs with
vector ﬁelds. arXiv preprint arXiv:1807.11783 , 2018. 2, 9
Edouard Oyallon and Stéphane Mallat. Deep roto-translation scattering for object classiﬁcation. In Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 2865–2873, 2015.
3
Qiang Qiu, Xiuyuan Cheng, Robert Calderbank, and Guillermo Sapiro. DCFNet: Deep neural network
with decomposed convolutional ﬁlters. In Proceedings of the 35th International Conference on Machine
Learning , volume 80 of Proceedings of Machine Learning Research , pp. 4198–4207, Stockholmsmässan,
Stockholm Sweden, 10–15 Jul 2018. PMLR. 3, 5
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection
with region proposal networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett
(eds.),Advances in Neural Information Processing Systems 28 , pp. 91–99. Curran Associates, Inc., 2015.
1
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image
segmentation. In Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F. Frangi (eds.),
Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015 , pp.234–241, Cham, 2015.
Springer International Publishing. ISBN 978-3-319-24574-4. 1
Laurent Sifre and Stéphane Mallat. Rotation, scaling and deformation invariant scattering for texture
discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.
1233–1240, 2013. 3
Ivan Sosnovik, Michał Szmaja, and Arnold Smeulders. Scale-equivariant steerable networks. In International
Conference on Learning Representations , 2020. 2, 8, 9, 12, 25
Ivan Sosnovik, Artem Moskalev, and Arnold Smeulders. Disco: accurate discrete scale convolutions. arXiv
preprint arXiv:2106.02733 , 2021. 2
Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoﬀ, and Patrick Riley. Tensor
ﬁeld networks: Rotation-and translation-equivariant neural networks for 3d point clouds. arXiv preprint
arXiv:1802.08219 , 2018. 2
Maurice Weiler and Gabriele Cesa. General e(2)-equivariant steerable cnns. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d /quotesingle.ts1Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing
Systems, volume 32. Curran Associates, Inc., 2019. 2, 3, 4, 9
Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco Cohen. 3d steerable cnns: Learn-
ing rotationally equivariant features in volumetric data. In Advances in Neural Information Processing
Systems, pp. 10381–10392, 2018a. 2, 3
Maurice Weiler, Fred A Hamprecht, and Martin Storath. Learning steerable ﬁlters for rotation equivariant
cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 849–858,
2018b. 2, 9
Daniel Worrall and Gabriel Brostow. Cubenet: Equivariance to 3D rotation and translation. In Proceedings
of the European Conference on Computer Vision (ECCV) , pp. 567–584, 2018. 2
Daniel Worrall and Max Welling. Deep scale-spaces: Equivariance over scale. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d /quotesingle.ts1Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing
Systems, volume 32. Curran Associates, Inc., 2019. 2, 8
15Published in Transactions on Machine Learning Research (07/2022)
Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Harmonic networks:
Deep translation and rotation equivariance. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pp. 5028–5037, 2017. 2
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms, 2017a. 8
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747 , 2017b. 25
Yichong Xu, Tianjun Xiao, Jiaxing Zhang, Kuiyuan Yang, and Zheng Zhang. Scale-invariant convolutional
neural networks. arXiv preprint arXiv:1411.6369 , 2014. 2
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 , 2016.
12
Yongheng Zhao, Tolga Birdal, Jan Eric Lenssen, Emanuele Menegatti, Leonidas Guibas, and Federico
Tombari. Quaternion equivariant capsule networks for 3d point clouds. In Andrea Vedaldi, Horst Bischof,
Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision – ECCV 2020 , pp. 1–19, Cham, 2020.
Springer International Publishing. ISBN 978-3-030-58452-8. 2
Yanzhao Zhou, Qixiang Ye, Qiang Qiu, and Jianbin Jiao. Oriented response networks. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition , pp. 519–528, 2017. 2
Wei Zhu, Qiang Qiu, Robert Calderbank, Guillermo Sapiro, and Xiuyuan Cheng. Scale-equivariant neural
networks with decomposed convolutional ﬁlters. arXiv preprint arXiv:1909.11193 , 2019. 2, 3, 5, 8, 9, 12,
20, 22, 24, 25
A Appendix
B Proof of Proposition 1
We ﬁrst recall for an input RGB image x(0)(u,λ), a roto-scale-translation transformation on this image can
be understood as an RST-action (representation) D(0)
g=D(0)
η,β,vonx(0).
[D(0)
η,β,vx(0)](u,λ) =x(0)/parenleftbig
R−η2−β(u−v),λ/parenrightbig
. (18)
For the hidden layers 1≤l≤L, the action D(l)
η,β,von the spaceX(l)consisting of features in the form of
x(l)(u,θ,α,λ )is deﬁned as
[D(l)
η,β,vx(l)](u,θ,α,λ ) =x(l)/parenleftbig
R−η2−β(u−v),θ−η,α−β,λ/parenrightbig
,∀l≥1. (19)
We restate below Proposition 1 of the paper that outlines the general network architecture to achieve RST-
equivariance under the representations D(l)
η,β,v,0≤l≤L.
Proposition 1. AnL-layer feedforward neural network is RST-equivariant under the representations (18)
(19)if and only if the layer-wise operations are deﬁned as (20)and(21):
x(1)[x(0)](u,θ,α,λ ) =σ/bracketleftbigg/summationdisplay
λ/prime/integraldisplay
R2x(0)(u+u/prime,λ/prime)·2−2αW(1)
λ/prime,λ/parenleftbig
2−αR−θu/prime/parenrightbig
du/prime+b(1)(λ)/bracketrightbigg
,(20)
x(l)[x(l−1)](u,θ,α,λ ) =σ/bracketleftbigg/summationdisplay
λ/prime/integraldisplay
R2/integraldisplay
S1/integraldisplay
Rx(l−1)(u+u/prime,θ+θ/prime,α+α/prime,λ/prime)
·2−2αW(l)
λ/prime,λ/parenleftbig
2−αR−θu/prime,θ/prime,α/prime/parenrightbig
dα/primedθ/primedu/prime+b(l)(λ)/bracketrightbigg
(21)
whereσ:R→Ris a pointwise nonlinearity, W(1)
λ/prime,λ(u)is the spatial convolutional ﬁlter in the ﬁrst layer with
output channel λand input channel λ/prime,W(l)
λ/prime,λ(u,θ,α )is theRSTjoint convolutional ﬁlter for layer l >1,
and/integraltext
S1f(α)dαdenotes the normalized S1integral1
2π/integraltext2π
0f(α)dα.
16Published in Transactions on Machine Learning Research (07/2022)
B.1 Suﬃcient Part of the Proof of Proposition 1
Proof.We ﬁrst prove the suﬃcient part of Proposition 1. That is, given the layer-wise deﬁnition of the
L-layer feedforward neural network (20) and (21), RST-equivariance under the regular representation (18)
(19) can be achieved, i.e.,
x(l)[D(l−1)
η,β,vx(l−1)] =D(l)
η,β,vx(l)[x(l−1)],∀l≥1. (22)
Indeed, for the ﬁrst layer, i.e., l= 1, we write the LHS of (22) as
x(1)[D(0)
η,β,vx(0)](u,θ,α,λ ) =σ/parenleftBigg/summationdisplay
λ/prime/integraldisplay
R22−2αD(0)
η,β,vx(0)(u+u/prime,λ/prime)W(1)
λ/prime,λ(2−αR−θu/prime)du/prime+b(1)(λ)/parenrightBigg
=σ/parenleftBigg/summationdisplay
λ/prime/integraldisplay
R22−2αx(0)/parenleftbig
R−η2−β(u+u/prime−v),λ/prime/parenrightbig
W(1)
λ/prime,λ(2−αR−θu/prime)du/prime+b(1)(λ)/parenrightBigg
, (23)
The RHS of (22) when l= 1is
D(1)
η,β,vx(1)[x(0)](u,θ,α,λ ) =x(1)[x(0)]/parenleftbig
R−η2−β(u−v),θ−η,α−β,λ/parenrightbig
=σ/parenleftBigg/summationdisplay
λ/prime/integraldisplay
R22−2(α−β)x(0)/parenleftbig
R−η2−β(u−v) + ˜u,λ/prime/parenrightbig
W(1)
λ/prime,λ(2−(α−β)Rη−θ˜u)d˜u+b(1)(λ)/parenrightBigg
=σ/parenleftBigg/summationdisplay
λ/prime/integraldisplay
R22−2α22βx(0)/parenleftbig
R−η2−β(u−v) + 2−βR−ηu/prime,λ/prime/parenrightbig
W(1)
λ/prime,λ(2−αR−θu/prime)2−2βdu/prime+b(1)(λ)/parenrightBigg
(24)
=σ/parenleftBigg/summationdisplay
λ/prime/integraldisplay
R22−2αx(0)/parenleftbig
R−η2−β(u+u/prime−v),λ/prime/parenrightbig
W(1)
λ/prime,λ(2−αR−θu/prime)du/prime+b(1)(λ)/parenrightBigg
, (25)
where (24) uses the change of variable u/prime= 2−βR−η˜u. Thus (23) and (25) implies that (22) holds valid for
l= 1.
Forl>1, with the deﬁnition of the l-th layer operation (21), the LHS of (22) becomes
x(l)[D(l−1)
η,β,vx(l−1)](u,θ,α,λ )
=σ/parenleftBigg/summationdisplay
λ/prime/integraldisplay /integraldisplay /integraldisplay
2−2αD(l−1)
η,β,vx(l−1)(u+u/prime,θ+θ/prime,α+α/prime,λ/prime)W(l)
λ/prime,λ/parenleftbig
2−αR−θu/prime,θ/prime,α/prime/parenrightbig
dα/primedθ/primedu/prime+b(l)(λ)/parenrightBigg
(26)
=σ/parenleftbigg/summationdisplay
λ/prime/integraldisplay /integraldisplay /integraldisplay
2−2αx(l−1)/parenleftbig
R−η2−β(u+u/prime−v),θ+θ/prime−η,α+α/prime−β,λ/prime/parenrightbig
·W(l)
λ/prime,λ/parenleftbig
2−αR−θu/prime,θ/prime,α/prime/parenrightbig
dα/primedθ/primedu/prime+b(l)(λ)/parenrightbigg
.(27)
17Published in Transactions on Machine Learning Research (07/2022)
On the other hand, the RHS of (22) is
D(l)
η,β,vx(l)[x(l−1)](u,θ,α,λ )
=x(l)[x(l−1)]/parenleftbig
R−η2−β(u−v),θ−η,α−β,λ/parenrightbig
(28)
=σ/parenleftBigg/summationdisplay
λ/prime/integraldisplay /integraldisplay /integraldisplay
2−2(α−β)x(l−1)/parenleftbig
R−η2−β(u−v) + ˜u,θ−η+θ/prime,α−β+α/prime,λ/prime/parenrightbig
·W(l)
λ/primeλ/parenleftBig
2−(α−β)Rη−θ˜u,θ/prime,α/prime/parenrightBig
dα/primedθ/primed˜u+b(l)(λ)/parenrightBigg
(29)
=σ/parenleftBigg/summationdisplay
λ/prime/integraldisplay /integraldisplay /integraldisplay
2−2α22βx(l−1)/parenleftbig
R−η2−β(u−v) +R−η2−βu/prime,θ−η+θ/prime,α−β+α/prime,λ/prime/parenrightbig
·W(l)
λ/prime,λ/parenleftbig
2−αR−θu/prime,θ/prime,α/prime/parenrightbig
2−2βdα/primedθ/primed˜u+b(l)(λ)/parenrightBigg
, (30)
where the last equation again uses the same change of variable. Equation (27) combined with (30) implies
that (22) holds true for all l>1.
B.2 Necessary Part of the Proof of Proposition 1
Proof.We ﬁrst note that a general feedforward neural network propagating through the feature spaces X(l)
has the following form: when l= 1,
x(1)[x(0)](u,θ,α,λ ) =σ/parenleftbigg/summationdisplay
λ/prime/integraldisplay
R2x(0)(u+u/prime)W(1)(u/prime,λ/prime,u,θ,α,λ )du/prime+b(1)(λ)/parenrightbigg
, (31)
and, forl>1,
x(l)[x(l−1)](u,θ,α,λ ) =σ/parenleftbigg/summationdisplay
λ/prime/integraldisplay
R2/integraldisplay
S1/integraldisplay
Rx(l−1)(u+u/prime,θ+θ/prime,α+α/prime,λ/prime)
·W(l)(u/prime,θ/prime,α/prime,λ/prime,u,θ,α,λ )dα/primedθ/primedu/prime+b(l)(λ)/parenrightbigg
.(32)
To prove the necessary part of Proposition 1, we want to verify that RST-equivariance (22) implies that the
weight matrices W(l)in (31) and (32) take the special convolutional form in (20) and (21).
Indeed, when l= 1, the RHS of (22) under the operation (31) is
D(1)
η,β,vx(1)[x(0)](u,θ,α,λ ) (33)
=x(1)[x(0)]/parenleftbig
R−η2−β(u−v),θ−η,α−β,λ/parenrightbig
(34)
=σ/parenleftBigg/summationdisplay
λ/prime/integraldisplay
x(0)(R−η2−β(u−v) + ˜u,λ/prime)W(1)(˜u,λ/prime,R−η2−β(u−v),θ−η,α−β,λ)d˜u+b(1)(λ)/parenrightBigg
(35)
=σ/parenleftBigg/summationdisplay
λ/prime/integraldisplay
2−2βx(0)(R−η2−β(u+u/prime−v),λ/prime)
·W(1)/parenleftbig
R−η2−βu/prime,λ/prime,R−η2−β(u−v),θ−η,α−β,λ/parenrightbig
du/prime+b(1)(λ)/parenrightbigg
, (36)
18Published in Transactions on Machine Learning Research (07/2022)
with change of variable similar to the suﬃcient part. For the RHS, we have
x(1)[D(0)
η,β,vx(0)]](u,θ,α,λ ) (37)
=σ/parenleftBigg/summationdisplay
λ/prime/integraldisplay
D(1)
η,β,vx(0)(u+u/prime,λ/prime)W(1)(u/prime,λ/prime,u,θ,α,λ )du/prime+b(1)(λ)/parenrightBigg
(38)
=σ/parenleftBigg/summationdisplay
λ/prime/integraldisplay
x(0)(R−η2−β(u+u/prime−v),λ/prime)W(1)(u/prime,λ/prime,u,θ,α,λ )du/prime+b(1)(λ)/parenrightBigg
(39)
Hence, for (22) to hold when l= 1, we need
W(1)(u/prime,λ/prime,u,θ,α,λ ) = 2−2βW(1)(R−η2−βu/prime,λ/prime,R−η2−β(u−v),θ−η,α−β,λ), (40)
For allu,θ,α,λ,u/prime,λ/prime,v,η,β. Keeping u,θ,α,λ,u/prime,λ/prime,η,βﬁxed while varying vin (40), we deduce
thatW(1)(u/prime,λ/prime,u,θ,α,λ )does not depend on the third variable u. HenceW(1)(u/prime,λ/prime,u,θ,α,λ ) =
W(1)(u/prime,λ/prime,0,θ,α,λ ),∀u∈R2. Further deﬁne W(1)
λ,λ/prime(u/prime)as
W(1)
λ,λ/prime(u/prime) =W(1)(u/prime,λ/prime,0,0,0,λ). (41)
Therefore, for any ﬁxed u/prime,λ/prime,u,θ,α,λ , settingβ=α,η=θin (40) yields
W(1)(u/prime,λ/prime,u,θ,α,λ ) =W(1)(R−θ2−αu/prime,λ/prime,R−θ2−α(u−v),θ−θ,α−α,λ)2−2α(42)
=W(1)(R−θ2−αu/prime,λ/prime,0,0,0,λ)2−2α=W(1)
λ,λ/prime(R−θ2−αu/prime)2−2α(43)
Hence (31) has the special form (20).
For the subsequent layers l>1, a similar argument yields
W(l)(u/prime,θ/prime,α/prime,λ/prime,u,θ,α,λ ) =W(l)(R−η2−βu/prime,θ/prime,α/prime,λ/prime,R−η2−β(u−v),θ−η,α−β,λ)2−2β,(44)
for allu,θ,α,λ,u/prime,θ/prime,α/prime,λ/prime,v,η,β. Similarly, we can keep u,θ,α,λ,u/prime,θ/prime,α/prime,λ/prime,η,βﬁxed while varying vin
(44), which implies that W(l)(u/prime,θ/prime,α/prime,λ/prime,u,θ,α,λ )does not depend on the ﬁfth variable u. Again, let us
deﬁne
W(l)
λ,λ/prime(u/prime,θ/prime,α/prime) =W(l)(u/prime,θ/prime,α/prime,λ/prime,0,0,0,λ). (45)
For any given u/prime,θ/prime,α/prime,λ/prime,u,θ,α,λ , settingβ=α,η=θleads us to
W(l)(u/prime,θ/prime,α/prime,λ/prime,u,θ,α,λ ) =W(l)(R−θ2−αu/prime,θ/prime,α/prime,λ/prime,R−θ2−α(u−v),0,0,λ)2−2α
=W(l)(R−θ2−αu/prime,θ/prime,α/prime,λ/prime,0,0,0,λ)2−2α=W(l)
λ/prime,λ(2−αu/prime,θ/prime−θ,α/prime−α), (46)
which implies that (32) can be written in the form of (21). This concludes the proof of Proposition 1.
C Proof of Theorem 1
We prove, in this section, the deformation stability of the RST-CNN (Theorem 1 of the paper) under the
following three assumptions:
(A1):The pointwise nonlinearity σ:R→Ris non-expansive, i.e., |σ(x)−σ(y)|≤|x−y|,∀x,y∈R.
(A2):The convolutional ﬁlters are bounded in the following sense: Al≤1,∀l≥1, where
A1:=πmax/braceleftBig
sup
λ/summationdisplay
λ/prime/bardbla(1)
λ/prime,λ/bardblFB,M0
M1sup
λ/prime/summationdisplay
λ/prime/bardbla(1)
λ/prime,λ/bardblFB/bracerightBig
(47)
Al:=πmax/braceleftBig
sup
λ/summationdisplay
λ/prime/summationdisplay
n/bardbla(l)
λ/prime,λ(·,n)/bardblFB,2Ml−1
Ml/summationdisplay
nsup
λ/prime/summationdisplay
λ/bardbla(l)
λ/prime,λ(·,n)/bardblFB/bracerightBig
,∀l>1,(48)
19Published in Transactions on Machine Learning Research (07/2022)
in which the FB-norm /bardbl·/bardbl FBof a sequence{a(k)}k≥0and double sequence {b(k,m)}k,m≥0is the weighted
l2-norm deﬁned as
/bardbla/bardbl2
FB=/summationdisplay
kµka(k)2,/bardblb/bardbl2
FB=/summationdisplay
k/summationdisplay
mµkb(k,m)2, (49)
withµkbeing the eigenvalues of the Dirichlet Laplacian on a unit disk.
(A3):The input distortion is small. More speciﬁcally, let
Dτx(0)(u,λ) =x(0)(ρ(u),λ),andDτx(l)(u,θ,α,λ ) =x(l)(ρ(u),θ,α,λ ), l≥1, (50)
whereρ(u) =u−τ(u), andτ:R2→R2is aC2local (spatial) distortion. We assume
|∇τ|∞:= sup
u/bardbl∇τ(u)/bardbl<1/5, (51)
with/bardbl·/bardblbeing the operator norm.
We repeat below Theorem 1 of the paper that quantiﬁes the deformation stability of the equivariant repre-
sentation in anRST-CNN under the assumptions (A1)-(A3):
Theorem 2. LetD(l)
η,β,v,0≤l≤L, be theRSTgroup actions deﬁned in (18)(19), and letDτbe a small
input deformation deﬁne in (50). If anRST-CNN satisﬁes the assumptions (A1)-(A3), we have, for any
L,
/vextenddouble/vextenddouble/vextenddoublex(L)[D(0)
η,β,v◦Dτx(0)]−D(L)
η,β,vx(L)[x(0)]/vextenddouble/vextenddouble/vextenddouble≤2β+1/parenleftbig
4L|∇τ|∞+ 2−jL|τ|∞/parenrightbig
/bardblx(0)/bardbl, (52)
where the layer-wise feature norm is deﬁned as as


/vextenddouble/vextenddouble/vextenddoublex(0)/vextenddouble/vextenddouble/vextenddouble2
=1
M0M0/summationdisplay
λ=1/integraldisplay
R2|x(0)(u,λ)|2du,
/vextenddouble/vextenddouble/vextenddoublex(l)/vextenddouble/vextenddouble/vextenddouble2
= sup
α1
MlMl/summationdisplay
λ=1/integraldisplay/integraldisplay
S1×R2|x(l)(u,θ,α,λ )|2dudθ,∀l≥1(53)
The proof of Theorem 1 follows the similar steps in (Zhu et al., 2019). More speciﬁcally, we aim to establish
the following two propositions that show the layer-wise non-expansiveness of the model (Proposition 2) and
quantify the perturbation of equivariance with the presence of layer-wise spatial deformation Dτ(Proposi-
tion 3).
Proposition 2. Under (A1)and(A2), anRST-CNN satisﬁes:
(a) For any l≥1, thel-th layer mapping x(l)[·]deﬁned in (21)is non-expansive, i.e.,
/bardblx(l)[x1]−x(l)[x2]/bardbl≤/bardblx1−x2/bardbl,∀x1,x2. (54)
(b) Letx(l)
0(u,θ,α,λ )be thel-th layer output given a zero input x(0)(u,λ) = 0, thenx(l)
0(u,θ,α,λ )
depends only on λ, i.e.,x(l)
0(u,θ,α,λ ) =x(l)
0(λ).
(c) Letx(l)
cbe the centered version of x(l)after subtracting x(l)
0, i.e.,
x(0)
c(u,λ):=x(0)(u,λ)−x(0)
0(λ) =x(0)(u,λ), x(l)
c(u,θ,α,λ ):=x(l)(u,θ,α,λ )−x(l)
0(λ), l≥1,
(55)
then/bardblx(l)
c/bardbl≤/bardblx(l−1)
c/bardbl,∀l≥1. As a result,/bardblx(l)
c/bardbl≤/bardblx(0)
c/bardbl=/bardblx(0)/bardbl.
Proposition 3. In anRST-CNN satisfying (A1)to(A3), the following statements hold true.
20Published in Transactions on Machine Learning Research (07/2022)
(a) Given any l≥1,
/vextenddouble/vextenddouble/vextenddoublex(l)[Dτx(l−1)]−Dτx(l)[x(l−1)]/vextenddouble/vextenddouble/vextenddouble≤8|∇τ|∞/vextenddouble/vextenddouble/vextenddoublex(l−1)
c/vextenddouble/vextenddouble/vextenddouble, (56)
wherex(l−1)
cis deﬁned in (55)
(b) Given any l≥1, we have
/vextenddouble/vextenddouble/vextenddoubleD(l)
η,β,vx(l)/vextenddouble/vextenddouble/vextenddouble= 2β/vextenddouble/vextenddouble/vextenddoublex(l)/vextenddouble/vextenddouble/vextenddouble, (57)
and
/vextenddouble/vextenddouble/vextenddoublex(l)[D(l−1)
η,β,v◦Dτx(l−1)]−D(l)
η,β,vDτx(l)[x(l−1]/vextenddouble/vextenddouble/vextenddouble≤2β+3|∇τ|∞/vextenddouble/vextenddouble/vextenddoublex(l−1)
c/vextenddouble/vextenddouble/vextenddouble. (58)
(c) For any l≥1,
/vextenddouble/vextenddouble/vextenddoublex(l)[D(0)
η,β,v◦Dτx(l−1)]−D(l)
η,β,vDτx(l)[x(0]/vextenddouble/vextenddouble/vextenddouble≤2β+3l|∇τ|∞/vextenddouble/vextenddouble/vextenddoublex(0)
c/vextenddouble/vextenddouble/vextenddouble. (59)
(d) For any l≥1,
/vextenddouble/vextenddouble/vextenddoubleD(l)
η,β,vDτx(l)−D(l)
η,β,vx(l)/vextenddouble/vextenddouble/vextenddouble≤2β+1−jl|τ|∞/vextenddouble/vextenddouble/vextenddoublex(l−1)
c/vextenddouble/vextenddouble/vextenddouble≤2β+1−jl|τ|∞/vextenddouble/vextenddouble/vextenddoublex(0)/vextenddouble/vextenddouble/vextenddouble. (60)
C.1 Proof of Proposition 2
Before proving Proposition 2, we present the following two lemmas that are crucial to bound various norms
of the convolutional ﬁlters using their Fourier-Bessle (FB) norm
Lemma 1. Let{ψk}kbe the Fourier-Bessel basis on the unit disk D⊂R2, and let{ϕm}mbe the Fourier
basis on the unit circle S1. Assume that
F(u) =/summationdisplay
ka(k)ψk(u), G (u,θ) =/summationdisplay
k/summationdisplay
mb(k,m)ψk(u)ϕm(θ) (61)
are functions in H1
0(2jD)andH1
0(2jD)×L2(S1), respectively. Then
/integraldisplay
|F(u)|du,/integraldisplay
|u||∇F(u)|du,/integraldisplay
|∇F(u)|du≤π/bardbla/bardblFB=π/parenleftBigg/summationdisplay
kµka(k)2/parenrightBigg1/2
, (62)
/integraldisplay/integraldisplay
|G(u,θ)|dudθ,/integraldisplay/integraldisplay
|u||∇uG(u,θ)|dudθ,/integraldisplay/integraldisplay
|∇uG(u)|dudθ≤π/bardblb/bardblFB=π
/summationdisplay
k,mµkb(k,m)2
1/2
.
(63)
The proof of Lemma 1 can be found in Proposition A.1 of (Cheng et al., 2019). A direct application of
Lemma 1 leads to the following lemma.
Lemma 2. Leta(l)
λ/prime,λ(k,m,n )be the coeﬃcients of the ﬁlter W(l)
λ/prime,λ(u,θ,α )(supported on 2jlD×S1×[−1,1])
under the separable bases {ψjl,k(u)}k,{ϕm(θ)}mand{ξn(α)}ndeﬁned in the main paper, and deﬁne
W(l)
λ/prime,λ,n(u,θ)as
W(l)
λ/prime,λ,n(u,θ):=/summationdisplay
k/summationdisplay
ma(l)
λ/prime,λ(k,m,n )ψjl,k(u)ϕm(θ). (64)
We have
B(1)
λ/prime,λ,C(1)
λ/prime,λ,2j1D(1)
λ/prime,λ≤π/bardbla(1)
λ/prime,λ/bardblFB, B(l)
λ/prime,λ,n,C(l)
λ/prime,λ,n,2jlD(l)
λ/prime,λ,n≤π/bardbla(l)
λ/prime,λ(·,n)/bardblFB,∀l>1,(65)
21Published in Transactions on Machine Learning Research (07/2022)
where


B(1)
λ/prime,λ:=/integraldisplay/vextendsingle/vextendsingle/vextendsingleW(1)
λ/prime,λ(u)/vextendsingle/vextendsingle/vextendsingledu, B(l)
λ/prime,λ,n:=/integraldisplay
S1/integraldisplay
R2/vextendsingle/vextendsingle/vextendsingleW(l)
λ/prime,λ,n(u,θ)/vextendsingle/vextendsingle/vextendsingledudθ, l> 1,
C(1)
λ/prime,λ:=/integraldisplay
|u|/vextendsingle/vextendsingle/vextendsingle∇uW(1)
λ/prime,λ(u)/vextendsingle/vextendsingle/vextendsingledu, C(l)
λ/prime,λ,n:=/integraldisplay
S1/integraldisplay
R2|u|/vextendsingle/vextendsingle/vextendsingle∇uW(l)
λ/prime,λ,n(u,θ)/vextendsingle/vextendsingle/vextendsingledudθ, l> 1,
D(1)
λ/prime,λ:=/integraldisplay/vextendsingle/vextendsingle/vextendsingle∇uW(1)
λ/prime,λ(u)/vextendsingle/vextendsingle/vextendsingledu, D(l)
λ/prime,λ,n:=/integraldisplay
S1/integraldisplay
R2/vextendsingle/vextendsingle/vextendsingle∇uW(l)
λ/prime,λ,n(u,θ)/vextendsingle/vextendsingle/vextendsingledudθ, l> 1.(66)
Hence we have
Bl,Cl,2jlDl≤Al, (67)
where
B1:= max/braceleftBigg
sup
λM0/summationdisplay
λ/prime=1B(1)
λ/prime,λ,M0
M1sup
λ/primeM1/summationdisplay
λ=1B(1)
λ/prime,λ/bracerightBigg
,
C1:= max/braceleftBigg
sup
λM0/summationdisplay
λ/prime=1C(1)
λ/prime,λ,M0
M1sup
λ/primeM1/summationdisplay
λ=1C(1)
λ/prime,λ/bracerightBigg
,
D1:= max/braceleftBigg
sup
λM0/summationdisplay
λ/prime=1D(1)
λ/prime,λ,M0
M1sup
λ/primeM1/summationdisplay
λ=1D(1)
λ/prime,λ/bracerightBigg
,(68)
and, forl>1,
Bl:= max

sup
λMl−1/summationdisplay
λ/prime=1/summationdisplay
nB(l)
λ/prime,λ,n,2Ml−1
Ml/summationdisplay
nBl,n

, Bl,n:= sup
λ/primeMl/summationdisplay
λ=1B(l)
λ/prime,λ,n,
Cl:= max

sup
λMl−1/summationdisplay
λ/prime=1/summationdisplay
nC(l)
λ/prime,λ,n,2Ml−1
Ml/summationdisplay
nCl,n

, Cl,n:= sup
λ/primeMl/summationdisplay
λ=1C(l)
λ/prime,λ,n,
Dl:= max

sup
λMl−1/summationdisplay
λ/prime=1/summationdisplay
nD(l)
λ/prime,λ,n,2Ml−1
Ml/summationdisplay
nDl,n

, Dl,n:= sup
λ/primeMl/summationdisplay
λ=1D(l)
λ/prime,λ,n.(69)
The bound on Al,∀l≥1, i.e., (A2), therefore implies that
Bl,Cl,2jlDl≤1,∀l≥1. (70)
Proof of Lemma 2. Applying Lemma 1 to the ﬁlters W(1)
λ/prime,λ(u)andW(l)
λ/prime,λ,n(u,θ)deﬁned in (64) after rescaling
the spatial variable ueasily leads to the desired bounds (67). The rest of the lemma follows from the
assumption (A2).
Proof of Proposition 2. To avoid cumbersome notations, we drop the layer index (l)in the ﬁlters W(l)
λ/prime,λ
andb(l), and letM=Ml,M/prime=Ml−1when the context is clear. The proof of (a) is similar to that of
Proposition 2(a) in (Zhu et al., 2019) after a further integration on S1. More speciﬁcally, when l= 1, the
deﬁnition of B1in (66) implies that
sup
λ/summationdisplay
λ/primeB(1)
λ/prime,λ≤B1,sup
λ/prime/summationdisplay
λB(1)
λ/prime,λ≤B1M
M/prime(71)
22Published in Transactions on Machine Learning Research (07/2022)
Therefore, given two inputs x1andx2, we have
/vextendsingle/vextendsingle/vextendsingle/parenleftBig
x(1)[x1]−x(1)[x2]/parenrightBig
(u,θ,α,λ )/vextendsingle/vextendsingle/vextendsingle2
(72)
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleσ/parenleftBigg/summationdisplay
λ/prime/integraldisplay
x1(u+u/prime,λ/prime)Wλ/prime,λ/parenleftbig
2−αR−θu/prime/parenrightbig
2−2αdu/prime+b(λ)/parenrightBigg
−σ/parenleftBigg/summationdisplay
λ/prime/integraldisplay
x2(u+u/prime,λ/prime)Wλ/prime,λ/parenleftbig
2−αR−θu/prime/parenrightbig
2−2αdu/prime+b(λ)/parenrightBigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(73)
≤/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
λ/prime/integraldisplay
x1(u+u/prime,λ/prime)Wλ/prime,λ/parenleftbig
2−αR−θu/prime/parenrightbig
2−2αdu/prime−/summationdisplay
λ/prime/integraldisplay
x2(u+u/prime,λ/prime)Wλ/prime,λ/parenleftbig
2−αR−θu/prime/parenrightbig
2−2αdu/prime/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(74)
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
λ/prime/integraldisplay
(x1−x2)(u+u/prime,λ/prime)Wλ/prime,λ/parenleftbig
2−αR−θu/prime/parenrightbig
2−2αdu/prime/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(75)
≤/parenleftBigg/summationdisplay
λ/prime/integraldisplay
|(x1−x2)(u+u/prime,λ/prime)|2/vextendsingle/vextendsingleWλ/prime,λ(2−αR−θu/prime)/vextendsingle/vextendsingle2−2αdu/prime/parenrightBigg/summationdisplay
λ/prime/integraldisplay/vextendsingle/vextendsingleWλ/prime,λ(2−αR−θu/prime)/vextendsingle/vextendsingle2−2αdu/prime(76)
=/parenleftBigg/summationdisplay
λ/prime/integraldisplay
|(x1−x2)(u+u/prime,λ/prime)|2/vextendsingle/vextendsingleWλ/prime,λ(2−αR−θu/prime)/vextendsingle/vextendsingle2−2αdu/prime/parenrightBigg/parenleftBigg/summationdisplay
λ/primeB(1)
λ/prime,λ/parenrightBigg
(77)
≤B1/summationdisplay
λ/prime/integraldisplay
|(x1−x2)(˜u,λ/prime)|2/vextendsingle/vextendsingleWλ/prime,λ(2−αR−θ(˜u−u))/vextendsingle/vextendsingle2−2αd˜u (78)
Hence, given any α∈R, we have
/summationdisplay
λ/integraldisplay
S1/integraldisplay
R2/vextendsingle/vextendsingle/vextendsingle/parenleftBig
x(1)[x1]−x(1)[x2]/parenrightBig
(u,α,θ,λ )/vextendsingle/vextendsingle/vextendsingle2
dudθ
≤/summationdisplay
λ/integraldisplay
S1/integraldisplay
R2B1/summationdisplay
λ/prime/integraldisplay
|(x1−x2)(˜u,λ/prime)|2/vextendsingle/vextendsingleWλ/prime,λ(2−αR−θ(˜u−u))/vextendsingle/vextendsingle2−2αd˜ududθ (79)
=B1/summationdisplay
λ/prime/integraldisplay
R2|(x1−x2)(˜u,λ/prime)|2/parenleftBigg/summationdisplay
λ/integraldisplay
S1/integraldisplay
R2/vextendsingle/vextendsingleWλ/prime,λ(2−αR−θ(˜u−u))/vextendsingle/vextendsingle2−2αdudθ/parenrightBigg
d˜u (80)
=B1/summationdisplay
λ/prime/integraldisplay
|(x1−x2)(˜u,λ/prime)|2/parenleftBigg/summationdisplay
λB(1)
λ/prime,λ/parenrightBigg
d˜u (81)
≤B2
1M
M/prime/summationdisplay
λ/prime/integraldisplay
|(x1−x2)(˜u,λ/prime)|2d˜u (82)
=B2
1M/bardblx1−x2/bardbl2(83)
≤M/bardblx1−x2/bardbl2, (84)
where the last inequality comes from Lemma 2, and (80) makes use of the fact that
/integraldisplay
R2/vextendsingle/vextendsingleW(2−αR−θu)/vextendsingle/vextendsingle2−2αdu=/integraldisplay
R2|W(u)|du,∀α∈R,∀θ∈S1, (85)
and/integraltext
S1dθ= 1due to the normalization factor 1/2πin the deﬁnition. Therefore, we have
/bardblx(1)[x1]−x(1)[x2]/bardbl2= sup
α1
M/summationdisplay
λ/integraldisplay/integraldisplay/vextendsingle/vextendsingle/vextendsingle/parenleftBig
x(1)[x1]−x(1)[x2]/parenrightBig
(u,θ,α,λ )/vextendsingle/vextendsingle/vextendsingle2
dudθ≤/bardblx1−x2/bardbl2.(86)
This concludes the proof of (a) for the case l= 1. For the case l >1, the same technique applies by
considering the joint convolution/integraltext
S1/integraltext
R2(···)dudθwhile making use of (85), and we omit the detail.
23Published in Transactions on Machine Learning Research (07/2022)
For part (b), we use mathematical induction. More speciﬁcally, x(0)
0(u,λ) = 0by deﬁnition. For l= 1,
x(1)
0(u,θ,α,λ ) =σ(b(1)(λ)). Assuming that x(l−1)
0(u,θ,α,λ ) =x(l−1)
0(λ)for somel>1, we have
x(l)
0(u,α,λ )
=σ/parenleftBigg/summationdisplay
λ/prime/integraldisplay
S1/integraldisplay
R2/integraldisplay
Rx(l−1)
0(u+u/prime,θ+θ/prime,α+α/prime,λ/prime)W(l)
λ/prime,λ/parenleftbig
2−αR−θu/prime,θ/prime,α/prime/parenrightbig
2−2αdα/primedu/primedθ/prime+b(l)(λ)/parenrightBigg
(87)
=σ/parenleftBigg/summationdisplay
λ/primex(l−1)
0(λ/prime)/integraldisplay
S1/integraldisplay
R2/integraldisplay
RW(l)
λ/prime,λ/parenleftbig
2−αR−θu/prime,θ/prime,α/prime/parenrightbig
2−2αdα/primedu/primedθ/prime+b(l)(λ)/parenrightBigg
(88)
=σ/parenleftBigg/summationdisplay
λ/primex(l−1)
0(λ/prime)/integraldisplay
S1/integraldisplay
R2/integraldisplay
RW(l)
λ/prime,λ(u/prime,θ/prime,α/prime)dα/primedu/primedθ/prime+b(l)(λ)/parenrightBigg
(89)
=x(l)
0(λ). (90)
To prove part (c): for any l>1, we have
/bardblx(l)
c/bardbl=/bardblx(l)−x(l)
0/bardbl=/bardblx(l)[x(l−1)]−x(l)
0[x(l−1)
0]/bardbl≤/bardblx(l−1)−x(l−1)
0/bardbl=/bardblx(l−1)
c/bardbl, (91)
where the inequality comes from the layer-wise non-expansiveness (54) in part (a). An easy induction leads
to (c).
C.2 Proof of Proposition 3
Proof.Just like Proposition 2(a), the proofs of part (a) and part (d), respectively, of Proposition 3 are similar
to those of Proposition 3(a) and Proposition 4 of (Zhu et al., 2019). More speciﬁcally, making use of (85)
and/integraltext
S1dθ= 1whenl= 1, and further taking the integral/integraltext
S1/integraltext
R2(···)dudθoverS1×R2instead of just R2
whenl>1, we arrive at the following
/vextenddouble/vextenddouble/vextenddoublex(l)[Dτx(l−1)]−Dτx(l)[x(l−1)]/vextenddouble/vextenddouble/vextenddouble≤4(Bl+Cl)|∇τ|∞/vextenddouble/vextenddouble/vextenddoublex(l−1)
c/vextenddouble/vextenddouble/vextenddouble≤4(Bl+Cl)|∇τ|∞/vextenddouble/vextenddouble/vextenddoublex(0)/vextenddouble/vextenddouble/vextenddouble,∀l≥1,(92)
/vextenddouble/vextenddouble/vextenddoubleD(l)
η,β,vDτx(l)−D(l)
η,β,vx(l)/vextenddouble/vextenddouble/vextenddouble≤2β+1|τ|∞Dl/vextenddouble/vextenddouble/vextenddoublex(l−1)
c/vextenddouble/vextenddouble/vextenddouble≤2β+1|τ|∞Dl/vextenddouble/vextenddouble/vextenddoublex(0)/vextenddouble/vextenddouble/vextenddouble,∀l≥1,(93)
whereBl,Cl,Dlare deﬁned in (68) and (69). Lemma 2 thus implies that
/vextenddouble/vextenddouble/vextenddoublex(l)[Dτx(l−1)]−Dτx(l)[x(l−1)]/vextenddouble/vextenddouble/vextenddouble≤8|∇τ|∞/vextenddouble/vextenddouble/vextenddoublex(l−1)
c/vextenddouble/vextenddouble/vextenddouble≤8|∇τ|∞/vextenddouble/vextenddouble/vextenddoublex(0)/vextenddouble/vextenddouble/vextenddouble,∀l≥1,(94)
/vextenddouble/vextenddouble/vextenddoubleD(l)
η,β,vDτx(l)−D(l)
η,β,vx(l)/vextenddouble/vextenddouble/vextenddouble≤2β+1−jl|τ|∞/vextenddouble/vextenddouble/vextenddoublex(l−1)
c/vextenddouble/vextenddouble/vextenddouble≤2β+1−jl|τ|∞/vextenddouble/vextenddouble/vextenddoublex(0)/vextenddouble/vextenddouble/vextenddouble,∀l≥1.(95)
For part (b), given any (η,β,v )∈S1×R×R2, we have
/bardblD(l)
η,β,vx(l)/bardbl2= sup
α1
Ml/summationdisplay
λ/integraldisplay
S1/integraldisplay
R2/vextendsingle/vextendsingle/vextendsingleD(l)
η,β,vx(l)(u,θ,α,λ )/vextendsingle/vextendsingle/vextendsingle2
dudθ (96)
= sup
α1
Ml/summationdisplay
λ/integraldisplay
S1/integraldisplay
R2/vextendsingle/vextendsingle/vextendsinglex(l)(2−βR−θ(u−v),θ−η,α−β,λ)/vextendsingle/vextendsingle/vextendsingle2
dudθ (97)
= sup
α1
Ml/summationdisplay
λ/integraldisplay
S1/integraldisplay
R2/vextendsingle/vextendsingle/vextendsinglex(l)(u/prime,θ−η,α−β,λ)/vextendsingle/vextendsingle/vextendsingle2
22βdu/primedθ (98)
= 22β/bardblx(l)/bardbl2(99)
24Published in Transactions on Machine Learning Research (07/2022)
Thus (57) is valid. The second half of part (b) holds since
/vextenddouble/vextenddouble/vextenddoublex(l)[D(l)
η,β,v◦Dτx(l−1)]−D(l)
η,β,vDτx(l)[x(l−1)]/vextenddouble/vextenddouble/vextenddouble
=/vextenddouble/vextenddouble/vextenddoubleD(l)
η,β,vx(l)[Dτx(l−1)]−D(l)
η,β,vDτx(l)[x(l−1)]/vextenddouble/vextenddouble/vextenddouble (100)
=2β/vextenddouble/vextenddouble/vextenddoublex(l)[Dτx(l−1)]−Dτx(l)[x(l−1)]/vextenddouble/vextenddouble/vextenddouble (101)
≤2β+3|∇τ|∞/bardblx(l−1)
c/bardbl, (102)
where the ﬁrst equality holds because of the RST-equivariance, i.e., Theorem 1, and the second equality
follows from (57).
The proof of part (c) is exactly the same as that of Proposition 3(c) of (Zhu et al., 2019). Speciﬁcally,
we telescope the inequality (58) while making use of the non-expansiveness of the layer-wise features, i.e.,
Proposition 2(c). The detail is omitted.
C.3 Proof of Theorem 1
Proof.Theorem 1 is a direct consequence of Proposition 3. More speciﬁcally,
/vextenddouble/vextenddouble/vextenddoublex(L)[D(0)
η,β,v◦Dτx(0)]−D(L)
η,β,vx(L)[x(0)]/vextenddouble/vextenddouble/vextenddouble
≤/vextenddouble/vextenddouble/vextenddoublex(L)[D(0)
η,β,v◦Dτx(0)]−D(L)
η,β,vDτx(L)[x(0)]/vextenddouble/vextenddouble/vextenddouble+/vextenddouble/vextenddouble/vextenddoubleD(L)
η,β,vDτx(L)[x(0)]−D(L)
η,β,vx(L)[x(0)]/vextenddouble/vextenddouble/vextenddouble(103)
≤2β+3L|∇τ|∞/bardblx(0)/bardbl+ 2β+1−jL|τ|∞/bardblx(0)/bardbl (104)
=2β+1/parenleftbig
4L|∇τ|∞+ 2−jL|τ|∞/parenrightbig
/bardblx(0)/bardbl, (105)
where the second inequality comes from Proposition 3(c) and Proposition 3(d). This concludes the proof of
Theorem 1.
D Reproducibility
D.1 License of Datasets
•MNIST. Creative Commons Attribution-Share Alike 3.0 license LeCun (1998).
•Fashion-MNIST. MIT license Xiao et al. (2017b).
•STL-10. BSD 3-Clause License Coates et al. (2011a).
D.2 Code Implementation
Our code and experiments in our paper are available at https://github.com/gaoliyao/
Roto-scale-translation-Equivariant-CNN . We speciﬁcally include the experiments for MNIST
and Fashion-MNIST for this version.
The code is built upon the GitHub repository of the paper Sosnovik et al. (2020) under MIT license
https://github.com/ISosnovik/sesn . For the implementation of Fourier-Bessel bases and Decomposed
Convolutional Filters, we build our code on Zhu et al. (2019). In the submitted ﬁles, we keep the names of
the authors of Sosnovik et al. (2020), while our names remain anonymous throughout the repository.
25