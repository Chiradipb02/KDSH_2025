Under review as submission to TMLR
Combating Client Dropout in Federated Learning via Friend
Model Substitution
Anonymous authors
Paper under double-blind review
Abstract
Federated learning (FL) is a new distributed machine learning framework known for its
benefits on data privacy and communication efficiency. Since full client participation in
many cases is infeasible due to constrained resources, partial participation FL algorithms
have been investigated that proactively select/sample a subset of clients, aiming to achieve
learning performance close to the full participation case. This paper studies a passivepartial
client participation scenario that is much less well understood, where partial participation
is a result of external events, namely client dropout, rather than a decision of the FL
algorithm. We cast FL with client dropout as a special case of a larger class of FL problems
where clients can submit substitute (possibly inaccurate) local model updates. Based on our
convergence analysis, we develop a new algorithm FL-FDMS that discovers friends of clients
(i.e., clientswhosedatadistributionsaresimilar)on-the-flyandusesfriends’localupdatesas
substitutesforthedropoutclients, therebyreducingthesubstitutionerrorandimprovingthe
convergence performance. A complexity reduction mechanism is also incorporated into FL-
FDMS, making it both theoretically sound and practically useful. Experiments on MNIST
and CIFAR-10 confirmed the superior performance of FL-FDMS in handling client dropout
in FL.
1 Introduction
Federated learning (FL) is a distributed machine learning paradigm where a set of clients with decentralized
data work collaboratively to learn a model under the coordination of a centralized server. Depending on
whether or not all clients participate in every learning round, FL is classified as either full participation
orpartial participation . While full participation is the ideal FL mode that achieves the best convergence
performance, a lot of effort has been devoted to developing partial participation strategies via client selec-
tion/sampling Karimireddy et al. (2019); Li et al. (2019a); Yang et al. (2020); Ribero & Vikalo (2020); Chen
et al. (2020a); Cho et al. (2020); Lai et al. (2021); Wu & Wang (2022); Balakrishnan et al. (2021) due to the
attractive benefit of reduced resource (i.e. communication and computation) consumption. Existing works
showthatsomeofthesepartialparticipationstrategiesLietal.(2019a);Yangetal.(2020)canindeedachieve
performance close to full participation. Although the details differ, the principal idea of these strategies is
the careful selection of appropriate clients to participate in each FL round. For example, in many cases
Karimireddy et al. (2019); Li et al. (2019a); Yang et al. (2020), clients are sampled uniformly at random so
that the participating clients form an “unbiased” representation of the whole client population in terms of
the data distribution. In others Ribero & Vikalo (2020); Chen et al. (2020a); Cho et al. (2020); Lai et al.
(2021); Wu & Wang (2022); Balakrishnan et al. (2021), “important” clients are selected more often to lead
FL towards the correct loss descending direction.
This paper studies partial participation FL, but from an angle in stark contrast with existing works. In
our considered problem, partial participation is a result of an arbitrary client dropout process, which the
FL algorithm has absolutely no control over. For example, a client may not be able to participate (in other
words, drop out) in a FL round due to, e.g., dead/low battery or loss of the communication signal. This
means that the subset of clients participating in a FL round may not be “representative” or “important”
in any sense. Note that the client dropout process by no means must be stochastic. Does FL still work
1Under review as submission to TMLR
with arbitrary client dropout? How does client dropout affect the FL performance? How to mitigate the
negative impact of client dropout on FL convergence? These are the central questions that this paper strives
to answer.
We shall note that client dropout can occur simultaneously with client selection/sampling and hence partial
participation can be a mixed result of both. As will become clear, our algorithm can be readily applied to
this scenario and our theoretical results can also be extended provided that the client selection/sampling
strategy used in conjunction has its own theoretical performance guarantee. However, since these results
will depend on the specific client selection/sampling strategy adopted, and in order to better elucidate our
main idea, this paper will not consider client selection/sampling.
Main Contributions . (1) Our study starts with a general convergence analysis for a class of FL problems
where clients can submit substitute (inaccurate) local model updates for their true local model updates in
each round. It turns out that this class of FL problems includes FL with client dropout as a special case.
The main insight derived from this analysis is that the FL convergence performance depends on the total
substitution error (i.e., the difference between the true update and its substitute) over the entire course of
learning. As such, reducing the substitution error is the key to improving the FL performance with client
dropout. (2) We then introduce the notion of “friendship” among clients, where clients are friends if their
data distributions and hence their local model updates are similar enough. Therefore, an intuitive idea
for mitigating the impact of client dropout is to use the local model update of a friend client (which does
not drop out) as a substitute of that of the dropout client when computing the next global model, since
doing so incurs a small substitution error. (3) Although the idea sounds promising, realizing it is highly
non-trivial because the friendship is unknown . Thus, our algorithm must discover the friendship over the
FL rounds and use the discovered friends for the local model update substitution purpose. We prove that
our algorithm asymptotically achieves the same FL convergence bound when assuming that the friendship
is fully revealed. Furthermore, because the vanilla friends discovery mechanism can involve a large number
of model update comparison computations, we develop an improved algorithm that significantly reduces the
computational complexity. With this improvement, our algorithm is both theoretically sound and practically
useful. (4) Experiments on MNIST and CIFAR-10 confirmed that our algorithm can significantly improve
the FL performance in the presence of client dropout.
2 Related Work
The FedAvg algorithm is an early FL algorithm proposed in konevcny et al. (2016), which sparked many
follow-up works on FL. Early works focus on FL under the assumptions of i.i.d. datasets and full client
participation Stich (2018); Yu et al. (2019b); Wang & Joshi (2018), and most of the theoretical works show
a linear speedup for convergence for a sufficiently large number of learning rounds. For non-i.i.d. datasets,
the performance of FedAvg and its variants were demonstrated empirically Karimireddy et al. (2019); Jeong
et al. (2018); Zhao et al. (2018); Li et al. (2020b); Sattler et al. (2019), and convergence bounds were proven
for the full participation case in Karimireddy et al. (2019); Stich et al. (2018); Yu et al. (2019a); Reddi et al.
(2020). With partial client participation, the convergence of FedAvg was proven in Li et al. (2019a) for
strongly convex functions, and the convergence of a generalized FedAvg with two-sided learning rates was
analyzed in Karimireddy et al. (2019); Yang et al. (2020).
Most existing works on partial participation study strategies that proactively select or sample a subset of
clients to participate in FL. The convergence of simple strategies such as random selection has been studied
in Karimireddy et al. (2019); Li et al. (2019a); Yang et al. (2020). This analysis is relatively easy because
of the “unbiased” nature of random selection. As a more sophisticated strategy, Cho et al. (2020) selects
clients with higher local loss and proves an increased convergence speed. However, the vast majority of client
selection strategies Ribero & Vikalo (2020); Lai et al. (2021); Wu & Wang (2022) only empirically shows the
performance improvement. Very few works studied biased client selection in FL. In Cho et al. (2022), the
authors presented a convergence rate bound under biased client selection, which contains a constant term
due to the bias. In our paper, partial participation is nota decision of the FL algorithm, and our focus is
on how to mitigate the negative impact of client dropout.
2Under review as submission to TMLR
Client dropout is related to the “straggler” issue in FL, which is caused by the delayed local model upload-
ing by some clients. Existing solutions to the straggler issue can be categorized into the following three
types: doing nothing but waiting McMahan et al. (2017), allowing clients to upload their local models asyn-
chronously to the server Wu et al. (2020); Li et al. (2019b); Xie et al. (2019); Chen et al. (2020b), and
using the stored last updates of the inactive clients to join the model aggregation Yan et al. (2020); Gu
et al. (2021). A straggler-resilient FL is proposed in Reisizadeh et al. (2020) that incorporates statistical
characteristics of the clients’ data to adaptively select clients. Some other recent works Ruan et al. (2021);
Yang et al. (2022) studied FL with flexible client participation where client dropout is part of the considered
scenario and provided general convergence analysis. However, unlike our paper, these works Ruan et al.
(2021); Yang et al. (2022) do not propose mitigation schemes for client dropout.
Our proposed algorithm exploits client similarity for model substitution, which is related to clustered FL
Ghosh et al. (2020); Ruan & Joe-Wong (2022); Dennis et al. (2021) which clusters clients with similar data
distributions into the same group. However, our algorithm has a very different motivation and objective.
Moreover, our algorithm only requires computing pair-wise similarity, which can be computed easily on the
server, whereas clustered FL needs to identify a set of clients belonging to the same cluster, which incurs
much higher computation and communication overhead.
3 Federated Learning with Client Dropout
We consider a server and a set of Kclients, indexed by K={1,...,K}, who work together to train a machine
learning model by solving a distributed optimization problem:
min
w∈Rd/braceleftigg
f(w) :=1
KK/summationdisplay
k=1Eξk∼Dk[Fk(w;ξk)]/bracerightigg
(1)
whereFk:Rd→Rdenotes the objective function, ξk∼Dkrepresents the sample/s drawn from distribution
Dkat thek-th client and w∈Rdis the model parameter to learn. In a non-i.i.d. data setting, which is the
focus of this paper, the distributions Dkare different across the clients.
We consider a typical FL algorithm konevcny et al. (2016) working in the client dropout setting. In each
roundt, only a subsetSt⊆ Kof clients participate due to external reasons uncontrollable by the FL
algorithm. We call the clients that cannot participate or complete the task dropout(orinactive) clients.
Then, FL executes the following four steps among the non-dropout (oractive) clients in round t:
1.Global model download . Each client k∈Stdownloads the global model wtfrom the server.
2.Local model update . Each client k∈Stuseswtas the initial model to train a new local model
wk
t+1, typically by using mini-batch stochastic gradient descent (SGD) as follows:
wk
t,0=wt
wk
t,τ+1=wk
t,τ−ηLgk
t,τ,∀τ= 1,...,E (2)
wk
t+1=wk
t,E
whereξk
t,τis a mini-batch of data samples independently sampled uniformly at random from the
local dataset of client k,gk
t,τ=∇Fk(wk
t,τ;ξk
t,τ)is the mini-batch stochastic gradient, ηLis the client
local learning rate and Eis the number of epochs for local training.
3.Local model upload . Clients upload their local model updates to the server. Instead of uploading
the local model wk
t+1itself, client kcan simply upload the local model update ∆k
t, which is defined
as the accumulative model parameter difference as follows:
∆k
t=−E−1/summationdisplay
τ=0gk
t,τ (3)
3Under review as submission to TMLR
4.Global model update . The server updates the global model by using the aggregated local model
updates of the clients in St:
wt+1=wt+ηηL∆t,where ∆t:=1
St/summationdisplay
k∈St∆k
t (4)
andηis the global learning rate and St≜|St|denotes the number of the non-dropout clients.
For the main result of this paper, we consider the most general case of the client dropout process by imposing
onlyanupperlimitonthedropoutratio. Thatis, thereexistsaconstant α∈[0,1)suchthat (K−St)/K≤α.
If all clients drop out in a round, then essentially the round is skipped. Later in this paper, we will impose
additional conditions on the dropout process to facilitate our theoretical analysis.
Also note that if Stwere a choice of the FL algorithm, then the problem would become FL with client
selection/sampling. We stress again that in our problem, Stis not a choice, it is an uncontrollable client
participation scenario.
4 Convergence Analysis
Consider an FL round twhere the setStof clients are active while the remaining set K\Stof clients dropped
out. Thus, one can only use the local model updates ∆k
tof the active clients in Stto perform global model
updates since the inactive clients upload nothing to the server. However, rather than completely ignoring
the inactive clients, we write the aggregate model update ∆tin a different way to include all clients in the
equation:
∆t:=1
St/summationdisplay
k∈St∆k
t=1
K
/summationdisplay
k∈St∆k
t+/summationdisplay
k∈K\S t˜∆k
t
 (5)
where in the second equality we simply take ˜∆k
t=1
St/summationtext
k∈St∆k
t. In other words, although the inactive clients
did not participate in the round t’s learning, it is equivalent to the case where an inactive client k∈K\St
uses ˜∆k
t=1
St/summationtext
k∈St∆k
tas a substitute of its true local update ∆k
t(which it may not even calculate due to
dropout). Apparently, because ˜∆k
t̸= ∆k
tin general, similar substitutes lead to a biased error in the global
update and hence affect the FL convergence performance.
Leveraging the above observation, we consider a larger class of FL problems that include client dropout
as a special case. Specifically, imagine that an inactive client k, instead of contributing nothing, uses a
substitute ˜∆k
tfor∆k
twhen submitting its local model update. Apparently, ˜∆k
t=1
St/summationtext
k∈St∆k
tis a specific
choice of the substitute. We will still use the notation ∆tas the aggregate model update with local update
substitution and the readers should not be confused. Our convergence analysis will utilize the following
standard assumptions about the FL problem.
Assumption 1 (Lipschitz Smoothness) The local objective functions satisfy the Lipschitz smoothness
property, i.e.,∃L>0, such that∥∇Fk(x)−∇Fk(y)∥≤L∥x−y∥,∀x,y∈Rdand∀k∈K.
Assumption 2 (Unbiased Local Gradient Estimator) The mini-batch based local gradient estimator
is unbiased, i.e. Eξk∼Dk[∇Fk(x;ξk)] =∇Fk(x),∀k∈K.
Assumption 3 (Bounded Local and Global Variance) There exist constants ρL>0andρG>0such
that the variance of each local gradient estimator is bounded, i.e., Eξk∼Dk/bracketleftbig
∥∇Fk(x;ξk)−∇Fk(x)∥2/bracketrightbig
≤
ρ2
L,∀x,∀k∈ K. And the global variability of the local gradient is bounded by ∥∇Fk(x)−∇f(x)∥2≤
ρ2
G,∀x,∀k∈K.
The following result on the upper bound for the τ-step SGD under full participation will be used.
4Under review as submission to TMLR
Lemma 1 (Lemma 4 in Reddi et al. (2020)) For any step-size satisfying ηL≤1
8LE, we have:∀τ=
0,...,E−1
1
KK/summationdisplay
k=1E[∥wk
t,τ−wt∥2]≤5Eη2
L(ρ2
L+ 6Eρ2
G) + 30E2η2
L∥∇f(wt)∥2(6)
Several additional notations will be handy. Let ¯∆t:=1
K/summationtext
k∈K∆k
tbe the average local model update
assuming that all clients are active in round tand submitted their true local updates. Thus, et:= ∆t−¯∆t
represents aggregate global update error due to client dropout and local update substitution in round t.
Furthermore, let ek
t:=˜∆k
t−∆k
t,∀k∈K\Stbe the individual substitution error for an individual inactive
clientkin roundt.
Theorem 1 Let constant local and global learning rates ηLandηbe chosen as such that ηL≤1
8ELand
ηηL≤1
4EL. Under Assumption 1-3, the sequence of model wtgenerated by using model update substitution
with a substitution error sequence e0,...,eT−1satisfies
min
t=0,...,T−1E∥∇f(wt)∥2≤f0−f∗
cηηLET+ Φ + Ψ(e0,...,eT−1) (7)
where Φ =1
c/bracketleftig
5η2
LEL2(ρ2
L+ 6Eρ2
G) +ηηLL
Kρ2
L/bracketrightig
,cis a constant, f0≜f(w0),f∗≜f(w∗),w∗is the optimal
model and
Ψ(e0,...,eT−1) =1 + 3ηηLLE
cE2TT−1/summationdisplay
t=0Et[∥et∥2] (8)
where the expectation is over the local dataset samples among the clients.
The above convergence bound contains three parts: a vanishing termf0−f∗
cηηLETasTincreases, a constant
term Φwhose size depends on the problem instance parameters and is independent of T, and a third term
that depends on the sequence of substitution errors e0,...,eT−1. Thus, by using constant learning rates
ηandηLand assuming that ∥et∥2is bounded, then the convergence bound is O(1/T) +C, whereCis a
constant. The key insight derived by Theorem 1 is that the FL convergence bound depends on the cumulative
substitution error/summationtextT−1
t=0Et[∥et∥2]. When there is no dropout client, namely all clients participated in every
round and submitted their true local model updates, the cumulative substitution error is 0 and hence,
Ψ(e0,...,eT−1) = 0. Thus, the convergence bound is simplyf0−f∗
cηηLET+ Φ, which degenerates to the same
bound established in Yang et al. (2020) for the normal full participation case.
We want to clarify that we did not intend to introduce a new convergence analysis technique for FL. Instead,
our work’s novelty lies in formalizing the effects of client dropout and, more broadly, model substitution on
FL convergence and, based on this understanding, developing mitigation methods to enhance convergence.
Our analysis provides an intuitive understanding of the impact of client dropout and model substitution
on FL convergence and characterizes the convergence under a biased scenario, which is an important and
previously unaddressed issue in FL research. The main challenge was consolidating all the bias-related errors
into a single term in the final convergence bound.
Next, we derive a more specific bound on Ψ(e0,...,eT−1)for the naive dropout case where an inactive client
uploads nothing to the server or, equivalently, uses1
St/summationtext
k∈St∆k
tas a substitute. The following additional
assumption is needed.
Assumption 4 For any two clients iandj, the local model update difference is bounded as follows:
E[∥∆i(w)−∆j(w)∥2]≤σ2
i,j,∀w (9)
where the expectation is over the local dataset samples.
5Under review as submission to TMLR
Assumption4providesapairwisecharacterizationofclients’datasetheterogeneityintermsofthelocalmodel
updates. Whentwoclients i,jhavethesamedatadistributionandassumingthatthemin-batchSGDutilizes
the entire local dataset (i.e., the local gradient estimator is accurate), then it is obvious σ2
i,j= 0. We let
σ2
P≜maxi,jσ2
i,jbe the maximum pairwise difference.
With Assumption 4, the round- tsubstitution error can then be bounded as E[∥et∥2]≤α2σ2
P(See details in
Appendix A.4). Plugging this bound into Ψ(e0,...,eT−t), we have
Ψ(e0,...,eT−1)≤α2σ2
P(1 + 3ηηLLE)
cE2≜¯Ψ (10)
Note that ¯Ψis a constant independent of T. This implies that, with constant learning rates ηLandη,
mintE∥∇f(wt)∥2converges to some value at most Φ +¯ΨasT→∞.
Convergence Rate : By using a local learning rate ηL∼O(1√
T)and a global learning rate η∼O(1), the
convergence rate can be improved to
O(1
E√
T) +O(E2
T) +O(1
K√
T)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Φ+O(α2σ2
P√
T) +O(α2σ2
P
E)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
¯Ψ(11)
Similar to the biased client selection in Cho et al. (2022), even with decaying learning rates, the convergence
bound contains a non-vanishing term (i.e., the last term) in the general case. However, in our case, the non-
vanishing term is due to replacing the model of the dropout clients with an arbitrary model. Nevertheless,
with a decreasing and vanishing dropout rate α, the last term also vanishes and the mintE∥∇f(wt)∥2
converges to the stationary point.
5 Friend Model Substitution
In this section, we develop a new algorithm to reduce or even eliminate the non-vanishing term in the
convergence bound of FL with client dropout. Our key idea is to find a better substitute ˜∆k
tfor∆k
twhen
clientkdrops out in round tin order to reduce Ψ(e0,...,eT−1). This is possible by noticing that σ2
i,jare
different across client pairs and the local model updates are more similar when the clients’ data distributions
are more similar. Thus, when a client idrops out, one can use the local model update ∆j
tas a replacement
of∆i
tifjshares a similar data distribution with i, or in our terminology, jis a friend of i. We make
“friendship” formal in the following definition.
Definition 1 (Friendship) Letσ2
F< σ2
Pbe some constant. We say that clients iandjare friends if
σ2
i,j≤σ2
F. Further, denote Bkas the set of friends of client kandBk=|Bk|as the size ofBk.
Assumption 5 (Friend Presence) In any round t, for any inactive client i∈K\St, there exists at least
one active client jthat is client i’s friend.
Assumption 5 states that using the local model update of a friend to replace that of an inactive client is
feasible. Nevertheless, this assumption can be relaxed so that friends may not be present in every round
when a client drops out. We will cover the relaxed case in Appendix B.
Note that although such “friendship” exists among the clients, they are hidden from the FL algorithm.
Thus, it is not straightforward to substitute the model update of a dropout client with that of its friends.
Nevertheless, it is still useful to understand what can be achieved assuming that the friendship information
is fully revealed. The convergence bound in this non-realizable case will serve as an optimal baseline for our
algorithm to be developed.
With friend model substitution, the accumulated substitution error can be bounded as E[∥et∥2]≤α2σ2
F(see
Appendix A.4). Substituting this bound into Ψ(e0,...,eT−1), we have
Ψ(e0,...,eT−1)≤α2σ2
F(1 + 3ηηLLE)
cE2≜Ψ∗(12)
6Under review as submission to TMLR
Note that Ψ∗is still a constant independent of Tbut it is much smaller than ¯Ψ, since typically σ2
F≪σ2
P.
Therefore, the convergence bound can be significantly improved if the algorithm utilizes the friendship
information.
Convergence Rate : With a local learning rate ηL∼O(1√
T)and a global learning rate η∼O(1), the
convergence rate can be improved to
O(1
E√
T) +O(E2
T) +O(1
K√
T)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Φ+O(α2σ2
F√
T) +O(α2σ2
F
E)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Ψ∗(13)
The above convergence bound still has a non-vanishing constant term (i.e., the last term) in the general case.
In some special cases where σ2
Fdynamically decreases over round, this term may also vanish as T→∞.
For example, consider that clients are grouped into clusters and clients from the same cluster have identical
data distribution. Borrowing the idea from Shi et al. (2022) on the increasing mini-batch size, the local
model update difference of two clients, i.e., σ2
i,jfrom the same cluster also decreases with an increasing batch
sizeB. Assuming E[∥∆i(w)−∆j(w)∥2]≤σ2
F/B,∀w, and by using the mini-batch size sequence Bt=B0t
whereB0is the initial mini-batch size, then the last term becomes O(α2σ2
FlnT
TE), which goes to 0asT→∞.
Therefore, the constant term in the convergence rate is eliminated even with a constant dropout rate α.
The above analysis shows that the FL convergence can be substantially improved if the algorithm can
utilize the friendship information. Next, we develop a learning-assisted FL algorithm, called FL with Friend
Discovery and Model Substitution (FL-FDMS), that discovers the friends of clients and uses the model
update of the discovered friends for substitution. We prove that FL-FDMS achieves asymptotically the
optimal error bound Ψ∗.
5.1 Algorithm (FL-FDMS)
In each FL round t, in addition to the regular steps in the FL algorithm described in Section 3, FL-
FDMS performs different actions depending on whether or not a client drops out. For active clients, FL-
FDMS calculates pairwise similarity scores to learn the similarity between clients. For inactive clients,
FL-FDMS uses the historical similarity scores to find active friend clients and use their local model updates
as substitutes. We describe these two cases in more detail below.
Active Clients . For any pair of active clients iandjinSt. The server calculates a similarity score
ri,j
t=r(∆i
t,∆j
t)based on their uploaded local model updates ∆i
tand∆j
t. Many functions can be used to
calculate the score. For example, r(∆i
t,∆j
t)can simply be the negative model difference, i.e., −∥∆i
t−∆j
t∥,
or the normalized cosine similarity, i.e.,
r(∆i
t,∆j
t) =1
2/parenleftigg
⟨∆i
t,∆j
t⟩
∥∆i
t∥∥∆j
t∥+ 1/parenrightigg
(14)
Because the normalized cosine similarity takes value from a bounded and normalized range [0,1], which is
more amenable for mathematical analysis, we will use this function in this paper. Clearly, a higher similarity
score implies that the two clients are more similar in terms of their data distribution.
However, a single similarity score calculated in one particular round does not provide accurate similarity
information because of the randomness in the initial model in that round and the randomness in the mini-
batch SGD for local model computation. Thus, the server maintains and updates an average similarity score
Ri,j
tfor clientsiandjbased on all similarity scores calculated so far as follows,
Ri,j
t=

Ni,j
t−1
Ni,j
t−1+1Ri,j
t−1+1
Ni,j
t−1+1ri,j
t,ifi,j∈St
Ri,j
t−1, otherwise(15)
whereNi,j
tis the number of rounds where both clients iandjdid not drop out up to round t.
7Under review as submission to TMLR
Inactive Clients . For any inactive client k, the server looks up Rk,i
tbetweenkand every active client
i∈St, finds the one with the highest similarity score, denoted by ϕt(k) = arg max i∈StRk,i
t, and uses the
local model update ∆ϕt(k)
tas a substitute for ∆k
twhen computing the global update.
Remark on Privacy : The averaged similarity score is calculated based on the uploaded model and does
not require any additional information from the client. Previous works such as Ruan & Joe-Wong (2022);
Ghosh et al. (2020), albeit addressing different FL problems, also rely on finding the client relationships or
clusters. Thus, the privacy protection level of our algorithm is similar to that of those algorithms.
5.2 Convergence Analysis
In this subsection, we analyze the convergence of FL-FDMS. Since Theorem 1 has already proven the FL
convergence bound depending on a general error sequence, we will focus only on bounding Ψ(e0,e1,...,eT−1)
in our algorithm. The following additional assumptions on the dropout process are needed.
Assumption 6 (Sufficiently Many Common Rounds) There exists a constant β∈(0,1]so that for
any pair of clients iandj,Ni,j
tsatisfiesNi,j
t≥βt,∀t.
We first establish a bound on the probability that ϕt(k)selected by our algorithm is not a friend of client
k. Let Et[ri,j
t] =µi,jbe the expected similarity score between clients iandj. Thusδk= mini∈Bkµk,i−
maxi̸∈Bkµk,idenotes the minimum similarity score gap between a friend client and a non-friend client. We
further letδmin= minkδkfor allk∈K.
Lemma 2 The probability that our algorithm selects a non-friend client for an inactive client in round tis
upper bounded by 2Kexp/parenleftig
−βδ2
mint
2/parenrightig
.
Now, we are ready to bound E∥et∥2by using FL-FDMS. Firstly, we can show the following bound on E∥et∥2
(see Appendix A.4):
E∥et∥2≤α2/parenleftbigg
σ2
F+ 2Kexp/parenleftbigg−βδ2
mint
2/parenrightbigg
(σ2
P−σ2
F)/parenrightbigg
(16)
Plugging this bound into Ψ(see Appendix A.5), we have
Ψ(e0,...,eT−1)≤Ψ∗+ 2K¯Ψ1−exp/parenleftig
−βδ2
minT
2/parenrightig
T/parenleftig
1−exp/parenleftig
−βδ2
min
2/parenrightig/parenrightig (17)
Notethatas T→∞,Ψ(e0,...,eT−1)→Ψ∗. Therefore,FL-FDMSasymptoticallyapproachestheconvergence
bound obtained in the fully revealed friendship information case.
5.3 Reducing Similarity Computation Complexity
FL-FDMS requires pairwise similarity score computation for all active clients in every round, causing a big
computation burden when Stis large. We now design a simple mechanism inspired by Hoeffding races Maron
& Moore (1993) to reduce the computational complexity and incorporate it in FL-FDMS. To this end, we
introduce a notion called candidate (friend) set Ck
tfor each client k, which keeps the potential friend list of
clientk. Initially,Ck
tis the entire client set K, but over time,Ck
tshrinks by eliminating non-friend clients
with high probability. Thus, the server only needs to compute the similarity score of clients in the candidate
friend set of a client kand pick a client from this set for substitution purposes. The updating rule for Ck
tis
as follows. In every round t, for each client k: (1) Find the client ϕt(k)with the highest similarity score, i.e.,
ϕt(k) = arg max i∈Ct
kRk,i
t. (2) Compute the similarity score gap gk,i=Rk,ϕt(k)
t−Rk,i
tfor any other client
i̸=ϕt(k),i∈Ck
t. (3) Eliminate ifromCk
tifgk,j≥Θt, namelyCk
t+1←Ck
t−{i:gk,i≥Θt}, where Θtis a
threshold parameter decreasing over t. In Theorem 2, we design a specific threshold sequence Θtand prove
that our convergence bound in the previous section holds with high probability.
8Under review as submission to TMLR
Theorem 2 Letδf= maxk{maxi∈Bkµk,i−mini∈Bkµk,i}be the maximum similarity score gap among friends
of any client, and Bmax= maxkBkbe the maximum number of friends that any client can have. For any
p∈(0,1), by setting Θt=/radicalig
2 ln(2K2TBmax)−2 lnp
βt+δf, FL-FDMS with complexity reduction yields, with
probability at least 1−p, the same bound on Ψas in equation 74.
Note that in order to establish the convergence bound, we used several loose bounds in the proof of Theorem
2, thereby resulting in an unnecessarily large threshold sequence Θt. In practice, the threshold sequence Θt
can be chosen much smaller than what is given in Theorem 2, and hence non-friend clients can be eliminated
more quickly with high confidence.
6 Experiments
6.1 Setup
We use Python3 and the Pytorch library, and our code is adapted from Jadhav (2020), which is under the
MIT License. The experiments were run on an Ubuntu 18.04 machine with an Intel Core i7-10700KF 3.8GHz
CPU and GeForce RTX 3070 GPU. All experiment results are averaged over 10 repeats.
We perform experiments on two standard public datasets, namely MNIST and CIFAR-10, which are widely
used in FL experiments, in a clustered setting as well as a general setting. In the clustered settings (one
on MNIST and one on CIFAR-10), we artificially create 5 client clusters where clients in the same cluster
possess data samples with the same labels. Thus, clients in the same cluster are naturally regarded as
friends. However, the clustering structure is unknown to our algorithm. Such a clustering setting provides
a controlled environment for us to evaluate the friend discovery performance of FL-FDMS. In the general
setting (on CIFAR-10), 20 clients receive a random subset of the whole dataset using a common way of
generating non-iid FL datesets that is widely used in existing works.
6.1.1 FL Dataset
Clustered Setting - MNIST : The MNIST dataset has 60000 training data samples with 10 classes. The
training dataset is first split into 10 sub-datasets with samples in the same sub-dataset having the same label.
There are 20 clients which are grouped into 5 client clusters with an equal number of clients. Each client
cluster is associated with 2 randomly drawn sub-datasets. Then each client randomly draws 200 samples
from its corresponding two sub-datasets. This approach to creating the FL dataset was introduced in a
recent clustered FL work Ghosh et al. (2020).
Clustered Setting - CIFAR-10 : The CIFAR-10 dataset has 50000 training data samples with 10 classes.
The training dataset is first split into 10 sub-datasets with samples in the same sub-dataset having the same
label. There are 20 clients which are grouped into 5 client clusters with an equal number of clients. Each
client cluster is associated with 2 randomly drawn sub-datasets. Then each client randomly draws 1000
samples from its corresponding two sub-datasets.
General Setting - CIFAR-10 : The CIFAR-10 dataset has 50000 training data samples. After shuffling the
samples in label order, all samples are divided into 200 partitions with each partition having 250 samples.
There are 20 clients. Each client then randomly picks 2 partitions. This method is a common way of
generating non-i.i.d. FL dataset, which is widely used in the existing works McMahan et al. (2017); Li et al.
(2021)
6.1.2 FL Models
MNIST : The CNN model has two 5 ×5 convolution layers, a fully connected layer with 320 units and
ReLU activation, and a final output layer with softmax. The first convolution layer has 10 channels while
the second one has 20 channels. Both layers are followed by 2 ×2 max pooling. The following parameters
are used for training: the local batch size BS= 5, the number of local iterations E= 2, the local learning
rateηL= 0.1and the global learning rate η= 0.1.
9Under review as submission to TMLR
CIFAR-10 : The CNN model has two 5 ×5 convolution layers, three fully connected layers and ReLU
activation, and a final output layer with softmax. The following parameters are used for training: the local
batch sizeBS= 20, the number of local iterations E= 2, the local learning rate ηL= 0.1and the global
learning rate η= 0.1.
6.1.3 FL Benchmarks
We compare FL-FDMS with three variants of FedAvg since in this paper we describe FL-FDMS in the
context of FedAvg.
Full Participation (Full) . This is the ideal case where all clients participate in FL without dropout. It is
used as a performance upper bound.
Client Dropout (Dropout) . In this case, the server simply ignores the dropout clients and performs
global aggregation on the non-dropout clients.
Staled Substitute (Stale) . Another method to deal with dropout clients is to use their last uploaded local
model updates for the current round’s global aggregation. Such a method was also used to deal with the
“straggler” issue in FL in some previous works Yan et al. (2020); Gu et al. (2021). Two apparent drawbacks
of this method are, firstly, the local model updates can be very outdated if a client keeps dropping out,
and secondly, the server has to keep a copy of the most recent local model update for every client, thereby
incurring a large storage cost when the number of clients is large.
0 20 40 60 80 100
Number of Rounds0.50.60.70.80.91.0T est AccuracyFull
Dropout
Stale
FDMS
(a) MNIST ( α= 0.3)
0 20 40 60 80 100
Number of Rounds0.50.60.70.80.91.0T est AccuracyFull
Dropout
Stale
FDMS (b) MNIST ( α= 0.5)
0 20 40 60 80 100
Number of Rounds0.50.60.70.80.91.0T est AccuracyFull
Dropout
Stale
FDMS (c) MNIST ( α= 0.7)
0 100 200 300 400 500
Number of Rounds0.10.20.30.40.5T est AccuracyFull
Dropout
Stale
FDMS
(d) CIFAR-10 ( α= 0.3)
0 100 200 300 400 500
Number of Rounds0.10.20.30.40.5T est AccuracyFull
Dropout
Stale
FDMS (e) CIFAR-10 ( α= 0.5)
0 100 200 300 400 500
Number of Rounds0.10.20.30.40.5T est AccuracyFull
Dropout
Stale
FDMS (f) CIFAR-10 ( α= 0.7)
Figure 1: Performance comparison on the clustered setting with various α
6.2 Performance Comparison
We first compare the convergence performance in the clustered setting under different dropout ratios α∈
{0.3,0.5,0.7}. Fig. 1 plots the convergence curves on the MNIST dataset and the CIFAR-10 dataset,
respectively. Several observations are made as follows. First, FL-FDMS outperforms Dropout andStale
in terms of test accuracy and convergence speed and achieves performance close to Fullin all cases. Second,
FL-FDMS reduces the fluctuations caused by the client dropout on the convergence curve. Third, with
10Under review as submission to TMLR
a larger dropout ratio, the performance improvement of FL-FDMS is larger. Fourth, on more complex
datasets (e.g., CIFAR-10), FL-FDMS achieves an even more significant performance improvement.
We note that Staleis the most sensitive to the dropout ratio αand its performance degrades significantly
asαincreases. This is because with a larger α, individual clients have few participating opportunities. As
a result, the staled local models become too outdated to provide useful information for the current round’s
global model updating.
0 100 200 300 400 500
Number of Rounds0.10.20.30.40.5T est AccuracyFull
Dropout
Stale
FDMS
(a) CIFAR-10 ( α= 0.3)
0 100 200 300 400 500
Number of Rounds0.10.20.30.40.5T est AccuracyFull
Dropout
Stale
FDMS (b) CIFAR-10 ( α= 0.5)
0 100 200 300 400 500
Number of Rounds0.10.20.30.40.5T est AccuracyFull
Dropout
Stale
FDMS (c) CIFAR-10 ( α= 0.7)
Figure 2: Performance comparison on the CIFAR-10 general setting with various α
We also perform experiments in the more general non-iid case to illustrate the wide applicability of the
proposed algorithm. Fig. 2 plots the convergence curves on CIFAR-10 under the general setting. The results
confirm the superiority of FL-FDMS . However, we also note that the improvement is smaller than that
in the clustered setting. This suggests a limitation of FL-FDMS , which works best when the “friendship”
relationship among the clients is stronger.
6.3 Friend Discovery
FL-FDMS relies on successfully discovering the friends of dropout clients. In Fig. 3, we show the pairwise
similarity scores in the final learning round. In our controlled clustered setting, 20 clients were grouped into
5 clusters, but this information was not known by the algorithm at the beginning. As the figure shows, the
similarity scores obtained by FL-FDMS are larger for intra-cluster client pairs and smaller for inter-cluster
client pairs, indicating that the clustering/friendship information can be successfully discovered. Moreover,
our experiments show that the discovered friendship is more obvious for CIFAR-10 than for MNIST. This is
likely due to the different dataset structures and the different CNN models adopted.
0.00.20.40.60.81.0
(a) MNIST dataset
0.00.20.40.60.81.0 (b) CIFAR-10 dataset
Figure 3: Pairwise similarity scores in the final learning round.
6.4 Computation Complexity
In this set of experiments, we investigate the computational complexity of FL-FDMS with and without the
complexity reduction mechanism (CR) on CIFAR-10 in the clustered setting. For the complexity reduction
11Under review as submission to TMLR
mechanism, we use two different threshold sequences: Θtin Theorem 2 (Case 1), and 0.5Θt(Case 2).
In Fig. 4, we can see that the complexity reduction mechanism can significantly reduce the computation
complexity while still achieving a similar learning performance. We also notice that a smaller threshold
sequence accelerates the client elimination process while keeping a similar learning performance.
100 200 300 400
Number of Rounds102030405060708090100Computation Per RoundWithout CR
With CR case 1
With CR case 2
(a) Computation complexity
0 100 200 300 400 500
Number of Rounds0.10.20.30.40.5T est Accuracy
Without CR
With CR case 1
With CR case 2 (b) Learning performance
Figure 4: Complexity reduction on the CIFAR-10 clustered setting ( α= 0.5).
6.5 Impact of non-i.i.d. level
We conducted experiments comparing different degrees of non-i.i.d.-ness, and the results are based on a
general setting which is introduced in section 6.1.3. To create non-i.i.d. data with varying degrees, we
divided the dataset into 100, 200, and 500 partitions for high, medium, and low degrees of non-i.i.d.-ness,
respectively. Then, we randomly selected one partition for the high degree, two partitions for the medium
degree, and five partitions for the low degree. In all cases, each client’s dataset comprises 500 samples.
0 100 200 300 400 500
Number of Rounds0.100.150.200.250.300.350.400.450.50T est AccuracyFull
Dropout
Stale
FDMS
(a) CIFAR-10 (P:500, S:100, Low)
0 100 200 300 400 500
Number of Rounds0.10.20.30.40.5T est AccuracyFull
Dropout
Stale
FDMS (b) CIFAR-10 (P:200, S:250, Medium)
0 100 200 300 400 500
Number of Rounds0.100.150.200.250.300.350.400.45T est AccuracyFull
Dropout
Stale
FDMS (c) CIFAR-10 (P:100, S:500, High)
Figure 5: Performance comparison on the CIFAR-10 general setting ( α= 0.5) with various non-i.i.d. level
The results are presented in Fig. 5. We observed that as the level of non-i.i.d.-ness increased, our method
FL-FDMS became less effective compared to Full, likely due to the increased difficulty in finding friends to
perform the model substitution. Nonetheless, FL-FDMS still outperformed both the Dropout andStale
benchmarks, demonstrating that model substitution can enhance convergence compared to doing nothing or
using an outdated model.
7 Conclusion
This paper investigated the impact of client dropout on the convergence of FL. Our analysis treats client
dropout as a special case of local update substitution and characterizes the convergence bound in terms of
the total substitution error. This inspired us to develop FL-FDMS, which discovers friend clients on-the-fly
and uses friends’ updates to reduce substitution errors, thereby mitigating the negative impact of client
dropout. Extensive experiment results show that discovering the client’s “friendship” is possible and it can
be a useful resort for addressing client dropout problems.
12Under review as submission to TMLR
References
Ravikumar Balakrishnan, Tian Li, Tianyi Zhou, Nageen Himayat, Virginia Smith, and Jeff Bilmes. Di-
verse client selection for federated learning via submodular maximization. In International Conference on
Learning Representations , 2021.
Wenlin Chen, Samuel Horvath, and Peter Richtarik. Optimal client sampling for federated learning. arXiv
preprint arXiv:2010.13723 , 2020a.
Yujing Chen, Yue Ning, Martin Slawski, and Huzefa Rangwala. Asynchronous online federated learning
for edge devices with non-iid data. In 2020 IEEE International Conference on Big Data (Big Data) , pp.
15–24. IEEE, 2020b.
Yae Jee Cho, Jianyu Wang, and Gauri Joshi. Client selection in federated learning: Convergence analysis
and power-of-choice selection strategies. arXiv preprint arXiv:2010.01243 , 2020.
Yae Jee Cho, Jianyu Wang, and Gauri Joshi. Towards understanding biased client selection in federated
learning. In International Conference on Artificial Intelligence and Statistics , pp. 10351–10375. PMLR,
2022.
Don Kurian Dennis, Tian Li, and Virginia Smith. Heterogeneity for the win: One-shot federated clustering.
InInternational Conference on Machine Learning , pp. 2611–2620. PMLR, 2021.
Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for clustered
federated learning. Advances in Neural Information Processing Systems , 33:19586–19597, 2020.
Xinran Gu, Kaixuan Huang, Jingzhao Zhang, and Longbo Huang. Fast federated learning in the presence
of arbitrary device unavailability. Advances in Neural Information Processing Systems , 34:12052–12064,
2021.
Ashwin R Jadhav. Federated-learning (pytorch), 2020. URL https://github.com/AshwinRJ/
Federated-Learning-PyTorch .
Eunjeong Jeong, Seungeun Oh, Hyesung Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim.
Communication-efficient on-device machine learning: Federated distillation and augmentation under non-
iid private data. arXiv preprint arXiv:1811.11479 , 2018.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for on-device federated learning. 2019.
Jakub konevcny, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint
arXiv:1610.05492 , 2016.
FanLai, XiangfengZhu, HarshaVMadhyastha, andMosharafChowdhury. Oort: Efficientfederatedlearning
via guided participant selection. In 15th Symposium on Operating Systems Design and Implementation ,
pp. 19–35, 2021.
Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. Federated learning on non-iid data silos: An
experimental study. arXiv preprint arXiv:2102.02079 , 2021.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated
optimization in heterogeneous networks. Proceedings of Machine learning and systems , 2:429–450, 2020a.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated
optimization in heterogeneous networks. Proceedings of Machine Learning and Systems , 2:429–450, 2020b.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg
on non-iid data. arXiv preprint arXiv:1907.02189 , 2019a.
13Under review as submission to TMLR
Yanan Li, Shusen Yang, Xuebin Ren, and Cong Zhao. Asynchronous federated learning with differential
privacy for edge intelligence. arXiv preprint arXiv:1912.07902 , 2019b.
Oded Maron and Andrew Moore. Hoeffding races: Accelerating model selection search for classification and
function approximation. Advances in neural information processing systems , 6, 1993.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and
statistics , pp. 1273–1282. PMLR, 2017.
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konečn` y, Sanjiv
Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295 ,
2020.
AmirhosseinReisizadeh, IsidorosTziotis, HamedHassani, AryanMokhtari, andRamtinPedarsani. Straggler-
resilient federated learning: Leveraging the interplay between statistical accuracy and system heterogene-
ity.arXiv preprint arXiv:2012.14453 , 2020.
Monica Ribero and Haris Vikalo. Communication-efficient federated learning via optimal client sampling.
arXiv preprint arXiv:2007.15197 , 2020.
Yichen Ruan and Carlee Joe-Wong. Fedsoft: Soft clustered federated learning with proximal local updating.
InProceedings of the AAAI Conference on Artificial Intelligence , volume 36, pp. 8124–8131, 2022.
Yichen Ruan, Xiaoxi Zhang, Shu-Che Liang, and Carlee Joe-Wong. Towards flexible device participation in
federated learning. In International Conference on Artificial Intelligence and Statistics , pp. 3403–3411.
PMLR, 2021.
Felix Sattler, Simon Wiedemann, Klaus-Robert Müller, and Wojciech Samek. Robust and communication-
efficient federated learning from non-iid data. IEEE transactions on neural networks and learning systems ,
31(9):3400–3413, 2019.
Dian Shi, Liang Li, Maoqiang Wu, Minglei Shu, Rong Yu, Miao Pan, and Zhu Han. To talk or to work:
Dynamic batch sizes assisted time efficient federated learning over future mobile edge devices. IEEE
Transactions on Wireless Communications , 2022.
Sebastian U Stich. Local sgd converges fast and communicates little. arXiv preprint arXiv:1805.09767 , 2018.
Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified sgd with memory. Advances in
Neural Information Processing Systems , 31, 2018.
Jianyu Wang and Gauri Joshi. Cooperative sgd: A unified framework for the design and analysis of
communication-efficient sgd algorithms. arXiv preprint arXiv:1808.07576 , 2018.
Hongda Wu and Ping Wang. Node selection toward faster convergence for federated learning on non-iid
data.IEEE Transactions on Network Science and Engineering , 2022.
WentaiWu, LigangHe, WeiweiLin, RuiMao, CarstenMaple, andStephenJarvis. Safa: asemi-asynchronous
protocol for fast federated learning with low overhead. IEEE Transactions on Computers , 70(5):655–668,
2020.
Cong Xie, Sanmi Koyejo, and Indranil Gupta. Asynchronous federated optimization. arXiv preprint
arXiv:1903.03934 , 2019.
Yikai Yan, Chaoyue Niu, Yucheng Ding, Zhenzhe Zheng, Fan Wu, Guihai Chen, Shaojie Tang, and Zhihua
Wu. Distributed non-convex optimization with sublinear speedup under intermittent client availability.
arXiv preprint arXiv:2002.07399 , 2020.
Haibo Yang, Minghong Fang, and Jia Liu. Achieving linear speedup with partial worker participation in
non-iid federated learning. In International Conference on Learning Representations , 2020.
14Under review as submission to TMLR
Haibo Yang, Xin Zhang, Prashant Khanduri, and Jia Liu. Anarchic federated learning. In International
Conference on Machine Learning , pp. 25331–25363. PMLR, 2022.
Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient momentum sgd
fordistributednon-convexoptimization. In International Conference on Machine Learning , pp.7184–7193.
PMLR, 2019a.
HaoYu, SenYang, andShenghuoZhu. Parallelrestartedsgdwithfasterconvergenceandlesscommunication:
Demystifying why model averaging works for deep learning. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 33, pp. 5693–5700, 2019b.
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated learning
with non-iid data. arXiv preprint arXiv:1806.00582 , 2018.
15Under review as submission to TMLR
A Proofs
A.1 Proof of Lemma 2
Consider a dropout client kand a non-friend client i̸∈Bk. We analyze the probability that it is selected by
the algorithm. According to our selection rule, iis selected only if it has the highest similarity score with
clientkso far. Hence, Ri,k
tmust be greater than Rj,k
tfor at least one j∈Bk∩St. Thus, the following
inequality holds
Pr{ϕt(k) =i}≤Pr{Ri,k
t≥Rj,k
t,for somej∈Bk∩St} (18)
≤Pr{Ri,k
t≥µi,k+δ
2}+Pr{Rj,k
t≤µj,k−δ
2} (19)
≤exp/parenleftigg
−Ni,k
tδ2
2/parenrightigg
+ exp/parenleftigg
−Nj,k
tδ2
2/parenrightigg
(20)
≤2 exp/parenleftbigg−βδ2
kt
2/parenrightbigg
(21)
whereδ=µj,k−µi,k≥δk. Because the number of non-friend clients of a client kis at most K, the
probability of selecting a non-friend client is thus upper-bounded by 2Kexp/parenleftig
−βδ2
kt
2/parenrightig
. Taking into account
δmin= minkδkcompletes the proof.
A.2 Proof of Theorem 1
In this section, we give the proofs in detail. Due to the smoothness in Assumption 1, taking the expectation
off(wt+1)over the randomness in round t, we have
Et[f(wt+1)] (22)
≤f(wt) +⟨∇f(wt),Et[wt+1−wt]⟩+L
2Et[∥wt+1−wt∥2] (23)
=f(wt) +⟨∇f(wt),Et[ηηL∆t+ηηLE∇f(wt)−ηηLE∇f(wt)]⟩+L
2η2η2
LEt[∥∆t∥2] (24)
=f(wt)−ηηLE∥∇f(wt)∥2+η⟨∇f(wt),E[ηL∆t+ηLE∇f(wt)]⟩/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
A1+L
2η2η2
LEt[∥∆t∥2]/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
A2(25)
16Under review as submission to TMLR
Note that the term A1can be bounded as follows:
A1=⟨∇f(wt),Et[ηL∆t+ηLE∇f(wt)]⟩ (26)
=⟨∇f(wt),Et[ηL¯∆t+ηLet+ηLE∇f(wt)]⟩ (27)
=/angbracketleftigg
∇f(wt),Et/bracketleftigg
−1
KK/summationdisplay
k=1E−1/summationdisplay
τ=0ηL∇Fk(wk
t,τ) +ηLet+ηLE1
KK/summationdisplay
k=1∇Fk(wt)/bracketrightigg/angbracketrightigg
(28)
=/angbracketleftigg
/radicalbig
ηLE∇f(wt),−√ηL
K√
EEt/bracketleftiggK/summationdisplay
k=1E−1/summationdisplay
τ=0(∇Fk(wk
t,τ)−∇Fk(wt))−Ket/bracketrightigg/angbracketrightigg
(29)
(a1)=ηLE
2∥∇f(wt)∥2+ηL
2EK2Et/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleK/summationdisplay
k=1E−1/summationdisplay
τ=0(∇Fk(wk
t,τ)−∇Fk(wt))−Ket/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
−ηL
2EK2Et/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleK/summationdisplay
k=1E−1/summationdisplay
τ=0∇Fk(wk
t,τ)−Ket/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(30)
(a2)
≤ηLE
2∥∇f(wt)∥2+ηL
EK2Et/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleK/summationdisplay
k=1E−1/summationdisplay
τ=0(∇Fk(wk
t,τ)−∇Fk(wt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
−ηL
2EK2Et/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleK/summationdisplay
k=1E−1/summationdisplay
τ=0∇Fk(wk
t,τ)−Ket/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ηLEt∥et∥2
E(31)
(a3)
≤ηLE
2∥∇f(wt)∥2+ηL
KK/summationdisplay
k=1E−1/summationdisplay
τ=0Et/vextenddouble/vextenddouble∇Fk(wk
t,τ)−∇Fk(wt)/vextenddouble/vextenddouble2
−ηL
2EK2Et/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleK/summationdisplay
k=1E−1/summationdisplay
τ=0∇Fk(wk
t,τ)−Ket/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ηLEt∥et∥2
E(32)
(a4)
≤ηLE
2∥∇f(wt)∥2+ηLL2
KK/summationdisplay
k=1E−1/summationdisplay
τ=0Et/vextenddouble/vextenddoublewk
t,τ−wt/vextenddouble/vextenddouble2
−ηL
2EK2Et/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleK/summationdisplay
k=1E−1/summationdisplay
τ=0∇Fk(wk
t,τ)−Ket/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ηLEt∥et∥2
E(33)
(a5)
≤ηLE(1
2+ 30η2
LE2L2)∥∇f(wt)∥2+ 5η3
LE2L2(ρ2
L+ 6Eρ2
G)
−ηL
2EK2Et/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleK/summationdisplay
k=1E−1/summationdisplay
τ=0∇Fk(wk
t,τ)−Ket/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ηLEt∥et∥2
E(34)
where (a1)follows from that ⟨x,y⟩=1
2[∥x∥2+∥y∥2−∥x−y∥2],(a2)is due to that E∥x1+x2∥2≤
2E[∥x1∥2+∥x2∥2],(a3)is due to that E∥x1+...+xn∥2≤nE[∥x1∥2+...∥xn∥2],(a4)is due to Assumption
1 and (a5)follows from Lemma 1.
17Under review as submission to TMLR
The termA2can be bounded as
A2=Et[∥∆t∥2] =Et[∥¯∆t+et∥2] (35)
(a6)
≤2Et∥¯∆t∥2+ 2Et∥et∥2
≤2
K2Et
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleK/summationdisplay
k=1E−1/summationdisplay
τ=0gk
t,τ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ 2Et∥et∥2(36)
(a7)
≤2
K2Et
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleK/summationdisplay
k=1E−1/summationdisplay
τ=0(gk
t,τ−∇Fk(wk
t,τ))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+2
K2Et
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleK/summationdisplay
k=1E−1/summationdisplay
τ=0∇Fk(wk
t,τ)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ 2Et∥et∥2(37)
(a8)
≤2E
Kρ2
L+4
K2Et
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleK/summationdisplay
k=1E−1/summationdisplay
τ=0∇Fk(wk
t,τ)−Ket/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+4
K2Et∥Ket∥2+ 2Et∥et∥2
=2E
Kρ2
L+4
K2Et
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleK/summationdisplay
k=1E−1/summationdisplay
τ=0∇Fk(wk
t,τ)−Ket/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ 6Et∥et∥2(38)
where both (a6)is due to that E∥x1+x2∥2≤2E[∥x1∥2+∥x2∥2],(a7)follows the fact that E[∥x∥2] =
E[∥x−Ex∥2] +∥Ex∥2, and (a8)is due to Assumption 3. Substituting the inequalities of A1andA2into the
original inequality, we have:
Et[f(wt+1)] (39)
≤f(wt)−ηηLE∥∇f(wt)∥2+η⟨∇f(wt),E[ηL∆t+ηLE∇f(wt)]⟩/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
A1+L
2η2η2
LEt[∥∆t∥2]/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
A2(40)
≤f(wt)−ηηLE∥∇f(wt)∥2
+ηηLE(1
2+ 30η2
LE2L2)∥∇f(wt)∥2+ 5ηη3
LE2L2(ρ2
L+ 6Eρ2
G)
−ηηL
2EK2Et/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleK/summationdisplay
k=1E−1/summationdisplay
τ=0∇Fk(wk
t,τ)−Ket/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ηηLEt∥et∥2
E
+ELη2η2
L
Kρ2
L+2Lη2η2
L
K2Et
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleK/summationdisplay
k=1E−1/summationdisplay
τ=0∇Fk(wk
t,τ)−Ket/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ 3η2η2
LLEt∥et∥2(41)
=f(wt)−ηηLE(1
2−30η2
LE2L2)∥∇f(wt)∥2
+ 5ηη3
LE2L2(ρ2
L+ 6Eρ2
G) +ELη2η2
L
Kρ2
L+/parenleftigηηL
E+ 3η2η2
LL/parenrightig
Et∥et∥2
−/parenleftbiggηηL
2EK2−2Lη2η2
L
K2/parenrightbigg
Et/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleK/summationdisplay
k=1E−1/summationdisplay
τ=0∇Fk(wk
t,τ)−Ket/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(42)
(a9)
≤f(wt)−cηηLE∥∇f(wt)∥2+ 5ηη3
LE2L2(ρ2
L+ 6Eρ2
G) +ELη2η2
L
Kρ2
L+/parenleftigηηL
E+ 3η2η2
LL/parenrightig
Et∥et∥2(43)
where (a9)follows from/parenleftig
ηηL
2EK2−2Lη2η2
L
K2/parenrightig
<0ifηηL≤1
4EL, and that there exits a constant c>0satisfying
(1
2−30η2
LE2L2)>c> 0ifηL<1√
60EL.
18Under review as submission to TMLR
Rearranging and summing from t= 0,...,T−1, we have:
T−1/summationdisplay
t=0cηηLEE∥∇f(wt)∥2(44)
≤f(w0)−f(wT) +TEηηL/bracketleftbigg
5η2
LEL2(ρ2
L+ 6Eρ2
G) +ηηLL
Kρ2
L/bracketrightbigg
+/parenleftigηηL
E+ 3η2η2
LL/parenrightigT=1/summationdisplay
t=0Et∥et∥2(45)
which implies,
min
t=0,...,T−1E∥∇f(wt)∥2≤f0−f∗
cηηLET+ Φ + Ψ(e0,...,eT−1) (46)
where
Φ =1
c/bracketleftbigg
5η2
LEL2(ρ2
L+ 6Eρ2
G) +ηηLL
Kρ2
L/bracketrightbigg
(47)
Ψ(e0,...,eT−1) =1 + 3ηηLLE
cE2TT−1/summationdisplay
t=0Et∥et∥2(48)
This completes the proof.
A.3 Proof of Theorem 2
A sufficient condition for the bound to hold is that after TFL rounds, no friend of client kwas eliminated
fromCt
kby running our algorithm. Thus, we are interested in bounding the probability that any particular
friend client iis eliminated in a particular round tbeforeT.
Pr(iis eliminated in round t) (49)
≤Pr(Rk,j
t−Rk,i
t≥Θt, for somej̸=i) (50)
≤/summationdisplay
j̸=iPr(Rk,j
t−Rk,i
t≥Θt) (51)
≤K/parenleftbigg
Pr(Rk,i≤µk,i−Θt−δf
2) +Pr(Rk,j∗≥µk,j∗+Θt−δf
2)/parenrightbigg
(52)
≤2Kexp/parenleftbigg−β(Θt−δf)2t
2/parenrightbigg
=q (53)
wherej∗is the best friend of client k. The last equality holds by letting
Θt=/radicaligg
2 ln(2K)−2 lnq
βt+δf (54)
Next, the probability that a friend client iis eliminated in any round up to round Tis bounded as follows
Pr(iis eliminated up to round T)≤/summationdisplay
t≤T−1Pr(iis eliminated in round t)≤Tq (55)
Thus,
Pr(any friend of client kis eliminated up to round T)≤|Bk|Tq (56)
Furthermore,
Pr(any friend of any client is eliminated up to round T)≤K|Bk|Tq (57)
19Under review as submission to TMLR
Therefore, by letting p=KBmaxTqand
Θt=/radicaligg
2 ln(2K2TBmax)−2 lnp
βt+δf (58)
we ensure that the probability that no friend of any client was eliminated from the corresponding candidate
set byTis at least 1−p. This concludes the proof.
A.4 Bounds on E∥et∥2
The error bound with client dropout:
E[∥et∥2] =E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
K/summationdisplay
k∈K\S t(˜∆k
t−∆k
t)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
=E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
K/summationdisplay
k∈K\S t1
St/summationdisplay
k′∈St(∆k′
t−∆k
t)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
 (59)
≤(K−St)2
K2σ2
P≤α2σ2
P (60)
The error bound with friend model substitution (full information) :
E[∥et∥2] =E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
K/summationdisplay
k∈K\S t(˜∆k
t−∆k
t)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
=E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
K/summationdisplay
k∈K\S t(∆ϕt(k)
t−∆k
t)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
 (61)
≤(K−St)2
K2σ2
F≤α2σ2
F (62)
whereϕt(k)is a friend of kthat does not dropout in round t.
The error bound with friend model substitution (learning):
E∥et∥2=E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
K/summationdisplay
k∈K\S t(˜∆k
t−∆k
t)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤K−St
K2/summationdisplay
k∈K\S tE∥˜∆k
t−∆k
t∥2(63)
≤K−St
K2/summationdisplay
k∈K\S t/parenleftbigg
2Kexp/parenleftbigg−βδ2
kt
2/parenrightbigg
σ2
P+ (1−2Kexp/parenleftbigg−βδ2
kt
2/parenrightbigg
)σ2
F/parenrightbigg
(64)
≤(K−St)2
K2/parenleftbigg
σ2
F+ 2Kexp/parenleftbigg−βδ2
mint
2/parenrightbigg
(σ2
P−σ2
F)/parenrightbigg
(65)
≤α2/parenleftbigg
σ2
F+ 2Kexp/parenleftbigg−βδ2
mint
2/parenrightbigg
(σ2
P−σ2
F)/parenrightbigg
(66)
20Under review as submission to TMLR
A.5 Bounds on Ψ(e0,...,eT−1)with friend model substitution (learning)
Ψ(e0,...,eT−1) (67)
=1 + 3ηηLLE
cE2TT−1/summationdisplay
t=0Et[∥et∥2] (68)
≤α2σ2
F(1 + 3ηηLLE)
cE2+ 2Kα2(σ2
P−σ2
F)(1 + 3ηηLLE)
cE2TT−1/summationdisplay
t=0exp/parenleftbigg−βδ2
mint
2/parenrightbigg
(69)
≤α2σ2
F(1 + 3ηηLLE)
cE2+ 2Kα2σ2
P(1 + 3ηηLLE)
cE2TT−1/summationdisplay
t=0exp/parenleftbigg−βδ2
mint
2/parenrightbigg
(70)
≤α2σ2
F(1 + 3ηηLLE)
cE2+ 2Kα2σ2
P(1 + 3ηηLLE)
cE21−exp/parenleftig
−βδ2
minT
2/parenrightig
T/parenleftig
1−exp/parenleftig
−βδ2
min
2/parenrightig/parenrightig (71)
≤Ψ∗+ 2K¯Ψ1−exp/parenleftig
−βδ2
minT
2/parenrightig
T/parenleftig
1−exp/parenleftig
−βδ2
min
2/parenrightig/parenrightig (72)
B The Relaxed Friend Presence Case
In this section, we consider a relaxed case without Assumption 5. Suppose that a friend of the dropout client
is present in each round with a probability 1−r, then the probability that our algorithm selects a non-friend
client for a dropout client in round tis upper bounded by (1−r)2Kexp/parenleftig
−βδ2
mint
2/parenrightig
+r. Then we can get the
bound on E∥et∥2:
E∥et∥2≤α2/parenleftbigg
σ2
F+ 2(1−r)Kexp/parenleftbigg−βδ2
mint
2/parenrightbigg
σ2
P+rσ2
P/parenrightbigg
(73)
Plugging this bound into Ψ(e0,...,eT−1), we can get the accumulate substitution error as follows:
Ψ(e0,...,eT−1)≤Ψ∗+r¯Ψ + 2(1−r)K¯Ψ1−exp/parenleftig
−βδ2
minT
2/parenrightig
T/parenleftig
1−exp/parenleftig
−βδ2
min
2/parenrightig/parenrightig (74)
The convergence bound without Assumption 5 has an additional constant term resulting from friend absence,
and the additional constant term cannot be eliminated with time or batch size increase.
Proof 1 The error bound with friend model substitution (learning) under the relaxed friend presence case
is
E∥et∥2=E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
K/summationdisplay
k∈K\S t(˜∆k
t−∆k
t)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤K−St
K2/summationdisplay
k∈K\S tE∥˜∆k
t−∆k
t∥2(75)
≤α2/parenleftbigg
(1−r)2Kexp/parenleftbigg−βδ2
mint
2/parenrightbigg
+r/parenrightbigg
σ2
P+α2/parenleftbigg
1−/parenleftbigg
(1−r)2Kexp/parenleftbigg−βδ2
mint
2/parenrightbigg
+r/parenrightbigg/parenrightbigg
σ2
F(76)
≤α2σ2
F+α2/parenleftbigg
(1−r)2Kexp/parenleftbigg−βδ2
mint
2/parenrightbigg
+r/parenrightbigg
σ2
P (77)
≤α2/parenleftbigg
σ2
F+ 2(1−r)Kexp/parenleftbigg−βδ2
mint
2/parenrightbigg
σ2
P+rσ2
P/parenrightbigg
(78)
21Under review as submission to TMLR
And the corresponding accumulated substitution error equals
Ψ(e0,...,eT−1) (79)
=1 + 3ηηLLE
cE2TT−1/summationdisplay
t=0Et[∥et∥2] (80)
≤α2(σ2
F+rσ2
P)(1 + 3ηηLLE)
cE2+ 2Kα2(σ2
P−σ2
F)(1 + 3ηηLLE)
cE2TT−1/summationdisplay
t=0exp/parenleftbigg−βδ2
mint
2/parenrightbigg
(81)
≤α2(σ2
F+rσ2
P)(1 + 3ηηLLE)
cE2+ 2Kα2σ2
P(1 + 3ηηLLE)
cE2TT−1/summationdisplay
t=0exp/parenleftbigg−βδ2
mint
2/parenrightbigg
(82)
≤α2(σ2
F+rσ2
P)(1 + 3ηηLLE)
cE2+ 2Kα2σ2
P(1 + 3ηηLLE)
cE21−exp/parenleftig
−βδ2
minT
2/parenrightig
T/parenleftig
1−exp/parenleftig
−βδ2
min
2/parenrightig/parenrightig (83)
≤Ψ∗+r¯Ψ + 2K¯Ψ1−exp/parenleftig
−βδ2
minT
2/parenrightig
T/parenleftig
1−exp/parenleftig
−βδ2
min
2/parenrightig/parenrightig (84)
22Under review as submission to TMLR
C Additional Experiments
C.1 Impact of number of local iterations E
The error bound of FL-FDMS E∥et∥2in Eq. 16 is influenced by the number of local iterations Eand the
number of clients K. Next, we perform additional experiments to explore their impacts. We present more
results on the performance comparison in the MNIST clustered setting and the CIFAR-10 clustered setting
with different E. We fixα= 0.5andK= 20for all the following experiments. To investigate the impact of
E, we consider two values E= 1andE= 5.
0 20 40 60 80 100
Number of Rounds0.50.60.70.80.91.0T est AccuracyFull
Dropout
Stale
FDMS
(a) MNIST ( E= 1)
0 20 40 60 80 100
Number of Rounds0.50.60.70.80.91.0T est AccuracyFull
Dropout
Stale
FDMS (b) MNIST ( E= 5)
0 100 200 300 400 500
Number of Rounds0.10.20.30.40.5T est AccuracyFull
Dropout
Stale
FDMS
(c) CIFAR-10 ( E= 1)
0 100 200 300 400 500
Number of Rounds0.10.20.30.40.5T est AccuracyFull
Dropout
Stale
FDMS (d) CIFAR-10 ( E= 5)
Figure 6: Performance comparison with α= 0.5andK= 20
In Fig. 6, we find that the FL-FDMS still shows superior performance in terms of test accuracy and
convergence speed. However, Dropout andStaleshow different trends for different E. For a larger E,
using staled models tends to help the dropout situation better.
C.2 Impact of number of clients K
To investigate the impact of K, we fixE= 2and increase the number of clients to K= 40. To keep the
same total amount of data in the system, we adjust just the number of data samples on each client. For
MNIST, each client now has 100 samples. For CIFAR-10, each client has 500 samples.
By comparing Fig. 7 and the corresponding parts in Fig. 1, we find that as Kincreases, the FL-FDMS
outperforms Dropout andStaleeven more. This is because as Kincreases, more clients dropout. If
the model updates from dropout clients are not compensated, the global model can gradually deviate from
the optimal value and eventually degrade the learning performance and affect the system’s stability. The
additional experiments further verify that FL-FDMS can handle well the client dropout in FL.
C.3 Experiments with FedProx
In the Cifar10 clustered setting, we have conducted additional performance comparison experiments us-
ing FedProx Li et al. (2020a) with the FedProx parameter value µ= 0.2. Our results, shown in Fig. 8,
demonstrate that the proposed FL-FDMS algorithm remains effective.
23Under review as submission to TMLR
0 20 40 60 80 100
Number of Rounds0.50.60.70.80.91.0T est AccuracyFull
Dropout
Stale
FDMS
(a) MNIST ( K= 40)
0 100 200 300 400 500
Number of Rounds0.10.20.30.40.5T est AccuracyFull
Dropout
Stale
FDMS (b) CIFAR-10 ( K= 40)
Figure 7: Performance comparison with α= 0.5andE= 2
0 100 200 300 400 500
Number of Rounds0.10.20.30.40.5T est AccuracyFull
Dropout
Stale
FDMS
(a) CIFAR-10 (FedAvg)
0 100 200 300 400 500
Number of Rounds0.10.20.30.40.5T est AccuracyFull
Dropout
Stale
FDMS (b) CIFAR-10 (FedProx)
Figure 8: Performance comparison on the CIFAR-10 clustered setting ( α= 0.5) with different FL algorithms
C.4 Experiments on the FMNIST datasets
We present additional performance comparison results in the FMNIST clustered setting, and the results in
Fig. 9 are consistent with the conclusions we have drawn from prior other datasets.
0 50 100 150 200 250 300
Number of Rounds0.30.40.50.60.70.80.9T est AccuracyFull
Dropout
Stale
FDMS
(a) FMNIST ( α= 0.3)
0 50 100 150 200 250 300
Number of Rounds0.30.40.50.60.70.80.9T est AccuracyFull
Dropout
Stale
FDMS (b) FMNIST ( α= 0.5)
0 50 100 150 200 250 300
Number of Rounds0.30.40.50.60.70.80.9T est AccuracyFull
Dropout
Stale
FDMS (c) FMNIST ( α= 0.7)
Figure 9: Performance comparison on the FMNIST clustered setting with various α
24