Published in Transactions on Machine Learning Research (12/2023)
Mixture of Dynamical Variational Autoencoders
for Multi-Source Trajectory Modeling and Separation
Xiaoyu Lin xiaoyu.lin@inria.fr
Inria @ Univ. Grenoble Alpes, LJK, CNRS, France
Laurent Girin laurent.girin@grenoble-inp.fr
Univ. Grenoble Alpes, Grenoble-INP, GIPSA-lab, France
Xavier Alameda-Pineda xavier.alameda-pineda@inria.fr
Inria @ Univ. Grenoble Alpes, LJK, CNRS, France
Reviewed on OpenReview: https: // openreview. net/ forum? id= sbkZKBVC31
Abstract
In this paper, we propose a latent-variable generative model called mixture of dynamical
variational autoencoders (MixDVAE) to model the dynamics of a system composed of mul-
tiple moving sources. A DVAE model is pre-trained on a single-source dataset to capture the
source dynamics. Then, multiple instances of the pre-trained DVAE model are integrated
into a multi-source mixture model with a discrete observation-to-source assignment latent
variable. The posterior distributions of both the discrete observation-to-source assignment
variable and the continuous DVAE variables representing the sources content/position are
estimated using a variational expectation-maximization algorithm, leading to multi-source
trajectories estimation. We illustrate the versatility of the proposed MixDVAE model on
two tasks: a computer vision task, namely multi-object tracking, and an audio processing
task, namely single-channel audio source separation. Experimental results show that the
proposed method works well on these two tasks, and outperforms several baseline methods.
1 Introduction
1.1 Latent-variable generative models: From GMMs to DVAEs
Latent-variable generative models (LVGMs) are a very general class of probabilistic models that introduce
latent (unobserved) variables to model complex distributions over the observed variables. Depending on the
type of considered latent variables, different LVGM structures can be defined. Mixture models , such as the
Gaussian Mixture Model (GMM), are LVGMs with a discrete latent variable (McLachlan & Basford, 1988).
They are widely used in various pattern recognition and signal processing tasks to model the distribution
of a signal that can take several different states, each state being encoded by a different value of the latent
variable. The (marginal) distribution of the observed data is modeled as a linear combination of compo-
nent distributions, which are conditioned on the latent variable (in the case of GMM, these are Gaussian
distributions), each corresponding to a different possible state. The mixing coefficients represent the prior
distribution of the discrete latent variable and determine the relative weight of each component.
Simple mixture models are appropriate to model ‘static’ data since they do not consider possible temporal
correlations in data sequences. More sophisticated LVGMs can be designed to model the dynamics, i.e. the
temporal dependencies, of sequential data. First, mixture models can be generalized by applying a Markov
model on the latent variable at different time steps, resulting in a hidden Markov model (HMM) (Rabiner &
Juang, 1986). For example, when applying a first-order Markov model, the prior distribution of the latent
1Published in Transactions on Machine Learning Research (12/2023)
variableattime tdependsonthelatentvariableattime t−1viaatransitionmatrix. Themixturecomponent,
often called the observation model in this context, is conditioned on the latent variable at time t. A variety
of different conditional distributions can be used. For example, the observation model can be a Gaussian (as
in the GMM) or it can be itself a GMM (i.e., one GMM for each state). The latter HMM-GMM combination
has been the state-of-the-art in automatic speech recognition for a decade in the pre-deep-learning era (Yu
& Deng, 2016). Another notable extension of HMMs is the factorial HMMs (Ghahramani & Jordan, 1995),
which consider a set of parallel factorial discrete latent variables instead of a single one. Each latent variable
follows a Markov model and has its own transition matrix. The conditional distribution of the observed
variable at time step tis a Gaussian distribution with the mean being a weighted combination of the latent
variables at the same time step, and with a common covariance matrix. Factorial HMMs are suitable for
modeling sequential data generated by the interaction of multiple independent processes.
A discrete latent variable can only be used to model categorical latent generative factors. If we consider that
the latent state evolves continuously and is better represented by a continuous latent variable, we enter in the
world of continuous dynamical models and state space models (SSMs) (Aoki, 2013). The simplest and most
commonly used continuous dynamical model is the linear(-Gaussian) dynamical system (LDS), in which the
distribution of the latent variable at time step tis a Gaussian with the mean being a linear function of the
latentvariableattimestep t−1(Ghahramani&Hinton,1996). IftheobservationmodelisalsoGaussianwith
the mean being a linear function of the latent variable at time t, a famous analytical solution1for the LDS is
the Kalman filter (Kalman, 1960). However, the linear-Gaussian assumption can be a significant limitation
for real-world signals with complex dynamics. A widely-used generalization of the Kalman filter to sequential
data with non-linear dynamics is the extended Kalman filter (Einicke & White, 1999; Zarchan, 2005), which
is a first-order Gaussian approximation to the Kalman filter based on local linearization using the Taylor
series expansion. Another interesting extension is to combine a set of LDSs with an HMM, resulting in the
switching state space model (also named switching Kalman filter) (Murphy, 1998; Ghahramani & Hinton,
2000). This model segments the sequential data into different regimes, each regime being modeled by an
LDS, and the succession of regimes is ruled by the HMM. Recently, deep neural networks (DNNs), and in
particular recurrent neural networks (RNNs), have been used within LVGM structures to model sequential
data. In this line, the dynamical variational autoencoders (DVAEs) (Girin et al., 2021) are a family of
powerful LVGMs that extend the famous variational autoencoder (VAE) (Kingma & Welling, 2014; Rezende
et al., 2014) to model complex non-linear temporal dependencies within a sequence of observed data vectors
and corresponding (continuous) latent vectors. DVAEs have been successfully applied on different types of
sequential data such as speech signals (Bie et al., 2021) and 3D human motion data (Bie et al., 2022a).
1.2 Contribution: Multi-Source Mixture of DVAEs – Model and solution
All the LVGMs discussed so far have been used to model the distribution of single-source data, the source
being either static and have several possible states (in the case of mixture models) or sequential with different
types of underlying dynamics (e.g., locally linear). In real-life scenarios, we often encounter situations where
a number of sequential source signals appear concurrently in a natural scene for a certain period of time and
are observed jointly. Each underlying source can have its own dynamics, and the problem is to obtain an
estimationofthecontentand/orthepositionalongtimeofeachsourceseparately, whichincludesconsistently
recovering the identity of each source over time. In short, we want to estimate and separate each source’s
trajectory from a set of mixed-up observations.
In this paper, we propose to tackle this problem within a deep LVGM probabilistic framework, with a model
combining the following two bricks: (a) a deep LVGM for modeling the dynamics of each source indepen-
dently; in this work, we propose to use a DVAE to model each individual source. The DVAE-generated
random vector represents the source vector, i.e. the source content/position that we want to track over
time, and the latent random vector represents the underlying (continuous) hidden state/factor that governs
the source dynamics. (b) A discrete latent assignment variable which assigns each observation in the set
of (mixed-up) observations to a source. We name the resulting model as Multi-Source Mixture of DVAEs
(MixDVAE). In addition to the MixDVAE model, we propose a multi-source trajectory estimation algo-
rithm (i.e., a solution to the MixDVAE model). Importantly, this estimation method does not require a
1In the present context, a model solution is an algorithm for model parameters estimation and latent variables inference.
2Published in Transactions on Machine Learning Research (12/2023)
massive multi-source annotated dataset for model training. Instead, we first pre-train the DVAE model on
an unlabeled (synthetic or natural) single-source trajectory dataset, to capture the dynamics of an indi-
vidual source type. Afterwards, the pre-trained DVAE is plugged into the MixDVAE model together with
the observation-to-source assignment latent variable to solve the problem for each multi-source test data
sequence to process. For each test data sequence, the (approximate) posterior distributions of both the
observation-to-source assignment variable and the source vector of each source are derived using the varia-
tional inference methodology – more specifically, we propose a variational expectation-maximization (VEM)
algorithm (Jordan et al., 1999; Bishop, 2006; Wainwright et al., 2008).
The proposed model and method are versatile in essence. They can be easily adapted and applied to a
variety of estimation problems with multiple dynamical sources with different configurations. For example,
if all sources are assumed to have similar dynamics, a single DVAE model can be used to model all sources
(more specifically, a different instance of the same DVAE model can be used for each source) and only one
pre-training is made on a single single-source dataset. If different types of sources are considered, with
different dynamics, one can use different instances of the same DVAE model, but pre-trained on different
single-source datasets, or one can use (different instances of) different DVAE models, also pre-trained on
differentsingle-sourcedatasets. Inanycase, asstatedabove, thereisnoneedforamassivedatasetcontaining
annotated mixtures of simultaneous sources, as would be the case with a fully-supervised approach. Labeled
multi-source datasets must be much larger than single-source datasets, since the mixture process intrinsically
multipliesthecontentdiversity, andthuscanbeverycostlyanddifficulttoobtain. Therefore, ourmethodcan
be considered as data-frugal and weakly supervised compared to a fully-supervised method. One limitation
of the proposed model, though, is that the VEM algorithm applied at test time is relatively costly in
computation (this point is investigated in our study). Another limitation is the fact that all sources are
assumed to behave independently. In other words, the proposed MixDVAE does not explicitly model the
possible interactions between the different sources. This is planned for future work. We illustrate the
versatility of MixDVAE by applying it to two notably different tasks, in computer vision and in audio
processing – namely multi-object tracking (MOT) and single-channel audio source separation (SC-ASS) –
and we report corresponding experimental results.
In short, the contributions of this paper are:
•A generic latent-variable generative model called MixDVAE for the separation of mixed observations
into independent sources with non-linear dynamics.
•A learning and inference (variational EM) algorithm associated with MixDVAE, derived from the
corresponding variational lower bound.
•A set of experiments demonstrating the interest of MixDVAE for various tasks and using different
types of data.
•MixDVAE is data-frugal and weakly supervised since it does not require a massive labeled multi-
source dataset at training time, but only one or several single-source dataset(s) of much moderate
size.
1.3 Application to multi-object tracking
In multi-object tracking (MOT) (also called multi-target tracking (MTT) depending on the context, scientific
community and applications), a number of objects/targets appear simultaneously in the same visual scene
and each of them has its own moving trajectory, e.g., pedestrians/vehicles in videos or aircrafts in radar
scans. The problem is to estimate the position of each object/target at every time step, and track it along
time by assigning a unique identity to it (Vo et al., 2015; Ciaparrone et al., 2020). In the popular ‘tracking-
by-detection’ configuration, a set of detection bounding boxes (DBBs) are given at each time step by a
front-end detection algorithm, each of them potentially corresponding to one of the targets. These DBBs
are then used as the observations. In Section 5, we apply the MixDVAE model to the MOT problem in this
configuration (i.e., using the set of DBBs as observations). We do that in a simplified scenario where the
number of objects is assumed known and constant during the observation measurements. A complete and
fully-operational MOT system would require to include a module managing the ‘birth’ and ‘death’ of target
3Published in Transactions on Machine Learning Research (12/2023)
tracks (i.e., objects disappearing of the scene and new objects appearing in the scene). We do not address
this problem since the purpose of this work is not to propose a fully-operational MOT system but rather to
focus on the problem of multi-source dynamics modeling with DVAEs. It must be noted however that even
if we assume that the actual number of objects present in the scene is known and does not vary across the
modeled sequence, at any time step t, it is not necessarily equal to the number of DBBs, since occlusions
(leading to missed detections) may occur. We will see in our reported experiments that MixDVAE is able
to deal with these difficulties.
Related work in MOT/MTT. In the MOT/MTT literature, few works have considered the detection
assignment problem (which is also called the data association problem) and the target dynamics modeling
problemjointlyinaunifiedprobabilisticframework. Infact, thedataassociationproblemisusuallysolvedby
designing a complicated association algorithm (such as the joint probabilistic data association filter (JPDAF)
or the multiple hypothesis tracking (MHT) algorithm) and the target trajectories estimation is obtained by
applying a post-processing track filtering algorithm (such as the Kalman filter, the extended Kalman filter,
or a particle filter) separately on each estimated track (Vo et al., 2015). Recent MOT algorithms use DNNs
such as RNNs or convolutional neural networks (CNNs) to extract several features (e.g., visual features,
motion features, targets interaction features) from the videos and use these features for data association
(Luo et al., 2021; Ciaparrone et al., 2020). The motion models are generally used for track refinement. The
work that is the closest to ours is that of Ban et al. (2021), who proposed a unified probabilistic framework for
audio-visual multi-speaker tracking. However, the dynamical model that was used in their work is a simple
linear-Gaussian dynamical model. In this paper, we use DVAEs to model non-linear objects dynamics.
1.4 Application to audio source separation
The second problem on which we apply the proposed MixDVAE model is the single-channel audio source
separation(SC-ASS)task. Here, therecorded(observed)signalisaunique(digital)waveformthatisassumed
to result from the physical summation of individual source waveforms, such as several speakers speaking
simultaneously or several musical instruments playing together. The goal is to estimate the different source
signals composing the mixture (Vincent et al., 2011). At first sight, this scenario is not an appropriate
configuration for MixDVAE because, for each discretized time, we do not have a set of observed samples
to assign to one of the source signals. However, a widely-used approach in SC-ASS is to work in the time-
frequency (TF) domain, most often using the short-time Fourier transform (STFT), and exploit the sparsity
of audio signals in the TF domain (Yilmaz & Rickard, 2004). This means that at each TF bin of the STFT,
the observed mixture signal is assumed to be composed of one dominant source (with a power that is much
larger than that of the other sources), so that the observation at that TF bin can be (totally or mainly)
attributed to that dominant source. This can be done using an assignment variable, which is often referred
to as aTF mask in the SC-ASS literature (Wang & Chen, 2018). Therefore, we can apply the proposed
MixDVAE model by combining this assignment variable with a set of DVAEs modeling the dynamics of the
audio sources in the TF domain. The main difference compared to the MOT problem is that we have to
consider the frequency dimension in addition to time, and at a given TF bin, we have here only one single
observation to assign to a source, instead of a set of observations. We apply this principle and illustrate the
use of MixDVAE for SC-ASS in Section 6. It can be noted that the dynamics of different types of audio
source signals (speech, musical instruments, noises, etc.) can be very different. So, this is a typical use-case
where we can pre-train different DVAE models on different single-source datasets to capture the dynamics
of different types of source.
Related work in SC-ASS. State-of-the-artSC-ASSmethodsarebasedontheuseofhugeandsophisticated
DNNs that directly map the mixture signal to the individual source signals or to the TF masks (Chandna
et al., 2017; Wang & Chen, 2018). These methods obtain impressive separation performance, but they
require to adopt a fully-supervised approach using huge parallel datasets (that contain both the mixture
and the aligned separate source signals). This contrasts with the spirit of MixDVAE which, again, can be
considered as weakly supervised, and does not require a huge parallel dataset, but only a reasonable-size
single-source dataset for any type of source to separate. Weakly-supervised (or at the extreme unsupervised)
methods for audio source separation are still quite rare and their development is a largely open topic. We
can find some connection with the SC-ASS method of Ozerov et al. (2009) based on factorial HMMs. A
4Published in Transactions on Machine Learning Research (12/2023)
DVAE-based unsupervised speech enhancement method was recently presented by (Bie et al., 2022b), but a
unique DVAE instance was used to model the speech signal and the noise (power spectrogram) was modeled
with a nonnegative matrix factorization (NMF) model. To our knowledge, the present work is the first time
a mixture of DVAEs is used for SC-ASS (and for audio processing in general).
1.5 Organization of the paper
In Section 2, we present the general methodological background for developing the MixDVAE model, in-
cluding the variational inference principle and the DVAEs. In Section 3, we present the MixDVAE model
and the general principle we used to derive its solution. The solution itself, i.e. the MixDVAE algorithm,
is presented in Section 4. Section 5 and Section 6 illustrate the application of MixDVAE to the MOT and
SC-ASS problems, respectively, including experiments. Section 7 concludes the paper.
2 Methodological background
2.1 Latent-variable generative models and variational inference
AnLVGMdepictstherelationshipbetweenanobserved oandalatent hrandom(vector)variablefromwhich
ois assumed to be generated. We consider an LVGM defined via a parametric joint probability distribution
pθ(o,h), whereθdenotes the set of parameters. In a general manner, we are interested in two problems
closely related to each other: (i) estimate the parameters θthat maximize the observed data (marginal)
likelihoodpθ(o), and (ii) derive the posterior distribution pθ(h|o)so as to infer the latent variable hfrom
the observation o.
A prominent tool to estimate θis the family of expectation-maximisation (EM) algorithms (Bishop, 2006;
McLachlan & Krishnan, 2007), that maximizes the following lower bound of the marginal likelihood, called
the evidence lower-bound (ELBO):
L(θ,q;o) =Eq(h|o)/bracketleftbig
logpθ(o,h)−logq(h|o)/bracketrightbig
≤logpθ(o), (1)
whereq(h|o)is a distribution on hconditioned on o. The EM algorithm is an iterative alternate optimisation
procedure that maximizes the ELBO w.r.t. the distribution q(E-step) and the parameters θ(M-step).
Maximizing the ELBO w.r.t. qis equivalent to minimising the Kullback-Leibler divergence (KLD) between
q(h|o)and the exact posterior distribution pθ(h|o)(Bishop, 2006; McLachlan & Krishnan, 2007). When
pθ(h|o)is computationally tractable, it is optimal to choose q(h|o) =pθ(h|o), then the ELBO is tight
and the EM is called “exact.” Otherwise, the optimization w.r.t. qis constrained within a given family of
computationally tractable distributions and the bound is not tight anymore. We then have to step in the
variational inference (VI) framework (Jordan et al., 1999; Wainwright et al., 2008).
A first family of VI approaches is the structured mean-field method (Parisi & Shankar, 1988) which consists
in splitting hinto a set of disjoint variables h= (h1,...,hM). The approximate posterior distribution qis
thus assumed to factorize over this set, i.e. q(h|o) =/producttextM
i=1qi(hi|o), which leads to the following optimal
factor, given the parameters computed at the previous M-step, θold:
q∗
i(hi|o)∝exp/parenleftig
E/producttext
j̸=iqj(hj|o)/bracketleftbig
logpθold(o,h)/bracketrightbig/parenrightig
. (2)
Since each factor is expressed as a function of the others, this formula is iteratively applied in the E-step of
the EM algorithm until some convergence criterion is met, leading to the VEM family of algorithms (Bishop,
2006).
A second approach is to rely on amortized inference (Hoffman et al., 2013), where a set of shared parameters
is used to compute the parameters of the approximate posterior distribution. A very well-known example
is the variational autoencoder (VAE) (Kingma & Welling, 2014; Rezende et al., 2014). For a reason that
will become clear in Section 3.1, let us here denote by sthe observed variable and by zthe latent one.
In a VAE, the joint distribution pθ(s,z) =pθ(s|z)p(z)is defined via the prior distribution on z, generally
chosen as the standard Gaussian distribution p(z) =N(z;0,I), and via the conditional distribution on s,
5Published in Transactions on Machine Learning Research (12/2023)
generally chosen as a Gaussian with diagonal covariance matrix pθ(s|z) =N/parenleftbig
s;µθ(z),diag(vθ(z))/parenrightbig
. The
mean and variance vectors µθ(z)andvθ(z)are nonlinear functions of zprovided by a DNN, called the
decoder network , taking zas input.θis here the (amortized) set of parameters of the DNN. The posterior
distribution pθ(z|s)correspondingtothismodeldoesnothaveananalyticalexpressionanditisapproximated
by a Gaussian distribution with diagonal covariance matrix qϕ(z|s) =N/parenleftbig
z;µϕ(s),diag(vϕ(s))/parenrightbig
, where the
mean and variance vectors µϕ(s)andvϕ(s)are non-linear functions of simplemented by another DNN called
theencoder network and parameterized by ϕ. The ELBO is here given by:
L(θ,ϕ;s) =Eqϕ(z|s)/bracketleftbig
logpθ(s,z)−logqϕ(z|s)/bracketrightbig
. (3)
In practice, the ELBO is jointly optimized w.r.t. θandϕon a training dataset using a combination of
stochastic gradient descent (SGD) and sampling (Kingma & Welling, 2014). This is in contrast with the EM
algorithm where qandθare optimized alternatively.
2.2 From VAE to DVAE
IntheVAE,eachobserveddatavector sisconsideredindependentlyoftheotherdatavectors. Thedynamical
variational autoencoders (DVAEs) are a class of models that extend and generalize the VAE to model
sequences of data vectors correlated in time (Girin et al., 2021). Roughly speaking, DVAE models combine
a VAE with temporal models such as RNNs and/or SSMs.
Lets1:T={st}T
t=1andz1:T={zt}T
t=1beadiscrete-timesequenceofobservedandlatentvectors, respectively,
and let pt={s1:t−1,z1:t−1}denote the set of past observed and latent vectors at time t. Using the chain rule,
the most general DVAE generative distribution can be written as the following causal generative process:
pθ(s1:T,z1:T) =T/productdisplay
t=1pθs(st|pt,zt)pθz(zt|pt), (4)
wherepθs(st|pt,zt)andpθz(zt|pt)are arbitrary generative distributions, which parameters are provided
sequentially by RNNs taking the respective conditioning variables as inputs. A common choice is to use
Gaussian distributions with diagonal covariance matrices:
pθs(st|pt,zt) =N/parenleftbig
st;µθs(pt,zt),diag(vθs(pt,zt))/parenrightbig
, (5)
pθz(zt|pt) =N/parenleftbig
zt;µθz(pt),diag(vθz(pt))/parenrightbig
. (6)
It can be noted that the distribution of ztis more complex than the standard Gaussian used in the vanilla
VAE. Also, the different models belonging to the DVAE class differ in the possible conditional independence
assumptions that can be made in (4).
Similarly to the VAE, the exact posterior distribution pθ(z1:T|s1:T)corresponding to the DVAE generative
model is not analytically tractable. Again, an inference model qϕz(z1:T|s1:T)is defined to approximate the
exact posterior distribution. This inference model factorises as:
qϕz(z1:T|s1:T) =T/productdisplay
t=1qϕz(zt|qt), (7)
where qt={z1:t−1,s1:T}denotes the set of past latent variables and all observations. Again, the Gaussian
distribution with diagonal covariance matrix is generally used:
qϕz(zt|qt) =N/parenleftbig
zt;µϕz(qt),diag(vϕz(qt))/parenrightbig
, (8)
where the mean and variance vectors are provided by an RNN (the encoder network) taking qtas input and
parameterized by ϕ. With the most general generative model defined in (4), the conditional distribution in
(7)cannotbesimplified. However, ifconditionalindependenceassumptionsaremadein(4), thedependencies
inqϕz(zt|qt)can be simplified using the D-separation method (Bishop, 2006; Geiger et al., 1990), see (Girin
et al., 2021) for details. In addition, we can force the inference model to be causal by replacing s1:Twith
6Published in Transactions on Machine Learning Research (12/2023)
Table 1: Summary of the variable notations.
Variable notation Definition
T,t∈{1,...,T} Sequence length and frame index
N,n∈{1,...,N} Total number of sources and source index
Kt,k∈{1,...,Kt}Number of observations at t, and obs. index
stn∈RSTrue position/content of source nat timet
ztn∈RLLatent variable of source nat timet
otk∈ROObservation kat timet
wtk∈{1,...,N} Assignment variable of observation kat timet
s:,n=s1:T,n Source vector sequence for source n
st,:=st,1:N Set of all source vectors at time t
s=s1:T,1:N Set of all source vectors
z:,n,zt,:,z Analogous for the latent variable
o=o1:T,1:Kt Set of all observations
w=w1:T,1:Kt Set of all assignment variables
s1:tinqt. This is particularly suitable for on-line processing. In the rest of the paper, we will use the causal
inference model, i.e. qt={z1:t−1,s1:t}in (7) and (8).
Similar to the VAE, and following the general VI principle, the DVAE model is also trained by maximizing
the ELBO with a combination of SGD and sampling, the sampling being here recursive. The ELBO has
here the following general form (Girin et al., 2021):
L(θs,θz,ϕz;s1:T) =Eqϕz(z1:T|s1:T)/bracketleftbig
logpθ(s1:T,z1:T)−logqϕz(z1:T|s1:T)/bracketrightbig
. (9)
3 MixDVAE model
3.1 Problem formulation and notations
Let us consider a sequence containing Nsources or targets that we observe over time. Let n∈{1,...,N}
denote the source index and let stn∈RSbe here the true (unknown) n-thsource vector at time frame t. At
every time frame t, we gather Ktobservations, and this number can vary over time. We denote by otk∈RO,
k∈{1,...,Kt}, thek-th observation at frame t. The problem tackled in this paper consists in estimating the
sequence of hidden source vectors s1:T,n={stn}T
t=1, for each source n, from the complete set of observations
o1:T,1:Kt={otk}T,Kt
t=1,k=1.
To solve this problem, we define two additional sets of latent variables. First, for each source nat time
framet, we define a latent variable ztn∈RLassociated with stnthrough a DVAE model. This DVAE
model, which might be identical for all sources or not, is used to model the dynamics of each individual
source and is plugged into the proposed probabilistic MixDVAE model. Second, for each observation otk, we
define a discrete observation-to-source assignment variable wtktaking its value in {1,...,N}.wtk=nmeans
that observation kat time frame tis assigned to/was generated by source n. This results in per-source
sequences of assigned observations.
Hereinafter, to simplify the notations, we use “:” as a shortcut subscript for the set of all values of the
corresponding index. For example, s:,n=s1:T,nis the complete trajectory of source nandst,:=st,1:Nis the
set of all source vectors at time frame t. All notations are summarized in Table 1.
3.2 General principle of the proposed model and solution
The general methodology of MixDVAE is to define a parametric joint distribution of all variables
pθ(o,s,z,w), then estimate its parameters θand (an approximation of) the corresponding posterior distribu-
tionpθ(s,z,w|o), from which we can deduce an estimate of s1:T,nfor each source n. The proposed MixDVAE
7Published in Transactions on Machine Learning Research (12/2023)
Figure 1: Graphical representation of the proposed MixDVAE model.
generative model pθ(o,s,z,w)is presented in Section 3.3. As briefly stated above, it integrates the DVAE
generative model (4)–(6) for modeling the sources dynamics and does not use any human-annotated data for
training. As is usually the case in (D)VAE-based generative models, both the exact posterior distribution
pθ(s,z,w|o)and the marginalization of the joint distribution pθ(o,s,z,w)w.r.t. the latent variables are
analytically intractable. Therefore we cannot directly use an exact EM algorithm and we resort to VI. We
propose the following strategy, inspired by the structured mean-field method that we summarized in Sec-
tion 2.1, with hbeing here equal to {s,z,w}. In Section 3.4, we define an approximate posterior distribution
qϕ(s,z,w|o)that partially factorizes over {s,z,w}. Just like the proposed MixDVAE generative model
includes the DVAE generative model, the approximate posterior distribution includes the DVAE inference
model as one of the factors. This factorization makes possible the derivation of a model solution in the form
of a VEM algorithm, as detailed in Section 4.
3.3 Generative model
Let us now specify the joint distribution of observed and latent variables pθ(o,w,s,z). We assume that the
observation variable oonly depends on wands, while the assignment variable wis a priori independent
of the other variables. The graphical representation of MixDVAE is shown in Figure 1. Applying the chain
rule and these conditional dependency assumptions, the joint distribution can be factorised as follows:
pθ(o,w,s,z) =pθo(o|w,s)pθw(w)pθsz(s,z). (10)
Observation model. We assume that the observations are conditionally independent through time and
independent of each other, that is to say, at any time frame t, the observation otkonly depends on its
corresponding assignment wtkand source vector at the same time frame. The observation model pθo(o|w,s)
can thus be factorised as:2
pθo(o|w,s) =T/productdisplay
t=1Kt/productdisplay
k=1pθo(otk|wtk,st,:). (11)
Given the value of the assignment variable, the distribution p(otk|wtk,st,:)is modeled by a Gaussian distri-
bution:
pθo(otk|wtk=n,stn) =N(otk;stn,Φtk). (12)
This equation models only the observation noise via the covariance Φtk∈RO×Oand thus assumes that the
assigned observation lies close to the true source vector.3
2In this equation, we use st,:and not stn, since the value of wtkis not specified.
3For simplicity of presentation, we state the case in which the observation and source vector dimensions are the same,
i.e.O=S. In a more general case where O̸=S, we can consider the use of a projection matrix Pk∈RO×Sand define
pθo(otk|wtk=n,stn) =N(otk;Pkstn,Φtk). Again, for simplicity, we consider Pk=Iin the rest of the paper. All derivations
and results are generalizable to Pk̸=I.
8Published in Transactions on Machine Learning Research (12/2023)
Assignment model. Similarly, we assume that, a priori, the assignment variables are independent across
time and observations:
pθw(w) =T/productdisplay
t=1Kt/productdisplay
k=1pθw(wtk). (13)
For each time frame tand each observation k, the assignment variable wtkis assumed to follow a uniform
prior distribution:
pθw(wtk) =1
N. (14)
Dynamical model. Finally,pθsz(s,z)is modeled with a DVAE. The different sources are assumed to be
independent of each other. This implies that in the present work we do not consider possible interactions
among sources. More complex dynamical models including source interaction are beyond the scope of this
paper. With this assumption, the joint distribution of all source vectors and corresponding latent variable
pθsz(s,z)can be factorized across sources as:
pθsz(s,z) =N/productdisplay
n=1pθsz(s:,n,z:,n), (15)
wherepθsz(s:,n,z:,n)is the DVAE model defined in (4)–(6) and applied to s:,nandz:,n(defining pt,n=
{s1:t−1,n,z1:t−1,n}).4As mentioned before, the DVAE model can be either the same architecture for all
sources, pre-trained on a unique single-source dataset, or the same architecture but pre-trained on different
single-source datasets for different sources, or completely different architectures for each source.
Overall, the parameters in the generative model to be estimated are θ={θo={Φtk}T,Kt
t,k=1,θs,θz}(note that
θw=∅).
3.4 Inference model
The exact posterior distribution corresponding to the MixDVAE generative model described in Section 3.3
is neither analytically nor computationally tractable. Therefore, we propose the following factorized approx-
imation that leads to a computationally tractable inference model:
qϕ(s,z,w|o) =qϕw(w|o)qϕz(z|s)qϕs(s|o), (16)
whereqϕz(z|s)corresponds to the inference model of the DVAE and the optimal distributions qϕs(s|o)and
qϕw(w|o)are derived below in the E-steps of the MixDVAE algorithm. The factorization (16) is inspired
by the structured mean-field method (Parisi & Shankar, 1988), since we break the posterior dependency
between wand{s,z}. However, we keep the dependency between sandzat inference time since it is the
essence of the DVAE. In addition, we assume that the posterior distribution of the DVAE latent variable is
independent for each source, so that we have:
qϕz(z|s) =N/productdisplay
n=1qϕz(z:,n|s:,n), (17)
whereqϕz(z:,n|s:,n)is given by (7) and (8) applied to {z:,n,s:,n}. This is coherent with the generative model,
where we assumed that the dynamics of the various sources are independent of each other.
4 MixDVAE solution: A variational expectation-maximization algorithm
Let us now present the proposed algorithm for jointly deriving the terms of the inference model (other
than the DVAE terms) and estimating the parameters of the complete MixDVAE model, based on the
maximization of the corresponding ELBO. The inference is done directly on each multi-source test sequence
4Here we denote the DVAE parameters by θszinstead ofθ, to differentiate the DVAE parameters from the other parameters.
9Published in Transactions on Machine Learning Research (12/2023)
to process and does not require previous supervised training with a labeled multi-source dataset. It only
requires to pre-train the DVAE model on synthetic or natural single-source sequences.
As discussed in Section 2, in many generative models, the optimization of the ELBO is done either following
the structured mean-field method (2) or using amortized inference as in (D)VAEs. In our case, we cannot
directly use the generic structured mean-field inference procedure, since the proposed approximation (16)
does not factorize completely in a set of disjoint latent variables (e.g., qϕz(z|s)is conditioned on s). Al-
ternatively, one could resort to purely amortized inference and conceive a deep encoder that approximates
the distributions in (16), leading to a looser approximation bound. We propose a strategy that is a middle
ground between these two worlds. We use the structured mean-field principles that provide a tighter bound
since they do not impose a distribution family for qϕwandqϕs, and we use the philosophy of amortized
inference for qϕzso as to exploit the pre-trained DVAE encoder.
To do so, we have to go back to the fundamentals of VI and iteratively maximize the MixDVAE model
ELBO defined by:
L(θ,ϕ;o) =Eqϕ(s,z,w|o)[logpθ(o,s,z,w)−logqϕ(s,z,w|o)]. (18)
By injecting (10) and (16) into (18), we can develop L(θ,ϕ;o)as follows:
L(θ,ϕ;o) =Eqϕw(w|o)qϕs(s|o)/bracketleftbig
logpθo(o|w,s)/bracketrightbig
+Eqϕw(w|o)/bracketleftbig
logpθw(w)−logqϕw(w|o)/bracketrightbig
+Eqϕs(s|o)/bracketleftig
Eqϕz(z|s)/bracketleftbig
logpθsz(s,z)−logqϕz(z|s)/bracketrightbig/bracketrightig
−Eqϕs(s|o)/bracketleftbig
logqϕs(s|o)/bracketrightbig
. (19)
The ELBO maximization is done by alternatively and iteratively maximizing the different terms correspond-
ing to the various posterior and generative distributions. In our case, we obtain a series of variational E and
M steps. While the E steps associated to qϕwandqϕsfollow the structured mean-field principle, the E step
associated to qϕzis based on the principle of amortized inference commonly used in (D)VAEs.
4.1 E-S step
We first consider the computation of the optimal posterior distribution qϕs(s|o). To this aim, we first select
the terms in (19) that depend on s, the other terms being here considered as a constant:
Ls(θ,ϕ;o) =Eqϕs(s|o)/bracketleftig
Eqϕw(w|o)/bracketleftbig
logpθo(o|w,s)/bracketrightbig
+Eqϕz(z|s)/bracketleftbig
logpθsz(s,z)−logqϕz(z|s)/bracketrightbig
−logqϕs(s|o)/bracketrightig
.
(20)
Let us define:
˜p(s|o) =C′exp/parenleftig
Eqϕw(w|o)/bracketleftbig
logpθo(o|w,s)/bracketrightbig
+Eqϕz(z|s)/bracketleftbig
logpθsz(s,z)−logqϕz(z|s)/bracketrightbig/parenrightig
,(21)
whereC′>0is the appropriate normalisation constant. (20) rewrites:
Ls(θ,ϕ;o) =−Dkl/parenleftbig
qϕs(s|o)∥˜p(s|o)/parenrightbig
+C, (22)
whereDkl(·|·)denotes the Kullback-Leibler divergence (KLD). Therefore, the optimal distribution is the
one minimising the above KLD:
qϕs(s|o) = ˜p(s|o)∝exp/parenleftig
Eqϕw(w|o)/bracketleftbig
logpθo(o|w,s)/bracketrightbig
+Eqϕz(z|s)/bracketleftbig
logpθsz(s,z)−logqϕz(z|s)/bracketrightbig/parenrightig
.(23)
Since for any pair (t,k), the assignment variable wtkfollows a discrete posterior distribution, we can denote
the corresponding probability values by ηtkn=qϕw(wtk=n|otk). These values will be computed in the
E-W step below. The expectation with respect to qϕw(w|o)in (23) can be calculated using these values.
However, the expectation with respect to qϕz(z|s)cannot be calculated in closed form. As usually done in
the (D)VAE methodology, it is thus replaced by a Monte Carlo estimate using sampled sequences drawn
from the DVAE inference model at the previous iteration (see Section 4.5). Replacing the distributions in
10Published in Transactions on Machine Learning Research (12/2023)
(23) with (11), (15), and (17), and calculating the expectations with respect to qϕw(w|o)andqϕz(z|s), we
find thatqϕs(s|o)factorizes with respect to nas follows:
qϕs(s|o) =N/productdisplay
n=1qϕs(s:,n|o). (24)
Each of these factors corresponds to the posterior distribution of the n-th source vector. Given (23) and the
DVAE generative and inference models, we see that at a given time t, the distribution over stnhas non-linear
dependencies w.r.t. the previous and current DVAE latent variables z1:t,nand the previous source vectors
s1:t−1,n. These non-linear dependencies impede to obtain an efficient closed-form solution. We resort to
point sample estimates obtained using samples of z1:t,nand of s1:t−1,n, at the current iteration, denoted
z(i)
1:t,nands(i)
1:t−1,n. Using these samples, the posterior distribution is approximated with (details can be
found in Appendix A.1):
qϕs(s:,n|o)≈T/productdisplay
t=1qϕs(stn|s(i)
1:t−1,n,z(i)
1:t,n,o), (25)
where each term of the product is shown to be a Gaussian:
qϕs(stn|s(i)
1:t−1,n,z(i)
1:t,n,o) =N(stn;mtn,Vtn), (26)
with covariance matrix and mean vector given by:
Vtn=/parenleftigKt/summationtext
k=1ηtknΦ−1
tk+diag(v(i)
θs,tn)−1/parenrightig−1
, (27)
mtn=Vtn/parenleftigKt/summationtext
k=1ηtknΦ−1
tkotk+diag(v(i)
θs,tn)−1µ(i)
θs,tn/parenrightig
, (28)
wherev(i)
θs,tnandµ(i)
θs,tnare simplified notations for vθs(s(i)
1:t−1,n,z(i)
1:t,n)andµθs(s(i)
1:t−1,n,z(i)
1:t,n), respectively
denoting the variance and mean vector provided by the DVAE decoder network for source nat time frame
t. As we have to sample both s:,nandz:,n, we need to pay attention to the sampling order. This will be
discussed in detail in Section 4.5. Importantly, in practice, mtnis used as the estimate of stn.
Eq. (28) shows that the estimated n-th source vector is obtained by combining the observations otkand
the mean source vector µ(i)
θs,tnpredicted by the DVAE generative model. The balance between these two
terms depends on the assignment variable ηtkn, the observation model covariance matrix Φtkand the source
variance predicted by the DVAE generative model v(i)
θs,tn. Ideally, the model should be able to appropriately
balance these two terms so as to optimally exploit both the observations and the DVAE predictions.
4.2 E-Z step
In the E-Z step, we consider the DVAE inference model qϕz(z|s), defined by (17), (7) and (8). In (19), the
corresponding term is the third one, which we denote by Lz(θs,θz,ϕz;o)and which factorizes across sources
as follows (see Appendix A.2):
Lz(θs,θz,ϕz;o) =N/summationdisplay
n=1Lz,n(θs,θz,ϕz;o), (29)
with
Lz,n(θs,θz,ϕz;o) =Eqϕs(s:,n|o)/bracketleftig
Eqϕz(z:,n|s:,n)/bracketleftbig
logpθsz(s:,n,z:,n)/bracketrightbig
−Eqϕz(z:,n|s:,n)/bracketleftbig
logqϕz(z:,n|s:,n)/bracketrightbig/bracketrightig
.(30)
Inside the expectation Eqϕs(s:,n|o)[·], we recognize the DVAE ELBO defined in (9) and applied to source
n. This suggests the following strategy. Previously to and independently of the MixDVAE algorithm, we
11Published in Transactions on Machine Learning Research (12/2023)
pre-train the DVAE model on a dataset of synthetic or natural unlabeled single-source sequences (this is
detailed in Sections 5.1 and 6.1). This is done only once, and the resulting DVAE is then plugged into the
MixDVAE algorithm to process multi-source sequences. This provides the E-Z step with very good initial
values of the DVAE parameters θs,θzandϕz. As for the following of the E-Z step, the expectation over
qϕs(s:,n|o)in (30) is not analytically tractable. A Monte Carlo estimate is thus used instead, using samples
of both zands, similarly to what was done in the E-S step. Finally, SGD is used to maximize (the Monte
Carlo estimate of) Lz(θs,θz,ϕz;o), jointly updating θs,θzandϕz; that is, we fine-tune the DVAE model
within the MixDVAE algorithm, using the observations o. Note that in our experiments, we also consider the
case where we neutralize the fine-tuning, i.e. we remove the E-Z step and use the DVAE model as provided
by the pre-training phase.
4.3 E-W step
Thanks to the separation of wfrom the two other latent variables in (16), the posterior distribution qϕw(w|o)
can be calculated in closed form by directly applying the optimal structured mean-field update equation (2)
to our model. It can be shown that this is equivalent to maximizing (19) w.r.t. qϕw(w|o). We obtain (see
Appendix A for details):
qϕw(w|o)∝T/productdisplay
t=1Kt/productdisplay
k=1qϕw(wtk|o), (31)
with
qϕw(wtk=n|o) =ηtkn=βtkn/summationtextN
i=1βtki, (32)
where
βtkn=N(otk;mtn,Φtk) exp/parenleftig
−1
2Tr/parenleftbig
Φ−1
tkVtn/parenrightbig/parenrightig
. (33)
The parameters mtnandVtnin the above equation have been defined in (28) and (27), respectively.
4.4 M step
As discussed in Section 2, the maximization step generally consists in estimating the parameters θof the
generative model by maximizing the ELBO over θ. We recall that θ={θo={Φtk}T,Kt
t,k=1,θs,θz}. In this
work, the parameters of the DVAE decoder θsandθzare first estimated (offline) during the pre-training
of the DVAE and then fine-tuned in the E-Z step in an amortized way, all this jointly with the parameters
of the encoder ϕz. Therefore, in the M-step, we only need to estimate the observation model covariance
matricesθo={Φtk}T,Kt
t,k=1. In (19), only the first term depends on θo. Setting its derivative with respect to
Φtkto zero, we obtain (see Appendix A for details):
Φtk=N/summationdisplay
n=1ηtkn/parenleftig
(otk−mtn)(otk−mtn)T+Vtn/parenrightig
. (34)
In practice, it is difficult to obtain a reliable estimation using only a single observation. We address this
issue in Sections 5.2 and 6.2.
4.5 MixDVAE complete algorithm
As already mentioned in Section 4.1, we must pay attention to the sampling order of sandzwhen running
the iterations of the E-S and E-Z steps. As indicated in the pseudo-code of Algorithm 1, in practice, the
E-S and E-Z steps are processed jointly. We start with the initial source vectors sequence s(0)
1:T,1:Nand initial
mean source vectors sequence m(0)
1:T,1:N. At any iteration iof the E-Z and E-S steps, for each source nand
each time frame t, we sample in the following order:
12Published in Transactions on Machine Learning Research (12/2023)
Algorithm 1 MixDVAE algorithm
Input:
Observation vectors o=o1:T,1:Kt;
Output:
Parameters of qϕs(s) :{m(I)
tn,V(I)
tn}T,N
t,n=1(the estimated n-th source vector at time frame tismtn);
Values of the assignment variable {η(I)
tkn}T,N,Kt
t,n,k =1;
1:Initialization
2:See Sections 5.2 and 6.2
3:fori←1toIdo
4: E-W Step
5: forn←1toNdo
6: fort←1toTdo
7: fork←1toKtdo
8: Computeη(i)
tknusing (32) and (33);
9: end for
10: end for
11: end for
12: E-Z and E-S Step
13: forn←1toNdo
14: fort←1toTdo
15: Encoder ;
16: Computeµ(i)
ϕz,tn,v(i)
ϕz,tnwith input s(i−1)
1:T,nandz(i)
1:t−1,n;
17: Sample z(i)
tnfromqϕz(ztn|s(i−1)
1:T,n,z(i)
1:t−1,n) =N/parenleftbig
ztn;µ(i)
ϕz,tn,diag(v(i)
ϕz,tn)/parenrightbig
;
18: Decoder ;
19: Computeµ(i)
θz,tnandv(i)
θz,tnwith input s(i)
1:t−1,nandz(i)
1:t−1,n;
20: Computeµ(i)
θs,tnandv(i)
θs,tnwith input s(i)
1:t−1,nandz(i)
1:t,n;
21: E-S update ;
22: Compute m(i)
tn,V(i)
tnusing (28) and (27);
23: Sample s(i)
tnfromN(stn;m(i)
tn,V(i)
tn);
24: end for
25: E-Z update ;
26: Compute/hatwideLn(θs,θz,ϕz;o)using (35);
27: end for
28:Compute/hatwideL(θs,θz,ϕz;o) =/summationtextN
n=1/hatwideLn(θs,θz,ϕz;o);
29:Fine-tune the DVAE parameters { θs,θz,ϕz} by applying SGD on /hatwideL(θs,θz,ϕz;o);
30: M Step
31:Compute Φ(i)
tkusing (34) or following Sections 5.2 and 6.2;
32:end for
1. Compute the parameters µ(i)
ϕz,tnandv(i)
ϕz,tn5of the posterior distribution of ztusing the DVAE encoder
network with inputs s(i−1)
1:T,nsampled at the previous iteration and z(i)
1:t−1,nsampled at the current iteration.
Then, sample z(i)
tnfromqϕz(ztn|s(i−1)
1:T,n,z(i)
1:t−1,n).
2. Compute the parameters µ(i)
θz,tnandv(i)
θz,tn6of the generative distribution of ztusing the corresponding
DVAE decoder network with inputs s(i)
1:t−1,nandz(i)
1:t−1,n, both sampled at the current iteration.
3. Compute the parameters µ(i)
θs,tnandv(i)
θs,tnof the generative distribution of stusing the corresponding
DVAE decoder network with inputs s(i)
1:t−1,nandz(i)
1:t,n, both sampled at the current iteration. Compute
5µ(i)
ϕz,tnandv(i)
ϕz,tnare shortcuts for µϕz/parenleftbig
s(i−1)
1:T,n,z(i)
1:t−1,n/parenrightbig
andvϕz/parenleftbig
s(i−1)
1:T,n,z(i)
1:t−1,n/parenrightbig
respectively.
6Analogous definitions hold.
13Published in Transactions on Machine Learning Research (12/2023)
Observations  at frame  tot,1:KtSource vectors estimated  at iteration  i−1s(i−1)1:T,1:NDV AE predicted mean and  variance vectors  Assignment  variable η(i)tknEstimated source mean vector and  covariance matrix , m(i)t,1:NV(i)t,1:NE-Z StepUpdate ϕzMixDV AE algorithmUpdate  , θzθsE-S StepE-W StepUpdate  Φ(i)tkM Stepϕ(i)zθ(i)szℝO
T framesℝSDV AE modelOffline pre-trainingϕzθszT framesℝST framesℝSℝS  μθs(s(i−1)1:t−1,1:N,z1:t,1:N)νθs(s(i−1)1:t−1,1:N,z1:t,1:N)
Figure 2: Overview of the proposed MixDVAE algorithm at a given time frame t. The DVAE model
is pretrained offline using a (synthetic or natural) single-source dataset. It takes as input the sequence of
source vectors, encodes them into a sequence of latent vectors, which are then decoded into the reconstructed
sequence of source vectors. For a given time frame t, the MixDVAE algorithm takes as input the observations
at timetas well as the mean and variance vectors estimated by the DVAE model. By iterating the E-S,
E-Z, E-W and M steps, we obtain estimates of the assignment variable and of each source vector.
the parameters m(i)
tnandV(i)
tnof the posterior distribution of stwith (27) and (28), and sample s(i)
tnfrom
it.
Note that with the above sampling order, the Monte Carlo estimate of the ELBO term maximized in the
E-Z step (30) is given by (for source n):
/hatwideLz,n(θs,θz,ϕz;o) =T/summationdisplay
t=1logpθs(s(i)
tn|s(i)
1:t−1,n,z(i)
1:t,n)−T/summationdisplay
t=1Dkl/parenleftbig
qϕz(ztn|s(i−1)
1:T,n,z(i)
1:t−1,n)||pθz(ztn|s(i)
1:t−1,n,z(i)
1:t−1,n)/parenrightbig
.
(35)
The whole MixDVAE algorithm, taking into account these practical aspects, is summarized in the form of
pseudo-code in Algorithm 1.7In addition, Figure 2 shows a schematic overview of the algorithm.
4.6 Choice of the DVAE model
WerecallthattheDVAEisageneralclassofmodelsthatdifferbyadoptingdifferentconditionalindependence
assumptions for the generative distributions in the right-hand-side of (4). In Girin et al. (2021), seven DVAE
models from the literature have been extensively discussed, and six of them have been benchmarked on the
analysis-resynthesis task (on speech signals and 3D human motion data). We chose to use here the stochastic
recurrent neural network (SRNN) model initially proposed in Fraccaro et al. (2016), because it was shown
in Girin et al. (2021) to provide a very good trade-off between model complexity and modeling power. The
probabilistic dependencies of the SRNN generative model are defined as follows:
pθsz(s1:T,z1:T) =T/productdisplay
t=1pθs(st|s1:t−1,zt)pθz(zt|s1:t−1,zt−1). (36)
To perform online estimation, we use the following causal SRNN inference model:
qϕz(z1:T|s1:T) =T/productdisplay
t=1qϕz(zt|s1:t,zt−1). (37)
7As illustrated in Sections 5.2 and 6.2, in practice, we can choose different VEM step orders. Here we present the algorithm
with the order E-S/E-Z Step, E-W Step and M Step.
14Published in Transactions on Machine Learning Research (12/2023)
The implementation details of the SRNN model can be found in Appendix D.
5 Application of MixDVAE to multiple object tracking
As mentioned in Section 1.3, under the tracking-by-detection configuration, the objective of the MOT task
is to estimate the trajectories of moving objects from a set of given DBBs. In this case, the source vector stn
represents the position of object nat time frame t, which is given in practice by the coordinates of the (top-
left and bottom-right points of the) “true” corresponding bounding box, i.e. stn= (sl
tn,st
tn,sr
tn,sb
tn)∈R4.
The observation vector otk= (ol
tk,ot
tk,or
tk,ob
tk)∈R4contains the coordinates of the (top-left and bottom-
right points of the) k-th DBB at frame t. In a VAE or DVAE, the dimension Lof the latent vector ztnis
usually smaller than the dimension of the observed vector, in order to obtain a compact data representation.
Since in the MOT task the data dimension is already small ( O=S= 4), we also set L= 4. The sequence of
estimated source position vectors is given directly by (28), for n= 1toNandt= 1toT, directly forming
source trajectories, with no further post-processing.
5.1 DVAE pre-training
Dataset. We consider pedestrian tracking for the MOT task and assume that all the moving sources have
similar dynamical patterns. We thus pre-train a single DVAE model on a synthetic single-source trajectory
dataset. This dataset contains synthetic bounding box trajectories in the form of T-frame sequences ( T= 60)
of 4D vectors{(xl
t,xt
t,xr
t,xb
t)}T
t=1. These trajectories are generated using piece-wise combinations of several
elementary functions, namely: static a(t) =a0, constant velocity a(t) =a1t+a0, constant acceleration
a(t) =a2t2+a1t+a0, and sinusoidal (allowing for circular trajectories) a(t) =asin(ωt+ϕ0). The parameters
a1,a2,ω, andϕ0are sampled from some pre-defined distributions, whose parameters are estimated from
the detections on the training subset of the MOT17 dataset (Dendorfer et al., 2021), which is a widely-used
pedestrian tracking dataset (rapidly described it in the next subsection). The two remaining parameters,
a0anda, are set to the values that ensure continuous trajectories. More details about the single-source
synthetic trajectories generation can be found in Appendix E.1. Overall, we generated 12,105sequences for
the training dataset and 3,052sequences for the validation dataset.
Training details. The SRNN model used in our experiments is an auto-regressive model, i.e., it uses the
past source vectors s1:t−1to predict the current one st. In practice, the estimated past vectors are used for
this prediction, rather than the ground-truth past vectors. To make the model robust to this problem, we
trained the model in the scheduled sampling mode (Bengio et al., 2015). This means that during training,
we gradually replace the ground-truth past values with the previously generated ones to predict the current
value (see (Girin et al., 2021, Chapter 4) for a discussion on this issue). The model was trained using the
Adam optimizer (Kingma & Ba, 2014) with a learning rate set to 0.001and a batch size set to 256. An
early-stopping strategy was adopted, with a patience of 50epochs.
5.2 MixDVAE evaluation set-up
Dataset. For the evaluation of the proposed MixDVAE algorithm, we used the training set of MOT17.
MOT17 contains pedestrian scenes filmed in different places such as in a shopping mall or in a street, with
static or moving cameras. The motion patterns of the pedestrians in these videos are quite diverse and
challenging. The MOT17 training set contains seven sequences with length varying from twenty seconds to
one minute, with different frame rates (14, 25, and 30 fps). The ground-truth bounding boxes are provided,
as well as the detection results obtained with three customized detectors, namely DPM (Felzenszwalb et al.,
2010), Faster-RCNN (Ren et al., 2015), and SDP (Yang et al., 2016). As briefly stated in the introduction,
we focus our study on modeling the source dynamics for multiple-source tasks. Therefore, we leave aside
the problem of appearing/disappearing sources (usually referred to as birth/death processes) and consider a
fixed number of N= 3tracks. We have thus designed a new dataset from the MOT17 training set, which we
call the MOT17-3T dataset. The MOT17-3T dataset uses the publicly-released DBBs of the MOT17 dataset.
We split a complete video sequence into subsequences of sequence length T. Three values of Tare evaluated
in our experiments: 60, 120, and 300 frames (respectively corresponding to 2, 4, and 10 seconds at 30 fps).
15Published in Transactions on Machine Learning Research (12/2023)
Each test sequence contains three source trajectories with possible occlusions and detection absences, see an
example in Fig. 3. More details on the design of the MOT17-3T dataset can be found in Appendix E.2. We
have finally created 1,712,1,161, and 1,1373-source test sequences of length T= 60,120, and 300frames,
respectively. Notice that the pre-trained DVAE is not fine-tuned on these test sequences.
Algorithm initialization. Before starting the iterations of the proposed VEM algorithm, we need to
initialize the values of several parameters and variables. Theoretically, there is no preference in the order
of the three E-steps. In practice, however, for initialization convenience, we followed the order E-W Step,
E-Z/E-S Step. Indeed, starting with E-W Step requires the initialization of the mean vector and covariance
matrix of the source vector posterior distribution mtn,Vtn, the input vectors of the DVAE encoder s1:T,n
and the observation covariance matrices Φtk. For MOT, mtncan be easily initialised over a short sequence
by assuming that the source does not move too much. Indeed, the initial values of mtncan be set to the
value of the observed bounding box at the beginning of the sequence m0n. While this strategy is very
straightforward to implement, it is too simple for many tracking scenarios, especially for long sequences. We
thus propose to split a long sequence into sub-sequences. For each sub-sequence, we initialise mtnto the
value at the beginning of the sub-sequence. After this initialisation, we run a few iterations of the VEM
algorithm over the sub-sequence, allowing us to have an estimate of the source position at the end of the
sub-sequence. This value is then used to provide a constant initialisation for the next sub-sequence. At the
end, all these initializations are concatenated, providing a piece-wise constant initialization for mtnover the
entire long sequence. More implementation details, as well as the pseudo-code of this cascade initialization
strategy, are provided in Appendix C. The input vectors of the DVAE encoder are initialized with the same
values as the ones used for mtn.
Observation covariance matrix. In our experiments, we observed that the estimated values of both
Φtkandvθs,tnin (28) increased very quickly with the VEM algorithm iterations. This caused instability
and unbalance between these two terms, which finally conducted the whole model to diverge. To solve this
problem, we set Φtkto a given fixed value, which is constant on the whole analyzed T-frame sequence and
not updated during the VEM iterations. Specifically, for the MOT task, Φtkis set to a diagonal matrix,
and the diagonal entries are set to r2
Φ/bracketleftbig
(or
1k−ol
1k)2,(ot
1k−ob
1k)2,(or
1k−ol
1k)2,(ot
1k−ob
1k)2/bracketrightbig
, whererΦis a
factor lower than 1. In common terms, Φtkis set to a fraction of the (squared) size of the corresponding
observation at frame 1. The covariance matrices Vtnare initialized with the same values as Φtk.
Hyperparameters. The VEM algorithm of MixDVAE has four hyperparameters to be set. The observation
covariance matrix ratio rΦis set to 0.04, the initialization subsequence length Jis set to 30, and the
initialization iteration number I0is set to 20. The MixDVAE algorithm itself is run for I= 70iterations,
which was experimentally shown to lead to convergence.
Baselines. We compare our model with two recent state-of-the-art probabilistic MOT methods: The
Autoregressive Tracklet Inpainting and Scoring for Tracking (ArTIST) model of Saleh et al. (2021) and
the Variational Kalman Filter (VKF) of Ban et al. (2021). In addition to that, in order to demonstrate
the advantage of using a DVAE model for modeling the dynamics of single-source trajectories, we consider
replacing the DVAE model with a simpler deep auto-regressive (Deep AR) model. ArTIST is a supervised
stochastic autoregressive model that learns the discretized multi-modal distribution of human motion using
annotated MOT sequences. It can assign detections to tracks by scoring tracklet8proposals with their
likelihood. And it can also generate continuations of the source trajectories and inpaint those containing
missing detections. We have reused the trained models as well as the tracklet scoring and inpainting code
provided by the authors9and reimplemented the object tracking part according to the paper, as this part was
not provided. Implementation details can be found in Appendix F. Alike the proposed MixDVAE algorithm,
the VKF algorithm for MOT (Ban et al., 2021) is also based on the VI methodology to combine source
position estimation and detection-to-source assignment. However, a basic one-step linear dynamical model
is used in VKF instead of the DVAE model in the proposed MixDVAE algorithm. In short, the dynamical
model we use in VKF is pθs(st|st−1) =/producttextN
n=1N(stn;Dst−1,n,Λtn), where Dis assumed to be the identity
matrix and Λtnis estimated in the M step. Hence, the VKF MOT algorithm is a combination of VI and
Kalmanfilterupdateequations. In(Banetal.,2021), themethodwasproposedinanaudiovisualset-up. The
8A tracklet indicates a sequence of estimated position vectors consistent over time and assigned to the same object.
9available at https://github.com/fatemeh-slh/ArTIST
16Published in Transactions on Machine Learning Research (12/2023)
Table 2: MOT results for short ( T= 60), medium ( T= 120), and long ( T= 300) sequences.
Dataset Method MOTA ↑MOTP↑IDF1↑#IDS↓%IDS↓MT↑ML↓#FP↓%FP↓#FN↓%FN↓
ShortArTIST 63.7 84.1 48.7 86371 28.0 4684 0 9962 3.2 15525 5.0
VKF 56.0 82.7 77.3 5660 1.8 3742 761 64945 21.1 64945 21.1
Deep AR 67.4 76.1 83.1 5248 1.7 3670 129 49595 16.0 49595 16.0
MixDVAE 79.1 81.3 88.4 4966 1.6 4370 50 29808 9.7 29808 9.7
MediumArTIST 61.0 84.2 43.9 102978 24.6 2943 0 25388 6.1 34812 8.3
VKF 57.5 83.3 77.6 7657 1.8 2563 487 85053 20.3 85053 20.3
Deep AR 65.3 76.0 81.8 5387 1.3 2435 149 71775 17.0 71775 17.0
MixDVAE 78.6 82.2 88.0 6107 1.5 2907 120 41747 9.9 41747 9.9
LongArTIST 53.5 84.5 40.7 205263 20.1 2513 4135401 13.2 135401 13.2
VKF 74.4 86.2 84.4 30069 2.9 2756 100 116160 11.4 116160 11.4
Deep AR 75.5 76.6 87.1 26506 2.6 2555 18 123262 12.1 123262 12.1
MixDVAE 83.2 82.4 90.0 23081 2.3 2890 12 74550 7.3 74550 7.3
observations contain not only the DBB coordinates, but also appearance features and multichannel audio
recordings. For a fair comparison with MixDVAE, we use here the same observations, i.e., we simplified
VKF by using only the DBB coordinates. For both ArTIST and VKF, the tracked sequences are initialized
using the DBBs at the first frame, as what we have done for MixDVAE. For VKF, similarly to MixDVAE,
we need to provide initial values for mtnandVtn. For a fair comparison, we applied the same cascade
initialization as the one presented above, except that a linear dynamical model is used in place of the
DVAE to ensure the transition between two consecutive subsequences. The covariance matrices Vtnare
initialized with pre-defined values that stabilize the EM algorithm. The covariance matrices Φtkare fixed
to the same values as for MixDVAE. The covariance matrices of the linear dynamical model (denoted Λtn
in (Ban et al., 2021)) are initialized with the same values as Vtn. Finally the simpler Deep AR baseline
model is a deep generative model without stochastic latent variables. In this baseline, the dynamical model
becomespθs(st|st−1) =/producttextN
n=1N(st,n;µθs(s1:t−1,n),diag(vθs(s1:t−1,n))). In practice, the Deep AR model is
implemented with an LSTM layer. The hidden dimension of the LSTM layer is set to match that of the
LSTM layers employed in the DVAE model, i.e. it is equal to 8.
Evaluation metrics. We use the standard MOT metrics (Bernardin & Stiefelhagen, 2008; Ristani et al.,
2016) to evaluate the tracking performance of MixDVAE and compare it to the baselines, namely: multi-
object tracking accuracy (MOTA), multi-object tracking precision (MOTP), identity F1 score (IDF1), num-
berofidentityswitches(IDS),mostlytracked(MT),mostlylost(ML),falsepositives(FP)andfalsenegatives
(FN). The three test subsets contain a different number of test sequences, with a different sequence length T.
Therefore, for IDS, FP and FN, we report both the number of occurrences and the corresponding percentage.
Among them, MOTA is considered to be the most representative metric. It is defined by aggregating the
frame-wise versions of the metrics FP t, FNt, and IDtover frames:
MOTA = 1−/summationtext
t(FNt+FPt+IDSt)/summationtext
tGTt, (38)
where GT tdenotes the number of ground-truth tracks at frame t. Higher MOTA values imply less errors (in
terms of FPs, FNs, and IDS), and hence better tracking performance. MOTP defines the averaged overlap
between all correctly matched sources and their corresponding ground truth. Higher MOTP implies more
accurate position estimations. IDF1 is the ratio of correctly identified detections over the average number
of ground-truth and computed detections. IDS reflects the capability of the model to preserve the identity
of the tracked sources, especially in case of occlusion and track fragmentation. MT and ML represent how
much the trajectory is recovered by the tracking algorithm. A source track is mostly tracked (resp. mostly
lost) if it is covered by the tracker for at least 80%(resp. not more than 20%) of its life span.
5.3 Experimental results
Quantitative analysis. We now present and discuss the tracking results obtained with the proposed
MixDVAE algorithm and compare them with those obtained with the baselines. In these experiments, the
17Published in Transactions on Machine Learning Research (12/2023)
value of the observation variance ratio rΦis set to 0.04 and no fine-tuning is applied to SRNN in the E-Z
step. Ablation study on these factors is presented in Appendix I.
The values of the MOT metrics obtained on short, medium and long sequence subsets ( T= 60,120, and 300
frames, respectively) are shown in Table 2. We see that the proposed MixDVAE algorithm obtains the best
MOTA scores for the three subsets (i.e., for the three different sequence length values). This is remarkable
given that ArTIST was trained on the MOT17 training dataset, whereas MixDVAE never saw the ground-
truth sequences before the test. Furthermore, we notice that both VKF and MixDVAE have much less IDS
and much higher IDF1 scores than ArTIST, which implies that the observation-to-source assignment based
on the VI method is more efficient than direct estimation of the position likelihood distribution to preserve
the correct source identity during tracking. Besides, the MixDVAE model also has better scores than the
VKF model for these two metrics, which implies that the DVAE-based dynamical model performs better on
identity preservation than the linear dynamical model of VKF. For the 60- and 120-frame sequences, the
ArTIST model has lower FP and FN percentages and higher MOTP scores (though the MOTP scores of
all three algorithms are quite close for every value of T). This is reasonable because, again, ArTIST was
trained on the same dataset using the ground-truth sequences while our model is unsupervised. Overall, the
adverse effect caused by frequent identity switches is much greater than the positive effect of lower FP and
FN for the ArTIST model. That explains why MixDVAE has much better MOTA scores than ArTIST. For
the long (300-frame) sequences, MixDVAE obtains an overall much better performance than the ArTIST
model, since it obtains here the best scores for 6 metrics out of 8, including FP and FN. This shows that
MixDVAE is particularly good at tracking objects on the long term (we recall that T= 300represents 10 s
of video at 30 fps).
Besides, MixDVAE also globally exhibits notably better performance than VKF on all of the three datasets.
This clearly indicates that the modeling of the sources dynamics with a DVAE model outperforms the use
of a simple linear-Gaussian dynamical model and can greatly improve the tracking performance. We can
also notice that the VKF algorithm globally performs much better on 300-frame sequences than on 60- and
120-frame sequences. One possible explanation for this phenomenon is that the dynamical patterns of long
sequences are simpler than those of short and medium sequences. In fact, the data statistics show that the
average velocity in long sequences is much lower than that in short and medium sequences. In this case, the
linear dynamical model can perform quite well –although not as well as the DVAE.
Finally, we can see in Table 2 that MixDVAE with SRNN as dynamical model has an overall significantly
better performance than MixDVAE with the baseline Deep AR dynamical model. This demonstrates the
important role of the latent variables in SRNN for the dynamical modeling of sequential data. We remind
that the latent vector z:,nis assumed to efficiently encode the generative factors of source n’s trajectory.
Qualitative analysis. To illustrate the behavior of MixDVAE and the baseline models, we present an
example of tracking result in Fig. 3. More examples can be found in Appendix G. In the example of Fig. 3,
the detection for Source 3 ( o3in the figure) is absent from t= 2and reappears after t= 20. But we limit
the plot to t= 10for a better visualization. This is a case of long-term detection absence. An immediate
identity switch occurs at t= 2for the ArTIST model. Then, the track obtained by ArTIST is no longer
stable. We speculate the reason for the frequent identity switches made by ArTIST is that the estimated
distributions do not correspond well to the true sequential position distributions, which is possibly due to
the way these distributions are discretized. In addition to the identity switches, the estimations generated
by ArTIST at t= 5,8, and 10are not accurate. This causes a decrease of the tracking performance. For the
VKF model, the estimated bounding boxes for Sources 2 and 3 ( m2andm3in the figure) overlap each other.
This means that the two observations are both assigned to the same source, which is Source 2. From (32)
and (33), we know that the value of the assignment variable depends on the posterior mean and variance
vectors mtnandVtn, which themselves depend on the dynamical model. With a linear dynamical model,
VKF is not able to correctly predict distinct m2andm3trajectories. The Deep AR model succeed to predict
distinctm2andm3trajectories. However, the trajectory m3is not accurate due to the absence of o3. In
contrast, the very good dynamical modeling capacity of the DVAE makes MixDVAE able to keep tracking
despite of the long-term detection absence and generate reasonable m3estimations, which correspond well
to the ground-truth trajectory of Source 3 ( s3in the figure).
18Published in Transactions on Machine Learning Research (12/2023)
t = 1t = 2t = 3t = 4t = 5t = 6t = 7t = 8t = 9t = 10Ground TruthDetectionArTISTVKFDeep ARMixDV AE
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
Figure 3: Example of tracking result obtained with the proposed MixDVAE algorithm and the two baselines.
For clarity of presentation, the simplified notations s1,o1, andm1denote the ground-truth source position,
the observation, and the estimated source position, respectively (for Source 1, and the same for the two other
sources). Best seen in color.
6 Application of MixDVAE to single-channel audio source separation
When applying MixDVAE to the SC-ASS task, we work in the short-time Fourier transform (STFT) domain.
Thisimpliesthatboththesourceandobservationvectorsarecomplex-valued. Moreprecisely, the n-thsource
vector stn={stn,f}F
f=1∈CFis the short-time spectrum of audio source nat time frame t(fdenotes the
frequency bin and s∈CT×Fis the complete STFT spectrogram). The number of frequency bins, F, is
typically set to 256,512or1024(a power of 2 is preferred to use the fast Fourier transform). As is usually
adopted in audio processing, stnis assumed to follow a zero-mean circularly-symmetric complex Gaussian
priordistribution(Févotteetal.,2009;Liutkusetal.,2011;Girinetal.,2019), i.e.(5)becomes pθs(st|pt,zt) =
Nc/parenleftbig
st;0,diag(vθs(pt,zt))/parenrightbig
. The latent space dimension Lis typically set to a value significantly lower than
F. Also, here, o={otf}TF
t,f=1∈CF×Tdenotes the STFT spectrogram of the observed mixture signal. We
define thek-th observation variable at frame tasotk=otf∈C, which is the STFT coefficient of the mixture
signal at TF bin (t,f). In other words, the observation index kis identified with the frequency bin/index
f, and the total number of observations at any frame tequals the number of frequency bins, i.e. Kt=F
for eacht. We note that in this case, stnandotkdo not have the same dimension, even though stnandot,:
do. Therefore, as mentioned in Footnote 3, we need to define a projection matrix Pk∈C1×F, which is here
the transposed one-hot vector activated at the k-th index. Finally, the observation otk=otfis modeled
with a conditional circularly-symmetric complex Gaussian, centered at the corresponding source coefficient,
and (12) becomes pθo(otk|wtk=n,stn) =Nc(otk;Pkstn,Φtk). We can see that the assignment variable wtk
associates each TF-bin of the observed mixture spectrogram to one of the sources, and thus implicitly defines
a TF mask. All these adaptations yield minimal changes in the MixDVAE derivation and solution. These
changes are provided in Appendix B, including the final source vector estimate. Note that the estimated
n-th source waveform is obtained by applying the inverse STFT on m1:T,n.
6.1 DVAE pre-training
Dataset. We illustrate the application of MixDVAE to the SC-ASS problem with the separation of a speech
signal and a musical instrument, in the present case the Chinese bamboo flute (CBF). These two audio
sources have very different spectral and dynamical patterns. So, we choose here to pre-train two instances
of the same DVAE model separately on two single-source datasets, a speech dataset and a CBF dataset.
19Published in Transactions on Machine Learning Research (12/2023)
For the speech dataset, we used the Wall Street Journal (WSJ0) dataset (Garofolo et al., 1993), which
is composed of 16-kHz monophonic speech signals, with three subsets: si_tr_s,si_dt_05 andsi_et_05 ,
used for model training, validation and test, and containing 24.9, 2.2 and 1.5 hours of speech recordings,
respectively. For the musical instrument dataset, we used the CBF dataset of Wang et al. (2022), which
contains CBF performances recorded by 10 professional CBF performers. The dataset comprises recordings
of both isolated playing techniques and full-length pieces. We only used the full pieces recordings in our
experiments. The original recordings are stereo and at a sampling frequency of 44.1kHz. In our experiments,
we used only one channel and downsampled the signals to 16-kHz, to match the speech signals rate. We
selected the second half pieces recordings of player 1 and 2 as the validation set, the second half pieces
recordings of player 3, 4 and 5 as the test set, and use all other recordings for DVAE pre-training. The total
duration of the training, validation and test sets are 2.1, 0.2 and 0.3 hours respectively. For both datasets,
we used the training set for DVAE pre-training and the validation set for early stopping.
Pre-processing. For both the speech and CBF dataset, we pre-processed the raw audio signals in the
following way. First, the silence at the beginning and end of each signal are trimmed with a voice activity
detection threshold of 30 dB. Then, the waveform signals are normalized by dividing their absolute maximum
value. The STFT coefficients are computed with a 64-ms sine window (1024 samples) and a 75%-overlap
(256-sample hop length), resulting in sequences of 513-dimensional discrete Fourier coefficient vectors (for
positive frequencies). Note that in practice, the DVAE model will input speech power spectrograms, i.e., the
squared modulus of s, instead of the complex-valued STFT spectrograms (Girin et al., 2021, Chapter 13),
and these STFT power spectrograms are split into smaller sequences of length 50 frames (corresponding to
audio segments of 0.8s) for training.
Training details. We also used the SRNN model with scheduled sampling training (see Section 5.1). The
model was trained with the Adam optimizer with a learning rate set to 0.002 and a batch size set to 256.
The latent space dimension Lwas set to 16. The early-stopping patience was set to 50 epochs for the WSJ0
dataset and 200 epochs for the CBF dataset.
6.2 MixDVAE evaluation set-up
Dataset. To generate the test mixture signals, we first randomly selected two signals from the WSJ0 test
set and the CBF test set, respectively. Then, we removed the silence at the beginning and end in the
same way as for the pre-processing. The clipped speech and CBF signals were mixed together with several
different speech-to-music (power) ratios, namely −10,−5,0and5dB. The waveform mixture signals were
then normalized and transformed to STFT spectrograms in the same way as in the pre-processing. Similar
to the MOT scenario, we tested MixDVAE with different test sequence length values. To this aim, the
mixed signal STFT spectrograms were split into subsequences of length 50, 100 and 300 frames (respectively
corresponding to audio segments of 0.8, 1.6 and 4.8s). Overall, we generated 878,491, and 372mixed test
signals of length T= 50,100and300frames, respectively.
Algorithm initialization. As for MOT, we need to initialize the values of several parameters and variables.
For SC-ASS, it is more difficult to obtain a reasonable initialization for mtn(complex-valued) using directly
the observed mixture signal. We thus choose the following VEM iteration order: E-Z/E-S Step, E-W Step.
In this case, we have to initialize the posterior distribution of the assignment variable (i.e. all the values
ofηtkn), the input vectors of the DVAE encoder s1:T,n(for the two sources), and the observation model
covariance matrices Φtk. We initialize ηtknwith a discrete uniform distribution. As for the DVAE encoder,
we first input the power spectrogram of the mixture signal (recall that the two DVAEs were pre-trained on
different natural single-source datasets). We then use the reconstructed output power spectrograms as the
initialization of the DVAE encoders input.
Observation variance. Similar to the MOT case, Φtkis not estimated in the M Step, but fixed to r2
Φ|otk|2.
In plain words, Φtkis set to a fraction of the observation power. This setting turned out to stabilize the
VEM iteration process and finally led to very satisfying estimation results.
Hyperparameters. Regarding the hyperparameters of the MixDVAE VEM algorithm for the SC-ASS task,
the observation covariance matrix ratio rΦis set to 0.01. The total number of iterations Iis set to 70. And
the DVAE model is not fine-tuned in the E-Z step for the reported experiments.
20Published in Transactions on Machine Learning Research (12/2023)
Baselines. As mentioned in Section 1.4, the state-of-the-art SC-ASS methods are mostly fully supervised,
thus requiring a very large amount of paired (aligned) mixture signals and individual source signals for
training. Very few methods are under the weakly-supervised or unsupervised settings. Thus, it is difficult
to find a fairly comparable baseline model. In the presented experiments, we have compared the proposed
MixDVAE method with the unsupervised audio source separation method called MixIT (Wisdom et al.,
2020) and with two weakly-supervised methods based on NMF, namely a vanilla NMF model (Févotte
et al., 2018) and an NMF model with temporal extensions (Virtanen, 2007). MixIT is a deep-learning-based
unsupervised single-channel source separation method. It is trained on a dataset constructed by mixing up
the existing mixture audio signals. The model separates them into a variable number of latent source signals
that can be remixed to approximate the original mixtures. In a totally unsupervised setting, MixIT does
not require having the separated source signals for training. For the implementation of the MixIT model,
we have reused the code provided by the authors and adapted it for the speech-CBF source separation task.
In the weakly-supervised NMF baseline methods, an NMF model is first pre-trained on each single-source
dataset separately, resulting in a dictionary of non-negative spectral templates Wnfor each of the sources
to separate. Such pre-training is similar in spirit to the pre-training stage of the MixDVAE method. After
that, the obtained spectral template dictionaries of all sources are fixed and concatenated together so as to
learn the temporal activation matrix Hfor the test mixture signal. Then the Hentries corresponding to
the spectral templates in Wnare used to separate source n(in practice, Wiener filters are build to separate
the sources in the STFT domain, see (Févotte et al., 2018) and (Virtanen, 2007) for details). We have
re-implemented both NMF-based baselines according to the formula given in the corresponding papers. The
latent dimension Kof the NMF model for both speech and CBF data is set to 128, which is determined by
grid search. To demonstrate the interest of using a DVAE model for modeling the audio source dynamics in
MixDVAE, we made additional experiments with replacing the DVAE model in MixDVAE with two other
dynamical models: a linear-Gaussian dynamical model and a deep auto-regressive dynamical model, which
results in baseline models similar to the VKF model and the Deep AR model that we have already used in
our MOT experiments (see Section 5.2). For the VKF model, we initialize the values of ηtknin two ways:
the ground-truth assignment mask, which is also named as ideal binary mask (IBM) in the audio source
separation literature (we call the resulting model VKF-oracle) and the mask defined from the outputs of the
pre-trained DVAEs when inputing the mixture signal spectrogram (we call the resulting model VKF-DVAE-
init). Note that VKF-oracle provides an (unrealistic) upper bound of separation performance with a linear
dynamical model, whereas VKF-DVAE-init uses the same initial information as MixDVAE. Φtkare fixed to
the same values as for MixDVAE and Λtnare initialized with the identity matrix multiplied by a scalar. For
the Deep AR model, it is implemented with an LSTM layer, with the hidden dimension set equal to that of
the LSTM layers employed in the DVAE model. Finally, to investigate the effects of the VEM algorithm in
the MixDVAE model, we also compared our model with the direct reconstruction of the source signals from
the output of the pre-trained DVAEs when using the mixture spectrogram as the input, i.e. the information
used to intialize both MixDVAE and VKF-DVAE-init (we call this baseline method DVAE-init). As these
output spectrograms are power spectrograms, we combined their square root (amplitude spectrogram) with
the phase spectrogram of the mixture signal to reconstruct the waveform of the baseline separated signals.
Evaluation metrics. We used four source separation performance metrics widely-used in speech/audio
processing. The root mean squared error (RMSE), the scale-invariant signal-to-distortion ratio (SI-SDR)
(Roux et al., 2019) in dB, and the perceptual evaluation of speech quality (PESQ) score (Rix et al., 2001)
(values in [−0.5,4.5]).10For all metrics, the higher the better.
6.3 Experimental results
Quantitative analysis. We report the speech-CBF separation results on the short, medium and long test
sequence subsets ( T= 50,100,300frames, respectively) in Table 3. In addition to the results obtained by
the different models, we also report the values of the evaluation metrics when applied on the mixture signal,
for reference.
10The PESQ objective measure was developed mostly for evaluating the quality of speech signals, but since it is largely
based on a model of Human auditory perception, we assume we can also use it on the CBF sounds to avoid to complicate the
evaluation protocol.
21Published in Transactions on Machine Learning Research (12/2023)
Table 3: SC-ASS results for short ( T= 50), medium ( T= 100), and long ( T= 300) sequences.
Dataset MethodSpeech Chinese bamboo flute
RMSE↓SI-SDR↑PESQ↑RMSE↓SI-SDR↑PESQ↑
ShortMixture 0.016 -4.94 1.22 0.016 4.93 1.09
VKF-Oracle 0.004 14.83 2.00 0.004 20.15 2.33
DVAE-init 0.013 -0.51 1.20 0.019 3.04 1.44
VKF-DVAE-init 0.012 2.24 1.21 0.012 8.06 1.33
Deep AR 0.009 5.32 1.29 0.018 5.19 1.48
MixIT 0.011 3.26 - 0.009 7.15 -
Vanilla NMF 0.011 3.01 1.40 0.012 9.09 1.37
Temporal NMF 0.009 4.99 1.53 0.011 10.26 1.53
MixDVAE 0.006 9.23 1.73 0.007 13.50 2.30
MediumMixture 0.016 -4.44 1.17 0.016 4.44 1.08
VKF-Oracle 0.004 14.88 1.88 0.003 20.24 2.41
DVAE-init 0.014 0.10 1.15 0.020 2.42 1.27
VKF-DVAE-init 0.013 1.25 1.12 0.013 7.42 1.26
Deep AR 0.010 4.88 1.21 0.017 5.17 1.35
MixIT 0.009 4.75 - 0.009 8.74 -
Vanilla NMF 0.011 3.28 1.41 0.011 8.88 1.35
Temporal NMF 0.010 5.12 1.48 0.011 9.96 1.44
MixDVAE 0.007 9.32 1.65 0.007 13.05 2.16
LongMixture 0.016 -4.52 1.19 0.016 4.53 1.10
VKF-Oracle 0.004 14.65 1.89 0.003 20.45 2.60
DVAE-init 0.013 0.20 1.15 0.020 2.29 1.22
VKF-DVAE-init 0.013 0.34 1.10 0.013 7.35 1.24
Deep AR 0.010 3.87 1.17 0.017 4.74 1.27
MixIT 0.006 10.2 - 0.007 11.76 -
Vanilla NMF 0.011 3.31 1.40 0.011 8.98 1.35
Temporal NMF 0.010 5.01 1.47 0.011 10.06 1.42
MixDVAE 0.007 9.06 1.64 0.007 12.92 2.06
We observe that on the short and medium sequence subsets, MixDVAE achieves the best performance for
all of the evaluation metrics. While on the long sequences subsets, MixIT obtains slightly better results
than MixDVAE for the speech. This demonstrates that the proposed method works well on the SC-ASS
task. Unsurprisingly, VKF-Oracle obtains the best scores on all metrics because it was initialized with the
ground-truth mask. When comparing MixDVAE with the methods of different dynamical models, we find
that MixDVAE obtains overall better performance than both VKF-DVAE-init and Deep AR on all of the
metricsforallofthethreesubsets. Itisclearthatthenon-linearDVAEmodelwithstochasticlatentvariables
is much more efficient than the linear-Gaussian model and the deep auto-regressive model without latent
variables for modeling the audio source dynamics. Besides, with the increase of the sequence length, the
performance of MixDVAE dropped quite moderately (less than 0.6dB and less than 0.3dB in SI-SDR gain
decrease on speech and CBF respectively), while the performance of VKF dropped by 2.32dB on speech
and by 0.71dB on CBF, and the performance of Deep AR dropped by 1.45dB on speech and by 0.45dB
on CBF.
Compared to DVAE-init, both MixDVAE and VKF-DVAE-init exhibit better separation performance (at
least in terms of SI-SDR for VKF-DVAE-init). This indicates that the multi-source dynamical model with
the observation-to-source assignment latent variable plays an important role in separating the content of
different audio sources.
22Published in Transactions on Machine Learning Research (12/2023)
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5
Time (s)010002000300040005000600070008000 Hz
(a) Mixture
Speech
(b) Ground TruthFlute
(c) VKF-Oracle
40
30
20
10
010203040
Speech
(d) DVAE-initFlute
(e) VKF-DVAE-init
 (f) MixDVAE
40
30
20
10
010203040
Figure 4: An example of audio source separation result obtained with the proposed MixDVAE algorithm
and the baselines (speech and CBF power spectrograms). Best seen in color.
AlthoughMixITobtainsslightlybetterperformancethanMixDVAEforspeechonthelongsequencessubsets,
its performance on short and medium sequences subsets is quite bad (in terms of SI-SDR, only 3.26dB for
speech and 7.15dB for CBF on the short sequences subset, and 4.75dB for speech and 8.74dB for CBF
on the medium sequences subset). As for the NMF based models, though adding temporal extensions to
the vanilla NMF model indeed improves the model performance, the obtained results remain significantly
inferior to that obtained by MixDVAE.
Qualitative analysis. To illustrate the behavior of the different models, we selected an audio source
separation example and plotted the spectrograms in Fig. 4. More examples can be found in Appendix H. In
the given example, the sequence length of the spectrograms is 300 frames. It is obvious that the ground-truth
spectrograms of both the speech and CBF have spectral components with non-linear trajectories over time.
Even though VKF-Oracle achieves the best performance, we observe in Subfigure (c) that there are several
stationary traces (artifactual horizontal spectral lines) in the spectrograms caused by the inappropriate linear
dynamics hypothesis. This phenomenon becomes even worse when VKF is initialized with the mask defined
by the outputs of the pre-trained DVAEs (VKF-DVAE-init). In Subfigure (d), we clearly see the stationary
traces, especially for the separated speech spectrogram. We believe that this is the reason why VKF-DVAE-
init showed poor separation performance in general. When looking at the outputs of the pre-trained DVAE
models, we find that the pre-trained DVAE models can provide a relatively good initialization for the VEM
algorithm, even if we are still far from separated sources. In fact, since in the SC-ASS task, we pre-trained
separately two DVAE models on the speech dataset and on the CBF dataset, the pre-trained DVAE models
already have some prior information about the single-source dynamics. Though we only give the mixture
23Published in Transactions on Machine Learning Research (12/2023)
spectrogram as input, the pre-trained DVAE models can, to some extent, enhance the information of the
source used in pre-training and attenuate the information of the other source. However, this kind of filtering
is not very efficient. As we can see in the top figure of Subfigure (e), the output spectrogram provided by
the DVAE model pre-trained on the speech dataset still keeps a significant amount of information on the
CBF. Finally, even if the initialization is not that accurate, we see in Subfigure (f) that MixDVAE achieved
a good separation of the two sources after running the VEM iterations.
7 Conclusion and future work
In this paper, we introduce MixDVAE, an LVGM designed to model the dynamics of multiple, jointly
observed, sources. MixDVAE involves two main modules: A DVAE model for capturing the dynamics of
each individual source and a discrete latent assignment variable that assign observations to sources, thus
enabling us to form complete trajectories. The model learning process consists of two stages. During the
first stage, the same or different DVAE model(s) is/are pre-trained on the synthetic or natural single-source
trajectory dataset(s) to obtain prior information about the sources dynamics. During the second stage,
the pre-trained DVAE model(s) is/are integrated into the general MixDVAE model. The entire MixDVAE
model is solved using the VI framework with a VEM algorithm that combines the structured mean-field
approximation and the amortized inference principles. The VEM algorithm is run directly on each multi-
source test data sequence to process and the entire method does not require massive multi-source annotated
datasets for training, which are difficult to obtain, especially for natural data. Hence, we consider it as
weakly-supervised, as opposed to the fully-supervised approaches most commonly used in many multi-source
processing applications. We illustrate the versatility of MixDVAE by applying it to two distinct scenarios:
the MOT task and the SC-ASS task. Experimental results demonstrate that MixDVAE performs well on
both tasks. Specifically, thanks to the strong dynamical modeling capacity of DVAE, MixDVAE shows to be
more efficient than the combination of a linear dynamical model with the assignment variable. In addition,
MixDVAE can generate reasonable predictions of the source vector even in the absence of observations,
resultinginsmoothandrobusttrajectories, asdemonstratedintheMOTtask. Ourexperimentsdemonstrate
the generalization capability of MixDVAE trained on a synthetic single-target dataset, and evaluated in a
multiple-target dataset. Finally, we believe that MixDVAE has a very strong potential for modeling the
dynamics of multiple-source systems in general, and can be applied to various other tasks. However, we
acknowledge that MixDVAE also has certain limitations, such as the assumption that each source behaves
independently and the lack of consideration for interactions among them. We leave this as a challenging
topic for future research.
Acknowledgments
ThisresearchwaspartiallyfundedbytheHorizon2020SPRINGprojectfundedbytheEuropeanCommission
(under GA #871245), by the French Research Agency Young Researchers Program ML3RI project (under
GA #ANR-19-CE33-0008-01) and by the Multidisciplinary Institute of Artificial Intelligence (under GA
#ANR-19-P3IA-0003).
References
Masanao Aoki. State space modeling of time series . Springer Science & Business Media, 2013.
Yutong Ban, Xavier Alameda-Pineda, Laurent Girin, and Radu Horaud. Variational Bayesian inference for
audio-visual tracking of multiple speakers. IEEE Trans. Pattern Anal. Mach. Intell. , 43(5):1761–1776,
2021.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction
withrecurrentneuralnetworks. In Advances in Neural Inform. Process. Systems (NeurIPS) ,pp.1171–1179,
2015.
Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking performance: The CLEAR
MOT metrics. EURASIP J. Image Video Process. , 2008:10, 2008.
24Published in Transactions on Machine Learning Research (12/2023)
Xiaoyu Bie, Laurent Girin, Simon Leglaive, Thomas Hueber, and Xavier Alameda-Pineda. A benchmark of
dynamical variational autoencoders applied to speech spectrogram modeling. In Proc. Interspeech Conf. ,
Brno, Czech Republic, 2021.
Xiaoyu Bie, Wen Guo, Simon Leglaive, Lauren Girin, Francesc Moreno-Noguer, and Xavier Alameda-
Pineda. Hit-DVAE:HumanmotiongenerationviahierarchicaltransformerdynamicalVAE. arXiv preprint
arXiv:2204.01565 , 2022a.
Xiaoyu Bie, Simon Leglaive, Xavier Alameda-Pineda, and Laurent Girin. Unsupervised speech enhancement
using dynamical variational autoencoders. IEEE/ACM Trans. Audio, Speech, Lang. Process. , 30:2993–
3007, 2022b.
Christopher M. Bishop. Pattern recognition and machine learning . Springer-Verlag, Berlin, 2006.
Pritish Chandna, Marius Miron, Jordi Janer, and Emilia Gómez. Monoaural audio source separation using
deep convolutional neural networks. In Proc. Latent Variable Analysis and Signal Separation (LVA/ICA) ,
Grenoble, France, 2017.
Gioele Ciaparrone, Francisco Luque Sánchez, Siham Tabik, Luigi Troiano, Roberto Tagliaferri, and Francisco
Herrera. Deep learning in video multi-object tracking: A survey. Neurocomputing , 381:61–88, 2020.
Patrick Dendorfer, Aljoša Ošep, Anton Milan, Konrad Schindler, Daniel Cremers, Ian Reid, Stefan Roth,
and Laura Leal-Taixé. MOTChallenge: A benchmark for single-camera multiple target tracking. Int. J.
Comput. Vis. , 129:845–881, 2021.
Garry A Einicke and Langford B White. Robust extended Kalman filtering. IEEE Trans. Signal Process. ,
47(9):2596–2599, 1999.
Pedro F. Felzenszwalb, Ross B. Girshick, David McAllester, and Deva Ramanan. Object detection with
discriminatively trained part-based models. IEEE Trans. Pattern Anal. Mach. Intell. , 32(9):1627–1645,
2010.
Cédric Févotte, Emmanuel Vincent, and Alexey Ozerov. Single-channel audio source separation with nmf:
Divergences, constraints and algorithms. Audio Source Separation , pp. 1–24, 2018.
Marco Fraccaro, Søren Kaae Sønderby, Ulrich Paquet, and Ole Winther. Sequential neural models with
stochastic layers. In Advances in Neural Inform. Process. Systems (NeurIPS) , pp. 2207–2215, 2016.
Cédric Févotte, Nancy Bertin, and Jean-Louis Durrieu. Nonnegative matrix factorization with the Itakura-
Saito divergence: With application to music analysis. Neural Comp. , 21(3):793–830, 2009.
John S. Garofolo, David Graff, Doug Paul, and David Pallett. CSR-I (WSJ0) Sennheiser LDC93S6B.
Philadelphia: Linguistic Data Consortium , 1993.
Dan Geiger, Thomas Verma, and Judea Pearl. Identifying independence in Bayesian networks. Networks ,
20(5):507–534, 1990.
Zoubin Ghahramani and Geoffrey E Hinton. Parameter estimation for linear dynamical systems. Technical
Report, University of Toronto , 1996.
Zoubin Ghahramani and Geoffrey E. Hinton. Variational learning for switching state-space models. Neural
Comp., 12(4):831–864, 2000.
Zoubin Ghahramani and Michael Jordan. Factorial hidden Markov models. In Advances in Neural Inform.
Process. Systems (NeurIPS) , 1995.
Laurent Girin, Fanny Roche, Thomas Hueber, and Simon Leglaive. Notes on the use of variational autoen-
coders for speech and audio spectrogram modeling. In Proc. Int. Conf. Digital Audio Effects (DAFx) ,
Birmingham, UK, 2019.
25Published in Transactions on Machine Learning Research (12/2023)
Laurent Girin, Simon Leglaive, Xiaoyu Bie, Julien Diard, Thomas Hueber, and Xavier Alameda-Pineda.
Dynamical variational autoencoders: A comprehensive review. Found. Trends Mach. Learn. , 15(1-2):
1–175, 2021.
Matthew D. Hoffman, David M. Blei, Chong Wang, and John Paisley. Stochastic variational inference. J.
Mach. Learn. Res. , 14(4):1303–1347, 2013.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to
variational methods for graphical models. Mach. Learn. , 37(2):183–233, 1999.
Rudolf Emil Kalman. A new approach to linear filtering and prediction problems. J. Basic Eng. , 82(1):
35–45, 1960.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In Proc. Int. Conf. Learn. Repres.
(ICLR), 2014.
Harold W Kuhn. The Hungarian method for the assignment problem. Nav. Res. Logist. Q. , 2(1-2):83–97,
1955.
Antoine Liutkus, Roland Badeau, and Gäel Richard. Gaussian processes for underdetermined source sepa-
ration.IEEE Trans. Signal Process. , 59(7):3155–3167, 2011.
Wenhan Luo, Junliang Xing, Anton Milan, Xiaoqin Zhang, Wei Liu, and Tae-Kyun Kim. Multiple object
tracking: A literature review. Artif. Intell. , 293:103448, 2021.
Santiago Manen, Michael Gygli, Dengxin Dai, and Luc Van Gool. Pathtrack: Fast trajectory annotation
with path supervision. In Proc. IEEE Int. Conf. Computer Vision (ICCV) , pp. 290–299, 2017.
Geoffrey J McLachlan and Kaye E Basford. Mixture models: Inference and applications to clustering . M.
Dekker New York, 1988.
Geoffrey J McLachlan and Thriyambakam Krishnan. The EM algorithm and extensions . John Wiley &
Sons, 2007.
Kevin Murphy. Switching Kalman Filters . Citeseer, 1998.
Alexey Ozerov, Cédric Févotte, and Maurice Charbit. Factorial scaled hidden Markov model for polyphonic
audio representation and source separation. In Proc. IEEE Workshop Appl. Signal Process. Audio Acoust.
(WASPAA) , pp. 121–124, 2009.
Giorgio Parisi and Ramamurti Shankar. Statistical field theory. Phys. Today , 41(12):110, 1988.
LawrenceRabinerandBiinghwangJuang. AnintroductiontohiddenMarkovmodels. IEEE ASSP Magazine ,
3(1):4–16, 1986.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection
with region proposal networks. In Advances in Neural Inform. Process. Systems (NeurIPS) , 2015.
DaniloJimenezRezende,ShakirMohamed,andDaanWierstra. Stochasticbackpropagationandapproximate
inference in deep generative models. In Proc. Int. Conf. Mach. Learn. (ICML) , pp. 1278–1286, 2014.
Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi. Performance measures and
a data set for multi-target, multi-camera tracking. In Proc. Europ. Conf. Computer Vision (ECCV) , pp.
17–35. Springer, 2016.
Antony Rix, John Beerends, Michael Hollier, and Andries Hekstra. Perceptual evaluation of speech quality
(PESQ) - A new method for speech quality assessment of telephone networks and codecs. In Proc. IEEE
Int. Conf. Acoust., Speech, Signal Process. (ICASSP) , Salt Lake City, UT, 2001.
26Published in Transactions on Machine Learning Research (12/2023)
Jonathan Le Roux, Scott Wisdom, Hakan Erdogan, and John R. Hershey. SDR – Half-baked or well done?
InProc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP) , Brighton, UK, 2019.
Fatemeh Saleh, Sadegh Aliakbarian, Hamid Rezatofighi, Mathieu Salzmann, and Stephen Gould. Proba-
bilistic tracklet scoring and inpainting for multiple object tracking. In Proc. IEEE Int. Conf. Computer
Vision Pattern Recogn. (CVPR) , pp. 14329–14339, 2021.
Emmanuel Vincent, Maria G Jafari, Samer A Abdallah, Mark D Plumbley, and Mike E Davies. Probabilis-
tic modeling paradigms for audio source separation. In Machine Audition: Principles, Algorithms and
Systems, pp. 162–185. IGI global, 2011.
Tuomas Virtanen. Monaural sound source separation by nonnegative matrix factorization with temporal
continuity and sparseness criteria. IEEE Trans. Audio, Speech, Lang. Process. , 15(3):1066–1074, 2007.
Ba-ngu Vo, Mahendra Mallick, Yaakov Bar-Shalom, Stefano Coraluppi, Richard Osborne, Ronald Mahler,
and Ba-tuong Vo. Multitarget tracking. Wiley encyclopedia of electrical and electronics engineering , 2015.
Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational
inference. Found. Trends Mach. Learn. , 1(1–2):1–305, 2008.
ChanghongWang, EmmanouilBenetos, VincentLostanlen, andElaineChew. Adaptivescatteringtransforms
for playing technique recognition. IEEE Trans. Audio, Speech, Lang. Process. , 30:1407–1421, 2022.
De Liang Wang and Jitong Chen. Supervised speech separation based on deep learning: An overview.
IEEE/ACM Trans. Audio, Speech, Lang. Process. , 26(10):1702–1726, 2018.
Scott Wisdom, Efthymios Tzinis, Hakan Erdogan, Ron Weiss, Kevin Wilson, and John Hershey. Unsuper-
vised sound separation using mixture invariant training. In Advances in Neural Inform. Process. Systems
(NeurIPS) , volume 33, pp. 3846–3857, 2020.
Fan Yang, Wongun Choi, and Yuanqing Lin. Exploit all the layers: Fast and accurate CNN object detector
with scale dependent pooling and cascaded rejection classifiers. In Proc. IEEE Int. Conf. Computer Vision
Pattern Recogn. (CVPR) , pp. 2129–2137, 2016.
Ozgur Yilmaz and Scott Rickard. Blind separation of speech mixtures via time-frequency masking. IEEE
Trans. Signal Process. , 52(7):1830–1847, 2004.
Dong Yu and Li Deng. Automatic speech recognition . Springer, 2016.
Paul Zarchan. Progress in astronautics and aeronautics: fundamentals of Kalman filtering: a practical
approach , volume 208. Aiaa, 2005.
27Published in Transactions on Machine Learning Research (12/2023)
A MixDVAE algorithm calculation details
A.1 E-S Step
Here we detail the calculation of the posterior distribution qϕs(s|o). Using (11), the first expectation term
in (23) can be developed as:
Eqϕw(w|o)/bracketleftbig
logpθo(o|w,s)/bracketrightbig
=Eqϕw(w|o)/bracketleftbiggT/summationdisplay
t=1Kt/summationdisplay
k=1logpθo(otk|wtk,st,1:N)/bracketrightbigg
=T/summationdisplay
t=1Kt/summationdisplay
k=1Eqϕw(wtk|otk)/bracketleftbig
logpθo(otk|wtk,st,1:N)/bracketrightbig
. (39)
Since for any pair (t,k), the assignment variable wtkfollows a discrete posterior distribution, we can denote
its values by
qϕw(wtk=n|otk) =ηtkn,
which will be calculated later in the E-W Step. With this notation, we have:
Eqϕw(w|o)/bracketleftbig
logpθo(o|w,s)/bracketrightbig
=T/summationdisplay
t=1Kt/summationdisplay
k=1N/summationdisplay
n=1ηtknlogpθo(otk|wtk=n,stn). (40)
The second expectation in (23) cannot be computed analytically as a distribution on sbecause of the non-
linearity in the decoder and in the encoder. In order to avoid a tedious sampling procedure and obtain
a computationally efficient solution, we further approximate this term by assuming qϕz(z|s)≈qϕz(z|s=
m(i−1)), where m(i−1)is the mean value of the posterior distribution of sestimated at the previous iteration.
By using this approximation, the term Eqϕz(z|s)/bracketleftbig
logqϕz(z|s)/bracketrightbig
is now considered as a constant.
In addition, we observe that the second term of (23) can be rewritten as:
Eqϕz(z|s)/bracketleftbig
logpθsz(s,z)/bracketrightbig
=N/summationdisplay
n=1Eqϕz(z:,n|m(i−1)
:,n)/bracketleftbig
logpθsz(s:,n,z:,n)/bracketrightbig
, (41)
since both the DVAE joint distribution and posterior distribution factorise over the sources, as formalized
in (15) and (17). As a consequence, the posterior distribution of sfactorises over the sources too:
qϕs(s|o) =N/productdisplay
n=1qϕs(s:,n|o), (42)
and therefore:
qϕs(s:,n|o)∝exp/parenleftig
Eqϕz(z:,n|m(i−1)
:,n)/bracketleftbig
logpθsz(s:,n,z:,n)/bracketrightbig/parenrightigT/productdisplay
t=1Kt/productdisplay
k=1exp/parenleftbig
ηtknlogpθo(otk|wtk=n,stn)/parenrightbig
.
In the above equation, the expectation term cannot be calculated in closed form. As usually done in the
DVAE methodology, it is thus replaced by a Monte Carlo estimate using sampled sequences drawn from the
DVAE inference model. Let us denote by z(i)
:,n∼qϕz(z:,n|m(i−1)
:,n)such a sampled sequence. In the present
work, we use single point estimate, thus obtaining:
qϕs(s:,n|o)∝pθsz(s:,n,z(i)
:,n)T/productdisplay
t=1Kt/productdisplay
k=1exp/parenleftbig
ηtknlogpθo(otk|wtk=n,stn)/parenrightbig
∝T/productdisplay
t=1/parenleftig
pθs(stn|s1:t−1,n,z(i)
1:t,n)pθz(z(i)
tn|s1:t−1,n,z(i)
1:t−1,n)Kt/productdisplay
k=1exp/parenleftbig
ηtknlogpθo(otk|wtk=n,stn)/parenrightbig/parenrightig
.
(43)
28Published in Transactions on Machine Learning Research (12/2023)
We observe that the t-th element of the previous factorisation is a distribution over stnconditioned by
s1:t−1,n. As forqϕz(z:,n|s:,n), the dependency with s1:t−1,nis non-linear and therefore would impede to
obtain a computationally efficient closed-form solution. In the same attempt of avoiding costly sampling
strategies, we approximate the previous expression replacing s1:t−1,nwiths(i)
1:t−1,n, obtaining:
qϕs(s:,n|o)≈T/productdisplay
t=1qϕs(stn|s(i)
1:t−1,n,o), (44)
with
qϕs(stn|s(i)
1:t−1,n,o)∝pθs(stn|s(i)
1:t−1,n,z(i)
1:t,n)Kt/productdisplay
k=1exp/parenleftbig
ηtknlogpθo(otk|wtk=n,stn)/parenrightbig
, (45)
since the term pθz(z(i)
tn|s(i)
1:t−1,n,z(i)
1:t−1,n)becomes a constant.
Another interesting consequence of sampling s1:t−1,nis that the dependency with the future observations
ofqϕs(stn|s(i)
1:t−1,n,o)disappears. Indeed, since we are sampling at every time step , the future posterior
distributions qϕs(st+k,n|s(i)
1:t+k−1,n,o)do not depend on stn, and therefore the posterior distribution of stn
will not depend on the future observations.
The two distributions in the above equation are Gaussian distributions defined in (5), and (12). Therefore, it
canbeshownthatthevariationalposteriordistributionof stnisaGaussiandistribution: qϕs(stn|s(i)
1:t−1,n,o) =
N(stn;mtn,Vtn)withcovariancematrixandmeanvectorprovidedin(27)and(28)respectively, andrecalled
here for completeness:
Vtn=/parenleftigKt/summationtext
k=1ηtknΦ−1
tk+diag(v(i)
θs,tn)−1/parenrightig−1
, (46)
mtn=Vtn/parenleftigKt/summationtext
k=1ηtknΦ−1
tkotk+diag(v(i)
θs,tn)−1µ(i)
θs,tn/parenrightig
, (47)
wherev(i)
θs,tnandµ(i)
θs,tnare simplified notations for vθs(s(i)
1:t−1,n,z(i)
1:t,n)andµθs(s(i)
1:t−1,n,z(i)
1:t,n)respectively,
denoting the variance and mean vector estimated by the DVAE for source nat time frame t.
A.2 E-Z Step
Here we detail the calculation of the ELBO term (29).
L(θs,θz,ϕz;o) =Eqϕs(s|o)/bracketleftig
Eqϕz(z|s)/bracketleftbig
logpθsz(s,z)−logqϕz(z|s)/bracketrightbig/bracketrightig
=EN/producttext
n=1qϕs(s:,n|o)/bracketleftbigg
EN/producttext
n=1qϕz(z:,n|s:,n)/bracketleftigN/summationtext
n=1logpθsz(s:,n,z:,n)/bracketrightig
−EN/producttext
n=1qϕz(z:,n|s:,n)/bracketleftigN/summationtext
n=1logqϕz(z:,n|s:,n)/bracketrightig/bracketrightbigg
=N/summationdisplay
n=1Eqϕs(s:,n|o)/bracketleftig
Eqϕz(z:,n|s:,n)/bracketleftbig
logpθsz(s:,n,z:,n)/bracketrightbig
−Eqϕz(z:,n|s:,n)/bracketleftbig
logqϕz(z:,n|s:,n)/bracketrightbig/bracketrightig
=N/summationdisplay
n=1Ln(θs,θz,ϕz;o), (48)
with
Ln(θs,θz,ϕz;o) =Eqϕs(s:,n|o)/bracketleftig
Eqϕz(z:,n|s:,n)/bracketleftbig
logpθsz(s:,n,z:,n)/bracketrightbig
−Eqϕz(z:,n|s:,n)/bracketleftbig
logqϕz(z:,n|s:,n)/bracketrightbig/bracketrightig
.(49)
29Published in Transactions on Machine Learning Research (12/2023)
A.3 E-W Step
Here we detail the calculation of the posterior distribution qϕw(w|o). Applying the optimal update equa-
tion (2) to w, we have:
qϕw(w|o)∝exp/parenleftig
Eqϕs(s|o)qϕz(z|s)/bracketleftbig
logpθ(o,w,s,z)/bracketrightbig/parenrightig
. (50)
Using (10), we derive:
qϕw(w|o)∝pθw(w) exp/parenleftig
Eqϕs(s|o)/bracketleftbig
logpθo(o|w,s)/bracketrightbig/parenrightig
. (51)
Using (11), the expectation term can be developed as:11
Eqϕs(s|o)/bracketleftbig
logpθo(o|w,s)/bracketrightbig
=Eqϕs(s|o)/bracketleftigT/summationtext
t=1Kt/summationtext
k=1logpθo(otk|wtk,st,:)/bracketrightig
=T/summationdisplay
t=1Kt/summationdisplay
k=1Eqϕs(st,:|o)/bracketleftbig
logpθo(otk|wtk,st,:)/bracketrightbig
. (52)
Combining (13) and the previous result, we have:
qϕw(w|o)∝T/productdisplay
t=1Kt/productdisplay
k=1pθw(wtk) exp/parenleftig
Eqϕs(st,:|o)/bracketleftbig
logpθo(otk|wtk,st,:)/bracketrightbig/parenrightig
, (53)
which we can rewrite
qϕw(w|o)∝T/productdisplay
t=1Kt/productdisplay
k=1qϕw(wtk|o), (54)
with
qϕw(wtk|o) =pθw(wtk) exp/parenleftig
Eqϕs(st,:|o)/bracketleftbig
logpθo(otk|wtk,st,:)/bracketrightbig/parenrightig
. (55)
The assignment variable wtkfollows a discrete distribution and we denote:
ηtkn=qϕw(wtk=n|o)∝pϕw(wtk=n) exp/parenleftig
Eqϕs(stn|o)/bracketleftbig
logpθo(otk|wtk=n,stn)/bracketrightbig/parenrightig
. (56)
Using the fact that both pθo(otk|wtk=n,stn)andqϕs(stn|o)are multivariate Gaussian distributions (defined
in (12) and (26)–(28), respectively), the previous expectation can be calculated in closed form:
Eqϕs(stn|o)/bracketleftbig
logpθo(otk|wtk=n,stn)/bracketrightbig
=/integraldisplay
stnN(stn;mtn,Vtn) logN(otk;stn,Φtk)dstn
=−1
2/bracketleftig
log|Φtk|+ (otk−mtn)TΦ−1
tk(otk−mtn) +Tr/parenleftbig
Φ−1
tkVtn/parenrightbig/bracketrightig
.
(57)
By using (14), the previous result, and normalizing to 1, we finally get:
ηtkn=βtkn/summationtextN
i=1βtki, (58)
where
βtkn=N(otk;mtn,Φtk) exp/parenleftig
−1
2Tr/parenleftbig
Φ−1
tkVtn/parenrightbig/parenrightig
. (59)
11In fact, the posterior distribution qϕs(st,:|o)is also conditioned on s1:t−1,:andz1:t,:. We use this abuse of notation for
concision.
30Published in Transactions on Machine Learning Research (12/2023)
A.4 M Step
Here we detail the calculation of Φtk. In the ELBO expression (19), only the first term depends on θo:
L(θo;o) =Eqϕw(w|o)qϕs(s|o)/bracketleftbig
logpθo(o|w,s)/bracketrightbig
=N/summationdisplay
n=1T/summationdisplay
t=1Kt/summationdisplay
k=1ηtkn/integraldisplay
stnN(stn;mtn,Vtn) logN(otk;stn,Φtk)dstn
=−1
2N/summationdisplay
n=1T/summationdisplay
t=1Kt/summationdisplay
k=1ηtkn/bracketleftig
log|Φtk|+ (otk−mtn)TΦ−1
tk(otk−mtn) +Tr(Φ−1
tkVtn)/bracketrightig
.(60)
By computing the derivative of L(θo;o)with respect to Φtkand setting it to 0, we find the optimal value of
Φtkthat maximizes the ELBO:
Φtk=N/summationdisplay
n=1ηtkn/parenleftig
(otk−mtn)(otk−mtn)T+Vtn/parenrightig
. (61)
B Formulas for SC-ASS
With the adaptations in the model mentioned at the beginning of Section 6 for the SC-ASS task, the solution
formulas are as following. In the E-S Step, (27) and (28) become:
Vtn=/parenleftigKt/summationtext
k=1ηtknPT
kΦ−1
tk+diag(v(i)
θs,tn)−1/parenrightig−1
, (62)
mtn=Vtn/parenleftigKt/summationtext
k=1ηtknPT
kΦ−1
tkotk/parenrightig
. (63)
The E-Z Step is not changed. In the E-W Step, (33) become:
βtkn=Nc(otk;Pkmtn,Φtk) exp/parenleftig
−1
2Tr/parenleftbig
PT
kΦ−1
tkPkVtn/parenrightbig/parenrightig
. (64)
Finally, in the M Step, (34) become:
Φtk=N/summationdisplay
n=1ηtkn/parenleftig
(otk−Pkmtn)(otk−Pkmtn)T+PkVtnPT
k/parenrightig
. (65)
C Cascade initialization in MOT
Fortheinitializationofthesource(position)vector, wefirstsplitthelongsequenceindexedby t∈{1,2,...,T}
intoJsmaller sub-sequences indexed by {{1,...,t 1},{t1+ 1,...,t 2},...,{tJ−1+ 1,...,T}}. For the first sub-
sequence, the mean vector sequence m1:t1,nis initialized as the detected vector at the first frame o1krepeated
fort1timeswithaarbitraryorderofassignment. Thus,thereareasmanytrackedsourcesasinitialdetections,
i.e., this implicitly sets N=K1. The subsequence of source position vectors s1:t1,nis initialized with the
same values as for the mean vector. Then, we run the MixDVAE algorithm on the first subsequence for
I0iterations. Next, we initialize the mean vector sequence mt1+1:t2,nof the second subsequence with mt1n
repeated for t2−t1times (and the same for st1+1:t2,n). And so on for the following subsequences. Finally, the
initialized subsequences are concatenated together to form the initialized whole sequence. The pseudo-code
of the cascade initialization can be found in Algorithm 2.
31Published in Transactions on Machine Learning Research (12/2023)
Algorithm 2 Cascade initialization of the position vector sequence
Input:
Detected bounding boxes at the first frame o1,1:K1;
Pre-trained DVAE parameters { θs,θz,ϕz};
Initialized observation model covariance matrices {Φ(0)
tk}T,Kt
t,k=1;
Initialized covariance matrices {V(0)
tn}T,N
t,n=1;
Output:
Initialized mean position vector sequence {m(0)
tn}T,N
t,n=1;
Initialized sampled position vector sequence {s(0)
tn}T,N
t,n=1;
1:Split the whole observation sequence ointoJsub-sequences indexed by {t0= 1,...,t 1},{t1+ 1,...,t 2},
...,{tJ−1+ 1,...,tJ=T};
2:forj== 1 do
3: fork←1toK1do
4:n←k;
5: fort←1tot1do
6: m(0)
tn,s(0)
tn=o1k;
7: end for
8: end for
9:end for
10:forj←2toJdo
11: forn←1toNdo
12: fort←tj−1+ 1totjdo
13: m(0)
tn,s(0)
tn=m(I0)
tj−1n;
14: end for
15:{m(I0)
tn}tj
t=tj−1+1=MixDVAE (I0,{{θs,θz,ϕz},
16:{Φ(0)
tk,m(0)
tn,V(0)
tn,s(0)
tn}tj,Kt,N
t=tj−1+1,k=1,n=1});
17: end for
18:end for
19:m(0)
1:T,1:N=/bracketleftbig
m(0)
1:t1,1:N,...,m(0)
tJ−1+1:T,1:N/bracketrightbig
;
20:s(0)
1:T,1:N=/bracketleftbig
s(0)
1:t1,1:N,...,s(0)
tJ−1+1:T,1:N/bracketrightbig
;
D SRNN implementation details
The SRNN generative model is implemented with a forward LSTM network, which embeds all the past
information of the sequence s. Then, a dense layer with the tanh activation function plus a linear layer
provide the parameters µθs,vθs. Similarly, the parameters µθz,vθzare computed with two dense layers with
tanh activation function plus a linear layer appended to the LSTM as well. The inference model shares the
hidden variables of the forward LSTM network of the generative model and uses two dense layers with the
tanh activation function plus a linear layer to compute the parameters µϕz,vϕz.
In the MOT set-up, both standztare of dimension 4. While in the SC-ASS set-up, stis of dimension 513
andztis of dimension 16. The SRNN generative distributions in the right-hand side of (36) are implemented
as:
ht=dh(st−1,ht−1), (66)
/bracketleftbig
µθz,vθz/bracketrightbig
=dz(ht,zt−1), (67)
pθz(zt|s1:t−1,zt−1) =N/parenleftbig
zt;µθz,diag(vθz)/parenrightbig
, (68)
/bracketleftbig
µθs,vθs/bracketrightbig
=ds(ht,zt), (69)
pθs(st|s1:t−1,zt) =N/parenleftbig
st;µθs,diag(vθs)/parenrightbig
, (70)
32Published in Transactions on Machine Learning Research (12/2023)
Figure 5: Schema of the SRNN model architecture. The “plus” symbol represents the concatenation of the
input vectors.
where the function dhin (66) is implemented by a forward RNN and htdenotes the RNN hidden state
vector, the dimension of which is set to 8 for MOT and 128 for SC-ASS. In practice, LSTM networks are
used. The function dsin (69) is implemented by a dense layer of dimension 16 for MOT and of dimension 256
for SC-ASS, with the tanh activation function, followed by a linear layer, which outputs are the parameters
µθs,vθs. The function dzin (67) is implemented by two dense layers of dimension 8, 8 respectively for MOT
and of dimension 64, 32 respectively for SC-ASS, with the tanh activation function, followed by a linear
layer, which outputs are the parameters µθz,vθz.
The SRNN inference model in the right-hand side of (37) is implemented as:
/bracketleftbig
µϕz,vϕz/bracketrightbig
=ez(ht,st,zt−1), (71)
qϕz(zt|zt−1,s1:t) =N/parenleftbig
zt;µϕz,diag(vϕz)/parenrightbig
, (72)
where the function ezin (71) is implemented by two dense layers of dimension 16 and 8 respectively for
MOT and of dimension 64 and 32 respectively for SC-ASS, with the tanh activation function, followed by a
linear layer, which outputs are the parameters µϕz,vϕz.
TheSRNNarchitectureisschematizedinFigure5. ItcanbenotedthattheRNNinternalstate htcumulating
the information on s1:t−1is shared by the encoder and the decoder, see (Girin et al., 2021, Chapter 4) for a
discussion on this issue.
E MOT dataset processing
E.1 Synthetic trajectory dataset generation
To generate bounding boxes with reasonable size, we generate the coordinates of the top-left point (noted
asxL
tandxT
t) plus the height (noted as at) and width (noted as bt) of the bounding boxes and deduce
the coordinates of the bottom-right point. The width-height ratio is sampled randomly, and kept constant
during the trajectory. While the trajectory of one coordinate is generated using piece-wise combinations of
elementary functions, which are: static a(t) =a0, constant velocity a(t) =a1t+a0, constant acceleration
a(t) =a2t2+a1t+a0, and sinusoidal (allowing for circular trajectories) a(t) =asin(ωt+ϕ0). That is to say,
we split the whole sequence into several segments, and each segment is dominated by a certain elementary
33Published in Transactions on Machine Learning Research (12/2023)
Algorithm 3 Synthetic trajectories generation
Input:
Total sequence length T;
Maximum sub-sequence number smax;
Distribution parameters µb0,σb0,µr,σr,µa1,σa1,µa2,σa2,µω,σω,µϕ0,σϕ0;
Discrete probability distribution of different elementary trajectory function types p= [p1,p2,p3,p4];
Output:
Synthetic bounding box position sequence gen_seq={(xl
t,xt
t,xr
t,xb
t)}T
t=1;
1:function GenSeq (x0,s,tsplit,params_prob,p)
2:start =x0;
3: fori←0tosdo
4: Samplefunction _typeusingp;
5: Sample trajectory function parameters params_listusingparams_prob;
6:ti=tsplit[i];
7:x_subi=GenTraj (start,func_type,
8:params_list);
9:start =x_subi[ti];
10: end for
11:x= [x_sub0,...,x_subs−1];
12: returnx;
13:end function
14:Samplex0,y0fromU(0,1);
15:Sampleb0from logN(µb0,σb0);
16:SamplerabfromN(µr,σr);
17:Randomly sample sin{0,...,smax};
18:Randomly sample tsplit={t0,...,ts−1}in{1,...,T};
19:x=GenSeq (x0,s,tsplit,params_prob,p);
20:y=GenSeq (y0,s,tsplit,params_prob,p);
21:b=GenSeq (w0,s,tsplit,params_prob,p);
22:a=b∗rab;
23:gen_seq= [x,y,x +b,y−a];
function. An example of a 3-segment combination could be:
a(t) =

a0 1≤t<t 1,
a2t2+a1t+a′
0t1≤t<t 2,
a3sin(ωt+ϕ0)t1≤t≤T,(73)
where the segments length is sampled from some pre-defined distributions to generate reasonable and con-
tinuous trajectories. The number of segments sis first uniformly sampled in the set {1,...,s max}. We then
samplessegment lengths that sum up to T. This defines the segment boundaries t1,...,ts−1. For each
segment, one of the four elementary functions is randomly selected. The function parameters are sampled
as follow:a1∼N (µa1,σ2
a1),a2∼N (µa2,σ2
a2),ω∼N (µω,σ2
ω)andϕ0∼N (µϕ0,σ2
ϕ0). The two remaining
parameters, a0anda, are set to the values needed to ensure continuous trajectories, thus initialising the
trajectories at every segment, except for the first one. The very initial trajectory point is sampled randomly
fromU(0,1). And the initial width is sampled from a log-normal distribution b0∼logN(µb0,σ2
b0). Finally,
the ratio between the height and width is supposed to be constant with respect to time. It is sampled from
a log-normal distribution rab=a
b∼logN(µr,σ2
r)and the height is obtained by multiplying the width and
the ratio. More implementation details can be found in Algorithm 3.
In our experiments, the total sequence length of the generated trajectories for DVAE pre-training equals
toT= 60frames. And the maximum number of segments is set to smax= 3. The parameters of the a1,
a2,ω,ϕ0,w0, andrhwdistributions are determined by estimating the statistical characteristics of publicly
published detections of the MOT17 training dataset. More precisely, we estimated the empirical mean and
34Published in Transactions on Machine Learning Research (12/2023)
standard deviation of the speed and acceleration for all matched detection sequences (i.e., the first and
second order differentiation of the position sequences).
E.2 MOT17-3T dataset construction
To construct the MOT17-3T dataset, first, we matched the detected bounding boxes to the ground-truth
bounding boxes using the Hungarian algorithm (Kuhn, 1955) and retained only the matched detected bound-
ing boxes (i.e., the detected bounding boxes that were not matched to any ground-truth bounding boxes
were discarded). The cost matrix were computed according to the the Intersection-over-Union (IoU) dis-
tance between bounding boxes. We split each complete video sequence into subsequences of length T(three
different values of Tare tested in our experiments, as detailed below) and only kept the tracks with a length
no shorter than T. For each subsequence, we randomly chose three tracks that appeared in this subsequence
from the beginning to the end. The detected bounding boxes of these three tracks form one test data sample.
We have tested three values for the sequence length Tto evaluate its influence on the tracking performance
of our algorithm: 60, 120, and 300 frames (respectively corresponding to 2, 4, and 10 seconds at 30 fps).
Among the three public detection results provided with the MOT17 dataset, SDP has the best detection
performance. So, we used the detection results of SDP to create our dataset.
F MOT baselines implementation details
ArTIST Saleh et al. (2021) is a probabilistic auto-regressive model which consists of two main blocks: MA-
Net and the ArTIST model. MA-Net is a recurrent autoencoder that is trained to learn a representation
of the dynamical interaction between all agents in the scene. ArTIST is an RNN that takes as input a 4D
velocity vector of the current frame for one object as well as the corresponding 256-dimensional interaction
representation learned by MA-Net, and outputs a probability distribution for each dimension of the motion
velocity for the next frame. As indicated in Saleh et al. (2021), the models are trained on the MOT17
training set and the PathTrack Manen et al. (2017) dataset. We have reused the trained models as well
as the tracklet scoring and inpainting code provided by the authors and reimplemented the object tracking
part according to the paper, as this part was not provided. The tracklets are initialized with the bounding
boxes detected in the first frame. For any time frame t, the score of assigning a detection otkto a tracklet
nis obtained by evaluating the likelihood of this detection under the distribution estimated by the ArTIST
model. The final assignment is computed using the Hungarian algorithm. For any tentatively alive tracklet
whose last assignment is prior to t−1with a non-zero gap (implying that there exists a detection absence),
the algorithm first performs tracklet inpainting to fill the gap up to t−1, then computes the assignment score
with the inpainted tracklet. As described in Saleh et al. (2021), the inpainting is done with multinominal
sampling, and a tracklet rejection scheme (TRS) is applied to select the best inpainted trajectory. In order
to eliminate possible inpainting ambiguities, the Hungarian algorithm is run twice, once only for the full
sequences without gaps and the second time for the inpainted sequences. The number of candidates for
multinominal sampling is set to 50. For the TRS, the IoU threshold used in Saleh et al. (2021) is 0.5. In our
test scenario, there are less tracklets and the risk of false negative is much greater than that of false positive.
So, we decreased the threshold to 0.1, which provided better results than the original value.
G More MOT tracking examples
The first example plotted in Fig. 6 illustrates the case where two persons cross each other. This is one of the
most complicated situations that may cause an identity switch and even lead to tracking loss. Considering
the limited space for the figure, we display the bounding boxes every ten frames to view the whole process
of crossing. For t= 60, when the ground-truth bounding boxes of Sources 2 and 3 ( s2ands3in the figure)
strongly overlap, Detection o2disappears. Again, ArTIST exhibits frequent identity switches. Besides,
att= 20, the estimated bounding box m1is totally overlapped with that of m3. And att= 90, the
estimated bounding boxes for all of the three sources are getting very close to each other. This indicates
that the identity switches can cause unreasonable trajectories estimation. For VKF, the observations for
both Sources 2 and 3 are assigned to the same target s3all along the sequence, due to s2ands3being close
35Published in Transactions on Machine Learning Research (12/2023)
t = 0t = 10t = 20t = 30t = 40t = 50t = 60t = 70t = 80t = 90Ground TruthDetectionArTISTVKF
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 
m 2 m 1 
 Deep AR
MixDV AE
(a) Example 1: Crossing sources.
t = 0t = 10t = 20t = 30t = 40t = 50t = 60t = 70t = 80t = 90Ground TruthDetectionArTISTVKFDeep ARMixDV AE
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
(b) Example 2: Crossing sources with frequent detection absence.
Figure6: ExamplesoftrackingresultobtainedwiththeproposedMixDVAEalgorithmandthetwobaselines.
For clarity of presentation, the simplified notations s1,o1, andm1denote the ground-truth source position,
the observation, and the estimated position, respectively (for Source 1, and the same for the two other
sources). Best seen in color.
to each other, so that the estimated bounding boxes m2andm3overlap completely. For the Deep AR, the
estimation of m3becomes inaccurate from t= 70and it disappears at t= 80andt= 90(the estimation is
out of the frame). In contrast, MixDVAE displays a consistent tracking of the three sources. For t= 50,60,
and70, the estimated bounding boxes m2andm3overlap due to the ground-truth bounding boxes s2and
s3strongly overlap each other. However, the tracking is correctly resumed at t= 80, with no identity switch
(i.e., the crossing of Sources 2 and 3 is correctly captured by the model).
The second example displayed in Fig. 6 is another more complicated situation with two sources very close
to each other and frequent detection absence. At t= 20when observation o3disappears, both ArTIST and
36Published in Transactions on Machine Learning Research (12/2023)
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5
Time (s)010002000300040005000600070008000 Hz
(a) Mixture
Speech
(b) Ground TruthFlute
(c) VKF-Oracle
40
30
20
10
010203040
Speech
(d) DVAE-initFlute
(e) VKF-DVAE-init
 (f) MixDVAE
40
30
20
10
010203040
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5
Time (s)010002000300040005000600070008000 Hz
(a) Mixture
Speech
(b) Ground TruthFlute
(c) VKF-Oracle
40
30
20
10
010203040
Speech
(d) DVAE-initFlute
(e) VKF-DVAE-init
 (f) MixDVAE
40
30
20
10
010203040
Figure 7: Examples of audio source separation results obtained with the proposed MixDVAE algorithm and
the baselines. Best seen in color.
37Published in Transactions on Machine Learning Research (12/2023)
Table 4: Results obtained by MixDVAE on MOT17-3T (short sequences subset) for different values of rΦ.
The values on the left (resp. right) side of the slashes are obtained without (resp. with) the fine-tuning of
SRNN in the E-Z Step.
rΦMOTA↑MOTP↑IDF1↑ #IDs↓ %IDs↓MT↑ ML↓ #FP↓ %FP↓ #FN↓ %FN↓
0.01 35.9/32.8 84.5/84.866.6/65.5 4914/3216 1.6/1.0 2946/2714 916/913 96438/102062 31.3/33.1 96438/102062 31.3/33.1
0.02 65.5/61.8 84.2/84.7 81.3/79.8 5319/3073 1.7/1.0 3932/3652 407/379 50596/57291 16.4/18.6 50596/57291 16.4/18.6
0.03 74.9/70.0 83.1/84.3 86.1/84.4 5088/ 28531.7/0.94232/3931 158/160 36165/43777 11.7/14.2 36165/43777 11.7/14.2
0.04 79.1/75.1 81.3/83.5 88.4/86.7 4966/2862 1.6/0.9 4370/4067 50/64 29808/36990 9.7/11.9 29808/36990 9.7/11.9
0.05 76.4/ 75.679.2/82.6 87.1/ 87.14982/2919 1.6/0.9 4268/ 4066 42 /5333924/ 36088 11.0/11.733924/ 36088 11.0/11.7
0.06 69.2/70.1 76.9/82.0 83.5/84.4 5297/3005 1.7/1.0 3978/3845 73/137 44793/44598 14.5/14.5 44793/44598 14.5/14.5
0.07 59.8/66.8 74.8/80.3 78.9/82.9 5146/3000 1.7/1.0 3688/3775 188/285 59348/49646 19.2/16.1 59348/49646 19.2/16.1
0.08 48.5/60.6 73.1/79.4 73.3/79.9 5097/3119 1.7/1.0 3303/3637 337/432 76865/59220 24.9/19.2 76865/59220 24.9/19.2
Figure 8: MOTA score obtained by MixDVAE as a function of the number of VEM iterations, for different
values ofrΦ.
VKF lose one of the tracks, whereas MixDVAE keeps a reasonable tracking of the three tracks. From t= 60
to80, botho2ando3are absent. The tracks inpainted by ArTIST are not consistent anymore and VKF
still misses one track. The estimations of Deep AR are inaccurate when the detections are absent. However,
even in this difficult scenario, MixDVAE keeps on providing three reasonable trajectories.
H More SC-ASS examples
In Figure 7 we plot two other SC-ASS examples.
I Ablation study
In order to better understand the MixDVAE model, we conducted ablation studies on the influence of the
pre-trained DVAE model quality, the influence of fine-tuning the DVAE, and the influence of the observation
variation matrix ratio rΦ.
I.1 Influence of the pre-trained DVAE model quality
We have conducted an ablation study on the influence of the pre-trained DVAE model quality on the whole
MixDVAE algorithm, for both the MOT task and the SC-ASS task. Specifically, we have pre-trained the
DVAE model at different data scales and tested the performance of MixDVAE using these different pre-
trained models.
38Published in Transactions on Machine Learning Research (12/2023)
Table 5: Capacity of the SRNN model pre-trained at three data scales of the synthetic trajectories dataset.
SRNN-full, SRNN-half, and SRNN-quarter stand for SRNN pre-trained on the totality, half of and quarter
of our original training set, respectively.
Model name Training loss Validation loss
SRNN-full -40.77 -40.15
SRNN-half -39.36 -38.86
SRNN-quarter -36.55 -35.27
Table 6: MOT results obtained by MixDVAE with SRNN pre-trained at the three data scales. The results
are reported for the short sequence test subset ( T= 60frames).
Model name MOTA ↑MOTP↑IDF1↑#IDs↓%IDs↓MT↑ML↓#FP↓%FP↓#FN↓%FN↓
SRNN-full 79.1 81.3 88.4 4966 1.6 4370 50 29808 9.7 29808 9.7
SRNN-half 74.7 84.4 86.6 5624 1.8 4039 94 38153 12.4 38153 12.4
SRNN-quarter 75.2 84.4 86.9 5598 1.8 4040 91 37443 12.2 37443 12.2
Table 7: Capacity of the SRNN model pre-trained at three data scales of the WSJ0 and the CBF datasets.
SRNN-full, SRNN-half, and SRNN-quarter stand for SRNN pre-trained on the totality, half of and quarter
of our original training set, respectively.
Model nameWSJ0 CBF
Training loss Validation loss Training loss Validation loss
SRNN-full 353.89 373.61 521.76 779.69
SRNN-half 358.13 389.58 489.53 949.11
SRNN-quarter 361.58 383.64 646.55 1106.27
MOT task. We conducted pre-training of the SRNN model on three separate datasets with different scales:
the full synthetic trajectories training set used in Section 5, consisting of 12,105trajectories, a dataset
containing half of these synthetic trajectories, randomly selected ( 6,052trajectories), and another dataset
with a quarter of these synthetic trajectories, randomly selected ( 3,026trajectories). We use the ELBO loss
to represent the quality of the resulting pre-trained SRNN models. The ELBO loss values are reported in
Table 5. As expected, we observe that by decreasing the training data size, the performance of the SRNN
model drops (with higher training and validation loss).
We run the MixDVAE inference algorithm with the three pre-trained SRNN models on the short sequence
test subset ( T= 60frames), and the obtained results are reported in Table 6. We can see that the overall
performance of the MixDVAE algorithm with SRNN-half and SRNN-quarter drops compared to that with
SRNN-full, but this drop is relatively limited, at least for some of the metrics, including the key MOTA
metric. Moreover, the difference between the performance of MixDVAE with SRNN-half and with SRNN-
quarter is quite small. Therefore, even if it is hard to draw a general conclusion from a single experiment with
three dataset sizes, this seems to indicate some robustness of MixDVAE w.r.t. the DVAE training dataset
size, and confirm its interest as a data-frugal weakly supervised method (here for the MOT application).
SC-ASS task . Similar to the MOT task, we also generated two additional subsets of the training data
for both the WSJ0 and CBF datasets, comprising half and a quarter of our original training dataset (used
in Section 6), randomly selected. The two new subsets of WSJ0 contains 12.45and6.29hours of speech
recordings respectively. And the two new subsets of CBF contains 1.07and0.55hours of CBF recordings
respectively. The performance of the SRNN model pre-trained on these different datasets is reported in
Table 7. For the WSJ0 dataset, we observe that the training and validation losses are relatively close to each
other, and both increase when decreasing the training data size, but the increase is moderate. Therefore,
39Published in Transactions on Machine Learning Research (12/2023)
Table 8: SC-ASS results obtained by MixDVAE with SRNN pre-trained at the three data scales. The results
are reported for the short sequence test subset ( T= 50).
Model nameSpeech Chinese bamboo flute
RMSE↓SI-SDR↑PESQ↑RMSE↓SI-SDR↑PESQ↑
SRNN-full 0.006 9.23 1.73 0.007 13.50 2.30
SRNN-half 0.006 9.66 1.82 0.009 12.29 2.28
SRNN-quarter 0.007 8.83 1.79 0.011 10.29 2.13
Table 9: MOT results obtained by MixDVAE with and without the fine-tuning of SRNN. The results are
reported for the short, medium and long sequence test subsets ( T= 60,120, and 300frames, respectively).
Dataset Fine-tuning MOTA ↑MOTP↑IDF1↑#IDs↓%IDs↓MT↑ML↓#FP↓%FP↓#FN↓%FN↓
ShortYes 75.1 83.5 86.7 2862 0.9 4067 64 36990 11.9 36990 11.9
No 79.1 81.3 88.4 4966 1.6 4370 50 29808 9.7 29808 9.7
MediumYes 73.1 84.0 85.9 3044 0.7 2705 136 54604 13.1 54604 13.1
No 78.6 82.2 88.0 6107 1.5 2907 120 41747 9.9 41747 9.9
LongYes 65.6 84.9 81.6 8670 0.8 2286 67 171515 13.8 171515 13.8
No 83.2 82.4 90.0 23081 2.3 2890 12 74550 7.3 74550 7.3
the capacity of SRNN drops, but quite slightly. However, for the CBF dataset, the gap between the training
and validation losses is higher, and the training loss of SRNN-half decreases compared to SRNN-full while
the validation loss increases significantly, increasing the gap. Both the training loss and the validation loss
of SRNN-quarter are higher than that of SRNN-full and SRNN-half, and the gap between training and
validation is also relatively large. This shows that the size of the (full) CBF dataset may be a bit too
limited, and reducing this dataset may harm the generalization capacity of SRNN.
The SC-ASS results obtained by MixDVAE with SRNN pre-trained at the three different data scales are
reported in Table 8. The experiments are conducted on the short sequence subset ( T= 50). We find
that, surprisingly, the separation performance of MixDVAE with SRNN-half on the speech signals has been
slightly improved over SRNN-full, whereas (much less surprisingly) the performance on the CBF signals has
decreased. This may be caused by the lower generalization ability of SRNN-half on the CBF dataset. For
SRNN-quarter, the performance of MixDVAE on both the speech and the CBF decrease, but the decrease
for the speech is quite moderate ( 0.4dB SI-SDR w.r.t. SRNN-full; the PESQ value is even slightly better),
whereas the CBF is loosing about 3.2dB SI-SDR. Again, even if it is difficult to draw a general conclusion
fromthissingleexperiment, thoseresultsseemtoindicatearelativerobustnessofMixDVAEtothelimitation
of the DVAE training dataset size, provided that the DVAE keeps a sufficient generalization capability.
I.2 Influence of the DVAE fine-tuning
As mentioned in Section 4.2, the DVAE model can either be fine-tuned or not in the MixDVAE algorithm.
We have studied the effect of fine-tuning SRNN on both MOT and SC-ASS tasks.
MOT task. Table 9 shows the MOT scores obtained by MixDVAE on the three test subsets with and
without the fine-tuning of SRNN in the E-Z step. We observe that for all three datasets, not fine-tuning
the DVAE model leads to the best overall performance (as measured by MOTA in particular). Though
fine-tuning the DVAE model can indeed increase the MOTP score and decrease the number of identity
switches, it does not improve the overall tracking performance. Indeed, fine-tuning increases the FP and FN
numbers/proportions, and thus decreases the MOTA scores. Especially on the long sequence dataset, the
MOTA score drops from 83.2to65.6.
SC-ASS task. Table 10 shows the performance of MixDVAE on the three test subsets with and without
fine-tuning SRNN in the E-Z step. Similar to the MOT task, for all three datasets, not fine-tuning SRNN
leads to the best overall source separation performance (on all of the evaluation metrics).
40Published in Transactions on Machine Learning Research (12/2023)
s 2 s 1 s 3 
s 2 s 1 s 3 
s 2 s 1 s 3 
s 2 s 1 s 3 
s 2 s 1 s 3 
s 2 s 1 s 3 
s 2 s 1 s 3 
s 2 s 1 s 3 
s 2 s 1 s 3 
s 2 s 1 s 3 
o 2 o 1 
o 2 o 1 o 3 
o 2 o 1 
o 2 o 1 
o 2 o 1 o 3 
o 2 o 1 
o 2 o 1 o 3 
o 2 o 1 o 3 
o 2 o 1 o 3 
o 2 o 1 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
t = 0t = 5t = 10t = 15t = 20t = 25t = 30t = 35t = 40t = 45Ground TruthDetectionWithout Fine-tuning
With Fine-tuning
(a) Example 1
s 2 s 1 s 3 
s 2 s 1 s 3 
s 2 s 1 s 3 
s 2 s 1 s 3 
s 2 s 1 s 3 
s 2 s 1 s 3 
s 2 s 1 s 3 
s 2 s 1 s 3 
s 2 s 1 s 3 
s 2 s 1 s 3 
o 2 o 1 o 3 
o 2 o 1 o 3 
o 2 o 1 o 3 
o 2 o 1 
o 2 o 1 
o 2 o 1 
o 2 o 1 
o 2 o 1 o 3 
o 2 o 1 o 3 
o 2 o 1 o 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
m 2 m 1 m 3 
t = 0t = 10t = 20t = 30t = 40t = 50t = 60t = 70t = 80t = 90Ground TruthDetectionWithout Fine-tuning
With Fine-tuning
(b) Example 2
Figure9: TwoexamplesoftrackingresultobtainedwiththeproposedMixDVAEalgorithm, withandwithout
fine-tuning during the E-Z step. For clarity of presentation, the simplified notations s1,o1, andm1denote
the ground-truth source position, the observation, and the estimated position, respectively (for Source 1,
and the same for the two other sources). Best seen in color.
Table 10: SC-ASS results obtained by MixDVAE with and without the fine-tuning of SRNN. The results
are reported for the short ( T= 50), medium ( T= 100) and long ( T= 300) test sequence subsets.
Dataset FinetuningSpeech Chinese bamboo flute
RMSE↓SI-SDR↑PESQ↑RMSE↓SI-SDR↑PESQ↑
ShortYes 0.007 8.00 1.63 0.007 12.73 2.15
No 0.006 9.23 1.73 0.007 13.50 2.30
MediumYes 0.008 8.00 1.55 0.008 12.23 2.02
No 0.007 9.32 1.65 0.007 13.05 2.16
LongYes 0.008 7.02 1.49 0.008 11.40 1.88
No 0.007 9.06 1.64 0.007 12.92 2.06
41Published in Transactions on Machine Learning Research (12/2023)
0 10 20 30 40 50 600.750.760.770.78
VEM iterationsMOTA values
0 10 20 30 40 50 60910111213
VEM iterationsFlute SI-SDR values
0 10 20 30 40 50 6002468
VEM iterationsSpeech SI-SDR values
Figure 10: Evolution of the performance of MixDVAE as a function of the number of VEM iterations (MOTA
score for the MOT task and SI-SDR scores for the SC-ASS task).
We therefore observe that for both tasks, fine-tuning the DVAE model results in performance degradation.
The possible reason is that fine-tuning could make the model more sensible to observation noise, and lead to
a generative model with worse performance. To verify this conjecture and to better understand the effect of
fine-tuning, we have plotted in Figure 9 two examples for the MOT task, extracted from the long sequence
test subset ( T= 300frames). To make possible the display of a long sequence in a limited space, the first
example is plotted every 5 frames, whereas the second example is plotted every 10 frames. In Example 1, we
observe that the detection for source s3is missed for t= 0,t= 10,t= 15,t= 25andt= 45. At these frames,
MixDVAE without SRNN fine-tuning can still make a good estimation of s3’s position, whereas MixDVAE
with SRNN fine-tuning can not make an accurate prediction. In the latter case, this caused a large error
between the estimated source position and the ground truth. We can see a similar phenomena in Example
2. At frame t= 30,t= 40,t= 50andt= 60, when the detection bounding box for source s3is absent,
the estimation obtained by MixDVAE with SRNN fine-tuning is bad (it is particularly bad for t= 40). This
phenomenon confirms our conjecture that the observation noise, particularly the lack of observations, can
introduce unforeseen effects during fine-tuning, resulting in a model with degraded performance.
I.3 Influence of the observation variance ratio
Table 4 reports the MOT scores obtained with MixDVAE as a function of rΦ. These experiments are
conducted on the subset of short sequences. We report the results for both with and without fine-tuning
SRNN in the E-Z step. Apart from the value of rΦand the fine-tuning option, all other conditions are
exactly the same across experiments. Table 4 shows that, whether fine-tuning SRNN in the E-Z step or
not, the MOT scores first globally increase with rΦ,12reach their optimal values for rΦ= 0.04or0.05
(for most metrics), and then decrease for greater rΦvalues. For confirmation, we have also computed the
averaged empirical ratio ˆrΦof the detected bounding boxes (with the SDP detector), which is calculated as
1
4T/summationtextT
t=11
Kt/summationtextKt
k=1(|sl
tk−ol
tk|
or
tk−ol
tk+|st
tk−ot
tk|
ot
tk−ob
tk+|sr
tk−or
tk|
or
tk−ol
tk+|sb
tk−ob
tk|
ot
tk−ob
tk).13This value equals to 0.053,0.053and0.047
respectively for the short, medium and long sequence dataset. These values, which are close to each other
because we used the same detector, correspond well to the rΦvalue for the best performing model in Table 4.
We can conclude that the model has better performance if the value of rΦcorresponds (empirically) to the
detector performance. Besides, we have also observed that the value of rΦhas an impact on the convergence
of the MixDVAE algorithm. Fig. 8 displays the MOTA score as a function of the number of MixDVAE
iterations (here with the fine-tuning of the DVAE model). It appears clearly that for too high values of rΦ,
the model exhibits a lower and more hectic performance than for the optimal value.
12Except for the MOTP score, which continually decreases with the increase of rΦ. This can be explained as follows. MOTP
measures the precision of the position estimation for the matched bounding boxes. The estimated position mtnin (28) is a
weighted combination of the observation and the DVAE prediction. When Φtkincreases, the contribution of the observation
decreases and mtnis closer to the DVAE prediction. Since the error of the DVAE prediction may accumulate over time, this
finally decreases the position estimation accuracy.
13Note that here stkdenotes the position of the target matched with the observation otkat time frame t. We omit the target
positions that are not matched with any observation.
42Published in Transactions on Machine Learning Research (12/2023)
Table 11: Averaged processing time per sequence for the MOT task.
Sequence length (frames) 60 120 300
# sources 3 6 3 6 3 6
Computation time per sequence (s) 23.01 57.29 45.05 110.41 112.93 272.94
Table 12: Pre-training computational cost on different datasets at different scales.
Task Data set Data scale One epoch training time (s)
MOT Synthetic trajectoriesFull 15
Half 7.8
Quarter 4.8
SC-ASS WSJ0Fall 190.8
Half 121.2
Quarter 63
J Discussion on the computational complexity
The proposed method is based on two parts: (i) the pre-training of a DVAE model on a single-source dataset,
and (ii) the MixDVAE variational EM algorithm for source tracking. The computational cost for the pre-
training stage mainly depends on the data type and data size of the single-trajectory dataset. To give a
general idea, we measured the average training time required for a single epoch (iteration over the whole
training set) on both the synthetic trajectories dataset for MOT and the WSJ0 dataset for SC-ASS. The
measurement is conducted on an NVIDIA Quadro RTX 8000, in a machine with an Intel(R) Xeon(R) Gold
5218R CPU @ 2.10GHz. The obtained results for different data scales as mentioned in Section I.1 is reported
in Table 12. We have observed that doubling the size of the training data results in almost a doubling of the
trainingtime. Ontheotherhand, thecomputationcomplexityoftheMixDVAEalgorithmmainlydependson
three factors: the number of VEM iterations, the number of sources to track and separate, and the sequence
length. Typically, the performance of MixDVAE exhibits an initial rapid increase over the VEM iterations,
followed by stabilization towards a plateau. In Figure 10, we plot the evolution of the averaged performance
of MixDVAE over the medium sequence test dataset as a function of the number of VEM iteration (the
performance is represented by the MOTA score for the MOT task and by the SI-SDR score for the SC-ASS
task). We observe that for the MOT task, the performance of MixDVAE has been stabilized from around
10 iterations, whereas for the SC-ASS task, the performance has been stabilized from around 20 iterations.
In practice, we run the algorithm for more iterations to guarantee the convergence. Taking computational
time optimization into account, it is possible to identify an optimal number of iterations by applying a grid
search, for a specific task and dataset. To quantify the computational time of the MixDVAE algorithm, we
compute the averaged processing time for one sequence on the MOT task, for the three considered values
of the sequence length, and for the case of 3 and 6 sources. This average processing time is measured on
an NVIDIA Quadro RTX 4000 GPU, in a machine with an Intel(R) Xeon(R) W-2145 CPU@3.70GHz, and
it is averaged on 10 test sequences. The results are reported in Table 11. We observe a linear increase of
the computation time as a function of the sequence length. As the number of sources to track doubles, the
computation time exhibits more than a twofold increase. The computation complexity can be a bottleneck
for the MixDVAE especially for long sequences with a large number of sources. However, further algorithm
and code optimization might be possible, since we did not focus on this aspect of the problem so far.
43