Under review as submission to TMLR
STC-ViT: Spatio Temporal Continuous Vision Transformer
for Weather Forecasting
Anonymous authors
Paper under double-blind review
Abstract
Operational weather forecasting system relies on computationally expensive physics-based
models. Recently, transformer based models have shown remarkable potential in weather
forecasting achieving state-of-the-art results. However, transformers are discrete and
physics-agnostic models which limit their ability to learn the continuous spatio-temporal
features of the dynamical weather system. We address this issue with STC-ViT , a Spatio-
Temporal Continuous Vision Transformer for weather forecasting. STC-ViT incorporates
the continuous time Neural ODE layers with multi-head attention mechanism to learn the
continuous weather evolution over time. The attention mechanism is encoded as a differ-
entiable function in the transformer architecture to model the complex weather dynam-
ics. Further, we define a customised physics informed loss for STC-ViT which penalize
the model’s predictions for deviating away from physical laws. We evaluate STC-ViT
against operational Numerical Weather Prediction (NWP) model and several deep learning
based weather forecasting models. STC-ViT, trained on 1.5-degree 6-hourly data, demon-
strates competitive performance compared to state-of-the-art data-driven models trained
on higher-resolution data for global forecasting. Our code and checkpoints are available at
https://anonymous.4open.science/r/STC-ViT-92C7
1 Introduction
OperationalweatherforecastingsystemsrelyonphysicsbasedNumericalWeatherPrediction(NWP)models.
While highly accurate, these system tend to be extremely slow with aggregated approximation errors and
requiring high computational resources (Palmer et al., 2005; Andersson, 2022).
Weather forecasting can be formulated as a continuously evolving physical phenomenon which is typically ex-
pressed as time-dependent partial differential equations (PDEs). These PDEs capture the dynamic evolution
of atmospheric variables over time, enabling the simulation of complex weather patterns and phenomena.
Encoding these physical priors leads to more interpretable and physically consistent forecasting models
(Couairon et al., 2024). Recent works have shown that using deep learning models as surrogates to model
complex multi-scale spatio-temporal phenomena can lead to training-efficient models (Gupta & Brandstetter,
2022). Vision Transformer by Dosovitskiy et al. (2020) has emerged one of such models (Lessig et al., 2023;
Nguyen et al., 2023b).
However, transformers only process discrete sequences of input data and are fundamentally discrete in
nature Fonseca et al. (2023) which limit their ability in modelling the continuous evolving dynamics of
weather system. Additionally, these models ignore the fundamental physical laws of our atmosphere which
is essential for faithful weather modelling. In this work, we address the non-continuity problem in Vision
Transformer (ViT) which arise in weather forecasting systems where data is present in discrete form. The
distinctive and continuously evolving characteristics of weather data pose significant challenge in producing
accurate forecasts. Additionally, we penalise the model for not following physical dynamics in the form a
customized physics informed loss function.
We build upon the core ideas of Neural Ordinary Differential Equation (NODE) by Chen et al. (2018) and
ViT (Dosovitskiy et al., 2020). We propose STC-ViT which leverages the continuous learning paradigm to
1Under review as submission to TMLR
effectively learn the complex spatio-temporal changes even from weather data recorded at coarser resolution.
The idea is to parameterize the attention mechanism by converting it into a differentiable function. Con-
tinuous temporal attention is calculated sample-wise and combined with the patch wise spatial attention to
learn the spatio-temporal mapping of weather variables in the embedding space of the vision transformer.
Furthermore, we add derivation as a pre-processing step to prepare the discrete data for continuous model
and explore the role of normalization in continuous modelling.
In this paper we focus on the following research question: How to design a computationally efficient data-
driven weather forecasting system that learns the continuous latent representation while respecting the fun-
damental laws of atmospheric physics . In summary, our contributions are as follows:
1. We propose spatio-temporal continuous attention to effectively learn and interpolate the spatio-
temporal information for weather forecasting.
2. We introduce derivation as a pre-processing step to ensure better feature extraction for continuous
spatio-temporal models.
3. We introduce physical constraints in our model via soft penalties in the form of a custom physics
informed loss based on three fundamental physical laws of thermodynamics, kinetic enrgy and po-
tential energy.
4. We perform extensive experiments on both WeatherBench and WeatherBench 2 to show the com-
petitive performance of STC-ViT against state-of-the-art forecasting models.
2 Background
2.1 Neural ODE
Neural ODEs are the continuous time models which learn the evolution of a system over time using Ordinary
DifferentialEquations(ODE)(Chenetal.,2018). ThekeyideabehindNeuralODEistomodelthederivative
of the hidden state using a neural network. Consider a hidden state h(t)at timetIn a traditional neural
network, the transformation from one layer to the next could be considered as moving from time ttot+ 1.
In Neural ODEs, instead of discrete steps, the change in h(t)over time is defined by an ordinary differential
equation parameterized by a neural network:
dh(t)
dt=f(h(t),t,θ) (1)
wheredh(t)
dtis the derivative of the hidden state with respect to time, fis a neural network with parameters
θandtis the time variable, allowing the dynamics of h(t)to change with time.
ResNets vs Neural ODEs To see the connection between ResNets and Neural ODEs, consider a ResNet
with layers updating the hidden state as:
ht+1=ht+f(ht,θt) (2)
In the limit, as the number of layers goes to infinity and the updates become infinitesimally small, this
equation resembles the Euler method for numerical ODE solving, where:
h(t+△t) =h(t) +△t·f(h(t),t,θ) (3)
Reducing△tto an infinitesimally small value transforms the discrete updates into a continuous model
described by the ODE given earlier. To compute the output of a Neural ODE, integration is used as a
backpropagation technique. This is done using numerical ODE solvers, such as Euler, Runge-Kutta, or more
sophisticated adaptive methods, which can efficiently handle the potentially complex dynamics encoded by
f.
2Under review as submission to TMLR
2.2 Physics Constrained Models
In weather and climate modeling, incorporating physical constraints ensures that the model adheres to the
governing physical laws. These constraints can be added in two ways: hard constraints and soft constraints.
Hard Constraints: For a given physical constraint f(x) = 0wheref(x)is the governing physical law, a
hard constraint would mean that the machine learning model’s prediction ˆymust always satisfy equation 4.
f(ˆy) = 0 (4)
This constraint can be embedded into model’s architecture as a constrained layer or optimizer.
Soft Constraints: Soft constraints adds a penalty term to the loss function that minimizes the violation
of a physical law f(ˆy)as shown in equation 5
minθ=L(y,ˆy) +α||f(ˆy)||2(5)
whereαcontrols the weight of the penalty for violating the physical constraint f(ˆy)and||f(ˆy)||2measures
the degree of violation (e.g., deviation from mass conservation).
3 Related Work
Integrated Forecasting System (IFS) ECMWF (2023) is the operational NWP based weather forecasting
system which generates forecasts at a high resolution of 1km. IFS combines data assimilation and earth
system model to generate accurate forecasts using high computing super computers. In contrast, data-
driven methodologies have now outperformed NWP models with much less computational complexities.
WeatherBench by Rasp et al. (2020) provides a benchmark platform to evaluate data-driven systems for
effective development of weather forecasting models. Current state-of-the-art data-driven forecasting models
are mostly based on Graph Neural Networks (GNNs) and Transformers. Keisler (2022) implemented a
message passing GNN based forecasting model which was further extended by GraphCast (Lam et al., 2023)
which used multi-mesh GNN to achieve state-of-the-art results. FourCastNet (FCN) Kurth et al. (2023)
used a combination of Fourier Neural Operator (FNO) and ViT and was reported to be 80,000 times faster
than NWP models. Several more transformer based models emerged including Pangu-Weather (Bi et al.,
2023), ClimaX (Nguyen et al., 2023a), FengWu (Chen et al., 2023a), FuXi (Chen et al., 2023b) and Stormer
(Nguyen et al., 2023b) all showcased remarkable capabilities for short to medium range forecasting.
While being highly accurate and showcasing remarkable scaling capabilities, these models are discrete space-
time models and do not account for the continuous dynamics of weather system. Recently, Verma et al.
(2023) proposed ClimODE which used Neural ODE to incorporate a continuous-time process that models
weather evolution and advection, enabling it to capture the dynamics of weather transport across the globe
effectively. However it currently yields less precise forecast results compared to state-of-the-art models,
offering significant potential for further enhancements. Further, Kochkov et al. (2023) proposed Neural
GCM, which integrates a differentiable solver with neural networks resulting in physically consistent models.
4 Methodology
Problem Formulation. Consider a model freceives weather data as input of the form XN×W×Hat time
tand predicts the weather information at time t+△tas shown in equation 6 where Nis the number of
weather variables such as temperature, wind speed, pressure, etc. and H×Wrefers to the spatial resolution
of the variable.
The objective of the model is to learn the continuous temporal dependencies while accounting for spatial
correlations within the H×Wgrid. Since the weather changes continuously over time, it is essential to
capture the continuous change within the provided fixed time step data. The main aim of STC-ViT is to to
learn the continuous latent representation of the weather data using Spatio-Temporal Continuous Attention
and Neural ODEs. The evolution of the weather system from ttot+△tcan be represented as:
X(t+△t) =f(X,t) (6)
3Under review as submission to TMLR
dX(t)
dt=f(X,t,θ )
/integraltextt+△t
tdX(t)
dt=/integraltextt+△t
tf(X,t,θ )
X(t+△t) = [f(X,t,θ )]t+△t
tθ=learnable model parameters
Notation: Throughout the paper tandt−1is used to denote the information at current and previous time
step respectively. Vi(x,y,t )is used to denote weather variable with x and y dimensions at current time step
t.
Derivation as a pre-processing step. Weather information is highly variational and complex at both
temporal and spatial levels. Temporal derivatives of each weather variable are calculated to preserve weather
dynamics and ensure better feature extraction from discretized data. We perform sample wise derivation at
pixel level to capture the continuous changes in weather events over time as shown in equation 7.
dV(x,y,t )
dt=V(x,y,t )−V(x,y,t−1)
∆t(7)
whereV(x,y,t )is pixel value of weather variable at time t and V(x,y,t−1)is the pixel value of same
variable at t-1.
4.1 Spatio-Temporal Continuous Vision Transformer
In the STC-ViT architecture, we enhance vision transformer-based weather forecasting by introducing Tem-
poral Continuous Attention (TCA) and Spatial Attention (SA) mechanisms to capture dynamically evolving
weather patterns. We build upon the variable tokenization and aggregation scheme proposed by (Nguyen
et al., 2023a) followed by Continuous attention mechanism to enhance interpolated feature learning. The
detailed architecture of STC-ViT is shown in figure 2.
Temporal Continuous Attention (TCA). We model the temporal dynamics with attention by incorpo-
rating derivatives (temporal changes) over time directly into the query, key, and value representations.
We formulate Query (Q) as the "current" time step’s information and is designed to seek relevant patterns
or changes reflecting the model’s focus on upcoming changes equation 8.
Qt=dVt
dt(8)
Key (K) models the "context" of prior time step states, which provide historical dynamics incorporating
past time step derivatives equation 9.
Kt−1=f(Vt−1,dV
dt|t−1) (9)
Value (V) represents the updated information allowing the model to interpret them in the context of
previous and current changes. This modification allows the model to learn the transitional changes from one
time step to another which is important for capturing unprecedented changes in weather.
To compute TCA and capture the temporal change for the same variable, we calculate the attention between
Qt,iandKt−1,ifocusing on the temporal evolution. The attention mechanism to capture the temporal
continuity between time steps tandt−1is given by:
d
dt(Qt,i·Kt−1,i) =Qt,id(Kt−1,i)
dt+Kt−1,id(Qt,i)
dt(10)
Yi=Qt,id(Kt−1,i)
dt+Kt−1,id(Qt,i)
dt(11)
Att(Qt,i,Kt−1,i) =Softmax (Yi√dk) (12)
4Under review as submission to TMLR
TCAt,t−1=N/summationdisplay
k=1Att(Qt,i,Kt−1,i)·Vt−1,i (13)
The resulting output represents the attention weighted sum of values for similar variables across input
samples at time t0andt1. This approach models the dynamics of each variable independently over time,
allowing the model to capture temporal dependencies and changes effectively.
Spatial Attention (SA). For each variable i, we calculate the attention between spatial positions (x,y)
and(x′,y′)given by equation 15:
Attention (Qx,y,i,Kx′,y′,i) =softmax (Qx,y,i·Kx′,y′,i√dk) (14)
SAt,i=/summationdisplay
x′,y′Attention (Qx,y,i,Kx′,y′,i)·Vx′,y′,i (15)
where Q, K are matrices formed from all queries, keys, and values, respectively, dkis the dimensionality of
the keys and queries, and the division by√dkis a scaling factor to deal with gradient vanishing problems
during training. Use the spatial attention weights to obtain the spatially-enhanced representation for each
variable i by weighting the values:
Concatenation and Fusion. We concatenate the outputs of the TCA and SA along the feature dimension
(as shown in equation 16) to provide a comprehensive representation, considering both temporal and spatial
dependencies. This dual attention approach allows the Vision Transformer to effectively model complex
spatio-temporal dependencies in the weather reanalysis data. It captures both the temporal evolution of
each variable over time and the spatial interactions within each time step, enhancing the model’s ability
to understand the continuous, spatiotemporal dynamics of the data. The outputs of temporal and spatial
attention are concatenated and projected through an output layer which forms the input for the Neural ODE
component.
Attention (h) =concat (TCAt,i,SAt,i) (16)
Neural ODE Integration. To enable continuous transformations, the output of the attention mechanism
is treated as the initial state of a Neural ODE. The evolution of this state is governed by equation 17:
dh(t)
dt=f(h(t),t,θ) (17)
wherefis a learnable function parameterized as a multi-layer perceptron (MLP). The architecture of Neural
ODE layer is shown in Figure 1
Neural 
ODE 
LayerIntegrate using Runge Kutta Solver
ReLU
ActivationLinear
LayerLinear
Layer
Figure 1: Neural ODE layer architectureThe Neural ODE evolves the input state h(t)contin-
uously between t= 0andt= 1as equation 18. The
integration is performed numerically using the odeint
functionwithRunge-Kutta(rk4)Runge(1895)numer-
ical ODE solver, which evaluates f(h(t),t,θ)multiple
times for precision.
h(t1) =h(t0) +/integraldisplayt1
t0f(h(t),t,θ)dt(18)
The following equation 19 provides output for each
lead time:
hn+1=hn+ODEBlock (LayerNorm (Attention (hn))) +ODEBlock (LayerNorm (MLP (hn)))(19)
5Under review as submission to TMLR
The model leverages Neural ODEs to capture smoother, continuous transformations. While this formulation
aligns with continuous-depth modeling principles, the practical implementation focuses on fixed-interval
evaluations rather than arbitrary time points.
4.2 Latitude Weighted Physics Informed Loss Function
We use latitude weighted mean squared error to compute loss for the predicted variables.
Llat−weight =1
N×H×WN/summationdisplay
n=1H/summationdisplay
i=1W/summationdisplay
j=1(Li)(ˆXn,i,j
t+∆t−Xn,i,j
t+∆t) (20)
whereLiaccounts for Latitude weights:
L(i) =cos(lat(i))
1
H/summationdisplayH
i′=1cos(lat(i′))
Additionally, we account for three fundamental physical laws i.e. Potential Energy, Kinetic Energy and
Thermodynamic Balance in our loss function. Kinetic energy in atmospheric science refers to the energy
associated with the motion of air masses. Geopotential is used in meteorology to express the potential energy
of an air parcel in the Earth’s gravitational field. Thermodynamic balance equation describes the evolution
of temperature in an air parcel due to processes like heat addition and pressure changes. These fundamental
principles, coupled with wind components and thermodynamic variables like temperature and geopotential,
form the core of atmospheric dynamics used in climate and weather models.
Lkinetic =/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
2(u2
pred+v2
pred)−1
2(u2
true+v2
true)/vextendsingle/vextendsingle/vextendsingle/vextendsingle(21)
where u is the eastward wind component (m/s) and v is the northward wind component (m/s).
Lpotential =|g·zpred−g·ztrue| (22)
where g is the gravitational acceleration constant i.e. 9.81and z geopotential m2/s2. Since z already reflects
the potential energy, here it is used as a weighting mechanism to ensure that the loss calculation emphasizes
discrepancies in regions with higher potential energy, thereby aligning the predictions more closely with the
physical significance of geopotential.
Lthermo =/vextendsingle/vextendsingle/vextendsingle/vextendsingledT
dt+udT
dx+vdT
dy/vextendsingle/vextendsingle/vextendsingle/vextendsingle(23)
wheredT
dtis the temporal change of temperature (model output), udT
dx,vdT
dyrepresent the advection of tem-
perature by wind in the x and y directions, respectively.
Lossphysics =Llat−weight +α×Lkinetic +β×Lpotential +γ×Lthermo (24)
whereα,βandγare the weight factors for physics based loss. The resulting the loss function is inspired
by physical principles, however it does not enforce strict physical laws but instead guides the model toward
outputs that are consistent with physical expectations.
5 Experiments and Results
5.1 Dataset
We train STC-ViT on ERA5 dataset Hersbach et al. (2018; 2020) provided by the European Center for
Medium-Range Weather Forecasting (ECMWF). We compare STC-ViT against several weather forecasting
models by training it at two different resolutions of 5.625◦(32 x 64 grid points) provided by WeatherBench
(Rasp et al., 2020) and 1.5◦(121 x 240 grid points) provided by WeatherBench2 (Rasp et al., 2024).
6Under review as submission to TMLR
Pre-Processing
Temperature at t-1 Temperature at t
xi,yi,t-1 xi,yi,t
Input W eather Data
N x W x H
Output W eather Data
N x W x HVariable Tokenization 
and  Aggregation
Temporal Continuous
AttentionSpatial 
AttentionConcatenateLayer NormalizationNeural 
ODE 
Layer
Layer Normalization
Multi layer 
PerceptronNeural 
ODE 
LayerTransformer Block
q,k,v at t0q,k,v at t1Calculating Dif ferential Coef ficient at same spatial position,
at dif ferent time points
Runge Kutta 
Solver
NormalizationLinear Embedding
Derivative Calculation
Latitude W eighted Physics Informed RMSE
Kinetic
Energy
LossPotential
Energy
LossThermo-
dynamic
Balance
LossMinimize
Loss
Figure2: Overallpredictionpipelineof STC-ViT asthemodelreceivesspatio-temporalweatherinformation
and passed through pre-derivation and normalization steps and input to transformer encoder where TCA
and SA learns the continuous weather features and outputs the prediction
5.2 Training Details.
We consider weather forecasting as a continuous spatio temporal forecasting problem i.e a tensor of shape
N×H×Wat timetis fed to the pre-processing layer of the model where it passes through pre-derivation
to STC-ViT and outputs a tensor of N′×H′×W′at future time step t+△t. Complete training details of
the model are given in the in Appendix A.
5.3 WeatherBench
We train STC-ViT on hourly data with following set of variables: Land Sea Mask (LSM), Orography, 10-
meterUandVwindcomponents(U10andV10)and2-metertemperature(T2m)inadditionto6atmospheric
variables: geopotential (Z), temperature (T), U and V wind components, specific humidity (Q) and relative
humidity (R) at 7 pressure levels: 50 250 500 600 700 850 925. We use data from 1979-2015 for training,
2016 for validation and 2017-2018 for testing phase. We compare STC-ViT with ClimaX, and ClimODE on
ERA5 dataset at 5.625◦resolution provided by WeatherBench (Rasp et al., 2020). To ensure fairness, we
retrained ClimaX from scratch without any pre-training.
STC-Vit outperforms ClimaX and ClimODE at all lead times which shows that replacing regular attention
with continuous attention in ViT architecture derives improved feature extraction by mapping the changes
occurring between successive time steps. Additionally, enforcing physical constraints in the model lead to
improved prediction scores. The RMSE and ACC results are shown in Table 1.
5.4 WeatherBench2
To keep training consistent with WeatherBench 2, we utilize the training data from 1979 to 2018, validation
data from 2019, and test data from 2020 year. We train STC-ViT on 6 hourly data for following variables:
7Under review as submission to TMLR
Table 1: RMSE and ACC results of STC 5.6compared against ClimODE (ODE based) and ClimaX (non-
pretrained) trained on ERA5 at 5.625◦resolution
RMSE (Lower is better) ACC (Higher is better)
Variable Lead Time (hrs.) STC-ViT ClimODE ClimaX STC-ViT ClimODE ClimaX
z500 (m2\s2)6 78.36 102.2 249 0.99 0.99 0.97
12 99.42 132.7 265.3 0.99 0.99 0.96
18 117.16 163 319.8 0.99 0.98 0.95
24 141.45 193.4 455 0.98 0.98 0.89
36 206.59 259.6 455 0.97 0.96 0.89
U10(m\s)6 0.92 1.44 1.58 0.97 0.91 0.92
12 1.11 1.80 1.96 0.96 0.89 0.88
18 1.28 1.97 2.24 0.95 0.88 0.84
24 1.46 2.00 2.49 0.93 0.87 0.80
36 1.89 2.25 2.95 0.89 0.83 0.70
V10(m\s)6 0.95 1.53 1.60 0.97 0.92 0.92
12 1.15 1.81 1.97 0.96 0.89 0.88
18 1.32 1.95 2.26 0.94 0.88 0.83
24 1.50 2.02 2.48 0.93 0.86 0.80
36 1.92 2.29 2.94 0.88 0.83 0.70
T2m (K)6 0.87 1.20 2.02 0.98 0.97 0.92
12 1.04 1.44 2.26 0.97 0.96 0.90
18 1.13 1.42 2.45 0.97 0.96 0.88
24 1.18 1.40 2.37 0.97 0.96 0.89
36 1.42 1.70 2.85 0.96 0.94 0.84
T850 (K)6 0.83 1.16 1.64 0.98 0.97 0.94
12 0.99 1.32 1.77 0.97 0.96 0.93
18 1.09 1.47 1.93 0.97 0.96 0.92
24 1.19 1.55 2.17 0.97 0.95 0.90
36 1.44 1.75 2.49 0.95 0.94 0.86
u50(m\s)6 1.42 2.01 2.05 0.98 0.92 0.92
12 1.75 2.56 2.68 0.98 0.91 0.90
18 1.90 2.89 2.92 0.96 0.90 0.89
24 2.06 2.97 3.01 0.96 0.90 0.89
36 2.37 3.03 3.12 0.94 0.88 0.87
v50(m\s)6 1.45 2.01 2.12 0.95 0.90 0.90
12 1.73 2.20 2.32 0.94 0.89 0.88
18 1.89 2.27 2.31 0.92 0.87 0.85
24 2.00 2.35 2.41 0.91 0.87 0.84
36 2.19 2.51 2.59 0.90 0.85 0.83
T2m, u10 and v10 wind components and mean sea-level pressure (MSLP) along with five atmospheric
variables: geopotential height (Z), temperature (T), U and V wind components, and specific humidity (Q).
These atmospheric variables are considered at 13 pressure levels: 50, 100, 150, 200, 250, 300, 400, 500, 600,
700, 850, 925, 1000 hPa. We also compare our results with both versions of IFS Andersson (2022), IFS-HRES
which is state-of-the-art forecasting model at high resolution of 0.1◦and IFS-ENS, ensemble version trained
at0.2◦. Additionally, we compare STC-ViT against GraphCast and PanguWeather which are trained at a
higher resolution of 0.25◦and finally with Neural GCM trained at 0.7◦.
8Under review as submission to TMLR
STC-ViTshowscompetitiveperformance, outperformingPanguandGraphCastparticularlyforgeopotential,
temperature and wind variables. This is due to the physics loss penalizing the model for not obeying
physics and encouraging outputs that balance temperature evolution and respect thermodynamic laws by
penalizing deviations from these energy fluxes. We find out that Neural GCM is the best performing model
on atmospheric variables which could be due to the physics embedded in the dynamical core. We believe
that STC-ViT could also benefit from high resolution training and physics directly incorporated in the model
architecture which we will explore in future. The RMSE and ACC results are shown in Figures 3 and 5
respectively.
2 4 6 8100200300400500600700
z500 (m²/s²)
2 4 6 80.51.01.52.02.53.03.5
T850 (k)
2 4 6 80.500.751.001.251.501.752.00
Q500 (g/kg)
2 4 6 80.51.01.52.02.5
T2m (k)
2 4 6 80.51.01.52.02.53.03.54.0
u10 (m/s)
2 4 6 81234
v10 (m/s)
2 4 6 82468
v500 (m/s)
2 4 6 812345678
u500 (m/s)
STC-ViT GraphCast PanguWeather IFS-HRES IFS-ENS GCMLead Time (DAYS)RMSE
Figure 3: RMSE comparison of STC-ViT trained at 1.5◦with GraphCast, PanguWeather, Neural GCM,
IFS-ENS at 0.2◦and IFS-HRES at 0.1◦resolution data for lead times ranging from 1 to 10 days
5.5 Ablation Studies
5.5.1 STC-ViT Component Analysis
Vanilla Vision Transformer Vision Transformer has emerged as a powerful architecture which captures
long-term dependencies better than any model. For this ablation study, we simply train a basic ViT archi-
tecture on ERA5 dataset. Compared with STC-ViT, ViT under performs for prediction accuracy showing
the superiority of continuous models in weather forecasting systems.
Vanilla Neural ODE Network. Another ablation study is done using vanilla Neural ODE model. We
replace the transformer with Neural ODE architecture as proposed in the original paper Chen et al. (2018).
While Neural ODEs are computationally efficient, they only aid in interpolating the temporal irregularities
and ignore the spatial continuity. This study proves that STC-ViT learns both spatio-temporal continuous
features from the discrete data and is better at representing dynamical forecast systems.
Continuous Attention. To understand the importance of continuous attention, we remove the neural ODE
layer and provide the output of attention mechanism to the basic feed forward network. While continuous
attention only model does not outperform the STC-ViT, it provides better prediction accuracy than the
traditional attention model.
9Under review as submission to TMLR
2 4 6 80.60.70.80.91.0
z500 (m²/s²)
2 4 6 80.50.60.70.80.91.0
T850 (k)
2 4 6 80.40.50.60.70.80.9
Q850 (g/kg)
2 4 6 80.50.60.70.80.91.0
T2m (k)
2 4 6 80.40.50.60.70.80.91.0
u10 (m/s)
2 4 6 80.40.50.60.70.80.91.0
v10 (m/s)
2 4 6 80.40.50.60.70.80.91.0
v500 (m/s)
2 4 6 80.50.60.70.80.91.0
u500 (m/s)
STC-ViT GraphCast PanguWeather IFS-HRES IFS-ENS Neural-GCMLead Time (DAYS)ACC
Figure 4: ACC comparison of STC-ViT trained at 1.5◦with GraphCast, PanguWeather, Neural GCM, IFS-
ENS at 0.2◦and IFS-HRES at 0.1◦resolution data for lead times ranging from 1 to 10 days
Continuous Attention + Neural ODE layer. We perform another ablation study to compare how well
Neural ODE capture continuity in weather information when simply added as a layer in the transformer
architecture. Neural ODE Chen et al. (2018) is a continuous depth neural network designed to learn con-
tinuous information in the process as well. For this study, we replace the continuous attention with vanilla
attention and add a Neural ODE layer in the feed forward block. The feed forward solution is approximated
using Runge-Kutta (RK) numerical solver.
2 4 6 8200400600800
z500 (m²/s²)
2 4 6 81.01.52.02.53.03.54.0
T850 (k)
2 4 6 81.01.52.02.53.03.5
T2m (k)
2 4 6 81234
u10 (m/s)
Cont-Att ViT-Node ViT NODELead Time (DAYS)RMSE
Figure 5: Ablation studies highlight the results for different components of STC-ViT individually and show
that the continuous attention component alone performs better than all other even NODE. Thus combined
with NODE outperforms state-of-the-art results
5.5.2 STC-ViT Loss Analysis
We further performed ablation studies to understand the effect of each loss function on the predictions. The
results are shown in figure 6.
10Under review as submission to TMLR
Physics Uninformed - Latitude Weighted RMSE: WetrainedSTC-ViTsolelyusingLatitudeWeighted
RMSE without including any physical loss terms. We observe that model’s performance got worse without
considering the physical laws in the loss function.
Potential Energy Loss (PE): To understand the role of PE loss, we assign the highest weight (0.8) to the
potential energy loss function and keep the weights of Kinetic Energy and Thermodynamic loss to 0.1. We
observe that it has the lowest influence on the predictions.
Kinetic Energy Loss (KE): We repeated the above study by assigning the highest weight (0.8) to KE
loss and set the weights of PE and Thermodynamic loss equal to 0.1. The results suggest that its influence
on the overall prediction accuracy is more as compared to Potential Energy Loss.
Thermodynamic Loss: Finally, we assign the highest weight to the Thermo Loss (0.8) while keeping the
weights of PE and KE loss to negligible 0.1. We observe that Thermo loss has the highest influence on
overall prediction accuracy resulting in the lowest RMSE as compared to all other loss terms.
10 20 30200400600800
z500 (m²/s²)
10 20 301.01.52.02.53.03.54.0
T850 (k)
10 20 301.01.52.02.53.03.5
T2m (k)
10 20 301234
u10 (m/s)
Thermodynamic Loss Kinetic Loss Geopotential Loss Latitude Wegihted RMSE (No Physics)Lead Time (hours)RMSE
Figure 6: Ablation studies highlight the results for different loss terms of STC-ViT individually and show
that the Thermodynamic Loss has the largest effect on the resulting RMSE.
6 Conclusion and Future Work
In this paper, we present STC-ViT, a novel technique designed to capture continuous dynamics of weather
data while obeying the fundamental physical laws of Earth’s atmosphere. STC-ViT achieves competitive
results in weather forecasting which shows that vision transformers can model continuous nature of spatio-
temporal dynamic systems with carefully designed attention mechanism.
While STC-ViT performs competitively with state-of-the-art data-driven weather forecasting models, it is
important to address its limitations in weather forecasting systems. STC-ViT is inherently based on trans-
former architecture which has a limitation of having higher training times when scaled to higher resolutions
as shown in Table 2. Further the deterministic nature of our approach does not account for uncertainties
whichcanproduceunrealisticresults. Additionally, predictionsonlongerleadtimesresultinblurryforecasts.
Extending STC-ViT to a probabilistic model can be addressed in future works.
It is also important here to address the biases resulting from training on one dataset. The ERA5 dataset,
while a robust and widely used weather dataset also has inherent limitations. Potential biases in data-sparse
regions, challenges in representing local phenomena, and inconsistencies in observational continuity may
impact model generalizability. To address this, future work will explore: data augmentation to synthetically
increase dataset diversity. Integration with Diverse Datasets, such as regional or event-specific datasets, to
mitigate geographical and climatic biases. Also, scaling STC-ViT to better accommodate the multi-modal
high resolution training and evaluation presents an opportunity as future research. Finally, addressing the
black-box problem of STC-ViT can shed light on model learning insights which is equally important for
climate science community.
11Under review as submission to TMLR
Table 2: Run-Time comparison against several data-driven weather forecasting models
Model Parameters Training Time Train Device
PanguWeather 256M 64 days 192 NVIDIA Tesla-V100 GPUs
GraphCast 37M 4 weeks 32 TPUs
ViT (ClimaX-non pretrained) 107M 2 days 8 V100
STC-ViT (5.625/1.5 degree) 98M 2days/20days 4 V100/2 A100
Societal Impact
Our research focuses on modelling the continuous dynamics of weather forecasting system through the
integration of deep learning (DL) techniques. The study shows that leveraging data-driven approaches can
achieve significantly improved forecast results with compute efficient resources. The environmental benefits
of compute-efficient forecasting systems are significant. Lowering the carbon footprint of computational
processes contributes to global efforts in combating climate change. By integrating ML to improve accuracy
while optimizing computational efficiency, we can create a sustainable and inclusive approach to weather
forecasting that serves the global community more effectively. Additionally, training STC-ViT at higher
resolution on more diverse datasets and focusing on high resolution regional forecasts as a future work
can directly contribute to enhanced climate resilience, enabling societies to better anticipate and adapt to
extreme weather events such as hurricanes, droughts, and floods. Further, extending the study to cater
for longer lead times of few months and seasonal can help in disaster preparedness, as timely and precise
forecasts allow governments and organizations to implement early warning systems, plan evacuations, and
allocate resources effectively, thereby mitigating loss of life and property.
References
Erik Andersson. Medium-range forecasts. 2022.
Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Accurate medium-range
global weather forecasting with 3d neural networks. Nature, pp. 1–6, 2023.
Kang Chen, Tao Han, Junchao Gong, Lei Bai, Fenghua Ling, Jing-Jia Luo, Xi Chen, Leiming Ma, Tianning
Zhang, Rui Su, et al. Fengwu: Pushing the skillful global medium-range weather forecast beyond 10 days
lead.arXiv preprint arXiv:2304.02948 , 2023a.
Lei Chen, Xiaohui Zhong, Feng Zhang, Yuan Cheng, Yinghui Xu, Yuan Qi, and Hao Li. Fuxi: a cascade
machine learning forecasting system for 15-day global weather forecast. npj Climate and Atmospheric
Science, 6(1):190, 2023b.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential
equations. Advances in neural information processing systems , 31, 2018.
Guillaume Couairon, Christian Lessig, Anastase Charantonis, and Claire Monteleoni. Archesweather: An
efficient ai weather forecasting model at 1.5 {\deg}resolution. arXiv preprint arXiv:2405.14527 , 2024.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.
ECMWF. Ifs documentation cy48r1. ecmwf. 2023.
William A Falcon. Pytorch lightning. GitHub, 3, 2019.
Antonio H de O Fonseca, Emanuele Zappala, Josue Ortega Caro, and David Van Dijk. Continuous spa-
tiotemporal transformers. arXiv preprint arXiv:2301.13338 , 2023.
12Under review as submission to TMLR
Jayesh K Gupta and Johannes Brandstetter. Towards multi-spatiotemporal-scale generalized pde modeling.
arXiv preprint arXiv:2209.15616 , 2022.
H Hersbach, B Bell, P Berrisford, G Biavati, A Horányi, J Muñoz Sabater, J Nicolas, C Peubey, R Radu,
I Rozum, et al. Era5 hourly data on single levels from 1959 to present [dataset]. copernicus climate change
service (c3s) climate data store (cds), 2018.
Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, András Horányi, Joaquín Muñoz-Sabater, Julien
Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, et al. The era5 global reanalysis. Quarterly
Journal of the Royal Meteorological Society , 146(730):1999–2049, 2020.
Stephan Hoyer and Joe Hamman. xarray: Nd labeled arrays and datasets in python. Journal of Open
Research Software , 5(1):10–10, 2017.
Ryan Keisler. Forecasting global weather with graph neural networks. arXiv preprint arXiv:2202.07575 ,
2022.
Dmitrii Kochkov, Janni Yuval, Ian Langmore, Peter Norgaard, Jamie Smith, Griffin Mooers, James Lottes,
Stephan Rasp, Peter Düben, Milan Klöwer, et al. Neural general circulation models. arXiv preprint
arXiv:2311.07222 , 2023.
Thorsten Kurth, Shashank Subramanian, Peter Harrington, Jaideep Pathak, Morteza Mardani, David Hall,
Andrea Miele, Karthik Kashinath, and Anima Anandkumar. Fourcastnet: Accelerating global high-
resolution weather forecasting using adaptive fourier neural operators. In Proceedings of the Platform for
Advanced Scientific Computing Conference , pp. 1–11, 2023.
Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet,
Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, et al. Learning skillful medium-range global
weather forecasting. Science, pp. eadi2336, 2023.
Christian Lessig, Ilaria Luise, Bing Gong, Michael Langguth, Scarlet Stadtler, and Martin Schultz. Atmorep:
A stochastic model of atmosphere dynamics using large scale representation learning. arXiv preprint
arXiv:2308.13280 , 2023.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 ,
2017.
Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K Gupta, and Aditya Grover. Climax: A
foundation model for weather and climate. arXiv preprint arXiv:2301.10343 , 2023a.
Tung Nguyen, Rohan Shah, Hritik Bansal, Troy Arcomano, Sandeep Madireddy, Romit Maulik, Veerabhadra
Kotamarthi, Ian Foster, and Aditya Grover. Scaling transformer neural networks for skillful and reliable
medium-range weather forecasting. arXiv preprint arXiv:2312.03876 , 2023b.
TN Palmer, GJ Shutts, R Hagedorn, FJ Doblas-Reyes, Thomas Jung, and M Leutbecher. Representing
model uncertainty in weather and climate prediction. Annu. Rev. Earth Planet. Sci. , 33:163–193, 2005.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
learning library. Advances in neural information processing systems , 32, 2019.
StephanRasp, PeterDDueben, SebastianScher, JonathanAWeyn, SoukaynaMouatadid, andNilsThuerey.
Weatherbench: abenchmarkdatasetfordata-drivenweatherforecasting. Journal of Advances in Modeling
Earth Systems , 12(11):e2020MS002203, 2020.
Stephan Rasp, Stephan Hoyer, Alexander Merose, Ian Langmore, Peter Battaglia, Tyler Russell, Alvaro
Sanchez-Gonzalez, Vivian Yang, Rob Carver, Shreya Agrawal, et al. Weatherbench 2: A benchmark for
the next generation of data-driven global weather models. Journal of Advances in Modeling Earth Systems ,
16(6):e2023MS004019, 2024.
13Under review as submission to TMLR
Carl Runge. Über die numerische auflösung von differentialgleichungen. Mathematische Annalen , 46(2):
167–178, 1895.
Yogesh Verma, Markus Heinonen, and Vikas Garg. Climode: Climate forecasting with physics-informed
neural odes. In The Twelfth International Conference on Learning Representations , 2023.
A Experiment Details
A.1 Evaluation Metrics.
We used Root Mean Square Error (RMSE), Anomaly Correlation Coefficient (ACC) and Mean Absolute
Error (MAE) to evaluate our model predictions. The formula used for RMSE is:
RMSE =1
NN/summationdisplay
k=1/radicaltp/radicalvertex/radicalvertex/radicalbt1
H×WH/summationdisplay
i=1W/summationdisplay
j=1L(i)(ˆXk,i,j−Xk,i,j)2 (25)
whereH×Wis the spatial resolution of the weather input and N is the number of total samples used for
training or testing. L(i) is used to account for non-uniformity in grid cells.
ACC =/summationtext
k,i,jˆX′k,i,jX′
k,i,j/radicalig/summationtext
k,i,jL(i)ˆX′2
k,i,j/summationtext
k,i,jL(i)X′2
k,i,j(26)
Where ˆX′=ˆX′−CandX′=X′−Cand C is the temporal mean of the entire test set C=1
N/summationtext
kX
MAE =1
NN/summationdisplay
k=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
H×WH/summationdisplay
i=1W/summationdisplay
j=1L(i)(ˆXk,i,j−Xk,i,j)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(27)
A.2 Optimization
WeusetheAdamWoptimizerLoshchilov&Hutter(2017)withalearningrateof5e-5. WeutilizeaCosineAn-
nealing learning rate scheduler is adopted which progressively lowers the learning rate to zero following the
warm-up period which is 10% of the total epochs. We train STC-ViT for 50 epochs for 5.625◦and 100
epochs for 1.5◦resolution data .
A.3 Hyperparameters
A.4 Normalization
In our experiments, we normalize all inputs during training and re-scale them to their original range before
computing final predictions. We perform z-score normalization for every variable, at each atmospheric
pressure level, we calculate the mean and standard deviation to standardize them to zero mean and unit
variance.
A.5 Software and Hardware Requirements
We use PyTorch Paszke et al. (2019), Pytorch Lightning Falcon (2019), torchdiffeq Chen et al. (2018), and
xarray Hoyer & Hamman (2017) to implement our model. We use 2 NVIDIA DGX A100 devices with
80GB RAM for training STC-ViT at 1.5◦and 4 NVIDIA Tesla Volta V100-SXM2-32GB for the training at
resolution of 5.625◦.
14Under review as submission to TMLR
Table 3: Hyperparameters of STC-ViT
Hyperparameters Meaning Value
p Patch size 2
Heads Number of continuous attention heads 16
Depth Number of Transformer layers 4
Dimension Hidden dimensions 1024
Dropout Dropout rate 0.1
batch_size Batch Size 12 for 5.625◦, 2 for 1.5◦
β1 First Moment Decay Rate of AdamW optimizer 0.9
β2 Second Moment Decay Rate of AdamW optimizer 0.999
ES Early stopping True
ES rate Early stopping tolerance 10
α Kinetic Loss weight factor 0.1-0.5
β Potential Loss weight factor 0.1-0.5
γ Thermodynamic Loss weight factor 0.8
B Results
B.1 Quantitative Results
Table 4: RMSE for sub seasonal forecasts ranging from 2 weeks to 8 weeks
Variable 2 weeks 4 weeks 6 weeks 8 weeks
z500 (m2/s2)825.68 857.09 968.5 1068.5
T2m (k) 3.68 4.31 5.24 6.35
T850 (k) 3.67 4.05 4.82 5.54
u10 (m/s) 4.00 4.05 4.23 4.45
v10 (m/s) 4.07 4.13 4.28 4.42
Table 5: Mean Absolute Error (lower is better) of STC 5.6
Variable 6 hr 12 hr 18 hr 1 day 3 day 6 day 2 week 4 week 6 week
z500 (m2\s2)60.33 72.33 84.79 98.67 351.34 519.87 529.97 557.38 722.47
u10(m\s)0.65 0.79 0.88 1.00 2.30 2.83 2.85 2.92 3.11
v10(m\s)0.67 0.81 0.91 1.03 2.36 2.89 2.92 2.99 3.16
T2m (K) 0.61 0.72 0.78 0.81 1.44 2.26 2.39 2.77 4.15
T850 (K) 0.60 0.726 0.79 0.86 1.81 2.53 2.63 2.90 3.96
Spatial Resolution Analysis
When analyzing inference RMSE across different spatial resolutions as shown in figure 7, we observe that
STC-ViT trained at 1.5-degree spatial resolution offers significant improved prediction accuracy, however,
as the lead time increases the gap between the RMSE values decreases which shows that models trained on
5.625 degree can offer better insights into longer sub seasonal forecasts and STC-ViT trained on 1.5 degree
can be used to generate localized high resolution forecast for nowcasting and extreme event forecasting.
In conclusion, there is definitely a trade-off between resolution and accuracy across different forecasting
scenarios and computational constraints.
15Under review as submission to TMLR
1 3 5 7 90200400600800z500 (m²/s²)
1 3 5 7 90.00.51.01.52.02.53.0T2m (k)
1 3 5 7 901234u10 (m/s)
1 3 5 7 901234u10 (m/s)
STC-ViT (5.625°) STC-ViT (1.5°)Lead Time (DAYS)RMSE
Figure 7: RMSE comparison between STC-ViT trained on 5.625 and 1.5 degree resolution
B.2 Qualitative Results
Z500Ground Truth
2.0
1.5
1.0
0.5
0.00.51.0
Prediction
2.0
1.5
1.0
0.5
0.00.51.0
Bias
0.1
0.00.1
T850Ground Truth
2
1
01
Prediction
2
1
01
Bias
0.3
0.2
0.1
0.00.10.20.3
T2mGround Truth
2
1
01
Prediction
2
1
01
Bias
0.4
0.3
0.2
0.1
0.00.10.2
U10Ground Truth
3
2
1
0123
Prediction
2
1
0123
Bias
1.0
0.5
0.00.51.0
V10Ground Truth
2
024
Prediction
2
024
Bias
1.5
1.0
0.5
0.00.51.0
Figure 8: 12hr forecast results of STC-ViT
16Under review as submission to TMLR
Z500Ground Truth
2.0
1.5
1.0
0.5
0.00.51.0
Prediction
1.5
1.0
0.5
0.00.51.0
Bias
0.2
0.1
0.00.10.20.3
T850Ground Truth
2
1
01
Prediction
2
1
01
Bias
0.4
0.2
0.00.20.4
T2mGround Truth
2
1
01
Prediction
2
1
01
Bias
0.2
0.00.20.4
U10Ground Truth
2
024
Prediction
2
1
0123
Bias
2
1
01
V10Ground Truth
2
024
Prediction
2
02
Bias
2
1
01
Figure 9: 1 day forecast results of STC-ViT
17Under review as submission to TMLR
Z500Ground Truth
2.0
1.5
1.0
0.5
0.00.51.0
Prediction
1.0
0.5
0.00.51.0
Bias
1.0
0.5
0.00.51.0
T850Ground Truth
2
1
01
Prediction
1.5
1.0
0.5
0.00.51.01.5
Bias
1.0
0.5
0.00.5
T2mGround Truth
2
1
01
Prediction
2.0
1.5
1.0
0.5
0.00.51.0
Bias
1.0
0.5
0.00.5
U10Ground Truth
2
02
Prediction
1.5
1.0
0.5
0.00.51.01.5
Bias
2
024
V10Ground Truth
2
024
Prediction
1.0
0.5
0.00.51.01.5
Bias
2
02
Figure 10: 2 weeks forecast results of STC-ViT
18