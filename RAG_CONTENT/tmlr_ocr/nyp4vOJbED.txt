Under review as submission to TMLR
Expressive Higher-Order Link Prediction through Hyper-
graph Symmetry Breaking
Anonymous authors
Paper under double-blind review
Figure 1: An illustration of a hypergraph of recipes. The nodes are the ingredients and the hyperedges are
the recipes. The task of higher order link prediction is to predict hyperedges in the hypergraph. A negative
hyperedge sample would be the dotted hyperedge. The Asian ingredient nodes ( α) and the European
ingredient nodes ( β) form two separate isomorphism classes. However, GWL-1 cannot distinguish between
these classes and will predict a false positive for the negative sample.
Abstract
A hypergraph consists of a set of nodes along with a collection of subsets of the nodes
called hyperedges. Higher order link prediction is the task of predicting the existence of a
missing hyperedge in a hypergraph. A hyperedge representation learned for higher order
link prediction is fully expressive when it does not lose distinguishing power up to an iso-
morphism. Many existing hypergraph representation learners, are bounded in expressive
power by the Generalized Weisfeiler Lehman-1 (GWL-1) algorithm, a generalization of the
Weisfeiler Lehman-1 algorithm. However, GWL-1 has limited expressive power. In fact,
induced subhypergraphs with identical GWL-1 valued nodes are indistinguishable. Further-
more, message passing on hypergraphs can already be computationally expensive, especially
on GPU memory. To address these limitations, we devise a preprocessing algorithm that
can identify certain regular subhypergraphs exhibiting symmetry. Our preprocessing algo-
rithm runs once with complexity the size of the input hypergraph. During training, we
randomly replace subhypergraphs identifed by the algorithm with covering hyperedges to
breaksymmetry. WeshowthatourmethodimprovestheexpressivityofGWL-1. Ourexten-
sive experiments1also demonstrate the effectiveness of our approach for higher-order link
prediction on both graph and hypergraph datasets with negligible change in computation.
1 Introduction
In real world networks, it is common for a relation amongst nodes to be defined beyond a pair of nodes.
Hypergraphsarethemostgeneralexamplesofthis. ThesehaveapplicationsinrecommendersystemsLüetal.
1https://anonymous.4open.science/r/HypergraphSymmetryBreaking-B07F/
1Under review as submission to TMLR
(2012), visual classification Feng et al. (2019), and social networks Li et al. (2013). Given an unattributed
hypergraph, our goal is to perform higher order link prediction (finding missing hyperedges) with deep
learning methods while also respecting symmetries of the hypergraph.
Current approaches towards hypergraph representation are based on the Generalized Weisfeiler Lehman 1
(GWL-1) algorithm Huang & Yang (2021), a hypergraph isomorphism testing approximation algorithm that
generalizes the message passing algorithm called Weisfeiler Lehman 1 (WL-1) Weisfeiler & Leman (1968)to
hypergraphs. Due to message passing locally within a node’s neighborhood, Weisfeiler Lehman 1 views a
graph as a collection of trees rooted at each node. However this can violate the true meaning of the graph, in
particular its symmetry properties. GWL-1 also views the hypergraph as a collection of rooted trees. These
hypergraph representation methods, called hyperGNNs, are parameterized versions of GWL-1.
To improve the expressivity of hypergraph/graph isomorphism approximators like GWL-1 or WL-1, it is
common to augment the nodes with extra information You et al. (2021); Sato et al. (2021). We devise a
method that, instead, selectively breaks the symmetry of the hypergraph topology itself coming from the
limitations of the hyperGNN architecture. Since message passing on hypergraphs can be very computation-
ally expensive, our method is designed as a preprocessing algorithm that can improve the expressive power
of GWL-1 for higher order link prediction. Since the preprocessing only runs once with complexity linear in
the input, we do not add to any computational complexity from training.
Similar to a substructure counting algorithm Bouritsas et al. (2022), we identify certain symmetries in
induced subhypergraphs. However, unlike in existing work where node attributes are modified, we directly
target and modify the symmetries in the topology. During training, we randomly replace the hyperedges
of the identified symmetric regular induced subhypergraphs with single hyperedges that cover the nodes of
each subhypergraph. We show that our method can increase the expressivity of existing hypergraph neural
networks.
We summarize our contributions as follows:
•We characterize the expressive power and limitations of GWL-1.
•We devise an efficient hypergraph preprocessing algorithm to improve the expressivity of GWL-1 for
higher order link prediction
•We perform extensive experiments on real world datasets to demonstrate the effectiveness of our
approach.
2 Background
We go over what a hypergraph is and how these structures are represented as tensors. We then define what
a hypergraph isomorphism is.
2.1 Isomorphisms on Higher Order Structures
A hypergraph is a generalization of a graph. Hypergraphs allow for all possible subsets over a set of vertices,
called hyperedges. We can thus formally define a hypergraph as:
Definition 2.1. An undirected hypergraph is a pair H= (V,E)consisting of a set of vertices Vand a set of
hyperedgesE⊂2V\({∅}∪{{v}|v∈V} )where 2Vis the power set of the vertex set V.
We will assume all hypergraphs are undirected as in Definition 2.1. For a given hypergraph H= (V,E), a
hypergraphG= (V′,E′)is a subhypergraph of HifV′⊆VandE′⊆E. For aW⊆V, an induced hypergraph
is a subhypergraph (W,F= 2W∩E).
For a given hypergraph H, we also useVHandEHto denote the sets of vertices and hyperedges of H
respectively. According to the definition, a hyperedge is a nonempty subset of the vertices. A hypergraph
withallhyperedgesthesamesize discalledad-uniformhypergraph. A 2-uniformhypergraphisanundirected
graph, or just graph.
2Under review as submission to TMLR
When viewed combinatorially, a hypergraph can include some symmetries, which are called isomorphisms.
On a hypergraph, isomorphisms are defined by bijective structure preserving maps. Such maps are a pair
maps that respect hyperedge structure.
Definition 2.2. For two hypergraphs HandD, a structure preserving map ρ:H→Dis a pair of maps ρ=
(ρV:VH→VD,ρE:EH→ED)such that∀e∈EH,ρE(e)≜{ρV(vi)|vi∈e}∈ED. A hypergraph isomorphism
is a structure preserving map ρ= (ρV,ρE)such that both ρVandρEare bijective. Two hypergraphs are said
to be isomorphic, denoted as H∼=D, if there exists an isomorphism between them. When H=D, an
isomorphism ρis called an automorphism on H. All the automorphisms form a group, which we denote as
Aut(H).
A graph isomorphism is the special case of a hypergraph isomorphism between 2-uniform hypergraphs
according to Definition 2.2.
Aneighborhood N(v)≜(/uniontext
v∈ee,{e:v∈e})ofanodev∈VofahypergraphH= (V,E)isthesubhypergraph
ofHinduced by the set of all hyperedges incident to v. Thedegreeofvis denoted deg(v) =|EN(v)|. A
simple but very common symmetric hypergraph is of importance to our task, namely the neighborhood-
regular hypergraph, or just regular hypergraph.
Definition 2.3. A neighborhood-regular hypergraph is a hypergraph where all neighborhoods of each node
are isomorphic to each other.
Ad-uniform neighborhood of vis the set of all hyperedges of size din the neighborhood of v. Thus, in a
neighborhood-regular hypergraph, all nodes have their d-uniform neighborhoods of the same degree for all
d∈N.
Representing Higher Order Structures as Tensors :There are many data stuctures one can define on
a higher order structure like a hypergraph. An n-order tensor Maron et al. (2018), as a generalization of
an adjacency matrix on graphs can be used to characterize the higher order connectivities. For simplicial
complexes, which are hypergraphs where all subsets of a hyperedge are also hyperedges, a Hasse diagram,
which is a multipartite graph induced by the poset relation of subset amongst hyperedges, or simplices,
differing in exactly one node, is a common data structure Birkhoff (1940). Similarly, the star expansion
matrix Agarwal et al. (2006) can be used to characterize hypergraphs up to isomorphism.
In order to define the star expansion matrix, we define the star expansion bipartite graph.
Definition 2.4 (star expansion bipartite graph) .Given a hypergraph H= (V,E), thestar expansion bipar-
tite graphBV,Eis the bipartite graph with vertices V/unionsqtextEand edges{(v,e)∈V×E|v∈e}.
Definition 2.5. The star expansion incidence matrix Hof a hypergraphH= (V,E)is the|V|× 2|V|0-1
incidence matrix HwhereHv,e= 1iffv∈efor(v,e)∈V×E for some fixed orderings on both Vand2V.
In practice, as data to machine learning algorithms, the matrix His sparsely represented by its nonzeros.
Tostudythesymmetriesofagivenhypergraph H= (V,E), weconsiderthepermutationgrouponthevertices
V, denoted as Sym(V), which acts jointly on the rows and columns of star expansion adjacency matrices.
For an introduction to group theory, see Dummit & Foote (2004). We assume the rows and columns of a
star expansion adjacency matrix have some canonical ordering, say lexicographic ordering, given by some
prefixed ordering of the vertices. Therefore, each hypergraph Hhas a unique canonical matrix representation
H.
We define the action of a permutation π∈Sym(V)on a star expansion adjacency matrix H:
(π·H)v,e=(u1...v...uk)≜Hπ−1(v),π−1(e)=(π−1(u1)...π−1(v)...π−1(uk)) (1)
Based on the group action, consider the stabilizer subgroup of Sym(V)on an incidence matrix H:
StabSym (V)(H) ={π∈Sym(V)|π·H=H} (2)
For simplicity we omit the lower index of Sym(V)when the permutation group is clear from context. It can
be checked that Stab(H)⊆Sym(V)is a subgroup. Intuitively, Stab(H)consists of all permutations that fix
H. These are equivalent to hypergraph automorphisms on the original hypergraph H.
3Under review as submission to TMLR
Proposition 2.1. Aut(H)∼=Stab(H)are equivalent as isomorphic groups.
We can also define a notion of isomorphism between k-node sets using the stabilizers on H.
Definition 2.6. For a given hypergraph Hwith star expansion matrix H, twok-node setsS,T⊆Vare
calledisomorphic , denoted as S≃T, if∃π∈Stab(H),π(S) =Tandπ(T) =S.
Such isomorphism is an equivalance relation on k-node sets. When k= 1, we have isomorphic nodes, denoted
u∼=Hvforu,v∈V. Node isomorphism is also studied as the so-called structural equivalence in Lorrain &
White (1971). Furthermore, when S≃Twe can then say that there is a matching amongst the nodes in
setsSandTso that matched nodes are isomorphic.
2.2 Invariance and Expressivity
For a given hypergraph H= (V,E), we want to do hyperedge prediction on H, which is to predict missing
hyperedgesfrom k-nodesets for k≥2. Let|V|=n,|E|=m, andH∈Zn×2n
2bethe star expansion adjacency
matrix ofH. To do hyperedge prediction, we study k-node representations h: [V]k×Zn×2n
2→Rdthat map
k-node sets of hypergraphs to d-dimensional Euclidean space. Ideally, we want a most-expressive k-node
representation for hyperedge prediction, which is intuitively a k-node representation that is injective on k-
node set isomorphism classes from H. We break up the definition of most-expressive k-node representation
into possessing two properties, as follows:
Definition 2.7. Leth: [V]k×Zn×2n
2→Rdbe ak-node representation on a hypergraph H. LetH∈Zn×2n
2
be the star expansion adjacency matrix of Hfornnodes. The representation hisk-node most expressive if
∀S,S′⊂V,|S|=|S′|=k, the following two conditions are satisfied:
1.hisk-node invariant :∃π∈Stab(H),π(S) =S′=⇒h(S,H) =h(S′,H)
2.hisk-node expressive ∄π∈Stab(H),π(S) =S′=⇒h(S,H)̸=h(S′,H)
The first condition of a most expressive k-node representation states that the representation must be well
definedonthe knodesuptoisomorphism. Thesecondconditionrequirestheinjectivityofourrepresentation.
These two conditions mean that the representation does not lose any information when doing prediction for
missingk-sized hyperedges on a set of knodes.
2.3 Generalized Weisfeiler-Lehman-1
We describe a generalized Weisfeiler-Lehman-1 (GWL-1) hypergraph isomorphism test similar to Huang &
Yang (2021); Feng et al. (2023) based on the WL-1 algorithm for graph isomorphism testing. There have
been many parameterized variants of the GWL-1 algorithm implemented as neural networks, see Section 3.
LetHbe the star expansion matrix for a hypergraph H. We define the GWL-1 algorithm as the following
two step procedure on Hat iteration number i≥0.
f0
e←{},h0
v←{}
fi+1
e←{{ (fi
e,hi
v)}}v∈e,∀e∈EH(H)
hi+1
v←{{ (hi
v,fi+1
e)}}v∈e,∀v∈VH(H)(3)
This is slightly different from the algorithm presented in Huang & Yang (2021) at the fi+1
eupdate step. Our
update step involves an edge representation fi
e, which is not present in their version. Thus our version of
GWL-1 is more expressive than that in Huang & Yang (2021). However, they both possess some of the same
issues that we identify. We denote fi
e(H)andhi
v(H)as the hyperedge and node ith iteration GWL-1, called
i-GWL-1, values on an unattributed hypergraph Hwith star expansion H. If GWL-1 is run to convergence
then we omit the iteration number i. We also mean this when we say i=∞.
For a hypergraph Hwith star expansion matrix H, GWL-1 is strictly more expressive than WL-1 on
A=H·D−1
e·HTwithDe=diag(HT·1n), the node to node adjacency matrix, also called the clique
4Under review as submission to TMLR
expansion ofH. This follows since a triangle with its 3-cycle boundary: Tand a 3-cycle C3have exactly the
same clique expansions. Thus WL-1 will give the same node values for both TandC3. GWL-1 on the star
expansions HTandHC3, on the other hand, will identify the triangle as different from its bounding edges.
Letfi(H)≜[fi
e1(H),···fi
em(H)]andhi(H)≜[hi
v1(H),···hi
vn(H)]be two vectors whose entries are ordered
by the column and row order of H, respectively.
Proposition 2.2. The update steps fi(H)andhi(H)of GWL-1 are permutation equivariant; For any
π∈Sym(V), let:π·fi(H)≜[fi
π−1(e1)(H),···,fi
π−1(em)(H)]andπ·hi(H)≜[hi
π−1(v1)(H),···hi
π−1(vn)(H)]:
∀i∈N,π·fi(H) =fi(π·H),π·hi(H) =hi(π·H) (4)
Define the operator AGGas ak-set map to representation space Rd. Define the following representation of
ak-node subset S⊂Vof hypergraphHwith star expansion matrix H:
h(S,H) =AGG [{hi
v(H)}v∈S] (5)
wherehi
v(H)is the node value of i-GWL-1 on Hfor nodev. The representation h(S,H)preserves hyperedge
isomorphism classes as shown below:
Proposition 2.3. Leth(S,H) =AGGv∈S[hi
v(H)]with injective AGG and hi
vpermutation equivariant. The
representation h(S,H)isk-node invariant but not necessarily k-node expressive for Sa set ofknodes.
It follows that we can guarantee a k-node invariant representation by using GWL-1. For deep learning, we
parameterize AGGas a universal set learner. The node representations hi
v(H)are also parameterized and
rewritten into a message passing hypergraph neural network with matrix equations Huang & Yang (2021).
3 Related Work and Existing Issues
There are many hyperlink prediction methods. Most message passing based methods for hypergraphs are
based on the GWL-1 algorithm. These include Huang & Yang (2021); Yadati et al. (2019); Feng et al.
(2019); Gao et al. (2022); Dong et al. (2020); Srinivasan et al. (2021); Chien et al. (2022); Zhang et al. (2018).
Examples of message passing based approaches that incorporate positional encodings on hypergraphs include
SNALS Wan et al. (2021). The paper Zhang et al. (2019) uses a pair-wise node attention mechanism to do
higher order link prediction. For a survey on hyperlink prediction, see Chen & Liu (2022).
There have been methods to improve the expressive power due to symmetries in graphs. In Papp & Wat-
tenhofer (2022), substructure labeling is formally analyzed. One of the methods analyzed includes labeling
fixed radius ego-graphs as in You et al. (2021); Zhang & Li (2021). Other methods include appending ran-
dom node features Sato et al. (2021), labeling breadth-first and depth-first search trees Li et al. (2023b)
and encoding substructures Zeng et al. (2023); Wijesinghe & Wang (2021). All of the previously mentioned
methods depend on a fixed subgraph radius size. This prevents capturing symmetries that span long ranges
across the graph. Zhang et al. (2023) proposes to add metric information of each node relative to all other
nodes to improve WL-1. This would be very computationally expensive on hypergraphs.
Cycles are a common symmetric substructure. There are many methods that identify this symmetry. Cy2C
Choi et al. is a method that encodes cycles to cliques. It has the issue that if the the cycle-basis algorithm is
not permutation invariant, isomorphic graphs could get different cycle bases and thus get encoded by Cy2C
differently, violating the invariance of WL-1. Similarly, the CW Network Bodnar et al. (2021) is a method
that attaches cells to cycles to improve upon the distinguishing power of WL-1 for graph classification.
However, inflating the input topology with cells as in Bodnar et al. (2021) would not work for link predicting
since it will shift the hyperedge distribution to become much denser. Other works include cell attention
networks Giusti et al. (2022) and cycle basis based methods Zhang et al. (2022). For more related work, see
the Appendix.
5Under review as submission to TMLR
4 A Characterization of GWL-1
A hypergraph can be represented by a bipartite graph BV,EfromVtoEwhere there is an edge (v,e)in
the bipartite graph iff node vis incident to hyperedge e. This bipartite graph is called the star expansion
bipartite graph.
We introduce a more structured version of graph isomorphism called a 2-color isomorphism to characterize
hypergraphs. It is a map on 2-colored graphs, which are graphs that can be colored with two colors so that
no two nodes in any graph with the same color are connected by an edge. We define a 2-colored isomorphism
formally here:
Definition 4.1. A2-colored isomorphism is a graph isomorphism on two 2-colored graphs that preserves
node colors. It is denoted by ∼=c.
A bipartite graph always has a 2-coloring. In this paper, we canonically fix a 2-coloring on all star expansion
bipartite graphs by assigning red to all the nodes in the node partition and and blue to all the nodes in the
hyperedge partition. See Figure 2(a) as an example. We let BV,BEbe the red and blue colored nodes in
BV,Erespectively.
Proposition 4.1. We have two hypergraphs (V1,E1)∼=(V2,E2)iffBV1,E1∼=cBV2,E2whereBVi,Eiis the star
expansion bipartite graph of (Vi,Ei)
We define a topological object for a graph originally from algebraic topology called a universal cover:
Definition 4.2 (Hatcher (2005)) .Theuniversal covering of a connected graph Gis a (potentially infinite)
graph ˜Gtogether with a map pG:˜G→Gsuch that:
1.∀x∈V(˜G),pG|N(x)is an isomorphism onto N(pG(x)).
2.˜Gis simply connected (a tree)
We call such pGtheuniversal covering map and ˜Gtheuniversal cover ofG. A covering graph is a graph
that satisfies property 1 but not necessarily 2 in Definition 4.2. The universal covering ˜Gis essentially
unique Hatcher (2005) in the sense that it can cover all connected covering graphs of G. Furthermore, define
a rooted isomorphism Gx∼=Hyas an isomorphism between graphs GandHthat maps xtoyand vice
versa. It is a known result that:
Theorem 4.2. [Krebs & Verbitsky (2015)] Let GandHbe two connected graphs. Let pG:˜G→G,pH:
˜H→Hbe the universal covering maps of GandHrespectively. For any i∈N, for any two nodes x∈G
andy∈H:˜Gi
˜x∼=˜Gi
˜yiff the WL-1 algorithm assigns the same value to nodes x=pG(˜x)andy=pH(˜y).
We generalize the second result stated above about a topological characterization of WL-1 for GWL-1 for
hypergraphs. In order to do this, we need to generalize the definition of a universal covering to suite the
requirements of a bipartite star expansion graph. To do this, we lift BV,Eto a2-colored tree universal cover
˜BV,Ewhere the red/blue nodes of BV,Eare lifted to red/blue nodes in ˜BV,E. Furthermore, the labels {}are
placed on the blue nodes corresponding to the hyperedges in the lift and the labels Xvare placed on all its
corresponding red nodes in the lift. Let (˜Bk
V,E)˜xdenote the k-hop rooted 2-colored subtree with root ˜xand
pBV,E(˜x) =xfor anyx∈V(BV,E).
Theorem 4.3. LetH1= (V1,E1)andH2= (V2,E2)be two connected hypergraphs. Let BV1,E1andBV2,E2
be two canonically colored bipartite graphs for H1andH2(vertices colored red and hyperedges colored blue).
LetpBV1,E1:˜BV1,E1→BV1,E1,pBV2,E2:˜BV2,E2→BV2,E2be the universal coverings of BV1,E1andBV2,E2
respectively. For any i∈N+, for any of the nodes x1∈BV1,e1∈BE1andx2∈BV2,e2∈BE2:
(˜B2i−1
V1,E1)˜e1∼=c(˜B2i−1
V2,E2)˜e2ifffi
e1=fi
e2
(˜B2i
V1,E1)˜x1∼=c(˜B2i
V2,E2)˜x2iffhi
x1=hi
x2, withfi
•,hi
•theith GWL-1 values for the hyperedges and nodes
respectively where e1=pBV1,E1(˜e1),x1=pBV1,E1(˜x1),e2=pBV2,E2(˜e2),x2=pBV2,E2(˜x2).
6Under review as submission to TMLR
Figure 2: An illustration of hypergraph symmetry breaking. (c,d) 3-regular hypergraphs C3
4,C3
5with 4and5
nodes respectively and their corresponding universal covers centered at any hyperedge (˜BC3
4)e∗,∗,∗,(˜BC3
5)e∗,∗,∗
with universal covering maps pBC3
4,pBC3
5. (b,e) the hypergraphs ˆC3
4,ˆC3
5, which are C3
4,C3
5with 4,5-sized
hyperedges attached to them and their corresponding universal covers and universal covering maps. (a,f)
are the corresponding bipartite graphs of ˆC3
4,ˆC3
5. (c,d) are indistinguishable by GWL-1 and thus will give
identical node values by Theorem 4.3. On the other hand, (b,e) gives node values which are now sensitive
to the the order of the hypergraphs 4,5, also by Theorem 4.3.
See Figure 2 for an illustration of the universal covering of two 3-uniform neighborhood regular hypergraphs
and their corresponding bipartite graphs. Notice that by Theorems 4.3, 4.2 GWL-1 reduces to computing
WL-1 on the bipartite graph up to the 2-colored isomorphism.
4.1 A Limitation of GWL-1
For two neighborhood-regular hypergraphs C1andC2, the red/blue colored universal covers ˜BC1,˜BC2of
the star expansions of C1andC2are isomorphic, with the same GWL-1 values on all nodes. However, two
neighborhood-regular hypergraphs of different order become distinguishable if a single hyperedge covering all
the nodes of each neighborhood-regular hypergraph is added. Furthermore, deleting the original hyperedges,
does not change the node isomorphism classes of each hypergraph. Referring to Figure 2, consider the
hypergraphC=C3
4⊔C3
5, the hypergraph with two 3-regular hypergraphs C3
4andC3
5acting as two connected
components ofC. As shown in Figure 2, the node representations of the two hypergraphs are identical due
to Theorem 4.3.
Given a hypergraph H, we define a special induced subhypergraph R⊂Hwhose node set GWL-1 cannot
distinguish from other such special induced subhypergraphs.
Definition 4.3. AL-GWL-1 symmetric induced subhypergraph R⊂HofHis a connected induced subhy-
pergraph determined by VR⊂VH, some subset of nodes that are all indistinguishable amongst each other by
L-GWL-1:
hL
u(H) =hL
v(H),∀u,v∈VR (6)
WhenL=∞, we call suchRa GWL-1 symmetric induced subhypergraph. Furthermore, if R=H, then we
sayHisGWL-1 symmetric .
This definition is similar to that of a symmetric graph from graph theory Godsil & Royle (2001), except that
isomorphic nodes are determined by the GWL-1 approximator instead of an automorphism. The following
observation follows from the definitions.
Observation 1. A hypergraphHis GWL-1 symmetric if and only if it is L-GWL-1 symmetric for all L≥1
if and only ifHis neighborhood regular.
Our goal is to find GWL-1 symmetric induced subhypergraphs in a given hypergraph and break their
symmetry without affecting any other nodes.
7Under review as submission to TMLR
5 Method
Our goal is to predict higher order links in a hypergraph transductively. This can be formulated as follows:
Problem 1. Given a hypergraph H= (V,E)and ground truth hypergraph Hgt= (V,Egt),E⊂Egt, whereE
is observable: Predict the existence of the unobserved hyperedges Egt\E.
We will assume that the unobservable hyperedges are of the same size kso that we only need to predict on
k-node sets. In order to preserve the most information while still respecting topological structure, we aim
to start with an invariant multi-node representation to predict hyperedges and increase its expressiveness,
as defined in Definition 2.7. For input hypergraph Hand its matrix representation H, to do the prediction
of a missing hyperedge on node subsets, we use a multi-node representation h(S,H)forS⊂V(H)as in
Equation 5 due to its simplicity, guaranteed invariance, and improve its expressivity. We aim to not affect
the computational complexity since message passing on hypergraphs is already quite expensive, especially
on GPU memory.
Our method is a preprocessing algorithm that operates on the input hypergraph. In order to increase
expressivity, we search for potentially indistinguishable regular induced subhypergraphs so that they can
be replaced with hyperedges that span the subhypergraph to break the symmetries that prevent GWL-1
from being more expressive. We devise an algorithm, which is shown in Algorithm 1. It takes as input a
hypergraphHwith star expansion matrix H. The idea of the algorithm is to identify nodes of the same
GWL-1 value that are maximally connected and use this collection of node subsets to break the symmetry
ofH.
First we introduce some combinatorial definitions for hypergraph data that we will use in our algorithm:
Definition 5.1. A hypergraphH= (V,E)isconnected ifBV,Eis a connected graph.
Aconnected component ofHis a connected induced subhypergraph which is not properly contained in any
connected subhypergraph of H.
Definition 5.2. Chitra & Raphael (2019) A random walk on a hypergraphH= (V,E)is a Markov chain
with state spaceVwith transition probabilities Pu,v≜/summationtext
e⊃{u,v}:e∈Eω(e)
deg(u)|e|, whereω(e) :E→ [0,1]is some
discrete probability distribution on the hyperedges. When not specified, this is the uniform distribution.
Definition 5.3. Astationary distribution π:V→ [0,1]for a Markov chain with transition probabilities
Pu,vis defined by the relationship/summationtext
u∈VPu,vπ(u) =π(v).
For a hypergraph random walk we have the closed form: π(v) =deg(v)/summationtext
u∈Vdeg(u)forv∈VassumingHis a
connected hypergraph.
Algorithm: Our method is explicitly given in Algorithm 1. For a given L∈N+and anyL-GWL-1 node
valuecL, we construct the induced subhypergraph HcLfrom theL-GWL-1 class of nodes:
VcL≜{v∈V:cL=hL
v(H)}, (7)
wherehL
vdenotes the L-GWL-1 class of node v. We then compute the connected components of HcL.
DenoteCcLas the set of all connected components of HcL. IfL=∞, then drop L. Each of these connected
components is a subhypergraph of H, denotedRcL,iwhereRcL,i⊂HcL⊂Hfori= 1...|CcL|.
Downstream Training: After executing Algorithm 1, we collect its output (RV,RE). During training,
for eachi= 1...|CcL|we randomly perturb RcL,iby:
•Attaching a single hyperedge that covers VRcL,iwith probability qiand not attaching with proba-
bility 1−qi.
•All the hyperedges in RcL,iare dropped or kept with probability pand1−prespectively.
Let ˆHbe the estimator of the input hypergraph Has determined by the random drop and attaching
operations. Since ˆHis random, each sample of ˆHhas a stationary distribution ˆπ. The expected stationary
8Under review as submission to TMLR
distribution, denoted E[ˆπ], is the expectation of ˆπover the distribution determined by ˆH. We show in
Proposition 5.6 that the probabilities p,qi,i= 1...|CcL|can be chosen so that ˆπis unbiased.
Our method is similar to the concept of adding virtual nodes Hwang et al. (2022) in graph representation
learning. This is due to the equivalence between virtual nodes and hyperedges by Proposition 4.1. For a
guarantee of improving expressivity, see Lemma 5.2 and Theorems 5.3, 5.4. For an illustration of the data
augmentation, see Figure 2.
Alternatively, downstream training using the output of Algorithm 1 can be done. Similar to subgraph NNs,
this is done by applying an ensemble of models Alsentzer et al. (2020); Papp et al. (2021); Tan et al. (2023),
with each model trained on transformations of Hwith its symmetric subhypergraphs randomly replaced.
This, however, is computationally expensive.
Algorithm 1: A Symmetry Finding Algorithm
Data:HypergraphH= (V,E), represented by its star expansion matrix H.L∈N+is the number
of iterations to run GWL-1.
Result: A pair of collections: (RV={VRj},RE=∪j{ERj})whereRjare disconnected
subhypergraphs exhibiting symmetry in Hthat are indistinguishable by L-GWL-1.
1UL←hL
v(H);GL←{UL[v] :∀v∈V}; /*UL[v]is the L-GWL-1 value of node v∈ V. */
2BVH,EH←Bipartite (H)/* Construct the bipartite graph from H. */
3RV←{};RE←{}
4forcL∈GLdo
5VcL←{v∈V:UL[v] =cL},EcL←{e∈E:u∈VcL,∀u∈e}
6CcL←ConnectedComponents (HcL= (VcL,EcL))
7forRcL,i∈CcLdo
/* There should be at least 3nodes to form a nontrivial hyperedge */
8if|VRcL,i|≥3then
9RV←RV∪{VRcL,i};RE←RE∪ERcL,i
10 end
11end
12end
13return (RV,RE)
Algorithm Guarantees: We show some guarantees for the output of Algorithm 1.
Notation: LetH= (V,E)be a hypergraph with star expansion matrix Hand let (RV,RE)be the output
of Algorithm 1 on HforL∈N+. Let ˆHL≜(V,E∪RV)beHafter adding all the hyperedges from RVand
letˆHLbe the star expansion matrix of the resulting hypergraph ˆHL. LetVcL,s≜{v∈VcL:v∈R,R∈
CcL,|VR|=s}be the set of all nodes of L-GWL-1 class cLbelonging to a connected component in CcLof
s≥1nodes inHcL, the induced subhypergraph of L-GWL-1. LetGL≜{hL
v(H) :v∈V}be the set of all
L-GWL-1 values on H. LetScL≜{|VRcL,i|:RcL,i∈CcL}be the set of node set sizes of the connected
components inHcL.
Proposition 5.1. IfL=∞, for any GWL-1 node value ccomputed onH, all connected component
subhypergraphsRc,i∈Ccare GWL-1 symmetric as hypergraphs.
Lemma 5.2. IfL∈N+is small enough so that after running Algorithm 1 on L, for anyL-GWL-1
node class cLonVnone of the discovered VRcL,iare within Lhyperedges away from any VRcL,jfor all
i,j∈1...|CcL|,i̸=j, then after forming ˆHL, the newL-GWL-1 node classes of VRcL,ifori= 1...CcLinˆHL
are all the same class c′
Lbut are distinguishable from cLdepending on|VRcL,i|.
We also have the following guarantee on the number of pairs of distinguishable k-node sets on ˆH:
Theorem 5.3. Let|V|=n,L∈N+. Ifvol(v)≜/summationtext
e∈E:e∋v|e|=O(log1−ϵ
4Ln),∀v∈Vfor any constant
ϵ > 0;|ScL|≤S,∀cL∈CL,Sconstant, and|VL
cL,s|=O(nϵ
log1
2k(n)),∀s∈CcL, then fork∈N+andk-
tupleC= (cL,1...cL,k),cL,i∈GL,i= 1..kthere exists ω(n2kϵ)many pairs of k-node sets S1̸≃S2such
9Under review as submission to TMLR
that (hL
u(H))u∈S1= (hL
v∈S2(H)) =C, as ordered k-tuples, while h(S1,ˆHL)̸=h(S2,ˆHL)also byLsteps of
GWL-1.
We show that our algorithm increases expressivity (Definition 2.7) for h(S,H)of Equation 5.
Theorem 5.4 (Invariance and Expressivity) .IfL=∞, GWL-1 enhanced by Algorithm 1 is still invari-
ant to node isomorphism classes of Hand can be strictly more expressive than GWL-1 to determine node
isomorphism classes.
Proposition 5.5 provides the time complexity of our algorithm.
Proposition 5.5 (Complexity) .Algorithm 1 runs in time O(nnz(H)L+ (n+m)), which is order linear
in the size of the input star expansion matrix Hfor hypergraphH= (V,E), ifLis independent of nnz(H),
wheren=|V|,nnz(H) =vol(V)≜/summationtext
v∈Vdeg(v)andm=|E|.
Since Algorithm 1 runs in time linear in the size of the input when Lis constant, in practice it only takes a
small fraction of the training time for hypergraph neural networks.
For the downstream training, we show that there are Bernoulli hyperedge drop/attachment probabilities
p,qirespectively for each RcL,iso that the stationary distribution doesn’t change. This shows that our data
augmentation can still preserve the low frequency random walk signal.
Proposition 5.6. For a connected hypergraph H= (V,E), let (RV,RE)be the output of Algorithm 1 on
H. Then there are Bernoulli probabilities p,qifori= 1...|RV|for attaching a covering hyperedge so that ˆπ
is an unbiased estimator of π.
6 Evaluation
PR-AUC↑Baseline Ours Baseln.+edrop
HGNN 0.98±0.03 0.99±0.08 0.96±0.02
HGNNP 0.98±0.02 0.98±0.09 0.96±0.10
HNHN 0.98±0.01 0.96±0.07 0.97±0.04
HyperGCN 0.98±0.07 0.98±0.11 0.98±0.03
UniGAT 0.99±0.06 0.99±0.03 0.99±0.07
UniGCN 0.99±0.00 0.99±0.03 0.99±0.08
UniGIN 0.87±0.02 0.86±0.10 0.85±0.08
UniSAGE 0.86±0.04 0.86±0.05 0.84±0.09
(a) cat-edge-DAWNPR-AUC↑Baseline Ours Baseln.+edrop
HGNN 0.90±0.13 1.00±0.00 0.90±0.13
HGNNP 0.90±0.09 1.00±0.07 1.00±0.03
HNHN 0.90±0.09 0.91±0.02 0.90±0.08
HyperGCN 1.00±0.00 1.00±0.03 1.00±0.02
UniGAT 0.90±0.06 1.00±0.03 1.00±0.06
UniGCN 1.00±0.01 0.91±0.01 0.82±0.09
UniGIN 0.90±0.12 0.95±0.06 0.90±0.11
UniSAGE 0.90±0.16 1.00±0.08 0.90±0.17
(b) cat-edge-music-blues-reviewsPR-AUC↑Baseline Ours Baseln.+ edrop
HGNN 0.96±0.10 0.98±0.05 0.96±0.04
HGNNP 0.96±0.05 0.98±0.09 0.97±0.07
HNHN 0.96±0.02 0.97±0.08 0.97±0.06
HyperGCN 0.93±0.05 0.98±0.07 0.96±0.09
UniGAT 0.96±0.01 0.98±0.14 0.97±0.04
UniGCN 0.96±0.04 0.96±0.11 0.96±0.09
UniGIN 0.97±0.03 0.97±0.11 0.96±0.05
UniSAGE 0.96±0.10 0.96±0.10 0.96±0.02
(c) contact-high-school
PR-AUC↑Baseline Ours Baseln.+edrop
HGNN 0.95±0.03 0.96±0.01 0.95±0.03
HGNNP 0.95±0.02 0.96±0.09 0.96±0.07
HNHN 0.94±0.07 0.97±0.10 0.95±0.05
HyperGCN 0.97±0.01 0.97±0.05 0.96±0.08
UniGAT 0.95±0.02 0.98±0.07 0.98±0.02
UniGCN 0.96±0.00 0.97±0.14 0.97±0.10
UniGIN 0.95±0.09 0.97±0.02 0.95±0.05
UniSAGE 0.96±0.08 0.95±0.05 0.96±0.02
(d) contact-primary-schoolPR-AUC↑Baseline Ours Baseln.+edrop
HGNN 0.95±0.07 0.97±0.08 0.96±0.07
HGNNP 0.95±0.07 0.96±0.02 0.96±0.01
HNHN 0.94±0.01 0.97±0.02 0.95±0.06
HyperGCN 0.92±0.01 0.94±0.06 0.94±0.08
UniGAT 0.94±0.08 0.98±0.14 0.97±0.08
UniGCN 0.97±0.08 0.97±0.14 0.97±0.06
UniGIN 0.93±0.07 0.94±0.11 0.93±0.09
UniSAGE 0.93±0.07 0.93±0.08 0.92±0.04
(e) email-EuPR-AUC↑Baseline Ours Baseln.+edrop
HGNN 0.75±0.09 0.85±0.09 0.71±0.14
HGNNP 0.83±0.09 0.85±0.08 0.85±0.04
HNHN 0.72±0.09 0.82±0.03 0.74±0.09
HyperGCN 0.87±0.08 0.83±0.05 1.00±0.07
UniGAT 0.80±0.09 0.83±0.03 0.78±0.05
UniGCN 0.84±0.08 0.89±0.10 0.71±0.07
UniGIN 0.69±0.14 0.76±0.05 0.61±0.11
UniSAGE 0.72±0.11 0.71±0.10 0.64±0.10
(f) cat-edge-madison-restaurants
Table 1: Transductive hyperedge prediction PR-AUC scores on six different hypergraph datasets. The
highest scores per HyperGNN architecture (row) is colored. Red text denotes the highest average scoring
method. Orange text denotes a two-way tie and brown text denotes a three-way tie. All datasets involve
predicting hyperedges of size 3.
We evaluate our method on higher order link prediction with many of the standard hypergraph neu-
ral network methods. Due to potential class imbalance, we measure the PR-AUC of higher order
link prediction on the hypergraph datasets. These datasets are: cat-edge-DAWN, cat-edge-music-
blues-reviews, contact-high-school, contact-primary-school, email-Eu, cat-edge-madison-
restaurants . These datasets range from representing social interactions as they develop over time to
collections of reviews to drug combinations before overdose. We also evaluate on the amherst41 dataset,
which is a graph dataset. All of our datasets are unattributed hypergraphs/graphs.
Data Splitting: For the hypergraph datasets, each hyperedge in it is paired with a timestamp (a real
number). These timestamps are a physical time for which a higher order interaction, represented by a
10Under review as submission to TMLR
PR-AUC↑ HGNN HGNNP HNHN HyperGCN UniGAT UniGCN UniGIN UniSAGE
Ours 0.73±0.10 0.61±0.05 0.64±0.06 0.71±0.09 0.72±0.08 0.70±0.08 0.73±0.03 0.73±0.06
HyperGNN Baseline 0.62±0.09 0.62±0.10 0.63±0.04 0.71±0.07 0.70±0.06 0.69±0.07 0.73±0.06 0.73±0.09
HyperGNN Baseln.+edrop 0.61±0.03 0.61±0.03 0.61±0.09 0.71±0.06 0.71±0.02 0.69±0.05 0.73±0.09 0.73±0.04
APPNP 0.42±0.07 0.42±0.07 0.42±0.07 0.42±0.07 0.42±0.07 0.42±0.07 0.42±0.07 0.42±0.07
APPNP+edrop 0.42±0.03 0.42±0.03 0.42±0.03 0.42±0.03 0.42±0.03 0.42±0.03 0.42±0.03 0.42±0.03
GAT 0.49±0.06 0.49±0.06 0.49±0.06 0.49±0.06 0.49±0.06 0.49±0.06 0.49±0.06 0.49±0.06
GAT+edrop 0.49±0.06 0.49±0.06 0.49±0.06 0.49±0.06 0.49±0.06 0.49±0.06 0.49±0.06 0.49±0.06
GCN2 0.56±0.12 0.56±0.12 0.56±0.12 0.56±0.12 0.56±0.12 0.56±0.12 0.56±0.12 0.56±0.12
GCN2+edrop 0.54±0.02 0.54±0.02 0.54±0.02 0.54±0.02 0.54±0.02 0.54±0.02 0.54±0.02 0.54±0.02
GCN 0.40±0.03 0.40±0.03 0.40±0.03 0.40±0.03 0.40±0.03 0.40±0.03 0.40±0.03 0.40±0.03
GCN+edrop 0.65±0.04 0.65±0.04 0.65±0.04 0.65±0.04 0.65±0.04 0.65±0.04 0.65±0.04 0.65±0.04
GIN 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10
GIN+edrop 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10
GraphSAGE 0.44±0.01 0.44±0.01 0.44±0.01 0.44±0.01 0.44±0.01 0.44±0.01 0.44±0.01 0.44±0.01
GraphSAGE+edrop 0.44±0.10 0.44±0.10 0.44±0.10 0.44±0.10 0.44±0.10 0.44±0.10 0.44±0.10 0.44±0.10
Table 2: PR-AUC on graph dataset amherst41 . Each column is a comparison of the baseline PR-AUC
scores against the PR-AUC score for our method (first row) applied to a standard HyperGNN architecture.
The coloring scheme is the same as in Table 1.
hyperedge, occurs. We form a train-val-test split by letting the train be the hyperedges associated with the
80th percentile of timestamps, the validation be the hyperedges associated with the timestamps in between
the 80th and 85th percentiles. The test hyperedges are the remaining hyperedges. The train validation and
test datasets thus form a partition of the nodes. We do the task of hyperedge prediction for sets of nodes
of size 3, also known as triangle prediction. Half of the size 3hyperedges in each of train, validation and
test are used as positive examples. For each split, we select random subsets of nodes of size 3that do not
form hyperedges for negative sampling. We maintain positive/negative class balance by sampling the same
number of negative samples as positive samples. Since the test distribution comes from later time stamps
than those in training, there is a possibility that certain datasets are out-of-distribution if the hyperedge
distribution changes.
For the graph dataset, the single graph is deterministically split into 80/5/15 for train/val/test. We remove
10%of the edges in training and let them be positive examples Ptrto predict. For validation and test, we
remove 50%of the edges from both validation and test to set as the positive examples Pval,Pteto predict.
For train, validation, and test, we sample |Ptr|,|Pval|,|Pte|negative link samples from the links of train,
validation and test.
6.1 Architecture and Training
Our algorithm serves as a preprocessing step for selective data augmentation. Given a single training
hypergraphH, the Algorithm 1 is applied and during training, the identified hyperedges of the symmetric
induced subhypergraphs of Hare randomly replaced with single hyperedges that cover all the nodes of each
induced subhypergraph. Each symmetric subhypergraph has a p= 0.5probability of being selected. To get
a large set of symmetric subhypergraphs, we run 2iterations of GWL-1.
We implement h(S,H)from Equation 5 as follows. Upon extracting the node representations from the
hypergraph neural network, we use a multi-layer-perceptron (MLP) on each node representation, sum across
such compositions, then apply a final MLP layer after the aggregation. We use the binary cross entropy loss
on this multi-node representation for training. We always use 5layers of hyperGNN convolutions, a hidden
dimension of 1024, and a learning rate of 0.01.
6.2 Higher Order Link Prediction Results
We show in Table 1 the comparison of PR-AUC scores amongst the baseline methods of HGNN, HGNNP,
HNHN, HyperGCN, UniGIN, UniGAT, UniSAGE, their hyperedge dropped versions, and "Our" method,
which preprocesses the hypergraph to break symmetry during training. For the hyperedge drop baselines,
there is a uniform 50%chance of dropping any hyperedge. We use the Laplacian eigenmap Belkin & Niyogi
(2003) positional encoding on the clique expansion of the input hypergraph. This is common practice in
(hyper)link prediction and required for using a hypergraph neural network on an unattributed hypergraph.
11Under review as submission to TMLR
We show in Table 2 the PR-AUC scores on the amhrest41 . Along with HyperGNN architectures we use for
the hypergraph experiments, we also compare with standard GNN architectures: APPNP Gasteiger et al.
(2018), GAT Veličković et al. (2017), GCN2 Chen et al. (2020), GCN Kipf & Welling (2016a), GIN Xu et al.
(2018), and GraphSAGE Hamilton et al. (2017). For every HyperGNN/GNN architecture, we also apply
drop-edge Rong et al. (2019) to the input graph and use this also as baseline. The number of layers of each
GNN is set to 5and the hidden dimension at 1024. For APPNP and GCN2, one MLP is used on the initial
node positional encodings.
Overall, our method performs well across a diverse range of higher order network datasets. We observe
that our method can often outperform the baseline of not performing any data perturbations as well as
the same baseline with uniformly random hyperedge dropping. Our method has an added advantage of
being explainable since our algorithm works at the data level. There was also not much of a concern for
computational time since our algorithm runs in time O(nnz(H) +n+m), which is optimal since it is the
size of the input.
6.3 Empirical Observations on the Components Discovered by the Algorithm
As we are primarily concerned with symmetries in a hypergraph, we empirically measure the size and
frequency of the components found by the Algorithm for real-world datasets. For the real-world datasets
listed in Appendix D, in Figure 3a, we plot the fraction of connected components of the same L-GWL-1
value (L= 2) that are large enough to be used by Algorithm 1 as a function of the number of nodes of
the hypergraph. We notice that the fraction of connected components is not large, however every dataset
has a nonzero fraction. On the right, in Figure 3b we show the distribution of the sizes of the connected
components found by Algorithm 1. We see that, on average, the connected components are at least an order
of magnitude smaller compared to the total number of nodes. Common to both plots, the graph datasets
appear to have more nodes and a consistent fraction and size of components, while the hypergraph datasets
have higher variance in the fraction of components, which is expected since there are more possibilities for the
connections in a hypergraph. In terms of the number of identified connected components, there are at least
0 5000 10000 15000 20000
Num. Nodes0.000.020.040.060.080.10Frac. of Same GWL-1 Valued Conn. Comps. of Size Atleast 3hypergraph datasets
graph datasets
(a) Fraction of components of size atleast 3
selected by Algorithm 1.
0 5000 10000 15000 20000
Num. Nodes34567Average Size of Conn. Comps. with Size Atleast 3hypergraph datasets
graph datasets(b) Average size of components of size atleast
3from Algorithm 1.
Figure 3
exponentially many interventions that can be imposed on the hypergraph from simply dropping components.
Thus, even finding just 10components result in at least 210≈103many counterfactual hypergraphs. It is
known, that a large set of data augmentations during learning improves learner generalization.
7 Conclusion
We have characterized and identified the limitations of GWL-1, a hypergraph isomorphism testing algorithm
that underlies many existing HyperGNN architectures. A common issue with distinguishing regular hyper-
graphs exists. In fact more generally, maximally connected subsets of nodes that share the same value of
12Under review as submission to TMLR
GWL-1, which act like regular hypergraphs, are indistinguishable. To address this issue while respecting
the structure of a hypergraph, we have devised a preprocessing algorithm that improves the expressivity
of any GWL-1 based learner. The algorithm searches for indistinguishable regular subhypergraphs and
simplifies them by a single hyperedge that covers the nodes of the subhypergraph. We perform extensive
experiments to evaluate the effectiveness of our approach and make empirical observations about the output
of the algorithm on hypergraph data.
13Under review as submission to TMLR
References
Sameer Agarwal, Kristin Branson, and Serge Belongie. Higher order learning with graphs. In Proceedings of
the 23rd international conference on Machine learning , pp. 17–24, 2006.
Emily Alsentzer, Samuel Finlayson, Michelle Li, and Marinka Zitnik. Subgraph neural networks. Advances
in Neural Information Processing Systems , 33:8017–8029, 2020.
Ilya Amburg, Nate Veldt, and Austin R. Benson. Clustering in graphs and hypergraphs with categorical
edge labels. In Proceedings of the Web Conference , 2020a.
Ilya Amburg, Nate Veldt, and Austin R Benson. Fair clustering for diverse and experienced groups.
arXiv:2006.05645 , 2020b.
Devanshu Arya, Deepak K Gupta, Stevan Rudinac, and Marcel Worring. Hypersage: Generalizing inductive
representation learning on hypergraphs. arXiv preprint arXiv:2010.04558 , 2020.
Song Bai, Feihu Zhang, and Philip HS Torr. Hypergraph convolution and hypergraph attention. Pattern
Recognition , 110:107637, 2021.
Pierre Baldi and Peter Sadowski. The dropout learning algorithm. Artificial intelligence , 210:78–122, 2014.
MikhailBelkinandParthaNiyogi. Laplacianeigenmapsfordimensionalityreductionanddatarepresentation.
Neural computation , 15(6):1373–1396, 2003.
Austin R. Benson, Rediet Abebe, Michael T. Schaub, Ali Jadbabaie, and Jon Kleinberg. Simplicial closure
and higher-order link prediction. Proceedings of the National Academy of Sciences , 2018a. ISSN 0027-8424.
doi: 10.1073/pnas.1800683115.
Austin R Benson, Rediet Abebe, Michael T Schaub, Ali Jadbabaie, and Jon Kleinberg. Simplicial closure and
higher-order link prediction. Proceedings of the National Academy of Sciences , 115(48):E11221–E11230,
2018b.
Garrett Birkhoff. Lattice theory , volume 25. American Mathematical Soc., 1940.
Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Lio, Guido F Montufar, and Michael
Bronstein. Weisfeiler and lehman go cellular: Cw networks. Advances in Neural Information Processing
Systems, 34:2625–2640, 2021.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translat-
ing embeddings for modeling multi-relational data. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahra-
mani, and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems , volume 26. Cur-
ran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper_files/paper/2013/file/
1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf .
Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M Bronstein. Improving graph neural
network expressivity via subgraph isomorphism counting. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 45(1):657–668, 2022.
Derun Cai, Moxian Song, Chenxi Sun, Baofeng Zhang, Shenda Hong, and Hongyan Li. Hypergraph structure
learning for hypergraph neural networks. In Proceedings of the Thirty-First International Joint Conference
on Artificial Intelligence, IJCAI-22 , pp. 1923–1929, 2022.
Can Chen and Yang-Yu Liu. A survey on hyperlink prediction. arXiv preprint arXiv:2207.02911 , 2022.
Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional
networks. In International conference on machine learning , pp. 1725–1735. PMLR, 2020.
14Under review as submission to TMLR
Samantha Chen, Sunhyuk Lim, Facundo Memoli, Zhengchao Wan, and Yusu Wang. Weisfeiler-lehman
meets gromov-Wasserstein. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang
Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning ,
volume 162 of Proceedings of Machine Learning Research , pp. 3371–3416. PMLR, 17–23 Jul 2022. URL
https://proceedings.mlr.press/v162/chen22o.html .
Samantha Chen, Sunhyuk Lim, Facundo Mémoli, Zhengchao Wan, and Yusu Wang. The weisfeiler-lehman
distance: Reinterpretation and connection with gnns. arXiv preprint arXiv:2302.00713 , 2023.
Eli Chien, Chao Pan, Jianhao Peng, and Olgica Milenkovic. You are allset: A multiset function framework
for hypergraph neural networks. arXiv preprint arXiv:2106.13264 , 2021.
Eli Chien, Chao Pan, Jianhao Peng, and Olgica Milenkovic. You are allset: A multiset function framework
for hypergraph neural networks. In International Conference on Learning Representations , 2022. URL
https://openreview.net/forum?id=hpBTIv2uy_E .
Uthsav Chitra and Benjamin Raphael. Random walks on hypergraphs with edge-dependent vertex weights.
In KamalikaChaudhuri andRuslan Salakhutdinov (eds.), Proceedings of the 36th International Conference
on Machine Learning , volume 97 of Proceedings of Machine Learning Research , pp. 1172–1181. PMLR,
09–15 Jun 2019. URL https://proceedings.mlr.press/v97/chitra19a.html .
Yun Young Choi, Sun Woo Park, Youngho Woo, and U Jin Choi. Cycle to clique (cy2c) graph neural
network: A sight to see beyond neighborhood aggregation. In The Eleventh International Conference on
Learning Representations .
Nicolas A Crossley, Andrea Mechelli, Petra E Vértes, Toby T Winton-Brown, Ameera X Patel, Cedric E
Ginestet, Philip McGuire, and Edward T Bullmore. Cognitive relevance of the community structure of
the human brain functional coactivation network. Proceedings of the National Academy of Sciences , 110
(28):11583–11588, 2013.
Yihe Dong, Will Sawin, and Yoshua Bengio. Hnhn: Hypergraph networks with hyperedge neurons. arXiv
preprint arXiv:2006.12278 , 2020.
David Steven Dummit and Richard M Foote. Abstract algebra , volume 3. Wiley Hoboken, 2004.
Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph neural networks. In
Proceedings of the AAAI conference on artificial intelligence , volume 33, pp. 3558–3565, 2019.
Yifan Feng, Jiashu Han, Shihui Ying, and Yue Gao. Hypergraph isomorphism computation. arXiv preprint
arXiv:2307.14394 , 2023.
Yue Gao, Yifan Feng, Shuyi Ji, and Rongrong Ji. Hgnn+: General hypergraph neural networks. IEEE
Transactions on Pattern Analysis and Machine Intelligence , 2022.
Johannes Gasteiger, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate: Graph
neural networks meet personalized pagerank. arXiv preprint arXiv:1810.05997 , 2018.
LorenzoGiusti, ClaudioBattiloro, LuciaTesta, PaoloDiLorenzo, StefaniaSardellitti, andSergioBarbarossa.
Cell attention networks. arXiv preprint arXiv:2209.08179 , 2022.
Chris Godsil and Gordon F Royle. Algebraic graph theory , volume 207. Springer Science & Business Media,
2001.
Saiping Guan, Xiaolong Jin, Yuanzhuo Wang, and Xueqi Cheng. Link prediction on n-ary relational data.
InProceedings of the 28th International Conference on World Wide Web (WWW’19) , pp. 583–593, 2019.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances
in neural information processing systems , 30, 2017.
Allen Hatcher. Algebraic topology . Web, 2005.
15Under review as submission to TMLR
Yang Hu, Xiyuan Wang, Zhouchen Lin, Pan Li, and Muhan Zhang. Two-dimensional weisfeiler-lehman graph
neural networks for link prediction. arXiv preprint arXiv:2206.09567 , 2022.
Jing Huang and Jie Yang. Unignn: a unified framework for graph and hypergraph neural networks. arXiv
preprint arXiv:2105.00956 , 2021.
EunJeong Hwang, Veronika Thost, Shib Sankar Dasgupta, and Tengfei Ma. An analysis of virtual nodes in
graph neural networks for link prediction. In The First Learning on Graphs Conference , 2022.
Jinwoo Kim, Saeyoon Oh, Sungjun Cho, and Seunghoon Hong. Equivariant hypergraph neural networks. In
European Conference on Computer Vision , pp. 86–103. Springer, 2022.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv
preprint arXiv:1609.02907 , 2016a.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308 , 2016b.
Oliver Knill. A brouwer fixed-point theorem for graph endomorphisms. Fixed Point Theory and Applications ,
2013(1):1–24, 2013.
Andreas Krebs and Oleg Verbitsky. Universal covers, color refinement, and two-variable counting logic:
Lower bounds for the depth. In 2015 30th Annual ACM/IEEE Symposium on Logic in Computer Science ,
pp. 689–700. IEEE, 2015.
Dongjin Lee and Kijung Shin. I’m me, we’re us, and i’m us: Tri-directional contrastive learning on hyper-
graphs.arXiv preprint arXiv:2206.04739 , 2022.
Jure Leskovec, Jon Kleinberg, and Christos Faloutsos. Graph evolution: Densification and shrinking diam-
eters.ACM Transactions on Knowledge Discovery from Data , 1(1), 2007. doi: 10.1145/1217299.1217301.
URL https://doi.org/10.1145/1217299.1217301 .
Dong Li, Zhiming Xu, Sheng Li, and Xin Sun. Link prediction in social networks based on hypergraph. In
Proceedings of the 22nd international conference on world wide web , pp. 41–42, 2013.
Mengran Li, Yong Zhang, Xiaoyong Li, Yuchen Zhang, and Baocai Yin. Hypergraph transformer neural
networks. ACM Transactions on Knowledge Discovery from Data , 17(5):1–22, 2023a.
Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance encoding: Design provably more
powerful neural networks for graph representation learning. Advances in Neural Information Processing
Systems, 33:4465–4478, 2020.
Shouheng Li, Dongwoo Kim, and Qing Wang. Local vertex colouring graph neural networks. 2023b.
Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar Bhalerao, and Ser Nam Lim.
Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods. Advances
in Neural Information Processing Systems , 34:20887–20902, 2021.
Francois Lorrain and Harrison C White. Structural equivalence of individuals in social networks. The Journal
of mathematical sociology , 1(1):49–80, 1971.
Linyuan Lü, Matúš Medo, Chi Ho Yeung, Yi-Cheng Zhang, Zi-Ke Zhang, and Tao Zhou. Recommender
systems. Physics reports , 519(1):1–49, 2012.
HaggaiMaron,HeliBen-Hamu,NadavShamir,andYaronLipman. Invariantandequivariantgraphnetworks.
arXiv preprint arXiv:1812.09902 , 2018.
Rossana Mastrandrea, Julie Fournet, and Alain Barrat. Contact patterns in a high school: A comparison
between data collected using wearable sensors, contact diaries and friendship surveys. PLOS ONE , 10(9):
e0136497, 2015. doi: 10.1371/journal.pone.0136497. URL https://doi.org/10.1371/journal.pone.
0136497.
16Under review as submission to TMLR
Pál András Papp and Roger Wattenhofer. A theoretical comparison of graph neural network extensions. In
International Conference on Machine Learning , pp. 17323–17345. PMLR, 2022.
Pál András Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Random dropouts
increase the expressiveness of graph neural networks. Advances in Neural Information Processing Systems ,
34:21997–22009, 2021.
Petar Ristoski and Heiko Paulheim. Rdf2vec: Rdf graph embeddings for data mining. In The Seman-
tic Web–ISWC 2016: 15th International Semantic Web Conference, Kobe, Japan, October 17–21, 2016,
Proceedings, Part I 15 , pp. 498–514. Springer, 2016.
Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional
networks on node classification. arXiv preprint arXiv:1907.10903 , 2019.
Nicolò Ruggeri, Federico Battiston, and Caterina De Bacco. A framework to generate hypergraphs with
community structure. arXiv preprint arXiv:2212.08593 , 22, 2023.
Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random features strengthen graph neural networks.
InProceedings of the 2021 SIAM international conference on data mining (SDM) , pp. 333–341. SIAM,
2021.
Sameer Singh, Amarnag Subramanya, Fernando Pereira, and Andrew McCallum. Wikilinks: A large-scale
cross-document coreference corpus labeled via links to Wikipedia. Technical Report UM-CS-2012-015,
2012.
Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June (Paul) Hsu, and Kuansan Wang. An
overview of microsoft academic service (MAS) and applications. In Proceedings of the 24th International
Conference on World Wide Web . ACM Press, 2015. doi: 10.1145/2740908.2742839. URL https://doi.
org/10.1145/2740908.2742839 .
Balasubramaniam Srinivasan and Bruno Ribeiro. On the equivalence between positional node embeddings
and structural graph representations. arXiv preprint arXiv:1910.00452 , 2019.
Balasubramaniam Srinivasan, Da Zheng, and George Karypis. Learning over families of sets-hypergraph
representation learning for higher order tasks. In Proceedings of the 2021 SIAM International Conference
on Data Mining (SDM) , pp. 756–764. SIAM, 2021.
Juliette Stehlé, Nicolas Voirin, Alain Barrat, Ciro Cattuto, Lorenzo Isella, Jean-François Pinton, Marco
Quaggiotto, Wouter Van den Broeck, Corinne Régis, Bruno Lina, et al. High-resolution measurements of
face-to-face contact patterns in a primary school. PloS one , 6(8):e23176, 2011.
QiaoyuTan, XinZhang,NinghaoLiu,DaochenZha, LiLi, RuiChen, Soo-HyunChoi,andXiaHu. Bringyour
own view: Graph neural networks for link prediction with personalized subgraph selection. In Proceedings
of the Sixteenth ACM International Conference on Web Search and Data Mining , pp. 625–633, 2023.
Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio.
Graph attention networks. arXiv preprint arXiv:1710.10903 , 2017.
Changlin Wan, Muhan Zhang, Wei Hao, Sha Cao, Pan Li, and Chi Zhang. Principled hyperedge prediction
with structural spectral features and neural networks. arXiv preprint arXiv:2106.04292 , 2021.
Haorui Wang, Haoteng Yin, Muhan Zhang, and Pan Li. Equivariant and stable positional encoding for more
powerful graph neural networks. arXiv preprint arXiv:2203.00199 , 2022.
Xiyuan Wang, Pan Li, and Muhan Zhang. Improving graph neural networks on multi-node tasks with
labeling tricks. arXiv preprint arXiv:2304.10074 , 2023.
Tianxin Wei, Yuning You, Tianlong Chen, Yang Shen, Jingrui He, and Zhangyang Wang. Augmentations
in hypergraph contrastive learning: Fabricated and generative. arXiv preprint arXiv:2210.03801 , 2022.
17Under review as submission to TMLR
Boris Weisfeiler and Andrei Leman. The reduction of a graph to canonical form and the algebra which
appears therein. nti, Series , 2(9):12–16, 1968.
Jianfeng Wen, Jianxin Li, Yongyi Mao, Shini Chen, and Richong Zhang. On the representation and embed-
ding of knowledge bases beyond binary relations. arXiv preprint arXiv:1604.08642 , 2016.
Asiri Wijesinghe and Qing Wang. A new perspective on" how graph neural networks go beyond weisfeiler-
lehman?". In International Conference on Learning Representations , 2021.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive
survey on graph neural networks. IEEE transactions on neural networks and learning systems , 32(1):4–24,
2020.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks?
arXiv preprint arXiv:1810.00826 , 2018.
Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand Louis, and Partha Talukdar.
Hypergcn: A new method for training graph convolutional networks on hypergraphs. Advances in neural
information processing systems , 32, 2019.
Hao Yin, Austin R. Benson, Jure Leskovec, and David F. Gleich. Local higher-order graph clustering.
InProceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining. ACM Press, 2017. doi: 10.1145/3097983.3098069. URL https://doi.org/10.1145/3097983.
3098069.
Jiaxuan You, Jonathan M Gomes-Selman, Rex Ying, and Jure Leskovec. Identity-aware graph neural net-
works. In Proceedings of the AAAI conference on artificial intelligence , volume 35, pp. 10737–10745,
2021.
Dingyi Zeng, Wanlong Liu, Wenyu Chen, Li Zhou, Malu Zhang, and Hong Qu. Substructure aware graph
neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 37, pp. 11129–
11137, 2023.
Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of gnns via graph
biconnectivity. arXiv preprint arXiv:2301.09505 , 2023.
Muhan Zhang and Yixin Chen. Weisfeiler-lehman neural machine for link prediction. In Proceedings of the
23rd ACM SIGKDD international conference on knowledge discovery and data mining , pp. 575–583, 2017.
Muhan Zhang and Pan Li. Nested graph neural networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S.
Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems , volume 34,
pp. 15734–15747. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/
paper/2021/file/8462a7c229aea03dde69da754c3bbcc4-Paper.pdf .
Muhan Zhang, Zhicheng Cui, Shali Jiang, and Yixin Chen. Beyond link prediction: Predicting hyperlinks
in adjacency space. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 32, 2018.
MuhanZhang, PanLi, YinglongXia, KaiWang, andLongJin. Labelingtrick: Atheoryofusinggraphneural
networks for multi-node representation learning. Advances in Neural Information Processing Systems , 34:
9061–9073, 2021.
Ruochi Zhang, Yuesong Zou, and Jian Ma. Hyper-sagnn: a self-attention based graph neural network for
hypergraphs. arXiv preprint arXiv:1911.02613 , 2019.
Simon Zhang, Soham Mukherjee, and Tamal K Dey. Gefl: Extended filtration learning for graph classifica-
tion. InLearning on Graphs Conference , pp. 16–1. PMLR, 2022.
Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng
Li, and Maosong Sun. Graph neural networks: A review of methods and applications. AI open, 1:57–81,
2020.
18Under review as submission to TMLR
Appendix
A More Background
We discuss in this section about the basics of graph representation learning and link prediction. Graphs
are hypergraphs with all hyperedges of size 2. Simplicial complexes and hypergraphs are generalizations of
graphs. We also discuss more related work.
A.1 Graph Neural Networks and Weisfeiler-Lehman 1
The Weisfeiler-Lehman (WL-1) algorithm is an isomorphism testing approximation algorithm. It involves
repeatedly message passing all nodes with their neighbors, a step called node label refinement. The WL-1
algorithm never gives false negatives when predicting whether two graphs are isomorphic. In other words,
two isomorphic graphs are always indistinguishable by WL-1.
The WL-1 algorithm is the following successive vertex relabeling applied until convergence on a graph
G= (X,A)(a pair of the set of node attributes and the graph’s adjacency structure):
h0
v←Xv,∀v∈VG
hi+1
v←{{ (hi
v,hi
u)}}u∈NbrA(v),∀v∈VG(8)
The algorithm terminates after the vertex labels converge. For graph isomorphism testing, the concatenation
of the histograms of vertex labels for each iteration is output as the graph representation. Since we are only
concerned with node isomorphism classes, we ignore this step and just consider the node labels hi
vfor every
v∈VC.
The WL-1 isomorphism test can be characterized in terms of rooted tree isomorphisms between the universal
covers for connected graphs Krebs & Verbitsky (2015). There have also been characterizations of WL-1 in
terms of counting homomorphisms Knill (2013) as well as the Wasserstein Distance Chen et al. (2022) and
Markov chains Chen et al. (2023).
A graph neural network (GNN) is a message passing based node representation learner modeled after the
WL-1 algorithm. It has the important inductive bias of being equivariant to node indices. As a neural
model of the WL-1 algorithm, it learns neural weights common across all nodes in order to obtain a vector
representation for each node. A GNN must use some initial node attributes in order to update its neural
weights. There are many variations on GNNs, including those that improve the distinguishing power beyond
WL-1. For two surveys on the GNNs and their applications, see Zhou et al. (2020); Wu et al. (2020).
A.2 Link Prediction
The task of link prediction on graphs involves the prediction of the existence of links. There are two kinds of
link prediction. There is transductive link prediction where the same nodes are used for all of train validation
and testing. There is also inductive link prediction where the test validation and training nodes can all be
disjoint. Some existing works on link prediction include Zhang & Chen (2017). Higher order link prediction
is a generalization of link prediction to hypergraph data.
A common way to do link prediction is to compute a node-based GNN and for a pair of nodes, aggregate,
similar to in graph auto encoders Kipf & Welling (2016b), the node representations in any target pair in
order to obtain a 2-node representation. Such aggregations are of the form:
h(S={u,v}) =σ(hu·hv) (9)
19Under review as submission to TMLR
whereSis a pair of nodes. As shown in Proposition B.4, this guaranteems an equivariant 2-node represen-
tation but can often give false predictions even with a fully expressive node-based GNN Wang et al. (2023).
A common remedy for this problem is to introduce positional encodings such as SEAL Wang et al. (2022)
and DistanceEncoding Li et al. (2020). Positional encodings encode the relative distances amongst nodes
via a low distortion embedding for example. In the related work section we have gone over many of these
embeddings. We have also used these in our evaluation since they are common practice and must exist to
compute a hypergraph neural network if there are no ground truth node attributes. According to Srinivasan
& Ribeiro (2019), fully expressive pairwise node representations, as defined by 2-node invariance and expres-
sivity, can be represented by some fully expressive positional embedding, which is a positional embedding
that is injective on the node pair isomorphism classes. It is not clear how one would achieve this in practice,
however. Another remedy is to increase the expressive power of WL-1 to WL-2 for link prediction Hu et al.
(2022).
A.3 More Related Work
The work of Wei et al. (2022) also does a data augmentation scheme. It considers randomly dropping edges
and generating data through a generative model on hypergraphs. The work of Lee & Shin (2022) also
performs data augmentation on a hypergraph so that homophilic relationships are maintained. It does this
through contrastive losses at the node to node, hyperedge to hyperedge and intra hyperedge level. Neither
of these methods provide guarantees for their data augmentations.
Asmentionedinthemaintext, anensembleofneuralnetworkscanbeusedwithadrop-outBaldi&Sadowski
(2014) like method on the output of the Algorithm. Subgraph neural networks Alsentzer et al. (2020); Tan
et al. (2023) are ensembles of models on subgraphs of the input graph.
Some more of the many existing hypergraph neural network architectures include: Kim et al. (2022); Cai
et al. (2022); Chien et al. (2021); Bai et al. (2021); Li et al. (2023a); Arya et al. (2020).
20Under review as submission to TMLR
B Proofs
In this section we provide the proofs for all of the results in the main paper along with some additional
theory.
B.1 Hypergraph Isomorphism
We first repeat the definition of a hypergraph and its corresponding matrix representation called the star
expansion matrix::
Definition B.1. An undirected hypergraph is a pair H= (V,E)consisting of a set of vertices Vand a set
of hyperedgesE⊂2V\({∅}∪{{v}|v∈V} )where 2Vis the power set of the vertex set V.
Definition B.2. The star expansion incidence matrix Hof a hypergraphH= (V,E)is the|V|× 2|V|0-1
incidence matrix HwhereHv,e= 1iffv∈efor(v,e)∈V×E for some fixed orderings on both Vand2V.
We recall the definition of an isomorphism between hypergraphs:
Definition B.3. For two hypergraphs HandD, a structure preserving map ρ:H → D is a pair of
mapsρ= (ρV:VH→ VD,ρE:EH→ ED)such that∀e∈ EH,ρE(e)≜{ρV(vi)|vi∈e} ∈ ED. A
hypergraph isomorphism is a structure preserving map ρ= (ρV,ρE)such that both ρVandρEare bijective.
Two hypergraphs are said to be isomorphic, denoted as H∼=D, if there exists an isomorphism between them.
WhenH=D, an isomorphism ρis called an automorphism on H. All the automorphisms form a group,
which we denote as Aut(H).
The action of π∈Sym(V)on the star expansion adjacency matrix His repeated here for convenience:
(π·H)v,e=(u1...v...uk)≜Hπ−1(v),π−1(e)=(π−1(u1)...π−1(v)...π−1(uk)) (10)
Based on the group action, consider the stabilizer subgroup of Sym(V)on the star expansion adjacency
matrixHdefined as follows:
StabSym (V)(H) ={π∈Sym(V)|π·H=H} (11)
For simplicity we omit the lower index when the permutation group is clear from the context. It can be
checked that Stab(H)≤Sym(V)is a subgroup. Intuitively, Stab(H)consists of all permuations that leave
Hfixed.
For a given hypergraph H= (V,E), there is a relationship between the group of hypergraph automorphisms
Aut(H)and the stabilizer group Stab(H)on the star expansion adjacency matrix.
Proposition B.1. Aut(H)∼=Stab(H)are equivalent as isomorphic groups.
Proof.Considerρ∈Aut(H), define the map Φ :ρ∝⇕⊣√∫⊔≀→π:=ρ|V(H). The group element π∈Sym(V)acts as a
stabilizer of Hsince for any entry (v,e)inH,Hπ−1(v),π−1(e)= (π·H)v,e= 1iffπ−1(e)∈EHiffe∈EHiff
Hv,e= 1 =Hπ◦π−1(v),π◦π−1(e). Since (v,e)was arbitrary, πpreserves the positions of the nonzeros.
We can check that Φis a well defined injective homorphism as a restriction map. Furthermore it is surjective
since for any π∈Stab(H), we must have Hv,e= 1iff(π·H)v,e=Hπ−1(v),π−1(e)= 1which is equivalent to
v∈e∈Eiffπ(v)∈π(e)∈Ewhich implies e∈Eiffπ(e)∈E. Thus Φis a group isomorphism from Aut(H)
toStab(H)
In other words, to study the symmetries of a given hypergraph H, we can equivalently study the auto-
morphisms Aut(H)and the stabilizer permutations Stab(H)on its star expansion adjacency matrix H.
Intuitively, the stabilizer group 0≤Stab(H)≤Sym(V)characterizes the symmetries in a graph. When
the graph has rich symmetries, say a complete graph, Stab(H) =Sym(V)can be as large as the whole
permutaion group.
Nontrivial symmetries can be represented by isomorphic node sets which we define as follow:
21Under review as submission to TMLR
Definition B.4. For a given hypergraph Hwith star expansion matrix H, twok-node setsS,T⊆Vare
calledisomorphic , denoted as S≃T, if∃π∈Stab(H),π(S) =Tandπ(T) =S.
Whenk= 1, we have isomorphic nodes, denoted u∼=Hvforu,v∈V. Node isomorphism is also studied
as the so-called structural equivalence in Lorrain & White (1971). Furthermore, if S≃Twe can then say
that there is a matching amongst the nodes in the two node subsets so that matched nodes are isomorphic.
Definition B.5. Ak-node representation hisk-permutation equivariant if:
for allπ∈Sym(V),S∈2Vwith|S|=k:h(π·S,H) =h(S,π·H)
Proposition B.2. Ifk-node representation hisk-permutation equivariant, then hisk-node invariant.
Proof.givenS,S′∈Cwith|S|=|S′|=k,
if there exists a π∈Stab(H)(meaningπ·H=H) andπ(S) =S′then
h(S′,H) =h(S′,π·H)(byπ·H=H)
=h(S,H)(byk-permutation equivariance of handπ(S) =S′)(12)
22Under review as submission to TMLR
B.2 Properties of GWL-1
Here are the steps of the GWL-1 algorithm on the star expansion matrix Hwith node attributes Xis
repeated here for convenience:
f0
e←{},h0
v←Xv
fi+1
e←{{ (fi
e,hi
v)}}v∈e,∀e∈E(H)
hi+1
v←{{ (hi
v,fi+1
e)}}v∈e,∀v∈V(H)(13)
WhereE(H)denotes the nonzero columns of HandV(H)denotes the rows of H.
We make the following observations about each of the two steps of the GWL-1 algorithm:
Observation 2.
{{(fi
e,hi
v)}}v∈e={{(f′i
e,h′i
v)}}v∈eiff(fi
e,{{hi
v}}v∈e) = (f′i
e,{{h′i
v}}v∈e)∀e∈E(H)and (14a)
{{(hi
v,fi+1
e}}v∈e={{(h′i
v,f′i+1
e}}v∈eiff(hi
v,{{fi+1
e}}v∈e) = (h′i
v,{{f′i+1
e}}v∈e)∀v∈V(H)(14b)
Proof.Equation 14a follows since
{{(fi
e,hi
v)}}v∈e={{(f′i
e,h′i
v)}}v∈e∀e∈E(H) (15a)
ifffi
e=f′i
eand{{hi
v}}v∈e={{h′i
v}}v∈e∀e∈E(H) (15b)
iff(fi
e,{{hi
v}}v∈e) = (f′i
e,{{h′i
v}}v∈e)∀e∈E(H) (15c)
For Equation 14b, we have:
{{(hi
v,fi+1
e}}v∈e={{(h′i
v,f′i+1
e}}v∈e∀v∈V(H) (16a)
iff{{(hi
v,{{(fi
e,hi
u)}}u∈e)}}v∈e={{(h′i
v,{{(f′i
e,h′i
u)}}u∈e)}}v∈e∀v∈V(H) (16b)
iffhi
v=h′i
vand{{(fi
e,hi
u)}}u∈e,v∈e={{(f′i
e,h′i
u)}}u∈e,v∈e∀v∈V(H) (16c)
iffhi
v=h′i
vand{{fi+1
e}}={{f′i+1
e}}∀v∈V(H) (16d)
These follow by the definition of multiset equality and since there is no loss of information upon factoring
out a constant tuple entry of each pair in the multisets.
Proposition B.3. The update steps of GWL-1: fi(H)≜[fi
e1(H),···fi
em(H)]andhi(H)≜
[hi
v1(H),···hi
vn(H)], are permutation equivariant; in other words, For any π∈Sym(V), letπ·fi(H)≜
[fi
π−1(e1)(H),···,fi
π−1(em)(H)]andπ·hi(H)≜[hi
π−1(v1)(H),···hi
π−1(vn)(H)], we have∀i∈N,π·fi(H) =
fi(π·H)andπ·hi(H) =hi(π·H)
Proof.We prove by induction on i:
Base case,i= 0:
[π·f0(H)]e={v1...vk}={}=f0
π−1(e)={π−1(v1)...π−1(vk)}(H) =f0
e(π·H)since theπcannot affect a list of
empty sets and the definition of the action of πonHas defined in Equation 10.
[π·h0(H)]v= [π·X]v=Xπ−1(v)=h0
π−1(v)(H) =h0
v(π·H)by definition of the group action Sym(V)acting
on the node indices of a node attribute tensor as defined in Equation 10.
Induction Hypothesis:
[π·fi(H)]e=fi
π−1(e)(H) =fi
e(π·H)and[π·hi(H)]v=hi
π−1(v)(H) =hi
v(π·H) (17)
Induction Step:
23Under review as submission to TMLR
[π·hi+1(H)]v={{([π·hi(H)]v,[π·fi+1(H)]e)}}v∈e
={{([π·hi(H)]v,{{([π·fi(H)]e,[π·hi(H)]u)}}u∈e)}}v∈e
={{hi
v(π·H),{{(fi
e(π·H),hi
u(π·H))}}u∈e}}v∈e
=hi+1
v(π·H)(18)
[π·fi+1(H)]e={{([π·fi(H)]e,[π·hi(H)]v)}}v∈e
={{(fi
e(π·H),hi
v(π·H)}}v∈e
=fi+1
e(π·H)(19)
Definition B.6. Leth: [V]k×Zn×2n
2→Rdbe ak-node representation on a hypergraph H. LetH∈Zn×2n
2
be the star expansion adjacency matrix of Hfornnodes. The representation hisk-node most expressive if
∀S,S′⊂V,|S|=|S′|=k, the following two conditions are satisfied:
1.hisk-node invariant :∃π∈Stab(H),π(S) =S′=⇒h(S,H) =h(S′,H)
2.hisk-node expressive ∄π∈Stab(H),π(S) =S′=⇒h(S,H)̸=h(S′,H)
LetAGGbe a permutation invariant map from a set of node representations to Rd.
Proposition B.4. Leth(S,H) =AGGv∈S[hi
v(H)]with injective AGG and hi
vpermutation equivariant. The
representation h(S,H)isk-node invariant but not necessarily k-node expressive for Sa set ofknodes.
Proof.∃π∈Stab(H)s.t.π(S) =S′,π·H=H
⇒π(vi) =v′
ifori= 1...|S|,π·H=H
⇒hi
π(v)(H) =hi
v(π·H) =hi
v(H)(By permutation equivariance of hi
vandπ·H=H)
⇒AGGv∈S[hi
v(H)] =AGGv′∈S′[hi
v′(H)](By Proposition B.2 and AGG being permutation invariant)
The converse, that h(S,H)isk-node expressive, is not necessarily true since we cannot guarantee h(S,H) =
h(S′,H)implies the existence of a permutation that maps StoS′(see Zhang et al. (2021)).
A hypergraph can be represented by a bipartite graph BV,EfromVtoEwhere there is an edge (v,e)in the
bipartite graph iff node vis incident to hyperedge e. This bipartite graph BV,Eis called the star expansion
bipartite graph.
We introduce a more structured version of graph isomorphism called a 2-color isomorphism to characterize
hypergraphs. It is a map on 2-colored graphs, which are graphs that can be colored with two colors so that
no two nodes in any graph with the same color are connected by an edge. We define a 2-colored isomorphism
formally here:
Definition B.7. A2-colored isomorphism is a graph isomorphism on two 2-colored graphs that preserves
node colors. In particular, between two graphs G1andG2the vertices of one color in G1must map to
vertices of the same color in G2. It is denoted by ∼=c.
A bipartite graph must always have a 2-coloring. In fact, the 2-coloring with all the nodes in the node
bipartition colored red and all the nodes in the hyperedge bipartition colored blue forms a canonical 2-
coloring ofBV,E. Assume that all star expansion bipartite graphs are canonically 2-colored.
Proposition B.5. We have two hypergraphs (V1,E1)∼=(V2,E2)iffBV1,E1∼=cBV2,E2whereBV,Eis the star
expansion bipartite graph of (V,E)
24Under review as submission to TMLR
Proof.DenoteL(BVi,Ei)as the left hand (red) bipartition of BVi,Eito represent the nodes Viof(Vi,Ei)and
R(BVi,Ei)as the right hand (blue) bipartition of BVi,Eito represent the hyperedges Eiof(Vi,Ei). We use the
left/right bipartition and Vi/Eiinterchangeably since they are in bijection.
⇒If there is an isomorphism π:V1→V 2, this means
•πis a bijection and
•has the structure preserving property that (u1...uk)∈E1iff(π(u1)...π(uk))∈E2.
We may induce a 2-colored isomorphism π∗:V(BV1,E1)→V(BV1,E1)so thatπ∗|L(BV1,E1)=πwhere equality
here means that π∗|L(BV1,E1)acts onL(BV1,E1)the same way that πdoes onV1. Furthermore π∗has the
property that π∗|R(BV1,E1)(u1...uk) = (π(u1)...π(uk)),∀(u1...uk)∈ E 1, following the structure preserving
property of isomorphism π.
The mapπ∗is a bijection by definition of being an extension of a bijection.
The mapπ∗is also a 2-colored map since it maps L(BV1,E1)toL(BV2,E2)andR(BV1,E1)toR(BV2,E2).
We can also check that the map is structure preserving and thus a 2-colored isomorphism since
(ui,(u1...ui...uk))∈ E(BV1,E1),∀i= 1...kiff (ui∈ V 1and (u1...ui...uk)∈ E 1) iffπ(ui)∈ V 2and
(π(u1)...π(ui)...π(uk))∈ E 2iff(π∗(ui),(π∗(u1,...ui,...uk))∈ E(BV2,E2),∀i= 1...k. This follows from π
being structure preserving and the definition of π∗.
⇐If there is a 2-colored isomorphism π∗:BV1,E1→BV2,E2then it has the properties that
•π∗is a bijection,
•(is2-colored):π∗|L(BV1,E1) :L(BV1,E1)→L(BV2,E2)andπ∗|R(BV1,E1):R(BV1,E1)→R(BV2,E2)
•(it is structure preserving): (ui,(u1...ui...uk))∈E(BV1,E1),∀i= 1...kiff(π∗(ui),π∗(u1...ui...uk))∈
E(BV2,E2),∀i= 1...k.
This then means that we may induce a π:V1→V 2so thatπ=π∗|L(BV1,E1).
We can check that πis a bijection since πis the 2-colored bijection π∗restricted to L(BV1,E1), thus remaining
a bijection.
We can also check that πis structure preserving. This means that (u1...uk)∈E1iff(ui,(u1...ui...uk))∈
E(BV1,E1)∀i= 1...kiff(π∗(ui),(π∗(u1...ui...uk)))∈ E(BV2,E2)∀i= 1...kiff(π∗(u1...uk))∈R(BV2,E2)iff
(π(u1)...π(uk))∈E2
We define a topological object for a graph originally from algebraic topology called a universal cover:
Definition B.8. (Hatcher (2005)) A universal covering of a connected graph Gis a (potentially infinite)
graph ˜G, s.t. there is a map pG:˜G→Gcalled the universal covering map where:
1.∀x∈V(˜G),pG|N(x)is an isomorphism onto N(pG(x)).
2.˜Gis simply connected (a tree)
A covering graph is a graph that satisfies property 1 but not necessarily property 2 in Definition B.8. It is
known that a universal covering ˜Gcovers all the graph covers of the graph G. LetTG
xdenote a tree with
rootx. Furthermore, define a rooted isomorphism Gx∼=Hyas an isomorphism between graphs GandH
that mapsxtoyand vice versa. We will use the following result to prove a characterization of GWL-1:
Lemma B.6 (Krebs & Verbitsky (2015)) .LetTandSbe trees and x∈V(T)andy∈V(S)be their vertices
of the same degree with neighborhoods N(x) ={x1,...,xk}andN(y) ={y1,...,yk}. Letr≥1. Suppose that
Tr−1
x∼=Sr−1
yandTr
xi∼=Sr
yifor alli≤k. ThenTr+1
x∼=Sr+1
y.
25Under review as submission to TMLR
A universal cover of a 2-colored bipartite graph is still 2colored. When we lift nodes vand hyperedge nodes
eto their universal cover, we keep their respective red and blue colors.
Define a rooted colored isomorphism Tk
˜e1∼=cTk
˜e2as a colored tree isomorphism where blue/red node ˜e1/˜v1
maps to blue/red node ˜e2/˜v2and vice versa.
In fact, Lemma B.6 holds for 2-colored isomorphisms, which we show below:
Lemma B.7. LetTandSbe2-colored trees and x∈V(T)andy∈V(S)be their vertices of the same degree
with neighborhoods N(x) ={x1,...,xk}andN(y) ={y1,...,yk}. Letr≥1. Suppose that Tr−1
x∼=cSr−1
yand
Tr
xi∼=cSr
yifor alli≤k. ThenTr+1
x∼=cSr+1
y.
Proof.Certainly 2-colored isomorphisms are rooted isomorphisms on 2-colored trees. The converse is true
if the roots match in color since recursively all descendants of the root must match in color.
IfTr−1
x∼=cSr−1
yandTr
xi∼=cSr
yifor alli≤kandN(x) ={x1...xk},N(y) ={y1..yk}, the rootsxandymust
match in color. The neighborhoods N(x)andN(y)then must both be of the opposing color. Since rooted
colored isomorphisms are rooted isomorphisms, we must have Tr−1
x∼=Sr−1
yandTr
xi∼=Sr
yifor alli≤k. By
Lemma B.6, we have Tr+1
x∼=Sr+1
y. Once the roots match in color, a rooted tree isomorphism is the same
as a rooted 2-colored tree isomorphism. Thus, since xandyshare the same color, Tr+1
x∼=cSr+1
y
Theorem B.8. LetH1= (V1,E1)andH2= (V2,E2)be two connected hypergraphs. Let BV1,E1andBV2,E2
be two canonically colored bipartite graphs for H1andH2(vertices colored red and hyperedges colored blue)
For anyi∈N+, for any of the nodes x1∈BV1,e1∈BV1,E1andx2∈BV1,e2∈BV2,E2:
(˜B2i−1
V1,E1)˜e1∼=c(˜B2i−1
V2,E2)˜e2ifffi
e1=fi
e2
(˜B2i
V1,E1)˜x1∼=c(˜B2i
V2,E2)˜x2iffhi
x1=hi
x2, withfi
•,hi
•theith GWL-1 values for the hyperedges and nodes
respectively where e1=pBV1,E1(˜e1),x1=pBV1,E1(˜x1),e2=pBV1,E1(˜e2),x2=pBV1,E1(˜x2). The maps pBV1,E1:
˜BV1,E1→BV1,E1,pBV2,E2:˜BV2,E2→BV2,E2are the universal covering maps of BV1,E1andBV2,E2respectively.
Proof.We prove by induction:
LetTk
˜e1:= ( ˜Bk
V1,E1)˜e1where ˜e1is a pullback of a hyperedge, meaning pBV1,E2(˜e1) =e1. Similarly, let
Tk
˜e2:= ( ˜Bk
V2,E2)˜e2,Tk
˜x1:= ( ˜Bk
V1,E1)˜x1,Tk
˜x2:= ( ˜Bk
V2,E2)˜x2,∀k∈N, where ˜e1,˜e2,˜x1,˜x2are the respective
pullbacks of e1,e2,x1,x2.
Definean( 2-colored)isomorphismofmultisetstomeanthatthereexistsabijectionbetweenthetwomultisets
so that each element in one multiset is ( 2-colored) isomorphic with exactly one other element in the other
multiset.
By Observation 3 we can rewrite GWL-1 as:
f0
e←{},h0
v←Xv (20)
fi+1
e←(fi
e,{{hi
v}}v∈e)∀e∈EH (21)
hi+1
v←(hi
v,{{fi+1
e}}v∈e)∀v∈VH (22)
Base Casei= 1:
T1
˜e1∼=cT1
˜e2iff(T0
˜e1∼=cT0
˜e2and{{T0
˜x1}}˜x1∈N(˜e1)∼=c{{T0
˜x2}}˜x2∈N(˜e2))(By Lemma B.7) (23a)
iff(f0
e1=f0
e2and{{h0
x1}}={{h0
x2}})(By Equation 20) (23b)
ifff1
e1=f1
e2(By Equation 21) (23c)
T2
˜x1∼=cT2
˜x2iff(T0
˜x1∼=cT0
˜x2and{{T1
˜e1}}˜e1∈N(˜x1)∼=c{{T1
˜e2}}˜e2∈N(˜x2))(By Lemma B.7) (24a)
26Under review as submission to TMLR
iff(h0
e1=h0
e2and{{f1
x1}}={{f1
x2}})(By Equation 20) (24b)
ifff1
e1=f1
e2(By Equation 22) (24c)
Induction Hypothesis: For i≥1,T2i−1
˜e1∼=cT2i−1
˜e2ifffi
e1=fi
e2andT2i
˜x1∼=cT2i
˜x2iffhi
x1=hi
x2
Induction Step:
T2i+1
˜e1∼=cT2i+1
˜e2iff(T2i−1
˜e1∼=cT2i−1
˜e2and{{T2i
˜x1}}˜x1∈N(˜e1)∼=c{{T2i
˜x2}}˜x2∈N(˜e2))(By Lemma B.7) (25a)
iff(fi
e1=fi
e2and{{hi
x1}}={{hi
x2}})(By Induction Hypothesis) (25b)
ifffi+1
e1=fi+1
e2(By Equation 21) (25c)
T2i
˜x1∼=cT2i
˜x2iff(T2i−2
˜x1∼=cT2i−2
˜x2and{{T2i−1
˜e1}}˜e1∈N(˜x1)∼=c{{T2i−1
˜e2}}˜e2∈N(˜x2))(By Lemma B.7) (26a)
iff(hi
e1=hi
e2and{{fi
x1}}={{fi
x2}})(By Equation 20) (26b)
iffhi
x1=hi
x2(By Equation 22) (26c)
Observation 3. If the node values for nodes xandyfrom GWL-1 for iiterations on two hypergraphs H1
andH2are the same, then for all jwith 0≤j≤i, the node values for GWL-1 for jiterations on xandy
also agree. In particular deg(x) = deg(y).
Proof.There is a 2-color isomorphism on subtrees (˜Bj
V1,E1)˜xand (˜Bj
V2,E2)˜yof thei-hop subtrees of the
universal covers rooted about nodes x∈V 1andy∈V 2for0≤j≤isince (˜Bi
V1,E1)˜x∼=c(˜Bi
V2,E2)˜y. By
Theorem B.8, we have that GWL-1 returns the same value for xandyfor each 0≤j≤i.
Proposition B.9. If GWL-1 cannot distinguish two connected hypergraphs H1andH2then HyperPageRank
will not either.
Proof.HyperPageRank is defined on a hypergraph with star expansion matrix Has the following stationary
distribution Π:
lim
n→∞(D−1
v·H·D−1
e·HT)n= Π (27)
IfHis a connected bipartite graph, Πmust be the eigenvector of (D−1
v·H·D−1
e·HT)for eigenvalue 1. In
other words, Πmust satisfy
(D−1
v·H·D−1
e·HT)·Π = Π (28)
By Theorem 1 of Huang & Yang (2021), we know that the UniGCN defined by:
hi+1
e←ϕ2(hi
e,hi
v) =We·HT·hi
v (29a)
hi+1
v←ϕ1(hi
v,hi+1
e) =Wv·H·hi+1
e (29b)
for constant WeandWvweight matrices, is equivalent to GWL-1 provided that ϕ1andϕ2are both injective
as functions. Without injectivity, we can only guarantee that if UniGCN distinguishes H1,H2then GWL-1
distinguishes H1,H2. In fact, each matrix power of order nin Equation 27 corresponds to hn
vso long as we
satisfy the following constraints:
We←D−1
e,Wv←D−1
vandh0
v←I (30)
We show that the matrix powers are UniGCN under the constraints of Equation 30 by induction:
27Under review as submission to TMLR
Base Case: n= 0:h0
v=I
Induction Hypothesis: n>0:
(D−1
v·H·D−1
e·HT)n=hn
v (31)
Induction Step:
(D−1
v·H·hn
e) (32a)
= (D−1
v·H·((D−1
e·HT)·hn
v)) (32b)
= (D−1
v·H·D−1
e·HT)·(D−1
v·H·D−1
e·HT)n(32c)
= (D−1
v·H·D−1
e·HT)n+1=hn+1
v (32d)
Since we cannot guarantee that the maps ϕ1andϕ2are injective in Equation 32b, it must be that the output
hn
v, coming from UniGCN with the constraints of Equation 30, is at most as powerful as GWL-1.
In general, injectivity preserves more information. For example, if ϕ1is injective and if ϕ′
1is an arbitrary
map (not guaranteed to be injective) then:
ϕ1(h1) =ϕ1(h2)⇒h1=h2⇒ϕ′
1(h1) =ϕ′
1(h2) (33)
HyperpageRank is exactly as powerful as UniGCN under the constraints of Equation 30. Thus HyperPageR-
ank is at most as powerful as GWL-1 in distinguishing power.
28Under review as submission to TMLR
Figure 4: An illustration of hypergraph symmetry breaking. (c,d) 3-regular hypergraphs C3
4,C3
5with 4and5
nodes respectively and their corresponding universal covers centered at any hyperedge (˜BC3
4)e∗,∗,∗,(˜BC3
5)e∗,∗,∗
with universal covering maps pBC3
4,pBC3
5. (b,e) the hypergraphs ˆC3
4,ˆC3
5, which are C3
4,C3
5with 4,5-sized
hyperedges attached to them and their corresponding universal covers and universal covering maps. (a,f)
are the corresponding bipartite graphs of ˆC3
4,ˆC3
5. (c,d) are indistinguishable by GWL-1 and thus will give
identical node values by Theorem B.8. On the other hand, (b,e) gives node values which are now sensitive
to the the order of the hypergraphs 4,5, also by Theorem B.8.
B.3 Method
We repeat here from the main text the symmetry finding algorithm:
Algorithm 2: A Symmetry Finding Algorithm
Data:HypergraphH= (V,E), represented by its star expansion matrix H.L∈N+is the number
of iterations to run GWL-1.
Result: A pair of collections: (RV={VRj},RE=∪j{ERj})whereRjare disconnected
subhypergraphs exhibiting symmetry in Hthat are indistinguishable by L-GWL-1.
1Edeg←{{deg(v) :v∈e}:∀e∈E}
2UL←hL
v(H);GL←{UL[v] :∀v∈V}; /*UL[v]is the L-GWL-1 value of node v∈ V. */
3BVH,EH←Bipartite (H)/* Construct the bipartite graph from H. */
4RV←{};RE←{}
5forcL∈GLdo
6VcL←{v∈V:UL[v] =cL},EcL←{e∈E:u∈VcL,∀u∈e}
7CcL←ConnectedComponents (HcL= (VcL,EcL))
8forRcL,i∈CcLdo
9if|VRcL,i|≥3and{deg(v) :v∈(VRcL,i)}/∈Edegthen
10RV←RV∪{VRcL,i};RE←RE∪ERcL,i
11 end
12end
13end
14return (RV,RE)
We also repeat here for convenience some definitions used in the proofs. Given a hypergraph H= (V,E), let
VcL:={v∈V:cL=hL
v(H)} (34)
be the set of nodes of the same class cLas determined by L-GWL-1. LetHcLbe an induced subgraph of H
byVcL.
Definition B.9. AL-GWL-1 symmetric induced subhypergraph R⊂HofHis a connected induced subhy-
pergraph determined by VR⊂VH, some subset of nodes that are all indistinguishable amongst each other by
29Under review as submission to TMLR
L-GWL-1:
hL
u(H) =hL
v(H),∀u,v∈VR (35)
WhenL=∞, we call suchRa GWL-1 symmetric induced subhypergraph. Furthermore, if R=H, then we
sayHisGWL-1 symmetric .
Definition B.10. A neighborhood-regular hypergraph is a hypergraph where all neighborhoods of each node
are isomorphic to each other.
Observation 4. A hypergraphHis GWL-1 symmetric if and only if it is L-GWL-1 symmetric for all L≥1
if and only ifHis neighborhood regular.
Proof.
1. First if and only if :
By Theorem B.8, GWL-1 symmetric hypergraph H= (V,E)means that for every pair of nodes u,v∈
V,(˜BV,E)˜u∼=c(˜BV,E)˜v. This implies that for any L≥1,(˜B2L
V,E)˜u∼=c(˜B2L
V,E)˜vby restricting the rooted
isomorphism to 2L-hop rooted subtrees, which means that hL
u(H) =hL
v(H). The converse is true since Lis
arbitrary. If there are no cycles, we can just take the isomorphism for the largest. Otherwise, an isomorphism
can be constructed for L=∞by infinite extension.
2. Second if and only if :
LetpBV,Ebe the universal covering map for BV,E. Denote ˜v,˜uby the lift of some nodes v,u∈VbypBV,E.
Let(˜N(˜u))˜ube the rooted bipartite lift of (N(u))u. IfHisL-GWL-1 symmetric for all L≥1then with
L= 1,(˜B2
V,E)˜u∼=c(˜N(u))˜u∼=c(˜N(v))˜v∼=c(˜B2
V,E)˜v, iff(N(u))u∼=(N(v))˜v,∀u,v∈VsinceN(u)andN(v)
are cycle-less for any u,v∈V. For the converse, assume all nodes v∈Vhave (N(v))v∼=(N1)xfor some
1-hop rooted tree (N1)xrooted at node x, independent of any v∈V. We prove by induction that for all
L≥1and for allv∈V,(˜B2L
V,E)˜v∼=c(˜N2L)xfor a 2L-hop tree (˜N2L)xrooted at node x.
Base case: L= 1is by assumption.
Inductive step: If (˜B2L
V,E)˜v∼=c(N2L)x, we can form (˜B2L+2
V,E)˜vby attaching (˜N(˜u))˜uto each node ˜uin the
2L-th layer of (˜B2L
V,E)˜v∼=c(˜N2L)x. Each (˜N(u))˜uis independent of the root ˜vsince every u∈ Vhas
(˜N(˜u))˜u∼=c(˜N2)xiff(N(˜u))˜u∼=(N1)xfor anxindependent of u∈V. This means (˜B2L+2
V,E)˜v∼=c(˜N2L+2)x
for the same root node xwhere (˜N2L+2)xis constructed in the same manner as (˜B2L
V,E)˜v,∀v∈V.
B.3.1 Algorithm Guarantees
Continuing with the notation, as before, let H= (V,E)be a hypergraph with star expansion matrix H
and let (RV,RE)be the output of Algorithm 1 on HforL∈N+. DenoteCcLas the set of all connected
components ofHcL:
CcL≜{CcL:conn. comp. CcLofHcL} (36)
IfL=∞, then drop the L. Thus, the hypergraphs represented by (RV,RE)come fromCcLfor eachcL.
Let:
ˆHL≜(V,E∪RV) (37)
beHafter adding all the hyperedges from RVand let ˆHLbe the star expansion matrix of the resulting
hypergraph ˆHL. Let:
GL≜{hL
v(H) :v∈V} (38)
be the set of all L-GWL-1 values on H. Let:
VcL,s≜{v∈VcL:v∈R,R∈CcL,|VR|=s} (39)
be the set of all nodes of L-GWL-1 class cLbelonging to a connected component in CcLofs≥1nodes in
HcL, the induced subhypergraph of L-GWL-1. Let:
ScL≜{|VRcL,i|:RcL,i∈CcL} (40)
30Under review as submission to TMLR
be the set of node set sizes of the connected components in HcL.
Proposition B.10. IfL=∞, for any GWL-1 node value cforH, the connected component induced
subhypergraphsRc,i, fori= 1...|Cc|are GWL-1 symmetric and neighborhood-regular.
Proof.LetpBV,Ebe the universal covering map for BV,E. Denote ˜v,˜u,˜v′,˜u′by the lift of some nodes
v,u,v′,u′∈VbypBV,E.
LetL=∞and letHc= (Vc,Ec). For any i, sinceu,v∈Vc,(˜BV,E)u∼=c(˜BV,E)vfor allu,v∈VRc,i.
SinceRc,iis maximally connected we know that every neighborhood NHc(u)foru∈Vcinduced byHchas
NHc(u)∼=N(u)∩Hc. SinceL=∞we have that NHc(u)∼=NHc(v),∀u,v∈VRc,isince otherwise WLOG
there areu′,v′∈VRc,iwithNHc(u′)̸∼=NHc(v′)then WLOG there is some hyperedge e∈ENHc(u′)with
somew∈e,w̸=u′whereecannot be in isomorphism with any e′∈ENHc(v′). For two hyperedges to be in
isomorphism means that their constituent nodes can be bijectively mapped to each other by a restriction of
an isomorphism ϕbetweenNHc(u′),NHc(v′)to one of the hyperedges. This means that (˜BV\{u′},E)wis the
rooted universal covering subtree centered about wnot passing through u′that is connected to u′∈(˜BV,E)u′
bye. However, v′has noeand thus cannot have a Txforx∈V(˜N(v′))vsatisfyingTx∼=c(˜BV\{u′},E)wwith
xconnected to v′by a hyperedge e′isomorphic to ein its neighborhood in (˜BV,E)v′. This contradicts that
(˜BV,E)u′∼=c(˜BV,E)v′.
We have thus shown that all nodes in Vchave isomorphic induced neighborhoods. By the Observation 4,
this is equivalent to saying that Rc,iis GWL-1 symmetric and neighborhood regular.
Definition B.11. A star graph Nxis defined as a tree rooted at xof depth 1. The root xis the only node
that can have degree more than 1.
Lemma B.11. IfL∈N+is small enough so that after running Algorithm 1 on L, for anyL-GWL-1
node class cLonVnone of the discovered VRcL,iare within Lhyperedges away from any VRcL,jfor all
i,j∈1...|CcL|,i̸=j, then after forming ˆHL, the newL-GWL-1 node classes of VRcL,ifori= 1...|CcL|inˆHL
are all the same class c′
Lbut are distinguishable from cLdepending on|VRcL,i|.
Proof.After running Algorithm 1 on H= (V,E), let ˆHL= (ˆVL,ˆEL≜E∪/uniontext
cL,i{VRcL,i})be the hypergraph
formed by attaching a hyperedge to each VRcL,i.
For anycL, aL-GWL-1 node class, let RcL,i,i= 1...|CcL|be a connected component subhypergraph of HcL.
Over all (cL,i)pairs, all theRcL,iare disconnected from each other and for each cLeachRcL,iis maximally
connected onHcL.
Upon covering all the nodes VRcL,iof each induced connected component subhypergraph RcL,iwith a single
hyperedgee=VRcL,iof sizes=|VRcL,i|, we claim that every node of class cLbecomescL,s, aL-GWL-1
node class depending on the original L-GWL-1 node class cLand the size of the hyperedge s.
Consider for each v∈VRcL,ithe2L-hop rooted tree (˜B2L
V,E)˜vforpBV,E(˜v) =v. Also, for each v∈VRcL,i,
define the tree
Te≜(˜B2L−1
ˆV\{v},ˆE)˜e (41)
We do not index the tree Tebyvsince it does not depend on v∈VRcL,i. We prove this in the following.
proof for: Tedoes not depend on v∈VRcL,i:
Let node ˜ebe the lift of eto(˜B2L−1
ˆV,ˆE)˜e. Define the star graph (N(˜e))˜eas the 1-hop neighborhood of ˜ein
(˜B2L−1
ˆV,ˆE)˜e. We must have:
(˜B2L−1
ˆV,ˆE)˜e∼=c((N(˜e))˜e∪/uniondisplay
˜u∈VN(˜e)\{˜e}(˜B2L−2
V,E)˜u)˜e (42)
Define for each node v∈ewith lift ˜v:
(N(˜e,˜v))˜e≜(V(N(˜e))˜e\{˜v},E(N(˜e))˜e\{(˜e,˜v)})˜e (43)
31Under review as submission to TMLR
The tree (N(˜e,˜v))˜eis a star graph with the node ˜vdeleted from (N(˜e))˜e. The star graphs (N(˜e,˜v))˜e⊂
(N(˜e))˜edo not depend on ˜vas long as ˜v∈V(N(˜e))˜e. In other words,
(N(˜e,˜v))˜e∼=c(N(˜e,˜v′))˜e,∀˜v,˜v′∈V(N(˜e))˜e\{˜e} (44)
Since the rooted tree (˜B2L−1
ˆV,ˆE)˜e, where ˜eis the lift of eby universal covering map pBV,E, has all pairs of nodes
˜u,˜u′∈˜ein it with (˜B2L
V,E)˜u∼=c(˜B2L
V,E)˜u′, which implies
(˜B2L−2
V,E)˜u∼=c(˜B2L−2
V,E)˜u′,∀˜u,˜u′∈˜e (45)
By Equations 45, 44, we thus have:
(˜B2L−1
ˆV\{v},ˆE)˜e∼=c((N(˜e,˜v))˜e∪/uniondisplay
˜u∈V(N(˜e,˜v))˜e\{˜e}(˜B2L−2
V,E)˜u)˜e (46)
This proves that Tedoes not need to be indexed by v∈VRcL,i.
We continue with the proof that all nodes in VRcL,ibecome the L-GWL-1 node class cL,sfors=|VRcL,i|.
Since every v∈VRcL,ibecomes connected to a hyperedge e=VRcL,iinˆH, we must have:
(˜B2L
ˆV,ˆE)˜v∼=c((˜B2L
V,E)˜v∪(˜v,˜e)Te)˜v,∀v∈VRcL,i (47)
The notation ((˜B2L
V,E)˜v∪(˜v,˜e)Te)˜vdenotes a tree rooted at ˜vthat is the attachment of the tree Terooted at
eto the node ˜vby the edge (˜v,˜e). As is usual, we assume ˜v,˜eare the lifts of v∈V,e∈Erespectively. We
only need to consider the single esinceLwas chosen small enough so that the 2L-hop tree (˜B2L
ˆV,ˆE)˜vdoes not
contain a node ˜usatisfyingpBV,E(˜u) =uwithu∈VRcL,jfor allj= 1...|CcL|,j̸=i.
SinceTedoes not depend on v∈VRcL,i,
(˜B2L
ˆV,ˆE)˜u∼=c(˜B2L
ˆV,ˆE)˜v,∀u,v∈VRcL,i (48)
This shows that hL
u(ˆH) =hL
v(ˆH),∀u,v∈VRcL,iby Theorem B.8. Furthermore, since each v∈VRcL,i⊂ˆV
inˆHis now incident to a new hyperedge e=VRcL,i, we must have that the L-GWL-1 class cLofVRcL,ion
His now distinguishable by |VRcL,i|.
We will need the following definition to prove the next lemma.
Definition B.12. A partial universal cover of hypergraph H= (V,E)with an unexpanded induced subhy-
pergraphR, denotedU(H,R)V,Eis a graph cover of BV,Ewhere we freezeBVR,ER⊂˜BV,Eas an induced
subgraph.
Al-hop rooted partial universal cover of hypergraph H= (V,E)with an unexpanded induced subhypergraph
R, denoted (Ul(H,R)V,E)˜uforu∈Vor(Ul(H,R)V,E)˜efore∈E, where ˜v,˜eare lifts of v,e, is a rooted
graph cover ofBV,Ewhere we freezeBVR,ER⊂˜BV,Eas an induced subgraph.
Lemma B.12. Assuming the same conditions as Lemma B.11, where H= (V,E)is a hypergraph and
for allL-GWL-1 node classes cLwith connected components RcL,i, as discovered by Algorithm 1, so that
L≥diam (RcL,i). Instead of only adding the hyperedges {VRcL,i}cL,itoEas stated in the main paper, let
ˆH†≜(V,(E\RE)∪RV), meaningHwith eachRcL,ifori= 1...|CcL|having all of its hyperedges dropped
and with a single hyperedge that covers VRcL,iand let ˆH= (V,E∪RV)then:
The GWL-1 node classes of VRcL,ifori= 1...|CcL|inˆHare all the same class c′
Lbut are distinguishable
fromcLdepending on|VRcL,i|.
32Under review as submission to TMLR
Proof.For anycL, aL-GWL-1 node class, let RcL,i,i= 1...|CcL|be a connected component subhypergraph
ofHcL. These connected components are discovered by the algorithm. Over all (cL,i)pairs, all theRcL,i
are disconnected from each other. Upon arbitrarily deleting all hyperedges in each such induced connected
component subhypergraph RcL,iand adding a single hyperedge of size s=|VRcL,i|, we claim that every
node of class cLbecomescL,s, aL-GWL-1 node class depending on the original L-GWL-1 node class cLand
the size of the hyperedge s.
Define the subhypergraph made up of the disconnected components RcL,ias:
R:=/uniondisplay
c,iRcL,i (49)
SinceL≥diam (RcL,i), we can construct the 2L-hop rooted partial universal cover with unexpanded induced
subhypergraphR, denoted by (U2L(H,R)V,E)˜v,∀v∈VofHas given in Definition B.12.
Denote the hyperedge nodes, or right hand nodes of the bipartite graph by B(VR,ER)byR(B(VR,ER)).
Their corresponding hyperedges are ER⊂E(U(H,R))⊂E. Since eachRcL,iis maximally connected, for
any nodesu,v∈VRwe have:
(U2L(H,R)˜u\R(B(VR,ER)))˜u∼=c(U2L(H,R)˜v\R(B(VR,ER)))˜v (50)
by Proposition B.10, where U2L(H,R)˜v\R(B(VR,ER))denotes removing the nodes R(B(VR,ER))from
U2L(H,R)˜v. This follows since removing R(B(VR,ER))removes an isomorphic neighborhood of hyperedges
from each node in VR. This requires assuming maximal connectedness of each RcL,i. Upon adding the
hyperedge
ecL,i≜VRcL,i (51)
covering all ofVRcL,iafter the deletion of ERcL,ifor every (cL,i)pair, we see that any node u∈VRcL,iis
connected to any other node v∈VRcL,ithroughecL,iin the same way for all nodes u,v∈VRcL,i. In fact,
we claim that all the nodes in VRcL,istill have the same GWL-1 class.
We can write ˆH†equivalently as (V,/uniontext
cL,i(E\E (RcL,i)∪{ecL,i})), which is the hypergraph formed by the
algorithm. The replacement operation on Hcan be viewed in the universal covering space ˜BV,Eas taking
U(H,R)and replacing the frozen subgraph BVR,ERwith the star graphs (NˆH†(˜ecL,i))˜ecL,iof root node ˜ecL,i
determined by hyperedge ecL,ifor each connected component indexed by (cL,i). Since the star graphs
(NˆH†(˜ecL,i))˜ecL,iare cycle-less, we have that:
(U(H,R)\R(B(VR,ER)))∪/uniondisplay
cL,i(NˆH†(˜ecL,i))˜ecL,i∼=c˜BVˆH†,EˆH†(52)
Viewing Equation 52 locally, by our assumptions on L, for anyv∈VRcL,i, we must also have:
(U2L(H,R)˜v\R(B(VR,ER)))/uniondisplay
(NˆH†(˜ec,i))˜ec,i∼=c˜BVˆH†,EˆH†(53)
We thus have (˜B2L
VˆH†,EˆH†)˜u∼=c(˜B2L
VˆH†,EˆH†)˜vfor everyu,v∈VRcL,iwith ˜u,˜vbeing the lifts of u,vbypBV,E,
since (U2L(H,R)˜u\R(B(VR,ER)))˜u∼=c(U2L(H,R)˜v\R(B(VR,ER)))˜vfor everyu,v∈VRcL,ias in Equation
50. These rooted universal covers now depend on a new hyperedge ecL,iand thus depend on its size s.
This proves the claim that all the nodes in VRcL,iretain the same L-GWL-1 node class by changing HtoˆH†
and that this new class is distinguishable by s=|VRcL,i|. In otherwords, the new class can be determined
bycs. Furthermore, cL,son the hyperedge ecL,icannot become the same class as an existing class due to
the algorithm.
Theorem B.13. Let|V|=n,L∈N+. Ifvol(v)≜/summationtext
e∈E:e∋v|e|=O(log1−ϵ
4Ln),∀v∈Vfor any constant
ϵ > 0;|ScL|≤S,∀cL∈GL,Sconstant, and|VcL,s|=O(nϵ
log1
2k(n)),∀s∈CcL, then fork∈N+andk-
tupleC= (cL,1...cL,k),cL,i∈GL,i= 1..kthere exists ω(n2kϵ)many pairs of k-node sets S1̸≃S2such
that (hL
u(H))u∈S1= (hL
v∈S2(H)) =C, as ordered k-tuples, while h(S1,ˆHL)̸=h(S2,ˆHL)also byLsteps of
GWL-1.
33Under review as submission to TMLR
Proof.
1. Constructing forests from the rooted universal cover trees :
The first part of the proof is similar to the first part of the proof of Theorem 2 of Zhang et al. (2021).
Consider an arbitrary node v∈Vand denote the 2L-tree rooted at vfrom the universal cover as (˜B2L
V,E)vas
in Theorem B.8. As each node v∈Vhas volume vol(v) =/summationtext
v∈e|e|=O(log1−ϵ
4Ln), then every edge e∈E
has|e|=O(log1−ϵ
4Ln)and for all v∈Vwe have that deg(v) =O(log1−ϵ
4Ln), we can say that every node in
(˜B2L
V,E)˜vhas degreed=O(log1−ϵ
4Ln). Thus, the number of nodes in (˜B2L
V,E)˜v, denoted by|V((˜B2L
V,E)˜v|, satisfies
|V((˜B2L
V,E)˜v|≤/summationtext2L
i=0di=O(d2L) =O(log1−ϵ
2n). We setK≜maxv∈V|V((˜B2L
V,E)˜v|as the maximum number
of nodes of (˜B2L
V,E)˜vand thusK=O(log1−ϵ
2n). For allv∈V, expand trees (˜B2L
V,E)˜vto(˜B2L
V,E)˜vby adding
K−|V((˜B2L
V,E)˜v|independent nodes. Then, all (˜BL
V,E)˜vhave the same number of nodes, which is K, becoming
forests instead of trees.
2. Counting|GL|:
Next, we consider the number of non-isomorphic forests over Knodes. Actually, the number of non-
isomorphic graph over K nodes is bounded by 2(K
2)=exp(O(log1−ϵ
2n)) =o(n1−ϵ). Therefore, due to
the pigeonhole principle, there existn
o(n1−ϵ)=ω(nϵ)many nodes vwhose (˜BL
V,E)˜vare isomorphic to each
other. DenoteGLas the set of all L-GWL-1 values. Denote the set of these nodes as VcL, which consist of
nodes whose L-GWL-1 values are all the same value cL∈GLafterLiterations of GWL-1 by Theorem B.8.
For a fixed L, the setsVcLform a partition of V, in other words,/unionsqtext
cL∈GLVcL=V. Next, we focus on looking
atk-sets of nodes that are not equivalent by GWL-1.
For anycL∈GL, there is a partition VcL=/unionsqtext
sVcL,swhereVcL,sis the set of nodes all of which have L-GWL-
1 classcLand that belong to a connected component of size sinHcL. LetScL≜{|VRcL,i|:RcL,i∈CcL}
denote the set of sizes s≥1of connected component node sets of HcL. We know that|ScL|≤SwhereSis
independent of n.
3. Computing the lower bound:
LetYdenote the number of pairs of k-node setsS1̸≃S2such that (hL
u(H))u∈S1= (hL
v(H))v∈S2=C=
(cL,1...cL,k), as ordered tuples, from L-steps of GWL-1. Since if any pair of nodes u,vhave sameL-GWL-
1 valuescL, then they become distinguishable by the size of the connected component in HcLthat they
belong to. We can lower bound Yby counting over all pairs of ktuples of nodes ((u1...uk),(v1...vk))∈
(/producttextk
i=1VcL,i)×(/producttextk
i=1VcL,i)that both have L-GWL-1 values (cL,1...cL,k)where there is atleast one i∈{1..k}
whereuiandvibelong to different sized connected components si,s′
i∈ScL,iwithsi̸=s′
i. We have:
Y≥1
k![/summationdisplay
((si)k
i=1,(s′
i)k
i=1)∈[(/producttextk
i=1SL
cL,i)]2
:(si)k
i=1̸=(s′
i)k
i=1k/productdisplay
i=1|VL
(cL,i),si||VL
(cL,i),s′
i|] (54a)
=1
k![k/productdisplay
i=1(/summationdisplay
si∈SLci|VL
(cL,i),si|)2−/summationdisplay
(si)k
i=1∈/producttextk
i=1SL
(cL,i)(k/productdisplay
i=1|VL
(cL,i),si|2)] (54b)
Using the fact that for each i∈ {1...k},|VcL,i|=/summationtext
si∈ScL,i|V(cL,i),si|and by assumption |V(cL,i),si|=
O(nϵ
log1
2kn)for anysi∈ScL,i, thus we have:
Y≥ω(n2kϵ)−O(|S|kn2kϵ
logn)] =ω(n2kϵ) (55)
Forthefollowingproof, wewilldenote ∼=Hasanodeorhypergraphisomorphismwithrespecttoahypergraph
H.
34Under review as submission to TMLR
Theorem B.14 (Invariance and Expressivity) .IfL=∞, GWL-1 enhanced by Algorithm 1 is still invari-
ant to node isomorphism classes of Hand can be strictly more expressive than GWL-1 to determine node
isomorphism classes.
Proof.
1. Expressivity :
LetL∈N+be arbitrary. We first prove that L-GWL-1 enhanced by Algorithm 1 is strictly more expressive
for node distinguishing than L-GWL-1 on some hypergraph(s). Let C3
4andC3
5be two 3-regular hypergraphs
from Figure 2. Let H=C3
4/unionsqtextC3
5be the disjoint union of the two regular hypergraphs. Literations of GWL-1
will assign the same node class to all of VH. These two subhypergraphs can be distinguished by L-GWL-1
forL≥1after editing the hypergraph Hfrom the output of Algorithm 1 and becoming ˆH=ˆC3
4∪ˆC3
5. This
is all shown in Figure 2. Since Lwas arbitrary, this is true for L=∞.
2. Invariance :
For any hypergraph H, let ˆH= (ˆV,ˆE)beHmodified by the output of Algorithm 1 by adding hyperedges to
VRc,i. GWL-1 remains invariant to node isomorphism classes of HonˆH.
a. Case 1 :
LetL∈N+be arbitrary. For any node uwithL-GWL-1 class cchanged to csinˆH, ifu∼=Hvfor any
v∈V, then the GWL-1 class of vmust also be cs. In otherwords, both uandvbelong tos-sized connected
components inHcWe prove this by contradiction.
Sayubelong to a L-GWL-1 symmetric induced subhypergraph Swith|VS|=s. Sayvis originally of
L-GWL-1 class cand changes to L-GWL-1cs′fors′< sonˆH, WLOG. If this is the case then vbelongs
to aL-GWL-1 symmetric induced subhypergraph S′with|VS′|=s′. Since there is a π∈Aut(H)with
π(u) =vand sinces′< s, by the pigeonhole principle some node w∈VSmust haveπ(w)/∈VS′. SinceS
andS′are maximally connected, π(w)cannot share the same L-GWL-1 class as w. Thus, it must be that
(˜B2L
V,E)]π(w)̸∼=c(˜B2L
V,E)˜wwhere ˜w,]π(w)are the lifts of w,π(w)by universal covering map pBV,E. However by
the contrapositive of Theorem B.8, this means π(w)̸=w. However wandπ(w)both belong to L-GWL-1
classcinH, meaning (˜B2L
V,E)]π(w)∼=c(˜B2L
V,E)˜w, contradiction. The argument for when vdoes not change its
classcafter the algorithm, follows by noticing that c̸=csas GWL-1 node classes of uandvimpliesu̸∼=Hv
once again by the contrapositive of Theorem B.8. Since Lwas arbitrary, this is true for L=∞
b. Case 2 :
Now assume L=∞. LetpBV,Ebe the universal covering map of BV,E. For all other nodes u′∼=Hv′for
u′,v′∈Vunaffectedbythereplacement, meaningtheydonotbelongtoany Rc,idiscoveredbythealgorithm,
if the rooted universal covering tree rooted at node ˜u′connects to any node ˜winlhops in (˜Bl
V,E)˜u′where
pBV,E(˜u′) =u′,pBV,E( ˜w) =wand where whas any class cinH, then ˜v′must also connect to a node ˜zin
lhops in (˜Bl
V,E)˜u′wherepBV,E(˜z) =zandw∼=Hz. Furthermore, if wbecomes class csinHdue to the
algorithm, then zalso becomes class csinˆH. This will follow by the previous result on isomorphic wandz
both of class cwithwbecoming class csinˆH.
SinceL=∞: For anyw∈Vconnected by some path of hyperedges to u′∈V, consider the smallest lfor
which (˜Bl
V,E)˜u′, thel-hop universal covering tree of Hrooted at ˜u′, the lift of u′, contains the lifted ˜wof
w∈Vwith GWL-1 node class cat layerl. Sinceu′∼=Hv′byπ. We can use πto find some z=π(w).
We claim that ˜zislhops away from ˜v′. Sinceu′∼=Hv′due to some π∈Aut(H)withπ(u′) =v′, using
Proposition B.2 for singleton nodes and by Theorem B.8 we must have (˜Bl
V,E)˜u′∼=c(˜Bl
V,E)˜v′as isomorphic
rooteduniversalcoveringtreesduetoaninducedisomorphism ˜πofπwherewedefineaninducedisomorphism
˜π: (˜BV,E)˜u′→(˜BV,E)˜v′between rooted universal covers (˜BV,E)˜u′and(˜BV,E)˜v′for˜u′,˜v′∈V(˜BV,E)as˜π(˜a) =˜b
ifπ(a) =b∀a,b∈V(BV,E)connected to u′andv′respectively and pBV,E(˜a) =a,pBV,E(˜b) =b. Sincelis the
shortest path distance from ˜u′to˜w, there must exist some shortest (as defined by the path length in BV,E)
pathPof hyperedges from u′towwith no cycles. Using π, we must map Pto another acyclic shortest path
of the same length from v′toz. This path correponds to a llength shortest path from ˜v′to˜zin(˜BV,E)˜v′.
35Under review as submission to TMLR
Ifwhas GWL-1 class cinHthat doesn’t become affected by the algorithm, then zalso has GWL-1 class c
inHsincew∼=Hz.
Ifwhas classcand becomes csinˆH, by the previous result, since w∼=Hzwe must have the GWL-1 classes
c′andc′′ofwandzinˆHbe both equal to cs.
The nodewconnected to u′was arbitrary and so both ˜wand the isomorphism induced ˜zarelhops away
from ˜u′and˜v′respectively, with the same GWL-1 class c′inˆH, thus (˜BˆV,ˆE)˜u′∼=c(˜BˆV,ˆE)˜v′.
We have thus shown, if u∼=Hvforu,v∈H, then in ˆHwe havehL
u(ˆH) =hL
v(ˆH)using the duality between
universal covers and GWL-1 from Theorem B.8.
Proposition B.15 (Complexity) .LetHbe the star expansion matrix of H. Algorithm 1 runs in time
O(L·nnz(H)+(n+m)), the size of the input hypergraph when viewing Las constant, where nis the number
of nodes,nnz(H) =vol(V)≜/summationtext
v∈Vdeg(v)andmis the number of hyperedges.
Proof.Computing Edeg, which requires computing the degrees of all the nodes in each hyperedge takes time
O(nnz(H)). The setEdegcan be stored as a hashset datastructure. Constructing this takes O(nnz(H)).
Computing GWL-1 takes O(L·nnz(H))time assuming a constant Lnumber of iterations. Constructing the
bipartite graphs for Htakes time O(nnz(H) +n+m)since it is an information preserving data structure
change. Define for each c∈C,nc:=|Vc|,mc:=|Ec|. Since the classes partition V, we must have:
n=/summationdisplay
c∈Cnc;m=/summationdisplay
c∈Cmc;nnz(H) =/summationdisplay
c∈Cnnz(Hc) (56)
whereHcis the star expansion matrix of Hc. Extracting the subgraphs can be implemented as a masking
operation on the nodes taking time O(nc)to formVcfollowed by searching over the neighbors of Vcin time
O(mc)to constructEc. Computing the connected components for Hcfor a predicted node class ctakes
timeO(nc+mc+nnz(Hc)). Iterating over each connected component for a given cand extracting their
nodes and hyperedges takes time O(nci+mci)wherenc=/summationtext
inci,mc=/summationtext
imci. Checking that a connected
component has size at least 3takesO(1)time. Computing the degree on Hfor all nodes in the connected
component takes time O(nci)since computing degree takes O(1)time. Checking that the set of node degrees
of the connected component doesn’t belong to Edegcan be implemented as a check that the hash of the set
of degrees is not in the hashset datastructure for Edeg.
Adding up all the time complexities, we get the total complexity is:
O(nnz(H)) +O(nnz(H) +n+m) +/summationdisplay
c∈C(O(nc+mc+nnz(Hc)) +/summationdisplay
conn. comp. iofHcO(nci+mci))(57a)
=O(nnz(H) +n+m) +/summationdisplay
c∈C(O(nc+mc+nnz(Hc)) +O(nc+mc)) (57b)
=O(nnz(H) +n+m) (57c)
Proposition B.16. For a connected hypergraph H, let(RV,RE)be the output of Algorithm 1 on H. Then
there are Bernoulli probabilities p,qifori= 1...|RV|for attaching a covering hyperedge so that ˆπis an
unbiased estimator of π.
Proof.LetCcL={RcL,i}ibe the maximally connected components induced by the vertices with L-GWL-1
valuescL. The set of vertex sets {V(RcL,i)}and the set of all hyperedges ∪i{E(RcL,i)}over all the connected
componentsRcL,ifori= 1...CcLform the pair (RV,RE).
For a hypergraph random walk on connected H= (V,E), its stationary distribution πonVis given by the
closed form:
π(v) =deg(v)/summationtext
u∈Vdeg(u)(58)
36Under review as submission to TMLR
forv∈V.
Let ˆH= (V,ˆE)be the random hypergraph as determined by pandqifori= 1...|RV|. These probabilities
determine ˆHby the following operations on the hypergraph H:
•Attaching a single hyperedge that covers VRcL,iwith probability qiand not attaching with proba-
bility 1−qi.
•All the hyperedges in RcL,iare dropped/kept with probability pand1−prespectively.
1. Setup:
Letdeg(v)≜|{e:e∋v}|forv∈V(H)anddeg(S)≜/summationtext
u∈V(S)deg(u)forS⊂Ha subhypergraph.
Let
Bernoulli (p)≜/braceleftigg
1prob.p
0prob. 1−p(59)
and
Binom (n,p)≜n/summationdisplay
i=1Bernoulli (p) (60)
Define for each v∈V,C(v)to be the unique RcL,iwherev∈RcL,iThis means that we have the following
independent random variables:
Xe≜Bernoulli (1−p),∀e∈E(i.i.d. across all e∈E) (61a)
XC(v)≜Bernoulli (qi) (61b)
As well as the following constant, depending only on C(v):
mC(v)≜/summationdisplay
u∈V\C(v)deg(u) (62)
whereC(v)⊂V,∀v∈V
Letˆπbe the stationary distribution of ˆH. Its expectation E[ˆπ]can be written as:
ˆπ(v)≜/summationtext
e∋vXe+XC(v)
mC(v)+/summationtext
e∋u:u∈C(v)Xe+XC(v)(63)
Letting
Nv≜/summationdisplay
e∋vXe=Binom (deg(v),1−p) (64a)
N≜Nv+XC(v) (64b)
D≜mC(v)+/summationdisplay
e∋u:u∈C(v)Xe+XC(v) (64c)
C≜D−(/summationdisplay
e∋v|e|)Nv−mC(v)=/summationdisplay
e∋u:v/∈e,u∈C(v)Xe−mC(v)=Binom (Fv,1−p)−mc(v) (64d)
whereFv≜|{e∋u:v /∈e,u∈C(v)}|
and so we have: ˆπ(v) =N
D
We have the following joint independence Nv⊥XC(v)⊥Cdue to the fact that each random variable
describes disjoint hyperedge sets.
2. Computing the Expectation:
37Under review as submission to TMLR
Writing out the expectation with conditioning on the joint distribution P(D,Nv,XC(v)), we have:
E[ˆπ(v)] =1/summationdisplay
i=0deg(v)/summationdisplay
j=0deg(C(v))/summationdisplay
k=mC(v)E[ˆπ(v)|D=k,N =j]P(D=k,Nv=j,XC(v)=i) (65a)
=1/summationdisplay
i=0deg(v)/summationdisplay
j=0deg(C(v))/summationdisplay
k=mC(v)1
kE[N|D=k,Nv=j,XC(v)=i]P(D=k,Nv=j,XC(v)=i) (65b)
=1/summationdisplay
i=0deg(v)/summationdisplay
j=0deg(C(v))/summationdisplay
k=mC(v)j+i
kP(D=k,Nv=j,XC(v)=i) (65c)
=1/summationdisplay
i=0deg(v)/summationdisplay
j=0deg(C(v))/summationdisplay
k=mC(v)j+i
kP(D=k)P(Nv=j)P(XC(v)=i) (65d)
=1/summationdisplay
i=0deg(v)/summationdisplay
j=0deg(C(v))/summationdisplay
k=mC(v)j+i
kP(C=k−deg(v)j−mC(v))P(Nv=j)P(XC(v)=i) (65e)
=1/summationdisplay
i=0deg(v)/summationdisplay
j=0deg(C(v))/summationdisplay
k=mC(v)j+i
kP(Binom (Fv,1−p),1−p) =k−deg(v)j−mC(v))
·P(Binom (deg(v),1−p) =j)·P(Bernoulli (q) =i)(65f)
=1/summationdisplay
i=0deg(v)/summationdisplay
j=0deg(C(v))/summationdisplay
k=mC(v)j+i
k/parenleftbiggFv
k−deg(v)j−mC(v)/parenrightbigg
(1
2)Fv·/parenleftbiggdeg(v)
j/parenrightbigg
(1
2)deg(v)·P(Bernoulli (q) =i)(65g)
=1/summationdisplay
i=0deg(v)/summationdisplay
j=0deg(C(v))/summationdisplay
k=mC(v)j+i
k/parenleftbiggFv
k−deg(v)j−mC(v)/parenrightbigg
(1
2)Fv
·/parenleftbiggdeg(v)
j/parenrightbigg
(1
2)deg(v)·P(Bernoulli (q) =i)(65h)
=deg(v)/summationdisplay
j=0deg(C(v))/summationdisplay
k=mC(v)/parenleftbiggFv
k−deg(v)j−mC(v)/parenrightbigg
(1
2)Fv·/parenleftbiggdeg(v)
j/parenrightbigg
(1
2)deg(v)[(1−q)j
k+qj+ 1
k](65i)
=deg(v)/summationdisplay
j=0deg(C(v))/summationdisplay
k=mC(v)/parenleftbiggFv
k−deg(v)j−mC(v)/parenrightbigg
(1−p)Fv−(k−deg(v))pk−deg(v)·/parenleftbiggdeg(v)
j/parenrightbigg
(1−p)jpdeg(v)−j[j
k+q1
k]
(65j)
=C1(p) +qC2(p) (65k)
3. Pickpandq:
We want to find pandqso that E[ˆπ(v)] =C1(p) +qC2(p) =π(v)
We know that for a given p∈[0,1], we must have:
q=π(v)−C1(p)
C2(p)(66)
In order for q∈[0,1], must have π(v)≥C1(p)andπ(v)−C1(p)≤C2(p).
38Under review as submission to TMLR
a. Pickpsufficiently large:
Notice that
0≤C1(p)≤O(E[1
Binom (Fv,1−p) +mC(v)]·E[Binom (deg(v),1−p)]) =O(1
mC(v)deg(v)(1−p))(67)
and that
0≤C1(p)≤C2(p) (68)
forp∈[0,1]sufficiently large. This is because
C1(p)≤O(1
mC(v)deg(v)(1−p)) (69)
and
Ω(1
mC(v)deg(v)(1−p))≤C2(p) (70)
Piecing these two inequalities together gets the desired inequality 68.
We can then pick a p∈[0,1]even larger than the previous pso that for the C′>0which gives C1(p)≤
C′
mC(v)deg(v)(1−p), we achieve
C1(p)≤C′
mC(v)deg(v)(1−p)<π(v) =deg(v)
mC(v)+/summationtext
u∈C(v)deg(u)(71)
We then have that there exists a s>1so that
sC1(p) =π(v) (72)
Using this relationship, we can then prove that for a sufficiently large p∈[0,1], we must have a q∈[0,1]
b.p∈[0,1]sufficiently large implies q≥0:
We thus have q≥0since its numerator is nonnegative:
π(v)−C1(p) = (s−1)C1(p)≥0⇒q≥0 (73)
c.p∈[0,1]sufficiently large implies q≤1:
π(v)−C1(p) =sC1(p)−C1(p) = (s−1)C1(p)≤C2(p)⇒q≤1 (74)
39Under review as submission to TMLR
C Additional Experiments
We show in Figure 5 the distributions for the component sizes over all connected components for samples
from the Hy-MMSBM model Ruggeri et al. (2023). This is a model to sample hypergraphs with community
structure. InFigure5awesamplehypergraphswith 3isolatedcommunities, meaningthatthereis 0chanceof
any interconnections between any two communities. In Figure 5b we sample hypergraphs with 3communities
whereeverynodeinacommunityhasaweightof 1tostayinitscommunityandaweightof 0.2tomoveoutto
any other community. We plot the boxplots as a function of increasing number of nodes. We notice that the
more communication there is between communities for more nodes there is more spread in possible connected
component sizes. Isolated communities should make for predictable clusters/connected components.
64 128 256 512 1024 2048
Number of Nodes020406080100Size of Maximally Connected GWL-1 Classes
(a) Boxplot of the sizes of the connected
components with equal GWL-1 node values from
the hy-MMSBM sampling algorithm where there
are three independent communities.
64 128 256 512 1024 2048
Number of Nodes020406080100120140Size of Maximally Connected GWL-1 Classes
(b) Boxplot of the sizes of the connected
components with equal GWL-1 node values from
the hy-MMSBM sampling algorithm where any
two of the three communities can communicate.
Figure 5
40Under review as submission to TMLR
C.1 Additional Experiments on Hypergraphs
In Table 3, we show the PR-AUC scores for four additional hypergraph datasets, cat-edge-Brain ,cat-
edge-vegas-bar-reviews ,WikiPeople-0bi , and JF17Kfor predicting size 3hyperedges.
PR-AUC↑Baseline Ours Baseln.+edrop
HGNN 0.75±0.01 0.79±0.11 0.74±0.09
HGNNP 0.75±0.05 0.78±0.10 0.74±0.12
HNHN 0.74±0.04 0.74±0.02 0.74±0.05
HyperGCN 0.74±0.09 0.50±0.07 0.50±0.12
UniGAT 0.73±0.07 0.81±0.10 0.81±0.09
UniGCN 0.78±0.04 0.81±0.09 0.71±0.08
UniGIN 0.74±0.09 0.74±0.03 0.74±0.07
UniSAGE 0.74±0.03 0.74±0.12 0.74±0.01
(a) cat-edge-BrainPR-AUC↑Baseline Ours Baseln.+edrop
HGNN 0.95±0.10 0.99±0.04 0.96±0.09
HGNNP 0.95±0.06 0.96±0.09 0.96±0.08
HNHN 1.00±0.08 0.99±0.09 0.95±0.10
HyperGCN 0.76±0.03 0.67±0.14 0.68±0.09
UniGAT 0.87±0.07 1.00±0.09 0.99±0.08
UniGCN 0.99±0.07 0.96±0.09 0.92±0.05
UniGIN 0.98±0.06 0.96±0.08 0.95±0.06
UniSAGE 0.94±0.05 0.98±0.07 0.97±0.07
(b) cat-edge-vegas-bar-reviews
PR-AUC↑Baseline Ours Baseln.+edrop
HGNN 0.52±0.01 0.57±0.08 0.54±0.10
HGNNP 0.52±0.03 0.54±0.07 0.54±0.06
HNHN 0.73±0.03 0.73±0.07 0.73±0.00
HyperGCN 0.54±0.05 0.55±0.02 0.49±0.10
UniGAT 0.49±0.09 0.54±0.04 0.53±0.04
UniGCN 0.46±0.08 0.68±0.08 0.51±0.08
UniGIN 0.73±0.09 0.73±0.01 0.73±0.02
UniSAGE 0.73±0.06 0.73±0.02 0.73±0.08
(c) WikiPeople-0biPR-AUC↑Baseline Ours Baseln.+edrop
HGNN 0.59±0.04 0.63±0.04 0.45±0.09
HGNNP 0.71±0.07 0.63±0.07 0.57±0.04
HNHN 0.73±0.04 0.73±0.03 0.73±0.04
HyperGCN 0.59±0.05 0.58±0.09 0.48±0.01
UniGAT 0.61±0.07 0.61±0.04 0.51±0.08
UniGCN 0.58±0.00 0.60±0.03 0.59±0.02
UniGIN 0.80±0.04 0.77±0.08 0.75±0.05
UniSAGE 0.79±0.02 0.77±0.08 0.74±0.01
(d) JF17K
Table 3: PR-AUC on four other hypergraph datasets. The top average scores for each hyperGNN method,
or row, is colored. Red scores denote the top scores in a row. Orange scores denote a two way tie and brown
scores denote a threeway tie.
C.2 Experiments on Graph Data
We show in Tables 4, 5, 6 the PR-AUC test scores for link prediction on some nonattributed graph datasets.
The train-val-test splits are predefined for FB15k-237 and for the other graph datasets a single graph is
deterministically split into 80/5/15 for train/val/test. We remove 10%of the edges in training and let
them be positive examples Ptrto predict. For validation and test, we remove 50%of the edges from both
validation and test to set as the positive examples Pval,Pteto predict. For train, validation, and test, we
sample 1.2|Ptr|,1.2|Pval|,1.2|Pte|negative link samples from the links of train, validation and test. Along
with HyperGNN architectures we use for the hypergraph experiments, we also compare with standard
GNN architectures: APPNP Gasteiger et al. (2018), GAT Veličković et al. (2017), GCN2 Chen et al.
(2020), GCN Kipf & Welling (2016a), GIN Xu et al. (2018), and GraphSAGE Hamilton et al. (2017).
For every HyperGNN/GNN architecture, we also apply drop-edge Rong et al. (2019) to the input graph
and use this also as baseline. The number of layers of each GNN is set to 5and the hidden dimension at
1024. For APPNP and GCN2, one MLP is used on the initial node positional encodings. Since graphs
do not have any hyperedges beyond size 2, graph neural networks fit the inductive bias of the graph data
moreeasilyandthusmayperformbetterthanhypergraphneuralnetworkbaselinesmoreoftenthanexpected.
41Under review as submission to TMLR
PR-AUC↑ HGNN HGNNP HNHN HyperGCN UniGAT UniGCN UniGIN UniSAGE
Ours 0.71±0.04 0.71±0.09 0.69±0.09 0.75±0.14 0.75±0.09 0.74±0.09 0.65±0.08 0.65±0.07
HyperGNN Baseline 0.68±0.00 0.69±0.06 0.67±0.02 0.75±0.04 0.74±0.02 0.74±0.00 0.65±0.05 0.64±0.08
HyperGNN Baseln.+edrop 0.67±0.02 0.70±0.07 0.66±0.00 0.75±0.03 0.73±0.08 0.74±0.05 0.63±0.01 0.64±0.03
APPNP 0.40±0.03 0.40±0.03 0.40±0.03 0.40±0.03 0.40±0.03 0.40±0.03 0.40±0.03 0.40±0.03
APPNP+edrop 0.40±0.13 0.40±0.13 0.40±0.13 0.40±0.13 0.40±0.13 0.40±0.13 0.40±0.13 0.40±0.13
GAT 0.49±0.03 0.49±0.03 0.49±0.03 0.49±0.03 0.49±0.03 0.49±0.03 0.49±0.03 0.49±0.03
GAT+edrop 0.51±0.05 0.51±0.05 0.51±0.05 0.51±0.05 0.51±0.05 0.51±0.05 0.51±0.05 0.51±0.05
GCN2 0.50±0.09 0.50±0.09 0.50±0.09 0.50±0.09 0.50±0.09 0.50±0.09 0.50±0.09 0.50±0.09
GCN2+edrop 0.56±0.07 0.56±0.07 0.56±0.07 0.56±0.07 0.56±0.07 0.56±0.07 0.56±0.07 0.56±0.07
GCN 0.73±0.02 0.73±0.02 0.73±0.02 0.73±0.02 0.73±0.02 0.73±0.02 0.73±0.02 0.73±0.02
GCN+edrop 0.73±0.01 0.73±0.01 0.73±0.01 0.73±0.01 0.73±0.01 0.73±0.01 0.73±0.01 0.73±0.01
GIN 0.73±0.06 0.73±0.06 0.73±0.06 0.73±0.06 0.73±0.06 0.73±0.06 0.73±0.06 0.73±0.06
GIN+edrop 0.73±0.01 0.73±0.01 0.73±0.01 0.73±0.01 0.73±0.01 0.73±0.01 0.73±0.01 0.73±0.01
GraphSAGE 0.73±0.08 0.73±0.08 0.73±0.08 0.73±0.08 0.73±0.08 0.73±0.08 0.73±0.08 0.73±0.08
GraphSAGE+edrop 0.73±0.09 0.73±0.09 0.73±0.09 0.73±0.09 0.73±0.09 0.73±0.09 0.73±0.09 0.73±0.09
Table 4: PR-AUC on graph dataset johnshopkins55 . Each column is a comparison of the baseline PR-AUC
scores against the PR-AUC score for our method (first row) applied to a standard HyperGNN architecture.
Red color denotes the highest average score in the column. Orange color denotes a two-way tie in the column,
and brown color denotes a three-or-more-way tie in the column.
PR-AUC↑ HGNN HGNNP HNHN HyperGCN UniGAT UniGCN UniGIN UniSAGE
Ours 0.66±0.06 0.78±0.02 0.63±0.07 0.82±0.10 0.75±0.05 0.74±0.03 0.75±0.03 0.75±0.06
HyperGNN Baseline 0.65±0.06 0.65±0.06 0.65±0.04 0.82±0.09 0.74±0.04 0.74±0.05 0.75±0.03 0.77±0.01
HyperGNN Baseln.+edrop 0.65±0.09 0.65±0.00 0.64±0.05 0.82±0.00 0.72±0.00 0.74±0.07 0.73±0.03 0.72±0.07
APPNP 0.72±0.10 0.72±0.10 0.72±0.10 0.72±0.10 0.72±0.10 0.72±0.10 0.72±0.10 0.72±0.10
APPNP+edrop 0.71±0.05 0.71±0.05 0.71±0.05 0.71±0.05 0.71±0.05 0.71±0.05 0.71±0.05 0.71±0.05
GAT 0.64±0.06 0.64±0.06 0.64±0.06 0.64±0.06 0.64±0.06 0.64±0.06 0.64±0.06 0.64±0.06
GAT+edrop 0.61±0.09 0.61±0.09 0.61±0.09 0.61±0.09 0.61±0.09 0.61±0.09 0.61±0.09 0.61±0.09
GCN2 0.66±0.03 0.66±0.03 0.66±0.03 0.66±0.03 0.66±0.03 0.66±0.03 0.66±0.03 0.66±0.03
GCN2+edrop 0.65±0.10 0.65±0.10 0.65±0.10 0.65±0.10 0.65±0.10 0.65±0.10 0.65±0.10 0.65±0.10
GCN 0.69±0.03 0.69±0.03 0.69±0.03 0.69±0.03 0.69±0.03 0.69±0.03 0.69±0.03 0.69±0.03
GCN+edrop 0.71±0.06 0.71±0.06 0.71±0.06 0.71±0.06 0.71±0.06 0.71±0.06 0.71±0.06 0.71±0.06
GIN 0.73±0.03 0.73±0.03 0.73±0.03 0.73±0.03 0.73±0.03 0.73±0.03 0.73±0.03 0.73±0.03
GIN+edrop 0.56±0.07 0.56±0.07 0.56±0.07 0.56±0.07 0.56±0.07 0.56±0.07 0.56±0.07 0.56±0.07
GraphSAGE 0.46±0.15 0.46±0.15 0.46±0.15 0.46±0.15 0.46±0.15 0.46±0.15 0.46±0.15 0.46±0.15
GraphSAGE+edrop 0.47±0.01 0.47±0.01 0.47±0.01 0.47±0.01 0.47±0.01 0.47±0.01 0.47±0.01 0.47±0.01
Table 5: PR-AUC on graph dataset FB15k-237 . Each column is a comparison of the baseline PR-AUC
scores against the PR-AUC score for our method (first row) applied to a standard HyperGNN architecture.
Red color denotes the highest average score in the column. Orange color denotes a two-way tie in the column,
and brown color denotes a three-or-more-way tie in the column.
PR-AUC↑ HGNN HGNNP HNHN HyperGCN UniGAT UniGCN UniGIN UniSAGE
Ours 0.79±0.11 0.73±0.10 0.73±0.02 0.85±0.07 0.75±0.10 0.84±0.09 0.72±0.03 0.72±0.12
HyperGNN Baseline 0.72±0.07 0.72±0.07 0.72±0.06 0.85±0.05 0.75±0.09 0.84±0.05 0.72±0.07 0.72±0.06
HyperGNN Baseln.+edrop 0.72±0.05 0.72±0.08 0.72±0.06 0.85±0.07 0.73±0.09 0.84±0.06 0.72±0.03 0.72±0.07
APPNP 0.81±0.12 0.81±0.12 0.81±0.12 0.81±0.12 0.81±0.12 0.81±0.12 0.81±0.12 0.81±0.12
APPNP+edrop 0.80±0.05 0.80±0.05 0.80±0.05 0.80±0.05 0.80±0.05 0.80±0.05 0.80±0.05 0.80±0.05
GAT 0.50±0.02 0.50±0.02 0.50±0.02 0.50±0.02 0.50±0.02 0.50±0.02 0.50±0.02 0.50±0.02
GAT+edrop 0.33±0.02 0.33±0.02 0.33±0.02 0.33±0.02 0.33±0.02 0.33±0.02 0.33±0.02 0.33±0.02
GCN2 0.83±0.05 0.83±0.05 0.83±0.05 0.83±0.05 0.83±0.05 0.83±0.05 0.83±0.05 0.83±0.05
GCN2+edrop 0.78±0.04 0.78±0.04 0.78±0.04 0.78±0.04 0.78±0.04 0.78±0.04 0.78±0.04 0.78±0.04
GCN 0.73±0.14 0.73±0.14 0.73±0.14 0.73±0.14 0.73±0.14 0.73±0.14 0.73±0.14 0.73±0.14
GCN+edrop 0.75±0.08 0.75±0.08 0.75±0.08 0.75±0.08 0.75±0.08 0.75±0.08 0.75±0.08 0.75±0.08
GIN 0.73±0.00 0.73±0.00 0.73±0.00 0.73±0.00 0.73±0.00 0.73±0.00 0.73±0.00 0.73±0.00
GIN+edrop 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10 0.73±0.10
GraphSAGE 0.46±0.15 0.46±0.15 0.46±0.15 0.46±0.15 0.46±0.15 0.46±0.15 0.46±0.15 0.46±0.15
GraphSAGE+edrop 0.47±0.01 0.47±0.01 0.47±0.01 0.47±0.01 0.47±0.01 0.47±0.01 0.47±0.01 0.47±0.01
Table 6: PR-AUC on graph dataset AIFB. Each column is a comparison of the baseline PR-AUC scores
against the PR-AUC score for our method (first row) applied to a standard HyperGNN architecture. Red
color denotes the highest average score in the column. Orange color denotes a two-way tie in the column,
and brown color denotes a three-or-more-way tie in the column.
42Under review as submission to TMLR
D Dataset and Hyperparameters
Table 7 lists the datasets and hyperparameters used in our experiments. All datasets are originally from Ben-
son et al. (2018b) or are general hypergraph datasets provided in Sinha et al. (2015); Amburg et al. (2020a).
We list the total number of hyperedges |E|, the total number of vertices |V|, the positive to negative label
ratios for train/val/test, and the percentage of the connected components searched over by our algorithm
that are size atleast 3. A node isomorphism class is determined by our isomorphism testing algorithm. By
Proposition B.2 we can guarantee that if two nodes are in separate isomorphism classes by our isomorphism
tester, then they are actually nonisomorphic.
We use 1024dimensions for all HyperGNN/GNN layer latent spaces, 5layers for all hypergraph/graph neural
networks, and a common learning rate of 0.01. Exactly 2000epochs are used for training.
The HyperGNN architecture baselines are described in the follwoing:
•HGNN Feng et al. (2019) A neural network that generalizes the graph convolution to hypergraphs
where there are hyperedge weights. Its architecture can be described by the following update step
for thel+ 1-layer from the lth layer:
X(l+1)=σ(D−1
2vHWD−1
eHTD−1
2vX(l)W(l)) (75)
whereDv∈Rn×nis the diagonal node degree matrix, De∈Rm×mis the diagonal hyperedge degree
matrix,H∈Rn×mis the star incidence matrix, X(l)∈Rn×dis a node signal matrix, W(l)∈Rd×d
is a weight matrix, and σis a nonlinear activation. Following the matrix products, as a message
passing neural network, HGNN is GWL-1 based since the nodes pass to the hyperedges and back.
•HGNNP Feng et al. (2023) is an improved version of HGNN where asymmetry is introduced into the
message passing weightings to distinguish the vertices from the hyperedges. This is also a GWL-1
based message passing neural network. It is described by the following node signal update equation:
X(l+1)=σ(D−1
vHWD−1
eHTX(l)W(l)) (76)
where the matrices are exactly the same as from HGNN.
•HyperGCN Yadati et al. (2019) computes GCN on a clique expansion of a hypergraph. This has an
updateable adjacency matrix defined as follows:
A(l)
i,j=/braceleftigg
1 (i,j)∈E(l)
0 (i,j)/∈E(l)(77)
where
E(l)={(ie,je) =argmaxi,j∈e|X(l)
i−X(l)
j|:e∈E} (78)
X(l+1)
v =σ(/summationdisplay
u∈N(v)([A(l)]v,uX(l)
uW(l))) (79)
TheX(l)∈Rn×dis the node signal matrix at layer l, theW(l)∈Rd×dis the weight matrix at layer
l, andσis some nonlinear activation. This architecture has less expressive power than GWL-1.
•HNHN Dong et al. (2020) This is like HGNN but where the message passing is explicitly broken up
into two hyperedge to node and node to hyperedge layers.
X(l)
E=σ(HTX(l)
VW(l)
E+b(l)
E) (80a)
and
X(l+1)
V =σ(HX(l)
EW(l)
V+b(l)
V) (80b)
whereH∈Rn×mis the star expansion incidence matrix, W(l)
E,W(l)
V∈Rd×d,b(l)
E∈Rm,b(l)
V∈Rn
are weights and biases, X(l)
E,X(l)
Vare the hyperedge and node signal matrices at layer l, andσis a
nonlinear activation function. The bias vectors prevent HNHN from being permutation equivariant.
43Under review as submission to TMLR
•UniGNN Huang & Yang (2021) The idea is directly related to generalizing WL-1 GNNs to Hyper-
graphs. Define the following hyperedge representation for hyperedge e∈E:
h(l)
e=1
|e|/summationdisplay
u∈eX(l)
u (81)
–UniGCN: a generalization of GCN to hypergraphs
X(l)
v=1√dv/summationdisplay
e∋v1√
deW(l)h(l)
e (82)
–UniGAT: a generalization of GAT to hypergraphs
αue=σ(aT[X(l)
iW(l);X(l)
jW(l)]) (83a)
˜αue=eαue
/summationtext
v∈eeαve(83b)
X(l+1)
v =/summationdisplay
e∋vαveheW(l)(83c)
–UniGIN: a generalization of GIN to hypergraphs
X(l+1)
v = ((1 +ϵ)X(l)
v+/summationdisplay
e∋vhe) (84)
–UniSAGE: a generalization of GraphSAGE to hypergraphs
X(l+1)
v = (X(l)
v+/summationdisplay
e∋v(he)) (85)
All positional encodings are computed from the training hyperedges before data augmentation. The loss we
use for higher order link prediction is the Binary Cross Entropy Loss for all the positive and negatives sam-
ples. Hypergraphneuralnetworkimplementationsweremostlytakenfrom https://github.com/iMoonLab/
DeepHypergraph , which uses the Apache License 2.0.
Dataset Information
Dataset |E||V|∆+,tr
∆−,tr∆+,val
∆−,val∆+,te
∆−,te% of Conn. Comps. Selected
cat-edge-DAWN 87,104 2,109 8,802/10,547 1,915/2,296 1,867/2,237 0.05%
email-Eu 234,760 998 1,803/2,159 570/681 626/749 0.6%
contact-primary-school 106,879 242 1,620/1,921 461/545 350/415 9.3%
cat-edge-music-blues-reviews 694 1,106 16/19 7/6 3/3 0.14%
cat-edge-vegas-bars-reviews 1,194 1,234 72/86 12/14 11/13 0.7%
contact-high-school 7,818 327 2,646/3,143 176/208 175/205 5.6%
cat-edge-Brain 21,180 638 13,037/13,817 2,793/3,135 2,794/3,020 9.9%
johnshopkins55 298,537 5,163 29,853/35,634 9,329/11,120 27,988/29,853 2.0%
AIFB 46,468 8,083 4,646/5,575 1,452/1,739 4,356/5,222 0.02%
amherst41 145,526 2,234 14,552/17,211 4,547/5,379 16,125/13,643 4.4%
FB15k-237 272,115 14,505 27,211/32,630 8,767/10,509 10,233/12,271 2.1%
WikiPeople-0bi 18,828 43,388 27,211/32,630 10,254/12,301 1,164/1,396 0.05%
JF17K 76,379 28,645 11,907/14,287 1,341/1,608 1,341/1,608 0.6%
Table 7: Dataset statistics and training hyperparameters used for all datasets in scoring all experiments.
We describe here some more information about each dataset we use in our experiments as provided by
Benson et al. (2018b): Here is some information about the hypergraph datasets:
44Under review as submission to TMLR
•Amburg et al. (2020a) cat-edge-DAWN : Here nodes are drugs, hyperedges are combinations of
drugs taken by a patient prior to an emergency room visit and edge categories indicate the patient
disposition (e.g., "sent home", "surgery", "released to detox").
•Benson et al. (2018a); Yin et al. (2017); Leskovec et al. (2007) email-Eu: This is a temporal higher-
ordernetworkdataset, whichheremeansasequenceoftimestampedsimplices, orhyperedgeswithall
itsnodesubsetsexistingashyperedges,whereeachsimplexisasetofnodes. Inemailcommunication,
messages can be sent to multiple recipients. In this dataset, nodes are email addresses at a European
research institution. The original data source only contains (sender, receiver, timestamp) tuples,
where timestamps are recorded at 1-second resolution. Simplices consist of a sender and all receivers
such that the email between the two has the same timestamp. We restricted to simplices that consist
of at most 25 nodes.
•Stehlé et al. (2011) contact-primary-school: This is a temporal higher-order network dataset,
which here means a sequence of timestamped simplices where each simplex is a set of nodes. The
dataset is constructed from interactions recorded by wearable sensors by people at a primary school.
The sensors record interactions at a resolution of 20 seconds (recording all interactions from the pre-
vious 20 seconds). Nodes are the people and simplices are maximal cliques of interacting individuals
from an interval.
•Amburg et al. (2020b) cat-edge-vegas-bars-reviews: Hypergraph where nodes are Yelp users
and hyperedges are users who reviewed an establishment of a particular category (different types of
bars in Las Vegas, NV) within a month timeframe.
•Benson et al. (2018a); Mastrandrea et al. (2015) contact-high-school: This is a temporal higher-
order network dataset, which here means a sequence of timestamped simplices where each simplex
is a set of nodes. The dataset is constructed from interactions recorded by wearable sensors by
people at a high school. The sensors record interactions at a resolution of 20 seconds (recording all
interactions from the previous 20 seconds). Nodes are the people and simplices are maximal cliques
of interacting individuals from an interval.
•Crossley et al. (2013) cat-edge-Brain: This is a graph whose edges have categorical edge labels.
Nodes represent brain regions from an MRI scan. There are two edge categories: one for connecting
regions with high fMRI correlation and one for connecting regions with similar activation patterns.
•Lim et al. (2021) johnshopkins55: Non-homophilous graph datasets from the facebook100 dataset.
•Ristoski & Paulheim (2016) AIFB:The AIFB dataset describes the AIFB research institute in terms
of its staff, research groups, and publications. The dataset was first used to predict the affiliation
(i.e., research group) for people in the dataset. The dataset contains 178 members of five research
groups, however, the smallest group contains only four people, which is removed from the dataset,
leaving four classes.
•Lim et al. (2021) amherst41: Non-homophilous graph datasets from the facebook100 dataset.
•Bordes et al. (2013) FB15k-237: A subset of entities that are also present in the Wikilinks database
Singh et al. (2012) and that also have at least 100 mentions in Freebase (for both entities and
relationships). Relationships like ’!/people/person/nationality’ which just reverses the head and tail
compared to the relationship ’/people/person/nationality’ are removed. This resulted in 592,213
triplets with 14,951 entities and 1,345 relationships which were randomly split.
•Guanetal.(2019) WikiPeople-0bi: TheWikidatadumpwasdownloadedandthefactsconcerning
entities of type human were extracted. These facts are denoised. Subsequently, the subsets of
elements which have at least 30 mentions were selected. And the facts related to these elements
were kept. Further, each fact was parsed into a set of its role-value pairs. The remaining facts were
randomly split into training set, validation set and test set by a percentage of 80%:10%:10%. All
binary relations are removed for simplicity. This modifies WikiPeople to WikiPeople-0bi.
45Under review as submission to TMLR
•Wen et al. (2016) JF17K: The full Freebase data in RDF format was downloaded. Entities involved
in very few triples and the triples involving String, Enumeration Type and Numbers were removed.
A fact representation was recovered from the remaining triples. Facts from meta-relations having
only a single role were removed. From each meta-relation containing more than 10,000 facts, 10,000
facts were randomly selected.
46Under review as submission to TMLR
D.1 Timings
We perform experiments on a cluster of machines equipped with AMD MI100s GPUs and 112 shared AMD
EPYC 7453 28-Core Processors with 2.6 PB shared RAM. We show here the times for computing each
method. The timings may vary heavily for different machines as the memory we used is shared and during
peak usage there is a lot of paging. We notice that although our data preprocessing algorithm involves
seemingly costly steps such as GWL-1, connected connected components etc. The complexity of the entire
preprocessing algorithm is linear in the size of the input as shown in Proposition B.15. Thus these operations
are actually very efficient in practice as shown by Tables 9 and 10 for the hypergraph and graph datasets
respectively. The preprocessing algorithm is run on CPU while the training is run on GPU for 2000 epochs.
Timings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 2m:45s±108s 35m:9s±13s
HGNNP 1m:52s±0s 35m:16s±0s
HNHN 1m:55s±0s 35m:0s±1s
HyperGCN 1m:50s±0s 58m:17s±79s
UniGAT 1m:54s±0s 1h:19m:34s±0s
UniGCN 1m:50s±2s 35m:19s±2s
UniGIN 1m:50s±1s 35m:12s±1288s
UniSAGE 1m:51s±0s 35m:16s±0s
(a)cat-edge-DAWNTimings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 1.72s±5s 2m:11s±11s
HGNNP 1.42s±0s 2m:10s±0s
HNHN 1.99s±0s 3m:43s±2s
HyperGCN 1.47s±2s 4m:12s±3s
UniGAT 1.85s±0s 3m:54s±287s
UniGCN 2.93s±0s 3m:15s±19s
UniGIN 2.24s±0s 3m:17s±18s
UniSAGE 2.04s±0s 3m:13s±3s
(b)cat-edge-music-blues-reviews
Timings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 4.17s±0s 2m:34s±1954s
HGNNP 4.54s±0s 2m:41s±53s
HNHN 3.06s±0s 2m:27s±15s
HyperGCN 1.81s±1s 2m:27s±0s
UniGAT 1.91s±0s 2m:27s±306s
UniGCN 2.84s±0s 2m:30s±72s
UniGIN 3.20s±0s 2m:27s±1189s
UniSAGE 1.65s±0s 2m:27s±0s
(c)cat-edge-vegas-bars-reviewsTimings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 5.84s±1s 6m:49s±8s
HGNNP 5.82s±0s 9m:8s±19s
HNHN 5.95s±0s 8m:21s±19s
HyperGCN 5.74s±0s 10m:16s±1s
UniGAT 8.80s±0s 2m:31s±282s
UniGCN 6.35s±0s 6m:9s±957s
UniGIN 5.99s±0s 10m:41s±43s
UniSAGE 5.97s±0s 9m:50s±0s
(d)contact-primary-school
Timings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 23.25s±1s 25m:41s±17s
HGNNP 23.25s±0s 19m:52s±49s
HNHN 24.27s±1s 5m:12s±63s
HyperGCN 24.00s±0s 21m:16s±0s
UniGAT 14.27s±0s 5m:13s±243s
UniGCN 25.44s±0s 5m:51s±1019s
UniGIN 13.71s±1s 19m:10s±3972s
UniSAGE 14.08s±2s 36m:29s±5s
(e)email-EuTimings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 4.89s±6s 1m:27s±8s
HGNNP 2.12s±0s 2m:42s±30s
HNHN 2.12s±0s 2m:39s±42s
HyperGCN 2.11s±0s 40.11s±3s
UniGAT 2.13s±0s 3m:18s±8s
UniGCN 2.11s±0s 3m:21s±2s
UniGIN 2.11s±0s 2m:24s±70s
UniSAGE 2.11s±0s 2m:8s±49s
(f)cat-edge-madison-restaurant-reviews
47Under review as submission to TMLR
Timings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 15.11s±4s 4m:59s±1s
HGNNP 12.72s±0s 2m:29s±0s
HNHN 12.17s±0s 3m:6s±0s
HyperGCN 12.47s±0s 49.25s±0s
UniGAT 12.74s±0s 2m:1s±1s
UniGCN 12.50s±0s 2m:29s±3s
UniGIN 12.57s±0s 2m:16s±3s
UniSAGE 12.67s±0s 1m:50s±29s
(a)contact-high-schoolTimings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 11.34s±10s 4m:24s±6s
HGNNP 6.02s±0s 4m:13s±2s
HNHN 6.01s±0s 5m:31s±1s
HyperGCN 6.32s±0s 1m:33s±0s
UniGAT 6.04s±0s 4m:11s±0s
UniGCN 5.79s±0s 4m:12s±0s
UniGIN 6.64s±1s 3m:4s±1s
UniSAGE 5.79s±0s 3m:2s±0s
(b)cat-edge-Brain
Timings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 3m:30s±5s 1h:29m:33s±6s
HGNNP 3m:34s±1s 1h:48m:57s±1s
HNHN 3m:41s±1s 2h:9m:34s±1s
HyperGCN 3m:24s±1s 58m:27s±1s
UniGAT 3m:50s±1s 4h:21m:24s±1s
UniGCN 3m:38s±1s 29m:14s±1s
UniGIN 3m:50s±1s 27m:50s±1s
UniSAGE 3m:41s±1s 27m:22s±1s
(c)WikiPeople-0biTimings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 8m:11s±52s 37m:18s±9s
HGNNP 7m:34s±1s 47m:56s±1s
HNHN 6m:21s±1s 49m:33s±1s
HyperGCN 8m:20s±1s 28m:25s±1s
UniGAT 10m:40s±1s 1h:54m:36s±1s
UniGCN 7m:25s±1s 2h:40m:20s±1s
UniGIN 10m:37s±1s 2h:48m:35s±1s
UniSAGE 6m:58s±1s 2h:35m:4s±1s
(d)JF17K
Table 9: Timings for our method broken up into the preprocessing phase and training phases ( 2000epochs)
for the hypergraph datasets.
48Under review as submission to TMLR
Timings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 11m:14s±75s 53m:21s±2845s
HGNNP 11m:10s±21s 1h:34m:25s±35s
HNHN 5m:15s±395s 1h:35m:15s±419s
HyperGCN 33.98s±0s 5m:8s±0s
UniGAT 1m:59s±120s 2h:2m:47s±25s
UniGCN 34.37s±0s 1h:17m:38s±2s
UniGIN 34.05s±0s 1h:16m:38s±7s
UniSAGE 34.36s±0s 1h:16m:34s±3s
(a)johnshopkins55Timings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 17m:9s±164s 12m:38s±549s
HGNNP 15m:34s±61s 20m:26s±124s
HNHN 15m:31s±83s 18m:11s±30s
HyperGCN 15m:46s±32s 4m:17s±80s
UniGAT 1m:27s±6s 16m:30s±0s
UniGCN 15m:57s±24s 18m:42s±170s
UniGIN 16m:14s±73s 16m:22s±39s
UniSAGE 8m:42s±610s 8m:49s±324s
(b)AIFB
Timings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 4m:1s±11s 22m:30s±1177s
HGNNP 3m:53s±4s 39m:30s±3s
HNHN 3m:16s±22s 44m:7s±71s
HyperGCN 3m:35s±23s 5m:22s±25s
UniGAT 11.92s±0s 1h:51m:53s±123s
UniGCN 3m:20s±6s 39m:18s±51s
UniGIN 3m:21s±8s 38m:3s±0s
UniSAGE 11.27s±0s 58m:48s±956s
(c)amherst41Timings (hh:mm) ±(s)
Method Preprocessing Time Training Time
HGNN 3m:32s±9s 1h:19m:5s±4684s
HGNNP 3m:26s±10s 2h:19m:44s±3586s
HNHN 3m:27s±0s 1h:55m:48s±22s
HyperGCN 3m:28s±0s 10m:31s±18s
UniGAT 3m:24s±5s 3h:50m:24s±91s
UniGCN 3m:19s±4s 1h:39m:46s±13s
UniGIN 3m:17s±0s 1h:36m:47s±35s
UniSAGE 3m:25s±13s 1h:37m:16s±102s
(d)FB15k-237
Table 10: Timings for our method broken up into the preprocessing phase and training phases ( 2000epochs)
for the graph datasets.
49