Published in Transactions on Machine Learning Research (/)
Choosing the parameter of the Fermat distance: navigating
geometry and noise
Frédéric Chazal frederic.chazal@inria.fr
Institut de Mathématique d’Orsay, Faculté des Sciences d’Orsay, Université Paris-Saclay, France
Laure Ferraris laure.ferraris@inria.fr
Institut de Mathématique d’Orsay, Faculté des Sciences d’Orsay, Université Paris-Saclay, France
Pablo Groisman pgroisma@dm.uba.ar
IMAS-CONICET, Dep. de Matemática, Fac. Cs. Exactas y Naturales, Universidad de Buenos Aires, Argentina
Matthieu Jonckheere, matthieu.jonckheere@laas.fr,
LAAS-CNRS, Université de Toulouse, CNRS, Toulouse, France
Frédéric Pascal frederic.pascal@centralesupelec.fr
Université Paris-Saclay, CNRS, CentraleSupélec, Laboratoire des Signaux et Systèmes, 91190, Gif-sur-Yvette, France
Facundo Sapienza fsapienza@berkeley.edu
Department of Statistics, University of California, Berkeley, CA, USA
Reviewed on OpenReview:
Abstract
The Fermat distance has been recently established as a valuable tool for machine learning
tasks when a natural distance is not directly available to the practitioner or to improve the
results given by Euclidean distances by exploiting the geometrical and statistical properties
of the dataset. This distance depends on a parameter αthat significantly affects the perfor-
mance of subsequent tasks. Ideally, the value of αshould be large enough to navigate the
geometric intricacies inherent to the problem. At the same time, it should remain restrained
enough to avoid any deleterious effects stemming from noise during the distance estimation
process. We study both theoretically and through simulations how to select this parameter.
Keywords— Fermat distance, clustering, geometry, density, metric learning
1 Introduction
The Fermat distance and other density based distances have recently attracted interest in machine learning and
shown usefulness in various application domains. For example, they have been used in topological data analysis to
detect cancer fingerprints (4), in signal analysis to detect periodicity and anomalies in ECG and in the reconstruction
of dynamical systems (9). The also have shown interesting potential in semi-supervised learning and in definition of
notions of data depth (3). Exploiting both the geometric and statistical properties of datasets, the Fermat distance
favor geodesic paths to go through high data density areas - by reducing distances between points there - and avoid
low density areas - by increasing distances between points there - of the space in which they are defined. The
influence of the density in the definition of the Fermat distance depends on a parameter αwhose choice is critical:
the performances of subsequent tasks, such as clustering, tend to improve as αincreases while at the same time the
estimation of the Fermat distance tends to become dramatically sensitive to noise. The overall objective of this paper
1Published in Transactions on Machine Learning Research (/)
is to better understand the role and behavior of this parameter αin the estimation of the Fermat distance and its
impact on clustering subsequent tasks.
LetQn={q1,...,qn}⊂RDbe independent random points with common density f:M⊂RD∝⇕⊣√∫⊔≀→R≥0supported on
a smooth Riemannian manifold Membedded in RD. We assume that fis a density with respect to the volume form
onM. Typically, d:= dim(M)<D, but it can take any value 1≤d≤D.
We first recall the definition of the sample Fermat distance and themacroscopic Fermat distance (14; 11; 18).
Definition 1.1 (Sample Fermat distance) .Forα≥1andx,y∈RDwe define,
DQn,α(x,y) = inf/braceleftiggk−1/summationdisplay
j=1|qj+1−qj|α: (q1,...,qk)is a path from xtoy, k≥1/bracerightigg
, (1.1)
where|·|denotesthe D-dimensionalEuclideannorm, q1andqkarethenearestneighborsin Qnofxandy, respectively.
Definition 1.2 (Macroscopic Fermat distance) .Forα≥1andx,y∈M,
Dd
α(x,y) = inf
γ∈Γx,yˆ
γ1
f(α−1)/d, (1.2)
where Γx,yis the set of rectifiable paths γ: [0,1]→Msuch thatγ(0) =x,γ(1) =y.
These distances have been used in several situations and proved to be useful in a variety of learning tasks (18; 16; 9;
11; 26; 5; 17). Under different assumptions on Mandf, it has been shown that when α>1, asn→∞,
n(α−1)/dDQn,α(x,y)−→µ(α,d)Dd
α(x,y),a.s., (1.3)
whereµ(α,d)is a positive constant depending only on the intrinsic dimension dofMand the parameter α(11; 9; 14).
It is worth noting that in the works (16; 18), a different definition of Fermat distance is introduced, specifically
expressed as
¯DQn,α(x,y) =DQn,α(x,y)1/α. (1.4)
This alternative definition carries implications for practical implementations, each accompanied by its advantages
and limitations, which must be considered when using it along with specific subsequent machine learning tasks.
Our study sheds light on some key points about the use of the Fermat distance, as given in Definition 1.2, for both
clustering and classification tasks.
Contributions. Firstly, we demonstrate the existence of a critical value α0, dependent on the underlying distribu-
tion’s geometric parameters and its intrinsic dimensionality. For values α > α 0, both classification and clustering
tasks become feasible for large sample sizes, which implies that the derivation of a meaningful distance measurement
hinges upon αexceeding this pivotal threshold. Under regularity assumptions, we show that α0scales linearly with
the dimension. Contrary to this point, we also highlight that using large values of αleads to drawbacks due to
increased variability in the sample Fermat distance. This effect has a negative impact on the distance estimation for
finite samples. We prove that the variance of the sample Fermat distance scales exponentially with αin dimension
one, and we obtain exponential bounds in terms of α/din higher dimensions. We conclude that a sensible choice for
αis crucial and provide guidelines to perform this choice.
To illustrate our findings, we conduct experiments on synthetic datasets and observe that there is a practical critical
window of values of αwhere the performance is optimal. Although the definitions and results presented in this study
are relevant to both the supervised context of classification and the unsupervised tasks of clustering, our subsequent
analysis in the following sections will be framed within the context of clustering analysis.
2Published in Transactions on Machine Learning Research (/)
2 Population clusters and a lower bound on α
We start with a definition of clusters that extends the one given by the level sets of a density.
Definition 2.1 (Clusters) .For a measure with density f(with respect to the volume form) on the Riemannian
manifoldM⊂RDand a family of sets C:= (Ci)m
i=1,Ci⊂RD, we say thatCareα−clusters offif there exists ϵ>0
such that
Dd
α(x,y)≤Dd
α(x,y′)−ϵ,for all 1≤i≤m,j̸=i,x,y∈Ci,y′∈Cj. (2.1)
If this holds for some α≥1, we will say that the clustering problem (C,f)isα−feasible or just feasible.
Note that this definition of clusters aligns with clustering methods that find clusters by comparing intra-cluster
distances with inter-cluster distances. In practice, this is usually done by finding centers or centroids c1,...,cm(e.g.,
K−medoids or K−means techniques) such that,
Dd
α(x,ci)≤Dd
α(x,cj)−ϵ,for all 1≤i≤m,j̸=i,x∈Ci. (2.2)
This last notion is the one that is used to find the empirical centroids ( i.e., estimators of c1,...,cm) and then the
clusters. We could adopt this definition and state our results regarding this property, but we prefer to use (2.1) to
avoid dealing with centroid estimations and their consistency properties. We remark that all the statements and
definitions below can be rephrased to fit the notion of clusters that involves centroids.
If a macroscopic clustering problem is feasible, we define the critical parameter as the least α0≥1for which (2.1)
holds for every α>α 0. More precisely,
α0= inf{α′≥1:Careα−clusters offfor allα>α′}. (2.3)
We will demonstrate that using the Fermat distance in our definition enables us to address various natural scenarios
characterized by clusters effectively. This also generalizes other notions of clusters based on level sets of f(see
Proposition 2.2 below). We will also show the existence of a finite critical parameter α0under mild conditions.
Observe that the clusters of fneed not be uniquely defined.
2.1 Preliminaries
We first introduce notations used in the sequel. Given a set C∈RD,the distance function toC, denoted by dC, is
defined by
dC(x) = inf
y∈C|x−y|. (2.4)
Notice that since C=/braceleftbig
x∈RD:dC(x) = 0/bracerightbig
, the distance function dCfully characterize Cas soon asCis closed.
We callr-offsetofC, denoted by Cr, the set of points at distance at most rofC, or equivalently the sublevel set
defined by
Cr=/braceleftbig
x∈RD|dC(x)≤r/bracerightbig
. (2.5)
Finally, we define the reachofC, denoted as rch (C)as the largest value r>0such that for all y∈Cr, there exists a
uniquex∈Csuch that|x−y|=dC(y). The reach of a set quantifies how far a compact set Cis from being convex.
Notice that rch (C)is small if either Cis not smooth or if Cis close to being self-intersecting. On the other hand, if
Cis convex, then rch (C) =∞.
Consider two disjoint, connected and compact sets A,B⊂RDsuch thatC=A∪B. We denote the shortest Euclidean
distance between AandBby
dist(A,B) = inf{|a−b|:a∈A, b∈B}. (2.6)
If dist (A,B) =δ >0and if we call p∈RD∖Cthe middle point of a shortest straight line from AtoB, then the
projection of pontoCis not unique as it has at least one candidate on Aand one on B. We can then state that
rch(C)≤δ/2. Moreover, if AandBare convex, rch (C) =δ/2. The interested reader can refer to (1; 8; 6) for a more
detailed introduction to these concepts.
3Published in Transactions on Machine Learning Research (/)
2.2 Existence of a critical parameter
We can now state the existence of a critical parameter α0.
Proposition 2.2. LetC= (Ci)m
i=1be a family of compact and connected sets such that C=/uniontextm
i=1Cihas positive
reachτ >0. Suppose that f(x)≥a1for allx∈C, andf(x)≤a0for allx /∈C, witha1>a0. Then, there exists a
positive constant β0≥0depending on the diameters of the Ci’s, the reach of Cand the intrinsic dimension dofC
such that for all
α>1 +dβ0
log(a1/a0), (2.7)
the macroscopic clustering problem (C,f)isα−feasible. If the length of geodesics in Cis uniformly bounded, the
constantβ0can be taken independent of d(and hence, the bound from below for the critical parameter is linear in d).
Proof.The proof of this proposition is postponed to Appendix A.
Remark 2.3. As pointed out before, the reach encodes the geometric complexity of the problem. A small reach is
associated with a more complex problem. This is present in the previous result in the dependence of the constant β0
on the reach. The constant β0can be computed explicitly, and in fact, an explicit formula is given in the course of
the proof of the proposition. When Cis such that the geodesic path lengths are not uniformly upper bounded, the
parameterβ0turns out to scale proportional to d, giving that the bound from below for α0scales asd2.
Remark 2.4. The hypothesis of Proposition 2.2 forces fto be discontinuous (otherwise, we necessary have a0=a1).
We prefer to first deal with this restrictive situation to simplify the exposition, but in fact, this restriction can be
removed to cover the case when fis a continuous density by defining a0more carefully. In that case, we choose
a0<a1in such a way that there is a region around each cluster that separates the level sets f−1(a0)andf−1(a1).
It is worth observing the behavior of the lower bound as a0→0and asa0→a1.
Proposition 2.5. LetC= (Ci)1≤i≤mandC=/uniontext
Cias before. Suppose that f≥a1inC, and leta0be such that
0<η= inf/braceleftig
|s−t|:s∈f−1([0;a0]), t∈f−1([a1;∞])/bracerightig
<τ=rch(C). Then, there exists c>0such that for every
0<r<τ−η
α>1 +dlog/parenleftig
r
τ−(r+η)(cr−d∨1)/parenrightig
log(a1/a0), (2.8)
the macroscopic clustering problem is α−feasible. If the length of geodesics in Cis uniformly bounded, we can take α
as in(2.7)with the constant β0independent of d(and hence the bound from below for the critical parameter is linear
ind).
Proof.The proof is postponed to Appendix A.
3 Empirical clusters
In this section, we leverage the concepts established in the preceding section to address the clustering quandary in
the context of finite samples. Our goal is to ascertain the specific conditions under which a clustering problem can
be solved solely by harnessing the properties of the Fermat distance.
Definition 3.1 (Clustering) .Given a family of clusters (Ci)m
i=1⊂RDand points Qn={q1,...,qn}⊂RD, we say
that the empirical clustering problem is α−feasible for Qnif there exists ϵ>0such that
n(α−1)/dDQn,α(x,y)≤n(α−1)/dDQn,α(x,y′)−ϵ, (3.1)
for all 1≤i≤m,x,y∈Qn∩Ci, andy′∈Qn∩Cj,j̸=i. WhenQnis random, we call F(α,ϵ,n ) =
F(α,ϵ,Qn,C1,...,Cm)the event under which the samples are such that (3.1) holds.
TheideahereisthatasimpleclassificationruleusingFermatdistanceshouldbesufficienttoreachaperfectclustering.
We can now state the main result of this section, which relates the macroscopic problem to the microscopic one.
4Published in Transactions on Machine Learning Research (/)
Proposition 3.2. LetC= (Ci)1≤i≤mbe a family of disjoint, compact and connected sets, and C=/uniontextm
i=1Ci. Assume
(C,f)isα−feasible for αlarge enough. Assume further that Cis a closed d−dimensional Riemannian manifold
embedded in RDandfis a smooth probability density function with lim infMf(x)>0.
Then there exist ϵ>0, constants δ,c,γ > 0and an integer ¯nsuch that for n≥¯n, we have for α>α 0,
P(F(α,ϵ,n )c)≤e−cnγ.
Conversely, for every δ>0there isα0−δ<α<α 0such that for every n≥¯n,
P(F(α,ϵ,n )c)≥1−e−cnγ.
Proof.The proof is given in Appendix A
Remark 3.3. Following (9), given αandd, the constant γcan be chosen in the open interval (0,α(2α+d)). It is
an open problem to determine ¯nas a function of αandd.
3.1 Example with piecewise constant densities (clutter)
InRd, we denote the d−dimensional ball centered in the origin 0with radius rbyBd(r), and the volume of Bd(1)
byωd. Consider two d−dimensional ring C1,C2centered at 0contained in an hypercube K:C1=Bd(5
4)∖ Bd(1),
C2=Bd(10
4)∖ Bd(9
4),K= [−3,3]d. LetPCbe the uniform measure supported on C=C1∪C2, andPKbe the
uniform measure supported on K. SupposeQn={q1,...,qn}is a random sample generated from a mixture of PC
andPK,
X1,...,Xn≡Pcl∼λPC+ (1−λ)PK, (3.2)
withλ∈[0,1]being a proportion parameter that quantifies the signal-to-noise ratio in the sample (see Figure 1a).
This model is usually called clutter noise in the literature. We are observing data points near or in C, but we aim at
finding a bi-partition of Ksuch thatC1andC2are strictly contained in two different clusters. We plug the sample
Fermat distance in the K−medoids algorithm, that we denote Fermat K−medoids (see Section 5 for more details).
We are interested in the relation between the parameter αand the predictions for any point in C1∪C2.
Notice that the distribution Pcladmits a density fwith respect to the Lebesgue measure,
f(x) =λ
|C1|+|C2|(1C1(x) +1C2(x)) + (1−λ)1
|K|1K(x), (3.3)
where|C1|,|C2|and|K|denote the volume of C1,C2andK, respectively.
Our goal now is to find an explicit bound for the critical parameter. The argument follows the same lines as the proof
of Proposition 2.2 but with explicit quantities. The idea is that it is sufficient to set αsuch that the Fermat cost of
the shortest straight line between C1andC2and outside C1∪C2, is always greater than any Fermat distance between
two points inside the same cluster. We need to compute the longest geodesic distance (with respect to the Euclidean
norm) lying in Cdenoted by L. This model’s explicit upper bound is L≤5π/2. In the proof of Proposition 2.2, the
upper bound on Lis computed for a general case and depends on the r-covering number of C, with 0<r<rch(C).
We have, rch(C) =dist(C1,C2)/2 = 1/2,a1=λ/(|C1|+|C2|), anda0= 1−λ/|K|.
Any continuous path that intersects both C1andC2has an Euclidean length of at least dist (C1,C2) = 1. Then, the
Fermat distance of any path lying in two distinct clusters is at least a(1−α)/d
0.It suffices to choose αsuch that the
Fermat cost between C1andC2is always greater than
a(1−α)/d
0>La(1−α)/d
1, α> 1 +dlog(L)
log(a1/a0). (3.4)
We have obtained the upper bound
α0≤1 +dlog/parenleftbig5π
2/parenrightbig
log(a1/a0), (3.5)
meaning that (C1,C2)areα−clusters for every α≥α0. Figure 1 shows the result of applying K−medoids for
different choices of αand different values of λwith sample size n. We observe that we can achieve clustering above
this threshold for the theoretical values of α0found using (3.5).
5Published in Transactions on Machine Learning Research (/)
-3 33True labels
noise
-3 33alpha = 1
-3 33alpha = 1.2
-3 33alpha = 1.4
-3 33alpha = 1.6
-3 33alpha = 1.8
-3 33alpha = 2
-3 33alpha = 3
-3 33alpha = 4
-3 33alpha = 5
(a)
-3 33True labels
noise
-3 33alpha = 1
-3 33alpha = 1.2
-3 33alpha = 1.4
-3 33alpha = 1.6
-3 33alpha = 1.8
-3 33alpha = 2
-3 33alpha = 3
-3 33alpha = 4
-3 33alpha = 5
(b)
Figure 1: An example of predictions of the Fermat K−medoids algorithm on data with common density f
for (A)λ= 0.99,n= 1000and (B)λ= 2/3,n= 1500. The theoretical values for the critical value of α
based on (3.5) give α0≤1.45andα0≤2.2, respectively, meaning that above those values we are able to
perform an efficient clustering with high probability for large n.
Remark that α0is the theoretical critical parameter. In practice, we use the sample Fermat distance, so the threshold
fluctuates (see Proposition 3.2). This synthetic example illustrates that the parameter αcalibrates the sensitivity of
the distance to both the geometry and density of the data. Note that when α= 1, the Fermat distance boils down to
the Euclidean distance, and predictions result from the classical K−medoids algorithm. By pushing up the αvalue,
the algorithm captures the structure of both rings despite the noise and performs a partition that perfectly identifies
C1andC2.
4 Variability as a function of α
When opting for a large value of αto get a distance that aptly captures the intricacies of the geometry and the data
distribution, it is essential to recognize that this choice can introduce two noteworthy challenges.
1.Heightened variability of Sample Fermat Distance . Choosing a large value of αcan lead to amplified vari-
ability within the sample Fermat distance.
6Published in Transactions on Machine Learning Research (/)
2.Floating point arithmetic. There is the concern of numerical inaccuracies arising due to the utilization of
largerαthat lead to the manipulation of numbers that differ in orders of magnitude.
This section delves into the first of these issues, furnishing preliminary insights into the exploration of variance within
the sample Fermat distance statistic.
Derivingexactcharacterizationsofthevarianceof n(α−1)/dDQn,α(x,y)indimensionslargerthanonehavebeenshown
to be challenging (2; 13) and is still an open problem in the context of First Passage Percolation. The one-dimensional
case is much more tractable, but the computations are not obvious even in this case.
Since we are interested in comparing the variability of the Fermat distance for different values of α, we need a measure
that does not depend on the scale of the statistic as αvaries. We consider the coefficient of variation of Dd
Qn,α,
/radicaligg
Var/parenleftbig
Dd
Qn,α/parenrightbig
E[Dd
Qn,α]2, (4.1)
whenαvaries andnis fixed. In particular, we aim to understand the effect of the parameter αon the variation,
giving an idea of a threshold which we might better set αbelow in practice.
4.1 One-dimensional case
For the one-dimensional case, we can explicitly calculate the spacings between consecutive points in the optimal path.
In that case, we are going to consider a sample Qn={q1,q2,...,qn}of uniform independent points in [0,1]and study
the microscopic Fermat distance. In this simple case, it is given by,
Dd=1
Qn,α=n/summationdisplay
i=0|q(i+1)−q(i)|α=:n/summationdisplay
i=0∆α
i (4.2)
withq(1),...,q(n)the order statistics of Qn,q(0)= 0andq(n+1)= 1.
Proposition 4.1. The expectation and variance of Dd=1
Qn,αfor the uniform case verify
lim
n→∞E[nα−1Dd=1
Qn,α] = Γ(α+ 1),
lim
n→∞nVar[nα−1Dd=1
Qn,α] = Γ(2α+ 1)−(α2+ 1)Γ(α+ 1)2
Proof.The proof is provided in the Appendix A.
This result shows how the variability of the Fermat distance increases exponentially with the value of α. If we consider
the coefficient of variation given by
/radicaligg
Var(Dd=1
Qn,α)
E[Dd=1
Qn,α]2≍
n→∞1√n/radicalbigg
Γ(2α+ 1)−(α2+ 1)Γ(α+ 1)2
Γ(α+ 1)2(4.3)
this scales exponentially as a function of α, given large values even for small values of α. Notice that this result
further implies that the constant µ(α,d)involved in Equation (1.3) satisfies µ(α,1) = Γ(α+ 1).
4.2 Higher dimensions
Inhigherdimensions, asalreadymentioned, theasymptoticbehaviorin nofthevarianceofFermatdistanceisanopen
problem even for Poisson point processes in Euclidean space. Similarly, we cannot prove a sufficiently informative
bound on the variance of the Fermat distance in terms of the parameter α. This is due to the difficulty in finding
quantitative lower bounds for the Fermat distance.
However, we conjecture that the coefficient of variation scales exponentially in θ=α/d. This suggests that we should
chooseαin order to guarantee that α/dis not too large. We now prove a bound for the moments of the Fermat
distance for an intensity one Poisson process in Euclidean space, showing that moments of order kare at most of
order Γ(kθ+ 1).
7Published in Transactions on Machine Learning Research (/)
Proposition 4.2. LetQnbe a Poisson process with intensity n in the hypercube of Rd. Then we have,
E/parenleftig/parenleftbig
nα−1
dDd
Qn,α(0,e1)/parenrightbigk/parenrightig
≤2edθ+α/2θθ(1 + Γ(kθ+ 1)).
Proof.The proof is provided in the Appendix A.
5 Experiments
We consider here a series of clustering experiments involving the Fermat distance for different choices of α. We are
going to evaluate the trade-off between small values of αfor which clustering is not feasible (Section 2)and large
values ofαwhere finite sampling effects distort the distance (Section 4).
5.1 Fermat K−medoids
We use the K−medoids algorithm for clustering (15; 10), a robust version of K−means in which the centroids of
each cluster are forced to be a point in the set Qn. An advantage of K−medoids is that different notions of distances
can be used, leading to different partitions. The algorithm consists of alternating updates in the estimated cluster
centroids ˆciand cluster ˆCi⊂Qnuntil the cluster assignment does not change. These updates are sequentially given
by
ˆci←arg minc∈Ci/summationdisplay
x∈ˆCiDQn,α(x,c), (5.1)
ˆCi←{x∈Qn:DQn,α(ˆci,x)≤DQn,α(ˆcj,x)∀j̸=i}. (5.2)
Note that different values of αgive different partitions. If α= 1, the sample Fermat distance boils down to the
Euclidean distance, and both Fermat K−medoids and classical K−medoids coincide. For α > 1, clusters are not
necessarily convex.
5.2 Synthetic data
We evaluate the performance of the Fermat K-medoids clustering algorithm when analysing the Swiss-roll generated
dataset. We consider four clusters sampled from a Gaussian distribution in two dimensions with different means and
then mapped to three dimensions using the map (x,y)∝⇕⊣√∫⊔≀→(xcos(ωx),ay,x sin(ωx))fora= 3,ω= 15. We consider a
total ofn= 1000random samples, equally split between four different clusters. This generates the dataset illustrated
in Figure 2. When evaluating the performance of Fermat K-medoids using adjusted mutual information, adjusted
rand index, accuracy, and F1 score (19; 27; 20), we observe that smaller values of αresult in poor performances for
all performance metrics. As we increase the value of α, the performance increases until the performance decays again
for large values of α, having an optimal value at some middle range of values for α. We also observe large variances
in performance for large values of α, resulting in decaying median performances over new samplings. We compare
the performance using K-medoids but with the distance obtained using Isomap (25) and C-Isomap (24).
These experiments show that there is an optimal range of values of αthat results in better clustering performance.
This illustrates the point made in previous sections, showing that both small and large values of αresult in poor
performances.
5.3 Clustering MNIST
We now consider an example that inhabits a higher-dimensional and inherently more realistic realm: the digits 3
and 8 extracted from the MNIST dataset. Our approach begins by subjecting the data to preprocessing through
PCA, resulting in a reduced-dimensional representation of dimension 40. We then cluster this representation using
K−medoids with the Fermat distance. We compute the mean adjusted mutual information (AMI) and compare it
to the performances of K−means with Euclidean distance and a robust EM procedure (21; 22) recognized for its
consistently good performance as a fully unsupervised method.
The findings are consolidated in Figure 3. In this instance, the clear presence of an advantageous parameter range
becomes apparent. Notably, the remarkable advantage gained from operating within this window, compared to
8Published in Transactions on Machine Learning Research (/)
α
αα
α
Figure 2: Different clustering performance indices for the Swiss Roll dataset. Solid lines correspond to the
mean, median, and best performance archived by Fermat K-medoids over different samplings of the dataset.
Dashed lines correspond to the performance obtained by the best output between Isomap and C-Isomap.
This experiment has been presented previously in ICLR 2018 Workshop Track (23).
0.6
0.5
0.4
0.3
0.2
0.1
1 2 4 8 6Adjusted Mutual Information (AMI)
αRobust EM
K-Means
Fermat K-Medoids
Figure 3: Performance of Fermat K−medoids algorithm compared to standard K−means and Robust
expectation-maximization. Simulations credit to Alfredo Umfurer.
employing the Euclidean distance, is striking. Furthermore, it is remarkable that even a relatively straightforward
algorithm like K−medoids can yield performance akin to significantly more computationally intricate methods when
paired with the appropriate distance parameter.
9Published in Transactions on Machine Learning Research (/)
Mean value
25 Percentile
75 PercentileMean value
25 Percentile
75 PercentileMean value
25 Percentile
75 PercentileCoeﬃcient of Variationd=4 d=3 d=2
α/d
Figure 4: Coefficient of variation for the uniform distributions as a function of α/dfor different ambient
dimensions d= 2,3,4.
5.4 Coefficient of variation.
We conclude this section by conducting a numerical exploration of the influence of noise on the coefficient of variation
of the Fermat distance. We plot in Figure 4 the coefficient of variation defined in 4.1 in the case of a uniform
distribution on [0,1]das a function of α/d. As conjectured, we observe an exponential behaviour in the parameter
α/d, demonstrated in dimension one and conjectured in higher dimensions.
Acknowledgments
We acknowledge the support of the STIC AMSUD Project LAGOON for Pablo Groisman and Matthieu Jonckheere
as well as the DATAIA support for Facundo Sapienza and Laure Ferraris.
References
[1] Eddie Aamari, Clément Berenfeld, and Clément Levrard. Optimal reach estimation and metric learning. arXiv
preprint arXiv:2207.06074 , 2022.
[2] Antonio Auffinger, Michael Damron, and Jack Hanson. 50 years of first-passage percolation , volume 68 of Univ.
Lect. Ser. Providence, RI: American Mathematical Society (AMS), 2017.
[3] Jeff Calder and Mahmood Ettehad. Hamilton-jacobi equations on graphs with applications to semi-supervised
learning and data depth. Journal of Machine Learning Research , 23(318):1–62, 2022.
[4] A. Carpio, L. L. Bonilla, J. C. Mathews, and A. R. Tannenbaum. Fingerprints of cancer by persistent homology.
bioRxiv, 2019.
[5] Ana Carpio, LL Bonilla, JC Mathews, and AR Tannenbaum. Fingerprints of cancer by persistent homology.
bioRxiv, page 777169, 2019.
[6] Alejandro Cholaquidis, Ricardo Fraiman, and Leonardo Moreno. Universally consistent estimation of the reach.
J. Stat. Plann. Inference , 225:110–120, 2023.
[7] A. Erdélyi and F. G. Tricomi. The asymptotic expansion of a ratio of gamma functions. Pacific Journal of
Mathematics , 1(1):133 – 142, 1951.
[8] Herbert Federer. Curvature measures. Transactions of the American Mathematical Society , 93(3):418–491, 1959.
[9] Ximena Fernández, Eugenio Borghini, Gabriel Mindlin, and Pablo Groisman. Intrinsic persistent homology via
density-based metric learning. Journal of Machine Learning Research , 24(75):1–42, 2023.
[10] J. Friedman, T. Hastie, and R. Tibshirani. The elements of statistical learning , volume 1. Springer series in
statistics New York, 2001.
10Published in Transactions on Machine Learning Research (/)
[11] Pablo Groisman, Matthieu Jonckheere, and Facundo Sapienza. Nonhomogeneous euclidean first-passage perco-
lation and distance learning. Bernoulli , 28(1):255–276, 2022.
[12] C. D. Howard and C. M. Newman. Euclidean models of first-passage percolation. Probability Theory and Related
Fields, 108(2):153–170, 1997.
[13] C. D. Howard and C. M. Newman. Geodesics and spanning trees for Euclidean first-passage percolation. Ann.
Probab., 29(2):577–623, 2001.
[14] S. J. Hwang, S. B. Damelin, and A. O. Hero, III. Shortest path through random points. Ann. Appl. Probab. ,
26(5):2791–2823, 2016.
[15] Leonard Kaufman. Partitioning around medoids (program pam). Finding Groups in Data , 344:68–125, 1990.
[16] A. Little, D. McKenzie, and J. M. Murphy. Balancing geometry and density: Path distances on high-dimensional
data.SIAM Journal on Mathematics of Data Science , 4:1, 2022.
[17] Andriana Manousidaki, Anna Little, and Yuying Xie. Clustering and visualization of single-cell rna-seq data
using path metrics. bioRxiv, pages 2021–12, 2021.
[18] D. Mckenzie and S. Damelin. Power weighted shortest paths for clustering euclidean data. Foundations of Data
Science, 1(3):307, 2019.
[19] Marina Meilă. Comparing clusterings—an information based distance. Journal of multivariate analysis ,
98(5):873–895, 2007.
[20] David M. W. Powers. Evaluation: from precision, recall and f-measure to roc, informedness, markedness and
correlation, 2020.
[21] Violeta Roizman, Matthieu Jonckheere, and Frédéric Pascal. A flexible em-like clustering algorithm for noisy
data.arXiv preprint arXiv:1907.01660 , 2019.
[22] Violeta Roizman, Matthieu Jonckheere, and Frédéric Pascal. Robust clustering and outlier rejection using the
mahalanobis distance distribution. In 2020 28th European Signal Processing Conference (EUSIPCO) , pages
2448–2452, 2021.
[23] F. Sapienza, P. Groisman, and M. Jonckheere. Weighted geodesic distance following Fermat’s principle. In
International Conference on Learning Representation , 2018.
[24] Vin Silva and Joshua Tenenbaum. Global versus local methods in nonlinear dimensionality reduction. Advances
in neural information processing systems , 15, 2002.
[25] Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. A global geometric framework for nonlinear dimen-
sionality reduction. Science, 290(5500):2319–2323, 2000.
[26] Nicolás García Trillos, Anna Little, Daniel McKenzie, and James M Murphy. Fermat distances: Metric approx-
imation, spectral convergence, and clustering algorithms. arXiv preprint arXiv:2307.05750 , 2023.
[27] Nguyen Xuan Vinh, Julien Epps, and James Bailey. Information theoretic measures for clusterings compar-
ison: Variants, properties, normalization and correction for chance. Journal of Machine Learning Research ,
11(95):2837–2854, 2010.
[28] Mariia Vladimirova, Stéphane Girard, Hien Nguyen, and Julyan Arbel. Sub-weibull distributions: Generalizing
sub-gaussian and sub-exponential properties to heavier tailed distributions. Stat, 9(1):e318, 2020. e318 sta4.318.
11Published in Transactions on Machine Learning Research (/)
A Proofs
Proof of Proposition 2.2. We prove that there exists α0≥1such that for all α≥α0there isϵ >0such that for
1≤i≤mandx,y∈Ci,
Dd
α(x,y)≤Dd
α(x,y′)−ϵ,for ally′∈Cj, j̸=i.
First, we bound from below the Fermat distance between two points in two different sets. By assumption, rch (C) =
τ >0. Sinceτ≤min
i̸=jdist(Ci,Cj)/2, for allx /∈C,f(x)≤a0and any path from CitoCjhas measure at least 2τin
the complement of Cwheni̸=j, we have for x∈Ci,y′∈Cj,
Dd
α(x,y′)≥a(1−α)/d
0 2τ. (A.1)
Second, we bound from above the Fermat distance between two points xandyin the same cluster Ci. Since
Cis compact, we can consider a finite covering by NrD-dimensional balls, BD, with radius r < τ, and centers
V={v1,...,vNr}⊂C,i.e.,
C⊂Nr/uniondisplay
l=1BD(vl,r)⊂Cr.
We are going to build a path from xtoyinsideCr
i⊂Crand project it onto C. By construction, we know that
the path will be projected onto Ci. LetVi={V∩Ci}. Observe that (BD(vl,r):vl∈ Vi)is a covering of Ci
by card (Vi) =Nr(Ci)< Nrballs. Consider the weighted graph Giwith vertex setViin which two vertices are
connected if and only if their respective balls intersect. The weight of an edge is given by the Euclidean distance
between the two points. By construction, this graph is connected, and the weight of each edge is at most 2r. We
connectxto the center v(x)∈Viof a ball containing it. We have |x−v(x)|≤r. Similarly, we connect yto a
v(y)∈Visuch that|y−v(y)|≤r.
By definition, the shortest path from xtoyinsideGi, denoted by γ, cannot go through the same vertex more than
once. Then the Euclidean length of γ, denoted by|γ|verifies
|γ|≤Nr(Ci) 2r≤Nr2r.
Asγ⊂Crandr<τ, the projection of γontoC,πC(γ) =γi, is well defined. Moreover πC(γ)⊂Ci. The projection
map is Lipschitz, with Lipschitz constantτ
τ−r((8, Theorem 4.8)). Then we have,
|γi|=|πC(γ)|≤τ
τ−rNr2r.
The pathγi, fromxtoy, is contained in Ci⊂Cand for allx∈Ci,f(x)≥a1. Then,
Dd
α(x,y)≤ˆ
γi1
f(α−1)/d≤a(1−α)/d
1τ
τ−rNr2r. (A.2)
Next, we calibrate αto ensure the clustering feasibility, i.e.,one get,
a(1−α)/d
0 2τ >a(1−α)/d
1τ
τ−rNr2r, (A.3)
which is equivalent to,
α>1 +dlog/parenleftigr
τ−rNr/parenrightig
log(a1/a0).
Now, suppose that Nris optimal, i.e., it is the smallest number of balls BD(vl,r)that covers C. AsCis compact,
there exists a positive constant c(depending on the dimension) such that Nr≤cr−d∨1, wheredis the intrinsic
dimension of C. Chooser=τ
2to get,
α>1 +dlog/parenleftbig
c(τ
2)−d∨1/parenrightbig
log(a1/a0). (A.4)
12Published in Transactions on Machine Learning Research (/)
The first part of the proof finishes by taking β0= (c(τ
2)−d∨1).
IfCis smooth enough so that the length of geodesics in Cis uniformly bounded by R, we can take instead γito be
the geodesic from xtoyto obtain the bound
Dd
α(x,y)≤ˆ
γi1
f(α−1)/d≤a(1−α)/d
1R,
instead of (A.2). The rest of the proof follows verbatim to obtain
α>1 +dlog(R/2τ)
log(a1/a0). (A.5)
Proof of Proposition 2.5. The proof follows the same lines as the one of Proposition 2.2, but we need to be more
careful. Recall that Cis compact, f≥a1inCanda0is such that there exists a region around each cluster that
strictly separates the level-sets f−1(a1)andf−1(a0)with the projection being well defined. More precisely, we have
that for
η= inf/braceleftbig
|s−t|:s∈f−1([0;a0]), t∈f−1([a1;∞])/bracerightbig
>0,
the inequality τ=rch(C)>ηis verified.
We first prove that rch (Cη) =rch(C)−η=τ−η. To do that, observe that for any δ >0,(Cη)δ=Cη+δ. In fact,
ifx∈(Cη)δ, asCis compact, we know that there exists at least one projection of xontoCdenoted by πC(x).
Applying the customs passage theorem, we have {[x,πC(x)]∩∂(Cη)}̸=∅.Callzthe point lying at this intersection.
We have,
dC(x) = inf
p∈C|x−p|≤|x−z|+ inf
p∈C|z−p|≤δ+η.
Thenx∈Cη+δ. On the other hand, for x∈Cη+δeitherdC(x)≤ηandx∈(Cη)δ, ordC(x)> η. Suppose
dCη(x)>δ. As we assumed that dCη(x)>δ,|x−z|>δand
|x−πC(x)|=|x−z|+|x−πC(x)|>δ+η.
A contradiction. Hence dCη(x)≤δandx∈Cη+δ= (Cη)δ. This proves our claim
rch(Cη) =rch(C)−η=τ−η.
The bound from below for the macroscopic Fermat distance between two points lying in two different clusters is the
same as in A.1, and we omit it here.
The upper bound for the macroscopic Fermat distance between two points lying in the same cluster is also very
similar. The only difference is that we need to build an r-covering of Cηinstead ofC, withr<rch(Cη) =τ−η. We
define the shortest path (in the neighborhood graph restricted to Cη
i) inside (Cη
i)r=Cη+r
i,1≤i≤m, in the same
manner. The length of this path is smaller than 2(τ−η)Nr, whereNris the covering number of Cη.
We first project this path on Cη, which will be projected on Cη
iby construction. We project this new path on C,
which is still by construction projected on Ci. Applying two times Federer’s theorem ((8), Theorem 4.8-(8)), the
condition A.3 on αbecomes
a(1−α)/d
0 2τ >a(1−α)/d
1/parenleftbigg
τ−η
τ−η−r/parenrightbigg/parenleftbigg
τ
τ−η/parenrightbigg
Nr2r,
which is equivalent to
α>1 +dlog/parenleftig
r
τ−(r+η)Nr/parenrightig
log(a1/a0).
13Published in Transactions on Machine Learning Research (/)
Next, we prove Proposition 3.2. We will use the following lemma, whose proof is elementary and is omitted.
Lemma A.1. AssumeMis compact and that for all x∈M,|f(x)|> ι > 0. Then for every ϵ >0there isδ >0
such that for every u,v∈Mwith|u−v|≤δwe have|Dd
α(u,x)−Dd
α(v,x)|≤ϵfor everyx∈M.
Proof of Proposition 3.2. We first prove that the definition of α-clusters implies with overwhelming probability for n
large enough. Using the definition of α0, forα>α 0>1there exists ϵ>0such that,
Dd
α(x,y)≤Dd
α(x,y′)−ϵ,for allx,y∈Ci, y′∈Cj, j̸=i. (A.6)
Consider the events,
An,α=/braceleftig/vextendsingle/vextendsingle/vextendsinglen(α−1)/dDQn,α(x,y)−µDd
α(x,y)/vextendsingle/vextendsingle/vextendsingle≤ϵ/3,for allx∈Qn/bracerightig
.
By Proposition 2.6 in(9), there is γ >0andc>0such that for nlarge enough
P(Ac
n,α)≤e−cnγ∀1≤i≤m.
Now, onAn,α, we have for x,y∈Ciandy′∈Cj,j̸=i,
n(α−1)/dDQn,α(x,y)≤µDd
α(x,y) +ϵ/3
≤µDd
α(x,y′) +ϵ/3−ϵ(by the clustering condition (2.1))
≤n(α−1)/dDQn,α(x,y′) + 2ϵ/3−ϵ.
Since 2ϵ/3−ϵ=−ϵ/3<0, we have
n(α−1)/dDQn,α(x,y)≤n(α−1)/dDQn,α(x,y′)−ϵ/3,
onAn,α. Hence
P(F(α,n)c)≤P(Ac
n,α)≤e−cnγ,
fornlarge enough.
Next, takeα<α 0such that (C,f)is notα−feasible and suppose that the microscopic clustering conditions (3.1) are
satisfied, i.e., there exists ϵ>0such that for every i̸=j,x,y∈Ci∩Qnandy′∈Cj∩Qnwe have
n(α−1)/dDQn,α(x,y)≤n(α−1)/dDQn,α(x,y′)−ϵ. (A.7)
So, onAn,αwe have,
µDd
α(x,y)≤n(α−1)/dDQn,α(x,y) +ϵ/3
≤n(α−1)/dDQn,α(x,y′) +ϵ/3−ϵ(by (A.7))
≤µDd
α(x,y′) + 2ϵ/3−ϵ.
We have,
Dd
α(x,y)≤Dd
α(x,y′)−ϵ/3µ,
whichcannotholdsince (C,f)isnotα−feasible. Thismeansthat An⊂F(n,α)candtheconclusionoftheproposition
follows.
Proof of proposition 4.1. The study of the statistic Dd=1
Qn,αboils down to the study of the uniform spacings given by
∆i=q(i+1)−q(i)fori= 1,2,...,n−1,∆0=q(1)and∆n= 1−q(n). Using that the joint density pof the order
statisticsq(1),q(2),...,q(n)is given by
p(t1,t2,...,tn) =n! 1{0≤t1≤...≤tn}≤1}
14Published in Transactions on Machine Learning Research (/)
it is easy to derive the joint distribution of the vector (∆0,..., ∆n−1)which is uniformly distributed on the n
dimensional simplex denoted by
Sn=/braceleftigg
(s0,s1,...,sn−1)∈Rn, s0,s1,...,sn−1>0,n−1/summationdisplay
i=0si≤1/bracerightigg
.
That is (∆0,..., ∆n)∼Dir(a), where Dir (a)denotes the flat Dirichlet distribution with parameter a∈Rn+1,
a= (1,..., 1)and∆n= 1−/summationtextn−1
i=0∆i. The moments of the Dirichlet distribution can be easily found as
E[∆α
i] =E(∆0,...,∆n)∼Dir((1,1,...,1))[∆α
0] =Γ(n+ 1)
Γ(n+α+ 1)Γ(α+ 1),
E[∆α
i∆α
j] =E(∆0,...,∆n)∼Dir((1,1,...,1))[∆α
0∆α
1] =Γ(n+ 1)
Γ(n+ 2α+ 1)Γ(α+ 1)2.
The relative asymptotic behaviour of Gamma functions given by (7)
Γ(x+a)
Γ(x+b)=xa−b/parenleftbigg
1 +(a−b)(a+b−1)
2x+O/parenleftig1
x2/parenrightig/parenrightbigg
. (A.8)
implies that Γ(n+α+ 1)/(n+ 1)αΓ(n+ 1)→1asn→∞. Then,
lim
n→∞nαE[∆α
i] = Γ(α+ 1), lim
n→∞n2αE[∆α
i∆α
j] = Γ(α+ 1)2.
Finally,
nα−1E[Dd=1
Qn,α] =nα−1E/bracketleftiggn/summationdisplay
i=0∆α
i/bracketrightigg
=E[(n∆1)α]n→∞−−−−→ Γ(α+ 1).
On the other hand, the second moment can be computed as
E[(Dd=1
Qn,α)2] =E/bracketleftigg
(n/summationdisplay
i=0∆α
i)2/bracketrightigg
= (n+ 1)E/bracketleftbig
∆2α
1/bracketrightbig
+n(n+ 1)E[∆α
1∆α
2]
= (n+ 1)/parenleftbig
E/bracketleftbig
∆2α
1/bracketrightbig
−E[∆α
1]2/parenrightbig
+E[Dd=1
Qn,α]2
+n(n+ 1) (E[∆α
1∆α
2]−E[∆α
1]E[∆α
2]).
Rearranging the terms in the last expression, we obtain
nVar[nα−1Dd=1
Qn,α] =n+ 1
n/parenleftbig
E[n2α∆2α
1]−E[nα∆α
1]2/parenrightbig
+ (n+ 1) Cov(nα∆α
1,nα∆2
2).
Asn→∞, the first term on the right-hand side converges to Γ(2α+ 1)−Γ(α+ 1)2. Based again in Equation (A.8),
we have
E[∆α
1∆α
2]−E[∆α
1]E[∆α
2] =/parenleftbigg
Γ(n+ 1)
Γ(n+ 2α+ 1)−Γ(n+ 1)2
Γ(n+α+ 1)2/parenrightbigg
Γ(α+ 1)2
=/parenleftbigg
1
n2α
+/parenleftbigg
1−2α(2α−1)
2n++O(n−2)/parenrightbigg
−1
n2α
+/parenleftbigg
1−α(α−1)
2n++O(n−2)/parenrightbigg2/parenrightbigg
Γ(α+ 1)2
=/parenleftbigg
−α2
n++O(n−2)/parenrightbigg
Γ(α+ 1)2
n2α
+,
wheren+=n+ 1. This implies
lim
n→∞(n+ 1) Cov(nα∆α
1,nα∆2
2) =−α2Γ(α+ 1)2,
15Published in Transactions on Machine Learning Research (/)
which finally gives
lim
n→→∞nVar[nα−1Dd=1
Qn,α] = Γ(2α+ 1)−(α2+ 1)Γ(α+ 1)2.
Proof of Proposition 4.2. Note that we can turn results for a Poisson process of intensity nin the hypercube to a
scaled Poisson process n1/dQnwhich is essentially (up to exponentially small probability events) to a unit Poisson
processPin the hyperplane (see Theorem 2.4 in (12)). Hence with l=n1/d, we replace n(α−1)/dDd
Qn,α(0,e1)by
l−1Dd
P,α(0,le1), andnowworkwithaunitPoissonprocess. Theproofisbasedonconstructingaspecific(sub-optimal)
path that has been considered previously in (13). Define the sets,
Dt(q) ={q+b∈Rd,≤b1,for2≤i≤d}0≤b1≤t,0≤σibi
whereσi=−1qi>0+1qi≥0. Let us now construct inductively the specific path of lpointsq0,...qlsuch that for k≤l,
˜q0= 0,
Xk= inf{t>0,there is a point in Dt(˜qk−1)},
˜qk=particle present in DXk(˜qk−1),
Finally,ql+1=le1. Observe that,
|˜qk−˜qk−1|α≤dα
2Xα
kand|˜ql+1−le1|α≤l/summationdisplay
i=1dα
2Xα
i.
Using basic properties of Poisson processes, we get that the random variables (Xi)1≤i≤lare i.i.d. Now,
P(Xi≥x) =P(no points inDx(0) ) =e−(1/d)xd.
Therefore,
P(Xα
i≥x) =e−(1/d)xd/α=e−(1/d)x1/θ.
Hence,
Dd
α,P(0,le1)≤l/summationdisplay
i=1|˜qi−˜qi−1|α+|˜ql−le1|α,
which gives that
P(Dd
α,P(0,le1)≥lx)≤P/parenleftigg
2N/summationdisplay
i=1|˜qi−˜qi−1|α>lx/parenrightigg
,
≤P/parenleftigg
2l/summationdisplay
i=1|˜qi−˜qi−1|α>lx/parenrightigg
≤P/parenleftiggl/summationdisplay
i=1Xα
i>lx/ 2dα/2/parenrightigg
.
Note that if α>d, we cannot use the usual large deviation bounds as the variable Xidoes not have an exponential
moment. We can, however, use their usual moments. Our computations are similar to the ones in (28).
Observe that P(Y≥x) =e−x1
θ/dimplies E(Yγ) =dγθΓ(γθ+ 1).
16Published in Transactions on Machine Learning Research (/)
LetYi=Xα
iand takeγ≥1. Then,
P/parenleftiggl/summationdisplay
i=1Yi≥t/parenrightigg
≤E/bracketleftig/parenleftigl/summationdisplay
i=1Yi/parenrightigγ/bracketrightig
t−γ, (A.9)
≤/parenleftiggl/summationdisplay
i=1E(Yγ
i)1/γ/parenrightiggγ
t−γ, (A.10)
=lγdγθΓ(γθ+ 1)t−γ, (A.11)
≤lγdγθ(γθ)γθt−γ. (A.12)
We use the bound Γ(x+ 1)≤xxforx≥1in the last line. Taking tsuch thatlγdθγ(γθ)γθt−γ=e−γ, we can state
that for all γ≥1
P/parenleftiggl/summationdisplay
i=1Yi≥edθ(γθ)θl/parenrightigg
≤e−γ,
andγθc=xwithc= 2edθ+α/2θθ, leads to
P/parenleftigg
1
ll/summationdisplay
i=1Yi≥x
2dα/2/parenrightigg
≤e−(x/c)1/θ,
for allx>c. Hence
E/parenleftigg
1
ll/summationdisplay
i=1Yi/parenrightiggk
≤c+ˆ∞
ce−(x/c)1/kθdx,
≤c+cΓ(kθ+ 1).
We obtain,
E/parenleftig/parenleftbig
l−1Dd
α,P(0,le1)/parenrightbigk/parenrightig
≤c+cΓ(kθ+ 1).
17