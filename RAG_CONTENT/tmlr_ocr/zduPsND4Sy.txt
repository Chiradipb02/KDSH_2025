Under review as submission to TMLR
Learning Nonparametric Differential Equations via Multivari-
ate Occupation Kernel Functions
Anonymous authors
Paper under double-blind review
Abstract
Learning a nonparametric system of ordinary differential equations from trajectories in a
d-dimensional state space requires learning dfunctions of dvariables. Explicit formulations
often scale quadratically in dunless additional knowledge about system properties, such
as sparsity and symmetries, is available. In this work, we propose a linear approach, the
occupation kernel method (OCK), using the implicit formulation provided by vector-valued
Reproducing Kernel Hilbert Spaces. The solution for the vector field relies on multivariate
occupation kernel functions associated with the trajectories and scales linearly with the
dimension of the state space. We validate through experiments on a variety of simulated and
real datasets ranging from 2 to 1024 dimensions. The OCK method outperforms all other
comparators on 3 of the 9 datasets on full trajectory predictions, and 4 out of the 9 datasets
on next point prediction.
1 Introduction
1.1 Description of the problem
The task of learning dynamical systems derived from ordinary differential equations has garnered a lot of
interest in the past couple of decades. In this framework, we are often provided noisy data from trajectories
representative of the dynamics we wish to learn, and we provide a candidate model to describe the dynamics.
We present a learning algorithm for the vector field guiding the dynamical system that scales linearly with
the dimensionality of the dynamical system’s state space.
1.2 Examples of applications
High-dimensional dynamical systems play a crucial role in various domains. For instance, finite difference
methods are commonly used to solve partial differential equations (PDEs), resulting in dynamical systems
whose dimensionality scales with the PDE’s discretization size Moczo et al. (2004). When the dynamics can be
derived from physical principles, modeling them using differential equations leads to favorable generalization.
1.3 State of the art
We compare various system identification methods that apply to our relevant experiments, including sparse
identification of nonlinear dynamics, reduced order models, deep learning methods, and neural ODEs. These
methods learn system dynamics from trajectories in the state space with good accuracy, scale to tens,
hundreds, or thousands of state dimensions, and are robust to noise. A detailed description of the comparators
is presented in section 4.3.
1.4 Main contributions
We propose to use the occupation kernel (OCK) method, that learns a dynamical system and scales linearly
with the number of dimensions of the state space. Building upon Rosenfeld et al. (2019a;b); Russo et al.
1Under review as submission to TMLR
(2021), our work provides an alternative derivation with the help of vector-valued Reproducing Kernel
Hilbert spaces (RKHSs) which allows for the reduction in computational complexity and extensions to
physics informed kernels. We also emphasize the simplicity of the algorithm and demonstrate competitive
performance on high-dimensional data, as well as noisy data. Finally, in Section 5, we craft an RKHS for
learning divergence-free vector fields, demonstrating the versatility of the OCK method.
2 Background
2.1 RKHS for modeling vector fields
Scalar RKHSs are spaces of real-valued functions that were made popular for their use in constructing the
kernelized support vector machine classifier Schölkopf & Smola (2002) chapter III. Scalar RKHSs generalize
to vector-valued RKHSs. Vector-valued RKHSs, or simply RKHSs, provide simple non-parametric models -
models whose parameter size scales with the training set size - for vector fields. A matrix-valued kernel fully
characterizes an RKHS. The choice of this kernel is left to the user and allows for encoding various properties
of the functions in the corresponding RKHS. Choosing a kernel with Lipschitz continuous diagonal elements
guarantees that all functions in the RKHS Hare Lipschitz continuous. This, in turn, guarantees the local
existence and uniqueness of the associated ordinary differential equations ˙x=f(x)wheref∈H. Furthermore,
one may allow the inclusion of physics-inspired constraints into the function space by appropriately choosing
the kernel, and an example of a divergence-free RKHS is provided in Section 5. As is well known in
the scalar case, RKHSs are spaces of functions amenable to constrained and unconstrained optimization
thanks to the representer theorem. Finally, kernels come in two forms: implicit and explicit. The former
implicitly characterizes a mapping from Rdto a Hilbert space, which can be of infinite dimension. Examples
include the family of Matérn kernels, which contain the familiar Gaussian and Laplace kernels. The latter
explicitly characterizes a mapping from RdtoRpfor somep. Examples include polynomial kernels as well as
random Fourier features. Both kernel types will be used in the experiments in Section 4. We now provide a
mathematical presentation of kernels and RKHSs.
2.2 Vector valued RKHS
Definition 1.LetM(d,d)be the set of (d,d)matrices,d≥1. A positive definite matrix-valued kernel, or
simply kernel K is a mapping: Rd×Rd∝⇕⊣√∫⊔≀→M (d,d), such that
1. symmetric: K(x,x′) =K(x′,x)T, for anyx,x′∈X;
2. positive definite: for any x1,...,xninX, and for any w1,...,wninRd,
/summationdisplay
i,jwT
iK(xi,xj)wj≥0 (1)
The simplest kernels are the separable kernels
K(x,x′) =k(x,x′)A (2)
wherekis a positive definite scalar kernel and Ais a positive semi-definite matrix. When A=I,His made
ofdcopies of the RKHS of k. All kernels used in our experiments in Section 4 are of this form, while the
divergence-free kernel in Section 5 is not a separable kernel. Similarly to the scalar case, there are three ways
to characterize a vector-valued RKHS. They are presented in the supplementary material for completeness.
Here, we briefly present the construction using the Riesz representation theorem, critical in developing the
OCK algorithm.
Definition 2.LetHbe a Hilbert space of functions Rd→Rdsuch that for any x∈Rd, the evaluation
functionalf∝⇕⊣√∫⊔≀→f(x)is continuous, that is, there is a constant Mx∈R, such that
||f(x)||Rd≤Mx||f||H (3)
then, using the Riesz representation, for any v∈Rd, there exists a unique ϕx,v∈Hsuch that
vTf(x) =⟨f,ϕx,v⟩ (4)
2Under review as submission to TMLR
Define the matrix-valued kernel
Kij(x,x′) =/angbracketleftbig
ϕx,ei,ϕx′,ej/angbracketrightbig
,i,j= 1...d (5)
wheree1,...,edis the natural basis of Rd. ThenKis a kernel, His the RKHS of Kandϕx,v(·) =K(·,x)v.
2.3 Occupation kernel functions for vector-valued RKHS
Central to this work is the notion of occupation kernel functions. Let Hbe an RKHS of functions from Rd
toRdwith kernel K. Consider a parametric curve [a,b]→Rd,t∝⇕⊣√∫⊔≀→x(t)and define the functional from H
toRd,Lx(f) =´b
af(x(t))dt.Lxis linear. If x∝⇕⊣√∫⊔≀→Kii(x,x)is continuous for each i= 1...d, thenLxis also
continuous1. For eachv∈Rd, the occupation kernel function L∗
x(v)is then, due to the Riesz representation
theorem, the element in Huniquely defined by the equation vTLx(f) =⟨f,L∗
x(v)⟩. Note that for any w∈Rd
wTL∗
x(v)(y) =⟨L∗
x(v),K(·,y)w⟩
=⟨K(·,y)w,L∗
x(v)⟩
=vTLx(K(·,y)w)
=vTˆb
aK(x(t),y)wdt
=wT/bracketleftiggˆb
aK(y,x(t))dt/bracketrightigg
v (6)
thus
L∗
x(v) =/bracketleftiggˆb
aK(·,x(t))dt/bracketrightigg
v (7)
Here, we used the reproducing property equation 42, the symmetry of the inner product, the definition of the
occupation kernel and the functional Lx, and the symmetry of the kernel. Using the same arguments, we find
/angbracketleftbig
L∗
x(v),L∗
y(w)/angbracketrightbig
=vT/bracketleftiggˆb
aˆb
aK(x(s),y(t))dsdt/bracketrightigg
w (8)
3 Methods
3.1 The occupation kernel (OCK) algorithm
We describe the OCK algorithm in two steps. In the first step, we assume that we observe ntrajectories
or curves in Rd. Each curve is a solution of the ODE defined on a given time interval, with a different
known initial condition. We aim to recover the vector field driving this ODE. Assuming that this vector field
belongs to an RKHS, and under a penalized least square loss, we provide a solution expressed in terms of
occupation kernel functions. In the second step, we relax the hypothesis and assume that snapshots along
these trajectories are provided in place of the trajectories. Rearranging these trajectories and replacing the
integrals with a numerical quadrature thus provides the proposed numerical solution.
3.2 The occupation kernel algorithm from curves
Consider the ODE
˙x=f0(x),x∈Rd(9)
wheref0:Rd→Rdis a fixed, unknown vector field. Consider also ncurvesx1(t),...,xn(t), from [ai,bi]to
Rd. Let us design an optimization setting, an inverse problem, for recovering f0from these trajectories. Let
1A derivation is provided in the supplementary material
3Under review as submission to TMLR
Hbe an RKHS of continuous vector-valued functions from RdtoRd,λ>0, a constant, and let us define the
functional,
J(f) =1
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleˆbi
aif(xi(t))dt−xi(bi) +xi(ai)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Rd+λ∥f∥2
H(10)
By the fundamental theorem of calculus, the first term is minimized when f=f0. However, aside from
degenerate cases, this minimizer is not unique. Thus, the second term is a regularization term. The problem
of minimizing equation 10 over His well-posed. It has a unique solution that can be expressed using the
occupation kernel functions as follows:
Theorem 3.1. The unique minimizer of Jover the RKHS Hwith kernel K is
f∗=n/summationdisplay
i=1L∗
xi(αi),αi∈Rd(11)
whereL∗
xiis the occupation kernel function of the curve xialong the interval [ai,bi], that is
L∗
xi(v) =/bracketleftiggˆbi
aiK(·,xi(t))dt/bracketrightigg
v (12)
and where the vector α= (αT
1,...,αT
n)Tis a solution of
(L∗+λnI)α=x(b)−x(a),
x(a) =/parenleftbig
x1(a1)T,...,xn(an)T/parenrightbigT,
x(b) =/parenleftbig
x1(b1)T,...,xn(bn)T/parenrightbigT(13)
andL∗is the (nd,nd )matrix made of (d,d)blocks, each defined by
L∗
ij=ˆbj
t=ajˆbi
s=aiK(xi(s),xj(t))dsdt (14)
The proof is a straightforward generalization of the representer theorem in RKHSs. It is presented in appendix
F. Figure 1 illustrates the result of the theorem.
3.3 The occupation kernel algorithm from data
Let us now assume that instead of observing ncurves that are solutions of equation 9, we observe m+ 1
snapshots, sometimes noisy, zi=/parenleftig
z(0)
i,...,z(m)
i/parenrightig
at time-points t0,...,tmcoming from a true trajectory xi,
i= 1...p. Firstly, we reshape this data into observations coming from n=mptrajectories, each made of
two samples. This is done by viewing every couple of consecutive observations as two observations associated
with a single trajectory. In other words, we assume that we observe (possibly with noise) the initial and final
points ofntrajectories xi,i= 1...n, that we denote by zi= (z(0)
i,z(1)
i).zican therefore be viewed as a
matrix of dimensions dby2.
Secondly, we replace the integral in equation 12 by an integral quadrature, noted¸
, and the double integral
in equation 14 by a double integral quadrature, noted‚. The observations ziare used to compute the
quadrature involving trajectory xi. In both cases, we use the trapezoidal rule. The resulting algorithms for
learning the vector field and predicting a trajectory given an initial condition are presented in Alg. 1 and Alg
2 respectively. Alg. 1 and Alg 2 are simplified and do not contain the optimizations implemented to reduce
the computational complexity (see section 3.4).
4Under review as submission to TMLR
Figure 1: Illustration of the OCK algorithm with the Gaussian separable kernel. We observe the two endpoints
of the red and blue trajectories x1andx2(running counterclockwise). The horizontal and vertical red vector
fields correspond respectively to the occupation kernel functions Lx1((1,0))andLx1((0,1))and similarly for
the horizontal and vertical blue vector fields. The influence of each trajectory is local. The grey vector field
is the OCK algorithm solution, a linear combination of the red and blue vector fields.
Algorithm 1 OCK learning: Estimate the parameters defining the vector field.
Require: Training data .zi,i= 1...n(nmatrices of dimension (d,2))
1:Computeδ=/parenleftig
z(1)
1−z(0)
1,...,z(1)
n−z(0)
n/parenrightigT
∈Rn×d
2:fori= 1...n,j = 1...ndo
3:L∗
ij←‚
K(xi(s),xj(t))dsdt
4:end for
5:Solve forαthe linear system (L∗+λnI)α=δ
6:Return:α
5Under review as submission to TMLR
Algorithm 2 OCK inference: Generate trajectories given initial conditions.
Require: Training data .zi,i= 1...n. (nmatrices of dimension (d,2))
Require: Output of algorithm 1 :α= (α1,...,αn)T.
Require: Initial conditions .pvectors : (y0
1,...,y0
p)
1:forj= 1...pdo
2:Using a numerical integrator, generate the solution of:
/braceleftigg
˙yj=f∗(yj)
yj(0) =y0
j
3:f∗(yj) =/summationtextn
i=1/bracketleftbig¸
K(yj,xi(t))dt/bracketrightbig
αi
4:end for
3.4 Computational complexity
We specialize to the separable kernels with A=I, see section 2.2. In such a case, the linear system equation 13
becomes equivalent to solving an n×nlinear system for an n×dsolution. Therefore, this requires O/parenleftbig
dn2/parenrightbig
to
construct the kernel matrix, O/parenleftbig
n2/parenrightbig
to estimate the integrals, and O/parenleftbig
dn3/parenrightbig
to solve the linear system. Overall,
the OCK algorithm is thus linear in d. The space complexity is O/parenleftbig
n2/parenrightbig
.
On the other hand, if we let ψ(x)∈Rd×q, thenK(x,y) =ψ(x)Tψ(y)∈Rd×dis an explicit kernel. In such a
case, aq×qlinear system is solved to find f, requiring n,d×qbyq×dmatrix-matrix multiplications to
construct the system. This provides a computational complexity of O(dnq2) +O(q3). Further details are
provided in appendix D.
4 Experiments
(a) FHN oscillator
 (b) Lorenz63
 (c) Lorenz96 128 dimensions
Figure 2: The grey points are the training data. The black curves are the test trajectories. The red curves
are the predictions on the test set. For data in dimensions higher than two, only the first two dimensions are
shown.
4.1 Datasets
We test the methods on a diverse set of synthetic and real-world datasets that reflect the challenges of learning
systems of ODEs. A summary of the datasets is provided in table 2 in appendix B.
Noisy Fitz-Hugh Nagumo (NFHN) The FitzHugh-Nagumo oscillator FitzHugh (1961) is a nonlinear 2D
dynamical system that models the basic behavior of excitable cells, such as neurons and cardiac cells2. We
add considerable noise to this otherwise simple dataset (see figure 7).
Noisy Lorenz63 (Lorenz63) The Lorenz63 system Lorenz (1963) is a 3D system provided as a simplified
model of atmospheric convection.
2Further details of the all datasets are provided in appendix B.
6Under review as submission to TMLR
Lorenz96 The Lorenz96 data arises from Lorenz (1995) in which a system of equations is proposed that may
be chosen to have any dimension greater than 3. (Lorenz96-16) has 16 dimensions, etc.
CMU Walking (CMU) The Carnegie Mellon University (CMU) Walking data is a repository of publicly
available data. It can be found at http://mocap.cs.cmu.edu/.
Plasma This dataset includes imaging and plasma biomarkers from the WRAP study, see Johnson et al.
(2018).
Imaging This dataset includes regional imaging biomarkers from the WRAP study, see Johnson et al. (2018).
4.2 Kernels
We train the OCK models using the scalar-valued Matérn kernels, including the Gaussian, Laplace, and C10
kernels Fuselier et al. (2016). We also train an explicit kernel with 200 Fourier Random features Rahimi &
Recht (2007).
4.3 Comparable methods
We select competitive methods covering various categories.
Sparse Identification of Nonlinear Dynamics (SINDy) SINDy is a well-developed class of data-driven
methods for identifying dynamical systems models from trajectories. Brunton et al. (2016) These methods
rely on sparse regression techniques to isolate the most relevant terms in the governing equations from a set
of candidate functions. SINDy demonstrates robustness for sparse and limited data Kaheman et al. (2020);
Fasel et al. (2022).
Reduced Order Models (ROMs) ROMs are a class of methods for simplifying high-dimensional
dynamical systems by projecting them onto a lower-dimensional space. Dynamic mode decomposition (DMD)
is a data-driven method for extracting spatiotemporal patterns and coherent structures from high-dimensional
data generated by dynamical systems Tu (2013). eDMD extends DMD to handle nonlinear dynamics by
working in a higher-dimensional feature space. Williams et al. (2015). We benchmark with eDMD-RFF Lew
et al. (2023), eDMD-Poly Williams et al. (2015), and eDMD-Deep Yeung et al. (2019)
Deep Learning Methods Deep learning methods can be used in conjunction with ROMs to learn
transformations to lower-dimensional spaces. These methods use deep learning to construct an efficient
representation of the dynamical system and to capture nonlinear dynamics Lusch et al. (2018); Yeung et al.
(2019); Li et al. (2019). Deep learning methods can also be incorporated into the SINDy framework to identify
sparse, interpretable, and predictive models from data Champion et al. (2019); Bakarji et al. (2022). We
benchmark with ResNet He et al. (2016) Lu et al. (2021)
Latent ODEs for Irregularly-Sampled Time Series The Latent ODE method (also a deep learning
method Rubanova et al. (2019)) is an update of the Neural ODEs model introduced in Chen et al. (2018).
The core idea is to represent the hidden dynamics of time series data in a continuous latent space using ODEs.
Given the latent trajectory, the observations of the time series are assumed to be independent. The latent
trajectory dymanics are governed by a neural ODE model. The model uses an encoder-decoder framework
where the encoder maps observed data to a latent initial value via an RNN architecture where the hidden
states are carried through neural ODEs between times of observations, and the decoder generates the latent
trajectory forward in time and predicts future observations.3
The hyperparameters for each method are shown in Table 3 in appendix C. All experiments are run in Google
Colab using the default settings. The GPU was only enabled (A100, High-RAM) for the deep learning
techniques. Lastly, to get an idea of the difficulty of each problem, we compare all methods against the null
model, which predicts no change, that is x(t) =x0.
3Latent ODEs was implemented using the Github repository: https://github.com/YuliaRubanova/latent_ode. The subsam-
pling was disabled for the Plasma and Imaging datasets because the training trajectories were too short in these datasets.
7Under review as submission to TMLR
4.4 Evaluation Method
For each dataset, the trajectories were split as follows: training, validation, and testing. For valida-
tion of the OCK method, we used gaussian process hyper parameter tuning (on a log scale) using
https://skopt.readthedocs.io/en/stable/. At test time, we integrated the predicted trajectories starting
at the given initial conditions and compared to the true trajectories. To measure the performance of each
method, we define two types of errors for each trajectory of the test set. We define:
Err=/radicaltp/radicalvertex/radicalvertex/radicalbtn/summationdisplay
i=2(ti−ti−1)∥yi−ˆyi∥2 (15)
whereyiandˆyiare the observed and predicted trajectory samples at time ti, respectively. Additionally, we
define 1-Err by setting n= 2in equation 15. All the methods were trained, validated, and tested on the
same data. Table 2 provides the average Err and 1-Err errors across the trajectories of the test set. To test
whether the OCK methods are (statistically) significantly better than comparable methods and vice versa,
we first generated a Wilcoxon test p-value for every OCK method (four different kernels) against the other
comparable methods. The pairs of observations used in the test are the errors of the two compared methods
for every trajectory in the test set. Finally, to generate a p-value comparing all OCK methods together
against another comparable method, we used Fisher’s method Mosteller & Fisher (1948) to combine the
four p-values of the OCK methods. We report a ∗in Table 1 if the group of OCK methods is significantly
better (p<0.01) and†if the compared method outperforms OCK with statistical significance ( p<0.01).
Otherwise, no method is significantly better than the other.
L R
L R
00.681.422.7
(a) (0 years) Measurements
L R
L R
00.681.422.7 (b) (0 years) OCK Initial State
L R
L R
00.681.422.7
(c) (+7.7 years) Measurements
L R
L R
00.681.422.7 (d) (+7.7 years) OCK Prediction
Figure 3: OCK Predicted Biomarker Values from the imaging dataset.
4.5 Evaluation Performance
Examples of the output of the OCK algorithm are presented visually in figures 2 and 3. Table 1 summarizes
the prediction errors for each experiment. While no single method outperforms other methods over all
datasets, OCK outperforms in 3 of the 9 datasets (on the task of full trajectory prediction, denoted Err), and
outperforms all other methods on 4 of the 9 datasets on the task of predicting the next sample (denoted
1-Err). It also performs competitively on the remaining datasets on both tasks. Notably, eDMD-Deep yielded
the best results for the CMU experiments. Deep learning models and DMD methods learn the dynamics
in a feature space and therefore have less stringent constraints on the dynamics. We believe this is why
8Under review as submission to TMLR
Table 1: Dynamical System Estimation Results
Lorenz63
Err 1-Err
ResNet 2.07∗.014∗
eDMD-Deep 2.91∗.009∗
eDMD-Poly 2.22∗.012∗
eDMD-RFF 1.93∗.011∗
null 2.56∗.011∗
SINDy-Poly .99∗.002
ockG .66 .002
ockL .65.002
ockM 2.52 .012
ockF 2.52 .012
Lode 1.49∗.092∗NFHN
Err 1-Err
ResNet 5.94∗.064∗
eDMD-Deep 4.54∗.036∗
eDMD-Poly 4.10∗.037∗
eDMD-RFF 4.01∗.037∗
null 9.69∗.043∗
SINDy-Poly 1.68∗.022†
ockG 1.40 .029
ockL 1.11 .030
ockM 1.41 .029
ockF 1.38 .028
Lode .64†.131∗Lorenz96-16
Err 1-Err
ResNet 6.42∗.010∗
eDMD-Deep 5.93∗.040∗
eDMD-Poly 6.91∗.036∗
eDMD-RFF 6.22∗.029∗
null 7.50∗.039∗
SINDy-Poly .40∗.0003∗
ockG .69 .0007
ockL .22 .0001
ockM .22 .0001
ockF .22 .0001
Lode 3.82∗.138∗
Lorenz96-32
Err 1-Err
ResNet 11.56∗.020∗
eDMD-Deep 8.79∗.057∗
eDMD-Poly 9.67∗.051∗
eDMD-RFF 8.10∗.028∗
null 10.49∗.056∗
SINDy-Poly 8.56∗.037∗
ockG .47 .0001
ockL .48 .0001
ockM .49 .0001
ockF .47 .0001
Lode 6.06∗.217∗Lorenz96-128
Err 1-Err
ResNet 24∗2.64∗
eDMD-Deep 17∗.089∗
eDMD-Poly 19∗.109∗
eDMD-RFF 16†.201∗
null 21∗.112∗
SINDy-Poly 19∗.066∗
ockG 17 .064
ockL 16 .035
ockM 16 .040
ockF 16 .036
Lode 15†.342∗Lorenz96 -1024
Err 1-Err
ResNet 66.1∗6.48∗
eDMD-Deep 26.9∗.41∗
eDMD-Poly 61.5∗.42∗
eDMD-RFF 54.2∗.38∗
null 67.0∗.16∗
SINDy-Poly∗∗NA NA
ockG 18.0 .013
ockL 17.8 .014
ockM 17.9 .013
ockF 18.0 .015
Lode 15.8†1.04∗
Plasma
Err 1-Err
ResNet 4.93∗2.89∗
eDMD-Deep 4.09 2.42
eDMD-Poly 4.01 2.39
eDMD-RFF 4.07 2.43
null 4.00∗2.41
SINDy-Poly 3.952.40
ockG 3.96 2.41
ockL 3.96 2.41
ockM 3.96 2.41
ockF 3.96 2.41
Lode 4.28∗2.68∗Imaging
Err 1-Err
ResNet 27.4∗19.4∗
eDMD-Deep 15.6∗12.5
eDMD-Poly 15.7 12.5
eDMD-RFF 27.9∗19.3∗
null 16.2∗12.6∗
SINDy-Poly 96.1∗53.7∗
ockG 15.6 12.3
ockL 15.6 12.3
ockM 15.7 12.3
ockF 15.8 12.4
Lode 14.8†11.7CMU
Err 1-Err
ResNet 22.6∗.86†
eDMD-Deep 15.7†2.03∗
eDMD-Poly 14.9†.78†
eDMD-RFF 14.9†.76†
null 21.6∗1.01∗
SINDy-Poly 33.8∗.91∗
ockG 21 .89
ockL 19.6 .93
ockM 21.5 1.01
ockF 21.6 1.01
Lode 14.9†1.88∗
Description: Minimum values are in bold. A ∗indicates a significant (Fisher’s method) p-value ( <0.01)
in favor of the ock methods in the comparison. A †indicates a significant p-value in favor of the method
compared with all ock methods. There is no statistically significant difference otherwise. Our models are
labelled as ockG - occupation kernel method with Gaussian kernel, ockM - occupation kernel method with
Matern kernel, ockF - occupation kernel method with Random Fourier Features (RFF), and ockL - occupation
kernel with Laplace kernel (See section 4.2). We compare against SINDy-Poly, eDMD-Deep, eDMD-Poly,
eDMD-RFF, ResNet and Latent Ode (Lode) (See section 4.3).∗∗No result could be obtained for SINDy-Poly
for this dataset.
9Under review as submission to TMLR
they outperform with datasets such as the CMU dataset as our model assumes the dynamics arises from a
homogeneous system which is just not the case.
The differences observed in table 1 are not always significant. Recall that we use the sign †when a method is
better than all the OCK methods. We count 5 models out of 9 with at least one †for Err and only 2 out of 9
for the 1-Err. Note that the OCK algorithm is optimized for the 1-Err since the trajectories of the training
set are systematically reshaped in trajectories of two observations, thus it is not surprising that it performs
better for this metric.
When comparing the kernels used for the OCK method, the Laplace kernel performs the best overall, consistent
with other analyses of this kernel (Geifman et al. (2020)). Note also that the random Fourier features kernel,
which approximates the Gaussian kernel, performs well, allowing for linear implementations in n, which is
the number of examples in the training set.
The trajectories predicted by ockL for NFHN (in blue in figure 6) are visually close to the ground truth
(in black) in all but the last case. SINDy-poly (in yellow) is competitive. ResNet provides very irregular
trajectories. Latent ODE provides good long-term predictions. However, the initial points do not coincide
with the initial points of the ground truth provided for all the algorithms. In other words, while Latent-ODE
performs very well on the task of predicting full trajectories, it performs very poorly on the next sample
prediction task which is an inherent limitation of some deep learning models like Latent-ODE and Resnet.
We tested OCK with a dataset of 1024 dimensions to show how well it scales. The implementation of
SINDy-Poly that we used did not provide a result for this data; see NA in the table 1.
(a) Scalar Matérn kernel
 (b) Divergence-free kernel
Figure 4: The magnitude of the error in vector field approximation of the pendulum problem obtained using
different choices of kernel along with the training set (gray).
5 Learning vector fields with constraints
Divergence-free (∇·v= 0)and curl-free (∇×v= 0)vector fields appear in a diverse set of applications
including fluid dynamics Wendland (2009); Fuselier et al. (2016), magnetohydrodynamics McNally (2011),
image processing Polthier & Preuß (2003), and surface reconstruction Drake et al. (2022). Accordingly, a
significant amount of research has been devoted to the development of curl-free and divergence-free kernel
methods in recent years Narcowich & Ward (1994); Lowitzsch (2005); Narcowich et al. (2007); Fuselier
(2008); Fuselier & Wright (2009); Gao et al. (2022). We will demonstrate how the occupation kernel method
can be adapted to ensure that the recovered vector field analytically satisfies the divergence-free/curl-free
constraint for an appropriate choice of matrix kernel K. Then, for ease of presentation, we will apply just the
divergence-free kernels to both real and synthetic datasets.
10Under review as submission to TMLR
5.1 Divergence-free kernels
Letx∈Rdandd= 2,3, and letϕ(∥x∥)be a radial basis function. Then the standard construction Fuselier
(2008) for divergence-free and curl-free matrix-valued radial basis functions are
{−∆I+∇∇T}ϕ(∥x∥)and−∇∇Tϕ(∥x∥), (16)
respectively, where ∆ =/summationtextd
i=1∂2/∂x2
iand[∇∇T]ij=∂2/∂xi∂xj,1≤i,j≤d. For example, if d= 2, then
{−∆I+∇∇T}ϕ(∥x∥) =/bracketleftigg
−∂2ϕ
∂x2
2∂2ϕ
∂x1∂x2
∂2ϕ
∂x2∂x1−∂2ϕ
∂x2
1/bracketrightigg
(∥x∥). (17)
Thus, the columns of the matrix-valued radial basis function {−∆I+∇∇T}ϕ(∥x∥)are divergence-free.
Similarly, we have that the columns of −∇∇Tϕ(∥x∥)are curl-free.
We select the divergence-free kernel K(x,y) ={−∆I+∇∇T}ϕ(∥x−y∥)where our choice of scalar radial
basis function is the C10Matérn kernel Fuselier et al. (2016).
5.2 Hamiltonian systems
A system
x′
1=f(x1,x2), x′
2=g(x1,x2) (18)
is called a Hamiltonian system if there exists a function H(x1,x2)(called the Hamiltonian ) for which f=Hx2
andg=−Hx1. This implies that the vector field (f,g)is divergence-free. Furthermore, along every orbit
we haveH(x1,x2) =constant and any conservative dynamical equation u′′=f(u)leads to a Hamiltonian
system where the Hamiltonian coincides with the total energy. For example, every orbit of the conservative
system corresponding to the equation describing the motion of a pendulum
x′
1=x2, x′
2=−g
ℓsin(x1) (19)
satisfies the conservation law
H(x1,x2) =1
2x2
2−g
ℓcos(x1) =E (20)
for some constant E.
5.3 Experiments
We test the divergence-free OCK method on the following real and synthetic datasets.
Pendulum Problem We generate data using (19) with ℓ= 1andg= 9.8. This 2-dimensional synthetic
dataset consists of 100 trajectories of 50 samples each. This dataset allows us to test whether the OCK
method can, simultaneously, learn a known Hamiltonian system while analytically enforcing the divergence
free constraint.
Buoy Data We obtained a buoy dataset from https://oceanofthings.darpa.mil/data#tab-all, which contains
two-dimensional trajectories of buoys submerged in the ocean. We sampled every tenth observation of the
raw data and converted the time measurements into years. Trajectories with only one sample were dropped.
5.4 Evaluation
The loss was computed using (15) for both datasets. For the pendulum problem, the scalar Matérn kernel
provided a loss of 1.5×10−1while the divergence-free kernel provided a loss of 1.9×10−2, an improvement
by an order of magnitude. Not only does the divergence-free kernel allow for a physically constrained solution,
but it is also more accurate. For the buoy data, the scalar Matérn kernel provided a loss of 3.5×101while
the divergence-free kernel provided a loss of 3.1×101. Thus, despite the presence of noise (which is not
necessarily divergence-free), we still achieve a reduction in the loss of about 10%.
Figure 4 shows the error in the computed vector fields of the pendulum problem. A plot of the results for the
buoy data problem using the divergence-free kernel, along with the training data, is shown in figure 5.
11Under review as submission to TMLR
6 Summary and discussion
The implicit matrix-valued kernel formulation has provided us the means to demonstrate that the OCK
algorithm, originally proposed by J. Rosenfeld and co-authors, allows for learning the vector field of an
ODE, and scales linearly with the dimension of the state space. We have compared the algorithm to a
representative selection of competitive algorithms. We have provided convincing experimental evidence
of superior performance of the method on many of the datasets we tested, and consistently competitive
performance on all datasets. The surprising simplicity of the technique lends to many simple and exciting
follow-up ideas for optimizations, generalizations, and explorations. Among other topics, we expect follow-up
work to push the state-of-the-art further into the regime of high-dimensional dynamics learning, applications
to PDEs, and a thorough analysis of the theoretical convergence of the method.
References
Joseph Bakarji, Kathleen Champion, J Nathan Kutz, and Steven L Brunton. Discovering governing equations
from partial measurements with deep delay autoencoders. arXiv preprint arXiv:2201.05136 , 2022.
Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations from data by
sparse identification of nonlinear dynamical systems. Proceedings of the national academy of sciences , 113
(15):3932–3937, 2016.
Kathleen Champion, Bethany Lusch, J Nathan Kutz, and Steven L Brunton. Data-driven discovery of
coordinates and governing equations. Proceedings of the National Academy of Sciences , 116(45):22445–22451,
2019.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential
equations. Advances in neural information processing systems , 31, 2018.
Anthony M DeGennaro and Nathan M Urban. Scalable extended dynamic mode decomposition using random
kernel approximation. SIAM Journal on Scientific Computing , 41(3):A1482–A1499, 2019.
Kathryn P. Drake, Edward J. Fuselier, and Grady B. Wright. Implicit surface reconstruction with a curl-free
radial basis function partition of unity method. SIAM Journal on Scientific Computing , 44(5):A3018–A3040,
2022. doi: 10.1137/22M1474485.
Urban Fasel, J Nathan Kutz, Bingni W Brunton, and Steven L Brunton. Ensemble-sindy: Robust sparse
model discovery in the low-data, high-noise limit, with active learning and control. Proceedings of the Royal
Society A , 478(2260):20210904, 2022.
Richard FitzHugh. Impulses and physiological states in theoretical models of nerve membrane. Biophysical
journal, 1(6):445–466, 1961.
E. Fuselier, V. Shankar, and G. Wright. A high-order radial basis function (rbf) leray projection method for
the solution of the incompressible unsteady stokes equations. Computers and Fluids , 128:41–52, 2016.
E. J. Fuselier. Sobolev-type approximation rates for divergence-free and curl-free rbf interpolants. Math.
Comput., 77:1407–1423, 2008.
E. J. Fuselier and G. B. Wright. Stability and error estimates for vector field interpolation and decomposition
on the sphere with rbfs. SIAM J. Num. Anal. , 47:3213–3239, 2009.
W. W. Gao, G. E. Fasshauer, and N. Fisher. Divergence-free quasi-interpolation. Appl. Comp. Harmon.
Anal., 60:471–488, 2022.
Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Basri Ronen. On the
similarity between the laplace and neural tangent kernels. Advances in Neural Information Processing
Systems, 33:1451–1461, 2020.
12Under review as submission to TMLR
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016.
Sterling C Johnson, Rebecca L Koscik, Erin M Jonaitis, Lindsay R Clark, Kimberly D Mueller, Sara E
Berman, Barbara B Bendlin, Corinne D Engelman, Ozioma C Okonkwo, Kirk J Hogan, et al. The wisconsin
registry for alzheimer’s prevention: a review of findings and current directions. Alzheimer’s & Dementia:
Diagnosis, Assessment & Disease Monitoring , 10:130–142, 2018.
Kadierdan Kaheman, J Nathan Kutz, and Steven L Brunton. Sindy-pi: a robust algorithm for parallel implicit
sparse identification of nonlinear dynamics. Proceedings of the Royal Society A , 476(2242):20200279, 2020.
Ethan Lew, Abdelrahman Hekal, Kostiantyn Potomkin, Niklas Kochdumper, Brandon Hencey, Stanley Bak,
and Sergiy Bogomolov. Autokoopman: A toolbox for automated system identification via koopman operator
linearization. In International Symposium on Automated Technology for Verification and Analysis (ATVA) ,
2023. Under review.
Yunzhu Li, Hao He, Jiajun Wu, Dina Katabi, and Antonio Torralba. Learning compositional koopman
operators for model-based control. arXiv preprint arXiv:1910.08264 , 2019.
Edward N Lorenz. Deterministic nonperiodic flow. Journal of atmospheric sciences , 20(2):130–141, 1963.
E.N. Lorenz. Predictability: a problem partly solved. In Seminar on Predictability , volume 1, pp. 1–18,
Shinfield Park, Reading, 1995. ECMWF.
S. Lowitzsch. Error estimates for matrix-valued radial basis function interpolation. Journal of Approximation
Theory, 137:238–249, 2005.
Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. Deepxde: A deep learning library for solving
differential equations. SIAM review , 63(1):208–228, 2021.
Bethany Lusch, J Nathan Kutz, and Steven L Brunton. Deep learning for universal linear embeddings of
nonlinear dynamics. Nature communications , 9(1):4950, 2018.
Colin P McNally. Divergence-free interpolation of vector fields from point values—exact ∇·B = 0 in numerical
simulations. Monthly Notices of the Royal Astronomical Society: Letters , 413(1):L76–L80, 2011.
Peter Moczo, Jozef Kristek, and Ladislav Halada. The finite-difference method for seismologists. An
Introduction , 161, 2004.
Frederick Mosteller and R. A. Fisher. Questions and answers. The American Statistician , 2(5):30–31, 1948.
ISSN 00031305. URL http://www.jstor.org/stable/2681650 .
F. Narcowich and J. Ward. Generalized hermite interpolation via matrix-valued conditionally positive definite
functions. Mathematics of Computation , 63:661–688, 1994.
F. Narcowich, J. Ward, and G. B. Wright. Divergence-free rbfs on surfaces. J. Fourier Anal. Appl. , 13:
643–663, 2007.
Konrad Polthier and Eike Preuß. Identifying vector field singularities using a discrete hodge decomposition.
In Hans-Christian Hege and Konrad Polthier (eds.), Visualization and Mathematics III , pp. 113–134, Berlin,
Heidelberg, 2003. Springer Berlin Heidelberg.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Neural Information
Processing Systems , 2007. URL https://api.semanticscholar.org/CorpusID:877929 .
Joel A Rosenfeld, Rushikesh Kamalapurkar, Benjamin Russo, and Taylor T Johnson. Occupation kernels and
densely defined liouville operators for system identification. In 2019 IEEE 58th Conference on Decision
and Control (CDC) , pp. 6455–6460. IEEE, 2019a.
Joel A Rosenfeld, Benjamin Russo, Rushikesh Kamalapurkar, and Taylor T Johnson. The occupation kernel
method for nonlinear system identification. arXiv preprint arXiv:1909.11792 , 2019b.
13Under review as submission to TMLR
Yulia Rubanova, Ricky TQ Chen, and David K Duvenaud. Latent ordinary differential equations for
irregularly-sampled time series. Advances in neural information processing systems , 32, 2019.
Benjamin P Russo, Rushikesh Kamalapurkar, Dongsik Chang, and Joel A Rosenfeld. Motion tomography via
occupation kernels. arXiv preprint arXiv:2101.02677 , 2021.
Bernhard Schölkopf and Alexander J. Smola. Learning with kernels : support vector machines, regularization,
optimization, and beyond . Adaptive computation and machine learning. MIT Press, 2002. URL http:
//www.worldcat.org/oclc/48970254 .
Jonathan H Tu. Dynamic mode decomposition: Theory and applications . PhD thesis, Princeton University,
2013.
H. Wendland. Divergence-free kernel methods for approximating the stokes problem. SIAM J. Numer. Anal. ,
47:3158–3179, 2009.
Matthew O Williams, Ioannis G Kevrekidis, and Clarence W Rowley. A data–driven approximation of the
koopman operator: Extending dynamic mode decomposition. Journal of Nonlinear Science , 25:1307–1346,
2015.
Enoch Yeung, Soumya Kundu, and Nathan Hodas. Learning deep neural network representations for koopman
operators of nonlinear dynamical systems. In 2019 American Control Conference (ACC) , pp. 4832–4839.
IEEE, 2019.
A Experiment Code
We created a code capsule on Code Ocean that hosts all the methods used in our experiments for system
identification of ordinary differential equations. The code repository allows for the repeatability of our results
for a single experiment—Lorenz63—and facilitates the evaluation of the performance of different models on
synthetic and real-world datasets. The real world plasma and imaging experiment are restricted data and
will not be available for the evaluation.
We provide an anonymized zipfile (6.6 MB) export of the code capsule for review at https://drive.google.
com/file/d/1p36HH00dHGLuBeHEfhJyMlfxjv1At5La/view?usp=sharing . The code can be run locally in a
Docker container with instructions in the README.md and REPRODUCING.md. After review, we will
publish a de-anonymized capsule available on the Code Ocean platform, as well as a public GitHub repository
of the code with a detailed README.md covering the Papers with Code release checklist4.
To ensure the reproducibility of the experiment, we have tuned the hyperparameters for each method,
except for ResNet, which is known to require substantial tuning time. The code repository provides detailed
instructions for running each method experiment, including the necessary input data and the corresponding
hyperparameter settings. Please note that due to limitations on Code Ocean, we have set the TensorFlow
library to use the CPU rather than the GPU, as using the GPU resulted in numerous errors. This may
increase the runtime of the experiments. For the paper, we ran some experiments in google colab on A100
GPUs where the training time was much lower.
Upon running the code repository, the experiments will take approximately 1.5 hours to complete. The results
of each experiment are provided in the form of trajectory CSV files, capturing the predicted trajectories of
the identified systems. These trajectory files can be further analyzed or visualized as desired. Additionally, a
summary of the performance of each method is available in the code repository.
B Datasets
In table 2, d refers to the number of dimensions of the system, n the total number of trajectories, and m the
average number of samples per trajectory.
4https://github.com/paperswithcode/releasing-research-code
14Under review as submission to TMLR
Table 2: Datasets
Name Type d n m
NFHN Synthetic 2 150 201
Lorenz63 Synthetic 3 150 201
Lorenz96-16 Synthetic 16 100 243
Lorenz96-32 Synthetic 32 100 243
Lorenz96-128 Synthetic 128 100 243
Lorenz96-1024 Synthetic 1024 100 100
CMU Real 50 75 106.7
Plasma Real 6 425 2.95
Imaging Real 117 231 2.26
B.1 Synthetic data
Noisy Fitz-Hugh Nagumo The FitzHugh-Nagumo oscillator FitzHugh (1961) is a nonlinear 2D dynamical
system that models the basic behavior of excitable cells, such as neurons and cardiac cells (See figure 7). The
system is popular in the analysis of dynamical systems for its rich behavior, including relaxation oscillations
and bifurcations. The FHN oscillator consists of two coupled ODEs that describe the membrane potential v
and recovery variable w,
˙v=v−v3
3−w+RI
τ˙w=v+a−bw(21)
Here,Iis the external current input, aandbare positive constants that affect the shape and duration of the
action potential, and τis the time constant that determines the speed of the recovery variable’s response.
We added random Gaussian noise of standard deviation 0.12 to this dataset.
Noisy Lorenz63 The 3D Lorenz 63 system Lorenz (1963) is a simplified model of atmospheric convection,
but has since become a canonical example of chaos in dynamical systems. The Lorenz 63 system exhibits
sensitive dependence on initial conditions and parameters, which gives rise to its characteristic butterfly-shaped
chaotic attractor. The equations are
dx
dt=σ(y−x)
dy
dt=x(ρ−z)−y
dz
dt=xy−βz. (22)
Here,x,y, andzare state variables representing the fluid’s convective intensity, and σ,ρ, andβare positive
parameters representing the Prandtl number, the Rayleigh number, and a geometric factor, respectively. We
added random Gaussian noise with standard deviation 0.5to this dataset.
Lorenz96 The Lorenz96 data arises from Lorenz (1995) in which a system of equations is proposed that
may be chosen to have any dimension greater than 3. The chaotic system is defined by:
dxk
dt=−xk−1xk−2+xk+1xk−1−xk+F (23)
where we take F= 8and consider 16, 32, 128 and 1024 dimensional systems. Indices are assumed to wrap so
that for an ndimensional system x−2=xn−2.
B.2 Real Data
CMU Walking The Carnegie Mellon University (CMU) Walking data is a repository of publicly available
data. It can be found at http://mocap.cs.cmu.edu/. It was generated by placing sensors on a number of
15Under review as submission to TMLR
subjects and recording the position of the sensors as the subjects walked forward by a fixed amount and then
walked back. There were 50 spatial dimensions of data as recorded by the sensors, which were recorded at
regular times. Since the observation times were not provided, we separated each observation in time by 0.1
units and began each trajectory at time zero. The CMU Walking data which we used for our experiments
consists of 75 trajectories with an average of 106.7 observations per trajectory. We could not use the full
amount of data provided because some subjects did not walk in the same manner as other subjects.
It should be noted that it may not be appropriate to consider the CMU walking data as being generated by a
single dynamical system. Since there were multiple subjects in the dataset, it is reasonable to conclude that
there are multiple dynamical systems responsible for the motion of these different subjects. However, we fit
our model to this dataset assuming that it was generated by a single dynamical system.
Plasma This dataset includes imaging and plasma biomarkers from the WRAP study, see Johnson et al.
(2018). The imaging biomarker is a distribution volume ratio obtained from Pittsburgh Compound-B
positron emission tomography. The plasma biomarkers include Aβ40/Aβ 42that reflects specific amyloid
beta (Aβ) proteoforms; ptau217, which has a high correlation with amyloid PET positivity, GFAP, which
measures the levels of the astrocytic intermediate filament glial fibrillary acidic protein, and NFL, a recognized
biomarker of subcortical large-caliber axonal degeneration, for a total of 6 biomarkers. There are a total
n= 425trajectories, one per subject, and, on average, 2.95 time points per trajectory. The data was split
(70%,10%,20%)corresponding resp. to training, validation, and testing.
Imaging This dataset includes regional imaging biomarkers from the WRAP study, see Johnson et al. (2018).
The imaging biomarker is an atlas-based distribution volume ratio obtained from Pittsburgh Compound-B
positron emission tomography. There are a total of 117 biomarkers. There are a total n= 231trajectories,
one per subject, and, on average, 2.26 time points per trajectory. The data was split (70%,10%,20%)
corresponding resp. to training, validation, and testing.
C Comparators:
The methods we benchmark against are presented in the table below with references included. The Null
Table 3: Methods Considered
Method Hyperparameters
Null N/A
Sparse Identification of Nonlinear Dynamics
SINDy Brunton et al. (2016) polynomial degree, sparsity threshold
Reduced Order Models
eDMD-RFF DeGennaro & Urban (2019) number of features, lengthscale
eDMD-Poly Williams et al. (2015) polynomial degree
Deep Learning
ResNet He et al. (2016) Lu et al. (2021) network depth/breadth
eDMD-Deep Yeung et al. (2019) latent space dimension, autoencoder depth/breadth
Latent Ode Rubanova et al. (2019) latent space dimension, autoencoder and decoder depth/breadth
model is used as a baseline to determine that something was learned of the dataset. Our null model has no
parameters and is the trivial dynamical system for which the slope-field is zero everywhere. A model that
fails to outperform the null model likely did not learn any useful information about the dynamical system.
D Computational Complexity
We detail below an analysis of the complexity in terms of d, the dimension of the state space, and n, the
total number of observations.
16Under review as submission to TMLR
D.1 Implicit kernel
In our experiments, an implicit Gaussian kernel was used. The Gram matrix for the kernel is n×mwheren
is the total number of samples in the dataset Xandmis the total number of samples in the dataset Y. If
X∈Rd×n, andY∈Rd×m, we may compute any radial basis function kernel defined by:
Gi,j=f(∥xi−yj∥2) (24)
by observing:
D=outer (sum(X⊛X),1m)−2XTY+outer (1n,sum(Y⊛Y)) (25)
WhereDi,j=∥Xi−Yj∥2is the matrix of square distances, ⊛is the Hadamard product, sumis column
sum,outeris an outer product, and 1nis the ones vector in ndimensions. G∈Rn×nis then calculated by
applyingfcomponent by component to Dfor the training set XwithX. The computational bottleneck of
this Gram matrix computation is the n×dbyd×mmatrix matrix multiplication, with a worst case run
time that is O(dnm) =O(dn2)whenY=X(with a naive implementation of matrix matrix multiplication).
We integrate over intervals of a single pair of samples, and these integrals are estimated using the trapezoid
rule quadrature. Therefore, for instance, if a single trajectory is given, we would get our estimate of all our
integrals by simply taking the kernel
k=h2
4/bracketleftbigg
1 1
1 1/bracketrightbigg
(26)
and convolving it with the Gram matrix G. If multiple trajectories are given, we apply the same convolution
toGand ignore terms involving sums that mix samples from different trajectories. In Numpy indexing
notation, we may apply the above convolution with the expression:
M=h2
4(G[1 :,1 :] +G[:−1,1 :] +G[1 :,:−1] +G[:−1,:−1]) (27)
This adds an additional O(n2)computational complexity (for fixed G∈Rn×n).
Finally, the linear system we solve is:
(M+λI)A=Y (28)
WhereM∈Rn×nandA,Y∈Rn×d, which adds the dominating computational complexity term of O(dn3).
D.2 Explicit Kernel
Assuming the vector valued RKHS has a matrix valued kernel of the form
K(x,y) =ψ(x)ψT(y) (29)
where
ψ:Rd→Rd×q(30)
we may recall:
f(x) =/summationdisplay
iˆbi
aiψ(x)ψ(xi(τ))Tdταi=ψ(x)/summationdisplay
iˆbi
aiψ(xi(τ))Tdταi=ψ(x)β (31)
whereβ∈Rqis defined by β=/summationtext
i´bi
aiψ(xi(τ))Tdταi. Letting
Ψ∈Rnd×q(32)
be defined by:
Ψi∈Rd×q=ˆbi
aiψ(xi(τ))dτ (33)
17Under review as submission to TMLR
we may observe that
L∗= ΨΨT(34)
and
αTL∗α=βTβ (35)
Thus, we reduce the minimization problem 59 to
min
βJ(β) =1
nn/summationdisplay
i=1∥[Ψβ]i−xi(bi) +xi(ai)∥2
Rd+λβTβ (36)
This simplifies to/parenleftbig
ΨTΨ +λI/parenrightbig
β= ΨTy (37)
wherey∈Rndwithyi∈Rdandyi=xi(bi)−xi(ai). Thus, in the explicit kernel case, we require evaluating
Ψwhich isO(ndq), aq×ndbynd×qmatrix matrix multiplication O(q2(nd))and aq×qlinear system
solve which is O(q3). Notice, in the explicit kernel case, no assumptions on the overall structure of the kernel
matrixK(x,y)are needed. For instance, the matrices no longer need to be diagonal or proportional to the
identity matrix. However, in practice, for best results, qwill typically be dependent on the dimension dand
the sample size.
E Vector Valued Reproducing Kernel Hilbert Spaces
Similarly to the scalar case, there are three ways to characterize an RKHS.
Firstly, we can construct an RKHS with linear functions of the kernel:
Definition 3.Let
H0={f;f(x) =n/summationdisplay
i=1K(x,xi)wi,xi∈X,wi∈Rd,i= 1 :n} (38)
then, consider the encoding in H0of the functions fandg,
f←→/braceleftbiggx1,...,xn
w1,...,wnandg←→/braceleftbiggy1,...,ym
v1,...,vm(39)
Define the inner product
⟨f,g⟩=n/summationdisplay
i=1m/summationdisplay
j=1wT
iK(xi,yj)vj (40)
then the closure of H0for⟨.,.⟩is the RKHS of K.
The second construction starts from a Hilbert space and uses the Riesz representation theorem.
Definition 4.LetHbe a Hilbert space of functions Rd→Rdsuch that for any x∈Rd, the evaluation
functionalf∝⇕⊣√∫⊔≀→f(x)is continuous: there is a constant Mx∈R, such that
||f(x)||Rd≤Mx||f||H (41)
then, using the Riesz representation, for any v∈Rdthere exists a unique Kx,v∈Hsuch that
vTf(x) =⟨f,Kx,v⟩H(42)
This equation is the reproducing property .
Define the matrix-valued kernel
Kij(x,x′) =/angbracketleftbig
Kx,ei,Kx′,ej/angbracketrightbig
,i,j= 1...d (43)
wheree1,...,edis the natural basis of Rd. ThenKis a kernel and His the RKHS of K.
18Under review as submission to TMLR
The third formulation allows for checking that a Hilbert space is an RKHS.
Definition 5.LetHbe a Hilbert space of functions Rd→Rd. LetK(d,d) be a kernel. H is the RKHS of K
when
1. For any x′∈X,v∈Rd,x∝⇕⊣√∫⊔≀→K(x,x′)v∈H
2. The reproducing property holds: for any f∈H,x∈X,v∈Rd, equation 42
E.1 Verifying the continuity for the occupation kernel in multiple dimensions
(e1,...,ed)Tis the standard basis of Rd.
|eT
jLx(f)|=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleˆT
0eT
jf(x(t))dt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(44)
≤ˆT
0/vextendsingle/vextendsingleeT
jf(x(t))/vextendsingle/vextendsingledt (45)
=ˆT
0|⟨f,K(.,x(t))ej⟩|dt (46)
≤ ||f||ˆT
0/radicalig
Kjj(x(t),x(t))dt (47)
where we have used the Cauchy-Schwartz inequality. Since t∝⇕⊣√∫⊔≀→x(t)is continuous, if moreover x∝⇕⊣√∫⊔≀→Kjj(x,x)
is continuous for each j= 1...d, then for each xand each 1≤j≤d,eT
jLxis bounded which implies that
Lxis bounded and thus continuous.
F Proof of Theorem 1
Consider the linear span of the occupation kernel functions L∗
xi,i= 1...n
F={f∈H,f=n/summationdisplay
i=1L∗
xi(αi),α= (α1,...,αn)∈Rdn} (48)
Note thatFis linear and finite-dimensional; thus, it is a closed linear subspace of H. We can then project
any function in Horthogonally onto it
f=fF+fF⊥ (49)
Now, for each i= 1...n, andv∈Rd,
vTˆbi
aif(xi(t))dt=vTLxi(f) =/angbracketleftbig
L∗
xi(v),f/angbracketrightbig
=/angbracketleftbig
L∗
xi(v),fF+fF⊥/angbracketrightbig
=/angbracketleftbig
L∗
xi(v),fF/angbracketrightbig
(50)
where the previous to last equality comes from the fact that fF⊥is perpendicular to L∗
xi(v). Thus,
ˆbi
aif(xi(t))dt=Lxi(fF) (51)
Next, using the Pythagorean equality
||f||2=||fF||2+||fF⊥||2≥||fF||2(52)
19Under review as submission to TMLR
Thus, for any f∈H,J(fF)≤J(f)which proves that the minimum of Jactually belongs to F. Moreover,
note that
||fF||2=/angbracketleftiggn/summationdisplay
i=1L∗
xi(αi),n/summationdisplay
i=1L∗
xi(αi)/angbracketrightigg
=n/summationdisplay
i,j=1/angbracketleftig
L∗
xi(αi),L∗
xj(αj)/angbracketrightig
=n/summationdisplay
i,j=1αT
i/bracketleftiggˆbi
aiˆbj
ajK(xi(s),xj(t))dsdt/bracketrightigg
αj=n/summationdisplay
i,j=1αT
iL∗
ijαj=αTL∗α(53)
Also,
Lxi(fF) =Lxi
n/summationdisplay
j=1L∗
xj(αj)
 (54)
=ˆbi
ain/summationdisplay
j=1L∗
xj(αj)(xi(t))dt (55)
=n/summationdisplay
j=1/bracketleftiggˆbi
aiˆbj
ajK(xi(t),xj(s))dsdt/bracketrightigg
αj (56)
=n/summationdisplay
j=1L∗
ijαj (57)
= [L∗α]i(58)
The minimization problem in fis thus equivalent to minimizing in α
J(α) =1
nn/summationdisplay
i=1∥[L∗α]i−xi(bi) +xi(ai)∥2
Rd+λαTL∗α (59)
or equivalently,
J(α) =1
n∥L∗α−x(b) +x(a)∥2
Rnd+λαTL∗α (60)
sinceλ>0andL∗is psd, this is solved by
(L∗+λnI)α=x(b)−x(a) (61)
G Vector field learned from the buoy data
20Under review as submission to TMLR
Figure 5: A detail of the vector field learned using the divergence-free kernel from the buoy data together
with the training set (red).
21Under review as submission to TMLR
True
eDMD-deep
eDMD-poly
eDMD-rff
SINDy-poly
ResNet
latent-ODE
ockL
Figure 6: These are 25 randomly sampled trajectories in the test set for FHN. Black is the true trajectory,
red is eDMD-deep, green is eDMD-poly, purple is eDMD-rff, yellow is SINDy-poly, orange is ResNet, brown
is latent-ODE, and blue is ockL.
22Under review as submission to TMLR
Figure 7: 25 randomly sampled trajectories in the NFHN (Noisy FHN) training set.
23