Under review as submission to TMLR
Improved rate for Locally Differentially Private Linear Bandits
Anonymous authors
Paper under double-blind review
Abstract
In this paper, we propose a stochastic linear contextual bandit algorithm that ensures
local differential privacy (LDP). Our algorithm is (ϵ,δ)−Locally Differentially Private and
guarantees ˜O/parenleftig√
dT3/4/parenrightig
regret with high probability . This is a factor of d1/4improvement
over the previous state-of-the-art (Zheng et al., 2020). Furthermore, our regret guarantee
improves to ˜O/parenleftig√
dT/parenrightig
when the action space is well-conditioned. This regret bound matches
the optimal non-private asymptotic bound, thus demonstrating that we can achieve privacy
for free even in the stringent LDP model. Our algorithm is the first algorithm that achieves
˜O(√
T)regret in a privacy setting that is stronger than the central settings.
1 Introduction
The stochastic linear contextual bandit problem consists of a sequence of T“rounds” of interaction between
a learner and an environment. In the tth round, a learner receives context ct, which determines a decision
setDt:={ϕ(ct,a)|a∈A}⊂ RdwhereAis a set of possible actions and ϕ(·,·)is a function that maps
context-action pairs to Rd. Then the learner chooses an “action” at∈Awhich corresponds to a “decision”
xt:=ϕ(ct,at)∈Dt, and receives a reward ytsuch that E[yt|ct,at] =⟨θ⋆,xt⟩for some unknown θ⋆∈Rd.
Similar to other bandit settings, we measure the performance of our algorithm by evaluating the regret,
defined as the gap between the cumulative rewards of our algorithm and the best possible cumulative reward:
RegretT=T/summationdisplay
t=1/bracketleftbigg
max
a∈A⟨θ⋆,ϕ(ct,a)⟩−⟨θ⋆,ϕ(ct,at)⟩/bracketrightbigg
=T/summationdisplay
t=1/bracketleftbigg
max
x∈Dt⟨θ⋆,x⟩−⟨θ⋆,xt⟩/bracketrightbigg
Our goal is to come up with an algorithm that achieves sublinear regret ( RT≤o(T)), which means that on
average we are doing as well as the best possible actions in hindsight. This problem has been well-studied in
the literature, and the asymptotically optimal regret is O(d√
T)(Lattimore & Szepesvári, 2020; Li et al.,
2019). Furthermore, we want to ensure that our algorithm enforces a privacyguarantee - which we will
quantify via the framework of local differential privacy (LDP).
To motivate this contextual bandit setting and the need to ensure privacy, consider a personalized medical
app where each user has their own treatment plan based on their medical history and the weekly data
they provide to the central server (app provider). This application can be modeled as a contextual bandit
problem by letting the medical history/weekly data be the context, the treatment plan be the action, and
the user’s health outcome be the reward. Another example usage of the linear contextual bandit setting
is the personalized recommendation system in streaming services. In this example, the context could be a
feature vector that contains the user’s personal information (age, demography, past activities), the action is
the movie the system recommends, and the reward could be the rating the user gives. It is clear in these
scenarios that the context/action and the reward are sensitive information that the user wishes to protect.
Thus, in order to maximize outcomes and user’s experience, we need a contextual bandit algorithm, while in
order to protect sensitive information we need a differential private algorithm to ensure privacy.
1Under review as submission to TMLR
Our work will ensure a “local” model of privacy, in contrast to a weaker “central” model. In a central privacy
model, the users send their raw data to the central server that will be responsible for making decisions. Then
the server would inject sufficient noise into its decisions so that potential attackers cannot tell if a particular
user’s data is used by the server or not. Even though this central model provides protection from outside
attackers, it requires the user to have complete trust in the server. Another way to protect users’ privacy is
the local model, which is the model that we consider in this paper. In this model, before sending their data
to the server, each user would inject noise into their own data. Thus, the local model ensures that every
user’s data is safe without relying on any external sources. Consider a framework with Tlocal users, each
holding their private data, and a central server that collects information from these users. Each user applies
a randomized mechanism to their data before sending it to the server. The system satisfies (ϵ,δ)−Locally
Differential Privacy (LDP) if the following condition holds:
Definition 1.1. (Local Differential Privacy (Dwork & Roth, 2014)) A randomized algorithm M:X∝⇕⊣√∫⊔≀→S
satisfies (ϵ,δ)−local differential privacy (( ϵ,δ)-LDP) if for any pair of users x,x′∈Xand any event E⊆S,
it holds that:
P[M(x)∈E]≤exp(ϵ)P[M(x′)∈E] +δ
Roughly speaking, LDP ensures that the output of a randomized algorithm on any pair of users would
bealmostindistinguishable with high probability. This local model is a stronger notion of privacy than
regular DP in the sense that any algorithm that satisfies LDP also satisfies regular DP (Dwork et al., 2010).
Furthermore, LDP is also a more user-friendly notion of privacy than regular DP since it allows the user
to protect their own data without relying on a trusted server, making it appealing for real-life application
(Cormode et al., 2018). However, it is also a lot more difficult to recover the asymptotically optimal ˜O(√
T)
regret guarantee since the private mechanism in the central model (Shariff & Sheffet, 2018) fails in the local
model. Indeed, the current best regret guarantee for LDP is only ˜O/parenleftig
(dT)3/4/√ϵ+d√
T/parenrightig
(Zheng et al.,
2020). Another line of work on private stochastic contextual linear bandit is the shuffle model (Erlingsson
et al., 2019; Cheu et al., 2019). In this model, there exists a trusted shuffler between the users and the server.
The shuffler receives noisy data from the users, and permutes them before sending the data to the server.
This shuffling step adds another layer of protection which allows for finer privacy-utility trade-offs compared
to the local model. In this model, recent works by (Chowdhury & Zhou, 2022) and (Garcelon et al., 2022)
achieve the regret of ˜O(dT3/5)and ˜O(dT2/3)respectively. However, both of these works fail to achieve the
˜O/parenleftig√
T/parenrightig
regret in any setting. Furthermore, since both works rely on privacy amplification by shuffling, their
guarantees only work for ϵ≤O(1/T3/10)andϵ≤O(1/T1/4)respectively, which are a lot smaller than what
is used in practice (which is typically O(1)). Therefore, a question naturally arises:
Is it possible to achieve ˜O/parenleftig√
T/parenrightig
regret in a stronger privacy setting than the central model?
In this paper, we will provide sufficient conditions under which the answer to the above question is yes.
Contributions. We introduce a new private variant of the LinUCB algorithm (Chu et al., 2011; Abbasi-
yadkori et al., 2011) where the confidence set is constructed using the predictions of an online learner
(Abbasi-Yadkori et al., 2012). By carefully choosing the online learner and the loss function, this new
approach allows us to construct tighter confidence sets for the unknown parameter θ⋆which results in a
O/parenleftig√
dT3/4/ϵ/parenrightig
regret with high probability, improving the best known bound of ˜O/parenleftbig
(dT)3/4/√ϵ/parenrightbig
(Zheng
et al., 2020) whenever ϵ≥1√
d. Further, when the minimum eigenvalue of the expected gram matrix Et[xtxT
t]
is bounded from below, the regret guarantee of the new algorithm improves to ˜O/parenleftig√
dT/ϵ/parenrightig
, recovering the
asymptotic regret guarantee of the central model while guaranteeing LDP. This regret, to the best of our
knowledge, is the first ˜O(√
T)regret guarantee for LDP stochastic contextual linear bandit in any setting.
Finally, we test our algorithm in the same experiments as in (Chowdhury & Zhou, 2022) and show that our
algorithm has empirical improvements over previous works.
2Under review as submission to TMLR
Algorithm 1 Private (Contextual) Online LinUCB
1:Input: Privacy parameters ϵ,δ, failure parameter α, covariance matrix Σ =E[ηx,tηT
x,t], minimum
eigenvalueλminofE[xtxT
t], domain diameter D, time horizon T, threshold ¯λ.
2:Initializeθ1= 0,˜V0=Id×d,˜u0= 0,σ= 2/radicalbig
2 log(1.25/δ)/ϵ,∆2= 0.
3:ifλmin≤¯λthen
4: ∆2=¯λ
5:end if
6:fort= 1...Tdo
7:Local step performed by user t:
8:Receiveθt,˜Vt−1,˜ut−1from the server. Construct the confidence set Ct−1using Lemma 3.3.
9: (xt,˜θt) = arg max(x,θ)∈Dt×Ct−1⟨x,θ⟩
10:Playxtand observe reward yt
11:Perturbxtwith a small amount of noise: ¯xt←xt+ζtwhereζt∼N/parenleftbig
0,∆2Id/parenrightbig
.
12:Update ˜xt= ¯xt+ηx,t,˜yt=yt+ηy,twhereηx,t∼N(0,σ2Id),ηy,t∼N(0,σ2).
13:Get the loss lt(θ) = (⟨˜xt,θt⟩−˜yt)2−∥θ∥2
Σ
14:Computegt= 2˜xt(⟨˜xt,θt⟩−˜yt)−2Σθt
15:Send ˜xt,˜yt, andgtto the server.
16:Server step:
17:Sentθt,˜xt,˜yt, andgtto Maler (Algorithm 5) and get back θt+1.
18:Update the history {θ1,...,θt}∪{θt+1}
19:Update ˜Vt=˜Vt−1+ ˜xt˜xT
t,˜ut= ˜ut−1+⟨θt,˜xt⟩˜xt
20:end for
21:return ˜x1,..., ˜xTand˜y1,..., ˜yT
2 Problem Setup
LetAbe an action space and Ca context space. In the stochastic contextual linear bandit setting that we
are considering, in every round t∈[T], the learner receives an i.i.d random context ct∈Cand a function
ϕ(·,·) :C×A∝⇕⊣√∫⊔≀→ Rd, and picks an action atcorresponding to xt:=ϕ(ct,at)∈Dt. The learner then receives a
random reward yt=⟨θ⋆,xt⟩+ηtwhereηtis a zero-mean and independent R2−subgaussian random variable
whereRis a positive constant. Then we define the regret as:
RegretT=T/summationdisplay
t=1/bracketleftbigg
max
x∈Dt⟨θ⋆,x⟩−⟨θ⋆,xt⟩/bracketrightbigg
Our goal is to design an algorithm that achieves sublinear regret bound and guarantees that the actions and
rewards sequences {(x1,y1),..., (xT,yT)}are LDP. For n∈N, we denote the set {1,...,n}as[n]. We use
the standard big- Onotation to hide constants and ˜Oto hide additional logarithmic factors. Throughout
the paper,∥·∥is used to indicate the Euclidean norm unless specified otherwise. A symmetric matrix
M∈Rd×dis a positive-semidefinite matrix if xTMx≥0for anyx∈Rdand we define its associated norm as
∥x∥M=√
xTMx. We also define Ex[·]as the expectation over the randomness of some random variable x
andlog(x)as the natural logarithm of x.
Assumption : We assume the reward, and the norm of the parameter θ⋆and feature map xt=ϕ(ct,at)are all
bounded:∥xt∥≤1,|yt|≤1,|⟨xt,θ⋆⟩|≤1for allt∈[T], and∥θ⋆∥≤1. We also assume that we have access
toTunique users. Note that these are all standard assumptions from the literature (Shariff & Sheffet, 2018;
Chowdhury & Zhou, 2022).
3Under review as submission to TMLR
3 Online LDP LinUCB
Our private method described in Algorithm 1 is a private variant of the LinUCB algorithm (Chu et al., 2011;
Abbasi-yadkori et al., 2011). The main task of the algorithm is to derive an ellipsoid confidence set defined as
Ct−1:=/braceleftbig
θ∈Rd:∥θ−V−1
t−1ut−1∥Vt−1≤βt/bracerightbig
(1)
whereVt=/summationtextt
i=1xtxT
t,βtis the width of the confidence set, and ut−1=/summationtextt−1
i=1xiyi. Our goal is to pick an
appropriate βtsuch that the optimal parameter θ⋆is inside the ellipsoid with high probability for all t∈[T].
LinUCB identifies xt∈Dtandθt∈Ct−1that maximizes⟨x,θ⟩and playsxt. Overall, LinUCB guarantees
the following regret:
RegretT≤˜O/parenleftig
max
tβt√
dT/parenrightig
Thus, as long as we can design a tight confidence ellipsoid (small βt), our linear bandit algorithm will have a
small regret.
To ensure privacy, we unfortunately cannot update our algorithm with the true value of Vtandut. Instead, we
have to use private approximations ˜Vtand˜utto define an analogous ˜Ct−1. Let ˜Vt=Vt+Htand˜ut=ut+ht.
Assuming∥Ht∥≤ρmax,∥ht∥H−1
t≤νfor someρmax,ν≥0, then from (Shariff & Sheffet, 2018), we know
that the confidence width is bounded by:
βt≤˜O/parenleftig√
d+√ρmax+ν/parenrightig
(2)
Since higher βtleads to higher regret, we would like to minimize the error measures ρmaxandνintroduced
by the private approximations.
We can now shed light on why the regret guarantees for local models are worse than for the central model.
In the central model, since the server is allowed to see the raw data xtandyt, the server can compute
˜Vtand˜utprivately using the tree-aggregation mechanism (Chan et al., 2011; Dwork et al., 2010). Then,
we haveρmax≤˜O(√
d/ϵ)andRegretT≤˜O(d√
T+d3/4√
T/√ϵ). However, in the local model, since each
user perturbs their data before sending them to the server, applying tree-aggregation is off the table. If we
naively apply the Gaussian Mechanism with T rounds of compositions, ρmaxnow is ˜O(√
dT/ϵ )and the regret
becomes ˜O(d√
T+d3/4T3/4/√ϵ).
In this section, we propose a new approach for designing the confidence sets LDP LinUCB. Our approach
is based on the online-to-confidence-set conversion in (Abbasi-Yadkori et al., 2012) where the main idea is
that the predictions of any online algorithm that predicts the responses of the chosen inputs in a sequential
manner can be “converted” to a confidence set. In each round t, the online algorithm will receive xt,yt,
predictθt, and suffer the loss lt(θt) =(⟨θt,xt⟩−yt)2. The goal of the online learner is to discover the “true”
valueθ⋆. We measure its performance via its own notion of regret ( RegretOL), and we define MTto be a
known upper-bound on RegretOL.
RegretOL≜T/summationdisplay
t=1lt(θt)−lt(θ⋆) (3)
MT≥RegretOL (4)
Intuitively, a low regret means that the online learner is able to predict a good approximate of the optimal
θ⋆. Now, (Abbasi-Yadkori et al., 2012) show how to use the bound MTto construct a confidence set with the
width bounded by:
βT≤˜O/parenleftig/radicalbig
MT/parenrightig
Since the width of the confidence ellipsoid depends on the regret of the online learner, one could hope
that with carefully designed online learner and loss function, the confidence width would be small and we
4Under review as submission to TMLR
can see improvements in the final regret bound for LinUCB. We will now show that this is indeed the
case and Algorithm 1 using this approach can achieve ˜O(√
dT)regret when the second-moment matrix
E[xtxT
t]⪰λminIfor someλmin≈O(1)(refer to Remark 3.7 for more details).
Algorithm 1 is based on the Online LinUCB algorithm (Abbasi-Yadkori et al., 2012) (for more details, refer
to Section A.2). In every round t, a unique local user treceives some private information from the server
that they can use to construct the confidence set Ct. Then the user chooses xtthat maximizes the upper
confidence bound maxθ∈Ct−1⟨x,θ⟩(step 9) and receives reward yt. Ifxtis not well-conditioned ( λminis
smaller than some threshold ¯λin step 3), the user perturbs xtwith a small amount of noise in step 11 to
make sure the loss lt(θ)that is sent to the online learner is strongly-convex in expectation. Finally, user t
perturbsxt,ytwith Gaussian noise to maintain LDP, computes the gradient using private information, and
sends ˜xt,˜yt, andgtto the server. The server then uses this new information to update the prediction of the
online learner ( θt+1), and the history ˜Vt+1and˜utso that the next user can make a better decision.
Our algorithm elaborates on this Online LinUCB strategy with two key ideas. First, instead of using the
intuitive squared loss lt(θ) = (⟨θ,xt⟩−yt)2, we use the more peculiar choice lt(θ) =(⟨˜xt,θt⟩−˜yt)2−∥θ∥2
Σ
for some to-be-specified Σ. Second, we employ the advanced online learning algorithm Maler (Wang et al.,
2020b) as the online learner.
The reason for the choice of the loss is a bit technical. Intuitively, if we want the online learner to accurately
approximate θ⋆, we wantθ⋆to be the minimizer of the loss lt(θ). However, due to the noise in xtandyt,θ⋆
is not the minimizer of the square loss (⟨˜xt,θt⟩−˜yt)2. To counteract this issue, we incorporate a negative
regularizer term, −∥θ∥2
Σ, which serves to neutralize the variance introduced by the privacy noise in xt. Now,
with the added regularizer, θ⋆becomes the minimizer of E[lt(θ)]. At first glance, this new loss appears to be
intractable because it is non-convex. However, it is convex in expectation , which is sufficient to guarantee an
O(√
T)regret. This in turn translates to ˜O(T3/4)for the final regret bound.
The online learner can potentially do even better than O(√
T)regret in certain favorable settings. Specifically,
when E[xtxT
t]⪰λminIforλmin>0,lt(θ)isstrongly convex in expectation , despite not being convex. This
necessitates the use of an online learner capable of adapting to such advantageous scenarios and refining
the regret to O(logT). This is precisely the scenario where Maler proves invaluable. Maler (described in
Algorithm 5) is an online learner that adjusts to achieve the optimal regret across various types of loss
functions, including convex, strongly convex, and exp-concave. We demonstrate that implementing Maler
with our loss function yields a dimension-independent regret of O(logT)with high probability. This allows us
to attain a regret of ˜O(√
dT),which is not only asymptotically optimal but also improves upon the non-private
worst-case regret guarantee for general settings by an order of O(√
d)(see Section 3.2 for more discussion).
Remark 3.1.Let us discuss one specific example when the condition E[xtxT
t]⪰λminIis satisfied, giving
our algorithm the optimal regret guarantee of ˜O(√
dT). Assuming we have kavailable actions, and let each
action bexi∼N(0,σ2Id). Then, for the condition E/bracketleftbig
xtxT
t/bracketrightbig
⪰λminIfor someλmin≈O(1)to be true,
we need to show that E/bracketleftbig
vTxtxT
tv/bracketrightbig
≥CwhereCis a positive constant and vis a unit vector. Notice that
E/bracketleftbig
vTxtxT
tv/bracketrightbig
≥E/bracketleftbig
min1≤i≤kvTxixT
iv/bracketrightbig
, thus if we can show a constant lower bound for E/bracketleftbig
min1≤i≤kvTxixT
iv/bracketrightbig
then we are done. We have vTxi∼N(0,σ2)(sincevis a unit vector) for every i∈[k], thus by Lemma
H.15,P[|xi|≤t]≤√
2
σ√πtfort >0. Then, we can apply Theorem H.14 to get E/bracketleftbig
min1≤i≤kvTxixT
iv/bracketrightbig
=
E/bracketleftbig
min1≤i≤k|⟨v,xi⟩|2/bracketrightbig
≥σ2π
6k2. Thus, as long as the number of actions is not too large, this example would
fall under our favorable setting.
Before we prove the utility guarantee of Algorithm 1, let us first show that Algorithm 1 is (ϵ,δ)−LDP.
Theorem 3.2. (Privacy Guarantee) Algorithm 1 guarantees (ϵ,δ)−LDP.
Proof.Let us define the local step of Algorithm 1 as the local mechanism Mt. Letx′
tandy′
tbe the action
and reward of a new user at time t. By the boundedness assumption, we have maxxt,x′
t∈X∥xt−x′
t∥≤2and
maxyt,y′
t∈Y|yt−y′
t|≤2for allt∈[T]. Thus, by the classic Gaussian Mechanism in (Dwork et al., 2010), the
outputs ˜xtand˜ytof the local mechanism Mtsatisfy (ϵ,δ)−LDP for all t. Further, since θtandgtare computed
using a sequence of private parameters ˜x1,˜y1,..., ˜xt−1,˜yt−1and no other sensitive information (we only
5Under review as submission to TMLR
want to protect{(x1,y1),..., (xt,yt)}),θtandgtalso satisfy (ϵ,δ)−LDP by post-processing. Consequently,
Algorithm 1 guarantees (ϵ,δ)−LDP for every user t∈[T], as each local mechanism Mtis(ϵ,δ)−LDP.
To make the analysis more succinct and easier to follow, let us define the “good event” Eas in Section B in the
Appendix. Roughly speaking, Eis the event in which a small number of standard martingale concentration
bounds hold simultaneously. Then, from Lemma B.1, we know that the good event Ehappens with high
probability. Now, we can show the following result on the confidence set.
Lemma 3.3. We define ˜VN−1=/summationtextN−1
t=1˜xt˜xT
t,˜uN−1=/summationtextN−1
t=1⟨θt,˜xt⟩˜xt(θtis the prediction of the online
learner), and ˆθN=˜V−1
N−1˜uN−1. Assuming∥θt∥≤D, then under event E, the true parameter θ⋆lies in the
set:
CN−1=/braceleftig
θ∈Rd:∥θ−ˆθN∥2
˜VN−1≤MN+KN/bracerightig
for anyN≥1and
KN=γD∆2log(T/α)/radicaltp/radicalvertex/radicalvertex/radicalbtNN/summationdisplay
t=1∥θt−θ⋆∥2+γ/parenleftigg
R+D/radicalbig
log(1/δ)
ϵ+D∆/parenrightigg2
log(T/α)
+γ/parenleftbigglog(1/δ)
ϵ2+ ∆2/parenrightbigg
log(T/α)N/summationdisplay
t=1∥θt−θ⋆∥2
for a sufficient large constant γ >0.
We now provide a sketch of the proof of Lemma 3.3 below. For the full proof, refer to section D in the
appendix.
Proof.Our proof follows the proof of Theorem 1 in (Abbasi-Yadkori et al., 2012). From our definition of MN
and the loss lt, we have:
MN≥N/summationdisplay
t=1(⟨˜xt,θt⟩−˜yt)2−∥θt∥2
Σ−(⟨˜xt,θ⋆⟩−˜yt)2+∥θ⋆∥2
Σ
Plugging in ˜yt=⟨xt,θ⋆⟩+rt+ηy,tand˜xt=xt+ζt+ηx,t:
=N/summationdisplay
t=1(⟨xt,θt−θ⋆⟩+⟨ηx,t,θt⟩+⟨ζt,θt⟩−rt−ηy,t)2−∥θt∥2
Σ−(⟨ηx,t,θ⋆⟩+⟨ζt,θ⋆⟩−rt−ηy,t)2+∥θ⋆∥2
Σ
Let us denote zt=rt+ηy,t. Now expanding the squares and rearranging the terms we get:
N/summationdisplay
t=1(⟨xt,θt−θ⋆⟩)2≤MN+N/summationdisplay
t=1−2(⟨ηx,t+ζt,θt⟩−zt)⟨xt,θt−θ⋆⟩/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
At+2zt⟨ηx,t+ζt,θt−θ⋆⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Bt−⟨ζt,θt⟩2+⟨ζt,θ⋆⟩2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Ct
−2(⟨ζt,θt⟩⟨ηx,t,θt⟩−⟨ζt,θ⋆⟩⟨ηx,t,θ⋆⟩)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Dt−(θt−θ⋆)T(ηx,tηT
x,t−Σ)(θt+θ⋆)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Et(5)
Under eventE, we have
At+Bt=−N/summationdisplay
t=12(⟨ηx,t+ζt,θt⟩−zt)⟨xt,θt−θ⋆⟩+ 2zt⟨ηx,t+ζt,θt−θ⋆⟩
≤˜O
/radicaltp/radicalvertex/radicalvertex/radicalbtN/summationdisplay
t=1(⟨xt,θt−θ⋆⟩)2+/radicaltp/radicalvertex/radicalvertex/radicalbtN/summationdisplay
t=1⟨ηx,t+ζt,θt−θ⋆⟩2
 (6)
6Under review as submission to TMLR
Notice that the first sum in the right-hand side of Eq.6 is exactly the sum in the left-hand side of Eq.5. Thus,
we can use Proposition H.8 and H.9 to bound this term. For the second term in the right-hand side of Eq.6,
we can again use the fact that we are under the good event Eto control the sum.
The sum of CtandDtcan be written as follows:
N/summationdisplay
t=1⟨ζt,θ⋆⟩2−⟨ζt,θt⟩2+ 2(⟨ζt,θ⋆⟩⟨ηx,t,θ⋆⟩−⟨ζt,θt⟩⟨ηx,t,θt⟩) =N/summationdisplay
t=1⟨ζt,θ⋆−θt⟩⟨ζt,θ⋆+θt⟩
+ 2(θ⋆−θt)TζtηT
x,t(θ⋆−θt) + 2θT
tζtrT
t(θ⋆−θt) + 2(θ⋆−θt)TζtrT
tθt
Using norm bound of Gaussian random vector and corollary H.12:
Ct+Dt≤O
D∆2log(T/α)/radicaltp/radicalvertex/radicalvertex/radicalbtNN/summationdisplay
t=1∥θt−θ⋆∥2
+O
D∆/radicalbig
log(1/δ)
ϵ/radicaltp/radicalvertex/radicalvertex/radicalbtlog(T/α)T/summationdisplay
t=1∥θt−θ⋆∥2

For the term Etin Eq.5, since E/bracketleftbig
ηx,tηT
x,t/bracketrightbig
= Σ, it is a Martingale difference sequence and by Theorem H.2
we have:
|N/summationdisplay
t=1(θt−θ⋆)T(ηx,tηT
x,t−Σ)(θt+θ⋆)|≤˜O
Dlog(1/δ) log(T/α)
ϵ2/radicaltp/radicalvertex/radicalvertex/radicalbtN/summationdisplay
t=1∥θt−θ⋆∥2

Now we can combine the bounds of all the terms and use Proposition H.8 to get:
N/summationdisplay
t=1(⟨˜xt,θt−θ⋆⟩)2≤MN+KN
Let us denote the set CN−1as the ellipsoid underlying the covariance matrix ˜VN−1=I+/summationtextN−1
t=1˜xt˜xT
tand
centering at
ˆθN= arg min
θ∈Rd/parenleftigg
∥θ∥2
2+N−1/summationdisplay
t=1(⟨˜xt,θt−θ⟩)2/parenrightigg
=˜V−1
N−1/parenleftiggN−1/summationdisplay
t=1⟨θt,˜xt⟩˜xt/parenrightigg
=˜V−1
N−1˜uN−1
We can thus express the ellipsoid as:
ˆCN−1=/braceleftigg
θ∈Rd: (θ−ˆθN)T˜VN−1(θ−ˆθN) +∥ˆθN∥2
2+N−1/summationdisplay
t=1(⟨˜xt,θt−ˆθN⟩)2≤MN+KN/bracerightigg
The ellipsoid is contained in a larger ellipsoid
ˆCN−1⊆CN−1=/braceleftig
θ∈Rd:∥θ−ˆθN∥2
˜Vn−1≤MN+KN/bracerightig
Thus,θ⋆lies inCN−1with high probability.
With Lemma 3.3 in hand, we can show a general regret bound:
Theorem 3.4. (Utility guarantee) Recall that MTis the regret of our online learner (see equation (4)), and
KTis as defined in Lemma 3.3. Under event E, the regret of Algorithm 1 is:
RegretT≤˜O/parenleftigg/radicalbig
MT+KT/radicaligg
2Tdlog/parenleftbigg
1 +T
d/parenrightbigg/parenrightigg
7Under review as submission to TMLR
As we can see, this regret bound is quite similar to the regret bound of its non-private counterpart assuming
MTandKTcan be controlled. Now we show that at worst, this bound is ˜O/parenleftig√
dT3/4/ϵ/parenrightig
which isO/parenleftbig
d1/4/parenrightbig
improvement over the current best known bound for LDP Stochastic Linear Bandit whenever ϵ≥1√
d. Then,
we show that in certain settings, this bound becomes ˜O(√
dT), which to the best of our knowledge, is the
new state-of-the-art bounds for any stronger privacy model than the central model.
3.1 ˜O(√
dT)regret bound
From Theorem 3.4, it is clear that if we want to have ˜O(√
T)regret, we need our online learner to have
logarithmic regret i.e MT=O(logT). However, for this to be true, one might think that we need our loss to
be either strongly convex or exp-concave with a sufficiently large strong-convexity/exp-concavity constant.
Unfortunately, the loss lt(θ) = (⟨˜xt,θt⟩−˜yt)2−∥θ∥2
Σdoes not fall into either of these family of functions.
Surprisingly, it may still be possible to guarantee low regret with high probability using lt(θt). Denote
L(θt) =E[lt(θt)]. Then:
∇L(θt) =E[2(¯xt+ηx,t)(⟨¯xt,θt⟩+⟨ηx,t,θt⟩−yt−ηy,t)−2Σθt]
Sinceηx,t,ηy,tare zero-mean and ¯xt,ηx,t,ηy,tare independent:
∇L(θt) =E[2¯xt(⟨¯xt,θt⟩−yt) + 2ηx,t⟨ηx,t,θt⟩−2Σθt]
⇒∇2L(θt) =E[2¯xt¯xT
t] =E[2xtxT
t+ 2ζtζT
t]
SincextxT
tis a positive semi-definite matrix for all t∈[T], we have E[xtxT
t]⪰λminIdfor someλmin≥0.
Thus,lt(θt)isµ−strongly convex in expectation where µ= 2/parenleftbig
λmin+ ∆2/parenrightbig
. This is great news since even
thoughlt(θt)is not strongly convex, we can still show that the online learner Maler guarantees logarithmic
regret with high probability using the following lemma:
Lemma 3.5. (Strongly convex regret) Assuming L(θ) =E[lt(θt)]isµ−strongly convex and maxt,t′∥θt−θ′
t∥≤
2D. Then w.p at least 1−α, Maler (Algorithm 5) under the event Eguarantees:
T/summationdisplay
t=1lt(θt)−lt(θ⋆)≤O/parenleftbigg/parenleftbiggG2
µ+GD/parenrightbigg
logT+G2
µlog(1/α)/parenrightbigg
Furthermore, we have:
T/summationdisplay
t=1∥θt−θ⋆∥2≤O/parenleftbigg/parenleftbiggG2
µ2+GD
µ/parenrightbigg
logT+G2
µ2log(1/α)/parenrightbigg
The proof for Lemma 3.5 is provided in Section C in the Appendix. Notice that from this lemma, we
immediately have a high probability bound for the confidence ellipsoid in Lemma 3.3. Specifically, with
α≥1
T, we have:
MT≤O/parenleftbigg/parenleftbiggG2
λmin+ ∆2+GD/parenrightbigg
logT/parenrightbigg
And,
KT=O/parenleftigg
D∆2log(T/α)/radicaligg
TlogT/parenleftbiggG2
(λmin+ ∆2)2+GD
λmin+ ∆2/parenrightbigg/parenrightigg
+O
/parenleftigg
R+D/radicalbig
log(1/δ)
ϵ+D∆/parenrightigg2
log(T/α)

+O/parenleftbigg
log2T/parenleftbigglog(1/δ)
ϵ2+ ∆2/parenrightbigg/parenleftbiggG2
(λmin+ ∆2)2+GD
λmin+ ∆2/parenrightbigg/parenrightbigg
8Under review as submission to TMLR
LetH=G2
(λmin+∆2)2+GD
λmin+∆2. Then:
KT≤O
D∆2log(T/α)/radicalbig
THlogT+/parenleftigg
R+D/radicalbig
log(1/δ)
ϵ/parenrightigg2
log(T/α) +Hlog(1/δ)
ϵ2log2T

Now plugging KTandMTinto Theorem 3.4 we get the following corollary:
Corollary 3.6. Assuming∥θt∥≤D. Under eventE, for anyλmin≥0such that E[xtxT
t]⪰λminId×dand
α≥1/T, the regret of Algorithm 1 with Maler as the online learner and with threshold ¯λ=1
T1/4is upper
bounded by
O/parenleftigg/parenleftigg/parenleftigg
R+D/radicalbig
log(1/δ)
ϵ/parenrightigg
/radicalbig
log(T/α) +/radicalig
D∆2log(T/α)/radicalbig
THlogT+logT/radicalbig
Hlog(1/δ)
ϵ/parenrightigg
×/radicalbig
dTlog(T/d)/parenrightig
≤˜O/parenleftigg
min/braceleftigg√
dT3/4
ϵ,√
dT
ϵλmin/bracerightigg/parenrightigg
whereH=G2
(λmin+∆2)2+GD
λmin+∆2.
Let us discuss the implication of the regret bound in Corollary 3.6. Consider the best-case scenario, which
is whenλmin=O(1)(e.g.xtfollows some random distribution with constant variance). Then, ∆2= 0
and Algorithm 1 is (ϵ,δ)−LDP and guarantees ˜O(√
dT/ϵ )regret with high probability. Thus, Algorithm 1
guarantees the same regret bound asymptotically of non-private LinUCB. Further, by running our online
learner on a bounded domain, our online regret depends on the radius Dof the domain (which is set by the
user) rather than the dimension dof the feature vector xt. As a result, we are able to improve the dimension
dependence from O(d3/4)in previous works to O(√
d). Our regret bound would get worse as λmindecreases.
However, when λmin≤1
T1/4, notice that now ∆2=1
T1/4and Algorithm 1 guarantees ˜O(√
dT3/4/ϵ)regret,
which is the same asymptotic regret as the current best regret for LDP (contextual) linear bandit (Shariff &
Sheffet, 2018) but with improved dimension dependence. Overall, the regret of Algorithm 1 is always between
˜O(√
dT/ϵ )and ˜O(√
dT3/4/ϵ).
Remark 3.7.Since the regret in (Chowdhury & Zhou, 2022) is ˜O(dT3/5), Algorithm 1 has a better regret as
long asλmin≥Ω/parenleftbig1
T1/10/parenrightbig
while also providing stronger privacy guarantee and having no restriction on ϵ.
3.2 Comparisons with previous results
In the worst-case scenario ( λmin≤1
T1/4), Algorithm 1 provides a regret bound of ˜O(√
dT3/4/ϵ+√
dT)
with high probability. This surpasses the state-of-the-art result for LDP stochastic linear bandit, which is
˜O((dT)3/4/√ϵ+d√
T)wheneverϵ≥1√
d(which covers many practical scenarios where ϵis typically larger
than 1). Although both exhibit the same asymptotic regret of ˜O(T3/4), Algorithm 1 demonstrates superior
dimension dependence in both private and non-private terms. The key to this improvement lies in the
unique approach of Algorithm 1 concerning noise injection and the employment of an online learner with a
dimension-free regret. In (Zheng et al., 2020), the privacy guarantee is achieved by adding a Gaussian matrix
HttoVtand a Gaussian vector httout. From the concentration inequality of the Gaussian matrices, ∥Ht∥
is bounded by ˜O(√
dT/ϵ )with high probability. Thus, plugging this back in Eq. 1 and Eq. 2 yields the
˜O((dT)3/4/√ϵ+d√
T)regret. On the other hand, Algorithm 1 injects noise directly to xtandyt, instead of
Vtandut. Thus, we are not restricted by the√
dfactor that comes from the concentration bound of the
Gaussian matrix. The regret now hinges on the performance of the online learner Maler, which is O(D√
T)
whereDis the user-set bound for θt. As a result, we are able to improve the dimension dependence to O(√
d).
In the favorable settings ( λminisO(1)), the regret of Algorithm 1 is improved to O(√
dT). Comparatively, this
shows a notable advancement over other private algorithms. Specifically, Algorithm 1 outperforms the shuffle
model (Chowdhury & Zhou, 2022), which has a regret of ˜O(dT3/5), and matches the asymptotic regret of the
central model. However, our algorithm demonstrates a more favorable dimension dependence, attributable to
9Under review as submission to TMLR
Algorithm 2 Private Bandits Combiner
1:Input:Receive base learner 1 (Algorithm 9) and base learners ifori∈(1,M](Algorithm 1). Constants
L1,...,LM,α1,...,αM,R1,...,RM,Tusers, failure probability α, universal constant p, privacy noise
varianceσ2, privacy parameters ϵ,δ, covariance matrix Σ, domain diamater D, power constant k.
2:Initialize base learner 1 with ϵ,δ,α,Σ,D,λmin=1
T1/8andk= 1/8.
3:Initialize each base learner iwithϵ,δ,α,Σ,D,λmin=λi=2i−1
T1/8fori∈(1,M], and ¯λ= 1/T1/8.
4:SetT(i,0) = 0andˆµi
0= 0for alli, and setI1={1,...,M}.
5:Initializeθi
1= 0,˜Vi
0=Id×d,˜ui
0= 0for alli∈[M].
6:
7:fort= 1...Tdo
8:For the server:
9:SetU(i,t−1) = ˆµi
T(i,t−1)+ min
1,LiT(i,t−1)αi+p/radicalig
T(i,t)(1+σ2) log/parenleftbigT3MlogT(i,t)(1+σ2)
α/parenrightbig
T(i,t−1)
−Ri
Tfor alli.
10:Setit= arg maxi∈ItU(i,t−1).
11:For the local user t:
12:Receive the base learner index itandθit
t,˜Vit
t−1,˜uit
t−1from the server.
13:Usertfollows the policy of the base learner itand playxit
t, receive reward yit
t, andθt+1.
14:Send the noisy feature vector ˜xit
t, noisy reward ˜yit
t, and parameter θt+1to the server.
15:For the server:
16:UpdateT(it,t) =T(it,t−1) + 1andT(j,t) =T(j,t−1)forj̸=it.
17:Updateθit
t+1=θt+1,˜Vi
t=˜Vi
t−1+ ˜xit
t(˜xit
t)T,˜uit
t= ˜uit
t−1+⟨θit
t,˜xit
t⟩˜xit
t.
18:Updateθi
t+1=θi
t,˜Vi
t=˜Vi
t−1,˜ui
t= ˜ui
t−1for alli̸=it.
19:Update ˆµit
T(it,t)=1
T(it,t)/summationtextT(it,t)
τ=1˜yit
t.
20:if/summationtextT(it,t)
τ=1ˆµit
τ−1−ˆtitτ≥LitT(it,t)αit+p/radicalbigg
T(i,t)(1 +σ2) log/parenleftig
T3MlogT(i,t)(1+σ2)
α/parenrightig
then
21:It=It−1−{it}
22:else
23:It=It−1
24:end if
25:end for
the same reasons discussed in relation to (Zheng et al., 2020). Interestingly, Algorithm 1 also exhibits better
dimension dependence than even non-private algorithms (Abbasi-Yadkori et al., 2012; Abbasi-yadkori et al.,
2011) in this specific setting. This is because the non-private models consider worst-case bounds universally,
suggesting that these bounds might be further optimized under certain favorable conditions.
4 Online model selection for LDP LinUCB
From the previous section, we show that Algorithm 1 achieves ˜O(√
dT/ϵ )regret when λmin≈O(1)and
degrades to ˜O(√
dT3/4/ϵ)whenλmin≤1
T1/4. However, to run Algorithm 1, we need to know the minimum
eigenvalueλminofE[xtxT
t](the online learner does not need to know λminbut we still need λminto set the
confidence width). One might think that we can run a grid search through a range of values for λminto
find the one that works the best. However, running the algorithm multiple times using the same dataset
can degrade our privacy guarantee (though in practice people still tune their algorithms). Moreover, in a
truly online setting it may be that the minimum eigenvalue observed during this “training” period does not
capture the later values. In this section, we present a new algorithm that bypasses this problem.
Algorithm 2 is built around a meta-learner that has access to Mdistinct base learners (which are M−1
copies of Algorithm 1 and 1 copy of Algorithm 9 initialized with different guesses of λmin=λifori∈[M]).
10Under review as submission to TMLR
The reason we have to use two different types of base learners is that model selection algorithms usually
require anytime regret guarantees, which Algorithm 1 satisfies only under the condition where perturbation
ofxtis unnecessary (i.e., ∆2= 0). Thus, in scenarios where the actual λminis small, necessitating the
perturbation of xt, we instead utilize the anytime variant of Algorithm 1, which is obtained by applying the
classic “doubling trick” to Algorithm 1, as described in Algorithm 9. This variant serves as the first base
learner of Algorithm 2. Then, by setting λi=2i−1
T1/8fori∈[M], we can guarantee that at least one of our
guesses would be at most a constant factor away from the actual λminifλmin≥1
T1/8. In the scenario where
λmin<1
T1/8, the actual value of λminis not as important. This is due to our setting of the threshold ¯λat
1
T1/8. Under this condition, the base learner with the smallest λestimate automatically ensures a regret of
˜O(√
dT3/4). The goal of the meta-learner is to “combine” the outputs of Mbase learners into one output in
such a way that the final regret is not much worse than if we had selected the best base learner in hindsight.
We employ the Bandit Combiner Algorithm in (Cutkosky et al., 2020) as our meta-learner. We note that
there are more recent meta-learners with more refined guarantees and techniques (Pacchiano et al., 2023;
Cutkosky et al., 2021), as well as techniques that work even in the fully adversarial setting (Agarwal et al.,
2017; Pacchiano et al., 2020). However, (Cutkosky et al., 2020) is somewhat easier to use in our setting
because it applies out-of-the-box to combine base learners whose individual regret bounds have different
asymptotic rates.
The Bandit Combiner Algorithm (Algorithm 2) employs the use of the Upper Confidence Bound (UCB)
strategy by treating each base learner as an arm in a multi-armed bandit setup. This approach involves using
the average reward received by each base learner and their respective regret to establish the upper-confidence
bound. Through this method, the algorithm sequentially identifies the most effective learner. Overall,
Algorithm 2 guarantees O(RJ
T)regret where RJ
Tis the regret of the best base learner. For a more detailed
discussion of the algorithm, refer to (Cutkosky et al., 2020). We have the following guarantee for Algorithm
2:
Theorem 4.1. (see Corollary 2 (Cutkosky et al., 2020)) Let η1=ϵT1/8
√
dT(Plog3/2(T)ϵT1/8+1)andηi=
ϵ
P′log3/2(T)√
dT,L1=Plog3/2(T)√
d/parenleftig
ϵT1/8+1
ϵT1/8/parenrightig
andLi=P′log3/2(T)√
d
ϵλifor positive constants PandP′,
α1=3
4andαi=1
2fori∈[2,M], and setRivia:
Ri=LiTαi+(1−αi)1−αi
αi(1 +αi)1
αi
α1−αi
αi
iL1
αi
iTη1−αi
αi
i + 288 log(T3N/α)Tηi+/summationdisplay
k̸=i1
ηk
for alli. Letjbe the index of the base learner with the smallest regret. If λmin≥1
T1/8, then w.p at least
1−3α, the regret of Algorithm 2 under event Esatisfies:
RegretT≤˜O/parenleftigg√
dT
ϵλ2
min/parenrightigg
If0≤λmin<1
T1/8, then w.p at least 1−3α, the regret of Algorithm 2 under event Esatisfies:
RegretT≤˜O/parenleftigg
√
dT5/6+√
dT17/24
ϵ/parenrightigg
Overall, Algorithm 2 guarantees ˜O(√
dT/ϵ )regret when λminisO(1)without requiring the knowledge of
λmin. The guarantee then degrades with a rate of O(1/λ2
min), yet it remains below ˜O/parenleftig√
dT5/6+√
dT17/24
ϵ/parenrightig
.
This result reveals the trade-offs involved when combining base learners that have different asymptotic
regret guarantees. Since the general regret guarantee of Algorithm 2 is ˜O/parenleftigg
L1
aj
jTη1−αj
αj
j +/summationtext
k̸=i1
ηk/parenrightigg
, different
asymptotic rate with different ajrequires different settings of ηjfor the final rate to be optimal. Thus, to
adapt to the ˜O(√
dT)rate, we suffer the worst case rate of ˜O(√
dT5/6)instead of ˜O(√
dT3/4)as in Algorithm
11Under review as submission to TMLR
1. However, if we have a reason to believe that we are not in the favorable setting or if we want to preserve
the worst-case rate of Algorithm 1, we can instead run Algorithm 1 with λmin= 0and¯λ=1
T1/4to always
guarantee ˜O(√
dT3/4/ϵ)regret.
5 Experiments
In this section, we will compare the empirical performance of Algorithm 1 to that of previous works.
Specifically, we compare our algorithm to Shuffle Private LinUCB (SDP) (Chowdhury & Zhou, 2022), Joint
Differentially Private LinUCB (JDP) (Shariff & Sheffet, 2018), Locally Private LinUCB (LDP) (Zheng et al.,
2020), and the Non-private LinUCB (LinUCB) (Abbasi-yadkori et al., 2011). For our algorithm (OnlineUCB),
we implement Algorithm 1 with Maler as the online learner. In our experiment, we consider 100 arms with
dimensiond= 5. We run our algorithm over T= 20000 rounds and average the regrets over 50 trials. We
generate the optimal parameter θ⋆and the feature vector xtby sampling a (d−1)−dimensional vector of
norm 1/√
2uniformly at random and append it with a 1/√
2entry. We also use Bernoulli rewards to ensure
boundedness. This is the same exact setting as in (Chowdhury & Zhou, 2022). As we can see from Figure 1,
Figure 1: (a) ϵ= 0.2(b)ϵ= 1(c)ϵ= 10
in the low-privacy domain ( ϵ= 1andϵ= 10), our OnlineUCB algorithm significantly outperforms not only
the previous best-known LDP algorithm but also SDP and JDP (which use the weaker shuffle and central
models of differential privacy respectively) while having stronger privacy guarantee. This result is consistent
with the theory since when there exists a λmin=O(1)such that Et/bracketleftbig
xtxT
t/bracketrightbig
⪰λminI(which is our experiment
settings), OnlineUCB has a better regret guarantee than all of the previous private stochastic linear bandits
12Under review as submission to TMLR
algorithms. In the high privacy domain ( ϵ= 0.2), OnlineUCB does not do as well but still outperforms LDP
LinUCB and has comparable performance to Shuffle LinUCB. We also note that even though we use Maler
to be consistent with the theory, one could also use Online Gradient Descent (which runs a lot faster than
Maler) as the online learner to get the same empirical performance.
6 Conclusions
In this paper, we present a new algorithm for differentially private linear (contextual) stochastic bandit
in the local settings that uses an online learner to construct the confidence set. By carefully choosing the
online learner as well as the loss function sent to the online learner, our algorithm guarantees the regret
˜O/parenleftig
min/braceleftig√
dT3/4
ϵ,√
dT
ϵλmin/bracerightig/parenrightig
with high probability where λminis the lower bound on Et[xtxT
t]. Thus, when
λmin≈O(1), our algorithm is the first algorithm that guarantees ˜O(√
T)for the LDP setting. Further,
by running the online learner on a bounded domain, we are able to improve the regret dependence on the
dimensiondof the feature vector xtfromO(d3/4)toO(√
d). There are several limitations that one could
further explore to improve the results of this paper. The most natural question is if it is possible to guarantee
˜O/parenleftig√
T/parenrightig
for the local model without any further assumption. In our current result, we still rely heavily on
the fact that Et[xtxT
t]⪰λminIdto get ˜O/parenleftig√
T/parenrightig
regret. If that is not possible, what are other settings that
allow us to achieve ˜O/parenleftig√
T/parenrightig
regret? One could hope that by further exploiting the properties of specific
online learners from the rich literature of online optimization, we can find other favorable domains as well.
References
Yasin Abbasi-yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic bandits.
In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger (eds.), Advances in Neural
Information Processing Systems , volume 24. Curran Associates, Inc., 2011. URL https://proceedings.
neurips.cc/paper/2011/file/e1d5be1c7f2f456670de3d53c7b54f4a-Paper.pdf .
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Online-to-confidence-set conversions and ap-
plication to sparse stochastic bandits. In Neil D. Lawrence and Mark Girolami (eds.), Proceedings of
the Fifteenth International Conference on Artificial Intelligence and Statistics , volume 22 of Proceed-
ings of Machine Learning Research , pp. 1–9, La Palma, Canary Islands, 21–23 Apr 2012. PMLR. URL
https://proceedings.mlr.press/v22/abbasi-yadkori12.html .
Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E Schapire. Corralling a band of bandit
algorithms. In Conference on Learning Theory , pp. 12–38. PMLR, 2017.
T.-H. Hubert Chan, Elaine Shi, and Dawn Song. Private and continual release of statistics. ACM Trans. Inf.
Syst. Secur. , 14(3), nov 2011. ISSN 1094-9224. doi: 10.1145/2043621.2043626. URL https://doi.org/10.
1145/2043621.2043626 .
Albert Cheu, Adam Smith, Jonathan Ullman, David Zeber, and Maxim Zhilyaev. Distributed differential
privacy via shuffling. In Annual International Conference on the Theory and Applications of Cryptographic
Techniques , pp. 375–403. Springer, 2019.
Sayak Ray Chowdhury and Xingyu Zhou. Shuffle private linear contextual bandits. arXiv preprint
arXiv:2202.05567 , 2022.
Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In
Geoffrey Gordon, David Dunson, and Miroslav Dudík (eds.), Proceedings of the Fourteenth International
Conference on Artificial Intelligence and Statistics , volume 15 of Proceedings of Machine Learning Research ,
pp. 208–214, Fort Lauderdale, FL, USA, 11–13 Apr 2011. PMLR. URL https://proceedings.mlr.press/
v15/chu11a.html .
13Under review as submission to TMLR
Graham Cormode, Somesh Jha, Tejas Kulkarni, Ninghui Li, Divesh Srivastava, and Tianhao Wang. Privacy
at scale: Local differential privacy in practice. In Proceedings of the 2018 International Conference on
Management of Data , SIGMOD ’18, pp. 1655–1658, New York, NY, USA, 2018. Association for Computing
Machinery. ISBN 9781450347037. doi: 10.1145/3183713.3197390. URL https://doi.org/10.1145/
3183713.3197390 .
Ashok Cutkosky, Abhimanyu Das, and Manish Purohit. Upper confidence bounds for combining stochastic
bandits. arXiv preprint arXiv:2012.13115 , 2020.
Ashok Cutkosky, Christoph Dann, Abhimanyu Das, Claudio Gentile, Aldo Pacchiano, and Manish Purohit.
Dynamic balancing for model selection in bandits and rl. In Marina Meila and Tong Zhang (eds.),
Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of
Machine Learning Research , pp. 2276–2285. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.
press/v139/cutkosky21a.html .
Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Found. Trends
Theor. Comput. Sci. , 9(3–4):211–407, aug 2014. ISSN 1551-305X. doi: 10.1561/0400000042. URL
https://doi.org/10.1561/0400000042 .
Cynthia Dwork, Moni Naor, Toniann Pitassi, and Guy N. Rothblum. Differential privacy under continual
observation. In Proceedings of the Forty-Second ACM Symposium on Theory of Computing , STOC ’10, pp.
715–724, New York, NY, USA, 2010. Association for Computing Machinery. ISBN 9781450300506. doi:
10.1145/1806689.1806787. URL https://doi.org/10.1145/1806689.1806787 .
Úlfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Abhradeep
Thakurta. Amplification by shuffling: From local to central differential privacy via anonymity. In
Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms , pp. 2468–2479. SIAM,
2019.
Evrard Garcelon, Kamalika Chaudhuri, Vianney Perchet, and Matteo Pirotta. Privacy amplification via
shuffling for linear contextual bandits. In International Conference on Algorithmic Learning Theory , pp.
381–407. PMLR, 2022.
Y. Gordon, Alexander Litvak, Carsten Schuett, and Elisabeth Werner. On the minimum of several random
variables. Proceedings of the American Mathematical Society , 134:3665–3675, 12 2006. doi: 10.2307/4098204.
Steven R Howard, Aaditya Ramdas, Jon McAuliffe, and Jasjeet Sekhon. Time-uniform, nonparametric,
nonasymptotic confidence sequences. 2021.
Tor Lattimore and Csaba Szepesvári. Bandit Algorithms . Cambridge University Press, 2020.
Yingkai Li, Yining Wang, and Yuan Zhou. Nearly minimax-optimal regret for linearly parameterized bandits.
InConference on Learning Theory , pp. 2173–2174. PMLR, 2019.
Aldo Pacchiano, My Phan, Yasin Abbasi Yadkori, Anup Rao, Julian Zimmert, Tor Lattimore, and Csaba
Szepesvari. Model selection in contextual stochastic bandit problems. Advances in Neural Information
Processing Systems , 33:10328–10337, 2020.
Aldo Pacchiano, Christoph Dann, and Claudio Gentile. Data-driven regret balancing for online model selection
in bandits. arXiv preprint arXiv:2306.02869 , 2023.
Roshan Shariff and Or Sheffet. Differentially private contextual linear bandits. Advances in Neural Information
Processing Systems , 31, 2018.
Roman Vershynin. High-dimensional probability. 2018.
Guanghui Wang, Shiyin Lu, and Lijun Zhang. Adaptivity and optimality: A universal algorithm for online
convex optimization. In Ryan P. Adams and Vibhav Gogate (eds.), Proceedings of The 35th Uncertainty in
Artificial Intelligence Conference , volume 115 of Proceedings of Machine Learning Research , pp. 659–668.
PMLR, 22–25 Jul 2020a. URL https://proceedings.mlr.press/v115/wang20e.html .
14Under review as submission to TMLR
Guanghui Wang, Shiyin Lu, and Lijun Zhang. Adaptivity and optimality: A universal algorithm for online
convex optimization. In Uncertainty in Artificial Intelligence , pp. 659–668. PMLR, 2020b.
Jiujia Zhang and Ashok Cutkosky. Parameter-free regret in high probability with heavy tails. arXiv preprint
arXiv:2210.14355 , 2022.
Kai Zheng, Tianle Cai, Weiran Huang, Zhenguo Li, and Liwei Wang. Locally differentially private (contextual)
bandits learning. Advances in Neural Information Processing Systems , 33:12300–12310, 2020.
15Under review as submission to TMLR
A Backgrounds on LinUCB
A.1 LinUCB
Algorithm 3 LinUCB
1:fort= 1...Tdo
2: (xt,˜θt) = arg max(x,θ)∈Dt×Ct−1⟨x,θ⟩
3:Playxtand observe reward yt
4:UpdateCt
5:end for
At its core, our algorithm is simply the private version of LinUCB (Algorithm 3). Since its introduction
in 2011 (Chu et al., 2011; Abbasi-yadkori et al., 2011), LinUCB has been the method of choice for linear
bandit problems due to its good theoretical guarantee and its effectiveness in practice. LinUCB is based on
the idea of optimisim-in-the-face-of-uncertainty (OFU). The OFU principle elegantly solves the exploration-
exploitation dilemma of bandit problems. The basic idea of this principle is to maintain a confidence set for
the vector of coefficients of the linear function. At every round t, LinUCB constructs a confidence set Ctthat
contains the optimal parameter θ⋆with high probability. It then computes an upper confidence bound on
the reward of each action in the decision set Dt, then chooses the action with the highest upper confidence
bound:xt←arg maxx∈Dt(maxθ∈Ct⟨θ,x⟩). LetVt=/summationtextt
i=1xixT
i+λI,ut=/summationtextt
i=1xiyi,X<t∈R(t−1)×dwhere
X<t,s=xT
sfors < t, andY<t∈Rt−1be the vector of rewards up to round t. Since in our setting, the
rewards are linear functions of the actions with subgaussian noises, it is natural to center the confidence set
Cton the linear regression estimate:
ˆθt= arg min
θ∈Rd∥θX<tθ−Y<t∥2+∥θ∥2
=V−1
tut
Whenever we get a new reward yt, it gives us information about the projection of θ⋆onto the action space,
thus ˆθis closer to θ⋆along the directions where many actions have been taken. This motivates the use of
ellipsoid confidence set that is smaller in such directions. LinUCB defines the confidence set as follows:
Ct=/braceleftig
θ∈Rd:∥θ−ˆθt∥Vt≤βt/bracerightig
(7)
whereβtis the width of the confidence set. (Abbasi-yadkori et al., 2011) then shows that the optimal θ⋆is
insideCtwith high probability with appropriate βtby bounding the difference between the actual rewards
and the predicted rewards using Algorithm 3.
Theorem A.1. (Theorem 2 in (Abbasi-yadkori et al., 2011)) Define yt=⟨xt,θ⋆⟩+ηtwhereηtis
R−subgaussian noise and assume that ∥θ⋆∥≤S. Then for any δ > 0, with probability at least 1−δ,
for allt≥1,θ⋆lies in the set:
Ct=/braceleftig
θ∈Rd:∥θ−ˆθt∥Vt≤βt/bracerightig
whereβt=R/radicalbigg
2 log/parenleftig
det(Vt)1/2det(λI)−1/2
δ/parenrightig
+λ1/2S.
Finally, (Abbasi-yadkori et al., 2011) proves the following regret bound:
RegretT≤˜O(βt√
dn)
Now, we can plug in the value of βtin Theorem A.1 and use the fact that log(det(Vt))≤O(d)(Lemma 10 in
(Abbasi-yadkori et al., 2011)) to get the ˜O(d√
T)regret of non-private LinUCB.
16Under review as submission to TMLR
A.2 LinUCB with online-to-confidence conversion
Algorithm 4 Online LinUCB
1:fort= 1...Tdo
2:Construct confidence set Ct−1
3: (xt,˜θt) = arg max(x,θ)∈Dt×Ct−1⟨x,θ⟩
4:Get prediction ˆytfrom online learner M
5:Playxtand observe reward yt
6:UpdateCt
7:end for
Building on the original LinUCB algorithm, (Abbasi-Yadkori et al., 2012) introduces a new variant of LinUCB
where we construct the confidence set using the predictions from an online optimization algorithm. The main
procedure is described in Algorithm 4. In every round t, Algorithm 4 sends the action xt, the reward ytto an
online learner M and gets back some predictions ˆyt. The online learner then suffers a loss lt(ˆyt)(e.g: the
squared loss lt(ˆyt) = ( ˆyt−yt)2). Then, if we define Mnas the upper bound on the regret at time nof the
online learner M ( Mn≥/summationtextN
t=1lt(θt)−lt(θ⋆)), (Abbasi-Yadkori et al., 2012) shows that we can construct a
confidence set that depends on Mnwhereθ⋆is inside the confidence set with high probability.
Theorem A.2. (Corollary 2 in (Abbasi-Yadkori et al., 2012)) Assume ∥θ⋆∥≤S. Then for any δ∈(0,1/4],
with probability at least 1−δ, the true parameter θ⋆lies in the intersections of the sets
Cn=/braceleftigg
∥θ∥2+n/summationdisplay
t=1(ˆyt−⟨θ,xt⟩)2≤β2
n/bracerightigg
whereβ2
n=S2+ 1 + 2Mn+ 32R2log/parenleftig
R√
8+√1+Mn
δ/parenrightig
For a more detailed analysis, please refer to (Abbasi-Yadkori et al., 2012) or our proofs of Lemma 3.3. Since
Algorithm 1 is based on LinUCB with online-to-confidence conversion, the analysis of Algorithm 1 and
Algorithm 4 are very similar. We have the final regret bound for Algorithm 4 as follows:
RegretT≤˜O/parenleftbigg
max
1≤t≤Tβt√
dT/parenrightbigg
At first glance, this regret bound is exactly the same as the regret bound of the original LinUCB discussed
in Section A.1. However, notice that the confidence width of the original LinUCB in Section A.1 is always
O(√
d)due to its dependence on log(det(Vt))while Online LinUCB depends on the regret of the online
learnerMinstead. Though the worst case bound of both algorithms are ˜O(d√
T), Online LinUCB allows
us the flexibility of choosing our own online optimization algorithms to adapt to different problem settings.
Specifically, (Abbasi-Yadkori et al., 2012) shows that when θ⋆is a sparse vector, Online LinUCB guarantees
˜O(/radicalbig
dT∥θ⋆∥0)which is better than the worst-case bound ˜O(d√
T). This is the reason why we chose Online
LinUCB as the base algorithm for our private method since it allows us to potentially improve the final regret
bound for favorable scenarios.
17Under review as submission to TMLR
B Good Event and High-confidence Argument
Letzt=rt+ηy,t,θtis the prediction of the online learner in Algorithm 1, lt(θt)is the loss we sent to the
online learner, L(θt) =E[lt(θt)], andGis the clipping constant. We define the following good events:
E1=/braceleftig
∀t∈[T] :∥ηx,t∥≤σ/radicalbig
2 log(2T/α)/bracerightig
E2=/braceleftig
∀t∈[T] :∥ηy,t∥≤σ/radicalbig
2 log(2T/α)/bracerightig
E3=/braceleftig
∀t∈[T] :∥ζt∥≤∆/radicalbig
2 log(2T/α)/bracerightig
E4=/braceleftig
∀t∈[T] :/vextenddouble/vextenddoubleηx,tηT
x,t−Σ/vextenddouble/vextenddouble
op≤2DCσ2(d+ log(2T/α))/bracerightig
E5=/braceleftigg
∀N∈[T] :|N/summationdisplay
t=1(zt−⟨ηx,t+ζt,θt⟩)⟨xt,θt−θ⋆⟩|
≤/parenleftigg
R+2(D+ 1)/radicalbig
2 log(1.25/δ)
ϵ+D∆/parenrightigg/radicaltp/radicalvertex/radicalvertex/radicalbt2/parenleftigg
1 +N/summationdisplay
t=1⟨xt,θt−θ⋆⟩2/parenrightigg/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtlog
/radicalig
1 +/summationtextN
t=1⟨xt,θt−θ⋆⟩2
α



E6=

∀N∈[T] :|N/summationdisplay
t=1zt⟨ηx,t+ζt,θt−θ⋆⟩|≤/parenleftigg
R+2/radicalbig
2 log(1.25/δ)
ϵ/parenrightigg/radicaltp/radicalvertex/radicalvertex/radicalbt2/parenleftigg
1 +N/summationdisplay
t=1⟨ηx,t+ζt,θt−θ⋆⟩2/parenrightigg


E7=/braceleftigg
∀N∈[T] :/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
t=1(θ⋆−θt)TζtηT
x,t(θ⋆−θt)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤∆∥θt−θ⋆∥/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbt2/parenleftigg
1 +N/summationdisplay
t=1⟨ηx,t,θ⋆−θt⟩2/parenrightigg
log
/radicalig
1 +/summationtextN
t=1⟨ηx,t,θ⋆−θt⟩2
α



E8=/braceleftigg
∀N∈[T] :/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
t=1θT
tζtrT
t(θ⋆−θt)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
t=1(θ⋆−θt)TζtrT
tθt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤∆∥θt∥/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbt2/parenleftigg
1 +N/summationdisplay
t=1⟨ηx,t,θ⋆−θt⟩2/parenrightigg
log
/radicalig
1 +/summationtextN
t=1⟨ηx,t,θ⋆−θt⟩2
α

+σ∥θt∥/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbt2/parenleftigg
1 +N/summationdisplay
t=1⟨ζt,θ⋆−θt⟩2/parenrightigg
log
/radicalig
1 +/summationtextN
t=1⟨ζt,θ⋆−θt⟩2
α



E9=/braceleftigg
∀N∈[T] :/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleN/summationdisplay
t=1(θ⋆−θt)T(ηx,tηT
x,t−Σ)(θt+θ⋆)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤20σ2(D+ 1) log(2T/α)/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalvertex/radicalbtN/summationdisplay
t=1∥θ⋆−θt∥2log
16
α
log
e2
/radicaltp/radicalvertex/radicalvertex/radicalbtN/summationdisplay
t=116(D+ 1)2σ4log2/parenleftbig2T
α/parenrightbig
∥θ⋆−θt∥2
ν2

1

2

+23 max(ν,max
t4(D+ 1)σ2log/parenleftbigg2T
α/parenrightbigg
∥θ⋆−θt∥) log
224
α/bracketleftigg
log/parenleftigg
2e2max(ν,maxt4(D+ 1)σ2log/parenleftbig2T
α/parenrightbig
∥θ⋆−θt∥)
ν/parenrightigg/bracketrightigg2



18Under review as submission to TMLR
for some arbitrary constant ν≥0and[x]1:= max(1,x).
E10=

T/summationdisplay
t=1⟨∇L(θt)−∇lt(θt),θt−θ⋆⟩≤2/radicaltp/radicalvertex/radicalvertex/radicalbt2G2log(1/α)T/summationdisplay
t=1∥θt−θ⋆∥2


E=10/uniondisplay
i=1Ei
Lemma B.1. EventEhappens with probability at least 1−11α.
Proof.Sinceηx,t∼N(0,σ2
X), from Theorem H.10, w.p at least 1−α/T:
∥ηx,t∥≤σ/radicalbig
2 log(2T/α)
Using the union bound over all t∈[T]to get that event E1happens w.p at least 1−α. Now repeat the same
argument with ηy,t∼N(0,σ2)andζt∼N(0,∆2)to get the high probability bound for E2andE3.
ForE4, we can use Theorem H.7 to get w.p at least 1−αfor everyt∈[T]:
/vextenddouble/vextenddoubleηx,tηT
x,t−Σ/vextenddouble/vextenddouble
op≤2DC(d+ log(2T/α))∥Σ∥op
= 2DCσ2(d+ log(2T/α))
for an universal constant C > 0.
E5:It’seasytoseethat {/summationtextN
t=1(zt−⟨ηx,t+ζt,θt⟩)⟨xt,θt−θ⋆⟩}∞
N=0isamartingalesequenceand zt−⟨ηx,t+ζt,θt⟩
is/radicalbig
R2+σ2+ (σ2+ ∆2)∥θt∥2−subgaussian. Then using corollary H.12,w.p at least 1−αwe have:
|N/summationdisplay
t=1(zt−⟨ηx,t+ζt,θt⟩)⟨xt,θt−θ⋆⟩|≤/parenleftigg
R+2(D+ 1)/radicalbig
2 log(1.25/δ)
ϵ+D∆/parenrightigg/radicaltp/radicalvertex/radicalvertex/radicalbt2/parenleftigg
1 +N/summationdisplay
t=1⟨xt,θt−θ⋆⟩2/parenrightigg
×/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtlog
/radicalig
1 +/summationtextN
t=1⟨xt,θt−θ⋆⟩2
α

E6:Similarly, we can apply corollary H.12 to get E6. W.p at least 1−α:
|N/summationdisplay
t=1zt⟨ηx,t+ζt,θt−θ⋆⟩|≤/parenleftigg
R+2/radicalbig
2 log(1.25/δ)
ϵ/parenrightigg/radicaltp/radicalvertex/radicalvertex/radicalbt2/parenleftigg
1 +N/summationdisplay
t=1⟨ηx,t+ζt,θt−θ⋆⟩2/parenrightigg
×/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtlog
/radicalig
1 +/summationtextN
t=1⟨ηx,t+ζt,θt−θ⋆⟩2
α

E7:/braceleftig/summationtextN
t=1(θ⋆−θt)TζtηT
x,t(θ⋆−θt)/bracerightig∞
N=0is a martingale sequence and (θ⋆−θt)Tζtis∆∥θt−θ⋆∥−subgaussian.
Using corollary H.12, w.p 1−αwe get:
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
t=1(θ⋆−θt)TζtηT
x,t(θ⋆−θt)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤∆∥θt−θ⋆∥/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbt2/parenleftigg
1 +N/summationdisplay
t=1⟨ηx,t,θ⋆−θt⟩2/parenrightigg
log
/radicalig
1 +/summationtextN
t=1⟨ηx,t,θ⋆−θt⟩2
α

19Under review as submission to TMLR
E8:Both/braceleftig/summationtextN
t=1θT
tζtηT
x,t(θ⋆−θt)/bracerightig∞
N=0and/braceleftig/summationtextN
t=1(θ⋆−θt)TζtηT
x,tθt/bracerightig∞
N=0are martingale sequences. Thus
we can again apply corollary H.12 w.p at least 1−2α:
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
t=1θT
tζtηT
x,t(θ⋆−θt)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
t=1(θ⋆−θt)TζtηT
x,tθt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤∆∥θt∥/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbt2/parenleftigg
1 +N/summationdisplay
t=1⟨ηx,t,θ⋆−θt⟩2/parenrightigg
log
/radicalig
1 +/summationtextN
t=1⟨ηx,t,θ⋆−θt⟩2
α

+σ∥θt∥/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbt2/parenleftigg
1 +N/summationdisplay
t=1⟨ζt,θ⋆−θt⟩2/parenrightigg
log
/radicalig
1 +/summationtextN
t=1⟨ζt,θ⋆−θt⟩2
α

E9:We have w.p at least 1−α,∥(θt−θ⋆)Tηx,tηT
x,t(θt+θ⋆)∥ ≤ 2σ2∥θt−θ⋆∥∥θt+θ⋆∥log(2T/α)and
∥(θt−θ⋆)TΣ(θt+θ⋆)∥≤2σ2∥θt−θ⋆∥∥θt+θ⋆∥log(2T/α). Thus:
∥(θ⋆−θt)T(ηx,tηT
x,t−Σ)(θt+θ⋆)∥2≤2∥(θ⋆−θt)Tηx,tηT
x,t(θt+θ⋆)∥2+ 2∥(θ⋆−θt)TΣ(θt+θ⋆)∥2
≤16σ4log2(2T/α)∥θ⋆−θt∥2∥θ⋆+θt∥2
≤16(D+ 1)2σ4log2(2T/α)∥θ⋆−θt∥2
LetXt= (θt−θ⋆)T(ηx,tηT
x,t−Σ)(θt+θ⋆)⇒E[Xt|ηx,1,θ1,...,ηx,t−1,xt−1] = 0. Thus,Xtis a martingale dif-
ference sequence and Xtis/parenleftbig
4(D+ 1)σ2log(2T/α)∥θ⋆−θt∥,8(D+ 1)σ2log(2T/α)∥θ⋆−θt∥/parenrightbig
sub-exponential
by Proposition H.1. Now applying Theorem H.2, w.p at least 1−2αwe get
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleN/summationdisplay
t=1(θ⋆−θt)T(ηx,tηT
x,t−Σ)(θt+θ⋆)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤5/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalvertex/radicalbtN/summationdisplay
t=116(D+ 1)2σ4log2/parenleftbigg2T
α/parenrightbigg
∥θ⋆−θt∥2log
16
α
log

/radicaltp/radicalvertex/radicalvertex/radicalbtN/summationdisplay
t=116(D+ 1)2σ4log2/parenleftbig2T
α/parenrightbig
∥θ⋆−θt∥2
ν2

1
+ 2
2

+ 23 max(ν,max
t4(D+ 1)σ2log/parenleftbigg2T
α/parenrightbigg
∥θ⋆−θt∥) log
224
α/bracketleftigg
log/parenleftigg
2 max(ν,maxt4(D+ 1)σ2log/parenleftbig2T
α/parenrightbig
∥θ⋆−θt∥)
ν/parenrightigg
+ 2/bracketrightigg2

= 20σ2(D+ 1) log(2T/α)/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalvertex/radicalbtN/summationdisplay
t=1∥θ⋆−θt∥2log
16
α
log
e2
/radicaltp/radicalvertex/radicalvertex/radicalbtN/summationdisplay
t=116(D+ 1)2σ4log2/parenleftbig2T
α/parenrightbig
∥θ⋆−θt∥2
ν2

1

2

+ 23 max(ν,max
t4(D+ 1)σ2log/parenleftbigg2T
α/parenrightbigg
∥θ⋆−θt∥) log
224
α/bracketleftigg
log/parenleftigg
2e2max(ν,maxt4(D+ 1)σ2log/parenleftbig2T
α/parenrightbig
∥θ⋆−θt∥)
ν/parenrightigg/bracketrightigg2

for some arbitrary constant ν≥0and[x]1:= max(1,x).
E10:Since{⟨∇L(θt)−∇lt(θt),θt−θ⋆⟩}T
t=1is a martingale difference sequence and −2G∥θt−θ⋆∥≤⟨∇L(θt)−
∇lt(θt),θt−θ⋆⟩≤2G∥θt−θ⋆∥, applying Azuma-Hoeffding inequality (Theorem H.16), we have w.p at least
1−α:
T/summationdisplay
t=1⟨∇L(θt)−∇lt(θt),θt−θ⋆⟩≤2/radicaltp/radicalvertex/radicalvertex/radicalbt2G2log(1/α)T/summationdisplay
t=1∥θt−θ⋆∥2
Now we can use the union bound over all events to conclude the result.
20Under review as submission to TMLR
C Maler Optimizer (Wang et al., 2020a)
We denote our loss as lt(θt)andL(θt) =E[lt(θt)]. We have the following lemma on the Lipschitz constant of
lt(θt)under the good event E.
Corollary C.1. Under the eventEwe have:
max
t∥∇lt(θt)∥≤G
whereG= 2/radicalbig
2 log(2T/α)(2Dσ+2σ+D∆+∆)+2 log(2T/α)(2D∆2+4D∆σ+2∆σ+2Dσ2+2σ2+DCσ2)+
2D+ 2.
Proof.We have:
∇lt(θt) = 2˜xt(⟨˜xt,θt⟩−˜yt)−2Σθt
= 2(xt+ζt+ηx,t))(⟨xt+ζt+ηx,t,θt⟩−˜yt)−2Σθt
Then,
∥∇lt(θt)∥≤2/parenleftbig
∥xt∥2∥θt∥+ 2∥xt∥∥ηx,t∥∥θt∥+∥xt∥|yt|+∥xt∥∥ηy,t∥+ 2∥xt∥∥ζt∥∥θt∥+∥ζt∥2∥θt∥+ 2∥ζt∥∥ηt∥∥θt∥
+∥ζt∥|yt|+∥ζt∥∥ηy,t∥+∥ηx,t∥2∥θt∥+∥ηx,t∥∥ηy,t∥+∥ηx,t∥|yt|+∥(ηx,tηT
x,t−Σ)θt∥/parenrightbig
Thus, under the event E:
max
t∥∇lt(θt)∥≤G
Using the result of the above Corollary we have ∥∇lt(θt)∥≤Gand the domainDofθtis bounded by 2Dfor
allt∈[T]. To run Maler, we need to define the surrogate loss as follows:
sr
t(θ) =−η(θt−θ)Tgt+η2G2∥θt−θ∥2
lr
t(θ) =−η(θt−θ)Tgt+η2(θ−θt)TgtgT
t(θ−θt)
ct(θ) =−ηc(θt−θ)Tgt+ (2ηcGD)2
We present the Maler algorithm in Algorithm 5:
Theorem C.2. AssumingL(θt) =E[lt(θt)]isµ−strongly convex and maxt,t′∥θt−θ′
t∥ ≤ 2D. Maler
(Algorithm 5) under the event Eguarantees:
T/summationdisplay
t=1L(θt)−L(θ⋆)≤/parenleftbigg
40GD+18G2
µ/parenrightbigg/parenleftbigg
2ln/parenleftbigg√
3/parenleftbigg1
2log2T+ 3/parenrightbigg/parenrightbigg
+ 1 + logT/parenrightbigg
+32G2log(1/α)
µ
≤O/parenleftbigg/parenleftbiggG2
µ+GD/parenrightbigg
logT+G2
µlog(1/α)/parenrightbigg
Furthermore, we have:
T/summationdisplay
t=1∥θt−θ⋆∥2≤/parenleftbigg80GD
µ+36G2
µ2/parenrightbigg/parenleftbigg
2ln/parenleftbigg√
3/parenleftbigg1
2log2T+ 3/parenrightbigg/parenrightbigg
+ 1 + logT/parenrightbigg
+64G2log(1/α)
µ2
≤O/parenleftbigg/parenleftbiggG2
µ2+GD
µ/parenrightbigg
logT+G2
µ2log(1/α)/parenrightbigg
21Under review as submission to TMLR
Algorithm 5 Maler
1:Input:Learning rate ηc,η1,η2,..., prior weights πc
1,πη1,s
1,πη2,s
1,...andπη1,l
1,πη2,l
1,...
2:fort= 1,...,Tdo
3:Get predictions θc
tfrom Algorithm 6, θη,ℓ
tfrom Algorithm 7, and θη,s
tfrom Algorithm 8 for all η.
4:Playθt=πc
tηcθc
t+/summationtext
ηπη,s
tηθη,s
t+πη,ℓ
tηθη,ℓ
t
πc
tηc+/summationtext
ηπη,s
tη+πη,ℓ
tη
5:Observe gradient gtand send it to the experts
6:Update weights:
πc
t+1=πc
te−ct(θc
t)
ϕt
πη,s
t+1=πη,s
te−sη
t(θη,s
t)
ϕt
πη,ℓ
t+1=πη,ℓ
te−lη
t(θη,ℓ
t)
ϕt
where:
ϕt=πc
te−ct(θc
t)+/summationdisplay
ηπη,s
te−sη
t(θη,s
t)+πη,ℓ
te−lη
t(θη,ℓ
t)
7:end for
Algorithm 6 Convex Expert Algorithm
1:Initializeθc
1= 0
2:fort= 1,...,Tdo
3:Sendθc
tto Algorithm 5
4:Receive gradient gtfrom Algorithm 5
5:Updateθc
t+1= ΠId
D/parenleftig
θc
t−2D
ηcG√
t∇ct(θc
t)/parenrightig
6:end for
Proof.Our analysis follows the analysis of Maler which in turn follows the analysis of Metagrad. We have:
T/summationdisplay
t=1L(θt)−L(θ⋆)≤T/summationdisplay
t=1⟨∇L(θt),θt−θ⋆⟩−µ
2∥θt−θ⋆∥2
=T/summationdisplay
t=1⟨∇lt(θt),θt−θ⋆⟩−µ
2∥θt−θ⋆∥2+⟨∇L(θt)−∇lt(θt),θt−θ⋆⟩
=T/summationdisplay
t=1⟨gt,θt−θ⋆⟩−µ
2∥θt−θ⋆∥2+⟨∇L(θt)−∇lt(θt),θt−θ⋆⟩+⟨∇lt(θt)−gt,θt−θ⋆⟩
Under eventE,gt=∇lt(θt):
=T/summationdisplay
t=1⟨gt,θt−θ⋆⟩−µ
2∥θt−θ⋆∥2+⟨∇L(θt)−∇lt(θt),θt−θ⋆⟩
=/summationtextT
t=1−sr
t(θ⋆) +η2G2∥θ⋆−θt∥2
η−µ
2∥θt−θ⋆∥2+⟨∇L(θt)−∇lt(θt),θt−θ⋆⟩
Using Lemma H.6, we get
≤/parenleftbigg
10GD+9G2
2µ/parenrightbigg/parenleftbigg
2ln/parenleftbigg√
3/parenleftbigg1
2log2T+ 3/parenrightbigg/parenrightbigg
+ 1 + logT/parenrightbigg
+⟨∇L(θt)−∇lt(θt),θt−θ⋆⟩
22Under review as submission to TMLR
Algorithm 7 Exp-concave Expert Algorithm
1:Input:Learning rate η
2:θη,l
1= 0,α=1
2min{2
GℓD,1}whereGℓ=7
50D,Σ1=1
4α2D2Id
3:fort= 1,...,Tdo
4:Sendθη,ℓto Algorithm 5
5:Receive gradient gtfrom Algorithm 5
6:Update
Σt+1= Σt+∇lη
t/parenleftig
θη,l
t/parenrightig/parenleftig
∇lη
t/parenleftig
θη,l
t/parenrightig/parenrightigT
θη,l
t+1= ΠΣt+1
D/parenleftbigg
θη,l
t−1
αΣ−1
t+1∇lη
t/parenleftig
θη,l
t/parenrightig/parenrightbigg
7:end for
Algorithm 8 Strongly-convex Expert Algorithm
1:Input:Learning rate η
2:θη,s
1= 0
3:fort= 1,...,Tdo
4:Sendθη,sto Algorithm 5
5:Receive gradient gtfrom Algorithm 5
6:Update
θη,s
t+1= ΠId
D/parenleftbigg
θη,s
t−1
2η2G2t∇sη
t(θη,s
t)/parenrightbigg
7:end for
We have:
T/summationdisplay
t=1L(θt)−L(θ⋆)≤/parenleftbigg
20GD+9G2
2µ/parenrightbigg/parenleftbigg
2ln/parenleftbigg√
3/parenleftbigg1
2log2T+ 3/parenrightbigg/parenrightbigg
+ 1 + logT/parenrightbigg
+ 2/radicaltp/radicalvertex/radicalvertex/radicalbt2G2log(1/α)T/summationdisplay
t=1∥θt−θ⋆∥2
≤/parenleftbigg
20GD+9G2
2µ/parenrightbigg/parenleftbigg
2ln/parenleftbigg√
3/parenleftbigg1
2log2T+ 3/parenrightbigg/parenrightbigg
+ 1 + logT/parenrightbigg
+ 4/radicaltp/radicalvertex/radicalvertex/radicalbtG2
µlog(1/α)T/summationdisplay
t=1L(θt)−L(θ⋆)
Applying Proposition H.8:
T/summationdisplay
t=1L(θt)−L(θ⋆)≤/parenleftbigg
40GD+18G2
µ/parenrightbigg/parenleftbigg
2ln/parenleftbigg√
3/parenleftbigg1
2log2T+ 3/parenrightbigg/parenrightbigg
+ 1 + logT/parenrightbigg
+32G2log(1/α)
µ
Furthermore, from strong convexity, we have: ∥θt−θ⋆∥2≤2(L(θt)−L(θ⋆))
µ, thus:
T/summationdisplay
t=1∥θt−θ⋆∥2≤/parenleftbigg80GD
µ+36G2
µ2/parenrightbigg/parenleftbigg
2ln/parenleftbigg√
3/parenleftbigg1
2log2T+ 3/parenrightbigg/parenrightbigg
+ 1 + logT/parenrightbigg
+64G2log(1/α)
µ2
Theorem C.3. (Strongly convex regret) Assuming L(θ) =E[lt(θt)]isµ−SC and maxt,t′∥θt−θ′
t∥≤2D.
Then w.p at least 1−α, Maler under the event Eguarantees:
T/summationdisplay
t=1lt(θt)−lt(θ⋆)≤O/parenleftbigg/parenleftbiggG2
µ+GD/parenrightbigg
logT+G2
µlog(1/α)/parenrightbigg
23Under review as submission to TMLR
Proof.Let us define At=lt(θt)−lt(θ⋆)−(L(θt)−L(θ⋆)). It’s easy to see that E[At|A1,...,At−1] = 0and
E[At]≤∞. Thus the sequence {At}T
t=1is a Martingale difference sequence. Furthermore, we have
E[∥At∥2|A1,...,At] =E[∥lt(θt)−lt(θ⋆)−(L(θt)−L(θ⋆))∥2]
≤E[2∥lt(θt)−lt(θ⋆)∥2+ 2∥(L(θt)−L(θ⋆))∥2]
≤4G2∥θt−θ⋆∥2
where the last inequality comes from lipschitz assumption. Similarly, we also have ∥At∥≤2G∥θt−θ⋆∥. Thus
by Proposition H.1, Xtis(2G∥θt−θ⋆∥,4G∥θt−θ⋆∥)sub-exponential. Now applying Theorem H.2, for some
arbitrary constant ν≥0, we have:
∥T/summationdisplay
t=1lt(θt)−lt(θ⋆)−(L(θt)−L(θ⋆))∥≤5/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalvertex/radicalbt4G2T/summationdisplay
t=1∥θt−θ⋆∥2log
16
α
log

/radicaltp/radicalvertex/radicalvertex/radicalbt4G2T/summationdisplay
t=1∥θt−θ⋆∥2/ν2

1
+ 2
2

+ 23 max(ν,max
i≤tbi) log/parenleftigg
224
α/bracketleftbigg
log/parenleftbigg2 max(ν,maxi≤tbi)
ν/parenrightbigg
+ 2/bracketrightbigg2/parenrightigg
where [x]1=max(1,x). Now let M1= 23 max(ν,maxi≤tbi)log/parenleftbigg
224
α/bracketleftig
log/parenleftig
2 max(ν,maxi≤tbi)
ν/parenrightig
+ 2/bracketrightig2/parenrightbigg
and
denoteRegretSC
T=/summationtextT
t=1L(θt)−L(θ⋆)we get:
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleT/summationdisplay
t=1lt(θt)−lt(θ⋆)−(L(θt)−L(θ⋆))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤5/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalvertex/radicalbt8G2RegretSC
T
µlog
16
α
log

/radicaligg
8G2RegretSC
T
µν2

1
+ 2
2
+M1
Thus w.p at least 1−2α
T/summationdisplay
t=1lt(θt)−lt(θ⋆)≤RegretSC
T+ 5/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalvertex/radicalbt8G2RegretSC
T
µlog
16
α
log

/radicaligg
8G2RegretSC
T
µν2

1
+ 2
2
+M1
Plug inRegretSC
Tfrom Theorem C.2:
T/summationdisplay
t=1lt(θt)−lt(θ⋆)≤O/parenleftbigg/parenleftbiggG2
µ+GD/parenrightbigg
logT+G2
µlog(1/α)/parenrightbigg
D Proofs of section 3
D.1 Proofs of general results
Lemma 3.3. We define ˜VN−1=/summationtextN−1
t=1˜xt˜xT
t,˜uN−1=/summationtextN−1
t=1⟨θt,˜xt⟩˜xt(θtis the prediction of the online
learner), and ˆθN=˜V−1
N−1˜uN−1. Assuming∥θt∥≤D, then under event E, the true parameter θ⋆lies in the
set:
CN−1=/braceleftig
θ∈Rd:∥θ−ˆθN∥2
˜VN−1≤MN+KN/bracerightig
24Under review as submission to TMLR
for anyN≥1and
KN=γD∆2log(T/α)/radicaltp/radicalvertex/radicalvertex/radicalbtNN/summationdisplay
t=1∥θt−θ⋆∥2+γ/parenleftigg
R+D/radicalbig
log(1/δ)
ϵ+D∆/parenrightigg2
log(T/α)
+γ/parenleftbigglog(1/δ)
ϵ2+ ∆2/parenrightbigg
log(T/α)N/summationdisplay
t=1∥θt−θ⋆∥2
for a sufficient large constant γ >0.
Proof.We have:
Mn≥N/summationdisplay
t=1(⟨˜xt,θt⟩−˜yt)2−∥θt∥2
Σ−(⟨˜xt,θ⋆⟩−˜yt)2+∥θ⋆∥2
Σ
=N/summationdisplay
t=1(⟨xt,θt−θ⋆⟩+⟨ηx,t,θt⟩+⟨ζt,θt⟩−rt−ηy,t)2−∥θt∥2
Σ−(⟨ηx,t,θ⋆⟩+⟨ζt,θ⋆⟩−rt−ηy,t)2+∥θ⋆∥2
Σ
Letzt=rt+ηy,t:
MN≥N/summationdisplay
t=1(⟨xt,θt−θ⋆⟩)2+ 2⟨xt,θt−θ⋆⟩⟨ηx,t,θt⟩+ (⟨ηx,t,θt⟩)2+ 2 (⟨ζt,θt⟩−zt) (⟨xt,θt−θ⋆⟩+⟨ηx,t,θt⟩)
+⟨ζt,θt⟩2−2zt⟨ζt,θt⟩+z2
t−∥θt∥2
Σ−(⟨ηx,t,θ⋆⟩)2−2(⟨ζt,θ⋆⟩−zt)⟨ηx,t,θ⋆⟩−⟨ζt,θ⋆⟩2
+ 2zt⟨ζt,θ⋆⟩−z2
t+∥θ⋆∥2
Σ
=N/summationdisplay
t=1(⟨xt,θt−θ⋆⟩)2+ 2(⟨ηx,t+ζt,θt⟩−zt)⟨xt,θt−θ⋆⟩−2zt⟨ηx,t,θt−θ⋆⟩+⟨ζt,θt⟩2−⟨ζt,θ⋆⟩2
+ 2(⟨ζt,θt⟩⟨ηx,t,θt⟩−⟨ζt,θ⋆⟩⟨ηx,t,θ⋆⟩)−2zt⟨ζt,θt−θ⋆⟩+θT
t(ηx,tηT
x,t−Σ)θt−(θ⋆)T(ηx,tηT
x,t−Σ)θ⋆
=N/summationdisplay
t=1(⟨xt,θt−θ⋆⟩)2+ 2(⟨ηx,t+ζt,θt⟩−zt)⟨xt,θt−θ⋆⟩−2zt⟨ηx,t,θt−θ⋆⟩+⟨ζt,θt⟩2−⟨ζt,θ⋆⟩2
+ 2(⟨ζt,θt⟩⟨ηx,t,θt⟩−⟨ζt,θ⋆⟩⟨ηx,t,θ⋆⟩)−2zt⟨ζt,θt−θ⋆⟩+ (θt−θ⋆)T(ηx,tηT
x,t−Σ)(θt+θ⋆)
Rearrange the terms:
N/summationdisplay
t=1(⟨xt,θt−θ⋆⟩)2≤MNN/summationdisplay
t=1−2(⟨ηx,t+ζt,θt⟩−zt)⟨xt,θt−θ⋆⟩/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
At+2zt⟨ηx,t+ζt,θt−θ⋆⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Bt−⟨ζt,θt⟩2+⟨ζt,θ⋆⟩2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Ct
−2(⟨ζt,θt⟩⟨ηx,t,θt⟩−⟨ζt,θ⋆⟩⟨ηx,t,θ⋆⟩)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Dt−(θt−θ⋆)T(ηx,tηT
x,t−Σ)(θt+θ⋆)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Et
Bounding/summationtextN
t=1At:Using the result from E5:
|N/summationdisplay
t=1(zt−⟨ηx,t+ζt,θt⟩)⟨xt,θt−θ⋆⟩|≤/parenleftigg
R+2(D+ 1)/radicalbig
2 log(1.25/δ)
ϵ+D∆/parenrightigg/radicaltp/radicalvertex/radicalvertex/radicalbt2/parenleftigg
1 +N/summationdisplay
t=1⟨xt,θt−θ⋆⟩2/parenrightigg
×/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtlog
/radicalig
1 +/summationtextN
t=1⟨xt,θt−θ⋆⟩2
α

≤˜O
/parenleftigg
R+D/radicalbig
log(1/δ)
ϵ+D∆/parenrightigg/radicaltp/radicalvertex/radicalvertex/radicalbtlog(N/α)N/summationdisplay
t=1⟨xt,θt−θ⋆⟩2

25Under review as submission to TMLR
Bounding/summationtextN
t=1Bt:FromE6,
|N/summationdisplay
t=1zt⟨ηx,t+ζt,θt−θ⋆⟩|≤/parenleftigg
R+2/radicalbig
2 log(1.25/δ)
ϵ/parenrightigg/radicaltp/radicalvertex/radicalvertex/radicalbt2/parenleftigg
1 +N/summationdisplay
t=1⟨ηx,t+ζt,θt−θ⋆⟩2/parenrightigg
×/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtlog
/radicalig
1 +/summationtextN
t=1⟨ηx,t+ζt,θt−θ⋆⟩2
α

Since we’re under good event E:
≤/parenleftigg
R+2/radicalbig
2 log(1.25/δ)
ϵ/parenrightigg/radicaltp/radicalvertex/radicalvertex/radicalbt2/parenleftigg
1 + 2 log(2T/α)N/summationdisplay
t=1(σ2+ ∆2)∥θt−θ⋆∥2/parenrightigg
×/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtlog
/radicalig
1 + 2 log(2T/α)/summationtextN
t=1(σ2+ ∆2)∥θt−θ⋆∥2
α

≤O
/parenleftigg
R+/radicalbig
log(1/δ)
ϵ/parenrightigg/radicaltp/radicalvertex/radicalvertex/radicalbtlog2(2T/α)N/summationdisplay
t=1/parenleftbigglog(1/δ)
ϵ2+ ∆2/parenrightbigg
∥θt−θ⋆∥2

Bounding/summationtextN
t=1Ct:We have:
N/summationdisplay
t=1⟨ζt,θ⋆⟩2−⟨ζt,θt⟩2=N/summationdisplay
t=1⟨ζt,θ⋆−θt⟩⟨ζt,θt+θ⋆⟩
Using∥θt+θ⋆∥≤D+ 1,
N/summationdisplay
t=1⟨ζt,θ⋆⟩2−⟨ζt,θt⟩2≤∆(D+ 1)/radicalbig
2 log(2T/α)N/summationdisplay
t=1|⟨ζt,θ⋆−θt⟩|
≤∆(D+ 1)/radicalbig
2 log(2T/α)/radicaltp/radicalvertex/radicalvertex/radicalbtNN/summationdisplay
t=1⟨ζt,θ⋆−θt⟩2
≤∆(D+ 1)/radicalbig
2 log(2T/α)/radicaltp/radicalvertex/radicalvertex/radicalbt2Nlog(2T/α)∆2N/summationdisplay
t=1∥θt−θ⋆∥2
≤O
D∆2log(T/α)/radicaltp/radicalvertex/radicalvertex/radicalbtNN/summationdisplay
t=1∥θt−θ⋆∥2

Bounding/summationtextN
t=1Dt:We have
N/summationdisplay
t=1⟨ζt,θ⋆⟩⟨ηx,t,θ⋆⟩−⟨ζt,θt⟩⟨ηx,t,θt⟩= (θ⋆−θt)TζtηT
x,t(θ⋆−θt) +θT
tζtηT
x,tθ⋆+ (θ⋆)TζtηT
x,tθt−2θT
tζtηT
x,tθt
= (θ⋆−θt)TζtηT
x,t(θ⋆−θt) +θT
tζtηT
x,t(θ⋆−θt) + (θ⋆−θt)TζtηT
x,tθt
26Under review as submission to TMLR
FromE7:
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
t=1(θ⋆−θt)TζtηT
x,t(θ⋆−θt)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤∆∥θt−θ⋆∥/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbt2/parenleftigg
1 +N/summationdisplay
t=1⟨ηx,t,θ⋆−θt⟩2/parenrightigg
log
/radicalig
1 +/summationtextN
t=1⟨ηx,t,θ⋆−θt⟩2
α

≤O
D∆/radicaltp/radicalvertex/radicalvertex/radicalbtlog(T/α)N/summationdisplay
t=1σ2∥θt−θ⋆∥2

=O
D∆/radicalbig
log(1/δ)
ϵ/radicaltp/radicalvertex/radicalvertex/radicalbtlog(T/α)N/summationdisplay
t=1∥θt−θ⋆∥2

SinceE8⊂E:
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
t=1θT
tζtηT
x,t(θ⋆−θt)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
t=1(θ⋆−θt)TζtηT
x,tθt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤∆∥θt∥/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbt2/parenleftigg
1 +N/summationdisplay
t=1⟨ηx,t,θ⋆−θt⟩2/parenrightigg
log
/radicalig
1 +/summationtextN
t=1⟨ηx,t,θ⋆−θt⟩2
α

+σ∥θt∥/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbt2/parenleftigg
1 +N/summationdisplay
t=1⟨ζt,θ⋆−θt⟩2/parenrightigg
log
/radicalig
1 +/summationtextN
t=1⟨ζt,θ⋆−θt⟩2
α

≤O
D∆/radicaltp/radicalvertex/radicalvertex/radicalbtN/summationdisplay
t=1⟨ηx,t,θ⋆−θt⟩2log (T/α) +Dσ/radicaltp/radicalvertex/radicalvertex/radicalbtN/summationdisplay
t=1⟨ζt,θ⋆−θt⟩2log (T/α)

Plugging in σ:
≤O
D∆/radicalbig
log(1/δ)
ϵ/radicaltp/radicalvertex/radicalvertex/radicalbtlog(T/α)N/summationdisplay
t=1∥θ⋆−θt∥2

Overall,
N/summationdisplay
t=1⟨ζt,θ⋆⟩⟨ηx,t,θ⋆⟩−⟨ζt,θt⟩⟨ηx,t,θt⟩≤O
D∆/radicalbig
log(1/δ)
ϵ/radicaltp/radicalvertex/radicalvertex/radicalbtlog(T/α)N/summationdisplay
t=1∥θt−θ⋆∥2

Bounding/summationtextN
t=1Et:FromE9:
27Under review as submission to TMLR
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleN/summationdisplay
t=1(θ⋆−θt)T(ηx,tηT
x,t−Σ)(θt+θ⋆)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤20σ2(D+ 1) log(2T/α)/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalvertex/radicalbtN/summationdisplay
t=1∥θ⋆−θt∥2log
16
α
log
e2
/radicaltp/radicalvertex/radicalvertex/radicalbtN/summationdisplay
t=116(D+ 1)2σ4log2/parenleftbig2T
α/parenrightbig
∥θ⋆−θt∥2
ν2

1

2

+ 23 max(ν,max
t4(D+ 1)σ2log/parenleftbigg2T
α/parenrightbigg
∥θ⋆−θt∥) log
224
α/bracketleftigg
log/parenleftigg
2e2max(ν,maxt4(D+ 1)σ2log/parenleftbig2T
α/parenrightbig
∥θ⋆−θt∥)
ν/parenrightigg/bracketrightigg2

≤˜O
Dlog(1/δ) log(T/α)
ϵ2/radicaltp/radicalvertex/radicalvertex/radicalbtN/summationdisplay
t=1∥θt−θ⋆∥2

Now combine all the bounds we get:
N/summationdisplay
t=1(⟨xt,θt−θ⋆⟩)2≤MN+O
/parenleftigg
R+D/radicalbig
log(1/δ)
ϵ+D∆/parenrightigg/radicaltp/radicalvertex/radicalvertex/radicalbtlog(T/α)N/summationdisplay
t=1⟨xt,θt−θ⋆⟩2

+O
/parenleftigg
R+/radicalbig
log(1/δ)
ϵ/parenrightigg/radicaltp/radicalvertex/radicalvertex/radicalbtlog2(T/α)N/summationdisplay
t=1/parenleftbigglog(1/δ)
ϵ2+ ∆2/parenrightbigg
∥θt−θ⋆∥2

+O
D∆2log(T/α)/radicaltp/radicalvertex/radicalvertex/radicalbtNN/summationdisplay
t=1∥θt−θ⋆∥2

+O
D∆/radicalbig
log(1/δ)
ϵ/radicaltp/radicalvertex/radicalvertex/radicalbtlog(T/α)N/summationdisplay
t=1∥θt−θ⋆∥2
+˜O
Dlog(1/δ) log(T/α)
ϵ2/radicaltp/radicalvertex/radicalvertex/radicalbtN/summationdisplay
t=1∥θt−θ⋆∥2

≤MN+O
/parenleftigg
R+D/radicalbig
log(1/δ)
ϵ+D∆/parenrightigg/radicaltp/radicalvertex/radicalvertex/radicalbtlog(T/α)N/summationdisplay
t=1⟨xt,θt−θ⋆⟩2

+O
/parenleftigg
R+/radicalbig
log(1/δ)
ϵ/parenrightigg/radicaltp/radicalvertex/radicalvertex/radicalbtlog2(T/α)N/summationdisplay
t=1/parenleftbigglog(1/δ)
ϵ2+ ∆2/parenrightbigg
∥θt−θ⋆∥2

+O
D∆2log(T/α)/radicaltp/radicalvertex/radicalvertex/radicalbtNN/summationdisplay
t=1∥θt−θ⋆∥2
+˜O
Dlog(1/δ) log(T/α)
ϵ2/radicaltp/radicalvertex/radicalvertex/radicalbtN/summationdisplay
t=1∥θt−θ⋆∥2

Applying Proposition H.9 and Proposition H.8 to get:
N/summationdisplay
t=1(⟨xt,θt−θ⋆⟩)2≤O
MN+/parenleftigg
R+/radicalbig
log(1/δ)
ϵ/parenrightigg/radicaltp/radicalvertex/radicalvertex/radicalbtlog2(T/α)N/summationdisplay
t=1/parenleftbigglog(1/δ)
ϵ2+ ∆2/parenrightbigg
∥θt−θ⋆∥2

+O
D∆2log(T/α)/radicaltp/radicalvertex/radicalvertex/radicalbtNN/summationdisplay
t=1∥θt−θ⋆∥2
+˜O
Dlog(1/δ) log(T/α)
ϵ2/radicaltp/radicalvertex/radicalvertex/radicalbtN/summationdisplay
t=1∥θt−θ⋆∥2

+O
/parenleftigg
R+D/radicalbig
log(1/δ)
ϵ+D∆/parenrightigg2
log(T/α)

28Under review as submission to TMLR
Further:
N/summationdisplay
t=1(⟨˜xt,θt−θ⋆⟩)2=N/summationdisplay
t=1(⟨xt,θt−θ⋆⟩+⟨ηx,t+ζt,θt−θ⋆⟩)2
≤N/summationdisplay
t=12(⟨xt,θt−θ⋆⟩)2+ 2(⟨ηx,t+ζt,θt−θ⋆⟩)2
≤N/summationdisplay
t=12(⟨xt,θt−θ⋆⟩)2+ 4/parenleftbigg8 log(1.25/δ)
ϵ2+ ∆2/parenrightbigg
log(2T/α)N/summationdisplay
t=1∥θt−θ⋆∥2
≤O
D∆2log(T/α)/radicaltp/radicalvertex/radicalvertex/radicalbtNN/summationdisplay
t=1∥θt−θ⋆∥2+/parenleftigg
R+D/radicalbig
log(1/δ)
ϵ+D∆/parenrightigg2
log(T/α)

+O/parenleftigg/parenleftbigglog(1/δ)
ϵ2+ ∆2/parenrightbigg
log(T/α)N/summationdisplay
t=1∥θt−θ⋆∥2/parenrightigg
+MN
Letγbe a sufficiently large positive constant and
KN=γD∆2log(T/α)/radicaltp/radicalvertex/radicalvertex/radicalbtNN/summationdisplay
t=1∥θt−θ⋆∥2+γ/parenleftigg
R+D/radicalbig
log(1/δ)
ϵ+D∆/parenrightigg2
log(T/α)
+γ/parenleftbigglog(1/δ)
ϵ2+ ∆2/parenrightbigg
log(T/α)N/summationdisplay
t=1∥θt−θ⋆∥2
Then,
N/summationdisplay
t=1(⟨˜xt,θt−θ⋆⟩)2≤MN+KN
Let us denote the set Cn−1as an ellipsoid underlying the covariance matrix ˜VN−1=I+/summationtextN−1
t=1˜xt˜xT
tand
centering at
ˆθN= arg min
θ∈Rd/parenleftigg
∥θ∥2
2+N−1/summationdisplay
t=1(⟨˜xt,θt−θ⟩)2/parenrightigg
=˜V−1
N−1/parenleftiggN−1/summationdisplay
t=1⟨θt,˜xt⟩˜xt/parenrightigg
=˜V−1
N−1˜uN−1
We can thus express the ellipsoid as:
ˆCN−1=/braceleftigg
θ∈Rd: (θ−ˆθn)T˜Vn−1(θ−ˆθN) +∥ˆθN∥2
2+N−1/summationdisplay
t=1(⟨˜xt,θt−ˆθN⟩)2≤MN+KN/bracerightigg
The ellipsoid is contained in a larger ellipsoid
ˆCN−1⊆CN−1=/braceleftig
θ∈Rd:∥θ−ˆθN∥2
˜VN−1≤MN+KN/bracerightig
Thus,θ⋆lies inCN−1with high probability.
29Under review as submission to TMLR
Theorem 3.4. (Utility guarantee) Recall that MTis the regret of our online learner (see equation (4)), and
KTis as defined in Lemma 3.3. Under event E, the regret of Algorithm 1 is:
RegretT≤˜O/parenleftigg/radicalbig
MT+KT/radicaligg
2Tdlog/parenleftbigg
1 +T
d/parenrightbigg/parenrightigg
Proof.First we bound the instantaneous regret using (xt,˜θt) = arg max(x,θ)∈Dt×Ct−1⟨x,θ⟩:
⟨x⋆,θ⋆⟩−⟨xt,θ⋆⟩≤⟨xt,˜θt⟩−⟨xt,θ⋆⟩
=⟨xt,˜θt−θ⋆⟩
=⟨xt,˜θt−ˆθt⟩+⟨xt,ˆθt−θ⋆⟩
≤∥xt∥˜V−1
t−1∥˜θt−ˆθt∥˜Vt−1+∥xt∥˜V−1
t−1∥ˆθt−θ⋆∥˜Vt−1
≤˜O/parenleftig/radicalbig
Mt−1+Kt−1/parenrightig
×∥xt∥˜V−1
t−1
Use the assumption |⟨x,θ⋆⟩|≤1and sum over all t to get the regret:
RegretT=T/summationdisplay
t=1⟨x⋆−xt,θ⋆⟩
≤T/summationdisplay
t=1˜O/parenleftig/radicalbig
MT+KT/parenrightig
min{1,∥xt∥˜V−1
t−1}
≤˜O/parenleftig/radicalbig
MT+KT/parenrightig
×/radicaltp/radicalvertex/radicalvertex/radicalbtT×T/summationdisplay
t=1min{1,∥xt∥2
˜V−1
t−1}
Applying Lemma H.3, we get:
RegretT≤˜O/parenleftigg/radicalbig
MT+KT/radicaligg
2Tdlog/parenleftbigg
1 +T
d/parenrightbigg/parenrightigg
E Anytime version of Algorithm 1
Since the Bandit Combiner Algorithm (Algorithm 2) requires the base learner to have anytime guarantee, we
can not directly use Algorithm 1 for all the base learners since the regret of Algorithm 1 depends on the
total iterations Twhen ∆2̸= 0. Fortunately, we can easily convert any non-anytime algorithm to an anytime
algorithm using the doubling trick. We will describe the anytime version of Algorithm 1 in Algorithm 9: We
Algorithm 9 Anytime Private (Contextual) Online LinUCB
Input:Privacy parameters ϵ,δ, failure parameter α, covariance matrix Σ =E[ηx,tηT
x,t], minimum eigenvalue
λminofE[xtxT
t], domain diameter D, universal constant C, powerk.
form= 0,1,2,...do
Initialize Algorithm 1 with ϵ,δ,α,Σ,λmin,D,C, and set ¯λ= ∆2
m=1
(2m)k.
Run Algorithm 1 for [2m,2m+1−1]rounds.
Reset Algorithm 1.
end for
have the following guarantee:
30Under review as submission to TMLR
Theorem E.1. LetTbe the maximum number of iterations and P > 0be an absolute constant. For any
t∈[T], under eventE, Algorithm 9 guarantees:
Regrett≤Plog3/2(t)×/parenleftigg
√
dt3/4+√
dt5/8
ϵ/parenrightigg
Proof.In Algorithm 9, in each round m, each copy of Algorithm 1 runs at most Tm= 2miterations. From
Corollary 3.6, we have the regret of Algorithm 1 after Tmiterations is upper bounded by:
O/parenleftigg/parenleftigg/parenleftigg
R+D/radicalbig
log(1/δ)
ϵ/parenrightigg
/radicalbig
log(Tm/α) +/radicalig
D∆2mlog(Tm/α)/radicalbig
TmHmlogTm+logTm/radicalbig
Hmlog(1/δ)
ϵ/parenrightigg
×/radicalbig
dTmlog(Tm/d)/parenrightig
whereHm=G2
(λmin+∆2
m)2+GD
λmin+∆2
m. Thus, there exists absolute constant K > 0such that the regret of
Algorithm 1 of round m∈[⌈log2t⌉]is:
Regretm≤Klog3/2(Tm)×/parenleftbigg/radicalig
∆2m/radicalbig
Hm√
dT3/4
m+√Hm
ϵ/radicalbig
dTm/parenrightbigg
DenoteM=⌊log2t⌋, then the total regret of Algorithm 9 at any iteration t∈[T]is:
Regrett≤M/summationdisplay
m=0Regretm
≤M/summationdisplay
m=0Klog3/2(Tm)×/parenleftbigg/radicalig
∆2m/radicalbig
Hm√
dT3/4
m+√Hm
ϵ/radicalbig
dTm/parenrightbigg
Since log(·)is monotonically increasing and Tm≤t,
≤Klog3/2(t)M/summationdisplay
m=0/parenleftbigg/radicalig
∆2m/radicalbig
Hm√
dT3/4
m+√Hm
ϵ/radicalbig
dTm/parenrightbigg
We have for every m∈[M]that:
/radicalig
∆2m/radicalbig
Hm≤/radicaltp/radicalvertex/radicalvertex/radicalbt∆2m/parenleftigg
G
λmin+ ∆2m+√
GD/radicalbig
λmin+ ∆2m/parenrightigg
≤/radicaltp/radicalvertex/radicalvertex/radicalbt∆2m/parenleftigg
G
∆2m+√
GD/radicalbig
∆2m/parenrightigg
=/radicalig
G+λm√
GD
Sinceλm≤1,
≤/radicalig
G+√
GD
31Under review as submission to TMLR
Sinceλmdecreases as mincreases,Hm≤HM≤Ht=G2
(λmin+∆2
t)2+GD
λmin+∆2
twhere ∆2
t=1
tk, then:
Regrett≤Klog3/2(t)/parenleftigg/radicalig
d(G+√
GD)/parenleftiggM/summationdisplay
m=0T3/4
m/parenrightigg
+√dHt
ϵ/parenleftiggM/summationdisplay
m=1/radicalbig
Tm/parenrightigg/parenrightigg
≤Klog3/2(t)
/radicalig
d(G+√
GD)
⌊log2t⌋/summationdisplay
m=0(23/4)m)
+√dHt
ϵ
⌊log2t⌋/summationdisplay
m=0(√
2)m


=Klog3/2(t)/parenleftbigg/radicalig
d(G+√
GD)(23/4)⌊log2t⌋+1−1
23/4−1+√dHt
ϵ(21/2)⌊log2t⌋+1−1
21/2−1/parenrightbigg
≤Klog3/2(t)/parenleftbigg
3/radicalig
d(G+√
GD)t3/4+4√dHt
ϵ√
t/parenrightbigg
Now we can choose an absolute constant Psufficiently large to conclude the result.
F Proofs of section 4
The result of Algorithm 2 is simply a straightforward application of the result of Algorithm 1 in (Cutkosky
et al., 2020). The only difference is the concentration result in Lemma 8 (Cutkosky et al., 2020) since the
reward ˜ytis not bounded by [−1,1]due to the added noise to ensure privacy. We will prove a modified result
of Lemma 8 below. For ease of analysis, let us redefine some notations used in (Cutkosky et al., 2020). Let
r(a,c):=⟨θ⋆,ϕ(c,a)⟩andˆr(c,a,η ):=⟨θ⋆,ϕ(c,a)⟩+η. We also use the shorthand notations ri
t=r(ai
t,ct)and
ˆrt=ˆr(ai
t,ct,ηt)to denote the random reward and the expected reward that the base learner ireceives at
round t. Thus Eηt,ct[ˆrt] =rtandˆrt= ˜yt. We have the following lemma:
Lemma F.1. For all rounds t∈[T]and base learner i∈[M], the following inequalities hold
−k/radicaligg
T(i,t)(1 +σ2) log/parenleftbiggT3MlogT(i,t)(1 +σ2)
α/parenrightbigg
≤T(i,t)/summationdisplay
τ=1ˆri
t−ri
t≤k/radicaligg
T(i,t)(1 +σ2) log/parenleftbiggT3MlogT(i,t)(1 +σ2)
α/parenrightbigg
w.p at least 1−α/(T3M)
Proof.The proof is based on the proof of Lemma B.2 in (Pacchiano et al., 2023). Let Ftbe
the sigma-field induced by all variables up to round tbefore the reward is revealed, i.e., Ft=
σ/parenleftbig
{al,cl,il,ηl}l∈[t−1]∪{at,ct,it,ηt}/parenrightbig
. Sinceηy,tis mean zero Gaussian noise for all t∈[T],{Xt=
ˆri
t−ri
t}T(i,t)
t=1is a martingale difference sequence w.r.t Ft. Using the terminology and definition in (Howard
et al., 2021), the process ST(i,t)=/summationtextT(i,t)
t=1Xtis a sub-ψNwith variance process VT(i,t)=T(i,t)(σ2+ 1). Thus
using the boundary choice in Eq.11 of (Howard et al., 2021), we get:
ST(i,t)≤1.7/radicalig
VT(i,t)(log log(2VT(i,t))) + 0.72 log(5.2/α)
= 1.7/radicalbig
T(i,t)(σ2+ 1)(log log(2 T(i,t)(σ2+ 1)) + 0.72 log(5.2/α)
Applying the same argument to −ST(i,t)gives that:
|ST(i,t)|≤3∨1.7/radicalbig
T(i,t)(σ2+ 1)(log log(2 T(i,t)(σ2+ 1)) + 0.72 log(10.4/α)
Now, setα=α/(T3M), and pick the absolute constant ksufficiently large to conclude the proof.
The rest of the analysis follows the analysis in (Cutkosky et al., 2020). We have the following general regret
guarantee for UCB combiner algorithm.
32Under review as submission to TMLR
Theorem F.2. (Corollary 2 (Cutkosky et al., 2020)) Suppose jis the index of the best base learner and w.p
at least 1−α, we have/summationtextτ
t=1maxx∈Dt⟨θ⋆,x⟩−⟨θ⋆,xj
τ⟩≤Ljtαj. Further, suppose we are given M positive
real numbers η1,...,ηM. SetRivia:
Ri=LiTαi+(1−αi)1−αi
αi(1 +αi)1
αi
α1−αi
αi
iL1
αi
iTη1−αi
αi
i + 288 log(T3N/α)Tηi+/summationdisplay
k̸=i1
ηk
Then, w.p at least 1−3α, the regret of Algorithm 2 satisfies:
RegretT≤˜O
LjTαj+L1
aj
jTη1−αj
αj
j +Tηj+/summationdisplay
k̸=i1
ηk

Before we state the regret of Algorithm 2, let us first show that with our settings of λi, we are guaranteed to
have at least a base learner that has a regret bound that is the same up to some constant factor as the regret
bound of a base learner that is initialized with the correct value of λmin.
Lemma F.3. Letλ⋆
minbe the actual minimum eigenvalue of E[xtxT
t]andRegret⋆
Tbe the regret of an instance
of Algorithm 1 using the correct minimum eigenvalue. Let λi=2i−1
T1/8fori∈[M]be theλminof the base
learneriin Algorithm 2. If M=⌈1
8log2T⌉+ 1, then there exists at least a base learner iwith regret guarantee
Regreti
Tsuch that:
Regreti
T=KRegret⋆
T
for some constant K > 0.
Proof.Let us consider the case where λ⋆
min≥1
T1/8. Since∥xt∥≤1for allxt∈Dtandλ⋆
minis the minimum
eigenvalue of E[xtxT
t],1
T1/8≤λ⋆
min≤1. Thus,
20
T1/8≤λ⋆
min≤2(1/8 log2T+1−1)
T1/8
⇔λ1≤λ⋆
min≤λM
In other words, the actual minimum eigenvalue λ⋆
minis always within the range covered by our guess of λmin.
Ifλ⋆
min=λ1orλ⋆
min=λM, then the first statement of the lemma is true with K= 1. If this is not the case,
that means there exist a learner ksuch thatλk≤λ⋆
min≤λk+1. We have:
λ⋆
min
λk≤λk+1
λk= 2
Since the regret of any instance of Algorithm 2 that uses λ≤λ⋆
minas the input is ˜O/parenleftig√
dT
λ/parenrightig
, the regret of the
base learner kwithλkis at most a constant factor worse than Regret⋆
T.
Whenλ⋆
min<1
T1/8, since we also set the threshold ¯λ=1
T1/8, from Theorem E.1, we know that the first base
learner guarantees:
Regrett≤Plog3/2(T)×/parenleftigg
√
dT3/4+√
dT5/8
ϵ/parenrightigg
for a sufficiently large positive constant P. Since we also have R1
T≤˜O/parenleftig
log3/2(T)×/parenleftig√
dT3/4+√
dT5/8
ϵ/parenrightig/parenrightig
,
Regret1
T=O(Regret⋆
T).
We are now ready to state our guarantee for Algorithm 2.
33Under review as submission to TMLR
Theorem 4.1. (see Corollary 2 (Cutkosky et al., 2020)) Let η1=ϵT1/8
√
dT(Plog3/2(T)ϵT1/8+1)andηi=
ϵ
P′log3/2(T)√
dT,L1=Plog3/2(T)√
d/parenleftig
ϵT1/8+1
ϵT1/8/parenrightig
andLi=P′log3/2(T)√
d
ϵλifor positive constants PandP′,
α1=3
4andαi=1
2fori∈[2,M], and setRivia:
Ri=LiTαi+(1−αi)1−αi
αi(1 +αi)1
αi
α1−αi
αi
iL1
αi
iTη1−αi
αi
i + 288 log(T3N/α)Tηi+/summationdisplay
k̸=i1
ηk
for alli. Letjbe the index of the base learner with the smallest regret. If λmin≥1
T1/8, then w.p at least
1−3α, the regret of Algorithm 2 under event Esatisfies:
RegretT≤˜O/parenleftigg√
dT
ϵλ2
min/parenrightigg
If0≤λmin<1
T1/8, then w.p at least 1−3α, the regret of Algorithm 2 under event Esatisfies:
RegretT≤˜O/parenleftigg
√
dT5/6+√
dT17/24
ϵ/parenrightigg
Proof.Let us first prove the first statement of Theorem 4.1. If1
T1/8≤λmin, then from Lemma F.3, there
exists at least a base learner jsuch that,Regretj
T≤˜O/parenleftig√
dT
ϵλj/parenrightig
. Then, plug in ηi,Ci, andαiwe have
RegretT≤˜O
√
dT
ϵλj+(√
d)2
(ϵλj)2Tϵ√
dT+Tϵ√
dT+/summationdisplay
k̸={1,j}√
dT
ϵ+√
dT/parenleftbigg
1 +1
ϵT1/8/parenrightbigg

≤˜O/parenleftigg√
dT
ϵλ2
j/parenrightigg
=˜O/parenleftigg√
dT
ϵλ2
min/parenrightigg
Ifλmin<1
T1/8, then the regret of an instance of Algorithm 1 that uses the correct λminis
˜O/parenleftig√
dT3/4+√
dT5/8
ϵ/parenrightig
. From Lemma F.3, we know that the regret of the first base learner is also
˜O/parenleftig√
dT3/4+√
dT5/8
ϵ/parenrightig
. Thus,
RegretT≤˜O/parenleftigg
√
d/parenleftbiggϵT1/8+ 1
ϵT1/8/parenrightbigg
T3/4+/parenleftbigg√
d/parenleftbiggϵT1/8+ 1
ϵT1/8/parenrightbigg/parenrightbigg4/3
T/parenleftbiggϵT1/8
√
dT(ϵT1/8+ 1)/parenrightbigg1/3
+TϵT1/8
√
dT(ϵT1/8+ 1)+/summationdisplay
k̸=1√
dT
ϵ

≤˜O/parenleftigg
√
dT5/6+√
dT17/24
ϵ/parenrightigg
G Extra experiments details
In this section, we compute the empirical minimum eigenvalue of xtxT
twhere each x∈Dtis uniformly
sampled from a sphere as in Section 5. We compute this value over multiple settings of k∈{5,25,100}and
T∈{200,2000,20000}wherekis the number of arms and Tis the number of iterations. For all experiments,
the dimension of the arm d= 5and we compute the minimum eigenvalue using the non-private LinUCB. We
report the portion of training where the minimum eigenvalue is O(1)in the table below:
As we can see from Table 1, our experiments in Section 5 is in the favorable settings for Algorithm 1 with
high probability, especially when the number of iterations is significantly larger than the number of arms.
34Under review as submission to TMLR
T=200 T=2000 T=20000
k=5 99.5% 100% 100%
k=25 97% 97.7% 99.9%
k=100 88% 89.1% 99.8%
Table 1: Percentage of iterations where the minimum eigenvalue of1
M/summationtextM
m=1xt,mxT
t,m(M is the number of
repeated experiments, xt,mis the arm played at iteration tand sampled at experiment m) is O(1)for multiple
settings of k and T
H Technical results
Proposition H.1. (Proposition 17 in (Zhang & Cutkosky, 2022)) Suppose {Xt,Ft}is a Martingale difference
sequence such that E[X2
t|Ft]≤σ2
tand|Xt|≤btalmost everywhere for all t for some sequence of random
variable{σt,bt}such thatσt,btisFt−1−measurable. Then Xtis(σt,2bt)sub-exponential.
Theorem H.2. (Theorem 19 in (Zhang & Cutkosky, 2022)) Suppose that {Xt,Ft}is a vector-valued
martingale difference sequence such that E[∥Xt∥2|Ft−1]≤σ2
tand∥Xt∥≤btalmost everywhere for some
sequence{σt,bt}such thatσt,btisFt−1-measurable. Let ν≥0be an arbitrary constant. Then with probability
at least 1−α, for alltwe have:
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublet/summationdisplay
i=1Xi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤5/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalvertex/radicalbtt/summationdisplay
i=1σ2
ilog
16
α
log

/radicaltp/radicalvertex/radicalvertex/radicalbtt/summationdisplay
i=1σ2
i/ν2

1
+ 2
2

+ 23 max(ν,max
i≤tbi) log/parenleftigg
224
α/bracketleftbigg
log/parenleftbigg2 max(ν,maxi≤tbi)
ν/parenrightbigg
+ 2/bracketrightbigg2/parenrightigg
where [x]1= max(1,x).
Lemma H.3. (Lemma 11 in (Abbasi-Yadkori et al., 2012)) Let x1,...,xn∈Rdand letVt=I+/summationtextt
s=1xsxT
s,
then it holds that
T/summationdisplay
t=1min/braceleftig
1,∥xt∥2
V−1
t−1/bracerightig
≤2 log(det(VT))
Furthermore, if∥xt∥2≤Xfor alltthen
log(det(VT))≤dlog/parenleftbigg
1 +TX2
d/parenrightbigg
Lemma H.4. (Lemma 1 in (Wang et al., 2020a)) Define sr
t(x) =η(xt−x)Tgt+η2G2∥xt−x∥2. For every
grid pointη, we have:
T/summationdisplay
t=1sr
t(xt)−sr
t(xη,s
t)≤2ln/parenleftbigg√
3/parenleftbigg1
2log2T+ 3/parenrightbigg/parenrightbigg
Lemma H.5. (Lemma 2 in (Wang et al., 2020a)) Define sr
t(x) =η(xt−x)Tgt+η2G2∥xt−x∥2. For every
grid pointηand anyu∈Rd, we have:
T/summationdisplay
t=1sr
t(xη,s
t))−sη
t(u)≤1 + logT
35Under review as submission to TMLR
Lemma H.6. (Corollary 2 and Theorem 1 in (Wang et al., 2020a)) Suppose the loss function is G−Lipschitz
and the diameter of domain is bounded by D. Forµ−SC functions, the regret of Maler is upper bounded by
R(T)≤/parenleftbigg
10GD+9G2
2µ/parenrightbigg/parenleftbigg
2ln/parenleftbigg√
3/parenleftbigg1
2log2T+ 3/parenrightbigg/parenrightbigg
+ 1 + logT/parenrightbigg
≤O/parenleftbigg1
µlogT/parenrightbigg
For general convex loss, the regret of Maler is bounded by:
R(T)≤/parenleftbigg
2ln3 +3
2/parenrightbigg
GD√
T
Theorem H.7. (Exercise 4.7.3 - (Vershynin, 2018)) Let x1,...,xnbe an i.i.d sequence of σsub-gaussian
random vectors such that Σ =E[xixT
i]for alliand ˆΣn=1
n/summationtextn
i=1xixT
ibe the emprical covariance matrix.
Then there exists a universal constant C > 0such that for α∈(0,1), w.p at least 1−α
∥ˆΣn−Σ∥op≤Cmax/braceleftigg/radicalbigg
d+ log(2/α)
n,d+ log(2/α)
n/bracerightigg
∥Σ∥op
Proposition H.8. (Square-root trick (Abbasi-Yadkori et al., 2012)) Let a,b≥0. Ifz2≤a+bz, then
z≤b+√a.
Proposition H.9. (Logarithmic Trick (Abbasi-Yadkori et al., 2012)) Let c≥1,f >0,α∈(0,1/4]. Ifz≥1,
andz≤c+f/radicalbig
log(z/α)thenz≤c+f/radicalbigg
2 log/parenleftig
c+f
α/parenrightig
Theorem H.10. LetX∼N(µ,σ2I)whereµ∈Rdandσ2∈R. Then:
P[∥X−µ∥2>t]≤2 exp/parenleftbigg
−t2
2dσ2/parenrightbigg
Theorem H.11. (Self-normalized bound for martingales (Abbasi-Yadkori et al., 2012)) Let {Ft}∞
t=1be a
filtration. Let τbe a stopping time w.r.tthe filtration{Ft+1}∞
t=1i.e the event{τ≤t}belongs toFt+1. Let
{Zt}∞
t=1be a sequence of real-valued variables such that ZtisFt−measurable. Let {rt}∞
t=1be a sequence
of real-valued random variables such that rtisFt+1−measurable and is continuously R−sub-Gaussian. Let
V > 0be deterministic. Then, for any α>0, with probability at least 1−α
(/summationtextτ
t=1rtZt)2
V+/summationtextτ
t=1Z2
t≤2R2log/parenleftigg/radicalbig
V+/summationtextτ
t=1Z2
t
α√
V/parenrightigg
Corollary H.12. (Uniform Bound (Abbasi-Yadkori et al., 2012)) Under the same assumptions as in Theorem
H.11, for any α>0, w.p at least 1−α, for alln≥0,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
t=1rtZt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤R/radicaltp/radicalvertex/radicalvertex/radicalbt2/parenleftigg
1 +n/summationdisplay
t=1Z2
t/parenrightigg
log/parenleftigg/radicalbig
1 +/summationtextτ
t=1Z2
t
α/parenrightigg
Corollary H.13. ((Vershynin, 2018)) Let Abe ann×nsymmetric random matrix whose entries Aijon and
above the diagonal are independent, mean zero, sub-gaussian random variables. Then, for any t>0we have
∥A∥≤CK/parenleftbig√n+t/parenrightbig
with probability at least 1−4 exp(−t2). HereK= maxi,j∥Aij∥ψ2and∥X∥ψ2is defined as follows:
∥X∥ψ2:= inf/braceleftbig
t>0 :E/bracketleftbig
exp(X2/t2)/bracketrightbig
≤2/bracerightbig
Specifically, if X∼N(0,σ2), then:
∥X∥ψ2≤Cσ
for someC > 0.
36Under review as submission to TMLR
Theorem H.14. (Theorem 5 in (Gordon et al., 2006)) Let (xi)n
i=1be a sequence of real numbers and
ζ1,...,ζnbe random variables satisfying the following condition:
P[|ζi|≤t]≤αt∀i∈[n]
wheret≥0andα>0. Letp>0, then:
1
1 +pα−p/parenleftiggn/summationdisplay
i=11
|xi|/parenrightigg−p
≤E/bracketleftbigg
min
1≤i≤n|xiζi|p/bracketrightbigg
Lemma H.15. Ifζ∼N(0,σ2), then:
P[|ζ|≤t]≤√
2
σ√πt
Proof.Using the CDF of Gaussian random variable we have:
P[|ζ|≤t] = Φ/parenleftbiggt
σ/parenrightbigg
−Φ/parenleftbigg−t
σ/parenrightbigg
=1
2/bracketleftbigg
1 + erf/parenleftbiggt
σ√
2/parenrightbigg/bracketrightbigg
−1
2/bracketleftbigg
1 + erf/parenleftbigg−t
σ√
2/parenrightbigg/bracketrightbigg
Since erf(·)is an odd function:
= erf/parenleftbiggt
σ√
2/parenrightbigg
=2√π/integraldisplayt
σ√
2
0exp(−x2)dx
≤2t
σ√
2π
=t√
2
σ√π
Theorem H.16. (Azuma-Hoeffding Inequality) For a sequence of Martingale Difference Sequence random
variable{Dt}∞
t=1with respect to some other sequence of random variable {Xt}∞
t=1, if we have Dt∈[at,bt]
almost surely for some constants at,btandt= 1,2,...,T, then:
P/bracketleftiggT/summationdisplay
t=1Dt≥ϵ/bracketrightigg
≤exp/parenleftigg
−2ϵ2
/summationtextT
t=1(bt−at)2/parenrightigg
37