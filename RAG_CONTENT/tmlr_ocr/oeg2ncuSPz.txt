Under review as submission to TMLR
Approximation Rates and VC-Dimension Bounds for (P)ReLU
MLP Mixture of Experts
Anonymous authors
Paper under double-blind review
Abstract
Mixture-of-Experts (MoEs) can scale up beyond traditional deep learning models by employ-
ing a routing strategy in which each input is processed by a single “expert” deep learning
model. This strategy allows us to scale up the number of parameters defining the MoE
while maintaining sparse activation, i.e., MoEs only load a small number of their total
parameters into GPU VRAM for the forward pass depending on the input. In this paper,
we provide an approximation and learning-theoretic analysis of mixtures of expert MLPs
with (P)ReLU activation functions. We first prove that for every error level ε>0and every
Lipschitz function f: [0,1]n→R, one can construct a MoMLP model (a Mixture-of-Experts
comprising of (P)ReLU MLPs) which uniformly approximates ftoεaccuracy over [0,1]n,
while only requiring networks of O(ε−1)parameters to be loaded in memory. Additionally,
we show that MoMLPs can generalize since the entire MoMLP model has a (finite) VC
dimension of ˜O(Lmax{nL,JW}), if there are Lexperts and each expert has a depth and
width ofJandW, respectively.
1 Introduction
With the advent of large foundation models, scaling deep learning models beyond the capacity of a single
machine has become increasingly important. Mixture of Experts (MoE) models offer a solution to this
challenge through a sparse activation strategy. In MoEs, each input is first routedto one of many expertdeep
learning models and then processed by that expert. This approach allows MoEs to scale effectively while
maintaining a low or constant computational cost during the forward pass, as only a subset of the overall
model needs to be loaded into GPU video random-access memory (VRAM) for a given input. This has led to
MoEs such as Mixtral (Jiang et al., 2024), Gemini (Google, 2023), and several others, e.g. (Jacobs et al.,
1991; Jordan & Xu, 1995; Meila & Jordan, 2000; Shazeer et al., 2017; Guu et al., 2020; Lepikhin et al., 2021;
Fedus et al., 2022; Barham et al., 2022; Majid & Tudisco, 2024), to become a viable solution in scaling up
large language models (Radford et al., 2018; Brown et al., 2020). However, the analytical and statistical
foundations of MoEs in deep learning are less understood compared to their empirical investigations.
This paper adds to the theoretical understanding of this subject by studying MoEs whose experts are (small)
multilayer perceptrons (MLPs) with (P)ReLU activation function (MoMLPs). A key feature of MoEs is that
they can maintain a small/fixed computational cost during the forward pass, for any given input x∈[0,1]n,
even if the overall model complexity may be large. Our main result (Theorem 4.1) analyzes the complexity
of MoEs when uniformly approximating an arbitrary Lipschitz (Lebesgue) almost-everywhere continuously
differentiable function f: [0,1]n→Rmby an MoMLP with (P)ReLU activation function to any prespecified
errorε >0. We focus on the trade-off between the maximum number of parameters loaded into VRAM
by any expert model {ˆfl}ℓ
l=1while predicting from any given input, against the total number of experts
required to maintain that constant number of activated parameters in the forward pass. Summarized in
Table 1, our main result shows that a constant active complexity in the forward pass can be maintained
among all experts, but at the cost of an exponentially large number of locally-specialized experts {ˆf}ℓ
l=1
and regions of specialization {Cl}ℓ
l=1. Our complexity estimates are approximately optimal as they nearly
match the Vapnik-Chervonenkis (VC) lower bounds derived in Shen et al. (2021). That is, the uniform
approximation of an arbitrary such fon[0,1]n, with an an error of ε>0, requires at least Ω(ε−n/2)total
1Under review as submission to TMLR
Table 1: No. Parameters and VC-Dimension of MoMLP with no. experts-to-expert-complexity parameter
r∈(−∞,2
n]; performing an 0<ε≤1approximation of an α-Hölder function f: [0,1]n→R;n∈N. Whenr≥0
more model complexity is distributed across many “small experts”. When r < 0, fewer experts define the MoE
and, as a result, each expert MLP must depend on more parameters such that the entire MoE obtain an accurate
approximation of the target function. We also record the total number of parameters defining the MoMLP, including
those which are not loaded in the forward pass but can be stored offline.
Parameter Estimate
No. Parameters Per Expert O(max{1,ε−r})
No. Experts O/parenleftbig
max{1,ε2r/n−1/α}/parenrightbig
Parameters MoMLP (Offline) O/parenleftbig
max{1,ε−r}max{1,ε2r/n−1/α}/parenrightbig
VC Dimension MoE ˜O/parenleftbig
max{1,ε2r/n−1/α}max{ε2r/n−1/α,ε−r}/parenrightbig
model parameters (Yarotsky, 2018; Kratsios & Papon, 2022; Shen et al., 2021; 2022b). It is here where MoEs
have an advantage since not all of their parameters need to be loaded into active memory for any given input;
thus, MoEs are genuinely sparsely activated.
It is important to note that the results recorded in Table 1 represent the worst-case scenario, meaning
they pertain to the most challenging target function within the class of α-Hölder functions. Approximating
such a function necessarily requires a large number of experts in the MoE to achieve the desired accuracy,
similar to how very large MLPs would be theoretically required. Nevertheless, in practice, one typically does
not encounter worst-case functions. MoEs can still perform well even with a small number of experts in
the mixture model and relatively few parameters per expert. It is worth noting, however, that analyzing
non-worst-case scenarios is an interesting research question, separate from the worst-case analysis addressed
in this manuscript.
From the statistical learning perspective, a key property of MoEs (e.g. the top MoMLP model) is that they
can maintain a given level of activation in the forward pass while the entire MoE model can maintain a finite
VC-dimension (Theorem 4.2). This is key, for instance, in classification applications, as the fundamental
theorem of PAC learning (see e.g. (Shalev-Shwartz & Ben-David, 2014, Theorem 6.7) or the results of Blumer
et al. (1989); Hanneke (2016); Brukhim et al. (2022)) implies that such a machine learning model generalizes
beyond the training data if and only if it has finite VC dimension.
Summary of Contributions Table 1 summarizes our main contributions, both to the approximation
theory and learning theory of MoE models, in the context of the toy mixture of (P)ReLU MLP models. All
results illustrate the trade-off between individual (expert) complexity and the complexity shared across the
set of experts when uniformly approximating a target α-Hölder function; 0<α≤1.
Ourapproximation theorem (Theorem 4.1) records the number of parameters required to perform a uniform
approximation on a high-dimensional Euclidean space on [0,1]n. The first result juxtaposes the complexity
ofeach expert (PReLU MLP) against the total number of experts required to achieve a given level of
approximation accuracy. The user controls the number of experts vs. the complexity of each expert using a
hyperparameter r∈R. As shown in Table 1. Small values of r<0encode the “few large experts regime”,
whereas large values of 0≤rcapture the “many small experts regime”.
Ourstatistical learning guarantee result (Theorem 4.2) yields a bound on the VC dimension of the entire
class of MoEs with just enough approximation power to perform this approximation. As summarized in
Table 1, the result quantitatively shows the degradation of model generalization as the number of experts
increases; i.e. rbecomes large.
Observe that, setting r=2
nin Table 1, yields for ReLU MLPs derived in Yarotsky (2017); the optimality
of which is expressed in terms of VC dimension in Shen et al. (2022b). Likewise, the VC dimension of the
MoMLP is roughly equal to that of ReLU MLPs computed in Bartlett et al. (2019).
2Under review as submission to TMLR
2 Related Work
Deep Learning Models with Few Parameters in Active Memory. Deep learning models with
highly oscillatory “super-expressive” activation functions (Yarotsky & Zhevnerchuk, 2020; Yarotsky, 2021;
Zhang et al., 2022) are known to achieve dimension-free approximation rates, thus effectively require a
(relatively) feasible number of parameters to be loaded into VRAM during the forward pass. As we will see in
Proposition 4.4, many of these models have an infinite VC dimension even when they are restricted to having a
bounded depth and width; see Jiao et al. (2023, Lemma 3.1) for ReLU-Sin-2x-networks. Their unbounded VC
dimension implies that the classifiers implemented by these models do not generalize on classification problems.
Thus, the real performance of these models does not need to achieve the approximation-theoretic optima
since they can only learn from a finite number of noisy training instances. Alternatively, a feasible number of
parameters in deep learning models with standard activation functions may be guaranteed by restricting classes
of well-behaved target functions such as Barron functions (Barron, 1993), functions of mixed-smoothness
(Suzuki, 2018), highly smooth functions (Mhaskar, 1996; Galimberti et al., 2022; Gonon et al., 2023; Opschoor
et al., 2022), convex functions (Bach, 2017), functions with compositional structure (Mhaskar et al., 2017), or
other restricted classes. However, there are generally no guarantees that a target function encountered in
practice has the necessary structure for these desired approximation theorems to hold.
Universal Approximation in Deep Learning. Several results have recently considered the expression
power of deep learning models. These include universal approximation guarantees for MLPs (Cybenko, 1989;
Hornik et al., 1989; Lu et al., 2017; Suzuki, 2018; Yarotsky, 2017; 2018; Voigtlaender & Petersen, 2019;
Bolcskei et al., 2019; Gühring et al., 2020; De Ryck et al., 2021; DeVore et al., 2021; Daubechies et al., 2022;
Kratsios & Zamanlooy, 2022; Zhang et al., 2022; Opschoor et al., 2022; Zamanlooy & Kratsios, 2022; Shen
et al., 2022b; Cuchiero et al., 2023; Voigtlaender, 2023; Benth et al., 2023; Mao & Zhou, 2023; Yang & Zhou,
2024), CNNs (Petersen & Voigtlaender, 2020; Yarotsky, 2022), spiking neural networks (Neuman & Petersen,
2024), residual neural networks (Tabuada & Gharesifard, 2021), transformers (Yun et al., 2019; 2020; Kratsios
& Papon, 2022; Fang et al., 2023), random neural networks (Gonon et al., 2023), recurrent neural network
models (Grigoryeva & Ortega, 2018; Gonon & Ortega, 2021; Hutter et al., 2022; Galimberti et al., 2022; hoon
Song et al., 2023), and several others. In each these cases, one typically considers the expressivity of a single
“expert” model and not a mixture thereof. Our analysis can be customized to any of these settings to yield
analogues of Theorem 4.1.
Foundations of MoEs. MoE models have been heavily studied since their inception. Most results have
focused on identifying the correct expert to best route any given input to (Teicher, 1960; 1963; Wang et al.,
1996), the construction of effective routing mechanisms (Wang et al., 2017) selection (Wang et al., 1996),
MoE training (Larochelle et al., 2009; Akbari et al., 2024), statistical convergence guarantees for classes
of MoEs (Chen, 1995; Ho et al., 2022), robustness guarantees for such models (Puigcerver et al., 2022),
amongst several other types of guarantees. However, to our knowledge, there are no available approximation
guarantees for MoE or VC-dimension bounds for deep-learning-based MoEs. Thus, our results would be
adding to the approximation theoretic foundations of MoE models as well as to the statistical foundations of
deep-learning-based MoEs.
Prototypes and Partitioning. Each region in our learned partition of the input space is associated
with arepresentative point therein called a prototype . Prototypes (also called landmarks ) are routinely
used in image classification (Mensink et al., 2012), few-shot learning (Snell et al., 2017; Cao et al., 2020),
dimensionality reduction (Law et al., 2019), in complex networks (Keller-Ressel & Nargang, 2023), and
geometric deep learning (Ghadimi Atigh et al., 2021) to tractably encode massive structures. They are
also standard in classical clustering algorithms such as K-medoids or K-means, wherein the part associated
with each medoid (resp. centroid) defines a Voronoi cell or Voronoi region (Voronoi, 1908). Moreover,
while partitioning is commonly employed in deep learning for various purposes, such as proving universal
approximation theorems (Yarotsky, 2017; Lu et al., 2021b; Gühring & Raslan, 2021) or facilitating clustering-
based learning (Zamanlooy & Kratsios, 2022; Trask et al., 2022; Ali & Nouy, 2023; Srivastava et al., 2022),
existing approaches typically involve loading the entire model into VRAM. Our approach, however, differs by
relying on a learned partition of the input space, where each part is associated with a distinct small neural
3Under review as submission to TMLR
network. Importantly, the complete set of networks forming the MoMLPs does not need to be simultaneously
loaded into VRAM during training or inference.
Paper Overview. Our paper is organized as follows. Section 3 contains preliminary notation, definitions,
and mathematical background required for the formulation of our main results. Section 4 contains our main
approximation (Theorem 4.1) and learning theoretic (Theorem 4.2) guarantees. Section 5 dives into the details
of whyMoEs can achieve arbitrary precision while maintaining a feasible active computational complexity by
explaining the derivation of our main approximation theorem; the details of which are relegated to Appendix B.
A technical version (Theorem 5.3) of our main approximation guarantee is then presented, which allows for
the approximation of continuous functions of arbitrarily low regularity and for the organization of the experts
defining the MoMLP via a decision tree implementing the indicator function to a Voronoi diagram of [9,1]d.
Section 6 contains technical derivations of our main results as well as experimental elucidation of the benefit
of MoEs, and specifically the toy MoMLP model.
3 Preliminaries
We standardize our notation, define the necessary mathematical formalisms to state our main results and
define our toy MoE Model.
Notation We use the following notation: for any f,g:R→R, we writef∈O(g)if there exist x0∈R
andM≥0such that for each x≥x0we have|f(x)|≤Mg(x0). Similarly, we write f∈Ω(g)to denote the
relationg∈O(f). The ReLU activation function is given for every x∈RbyReLU (x) =max{x,0}. For
eachn∈N+andC⊆Rn, theindicator function ICofCis defined by: for each x∈RnsetIC(x) = 1if
x∈Cand is 0otherwise.
3.1 Background
Multi-Layer Perceptrons We will consider MLPs with trainable PReLU activation functions.
Definition 3.1 (Trainable PReLU) .We define the trainable PReLU activation function σ:R×R→Rfor
each input x∈Rand each parameter γ∈Ras follows:
σγ(x)def.=σ(x,γ)def.=/braceleftigg
x ifx≥0,
γxotherwise.
PReLU generalizes ReLU since ReLU (x) =σ0(x), and it makes the hyperparameter γof a Leaky ReLU
learnable. We will often be applying our trainable activation functions component-wise. For positive integers
n,m, we denote the set of n×mmatrices by Rn×m. More precisely, we mean the following operation defined
for anyN∈N,¯γ∈RNwithithentry denoted as ¯γi, andx∈RN, by
σ¯γ•xdef.=/parenleftbig
σ¯γi(xi)/parenrightbigN
i=1.
We now define the class of multilayer perceptions (MLPs), with trainable activation functions. Fix J∈Nand
a multi-index [d]def.=(d0,...,dJ+1), and letP([d]) =/summationtextJ
j=0dj(dj+1+ 2). We identify any vector θ∈RP([d])
with
θ↔/parenleftbig
A(j),b(j),¯γ(j)/parenrightbigJ
j=0and (A(j),b(j),¯γ(j))∈Rdj+1×dj×Rdj×Rdj. (1)
We recursively define the representation function of a [d]-dimensional network by
RP([d])×Rd0∋(θ,x)∝⇕⊣√∫⊔≀→ˆfθ(x)def.=A(J)x(J)+b(J),
x(j+1)def.=σ¯γ(j)•(A(j)x(j)+b(j))forj= 0,...,J−1
x(0)def.=x.(2)
We denote byNNσ
[d]the family of [d]-dimensional multilayer perceptrons (MLPs),{ˆfθ}θ∈RP([d])described by
equation 2. The subset of NNσ
[d]consisting of networks ˆfθwith each ¯γ(j)
i= (1,0)in equation 2 is denoted by
4Under review as submission to TMLR
NNReLU
[d]and consists of the familiar deep ReLU MLPs. The set of ReLU MLPs with depthJandwidth
Wis denoted byNNσ
J,W:n,m=∪[d]NNσ
[d], where the union is taken over all multi-indices [d] = [d0,...,d ˜J]
withn=d0,m=dJ+1,d0,...,dJ+1≤W, and ˜J≤J.
VC dimension LetFbe a set of functions from a subset X⊆Rnto{0,1}; i.e. binary classifiers on X.
The setFshatters (in the classical sense) a k-point subset{xi}k
i=1⊆XifFcan represent every possible set
of labels on those k-points; i.e. if #{(ˆf(xi))k
i=1∈{0,1}k:ˆf∈F} = 2k.
As in Shen et al. (2022b), we extend the definition of shattering from binary classifiers to real-valued functions
as follows. LetFbe a set of functions from [0,1]ntoR. The setFis said to shatter a k-point set{xi}k
i=1⊆X
if
{I(0,∞)◦f:f∈F} (3)
shatters it, i.e. if all possible classifiers on {xi}k
i=1are implementable in the sense that {I(0,∞)◦f:f∈F} =
{0,1}{xi}k
i=1; hereI(0,∞)(t) = 1ift>0and equals to 0otherwise. Denoted by VC(F), theVC dimension of
Fis the cardinality of the largest k-point subset shattered by F. Ifkis unbounded, then we say that Fhas
an infinite VC dimension (over X). One can show, see Bartlett et al. (2019), that the VC-dimension of any
suchFis roughly the same as the pseudo-dimension of Pollard (1990) for a small modification of F.
VC dimension measures the richness of a class of functions. For example, in Harvey et al. (2017, Theorem
1), the authors showed that the set of MLPs with ReLUactivation function with L∈N+layers, width and
W∈N+satisfyingW >O (L)>C2, whereC≥640, satisfies
VC(NNσ
W,L)∈Ω/parenleftig
WL log2(W/L )/parenrightig
. (4)
Nearly matching upper bounds are in Bartlett et al. (2019).
Load Network Attributed
to Near est Pr ototype to x 
Figure 1: 1) The distance from each input
xto all prototypes p1,...,p 8(ℓ= 8) is
queried. 2) The network ( ˆf2in the figure) as-
signed to the nearest prototype ( p2), is loaded
onto the GPU and used for prediction.Definition: Our Toy Mixture of Experts Model We
study the following toy MoE model, where each expert (P)ReLU
MLP specializes in a distinct region of the input domain [0,1]n.
Informally, these regions C1,...,Cℓcorrespond to the sets
of closest points (Voronoi cells) from a finite set of proto-
types/landmarks p1,...,pℓin[0,1]n, as illustrated in Figure 1.
Associated to each region Ciis a single expert MLP ˆfiwith
(P)ReLU activation function responsible for approximating the
target function only thereon . Here, the sparse gating proce-
dure which routesany given input x∈[0,1]nto the expert
corresponding to the nearest prototype piis implemented by
a (finite) routing treeT= (V,E)whose nodes Vare points in
[0,1]nand leaves (terminal nodes) are the points p1,...,pℓ.
We now formally define the classes of MoMLPs.
Definition 3.2 (MoMLPs) .LetJ,W,L,n∈Nand fix an acti-
vation function σ∈C(R). The set of MoMLPs with at-most L
leaves, depth J, and width W, denoted byNPσ
J,W,L :n,m, con-
sists of all functions ˆf:Rn→Rmsatisfying ˆf=/summationtextL
i=1fiICi
where for f1,...,fL∈ NNPReLU
J,W, anddistinct prototypes
p1,...,pL∈Rn; inducing the Voronoi cells {Ci}L
i=1where
Cidef.=˜Ci\/uniondisplay
j<i˜Cj,(5)
where fori= 1,...,Lthe (non-disjoint) cells are
˜Cidef.=/braceleftbig
x∈[0,1]n:∥x−pi∥= min
j∈{1,...,L}∥x−pj∥/bracerightbig
.(6)
5Under review as submission to TMLR
The Routing Trees The structure in any MoMLP is any tree with root node Rnand leaves given by the
pairs{(pi,fi)}L
i=1or equivalently{(Ci,fi)}L
i=1. The purpose of any such tree is simply to efficiently route an
inputx∈Rnto one of the L“leaf networks” (the experts) f1,...,fLusingO(log(L))queries; to identify
which Voronoi cells {Ci}L
i=1the pointxis contained in. We leave the precise set of queries executed by the
routing treeTabstract so as to allow for maximal design freedom. However, we do ask that Tencodes a
decision tree executing a sequence of queries at each node along a branch which implements the following
function, routing any x∈[0,1]dto its nearest cell in the disjoint Voronoi cells {˜Cl}; i.e.Timplements
Rd∋x∝⇕⊣√∫⊔≀→L/summationdisplay
l=1lIx∈Cl∈[L]. (7)
Example 1 (Toy Implementation of equation 7 on Real Line) .Setd= 1, consider the prototypes
{1/8,3/8,5/8,7/8}, and queries q1,1(x)def.=I(|x−1/4|<|x−3/4|),q2,1(x)def.=I(|x−1/8|<|x−3/8|), and
q2,2def.=I(|x−5/8|<|x−7/8|). The decision tree in Algorithm 1 implements equation 7.
Remark 3.3 (Partitioning in equation 5 in classical computer science) .The partitioning technique used to
define equation 5 is standard, see e.g. Krauthgamer et al. (2005, Proof of Lemma 1.7; page 846). It is employed
to ensure disjointness of the Voronoi cells; this guarantees that no input is assigned to multiple prototypes.
To keep notation tidy, we use NPσ
J,W,L(resp.Nσ
W,L), to abbreviateNPσ
J,W,L :n,m(resp.Nσ
W,L:n,m) whenever
nandmare clear from the context.
4 Main Results
Algorithm 1: Routing Tree from Exam-
ple 1.
1ifq1,1= 1then
2ifq2,1= 1then
3 Index←1
4else
5 Index←2
6end if
7else
8ifq2,2= 1then
9 Index←2
10else
11 Index←3
12end if
13end ifWe first present our main approximation theoretic guarantee,
which gives complexity estimates for mixtures of MLPs with
trainable PReLU activation functions when uniformly approx-
imating arbitrary locally-Hölder function on the closed unit
ball of Rn, defined by Bn(0,1)def.={x∈Rn:∥x∥≤1}.
Our rates depend on a “number of experts-to-expert complex-
ity trade-off parameter” r∈Rwhich determines how fast the
overall MoE complexity scales, in terms of the number of ex-
perts and the complexity of each expert, as the approximation
error becomes small. Setting r<0implies that more model
complexity will be loaded into each expert MLP and there will
be fewer experts defining the MoE. In contrast, setting r>0
loads less complexity in each expert MLP at the cost of more
experts in the MoE. In particular, when r= 0, each expert
will have constant complexity even when the approximation
error becomes arbitrarily small.
Theorem 4.1 (Trade-Off: No. Expert vs. Expert Complexity) .Suppose that σsatisfies Definition 3.1. Fix an
“number of experts-to-expert complexity trade-off parameter” r∈R. For every α-Hölder map f:Bn(0,1)→Rm
with 0<α≤1and each approximation error ε>0, there is a p∈N+, a binary treeTdef.=(V,E)with leaves
Ldef.={(vi,θi)}L
i=1⊆Bn(0,1)×Rpand a family of MLPs with (P)ReLU activation function {ˆfθi}L
i=1defined
bypparameters and mapping RntoRmsatisfying:
max
x∈Bn(0,1)min
(vi,θi)∈L∥x−vi∥∈Θ/parenleftbig
ε1/α−2r/n/parenrightbig
and for each x∈Bn(0,1)andi= 1,...,L, if∥x−vi∥<δthen
∥f(x)−fθi(x)∥<ε.
The depth and width of each ˆfθiand the number of leaves, height, and number of nodes required to build the
binary tree are all recorded in Table 1.
6Under review as submission to TMLR
A more general version of Theorem 4.1 is presented below as Theorem 5.3. In this version of our main
approximation theorem, the target function can be any arbitrary continuous function defined on a non-empty
compact subset of Rn, and the routing tree can be ν-ary for any natural number ν≥2.
Next, we demonstrate that the MoMLP model can generalize and generate functions that are PAC-learnable,
thanks to its finite VC dimension. This property, however, breaks down in MLP models with super-expressive
activation functions.
Theorem 4.2 (VC-Dimension Bounds for MoMLPs - MoMLPs Can Generalize) .LetJ,W,L,n∈N+. Then
VC/parenleftbig
NNPReLU
J,W,L :n,1/parenrightbig
is of
O/parenleftbig
Llog(L)2max{nLlog(L),JW2log(JW)}/parenrightbig
(8)
In particular, VC(NPReLU
J,W,L :n,1)<∞.
4.1 Discussion
Trade-off between Number of Experts and Expert Complexity. Our results suggest that, theoretically,
successful MoE models may not need each expert to be highly overparameterized if there are enough experts.
This hypothesis is ablated experimentally in Section 6 in the context of irregular function approximation in
low-dimension space; which is equivalent to high-dimensional regular function approximation (see Appendix C
for a discussion on this later point).
Pruning. Additionally, one might consider the option of pruning a sizable model, conceivably trained on a
GPU with a larger VRAM, for utilization on a smaller GPU during inference as an alternative to our method.
Nevertheless, in frameworks like PyTorch, pruning does not result in a reduction of operations, acceleration,
or diminished VRAM memory usage. Instead, pruning only masks the original model weights with zeros.
The reduction in model size occurs only when saved in offline memory in sparse mode, which, in any case, is
not a significant concern.
Logarithmic number of queries via trees. For many prototypes, as in our main guarantee, the MoMLPs
onlyneedtoevaluatethedistancebetweenthegiveninputandalogarithmicnumberofprototypes—specifically,
one for each level in the tree—when using deep binary trees to hierarchically refine the Voronoi cells. Thus, a
given machine never processes the exponential number of prototypes, and only ν⌈logν(K)⌉prototypes are
ever queried for any given input; when trees are ν-ary (as in Theorem 5.3), and where Kdenotes the number
of prototypes. Since we consider that prototypes are queried separately and before loading MoMLPs, we do
not take them into account when counting the number of learnable parameters. Moreover, the size of our
prototypes is negligible in our experiments.
Functions which can be approximated by MoEs with small numbers of experts. Our theoretical
analysis adopts a worst-case perspective, focusing on the most difficult-to-approximate function within a
givenα-Hölder class. Consequently, the number of experts in the MoE can become very large. However, this
is often not the case in practice or in our experiments presented in Section 6. We anticipate that favorable
approximation guarantees can be achieved with only a small number of expert ReLU MLPs, each having a
limited number of non-zero parameters. Similar to the approximation theory for ReLU MLPs, we expect
this to hold when the target function is sufficiently smooth, such as functions belonging to certain Besov
spaces (Suzuki, 2018; Gühring & Raslan, 2021; Siegel & Xu, 2022), or when the function’s structure aligns
well with that of deep neural networks (Mhaskar et al., 2017; Cheridito et al., 2021b). Exploring these
scenarios represents an interesting research direction that complements the worst-case analysis presented in
this manuscript.
4.2 Application: Controlling The Complexity in VRAM maintaining a Finite VC Dimension
Super-Expressive Activation Functions Have Infinite VC-Dimension. We complement the main
result of Bartlett et al. (2019) by demonstrating that the class of unstable MLPs (Shen et al., 2022a) possesses
infinite VC dimension, even when they have finite depth and width. Thus, while they may serve as a gold
standard from the perspective of approximation theory, they should not be considered a benchmark gold
standard from the viewpoint of learning theory.
7Under review as submission to TMLR
Table 2: VC Dimension of the MoMLPs, ReLU MLP, and MLP model with Super-Expressive Activation function of
Shen et al. (2022a). All models have depth J, widthW, and (when applicable) Lleaves; where J,W,L,n∈N+.
Model VC Dim Ref.
MoMLPs O/parenleftbig
Llog(L)2max{nLlog(L),JW2log(JW)}/parenrightbig
Thrm 4.2
ReLU MLP O/parenleftbig
JW2log(JW)/parenrightbig
Bartlett et al. (2019)
Super-Expressive ∞ Prop 4.4
We consider a mild extension of the super-expressive activation function of Shen et al. (2022a). This parametric
extension allows it to implement the identity map on the real line as well as the original super-expressive
activation function thereof.
Definition 4.3 (Trainable Super-Expressive Activation Function) .A trainable action function σ:R×R→R
is of super-expressive type if for all γ∈R
σγ:R∋x∝⇕⊣√∫⊔≀→γx+ (1−γ)σ⋆(x)
whereσ⋆:R→Ris given by
σ⋆(x)def.=|xmod(2)|Ix∈[0,∞+x
|x|+ 1Ix∈(−∞,0) (9)
Proposition 4.4 (MLPs with Super-Expressive Activation Do Not Generalize) .LetFbe the set of MLPs with
activation function in Definition 4.3, depth at-most 15, and width at-most 36n(2n+ 2). Then VC(F) =∞.
The VC dimension bounds for the standard MLP model, MLP with a super-expressive activation function as
proposed by Shen et al. (2022a), and the MoMLP model are summarized in Table 2.
5 Overview of Derivation
We now overview the proof of our main result and its full technical formulation. These objectives require us
to recall definitions from the analysis of metric spaces, which were not required in the statement of our main
result but which are required when overviewing our proof.
5.1 Technical Definitions
The metric ball in (X,d)of radiusr>0atx∈Xis denoted by Ball (X,d)(x,r)def.={z∈X:d(x,z)<r}. A
metric space (X,d)is calleddoubling, if there is C∈N+for which every metric ball in (X,d)can be covered
by at most Cmetric balls of half its radius. The smallest such constant is called (X,d)’sdoubling number ,
and is here denoted by C(X,d). Though this definition may seem abstract at first, Heinonen (2001, Theorem
12.1) provides an almost familiar characterization of all doubling metric spaces; indeed, Kis a doubling metric
space if and only if it can be identified via a suitable invertible map1with a subset of some Euclidean space.
Every subset of Rn, for anyn∈N+, is a doubling metric space; see Robinson (2011, Chapter 9) for details.
Example 2 (Subsets of Euclidean Spaces) .Fix a dimension n∈N+. The doubling number of any subset of
Euclidean space is2at most 2n+1.
In what follows, all logarithms will be taken base 2, unless explicitly stated otherwise, i.e. logvis basevfor a
givenv∈N+andlogdef.= log2. As in Petrova & Wojtaszczyk (2023, page 762), the radius of a subset A⊆Rn,
denoted by rad(A), is defined by
rad(A)def.= inf
x∈Rnsup
a∈A∥x−a∥. (10)
1So called quasi-symmetric maps, see Heinonen (2001, page 78).
2See Robinson (2011, Lemma 9.2) and the brief computations in the proof of Robinson (2011, Lemma 9.4).
8Under review as submission to TMLR
The diameter of any such set A, denoted by diam(A), satisfies the inequality diam(A)≤2 rad(A).
Finally, let us recall the notion of a uniformly continuous function. Fix n,m∈N+and letX⊂Rn. Let
ω: [0,∞)→[0,∞)be a monotonically increasing function which is continuous at 0and satisfies ω(0) = 0.
Such anωis called a modulus of continuity . A function f:X→Rmis said to be ω-uniformly continuous if
∥f(x)−f(y)∥≤ω/parenleftbig
∥x−y∥/parenrightbig
holds for all x,y∈X. We note that every continuous function is uniformly continuous if Xis compact and
that its modulus of continuity may depend on X. Furthermore, we note that every (α,L)-Hölder function is
uniformly continuous with modulus of continuity ω(t) =Ltα.
5.2 Helping to Explain MoEs via Proof Sketch
Lemma 5.1 (Size of a Tree Whose Nodes Form a δ-net of a Compact Subset of Rn).LetKbe a compact
subset of Rnwhose doubling number is C. Fixv∈Nwithv≥2, and 0<δ≤rad(K). There exists an v-ary
treeTdef.= (V,E)with leavesL⊆Ksatisfying
max
x∈Kmin
v∈L∥x−v∥<δ. (11)
Furthermore, the number of leaves Ldef.= #L, height, and total number of nodes #Vof the treeTare
(i)Leaves: at mostL=v/ceilingleftbig
clog(C)/parenleftbig
1+log(δ−1diam(K))/parenrightbig/ceilingrightbig
,
(ii)Height:/ceilingleftbig
clog(C)/parenleftbig
1 + log(δ−1diam(K))/parenrightbig/ceilingrightbig
,
(iii)Nodes: At mostv⌈clog(C)(1+log(δ−1diam(K)))⌉+1−1 /ceilingleftbig
clog(C)/parenleftbig
1+log(δ−1diam(K))/parenrightbig/ceilingrightbig
−1
wherecdef.= 1/log(v).
At each node of the tree, we will place an MLP which only locally approximates the target function on a little
ball of suitable radius (implied by the tree valency vand heighth) of lemma 5.1. I.e. by the storage space we
would like to allocate to our MoMLP model. The next step of the proof relies on a mild extension of the
quantitative universal approximation theorem in Shen et al. (2022a); Lu et al. (2021a) to the multivariate
case, as well as an extension of the multivariate approximation result of Acciaio et al. (2023, Proposition
3.10) beyond the Hölder case.
Lemma 5.2 (Vector-Valued Universal Approximation Theorem with Explicit Diameter Dependence) .Let
n,m∈N+withn≥3,K⊆Rnbe compact set of radius δ≥0,f:K→Rmbe uniformly continuous with
strictly monotone continuous modulus of continuity ω. Letσbe an activation function as in Definitions 4.3
or 3.1. For each ε>0, there exists an MLP ˆfθ:Rn→Rmwith trainable activation function σsatisfying the
uniform estimate
sup
x∈K∥f(x)−ˆfθ(x)∥≤ϵ.
The depth and width of ˆfare recorded in Table 3.
Table 3: Complexity of the MLP ˆfθin Lemma 5.2. See Table 7 in Appendix A for more detailed estimates.
Activationσ Super Expressive 4.3 PReLU 3.1
Depth (J)O(1) O/parenleftbig
(δ/ω−1(ε))n/2/parenrightbig
Width ( maxjdj)O(1) O(1)
Combining Lemmata 5.1 and 5.2 we obtain Theorem 4.1. We now present the technical version of Theorem 4.1.
This result allows distributed neural computing using ν-ary trees and allows for the approximation general
uniformly continuous target functions.
9Under review as submission to TMLR
Theorem 5.3 (Trade-Off: No. Expert vs. Expert Complexity - Technical Version of Theorem 4.1) .Suppose
thatσsatisfies Definition 3.1. Let Kbe a compact subset of Rnwhose doubling number is C. Fix an “number
of experts-to-expert complexity trade-off parameter” r∈Rand a “valency parameter” ν∈Nwithν≥2.
For every uniformly continuous map f:K→Rmwith modulus of continuity ωand each approximation error
ε>0,p∈N+, there is an ν-ary treeTdef.=(V,E)with leavesLdef.={(vi,θi)}L
i=1⊆K× Rpand a family of
MLPs with (P)ReLU activation function {ˆfθi}L
i=1defined bypparameters mapping RntoRmsatisfying:
max
x∈Kmin
(vi,θi)∈L∥x−vi∥<ε−2r/n
2ω−1/parenleftbiggε
131 (nm)1/2/parenrightbigg
and for each x∈Kandi= 1,...,L, if∥x−vi∥<δthen
∥f(x)−fθi(x)∥<ε.
The depth and width of each ˆfθiand the number of leaves, height, and number of nodes required to build the
ν-ary tree are all recorded in Table 1.
6 Does one Need Overparameterized Experts if there are Enough Experts?
We evaluate our approach in two standard machine learning tasks: regression and classification. We
experimentallyshowthatMoMLPs, whichdistributepredictionsovermultipleneuralnetworks, arecompetitive
with a single large neural network containing as many model parameters as all the MoMLPs combined. This
is desirable in cases where the large neural network does not fit into the memory of a single machine. On
the contrary, the MoMLP model can be trained by distributing each MoMLP on separate machines (or
equivalently serially on a single machine). Inference can then be performed by loading only a single MoMLP
at a time into the GPU.
6.1 Regression
We first consider regression, where the goal is to approximate non-convex synthetic functions often used for
performance test problems. In particular, we choose 1-dimensional Hölder functions, as well as Ackley (Ackley,
1987) and Rastrigin (Rastrigin, 1974) functions, whose formulations are detailed in Appendix D.1.
1D Hölder Functions. We illustrate our primary finding, as encapsulated in Theorem 4.1, by leveraging
1-dimensional functions characterized by very low regularity. This choice is motivated by the jagged structure
inherent in such functions, necessitating an exponentially higher sampling frequency compared to smooth
functions for achieving an accurate reconstruction. Indeed, this crucial sampling step forms the foundation of
many quantitative universal approximation theorems (Yarotsky, 2017; Shen et al., 2021; Kratsios & Papon,
2022). As elaborated in Appendix C, approximating a well-behaved (Lipschitz) function in ddimensions
poses a challenge equivalent to approximating a highly irregular function ( 1/d-Hölder) in a single dimension.
A visual representation of a 1/d-Hölder function is presented in Figure 2, exemplified by the trajectory
of afractional Brownian motion with a Hurst parameter of α= 1/d. A formal definition is available in
Appendix C.
2Dand3DFunctions. WeselecttheAckleyandRastriginfunctions, withtheirrespective2Drepresentations
showcased in Figure 3, as widely recognized benchmarks in the field of optimization.
Evaluation protocol. We consider the setup where the domain of a function that we try to approximate is
then-dimensional closed set [a,b]n. For instance, we arbitrarily choose the domain [0,1]whenn= 1, and
[−1,1]nwhenn≥2. Our training and test samples are the snvertices of the regular grid defined on [a,b]n.
At each run, 80%of the samples are randomly selected for training and validation, and the remaining 20%for
testing. During training, for a given and fixed set of Kprototypes pdef.=(p1,...,pK), we assign each training
samplexto its nearest prototype pkand associated neural network ˆfk. We learn the prototypes as explained
in Appendix D.2. For simplicity, we set the number of prototypes to K= 4; we also set s= 10,000ifn= 1,
s= 150ifn= 2(i.e., 1502= 22,500samples), and s= 30ifn= 3(i.e., 27,000samples). More details can
be found in Appendix D.
10Under review as submission to TMLR
0.0 0.2 0.4 0.6 0.8 1.01.5
1.0
0.5
0.00.51.01.5Comparison of Functions of Low vs. High Hölder () Regularity
: 9
10 (Hig Reg. - Smooth)
: 1
2 (Brownian Motion)
: 1
10 (Low Reg. - Rough)
Figure 2: Visual Comparison of Functions with High ( α≈1) vs. Low (α≈0) Hölder regularity. If α≈1, the function
(green) is approximately differentiable almost everywhere, meaning it does not osculate much locally and thus is
simple to approximate. If α≈0, the function may be nowhere differentiable and jagged; its extreme details make it
difficult to approximate.
(a)Ground truth Ackley
 (b)Ground truth Rastrigin
(c)Predicted Ackley
 (d)Predicted Rastrigin
 (e)log MSE
 (f)log MSE
Figure 3: Comparison of ground truth and predicted results for 2D Ackley and Rastrigin functions over the domain
[−1,1]2.
Test performance. In Table 4, we present the mean squared error obtained on the test set across 10 random
initializations and various splits of the training/test sets. The baseline consists of a single neural network
with the same overall architecture as each MoMLP but possesses as many parameters as all the MoMLPs
combined. In all instances, the MoMLP model demonstrates a significant performance advantage compared
to the baseline. Figure 3 illustrates the predictions generated by our MoMLPs, showcasing the capability of
our approach to achieve a good approximation of the ground truth functions.
Table 4: Test mean squared error (average and standard deviation) for the different functions of the regression task.
1D Hölder 2D Ackley 3D Ackley 2D Rastrigin 3D Rastrigin
MoMLPs (ours) 0.057±0.085 0.00015 ±0.00006 0.00068 ±0.00010 0.0480 ±0.0073 1.0062 ±0.0446
Baseline 0.128±0.012 0.08723 ±0.01059 0.09303 ±0.03156 3.0511 ±0.3581 8.0376 ±4.0499
11Under review as submission to TMLR
Table 5: Test classification accuracy using DINOv2 features as input (average and standard deviation).
Dataset CIFAR-10 CIFAR-100 Food-101
Ours (weighted) 98.40±0.05 90.01±0.11 91.86±0.10
Ours (unweighted) 98.42±0.04 89.62±0.25 91.79±0.16
Baseline 98.45±0.06 89.85±0.17 91.45±1.09
6.2 Classification
Datasets. We evaluate classification on standard image datasets such as CIFAR-10 (Krizhevsky & Hinton,
2010), CIFAR-100, and Food-101 (Bossard et al., 2014), which consist of 10, 100, and 101 different classes,
respectively. We use the standard splits of training/test sets: the datasets include (per category) 5,000
training and 1,000 test images for CIFAR-10, 500 training and 100 test images for CIFAR-100, and 750 and
250 for Food-101.
Training. Our MoMLP model takes as input latent DINOv2 encodings (Oquab et al., 2023) of images from
the aforementioned datasets. Each sample x∈R768corresponds to a DINOv2 embedding (i.e., n= 768).
Additionally, we set the prototypes as centroids obtained through the standard K-means clustering on the
DINOv2 embedding space. The replacement of our original prototype learning algorithm is sensible in this
context, as we operate within a structured latent space optimized through self-supervised learning using
large-scale compute and datasets.
Due to the potential class imbalance in the various Voronoi cells formed by the prototypes, we utilize two
variations of the cross-entropy loss for each MoMLP ˆfk. The first variation, termed unweighted , assigns
equal weight to all categories. The weighted variation assigns a weight that is inversely proportional to the
distribution of each category in the Voronoi cell defined by the prototype pk.
Test performance. We present the test classification accuracy over 10 different runs (with random
initialization) of both the baseline and our MoMLPs in Table 5. The weighted version performs slightly
better on datasets with a large number of categories. Nonetheless, our approach achieves comparable results
with the baseline (i.e., no difference with statistical significance), effectively decomposing the prediction
across multiple smaller models while requiring less VRAM per neural network. We report the same type of
experiment on the CIFAR datasets at the pixel level in Appendix D.4.
6.3 Discussion
OurexperimentsintheabovesubsectionsdemonstratethatMoMLPscanoutperformormatchtheperformance
ofasinglelargeneuralnetworkwiththesameoverallarchitectureandanequivalenttotalnumberofparameters,
which aligns with our theoretical insights. This advantage is particularly significant in scenarios where a single
large neural network cannot be stored on a given machine or cluster due to its high VRAM requirements, a
common challenge in the context of large language models and other recent large-scale models. While this
work focuses on smaller-scale experiments, we believe our theoretical framework represents an important initial
step toward addressing and understanding these challenges from a mathematically principled perspective.
The concept of decomposing a single large neural network into multiple smaller models that run in parallel has
been successfully applied in domains such as computer vision, as demonstrated in works like (Ren et al., 2024;
Song et al., 2024). However, existing studies in the literature are largely empirical and application-focused,
lacking the theoretical approximation rates provided by our work.
7 Conclusion
We presented approximation-theoretic and statistical foundations for MoEs by analysing the MoMLP model.
We found that MoMLPs can achieve arbitrary uniform approximation accuracy of continuous functions on
compact subsets of Euclidean space while maintaining a feasible number of parameters in active VRAM
memory (Theorem 5.3). However, this naturally comes at the cost of requiring an exponential number of
experts. We obtain upper bounds on the VC dimension of the MoMLP model (Theorem 4.2), akin to the
results of Bartlett et al. (2019) for ReLU MLPs, showing that deep-learning-based MoEs can generalize.
12Under review as submission to TMLR
References
Beatrice Acciaio, Anastasis Kratsios, and Gudmund Pammer. Designing universal causal deep learning
models: The geometric (hyper) transformer. Mathematical Finance , 2023.
D. Ackley. A Connectionist Machine for Genetic Hillclimbing . The Springer International Series in Engineering
and Computer Science. Springer US, 1987. ISBN 9780898382365. URL https://books.google.ca/books?
id=p3hQAAAAMAAJ .
Hassan Akbari, Dan Kondratyuk, Yin Cui, Rachel Hornung, Huisheng Wang, and Hartwig Adam. Alternating
gradient descent and mixture-of-experts for integrated multimodal perception. Advances in Neural
Information Processing Systems , 36, 2024.
Mazen Ali and Anthony Nouy. Approximation theory of tree tensor networks: tensorized univariate functions.
Constr. Approx. , 58(2):463–544, 2023. ISSN 0176-4276,1432-0940. doi: 10.1007/s00365-023-09620-w. URL
https://doi.org/10.1007/s00365-023-09620-w .
Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine Learning
Research , 18(19):1–53, 2017.
Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Daniel Hurt, Michael
Isard, Hyeontaek Lim, Ruoming Pang, Sudip Roy, et al. Pathways: Asynchronous distributed dataflow for
ml.Proceedings of Machine Learning and Systems , 4:430–449, 2022.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information theory , 39(3):930–945, 1993.
Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension and
pseudodimension bounds for piecewise linear neural networks. The Journal of Machine Learning Research ,
20(1):2285–2301, 2019.
Fred Espen Benth, Nils Detering, and Luca Galimberti. Neural networks in fréchet spaces. Annals of
Mathematics and Artificial Intelligence , 91(1):75–103, 2023.
Yoav Benyamini and Joram Lindenstrauss. Geometric nonlinear functional analysis. Vol. 1 , volume 48 of
American Mathematical Society Colloquium Publications . American Mathematical Society, Providence, RI,
2000. ISBN 0-8218-0835-4. doi: 10.1090/coll/048. URL https://doi.org/10.1090/coll/048 .
Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Learnability and the
Vapnik-Chervonenkis dimension. J. Assoc. Comput. Mach. , 36(4):929–965, 1989. ISSN 0004-5411,1557-735X.
doi: 10.1145/76359.76371. URL https://doi.org/10.1145/76359.76371 .
Helmut Bolcskei, Philipp Grohs, Gitta Kutyniok, and Philipp Petersen. Optimal approximation with sparsely
connected deep neural networks. SIAM Journal on Mathematics of Data Science , 1(1):8–45, 2019.
Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101–mining discriminative components
with random forests. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland,
September 6-12, 2014, Proceedings, Part VI 13 , pp. 446–461. Springer, 2014.
Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge University Press, Cam-
bridge, 2004. ISBN 0-521-83378-7. doi: 10.1017/CBO9780511804441. URL https://doi.org/10.1017/
CBO9780511804441 .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:1877–1901, 2020.
Nataly Brukhim, Daniel Carmon, Irit Dinur, Shay Moran, and Amir Yehudayoff. A characterization of
multiclass learnability. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science
(FOCS), pp. 943–955. IEEE, 2022.
13Under review as submission to TMLR
Tianshi Cao, Marc T Law, and Sanja Fidler. A theoretical analysis of the number of shots in few-shot
learning. In International Conference on Learning Representations , 2020. URL https://openreview.net/
forum?id=HkgB2TNYPS .
Jiahua Chen. Optimal rate of convergence for finite mixture models. The Annals of Statistics , pp. 221–233,
1995.
Patrick Cheridito, Arnulf Jentzen, and Florian Rossmannek. Efficient approximation of high-dimensional
functions with neural networks. IEEE Transactions on Neural Networks and Learning Systems , 2021a.
Patrick Cheridito, Arnulf Jentzen, and Florian Rossmannek. Efficient approximation of high-dimensional
functions with neural networks. IEEE Transactions on Neural Networks and Learning Systems , 33(7):
3079–3093, 2021b.
Gabriel Conant. Upper bound on vc-dimension of partitioned class. MathOverflow, 2023. URL https:
//mathoverflow.net/q/461604 . URL:https://mathoverflow.net/q/461604 (version: 2024-01-05).
Christa Cuchiero, Philipp Schmocker, and Josef Teichmann. Global universal approximation of functional
input maps on weighted spaces. arXiv preprint arXiv:2306.03303 , 2023.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals
and systems , 2(4):303–314, 1989.
Ingrid Daubechies, Ronald DeVore, Simon Foucart, Boris Hanin, and Guergana Petrova. Nonlinear approxi-
mation and (deep) relu networks. Constructive Approximation , 55(1):127–172, 2022.
Tim De Ryck, Samuel Lanthaler, and Siddhartha Mishra. On the approximation of functions by tanh neural
networks. Neural Networks , 143:732–750, 2021.
Ronald DeVore, Boris Hanin, and Guergana Petrova. Neural network approximation. Acta Numerica , 30:
327–444, 2021.
Zhiying Fang, Yidong Ouyang, Ding-Xuan Zhou, and Guang Cheng. Attention enables zero approximation
error, 2023. URL https://openreview.net/forum?id=AV_bv4Ydcr9 .
Herbert Federer. Colloquium lectures on geometric measure theory. Bull. Amer. Math. Soc. , 1978.
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models
with simple and efficient sparsity. Journal of Machine Learning Research , 23(120):1–39, 2022.
Luca Galimberti, Anastasis Kratsios, and Giulia Livieri. Designing universal causal deep learning models: The
case of infinite-dimensional dynamical systems from stochastic analysis. arXiv preprint arXiv:2210.13300 ,
2022.
Mina Ghadimi Atigh, Martin Keller-Ressel, and Pascal Mettes. Hyperbolic busemann learning with ideal
prototypes. Advances in Neural Information Processing Systems , 34:103–115, 2021.
Lukas Gonon and Juan-Pablo Ortega. Fading memory echo state networks are universal. Neural Networks ,
138:10–13, 2021.
Lukas Gonon, Lyudmila Grigoryeva, and Juan-Pablo Ortega. Approximation bounds for random neural
networks and reservoir systems. The Annals of Applied Probability , 33(1):28–69, 2023.
Google. Gemini. Google, 2023. URL https://gemini.google.com/ .
Lyudmila Grigoryeva and Juan-Pablo Ortega. Universal discrete-time reservoir computers with stochastic
inputs and linear readouts using non-homogeneous state-affine systems. Journal of Machine Learning
Research , 19(24):1–40, 2018.
Ingo Gühring and Mones Raslan. Approximation rates for neural networks with encodable weights in
smoothness spaces. Neural Networks , 134:107–130, 2021.
14Under review as submission to TMLR
Ingo Gühring, Gitta Kutyniok, and Philipp Petersen. Error bounds for approximations with deep relu neural
networks in w s, p norms. Analysis and Applications , 18(05):803–859, 2020.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language
model pre-training. In International conference on machine learning , pp. 3929–3938. PMLR, 2020.
Steve Hanneke. The optimal sample complexity of PAC learning. J. Mach. Learn. Res. , 17:Paper No. 38, 15,
2016. ISSN 1532-4435,1533-7928.
Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension bounds for piecewise linear
neural networks. In Conference on learning theory , pp. 1064–1068. PMLR, 2017.
Juha Heinonen. Lectures on analysis on metric spaces . Universitext. Springer-Verlag, New York, 2001.
Nhat Ho, Chiao-Yu Yang, and Michael I Jordan. Convergence rates for gaussian mixtures of experts. Journal
of Machine Learning Research , 23(323):1–81, 2022.
Chang hoon Song, Geonho Hwang, Jun ho Lee, and Myungjoo Kang. Minimal width for universal property
of deep rnn. Journal of Machine Learning Research , 24(121):1–41, 2023.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal
approximators. Neural networks , 2(5):359–366, 1989.
Clemens Hutter, Recep Gül, and Helmut Bölcskei. Metric entropy limits on recurrent neural network learning
of linear dynamical systems. Applied and Computational Harmonic Analysis , 59:198–223, 2022.
Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local
experts. Neural computation , 3(1):79–87, 1991.
Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford,
Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of
experts. arXiv preprint arXiv:2401.04088 , 2024.
Yuling Jiao, Yanming Lai, Xiliang Lu, Fengru Wang, Jerry Zhijian Yang, and Yuanyuan Yang. Deep neural
networks with ReLU-sine-exponential activations break curse of dimensionality in approximation on Hölder
class.SIAM J. Math. Anal. , 55(4):3635–3649, 2023. ISSN 0036-1410,1095-7154. doi: 10.1137/21M144431X.
URL https://doi.org/10.1137/21M144431X .
Michael I Jordan and Lei Xu. Convergence results for the em approach to mixtures of experts architectures.
Neural networks , 8(9):1409–1431, 1995.
Heinrich Jung. Ueber die kleinste kugel, die eine räumliche figur einschliesst. Journal für die reine und
angewandte Mathematik , 123:241–257, 1901. URL http://eudml.org/doc/149122 .
Martin Keller-Ressel and Stephanie Nargang. Strain-minimizing hyperbolic network embeddings with
landmarks. Journal of Complex Networks , 11(1):cnad002, 2023.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Anastasis Kratsios and Léonie Papon. Universal approximation theorems for differentiable geometric deep
learning. The Journal of Machine Learning Research , 23(1):8896–8968, 2022.
Anastasis Kratsios and Behnoosh Zamanlooy. Do relu networks have an edge when approximating compactly-
supported functions? Transactions on Machine Learning Research , 2022.
R. Krauthgamer, J. R. Lee, M. Mendel, and A. Naor. Measured descent: a new embedding method for
finite metrics. Geom. Funct. Anal. , 15(4):839–858, 2005. ISSN 1016-443X,1420-8970. doi: 10.1007/
s00039-005-0527-6. URL https://doi.org/10.1007/s00039-005-0527-6 .
15Under review as submission to TMLR
Alex Krizhevsky and Geoff Hinton. Convolutional deep belief networks on cifar-10. Unpublished manuscript ,
40(7):1–9, 2010.
Hugo Larochelle, Yoshua Bengio, Jérôme Louradour, and Pascal Lamblin. Exploring strategies for training
deep neural networks. Journal of machine learning research , 10(1), 2009.
Marc T Law, Jake Snell, Amir massoud Farahmand, Raquel Urtasun, and Richard S Zemel. Dimensionality
reduction for representing the knowledge of probabilistic models. In International Conference on Learning
Representations , 2019.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim
Krikun, Noam Shazeer, and Zhifeng Chen. {GS}hard: Scaling giant models with conditional computation
and automatic sharding. In International Conference on Learning Representations , 2021. URL https:
//openreview.net/forum?id=qrwe7XHTmYb .
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on
Learning Representations , 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7 .
JianfengLu, ZuoweiShen, HaizhaoYang, andShijunZhang. Deepnetworkapproximationforsmoothfunctions.
SIAM J. Math. Anal. , 53(5):5465–5506, 2021a. ISSN 0036-1410,1095-7154. doi: 10.1137/20M134695X.
URL https://doi.org/10.1137/20M134695X .
JianfengLu, ZuoweiShen, HaizhaoYang, andShijunZhang. Deepnetworkapproximationforsmoothfunctions.
SIAM J. Math. Anal. , 53(5):5465–5506, 2021b. ISSN 0036-1410,1095-7154. doi: 10.1137/20M134695X.
URL https://doi.org/10.1137/20M134695X .
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of neural
networks: A view from the width. Advances in neural information processing systems , 30, 2017.
Harris Abdul Majid and Francesco Tudisco. Mixture of neural operators: Incorporating historical information
for longer rollouts. In ICLR 2024 Workshop on AI4DifferentialEquations In Science , 2024.
Tong Mao and Ding-Xuan Zhou. Rates of approximation by relu shallow neural networks. Journal of
Complexity , 79:101784, 2023.
Marina Meila and Michael I Jordan. Learning with mixtures of trees. Journal of Machine Learning Research ,
1(Oct):1–48, 2000.
Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka. Metric learning for large scale
image classification: Generalizing to new classes at near-zero cost. In Computer Vision–ECCV 2012: 12th
European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part II 12 , pp.
488–501. Springer, 2012.
Hrushikesh Mhaskar, Qianli Liao, and Tomaso Poggio. When and why are deep networks better than shallow
ones? In Proceedings of the AAAI conference on artificial intelligence , volume 31, 2017.
Hrushikesh N Mhaskar. Neural networks for optimal approximation of smooth and analytic functions. Neural
computation , 8(1):164–177, 1996.
A. Martina Neuman and Philipp Christian Petersen. Efficient learning using spiking neural networks equipped
with affine encoders and decoders, 2024.
Joost AA Opschoor, Ch Schwab, and Jakob Zech. Exponential relu dnn expression of holomorphic maps in
high dimension. Constructive Approximation , 55(1):537–582, 2022.
Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre
Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual
features without supervision. arXiv preprint arXiv:2304.07193 , 2023.
16Under review as submission to TMLR
Philipp Petersen and Felix Voigtlaender. Equivalence of approximation by convolutional neural networks and
fully-connected networks. Proceedings of the American Mathematical Society , 148(4):1567–1581, 2020.
Guergana Petrova and Przemysł aw Wojtaszczyk. Lipschitz widths. Constr. Approx. , 57(2):759–805,
2023. ISSN 0176-4276,1432-0940. doi: 10.1007/s00365-022-09576-3. URL https://doi.org/10.1007/
s00365-022-09576-3 .
David Pollard. Empirical processes: theory and applications , volume 2 of NSF-CBMS Regional Conference
Series in Probability and Statistics . Institute of Mathematical Statistics, Hayward, CA; American Statistical
Association, Alexandria, VA, 1990. ISBN 0-940600-16-1.
Joan Puigcerver, Rodolphe Jenatton, Carlos Riquelme, Pranjal Awasthi, and Srinadh Bhojanapalli. On the
adversarial robustness of mixture of experts. Advances in Neural Information Processing Systems , 35:
9660–9671, 2022.
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding
by generative pre-training, 2018.
Leonard Andreevič Rastrigin. Systems of extremal control. Nauka, 1974.
Xuanchi Ren, Jiahui Huang, Xiaohui Zeng, Ken Museth, Sanja Fidler, and Francis Williams. Xcube: Large-
scale 3d generative modeling using sparse voxel hierarchies. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , 2024.
James C. Robinson. Dimensions, embeddings, and attractors , volume 186 of Cambridge Tracts in Mathematics .
Cambridge University Press, Cambridge, 2011. ISBN 978-0-521-89805-8.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms .
Cambridge university press, 2014.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff
Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint
arXiv:1701.06538 , 2017.
Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network with approximation error being reciprocal of
width to power of square root of depth. Neural Computation , 33(4):1005–1036, 2021.
Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network approximation: Achieving arbitrary accuracy
with fixed number of neurons. The Journal of Machine Learning Research , 23(1):12653–12712, 2022a.
Zuowei Shen, Haizhao Yang, and Shijun Zhang. Optimal approximation rate of relu networks in terms of
width and depth. Journal de Mathématiques Pures et Appliquées , 157:101–135, 2022b.
Jonathan W Siegel and Jinchao Xu. High-order approximation rates for shallow neural networks with cosine
and reluk activation functions. Applied and Computational Harmonic Analysis , 58:1–26, 2022.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in
neural information processing systems , 30, 2017.
Yanke Song, Jonathan Lorraine, Weili Nie, Karsten Kreis, and James Lucas. Multi-student diffusion distillation
for better one-step generators, 2024. URL https://arxiv.org/abs/2410.23274 .
Shivin Srivastava, Kenji Kawaguchi, and Vaibhav Rajan. Expertnet: A symbiosis of classification and
clustering, 2022.
Taiji Suzuki. Adaptivity of deep relu network for learning in besov and mixed smooth besov spaces: optimal
rate and curse of dimensionality. arXiv preprint arXiv:1810.08033 , 2018.
Paulo Tabuada and Bahman Gharesifard. Universal approximation power of deep residual neural networks
via nonlinear control theory. In International Conference on Learning Representations , 2021. URL
https://openreview.net/forum?id=-IXhmY16R3M .
17Under review as submission to TMLR
Henry Teicher. On the mixture of distributions. The Annals of Mathematical Statistics , 31(1):55–73, 1960.
Henry Teicher. Identifiability of finite mixtures. The annals of Mathematical statistics , pp. 1265–1269, 1963.
Nathaniel Trask, Amelia Henriksen, Carianne Martinez, and Eric Cyr. Hierarchical partition of unity networks:
fast multilevel training. In Bin Dong, Qianxiao Li, Lei Wang, and Zhi-Qin John Xu (eds.), Proceedings of
Mathematical and Scientific Machine Learning , volume 190 of Proceedings of Machine Learning Research ,
pp. 271–286. PMLR, 15–17 Aug 2022. URL https://proceedings.mlr.press/v190/trask22a.html .
Felix Voigtlaender. The universal approximation theorem for complex-valued neural networks. Applied and
Computational Harmonic Analysis , 64:33–61, 2023.
Felix Voigtlaender and Philipp Petersen. Approximation in l p ( µ) with deep relu neural networks. In 2019
13th International conference on Sampling Theory and Applications (SampTA) , pp. 1–4. IEEE, 2019.
Georges Voronoi. Nouvelles applications des paramètres continus à la théorie des formes quadratiques.
deuxième mémoire. recherches sur les parallélloèdres primitifs. Journal für die reine und angewandte
Mathematik (Crelles Journal) , 1908(134):198–287, 1908.
Peiming Wang, Martin L Puterman, Iain Cockburn, and Nhu Le. Mixed poisson regression models with
covariate dependent rates. Biometrics , pp. 381–400, 1996.
Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E. Gonzalez. SkipNet: Learning Dynamic
Routing in Convolutional Networks. CVPR, 2017.
Yunfei Yang and Ding-Xuan Zhou. Optimal rates of approximation by shallow reluk neural networks and
applications to nonparametric regression. Constructive Approximation , pp. 1–32, 2024.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks , 94:103–114,
2017.
Dmitry Yarotsky. Optimal approximation of continuous functions by very deep relu networks. In Sébastien
Bubeck, Vianney Perchet, and Philippe Rigollet (eds.), Proceedings of the 31st Conference On Learning
Theory, volume 75 of Proceedings of Machine Learning Research , pp. 639–649. PMLR, 06–09 Jul 2018.
URL https://proceedings.mlr.press/v75/yarotsky18a.html .
Dmitry Yarotsky. Elementary superexpressive activations. In International Conference on Machine Learning ,
pp. 11932–11940. PMLR, 2021.
Dmitry Yarotsky. Universal approximations of invariant maps by neural networks. Constructive Approximation ,
55(1):407–474, 2022.
Dmitry Yarotsky and Anton Zhevnerchuk. The phase diagram of approximation rates for deep neural networks.
Advances in neural information processing systems , 33:13005–13015, 2020.
ChulheeYun, SrinadhBhojanapalli, AnkitSinghRawat, SashankJReddi, andSanjivKumar. Aretransformers
universal approximators of sequence-to-sequence functions? arXiv preprint arXiv:1912.10077 , 2019.
Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar.
O (n) connections are expressive enough: Universal approximability of sparse transformers. Advances in
Neural Information Processing Systems , 33:13783–13794, 2020.
Behnoosh Zamanlooy and Anastasis Kratsios. Learning sub-patterns in piecewise continuous functions.
Neurocomputing , 480:192–211, 2022.
Shijun Zhang, Zuowei Shen, and Haizhao Yang. Deep network approximation: Achieving arbitrary accuracy
with fixed number of neurons. Journal of Machine Learning Research , 23(276):1–60, 2022.
18Under review as submission to TMLR
A Detailed Tables And Rates
Table 6: Complexity of Feedforward Neural Network ˆfθand theν-ary routing tree in Theorem 5.3. Here cdef.= log (v)−1.
See Table 8 in Appendix A for more detailed estimates.
Parameter Estimate
Depth (J)O(max{1,ε−r})
Width ( maxjdj)O(1)
No. Experts O/parenleftbig
max{1,ε2r/n/ω−1(ε)}/parenrightbig
Routing Complexity O/parenleftbig
max{1,log(ε2r/n/ω−1(ε))}/parenrightbig
Table 7: Complexity of the MLP ˆfθin Lemma 5.2.
Activationσ Super Expressive 4.3 PReLU 3.1
Depth (J) 15 m m/parenleftbigg
19 + 2n+ 11/ceilingleftbigg/parenleftig
δ23/2n1/2
(n+1)1/2ω−1/parenleftbig
ε/(131√nm)/parenrightbig/parenrightign/2/ceilingrightbigg/parenrightbigg
Width ( maxjdj)36n(2n+ 1) +m 16 max{n,3}+m
Table 8: Complexity of Feedforward Neural Network ˆfθand theν-ary tree in Theorem 5.3. Here cdef.= log(v)−1.
Parameter Estimate
Depth (J) m(19 + 2n+ 11⌈ε−r⌉)
Width ( maxjdj) 16 max{n,3}+m
No. Experts (No. Leaves) O/parenleftig
v/ceilingleftbig
clog(C)/parenleftbig
1+log(ε2r/ndiam(K))//parenleftbig
2ω−1/parenleftbig
ε
131 (nm)1/2/parenrightbig/parenrightbig/parenrightbig/ceilingrightbig
,/parenrightig
Height (Routing Complexity)/ceilingleftbig
clog(C)/parenleftbig
1 + log(ϵ2r/ndiam(K)//parenleftbig
2ω−1/parenleftbigε
131 (nm)1/2/parenrightbig/parenrightbig
)/parenrightbig/ceilingrightbig
Nodes O/parenleftigg
v⌈clog(C)(1+log(ε2r/ndiam(K)//parenleftbig
2ω−1(ε
131 (nm)1/2)/parenrightbig
))⌉+1
−1
/ceilingleftbig
clog(C)/parenleftbig
1+log(ε2r/n
2diam(K))/ω−1/parenleftbigg
ε
131 (nm)1/2/parenrightbigg/parenrightbig/ceilingrightbig
−1/parenrightigg
B Appendix: Proofs
B.1 Proof of Theorem 4.1
Proof of Lemma 5.1. SinceKis a doubling metric space then, (Acciaio et al., 2023, Lemma 7.1), for each
δ>0, there exist x1,...,xN∈Ksatisfying
max
x∈Kmin
i=1,...,Nδ∥x−xi∥<δandNδ≤C⌈log(diam(K)/δ)⌉.
In particular, since the doubling number of KisC, we have the upper-bound
Nδ≤CClog(δ−1diam(K)). (12)
Anelementarycomputationshowsthatthecomplete v-arytreeofheight hhasleavesLandtotalvertices/nodes
Vgiven by
L=vhandV=vh+1−1
h−1. (13)
19Under review as submission to TMLR
Taking the formulation of Lgiven in equation 13, to be the least integerupper bound of the right-hand side
of equation 12, which is itself an upper-bound for Nδ, and solving for hyields:
h=/ceilingleftig
logv(C)/parenleftbig
1 + log(δ−1diam(K))/parenrightbig/ceilingrightig
, (14)
where the integer ceiling was applied since hmust be an integer.
LetLbe any set vhpoints inKcontaining the set {xi}Nδ
i=1. LetTdef.=(V,E)be any complete binary tree
with leavesL; note that,L⊆V. By construction, and the computation in equation 13, Thasvhleaves and
Lv−1
h−1nodes.
For completeness, we include a minor modification of the proof of Acciaio et al. (2023, Proposition 3.10),
which allows for the approximation of uniformly continuous functions of arbitrarily low regularity. The
original formulation of that result only allows for α-Hölder function.
Proof of Lemma 5.2. Iff(x) =cfor some constant c>0, then the statement holds with the neural network
ˆf(x) =c, which can be represented as in equation 2 with [d] = (n,m), whereAjis the 0matrix for all j, and
the “c” in equation 2 is taken to be this constant c. Therefore, we henceforth only need to consider the case
wherefis not constant. Let us observe that, if we pick some x⋆∈K, then for any multi-index [d]and any
neural network ˆfθ∈NNσ
[d],ˆfθ(x)−f(x⋆)∈NNσ
[d], sinceNNσ
[d]is invariant to post-composition by affine
functions. Thus, we represent ˆfθ(x)−f(x⋆) =ˆfθ⋆(x), for someθ⋆∈RP([d]). Consequently:
sup
x∈K/vextendsingle/vextendsingle/vextendsingle∥(f(x)−f(x⋆))−ˆfθ⋆(x)∥−∥f(x)−ˆfθ(x)∥/vextendsingle/vextendsingle/vextendsingle= 0.
Therefore, without loss of generality, we assume that f(x∗) = 0for somex∗∈ K. By Benyamini &
Lindenstrauss (2000, Theorem 1.12), there exists an ω-uniformly continuous map F:Rn→Rmextendingf.
Step 1 – Normalizing ˜fto the Unit Cube: First, we identify a hypercube “nestling” K. To this end, let
rKdef.= diam(K)/radicalbiggn
2(n+ 1). (15)
By Jung’s Theorem (see Jung (1901)), there exists x0∈Rnsuch that the closed Euclidean ball
Ball (Rn,dn)(x0,rK)containsK. Therefore, by Hölder’s inequality, we have that the n-dimensional hy-
percube [x0−rK¯1,x0+rK¯1]3containsB(Rn,dn)(x0,rK), where ¯1 = (1,..., 1)∈Rn. Consequently,
K⊆ [x0−rK¯1,x0+rK¯1]. Let ˜fdef.=F|[x0−rK¯1,x0+rK¯1], then ˜f∈C([x0−rK¯1,x0+rK¯1],Rm)is anω-uniformly
continuous extension of fto[x0−rK¯1,x0+rK¯1].
SinceKhas at least two distinct points, then rK>0. Hence, the affine function
T:Rn∋x∝⇕⊣√∫⊔≀→(2rK)−1(x−x0+rK¯1)∈Rn
is well-defined, invertible, not identically 0, and maps [x0−rK¯1,x0−rK¯1]to[0,1]n. A direct computation
shows that gdef.=˜f◦T−1is also uniformly continuous, whose modulus of continuity ˜ω: [0,∞)→[0,∞)is
given by
˜ω(t)def.=ω(2rKt) (16)
for allt∈[0,∞). Furthermore, since for each i= 1,...,m, define pji:Rm∋y∝⇕⊣√∫⊔≀→yi∈R. Since each pjiis
1-Lipschitz then, for each i= 1,...,m, the mapgidef.= pji◦g: [0,1]n→Ris also ˜ω-uniformly continuous.
By orthogonality, we also note that g(x) =/summationtextm
i=1gi(x)ei, for eachx∈Rn, wheree1,...,emis the standard
orthonormal basis of Rm; i.e. theithcoordinate of ejis1if and only if i=jand0is otherwise.
Step 2 – Constructing the Approximator: Fori= 1,...,m, let ˆfθ(i)∈NNσ
[d(i)]for some multi-index
3Forx, y∈Rnwe denote by [x, y]the hypercube defined by/producttextn
i=1[xi, yi].
20Under review as submission to TMLR
[d(i)] = (d(i)
0,...,d(i)
J)withn-dimensional input layer and 1-dimensional output layer, i.e. d(i)
0=nand
d(i)
J= 1, and letθ(i)∈RP([d(i)])be the parameters defining ˆfθ(i). Since the pre-composition by affine functions
and the post-composition by linear functions of neural networks in NNσ
[d(i)]are again neural networks in
NNσ
[d(i)], we have that gθ(i)def.=ˆfθ(i)◦T−1belongs toNNσ
[d(i)]. Denote the standard basis of Rmby{ei}m
i=1.
We compute:
sup
x∈K/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublef(x)−m/summationdisplay
i=1ˆfθ(i)(x)ei/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
= sup
x∈K/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜f(x)−m/summationdisplay
i=1ˆfθ(i)(x)ei/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤ sup
x∈[x0−rK¯1,x0+rK¯1]/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜f(x)−m/summationdisplay
i=1ˆfθ(i)(x)ei/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
= sup
x∈[x0−rK¯1,x0+rK¯1]/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜f◦T−1◦T(x)−m/summationdisplay
i=1ˆfθ(i)◦T−1◦T(x)ei/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
= sup
u∈[0,1]n/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1gi(u)ei−m/summationdisplay
i=1gθ(i)(u)ei/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤√mmax
u∈[0,1]nmax
1≤i≤m|gi(u)−gθ(i)(u)|. (17)
Fix˜ε>0, to be determined below. For each i= 1,...,m, depending on which assumption σsatisfies, (Shen
et al., 2022b, Theorem 1.1) (resp. (Shen et al., 2022a, Theorem 1) if σis as in Definition 9) imply that there
is a neural network with activation function σ⋆:R→Rsatisfying
max
u∈[0,1]n|gi(u)−gθ(i)(u)|<˜ε. (18)
Furthermore, the depth and width of these MLPs can be bounded above on a case-by-case basis as follows:
(i)Ifσsatisfies Definition 4.3 then, setting each γ= 0implies that σ0(x) =σ⋆(x), as defined in
equation 9; thus
J(i)≤11and max
1≤j≤J(i)dj≤36n(2n+ 1)
In this case, we set ˜εdef.=ε/√m; we have used Shen et al. (2022a, Theorem 1).
(ii)Ifσsatisfies Definition 3.1 then, setting each γ= 1implies that σ0(x) =ReLU (x)def.= max{0,x};
yielding
J(i)≤18 + 2n+ 11/ceilingleftig/parenleftig2rK
ω−1/parenleftbig
ε/(131√nm)/parenrightbig/parenrightign/2/ceilingrightig
and max
1≤j≤J(i)dj≤16 max{n,3}
in this case, we have employed Shen et al. (2022b, Theorem 1.1).
In either case, the estimate in equation 17 yields
max
x∈K/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublef(x)−m/summationdisplay
i=1ˆfθ(i)(x)ei/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble<ε.
21Under review as submission to TMLR
Step 3 – Assembling into an MLP: Letg1•g2denote the component-wise composition of a univariate
functiong1with a multivariate function g2.
If the activation function σis either in Definitions 4.3 or 3.1, then it trivially implements the identity IRonR
by settingγ= 0; i.e.σ1=IR. Consequentially, for any k∈N+, ifIkdenotes the k×k-identity matrix, then
Ikσ1•Ik∈NNσ
[dk]withP([d]) = 2k, andIkσ1•Ik= 1Rk. Therefore, mutatis mutandis, NNσ
[·]satisfies the
c-identity requirement with4c= 2, as defined in Cheridito et al. (2021a, Definition 4). From there, mutatis
mutandis, we may apply Cheridito et al. (2021a, Proposition 5). Thus, there is a multi-index [d] = (d0,...,dJ)
withd0=nanddJ=m, and a network ˆfθ∈NNσ
[d]implementing/summationtextm
i=1ˆfθ(i)ei, i.e.
m/summationdisplay
i=1ˆfθ(i)ei=ˆfθ,
such that ˆfθ’s depth and width are bounded-above, on a case-by-case basis, by
(i) Ifσsatisfies Definition 4.3 then, setting each γ= 0
J≤15m
and max
1≤j≤J(i)dj≤36n(2n+ 1) +m.
In this case, we set ˜εdef.=ε/√m.
(ii) Ifσsatisfies Definition 3.1 then, setting each γ= 0yields
J≤m/parenleftigg
19 + 2n+ 11/ceilingleftig/parenleftig2rK
ω−1/parenleftbig
ε/(131√nm)/parenrightbig/parenrightign/2/ceilingrightig/parenrightigg
and max
1≤j≤J(i)dj≤16 max{n,3}+m.
Incorporating the definition of rKin equation 15 and employing the inequality diam (K)≤2rad(K)completes
the proof.
Lemma B.1 (Trade-Off: No. Expert vs. Expert Complexity) .LetKbe a compact subset of Rnwhose
doubling number is Cand a uniformly continuous map f:K→Rmwith modulus of continuity ω.
Fixv∈Nwithv≥2,0<δ≤rad(K), andε>0. Suppose that σsatisfies Definition 3.1. There exists a
p∈N+and av-ary treeTdef.= (V,E)with leavesLdef.={(vi,θi)}L
i=1⊆K× Rpsatisfying
max
x∈Kmin
(vi,θi)∈L∥x−vi∥<δ. (19)
Furthermore, for each x∈Kand eachi= 1,...,L, if∥x−vi∥<δthen
∥f(x)−fθi(x)∥<ε.
We have the following estimates:
(i)Depth.Depth of each ˆfθiism/parenleftbigg
19 + 2n+ 11/ceilingleftig/parenleftig
δ23/2n1/2
(n+1)1/2ω−1/parenleftbig
ε/(131√nm)/parenrightbig/parenrightign/2/ceilingrightig/parenrightbigg
(ii)Width. Width of each ˆfθiis16 max{n,3}+m
(iii)Leaves: at mostL=v/ceilingleftbig
clog(C)/parenleftbig
1+log(δ−1diam(K))/parenrightbig/ceilingrightbig
,
4Formally, it satisfies what is the 1-identity requirement, thus it satisfies the c-identity requirement for all integers c≥2.
However, the authors of Cheridito et al. (2021a) do not explicitly consider the extremal case where c= 1.
22Under review as submission to TMLR
(iv)Height:/ceilingleftbig
clog(C)/parenleftbig
1 + log(δ−1diam(K))/parenrightbig/ceilingrightbig
,
(v)Nodes: At mostv⌈clog(C)(1+log(δ−1diam(K)))⌉+1−1 /ceilingleftbig
clog(C)/parenleftbig
1+log(δ−1diam(K))/parenrightbig/ceilingrightbig
−1
wherecdef.= log(v)−1.
Proof of Lemma B.1. Consider the tree ˜Tgiven by Lemma 5.1. For each leaf viof˜T, we apply Lemma 5.2 to
deduce the existence of an MLP ˆfθiwith explicit depth and width estimates given by that lemma, satisfying
the uniform estimate
max
∥x−vi∥≤δ∥f(x)−ˆfθi(x)∥<ε. (20)
LetTbe the same tree as ˜Twith leaves identified with {(vi,θi)}L
i=1.
We now prove our main technical version, namely Theorem 5.3, which directly implies the special case
recorded in Theorem 4.1.
Proof of Theorem 5.3 (and this Theorem 4.1). Applying Lemma B.1 with δgiven as the solution of
/parenleftigδ23/2n1/2
(n+ 1)1/2ω−1(ε/131 (nm)1/2)/parenrightign/2
≤/parenleftigδ23/2n1/2
(2n1/2ω−1(ε/131 (nm)1/2)/parenrightign/2
=ε−r. (21)
Solving equation 21 for δimplies that it is given by
δ=ε−2r/n
2ω−1/parenleftbiggε
131 (nm)1/2/parenrightbigg
.
This completes the proof.
Proof of Theorem 4.1. SettingK=Bn(0,1),r= 1/2,ω(t) =Lt, and thusω−1(t) =L−1t1/α, in Theorem 5.3
yields the conclusion. Finally, by the computation in Example 2, we have that C≤2n+1; thus, log(C) =
(n+ 1) log(2)≤2n. Noting that c= 1/log(2) = 1 completes the proof.
Remark B.2.The constant hidden under the big Oin is 1 + max{1,log(L1/α262(nm)1/(2α))}.
B.2 Proofs of Theorem 4.2
We use the following lemma and its proof due to Gabriel Conant .
Lemma B.3 (Conant (2023)) .Fixn,L,d∈N+. LetHbe a non-empty set of functions from Rnto{0,1}of
VC dimension at-most d. LetCLbe the set of all ordered partitions (Voronoi diagrams) (Cl)˜L
l=1covering Rn,
where ˜L≤L, and for which there exist distinct p1,...,p ˜L∈Rnsuch that: for each l= 1,..., ˜L
Cldef.=˜Cl\/uniondisplay
s<l˜Cs
˜Cldef.={x∈Rn:∥x−pl∥= min
s=1,...,˜L∥x−ps∥}.(22)
LetHLbe the set of functions from Rnto{0,1}of the form
f=˜L/summationdisplay
l=1flICl
where ˜L∈N+with ˜L≤L,f1,...,f ˜L∈H, and (Cl)˜L
l=1∈CL. Then, the VC dimension of HLsatisfies
VC(HL)≤8Llog(max{2,L})2/parenleftbig
max{d,2(n+ 1) (L−1) log(3L−3)}/parenrightbig
.
23Under review as submission to TMLR
Proof of Lemma B.3. Let us first fix our notation. Each x0∈Rn\{0}andt∈Rdefines a halfspace inRn
given byHSx0,tdef.={x∈Rn:⟨x0,x⟩≤t}(see (Boyd & Vandenberghe, 2004, Section 2.2.1) for details). We
denote set of all halfspaces in RnbyHSndef.={HSx0,t:∃x0∈Rn\{0}∃t∈R}. Consider the set C(L)of all
C⊆Rnof the form
C=˜L−1/intersectiondisplay
l=1Xl (23)
for some positive integer 2≤˜L≤max{2,L}andX1,...,X ˜L−1∈HSn.
Step 1 - Reformulation as Set of Sets
By definition of the powerset 2Rnof the set Rn, each subset A⊆Rncan be identifies with a function
(classifier) from Rnto{0,1}via the bijection mapping any X∈2Rnto the binary classifier IX(i.e. the
indicator function of the set X). Using this bijection, we henceforth identify both HandHLwith subsets of
the powerset 2Rn.
Under this identification, the class HLcan be represented as the collection of subsets XofRnof the form
X=˜L/uniondisplay
l=1Hl∩Cl, (24)
where ˜L∈N+satisfies ˜L≤L, and for each l= 1,..., ˜Lwe haveHl∈Hand(Cl)˜L
l=1∈CLis of the
form equation 22 for some distinctpointsp1,...,p ˜L∈Rn.
Step 2 - VC Dimension of Voronoi Diagrams with at-most LCells
An element of (Cl)˜L
l=1ofCLis, by definition, a Voronoi diagram in Rnand thus, Boyd & Vandenberghe (2004,
Exercise 2.9) implies that each C1,...,C ˜Lis the intersection of ˜L−1≤L−1halfspaces; i.e. C1,...,C ˜L∈C(L)
(see equation 23). Since CL={∩˜L
l=1Hi:∃˜L∈N+H1,...,H ˜L∈HSn˜L≤L}then Blumer et al. (1989,
Lemma 3.2.3) implies that
VC(CL)≤2 VC(HSn) (L−1) log(3L−3)
≤2(n+ 1) (L−1) log(3L−3);(25)
the second inequality in equation 25 holds since VC(HSn) =n+ 1by Shalev-Shwartz & Ben-David (2014,
Theorem 9.3).
Step 3 - VC Dimension of The Class HL
DefineH∩C (L)def.={H∩C:H∈HandC∈C(L)}. Again using Blumer et al. (1989, Lemma 3.2.3), we
have
VC/parenleftbig
H∩C (L)/parenrightbig
≤2/parenleftbig
max{VC(H),VC(C(L))}/parenrightbig
2 log(6)
≤4 log(6)/parenleftbig
max{VC(H),VC(C(L))}/parenrightbig
≤4 log(6)/parenleftbig
max{d,2(n+ 1) (L−1) log(3L−3)}/parenrightbig
.(26)
Consider the set ˜Hdef.={∪˜L
l=1Hl:˜L∈N+,˜L≤L,∀l= 1,..., ˜L, Hl∈H∩C (L)}. Applying Blumer et al.
(1989, Lemma 3.2.3), one final time yields
VC/parenleftbig˜H/parenrightbig
≤2 VC/parenleftbig
H∩C (L)/parenrightbig
Llog(3L) (27)
≤2/parenleftig
4 log(6)/parenleftbig
max{d,2(n+ 1) (L−1) log(3L−3)}/parenrightbig/parenrightig
Llog(3L) (28)
≤8Llog(max{2,L})2/parenleftbig
max{d,2(n+ 1) (L−1) log(3L−3)}/parenrightbig
(29)
where equation 28 held by the estimate in equation 26. Finally, since VC(A)≤VC(B)wheneverA⊆Bfor
any setBthen sinceHL⊆˜Hthen equation 27-equation 29 yields the desired conclusion.
We may now derive Theorem 4.2 by merging Lemma B.3 and one of the main results of Bartlett et al. (2019).
24Under review as submission to TMLR
Proof of Theorem 4.2. Letn,J,W,L∈N+and consider the (non-empty) set of real-valued functions
NNPReLU
J,W:n,1. By definition of the VC dimension of a set of real-valued functions, given circa equation 3, we
have
VC/parenleftbig
NNPReLU
J,W:n,1/parenrightbigdef.= VC/parenleftbig
I(0,∞)◦NNPReLU
J,W:n,1/parenrightbig
VC/parenleftbig
NPPReLU
J,W,L :n,1/parenrightbigdef.= VC/parenleftbig
I(0,∞)◦NPPReLU
J,W,L :n,1/parenrightbig
. (30)
By Bartlett et al. (2019, Theorem 7), we have that
VC(I(0,∞)◦NNPReLU
J,W:n,1)≤D⋆def.=/ceilingleftbig
J+ (J+ 1)W2log2/parenleftbig
e4(J+ 1)Wlog2(e2(J+ 1)W)/parenrightbig/ceilingrightbig
.(31)
Therefore, applying Lemma B.3 with H=/parenleftbig
I(0,∞)◦NNPReLU
J,W:n,1/parenrightbig
yields the estimate
VC/parenleftbig
I(0,∞)◦NPPReLU
J,W,L :n,1/parenrightbig
≤8Llog(max{2,L})2/parenleftbig
max{D⋆,2(n+ 1) (L−1) log(3L−3)}/parenrightbig
(32)
Combining equation 32 and the definition equation 30 yields the bound. In particular,
VC/parenleftbig
NPPReLU
J,W,L :n,1/parenrightbig
∈O/parenleftbig
Llog(L)2max{nLlog(L),JW2log(JW)}/parenrightbig
yielding the second conclusion.
B.3 Proof of Proposition 4.4
Proof.We argue by contradiction. Suppose that Fhas finite VC dimension VC(F). Then, Shen et al. (2022b,
Theorem 2.4) implies that there exists a 1-Lipschitz map f: [0,1]n→Rsuch that does not exist a strictly
positiveε∈(0,VC(F)−1/n/9)satisfying such that
inf
ˆf∈Fsup
x∈[0,1]n|ˆf(x)−f(x)|≤ε. (33)
However, Shen et al. (2022a, Theorem 1) implies that, for every 1-Lipschitz function, in particular for f, and
for each ˜ε>0there exists a ˆf˜ε∈Fsatisfying
sup
x∈[0,1]n|ˆf˜ε(x)−f(x)|≤˜ε. (34)
Setting ˜ε=VC(F)−1/n/18yieldsacontradictionasequation33andequation34cannotbothbesimultaneously
true. Therefore,Fhas infinite VC dimension.
C The Curse of Irregularity
We now explain why learning Hölder functions of low regularity ( (1,1/d)-Hölder) functions on the real line
segment [0,1]is equally challenging as learning regular functions ( 1-Lipschitz) on [0,1]n.
C.1 Hölder Functions
Fixn,m∈Nand letX⊂Rnbe non-empty and compact of diameter D. Fix 0< α≤1,L≥0, and let
f:X→Rmbe(α,L)-Hölder continuous, meaning
∥f(x)−f(y)∥≤L∥x−y∥α
holds for each x,y∈X. For anyL≥0and0<α≤1, we denote set of all (α,L)-Hölder functions from Xto
Rnis denoted by Cα([0,1]n,R;L).
We focus on the class of locally Hölder functions since they are generic, i.e. universal, amongst all continuous
functions by the Stone–Weierstrass theorem. In this case, Hölder functions are sufficiently rich to paint a
25Under review as submission to TMLR
full picture of the hardness to approximate arbitrary Hölder functions either by MLPs against the proposed
model.
In contrast to smaller generic function classes, such as polynomials, Hölder functions provide more freedom
in experimentally visualizing our theoretical results. This degree of freedom is the parameter α, which
modulates their regularity . Asαtends to 0the Hölder functions become complicated and when α= 1
the Rademacher-Stephanov theorem, see Federer (1978, Theorems 3.1.6 and 3.1.9) characterizes Lebesgue
almost-everywhere differentiable functions as locally (1,L)-Hölder maps. Note that (1,L)-Hölder functions
are also called L-Lipschitz maps and, in this case, L=supx∥∇f(x)∥opwhere the supremum is taken over all
points where fis differentiable and where ∥·∥opis the operator norm.
C.2 The Curse of Irregularity
The effect of low Hölder regularity , i.e. when α≈0, has the same effect as high-dimensionality on the
approximability of arbitrary α-Hölder functions. This is because any real-valued model/hypothesis class F1
of functions on Rapproximating an arbitrary (1
d,1)-Hölder functions By Shen et al. (2022b, Theorem 2.4),
we have the lower minimax bound: if for each ε>0we have“the curse of irregularity”
sup
finf
ˆf∈F1sup
0≤x≤1|f(x)−ˆf(x)|≤ε⇒VC(F1)∈Ω(ε−d) (35)
where the supremum is taken over all f∈C1/d([0,1],R; 1). The familiar curse of dimensionality also expresses
the hardness to approximate an arbitrary 1-Lipschitz ( (1,1)-Hölder), thus relatively regular, function on
[0,1]n. As above, consider any model/hypothesis class F2of real-valued maps on Rdthen, again using Shen
et al. (2022b, Theorem 2.4), one has the lower-bound
sup
finf
ˆf∈F2sup
x∈[0,1]n∥f(x)−ˆf(x)∥≤ε⇒VC(F2)∈Ω(ε−d) (36)
where the supremum is taken over all f∈C1([0,1]n,R; 1). Comparing equation 35 and equation 36, we
find that the difficulty of uniformly approximating an arbitrary low-regularity ((1
d,1)-Hölder) function on
a1-dimensional domain is roughly just as complicated as approximating a relatively regular ( 1-Lipschitz)
function on a high-dimensional domain.
Incorporating theselower bounds withthe lower-bound in equation4, weinfer thatthe minimum numberlayers
(L)and minimal width (W)of each MLP approximating a low-regularity function on the low-dimensional
domain [0,1]is roughly the same as the minimal number of layers and width of an MLP approximating a
high-regularity map on the high-dimensional domain [0,1]n.
D Experimental details
We include here experimental details, we refer to the source code in the supplementary material for more
details. We first outline the algorithm used to train the MoMLP MoE model. We then provide details on the
trained architecture and hyperparameter details in the implementation.
D.1 Definitions of the Ackley and Rastrigin functions
Let us note x= (x1,...,xn)⊤∈Rnthen-dimensional representation of a sample, we use the following
formulation of the Ackley function:
Ackley(x) = 20 + exp(1)−aexp
−b/radicaltp/radicalvertex/radicalvertex/radicalbt1
nn/summationdisplay
i=1x2
i
−exp/parenleftigg
1
nn/summationdisplay
i=1cos(2πxi)/parenrightigg
(37)
wherea= 20andb= 0.2. We also use the following formulation of the Rastrigin function:
Rastrigin(x) =n/summationdisplay
i=1x2
i+ 10/parenleftigg
n−n/summationdisplay
i=1cos(2πxi)/parenrightigg
(38)
26Under review as submission to TMLR
D.2 Training Algorithm
We now provide an explanation for the training algorithm. As discussed, we mitigate down the algorithm into
two parts: discovering the prototypes and training the networks. Conceptually, the prototypes define where
in the input space the networks are located, or in other words, where in the input space we expect each of
the networks to have the best performance. During inference, we will route a given input to the appropriate
network based on its nearest prototype. In essence, each network learns to approximate a specific region of
the overall input domain.
Discovering prototypes. In principle, we may not know how to partition the input space. One approach is
to utilize standard clustering algorithms like K-means, but this might be suboptimal for the downstream
task unless we are already operating in a structured latent space, such as those found in pre-trained models
(further discussion on this is available in Section 6). Another way is to learn it via gradient descent by
optimizing the location of the prototypes for a specific task by following the gradient of the downstream loss.
At the beginning of training, we have ˆF(x)def.=(ˆf1(x),..., ˆfK(x))which contains a collection of Krandomly
initialized shallow or small networks (i.e., much smaller than our MoMLPs described later). In this first step,
we assume that we are able to load all randomly initialized networks into our GPU memory. In particular,
this is true because we use small networks with few parameters, which we will later “deepen” in the next step
by adding additional hidden layers. We initialize the prototypes pdef.=(p1,...,pK)randomly from a uniform
distribution within the bounds of our training dataset input samples. We use the following expression to
train the location of our prototypes {pk}K
k=1inRnby minimizing the energy:
/summationdisplay
(x,y)∈Dℓ/parenleftbig
softmax/parenleftbig
−∥x−pi∥K
i=1/parenrightbig⊤ˆF(x),y/parenrightbig
. (39)
where the loss ℓis task-specific; for example, one could use mean squared error for regression and cross-entropy
for classification. The softmax weights the importance of the prediction of each of the MoMLPs in ˆFfor a
given input x, as a function of the input’s distance to the prototypes, ∥x−pi∥K
i=1def.=(∥x−p1∥,...,∥x−pK∥).
Both the locations of the prototypes and the shallow randomly initialized neural networks assigned to them
are optimized.
Deepening the Networks. After the initial training phase, we enhance the networks by incorporating
additional layers. Specifically, we introduce linear layers with weights initialized to the identity matrix and
bias set to zero, just before the final output layer of each network. To encourage gradient flow in these new
layers during the subsequent training stage, we slightly perturb this initialization with small Gaussian noise.
This approach is driven by the fact that in the second training stage, each MoMLP can be optimized in a
distributed manner. Consequently, we can work with larger networks without the need to load all of them
simultaneously into our GPU, allowing for more model parameters. During the first stage, we have already
optimized our networks alongside the prototype locations, converging towards a minimum. By initializing the
networks with the new layers close to the identity, we can ensure that their output at the start of the second
stage of training is similar to that produced by the original networks. This allows us to smoothly continue
the optimization process from the point where we previously halted.
MoMLP Training. Once prototype locations have been fixed we can independently train MoMLPs
ˆf1,..., ˆfKby minimizing for all k∈{1,...,K}:
/summationdisplay
(x,y)∈D
k∈arg minj∈{1,...,K}{∥x−pj∥}ℓ(ˆfk(x),y) (40)
over all the networks. We optimize the MoMLP network ˆfkfor training data points that are closest to
prototypepk. The training procedure is summarized in Algorithm 2.
Inference. At inference time, each test sample xis assigned to its nearest prototype pkwherek∈
arg min
j∈{1,...,K}{∥x−pj∥}and the prediction is made by the k-th MoMLP ˆfk.
Comparison to Standard Distributed Training. One can distribute the complexity of feedforward
models by storing each of their layers in offline memory and then loading them sequentially into VRAM
27Under review as submission to TMLR
Algorithm 2: MoMLPs Training.
Require: Training dataDdef.={(xj,yj)}N
j=1, no. of prototypes K∈N+, loss function ℓ.
Discovering Prototypes:
(ˆF,p)←arg min
ˆF,p/summationdisplay
(x,y)∈Dℓ/parenleftbig
softmax (x|p)⊤ˆF(x),y/parenrightbig
Deepen networks:
Fork= 1,...,K:
ˆfk←deepen (ˆfk)
MoMLP Training:
Fork= 1,...,K:
ˆfk←arg min
ˆfk/summationtext
(x,y)∈D
k∈arg minj{∥x−pj∥}ℓ(ˆfk(x),y)
returnMoMLP parameters {ˆfk}K
k=1and prototype locations {pk}K
k=1.
during the forward pass. This does avoid loading more than O(Width )active parameters into VRAM at
any given time, where Widthdenotes the width of the feedforward model. However, doing so implies that
all the model parameters are ultimately loaded during the forward pass. This contrasts with the MoMLP
model, which requires O/parenleftbig
log2(K)Width2Depth/parenrightbig
to be loaded into memory during a forward pass; where
WidthandDepthare respectively the largest width and depth of the MLP at any leaf of the tree defining a
given MoMLP, and Kdenotes the number of prototypes. However, in the forward pass, one loads O(ε−n/2)
parameters for the best worst-case MLP while only O(nlog(1/ε)/ε)are needed in the case of the MoMLPs.
The number of parameters here represents the optimal worst-case rates for both models (see Table 8 and
Theorem 5.3).
D.3 Architectures and hyperparameters
Lastly, we detail the model architectures and hyperparameters used in our experiments.
D.3.1 MoMLPs
We set the width of our MoMLPs to w= 1000. In other words, each hidden layer of our MoMLPs contains a
linear matrix of size w×w.
In the regression task, our MoMLPs contain 3 hidden layers and we use a learnable PReLU as activation
function. For training, we use the Adam (Kingma & Ba, 2014) optimizer with a learning rate of 10−4and
the default Pytorch hyperparameters.
In the classification task, we follow the setup of Oquab et al. (2023) and use AdamW (Loshchilov & Hutter,
2019) as the optimizer with a learning rate of 10−3and the default parameters from PyTorch. Our MoMLPs
consist of four hidden layers for the classification task, and we apply BatchNorm1d before the PReLU
activation function.
D.3.2 Baseline
The baseline shares the same architecture as the MoMLPs described above. However, if we let Kdenote the
number of prototypes and assume that√
Kis a natural number, the width of the baseline is w√
K, ensuring
that the total number of hidden parameters matches that of all the MoMLPs combined. In our experiments,
we setK= 4, so√
K= 2.
D.4 Additional classification experiment
Instead of using the DINOv2 features as input to our model, we also conducted an experiment where the
32×32CIFAR RGB images were vectorized into 3072-dimensional vectors, which were then used as input to
28Under review as submission to TMLR
our model (recall that 32×32×3 = 3072 ). The elements of these vectors were normalized to lie within the
interval [0,1].
The accuracy scores, reported in Table 9, demonstrate that the performance of our approach is comparable to
that of the baseline while requiring less VRAM per expert. It is important to note, however, that since we are
using standard MLPs, the reported scores are lower than those achievable with neural networks specifically
designed for computer vision tasks.
Table 9: Test classification accuracy using the vectorized RGB images as input (average and standard deviation).
Dataset CIFAR-10 CIFAR-100
Ours (weighted) 46.79±0.91 20.82±0.29
Ours (unweighted) 46.09±0.74 20.62±0.28
Baseline 47.36±1.58 19.01±3.42
29