Published in Transactions on Machine Learning Research (04/2024)
Diﬀerential Equation Scaling Limits of
Shaped and Unshaped Neural Networks
Mufan (Bill) Li mufan.li@princeton.edu
Princeton University
Mihai Nica nicam@uoguelph.ca
University of Guelph and Vector Institute
Reviewed on OpenReview: https: // openreview. net/ forum? id= iRDwUXYsSJ
Abstract
Recent analyses of neural networks with shaped activations (i.e. the activation function is
scaled as the network size grows) have led to scaling limits described by diﬀerential equations.
However, these results do not a priori tell us anything about “ordinary” unshaped networks,
where the activation is unchanged as the network size grows. In this article, we ﬁnd similar
diﬀerential equation based asymptotic characterization for two types of unshaped networks.
•Firstly, we show that the following two architectures converge to the same inﬁnite-depth-
and-width limit at initialization: (i) a fully connected ResNet with a d−1/2factor on
the residual branch, where dis the network depth. (ii) a multilayer perceptron (MLP)
with depth d≪width nand shaped ReLU activation at rate d−1/2.
•Secondly, for an unshaped MLP at initialization, we derive the ﬁrst order asymptotic
correction to the layerwise correlation. In particular, if ρℓis the correlation at layer ℓ,
then qt=ℓ2(1−ρℓ)with t=ℓ
nconverges to an SDE with a singularity at t= 0.
These results together provide a connection between shaped and unshaped network
architectures, and opens up the possibility of studying the eﬀect of normalization methods
and how it connects with shaping activation functions.
1 Introduction
Martens et al. (2021);Zhang et al. (2022) proposed transforming the activation function to be more linear as
the neural network becomes larger in size, which signiﬁcantly improved the speed of training deep networks
without batch normalization. Based on the inﬁnite-depth-and-width limit analysis of Li et al. (2022), the
principle of these transformations can be roughly summarized as follows: choose an activation function
φs:R→Ras a perturbation of the identity map depending on the network width n(or depth d=n2p, p > 0)
φs(x) =x+1
nph(x) +O(n−2p) =x+1√
dh(x) +O(d−1), (1.1)
where for simplicity we will ignore the higher order terms for now. Li et al. (2022) also showed the limiting
multilayer perceptron (MLP) can be described by a Neural Covariance stochastic diﬀerential equation (SDE).
Furthermore, it appears the choice of p=1
2is necessary to reach a non-degenerate nor trivial limit, at least
when the depth-to-width ratiod
nconverges to a positive constant (see ( Li et al. ,2022, Proposition 3.4)).
Recently, Hayou & Yang (2023);Cirone et al. (2023) also studied the inﬁnite-depth-and-width limit of a
speciﬁc ResNet architecture ( He et al. ,2016). Most interestingly, the limit is described by an ordinary
diﬀerential equation (ODE) very similar to the neural covariance SDE. Furthermore, Hayou & Yang (2023)
showed the width and depth limits commute, i.e. no dependence on the depth-to-width ratiod
n. It is then
natural to consider a more careful comparison and understanding of the two diﬀerential equations.
1Published in Transactions on Machine Learning Research (04/2024)
4
 0 4 8
Transformed Correlation rt00.050.100.150.20Density
SDE (Ours) NN
Figure 1: Empirical distribution of the transformed correlation rt= log( ℓ2(1−ρℓ))for an unshaped ReLU
MLP, SDE sample density computed via kernel density estimation. Simulated with n=d= 150 , ρ0=
0.3, r0= log(1 −ρ0) = log(0 .7), SDE step size 10−2, and 213samples.
At the same time, Li et al. (2022) demonstrated the unshaped network also has an accurate approximation
via a Markov chain. Jakub & Nica (2023) further studied the large width asymptotics of the Markov chain
updates, where the transition kernel still depends on the width. Since the Markov chain quickly converges to
a ﬁxed point, it does not immediately appear to have a scaling limit. However, this motivates us to consider
a modiﬁed scaling around the ﬁxed point, so that we can recover a ﬁrst order asymptotic correction term.
In this note, we provide two technical results that address both of the above questions. Both of the results
are achieved by considering a modiﬁcation of the scaling, which leads to the following results.
•Firstly, we demonstrate that shaping the activation has a close connection to ResNets, and the
covariance ODE is in fact just the deterministic drift component of the covariance SDE. Furthermore,
in the limit as the scaled ratio ofd
n2pto converges to a positive constant and p∈(0,1
2), the shaped
MLP covariance also converges to the same ODE.
•Secondly, we analyze the correlation of an unshaped MLP, providing a derivation of the ﬁrst order
asymptotic correction. The correction term arises from rescaling the correlation ρℓin layer ℓby
qℓ=ℓ2(1−ρℓ), and we show it is closely approximated by an SDE.
The rest of this article is organized as follows. Firstly, we will provide a brief literature review in the rest
of this section. Next, we will review the most relevant known results on this covariance SDEs and ODEs in
Section 2. Then in Section 3, we will make the connection between shaping and ResNets precise. At the
same time, we will also provide a derivation of the unshaped regime in Section 4, where we show that by
modifying the scaling yet again, we can recover another SDE related to the correlation of a ReLU MLP.
1.1 Related Work
On a conceptual level, the main diﬃculty of analyzing neural networks is due to the lack of mathematical
tractability. In a seminal work, Neal (1995) showed that two layer neural networks at initialization converges
to a Gaussian process. Beyond the result itself, the conceptual breakthrough opened up the ﬁeld to analyzing
large size asymptotics of neural networks. In particular, this led to a large body of work on large or inﬁnite
width neural networks ( Lee et al. ,2018;Jacot et al. ,2018;Du et al. ,2019;Mei et al. ,2018;Sirignano &
Spiliopoulos ,2018;Yang ,2019;Bartlett et al. ,2021). However, majority of these results relied on the network
converging to a kernel limit, which are known to perform worse than neural networks ( Ghorbani et al. ,2020).
The gap in performance is believed to be primarily due to a lack of feature learning ( Yang & Hu ,2021;Abbe
2Published in Transactions on Machine Learning Research (04/2024)
Notation Description Notation Description
nin∈N Input dimension nout∈N Output dimension
n∈N Hidden layer width d∈N Number of hidden layers (depth)
φ(·) Base activation φs(·) Shaped activation
xα∈Rnin Input for 1≤α≤m W0∈Rnin×nWeight matrix at layer 0
zα
out∈RnoutNetwork output Wout∈Rn×noutWeight matrix at ﬁnal layer
zα
ℓ∈RnNeurons (pre-activation)
for layer 1≤ℓ≤dWℓ∈Rn×nWeight matrix at layer 1≤ℓ≤d
All weights initialized iid ∼ N (0,1)
φα
ℓ∈RnNeurons (post-activation)
for layer 1≤ℓ≤dc∈R Normalizing constant
c:=/parenleftbig
Eφ(g)2/parenrightbig−1forg∼ N (0,1)
Vαβ
ℓ∈R Covariancec
n⟨φα
ℓ, φβ
ℓ⟩ ραβ
ℓ∈[−1,1] Correlation Vαβ
ℓ//radicalbig
Vαα
ℓVββ
ℓTable 1: Notation
et al. ,2022;Ba et al. ,2022). While this motivated the study of several alternative scaling limits, in this
work we are mostly interested in the inﬁnite-depth-and-width limit.
First investigated by Hanin & Nica (2019b ), it was shown that not only does this regime not converge to a
Gaussian process at initialization, it also learns features Hanin & Nica (2019a ). This limit has since been
analyzed with transform based methods ( Noci et al. ,2021) and central limit theorem approaches ( Li et al. ,
2021). As we will describe in more detail soon, the result of most interest is the covariance SDE limit of Li
et al. (2022). The MLP results were also further extended to the transformer setting ( Noci et al. ,2023).
Thed−1/2scaling for ResNets was ﬁrst considered by Hayou et al. (2021), with the depth limit carefully
studied afterwards ( Hayou ,2022;Hayou & Yang ,2023;Hayou ,2023).Fischer et al. (2023) also arrived at the
same scaling through a diﬀerent theoretical approach. This scaling has found applications for hyperparameter
tuning ( Bordelon et al. ,2023;Yang et al. ,2023) when used in conjunction with the µP scaling ( Yang & Hu ,
2021).
Batch and layer normalization methods were introduced as a remedy for unstable training ( Ioﬀe & Szegedy ,
2015;Ba et al. ,2016), albeit theoretical analyses of these highly discrete changes per layer has been
challenging. A recent promising approach studies the isometry gap, and shows that batch normalization
methods achieves a similar eﬀect as shaping activation functions ( Meterez et al. ,2023). Theoretical
connections between these approaches using a diﬀerential equation based description remains an open
problem.
2Background on Shaped Networks and ResNets
Let{xα}m
α=1be a set of input data points in Rnin, and let zα
ℓ∈Rndenote the ℓ-th hidden layer with respect
to the input xα. We consider the standard width- ndepth- dMLP architecture with He-initialization ( He
et al. ,2015) deﬁned by the following recursion
zα
out=/radicalbiggc
nWoutφ(zα
d), zα
ℓ+1=/radicalbiggc
nWℓφs(zα
ℓ), zα
1=1√ninWinxα, (2.1)
where φs:R→Risthe activation function to be speciﬁed ,c−1=Eφs(g)2forg∼N(0,1),zα
ℓ∈Rn, zα
out∈
Rnout, and the matrices Wout∈Rnout×n, Wℓ∈Rn×n, W in∈Rn×ninare initialized with iid N(0,1)entries.
The main structure used to study neural networks at initialization, such as the neural network Gaussian
processes (NNGP) ( Neal,1995;Lee et al. ,2018), is on the conditional Gaussian property. More precisely, if
we condition on the previous layers Fℓ=σ((zα
k)α∈[m],k≤ℓ), we have that
[zα
ℓ+1]m
α=1|Fℓd=N/parenleftBig
0,c
n[⟨φα
ℓ, φβ
ℓ⟩]m
α,β=1⊗In/parenrightBig
, (2.2)
where we use the notation [·]m
α=1to vertically stack vectors, let φα
ℓ=φs(zα
ℓ)be the post activation hidden
layer, and let ⊗be the Kronecker product.
3Published in Transactions on Machine Learning Research (04/2024)
This naturally leads us to deﬁne the covariance matrix as Vℓ:=c
n[⟨φα
ℓ, φβ
ℓ⟩]m
α,β=1. The NNGP results
essentially reduces down to applying the Law of Large Numbers inductively to show the covariance Vαβ
ℓ
converges to its expect value. More precisely, if [zα
ℓ]m
α=1∼ N (0, Vℓ−1⊗In), then in the limit as n→ ∞
Vαβ
ℓ=c
nn/summationdisplay
i=1φs(zα
ℓ,i)φs(zβ
ℓ,i)d− →cEφs(gα)φs(gβ),where [gα, gβ]⊤∼ N/parenleftBigg
0,/bracketleftBigg
Vαα
ℓ−1Vαβ
ℓ−1
Vαβ
ℓ−1Vββ
ℓ−1/bracketrightBigg/parenrightBigg
.(2.3)
However, since the next layer covariance Vℓis a deterministic function of the previous Vℓ−1, this forms a ﬁxed
point type iteration Vℓ=f(Vℓ−1). Indeed, if we observe the correlations ραβ
ℓ=⟨φα
ℓ,φβ
ℓ⟩
|φα
ℓ| |φβ
ℓ|, which is bounded
in[−1,1], it does in fact converge to a ﬁxed point at ρ∞= 1for ReLU activations (see e.g. Proposition 3.4
(i)Li et al. (2022)).
This degeneracy of correlations also causes unstable gradients, which led to a proposal by Martens et al.
(2021);Zhang et al. (2022) to modify the shape of activation functions φsdepending on the size of the
network, leading to improved training speeds without using normalization methods. However, as both of
these works computed the activation shape based on a set of criteria, it is unclear what is the appropriate
modiﬁcation in the scaling limit as depth and width both approaches inﬁnity.
2.1 Shaped Limit of Neural Networks
To this end, Li et al. (2022) is the ﬁrst to describe the limit as d, n→ ∞ withd
n→T > 0and the activation
function φsisshaped at a precise rate to be closer to the identity as we increase n. In particular, the
covariance matrix Vℓforms a Markov chain, as it satisﬁes the deﬁnition of Vℓ+1|Fℓ=Vℓ+1|σ(Vℓ)(see e.g.
(2.3)). The main result describes the scaling limit of the Markov chain V⌊tn⌋via a stochastic diﬀerential
equation (SDE), which we can intuitively interpret as the Euler discretization converging to the diﬀerential
equation
Vℓ+1=Vℓ+b(Vℓ)
n+Σ(Vℓ)1/2ξℓ√n+O(n−3/2)n→∞− − − − → dVt=b(Vt)dt+ Σ(Vt)dBt, (2.4)
where ξℓare iid. zero mean identity variance random vectors, and we interpret1
nas the step size of the
discretization.
The activation functions are modiﬁed as follows. For a ReLU-like activation, we choose
φs(x) =s+max( x,0) +s−min(x,0), s ±= 1 +c±
np, c±∈R, p≥0, (2.5)
or for a smooth activation φ∈C4(R)such that φ(0) = 0 , φ′(0) = 1 andφ(4)(x)bounded by a polynomial,
we choose
φs(x) =s φ/parenleftBigx
s/parenrightBig
, s =anp, a̸= 0, p≥0. (2.6)
We will ﬁrst recall one of the main results of Li et al. (2022), which is stated informally1below.
Theorem 2.1 (Theorem 3.2 and 3.9 of Li et al. (2022), Informal) .Letp=1
2. Then in the limit as
d, n→ ∞ ,d
n→T > 0, and φsdeﬁned as above, we have that the upper triangular entries of V⌊tn⌋(ﬂattened
to a vector) converges to the following SDE weakly
dVt=b(Vt)dt+ Σ(Vt)1/2dBt, V 0=1
nin[⟨xα, xβ⟩]m
α,β=1, (2.7)
where Σ(V)|αβ,γδ =VαγVβδ+VαδVβγ, and if φis a ReLU-like activation we have
b(V)|αβ=ν(ραβ)√
VααVββ, ραβ=Vαβ
√
VααVββ, ν (ρ) =(c+−c−)2
2π/parenleftBig/radicalbig
1−ρ2−ρarccos ρ/parenrightBig
,(2.8)
1The statement is “informal” in the sense that we have stated what the ﬁnal limit is, but not the precise sense of the
convergence. See Appendix Afor a rigorous treatment of the convergence result.
4Published in Transactions on Machine Learning Research (04/2024)
or else if φis a smooth activation we have
bαβ(Vt) =φ′′(0)2
4a2/parenleftBig
Vαα
tVββ
t+Vαβ
t(2Vαβ
t−3)/parenrightBig
+φ′′′(0)
2a2Vαβ
t(Vαα
t+Vββ
t−2). (2.9)
While the formulae may seem overwhelming, there is actually a fairly straight forward interpretation of
both the drift band the diﬀusion coeﬃcient Σ1/2. In particular, for the unshaped ReLU network, that is
φs(x) = max( x,0), the deterministic component of the update for the covariance matrix compared to the
shaped network are as follows
Unshaped: EVℓ+1−Vℓ∝b(Vℓ),
Shaped: EVℓ+1−Vℓ∝b(Vℓ)
n.(2.10)
Eﬀectively, the drift component got slowed down by a multiplicative factor of1
n, which can interpreted as
an Euler discretization step. In order to achieve a stable limit as we take depth dto inﬁnity, we would
also require each layer to contribute inﬁnitesimally as a rate proportional to1
d, therefore this is a desired
rescaling.
The diﬀusion coeﬃcient is much more interesting. Since we can interpret this diﬀusion to be on the manifold
of symmetric positive deﬁnite matrices, we would expect the diﬀusion coeﬃcient of Brownian motions on this
manifold to correspond to a Riemannian metric. Indeed, this is shown in Li et al. (2024), as Σ−1corresponds
to the aﬃne invariant metric. More precisely, for all V∈SPD( m), the inner product corresponding to Σ−1
⟨A, B⟩Σ−1(V)=/summationdisplay
1≤i≤j≤mAijBijΣ−1(V)ij=1
2Tr(AV−1BV−1),for all A, B∈Sym( m). (2.11)
Furthermore, this gives the linear network SDE dVt= Σ(Vt)1/2, dB tan interpretation as the dual Brownian
motion in information geometry, which uses a pair of dually ﬂat aﬃne connections instead of the standard
Levi-Civita connection.
2.2 ODE Limit of Residual Networks
At the same time, Hayou & Yang (2023);Cirone et al. (2023) found an ordinary diﬀerential equation (ODE)
limit describing the covariance matrix for inﬁnite-depth-and-width ResNets. The authors considered a
ResNet architecture with a1√
dfactor on their residual branch, more precisely their recursion is deﬁned
as follows (in our notation and convention)
zα
ℓ+1=zα
ℓ+1√
dnWℓφ(zα
ℓ),where φ(x) = max( x,0). (2.12)
Intuitively, this is similar to shaping activations, it also weakens the eﬀect of each layer as we take dto
inﬁnity. This will be discussed in more details in the following section.
One of their main results can be stated informally as follows.
Theorem 2.2 (Theorem 2 of Hayou & Yang (2023), Informal) .Letd, n→ ∞ (in any order), and the
covariance process Vαβ
⌊td⌋converges to the following ODE
d
dtVαβ
t=1
2f(ραβ
t)
ραβ
tVαβ
t, (2.13)
where f(ρ) =1
π(ρarcsin ρ+/radicalbig
1−ρ2) +1
2ρ.
Here, we observe this ODE ( 2.13) is exactly the drift component of the covariance SDE ( 2.7), i.e.
1
2f(ραβ
t)
ραβ
tVαβ
t=ν(ραβ
t)/radicalBig
Vαα
tVββ
t,if(c+−c−)2= 1,d
n= 1. (2.14)
5Published in Transactions on Machine Learning Research (04/2024)
To see this, we just need to use the identity arcsin ρ=π
2−arccos ρto get that1
2f(ρ) =ν(ρ), and that
Vαβ
t
ραβ
t=/radicalBig
Vαα
tVββ
tis exactly the deﬁnition of ραβ
t. We also note the correlation ODEd
dtραβ
t=ν(ραβ
t)was
ﬁrst derived in ( Zhang et al. ,2022, Proposition 3), where they considered the sequential width then depth
limit with a ﬁxed initial and terminal condition.
In the next section, we will describe another way to recover this ODE from an alternative scaling limit of
the shaped MLP.
3 An Alternative Shaped Limit for p∈(0,1
2)
We start by providing some intuitions on this result. The shaped MLP can be seen as a layerwise perturbation
of the linear network
zℓ+1=/radicalbiggc
nWℓφs(zℓ)≈/radicalbiggc
nWℓzℓ+1√
d/radicalbiggc
nWℓh(zℓ), (3.1)
where c−1=Eφs(g)2forg∼N(0,1)corresponds to the He-initialization ( He et al. ,2015), and Wℓ∈Rn×n
has iid N(0,1)entries.
On an intuitive level (which we will make precise in Remark 3.3), if we take the inﬁnite-width limit ﬁrst,
then this removes the eﬀect of the random weights. In other words, if we replace the weights1√nWℓwith the
identity matrix Inin each hidden layer, we get the same limit at initialization. Therefore, we can heuristically
write
zℓ+1≈zℓ+1√
dh(zℓ), (3.2)
where we also used the fact c→1in the limit.
Observe this is resembling a ResNet, where the ﬁrst zℓis the skip connection. In fact, we can again
heuristically add back in the weights on the residual branch to get
zℓ+1≈zℓ+1√
dWℓh(zℓ), (3.3)
which exactly recovers the ResNet formulation of Hayou et al. (2021);Hayou & Yang (2023), where the
authors studied the case when h(x) = max( x,0)is the ReLU activation.
Remark 3.1.On a heuristic level, this implies that whenever the width limit is taken ﬁrst (or equivalently
d=n2pforp∈(0,1/2)), the shaped network with shaping parameter d−1/2hasthe same limiting distribution
at initialization as a ResNet with a d−1/2weighting on the residual branch.
However, we note that despite having identical ODE for the covariance at initialization, this does not imply
the training dynamics will be the same — it will likely be diﬀerent. Furthermore, since Hayou & Yang
(2023) showed the width and depth limits commute for ResNets, this provides the additional insight that
noncommunitativity of limits in shaped MLPs arises from the product of random matrices.
3.1 Precise Results
The core object that forms a Markov chain is the post-activation covariance matrixc
n[⟨φα
ℓ, φβ
ℓ⟩]m
α,β=1. To see
this, we will use the property of Gaussian matrix multiplication, where we let W∈Rn×nwith iid entries
Wij∼N(0,1), and{uα}m
α=1∈Rnbe a collection of constant vectors, which gives us
[Wuα]m
α=1d=N(0,[⟨uα, uβ⟩]m
α=1⊗In), (3.4)
where we use the notation [vα]m
α=1to stack the vectors vertically. This forms a Markov chain because we
can condition on Fℓ=σ([zα
ℓ]m
α=1)to get
[zα
ℓ+1]m
α=1|Fℓ= [zα
ℓ+1]m
α=1|σ(Vℓ)∼N(0, Vℓ⊗In). (3.5)
and we can see that Vℓ+1|Fℓ=Vℓ+1|σ(Vℓ), which is exactly the deﬁnition of a Markov chain.
6Published in Transactions on Machine Learning Research (04/2024)
We will start by deriving the precise Markov chain update up to a term of size O(n−3p), which will be a
slight modiﬁcation of the Euler discretization we saw in ( 2.4).
Lemma 3.2 (Covariance Markov Chain for the Shaped MLP) .Letzα
ℓbe the MLP in deﬁned in ( 2.1) with
shaped ReLU activations deﬁned in ( 2.5). For p∈(0,1
2)andd=n2p, the Markov chain satisﬁes
Vℓ+1=Vℓ+b(Vℓ)
d+Σs(Vℓ)1/2ξℓ√n+O(d−3/2), (3.6)
where {ξℓ}ℓ≥0are iid zero mean and identity covariance random vectors, and in the limit as n, d→ ∞ we
have that Σs→Σ, and the coeﬃcients are deﬁned as
b(V)αβ=ν(ραβ)√
VααVββ,Σ(V)αβ,γδ=VαγVβδ+VαδVβγ, (3.7)
where ραβ=Vαβ
√
VααVββ, and ν(ρ) =(c+−c−)2
2π/parenleftBig/radicalbig
1−ρ2+ρarccos ρ/parenrightBig
.
Proof. To start, we will observe that conditioned on Fℓ, we have that
Vαβ
ℓ+1|Fℓ=c
nn/summationdisplay
i=1φs(zα
ℓ+1,i)φs(zβ
ℓ+1,i) =|φα
ℓ||φβ
ℓ|c
nn/summationdisplay
i=1φs(gα
i)φs(gβ
i), (3.8)
where used the fact that zℓ+1are jointly Gaussian, and we have that
/bracketleftbigggα
i
gβ
i/bracketrightbiggn
i=1∼ N/parenleftbigg
0,/bracketleftbigg
1ραβ
ℓ
ραβ
ℓ1/bracketrightbigg
⊗In/parenrightbigg
, (3.9)
forραβ=Vαβ
√
VααVββ. At this point, we can deﬁne K1(ραβ) =Eφs(gα
i)φs(gβ
i)and the random variable
Rαβ
ℓ=1√nn/summationdisplay
i=1cφs(gα
i)φs(gβ
i)−cK1(ραβ
ℓ), (3.10)
which allows us to decompose this into a deterministic and a random component as
Vαβ
ℓ+1|Fℓ=|φα
ℓ||φβ
ℓ|/parenleftbigg
cK1(ραβ
ℓ) +1√nRαβ
ℓ/parenrightbigg
. (3.11)
We can Taylor expand cK1(ρ)in terms of n−pto get the following result from Lemma B.1
cK1(ρ) =ρ+ν(ρ)
n2p+O(n−3p), ν (ρ) =(c+−c−)2
2π/parenleftBig/radicalbig
1−ρ2+ρarccos ρ/parenrightBig
. (3.12)
Similarly from Lemma B.3, we also have the approximation
ERαβ
ℓRγδ
ℓ=ραγ
ℓρβδ
ℓ+ραδ
ℓρβγ
ℓ+O(n−p). (3.13)
Putting everything together, we can write
cK1(ραβ
ℓ) +1√nRαβ
ℓ=ραβ
ℓ+ν(ραβ
ℓ)
n2p+1√nRαβ
ℓ+O(n−3p), (3.14)
which implies we can write
Vαβ
ℓ+1=Vαβ
ℓ+b(Vℓ)αβ
n2p+Rαβℓ
√n+O(n−3p), (3.15)
where b(V) =ν(ραβ
ℓ)/radicalBig
Vαα
ℓVββ
ℓ. Now taking the upper triangular entries of Vℓas a vector, we have that
Vℓ+1=Vℓ+b(Vℓ)
n2p+Σs(Vℓ)ξℓ√n+O(n−3p), (3.16)
where Σs(V)|αβ,γδ =√
VααVββ(ραγρβδ+ραδρβγ) +O(n−p) =VαγVβδ+VαδVβγ+O(n−p), and ξℓis a zero
mean identity covariance random vector. We recover the exact desire result from writing d=n2p.
7Published in Transactions on Machine Learning Research (04/2024)
Remark 3.3.We note that the drift term arising from the activation depends only on the depth dand
the random term only depends on the width n. If we decouple the dependence on dandn, and take the
inﬁnite-width limit ﬁrst, we arrive at
Vℓ+1=Vℓ+bs(Vℓ)
d+O(n−3/2), (3.17)
which is equivalent to removing the randomness of the weights.
We note this Markov chain behaves like a sum of two Euler updates with step sizes1
n2pand1
n, where the
1
ncorresponds to the random term with coeﬃcient1√n. However since p∈(0,1
2), the ﬁrst term with step
size1
n2pwill dominate, which is the term that corresponds to shaping the activation function. Therefore, we
expect the random term to vanish in the limit, and hence leaving us with an ODE only. We make this result
precise in the following Proposition.
Proposition 3.4 (Covariance ODE for the Shaped ReLU MLP) .Letp∈(0,1
2). Then in the limit as
d, n→ ∞ ,d
n2p→1, and φsis the shaped ReLU deﬁned in ( 2.5), we have that the upper triangular entries
ofV⌊tn⌋(ﬂattened to a vector) converges to the following ODE weakly with respect to the Skorohod topology
ofDR+,Rm(m+1)/2
dVt=b(Vt)dt , V 0=1
nin[⟨xα, xβ⟩]m
α,β=1, (3.18)
where bis deﬁned in Theorem 2.1and Lemma 3.2.
Proof. Starting with the Markov chain in Lemma 3.2, we will treat the random term of order O(n−1/2)as
part of the drift instead. More precisely, we let
/hatwideb(V)
n2p=b(V)
n2p+Σs(V)1/2ξℓ√n, (3.19)
which in expectation just equals to b(V)n−2p. Since there is no random term at the order of n−p, we can
apply Proposition A.7to the special as if the diﬀusion coeﬃcient is equal to zero. At the same time, since
the higher order term in the Markov chain is at the desired order of O(n−3p), which will vanish in the limit,
we get the desired result.
At this point it’s worth pointing out the regime of p∈(0,1
2)was studied in Li et al. (2022), but the scaling
limit was taken to bed
n→Tinstead ofd
n2p. This led to a “degenerate” regime where ρt= 1for all t >0.
The above ODE result implies that the degenerate limit can be characterized in a more reﬁned way if the
scaling is chosen carefully.
In the next and ﬁnal section, we show that actually even when the network is unshaped (i.e. p= 0), there
exists a scaling such that we can characterize the limiting Markov chain up to the ﬁrst order asymptotic
correction.
4 An SDE for the Unshaped ReLU MLP
In this section, we let φs(x) =φ(x) = max( x,0), and we are interested in studying the correlation
ραβ
ℓ=Vαβ
ℓ/radicalBig
Vαα
ℓVββ
ℓ=⟨φα
ℓ, φβ
ℓ⟩
|φα
ℓ||φβ
ℓ|. (4.1)
From this point onwards, we will only consider the marginal over two inputs, so we will drop the superscript
αβ. Similar to the previous section, we will also start by providing an intuitive sketch.
Many existing work has derived the rough asymptotic order of the unshaped correlation to be ρℓ= 1−O(ℓ−2),
where ℓis the layer (see for example Appendix E of Li et al. (2022) and Jakub & Nica (2023)). Firstly, this
8Published in Transactions on Machine Learning Research (04/2024)
implies that a Taylor expansion of all functions of ρin the Markov chain update around ρ= 1will be very
accurate. At the same time, it is natural to magnify the object inside the big Oby reverting the scaling, or
more precisely consider the object
qℓ=ℓ2(1−ρℓ), (4.2)
which will hopefully remain at size Θ(1).
For simplicity, we can consider the inﬁnite-width update of the unshaped correlation (which corresponds to
the zeroth-order Taylor expansion in1
n)
ρℓ+1=ρℓ+c1(1−ρℓ)3/2+O((1−ρℓ)5/2), (4.3)
where for the sake of illustration we will take c1= 1and drop the big Oterm for now. Substituting in qℓ,
we can recover the update
qℓ+1=qℓ+2qℓ
ℓ−q3/2
ℓ
ℓ. (4.4)
While this doesn’t quite look like an Euler update just yet, we can substitute in t=ℓ
nfor the time scale,
which will lead us to have
qℓ+1=qℓ+1
tn/parenleftBig
2qℓ−q3/2
ℓ/parenrightBig
, (4.5)
hence (heuristically) giving us the singular ODE
dqt=2qt−q3/2
t
t. (4.6)
To recover the SDE, we will simply include the additional terms of the Markov chain instead taking the
inﬁnite-width limit ﬁrst.
4.1 Full Derivation
In the rest of this section, we will provide a derivation of an SDE arising from an appropriate scaling of ρℓ.
Theorem 4.1 (Rescaled Correlation) .Letqℓ=ℓ2(1−ρℓ). Then for all t0>0, the process {q⌊tn⌋}t≥t0
converges to a solution of the following SDE weakly in the Skorohod topology (see ( Li et al. ,2022, Appendix
A))
dqt= 2qt/parenleftBigg
1−√
2
3πq1/2
t
t−1/parenrightBigg
dt+ 2√
2qtdBt. (4.7)
The above statement holds only when t0>0, and there is a interesting technicality that must be resolved
to interpret what happens as t→0+. In particular, the Markov chain is not time homogeneous, and the
limiting SDE has a singularity at t= 0. The contribution of the singularity needs to be controlled in order
to establish convergence for all t≥0. Furthermore, due to the singularity issue, it is also unclear what the
initial condition of qtshould be.
In our simulations for Figure 1, we addressed the time singularity by shifting the time evaluation of1
tto
the next step of1
t+∆t, where ∆t>0is the time step size. More precisely, we ﬁrst consider the log version
ofrt= log qt
drt=−2/parenleftBigg
1−1−√
2
3πexp(rt
2)
t/parenrightBigg
dt+ 2√
2dBt. (4.8)
Then we choose the following discretization
rt+∆t=rt−2/parenleftBigg
1−1−√
2
3πexp(rt
2)
t+ ∆ t/parenrightBigg
∆t+ 2√
2ξt/radicalbig
∆t, (4.9)
where ξt∼N(0,1). For initial conditions, we also noticed that since the initial correlation must be contained
in the interval [−1,1], the end result was not very sensitive to the choice of r0.
9Published in Transactions on Machine Learning Research (04/2024)
Proof of Theorem 4.1.Firstly, we will introduce the deﬁnitions
Kp,r(ρ) =Eφs(g)pφs(ˆg)r, (4.10)
where g, w∼N(0,1)and we deﬁne ˆg=ρg+qwwith q=/radicalbig
1−ρ2. We will also use the short hand notation
to write Kp:=Kp,p. Here we will recall several formulae calculated in Cho & Saul (2009) and ( Li et al. ,
2022, Lemma B.4)
K0(ρ) =E1{g>0}1{ρg+qw> 0}=arccos( −ρ)
2π,
K1(ρ) =Eφ(g)φ(ρg+qw) =q+ρarccos( −ρ)
2π,
K2(ρ) =Eφ(g)2φ(ρg+qw)2=3ρq+ arccos( −ρ)(1 + 2 ρ2)
2π,
K3,1(ρ) =Eφ(g)3φ(ρg+qw) =q(2 +ρ2) + 3 arccos( −ρ)ρ
2π. .(4.11)
Furthermore, we will deﬁne
M2:=E[cφ(g)2−1]2= 5. (4.12)
Using the steps of ( Li et al. ,2022, Proposition B.8), we can establish an approximate Markov chain
ρℓ+1=cK1(ρℓ) +/hatwideµr(ρℓ)
n+σr(ρℓ)ξℓ√n+O(n−3/2), (4.13)
where ξℓare iid with zero mean and unit variance, and
µr(ρℓ) =E[/hatwideµr(ρℓ)|ρℓ] =c
4/bracketleftbig
K1(c2K2+ 3M2+ 3)−4cK3,1/bracketrightbig
,
σ2
r(ρℓ) =c2
2/bracketleftbig
K2
1(c2K2+M2+ 1)−4cK1K3,1+ 2K2/bracketrightbig
,(4.14)
and we write K·=K·(ρℓ).
Here we use the big O(f(n, ℓ))to denote a random variable Xsuch that for all p≥1
E|X|p
f(n, ℓ)p≤Cp<∞, (4.15)
for some constants Cp>0independent of nandℓ.
In view of the SDE convergence theorem Proposition A.7, if we eventually reach an SDE, we will only need to
keep track of the expected drift µrinstead of the random drift. We can then Taylor expand the coeﬃcients in
terms of ρℓabout ρℓ= 1(from the negative direction) using SymPy ( Meurer et al. ,2017), which translates
to the following update rule
ρℓ+1=ρℓ+2√
2
3π(1−ρℓ)3/2+√
2
30π(1−ρℓ)5/2
+1
n/parenleftbigg
−2(1−ρℓ) +4√
2
π(1−ρℓ)3/2+ 3(1 −ρℓ)2−73√
2
15π(1−ρℓ)5/2/parenrightbigg
+ξℓ√n/parenleftbigg
2√
2(1−ρℓ)−56
15π(1−ρℓ)3/2/parenrightbigg
+O((1−ρℓ)4+n−3/2).(4.16)
We note up to this point, a similar approach was taken in Jakub & Nica (2023). However, we will diverge
here by consider the following scaling
qℓ=ℓ2(1−ρℓ). (4.17)
10Published in Transactions on Machine Learning Research (04/2024)
The choice of ℓ2scale is motivated by the inﬁnite-width limit, where (1−ρℓ)shown to be of order O(ℓ−2)
in (Li et al. ,2022, Appendix E). This implies the following update
ℓ2
(ℓ+ 1)2qℓ+1=qℓ−ℓ22√
2
3π(1−ρℓ)3/2−ℓ2√
2
30π(1−ρℓ)5/2
−ℓ2
n/parenleftbigg
−2(1−ρℓ) +4√
2
π(1−ρℓ)3/2+ 3(1 −ρℓ)2−73√
2
15π(1−ρℓ)5/2/parenrightbigg
+ξℓℓ2
√n/parenleftbigg
2√
2(1−ρℓ)−56
15π(1−ρℓ)3/2/parenrightbigg
+O(ℓ−8q4
ℓ+ℓ−2n−3/2).(4.18)
Next, we will drop all higher order terms in ℓ, then using the fact thatℓ2
(ℓ+1)2= 1−2
ℓ+O(ℓ−2), we can write
qℓ+1=qℓ(1 + 2 ℓ−1)−2√
2
3πq3/2
ℓ
ℓ−2qℓ
n+ξℓ√n2√
2qℓ+O(ℓ−2+n−1/2ℓ−1), (4.19)
where we dropped ℓ−2n−3/2in the big Osince it gets dominated by ℓ−2.
Choosing the time scaling t=ℓ
nthen gives us
qℓ+1=qℓ+2
nqℓ/parenleftBigg
1−√
2
3πq1/2
ℓ
t−1/parenrightBigg
+ 2/radicalbigg
2
nqℓξℓ+O(t−2n−2+t−1n−3/2). (4.20)
Finally, we will use the Markov chain convergence result to an SDE result Proposition A.7, which leads to
the desired SDE for t≥t0>0
dqt= 2qt/parenleftBigg
1−√
2
3πq1/2
t
t−1/parenrightBigg
dt+ 2√
2qtdBt. (4.21)
5Discussion
In this section, we provide some discussion on the potential impact of this work, from both a practical and
a theoretical point of view.
Stable Training via Depth Scaling. Martens et al. (2021) make the key observation that as depth
increases, the increased instability of training dynamics is due to the key role of nonlinear activation functions.
Since then, it is better understood that to achieve stable training in large depth, it is necessary to weaken
the nonlinearities of each layer ( Noci et al. ,2023;Bordelon et al. ,2023;Yang et al. ,2023). Our results
observe a key connection between two strategies, either weakening the activation function, or weakening
the entire layer via skip connections directly. From a practical point of view, since both the shaping and
ResNet approaches can lead to the same covariance ODE at initialization, we understand that the key to
preventing unstable gradients is weakening nonlinearities. To choose between the two regimes, it is therefore
left to study the role of weakening the weight matrix or otherwise, and how this aﬀects training dynamics.
In particular, we note that the shaped limit admits feature learning without modifying the scaling ( Hanin
& Nica ,2019a ), which is fundamentally diﬀerent than the µP regime ( Yang & Hu ,2021).
Analysis of Normalization Methods. Since the correlation Markov chain has large jumps, and quickly
to a degenerate ﬁxed point, it is intuitive to assume this chain does not admit a continuous time limit, and
therefore diﬃcult to analyze from an analytical approach. Indeed, this is the approach taken by Meterez
et al. (2023), which yielded one of the ﬁrst analysis of normalization methods in a deep network. However,
our result provides a counter-intuitive understanding: it remains possible to analyze seemingly large discrete
jumps in a Markov chain if rescaled appropriately. In our case, since the Markov chain converges quickly to
11Published in Transactions on Machine Learning Research (04/2024)
the ﬁxed point at ρ= 1, zooming in around the ﬁxed point as a function of time (or layer ℓ) allows us to view
the dynamics at the correct scale. This overcomes a previously known technical hurdle, and opens up the
possibility of analyzing normalization methods, which is still lacking theoretical progress. More speciﬁcally,
we would like to understand how normalization methods may help or hurt performance in practice, and how
to best choose and tune these methods given their many variants, all of which are now made easier by our
scaling approach.
Foundation for Studying Training Dynamics. Until recently, almost all of the work on inﬁnite-
depth neural networks remain at initialization. This is not due to a lack of attempts, but rather due to
a lack of mathematical techniques available. In particular, we also emphasize the seminal work of neural
tangent kernels for training dynamics ( Jacot et al. ,2018) is entirely built on the same techniques studying
initialization ( Neal,1995;Lee et al. ,2018). For this reason, it is important to slowly yet ﬁrmly build
up a foundation of theoretical results, that understands as much structure as possible at initialization.
To this goal, the key distinction from the inﬁnite-width regime is that each layer must be viewed as an
inﬁnitesimal discretization of a continuous “layer time. ” This approach is what helped yield some of the ﬁrst
characterization of training dynamics for inﬁnite-depth ResNets ( Bordelon et al. ,2023;Yang et al. ,2023). In
fact, any theory of training dynamics must also account for this inﬁnitesimal treatment of layers, otherwise
the limit cannot possibly be stable. Therefore, we view this line of work as building towards a theory of
training dynamics, and eventually generalization as well.
References
Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary
and nearly suﬃcient condition for sgd learning of sparse functions on two-layer neural networks. In
Conference on Learning Theory , pp. 4782–4887. PMLR, 2022.
Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. High-dimensional
asymptotics of feature learning: How one gradient step improves the representation. arXiv preprint
arXiv:2205.01445 , 2022.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoﬀrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450 , 2016.
Peter L Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical viewpoint. Acta
numerica , 30:87–201, 2021.
Blake Bordelon, Lorenzo Noci, Mufan Bill Li, Boris Hanin, and Cengiz Pehlevan. Depthwise hyperparameter
transfer in residual networks: Dynamics and scaling limit, 2023.
Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in Neural Information
Processing Systems (NeurIPS) , pp. 342–350, 2009.
Nicola Muca Cirone, Maud Lemercier, and Cristopher Salvi. Neural signature kernels as inﬁnite-width-depth-
limits of controlled resnets. arXiv preprint arXiv:2303.17671 , 2023.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds global minima of
deep neural networks. In Int. Conf. Machine Learning (ICML) , pp. 1675–1685. PMLR, 2019.
Stewart N Ethier and Thomas G Kurtz. Markov processes: characterization and convergence . John Wiley
& Sons, 2009.
Kirsten Fischer, David Dahmen, and Moritz Helias. Optimal signal propagation in resnets through residual
scaling. arXiv preprint arXiv:2305.07715 , 2023.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural networks
outperform kernel methods? Advances in Neural Information Processing Systems , 33:14820–14830, 2020.
Boris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel. In Int. Conf.
Learning Representations (ICLR) , 2019a.
12Published in Transactions on Machine Learning Research (04/2024)
Boris Hanin and Mihai Nica. Products of many large random matrices and gradients in deep neural networks.
Communications in Mathematical Physics , pp. 1–36, 2019b.
Souﬁane Hayou. On the inﬁnite-depth limit of ﬁnite-width neural networks. Transactions on Machine
Learning Research , 2022.
Souﬁane Hayou. Commutative width and depth scaling in deep neural networks, 2023.
Souﬁane Hayou and Greg Yang. Width and depth limits commute in residual networks. arXiv preprint
arXiv:2302.00453 , 2023.
Souﬁane Hayou, Eugenio Clerico, Bobby He, George Deligiannidis, Arnaud Doucet, and Judith Rousseau.
Stable ResNet. In Int. Conf. Artiﬁcial Intelligence and Statistics (AISTATS) , pp. 1324–1332. PMLR,
2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-
level performance on imagenet classiﬁcation. In Proc. IEEE Int. Conf. Computer Vision , pp. 1026–1034,
2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016.
Sergey Ioﬀe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In International conference on machine learning , pp. 448–456. pmlr, 2015.
Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. In Advances in Information Processing Systems (NeurIPS) , 2018.
Cameron Jakub and Mihai Nica. Depth degeneracy in neural networks: Vanishing angles in fully connected
relu networks on initialization. arXiv preprint arXiv:2302.09712 , 2023.
O. Kallenberg. Foundations of Modern Probability . Probability theory and stochastic modelling. Springer,
2021. ISBN 9783030618728.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeﬀrey Pennington, and Jascha Sohl-
Dickstein. Deep neural networks as gaussian processes. In Int. Conf. Learning Representations (ICLR) ,
2018.
Mufan Li, Mihai Nica, and Dan Roy. The future is log-gaussian: Resnets and their inﬁnite-depth-and-width
limit at initialization. Advances in Neural Information Processing Systems , 34, 2021.
Mufan Li, Jaume de Dios Pont, Mihai Nica, and Daniel M. Roy. Geometric dyson brownian motion and the
free log-normal for minor of products of random matrices. In Preparation, 2024.
Mufan Bill Li, Mihai Nica, and Daniel M Roy. The neural covariance sde: Shaped inﬁnite depth-and-width
networks at initialization. arXiv preprint arXiv:2206.02768 , 2022.
James Martens, Andy Ballard, Guillaume Desjardins, Grzegorz Swirszcz, Valentin Dalibard, Jascha Sohl-
Dickstein, and Samuel S Schoenholz. Rapid training of deep neural networks without skip connections or
normalization layers using deep kernel shaping. arXiv preprint arXiv:2110.01765 , 2021.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean ﬁeld view of the landscape of two-layer
neural networks. Proceedings of the National Academy of Sciences , 115(33):E7665–E7671, 2018.
Alexandru Meterez, Amir Joudaki, Francesco Orabona, Alexander Immer, Gunnar Rätsch, and Hadi
Daneshmand. Towards training without depth limits: Batch normalization without gradient explosion,
2023.
13Published in Transactions on Machine Learning Research (04/2024)
Aaron Meurer, Christopher P. Smith, Mateusz Paprocki, Ondřej Čertík, Sergey B. Kirpichev, Matthew
Rocklin, AMiT Kumar, Sergiu Ivanov, Jason K. Moore, Sartaj Singh, Thilina Rathnayake, Sean Vig,
Brian E. Granger, Richard P. Muller, Francesco Bonazzi, Harsh Gupta, Shivam Vats, Fredrik Johansson,
Fabian Pedregosa, Matthew J. Curry, Andy R. Terrel, Štěpán Roučka, Ashutosh Saboo, Isuru Fernando,
Sumith Kulal, Robert Cimrman, and Anthony Scopatz. Sympy: symbolic computing in python. PeerJ
Computer Science , 3:e103, January 2017. ISSN 2376-5992. doi: 10.7717/peerj-cs.103. URL https://doi.
org/10.7717/peerj-cs.103 .
Radford M Neal. Bayesian learning for neural networks , volume 118. Springer Science & Business Media,
1995.
Lorenzo Noci, Gregor Bachmann, Kevin Roth, Sebastian Nowozin, and Thomas Hofmann. Precise
characterization of the prior predictive distribution of deep relu networks. Advances in Neural Information
Processing Systems , 34, 2021.
Lorenzo Noci, Chuning Li, Mufan Bill Li, Bobby He, Thomas Hofmann, Chris Maddison, and Daniel M
Roy. The shaped transformer: Attention models in the inﬁnite depth-and-width limit. arXiv preprint
arXiv:2306.17759 , 2023.
Justin Sirignano and Konstantinos Spiliopoulos. Mean ﬁeld analysis of neural networks: A law of large
numbers, 2018.
Daniel W Stroock and SR Srinivasa Varadhan. Multidimensional diﬀusion processes , volume 233. Springer
Science & Business Media, 1997.
Greg Yang. Tensor programs i: Wide feedforward or recurrent neural networks of any architecture are
gaussian processes, 2019.
Greg Yang and Edward J. Hu. Feature learning in inﬁnite-width neural networks. In Int. Conf. Machine
Learning (ICML) , 2021.
Greg Yang, Dingli Yu, Chen Zhu, and Souﬁane Hayou. Tensor programs vi: Feature learning in inﬁnite-depth
neural networks, 2023.
Guodong Zhang, Aleksandar Botev, and James Martens. Deep learning without shortcuts: Shaping the
kernel with tailored rectiﬁers. arXiv preprint arXiv:2203.08120 , 2022.
14Published in Transactions on Machine Learning Research (04/2024)
A Background on Markov Chain Convergence to SDEs
In this section, we will review the main technical results used for Markov chain convergence to SDEs. In
particular, we will consider piecewise constant interpolations of Markov chains in continuous time deﬁned by
V(n)
⌊tn⌋, which converges to a continuous process Vt. Due to measurability concerns of the Markov chain with
jumps, in order to deﬁne weak convergence on the space of processes, we need to deﬁne a precise space along
with its equipped topology ﬁrst introduced by Skorohod. We will mostly follow the references Kallenberg
(2021);Ethier & Kurtz (2009);Stroock & Varadhan (1997) in the rest of this section, and the content will
be essentially a rephrasing of Appendix A in Li et al. (2022).
To start, we will let Sbe a complete separable metric space, where the stochastic processes takes value in
(in our case, it’s the upper triangular entries Rm(m+1)/2). Let DR+,Sbe the space of càdlàg functions, i.e.
functions that are right continuous with left limits, from R+toS. For xn∈R+, we use xnul− →xto denote
locally uniform convergence, or uniform convergence on compact subsets of R+. We also consider a class
of bijections λonR+such that λis strictly increasing, and λ0= 0. We deﬁne Skorohod convergence ,
written as xns− →xonDR+,S, if there exists a sequence of such bijections λnsuch that
λnul− →Id, x n◦λnul− →x . (A.1)
Intuitively, this allows us to side step any discontinuities in the notion of convergence, as λnis allowed to
perform a “time change” so we always land on the “correct side” of the discontinuity.
Importantly, we note that DR+,Swith the above notion of convergence forms a well behaved probability
space.
Theorem A.1 (Theorem A5.3, Kallenberg (2021)).For any separable complete metric space S, there exists
a topology TonDR+,Ssuch that
(i)Tinduces the Skorohod convergence xns− →x,
(ii)DR+,Sis Polish (separable completely metrizable topological space) under T,
(iii)Tgenerates the Borel σ-ﬁeld generated by the evaluation maps πt,t≥0, where πt(x) =xt.
To be fully self-contained, we will also introduce the deﬁnitions of a Feller semi-group, generator, and the
core. Let Sbe a locally compact separable metric space, and let C0=C0(S)be the space of continuous
functions that vanishes at inﬁnity, equipped with the sup norm, hence making C0a Banach space. We say
T:C0→C0is apositive contraction operator if for all 0≤f≤1, we have that 0≤Tf≤1. A
semi-group (Tt)of positive contraction operators on C0is called a Feller semi-group if it also satisﬁes
TtC0⊂C0, t≥0,
Ttf(x)→x ,ast→0, f∈C0, x∈S .(A.2)
LetD ⊂ C0andA:D → C0. We say the pair (A,D)is agenerator of the semi-group (Tt)ifDis the
maximal set such that
lim
t→0Ttf−f
t=Af . (A.3)
We say an operator Awith domain Don a Banach space Bisclosed , if its graph G={(f, Af ) :f∈ D} is
a closed subset of B×B. If the closure of Gis the graph of an operator A, we say that Ais the closure of
A.
We say a linear subspace D⊂ D is acore ofA, if the closure of A|DisA. Intuitively, all important properties
ofAcan be recovered via a limit point of Afn, forfn∈D. Furthermore, if (A,D)is a geneator of a Feller
semi-group, every dense invariant subspace D⊂ D is a core of A(Kallenberg ,2021, Proposition 17.9). In
particular, the core we will work with is the space C∞
0of smooth functions that vanishes at inﬁnity.
The following is a suﬃcient condition for a semi-group to be Feller.
15Published in Transactions on Machine Learning Research (04/2024)
Theorem A.2 (Section 8, Theorem 2.5, Ethier & Kurtz (2009)).Letaij∈C2(Rd)with∂k∂ℓaijbe bounded
for all i, j, k, ℓ ∈[d], and let b:Rd→Rdbe Lipschitz. Then the generator deﬁned by the elliptic operator
Af=1
2d/summationdisplay
i,j=1aij∂i∂jf+d/summationdisplay
i=1bi∂if , (A.4)
generates a Feller semi-group on C0.
Next, we will state a set of equivalent conditions for convergence of Feller processes.
Theorem A.3 (Theorem 17.25, Kallenberg (2021)).LetX, X1, X2, X3,···be Feller processes in Swith
semi-groups (Tt),(Tn,t)and generators (A,D),(An,Dn), respectively, and ﬁx a core DforA. Then these
conditions are equivalent:
(i)for any f∈D, there exists some fn∈ D nwithfn→fandAnfn→Af,
(ii)Tn,t→Ttstrongly for each t >0,
(iii) Tn,tf→Ttffor every f∈C0, uniformly for bounded t >0,
(iv)Xn
0d− →X0inS⇒Xnd− →Xin the Skohorod topology of DR+,S.
We note that it is common to choose the core D=C∞
0, and that checking the ﬁrst condition is suﬃcient for
convergence in the Skorohod topology. The next result will help us apply the above criterion to continuous
time interpolated Markov chains.
Theorem A.4 (Theorem 17.28, Kallenberg (2021)).LetY1, Y2, Y3,···be discrete time Markov chains in
Swith transition operators U1, U2, U3,···, and let Xbe a Feller process with semi-group (Tt)and generator
A. Fix a core DforA, and let 0< h n→0. Then conditions (i)−(iv)of Theorem A.3remain equivalent
for the operators and processes
An=h−1
n(Un−I), T n,t=U⌊t/hn⌋
n , Xn
t=Yn
⌊t/hn⌋. (A.5)
Here we note that in our applications, we will always choose hn=n−2p, which is essentially dependent on
the width of the neural network.
At this point, we still need to check that the generators Anconverges to Awith respect to the core D=C∞
0.
For this goal, we will use a lemma from Stroock & Varadhan (1997). Here we will deﬁne Πn(x, dy )to be the
Markov transition kernel of Yn, and let
aij
n(x) =1
hn/integraldisplay
|y−x|≤1(yi−xi)(yj−xj) Πn(x, dy ),
bi
n(x) =1
hn/integraldisplay
|y−x|≤1(yi−xi) Πn(x, dy ),
∆ϵ
n(x) =1
hnΠn(x,Rd\B(x, ϵ)).(A.6)
Lemma A.5 (Lemma 11.2.1, Stroock & Varadhan (1997)).The following two conditions are equivalent:
(i)For any R > 0, ϵ > 0we have that
lim
n→∞sup
|x|≤R∥an(x)−a(x)∥op+|bn(x)−b(x)|+ ∆ϵ
n(x) = 0 , (A.7)
(ii)For each f∈C∞
0(Rd), we have that
1
hnAnf→Af , (A.8)
uniformly on compact sets of Rd, where Ais deﬁned as ( A.3).
16Published in Transactions on Machine Learning Research (04/2024)
We will also need to weaken the deﬁnition slightly for non-Lipschitz coeﬃcients.
Deﬁnition A.6. We say a sequence of processes Xnconverge locally toXin the Skorohod topology if for
anyr >0, we deﬁne the following stopping times
τn:={t≥0 :|Xn
t| ≥r}, τ :={t≥0 :|Xt| ≥r}, (A.9)
and we have that Xn
t∧τnconverge to Xt∧τin the Skorohod topology.
Finally, to summarize everything into a useful form for this paper, we will state the following proposition.
Proposition A.7 (Convergence of Markov Chains to SDE, Proposition A.6, Li et al. (2022)).LetYnbe a
discrete time Markov chain on RNdeﬁned by the following update for p, δ > 0
Yn
ℓ+1=Yn
ℓ+/hatwidebn(Yn
ℓ, ωn
ℓ)
n2p+σn(Yn
ℓ)
npξn
ℓ+O(n−2p−δ), (A.10)
where ξn
ℓ∈RNare iid random variables with zero mean, identity covariance, and moments uniformly bounded
inn. Furthermore, ωn
ℓare also iid random variables such that E[/hatwidebn(Yn
ℓ, ωn
ℓ)|Yn
ℓ=y] =bn(y)and/hatwidebn(y, ωn
ℓ)
has uniformly bounded moments in n. Finally, σnis a deterministic function, and the remainder terms in
O(n−2p−δ)have uniformly bounded moments in n.
Suppose bn, σnare uniformly Lipschitz functions in nand converges to b, σuniformly on compact sets, then
in the limit as n→ ∞ , the process Xn
t=Yn
⌊tn2p⌋converges in distribution to the solution of the following
SDE in the Skorohod topology of DR+,RN
dXt=b(Xt)dt+σ(Xt)dBt, X 0= lim
n→∞Yn
0. (A.11)
Suppose otherwise bn, σnare only locally Lipschitz (but still uniform in n), then Xnconverges locally to Xin
the same topology (see Deﬁnition A.6). More precisely, for any ﬁxed r >0, we consider the stopping times
τn:= inf{t≥0 :|Xn
t| ≥r}, τ := inf{t≥0 :|Xt| ≥r}, (A.12)
then the stopped process Xn
t∧τnconverges in distribution to the stopped solution Xt∧τof the above SDE in
the same topology.
B Technical Lemmas for Shaped Activations
Here we will recall and slightly modify a collection of deﬁnitions and technical results from Appendix B and
C of Li et al. (2022), which are related to the shaped activation that we will use in the main theorems. To
start, we will let φ(x) = max( x,0), and recall the ReLU-like activation function as
φs(x) =s+max( x,0) +s−min(x,0) = s+φ(x)−s−φ(x). (B.1)
Letg∼ N (0,1), then we will restate Li et al. (2022, Lemma B.3, B.6)
Eφ(g) =1√
2π,Eφ(g)2=1
2,Eφ(g)4=3
2.
Eφs(g) =s+−s−√
2π,Eφs(g)2=s2
++s2
−
2,Eφs(g)4=3
2(s4
++s4
−).(B.2)
We will also recall the deﬁnitions
¯Jp,r(ρ):=Eφ(g)pφ(ˆg)r, K p,r(ρ):=Eφs(g)pφs(ˆg)r, (B.3)
where g, w are iid N(0,1)and we deﬁne ˆg=ρg+qwwith q=/radicalbig
1−ρ2. We will also use the short hand
notation to write ¯Jp:=¯Jp,p, Kp:=Kp,p.
17Published in Transactions on Machine Learning Research (04/2024)
Here we recall from Cho & Saul (2009) and Li et al. (2022, Lemma B.7) that
¯J1(ρ) =/radicalbig
1−ρ2+ (π−arccos ρ)ρ
2π, K 1(ρ) = (s2
++s2
−)¯J1(ρ)−2s+s−¯J1(−ρ). (B.4)
Next we will slightly modify a Taylor expansion result from ( Li et al. ,2022, Lemma C.1)
Lemma B.1 (Taylor Expand Shaping Correlation) .Recall φs(x) =s+max( x,0) +s−min(x,0). Let s±=
1 +c±
npforp >0, then we have the following Taylor expansion
cK1(ρ) =ρ+ν(ρ)
n2p+O(n−3p), ν (ρ) =(c+−c−)2
2π/parenleftBig/radicalbig
1−ρ2+ρarccos ρ/parenrightBig
. (B.5)
Proof. We start by expanding the formula for K1(ρ)to get
cK1(ρ) =2
s2
++s2
−1
2π/parenleftBig
(s2
++s2
−)/parenleftBig/radicalbig
1−ρ2+ (π−arccos ρ)ρ/parenrightBig
−2s+s−/parenleftBig/radicalbig
1−ρ2−(arccos ρ)ρ/parenrightBig/parenrightBig
.(B.6)
At this point, we can plug in s±= 1 +c±
np, and Taylor expanding gives us
cK1(ρ) =ρarccos ( ρ)
π+ρ(π−arccos ( ρ))
π
+/parenleftbig
n−p/parenrightbig2/parenleftBigg
−ρc2
+arccos ( ρ) + 2ρc+c−arccos ( ρ)−ρc2
−arccos ( ρ)
2π
+c2
+/radicalbig
1−ρ2−2c+c−/radicalbig
1−ρ2+c2
−/radicalbig
1−ρ2
2π/parenrightBigg
+O/parenleftBig/parenleftbig
n−p/parenrightbig3/parenrightBig
.(B.7)
Simplifying the expressions gives us the desired result of
cK1(ρ) =ρ+ν(ρ)
n2p+O(n−3p). (B.8)
Similar to Li et al. (2022, Lemma C.2), we can also approximate the fourth moment of shaped activation
functions via the fourth moment of Gaussians.
Lemma B.2 (Fourth Moment Approximation) .Considers the jointly Gaussian random variables
[gα]4
α=1∼ N/parenleftbig
0,[ραβ]4
α,β=1/parenrightbig
, (B.9)
where ραα= 1 for all α. Let φs(x) =s+max( x,0) + s−min(x,0)with coeﬃcients s±= 1 +c±
np, then we
have
E4/productdisplay
α=1φs(gα) =E4/productdisplay
α=1gα+O(n−p) =ρ12ρ34+ρ13ρ24+ρ14ρ23+O(n−p). (B.10)
Proof. Observe that we can write φs(x)as a perturbation of identity
φs(x) =x+1
np(c+φ(x)−c−φ(−x)) =x+O(n−p). (B.11)
This means we can approximate the product of shaped activations without the activation
4/productdisplay
α=1φs(gα) =4/productdisplay
α=1gα+O(n−p). (B.12)
18Published in Transactions on Machine Learning Research (04/2024)
Finally, we can use the Isserlis Theorem to write
E4/productdisplay
α=1gα=Eg1g2Eg3g4+Eg1g3Eg2g4+Eg1g4Eg2g3=ρ12ρ34+ρ13ρ24+ρ14ρ23, (B.13)
which is the desired result.
Furthermore, we can characterize the covariance of the following random variables
Rαβ=1√nn/summationdisplay
i=1/bracketleftBig
cφs(gα
i)φs(gβ
i)−cK1(ραβ)/bracketrightBig
, (B.14)
where the random vector [gα
i, gβ
i]are iid copies of the same Gaussian vector. This follows from a modiﬁcation
ofLi et al. (2022, Lemma C.3).
Lemma B.3 (Covariance of Rαβ).LetRαβbe deﬁned as above. Then if s±= 1 +c±
npfor the shaped
activation, we have that
ERαβRγδ=ραγρβδ+ραδρβγ+O(n−p). (B.15)
Proof. Firstly, we note since each entry of the sum in Rαβare iid, we will only need to compute the moments
for a single entry. This means
ERαβRγδ=Ec2/parenleftBig
φs(gα
i)φs(gβ
i)−K1(ραβ)/parenrightBig/parenleftbig
φs(gγ
i)φs(gδ
i)−K1(ργδ)/parenrightbig
. (B.16)
At this point, we observe that c= 1 + O(n−p),K1(ρ) =ρ+O(n−2p), and we can write
ERαβRγδ=E/parenleftBig
φs(gα
i)φs(gβ
i)−ραβ/parenrightBig/parenleftbig
φs(gγ
i)φs(gδ
i)−ργδ/parenrightbig
+O(n−p). (B.17)
Finally, the fourth moment approximation Lemma B.2gives us the desired result
ERαβRγδ=ραβργδ+ραγρβδ+ραδρβγ−ραβργδ−ραβργδ+ραβργδ+O(n−p)
=ραγρβδ+ραδρβγ+O(n−p).(B.18)
19