Published in Transactions on Machine Learning Research (03/2024)
New Guarantees for Learning Revenue Maximizing Menus
of Lotteries and Two-Part Tariffs
Maria-Florina Balcan ninamf@cs.cmu.edu
Carnegie Mellon University
Hedyeh Beyhaghi hedyeh@cmu.edu
Carnegie Mellon University
Reviewed on OpenReview: https: // openreview. net/ forum? id= mhawjZcmrJ
Abstract
We advance a recently flourishing line of work at the intersection of learning theory and com-
putational economics by studying the learnability of two classes of mechanisms prominent
in economics, namely menus of lotteries andtwo-part tariffs . The former is a family of ran-
domized mechanisms designed for selling multiple items, known to achieve revenue beyond
deterministic mechanisms, while the latter is designed for selling multiple units (copies) of
a single item with applications in real-world scenarios such as car or bike-sharing services.
We focus on learning high-revenue mechanisms of this form from buyer valuation data in
both distributional settings, where we have access to buyers’ valuation samples up-front,
and the more challenging and less-studied online settings, where buyers arrive one-at-a-time
and no distributional assumption is made about their values. We provide a suite of results
with regard to these two families of mechanisms. We provide the first online learning al-
gorithms for menus of lotteries and two-part tariffs with strong regret-bound guarantees.
Since the space of parameters is infinite and the revenue functions have discontinuities, the
known techniques do not readily apply. However, we are able to provide a reduction to
online learning over a finite number of experts, in our case, a finite number of parameters.
Furthermore, in the limited buyers type case, we show a reduction to online linear opti-
mization, which allows us to obtain no-regret guarantees by presenting buyers with menus
that correspond to a barycentric spanner. In addition, we provide algorithms with improved
running times over prior work for the distributional settings. Finally, we demonstrate how
techniques from the recent literature in data-driven algorithm design are insufficient for our
studied problems.
1 Introduction
Overview. In recent years, a growing body of work has emerged in the field of machine learning for pricing
and mechanism design problems. These problems involve selling items to buyers with the objective of max-
imizing revenue. The majority of the existing research has primarily concentrated on distributional settings ,
i.e., when the buyers’ values for the items are drawn from an unknown distribution. Less attention has been
paid to the more challenging case of online setting , where buyers arrive one by one and no distributional
assumptionaboutbuyers’valuesisconsidered. Inthiscase, thepreviousliteraturehasmostlyfocusedonsim-
ple mechanisms such as posted pricing or, more generally, mechanisms that sell the items separately (Blum
et al., 2004; Kleinberg and Leighton, 2003; Blum and Hartline, 2005; Balcan and Blum, 2006; Bubeck et al.,
2017; Cesa-Bianchi et al., 2014; Balcan et al., 2018b; 2020a). We advance this line of work by studying the
learnability of two prominent classes of mechanisms, both represented as menus providing the buyers a list
of allocation and payment options to choose from, namely menus of two-part tariffs and lotteries. These
mechanisms go beyond selling the items separately, resulting in potentially higher revenue guarantees with
applications to modern real-world scenarios. We provide a collection of results for these mechanisms while
1Published in Transactions on Machine Learning Research (03/2024)
discovering technical surprises compared to prior work in data-driven algorithm and mechanism design. Our
results include the first online learning guarantees for menus of two-part tariffs and lotteries and improved
guarantees for distributional learning. In the process, we establish a data-independent discretization method,
despite the drastic failure of this technique in problems with a similar utility function (Balcan et al., 2017;
2018a; 2023a;b). In addition, we demonstrate inadequacy of recently developed techniques in data-driven
algorithm design for our settings. In particular, for the first time, we provide evidence for the failure of the
dispersion property (Balcan et al., 2018b; 2020a)—a sufficient condition to provide a no-regret algorithm
under the smooth distributional assumption, which is widely applied to parametric algorithm and mechanism
design problems—for a specific problem (menus of lotteries).
Problem Setup. The first class we study is menus of two-part tariffs (Lewis, 1941), used for selling multiple
units (i.e., copies) of a single item. In this family of mechanisms, the buyer is presented with a list (menu)
oftwo-part tariffs , wheretariffiis a pair consisting of an up-front fee, p(i)
1, and a per-unit fee, p(i)
2. If the
buyer wishes to buy k≥1units of tariff i, she pays in total p(i)
1+kp(i)
2, and if she does not want to buy
anything, she does not pay anything. The buyer has the freedom to select any of the tariffs. In particular,
the cost for purchasing k≥1units is the minimum cost among all the tariffs, i.e., mini(p(i)
1+kp(i)
2). Various
products in the real world are sold via menus of two-part tariffs; for example, car or bike-sharing services
and delivery service subscriptions.
The second class we study is the menus of lotteries for selling multiple items. In this context, the buyer
is presented with a list (menu) of lotteries, wherelotteryiis defined as a pair consisting of a vector of
probabilitiesforallocatingeachitem, ϕ(i),andaprice, p(i). Ifthebuyerwishestochooselottery i,shereceives
each itemjwith probability ϕ(i)[j]and paysp(i). Menus of lotteries are a crucial family of mechanisms
because (1) this family captures all possible mechanisms, including the optimal one (Dasgupta et al., 1979;
Guesnerie and Oddou, 1981), and (2) menus of lotteries achieve revenue beyond other well-studied families
of mechanisms such as posted pricing and, more generally, any deterministic mechanism (Briest et al., 2010;
Hart and Nisan, 2019).
Westudymenusoftwo-parttariffsandlotteriesinthecontextofparameteroptimization, wheretheobjective
function (revenue) depends on parameter vectors. In menus of two-part tariffs, the parameters determining
the mechanisms are the up-front fees and per-unit fees for each tariff, while for menus of lotteries, the allo-
cation probability vectors and the prices for the lotteries determine the mechanism. In the parameter space,
each point corresponds to a mechanism. A common approach in learning algorithms involves considering the
objective function for a fixed buyer’s valuation (Balcan et al., 2017; 2018c;b). In our context, the mechanism
designer faces a utility-maximizing buyer, who, given the parameters determining the menu, chooses the
entry, i.e., a lottery or a two-part tariff, in the menu that maximizes her utility. Therefore, the revenue
function at any parameter vector is equal to the payment corresponding to the entry selected by the buyer.
1.1 Our Contributions
We study the learnability of menus of two-part tariffs and lotteries in both online and distributional settings.
We advance the state-of-the-art in several aspects.
Technical challenges, Structural Properties, and a Revenue Preserving Cover. “Discretization”
is a natural technique in data-driven algorithm design. In this approach, a finite set of parameter vectors,
each representing a menu in the parameter space, are selected, and the algorithms optimize over that set.
The smaller the set, the better the generalization guarantees will be in the distributional setting, and the
better the regret guarantees will be in the online setting, with respect to the best menu in the set. In our
setting, a proper data-independent discretization scheme would guarantee that independent of the buyer’s
valuation, this set always contains a nearly optimal menu. More specifically, for any arbitrary parameter
vector representing a menu, a menu in the set should generate almost as much revenue, independent of
the buyer’s valuation. However, due to sharp discontinuities of revenue in the parameter space, devising
such a discretization can be challenging. For instance, consider a menu with two high-utility entries for a
buyer such that these entries have similar utility for the buyer but very different prices (e.g., one with high
allocation and high price, the other with low allocation and low price). Minor changes in the parameters of
2Published in Transactions on Machine Learning Research (03/2024)
these entries; e.g., rounding the parameters down to multiples of ϵ, may alter their utility order, causing the
buyer to switch between them, resulting in an arbitrary loss in revenue.
By extracting structural properties for menus of two-part tariffs , we develop a novel discretization method
that identifies a finite set of menus that approximate the revenue of any arbitrary limited-length menu
(Theorem 1). At a high level, in finding a corresponding menu with approximate revenue guarantee, the
options (tariff and quantity pairs) with higher prices need to experience a more significant decrease in price
(compared to the lower price ones) so that no buyer switches from a high-price to a low-price option. In
menus of lotteries , we extend the discretization of menus of lotteries developed by Dughmi et al. (2014)
(Theorem 27). Our extension is three-fold: we remove the lower bound assumption on value distribution,
support additive valuations, and provide improved regret bounds and running times when the size of the
menu is limited. In both settings (two-part tariffs lotteries), our discretization is data-independent; e.g., the
set of discretized menus consists of all menus with parameters that are multiples of εor powers of (1−ε). The
novelty of the result, however, lies in the analysis, which illustrates despite the challenges discussed above,
for each arbitrary menu and valuation, this set contains a corresponding approximately revenue-preserving
menu.
Online Learning (adversarial inputs and smooth distributional assumptions). For menus of two-
part tariffs , we provide the first no-regret online learning algorithms under adversarial (worst-case) inputs
and also smooth distributional assumptions. For the full information setting, both settings lead to similar
regret terms; however, the comparison of their running time depends on the support of the distribution and
the maximum number of units available (Theorems 7 and 10). In the bandit setting, again, the regrets
of both settings are similar. However, the comparison between the efficiencies of the algorithms depends
on the smoothness factor of the distributions (Theorems 8 and 11). Furthermore, we provide the first no-
regret algorithm for a semi-bandit setting (Theorem 12) with a polynomial running time in the number of
discontinuities in the parameter space. This setting lies between the full-information and bandit settings, and
thelearnerobservestherevenuefunctionforasetofmenuscontainingthemenuused. Formenusof lotteries ,
we provide the first no-regret online learning algorithms under adversarial inputs (Theorems 28 to 30). In
addition, we provide evidence that menus of lotteries may not satisfy dispersion (Balcan et al., 2018b;
2020a)—a sufficient condition to provide a no-regret algorithm under smooth distributional assumption—
without assuming extra structures about the optimal solution (Theorem 33). Menus of lotteries are the first
family of mechanisms for which there is evidence of a potential failure of the dispersion property.
Distributional Learning. We also provide novel distributional learning algorithms for menus of two-part
tariffs and lotteries. Our algorithms choose several menus in a data-independent way (via data-independent
discretization) and then select the best of them based on the data. In the context of two-part tariffs ,
our algorithm is much simpler than prior ones for the same problem, yet it enjoys improved worst-case
runtime guarantees compared to them (Balcan et al., 2018c; 2020b) when the length of the menu is more
than one (Theorem 26). We note that for other data-driven algorithm design problems, such as data-driven
clustering and data-driven learning to branch, it was proven that algorithms that use data-independent
discretization could perform very poorly (Balcan et al., 2017; 2018a; 2023a). Thus, by contrast, our work
shows the power of data-independent discretization for data-driven mechanism design and algorithm design
more generally. In the context of lotteries , compared to the previous distributional learning results for
fixed-length menus (Balcan et al., 2018c), our algorithm requires similar sample complexity; however, it has
an efficient implementation (Theorems 34 and 57).
Limited Buyer Types. For limited buyer types, we provide improved regret bounds for both the full-
information and partial-information (bandit) settings for both menus of two-part tariffs and lotteries (The-
orems 24, 25, 31 and 32). The high-level idea is as follows. Consider the revenue function in the parameter
space for a fixed buyer. The parameter space is partitioned into regions where, within each region, the
buyer selects the same option in the menu, e.g., the same lottery, resulting in a continuous revenue function.
Discontinuity occurs across regions. For limited-type buyers, by superimposing the revenue functions for all
types, the parameter space divides into more (albeit still a limited number of) regions. Regardless of the
buyer type at hand, the revenue function is continuous within each region and in our case, linear. Therefore,
it is sufficient to only consider the corner points as potential parameter vectors that maximize the revenue.
3Published in Transactions on Machine Learning Research (03/2024)
We show that in the full information case, running the weighted majority algorithm on the set of menus
corresponding to the regions’ corner points results in sublinear regret.
Inthepartialinformationsetting, weshowareductiontoonlinelinearoptimization, allowingustoobtainno-
regret guarantees by presenting buyers with menus corresponding to a barycentric spanner. Our reduction
is inspired by Balcan et al. (2015); however, we apply the reduction in the different contexts of pricing
schemes. In the partial information setting, in each round, we only observe the revenue of the current menu.
To estimate the revenue from all the menus efficiently, or in other words, to find an unbiased estimator with
abounded range , we employ the notion of barycentric spanners in online learning introduced by Awerbuch
and Kleinberg (2008). By utilizing this concept, we provide algorithms with a regret bound that is sublinear
in the number of timesteps and polynomial in other parameters. This is the first time that the barycentric
spanner notion has been applied to an auction design setting.
1.2 Summary of Contributions
First, we overview the results related to menus of two-part tariffs .
•By extracting structural properties, we develop a novel discretization method that identifies a finite
set of menus that approximate the revenue of any arbitrary menu, including the optimum for any
valuation. This allows the development of new no-regret online learning algorithms as well as
improved distributional learning algorithms (see the two bullet points below).
•We provide the first no-regret online learning algorithms under adversarial inputs, smooth distribu-
tional assumptions, and limited buyer-type assumptions (under full information, bandit setting, and
semi-bandit setting).
•We also provide a novel distributional learning algorithm for menus of two-part tariffs. Our algo-
rithm chooses several menus of two-part tariffs in a data-independent way (via data-independent
discretization) and then selects the best of them based on data. This is much simpler than previ-
ous algorithms (Balcan et al., 2018c; 2020b) for the same problem, yet it enjoys improved runtime
guarantees in the worst-case scenario when the length of the menu is more than one.
•For limited buyer types, we provide improved regret bounds for both the full-information and bandit
settings. We show a reduction to online linear optimization, which allows us to obtain no-regret
guarantees by presenting buyers with menus that correspond to a barycentric spanner.
Next, we overview our results related to menus of lotteries .
•We extend the discretization of menus of lotteries developed by Dughmi et al. (2014). Our exten-
sion is three-fold: we remove the lower bound assumption on value distribution, support additive
valuations, and provide improved regret bounds and running times when the size of the menu is
limited.
•We provide the first no-regret online learning algorithms under adversarial inputs.
•Comparedtothepreviousdistributionallearningresultsforfixed-lengthmenusBalcanetal.(2018c),
our algorithm requires similar sample complexity; however, it has an efficient implementation.
•We provide evidence that menus of lotteries may not satisfy dispersion—a sufficient condition to
provide a no-regret algorithm under smooth distributional assumption—without assuming extra
structures about the optimal solution. Menus of lotteries are the first family of mechanisms where
there is evidence for potential failure of the dispersion property.
•For limited buyer types, we provide improved regret bounds for both the full-information and bandit
settings. We show a reduction to online linear optimization, which allows us to obtain no-regret
guarantees by presenting buyers with menus that correspond to a barycentric spanner.
4Published in Transactions on Machine Learning Research (03/2024)
1.3 Related Work
Studying learnability of classes of mechanisms for the revenue maximization objective has been of great
interest in recent years (Alon et al., 2017a; Cole and Roughgarden, 2014; Devanur et al., 2016; Elkind, 2007;
Gonczarowski and Nisan, 2017; Guo et al., 2019; Roughgarden and Schrijvers, 2016). These mechanisms
have been studied mostly in a distributional setting, where buyers’ values are drawn from an unknown
distribution, and the online setting, where there is no distributional assumption on the buyers’ values,
has been explored less.1In the distributional setting, various mechanism classes, including posted-price
mechanisms, second-price auctions with reserves, menus of two-part tariffs, and menus of lotteries, are known
to be learnable (Morgenstern and Roughgarden, 2015; 2016; Balcan et al., 2016; 2018c; 2021a; Dughmi et al.,
2014; Gonczarowski and Weinberg, 2021; Mohri and Medina, 2016; Syrgkanis, 2017; Dütting et al., 2019).
In the online setting, under adversarial input (Blum et al., 2004; Kleinberg and Leighton, 2003; Blum and
Hartline, 2005; Balcan and Blum, 2006; Roughgarden and Wang, 2016; Bubeck et al., 2017), and also under
stochastic input (Cesa-Bianchi et al., 2014; Balcan et al., 2018b; 2020a) mostly simple mechanisms such as
posted pricing and second-price auction are considered where both mechanisms sell the items separately.
An exception is Roughgarden and Wang (2016) who study Vickrey-Clarke-Groves (VCG) mechanism with
multiple reserves; however, the algorithms provided are not no-regret in the classic sense but are bounded-
regret compared to a constant approximation of the optimal solution.
Two of the prominent approaches used for developing distributional results are pseudo-dimension-based and
discretization-based. In the first approach, despite the discontinuity present in the utility of buyers as a
function of the parameters used in the mechanism, it is shown that the pseudo-dimension of the family is
bounded by using smoothness assumptions on the distribution. This approach applies to all the mechanisms
mentioned above. In the discretization approach, a finite set of parameters is identified such that limiting
the search space to this set is approximately optimal. This approach has been used for a limited number
of mechanisms, such as item-pricing for combinatorial auctions for unrestricted supply (Balcan et al., 2008)
and menus of lotteries in a limited setting (Dughmi et al., 2014). In the online setting, Balcan et al. (2018b)
and Balcan et al. (2020a) introduce dispersion as a sufficient condition for online learnability of families of
mechanisms. They show several classes of mechanisms, such as posted-price mechanisms and second-price
auctions with reserves, satisfy dispersion and, therefore, establish strong regret bounds for online learning.
Discretization-based techniques in online learning scenarios have been used for the simple cases of item-
pricing (Blum et al., 2004) and the second-price auctions (Cesa-Bianchi et al., 2014).
Two-Part Tariffs. Two-part tariff pricing schemes were first introduced by Lewis (1941) and later analyzed
by Oi (1971). Menus of two-part tariffs have been studied recently in the context of distributional learn-
ing (Balcan et al., 2018c; 2020b; 2022b). A recent work (Balcan et al., 2022b) provides improved running
time bounds over Balcan et al. (2020b) for distributional learning of two-part tariffs in the case where the
number of pieces with continuous sum of utility functions u(xi,·)across all problem instances is small (as
defined in Section 3.2.2 the utility function u(xi,.)measures the performance of our two-part tariff mecha-
nisms on a fixed problem instance xias a function of its parameters). However, for the case where the menu
length is strictly greater than 1, Balcan et al. (2022b)’s approach does not lead to improved running time
over Balcan et al. (2020b) for worst-case instances. So, for worst-case instances and menu length >1, our
approach for distributional learning improves over previously best-known results.
Menus of Lotteries. Menus of lotteries capture all possible mechanisms, including the optimal one, for
selling items to buyers. The Taxation Principle (Dasgupta et al., 1979; Guesnerie and Oddou, 1981) asserts
that any mechanism for a single buyer can be represented as a menu of lotteries, where the buyer selects their
favorite lottery (that is, the one that maximizes the buyer’s expected value for the randomized allocation
minus the price paid). Furthermore, menus of lotteries achieve revenue beyond other well-studied families
of mechanisms such as posted pricing and, more generally, any deterministic mechanism. For a correlated
buyer (a buyer whose values for items are correlated), even in the simple cases where the buyer is additive
(their value for a bundle of items is the sum of the value for individualized items) or unit-demand (their
value for a bundle of items is the maximum value for an item in the bundle), the gap between optimal
1Some online learning algorithms, including those proved via the dispersion method, explained later, still make distributional
assumptions; however, unlike the distributional learning setting, the draws are not necessarily from identical distributions.
5Published in Transactions on Machine Learning Research (03/2024)
randomized mechanism (lotteries) and item-pricing is infinite (Briest et al., 2010; Hart and Nisan, 2019).
Daskalakis et al. (2014) show that even for an independent additive buyer (the values for the items are
independent), lotteries (randomized mechanisms) are necessary and provide strictly more revenue compared
to any deterministic mechanism, including pricing mechanisms.
Failure of data-independent discretization-based learning. Discretization is a natural approach for
designing algorithms to tune parameters (e.g., prices for menus of two-part tariffs and allocation probabilities
and prices for menus of lotteries) and is commonly used in applied fields such as applied machine learning.
However, recent work has shown that in tuning parameters of algorithms for solving discrete combinatorial
problems, discretization in the context of data-driven algorithm design does not always work if discretization
is done in a data-independent way. For the case of tuning parameters for linkage-based algorithms, Balcan
et al. (2017) showed that for several natural parameterized families of clustering procedures, for any data-
independent discretization, there exists an infinite family of clustering instances such that any of the discrete
parameters will output a clustering that is an Ω(n)factor worse than the optimal parameter, where nis the
input size. Here, the quality of clustering can be defined according to several well-known objectives, including
k-median,k-means, and k-center. Balcan et al. (2018a; 2023a) show that for the data-driven problem of
learning to branch for solving mixed integer linear programs (MILPs), data-independent discretization will
not work either. More specifically, for any discretization of the parameter space [0,1], there exists an infinite
family of distributions over MILP problem instances such that for any parameter in the discretization, the
expected tree size is exponential in the input parameter. Yet, there exists an infinite number of parameters
such that the tree size is just a constant (with probability 1). Remarkably, we show that in our context,
even data-independent discretization works.
Dispersion and Online Data-Driven Algorithm Design. Dispersion is a recently-developed notion
for families of algorithmic and mechanism design problems and serves as a sufficient condition for the ex-
istence of bounded-regret online learning algorithms (Balcan et al., 2018b; 2020a; Balcan, 2020) and differ-
entially private distributional learning algorithms (Balcan et al., 2018b). Generally speaking, this condition
bounds the concentration of discontinuities of the objective function in any small regions in the parameter
space. Dispersion-based techniques have been established successfully for a variety of algorithms (Balcan and
Sharma, 2021; Balcan et al., 2021b; 2022a), among which is tuning parameters in combinatorial problems,
such as clustering problems discussed above (Balcan et al., 2018b). For menus of two-part tariffs, we show
that the dispersion condition is satisfied, immediately implying no-regret online learning algorithms and
differentially private algorithms for distributional learning. Surprisingly, we present evidence that dispersion
might not apply to menus of lotteries. In particular, we show in menus of lotteries the objective function
might have sharp discontinuities concentrated in a small region. This structural property is in stark contrast
with menus of two-part tariffs and other mechanism and algorithm families satisfying dispersion. Despite this
evidence, we show that a simple discretization-based approach leads to no-regret online learning algorithms
for menus of lotteries.
Sample Complexity for Menus of Lotteries. The sample complexity for menus of lotteries has been
studied under two different assumptions: independence of valuation across items, as studied by Gonczarowski
and Weinberg (2021) and correlated valuation across items, as studied by Dughmi et al. (2014); Brustle et al.
(2020). By assuming independence simultaneously among the buyers and the items, a significant improve-
ment over the sample complexity is possible (Gonczarowski and Weinberg, 2021). However, when the values
for the items are possibly correlated, Dughmi et al. show a lower bound on the sample complexity, verifying
an exponential gap on the dependence in the number of items compared to Gonczarowski and Weinberg.
Brustle et al. (2020) study a setting between the two extremes of arbitrary correlation and independence
where they assume structured dependence across items, generalizing the results of Gonczarowski and Wein-
berg (2021) and improving the sample complexity over Dughmi et al. (2014) for special cases of correlation.
Similar to Dughmi et al. and in contrast with Gonczarowski and Weinberg, we do not assume independence
(or structured dependence) across items and only assume independence among the buyers.
6Published in Transactions on Machine Learning Research (03/2024)
2 Model and Preliminaries
We consider selling items to a single buyer for the revenue objective through parameterized families of
mechanisms. In this paper, the family of mechanisms is either the set of menus of two-part tariffs or
lotteries. To put our notations in context, in this section, we focus on menus of two-part tariffs as our
running example. The discussed settings also hold for menus of lotteries – we defer the discussion related to
menus of lotteries to Section 4.
Menus of two-part tariffs are used for selling multiple units (i.e., copies) of a single item through a list of up-
frontandper-unitfeepairsthatthebuyercanchoosefrom. Menu M=/braceleftig/parenleftig
p(1)
1,p(1)
2/parenrightig
,...,/parenleftig
p(ℓ)
1,p(ℓ)
2/parenrightig/bracerightig
⊆R2ℓ,
isalength-ℓmenuoftwo-parttariffs. Eachmenu Misparameterizedby ρwhichinthiscaseis 2ℓ-dimensional
and contains all p(j)
1andp(j)
2where allp(j)
1,p(j)
2∈[0,H].p(j)
1andp(j)
2are called the up-front fee (price)
and per-unit fee (price) of tariff j, respectively. We denote a buyer’s valuations for all 1,2,...,Kunits
byv= (v(1),...,v (K)), where the values are nonnegative, monotonically increasing, belong to [0,H], and
v(0) = 0. Under the tariff jdenoted by/parenleftig
p(j)
1,p(j)
2/parenrightig
and the number of units k∈{1,...,K}that the buyer
selects, she receives kunits of the item and pays p(j)
1+kp(j)
2. The buyer’s utility is her value for the number of
units bought v(k)less the payment. Each buyer has the option of buying their utility-maximizing tariff and
number of units. In other words, the buyer will buy kunits using tariff jthat maximizes v(k)−p(j)
1−kp(j)
2
or does not buy and does not pay anything.
LetMbe an infinite set of mechanisms parameterized by a set C⊆Rd. In this paper,Mis either the set
of two-part tariff menus or lottery menus. Consider the case where Mis the set of two-part tariff menus
for selling multiple units of a single item to a buyer with value vwhile the menu corresponds to parameter
ρ∈C. Next, let Πbe a set of problem instances for M, such as a set of buyer valuations v, and let
u: Π×C→ [0,H]be a utility function where u(x,ρ)measures the performance of the mechanism with
parametersρon problem instance x∈Π. In our case, u(x,ρ)is the revenue of the mechanism (a menu of
two-part tariffs or lotteries) with parameters ρon inputx. For example, for the menus of two-part tariffs,
Mis the set of possible menus Mand since each menu is 2ℓ-dimensional with each dimension in [0,H],
C= [0,H]2ℓ⊆R2ℓ.Πis the set of buyer valuations vandu: Π×C→ [0,H]be a utility function where
u(v,ρ)measures the revenue of the menu with parameters ρon buyer valuations v∈Π.
Online Setting. In this setting, a sequence of functions u1,...,uT:C→ [0,H]arrive one by one. Unlike u,
utonly takes parameter ρtas the input and is defined as ut(ρt) :=u(ρt,xt), wherextis the problem instance
at timestep t. At the time t, the no-regret learning algorithm chooses a parameter vector ρtand then either
observes the function utin the full information setting, the scalar ut(ρt)in the bandit setting, or ut(ρt)for a
set ofρin the semi-bandit setting. The goal is to minimize the expected regret, E[maxρ∈C/summationtextut(ρ)−ut(ρt)].
We study the online setting both under adversarial input, where ut()are selected adversarially, and under
smoothed distribution inputs which assume more structure. The expectation in the regret formula is taken
over the randomness of the algorithm in the adversarial setting and over the randomness of the algorithm
and distribution of buyers in the smoothed distributional setting.
Distributional Setting. In the distributional setting, the algorithm receives samples from an unknown
distributionDover problem instances Π. The goal is to find a parameter vector ˆρthat nearly maximizes
the expected utility, i.e., maxρ∈CEx∼D[u(x,ρ)]similar to statistical learning theory (Vapnik, 1998) or PAC
learning (Valiant, 1984).
3 Menus of Two-Part Tariffs
In this section, we consider M=/braceleftig/parenleftig
p(1)
1,p(1)
2/parenrightig
,...,/parenleftig
p(ℓ)
1,p(ℓ)
2/parenrightig/bracerightig
⊆R2ℓas a length- ℓmenu of two-part tariffs.
See Section 2 for a detailed description.
7Published in Transactions on Machine Learning Research (03/2024)
3.1 Discretization Procedure
This section shows a discretization procedure for the menus of two-part tariffs. Given any menu and value
0<α< 1, we provide an alternate menu such that all the price elements, p(i)
1andp(i)
2for alli, are multiples
ofαand the alternate menu provides nearly as much revenue as the given menu up to a term that depends
onα. The main result of this section is summarized in the following statement.
Theorem 1. Given a menu of two-part tariffs Mand parameter 0< α < 1, Algorithm 1 outputs menu
M′whose revenue is at least the revenue of Mminus 2Kαℓ, for any buyer’s valuation. Furthermore, for
alli, allp(i)
1andp(i)
2are multiples of α. The set of potential outcomes constitutes a space with at most
min{(H/α)2ℓ,2H2/α2}menus, where His the maximum value for any number of units.
Correctness of the rounding procedure (Algorithm 1) as in the proof of Theorem 1 implies that the set of
menus whose prices, i.e., p(i)
1,p(i)
2, are multiples of αconstitute (an almost) revenue-preserving set of menus.
Algorithm 1: (Almost) revenue preserving rounding for menus of two-part tariffs
Input:MenuM, discretization parameter α.
1:LetM′be the menu of Pareto frontier tariffs inM, derived by one by one deleting tariffs ifor which
there exists tariff j̸=isuch thatp(i)
1≥p(j)
1andp(i)
2≥p(j)
2.
2:Reindex the tariffs in M′in increasing order of p1(and hence, decreasing order of p2).
3:For each tariff i, decreasep(i)
1andp(i)
2by(i−1)α. ▷The revenue preserving step.
4:Round down all p(i)
1andp(i)
2to the closest multiple of α.
5:Remove the duplicate tariffs.
Output: MenuM′.
ProofideaofTheorem1andintuitionbehindAlgorithm1. Atahighlevel, forfindingcorresponding
menus through Algorithm 1, the options (tariff quantity pairs) with higher prices need to experience a larger
decrease in price so that no buyer switches from a high-price to a low-price option. The main structural ideas
deriving the algorithm and the proof of the revenue guarantee are as follows: (i) for a fixed number of units
kto be purchased, the utility-maximizing tariff is the same across all the buyer’s valuations; namely, the
tariff that has the smallest overall price (upfront price plus ktimes per-unit price), and (ii) as the number
of units to be purchased increases, the per-unit price of the utility-maximizing tariff decreases. The main
idea of the rounding algorithm is decreasing the corresponding prices of tariffs with lower per-unit fees by
a larger amount (Line 3). By doing so, for each buyer, the total price of buying more units decreases more
than the total price of buying fewer units. This step ensures that the buyer does not switch from purchasing
more units to fewer units after the rounding. This property is sufficient for the revenue guarantees. The
other steps of the algorithm delete redundant tariffs (Lines 1 and 5) and ensure the final prices are multiples
ofα(Line 4). The theorem provides two upper bounds for the size of the discretized space. By Line 4, all
the prices are multiples of α. Therefore, the 2ℓprice components in a length- ℓmenu each have H/αoptions.
This gives the first bound. On the other hand, if we consider a single tariff, each of the up-front fee and the
per-unit fee has H/αpossibilities, therefore, the total number of possible unique tariffs are H2/α2. Each of
these possible tariffs may or may not be on the menu, giving the second bound. The full proof is provided
below.
Remark. Our rounding scheme (Algorithm 1) is only described for the purpose of the proof to argue that
the multiples of αprovide (an almost) revenue-preserving set of menus. Algorithmically, we only need to
enumerate the multiples of α.
3.1.1 Proof of Theorem 1
Before providing the proof of the discretization procedure, we provide intuition as to why discretization is
a nontrivial procedure for menus of two-part tariffs. For this family of mechanisms, standard procedures,
such as rounding down the prices to multiples of α, may result in arbitrary revenue loss because the price
8Published in Transactions on Machine Learning Research (03/2024)
parameters of each tariff decrease by different amounts affecting unpredictable changes in utilities of selecting
each tariff and number of units. It would be possible that the utility-maximizing choice for a buyer switches
from a higher-price tariff and more units (that originally has slightly higher utility for the buyer) to a low-
price tariff and fewer units (that originally has slightly lower utility for the buyer) after a simple rounding.
Now, we provide structural results that enable us to design a discretization procedure. Given a menu of
two-part tariffs, the following definition deletes the dominated tariffs (independent of the valuation).
Definition 2 (Pareto frontier tariffs) .Given menu Mwith distinct tariffs, the Pareto frontier of M′is
derived by deleting all tariffs ifor which there exists a tariff j̸=isuch thatp(j)
1≤p(i)
1andp(j)
2≤p(i)
2.
Lemma 3. Given a menu of tariffs, a user only selects a tariff in the Pareto frontier.
Lemma 4. Sorting the tariffs in the Pareto frontier in increasing order of p1is equivalent to sorting them
in decreasing order of p2.
Lemma 5. For any fixed number of units k, the highest utility tariff in Misargminp(i)
1+kp(i)
2. This is
independent of the buyers’ values.
The following lemma states that as we increase the number of units the utility-maximizing tariff has higher
p1and lowerp2.
Lemma 6. LetM′be the menu of Pareto frontier tariffs derived from menu M. Suppose the tariffs in M′
are reindexed in increasing order of p1. Consider the index of the utility-maximizing tariff for each number
of units. This index is increasing as a function of the number of units.
Proof of Theorem 1. First, we reason about the length of the outcome menu. Let ℓandℓ′be the length of
the original menu and outcome menu, respectively. First, note that ℓ′is also the length of the menu after
rounding down p(i)
1andp(i)
2to their closest multiples of α. Observe that ℓ′is at mostℓ(because we never add
extra tariffs) and also at most H2/α2because there are H/αdistinct options for each p1andp2. Therefore,
ℓ′≤min{ℓ,H2/α2}.
Then, we reason about the maximum loss in revenue. First, note that for any fixed tariff and number of
units, the total price decreases by at most 2Kℓ′α. We only need to show that the buyer does not switch
from buying more units to fewer. Switching in the opposite order does not decrease the revenue more than
2Kℓ′α. The reason is that the total price of each tariff is an increasing function as the number of units.
Therefore, the minimum total price is increasing as a function of the number of units. Next, we prove that
a buyer never switches from buying more units to less. We show two cases: switching between tariffs and
staying with the same tariff. In the first case, by Lemma 6, this means that that a buyer never switches from
a tariff with higher p1(lowerp2) to a lower p1(higherp2). Since in the discretization procedure, the price
of tariffs with higher p1decreases more than lower p1, the lower p1tariffs do not become utility-maximizing
if they were not before. In the second case, by the rounding procedure, the total price of more units in the
same tariff always decreases more; therefore, the lower number of units never becomes utility maximizing.
Therefore, we conclude the payment of each tariff and therefore the revenue decreases at most by 2Kℓ′α.
Thus, Rev(M′)≥Rev(M)−2Kαℓ.
Finally, we find the total number of possible menus. Also, after the discretization all p(i)
1andp(i)
2are
multiples of α. Therefore, when restricted to length- ℓmenus, there are H/αchoices for each 2ℓparameter of
the menu, making an upper bound of (H/α)2ℓ. On the other hand, there are at most H2/α2possible tariffs,
and each one of them may appear or not in the menu. Therefore, the number of menus is also bounded by
2H2/α2.
Technical contribution. The establishment of data-driven discretization (and the subsequent online learn-
ing and distributional learning algorithms) are in contrast with previous findings. For other data-driven algo-
rithm design problems, such as data-driven clustering and data-driven learning to branch that share a similar
piecewise structure in the utility functions, it has been proven that algorithms that use data-independent
discretization could perform very poorly (Balcan et al., 2017; 2018a; 2023a). Thus, by contrast, our work
shows the power of data-independent discretization for data-driven mechanism design and algorithm design
more generally.
9Published in Transactions on Machine Learning Research (03/2024)
3.2 Online Learning
We provide bounded-regret online learning algorithms in full and partial information settings. Sections 3.2.1
to 3.2.3 provide online algorithms under adversarial input, under smooth distributions, and for limited type
buyers, respectively. No online learning algorithms have been known previously for menus of two-part tariffs.
3.2.1 Online Learning Under Adversarial Inputs
The main statements are Theorems 7 and 8 which provide regret guarantees for the full-information case
and partial-information case, respectively. Using the discretization in Section 3.1, we show a reduction to a
finite number of experts and run standard learning algorithms (weighted majority and Exp3) over the menus
in the discretized set. Similar ideas were used in previous papers, for example Blum et al. (2004); Balcan
et al. (2018b).
Full Information. In the full information setting, the seller sees the revenue generated for all the possible
menus. To design an online algorithm in this case, we use a variant of the weighted majority algorithm
by Auer et al. (1995). The experts in our case are the discretized menus from the previous section, denoted
in the algorithm by set X=m1,...,mn. Furthermore, vtis the valuation of the buyer are time tand
Revk(v1,...,vt)is the cumulative revenue of menu mkfor the buyers until time step t.
Algorithm 2: Full-information (Weighted majority on discretized menus)
Input:Set of menus (experts) X=m1,...,mn, learning rate β∈(0,1].
1:Initialize: For each menu mk, initialize Revk() = 0,wk(0) = 1
2:forbuyert= 1,...,Tdo
Select menu at time tto bemkwith probability πk[t] =wk(t−1)/summationtextn
j=1wj(t−1);
Observe valuation of buyer tasvt;
For each menu mk, update Revk(v1,...,vt)andwk(t) = (1 +β)Revk(v1,v2,...,vt)/H;
Theorem7. In the full information case for length- ℓmenus of two-part tariffs, running Algorithm 2 over dis-
cretized set of menus specified in Theorem 1 for α=β= 1/√
Thas regret bounded by ˜O/parenleftig
ℓ(K+HlnH)√
T/parenrightig
,
and running time O(TℓK min{H2ℓTℓ,2H2T}).
The proof follows by combining the guarantees of the discretization procedure (Theorem 1) and previously
known results (specifically Auer et al. (1995), Theorem 3.2) and is deferred to Appendix A.
Partial Information (Bandit Setting). In the partial information setting, the seller does not see the
outcome for all the possible menus and only observes the outcome of the menu used (the tariff and number
of units chosen by the buyer). To design an online algorithm in this case, we use a version of the Exp3
algorithm in Auer et al. (1995). This variant of the Exp3 algorithm contains the weighted majority algorithm
(Algorithm 2) as a subroutine. At each step, we mix the probability distribution π, used by the weighted
majority algorithm, with the uniform distribution to obtain a modified probability distribution π, which is
then used to select a menu from our discretized set. Following the tariff and the number of units chosen by
buyert, we use the price paid (the gain from the chosen menu) to formulate a simulated gain vector, which
is then used to update the weights maintained by the weighted majority algorithm.
Theorem 8. In the partial information case for length- ℓmenus of two-part tariffs, running Algorithm 3
over discretized set of menus in Theorem 1 for α=T−1/(2(1+ℓ)),β=γ=T−1/(4(1+ℓ))has regret bound
˜O/parenleftig
T1−1
2(1+ℓ)ℓ(K+H2ℓ+1)/parenrightig
, and running time O(Tmin{min{H2ℓTℓ,2H2T},2H2T}).
The proof follows by combining the guarantees of the discretization procedure (Theorem 1) and previously
known results (specifically (Auer et al., 1995), Theorem 4.1) and is deferred to Appendix A.
10Published in Transactions on Machine Learning Research (03/2024)
Algorithm 3: Partial-information (Exp3 on discretized menus)
Input:Set of menus (experts) X=m1,...,mn, learning rate β∈(0,1], parameter γ∈(0,1].
1:Initialize: For each menu mk, initialize Revk() = 0,wk(0) = 1
2:forbuyert= 1,...,Tdo
Select menu at time tto bemkwith probability πk(t) = (1−γ)πk(t) +γ/nwhere
πk[t] =wk(t−1)/summationtextn
j=1wj(t−1);
For the selected menu k∗, setgk∗(t)to be the price paid by buyer t(i.e.,gk∗(t)is equal to
pj
1+kpj
2, wherejandkare the tariff index and quantity chosen by buyer t). Setgk∗(t) =γ
ngk∗(t)
πk∗(t);
For all other menus k, setgk(t) = 0; For all menus k, update Revk(t) = Revk(t−1) +gk(t)and
wk(t) = (1 +β)Revk(t)/H;
3.2.2 Online Learning Under Smooth Distributions
Recent papers studying online learning of mechanisms, e.g., Balcan et al. (2018b; 2020a), studied the problem
in a restricted setting, where at each point in time, instead of a worst-case value, the value is drawn from a
bounded-density distribution. This assumption is in the same spirit as the “smoothed analysis” paradigm of
Spielman and Teng (Spielman and Teng, 2004) and is used in similar contexts in papers, including Cohen-
Addad and Kanade (2017); Gupta and Roughgarden (2017). Specifically, we assume the buyers’ valuations
come from κ-bounded distributions, where the density function is bounded at all points by κ. This assump-
tion has proved to be sufficient for a few classes of mechanisms, including posted-pricing and second-price
mechanisms, to establish dispersion . At a high level, dispersion ensures that the number of discontinuities
in a small ball in the parameter space is limited with high probability and is a sufficient condition for
bounded-regret online algorithms. We prove that menus of two-part tariffs satisfy dispersion and use it to
derive bounded-regret algorithms for full-information, bandit, and semi-bandit settings. The main difference
between the algorithms used in this section compared to the adversarial input setting in Section 3.2.1 is that
we previously needed to go through a careful data-independent discretization step (Section 3.1) to reduce
the problem to a finite number of experts. However, under smooth distributions, the assumed properties of
the distribution influence the set of experts chosen.
We provide the main results in this setting, followed by a discussion of the key ideas behind the algorithms
and proofs. After establishing the dispersion constraint for menus of two-part tariffs, it is sufficient to
employ previously known algorithms designed for dispersed settings to achieve no-regret guarantees. The
primary purpose of this section is to compare the regret guarantees from the recently developed online
learning technique of dispersion and the discretization approach discussed in the previous section. The
formal definition of dispersion and technical descriptions of the algorithms and proofs are deferred to the
appendix. The main results are as follows2:
Definition 9. [κ-bounded] A density function f:R→Rcorresponds to a κ-bounded distribution if
max{f(x)}≤κ.
Theorem 10. Letu1,...,uT:C → [0,H]be the revenue functions of two-part tariff menus such that
ut(ρ)denotes the revenue of a mechanism associated with menu parameters ρfor the buyer arriving at
timet. Let the samples of buyers’ values be drawn from S ∼D(1)×···×D(T). Suppose v(k)∈[0,H]
for any number of units k∈[K]. Also, suppose that for each distribution D(t), and every pair of number
of unitskandk′,v(k)andv(k′)have aκ-bounded joint distribution. An efficient implementation of the
exponentially weighted forecaster with λ=/radicalig
2ℓln(2H2κ√
T)/T/H(Algorithm 4) has expected regret bounded
by˜O((Hℓ2K2√logκ+ 1/(Hκ))√
T)and runs in time ˜O((T+ 1)poly(ℓ,K)poly(ℓ,√
T) +KT√
T).
Theorem 11. Letu1,...,uT:C→ [0,H]be the revenue functions of two-part tariff menus such that ut(ρ)
denotes the revenue of a mechanism associated with menu parameters ρfor the buyer arriving at time t. Let
2The regret term in the semi-bandit algorithm (Theorem 12) is smaller than the full-information algorithm (Theorem 10)
since different notions of dispersion are used. Also, the stated running time of both algorithms are the same; however, this is
in the worst case, and the semi-bandit algorithm potentially performs fewer computations.
11Published in Transactions on Machine Learning Research (03/2024)
the samples of buyers’ values be drawn from S∼D(1)×···×D(T). Supposev(k)∈[0,H]for any number
of unitsk∈[K]. Also, suppose that for each distribution D(t), and every pair of number of units kandk′,
v(k)andv(k′)have aκ-bounded joint distribution. There is a bandit-feedback online optimization algorithm
with expected regret ˜O/parenleftig
T(2ℓ+1)/(2ℓ+2)/parenleftig
H2K√
ℓκd/2√logκ/parenrightig
+1/Hκ+Hℓ2K2/parenrightig
. The per-round running time
isO(H4ℓκ2ℓTℓ).
Theorem 12. Suppose the buyers’ values are drawn from D(1)×···×D(T), where eachD(t)isκ-bounded for
κ= ˜o(T). Then, running the continuous Exp3-SET algorithm (Algorithm 7) for menus of two-part tariffs
under semi-bandit feedback has expected regret bounded by ˜O(H√
ℓT). An efficient implementation has the
same regret bound and running time ˜O((T+ 1)poly(ℓ,K)poly(ℓ,√
T) +KT√
T).
Smoothed Distributional Assumptions. In an online setting under smoothed distributions, the algo-
rithm receives samples S∼DT, whereDis an arbitrary distribution over problem instances Π(which in
our case is the buyer valuations). The goal is to find ˆρthat nearly maximizes/summationtext
v∈Su(v,ρ). In this setting,
the goal is to find a value ρthat is nearly optimal in hindsight over a stream v1,...,vTof instances, or
equivalently, over a stream u1=u(v1,·),...,uT=u(vT,·)of functions. Each vtis drawn from a distribution
D(t), which may be adversarial. Therefore, {v1,...,vT}∼D(1)×···×D(T).
Dispersion. Letu1,...,uTbe a set of functions mapping a set C ⊆Rdto[0,H]. In this paper, we
study the mechanism selection setting, given a collection of problem instances v1,...,vT∈Πand a utility
functionu: Π×C→ [0,H], each function ui(·)might equal the function u(vi,·), measuring a mechanism’s
performance on a fixed problem instance as a function of its parameters. Informally, dispersion is a constraint
on the functions u1,...,uTthat guarantees although each function uimay have discontinuities, they do
not concentrate in a small region of space. We study two definitions of dispersion previously introduced in
algorithmandmechanismselectionproblems. Weshowthatmenusoftwo-parttariffssatisfybothdefinitions;
(w,k)-dispersion (Definition 15) and β-dispersion (Definition 38). Then, we use the first to establish online
learning results for full-information and bandit settings and the second for the semi-bandit setting.
Inordertoprovemenusoftwo-parttariffssatisfydispersionundersmoothedassumptions,weshowthisfamily
of mechanisms satisfies certain structural properties. Balcan et al. (2018c) show in two-part tariff menus,
for each function ui, the parameter space Cis partitioned into sets P1,...,Pnsuch thatuiisL-Lipschitz
on each piece, but uimay have discontinuities at the boundaries between pieces.3We refine this structural
property and show that multi-sets of parallel hyperplanes, corresponding to the stream of buyer valuations,
partition the parameter space Cinto convex polytopes with bounded-degree linear utility functions inside
each polytope. Later, we show this property is sufficient for proving dispersion and employing the related
algorithms.
Partitioning of parameter space to convex regions with linear utilities (Balcan et al., 2018c).
Consider the sequence of buyers valuations b. At each time step, a buyer is presented with a menu, and based
on the menu and their valuation, they select the tariff index and number of units that maximize their utility.
Formally, given menu ρ, buyeriwith valuation biselects option (j,k), wherejis the tariff index and kis
the number of units if this option produces more utility for the buyer than any other options. Concretely,
bi(k)−I{k≥1}/parenleftig
p(j)
1(ρ) +kp(j)
2(ρ)/parenrightig
≥bi(k′)−I{k′≥1}/parenleftig
p(j′)
1(ρ) +k′p(j′)
2(ρ)/parenrightig
∀j′,k′(1)
wherep(j)
1(ρ)andp(j)
2(ρ)are the up-front fee and per-unit fee of tariff jin menuρ. The above inequalities
identify a convex polytope of parameter vectors (menus ρ) with hyperplane boundaries. Since the tariff index
and the number of units that biselects are fixed in the region, the revenue, I{k≥1}/parenleftig
p(j)
1(ρ) +kp(j)
2(ρ)/parenrightig
,
is continuous and more specifically linear in the region (formally proved in Lemma 20). Following the same
argument for the buyers in the sequence, the parameter space for each buyer is partitioned into convex
polytopes where the revenue for the buyer’s valuation is linear inside the polytopes. By superimposing these
partitionings, since the intersections of convex regions are also convex, and the sum of linear functions (here
revenues)islinear, theparameterspace, Cispartitionedintoconvexregionssuchthatthecumulativerevenue
3This previously-known structural result suffices for the techniques used in the setting with the limited number of buyers
(Section 3.2.3 and appendix A.1.3); however, we need a refined statement for proving dispersion.
12Published in Transactions on Machine Learning Research (03/2024)
for the sequence is linear in each region. Inside each region, the utility-maximizing choice of each buyer is
fixed; therefore, eachregionisassociatedwitha mapping frombuyervaluationstotheircorrespondingutility-
maximizing tariff index and number of units. We may use the mapping, formally defined in Section 3.2.3, to
denote the region, e.g., region Pµcorresponding to mapping µ, or simply use cardinal indices for the regions
P1,P2,....
p(j)
1p(j′)
2
HHFigure 1: Thefigureisanabstractionoftheregionsforparameterspaceof
two-part tariffs drawn in two dimensions for illustration. The coordinates
are the up-front and per-unit fees for the tariff indices. The dashed hyper-
planes correspond to a buyer valuation having the same utility through
two pairs of tariff indices and the number of units; see Equation (1). The
colored region area is defined by hyperplane boundaries. Inside each such
region, any buyer valuation selects a fixed tariff index and the number of
units, resulting in a linear cumulative revenue function.
Lemma 13. Consider the sequence of buyer valuations varrived until time t. For menus of two-part tariffs,
the parameter space Cis partitioned into convex polytopes, P1,...,Pnby multisets of parallel hyperplanes,
such that the utility function at each time step inside each region Pjis a linear function satisfying (K+ 1)-
Lipschitz continuity.
Proof.Part of the proof that identifies the regions with linear utilities has been shown previously in Balcan
et al. (2018c), Lemma 3.15. We reiterate that part for completeness and also prove the extra structural
properties, i.e., parallel hyperplanes and (K+ 1)-Lipschitz continuity. Consider the set of menus for which
the buyer with valuation v(i)arriving at time iselects the tariff index jand the number of units k. The
buyer selects this option for menu ρif it produces more utility for the buyer than any other option. Formally,
v(i)(k)−I{k≥1}/parenleftig
p(j)
1(ρ) +kp(j)
2(ρ)/parenrightig
≥v(i)(k′)−I{k′≥1}/parenleftig
p(j′)
1(ρ) +k′p(j′)
2(ρ)/parenrightig
.∀j′,k′(2)
The above inequalities identify a convex polytope of parameter vectors (menus ρ) with hyperplane bound-
aries. Considering all the possible selections (j,k)(the tariff index and the number of units), the parameter
space forv(i)is partitioned into convex polytopes where inside each polytope the payment of v(i)is linear;
i.e.,I{k≥1}/parenleftig
p(j)
1(ρ) +kp(j)
2(ρ)/parenrightig
. Considering the same analysis for all the buyers’ valuations in the se-
quence, for each buyer, the parameter space is partitioned into convex polytopes where inside each polytope,
the revenue function is linear and (K+ 1)-Lipschitz. Since convex polytopes are closed under intersection,
superimposing the partitions for i= 1,...,tresults in polytopes with the properties in the statement.
For a fixed valuation vector v(i), the discontinuities in the utility function are defined by at most ℓ2K2
hyperplanes: v(i)(k)−I{k≥1}/parenleftig
p(j)
1(ρ) +kp(j)
2(ρ)/parenrightig
=v(i)(k′)−I{k′≥1}/parenleftig
p(j′)
1(ρ) +k′p(j′)
2(ρ)/parenrightig
. Let Ψv
be the multi-set union of all these hyperplanes. Consider a set S=/braceleftbig
v(1),...,v(t)/bracerightbig
with corresponding
multi-sets Ψv(1),..., Ψv(t)of hyperplanes. We now partition the multi-set union of Ψv(1),..., Ψv(t)into at
mostℓ2K2multi-setsBj,k,j′,k′for allj,j′∈[ℓ]andk,k′∈[K]andi∈[t]such that for each Bj,k,j′,k′,
the hyperplanes in Bj,k,j′,k′are parallel with probability 1 over the draw of S. To this end, define a single
multi-setBj,k,j′,k′to consist of the hyperplanes
{v(1)(k)−I{k≥1}/parenleftig
p(j)
1(ρ) +kp(j)
2(ρ)/parenrightig
=v(1)(k′)−I{k′≥1}/parenleftig
p(j′)
1(ρ) +k′p(j′)
2(ρ)/parenrightig
,
v(2)(k)−I{k≥1}/parenleftig
p(j)
1(ρ) +kp(j)
2(ρ)/parenrightig
=v(2)(k′)−I{k′≥1}/parenleftig
p(j′)
1(ρ) +k′p(j′)
2(ρ)/parenrightig
,
...,
v(t)(k)−I{k≥1}/parenleftig
p(j)
1(ρ) +kp(j)
2(ρ)/parenrightig
=v(t)(k′)−I{k′≥1}/parenleftig
p(j′)
1(ρ) +k′p(j′)
2(ρ)/parenrightig
};
13Published in Transactions on Machine Learning Research (03/2024)
where the only variables are coordinates of ρ. The hyperplanes inside each multi-set are parallel and the
utility of the regions defined by the hyperplanes are linear and K+ 1-Lipschitz.4
Next, we establish an upper bound on the number of regions with continuous (linear) regions.
Lemma 14. The partitioning of the parameter space for menus of two-part tariffs explained in Lemma 13
afterTrounds results in O((T+ 1)ℓ2K2)regions, with linear cumulative utility function inside each region.
Proof.Lemma 13 identifies multi-sets Bj,k,j′,k′of sizeTfor eachj,k,j′,k′such that the hyperplanes inside
the multi-sets are parallel. Therefore, each multi-set divides the parameter space into T+ 1parts. Thus,
each region with continuous utility can be defined as the intersection at most ℓ2K2parts, where each part
corresponds to a distinct multi-set. This results in at most O((T+ 1)ℓ2K2)such regions.
Dispersion for menus of two-part tariffs. We provide intuition as to why menus of two-part tariffs
for bounded density distributions satisfy dispersion; that is, the discontinuities in the revenue function
do not concentrate with high probability. To prove this, we focus on Equation (1) for fixed values of
j,k,j′,k′, i.e., pairs of tariffs and units, and for all bi∈b. The equalities for all of these equations are
met at parallel hyperplanes because, for each ρand fixed pairs of tariffs and units, other parameters, i.e.,
k,k′,p(j)
1,p(j)
2,p(j′)
1,p(j′)
2are fixed, and the equations are only different in bi. Assuming independence of
distributions among buyers and κ-bounded joint distributions over bi(k)andbi(k′), with high probability
the intersection of multisets of parallel hyperplanes, defined by Equation (1) do not concentrate, implying
dispersion.
We first provide the formal definition of (w,k)-dispersion. Recall that Πis a set of instances, C⊂Rdis
a parameter space, and uis an abstract utility function. We use the l2distance and let B(ρ,r) ={ρ′∈
Rd:∥ρ−ρ′∥2≤r}denote a ball of radius rcentered atρ. We use this notion of dispersion to derive our
full-information and bandit setting results.
Definition 15 ((Balcan et al., 2018b), (w,k)-dispersion) .Letu1,...,uT:C → [0,H]be a collection of
functions where uiis piecewise Lipschitz over a partition PiofC. We say thatPisplits a set AifA
intersects with at least two sets in Pi. The collection of functions is (w,k)-dispersed if every ball of radius
wis split by at most kof the partitionsP1,...,PT. More generally, the functions are (w,k)-dispersed at a
maximizer if there exists a point ρ∗∈argmaxρ∈C/summationtextT
i=1ui(ρ)such that the ball B(ρ∗,w)is split by at most
kof the partitionsP1,...,PT.
We now prove menus of two-part tariffs satisfy (w,k)-dispersion and use it to derive no-regret online learning
results for full-information and bandit settings.
Proposition 16. Suppose that u(v,ρ)is the revenue of the two-part tariff menu mechanism with prices ρ
and buyer’s values v. With probability at least 1−ζover the drawS∼D(1)×···×D(T)for anyα≥1/2
the following statement holds:
Supposev(k)∈[0,H]for any number of units k∈[K]. Also, suppose that for each distribution D(t), and
every pair of number of units kandk′,v(k)andv(k′)have aκ-bounded joint distribution. Then uis
/parenleftigg
1
2HκT1−α,O/parenleftigg
ℓ2K2Tα/radicaligg
lnℓK
ζ/parenrightigg/parenrightigg
-dispersed
with respect toS.
Proof.Lemma 13 gives multisets of parallel hyperplanes that partition the parameter space into regions with
K+ 1-Lipschitz continuous utility functions. Since the samples are drawn independently from κ-bounded
distributions with support [0,H], the offsets of the hyperplanes in each multiset Bj,k,j′,k′are independent
random variables with Hκ-bounded distributions. Furthermore, the number of multisets is at most ℓ2K2.
Using these properties, Theorem 32 of Balcan et al. (2018b) gives the statement.
4Partitioning of the parameter space by parallel multisets of hyperplanes has been established before for other families of
mechanism design such as posted pricing (Balcan et al., 2018b). We extend this idea to the more complicated case of two-part
tariffs.
14Published in Transactions on Machine Learning Research (03/2024)
After establishing dispersion and showing that the parameter space is partitioned into convex regions with
cumulative linear utility inside each region, the no-regret guarantees and their performances are implied by
prior results.
Algorithm 4: Full-information online learning of two-part tariffs under smoothed distributional assump-
tions (Adapted to two-part tariffs from (Balcan et al., 2018b), Algorithm 4)
Input:λ∈(0,1/H],η,ζ∈(0,1).
1:Setu0(·) = 0(to be the constant 0 function over C).
2:forbuyert= 1,2,...,Tdo
Present menu ρtsampled with probability approximately proportional to eg(ρt)to the buyer,
where where, g(·) =λ/summationtextt−1
s=0us(·). (Use Algorithm 6, with approximation parameter η/4and
confidence parameter ζ/T).;
Observe the revenue for all the potential menus as function ut(·). Receive payment
ut(ρt) =I{k≥1}(p(i)
1(ρt) +kp(i)
2(ρt)), whereiandkare the tariff index and the number of units
chosen by buyer trespectively given menu ρt.
Overview of Algorithms. We provide high-level ideas for the full-information, bandit, and semi-bandit
setting algorithms used for Theorems 10 to 12, respectively. Generic forms of these algorithms were devised
by Balcan et al. (2018b; 2020a) for dispersed families of algorithms. The full information algorithm considers
the cumulative revenue function up until the time t−1over the parameter space and samples the menu
to present at time tproportional to an exponential function of its cumulative revenue. In order to have
an efficient implementation, they use techniques from high-dimensional geometry and approximately sample
menuρt. LetP1,...,Pnbe the partition of Cuntil time t. The algorithm picks Piwith probability
approximately proportional to the region’s cumulative weight and outputs a sample from the conditional
distribution of menus in Pi. The bandit-setting algorithm considers a grid over the parameter space, whose
granularity depends on the dispersion parameters, and runs the Exp3 algorithm over menus corresponding
to the grid. The semi-bandit setting algorithm is a continuous version of the Exp3-SET algorithm of Alon
et al. (2017b). At each time step, the algorithm learns the revenue function (only) inside the region Pithat
the presented menu belongs to and updates the menu weights for the next round accordingly.
Comparison to the results in Section 3.2.1 . Although the discretization-based algorithms work under
adversarial inputs and are more general, they provide similar regret bounds and even improved running times
in some cases. In the full information case, the dependence on the regret bound in parameter Tis similar
in both algorithms. In running time, the discretization-based algorithm suffers worse dependence in H, but
enjoys better dependence in TandK(the maximum number of units) compared to the dispersion-based
algorithm. In the bandit setting, similarly, the regret bounds are similar in their dependence on T, while
the running-time comparison depends on the value of κ(maximum density under smoothness assumption)
such that lower-density distributions may result in better running times.
Comparison to prior work. For menus of two-part tariffs, it has been shown in Balcan et al. (2018b)that
based on the values observed from users until time t, the parameter space is partitioned into convex regions
with hyperplane boundaries such that the utility inside each region satisfies Lipschitz continuity. We give a
more refined characterization by showing that (1) the utility function inside each region is linear, and (2) the
boundary hyperplanes constitute a multiset of parallel hyperplanes. Properties (1) and (2) are important
for establishing dispersion and obtaining no-regret online learning algorithms under smooth distributional
assumptions, as in Theorems 10 and 11. After establishing dispersion, we use previously developed results,
i.e., regret bounds for dispersed settings, from prior work (e.g., Theorem 1 in Balcan et al. (2018b) for
full information and Theorem 3 in (Balcan et al., 2018b) for bandit setting). The algorithms for full-
information, semi-bandit, and bandit settings were previously developed in a general format (Balcan et al.,
2018b; 2020a) for any problem setting satisfying dispersion property. We adapt those algorithms to our
settings in Algorithms 4, 6 and 7.
15Published in Transactions on Machine Learning Research (03/2024)
3.2.3 Limited Buyer Types
In this section, we assume that there are a finite number of known buyer types. This information provides
extra structures compared to the general setting considered previously. In particular, now the mechanism
designer is aware of where the potential discontinuities happen as a function of the parameter space. We
providealgorithmswithboundedregretsbothforthefullinformationandpartialinformationsettingsspecific
to limited types. These algorithms improve the regret bounds significantly when the number of buyer types
is small. This section is inspired by Balcan et al. (2015) and includes similar algorithms and notations.
Balcan et al. (2015) study a security games setting, in which at each time step, the defender has a mixed
strategy (a probability distribution) for protecting the attack targets . Knowing this mixed strategy, the
attacker selects a target to attack, which maximizes the attacker’s utility (depending on the attacker’s type).
Considering the target selected by each attacker type as a function of the defender’s mixed strategy, the
mixed strategy space is partitioned into regions where the action of each attacker type is fixed throughout
each region. This is very similar to our setting, where the parameter space is partitioned into regions, where
inside each region, each buyer type selects a fixed tariff index and the number of units (see the discussion
on partitioning the parameter space in Section 3.2.2). Balcan et al. use the linear structure of the utility
function inside each region to develop a no-regret full-information algorithm. In the partial information
setting, other than the linearity of utility functions, they use the dependence of an agent’s (in their case,
attacker, and in our case, buyer) actions across different regions and identify a limited number of mixed
strategies (corresponding to menus in our case) such that observing the agent’s response to them suffice to
estimate the utility of other strategies. We use similar machinery in both the full and partial information
settings. However, the source of linearity of the utility is different across the two settings. In the security
games context, the attacker’s action corresponds to a fixed coordinate axis in the parameter space, and the
utility is defined as a fixed linear function of that coordinate. In our setting, however, the utility depends on
multiple coordinates, and its formula depends on the buyer’s choice. Nevertheless, we show the cumulative
utility is a linear function of coordinates (See Lemma 20). For completeness and to make the paper self-
contained, we include a full description of the algorithms and techniques adapted to our setting and using
our terminology.
In this setting, we utilize the knowledge of the potential buyer types to design a limited number of menus
and optimize over this set. In contrast to the previous section, where the valuations were realized after the
arrival of the buyers, here, we have access to all potential buyer types up-front, but similarly, as discussed in
Section 3.2.2, the piecewise linear structure of the utility for the buyers partition the parameter space such
that each part has linear cumulative utility (Balcan et al., 2018c). This partitioning is equivalent to dividing
the parameter space into convex regions such that in each region, there is a fixed mapping from the buyer
types to the menu options that each buyer selects. We show that in each region, we need to consider only a
limited number of menus, namely the extreme points.
Considerv1,...,vVas the set of all potential buyer valuations. Vdenotes the number of buyer types. In
order to define the behavior of buyers in each region, we need to define a concept called menu options , which
determines the buyers’ choices.
Definition 17 (menu option for menus of two-part tariffs, O).A pair (j,k), wherejis the tariff index
1,...,ℓ, andkis the number of units 0,1,...,Kis a menu option. We denote the set of all menu options
asO. This set identifies all potential actions of a buyer when presented with a menu.
Definition 18 (mappingµ, feasible mappings, Pµ).A mapping µis a function from buyer types, v1,...,vV
to menu options (j,k), wherejandkare the tariff index and the number of units assigned to the buyer
type respectively. Mapping µis feasible if there is a menu corresponding to the mapping, i.e., a menu
that, if presented to the buyers, each buyer selects their corresponding option in the mapping as their utility
maximizing option. Pµdenotes the region of the parameter space corresponding to µ, i.e., the set of menus
inducing mapping µ.
Using the discussion in Section 3.2.2, the parameter space is partitioned to convex polytopes, each with
a linear utility function for any sequence of buyer types. We reiterate this result in Lemmas 19 and 20,
adapting the statements to the limited buyer type setting and corresponding notations.
16Published in Transactions on Machine Learning Research (03/2024)
Lemma 19. For each feasible mapping µ, as defined in Definition 18, Pµis a convex polytope with hyperplane
boundaries.
Proof.The statement is a corollary of Lemma 13. For a fixed buyer type iand option (j,k), letP(i)
(j,k)be
the set of all parameter vectors ρcorresponding to the length- ℓmenus that buyer type iselects option (j,k).
The buyer selects option (j,k)for menuρif this option produces more utility for the buyer than any other
option. Formally,
vi(k)−I{k≥1}/parenleftig
p(j)
1(ρ) +kp(j)
2(ρ)/parenrightig
≥vi(k′)−I{k′≥1}/parenleftig
p(j′)
1(ρ) +k′p(j′)
2(ρ)/parenrightig
.∀j′,k′
The above inequalities identify a convex polytope of parameter vectors (menus ρ) with hyperplane bound-
aries.Pµis the intersection of P(i)
µ(i)fori= 1,...,V. Therefore,Pµis also a convex region with hyperplane
boundaries.
Lemma 20. For each feasible mapping µand any sequence of buyer valuations bthe cumulative utility,/summationtext
iu(bi,ρ), is linear inPµ.
Proof.Before presenting the proof, we point out the difference between the proof of linearity in Balcan et al.
(2015) and in this lemma. In Balcan et al. (2015), in each region, the attacker (corresponding to buyer in
our case) chooses a target. There is a one-to-one correspondence between targets and coordinate indices
of the parameter space. The utility is defined as a fixed linear function of the corresponding coordinate;
immediately implying its linearity in the parameter space in each region. In our setting, however, the utility
depends on multiple coordinates and its formula depends on the buyer’s choice.
The proof builds on Lemma 13. We show that for any buyer valuation viin the sequence, u(vi,ρ)is linear
in the region. Proving this claim is sufficient for concluding the statement. Let (j,k) =µ(vi), i.e.,jis
the tariff index and kis number of units that buyer valuation viselects under µ. Therefore, the utility for
the mechanism designer for menu ρ∈PµisI{k≥1}/parenleftig
p(j)
1(ρ) +kp(j)
2(ρ)/parenrightig
. Bothp(j)
1(ρ)andp(j)
2(ρ)grow
linearly as a function of ρ. Therefore, since the option that each buyer valuation selects (the tariff index
and the number of units) is fixed inside Pµ, the utility is also linear.
After establishing the partitioning of parameter space into convex polytopes with linear utilities, for op-
timization purposes, it seems enough only to consider menus corresponding to the extreme points. This
intuition is accurate conditioned on a small tweak. Depending on the tie-breaking rule of buyers among
menu options producing the same utility, the polytopes Pµmay not be closed. Therefore, depending on the
tie-breaking rule, we consider a menu in proximity to the extreme point but inside the polytope.
Definition 21 (E, extended set of extreme points (Balcan et al., 2015)) .For a given ε >0, setEis the
set of menus as follows: for any µand anyρthat is an extreme point of the closure of Pµ, ifρ∈Pµ, then
ρ∈E, otherwise, there exists ρ′∈Esuch thatρ′∈Pµand||ρ−ρ′||1≤ε. From now on, we may refer to E
as the extreme points.
Lemma 22. The number of extreme points, |E|is at most (Vℓ2K2/4)2ℓ.
Proof.Length-ℓmenus of two-part tariffs occupy a 2ℓ-dimensional parameter space. In each d-dimensional
space, an extreme point is the intersection of dlinearly independent hyperplanes. The total number of
hyperplanes defining the regions is H=V/parenleftbigℓ
2/parenrightbig/parenleftbigK
2/parenrightbig
, where for each buyer type compares the utility of any pair
of options, i.e., the number of units 0,...,Kand tariff indices 1,...,ℓ. Out of these hyperplanes, we need
2ℓof them to intersect to form an extreme point. Therefore, the number of extreme points is at most/parenleftbigH
2ℓ/parenrightbig
,
implying the statement.
The following lemma bounds the loss in utility where the set of menus is limited to the extreme points E. The
proof is similar to Balcan et al. (2015); however, the loss depends on the problem-specific utility functions.
17Published in Transactions on Machine Learning Research (03/2024)
Lemma 23. LetEbe as defined in Definition 21, then for any sequence of buyer valuations b=b1,...,bT,
andρ∗as the optimal menu in the hindsight:
maxρ∈ET/summationdisplay
t=1u(bt,ρ)≥T/summationdisplay
t=1u(bt,ρ∗)−2KεT.
Proof.The proof consists of a few simple steps: (i) since the mappings partition the space into regions with a
fixed mapping, there exists a mapping µsuch thatρ∗∈Pµ, (ii) the revenue of the buyer valuation sequence is
linear inPµas shown in Lemma 20, (iii) the closure of Pµis a convex polytope whose extreme points contain
the maximizers of the linear function/summationtext
bi∈bu(bi,ρ), (iv) one of the maximizers has cumulative utility at
least asρ∗, (v) the parameter vectors in εproximity of the extreme point inside Pµapproximately preserve
the revenue of the extreme points, (vi) since by definition of EtheL1distance of each member to an extreme
point is at most ε, there is at most εdistance in the upfront fee and per-unit fee for any tariffs, resulting in
the bound in the statement.
Full Information. We first provide an algorithm for the full information case specific to the finite number
of buyers. The main result of this section is provided below. The algorithm to achieve this regret guarantee
is a weighted majority algorithm (Algorithm 2) on the set of menus corresponding to the extreme points E.
Theorem 24. In the full information case for length- ℓmenus of two-part tariffs, when there are Vtypes of
buyers, running Algorithm 2 over the set of menus corresponding to set Eforβ= 1/√
Thas regret bounded
by˜O(Hℓ√
Tln(VℓK )).
The proof follows from Lemma 23 and the guarantee of weighted majority algorithm and is deferred to the
appendix.
Partial Information (bandit). In the partial information setting, in each time step t, we present the
arriving buyer a menu and only observe the option selected by the buyer (e.g., the tariff and the number
of units) in the presented menu. A natural approach in this setting is running the EXP3 algorithm and
using the weighted majority algorithm for the full information case as a subroutine. However, this approach
leads to a regret bound that is exponential in the size of the menu (this result is presented formally in
Appendix A). An alternative to this approach is estimating the revenue of other menus, more technically
finding an unbiased estimator withbounded range for the revenue of all the menus, and then running the full
informationalgorithmwiththeestimates,asintroducedbyAwerbuchandMansour(2003). Wetakethelatter
approach and find the estimates by employing the notion of barycentric spanners (Awerbuch and Kleinberg,
2008). A barycentric spanner is a basis in a vector space such that any vector can be represented as a linear
combination of basis vectors with bounded coefficients. By utilizing this concept, we provide algorithms with
a regret bound that is sublinear in the number of timesteps and polynomial in other parameters. Similar
ideas were employed in Balcan et al. (2015).
There are two main ideas deriving our bounded-regret algorithm. The first is a reduction from the partial
information case to the full information case assuming Oracle access to proper estimates of utilities for all
the menus, and the second is deriving these estimates. The first idea was introduced by Awerbuch and
Mansour (2003), and we directly use an inspired theorem by Balcan et al. (2015) that suits our setting more
accurately. For the second, we also use similar machinery to Balcan et al. (2015).
We first show how to estimate the utility of any menu by only using the response of the buyers to a limited
number of menus. In doing so, we take advantage of the dependence between responses of the buyers for
different menus to obtain estimates for unused menus. In order to estimate the expected revenue of each
menu over a time interval, it is sufficient to estimate the probability of selection of each option in the menu
(tariff index and number of units) by the buyers. Since the price of each option is determined by the menu,
we can infer the expected revenue using these probabilities. Note that the option that each buyer type selects
is fixed throughout each region. Balcan et al. (2015) use the dependence between these probabilities across
regions to find a limited set of menus that infer the estimates. An analogous argument to theirs in our setting
is as follows. Let Ibe the set of length- Vindicator vectors that, for each region Pµand each option (j,k),
indicate the (maximal set of) buyer types that select the option (j,k)given menus inPµ. The algorithm
18Published in Transactions on Machine Learning Research (03/2024)
presents the menus corresponding to the barycentric spanner of Ito buyers at random times and records
whether the buyer selects the corresponding option. We show the utility of each menu can be represented as
a linear function of its corresponding vectors in Iand, therefore, a linear function of the barycentric spanner
vectors of I. This is enough to derive the estimates.
Now, we describe the overall structure of the algorithm. The algorithm operates in time blocks, with each
block consisting of exploitation and exploration time steps. The exploration time steps are selected uniformly
at random within the block and are limited in number. In an exploitation step, the menu used is the output
of the full information algorithm, employing unbiased estimators from the previous time block. These menus
are always the extreme points E. During exploration time steps, the menus corresponding to the barycentric
spanner are used. At the end of each time block, the algorithm refines the estimators of all corner points
usingtheinformationgatheredintheexplorationphases. Theuniformrandomselectionoftimestepsensures
that under any arbitrary sequence of valuations, the values observed in exploration time steps are selected
uniformly at random, and thus, the estimator is unbiased (the expected value of the utility estimator for
each menu is equal to the utility of that menu). A detailed description and proof of the theorem are provided
in the appendix.
Theorem 25. In the partial information (bandit) case for length- ℓmenus of two-part tariffs, when there are
Vdifferent types of buyers, there is an algorithm with regret bound of ˜O(T2/3ℓ(HKV )1/3log1/3(VℓK )).
Technical contribution. Although the general structure of the algorithm is similar to Balcan et al. (2015),
theproblemsettingsarequitedifferent, andwhethersimilarideascouldworkinbothsettingsisnotapparent.
We are able to adapt the ideas to provide no-regret algorithms for menus of two-part tariffs. This adaptation
requires establishing new properties and definitions for our problem settings.
3.3 Distributional Learning for Two-Part Tariffs
We present distributional learning results for menus of two-part tariffs. The learning algorithm simply con-
siders all menus in the discretized set specified by Theorem 1 and outputs the empirical revenue-maximizing
menu given the samples. More specifically, for each menu in the discretized set, the algorithm computes the
cumulative revenue achieved from the samples and outputs the menu with the maximum cumulative revenue.
The revenue from each sample (buyer) for a fixed menu is the total payment corresponding to the buyer’s
utility maximizing option (tariff index and the number of units). This approach has a major difference with
the previous line of work, e.g., (Balcan et al., 2018c; 2020b; 2022b), that did not use a discretization and
optimized over the infinite parameter space.
Theorem 26. In the distributional setting, for length- ℓmenus of two-part tariffs, there ex-
ists a learning algorithm with sample complexityH2
2ε2(2ℓln (2KHℓ
ε) + ln (2/δ)),and running time
H2
2ε2/parenleftbig
2ℓln/parenleftbig2KHℓ
ε/parenrightbig
+ ln (2/δ)/parenrightbig
Kℓ/parenleftbig2HKℓ
ε/parenrightbig2ℓ.
Remark. For menus of length larger than one, i.e., ℓ>1, Theorem 26 provides much simpler algorithm and
its running time is roughly the square root of the running time of the previous result (Balcan et al., 2020b;
2022b) in the worst case in terms of parameters H,K, and 1/ε. Under extra structural assumptions, (Balcan
et al., 2022b) may result in better running times (see Appendix B for more details). Furthermore, in the
real-world applications of menus of two-part tariffs, the length of the menu is often a small number; for
example, there are a limited number of gym membership or delivery subscription options. Therefore, the
exponential dependence on the length of the menu might not be a significant issue in such settings.
Technical comparison to prior work. For both menus of lotteries and two-part tariffs, distributional
learning results were presented before (Balcan et al., 2018c; 2020b). Our discretization-based techniques
lead to improvements over the previously best-known algorithms. Our algorithms choose several menus in a
data-independent way (via data-independent discretization) and then select the best of them based on the
data (empirical risk minimization over a cover); however, the prior algorithms optimize over the infinite space
based on the sampled data (empirical risk minimization over the entire space utilizing geometric structure
of utility functions). In the context of two-part tariffs, our algorithm is much simpler than prior ones for the
same problem, yet it enjoys improved worst-case runtime guarantees compared to them Balcan et al. (2018c;
2020b) when the length of the menu is more than one (Theorem 26).
19Published in Transactions on Machine Learning Research (03/2024)
4 Menus of Lotteries
Consider selling mitems to a buyer. A set M=/braceleftbig/parenleftbig
ϕ(0),p(0)/parenrightbig
,/parenleftbig
ϕ(1),p(1)/parenrightbig
,...,/parenleftbig
ϕ(ℓ),p(ℓ)/parenrightbig/bracerightbig
⊆Rm×R,
whereϕ(0)=0andp(0)= 0is a length- ℓmenu of lotteries. Each ϕ(j)is a vector of length m. Under the
lottery/parenleftbig
ϕ(j),p(j)/parenrightbig
, a buyer receives each item iwith probability ϕ(j)[i]and pays a price of p(j). The buyer’s
expected utility for the lottery/parenleftbig
ϕ(j),p(j)/parenrightbig
is their expected value for the lottery less their payment. We
consider additive and unit-demand buyers. For additive buyers, their value for lottery jis/summationtextm
i=1v(ei)·ϕ(j)[i],
wherev(ei)is their value for item i. The buyer’s expected utility is/summationtextm
i=1v(ei)·ϕ(j)[i]−p(j). Note that
for additive buyers, due to linearity of expectation, it does not matter whether the allocations of the items
in a lottery, are independent or correlated. For unit-demand buyers, without loss of generality, we only
consider lotteries such that/summationtextm
i=1ϕ(j)[i]≤1. Under this constraint, for each lottery j, the allocations of the
items are dependent, and the buyer never receives more than one item. In this case, the utility for lottery
jhas the same expression as for additive buyers. Presented with a menu of lotteries, the buyer selects a
utility-maximizing lottery/parenleftbig
ϕ(j∗),p(j∗)/parenrightbig
and the mechanism achieves revenue p(j∗).
Putting the problem formulation in the context of Section 2, Mis the set of all menus of lotteries, each
parameterizedby ρwhichinthiscasecontainsall ϕ(j)andp(j),whereeachϕ(j)[i]∈[0,1]andp(j)is∈[0,mH ]
for the additive setting (and ∈[0,H]for the unit-demand setting). Πis the set of buyer valuations and
u: Π×C→ [0,mH ]be a utility function where u(v,ρ)measures the revenue of the menu with parameters
ρon buyer valuations v∈Π.
4.1 Discretization procedure
In this section, we introduce a rounding procedure for menus of lotteries. In this procedure, given any
vector of parameters (representing a menu) with arbitrary coordinates, we find a transformation to another
vector that has two properties; first, the revenue of the output is nearly as high as the original menu for any
valuation; secondly, the coordinates corresponding to allocation probabilities and prices belong to a finite
set of values. This rounding procedure performed on all possible menus results in a final set of outcomes.
We perform the learning algorithms over this finite set.
Theorem 27. Given a menu of lotteries Mand parameters 0< α < 1,0< δ < 1, andK, an arbitrary
natural number, Algorithm 5 outputs menu M′such that Rev(M′)≥Rev(M)(1−δ)(1−α)K−(2K+ 1)α−
mH(1−δ)K. The set of possible allocation probabilities is {0,(1−α)K′,(1−α)K′−1,...(1−α)0= 1},
whereK′=⌊1/αln (Hm/α )⌋and the set of possible prices is {0,Hmα, 2Hmα,...Hm}. This constitutes
a space with at most O/parenleftig
(1/αℓm+ℓ) (ln (Hm/α ))lm/parenrightig
discrete points, when limiting to length- ℓmenus and
O/parenleftig
2(1/αm+1)(ln (Hm/α ))m/parenrightig
discrete points for arbitrary-length menus.
Overview of Algorithm 5. The algorithm consists of three main steps, and its logic is similar to that
of Dughmi et al. (2014). In step 1, we divide the lotteries in the menu exceeding a minimum price into K
levels based on their price (and remove the ones below the minimum). The division in prices is proportional
to powers of (1−δ)with a higher level khaving a higher price, compared to a lower level k′< k. Step 2
rounds down the allocation probability coordinates to a finite set. By multiplying ϕby(1−α)K−kand then
rounding to integer powers of (1−α), the allocation probabilities of lower-price levels decrease by a larger
factor, making lower-price levels less desirable. Step 3 rounds down the prices, first by multiplying all prices
by the same factor, (1−α)K, then by rounding to multiples of αand finally by subtracting 2kα, which results
in more subtraction of price for originally higher-price entries. The main insight behind nearly preserving the
revenue of the original menu (and circumventing the issue with simple rounding) is that prices of the more
expensive lotteries (higher-price level) are decreased more than the lower-price ones, while their allocation
decreases by a lower factor. This ensures that no buyer with any valuation , switches from a higher-price
level to a lower-price, after the rounding.
Before providing the proof of the discretization step, we note that this procedure for menus of lotteries needs
extra care and the common rounding of the parameters may result in arbitrarily lower revenue. For example,
if there are two lotteries with a similar utility for the buyer but a large difference in prices, minor changes
20Published in Transactions on Machine Learning Research (03/2024)
Algorithm 5: (Almost) revenue preserving rounding for menus of lotteries
Input:Menu of lotteries Mwith entries of pairs (ϕ,p),K∈N, andαsuch that 0<α< 1.
Step 1: Partition the entries (ϕ,p)of the menu Minto levels, where each level k, fork= 1,...,K,
contains all entries whose price is in the range mH(1−δ)K−k+1<p≤mH(1−δ)K−k.
For every entry (ϕ,p)in levelk, put an entry (ϕ′,p′)inM′whereϕ′is the outcome of step 2 and p′is
obtained by step 3.
Step 2: multiplyϕby(1−α)K−k, and round down all allocation probabilities to the set of zero and all
integer powers of (1−α)in the range [α
Hm,1].
Step 3: First, multiplying pby a factor of (1−α)K, then rounding pdown to an integer multiple of α,
and then subtracting 2kα.
Output:M′: the modified menu.
in the probability of allocations or the prices may make the user switch from the high-price lottery to the
low-price one. What follows is a concrete example of why standard rounding procedures fail.
Example 1. Consider a menu of three lotteries.
alloc. prob. priceutility
0 00
0.26 0.24-0.084
0.95 0.520.05alloc. prob. priceutility
0 00
0.25 0.1250.025
0.5 0.5-0.2
alloc. prob. priceutility
0 00
0.5 0.1250.175
1 0.50.1alloc. prob. priceutility
0 00
0.5 0.250.05
1 1-0.4
Consider the buyer that has value 0.6for the item. The first table shows the original menu. With this menu
the buyer’s highest utility option is the last lottery that causes the highest revenue, i.e., Rev = 0.52. The
following tables show the new menus after rounding down the allocation probabilities and prices, rounding
up allocation probabilities and rounding down prices, and rounding up allocation probabilities and prices (all
to powers of 1/2), respectively. All these transformations result in the highest utility lottery changing to the
middle lottery which causes smaller revenue.
Proof of Theorem 27. Most of this proof is identical to that of Dughmi et al. (2014). Note that in the
algorithm, the original entries in a menu are divided into levels k= 1,...,Ksuch thatk= 1is the
lowest-price level and k=Kis the highest price one. First, we show that if a buyer’s utility-maximizing
lottery is in level kgivenM, their utility-maximizing lottery in M′is never in a lower-price level k′< k.
Intuitively, the reason is that the lotteries with lower-level prices have their allocation reduced more and
their prices reduced less than the ones in higher levels. More formally, let (x,p)be at level kand(y,q)at
levelk′<k. Also, let (x′,p′)and(y′,q′)be the transformed lotteries in the output of the algorithm. Than,
p′−q′<((1−α)Kp−2kα)−((1−α)Kq−2k′α−α)≤(1−α)K(p−q)−α, and for every valuation v,
x′·v−y′·v >((1−α)K−k+1x·v−α)−(1−α)K−k′y·v≥(1−α)K(x·v−y·v)−α. Now, consider an
arbitrary valuation vthat has higher utility choosing (x,p)thany,q. Therefore x·v−p≥y·v−q, and
thereforep−q≤x·v−y·v. Combining this inequality with the ones above implies x′·v−p′≥y′·v−q′.
Secondly, we compute an upper bound on the loss incurred. Suppose the original utility-maximizing lottery
was(x,p)inM. Also, suppose in M′, the utility-maximizing lottery is (y′,q′)which is the transformation
of(y,q). The first scenario is when p≥mH(1−δ)K. Note that in this case, qmay be smaller by
a factor (1−δ)thanp, then to obtain q′we first lost a multiplicative factor of (1−α)Kand then an
additive factor of at most (2K+ 1)α(including the rounding). Thus q′≥(1−δ)(1−α)Kp−(2K+ 1)α.
In the second case where p < mH (1−δ)K, the loss is at most mH(1−δ)K. Therefore, in any case,
q′≥(1−δ)(1−α)Kp−(2K+ 1)α−mH(1−δ)K.
21Published in Transactions on Machine Learning Research (03/2024)
Thirdly, the set of possible prices is {0,Hmα, 2Hmα,...Hm}which is of size 1/αand the set of possible
allocation probabilities is {0,(1−α)K′,(1−α)K′−1,...(1−α)0= 1}, forK′=⌊1/αln (Hm/α )⌋which is
of size 1/αln(Hm/α ). In theℓ-length menus, there are ℓprices andmℓallocation probabilities in total. In
the unlimited-length menus, we consider the possibility that each potential lottery (each distinct vector of
parameters) belongs to the lottery or not. This analysis gives us the final size of the discrete points.
Technical contribution. Our discretization scheme (Algorithm 5) extends that of Dughmi et al. (2014)
in the following aspects: (i) We remove the lower bound assumption on value distribution: Dughmi et al.
(2014) assume values belong to [1,H], and we extend the discretization scheme to work when there is no
lower bound on value distributions; i.e., values are in [0,H]. (ii) Supporting additive valuations: The original
discretization in Dughmi et al. (2014) works for unit-demand valuations. (iii) We also modify the algorithm
to support limited-length menus. As a consequence, we are able to provide improved regret bounds and
running times when the size of the menu is limited. These extensions are done by small modifications to the
algorithm and expand the scope of the application of the scheme.
4.2 Online Learning
We provide bounded-regret online learning algorithms in full and partial information settings for fixed and
arbitrary-length menus of lotteries. The setting considered is as follows. In each round, a new buyer arrives,
and a length- ℓlottery menu is presented to the buyer. The buyer selects her utility-maximizing lottery jand
paysp(j). The mechanism achieves revenue p(j). Missing proofs and explicit descriptions of the algorithms
are deferred to Appendix B.
In the full information setting, the seller sees the revenue generated for all the possible menus. Similar to
the previous section, we run Algorithm 2 (a weighted majority algorithm) over the discretized set as the
outcome of Algorithm 5 and derive the following results for the length- ℓmenus and arbitrary length menus.
Theorem 28. In the full information case for length- ℓmenus of lotteries, running Algorithm 2 over the
discretized set of menus specified in Theorem 27 for α=T−1,β=T−0.5,K=T0.5, andδ=T−0.5has
regret ˜O(m2Hℓ√
T).
Theorem 29. In the full information case for arbitrary length menus of lotteries, running Algorithm 2 on
menus specified in Theorem 27 for α=T−1/(2m+2),β=T−1/(m+1),K=T1/(m+1), andδ=T−1/(m+1)has
regret ˜O(mHT1−1/(2m+4)lnm(mHT )).
Inthepartialinformationsetting,theselleronlyobservestherevenuegeneratedforthemenuathand. Similar
to the previous section, we run Algorithm 3 (EXP3 algorithm) over the discretized set as the outcome of
Algorithm 5 and derive the following result for length ℓmenus.
Theorem 30. In the partial information case for length- ℓmenus of lotteries, running Algorithm 3 over
discretized set of menus in Theorem 27 for α=T−1/(ℓm+2),β=γ=T−1/(4ℓm+8),K=T1/(2ℓm+4), and
δ=T−1/(2ℓm+4)has regret ˜O(m2HℓT1−1/(2ℓm+4)lnℓm+1(mHT )).
Forthecasewith Vbuyertypes,weusesimilarmachinerytoSection3.2.3toderiveboundedregretalgorithms
in the full and partial information settings. The discussion of how to adapt to the lotteries setting is deferred
to the appendix. the partial information case.
Theorem 31. In the full information case for length- ℓmenus of lotteries, when there are Vtypes of buyers,
there is an algorithm with regret bound of O(m2Hℓ√
Tln (Vℓ)).
Theorem 32. In the partial information (bandit) case for length- ℓmenus of lotteries,
when there are Vdifferent types of buyers, there is an algorithm with regret bound of
O(T2/3(ℓm)4/3(HV)1/3log1/3(Vℓ)).
Remark. The above results hold under adversarial input. Unlike menus of two-part tariffs (and many other
families of algorithms and mechanisms discussed in Balcan et al. (2018b; 2020a)), for menus of lotteries, we
provide evidence that dispersion , a sufficient condition for online learning under smooth distributions, may
not hold. A formal result is stated as Theorem 33.
22Published in Transactions on Machine Learning Research (03/2024)
4.2.1 Failure of Dispersion for menus of lotteries
In this section, we prove that without making extra assumptions about optimal menus of lotteries, both
definitions of dispersion (Definitions 15 and 38) fail. In particular, we show that the failure of both conditions
happens if the optimal menu (maximizer) has two lotteries close to each other (similar coordinates) and
satisfies some other properties. Example 2 illustrates a setting where there are lotteries with arbitrarily close
coordinates in the optimal menu.
Theorem 33. Let the maximizer ρ∗have the following properties, where ϕ(1)
ρ∗,p(1)
ρ∗,ϕ(2)
ρ∗,p(2)
ρ∗are the coor-
dinates ofρ∗, respectively illustrating the probability of allocating item one in lottery 1, the price of lottery
1, the probability of allocating item one in lottery 2, the price of lottery 2, and the allocation probability for
other items are the same across these lotteries.
1.p(1)
ρ∗−p(2)
ρ∗= (L+ 1/2)ε, whereLis the Lipschitz parameter.
2.ϕ(1)
ρ∗−ϕ(2)
ρ∗= (L+ 1)ε/c+ε/2.
3.cis a constant such that c≤H.
In this case, for every κ-bounded distribution whose density is also lower-bounded by 1/κ, the conditions of
Definitions 15 and 38, are violated. In particular, in Definition 15, the probability of a hyperplane crossing
theε-radius ball centered at the maximizer is a constant depending on c; and in Definition 38, there exists a
pair of points such that the expected number of times that their loss function difference violates the Lipschitz
condition for any Lipschitz constant L′=L/2is a constant depending on c.
Proof.We first show why Definition 15 fails. Consider a ball of radius εcentered at the maximizer ρ∗. Let
this ball be B. We show that the probability of a hyperplane crossing Bis constant. Consider a point
ρ∈B. We first find the probability density of hyperplanes going through ρ. Then, we integrate it to find the
probability of crossing the ball. The following equation shows for what value of v(the value for the item),
the hyperplane goes through ρ.
vϕ(1)
ρ−p(1)
ρ=vϕ(2)
ρ−p(2)
ρ
v=p(1)
ρ−p(2)
ρ
ϕ(1)
ρ−ϕ(2)
ρ
Letvmin
Bandvmax
Bbe the minimum value of vfor which the hyperplane crosses the ball (i.e., there is ρ∈B
such thatvmin
B=p(1)
ρ−p(2)
ρ
ϕ(1)
ρ−ϕ(2)
ρ), and the maximum value respectively. The probability that the hyperplane
crosses the ball is/integraltextvmax
B
vmin
Bf(v)dv, wheref(v)is the density function of the value for the item.
We consider the following points. These points are all in εproximity of ρ∗, therefore, fall in a ball of radius
εcentered at ρ∗. Consider points with p(2)=p(2)
ρ∗andϕ(2)=ϕ(2)
ρ∗. Letp(1)be in [p(2)
ρ∗+Lε,p(2)
ρ∗+ (L+ 1)ε].
Letϕ(1)be in [ϕ(2)
ρ∗+ (L+ 1)ε/c,ϕ(2)
ρ∗+ (L+ 1)ε/c+ε].
With the above construction, the numerator ranges from Lεto(L+ 1)ε, and the denominator ranges from
(L+ 1)ε/cto(L+ 1)ε/c+ε. Therefore, vmin
B=Lc
L+c+1andvmax
B=c. Forκ-bounded distribution with
support [0,1],/integraltextvmax
B
vmin
Bf(v)dvis at least
c−Lc
L+c+1
κ=c(c+1)
L+c+1
κ;
which is constant for a constant c.
23Published in Transactions on Machine Learning Research (03/2024)
Now, we show that Definition 38 fails. To do so, we still consider pair of points ρandρ′which correspond
tovmin
Bandvmax
B, respectively. If we consider the line segment connecting ρandρ′, the probability of
the hyperplane crossing these two points is still/integraltextvmin
B
vmin
Bf(v)dvwhich again for κ-bounded distribution with
support [0,1]whose density is also lower-bounded by 1/κ,/integraltextvmax
B
vmin
Bf(v)dvis at least
c−Lc
L+c+1
κ=c(c+1)
L+c+1
κ;
which is constant for a constant c. Note that|p(1)
ρ−p(2)
ρ′|≥Lεand|p(2)
ρ−p(1)
ρ′|≥Lεwhich implies anytime
the hyperplane crosses between ρandρ′, the difference in the loss, |ℓt(ρ)−ℓt(ρ′)|is at leastLε. Also, the
Euclidean distance between ρandρ′is less than 2ε. Therefore, the Lipschitz condition for constant L′=L/2
is violated a constant fraction of times in expectation.
The following example shows that in the optimal menu of lotteries, lottery pairs can be arbitrarily close to
each other.
Example 2 ((Daskalakis et al., 2014)) .Consider the case of two items, when the buyer’s value for each item
is drawn i.i.d. from the distribution supported on [0,1]with density function f(x) = 2(1−x). Daskalakis
et al. prove for this example that the unique (up to differences of measure zero) optimal mechanism has
uncountable menu complexity. That is, the number of distinct options available for the buyer to purchase
is uncountable. They show that the optimal mechanism contains the following four kinds of options: (a)
the buyer can receive item one with probability 1, and item two with probability2
(4−5x)2paying the price
2−3x
4−5x+2x
(4−5x)2, for anyx∈[0,≈.0618), (b) the buyer can receive item two with probability 1, and item one
with probability2
(4−5x)2paying the price2−3x
4−5x+2x
(4−5x)2, for anyx∈[0,≈.0618), (c) the buyer can receive
both items and pay ≈.5535, and (d) the buyer can receive neither item and pay nothing.
Technical contribution compared to prior work. Dispersion property has been shown to hold for
various algorithm and mechanism design problems (Balcan et al., 2018b; 2020a; Balcan and Sharma, 2021;
Balcan et al., 2022a). This section illustrates the first evidence for the failure of the dispersion property.
4.3 Distributional Learning
In the distributional setting, we have sample access to buyers’ valuations. The value of the buyer for item iis
drawn from distribution Diwith support [0,H]m; we do not assume independence among items. Similar to
the distributional learning algorithm for menus of two-part tariffs, the algorithm simply considers all menus
in the discretized set specified by Theorem 27 and outputs the empirical revenue-maximizing menu given
the samples. The revenue from each sample (buyer) for a fixed menu is the payment corresponding to the
buyer’s utility-maximizing lottery in the menu.
Theorem 34. For length- ℓmenus of lotteries, there is a discretization-based distributional
learning algorithm with sample complexity ˜O/parenleftbig
m2H2/ε2(ℓm+ ln (2/δ))/parenrightbig
, and running time
˜O/parenleftig/parenleftbig
2m2H2/ε2/parenrightbigℓm+ℓ+1ℓ(ℓm+ ln (2/δ)) lnℓm(mH/ε ln (mH/ε ))/parenrightig
.
Remark. For the limited menu length, the sample complexity of Theorem 34 is roughly the same as Balcan
et al. (2018c), but the advantage is that we provide an efficient algorithm when mandℓare constant. The
analysis for arbitrary-length menus is provided in the appendix as Theorem 57. The sample complexity
and running time provided are similar to that of Dughmi et al. (2014), however, Theorem 57 works for a
more general setting. Dughmi et al. (2014) provide a lower bound on the sample complexity, verifying an
exponential dependence on the number of items.
Technical contribution compared to prior work. Similar to menus of two-part tariffs, our algorithms
choose several menus in a data-independent way (via data-independent discretization) and then select the
bestofthembasedonthedata(empiricalriskminimizationoveracover); however, theprioralgorithms(Bal-
can et al., 2018c; 2020b) optimize over the infinite space based on the sampled data (empirical risk mini-
mization over the entire space utilizing geometric structure of utility functions). In the context of lotteries,
24Published in Transactions on Machine Learning Research (03/2024)
compared to the previous distributional learning results for fixed-length menus (Balcan et al., 2018c), our
algorithm requires similar sample complexity; however, it has an efficient implementation. For arbitrary-
length menus, our algorithm provides similar sample complexity and running time compared to (Dughmi
et al., 2014); however, it works for a slightly more general setting.
5 Discussion
This paper contributes to both learning theory and mechanism design by studying prominent families of
mechanisms from a learning perspective. Our work is focused on learning menu mechanisms that go beyond
selling the items separately. Menus of lotteries provide a list of randomized allocations and their corre-
sponding prices to the buyers and are specifically advantageous for selling multiple items. Menus of two-part
tariffs, on the other hand, are employed for selling multiple units (copies) of an item by presenting a list of
up-front fees and per-unit fees to the buyer. The two families of mechanisms are pricing schemes commonly
studied for revenue maximization in the sale of goods. ‌Both are presented as lists (menus) of options from
which potential buyers can choose. From a structural perspective, both problems involve a utility function
(in this case, revenue) that is piecewise linear in the parameter space. Our findings suggest that similar
techniques can be applied to both problems.
We provide a suite of results with regard to these two families of mechanisms. By leveraging the structure
of menus of two-part tariffs and lotteries, we provide a revenue-preserving reduction to a finite number of
menus (discretization). Using this approach, we provide the first online learning algorithms for menus of
lotteries and two-part tariffs with strong regret-bound guarantees and propose algorithms with significantly
improved running times over prior work for the distributional settings. When there is a limited number
of buyer types, we provide a reduction to online linear optimization, which enables us to obtain no-regret
guarantees by presenting buyers with menus that correspond to a barycentric spanner. Finally, for the first
time, weprovideevidenceofthefailureofthe“dispersion”property(Balcanetal.,2018b;2020a)—asufficient
condition to provide a no-regret algorithm under smooth distributional assumption, which is widely applied
to parametric algorithm and mechanism design problems—for a specific problem (menus of lotteries).
DiscretizationversusDispersion. Themajorityofthepaperfocusesononlinelearningofthesefamiliesof
mechanisms. Two of the commonly used techniques for this setting are (the more traditional) discretization-
based and (the recently developed) dispersion-based techniques. Menus of lotteries and two-part tariffs
are examples of parametric algorithm or mechanism design, where the objective function, here revenue,
has sharp discontinuities in the parameter space, and the standard procedures, such as rounding down
the parameters to multiples of ε, may result in arbitrary revenue loss. A discretization scheme means
that there exists a grid in the parameter space such that for any arbitrary parameter vector, there is a
corresponding parameter vector in proximity over the grid generating similar revenue. However, finding the
corresponding parameter vector (the direction to move from the original parameter vector in the space) needs
taking extra care, and moving in an arbitrary direction may cause a large revenue loss. In contrast to the
discretization scheme, another method developed for proving online learnability of parameterized algorithms,
calleddispersion (Balcan et al., 2018b; 2020a), asserts that under smoothness assumptions moving in a small
ball of parameter vectors, does not face sharp discontinuities with high probability. This means that with
high probability, moving in any direction preserves similar revenue. Nevertheless, we show evidence that the
dispersion may not hold for menus of lotteries (Theorem 33) and while dispersion holds for menus of two-part
tariffs (Propositions 16 and 39), it heavily uses the smoothness assumption. In conclusion, although a small
but arbitrary modification may change the revenue drastically when starting from a parameter vector, in
designing our discretization scheme, we show a specific direction such that a small modification along that
direction preserves the revenue. See Theorems 1 and 27.
Lower bound for regret terms. In the full information case, the dependence of the regret bounds on T
is tight according to Nisan et al. (2007), Theorem 4.8. In the bandit setting, our dependence on Tmatches
a lower bound provided by Kleinberg et al. (2008) for general globally Lipschitz functions even though
the utility functions in our case are only piecewise Lipschitz (not globally Lipschitz). The construction
in Kleinberg et al. (2008) does not immediately imply a lower bound for our case since, since on one hand,
learning piecewise Lipschitz functions is harder than globally Lipschitz ones, and on the other hand, in our
25Published in Transactions on Machine Learning Research (03/2024)
case, the utility functions have more structure beyond Lipschitzness in each piece. It is a nontrivial open
question if the dependence is tight for our case. Finding a lower bound for the dependence on the other
parameters is an interesting open problem. Similar dependence appears both in our discretization-based
and dispersion-based algorithms. The question of whether such dependencies can be avoided motivated us
to study more structured settings, such as the limited buyer type setting. In the limited buyers’ type case,
where we utilize the knowledge of the potential buyer types and interdependence of utilities across experts,
the dependence is improved.
Computational Efficiency. Our discretization-based learning approach has the strength of not relying
on any extra assumptions about the data and results in no-regret learning algorithms in the online learning
setting without any extra assumptions. However, the drawback of this approach is that the algorithms may
notbecomputationallyefficient. Concerningknownefficientalgorithms, fortheproblemofmenusoftwo-part
tariff even in the simpler problem of distributional learning, prior results were not computationally efficient
either (Balcan et al., 2020b), and our discretization-based algorithms improve upon those and satisfy the
best computational guarantees in the worst case. Exploring computational complexity and the existence of
more efficient algorithms is an interesting open direction. We have also taken steps to explore the possibility
of more efficient algorithms by adding extra structure, e.g., smooth distributional assumption or limited
buyer type assumption, to our settings. Establishing the “dispersion” property under smooth distributional
assumptions enables us to use more refined online learning algorithms (a continuous version of multiplicative
weight update algorithm that uses the geometric structure of utility functions), but this property has not
been studied for menus of lotteries or two-part tariffs. One of our contributions is establishing this property
for menus of two-part tariffs and obtaining more efficient algorithms. However, surprisingly we show this
property does not work for lotteries. In the limited buyer type settings, we utilize the knowledge of the
potential buyer types and interdependence of utilities across menus and provide algorithms with improved
running time.
Open Directions. An open question is whether there is a generalization capturing the techniques applied
to both menus of lotteries and two-part tariffs. It is unclear whether such a generalization capturing both
problems exists. The key difficulty we face in generalizing the techniques we used for menus of two-part
tariffs and lotteries stems from the difference in the structure of the utility functions, specifically, the shape
of discontinuity hyperplanes. As shown in Theorem 33, we provide evidence of the failure of the disper-
sion technique for lotteries; however, this technique works for two-part tariffs (Propositions 16 and 39).
Furthermore, the two mechanisms needed different discretization methods.
6 Acknowledgement
The authors would like to thank Avrim Blum, Misha Khodak, Rattana Pukdee, Dravyansh Sharma, and
anonymous reviewers for helpful feedback and comments. This material is based on work supported in part
by the National Science Foundation under grant CCF-1910321 and a Simons Investigator Award.
References
Noga Alon, Moshe Babaioff, Yannai A. Gonczarowski, Yishay Mansour, Shay Moran, and Amir Yehudayoff.
Submultiplicative glivenko-cantelli and uniform convergence of revenues. In Isabelle Guyon, Ulrike von
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett,
editors,Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pages 1656–1665, 2017a.
Noga Alon, Nicolò Cesa-Bianchi, Claudio Gentile, Shie Mannor, Yishay Mansour, and Ohad Shamir. Non-
stochastic multi-armed bandits with graph-structured feedback. SIAM J. Comput. , 46(6):1785–1826,
2017b. doi: 10.1137/140989455.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. Gambling in a rigged casino: The
adversarial multi-armed bandit problem. In Proceedings of IEEE 36th annual foundations of computer
science, pages 322–331. IEEE, 1995.
26Published in Transactions on Machine Learning Research (03/2024)
Baruch Awerbuch and Robert Kleinberg. Online linear optimization and adaptive routing. J. Comput. Syst.
Sci., 74(1):97–114, 2008. doi: 10.1016/j.jcss.2007.04.016.
Baruch Awerbuch and Yishay Mansour. Adapting to a reliable network path. In Elizabeth Borowsky and
Sergio Rajsbaum, editors, Proceedings of the Twenty-Second ACM Symposium on Principles of Distributed
Computing, PODC 2003, Boston, Massachusetts, USA, July 13-16, 2003 , pages 360–367. ACM, 2003. doi:
10.1145/872035.872090.
Maria-Florina Balcan. Data-driven algorithm design. In Tim Roughgarden, editor, Beyond the Worst-Case
Analysis of Algorithms , pages 626–645. Cambridge University Press, 2020. doi: 10.1017/9781108637435.
036. URL https://doi.org/10.1017/9781108637435.036 .
Maria-Florina Balcan and Avrim Blum. Approximation algorithms and online mechanisms for item pricing.
In Joan Feigenbaum, John C.-I. Chuang, and David M. Pennock, editors, Proceedings 7th ACM Conference
on Electronic Commerce (EC-2006), Ann Arbor, Michigan, USA, June 11-15, 2006 , pages 29–35. ACM,
2006. doi: 10.1145/1134707.1134711.
Maria-Florina Balcan and Dravyansh Sharma. Data driven semi-supervised learning. In Marc’Aurelio Ran-
zato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Ad-
vances in Neural InformationProcessingSystems 34: AnnualConference onNeural InformationProcessing
Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual , pages 14782–14794, 2021.
Maria-Florina Balcan, Avrim Blum, Jason D Hartline, and Yishay Mansour. Reducing mechanism design to
algorithm design via machine learning. Journal of Computer and System Sciences , 74(8):1245–1270, 2008.
Maria-Florina Balcan, Avrim Blum, Nika Haghtalab, and Ariel D. Procaccia. Commitment without re-
grets: Online learning in stackelberg security games. In Tim Roughgarden, Michal Feldman, and Michael
Schwarz, editors, Proceedings of the Sixteenth ACM Conference on Economics and Computation, EC ’15,
Portland, OR, USA, June 15-19, 2015 , pages 61–78. ACM, 2015. doi: 10.1145/2764468.2764478.
Maria-Florina Balcan, Tuomas Sandholm, and Ellen Vitercik. Sample complexity of automated mechanism
design.Advances in Neural Information Processing Systems , 29, 2016.
Maria-Florina Balcan, Vaishnavh Nagarajan, Ellen Vitercik, and Colin White. Learning-theoretic foun-
dations of algorithm configuration for combinatorial partitioning problems. In Conference on Learning
Theory, pages 213–274. PMLR, 2017.
Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to branch. In Interna-
tional conference on machine learning , pages 344–353. PMLR, 2018a.
Maria-Florina Balcan, Travis Dick, and Ellen Vitercik. Dispersion for data-driven algorithm design, online
learning, and private optimization. In 2018 IEEE 59th Annual Symposium on Foundations of Computer
Science (FOCS) , pages 603–614. IEEE, 2018b.
Maria-Florina Balcan, Tuomas Sandholm, and Ellen Vitercik. A general theory of sample complexity for
multi-item profit maximization. In Proceedings of the 2018 ACM Conference on Economics and Compu-
tation, pages 173–174, 2018c.
Maria-Florina Balcan, Travis Dick, and Wesley Pegden. Semi-bandit optimization in the dispersed setting.
InConference on Uncertainty in Artificial Intelligence , pages 909–918. PMLR, 2020a.
Maria-Florina Balcan, Siddharth Prasad, and Tuomas Sandholm. Efficient algorithms for learning revenue-
maximizing two-part tariffs. In Proceedings of the Twenty-Ninth International Joint Conference on Arti-
ficial Intelligence, {IJCAI-20}, 2020b.
Maria-Florina Balcan, Dan DeBlasio, Travis Dick, Carl Kingsford, Tuomas Sandholm, and Ellen Vitercik.
Howmuchdataissufficienttolearnhigh-performingalgorithms? generalizationguaranteesfordata-driven
algorithm design. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing ,
pages 919–932, 2021a.
27Published in Transactions on Machine Learning Research (03/2024)
Maria-Florina Balcan, Mikhail Khodak, Dravyansh Sharma, and Ameet Talwalkar. Learning-to-learn non-
convexpiecewise-lipschitzfunctions. InMarc’AurelioRanzato,AlinaBeygelzimer,YannN.Dauphin,Percy
Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34:
Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021,
virtual, pages 15056–15069, 2021b.
Maria-Florina Balcan, Misha Khodak, Dravyansh Sharma, and Ameet Talwalkar. Provably tuning the
elasticnet across instances. In NeurIPS , 2022a.
Maria-Florina Balcan, Christopher Seiler, and Dravyansh Sharma. Faster algorithms for learning to link,
align sequences, and price two-part tariffs. arXiv preprint arXiv:2204.03569 , 2022b.
Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to branch: Generaliza-
tion guarantees and limits of data-independent discretization. J. ACM, dec 2023a. ISSN 0004-5411.
Maria-FlorinaBalcan, TuomasSandholm, andEllenVitercik. Generalizationguaranteesformulti-itemprofit
maximization: Pricing, auctions, and randomized mechanisms. Operations Research, 2023b.
Raef Bassily, Adam D. Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient
algorithms and tight error bounds. In 55th IEEE Annual Symposium on Foundations of Computer Science,
FOCS 2014, Philadelphia, PA, USA, October 18-21, 2014 , pages 464–473. IEEE Computer Society, 2014.
doi: 10.1109/FOCS.2014.56.
Avrim Blum and Jason D. Hartline. Near-optimal online auctions. In Proceedings of the Sixteenth Annual
ACM-SIAM Symposium on Discrete Algorithms, SODA 2005, Vancouver, British Columbia, Canada, Jan-
uary 23-25, 2005 , pages 1156–1163. SIAM, 2005. URL http://dl.acm.org/citation.cfm?id=1070432.
1070597.
Avrim Blum, Vijay Kumar, Atri Rudra, and Felix Wu. Online learning in online auctions. Theoretical
Computer Science , 324(2-3):137–146, 2004.
Patrick Briest, Shuchi Chawla, Robert Kleinberg, and S Matthew Weinberg. Pricing randomized allocations.
InProceedings of the twenty-first annual ACM-SIAM symposium on Discrete Algorithms , pages 585–597.
SIAM, 2010.
Johannes Brustle, Yang Cai, and Constantinos Daskalakis. Multi-item mechanisms without item-
independence: Learnability via robustness. In Péter Biró, Jason D. Hartline, Michael Ostrovsky, and
Ariel D. Procaccia, editors, EC ’20: The 21st ACM Conference on Economics and Computation, Virtual
Event, Hungary, July 13-17, 2020 , pages 715–761. ACM, 2020. doi: 10.1145/3391403.3399541.
Sébastien Bubeck, Nikhil R. Devanur, Zhiyi Huang, and Rad Niazadeh. Online auctions and multi-scale
online learning. In Constantinos Daskalakis, Moshe Babaioff, and Hervé Moulin, editors, Proceedings of
the 2017 ACM Conference on Economics and Computation, EC ’17, Cambridge, MA, USA, June 26-30,
2017, pages 497–514. ACM, 2017. doi: 10.1145/3033274.3085145.
Nicolo Cesa-Bianchi, Claudio Gentile, and Yishay Mansour. Regret minimization for reserve prices in second-
price auctions. IEEE Transactions on Information Theory , 61(1):549–564, 2014.
Vincent Cohen-Addad and Varun Kanade. Online optimization of smoothed piecewise constant functions. In
AartiSingh andXiaojin(Jerry)Zhu, editors, Proceedings of the 20th International Conference on Artificial
Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA , volume 54 of
Proceedings of Machine Learning Research , pages 412–420. PMLR, 2017.
Richard Cole and Tim Roughgarden. The sample complexity of revenue maximization. In David B. Shmoys,
editor,Symposium on Theory of Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014 ,
pages 243–252. ACM, 2014.
Partha Dasgupta, Peter Hammond, and Eric Maskin. The implementation of social choice rules: Some
general results on incentive compatibility. The Review of Economic Studies , 46(2):185–216, 1979.
28Published in Transactions on Machine Learning Research (03/2024)
Constantinos Daskalakis, Alan Deckelbaum, and Christos Tzamos. The complexity of optimal mechanism
design. In Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms , pages
1302–1318. SIAM, 2014.
Nikhil R. Devanur, Zhiyi Huang, and Christos-Alexandros Psomas. The sample complexity of auctions with
side information. In Daniel Wichs and Yishay Mansour, editors, Proceedings of the 48th Annual ACM
SIGACT Symposium on Theory of Computing, STOC 2016, Cambridge, MA, USA, June 18-21, 2016 ,
pages 426–439. ACM, 2016.
Shaddin Dughmi, Li Han, and Noam Nisan. Sampling and representation complexity of revenue maximiza-
tion. InInternational Conference on Web and Internet Economics , pages 277–291. Springer, 2014.
Paul Dütting, Zhe Feng, Harikrishna Narasimhan, David Parkes, and Sai Srivatsa Ravindranath. Optimal
auctions through deep learning. In International Conference on Machine Learning , pages 1706–1715.
PMLR, 2019.
Edith Elkind. Designing and learning optimal finite support auctions. In Nikhil Bansal, Kirk Pruhs, and Clif-
ford Stein, editors, Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,
SODA 2007, New Orleans, Louisiana, USA, January 7-9, 2007 , pages 736–745. SIAM, 2007.
Yannai A. Gonczarowski and Noam Nisan. Efficient empirical revenue maximization in single-parameter
auction environments. In Hamed Hatami, Pierre McKenzie, and Valerie King, editors, Proceedings of the
49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, Montreal, QC, Canada,
June 19-23, 2017 , pages 856–868. ACM, 2017.
Yannai A Gonczarowski and S Matthew Weinberg. The sample complexity of up-to- εmulti-dimensional
revenue maximization. Journal of the ACM (JACM) , 68(3):1–28, 2021.
Roger Guesnerie and Claude Oddou. Second best taxation as a game. Journal of Economic Theory , 25(1):
67–91, 1981. ISSN 0022-0531. doi: https://doi.org/10.1016/0022-0531(81)90017-X.
Chenghao Guo, Zhiyi Huang, and Xinzhi Zhang. Settling the sample complexity of single-parameter revenue
maximization. In Moses Charikar and Edith Cohen, editors, Proceedings of the 51st Annual ACM SIGACT
Symposium on Theory of Computing, STOC 2019, Phoenix, AZ, USA, June 23-26, 2019 , pages 662–673.
ACM, 2019.
Rishi Gupta and Tim Roughgarden. A PAC approach to application-specific algorithm selection. SIAM J.
Comput., 46(3):992–1017, 2017. doi: 10.1137/15M1050276.
Sergiu Hart and Noam Nisan. Selling multiple correlated goods: Revenue maximization and menu-size
complexity. Journal of Economic Theory , 183:991–1029, 2019.
Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Multi-armed bandits in metric spaces. In Proceedings
of the fortieth annual ACM symposium on Theory of computing , pages 681–690, 2008.
Robert D. Kleinberg and Frank Thomson Leighton. The value of knowing a demand curve: Bounds on regret
for online posted-price auctions. In 44th Symposium on Foundations of Computer Science (FOCS 2003),
11-14 October 2003, Cambridge, MA, USA, Proceedings , pages 594–605. IEEE Computer Society, 2003.
doi: 10.1109/SFCS.2003.1238232.
W Arthur Lewis. The two-part tariff. Economica , 8(31):249–270, 1941.
László Lovász and Santosh S. Vempala. Fast algorithms for logconcave functions: Sampling, rounding,
integration and optimization. In 47th Annual IEEE Symposium on Foundations of Computer Science
(FOCS 2006), 21-24 October 2006, Berkeley, California, USA, Proceedings , pages 57–68. IEEE Computer
Society, 2006. doi: 10.1109/FOCS.2006.28.
Mehryar Mohri and Andrés Munoz Medina. Learning algorithms for second-price auctions with reserve. The
Journal of Machine Learning Research , 17(1):2632–2656, 2016.
29Published in Transactions on Machine Learning Research (03/2024)
Jamie Morgenstern and Tim Roughgarden. Learning simple auctions. In Conference on Learning Theory ,
pages 1298–1318. PMLR, 2016.
Jamie H Morgenstern and Tim Roughgarden. On the pseudo-dimension of nearly optimal auctions. Advances
in Neural Information Processing Systems , 28, 2015.
Noam Nisan, Tim Roughgarden, Eva Tardos, and Vijay V. Vazirani. Algorithmic Game Theory . Cambridge
University Press, 2007.
Walter Y Oi. A disneyland dilemma: Two-part tariffs for a mickey mouse monopoly. The Quarterly Journal
of Economics , 85(1):77–96, 1971.
Tim Roughgarden and Okke Schrijvers. Ironing in the dark. In Vincent Conitzer, Dirk Bergemann, and
Yiling Chen, editors, Proceedings of the 2016 ACM Conference on Economics and Computation, EC ’16,
Maastricht, The Netherlands, July 24-28, 2016 , pages 1–18. ACM, 2016.
Tim Roughgarden and Joshua R. Wang. Minimizing regret with multiple reserves. In Vincent Conitzer,
Dirk Bergemann, and Yiling Chen, editors, Proceedings of the 2016 ACM Conference on Economics and
Computation, EC ’16, Maastricht, The Netherlands, July 24-28, 2016 , pages 601–616. ACM, 2016. doi:
10.1145/2940716.2940792.
Daniel A. Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex algorithm
usually takes polynomial time. J. ACM, 51(3):385–463, 2004. doi: 10.1145/990308.990310. URL https:
//doi.org/10.1145/990308.990310 .
Vasilis Syrgkanis. A sample complexity measure with applications to learning optimal auctions. Advances
in Neural Information Processing Systems , 30, 2017.
Leslie G Valiant. A theory of the learnable. Communications of the ACM , 27(11):1134–1142, 1984.
Vladimir Vapnik. Statistical Learning Theory . Wiley, 1998.
30Published in Transactions on Machine Learning Research (03/2024)
A Missing Proofs of Section 3
A.1 Online Learning
A.1.1 Online Learning Under Adversarial Inputs
Full Information
Proposition 35 ((Auer et al., 1995), Theorem 3.2) .For any sequence of valuations ¯v,
Rev WM(¯v)≥OPTX(¯v)−β
2OPTX(¯v)−Hlnn
β,
whereX=m1,...,mnare the set of experts (two-part tariff menus), Rev WM(¯v)is the expected revenue
outcome of Algorithm 2, and OPTX(¯v)is the revenue of the optimal menu in X.
Theorem7. In the full information case for length- ℓmenus of two-part tariffs, running Algorithm 2 over dis-
cretized set of menus specified in Theorem 1 for α=β= 1/√
Thas regret bounded by ˜O/parenleftig
ℓ(K+HlnH)√
T/parenrightig
,
and running time O(TℓK min{H2ℓTℓ,2H2T}).
Proof.Letnbe the number of menus resulting from the discretization procedure in Section 3.1. Let vibe
the valuation of the buyer at step i, and ¯vbe the vector of valuation of all buyers in rounds 1throughT.
We denote RevM′()as the maximum revenue obtained in the set of menus resulting from the discretization
procedure, OPT()as the optimal revenue, and Rev WM()as the revenue obtained from the weighted majority
algorithm discussed above on the set of outcome menus of the discretization procedure. Then,
n= (H/α)2ℓ,
Rev WM(¯v)≥Rev(M′) (¯v)−β
2Rev(M′) (¯v)−Hlnn
β,
RevM′(¯v) =T/summationdisplay
i=1RevM′(vi),
RevM′(vi)≥OPT (vi)−2Kℓα;
wherethefirstexpressionisaresultofthediscretizationprocedure,thesecondexpressionusesProposition35,
the third expands the revenue over Tterms, and the last uses Theorem 1. Rearranging the terms, we have:
RevM′(vi)≥OPT (vi)−2Kℓα
RevM′(¯v)≥OPT (¯v)−2KℓαT
Rev WM(¯v)≥OPT (¯v)−2KℓαT−βHT
2−Hlnn
β
Rev WM(¯v)≥OPT (¯v)−2KℓαT−βHT
2−2Hℓ(ln (H/α))
β
We set variables αandβto minimize the exponent of Tin the regret. By substituting n, the regret is upper
bounded by
2KℓαT +βHT
2+2Hℓ(lnH−lnα)
β.
By setting α=β=1√
T, The regret will be ˜O/parenleftig
ℓ(K+HlnH)√
T/parenrightig
. Based on the parameters chosen, the
number of menus is O(min{H2ℓTℓ,2H2T}). The algorithm needs to maintain the weights for these menus
and update them based on the revenue at each time step. The revenue of each menu can be calculated in
O(Kℓ)given the buyer’s valuation, resulting in the stated running time. The running time in each round is
the number of menus times the time to calculate the revenue for each menu.
31Published in Transactions on Machine Learning Research (03/2024)
Partial Information
Proposition 36 ((Auer et al., 1995), Theorem 4.1) .For any sequence of valuations ¯v,
Rev Exp3(¯v)≥OPTX−/parenleftbigg
γ+β
2/parenrightbigg
OPTX−Hnlnn
βγ,
whereX=m1,...,mnare the set of experts (two-part tariff menus), Rev Exp3(¯v)is the expected revenue
outcome of Algorithm 3, and OPTX(¯v)is the revenue of the optimal menu in X.
Theorem 8. In the partial information case for length- ℓmenus of two-part tariffs, running Algorithm 3
over discretized set of menus in Theorem 1 for α=T−1/(2(1+ℓ)),β=γ=T−1/(4(1+ℓ))has regret bound
˜O/parenleftig
T1−1
2(1+ℓ)ℓ(K+H2ℓ+1)/parenrightig
, and running time O(Tmin{min{H2ℓTℓ,2H2T},2H2T}).
Proof.The proof follows the same logic as that of Theorem 7. We denote Rev Exp3()as the revenue obtained
from the Exp3 algorithm described above on the set of outcome menus of the discretization procedure.
Similar to the proof of Theorem 7, in what follows ndenotes the number of menus resulting from the
discretization procedure in Section 3.1. viis the valuation of the buyer at step i, and ¯vis the sequence of
valuation of all buyers in rounds 1throughT.RevM′()is the maximum revenue obtained in the set of menus
resulting from the discretization procedure and OPT()is the optimal revenue.
n= (H/α)2ℓ,
Rev Exp3(¯v)≥Rev(M′) (¯v)−/parenleftbigg
γ+β
2/parenrightbigg
Rev(M′) (¯v)−Hnlnn
βγ,
RevM′(¯v) =T/summationdisplay
i=1RevM′(vi),
RevM′(vi)≥OPT (vi)−2Kℓα;
where the first expression is a result of Theorem 1, the second expression uses Proposition 36, the third
expands the revenue over T terms, and the last uses Theorem 1. Rearranging the terms gives:
RevM′(¯v)≥OPT (¯v)−2KℓαT
Rev Exp3(¯v)≥OPT (¯v)−2KℓαT−/parenleftbigg
γ+β
2/parenrightbigg
HT−Hnlnn
βγ
Rev Exp3(¯v)≥OPT (¯v)−2KℓαT−/parenleftbigg
γ+β
2/parenrightbigg
HT−2H(H/α)2ℓℓ(lnH−lnα)
βγ
We set variables αandβas a function of Tto minimize the exponent of Tin the regret. By setting
α=T−1/(2(1+ℓ)),β=γ=T−1/(4(1+ℓ)), the regret is O/parenleftig
T1−1
2(1+ℓ)ln (T)ℓ(K+H2ℓ+1lnH)/parenrightig
. The algorithm
involves maintaining weights for all the menus in the discretized set at each time step, therefore the running
time at each time step is proportional to the number of the menus that are derived based on parameter
α.
A.1.2 Online Learning Under Smooth Distributions
Full Information
For completeness we include previously established algorithms for the full information setting, under
dispersion condition, adapted to our setting.
Overview of Algorithms 4 and 6, related to Theorem 10. Algorithm 4 (Balcan et al., 2018b)
is an efficient algorithm for online learning in the full-information setting under smoothed distributional
assumptions that uses Algorithm 6 (Balcan et al., 2018b) as a subroutine. The algorithm considers the
cumulative revenue function up until the time t−1over the parameter space,/summationtextt−1
0us, and samples the
32Published in Transactions on Machine Learning Research (03/2024)
Algorithm 6: Multi-dimensional sampling algorithm ((Balcan et al., 2018b), Algorithm 2)
Input:Functiong, partition with regions P1,...,Pn, approximation parameter η, confidence parameter ζ.
1:Defineα=β=η/3.
2:Leth(ρ) = exp(g(ρ))andhi(ρ) =I{ρ∈Pi}h(ρ)behrestricted toPi.
3:For eachi∈[n], let ˆZi=Aintegrate (hi,α,ζ/ (2n)).
4:Choose random partition index I=iwith probability ˆZi//summationtext
jˆZj.
5:Letˆρbe the sample output by Asample (hI,β,ζ/ 2).
Output:ρ
menu to be presented at time tapproximately proportional to an exponential function of its cumulative
revenue, i.e., eg(ρt), whereg=λ/summationtextt−1
0us. In order to have an efficient implementation for sampling menu ρt
approximately from distribution µwith density fµ(ρ)∝eg(ρt), techniques from high-dimensional geometry
are used in Algorithm 6. This algorithm is used when gis piecewise concave (in our case, linear), and each
piece is a convex set (in our case, convex polytopes where each buyer already in the sequence selects a fixed
tariff index and the number of units) as shown in Lemma 13. Let P1,...,Pnbe the partition of Cuntil timet.
Thealgorithmfirstpicks Piwithprobabilityproportionaltotheintegralof fµonthatregionandthenoutputs
a sample from the conditional distribution of menus in Pi. The algorithm assumes access to two procedures
for approximate integration and sampling, namely Aintegrate (h,α,ζ )andAsample (h,β,ζ ).Aintegrate (hi,α,ζ)
is a polynomial running-time procedure that takes the approximate integral of any logconcave function hi
restricted to region Piwith accuracy parameter αand failure probability ζ.Asample (hi,β,ζ)is a polynomial
procedure that approximately samples a menu with probability distribution according to hiin the regionPi
with accuracy parameter βand failure probability ζ.
Definition 37 (Aintegrate (h,α,ζ )andAsample (h,β,ζ )(Balcan et al., 2018b)) .For any logconcave function
h:Rd→R, any accuracy parameter α > 0, and any failure probability ζ >0,Aintegrate (h,α,ζ )outputs a
numberZthat with probability at least 1−ζsatisfiese−α/integraltext
h≤Z≤eα/integraltext
h. For any logconcave function h:
Rd→R, any accuracy parameter β >0, and any failure probability ζ >0,Asample (h,β,ζ )outputs a sample
Xdrawn from a distribution ˆuhthat with probability at least 1−ζ,D∞(µ,ˆµ)≤β, whereD∞(µ,ˆµ)is the
relative (multiplicative) distance between probability measures µandˆµ. Formally, D∞(µ,ˆµ) = supρ|logdµ
dˆµ|,
wheredµ
dˆµdenotes the Radon-Nikodym derivative.
Similar to Balcan et al. (2018b), we use the implementation of Aintegrateby Lovász and Vempala (2006) and
Asampleby Bassily et al. (2014), Algorithm 6. These implementations satisfy the conditions in Definition 37.
The first runs in time poly (d,1
α,log1
ζ,logR
r), where the domain of function his a subset of a ball of radius
Rand its level set of probability mass 1/8is a superset of a ball with radius r. The second succeeds with
probability 1and runs in time poly (d,L,1
β,logR
r).
Theorem 10. Letu1,...,uT:C → [0,H]be the revenue functions of two-part tariff menus such that
ut(ρ)denotes the revenue of a mechanism associated with menu parameters ρfor the buyer arriving at
timet. Let the samples of buyers’ values be drawn from S ∼D(1)×···×D(T). Suppose v(k)∈[0,H]
for any number of units k∈[K]. Also, suppose that for each distribution D(t), and every pair of number
of unitskandk′,v(k)andv(k′)have aκ-bounded joint distribution. An efficient implementation of the
exponentially weighted forecaster with λ=/radicalig
2ℓln(2H2κ√
T)/T/H(Algorithm 4) has expected regret bounded
by˜O((Hℓ2K2√logκ+ 1/(Hκ))√
T)and runs in time ˜O((T+ 1)poly(ℓ,K)poly(ℓ,√
T) +KT√
T).
Proof.Proposition 16 determines the dispersion for two-part tariff menus with probability 1−ζ. Theorem 1
in Balcan et al. (2018b) relates dispersion to a regret bound for full information online learning algorithms.
It states if a sequence of piecewise L-Lipschitz functions in ddimensions is (w,k)-dispersed, there is an
exponentially weighted forecaster with expected regret O(H(/radicalbig
TdlogR/w +k) +TLw ). Since dispersion
holds with probability 1−ζ, the final regret bound is O((1−ζ)(H(/radicalbig
TdlogR/w +k) +TLw )) +ζH.
33Published in Transactions on Machine Learning Research (03/2024)
Substituting wandkby dispersion found in Proposition 16 gives:
O/parenleftigg
H/parenleftigg
/radicalbig
2Tℓlog(2H2κT1−α) +ℓ2K2Tα/radicaligg
lnℓK
ζ/parenrightigg
+Tα
2Hκ+ζHT/parenrightigg
.
For all rounds, t∈[T], the sum of utilities is linear over at most (T+ 1)ℓ2K2pieces, and all the pieces
are convex. In this case, we may use Algorithm 6 as a subroutine to Algorithm 4 for a more efficient but
approximate implementation. Setting dispersion parameters ζ= 1/√
Tandα= 0.5and approximation
parameters η=ζ= 1/√
Tand using Theorem 1 in Balcan et al. (2018b), gives the statement’s regret bound
and running time.
Bandit Setting
The bandit-setting algorithm considers a grid over the parameter space, whose granularity depends on the
dispersion parameters, and runs the Exp3 algorithm over menus corresponding to the grid.
Theorem 11. Letu1,...,uT:C→ [0,H]be the revenue functions of two-part tariff menus such that ut(ρ)
denotes the revenue of a mechanism associated with menu parameters ρfor the buyer arriving at time t. Let
the samples of buyers’ values be drawn from S∼D(1)×···×D(T). Supposev(k)∈[0,H]for any number
of unitsk∈[K]. Also, suppose that for each distribution D(t), and every pair of number of units kandk′,
v(k)andv(k′)have aκ-bounded joint distribution. There is a bandit-feedback online optimization algorithm
with expected regret ˜O/parenleftig
T(2ℓ+1)/(2ℓ+2)/parenleftig
H2K√
ℓκd/2√logκ/parenrightig
+1/Hκ+Hℓ2K2/parenrightig
. The per-round running time
isO(H4ℓκ2ℓTℓ).
Proof.Proposition 39 determines dispersion for two-part tariff menus with probability 1−ζ. Theorem 3
in Balcan et al. (2018b) relates dispersion to a regret bound for the bandit setting. It states if a sequence
of piecewise L-Lipschitz functions that are (w,k)-dispersed and when the parameter space is contained in a
ball of radius R, running Exp3 algorithm has regret
O
H/radicaligg
Td/parenleftbigg3R
w/parenrightbiggd
logR
w+TLw +Hk
.
The per-round running time is O((3R/w)d). Note that dispersion holds only with probability 1−ζand with
probability ζ, regret is bounded by HT. In our case, L=K+ 1,R=Handd= 2ℓ. Substituting these
terms along with wandk, and setting α=2ℓ+1/2ℓ+2andζ= 1/√
Tgives the regret bound and running time
in the theorem statement.
Semi-BanditSetting Forthesemi-banditsetting, weneedtoinvokeamorerecentdefinitionofdispersion.
Definition 38 ((Balcan et al., 2020a), β-point-dispersion) .The sequence of loss functions l1,l2,...isβ-
point-dispersed for the Lipschitz constant Lif for allTand for all ε≥T−β, we have that, in expectation,
the maximum number of functions among l1,...,lTthat fail the L-Lipschitz condition for any pair of points
at distance εinCis at most ˜O(εT). That is, for all Tand for all ε≥T−β, we have E/bracketleftbig
maxρ,ρ′/vextendsingle/vextendsingle{t∈[T] :
|lt(ρ)−lt(ρ′)|>L∥ρ−ρ′∥2}/vextendsingle/vextendsingle/bracketrightbig
=˜O(εT).where the max is taken over all ρ,ρ′∈C:∥ρ−ρ′∥2≤ε.
Proposition 39. Supposelt(ρ) =H−ut(ρ), whereut(ρ)is the revenue of the two-part tariff menu mecha-
nism with prices ρand buyer’s values vtat timet, where buyers’ values are drawn from D(1)×···×D(T). If
D(i)areκ-bounded, where κ= ˜o(T), andKandℓ, the maximum number of units and the number of tariffs,
are polynomial in T, these loss functions are β-point-dispersed for β= 1/2.
Proof.We use the following statement from Balcan and Sharma (2021), theorem 7.
Proposition 40. (Balcan and Sharma, 2021) Let l1,...,lT:Rd→Rbe independent piecewise L-
Lipschitz functions, each having discontinuities specified by a collection of at most K′algebraic hyper-
surfaces of bounded degree. Let Pdenote the set of axis-aligned paths between pairs of points in Rd,
and for each s∈PdefineD(T,s) =|{1≤t≤T|lthas a discontinuity along s}|. Then we have
E[sups∈PD(T,s)]≤sups∈PE[D(T,s)] +O(/radicalbig
Tlog(TK′)).
34Published in Transactions on Machine Learning Research (03/2024)
The number of hyperplanes, defined as K′in the theorem, is at most Tℓ2K2andlts are piecewise (K+ 1)-
Lipschitz function (by Lemma 52); where Tis the number of buyers (rounds), ℓis the number of tariffs, and
Kis the maximum number of units. Note that, as shown in Lemma 13. The independence of lts comes from
the assumptions of this setting, where the buyer valuations for each round are drawn independently.
Definition 38 counts the number of times (in Ttime intervals) that the difference in utility of the pair violates
theL-Lipschitz condition, and finds the worst pair for this property. Proposition 40, counts the number of
times that in an axis-aligned path, the utility function has discontinuities. Therefore, sups∈PE[D(T,s)] +
O(/radicalbig
Tlog(TK′))is an upper bound on E/bracketleftbig
maxρ,ρ′/vextendsingle/vextendsingle{t∈[T] :|ut(ρ)−ut(ρ′)|> L∥ρ−ρ′∥2}/vextendsingle/vextendsingle/bracketrightbig
. To find the
dispersion we need to find sups∈PE[D(T,s)].
Recall from the proof of Proposition 16 that the discontinuities can be partitioned into ℓ2K2multisets of
parallel hyperplanes, such that multiset Bj,k,j′,k′corresponds to pairs of tariffs and the number of units
(j,k)and(j′,k′). In addition, since we assume the buyers’ valuations are in the range [0,H]and are drawn
from pairwise κ-bounded joint distributions, the offsets of the hyperplanes are independent draws from a
Hκ-bounded distribution. The number of multi-sets is ℓ2K2, and the size of each multi-set is T. The
hyperplanes within each multi-set are well-dispersed. For a multi-set Bj,k,j′,k′, let Θj,k,j′,k′be the multi-
set of the hyperplanes’ offsets. By assumption, the elements of Θj,k,j′,k′are independently drawn from
Hκ-bounded distributions. Since the offsets are Hκ-bounded, the probability that it falls in any interval
of lengthεisO(Hκε). The expected number of hyperplanes crossed from each multiset in distance ε
along each axis is at most Hκε|Bj,k,j′,k′|, and since there are 2ℓdimensions, the total expected number
of crossings is 2ℓHκε|Bj,k,j′,k′|. Using the upper bound on |Bj,k,j′,k′|, in total, for any pair of points at
distanceε,sups∈PE[D(T,s)] =O(ℓ3K2HκεT ). By Proposition 40, E[sups∈PD(T,s)]≤sups∈PE[D(T,s)]+
O(/radicalbig
Tlog(TKℓ)), which in our case is upper bounded by: O(ℓ3K2HκεT +/radicalbig
Tlog(TKℓ)). Forκ= ˜o(T),
K=O(poly(T))andℓ=O(poly(T)),E[sups∈PD(T,s)] = ˜O(εT). Therefore, these loss functions are
β-point dispersed for β= 1/2, satisfying the statement.
Overview of Algorithm 7 The generic algorithm for the semi-bandit case was previously developed
in Balcan et al. (2020a). We adapt it to our setting and consider an efficient implementation using the
approximate integration and sampling from Balcan et al. (2018b) discussed in Definition 37. The semi-
bandit-setting algorithm is a continuous version of the Exp3-SET algorithm of Alon et al. (2017b). At each
time step, the algorithm learns the revenue function (only) inside the region P(t)∋ρtthat the presented
menu belongs to and updates the menu weights for the next round accordingly.
Algorithm 7: Semi-bandit two-part tariff under smoothed distributional assumptions (Adapted from
(Balcan et al., 2020a), Algorithm 1 for two-part tariffs)
Input:Step sizeλ∈[0,1]
1:Letw1(ρ) = 1for allρ∈C
2:forbuyert= 1,...,Tdo
Letpt(ρ) =wt(ρ)
Wt, whereWt=/integraltext
Cwt(ρ)dρ;
Sampleρtfrompt, present it to buyer t, observe the tariff index jand the number of units k
selected by the buyer and region P(t)for which the buyer takes this action; the revenue inside P(t)
isut(ρ) =I{k≥1}(p(i)
1(ρ) +kp(i)
2(ρ))and the normalized loss is lt(ρ) =H−ut(ρ)
Hfor allρ∈P(t);
Letˆlt(ρ) =I{ρ∈P(t)}
pt(P(t))lt(ρ), where we define pt(P(t)) =/integraltext
P(t)pt(ρ)dρ;
Letwt+1(ρ) =wt(ρ) exp(−λˆlt(ρ))for allρ.
Theorem 12. Suppose the buyers’ values are drawn from D(1)×···×D(T), where eachD(t)isκ-bounded for
κ= ˜o(T). Then, running the continuous Exp3-SET algorithm (Algorithm 7) for menus of two-part tariffs
under semi-bandit feedback has expected regret bounded by ˜O(H√
ℓT). An efficient implementation has the
same regret bound and running time ˜O((T+ 1)poly(ℓ,K)poly(ℓ,√
T) +KT√
T).
Proof.For the regret bound, we invoke Theorem 2 of Balcan et al. (2020a), stating that if the loss functions
are Lipschitz functions satisfying β-point-dispersion, running Algorithm 7 has expected regret bounded by
35Published in Transactions on Machine Learning Research (03/2024)
˜O(√
dT+T1−β), when the loss function is in [0,1]. In our case, d, the number of dimensions is 2ℓ, the
dispersion parameter β= 1/2, and the loss function is in [0,H]. This implies the regret bound.
Now, we discuss the running time of the algorithm. At each time t, using the buyer’s valuation vector, the
tariffj, and the number of units kselected by the buyer, we can determine the region P(t), where the buyer
makes the same selection and whose utility function is linear by solving a linear program (the inequalities
in Equation (2)). This computation is done in time poly (ℓ,K). Next, for the integration procedures inside
the algorithm, we use the approximate version introduced in Definition 37, and for sampling, we use the
efficient implementation demonstrated in Algorithm 6. In particular, we consider η=ζ= 1/(3√
T). For/integraltext
Cwt(ρ)dρ, we use lines 1 through 3 of Algorithm 6 and take the sum of the integration outcomes of line
3, forη′=η/4andζ′=ζ/T. Forpt(P(t)) =/integraltext
P(t)pt(ρ)dρwe do the same, except that now we do the
integration operations in line 3 only for the regions inside P(t). For sampling ρtfrompt, we use the complete
procedure Algorithm 6 that takes the regions with linear cumulative utility, λ=/radicalig
2ℓln(2H2κ√
T)/T/H,
g=λ/summationtextt−1
s=0usandη=ζ= 1/(3√
T). Note that since the loss is only updated for P(t), for any regions
outside this part, we do not need to repeat the integration operations in Algorithm 6. This may result in
potentially better running time for semi-bandit compared to full-information; however, we do not quantify
the improvement. Using union bound, with probability at least 1−1/√
T, all the approximate integration
and sampling operations performed in the algorithm succeed and the density function of the approximate
distribution used for sampling is always within (1−η)fraction of the exact distribution. Using these
parameters together with Theorem 1 in (Balcan et al., 2018b) conclude that the same regret bound is
achievable from the approximate operations and give the running time in the statement.
A.1.3 Limited Buyer Types
Full Information Setting
Theorem 24. In the full information case for length- ℓmenus of two-part tariffs, when there are Vtypes of
buyers, running Algorithm 2 over the set of menus corresponding to set Eforβ= 1/√
Thas regret bounded
by˜O(Hℓ√
Tln(VℓK )).
Proof.We run the weighted majority algorithm Algorithm 2 with parameter β= 1/√
Ton the setEas the
set of menus (experts). The proof directly follows from Lemma 23 and Proposition 35. Let n=|E|. Letbi
be the valuation of the buyer at step i, and ¯bbe the vector of valuation of all buyers in rounds 1through
T. We denote RevE()as the maximum revenue obtained in the set of E,OPT()as the optimal revenue, and
Rev WM()as the revenue obtained from Algorithm 2 on the set of experts X=E. Then,
n≤(Vℓ2K2/4)2ℓ,
Rev WM/parenleftbig¯b/parenrightbig
≥Rev(E)/parenleftbig¯b/parenrightbig
−β
2Rev(E)/parenleftbig¯b/parenrightbig
−Hlnn
β,
RevE/parenleftbig¯b/parenrightbig
=T/summationdisplay
i=1RevE(bi),
RevE(bi)≥OPT (bi)−2Kε;
where the first expression uses the size of Ein Lemma 22, the second expression uses Proposition 35, the
third expands the revenue over T terms, and the last uses Lemma 23. Rearranging the terms, we have:
RevE(bi)≥OPT (bi)−2Kε
RevE/parenleftbig¯b/parenrightbig
≥OPT/parenleftbig¯b/parenrightbig
−2KεT
Rev WM/parenleftbig¯b/parenrightbig
≥OPT/parenleftbig¯b/parenrightbig
−2KεT−βHT
2−Hlnn
β
Rev WM/parenleftbig¯b/parenrightbig
≥OPT/parenleftbig¯b/parenrightbig
−2KεT−βHT
2−2ℓH(ln (VℓK ))
β
36Published in Transactions on Machine Learning Research (03/2024)
We set variables εandβto minimize the exponent of Tin the regret. By setting β=1√
Tandε= 1/(K√
T),
The regret will be O(Hℓ√
Tln (VℓK )).
Partial Information Setting We first show how to estimate the utility of any menu by only using the
response of the buyer to a limited number of menus. In doing so, we take advantage of the interdependence
of the buyers’ responses for different menus to obtain estimates for unused menus. In particular, using
barycentric spanner concept from Awerbuch and Kleinberg (2008), we devise a basis for the menus such that
observing buyers’ responses to them is sufficient for estimating the revenue of other menus.
LetIbe a set of length- Vindicator vectors, such that for each feasible mapping µand option to select
(j,k), which is the tariff index and the number of units, there is a vector in I. This vector indicates the
(maximal) set of buyer types that select this option in mapping µ. As an example, if in mapping µ,{v2,v3}
is the exact set of valuation types that select the same option (j,k), vector (0,1,1,0,...)belongs to I. For
I∈I,µIand(j,k)Idenote the corresponding mapping and option to I, respectively. Similarly, Iµ,(j,k)is
the vector in I, corresponding to mapping µand option (j,k). Using principles from linear algebra, since
the vectors are V-dimensional, there is a set of at most Vvectors in Isuch that any other vector in Iis a
linear combination of the vectors in this set. Awerbuch and Kleinberg make this property stronger and show
that there is a set of Vvectors in I, called the barycentric spanner orspannerfor short, we denote it by S,
such that any member of Ican be written as a linear combination of vectors in Swith coefficients in [−1,1].
Lemma 41. There exists setSinIsuch that, for all I∈I, there exists coefficients λ1,...,λV∈[−1,1],
so thatI=/summationtextV
j=1λisj.
Proof.The statement is a direct corollary of Awerbuch and Kleinberg (2008) Proposition 2.2.
Here is the main idea on how to find estimates for the utility of all the menus by only presenting the menus
corresponding to the spanner Sto the buyers. First, similar to Balcan et al. (2015), we define function fτ(·)
for the vectors in Ithat will be instrumental in computing the utility for all the menus based on the spanner.
Recall that each vector IinIcorresponds to a mapping µIand an option (j,k)I. Letfτ(I)be the number
of times during a time block τthat given a menu in Pµthe arriving buyer selects option (j,k). First, we
show how the quantity of this function on inputs from the spanner is sufficient for finding the revenue of
arbitrary menus and then show how to estimate it.
Lemma 42. For each menu ρand any time block τ:t+ 1,...,t +τℓ, letuτ(ρ)represent the average utility
ofρfor buyer types in τ. Then,
uτ(ρ) =1
ℓτ/summationdisplay
(j,k)∈OI{k≥1}/parenleftig
p(j)
1(ρ) +kp(j)
2(ρ)/parenrightigV/summationdisplay
i=1λi(Iµρ,(j,k))fτ(si)
Proof.By definition, uτ(ρ)is the average utility of menu ρfor buyers arriving in τ. Menuρ, corresponds
to a feasible mapping µρ. By definition, the buyers in time block τselect option (j,k)equal tofτ(Iµρ,(j,k))
number of times. By Lemma 41, Iµρ,(j,k)can be written as a linear combination of the vectors in the
spanner. Furthermore, fτ(.)is a linear function as it is equivalent to the dot product of a vector indicating
the frequency, i.e., the number of arrivals, of each buyer type during τand the function input. Therefore,
uτ(ρ) =1
ℓτ/summationdisplay
(j,k)∈OI{k≥1}/parenleftig
p(j)
1(ρ) +kp(j)
2(ρ)/parenrightig
fτ(Iµρ,(j,k))
=1
ℓτ/summationdisplay
(j,k)∈OI{k≥1}/parenleftig
p(j)
1(ρ) +kp(j)
2(ρ)/parenrightigV/summationdisplay
i=1λi(Iµρ,(j,k))fτ(si).
Letˆfτ(si)be the estimator to fτ(si)/ℓτfor the spanner vectors. Let µsibe the corresponding mapping to
si. Recall that fτ(si)is the number of times during τthat given a menu in Pµsi, the arriving buyer, selects
37Published in Transactions on Machine Learning Research (03/2024)
option (j,k)si. In order to estimate this quantity we present a corresponding menu to si, i.e., a menu in
Pµsi, once uniformly at random during the time block τ. If the buyer selects option (j,k)si, we let ˆfτ(si)
equal to 1and otherwise set it to 0. The next lemma shows that ˆfτ(si)has the same expected value and
has range [0,1]. Intuitively, the reason is that due to the uniform random selection of the time step, the
estimator has the same expected value.
Lemma 43 (Adapted from Balcan et al. (2015) Lemma 6.3) .For anys∈S,E[ˆfτ(s)]ℓτ=fτ(s).
Proof.Note that ˆfτ(s) = 1if and only if at the time step that menu ρswas presented, (j,k)swas selected.
Sinceρsis presented once uniformly at random over the time steps and is independent of the sequence of
buyers, the buyer presented with ρsis also picked uniformly at random over the time steps. Therefore,
E[ˆfτ(s)]is the probability that a randomly chosen buyer from time block τselects (j,k)s.
Now, we prove that the expected value of the utility estimator for each menu is equal to the utility of that
menu, i.e., the estimator is unbiased and, moreover, has a bounded range. The utility estimator is defined
as follows, where fτ(si)/ℓτin the utility formula is replaced by its estimator ˆfτ(si).
ˆuτ(ρ) =/summationdisplay
(j,k)∈OI{k≥1}/parenleftig
p(j)
1(ρ) +kp(j)
2(ρ)/parenrightigV/summationdisplay
i=1λi(Iµρ,(j,k))ˆfτ(si)
Lemma 44. For any menu ρ,E[ˆuτ(ρ)] =uτ(ρ)andˆuτ(ρ)∈[−ℓKVH,ℓKVH ].
Proof.The proof of the equality of the expectation simply follows from ˆuτ(ρ)anduτ(ρ)definitions and
Lemma 43. Now, we prove the range of the estimator. Since Sis a barycentric spanner, for any I∈I,
λi(I)∈[−1,1]. Also, ˆfτ(.)belongs to{0,1}. Also, the utility of the buyer selecting each option in the menu,
e.g.,p(j)
1(ρ) +kp(j)
2(ρ), is always in [0,H]. Therefore, using the formula of the estimator, it is bounded by
Htimes the number of options times the number of buyer types.
We use the algorithm below along with the weighted majority algorithm in the full-information (similar to
Algorithm 2) that uses the utility (revenue) estimates. We use Eas the set of experts (menus) and obtain
distribution qover setEas the weight vector.
Overview of Algorithm 8 First, we provide a high-level structure of the algorithm and then discuss the
details. The algorithm operates in time blocks, with each block consisting of exploitation and exploration
time steps. The exploration time steps are selected uniformly at random within the block and are limited in
number. Inanexploitationstep, themenuusedistheoutputofthefullinformationalgorithm, employingthe
utility estimators from the previous time block. These menus are always the extreme points of the continuity
regions, as discussed at the beginning of the section. During exploration time steps, the corresponding menu
toavectorinthespannerisused. Attheendofeachtimeblock, thealgorithmrefinestheunbiasedestimators
of the utility of all extreme points using the information gathered in the exploration phases.
Zis the number of time blocks, with each time block consisting of T/Ztime steps. The algorithm uniformly
at random picks time steps t1,...,tVand their permutation πin the current time block. Whenever the time
step is equal to ti, the algorithm runs an exploration step; otherwise, the algorithm runs an exploitation
step. In the exploration step at time step ti, a menu corresponding to si,ρsπ(i), is presented to the arriving
buyer and the estimator ˆfτ(sπ(i))will be assigned as 1if the buyer selects (j,k)sπ(i)and will be assigned as 0,
otherwise. At the end of the time block, we update the estimates of the revenue of the menus corresponding
to the extreme points.
Lemma 45. [(Balcan et al., 2015) Lemma 6.2] Let Mbe the set of all actions. For any time block (set
of consecutive time steps) T′and action j∈M, letcT′(j)be the average loss of action joverT′. Assume
thatS⊆Mis such that by sampling all actions in S, we can compute ˆcT′(j)for allj∈Mwith the
following properties: E[ˆcT′(j)] =cT′(j)andˆcT′(j)∈[−κ,κ]. Then there is an algorithm with a loss Lalg≤
Lmin+O/parenleftig
T2
3|S|1
3κ1
3log1
3(|M|)/parenrightig
, whereLminis the loss of the best action in hindsight.
38Published in Transactions on Machine Learning Research (03/2024)
Algorithm 8: Partial-Information Algorithm for Limited Buyer Types
(adapted from (Balcan et al., 2015) Algorithm 1)
Input:V:the number of buyer types, O:the set of menu options ( |O|=ℓ(K+ 1))
1:Z←(T2|O|2Vlog(|O|V))1/3▷the number of time blocks
2:Create set I={Iµ,(j,k)|for all options (j,k)and feasible mappings µ}such that the ith component of
Iµ,(j,k)is1iffviselects (j,k)inµand is 0otherwise.
3:Find a barycentric spanner S={s1,...,sV}forI. For everys∈S, letµsbe the corresponding
mapping, (j,k)s, the corresponding option, and ρsa menu inPµs.
4:forallI∈Ido
letλ(I)be the representation of Iin spannerS. That is/summationtextV
i=1λi(I)si=I.
5:Letq1be the uniform distribution over E. ▷initial weight vector over menus in E
6:forτ= 1,...,Zdo ▷time blocks
Choose a random permutation πover [V]andt1,...,tVfrom [T/Z].;
fort= (τ−1)(T/Z) + 1,...,τ (T/Z),do ▷time steps in a time block
ift=tifor somei∈[V],then ▷exploration time step
ρt←ρsπ(j);
If(j,k)sπ(j)is selected, then ˆfτ(sπ(j))←1, otherwise ˆfτ(sπ(j))←0;
else ▷exploitation time step
drawρtat random from distribution qτ;
forallρ∈E, forµsuch thatρ∈Pµ,do
ˆuτ(ρ) =/summationtext
(j,k)∈OI{k≥1}/parenleftig
p(j)
1(ρ) +kp(j)
2(ρ)/parenrightig/summationtextV
i=1λi(Iµρ,(j,k))ˆfτ(si).;
Call Algorithm 2 for experts Eand ( ˆuτ) as their revenue function;
And receive qτ+1as a distribution over all mixed strategies in E.
We are now ready to prove the main result of this section.
Theorem 25. In the partial information (bandit) case for length- ℓmenus of two-part tariffs, when there are
Vdifferent types of buyers, there is an algorithm with regret bound of ˜O(T2/3ℓ(HKV )1/3log1/3(VℓK )).
Proof.In Lemma 45,|S|is the number of dimensions (barycentric spanner set), κis the maximum revenue
times the number of buyer types times the number of their options (entries in the menu), |M|is the number
of extreme points. In our case, |S|= 2ℓ,κ=HℓKV, and|M|≤(Vℓ2K2/4)2ℓ. By Lemma 44, the expected
value of the estimated utility is equal to the exact value of utility with range [−HℓKV,HℓKV ].
Using Lemma 45, the regret for menus of two-part tariffs is bounded by
O(T2/3ℓ1/3(HℓKV )1/3ℓ1/3log1/3(VℓK ))∈O(T2/3ℓ(HKV )1/3log1/3(VℓK )).
The following quantifies the regret of simply running the Exp3 algorithm on the set of extreme points.
Proposition 46. In the partial information case for length- ℓmenus of two-part tariffs when there are V
buyer types, running Algorithm 3 over menus corresponding to Eforβ=γ=T−1/3has regret bound
O/parenleftbig
T2/3ℓH(Vℓ2K2/4)2ℓln (VℓK )/parenrightbig
.
Proof.The proof is similar to that of Theorem 11. We denote Rev Exp3()as the revenue obtained from the
Exp3 algorithm as presented in Algorithm 3 on the set of menus corresponding to E. Letndenote the
number of such menus. biis the valuation of the buyer at step i, and ¯bis the sequence of valuation of all
buyers in rounds 1throughT.RevE()is the maximum revenue obtained in the set EandOPT()is the
39Published in Transactions on Machine Learning Research (03/2024)
optimal revenue.
n≤(Vℓ2K2/4)2ℓ,
Rev Exp3/parenleftbig¯b/parenrightbig
≥Rev(E)/parenleftbig¯b/parenrightbig
−/parenleftbigg
γ+β
2/parenrightbigg
Rev(E)/parenleftbig¯b/parenrightbig
−Hnlnn
βγ,
RevE/parenleftbig¯b/parenrightbig
=T/summationdisplay
i=1RevE(bi),
RevE(bi)≥OPT (bi)−2Kε;
where the first expression uses the size of Ein Lemma 22, the second expression uses Proposition 36, the
third expands the revenue over T terms, and the last uses Lemma 23. Rearranging the terms, we have:
RevE(bi)≥OPT (bi)−2Kε
RevE/parenleftbig¯b/parenrightbig
≥OPT/parenleftbig¯b/parenrightbig
−2KεT
Rev Exp3/parenleftbig¯b/parenrightbig
≥OPT/parenleftbig¯b/parenrightbig
−2KεT−/parenleftbigg
γ+β
2/parenrightbigg
HT−Hnlnn
βγ
Rev Exp3/parenleftbig¯b/parenrightbig
≥OPT/parenleftbig¯b/parenrightbig
−2KεT−/parenleftbigg
γ+β
2/parenrightbigg
HT−2ℓH(Vℓ2K2/4)2ℓ(ln (VℓK ))
βγ
We set variables εinEandβ=γas a function of Tto minimize the exponent of Tin the regret. By setting
β=γ=T−1/3andε=T−1/2, the regret is O/parenleftbig
T2/3ℓH(Vℓ2K2/4)2ℓln (VℓK )/parenrightbig
.
Remark. The standard technique for the partial information algorithm of running the Exp3 algorithm
on the extreme points leads to a regret bound that is exponential in the size of the menu as stated in
Proposition 46; however, Algorithm 8 has regret bound polynomial in the size of the menus. Therefore, the
new technique results in a significant improvement.
A.2 Distributional Learning
Theorem 26. In the distributional setting, for length- ℓmenus of two-part tariffs, there ex-
ists a learning algorithm with sample complexityH2
2ε2(2ℓln (2KHℓ
ε) + ln (2/δ)),and running time
H2
2ε2/parenleftbig
2ℓln/parenleftbig2KHℓ
ε/parenrightbig
+ ln (2/δ)/parenrightbig
Kℓ/parenleftbig2HKℓ
ε/parenrightbig2ℓ.
Proof.We need to find the number of samples such that with probability 1−δ, the difference between the
expected revenue of our algorithm and the optimal revenue is at most ε. Note that since our algorithm uses
discretization of possible menus, we face two types of errors: the discretization error, and the usual empirical
error in a PAC learning setting. We find the sample complexity and discretization parameters such that the
total error is bounded by ε.
The possible number of menus after discretization using parameter Nis computed by the following formula.
|H|= (H/α)2ℓ.
Using uniform convergence in the PAC learning setting, the sample complexity for empirical error ε′is as
follows.
|S|≥H2
2ε′2(ln|H|+ ln (2/δ)).
Replacing lnHwe have,
|S|≥H2
2ε′2(2ℓln (H/α) + ln (2/δ)).
Also, the revenue loss compared to the optimum for arbitrary buyer iwith valuation viis:
RevM′(vi)≥OPT (vi)−2Kℓα.
40Published in Transactions on Machine Learning Research (03/2024)
The total error (from discretization and empirical error), when the empirical error is set to ε′, is
2Kℓα +ε′.
By setting 2Kℓα =ε′, we have
α=ε′
2Kℓ,
Replacingαgives the following sample complexity:
|S|≥H2
2ε′2(2ℓln (H/α) + ln (2/δ))
≥H2
2ε′2(2ℓln (2KℓH/ε′) + ln (2/δ))
which by replacing ε′withε/2results inεtotal error.
The computational complexity of finding the empirical optimal menu for |S|buyers and menu of size ℓis:
O(|S|Kℓ|H|) =|S|Kℓ/parenleftbigg2HKℓ
ε/parenrightbigg2ℓ
.
This implies the efficiency of the algorithm.
Lemma 47. The running time of distributional learning algorithm for two-part tariffs in (Balcan et al.,
2020b) is at least/parenleftigg
c/parenleftbiggH
ε/parenrightbigg2/parenleftbigg
18ℓlog (82K2ℓ3) + log1
δ/parenrightbigg/parenrightigg2ℓ+1
K4ℓ+2(2ℓ)2+1/18.
Proof.The algorithm involves computing N2ℓK4ℓregions, where Nisc(H/ε)2(18ℓlog (8K2ℓ3) + log1
δ),
and solving a linear program for each region with 2ℓvariables and NK2constraints, which takes
˜O((2ℓ)2+1/18NK2).
Comparisonwithpreviousresults. Thesamplecomplexityusingthepseudo-dimensionmethodof(Bal-
can et al., 2018c) is O(H2/ε2(ℓlog (Kℓ) + log (1/δ)))and the best previously-known running time (Bal-
can et al., 2022b) is O/parenleftbig
R2(2ℓ)2ℓ+1KH2/ε2(ℓlog (Kℓ) + log (1/δ))/parenrightbig
, whereRthe number of disconti-
nuity regions is bounded by O([H2/ε2(ℓlog (Kℓ) + log (1/δ))]3K), resulting in the worst case running
time ofO/parenleftig/parenleftbig
H2/ε2(ℓlog (Kℓ) + log (1/δ))/parenrightbig2ℓ+1K4ℓ+2(2ℓ)2+1/18/parenrightig
due to (Balcan et al., 2020b; 2022b) (See
Lemma 47).
B Missing Proofs of Section 4
B.1 Online Learning
Similar to the section on two-part tariffs, using the outcome of the discretization summarized in Theorem 27,
we show a reduction to a finite number of experts and run standard learning algorithms (weighted majority
and Exp3) over the menus in the discretized set.
B.1.1 Full Information
In the full information setting, the seller sees the revenue generated for all the possible menus. To design
an online algorithm in this case, we use a variant of the weighted majority algorithm by (Auer et al., 1995).
The experts in our case are the discretized menus from the previous section, denoted in the algorithm by
setX=m1,...,mn. Furthermore, vtis the valuation of the buyer are time tandRevk(v1,...,vt)is the
cumulative revenue of menu mkfor the buyers until time step t.
41Published in Transactions on Machine Learning Research (03/2024)
Similar to two-part tariffs, we use Algorithm 2 for the full information case. The only difference is that since
the maximum revenue in lotteries is mH, as opposed to two-part tariffs where it is H, in the algorithm we
need to replace HwithmH.
Proposition 48 ((Auer et al., 1995), Theorem 3.2) .For any sequence of valuations ¯v,
Rev WM(¯v)≥/parenleftbigg
1−β
2/parenrightbigg
OPTX(¯v)−mHlnn
β,
whereX=m1,...,mnare the set of experts (lottery menus), Rev WM(¯v)is the expected revenue outcome of
Algorithm 2 where His replaced with mH, and OPTX(¯v)is the revenue of the optimal menu in X.
Theorem 28. In the full information case for length- ℓmenus of lotteries, running Algorithm 2 over the
discretized set of menus specified in Theorem 27 for α=T−1,β=T−0.5,K=T0.5, andδ=T−0.5has
regret ˜O(m2Hℓ√
T).
Proof.Letnbe the number of menus resulting from Algorithm 5. Let vibe the valuation of the buyer at
stepi, and ¯vbe the vector of valuation of all buyers in rounds 1throughT. We denote RevM′()as the
maximum revenue obtained in the set of menus resulting from Algorithm 5, OPT()as the optimal revenue,
andRev WM()as the revenue obtained from the weighted majority algorithm discussed above on the set of
outcome menus of Algorithm 5. We have
n= (1/αℓm+ℓ) (ln (Hm/α ))lm,
Rev WM(¯v)≥RevM′(¯v)−β
2Rev(M′) (¯v)−mHlnn
β,
RevM′(¯v) =T/summationdisplay
i=1RevM′(vi),
RevM′(vi)≥OPT (vi) (1−δ)(1−α)K−(2K+ 1)α−mH(1−δ)K;
where the first expression is a result of Algorithm 5, the second expression uses Proposition 48, the third
expands the revenue over Tterms, and the last uses Theorem 27. Rearranging the terms, we have:
RevM′(vi)≥OPT (vi) (1−δ)(1−α)K−(2K+ 1)α−mH(1−δ)K
≥OPT (vi)−OPT (vi)/parenleftbig
1−(1−δ)(1−α)K/parenrightbig
−(2K+ 1)α−mH(1−δ)K
≥OPT (vi)−mH/parenleftbig
1−(1−δ)(1−α)K/parenrightbig
−(2K+ 1)α−mH(1−δ)K
RevM′(¯v)≥OPT (¯v)−mHT/parenleftbig
1−(1−δ)(1−α)K/parenrightbig
−T(2K+ 1)α−mHT (1−δ)K
Rev WM(¯v)≥OPT (¯v)−mHT/parenleftbig
1−(1−δ)(1−α)K/parenrightbig
−T(2K+ 1)α
−mHT (1−δ)K−βmHT
2−mHlnn
β
We set variables K,α,δ, andβas a function of Tto minimize the exponent of Tin the regret. The regret
is upper bounded by
mHT/parenleftbig
1−(1−δ)(1−α)K/parenrightbig
+T(2K+ 1)α+mHT (1−δ)K+βmHT
2+mHlnn
β,
≤mHT/parenleftbig
1−(1−δ)(1−α)K/parenrightbig
+T(2K+ 1)α+mHT (1−δ)K+βmHT
2+mHO (ℓmln (Hm/α ))
β;
where the inequality is followed by upper bounding n. By setting α=T−1,β=T−0.5,K=T0.5, and
δ=T−0.5the regret is bounded by ˜O(m2Hℓ√
T).
Theorem 29. In the full information case for arbitrary length menus of lotteries, running Algorithm 2 on
menus specified in Theorem 27 for α=T−1/(2m+2),β=T−1/(m+1),K=T1/(m+1), andδ=T−1/(m+1)has
regret ˜O(mHT1−1/(2m+4)lnm(mHT )).
42Published in Transactions on Machine Learning Research (03/2024)
Proof.The proof follows the same argument as Theorem 28. The only difference in the parameters is n,
the number of experts, which in this case is n= 2(1/αm+1)(ln (Hm/α ))m.We set variables K,α,δ, andβas
a function of Tto minimize the exponent of Tin the regret. The regret is upper bounded by the formula
below after substituting n
mHT/parenleftbig
1−(1−δ)(1−α)K/parenrightbig
+T(2K+ 1)α+mHT (1−δ)K
+βmHT
2+mH(1/αm+1)(ln (Hm/α ))mln2
β
By setting α=T−1/(2m+2),β=T−1/(m+1),K=T1/(m+1), andδ=T−1/(m+1), the regret is bounded by
˜O(mHT1−1/(2m+4)lnm(mHT )).
B.1.2 Bandit Setting
In the partial information setting, the seller does not see the outcome for all the possible menus and only
observes the outcome of the menu used (the lottery chosen by the buyer). Similar to the two-part tariffs
results, todesignanonlinealgorithminthiscase, weuseaversionoftheExp3algorithmin(Aueretal.,1995).
This variant of the Exp3 algorithm contains the weighted majority algorithm (Algorithm 2) a subroutine. At
each step, we mix the probability distribution π, used by the weighted majority algorithm, with the uniform
distribution to obtain a modified probability distribution π, which is then used to select a menu from our
discretized set. Following the lottery chosen by buyer t, we use the price paid (the gain from the chosen
menu) to formulate a simulated gain vector, which is then used to update the weights maintained by the
weighted majority algorithm.
Similar to two-part tariffs, we use Algorithm 3 for the bandit case. The only difference is that since the
maximum revenue in lotteries is mH, as opposed to two-part tariffs where it is H, in the algorithm we need
to replaceHwithmH.
Proposition 49 ((Auer et al., 1995), Theorem 4.1) .For any sequence of valuations ¯v,
Rev Exp3(¯v)≥OPTX−/parenleftbigg
γ+β
2/parenrightbigg
OPTX−mHn lnn
βγ,
whereX=m1,...,mnare the set of experts (lottery menus), Rev Exp3(¯v)is the expected revenue outcome
of Algorithm 3 where His replaced with mH, and OPTX(¯v)is the revenue of the optimal menu in X.
Theorem 30. In the partial information case for length- ℓmenus of lotteries, running Algorithm 3 over
discretized set of menus in Theorem 27 for α=T−1/(ℓm+2),β=γ=T−1/(4ℓm+8),K=T1/(2ℓm+4), and
δ=T−1/(2ℓm+4)has regret ˜O(m2HℓT1−1/(2ℓm+4)lnℓm+1(mHT )).
Proof.The proof follows the same logic as that of Theorem 28. We denote Rev Exp3()as the revenue obtained
from the Exp3 algorithm described above on the set of outcome menus of Algorithm 5. Similar to the proof
of Theorem 28, in what follows ndenotes the number of menus resulting from the procedure Algorithm 5.
viis the valuation of the buyer at step i, and ¯vis the vector of valuation of all buyers in rounds 1through
T.RevM′()is the maximum revenue obtained in the set of menus resulting from Algorithm 5 and OPT()
as the optimal revenue.
n= (1/αℓm+ℓ) (ln (Hm/α ))lm,
Rev Exp3(¯v)≥RevM′−/parenleftbigg
γ+β
2/parenrightbigg
RevM′−mHn lnn
βγ,
RevM′(¯v) =T/summationdisplay
i=1RevM′(vi),
RevM′(vi)≥OPT (vi) (1−δ)(1−α)K−(2K+ 1)α−mH(1−δ)K;
where the first expression is a result of Algorithm 5, the second expression uses Proposition 49, the third
expands the revenue over Tterms, and the last uses Theorem 27. Rearranging the terms, we have:
43Published in Transactions on Machine Learning Research (03/2024)
Rev Exp3(¯v)≥RevM′(¯v)−/parenleftbigg
γ+β
2/parenrightbigg
RevM′(¯v)−mHn lnn
βγ
≥RevM′(¯v)−/parenleftbigg
γ+β
2/parenrightbigg
mHT−mHn lnn
βγ
≥OPT (¯v)−mHT/parenleftbig
1−(1−δ)(1−α)K/parenrightbig
−T(2K+ 1)α−mHT (1−δ)K
−/parenleftbigg
γ+β
2/parenrightbigg
mHT−mHn lnn
βγ
We set variables K,α,δ,β, andγas a function of Tto minimize the exponent of Tin the regret. After
substituting n, the regret is upper bounded by
mHT/parenleftbig
1−(1−δ)(1−α)K/parenrightbig
+T(2K+ 1)α+mHT (1−δ)K+/parenleftbigg
γ+β
2/parenrightbigg
mHT
+2ℓm2H(1/αℓm+ℓ) (ln (Hm/α ))ℓm+1
βγ
By setting α=T−1/(ℓm+2),β=γ=T−1/(4ℓm+8),K=T1/(2ℓm+4), andδ=T−1/(2ℓm+4), the regret is
bounded by ˜O(m2HℓT1−1/(2ℓm+4)lnℓm+1(mHT )).
B.2 Limited Buyer Types
The ideas for designing a specific algorithm specific to the limited buyer types in the menus of lotteries are
similar to those for menus of two-part tariffs. There are a few changes that we overview here.
One of the main differences is the menu options O. Unlike two-part tariffs that given a menu, the buyer
needed to select a tariff and number of units that maximized the buyer’s utility; for menus of lotteries,
the options are exactly aligned with menu entries, and |O|=ℓ+ 1for length-ℓlotteries. The mechanism
designer’s utility (revenue) given menu ρis equal to p(j)(ρ)if the buyer selects entry j. The buyer selects
entryj, if this entry results in higher utility than any other entry in menu ρ. These inequalities identify
regionsPµ, where the buyer’s utility maximizing option is aligned with µ.
Definition 50 (menu option for menus of lotteries, O).Indexjsuch that 0≤j≤ℓindicating a lottery
index in the menu is a menu option. We denote the set of all menu options as O. This set identifies all
potential actions of a buyer when presented with a menu.
Definition 51 (mappingµ, feasible mappings, Pµ).A mapping µis a function from buyer types, v1,...,vV
to menu options j= 0,1,...,ℓ, wherejis the lottery index assigned to the buyer type. Mapping µis feasible
if there is a menu corresponding to the mapping, i.e., a menu that if presented to the buyers, each buyer
selects their corresponding option in the mapping as their utility maximizing option. Pµdenotes the region
of the parameter space corresponding to µ, i.e., the set of menus inducing mapping µ.
Lemma 52. For each feasible mapping µ, as defined in Definition 51, Pµis a convex polytope with hyperplane
boundaries.
Proof.For a fixed buyer type iand option j= 0,...,ℓ, letP(i)
jbe the set of all parameter vectors ρ
corresponding to the length- ℓmenus that buyer type iselects option j. The buyer selects option jfor menu
ρif this option produces more utility for the buyer than any other option. Formally,
m/summationdisplay
k=1v(ek)ϕ(j)[k](ρ)−p(j)(ρ)≥m/summationdisplay
k=1v(ek)ϕ(j′)[k](ρ)−p(j′)(ρ);∀j′.
The above inequalities identify a convex polytope of parameter vectors (menus ρ) with hyperplane bound-
aries.Pµis the intersection of P(i)
µ(i)fori= 1,...,V. Therefore,Pµis also a convex region with hyperplane
boundaries.
44Published in Transactions on Machine Learning Research (03/2024)
Lemma 53. For each feasible mapping µand any sequence of buyer valuations bthe cumulative utility,/summationtext
iu(bi,ρ), is linear inPµ.
Proof.We show that for any buyer valuation viin the sequence, u(vi,ρ)is linear in the region. Proving this
claim is sufficient for concluding the statement. Let j=µ(vi), i.e.,jis the lottery index that buyer valuation
viselects under µ. Therefore, the utility for this buyer for menu ρ∈Pµis/summationtextm
k=1v(ek)ϕ(j)[k](ρ)−p(j)(ρ).
Note thatϕ(j)[k](ρ)is a coordinate of ρand therefore, has a linear dependence on ρ. Therefore, since the
option that each buyer valuation selects is fixed inside Pµ, the utility is also linear.
Lemma 54. The number of extreme points for menus of lotteries, |E|, is at most (Vℓ2)m(ℓ+1).
Proof.Length-ℓmenus of lotteries occupy a ℓ(m+ 1)-dimensional parameter space. In each d-dimensional
space, an extreme point is the intersection of dlinearly independent hyperplanes. The total number of
hyperplanes defining the regions is H=V/parenleftbigℓ
2/parenrightbig
, where for each buyer type compares the utility of two menu
entries. Out of these hyperplanes, we need ℓ(m+ 1)of them to intersect for an extreme point. Therefore,
the number of extreme points is at most/parenleftbigH
ℓ(m+1)/parenrightbig
, implying the statement.
The following lemma bounds the loss in utility where the set of menus is limited to the extreme points E. The
proof is similar to Balcan et al. (2015); however, the loss depends on the problem-specific utility functions.
Lemma 55. LetEbe as defined in Definition 21, then for any sequence of buyer valuations b=b1,...,bT,
andρ∗as the optimal menu in the hindsight:
maxρ∈ET/summationdisplay
t=1u(bt,ρ)≥T/summationdisplay
t=1u(bt,ρ∗)−εT.
Proof.The proof is similar to that of Lemma 23. The only difference is in step (vi) which computes the
loss in revenue between menus that are at ε L1distance. In menus of lotteries, this distance implies a price
difference of at most εin any of the lotteries in the menu, and therefore causes εtotal loss per time step.
Full Information Setting
Theorem 31. In the full information case for length- ℓmenus of lotteries, when there are Vtypes of buyers,
there is an algorithm with regret bound of O(m2Hℓ√
Tln (Vℓ)).
Proof.The proof follows the same logic as of theorem 24. We run the weighted majority algorithm (Algo-
rithm 2, where His replaced by mH) with parameter β= 1/√
Ton the setEas the set of menus (experts).
The proof directly follows from Lemma 55 and Proposition 48. Let n=|E|. Letbibe the valuation of the
buyer at step i, and ¯bbe the vector of valuation of all buyers in rounds 1throughT. We denote RevE()as
the maximum revenue obtained in the set of E,OPT()as the optimal revenue, and Rev WM()as the revenue
obtained from Algorithm 2 on the set of experts X=E. Then,
n≤(Vℓ2)m(ℓ+1),
Rev WM/parenleftbig¯b/parenrightbig
≥Rev(E)/parenleftbig¯b/parenrightbig
−β
2Rev(E)/parenleftbig¯b/parenrightbig
−mHlnn
β,
RevE/parenleftbig¯b/parenrightbig
=T/summationdisplay
i=1RevE(bi),
RevE(bi)≥OPT (bi)−ε;
45Published in Transactions on Machine Learning Research (03/2024)
where the first expression uses the size of Ein Lemma 54, the second expression uses Proposition 48, the
third expands the revenue over T terms, and the last uses Lemma 55. Rearranging the terms, we have:
RevE(bi)≥OPT (bi)−ε
RevE/parenleftbig¯b/parenrightbig
≥OPT/parenleftbig¯b/parenrightbig
−εT
Rev WM/parenleftbig¯b/parenrightbig
≥OPT/parenleftbig¯b/parenrightbig
−εT−βmHT
2−mHlnn
β
Rev WM/parenleftbig¯b/parenrightbig
≥OPT/parenleftbig¯b/parenrightbig
−εT−βmHT
2−m2(ℓ+ 1)H(ln (Vℓ))
β
We set variables εandβto minimize the exponent of Tin the regret. By setting β=1√
Tandε= 1/(√
T),
The regret will be O(m2Hℓ√
Tln (Vℓ)).
Partial Information (Bandit) Setting In the partial information setting, the change in the menu
options also affects the definition of set Ithat consists of indicator vectors over the buyer types that select
the same menu entry jin a mapping µ. The changes that need to be made in Algorithm 8 to work for menus
of lotteries include changing |O|toℓ+ 1, using option (menu entry) jinstead of (j,k), and changing utility
fromI{k≥1}/parenleftig
p(j)
1(ρ) +kp(j)
2(ρ)/parenrightig
top(j)(ρ). After making these changes, we can perform the modified
algorithm to achieve a bounded regret.
Lemma 56. For any menu ρ,E[ˆuτ(ρ)] =uτ(ρ)andˆuτ(ρ)∈[−mH(ℓ+ 1)V,mH (ℓ+ 1)V].
Proof.The proof is similar to Lemma 56. The proof of the equality of the expectation simply follows
from ˆuτ(ρ)anduτ(ρ)definitions and Lemma 43. Now, we prove the range of the estimator. Since Sis a
barycentric spanner, for any I∈I,λi(I)∈[−1,1]. Also, ˆfτ(.)belongs to{0,1}. Additionally, the utility of
the buyer selecting each option in the menu, e.g., p(j)(ρ), is always in [0,mH ]. Therefore, using the formula
of the estimator, it is bounded by mHtimes the number of options times the number of buyer types.
Theorem 32. In the partial information (bandit) case for length- ℓmenus of lotteries,
when there are Vdifferent types of buyers, there is an algorithm with regret bound of
O(T2/3(ℓm)4/3(HV)1/3log1/3(Vℓ)).
Proof.The proof follows the same logic as of theorem 25. In Lemma 45, |S|is the number of dimensions
(barycentric spanner set), κis the maximum revenue times the number of buyer types times the number
of their options (entries in the menu), |M|is the number of extreme points. In our case, |S|=ℓ(m+ 1),
κ=mHV (ℓ+1), and|M|≤(Vℓ2)m(ℓ+1). By Lemma 56, the expected value of the estimated utility is equal
to the exact value of utility with range [−mH(ℓ+ 1)V,mH (ℓ+ 1)V].
Using Lemma 45, the regret for menus of lotteries is bounded by
O(T2/3(ℓm)4/3(HV)1/3log1/3(Vℓ)).
B.3 Distributional Learning
Theorem 34. For length- ℓmenus of lotteries, there is a discretization-based distributional
learning algorithm with sample complexity ˜O/parenleftbig
m2H2/ε2(ℓm+ ln (2/δ))/parenrightbig
, and running time
˜O/parenleftig/parenleftbig
2m2H2/ε2/parenrightbigℓm+ℓ+1ℓ(ℓm+ ln (2/δ)) lnℓm(mH/ε ln (mH/ε ))/parenrightig
.
Proof.We need to find the number of samples such that with probability 1−δ, the difference between the
expected revenue of our algorithm and the optimal revenue is at most ε. Note that since our algorithm uses
discretization of possible menus, we face two types of errors: the discretization error, and the usual empirical
46Published in Transactions on Machine Learning Research (03/2024)
error in a PAC learning setting. We find the sample complexity and discretization parameters such that the
total error is bounded by ε.
The possible number of menus after discretization using Algorithm 5 with parameter αis computed by the
following formula.
|H|= (1/αℓm+ℓ) (ln (Hm/α ))ℓm
Using uniform convergence in the PAC learning setting, the sample complexity for empirical error ε′is as
follows.
|S|≥m2H2
2ε′2(ln|H|+ ln (2/δ))
Replacing lnHwe have,
|S|≥m2H2
2ε′2(ℓm(ln(1/α) + ln ln (mH) + ln (2/δ))
Also, the revenue loss compared to the optimum for arbitrary buyer iwith valuation viwhen using Algo-
rithm 5 with parameters α,K, andd(we usedinstead ofδin Algorithm 5 and reserve δfor(ε,δ)-learning)
is computed by the following formula.
RevM′(vi)≥OPT(vi)(1−d)(1−α)K−(2K+ 1)α−mH(1−d)K
The total error (from discretization and empirical error), when the empirical error is set to ε′, is
mH[1−(1−d)(1−α)K] + (2K+ 1)α+mH(1−d)K+ε′
By setting d=ε′/(2mH),K= 2mH/ε′ln(mH/ε′), andα=ε′/(2m2H2ln(mH/ε′)), the total mistake is
less than 4ε′.
Replacing these parameters and substituting ε′withε/4to satify total error ε, we have the following sample
complexity:
|S|≥m2H2
2ε′2(ℓm(ln(1/α) + ln ln (mH) + ln (2/δ))
=˜O/parenleftbiggm2H2
ε2(ℓm+ ln (2/δ))/parenrightbigg
Also, replacing the parameters we have:
|H|=O/parenleftigg/parenleftbigg2m2H2
ε2/parenrightbiggℓm+ℓ
lnℓm(mH/ε ln (mH/ε ))/parenrightigg
The computational complexity of finding the empirical optimal menu for |S|buyers and menu of size ℓis:
|S|ℓ|H|=˜O/parenleftigg/parenleftbigg2m2H2
ε2/parenrightbiggℓm+ℓ+1
ℓ(ℓm+ ln (2/δ)) lnℓm(mH/ε ln (mH/ε ))/parenrightigg
This implies the computational complexity of the algorithm.
Theorem 57. For arbitrary-length menus of lotteries, there is a discretization-based distributional learning
algorithm with sample complexity
O/parenleftbiggm2H2
ε2/parenleftbig
(32m2H2/ε2)m+1lnm(mH/ε ln(mH/ε )) lnm+1(mH/ε ) + ln (1/δ)/parenrightbig/parenrightbigg
,
and running time
O/parenleftig
2(32m2H2/ε2)m+1lnm(mH/ε ln(mH/ε )) lnm+1(mH/ε )/parenrightig
.
47Published in Transactions on Machine Learning Research (03/2024)
Proof.This proof follows the same line as the proof of Theorem 34. We need to find the number of samples
such that with probability 1−δ, the difference between the expected revenue of our algorithm and the
optimal revenue is at most ε. Note that since our algorithm uses discretization of possible menus, we face
two types of errors: the discretization error, and the usual empirical error in a PAC learning setting. We
find the sample complexity and discretization parameters such that the total error is bounded by ε.
The possible number of menus after discretization using Algorithm 5 with parameter αis computed by the
following formula.
|H|=O/parenleftig
2(1/αm+1)(ln (Hm/α ))m/parenrightig
Using uniform convergence in the PAC learning setting, the sample complexity for empirical error ε′is as
follows.
|S|≥m2H2
2ε′2(ln|H|+ ln (2/δ))
Replacing lnHwe have,
|S|≥m2H2
2ε′2/parenleftbigglnm(Hm/α )
αm+1+ ln (2/δ)/parenrightbigg
Also, the revenue loss compared to the optimum for arbitrary buyer iwith valuation viwhen using Algo-
rithm 5 with parameters α,K, andd(we usedinstead ofδin Algorithm 5 and reserve δfor(ε,δ)-learning)
is computed by the following formula.
RevM′(vi)≥OPT(vi)(1−d)(1−α)K−(2K+ 1)α−mH(1−d)K
The total error (from discretization and empirical error) when the empirical error is set to ε′is
mH[1−(1−d)(1−α)K] + (2K+ 1)α+mH(1−d)K+ε′
By setting d=ε′/(2mH),K= 2mH/ε′ln(mH/ε′), andα=ε′/(2m2H2ln(mH/ε′)), the total mistake is
less than 4ε′.
Replacing these parameters and substituting ε′withε/4to satify total error ε, we have the following sample
complexity:
|S|≥m2H2
2ε′2/parenleftbigglnm(mH/α )
αm+1+ ln (2/δ)/parenrightbigg
=O/parenleftbiggm2H2
ε2/parenleftbig
(32m2H2/ε2)m+1lnm(mH/ε ln(mH/ε )) lnm+1(mH/ε ) + ln (1/δ)/parenrightbig/parenrightbigg
Also, replacing the parameters we have:
|H|=O/parenleftig
2(1/αm+1) lnm(Hm/α )/parenrightig
=O/parenleftig
2(32m2H2/ε2)m+1lnm(mH/ε ln(mH/ε )) lnm+1(mH/ε )/parenrightig
The computational complexity of finding the empirical optimal menu for |S|buyers is the number of potential
menus|H|times|S|times the maximum size of a menu which is O(ln(H)).
Lemma 58. The sample complexity of length ℓmenus of lotteries using the techniques in (Balcan et al.,
2018b) is bounded by
c/parenleftbiggH
ε/parenrightbigg2/parenleftbigg
9ℓ(m+ 1) log/parenleftbig
4ℓ(m+ 1)/parenleftbig
(ℓ+ 1)2+mℓ/parenrightbig/parenrightbig
+ log1
δ/parenrightbigg
.
48Published in Transactions on Machine Learning Research (03/2024)
Proof.Balcan et al. (2018c) introduce delineability as a condition to upper bound the pseudo-dimension
and therefore, the sample complexity. They show the class of lotteries is (ℓ(m+ 1),(ℓ+ 1)2+
mℓ)-delineable. Also, if Mis a mechanism class that is (d,t)-delineable, then the pseudo di-
mension of Mis at most 9dlog(4dt). Therefore, the pseudo-dimension for menus of lotteries is
bounded by 9ℓ(m+ 1) log/parenleftbig
4ℓ(m+ 1)/parenleftbig
(ℓ+ 1)2+mℓ/parenrightbig/parenrightbig
. Furthermore, the sample complexity is at most
c(H/ε)2(Pdim(H) + log (1/δ)), which by replacing pseudo dimension for this class of mechanism completes
the proof.
49