Published in Transactions on Machine Learning Research (12/2024)
Generalizing Denoising to Non-Equilibrium Structures
Improves Equivariant Force Fields
Yi-Lun Liao ylliao@mit.edu
Massachusetts Institute of Technology
Work partially done during an internship at FAIR, Meta
Tess Smidt tsmidt@mit.edu
Massachusetts Institute of Technology
Muhammed Shuaibi‹mshuaibi@meta.com
FAIR, Meta
Abhishek Das‹abhshkdz@gmail.com
Work done at FAIR, Meta
‹Equal contribution
Reviewed on OpenReview: https://openreview.net/forum?id=whGzYUbIWA
Code: https://github.com/atomicarchitects/DeNS
Abstract
Understanding the interactions of atoms such as forces in 3D atomistic systems is fundamental
to many applications like molecular dynamics and catalyst design. However, simulating
these interactions requires compute-intensive ab initio calculations and thus results in
limited data for training neural networks. In this paper, we propose to use denoising
non-equilibrium structures ( DeNS) as an auxiliary task to better leverage training data and
improve performance. For training with DeNS, we first corrupt a 3D structure by adding
noise to its 3D coordinates and then predict the noise. Different from previous works on
denoising, which are limited to equilibrium structures, the proposed method generalizes
denoising to a much larger set of non-equilibrium structures. The main difference is that
a non-equilibrium structure does not correspond to local energy minima and has non-zero
forces, and therefore it can have many possible atomic positions compared to an equilibrium
structure. This makes denoising non-equilibrium structures an ill-posed problem since the
target of denoising is not uniquely defined. Our key insight is to additionally encode the
forces of the original non-equilibrium structure to specify which non-equilibrium structure
we are denoising. Concretely, given a corrupted non-equilibrium structure and the forces
of the original one, we predict the non-equilibrium structure satisfying the input forces
instead of any arbitrary structures. Since DeNS requires encoding forces, DeNS favors
equivariant networks, which can easily incorporate forces and other higher-order tensors in
node embeddings. We study the effectiveness of training equivariant networks with DeNS on
OC20, OC22 and MD17 datasets and demonstrate that DeNS can achieve new state-of-the-art
results on OC20 and OC22 and significantly improve training efficiency on MD17.
1 Introduction
Graph neural networks (GNNs) have made remarkable progress in approximating high-fidelity, compute-
intensive quantum mechanical calculations like density functional theory (DFT) for atomistic systems (Gilmer
1Published in Transactions on Machine Learning Research (12/2024)
Figure 1: Illustration of denoising equilibrium and non-equilibrium structures. As shown in (a), we relax a non-
equilibrium structure Snon-eqand obtain the final relaxed, equilibrium structure Seq. The path between Snon-eqand
Seqforms a relaxation trajectory. All structures Salong the trajectory except Seqare non-equilibrium and have
non-zero forces FpSq. For denoising structures S, we add noise to their 3D atomic coordinates to obtain corrupted
structures ˜Sand predict the corresponding noise given ˜S. We compare denoising non-equilibrium and equilibrium
structures in (b) and (c), respectively, and show that denoising non-equilibrium structures can be ill-posed in (b). The
issue in (b) can be addressed with force encoding, where we take forces as additional inputs, as in (d) and (e).
et al., 2017; Zhang et al., 2018; Unke et al., 2021; Batzner et al., 2022; Rackers et al., 2023; Lan et al., 2022),
enabling new insights in applications such as molecular dynamics simulations (Musaelian et al., 2023) and
catalyst design (Chanussot* et al., 2021; Lan et al., 2022). However, unlike other domains such as natural
language processing (NLP) and computer vision (CV), the scale of atomistic data is quite limited since
generating data requires compute-intensive ab initio calculations. For example, the largest atomistic dataset,
OC20 (Chanussot* et al., 2021), contains about 138M examples while GPT-3 (Brown et al., 2020) is trained
on hundreds of billions of words and ViT-22B (Dehghani et al., 2023) is trained on around 4B images.
To start addressing this gap, we take inspiration from self-supervised learning methods in NLP and CV
and explore how we can adapt them to learn better atomistic representations from existing labeled data.
Specifically, one of the most popular self-supervised learning methods in NLP (Devlin et al., 2019) and
CV (He et al., 2022) is training a denoising autoencoder (Vincent et al., 2008), where the idea is to mask or
corrupt a part of the input data and learn to reconstruct or denoise the original, uncorrupted data. Denoising
assumes we know a unique target structure such as a sentence and an image in the case of NLP and CV.
Indeed, this is the case for equilibrium structures (e.g., Seqat a local energy minimum in Figure 1(c)) as has
been demonstrated by previous works leveraging denoising for pretraining on atomistic data (Jiao et al., 2022;
Zaidi et al., 2023; Liu et al., 2023; Wang et al., 2023; Feng et al., 2023a). However, most previous works are
limited to equilibrium structures, and equilibrium structures constitute only a small portion of available data
since structures along a relaxation trajectory to get to a local minimum are all non-equilibrium as shown in
Figure 1. Hence, it is important to generalize denoising to leverage the much larger set of non-equilibrium
structures.
Since a non-equilibrium structure has non-zero atomic forces and atoms are not confined to local energy
minima, it can have more possible atomic positions than a structure at equilibrium. As shown in Figure 1(b),
this can make denoising a non-equilibrium structure an ill-posed problem since there are many possible target
structures. To address the issue, we propose force encoding and take the forces of the original non-equilibrium
structures as inputs when denoising non-equilibrium structures. Intuitively, the forces constrain the atomic
positions of a non-equilibrium structure. With the additional information, we are able to predict the original
non-equilibrium structure satisfying the input forces instead of predicting any arbitrary structures as shown
in Figure 1(d). Previous works on denoising equilibrium structures (Godwin et al., 2022; Jiao et al., 2022;
2Published in Transactions on Machine Learning Research (12/2024)
Zaidi et al., 2023; Liu et al., 2023; Feng et al., 2023b;a) end up being a special case where the forces of original
structures are close to zero as in Figure 1(e).
Based on the insight, in this paper, we propose to use denoisingnon-equilibrium structures ( DeNS) as an
auxiliary task to better leverage atomistic data. For training DeNS, we first corrupt a structure by adding
noise to its 3D atomic coordinates and then reconstruct the original uncorrupted structure by predicting the
noise. For noise prediction, a model is given the forces of the original uncorrupted structure as inputs to make
the transformation from a corrupted non-equilibrium structure to an uncorrupted non-equilibrium structure
tractable. When used along with the original tasks like predicting the energy and forces of non-equilibrium
structures, DeNS improves the performance on the original tasks with a marginal increase in training cost. We
further discuss how DeNS can better leverage training data to improve performance and show the connection
to self-supervised learning methods in other domains.
Because DeNS requires encoding forces, it favors equivariant networks, which build up equivariant features
at each node with vector spaces of irreducible representations (irreps) and have interactions or message
passing between nodes with equivariant operations like tensor products. Since forces can be projected to
vector spaces of irreps with spherical harmonics, equivariant networks can easily incorporate forces in node
embeddings. Moreover, with the reduced complexity of equivariant operations (Passaro & Zitnick, 2023) and
incorporation of the Transformer network design (Liao & Smidt, 2023; Liao et al., 2023) from NLP (Vaswani
et al., 2017) and CV (Dosovitskiy et al., 2021), equivariant networks have become the state-of-the-art methods
on large-scale atomistic datasets.
We mainly focus on how DeNS can improve equivariant networks and conduct extensive experiments on
OC20 (Chanussot* et al., 2021), OC22 (Tran* et al., 2022) and MD17 (Chmiela et al., 2017; Schütt et al.,
2017; Chmiela et al., 2018) datasets. On OC20 S2EF-2M dataset, EquiformerV2 (Liao et al., 2023) trained
with DeNS can achieve better energy and force results and save 2.3ˆtraining time compared to training
without DeNS (Section 4.1.1). On OC20 S2EF-All+MD dataset, EquiformerV2 trained with DeNS achieves
new state-of-the-art results on Structure to Energy and Forces (S2EF) and Initial Structure to Relaxed
Energy (IS2RE) tasks (Section 4.1.2). Similarly, EquiformerV2 trained with DeNS sets new state-of-the-art
results on OC22 dataset, improving energy by up to 15%, forces by up to 12%, and IS2RE by up to 15%
(Section 4.2). On MD17 dataset, Equiformer ( Lmax“2) (Liao & Smidt, 2023) trained with DeNS achieves
better results and saves 3.1ˆtraining time compared to Equiformer ( Lmax“3) without DeNS (Section 4.3),
whereLmaxdenotes the maximum degree. Additionally, DeNS can improve other equivariant networks like
eSCN (Passaro & Zitnick, 2023) on OC20 and SEGNN-like networks (Brandstetter et al., 2022) on MD17.
2 Related Works
Denoising 3D Atomistic Structures. Denoising structures have been used to boost the performance of
GNNs on 3D atomistic datasets (Godwin et al., 2022; Jiao et al., 2022; Zaidi et al., 2023; Liu et al., 2023;
Feng et al., 2023b; Wang et al., 2023; Feng et al., 2023a). The approach is to first corrupt data by adding
noise and then train a denoising autoencoder to reconstruct the original data by predicting the noise, and the
motivation is that learning to reconstruct data enables learning generalizable representations (Devlin et al.,
2019; He et al., 2022; Godwin et al., 2022; Zaidi et al., 2023). Since denoising equilibrium structures do not
require labels and is self-supervised, similar to BERT (Devlin et al., 2019) and MAE (He et al., 2022), it is
common to pre-train via denoising on a large dataset of equilibrium structures like PCQM4Mv2 (Nakata &
Shimazaki, 2017) and then fine-tune with supervised learning on smaller downstream datasets. Besides, the
work of Noisy Nodes (Godwin et al., 2022) uses denoising equilibrium structures as an auxiliary task along
with original tasks without pre-training on another larger dataset. However, most of the previous works are
limited to equilibrium structures, which occupy a much smaller amount of data than non-equilibrium ones.
In contrast, the proposed DeNS generalizes denoising to non-equilibrium structures with force encoding so
that we can improve the performance on the larger set of non-equilibrium structures. We provide a detailed
comparison to previous works on denoising in Section A.1.
3Published in Transactions on Machine Learning Research (12/2024)
SE(3)/E(3) -Equivariant Networks. Please refer to Section A.2 for discussion on equivariant networks.
In addition, since we mainly focus on applying the proposed DeNS to Equiformer series (Liao & Smidt, 2023;
Liao et al., 2023), we provide a brief introduction to them in Section A.3.
3 Method
3.1 Problem Setup
Calculating quantum mechanical properties like energy and forces of 3D atomistic systems is fundamental
to many applications. An atomistic system can be one or more molecules, a crystalline material and so on.
Specifically, each system Sis an example in a dataset and can be described as S“tpzi,piq|iPt1,...,|S|uu,
whereziPNdenotes the atomic number of the i-th atom and piPR3denotes the 3D atomic position. The
energy ofSis denoted as EpSqPR, and the atom-wise forces are denoted as FpSq“␣
fiPR3|iPt1,...,|S|u(
,
where fiis the force acting on the i-th atom. In this paper, we define a system to be an equilibrium structure
if all of its atom-wise forces are close to zero. Otherwise, we refer to it as a non-equilibrium structure. Since
non-equilibrium structures have non-zero atomic forces and thus are not at an energy minimum, they have
more degrees of freedom and constitute a much larger set of possible structures than those at equilibrium.
In this work, we focus on the task of predicting energy and forces given non-equilibrium structures. Specifically,
given a non-equilibrium structure Snon-eq, GNNs predict energy ˆEpSnon-eqqand atom-wise forces ˆFpSnon-eqq“!
ˆfipSnon-eqqPR3|iPt1,...,|Snon-eq|u)
and minimize the following loss function:
λE¨LE`λF¨LF“λE¨|E1pSnon-eqq´ˆEpSnon-eqq|`λF¨1
|Snon-eq||Snon-eq|ÿ
i“1|f1
ipSnon-eqq´ˆfipSnon-eqq|2(1)
whereλEandλFare energy and force coefficients controlling the relative importance between energy and force
predictions. E1pSnon-eqq“EpSnon-eqq´µE
σEis the normalized ground truth energy obtained by first subtracting
the original energy EpSnon-eqqby the mean of energy labels in the training set µEand then dividing by the
standard deviation of energy labels σE. Similarly, f1
i“fi
σFis the normalized atom-wise force. For force
prediction, we can either use direct methods, which is to directly predict forces from latent representations
like node embeddings, as commonly used for OC20 and OC22 datasets or gradient methods, which is to take
the negative gradients of predicted energy with respect to atomic positions, for datasets like MD17.
3.2 Denoising Non-Equilibrium Structures (DeNS)
3.2.1 Formulation of Denoising
Denoising structures has been used to improve the performance of GNNs on 3D atomistic datasets (Godwin
et al., 2022; Zaidi et al., 2023; Feng et al., 2023b; Wang et al., 2023). We first corrupt data by adding noise
and then train a denoising autoencoder to reconstruct the original data by predicting the noise. Specifically,
given a 3D atomistic system S“tpzi,piq|iPt1,...,|S|uu, we create a corrupted structure ˜Sby adding
Gaussian noise with a tuneable standard deviation σto the atomic positions piof the original structure S:
˜S“tpzi,˜piq|iPt1,...,|S|uu, where ˜pi“pi`ϵiand ϵi„Np0,σI3q (2)
We denote the set of noise added to SasNoisepS,˜Sq“␣
ϵiPR3|iPt1,...,|S|u(
. When training a denoising
autoencoder, GNNs take ˜Sas inputs, output atom-wise noise prediction ˆϵp˜Sqiand minimize the L2 difference
between normalized noiseϵi
σand noise prediction ˆϵp˜Sqi:
EppS,˜Sq»
–1
|S||S|ÿ
i“1ˇˇˇϵi
σ´ˆϵp˜Sqiˇˇˇ2fi
fl (3)
whereppS,˜Sqdenotes the probability of obtaining the corrupted structure ˜Sfrom the original structure S.
We divide the noise ϵiby the standard deviation σto normalize the outputs of noise prediction.
4Published in Transactions on Machine Learning Research (12/2024)
When the original structure Sis an equilibrium structure, denoising is to find the structure corresponding to
the nearest energy local minima given a high-energy corrupted structure. This makes denoising equilibrium
structures a many-to-one mapping and therefore a well-defined problem. However, when the original structure
Sis a non-equilibrium structure, denoising, the transformation from a corrupted non-equilibrium structure to
the original non-equilibrium one, can be an ill-posed problem since there are many possible target structures
as shown in Figure 1(b).
3.2.2 Force Encoding
The reason that denoising non-equilibrium structures can be ill-posed is because we do not provide sufficient
information to specify the properties of the target structures. Concretely, given an original non-equilibrium
structureSnon-eqand its corrupted counterpart ˜Snon-eq, some structures interpolated between Snon-eqand
˜Snon-eqcould be in the same data distribution and therefore be the potential target structures of denoising.
In contrast, when denoising equilibrium structures as shown in Figure 1(c), we implicitly provide the extra
information that the target structure should be at equilibrium with near-zero forces, and this therefore
limits the possibility of the target of denoising. Motivated by the assumption that the forces of the original
structures are close to zeros when denoising equilibrium ones, we propose to encode the forces of original
non-equilibrium structures when denoising non-equilibrium ones as illustrated in Figure 1(d). Specifically,
when training denoisingnon-equilibrium structures (DeNS), GNNs take both a corrupted non-equilibrium
structure ˜Snon-eqand the forces FpSnon-eqqof the original non-equilibrium structure Snon-eqas inputs, output
atom-wise noise prediction ˆϵ`˜Snon-eq,FpSnon-eqq˘
iand minimize the L2 difference between normalized noise
ϵi
σand noise prediction ˆϵ`˜Snon-eq,FpSnon-eqq˘
i:
LDeNS“EppSnon-eq,˜Snon-eqq»
–1
|Snon-eq||Snon-eq|ÿ
i“1ˇˇˇϵi
σ´ˆϵ`˜Snon-eq,FpSnon-eqq˘
iˇˇˇ2fi
fl (4)
Equation 4 is more general and reduces to Equation 3 when the target of denoising becomes equilib-
rium structures with near-zero forces. Since we train GNNs with ˜Snon-eqandFpSnon-eqqas inputs and
NoisepSnon-eq,˜Snon-eqqas outputs, they implicitly learn to leverage FpSnon-eqqto reconstruct Snon-eqinstead
of predicting any arbitrary non-equilibrium structures. Empirically, we find that force encoding is critical to
the effectiveness of DeNS on OC20 S2EF-2M dataset (Index 2 and Index 3 in Table 1(b)) and MD17 dataset
(Index 2 and Index 3 in Table 6).
Since DeNS requires encoding forces, DeNS favors equivariant networks, which can easily incorporate forces as
well as other higher-degree tensors like stress into their node embeddings. Specifically, the node embeddings
of equivariant networks are equivariant irreps features built from vectors spaces of irreducible representations
(irreps) and contain CLchannels of type- Lvectors with degree Lbeing in the range from 0to maximum
degreeLmax.CLandLmaxare architectural hyper-parameters of equivariant networks. To obtain the
force embedding xffrom the input force f, we first project finto type-Lvectors with spherical harmonics
YpLq´
f
||f||¯
, where 0ďLďLmax, and then expand the number of channels from 1toCLwith anSOp3q
linear layer (Geiger et al., 2022; Geiger & Smidt, 2022; Liao & Smidt, 2023):
xpLq
f“SO3_LinearpLqˆ
||f||¨YpLqˆf
||f||˙˙
(5)
xpLq
fdenotes the channels of type- Lvectors in force embedding xf, andSO3_LinearpLqdenotes the SOp3q
linear operation on type- Lvectors. Since we normalize the input force when using spherical harmonics,
we multiply YpLq´
f
||f||¯
with the norm of input force ||f||to recover the information of force magnitude.
After computing force embeddings for all atom-wise forces, we simply add the force embeddings to initial
node embeddings to encode forces in equivariant networks. The pseudocode for encoding forces into node
embeddings can be found in Section E.
On the other hand, it might not be that intuitive to encode forces in invariant networks since their internal
latent representations such as node embeddings and edge embeddings are scalars instead of type- Lvectors
5Published in Transactions on Machine Learning Research (12/2024)
or geometric tensors. One potential manner of encoding forces in latent representations is to project them
into edge embeddings by taking inner products between forces and edge vectors of relative positions. This
process is the inverse of how GemNet-OC (Gasteiger et al., 2022) decodes forces from latent representations.
Since equivariant networks have been shown to outperform invariant networks on atomistic datasets and are
a more natural fit to encoding forces, we mainly focus on equivariant networks and how DeNS can further
advance their performance. As for other work (Duval et al., 2023) leveraging frame averaging to achieve
equivariance through data transformations, we can follow how unit cell Cartesian coordinates are projected in
their framework to encode forces when optimizing for DeNS. Specifically, we compute one set of frame axes
using only 3D atomic positions and project the input forces to the frame axes. The projected forces remain
the same under any Ep3qtransformation and enable using unconstrained functions to encode the input forces.
3.2.3 Training with DeNS
Auxiliary Task. We propose to use DeNS as an auxiliary task along with the original task of predicting
energy and forces and summarize the training process in Figure 2. Specifically, given a batch of structures,
for each structure, we decide whether we optimize the objective of DeNS (Figure 2(b) or Figure 2(c)) or
the objective of the original task (Figure 2(a)). This introduces an additional hyper-parameter pDeNS,
the probability of optimizing DeNS. We use an additional noise head for noise prediction, which slightly
increases training time. Additionally, when training DeNS, similar to Noisy Nodes (Godwin et al., 2022),
we also leverage energy labels and predict the energy of original structures. Therefore, given an original
non-equilibrium structure Snon-eqand the corrupted counterpart ˜Snon-eq, training DeNS corresponds to
minimizing the following loss function:
λE¨LE`λDeNS¨LDeNS“λE¨ˇˇˇE1pSnon-eqq´ˆE`˜Snon-eq,FpSnon-eqq˘ˇˇˇ`λDeNS¨LDeNS (6)
whereLDeNSdenotes the loss function of denoising as defined in Equation 4. We note that we also encode
forces as discussed in Section 3.2.2 to predict the energy of Snon-eq, and we share the energy prediction head
across Equation 1 and Equation 6. The loss function introduces another hyper-parameter λDeNS, DeNS
coefficient, controlling the relative importance of the auxiliary task. Besides, the process of corrupting
structures also results in another hyper-parameter σas shown in Equation 2. We provide the pseudocode in
Section F. We note that the force encoding in DeNS only relies on force labels on the training set and we do
not use any force labels on the validation or testing sets.
Partially Corrupted Structures. Empirically, we find that adding noise to all atoms in a structure can
sometimes lead to limited performance gain of DeNS. We surmise adding noise to all atoms makes denoising
too difficult and potentially not well-defined and that there are still several structures satisfying input forces.
To address this issue, we use partially corrupted structures, where we only add noise to and denoise a random
subset of atoms as shown in Figure 2(c). Specifically, for corrupted atoms, we encode their atom-wise forces
and predict the noise. For other uncorrupted atoms, we do not encode forces and train for the original task
of predicting forces. We also predict the energy of the original structures given partially corrupted structures.
This introduces an additional hyper-parameter, corruption ratio rDeNS, which is the ratio of the number of
corrupted atoms to that of all atoms.
3.2.4 Discussion
Why Does DeNS Help. DeNS can better leverage training data to improve the performance in the
following two ways. First, DeNS adds noise to non-equilibrium structures to generate structures with new
geometries and therefore naturally achieves data augmentation (Godwin et al., 2022). Second, training DeNS
enourages learning a different yet highly correlated interaction. Since we encode forces as inputs and predict
the original structures in terms of noise corrections, DeNS enables learning the interaction of transforming
forces into structures, which is the inverse of predicting forces. As demonstrated in the works of Noisy
Nodes (Godwin et al., 2022) and UL2 (Tay et al., 2023), training a single model with multiple correlated
objectives to learn different interactions can help the performance on original tasks.
6Published in Transactions on Machine Learning Research (12/2024)
(a) Original task of predicting energy and forces.
(b) DeNS without partially corrupted structures. All the atoms in a structure are corrupted with Gaussian noise. We
encode all the atom-wise forces to predict noise.
(c) DeNS with partially corrupted structures. We add noise to and denoise a random subset of atoms. For corrupted
atoms, we encode their forces and predict the noise. For other uncorrupted atoms, we instead predict their forces.
Here only the top two white atoms are corrupted.
Figure 2: Training process when incorporating DeNS as an auxiliary task. The pseudocode for training with DeNS
can be found in Section F. “Equivariant GNN”, “energy head”, “force head” and “noise head” are shared across (a),
(b) and (c). For each batch of structures, we use the original task (a) for some structures and DeNS ((b) or (c)) for
the others. Using partially corrupted structures as in (c) is empirically better than (b). We note that the force label
FpSnon-eqqand energy label EpSnon-eqqused in (b) and (c) are the same as those in (a) and that training with DeNS
does not require any additional data.
Connection to Self-Supervised Learning. DeNS shares similar intuitions to self-supervised learning
methods like BERT (Devlin et al., 2019) and MAE (He et al., 2022) and other denoising methods (Vincent
et al., 2008; 2010; Godwin et al., 2022; Zaidi et al., 2023) – they remove or corrupt a portion of input data
and then learn to predict the original data. Learning to reconstruct data can help learning generalizable
representations, and therefore these methods can use the task of reconstruction to improve the performance
on downstream tasks. However, unlike those self-supervised learning methods (Devlin et al., 2019; He et al.,
2022; Zaidi et al., 2023), DeNS requires force labels for encoding, and therefore, we propose to use DeNS as an
7Published in Transactions on Machine Learning Research (12/2024)
EquiformerV2 EquiformerV2 + DeNS
Number of training time Number of training time
Epochs forces energy parameters (GPU-hours) forces energy parameters (GPU-hours)
12 20.46 285 83M 1398 19.09 269 89M 1501
20 19.78 280 83M 2330 18.58 260 89M 2501
30 19.42 278 83M 3495 18.02 251 89M 3752
eSCN eSCN + DeNS
20 20.61 290 52M 1802 19.14 268 52M 1829
(a) Comparison of training with and without DeNS.Method
Force λE‰0Partially corrupted
Index DeNS encoding in Eq. 6 structures forces energy
1 20.46 285
2 ✓ ✓ ✓ ✓ 19.09 269
3 ✓ ✓ ✓ 21.32 278
4 ✓ ✓ ✓ 18.87 285
5 ✓ ✓ ✓ 19.54 279
(b) Design Choices.
Table 1: Ablation results of training with DeNS on OC20 S2EF-2M dataset. We report mean absolute errors for forces
in meV/Å and energy in meV, and lower is better. Errors are averaged over the four validation sub-splits of OC20. (a)
We train EquiformerV2 and eSCN and compare the results of training with and without DeNS. The training time is
measured on V100 GPUs. (b) We train EquiformerV2 for 12epochs to verify the design choices of DeNS.
auxiliary task optimized along with original tasks and do not follow the previous practice of first pre-training
and then fine-tuning. Additionally, we note that before obtaining a single equilibrium structure, we need to
run relaxations and generate many intermediate non-equilibrium ones (Figure 1(a)), which is the labeling
process as well. We hope that the ability to leverage more from non-equilibrium structures as proposed
in this work can encourage researchers to release data containing intermediate non-equilibrium structures
in addition to final equilibrium ones. Moreover, we note that DeNS can also be used in fine-tuning. For
example, we can first pre-train models on PCQM4Mv2 dataset and then fine-tune them on the smaller MD17
dataset with both the original task and DeNS.
Marginal Increase in Training Time. Since we use an additional noise head for denoising, training with
DeNS marginally increases the time of each training iteration. We optimize DeNS for some structures and
the original task for the others for each training iteration, and we demonstrate that DeNS can improve the
performance given the same amount of training iterations. Therefore, training with DeNS only marginally
increase the overall training time.
4 Experiments
4.1 OC20 Dataset
Dataset and Tasks. We start with experiments on the large and diverse Open Catalyst 2020 dataset
(OC20) (Chanussot* et al., 2021), which consists of about 1.2M Density Functional Theory (DFT) relaxation
trajectories. Each DFT trajectory in OC20 starts from an initial structure of an adsorbate molecule placed on a
catalyst surface, which is then relaxed with the revised Perdew-Burke-Ernzerhof (RPBE) functional (Hammer
et al., 1999) calculations to a local energy minimum. Relevant to DeNS, all the intermediate structures from
a relaxation trajectory, except the relaxed structure, are considered non-equilibrium structures. The relaxed
or equilibrium structure has forces close to zero.
The primary task in OC20 is Structure to Energy and Forces (S2EF), which is to predict the energy and
per-atom forces given an equilibrium or non-equilibrium structure from any point in the trajectory. These
predictions are evaluated on energy and force mean absolute error (MAE). Most of the previous works
use direct methods to predict forces on OC20, and we follow this practice and investigate how DeNS can
improve direct methods for force prediction. Once a model is trained for S2EF, it is used to run structural
relaxations from an initial structure using the predicted forces till a local energy minimum is found. The
energy predictions of these relaxed structures are evaluated on the Initial Structure to Relaxed Energy
(IS2RE) task.
Training Details. Please refer to Section B.1 for details on DeNS, architectures, hyper-parameters and
training time.
8Published in Transactions on Machine Learning Research (12/2024)
4.1.1 Ablation Studies
We use OC20 S2EF-2M dataset to compare the results of training with and without DeNS and verify the
design choices of DeNS.
Comparison between Training with and without DeNS. Table 1(a) summarizes the results of training
with and without DeNS. For EquiformerV2, incorporating DeNS as an auxiliary task boosts the performance
on energy and force predictions and only increases training time by 7.4%and the number of parameters from
83M to 89M. Particularly, EquiformerV2 trained with DeNS for 12epochs outperforms EquiformerV2 trained
without DeNS for 30epochs and saves 2.3ˆtraining time. This suggests that using data augmentation and
learning an auxiliary task are more efficient to improve performance compared to simply training for longer.
Additionally, we show that DeNS can be applicable to other equivariant networks like eSCN (Passaro &
Zitnick, 2023). Training eSCN with DeNS for 20 epochs results in better energy and force MAE compared to
EquiformerV2 trained without DeNS for 30 epochs while requiring 1.9ˆless training time. DeNS slightly
increases training time of eSCN by 1.5%, and the different amounts of increase in training time between
EquiformerV2 and eSCN are because they use different modules for noise prediction.
Design Choices. We train EquiformerV2 for 12 epochs with DeNS to verify the design choices of DeNS
and summarize the results in Table 1(b). Comparing Index 2 and Index 3, we show that encoding forces
FpSnon-eqqin Equations 4 and 6 enables denoising non-equilibrium structures to significantly improve both
energy and force MAE. Moreover, DeNS without force encoding (Index 3) results in clearly worse force MAE
compared to training without DeNS (Index 1). These results verify our claim that denoising non-equilibrium
structures naively can be ill-posed and potentially harmful to performance and that force encoding is critical
to leverage the gain of denoising. We note that force encoding only relies on force labels in the training
set and does not require any additional data. We also compare DeNS with and without force encoding on
MD17 in Section 4.3 and have similar observations. Comparing Index 2 and Index 4, we demonstrate that
predicting energy of original structures given corrupted ones can improve energy MAE by 5.6%while slightly
increasing force MAE by 1.2%. The increase in force MAE is because predicting energy given corrupted
structures is equivalent to using λE‰0in Equation 6 and implicitly decreases the relative importance of
force prediction. Since the decrease in energy MAE is greater than the increase in force MAE, we adopt the
practice of predicting energy given corrupted structures. Finally, the comparison between Index 2 and Index
5 shows that partially corrupted structures can further improve the performance gain of DeNS.
4.1.2 Main Results
We train EquiformerV2 ( 160M) with DeNS on OC20 S2EF-All+MD dataset. The model follows the same
configuration as EquiformerV2 ( 153M) trained without DeNS, and the additional parameters are due to force
encoding and one additional block of equivariant graph attention for noise prediction. We report results
in Table 2. All test results are computed via the EvalAI evaluation server1. EquiformerV2 trained with
DeNS achieves new state-of-the-art results on both S2EF and IS2RE tasks. On the S2EF validation split,
EquiformerV2 trained with DeNS improves energy MAE by 5meV and force MAE by 1.0meV/Å, which is
comparable to the gain brought by increasing the size of EquiformerV2 from 31M to 153M (energy MAE
improved by 5meV and force MAE improved by 1.3meV/Å). On the S2EF test split, the improvement in
energy and force predictions is smaller, which is probably because of different splits. On the IS2RE test split,
training EquiformerV2 with DeNS reduces MAE by 16meV, achieving similar performance gain of going
from eSCN (Passaro & Zitnick, 2023) to EquiformerV2 (Liao et al., 2023) (IS2RE MAE improved by 14meV)
and demonstrating the effectiveness of training with DeNS. We note that the improvement might not be as
significant as that on OC20 S2EF-2M (Section 4.1.1), OC22 (Section 4.2) and MD17 (Section 4.3) datasets
since the OC20 S2EF-All+MD training set contains much more structures along relaxation trajectories,
making new 3D geometries generated by DeNS less helpful. However, DeNS is still valuable because most
datasets are not as large as OC20 S2EF-All+MD dataset (about 172M structures in the training set) but
have sizes closer to OC20 S2EF-2M ( 2M structures), OC22 ( 8.2M structures), and MD17 ( 950structures)
datasets.
1eval.ai/web/challenges/challenge-page/712
9Published in Transactions on Machine Learning Research (12/2024)
Throughput S2EF validation S2EF test IS2RE test
Samples / Energy MAE Force MAE Energy MAE Force MAE Energy MAE
Model GPU sec. Ò(meV)Ó(meV/Å)Ó(meV)Ó(meV/Å)Ó(meV)Ó
GemNet-OC-L-E (Gasteiger et al., 2022) 7.5 239 22.1 230 21.0 -
GemNet-OC-L-F (Gasteiger et al., 2022) 3.2 252 20.0 241 19.0 -
GemNet-OC-L-F+E (Gasteiger et al., 2022) - - - - - 348
SCN L=6 K=16 (4-tap 2-band) (Zitnick et al., 2022) - - - 228 17.8 328
SCN L=8 K=20 (Zitnick et al., 2022) - - - 237 17.2 321
eSCN L=6 K=20 (Passaro & Zitnick, 2023) 2.9 243 17.1 228 15.6 323
EquiformerV2 ( λE“4,31M) (Liao et al., 2023) 7.1 232 16.3 228 15.5 315
EquiformerV2 ( λE“4,153M) (Liao et al., 2023) 1.8 227 15.0 219 14.2 309
EquiformerV2 + DeNS ( λE“4,160M) 1.8 222 14.0 214 13.3 293
Table 2: OC20 results on S2EF validation and test splits and IS2RE test split when trained on OC20 S2EF-All+MD
dataset. Throughput is reported as the number of structures processed per GPU-second during training and measured
on V100 GPUs.
S2EF-Total validation S2EF-Total test IS2RE-Total test
Number of Energy MAE (meV) ÓForce MAE (meV/Å) ÓEnergy MAE (meV) ÓForce MAE (meV/Å) ÓEnergy MAE (meV) Ó
Model parameters ID OOD ID OOD ID OOD ID OOD ID OOD
GemNet-OC (Gasteiger et al., 2022) 39M 545 1011 30 40 374 829 29.4 39.6 1329 1584
GemNet-OC (trained on OC20 + OC22) (Gasteiger et al., 2022) 39M 464 859 27 34 311 689 26.9 34.2 1200 1534
eSCN (Passaro & Zitnick, 2023) 200M - - - - 350 789 23.8 35.7 - -
EquiformerV2 ( λE“4,λF“100) (Liao et al., 2023) 122M 433 629 22.88 30.70 263.7 659.8 21.58 32.65 1119 1440
EquiformerV2 + DeNS ( λE“4,λF“100) 127M391.6 533.0 20.66 27.11 236.4 590.7 20.04 29.31 951 1282
Table 3: OC22 results on S2EF-Total validation and test splits and IS2RE-Total test split. Models are trained on the
OC22 S2EF-Total training split unless otherwise noted.
4.2 OC22 Dataset
Dataset and Tasks. The Open Catalyst 2022 (OC22) dataset (Tran* et al., 2022) focuses on oxide
electrocatalysis and consists of about 62k DFT relaxations obtained with Perdew-Burke-Ernzerhof (PBE)
functional calculations. One crucial difference between OC22 and OC20 is that the energy targets in OC22
are DFT total energies. DFT total energies are harder to predict but are the most general and closest to a
DFT surrogate, offering the flexibility to study property prediction beyond adsorption energies. Analogous to
the task definitions in OC20, the primary tasks in OC22 are S2EF-Total and IS2RE-Total. We train models
on the OC22 S2EF-Total dataset, which has 8.2M structures, and evaluate them on energy and force MAE
on the S2EF-total validation and test splits. Same as OC20, we use direct methods to predict forces here.
Then, we use these models to perform relaxations starting from initial structures in the IS2RE-Total test
split and evaluate the predicted relaxed energies on energy MAE.
Training Details. Please refer to Section C.1 for details on architectures, hyper-parameters and training
time.
Results. We use EquiformerV2 with energy coefficient λE“4and force coefficient λF“100to demonstrate
how DeNS can further improve state-of-the-art results and summarize the comparison in Table 3. Compared
to EquiformerV2 ( λE“4,λF“100) trained without DeNS, EquiformerV2 trained with DeNS consistently
achieves better energy and force MAE across all the S2EF-Total validation and test splits, with 9.6%to15.3%
improvement in energy MAE and 7.1%to11.7%improvement in force MAE. For IS2RE-Total, EquiformerV2
trained with DeNS achieves the best energy MAE results. The improvement in IS2RE-Total from training
with DeNS on only OC22 is greater than that of training on the much larger OC20 and OC22 datasets
reported in previous works. Specifically, training GemNet-OC on OC20 and OC22 datasets (about 134M
+8.2M structures) improves IS2RE-Total energy MAE ID by 129meV and OOD by 50meV compared to
training GemNet-OC on only OC22 dataset ( 8.2M structures). Compared to training without DeNS, training
EquiformerV2 with DeNS improves ID by 168meV and OOD by 158meV. Thus, training with DeNS clearly
improves data efficiency and performance on OC22.
10Published in Transactions on Machine Learning Research (12/2024)
Aspirin Benzene Ethanol Malonaldehyde Naphthalene Salicylic acid Toluene Uracil Training time Number of
Model energy forces energy forces energy forces energy forces energy forces energy forces energy forces energy forces (GPU-hours) Óparameters
SchNet (Schütt et al., 2017) 16.0 58.5 3.5 13.4 3.5 16.9 5.6 28.6 6.9 25.2 8.7 36.9 5.2 24.7 6.1 24.3 - -
DimeNet (Gasteiger et al., 2020) 8.8 21.6 3.4 8.1 2.8 10.0 4.5 16.6 5.3 9.3 5.8 16.2 4.4 9.4 5.0 13.1 - -
PaiNN (Schütt et al., 2021) 6.9 14.7 - - 2.7 9.7 3.9 13.8 5.0 3.3 4.9 8.5 4.1 4.1 4.5 6.0 - -
TorchMD-NET (Thölke & Fabritiis, 2022) 5.3 11.0 2.5 8.5 2.3 4.7 3.3 7.3 3.7 2.64.0 5.63.2 2.94.1 4.1 - -
NequIP (Lmax“3) (Batzner et al., 2022) 5.7 8.0 - - 2.2 3.1 3.3 5.6 4.9 1.7 4.6 3.9 4.0 2.0 4.5 3.3 - -
Equiformer ( Lmax“2) 5.3 7.2 2.2 6.62.2 3.1 3.3 5.8 3.7 2.1 4.5 4.1 3.8 2.1 4.3 3.3 17 3.50M
Equiformer ( Lmax“3) 5.3 6.6 2.5 8.1 2.2 2.93.2 5.4 4.4 2.0 4.3 3.9 3.7 2.1 4.3 3.4 59 5.50M
Equiformer ( Lmax“2) + DeNS 5.1 5.7 2.3 6.1 2.2 2.63.2 4.43.7 1.7 4.4 3.7 3.5 1.9 4.2 3.3 19 4.00M
Equiformer ( Lmax“3) + DeNS 5.0 5.2 2.36.1 2.2 2.4 3.2 4.1 3.7 1.6 4.23.2 3.41.8 4.1 2.9 61 6.35M
Table 4: Mean absolute error results on the MD17 testing set. Energy and force are in units of meV and meV/ Å. We
additionally report the time of training different Equiformer models averaged over all molecules and the numbers of
parameters to show that the proposed DeNS can improve performance with minimal overhead.
Method Aspirin Benzene Ethanol Malonaldehyde Naphthalene Salicylic acid Toluene Uracil
Index Attention Layer normalization DeNS energy forces energy forces energy forces energy forces energy forces energy forces energy forces energy forces
1 ✓ ✓ 5.3 7.2 2.2 6.62.2 3.1 3.3 5.8 3.7 2.1 4.5 4.1 3.8 2.1 4.3 3.3
2 ✓ ✓ ✓ 5.1 5.7 2.36.1 2.2 2.6 3.2 4.4 3.7 1.7 4.43.7 3.51.9 4.2 3.3
3 ✓ 5.2 7.7 2.4 6.2 2.3 3.9 3.3 6.2 3.8 2.2 4.1 4.73.3 2.44.2 4.4
4 ✓ ✓ 5.2 6.1 2.4 6.1 2.2 2.93.2 5.13.7 1.7 4.2 3.9 3.4 2.0 4.2 3.4
5 5.3 9.3 2.4 9.2 2.3 4.5 3.4 8.2 3.7 2.4 4.2 5.5 3.3 2.94.2 4.8
6 ✓ 5.2 7.3 2.4 8.1 2.2 3.4 3.4 6.7 3.7 1.9 4.2 4.4 3.4 2.2 4.2 3.8
Table 5: Mean absolute error results of variants of Equiformer ( Lmax“2) without attention and layer normalization
on the MD17 testing set. Energy and force are in units of meV and meV/ Å. Index 1 and Index 2 correspond to
“Equiformer ( Lmax“2)” and “Equiformer ( Lmax“2) + DeNS” in Table 4.
Method Aspirin Benzene Ethanol Malonaldehyde Naphthalene Salicylic acid Toluene Uracil
ForceλE‰0Partially corrupted
Index DeNS encoding in Eq. 6 structures energy forces energy forces energy forces energy forces energy forces energy forces energy forces energy forces
1 5.3 7.2 2.2 6.62.2 3.1 3.3 5.8 3.7 2.1 4.5 4.1 3.8 2.1 4.3 3.3
2 ✓ ✓ ✓ ✓ 5.1 5.7 2.36.1 2.2 2.6 3.2 4.4 3.7 1.7 4.4 3.73.5 1.9 4.2 3.3
3 ✓ ✓ ✓ 8.6 9.1 2.3 6.3 2.3 3.3 3.2 5.8 7.7 6.1 5.2 10.6 3.7 2.0 5.5 6.5
4 ✓ ✓ ✓ 5.1 5.8 2.3 6.1 2.32.6 3.2 4.53.7 1.7 4.83.6 3.71.9 4.2 3.3
5 ✓ ✓ ✓ 5.3 6.9 2.4 6.3 2.3 3.2 3.3 5.4 3.7 2.0 4.8 4.2 4.0 2.0 4.2 3.8
Table 6: Ablation study on the design choices of DeNS using MD17 dataset. Mean absolute error results are evaluated
on the testing set. Energy and force are in units of meV and meV/ Å. Index 1 and Index 2 correspond to “Equiformer
(Lmax“2)” and “Equiformer ( Lmax“2) + DeNS” in Table 4.
4.3 MD17 Dataset
Dataset. The MD17 dataset (Chmiela et al., 2017; Schütt et al., 2017; Chmiela et al., 2018) consists of
molecular dynamics simulations of small organic molecules. The task is to predict the energy and forces of
these non-equilibrium molecules. Following previous works, we adopt gradient methods for force prediction.
We use 950and50different configurations for training and validation sets and the rest for the testing set.
Training Details. Please refer to Section D.2 for additinoal implementation details of DeNS, hyper-
parameters and training time.
Main Results. We train Equiformer ( Lmax“2) (Liao & Smidt, 2023) and Equiformer ( Lmax“3)
with DeNS based on their official implementation, where Lmaxdenotes the maximum degree of equivariant
representations. As shown in Table 4, DeNS improves the results on all molecules. Along with the results on
OC20 and OC22 datasets, DeNS can generally improve the performance on force predictions with both direct
(i.e., OC20 and OC22) and gradient (i.e., MD17) methods. Particularly, Equiformer ( Lmax“2) trained
with DeNS acheives better results on all the tasks and requires 3.1ˆless training time than Equiformer
(Lmax“3) trained without DeNS. This demonstrates that for this small dataset, training an auxiliary task
and using data augmentation are more efficient and result in larger performance gain than increasing Lmax
from 2to3. Additionally, we find that the gains from training DeNS as an auxiliary task are comparable to
pre-training. For example, Zaidi et al. (2023) uses TorchMD-NET (Thölke & Fabritiis, 2022) pre-trained on
the PCQM4Mv2 dataset and reports results on Aspirin. Their improvement in force MAE is about 17.2%
(Table 3 in Zaidi et al. (2023)). Training Equiformer ( Lmax“2) with DeNS results in 20.8%improvement in
force MAE without relying on another dataset. Note that we only increase training time by 10.5%while their
method takes much more time since PCQM4Mv2 dataset is more than 3000ˆlarger than the training set of
11Published in Transactions on Machine Learning Research (12/2024)
MD17. Moreover, training with DeNS enables consistent improvement in all molecules when increasing Lmax
from 2to3, and Equiformer ( Lmax“3) trained with DeNS achieves overall best results. In contrast, when
DeNS is not used, increasing Lmaxfrom 2to3can lead to overfitting and worse results on some molecules
(i.e., Benzene and Uracil). Besides, we report simulation-based results in Section D.3.
Effect of DeNS on Different Network Architectures. We train variants of Equiformer ( Lmax“2) by
removing attention and layer normalization to investigate the performance gain of DeNS on different network
archtiectures. The results are summarized in Table 5, and DeNS improves all the model variants. We note
that Equiformer without attention and layer normalization reduces to SEGNN (Brandstetter et al., 2022)
but with a better radial basis function. Since the models cover many variants of equivariant networks, this
suggests that DeNS is general and can be helpful to many equivariant networks.
Ablation Study on the Design Choices of DeNS. We use Equiformer ( Lmax“2) to justify the design
choices of DeNS. The results are summarized in Table 6 and are similar to those on OC20 S2EF-2M dataset.
Comparing Index 2 and Index 3, force encoding consistently results in significant improvement in energy
and force MAE. Compared to training without denoising (Index 1), DeNS without force encoding (Index
3) only achieves slightly better results on some molecules (i.e., Benzene, Malondaldehyde, and Toluene)
and much worse results on others. For molecules on which DeNS without force encoding is helpful, adding
force encoding can achieve even better results. For others, force encoding is indispensable for DeNS to be
effective. Comparing Index 2 and Index 4, predicting energy given corrupted structures results in overall
better performance. Finally, the comparison between Index 2 and Index 5 demonstrates that partially
corrupted structures are necessary to achieve better results. For some molecules (i.e., Ethanol and Salicycli
acid), DeNS with noise added to all atoms (Index 5) can be worse than training without DeNS (Index 1).
5 Conclusion
In this paper, we propose to use denoisingnon-equilibrium structures (DeNS) as an auxiliary task to better
leverage training data and improve performance on original tasks of energy and force predictions. Denoising
non-equilibrium structures can be an ill-posed problem since there are many possible target structures. To
address the issue, we propose force encoding and take the forces of original structures as inputs to specify
which non-equilibrium structures we are denoising. With force encoding, DeNS successfully improves the
performance on original tasks when it is used as an auxiliary task. We conduct extensive experiments on
OC20, OC22 and MD17 datasets to demonstrate that DeNS can boost the performance on energy and force
predictions across datasets of various scales with minimal increase in training cost and is applicable to many
equivariant networks. Finally, we note that the proposed DeNS is general and can be directly applied to
other atomistic datasets containing non-equilibrium structures. Take Materials Project (Jain et al., 2013) for
example. Similar to OC20 and OC22 datasets, for each entry in the Materials Project database, the Materials
Project Trajectory (MPtrj) dataset (Deng et al., 2023) contains the corresponding relaxation trajectory
between initial and relaxed structures. Most structures in MPtrj are non-equilibrium and have energy and
force labels. Therefore, we can apply DeNS to MPtrj dataset in the same way as OC20 and OC22 datasets.
Acknowledgement
We thank Larry Zitnick, Xinlei Chen, Zachary Ulissi, Saro Passaro, Anuroop Sriram, and Brandon Wood
for helpful discussions. We acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing
Center (Reuther et al., 2018) for providing high performance computing and consultation resources that have
contributed to the research results reported within this paper.
Yi-Lun Liao and Tess Smidt were supported by DOE ICDI grant DE-SC0022215.
References
Rdkit: open-source cheminformatics. URL https://www.rdkit.org/ .
12Published in Transactions on Machine Learning Research (12/2024)
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arxiv preprint
arxiv:1607.06450 , 2016.
Ilyes Batatia, David Peter Kovacs, Gregor N. C. Simm, Christoph Ortner, and Gabor Csanyi. MACE: Higher
order equivariant message passing neural networks for fast and accurate force fields. In Advances in Neural
Information Processing Systems (NeurIPS) , 2022.
Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P. Mailoa, Mordechai Kornbluth, Nicola
Molinari, Tess E. Smidt, and Boris Kozinsky. E(3)-equivariant graph neural networks for data-efficient and
accurateinteratomicpotentials. Nature Communications , 13(1), May2022. doi: 10.1038/s41467-022-29939-5.
URL https://doi.org/10.1038/s41467-022-29939-5 .
Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J Bekkers, and Max Welling. Geometric and
physical quantities improve e(3) equivariant message passing. In International Conference on Learning
Representations (ICLR) , 2022. URL https://openreview.net/forum?id=_xwr8gOBeV1 .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, PranavShyam, GirishSastry, AmandaAskell, SandhiniAgarwal, ArielHerbert-Voss, Gretchen
Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher
Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot
learners. In Advances in Neural Information Processing Systems (NeurIPS) , 2020.
Lowik Chanussot*, Abhishek Das*, Siddharth Goyal*, Thibaut Lavril*, Muhammed Shuaibi*, Morgane
Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Aini Palizhati, Anuroop Sriram,
Brandon Wood, Junwoong Yoon, Devi Parikh, C. Lawrence Zitnick, and Zachary Ulissi. Open catalyst
2020 (oc20) dataset and community challenges. ACS Catalysis , 2021. doi: 10.1021/acscatal.0c04525.
Stefan Chmiela, Alexandre Tkatchenko, Huziel E. Sauceda, Igor Poltavsky, Kristof T. Schütt, and Klaus-
Robert Müller. Machine learning of accurate energy-conserving molecular force fields. Science Advances ,
3(5):e1603015, 2017. doi: 10.1126/sciadv.1603015. URL https://www.science.org/doi/abs/10.1126/
sciadv.1603015 .
Stefan Chmiela, Huziel E. Sauceda, Klaus-Robert Müller, and Alexandre Tkatchenko. Towards exact molecular
dynamics simulations with machine-learned force fields. Nature Communications , 9(1), sep 2018. doi:
10.1038/s41467-018-06169-2.
Taco S. Cohen, Mario Geiger, Jonas Köhler, and Max Welling. Spherical CNNs. In International Conference
on Learning Representations (ICLR) , 2018. URL https://openreview.net/forum?id=Hkbd5xZRb .
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas
Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer,
Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver,
Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran, Fisher Yu,
Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Patrick Collier, Alexey Gritsenko, Vighnesh Birodkar,
Cristina Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetić, Dustin Tran, Thomas
Kipf, Mario Lučić, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, and Neil Houlsby. Scaling vision
transformers to 22 billion parameters. arXiv preprint arXiv:2302.05442 , 2023.
Bowen Deng, Peichen Zhong, KyuJung Jun, Janosh Riebesell, Kevin Han, Christopher J. Bartel, and Gerbrand
Ceder. Chgnet as a pretrained universal neural network potential for charge-informed atomistic modelling.
Nature Machine Intelligence , 2023.
JacobDevlin, Ming-WeiChang, KentonLee, andKristinaToutanova. BERT:Pre-trainingofdeepbidirectional
transformers for language understanding. arxiv preprint arxiv:1810.04805 , 2019.
AlexeyDosovitskiy, LucasBeyer, AlexanderKolesnikov, DirkWeissenborn, XiaohuaZhai, ThomasUnterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
13Published in Transactions on Machine Learning Research (12/2024)
An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference
on Learning Representations (ICLR) , 2021. URL https://openreview.net/forum?id=YicbFdNTTy .
Alexandre Duval, Simon V. Mathis, Chaitanya K. Joshi, Victor Schmidt, Santiago Miret, Fragkiskos D.
Malliaros, Taco Cohen, Pietro Liò, Yoshua Bengio, and Michael Bronstein. A hitchhiker’s guide to geometric
gnns for 3d atomic systems. arXiv preprint arXiv:2312.07511 , 2024.
Alexandre Agm Duval, Victor Schmidt, Alex Hernández-García, Santiago Miret, Fragkiskos D. Malliaros,
Yoshua Bengio, and David Rolnick. FAENet: Frame averaging equivariant GNN for materials modeling. In
International Conference on Machine Learning (ICML) , 2023.
Rui Feng, Qi Zhu, Huan Tran, Binghong Chen, Aubrey Toland, Rampi Ramprasad, and Chao Zhang. May
the force be with you: Unified force-centric pre-training for 3d molecular conformations. arXiv preprint
arXiv:2308.14759 , 2023a.
Shikun Feng, Yuyan Ni, Yanyan Lan, Zhi-Ming Ma, and Wei-Ying Ma. Fractional denoising for 3D molecular
pre-training. In International Conference on Machine Learning (ICML) , 2023b.
Xiang Fu, Zhenghao Wu, Wujie Wang, Tian Xie, Sinan Keten, Rafael Gomez-Bombarelli, and Tommi
Jaakkola. Forces are not enough: Benchmark and critical evaluation for machine learning force fields with
molecular simulations. Transactions on Machine Learning Research (TMLR) , 2023.
Fabian Fuchs, Daniel E. Worrall, Volker Fischer, and Max Welling. Se(3)-transformers: 3d roto-translation
equivariant attention networks. In Advances in Neural Information Processing Systems (NeurIPS) , 2020.
Johannes Gasteiger, Janek Groß, and Stephan Günnemann. Directional message passing for molecular graphs.
InInternational Conference on Learning Representations (ICLR) , 2020.
Johannes Gasteiger, Muhammed Shuaibi, Anuroop Sriram, Stephan Günnemann, Zachary Ulissi, C Lawrence
Zitnick, and Abhishek Das. GemNet-OC: Developing Graph Neural Networks for Large and Diverse
Molecular Simulation Datasets. Transactions on Machine Learning Research (TMLR) , 2022.
Mario Geiger and Tess Smidt. e3nn: Euclidean neural networks. arXiv preprint arXiv:2207.09453 , 2022.
Mario Geiger, Tess Smidt, Alby M., Benjamin Kurt Miller, Wouter Boomsma, Bradley Dice, Kostiantyn
Lapchevskyi, Maurice Weiler, Michał Tyszkiewicz, Simon Batzner, Dylan Madisetti, Martin Uhrin, Jes
Frellsen, Nuri Jung, Sophia Sanborn, Mingjian Wen, Josh Rackers, Marcel Rød, and Michael Bailey.
e3nn/e3nn: 2022-04-13, April 2022. URL https://doi.org/10.5281/zenodo.6459381 .
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message
passing for quantum chemistry. In International Conference on Machine Learning (ICML) , 2017.
Jonathan Godwin, Michael Schaarschmidt, Alexander L Gaunt, Alvaro Sanchez-Gonzalez, Yulia Rubanova,
Petar Veličković, James Kirkpatrick, and Peter Battaglia. Simple GNN regularisation for 3d molecular
property prediction and beyond. In International Conference on Learning Representations (ICLR) , 2022.
URL https://openreview.net/forum?id=1wVvweK3oIb .
B. Hammer, L. B. Hansen, and J. K. Nørskov. Improved adsorption energetics within density-functional
theory using revised perdew-burke-ernzerhof functionals. Phys. Rev. B , 1999.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders
are scalable vision learners. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,
2022.
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic
depth. In European Conference on Computer Vision (ECCV) , 2016.
Anubhav Jain, Shyue Ping Ong, Geoffroy Hautier, Wei Chen, William Davidson Richards, Stephen T. Dacek,
Shreyas Cholia, Dan Gunter, David Skinner, Gerbrand Ceder, and Kristin Aslaug Persson. Commentary:
The materials project: A materials genome approach to accelerating materials innovation. APL Materials ,
2013.
14Published in Transactions on Machine Learning Research (12/2024)
Rui Jiao, Jiaqi Han, Wenbing Huang, Yu Rong, and Yang Liu. Energy-motivated equivariant pretraining for
3d molecular graphs. arXiv preprint arXiv:2207.08824 , 2022.
Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and Ron Dror. Learning
from protein structure with geometric vector perceptrons. In International Conference on Learning
Representations (ICLR) , 2021. URL https://openreview.net/forum?id=1YLJDvSx6J4 .
Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch–gordan nets: a fully fourier space spherical
convolutional neural network. In Advances in Neural Information Processing Systems 32 , pp. 10117–10126,
2018.
Janice Lan, Aini Palizhati, Muhammed Shuaibi, Brandon M Wood, Brook Wander, Abhishek Das, Matt
Uyttendaele, C Lawrence Zitnick, and Zachary W Ulissi. AdsorbML: Accelerating adsorption energy
calculations with machine learning. arXiv preprint arXiv:2211.16486 , 2022.
Tuan Le, Frank Noé, and Djork-Arné Clevert. Equivariant graph attention networks for molecular property
prediction. arXiv preprint arXiv:2202.09891 , 2022.
Yi-Lun Liao and Tess Smidt. Equiformer: Equivariant graph attention transformer for 3d atomistic graphs.
InInternational Conference on Learning Representations (ICLR) , 2023. URL https://openreview.net/
forum?id=KwmPfARgOTD .
Yi-Lun Liao, Brandon Wood, Abhishek Das*, and Tess Smidt*. EquiformerV2: Improved Equivariant
Transformer for Scaling to Higher-Degree Representations. arxiv preprint arxiv:2306.12059 , 2023.
Shengchao Liu, Hongyu Guo, and Jian Tang. Molecular geometry pretraining with SE(3)-invariant denoising
distance matching. In International Conference on Learning Representations (ICLR) , 2023. URL https:
//openreview.net/forum?id=CjTHVo1dvR .
Benjamin Kurt Miller, Mario Geiger, Tess E. Smidt, and Frank Noé. Relevance of rotationally equivariant
convolutions for predicting molecular properties. arxiv preprint arxiv:2008.08461 , 2020.
Albert Musaelian, Simon Batzner, Anders Johansson, Lixin Sun, Cameron J. Owen, Mordechai Kornbluth,
and Boris Kozinsky. Learning local equivariant representations for large-scale atomistic dynamics. arxiv
preprint arxiv:2204.05249 , 2022.
Albert Musaelian, Anders Johansson, Simon Batzner, and Boris Kozinsky. Scaling the leading accuracy of
deep equivariant models to biomolecular simulations of realistic size. arXiv preprint arXiv:2304.10061 ,
2023.
Maho Nakata and Tomomi Shimazaki. Pubchemqc project: A large-scale first-principles electronic structure
database for data-driven chemistry. Journal of chemical information and modeling , 57 6:1300–1308, 2017.
Saro Passaro and C Lawrence Zitnick. Reducing SO(3) Convolutions to SO(2) for Efficient Equivariant GNNs.
InInternational Conference on Machine Learning (ICML) , 2023.
Joshua A Rackers, Lucas Tecot, Mario Geiger, and Tess E Smidt. A recipe for cracking the quantum scaling
limit with machine learned electron densities. Machine Learning: Science and Technology , 4(1):015027, feb
2023. doi: 10.1088/2632-2153/acb314. URL https://dx.doi.org/10.1088/2632-2153/acb314 .
Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole von Lilienfeld. Quantum
chemistry structures and properties of 134 kilo molecules. Scientific Data , 1, 2014.
Albert Reuther, Jeremy Kepner, Chansup Byun, Siddharth Samsi, William Arcand, David Bestor, Bill
Bergeron, Vijay Gadepally, Michael Houle, Matthew Hubbell, Michael Jones, Anna Klein, Lauren Milechin,
Julia Mullen, Andrew Prout, Antonio Rosa, Charles Yee, and Peter Michaleas. Interactive supercomputing
on 40,000 cores for machine learning and data analysis. In 2018 IEEE High Performance extreme Computing
Conference (HPEC) , pp. 1–6. IEEE, 2018.
15Published in Transactions on Machine Learning Research (12/2024)
Lars Ruddigkeit, Ruud van Deursen, Lorenz C. Blum, and Jean-Louis Reymond. Enumeration of 166 billion
organic small molecules in the chemical universe database gdb-17. Journal of Chemical Information and
Modeling , 52(11):2864–2875, 2012. doi: 10.1021/ci300415d. URL https://doi.org/10.1021/ci300415d .
PMID: 23088335.
Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter W. Battaglia.
Learning to simulate complex physics with graph networks. In International Conference on Machine
Learning (ICML) , 2020.
Víctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks. In
International Conference on Machine Learning (ICML) , 2021.
K. T. Schütt, P.-J. Kindermans, H. E. Sauceda, S. Chmiela, A. Tkatchenko, and K.-R. Müller. Schnet: A
continuous-filter convolutional neural network for modeling quantum interactions. In Advances in Neural
Information Processing Systems (NeurIPS) , 2017.
Kristof T. Schütt, Farhad Arbabzadah, Stefan Chmiela, Klaus R. Müller, and Alexandre Tkatchenko.
Quantum-chemical insights from deep tensor neural networks. Nature Communications , 8(1), jan 2017. doi:
10.1038/ncomms13890.
Kristof T. Schütt, Oliver T. Unke, and Michael Gastegger. Equivariant message passing for the prediction of
tensorial properties and molecular spectra. In International Conference on Machine Learning (ICML) ,
2021.
Justin S. Smith, Olexandr Isayev, and Adrian E. Roitberg. Ani-1: A data set of 20m off-equilibrium dft
calculations for organic molecules. arXiv preprint arXiv:1708.04987 , 2017.
Justin S. Smith, Benjamin Tyler Nebgen, Nicholas Lubbers, Olexandr Isayev, and Adrian E. Roitberg. Less
is more: sampling chemical space with active learning. The Journal of chemical physics , 2018.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A
simple way to prevent neural networks from overfitting. Journal of Machine Learning Research , 15(56):
1929–1958, 2014. URL http://jmlr.org/papers/v15/srivastava14a.html .
Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung,
Dara Bahri, Tal Schuster, Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. UL2: Unifying
language learning paradigms. In International Conference on Learning Representations (ICLR) , 2023. URL
https://openreview.net/forum?id=6ruVLB727MC .
Philipp Thölke and Gianni De Fabritiis. Equivariant transformers for neural network based molecular
potentials. In International Conference on Learning Representations (ICLR) , 2022. URL https://
openreview.net/forum?id=zNHzqZ9wrRB .
Nathaniel Thomas, Tess E. Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley.
Tensor field networks: Rotation- and translation-equivariant neural networks for 3d point clouds. arxiv
preprint arXiv:1802.08219 , 2018.
Raphael J. L. Townshend, Brent Townshend, Stephan Eismann, and Ron O. Dror. Geometric prediction:
Moving beyond scalars. arXiv preprint arXiv:2006.14163 , 2020.
Richard Tran*, Janice Lan*, Muhammed Shuaibi*, Brandon Wood*, Siddharth Goyal*, Abhishek Das,
Javier Heras-Domingo, Adeesh Kolluru, Ammar Rizvi, Nima Shoghi, Anuroop Sriram, Zachary Ulissi, and
C. Lawrence Zitnick. The open catalyst 2022 (oc22) dataset and challenges for oxide electrocatalysis. arXiv
preprint arXiv:2206.08917 , 2022.
Oliver Thorsten Unke, Mihail Bogojeski, Michael Gastegger, Mario Geiger, Tess Smidt, and Klaus Robert
Muller. SE(3)-equivariant prediction of molecular wavefunctions and electronic densities. In A. Beygelzimer,
Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems
(NeurIPS) , 2021. URL https://openreview.net/forum?id=auGY2UQfhSu .
16Published in Transactions on Machine Learning Research (12/2024)
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems
(NeurIPS) , 2017.
Pascal Vincent, H. Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing
robust features with denoising autoencoders. In International Conference on Machine Learning (ICML) ,
2008.
Pascal Vincent, H. Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked
denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.
Journal of Machine Learning Research , 2010.
Yuyang Wang, Changwen Xu, Zijie Li, and Amir Barati Farimani. Denoise pretraining on nonequilibrium
molecules for accurate and transferable neural potentials. Journal of Chemical Theory and Computation ,
2023.
Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco Cohen. 3D Steerable CNNs:
Learning Rotationally Equivariant Features in Volumetric Data. In Advances in Neural Information
Processing Systems 32 , pp. 10402–10413, 2018.
Sheheryar Zaidi, Michael Schaarschmidt, James Martens, Hyunjik Kim, Yee Whye Teh, Alvaro Sanchez-
Gonzalez, Peter Battaglia, Razvan Pascanu, and Jonathan Godwin. Pre-training via denoising for molecular
property prediction. In International Conference on Learning Representations (ICLR) , 2023.
Linfeng Zhang, Jiequn Han, Han Wang, Roberto Car, and Weinan E. Deep potential molecular dynamics: A
scalable model with the accuracy of quantum mechanics. Phys. Rev. Lett. , 120:143001, Apr 2018. doi: 10.
1103/PhysRevLett.120.143001. URL https://link.aps.org/doi/10.1103/PhysRevLett.120.143001 .
Larry Zitnick, Abhishek Das, Adeesh Kolluru, Janice Lan, Muhammed Shuaibi, Anuroop Sriram, Zachary
Ulissi, and Brandon Wood. Spherical channels for modeling atomic interactions. In Advances in Neural
Information Processing Systems (NeurIPS) , 2022.
17Published in Transactions on Machine Learning Research (12/2024)
Appendix
A Related Works
A.1 Comparison to previous works on denoising
A.2SEp3q{Ep3q-equivariant networks
A.3 Equiformer series
B Details of experiments on OC20
B.1 Training details
C Details of experiments on OC22
C.1 Training details
D Details of experiments on MD17
D.1 Additional details of DeNS
D.2 Training details
D.3 Additional simulation-based results
E Pseudocode for force encoding
F Pseudocode for training with DeNS
G Visualization of corrupted structures
A Related Works
A.1 Comparison to Previous Works on Denoising
We discuss previous works on denoising (Godwin et al., 2022; Zaidi et al., 2023; Feng et al., 2023b; Wang
et al., 2023) in chronological order and compare them with this work as below.
Godwin et al. (2022) first proposes the idea of adding noise to 3D coordinates and then using denoising as an
auxiliary task. The auxiliary task is trained along with the original task without relying on another large
dataset. Their approach requires known equilibrium structures and therefore is limited to QM9 (Ramakrishnan
et al., 2014; Ruddigkeit et al., 2012) and OC20 IS2RE datasets and can not be applied to force prediction
such as OC20 S2EF dataset. For QM9, all the structures are at equilibrium, and for OC20 IS2RE, the
target of denoising is the relaxed, equilibrium structure. Denoising without force encoding is well-defined on
both QM9 and OC20 IS2RE datasets. In contrast, this work proposes using force encoding to generalize
their approach to non-equilibrium structures, which have much larger datasets than equilibrium ones. Force
encoding can achieve better results on OC20 S2EF-2M dataset with little overhead (Index 2 and Index 3 in
Table 1(b)) and is indispensable on MD17 dataset (Section 4.3).
Zaidi et al. (2023) adopts the denoising approach proposed by Godwin et al. (2022) as a pre-training method
and therefore requires another large dataset containing unlabelled equilibrium structures for pre-training. On
the other hand, Godwin et al. (2022) and this work use denoising along with the original task and do not use
any additional unlabeled data.
Feng et al. (2023b) follows the same practice of pre-training via denoising (Zaidi et al., 2023) and proposes a
different manner of adding noise. Specifically, they separate noise into dihedral angle noise and coordinate
noise and only learn to predict coordinate noise. However, adding noise to dihedral angles requires tools like
RDKit (rdk) to obtain rotatable bonds and cannot be applied to other datasets like OC20 and OC22.
Although Zaidi et al. (2023) and Feng et al. (2023b) report results of force prediction on MD17 dataset, they
first pre-train models on PCQM4Mv2 dataset (Nakata & Shimazaki, 2017) and then fine-tune the pre-trained
models on MD17 dataset. We note that their setting is different from ours since we do not use any dataset
for pre-training. As for fine-tuning on MD17 dataset, Zaidi et al. (2023) simply follows the same practice
18Published in Transactions on Machine Learning Research (12/2024)
in standard supervised training. Feng et al. (2023b) explores fine-tuning with objectives similar to Noisy
Nodes (Godwin et al., 2022), but the performance gain is much smaller than ours. Concretely, in Table 5 in
Feng et al. (2023b), the improvement in force prediction on Aspirin is about 2.6%while we improve force
MAE by 20.8%.
Wang et al. (2023) uses the same pre-training method as Zaidi et al. (2023) but applies it to ANI-1 (Smith
et al., 2017) and ANI-1x (Smith et al., 2018) datasets, which contain non-equilibrium structures. However,
Wang et al. (2023) does not encode forces, and we show in Section 4.3 that denoising non-equilibrium
structures without force encoding can sometimes lead to worse results compared to training without denoising.
A.2 SE(3)/E(3) -Equivariant Networks
We first discuss the concept of equivariance and how equivariant networks achieve equivariance and then
compare previous works below. We note that most of the content is adapted from Equiformer (Liao & Smidt,
2023) and EquiformerV2 (Liao et al., 2023). We refer readers to the works (Liao & Smidt, 2023; Liao et al.,
2023) for more detailed background on equivariant networks and the work (Duval et al., 2024) for a broader
review on geometric GNNs for modeling 3D atomistic systems.
3D atomistic systems are often described in 3D coordinate systems. We have the freedom to choose arbitrary
3D coordinate systems since we can change between different coordinates via the symmetries of 3D space.
The relevant 3D symmetris are rotation, translation and inversion. The Euclidean group Ep3qconsists of 3D
rotation, translation and inversion while the special Euclidean group SEp3qis comprised of 3D rotation and
translation. The laws of physics remain the same regardless of the coordinate we use, and thus the properties of
3D atomistic systems are equivariant to 3D symmetries. For instance, when a 3D atomistic system is rotated,
quantities like energy will remain identical while others like forces will rotate accordingly. Mathematically,
a function fmapping between vector spaces XandYis equivariant to a group of transformations Gif
for any input xPX, outputyPYand group element gPG, we havefpDXpgqxq“DYpgqfpxq“DYpgqy,
whereDXpgqandDYpgqare transformation matrices or group representations parametrized by ginXand
Y. Additionally, fis invariant when DYpgqis an identity matrix for any gPG.
Incorporating equivariance into neural networks as inductive biases can improve data efficiency and generaliz-
ability. Equivariant neural networks (Thomas et al., 2018; Kondor et al., 2018; Weiler et al., 2018; Fuchs
et al., 2020; Miller et al., 2020; Townshend et al., 2020; Batzner et al., 2022; Jing et al., 2021; Schütt et al.,
2021; Satorras et al., 2021; Brandstetter et al., 2022; Thölke & Fabritiis, 2022; Le et al., 2022; Musaelian
et al., 2022; Batatia et al., 2022; Liao & Smidt, 2023; Passaro & Zitnick, 2023; Liao et al., 2023) achieve
equivariance to 3D rotation and optionally inversion by using vector spaces of irreducible representations
(irreps) as equivariant features. The vector spaces of irreps are p2L`1q-dimensional, with degree Lbeing a
non-negative integer. Lcan be viewed as the angular frequency of vectors and determines how fast vectors
change with respect to a rotation of the coordinate system. Vectors of degree Lare referred to as type- L
vectors. They are transformed with Wigner-D matrices DpLqwhen rotating the coordinate system, and DpLq
of different Lacts on independent vector spaces. Euclidean vectors ⃗ rinR3such as relative positions and
forces can be projected into type- Lvectors by using spherical harmonics YpLqp⃗ r
||⃗ r||q. We concatenate multiple
type-Lvectors to build an equivariant feature f. Given the maximum degree Lmaxof equivariant features,
fhasCLtype-Lvectors, where 0ďLďLmaxandCLis the number of channels for type- Lvectors. Both
LmaxandCLare architectural hyper-parameters of equivariant networks. Equivariant operations are applied
to equivariant features to preserve equivariance, and two examples are tensor products and SOp3qlinear
operations. Tensor products are the fundamental operation in equivariant networks for interacting vectors of
differentLand are used to build convolutions and attention. On the other hand, SOp3qlinear operations
apply separate linear operations to each group of type- Lvectors. Relevant to the proposed method, we
encode forces into equivariant features by first projecting forces to type- Lvectors with spherical harmonics
and then expanding the number of channels to CLwith anSOp3qlinear operation.
Previous works on equivariant networks mainly differ in which equivariant operations are used and the
combination of those operations. TFN (Thomas et al., 2018) and NequIP (Batzner et al., 2022) use
tensor products for equivariant graph convolution with linear messages, with the latter utilizing extra gate
activation (Weiler et al., 2018). SEGNN (Brandstetter et al., 2022) applies gate activation to messages passing
19Published in Transactions on Machine Learning Research (12/2024)
for non-linear messages (Gilmer et al., 2017; Sanchez-Gonzalez et al., 2020). SE(3)-Transformer (Fuchs
et al., 2020) adopts equivariant dot product attention with linear messages. Equiformer (Liao & Smidt,
2023) improves upon previous models by combining MLP attention and non-linear messages and additionally
introducing equivariant layer normalization and regularizations like dropout (Srivastava et al., 2014) and
stochastic depth (Huang et al., 2016). However, these networks rely on compute-intensive SOp3qconvolutions
built from tensor products, and therefore they can only use small values for maximum degrees Lmaxof irreps
features. eSCN (Passaro & Zitnick, 2023) significantly reduces the complexity of SOp3qconvolutions by first
rotating irreps features based on relative positions and then applying SOp2qlinear layers, enabling higher
values ofLmax. EquiformerV2 (Liao et al., 2023) adopts eSCN convolutions and proposes an improved version
of Equiformer to better leverage the power of higher Lmax, achieving the current state-of-the-art results on
OC20 (Chanussot* et al., 2021) and OC22 (Tran* et al., 2022) datasets.
A.3 Equiformer Series
Equiformer (Liao & Smidt, 2023) is an SEp3q/Ep3q-equivariant graph neural network that combines the
inductive biases of 3D-related equivariance with the strength of Transformers (Vaswani et al., 2017). Starting
from Transformers, Equiformer introduces three architectural modifications. First, Equiformer adopts
equivariant features built from vector spaces of irreps as internal representations to incorporate equivariance.
Second, equivariant operations are applied to the equivariant features. These operations include tensor
products and the equivariant counterparts of the original operations in Transformers. The latter part consists
of equivariant linear operations (i.e., SOp3qlinear operations), equivariant layer normalization (Ba et al.,
2016) and gate activation (Weiler et al., 2018). Third, Equiformer proposes to apply non-linear functions
to both attention weights and message passing, which improves the expressivity of attention in standard
Transformers.
Although Equiformer demonstrates that Transformers generalize well to 3D atomistic systems, it is limited to
small values of maximum degree Lmaxbecause of the compute-intensive tensor product operations. Higher
degrees can better capture angular resolutions and directional information, and therefore lower Lmaxcan
limit the expressivity of Equiformer. To address this limitation, EquiformerV2 (Liao et al., 2023) adopts
eSCN (Passaro & Zitnick, 2023) convolutions, which significantly reduce the computational complexity of
tensor products, to incorporate higher-degree equivariant representations and proposes three architectural
improvements to better leverage the power of higher degrees. First, attention re-normalization is proposed
to introduce one additional layer normalization to the non-linear functions of attention weights. This
helps stabilize attention and improves empirical performance. Second, separable S2activation based on S2
activation (Cohen et al., 2018) is proposed to better mix the information of all degrees and stabilize training.
Third, separable layer normalization is proposed to replace the original equivariant layer normalization in
order to preserve the relative importance of different degrees.
B Details of Experiments on OC20
B.1 Training Details
Since each structure in OC20 S2EF dataset has a pre-defined set of fixed and free atoms and we only predict
forces of free atoms, we only apply DeNS to free atoms. When partially corrupted structures are used, we add
noise to and denoise a random subset of free atoms. When training EquiformerV2 on OC20 S2EF-All+MD
dataset, we only apply DeNS to structures from the All split.
For force prediction, we adopt direct methods following previous works for a fair comparison. We add an
additional block of equivariant graph attention to EquiformerV2 for noise prediction. We mainly follow the
hyper-parameters of training EquiformerV2 without DeNS on OC20 S2EF-2M and S2EF-All+MD datasets.
For training EquiformerV2 on OC20 S2EF-All+MD dataset, we increase the number of epochs from 1to
2for better performance. This results in higher training time than other methods. However, we note that
we already demonstrate training with DeNS can achieve better results given the same amount of training
time in Table 1(a). Table 7 summarizes the hyper-parameters of training EquiformerV2 with DeNS for the
ablation studies on OC20 S2EF-2M dataset in Section 4.1.1 and for the main results on OC20 S2EF-All+MD
20Published in Transactions on Machine Learning Research (12/2024)
EquiformerV2 ( 89M) on EquiformerV2 ( 160M) on
Hyper-parameters OC20 S2EF-2M dataset OC20 S2EF-All+MD dataset
Optimizer AdamW AdamW
Learning rate scheduling Cosine learning rate with linear warmup Cosine learning rate with linear warmup
Warmup epochs 0.1 0 .01
Maximum learning rate 2ˆ10´4for12epochs 4ˆ10´4
4ˆ10´4for20,30epochs
Batch size 64for12epochs 512
128for20,30epochs
Number of epochs 12,20,30 2
Weight decay 1ˆ10´31ˆ10´3
Dropout rate 0.1 0 .1
Stochastic depth 0.05 0 .1
Energy coefficient λE 2 4
Force coefficient λF 100 100
Gradient clipping norm threshold 100 100
Model EMA decay 0.999 0 .999
Cutoff radius (Å) 12 12
Maximum number of neighbors 20 20
Number of radial bases 600 600
Dimension of hidden scalar features in radial functions dedgep0,128q p 0,128q
Maximum degree Lmax 6 6
Maximum order Mmax 2 3
Number of Transformer blocks 12 20
Embedding dimension dembed p6,128q p 6,128q
fpLq
ijdimensiondattn_hidden p6,64q p 6,64q
Number of attention heads h 8 8
fp0q
ijdimensiondattn_alpha p0,64q p 0,64q
Value dimension dattn_value p6,16q p 6,16q
Hidden dimension in feed forward networks dffn p6,128q p 6,128q
Resolution of point samples R 18 18
Probability of optimizing DeNS pDeNS 0.5 0 .125
DeNS coefficient λDeNS 10 15
Standard deviation of Gaussian noise σ 0.1for12epochs 0.1
0.15for20,30epochs
Corruption ratio rDeNS 0.5 0 .25
Table 7: Hyper-parameters of training EquiformerV2 with DeNS on OC20 S2EF-2M dataset and OC20 S2EF-All+MD
dataset.
dataset in Section 4.1.2. Please refer to the work of EquiformerV2 (Liao et al., 2023) for details of the
architecture. For training eSCN with DeNS, we use the same DeNS-related hyper-parameters as those for
training EquiformerV2 for 20epochs and the same module as force prediction to predict noise.
V100 GPUs with 32GB are used to train models. We use 16GPUs for training EquiformerV2 for 12
epochs and eSCN on OC20 S2EF-2M dataset and use 32GPUs for training EquiformerV2 for 20 and 30
epochs. We train EquiformerV2 with 128GPUs on OC20 S2EF-All+MD dataset. The training time and the
numbers of parameters of different models on OC20 S2EF-2M dataset can be found in Table 1(a). For OC20
S2EF-All+MD dataset, the training time is 88982GPU-hours and the number of parameters is 160M.
C Details of Experiments on OC22
C.1 Training Details
Different from OC20, all the atoms in a structure in OC22 are free, and we apply DeNS to all the free
atoms. We add an additional block of equivariant graph attention to EquiformerV2 for noise prediction.
We follow the same practice as on OC20 and use direct methods for force prediction. We follow the same
hyper-parameters not relevant to DeNS, and Table 8 summarizes the hyper-parameters for the results on
OC22 in Table 3. We use 32V100 GPUs (32GB) for training. The training time is 5082GPU-hours, and the
number of parameters is 127M.
21Published in Transactions on Machine Learning Research (12/2024)
Hyper-parameters Value or description
Optimizer AdamW
Learning rate scheduling Cosine learning rate with linear warmup
Warmup epochs 0.1
Maximum learning rate 2ˆ10´4
Batch size 128
Number of epochs 6
Weight decay 1ˆ10´3
Dropout rate 0.1
Stochastic depth 0.1
Energy coefficient λE 4
Force coefficient λF 100
Gradient clipping norm threshold 50
Model EMA decay 0.999
Cutoff radius (Å) 12
Maximum number of neighbors 20
Number of radial bases 600
Dimension of hidden scalar features in radial functions dedgep0,128q
Maximum degree Lmax 6
Maximum order Mmax 2
Number of Transformer blocks 18
Embedding dimension dembed p6,128q
fpLq
ijdimensiondattn_hidden p6,64q
Number of attention heads h 8
fp0q
ijdimensiondattn_alpha p0,64q
Value dimension dattn_value p6,16q
Hidden dimension in feed forward networks dffn p6,128q
Resolution of point samples R 18
Probability of optimizing DeNS pDeNS 0.5
DeNS coefficient λDeNS 25
Standard deviation of Gaussian noise σ 0.15
Corruption ratio rDeNS 0.5
Table 8: Hyper-parameters for OC22 dataset.
D Details of Experiments on MD17
D.1 Additional Details of DeNS
It is necessary that gradients consider both the original task and DeNS when updating learnable parameters,
and this affects how we sample structures for DeNS when only a single GPU is used for training models
on the MD17 dataset. We zero out forces corresponding to structures used for the original task so that
a single forward-backward propagation can consider both DeNS and the original task. In contrast, if we
switch between DeNS and the original task for different iterations, gradients only consider either DeNS or the
original task, and we find that this does not result in better performance on the MD17 dataset than training
without DeNS.
D.2 Training Details
We use the official implementation of Equiformer (Liao & Smidt, 2023) for experiments on the MD17 dataset
and follow most of the original hyper-parameters for training with DeNS. Gradient methods are used to
predict forces. For training DeNS, we use an additional block of equivariant graph attention for noise
prediction, which slightly increases training time and the number of parameters. The hyper-parameters
introduced by training DeNS and the values of energy coefficient λEand force coefficient λFon different
molecules can be found in Table 9. Empirically, we find that linearly decaying DeNS coefficient λDeNSto
0thoughout the training can result in better performance. For the Equiformer variant without attention
and layer normalization, we find that using normal distributions to initialize weights can result in training
divergence and therefore we use uniform distributions. For some molecules, we find training Equiformer
variant without attention and layer normalization with DeNS is unstable and therefore reduce the learning
rate to 3ˆ10´4.
We use one A5000 GPU with 24GB to train different models for each molecule. The training time and the
numbers of parameters can be found in Table 4.
22Published in Transactions on Machine Learning Research (12/2024)
Hyper-parameter Aspirin Benzene Ethanol Malonaldehyde Naphthalene Salicylic acid Toluene Uracil
Energy coefficient λE 1 1 1 1 2 1 1 1
Force coefficient λF 80 80 80 100 20 80 80 20
Probability of optimizing DeNS pDeNS 0.25 0.25 0.25 0.25 0.25 0.25 0.125 0.25
DeNS coefficient λDeNS 5 5 5 5 5 5 5 5
Standard deviation of Gaussian noises σ0.05 0.05 0.05 0.05 0.05 0.05 0.025 0.05
Corruption ratio rDeNS 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25
Table 9: Hyper-parameters of training Equiformer ( Lmax“2) and Equiformer ( Lmax“3) with DeNS on the MD17
dataset. Other hyper-parameters not listed here are the same as the original Equiformer trained without DeNS.
Aspirin Ethanol Naphthalene Salicylic acid
Model energy ÓforcesÓstabilityÒhprqÓenergyÓforcesÓstabilityÒhprqÓenergyÓforcesÓstabilityÒhprqÓenergyÓforcesÓstabilityÒhprqÓ
Equiformer ( Lmax“2) 5.3 7.2 300 0.02 2.2 3.1 289.9 0.09 3.7 2.1 133.8 0.12 4.5 4.1 300 0.03
Equiformer ( Lmax“2) + DeNS 5.1 5.7 300 0.02 2.2 2.6 300 0.09 3.7 1.7 157.2 0.12 4.4 3.7 300 0.03
Table 10: Simulation-based results on MD17 dataset. We report simulation-based metrics, stability and distribution
of interatomic distances hprq. Models are the same as in Table 4. Energy and force are in units of meV and meV/ Å
and are evaluated on the testing set.
D.3 Additional Simulation-Based Results
Following the work (Fu et al., 2023), we run simulations on the four molecules (i.e., Aspirin, Ethanol,
Naphthalene and Salicylic Acid) and compare the two simulation-based metrics, which are stability and
distribution of interatomic distances hprq. We use the previously trained Equiformer for the simulations. We
note that Equiformer models are trained on 950examples for each molecule instead of 9,500as in Fu et al.
(2023). The results are summarized in Table 10. Training with DeNS helps energy and forces MAE as well
as stability. The results of hprqare similar. For Naphthalene, training with DeNS improves stability from
133.8{300to157.2{300. Both models become unstable when running simulations of Naphthalene, and we
surmise that is because we only use 950examples for training instead of 9,500. For others, the performance
gain in simulation-based metrics is not significant since the original Equiformer trained on 950examples
already achieves similar results to other top-performing models trained on 9,500examples, suggesting that
the potential room for improvement would be quite limited.
E Pseudocode for Force Encoding
We provide the pseudocode for force encoding in Algorithm 1. Here the original node embedding xicontains
only the atom embedding xi,z. Note that the type- Lvectors inxi,zare all zeros for Lą0sincexi,zis
obtained by applying an SOp3qlinear layer to type- 0vectors. We directly add the force embedding xi,fto
the original node embedding to encode forces.
Algorithm 1 Force Encoding
1:xi,zÐSO3_Linearpone_hotpziqq Ź Extend Equation 5 to all degrees Land apply to the one-hot
encoding of atomic numbers zifor each atom
2:xiÐxi,z
3:xi,fÐSO3_Linear´
||fi||¨Y´
fi
||fi||¯¯
4:xiÐxi`xi,f
F Pseudocode for Training with DeNS
We provide the pseudocode for training with DeNS in Algorithm 2 and note that Line 5 can be parallelized.
For denoising partially corrupted structures discussed in Section 3.2.3, we only add noise to a random subset
of atoms (Line 11 – 14) and predict the corresponding noise (Line 25).
23Published in Transactions on Machine Learning Research (12/2024)
Algorithm 2 Training with DeNS
1:Input:
pDeNS: probability of optimizing DeNS
λDeNS: DeNS coefficient
σ: standard deviation of Gaussian noise
rDeNS: corruption ratio
λE: energy coefficient
λF: force coefficient
GNN: graph neural network for predicting energy, forces and noise
2:whiletrainingdo
3:Ltotal“0
4:Sample a batch of BstructurestpSnon-eqqj|jPt1,...,Buufrom the training set
5:forj“1toBdo ŹThis for loop can be parallelized
6: LetpSnon-eqqj“!
pzi,piq|iPt1,...,|pSnon-eqqj|u)
7: Samplepfrom a uniform distribution Up0,1qto determine whether to optimize DeNS
8:ifpăpDeNSthen ŹOptimize DeNS based on Equation 6
9: fori“1to|pSnon-eqqj|do
10: qi„Up0,1q
11: ifqiărDeNSthen ŹAdd noise to and denoise the atom
12: ϵi„Np0,σI3q
13: ˜pi“pi`ϵi
14: mi“1 ŹDenoise the atom when calculating LDeNS
15: else
16: ˜pi“pi
17: mi“0
18: end if
19: ˜f1
i“f1
i¨mi ŹEncode the atomic force if the atom is corrupted
20: end for
21: Letp˜Snon-eqqj“!
pzi,˜piq|iPt1,...,|pSnon-eqqj|u)
22: Let˜F`
pSnon-eqqj˘
“!
˜f1
i|iPt1,...,|pSnon-eqqj|u)
23: ˆE,ˆF,ˆϵÐGNN`
p˜Snon-eqqj,˜F`
pSnon-eqqj˘˘
24: LE“ˇˇˇE1`
pSnon-eqqj˘
´ˆEˇˇˇ ŹPredict energy of the original structure
25: LDeNS“1
|pSnon-eqqj|ř|pSnon-eqqj|
i“1mi¨ˇˇϵi
σ´ˆϵiˇˇ2ŹPredict noise of corrupted atoms
26: LF“1
|pSnon-eqqj|ř|pSnon-eqqj|
i“1p1´miq¨|f1
i`
pSnon-eqqj˘
´ˆfi|2ŹPredict forces of uncorrupted
atoms
27: Ltotal“Ltotal`λE¨LE`λDeNS¨LDeNS`λF¨LF
28: else ŹOptimize the original task based on Equation 1
29: ˆE,ˆF,_ÐGNN`
pSnon-eqqj˘
30: LE“ˇˇˇE1`
pSnon-eqqj˘
´ˆEˇˇˇ
31: LF“1
|pSnon-eqqj|ř|pSnon-eqqj|
i“1|f1
i`
pSnon-eqqj˘
´ˆfi|2
32: Ltotal“Ltotal`λE¨LE`λF¨LF
33: end if
34:end for
35:Ltotal“Ltotal
B
36:Optimize GNN based on Ltotal
37:end while
24Published in Transactions on Machine Learning Research (12/2024)
G Visualization of Corrupted Structures
We visualize how adding noise of different scales affects structures in OC20, OC22 and MD17 datasets in
Figure 3, Figure 4 and Figure 5, respectively.
25Published in Transactions on Machine Learning Research (12/2024)
Figure 3: Visualization of corrupted structures in OC20 dataset. We add noise of different scales
to original structures (column 1). For each row, we sample ϵi„Np0,I3q, multiply ϵiwithσ“
0.1(column 2),0.3(column 3) and 0.5(column 4) , and add the scaled noise to the original structures. For columns 2,
3 and 4, the lighter colors denote the atomic positions of the original structures. Here we add noise to all the atoms in
a structure for better visual effects.
26Published in Transactions on Machine Learning Research (12/2024)
Figure 4: Visualization of corrupted structures in OC22 dataset. We add noise of different scales
to original structures (column 1). For each row, we sample ϵi„Np0,I3q, multiply ϵiwithσ“
0.1(column 2),0.3(column 3) and 0.5(column 4) , and add the scaled noise to the original structures. For columns 2,
3 and 4, the lighter colors denote the atomic positions of the original structures. Here we add noise to all the atoms in
a structure for better visual effects.
27Published in Transactions on Machine Learning Research (12/2024)
Figure 5: Visualization of corrupted structures in MD17 dataset. We add noise of different scales
to original structures (column 1). For each row, we sample ϵi„Np0,I3q, multiply ϵiwithσ“
0.01(column 2),0.03(column 3),0.05(column 4) and 0.07(column 5) , and add the scaled noise to the original struc-
tures. For columns 2, 3, 4 and 5, the lighter colors denote the atomic positions of the original structures. Here we add
noise to all the atoms in a structure for better visual effects.
28