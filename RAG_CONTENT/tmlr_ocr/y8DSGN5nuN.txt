Under review as submission to TMLR
FlexEControl: Flexible and Efficient Multimodal Control
for Text-to-Image Generation
Anonymous authors
Paper under double-blind review
Abstract
Controllable text-to-image (T2I) diffusion models generate images conditioned on both text
prompts and semantic inputs of other modalities like edge maps. Nevertheless, current
controllable T2I methods commonly face challenges related to efficiency and faithfulness,
especially when conditioning on multiple inputs from either the same or diverse modalities. In
this paper, we propose a novel FlexibleandEfficient method, FlexEControl, for controllable
T2I generation. At the core of FlexEControl is a unique weight decomposition strategy, which
allows for streamlined integration of various input types. This approach not only enhances
the faithfulness of the generated image to the control, but also significantly reduces the
computational overhead typically associated with multimodal conditioning. Our approach
achieves a reduction of 41% in trainable parameters and 30% in memory usage compared
with Uni-ControlNet. Moreover, it doubles data efficiency and can flexibly generate images
under the guidance of multiple input conditions of various modalities.
1 Introduction
In the realm of text-to-image (T2I) generation, diffusion models exhibit exceptional performance in trans-
forming textual descriptions into visually accurate images. Such models exhibit extraordinary potential
across a plethora of applications, spanning from content creation (Rombach et al., 2022; Saharia et al., 2022b;
Nichol et al., 2021; Ramesh et al., 2021a; Yu et al., 2022; Avrahami et al., 2023; Chang et al., 2023), image
editing (Balaji et al., 2022; Kawar et al., 2023; Couairon et al., 2022; Zhang et al., 2023; Valevski et al., 2022;
Nichol et al., 2021; Hertz et al., 2022; Brooks et al., 2023; Mokady et al., 2023), and also fashion design (Cao
et al., 2023). We propose a new unified method that can tackle two problems in text-to-image generation:
improve the training efficiency of T2I models concerning memory usage, computational requirements, and a
thirst for extensive datasets (Saharia et al., 2022a; Rombach et al., 2022; Ramesh et al., 2021b); and improve
their controllability especially when dealing with multimodal conditioning, e.g. multiple edge maps and at
the same time follow the guidance of text prompts, as shown in Figure 1 (c).
Controllable text-to-image generation models (Mou et al., 2023) often come at a significant training com-
putational cost, with linear growth in cost and size when training with different conditions. Our approach
can improve the training efficiency of existing text-to-image diffusion models and unify and flexibly handle
different structural input conditions all together. We take cues from the efficient parameterization strategies
prevalent in the NLP domain (Pham et al., 2018; Hu et al., 2021; Zaken et al., 2021; Houlsby et al., 2019) and
computer vision literature (He et al., 2022). The key idea is to learn shared decomposed weights for varied
input conditions, ensuring their intrinsic characteristics are conserved. Our method has several benefits:
It not only achieves greater compactness (Rombach et al., 2022), but also retains the full representation
capacity to handle various input conditions of various modalities; Sharing weights across different conditions
contributes to the data efficiency; The streamlined parameter space aids in mitigating overfitting to singular
conditions, thereby reinforcing the flexible control aspect of our model.
Meanwhile, generating images from multiple homogeneous conditional inputs, especially when they present
conflicting conditions or need to align with specific text prompts, is challenging. To further augment our
model’s capability to handle multiple inputs from either the same or diverse modalities as shown in Figure 1,
1Under review as submission to TMLR
(c) Controllable T2I w .
Same Input Conditions
(b) Controllable T2I w .
Different Input ConditionsText Pr ompt:  Stormtr ooper's lectur e at the football field(a) Ef ficiency Comparisons 
Figure 1: (a) FlexEControl excels in training efficiency, achieving superior performance with just half the
training data compared to its counterparts on (b) Controllable Text-to-Image Generation w. Different Input
Conditions (one edge map and one segmentation map). (c) FlexEControl effectively conditions on two canny
edge maps. The text prompt is Stormtrooper’s lecture at the football field in both Figure (b) and
Figure (c).
during training, we introduce a new training strategy with two new loss functions introduced to strengthen
the guidance of corresponding conditions. This approach, combined with our compact parameter optimization
space, empowers the model to learn and manage multiple controls efficiently, even within the same category
(e.g., handling two distinct segmentation maps and two separate edge maps). Our primary contributions are
summarized below:
•We propose FlexEControl, a novel text-to-image generation model for efficient controllable image
generation that substantially reduces training memory overhead and model parameters through
decomposition of weights shared across different conditions.
•WeintroduceanewtrainingstrategytoimprovetheflexiblecontrollabilityofFlexEControl. Compared
with previous works, FlexEControl can generate new images conditioning on multiple inputs from
diverse compositions of multiple modalities.
•FlexEControl shows on-par performance with Uni-ControlNet (Zhao et al., 2023) on controllable text-
to-image generation with 41% less trainable parameters and 30% less training memory. Furthermore,
FlexEControl exhibits enhanced data efficiency, effectively doubling the performance achieved with
only half amount of training data.
2 Method
The overview of our method is shown in Figure 2. In general, we use the copied Stable Diffusion encoder
(Stable Diffusion encoder block and Stable Diffusion middle block) which accepts structural conditional input
and then perform efficient training via parameter reduction using Kronecker Decomposition first (Zhang et al.,
2Under review as submission to TMLR
Text
Prompt
Multimodal
Conditioning
Updated
WeightsCopied 
Encoder
Original
parameter
size:
36New
parameter
size:
4+6nShared
across conditions
SD Encoder Cross-Attention
 Supervision Loss
Mask
 Diffusion LossZero
Conv
SD 
Decoder
Figure 2: Overview of FlexEControl : a decomposed green matrix is shared across different input
conditions , significantly enhancing the model’s efficiency and preserving the image content. During training,
we integrate two specialized loss functions to enable flexible control and to adeptly manage conflicting
conditions. In the example depicted here, the new parameter size is efficiently condensed to 4 + 6n, wheren
denotes the number of decomposed matrix pairs.
2021a) and then low-rank decomposition over the updated weights of the copied Stable Diffusion encoder. To
enhance the control from language and different input conditions, we propose a new training strategy with
two newly designed loss functions. The details are shown in the sequel.
2.1 Preliminary
We use Stable Diffusion 1.5 (Rombach et al., 2022) in our experiments. This model falls under the category
of Latent Diffusion Models (LDM) that encode input images xinto a latent representation zvia an encoder
E, such that z=E(x), and subsequently carry out the denoising process within the latent space Z. An LDM
is trained with a denoising objective as follows:
Lldm=Ez,c,e,t/bracketleftig
∥ˆϵθ(zt|c,t)−ϵ∥2/bracketrightig
(1)
where (z,c)constitute data-conditioning pairs (comprising image latents and text embeddings), ϵ∼N(0,I),
t∼Uniform (1,T), andθdenotes the model parameters.
2.2 Efficient Training for Controllable Text-to-Image (T2I) Generation
Our approach is motivated by empirical evidence that Kronecker Decomposition (Zhang et al., 2021a)
effectively preserves critical weight information. We employ this technique to encapsulate the shared relational
structures among different input conditions. Our hypothesis posits that by amalgamating diverse conditions
with a common set of weights, data utilization can be optimized and training efficiency can be improved. We
focus on decomposing and fine-tuning only the cross-attention weight matrices within the U-Net (Ronneberger
et al., 2015) of the diffusion model, where recent works (Kumari et al., 2023) show their dominance when
customizing the diffusion model. As depicted in Figure 2, the copied encoder from the Stable Diffusion will
accept conditional input from different modalities. During training, we posit that these modalities, being
transformations of the same underlying image, share common information. Consequently, we hypothesize
that the updated weights of this copied encoder, ∆W, can be efficiently adapted within a shared decomposed
low-rank subspace. This leads to:
∆W=n/summationdisplay
i=1Hi⊗/parenleftbig
uiv⊤
i/parenrightbig
(2)
3Under review as submission to TMLR
Figure 3: Visualization of decomposed shared “slow” weights (right image) for a single condition case where
the input conditions (left image) are the segmentation, depth, and sketch maps, with the text prompt Dog,
Soccer player, Basketball player . We averaged the decomposed shared weights from the last cross-
attention block across all attention heads in Stable Diffusion. The results demonstrate the content-preserving
properties of Kronecker Decomposition, where the "slow" weights effectively share semantic components.
withnis the number of decomposed matrices, ui∈Rk
n×randvi∈Rr×d
n, whereris the rank of the matrix
which is a small number, Hiare the decomposed learnable matrices shared across different conditions, and ⊗
is the Kronecker product operation. The low-rank decomposition ensures a consistent low-rank representation
strategy. This approach substantially saves trainable parameters, allowing efficient fine-tuning over the
downstream text-to-image generation tasks.
The intuition for why Kronecker decomposition works for finetuning partially is partly rooted in the findings
of Zhang et al. (2021a); Mahabadi et al. (2021); He et al. (2022). These studies highlight how the model
weights can be broken down into a series of matrix products and thereby save parameter space. As shown
in Figure 2, the original weights is 6x6, then decomposed into a series of matrix products. When adapting
the training approach based on the decomposition to controllable T2I, the key lies in the shared weights,
which, while being common across various conditions, retain most semantic information. The Kronecker
Decomposition is known for its multiplicative rank property and content-preserving qualities. For instance,
the shared “slow” weights (Wen et al., 2020) of an image, combined with another set of “fast” low-rank
weights, can preserve the original image’s distribution without a loss in semantic integrity, as illustrated in
Figure 3. This observation implies that updating the slow weights is crucial for adapting to diverse conditions.
Following this insight, it becomes logical to learn a set of condition-shared decomposed weights in each layer,
ensuring that these weights remain consistent across different scenarios. The data utilization and parameter
efficiency is also improved.
2.3 Enhanced Training for Conditional Inputs
We then discuss how to improve the control under multiple input conditions of varying modalities with the
efficient training approach.
Dataset Augmentation with Text Parsing and Segmentation To optimize the model for scenarios
involving multiple homogeneous (same-type) conditional inputs, we initially augment our dataset. We utilize
a large language model ( gpt-3.5-turbo ) to parse texts in prompts containing multiple object entities. The
parsing query is structured as: Given a sentence, analyze the objects in this sentence, give me
the objects if there are multiple. Following this, we apply CLIPSeg (Lüddecke and Ecker, 2022)
(clipseg-rd64-refined version) to segment corresponding regions in the images, allowing us to divide
structural conditions into separate sub-feature maps tailored to the parsed objects. These segmentation
masks are selectively used to augment the dataset, specifically when there is a clear, single mask for each
identified object. This selective approach helps maintain the robustness of the dataset and enhances training
performance.
Cross-Attention Supervision Loss For each identified segment, we calculate a unified attention map,
Ai, averaging attention across layers and relevant Ntext tokens:
Ai=1
LL/summationdisplay
l=1N/summationdisplay
i=1JTi∈TjKCAl
i, (3)
4Under review as submission to TMLR
where J·Kis the Iverson bracket, CAl
iis the cross-attention map for token iin layerl, andTjdenotes the set
of tokens associated with the j-th segment.
The model is trained to predict noise for image-text pairs concatenated based on the parsed and segmented
results. An additional loss term, designed to ensure focused reconstruction in areas relevant to each text-
derived concept, is introduced. This loss is calculated as the Mean Squared Error (MSE) deviation from
predefined masks corresponding to the segmented regions:
Lca=Ez,t/bracketleftig
∥Ai(vi,zt)−Mi∥2
2/bracketrightig
, (4)
whereAi(vi,zt)is the cross-attention map between token viand noisy latent zt, andMirepresents the mask
for thei-th segment, which is derived from the segmented regions in our augmented dataset and appropriately
resized to match the dimensions of the cross-attention maps.
Masked Diffusion Loss To ensure fidelity to the specified conditions, we apply a condition-selective
diffusion loss that concentrates the denoising effort on conceptually significant regions. This focused loss
function is applied solely to pixels within the regions delineated by the concept masks, which are derived from
the non-zero features of the input structural conditions. Specifically, the masks are binary where non-zero
feature areas are assigned a value of one, and areas lacking features are set to zero. Because of the sparsity
of pose features for this condition, we use the all-ones mask. These masks serve to underscore the regions
referenced in the corresponding text prompts:
Lmask =Ez,ϵ,t/bracketleftig
∥(ϵ−ϵθ(zt,t))⊙M∥2
2/bracketrightig
, (5)
whereMrepresents the union of binary mask obtained from input conditions, ztdenotes the noisy latent at
timestept,ϵthe injected noise, and ϵθthe estimated noise from the denoising network (U-Net).
The total loss function employed is:
Ltotal=Lldm+λcaLca+λmaskLmask, (6)
withλrecandλattnset to 0.01. The integration of LcaandLmaskensure the model will focus at reconstructing
the conditional region and attend to guided regions during generation.
3 Experiments
3.1 Datasets
In pursuit of our objective of achieving controlled Text-to-Image (T2I) generation, we employed the LAION
improved_aesthetics_6plus (Schuhmann et al., 2022) dataset for our model training. Specifically, we
meticulously curated a subset comprising 5,082,236 instances, undertaking the elimination of duplicates
and applying filters based on criteria such as resolution and NSFW score. Given the targeted nature of
our controlled generation tasks, the assembly of training data involved considerations of additional input
conditions, specifically edge maps, sketch maps, depth maps, segmentation maps, and pose maps. The
extraction of features from these maps adhered to the methodology expounded in(Zhang and Agrawala, 2023).
3.2 Experimental Setup
Structural Input Condition Extraction We start from the processing of various local conditions used in
our experiments. To facilitate a comprehensive evaluation, we have incorporated a diverse range of structural
conditions. These conditions include edge maps (Canny, 1986; Xie and Tu, 2015; Gu et al., 2022a), sketch
maps (Simo-Serra et al., 2016), pose information (Cao et al., 2017), depth maps (Ranftl et al., 2020), and
segmentation maps (Xiao et al., 2018), each extracted using specialized techniques. These conditions are
crucial for guiding the text-to-image generation process, enabling FlexEControl to produce images that are
both visually appealing and semantically aligned with the text prompts and structural inputs. The additional
details for extracting those conditions are given in the Appendix.
5Under review as submission to TMLR
Table 1:Text-to-image generation efficiency comparison : FlexEControl shows substantial reductions
in memory cost, trainable parameters, and training time, highlighting its improved training efficiency with
the same model architecture. Training times are averaged over three runs up to 400 iterations for consistency.
Models Memory Cost ↓# Params.↓Training Time↓
Uni-ControlNet (Zhao et al., 2023) 20.47GB 1271M 5.69 ±1.33s/it
LoRA (Hu et al., 2021) 17.84GB 1074M 3.97 ±1.27 s/it
PHM (Zhang et al., 2021a) 15.08GB 819M 3.90 ±2.01 s/it
FlexEControl ( ours) 14.33GB 750M 2.15 ±1.42 s/it
Table 2:Quantitative evaluation of controllability and image quality for single structural conditional
inputs. FlexEControl performs overall better while maintaining much improved efficiency.
ModelsCanny MLSD HED Sketch Depth Segmentation PosesFID↓CLIP Score↑(SSIM)↑(SSIM)↑(SSIM)↑(SSIM)↑(MSE)↓(mIoU)↑(mAP)↑
T2IAdapter (Mou et al., 2023) 0.4480 - - 0.5241 90.01 0.6983 0.3156 27.80 0.4957
ControlNet (Zhang and Agrawala, 2023) 0.4989 0.6172 0.4990 0.6013 89.08 0.7481 0.2024 27.62 0.4931
Uni-Control (Qin et al., 2023) 0.4977 0.6374 0.4885 0.5509 90.04 0.7143 0.2083 27.80 0.4899
Uni-ControlNet (Zhao et al., 2023) 0.4910 0.6083 0.4715 0.5901 90.17 0.7084 0.2125 27.74 0.4890
PHM (Zhang et al., 2021a) 0.4365 0.5712 0.4633 0.4878 91.38 0.5534 0.1664 27.91 0.4961
LoRA (Hu et al., 2021) 0.4497 0.6381 0.5043 0.5097 89.09 0.5480 0.1538 27.99 0.4832
FlexEControl ( ours) 0.4990 0.6385 0.5041 0.5518 90.93 0.7496 0.2093 27.55 0.4963
Evaluation Metrics We employ a comprehensive benchmark suite of metrics including mIoU (Rezatofighi
et al., 2019), SSIM (Wang et al., 2004), mAP, MSE, FID (Heusel et al., 2017), and CLIP Score (Hessel et al.,
2021; Radford et al., 2021)1.
Baselines In our comparative evaluation, we assess T2I-Adapter (Mou et al., 2023), PHM (Zhang et al.,
2021a), Uni-ControlNet (Zhao et al., 2023), and LoRA (Hu et al., 2021). The implementation details are
given in the Appendix.
Implementation Details In accordance with the configuration employed in Uni-ControlNet, we utilized
Stable Diffusion 1.52as the foundational model. Our model underwent training for a singular epoch,
employing the AdamW optimizer (Kingma and Ba, 2014) with a learning rate set at 10−5. Throughout all
experimental iterations, we standardized the dimensions of input and conditional images to 512×512. The
fine-tuning process was executed on P3 AWS EC2 instances equipped with 64 NVIDIA V100 GPUs.
For baseline implementations, we compare FlexEControl with T2I-Adapter (Mou et al., 2023), PHM (Zhang
et al., 2021a), Uni-ControlNet (Zhao et al., 2023), and LoRA (Hu et al., 2021) where we implement LoRA
and PHM layers over the trainable modules in Uni-ControlNet interms of generated image quality and
controllability. The rank of LoRA is set to 4. For PHM (Zhang et al., 2021a), we implement it by performing
Kronecker decomposition and share weights across different layer, with the number of decomposed matrix
being 4.
For quantitative assessment, a subset comprising 10,000 high-quality images from the LAION im-
proved_aesthetics_6.5plus dataset was utilized. The resizing of input conditions to 512×512was
conducted during the inference process.
3.3 Quantitative Results
Table 1 highlights FlexEControl’s superior efficiency compared to Uni-ControlNet. It achieves a 30% reduction
in memory cost, lowers trainable parameters by 41% (from 1271M to 750M), and significantly reduces training
time per iteration from 5.69s to 2.15s.
1https://github.com/jmhessel/clipscore
2https://huggingface.co/runwayml/stable-diffusion-v1-5
6Under review as submission to TMLR
Table 3:Quantitative evaluation of controllability and image quality on FlexEControl along with
its variants and Uni-ControlNet. For Uni-ControlNet, we implement multiple conditioning by adding two
homogeneous conditional images after passing them through the feature extractor.
ModelsCanny MLSD HED Sketch Depth Segmentation PosesFID↓CLIP Score↑(SSIM)↑(SSIM)↑(SSIM)↑(SSIM)↑(MSE)↓(mIoU)↑(mAP)↑
Single ConditioningUni-ControlNet 0.3268 0.4097 0.3177 0.4096 98.80 0.4075 0.1433 29.43 0.4844
FlexEControl (w/o Lca) 0.3698 0.4905 0.3870 0.4855 94.90 0.4449 0.1432 28.03 0.4874
FlexEControl (w/o Lmask) 0.3701 0.4894 0.3805 0.4879 94.30 0.4418 0.1432 28.19 0.4570
FlexEControl 0.3711 0.4920 0.3871 0.4869 94.83 0.4479 0.1432 28.03 0.4877
Multiple ConditioningUni-ControlNet 0.3078 0.3962 0.3054 0.3871 98.84 0.3981 0.1393 28.75 0.4828
FlexEControl (w/o Lca) 0.3642 0.4901 0.3704 0.4815 94.95 0.4368 0.1405 28.50 0.4870
FlexEControl (w/o Lmask) 0.3666 0.4834 0.3712 0.4831 94.89 0.4400 0.1406 28.68 0.4542
FlexEControl 0.3690 0.4915 0.3784 0.4849 92.90 0.4429 0.1411 28.24 0.4873
Table 2 provides a comprehensive comparison of FlexEControl’s performance against Uni-ControlNet and
T2IAdapter across diverse input conditions. After training on a dataset of 5M text-image pairs, FlexEControl
demonstrates better, if not superior, performance metrics compared to Uni-ControlNet and T2IAdapter. Note
that Uni-ControlNet is trained on a much larger dataset (10M text-image pairs from the LAION dataset).
Although there is a marginal decrease in SSIM scores for sketch maps and mAP scores for poses, FlexEControl
excels in other metrics, notably surpassing Uni-ControlNet and T2IAdapter. This underscores our method’s
proficiency in enhancing efficiency and elevating overall quality and accuracy in controllable text-to-image
generation tasks.
To validate FlexEControl’s effectiveness in handling multiple structural conditions, we compared it with
Uni-ControlNet through human evaluations. Two scenarios were considered: multiple homogeneous input
conditions (300 images, each generated with 2 canny edge maps) and multiple heterogeneous input conditions
(500 images, each generated with 2 randomly selected conditions). Results, summarized in Table 4, reveal that
FlexEControl was preferred by 64.00% of annotators, significantly outperforming Uni-ControlNet (23.67%).
This underscores FlexEControl’s proficiency with complex, homogeneous inputs. Additionally, FlexEControl
demonstrated superior alignment with input conditions (67.33%) compared to Uni-ControlNet (23.00%). In
scenarios with random heterogeneous conditions, FlexEControl was preferred for overall quality and alignment
over Uni-ControlNet.
In addition to our primary comparisons, we conducted an additional quantitative evaluation of FlexEControl
and Uni-ControlNet. This evaluation focused on assessing image quality under scenarios involving multiple
conditions from both the homogeneous and heterogeneous modalities. The findings of this evaluation
are summarized in Table 5. FlexEControl consistently outperforms Uni-ControlNet in both categories,
demonstrating lower FID scores for better image quality and higher CLIP scores for improved alignment with
text prompts.
3.3.1 Ablation Studies
To substantiate the efficacy of FlexEControl in enhancing training efficiency while upholding commendable
model performance, and to ensure a fair comparison, an ablation study was conducted by training models
on an identical dataset. We trained FlexEControl along its variants and Uni-ControlNet on a subset of
100,000 training samples from LAION improved_aesthetics_6plus . When trained with the identical
data, FlexEControl performs better than Uni-ControlNet. The outcomes are presented in Table 3. Evidently,
FlexEControl exhibits substantial improvements over Uni-ControlNet when trained on the same dataset.
This underscores the effectiveness of our approach in optimizing data utilization, concurrently diminishing
computational costs, and enhancing efficiency in the text-to-image generation process.
We also study the impact of λcaandλmasktrained on the subset of 100,000 samples from LAION im-
proved_aesthetics_6plus for 6,000 steps. We evaluated the score on SSIM of canny edge maps and
mIoU of segmentation maps, results are shown in Figure 4. As observed, FlexEControl achieves optimal
performance when both λca= 0.01andλmask = 0.01. Additionally, as indicated in Table 3, FlexEControl
outperforms the configurations where either λcaorλmaskis set to zero, demonstrating the importance of
incorporating both the cross-attention supervision loss and the masked diffusion loss. However, higher values
7Under review as submission to TMLR
0.01 1.00 10.00
Lambda Value for ca and mask
0.20000.30000.40000.50000.6000Accuracy
0.6496
0.5083
0.35780.3990
0.2842
0.2074Segmentation Maps
Canny Edge Maps
(a) FlexEControl performs the best when both
λca= 0.01andλmask = 0.01.
𝜆!"=0.01
𝜆#"$%=0.01𝜆!"=0𝜆#"$%=0
Condition1Condition2(b) Qualitative comparison. The text prompt is A mechanical
whale flying over a desert that has a tree.
Figure 4: Quantitative and qualitative comparison showing the effect of excluding cross-attention supervision
loss and masked diffusion loss.
Figure 5: Qualitative comparison of FlexEControl and existing controllable diffusion models with single
condition. Text prompt: A bed.The image quality of FlexEControl is comparable to existing methods and
Uni-ControlNet + LoRA, while FlexEControl has much more efficiency.
ofλcaandλmaskcould lead to instability during training, causing the model to focus excessively on local
conditions, which results in worse performance. We also show the qualitative comparison of λcaandλmaskin
Figure 4. Without the cross-attention loss, the concept of the whale tends to merge with the tree, causing
attribute leakage and misblending issues. Without the masked diffusion loss, the model is more likely to
generate the whale outside the intended region, leading to blurred regions and a lack of focus within the
maps.
3.3.2 Additional Results on Stable Diffusion 2
In our efforts to explore the versatility and adaptability of FlexEControl, we conducted additional experiments
using the Stable Diffusion 2.1 model, available at Hugging Face’s Model Hub. The results from these
experiments are depicted in Table 6. FlexEControl can leverage the advancements in Stable Diffusion 2.1 to
achieve even better performance in text-to-image generation tasks. For the sake of a fair comparison in the
main paper, we conduct experiments using Stable Diffusion 1.5 model.
8Under review as submission to TMLR
   Input Condition                  Input Condition2                   Uni-ControlNet                         LoRA                                Uni-Control                             OursText Pr ompt : A coffee and the candleText Pr ompt : A cat and a sofa
Figure 6: Qualitative comparison of FlexEControl and existing controllable diffusion models with multiple
heterogeneous conditions. First row: FlexEControl effectively integrates both the segmentation and edge maps
to generate a coherent image while Uni-ControlNet and LoRA miss the segmentation map and Uni-Control
generates a messy image. Additionally, the cats generated by Uni-ControlNet, LoRA, and Uni-Control lack
clear feline characteristics and does not align with the edge map. Second row: The input condition types are
one depth map and one sketch map. FlexEControl can do more faithful generation while all three others
generate the candle in the coffee.
Table 4: Human evaluation of FlexEControl and Uni-ControlNet under homogenous and heterogeneous struc-
tural conditions, assessing both human preference and condition alignment. "Win" indicates FlexEControl’s
preference, "Tie" denotes equivalence, and "Lose" indicates Uni-ControlNet’s preference. Results indicate that
under homogeneous conditions, FlexEControl outperforms Uni-ControlNet in both human preference and
condition alignment.
Condition Type Metric Win Tie Lose
HomogeneousHuman Preference (%) 64.00 12.33 23.67
Condition Alignment (%) 67.33 9.67 23.00
HeterogeneousHuman Preference (%) 9.8087.40 2.80
Condition Alignment (%) 6.6089.49 4.00
Table 5: Quantitative evaluation of controllability and image quality in scenarios with multiple conditions
from heterogeneous and homogeneous modalities for FlexEControl and Uni-ControlNet. The ’heterogeneous’
category averages the performance across one Canny condition combined with six other different modalities.
The ’homogeneous’ category represents the average performance across seven identical modalities (three
inputs).
Condition Type Baseline FID ↓CLIP Score↑
HeterogeneousUni-ControlNet 27.81 0.4869
FlexEControl 27.47 0.4981
HomogeneousUni-ControlNet 28.98 0.4858
FlexEControl 27.65 0.4932
3.4 Qualitative Results
We present qualitative results of our FlexEControl under three different settings: single input condition,
multiple heterogeneous conditions, and multiple homogeneous conditions, illustrated in Figure 5, Figure 6,
and Figure 7, respectively. The results indicate that FlexEControl is comparable to baseline models when
9Under review as submission to TMLR
Figure 7: Qualitative performance of FlexEControl when conditioning on diverse compositions of multiple
modalities. Each row in the figure corresponds to a unique type of condition, with the text prompts and
conditions as follows: (first row) two canny edge maps with the prompt A motorcycle in the forest ,
(second row) two depth maps for A car, (third row) two sketch maps depicting A vase with a green apple ,
(fourth row) two canny edge maps for Stormtrooper’s lecture at the football field , (fifth row) two
segmentation maps visualizing A deer in the forests , (sixth row) two MLSD edge maps for A sofa in a
desert, and (seventh row) one segmentation map and one edge map for A bird. These examples illustrate
the robust capability of FlexEControl to effectively utilize multiple multimodal conditions, generating images
that are not only visually compelling but also faithfully aligned with the given textual descriptions and input
conditions.
10Under review as submission to TMLR
Condition1Condition2FlexEControlFlexEControlSeg.Map(Teddybearwithnoisybackground)Seg.map(Dogwithnoisyforeground)A teddybearandabrownbear,ontherocksAteddybearinasweaterandablackbear,inanautumnforestDepthMap(Soccerballwithblurredforeground)DepthMap(Soccerplayer)Abluerobotiskickingasoccerball,onthegrass,threesoccer balls nearbyAn electric robotiskickingasoccerball,onthegrass,threesoccer balls nearby
FlexEControl
A teddy bear and a black bear with its back turned,inthewild
Awiredrobotiskickingasoccerball,onthegrass,threesoccer balls nearby
Figure 8: Qualitative results on multimodal control for generating multiple foregrounds.
Condition1Condition2Condition3
FlexEControlFlexEControlSeg.map(bear)Seg.map(baseball)Seg.map(avocado)A bear, looking at the baseball, next to an avocadoA bear, looking at the baseball, next to an avocado
Cannyedge(Car)Cannyedge(Mountain)Cannyedge(Tree)A car,mountain,tree,inthewild,cartoonA car,mountain,tree,inthewild,cartoon
Figure 9: Qualitative results on multimodal control using three homogeneous condition images
a single condition is input. However, with multiple conditions, FlexEControl consistently and noticeably
outperforms other models. Particularly, under multiple homogeneous conditions, FlexEControl excels in
generating overall higher quality images that align more closely with the input conditions, surpassing other
models.
We also shown in Figure 8 the qualitative results of generating multiple foregrounds.. In the teddy bear and
dog condition case, the teddy bear has a noisy background, and the dog image has a noisy foreground, while
the prompt asks the model to generate a teddy bear and a bear. As shown in the results, FlexEControl
effectively handles the noisy and conflicting semantic information, successfully following the prompt to
generate an image with two foregrounds: a teddy bear and a bear, even though the bear condition comes
from a dog image. In the robot case, the depth map for the soccer ball is blurred, and the player given is a
human soccer player, yet the model still follows the prompt to generate robot soccer player images with three
additional soccer balls. These examples demonstrate the strong controllability of FlexEControl.
Furthermore, we show in Figure 9 the qualitative results for FlexEControl on mulimodal control using three
homogeneous conditiona images. Further highlighting the versatility and effectiveness of FlexEControl in
11Under review as submission to TMLR
Table 6: Quantitative evaluation of controllability and image quality trained on a subset of 100,000 samples.
Human poses are evaluated solely within portrait images.
ModelsCanny MLSD HED Sketch Depth Segmentation PosesFID↓CLIP Score↑(SSIM)↑(SSIM)↑(SSIM)↑(SSIM)↑(MSE)↓(mIoU)↑(mAP)↑
FlexEControl 0.3711 0.4920 0.3871 0.4869 94.83 0.4479 0.1432 28.03 0.4877
FlexEControl-SD 2.1 0.3891 0.5273 0.4077 0.4960 93.58 0.4490 0.1562 25.08 0.5833
Condition1Condition2Ours
Condition1Condition2Ours
Figure 10: Failure cases . Failure cases in generating human images ( left): The text prompt is: a basketball
player with a helicopter. Due to limitations in the Stable Diffusion backbone, FlexEControl fails to
generate a correct basketball player, instead producing an image where the basketball player’s face is replaced
with a basketball. Failure cases in generating images with a weak text prompt for the background ( right): The
text prompt is a car is parking , which provides little information about the background. The background
segmentation map is complex, and the foreground uses a strong canny edge guidance. Consequently, the
generated image shows a weak effect of the segmentation, although the foreground is generated accurately.
handling multiple conditions. Additionally, we showcase the extensibility of FlexEControl in controllable
video generation. The results are presented in Figure 12 and Figure 13 in the Appendix, where results for
providing one condition and multiple conditions are demonstrated.
3.5 Limitations
Although FlexEControl achieves strong performance, it shares the inherent limitations of diffusion-based image
generation models. The LAION dataset exhibits certain biases, which can lead to suboptimal performance
in specific scenarios. FlexEControl could benefit from a more robust and strong pretrained text-to-image
generation backbone and could be further improved with access to better open-source datasets that mitigate
the risk of generating biased, toxic, sexualized, or other harmful content. Some failure cases and analyses
where FlexEControl struggled are illustrated in Figure 10.
4 Related Work
FlexEControl is an instance of efficient training and controllable text-to-image generation. Here, we overview
modeling efforts in the subset of efficient training towards reducing parameters and memory cost and
controllable T2I.
4.1 Efficient Training
Prior work has proposed efficient training methodologies both for pretraining and fine-tuning. These methods
have established their efficacy across an array of language and vision tasks. One of these explored strategies is
Prompt Tuning (Lester et al., 2021), where trainable prompt tokens are appended to pretrained models (Schick
and Schütze, 2020; Ju et al., 2021; Jia et al., 2022). These tokens can be added exclusively to input embeddings
or to all intermediate layers (Li and Liang, 2021), allowing for nuanced model control and performance
optimization. Low-Rank Adaptation (LoRA) (Hu et al., 2021) is another innovative approach that introduces
trainable rank decomposition matrices for the parameters of each layer. LoRA has exhibited promising fine-
tuning ability on large generative models, indicating its potential for broader application. Furthermore, the use
of Adapters inserts lightweight adaptation modules into each layer of a pretrained transformer (Houlsby et al.,
2019; Rücklé et al., 2021). This method has been successfully extended across various setups (Zhang et al.,
12Under review as submission to TMLR
2021b; Gao et al., 2021; Mou et al., 2023), demonstrating its adaptability and practicality. Other approaches
including post-training model compression (Fang et al., 2023) facilitate the transition from a fully optimized
model to a compressed version – either sparse (Frantar and Alistarh, 2023), quantized (Li et al., 2023a; Gu
et al., 2022b), or both. This methodology was particularly helpful for parameter quantization (Dettmers
et al., 2023). Different from these methodologies, FlexEControl puts forth a new unified strategy that aims
to enhance the efficient training of text-to-image diffusion models through the leverage of low-rank structure.
Yeh et al. (2023) and Marjit et al. (2024) explore the use of Kronecker Product for tuning T2I models.
These approaches focus on enhancing control and efficiency, particularly in subject-driven or style-preserving
generation tasks. FlexEControl is the first to leverage it for multimodal conditioning, leading to enhanced
flexibility and efficiency in handling diverse input modalities. FlexEControl is also a general approach that
can be applied to UniControl Qin et al. (2023) or other backbones.
4.2 Controllable Text-to-Image Generation
Recent developments in the text-to-image generation domain strives for more control over image generation,
enabling more targeted, stable, and accurate visual outputs, several models like T2I-Adapter (Mou et al.,
2023) and Composer (Huang et al., 2023) have emerged to enhance image generations following the semantic
guidance of text prompts and multiple different structural conditional control. However, existing methods are
struggling at dealing with multiple conditions from the same modalities, especially when they have conflicts,
e.g. multiple segmentation maps and at the same time follow the guidance of text prompts; Recent studies also
highlight challenges in controllable text-to-image generation (T2I), such as omission of objects in text prompts
and mismatched attributes (Lee et al., 2023; Bakr et al., 2023), showing that current models are struggling
at handling controls from different conditions. Towards these, the Attend-and-Excite method Chefer et al.
(2023) refines attention regions to ensure distinct attention across separate image regions. ReCo Yang et al.
(2023), GLIGEN Li et al. (2023b), and Layout-Guidance Chen et al. (2023) allow for image generation
informed by bounding boxes and regional descriptions. Mo et al. (2024) offers a training-free approach to
multimodal control. FlexEControl improves the model’s controllability by proposing a new training strategy,
distinguishing itself by targeting the flexibility and efficiency of multimodal control, especially in scenarios
with conflicting conditions from the same or different modalities (e.g., multiple segmentation maps combined
with text prompts).
5 Conclusion
In this work, we present FlexEControl, an approach designed to enhance both the flexibility and efficiency of
controllable diffusion-based text-to-image generation. Our method introduces several key innovations: Dataset
Augmentation with Text Parsing and Segmentation, Cross-Attention Supervision, and Masked Diffusion
Loss, which together significantly improve the model’s ability to handle diverse and conflicting multimodal
inputs. Additionally, we propose an Efficient Training strategy that optimizes parameter, data, and memory
efficiency without sacrificing performance or inference speed. We also demonstrate that sharing a common
set of decomposed weights across different multimodal conditions via Kronecker Decomposition can further
optimize parameter space and enhance efficiency. These findings suggest that FlexEControl can be readily
adapted to other architectures, offering a scalable solution for future developments in text-to-image generation.
Future work could explore more advanced decomposition techniques and their application to cutting-edge
diffusion backbones or Diffusion Transformers (DiTs), aiming to further optimize model efficiency, complexity,
and expressive power.
Broader Impact Statement
While FlexEControl demonstrates promising results in efficient and controllable text-to-image generation,
the ability of FlexEControl to generate realistic images based on textual descriptions raises ethical concerns,
especially regarding the creation of misleading or deceptive content. It is imperative to establish guidelines
and ethical standards for the use of such technology to prevent misuse in generating deepfakes or propagating
false information.
13Under review as submission to TMLR
References
Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. 2023. Break-a-scene:
Extracting multiple concepts from a single image. In SIGGRAPH Asia 2023 Conference Papers , pages
1–12.
Eslam Mohamed Bakr, Pengzhan Sun, Xiaogian Shen, Faizan Farooq Khan, Li Erran Li, and Mohamed
Elhoseiny. 2023. HRS-Bench: Holistic, Reliable and Scalable Benchmark for Text-to-Image Models. In
Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 20041–20053.
Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo
Aila, Samuli Laine, Bryan Catanzaro, et al. 2022. ediffi: Text-to-image diffusion models with an ensemble
of expert denoisers. arXiv preprint arXiv:2211.01324 .
Samy Bengio, Jason Weston, and David Grangier. 2010. Label embedding trees for large multi-class tasks. In
NIPS.
Tim Brooks, Aleksander Holynski, and Alexei A Efros. 2023. Instructpix2pix: Learning to follow image editing
instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pages 18392–18402.
John Canny. 1986. A computational approach to edge detection. IEEE Transactions on pattern analysis and
machine intelligence , (6):679–698.
Shidong Cao, Wenhao Chai, Shengyu Hao, Yanting Zhang, Hangyue Chen, and Gaoang Wang. 2023.
Difffashion: Reference-based fashion design with structure-aware transfer by diffusion models. IEEE
Transactions on Multimedia .
Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. 2017. Realtime multi-person 2d pose estimation
using part affinity fields. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 7291–7299.
Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin
Murphy, William T Freeman, Michael Rubinstein, et al. 2023. Muse: Text-to-image generation via masked
generative transformers. arXiv preprint arXiv:2301.00704 .
Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. 2023. Attend-and-excite: Attention-
based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG) ,
42(4):1–10.
Minghao Chen, Iro Laina, and Andrea Vedaldi. 2023. Training-free layout control with cross-attention
guidance. arXiv preprint arXiv:2304.03373 .
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for
contrastive learning of visual representations. arXiv preprint arXiv:2002.05709 .
Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. 2022. Diffedit: Diffusion-based
semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427 .
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of
quantized llms. arXiv preprint arXiv:2305.14314 .
Gongfan Fang, Xinyin Ma, and Xinchao Wang. 2023. Structural pruning for diffusion models. arXiv preprint
arXiv:2305.10924 .
Elias Frantar and Dan Alistarh. 2023. Massive language models can be accurately pruned in one-shot. arXiv
preprint arXiv:2301.00774 .
Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao.
2021. Clip-adapter: Better vision-language models with feature adapters. arXiv preprint arXiv:2110.04544 .
14Under review as submission to TMLR
Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. 2021. Knowledge distillation: A survey.
International Journal of Computer Vision , 129:1789–1819.
Geonmo Gu, Byungsoo Ko, SeoungHyun Go, Sung-Hyun Lee, Jingeun Lee, and Minchul Shin. 2022a. Towards
light-weight and real-time line segment detection. In Proceedings of the AAAI Conference on Artificial
Intelligence , volume 36, pages 726–734.
Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo.
2022b. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 10696–10706.
Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. 2021. Open-vocabulary object detection via vision
and language knowledge distillation. arXiv preprint arXiv:2104.13921 .
Qiushan Guo, Xinjiang Wang, Yichao Wu, Zhipeng Yu, Ding Liang, Xiaolin Hu, and Ping Luo. 2020. Online
knowledge distillation via collaborative learning. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 11020–11029.
Xuehai He, Chunyuan Li, Pengchuan Zhang, Jianwei Yang, and Xin Eric Wang. 2022. Parameter-efficient
fine-tuning for vision transformers. arXiv preprint arXiv:2203.16329 .
Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. 2022. Prompt-
to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626 .
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. CLIPScore: a reference-free
evaluation metric for image captioning. In EMNLP.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information
processing systems , 30.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 .
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-Efficient Transfer Learning for NLP.
arXiv:1902.00751 [cs, stat] .
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685 [cs] .
Xinting Hu, Kaihua Tang, Chunyan Miao, Xian-Sheng Hua, and Hanwang Zhang. Distilling Causal Effect of
Data in Class-Incremental Learning. page 10.
Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. 2023. Composer: Creative and
controllable image synthesis with composable conditions. arXiv preprint arXiv:2302.09778 .
Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam
Lim. 2022. Visual prompt tuning. arXiv preprint arXiv:2203.12119 .
Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi Xie. 2021. Prompting visual-language models
for efficient video understanding. arXiv preprint arXiv:2112.04478 .
Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal
Irani. 2023. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 6007–6017.
Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .
15Under review as submission to TMLR
Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. 2023. Multi-concept
customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 1931–1941.
Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad
Ghavamzadeh, and Shixiang Shane Gu. 2023. Aligning text-to-image models using human feedback. arXiv
preprint arXiv:2302.12192 .
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt
tuning.arXiv preprint arXiv:2104.08691 .
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong
Hoi. 2021. Align before fuse: Vision and language representation learning with momentum distillation.
Advances in neural information processing systems , 34:9694–9705.
Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation.
arXiv:2101.00190 [cs] .
Xiuyu Li, Long Lian, Yijiang Liu, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt
Keutzer. 2023a. Q-diffusion: Quantizing diffusion models. arXiv preprint arXiv:2302.04304 .
Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and
Yong Jae Lee. 2023b. Gligen: Open-set grounded text-to-image generation. 2023 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) .
Timo Lüddecke and Alexander Ecker. 2022. Image segmentation using text and image prompts. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 7086–7096.
Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. 2021. Compacter: Efficient Low-Rank
Hypercomplex Adapter Layers. arXiv:2106.04647 [cs] .
Shyam Marjit, Harshit Singh, Nityanand Mathur, Sayak Paul, Chia-Mu Yu, and Pin-Yu Chen. 2024.
Diffusekrona: A parameter efficient fine-tuning method for personalized diffusion model. arXiv preprint
arXiv:2402.17412 .
Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim
Salimans. 2023. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 14297–14306.
SichengMo, FangzhouMu, KuanHengLin, YanliLiu, BochenGuan, YinLi, andBoleiZhou.2024. Freecontrol:
Training-free spatial control of any text-to-image diffusion model with any condition. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 7465–7475.
Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. 2023. Null-text inversion
for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 6038–6047.
Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. 2023.
T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.
arXiv preprint arXiv:2302.08453 .
Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,
Ilya Sutskever, and Mark Chen. 2021. Glide: Towards photorealistic image generation and editing with
text-guided diffusion models. arXiv preprint arXiv:2112.10741 .
Matthew Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. Semi-supervised sequence
tagging with bidirectional language models. In ACL.
Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. 2018. Efficient neural architecture
search via parameter sharing. In ICML.
16Under review as submission to TMLR
Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles,
Caiming Xiong, Silvio Savarese, et al. 2023. Unicontrol: A unified diffusion model for controllable visual
generation in the wild. arXiv preprint arXiv:2305.11147 .
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural
language supervision. arXiv preprint arXiv:2103.00020 .
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya
Sutskever. 2021a. Zero-shot text-to-image generation. In ICML.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya
Sutskever. 2021b. Zero-shot text-to-image generation. In International Conference on Machine Learning ,
pages 8821–8831. PMLR.
René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. 2020. Towards robust
monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on
pattern analysis and machine intelligence , 44(3):1623–1637.
Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. 2019.
Generalized intersection over union: A metric and a loss for bounding box regression. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition , pages 658–666.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 10684–10695.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical
image segmentation. In International Conference on Medical image computing and computer-assisted
intervention , pages 234–241. Springer.
Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych.
2021. AdapterDrop: On the Efficiency of Adapters in Transformers. arXiv:2010.11918 [cs] .
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar
Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan
Ho, David J Fleet, and Mohammad Norouzi. 2022a. Photorealistic text-to-image diffusion models with
deep language understanding. arXiv:2205.11487 .
ChitwanSaharia, WilliamChan, SaurabhSaxena, LalaLi, JayWhang, EmilyLDenton, KamyarGhasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 2022b. Photorealistic text-to-image
diffusion models with deep language understanding. Advances in Neural Information Processing Systems ,
35:36479–36494.
Tim Salimans and Jonathan Ho. 2022. Progressive distillation for fast sampling of diffusion models. arXiv
preprint arXiv:2202.00512 .
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of
BERT: smaller, faster, cheaper and lighter. In EMC2 @ NeurIPS .
Timo Schick and Hinrich Schütze. 2020. Exploiting cloze questions for few shot text classification and natural
language inference. arXiv preprint arXiv:2001.07676 .
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,
Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022. Laion-5b: An open
large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402 .
Edgar Simo-Serra, Satoshi Iizuka, Kazuma Sasaki, and Hiroshi Ishikawa. 2016. Learning to simplify: fully
convolutional networks for rough sketch cleanup. ACM Transactions on Graphics (TOG) , 35(4):1–11.
17Under review as submission to TMLR
Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica
Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. 2021.
MLP-Mixer: An all-MLP Architecture for Vision. arXiv:2105.01601 [cs] .
Dani Valevski, Matan Kalman, Yossi Matias, and Yaniv Leviathan. 2022. Unitune: Text-driven image editing
by fine tuning an image generation model on a single image. arXiv preprint arXiv:2210.09477 .
Yixin Wang and Michael I. Jordan. 2021. Desiderata for Representation Learning: A Causal Perspective.
arXiv:2109.03795 [cs, stat] .
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. 2004. Image quality assessment: from
error visibility to structural similarity. IEEE transactions on image processing , 13(4):600–612.
Yeming Wen, Dustin Tran, and Jimmy Ba. 2020. Batchensemble: an alternative approach to efficient ensemble
and lifelong learning. arXiv preprint arXiv:2002.06715 .
Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. 2018. Unified perceptual parsing for scene
understanding. In Proceedings of the European conference on computer vision (ECCV) , pages 418–434.
SainingXieandZhuowenTu.2015. Holistically-nestededgedetection. In Proceedings of the IEEE international
conference on computer vision , pages 1395–1403.
Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu,
Ce Liu, Michael Zeng, et al. 2023. Reco: Region-controlled text-to-image generation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 14246–14255.
Shih-Ying Yeh, Yu-Guan Hsieh, Zhidong Gao, Bernard BW Yang, Giyeong Oh, and Yanmin Gong. 2023.
Navigating text-to-image customization: From lycoris fine-tuning to model evaluation. In The Twelfth
International Conference on Learning Representations .
Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,
Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. 2022. Scaling autoregressive models for content-rich
text-to-image generation. arXiv preprint arXiv:2206.10789 , 2(3):5.
Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. 2021. BitFit: Simple Parameter-efficient Fine-tuning
for Transformer-based Masked Language-models. arXiv:2106.10199 [cs] .
Aston Zhang, Yi Tay, Shuai Zhang, Alvin Chan, Anh Tuan Luu, Siu Cheung Hui, and Jie Fu. 2021a.
Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with
1/nparameters. arXiv preprint arXiv:2102.08597 .
Lvmin Zhang and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models.
arXiv preprint arXiv:2302.05543 .
Renrui Zhang, Rongyao Fang, Peng Gao, Wei Zhang, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.
2021b. Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling. arXiv:2111.03930
[cs].
Zhongping Zhang, Jian Zheng, Jacob Zhiyuan Fang, and Bryan A Plummer. 2023. Text-to-image editing by
image information removal. arXiv preprint arXiv:2305.17489 .
Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K Wong.
2023. Uni-controlnet: All-in-one control to text-to-image diffusion models. arXiv preprint arXiv:2305.16322 .
18Under review as submission to TMLR
Q K
V
Q K
V
Q K
V
Copied SD EncoderQ K
V
Q K
V
Q K
V
Q K
VQ K
V
SDFeature
extractor
Q K
VText Prompts
Efﬁcient Training
Figure 11: Detailed model architecture in FlexEControl. The Stable Diffusion part is fixed and others are
trainable.
This Appendix is organized as follows:
•Appendix A contains our detailed model architectures;
•Appendix B contains additional implementation details;
•Appendix C contains additional results;
•Appendix D contains additional related works;
•Appendix E contains details for the human evaluation setup;
A Detailed Model Architecture
In ControlNet (Zhang and Agrawala, 2023) and Uni-ControlNet (Zhao et al., 2023), the weights of Stable
Diffusion (SD) (Rombach et al., 2022) are fixed and the input conditions are fed into zero-convolutions and
added back into the main Stable Diffusion backbone. Specifically, for Uni-ControlNet, they uses a multi-scale
condition injection strategy that extracts features at different resolutions and uses them for condition injection
referring to the implementation of Feature Denormalization (FDN):
FDN (Z,c) = norm (Z)·(1 + Φ (zero ( hr(c))))
+ Φ (zero (hr(c))), (7)
whereZdenotes noise features, cdenotes the input conditional features, Φdenotes learnable convolutional
layers, and zerodenotes zero convolutional layer. The zero convolutional layer contains weights initialized to
zero. This ensures that during the initial stages of training, the model relies more on the knowledge from
the backbone part, gradually adjusting these weights as training progresses. The use of such layers aids in
preserving the architecture’s original behavior while introducing structure-conditioned inputs. We use the
similar model architecture while we perform efficient training proposed in the main paper. We show the
model architecture in Figure 11.
19Under review as submission to TMLR
B Additional Implementation Details
In this section, we provide further details about the implementation aspects of our approach.
B.1 Additional Details of Structural Input Conditions Extraction
•Edge Maps : For generating edge maps, we utilized two distinct techniques:
–Canny Edge Detector (Canny, 1986) - A widely used method for edge detection in images.
–HED Boundary Extractor (Xie and Tu, 2015) - Holistically-Nested Edge Detection, an advanced
technique for identifying object boundaries.
–MLSD (Gu et al., 2022a) - A method particularly designed for detecting multi-scale line segments
in images.
•Sketch Maps : We adopted a sketch extraction technique detailed in Simo-Serra et al. (2016) to
convert images into their sketch representations.
•Pose Information : OpenPose (Cao et al., 2017) was employed to extract human pose information
from images, which provides detailed body joint and keypoint information.
•Depth Maps : For depth estimation, we integrated Midas (Ranftl et al., 2020), a robust method for
predicting depth information from single images.
•Segmentation Maps : Segmentation of images was performed using the method outlined in Xiao
et al. (2018), which focuses on accurately segmenting various objects within an image.
Each of these conditions plays a crucial role in guiding the text-to-image generation process, helping
FlexEControl to generate images that are not only visually appealing but also semantically aligned with the
given text prompts and structural conditions.
B.2 Additional Details of Evaluation Metrics
mIoU (Rezatofighi et al., 2019): Mean Intersection over Union, a metric that quantifies the degree of
overlap between predicted and actual segmentation maps.
SSIM (Wang et al., 2004): Structural Similarity, a metric evaluating the structural similarity in generated
outputs, applied to Canny edges, HED edges, MLSD edges, and sketches.
mAP: Mean Average Precision, utilized for pose maps, measuring the precision of localization across
multiple instances.
MSE: Mean Squared Error, employed for depth maps, MSE quantifies the pixel-wise variance, providing an
assessment of image fidelity.
FID (Heusel et al., 2017): Fréchet Inception Distance, which serves as a metric to quantify the realism
and diversity of the generated images. A lower FID value indicates higher quality and diversity of the output
images.
CLIP Score (Hessel et al., 2021; Radford et al., 2021): Employing CLIP Score, we gauge the semantic
similarity between the generated images and the input text prompts.
C Additional Qualitative Results on Video Generation
In this section, we showcase the extensibility of FlexEControl in controllable video generation. The results
are presented in Figure 12 and Figure 13, where results for providing one condition and multiple conditions
are demonstrated.
20Under review as submission to TMLR
Input condition
 Generation
Text Prompt:
A swimming ﬁshText Prompt:
   A house
Text Prompt:
   A birdText Prompt:
A hamburger
Figure 12: Results from FlexEControl on controllable text-to video generation (single condition).
D Additional Related Works
Knowledge Distillation for Vision-and-Language Models Knowledge distillation (Gou et al., 2021),
as detailed in prior research, offers a promising approach for enhancing the performance of a more streamlined
“student” model by transferring knowledge from a more complex “teacher” model (Hinton et al., 2015; Sanh
et al., 2019; Hu et al.; Gu et al., 2021; Li et al., 2021). The crux of this methodology lies in aligning the
predictions of the student model with those of the teacher model. While a significant portion of existing
knowledge distillation techniques leans towards employing pretrained teacher models (Tolstikhin et al., 2021),
there has been a growing interest in online distillation methodologies (Wang and Jordan, 2021). In online
distillation (Guo et al., 2020), multiple models are trained simultaneously, with their ensemble serving as the
teacher. Our approach is reminiscent of online self-distillation, where a temporal and resolution ensemble
of the student model operates as the teacher. This concept finds parallels in other domains, having been
examined in semi-supervised learning (Peters et al., 2017), label noise learning (Bengio et al., 2010), and quite
recently in contrastive learning (Chen et al., 2020). Our work on distillation for pretrained text-to-image
generative diffusion models distinguishes our method from these preceding works. (Salimans and Ho, 2022;
21Under review as submission to TMLR
Figure 13: FlexEControl on using multiple conditions for video generation.
Figure 14: Screenshot for human evaluation tasks on the Amazon Mechanical Turk crowdsource evaluation
platform.
Meng et al., 2023) propose distillation strategies for diffusion models but they aim at improving inference
speed. Our work instead aims to distill the intricate knowledge of teacher models into the student counterparts,
ensuring both the improvements over training efficiency and quality retention.
E Human Evaluation Interface
We give the human evaluation interface in Figure 14. The human evaluators are mainly asked to finish two
tasks and choose their preference from three perspectives.
22