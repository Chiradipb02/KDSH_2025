PublishedinTransactionsonMachineLearningResearch(12/2023)
IndicT rans2: T owards High-Quality and Accessible Machine
T ranslation Models for all 22 Scheduled Indian Languages
Jay Gala∗1Pranjal A. Chitale∗1,2Raghavan AK1,2V arun Gumma3†Sumanth Doddapaneni1,2
Aswanth Kumar6†Janki Nawale1Anupama Sujatha1Ratish Puduppully7Vivek Raghavan1,4‡
Pratyush Kumar1,2,3§Mitesh M. Khapra1,2¶Raj Dabre5Anoop Kunchukuttan1,2,3
1Nilekani Centre at AI4Bharat2Indian Institute of T echnology Madras3Microsoft4EkStep F oundation
5National Institute of Information and Communications T echnology, K yoto, Japan6Flipkart
7Institute for Infocomm Research (I2R), A∗STAR, Singapore
Reviewed on OpenReview: https://openreview.net/forum?id=vfT4YuzAYA
Abstract
Indiahasarichlinguisticlandscape,withlanguagesfrom4majorlanguagefamiliesspokenbyover
a billion people. 22 of these languages listed in the Constitution of India (referred to as scheduled
languages ) are the focus of this work. Given the linguistic diversity, high-quality and accessible
MachineTranslation(MT)systemsareessentialinacountrylikeIndia. Beforethiswork,therewas
(i) no parallel training data spanning all 22 languages, (ii) no robust benchmarks covering all these
languagesandcontainingcontentrelevanttoIndia,and(iii)noexistingtranslationmodelsthatsup-
port all 22 scheduled languages of India. In this work, we aim to address this gap by focusing on
the missing pieces required for enabling wide, easy, and open access to good machine translation
systemsforall22scheduledIndianlanguages. Weidentifyfourkeyareasofimprovement: curating
and creating larger training datasets, creating diverse and high-quality benchmarks, training multi-
lingual models, and releasing models with open access. Our first contribution is the release of the
Bharat Parallel Corpus Collection (BPCC), the largest publicly available parallel corpora for Indic
languages. BPCCcontainsatotalof230Mbitextpairs,ofwhichatotalof126Mwerenewlyadded,
including644Kmanuallytranslatedsentencepairscreatedaspartofthiswork. Oursecondcontribu-
tion is the release of the first n-way parallel benchmark covering all 22 Indian languages, featuring
diverse domains, Indian-origin content, and conversational test sets. Next, we present IndicTrans2,
the first translation model to support all 22 languages, surpassing existing models in performance
on multiple existing and new benchmarks created as a part of this work. Lastly, to promote acces-
sibility and collaboration, we release our models and associated data with permissive licenses at
https://github.com/AI4Bharat/IndicTrans2 .
1 Introduction
India is a linguistically diverse region, with 1,369 distinct mother tongues identified in the census conducted in 2011.
Of these, 22 languages have been listed in the 8thSchedule of the Constitution of India. Approximately 97% of the
populationofIndiaspeaksoneofthese22languagesastheirfirstlanguage. Englishiswidelyspokenandservesasthe
defaultmediumofformalcommunicationinmanyareas,particularlyinbusiness,education,government,andjudiciary.
∗EqualContribution. AllauthorcontributionslistedinSection 10.
†WorkdoneasaMaster’sstudentatNilekaniCentreatAI4Bharat,IndianInstituteofTechnologyMadras.
‡WorkdonewhileatNilekaniCentreatAI4BharatandEkStepFoundation.
§WorkdonewhileatNilekaniCentreatAI4Bharat,IndianInstituteofTechnologyMadrasandMicrosoft.
¶CorrespondingAuthor: MiteshKhapra(miteshk@cse.iitm.ac.in).
1PublishedinTransactionsonMachineLearningResearch(12/2023)
Withsuchlinguisticdiversity,theimportanceinIndiaoflanguagetranslationforeffectivecommunication,socialinclu-
sion, equitable access, and national integrity cannot be over-emphasized. For example, for effective dissemination of
informationaboutgovernmentpoliciesandwelfareschemes,itisnecessarytotranslateoﬀicialdocumentsandwebsites
intoregionallanguages. Inthecontextofthejudiciary, itiscrucialtotranslatecourtproceedingsandjudgmentsinto
regionallanguagessothatthepetitioners,accused,andwitnessescanunderstandandbetterparticipateinthejudicial
process. Similarly,inthecontextofeducation,translationcanensurethathigh-qualitycontentbecomesaccessibleto
morelearnersintheirregionallanguages. Lastly,translationalsoplaysavitalroleinnationalintegrationbyensuring
thatpeoplemigrating/travelingtoandfromdifferentpartsofthecountrycancommunicatebetterwithpeopleintheir
newlocations.
The last decade has seen rapid progress in Neural Machine Translation, with the latest neural models ( Johnson et al. ,
2017;Liuetal.,2020a;Fanetal.,2020;Kimetal.,2021;Lepikhinetal. ,2021;Rameshetal. ,2022;Costa-jussàetal. ,
2022;Siddhantetal. ,2022)supporting hundreds oflanguagesand thousands oftranslationdirections. However,these
models either do not have a good coverage of Indian languages, or their performance on Indian languages is poor, or
both. Further,noneofthesemodelsareevaluatedonadiversesetofdomainsorcontentofIndianorigin,asthereareno
robustbenchmarksdesignedexplicitlyforIndianlanguages. AnotherevidenceoftheneglectofIndianlanguagesisthat
inthepast16yearssinceitsinception,thesharedtasksrunundertheWorkshoponMachineTranslation(WMT)have
onlycoveredatotalof4Indianlanguagessummedacrossalltheseyears.1WhiletheWorkshoponAsianTranslation
(WAT) (Nakazawa et al. ,2022) and the Workshop on Speech and Language Technologies for Dravidian Languages
(Madasamy et al. ,2022) have made significant contributions, they have not garnered the same level of popularity or
academicparticipationastheWMT.Asaresult,despitetherapidprogressinthebroaderfieldofMachineTranslation,
nosinglecommercialoropen-sourcetranslationmodelsupports allthe22languageslistedintheConstitution.
Inthispaper,weposethefollowingquestion: What are the missing pieces required for enabling wide and easy access to
high-quality machine translation for all 22 scheduled Indian languages? Webelievetherearefouraxesofimprovement
required: (a) curation and creation of significantly larger training datasets , (b) creation of high quality and diverse
benchmarks , (c) training and evaluation of multilingual models, and (d) releasing of models with open access . For
axis(a)trainingdatasets,weneedtocreatehigh-quality“seeddata”comprisingmanuallytranslatedparallelsentences
forall22languageswithrepresentationfromdiversedomains. Itistobenotedthatforseveralofthe22languages,no
publiclyavailabletranslationdataexists. Thismanuallycreateddatahastobesupplementedwithahighervolumeof
semi-automaticallygenerateddatabybitextminingfromweb-scalemonolingualcorporaandmultilingualdocuments.
For axis (b) benchmarks, we need expert-created highly accurate benchmarks for all 22 languages across variations
such as formality of language, length of sentences, domain of text, and source originality. For axis (c) models, we
needtotrainaccuratemultilingualmodelsthatexploitthesimilaritybetweenIndianlanguagesandparticularlybenefit
low-resource languages. We also need to improve processes for the evaluation of models by choosing robust metrics
thatareshowntocorrelatewithhumanevaluationforIndianlanguages. Inaddition,weneedtoevaluatemodelswith
othermetrics,suchasimprovementinpost-editingperformance. Finally,foraxis(d)openaccess,createdmodelsmust
have permissive licenses that can be commercially deployed. For instance, Meta’s NLLB models, though released in
theopen,haveaCC-BY-NClicenseprecludingcommercialusage. Inthispaper,wecontributeacrossthesefouraxes
withmanynotablefirststhatwehighlightbelow.
Training datasets. We release the largest publicly available parallel corpora for Indic languages, the Bharat
Parallel Corpus Collection (BPCC) . As summarized in Table 1, BPCC contains a total of ~230M bitext pairs, of
whichatotalof~126Mwerenewlyaddedaspartofthiswork. BPCCincludesthefollowing:
•Seed training data containing human translations of English sentences to all 22 Indic languages spanning multiple
domains. This has a total of 644K En-X translation pairs across all languages, including 7 languages for which no
manuallycreatedparalleldataexistedbeforethiswork.
•BitextpairsfromexistingcollectionssuchasSamanantar( Rameshetal. ,2022)andNLLB( Costa-jussàetal. ,2022)
whichwerefurtherfilteredusingLaBSE( Fengetal. ,2022)basedcosinesimilaritythresholds.
1Thisis,ofcourse,notacommentontheorganizersofWMTbutareflectionofthelackofacademicinterestinIndianlanguagesduetothelack
ofsuﬀicienttrainingandevaluationdata
2PublishedinTransactionsonMachineLearningResearch(12/2023)
•New bitext pairs mined from additional monolingual sources such as archive.org and IndicCorp v2 ( Doddapaneni
etal.,2023)whichwerenotcoveredintheexistingcollectionsmentionedabove.
•Newbitextpairsminedfromadditionaldocument-alignedparallelsourcessuchasNPTEL,UGCResources,Prabhu-
padaVani,etc. whichwerenotcoveredintheexistingcollectionsmentionedabove.
•Averylargesetof~800millionback-translatedsentencesfromdiversesourcessuchasIndicCorpv2( Doddapaneni
etal.,2023),monolingualsideofNLLBdata( Costa-jussàetal. ,2022)andCC-Matrix( Schwenketal. ,2021b).
We visualize these types of data in BPCC in Figure 7, to highlight the language coverage and our contributions in
relation to existing data. As can be seen, for many languages, BPCC makes the first available datasets, and for all
languages,itmakesasignificantincreaseinthedatasetsavailable.
Benchmarks. We create IN22, the first n-way parallel benchmark covering all 22 Indian languages with the
English side being source-original. For benchmarks to be of high quality, they must represent content from diverse
domains. We visualize the diversity of our created benchmark in Figure 8. Our benchmark contains high-quality
human translations for sentences taken from India-specific articles belonging to 13 different domains, viz., Culture,
Economy, Education, Entertainment, Geography, Government, Health, Industry, Legal, News, Religion, Sports, and
Tourism (see left chart of Figure 8). We refer to this subset as IN22-Gen . Our benchmark has another subset IN22-
Conv,thatcontainstranslationsforsentencestakenfromeverydayconversationsintheIndiancontextfrom16different
domains, which were manually created by in-house experts starting from carefully created conversation prompts (see
rightchartofFigure 8).
Models. We release IndicTrans2 (IT2), the first translation model to support all the 22 scheduled Indian lan-
guages, trainedontheBPCCdataset. Theprogressmadeinthequalityoftranslationinthisworkwithexistingopen
modelsiscapturedinFigure 1. TheplotshowsthechrF++metricforEnglishtodifferentlanguages(whichisusually
themorechallengingtranslationdirectionforlow-resourcelanguages). Eachlanguageisrepresentedbycircles,where
the size of the circle represents the number of speakers in that language. As can be seen, with IndicTrans2, we made
progress in translation quality across languages and now support moderate to high-quality translation for most speak-
ers in India. Later in the paper, we also report COMET scores, comparisons with commercial models, and human
evaluations of our translations. We find that IT2 is the first model for Indian languages, which performs at par not
onlywithopen-sourcemodelslikeNLLB( Costa-jussàetal. ,2022)butalsowithcommercialmodelsfromGoogleand
Microsoft. We release IndicTrans2-M2M, the first model to support direct translations between all the 22 scheduled
Indiclanguages,supporting462translationdirections.
Open Access. WeaimtopromotewideraccesstoaccuratetranslationmodelsforallIndianlanguages. Therefore,we
willreleaseIndicTrans2anditsderivatives(IndicTrans2-M2M,IndicTrans2-Dist)underanopen-sourcelicense,along
with all training data, source code, and tools to enable replication and further improvements by the research commu-
nity. Additionally,weprovideIndicTrans2-Dist,approximately1/5thesizeofIndicTrans2(~211M)withcomparable
performancetoreducedeploymentcosts. WehopeourpaperwillserveasastartingpointforfutureresearchonIndic
machinetranslation.
Figure2provides a comprehensive overview of the entire workflow, which involved the development of requisite hu-
man infrastructure, building high-quality seed datasets and robust India-centric benchmarks, and culminates with the
release of IndicTrans2, which is the first model to support all the 22 scheduled languages. Section 3describes the
processfollowedforthecreationofhigh-qualitybenchmarksandseedtrainingdata,whichentailstheestablishmentof
ahuman infrastructure, followedbya detailedaccount ofthe translationworkflowandthe qualitycontrolprocedures
implemented. Subsequently, Section 4outlines our bitext mining pipeline, incorporating both manual and automated
checksthatemploytoxicityandlanguagefilters. Afterthecreationofthebenchmarksandtrainingdata,thenexttask,as
coveredinSection 5isthetrainingofIndicTrans2withablationofmodelarchitecture,datasetselections,andtraining
procedures. Furthermore,Section 6describestherobustevaluationofIndicTrans2acrossexistingbenchmarkssuchas
FLORESandthebenchmarkswecreate,acrossdiversemetricsandagainstbothopen-sourceandcommercialmodels.
3PublishedinTransactionsonMachineLearningResearch(12/2023)
M2M-100
(2020)IT1
(2021)NLLB MoE
(2022)IT2
(2023)0204060chrF++
AssameseBengali
Bodo
Dogri
KonkaniGujaratiHindi
Kannada
Kashmiri
MaithiliMalayalamMarathi
ManipuriNepali
OdiaPunjabi
Sanskrit
SantaliSindhiTamil
TeluguUrdu
AssameseBengali
Bodo
Dogri
KonkaniGujaratiHindi
Kannada
Kashmiri
MaithiliMalayalam
Marathi
Manipuri
NepaliOdiaPunjabi
Sanskrit
Santali
SindhiTamilTelugu
UrduAssameseBengali
Bodo
Dogri
KonkaniGujaratiHindi
Kannada
KashmiriMaithiliMalayalamMarathi
ManipuriNepali
OdiaPunjabi
Sanskrit
Santali
SindhiTamilTeluguUrdu
AssameseBengali
BodoDogri
KonkaniGujaratiHindi
Kannada
KashmiriMaithiliMalayalam
Marathi
ManipuriNepali
OdiaPunjabi
Sanskrit
SantaliSindhiTamilTeluguUrduHigh chrF++ score Moderate chrF++ score Low chrF++ score No MT System
Figure 1: A visual representation of the advancements in machine translation systems for Indic languages using the
IN22-Gen Evaluation set in the En-Indic direction. The depicted values have been subjected to minor adjustments to
enhancereadability;however,theyaccuratelyconveytheoveralltrend. Thresholdsareutilizedtoestimateperformance
boundariesforvarioussystemsacrosslanguages. Thesizeofeachlanguagebubbleisproportionaltothespeakercount
forthatlanguage(seeTable 55).
Thepaperconcludeswithacomprehensivesummaryandoutlinespotentialfutureresearchdirections. TheAppendices
providesupplementaryresultsandadditionaldetails,includingmodelanddatasetcards.
2 Related Work
Languages of India. India,withapopulationofmorethan1.4billion,isadiversecountryknownforitsrichlinguistic
heritage,andhometosomeoftheworld’smostwidelyspokenlanguages. AccordingtotheCensusofIndia2011,1369
mother tongues have been identified of which 121 languages have at least 10,000 speakers and 31 languages have at
least a million speakers.222 of these languages have been listed in the 8thSchedule of the Constitution of India3,
recognizingthemasthescheduledlanguagesoftheRepublicofIndia. Accordingtotheschedule,theGovernmentof
Indiaisunderanobligationtotakemeasurestodeveloptheselanguagessuchthattheybecomeaneffectivemeansof
communication. Nine of the Indic languages are amongst the most spoken languages across the globe4: Hindi
(4th), Bengali (6th), Marathi (13th), T elugu (14th), Tamil (17th), Urdu (20th), Punjabi (22nd), Gujarati (24th)
and Bhojpuri (26th).Some of these languages are also widely spoken and/or are oﬀicial languages in neighboring
countries viz., Bangladesh, Nepal, and Pakistan. Indianlanguagesarealso fast-growingacrosstheglobe, particularly
in North America, the United Kingdom, Australia, and the Middle East. Beyond the Indic languages, English is also
2https://en.wikipedia.org/wiki/Languages_of_India
3https://rajbhasha.gov.in/en/languages-included-eighth-schedule-indian-constitution
4https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers
4PublishedinTransactionsonMachineLearningResearch(12/2023)
Table1: Overallstatisticsfordatacollatedfromdifferentsources(inthousands)forIndianlanguagesandresourcesin
thiswork. Inthisdocument,eachlanguageisidentifiedwithaBCP47tagsequencecomprisedofISO639-3language
subtagandISO15924scriptsubtag.
Existing BPCC(NewlyAdded)
Mined Human Mined Human
Name Language Samanantar NLLB NLLB ILCI MASSIVE Monolingual Comparable Wiki Daily
Assamese asm_Beng 58.8 506.3 - 82.1 -712.5 37.8 44.7 11.3
Bengali ben_Beng 2,946.3 13,580.5 - 123.8 16.5 16,055.1 258.2 48.0 8.5
Bodo brx_Deva -- - 83.2 - - <1 22.7 10.3
Dogri doi_Deva -- - - - - - 18.7 5.5
Konkani gom_Deva -- - 74.5 - - - 18.3 4.8
Gujarati guj_Gujr 1,379.2 7,090.3 - 107.4 - 11,630.3 573.0 25.0 3.2
Hindi hin_Deva 4,416.7 6,646.7 - 165.6 16.5 27,187.8 853.3 40.3 8.4
Kannada kan_Knda 1,692.2 8,871.1 - 76.4 16.5 12,501.0 380.2 32.2 8.5
Kashmirikas_Arab - 124.9 6.2 - - - - 15.5 4.3
kas_Deva - 194.0 6.2 - - - - - -
Maithili mai_Deva - 62.2 - - - - <1 24.4 4.2
Malayalam mal_Mlym 2,029.2 8,818.2 - 87.9 16.5 12,378.6 356.4 41.6 8.4
Marathi mar_Deva 1,366.1 6,393.2 - 117.0 - 10,806.0 432.4 54.3 4.6
Manipurimni_Beng - 346.9 6.2 13.1 - -20.1 - <1
mni_Mtei -- - 16.0 - - - 19.9 6.8
Nepali npi_Deva - 1,583.5 - 28.6 - 10.5 6.2 45.9 10.9
Odia ory_Orya 514.9 2,382.6 - - - 2,863.1 121.5 33.7 3.2
Punjabi pan_Guru 1,418.3 1,978.3 - 71.5 - 6,275.8 207.2 6.3 3.2
Sanskrit san_Deva - 244.1 - - - - <1 27.7 5.4
Santali sat_Olck -- - - - - - 22.5 1.8
Sindhisnd_Arab - 2,128.4 - - - - - - -
snd_Deva -- - - - - - 10.5 -
T amil tam_T aml 1,833.2 8,665.2 - 120.7 16.5 9,690.3 452.8 21.0 8.6
T elugu tel_T elu 1,780.5 10,062.8 - 73.6 16.5 11,100.0 437.2 29.7 8.5
Urdu urd_Arab - 5,321.0 - 101.0 16.5 484.9 225.3 41.3 8.4
# T otal 19,435.4 84,998.3 18.6 1,342.6 115.4 121,695.8 4,353.1 644.3 139.7
widely spoken by in India, with a speaker base of 246 million.5However, even with a large speaker base, many of
theselanguagesstilllackanonlinepresenceandhigh-qualityNLPtechnologies. Ofthe22scheduledlanguages,only
4 of them are so-called “Winners” according to the classification by Joshi et al. (2020). It is thus essential to support
translation technologies (and NLP technologies in general) for such a large population base to bring the benefits of
digitaltechnologiestoalargeaudience. WhatdistinguishestheIndiansubcontinentisnotonlythelargespeakerbase
of many languages but also the linguistic diversity of its languages. Languages from four major language families
(Indo-Aryan branch of the Indo-European family, Dravidian, Tibeto-Burman, and Austro-Asiatic) are spoken
in the subcontinent. According to Wikipedia,6India has amongst the highest linguistic diversity at around 0.914
to 0.93, depending on the measure. Indic languages are written in a variety of scripts, the majority of which are
derivedfromthe Brahmiscript. Up to 12 major scripts spanning abugida, alphabetic, and abjad script types are
used(Daniels & Bright ,1996). Underlying this diversity in languages and scripts is also a great deal of similarity at
variouslinguisticlevels,owingtolanguagerelatednessandcontactoveralongperiod( Emeneau,1956;Subbarao,2012;
Kunchukuttan&Bhattacharyya ,2020).The diversity of languages and their interactions provide for challenging
problems and opportunities in machine translation for Indic languages .
Datasets. We summarize some of the prominent parallel corpora created for Indian languages. The Indian Lan-
guages Corpora Initiative (ILCI) ( Choudhary & Jha ,2011) created n-way parallel annotated corpora containing 50K
5https://en.wikipedia.org/wiki/Indian_English
6https://en.wikipedia.org/wiki/Linguistic_diversity_index
5PublishedinTransactionsonMachineLearningResearch(12/2023)
IN22
Wiki + W eb (Gen)
IN22 Seed Data
Cleaned Existing
Data
Document Aligned
Parallel Data
Webscale Mined
DataIN22
Conv 
Other Human
Labelled Data
Bharat Parallel
Corpus CollectionIN22 T ranslation
Benchmark
LaBSE
FilterThreshold
Identification
LaBSE
MiningToxicity
LID FiltersManual
ReviewHuman
TranslationAutomatic
Copy
Checks
Source
Identification
VerificationFLORES NTREX
WMT WAT
UFAL
BLEU chrF++ COMETSection 7
IndicT rans2
Indic-En
En-Indic
Section 6
Human
EvaluationNewly Added
Existing Data
Web-Scale
CorporaDocument
Aligned
CorporaExisting
Parallel DataSource
SelectionConv .
PromptsEnglish
Conversations
Automatic
Evaluation
 Train Data 
 Test Data  784K 
 104.4M 
 4.3M 
 121.7M 
 1.4M  230.5M Human Annotation on Shoonya
Bitext Mining Section 5Section 410241503
Automated
Human Ef fortDomain Coverage
Length Distribution
Permissible LicenseTopic Selection
Prompt & Scenario
CreationBenchmarks
Figure2: OverviewoftheworkflowusedforbuildingBharatParallelCorpusCollection,IN22andIndicTrans2.
sentences per language for 12 major Indian languages, covering Health and Tourism domains. However, with the
advent of neural MT models, it has been established that these models need large-scale parallel corpora for superior
performance( Edunovetal. ,2018;Aharonietal. ,2019). SomeearlyattemptsincludetheIIT-BombayEnglish-Hindi
corpus(Kunchukuttanetal. ,2018)andthePMIndiacorpus( Haddow&Kirefu ,2020),whichalignedsentencesfrom
the Prime Minister’s speeches in English and 12 Indic languages. The CVIT-PIB corpus ( Philip et al. ,2021) aligned
paralleldocumentsfromthePressInformationBureauarchives,resultinginEnglishto11Indianlanguagepairs. WAT
2021 shared task compiled existing sources to create 9 million sentence pairs between English and Indic languages.
Creating parallel corpora for all Indic languages is challenging due to the lack of identifiable parallel documents and
theeffortrequiredforhumanannotationatscale. Consequently, attentionhasturnedtowardsminingparallelcorpora
fromnon-comparablesources,leveragingthemultilingualnatureofIndia’sinformationavailability,thoughidentifying
parallelpagesbasedonURLpatternsremainschallenging( Resnik&Smith ,2003). Followingpriorworksonmining
data from web-scale data ( Schwenk et al. ,2021b), Samanantar ( Ramesh et al. ,2022) was mined from IndicCorp v1
(Kakwani et al. ,2020) using LaBSE ( Feng et al. ,2022) based sentence embeddings, resulting in a 3-fold increase in
data compared to existing parallel data. Combined with existing data, Samanantar contained 49.7 million sentence
pairs between English and 11 Indic languages. In subsequent work, NLLB project ( Costa-jussà et al. ,2022) mined
paralleldatafromCommonCrawldumps( Wenzeketal. ,2020)usingLASER( Heffernanetal. ,2022)basedsentence
embeddings. This corpus resulted in 448 million sentence English-centric pairs covering 19 Indic languages. While
NLLB(Costa-jussàetal. ,2022)hadthelargestcoveragesofar,alltheseeffortsstilldonotcoverallthe22scheduled
languagesofIndia. Thisnecessitatestheneedtocreate“seed”data(referto §3)forthelow-resourcelanguagestohelp
boosttheperformanceofMTsystemsfortheselanguages.
Benchmarks and Shared Tasks. Benchmarks have improved NLP systems across various tasks ( Rajpurkar et al. ,
2016;Wang et al. ,2018;2019;Hu et al.,2020;Doddapaneni et al. ,2023). Over the years, an increasing focus has
beenonimprovingMTsystemsforIndiclanguages,withsustainedendeavorstodevelopappropriatebenchmarks. The
introductionoftheHindi-EnglishMTchallengeinWMT’14markedoneoftheearliestattemptstoestablishbenchmarks
forIndiclanguages( Bojaretal. ,2014). Subsequently,WMTextendeditseffortsbyincorporatingtheGujarati-English
and Tamil-English language pairs in 2019 ( Barrault et al. ,2019) and 2020 ( Barrault et al. ,2020), respectively. WAT
(Workshop on Asian Translation) has continuously supported IndicMT with the inclusion of the IITB Hindi-English
dataset (Kunchukuttan et al. ,2018) in the WAT 2016. Subsequently, WAT expanded its efforts, adding 6, 8, 10, and
6PublishedinTransactionsonMachineLearningResearch(12/2023)
15 languages in 2018, 2020, 2021, and 2022, respectively ( Nakazawa et al. ,2018;2020;2021a;2022).Siripragada
et al.(2020) introduced a benchmark consisting of roughly 2K-3K sentences from Mann ki Baat7, covering 9 Indic
languages translated to English. FLORES 101 ( Goyal et al. ,2022) was one of the first attempts to create a large-
scale MT benchmark with n-way parallel devtestand held-out testsets of around 1000 sentences for 101 languages,
including support for 14 Indic languages manually annotated from the Wikimedia content. This was followed up by
NLLB (Costa-jussà et al. ,2022), extending the total language coverage to 200, which includes 19 Indic languages
listed in the Constitution (plus a few more Indic languages). NTREX ( Federmann et al. ,2022) expanded coverage of
languages of test data from WMT 2019 ( Barrault et al. ,2019) to 128 languages and covers 16 Indic languages. The
testsetcontains1997manuallytranslatedsentences,primarilysourcedfromthenewsdomain.
Neural MT models. TheintroductionofNeuralMTandthecreationoflarge-scaleparallelcorporaledtosignificant
advancements in the field of Indic MT. Broadly, they follow the Embed - Encode - Attend - Decode approach. Initial
approachesusedRecurrentNeuralNetworks( Bahdanauetal. ,2015)andlatertransformer-basedapproaches( Vaswani
etal.,2017)becamemoreprominent. Theintroductionofattentionandsubword-basedmodelingaddressedtheissues
ofwordorderinganddatasparsity. Themodelswereabletogenerategrammaticallyfluentandaccurateoutputs. Some
noteworthyNeuralMTmodelsstudyingIndianlanguagesinclude( Philipetal. ,2021;Rameshetal. ,2022;Fanetal.,
2020;Costa-jussà et al. ,2022). These were followed up with multilingual and pre-trained MT models ( Kudugunta
etal.,2019;Liuetal.,2020b;Xueetal.,2021;Dabreetal. ,2022). Thesemodelswereabletotransferknowledgefrom
high-resourcetolow-resourcelanguagesbyleveraginglargeamountsoftrainingdataandlanguagesimilaritiesacross
languages,makingitpossibletotrainagood-qualityMTsystemforlow-resourcelanguages( Dabreetal. ,2021). Over
the last few years, large corpora ( Ramesh et al. ,2022;Costa-jussà et al. ,2022) and larger models ( Fan et al.,2020;
Costa-jussà et al. ,2022) marked significant improvements in the translation quality. Recent work has also explored
translation for extremely low-resource languages with hardly any parallel corpora and limited monolingual corpora
(Costa-jussàetal. ,2022;Bapnaetal. ,2022;Mauryaetal. ,2023).
3 Creating High-quality T ranslation Datasets at Scale
Inthissection,wedescribethetranslationprocess,andtheShoonya8infrastructuretoensureahigh-qualitytranslation
workflow. Wealsodescribeindetailthetranslationworkflowfollowedandqualitycontrolproceduresandthesalient
features of the resultant datasets created: (a) BPCC-Human, the training dataset from English to 22 Indic languages,
and(b)IN22,thetestsetfortranslationevaluationbetweenEnglishandIndianlanguages.
3.1 T ranslation Workflow
The overall translation workflow is described below and illustrated in Figure 3. The translation workflow comprises
fourstages. First,sentencesfortranslationarechosenbasedoncriteriasuchasdomaincoverage,length,andlicensing.
Thesesentencesaresourcedfromdiversedomains,includingNews,Business,andHealth. Next,theselectedsentences
undergo a verification process where annotators ensure their quality and correctness, tagging them accordingly. The
entireparagraphisrejectedincaseofanyinaccuratesentencestopreventambiguity. Oncetheverificationiscomplete,
the sentences are translated into 22 Indic languages, adhering to rigorous guidelines. Lastly, the translated content is
reviewedbyexperiencedtranslatorswhocheckforadherencetoguidelinesandoverallquality,suggestingimprovements
orcorrectionsasneeded. Ifatranslationisrejected, itissentbacktotheoriginaltranslatorforrevision, ensuringthe
highesttranslationstandards. Specificcustomizationstotheworkflowdependingonthekindofdatasetbeingcreated
(training/test)arediscussedinsubsequentsections.
Allthestagesintheworkflowareperformedon Shoonya,8anopen-source9platformwhichwasdevelopedasapartof
thisworkforsupportinglanguageannotationtaskscustomizedforIndianlanguages. Additionalinformationaboutthe
translationstages,includingtranslationguidelinesandtheinterfaceutilizedforgeneratinghuman-annotatedtranslation
dataalongwithitskeyfeatures,canbefoundinAppendix F.
7https://www.pmindia.gov.in/en/mann-ki-baat/
8https://ai4bharat.org/shoonya
9https://github.com/AI4Bharat/Shoonya
7PublishedinTransactionsonMachineLearningResearch(12/2023)
Source V erification
Sentence V alidity
Quality V erification
Toxicity Filters
Meta DataSource Selection
Domain Coverage
Length Distribution
Permissible LicenseQuality Checks
Maker-Checker Review
Auto Plagiarism Check
Linguistic Fairness
Source T ranslation
Translate to IN22
MT Assistance
Glossary Support
Active Discussion
Figure3: TranslationworkflowinShoonya
3.2 Building the IN22 T est set
Inthissection,wedescribetheIN22testset,whichisanewmanuallycreatedn-wayparalleltestsetcoveringEnglish
and 22 Indic languages. We motivate the need for such a benchmark, describe its features in detail, and explain the
constructionofthetestset.
While there are a few test sets for Indian languages, there is still a need for a comprehensive test set that satisfies the
followingneedsofIndianlanguagemachinetranslationandaddressesthelimitationsofexistingtestsets:
•We need a test set that covers all 22 Indic languages and enables evaluation between all possible pairs of these
scheduledlanguages. FLORES-200( Costa-jussàetal. ,2022)hasthelargestcoverageamongstexistingtestsets(n-
way,19languages). TheothertestsetsWAT2020( Nakazawaetal. ,2020),WAT2021( Nakazawaetal. ,2021a),WMT
2014(Bojaretal. ,2014),WMT2019( Barraultetal. ,2019),WMT2020( Barraultetal. ,2020),UFAL(Ramasamy
etal.,2012)andNTREX( Federmannetal. ,2022)havelimitedcoverage,withthemajorityhavingonlyafewofthe
top-10languagesrepresentedatthemost.
•Thetestsetshouldbediverseintermsofdomainscoveredandrepresentarealisticdistributionofsentencelengths
whilealsoencompassingtopicsrelevanttoIndia,whichwouldbetheprimaryusecaseformodelssupportingIndic
languages. ExistingtestsetslikeWMTandFLORESaremoregeneral-purposeandhavelimitedrepresentationfor
Indiantopicslikenamedentities,locale,culture-specificterms,etc.
Table2compares existing benchmarks based on test set size, language coverage, domain coverage, and the language
inwhichthedatasetissourceoriginal.
3.2.1 Corpus Description
WedescribethedetailsandsalientpointsoftheIN22testset. Thistestsetcomprisesthreesubsets,whichservedistinct
evaluationscenarios:
•Multi-Domain Wikipedia subset (512 sentences) : Thissubsetisdesignedtobemulti-domain,expandingtoatleast
five more domains than the existing benchmarks like FLORES-200 ( Costa-jussà et al. ,2022). Domain coverage is
presentedinTable 52.
•Multi-Domain Web Sources subset (512 sentences) : This subset was designed to represent content from sources
otherthanWikipediatohavemorediversityincontentandwritingstyleandwithmorefocusonIndia-centriccontent.
These were mainly sourced from PDFs and from sources that are not accessible or crawlable on the web, thereby
reducingthepossibilityofthesesentencesalreadybeingpartofanymineddata.
8PublishedinTransactionsonMachineLearningResearch(12/2023)
Table 2: Comparison of Various Benchmarks based on Test Set Size, Language Coverage, Domain Coverage, and
SourceOriginal.
Dataset TestSetSize LanguageCoverage DomainCoverage SourceOriginal
FLORES-200(devtest) 1012 19 8 eng
NTREX 1997 12 news(1) eng
WMT2014(hin) 2507 1 news(1) both
WMT2019(guj) ≈1000 1 1 both
WMT2020(tam) ≈1000 1 1 both
WAT2020 ≈3500 7 1 eng
WAT2021 ≈2390 10 1 eng
UFAL 2000 1 3 eng
IN22-Wiki 512 22 13 eng
IN22-Web 512 22 13 eng
IN22-Conv 1503 22 16 eng
•Conversation Translation Benchmark (1503 sentences) : Thissubsetwasdesignedtoevaluatetheperformanceof
models in day-to-day conversations in applications like chat. The translations are drawn from a multi-turn English
dialog dataset we built, enabling evaluation across all the axes, including sentence level, turn level, and document
level(completeconversation).
Thefollowingaresomekeyfeaturesofthebenchmark:
•Itisan n-wayparalleltestsetcontaining2527originalEnglishsentencestranslatedinto22Indiclanguageswithhigh-
qualitytranslationsdonebyin-housetranslatorsfromscratchwithoutrecoursetoanyexistingMTsystem. Metadata,
consistingofdomainsandcontextsentences(inraw,uneditedformat)forsourcesentences,isprovidedinthetestset
toenableafine-grainedanalysisoftranslationqualityforeachexample.
•IN22enablesevaluationin500+directions,including(i)sourceoriginaltranslationfromEnglishtootherlanguages.
(ii)IndictoEnglishtranslationevaluationandtheabilitytostudyrelativelanguageperformancesincetheunderlying
sentenceisthesame,(iii)comparisonof462inter-Indictranslationdirections.
•Thetestsetisdiverseintermsofthedomainscoveredandthedistributionofsentencelengths. TheWebsourcesand
Wikipediasubsetscover13domains, whiletheconversationalsubsetcovers16domains. Thelengthdistributionis
chosentoreflectarealisticdistributionwhilealsohavingasuﬀicientnumberoflongsentences,whichcanpresenta
challenge to MT models. Figure 10provide an overview of the domain v/s length distributions of our benchmarks,
whileTable 52providesanoverviewofthedomaindiversity.
•Table3provides some statistics about the test set. Wikipedia and Web Sources have longer sentences than the
conversational dataset. Conversational sentences have a higher perplexity compared to the other subsets, perhaps
hintingatthelowerrepresentationofsuchscenariosintheGPT2trainingcorpus.
3.2.2 Source Selection
We describe the selection of the source sentences for each of the three subsets: Wikipedia, Web Sources, and Con-
versation. The creation of the Wikipedia subset involved selecting English source sentences from various Wikipedia
categories to ensure broad coverage across different domains. Sentences were filtered based on length (less than 6
words or more than 80 words were discarded) and overlap with the FLORES-200 test set (4-gram overlap). For each
sentence, a context window of 3 sentences (typically one before and one after) was constructed. The Web Sources
subsetfocusedonIndiantopicsandusedGovernmentofIndiawebsitesanddigitallibrariesassources,withsentences
selected using a similar procedure. The Conversation subset involved creating English conversations with predefined
prompts and scenarios, which were then translated into 22 Indic languages. Overall, these subsets were created with
carefulconsiderationfordomaindiversityandlanguagecoverage. Appendix E.1providesdetailedinformationabout
theprocedurefollowedfortheselectionofsentencesforallthethreesubsetsofIN22.
9PublishedinTransactionsonMachineLearningResearch(12/2023)
Table3: StatisticsforthethreesubsetsintheIN22benchmark.
Subsets
Wikipedia WebSources Conversational
Numberofsentences 512 512 1503
Averagesentencelength(numberofEnglishcharacters) 169.27 144.53 54.18
Averagesentencelength(numberofEnglishwords) 26.30 23.20 9.88
Numberofcontextsentencesavailable 3 3 conversation
Numberofdomains 13 13 16
AverageperplexityofEnglish(computedusingGPT-2) 63.67 67.22 72.33
Table53containsthestatisticsoftheconversationsubsetofIN22testset. Thesubsetcontainsconversationssampled
from16domainsincluding‘arts’,‘history’,‘schoollife’,etc. Thedomainscoveradiversesetoftopicssuchas‘Govern-
mentschemes’,‘Movies’,‘HistoricalArchitectures’,etc. Table 54containsanEnglishexamplefromtheconversation
subsetofIN22testset. TheconversationsubsetofIN22benchmarkcanalsoberepurposedasadocumenttranslation
taskandwouldbeusefulinthecontextofevaluatingLLMs.
3.2.3 Quality Control Procedure.
In the process of test set creation, it is imperative to implement strict quality control guidelines to prevent the use of
MT outputs as a starting point by translators and ensure the fairness and reliability of the resulting benchmarks. As
a first step, we disable MT outputs in Shoonya for this translation task. To further ensure translators are not taking
recourse to MT outputs, we follow a systematic approach that involves conducting pairwise comparisons between
human translations and the outputs of widely accessible machine translation (MT) systems, such as Google, Azure,
NLLB (Costa-jussà et al. ,2022), and IndicTrans1 ( Ramesh et al. ,2022). The BLEU score ( Papineni et al. ,2002)
serves as an effective metric for detecting exact matches between translations and MT system outputs. Initially, we
generatepredictionsfrommultipleMTsystemsforabatchofsentencestranslatedbyanannotator. Subsequently, we
computeBLEUscores,denotedas B(Si, T),withrespecttothereferencetranslations( T)andeachMTsystemoutput
(Si). AseriesofconditionsareassessedbasedonthenumberofMTsystemssupportingaparticularlanguage(denoted
ask). ForlanguagessupportedbymultipleMTsystems,thesystemwiththehighestBLEUscore( Sj)isselected,where
j=argmax iB(Si, T).
|B(Si, T)−B(Sj, T)| ≤δ ∀i, j∈ {1, . . . , k } (1)
IfthepairwiseBLEUscoredifferencebetweenanytwosystemsfallswithinanacceptablethreshold(seeEquation( 1)),
then the translations are accepted. In this work, we set the δto be 10. Otherwise, a high difference in BLEU scores
indicates that the high-scoring model might have been a source for translation. In cases of high overlap with any of
themachinetranslationsystems,anewannotatorisassignedtothetask,andthequalitycontrolprocedureisrepeated,
ensuringthecreationofreliableandaccuratebenchmarks.
3.3 Building the BPCC T raining Set
WecreateBPCC-Human(BPCC-H),amanuallytranslated,multi-domain n-wayseedparallelcorpusbetweenEnglish
and22Indiclanguages.10Inthissection,wemotivatetheneedforhigh-quality,human-translatedtrainingdata,provide
anoverviewofthedataset,anddescribetheprocessofconstructionofthedataset.
Motivation for creating the seed dataset. Theprimarymethodtocreateparallelcorporaatscaleformanylanguages
isto minedata frompubliclyavailablesources. Whilethis approachhas shownsuccess forlanguagesthat havegood
10Currently,theseedcorporabeingreleasedarenotn-wayparallelsincedifferentlanguageteamsareindependentlytranslatingdifferentbatches
oftheEnglishsourcesentences. Thisisongoingwork.
10PublishedinTransactionsonMachineLearningResearch(12/2023)
representationinmonolingualcorpusandmultilingualmodels( Rameshetal. ,2022;Philipetal. ,2021;Kunchukuttan
et al.,2018), the same cannot extend to very-low resource languages. This makes it important to invest in building
high-quality, modest-sized parallel corpora. We take inspiration from previous efforts to manually create large multi-
lingualseedcorporaexplicitlyforbuildingmachinetranslationmodelslikeILCI( Jha,2010),ALT(Rizaetal.,2016),
and NLLB-Seed ( Costa-jussà et al. ,2022;Maillard et al. ,2023). These previous efforts have been instrumental in
significantly boosting MT efforts for low-resource languages; particularly, seed data also helps in bootstrapping the
developmentofvariousNLPtoolssuchaslanguageidentifiers,topicclassifiers,namedentityrecognition,etc.,where
minimalmonolingualsourcesexist.
3.3.1 Corpus Description
FollowingaresomekeyaspectsoftheBPCC-Hdataset:
•BPCC-H-Wiki is the largest publicly available manually translated multi-domain parallel corpora in terms of lan-
guage coverage. It contains a total of 644.3K sentence pairs, ranging from 6.3K to 54.3K pairs depending on the
language, averaging around 26K sentence pairs per language pair. These translations were performed by qualified
professional translators following a high-quality translation process and a systematic review of the sentence pairs,
unlikecrowdsourcingefforts. Per-languagesentencecountscanbeseeninTable 1.
•BPCC-H-Wikiprovides good seed parallel corpora for 4 extremely low-resource languages without public corpora,
viz.Bodo,Dogri,Santali,andGoanKonkani. Morethan10Ksentencepairsareavailableforeachoftheselanguages.
Therearehardlyanysourcesormodelstomineparallelcorporafortheselanguages.
•There are multiple scripts available for a few languages. However, for our current seed data creation efforts, we
restrictourselvestoonlyonescriptperlanguage,choosingthemostwidelyusedscriptforadministrativepurposes.
•A subset of BPCC-H, BPCC-H-Daily comprises spoken text particularly covering various types of sentences com-
monly used in different day-to-day scenarios, such as queries, commands, and feedback, across a range of applica-
tions including digital payment apps, grocery/food delivery apps, and government services apps. Our goal was to
encompassdiversenamedentitiesinrelevantdomains, coveringvariousexpressionsfromtheseservices. Thissub-
set, comprising 139.7K bitext pairs in 21 Indic languages except Sindhi, was developed from English sentences to
expandthediversityoftheparallelcorpora.
3.3.2 T ranslation Details
Thetranslationprocesshasalreadybeendescribedabove. Here,wediscussaspectsofthetranslationprocessspecific
toBPCC-H.
First,wechoosetotranslatefromEnglishsourcesentencestoIndiclanguagesinordertosimplifythesourcesentence
selection(easieravailabilityofcopyright-freeEnglishsentencesfortranslation,diversityindomains, etc.). TheIndian
language side, therefore would exhibit translationese effects ( Zhang & Toral ,2019). However, this is not uncommon,
andmanyparallelcorporaareEnglishoriginal( Costa-jussàetal. ,2022;Maillardetal. ,2023;FitzGeraldetal. ,2022).
The English source sentences were selected from Wikipedia. We identified various Wikipedia categories of interest
and then identified article pages within those categories. This was done to ensure broad coverage of domains. We
identified a block of three sentences following Goyal et al. (2022), of which one was to be translated, and the others
wouldbecontextsentencestoresolveanyambiguitiesduringtranslation. Thetranslatorshadtheoptionofpost-editing
MToutputsfromanexistingmodelwhereverfeasible.
4 Mining T raining Data at Scale
The quality of MT systems depends on access to good quality parallel data, and increasing parallel corpora improves
translationquality( Khayrallah&Koehn ,2018). However,obtaininghigh-qualityparallelcorporainlargequantitiesis
achallengingtask. Whilehumanannotationisonewaytosourcedata,itisnotscalablebeyondacertainpointtomeet
11PublishedinTransactionsonMachineLearningResearch(12/2023)
the demands of data-hungry models. Thus, there is a growing need to (semi-)automatically mine large-scale training
corporatoaddressthisissue.
Overtheyears,variousapproacheshavebeenproposedforgeneratingparalleldataformachinetranslation(MT)train-
ing. One set of approaches focused on mining parallel corpora from aligned documents identified from web-corpora
(Resnik&Smith ,2003;Bañónetal. ,2020;El-Kishkyetal. ,2020)orfromspecificdocumentcollectionslikeEuroParl
(Koehn,2005) and the United Nations ( Ziemski et al. ,2016). Document alignment is a non-trivial problem for open
web-corpora and relies on URL matching or translation-based matching in constrained settings. Specific document
collections may be limited in domain coverage and are often scarce. Instead of limiting mining to comparable docu-
ments, recent methods have explored the mining of sentence pairs from large sentence collections using multilingual
embeddings without regard for document alignment. This has allowed the mining of parallel data from arbitrary and
diversecollectionsofdata( Schwenketal. ,2021a;b;Costa-jussàetal. ,2022). Similarapproacheshavebeenextended
toIndiclanguages( Rameshetal. ,2022),establishingtheutilityoflarge-scaleminingforbuildingmultilingualNMT
models.
Major Indic languages have a reasonable online presence, with numerous websites publishing data in multiple Indic
languages, primarily pivoting through English or Hindi. Moreover, being a multilingual nation, several government
documents, books, judgments, legal proceedings, etc., are published in multiple Indic languages, which are directly
comparableandaretherebyalignedatadocumentlevel. Hence, weinvesteffortsinminingparallelcorporabylever-
aginglarge-scalemonolingualdataaswellasdocument-aligneddatafromcomparablesources.
Our mining efforts focus on 12 Indic languages: Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi,
Odia, Punjabi, Tamil, Telugu, and Urdu. These languages have a good representation in monolingual corpora, as
reported in Doddapaneni et al. (2023). However, the low-resource languages have comparatively lesser monolingual
data,andthequalityofsentenceembeddingsisunknown. Therefore,werelyonhigh-qualityhuman-translateddata,as
describedinSection 3, fortraininglow-resourcelanguages. Nepaliwasalsoconsideredinaninitialroundofmining,
andsomebitextdatawasmined. However,itwasdroppedfromminingsubsequentlysinceLaBSEembeddings( Feng
etal.,2022)wereobservedtobesuboptimalforNepali. Goingforward,weonlyfocusonminingparallelcorporafor
the12languagesmentionedabove.
Table1providesstatisticsoftheminedparallelcorpora. Thefollowingisasummaryoftheminedcorpora:
•In our mining efforts, a total of ~126 million sentence pairs were mined in addition to existing corpora, resulting
in an aggregated collection of ~230.5 million sentence pairs after deduplication, which is ~ 5×increase in parallel
corporasizeascomparedto Rameshetal. (2022).
•Mining from the monolingual corpus resulted in the largest parallel corpus gains, with 121 million sentence pairs
across13Indiclanguages.
•Mining from comparable corpora results in a diverse parallel corpus covering a wide range of topics like Religion,
Education,Legal,etc. Intotal 4.35millionsentencepairswereminedacross17Indiclanguages.
•Filteringexistingcorporaturnedouttobeanimportantexercise,asweobservedaround75%ofthedatawasdiscarded
duetopoorqualityofalignment. Insummary, Costa-jussàetal. (2022)wasfilteredandtherebyreducedfrom448.1
millionto~85millionsentencepairs,and Rameshetal. (2022)reducedfrom49.7millionto19.4millionsentence
pairs. Wedescribethefilteringprocessbelow.
4.1 Mining from Monolingual Corpora
Theprimaryideabehindminingparallelsentencepairsfromlargecorporaistorepresentsentencesfromalllanguagesin
acommonembeddingspaceusingLaBSE( Fengetal. ,2022),suchthatthedistancebetweenapairofsentencesreflects
their semantic difference. To achieve this, we project all the sentences into a shared space and search for the nearest
neighbors around a query sentence. Given a source sentence Sin language L, we look for the closest Approximate
NearestNeighbors(ANNs)to SLwithinaselectedthreshold. Themainchallengeliesinscalingthisprocesseﬀiciently
12PublishedinTransactionsonMachineLearningResearch(12/2023)
Archive
Books
IndicCorp
Wikipedia
Mined
BitextEnglish
IndicFAISS Index
Query V ectors
Toxicity
LID FiltersLaBSE
Filter
LaBSE MiningLaBSE
Figure4: MiningworkflowforMonolingualcorpora
Table 4: The total number of monolingual sentences and extracted parallel sentences count (in millions). The size of
the English monolingual corpus is 429Million. †indicates the mining for Nepali was performed on an intermediate
versionofIndicCorpv2( Doddapanenietal. ,2023).
Language Monolingual Corpus Extracted P airs
asm_Beng 3.3M 0.7M
ben_Beng 269.5M 16.0M
guj_Gujr 115.5M 11.6M
hin_Deva 473.2M 27.1M
kan_Knda 101.7M 12.5M
mal_Mlym 91.8M 12.3M
mar_Deva 64.7M 10.8M
npi_Deva†- 0.01M
ory_Orya 13.4M 2.8M
pan_Guru 38.6M 6.2M
tam_T aml 64.7M 9.6M
tel_T elu 108.5M 11.1M
urd_Arab 76.2M 0.4M
# T otal 2113M 121M
to project millions of sentences and compute nearest neighbors over a large search space in a scalable and eﬀicient
manner. Previouswork,suchasCCMatrix( Schwenketal. ,2021b),hasdemonstratedthatANNsearchcanbeeﬀiciently
performedatscaleusingquantization,eﬀicientindexing,andretrieval. Similarapproacheshavebeenusedinpriorwork
on Indic languages, such as Samanantar ( Ramesh et al. ,2022). Our work follows the same approach as Samanantar
forminingparallelsentencesfromlarge-scalemonolingualcorpora. WedifferfromSamanantar( Rameshetal. ,2022)
primarilyintheamountofmonolingualdatausedformining. Weusealargercollectionofmonolingualcorporaforour
work,comprisingIndicCorpv2( Doddapanenietal. ,2023),Wikipedia11anddatafromInternetArchive.12Specifically,
wehaveused2.1billionmonolingualIndicsentences,significantlyhigherthanSamanantar( Rameshetal. ,2022)(398.5
million). Moreover,thenumberofEnglishsentencesthatweusedforourbitextmininghasincreasedfrom54.3million
to429million. Additionally,wehavealsominedbitextforUrduandNepali.
Figure4shows an overview of the mining process. We provide details of the mining workflow below. The mining
frommonolingualsources resultedin 121million bitextpairs. Table 4showstheper-languagestatisticsof themined
corpora.
11https://dumps.wikimedia.org/
12https://archive.org
13PublishedinTransactionsonMachineLearningResearch(12/2023)
Table5: Pearson( ρ)andKendal( τ)correlationCosineSimilarityofLaBSEandLASERmodelwithHumanRatings
ontheSTSdatareleasedby Rameshetal. (2022).
LaBSE LASER
Language Sample Size ρ τ ρ τ
asm_Beng 1,971 0.3942 0.2989 0.3797 0.3021
ben_Beng 3,797 0.5149 0.4392 0.3137 0.2522
guj_Gujr 2,298 0.5437 0.4475 0.2945 0.3429
hin_Deva 4,616 0.5575 0.4691 0.4550 0.4005
kan_Knda 2,838 0.5211 0.4184 0.2640 0.2634
mal_Mlym 2,760 0.5331 0.4354 0.4368 0.3339
mar_Deva 1,984 0.4773 0.3916 0.3540 0.2660
ory_Orya 1,264 0.1148 0.1152 0.0361 0.0332
pan_Guru 2,222 0.5952 0.4725 0.3812 0.3435
tam_T aml 2,882 0.5099 0.4084 0.2296 0.2367
tel_T elu 2,516 0.4426 0.3780 0.2164 0.1936
Average - 0.4731 0.3886 0.3055 0.2698
Data Curation. Our data curation process commenced with the collection of documents from diverse sources, in-
cluding IndicCorp v2 ( Doddapaneni et al. ,2023), Wikipedia11and Internet Archive data12which were aggregated at
thedocumentlevel. However,asourobjectivewastominesentence-levelparalleldata,weusedtheIndicNLPlibrary
(Kunchukuttan ,2020) to segment these documents into individual sentences. Subsequently, we implemented a strict
qualitycontrolprocedure,whereweperformlanguageidentification(LID)atthesentencelevelusingLIDfiltersfrom
Costa-jussà et al. (2022). As previous studies have shown, web-scale data often contains offensive content ( Kreutzer
etal.,2022),thereforeweusean“offensivewordlist”tofilteroutsuchcontent. Thislistisaugmentedwithdatafrom
Toxicity-200( Costa-jussàetal. ,2022)andDoddapanenietal. (2023). Additionally,weremovesentencesthataretoo
short( <4words)ortoolong( >40words)aswefoundthatthequalityandreliabilityofembeddingsdeterioratebeyond
theselengths. Afterthisqualitycontrolprocedure, weapplystrictdeduplicationtoeliminateanypotentialduplicates
onthenormalizedsentencesinthemonolingualcorpora.
Sentence Embedding Model. Prior work such as Samanantar ( Ramesh et al. ,2022) and NLLB ( Costa-jussà et al. ,
2022) have employed the LaBSE ( Feng et al. ,2022) and LASER3 ( Heffernan et al. ,2022) models for bitext mining
respectively. However, to determine the optimal sentence embedding model for our mining purposes, we analyze the
correlation of the Semantic Textual Similarity Rating ( Agirre et al. ,2016) with the cosine similarity scores obtained
usingbothsentenceembeddingmodels. WeconsidertheSTSdatasetreleasedby Rameshetal. (2022)withahuman
ratingforasetof11languages. OuranalysissuggeststhatthecosinesimilarityscoresofLaBSEsentenceembeddings
exhibit a stronger correlation with the human ratings on a macro scale, as shown in Table 5. Therefore, we adopt the
LaBSEmodelastheprimarysentenceembeddingmodelforourbitextminingandfilteringpipelineandonlyfallback
toLASER3forthelanguagesnotsupportedbyLaBSE.WeuseLASER3forlanguagessuchasKashmiri(Devanagari),
Kashmiri(Arabic),Maithili,Manipuri(Bengali),Nepali,Sanskrit,andSindhi(Arabic).
Indexing. Toensureacommonembeddingspaceforalllanguages,weutilizedLaBSE( Fengetal. ,2022)tocompute
thesentenceembeddingsforallthesentences. Ourapproachforminingparallelsentencesinvolvessearchingthrough
English;thusweindexedalltheEnglishsentencesandtreatedtheIndiclanguagesentencesasqueries. Toaccommodate
thelargecorpusof429millionEnglishsentences,wepartitionedtheminto5shardsandindexedeachshardseparately.
Inlinewithpreviouswork( Rameshetal. ,2022),weutilizedaFAISSIndex13with100KclustersandemployedProduct
Quantization( Jégouetal. ,2011)toreducethedimensionalityoftheembeddingsfrom768to64,witheachdimension
representedbyan8-bitintegervalue.
13https://github.com/facebookresearch/faiss
14PublishedinTransactionsonMachineLearningResearch(12/2023)
Table6: URLsanddomainsofthesourcesusedforcomparablecorporamining.
Source URL Domain
isha https://isha.sadhguru.org/in/en/wisdom Religion, Education, Culture
mkb https://www.pmindia.gov.in/en/mann-ki-baat Government, News, Education
nios https://nios.ac.in/online-course-material.aspx Education
nptel https://nptel.ac.in/courses Education
pib https://pib.gov.in/AllRelease.aspx Government, News, Legal
spoken tutorial https://spoken-tutorial.org/tutorial-search Education
ugc http://ugceresources.in Education
vanipedia https://tinyurl.com/2sf547tn Religion, Education, Culture
Retrieval. To retrieve parallel sentence pairs for a given query sentence ( SL) in language L, we use LaBSE ( Feng
et al.,2022) to compute the embedding of the query sentence and perform a search on the FAISS Index constructed
fromtheEnglishsentences. First,weretrievethetop k(k= 1024)clustersbycomputingthecosinesimilaritybetween
theclustercentroidsandthequeryembedding. Subsequently,wesearchforANNswithintheseclusterstoretrievethe
closestmatch. However, aspointedoutby Rameshetal. (2022), thesimilarityscorescanvarywhenusingquantized
vectors ( 64d) while preserving the relative ranking among the sentence pairs. To ensure high-quality matches, we
recompute the cosine similarity using the original 768dvectors and only retain pairs with a similarity score above a
thresholdof0.80,indicatingastrongsemanticmatch. Theprocessisrepeatedoneachofthe5Englishpartitions,and
onlythehighest-scoringmatchisretained.
4.2 Mining from Comparable Corpora
For Indian languages, we explore the mining of parallel corpora from comparable sources, i.e., multilingual websites
containinghigh-qualityparalleldocuments. Wefirstalignpotentiallyparalleldocumentsusingheuristicstoreducethe
searchspace,followedbytheextractionofhigh-qualityparallelsentencesfromaligneddocuments.
Data Curation. We first identify several websites that publish content in multiple Indic languages. The articles on
thesewebsitesarealignedacrossdifferentlanguages,indicatingtheyareexacttranslationsofeachother. Owingtothis,
thesearchspaceisreducedconsiderablyascomparedtomonolingualcorpusmining. Theselectedsourcesarediverse
indomainscoveringarangeoftopicslikeEducation,Legal,Religion,etc.,andofhighqualityasverifiedbylanguage
experts. An overview of the sources is available in Table 6. We follow the same pre-processing steps to segment the
documentsintosentences,followedbylanguageidentificationandtoxicityfilters.
Indexing. Similartomonolingualcorpora,weusetheLaBSE( Fengetal. ,2022)modeltoindexboththesourceand
targetsentences. Sincethesearchspaceismuchsmallerincomparablecorpora,weperformafullsearchovertheentire
targetsentencesinthecorrespondingdocument.
Retrieval. LetS={s1, s2,· · ·, sm}be the set of source sentences and T={t1, t2,· · ·, tn}be the set of target
sentences. Let f(si, ti)bethescoringfunctionforcalculatingthesemanticsimilarity. Giventhat mandnareconsid-
erablysmallerthanthesizeofthemonolingualcorpus,weperformatotalof m×nscoringcomputations. Following
Artetxe&Schwenk (2019),weusethemargin-basedscoring(Equation 2)tofindtheclosestsemanticmatchbetween
agivensourceandtargetsentences. Thesentencesunderconsiderationarerepresentedbythepair (x, y). Wedenote
thekunique nearest neighbors of xandyin the other language as NN k(x)andNN k(y), respectively. We perform
margin-basedmininginbothforwardandbackwarddirectionstoeliminatethecandidatepairswithinconsistentalign-
ment and retain only those that intersect, resulting in high-quality bitext pairs. Following Costa-jussà et al. (2022)
we use a margin threshold of 1.06 with 4 nearest neighbors. Additionally, we set a cosine threshold of 0.80 for the
high-resourcelanguagesandperformLIDfilteringtoremovesubstandardsentencepairs. Consideringthehighmem-
oryrequirementsandthehighvariabilityofmarginscoresbasedonclustersizeswhenoperatinginshards,employing
margin-basedminingformonolingualcorpuswiththecurrentinfrastructurewasnotfeasible.
15PublishedinTransactionsonMachineLearningResearch(12/2023)
Table7: Statisticsofthebitextminingfromcomparablecorpora(tillOct2022).
Language Source Extracted P airs
asm_Beng mkb ,nios,pib,spoken-tutorial ,vanipedia 38,656
ben_Beng isha ,mkb,nios,nptel,pib spoken-tutorial ,ugc,vanipedia 263,394
brx_Deva spoken-tutorial 700
guj_Gujr isha ,mkb,nios,nptel,pib spoken-tutorial ,ugc vanipedia 594,847
hin_Deva isha ,mkb,nios,nptel,pib spoken-tutorial ,ugc vanipedia 891,464
kan_Knda isha ,mkb,nios,nptel,pib spoken-tutorial ,ugc vanipedia 386,408
mai_Deva spoken-tutorial 84
mal_Mlym isha ,mkb,nios,nptel,pib spoken-tutorial ,ugc vanipedia 365,893
mar_Deva isha ,mkb,nios,nptel,pib spoken-tutorial ,ugc vanipedia 453,371
mni_Beng mkb ,pib 22,322
npi_Deva isha ,spoken-tutorial ,vanipedia 6,247
ory_Orya mkb ,nios,pib spoken-tutorial ,vanipedia 125,143
pan_Guru mkb ,nios,pib spoken-tutorial ,vanipedia 216,108
san_Deva spoken-tutorial 702
tam_T aml isha ,mkb,nios,nptel,pib spoken-tutorial ,ugc,vanipedia 455,965
tel_T elu isha ,mkb,nios,nptel,pib spoken-tutorial ,ugc,vanipedia 449,239
urd_Arab mkb ,nios,pib,vanipedia 232,496
# T otal 4,503,039
margin( x, y) =cos(x, y)
∑
z∈NNk(x)cos(x, z)
2k+∑
z∈NNk(y)cos(y, z)
2k(2)
Following mining from Comparable Corpora, we extract 4.5 million sentence pairs across 17 Indic languages. The
statisticsandthesourcesfortheminedbitextareavailableinTable 7.
4.3 Filtering Existing Mined Parallel Corpora
Overtheyears,severalparallelcorporahavebeenreleasedforIndiclanguages( Kunchukuttanetal. ,2018;Nakazawa
et al.,2021b;Philip et al. ,2021;Tiedemann ,2012)inter alia. The corpora are of varying quality and created using
different approaches. We filter these existing corpora using some of the well-known practices to ensure we retain a
high-qualitysubsetformodeltraining.
Particularly,alargecollectionofparallelcorporawasminedaspartoftheNLLBproject( Costa-jussàetal. ,2022)using
LASER3embeddings( Heffernanetal. ,2022). Thecorpuswasminedusingthemargin-basedthresholddescribedin
Equation( 2),withathresholdof1.06. Theoriginaldatasetwasnotreleasedbytheauthorsof Costa-jussàetal. (2022).
However,AllenAI14hasreplicatedtheeffortsof Costa-jussàetal. (2022)andreleasedthedatasetcloselymatchingthe
numbersreportedbytheauthorsof( Costa-jussàetal. ,2022). Goingforward,weusethisdatasetforouruse-caseand
refertoitasAllen-NLLB15.Thecorpuscontains448millionsentencepairsacross19Indiclanguages,withmorethan
10millionsentencepairsin12languages. However,onperformingamanualinspectionofthebitext,itwasobserved
thatalargemajorityofthesentenceshadmisalignmentandsuboptimalparallelsentencepairs. Therefore,beforeusing
thiscorpusfortrainingMTmodels,itisimportanttofilterthecorpustoremovethenoisysentencepairs.
FollowingourbitextmininginSection 4.1andSection 4.2,weusetheLaBSEmodel( Fengetal. ,2022)withacosine
similaritythresholdof 0.80tofiltertheAllen-NLLBcorpus. WealsousetheLASER3model( Heffernanetal. ,2022)
as a fallback model for languages that are not supported by LaBSE ( viz.Nepali, Maithili, Sanskrit, Sindhi (Arabic),
14https://allenai.org/
15https://huggingface.co/datasets/allenai/nllb
16PublishedinTransactionsonMachineLearningResearch(12/2023)
Table8: Statisticsofpre-filteringandpost-filteringonexistingminedparallelcorporaconsistingofNLLB( Costa-jussà
etal.,2022)andSamanantar( Rameshetal. ,2022).
Language Pre-Filtering Post-Filtering Proportion (%)
asm_Beng 5,285,401 565,282 10.70
ben_Beng 70,400,333 16,514,684 23.46
guj_Gujr 14,458,054 8,442,476 58.39
hin_Deva 43,149,229 11,056,172 25.62
kan_Knda 38,368,723 10,532,571 27.45
kas_Arab 647,348 125,243 19.35
kas_Deva 1,042,450 194,528 18.66
mai_Deva 4,438,382 62,359 1.40
mal_Mlym 49,599,699 10,832,342 21.84
mar_Deva 35,585,104 7,742,065 21.76
mni_Beng 490,089 347,108 70.83
npi_Deva 19,624,054 1,583,922 8.07
ory_Orya 14,700,484 2,887,960 19.65
pan_Guru 14,057,042 3,391,710 24.13
san_Deva 3,095,396 244,367 7.89
snd_Arab 8,924,699 2,129,054 23.86
tam_T aml 47,777,362 10,489,852 21.96
tel_T elu 51,248,532 11,826,104 23.08
urd_Arab 25,303,579 5,322,290 21.03
# T otal 448,195,960 104,290,089 23.27
Kashmiri (Devanagari), Kashmiri (Arabic), Santali). Table 8shows that upon filtering, the dataset is reduced from
448.1 million sentence pairs to 104.2 million sentence pairs, i.e.close to 76% of data has been dropped with quality
filtering. For Santali, post LASER3 filtering, it was observed that the majority of the sentence pairs were dropped
during the filtering process. Post-hoc human evaluation confirmed that most of the parallel data for Santali-English
in the Allen-NLLB are noisy. We see the highest drops in Maithili, Sanskrit, and Nepali, which are considered to be
low-resource languages. Surprisingly, even in high-resource languages like Hindi and Bengali, we see that close to
75%of the data has been dropped during filtering. Similarly, we also apply the same filtering criteria to Samanantar
Corpus(Rameshetal. ,2022),asitwasnotedthatSamanantarwasminedwithanolderversionofLaBSEmodel( Feng
etal.,2022). Section 7.2describesouranalysisofthedataqualityv/sscaletrade-off.
5 Modeling
5.1 T raining Data
To train our translation models, we utilize a range of data sources, including data mined from text corpora (monolin-
gual corpora & comparable sources), human-annotated collections (BPCC-H-Wiki and BPCC-H-Daily), and filtered
versions of existing corpora ( Ramesh et al. ,2022;Costa-jussà et al. ,2022). We describe our filtering techniques in
Section4.3. Whilethesesourcesconstitutethemajorityofourtrainingcorpus,wealsoincorporateadditionalhuman-
labeled seed data from NLLB-seed ( Costa-jussà et al. ,2022;Maillard et al. ,2023), ILCI (Jha,2010;Choudhary &
Jha,2011), and MASSIVE ( FitzGerald et al. ,2022), totaling approximately 1.47 million sentence pairs. The ILCI
(Jha,2010;Choudhary&Jha ,2011)dataisprimarilydistributedacrossdomainssuchashealth,tourism,agriculture,
andentertainment,andcontributesaround1.34millionparallelsentencesacross16languages. Furthermore,weaug-
mentourdatawiththeIndicportionsofMASSIVE( FitzGeraldetal. ,2022),whichwasreleasedasSpokenLanguage
Understanding data and closely resembles the data in BPCC-H-Daily. Professional annotators manually translate the
sentencesinthisdatasetandcontribute139,000sentencepairsacrosssevenlanguages. Intotal,wehaveapproximately
230.5 million sentence pairs, out of which 2.2 million are gold sentence pairs that are manually annotated by profes-
sionaltranslators. ThedistributionofthedatasourcesacrossalllanguagesispresentedinTable 1.
17PublishedinTransactionsonMachineLearningResearch(12/2023)
Table9: Statisticsofthebi-texttrainingdataafterdeduplicationwithbenchmarks.
Language Dataset Size Language Dataset Size
asm_Beng 1,443,125 mni_Beng 386,916
ben_Beng 32,725,076 mni_Mtei 42,753
brx_Deva 1,13,839 npi_Deva 1,687,436
doi_Deva 24,160 ory_Orya 5,834,074
gom_Deva 97,660 pan_Guru 9,816,009
guj_Gujr 20,491,094 san_Deva 278,374
hin_Deva 39,144,013 sat_Olck 25,128
kan_Knda 23,285,105 snd_Arab 2,128,391
kas_Arab 135,843 snd_Deva 10,503
kas_Deva 200,094 tam_T aml 20,740,179
mai_Deva 87,888 tel_T elu 23,250,217
mal_Mlym 23,521,937 urd_Arab 6,176,951
mar_Deva 18,932,834
# T otal 230,579,599
5.2 Preprocessing
Wefollowthefollowingstepsinsequentialorderforourdatapreprocessingpipeline.
Standard Preprocessing. We apply standard preprocessing, which includes removing redundant spaces, removing
specialcharacters,andnormalizingthepunctuations. Additionally,weconverttheIndicnumeralstoEnglishnumerals
using a dictionary-based mapping. This facilitates the use of English numerals both at the input and output stages of
our model. However, a post-processing stage can be used to map English numerals back to their Indic equivalents, if
required.
Data Deduplication. To prevent any potential data leakages, we apply strict deduplication with all the available
benchmarks mentioned in Table 2. Our deduplication process involves standard preprocessing steps as mentioned
above, followed by text lowercasing, removal of all punctuations, removal of spaces, and identification of potential
matches on the monolingual side of both source and target sentences with the benchmarks. Correspondingly, any
bi-text pairs associated with these monolingual matches are discarded, and only the remaining data is considered for
trainingourmodels. Asaresultofthisdeduplication,ourprocesseddatasetcontainsatotalof~ 230.5Mbi-textpairs.
Theper-languagedistributionispresentedinTable 9
Additional Preprocessing. Based on human evaluation of the IndicTrans1 model ( Ramesh et al. ,2022), it was ob-
served that the model exhibits poor performance in dealing with special cases: emails, URLs, dates, numbers, and
specialcharacterslikepercentages. Thesespecialcasesshareacommoncharacteristicindicatingthattheyshouldide-
allynotbetranslatedbythemodelbutshouldbereproducedasitisinthetranslation. Toaddressthisissue,weemploy
regular expression patterns to identify text spans corresponding to these special cases. Subsequently, we wrap these
spansoftextwithspecialtags( <dnt> text span </dnt> )ontheinputsideofthemodel,therebyprovidingimplicit
supervisiontothemodeltoretainthesespecialcasesintheiroriginalforminthetranslation. Notethat,duringtraining,
wewrapthetextspanswithinspecialtagsonlyiftheyappearinboththesourceandtargetsentences.
Script Unification. Many Indic languages use scripts from the Brahmi family. To facilitate better transfer learning,
wherever feasible, we apply rule-based script conversion using IndicNLP library ( Kunchukuttan ,2020) to represent
most of these languages in a single script (Devanagari). Thus, effectively our models are trained with five scripts,
namelyPerso-Arabic(Sindhi,Urdu,Kashmiri),OlChiki(Santali),Meitei(Manipuri),Latin(English),andDevanagari
(alltherestofthelanguages).
18PublishedinTransactionsonMachineLearningResearch(12/2023)
5.3 T okenization
Subword-leveltokenization( Sennrichetal. ,2016b;Kudo&Richardson ,2018)isaneffectiveapproachforsegmenting
text into smaller sub-word units to build neural machine translation (NMT) systems that are robust against out-of-
vocabulary(OOV)issues. Inthiswork,wetraintwoseparatetokenizerswiththebyte-pair-encoding(BPE)algorithm
(Sennrich et al. ,2016b) using SentencePiece16library (Kudo & Richardson ,2018) for English and Indic languages
usingasampledcorpuscomprisingmonolingualsentencesfromIndicCorpv2( Doddapanenietal. ,2023)andNLLB
data (Costa-jussà et al. ,2022). We chose SentencePiece library because of its in-built support for normalization. To
ensure fair representation for each language, we upsample the low-resource languages and limit the high-resource
languages to 3M sentences each. We use a vocab size of 32Kand128Kfor our English and Indic SPM models,
respectively. WepreparethemonolingualdatafortrainingourEnglishandIndicSPMmodelsusingthepreprocessing
pipelinedescribedinsection 5.2exceptfortheadditionalpreprocessing. Wealsoaddspecialtags( <dnt>and </dnt>)
tothetrainedSPMmodels.
Aftertokenization,weprependspecialindicatortagsfollowingpriormultilingualNMTmodels( Johnsonetal. ,2017;
Tanetal.,2019;Tangetal.,2021). Inourcase,weaddboththesourceandtargetlanguagetagstoindicatethetranslation
direction. Specifically, when translating text from English to Hindi, we format the sample as eng_Latn hin_Deva
{processed text} .
5.4 Architecture
We train our English-centric neural models based on the transformer encoder-decoder architecture ( Vaswani et al. ,
2017)usingthefairseqlibrary17(Ottetal.,2019). Ourarchitecturecomprises18encoderlayersand18decoderlayers,
aninputdimensionof1024,pre-normalization( Xiongetal. ,2020)forallmodules,afeedforwarddimensionof8192,
and 16 attention heads. The total parameter count is 1.1B. Additionally, we use the GELU activation ( Hendrycks &
Gimpel,2016)insteadofReLU( Nair&Hinton ,2010).
5.5 T raining
To perform well across a wide range of domains, we adopt FLORES-200 ( Costa-jussà et al. ,2022) multi-domain
development set as our validation set rather than combining development sets from different benchmarks. However,
thisdevelopmentsetdoesnotcoverallthelanguagessupportedbyourmodels. Asaresult,weextendtheFLORES-200
development( Costa-jussàetal. ,2022)settoadditionallyincorporatefivemorelanguages( viz.Bodo,Dogri,Konkani,
Sindhi (Devanagari), Manipuri (Meitei)) to have a complete validation set to jointly optimize and achieve superior
performance on all the 22 scheduled Indic languages (including 25 language script combinations). We also make the
expandedversionof the FLORES-200 developmentset ( Costa-jussàet al. ,2022) publiclyavailable, and this has also
beenintegratedintotheoﬀicialFLORESrepository18.
We employ the BLEU metric specifically for checkpointing purposes, using validation BLEU scores to indicate the
model’sperformance on the aforementionedvalidationset. This choice is motivatedbyBLEU providingvaluablein-
sights into the model’s macro-level performance, making it a useful diagnostic tool for tracking the model’s progress
during training. However, it may not be the most suitable choice for fine-grained evaluations. This differs from In-
dicTrans1( Rameshetal. ,2022),whichutilizesvalidationlossforcheckpointing. Byincorporatingthecheckpointing
basedonvalidationBLEUscores,wecanensurethatthetrainingofourmodelsprogressesbasedontheirperformance
onthevalidationset,leadingtoanoverallimprovedmodel.
Our model training paradigm comprises two distinct phases: auxiliary training and downstream training, which are
describedbelow.
Auxiliary Training. The first phase of our model training paradigm, termed auxiliary training, involves training
intermediate models to augment large amounts of monolingual corpora through back translation. Back-translation
16https://github.com/google/sentencepiece
17https://github.com/facebookresearch/fairseq
18https://github.com/openlanguagedata/flores
19PublishedinTransactionsonMachineLearningResearch(12/2023)
Table10: Detailsofthehyperparametersusedforstage1trainingandstage2fine-tuning. Pleasenotethatweresetthe
learningscheduler,dataloaders,andoptimizerforstage2fine-tuning.
Hyperparameters Stage 1 training Stage 2 fine-tuning
Optimizer Adam(Kingma&Ba ,2014) Adam( Kingma&Ba ,2014)
Betavalues (β1, β2) (0.9,0.98) (0.9,0.98)
Learningrate 5e-4 3e-5
Scheduler Inversesqrt Inversesqrt
Criterion Cross-entropy Cross-entropy
Labelsmoothing( Szegedyetal. ,2016)0.1 0.1
Warmuplearningrate 1e-7 1e-7
Warmupsteps 4,000 2,000
Gradientclipping 1.0 1.0
Dropout(Srivastavaetal. ,2014) 0.2 0.2
Patience 10 10
Effectivebatchsize 262K 32K
Mixedprecisiontraining FP16 FP16
Maximumupdatesteps 1M 1M
Validationinterval 2,500 1,000
Maximumsequencelength 256 256
Checkpointmetric BLEU@beam=1 BLEU@beam=1
(Sennrich et al. ,2016a;Edunov et al. ,2018) is a technique that is effective in improving the performance of machine
translationmodels. Weadoptadeterministiccurriculumstrategyasproposedby Mohiuddinetal. (2022),whereinwe
first train the models from scratch on the entire parallel corpora listed in Table 1, followed by stage 2 fine-tuning on
high-quality seed data including BPCC-H-Wiki and the NLLB seed ( Costa-jussà et al. ,2022;Maillard et al. ,2023),
to improve the models further. Our approach differs from theirs in that we exclusively consider high-quality human-
generateddataforstage2modelfine-tuningratherthanselectingthetop p%ofbitextpairsfromtheoriginaldatabased
onaqualitymeasure. Anotherprominentadvantageofusingourhuman-generateddataisthatitprovidesmulti-domain
coverage,therebyallowingustooptimizeacrossmultipledomains,whichmaynotbefeasiblewhenselectingasubset
ofbitextpairsbasedonquality. Welistallthehyperparametersusedinbothstage1andstage2traininginTable 10.
Downstream Training. In the second phase, we train our models on the augmented parallel corpora that combine
original data with back-translated data. Mainly, we follow tagged back translation ( Caswell et al. ,2019) to provide
additionalsupervisiontothemodeltodistinguishbetweenthedifferentdatasourcesduringtraining. Weprependthe
specialsymboltothesyntheticallyaugmenteddatawhilekeepingtheoriginaldataintact. Wefollowthesametraining
hyperparametersandtwo-stagetrainingstrategyastheauxiliarytraining. Table 10showsallthehyperparametersused
inbothstage1andstage2training.
5.6 Data Augmentation
Usingexistingparallelcorporaastrainingdatamayeventuallyleadtosaturationinmodelperformance. Toaddressthis,
researchers have proposed data augmentation techniques to enhance data diversity and improve model performance.
One such approach involves augmenting pseudo-parallel corpora by leveraging diverse monolingual corpora. Back
translation ( Sennrich et al. ,2016a;Edunov et al. ,2018) is a widely used technique to synthetically augment training
dataforimprovingtranslationmodels. Giventhelargescaleofourmodels,weadoptthisapproachandgenerateback-
translated data, which is approximately 1.75times the size of the original training data. To generate back translation
data, we first identify potential sources of monolingual data for English and Indic languages, intending to maximize
both domain coverage and distributional diversity to improve the models. We use the intermediate checkpoints of
IndicTrans2togeneratethebacktranslateddataandcombinetheaugmenteddataalongwiththetrainingdatatofurther
improveourmodels.
20PublishedinTransactionsonMachineLearningResearch(12/2023)
Table11: Statisticsofthemonolingualdatausedforbacktranslation.
Language English BT Data Indic BT Data Language English BT Data Indic BT Data
asm_Beng 14,569,760 5,433,796 mni_Beng 17,437,961 60,224
ben_Beng 17,928,856 34,987,743 mni_Mtei 17,709,470 33,233
brx_Deva 17,597,825 144,246 npi_Deva 20,567,992 29,997,511
doi_Deva 18,157,864 44,291 ory_Orya 19,528,727 15,341,924
gom_Deva 13,478,802 2,937,179 pan_Guru 17,476,704 29,968,101
guj_Gujr 21,447,703 29,994,809 san_Deva 11,198,794 9,744,059
hin_Deva 20,648,256 37,472,261 sat_Olck 9,799,342 32,346
kan_Knda 10,970,576 32,496,971 snd_Arab 8,918,509 4,298,898
kas_Arab 12,717,571 44,276 snd_Deva 6,479,694 25,264
kas_Deva 11,599,085 154,465 tam_T aml 22,647,544 32,488,783
mai_Deva 15,598,363 1,813,669 tel_T elu 21,767,767 32,494,937
mal_Mlym 17,888,824 32,495,047 urd_Arab 20,006,656 33,471,969
mar_Deva 15,849,536 34,994,281
# T otal 401,992,181 400,970,283
English Data for Back Translation. For back translation, we source English data from several sources, including
the English side of IndicCorp v2 ( Doddapaneni et al. ,2023), the English side of the Indic subset of the NLLB data
(Costa-jussà et al. ,2022), and English data from a few high-resource pairs ( eng_Latn - {fra_Latn, por_Latn,
spa_Latn, ces_Latn} ) of NLLB data ( Costa-jussà et al. ,2022), along with additional miscellaneous sources like
SimpleWikipedia19andDDNews.20WesubjectedthissetofEnglishsentencestostandardpreprocessing,asoutlined
inSection 5.2,andthenfilteredthesettoretainonlysentenceswithaminimumoffiveandamaximumof100words.
As described in Section 5.2, we deduplicate this set of sentences with all the benchmarks available. Additionally, we
deduplicatethissetwiththetrainingdatatoensuremorediversityinEnglishdataandsamplecandidatesentencesfrom
a non-overlapping set. From this reduced candidate set, we randomly sampled approximately 400million sentences
forbacktranslation,followinganapproximatedistributionof 55%IndicCorp, 20%NLLBIndic, 20%NLLBHighRes,
and5%Miscellaneous sources. To ensure language-script diversity, we randomly subdivide the 400million set into
25parts, corresponding to the supported language-script combinations. We utilize the En-Indic model with a beam
value of 5to generate back-translated data. We proportionally distribute the English data across different language-
script combinations based on the normalized chrF++ ( Popović,2017) scores across all language-script combinations
described below in Equation ( 3) on the expanded version of FLORES-200 validation set ( Goyal et al. ,2022;Costa-
jussà et al. ,2022) described in section 5.5. Table11describes the distribution of the English data we consider for
back-translationforeachlanguage-scriptcombination.
Count (langi) =chrF++ (langi)∑
jchrF++ (langj)×N (3)
Here,chrF++ (langi)representsthenormalizedchrF++scoreforlanguage-scriptcombinationlangi,andNisthetotal
numberofEnglishmonolingualsentencestobeusedforbacktranslation.
Indic Data for Back Translation. We source the Indic monolingual data from IndicCorp v2 ( Doddapaneni et al. ,
2023) and the Indic side of the NLLB data ( Costa-jussà et al. ,2022) to generate back-translated data to improve our
En-Indic model. However, it is essential to note that our sources for Indic monolingual data are limited, which limits
the amount of data we can sample from each language-script combination. As a result, we do not adopt any pro-
portional sampling based on the model’s performance on the FLORES-200 validation set, as we do when generating
back-translateddatafrommonolingualEnglishdata. Therefore,wefollowasimplestrategytoincludealltheavailable
monolingual data from languages, where the availability of diverse monolingual data is scarce (less than 20million
19https://simple.wikipedia.org/wiki/Main_Page
20https://ddnews.gov.in/
21PublishedinTransactionsonMachineLearningResearch(12/2023)
sentences)anduniformlysamplefromthehigh-resourcelanguages. Weapplythesamepreprocessinganddatadedu-
plicationstepsasdescribedaboveforback-translationfromEnglish. WeusetheIndic-Enmodelwithabeamvalueof
5forgeneratingback-translationdata. WeprovidethedetailsoftheIndicmonolingualdatadistributionusedforback
translationinTable 11.
5.7 Postprocessing
Since our En-Indic model is trained on script-unified data, the output it generates must be mapped back to the na-
tive script of the target language. Therefore, we perform rule-based script conversion using the IndicNLP library
(Kunchukuttan ,2020)andmapthescript-unifiedoutputtothecorrespondingnativeIndicscript. Importantly,thispost-
processing is only necessary for the En-Indic model, as the outputs of the Indic-En model are already in the desired
format.
6 Evaluation
6.1 Models Compared
Wecompareourtrainedmodelswithpubliclyandcommerciallyavailableexistingmodelsandsystems:
•IndicTrans1. Ramesh et al. (2022) curated large parallel corpora by large-scale mining and trained multilingual
transformer models ( 474Mparameters) on this mined Samanantar dataset. These models support only 11 major
Indianlanguages.
•NLLB.Costa-jussàetal. (2022)trainedamulti-waymany-to-many 54.5BMixtureofExperts(MoE)modelsupport-
ing200languages. Thismodelsupports20language-scriptcombinationsfromthesetofscheduledIndiclanguages,
providingcoverageinatleastonescriptfor19ofthe22scheduledIndiclanguages.
•M2M-100. Fan et al. (2020) released many-to-many models supporting translation between 100 languages with
language-family-specific decoders trained using English-centric data and non-English-centric data. We use their
bestmodel( 12Bparameters)supporting12ofthe22scheduledIndiclanguagesforourcomparison.
•Microsoft Azure Translate.21MicrosoftAzureTranslateisacommercialtranslationenginesupportingtranslation
between16outofthe22scheduledIndiclanguagesatthetimeofwriting.
•Google Translate.22GoogleTranslateisacommercialtranslationenginesupportingtranslationbetween19outof
the22scheduledIndiclanguagesatthetimeofwriting.
•GPT -3.5. GPT-3.5isacommerciallyavailable,largelanguagemodeldevelopedbyOpenAI,23basedontheGPT-3
architecture ( Brown et al. ,2020), but with additional improvements and optimizations like instruction fine-tuning,
reinforcement learning with human feedback ( Ouyang et al. ,2022), and enhanced conversational support. It is a
decoder-only model trained using the causal language modeling objective and is currently available as a propriety
systemaccessibleviaapaidAPI.Weevaluatethe gpt-3.5-turbo model,whichacceptschatformatmessages,on
ourIN22benchmarkinazero-shotsetting.
For proprietary models, it is diﬀicult to do fair comparisons since little information is available about models and
training. Thus, the reported results should be seen as a reasonable approximation. In this work, we will henceforth
adoptthespecificshorthandnotations: theIndicTrans1modelwillbereferredtoasIT1,theM2M-100modelasM100,
the NLLB 1.2B distilled model as N1.2, the NLLB 54.5B MoE model as N54, Google Translate as Goog, Microsoft
AzureTranslateasAz,andourIndicTrans2modelasIT2. ThepredictionsofMicrosoftAzure,GoogleTranslate,and
GPT3.5weregeneratedusingtherespectiveAPIs,withdataretrievedon10thMay2023.
21https://azure.microsoft.com/en-us/products/cognitive-services/translator
22https://cloud.google.com/translate
23https://platform.openai.com/docs/models/gpt-3-5
22PublishedinTransactionsonMachineLearningResearch(12/2023)
6.2 Benchmarks
We evaluate our trained models (auxiliary and downstream) on our IN22 benchmark and all the publicly available
benchmarks: FLORES-200 ( Goyal et al. ,2022;Costa-jussà et al. ,2022), WAT 2020 ( Nakazawa et al. ,2020), WAT
2021(Nakazawaetal. ,2021a),WMT2014( Bojaretal. ,2014),WMT2019( Barraultetal. ,2019),WMT2020( Barrault
etal.,2020),UFAL(Ramasamyetal. ,2012)andNTREX( Federmannetal. ,2022).
Welistthedetailsoftheexistingbenchmarksbelow.
•IN22isacomprehensivebenchmarkforevaluatingmachinetranslationperformanceinmulti-domain, n-wayparallel
contexts across 22 Indic languages. It comprises three distinct subsets, namely IN22-Wiki, IN22-Web, and IN22-
Conv. TheWikipediaandWebsourcessubsetsofferdiversecontentspanningnews,entertainment,culture,legal,and
India-centric topics. Meanwhile, the conversation domain subset is designed to assess translation quality in typical
day-to-dayconversational-styleapplications.
From now on, we merge Wikipedia and Web Sources subsets, to create a consolidated set referred to as IN22-Gen
for translation evaluation. Our motivation for this is that these two subsets share a common language style, albeit
withvaryingtopics,whereastheConversationsubsetisdifferentinbothlanguagestyleandusagecontext.
•FLORES-101/200 (Goyal et al. ,2022;Costa-jussà et al. ,2022) is a multi-domain general-purpose benchmark de-
signed for evaluating translations across 200 languages, including 19 Indic languages. The English sentences are
source-original and have been translated into other languages. It comprises sentences sourced from Wikimedia en-
tities with equal portions of news, travel, and non-fiction content from children’s books. Tables 2and52provide
furtherdetailsonthestatisticsandfine-graineddomaincoverage.
•NTREX(Federmannetal. ,2022)isanews-domainbenchmarkthatexpandscoverageoflanguagesoftestdatafrom
WMT2019( Barraultetal. ,2019)to128languages. Outofthese,13arescheduledIndiclanguages.
•WMThas created benchmarks for selected Indic languages as part of shared tasks in 2014 (Hindi) ( Bojar et al. ,
2014),2019(Gujarati)( Barraultetal. ,2019)and2020(Tamil)( Barraultetal. ,2020).
•W AT 2020/2021 (Nakazawaetal. ,2020;2021a)includedsupportfortranslationsfor8Indiclanguagesinthenews
domain. Inaddition,theyreleaseddataforHindi-EnglishinInformationTechnologyandWikiNewsdomains. WAT
2021(Nakazawaetal. ,2021a)createdabenchmarkfortranslationbetween10IndiclanguagesandEnglish.
•UF AL(Ramasamy et al. ,2012) is an English-Tamil bilingual benchmark created from publicly available websites.
ThebenchmarkconsistsofEnglishsentencesfromdomainssuchascinema,news,andsomebiblicalsources.
Movingforward,weconsiderIN22andFLORES-200( Costa-jussàetal. ,2022)astheprimarybenchmarkstoevaluate
allthetranslationmodels. TheresultsobtainedfromthesebenchmarksarereportedanddiscussedinSection 7. Addi-
tionally,theperformanceofthemodelsonotherbenchmarksispresentedinAppendix B.Notethatalmostallthetest
setsareEnglish-original,buthavebeenusedforIndic-to-EnglishevaluationaswellasIndic-Indicevaluation.
6.3 Metrics
Several metrics have been developed over the years for automatically assessing translation quality, including string-
based metrics such as BLEU ( Papineni et al. ,2002), chrF (Popović,2015), and chrF++ ( Popović,2017), and model-
based metrics such as BLEURT ( Sellam et al. ,2020), COMET ( Rei et al.,2020;2022) and PRISM ( Thompson &
Post,2020). Recentresearch( Kocmietal. ,2021;Freitagetal. ,2021;2022)hasshownthatmodel-basedmetricstend
to exhibita strongercorrelation with human judgment. However, these model-based metrics are limited to languages
representedintheunderlyingpre-trainedmodel. Theyaretrainedonhumanjudgmentdatafromafewlanguages,and
theirperformanceonmanylow-resourcelanguageshasnotbeenevaluated. Webrieflydescribeallthemetricsusedin
ourworkbelow.
23PublishedinTransactionsonMachineLearningResearch(12/2023)
BLEU.BLEU(Papinenietal. ,2002)hasbeenastandardandwidelyusedmetricforevaluatingmachinetranslation
quality. However, a significant limitation of the standard BLEU metric is its tokenization dependency. To overcome
this,sacreBLEU24(Post,2018)providesstandardizationintermsoftokenizationtoensureafaircomparison. Weuse
sacreBLEU for evaluating our En-Indic and Indic-En trained models. We use the in-built default mteval-v13a tok-
enizer25forIndic-En26andIndictokenizerfromIndicNLP( Kunchukuttan ,2020)forEn-Indic27evaluations. Therefore,
we first tokenize the machine translations and reference translations using Indic tokenizers from IndicNLP28(version
0.92)andUrduhack29(ALi,2019)librariesbeforerunningsacreBLEU.
chrF++. chrF++ (Popović,2017), an extension of the chrF metric ( Popović,2015) that additionally considers word
unigramsandbigrams,andisbettercorrelatedwithhumanjudgmentsandusessacreBLEUtocomputechrF++scores.
SimilartothetokenizersusedforBLEU,forIndic-En30evaluation,weusethein-builtdefault mteval-v13a tokenizer,
whileforEn-Indic31evaluation,weuseIndictokenizersfromIndicNLPandUrduhacklibrariestotokenizethemachine
translationsandreferencetranslationsbeforerunningsacreBLEU.
COMET. COMETisamodel-basedmachinetranslationevaluationmetricintroducedby Reietal.(2020)toaddress
some of the limitations of existing metrics such as BLEU. However, one of the prominent concerns about COMET
is its extensibility to low-resource languages. Therefore, in this study, we report COMET-DA scores for the top 13
Indian languages: Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Nepali, Odia, Punjabi, Tamil,
Telugu,andUrduthataresupportedbytheXLM-RoBERTa( Conneauetal. ,2020)model. Specifically,weconducta
reference-basedevaluationusingtheCOMET-22DAmodel32(Reietal.,2022).
Choosing the Primary Metric. COMET,themostrecommendedmodel-basedmetric( Kocmietal. ,2021),doesnot
supportallthe22IndiclanguagessincetheyarenotrepresentedinXLM-R( Conneauetal. ,2020)whichistheunderly-
ingmodelonwhichCOMETisbased. Conversely,BLEUhasseveralsignificantlimitations,includingitstokenization
dependency and preferential bias towards translations that are closer to the reference translations in terms of lexical
and word order ( Ananthakrishnan et al. ,2006). Particularly in the context of morphologically rich Indian languages,
BLEU is limited in addressing morphological variants since it relies on exact word matches. Furthermore, chrF++ is
moresuitableforevaluatingtranslationqualityinlanguageswithcomplexmorphologyandinflections,suchasIndian
languages. Inthiswork,we,therefore,primarilyrelyonchrF++asourprimarymetricforevaluatingtranslationquality.
WealsoreportadditionalmetricssuchasBLEU( Papinenietal. ,2002)andCOMET( Reietal.,2022). Inaddition,we
alsoperformpairedbootstrapresampling-basedstatisticalsignificancetests( Koehn,2004)forallthemetricsfollowing
thedefaultconfigurations.
6.4 Generation
To generate predictions using IndicTrans2, initially, we preprocess and tokenize the source sentences from the bench-
mark test set, following the steps described in Section 5.2and Section 5.3, respectively. Subsequently, we feed the
tokenizedsentencesintothetrainedmodelsasinputtogeneratecandidatetranslations. Weutilizebeamsearchwitha
beam value of 5for our trained models. Finally, we employ post-processing techniques, as detailed in Section 5.7, to
mapthescriptunifiedoutputtothecorrespondingnativescript. Forotherbaselinesystems,wefollowtheirdocumented
inferenceprocedure. Foralltheopen-sourcebaselinemodels,weusethesamebeamsizeof5.
24https://github.com/mjpost/sacrebleu
25https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl
26Indic-En sacreBLEU BLEU signature:
nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1
27En-Indic sacreBLEU BLEU signature:
nrefs:1|case:mixed|eff:no|tok:none|smooth:exp|version:2.3.1
28https://github.com/anoopkunchukuttan/indic_nlp_library
29https://github.com/urduhack/urduhack
30Indic-En sacreBLEU chrF++ signature:
nrefs:1|case:mixed|eff:yes|nc:6|nw:2|space:no|version:2.3.1
31En-Indic sacreBLEU chrF++ signature:
nrefs:1|case:mixed|eff:yes|nc:6|nw:2|space:no|version:2.3.1
32https://huggingface.co/Unbabel/wmt22-comet-da
24PublishedinTransactionsonMachineLearningResearch(12/2023)
Table 12: chrF++ scores of all the systems on the IN22-Gen Evaluation set in the En-Indic and Indic-En directions.
Thebest-performingsystemisbolded,whileunderlinedresultsindicatesignificantperformancedifferencewhereIT2
outperformsthesystem. Therow Avg.meanstheaveragescoreofallthelanguagesthatsystemXsupports. ∆represents
the difference between the average scores of IT2 and the average scores of system X for the subset of languages that
both X and IT2 support. A positive value for ∆indicates IT2 is better than X and vice-versa. †indicates completely
off-targettranslations.
En-Indic Indic-En
language IT1 M100 N1.2 N54 IT2 Goog Az IT1 M100 N1.2 N54 IT2 Goog Az
asm_Beng 35.9- 41.7 42.9 47.145.545.056.1- 63.1 66.565.8 65.1 60.8
ben_Beng 48.640.647.849.2 51.849.949.858.452.860.863.5 63.2 64.160.2
brx_Deva - - - - 47.8- - - - - - 62.1- -
doi_Deva - - - - 57.847.8-- - - - 72.667.3-
gom_Deva - - - - 45.241.441.1- - - - 59.257.851.1
guj_Gujr 47.219.948.349.5 53.552.250.860.311.863.9 66.3 66.5 66.5 62.4
hin_Deva 53.347.152.853.9 56.754.654.160.754.962.264.8 65.464.862.0
kan_Knda 46.715.347.348.6 51.048.149.458.812.662.4 65.164.2 64.5 61.7
kas_Arab - - 34.6 35.4 40.2- - - - 54.9 58.2 60.4- -
mai_Deva - - 44.9 44.7 48.738.345.2- - 62.1 65.1 64.8 64.061.0
mal_Mlym 45.731.245.446.7 50.949.048.656.944.859.862.8 64.562.760.4
mar_Deva 44.334.544.746.1 51.047.148.257.746.960.963.6 63.7 64.460.3
mni_Mtei - - - - 44.635.0-- - - - 57.950.7-
npi_Deva - 17.7 44.844.8 49.045.546.3- 40.1 65.068.0 67.7 69.063.8
ory_Orya 40.38.242.441.543.9 40.5 45.460.014.463.7 66.766.2 64.6 61.1
pan_Guru 48.025.048.549.550.6 52.750.457.2 38.2 60.4 63.1 63.462.758.5
san_Deva - - 25.5 28.1 38.832.0-- - 48.2 51.3 54.853.8-
sat_Olck - - 1.0†25.5 33.4- - - - 36.3 41.4 45.3- -
snd_Deva - - - - 36.6- - - - - - 57.3- -
tam_T aml 45.512.347.047.5 49.548.549.453.926.356.959.1 59.859.6 56.8
tel_T elu 46.5- 48.1 49.5 52.450.850.657.7- 61.3 64.4 64.864.6 61.2
urd_Arab - 45.0 62.163.768.2 63.9 69.0- 52.6 68.371.2 73.071.868.2
Avg. 45.6 27.0 42.8 45.1 48.6 46.8 49.6 58.0 35.9 59.4 62.4 63.1 63.2 60.6
∆ 5.2 25.4 6.4 4.1 - 4.2 1.7 6.3 29.3 3.7 0.7 - 1.1 4.2
6.5 Evaluation
Following the generation of candidate translations, we evaluate their quality using the automatic metrics mentioned
in Section 6.3. We apply standard processing techniques to compute the evaluation metrics, followed by running
sacreBLEU. We use the standard Moses tokenizer for English, while for Indic languages, we perform tokenization
usingIndicNLPandUrduhacklibraries. Wereleaseourevaluationprocedureandscriptstoensurereproducibility. We
followthesameevaluationprocedureforallsystemslistedinSection 6.1.
7 Results and Discussion
7.1 Comparison with Existing Systems
Evaluation on IN22-Gen Set. We evaluate the translation quality of multiple En-Indic and Indic-En MT models
on the IN22-Gen set. The results are presented in Table 12. We observe that IndicTrans2 significantly improves
translation quality over IndicTrans1 ( Ramesh et al. ,2022) with an average improvement of 5.2 points in the En-Indic
directionand6.3pointsimprovementintheIndic-Endirection. Theproposedmodeloutperformsthebestcommercial
and open-source models for En-Indic translation by 1.7 and 4.1 points, respectively. For Indic-En translation, the
IndicTrans2 is comparable to existing models, with a delta of +0.7 and +1.1 for best open-source and commercial
models,respectively. Theresultsfurtherhighlightthesubstantialimprovementsmadeonlow-resourcelanguagessuch
25PublishedinTransactionsonMachineLearningResearch(12/2023)
Table 13: chrF++ scores of all the systems on the FLORES-200 devtest set in the En-Indic and Indic-En direction.
Thebest-performingsystemisbolded,whileunderlinedresultsindicatesignificantperformancedifferencewhereIT2
outperforms the system. Avg.means the average score of all the languages that system X supports. ∆represents the
differencebetweentheaveragescoresofIT2andtheaveragescoresofsystemXforthesubsetoflanguagesthatbothX
andIT2support. Apositivevaluefor ∆indicatesIT2isbetterthanXandvice-versa. †indicatescompletelyoff-target
translations.
En-Indic Indic-En
language IT1 M100 N1.2 N54 IT2 Goog Az IT1 M100 N1.2 N54 IT2 Goog Az
asm_Beng 33.5- 38.6 39.0 43.340.942.848.1- 55.3 57.856.9 57.753.4
ben_Beng 49.544.350.152.2 54.353.853.456.954.760.362.2 62.4 63.259.9
guj_Gujr 50.421.952.053.6 56.055.5 55.6 58.712.165.266.667.0 68.062.9
hin_Deva 56.653.256.558.259.6 60.259.661.360.065.066.567.5 68.065.3
kan_Knda 50.916.553.054.3 56.1 56.2 56.1 54.612.059.561.0 61.5 62.158.6
kas_Arab - - 37.2 38.0 39.7- - - - 57.8 60.2 59.7 - -
kas_Deva - - 18.7 18.8 19.2- - - - 47.7 50.648.3 - -
mai_Deva - - 46.1 47.550.5 41.4 51.0- - 66.6 68.3 69.568.865.2
mal_Mlym 49.837.849.252.6 57.3 57.3 56.857.251.761.862.9 64.3 64.5 61.3
mar_Deva 45.938.646.548.3 51.3 51.4 49.456.450.461.663.864.3 65.361.5
mni_Beng - - 37.1 42.138.2 - - - - 50.5 50.7 52.9- -
npi_Deva - 15.5 49.246.4 57.255.753.4- 41.1 65.266.968.1 68.763.9
ory_Orya 44.28.547.647.049.2 53.950.255.514.361.864.4 64.964.360.5
pan_Guru 50.626.850.951.353.5 54.3 54.2 60.044.564.566.3 66.4 67.162.7
san_Deva - - 25.8 27.1 31.6 31.3 -- - 47.8 50.7 51.6 51.2 -
sat_Olck - - 0.9 †27.0 28.4- - - - 38.7 44.339.3 - -
snd_Arab - 28.6 48.9 49.6 44.9 50.4 51.1- 19.6 64.0 66.365.1 66.659.8
tam_T aml 49.513.253.354.0 57.256.056.154.133.058.960.8 61.3 61.5 57.9
tel_T elu 52.6- 55.0 56.5 59.459.057.558.2- 63.4 65.566.1 66.763.4
urd_Arab - 39.9 49.450.3 52.251.351.6- 48.8 60.962.9 62.0 63.759.3
Avg. 48.5 28.7 43.3 45.7 48.0 51.8 53.3 56.5 36.9 58.8 60.9 61.0 64.2 61.0
∆ 5.8 25.4 4.7 2.3 - 0.3 0.2 7.4 27.7 2.2 0.1 - -0.5 3.5
as Dogri (+10), Konkani (+3.8), Kashmiri (+4.8), Maithili (+3.8), Manipuri (+9.6) for En-Indic and Dogri (+5.3),
Manipuri (+7.2), Santali (+3.9) for Indic-En translations when compared to the next best model. The observed gains
can be attributed to using high-quality human-annotated BPCC-H Wiki data for training MT models. These findings
suggest that the proposed model is well-suited for adoption in the Indian subcontinent, aligning with the objective of
buildingmodelssuitableforIndianlanguages. Additionally,wealsoreporttheCOMET( Reietal.,2022)andBLEU
(Papinenietal. ,2002)scoresforourmodelsinTable 39andTable 42(inAppendix B)whereweobservesimilartrends,
indicatingthattheobservationsarerobustacrossdifferentmetrics.
Evaluation on FLORES-200. WealsoevaluatetheMTmodelsontheFLORES-200benchmark( Costa-jussàetal. ,
2022). Throughthisevaluation,weaimtoassessthemodel’stranslationqualityonmoregeneralcontent,complement-
ingthe evaluationon ourIN22 testset whichis India-centric. Therefore, byevaluatingour models on bothIN22 and
FLORES-200,wecaneffectivelygaugethemodel’stranslationqualityindifferentsettings. TheresultsinTable 13ob-
tainedfromtheFLORES-200testsetshowasimilartrendasIN22,withIndicTrans2beingthebestopen-sourcemodel
performingcompetitivelywithcommercialmodels. TheresultsalsoshowasignificantimprovementfromIndicTrans1
to IndicTrans2, with +5.8 and +7.4 points improvement in En-Indic and Indic-En translations, respectively. We also
reporttheCOMETandBLEUscoresfortheFLORES-200benchmarkinTable 41andTable 44(inAppendix B).
Evaluation on IN22-Conv Set. WhileboththeIN22-GenSetandFLORES-200( Costa-jussàetal. ,2022)focuson
writtensentences,thereal-worldusageofMTisoftentask-orientedandinvolvesconversationallanguage. Toaddress
this,allthemodelsarefurtherevaluatedontheIN22-ConvSet,whichisdesignedtotestthetranslationqualityofMT
models on conversational language and daily use scenarios. The results of all the models on the IN22-Conv Set are
26PublishedinTransactionsonMachineLearningResearch(12/2023)
Table 14: chrF++ scores of all the systems on the IN22-Conv Evaluation set in the En-Indic and Indic-En directions.
Thebestperformingsystemisbolded,whileunderlinedresultsindicatesignificantperformancedifferencewhereIT2
outperforms the system. Avg.means the average score of all the languages that system X supports. ∆represents the
differencebetweentheaveragescoresofIT2andtheaveragescoresofsystemXforthesubsetoflanguagesthatbothX
andIT2support. Apositivevaluefor ∆indicatesIT2isbetterthanXandvice-versa. †indicatescompletelyoff-target
translations.
En-Indic Indic-En
language IT1 M100 N1.2 N54 IT2 Goog Az IT1 M100 N1.2 N54 IT2 Goog Az
asm_Beng 36.4- 42.6 43.4 46.843.646.652.5- 58.7 59.862.9 64.062.1
ben_Beng 47.539.747.148.5 49.748.948.855.248.155.457.058.4 59.658.3
brx_Deva - - - - 45.3- - - - - - 56.3- -
doi_Deva - - - - 53.940.1-- - - - 65.062.9-
gom_Deva - - - - 42.540.338.7- - - - 51.7 51.6 46.1
guj_Gujr 49.121.048.749.8 53.151.951.856.96.560.861.4 62.0 62.2 61.1
hin_Deva 48.642.747.648.349.6 50.648.757.450.658.7 59.7 60.1 60.0 59.3
kan_Knda 32.613.732.233.3 33.833.133.544.07.245.346.2 47.5 48.0 48.1
kas_Arab - - 25.7 27.1 35.6- - - - 44.6 45.2 52.6- -
mai_Deva - - 41.6 41.0 44.335.638.2- - 55.2 56.757.8 59.155.8
mal_Mlym 43.832.040.940.8 45.745.244.950.638.851.052.6 54.3 54.6 54.4
mar_Deva 43.733.944.847.3 48.646.646.354.240.456.257.558.5 59.458.3
mni_Mtei - - - - 40.231.2-- - - - 52.546.3-
npi_Deva - 15.3 44.944.3 51.546.146.4- 21.0 59.960.663.0 63.962.0
ory_Orya 38.97.641.3 40.9 40.2 37.7 42.155.611.559.3 59.8 60.3 59.058.7
pan_Guru 54.025.454.355.557.8 61.156.858.1 32.4 60.1 61.4 62.7 61.1 61.1
san_Deva - - 26.4 30.3 35.532.8-- - 38.9 40.248.3 49.2-
sat_Olck - - 0.8 18.0 34.6- - - - 33.6 37.4 43.5- -
snd_Deva - - - - 30.3- - - - - - 49.6- -
tam_T aml 37.719.237.237.1 39.138.7 39.144.122.545.7 46.845.8 46.8 46.4
tel_T elu 42.5- 39.9 40.5 45.544.644.948.5- 51.3 53.3 52.9 53.953.6
urd_Arab - 42.5 55.955.5 61.660.659.6- 47.9 61.562.3 65.5 65.3 64.9
Avg. 43.2 26.6 39.5 41.3 44.8 43.8 45.8 52.5 29.7 52.7 54.0 56.0 57.1 56.7
∆ 3.2 21.6 5.7 3.9 - 2.8 1.5 4.4 28.3 3.3 2.0 - 0.1 0.9
presentedinTable 14. Acrosstheboard,theresultsshowmoderatelystrongtranslationqualitybyallthemodels. Over-
all,asimilartrendisobservedforEn-Indictranslations,withIndicTrans2outperformingthebestopen-sourcemodels
andcommercialmodels. Similarly,inthecaseofIndic-Entranslations,IndicTrans2outperformsthebestopen-source
models and performs competitively with commercial models. The results further highlight significant improvements
inthequalityoftranslationsforlow-resourcelanguagessuchasDogri(+13.8),Kashmiri(+8.5),ManipuriMeitei(+9),
Sanskrit(+2.7), andSantali(+16.6)intheEn-IndicdirectionandKashmiri(+7.4), andSantali(+6.1)intheIndic-En
direction respectively, compared to the best available existing systems. Given that IndicTrans2 supports all 22 sched-
uledlanguagesandperformswellacrossallofthem,themodelisexpectedtohavegoodusabilityinbothinformational
and conversational settings. Additionally, we also report the COMET ( Rei et al.,2022) and BLEU ( Papineni et al. ,
2002)scoresforourmodelsintheTable 40andTable 43(inAppendix B).
Evaluation on Other Benchmarks. Weperformevaluationsonotherpubliclyavailablebenchmarksandthedetailed
results are presented in Appendix B, while a summary of the observations is presented in this section. Specifically,
we evaluate the models on WAT 2020 ( Nakazawa et al. ,2020) and WAT2021 ( Nakazawa et al. ,2021a), which were
created from the PMIndia corpus containing data from speeches and news from the Prime Minister of India. Across
the board, the results presented in Table 30and Table 31show that IndicTrans2 outperforms all open-source and
commercialmodelsinbothIndic-EnandEn-Indictranslationdirections,withtheexceptionofIndicTrans1. However,it
isimportanttonotethatperformanceimprovementforIndicTrans1stemsfromthefactthattheirvalidationsetconsisted
ofthedevelopmentsetsofvarioussharedtaskbenchmarkslikeWAT,WMT,andFLORES-200. Onthecontrary,our
27PublishedinTransactionsonMachineLearningResearch(12/2023)
workusedtheFLORES-200developmentsetasthevalidationsetwiththeaimofattainingstrongperformanceacross
multipledomains. Alongthesamelines,weevaluateourmodelsontheNTREX( Federmannetal. ,2022)Evaluation
set,whichisderivedfromthenewsdomain. TheresultspresentedinTable 27andTable 28showsimilarfindingswith
IndicTrans2 performing the best among all the compared models with +3 and +2.6 points improvement over the best
open-source model in En-Indic and Indic-En directions respectively. However, on the UFAL test set involving Tamil
language,amongopen-sourcemodels,weobservethatourmodellagsbehindtheIndicTrans1andNLLB1.2Bmodel
intheEn-Indicdirection(Table 36).
Best Open-Source Model. OurstudyevaluatedthetranslationqualityofIndicTrans2andotheropen-sourcemodels
on various benchmarks. While IN22 and FLORES-200 ( Costa-jussà et al. ,2022) evaluated the models on diverse
domaincontentsuchassports, news, andconversationaltexts, wefurthertestedthemodelsonWAT2020( Nakazawa
et al.,2020), WAT2021 ( Nakazawa et al. ,2021a), and NTREX ( Federmann et al. ,2022). Across all multi-domain
benchmarks, we observed that IndicTrans2 consistently outperformed other open-source models, demonstrating
its better translation capabilities. However,itisimportanttonotethatperformanceimprovementforIndicTrans1on
WAT2020( Nakazawaetal. ,2020)andWAT2021( Nakazawaetal. ,2021a)canbeattributedduetoexplicitoptimization
across different benchmarks by incorporating development sets of various shared tasks, in addition to FLORES-200.
Incontrast,ourdevelopmentsetonlycomprisesFLORES-200. Detailedresultsforallthebenchmarksandmodelsare
presentedinAppendix B(referTables 27,30and31). Additionally,IndicTrans2hasthehighestcoverageoflanguages
andwrittenscripts,withsupportfor22Indiclanguagesand25language-scriptcombinations. Further,whilethecurrent
SOTA open-source model, the NLLB 54B MoE model ( Costa-jussà et al. ,2022), is impressive in its capabilities, it
is impractical for deployment due to its high latency and resource requirements. Our study addresses this challenge
by developing comparatively compact models that can compete with large-scale models even when trained on
smaller datasets, emphasizing quality and cost-effectiveness. Results on different benchmarks confirm the robust
performance of our model across various domains and distributions. Therefore, we can conclude that our model has
fairgeneralizationcapabilities,performingwellacrossmostofthebenchmarks.
Supporting New Languages and Scripts. Our work bridges the gap left by existing open-source and commercial
systems by extending IndicTrans1 ( Ramesh et al. ,2022) to support all 22 scheduled Indic languages, including low-
resourcelanguagesandmultiplescripts. Wetrainthefirstopen-sourcemodelwithreasonableperformanceforthefol-
lowinglanguages: Bodo,Dogri,andKonkani. Forsomelanguages,wesupporttranslationinscriptsthatwerehitherto
unsupported like Sindhi (Devanagari script) or are only supported by commercial systems like Manipuri (Meitei). In
addition,wealsoimprovetranslationqualitysignificantlyforlow-resourcelanguagessuchasDogri,Maithili,Manipuri
(Meitei), and Nepali. The human-annotated seed parallel data (refer Table 1) for these languages help us outperform
othermodelswhichrelyonunsupervisedmethodsand/ormineddatafortheselow-resourcelanguages. Thissuggests
that investments in creating small parallel corpora for low-resource languages can substantially improve translation
quality,corroboratingfindingsfrom Costa-jussàetal. (2022).
Comparison across language families. Our analysisrevealsthat on low-resourcelanguagesfrom the Sino-Tibetan
and Austroasiatic language families models tend to consistently underperform compared to mid and high-resource
languages in the Indo-Aryan and Dravidian families. Conversely, on mid and high-resource languages, all models
seem to exhibit comparable performance. These observations suggest that the major differences in performance are
comingfromthelow-resourcelanguagefamilies. Notably,nootheropen-sourceorcommercialmodelcoversallfour
languagefamilies. TheresultsforallthemodelsonourprimarybenchmarksarepresentedinFigure 5.
Additionally,weconductasmall-scalehumanevaluationexercisetoverifyifthequalityofourmodeloutputscorrelates
withtheimprovementsobservedusingautomaticmetrics. Thispreliminaryhumanevaluationexercisefocusedonthe
En-Indicdirectionandincluded50exampleseachfromtheWikipediaandWebsourcessubsettoyieldatotalof100
sentence pairs from IN22-Gen and is described in Appendix C. However, future efforts should focus on large-scale
human evaluation to understand the potential biases and shortcomings of our IndicTrans2 models and assess their
feasibilityinpracticaluse-casescenarios.
28PublishedinTransactionsonMachineLearningResearch(12/2023)
Indo-Aryan Dravidian Sino-Tibetan Austroasiatic
Language Family203040506070Average ChrF++49.957.5
38.2
28.444.854.4
42.1
27.050.057.1
52.056.6FLORES-200 (En-Indic)
IT2 NLLB 54B MoE Google Azure
Indo-Aryan Dravidian Sino-Tibetan Austroasiatic
Language Family304050607080Average ChrF++63.7 63.3
52.9
39.362.4 62.5
50.7
44.364.4 63.761.3 60.3FLORES-200 (Indic-En)
IT2 NLLB 54B MoE Google Azure
Indo-Aryan Dravidian Sino-Tibetan Austroasiatic
Language Family102030405060Average ChrF++50.8 51.0
44.6
33.445.848.1
25.547.049.1
35.049.6 49.5IN22-Gen (En-Indic)
IT2 NLLB 54B MoE Google Azure
Indo-Aryan Dravidian Sino-Tibetan Austroasiatic
Language Family3040506070Average ChrF++65.063.3
57.9
45.364.062.9
41.464.362.9
50.760.960.0IN22-Gen (Indic-En)
IT2 NLLB 54B MoE Google Azure
Indo-Aryan Dravidian Sino-Tibetan Austroasiatic
Language Family102030405060Average ChrF++48.8
41.0 40.2
34.644.3
37.9
18.045.8
40.4
31.247.6
40.6IN22-Conv (En-Indic)
IT2 NLLB 54B MoE Google Azure
Indo-Aryan Dravidian Sino-Tibetan Austroasiatic
Language Family3040506070Average ChrF++59.8
50.152.5
43.556.8
49.7
37.459.8
50.8
46.358.9
50.6IN22-Conv (Indic-En)
IT2 NLLB 54B MoE Google Azure
Figure5: AverageperformanceimprovementsintermsofchrF++acrosslanguagefamiliesonIN22andFLORES-200
(Costa-jussàetal. ,2022)benchmarks.
7.2 Understanding Data Scale vs Quality tradeoff
PriorworkssuchasNLLB( Costa-jussàetal. ,2022)havefocusedonscalingthedatatoimprovethemodelperformance.
Theyuseamargin-basedminingapproachwithathresholdof1.06. However,fromanin-housemanualinspection,it
wasobservedthatthedatawasnoisy. Asaresult,weconductedanablationstudytounderstandthetrade-offbetween
data scale and quality for effectively training multilingual MT models. In this ablation, we consider existing mined
parallelcorporasuchasSamanantar( Rameshetal. ,2022)andNLLB( Costa-jussàetal. ,2022)andspecificallyfocus
on the subset of 11 languages that are common to both. We apply an additional quality filter, where we eliminate
the bitext pairs that fall below the LABSE ( Feng et al. ,2022) cosine similarity threshold of 0.80. This resulted in a
reductionfrom384M(Unfiltereddata)to94M(filtereddata)intotal. Subsequently,wetraintwoseparatemodelswith
the same architecture (refer to Section 5.4) and stage 1 hyperparameters (refer to Table 10) as our final IndicTrans2
modelsonfilteredandunfilteredversionsofthedata. TheresultsshowninTable 15demonstratethatthemodelstrained
on the high-quality filtered subset perform on par or even superior to the model trained on the unfiltered data. This
suggeststhat eliminating the noisy and suboptimal bitext pairs through this additional filter improves the model
performance and accelerates model convergence. We,therefore,adoptthisfilteringthresholdforourfinaltraining,
ensuringthatourmodelbenefitsfromtheimproveddataquality.
7.3 Impact of Sequential T raining with Human Annotated Data
Wetrainourmodelssequentially,wherestage1involvestrainingonacombinationofalltheexistingdata,mineddata,
andhigh-qualityseeddata,whilestage2involvesfine-tuningwithhigh-qualityseeddata(asdescribedinSection 5.5).
Our seed data involves a combination of NLLB Seed ( Costa-jussà et al. ,2022;Maillard et al. ,2023) and our human-
annotated data BPCC-H-Wiki (refer Table 1). As seed data for Sindhi (Arabic) is not present in both the sources, we
29PublishedinTransactionsonMachineLearningResearch(12/2023)
Table 15: chrF++ scores of the models trained on unfiltered (pre-filtering) and filtered data (post-filtering), on the
FLORES-200EvaluationsetintheEn-IndicandIndic-Endirections. Thebest-performingsystemisbolded. ∆repre-
sentsthedifferencebetweenthescoresofthemodeltrainedonfiltereddataandunfiltereddata. Apositivevaluefor ∆
indicatesthatthemodeltrainedonfiltereddata(post-filtering)isbetterthanunfiltered(pre-filtering)andvice-versa.
DatasetSize En-Indic Indic-En
language Pre-Filter Post-Filter Pre-Filter Post-Filter ∆ Pre-Filter Post-Filter ∆
asm_Beng 5.3M 0.5M 34.6 39.0 4.449.2 51.9 2.7
ben_Beng 70.4M 16.5M 52.2 53.1 0.960.0 60.2 0.2
guj_Gujr 14.4M 8.4M 51.4 52.4 1.0 64.0 63.9 -0.1
hin_Deva 43.1M 11M 58.1 58.7 0.664.4 64.6 0.2
kan_Knda 38.3M 10.5M 52.7 53.3 0.658.6 58.7 0.1
mal_Mlym 49.6M 10.8M 52.8 55.1 2.360.2 61.1 0.9
mar_Deva 35.6M 7.74M 46.9 48.5 1.660.6 60.7 0.1
ory_Orya 14.7M 2.9M 42.6 46.1 3.558.8 60.0 1.2
pan_Guru 14M 3.3M 49.1 50.6 1.562.7 63.1 0.4
tam_T aml 47.7M 10.4M 53.3 55.3 2.058.0 58.2 0.2
tel_T elu 51.2M 11.8M 56.0 56.8 0.863.0 63.2 0.2
Avg. - - 50.0 51.7 1.760.0 60.5 0.6
Table 16: Performance improvements of En-Indic and Indic-En models on chrF++ metric on our primary evaluation
benchmarksw.r.t. sequentialtraining.
Benchmark En-Indic Indic-En
FLORES-200 +1.5 +0.6
IN22-Gen +2.2 +0.5
IN22-Conv +2.7 +1.9
Average +2.1 +1.0
use the Sangam transliteration API33(Lehal & Saini ,2014) to transliterate the Sindhi BPCC-H-Wiki data (~ 10.5K)
fromDevanagariscripttoPerso-Arabicscript. Weobservethat fine-tuning our models with high-quality seed data
is beneficial and leads to an average improvement of 2.1points and 1point in En-Indic and Indic-En directions, re-
spectively,onourprimaryevaluationbenchmarksintermsofchrF++metric(seeTable 16). Thesefindingsalignwith
previousworks( Mohiuddinetal. ,2022),whichshowthatdeterministicdataselectioncurriculuminvolvespretraining
ongeneraldomaincorporafollowedbyfine-tuningwithhigh-qualitydatasubsetofgeneraldomaincorporaresultsin
solid performance improvements over the preliminary models. A critical distinction from the above approach is that
weonlyusethehuman-annotatedseeddataforfine-tuning,ratherthanretrievaloftop p%samplesfromtrainingdata
based on lexical similarity. Our observations indicate that although sequential training yields gains on an aggregate
level,itisimportanttonotethatforspecificlanguagessuchasSindhi(Arabic)(whereweusetransliterateddata),our
En-Indic model tends to degrade (~3 points in chrF++) in terms of performance, highlighting that it is crucial to use
high-qualityhumanannotateddataforfine-tuning.
Furthermore, Table 17reports the performance of IndicTrans2 models for various training stages on IN22-Gen Set.
Notably,thehighestimprovementwasobservedinSantalifortheEn-Indicdirectioninboth ∆1and∆2. Itisalsoworth
highlighting that the human-annotated seed data from previous work and our current work serves as the primary and
mostinfluentialsourceformid-resourceandlow-resourcelanguages,includingDogri,Konkani,Sindhi(Devanagari),
Santali,andManipuri(Meitei)asshowninTable 1. Despitethesmallersizeofseeddatacomparedtominedcorpora,
finetuningonthisleadstosuperiorperformanceacrossdifferentbenchmarks(referTables 12to14). Although ∆1and
∆2maybesmallerforafewlanguagesduetothesaturationofthedatadiversityduringmulti-stagetraining,theseed
dataprovestobebeneficialonanaggregatelevel,furtherreinforcingitspositiveimpact.
33https://sangam.learnpunjabi.org/
30PublishedinTransactionsonMachineLearningResearch(12/2023)
Table17: chrF++scoreonIN22-GenEvaluationSetforvarioustrainingstages. OGreferstothemodeltrainedonthe
originaltrainingcorpora,whileOG-Seedreferstotheseeddatafine-tunedversionoftheOGmodel. ∆1representsthe
gainsobtainedbyfine-tuningtheoriginalmodelwithseeddata. DAreferstothemodeltrainedonthecombinationof
originaltrainingdatawithaugmenteddata,whileDA-Seedreferstotheseeddatafine-tunedversionoftheDAmodel.
∆2representsthegainsobtainedbyfine-tuningonseeddataafterdataaugmentation.
En-Indic Indic-En
language OG OG-Seed ∆1DA DA+Seed ∆2OG OG-Seed ∆1DA DA+Seed ∆2
asm_Beng 43.4 45.6 2.2 44.8 47.12.361.9 62.1 0.2 64.9 65.80.9
ben_Beng 48.2 50.3 2.1 48.8 51.83.060.6 60.8 0.2 62.4 63.20.8
brx_Deva 44.5 47.1 2.6 46.3 47.81.558.1 58.4 0.3 61.9 62.10.2
doi_Deva 55.4 55.7 0.3 56.2 57.81.668.6 68.5 -0.1 72.7 72.6-0.1
gom_Deva 42.2 43.8 1.6 43.2 45.22.055.9 56.5 0.6 58.7 59.20.5
guj_Gujr 49.4 51.6 2.2 50.0 53.53.564.0 63.9 -0.1 65.7 66.50.8
hin_Deva 53.5 54.6 1.1 53.6 56.73.162.8 63.4 0.6 64.7 65.40.7
kan_Knda 47.3 49.7 2.4 47.7 51.03.361.7 62.0 0.3 63.2 64.21.0
kas_Arab 37.7 38.8 1.1 38.3 40.21.955.6 56.1 0.5 60.0 60.40.4
mai_Deva 45.9 47.3 1.4 46.2 48.72.562.1 61.9 -0.2 64.6 64.80.2
mal_Mlym 47.9 49.7 1.8 48.4 50.92.560.7 61.5 0.8 63.1 64.51.4
mar_Deva 45.7 48.6 2.9 46.6 51.04.460.7 61.1 0.4 62.3 63.71.4
mni_Mtei 39.6 41.3 1.7 41.8 44.62.853.2 53.3 0.1 57.6 57.90.3
npi_Deva 44.5 47.5 3.0 45.4 49.03.664.4 64.4 0.0 67.1 67.70.6
ory_Orya 40.1 41.9 1.8 41.0 43.92.963.1 63.4 0.3 65.3 66.20.9
pan_Guru 49.5 50.61.1 50.2 50.60.461.0 61.4 0.4 62.9 63.40.5
san_Deva 35.9 37.7 1.8 36.9 38.81.950.9 51.1 0.2 54.4 54.80.4
sat_Olck 24.2 27.3 3.1 26.5 33.46.943.6 43.8 0.2 44.5 45.30.8
snd_Deva 34.8 36.2 1.4 35.3 36.61.353.6 53.7 0.1 56.5 57.30.8
tam_T aml 47.3 48.7 1.4 47.9 49.51.657.2 57.5 0.3 59.1 59.80.7
tel_T elu 49.6 51.3 1.7 50.0 52.42.462.3 62.6 0.3 64.0 64.80.8
urd_Arab 63.8 67.1 3.3 65.4 68.22.869.5 69.9 0.4 72.5 73.00.5
Table18: ComparisonofaveragechrF++scoresbetweenourstage2auxiliarymodelandthebestopen-sourcebaseline
onFLORES-200( Costa-jussàetal. ,2022)Evaluationsetattheendofstage2auxiliarytraining. OG-seeddenotesthe
modeltrainedontheoriginaldatafollowedbyfine-tuningwithseeddata. ∆denotesthedifferencebetweenthescores
ofourstage2auxiliarymodelandthebestopen-sourcebaseline.
N54 OG-Seed ∆
xx-eng_Latn 60.9 58.1 -2.8
eng_Latn-xx 45.7 47.8 2.1
7.4 Impact of Data Augmentation
Section5.6describestheprocedureandheuristicsforsyntheticdatagenerationtofurtherimproveourauxiliarymodels.
Initially,weadoptedtheback-translationapproachforgeneratingtheaugmenteddata. Weprimarilybaseourdecisionto
startwithanauxiliaryEn-Indicmodelforgeneratingback-translationdataforIndic-Entranslationduetoitscompetitive
or better performance compared to the best open-source baseline (see Table 18). We combine the original data and
the English back-translated data, obtained using our auxiliary En-Indic model, to train our new Indic-En model from
scratch,followedbyhigh-qualityseeddatafine-tuning. Inthiscase,followingpriorstudy( Caswelletal. ,2019),weuse
“__bt__”indicatortagstoprovidesomesupervisiontothemodeltodistinguishoriginaldatafromtheback-translated
data. WeobserveaconsiderableperformanceimprovementacrossallourprimaryevaluationbenchmarksonourIndic-
En model, as shown in Figure 6when we perform training on the combination of original and back-translated data
(referTable 17).
31PublishedinTransactionsonMachineLearningResearch(12/2023)
FLORES-200
(En-Indic)FLORES-200
(Indic-En)IN22-Gen
(En-Indic)IN22-Gen
(Indic-En)IN22-Conv
(En-Indic)IN22-Conv
(Indic-En)
Benchmark3040506070Average ChrF++46.057.7
45.059.6
40.752.2
47.858.1
46.959.9
43.153.5
46.860.1
45.962.2
41.753.4
48.061.0
48.662.8
44.856.0Stage-wise Improvements of Our Models
Original Original + Seed FT Original + Data Augmentation Original + Data Augmentation + Seed FT
Figure6: AveragePerformanceofourEn-IndicandIndic-EnmodelsacrossdifferentstagesintermsofchrF++metric
onourprimaryevaluationsets.
Followingiterativebacktranslation( Hoangetal. ,2018),weusethestage2fine-tuneddownstreamIndic-Enmodelto
generatetheback-translationdataduetoitssuperiorperformancecomparedtotheauxiliaryIndic-Enmodel. Similarly,
wecombinetheIndicback-translateddataalongwiththeoriginaldatausingindicatortagsandtrainournewEn-Indic
model from scratch, followed by fine-tuning with seed data. However, we do not observe any gains for the new En-
Indicmodelcomparedtothestage2auxiliaryfine-tunedEn-Indicmodel. Furtherinvestigationisneededtodetermine
the exact reasons for the performance limitations of our newly trained En-Indic model, but we suspect that unlike for
Indic-En translation, the increase in the Indic target side data is insuﬀicient, both in terms of domain coverage and
amount. Thisconjectureisbasedonthefactthatasignificantportionofboththeoriginaltrainingcorpusandtheback-
translateddataissourcedfromthenewsdomain,resultinginconsiderableoverlapintheirdistributionalcoverage. The
lackofdiversityindomainsmaypotentiallyhinderthemodelfromreachingitsoptimalcapabilities. Furthermore,for
Indic-Entranslation,theamountoftargetsideEnglishdataalmosttriplesinamountwhenback-translateddataisadded
to the original parallel corpus. However, in the case of English-Indic translation, where multiple targetlanguagesare
involved,therelativeaugmentationperlanguageiscomparativelylower,whichmightpotentiallyexplainthemarginal
enhancement observed in the English-Indic direction. Increased availability of Indic language monolingual corpora,
ideallyfromvariousdomains,shouldhelpremedythisissue.
SincebacktranslationdidnothelpintheEn-Indicdirection,welookedatthefindingsfromdistillationworkslike Kim
&Rush(2016);Gummaetal. (2023),andtrainedanEn-Indicmodelonthecombinationoforiginaldataandforward
translateddata/distillationdata(flippingtheEnglishBTdata). Inthiscase,weuse“ __ft__”indicatortagsinsteadof
“__bt__”indicatortags. Here,weobservemarginalperformanceimprovementsforournewlytrainedEn-Indicmodel
on combining original data and forward translated data, as shown in Figure 6(refer Table 17). Although this model
isnotparticularlybetterthantheoneobtainedusingback-translation, itdoesexhibitbetterperformance, andthuswe
considerthisasourfinalEn-Indicmodel. Overall,ourEn-Indicmodeliscompetitiveorbetterwhencomparedtothe
baselines,butfurtherresearchisnecessarytoexploreeffectivemethodstoimprovetheEn-Indicmodel.
7.5 Indic-Indic Evaluation
Our IndicTrans2 models have exhibited strong performance across various benchmarks, as detailed in Section 7.1.
Buildinguponthesefindings,weaimtoconductacomprehensiveevaluationoftheIndic-Indictranslationcapabilities
ofourIndicTrans2modelsinbothpivot-basedanddirectsetups.
32PublishedinTransactionsonMachineLearningResearch(12/2023)
Table19: chrF++scoresofIndic-IndicevaluationonFLORES-200( Costa-jussàetal. ,2022)ofourIndicTrans2-Pivot
(IT2-Pivot) model, IndicTrans2-M2M (IT2-M2M) model, compressed IndicTrans2-M2M (IndicTrans2-Dist-M2M)
model and NLLB 54B MoE model. “ xx-{lang} ” and “ {lang}-xx ” denote the average chrF++ scores to that lan-
guageandfromthatlanguage,respectively.
xx-{lang} {lang}-xx
language N54 IT2-Pivot IT2-M2M IT2-Dist-M2M N54 IT2-Pivot IT2-M2M IT2-Dist-M2M
asm_Beng 36.7 38.0 37.9 37.4 39.5 41.0 39.7 39.3
ben_Beng 44.5 45.7 44.7 43.7 41.4 43.0 42.1 41.6
guj_Gujr 44.8 45.9 44.8 44.2 43.4 44.9 43.8 43.3
hin_Deva 48.4 48.6 47.7 46.8 42.9 44.6 43.8 43.6
kan_Knda 46.6 47.3 45.9 45.1 40.6 42.3 41.2 40.8
kas_Arab 32.6 33.8 33.1 32.8 40.7 41.7 39.9 39.2
mai_Deva 37.9 41.5 40.5 40.4 45.0 45.9 44.9 44.7
mal_Mlym 45.7 47.8 46.2 45.1 41.2 43.3 42.0 41.5
mar_Deva 41.9 43.6 42.5 41.7 42.4 44.1 43.0 42.5
npi_Deva 43.6 46.9 45.8 45.4 43.1 45.0 44.0 43.5
ory_Orya 41.1 41.6 40.8 40.2 42.7 44.3 43.3 42.8
pan_Guru 44.4 44.6 43.8 43.1 43.4 44.6 43.5 43.2
san_Deva 25.6 28.9 28.7 28.6 35.7 38.1 36.5 35.9
sat_Olck 25.7 26.6 26.3 26.1 32.4 31.4 32.5 31.5
tam_T aml 47.3 48.7 47.3 46.1 40.1 41.7 40.1 39.7
tel_T elu 47.0 48.5 47 46 41.9 43.7 42.6 41.8
urd_Arab 43.7 44.4 43.9 43.1 41.1 42.7 41.6 41
7.5.1 Pivoting
Pivoting(Gispert&Mariño ,2006;Utiyama&Isahara ,2007;Bertoldietal. ,2008)isawidelyusedapproachinnon-
English centric translation scenarios, where direct parallel corpora are limited or unavailable. It involves utilizing a
high-resource language as an intermediary, translating from the source to the pivot language and then to the target
language. Thepivotmethodisastrongbaselinefornon-Englishcentrictranslationcomparedtomanyothermethods
proposedtoaddressthistask( Freitag&Firat ,2020;Chenetal. ,2017;Firatetal. ,2016;Arivazhaganetal. ,2019;Al-
Shedivat & Parikh ,2019). In our study, we leverage our Indic-En model followed by the En-Indic model to facilitate
Indic-Indictranslation,asourIndicTrans2modelsaretrainedusingEnglish-centricparallelcorporaanduseEnglishas
the pivot language. To assess the Indic-Indic translation performance, we evaluate our IndicTrans2 models on n-way
paralleltestsetssuchasFLORES-200( Costa-jussàetal. ,2022)andIN22benchmarks. Thegenerationandevaluation
procedureforIndic-IndictranslationsisthesameasdescribedinSection 6.4andSection 6.5.
The performance in Indic-Indic translation for our pivot-based IndicTrans2 and NLLB ( Costa-jussà et al. ,2022) is
showninTable 19forFLORES-200,Table 20forIN22-GenandTable 21forIN22-Conv,usingaveragechrF++scores
over common languages across NLLB, our pivot as well as direct systems described in Section 7.5.2. For each lan-
guage ( lang), “ xx-{lang} ” denotes the average scores from all the common languages in that language, whereas
“{lang}-xx ”denotestheaveragescoresfromthatlanguageintoallthecommonlanguages. Table 19showsthatour
pivot-basedIndicTrans2outperformsorisonparwiththemulti-waytrainedNLLB54BMoEmodelacrossallIndic-
IndicdirectionsonFLORES-200( Costa-jussàetal. ,2022). ItisimportanttonotethatwedirectlyevaluatetheNLLB
54Bmodelbyusingthetranslationoutputs34releasedby Costa-jussàetal. (2022). However,fortheevaluationonthe
IN22 benchmark, we use the NLLB 1.2B distilled model instead of the NLLB 54B MoE model due to resource con-
straints due to the sheer number of translation directions. Our pivot-based IndicTrans2 significantly outperforms
the NLLB 1.2B distilled model , as shown in Tables 20and21. NLLB 1.2B distilled model provides a lower-bound
estimate of the performance. However, we anticipate a smaller difference between our pivot-based IndicTrans2 and
thebestNLLB54BMoEmodel. Basedonourpreviousresults,weexpectIndicTrans2scorestobecomparableifnot
34https://tinyurl.com/nllbflorestranslations
33PublishedinTransactionsonMachineLearningResearch(12/2023)
Table 20: chrF++ scores of Indic-Indic evaluation on IN22-Gen test set of our IndicTrans2-Pivot (IT2-Pivot) model,
IndicTrans2-M2M (IT2-M2M) model, compressed IndicTrans2-M2M (IndicTrans2-Dist-M2M) model and NLLB
1.2B distilled model. “ xx-{lang} ” and “ {lang}-xx ” denote the average chrF++ scores to that language and from
thatlanguage,respectively. †indicatescompletelyoff-targettranslations.
xx-{lang} {lang}-xx
language N1.2 IT2-Pivot IT2-M2M IT2-Dist-M2M N1.2 IT2-Pivot IT2-M2M IT2-Dist-M2M
asm_Beng 35.5 40.7 40.5 39.4 38.8 44.0 42.7 40.9
ben_Beng 39.9 45.1 44.8 43.2 37.4 43.2 42.3 41.2
guj_Gujr 39.2 45.4 44.3 42.9 39.0 43.8 43.2 39.9
hin_Deva 43.7 49.2 48.8 47.1 39.1 43.4 43.0 42.3
kan_Knda 39.4 44.6 44.5 43 38.4 43.9 43.1 39.8
kas_Arab 28.5 35.4 34.8 33.7 35.6 41.8 41.3 39.8
mai_Deva 36.6 42.0 41.9 40.3 39.1 44.2 43.7 42.8
mal_Mlym 38.5 44.9 43.5 42 36.4 42.9 42.2 40.6
mar_Deva 37.6 44.4 43.6 41.5 38.2 43.8 43.0 42.4
npi_Deva 37.3 41.4 41.1 39.6 39.0 44.8 44.0 43.1
ory_Orya 36.1 38.2 38.0 36.8 39.4 44.9 44.3 41.3
pan_Guru 39.0 43.2 42.2 40.9 36.8 41.7 40.5 39.1
san_Deva 23.3 35.8 35.8 34.6 32.8 39.8 39.0 37.6
sat_Olck 0.0†31.2 31.2 30 0.0†35.0 37.2 35.8
tam_T aml 40.1 45.0 44.1 42.6 35.4 41.3 40.2 39.3
tel_T elu 40.0 45.7 44.5 42.9 37.5 43.2 42.5 41.9
urd_Arab 47.7 54.6 53.2 50.8 39.4 45.2 44.4 43.6
Table21: chrF++ scores of Indic-Indic evaluationon IN22-Convtest set of our IndicTrans2-Pivot (IT2-Pivot)model,
IndicTrans2-M2M (IT2-M2M) model, compressed IndicTrans2-M2M (IndicTrans2-Dist-M2M) model and NLLB
1.2B distilled model. “ xx-{lang} ” and “ {lang}-xx ” denote the average chrF++ scores to that language and from
thatlanguage,respectively. †indicatescompletelyoff-targettranslations.
xx-{lang} {lang}-xx
language N1.2 IT2-Pivot IT2-M2M IT2-Dist-M2M N1.2 IT2-Pivot IT2-M2M IT2-Dist-M2M
asm_Beng 33.7 38.6 38.6 37.7 35.8 41.1 40.6 39.9
ben_Beng 37.6 41.6 41.5 40.5 34.8 39.8 39.8 39.1
guj_Gujr 38.1 43.5 43.1 42.1 36.5 40.9 40.5 39.7
hin_Deva 39.9 42.5 42.4 41.7 36.3 40.5 40.4 39.9
kan_Knda 28.2 30.8 30.7 30.1 30.8 35.5 34.6 33.7
kas_Arab 18.6 30.7 31.1 30.7 30.5 37.4 37.4 35.7
mai_Deva 32.2 37.9 38.4 37.8 34.8 40.0 39.6 38.9
mal_Mlym 34.9 39.7 39.0 38.0 32.9 37.7 37.3 36.4
mar_Deva 35.6 41.0 40.4 39.2 35.7 40.0 39.9 39.4
npi_Deva 35.6 42.2 42.0 41.2 36.2 41.1 40.9 40.1
ory_Orya 33.7 34.4 34.5 33.9 36.2 41.2 40.7 39.9
pan_Guru 40.9 45.5 45 44.0 35.6 40.3 39.8 39.2
san_Deva 22.3 31.8 32 31.5 26.8 34.8 34.4 33.1
sat_Olck 0.0†30.7 31.2 30.4 0.0†32.1 34.7 33.8
tam_T aml 33.2 36.2 35.6 34.9 30.7 34.3 34.0 33.3
tel_T elu 35.0 39.6 39.1 37.9 33.2 37.5 37.3 36.6
urd_Arab 43.7 49.2 48.8 47.9 36.5 41.7 41.4 40.7
34PublishedinTransactionsonMachineLearningResearch(12/2023)
betterthanthebestNLLB54BMoEmodel. ThishighlightstheeffectivenessofourrobustEnglish-centricmodelsand
theirpotentialinIndic-Indictranslationscenarios.
7.5.2 Direct Models
Whilethepivot-basedsolutiondemonstratesstrongIndic-Indicperformance,itsinherentsequentialdualmodelpipeline
resultsinincreasingtheinferencetimebyafactorof2comparedtotheEnglish-centricmodel. Toaddressthislimitation,
it is essential to build direct Indic-Indic (IndicTrans2-M2M) models that facilitate Indic-Indic translation with nearly
thesameinferencecostasEnglish-centricmodel. However,thescarcityofIndic-Indicdatamakestrainingsuchmodels
fromscratchchallenging. Asaresult,inspiredbypriorworks( Kimetal.,2019;Maetal.,2020),weleveragepre-trained
componentsfromourEnglish-centricmodelstoinitializetheIndicTrans2-M2Mmodel. Specifically,weinitializethe
IndicTrans2-M2M model using the Encoder from the Indic-En model and the Decoder from the En-Indic model. It
is important to note that these two pre-trained components undergo independent training and lack synchronization,
resulting in a lack of zero-shot performance post-initialization. Nevertheless, these pre-trained components serve as
stronginitializationstostartwithandcanbefurtheradaptedwithlimiteddata.
The BPCC-Wiki subset contains 9.2M bitext pairs spanning 462 Indic-Indic directions. This seed corpus is not com-
pletely n-way in the current form (see Section 3.3), and the data scales might be extremely low for some language
pairs. Asa result, weleveragedataaugmentation to syntheticallygenerate n-wayparallel corporajustbyperforming
ninferences instead ofnC2. Specifically, we use our IndicTrans2 En-Indic model to generate 100K synthetic bitext
pairsforeachtranslationdirectionbyselecting100KEnglishmonolingualsentencesfromIndicCorpv2( Doddapaneni
etal.,2023). Thisamountstoatotalof46.2Mpairsacross462Indic-Indiclanguagepairs. Ourfine-tuningdatasetfor
adaptingtheIndicTrans2-M2Mmodelconsistsofseedcorpusandsyntheticcorpus,resultinginatotalof55.4Mbitext
pairsacross462directions. ItisimportanttonotethatourIndicTrans2-M2Mmodelcoversall22scheduledlanguages
but lacks direct support for script variants like Kashmiri (Devanagari), Manipuri (Bengali), and Sindhi (Arabic) due
totheunavailabilityofseeddataforthesescripts. Tables 19to21showsthatourIndicTrans2-M2Machievescompet-
itive performance with a 1-point decrease in the chrF++ metric compared to the pivot-based approach at half of the
inferencecost. Furthermore,wealsoapplythesamerecipetoIndicTrans2-Dist(describedinSection 7.6)toimprove
the inference latency and compress it to about 350M parameters while achieving competitive performance with the
IndicTrans2-M2M1.2Bparametermodel(seeTables 19to21).
7.6 Distilled Models
WedistillourIndicTrans2(1.1Bparameters,12Gbsize)modelsintosmaller,eﬀicientcounterpartscalledIndicTrans2-
Dist(211Mparameters,2Gbsize)toenhancedeploymentfeasibilityinlow-infrastructuresettings. Followingthe deep
and thinarchitectureapproach( Gummaetal. ,2023),weretaintheencoder-decoderlayercountbutreduceotherfully-
connected dimensions. Acknowledging the robustness of our teacher model, we leverage a smaller, representative
dataset subset of ~110 million pairs across all 22 languages for a more data-eﬀicient distillation process. We adopt
Word-Level distillation ( Hinton et al. ,2015;Kim & Rush ,2016), facilitating direct student model training without a
separate distilled dataset. The student model is initially distilled from IndicTrans2 and subsequently fine-tuned using
the BPCC seed data. Tables 47to49in Appendix Dlist the hyperparameters and architecture of IndicTrans2-Dist
models.
In adherence to metrics used before, we report chrF++ scores of the distilled models on IN22-Gen in Table 22. The
chrF++ scores on FLORES-200 and IN22-Conv are presented in Tables 50and51in Appendix Drespectively. In
contrasttoourearlierfindings,wefindthatfine-tuningwithseeddatawasnotsobeneficialforthedistilledmodels. Our
distilled models trained with Word-Level distillation perform competitively with our best IT2 models and show
an average drop of 0.87 on Indic-En and 0.17 on En-Indic across all three benchmarks. Itisimportanttonotethat
wedonotuseanybacktranslationdatafordistillation. Notably,weobservehighergainsduetodistillationontheIN22-
ConvthanontheIN22-GenandFLORES-200intheIndic-Endirection. Low-resourcelanguageslikeDogri,Bodoand
ArabicscriptlanguageslikeKashmiriandUrdufaceadropofmorethan2.5chrF++pointsintheIndic-Endirection,
whereasSantalihasagainof2.7pointsinIN22-Genand2.8pointsinIN22-ConvascomparedtotheIndic-Enteacher
model. Almostallhigh-resourcelanguageslikeHindiandBengaliobserveanegligiblereductioninperformancewith
35PublishedinTransactionsonMachineLearningResearch(12/2023)
Table22: chrF++scoresofIndic-EnandEn-IndicdistilledmodelsonIN22-Gen. Distilled( Dist)isthemodeltrained
withWord-levelKD. ∆isthedifferencebetweenthedistilledModelfine-tunedonseeddata( Dist-Seed)&IT2. Higher
valuesof ∆arepreferable.
Indic-En En-Indic
language IT2 Dist Dist-Seed ∆IT2 Dist Dist-Seed ∆
asm_Beng 65.8 65.6 65.6 -0.2 47.1 46.4 47.1 0.0
ben_Beng 63.2 63.1 63.3 0.1 51.8 51.5 51.6 -0.2
brx_Deva 62.1 59.3 59.3 -2.8 47.8 47.6 47.7 -0.1
doi_Deva 72.6 70.2 70.2 -2.4 57.8 56.3 56.8 -1.0
gom_Deva 59.2 57.3 57.2 -2.0 45.2 44.5 44.8 -0.4
guj_Gujr 66.5 65.5 65.5 -1.0 53.5 52.9 53.2 -0.3
hin_Deva 65.4 63.7 63.8 -1.6 56.7 56.4 56.7 0.0
kan_Knda 64.2 64.3 64.3 0.1 51.0 50.4 50.9 -0.1
kas_Arab 60.4 57.6 57.8 -2.6 40.2 39.0 39.5 -0.7
mai_Deva 64.8 64.4 64.4 -0.4 48.7 48.5 48.7 0.0
mal_Mlym 64.5 63.2 63.3 -1.2 50.9 50.4 50.8 -0.1
mar_Deva 63.7 63.2 63.3 -0.4 51.0 50.4 50.6 -0.4
mni_Mtei 57.9 58.0 58.0 0.1 44.6 43.2 43.6 -1.0
npi_Deva 67.7 67.6 67.5 -0.2 49.0 48.7 49.0 0.0
ory_Orya 66.2 65.8 65.9 -0.3 43.9 43.5 43.9 0.0
pan_Guru 63.4 62.0 61.9 -1.5 50.6 50.6 50.4 -0.2
san_Deva 54.8 53.8 53.9 -0.9 38.8 37.9 38.2 -0.6
sat_Olck 45.3 47.5 48.0 2.7 33.4 33.0 33.8 0.4
snd_Deva 57.3 56.0 56.6 -0.7 36.6 36.6 36.6 0.0
tam_T aml 59.8 58.4 58.4 -1.4 49.5 49.3 49.3 -0.2
tel_T elu 64.8 63.0 63.0 -1.8 52.4 52.4 52.4 0.0
urd_Arab 73.0 70.8 70.9 -2.1 68.2 67.8 67.8 -0.4
Average 62.8 61.8 61.9 -0.9 48.6 48.1 48.3 -0.3
distillation. In contrast to the findings of Gumma et al. (2023), we observe that the most significant factor is a robust
teacher model coupled with high-quality, diverse data to develop compact student models that are comparable to the
teacher. However,extensiveexperimentsareneededtofurthervalidateandstrengthentheseobservationsinthefuture.
8 Conclusion
Inthispaper,wepresentedoureffortsonbuildingmachinetranslationsystemssupportingall22languagesinthe8th
scheduleoftheConstitutionofIndia. Wecreatedthemulti-domainIN22benchmarkandtheBPCCparallelcorpus,both
of which are first-of-their-kind evaluation and training corpora, the latter consisting of ~230M bitext pairs, covering
22 Indic languages. We trained and evaluated robust English-centric models containing 1.1B parameters as well as
theircompactversionswith211Mparameters,whichcanbeusedincompute-heavyaswellascompute-scarcesettings.
Additionally, we repurpose pre-trained components from our English-centric models for eﬀicient training of a direct
Indic-Indicmodelcontaining1.2Bparametersaswellasitscompactversionwith350Mparameters. Ourevaluations
focus on multiple automatic metrics such as BLEU, chrF++ (primary), and COMET which show that our models are
comparable,ifnotbetter,thanpubliclyavailableopenandcommercialsystems.
To summarize, our contributions comprehensively cover all three axes for translation systems, namely models, data,
andbenchmarks. Wewillopen-sourcethedata,benchmarks,andmodelartifactspubliclyandhopethatourworkwill
serveasafoundationaswellasaguideforfurtheradvancementsintranslationsystemsforIndicaswellaslow-resource
languages.
36PublishedinTransactionsonMachineLearningResearch(12/2023)
9 Limitations and Future Work
Our work has several significant positive outcomes, including the release of the first open-source model that is com-
petitivewithcommercialmodelsandsupportsall22scheduledIndianlanguages. However,somelimitationsopenup
avenuesforfutureresearchacrosseachofthefollowingaxes: Data,Models,Benchmark,Evaluation,andDeployment.
Data.One of the foremost challenges is the scarcity of high-quality human-annotated data for mid-resource or low-
resource languages, making it diﬀicult to develop robust models on these languages. Furthermore, the limited avail-
abilityofcontentintheselanguagesonthewebpreventstheuseofmining-basedapproachestoovercomedatascarcity
effectively. As a result, our IndicTrans2 models demonstrate limited generalization capabilities for languages such as
Manipuri(Meitei),Santali,andSindhi(Devnagari). Anotherimportantconcernisthelimitedeffectivenessofexisting
sentenceembeddingmodelswhenappliedtoIndiclanguages,whichcanleadtonoisyandsuboptimalpairs. Toaddress
thesechallenges,itiscrucialtocalibratesentenceembeddingmodelsusinghuman-annotateddatatoimprovetheircor-
relationwithhumanannotations. Moreover,expandingthelanguagecoverageofthesesentenceembeddingmodelsto
encompass all 22 scheduled languages will be pivotal in facilitating mining efforts for mid-resource or low-resource
languages.
Modeling. OurcurrentworkservesasaninitialefforttodevelopIndicTrans2modelssupporting22scheduledIndic
languages, including low-resource ones. Although consistently outperforming baseline systems, a performance gap
exists between low-resource and high-resource languages (as shown in Section 7.1). To bridge this gap, we need to
explore effective methods to leverage language relatedness for cross-lingual transfer and improve generalization in
low-resourcesettings. Furthermore, whileourIndicTrans2modelsreleasedwiththisworkprioritizegeneral-purpose
use cases, it is equally important to investigate sparse parameter-eﬀicient approaches for effective domain adaptation
whilealsopreservingthemodel’sgeneral-purposeutility. Furthermore,ourcurrentIndicTrans2supportstranslations
across 22 scheduled Indic languages, encompassing multiple scripts that cater to a vast majority of Indian speakers.
However, numerous Indic languages remain unincorporated, and exploring techniques to extend the current models
withoutcatastrophicforgettingisanimportantresearchdirection.
Benchmark. Accurate evaluation of translation models requires original test sets that encompass a wide range of
linguistic phenomena and translation challenges. The current test sets that are released are n-way constructed with
English as the original language, which is a common approach for including numerous languages. This implies that
whenweevaluateIndictoEnglishtranslationonbenchmarkslikeFLORES-200orIN22,oursourceistranslationese
instead of original. Prior research has emphasized the importance of utilizing source-original test sets to get a fair
evaluationof translation performance ( Zhang & Toral ,2019;Federmann et al. ,2022). Moreover, the development of
anIndicoriginalbenchmarkwouldprovideanadditionalaspectforassessingwhetherthesubtletiesofIndiclanguage
originalsentencesareaccuratelycapturedinEnglishtranslations. Therefore,wearecurrentlyworkingtowardscreating
Indic-original benchmarks to facilitate the fair evaluation of Indic-En translations. Soon, we intend to release Indic-
originaltoEnglishtranslationbenchmarksforall22scheduledIndiclanguages.
Evaluation. Evaluation of translation models is critical for understanding their strengths and weaknesses and guid-
ing further improvements. This evaluation typically involves two main approaches: human evaluation and automatic
evaluation. Our current work includes a preliminary human evaluation study on a sample of 100 sentences from our
IN22-GenbenchmarkforEn-Indictranslations. However,futureeffortsshouldfocusonconductingabroadandlarge-
scale human evaluation study that focuses on the free-form evaluation and task-oriented contexts to understand the
potentialbiasesandshortcomingsofourIndicTrans2modelsandassesstheirfeasibilityinpracticaluse-casescenarios,
thereby identifying areas for improvement. Additionally, developing better automatic evaluation metrics, particularly
suitedforIndiclanguages,isvitalforachievingamorecomprehensiveandquantitativeassessmentoftranslationquality
and facilitating model improvements. Current model-based metrics may not fully support certain languages, empha-
sizingtheneedtoexploreeffectivewaystocalibratethemforIndiclanguagesandimprovethecorrelationwithhuman
judgments.
37PublishedinTransactionsonMachineLearningResearch(12/2023)
Fairness. OurIndicTrans2modelsaretrainedonextensivedatacollectedfromtheweb,whichmayintroducesocial
biases. To ensure broader and safer accessibility, it is crucial to thoroughly identify and address these biases. Prior
works demonstrate that distilled models can further propagate or amplify biases from the teacher model ( Ahn et al.,
2022;Gupta et al. ,2022;Dhar et al. ,2021), underscoring the importance of conducting a comprehensive study and
developingalignmentmethodstomitigatesuchbiases.
10 Author Contributions
This project is a large team effort, with immense contributions from all the people involved. To list down the contri-
butionsoftheauthors,wedocumenttheareasandlisttheauthorscontributingsignificantlytoeachoftheseareas. In
each area, the contributors are listed sorted by last name. The lead authors, Jay Gala, and Pranjal A. Chitale, have
contributedacrossmultipleareasandco-ordinatedmanyactivities.
Parallel Corpus Collection and Mining: RaghavanAK,JayGala,andAswanthKumar.
Human Translation: PranjalA.Chitale,JayGala,MiteshM.Khapra,PratyushKumar,AnoopKunchukuttan,Janki
Nawale,andAnupamaSujatha.
Model Training: PranjalA.Chitale,RajDabre,JayGala,andVarunGumma.
Distillation: PranjalA.Chitale,RajDabre,JayGala,andVarunGumma.
Model Evaluation: Pranjal A. Chitale, Raj Dabre, Sumanth Doddapaneni, Jay Gala, Varun Gumma, Anoop
Kunchukuttan,andRatishPuduppully.
Research Leads: RajDabre,MiteshM.Khapra,PratyushKumar,andAnoopKunchukuttan.
Project Conceptualization and Direction: Mitesh M. Khapra, Pratyush Kumar, Anoop Kunchukuttan, and Vivek
Raghavan.
Acknowledgements
Embarkingonthismissionwasonlypossibleduetothesupportofnumerousorganizations,individualsandmembers
oftheIndianlanguagetechnologyecosystem. Wewouldliketotakeafewsentencestothankallofthem.
Sponsors/Donors : First and foremost, we thank the Ministry of Electronics and Information Technology (MeitY),
Government of India, for setting up the ambitious Digital India Bhashini Mission with the goal of advancing Indian
language technology. The human infrastructure comprising of a large team of translators, reviewers and language
experts who worked on this project were supported by the generous grant given by Digital India Bhashini Mission to
IITMadrastoserveastheDataManagementUnitforthemission.
We are indebted to Shri Nandan Nilekani and Shrimati Rohini Nilekani for believing in us and supporting our work
throughgenerousgrantsfromEkStepFoundationandNilekaniPhilanthropies. Thesegrantswereusedfor(i)support-
ing many of the students, research associates, and developers who worked on this project, (ii) fulfilling many of our
compute needs, and (iii) recruiting project managers to oversee the massive pan-India data collection activity under-
takenasapartofthiswork.
WethankMicrosoftfortheirgranttosupportthecreationofbenchmarksforIndianlanguages.
We thank the Centre for Development and Advancement of Computing, Pune (CDAC Pune) for access to its Param
Siddhisuper-computerwhichwasusedforminingbitextpairsatscale.
IIT Madras : WethankProf. VKamakoti(Director, IITMadras), Prof. MaheshVPanchagnula(Dean, IITMadras),
Prof. Ravindra Gettu (Dean, IIT Madras) and Prof. Manu Santhanam (Dean, IIT Madras) for their constant encour-
38PublishedinTransactionsonMachineLearningResearch(12/2023)
agement and administrative support. In particular, we are thankful for the oﬀice space provided to AI4Bharat which
housessomeofourstudents,researchers,languageexpertsandadministrativeteam.
Indian language technology community : WeextendourheartfeltgratitudetotheexpansiveIndianlanguagetechnol-
ogycommunity,comprisingacademia,startups,andthedeeptechindustry,bothwithinIndiaandacrosstheglobe. It
iswithimmensegratitudethatweacknowledgetheincrediblefoundationlaidbythegiantsofthiscommunity,whose
pioneeringworkhaspavedthewayforourendeavors. Wearetrulygratefulfortheknowledge,insights,andadvance-
mentsthatwehavebuiltupon, aswestandontheshouldersoftheseremarkablecontributors. Inparticular,wethank
Prof. Rajeev Sangal (Professor Emeritus, IIIT Hyderabad), Prof. Pushpak Bhattacharyya (IIT Bombay), Prof. Dipti
Mishra (IIIT Hyderabad), Prof. Hema Murthy (IIT Madras), Prof. Umesh S (IIT Madras), Prof. Rajat Moona (IIT
Gandhinagar),Prof. GaneshRamakrishnan(IITBombay),ParthaTalukdar(GoogleResearchIndia),Dr. SwaranLata
(MeitY),Dr. SobhaL(AU-KBC)andDr. RiteshKumar(Dr. B.R.AmbedkarUniversity)fortheircriticalinsightsand
constructive feedback in improving the translation guidelines used for creating the datasets released as a part of this
work(weapologizeifwehavemissedanyone).
Research organisations : WethankGoogleforopen-sourcingtheLaBSEembeddingswhichweusedextensivelyfor
miningandfilteringbitextpairs. WethankMetaforopen-sourcingtheirsemanticsearchinfrastructure,FAISS,which
we use for indexing and mining bitext pairs. We thank Allen-AI for reproducing the work of NLLB and releasing a
largeminedparallelcorpusforIndianlanguages.
Language Experts : Weexpressourdeepestgratitudetoourexceptionalandhighlydedicatedteamoflanguageexperts,
includingtranslatorsandreviewers,whoseinvaluablecontributionshavebeeninstrumentalinthecreationoftheseed
data and benchmark data. Their unwavering commitment to adhering to guidelines and their remarkable ability to
work seamlessly as a cohesive unit, despite being geographically dispersed, is truly commendable. The quality and
accuracyofthemanualdatasetsdevelopedaspartofthisendeavorowesmuchtotheirunwaveringefforts. Weextend
ourheartfeltthankstoeverymemberofourremarkablelanguageteamfortheiroutstandingdedicationandinvaluable
contributions.
Administration T eam : We are profoundly thankful to the remarkable individuals, Krishnan Karunganni S and Rav-
ishankarVenkateswaran,fortheirexceptionaldedication,patience,andextraordinaryleadershipinmanagingsuchan
expansiveteamoftalentedtranslators. Theirunwaveringcommitmenttoorchestratingandguidingthisdiversegroup
oflanguageexpertsistrulycommendable. Throughtheirexceptionalorganizationalskillsandexpertise,theyensured
seamless coordination and maintained the highest standards of quality throughout the translation process. We also
thank our support staff Shanthi S, Bhanumathy M, Bhavana R, Suganya Kumaresan, and Kalaivanan A, who helped
withrecruitmentandprocurement.
Development T eam : Wealsothankourdevelopmentteamcomprisingofourin-houseengineers,aswellas,engineers
fromTarentoforbuildingShoonyawhichenabledallthemanualtranslationwork. IntheabsenceofShoonya,itwould
havebeenimpossibletomanagesuchadiverseteamspreadacrossthecountryworkingtowardsacommongoal. We
thankmembersofourdevelopmentteamfortheirpatienceinworkingwiththelanguageexpertsandbuildingfeatures
thathelpedimproveboththespeedandqualityoftranslation.
Partners: We would also like to thank our start-up partners, viz., Desicrew, Devanagari, Language Services Bureau
andKeypointTechnologies,whohelpedinmeetingsomeofourmanualtranslationgoals.
Reviewers : WewouldliketothankDr. BenjaminMarie(4i)forreviewingthemodelingandevaluationsectionsofour
paperandhelpingusgainconfidenceinthecredibilityofourevaluationprocess.
NICT: Raj Dabre would like to thank Dr. Eiichiro Sumita and Dr. Masao Utiyama of ASTREC at NICT, for the
freedomandencouragementtocollaboratewithAI4Bharat.
Last,butnottheleast,wethanktheAlmightyforgivingusthecouragetoembarkonthismission!
39PublishedinTransactionsonMachineLearningResearch(12/2023)
References
Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and
Janyce Wiebe. SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. In
Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016) ,pp.497–511,SanDiego,
California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/S16-1081. URL https://
aclanthology.org/S16-1081 .
RoeeAharoni, MelvinJohnson, and Orhan Firat. Massivelymultilingual neural machinetranslation. In Jill Burstein,
ChristyDoran,andThamarSolorio(eds.), Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language T echnologies, NAACL-HLT 2019, Minneapolis, MN,
USA, June 2-7, 2019, Volume 1 (Long and Short P apers) ,pp.3874–3884.AssociationforComputationalLinguistics,
2019. doi: 10.18653/v1/n19-1388. URL https://doi.org/10.18653/v1/n19-1388 .
Jaimeen Ahn, Hwaran Lee, Jinhwa Kim, and Alice Oh. Why knowledge distillation amplifies gender bias and how
to mitigate from the perspective of DistilBERT. In Proceedings of the 4th Workshop on Gender Bias in Natural
Language Processing (GeBNLP) ,pp.266–272,Seattle,Washington,July2022.AssociationforComputationalLin-
guistics. doi: 10.18653/v1/2022.gebnlp-1.27. URL https://aclanthology.org/2022.gebnlp-1.27 .
MaruanAl-ShedivatandAnkurParikh. Consistencybyagreementinzero-shotneuralmachinetranslation. In Proceed-
ings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language T echnologies, Volume 1 (Long and Short P apers) ,pp.1184–1197,Minneapolis,Minnesota,June2019.As-
sociation for Computational Linguistics. doi: 10.18653/v1/N19-1121. URL https://aclanthology.org/N19-
1121.
IkramALi. Urduhacklibrary. https://github.com/urduhack/urduhack ,2019.
Ananthakrishnan, Pushpak Bhattacharyya, M. Sasikumar, and Ritesh M. Shah. Some issues in automatic evaluation
ofenglish-hindimt: Morebluesforbleu. 2006.
NaveenArivazhagan,AnkurBapna,OrhanFirat,RoeeAharoni,MelvinJohnson,andWolfgangMacherey.Themissing
ingredientinzero-shotneuralmachinetranslation. CoRR,abs/1903.07091,2019. URL http://arxiv.org/abs/
1903.07091 .
Mikel Artetxe and Holger Schwenk. Margin-based parallel corpus mining with multilingual sentence embeddings.
In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp. 3197–3203, Flo-
rence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1309. URL https:
//aclanthology.org/P19-1309 .
DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointlylearningtoalignand
translate. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference T rack Proceedings ,2015. URL http://arxiv.org/
abs/1409.0473 .
MartaBañón,PinzhenChen,BarryHaddow,KennethHeafield,HieuHoang,MiquelEsplà-Gomis,MikelL.Forcada,
AmirKamran,FaheemKirefu,PhilippKoehn,SergioOrtizRojas,LeopoldoPlaSempere,GemaRamírez-Sánchez,
ElsaSarrías,MarekStrelec,BrianThompson,WilliamWaites,DionWiggins,andJaumeZaragoza.ParaCrawl: Web-
scaleacquisitionofparallelcorpora.In Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics ,pp.4555–4567,Online,July2020.AssociationforComputationalLinguistics. doi: 10.18653/v1/2020.
acl-main.417. URL https://aclanthology.org/2020.acl-main.417 .
Ankur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Mengmeng Niu, Pallavi
Baljekar, Xavier Garcia, Wolfgang Macherey, Theresa Breiner, Vera Axelrod, Jason Riesa, Yuan Cao, Mia Xu
Chen, Klaus Macherey, Maxim Krikun, Pidong Wang, Alexander Gutkin, Apurva Shah, Yanping Huang, Zhifeng
Chen, Yonghui Wu, and Macduff Hughes. Building machine translation systems for the next thousand languages.
CoRR, abs/2205.03983, 2022. doi: 10.48550/arXiv.2205.03983. URL https://doi.org/10.48550/arXiv.
2205.03983 .
40PublishedinTransactionsonMachineLearningResearch(12/2023)
LoïcBarrault,OndřejBojar,MartaR.Costa-jussà,ChristianFedermann,MarkFishel,YvetteGraham,BarryHaddow,
Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias Müller, Santanu Pal, Matt Post, and
MarcosZampieri. Findingsofthe2019conferenceonmachinetranslation(WMT19). In Proceedings of the F ourth
Conference on Machine T ranslation (Volume 2: Shared T ask P apers, Day 1) ,pp.1–61,Florence,Italy,August2019.
Association for Computational Linguistics. doi: 10.18653/v1/W19-5301. URL https://aclanthology.org/
W19-5301 .
Loïc Barrault, Magdalena Biesialska, Ondřej Bojar, Marta R. Costa-jussà, Christian Federmann, Yvette Graham, Ro-
man Grundkiewicz, Barry Haddow, Matthias Huck, Eric Joanis, Tom Kocmi, Philipp Koehn, Chi-kiu Lo, Nikola
Ljubešić,ChristofMonz,MakotoMorishita,MasaakiNagata,ToshiakiNakazawa,SantanuPal,MattPost,andMar-
cos Zampieri. Findings of the 2020 conference on machine translation (WMT20). In Proceedings of the Fifth
Conference on Machine T ranslation ,pp.1–55,Online,November2020.AssociationforComputationalLinguistics.
URL https://aclanthology.org/2020.wmt-1.1 .
NicolaBertoldi,MadalinaBarbaiani,MarcelloFederico,andRoldanoCattoni. Phrase-basedstatisticalmachinetrans-
lation with pivot languages. In Proceedings of the 5th International Workshop on Spoken Language T ranslation:
P apers, pp. 143–149, Waikiki, Hawaii, October 20-21 2008. URL https://aclanthology.org/2008.iwslt-
papers.1 .
OndřejBojar,ChristianBuck,ChristianFedermann,BarryHaddow,PhilippKoehn,JohannesLeveling,ChristofMonz,
Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Aleš Tamchyna. Findings of the
2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine
T ranslation , pp. 12–58, Baltimore, Maryland, USA, June 2014. Association for Computational Linguistics. doi:
10.3115/v1/W14-3302. URL https://aclanthology.org/W14-3302 .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford,IlyaSutskever,andDarioAmodei. Languagemodelsarefew-shotlearners. InH.Larochelle,M.Ranzato,
R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp.
1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/
2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .
IsaacCaswell,CiprianChelba,andDavidGrangier. Taggedback-translation. In Proceedings of the F ourth Conference
on Machine T ranslation (Volume 1: Research P apers) , pp. 53–63, Florence, Italy, August 2019. Association for
ComputationalLinguistics. doi: 10.18653/v1/W19-5206. URL https://aclanthology.org/W19-5206 .
YunChen,YangLiu,YongCheng,andVictorO.K.Li. Ateacher-studentframeworkforzero-resourceneuralmachine
translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long P apers) , pp. 1925–1935, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:
10.18653/v1/P17-1176. URL https://aclanthology.org/P17-1176 .
Narayan Choudhary and Girish Nath Jha. Creating multilingual parallel corpora in indian languages. In Zygmunt
VetulaniandJosephMariani(eds.), Human Language T echnology Challenges for Computer Science and Linguistics
- 5th Language and T echnology Conference, LTC 2011, Poznań, Poland, November 25-27, 2011, Revised Selected
P apers,volume8387of Lecture Notes in Computer Science ,pp.527–537.Springer,2011. doi: 10.1007/978-3-319-
08958-4\_43. URL https://doi.org/10.1007/978-3-319-08958-4_43 .
Seamless Communication, Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-
AmbroiseDuquenne,HadyElsahar,HongyuGong,KevinHeffernan,JohnHoffman,ChristopherKlaiber,Pengwei
Li,DanielLicht,JeanMaillard,AliceRakotoarison,KaushikRamSadagopan,GuillaumeWenzek,EthanYe,Bapi
Akula,Peng-JenChen,NajiElHachem,BrianEllis,GabrielMejiaGonzalez,JustinHaaheim,PrangthipHansanti,
RussHowes,BernieHuang,Min-JaeHwang,HirofumiInaguma,SomyaJain,ElaheKalbassi,AmandaKallet,Ilia
Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh
41PublishedinTransactionsonMachineLearningResearch(12/2023)
Ramakrishnan,AnnaSun,KevinTran,TuanTran,IgorTufanov,VishVogeti,CarleighWood,YilinYang,BokaiYu,
PierreAndrews,CanBalioglu,MartaR.Costa-jussà,OnurCelebi,MahaElbayad,CynthiaGao,FranciscoGuzmán,
JustineKao,AnnLee,AlexandreMourachko,JuanPino,SravyaPopuri,ChristopheRopers,SafiyyahSaleem,Hol-
gerSchwenk,PadenTomasello,ChanghanWang,JeffWang,andSkylerWang.Seamlessm4t-massivelymultilingual
&multimodalmachinetranslation. arXiv preprint arXiv: 2308.11596 ,2023.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán,
Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation
learning atscale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pp.
8440–8451, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747.
URL https://aclanthology.org/2020.acl-main.747 .
MartaR.Costa-jussà,JamesCross,OnurÇelebi,MahaElbayad,KennethHeafield,KevinHeffernan,ElaheKalbassi,
Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi
Akula,LoicBarrault,GabrielMejiaGonzalez,PrangthipHansanti,JohnHoffman,SemarleyJarrett,KaushikRam
Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey
Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko,
Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind: Scaling human-
centeredmachinetranslation,2022.
RajDabre,ChenhuiChu,andAnoopKunchukuttan.Asurveyofmultilingualneuralmachinetranslation. ACM Comput.
Surv.,53(5):99:1–99:38,2021. doi: 10.1145/3406095. URL https://doi.org/10.1145/3406095 .
Raj Dabre, Himani Shrotriya, Anoop Kunchukuttan, Ratish Puduppully, Mitesh Khapra, and Pratyush Kumar. In-
dicbart: Apre-trainedmodelforindicnaturallanguagegeneration. InSmarandaMuresan,PreslavNakov,andAline
Villavicencio(eds.), Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-
27, 2022,pp.1849–1863.AssociationforComputationalLinguistics,2022.doi: 10.18653/v1/2022.findings-acl.145.
URL https://doi.org/10.18653/v1/2022.findings-acl.145 .
PeterTDanielsandWilliamBright. The world’s writing systems . OxfordUniversityPressonDemand,1996.
Prithviraj Dhar, Joshua Gleason, Aniket Basu Roy, Carlos Domingo Castillo, P. Jonathon Phillips, and Rama-
lingam Chellappa. Distill and de-bias: Mitigating bias in face recognition using knowledge distillation. ArXiv,
abs/2112.09786,2021. URL https://api.semanticscholar.org/CorpusID:245334459 .
SumanthDoddapaneni,RahulAralikatte,GowthamRamesh,ShreyaGoyal,MiteshM.Khapra,AnoopKunchukuttan,
and Pratyush Kumar. Towards leaving no Indic language behind: Building monolingual corpora, benchmark and
models for Indic languages. In Proceedings of the 61st Annual Meeting of the Association for Computational Lin-
guistics (Volume 1: Long P apers) , pp. 12402–12426, Toronto, Canada, July 2023. Association for Computational
Linguistics. doi: 10.18653/v1/2023.acl-long.693. URL https://aclanthology.org/2023.acl-long.693 .
Abteen Ebrahimi, Manuel Mager, Arturo Oncevay, Vishrav Chaudhary, Luis Chiruzzo, Angela Fan, John Ortega, Ri-
cardoRamos,AnnetteRios,IvanVladimirMezaRuiz,GustavoGiménez-Lugo,ElisabethMager,GrahamNeubig,
AlexisPalmer,RolandoCoto-Solano,ThangVu,andKatharinaKann. AmericasNLI:Evaluatingzero-shotnatural
language understanding of pretrained multilingual models in truly low-resource languages. In Proceedings of the
60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long P apers) , pp. 6279–6299,
Dublin,Ireland,May2022.AssociationforComputationalLinguistics. doi: 10.18653/v1/2022.acl-long.435. URL
https://aclanthology.org/2022.acl-long.435 .
SergeyEdunov,MyleOtt,MichaelAuli,andDavidGrangier. Understandingback-translationatscale. In Proceedings
of the 2018 Conference on Empirical Methods in Natural Language Processing , pp. 489–500, Brussels, Belgium,
October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1045. URL https:
//aclanthology.org/D18-1045 .
42PublishedinTransactionsonMachineLearningResearch(12/2023)
AhmedEl-Kishky, VishravChaudhary, FranciscoGuzmán, andPhilippKoehn. CCAligned: Amassivecollectionof
cross-lingual web-document pairs. In Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) , pp. 5960–5969, Online, November 2020. Association for Computational Linguistics.
doi: 10.18653/v1/2020.emnlp-main.480. URL https://aclanthology.org/2020.emnlp-main.480 .
MurrayB.Emeneau. Indiaasalingusticarea. Language,32:3,1956.
AngelaFan,ShrutiBhosale,HolgerSchwenk,ZhiyiMa,AhmedEl-Kishky,SiddharthGoyal,MandeepBaines,Onur
Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov,
EdouardGrave,MichaelAuli,andArmandJoulin. Beyondenglish-centricmultilingualmachinetranslation,2020.
Christian Federmann, Tom Kocmi, and Ying Xin. NTREX-128 – news test references for MT evaluation of 128 lan-
guages. In Proceedings of the First Workshop on Scaling Up Multilingual Evaluation ,pp.21–24,Online,November
2022.AssociationforComputationalLinguistics. URL https://aclanthology.org/2022.sumeval-1.4 .
FangxiaoyuFeng,YinfeiYang,DanielCer,NaveenArivazhagan,andWeiWang. Language-agnosticBERTsentence
embedding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long P apers) ,pp.878–891,Dublin,Ireland,May2022.AssociationforComputationalLinguistics. doi: 10.18653/
v1/2022.acl-long.62. URL https://aclanthology.org/2022.acl-long.62 .
OrhanFirat,BaskaranSankaran,YaserAl-onaizan,FatosT.YarmanVural,andKyunghyunCho. Zero-resourcetrans-
lationwithmulti-lingualneuralmachinetranslation. In Proceedings of the 2016 Conference on Empirical Methods
in Natural Language Processing ,pp.268–277,Austin,Texas,November2016.AssociationforComputationalLin-
guistics. doi: 10.18653/v1/D16-1026. URL https://aclanthology.org/D16-1026 .
Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann, Ana Sanchez, Aaron Nash, Liam
Urbach, Vishesh Kakarala, Richa Singh, Swetha Ranganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan
Tur, and Prem Natarajan. Massive: A 1m-example multilingual natural language understanding dataset with 51
typologically-diverselanguages. arXiv preprint arXiv: Arxiv-2204.08582 ,2022.
MarkusFreitagandOrhanFirat. Completemultilingualneuralmachinetranslation. In Proceedings of the Fifth Con-
ference on Machine T ranslation ,pp.550–560,Online,November2020.AssociationforComputationalLinguistics.
URL https://aclanthology.org/2020.wmt-1.66 .
MarkusFreitag,RicardoRei,NitikaMathur,Chi-kiuLo,CraigStewart,GeorgeFoster,AlonLavie,andOndřejBojar.
Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and
news domain. In Proceedings of the Sixth Conference on Machine T ranslation , pp. 733–774, Online, November
2021.AssociationforComputationalLinguistics. URL https://aclanthology.org/2021.wmt-1.73 .
Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George
Foster, Alon Lavie, and André F. T. Martins. Results of WMT22 metrics shared task: Stop using BLEU – neural
metrics are better and more robust. In Proceedings of the Seventh Conference on Machine T ranslation (WMT) , pp.
46–68, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics.
URL https://aclanthology.org/2022.wmt-1.2 .
TimnitGebru,JamieMorgenstern,BrianaVecchione,JenniferWortmanVaughan,HannaWallach,HalDauméIIIau2,
andKateCrawford. Datasheetsfordatasets,2021.
A. Gispert and José B. Mariño. Catalan-english statistical machine translation without parallel corpus : Bridging
throughspanish. ProceedingsofTheLanguageResourcesandEvaluationConference(LREC),2006.
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan,
Marc’AurelioRanzato,FranciscoGuzmán,andAngelaFan. TheFlores-101evaluationbenchmarkforlow-resource
and multilingual machine translation. T ransactions of the Association for Computational Linguistics , 10:522–538,
2022. doi: 10.1162/tacl_a_00474. URL https://aclanthology.org/2022.tacl-1.30 .
43PublishedinTransactionsonMachineLearningResearch(12/2023)
Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. Continuous measurement scales in human
evaluation of machine translation. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperabil-
ity with Discourse , pp. 33–41, Sofia, Bulgaria, August 2013. Association for Computational Linguistics. URL
https://aclanthology.org/W13-2305 .
VarunGumma,RajDabre,andPratyushKumar. Anempiricalstudyofleveragingknowledgedistillationforcompress-
ingmultilingualneuralmachinetranslationmodels. In Proceedings of the 24th Annual Conference of the European
Association for Machine T ranslation ,pp.103–114,Tampere,Finland,June2023.EuropeanAssociationforMachine
Translation. URL https://aclanthology.org/2023.eamt-1.11 .
Umang Gupta, Jwala Dhamala, Varun Kumar, Apurv Verma, Yada Pruksachatkun, Satyapriya Krishna, Rahul Gupta,
Kai-WeiChang,GregVerSteeg,andAramGalstyan. Mitigatinggenderbiasindistilledlanguagemodelsviacoun-
terfactual role reversal. In Findings of the Association for Computational Linguistics: ACL 2022 , pp. 658–678,
Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.55.
URL https://aclanthology.org/2022.findings-acl.55 .
BarryHaddowandFaheemKirefu. PMIndia–ACollectionofParallelCorporaofLanguagesofIndia. arXiv e-prints ,
art.arXiv:2001.09907,Jan2020.
Kevin Heffernan, Onur Çelebi, and Holger Schwenk. Bitext mining using distilled sentence representations for low-
resource languages. In Findings of the Association for Computational Linguistics: EMNLP 2022 , pp. 2101–2112,
AbuDhabi, UnitedArabEmirates, December2022.AssociationforComputationalLinguistics. doi: 10.18653/v1/
2022.findings-emnlp.154. URL https://aclanthology.org/2022.findings-emnlp.154 .
DanHendrycksandKevinGimpel. Bridgingnonlinearitiesandstochasticregularizerswithgaussianerrorlinearunits.
ArXiv,abs/1606.08415,2016.
GeoffreyHinton,OriolVinyals,andJeffDean. Distillingtheknowledgeinaneuralnetwork,2015.
Vu Cong Duy Hoang, Philipp Koehn, Gholamreza Haffari, and Trevor Cohn. Iterative back-translation for neural
machinetranslation. In Proceedings of the 2nd Workshop on Neural Machine T ranslation and Generation , pp.18–
24,Melbourne,Australia,July2018.AssociationforComputationalLinguistics.doi: 10.18653/v1/W18-2703.URL
https://aclanthology.org/W18-2703 .
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. XTREME: A mas-
sivelymultilingualmulti-taskbenchmarkforevaluatingcross-lingualgeneralization. CoRR,abs/2003.11080,2020.
URL https://arxiv.org/abs/2003.11080 .
GirishNathJha. TheTDILprogramandtheIndianlangaugecorporaintitiative(ILCI). In Proceedings of the Seventh
International Conference on Language Resources and Evaluation (LREC’10) ,Valletta,Malta,May2010.European
Language Resources Association (ELRA). URL http://www.lrec-conf.org/proceedings/lrec2010/pdf/
874_Paper.pdf .
Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda
Viégas,MartinWattenberg,GregCorrado,MacduffHughes,andJeffreyDean.Google’smultilingualneuralmachine
translation system: Enabling zero-shot translation. T ransactions of the Association for Computational Linguistics ,
5:339–351,2017. doi: 10.1162/tacl_a_00065. URL https://aclanthology.org/Q17-1024 .
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. The state and fate of linguis-
tic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics , pp. 6282–6293, Online, July 2020. Association for Computational Linguistics. doi:
10.18653/v1/2020.acl-main.560. URL https://aclanthology.org/2020.acl-main.560 .
HerveJégou,MatthijsDouze,andCordeliaSchmid. Productquantizationfornearestneighborsearch. IEEE T ransac-
tions on P attern Analysis and Machine Intelligence ,33(1):117–128,2011. doi: 10.1109/TPAMI.2010.57.
44PublishedinTransactionsonMachineLearningResearch(12/2023)
Divyanshu Kakwani, Anoop Kunchukuttan, Satish Golla, Gokul N.C., Avik Bhattacharyya, Mitesh M. Khapra, and
Pratyush Kumar. IndicNLPSuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual lan-
guagemodelsforIndianlanguages. In Findings of the Association for Computational Linguistics: EMNLP 2020 ,pp.
4948–4961, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-
emnlp.445. URL https://aclanthology.org/2020.findings-emnlp.445 .
HudaKhayrallahandPhilippKoehn.Ontheimpactofvarioustypesofnoiseonneuralmachinetranslation.In Proceed-
ings of the 2nd Workshop on Neural Machine T ranslation and Generation , pp. 74–83, Melbourne, Australia, July
2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-2709. URL https://aclanthology.
org/W18-2709 .
YoonKimandAlexanderM.Rush. Sequence-levelknowledgedistillation. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Processing , pp. 1317–1327, Austin, Texas, November 2016. Association
forComputationalLinguistics. doi: 10.18653/v1/D16-1139. URL https://aclanthology.org/D16-1139 .
Young Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andrés Felipe Cruz-Salinas, Liyang Lu, Amr Hendy,
Samyam Rajbhandari, Yuxiong He, and Hany Hassan Awadalla. Scalable and eﬀicient moe training for multitask
multilingualmodels. CoRR,abs/2109.10465,2021. URL https://arxiv.org/abs/2109.10465 .
YunsuKim,PetrePetrov,PavelPetrushkov,ShahramKhadivi,andHermannNey. Pivot-basedtransferlearningforneu-
ralmachinetranslationbetweennon-Englishlanguages. In Proceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pp. 866–876, Hong Kong, China, November 2019. Association for Computational Linguistics.
doi: 10.18653/v1/D19-1080. URL https://aclanthology.org/D19-1080 .
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference On
Learning Representations ,2014.
TomKocmi,ChristianFedermann,RomanGrundkiewicz,MarcinJunczys-Dowmunt,HitokazuMatsushita,andArul
Menezes. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation. In Pro-
ceedings of the Sixth Conference on Machine T ranslation , pp. 478–494, Online, November 2021. Association for
ComputationalLinguistics. URL https://aclanthology.org/2021.wmt-1.57 .
PhilippKoehn. Statisticalsignificancetestsformachinetranslationevaluation. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Processing ,pp.388–395,Barcelona,Spain,July2004.Associationfor
ComputationalLinguistics. URL https://aclanthology.org/W04-3250 .
PhilippKoehn. Europarl: Aparallelcorpusforstatisticalmachinetranslation. In Proceedings of Machine T ranslation
Summit X: P apers , pp. 79–86, Phuket, Thailand, September 13-15 2005. URL https://aclanthology.org/
2005.mtsummit-papers.11 .
Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera Tapo,
Nishant Subramani, Artem Sokolov, Claytone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb,
Benoît Sagot, Clara Rivera, Annette Rios, Isabel Papadimitriou, Salomey Osei, Pedro Javier Ortiz Suárez, Iroro
Orife, Kelechi Ogueji, Andre Niyongabo Rubungo, Toan Q. Nguyen, Mathias Müller, André Müller, Shamsud-
deen Hassan Muhammad, Nanda Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matan-
gira, Colin Leong, Nze Lawson, Sneha Kudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaventure
F. P. Dossou, Sakhile Dlamini, Nisansa de Silva, Sakine Çabuk Balli, Stella Biderman, Alessia Battisti, Ahmed
Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe Azime, Ayodele Awokoya, Duygu Ataman, Orevaoghene
Ahia, Oghenefego Ahia, Sweta Agrawal, and Mofetoluwa Adeyemi. Quality at a glance: An audit of web-crawled
multilingual datasets. T rans. Assoc. Comput. Linguistics , 10:50–72, 2022. doi: 10.1162/tacl\_a\_00447. URL
https://doi.org/10.1162/tacl_a_00447 .
Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detok-
enizerforneuraltextprocessing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
45PublishedinTransactionsonMachineLearningResearch(12/2023)
Processing: System Demonstrations ,pp.66–71,Brussels,Belgium,November2018.AssociationforComputational
Linguistics. doi: 10.18653/v1/D18-2012. URL https://aclanthology.org/D18-2012 .
Sneha Kudugunta, Ankur Bapna, Isaac Caswell, and Orhan Firat. Investigating multilingual NMT representations at
scale. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp. 1565–1575, Hong Kong,
China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1167. URL https:
//aclanthology.org/D19-1167 .
Anoop Kunchukuttan. The IndicNLP Library. https://github.com/anoopkunchukuttan/indic_nlp_
library/blob/master/docs/indicnlp.pdf ,2020.
Anoop Kunchukuttan and Pushpak Bhattacharyya. Utilizing language relatedness to improve machine translation: A
casestudyonlanguagesoftheindiansubcontinent,2020.
Anoop Kunchukuttan, Pratik Mehta, and Pushpak Bhattacharyya. The IIT Bombay English-Hindi parallel corpus.
In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018) ,
Miyazaki,Japan,May2018.EuropeanLanguageResourcesAssociation(ELRA). URL https://aclanthology.
org/L18-1548 .
GurpreetSinghLehalandTejinderSinghSaini. Sangam: APerso-ArabictoIndicscriptmachinetransliterationmodel.
In Proceedings of the 11th International Conference on Natural Language Processing , pp. 232–239, Goa, India,
December2014.NLPAssociationofIndia. URL https://aclanthology.org/W14-5135 .
DmitryLepikhin,HyoukJoongLee,YuanzhongXu,DehaoChen,OrhanFirat,YanpingHuang,MaximKrikun,Noam
Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding.
In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 .
OpenReview.net,2021. URL https://openreview.net/forum?id=qrwe7XHTmYb .
DanielLicht,CynthiaGao,JaniceLam,FranciscoGuzman,MonaDiab,andPhilippKoehn. Consistenthumanevalua-
tionofmachinetranslationacrosslanguagepairs. In Proceedings of the 15th biennial conference of the Association
for Machine T ranslation in the Americas (Volume 1: Research T rack) , pp. 309–321, Orlando, USA, September
2022. Association for Machine Translation in the Americas. URL https://aclanthology.org/2022.amta-
research.24 .
YinhanLiu, JiataoGu, NamanGoyal, XianLi, SergeyEdunov, MarjanGhazvininejad, MikeLewis, andLukeZettle-
moyer. Multilingual denoising pre-training for neural machine translation. T ransactions of the Association for
Computational Linguistics , 8:726–742, 2020a. doi: 10.1162/tacl_a_00343. URL https://aclanthology.org/
2020.tacl-1.47 .
YinhanLiu, JiataoGu, NamanGoyal, XianLi, SergeyEdunov, MarjanGhazvininejad, MikeLewis, andLukeZettle-
moyer. Multilingual denoising pre-training for neural machine translation. T rans. Assoc. Comput. Linguistics , 8:
726–742,2020b. doi: 10.1162/tacl\_a\_00343. URL https://doi.org/10.1162/tacl_a_00343 .
ShumingMa,JianYang,HaoyangHuang,ZewenChi,LiDong,DongdongZhang,HanyHassanAwadalla,Alexandre
Muzio,AkikoEriguchi,SakshamSinghal,XiaSong,ArulMenezes,andFuruWei. Xlm-t: Scalingupmultilingual
machinetranslationwithpretrainedcross-lingualtransformerencoders,2020.
AnandKumarMadasamy,AshaHegde,ShubhankerBanerjee,BharathiRajaChakravarthi,RubaPriyadharshini,Hosa-
halliShashirekha,andJohnMcCrae. OverviewofthesharedtaskonmachinetranslationinDravidianlanguages. In
Proceedings of the Second Workshop on Speech and Language T echnologies for Dravidian Languages ,pp.271–278,
Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.dravidianlangtech-
1.41. URL https://aclanthology.org/2022.dravidianlangtech-1.41 .
YashMadhani,SushaneParthan,PriyankaBedekar,GokulNc,RuchiKhapra,AnoopKunchukuttan,PratyushKumar,
andMiteshKhapra. Aksharantar: OpenIndic-languagetransliterationdatasetsandmodelsforthenextbillionusers.
46PublishedinTransactionsonMachineLearningResearch(12/2023)
In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics:
EMNLP 2023 , pp. 40–57, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/
v1/2023.findings-emnlp.4. URL https://aclanthology.org/2023.findings-emnlp.4 .
Jean Maillard, Cynthia Gao, Elahe Kalbassi, Kaushik Ram Sadagopan, Vedanuj Goswami, Philipp Koehn, Angela
Fan,andFranciscoGuzman. Smalldata,bigimpact: Leveragingminimaldataforeffectivemachinetranslation. In
Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long P apers) ,
pp.2740–2756,Toronto,Canada,July2023.AssociationforComputationalLinguistics. doi: 10.18653/v1/2023.acl-
long.154. URL https://aclanthology.org/2023.acl-long.154 .
Vukosi Marivate, Tshephisho Sefara, Vongani Chabalala, Keamogetswe Makhaya, Tumisho Mokgonyane, Rethabile
Mokoena, and Abiodun Modupe. Investigating an approach for low resource language dataset creation, curation
andclassification: Setswanaandsepedi. In Proceedings of the first workshop on Resources for African Indigenous
Languages , pp. 15–20, Marseille, France, May 2020. European Language Resources Association (ELRA). ISBN
979-10-95546-60-3. URL https://aclanthology.org/2020.rail-1.3 .
Kaushal Kumar Maurya, Rahul Kejriwal, Maunendra Sankar Desarkar, and Anoop Kunchukuttan. Utilizing lexical
similaritytoenablezero-shotmachinetranslationforextremelylow-resourcelanguages,2023.
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer,
Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the Conference
on F airness, Accountability, and T ransparency , FAT* ’19, pp. 220–229, New York, NY, USA, 2019. Association
forComputingMachinery. ISBN9781450361255. doi: 10.1145/3287560.3287596. URL https://doi.org/10.
1145/3287560.3287596 .
Nikita Moghe, Tom Sherborne, Mark Steedman, and Alexandra Birch. Extrinsic evaluation of machine translation
metrics. CoRR, abs/2212.10297, 2022. doi: 10.48550/arXiv.2212.10297. URL https://doi.org/10.48550/
arXiv.2212.10297 .
Tasnim Mohiuddin, Philipp Koehn, Vishrav Chaudhary, James Cross, Shruti Bhosale, and Shafiq Joty. Data se-
lection curriculum for neural machine translation. In Findings of the Association for Computational Linguistics:
EMNLP 2022 ,pp.1569–1582,AbuDhabi,UnitedArabEmirates,December2022.AssociationforComputational
Linguistics. doi: 10.18653/v1/2022.findings-emnlp.113. URL https://aclanthology.org/2022.findings-
emnlp.113 .
Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In International
Conference on Machine Learning ,2010.
Toshiaki Nakazawa, Katsuhito Sudoh, Shohei Higashiyama, Chenchen Ding, Raj Dabre, Hideya Mino, Isao Goto,
WinPaPa,AnoopKunchukuttan,andSadaoKurohashi. Overviewofthe5thworkshoponAsiantranslation. In Pro-
ceedings of the 32nd P acific Asia Conference on Language, Information and Computation: 5th Workshop on Asian
T ranslation: 5th Workshop on Asian T ranslation ,HongKong,1–3December2018.AssociationforComputational
Linguistics. URL https://aclanthology.org/Y18-3001 .
Toshiaki Nakazawa, Hideki Nakayama, Chenchen Ding, Raj Dabre, Shohei Higashiyama, Hideya Mino, Isao Goto,
Win Pa Pa, Anoop Kunchukuttan, Shantipriya Parida, Ondřej Bojar, and Sadao Kurohashi. Overview of the 7th
workshoponAsiantranslation. In Proceedings of the 7th Workshop on Asian T ranslation ,pp.1–44,Suzhou,China,
December2020.AssociationforComputationalLinguistics. URL https://aclanthology.org/2020.wat-1.1 .
ToshiakiNakazawa,HidekiNakayama,ChenchenDing,RajDabre,ShoheiHigashiyama,HideyaMino,IsaoGoto,Win
Pa Pa, Anoop Kunchukuttan, Shantipriya Parida, Ondřej Bojar, Chenhui Chu, Akiko Eriguchi, Kaori Abe, Yusuke
Oda,andSadaoKurohashi. Overviewofthe8thworkshoponAsiantranslation. In Proceedings of the 8th Workshop
on Asian T ranslation (WAT2021) ,pp.1–45,Online,August2021a.AssociationforComputationalLinguistics. doi:
10.18653/v1/2021.wat-1.1. URL https://aclanthology.org/2021.wat-1.1 .
47PublishedinTransactionsonMachineLearningResearch(12/2023)
Toshiaki Nakazawa, Hideki Nakayama, Isao Goto, Hideya Mino, Chenchen Ding, Raj Dabre, Anoop Kunchukuttan,
ShoheiHigashiyama,HiroshiManabe,WinPaPa,ShantipriyaParida,OndřejBojar,ChenhuiChu,AkikoEriguchi,
KaoriAbe,YusukeOda,KatsuhitoSudoh,SadaoKurohashi,andPushpakBhattacharyya(eds.). Proceedings of the
8th Workshop on Asian T ranslation (WAT2021) ,Online,August2021b.AssociationforComputationalLinguistics.
URL https://aclanthology.org/2021.wat-1.0 .
ToshiakiNakazawa,HideyaMino,IsaoGoto,RajDabre,ShoheiHigashiyama,ShantipriyaParida,AnoopKunchukut-
tan,MakotoMorishita,OndřejBojar,ChenhuiChu,AkikoEriguchi,KaoriAbe,YusukeOda,andSadaoKurohashi.
Overview of the 9th workshop on Asian translation. In Proceedings of the 9th Workshop on Asian T ranslation , pp.
1–36, Gyeongju, Republic of Korea, October 2022. International Conference on Computational Linguistics. URL
https://aclanthology.org/2022.wat-1.1 .
OpenAI. Gpt-4technicalreport. ARXIV .ORG ,2023. doi: 10.48550/arXiv.2303.08774.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli.
fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics (Demonstrations) , pp. 48–53, Minneapolis,
Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https:
//aclanthology.org/N19-4009 .
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,
Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow
instructionswithhumanfeedback,2022.
KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu. Bleu: amethodforautomaticevaluationofmachine
translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pp.311–
318,Philadelphia,Pennsylvania,USA,July2002.AssociationforComputationalLinguistics.doi: 10.3115/1073083.
1073135. URL https://aclanthology.org/P02-1040 .
JerinPhilip,ShashankSiripragada,VinayP.Namboodiri,andC.V.Jawahar. Revisitinglowresourcestatusofindian
languagesinmachinetranslation. InJayantR.Haritsa,ShouryaRoy,ManishGupta,SharadMehrotra,BalajiVasan
Srinivasan,andYogeshSimmhan(eds.), CODS-COMAD 2021: 8th ACM IKDD CODS and 26th COMAD, Virtual
Event, Bangalore, India, January 2-4, 2021 , pp. 178–187. ACM, 2021. doi: 10.1145/3430984.3431026. URL
https://doi.org/10.1145/3430984.3431026 .
Maja Popović. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the T enth Workshop
on Statistical Machine T ranslation ,pp.392–395,Lisbon,Portugal,September2015.AssociationforComputational
Linguistics. doi: 10.18653/v1/W15-3049. URL https://aclanthology.org/W15-3049 .
Maja Popović. chrF++: words helping character n-grams. In Proceedings of the Second Conference on Machine
T ranslation ,pp.612–618,Copenhagen,Denmark,September2017.AssociationforComputationalLinguistics. doi:
10.18653/v1/W17-4770. URL https://aclanthology.org/W17-4770 .
MattPost.AcallforclarityinreportingBLEUscores.In Proceedings of the Third Conference on Machine T ranslation:
Research P apers , pp. 186–191, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi:
10.18653/v1/W18-6319. URL https://aclanthology.org/W18-6319 .
RatishPuduppullyandMirellaLapata. Data-to-textgenerationwithmacroplanning. T ransactions of the Association
for Computational Linguistics ,9:510–527,2021.doi: 10.1162/tacl_a_00381.URL https://aclanthology.org/
2021.tacl-1.31 .
Ratish Puduppully, Yao Fu, and Mirella Lapata. Data-to-text generation with variational sequential planning. T rans-
actions of the Association for Computational Linguistics , 10:697–715, 2022. doi: 10.1162/tacl_a_00484. URL
https://aclanthology.org/2022.tacl-1.40 .
48PublishedinTransactionsonMachineLearningResearch(12/2023)
Mahima Pushkarna, Andrew Zaldivar, and Oddur Kjartansson. Data cards: Purposeful and transparent dataset docu-
mentationforresponsibleai,2022.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine
comprehension of text. In Jian Su, Xavier Carreras, and Kevin Duh (eds.), Proceedings of the 2016 Confer-
ence on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, T exas, USA, November 1-4,
2016, pp. 2383–2392. The Association for Computational Linguistics, 2016. doi: 10.18653/v1/d16-1264. URL
https://doi.org/10.18653/v1/d16-1264 .
Loganathan Ramasamy, Ondřej Bojar, and Zdeněk Žabokrtský. Morphological processing for English-Tamil sta-
tistical machine translation. In Proceedings of the Workshop on Machine T ranslation and P arsing in Indian
Languages , pp. 113–122, Mumbai, India, December 2012. The COLING 2012 Organizing Committee. URL
https://aclanthology.org/W12-5611 .
GowthamRamesh,SumanthDoddapaneni,AravinthBheemaraj,MayankJobanputra,RaghavanAK,AjiteshSharma,
Sujit Sahoo, Harshita Diddee, Mahalakshmi J, Divyanshu Kakwani, Navneet Kumar, Aswin Pradeep, Srihari Na-
garaj, Kumar Deepak, Vivek Raghavan, Anoop Kunchukuttan, Pratyush Kumar, and Mitesh Shantadevi Khapra.
Samanantar: The largest publicly available parallel corpora collection for 11 Indic languages. T ransactions of
the Association for Computational Linguistics , 10:145–162, 2022. doi: 10.1162/tacl_a_00452. URL https:
//aclanthology.org/2022.tacl-1.9 .
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. COMET: A neural framework for MT evaluation. In
Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 2685–
2702,Online,November2020.AssociationforComputationalLinguistics. doi: 10.18653/v1/2020.emnlp-main.213.
URL https://aclanthology.org/2020.emnlp-main.213 .
Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie,
Luisa Coheur, and André F. T. Martins. COMET-22: Unbabel-IST 2022 submission for the metrics shared task.
In Proceedings of the Seventh Conference on Machine T ranslation (WMT) , pp. 578–585, Abu Dhabi, United Arab
Emirates (Hybrid), December 2022. Association for Computational Linguistics. URL https://aclanthology.
org/2022.wmt-1.52 .
Philip Resnik and Noah A. Smith. The web as a parallel corpus. Comput. Linguistics , 29(3):349–380, 2003. doi:
10.1162/089120103322711578. URL https://doi.org/10.1162/089120103322711578 .
Hammam Riza, Michael Purwoadi, Gunarso, Teduh Uliniansyah, Aw Ai Ti, Sharifah Mahani Aljunied, Luong Chi
Mai, VuTatThang, NguyenPhuongThai, VichetChea, RapidSun, SethsereySam, SopheapSeng, KhinMarSoe,
Khin Thandar Nwet, Masao Utiyama, and Chenchen Ding. Introduction of the asian language treebank. In 2016
Conference of The Oriental Chapter of International Committee for Coordination and Standardization of Speech
Databases and Assessment T echniques (O-COCOSDA) ,pp.1–6,2016. doi: 10.1109/ICSDA.2016.7918974.
Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco Guzmán. Wikimatrix: Mining 135m
parallelsentencesin1620languagepairsfromwikipedia. InPaolaMerlo,JörgTiedemann,andReutTsarfaty(eds.),
Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main
V olume, EACL 2021, Online, April 19 - 23, 2021 ,pp.1351–1361.AssociationforComputationalLinguistics,2021a.
doi: 10.18653/v1/2021.eacl-main.115. URL https://doi.org/10.18653/v1/2021.eacl-main.115 .
Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. CCMatrix:
Miningbillionsofhigh-qualityparallelsentencesontheweb.In Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing
(V olume 1: Long P apers) , pp. 6490–6500, Online, August 2021b. Association for Computational Linguistics. doi:
10.18653/v1/2021.acl-long.507. URL https://aclanthology.org/2021.acl-long.507 .
Thibault Sellam, Dipanjan Das, and Ankur Parikh. BLEURT: Learning robust metrics for text generation. In
Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pp. 7881–7892, On-
line, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.704. URL https:
//aclanthology.org/2020.acl-main.704 .
49PublishedinTransactionsonMachineLearningResearch(12/2023)
RicoSennrich,BarryHaddow,andAlexandraBirch. Improvingneuralmachinetranslationmodelswithmonolingual
data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
P apers),pp.86–96,Berlin,Germany,August2016a.AssociationforComputationalLinguistics. doi: 10.18653/v1/
P16-1009. URL https://aclanthology.org/P16-1009 .
RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslationofrarewordswithsubwordunits. In
Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long P apers) ,
pp.1715–1725,Berlin,Germany,August2016b.AssociationforComputationalLinguistics. doi: 10.18653/v1/P16-
1162. URL https://aclanthology.org/P16-1162 .
Aditya Siddhant, Ankur Bapna, Orhan Firat, Yuan Cao, Mia Xu Chen, Isaac Caswell, and Xavier Garcia. Towards
the next 1000 languages in multilingual machine translation: Exploring the synergy between supervised and self-
supervisedlearning. arXiv preprint arXiv: 2201.03110 ,2022.
ShashankSiripragada,JerinPhilip,VinayP.Namboodiri,andCVJawahar. Amultilingualparallelcorporacollection
effort for Indian languages. In Proceedings of the Twelfth Language Resources and Evaluation Conference , pp.
3743–3751, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4.
URL https://aclanthology.org/2020.lrec-1.462 .
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple
waytopreventneuralnetworksfromoverfitting. J. Mach. Learn. Res. ,15(1):1929–1958,jan2014. ISSN1532-4435.
KarumuriVenkataSubbarao. Southasianlanguages: asyntactictypology. 2012.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception
architecture for computer vision. In 2016 IEEE Conference on Computer Vision and P attern Recognition (CVPR) ,
pp.2818–2826,2016. doi: 10.1109/CVPR.2016.308.
Xu Tan, Jiale Chen, Di He, Yingce Xia, Tao Qin, and Tie-Yan Liu. Multilingual neural machine translation with
languageclustering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) ,pp.963–973,Hong
Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1089. URL
https://aclanthology.org/D19-1089 .
Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan.
Multilingualtranslationfromdenoisingpre-training. In Findings of the Association for Computational Linguistics:
ACL-IJCNLP 2021 , pp. 3450–3466, Online, August 2021. Association for Computational Linguistics. doi: 10.
18653/v1/2021.findings-acl.304. URL https://aclanthology.org/2021.findings-acl.304 .
BrianThompson andMatt Post. Paraphrasegenerationaszero-shot multilingual translation: Disentangling semantic
similarity from lexical and syntactic diversity. In Proceedings of the Fifth Conference on Machine T ranslation ,
pp.561–570,Online,November2020.AssociationforComputationalLinguistics. URL https://aclanthology.
org/2020.wmt-1.67 .
JörgTiedemann. Paralleldata,toolsandinterfacesinOPUS. InNicolettaCalzolari,KhalidChoukri,ThierryDeclerck,
MehmetUgurDogan,BenteMaegaard,JosephMariani,JanOdijk,andSteliosPiperidis(eds.), Proceedings of the
Eighth International Conference on Language Resources and Evaluation, LREC 2012, Istanbul, T urkey, May 23-
25, 2012, pp. 2214–2218. European Language Resources Association (ELRA), 2012. URL http://www.lrec-
conf.org/proceedings/lrec2012/summaries/463.html .
MasaoUtiyamaand HitoshiIsahara. Acomparison of pivotmethodsforphrase-based statisticalmachinetranslation.
In Human Language T echnologies 2007: The Conference of the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main Conference , pp. 484–491, Rochester, New York, April 2007.
AssociationforComputationalLinguistics. URL https://aclanthology.org/N07-1061 .
50PublishedinTransactionsonMachineLearningResearch(12/2023)
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and
Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.
Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, USA , pp. 5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/
3f5ee243547dee91fbd053c1c4a845aa-Abstract.html .
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-
task benchmark and analysis platform for natural language understanding. In Tal Linzen, Grzegorz Chrupala, and
Afra Alishahi (eds.), Proceedings of the Workshop: Analyzing and Interpreting Neural Networks for NLP , Black-
boxNLP@EMNLP 2018, Brussels, Belgium, November 1, 2018 , pp. 353–355. Association for Computational Lin-
guistics,2018. doi: 10.18653/v1/w18-5446. URL https://doi.org/10.18653/v1/w18-5446 .
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,
and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding
systems. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B.
Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Con-
ference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancou-
ver , BC, Canada , pp. 3261–3275, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
4496bf24afe7fab6f046bf4923da8de6-Abstract.html .
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin,
andEdouardGrave.CCNet: Extractinghighqualitymonolingualdatasetsfromwebcrawldata.In Proceedings of the
T welfth Language Resources and Evaluation Conference , pp. 4003–4012, Marseille, France, May 2020. European
Language Resources Association. ISBN 979-10-95546-34-4. URL https://aclanthology.org/2020.lrec-
1.494.
Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei
Wang,andTie-YanLiu. Onlayernormalizationinthetransformerarchitecture. In Proceedings of the 37th Interna-
tional Conference on Machine Learning ,ICML’20.JMLR.org,2020.
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin
Raffel. mT5: Amassivelymultilingualpre-trainedtext-to-texttransformer. In Proceedings of the 2021 Conference
of the North American Chapter of the Association for Computational Linguistics: Human Language T echnologies ,
pp.483–498,Online,June2021.AssociationforComputationalLinguistics. doi: 10.18653/v1/2021.naacl-main.41.
URL https://aclanthology.org/2021.naacl-main.41 .
Mike Zhang and Antonio Toral. The effect of translationese in machine translation test sets. In Proceedings of the
F ourth Conference on Machine T ranslation (Volume 1: Research P apers) , pp. 73–81, Florence, Italy, August2019.
Association for Computational Linguistics. doi: 10.18653/v1/W19-5208. URL https://aclanthology.org/
W19-5208 .
Michal Ziemski, Marcin Junczys-Dowmunt, and Bruno Pouliquen. The united nations parallel corpus v1.0. In Nico-
lettaCalzolari,KhalidChoukri,ThierryDeclerck,SaraGoggi,MarkoGrobelnik,BenteMaegaard,JosephMariani,
Hélène Mazo, Asunción Moreno, Jan Odijk, and Stelios Piperidis (eds.), Proceedings of the T enth International
Conference on Language Resources and Evaluation LREC 2016, Portorož, Slovenia, May 23-28, 2016 . European
LanguageResourcesAssociation(ELRA),2016. URL http://www.lrec-conf.org/proceedings/lrec2016/
summaries/1195.html .
51PublishedinTransactionsonMachineLearningResearch(12/2023)
A Data Contribution and Coverage
050100150200250Seed Data
Existing Newly Added
010000200003000040000Mined Data
Existing Newly Added
asm_Beng ben_Beng brx_Deva doi_Devagom_Devaguj_Gujrhin_Deva kan_Knda kas_Arab kas_Deva mai_Deva mal_Mlym mar_Deva mni_Bengmni_Mtei npi_Deva ory_Orya pan_Guru san_Devasat_Olcksnd_Arab snd_Deva tam_Tamltel_Teluurd_Arab
Language0200004000060000Backtranslation Data
Indic BT English BTBitext Pairs (in K)
Figure 7: Overview of our training data contributions across different axes: Seed, Mined, and Backtranslation. Indic
BT indicates bitext pairs with the English side as synthetic and Indic side as original, whereas English BT indicates
viceversa.
culture7.8%economy
7.8%education
7.8%entertainment
7.8%geography
7.8%
governments
7.8%
health 7.8%
industry7.8%
legal7.8%
news6.2%
religion7.8%sports7.8%tourism7.8%IN22-Gen
arts6.3%banking
3.8%college_life
6.3%culture
4.9%daily_dialogue
7.8%entertainment
6.5%geography
7.6%
government7.7%
healthcare4.5%
history6.5%
hobbies8.0%
insurance5.5%
legal6.4%
school_life5.8%sports6.7%tourism6.1%IN22-Conv
Figure8: OverviewofthedomaincoverageofournewlycreatedIN22-Gen(left)andIN22-Conv(right)benchmarks.
52PublishedinTransactionsonMachineLearningResearch(12/2023)
B Additional Results
B.1 Zero-Shot T ranslation Capabilities of IndicT rans2 Through Cross-Lingual T ransfer
Zero-shot translation ( Johnson et al. ,2017) is a challenging task, but it is becoming increasingly feasible with the
development of more powerful MT models. Zero-shot translation refers to the ability of an MT model to translate
from a source language to a target language, even if it has never seen any training data for the language pair before.
This is primarily attributed to cross-lingual transfer learning that involves knowledge transfer from one language to
another. There are several benefits to good zero-shot performance. First, it indicates that the MT model has good
generalizationcapabilities,whichmeansthatthemodelisabletolearntheunderlyingstructureoflanguagesratherthan
simplymemorizingspecifictranslationpairs. Second,itsuggeststhattheMTmodelcanlearnlanguagerepresentations
shared across different languages. In addition, this makes it easier to extend the model to new languages, even with
limiteddata.
Table23: chrF++scoresofourIT2inthezero-shotsettingintheIndic-EndirectiononIndiclanguagesontheFLORES-
200Evaluationset. Thebest-performingsystemisbolded,and ∆representsthedifferencebetweenthezero-shotscore
ofIT2andthescoreoftheSOTAmodel. IT2resultsarepresentedbasedonthedecodingfortheMaithililanguagetag.
language N1.2 N54 IT2 ∆
awa_Deva 63.2 65.462.4 -3.0
bho_Deva 57.3 58.553.6 -4.9
hne_Deva 70.6 72.262.1 -10.1
mag_Deva 70.3 72.067.4 -4.6
In this study, we investigate the cross-lingual transfer and generalizability of our IndicTrans2 models. Our focus lies
on performing zero-shot evaluation on a set of additional low-resource Indic languages, which are supported by the
NLLB (Costa-jussà et al. ,2022) models (1.2B distilled and 54B MoE) and are included as part of the FLORES-200
(Costa-jussàetal. ,2022)evaluationset. Specifically,werestrictourevaluationstotheIndic-Enmodel,asthestructure
and syntax of these low-resource languages as target translation is unseen by the model and therefore, result in off-
targettranslation. However,inthecaseoftheIndic-Endirection,suchananalysisisfeasiblesincethetargetlanguage,
English,issupportedbythemodel. WeconsiderlanguageslikeAwadhi,Bhojpuri,Chhattisgarhi,andMagahithatare
writtenintheDevanagariscript,whichistheprominentscriptsupportedbyourmodels. Wealsohavetestsetsavailable
in FLORES-200 for evaluation. We employ a top-down approach based on language similarity to facilitate zero-shot
decoding. Specifically, we select the top-3 related languages that are closest to the aforementioned languages under
consideration. Usingthisapproach,weidentifyHindi,Maithili,andNepaliasthethreeclosestlanguagesandleverage
theirlanguagecodesforzero-shotdecodingofthenewIndiclanguages. Wefollowthesamegenerationandevaluation
procedurementionedinSection 6.4andSection 6.5. WeobservethatdecodingwiththelanguagetagofMaithiliyields
the best performance on the test set across all four languages, followed by Hindi and Nepali. This finding highlights
that Maithili is closer to these languages in the embedding space than Hindi or Nepali. Table 23demonstrates that
our IndicTrans2 model differs by around 4 points on average except for Chhattisgarhi when compared to the NLLB
54B MoE model that is explicitly using the sentence pairs of the aforementioned languages in training. Overall, our
IndicTrans2 model shows promising results in zero-shot performance on low-resource languages, highlighting
the potential for extending to new languages with limited data in the future .
B.2 T ranslation Capabilities of Zero-Shot Prompted LLMs
Large language models (LLMs) such as GPT ( Brown et al. ,2020;OpenAI,2023) have recently shown impressive
zero-shot performance on various tasks. In this work, we compare the zero-shot translation capabilities of GPT3.5
(as described in Section 6.1) with our best IndicTrans2 model. The prompt template “ Translate the following
sentence into {{lang}}\n {{text}} ” was used for evaluation. Table 24demonstrates that our IndicTrans2
models outperform GPT3.5 by a significant margin on both the IN22-Gen and IN22-Conv sets in both En-Indic
and Indic-En directions. However, it is important to note that this gap is comparatively lower on the IN22-Conv
53PublishedinTransactionsonMachineLearningResearch(12/2023)
Table24: chrF++scoresofGPT3.5( gpt-3.5-turbo )ontheIN22-Gen(left)andIN22-Conv(right)Evaluationsets
in the En-Indic and Indic-En directions. Avg.means the average score of all the top-13 languages. ∆represents the
differencebetweenthescoresofIT2andGPT3.5. Positive ∆indicatesIT2isbetterthanXandvice-versa.
IN22-Gen IN22-Conv
En-Indic Indic-En En-Indic Indic-En
language GPT3.5 IT2 ∆GPT3.5 IT2 ∆GPT3.5 IT2 ∆GPT3.5 IT2 ∆
asm_Beng 25.9 47.121.246.9 65.818.927.2 46.819.643.6 62.919.3
ben_Beng 39.9 51.811.952.1 63.211.139.9 49.79.852.9 58.45.5
guj_Gujr 35.6 53.517.951.7 66.514.836.0 53.117.150.9 62.011.1
hin_Deva 47.1 56.79.657.7 65.47.746.0 49.63.657.0 60.13.1
kan_Knda 34.5 51.016.551.7 64.212.527.9 33.85.942.1 47.55.4
mal_Mlym 31.6 50.919.347.8 64.516.730.4 45.715.344.0 54.310.3
mar_Deva 33.9 51.017.150.3 63.713.434.0 48.614.647.6 58.510.9
npi_Deva 37.2 49.011.854.2 67.713.538.3 51.513.252.0 63.011.0
ory_Orya 27.8 43.916.148.0 66.218.225.6 40.214.645.2 60.315.1
pan_Guru 36.2 50.614.451.7 63.411.740.6 57.817.253.3 62.79.4
tam_T aml 34.0 49.515.541.3 59.818.529.7 39.19.438.0 45.87.8
tel_T elu 34.3 52.418.146.5 64.818.332.1 45.513.442.4 52.910.5
urd_Arab 47.6 68.220.658.8 73.014.249.0 61.612.657.1 65.58.4
Avg. 35.8 52.016.250.7 65.214.635.1 47.912.848.2 58.09.8
set, likely because GPT3.5 was fine-tuned towards fluency in conversational and interactive contexts. Inaddition,
theaverage ∆acrossboththeIN22-GenandIN22-Convsetsislowerforhigh-resourcelanguagessuchasHindi(+6.6
forIndic-Enand+5.4forEn-Indic)thanlow-resourcelanguagessuchasAssamese(+19.1forIndic-Enand+20.4for
En-Indic). Overall,ourIndicTrans2modelsoutperformGPT3.5byanaverageof12.2pointsand14.5pointsinIndic-
En and En-Indic directions, respectively, on our IN22 benchmark. Even though LLMs show promising zero-shot
capabilities in multilingual settings, we observe that these still lag behind the task-specific models, particularly
for low-resource languages. Exploring how richer translations can be extracted from LLMs is an open problem and
canbeaworthyfuturestudy.
B.3 Comparison with SeamlessM4T Multimodal T ranslation Model
SeamlessM4T( Communicationetal. ,2023)isarecentlyreleasedmultimodaltranslationmodelsupporting16Indiclan-
guages. Intheinterestofthecommunity,wereportpreliminaryresultsofthismodelonourprimarybenchmarks,such
asFLORES-200,IN22-GenandIN22-ConvinTables 25and26. WeusetheSeamlessM4T-LargeandSeamlessM4T-
Largev2variants,whichareboth2.3Bparametermodelsandthebestmodelreleasedasapartofthework.
B.4 Results on NTREX
NTREX (Federmann et al. ,2022) is a news-domain benchmark that expands coverage of languages of test data from
WMT 2019 ( Barrault et al. ,2019) to 128 languages. Out of these, 13 are scheduled Indic languages. The detailed
resultsarereportedinTables 27to29.
B.5 Results on W A T2020 & W A T2021
WAT (Nakazawa et al. ,2020;2021a) included support for translations for 8 Indic languages in the news domain. In
addition,theyreleaseddataforHindi-EnglishdatainITandWikiNewsdomains. WAT2021( Nakazawaetal. ,2021a)
created a benchmark for translation between 10 Indic languages and English. The detailed results are reported in
Tables30to35.
54PublishedinTransactionsonMachineLearningResearch(12/2023)
Table25: chrF++scoresofSM4T(SeamlessM4T-Large),SM4Tv2(SeamlessM4T-Largev2)andIT2ontheFLORES-
200 Evaluation sets in the En-Indic and Indic-En directions. Avg.means the average score of all the supported lan-
guages.
En-Indic Indic-En
language SM4T SM4Tv2 IT2 SM4T SM4Tv2 IT2
asm_Beng 41.4 38.7 43.356.4 55.9 56.9
ben_Beng 52.0 50.3 54.361.9 60.3 62.4
guj_Gujr 53.3 51.9 56.066.2 65.0 67.0
hin_Deva 58.3 57.5 59.666.0 62.7 67.5
kan_Knda 54.2 52.4 56.160.4 59.1 61.5
mai_Deva 46.8 43.1 50.566.7 66.1 69.5
mal_Mlym 53.0 50.3 57.363.2 60.9 64.3
mar_Deva 48.8 46.3 51.363.0 62.1 64.3
mni_Beng 38.2 39.0 38.250.9 50.0 52.9
npi_Deva 52.8 50.7 57.266.3 65.5 68.1
ory_Orya 49.946.0 49.2 63.2 62.7 64.9
pan_Guru 52.6 50.6 53.565.4 64.2 66.4
snd_Arab 51.8 49.8 44.964.3 61.0 65.1
tam_T aml 54.8 52.6 57.259.4 57.7 61.3
tel_T elu 56.7 54.6 59.465.1 62.8 66.1
urd_Arab 50.1 49.4 52.262.0 59.9 62.0
Avg. 50.9 49.0 52.5 62.5 61.0 63.8
Table26: chrF++scoresofSM4T(SeamlessM4T-Large),SM4Tv2(SeamlessM4T-Largev2)andIT2ontheIN22-Gen
(left)andIN22-Conv(right)EvaluationsetsintheEn-IndicandIndic-Endirections. Avg.meanstheaveragescoreof
allthesupportedlanguages.
IN22-Gen IN22-Conv
En-Indic Indic-En En-Indic Indic-En
language SM4T SM4Tv2 IT2 SM4T SM4Tv2 IT2 SM4T SM4Tv2 IT2 SM4T SM4Tv2 IT2
asm_Beng 43.8 40.6 47.163.8 62.4 65.845.5 43.9 46.860.5 60.6 62.9
ben_Beng 47.9 46.2 51.861.7 58.7 63.247.7 46.8 49.757.8 57.2 58.4
guj_Gujr 49.1 47.5 53.564.9 62.5 66.549.7 48.9 53.161.7 60.8 62.0
hin_Deva 53.5 52.8 56.762.6 59.8 65.447.1 47.1 49.659.5 58.3 60.1
kan_Knda 47.5 46.4 51.062.7 59.9 64.232.3 32.0 33.846.3 45.2 47.5
mai_Deva 45.4 41.9 48.763.2 61.4 64.842.8 41.6 44.356.5 55.6 57.8
mal_Mlym 46.9 45.1 50.961.6 57.9 64.542.0 41.3 45.753.4 51.4 54.3
mar_Deva 45.3 43.3 51.061.6 60.1 63.746.0 44.5 48.657.6 57.1 58.5
npi_Deva 46.8 44.2 49.066.0 64.8 67.747.7 46.5 51.561.1 61.2 63.0
ory_Orya 45.240.9 43.9 64.2 62.6 66.2 42.741.0 40.2 60.2 60.0 60.3
pan_Guru 49.7 48.0 50.661.1 59.4 63.456.5 55.2 57.861.6 61.2 62.7
tam_T aml 47.5 45.9 49.5 57.954.8 59.837.4 37.0 39.1 46.245.4 45.8
tel_T elu 49.1 47.2 52.462.6 58.9 64.839.8 39.3 45.552.8 51.6 52.9
urd_Arab 62.7 60.5 68.269.4 66.2 73.055.0 54.2 61.662.8 62.7 65.5
Avg. 48.6 46.5 51.7 63.1 60.7 65.2 45.2 44.2 47.7 57.0 56.3 58.0
B.6 Results on WMT & UF AL
WMThascreatedbenchmarksforselectedIndiclanguagesaspartofsharedtasksin2014(Hindi)( Bojaretal. ,2014),
2019(Gujarati)( Barraultetal. ,2019)and2020(Tamil)( Barraultetal. ,2020).
55PublishedinTransactionsonMachineLearningResearch(12/2023)
Table27: chrF++scoresofallthesystemsontheNTREX( Federmannetal. ,2022)EvaluationsetintheEn-Indicand
Indic-En direction. The best-performing system is bolded, while underlined results indicate significant performance
differencewhereIT2outperformsthesystem. AvgmeanstheaveragescoreofallthelanguagesthatsystemXsupports.
∆represents the difference between the average scores of IT2 and the average scores of system X for the subset of
languagesthatbothXandIT2support. Apositivevaluefor ∆indicatesIT2isbetterthanXandvice-versa.
En-Indic Indic-En
Language IT1 M100 N1.2 IT2 Goog Az IT1 M100 N1.2 IT2 Goog Az
ben_Beng 48.445.850.8 54.053.552.055.953.860.462.9 63.359.9
guj_Gujr 44.419.547.8 49.649.3 49.757.510.963.7 66.8 66.7 61.9
hin_Deva 50.048.051.653.3 53.752.157.455.961.5 63.7 63.6 59.7
kan_Knda 49.214.351.2 54.1 54.0 54.1 52.612.057.9 61.2 61.3 57.3
mal_Mlym 43.432.641.7 48.648.047.051.947.356.7 59.660.056.5
mar_Deva 40.636.543.5 47.046.444.554.048.359.7 62.7 63.0 57.5
npi_Deva - 14.2 41.7 45.044.741.5- 37.4 62.2 64.4 65.559.8
pan_Guru 47.527.749.150.3 51.650.356.743.061.8 64.9 65.0 60.4
snd_Arab - 25.1 39.7 43.342.141.1- 17.8 55.8 58.2 58.5 52.1
tam_T aml 41.814.843.7 45.945.445.449.429.554.5 57.0 57.2 53.4
tel_T elu 42.0- 43.9 46.7 46.8 43.848.7- 53.1 55.6 55.8 52.2
urd_Arab - 41.7 51.4 53.753.152.9- 48.2 60.662.5 63.059.6
Avg. 45.3 29.1 46.3 49.3 49.1 47.9 53.8 36.7 59.0 61.6 61.9 57.6
∆ 4.6 20.4 3.0 - 0.2 1.4 7.7 25.5 2.6 - -0.3 4.0
Table28: COMETscoresofallthesystemsontheNTREX( Federmannetal. ,2022)EvaluationsetintheEn-Indicand
Indic-En direction. The best performing system is bolded, while underlined results indicate significant performance
differencewhereIT2outperformsthesystem.
En-Indic Indic-En
Language IT1 M100 N1.2 IT2 Goog Az IT1 M100 N1.2 IT2 Goog Az
ben_Beng 85.382.686.186.4 85.2 86.786.585.888.488.9 89.388.0
guj_Gujr 86.861.686.7 87.987.187.586.336.488.8 89.6 89.7 87.6
hin_Deva 77.674.377.9 78.777.9 78.786.285.688.0 88.4 88.5 86.9
kan_Knda 84.151.684.5 85.684.2 85.884.535.986.7 87.7 87.7 86.2
mal_Mlym 85.775.486.3 87.586.587.485.581.587.6 88.3 88.7 87.3
mar_Deva 71.865.873.9 74.673.1 74.585.480.487.6 88.4 88.5 86.5
npi_Deva - 51.8 79.1 80.679.879.9- 68.4 89.189.4 90.187.8
pan_Guru 82.460.5 83.1 83.0 82.9 83.2 84.373.686.987.6 87.885.4
tam_T aml 85.553.486.086.4 86.2 87.382.962.685.285.9 86.284.2
tel_T elu 83.5- 83.4 85.184.4 85.383.6- 86.2 87.0 87.1 85.3
urd_Arab - 72.7 81.082.2 82.3 83.7- 79.9 87.387.8 88.086.9
UFAL(Ramasamyetal. ,2012)isanEnglish-Tamilbilingualbenchmarkcreatedfrompubliclyavailablewebsites. The
benchmarkconsistsofEnglishsentencesfromdomainssuchascinema,news,andsomebiblicalsources.
DetailedresultsarereportedinTables 36to38.
B.7 COMET Scores for IN22 & FLORES
WereportCOMET( Reietal.,2022)scoresforIN22andFLORES( Costa-jussàetal. ,2022)inTables 39to41
B.8 BLEU Scores for IN22 & FLORES
WereportBLEU( Papinenietal. ,2002)scoresforIN22andFLORES( Costa-jussàetal. ,2022)inTables 42to44
56PublishedinTransactionsonMachineLearningResearch(12/2023)
Table29: BLEUscoresofallthesystemsontheNTREX( Federmannetal. ,2022). EvaluationsetintheEn-Indicand
Indic-En direction. The best performing system is bolded, while underlined results indicate significant performance
differencewhereIT2outperformsthesystem.
En-Indic Indic-En
Language IT1 M100 N1.2 IT2 Goog Az IT1 M100 N1.2 IT2 Goog Az
ben_Beng 17.715.319.8 22.9 23 20.830.727.736.6 40.2 40.5 35.4
guj_Gujr 15.43.118.7 20.4 20.7 20.5 32.10.540.5 45.244.637.4
hin_Deva 26.424.328.230.5 31.22931.128.937.7 41.340.734.6
kan_Knda 16.41.118.522 22.7 22.7 26.90.533.8 38.6 38.6 31.8
mal_Mlym 115.48.1 14.5 14.6 1425.619.531.434.8 35.928.9
mar_Deva 10.58.212.214.6 15.112.72821.835.6 40.1 40.1 31.1
npi_Deva - 0.8 11.5 13.7 13.7 10.8- 10.3 38.942.4 43.435.1
pan_Guru 22.68.524.525.5 26.824.631.51438.6 43.3 43.2 35.9
snd_Arab - 6.4 13.7 18.71615- 2 33.3 37.1 37 28.5
tam_T aml 90.89.9 11.8 11.9 11.423.56.130.2 33.4 33.6 26.9
tel_T elu 11- 12.1 15.4 15.6 1222.8- 28.7 32.6 32.6 27.1
urd_Arab - 18.3 27.7 30.530.129.5- 22 36.5 39.4 39.6 35.1
Avg. 15.6 8.4 17.1 20 20.1 18.6 28 13.9 35.2 39 39.2 32.3
∆ 4.1 12.1 2.9 - -0.1 1.4 10.8 25.7 3.8 - -0.2 6.7
Table30: chrF++scoresofallthesystemsontheWAT-2020( Nakazawaetal. ,2020). EvaluationsetintheEn-Indicand
Indic-En direction. The best performing system is bolded, while underlined results indicate significant performance
differencewhereIT2outperformsthesystem. AvgmeanstheaveragescoreofallthelanguagesthatsystemXsupports.
∆represents the difference between the average scores of IT2 and the average scores of system X for the subset of
languagesthatbothXandIT2support. Apositivevaluefor ∆indicatesIT2isbetterthanXandvice-versa.
En-Indic Indic-En
Language IT1 M100 N1.2 IT2 Goog Az IT1 M100 N1.2 IT2 Goog Az
ben_Beng 38.931.736.637.9 36.4 37.5 44.036.142.5 44.242.943.2
guj_Gujr 42.7 18.2 41.241.9 41.2 45.448.69.248.149.3 48.0 49.6
hin_Deva 43.335.641.341.8 42.7 41.7 48.140.547.048.9 49.5 49.5
mal_Mlym 38.429.836.2 38.838.2 38.644.531.943.145.0 43.5 45.5
mar_Deva 41.631.939.841.0 40.5 40.744.934.444.2 45.845.044.9
tam_T aml 37.515.136.4 37.936.8 37.9 42.919.841.7 42.941.3 43.0
tel_T elu 37.2- 36.7 37.7 36.8 38.043.0- 42.2 43.742.5 43.8
Avg. 39.9 27.1 38.3 39.6 38.9 40.0 45.1 28.7 44.1 45.7 44.7 45.6
∆ -0.3 12.8 1.4 - 0.7 -0.4 0.6 17.3 1.6 - 1.0 0.1
57PublishedinTransactionsonMachineLearningResearch(12/2023)
Table31: chrF++scoresofallthesystemsontheWAT-2021( Nakazawaetal. ,2021a). EvaluationsetintheEn-Indic
andIndic-Endirection. Thebestperformingsystemisbolded,whileunderlinedresultsindicatesignificantperformance
differencewhereIT2outperformsthesystem. AvgmeanstheaveragescoreofallthelanguagesthatsystemXsupports.
∆represents the difference between the average scores of IT2 and the average scores of system X for the subset of
languagesthatbothXandIT2support. Apositivevaluefor ∆indicatesIT2isbetterthanXandvice-versa.
En-Indic Indic-En
Language IT1 M100 N1.2 IT2 Goog Az IT1 M100 N1.2 IT2 Goog Az
ben_Beng 45.434.741.442.4 39.1 41.6 53.742.551.652.5 49.9 50.0
guj_Gujr 53.9 21.4 51.8 52.1 48.9 58.2 62.88.061.2 62.959.962.1
hin_Deva 60.851.059.259.7 59.3 59.6 66.154.263.465.1 64.9 65.9
kan_Knda 52.517.650.250.9 49.0 51.8 60.08.958.3 60.357.055.0
mal_Mlym 49.532.744.949.2 47.5 46.4 58.435.056.2 58.355.257.7
mar_Deva 50.436.247.849.0 47.5 48.0 57.140.255.2 57.154.355.1
ory_Orya 48.57.447.5 44.2 40.2 45.4 57.213.255.7 56.852.956.0
pan_Guru 56.1 25.6 53.054.2 52.6 58.7 65.231.962.964.8 62.2 63.5
tam_T aml 48.814.346.047.5 45.7 47.2 56.618.654.055.6 51.6 54.0
tel_T elu 46.7- 44.9 45.3 43.0 43.0 59.7- 56.5 59.656.058.3
Avg. 51.3 26.8 48.7 49.4 47.3 50.0 59.7 28.1 57.5 59.3 56.4 57.8
∆ -1.9 23.1 0.7 - 2.1 -0.6 31.2 1.8 - 2.9 1.5
Table32: COMETscoresofallthesystemsontheWAT2020( Nakazawaetal. ,2020). EvaluationsetintheEn-Indic
andIndic-Endirection. Thebestperformingsystemisbolded,whileunderlinedresultsindicatesignificantperformance
differencewhereIT2outperformsthesystem.
En-Indic Indic-En
Language IT1 M100 N1.2 IT2 Goog Az IT1 M100 N1.2 IT2 Goog Az
ben_Beng 86.482.786.1 86.685.686.583.678.283.6 83.9 83.8 83.5
guj_Gujr 90.266.689.9 90.490.1 90.586.435.886.6 86.886.486.5
hin_Deva 81.5 77.0 81.381.5 81.781.3 84.276.883.884.1 84.0 84.0
mal_Mlym 87.980.588.2 88.1 87.6 88.583.973.084.0 84.684.3 84.5
mar_Deva 77.669.577.1 77.877.2 77.783.772.983.9 84.284.083.9
tam_T aml 89.057.188.7 89.488.8 89.382.455.782.7 82.882.582.5
tel_T elu 86.3- 86.1 86.986.3 86.983.1- 83.2 83.783.483.4
Table33: COMETscoresofallthesystemsontheWAT2021( Nakazawaetal. ,2021a). EvaluationsetintheEn-Indic
andIndic-Endirection. Thebestperformingsystemisbolded,whileunderlinedresultsindicatesignificantperformance
differencewhereIT2outperformsthesystem.
En-Indic Indic-En
Language IT1 M100 N1.2 IT2 Goog Az IT1 M100 N1.2 IT2 Goog Az
ben_Beng 88.284.487.587.9 86.6 87.6 86.982.6 87.0 87.1 86.786.5
guj_Gujr 92.2 70.5 92.0 92.1 91.5 92.690.435.090.6 90.890.390.5
hin_Deva 86.482.386.1 86.1 86.1 86.2 90.5 86.290.3 90.790.5 90.7
kan_Knda 90.260.789.990.1 89.3 90.288.234.788.5 88.788.187.1
mal_Mlym 90.9 82.8 91.1 90.9 90.3 91.588.774.688.7 89.388.589.0
mar_Deva 81.272.980.7 80.9 80.1 80.7 87.8 77.387.8 88.187.587.6
ory_Orya 88.041.6 88.183.5 83.0 87.7 88.0 39.8 88.3 88.4 87.488.0
pan_Guru 89.3 70.3 89.0 88.9 88.9 89.690.069.790.0 90.289.789.7
tam_T aml 92.153.691.791.9 91.3 91.887.153.587.1 87.486.486.6
tel_T elu 86.6- 86.3 86.4 85.8 86.3 88.3 - 87.9 88.787.988.1
58PublishedinTransactionsonMachineLearningResearch(12/2023)
Table34: BLEUscoresofallthesystemsontheWAT-2020( Nakazawaetal. ,2020). EvaluationsetintheEn-Indicand
Indic-En direction. The best performing system is bolded, while underlined results indicate significant performance
differencewhereIT2outperformsthesystem. AvgmeanstheaveragescoreofallthelanguagesthatsystemXsupports.
∆represents the difference between the average scores of IT2 and the average scores of system X for the subset of
languagesthatbothXandIT2support. Apositivevaluefor ∆indicatesIT2isbetterthanXandvice-versa.
En-Indic Indic-En
Language IT1 M100 N1.2 IT2 Goog Az IT1 M100 N1.2 IT2 Goog Az
ben_Beng 126.19.7 9.8 8.5 9.7 19.912.718.119.5 17.5 18.1
guj_Gujr 15.5 2.4 14 14.2 13.5 18.6 24.1 0.323 24.222.1 24.3
hin_Deva 20.112.318 18 19.5 17.9 23.615.722.2 24.2 24.3 24.6
mal_Mlym 7.32.95.16.9 6.5 6.7 20.49 18.8 20.518.5 20.7
mar_Deva 13.26.411.5 11.7 11.4 11.6 20.411.219.3 20.619.219.5
tam_T aml 6.20.75.45.9 5.5 6.1 18.2 216.817.9 16 17.1
tel_T elu 8 - 7.4 7.5 7 8.418.5- 17.5 18.817.418.5
Avg. 11.8 5.1 10.2 10.6 10.3 11.3 20.7 8.5 19.4 20.8 19.3 20.4
Delta -1.2 6 0.4 - 0.3 -0.7 0.1 12.7 1.4 - 1.5 0.4
Table35: BLEUscoresofallthesystemsontheWAT-2021( Nakazawaetal. ,2021a). EvaluationsetintheEn-Indic
andIndic-Endirection. Thebestperformingsystemisbolded,whileunderlinedresultsindicatesignificantperformance
differencewhereIT2outperformsthesystem. AvgmeanstheaveragescoreofallthelanguagesthatsystemXsupports.
∆represents the difference between the average scores of IT2 and the average scores of system X for the subset of
languagesthatbothXandIT2support. Apositivevaluefor ∆indicatesIT2isbetterthanXandvice-versa.
En-Indic Indic-En
Language IT1 M100 N1.2 IT2 Goog Az IT1 M100 N1.2 IT2 Goog Az
ben_Beng 15.87.412.112.6 9.5 12.1 29.515.425.9 25.7 22 22.5
guj_Gujr 25.8 3.5 23.8 23.9 20.4 32.7 40.20.137.238.8 34.7 36.9
hin_Deva 38.827.336.737.6 37.2 36.4 43.928.839.741.6 40.6 43.1
kan_Knda 19.21.116.6 16.7 14.9 18.3 36.5033.8 36.331.329.5
mal_Mlym 15.13.99.213.7 12.4 9.5 34.69.731.233.6 29.2 32.4
mar_Deva 20.38.617.518.1 16.9 17.3 33.514.430.232.2 28 29.7
ory_Orya 19.10.117.9 13.6 10.6 15.1 34.40.231.632.7 27.7 30.6
pan_Guru 33.9 6.7 3031.1 29.7 37.7 43.26.239.341.5 37.6 38.9
tam_T aml 13.60.811.412.3 11.1 12.6 33.11.829.131.1 25.6 27
tel_T elu 14.5- 12.9 12 10.2 9.6 36.1- 31.6 34.4 29 31.1
Avg. 21.6 6.6 18.8 19.2 17.3 20.1 36.5 8.5 33 34.8 30.6 32.2
∆ -2.4 13.4 0.4 - 1.9 -0.9 -1.7 26.3 1.8 - 4.2 2.6
Table36: chrF++scoresofallthesystemsontheWMT( Bojaretal. ,2014;Barraultetal. ,2019;2020)sharedtasksand
UFAL (Ramasamy et al. ,2012) in the En-Indic and Indic-En direction. The best performing system is bolded, while
underlinedresultsindicatesignificantperformancedifferencewhereIT2outperformsthesystem.
En-Indic Indic-En
Benchmark Language IT1 M100 N1.2 IT2 Goog Az IT1 M100 N1.2 IT2 Goog Az
UFAL tam_T aml 45.5 15.4 44.9 43.9 43.9 45.753.3 25.3 52.453.2 51.2 53.8
WMT14 hin_Deva 50.545.950.752.1 52.751.956.653.660.462.1 62.760.4
WMT19 guj_Gujr 48.820.755.456.3 56.8 62.250.57.956.457 58.4 58.3
WMT20 tam_T aml 45.714.447.5 49.248.2 49.245.817.548.151.3 53.552.3
59PublishedinTransactionsonMachineLearningResearch(12/2023)
Table37: COMETscoresofallthesystemsontheWMT( Bojaretal. ,2014;Barraultetal. ,2019;2020)sharedtasks
and UFAL ( Ramasamy et al. ,2012) in the En-Indic and Indic-En direction. The best performing system is bolded,
whileunderlinedresultsindicatesignificantperformancedifferencewhereIT2outperformsthesystem.
En-Indic Indic-En
Benchmark Language IT1 M100 N1.2 IT2 Goog Az IT1 M100 N1.2 IT2 Goog Az
UFAL tam_T aml 85.8 54.3 86.3 85.8 85.4 86.882.055.982.7 83.082.582.7
WMT14 hin_Deva 81.277.581.3 81.7 81.6 81.7 84.180.286.2 86.886.485.4
WMT19 guj_Gujr 86.461.086.787.8 87.3 88.382.630.684.9 85.9 85.7 85.1
WMT20 tam_T aml 87.753.487.988.4 87.8 89.181.246.083.1 84.4 84.0 83.5
Table38: BLEUscoresofallthesystemsontheWMT( Bojaretal. ,2014;Barraultetal. ,2019;2020)sharedtasksand
UFAL (Ramasamy et al. ,2012) in the En-Indic and Indic-En direction. The best performing system is bolded, while
underlinedresultsindicatesignificantperformancedifferencewhereIT2outperformsthesystem.
En-Indic Indic-En
Benchmark Language IT1 M100 N1.2 IT2 Goog Az IT1 M100 N1.2 IT2 Goog Az
UFAL tam_T aml 10.90.9 10.68.9 9.6 10.8 30.24.328.5 28.8 25.7 28.3
WMT14 hin_Deva 25.621.025.8 27.8 28.1 27.029.726.535.1 37.5 37.2 34.1
WMT19 guj_Gujr 19.54.226.0 26.6 27.9 33.825.10.531.1 31.6 33.2 33.2
WMT20 tam_T aml 10.30.710.9 12.6 12.0 12.1 18.51.720.623.2 25.522.4
Table 39: COMET scores of all the systems on the IN22-Gen Evaluation set in the En-Indic and Indic-En direction.
Thebestperformingsystemisbolded,whileunderlinedresultsindicatesignificantperformancedifferencewhereIT2
outperformsthesystem.
En-Indic Indic-En
Language IT1 M100 N1.2 N54 IT2 Goog Az IT1 M100 N1.2 N54 IT2 Goog Az
asm_Beng 81.1- 83.4 83.2 84.784.083.584.1- 87.3 88.387.5 87.7 85.7
ben_Beng 85.480.685.685.7 86.885.286.286.883.887.8 88.688.1 88.787.5
guj_Gujr 87.562.087.687.6 88.687.788.088.034.789.3 89.9 89.7 89.5 88.1
hin_Deva 79.575.279.680.0 80.579.279.387.884.788.4 89.1 89.2 88.787.9
kan_Knda 84.052.784.584.9 85.783.685.386.534.187.9 88.587.9 88.0 86.8
mal_Mlym 86.173.786.487.1 87.786.787.386.577.287.688.5 88.987.986.9
mar_Deva 73.465.073.774.7 76.173.775.385.777.287.1 87.987.5 87.686.3
npi_Deva - 54.2 80.378.6 82.780.781.6- 69.5 89.6 90.4 89.8 90.688.9
ory_Orya 82.2 39.1 82.9 82.8 79.5 77.4 83.687.438.288.5 89.489.0 88.4 86.7
pan_Guru 82.560.882.6 82.8 83.0 82.8 82.884.667.586.2 87.0 87.0 86.384.5
tam_T aml 87.145.087.387.588.2 87.5 88.584.956.486.2 87.0 87.0 87.2 86.0
tel_T elu 85.1- 85.9 86.2 87.186.0 86.986.4- 87.8 88.6 88.7 88.6 87.1
urd_Arab - 73.8 84.284.685.3 85.0 86.5- 79.0 88.289.0 89.288.987.9
60PublishedinTransactionsonMachineLearningResearch(12/2023)
Table 40: COMET scores of all the systems on the IN22-Conv Evaluation set in the En-Indic and Indic-En direction.
Thebestperformingsystemisbolded,whileunderlinedresultsindicatesignificantperformancedifferencewhereIT2
outperformsthesystem.
En-Indic Indic-En
Language IT1 M100 N1.2 N54 IT2 Goog Az IT1 M100 N1.2 N54 IT2 Goog Az
asm_Beng 83.2- 85.7 85.6 86.584.785.884.7- 87.9 88.189.3 90.488.7
ben_Beng 89.585.789.489.7 90.188.389.888.384.689.0 89.5 89.7 89.9 89.7
guj_Gujr 91.470.290.791.2 92.191.691.590.138.391.3 91.7 91.7 91.9 91.1
hin_Deva 85.081.383.983.3 85.2 85.1 84.789.985.590.5 90.8 90.8 90.8 90.5
kan_Knda 84.258.983.784.7 85.184.3 84.981.636.781.782.0 84.083.283.4
mal_Mlym 89.482.489.7 90.2 90.1 89.4 90.087.279.487.9 88.3 88.5 88.8 88.5
mar_Deva 80.272.581.0 82.1 81.9 80.981.387.877.388.889.1 89.4 90.089.3
npi_Deva - 57.0 84.683.5 86.885.185.0- 57.6 91.090.991.4 92.291.4
ory_Orya 86.2 46.4 87.1 87.3 82.9 82.3 86.888.941.690.3 90.690.4 89.5 89.3
pan_Guru 88.267.488.388.8 88.8 89.188.688.672.789.6 90.1 90.2 90.2 89.2
tam_T aml 87.667.285.984.5 88.087.6 88.383.663.984.8 85.585.0 85.684.9
tel_T elu 88.1- 84.2 83.0 89.689.0 89.685.8- 87.3 88.0 87.7 88.587.8
urd_Arab - 79.6 85.985.188.7 89.489.0- 80.9 90.090.390.9 91.290.8
Table41: COMETscoresofallthesystemsontheFLORES-200( Costa-jussàetal. ,2022)EvaluationsetintheEn-Indic
andIndic-Endirection. Thebestperformingsystemisbolded,whileunderlinedresultsindicatesignificantperformance
differencewhereIT2outperformsthesystem.
En-Indic Indic-En
Language IT1 M100 N1.2 N54 IT2 Goog Az IT1 M100 N1.2 N54 IT2 Goog Az
asm_Beng 79.4- 82.2 81.5 83.782.682.681.0- 85.4 86.685.8 86.483.5
ben_Beng 86.182.686.387.1 87.586.687.287.385.888.789.3 89.1 89.688.3
guj_Gujr 87.962.087.687.7 89.188.788.988.336.190.290.8 90.7 91.189.4
hin_Deva 80.677.681.181.2 81.5 81.3 80.988.487.689.8 90.3 90.3 90.4 89.5
kan_Knda 85.552.786.386.4 87.486.5 87.485.834.488.0 88.688.5 88.787.2
mal_Mlym 87.177.487.788.3 89.589.0 89.387.283.189.089.5 89.6 89.988.4
mar_Deva 73.767.774.775.6 76.475.9 76.286.481.488.489.0 88.8 89.387.9
npi_Deva - 51.2 80.074.9 84.583.583.0- 72.8 90.791.1 91.2 91.589.8
ory_Orya 83.6 38.8 84.4 84.2 79.9 80.9 84.886.838.589.1 89.9 89.8 89.6 87.9
pan_Guru 83.861.284.1 83.6 84.4 84.5 84.6 87.676.289.3 89.989.7 89.988.2
tam_T aml 88.044.689.188.6 89.989.5 89.985.164.387.4 88.087.7 88.286.2
tel_T elu 85.9- 86.5 86.8 87.887.5 87.886.6- 88.6 89.489.3 89.588.0
urd_Arab - 73.4 82.2 81.8 82.6 83.0 83.6- 79.0 87.5 88.387.7 88.486.4
61PublishedinTransactionsonMachineLearningResearch(12/2023)
Table 42: BLEU scores of all the systems on the IN22-Gen Evaluation set in the En-Indic and Indic-En direction.
Thebestperformingsystemisbolded,whileunderlinedresultsindicatesignificantperformancedifferencewhereIT2
outperforms the system. Avgmeans the average score of all the languages that system X supports. ∆represents the
differencebetweentheaveragescoresofIT2andtheaveragescoresofsystemXforthesubsetoflanguagesthatbothX
andIT2support. Apositivevaluefor ∆indicatesIT2isbetterthanXandvice-versa. †indicatescompletelyoff-target
translations.
En-Indic Indic-En
Language IT1 M100 N1.2 N54 IT2 Goog Az IT1 M100 N1.2 N54 IT2 Goog Az
asm_Beng 9.9- 13.9 15.419.4 16.9 16.232.5- 40.4 44.643.1 42 35.4
ben_Beng 18.111.316.618.320.8 18.3 1833.426.336.139.3 39 39.834.9
brx_Deva - - - - 16.9- - - - - - 40.2- -
doi_Deva - - - - 33.522.2-- - - - 53.545.1-
gom_Deva - - - - 18.811.611.5- - - - 35.33325.8
guj_Gujr 17.93.918.720.3 25.723.321.236.30.440.2 43.4 43.7 4337.1
hin_Deva 28.322.127.628.9 33.530.229.236.127.137.441 42.539.836.1
kan_Knda 13.4113.414.9 1814.215.234.80.139 42.740.8 41 35
kas_Arab - - 9.9 10.5 14.4- - - - 31.5 35 38.3- -
mai_Deva - - 15.5 15.1 19.39.314.5- - 37.9 41.840.8 39.8 36.2
mal_Mlym 13.94.411.913.1 16.413.713.631.417.534.838.6 41.437.933.3
mar_Deva 13.9714.515.6 21.716.217.533.52037.2 40.8 40.2 40.6 35.1
mni_Mtei - - - - 17.510.8-- - - - 35.127.5-
npi_Deva - 2.6 14.414.816.8 13.8 14.4- 12.8 42.246 45.1 46.839.9
ory_Orya 10.20.112.211.8 14.510.7 14.136.7040.7 44.743.8 40.4 34.7
pan_Guru 23.57.223.925.3 25.5 29.925.233.510.437.440.6 41.439.634.7
san_Deva - - 3.7 4.3 11.15.5-- - 24.2 27.2 29.828.6-
sat_Olck - - 0.0†3.8 5.5- - - - 12.3 18.7 21.8- -
snd_Deva - - - - 14- - - - - - 35- -
tam_T aml 11.91.412.613 14.4 14 14.5 28.94.932.535 35.934.929.4
tel_T elu 15.5- 15.1 17.1 19.417.717.733.5- 37.6 41.5 42.341.335.7
urd_Arab - 23.1 4243.849.7 44.1 51.4- 26.5 46.550.5 53.750.946.3
Avg. 16 7.6 15.6 16.8 20.3 17.9 19.6 33.7 13.3 35.8 39.5 40.1 39.6 35.3
∆ 7.3 15.8 4.8 1.7 - 4.4 2.7 8.6 29.2 4.4 0.9 - 2.3 6.6
62PublishedinTransactionsonMachineLearningResearch(12/2023)
Table 43: BLEU scores of all the systems on the IN22-Conv Evaluation set in the En-Indic and Indic-En direction.
Thebestperformingsystemisbolded,whileunderlinedresultsindicatesignificantperformancedifferencewhereIT2
outperforms the system. Avgmeans the average score of all the languages that system X supports. ∆represents the
differencebetweentheaveragescoresofIT2andtheaveragescoresofsystemXforthesubsetoflanguagesthatbothX
andIT2support. Apositivevaluefor ∆indicatesIT2isbetterthanXandvice-versa. †indicatescompletelyoff-target
translations.
En-Indic Indic-En
Language IT1 M100 N1.2 N54 IT2 Goog Az IT1 M100 N1.2 N54 IT2 Goog Az
asm_Beng 11.6- 16.7 17.8 19.717.6 19.531.3- 38.6 40.4 43.8 44.6 41.7
ben_Beng 20.11319.320.7 21.3 21.5 20.332.925.833.33536.4 37.636
brx_Deva - - - - 15.4- - - - - - 35.5- -
doi_Deva - - - - 32.417.6-- - - - 45.642.6-
gom_Deva - - - - 14.21210.4- - - - 29.9 29.5 23.7
guj_Gujr 23.2422.824.1 27.2 26.7 25.734.70.339.739.9 41.1 41 39.1
hin_Deva 28.422.327.128.4 30.1 30.7 2835.528.337.338.4 39.338.337.7
kan_Knda 6.10.55.8 6.5 6.7 6.3 6.221.10.222.522.9 24.9 24.4 23.6
kas_Arab - - 4.5 4.6 11.3- - - - 23.3 24.1 31.8- -
mai_Deva - - 15.4 15.7 18.910.611.9- - 32.6 33.835.3 36.632.1
mal_Mlym 11.13.98.37.6 11.3 11.1 10.827.616.22829.5 31.6 31.1 30.8
mar_Deva 15.58.916.9 18.6 19.4 17.717.632.218.934.135.736.7 37.735.9
mni_Mtei - - - - 14.26.9-- - - - 31.925.7-
npi_Deva - 1.6 15.716.4 21.216.415.8- 3.5 38.939.6 42.4 43.1 40.8
ory_Orya 11.30.3 13.8 13.8 12.3 10.3 14.133.60.2 38.4 38.9 38.8 37.435.3
pan_Guru 32732.133.835.7 41.333.236.87.339.641.1 4339.540.7
san_Deva - - 2.8 4.7 6.35.2-- - 17.8 17.2 26.1 26.7 -
sat_Olck - - 0.0†3 6.6- - - - 113 17.8 23.1- -
snd_Deva - - - - 7.4- - - - - - 27.5- -
tam_T aml 7.71.57.17.47.6 8 8.4 20.8423 24.122.7 23.3 22.8
tel_T elu 12- 9.8 10.5 14.113.4 13.826.3- 29.5 31.6 31 31.5 31.1
urd_Arab - 19.4 35.635.3 43.742.240.1- 26.5 40.341.7 45.9 45.6 44.9
Avg. 16.3 7.5 14.9 15.8 18 17.5 18.4 30.3 11.9 31.1 32.5 34.7 35.3 34.4
∆ 4.5 14 3.5 2.8 - 2.7 1.8 6 24.7 3.8 3.4 - 0.9 1.8
63PublishedinTransactionsonMachineLearningResearch(12/2023)
Table44: BLEUscoresofallthesystemsontheFLORES-200( Costa-jussàetal. ,2022)devtestsetintheEn-Indicand
Indic-En direction. The best performing system is bolded, while underlined results indicate significant performance
differencewhereIT2outperformsthesystem. AvgmeanstheaveragescoreofallthelanguagesthatsystemXsupports.
∆represents the difference between the average scores of IT2 and the average scores of system X for the subset of
languagesthatbothXandIT2support. Apositivevaluefor ∆indicatesIT2isbetterthanXandvice-versa. †indicates
completelyoff-targettranslations.
En-Indic Indic-En
Language IT1 M100 N1.2 N54 IT2 Goog Az IT1 M100 N1.2 N54 IT2 Goog Az
asm_Beng 7.6- 11.4 11.7 1412.2 13.823.4- 31.3 33.932.5 32.9 27.1
ben_Beng 19.715.320.222.1 24.7 24.3 23.431.829.436.638.7 38.6 39.735.3
guj_Gujr 22.14.823.925.2 27.827.126.634.11.242.544.645.3 46.238.6
hin_Deva 34.530.934.336.7 38.6 39 38.4 37.535.642.144.4 46.1 46.4 43.1
kan_Knda 18.31.620.622.1 24.1 24.6 24.2 28.70.734.836.9 37.8 38.4 32.5
kas_Arab - - 10 10.5 11.9- - - - 33.7 36.7 36.1 - -
kas_Deva - - 1.9 2 2.2 - - - - 23.9 2725.1 - -
mai_Deva - - 16.5 18.219 11.8 20.8- - 44.1 46.7 48.246.641.8
mal_Mlym 15.97.914.118.3 22 22.4 22 31.425.337.639.1 41 41 35.8
mar_Deva 15.810.116.217.919.9 20.718.33124.637.140.341.1 42.137.3
mni_Beng - - 7.7 10.48.6 - - - - 27 27.5 28.5- -
npi_Deva - 1.7 18.718.525.5 23.9 20.9- 14 42.344.5 46.3 46.5 39.8
ory_Orya 13.60.317.1 16.9 17.3 24.418.629.80.538.241.6 42.441.635.1
pan_Guru 26.78.627.127.729.6 31.130.135.815.242.244.8 44.9 45.838.2
san_Deva - - 2.2 2.3 3.2 3.4 -- - 23.3 26.1 26.6 25-
sat_Olck - - 0.1†4.94.1 - - - - 14.5 21.716.7 - -
snd_Arab - 10.8 25.3 26.4 20.2 27.3 27.7 - 2.7 42 4543.6 45.536.3
tam_T aml 15.60.918.619.8 22.621.121.328.48.334.436.8 37.8 37.7 31.1
tel_T elu 21.3- 23.1 25.3 27.8 27.2 25.333.4- 40.9 43.6 44.7 45.1 39.6
urd_Arab - 16.9 25.827.2 29.128.228.2- 22.2 36.8 39.638.1 4034.4
Avg. 19.2 9.2 16.7 18.2 19.6 23.0 24 31.4 15 35.3 38 38.1 41.3 36.4
∆ 5.2 15.9 2.9 1.4 - -0.2 0.1 9.7 26.9 2.8 0.1 - -0.3 5.5
64PublishedinTransactionsonMachineLearningResearch(12/2023)
C Human Evaluation
Automated evaluation metrics provide a convenient and quick way to evaluate MT systems. However, as reported
by previous works ( Kocmi et al. ,2021;Moghe et al. ,2022), the degree of correlation between automatic evaluation
metrics and human ratings is not particularly strong. To obtain a more comprehensive understanding of the model’s
performance,itisimperativetoconducthumanevaluations( Kocmietal. ,2021).
We conduct a small-scale human evaluation exercise to verify if the quality of our model outputs correlates with the
improvements observed using automatic metrics. This exercise focused on the En-Indic direction and included 50
exampleseachfromtheWikipediaandWebsourcessubsettoyieldatotalof100sentencepairsfromIN22-Gen. We
seektostudyhumanevaluationofsentencesofdiverselengths(referFigure 10)anduniformlysamplesentencesfrom
eachbucket. OurhumanevaluatorsbelongtothesamepooloftranslatorswhocreatedtheIN22benchmark. Theyare
fluentspeakersofEnglishandtherespectivenativelanguageunderstudy. Basedontheavailabilityofannotators,we
conduct human evaluation studies for the following languages: Assamese, Bengali, Bodo, Dogri, Konkani, Gujarati,
Hindi, Kannada, Malayalam, Marathi, Nepali, Punjabi, Santali, Tamil, Telugu and Urdu. We compare IndicTrans2
model outputs along with those of NLLB ( Costa-jussà et al. ,2022), Google Translate, and Azure Translate. The
annotatorswerenotspecificallyawareofwhichoutputwasgeneratedbywhichsystem.
We use the XSTS methodology proposed by Licht et al. (2022) and adopted by Costa-jussà et al. (2022) for com-
paring different multilingual machine translation (MT) systems. XSTS relies on human raters to assess translations
without using reference translations, focusing more on adequacy (meaning preservation) than fluency. This approach
is particularly suitable for low-resource languages with relatively lower translation quality. XSTS also exhibits better
inter-annotatoragreementthanDirectAssessment( Grahametal. ,2013)asdemonstratedbypriorresearch Lichtetal.
(2022).
Briefinstructionsforhumanannotationsareprovidedbelow. Raterschoosescoresbetween1to5. Wereferthereaders
toFigure1in Lichtetal. (2022)forthedetaileddefinitionofthescores.
•Score of 1 indicates the sentences are unrelated to each other or maybe in similar topics but differ in more
thanhalfoftheircoreconcepts.
•Score of 2 indicates that the sentences are about similar topics but some key details about the main subject,
verb,orobjectareeitherdifferentorabsent.
•Scoreof3indicatesthatthesentencesareequivalenttoeachotherbutwithunimportantdifferences.
•Scoreof4indicatesthatthesentencesareparaphrasesofeachotherbuthaveminordifferencesinemphasis,
formality,idioms,etc.
•Scoreof5indicatesthesentencesmeanthesamewithnodifferenceinemphasis,formality,idioms,etc.
It is known that there is some variance in human evaluators, with some being overly critical while others being ex-
cessively generous when assessing MT outputs. Recent studies by Licht et al. (2022) andCosta-jussà et al. (2022)
emphasizetheimportanceofhavingacalibrationsettoensurethatXSTSscoresarecomparableacrosslanguages. To
addressthisconcern,ourevaluationmethodologyemploysasampleofthecalibrationset,comprisingpairsofEnglish
sentences released by NLLB Team ( Costa-jussà et al. ,2022). From each of the 5 scoring classes described in Licht
et al.(2022), we uniformly sample 10 sentences, forming a calibration set with 50 sentence pairs. The task frame-
workemployedforthispurposecloselyalignswiththeapproachsuggestedin Costa-jussàetal. (2022). Toaccountfor
extremecalibrationshifts,weusethe moderated calibration adjustmentasproposedin Costa-jussàetal. (2022).
Overall results. Our findings indicate that IndicTrans2 outperforms Google and NLLB 54B significantly, and per-
forms comparably with Azure. Statistical significance is computed using ANOVA with posthoc Tukey HSD test
(p≤0.05)following similar human evaluation in data-to-text generation ( Puduppully & Lapata ,2021;Puduppully
etal.,2022). However,itshouldbeacknowledgedthatthesamplesizeofsentencesusedforhumanevaluationislim-
ited,andtherefore,theseresultsmustbeinterpretedwithcaution. Futureworkshouldexpandthehumanevaluationto
coverall22IndiclanguagesandalsoincludeIN22-Convsettogainmorefine-grainedinsights.
65PublishedinTransactionsonMachineLearningResearch(12/2023)
1 2 3 4 5
Median score0102030405060FrequencyAssamese
1 2 3 4 5
Median score0102030405060FrequencyBodo
1 2 3 4 5
Median score0102030405060FrequencyDogri
1 2 3 4 5
Median score0102030405060FrequencyKonkani
1 2 3 4 5
Median score0102030405060FrequencyNepali
1 2 3 4 5
Median score0102030405060FrequencyPunjabi
1 2 3 4 5
Median score0102030405060FrequencySanskrit
1 2 3 4 5
Median score0102030405060FrequencyUrdu
(a)XSTSScoresforlow,mediumresourcelanguages
1 2 3 4 5
Median score0102030405060FrequencyBengali
1 2 3 4 5
Median score0102030405060FrequencyGujarati
1 2 3 4 5
Median score0102030405060FrequencyHindi
1 2 3 4 5
Median score0102030405060FrequencyKannada
1 2 3 4 5
Median score0102030405060FrequencyMalayalam
1 2 3 4 5
Median score0102030405060FrequencyMarathi
1 2 3 4 5
Median score0102030405060FrequencyTamil
1 2 3 4 5
Median score0102030405060FrequencyTelugu
(b)XSTSScoresforhighresourcelanguages
Figure9: DistributionofXSTSscoresforlow,mediumandhighresourcelanguagesinIN22
High vs. Low Resource Languages. Figure9in Appendix Cdepicts the trends in the distribution of ratings for a
selectedsetoflanguages,withlowandmediumresourcelanguagesintheupperhalf, andhighresourcelanguagesin
the lower half. IndicTrans2 outperforms other models significantly in low-resource languages like Konkani, Sanskrit,
andNepali. MostlanguagessupportedbyIndicTrans2achieveclosetoa4XSTSrating. High-resourcelanguages,such
asHindi,Bengali,andTelugu,showaright-skeweddistributionwithmanysentencepairsreceivinghigherratings. On
theotherhand,medium-performancelanguageslikeBodoexhibitamoresymmetricaldistributionaroundtherating.
66PublishedinTransactionsonMachineLearningResearch(12/2023)
Table 45: Post calibration results for human evaluation for En-XX language pairs using XSTS methodology. We
comparebetweenfourmodeloutputs: Azure,Google,NLLB(N54B)( Costa-jussàetal. ,2022),andIndicTrans2(IT2).
– indicates languages not supported by a model. The†after a value indicates statistically significant difference from
IndicTrans2usingANOVAwithpost-hocTukeyHSDtest( p≤0.05).∆representsthedifferenceofpre-calibrationand
post-calibration XSTS score for IndicTrans2, with a positive value indicating improvement in scores post-calibration
and vice-versa. Overall, IndicTrans2 is the top-ranked system comparable with Azure, and significantly better than
GoogleandNLLB.
language Azure Google N54B IT2 ∆
asm_Beng 3.44 3.63 3.833.62 -0.61
ben_Beng 3.94†3.92†4.05 4.18-0.18
brx_Deva – –– 3.750.04
doi_Deva – 4.13 – 4.320.08
gom_Deva – 3.84†– 4.45-0.09
guj_Gujr 4.50 4.26†4.28†4.530.12
hin_Deva 4.27†4.23†4.40 4.560.05
kan_Knda 4.12 3.86 4.01 4.180.08
mal_Mlym 3.96 3.73†3.84 4.060.23
mar_Deva 4.27 3.89†4.12 4.41-0.11
npi_Deva 3.89†3.87†3.81†4.410.13
pan_Guru 4.09 3.94 4.10 4.25-0.45
san_Deva – 2.87†2.83†3.68-0.37
tam_T aml 4.00 3.79 3.79 3.90 0.40
tel_T elu 4.293.94†4.24 4.290.03
urd_Arab 4.06 3.68†3.76†4.250.24
Average 4.07 3.84†3.93†4.18-0.02
Calibration. Thecolumn ∆inTable45indicatestherevisioninscorespost-calibrationforIndicTrans2,withaposi-
tivevalueindicatingimprovementinscoresandvice-versa. Wepresenttheresultscomparingpreandpost-calibration
proceduresforallthemodelsinTable 46inAppendix C.Weseethatscoresoflanguagesgetadjusted. Assameseand
Bengaliaretworelatedlanguageswrittenusingthesamescriptandsharingsubstantialvocabulary. Atthesametime,
Bengali is high-resource in comparison to Assamese; Bengali belongs to class 5 whereas Assamese belongs to class
2 in terms of the language resourcefulness classification ( Joshi et al. ,2020). From Table 46, we see that the scores
for Assamese and Bengali are comparable pre-calibration; however, after calibration, the scores for Assamese drop
comparedtothatofBengali. Amonglanguagesforwhichthescoreschangebymorethan0.2points,PunjabiandSan-
skritscoresdroppostcalibrationwhereasMalayalam,Tamil,andUrduscoresimprove. Thesefindingsunderscorethe
significanceofcalibrationinensuringthereliabilityandcomparabilityofXSTSscoresacrossdifferentlanguagesand
models. Overall, wesee an averagechangeof 0.23 inXSTSscores forIndicTrans2. Importantly, the relativeranking
ofthemachinetranslationmodelsbasedonXSTSscoresremainsunchanged,withIndicTrans2outperformingNLLB,
andGoogle,andcomparablewithAzure.
Correlation with Automatic Metrics. ThecorrelationbetweenXSTSscoresandautomaticmetricsisanimportant
aspect of evaluating machine translation performance. Our analysis reveals that XSTS scores for IndicTrans2 exhibit
moderate correlation with two widely used automatic metrics, namely BLEU, and chrF++. Specifically, we observe
Spearman rank correlations of 0.49 and 0.12, respectively, with BLEU and chrF++ across all languages, but the cor-
relations increase to 0.67 and 0.25, respectively, when Urdu is excluded from the analysis. This observation can be
partly attributed to the influence of Urdu tokenization, which had a greater impact on the BLEU and chrF++ scores
whencomparedtootherlanguages. ThiscanbeduetothehigherfertilityofUrduwhenusingtheUrduHacktokenizer,
which led to inflated scores for both metrics. As a result, the correlation was reduced between these metrics and the
actual quality of translations, deviating from the trend observed in other languages. In contrast, we find no correla-
tionbetweenXSTSscoresandtheCOMETmetric,whichisdesignedtoassessthefluencyandadequacyofmachine
translations. Additionally,weobservenocorrelationbetweenBLEU/chrF++andCOMETscores,indicatingthatthese
67PublishedinTransactionsonMachineLearningResearch(12/2023)
Table46: ComparisonofXSTSscorebeforeandafterapplyingcalibration
Pre-Calibration Post-Calibration
language Azure Google N54B IT2 Azure Google N54B IT2
asm_Beng 4.05 4.24 4.444.23 3.44 3.63 3.833.62
ben_Beng 4.13 4.11 4.24 4.363.94 3.92 4.05 4.18
brx_Deva --- 3.71--- 3.75
doi_Deva - 4.03 - 4.24- 4.13 - 4.32
gom_Deva - 3.93 - 4.54- 3.84 - 4.45
guj_Gujr 4.37 4.10 4.12 4.414.50 4.26 4.28 4.53
hin_Deva 4.20 4.15 4.34 4.514.27 4.23 4.40 4.56
kan_Knda 4.04 3.77 3.92 4.104.12 3.86 4.01 4.18
mal_Mlym 3.72 3.47 3.59 3.833.96 3.73 3.84 4.06
mar_Deva 4.38 4.00 4.23 4.524.27 3.89 4.12 4.41
npi_Deva 3.71 3.69 3.63 4.283.89 3.87 3.81 4.41
pan_Guru 4.54 4.39 4.54 4.704.09 3.94 4.10 4.25
san_Deva - 3.23 3.19 4.05- 2.87 2.83 3.68
tam_T aml 3.613.39 3.39 3.50 4.003.79 3.79 3.90
tel_T elu 4.263.90 4.21 4.26 4.29 3.94 4.24 4.29
urd_Arab 3.80 3.39 3.47 4.014.06 3.68 3.76 4.25
Average 4.07 3.85 3.95 4.204.07 3.84 3.93 4.18
metricscapturedifferentaspectsofmachinetranslationquality. Nonetheless,furtherinvestigationisnecessarytogain
adeeperunderstandingoftherelationshipbetweenmetrics.
68PublishedinTransactionsonMachineLearningResearch(12/2023)
D Distilled Models
Thissectionpresentsadetaileddescriptionofourstudentmodelarchitectureanddistillationtraininghyperparameters.
We share the weight of the decoder embedding and output projection to compress the student models as much as
possible. Thisalsoallowsustohaveequal-sizedstudentmodelsforbothdirections. Thisisparticularlyusefulforthe
En-Indicmodel, astheoutputprojectionisasignificantfractionofthemodelparameters( ≈60M).Tables 50and51
presentthecomparisonbetweenthestudentandteachermodelsonFLORES200andIN22-Convrespectively.
Table 47: Student architecture description. Specifically, our student models use the base18Larchitecture, following
Gummaetal. (2023).
Hyperaparameter Value
Modeldim 512
FFNdim 2048
EncoderLayers 18
DecoderLayers 18
Activation GELU(Hendrycks&Gimpel ,2016)
Pre-Normalization True(Xiongetal. ,2020)
EmbeddingLayerNorm True
Sharedecoderinputoutputembed True
Table48: Numberofparametersinteacheranddistilledstudentmodels.
#P arams Indic-En En-Indic
Teacher 1.02B 1.11B
Student 211.77M211.77M
Table49: HyperparametersetforKnowledgeDistillation. Therestoftheparametersnotmentionedinthetablearethe
sameastheonesusedfortrainingIT2(seeTable 10).
Hyperparameters Stage 1 Distillation Stage 2 fine-tuning
Learningrate 7e-4(en-xx),1e-3(xx-en)3e-5
Criterion KL-Divergence Cross-entropy
Labelsmoothing( Szegedyetal. ,2016)− 0.1
Effectivebatchsize 262K 8K
Checkpointmetric BLEU@beam= 5 BLEU@beam= 5
69PublishedinTransactionsonMachineLearningResearch(12/2023)
Table 50: chrF++ scores of Indic-En and En-Indic distilled models on FLORES-200. Distilled ( Dist) is the model
trainedwithWord-levelKD. ∆isthedifferencebetweenthedistilledModelfine-tunedonseeddata( Dist-Seed)&IT2.
Highervaluesof ∆arepreferable.
Indic-En En-Indic
language IT2 Dist Dist-Seed ∆IT2 Dist Dist-Seed ∆
asm_Beng 56.9 56.9 56.1 -0.8 43.3 42.7 43.0 -0.3
ben_Beng 62.4 61.4 61.4 -1.0 54.3 54.0 54.0 -0.3
guj_Gujr 67.0 65.5 65.6 -1.4 56.0 55.9 55.8 -0.2
hin_Deva 67.5 66.0 66.0 -1.5 59.6 59.3 59.3 -0.3
kan_Knda 61.5 60.0 60.2 -1.3 56.1 56.0 55.9 -0.2
kas_Arab 59.7 57.6 57.6 -2.1 39.7 40.0 40.1 0.4
kas_Deva 48.3 45.3 45.6 -2.7 19.2 19.3 19.8 0.6
mai_Deva 69.5 67.3 67.3 -2.2 50.5 50.8 51.0 0.5
mal_Mlym 64.3 62.7 62.8 -1.5 57.3 57.0 57.1 -0.2
mar_Deva 64.3 63.1 63.1 -1.2 51.3 51.3 51.1 -0.2
mni_Beng 52.9 51.0 50.9 -2.0 38.2 37.5 37.2 -1.0
npi_Deva 68.1 66.2 66.2 -1.9 57.2 57.1 57.2 0.0
ory_Orya 64.9 63.2 63.2 -1.7 49.2 48.6 48.7 -0.5
pan_Guru 66.4 64.8 65.0 -1.4 53.5 53.5 53.5 0.0
san_Deva 51.6 49.9 49.9 -1.7 31.6 31.5 31.3 -0.3
sat_Olck 39.3 40.5 40.9 1.6 28.4 28.2 28.6 0.2
snd_Arab 65.1 63.4 63.5 -1.6 44.9 45.1 45.0 0.1
tam_T aml 61.3 59.4 59.3 -2.0 57.2 57.0 57.0 -0.2
tel_T elu 66.1 64.9 64.8 -1.3 59.4 59.4 59.5 0.1
urd_Arab 62.0 60.6 60.7 -1.3 52.2 52.0 52.2 0.0
Average 61.0 59.4 59.5 -1.5 48.0 47.8 47.9 -0.1
70PublishedinTransactionsonMachineLearningResearch(12/2023)
Table51: chrF++scoresofIndic-EnandEn-IndicdistilledmodelsonIN22-Conv. Distilled( Dist)isthemodeltrained
withWord-levelKD. ∆isthedifferencebetweenthedistilledModelfine-tunedonseeddata( Dist-Seed)&IT2. Higher
valuesof ∆arepreferable.
Indic-En En-Indic
language IT2 Dist Dist-Seed ∆IT2 Dist Dist-Seed ∆
asm_Beng 62.9 62.3 63.0 0.1 46.8 46.1 46.6 -0.2
ben_Beng 58.4 58.3 58.7 0.3 49.7 49.6 49.8 0.1
brx_Deva 56.3 55.2 55.2 -1.1 45.3 45.4 45.4 0.1
doi_Deva 65.0 63.8 63.5 -1.5 53.9 52.3 53.0 -0.9
gom_Deva 51.7 50.8 50.8 -0.9 42.5 41.8 41.7 -0.8
guj_Gujr 62.0 61.5 62.0 0.0 53.1 53.1 53.1 0.0
hin_Deva 60.1 59.9 60.3 0.2 49.6 49.3 49.4 -0.2
kan_Knda 47.5 48.3 48.6 1.1 33.8 33.6 33.8 0.0
kas_Arab 52.6 50.2 50.5 -2.1 35.6 33.6 34.9 -0.7
mai_Deva 57.8 56.9 57.2 -0.6 44.3 43.8 44.3 0.0
mal_Mlym 54.3 53.8 53.9 -0.4 45.7 45.5 45.6 -0.1
mar_Deva 58.5 58.4 58.8 0.3 48.6 48.4 48.7 0.1
mni_Mtei 52.5 51.4 51.6 -0.9 40.2 39.5 40.0 -0.2
npi_Deva 63.0 63.1 63.3 0.3 51.5 51.1 51.3 -0.2
ory_Orya 60.3 60.3 60.7 0.4 40.2 39.8 40.0 -0.2
pan_Guru 62.7 61.7 62.1 -0.6 57.8 57.6 57.5 -0.3
san_Deva 48.3 46.9 46.9 -1.4 35.5 34.6 34.8 -0.7
sat_Olck 43.5 45.9 46.3 2.8 34.6 34.2 34.8 0.2
snd_Deva 49.6 50.1 50.5 0.9 30.3 30.1 30.0 -0.3
tam_T aml 45.8 45.7 45.7 -0.1 39.1 38.6 38.7 -0.4
tel_T elu 52.9 52.3 53.0 0.1 45.5 44.7 45.1 -0.4
urd_Arab 65.5 64.4 64.5 -1.0 61.6 61.5 61.4 -0.2
Average 56.0 55.5 55.8 -0.2 44.8 44.3 44.5 -0.3
71PublishedinTransactionsonMachineLearningResearch(12/2023)
E Additional details about IN22 Benchmark
This section provides a detailed overview of the source and domain diversity of the different subsets of the IN22
benchmark.
010203040IN22-Web
1 - 10 11 - 17 18 - 25 26 - 45 46 - 60 61 - 80
Culture
Economy Education
EntertainmentGeography
GovernmentsHealthIndustryLegal News
ReligionSportsTourism
Domain010203040IN22-Wiki
1 - 10 11 - 17 18 - 25 26 - 45 46 - 60 61 - 80Number of sentences
Figure 10: Domain vs. length distribution for the sentences from Web Sources (top) and Wikipedia (bottom) subsets
ofIN22
Table52: ComparisonofdiversityofdomainsinFLORES-200andIN22
FLORESdomain IN22domain
crime,disasters,politics news
entertainment entertainment
geography geography
health health
nature,science education
sports sports
travel tourism
- culture
politics government
- industry
- economy
- legal
- religion
72PublishedinTransactionsonMachineLearningResearch(12/2023)
Table53: StatisticsoftheConversationalSubsetofIN22
Statistic Value
Numberofuniqueconversations 44
Average turns per conversation ±std
dev34.2±4.9
Numberofuniquetopics 23
Randomlyselected5topics ‘Government schemes’, ‘Movies’, ‘Historical Architectures’,
‘GeographyofIndia’,‘LegalAﬀidavit/documents’
Numberofuniquedomains 16
Randomlyselected5domains ‘arts’,‘history’,‘schoollife’,‘healthcare’,‘legal’
Numberofuniqueprompts 44
Randomlyselected5prompts ‘Joint Aﬀidavit for Registration of Marriage’, ‘Diploma in web
designing’, ‘Qutub Minar- visiting time and student discounts’,
‘Howdoyoutakeouttimeforyourhobbies?’,‘SocialandEco-
nomicinequalities’
Numberofuniquescenarios 37
Randomlyselected5scenarios ‘Howtoapplyforaloan’,‘Askingforthedate/timingofthevot-
ing date’, ‘Housing/Colony’, ‘Learning Music’, ‘Challenges/Is-
suesinSportssector’
Avg number of speakers per conversa-
tion±stddev2.0±0.0
E.1 Source Selection
For the Wikipedia subset, we carefully chose English source sentences from various Wikipedia categories to ensure
broadcoverageacrossdifferentdomains. Initially,weselectedarticlepageswithinthosecategoriesandidentifyallthe
sentences as potential candidates. For each of these sentences, we construct a context window with a block size of 3,
which typically includes one sentence before and after the candidate sentence. To satisfy the length criteria, we filter
outsentencesthatarelessthan6ormorethan80words. TominimizeoverlapswiththeFLORES-200testset( Costa-
jussàetal. ,2022),wediscardthesentencesthatshare4-gramorhigheroverlapswithanysentenceintheFLORES-200
devanddevtestsets. Thecandidatesentencedomainsaremanuallyannotatedasdescribedabove. Followingthis,we
randomlyselectthefinalsetofsentencesbasedondomainandlengthconstraints. Thedetailedbucketsarepresented
inFigure10. Itisimportanttonotethatwedidnottranslateallthesentenceswithinthecontextblock,deviatingfrom
the approach followed in FLORES. This deviation was necessary to ensure the optimal length and domain diversity
constraintsweremet.
FortheWebSources,weidentifiedvariousGovt. ofIndiawebsitesanddigitallibrariesthatcouldbesourcesofmulti-
domain content with a focus on Indian topics. Many benchmarks like FLORES ( Costa-jussà et al. ,2022), NTREX
(Federmann et al. ,2022) do not have a fair representation of India-centric content, and we try to address this in the
creationofthissubset. WereliedonPDFformatdocumentstodiscoversentencesthatarehopefullynotpartofpublicly
available crawls like CommonCrawl ( Xue et al. ,2021;Conneau et al. ,2020) or IndicCorp ( Kakwani et al. ,2020;
Doddapaneni et al. ,2023). The selection of sentences for translation follows a similar procedure to the Wikipedia
subset. Figure 10providesthebucket-wiseanddomain-wisedistribution.
FortheConversationsubset,wefirstcreateEnglishconversationswithasetofpromptsandscenarios. Thepromptsare
predefinedtopicsorthemesthatareusedtoinitiateaconversation. Apromptcanbethoughtofasthestartingpointof
aconversation,whichsetsthetoneanddirectionfortheinteractionbetweenthetwospeakers. Forexample,aprompt
couldbe“Travelplansforthesummer”or“Discussinganewprojectatwork”. Thepromptisdesignedtoencouragethe
speakerstodiscussaparticulartopicortheme,anditservesasthefoundationfortheconversation. Ontheotherhand,
ascenarioisaspecificsituationorcontextinwhichtheconversationtakesplace. Itprovidesadditionalcontextforthe
speakersandhelpstoshapetheconversation. Forexample,ascenariocouldbe“PlanningafamilyvacationtoEurope”
or “Brainstorming ideas for a marketing campaign”. The scenario provides a specific context for the prompt, which
guidesthespeakersintheirconversation. Tocreateaconversation,twoannotatorsfromourannotatorteamplayedout
73PublishedinTransactionsonMachineLearningResearch(12/2023)
Table54: AnexamplefromtheConversationsubsetofIN22featuringaconversationbetweentwospeakers: akidand
his mother. The example belongs to the cultural domain, with festivities as a topic, the prompt of 14th April being a
holiday,andthescenariobeing‘Historicalimportance’. Notethatthespeakerinformationispartofmetadataandisnot
partofthetexttobetranslated. Eachturnintheconversationisadistinctinstancefromthebenchmark. Itispossible
toreconstructaconversationusingthemetadatareleasedalongwiththetranslations.
Speaker T urn
Speaker1 Mom,let’sgoforamovietomorrow.
Speaker1 Idon’thavetogotoschool.
Speaker1 Itisaholiday.
Speaker2 Oh,tomorrowisthe14thofAprilright?
Speaker2 Yourdadwillalsohavethedayofffrom
work.
Speaker2 Wecanmakeamovieplan!
Speaker1 That’sagoodnews!
Speaker1 Whyisitaholidaythough?
Speaker1 Are all schools, colleges and oﬀices
closedtomorrow?
Speaker2 ItisAmbedkarJayantitomorrow!
Speaker2 Thisdayiscelebratedannuallytomark
thebirthofDr. B.RAmbedkar.
Speaker2 Haveyouheardofhim?
Speaker1 I think I have seen him in my History
andCivicsbook.
Speaker1 IsherelatedtoourConstitution?
Speaker2 Absolutely! He is known as the father
oftheIndianConstitution.
Speaker2 Hewasacivilrightsactivistwhoplayed
amajorroleinformulatingtheConstitu-
tion.
Speaker2 He played a crucial part in shaping the
vibrant democratic structure that India
pridesitselfupon.
Speaker1 Iremembernow!
. . .
the two speaker roles. Once a conversation is ready, it is then translated into 22 Indic languages. During translation,
thetranslatorshavetheentireconversationcontextavailabletothem.
74PublishedinTransactionsonMachineLearningResearch(12/2023)
F T ranslation Workflow
F.1 T ranslation Stages
Source Sentence Selection Stage. The workflow begins with the selection of sentences to be translated based on
various criteria to be met like domain coverage, length distribution, licensing constraints, etc.This helps in ensuring
the right set of sentences as required for the project are shortlisted for translation. To ensure a broader vocabulary
coverage, the sentences are taken from multiple domains such as News, Tourism, Business, Entertainment, History,
Geography,Culture,Sports,andHealth.
Source V erification Stage. Once the candidate pool of source sentences is created, it is verified by annotators to
ensurethecorrectnessofthesourcesentencesandmetadata. Thisensuresthatthesentencesselectedarevalid,ofgood
quality, and translatable. Shoonya eﬀiciently supports a verification workflow where the annotator reads a sentence
(with context) and selects any one of the given tags: 1. Clean,2. Diﬀicult vocabulary ,3. Context Incomplete ,4.
Ambiguous sentence , and 5. Profane . Sentences with minor errors such as spelling mistakes, and punctuation errors
are corrected manually. If any sentence in a paragraph is discarded, the whole paragraph gets rejected, as context-
agnostic translations might turn out ambiguous. In addition, the annotators might also add metadata like the domain
andthetopictothesourcesentences.
Translation Stage. Theselectedsourcesentencesaretranslatedbytranslatorsacrossall22Indiclanguages. Toen-
surequality,standardtranslationguidelineshavebeendevelopedanditeratedbeforestartingthetranslationtask. There
is an active discussion amongst translators to ensure consistency. Translators of one language team help translators
ofanotherlanguageteamwhoarefromthesamelanguagefamilyorsharegeographicalboundaries. Thisensuresthe
authenticityoftransliteratedwordsandcross-culturalnuancesandgivesahumantouchtotheoutput.
Thetranslatorisprovidedwith:
•Sourcesentenceandthreecontextsentencesaroundthesourcesentencetohelpresolvetranslationambiguities.
•Translation outputs from one of the following engines (IndicTrans1 with fallback to Google Translate for
unsupported language), which can be post-edited. Translators could post-edit, translate from scratch, or use
anyalternativeMTsystemasastartingpoint. Notethatpost-editingsupportisprovidedonlyforthecreation
of training data. Providing MT as a referencehelps translators speed up and overcomethe existingmistakes
incurrenttranslationmodels. Afewlow-resourcelanguageslikeKashmiri,Konkani,andSantali,whereMT
systems are not available, are supported by the output of other related languages such as Urdu, Marathi, and
Bengali. This helps translators of low-resource languages to reuse syntactic structures and vocabulary from
related languages (as long as such vocabulary is acceptable in the target language). To create test sets, the
translators are expected to translate the sentences from scratch and are not shown any outputs from an MT
system.
•Tohelptranslatetechnicalvocabulary,thetranslatorscanconsultdictionariesandglossariesusingIndicGlos-
sary35. IndicGlossary contains approximately 2 million glossary items across 13 different Indic language
pairsandabout20domainsaggregatedfromvarioussources. TheseglossariesaresourcedfromtheCommis-
sion for Scientific and Technical Terminology (CSTT) and Technology Development for Indian Languages
(TDIL)whicharetherecommendedsourcesfortranslationterminologiesfordifferentdomains(Science,En-
gineering/Technology, Medical Science, Humanities, Social Sciences, Agricultural Science, and Veterinary
Science).
Forsomelow-resourcelanguages,sometranslatorswerenotproficientinEnglishbuthadproficiencyinanotherIndic
language (called the pivot language). For these languages, the translators are provided with the pivot language trans-
lation,whichtheyusetotranslateintotheirnativelanguage. Weusedthismethodforthefollowinglanguages: Dogri
(pivotHindi),Konkani(pivotMarathi),Maithili(pivotHindi),andSantali(pivotBengali).
35https://github.com/AI4Bharat/Indic-Glossaries
75PublishedinTransactionsonMachineLearningResearch(12/2023)
Quality Check Stage. Simultaneousreviewofthetranslatedsentencesisrequired,asithelpsprovidefeedbacktothe
translatorsandimprovestheoverallquality. Forthis,wehavededicatedreviewersineachlanguagewhoaretranslators
with 5+ years of experience. The job of the reviewer is to improve the overall quality of the translation by correcting
grammatical errors (if any), choosing better syntactic structures (if required), and rectifying inappropriate dialectical
featurestomakethetranslationsmorestandard. Thereviewermanuallyverifiesandcorrectseachtranslatedsentence(if
needed)toensureadherencetotheguidelinesbyselectinganyoneoftheoptionsonShoonya, 1. Accepted ,2. Accepted
with Changes ,and 3. Rejected . Rejectedsentencesgobacktothetranslatorwithanotefromthereviewer. Thereviewer
thencorrectsthetranslationbasedontheinputsprovidedinthenotefromthetranslator. Thecorrectedsentencesthen
gobacktothetranslatorforasecondroundofreview.
F.2 T ranslation Guidelines.
We developed an extensive set of translation guidelines to help the translators and ensure translation consistency and
quality across annotators and languages. These have been developed starting with the guidelines prepared by LDC36
for the BOLT Chinese-English translation task and further adapted for our scenarios and tasks. It is challenging to
translatein22languagesfrom4differentlanguagefamilies,followingthesamesetofrules,assyntaxandavailability
ofresourcesvarydrasticallyacrossthem. However,theguidelineswerecreatedconsideringthemaingoalof“getting
asnaturaltranslationsaspossible”. Intheguidelines,weensuredtheinclusivityofalluniquelinguisticfeaturessuch
as distinct word orders (SVO in Kashmiri), PNG agreement, tense/aspect differentiation in Manipuri, sociocultural
nuances,extremedialecticvariationsandchallengeslikeright-to-leftwriting(Urdu),scriptslikeMeiteiMayekandOl
Chiki,languageswhichdon’tsupportlongersyntacticstructureslikeEnglish,sentenceswithmanysubordinateclauses,
languages spoken in multiple regions such as Sindhi, unavailability of modern vocabulary in languages like Sanskrit,
inaccessibilityofdomain-specificdictionariesandglossariesinlanguageslikeBodo,Santaliandrevivingtheoriginal
formoflanguageslikeAssamese,Odiawhicharehighlyinfluencedbyhighresourcelanguagesinthesamearea(e.g.,
Bengali). The detailed guidelines are published as a standalone document here37. Some key highlights from these
guidelines.
•Thegeneralprincipleisthatthetranslationshouldmaintainthemeaning,style,tone,andregisterofthesource.
Noinformationshouldbeaddedordeleted.
•Oﬀicialnativescriptsofthelanguagesshouldbeused.
•Named entities and borrowed words can either be translated or transliterated. The exact choice depends on
theacceptedconventioninthelanguage,ifbothchoicesexist. Weavoidcoiningnewtranslationsifnoneexist,
andthewordsaretransliteratedinstead.
•Numbers,dates,andunitsaretobehandledaspernaturalconventionsinthetargetlanguage.
•In the context of historical events/people, translators can use more formal/older conventions or terms. For
morerecentevents/people,usingmorecasual/colloquialconventionsortermsispreferred.
•Fortestsets,sentenceswouldbetranslatedfromscratchwithoutaidfromanyMToutputtoavoidbiastowards
outputsofanyMTsystem.
F.3 Shoonya T ranslation Interface
Translations are performed using the translation task supported in Shoonya8. Shoonya has helped improve transla-
torproductivityandprojectmanagementbyprovidingfeaturesliketransliterationsupport, contextview, post-editing,
qualitycontrol,andcross-lingualsupport. Performingreviewsinreal-timehashelpedtheteamimprovethequalityof
translations whilst rectifying their mistakes. Shoonya supports right-to-left writing, which helps Urdu and Kashmiri
36https://catalog.ldc.upenn.edu/docs/LDC2008T18/
37https://github.com/AI4Bharat/IndicTrans2/blob/main/translation_guidelines.pdf
76PublishedinTransactionsonMachineLearningResearch(12/2023)
translatorstospeeduptheirtyping. Simplefeatureslike‘FindandReplace’, markingsentencesasdrafts, gettingran-
domizedsentencesacrossdomains,dailyprogresstab,etc. helpedtranslatorsimprovetheirproductivityandcollaborate
closelywiththeirpeers. BelowisasummaryofShoonya’sfeaturesthathavebenefittedthetranslationtask.
•T ransliteration Support : Romanized input with automatic transliteration to native scripts to help translators
notproficientinnativescriptkeyboards. Thetransliterationispoweredbytheopen-sourceIndicXlitmodels
(Madhanietal. ,2023),whichprovidetransliterationsupportfor20+Indianlanguages.
•Context View : Whentranslatingasentence,ithelpstohavethecontextinwhichthesentenceisbeingtranslated
to resolve any ambiguities. Shoonya allows translators to see paragraph-level context when translating an
individualsentence.
•P ost-Editing : ShoonyaenablespopulatingautomatictranslationsfromIndicTrans1models,currentlysupport-
ing11Indiclanguages. Thetranslatorscanpost-edittheseinitialtranslations.
•Quality Control : Shoonyaoffersvariousautomatedmaker-checkerflowstoevaluatethequalityoftranslated
data. Tofurtherensurequality,weimplementatwo-levelmaker-checkerparadigm,inwhichanexperienced
reviewer verifies each translation for conformance to the translation guidelines. This approach involves two
levelsofprocessingforeachsentence,providingarobustmechanismforensuringhightranslationquality.
•Cross-lingual Support : For low-resource languages, Shoonya supports showing annotators translations in
other related languages. For instance, given the task of translating English to Santali, the translators may
havediﬀicultyfullyunderstandingtheEnglishsentence. Insuchcases,wealsoshowthetranslatorsaBengali
translation(alanguagetheyareproficientin)ofthesamesentencetoaidthemwiththetask. Thisisacommon
scenario for many low-resource languages ( Costa-jussà et al. ,2022;Ebrahimi et al. ,2022;Marivate et al. ,
2020).
77PublishedinTransactionsonMachineLearningResearch(12/2023)
G Language of India
ThissectionprovidesanoverviewofIndianlanguagesbasedonthe2011censusdata,employinglanguageclassification
byJoshietal. (2020).
Table55:OverviewofIndianlanguages. NumberofNativeSpeakersasper2011census. Languageclassificationdone
accordingtothetaxonomyintroducedby Joshietal. (2020),whichclassifieslanguagesinto6classesfrom0to5with0
indicatingextremelylowresourceand5indicatinghighresourcelanguage. Manyoftheselanguagesarespokenacross
multiplestatesinthecountry. Samplecolumnindicatestheword”Bharat”writtenindifferentscripts.
Language
CodeName Family Sub-family Script Sample Class#Native
Speakers
asm_Beng Assamese Indo-AryanEastern
Indo-AryanBengali ভাৰত 2 15.3M
ben_Beng Bengali Indo-AryanEastern
Indo-AryanBengali ভারত 5 97.2M
brx_Deva Bodo Sino-Tibetan Boroic Devanagari भारत 1 1.4M
doi_Deva Dogri Indo-AryanNorthern
Indo-AryanDevanagari भारत 1 2.5M
gom_Deva Konkani Indo-AryanSouthern
Indo-AryanDevanagari भारत 1 2.2M
guj_Gujr Gujarati Indo-AryanWestern
Indo-AryanGujarati ભારત 4 55.4M
hin_Deva Hindi Indo-AryanCentral
Indo-AryanDevanagari भारत 5 528.3M
kan_Knda Kannada DravidianSouth
DravidianKannada ಾರತ್‍ 5 43.7M
kas_Arab
kas_DevaKashmiri Indo-AryanNorthern
Indo-AryanP erso-Arabic
Devanagariﺑﮭﺎرت
भारत1 6.7M
mai_Deva Maithili Indo-AryanEastern
Indo-AryanDevanagari भारत 1 13.5M
mal_Mlym Malayalam DravidianSouthern
DravidianMalayalam ഭാരത് 4 34.8M
mar_Deva Marathi Indo-AryanSouthern
Indo-AryanDevanagari भारत 4 83.0M
mni_Beng
mni_MteiManipuri Sino-TibetanCentral
Tibeto-BurmanBengali
Meiteiভারত
ꯚꯥꯔꯠ1 1.7M
npi_Deva Nepali Indo-AryanNorthern
Indo-AryanDevanagari भारत 2 2.9M
ory_Orya Odia Indo-AryanEastern
Indo-AryanOdia ଭାରତ 3 37.5M
pan_Guru Punjabi Indo-AryanNorth Western
Indo-AryanGurmukhi ਭਾਰਤ 3 33.1M
san_Deva Sanskrit Indo-Aryan Indo-Aryan Devanagari भारत 2 0.02M
sat_Olck Santali Austroasiatic Munda Ol Chiki ᱵᱷᱟᱨᱚᱛ 1 7.3M
snd_Arab
snd_DevaSindhi Indo-AryanNorth Western
Indo-AryanArabic
Devanagariﭜﺎرت
भारत1 2.7M
78PublishedinTransactionsonMachineLearningResearch(12/2023)
tam_T aml T amil Dravidian South Dravidian T amil பாரத் 4 69.0M
tel_T elu T elugu DravidianSouth Central
DravidianT elugu భారత్ 4 81.1M
urd_Arab Urdu Indo-AryanCentral
Indo-AryanUrdu ﺑﮭﺎرت 5 50.7M
79PublishedinTransactionsonMachineLearningResearch(12/2023)
H Examples of language translation
ThissectionshowsasentencetranslatedintoallIndianlanguagesasanillustrativeexample.
Table 57: The table shows an example of the same sentence translated into all 22 Indian languages. The sentence
translated is “All human beings are born free and equal in dignity and rights.” from the UN Declaration on Human
Rights.
Language T ranslation Romanized Sentence
asm_Beng সকেলা মান ুহ Ɯাধীন ৈহ জĮ ·হণ কেৰ আৰু
মযর্াদা আৰু অিধকাৰ সকেলােৰ সমান।Xokolumanuhswadhinhoijonmogrohonkore
arumarjyadaaruodhikaarxokolurexoman.
ben_Beng মানুষ জĮগতভােব Ɯাধীন এবং সŜান ও অিধ-
কার সবারই সমান।Manushjonmogotobhabeswadhinebongsom-
manoodhikarsobarisoman.
brx_Deva गाʹसबो सुबुंआनो उदांयै जोनोम जादों आरो गाʹसबो
सुबुंिन मान आरो मोनथायफोरा समान।gasibwsubungyanwudangywijwnwmjadwng
arwgasibwsubungnimanarwmwnthaiphwra
soman.
doi_Deva सब्भै मनुक्ख मान- प्र˃तश्ठा ते अ˃धकार ेंदेसंदभर् च
सुतैंतर ते इक बरोबर पैदा होंद ेन।Sabhe manukh maan-pratishtha teh ad-
hikaarein de sandarbh ch sutaintar teh ek
barobarpaidahaundeyn.
gom_Deva सगळे मनीस स्वत ंत्र म्हूण जल्माक येतात आनी प्र˃त-
श्ठा आनी हक्कांचे नदरेन समान आसतात.sagle manis swatantra mhun jalmak yetat and
pratishthaandhakkachenadrensamaanastat.
guj_Gujr બધાં મǙુષ્યો સ્ વતંત્ર જન્મે છે અને ગɳરમા અને અɵધ-
કારોમાં સમાન હોય છે.badha manushyo swatantra janme chhe ane
garimaaneadhikaromasamanhoychhe.
hin_Deva सभी मनुष्य स्वत ंत्र पैदा होते ह ैंऔर गȼरमा और अ˃ध-
कारों मेंसमान होते ह ैं।sabhi manushya swatantra paida hote hain aur
garimaauradhikaronmeinsamaanhotehain.
kan_Knda ಎಾಲ್ ಮನುಷಯ್ರು ಹುÖಟ್ßಂದĉೕ ಸವ್ತಂತರ್ರೂ
ಘನý ¥ಾಗೂ ಹಕುಕ್ಗಳ ದೃëಟ್åಂದ ಸಾನರೂ
ಆÐರುಾತ್Ĉ.ellā manushyaru huttinindalē svatantrarū
ghanate hāgū hakkugala drishtiyinda samā-
narūāgiruttāre.
kas_Deva सलीम इंसान छी जनमी आज़ाद ब ेबराबर ˃डिग्नटी ब े
हक़saliminsaanchijanmiAzadbebraaberdignity
behaq
kas_Arab ﺑﺮاﺑﺮﻣﻨٛﺰﺣﻘﻮﻗﺲﺗﮧٕﻋﺰتﺗﮧٕآزادﭼﮫاﻧﺴﺎنﺗﻤﺎم 
۔TamaaminsaanchiAzadtayezathtahaqooqs
Manzbraaber.
mai_Deva सभ मनुष स्वत ंत्र पैदा होयत अʺछ आओर अ˃धकार
आ प्र˃तष्ठा मे बराबर होयत अʺछ।Sabhmanukhswatantrapaidahoyatachhiaaor
adhikaaraaprtishthamebarabarhoyatachhi.
mal_Mlym എല്ലാ മനുഷ്യരും സ്വത്രന്തരായി ജനിച്ച-
വരും ഒപ്പം അന്തസിലും അവകാശങ്ങ-
ളിലും തുല്യരുമാണ് .ellaa manushyarum swathanthrarayi
janichavarum oppam anthassilum
avakaashangalilumthulyarumaanu.
mar_Deva सवर्मनुष्य स्वत ंत्र व्यक्तɃ म्हण ून जन्माला येतात आʺण
प्र˃तष्ठा आʺण हक्कांच्या द ुष्टीकोनातून समान असतात.sarv manushya swatantrya vyakti mhanun jan-
mala yetat aani pratishtha ani hakkanchya
dushtikonatunsamaanastat.
mni_Beng মীওইবাখুিদংমকনীংতřাঅমস ুংইকায়খুŔবাঅম-
সুং হকেশলগী লমদা চপ মাĭনা ৈলিমĭির।Mioiba khudingmak ningtamba amasung
ikaikhumnaba amasung hakselgi lamda chap
mannanaleiminnari.
mni_Mteiꯃꯤꯑꯣꯏꯕ ꯄꯨꯝꯅꯃꯛ ꯅꯤꯡꯇꯝꯕ ꯑꯃꯁꯨꯡ
ꯏꯀꯥꯏꯈꯨꯝꯅꯕ ꯑꯃꯁꯨꯡ ꯍꯛꯁꯤꯡꯗ ꯃ ꯥꯟꯅꯅ
ꯄꯣꯛꯏ꯫Mioiba pumnamak ningtamba amasung
ikaikhumnabaamasunghaksingmannanapok
I.
npi_Deva सबैमािनस स्वतन्त्र जन्मन्छन् र सम्मान तथा अ˃ध-
कारमा समान ह ुन्छन् छन्।sabai maanis swatrantra janmanchan ra sam-
maantathaaadhikaarmaasamaanchan.
ory_Orya ସମųମନ ୁ ଷ୍ୟଜନ ୍ମ ଗତଭାେବŹାଧୀନଓସřାନତଥା
ଅଧ ି କାରଦ ୃ ū ି ର ୁ ସମାନ ￿samasta manushya janmagata bhaabe swad-
hina o sammaana tathaa adhikaara drushtiru
samaan.
pan_Guru ਸੱਭ ਮਨੁੱਖ ਅਜ਼ਾਦ ਪੈਦਾ ਹੁੰਦੇ ਹਨ ਅਤੇ ਮਾਣ-ਸਨਮਾਨ ਤੇ
ਅਿਧਕਾਰਾਿਵੱਚਬਰਾਬਰਹਨ।Sabhmanukhazaadpaidahundehanatemann
samanteadhekaravichbarabarhan.
80PublishedinTransactionsonMachineLearningResearch(12/2023)
san_Deva सवǼमानवजीिवनः जन्मनः एव स्वतन्त्राः, मानदृष्ट्या
अ˃धकारदृष्ट्या समानाश्च।sarve maanavajivinah janmanah eva
svatantraah, maanadrishtyaa adhikaaradr-
ishtyaasamaanaashca.
sat_Olck ᱥᱟᱱᱟᱢ ᱢᱟᱹᱱᱢᱤ ᱜᱮ ᱯᱷᱩᱨᱜᱟᱹᱞ ᱟᱛᱮᱫ ᱠᱩ
ᱡᱟᱱᱟᱢᱚᱜᱼᱟ ᱟᱨ ᱢᱟᱹᱱ ᱟᱨ ᱟᱹᱭᱫᱟᱹᱨ ᱨᱮ ᱠᱩ
ᱥᱚᱢᱟᱱ ᱜᱤᱭᱟ ￿Sanammanmigephurgalatedkujanamog-aar
manaraydarrekusomangiya.
snd_Arab آﮬﻦﭤﯿﺎﭘﯿﺪاﺑﺮاﺑﺮ۾ﺣﻘﻦ۽ﻋﺰت۽آزاداﻧﺴﺎنﺳﭛ 
۔Sabh insaan aazad paida thiya aahin, ain izzat
ainhakkanmeinbarabaraahin.
snd_Deva सभई इंसान आज़ाद, ऐ ंमान ऐंहकिन मेंिहक जिहड़ा
ॼावल आिहन।Sabhai Insan aazad, ain maan ain hakan mein
hikjahidajavalaahin.
tam_T amlமனிதர்கள்ப¥றப்பால்சுதந்த¦ரமான-
வர்கள், மற்றும் சம உரிைமகளும்
கண்ணியமும்ெகாண்டவர்கள்.manitharkalpirappaalsuthanthiramaanavarkal,
matrum sama urimaykalum kanniyamum kon-
davarkal.
tel_T elu మనుషులంతా సేవ్చచ్గా గౌరవమరాయ్దలు, హకుక్లలో
సమానతవ్ంతో పుడతారు.manushulantaasvecchagaagouravamaryadalu,
hakkulalosamantvamtopudataru.
urd_Arab ﺣﻘﻮقاورﻋﺰتاورﮨﯿﮟﮨﻮﺋﮯﭘﯿﺪاآزاداﻧﺴﺎنﺗﻤﺎم 
ﮨﯿﮟ۔ﺑﺮاﺑﺮﺳﮯﻟﺤﺎظﮐﮯ tamaminsanazadpaidahuehainaurizzataur
huqooqkelihazsebarabarhain.
81PublishedinTransactionsonMachineLearningResearch(12/2023)
I Language Coverage of various MT models
This section provides an overview of the Indian languages supported by different open-source and commercial NMT
systems.
Table59: Coverageofthe22languageslistedinthe 8thScheduleoftheConstitutionofIndiabyvariousNMTsystems
NMT Systems
language IndicTrans1 IndicTrans2 Azure NLLB-200 GoogleTranslate
asm_Beng 3 3 3 3 3
ben_Beng 3 3 3 3 3
brx_Deva 7 3 7 7 7
doi_Deva 7 3 7 7 3
gom_Deva 7 3 3 7 3
guj_Gujr 3 3 3 3 3
hin_Deva 3 3 3 3 3
kan_Knda 3 3 3 3 3
kas_Arab 7 3 7 3 7
kas_Deva 7 3 7 3 7
mai_Deva 7 3 3 3 3
mal_Mlym 3 3 3 3 3
mar_Deva 3 3 3 3 3
mni_Beng 7 3 7 3 7
mni_Mtei 7 3 7 7 3
npi_Deva 7 3 3 3 3
ory_Orya 3 3 3 3 3
pan_Guru 3 3 3 3 3
san_Deva 7 3 7 3 3
sat_Olck 7 3 7 3 7
snd_Arab 7 3 3 3 3
snd_Deva 7 3 7 7 7
tam_T aml 3 3 3 3 3
tel_T elu 3 3 3 3 3
urd_Arab 7 3 3 3 3
# languages 11 22 16 19 19
# language-script combinations 11 25 16 20 19
82PublishedinTransactionsonMachineLearningResearch(12/2023)
J Model Card - IndicT rans2
Following Mitchelletal. (2019),weprovideamodelcardforourIndicTrans2models.
J.1 Model Details
•Person or organization developing model: IndicTrans2 models are multilingual translation models devel-
opedbyAI4Bharat.38
•Model data: IndicTrans2modelswerereleasedonMay26,2023.
•Model version: IndicTrans2modelsdescribedinthispaperareversion1.0.0.
•Model type: IndicTrans2modelsare18-layerencoder-decodertransformermodelswith1.1Bparameters,one
forEnglishtoIndictranslationdirectionwhiletheotherforIndictoEnglishtranslation.
•Information about training algorithms, parameters, fairness constraints or other applied approaches,
and features: IndicTrans2modelsweretrainedwiththeexacttrainingconfigurationdescribedinSection 5.5
and the training data described in Section 5.1. Sections 5.2,5.3and5.7describes the data preprocessing,
tokenization,andpostprocessingsteps,correspondingly,thathavebeenfollowedduringthetraining.
•Paper or other resources for more information: SeetherestofthispaperformoredetailsonIndicTrans2
models. Moredetailsarealsoavailablein IndicTrans2 ,39ouropen-sourceGitHubrepository.
•License:IndicTrans2modelsaremadeavailablethroughapermissiveMITlicense.40
•Where to send questions or comments about the model: Pleaseopenanissue41onouropen-sourceGitHub
repository.
J.2 Intended Use
•Primary intended uses: IndicTrans2modelsaremachinetranslationmodelsdesignedforresearchandcom-
mercial purposes, especially for Indic languages. These models enable the translation of the text, either in
singleorbatchformat,across22differentIndiclanguages,including25language-scriptcombinationstoand
from English. In addition, these models offer support for translation between Indic languages using a pivot-
basedapproach. Furtherinformationonhowtousethemodelscanbefoundat IndicTrans2 ,ouropen-source
GitHubrepository.
•Primary intended users: TheprimaryintendedusersoftheIndicTrans2modelsare:
–ResearchersaimingtoexploreandadvancelanguagetechnologiesforIndiclanguages.
–IndividualsseekingtotranslatecontenttoandfromthesupportedIndiclanguagesforvariousday-to-day
usecases.
–Organizations interested in translating their proprietary or internal content into the supported Indic lan-
guages.
•Out-of-scope use cases: IndicTrans2modelsarereleasedunderMITlicensewithoutanyusagelimitations.
38https://ai4bharat.iitm.ac.in/
39https://github.com/ai4Bharat/IndicTrans2
40https://opensource.org/license/mit/
41https://github.com/AI4Bharat/IndicTrans2/issues
83PublishedinTransactionsonMachineLearningResearch(12/2023)
J.3 Data, Metrics, Limitations, and Recommendations
•Training dataset: Section5.1describedtheparallelcorporausedfortrainingourmodels. Table 1providesthe
statistics of the bitext pairs from different sources. In addition, we augment the training data with synthetic
data as described in Section 5.6generated from the intermediate IndicTrans2 models for training the final
versionsoftheIndicTrans2models.
•Fine-tuning dataset: BPCC-H-Wiki (see Section 3.3) and NLLB-Seed ( Costa-jussà et al. ,2022) were used
forthefine-tuningoftheIndicTrans2modelsasdescribedinSection 5.5. Table1providesthestatisticsofthe
gold-standardbitextpairsfromdifferentsources.
•Evaluation dataset: Section6.2describesallthebenchmarksincludingFLORES-200andourIN22consid-
eredforevaluationofourIndicTrans2models. ThegenerationandevaluationprocedurefortheIndicTrans2
models are outlined in Section 6.4and Section 6.5. Additionally, the baselines compared in this paper also
followthesameevaluationprocedureastheIndicTrans2models.
•Metrics:IndicTrans2 models were evaluated using several metrics such as chrF++, BLEU and COMET as
described in Section 6.3. We use chrF++ as our primary metric. In addition, we also conduct the human
evaluationwiththeXSTSprotocolonasmallportionofIN22Combinedevaluationset(seeAppendix C).
•Limitations: Section9describestheknowncaveatsoftheIndicTrans2models.
•Recommendations for future work: IndicTrans2 serves as a strong foundational translation model with
extensive support for all 22 scheduled Indic languages. However, it is important to acknowledge that there
are additional languages that are currently not supported (see Section 2). There is a potential to expand
IndicTrans2modelstosupportmorelanguagesorimprovetheperformanceoftheexistingsupportedlanguages
throughminimalfine-tuning.
84PublishedinTransactionsonMachineLearningResearch(12/2023)
K Model Card - IndicT rans2-M2M
Following Mitchelletal. (2019),weprovideamodelcardforIndicTrans2-M2Mmodels.
K.1 Model Details
•Person or organization developing model: IndicTrans2-M2M models are multilingual translation models
developedbyAI4Bharat.42
•Model data: IndicTrans2-M2MmodelswerereleasedonDec01,2023.
•Model version: IndicTrans2-M2Mmodelsdescribedinthispaperareversion1.0.0.
•Model type: IndicTrans2-M2MandIndicTrans2-Dist-M2Mmodelsare18-layerencoder-decodertransformer
modelswith1.2BMparametersandthecompactvariantwith350Mparameters,respectively,supportingIndic
toIndictranslation.
•Information about training algorithms, parameters, fairness constraints or other applied approaches,
and features: IndicTrans2-M2M and IndicTrans2-Dist-M2M models were trained with the exact training
configuration and the training data described in Section 7.5.2. Sections 5.2,5.3and5.7describes the data
preprocessing, tokenization, and postprocessing steps, correspondingly, that have been followed during the
training.
•Paper or other resources for more information: SeetherestofthispaperformoredetailsonIndicTrans2-
M2MandIndicTrans2-Dist-M2Mmodels. Moredetailsarealsoavailablein IndicTrans2 ,43ouropen-source
GitHubrepository.
•License:IndicTrans2-M2MandIndicTrans2-Dist-M2MmodelsaremadeavailablethroughapermissiveMIT
license.44
•Where to send questions or comments about the model: Pleaseopenanissue45onouropen-sourceGitHub
repository.
K.2 Intended Use
•Primary intended uses: IndicTrans2-M2MandIndicTrans2-Dist-M2Mmodelsaremachinetranslationmod-
elsdesignedforresearchandcommercialpurposes,especiallyforIndiclanguages. Thesemodelsenablethe
translationofthetext,eitherinsingleorbatchformat,between22differentIndiclanguages. Furtherinforma-
tiononhowtousethemodelscanbefoundat IndicTrans2 ,ouropen-sourceGitHubrepository.
•Primary intended users: TheprimaryintendedusersoftheIndicTrans2-M2MandIndicTrans2-Dist-M2M
modelsare:
–ResearchersaimingtoexploreandadvancelanguagetechnologiesforIndiclanguages.
–IndividualsseekingtotranslatecontenttoandfromthesupportedIndiclanguagesforvariousday-to-day
usecases.
–Organizations interested in translating their proprietary or internal content into the supported Indic lan-
guages.
•Out-of-scope use cases: IndicTrans2-M2MandIndicTrans2-Dist-M2MarereleasedunderMITlicensewith-
outanyusagelimitations.
42https://ai4bharat.iitm.ac.in/
43https://github.com/ai4Bharat/IndicTrans2
44https://opensource.org/license/mit/
45https://github.com/AI4Bharat/IndicTrans2/issues
85PublishedinTransactionsonMachineLearningResearch(12/2023)
K.3 Data, Metrics, Limitations, and Recommendations
•Training dataset: Section7.5.2describedtheparallelcorporausedfortrainingourmodels.
•Evaluation dataset: Section6.2describesallthebenchmarksincludingFLORES-200andourIN22consid-
ered for evaluation of our IndicTrans2-M2M and IndicTrans2-Dist-M2M models. The generation and eval-
uation procedure for the IndicTrans2-Dist models are outlined in Section 6.4and Section 6.5. Additionally,
thebaselinescomparedinthispaperalsofollowthesameevaluationprocedureastheIndicTrans2-M2Mand
IndicTrans2-Dist-M2Mmodels.
•Metrics:IndicTrans2-M2MandIndicTrans2-Dist-M2Mmodelswereevaluatedusingseveralmetricssuchas
chrF++,BLEUandCOMETasdescribedinSection 6.3. WeusechrF++asourprimarymetric.
•Limitations: Section9describes the known caveats of the IndicTrans2-M2M and IndicTrans2-Dist-M2M
modelsmodels. WealsodonotconductanXSTShumanevaluationfortheIndicTrans2-Distmodels.
•Recommendations for future work: IndicTrans2-M2M and IndicTrans2-Dist-M2M models serves as a
strongfoundationaltranslationmodelwithextensivesupportforall22scheduledIndiclanguages. However,
it is important to acknowledge that there are additional languages that are currently not supported (see Sec-
tion2). ThereisapotentialtoexpandIndicTrans2-M2MandIndicTrans2-Dist-M2Mmodelstosupportmore
languagesorimprovetheperformanceoftheexistingsupportedlanguagesthroughminimalfine-tuning.
86PublishedinTransactionsonMachineLearningResearch(12/2023)
L Model Card - IndicT rans2-Dist
Following Mitchelletal. (2019),weprovideamodelcardforIndicTrans2-Distmodels.
L.1 Model Details
•Person or organization developing model: IndicTrans2-Dist models are multilingual translation models
developedbyAI4Bharat.46
•Model data: IndicTrans2-DistmodelswerereleasedonDec01,2023.
•Model version: IndicTrans2-Distmodelsdescribedinthispaperareversion1.0.0.
•Model type: IndicTrans2-Distmodelsare18-layerencoder-decodertransformermodelswith211Mparame-
ters,oneforEnglishtoIndictranslationdirectionwhiletheotherforIndictoEnglishtranslation.
•Information about training algorithms, parameters, fairness constraints or other applied approaches,
and features: IndicTrans2-Dist models were trained with the exact training configuration described in Ap-
pendixDand the training data described in Section 7.6. Sections 5.2,5.3and5.7describes the data prepro-
cessing,tokenization,andpostprocessingsteps,correspondingly,thathavebeenfollowedduringthetraining.
•Paper or other resources for more information: SeetherestofthispaperformoredetailsonIndicTrans2-
Distmodels. Moredetailsarealsoavailablein IndicTrans2 ,47ouropen-sourceGitHubrepository.
•License:IndicTrans2-DistmodelsaremadeavailablethroughapermissiveMITlicense.48
•Where to send questions or comments about the model: Pleaseopenanissue49onouropen-sourceGitHub
repository.
L.2 Intended Use
•Primary intended uses: IndicTrans2-Distmodelsaremachinetranslationmodelsdesignedforresearchand
commercial purposes, especially for Indic languages. These models enable the translation of the text, either
in single or batch format, across 22 different Indic languages, including 25 language-script combinations to
and from English. In addition, these models offer support for translation between Indic languages using a
pivot-based approach. Further information on how to use the models can be found at IndicTrans2 , our
open-sourceGitHubrepository.
•Primary intended users: TheprimaryintendedusersoftheIndicTrans2-Distmodelsare:
–ResearchersaimingtoexploreandadvancelanguagetechnologiesforIndiclanguages.
–IndividualsseekingtotranslatecontenttoandfromthesupportedIndiclanguagesforvariousday-to-day
usecases.
–Organizations interested in translating their proprietary or internal content into the supported Indic lan-
guages.
•Out-of-scope use cases: IndicTrans2modelsarereleasedunderMITlicensewithoutanyusagelimitations.
46https://ai4bharat.iitm.ac.in/
47https://github.com/ai4Bharat/IndicTrans2
48https://opensource.org/license/mit/
49https://github.com/AI4Bharat/IndicTrans2/issues
87PublishedinTransactionsonMachineLearningResearch(12/2023)
L.3 Data, Metrics, Limitations, and Recommendations
•Training dataset: Section7.6describedtheparallelcorporausedfortrainingourmodels.
•Fine-tuning dataset: BPCC-H-Wiki (see Section 3.3) and NLLB-Seed ( Costa-jussà et al. ,2022) were used
forthefine-tuningoftheIndicTrans2-DistmodelsasdescribedinSection 5.5. Table1providesthestatistics
ofthegold-standardbitextpairsfromdifferentsources.
•Evaluation dataset: Section6.2describes all the benchmarks including FLORES-200 and our IN22 con-
sidered for evaluation of our IndicTrans2-Dist models. The generation and evaluation procedure for the
IndicTrans2-DistmodelsareoutlinedinSection 6.4andSection 6.5. Additionally,thebaselinescomparedin
thispaperalsofollowthesameevaluationprocedureastheIndicTrans2-Distmodels.
•Metrics:IndicTrans2-DistmodelswereevaluatedusingseveralmetricssuchaschrF++,BLEUandCOMET
asdescribedinSection 6.3. WeusechrF++asourprimarymetric.
•Limitations: Section9describestheknowncaveatsoftheIndicTrans2-Distmodels. Wealsodonotconduct
anXSTShumanevaluationfortheIndicTrans2-Distmodels.
•Recommendations for future work: IndicTrans2-Distservesasacompactyetstrongfoundationaltranslation
modelwithextensivesupportforall22scheduledIndiclanguages. However, itisimportanttoacknowledge
that there are additional languages that are currently not supported (see Section 2). There is a potential to
expand IndicTrans2-Dist models to support more languages or improve the performance of the existing sup-
portedlanguagesthroughminimalfine-tuning.
88PublishedinTransactionsonMachineLearningResearch(12/2023)
M Dataset Card
Following Gebru et al. (2021);Pushkarna et al. (2022), we provide a dataset card for our Bharat Parallel Corpus Col-
lection,thedatasetusedtotrainIndicTrans2aswellasIN22,ourbenchmarktestsetsforIndiclanguages.
M.1 Dataset Description
•Dataset summary:
–Bharat Parallel Corpus Collection (BPCC) is a comprehensive and publicly accessible parallel corpus
comprising existing and newly added data for all 22 scheduled Indic languages. It consists of two com-
ponents: BPCC-Mined and BPCC-Human. BPCC contains a total of ~230M bitext pairs (see Table 1).
BPCC-Minedcomprises~228millionpairs,witharound~126millionpairsnewlyminedaspartofthis
work. On the other hand, BPCC-Human consists of 2.2 million gold standard En-X pairs, with addi-
tional contributions of 644K bitext pairs from English sentences sourced from Wikipedia (forming the
BharatParallelCorpusCollection-H-Wikisubset)and139Ksentencescoveringcontentfromday-to-day
use cases (forming the Bharat Parallel Corpus Collection-H-Daily subset). It is worth highlighting that
BPCC provides the first available datasets for many languages and significantly increases the available
dataforalllanguagescovered.
–IN22 is a newly created comprehensive benchmark for evaluating machine translation performance in
multi-domain,n-wayparallelcontextsacross22Indiclanguages. Ithasbeencreatedfromthreedistinct
subsets, namely IN22-Wiki, IN22-Web, and IN22-Conv. The Wikipedia and Web sources subsets of-
fer diverse content spanning news, entertainment, culture, legal, and India-centric topics. IN22-Wiki
and IN22-Web have been combined and considered for evaluation purposes and released as IN22-Gen.
Meanwhile,theconversationdomainsubsetIN22-Convisdesignedtoassesstranslationqualityintypical
day-to-dayconversational-styleapplications.
•How to use the data? We provide the links to access the data and directions for usage in the README of
IndicTrans2 ,50ouropen-sourcedGitHubrepository.
•Supported tasks and leaderboards: Theprovideddataisprimarilyintendedfortrainingmachinetranslation
models. Itservesasavaluabletrainingcorpusfordevelopingandimprovingsuchmodels. Furthermore,the
IN22benchmarkdatasetisincluded,whichservesasarobustevaluationsetforassessingtheperformanceof
machinetranslationmodels. InitialresultsoftheIndicTrans2modelsandtheexistingbaselinesareavailable
inouropen-sourceGitHubrepository,providinginsightsandcomparisonsasofthereleasedate.
•Languages: Thedatasetcoversatotalof22scheduledIndiclanguageswithmultiplescripts,amountingtoa
totalof25language-scriptcombinations.
M.2 Dataset Creation
•Curation rationale:
–BPCCiscreatedtotrainandimprovemachinetranslationmodels. Itisavaluableresourcefordeveloping
and improving such models. Section 4provides a detailed description of the procedure followed for
the mined data collection, referred to as BPCC-Mined. Similarly, Section 3.3outlines the motivation
andannotationprocedureforcreatinghigh-qualityseeddata,referredtoasBPCC-Human. Additionally,
section4.3providesinsightsintothecurationandfiltrationprocessoftheexistingminedparallelcorpora.
–IN22benchmarkdatasetiscreatedtoserveasareliableevaluationsetforassessingtheperformanceof
machine translation models. Section 3.2provides a comprehensive discussion about the subsets of the
evaluationbenchmarkandthecreationprocedureforeachsubset.
•Source data:
50https://github.com/AI4Bharat/IndicTrans2
89PublishedinTransactionsonMachineLearningResearch(12/2023)
–Table1summarizestheparallelcorporafromdifferentsources. BPCC-Mined(seeSection 4)component
is typically mined from IndicCorp v2 ( Doddapaneni et al. ,2023) and the Internet Archive.51BPCC-
Human-Wiki is a multi-domain seed data collection that is sourced from Wikipedia articles whereas
BPCC-Human-Dailyisanin-housecreateddatasetcoveringcontentfromday-to-dayconversationsand
usecases(seeSection 3.3).
–IN22benchmarkisacomprehensivemulti-domainbenchmarkthatconsistsofthreesubsets: IN22-Wiki,
sourced from Wikipedia articles; IN22-Web, sourced from PDFs available on various government web-
sites and open-source books; and IN22-Conv, an in-house benchmark created through the interplay be-
tweenannotators(seeSection 3.2).
•Annotations: TheannotationswereperformedontheShoonyaplatform.52Section3providesfurtherdetails
abouttheprocedureandtheguidelinesfollowedforannotations.
•Personal and sensitive information: Given that a substantial portion of the dataset is mined from publicly
availablewebsitesatalargescale,weacknowledgethepossibilityofunintentionalinclusionofpersonaland
sensitiveinformation. Ifthereareanyconcernsregardingpotentialleakagesofsuchinformation,pleasereach
outto miteshk@cse.iitm.ac.in forfurtherassistanceandresolution.
M.3 Considerations for Using the Data
•Social impact of the dataset: The dataset has a notable social impact, as it is specifically constructed and
curated to improve the translation quality of all 22 scheduled Indic languages. It also provides support for
low-resourced languages that utilize multiple scripts. This dataset contributes to the overall improvement of
translationmodels, benefitingawiderangeofusers, helpingbridgelanguagebarriers, andfacilitatingbetter
communicationandunderstandingacrossdiverselinguisticcommunities.
•Discussion of biases: Thecurrentworkdoesnotexplicitlyexaminebiasesinthedata. However,weacknowl-
edgetheimportanceofstudyingbiasesandhopetoconductfurtherinvestigationsinthisareainthefuture.
M.4 Additional Information
•Dataset Curators: ThedatasetwascuratedbyAI4Bharat,whocollecteddatafromvariousexistingsources,
includingcontributionsfromtheexistingdatasetcontributors. AI4Bharatalsoreleasesmineddata,in-house
createdseeddata,andbenchmarks.
•Licensing Information: BharatParallelCorpusCollection(BPCC)consistsofthelargestpubliclyavailable
parallelcorporaforIndiclanguages. Itincludesvarioustypesofcorporaobtainedfromdifferentsources. The
licensinginformationforeachcategoryisasfollows:
–ExistingMinedCorpora(NLLB&Samanantar): ThesecorporaarereleasedundertheCC0license.53
–Existing Seed Corpora (NLLB, ILCI, MASSIVE): The seed corpora are also released under the CC0
license.53
–Newly added mined corpora (Samanantar++ & Comparable): The newly added mined corpora are also
releasedundertheCC0license.53
–Newlyaddedseedcorpora(Wiki&Daily): ThenewlyaddedseedcorporaarereleasedundertheCCBY
4.0license.54
–NewlycreatedIN-22testset: TheIN22testsetisreleasedundertheCCBY4.0license.54
51https://archive.org
52https://ai4bharat.iitm.ac.in/shoonya
53https://creativecommons.org/share-your-work/public-domain/cc0/
54http://creativecommons.org/licenses/by/4.0
90