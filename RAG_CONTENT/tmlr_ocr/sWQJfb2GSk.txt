Published in Transactions on Machine Learning Research (05/2023)
Exploring the Approximation Capabilities of Multiplicative
Neural Networks for Smooth Functions
Ido Ben-Shaul ido.benshaul@gmail.com
Department of Applied Mathematics
Tel-Aviv University, Israel
eBay Research
Tomer Galanti galanti@mit.edu
Department of Brain and Cognitive Sciences
Massachusetts Institute of Technology (MIT)
Cambridge, MA, USA
Shai Dekel shaidekel6@gmail.com
Department of Applied Mathematics
Tel-Aviv University, Israel
Reviewed on OpenReview: https: // openreview. net/ forum? id= sWQJfb2GSk
Abstract
Multiplication layers are a key component in various influential neural network modules,
including self-attention and hypernetwork layers. In this paper, we investigate the ap-
proximation capabilities of deep neural networks with intermediate neurons connected by
simple multiplication operations. We consider two classes of target functions: generalized
bandlimited functions, which are frequently used to model real-world signals with finite
bandwidth, and Sobolev-Type balls, which are embedded in the Sobolev Space Wr,2. Our
results demonstrate that multiplicative neural networks can approximate these functions
with significantly fewer layers and neurons compared to standard ReLU neural networks,
with respect to both input dimension and approximation error. These findings suggest that
multiplicative gates can outperform standard fully-connected layers and potentially improve
neural network design.
1 Introduction
Deep learning has seen tremendous success in solving a wide range of tasks in recent years, including image
classification (He et al., 2016; Dosovitskiy et al., 2021; Zhai et al., 2022), language processing (Vaswani et al.,
2017; Devlin et al., 2019; Brown et al., 2020), interacting with open-ended environments (Silver et al., 2016;
Arulkumaran et al., 2019), and code synthesis (Chen et al., 2021).
Recent empirical studies have shown that neural networks that incorporate multiplication operations between
intermediate neurons (Durbin & Rumelhart, 1989; Urban & van der Smagt, 2016; Trask et al., 2018), such as
self-attention layers (Vaswani et al., 2017), dynamic convolutions (Wu et al., 2019) and hypernetworks (Ha
et al., 2017; Krueger et al., 2017; Littwin & Wolf, 2019; Littwin et al., 2020), are particularly effective. For
example, self-attention layers have been widely successful in computer vision (Dosovitskiy et al., 2021; Zhai
et al., 2022) and language processing (Cheng et al., 2016; Parikh et al., 2016; Paulus et al., 2018; Vaswani
et al., 2017). It has also been shown that one can achieve reasonable performance with Transformers even
without applying non-linear activation functions (Levine et al., 2020). Additionally, hypernetworks, which use
multiplication to generate network weights seem to improve the performance of neural networks on various
meta-learning tasks (von Oswald et al., 2020; Littwin & Wolf, 2019; Bensadoun et al., 2021). In a different
1Published in Transactions on Machine Learning Research (05/2023)
line of work, (Haber & Ruthotto, 2018; Eliasof et al., 2021) showed that incorporating non-linear interactions
can stabilize the training of deep networks, including convolutional and graph neural networks. Despite their
practical success, the theoretical reasons behind the effectiveness of multiplicative layers in deep learning
remain unclear.
In this work, we study the expressive power of neural networks with multiplication layers. Specifically, we
want to evaluate the number of neurons and layers needed to approximate a given function within a given
error tolerance using a specific architecture. A classic result in the theory of deep learning, known as the
universal approximation property, shows that neural networks can approximate any smooth target function
with as few as one hidden layer (Cybenko, 1989; Hornik et al., 1989; Funahashi, 1989; Leshno et al., 1993).
However, these papers do not provide specific information about the type of architecture and number of
parameters required to achieve a given level of accuracy. This is a crucial question, as a high requirement
for these resources could limit the universality of neural networks and explain their limited success in some
practical applications.
In a recent paper, (Jayakumar et al., 2020) investigated the role of multiplicative interaction as a framework
including various classical and contemporary neural network components, including gating, attention layers,
hypernetworks, and dynamic convolutions. The authors conjectured that multiplicative layers are well-suited
for modeling conditional computations and proved that networks with multiplicative connections are more
expressive than those with ReLU fully-connected layers. However, they did not investigate to what extent
the multiplicative networks serve as better function approximators than standard ReLU networks.
Previous work has demonstrated that functions in Sobolev spaces can be approximated by a one-hidden layer
neural network with analytic activation functions (Mhaskar, 1996). However, the number of neurons required
to approximate these functions with an error of at most ϵin theL∞norm scales asO(ϵ−d/r), wheredis the
input dimension, ris the smoothness degree of the target function, and ϵ>0is the error rate. This raises
the question of whether the curse of dimensionality, the phenomenon whereby the complexity of a model
grows exponentially with the input dimension, is inherent to neural networks.
On the other hand, DeVore et al. (1989) proved that any continuous function approximator that approximates
all Sobolev functions of order rand dimension dwithin error ϵrequires at least Ω(ϵ−d/r)parameters in the
L∞norm. This result meets the bound of Mhaskar (1996) and confirms that neural networks cannot avoid
the curse of dimensionality for the Sobolev space when approximating in the L∞norm. A key question is
whether neural networks can overcome this curse of dimensionality for certain sets of target functions, and
what kind of architectures provide the best guarantees for approximating these functions.
To overcome the curse of dimensionality, various studies (Mhaskar et al., 2017; Poggio et al., 2020; Kohler &
Krzyżak, 2017; Montanelli & Du, 2019; Blanchard & Bennouna, 2022; Galanti & Wolf, 2020) have investigated
the approximation capabilities of neural networks in representing other classes of functions with some
additional structural properties or by assuming weaker notions of distance, such as the L2distance. For
example, Mhaskar et al. (2017); Poggio et al. (2020) showed that smooth, compositionally sparse functions with
a degree of smoothness rcan be approximated with the L∞distance up to error ϵusing deep neural networks
withO(dϵ−2/r)neurons. Other structural constraints have been applied to functions with structured input
spaces (Mhaskar, 2010; Nakada & Imaizumi, 2022; Schmidt-Hieber, 2019), compositions of functions (Kohler
& Krzyżak, 2017), piecewise smooth functions (Petersen & Voigtländer, 2017; Imaizumi & Fukumizu, 2018).
A different line of research has focused on understanding the types of functions that certain neural network
architectures can implement with regularity constraints. For example, E et al. (2021) showed that the space
of 2-layer neural networks is equivalent to the Barron space when the size of their weights is restricted. They
further showed that Barron functions can be approximated within ϵusing 2-layer networks with O(ϵ−2)
neurons. Another line of research has considered spectral conditions on the function space, allowing functions
to be expressed as infinite-width limits of shallow networks (Barron, 1991; Klusowski & Barron, 2018).
In (Blanchard & Bennouna, 2022) they considered the space of Korobov functions, which are functions that
are practically useful for solving partial differential equations (PDEs). They showed any Korobov function
can be approximated up to error ϵinL2distance with a 2-layer neural network with ReLU activation function
withO(ϵ−1log(1/ϵ)1.5(d−1)+1)and with aO(log(d))-depth network with O(ϵ−0.5log(1/ϵ)1.5(d−1)+1)neurons.
2Published in Transactions on Machine Learning Research (05/2023)
It is generally known that real-world datasets (such as CIFAR10 or ImageNet) often (approximately) lie
on low-dimensional manifolds of the input space (Tenenbaum et al., 2000; Belkin & Niyogi, 2003). Several
publications (Nakada & Imaizumi, 2022; Schmidt-Hieber, 2019; Chen et al., 2019; Kohler et al., 2019; Suzuki,
2018) studied the adaptivity of deep ReLU networks to the intrinsic dimensionality of data and in particular
showed that the curse of dimensionality is avoidable when the data lies on a low-dimensional manifold. The
work of (Suzuki & Nitanda, 2019) further showed that deep ReLU networks can overcome the curse of
dimensionality even when the input data does not lie on a low-dimensional manifold, as long as the target
function is included in some anisotropic Besov space Nikol’skii (1975). In (Shen et al., 2022) the authors
showed that general target function classes (such as the Hölder or Lipschitz spaces) may be modeled by a
single network, of a certain architecture, such that only a small number of ‘intrinsic’ parameters are used to
approximate each individual function.
In a recent paper, Montanelli & Du (2021) provided approximation estimates for generalized bandlimited
functions. These functions are commonly used to model signals that have a finite range of frequencies (e.g.,
waves, video, and audio signals), which is known as a finite bandwidth. In (Montanelli & Du, 2021), it was
shown that any bandlimited function can be approximated in L2within error ϵusing a ReLU neural network
of depthO(log2(1/ϵ))withO(ϵ−2log2(1/ϵ))neurons.
In this paper, we compare the approximation capabilities of multiplicative neural network architectures
with those of standard ReLU networks with respect to the L2distance. In particular, we prove that a
multiplicative neural network of depth O(log(1
ϵ))withO(ϵ−2log(1
ϵ))neurons can approximate any generalized
bandlimited function up to an error of ϵ(with constants depending on the dimension and on the band). This
result represents an improvement compared to the findings in (Montanelli & Du, 2021) for traditional ReLU
networks. Additionally, we also study the approximation guarantees of neural networks for approximating
functions in Sobolev-Type balls of order r. We show that for the same error tolerance ϵ, multiplicative neural
networks can approximate these functions with depth O(dϵ−1/r)andO(dϵ−(2+1/r))neurons, while standard
ReLU neural networks require depth O(d2ϵ−2/r)andO(d2ϵ−(2+2/r))neurons. These results demonstrate the
superior performance of multiplicative gates compared to standard fully-connected layers. In Table 1 we
contrast our new bounds with preexisting bounds on the approximation power of neural networks for the
Sobolev space, bandlimited functions, and the Sobolev-Type ball.
Space Model # neurons Depth Reference
Wr,pC∞, non-polyO(ϵ−d/r)O(1) (Mhaskar, 1996)
Wr,∞ReLUO(ϵ−d/rlog1
ϵ)O(log1
ϵ) (Yarotsky, 2017)
Bandlimited functions ReLU O(ϵ−2log21
ϵ)O(log21
ϵ)(Montanelli & Du, 2021)
Bandlimited functions Multiplicative O(ϵ−2log1
ϵ)O(log1
ϵ) This paper
B2r,2⊊Wr,2ReLUO(d2ϵ−(2+2/r))O(d2ϵ−2/r) This paper
B2r,2⊊Wr,2Multiplicative O(dϵ−(2+1/r))O(dϵ−1/r) This paper
B1,1 Sigmodial O(dϵ−2)O(1) (Barron, 1993)
Table 1: Approximation results for Sobolev Wr,p, Bandlimited and B2r,2functions by ReLU and multiplicative
neural networks. The number of neurons and the depth are given in Onotation.
Our findings offer a deeper understanding of the application of deep learning techniques in solving partial
differential equations (PDEs) and signal processing. For instance, bandlimited functions are often prevalent
in signal processing applications, since the acquisition of data from sensors is frequently in the form of
bandlimited functions. In such applications, neural networks are used to approximate a given signal function.
Our results show that multiplicative neural networks are a preferable choice for approximating such functions.
In addition, recent papers suggest various approaches for solving PDEs with physics-informed neural
networks (Raissi et al., 2019; Karniadakis et al., 2021; Kahana et al., 2022; Ben-Shaul et al., 2023; Bar
& Sochen, 2021). In these methods, a deep network is trained to approximate a solution for a PDE by
minimizing a loss function associated with the differential equation. For various PDEs, such as the heat
equation, it was shown that if the initial condition at time t= 0exists in a specific smoothness space, such as
the Sobolev space Wr,p(refer to definition in Section 2.1), then the solution remains in that space for all
3Published in Transactions on Machine Learning Research (05/2023)
t>0. In other scenarios, if the initial condition lies in a Sobolev space Wr,p, the solution can be ensured
to be within a Sobolev space Ws,p, wheres≤r. Additionally, for elliptic boundary problems with smooth
coefficients over C2smooth domains, if a weak solution exists, it automatically belongs to Wr,2(Evans,
2010). Based on our findings, we recommend the use of physics-aware multiplicative networks as the preferred
architecture for such applications.
2 Problem Setup
We are interested in determining how complex (i.e., number of trainable parameters, number of neurons and
layers) a model ought to be in order to theoretically guarantee approximation of an unknown target function
fup to a given approximation error ϵ>0.
Formally, we consider a Banach space of functions V(for example, Lp([0,1]d)), equipped with a norm ∥·∥V
(for example,∥·∥Lp([0,1]d)), and a set of target functions U⊆V. We also consider a set of approximators H
and seek to quantify the ability of these approximators to approximate Uusing the following quantity
dV(H,U) = sup
f∈Uinf
ˆf∈H∥ˆf−f∥V,
which measures the maximal approximation error for approximating a target function f∈Uusing candidates
ˆffromH. Typically,His a parametric set of functions (e.g., neural networks of a certain architecture) and
we denote by ˆfθ∈Ha function that is parameterized by a vector of parameters θ∈RN. For simplicity, we
avoid writing θin the subscript when it is obvious from context.
2.1 Target Function Spaces
It is generally impossible to approximate arbitrary target functions using standard neural networks, as
demonstrated in Theorem 7.2 in (Devroye et al., 1996). As a result, we often consider specific spaces of target
functions that satisfy certain smoothness assumptions in order to obtain non-trivial results. In this work,
we focus specifically on target functions Uover the unit cube B= [0,1]dthat satisfy the following types of
smoothness assumptions.
Generalized Bandlimited functions. Bandlimited functions are functions whose spectrum, or the
set of frequencies that make up the function, is limited to a certain band or range of frequencies. This
property makes bandlimited functions well-suited for certain applications, such as signal processing, where it
is important to ensure that the signal does not contain frequencies outside of a certain range.
Generalized bandlimited functions are defined using a generalized analytic kernel K:R→C, that generalizes
the classic Fourier transform, and a band-limit M≥1. Formally, for a given function f:Rd→R, we denote
the set of all functions F: [−M,M ]d→Cthat retrieve ffrom the frequencies ω∈[−M,M ]dusing the
kernelKas follows:
Sf,K:=/braceleftig
F: [−M,M ]d→R|f(x) =/integraltext
[−M,M ]dF(ω)K(ω·x)dω/bracerightig
.
We defineFf=arg minF∈Sf,K∥F∥L2([−M,M ]d)to be the function in Sf,Kwith the smallest L2-norm. For
instance, when K(u) =exp(iu), the function Ffcorresponds to the normalized standard Fourier transform of
f, which is given by
F(ω) =1
(2π)d(Ff)(ω) =1
(2π)d/integraldisplay
Rdf(x) exp(−iω·x)dx.
The spaceHK,M(B)of generalized bandlimited functions is a Hilbert space of functions that can be represented
as a weighted sum of the function Kover a finite domain. This space is equipped with an inner product and
a norm, which allow us to measure the similarity and magnitude of these functions, respectively. We define
HK,M(B)as the functions f:B→Rsuch that
HK,M(B) :=/braceleftig
∀x∈B,f(x) =/integraltext
[−M,M ]dF(ω)K(ω·x)dω|F∈L2([−M,M ]d)/bracerightig
.
4Published in Transactions on Machine Learning Research (05/2023)
The inner product and norm in this space are defined as follows: ⟨f,g⟩HK,M(B)=/integraltext
[−M,M ]dFf(ω)Fg(ω)dω,
∥f∥HK,M(B)=∥Ff∥L2([−M,M ]d).
One of the key properties of generalized bandlimited functions is that they can be completely reconstructed
from a discrete set of samples. This is known as the Shannon-Nyquist theorem (Shannon, 1949), and it is
an important result in the field of signal processing and communication. An interesting consequence of this
theorem is that even in high-dimensions, where seemingly unpredictable geometrical phenomena may occur
(e.g. Blum et al. (2020), Chapter 2), a bandlimited function can still be perfectly reconstructed given its
values at the Nyquist frequency. For more details, see Appendix A.
Sobolevspaces. Sobolevspacesareoneofthemostextensivelystudiedclassesoffunctionsinapproximation
theory (DeVore & Lorentz, 1993; Yarotsky, 2017; Liang & Srikant, 2017). These spaces consist of functions
with bounded or p-integrable distributional derivatives up to a certain order. As already discussed in the
introduction, they are particularly useful in the study of PDEs.
We first define the Lpnorm, 1≤p <∞, of a given function f: Ω→Ras∥f∥Lp(Ω)= (/integraltext
Ω|f(x)|pdx)1/p,
where Ω⊂Rdis measurable. When p=∞, the essential supremum norm is used. A function fis said to be
inLp(Ω)if∥f∥Lp(Ω)<∞.
Letr∈Nand1≤p<∞. The Sobolev space Wr,p(B)consists of functions f:B→Rwithr-distributional
derivatives in Lp. The Sobolev norm ∥·∥Wr,p(B)is defined as
∥f∥Wr,p(B)=/summationdisplay
α:|α|1≤r∥Dαf∥Lp(B),
whereα= (α1,...,αd)∈{0,...,r}d,|α|1=α1+···+αd, andDαfis the respective distributional derivative.
We also present the semi-norm:
|f|r,p:=/summationdisplay
α:|α|=r∥Dαf∥Lp(B).
Classic results in the literature show that the number of parameters needed to approximate functions in
Wr,∞up to error ϵis lower bounded by Ω(ϵ−d/r)(DeVore et al., 1989). This exponential dependence on dis
known as the “curse of dimensionality”.
Sobolev-Type Balls. Sobolev-Type balls are typically subsets of Sobolev spaces with additional stronger
properties (Barron, 1993; Jones, 1992; Pinkus, 1985; Wahba, 1990; Blanchard & Bennouna, 2022). One
such property is that the magnitude of the function’s Fourier transform, |Ff(ω)|, decays fast enough as |ω|
approaches infinity. These constraints are imposed by comparing the magnitude of the Fourier transform of
the function,Ff(ω), with the magnitude of |ω|rfor some number r>0. In this paper we define a generalized
form of Sobolev-type balls:
Br,ρ=/braceleftbigg
f:Rd→R,f∈L2(Rd) :1
(2π)d/integraldisplay
Rd|(Ff)(ω)|dω≤1,1
(2π)d/integraldisplay
Rd|ω|r|(Ff)(ω)|ρdω≤1/bracerightbigg
.
In (Barron, 1993), they explored the ability of neural networks to approximate functions in the space
P1={f∈L2(Rd)|/integraltext
Rd|ω||(Ff)(ω)|<∞}. We now demonstrate that any function in P1has a normalized
representation in B1,1. For this, we show that the conditions on P1allow us to bound ∥Ff∥L1(Rd). Namely,
/integraldisplay
Rd|(Ff)(ω)|dω=/integraldisplay
[−1,1]d|(Ff)(ω)|dω+/integraldisplay
Rd\[−1,1]d|(Ff)(ω)|dω
≤C1/parenleftigg/integraldisplay
[−1,1]d|(Ff)(ω)|2dω/parenrightigg1/2
+/integraldisplay
Rd\[−1,1]d|ω||(Ff)(ω)|dω
≤C2∥f∥2+/integraldisplay
Rd|ω||(Ff)(ω)|dω.
5Published in Transactions on Machine Learning Research (05/2023)
Therefore, any function in the space P1can be scaled to a function in B1,1. Specifically, they showed that
sigmoidal neural networks with a bounded depth and O(dϵ−2)neurons can approximate such functions with
error at most ϵ.
In this paper we will focus on the space B2r,2. In Lemma 2 (Appendix D) we show that this space is embedded
as a proper subset of Wr,2, with the following norm:
∥f∥B2r,2=/parenleftbigg1
(2π)d/integraldisplay
Rd(1 +|ω|2r)|(Ff)(ω)|2dω/parenrightbigg1/2
. (1)
In Lemma 2 (Appendix D), we also show an example of why the Sobolev-type ball B2r,2is of particular
interest. Specifically, we show that the additional condition that Ff(ω)∈L1defines a subspace of Wr,2that
can be easily approximated using multiplicative networks. It is worth mentioning that Pinkus (1985) showed
that using traditional basis functions, functions of the space P2={f∈L2(Rd)|/integraltext
Rd|ω|2r|(Ff)(ω)|2<∞}
may be approximated with error at most ϵusingO(ϵ−d/2r)parameters. In this paper, we show that by
enforcing an additional constraint on the L1norm of the Fourier transform, we can circumvent exponential
dependence on the dimension d.
2.2 Neural Network Architectures
In the previous section, we described a setting in which a class of candidate functions Hserve as approximators
to a class of target functions U. In this work, we compare the approximation guarantees of standard multilayer
perceptrons and a generic set of neural networks that incorporate multiplication operations. Our goal is to
understand whether multiplication layers can provide better guarantees for generalized bandlimited functions
andB2r,2.
Multilayer perceptrons. A multilayer perceptron is a neural network architecture that consists of L
layers of affine linear transformations composed with element-wise non-linear activation functions (e.g., the
ReLU function). Typically, the last layer does not include a non-linear activation.
Definition 1 (Multilayer perceptron) .A multilayer perceptron f=yL,1:Rp0→Ris defined by a set of
functions/uniontextL
i=0{yi,j}pi
j=1. Each function yi,j:Rp0→R(also known as a neuron) is recursively computed in
the following manner
yL,j(x) =⟨wL,j,yL−1(x)⟩+bL,j
yi,j(x) =σ(⟨wi,j,yi−1(x)⟩+bi,j)
y0,j(x) =xj,
wherei∈[L−1],j∈[pi], andwi,j∈Rpi−1andbi,j∈Rare the weights and a bias of the neuron yi,jand
yi= (yi,1,...,yi,pi). The function σ:R→Ris a non-linear activation function.
In this work, we focus on neural networks with ReLU activations, which are defined as σ(x) =max(0,x).
However, it is worth noting that other activation functions have been proposed in the literature, such as
sigmoidal functions that are measurable functions ηthat satisfy η(x)→0asx→−∞andη(x)→1as
x→∞.
Multiplicative neural networks. In this work, we are interested in comparing the approximation abilities
of standard ReLU networks, with that of neural networks that incorporate multiplication layers (also known as
product units (Durbin & Rumelhart, 1989)). In order to fully understand the added benefits of multiplication
gates, we ask the following question: Are multiplications between neurons sufficient to substitute the non-linear
activations in multilayer perceptrons?
Definition 2 (Multiplicative network) .A multiplicative neural network is a function f=yL,1:Rp0→R
defined by a set of univariate functions/uniontextL
i=1{yi,j}pi
j=1. For each neuron, yi,j:Rp0→Rthere exists a pair of
indicesj1,j2∈{1,...,pi−1}(potentially dependent on i,j) such that yi,jis defined as follows
yi,j(x) =⟨wi,j,yi−1(x)⟩+ai,jyi−1,j1(x)yi−1,j2(x) +bi,j
y0,j(x) =xj,
whereai,j,bi,j∈R,wi,j∈Rpi−1are trainable parameters, and yi= (yi,1,...,yi,pi).
6Published in Transactions on Machine Learning Research (05/2023)
Multiplicative networks differ from multilayer perceptrons as they rely on (non-linear) multiplicative gates
between neurons instead of element-wise activation functions. Each neuron yi,jis very similar to a standard
neuron in a multilayer perceptron, with the distinction that a weighted multiplication between a single pair of
neuronsyi−1,j1,yi−1,j2is added to the output. The total number of trainable parameters in a multiplicative
layer ispi+1pi+ 2pi, which is comparable to the pi+1pi+piin a fully-connected layer. Following Yarotsky
(2017); Blanchard & Bennouna (2022) we measure the complexity of a neural network using its depth Land
its total number of neurons G=/summationtextL−1
i=1pi.
Self-Attention and multiplicative layers. Let us describe a single-headed self-attention operation in the
original Transformer (Vaswani et al., 2017). Each layer i∈[L]of a depth-LTransformer encoder is defined
as follows. The input to the ith layer is a sequence of Ntokens, denoted by xi={xi,j}N
j=1, where each
xi,j∈Rdxrepresents the jth token of the ith layer. To compute the output of the ith layer at a particular
positione∈[N], we use the following formula:
fi,e
SA(xi) =N/summationdisplay
j=1softmaxj/parenleftig
1√da⟨WQ,ixi,e,WK,ixi,e⟩/parenrightig
WV,ixi,j,
wheresoftmaxj(f(x)) = exp(f(x)j)//summationtext
j′exp(f(x)j′)is the softmax operator, and the trainable weight
matricesWK,i,WQ,i,WV,i∈Rda×dxconvert the representation from its dimension dxinto the attention
dimensionda=dx, creating ‘Key’, ‘Query’, and ‘Value’ representations, resp. As can be seen, the self-attention
layers use multiplicative connections when computing the following inner product ⟨WQ,ixi,e,WK,ixi,e⟩. This
operation computes multiplications between the coordinates of transformations of the same token xi,e. In
other words, it can be thought of as computing a multiplicative layer on an input vector xi,e. As a side note, in
addition to self-attention layers, transformers also incorporate commonly used layers such as fully-connected
layers, residual connections, and normalization layers, which are not the focus of this paper.
3 Representation Power of Multiplicative Neural Networks
In this section, we explore the expressive power of neural networks with multiplication layers. We first
demonstrate that these networks can easily represent polynomial functions, which we then use to approximate
bandlimited functions. This allows us to approximate functions in the space B2r,2without suffering from the
curse of dimensionality. Specifically, we prove the following lemma:
Lemma 1. For any polynomial pn:R→Rof degreenof the form pn(x) =/summationtextn
k=0ckxk, there exists a
multiplicative neural network fPOL
n :R3→R, of depthLn=O(n)withGn=O(n)neurons that satisfies
fPOL
n(x,x,c 0) =pn(x)forc0∈R, andx∈R.
Next, we may leverage the above lemma and proceed to show how multiplicative networks can be used to
approximate analytic functions For this purpose, we recall the notion of the Bernstein s-ellipse, which is a
geometric shape defined on the complex plane that is useful in approximation theory.
Definition 3. LetM > 0,s>1be two scalars. The Bernstein s-ellipse on [−M,M ]is defined as follows
EM
s=/braceleftig
x+iy∈C:x2
(aMs)2+y2
(bMs)2= 1/bracerightig
,
whose semi-major and semi-minor axes are aM
s=Ms+s−1
2andbM
s=Ms−s−1
2.
The parameter scontrols the shape of the ellipse. As sincreases, the ellipse converges to a circle. Before
stating our result in Theorem 2, we recall the following theorem of Trefethen (2019):
Theorem 1 (Theorem 8.2 of Trefethen (2019)) .LetM > 0,s> 2be scalars and K: [−M,M ]→Rbe an
analytic function that is analytically continuable to the ellipse EM
s, where it satisfies supx∈EMs|K(x)|≤CK
for some constant CK>0. For every n∈N, there exists a polynomial hn:R→Cof degreen, such that,
∥hn−K∥L∞([−M,M ])≤2CKs−n
s−1.
7Published in Transactions on Machine Learning Research (05/2023)
Theorem 1 states that any analytic function Kthat is bounded on the Bernstein s-ellipse EM
scan be
approximated by a polynomial of degree n, with the error decreasing exponentially as the degree nincreases.
As we show next, the use of multiplication layers in neural networks can improve the efficiency of function ap-
proximation in certain cases. The following theorem shows that deep multiplicative networks can approximate
real-valued analytic functions on bounded intervals.
Theorem 2. LetM≥1,s>2,CK>0andϵ∈(0,1)be scalars. Then, for any real-valued analytic function
K: [−M,M ]→Rthat is analytically continuable to the ellipse EM
swhere|K(x)|≤CK, there exists a
deep multiplicative network fMA: [−M,M ]3→R(MA stands for ‘Multiplicative Analytic’) of depth Lϵ=
O(1
log2slog2CK
ϵ)withGϵ=O(1
log2slog2CK
ϵ)neurons that satisfies ∥fMA(x,x,x )−K(x)∥L∞([−M,M ])≤ϵ.
Theorem 2 establishes that deep multiplicative networks with second-degree multiplications can approximate
any real-valued analytic function that is bounded on the Bernstein s-ellipse EM
swith error bounded by a
quantity that decreases exponentially with the depth of the network.
In (Montanelli & Du, 2021), the authors show that the kernel may be approximated using ReLU networks
with depth Lϵ=O(1
log2
2slog2
2CK
ϵ)withGϵ=O(1
log2
2slog2
2CK
ϵ)neurons. Thus, our approach achieves a
O(log(1/ϵ))improvement in depth and a O(log(1/ϵ))in the number of neurons.
Approximation of bandlimited functions. As a next step, we study the ability of neural networks to
approximate bandlimited functions. The following theorem shows that a deep multiplicative network can
approximate in B= [0,1]d, a bandlimited function up to a given error tolerance using a relatively small
number of neurons and depth.
Theorem 3. Letϵ∈(0,1),M > 1,d≥2, andK:R→Cbe an analytic kernel that holds the assumptions
of Theorem 2 with respect to s>2,CK>0, and bounded by a constant DK∈(0,1]on[−dM,dM ]. Letf
be a real-valued function in HK,M(B). Further, let F: [−M,M ]d→Cbe a square-integrable function such
thatf(x) =/integraltext
[−M,M ]dF(ω)K(ω·x)dω. We define CF=/integraltext
Rd|F(ω)|dω=/integraltext
[−M,M ]d|F(ω)|dω. Then, there
exists a deep multiplicative network fMBL:B→R(MBL stands for ‘Multiplicative bandlimited’) of depth
Lϵ=O/parenleftig
1
log2slog2CFCK
ϵ/parenrightig
withGϵ=O/parenleftig
C2
F
ϵ2log2slog2CFCK
ϵ/parenrightig
neurons that satisfies ∥fMBL−f∥L2(B)≤ϵ.
According to the above theorem, one can approximate bandlimited functions up to error ϵusing multiplicative
neural networks of depth Lϵ=O(log(1/ϵ))usingGϵ=O/parenleftbig
ϵ−2log(1/ϵ)/parenrightbig
neurons. In comparison, Montanelli
& Du (2021) showed that one can approximate bandlimited functions to the same level of approximation using
ReLU networks of depths Lϵ=O/parenleftbig
log2(1/ϵ)/parenrightbig
withGϵ=O/parenleftbig
ϵ−2log2(1/ϵ)/parenrightbig
neurons. This result demonstrates
the parameter efficiency of multiplicative networks in comparison with standard ReLU networks.
Approximation of Sobolev Functions. We now turn to show results for Sobolev-Type functions. We
use the results on bandlimited functions shown in Theorem 3 to approximate functions in B2r,2. We show
that using slightly stronger assumptions, we get an approximation rate comparable to those shown by Barron
(1993) (where the network is a shallow sigmoidal network model) using multiplicative networks (i.e. without
non-linear activations).
Theorem 4. Letd≥2,r∈N,f∈B 2r,2andϵ >0. There exists a deep ReLU network fRS(stand-
ing for “ReLU Sobolev”) with a depth of Lϵ=O(d2ϵ−2/r)andGϵ=O(d2ϵ−(2+2/r))neurons, such that/vextenddouble/vextenddoublefRS−f/vextenddouble/vextenddouble
L2(B)≤ϵ.
Theorem 5. Letd≥2,r∈N,f∈B 2r,2andϵ >0. There exists a deep ReLU network fMS(standing
for “Multiplicative Sobolev”) with a depth of Lϵ=O(dϵ−1/r)andGϵ=O(dϵ−(2+(1/r)))neurons, such that/vextenddouble/vextenddoublefMS−f/vextenddouble/vextenddouble
L2(B)≤ϵ.
In the following corollary, we demonstrate an application of Theorem 5 for approximating functions in the
Sobolev spaceWr,2with an integrable Fourier transform.
Corollary 1. Letd≥2,r∈N,f∈Wr,2such that∥Ff∥L1(Rd)<∞, andϵ>0. There exists a deep ReLU
networkfMS(standing for “Multiplicative Sobolev”) with a depth of Lϵ=O(ϵ−1/r)andGϵ=O(ϵ−(2+(1/r)))
neurons, such that/vextenddouble/vextenddoublefMS−f/vextenddouble/vextenddouble
L2(B)≤Cmax{|f|2
r,2,∥Ff∥1}ϵ, whereC > 0is a constant dependent on r,d.
8Published in Transactions on Machine Learning Research (05/2023)
Proof of Theorem 5. Letf∈B2r,2. We would like to approximate fusing a bandlimited function fMand
then approximate fMusing a multiplicative neural network fMS(standing for “Multiplicative Sobolev”). Let
M > 1be a band.
We recall the Inverse Fourier transform given by (F−1g)(x) =1
(2π)d/integraltext
Rdg(ω)exp(iω·x)dω. We define the
bandlimiting of f:Rd→RasfM=F−1(Ff 1[−M,M ]d), such that fM∈HK,M(B)forK(u) =exp(iu)and
F=1
(2π)dFf. We have
∥f−fM∥L2(B)≤/parenleftbigg
1
(2π)d/vextenddouble/vextenddoubleFf−Ff 1[−M,M ]d/vextenddouble/vextenddouble2
L2(Rd)/parenrightbigg1/2
=/parenleftbigg
1
(2π)d/integraldisplay
Rd\[−M,M ]d|Ff(ω)|2dω/parenrightbigg1/2
.
For anyω∈Rd\[−M,M ]d, we have|M−1ω|2r≥1. Therefore, since f∈B2r,2
/parenleftbigg
1
(2π)d/integraldisplay
Rd\[−M,M ]d|Ff(ω)|2dω/parenrightbigg1/2
≤/parenleftbigg
1
(2π)d/integraldisplay
Rd\[−M,M ]d/vextendsingle/vextendsingleM−1ω/vextendsingle/vextendsingle2r|Ff(ω)|2dω/parenrightbigg1/2
≤M−r/parenleftbigg
1
(2π)d/integraldisplay
Rd|ω|2r|Ff(ω)|2dω/parenrightbigg1/2
≤M−r.
For anyϵ>0, we setM= (2/ϵ)1/r. We will construct a neural network fMSto approximate the bandlimited
functionfM, such that∥fM−fMS∥L2(B)≤ϵ/2. Assuming we have constructed such fMS, then by the
triangle inequality we then arrive at
∥f−fMS∥L2(B)≤ ∥f−fM∥L2(B)+∥fM−fMS∥L2(B)≤M−r+ϵ/2≤ϵ.
Let us define the function F: [−M,M ]d→Cas followsF(ω) =1
(2π)d(Ff)(ω). We then have the following
identity
fM(x) =/integraldisplay
[−M,M ]dF(ω)K(ω·x)dω=/integraldisplay
[−M,M ]d1
(2π)dFf(ω) exp(iω·x)dω.
We may now work under the conditions of Theorem 3. We consider that CF=/integraltext
[−M,M ]d|F(ω)|dω=
1
(2π)d/integraltext
[−M,M ]d|Ff(ω)|dω≤1, wheretheinequalityisduetothedefinitionof B2r,2. Thekernel K(u) =exp(iu)
takes as input u=ω·x, forx∈Bandω∈[−M,M ]d. Therefore, u∈[−dM,dM ], andK: [−dM,dM ]→R.
SinceKis an entire function, it is continuable to EdM
sfor anys>2, let us choose s= 4. We notice that
adM
4:=dM(4 + 4−1)/2is the larger axis, and therefore the maximal norm of KonEdM
sis given by
CK≤exp(dM4+4−1
2).
Further, for any u∈Rwe have|K(u)|≤1 =DK. We now approximate fMwith a multiplicative network.
Using the results of Theorem 3, there exists a deep multiplicative neural network fMSthat approximates the
bandlimited function fMinL2(B)with error bounded by ϵ/2and depth
Lϵ≤C11
log24log2/parenleftig2CKCF
ϵ/parenrightig
≤C21
log24log2/parenleftigg
exp(dM4+4−1
2)
ϵ/parenrightigg
=C2
2/parenleftbigg
dM4 + 4−1
2 log(2)+ log2(1/ϵ)/parenrightbigg
≤C2
2(4dM+ log2(1/ϵ))
≤C2/parenleftbig
4dϵ−1/r+ log2(1/ϵ)/parenrightbig
≤C3·dϵ−1/r,(2)
9Published in Transactions on Machine Learning Research (05/2023)
(2, 3) (4, 6) (6, 9) (8, 12) (10, 15) (12, 18)00.10.20.30.4
Network
MLP
Mult
CapacityMSE
(2, 3) (4, 6) (6, 9) (8, 12) (10, 15) (12, 18)00.10.20.30.4
Network
MLP
Mult
CapacityMSE
T12 T15
Figure 1: Approximating Chebychev polynomials with networks of increasing complexity. We
present the minimal test mean squared error (MSE) loss for approximating Chebychev polynomials Tnusing
networks with increasing complexity, utilizing 10initialization seeds. The network complexity (x-axis) is
defined as (depth,width )and is increased linearly.
forsomeconstants C1,C2,C3>0. Thethirdstepfollowsfrom log2(xy) =log2(x)+log2(y)andloga(exp(x)) =
x/log(a)(for allx,y,a> 0) and the fifth inequality follows from substituting M= (2/ϵ)1/r. In addition, the
number of neurons can be bounded by
Gϵ≤C′
1C2
F·ϵ−2log22CKCF
ϵ
≤C′
2·ϵ−2log2exp(dM4+4−1
2)
ϵ
≤C′
2·ϵ−2(3d(2/ϵ)1/r+ log2(1/ϵ))
≤C′
3·dϵ−(2+1/r),(3)
for some constants C′
1,C′
2,C′
3>0.
Theorems 4-5 provide insights into several interesting properties of the Sobolev-Type ball B2r,2. First, we
see that approximating these functions does not suffer from the curse of dimensionality that occurs when
approximating the full Sobolev space. Secondly, for both ReLU networks and multiplicative networks, the
bound decreases as the smoothness index rincreases. In fact, as rapproaches infinity, the bound approaches
the one proposed in (Barron, 1993). Lastly, observe that for the same error tolerance ϵ, multiplicative
neural networks can approximate a target function f∈B2r,2with a depth ofO(dϵ−1/r)andO(dϵ−(2+1/r))
neurons, while standard ReLU neural networks require a depth of O(d2ϵ−2/r)andO(d2ϵ−(2+2/r))neurons.
This result demonstrates that multiplicative neural networks have stronger approximation guarantees when
approximating functions in the Sobolev-Type space.
4 Experiments
In previous sections, we demonstrated that multiplicative networks have lower complexity requirements for
approximating certain target functions when compared to multilayer perceptrons. To validate these results
empirically, we analyzed the influence of network complexity such as depth and width on the network’s
approximation error. To do so, we approximated Chebyshev polynomials of varying degrees Tnusing multilayer
perceptrons and multiplicative networks with increasing complexities (i.e., depth, width). As the degree
of the Chebyshev polynomials increased, we expected multilayer perceptrons to struggle in approximating
high-degree polynomials. Conversely, as proven in Lemma 1, Tncan be perfectly constructed using a
multiplicative network with depth Ln=nand widthGn= 3n.
10Published in Transactions on Machine Learning Research (05/2023)
Ground Truth
0 0.2 0.4 0.6 0.8 1−1−0.500.51
xy
0 0.2 0.4 0.6 0.8 1−1−0.500.51
xy
0 0.2 0.4 0.6 0.8 1−1−0.500.51
xy Approximation
0 20 40 60 8000.10.20.30.40.5
Width
MLP 10 test
Mult 10 test
EpochMSE
0 20 40 60 8000.10.20.30.40.50.6
Width
MLP 10 test
Mult 10 test
EpochMSE
0 20 40 60 8000.10.20.30.40.5
Width
MLP 10 test
Mult 10 test
EpochMSE
T9 T11 T13
Figure 2: Approximating Chebychev polynomials. We present the minimal test mean squared error
(MSE) loss for approximating Chebychev polynomials Tn, using networks of depth nand width 10, with 10
initialization seeds. The first row displays the actual polynomials, and the second row shows the approximation
errors using both multilayer perceptrons and multiplicative networks.
To validate Theorems 4 and 5, we approximated randomly generated Gaussian mixtures. Specifically, we
consider functions of the following form
f(x;µ,Σ) :=1
NN/summationdisplay
i=11
(2π)d/2|Σi|1/2exp/parenleftig
−1
2(x−µi)⊤Σ−1
i(x−µi)/parenrightig
,
where Σ∈RN×d×dandµ∈RN×d. We sample µiuniformly from [0,1]dandΣiuniformly from [0,0.4]d. It
is straightforward to show that Gaussian mixtures are in the Schwartz class (Schwartz, 1950). This implies
that they are inWr,2, and thatFf∈L1(Rd), which in turn means that they are also members of B2r,2up to
a constant factor (similar to the target functions considered in Corollary 1).
Estimating the approximation error of a model for a given target function is generally a challenging problem. In
the literature, this is usually done by training the model to fit the target function. However, the approximation
error may differ from the estimation due to optimization issues, such as sub-optimal initialization and
improperly tuned hyperparameters. To obtain a more precise estimation of the approximation error, we
trained each model using 10 different initialization seeds and selected the minimal test mean squared error
(MSE) loss of the model. This approach helped us avoid problems related to poor initialization.
In Figure 1, we approximated T12andT15using networks whose depth and width varied simultaneously. The
models were trained using the Adam optimizer with a learning rate of 0.001 for 100 epochs and a batch size of
64 to minimize their Mean Squared Error (MSE) (with respect to the target function). We used 1000 training
and 1000 test samples that were uniformly selected at random from [0,1]. As shown, multiplicative networks
achieved better results (lower errors) compared to multilayer perceptrons with the same depth and width.
In Figure 2, we approximated Tnof degreesn= 9,11,13using multilayer perceptrons and multiplicative
networks with depth nand width 10. We used the same training and testing processes as in the previous
experiment. As can be seen, multiplicative networks converged to a zero loss faster and provided better
approximation rates for the given polynomials. For the code for generating the experiments, see Appendix B.
We conducted experiments to approximate the mixtures of Gaussians using multilayer perceptrons and
multiplicative networks, while varying the dimensions and number of Gaussians ( dandN, respectively). All
11Published in Transactions on Machine Learning Research (05/2023)
d= 2Approx.
 d= 2Approx.
0 20 40 60 8033.544.5
Mult
MLP
EpochMSE
0 20 40 60 800.511.522.5
Mult
MLP
EpochMSE
0 20 40 60 800.70.80.911.11.21.31.4
Mult
MLP2
EpochMSE
N= 10 N= 25 N= 500N= 500 Approx.
0 20 40 60 800.70.80.911.11.21.31.4
Mult
MLP2
EpochMSE
0 20 40 60 800.30.40.50.60.70.80.91
Mult
MLP3
EpochMSE
0 20 40 60 801.61.71.81.922.12.2
Mult
MLP5
EpochMSE
d= 2 d= 3 d= 5
Figure 3: Approximating Gaussian mixtures with a varying number of Gaussians and dimensions.
We plot the test mean squared error (MSE) loss obtained by approximating Gaussian mixtures f(x;µ,Σ)
whereµ∈RN×dandΣ∈RN×d×dusing multiplicative and ReLU networks for different values of dandN.
We report their errors averaged over 20 initialization seeds along with their corresponding 95% confidence
intervals.
networks had a depth of 5 and a width of 10. We plotted the averaged approximation errors during training
over 20 initialization seeds, along with their corresponding 95% confidence intervals, while fixing the target
function. The training was performed with the same hyperparameters as in the previous experiments, except
for the learning rate, which was set to 10−4.
Our results, shown in Figure 3, indicate that multiplicative networks outperform multilayer perceptrons in
terms of approximation errors across various dimensions and numbers of Gaussians. Interestingly, we also
observed that training with multiplicative networks converges much faster than with multilayer perceptrons.
5 Conclusions
Previous papers have studied the approximation guarantees of standard fully-connected neural networks to
approximate functions in the Barron space B1,1(Barron, 1993), the space of bandlimited functions (Montanelli
& Du, 2021), and the Korobov space (Blanchard & Bennouna, 2022). These studies have shown that fully-
12Published in Transactions on Machine Learning Research (05/2023)
connected networks can approximate a wide range of smooth functions without suffering from the curse of
dimensionality and have provided insights into the tradeoffs between the width and depth of neural networks
in learning certain types of functions. However, these results are limited to variants of fully-connected
networks and do not provide information about other types of architectures.
In this paper, we extend these results by exploring the approximation guarantees of both multiplicative neural
networks and standard fully-connected networks to approximate bandlimited functions and members of the
Sobolev-Type ball B2r,2. Our results show that multiplicative neural networks achieve stronger approximation
guarantees compared to standard ReLU networks. In addition, we show that, unlike the Barron space and the
space of bandlimited functions, B2r,2is a subset of the Sobolev space Wr,2. Therefore, our results demonstrate
that it is possible to avoid the curse of dimensionality for wide subsets of the Sobolev space.
13Published in Transactions on Machine Learning Research (05/2023)
References
David W. Albrecht, Xuan Thinh Duong, and Alan Mcintosh. Operator theory and harmonic analysis. In
Instructional Workshop on Analysis and Geometry, Part III (Canberra, 1995) , volume 34, pp. 77–136. Proc.
Centre Math. Appl. Austral. Nat. Univ, 1996.
Kai Arulkumaran, Antoine Cully, and Julian Togelius. Alphastar: An evolutionary computation perspective.
InProceedings of the Genetic and Evolutionary Computation Conference Companion , GECCO ’19, pp.
314–315, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450367486. doi:
10.1145/3319619.3321894. URL https://doi.org/10.1145/3319619.3321894 .
Leah Bar and Nir Sochen. Strong solutions for pde-based tomography by unsupervised learning. SIAM
Journal on Imaging Sciences , 14(1):128–155, 2021. doi: 10.1137/20M1332827. URL https://doi.org/10.
1137/20M1332827 .
Andrew R. Barron. Approximation and estimation bounds for artificial neural networks. In Proceedings of
the Fourth Annual Workshop on Computational Learning Theory , COLT ’91, pp. 243–249, San Francisco,
CA, USA, 1991. Morgan Kaufmann Publishers Inc. ISBN 1558602135.
Andrew R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information Theory , 39(3):930–945, 1993. doi: 10.1109/18.256500.
Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation.
Neural Computation , 15(6):1373–1396, 2003. doi: 10.1162/089976603321780317.
Ido Ben-Shaul, Leah Bar, Dalia Fishelov, and Nir Sochen. Deep learning solution of the eigenvalue problem for
differential operators. Neural Computation , pp. 1–35, 2023. ISSN 0899-7667. doi: 10.1162/neco_a_01583.
URL https://doi.org/10.1162/neco_a_01583 .
Raphael Bensadoun, Shir Gur, Tomer Galanti, and Lior Wolf. Meta internal learning. In Proceedings of
the 34th International Conference on Advances in Neural Information Processing Systems , volume 34,
pp. 20645–20656. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/
file/ac796a52db3f16bbdb6557d3d89d1c5a-Paper.pdf .
Moise Blanchard and Mohammed Amine Bennouna. Shallow and deep networks are near-optimal approx-
imators of korobov functions. In International Conference on Learning Representations , 2022. URL
https://openreview.net/forum?id=AV8FPoMTTa .
Avrim Blum, John Hopcroft, and Ravindran Kannan. Foundations of Data Science . Cambridge University
Press, 2020. doi: 10.1017/9781108755528.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, PranavShyam, GirishSastry, AmandaAskell, SandhiniAgarwal, ArielHerbert-Voss, Gretchen
Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher
Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot
learners. In Proceedings of the 33rd International Conference on Advances in Neural Information Processing
Systems, volume 33, pp. 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,
Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,
Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel
Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin,
Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh
Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati,
Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech
Zaremba. Evaluating large language models trained on code. 2021.
14Published in Transactions on Machine Learning Research (05/2023)
Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao. Efficient approximation of deep
relu networks for functions on low dimensional manifolds. In Proceedings of the 32nd Inter-
national Conference on Advances in Neural Information Processing Systems , volume 32. Curran
Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/
fd95ec8df5dbeea25aa8e6c808bad583-Paper.pdf .
Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. In
Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pp. 551–561,
Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1053.
URL https://aclanthology.org/D16-1053 .
George V. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,
Signals and Systems , 2:303–314, 1989.
JacobDevlin, Ming-WeiChang, KentonLee, andKristinaToutanova. BERT:Pre-trainingofdeepbidirectional
transformers for language understanding. In Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long
and Short Papers) , pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational
Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423 .
Ronald A. DeVore and George G. Lorentz. Constructive Approximation . Grundlehren der mathematischen
Wissenschaften. Springer-Verlag, 1993. ISBN 9783540506270.
Ronald A. DeVore, Ralph Howard, and Charles A. Micchelli. Optimal nonlinear approximation. manuscripta
mathematica , 63:469–478, 1989. doi: https://doi.org/10.1007/BF01171759.
Luc Devroye, László Györfi, and Gábor Lugosi. A Probabilistic Theory of Pattern Recognition . Stochastic
Modelling and Applied Probability. Springer New York, NY, 1996. ISBN 978-0-387-94618-4. doi: https:
//doi.org/10.1007/978-1-4612-0711-5.
AlexeyDosovitskiy, LucasBeyer, AlexanderKolesnikov, DirkWeissenborn, XiaohuaZhai, ThomasUnterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference
on Learning Representations , 2021. URL https://openreview.net/forum?id=YicbFdNTTy .
Richard Durbin and David E. Rumelhart. Product units: A computationally powerful and biologically
plausible extension to backpropagation networks. Neural Comput. , 1(1):133–142, mar 1989. ISSN 0899-7667.
doi: 10.1162/neco.1989.1.1.133. URL https://doi.org/10.1162/neco.1989.1.1.133 .
Weinan E, Chao Ma, and Lei Wu. The barron space and the flow-induced function spaces for neural network
models.Constructive Approximation , 55:369–406, 2021. doi: https://doi.org/10.1007/s00365-021-09549-y.
Moshe Eliasof, Eldad Haber, and Eran Treister. Pde-gcn: Novel architectures for graph neural net-
works motivated by partial differential equations. In Proceedings of the 34th International Con-
ference on Advances in Neural Information Processing Systems , volume 34, pp. 3836–3849. Cur-
ran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/
1f9f9d8ff75205aa73ec83e543d8b571-Paper.pdf .
Lawrence C. Evans. Partial differential equations . American Mathematical Society, Providence, R.I., 2010.
ISBN 9780821849743 0821849743.
William Falcon et al. Pytorch lightning. GitHub. Note: https://github.com/PyTorchLightning/pytorch-
lightning , 2019.
Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks. Neural
Networks , 2(3):183–192, 1989. ISSN 0893-6080. doi: https://doi.org/10.1016/0893-6080(89)90003-8.
15Published in Transactions on Machine Learning Research (05/2023)
Tomer Galanti and Lior Wolf. On the modularity of hypernetworks. In Proceedings of the 33rd In-
ternational Conference on Neural Information Processing Systems , volume 33. Curran Associates Inc.,
2020. ISBN 9781713829546. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
75c58d36157505a600e0695ed0b3a22d-Paper.pdf .
David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. In International Conference on Learning
Representations , 2017. URL https://openreview.net/forum?id=rkpACe1lx .
Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverse Problems , 34(1),
2018. doi: 10.1088/1361-6420/aa9a90.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June
2016.
K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approximators.
Neural Networks , 2(5):359–366, jul 1989. ISSN 0893-6080.
Masaaki Imaizumi and Kenji Fukumizu. Deep neural networks learn non-smooth functions effectively. In
International Conference on Artificial Intelligence and Statistics , 2018.
Siddhant M. Jayakumar, Wojciech M. Czarnecki, Jacob Menick, Jonathan Schwarz, Jack Rae, Simon Osindero,
Yee Whye Teh, Tim Harley, and Razvan Pascanu. Multiplicative interactions and where to find them. In
International Conference on Learning Representations , 2020. URL https://openreview.net/forum?id=
rylnK6VtDH .
Lee K. Jones. A Simple Lemma on Greedy Approximation in Hilbert Space and Convergence Rates for
Projection Pursuit Regression and Neural Network Training. The Annals of Statistics , 20(1):608 – 613,
1992. doi: 10.1214/aos/1176348546. URL https://doi.org/10.1214/aos/1176348546 .
Adar Kahana, Eli Turkel, Shai Dekel, and Dan Givoli. A physically-informed deep-learning model using
time-reversal for locating a source from sparse and highly noisy sensors data. Journal of Computational
Physics, 470, 2022. ISSN 0021-9991. doi: 10.1016/j.jcp.2022.111592.
George Em Karniadakis, Ioannis G. Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang.
Physics-informed machine learning. Nature Reviews Physics , 3(6):422–440, 2021. ISSN 2522-5820. doi:
10.1038/s42254-021-00314-5.
Jason M. Klusowski and Andrew R. Barron. Risk bounds for high-dimensional ridge function combinations
including neural networks. arXiv, 2018.
Michael Kohler and Adam Krzyżak. Nonparametric regression based on hierarchical interaction models.
IEEE Transactions on Information Theory , 63(3):1620–1630, 2017. doi: 10.1109/TIT.2016.2634401.
Michael Kohler, Adam Krzyżak, and Sophie Langer. Estimation of a function of low local dimensionality by
deep neural networks. IEEE Transactions on Information Theory , 68(6):4032–4042, 2019. doi: 10.1109/
TIT.2022.3146620.
David Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner, Alexandre Lacoste, and Aaron Courville.
Bayesian hypernetworks. ArXiv, 2017.
Moshe Leshno, Vladimir Ya. Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward networks with
a non-polynomial activation function can approximate any function. Neural Networks , 6:861–867, 1993.
ISSN 0893-6080. doi: https://doi.org/10.1016/S0893-6080(05)80131-5.
Yoav Levine, Noam Wies, Or Sharir, Hofit Bata, and Amnon Shashua. Limits to depth efficiencies of self-
attention. In Proceedings of the 33rd International Conference on Advances in Neural Information Processing
Systems, volume 33, pp. 22640–22651. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
cc/paper/2020/file/ff4dfdf5904e920ce52b48c1cef97829-Paper.pdf .
16Published in Transactions on Machine Learning Research (05/2023)
Shiyu Liang and R. Srikant. Why deep neural networks for function approximation? In International
Conference on Learning Representations , 2017. URL https://openreview.net/forum?id=SkpSlKIel .
Etai Littwin, Tomer Galanti, Lior Wolf, and Greg Yang. On infinite-width hypernetworks.
InAdvances in Neural Information Processing Systems , volume 33, pp. 13226–13237. Curran
Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
999df4ce78b966de17aee1dc87111044-Paper.pdf .
Gidi Littwin and Lior Wolf. Deep meta functionals for shape representation. In Proceedings of the IEEE
International Conference on Computer Vision , pp. 1824–1833. Institute of Electrical and Electronics
Engineers Inc., 2019. doi: 10.1109/ICCV.2019.00191.
Hrushikesh N. Mhaskar. Neural networks for optimal approximation of smooth and analytic functions. Neural
Computation , 8(1):164–177, 1996. doi: 10.1162/neco.1996.8.1.164.
Hrushikesh N. Mhaskar. Eignets for function approximation on manifolds. Applied and Computational
Harmonic Analysis , 29(1):63–87, 2010. ISSN 1063-5203. doi: https://doi.org/10.1016/j.acha.2009.08.006.
Hrushikesh N. Mhaskar, Qianli Liao, and Tomaso Poggio. When and why are deep networks better
than shallow ones? Proceedings of the AAAI Conference on Artificial Intelligence , 31(1), 2017. doi:
10.1609/aaai.v31i1.10913. URL https://ojs.aaai.org/index.php/AAAI/article/view/10913 .
Hadrien Montanelli and Qiang Du. New error bounds for deep relu networks using sparse grids. SIAM
Journal on Mathematics of Data Science , 1(1):78–92, 2019. doi: 10.1137/18M1189336. URL https:
//doi.org/10.1137/18M1189336 .
Haizhao Montanelli, Hadrien Yang and Qiang Du. Deep relu networks overcome the curse of dimensionality
for generalized bandlimited functions. Journal of Computational Mathematics , 39(6):801–815, 2021. ISSN
1991-7139. doi: https://doi.org/10.4208/jcm.2007-m2019-0239. URL http://global-sci.org/intro/
article_detail/jcm/19912.html .
Ryumei Nakada and Masaaki Imaizumi. Adaptive approximation and generalization of deep neural network
with intrinsic dimensionality. Journal of Machine Learning Research , 21(1), 2022. ISSN 1532-4435.
S. M. Nikol’skii. Approximation of functions of several variables and imbedding theorems . Die Grundlehren
der mathematischen Wissenschaften in Einzeldarstellungen mit besonderer Berücksichtigung der Anwen-
dungsgebiete ; Bd. 205. Springer-Verlag, Berlin, 1975. ISBN 0387064427.
Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for
natural language inference. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing , pp. 2249–2255. Association for Computational Linguistics, 2016. doi: 10.18653/v1/D16-1244.
URL https://aclanthology.org/D16-1244 .
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai,
and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Pro-
ceedings of the 32nd International Conference on Advances in Neural Information Processing Systems ,
volume 32, pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf .
Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive sum-
marization. In International Conference on Learning Representations . OpenReview.net, 2018. URL
https://openreview.net/forum?id=HkAClQgA- .
Philipp Christian Petersen and Felix Voigtländer. Optimal approximation of piecewise smooth functions
using deep relu neural networks. Neural Networks , 108:296–330, 2017. doi: https://doi.org/10.1016/j.
neunet.2018.08.019.
17Published in Transactions on Machine Learning Research (05/2023)
Allan Pinkus. N-Widths in Approximation Theory . Ergebnisse der Mathematik und ihrer Grenzgebiete.
3. Folge / A Series of Modern Surveys in Mathematics. Springer Berlin, Heidelberg, 1985. doi: https:
//doi.org/10.1007/978-3-642-69894-1.
Gilles Pisier. Remarks on an unpublished result of B. Maurey, 1980-1981. URL http://www.numdam.org/
item/SAF_1980-1981____A5_0/ .
Tomaso Poggio, Andrzej Banburski, and Qianli Liao. Theoretical issues in deep networks. Proceedings
of the National Academy of Sciences , 117(48):30039–30045, 2020. doi: 10.1073/pnas.1907369117. URL
https://www.pnas.org/doi/abs/10.1073/pnas.1907369117 .
M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: A deep learning framework
for solving forward and inverse problems involving nonlinear partial differential equations. Journal of
Computational Physics , 378:686–707, 2019. ISSN 0021-9991. doi: https://doi.org/10.1016/j.jcp.2018.10.045.
URL https://www.sciencedirect.com/science/article/pii/S0021999118307125 .
John J. F. Fournier Robert A. Adams. Sobolev spaces . Pure and Applied Mathematics. Academic Press,
2 edition, 2003. ISBN 9780120441433,9781435608108,0120441438. URL http://gen.lib.rus.ec/book/
index.php?md5=72db1ed30d242bd3ebcb73bcd099a6b4 .
Johannes Schmidt-Hieber. Deep relu network approximation of functions on a manifold. arXiv, 2019.
Laurent Schwartz. Théorie des noyaux. In Proceedings of the International Congress of Mathematicians ,
volume 1, pp. 220–230, 1950. URL https://www.mathunion.org/fileadmin/IMU/ICM/Proceedings/
ICM1950.1/ICM1950.1.220.0.pdf .
Claude E. Shannon. Communication in the presence of noise. Proceedings of the IRE , 37(1):10–21, 1949. doi:
10.1109/JRPROC.1949.232969.
Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network approximation in terms of intrinsic parameters.
InProceedings of the 39th International Conference on Machine Learning , volume 162 of Proceedings of
Machine Learning Research , pp. 19909–19934. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.
press/v162/shen22g.html .
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian
Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe,
John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu,
Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search.
Nature, 529:484–489, 2016. ISSN 0028-0836. doi: 10.1038/nature16961.
Taiji Suzuki. Adaptivity of deep relu network for learning in besov and mixed smooth besov spaces: optimal
rate and curse of dimensionality. ArXiv, 2018.
Taiji Suzuki and Atsushi Nitanda. Deep learning is adaptive to intrinsic dimensionality of model smoothness
in anisotropic besov space. ArXiv, 2019.
Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. A global geometric framework for nonlinear
dimensionality reduction. Science, 290(5500):2319–2323, 2000. doi: 10.1126/science.290.5500.2319. URL
https://www.science.org/doi/abs/10.1126/science.290.5500.2319 .
Andrew Trask, Felix Hill, Scott Reed, Jack Rae, Chris Dyer, and Phil Blunsom. Neural arithmetic logic units.
InProceedings of the 31st International Conference on Neural Information Processing Systems , volume 31,
pp. 8046–8055. Curran Associates Inc., 2018. URL https://proceedings.neurips.cc/paper_files/
paper/2018/file/0e64a7b00c83e3d22ce6b3acf2c582b6-Paper.pdf .
Lloyd N. Trefethen. Approximation Theory and Approximation Practice, Extended Edition . Society for
Industrial and Applied Mathematics, Philadelphia, PA, 2019. doi: 10.1137/1.9781611975949. URL
https://epubs.siam.org/doi/abs/10.1137/1.9781611975949 .
18Published in Transactions on Machine Learning Research (05/2023)
Sebastian Urban and Patrick van der Smagt. A neural transfer function for a smooth and differentiable
transition between additive and multiplicative interactions. ArXiv, 2016.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 30th
International Conference on Advances in Neural Information Processing Systems , volume 30. Cur-
ran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .
Gideon Schechtman Vitali D. Milman. The Maurey-Pisier Theorem , pp. 85–97. Springer Berlin Heidelberg,
1986. ISBN 978-3-540-38822-7. doi: 10.1007/978-3-540-38822-7_13. URL https://doi.org/10.1007/
978-3-540-38822-7_13 .
Johannes von Oswald, Christian Henning, João Sacramento, and Benjamin F. Grewe. Continual learning
with hypernetworks. In International Conference on Learning Representations , 2020. URL https:
//openreview.net/forum?id=SJgwNerKvB .
Grace Wahba. Spline Models for Observational Data . Society for Industrial and Applied Mathematics, 1990.
doi: 10.1137/1.9781611970128. URL https://epubs.siam.org/doi/abs/10.1137/1.9781611970128 .
Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. Pay less attention with lightweight
and dynamic convolutions. In International Conference on Learning Representations , 2019. URL https:
//openreview.net/forum?id=SkVhlh09tX .
DmitryYarotsky. Errorboundsforapproximationswithdeeprelunetworks. Neural networks , 94:103–114, 2017.
ISSN 0893-6080. doi: https://doi.org/10.1016/j.neunet.2017.07.002. URL https://www.sciencedirect.
com/science/article/pii/S0893608017301545 .
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 12104–12113,
2022.
19Published in Transactions on Machine Learning Research (05/2023)
A Examples
Example 1. Letf∈L2(Rd)such thatfis Bandlimited function with band π(i.e., supp(Ff)⊂[−π,π]d). Let
ϕ(x) =sinc(x) =sin(πx)/πx, andϕk=ϕ(·−k)for allk∈Zd. By the Shannon-Nyquist theorem (Shannon,
1949), we have:
f(x) =/summationdisplay
k∈Zd⟨f,ϕk⟩ϕk(x) =/summationdisplay
k∈Zdf(k)ϕ(x−k).
This means that every Bandlimited function can be completely determined using a discrete set of integer
samples. This result is particularly surprising for high-dimensional functions ( dis large) since the maximal
distance between a point xand a sampling point grows with d. For example, the vertex of the unit cube in Rd
is of distance√
d/2away from its center. Despite this, we can still recover samples from the integer vertices,
even when the distances scale as√
d. This property is useful when recovering high-dimensional functions
using neural networks. See Figure 4 for illustration.
1
2
√
2
2√
3
2
Figure 4: Illustration of Example 1. A bandlimited function fcan be reconstructed using a discrete set of
values, despite the fact that the distances in each cube grow as O(√
d).
B Code
In this section, we provide the code used to generate the experiments. The network architectures are described
in Section 2.2. The “chebychev(n, x)” function calculates the Chebyshev polynomial of degree nfor a given
inputx. The “MLPPolynomialApproximation” and “MultPolynomialApproximation” classes are Pytorch
Lightning (Falcon et al., 2019) implementations for the multilayer perceptrons and multiplicative networks,
respectively. The “MultiplicativeLayer” class is based on the standard linear layer in PyTorch (Paszke et al.,
2019). Since the neurons in each layer are interchangeable, we arbitrarily connected the first two neurons in
each layer using a multiplicative operation. In each network, the layers are followed by ReLU activations.
from torch.nn.parameter import Parameter, UninitializedParameter
from torch.nn import init
from torch.nn import functional as F
import torch
from torch import Tensor
import torch.nn as nn
import torch.optim as optim
import pytorch_lightning as pl
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm
import numpy as np
from sklearn.model_selection import train_test_split
def chebychev(n, x):
coefs = [0 for _ in range(n)] + [1]
20Published in Transactions on Machine Learning Research (05/2023)
return np.polynomial.chebyshev.Chebyshev(coefs)(x)
class MultiplicativeLayer(nn.Module):
__constants__ = [‘in_features’, ‘out_features’]
in_features: int
out_features: int
weight: Tensor
def __init__(self, in_features: int=3, out_features: int=3, bias: bool = True,
device=None, dtype=None) -> None:
factory_kwargs = {‘device’: device, ‘dtype’: dtype}
super(AttentionLayer, self).__init__()
self.in_features = in_features
self.out_features = out_features
self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs)
)
self.bias = Parameter(torch.empty(out_features, **factory_kwargs))
self.mult_weight = Parameter(torch.empty(1, **factory_kwargs))
self.reset_parameters()
def reset_parameters(self) -> None:
init.kaiming_uniform_(self.weight, a=math.sqrt(5))
if self.bias is not None:
fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)
bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
init.uniform_(self.bias, -bound, bound)
init.uniform_(self.mult_weight, -bound, bound)
def forward(self, x: Tensor) -> Tensor:
bs, _ = x.shape
x = F.linear(x, self.weight, self.bias)
mult_output = torch.prod(x.gather(1, torch.tensor([0, 1]).repeat(bs, 1)), dim=1)
mask = torch.zeros_like(x)
mask[:, 1] = self.mult_weight*mult_output
x = x + mask
return x
class MultPolynomialApproximation(pl.LightningModule):
def __init__(self, width=3, depth=3, poly_deg=3, use_relu=False):
super().__init__()
self.width = width
self.depth = depth
self.poly_deg = poly_deg
self.layers = nn.ModuleList()
self.layers.append(nn.Linear(1, width))
self.train_loss = {}
self.val_loss = {}
for i in range(self.depth):
self.layers.append(AttentionLayer(width, width))
self.output = nn.Linear(width, 1)
self.use_relu = use_relu
def forward(self, x):
21Published in Transactions on Machine Learning Research (05/2023)
for layer in self.layers:
x = layer(x)
if self.use_relu:
x = torch.relu(x)
return self.output(x)
def training_step(self, batch, batch_idx):
x, y = batch
y_hat = self.forward(x)
loss = torch.mean((y_hat - y) ** 2)
self.log("train_loss", loss)
return {"loss": loss}
def training_epoch_end(self, outputs) -> None:
loss = sum(output[’loss’] for output in outputs) / len(outputs)
self.train_loss[self.current_epoch] = loss.item()
def validation_step(self, batch, batch_idx):
x, y = batch
y_hat = self.forward(x)
loss = torch.mean((y_hat - y) ** 2)
self.log("val_loss", loss, prog_bar=True)
return {"val_loss": loss}
def validation_epoch_end(self, outputs) -> None:
avg_loss = torch.stack([x["val_loss"] for x in outputs]).mean()
self.val_loss[self.current_epoch] = avg_loss.item()
def validation_end(self, outputs):
avg_loss = torch.stack([x["val_loss"] for x in outputs]).mean()
self.log("val_loss", avg_loss)
self.val_loss[self.current_epoch] = avg_loss.item()
return {"val_loss": avg_loss}
def prepare_data(self, val_split=0.2):
N = 1000
X = torch.rand(N, 1)
Y = chebychev(self.poly_deg, X[:, 0]).view(-1, 1)
X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=val_split)
self.train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)
self.val_dataset = torch.utils.data.TensorDataset(X_val, Y_val)
def train_dataloader(self):
return torch.utils.data.DataLoader(self.train_dataset, batch_size=64, shuffle=True
)
def val_dataloader(self):
return torch.utils.data.DataLoader(self.val_dataset, batch_size=32, shuffle=False)
def configure_optimizers(self):
optimizer = optim.Adam(self.parameters(), lr=1e-3)
return optimizer
22Published in Transactions on Machine Learning Research (05/2023)
class MLPPolynomialApproximation(pl.LightningModule):
def __init__(self, width=5, depth=3, poly_deg=3):
super().__init__()
self.width = width
self.depth = depth
self.poly_deg = poly_deg
self.layers = nn.ModuleList()
self.layers.append(nn.Linear(1, width))
self.train_loss = {}
self.val_loss = {}
for i in range(self.depth):
self.layers.append(nn.Linear(width, width))
self.output = nn.Linear(width, 1)
def forward(self, x):
for layer in self.layers:
x = torch.relu(layer(x))
return self.output(x)
def training_step(self, batch, batch_idx):
x, y = batch
y_hat = self.forward(x)
loss = torch.mean((y_hat - y) ** 2)
self.log("train_loss", loss)
return {"loss": loss}
def training_epoch_end(self, outputs) -> None:
loss = sum(output[’loss’] for output in outputs) / len(outputs)
self.train_loss[self.current_epoch] = loss.item()
def validation_step(self, batch, batch_idx):
x, y = batch
y_hat = self.forward(x)
loss = torch.mean((y_hat - y) ** 2)
self.log("val_loss", loss, prog_bar=True)
return {"val_loss": loss}
def validation_epoch_end(self, outputs) -> None:
avg_loss = torch.stack([x["val_loss"] for x in outputs]).mean()
self.val_loss[self.current_epoch] = avg_loss.item()
def validation_end(self, outputs):
avg_loss = torch.stack([x["val_loss"] for x in outputs]).mean()
self.log("val_loss", avg_loss)
self.val_loss[self.current_epoch] = avg_loss.item()
return {"val_loss": avg_loss}
def prepare_data(self, val_split=0.2):
N = 1000
X = torch.rand(N, 1)
Y = chebychev(self.poly_deg, X[:, 0]).view(-1, 1)
X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=val_split)
self.train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)
self.val_dataset = torch.utils.data.TensorDataset(X_val, Y_val)
23Published in Transactions on Machine Learning Research (05/2023)
N= 500 Approx.
0 20 40 60 801.61.71.81.922.12.2
Mult
MLP5
EpochMSE
0 20 40 60 8000.010.020.030.040.05
Mult
MLP
EpochMSE
0 20 40 60 8000.020.040.060.08
Mult
MLP
EpochMSE
D= 5 D= 20 D= 30
Figure 5:Approximating Gaussian mixtures with a varying number of Gaussians and dimensions.
We plot the test mean squared error (MSE) loss obtained by approximating Gaussian mixtures f(x;µ,Σ)
whereµ∈RN×dandΣ∈RN×d×dusing multiplicative and ReLU networks for different values of dandN.
We report their errors averaged over 20 initialization seeds along with their corresponding 95% confidence
intervals.
def train_dataloader(self):
return torch.utils.data.DataLoader(self.train_dataset, batch_size=64, shuffle=True
)
def val_dataloader(self):
return torch.utils.data.DataLoader(self.val_dataset, batch_size=32, shuffle=False)
def configure_optimizers(self):
optimizer = optim.Adam(self.parameters(), lr=1e-3)
return optimizer
C Additional Experiments
In this section, we add plots regarding approximation in higher dimensions d, shown in Figure 5. We
conducted the exact same experiment as in the bottom row of Figure 3 with N= 500andd= 5,20,30. As
can be seen, both architectures are able to achieve near 0error, yet we still see faster convergence using
multiplicative networks over multilayer perceptrons.
D Proofs
Definition 4. We say a subset Uof a Banach space V1is embedded in a Banach space V2if∥u∥V2≤C∥u∥V1
for some fixed constant Cand for any u∈U.
Lemma 2. Letr>0andd≥2. Then,B2r,2is embedded as a proper subset of Wr,2.
Proof.Sincef∈B2r,2, we havef∈L2(Rd), and therefore,∥f∥2<∞. Additionally, by Robert A. Adams
(2003)(Theorem 5.2), there exists a constant C1>0such that
∥f∥Wr,2≤C1(∥f∥2+|f|r,2),
whereC1is dependant on rand the dimension d. Therefore, it remains to prove that |f|r,2is bounded. For
this, we use some properties of multivariate function spaces.
24Published in Transactions on Machine Learning Research (05/2023)
We begin with the following claim that we will use in advance. Let ω∈Rd,d≥2, we have
|ω|2r=∥ω∥2r
l2=/parenleftiggd/summationdisplay
m=1ω2
m/parenrightiggr
=/summationdisplay
∥α∥1=r/parenleftbiggr
α1,...,αd/parenrightbigg
·(ωα)2, (4)
whereωα=/producttextd
i=1ωαi
iforα∈Rd. By Jensen’s inequality,
|f|2
r,2=
/summationdisplay
α:|α|=r∥Dαf∥2
2
≤dr/summationdisplay
α:|α|=r∥Dαf∥2
2
By Parseval’s Identity in Rd(Albrecht et al., 1996):
∥Dαf∥2
2=1
(2π)d/integraldisplay
Rd|F(Dαf)(ω)|2dω=1
(2π)d/integraldisplay
Rd|(Ff)(ω)|2|ωα|2dω.
Hence, by (4)
/summationdisplay
α:|α|=r∥Dαf∥2
2=/summationdisplay
α:|α|=r1
(2π)d/integraldisplay
Rd|(Ff)(ω)|2|ωα|2dω
≤1
(2π)d/integraldisplay
Rd|(Ff)(ω)|2
/summationdisplay
α:|α|=r/parenleftbiggr
α1,...,αd/parenrightbigg
·|ωα|2
dω
=1
(2π)d/integraldisplay
Rd|Ff(ω)|2|ω|2rdω.
where the inequality follows from the fact that/parenleftbigr
α1,...,αd/parenrightbig
≥1. Finally, we conclude that
∥f∥Wr,2≤C1(∥f∥2+|f|r,2)≤C1/parenleftig
∥f∥2+dr
(2π)d/2/parenleftbig/integraltext
Rd|(Ff)(ω)|2|ω|2rdω/parenrightbig1/2/parenrightig
≤C2∥f∥B2r,2,
for someC1,C2>0, where the second inequality is due to the definition of ∥·∥B2r,2as given in equation 1.
To see thatB2r,2is a proper subset of Wr,2, we present a prototype function f∈Wr,2/parenleftbig
Rd/parenrightbig
, such that
f /∈B2r,r. Letϵ>0andd,rsuch that 2r+ϵ<d.We define the function f∈Wr,2/parenleftbig
Rd/parenrightbig
through its Fourier
transform:
Ff(ω) =/braceleftigg
1|ω|≤1
|ω|−(r+(d+ϵ)/2)|ω|>1.
Indeed,f∈Wr,2/parenleftbig
Rd/parenrightbig
since:
/integraldisplay
Rd|ω|2r|Ff(ω)|2dω=/integraldisplay
|ω|≤1|ω|2r|Ff(ω)|2dω+/integraldisplay
|ω|>1|ω|2r|Ff(ω)|2dω
≤1 +/integraldisplay
|ω|>1|ω|−(d+ϵ)dω<∞.
We have shown above that |f|2
r,2≤dr1
(2π)d/integraltext
Rd|Ff(ω)|2|ω|2rdω, and sof∈Wr,2/parenleftbig
Rd/parenrightbig
. Let us see that
Ff(ω)/∈L1. Since 2r+ϵ<d⇒r+ (d+ϵ)/2<d, the following integral diverges
/integraldisplay
Rd|Ff(ω)|dω=/integraldisplay
|ω|<1|Ff(ω)|dω+/integraldisplay
|ω|>1|Ff(ω)|dω
=/integraldisplay
|ω|<1|Ff(ω)|dω+/integraldisplay
|ω|>1|ω|−(r+(d+ϵ)/2)dω
≤/integraldisplay
|ω|<1|Ff(ω)|dω+/integraldisplay
|ω|>1|ω|−ddω=∞.
25Published in Transactions on Machine Learning Research (05/2023)
x
x
c0x
x2
c0+c1xx
x3
p2,2(x)(wi,1)1= 1
ai,2= 1
ai,2= 1
(wi,3)2=ci
(wi,3)3= 1(wi,1)1= 1
ai,2= 1
ai,2= 1
(wi,3)2=ci
(wi,3)3= 1Input
layerHidden
layerOutput
layer
Figure 6: Illustration of the multiplicative network in Proof of Lemma 1, where the polynomial degree n= 2.
This implies that fdoes not satisfy the condition1
(2π)d/integraltext
Rd|Ff(ω)|dω≤1which is a necessary requirement
for membership in B2r,2.
Lemma 1. For any polynomial pn:R→Rof degreenof the form pn(x) =/summationtextn
k=0ckxk, there exists a
multiplicative neural network fPOL
n :R3→R, of depthLn=O(n)withGn=O(n)neurons that satisfies
fPOL
n(x,x,c 0) =pn(x)forc0∈R, andx∈R.
Proof.Letpn(x) =/summationtextn
k=0ckxkbe a polynomial of degree nandpn,i=/summationtexti
k=0ckxkits partial sum up to term
i. We construct a network fPOL
nwhoseith layer satisfies:
(fPOL
n)i(x,x,c 0) = (x,xi+1,pn,i(x)).
We notice that in this construction, the input of the network is a three-dimensional vector composed of
the inputxand the first coefficient c0. We construct the network by specifying its weights as defined in
Definition 2. At layer i, the model weights for the three neurons are defined as:
wi,1= (1,0,0), ai,2= 1for(j1,j2) = (1,2), wi,3= (0,ci,1)
The rest of the weights are zeros. In Figure 6, we plot the proposed architecture.
We argue that at any layer i≥0, neuron 1 contains x, neuron 2 contains xi+1and neuron 3 contains pn,i(x).
Letyi+1,1,yi+1,2,yi+1,3be the three neurons in layer i+ 1. Let us address each claim individually. Since
the only non-zero weight affecting the first neuron is in wi+1,1,yi+1,1=yi,1=xby the assumption. The
only non-zero weight affecting the second neuron appears in ai=1,2, the weight affecting the multiplicative
interaction, and therefore yi+1,2=yi,1yi,2=xi+1. Lastly,yi+1,3depends only on wi+1,3, and soyi+1,3=
ci+1yi,2+yi,3=pn,i(x) +ci+1xi+1:=pn,i+1(x). We conclude using the fact that pn,n(x) =pn(x). The third
neuron in the final layer then holds the value pn(x).
Theorem 2. LetM≥1,s>2,CK>0andϵ∈(0,1)be scalars. Then, for any real-valued analytic function
K: [−M,M ]→Rthat is analytically continuable to the ellipse EM
swhere|K(x)|≤CK, there exists a
deep multiplicative network fMA: [−M,M ]3→R(MA stands for ‘Multiplicative Analytic’) of depth Lϵ=
O(1
log2slog2CK
ϵ)withGϵ=O(1
log2slog2CK
ϵ)neurons that satisfies ∥fMA(x,x,x )−K(x)∥L∞([−M,M ])≤ϵ.
Proof.LetM≥1,s>2,CK>0,ϵ∈(0,1)and letKbe a real-valued analytic function with the required
assumptions with respect to s,CK. We wish to construct a network that approximates K. As a first step,
we approximate Kwith a polynomial hnof degreen≥2(to be defined later). Then, we realize hnwith a
deep multiplicative network as proposed in Lemma 1. Since Ksatisfies the conditions of Theorem 1, for any
integern≥2, there exists a polynomial hnof degreen, such that
∥hn−K∥L∞([−M,M ])≤CKs−n
s−1.
26Published in Transactions on Machine Learning Research (05/2023)
For the given ϵ>0, withs>2, we choose and estimate nas follows
n:=1
log2slog2CK
ϵ(s−1)≤1
log2slog2CK
ϵ.
This provides the following relation:
ϵ(s−1)
CK= 2log2(ϵ(s−1)
CK)
= 2−1
log2s(log2(CK
ϵ(s−1))) log2s
=s−1
log2s(log2(CK
ϵ(s−1)))=s−n.(5)
In particular,
∥hn−K∥L∞([−M,M ])≤CKs−n
s−1≤ϵ.
Given that hnis a polynomial of degree n, by Lemma 1, there exists fPOLandc0=hn,0such that
hn(x) =fPOL(x,x,c 0), and achieves the required bound.
Let us now recall Maurey’s Theorem (Pisier, 1980-1981; Vitali D. Milman, 1986) which will assist us in the
proof of Theorem 3.
Lemma 3 (Maurey’s theorem) .LetVbe a Hilbert space with norm ∥·∥V. Suppose there exists Q⊂V
such that for every q∈Q,∥q∥V≤bfor someb>0. Then, for every fin the convex hull of Qand every
integern≥1, there exists a fnin the convex hull of npoints inQand a constant c>b2−∥f∥2
Vsuch that
∥fn−f∥2
V≤c
n.
Theorem 3. Letϵ∈(0,1),M > 1,d≥2, andK:R→Cbe an analytic kernel that holds the assumptions
of Theorem 2 with respect to s>2,CK>0, and bounded by a constant DK∈(0,1]on[−dM,dM ]. Letf
be a real-valued function in HK,M(B). Further, let F: [−M,M ]d→Cbe a square-integrable function such
thatf(x) =/integraltext
[−M,M ]dF(ω)K(ω·x)dω. We define CF=/integraltext
Rd|F(ω)|dω=/integraltext
[−M,M ]d|F(ω)|dω. Then, there
exists a deep multiplicative network fMBL:B→R(MBL stands for ‘Multiplicative bandlimited’) of depth
Lϵ=O/parenleftig
1
log2slog2CFCK
ϵ/parenrightig
withGϵ=O/parenleftig
C2
F
ϵ2log2slog2CFCK
ϵ/parenrightig
neurons that satisfies ∥fMBL−f∥L2(B)≤ϵ.
Proof.Letf∈HK,M(B). Further, let F(ω) =|F(ω)|·exp(iθ(ω)), the polar representation of F(ω). We may
writefthrough its Fourier representation:
f(x) =Re/parenleftigg/integraldisplay
[−M,M ]dF(ω)K(ω·x)dω/parenrightigg
=Re/parenleftigg/integraldisplay
[−M,M ]dCFexp(iθ(ω))K(ω·x)|F(ω)|
CFdω/parenrightigg
=/integraldisplay
[−M,M ]dCF[cos(θ(ω))KR(ω·x)−sin(θ(ω))KI(ω·x)]|F(ω)|
CFdω,(6)
whereKR,KIare the real and imaginary parts of Krespectively. Since/integraltext
[−M,M ]d|F(ω)|
CFdω= 1, equation 6
representsfas an infinite convex combination of functions in
QK,M=/braceleftig
γ[cos(β)KR(ω·x)−sin(β)KI(ω·x)]/vextendsingle/vextendsingle/vextendsingle|γ|≤CF,β∈R,ω∈[−M,M ]d/bracerightig
.
This means that fis in the closure of the convex hull of QK,M. Due to the fact that x∈[0,1]dand
ω∈[−M,M ]d,ω·x∈[−dM,dM ]. By the definition of DK, functions in QK,Mare bounded in the L2(B)-
norm by 2CFDK≤2CF. Using Lemma 3, for any 0<ϵ0<1, to be defined at a later time, there exist real
27Published in Transactions on Machine Learning Research (05/2023)
coefficients{bj}and{βj}, and vectors ωj∈[−M,M ]d,1≤j≤⌈1/ϵ2
0⌉, such that:
fϵ0(x) =⌈1/ϵ2
0⌉/summationdisplay
j=1bj[cos(βj)KR(ωj·x)−sin(βj)KI(ωj·x)],⌈1/ϵ2
0⌉/summationdisplay
j=1|bj|≤CF,
and
∥fϵ0−f∥L2(B)≤2CFϵ0.
We are now ready to approximate fϵ0(x)using a deep multiplicative neural network fMBL(MBL stands
for ‘Multiplicative bandlimited’) on B. We notice that KRandKIare analytic kernels that satisfy the
assumptions of Theorem 2, for CK,s,M,ϵ 0. They can therefore be approximated to ϵ0error using networks
fRMA,fIMAof depth and number of neurons
Gϵ0∼Lϵ0=O(1
log2slog2CK
ϵ0),
where RMA stands for ‘Real Multiplicative Analytic’ and IMA stands for ‘Imaginary Multiplicative Analytic’.
Let us define the multiplicative network fMBL(x)by
fMBL(x) =⌈1/ϵ2
0⌉/summationdisplay
j=1bj/bracketleftbig
cos(βj)fRMA(ωj·x)−sin(βj)fIMA(ωj·x)/bracketrightbig
.
This network has depth Lϵ0=O(1
log2slog2CK
ϵ0)andGϵ0=O(1
ϵ2
0log2slog2CK
ϵ0)neurons.
∥fMBL(x)−fϵ0(x)∥L∞(B)≤⌈1/ϵ2
0⌉/summationdisplay
j=1|bj|∥fRMA(ωj·x)−KR(ωj·x)∥L∞(B)
+⌈1/ϵ2
0⌉/summationdisplay
j=1|bj|∥fIMA(ωj·x)−KI(ωj·x)∥L∞(B)
≤2CFϵ0,
that implies
∥fMBL(x)−f(x)∥L2(B)≤∥fMBL(x)−fϵ0(x)∥L2(B)+∥fϵ0(x)−f(x)∥L2(B)≤4CFϵ0,
where the last inequality is due to the fact that
∥g∥L2(B)=/parenleftbigg/integraldisplay
B|g|2/parenrightbigg1/2
≤∥g∥L∞(B).
Takingϵ0=ϵ/(4CF)achieves the sought result.
We further investigate how the constant CKfrom Theorem 3 varies as a function of Mands. Let
K(u) =exp(iu)be an example kernel, u∈[−dM,dM ]. We notice that adM
s=dM(s+s−1)/2fors>2, is
the larger axis, and therefore the maximal norm of KonEdM
sis given by
CK(s,dM ) = exp(dMs+s−1
2).
The resulting network fMBLthen has depth
Lϵ=O/parenleftbigg1
log2s/parenleftbigg
dMs+s−1
2+ log2CF
ϵ/parenrightbigg/parenrightbigg
28Published in Transactions on Machine Learning Research (05/2023)
and
Gϵ=O/parenleftbiggC2
F
ϵ2log2s/parenleftbigg
dMs+s−1
2+ log2CF
ϵ/parenrightbigg/parenrightbigg
neurons. In this scenario, we see that both a large band Mand a large dimension dwill linearly affect the
first term.
Theorem 4. Letd≥2,r∈N,f∈B 2r,2andϵ >0. There exists a deep ReLU network fRS(stand-
ing for “ReLU Sobolev”) with a depth of Lϵ=O(d2ϵ−2/r)andGϵ=O(d2ϵ−(2+2/r))neurons, such that/vextenddouble/vextenddoublefRS−f/vextenddouble/vextenddouble
L2(B)≤ϵ.
Proof.Letf∈B2r,2. Similar to the proof of Theorem 5, we define the bandlimiting of f:Rd→Rfor given
M > 1by
fM=F−1(Ff 1[−M,M ]d).
Let us define F: [−M,M ]d→C:
F(ω) =1
(2π)d(Ff)(ω),
andK(u) = exp(iu). We then have the following identity:
fM(x) =/integraldisplay
[−M,M ]dF(ω)K(ω·x)dω=/integraldisplay
[−M,M ]d1
(2π)dFf(ω) exp(iω·x)dω.
It is easy to see that fM∈HK,M(B). We choose M:= (2/ϵ)1/r. Using the same derivations as in the proof
of Theorem 5, we seek to approximate fMwith a network fRS(“ReLU Sobolev”) such that
∥fM−fRS∥L2(B)≤ϵ/2.
Once this is achieved, we have:
∥f−fRS∥L2(B)≤ ∥f−fM∥L2(B)+∥fM−fRS∥L2(B)
≤M−r+ϵ/2≤ϵ.(7)
We consider that CF=/integraltext
[−M,M ]d|F(ω)|dω=1
(2π)d/integraltext
[−M,M ]d|Ff(ω)|dω≤1, where the inequality is due to
the definition ofB2r,2. The kernel K(u) =exp(iu)takes as input u=ω·x, forx∈Bandω∈[−M,M ]d.
Therefore,u∈[−dM,dM ], andK: [−dM,dM ]→R. SinceKis an entire function, it is continuable to EdM
s
for anys>2, let us choose s= 4. We notice that adM
4=dM(4 + 4−1)/2is the larger axis, and therefore the
maximal norm of KonEdM
sis given by:
CK= exp(dM4+4−1
2).
Further, for any u∈Rwe have|K(u)|≤1 =DK. Using Theorem 3.2 from (Montanelli & Du, 2021) we can
construct a deep ReLU network fRSsuch that
∥fM−fRS∥L2(B)≤ϵ/2
whose depth is
Lϵ≤C11
log2
24log2
22CFCK
ϵ
≤C21
log2
24/parenleftigg
log2exp(dM4+4−1
2)
ϵ/parenrightigg2
≤12C2/parenleftigg
d/parenleftbigg2
ϵ/parenrightbigg1
r
−log21
ϵ/parenrightigg2
≤C3d2ϵ−2
r,(8)
29Published in Transactions on Machine Learning Research (05/2023)
for some constants C1,C2,C3>0. In addition, the number of neurons can be bounded by
Gϵ≤C′
1C2
F1
ϵ2log2
24log2
22CKCF
ϵ
≤C′
21
ϵ2log2
24/parenleftigg
log2exp(dM4+4−1
2)
ϵ/parenrightigg2
≤12C′
2
ϵ2/parenleftigg
d/parenleftbigg2
ϵ/parenrightbigg1
r
−log21
ϵ/parenrightigg2
≤C′
3d2ϵ−(2+2
r),(9)
for some constants C′
1,C′
2,C′
3>0.
Corollary 1. Letd≥2,r∈N,f∈Wr,2such that∥Ff∥L1(Rd)<∞, andϵ>0. There exists a deep ReLU
networkfMS(standing for “Multiplicative Sobolev”) with a depth of Lϵ=O(ϵ−1/r)andGϵ=O(ϵ−(2+(1/r)))
neurons, such that/vextenddouble/vextenddoublefMS−f/vextenddouble/vextenddouble
L2(B)≤Cmax{|f|2
r,2,∥Ff∥1}ϵ, whereC > 0is a constant dependent on r,d.
Proof.Letf∈Wr,2such thatFf∈L1(Rd), and letϵ>0. We define a function
˜f:=f
drmax{|f|2
r,2.∥Ff∥1}
We will show that ˜f∈B2r,2and then by applying Theorem 5 on ˜fwe will obtain the desired result.
By the definition of the Sobolev space, f∈L2(Rd)and therefore ˜f∈L2(Rd). We considerF˜f,
1
(2π)d/integraldisplay
Rd|F˜f(ω)|dω=1
(2π)ddrmax{|f|2
r,2,∥Ff∥1}/integraldisplay
Rd|Ff(ω)|dω
=∥Ff∥1
(2π)ddrmax{|f|2
r,2,∥Ff∥1}≤1.
We would like to show that1
(2π)d/integraltext
Rd|ω|2r|F˜f(ω)|2dω≤1. Letω∈Rd,d≥2, we have
|ω|2r=∥ω∥2r
l2=/parenleftiggd/summationdisplay
m=1ω2
m/parenrightiggr
=/summationdisplay
∥α∥1=r/parenleftbiggr
α1,...,αd/parenrightbigg
·(ωα)2, (10)
whereωα=/producttextd
i=1ωαi
iforα∈Rd. In addition,
|f|2
r,2=
/summationdisplay
α:|α|=r∥Dαf∥2
2
≥/summationdisplay
α:|α|=r∥Dαf∥2
2
By Parseval’s Identity in Rd(Albrecht et al., 1996):
∥Dαf∥2
2=1
(2π)d/integraldisplay
Rd|F(Dαf)(ω)|2dω=1
(2π)d/integraldisplay
Rd|(Ff)(ω)|2|ωα|2dω.
Hence, by (10)
/summationdisplay
α:|α|=r∥Dαf∥2
2=/summationdisplay
α:|α|=r1
(2π)d/integraldisplay
Rd|(Ff)(ω)|2|ωα|2dω
≥1
(2π)d/summationtext
α:|α|=r/parenleftbigr
α1,...,αd/parenrightbig/integraldisplay
Rd|(Ff)(ω)|2
/summationdisplay
α:|α|=r/parenleftbiggr
α1,...,αd/parenrightbigg
·|ωα|2
dω
=1
(2π)ddr/integraldisplay
Rd|Ff(ω)|2|ω|2rdω,
30Published in Transactions on Machine Learning Research (05/2023)
We then arrive at the following inequality:
1
(2π)ddr/integraldisplay
Rd|ω|2r|Ff(ω)|2dω≤|f|2
r,2.
Finally,
1
(2π)d/integraldisplay
Rd|ω|2r|F˜f(ω)|2dω=1
(2π)ddrmax{|f|2
r,2,∥Ff∥1}/integraldisplay
Rd|ω|2r|Ff(ω)|2dω
≤|f|2
r,2
max{|f|2
r,2,∥Ff∥1}≤1.
31