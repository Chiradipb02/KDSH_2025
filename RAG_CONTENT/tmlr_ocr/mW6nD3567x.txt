Published in Transactions on Machine Learning Research (10/2022)
From Optimization Dynamics to Generalization Bounds via
Łojasiewicz Gradient Inequality
Fusheng Liu fusheng@u.nus.edu
Institute of Data Science
National University of Singapore
Haizhao Yang hzyang@umd.edu
Department of Mathematics
University of Maryland College Park
Soufiane Hayou hayou@nus.edu.sg
Department of Mathematics
National University of Singapore
Qianxiao Li qianxiao@nus.edu.sg
Department of Mathematics
National University of Singapore
Reviewed on OpenReview: https: // openreview. net/ forum? id= mW6nD3567x
Abstract
Optimization and generalization are two essential aspects of statistical machine learning. In
this paper, we propose a framework to connect optimization with generalization by analyz-
ing the generalization error based on the optimization trajectory under the gradient flow
algorithm. The key ingredient of this framework is the Uniform-LGI, a property that is
generally satisfied when training machine learning models. Leveraging the Uniform-LGI,
we first derive convergence rates for gradient flow algorithm, then we give generalization
bounds for a large class of machine learning models. We further apply our framework to
three distinct machine learning models: linear regression, kernel regression, and two-layer
neural networks. Through our approach, we obtain generalization estimates that match or
extend previous results.
1 Introduction
From the perspective of statistical learning theory, the goal of machine learning is to find a predictive
function that can give accurate predictions on new data. For supervised learning problems, empirical risk
minimization (ERM) is a common practice to achieve this goal. The idea of ERM is to minimize a cost
function on observed data using an optimization algorithm. Therefore, a fundamental question is: given a
training algorithm, does it produce a solution with good generalization ? This question has been the subject
of a substantial body of literature, which answers this question in terms of the implicit bias of optimization
methods such as stochastic gradient descent (SGD). For example, one line of works considered the case where
gradient methods converge to minimal norm solutions on kernel regression (Bartlett et al., 2020; Tsigler &
Bartlett, 2020; Liang & Rakhlin, 2020; Liang et al., 2020), and then analyzed the generalization properties of
thoseminimalnormsolutionsbybias–variancetradeoff. AnotherlineofworksfocusedontheNeuralTangent
Kernel (NTK) regime (Allen-Zhu et al., 2019; Arora et al., 2019; Cao & Gu, 2020; Ji & Telgarsky, 2020; Chen
et al., 2021) where SGD iterates converges to a global minimum with a short distance from initialization.
They suggested to use norm-based measures to theoretically derive generalization bounds. Specifically,
these papers studied the generalization of (deep) neural networks on a norm-constrained parameter space
1Published in Transactions on Machine Learning Research (10/2022)
W={W:W∈B(0,R)}, whereWis the collection of weight matrices for all layers, and B(0,R)is a ball
with radius R. On the NTK regime, SGD with random initialization is proved to be able to find a global
minimum in the parameter space B(0,R)withR=/radicalig
y⊤/parenleftbig
Θ(L)/parenrightbig−1y, whereyis the label and Θ(L)is the
NTK matrix defined on the training input data (Arora et al., 2019; Cao & Gu, 2020). All of these works have
madesignificantprogressontheinterplayofoptimizationandgeneralization. However, thesettingsthatthey
focused on are specific in the sense that (1) the phenomenon of norm minimization has only been proven
to occur with the quadratic loss with an appropriate initialization scheme; (2) Rhas been theoretically
obtained only in the NTK regime; (3) these works has considered the models after convergence and the
generalization analysis of models during training are not completely understood. Therefore, the connection
between optimization and generalization still remains incomplete understandings in general scenarios.
In this paper, we aim to tackle these issues by studying the connection between optimization and generaliza-
tion of a wide class of machine learning models. Thus, a more precise analysis of how the parameter evolves
when the training time varies is needed. For this purpose, inspired by the Łojasiewicz gradient inequality
(LGI) condition in Bolte et al. (2007), which states that the gradient norm is lower bounded by some power
of the value function, we consider a uniform version of LGI that extends this condition to a non-vanishingly
small set. Specifically, we propose the Uniform-LGI (Definition 2.1), which is a modified version of the LGI.
This assumption plays a critical role in connecting optimization and generalization through norm-based gen-
eralization bounds. The first section is dedicated to the numerical validation of the Uniform-LGI on different
machine learning models. By introducing the localUniform-LGI condition along the optimization path, we
derive convergence rates and generalization bounds that yield bias-variance tradeoffs during training. Our
framework can be applied to a broad class of machine learning models to obtain optimization results and
generalization estimates.
Contributions. Our contributions are three-fold:
•First, we design a finite sample test algorithm to verify the Uniform-LGI condition along the training
path. Our numerical results suggest the Uniform-LGI condition is generally satisfied when training
machine learning models and is more general compared to the Polyak-Łojasiewicz (PL) condition
(Polyak, 1963). Then, we propose a framework for connecting optimization dynamics and general-
ization performance based on the Uniform-LGI condition.
•Specifically, we first analyze the convergence rate for the loss functions that satisfy the Uniform-
LGI condition (Theorem 2.2) under the gradient flow algorithm. Through Rademacher complexity
theory, we derive a generalization bound (Theorem 2.3) for a wide class of hypothesis spaces that
holds during the training process. The generalization bound exhibits bias–variance tradeoff pattern.
•We illustrate different use cases of our framework, showing how we obtain generalization estimates
for a linear regression problem (Theorem 3.3), kernel regression (Theorem 3.5), and two-layer neural
networks (Theorem 3.8 for shallow networks & Theorem E.8 for overparameterized case). These
bounds are derived in a unified way, and either match existing results derived for individual cases or
expand upon the scenarios where we can rigorously establish the phenomenon of benign overfitting.
2 Main Results
In this section, we present our main results. We first introduce the notations and the problem setting in
Section 2.1. We then in investigate the key ingredient, the Uniform-LGI condition of our framework in
Section 2.2. Lastly, we derive optimization results and generalization bounds for the gradient flow trajectory
in Section 2.3 under the Uniform-LGI condition.
2.1 Setup and Notations
Consider a hypothesis space F=/braceleftbig
f(w,·) :Rd− →R|w∈W/bracerightbig
, whereWis a parameter set in Euclidean
space. Given a loss function ℓ:R×R− →R, and a training set S={(xi,yi)}n
i=1⊆Rd×Rwithn
2Published in Transactions on Machine Learning Research (10/2022)
independent and identically distributed (i.i.d.) samples from a joint distribution D, the goal of ERM is to
optimize the empirical loss function Ln(w)onS:
min
wLn(w) :=1
nn/summationdisplay
i=1ℓ(f(w,xi),yi). (1)
Notations. We use∥·∥to denote the ℓ2norm of a vector or the spectral norm of a matrix, and use ∥·∥F
to denote the Frobenius norm of a matrix. For two vectors, we use ⟨,⟩to denote their inner product. For a
symmetric matrix A, we useλmin(A), resp.λmax(A)to denote the smallest, resp. the largest, eigenvalue of
A. For any non-negative integer n, let[0 :n] ={0,1,...,n}. We useO(·)to denote the Big-O bound.
2.2 Investigation on Uniform-LGI
Now we introduce the key component of our framework: the Uniform-LGI, a condition that holds for a
wide class of loss functions. We give several examples of loss functions that satisfy the Uniform-LGI. Our
numerical results suggest that the Uniform-LGI is generally satisfied when training neural network models.
The classical LGI gives a lower bound on the gradient of a differentiable function based on its value above
its minimum. Many functions, e.g., real analytic functions and subanalytic functions, satisfy this property,
at least locally (Bolte et al., 2007). Here, we require a uniform version of this inequality as a condition to
control the optimization trajectory. Let us define this notion below.
Definition 2.1 (Uniform-LGI) .A loss functionL(w)satisfies Uniform-LGI on a set Swith constants
θ∈[1/2,1)andc>0, if
∥∇L(w)∥≥c/parenleftbigg
L(w)−min
v∈SL(v)/parenrightbiggθ
,∀w∈S. (2)
The Uniform-LGI is a more general condition than the well-known PL-condition (Karimi et al., 2016) in
the sense that 1) The µ-PL condition is a special case for the Uniform-LGI when c=√2µ,θ= 1/2. The
Uniform-LGI contains a wider class of functions than the PL-condition. For instance, consider L(w) =w2k,
this function satisfies the PL-condition only when k= 1. Whenk≥2, it satisfies the general Uniform-LGI
withθ= 1−1/2k. Figures 1(a), 1(b), Figure 2(a), 2(b), 2(c) show that many nontrivial setups do not
haveθ= 1/2in practice. Besides, the well-known PL-condition in (Karimi et al., 2016) is a defined in a
neighborhood of a globalminimum, indicating that every stationary point is a global minimum, while the
Uniform-LGI is more general that is defined in an arbitrary set, where there may exist no global minimum.
This yields that the Uniform-LGI covers more scenarios than the PL-condition when analyzing optimization
properties as it is difficult for an optimization algorithm to find a global minimum in practice.
Note that this is an inequality, so the pair of (c,θ)such that the Uniform-LGI holds is not unique. Usually
the smallest θis defined as the Uniform-LGI exponent. The classical LGI states that for any subanalytic
function (Bolte et al., 2007) including a.e. differentiable models with squared loss, cross-entropy loss and
hinge loss, the exponent θis strictly less than 1. In the following, we give some examples of loss functions
satisfying the Uniform-LGI globally over their entire domains or locally along the optimization path.
Global Uniform-LGI. Let the loss function L(wL,...,w 1) = (wL···w1)2, which can be viewed as a
one-dimensional L-layer linear neural network model with squared loss on the data (1,0). Then∥∇L∥2=
4(wL···w1)4/summationtextL
i=11/w2
i≥4L(wL···w1)4−2/L= 4L(L)2−1/L. Therefore,Lsatisfies Uniform-LGI condition
globally on RLwithc= 2√
Landθ= 1−1/2LforL≥1.
Apart from the global Uniform-LGI, there exists a large number of loss functions satisfying the Uniform-
LGI locally within a given range. Since we want to study the optimization and generalization of a training
algorithm, next we investigate the Uniform-LGI condition along the optimization path during training.
(Local) Uniform-LGI along the optimization path. Given an initialization w(0), a gradient-based
optimization algorithm A(e.g. GD, SGD, etc.) produces a series of parameters w(0),w(1),w(2),...,w(k)for
the firstksteps. IfAconverges,kcan be infinite and the limiting parameter w(∞)is a stationary point
ofL(w). To numerically verify the Uniform-LGI condition (find θ,c) along the entire optimization path
3Published in Transactions on Machine Learning Research (10/2022)
/braceleftbig
w(i)/bracerightbig∞
i=0, i.e., findc,θsuch that log∥∇L(w)∥≥logc+θlog (L(w)−minv∈SL(v))holds onS={w(i)}∞
i=1,
we design an algorithm for finite sample test. First, for the first kcollected data points, we consider to use
linear regression to fit these kdata points and get the slope θk, then we adjust cto catch the worst case,
i.e., setck= mini∈[0:k−1]/vextenddouble/vextenddouble∇L(w(i))/vextenddouble/vextenddouble//parenleftbig
L(w(i))−L(w∗)/parenrightbigθk. Then the obtained ck,θkare the constants such
that the Uniform-LGI holds for the first kdata points. While numerical experiments can never provean
inequality, it is useful in estimating the constants by looking at the limiting values as the number of points
increases. This is called finite-size scaling analysis (Privman, 1990) and is frequently used in fields such as
statistical physics to investigate numerically the behaviour of infinite-size systems (e.g. phase transitions).
Algorithm 1 Finite sample test for Uniform-LGI
Input:lossfunctionL(w); acollectionofparameters w(0),w(1),w(2),...,w(K); theoptimallossvalue L(w∗);
start point K0; steps
1:fork= 0toKdo
2:calculate the gradient norm/vextenddouble/vextenddouble∇L(w(k))/vextenddouble/vextenddouble
3:end for
4:fork=K0toKstepsdo
5:collect thekdata/braceleftbig/parenleftbig
log/parenleftbig
L(w(i))−L(w∗)/parenrightbig
,log/vextenddouble/vextenddouble∇L(w(i))/vextenddouble/vextenddouble/parenrightbig/bracerightbigk−1
i=0
6:fit the data by linear regression, and return the slope θ
7:θk←θ
8:ck←mini∈[0:k−1]∥∇L(w(i))∥
(L(w(i))−L(w∗))θk
9:end for
10:fit the data/braceleftbig/parenleftbig
log/parenleftbig
L(w(i))−L(w∗)/parenrightbig
,log/vextenddouble/vextenddouble∇L(w(i))/vextenddouble/vextenddouble/parenrightbig/bracerightbigk
i=0by linear regression, and return the slope θ
11:θK+1←θ
12:cK+1←mini∈[0:K]∥∇L(w(i))∥
(L(w(i))−L(w∗))θK+1
Output: the estimated Uniform-LGI constants (θK0,cK0),(θK0+s,cK0+s),..., (θK+1,cK+1)
Notethatthepairof (c,θ)suchthattheUniform-LGIholdsisnotunique, andtheoutput (θK+1,cK+1)isone
possible pair of constants such that the Uniform-LGI holds on the first Kcollected data points. Intuitively,
if Algorithm 1 outputs a series of estimated Uniform-LGI constants that converge to some (θ∗,c∗), then
we have good evidence to support the assumption that L(w)satisfies equation (2) along the optimization
path/braceleftbig
w(i)/bracerightbig∞
i=0withθ∗,c∗. Moreover, if θ∈[1/2,1)andc >0, thenL(w)satisfies the Uniform-LGI on/braceleftbig
w(i)/bracerightbig∞
i=0. In the following, we present numerical experiments that confirms the validity of the Uniform-LGI
along GD/SGD paths on synthetic models and neural network models under Algorithm 1.
Syntheticmodels. WeconsidertrainingthreesyntheticmodelsbyGDwithoptimallossvalues L(w∗) = 0,
for which we can provably estimate the Uniform-LGI constants: (a) differentiable but non-analytic1loss
functionL(w) =e−1
|w|forw̸= 0,0forw= 0. For the non-analytic model, by the classical LGI property
we know that there exists no θandcsuch that the Uniform-LGI holds around the minimum w= 0, which
is consistent with Figure 1(a); (b) differentiable loss function L(w) =1
4w4
3+1
2w2. For this loss function,
we have|∇L(w)|
L(w)θ∼O (w1−4θ
3). Therefore, the Uniform-LGI constant θshould be at least 1/4, which is
consistent with the result in Figure 1(b) that the estimated θ= 0.308; (c) undetermined linear regression
modelL(w) =1
2n/summationtextn
i=1(w⊤xi−yi)2on a synthetic dataset. We know that the squared loss is always strongly
convex, thus it satisfies the PL-condition ( θ= 0.5), which is consistent with Figure 1(c) that the estimated
θ≈0.5. More details about the experiments are provided in Appendix A.
WereporttheestimatedUniform-LGIconstants (θK0,cK0),(θK0+s,cK0+s),..., (θK+1,cK+1)byfinitesample
test (Algorithm 1), and denote by (θ∗,c∗)the final estimation (θK+1,cK+1). Figure 1 shows three different
cases: (a) (θ,c)donotconverge; (b) (θ,c)convergewith θ∗<1/2andc∗>0; (c)(θ,c)convergewith θ∗≈0.5.
This demonstrates that our method is robust in determining whether a model satisfies the Uniform-LGI.
1ArealfunctionissaidtobeanalyticifitpossessesderivativesofallordersandagreeswithitsTaylorseriesinaneighborhood
of every point.
4Published in Transactions on Machine Learning Research (10/2022)
(a)
 (b)
 (c)
Figure 1: Investigation of the Uniform-LGI on synthetic models by finite sample test (Algorithm 1). For the
non-analytic model (a), both θandcdo not converge. For the synthetic model (b), cconverges to a positive
number but θconverges to a number that is <1/2. For the linear regression model (c), the Uniform-LGI
holds along the training path with θ∗≈0.5as expected since linear regression satisfies the PL condition.
(a)
 (b)
 (c)
Figure 2: Investigation of the Uniform-LGI condition on neural network models by finite sample test
(Algorithm 1). For all of these models, θandcconverges to θ∗∈[1/2,1)andc∗>0respectively. Hence,
these two models satisfy the Uniform-LGI condition along the optimization path with constants θ∗andc∗.
Neural network models. We train three neural network models: two-layer multilayer perceptron (MLP)
with width 100(no bias) on the MNIST dataset (LeCun et al., 1998); ResNet18 (He et al., 2016), Wide-
ReseNet-16-8 (Zagoruyko & Komodakis, 2016) on the CIFAR10 dataset (Krizhevsky et al., 2009). For each
experiment, we train the network using SGD (no momentum) with random shuffling, batch size 64and
fixed learning rate 0.01. For the MLP model, we stop the training with 1000epochs and set the optimal
loss value to be 0. For the ResNet models, we stop the training once the cross-entropy loss is less than
0.001and estimate the local optimal loss value as mink∈[0:K]L(w(k)), which is smaller than 10−3. To avoid
division by zero, we delete the parameter w∗inw(0),w(1),w(2),...,w(K). Then we apply the finite sample
test (Algorithm 1) with starting point K0= 50, steps= 1to obtain the estimated Uniform-LGI constants.
We report (θK0,cK0),(θK0+s,cK0+s),..., (θK+1,cK+1), and denote (θ∗,c∗)by the average of the last 5
iterates, i.e., θ∗= (θK−3+···+θK+1)/5,c∗= (cK−3+···+cK+1)/5. As shown in Figure 2, the estimated
(θ,c)converge for both models, and the Uniform-LGI holds along the optimization path.
Table 1 in Appendix A reports the confidence intervals of θ∗,c∗of10independent runs over random ini-
tialization and data reshuffling. One interesting observation is that by increasing the width and depth of
ResNet18, the SGD path of Wide-ResNet-16-8 has a smaller Uniform-LGI exponent θ∗that is close to 1/2.
This is parallel to the phenomenon in Liu et al. (2020) that overparameterized neural networks satisfy the
PL condition. More discussion for the Uniform-LGI on the overparameterized networks are in Appendix E.5.
5Published in Transactions on Machine Learning Research (10/2022)
2.3 Optimization and Generalization Results Under the Uniform-LGI Condition
In Section 2.2, empirical results suggest that the Uniform-LGI is generally satisfied on the training path for
various machine learning models. In this section, we derive optimization and generalization results assuming
that the Uniform-LGI holds. We consider to problem of optimizing the empirical loss (1) with gradient flow:
dw(t)
dt=−∇Ln(w(t)), t∈[0,+∞), (3)
wherew(t)is the parameter vector at time t,w(0)is the initialization. Assume ∀(x,y)∼D,∥x∥≤1,|y|≤1.
First, we give an optimization result under the Uniform-LGI condition. We show that when θ= 1/2, the
convergence rate is linear; when θ∈(1/2,1), the convergence rate is sublinear. Furthermore, we give an
explicit estimate for the distance between the initialization and the parameter during the training process.
Theorem 2.2 (Optimization) .For a fixed initialization w(0), suppose that there exist θn∈[1/2,1)and
cn>0such that the loss function Ln(w)satisfies the Uniform-LGI on {w(t):t≥0}withθn,cn. Thenw(t)
converges to a stationary point w(∞)with convergence rate given by
θn= 1/2 :Ln(w(t))−Ln(w(∞))≤e−c2
nt/parenleftig
Ln(w(0))−Ln(w(∞))/parenrightig
;
θn∈(1/2,1) :Ln(w(t))−Ln(w(∞))≤(1 +Mt)−1/(2θn−1)/parenleftig
Ln(w(0))−Ln(w(∞))/parenrightig
,
whereM=c2
n(2θn−1)/parenleftbig
Ln(w(0))−Ln(w(∞))/parenrightbig2θn−1. The distance between the initialization w(0)and the
parameterw(t)at timetis bounded by
∥w(0)−w(t)∥≤1
cn(1−θn)/bracketleftbigg/parenleftig
Ln(w(0))−Ln(w(∞))/parenrightig1−θn
−/parenleftig
Ln(w(t))−Ln(w(∞))/parenrightig1−θn/bracketrightbigg
.
The proof of Theorem 2.2 is given in Appendix B. This theorem shows that if the loss function satisfies
the Uniform-LGI along the gradient flow path, then it converges to a stationary point with an explicit
convergence rate and distance estimate. The estimation of cn,θnfor different nshould be analyzed case by
case based on the loss function, as shown in Section 3. Our optimization result can also be extended to the
standard gradient descent algorithm. See Appendix C for details.
Once we have a distance estimate for the parameter during the training process, we can derive generalization
bounds for the norm-constrained parameter space during the training process based on the Rademacher
complexity theory. For the generalization error analysis, we assume that there exists an almost everywhere
differentiable function Ψ :Rp+q− →Rsuch that the model f(w,·)can be represented in the following form,
∀x∈Rd, f(w,x) = Ψ/parenleftbig
α⊤
1x,...,α⊤
px,β1,...,βq/parenrightbig
, (4)
whereα1,...,αp∈Rd,β1,...,βq∈R, andw= vec ({α1,...,αp,β1,...,βq})∈Rpd+q.vecis the vectoriza-
tion that concatenates all elements into a column vector. A wide class of functions can be represented in the
form (4), including linear functions, fully connected neural networks and convolutional neural networks.
Additional notations. For the loss function ℓ, we useLℓ(S)to denote its Lipschitz constant (the
maximal gradient norm) on Swith respect to its first argument. For Ψin (4), we define LΨ(S) :=/parenleftig
L(1)
Ψ(S),···,L(p)
Ψ(S),L(p+1)
Ψ (S),···,L(p+q)
Ψ (S)/parenrightig⊤
, whereL(i)
Ψ(S)is the Lipschitz constant of ΨonSwith
respect to the i-th variable. Let w(0):= vec/parenleftig/braceleftig
α(0)
1,...,α(0)
p,β(0)
1,...,β(0)
q/bracerightig/parenrightig
and useLD(w)to de-
note the expected loss E(x,y)∼D[ℓ(f(w,x),y)]. Fora= (a1,...,ap)⊤andb= (b1,...,bq)⊤, we de-
fineSa,b={w:∀i∈[p],j∈[q],∥αi∥≤ai,|βj|≤bj}, andMa,b= supw∈Sa,b,∥x∥≤1,|y|≤1ℓ(f(w,x),y). Note
that for any loss function ℓ:R×R− →[0,1]that is 1-Lipschitz in the first argument, we have that
Lℓ(Sa,b) =Ma,b= 1for anya,b. An example for such a loss function is the ramp loss (Huang et al.,
2014) that is commonly used for classification.
6Published in Transactions on Machine Learning Research (10/2022)
In the next theorem, to simplify the notation, we consider studying the generalization of wεwhen the loss
Ln(wε)first reaches ε·Ln(w(0))for some given ε∈[0,1]. Then by Theorem 2.2, the distance ∥w(0)−wε∥
is bounded above by(Ln(w(0))−Ln(w(∞)))1−θn−(εLn(w(0))−Ln(w(∞)))1−θn
cn(1−θn). Finally by estimating the initial loss
value and the final loss value, we can use the distance estimate to get a generalization bound. To showcase a
bias-variance tradeoff, we bound the test error LD(wε)rather than the generalization gap LD(wε)−Ln(wε).
Theorem 2.3 (Generalization) .For a fixed initialization w(0), suppose that there exist θn∈[1/2,1)and
cn>0such that the loss function Ln(w)satisfies the Uniform-LGI on {w(t):t≥0}withθn,cn, and assume
that there exist Mδ,¯Mδsuch that with probability at least 1−δ/2over the training samples S,Ln(w(0))≤Mδ,
Ln(w(∞))≤¯Mδ. Then for any given ε,δ∈[0,1], with probability at least 1−δoverS, the generalization
bound of any parameter wεwithLn(wε) =εLn(w(0))is given by
LD(wε)≤εLn(w(0)) + sup
∥a∥2+∥b∥2≤2r2
n,δ,ε2√
2rn,δ,εLℓ(Sa,b)∥LΨ(Sa,b)∥√n+ 3Ma,b/radicalbigg
3(p+q) + log(4/δ)
2n,(5)
wherern,δ,ε=(Mδ−¯Mδ)1−θn−(εMδ−¯Mδ)1−θn
cn(1−θn).
The proof of Theorem 2.3 is in Appendix D. We use the distance estimate in Theorem 2.2 to get a norm-
based parameter space/braceleftbig
w:/vextenddouble/vextenddoublew−w(0)/vextenddouble/vextenddouble≤r/bracerightbig
. Then this allows us to use Rademacher complexity theory to
obtain the generalization result. There are several key terms in (5): (1) Mδ: This is a high probability upper
bound for loss value at initialization. In practice, for commonly used initialization schemes, such as Xavier
initialization (Glorot & Bengio, 2010) and Kaiming initialization (He et al., 2015), Ln(w(0))is uniformly
bounded with high probability; (2) θn,cn: These two quantities are the Uniform-LGI constants along the
optimization path. The asymptotic analysis of θn,cnis model-dependent, and we provide several examples in
Section 3; (3) Lℓ(Sa,b),Ma,b: These quantities are directly related to the loss function ℓ. For instance, given
a loss function ℓ:R×R− →[0,1]that is 1-Lipschitz in the first argument, we have that Lℓ(Sa,b) =Ma,b= 1;
(4)∥LΨ(Sa,b)∥: This term is related to the properties of f. For example, when fis linear, Ψ(x,y) =x+y,
then∥LΨ(Sa,b)∥=√
2. Whenfis a two-layer neural network, i.e., f(w,x) =/summationtextm
i=1viϕ(u⊤
ix), we will show in
Section 3.3 that sup∥a∥2+∥b∥2≤2r2∥LΨ(Sa,b)∥is bounded above by√
2r(equation (29)). For two-layer neural
networks,pandqare both equal to the number of the hidden units, which is usually much smaller than the
sample size n. In this case, the bound is dominated by the first two terms, which represent a bias-variance
tradeoff pattern during training. To see this, if ε= 1(no training, high bias, low variance), then rn,δ,ε= 0,
and the bound is dominated by the initial loss; if ε=¯Mδ/Mδ(convergence model, low bias, high variance),
then the generalization bound is dominated by the model complexity rn,δ,ε.
Remark2.4.The generalization bound also holds for the converged model by simply setting εto be ¯Mδ/Mδ.
In particular, when the final empirical loss value is 0,εcan be 0. For the gradient descent algorithm, one
can also directly use the optimization result (the distance bound) in Appendix C to derive the generalization
result. The only quantity that needs to be changed is that rn,δ,ε=(Mδ−¯Mδ)1−θn−(εMδ−¯Mδ)1−θn
cn(1−θn)(1−ηβL/2), whereηis
the learning rate, βLis the smoothness constant. See Appendix C for details.
3 Applications
In this section, we apply our framework (Theorem 2.2 and Theorem 2.3) to three machine learning models.
To obtain clean expressions of the generalization bound in terms of the sample size n, we consider a range
ofnrelated to the dimension d. In particular, we consider underdetermined systems where the ratio n/d
remains finite unless stated otherwise:
∃γ0,γ1∈(0,∞),s.t.∀d, γ 0d≤n=n(d)≤γ1d.
This setting is non-asymptotic since we do not require dto be sufficiently large. For each application,
we consider regression problems with squared loss ℓ(y,ˆy) = (y−ˆy)2/2, which is not globallyLipschitz nor
globally bounded. Thus, we cannot apply directly Theorem 2.3 to the squared loss, since it requires to bound
Lℓ(Sa,b)andMa,bwhich depend on a,band the model architecture. To mitigate this issue, we consider to
7Published in Transactions on Machine Learning Research (10/2022)
evaluate the generalization with a new loss ˜ℓthat is globally Lipschitz and globally bounded. Specifically,
let˜ℓ:R×R− →[0,l0]that is√l0-Lipschitz (on the first argument) for some l0>0and ˜ℓ(y,y) = 0.
For the squared loss, this can be achieved by truncating ℓat|y−ˆy|=√l0, and then using a constant
extension past the truncated point to make it continuous. In this case, Lℓ(Sa,b) =√l0,Ma,b=l0for any
a,b. Finally, to use Theorem 2.3, it remains to estimate the new empirical loss value1
n/summationtextn
i=1˜ℓ(f(wε,xi),yi)
in terms ofLn(wε). By the Lipschitzness property and Cauchy–Schwarz inequality, one can show that
1
n/summationtextn
i=1˜ℓ(f(wε,xi),yi)≤/radicalbig
2l0Ln(wε)(see the inequality (22) in Appendix E.1). Combining the above
derivations, applications of the framework are straightforward.
Remark 3.1.One can also get generalization bounds under the squared loss ℓ(y,ˆy) = (y−ˆy)2/2. Note that
Ma,b≤supw∈Sa,b,∥x∥≤1f(w,x)2+ 1andLℓ(Sa,b)≤supw∈Sa,b,∥x∥≤1|f(w,x)|+ 1. Then we can get an upper
bound forLD(wε)by Theorem 2.3.
3.1 Underdetermined ℓ2Linear Regression
We begin with an underdetermined linear regression model f(w,x) =w⊤xwith squared loss:
arg min
w∈RdLn(w) :=1
2nn/summationdisplay
i=1/parenleftbig
w⊤xi−yi/parenrightbig2, (6)
where the input data matrix X= (x1,...,xn)⊤∈Rn×dhas full row rank ( d>n). Then the above regression
model has at least one global minimum with zero loss.
Target function. Suppose the training data is generated from an underlying function g:Rd− →Rwith
yi=g(xi),∀i∈[n]. LetY= (y1,···,yn)⊤, and assume that there exits c∗>0such that
∥Y∥≤c∗/radicalig
λmax(XX⊤). (7)
(7) actually indicates that gis Lipschitz with a dimension independent Lipschitz constant. For example,
g(x) =ϕ(x⊤w∗), wherew∗∈Rdwith∥w∗∥2≤c∗for some constant c∗, andϕ(·)is Lipschitz with ϕ(0) = 0.
Assumption 3.2. There exists symmetric positive-definite matrices Σd∈Rd×dwith 0<λ0≤λmin(Σ2
d)≤
λmax(Σ2
d)≤λ1for anyd, such that the entries of XΣdare i.i.d. subgaussian random variables with zero
mean and subgaussian moments2bounded by 1.
This assumption is reasonable in practice in the sense that it only requires the data (rows of the data matrix
X) to be i.i.d. and entries of each row of X(feature vector components) are not necessarily independent.
By applying our framework, we get the following optimization and generalization results for the models
during training when the empirical loss function Ln(wε) =εLn(w(0)).
Theorem 3.3. Consider the undertermined ℓ2linear regression model (6). Suppose that there exists a
constantc0≥1that is independent of dsuch that/vextenddouble/vextenddoublew(0)/vextenddouble/vextenddouble
2≤c0. Then the followings hold:
1.Ln(w)satisfies Uniform-LGI along the gradient flow curve/braceleftbig
w(t):t≥0/bracerightbig
withcn=/radicalig
2λmin(XX⊤)
n, θn= 1/2.
2.Ln(w(t))converges to zero linearly, i.e., Ln(w(t))≤exp/parenleftbig
−2λmin(XX⊤)t/n/parenrightbig
Ln(w(0)).
3. Ifγ1∈(0,1), then under Assumption 3.2, for any ε≥0and any target function that satisfies (7),
with probability at least 1−δ−τd−n+1−τdover the training samples S, the generalization bound
of the parameter wεwithLn(wε) =εLn(w(0))for someε∈[0,1]is given by
E(x,y)∼D/bracketleftbig˜ℓ(f(wε,x),y)/bracketrightbig
≤/radicalig
2l0εLn(w(0))+4(c0+c∗)/radicalig
2(1−ε)λ1
λ0/parenleftbigg
C+C√γ0+/radicalig
log(4/δ)
cn/parenrightbigg
τ
C1/parenleftig
1√γ1−1/parenrightig√n+3l0/radicalbigg
3 + log(4/δ)
2n,
wherec,C,C 1>0andτ∈(0,1)depend only on the subgaussian moment of the entries.
2The subgaussian moment of Xis defined as inf/braceleftbig
M≥ 0|EetX≤eM2t2/2,∀t∈R/bracerightbig
.
8Published in Transactions on Machine Learning Research (10/2022)
The proof of Theorem 3.3 is given in Appendix E.1. The generalization bound reveals a bias-variance tradeoff
for the linear regression model in the high dimension setting. For ε= 1, the generalization bound is domi-
nated by the initial loss. For ε= 0(convergence model), we get a generalization bound O/parenleftig/radicalbig
log(1/δ)/n/parenrightig
.
Comparison. This result is related to Bartlett et al. (2020) that studied the phenomenon of benign over-
fitting in high-dimensional ℓ2linear regression. First, we summarize the different settings and assumptions
in our paper and Bartlett et al. (2020) through an example: the training data {(xi,yi)}n
i=1⊂Rd×Rare
i.i.d. drawn from D. In our case, for any (x,y)∈D,y=x⊤w∗for somew∗∈Rdwith uniformly bounded
norm ind, and entries of xare i.i.d. (thus λ0=λ1= 1) with mean 0, variance Σ =E[xx⊤](diagonal).
Thus Σhas a flat spectrum with condition number 1. We further assume that ∥x∥≤ 1for alld. Let
X= (x1,...,xn)⊤∈Rn×d,Y= (y1,...,yn)∈Rn, then the minimum norm solution ˆw=X†Y(†is the
pseudo inverse). Let the excess risk E=Ex[(x⊤ˆw−x⊤w∗)2], then our result states that E=O(1/√n)given
thatdandndiverge but their ratio remains finite. Now we consider a new data set {(˜xi,˜yi)}n
i=1⊂Rd×R
that are i.i.d. drawn from ˜D. In Bartlett et al. (2020), the setting is that for any (˜x,˜y)∈˜D,˜y= ˜x⊤˜w∗
for some ˜w∗∈Rdwith uniformly bounded norm in d, and entries of ˜xare independent with mean 0,
variance ˜Σ =E[˜x˜x⊤]. If the entries of ˜xare independent, then ˜Σis diagonal. Let the minimum norm
solution w.r.t. the new data set ˜w=˜X†˜Y, then it is shown in Bartlett et al. (2020) that when ˜Σhas a
suitable decaying eigenspectrum, the excess risk ˜E=E˜x[(˜x⊤˜w−˜x⊤˜w∗)2]− →0whenn− →∞. Therefore,
we can see that there exist two cases for benign overfitting: the data covariance matrix has a decaying
eigenspectrum (Bartlett et al., 2020); the data covariance matrix has a flat eigenspectrum but each data
vector has a bounded norm (Theorem 3.3). Due to the rescaling of the data vector, even though Σhas a
flat eigenspectrum, the benign overfitting provably happens when ˆwhas a bounded norm. To have a better
understanding of the benign overfitting phenomenon when the data covariance matrix has a flat eigenspec-
trum, we consider an example in Bartlett et al. (2020) that we can deduce from our result. Under the
same notation as above, we consider the following data transformation for any (x,y)∈D:˜x=˜Σ1/2x√
d,
˜w∗=˜Σ−1/2w∗/√
d
∥˜Σ−1/2w∗/√
d∥,˜y=y
∥˜Σ−1/2w∗/√
d∥. Then the induced new data distribution ˜Dsatisfies the assumptions
and settings in Bartlett et al. (2020). Note that ˜X=X˜Σ1/2√
d,˜Y=Y
∥˜Σ−1/2w∗/√
d∥, thus the minimum
norm solution ˜w=˜X†˜Y=˜Σ−1/2ˆw
∥˜Σ−1/2w∗∥. Therefore, the excess risk ˜E=E˜x[(˜x⊤˜w−˜x⊤˜w∗)2] =E
∥˜Σ−1/2w∗/√
d∥2.
We can see that there exists an explicit relation between ˜EandE. In the high dimension setting ( dand
ndiverge but have a finite ratio), our result shows that E=O(1/√
d). We consider a benign overfitting
example in (Bartlett et al., 2020, Part 1 of Theorem 6): the eigenvalues of ˜Σdecay with a rate given by
σj=j−1log−β(j+ 1)withβ > 1. Now we show that ˜E− →0when entries of w∗are order 1/√
d. Notice
thatE
∥˜Σ−1/2w∗/√
d∥2∼d√
d
σ−1
1+...+σ−1
d=d√
d/summationtextd
j=1jlogβ(j+1)<d√
d/summationtextd
j=2j− →0. In summary, we can transform our
assumptions and settings to analyze the case of sharp eigenspectrum for benign overfitting, and get a result
that is consistent with Bartlett et al. (2020).
3.2 Kernel Regression
Consider a positive definite kernel k:X×X− → Rwith a corresponding feature map φ:Rd− →Fsatisfying
⟨φ(x),φ(y)⟩F=k(x,y). We assume that |k(x,x)|≤1,∀x∈X. LetHbe the reproducing kernel Hilbert
space (RKHS) with respect to k. IfF=Rs, then the kernel regression model with ℓ2loss is to solve the
following problem
arg min
w∈RsLn(w) :=1
2nn/summationdisplay
i=1/parenleftbig
w⊤φ(xi)−yi/parenrightbig2. (8)
Similar to the ℓ2linear regression case, we consider the following target function:
Target function. Suppose the training data is generated by an underlying function g:Rd− →Rwith
yi=g(xi),∀i∈[n]. We further assume that there exists c∗>0such that
∥Y∥≤c∗·/radicalbig
λmax(k(X,X)), (9)
wherek(X,X)is then×nkernel matrix with k(X,X)ij=k(xi,xj).
9Published in Transactions on Machine Learning Research (10/2022)
For instance, the following functions satisfy (9): g(x) =ϕ(φ(x)⊤w∗)wherew∗∈Rswith (∀s)∥w∗∥2≤c∗for
some constant c∗, andϕ(·)is Lipschitz with ϕ(0) = 0. To get the generalization results of kernel regression,
we will discuss two types of kernels separately: radial basis function (RBF) (Broomhead & Lowe, 1988)
kernel and inner product kernel.
RBF kernel. We study the RBF kernel of the form k(x,y) =ϱ(∥y−x∥)for a certain RBF ϱ. For the
input data, we define the separation distance ofXasSD:=1
2mini̸=j∥xi−xj∥,∀i,j∈[n].
Inner product kernel. We consider k(x,y) =ϱ/parenleftig
x⊤Σ2
dy
d/parenrightig
, where Σ2
dis defined in Assumption 3.2.
Following El Karoui et al. (2010), we make the following assumption on the function ϱ:
Assumption 3.4. ϱisC3in a neighborhood of 0withϱ(0) = 0,ϱ(1)>ϱ′(0)≥0,ϱ′′(0)≥0.
We now apply our framework to get optimization and generalization results for the final convergence model
whenε= 0. FortheRBFkernel, thegeneralizationbounddependsontheseparationdistanceofthesamples.
For the inner product kernel, we study the spectrum property of the high-dimensional random kernel matrix.
Theorem 3.5. Consider the kernel regression model (8). Suppose that there exists a constant c0≥1that
is independent of dsuch that/vextenddouble/vextenddoublew(0)/vextenddouble/vextenddouble
2≤c0. Then the followings hold:
1.Ln(w)satisfies Uniform-LGI along the gradient flow curve/braceleftbig
w(t):t≥0/bracerightbig
withcn=/radicalig
2λmin(k(X,X))
n, θn= 1/2, wherecnis controlled by the kernel and input samples.
2.Ln(w(t))converges to zero linearly, i.e., Ln(w(t))≤exp (−2λmin(k(X,X))t/n)Ln(w(0)).
3. For any target function that satisfies (9) we have:
•For the RBF kernel3, suppose that ϱ:R≥0− →R≥0is a decreasing function and ϱ(∥x∥)∈L1(Rd).
If there exists two positive constants qminandqmaxsuch that SD∈[qmin,qmax]for alln, then
with probability at least 1−δover the training samples, the generalization bound of the parameter
wεwithLn(wε) =εLn(w(0))for someε∈[0,1]is given by
E(x,y)∼D/bracketleftbig˜ℓ(f(wε,x),y)/bracketrightbig
≤/radicalig
2l0εLn(w(0))+C(c0,c∗,ϱ,d,q min,qmax)√1−ε√n+3l0/radicalbigg
3 + log(4/δ)
2n,
whereC(c0,c∗,ϱ,d,q min,qmax)is a constant that depends only on c0,c∗,ϱ,d,q min,qmax.
•For the inner product kernel, under Assumption 3.2 & 3.4, if dis large enough and δ >0is
small enough such that d−1/2/parenleftbig√
3δ−1/2+ log0.51d/parenrightbig
≤0.5(ϱ(1)−ϱ′(0)), then with probability at
least 1−δ−d−2over the samples, the generalization bound of the parameter wεwithLn(wε) =
εLn(w(0))for someε∈[0,1]is given by
E(x,y)∼D/bracketleftbig˜ℓ(f(wε,x),y)/bracketrightbig
≤/radicalig
2l0εLn(w(0))+C/parenleftig
1 +/radicalbig
log(1/δ)/n/parenrightig√1−ε
√n+3l0/radicalbigg
3 + log(4/δ)
2n,
whereCdepends on c0,c∗,γ1,ϱ′′(0),ϱ′(0),ϱ(1)and the subgaussian moment of the entries.
The proof of Theorem 3.5 is given in Appendix E.2.
Example 1.Kernels satisfying the conditions and assumptions in Theorem 3.5 include (1) RBF Gaussian:
ϱ(r) =e−ρr2,ρ> 0; (2) RBF Multiquadrics: ϱ(r) = (ρ+r2)β/2,ρ> 0,β∈R\2N,β <−d; (3) Inner product
Polynomial kernel: ϱ(r) =rβ,β∈Z+,β≥2; (4) NTK corresponding to Two-layer ReLU neural networks
onSd−1(√
d):ϱ(r) =r(π−arccos(r))
2π.
Comparison. The result of the inner product kernel is related to Liang & Rakhlin (2020) who derived
generalization bounds for the minimum RKHS norm estimator. They showed that when the data covariance
3Heredis fixed and nis varied.
10Published in Transactions on Machine Learning Research (10/2022)
matrix and the kernel matrix enjoy certain decay of the eigenvalues, the generalization bound vanishes as n
goes to infinity. For example, for exponential kernel and the covariance matrix Σ :=E[xx⊤]with thej-th
eigenvalueλj(Σ) =j−α, theℓ2generalization bound becomes O(n−α
2α+1)whenα∈(0,1)andn > d. In
comparison, we do not assume the eigenvalue decay property for the covariance matrix and still are able to
obtain an optimal generalization bound O/parenleftbig
n−1/2/parenrightbig
in the high dimension setting. Further, we extend the
works in Liang & Rakhlin (2020) by proving a new result of the RBF kernel. Note that the result of the
RBF kernel is not under the high-dimensional setting; thus it is not a direct adaptation of Liang & Rakhlin
(2020), and the proof itself is of independent interest.
3.3 Two-layer Neural Networks
In this section, we show that our framework can be applied to neural network models. We obtain gener-
alization bounds for shallow neural networks under the Uniform-LGI assumption. First, define a two-layer
ReLU neural network with width m:
f(w,x) :=v⊤ϕ(Ux) =m/summationdisplay
i=1viϕ(u⊤
ix), (10)
whereϕ(x) = max{0,x},x∈Rdis the input, v= (v1,...,vm)⊤∈Rm,U= (u1,...,um)⊤∈Rm×d
are the parameters, w= vec ({v,U}). Trivially, there exists an almost everywhere differentiable function
Ψ :R2m− →Rwith Ψ (s1,...,sm,t1,...,tm) =/summationtextm
i=1siϕ(ti)such that (4) holds. We consider minimizing
the quadratic loss Ln(w) :=1
2n/summationtextn
i=1(f(w,xi)−yi)2by gradient flow (3) under a fixed initialization w(0).
The numerical observations in Figure 2 show that the Uniform-LGI condition is generally satisfied on each
optimization path when training the neural network models (including the two-layer neural networks). In
the next proposition, we theoretically prove that there exist cn>0,θn∈[1/2,1)such that the Uniform-LGI
holds for the gradient flow path over eachchoice of the training samples.
Proposition3.6. For a fixed initialization w(0)and a fixed choice of the training sample Snwith sizen, there
exist two constants cn(Sn)>0,θn(Sn)∈[1/2,1)such that the loss function Ln(w)satisfies the Uniform-LGI
along the gradient flow curve {w(t):t≥0}withcn(Sn),θn(Sn). Specifically, for the population loss LD(w)
and its induced gradient flow curve {w(t)
D:t≥0}, ifLD(w)is subanalytic (Bolte et al., 2007), then there
existcD>0,θD∈[1/2,1)such thatLD(w)satisfies the Uniform-LGI along {w(t)
D:t≥0}withcD,θD.
The proof can be found in Appendix E.3. Note that the Uniform-LGI constants cn(Sn),θn(Sn)are random
variables that depend on the choice of the training sample Sn. Based on the limiting values ( cD,θD), we
make the following assumption that cn(Sn)>0,θn(Sn)∈[1/2,1)hold uniformly with high probability over
the choice of Sn.
Assumption3.7. FortheUniform-LGIconstantsinProposition3.6andany δ∈(0,1), thereexists cn,δ>0,
θn,δ∈[1/2,1)such that with probability at least 1−δ/3over the sample Sn,θn(Sn)≤θn,δ,cn(Sn)≥cn,δ.
This assumption is also supported by the statistical results in Table 1. Indeed, the numerical results suggest
that the Uniform-LGI constants satisfy this assumption with high confidence over the training sample. We
are now ready to state the main result for the two-layer neural network model.
Theorem3.8. Consider the two-layer neural network model (10) with initialization w(0). Under Assumption
3.7, for any δ∈(0,1)we have the following:
•With probability at least 1−δ/3over the training sample, the convergence rate is given by
θn,δ= 1/2 :Ln(w(t))−Ln(w(∞))≤e−c2
n,δt(Ln(w(0))−Ln(w(∞)));
θn,δ∈(1/2,1) :Ln(w(t))−Ln(w(∞))≤(1 +Mt)−1/(2θn−1)(Ln(w(0))−Ln(w(∞))),
whereM=c2
n,δ(2θn,δ−1)/parenleftbig
Ln(w(0))/parenrightbig2θn,δ−1.
11Published in Transactions on Machine Learning Research (10/2022)
•Under Assumption 3.2, assume that there exist ¯Mδsuch that with probability at least 1−δ/2over the
training samples S,Ln(w(∞))≤¯Mδ. for any target function that satisfies (7), then with probability
at least 1−δover the training samples, the generalization bound of the parameter wεwithLn(wε) =
εLn(w(0))for someε∈[0,1]is given by
E(x,y)∼D/bracketleftbig˜ℓ(f(wε,x),y)/bracketrightbig
≤/radicalig
2l0εLn(w(0)) +4√n/parenleftigg/parenleftbig˜C(1 + log(1/δ)/n)/parenrightbig1−θn,δ−(εMδ−¯Mδ)1−θn,δ
cn,δ(1−θn,δ)/parenrightigg2
+
3l0/radicalbigg
6m+ log(12/δ)
2n,
where ˜Cdepends only on c∗,γ0,λ0,∥w(0)∥and the subgaussian moment of the entries.
The proof of Theorem 3.8 is provided in Appendix E.4. We can see that for shallow neural networks ( m
much smaller than n), the generalization bound is dominated by the first two terms. When the gradient
flow converges to a global minimum with zero training loss ( ¯Mδ= 0), then the generalization bound for the
convergence model ( ε= 0) reduces to/tildewideO/parenleftig
c−2
n,δ(1−θn,δ)−2n−1/2/parenrightig
, where/tildewideOhide the logarithmic factor.
Empirical evaluation. To demonstrate that our generalization bound is non-vacuous and can effectively
capture the test error, we perform experiments on the CIFAR10 dataset (first two classes). First, we get
the Uniform-LGI constants cn,δ,θn,δby finite sample test (Algorithm 1) with varied sample size n. Then,
we calculate the generalization bound (up to some constant) c−2
n,δ(1−θn,δ)−2n−1/2+/radicalbig
m/n. Finally, we
randomly flip the label and record the Uniform-LGI constants cn,δ,θn,δwith different ratio of label flip. See
Appendix A for implementation details. From Figure 3 (a), we observe that cn,δ,θn,δhave good dependency
on the sample size n. Figure 3 (b) shows that the generalization bound decreases as nincreases, indicating
that our bound is non-vacuous. As shown in Figure 3 (c), when the ratio of label flip increases, cn,δdecreases
andθn,δincreases. Our results (Theorem 3.8) show that if cn,δdecreases and θn,δincreases, then we should
expect worse optimization and generalization error, which also correlates with the known findings that
random labels negatively affect both optimization and generalization (Zhang et al., 2016).
(a)
 (b)
 (c)
Figure 3: (a) The Uniform-LGI constants cn,δ,θn,δobtained by finite sample test (Algorithm 1) with respect
to sample size n. (b) Dependence of our generalization bound c−2
n,δ(1−θn,δ)−2n−1/2+/radicalbig
m/non the sample
sizen. (c) The Uniform-LGI constants cn,δ,θn,δwith respect to the ratio of label flip. All Results are
reported with the mean and standard deviation over 5independent runs.
Overparameterized neural netowrks. Our generalization bound becomes vacuous when mis large
enough. But for overparameterized neural networks, the PL condition ( θn,δ= 1/2) is proved to hold in the
NTK regime (Liu et al., 2020). To make our framework complete, we derive optimization and generaliza-
tion results for the overparameterized two-layer neural network. The generalization result is based on the
Rademacher complexity theory in Arora et al. (2019) in the NTK regime. In particular, we theoretically show
thatθn,δ= 1/2andcn,δ=O(1)over the training sample and standard random initialization when mis large
enough, and we give a generalization bound of order O/parenleftig/radicalbig
log(n/δ)/n/parenrightig
, which confirms the phenomenon of
benign over-fitting in the high dimension setting. More details are provided in Appendix E.5.
12Published in Transactions on Machine Learning Research (10/2022)
4 Related Works
Optimization. Theoretically analyzing the training process of most machine learning models is a chal-
lenging problem as the loss landscapes are generally non-convex. One approach to studying non-convex
optimization problems is to use the Polyak-Łojasiewicz (PL) condition (Polyak, 1963), which characterizes
the local geometry of loss landscapes and ensures the existence of global minima. It is shown in Karimi
et al. (2016) that GD admits linear convergence for a class of optimization objective functions under the PL
condition. In this paper, we modify the original LGI to obtain convergence rates (not necessarily converges
to a global minimum) and generalization estimates duringthe training process.
Generalization. Traditional VC dimension-based generalization bounds depend on the number of pa-
rameters and are vacuous for large models such as overparameterized neural networks. To overcome this
limitation, several non-vacuous generalization bounds are proposed, e.g. the norm/margin-based generaliza-
tion bounds (Neyshabur et al., 2015; Bartlett et al., 2017; Golowich et al., 2018; Neyshabur et al., 2019),
and the PAC-Bayes-based bounds (Dziugaite & Roy, 2017; Neyshabur et al., 2018; Zhou et al., 2019; Ri-
vasplata et al., 2020). However, these bounds tend to ignore or focus less on optimization, e.g., norm-based
generalization bounds may not discuss how small-norm solutions are obtained through practical training. In
this paper, we connect the optimization and generalization by deriving generalization bounds based on the
optimization path length during training model training.
Interplay of optimization and generalization. Implicit bias builds the bridge between optimization
and generalization, which has been widely studied to explain the generalization ability of machine learning
models. Recent works (Soudry et al., 2018a;b; Nacson et al., 2019a;b; Lyu & Li, 2020) showed that linear
classifiers or deep neural networks trained by GD/SGD maximizes the margin of the separating hyperplanes
and therefore generalizes well. Other works (Arora et al., 2019; Zou & Gu, 2019; Cao & Gu, 2020; Ji
& Telgarsky, 2020; Chen et al., 2021) considered overparameterized neural networks in the lazy training
regime where the minimizer has good generalization due to the low “complexity” of the parameter space. In
this work, we focus on specific conditions on loss functions under which we can connect optimization and
generalization on the situations which do not satisfy those above.
Comparison with existing works on the LGI condition. Recent work by Frei & Gu (2021) studies
optimization and generalization via general PL-conditions. Their key assumptions, problem settings, and
type of results differ from ours on many aspects. In Frei & Gu (2021), the authors analyze a loss function f
thatsatisfiesthe (g,ξ,α,µ )-proxyPLinequality, whichisamoregeneralconditionthantheLGIinthispaper.
For example, by setting g=f,ξ= minwf(w), the proxy PL inequality corresponds to the LGI. However, a
key assumption in Frei & Gu (2021) is that f(w)satisfies a (g,ξ,α,µ )-proxy PL inequality globallyfor all
w∈Rp, while our assumption only requires that f(w)satisfies the LGI locally, e.g., along the optimization
path. For the problem settings, we cover different situations. Frei & Gu (2021) considers the online learning
setting where samples are observed one-by-one. In contrast, we focus on the classic gradient flow/descent
setting, where the training samples are given before training. Thus, these two works provide different types
of results. Through the global proxy-PL inequality, Frei & Gu (2021) gives an upper bound for the best case
of the test error, i.e., mint<TLD(w(t)). In this work, we obtain bounds for the test error LD(w(t))for any
timet, which can be used to showcase bias-variance trade-off patterns. For instance, a simple application of
our theory yields a new case of benign overfitting on linear/kernel regression in the high dimensional setting
(Theorem 3.3 & 3.5). Moreover, in this work, we propose a finite sample test algorithm that can be applied
to verify the Uniform-LGI and estimate the Uniform-LGI constants for standard deep learning settings.
Another work by Foster et al. (2018) studies similar LGI conditions. Under the key assumption that the
population riskLD(w)satisfies the LGI on some given parameter space W, the authors show that the excess
riskLD(w)−minw∈WLD(w)is bounded above by supw∈W∥∇Ln(w)−∇LD(w)∥. In practice, it is not easy
to validate whether LDsatisfies the LGI because it depends on the data distribution and requires a very
large number of samples. In comparison, we only assume that the empirical riskLn(w)satisfies the LGI
along the training path, and the Uniform-LGI condition can be numerically verified by the finite sample test
algorithm. Under these two different assumptions, both our results and those of Foster et al. (2018) yield
consistent generalization bounds O(1/√n)for the linear regression models in the high-dimensional setting.
A local version of the PL condition has been studied in Chatterjee (2022). This local PL condition is a
13Published in Transactions on Machine Learning Research (10/2022)
particular case of our Uniform-LGI with θ= 1/2. In the case of feed-forward neural networks, Chatterjee
(2022) show that gradient descent with proper initialization converges to a global minimum given that (1)
the activation functions are smooth and strictly increasing; (2) the minimum value of the parameters is large
enough; (3) the learning rate is small enough. In this work, we give both convergence guarantee (to a local
minimum) and generalization analysis based on the Uniform-LGI condition.
5 Conclusion
Inthiswork, weaddressthequestionsofwhenandwhyanoptimizationalgorithmfindsaminimumwithgood
generalization properties. For this purpose, we propose a framework to bridge the gap between optimization
and generalization based on the optimization path. The pivotal component is the Uniform-LGI condition:
a condition on the loss function that is generally satisfied during the training of standard machine learning
models. Using this assumption, we show that gradient flow converges to a stationary point with an explicit
convergence rate and we derive generalization bound that hold during training. Finally, we apply the
framework to three widely used machine learning models. By estimating the optimization path length, we
get non-vacuous generalization bounds, even in the high dimensional case.
6 Acknowledgement
We thank the anonymous TMLR reviewers for several helpful comments. H. Yang is partially supported
by the US National Science Foundation under award DMS- 2244988, DMS-2206333, and the Office of Naval
Research Young Investigator Award. Q. Li is supported by the National Research Foundation, Singapore,
under the NRF fellowship (project No. NRF-NRFF13-2021-0005).
14Published in Transactions on Machine Learning Research (10/2022)
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural
networks, going beyond two layers. In Advances in neural information processing systems , pp. 6158–6169,
2019.
SanjeevArora,SimonDu, WeiHu, ZhiyuanLi, andRuosongWang. Fine-grainedanalysisofoptimizationand
generalization for overparameterized two-layer neural networks. In International Conference on Machine
Learning , pp. 322–332. PMLR, 2019.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural
networks. In Advances in Neural Information Processing Systems , pp. 6240–6249, 2017.
PeterLBartlett, PhilipMLong, GáborLugosi, andAlexanderTsigler. Benignoverfittinginlinearregression.
Proceedings of the National Academy of Sciences , 117(48):30063–30070, 2020.
Jérôme Bolte, Aris Daniilidis, and Adrian Lewis. The łojasiewicz inequality for nonsmooth subanalytic
functions with applications to subgradient dynamical systems. SIAM Journal on Optimization , 17(4):
1205–1223, 2007.
DavidS.BroomheadandDavidLowe. Multivariablefunctionalinterpolationandadaptivenetworks. Complex
Syst., 2, 1988.
YuanCaoandQuanquanGu. Generalizationerrorboundsofgradientdescentforlearningover-parameterized
deep relu networks. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 34, pp. 3349–
3356, 2020.
Sourav Chatterjee. Convergence of gradient descent for deep neural networks. arXiv preprint
arXiv:2203.16462 , 2022.
Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is sufficient to
learn deep relu networks? In International Conference on Learning Representations , 2021.
Benedikt Diederichs and Armin Iske. Improved estimates for condition numbers of radial basis function
interpolation matrices. Journal of Approximation Theory , 238:38–51, 2019.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-
parameterized neural networks. In International Conference on Learning Representations , 2019.
Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep
(stochastic) neural networks with many more parameters than training data. In Proceedings of the 33rd
Annual Conference on Uncertainty in Artificial Intelligence (UAI) , 2017.
Noureddine El Karoui et al. The spectrum of kernel random matrices. Annals of statistics , 38(1):1–50, 2010.
Dylan J Foster, Ayush Sekhari, and Karthik Sridharan. Uniform convergence of gradients for non-convex
learning and optimization. Advances in Neural Information Processing Systems , 31, 2018.
Spencer Frei and Quanquan Gu. Proxy convexity: A unified framework for the analysis of neural networks
trained by gradient descent. Advances in Neural Information Processing Systems , 34:7937–7949, 2021.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks.
InProceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics , 2010.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural net-
works. In Conference On Learning Theory , pp. 297–299. PMLR, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-
level performance on imagenet classification. In Proceedings of the IEEE international conference on
computer vision , pp. 1026–1034, 2015.
15Published in Transactions on Machine Learning Research (10/2022)
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016.
Xiaolin Huang, Lei Shi, and Johan AK Suykens. Ramp loss linear programming support vector machine.
The Journal of Machine Learning Research , 15(1):2185–2211, 2014.
Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbitrarily small
test error with shallow relu networks. In International Conference on Learning Representations , 2020.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient
methods under the polyak-łojasiewicz condition. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases , pp. 795–811. Springer, 2016.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection.
Annals of Statistics , pp. 1302–1338, 2000.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to docu-
ment recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes . Springer
Science & Business Media, 2013.
Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel “ridgeless” regression can generalize. The
Annals of Statistics , 48(3):1329–1347, 2020.
Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai. On the multiple descent of minimum-norm interpolants
and restricted lower isometry of kernels. In Conference on Learning Theory , pp. 2683–2711. PMLR, 2020.
Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over-parameterized non-
linear systems and neural networks. arXiv preprint arXiv:2003.00307 , 2020.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In
International Conference on Learning Representations , 2020.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning . MIT press,
2018.
Stephen J Montgomery-Smith. The distribution of rademacher sums. Proceedings of the American Mathe-
matical Society , 109(2):517–522, 1990.
Mor Shpigel Nacson, Suriya Gunasekar, Jason D Lee, Nathan Srebro, and Daniel Soudry. Lexico-
graphic and depth-sensitive margins in homogeneous and non-homogeneous deep models. arXiv preprint
arXiv:1905.07325 , 2019a.
MorShpigelNacson,NathanSrebro,andDanielSoudry. Stochasticgradientdescentonseparabledata: Exact
convergence with a fixed learning rate. In The 22nd International Conference on Artificial Intelligence
and Statistics , pp. 3051–3059. PMLR, 2019b.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks.
InConference on Learning Theory , pp. 1376–1401, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to spectrally-
normalized margin bounds for neural networks. In International Conference on Learning Representations ,
2018.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. The role of
over-parametrization in generalization of neural networks. In International Conference on Learning Rep-
resentations , 2019.
16Published in Transactions on Machine Learning Research (10/2022)
B. Polyak. Gradient methods for the minimisation of functionals. Ussr Computational Mathematics and
Mathematical Physics , 3:864–878, 1963.
Vladimir Privman. Finite size scaling and numerical simulation of statistical systems . World Scientific, 1990.
Omar Rivasplata, Ilja Kuzborskij, Csaba Szepesvári, and John Shawe-Taylor. Pac-bayes analysis beyond
the usual bounds. arXiv preprint arXiv:2006.13057 , 2020.
Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix. Commu-
nications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical
Sciences, 62(12):1707–1739, 2009.
Mark Rudelson and Roman Vershynin. Non-asymptotic theory of random matrices: extreme singular values.
InProceedings of the International Congress of Mathematicians 2010 (ICM 2010) (In 4 Volumes) Vol. I:
Plenary Lectures and Ceremonies Vols. II–IV: Invited Lectures , pp. 1576–1602. World Scientific, 2010.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of
gradient descent on separable data. The Journal of Machine Learning Research , 19(1):2822–2878, 2018a.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data. In
International Conference on Learning Representations , 2018b.
Alexander Tsigler and Peter L Bartlett. Benign overfitting in ridge regression. arXiv preprint
arXiv:2009.14286 , 2020.
Holger Wendland. Scattered Data Approximation . Cambridge Monographs on Applied and Computational
Mathematics. Cambridge University Press, 2004.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 , 2016.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning requires rethinking generalization. arXiv preprint arXiv:1611.03530 , 2016.
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P. Adams, and Peter Orbanz. Non-vacuous generaliza-
tion bounds at the imagenet scale: a PAC-bayesian compression approach. In International Conference
on Learning Representations , 2019.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural networks.
InAdvances in Neural Information Processing Systems , 2019.
17Published in Transactions on Machine Learning Research (10/2022)
A Experiments
In this section, we provide statistical results for the Uniform-LGI constants obtained by Algorithm 1 and
details of the numerical evaluation of Figure 1 and 3.
Experiments details for Figure 1. All the three models are trained with gradient descent with fixed
learningrate η. Specifically, thefollowinghyper-parametersareusedfortraining: formodel(a), initialization
w(0)= 0.1,η= 0.1, epochK= 106, start point K0= 50, steps= 103; for model (b), w(0)= 1,η= 0.01,
epochK= 103, start point K0= 50, steps= 10; for the linear regression model (c), the dataset contains
100points{(xi,yi)}100
i=1⊆R200×R, wherexi(∀i∈[100])are uniformly drawn from the 200-dimensional
unit sphere, and yi(∀i∈[100])are generated by a linear target function yi=β⊤xifor someβ∈R200with
∥β∥= 1. We train with random initialization, η= 0.1, epochK= 105, start point K0= 50, steps= 100.
Next, we list experiments details for Figure 3.
Dataset. We use the CIFAR10 dataset in our numerical evaluation. In particular, we only select the
first two classes (airplane versus automobile) with totally 10000training images. For the experiments in
Figure 3 (a) and (b), we randomly choose the sample from the whole 10000training images with size
n= 1000,2000,..., 10000, where each class has the same sample size. For the experiment in Figure 3 (c),
we use the whole 10000training images.
Model. We use two-layer fully connected neural network (no bias) with width m= 512, ReLU activation
as the training model.
Optimizer. We optimize the cross entropy loss by full batch gradient descent with random initialization
and leaning rate 0.01. We stop training once the training loss is less than 0.001or epoch reaches 20000. We
do not use weight decay, dropout or batch normalization.
Label flip. We only flip the training labels with different ratio.
Uniform-LGI constants. We calculate the Uniform-LGI constants by the finite sample test (Algorithm
1) after the training. For the obtained training parameters w(k)at epochkfork∈[0 :K], we estimate
the optimum w∗ofL(w)asarg mink∈[0:K]. To avoid division by zero, we delete the parameter w∗in
w(0),...,w(K). Then we apply the finite sample test (Algorithm 1) with start point K0= 1000, steps= 100
to obtain the estimated Uniform-LGI constants. We set (cn,δ,θn,δ)to be the final estimation (cK+1,θK+1).
We report the results with the mean and standard deviation over 5independent runs.
The following table shows the statistical results of different models’ Uniform-LGI constants obtained by
Algorithm 1.
Model θ∗c∗
Linear Regression 0.496±0.001 0.063±0.0015
2-layer MLP 0.785±0.005 0.066±0.013
ResNet18 0.751±0.019 0.068±0.016
Wide ResNet 0.514±0.011 0.169±0.037
Table 1: Mean and standard error for the Uniform-LGI constants θ∗,c∗obtained by finite sample test
(Algorithm 1) over 10independent runs over random initialization and data reshuffling.
B Proof of Theorem 2.2
In this section we will prove Theorem 2.2. The proof contains two parts, one is the convergence of the
gradient flow, and another is the convergence rate derivation.
18Published in Transactions on Machine Learning Research (10/2022)
Proof.First, by the gradient flow (3), we have
dLn(w(t))
dt=/angbracketleftbigg
∇Ln(w(t)),dw(t)
dt/angbracketrightbigg
=−/vextenddouble/vextenddouble/vextenddouble∇Ln(w(t))/vextenddouble/vextenddouble/vextenddouble2
≤0,
which indicates that the loss value Ln(w(t))is non-increasing for t. SinceLn(w(t))is always non-negative,
thenLn(w(t))converges to some limit L∞, which is also the optimal loss value along the gradient flow path.
For the convergence rate, by the Uniform-LGI condition,
d/parenleftbig
Ln(w(t))−L∞/parenrightbig
dt=/angbracketleftbigg
∇Ln(w(t)),dw(t)
dt/angbracketrightbigg
=−/vextenddouble/vextenddouble/vextenddouble∇Ln(w(t))/vextenddouble/vextenddouble/vextenddouble2
≤−c2
n/parenleftig
Ln(w(t))−L∞/parenrightig2θn
.(11)
Therefore/parenleftig
Ln(w(t))−L∞/parenrightig−2θn
d/parenleftig
Ln(w(t))−L∞/parenrightig
≤−c2
ndt.
Integrating on both sides of the equation, we can get ∀t∈[0,∞),
whenθn=1
2,
Ln(w(t))−L∞≤e−c2
nt/parenleftig
Ln(w(0))−L∞/parenrightig
; (12)
when1
2<θn<1,
Ln(w(t))−L∞≤(1 +Mt)−1/(2θn−1)/parenleftig
Ln(w(0))−L∞/parenrightig
, (13)
whereM=c2
n(2θn−1)/parenleftbig
Ln(w(0))−L∞/parenrightbig2θn−1.
For the distance bound, we consider to bound the gradient flow path length/integraltextT
0/vextenddouble/vextenddouble/vextenddoubledw(t)
dt/vextenddouble/vextenddouble/vextenddoubledt. Notice that
dLn(w(t))
dt=/angbracketleftbigg
∇Ln(w(t)),dw(t)
dt/angbracketrightbigg
=−/vextenddouble/vextenddouble/vextenddouble∇Ln(w(t))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubledw(t)
dt/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤−cn/parenleftig
Ln(w(t))−L∞/parenrightigθn/vextenddouble/vextenddouble/vextenddouble/vextenddoubledw(t)
dt/vextenddouble/vextenddouble/vextenddouble/vextenddouble.
Hence∀t∈[0,∞),
/integraldisplayt
0/vextenddouble/vextenddouble/vextenddouble/vextenddoubledw(s)
ds/vextenddouble/vextenddouble/vextenddouble/vextenddoubleds≤/integraldisplayt
0−1
cn/parenleftig
Ln(w(s))−L∞/parenrightig−θn
dLn(w(s))
=1
cn(1−θn)/bracketleftbigg/parenleftig
Ln(w(0))−L∞/parenrightig1−θn
−/parenleftig
Ln(w(t))−L∞/parenrightig1−θn/bracketrightbigg
.
Now we begin to prove that w(t)converges. Notice that for any s>t≥0, we have
/integraldisplays
t/vextenddouble/vextenddouble/vextenddouble/vextenddoubledw(τ)
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddoubledτ≤/parenleftbig
Ln(w(t))−L∞/parenrightbig1−θn−/parenleftbig
Ln(w(s))−L∞/parenrightbig1−θn
cn(1−θn).
19Published in Transactions on Machine Learning Research (10/2022)
Then for any discrete time sequence t1,t2,...,tn,...that increases to infinity, we have ∀p>q> 0,
/integraldisplaytp
tq/vextenddouble/vextenddouble/vextenddouble/vextenddoubledw(s)
ds/vextenddouble/vextenddouble/vextenddouble/vextenddoubleds≤/parenleftbig
Ln(w(tq))−L∞/parenrightbig1−θn−/parenleftbig
Ln(w(tp))−L∞/parenrightbig1−θn
cn(1−θn).
Now we prove that the sequence w(t1),...,w(tn),...is a Cauchy sequence in the Euclidean space with ℓ2
metric. Notice that(Ln(w(tn))−L∞)1−θn
cn(1−θn)converges to zero, thus it is a Cauchy sequence. Then for any ε>0,
there exists M > 0such that∀p>q>M ,
∥w(tp)−w(tq)∥≤/integraldisplaytp
tq/vextenddouble/vextenddouble/vextenddouble/vextenddoubledw(s)
ds/vextenddouble/vextenddouble/vextenddouble/vextenddoubleds
≤/parenleftbig
Ln(w(tq))−L∞/parenrightbig1−θn−/parenleftbig
Ln(w(tp))−L∞/parenrightbig1−θn
cn(1−θn)
≤ε.
Therefore,w(tn)is a Cauchy sequence, and then it converges. This means that for any increasing sequence
tn− →∞,w(tn)converges. Hence, thelimits limn− →∞w(tn)mustbethesameforanysequence tn− →∞. Other-
wise, we can take two sequences tnk,tmk(k= 1,2,...)with different limits limk− →∞w(tnk)̸= limk− →∞w(tmk),
then there exists an increasing sequence ukthat contains two subsequences of nk,mk, such that these two
subsequences have different limits (since limk− →∞w(tnk)̸= limk− →∞w(tmk)), which contradicts with the con-
vergence of w(tuk). This yields that for any increasing discrete sequence tn− →∞,w(tn)converges to the
same limit. Finally, we conclude that the continuous dynamics w(t)converges to some limit w(∞), which is
a stationary point with Ln(w(∞)) =L∞. Then we can replace L∞toLn(w(∞))in all the above equations.
Finally, note that the length is always an upper bound for the distance, which completes the proof.
C Discrete time analysis
In this section, we provide optimization results for the gradient descent algorithm, i.e., we consider
w(t+1)=w(t)−η∇Ln(w(t)), t= 0,1,2,...
whereηis a fixed learning rate. We assume that the loss function is βL-smooth in its whole domain W, i.e.,
|Ln(w)−Ln(v)−⟨∇Ln(v),w−v⟩|≤βL
2∥w−v∥2,∀w,v∈W.
Note that a continuously differentiable function is β-smooth if its gradient is β-Lipschitz. In our analysis, we
only focus on the bounded region of the gradient descent path, and the gradient ∇Ln(w)is usually locally
Lipschitz inside this region. Hence, this is a reasonable assumption given that Wis a compact set. Now we
present our main theorem over the gradient descent algorithm.
Theorem C.1. For a fixed initialization w(0), suppose that there exist θn∈[1/2,1)andcn>0such that
the loss function Ln(w)satisfies the Uniform-LGI on {w(t):t= 0,1,...}withθn,cn. Let the learning rate
η∈(0,2/βL), thenw(t)converges to a stationary point w(∞)with convergence rate given by
θn= 1/2 :Ln(w(t))−Ln(w(∞))≤e−c2
n(η−η2βL/2)t/parenleftig
Ln(w(0))−Ln(w(∞))/parenrightig
;
θn∈(1/2,1) :Ln(w(t))−Ln(w(∞))≤(1 +Mt)−1/(2θn−1)/parenleftig
Ln(w(0))−L(w(∞))/parenrightig
,
whereM=c2
n(2θn−1)/parenleftig
η−η2βL
2/parenrightig/parenleftbig
Ln(w(0))−Ln(w(∞))/parenrightbig2θn−1. The distance between the initialization
w(0)and the parameter w(t)at steptis bounded by
∥w(0)−w(t)∥≤1
cn(1−θn)(1−ηβL/2)/bracketleftbigg/parenleftig
Ln(w(0))−Ln(w(∞))/parenrightig1−θn
−/parenleftig
Ln(w(t))−Ln(w(∞))/parenrightig1−θn/bracketrightbigg
.
20Published in Transactions on Machine Learning Research (10/2022)
Proof.First, by the βL-smoothness, we have
Ln(w(t+1))≤Ln(w(t)) +/angbracketleftig
∇Ln(w(t)),w(t+1)−w(t)/angbracketrightig
+βL
2/vextenddouble/vextenddouble/vextenddoublew(t+1)−w(t)/vextenddouble/vextenddouble/vextenddouble2
=Ln(w(t))−/parenleftbigg1
η−βL
2/parenrightbigg/vextenddouble/vextenddouble/vextenddoublew(t+1)−w(t)/vextenddouble/vextenddouble/vextenddouble2
<Ln(w(t)).
Therefore,Ln(w(t+1))is strictly decreasing for t. By the above argument, we can also get
/vextenddouble/vextenddouble/vextenddouble∇Ln(w(t))/vextenddouble/vextenddouble/vextenddouble2
≤Ln(w(t))−Ln(w(t+1))
η−η2βL/2. (14)
SinceLn(w(t))is non-increasing, then Ln(w(t))converges to some limit L∞, which is also the optimal loss
value along the gradient descent path.
For the convergence rate, by the Uniform-LGI condition and equation (14),
Ln(w(t+1))−Ln(w(t))≤−/parenleftbigg
η−η2βL
2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble∇Ln(w(t))/vextenddouble/vextenddouble/vextenddouble2
≤−c2
n/parenleftbigg
η−η2βL
2/parenrightbigg/parenleftig
Ln(w(t))−L∞/parenrightig2θn
.
Notice that
c2
n/parenleftbigg
η−η2βL
2/parenrightbigg
≤Ln(w(t))−Ln(w(t+1))
/parenleftbig
Ln(w(t))−L∞/parenrightbig2θn
=/integraldisplayLn(w(t))
Ln(w(t+1))1
/parenleftbig
Ln(w(t))−L∞/parenrightbig2θndLn(w)
≤/integraldisplayLn(w(t))
Ln(w(t+1))1
(Ln(w)−L∞)2θndLn(w).
whenθn=1
2, we have
/integraldisplayLn(w(t))
Ln(w(t+1))1
(Ln(w)−L∞)2θndLn(w) = logLn(w(t))−L∞
Ln(w(t+1))−L∞,
which yields that
Ln(w(t+1))−L∞
Ln(w(t))−L∞≤e−c2
n(η−η2βL
2).
Hence,
Ln(w(t))−L∞≤e−c2
n(η−η2βL
2)t/parenleftig
Ln(w(0))−L∞/parenrightig
.
When1
2<θn<1, we have
/integraldisplayLn(w(t))
Ln(w(t+1))1
(Ln(w)−L∞)2θndLn(w) =/parenleftbig
Ln(w(t))−L∞/parenrightbig1−2θn−/parenleftbig
Ln(w(t+1))−L∞/parenrightbig1−2θn
1−2θn,
which yields that
1
/parenleftbig
Ln(w(t+1))−L∞/parenrightbig2θn−1−1
/parenleftbig
Ln(w(t))−L∞/parenrightbig2θn−1≥c2
n(2θn−1)/parenleftbigg
η−η2βL
2/parenrightbigg
.
21Published in Transactions on Machine Learning Research (10/2022)
Hence,
Ln(w(t))−L∞≤(1 +Mt)−1/(2θn−1)/parenleftig
Ln(w(0))−L∞/parenrightig
,
whereM=c2
n(2θn−1)/parenleftig
η−η2βL
2/parenrightig/parenleftbig
Ln(w(0))−L∞/parenrightbig2θn−1.
For the distance bound, by the βL-smoothness and the Uniform-LGI condition, we have
/vextenddouble/vextenddouble/vextenddoublew(t+1)−w(t)/vextenddouble/vextenddouble/vextenddouble≤Ln(w(t))−L(w(t+1))/parenleftig
1−ηβL
2/parenrightig/vextenddouble/vextenddouble∇Ln(w(t))/vextenddouble/vextenddouble
≤Ln(w(t))−L(w(t+1))/parenleftig
1−ηβL
2/parenrightig
cn(Ln(w(t))−L∞)θn
=1
cn(1−ηβL
2)/integraldisplayLn(w(t))
Ln(w(t+1))1
/parenleftbig
Ln(w(t))−L∞/parenrightbigθndLn(w)
≤1
cn(1−ηβL
2)/integraldisplayLn(w(t))
Ln(w(t+1))1
(Ln(w)−L∞)θndLn(w)
=1
cn(1−θn)(1−ηβL
2)/bracketleftbigg/parenleftig
Ln(w(t))−L∞/parenrightig1−θn
−/parenleftig
Ln(w(t+1))−L∞/parenrightig1−θn/bracketrightbigg
.
Now we look at the sequence w(0),w(1)...,w(t),.... Notice thatLn(w(t))converges toL∞, hence
(Ln(w(t))−L∞)1−θn
cn(1−θn)(1−ηβL/2)converges to zero as tgoes to infinity. Therefore, the sequence(Ln(w(t))−L∞)1−θn
cn(1−θn)(1−ηβL/2)is a
Cauchy sequence. Then for any ε>0, there isM > 0such that
/parenleftbig
Ln(w(t))−L∞/parenrightbig1−θn
cn(1−θn)(1−ηβL/2)−/parenleftbig
Ln(w(s))−L∞/parenrightbig1−θn
cn(1−θn)(1−ηβL/2)<ε,∀s>t>M.
Hence∀s>t>M ,
/vextenddouble/vextenddouble/vextenddoublew(s)−w(t)/vextenddouble/vextenddouble/vextenddouble≤s−1/summationdisplay
i=t/vextenddouble/vextenddouble/vextenddoublew(i+1)−w(i)/vextenddouble/vextenddouble/vextenddouble≤/parenleftbig
Ln(w(t))−L∞/parenrightbig1−θn
cn(1−θn)(1−ηβL/2)−/parenleftbig
Ln(w(s))−L∞/parenrightbig1−θn
cn(1−θn)(1−ηβL/2)<ε.
Therefore, w(0),w(1)...,w(t),...is a Cauchy sequence in the Euclidean space with ℓ2metric. Thus, w(t)
converges to some limit w(∞), which is a stationary point with Ln(w(∞)) =L∞. Then we can replace L∞
toLn(w(∞))in all the above equations. Finally, we have
∥w(0)−w(t)∥≤1
cn(1−θn)(1−ηβL
2)/bracketleftbigg/parenleftig
Ln(w(0))−Ln(w(∞))/parenrightig1−θn
−/parenleftig
Ln(w(t))−Ln(w(∞))/parenrightig1−θn/bracketrightbigg
.
D Proof of Theorem 2.3
In this section, we will prove Theorem 2.3. This proof is based on the Rademacher complexity theory and
the covering number of ℓ2balls. The key idea is to use the Uniform-LGI condition to bound the gradient
flow path length/integraltextT
0/vextenddouble/vextenddouble/vextenddoubledw(t)
dt/vextenddouble/vextenddouble/vextenddoubledt. Then since the path length is always an upper bound for the distance, the
parameterw(T)lies in a norm-constrained parameter space/braceleftig
w:/vextenddouble/vextenddoublew−w(0)/vextenddouble/vextenddouble≤/integraltextT
0/vextenddouble/vextenddouble/vextenddoubledw(t)
dt/vextenddouble/vextenddouble/vextenddoubledt/bracerightig
. This allows
us to use Rademacher complexity theory to obtain the generalization results.
Now we introduce some known technical lemmas that are used to build our proof. In the first lemma, we
derive an explicit estimate for the gradient flow path length over the random choice of the training sample
S.
22Published in Transactions on Machine Learning Research (10/2022)
Lemma D.1. Under the same conditions and notations in Theorem 2.3, let Tbe the time when the empirical
lossLn(w(T)) =εLn(w(0))for someε∈[0,1], then with probability at least 1−δ/2overSwe have
/integraldisplayT
0/vextenddouble/vextenddouble/vextenddouble/vextenddoubledw(t)
dt/vextenddouble/vextenddouble/vextenddouble/vextenddoubledt≤/parenleftbig
Mδ−¯Mδ/parenrightbig1−θn−/parenleftbig
ε−¯Mδ/parenrightbig1−θn
cn(1−θn).
Proof.By the Uniform-LGI condition, we have
dLn(w(t))
dt=/angbracketleftbigg
∇Ln(w(t)),dw(t)
dt/angbracketrightbigg
=−/vextenddouble/vextenddouble/vextenddouble∇Ln(w(t))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubledw(t)
dt/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤−cn/parenleftig
Ln(w(t))−Ln(w(∞))/parenrightigθn/vextenddouble/vextenddouble/vextenddouble/vextenddoubledw(t)
dt/vextenddouble/vextenddouble/vextenddouble/vextenddouble.
Hence with probability at least 1−δ/2overS,
/integraldisplayT
0/vextenddouble/vextenddouble/vextenddouble/vextenddoubledw(t)
dt/vextenddouble/vextenddouble/vextenddouble/vextenddoubledt≤/integraldisplayT
0−1
cn/parenleftig
Ln(w(t))−Ln(w(∞))/parenrightig−θn
dLn(w(t))
=1
cn(1−θn)/bracketleftbigg/parenleftig
Ln(w(0))−Ln(w(∞))/parenrightig1−θn
−/parenleftig
Ln(w(T))−Ln(w(∞))/parenrightig1−θn/bracketrightbigg
=1
cn(1−θn)/bracketleftbigg/parenleftig
Ln(w(0))−Ln(w(∞))/parenrightig1−θn
−/parenleftig
εLn(w(0))−Ln(w(∞))/parenrightig1−θn/bracketrightbigg
≤1
cn(1−θn)/bracketleftbigg/parenleftig
Mδ−Ln(w(∞))/parenrightig1−θn
−/parenleftig
εMδ−Ln(w(∞))/parenrightig1−θn/bracketrightbigg
≤1
cn(1−θn)/bracketleftig/parenleftbig
Mδ−¯Mδ/parenrightbig1−θn−/parenleftbig
εMδ−¯Mδ/parenrightbig1−θn/bracketrightig
.
The last inequality is by the fact that the function f(x) = (t−x)1−θ−(s−x)1−θis non-decreasing on [0,s]
given thatt≥s.
The second lemma gives a generalization bound of a function class based on the Rademacher complexity,
which is proved in Mohri et al. (2018).
Lemma D.2. Consider a family of functions Fmapping fromZto[a,b]. LetDdenote the distribution
according to which samples are drawn. Then for any δ >0, with probability at least 1−δover the draw of
an i.i.d. sample S={z1,...,zn}of sizen, the following holds for all f∈F:
Ez∼D[f(z)]−1
nn/summationdisplay
i=1f(zi)≤2RS(F) + 3(b−a)/radicalbigg
log(2/δ)
2n,
whereRS(F)is the empirical Rademacher complexity with respect to the sample S, defined as:
RS(F) =Eσ/bracketleftigg
sup
f∈F1
nn/summationdisplay
i=1σif(zi)/bracketrightigg
.
Here{σi}n
i=1are i.i.d. random variables drawn from U{−1,1}.
In the next lemma, we prove a shifted version of the Ledoux-Talagrand contraction inequality (Ledoux &
Talagrand, 2013), which is useful to bound the length-based Rademacher complexity.
23Published in Transactions on Machine Learning Research (10/2022)
Lemma D.3 (Shifted contraction inequality) .Letg:R− →Rbe a convex and increasing function. Let
ϕi:R− →RbeL-Lipschitz functions, then for any bounded set T⊂Rand anyt(0)∈R, we have
Eσ
g
 sup
(t−t(0))∈Tn/summationdisplay
i=1σi/parenleftig
ϕi(ti)−ϕi/parenleftig
t(0)
i/parenrightig/parenrightig

≤Eσ
g
L sup
(t−t(0))∈Tn/summationdisplay
i=1σi/parenleftig
ti−t(0)
i/parenrightig

,
and
Eσ
sup
(t−t(0))∈T/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1σi/parenleftig
ϕi(ti)−ϕi/parenleftig
t(0)
i/parenrightig/parenrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤2LEσ
sup
(t−t(0))∈T/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1σi/parenleftig
ti−t(0)
i/parenrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
.
The special case for t(0)= 0corresponds to the original Ledoux-Talagrand contraction inequality. Here we
prove a shifted version.
Proof.First notice that
Eσ
g
 sup
(t−t(0))∈Tn/summationdisplay
i=1σi/parenleftig
ϕi(ti)−ϕi/parenleftig
t(0)
i/parenrightig/parenrightig

=Eσ1,...,σn−1
Eσn
g
 sup
(t−t(0))∈Tn/summationdisplay
i=1σi/parenleftig
ϕi(ti)−ϕi/parenleftig
t(0)
i/parenrightig/parenrightig


.
Letun−1(t) =/summationtextn−1
i=1σi/parenleftig
ϕi(ti)−ϕi/parenleftig
t(0)
i/parenrightig/parenrightig
, then
Eσn
g
 sup
(t−t(0))∈Tn/summationdisplay
i=1σi/parenleftig
ϕi(ti)−ϕi/parenleftig
t(0)
i/parenrightig/parenrightig


=1
2g
 sup
(t−t(0))∈Tun−1(t) +/parenleftig
ϕn(tn)−ϕn/parenleftig
t(0)
n/parenrightig/parenrightig
+1
2g
 sup
(t−t(0))∈Tun−1(t)−/parenleftig
ϕn(tn)−ϕn/parenleftig
t(0)
n/parenrightig/parenrightig
.
Suppose that the above two suprema can be reached at t′and˜trespectively, i.e.,
sup
(t−t(0))∈Tun−1(t) +/parenleftig
ϕn(tn)−ϕn/parenleftig
t(0)
n/parenrightig/parenrightig
=un−1(t′) +/parenleftig
ϕn(t′
n)−ϕn/parenleftig
t(0)
n/parenrightig/parenrightig
;
sup
(t−t(0))∈Tun−1(t)−/parenleftig
ϕn(tn)−ϕn/parenleftig
t(0)
n/parenrightig/parenrightig
=un−1/parenleftbig˜t/parenrightbig
−/parenleftig
ϕn/parenleftbig˜tn/parenrightbig
−ϕn/parenleftig
t(0)
n/parenrightig/parenrightig
.
Otherwise we add an arbitrary positive number εin the above equations. Therefore,
Eσn
g
 sup
(t−t(0))∈Tn/summationdisplay
i=1σi/parenleftig
ϕi(ti)−ϕi/parenleftig
t(0)
i/parenrightig/parenrightig


=1
2/bracketleftig
g/parenleftig
un−1(t′) +/parenleftig
ϕn(t′
n)−ϕn/parenleftig
t(0)
n/parenrightig/parenrightig/parenrightig/bracketrightig
+1
2/bracketleftig
g/parenleftig
un−1/parenleftbig˜t/parenrightbig
−/parenleftig
ϕn/parenleftbig˜tn/parenrightbig
−ϕn/parenleftig
t(0)
n/parenrightig/parenrightig/parenrightig/bracketrightig
.
Without loss of generality, we assume
un−1(t′) +/parenleftig
ϕn(t′
n)−ϕn/parenleftig
t(0)
n/parenrightig/parenrightig
≥un−1/parenleftbig˜t/parenrightbig
+/parenleftig
ϕn/parenleftbig˜tn/parenrightbig
−ϕn/parenleftig
t(0)
n/parenrightig/parenrightig
;
un−1/parenleftbig˜t/parenrightbig
−/parenleftig
ϕn/parenleftbig˜tn/parenrightbig
−ϕn/parenleftig
t(0)
n/parenrightig/parenrightig
≥un−1(t′)−/parenleftig
ϕn(t′
n)−ϕn/parenleftig
t(0)
n/parenrightig/parenrightig
.(15)
24Published in Transactions on Machine Learning Research (10/2022)
For the other cases, the method remains the same. We set
a=un−1/parenleftbig˜t/parenrightbig
−/parenleftig
ϕn/parenleftbig˜tn/parenrightbig
−ϕn/parenleftig
t(0)
n/parenrightig/parenrightig
,
b=un−1/parenleftbig˜t/parenrightbig
−L/parenleftig
˜tn−t(0)
n/parenrightig
,
a′=un−1(t′) +L/parenleftig
t′
n−t(0)
n/parenrightig
,
b′=un−1(t′) +/parenleftig
ϕn(t′
n)−ϕn/parenleftig
t(0)
n/parenrightig/parenrightig
.
Now our goal is to prove:
g(a)−g(b)≤g(a′)−g(b′). (16)
Considering the following four cases:
1.t′
n≥t(0)
nand˜tn≥t(0)
n. By the Lipschitzness of ϕnand equation (15) we know a≥b,b′≥b, and
(a−b)−(a′−b′) =ϕn(t′
n)−ϕn/parenleftbig˜tn/parenrightbig
−L/parenleftbig
t′
n−˜tn/parenrightbig
.
Ift′
n≥˜tn, we can get a−b≤a′−b′. By the fact that gis convex and increasing, we have g(y+x)−g(x)
is increasing in yfor everyx≥0. Hence for x=a−b,
g(a)−g(b) =g(b+x)−g(b)≤g(b′+x)−g(b′)≤g(a′)−g(b′).
Ift′
n<˜tn, we change ϕninto−ϕnand switch t′and˜t, and the proof is similar.
2.t′
n≤t(0)
nand˜tn≤t(0)
n. Similarly, by changing the signs we can get the same result.
3.t′
n≥t(0)
nand˜tn≤t(0)
n. For this case we have a≤bandb′≤a′, sog(a) +g(b′)≤g(b) +g(a′).
4.t′
n≤t(0)
nand˜tn≥t(0)
n. For this case we can change ϕnto−ϕn, then we have a≥banda′≤b′, and
finally we get g(a) +g(b′)≤g(b) +g(a′).
Thus equation (16) yields that
Eσn
g
 sup
(t−t(0))∈Tn/summationdisplay
i=1σi/parenleftig
ϕi(ti)−ϕi/parenleftig
t(0)
i/parenrightig/parenrightig


=1
2/bracketleftig
g/parenleftig
un−1(t′) +/parenleftig
ϕn(t′
n)−ϕn/parenleftig
t(0)
n/parenrightig/parenrightig/parenrightig/bracketrightig
+1
2/bracketleftig
g/parenleftig
un−1/parenleftbig˜t/parenrightbig
−/parenleftig
ϕn/parenleftbig˜tn/parenrightbig
−ϕn/parenleftig
t(0)
n/parenrightig/parenrightig/parenrightig/bracketrightig
≤1
2/bracketleftig
g/parenleftig
un−1(t′) +L/parenleftig
t′
n−t(0)
n/parenrightig/parenrightig/bracketrightig
+1
2/bracketleftig
g/parenleftig
un−1/parenleftbig˜t/parenrightbig
−L/parenleftig
˜tn−t(0)
n/parenrightig/parenrightig/bracketrightig
≤1
2
g
 sup
(t−t(0))∈Tun−1(t) +L/parenleftig
tn−t(0)
n/parenrightig

+1
2
g
 sup
(t−t(0))∈Tun−1(t)−L/parenleftig
tn−t(0)
n/parenrightig


=Eσn
g
 sup
(t−t(0))∈Tun−1(t) +σnL/parenleftig
tn−t(0)
n/parenrightig


Applying the same method to σn−1,...,σ 1successively, we obtain the first inequality
Eσ
g
 sup
(t−t(0))∈Tn/summationdisplay
i=1σi/parenleftig
ϕi(ti)−ϕi/parenleftig
t(0)
i/parenrightig/parenrightig

≤Eσ
g
L sup
(t−t(0))∈Tn/summationdisplay
i=1σi/parenleftig
ti−t(0)
i/parenrightig

.
25Published in Transactions on Machine Learning Research (10/2022)
For the second inequality, since |x|= [x]++ [x]−with [x]+= max(0,x)and[x]−= max(0,−x),
Eσ
sup
(t−t(0))∈T/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1σi/parenleftig
ϕi(ti)−ϕi/parenleftig
t(0)
i/parenrightig/parenrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle

≤Eσ
sup
(t−t(0))∈T/bracketleftiggn/summationdisplay
i=1σi/parenleftig
ϕi(ti)−ϕi/parenleftig
t(0)
i/parenrightig/parenrightig/bracketrightigg
+
+Eσ
sup
(t−t(0))∈T/bracketleftiggn/summationdisplay
i=1σi/parenleftig
ϕi(ti)−ϕi/parenleftig
t(0)
i/parenrightig/parenrightig/bracketrightigg
−

=2Eσ
sup
(t−t(0))∈T/bracketleftiggn/summationdisplay
i=1σi/parenleftig
ϕi(ti)−ϕi/parenleftig
t(0)
i/parenrightig/parenrightig/bracketrightigg
+
,
where the last equality is by [−x]−= [x]+andσhas the same distribution with −σ.
A simple fact is that
sup
(t−t(0))∈T/bracketleftiggn/summationdisplay
i=1σi/parenleftig
ϕi(ti)−ϕi/parenleftig
t(0)
i/parenrightig/parenrightig/bracketrightigg
+=
sup
(t−t(0))∈Tn/summationdisplay
i=1σi/parenleftig
ϕi(ti)−ϕi/parenleftig
t(0)
i/parenrightig/parenrightig

+.
Since max(0,x)is convex and increasing, then by the first inequality we have
Eσ
sup
(t−t(0))∈T/bracketleftiggn/summationdisplay
i=1σi/parenleftig
ϕi(ti)−ϕi/parenleftig
t(0)
i/parenrightig/parenrightig/bracketrightigg
+
=Eσ

sup
(t−t(0))∈Tn/summationdisplay
i=1σi/parenleftig
ϕi(ti)−ϕi/parenleftig
t(0)
i/parenrightig/parenrightig

+

≤Eσ

L sup
(t−t(0))∈Tn/summationdisplay
i=1σi/parenleftig
ti−t(0)
i/parenrightig

+

≤LEσ
sup
(t−t(0))∈T/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1σi/parenleftig
ti−t(0)
i/parenrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle

This completes the proof.
Now we apply Lemma D.3 to bound the empirical Rademacher complexity of an element-wise distance
constrained function class. In the following lemma, all the notations are consistent with Theorem 2.3 unless
stated otherwise.
Lemma D.4. Given a function class Fa,b:={x∝⇕⊣√∫⊔≀→f(w,x) :w∈Sa,b}and sample S={x1,...,xn}with
∥xi∥= 1for alli∈[n], then we have
RS(Fa,b)≤/radicaligg
∥a∥2+∥b∥2
n∥LΨ(Sa,b)∥.
Proof.By definition,
nRS(Fa,b) =Eσ/bracketleftigg
sup
w∈Sa,bn/summationdisplay
i=1σif(w,xi)/bracketrightigg
=Eσ/bracketleftigg
sup
w∈Sa,bn/summationdisplay
i=1σif(w,xi)/bracketrightigg
−Eσ/bracketleftiggn/summationdisplay
i=1σif(0,xi)/bracketrightigg
=Eσ/bracketleftigg
sup
w∈Sa,bn/summationdisplay
i=1σi(f(w,xi)−f(0,xi))/bracketrightigg
.
26Published in Transactions on Machine Learning Research (10/2022)
Now we decompose the term f(w,xi)−f(0,xi)as:
f(w,xi)−f(0,xi)
=Ψ/parenleftbig
x⊤
iα1,...,x⊤
iαp,β1,...,βq/parenrightbig
−Ψ (0,..., 0,0,..., 0)
=(Ψ(x⊤
iα1,...,x⊤
iαp,β1,...,βq)−Ψ(0,...,x⊤
iαp,β1,...,βq))+(Ψ(0,...,x⊤
iαp,β1,...,βq)−Ψ(0,0,...,x⊤
iαp,β1,...,βq))
+···+ (Ψ (0,..., 0,0,..., 0,βq)−Ψ (0,..., 0,0,..., 0)).
Then by the above decomposition and Lemma D.3, we have
Eσ/bracketleftigg
sup
w∈Sa,bn/summationdisplay
i=1σi(f(w,xi)−f(0,xi))/bracketrightigg
≤L(1)
Ψ(Sa,b)Eσ/bracketleftigg
sup
w∈Sa,bn/summationdisplay
i=1σix⊤
iα1/bracketrightigg
+···+L(p+q)
Ψ (Sa,b)Eσ/bracketleftigg
sup
w∈Sa,bn/summationdisplay
i=1σiβq/bracketrightigg
.
Notice that
Eσ/bracketleftigg
sup
w∈Sa,bn/summationdisplay
i=1σix⊤
iα1/bracketrightigg
=Eσ/bracketleftigg
sup
∥α1∥≤a1n/summationdisplay
i=1σix⊤
iα1/bracketrightigg
≤a1Eσ/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1σixi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/bracketrightigg
≤a1/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtEσ
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1σixi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

=a1√n.
And
Eσ/bracketleftigg
sup
w∈Sa,bn/summationdisplay
i=1σiβq/bracketrightigg
=Eσ/bracketleftigg
sup
|βq|≤bqn/summationdisplay
i=1σiβq/bracketrightigg
≤bqEσ/bracketleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1σi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracketrightigg
≤bq/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtEσ
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1σi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2

=bq√n.
Therefore, by the Cauchy-Schwarz inequality, we can get
Eσ/bracketleftigg
sup
w∈Sa,bn/summationdisplay
i=1σi(f(w,xi)−f(0,xi))/bracketrightigg
≤L(1)
Ψ(Sa,b)a1√n+···+L(p+q)
Ψ (Sa,b)bq√n
≤/radicalbigg
n/parenleftig
∥a∥2+∥b∥2/parenrightig
∥LΨ(Sa,b)∥.
Finally, we have
RS(Fa,b)≤/radicaligg
∥a∥2+∥b∥2
n∥LΨ(Sa,b)∥.
27Published in Transactions on Machine Learning Research (10/2022)
Lemma D.4 gives an upper bound of the Rademacher complexity based on the element-wise distance. To
obtain a norm-based generalization bound, we consider to use Sa,bto cover the norm-constrained space
{w:∥w∥≤R}, and then taking a union bound. For the ℓ2ball covering number, we use the following result
from (Neyshabur et al., 2019, Lemma 11).
Lemma D.5. Given anyϵ,D,β > 0, consider the set SD
β=/braceleftbig
x∈RD:∥x∥≤β/bracerightbig
.Then there exist Nsets
{Ti}N
i=1of the form Ti=/braceleftbig
x∈RD:|xj|≤αi
j,∀j∈[D]/bracerightbig
such thatSD
β⊆/uniontextN
i=1Tiand/vextenddouble/vextenddoubleαi/vextenddouble/vextenddouble≤β(1 +ϵ),∀i∈
[N]whereN=/parenleftbigK+D−1
D−1/parenrightbig
and
K=/ceilingleftbiggD
(1 +ϵ)2−1/ceilingrightbigg
.
Lemma D.6. For any two positive integers n,kwithn≥k, we have
/parenleftbiggn
k/parenrightbigg
≤/parenleftigen
k/parenrightigk
.
Proof.Note that/parenleftbiggn
k/parenrightbigg
=n!
k!(n−k)!≤nk
k!≤ek/parenleftign
k/parenrightigk
.
The last step is by
ek=∞/summationdisplay
i=0ki
i!≥kk
k!.
Now combining Lemma D.1, D.2, D.4, D.5 and D.6, we are ready to prove Theorem 2.3.
Proof of Theorem 2.3. Letrn,δ,ε =(Mδ−¯Mδ)1−θn−(ε−¯Mδ)1−θn
cn(1−θn), then by Lemma D.1, we may apply Lemma
D.5 withϵ=√
2−1,D=p+q, andβ=rn,δ,ε, then there exist NsetsSak,bksuch thatSD
β⊆/uniontextN
k=1Sak,bk
and/radicalig
∥ak∥2+∥bk∥2≤√
2β, withN=/parenleftbig2D−1
D−1/parenrightbig
.
Therefore, for each parameter space Sak,bk, by Lemma D.4 we have
RS(Fak,bk)≤/radicalbigg
2
nβ/vextenddouble/vextenddoubleLΨ(Sak,bk)/vextenddouble/vextenddouble.
Notice that the local Lipschitz constant of ℓinSak,bkisLℓ(Sak,bk). Hence, by Lemma D.2 and the Ledoux-
Talagrand contraction inequality, for any δ∈(0,1), with probability at least 1−δ/2Nover the training
sample, the following holds for all w∈Sak,bk:
LD(wε)≤ε+2√
2βLℓ(Sak,bk)/vextenddouble/vextenddoubleLΨ(Sak,bk)/vextenddouble/vextenddouble
√n+ 3Mβ/radicalbigg
log(4N/δ)
2n,
whereMβ= sup∥ak∥2+∥bk∥2≤2β2supw∈Sak,bk,∥x∥≤1,|y|≤1ℓ(f(w,x),y).
Sincewε−w(0)∈SD
β⊆/uniontextN
k=1Sak,bkwith probability at least 1−δ/2overS, by taking the union bound
over all setsSak,bk, we can get with probability at least 1−δover the training samples,
LD(wε)≤ε+ sup
∥a∥2+∥b∥2≤2β22√
2βLℓ(Sa,b)∥LΨ(Sa,b)∥√n+ 3Ma,b/radicalbigg
log(4N/δ)
2n.
Thus it remains to bound the term logN. ForD= 1,N= 1. ForD≥2, by Lemma D.6,
logN≤(D−1) log/parenleftbigge(2D−1)
D−1/parenrightbigg
<2.1(D−1)<3D= 3(p+q).
28Published in Transactions on Machine Learning Research (10/2022)
This completes the proof of Theorem 2.3.
E Proofs for Section 3
In this section, our goal is to prove all the theorems in Section 3. A crucial part of the proofs is the spectral
analysis of the random matrix X. Therefore, we start with introducing the non-asymptotic results of the
smallest and largest eigenvalues of subguassian matrices.
The first result is from (Rudelson & Vershynin, 2010, Proposition 2.4), characterizing the non-asymptotic
behavior of the largest singular value of subgaussian matrices.
Lemma E.1. LetAbe anN×nrandom matrix whose entries are independent mean zero subgaussian
random variables whose subgaussian moments are bounded by 1. Then for every t≥0, with probability at
least 1−2e−ct2over the randomness of the entries,
/radicalig
λmax(AA⊤)≤C(√
N+√n) +t,
wherecandCare two positive constants that depend only on the subgaussian moment of the entries.
The second result is from (Rudelson & Vershynin, 2009, Theorem 1.1), characterizing the non-asymptotic
behavior of the smallest singular value of subgaussian matrices.
Lemma E.2. LetAbe anN×nrandom matrix whose entries are independent and identically distributed
subgaussian random variables with zero mean and unit variance. If N > n, then for every ε > 0, with
probability at least 1−(C1ε)N−n+1−cN
1over the randomness of the entries,
/radicalig
λmin(A⊤A)≥ε(√
N−√
n−1),
whereC1>0andc1∈(0,1)depend only on the subgaussian moment of the entries.
In the next we introduce a useful lemma to bound λmax(XX⊤)andλmin(XX⊤).
Lemma E.3. LetX∈Rn×dbe a matrix with full row rank, Σd∈Rd×dbe a non-singular matrix, then
λmax(XX⊤)≤λmax(XΣdΣ⊤
dX⊤)
λmin(ΣdΣ⊤
d)
λmin(XX⊤)≥λmin(XΣdΣ⊤
dX⊤)
λmax(ΣdΣ⊤
d).
Proof.SinceXhas full row rank and Σdis non-singular, then XΣdalso has full row rank. Notice that
λmax(XX⊤) = sup
∥v∥=1/vextenddouble/vextenddoublev⊤XΣdΣ−1
d/vextenddouble/vextenddouble2≤sup
∥v∥=1/vextenddouble/vextenddoublev⊤XΣd/vextenddouble/vextenddouble2/vextenddouble/vextenddoubleΣ−1
d/vextenddouble/vextenddouble2.
Note that
∥Σ−1
d∥= sup
v̸=0∥Σ−1
dv∥
∥v∥= sup
Σ−1
dv̸=0∥Σ−1
dv∥
∥v∥= sup
v̸=0∥v∥
∥Σdv∥=1/radicalig
λmin(ΣdΣ⊤
d).
Thus,
λmax(XX⊤)≤sup∥v∥=1/vextenddouble/vextenddoublev⊤XΣd/vextenddouble/vextenddouble2
λmin(ΣdΣ⊤
d)=λmax(XΣdΣ⊤
dX⊤)
λmin(ΣdΣ⊤
d).
29Published in Transactions on Machine Learning Research (10/2022)
For the smallest eigenvalue, note that
λmin(XX⊤) = inf
∥v∥=1/vextenddouble/vextenddoublev⊤XΣdΣ−1
d/vextenddouble/vextenddouble2
= inf
∥v∥=1/vextenddouble/vextenddoublev⊤XΣdΣ−1
d/vextenddouble/vextenddouble2
∥v⊤XΣd∥2/vextenddouble/vextenddoublev⊤XΣd/vextenddouble/vextenddouble2
≥inf
∥v∥=1/vextenddouble/vextenddoublev⊤XΣdΣ−1
d/vextenddouble/vextenddouble2
∥v⊤XΣd∥2inf
∥v∥=1/vextenddouble/vextenddoublev⊤XΣd/vextenddouble/vextenddouble2
≥inf
v̸=0∥v⊤Σ−1
d∥2
∥v∥2λmin(XΣdΣ⊤
dX⊤)
= inf
v̸=0∥v∥2
∥Σ⊤
dv∥2λmin(XΣdΣ⊤
dX⊤)
=λmin(XΣdΣ⊤
dX⊤)
λmax(ΣdΣ⊤
d).
E.1 Proof of Theorem 3.3
In this section, we will prove Theorem 3.3. All the notations are consistent with Theorem 3.3 unless stated
otherwise.
Proof of Theorem 3.3. For theℓ2linear regression loss function Ln(w), notice that
∇Ln(w) =1
nn/summationdisplay
i=1/parenleftbig
w⊤xi−yi/parenrightbig
xi=1
nX⊤(Xw−Y).
Then sinceXhas full row rank, we have ∀w∈Rd,
∥∇Ln(w)∥=1
n/vextenddouble/vextenddoubleX⊤(Xw−Y)/vextenddouble/vextenddouble
≥/radicalbig
λmin(XX⊤)
n∥Xw−Y∥
=/radicalbigg
2λmin(XX⊤)
nLn(w)1/2.
Since the optimal loss value is zero for underdetermined linear regression, we have
cn=/radicalbigg
2λmin(XX⊤)
n, θn= 1/2.
The convergence rate can be proved by directly plugging cnandθninto Theorem 2.2.
Next, we prove the generalization bound for wε. Notice that for the linear hypothesis function fand the
Lipschitz function ˜ℓ, we haveLℓ(Sa,b) =Ma,b= 1,∥LΨ(Sa,b)∥=√
2,p= 1,q= 0,¯Mδ= 0.
By the property of the target function, we have for any w(0)that satisfies/vextenddouble/vextenddoublew(0)/vextenddouble/vextenddouble
2≤c0,
Ln(w(0)) =1
2n/vextenddouble/vextenddouble/vextenddoubleXw(0)−Y/vextenddouble/vextenddouble/vextenddouble2
2
≤1
n/parenleftbigg/vextenddouble/vextenddouble/vextenddoubleXw(0)/vextenddouble/vextenddouble/vextenddouble2
2+∥Y∥2
2/parenrightbigg
≤c2
0+ (c∗)2
nλmax(XX⊤).
30Published in Transactions on Machine Learning Research (10/2022)
Since in the following, we only need to bound the condition number of the data matrix X, with loss of
generality, we may assume that the entries of Xhave variance 1. Now we apply Lemma E.1 with A=XΣd
andt=/radicalig
log(4/δ)
c, then we have with probability at least 1−δ/2over the samples,
/radicalig
λmax(XΣdΣ⊤
dX⊤)≤C(√n+√
d) +/radicalbigg
log(4/δ)
c, (17)
wherecandCare two positive constants that depend only on the subgaussian moment of the entries.
By Lemma E.3, we have with the same probability,
/radicalig
λmax(XX⊤)≤C(√n+√
d) +/radicalig
log(4/δ)
c√λ0. (18)
Thus,
Mδ=c2
0+ (c∗)2
λ0/parenleftigg
C/parenleftigg
1 +/radicalbigg
d
n/parenrightigg
+/radicalbigg
log(4/δ)
cn/parenrightigg2
≤c2
0+ (c∗)2
λ0/parenleftigg
C/parenleftbigg
1 +1√γ0/parenrightbigg
+/radicalbigg
log(4/δ)
cn/parenrightigg2
.
Similarly, let τ=c1∈(0,1),ε=τ/C 1>0, then Lemma E.2 implies that with probability at least 1−
τd−n+1−τdover the samples,
/radicalig
λmin(XΣdΣ⊤
dX⊤)≥τ
C1(√
d−√
n−1), (19)
whereC1>0andτ∈(0,1)depend only on the subgaussian moment of the entries.
By Lemma E.3, we have with the same probability,
/radicalig
λmin(XX⊤)≥τ
C1√λ1(√
d−√
n−1). (20)
Taking the union bound, we have with probability at least 1−δ/2−τd−n+1−τdover the training samples,
rn,δ,ε=√
2n√Mδ−√εMδ/radicalbig
λmin(XX⊤)
≤/radicalbig
2(1−ε)n√Mδ
τ
C1√λ1(√
d−√n−1)
≤/radicalbig
2(1−ε)λ1c0+c∗
√λ0/parenleftbigg
C/parenleftig
1 +1√γ0/parenrightig
+/radicalig
log(4/δ)
cn/parenrightbigg
τ
C1/parenleftbigg/radicalig
d
n−/radicalig
1−1
n/parenrightbigg
≤/radicalbig
2(1−ε)λ1c0+c∗
√λ0/parenleftbigg
C/parenleftig
1 +1√γ0/parenrightig
+/radicalig
log(4/δ)
cn/parenrightbigg
τ
C1/parenleftig
1√γ1−1/parenrightig .(21)
31Published in Transactions on Machine Learning Research (10/2022)
Notice that ℓis√l0-Lipschitz on the first argument, then by Cauchy-Schwarz inequality, we have
1
nn/summationdisplay
i=1˜ℓ(f(wε,xi),yi)≤√l0
nn/summationdisplay
i=1|f(wε,xi)−yi|
≤/radicaltp/radicalvertex/radicalvertex/radicalbtl0
nn/summationdisplay
i=1|f(wε,xi)−yi|2
≤/radicalbig
2l0Ln(wε)
=/radicalig
2l0εLn(w(0)).(22)
Combining Theorem 2.3 and the inequality (21), we have with probability at least 1−δ−τd−n+1−τdover
the training samples,
E(x,y)∼D/bracketleftbig˜ℓ(f(wε,x),y)/bracketrightbig
≤/radicalig
2l0εLn(w(0)) +4rn,δ,ε√n+ 3l0/radicalbigg
3 + log(4/δ)
2n
≤/radicalig
2l0εLn(w(0)) +4(c0+c∗)/radicalig
2(1−ε)λ1
λ0/parenleftbigg
C/parenleftig
1 +1√γ0/parenrightig
+/radicalig
log(4/δ)
cn/parenrightbigg
τ
C1/parenleftig
1√γ1−1/parenrightig√n+ 3l0/radicalbigg
3 + log(4/δ)
2n,
wherec,C,C 1>0andτ∈(0,1)depend only on the subgaussian moment of the entries. This completes the
proof of Theorem 3.3.
E.2 Proof of Theorem 3.5
In this section, we will prove Theorem 3.5. First, we present some useful lemmas for proving our results,
and then we give the proofs of Theorem 3.5 for the RBF kernel and the inner product kernel separately.
For the RBF kernel k(x,y) =ϱ(∥y−x∥), the following two lemmas give non-asymptotic bounds for
λmax(k(X,X))andλmin(k(X,X))based on the separation distance SDofX.
The first lemma is from (Diederichs & Iske, 2019, Lemma 3.1), providing an upper bound for λmax(k(X,X)).
Lemma E.4. For the RBF kernel, if ϱ:R≥0− →R≥0is a decreasing function, then
λmax(k(X,X))≤ϱ(0) + 3d∞/summationdisplay
t=1(t+ 2)d−1ϱ(t·SD), (23)
and the sum of the infinite series in equation (23) is finite if and only if ϱ(∥x∥)∈L1(Rd).
The next lemma is from (Wendland, 2004, Theorem 12.3), giving a lower bound for λmin(k(X,X)).
Lemma E.5. Suppose that kis a positive-definite RBF kernel. If ϱ(∥x∥)∈L1(Rd), one can define the
Fourier transform of ϱasˆϱ(ω) := (2π)−d/2/integraltext
Rdϱ(ω)e−ix⊤ωdω. With a decreasing function ϱ0(M)and two
constantsMd,Cddefined as
ϱ0(M) := inf
∥x∥≤2Mˆϱ(x), Md= 6.38d, Cd=1
2Γ(d/2 + 1)/parenleftbiggMd
23/2/parenrightbiggd
,
where Γis the gamma function. Then a lower bound on λmin(k(X,X))is given by
λmin(k(X,X))≥Cd·ϱ0(Md/SD)·SD−d.
32Published in Transactions on Machine Learning Research (10/2022)
Fortheinnerproductkernel, itisshowninElKarouietal.(2010)thatthekernelmatrixcanbeapproximated
by the linear combination of all-ones matrix 11⊤, sample covariance matrix and identity matrix. To obtain
non-asymptotic results on the spectra of the kernel matrix, we borrow the technique from (Liang & Rakhlin,
2020, Proposition A.2), and show the result for subgaussian entries in the next lemma.
Lemma E.6. For the inner product kernel, under Assumption 3.2, we have with probability at least 1−δ−d−2
over the entries,/vextenddouble/vextenddoublek(X,X)−klin(X,X)/vextenddouble/vextenddouble≤d−1/2/parenleftig
δ−1/2+ log0.51d/parenrightig
,
whereklin(X,X)is defined as
klin(X,X) :=/parenleftbigg
ϱ(0) +ϱ′′(0)
d/parenrightbigg
11⊤+ϱ′(0)XΣ2
dX⊤
d+ (ϱ(1)−ϱ(0)−ϱ′(0))In×n.
Proof.Notethatthesamplecovarianceof XΣdisId×d, thenbyapplying(Liang&Rakhlin,2020, Proposition
A.2) with subgaussian random entries we can prove this lemma.
Lemma E.7. Suppose that A,B∈Rn×nare two symmetric matrices, then we have
λmin(A+B)≥λmin(A) +λmin(B).
Proof.Note that for any x∈Rnwith∥x∥= 1,
x⊤(A+B)x=x⊤Ax+x⊤Bx≥λmin(A) +λmin(B).
By definition, we have
λmin(A+B) = inf
∥x∥=1x⊤(A+B)x≥λmin(A) +λmin(B),
which completes the proof.
Now we are ready to prove Theorem 3.5.
Proof of Theorem 3.5. First, notice that ∀w∈Rs,
∥∇Ln(w)∥=1
n/vextenddouble/vextenddoubleφ(X)⊤(φ(X)w−Y)/vextenddouble/vextenddouble
≥/radicalbig
λmin(φ(X)φ(X)⊤)
n∥φ(X)w−Y∥
=/radicalbigg
2λmin(k(X,X))
nLn(w)1/2.
Since the optimal loss value is zero for kernel regression, we have
cn=/radicalbigg
2λmin(k(X,X))
n, θn= 1/2.
Sincekis a positive-definite kernel, the convergence rate can be proved by directly plugging cnandθninto
Theorem 2.2.
The proof of the generalization bound is two-sided. First, since ∀x∈X,∥φ(x)∥=/radicalbig
k(x,x)≤1, then the
kernel regression model (8) can be viewed as ℓ2linear regression on inputs φ(X). Hence, Ψis an identity
33Published in Transactions on Machine Learning Research (10/2022)
function with p= 1,q= 0, andLℓ(Sa,b) =Ma,b= 1,∥LΨ(Sa,b)∥=√
2for anya,b. The optimal empirical
loss value ¯Mδ= 0. This means that we only need to bound the terms Mδandrn,δ,ε.
By the property of the target function, for any w(0)that satisfies/vextenddouble/vextenddoublew(0)/vextenddouble/vextenddouble
2≤c0, we have
Ln(w(0)) =1
2n/vextenddouble/vextenddouble/vextenddoubleφ(X)w(0)−Y/vextenddouble/vextenddouble/vextenddouble2
2
≤1
n/parenleftbigg/vextenddouble/vextenddouble/vextenddoubleφ(X)w(0)/vextenddouble/vextenddouble/vextenddouble2
2+∥Y∥2
2/parenrightbigg
≤c2
0+ (c∗)2
nλmax(k(X,X)).(24)
Forrn,δ,ε, notice that
rn,δ,ε=√
2n√Mδ−√εMδ/radicalbig
λmin(k(X,X))
≤/radicalbig
2(1−ε) (c0+c∗)/radicaligg
λmax(k(X,X))
λmin(k(X,X)).
Then for the RBF kernel with fixed input dimension d, Lemma E.4 and Lemma E.5 indicate that
λmax(k(X,X))andλmin(k(X,X))areuniformlyboundedwithboundsdependonlyon ϱ,d,q min,qmax. Hence,
there exists a positive constant C(ϱ,d,q min,qmax)that only depends on ϱ,d,q min,qmax, such that
rn,δ,ε≤/radicalbig
2(1−ε) (c0+c∗)C(ϱ,d,q min,qmax).
Therefore, by equation (22), Theorem 2.3 and Lemma D.2, with probability at least 1−δover the training
samples,
E(x,y)∼D/bracketleftbig˜ℓ(f(wε,x),y)/bracketrightbig
≤/radicalig
2εLn(w(0)) +4rn,δ,ε√n+ 3/radicalbigg
3 + log(4/δ)
2n
≤/radicalig
2l0εLn(w(0)) +4/radicalbig
2(1−ε) (c0+c∗)C(ϱ,d,q min,qmax)√n+ 3l0/radicalbigg
3 + log(4/δ)
2n,
which completes the proof for the RBF kernel.
For the inner product kernel, first notice that
λmax(k(X,X)) =∥k(X,X)∥
≤/vextenddouble/vextenddoubleklin(X,X)/vextenddouble/vextenddouble+/vextenddouble/vextenddoublek(X,X)−klin(X,X)/vextenddouble/vextenddouble.(25)
By Lemma E.7, we can get
λmin(k(X,X))≥λmin(klin(X,X)) +λmin(k(X,X)−klin(X,X))
≥λmin(klin(X,X))−/vextenddouble/vextenddoublek(X,X)−klin(X,X)/vextenddouble/vextenddouble.(26)
Under Assumption 3.2 & 3.4, Lemma E.6 implies that
/vextenddouble/vextenddoubleklin(X,X)/vextenddouble/vextenddouble≤ϱ′′(0)
d/vextenddouble/vextenddouble11⊤/vextenddouble/vextenddouble+ϱ′(0)
d/vextenddouble/vextenddoubleXΣ2
dX⊤/vextenddouble/vextenddouble+ (ϱ(1)−ϱ′(0))
≤nϱ′′(0)
d+ϱ′(0)λmax(XΣ2
dX⊤)
d+ (ϱ(1)−ϱ′(0))
≤γ1ϱ′′(0) +ϱ′(0)λmax(XΣ2
dX⊤)
d+ (ϱ(1)−ϱ′(0)),
34Published in Transactions on Machine Learning Research (10/2022)
and
λmin(klin(X,X))≥ϱ(1)−ϱ′(0)>0.
Thus, by equation (26) we have
λmin(k(X,X))≥(ϱ(1)−ϱ′(0))−/vextenddouble/vextenddoublek(X,X)−klin(X,X)/vextenddouble/vextenddouble. (27)
Under Assumption 3.2, by equation (17), we have with probability at least 1−δ/3over the samples,
λmax(XΣ2
dX⊤)
d≤/parenleftigg
C/parenleftbigg/radicalbiggn
d+ 1/parenrightbigg
+/radicalbigg
log(6/δ)
cd/parenrightigg2
≤/parenleftigg
C(√γ1+ 1) +/radicalbigg
γ1log(6/δ)
cn/parenrightigg2
.
Therefore, by equation (25), with probability at least 1−δ/3over the samples,
λmax(k(X,X))≤γ1ϱ′′(0) +ϱ′(0)/parenleftigg
C(√γ1+ 1) +/radicalbigg
γ1log(6/δ)
cn/parenrightigg2
+ϱ(1)−ϱ′(0) +/vextenddouble/vextenddoublek(X,X)−klin(X,X)/vextenddouble/vextenddouble.
(28)
By Lemma E.6, for large dand smallδsuch thatd−1/2/parenleftbig√
3δ−1/2+ log0.51d/parenrightbig
≤0.5(ϱ(1)−ϱ′(0)), we have
with probability at least 1−δ/3−d−2over the entries,
/vextenddouble/vextenddoublek(X,X)−klin(X,X)/vextenddouble/vextenddouble≤0.5(ϱ(1)−ϱ′(0)).
Then equation (27) and (28) yields that with probability at least 1−2δ/3−d−2over the samples,
λmin(k(X,X))≥0.5(ϱ(1)−ϱ′(0)),
λmax(k(X,X))≤γ1ϱ′′(0) +ϱ′(0)/parenleftigg
C(√γ1+ 1) +/radicalbigg
γ1log(4/δ)
cn/parenrightigg2
+ 1.5(ϱ(1)−ϱ′(0)).
Hence, we have with probability at least 1−2δ/3−d−2over the samples,
rn,δ,ε=√
2n√Mδ−√εMδ/radicalbig
λmin(k(X,X))
≤/radicalbig
2(1−ε) (c0+c∗)/radicaligg
λmax(k(X,X))
λmin(k(X,X))
≤/radicalbig
2(1−ε) (c0+c∗)/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtγ1ϱ′′(0) +ϱ′(0)/parenleftbigg
C/parenleftbig√γ1+ 1/parenrightbig
+/radicalig
γ1log(4/δ)
cn/parenrightbigg2
+ 1.5(ϱ(1)−ϱ′(0))
0.5(ϱ(1)−ϱ′(0)).
Therefore, there exists a constant Cthat depends on c0,c∗,γ1,ϱ′′(0),ϱ′(0),ϱ(1)and the subgaussian moment
of the entries such that with probability at least 1−2δ/3−d−2over the samples,
rn,δ,ε≤C/parenleftigg
1 +/radicalbigg
log(1/δ)
n/parenrightigg
√
1−ε.
35Published in Transactions on Machine Learning Research (10/2022)
Combining equation (22) and Theorem 2.3, we get with probability at least 1−δ−d−2over the samples,
E(x,y)∼D/bracketleftbig˜ℓ(f(wε,x),y)/bracketrightbig
≤/radicalig
2l0εLn(w(0)) +4rn,δ,ε√n+ 3l0/radicalbigg
3 + log(4/δ)
2n
≤/radicalig
2l0εLn(w(0)) +C/parenleftig
1 +/radicalbig
log(1/δ)/n/parenrightig√1−ε
√n+ 3l0/radicalbigg
3 + log(4/δ)
2n,
whereCdepends on c0,c∗,γ1,ϱ′′(0),ϱ′(0),ϱ(1)and the subgaussian moment of the entries, which completes
the proof.
E.3 Proof of Proposition 3.6
In this section, we will prove Proposition 3.6.
Proof.We consider two phases: t∈[0,T]andt∈(T,∞). First, notice that the loss function Ln(w)is a
subanalytic function, thus it satisfies the classic LGI (Bolte et al., 2007). The classic LGI yields that there
existc∗
n(Sn)>0,θ∗
n(Sn)∈[1/2,1)such that the Uniform-LGI holds on some neighbor U∗of the stationary
pointw(∞). Thus, there exists T > 0such thatLn(w)satisfies the Uniform-LGI on/braceleftbig
w(t):t∈(T,∞)/bracerightbig
withc∗
n(Sn),θ∗
n(Sn). Now we consider the gradient flow curve that is outside U∗, i.e.,t∈[0,T], where the
gradient norm has a positive infimum. Then there exist ˆcn(Sn),ˆθn(Sn)such that the Uniform-LGI holds
fort∈[0,T]. Letcn(Sn) = min(c∗
n(Sn),ˆcn(Sn))>0,θn(Sn) = max(θ∗
n(Sn),ˆθn(Sn))∈[1/2,1), thenLn(w)
satisfies Uniform-LGI along the whole gradient flow curve with cn(Sn)andθn(Sn).
For the population loss function LD(w) =/integraltext
ℓ(f(w,x),y)dµ(x,y)with the probability measure dµ(x,y)over
the data distribution D, if it is subanalytic, then it satisfies the classic LGI (Bolte et al., 2007) around the
stationary point w(∞)
D. By the same argument, there exist cD>0,θD∈[1/2,1)such thatLD(w)satisfies
the Uniform-LGI along {w(t)
D:t≥0}withcD,θD.
E.4 Proof of Theorem 3.8
In this section, we will prove Theorem 3.8 for two-layer neural networks.
Proof of Theorem 3.8. Theconvergencerate canbe directly obtained byTheorem2.2. Forthegeneralization
result, we first give an upper bound for Ln(w(0)). Notice that
Ln(w(0)) =1
2nn/summationdisplay
i=1/parenleftig
(v(0))⊤ϕ(U(0)xi)−yi/parenrightig2
≤/summationtextn
i=1/parenleftbig
(v(0))⊤ϕ(U(0)xi)/parenrightbig2+y2
i
n
≤1
nn/summationdisplay
i=1/parenleftig
(v(0))⊤ϕ(U(0)xi)/parenrightig2
+(c∗)2
nλmax(XX⊤)
≤∥v(0)∥2/summationtextn
i=1∥U(0)xi∥2
n+(c∗)2
nλmax(XX⊤)
=∥v(0)∥2∥U(0)X⊤∥2
F
n+(c∗)2
nλmax(XX⊤)
≤∥v(0)∥2∥U(0)∥2
F∥X∥2
n+(c∗)2
nλmax(XX⊤)
≤/parenleftbigg1
2∥w(0)∥2+ (c∗)2/parenrightbiggλmax(XX⊤)
n.
36Published in Transactions on Machine Learning Research (10/2022)
Then by equation (18), we have with probability at least 1−δ/2over the samples,
Ln(w(0))≤/parenleftbigg1
2∥w(0)∥2+ (c∗)2/parenrightbigg
C(1 +/radicalbig
d/n) +/radicalig
log(4/δ)
cn√λ0
2
≤/parenleftbigg1
2∥w(0)∥2+ (c∗)2/parenrightbigg
C(1 + 1/√γ0) +/radicalig
log(4/δ)
cn√λ0
2
≤/parenleftbig
∥w(0)∥2+ 2(c∗)2/parenrightbig/parenleftig
C2(1 + 1/√γ0)2+log(4/δ)
cn/parenrightig
λ0.
Therefore, we can set Mδto be
Mδ=˜C/parenleftbigg
1 +log(1/δ)
n/parenrightbigg
,
where ˜Cis a constant that depends only on depends on c∗,γ0,λ0,∥w(0)∥and the subgaussian moment of
the entries.
Next, we bound the term sup∥a∥2+∥b∥2≤2r2
n,δ,ε∥LΨ(Sa,b)∥. Note that
Ψ (s1,...,sm,t1,...,tm) =m/summationdisplay
i=1siϕ(ti).
Hence
sup
∥a∥2+∥b∥2≤2r2
n,δ,ε∥LΨ(Sa,b)∥= sup
∥a∥2+∥b∥2≤2r2
n,δ,ε/radicaltp/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1s2
i+ϕ(ti)2
≤ sup
∥a∥2+∥b∥2≤2r2
n,δ,ε/radicaltp/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1s2
i+t2
i
=√
2rn,δ,ε.(29)
Now it remains to bound rn,δ,ε. Notice that with probability at least 1−δ/2over the training samples, we
have
rn,δ,ε≤M1−θn,δ
δ−(εMδ−¯Mδ)1−θn,δ
cn,δ(1−θn,δ).
Lastly, fortheglobalLipschitzevaluationlossfunction ˜ℓandthetwo-layerneuralnetwork, wehave Lℓ(Sa,b) =
Ma,b= 1,p=q=m. By equation (22) and Theorem 2.3, we can get with probability at least 1−δover
the training samples,
E(x,y)∼D/bracketleftbig˜ℓ(f(wε,x),y)/bracketrightbig
≤/radicalig
2l0εLn(w(0))+4√n/parenleftigg/parenleftbig˜C(1 + log(1/δ)/n)/parenrightbig1−θn,δ−(εMδ−¯Mδ)1−θn,δ
cn,δ(1−θn,δ)/parenrightigg2
+3l0/radicalbigg
6m+ log(12/δ)
2n,
where ˜Cis a constant that depends only on c∗,γ0,λ0,∥w(0)∥and the subgaussian moment of the entries.
E.5 Results for overparameterized two-layer neural networks
In this section, we derive the generalization result for the overparameterized two-layer neural networks, where
our generalization bound (Theorem 2.3) becomes non-vacuous. The generalization bound is based on the
37Published in Transactions on Machine Learning Research (10/2022)
Rademacher complexity theory in Arora et al. (2019) in the NTK regime. To simplify the analysis, we only
consider the case when ε= 0, i.e., the final convergence model.
Following the setting in Du et al. (2019), we only train the hidden layer Uand leave the output layer vas
random initialization to simplify the analysis.
NTK matrix. The NTK matrix Θ(t)is defined as: Θij(t) =/angbracketleftbig
∇Uf(w(t),xi),∇Uf(w(t),xj)/angbracketrightbig
, and denote/hatwideΘ
by the limiting matrix4:/hatwideΘij=x⊤
ixjEw∼N(0,1
dId)/bracketleftbig
ϕ′(w⊤xi)ϕ′(w⊤xj)/bracketrightbig
,∀i,j∈[n].
Standard random initialization. To get a clean non-asymptotic bound for the initial loss value, we
consider the following random initialization scheme. U(0)is drawn from Gaussian N(0,1
dIm×d)andv(0)
iare
drawn i.i.d. from uniform distribution U{−1/√m,1/√m},∀i∈[m]. These random initialization schemes
are also known as Xavier initialization (Glorot & Bengio, 2010) and Kaiming initialization (He et al., 2015).
Theorem E.8. Consider an overparameterized two-layer ReLU neural network (10). For any δ∈(0,1), if
m≥poly/parenleftig
n,λ−1
min(/hatwideΘ),δ−1/parenrightig
, then we have the followings:
•With probability at least 1−δover training samples and random initialization, Ln(w(t))satisfies the
Uniform-LGI on/braceleftbig
w(t):t≥0/bracerightbig
with
cn=/radicalig
λmin(/hatwideΘ)/n, θn= 1/2.
•Ln(w(t))converges to zero linearly:
Ln(w(t))≤exp/parenleftig
−λmin(/hatwideΘ)t/n/parenrightig
Ln(w(0)).
•Under Assumption 3.2, for any target function that satisfies (7), if γ1∈(0,1), then with probability
at least 1−δ−τd−n+1−τdover the samples and random initialization,
E(x,y)∼D/bracketleftig
˜ℓ/parenleftig
f(w(∞),x),y/parenrightig/bracketrightig
≤O/parenleftigg/radicalbigg
log(n/δ)
n/parenrightigg
,
whereτ∈(0,1)depends only on the subgaussian moment of the entries.
To prove Theorem E.8, we first introduce some important lemmas for proving our final results. The first two
lemmas are used to get an estimate for the initial loss value under the random initialization.
Lemma E.9 ((Montgomery-Smith, 1990)) .If{σi}n
i=1are i.i.d. drawn from U{−1,1}, then for any x=
(x1,...,xn)⊤∈Rn, with probability at least 1−δoverσ,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1σixi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤/radicalbig
2 log(2/δ)∥x∥.
The following lemma gives a sharp bound for a Chi-square variable, which is from (Laurent & Massart, 2000,
Lemma 1).
Lemma E.10. Let(Y1,...,YD)be i.i.d. Gaussian variables, with mean 0 and variance 1. Then with
probability at least 1−δoverY,
D/summationdisplay
i=1Y2
i≤D+ 2/radicaligg
Dlog/parenleftbigg1
δ/parenrightbigg
+ 2 log/parenleftbigg1
δ/parenrightbigg
.
4Hereλmin(/hatwideΘ)changes with n.
38Published in Transactions on Machine Learning Research (10/2022)
Lemma E.11 shows that the smallest eigenvalue of the NTK matrix Θ(t)has a lower bounded given the
overparameterization, by which we can prove the optimization result. In Lemma E.12, we show that the
eigenvalues of the NTK matrix are related to the data covariance matrix. Then by combining Lemma E.13
and Lemma E.9 we can prove the generalization result.
Lemma E.11. For anyδ∈(0,1), ifm≥poly/parenleftig
n,λ−1
min(/hatwideΘ),δ−1/parenrightig
, then with probability at least 1−δover
the random initialization,
λmin(Θ(t))≥1
2λmin(/hatwideΘ),∀t≥0.
Proof.The proof is the same as the proof of (Du et al., 2019, Lemma 3.4).
Lemma E.12.
λmin(/hatwideΘ)≥λmin/parenleftbig
XX⊤/parenrightbig
/4.
Proof.Notice that for ReLU activation ϕ, a simple fact is that ϕ′(ax) =ϕ′(x)holds for any x∈Rgiven
thata>0. Therefore,
/hatwideΘij=x⊤
ixjEw∼N(0,1
dId)/bracketleftbig
ϕ′(w⊤xi)ϕ′(w⊤xj)/bracketrightbig
=x⊤
ixjEw∼N(0,Id)/bracketleftbig
ϕ′(w⊤xi)ϕ′(w⊤xj)/bracketrightbig
=x⊤
ixj(π−arccos(x⊤
ixj))
2π
=x⊤
ixj
4+x⊤
ixj
2πarcsin(x⊤
ixj)
=x⊤
ixj
4+1
2π∞/summationdisplay
k=0(2k)!
4k(k!)2(2k+ 1)(x⊤
ixj)2k+2.
Then
/hatwideΘ =XX⊤
4+1
2π∞/summationdisplay
k=0(2k)!
4k(k!)2(2k+ 1)/parenleftbig
XX⊤/parenrightbig◦(2k+2)
=XX⊤
4+1
2π∞/summationdisplay
k=0(2k)!
4k(k!)2(2k+ 1)/parenleftig
(X⊤)⊙(2k+2)/parenrightig⊤
(X⊤)⊙(2k+2),
where◦is the element-wise product, and ⊙is the Khatri-Rao product5.
Since/parenleftbig
(X⊤)⊙(2k+2)/parenrightbig⊤(X⊤)⊙(2k+2)is positive semidefinite, we have
λmin(/hatwideΘ)≥λmin/parenleftbig
XX⊤/parenrightbig
/4,
which completes the proof.
The next lemma is quoted from (Arora et al., 2019, Lemma 5.4), giving an upper bound for the empirical
Rademacher complexity if one has an accurate estimate for the distance with respect to each hidden unit.
5ForA= (a1,...,a n)∈Rm×n,B= (b1,...,b n)∈Rp×n, thenA⊙B= [a1⊗b1,...,a n⊗bn], where⊗is the Kronecker
product.
39Published in Transactions on Machine Learning Research (10/2022)
Lemma E.13. GivenR> 0, consider the following function class
F=/braceleftig
x∝⇕⊣√∫⊔≀→f(w,x) :/vextenddouble/vextenddouble/vextenddoubleur−u(0)
r/vextenddouble/vextenddouble/vextenddouble≤R(∀r∈[m]),/vextenddouble/vextenddouble/vextenddoubleU−U(0)/vextenddouble/vextenddouble/vextenddouble
F≤B/bracerightig
Then for an i.i.d. sample S={x1,...,xn}and everyB > 0, with probability at least 1−δover the random
initialization, the empirical Rademacher complexity is bounded as:
RS(F)≤B√
2n/parenleftigg
1 +/parenleftbigg2 log(2/δ)
m/parenrightbigg1/4/parenrightigg
+ 2R2√
md+R/radicalbig
2 log(2/δ).
Now we are ready to prove Theorem E.8.
Proof of Theorem E.8. By Lemma E.11, if m≥poly/parenleftig
n,λ−1
min(/hatwideΘ),δ−1/parenrightig
, then with probability at least 1−δ
over the random initialization,
/vextenddouble/vextenddouble/vextenddouble∇Ln(w(t))/vextenddouble/vextenddouble/vextenddouble=1
n/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1(f(w,xi)−yi)∇f(w(t),xi)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
=1
n/vextenddouble/vextenddouble/vextenddouble∇f(w(t),X)⊤/parenleftig
f(w(t),X)−Y/parenrightig/vextenddouble/vextenddouble/vextenddouble
=1
n/radicalig/parenleftbig
f(w(t),X)−Y/parenrightbig⊤∇f(w(t),X)∇f(w(t),X)⊤/parenleftbig
f(w(t),X)−Y/parenrightbig
≥/radicalbigg
2λmin(Θ(t))
n/radicalig
Ln(w(t))
≥/radicaligg
λmin(/hatwideΘ)
n/radicalig
Ln(w(t))
holds for any t≥0, which means that Ln(w(t))satisfies the Uniform-LGI for any t≥0with
cn=/radicalig
λmin(/hatwideΘ)/n, θn= 1/2.
For the convergence rate, by equation (11), we can directly get Ln(w(t))converges to zero with a linear
convergence rate:
Ln(w(t))≤exp/parenleftig
−λmin(/hatwideΘ)t/n/parenrightig
Ln(w(0)).
For the generalization bound, we first bound the initial loss value under the random initialization. By the
property of the target function, we have
Ln(w(0)) =1
2nn/summationdisplay
i=1/parenleftig
(v(0))⊤ϕ(U(0)xi)−yi/parenrightig2
≤/summationtextn
i=1/parenleftbig
(v(0))⊤ϕ(U(0)xi)/parenrightbig2+y2
i
n
≤1
nn/summationdisplay
i=1/parenleftig
(v(0))⊤ϕ(U(0)xi)/parenrightig2
+(c∗)2
nλmax(XX⊤).
Since the entries of v(0)are drawn i.i.d. from U{−1/√m,1/√m}, then by Lemma E.9, for each i∈[n], with
probability at least 1−δ/12noverv(0),
1
n/parenleftig
(v(0))⊤ϕ(U(0)xi)/parenrightig2
≤2
mnlog/parenleftbigg24n
δ/parenrightbigg/vextenddouble/vextenddouble/vextenddoubleϕ(U(0)xi)/vextenddouble/vextenddouble/vextenddouble2
.
40Published in Transactions on Machine Learning Research (10/2022)
Taking the union bound over all i= 1,2,...,n, we have with probability at least 1−δ/12over the random
initialization,
1
nn/summationdisplay
i=1/parenleftig
(v(0))⊤ϕ(U(0)xi)/parenrightig2
≤2 log(24n/δ)
mnn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddoubleϕ(U(0)xi)/vextenddouble/vextenddouble/vextenddouble2
=2 log(24n/δ)
mn/vextenddouble/vextenddouble/vextenddoubleϕ/parenleftig
U(0)X⊤/parenrightig/vextenddouble/vextenddouble/vextenddouble2
F
≤2 log(24n/δ)
mn/vextenddouble/vextenddouble/vextenddoubleU(0)X⊤/vextenddouble/vextenddouble/vextenddouble2
F
≤2 log(24n/δ)
mn/vextenddouble/vextenddouble/vextenddoubleU(0)/vextenddouble/vextenddouble/vextenddouble2
F∥X∥2
=2 log(24n/δ)
mn/vextenddouble/vextenddouble/vextenddoubleU(0)/vextenddouble/vextenddouble/vextenddouble2
Fλmax(XX⊤).(30)
For the Gaussian random matrix U(0)∼N (0,1
dIm×d), by Lemma E.10, we have with probability at least
1−δ/24over the random initialization,
/vextenddouble/vextenddoubleU(0)/vextenddouble/vextenddouble2
F
m≤1 + 2/radicalbigg
log(24/δ)
md+2 log(24/δ)
md. (31)
Taking the union bound of equation (18), (30) and (31), we have with probability at least 1−δ/6over the
samples and the random initialization,
Ln(w(0))≤/parenleftbigg2 log(24n/δ)
mn/vextenddouble/vextenddouble/vextenddoubleU(0)/vextenddouble/vextenddouble/vextenddouble2
F+(c∗)2
n/parenrightbigg
λmax(XX⊤)
≤/parenleftigg
2 log(24n/δ)/parenleftigg
1 + 2/radicalbigg
log(24/δ)
md+2 log(24/δ)
md/parenrightigg
+ (c∗)2/parenrightigg/parenleftigg
C(1 +/radicalbig
d/n) +/radicalbigg
log(48/δ)
cn/parenrightigg2
/λ0
≤/parenleftbigg
2 log(24n/δ)/parenleftbigg
2 +3 log(24/δ)
md/parenrightbigg
+ (c∗)2/parenrightbigg/parenleftigg
C(1 + 1/√γ0) +/radicalbigg
log(48/δ)
cn/parenrightigg2
/λ0
≤/parenleftbigg
2 log(24n/δ)/parenleftbigg
2 +3γ1log(24/δ)
n/parenrightbigg
+ (c∗)2/parenrightbigg/parenleftigg
C(1 + 1/√γ0) +/radicalbigg
log(48/δ)
cn/parenrightigg2
/λ0
wherecandCare two positive constants that depend only on the subgaussian moment of the entries.
Therefore, there exists a constant ˜Cthat depends only on γ0,γ1,λ0and the subgaussian moment of the
entries, such that with probability at least 1−δ/6over the samples and the random initialization,
Ln(w(0))≤˜Clog(n/δ). (32)
Thus, we can set Mδ=˜Clog(n/δ).
By Lemma E.12, when ε= 0,rn,δ,εcan be bounded as
rn,δ,ε= 2/radicaligg
nMδ
λmin(/hatwideΘ)≤4/radicaligg
nMδ
λmin(XX⊤).
41Published in Transactions on Machine Learning Research (10/2022)
Taking the union bound of equation (20) and (32), if m≥poly/parenleftig
n,λ−1
min(/hatwideΘ),δ−1/parenrightig
, then with probability at
least 1−τd−n+1−τd−δ/6over the samples and random initialization,
rn,δ,ε≤4/radicaligg
nMδ
λmin(XX⊤)
≤4/radicaligg
n˜Clog(n/δ)
λmin(XX⊤)
≤4/radicalig
n˜Clog(n/δ)
τ
C1√λ1(√
d−√n−1)
≤O(/radicalbig
log(n/δ)).(33)
Now we begin to bound the distance/vextenddouble/vextenddouble/vextenddoubleu(t)
r−u(0)
r/vextenddouble/vextenddouble/vextenddoublefor eachr∈[m].
Notice that
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubledu(t)
r
dt/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble=/vextenddouble/vextenddouble/vextenddouble∇u(t)
rLn(w(t))/vextenddouble/vextenddouble/vextenddouble
=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1/parenleftig
f(w(t),xi)−yi/parenrightig
vrϕ′(u(t)
rxi)xi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤1
n√mn/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsinglef(w(t),xi)−yi/vextendsingle/vextendsingle/vextendsingle
≤1√nm/vextenddouble/vextenddouble/vextenddoublef(w(t),X)−Y/vextenddouble/vextenddouble/vextenddouble
≤/radicalbigg
2Ln(w(0))
mexp/parenleftig
−λmin(/hatwideΘ)t/2n/parenrightig
.
Hence,
/vextenddouble/vextenddouble/vextenddoubleu(t)
r−u(0)
r/vextenddouble/vextenddouble/vextenddouble≤/integraldisplayt
0/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubledu(s)
r
ds/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleds≤2n
λmin(/hatwideΘ)/radicalbigg
2Ln(w(0))
m≤/radicaligg
2n
mλmin(/hatwideΘ)rn,δ,ε.
Now we apply Lemma E.13 with B=rn,δ,ε,R=/radicalig2n
mλmin(/hatwideΘ)rn,δ,ε, we get with probability at least 1−δ/12
over the random initalization,
RS(F)≤B√
2n/parenleftigg
1 +/parenleftbigg2 log(24/δ)
m/parenrightbigg1/4/parenrightigg
+ 2R2√
md+R/radicalbig
2 log(24/δ).
Then by equation (33), if m≥poly/parenleftig
n,λ−1
min(/hatwideΘ),δ−1/parenrightig
, we have with probability at least 1−τd−n+1−τd−δ/4
over the samples and random initialization,
RS(F)≤O/parenleftigg/radicalbigg
log(n/δ)
n/parenrightigg
.
Finally, by Lemma D.2, we have with probability at least 1−τd−n+1−τd−δover the samples and random
initialization,
E(x,y)∼D/bracketleftig
˜ℓ/parenleftig
f(w(∞),x),y/parenrightig/bracketrightig
≤O/parenleftigg/radicalbigg
log(n/δ)
n/parenrightigg
,
42Published in Transactions on Machine Learning Research (10/2022)
for some constant τ∈(0,1)that depend only on the subgaussian moment of the entries.
This completes the proof.
Comparison. This result is related to Arora et al. (2019), which gave an NTK-based generalization bound
for overparameterized two-layer ReLU neural networks. This result matches with theirs in the sense that we
discover some underlying functions that are provably learnable. Examples of learnable target functions in
Arora et al. (2019) include polynomials y= (β⊤x)p, non-linear activations y= cos(β⊤x)−1,y=˜ϕ(β⊤x)
with ˜ϕ(z) =z·arctan(z/2),∥β∥≤ 1. Our result, furthermore, expands the target function class that is
provably learnable since we only require ˜ϕto be Lipshcitz. In addition, they set the standard deviation
of the initialization to be at most O(1/n), whereas we use a different initialization with order O(1/√
d)
that is more often applied in practice. This result is also related to Liu et al. (2020), which proved that
overparameterized deep neural networks satisfy the PL condition. Further, we extend this work by analyzing
the generalization.
43