Under review as submission to TMLR
Controlling Confusion via Generalisation Bounds
Anonymous authors
Paper under double-blind review
Abstract
We establish new generalisation bounds for multiclass classification by abstracting to a more
general setting of discretised error types. Extending the PAC-Bayes theory, we are hence
able to provide fine-grained bounds on performance for multiclass classification, as well as
applications to other learning problems including discretisation of regression losses. Tractable
training objectives are derived from the bounds. The bounds are uniform over all weightings
of the discretised error types and thus can be used to bound weightings not foreseen at
training, including the full confusion matrix in the multiclass classification case.
1 Introduction
Generalisation bounds are a core component of the theoretical understanding of machine learning algorithms.
For over two decades now, the PAC-Bayesian theory has been at the core of studies on generalisation abilities
of machine learning algorithms. PAC-Bayes originates in the seminal work of McAllester (1998; 1999) and
was further developed by Catoni (2003; 2004; 2007), among other authors—we refer to the recent surveys
Guedj (2019) and Alquier (2021) for an introduction to the field. The outstanding empirical successes of deep
neural networks in the past decade call for better theoretical understanding of deep learning, and PAC-Bayes
emerged as one of the few frameworks allowing the derivation of meaningful (and non-vacuous) generalisation
bounds for neural networks: the pioneering work of Dziugaite & Roy (2017) has been followed by a number
of contributions, including Neyshabur et al. (2018), Zhou et al. (2019), Letarte et al. (2019), Pérez-Ortiz et al.
(2021); Perez-Ortiz et al. (2021) and Biggs & Guedj (2021; 2022a;b), to name but a few.
Much of the PAC-Bayes literature focuses on the case of binary classification, or of multiclass classification
where one only distinguishes whether each classification is correct or incorrect. This is in stark contrast
to the complexity of contemporary real-world learning problems. This work aims to bridge this gap via
generalisation bounds that provide information rich measures of performance at test time by controlling the
probabilities of errors of any finite number of types, bounding combinations of these probabilities uniformly
over all weightings.
Previous results. We believe our framework of discretised error types to be novel. In the particular case
of multiclass classification, little is known from a theoretical perspective and, to the best of our knowledge,
only a handful of relevant strategies or generalisation bounds can be compared to the present paper. The
closest is the work of Morvant et al. (2012) on a PAC-Bayes generalisation bound on the operator norm of
the confusion matrix, to train a Gibbs classifier. We focus on a different performance metric, in the broader
setting of discretised error types. Koço & Capponi (2013) suggest to minimise the confusion matrix norm
with a focus on the imbalance between classes; their treatment is not done through PAC-Bayes. Laviolette
et al. (2017) extend the celebrated C-bound in PAC-Bayes to weighted majority votes of classifiers, to perform
multiclass classification. Benabbou & Lang (2017) present a streamlined version of some of the results from
Morvant et al. (2012) in the case where some examples are voluntarily not classified ( e.g., in the case of too
large uncertainty). More recently, Feofanov et al. (2019) derive bounds for a majority vote classifier where
the confusion matrix serves as an error indicator: they conduct a study of the Bayes classifier.
From binary to multiclass classification. A number of PAC-Bayesian bounds have been unified by
a single general bound, found in Bégin et al. (2016). Stated as Theorem 1 below, it applies to binary
classification. We use it as a basis to prove our Theorem 3, a more general bound that can be applied to,
1Under review as submission to TMLR
amongst other things, multiclass classification and discretised regression. While the proof of Theorem 3
follows similar lines to that given in Bégin et al. (2016), our generalisation to ‘soft’ hypotheses incurring any
finite number of error types requires a non-trivial extension of a result found in Maurer (2004). This extension
(Lemma 5), along with its corollary (Corollary 6) may be of independent interest. The generalisation bound
in Maurer (2004), stated below as Corollary 2, is shown in Bégin et al. (2016) to be a corollary of their bound.
In a similar manner, we derive Corollary 7 from Theorem 3. Obtaining this corollary is significantly more
involved than the analogous derivation in Bégin et al. (2016) or the original proof in Maurer (2004), requiring
a number of technical results found in Appendix B.
Briefly, the results in Bégin et al. (2016) and Maurer (2004) consider an arbitrary input set X, output set
Y={−1,1}, hypothesis space H⊆YXand i.i.d. sample S∈(X×Y )m. They then establish high probability
bounds on the discrepancy between the risk (probability of error an a new datapoint) of any stochastic classifier
Q(namely, a distribution on H) and its empirical counterpart (the fraction of the sample Qmisclassifies).
The bounds hold uniformly over all Qand contain a complexity term involving the Kullback-Leibler (KL)
divergence between Qand a reference distribution PonH(often referred to as a prior by analogy with
Bayesian inference—see the discussion in Guedj, 2019).
There are two ways in which the results in Bégin et al. (2016) and Maurer (2004) can be described as binary.
First, asYcontains two elements, this is obviously an instance of binary classification. But a more interesting
and subtle way to look at this is that only two cases are distinguished—correct classification and incorrect
classification. Specifically, since the two different directions in which misclassification can be made are counted
together, the bound gives no information on which direction is more likely.
More generally, the aforementioned bounds can be applied in the context of multiclass classification provided
one maintains the second binary characteristic by only distinguishing correct and incorrect classifications
rather than considering the entire confusion matrix. However, note that these bounds will not give information
on the relative likelihood of the different errors. In contrast, our new results can consider the entire confusion
matrix, bounding how far the true (read “expected over the data-generating distribution”) confusion matrix
differs from the empirical one, according to some metric. In fact, our results extend to the case of arbitrary
label setY, provided the number of different errors one distinguishes is finite.
Formally, we let/uniontextM
j=1Ejbe a user-specified disjoint partition of Y2into a finite number of Merror types ,
where we say that a hypothesis h∈Hmakes an error of type jon datapoint (x,y)if(h(x),y)∈Ej(by
convention, every pair (ˆy,y)∈Y2is interpreted as a predicted value ˆyfollowed by a true value y, in that
order). It should be stressed that some Ejneed not correspond to mislabellings—indeed, some of the Ej
may distinguish different correct labellings. We then count up the number of errors of each type that a
hypothesis makes on a sample, and bound how far this empirical distribution of errors is from the expected
distribution under the data-generating distribution (Theorem 3). Thus, in our generalisation, the (scalar) risk
and empirical risk ( RD(Q)andRS(Q), defined in the next section) are replaced by M-dimensional vectors
(RD(Q)andRS(Q)), and our discrepancy measure dis a divergence between discrete distributions on M
elements. Our generalisation therefore allows us to bound how far the true distribution of errors can be from
the observed distribution of errors. If we then associate a loss value ℓj∈[0,∞)to eachEjwe can derive a
bound on the total risk , defined as the sum of the true error probabilities weighted by the loss values. In fact,
the total risk is bounded with high probability uniformly over all such weightings. The loss values need not
be distinct; we may wish to understand the distribution of error types even across error types that incur the
same loss.
For example, in the case of binary classification with Y={−1,1}, we can take the usual partition into
E1={(−1,−1),(1,1)}andE2={(−1,1),(1,−1)}and loss values ℓ1= 0,ℓ2= 1, or the fine-grained
partitionY2={(0,0)}∪{ (1,1)}∪{ (0,1)}∪{ (1,0)}and the loss values ℓ1=ℓ2= 0,ℓ3= 1,ℓ4= 2. More
generally, for multiclass classification with Nclasses andY= [N], one may take the usual coarse partition
intoE1={(ˆy,y)∈Y2:ˆy=y}andE2={(ˆy,y)∈Y2:ˆy̸=y}(withℓ1= 0andℓ2= 1), or the fully refined
partition into Ei,j={(i,j)}fori,j∈[N](with correspondingly greater choice of the associated loss values),
or something in-between. Note that we still refer to Ejas an “error type” even if it contains elements that
correspond to correct classification, namely if there exists y∈Ysuch that (y,y)∈Ej. As we will see later, a
more fine-grained partition will allow more error types to be distinguished and bounded, at the expense of a
2Under review as submission to TMLR
looser bound. As a final example, for regression with Y=R, we may fix Mstrictly increasing thresholds
0 =λ1<λ2<···<λMand partitionY2intoEj={(y1,y2)∈Y2:λj≤|y1−y2|<λj+1}forj∈[M−1],
andEM={(y1,y2)∈Y2:|y1−y2|≥λM}.
Outline. We set our notation in Section 2. In Section 3 we state and prove generalisation bounds in the
setting of discretised error types: this significantly expands the previously known results from Bégin et al.
(2016) by allowing for generic output sets Y. Our main results are Theorem 3 and Corollary 7. To make our
findings profitable to the broader machine learning community we then discuss how these new bounds can be
turned into tractable training objectives in Section 4 (with a general recipe described in greater detail in
Appendix A). The paper closes with perspectives for follow-up work in Section 5 and we defer to Appendix B
the proofs of technical results.
2 Notation
For any set A, letM(A)be the set of probability measures on A. For any M∈Z>0, define
[M] :={1,2,...,M}, theM-dimensional simplex △M:={u∈[0,1]M:u1+···+uM= 1}
and its interior △>0
M:=△M∩(0,1)M. Form,M∈Z>0, define the integer counterparts Sm,M :=/braceleftbig
(k1,...,kM)∈ZM
≥0:k1+···+kM=m/bracerightbig
andS>0
m,M :=Sm,M∩ZM
>0.The setSm,Mis the domain of the
multinomial distribution with parameters m,Mand some r∈△M, which is denoted Mult (m,M, r)and has
probability mass function for k∈Sm,Mgiven by
Mult (k;m,M, r) :=/parenleftbiggm
k1k2···kM/parenrightbiggM/productdisplay
j=1rkj
j,where/parenleftbiggm
k1k2···kM/parenrightbigg
:=m!/producttextM
j=1kj!.
Forq,p∈△M, letkl(q∥p)denote the KL-divergence of Mult (1,M,q)fromMult (1,M,p), namely kl(q∥p) :=/summationtextM
j=1qjlnqj
pj, with the convention that 0ln0
x= 0forx≥0andxlnx
0=∞forx > 0. ForM= 2we
abuse notation and abbreviate kl((q,1−q)∥(p,1−p))tokl(q∥p), which is then the conventional definition of
kl(·∥·) : [0,1]2→[0,∞]found in the PAC-Bayes literature (as in Seeger, 2002, for example).
LetXandYbe arbitrary input ( e.g., feature) and output ( e.g., label) sets respectively. Let/uniontextM
j=1Ejbe a
partition ofY2into a finite sequence of Merror types , and to each Ejassociate a loss value ℓj∈[0,∞). The
only restriction we place on the loss values ℓjis that they are not all equal. This is not a strong assumption,
since if they were all equal then all hypotheses would incur equal loss and there would be no learning problem:
we are effectively ruling out trivial cases.
LetH⊆YXdenote a hypothesis class, D∈M (X×Y )a data-generating distribution and S∼Dman i.i.d.
sample of size mdrawn from D. Forh∈Handj∈[M]we define the empiricalj-riskandtruej-riskofh
to beRj
S(h) :=1
m/summationtext
(x,y)∈S 1[(h(x),y)∈Ej]andRj
D(h) :=E(x,y)∼D[ 1[(h(x),y)∈Ej]], respectively, namely,
the proportion of the sample Son whichhmakes an error of type Ejand the probability that hmakes an
error of type Ejon a new (x,y)∼D.
More generally, suppose H⊆M (Y)Xis a class of softhypotheses of the form H:X →M (Y), where,
for anyA⊆Y,H(x)[A]is interpreted as the probability according to Hthat the label of xis inA. It
is worth stressing that a soft hypothesis is still deterministic since a prediction is not drawn from the
distribution it returns. We then define the empiricalj-riskofHto beRj
S(H) :=1
m/summationtext
(x,y)∈SH(x)/bracketleftbig
{ˆy∈Y:
(ˆy,y)∈Ej}/bracketrightbig
, namely the mean—over the elements (x,y)ofS—probability mass Hassigns to predictions
ˆy∈Yincurring an error of type Ejwhen labelling each x. Further, we define the truej-riskofHto be
Rj
D(H) :=E(x,y)∼D/bracketleftbig
H(x)/bracketleftbig
{ˆy∈Y: (ˆy,y)∈Ej}/bracketrightbig/bracketrightbig
, namely the mean—over (x,y)∼D—probability mass H
assigns to predictions ˆy∈Yincurring an error of type Ejwhen labelling each x. We will see in Section 4
that the more general hypothesis class H⊆M (Y)Xis necessary for constructing a differentiable training
objective.
To each ordinary hypothesis h∈YXthere corresponds a soft hypothesis H∈M (Y)Xthat, for each
x∈X, returns a point mass on h(x). In this case, it is straightforward to show that Rj
S(h) =Rj
S(H)and
Rj
D(h) =Rj
D(H)for allj∈[M], where we have used the corresponding definitions above for ordinary and
3Under review as submission to TMLR
soft hypotheses. Since, in addition, our results hold identically for both ordinary and soft hypotheses, we
henceforth use the same notation hfor both ordinary and soft hypotheses and their associated values Rj
S(h)
andRj
D(h). It will always be clear from the context whether we are dealing with ordinary or soft hypotheses
and thus which of the above definitions of the empirical and true j-risks is being used.
We define the empirical risk andtrue risk of a (ordinary or soft) hypothesis hto be RS(h) :=
(R1
S(h),...,RM
S(h))andRD(h) := (R1
D(h),...,RM
D(h)), respectively. It is straightforward to show that
RS(h)andRD(h)are elements of△M. SinceSis drawn i.i.d. from D, the expectation of the empirical risk
is equal to the true risk, namely ES[Rj
S(h)] =Rj
D(h)for alljand thus ES[RS(h)] =RD(h). Finally, we
generalise to stochastic hypotheses Q∈M (H), which predict by first drawing a deterministic hypothesis
h∼Qand then predicting according to h, where a new his drawn for each prediction. Thus, we define the
empiricalj-riskandtruej-riskofQto be the scalars Rj
S(Q) :=Eh∼Q[Rj
S(h)]andRj
D(Q) :=Eh∼Q[Rj
D(h)],
forj∈[M], and simply the empirical risk andtrue risk ofQto be the elements of △Mdefined by
RS(Q) :=Eh∼Q[RS(h)]andRD(Q) :=Eh∼Q[RD(h)]. As before, since Sis i.i.d., we have (using Fubini this
time) that ES[RS(Q)] =RD(Q). Finally, given a loss vector ℓ∈[0,∞)M, we define the total risk ofQby
the scalarRT
D(Q) :=/summationtextM
j=1ℓjRj
D(Q). As is conventional in the PAC-Bayes literature, we refer to sample
independent and dependent distributions on M(H)(i.e.stochastic hypotheses) as priors(denotedP) and
posteriors (denotedQ) respectively, even if they are not related by Bayes’ theorem.
3 Inspiration and Main Results
We first state the existing results in Bégin et al. (2016) and Maurer (2004) that we will generalise from just
two error types (correct and incorrect) to any finite number of error types. These results are stated in terms
of the scalars RS(Q) :=1
m/summationtext
(x,y)∈S1[h(x)̸=y]andRD(Q) :=E(x,y)∼D 1[h(x)̸=y]and, as we demonstrate,
correspond to the case M= 2of our generalisations.
Theorem 1. (Bégin et al., 2016, Theorem 4) Let Xbe an arbitrary set and Y={−1,1}. LetD∈M (X×Y )
be a data-generating distribution and H⊆YXbe a hypothesis class. For any prior P∈M (H),δ∈(0,1],
convex function d: [0,1]2→R, sample size mandβ∈(0,∞), with probability at least 1−δover the random
drawS∼Dm, we have that simultaneously for all posteriors Q∈M (H)
d/parenleftbig
RS(Q),RD(Q)/parenrightbig
≤1
β/bracketleftbigg
KL(Q∥P) + lnId(m,β)
δ/bracketrightbigg
,
withId(m,β) :=supr∈[0,1]/bracketleftig/summationtextm
k=0Bin(k;m,r) exp/parenleftig
βd/parenleftbigk
m,r/parenrightbig/parenrightig/bracketrightig
, whereBin(k;m,r)is the binomial probability
mass function Bin(k;m,r) :=/parenleftbigm
k/parenrightbig
rk(1−r)m−k.
Note the original statement in Bégin et al. (2016) is for a positive integer m′, but the proof trivially generalises
to anyβ∈(0,∞). One of the bounds that Theorem 1 unifies—which we also generalise—is that of Seeger
(2002), later tightened in Maurer (2004), which we now state. It can be recovered from Theorem 1 by setting
β=mandd(q,p) =kl(q∥p) :=qlnq
p+ (1−q) ln1−q
1−p.
Corollary 2. (Maurer, 2004, Theorem 5) Let Xbe an arbitrary set and Y={−1,1}. LetD∈M (X×Y )
be a data-generating distribution and H⊆YXbe a hypothesis class. For any prior P∈M (H),δ∈(0,1]and
sample size m, with probability at least 1−δover the random draw S∼Dm, we have that simultaneously for
all posteriors Q∈M (H)
kl/parenleftbig
RS(Q),RD(Q)/parenrightbig
≤1
m/bracketleftbigg
KL(Q∥P) + ln2√m
δ/bracketrightbigg
.
We wish to bound the deviation of the empirical vector RS(Q)from the unknown vector RD(Q). Since
in general the stochastic hypothesis Qwe learn will depend on the sample S, it is useful to obtain bounds
on the deviation of RS(Q)fromRD(Q)that are uniform over Q, just as in Theorem 1 and Corollary 2.
In Theorem 1, the deviation d(RS(Q),RD(Q))between the scalars RS(Q),RD(Q)∈[0,1]is measured by
some convex function d: [0,1]2→R. In our case, the deviation d(RS(Q),RD(Q))between the vectors
RS(Q),RD(Q)∈△Mis measured by some convex function d:△2
M→R. In Section 3.2 we will derive
4Under review as submission to TMLR
Corollary 7 from Theorem 3 by selecting β=mandd(q,p) :=kl(q∥p), analogous to how Corollary 2 is
obtained from Theorem 1.
3.1 Statement and proof of the generalised bound
We now state and prove our generalisation of Theorem 1. The proof follows identical lines to that of Theorem
1 given in Bégin et al. (2016), but with additional non-trivial steps to account for the greater number of error
types and the possibility of soft hypotheses.
Theorem 3. LetXandYbe arbitrary sets and/uniontextM
j=1Ejbe a disjoint partition of Y2. LetD∈M (X×Y )
be a data-generating distribution and H⊆M (Y)Xbe a hypothesis class. For any prior P∈M (H),δ∈(0,1],
jointly convex function d:△2
M→R, sample size mandβ∈(0,∞), with probability at least 1−δover the
random draw S∼Dm, we have that simultaneously for all posteriors Q∈M (H)
d/parenleftbig
RS(Q),RD(Q)/parenrightbig
≤1
β/bracketleftbigg
KL(Q∥P) + lnId(m,β)
δ/bracketrightbigg
, (1)
whereId(m,β) :=supr∈△M/bracketleftig/summationtext
k∈Sm,MMult (k;m,M, r) exp/parenleftig
βd/parenleftbigk
m,r/parenrightbig/parenrightig/bracketrightig
. Further, the bounds are unchanged
if one restricts to an ordinary hypothesis class, namely if H⊆YX.
The proof begins on the following page after a discussion and some auxiliary results. One can derive multiple
bounds from this theorem, all of which then hold simultaneously with probability at least 1−δ. For example,
one can derive bounds on the individual error probabilities Rj
D(Q)or combinations thereof. It is this flexibility
that allows Theorem 3 to provide far richer information on the performance of the posterior Qon unseen
data. For a more in depth discussion of how such bounds can be derived, including a recipe for transforming
the bound into a differentiable training objective, see Section 4 and Appendix A.
To see that Theorem 3 is a generalisation of Theorem 1, note that we can recover it by setting Y={−1,1},
M= 2,E1={(−y,y) :y∈Y}andE2={(y,y) :y∈Y}. Then, for any convex function d: [0,1]2→R,
apply Theorem 3 with the convex function d′:△2
M→Rdefined byd′((u1,u2),(v1,v2)) :=d(u1,v1)so that
Theorem 3 bounds d′/parenleftbig
RS(Q),RD(Q)/parenrightbig
=d/parenleftbig
R1
S(Q),R1
D(Q)/parenrightbig
which equals d(RS(Q),RD(Q))in the notation
of Theorem 1. Further,
/summationdisplay
k∈Sm,2Mult (k;m,2,r) exp/parenleftig
βd′/parenleftbigk
m,r/parenrightbig/parenrightig
=m/summationdisplay
k=0Bin(k;m,r 1) exp/parenleftig
βd/parenleftbigk
m,r1/parenrightbig/parenrightig
,
so that the supremum over r1∈[0,1]of the right hand side equals the supremum over r∈△ 2of the left
hand side, which, when substituted into (1), yields the bound given in Theorem 1.
Our proof of Theorem 3 follows the lines of the proof of Theorem 1 in Bégin et al. (2016), making use of
the change of measure inequality Lemma 4. However, a complication arises from the use of soft classifiers
h∈M (Y)X. A similar problem is dealt with in Maurer (2004) when proving Corollary 2 by means of a
Lemma permitting the replacement of [0,1]-valued random variables by corresponding {0,1}-valued random
variables with the same mean. We use a generalisation of this, stated as Lemma 5 (Lemma 3 in Maurer, 2004
corresponds to the case M= 2), the proof of which is not insightful for our purposes and thus deferred to
Appendix B.1. An immediate consequence of Lemma 5 is Corollary 6, which is a generalisation of the first
half of Theorem 1 in Maurer (2004). While we only use it implicitly in the remainder of the paper, we state
it as it may be of broader interest.
The consequence of Lemma 5 is that the worst case (in terms of bounding d(RS(Q),RD(Q))) occurs when
R{(x,y)}(h)is a one-hot vector for all (x,y)∈Sandh∈H, namely whenH⊆M (Y)Xonly contains
hypotheses that, when labelling S, put all their mass on elements ˆy∈Ythat incur the same error type1. In
particular, this is the case for hypotheses that put all their mass on a single element of Y, equivalent to the
simpler caseH⊆YXas discussed in Section 2. Thus, Lemma 5 shows that the bound given in Theorem 3
cannot be made tighter only by restricting to such hypotheses.
1More precisely, when ∀h∈H ∀ (x, y)∈S∃j∈[M]such that h(x)[{ˆy∈Y: (ˆy, y)∈Ej)}] = 1.
5Under review as submission to TMLR
Lemma 4. (Change of measure, Csiszár, 1975, Donsker & Varadhan, 1975) For any set H, anyP,Q∈M (H)
and any measurable function ϕ:H→R,E
h∼Qϕ(h)≤KL(Q∥P) + ln E
h∼Pexp(ϕ(h)).
Lemma 5. (Generalisation of Lemma 3 in Maurer, 2004) Let X1,...,Xmbe i.i.d△M-valued random
vectors with mean µand suppose that f:△m
M→Ris convex. If X′
1,...,X′
mare i.i.d. Mult (1,M,µ)random
vectors, then E[f(X1,...,Xm)]≤E[f(X′
1,...,X′
m)].
Corollary 6. (Generalisation of Theorem 1 in Maurer, 2004) Let X1,...,Xmbe i.i.d△M-valued random
vectors with mean µ, and X′
1,...,X′
mbe i.i.d. Mult (1,M,µ). Define ¯X:=1
m/summationtextm
i=1Xiand ¯X′:=
1
m/summationtextm
i=1X′
i. Then E[exp(mkl(¯X∥µ)]≤E[exp(mkl(¯X′∥µ)].
Proof.(of Corollary 6) This is immediate from Lemma 5 since the average is linear, the kl-divergence is
convex and the exponential is non-decreasing and convex.
Proof.(of Theorem 3) The case H⊆YXfollows directly from the more general case by taking H′:={h′∈
M(Y)X:∃h∈Hsuch that∀x∈Xh′(x) =δh(x)}, whereδh(x)∈M (Y)denotes a point mass on h(x). For
the general caseH⊆M (Y)X, using Jensen’s inequality with the convex function d(·,·)and Lemma 4 with
ϕ(h) =βd(RS(h),RD(h)), we see that for all Q∈M (H)
βd/parenleftbig
RS(Q),RD(Q)/parenrightbig
=βd/parenleftbigg
E
h∼QRS(h),E
h∼QRD(h)/parenrightbigg
≤E
h∼Qβd/parenleftbig
RS(h),RD(h)/parenrightbig
≤KL(Q∥P) + ln/parenleftbigg
E
h∼Pexp/parenleftig
βd/parenleftbig
RS(h),RD(h)/parenrightbig/parenrightig/parenrightbigg
=KL(Q∥P) + ln(ZP(S)),
whereZP(S) :=Eh∼Pexp/parenleftbig
βd(RS(h),RD(h))/parenrightbig
. Note that ZP(S)is a non-negative random variable, so that
by Markov’s inequality P
S∼Dm/parenleftig
ZP(S)≤ES′∼DmZP(S′)
δ/parenrightig
≥1−δ.Thus, since ln(·)is strictly increasing, with
probability at least 1−δoverS∼Dm, we have that simultaneously for all Q∈M (H)
βd/parenleftbig
RS(Q),RD(Q)/parenrightbig
≤KL(Q∥P) + lnE
S′∼DmZP(S′)
δ. (2)
To bound ES′∼DmZP(S′), letXi:=R{(xi,yi)′}(h)∈△Mfori∈[m], where (xi,yi)′is thei’th element of the
dummy sample S′. Noting that each Xihas mean RD(h), define the random vectors X′
i∼Mult (1,M,RD(h))
andY:=/summationtextm
i=1X′
i∼Mult (m,M, RD(h)). Finally let f:△m
M→Rbe defined by f(x1,...,xm) :=
exp/parenleftbig
βd/parenleftbig1
m/summationtextm
i=1xi,RD(h)/parenrightbig/parenrightbig
,which is convex since the average is linear, dis convex and the exponential is
non-decreasing and convex. Then, by swapping expectations (which is permitted by Fubini’s theorem since
the argument is non-negative) and applying Lemma 5, we have that ES′∼DmZP(S′)can be written as
ES′∼DmZP(S′) = E
S′∼DmE
h∼Pexp/parenleftig
βd/parenleftbig
RS′(h),RD(h)/parenrightbig/parenrightig
=E
h∼PE
S′∼Dmexp/parenleftig
βd/parenleftbig
RS′(h),RD(h)/parenrightbig/parenrightig
=E
h∼PE
X1,...,Xmexp/parenleftigg
βd/parenleftigg
1
mm/summationdisplay
i=1Xi,RD(h)/parenrightigg/parenrightigg
≤E
h∼PE
X′
1,...,X′mexp/parenleftigg
βd/parenleftigg
1
mm/summationdisplay
i=1X′
i,RD(h)/parenrightigg/parenrightigg
=E
h∼PE
Yexp/parenleftbigg
βd/parenleftbigg1
mY,RD(h)/parenrightbigg/parenrightbigg
=E
h∼P/summationdisplay
k∈Sm,MMult/parenleftbig
k;m,M, RD(h)/parenrightbig
exp/parenleftig
βd/parenleftbigk
m,RD(h)/parenrightbig/parenrightig
6Under review as submission to TMLR
≤sup
r∈△M
/summationdisplay
k∈Sm,MMult/parenleftbig
k;m,M, r/parenrightbig
exp/parenleftig
βd/parenleftbigk
m,r/parenrightbig/parenrightig
.
Which is the definition of Id(m,β). Inequality (1) then follows by substituting this bound on ES′∼DmZP(S′)
into (2) and dividing by β.
3.2 Statement and proof of the generalised corollary
We now apply our generalised theorem with β=mandd(q,p) =kl(q∥p). This results in the following
corollary, analogous to Corollary 2 (although the multi-dimensionality makes the proof much more involved,
requiring multiple lemmas and extra arguments to make the main idea go through). We give two forms of
the bound since, while the second is looser, the first is not practical to calculate except when mis very small.
Corollary 7. LetXandYbe arbitrary sets and/uniontextM
j=1Ejbe a disjoint partition of Y2. LetD∈M (X×Y )
be a data-generating distribution and H⊆M (Y)Xbe a hypothesis class. For any prior P∈M (H),δ∈(0,1]
and sample size m, with probability at least 1−δover the random draw S∼Dm, we have that simultaneously
for all posteriors Q∈M (H)
kl/parenleftbig
RS(Q)∥RD(Q)/parenrightbig
≤1
m
KL(Q∥P) + ln
m!
δmm/summationdisplay
k∈Sm,MM/productdisplay
j=1kkj
j
kj!

 (3)
≤1
m/bracketleftigg
KL(Q∥P) + ln/parenleftigg
1
δ√πe1/(12m)/parenleftigm
2/parenrightigM−1
2M−1/summationdisplay
z=0/parenleftbiggM
z/parenrightbigg1
(πm)z/2Γ/parenleftbigM−z
2/parenrightbig/parenrightigg/bracketrightigg
, (4)
where the second inequality holds provided m≥M. Further, the bounds are unchanged if one restricts to an
ordinary hypothesis class, namely if H⊆YX.
While analogous corollaries can be obtained from Theorem 3 by other choices of convex function d, the
kl-divergence leads to convenient cancellations that remove the dependence of Ikl(m,β,r)onr, making
Ikl(m,β) :=supr∈△MIkl(m,β,r)simple to evaluate. Note (4) is logarithmic in 1/δ(typical of PAC-Bayes
bounds) and thus the confidence can be increased very cheaply. Ignoring logarithmic terms, (4) is O(1/m),
also as expected. As for M, a simple analysis shows that (4) grows only sublinearly in M, meaningMcan be
made quite large provided one has a reasonable amount of data. To prove Corollary 7 we require Lemma 8,
the proof of which is deferred to Appendix B.2.
Lemma 8. For integers M≥1andm≥M,/summationtext
k∈S>0
m,M1/producttextM
j=1√
kj≤πM
2mM−2
2
Γ(M
2).
Proof.(of Corollary 7) Applying Theorem 3 with d(q,p) =kl(q∥p)(defined in Section 2) and β=mgives that
withprobabilityatleast 1−δoverS∼Dm, simultaneouslyforallposteriors Q∈M (H),kl/parenleftbig
RS(Q)∥RD(Q)/parenrightbig
≤
1
m[KL(Q∥P) +lnIkl(m,m)
δ],whereIkl(m,m ) :=supr∈△M[/summationtext
k∈Sm,MMult (k;m,M, r)exp/parenleftbig
mkl(k
m,r/parenrightbig
)]. Thus,
to establish the first inequality of the corollary, it suffices to show that
Ikl(m,m )≤m!
mm/summationdisplay
k∈Sm,MM/productdisplay
j=1kkj
j
kj!. (5)
To see this, for each fixed r= (r1,...,rM)∈△MletJr={j∈[M] :rj= 0}. ThenMult (k;m,M, r) = 0for
anyk∈Sm,Msuch thatkj̸= 0for somej∈Jr. For the other k∈Sm,M, namely those such that kj= 0for all
j∈Jr, the probability term can be written as Mult (k;m,M, r) =m!/producttextM
j=1kj!/producttextM
j=1rkj
j=m!/producttext
j̸∈Jrkj!/producttext
j̸∈Jrrkj
j,
and (recalling the convention that 0 ln0
0= 0) the term exp(mkl(k
m,r))can be written as
exp
mM/summationdisplay
j=1kj
mlnkj
m
rj
= exp
/summationdisplay
j̸∈Jrkjlnkj
mrj
=/productdisplay
j̸∈Jr/parenleftbiggkj
mrj/parenrightbiggkj
=1
mm/productdisplay
j̸∈Jr/parenleftbiggkj
rj/parenrightbiggkj
,
7Under review as submission to TMLR
where the last equality is obtained by recalling that the kjsum tom. Substituting these two expressions into
the definition ofIkl(m,m )and only summing over those k∈Sm,Mwith non-zero probability, we obtain
/summationdisplay
k∈Sm,MMult (k;m,M, r) exp/parenleftbig
mkl/parenleftbigk
m,r/parenrightbig/parenrightbig
=/summationdisplay
k∈Sm,M:
∀j∈Jrkj=0Mult (k;m,M, r) exp/parenleftbig
mkl/parenleftbigk
m,r/parenrightbig/parenrightbig
=/summationdisplay
k∈Sm,M:
∀j∈Jrkj=0m!/producttext
j̸∈Jrkj!/productdisplay
j̸∈Jrrkj
j1
mm/productdisplay
j̸∈Jr/parenleftbiggkj
rj/parenrightbiggkj
=m!
mm/summationdisplay
k∈Sm,M:
∀j∈Jrkj=0/productdisplay
j̸∈Jrkkj
j
kj!
=m!
mm/summationdisplay
k∈Sm,M:
∀j∈Jrkj=0M/productdisplay
j=1kkj
j
kj!(because00
0!= 1)
≤m!
mm/summationdisplay
k∈Sm,MM/productdisplay
j=1kkj
j
kj!.
Since this is independent of r, it also holds after taking the supremum over r∈△Mof the left hand side.
We have thus established (5) and hence (3). Now, defining f:/uniontext∞
M=2Sm,M→Rbyf(k) =/producttext|k|
j=1kkj
j/kj!, we
see that to establish inequality (4) it suffices to show that
m!
mm/summationdisplay
k∈Sm,Mf(k)≤√πe1/12m/parenleftigm
2/parenrightigM−1
2M−1/summationdisplay
z=0/parenleftbiggM
z/parenrightbigg1
(πm)z/2Γ/parenleftbigM−z
2/parenrightbig. (6)
We show this by upper bounding each f(k)individually using Stirling’s formula: ∀n≥1√
2πn/parenleftbign
e/parenrightbign<n!<√
2πn/parenleftbign
e/parenrightbigne1
12n. Since we cannot use this to upper bound 1/kj!whenkj= 0, we partition the sum above
according to the number of coordinates of kat whichkj= 0. Letzindex the number of such coordinates.
Sincefis symmetric under permutations of its arguments,
/summationdisplay
k∈Sm,Mf(k) =M−1/summationdisplay
z=0/parenleftbiggM
z/parenrightbigg/summationdisplay
k∈S>0
m,M−zf(k). (7)
Fork∈S>0
m,MStirling’s formula yields f(k)≤/producttextM
j=1kkj
j√
2πkj/parenleftbigkj
e/parenrightbigkj=/producttextM
j=1ekj√
2πkj=em
(2π)M/2/producttextM
j=11√
kj.An
application of Lemma 8 now gives
/summationdisplay
k∈S>0
m,M−zf(k)≤em
(2π)M/2/summationdisplay
k∈S>0
m,M−zM/productdisplay
j=11/radicalbig
kj≤em
(2π)M
2πM−z
2mM−z−2
2
Γ/parenleftbigM−z
2/parenrightbig=emmM−2
2
2M
2(πm)z/2Γ/parenleftbigM−z
2/parenrightbig.
Substituting this into equation (7) and bounding m!using Stirling’s formula, we have
m!
mm/summationdisplay
k∈Sm,Mf(k)≤√
2πme1/12m
emM−1/summationdisplay
z=0/parenleftbiggM
z/parenrightbiggemmM−2
2
2M/2(πm)z/2Γ/parenleftbigM−z
2/parenrightbig
=√πe1/12m/parenleftigm
2/parenrightigM−1
2M−1/summationdisplay
z=0/parenleftbiggM
z/parenrightbigg1
(πm)z/2Γ/parenleftbigM−z
2/parenrightbig
which is(6), establishing(4)and therefore completing the proof.
8Under review as submission to TMLR
4 Implied Bounds and Construction of a Differentiable Training Objective
As already discussed, a multitude of bounds can be derived from Theorem 3 and Corollary 7, all of which then
hold simultaneously with high probability. For example, suppose after a use of Corollary 7 we have a bound
of the form kl(RS(Q)||RD(Q))≤B. The following proposition then yields the bounds Lj≤Rj
D(Q)≤Uj,
whereLj:=inf{p∈[0,1] :kl(Rj
S(Q)∥p)≤B}andUj:=sup{p∈[0,1] :kl(Rj
S(Q)∥p)≤B}. Moreover,
since in the worst case we have kl(RS(Q)||RD(Q)) =B, the proposition shows that the lower and upper
boundsLjandUjare the tightest possible, since if Rj
D(Q)̸∈[Lj,Uj]thenkl(Rj
S(Q)∥Rj
D(Q))>Bimplying
kl(RS(Q)||RD(Q))> B. For a more precise version of this argument and a proof of Proposition 9, see
Appendix B.3.
Proposition 9. Letq,p∈△M. Thenkl(qj∥pj)≤kl(q∥p)for allj∈[M], with equality when pi=1−pj
1−qjqi.
for alli̸=j.
As a second much more interesting example, suppose we can quantify how bad an error of each type is
by means of a loss vector ℓ∈[0,∞)M, whereℓjis the loss we attribute to an error of type Ej. We
may then be interested in bounding the total riskRT
D(Q)∈[0,∞)ofQwhich, recall, is defined by
RT
D(Q) :=/summationtextM
j=1ℓjRj
D(Q). Indeed, given a bound of the form kl(RS(Q)||RD(Q))≤B, we can derive
RT
D(Q)≤sup{/summationtextM
j=1ℓjrj:r∈△M,kl(RS(Q)||r)≤B}.This motivates the following definition of kl−1
ℓ(u|c).
To see that this is indeed well-defined (at least when u∈△>0
M), see the discussion at the beginning of
Appendix B.4.
Definition 10. Foru∈ △M,c∈[0,∞)andℓ∈[0,∞)M, define kl−1
ℓ(u|c) = sup{/summationtextM
j=1ℓjvj:v∈
△M,kl(u∥v)≤c}.
Can we calculate kl−1
ℓ(u|c)and hence fℓ(kl−1
ℓ(u|c))in order to evaluate the bound on the total risk?
Additionally, if we wish to use the bound on the total risk as a training objective, can we calculate the
partial derivatives of f∗
ℓ(u,c) :=fℓ(kl−1
ℓ(u|c))with respect to the ujandcso that we can use gradient
descent? Our Proposition 11 answers both of these questions in the affirmative, at least in the sense that it
provides a speedy method for approximating these quantities to arbitrary precision provided uj>0for all
j∈[M]andc>0. Indeed, the only approximation step required is that of approximating the unique root
of a continuous and strictly increasing scalar function. Thus, provided the ujthemselves are differentiable,
Corollary 7 combined with Proposition 11 yields a tractable and fully differentiable objective that can be
used for training. More details on how this can be done, including an algorithm written in pseudocode, can
be found in Appendix A. While somewhat analogous to the technique used in Clerico et al. (2022) to obtain
derivatives of the one-dimensional kl-inverse, our proposition directly yields derivatives on the total risk by
(implicitly) employing the envelope theorem (see for example Takayama & Akira, 1985). Since the proof of
Proposition 11 is rather long and technical, we defer it to Appendix B.4.
Proposition 11. Fixℓ∈[0,∞)Msuch that not all ℓjare equal, and define fℓ:△M→[0,∞)by
fℓ(v) :=/summationtextM
j=1ℓjvj. For all ˜u= (u,c)∈△>0
M×(0,∞), define v∗(˜u) :=kl−1
ℓ(u|c)∈△Mand letµ∗(˜u)∈
(−∞,−maxjℓj)be the unique solution to c=ϕℓ(µ), whereϕℓ: (−∞,−maxjℓj)→Ris given by ϕℓ(µ) :=
ln(−/summationtextM
j=1uj
µ+ℓj)+/summationtextM
j=1ujln(−(µ+ℓj)), which is continuous and strictly increasing. Then v∗(˜u) =kl−1
ℓ(u|c)
is given by
v∗(˜u)j=λ∗(˜u)uj
µ∗(˜u) +ℓjforj∈[M], whereλ∗(˜u) =
M/summationdisplay
j=1uj
µ∗(˜u) +ℓj
−1
.
Further, defining f∗
ℓ:△>0
M×(0,∞)→[0,∞)byf∗
ℓ(˜u) :=fℓ(v∗(˜u)), we have that
∂f∗
ℓ
∂uj(˜u) =λ∗(˜u)/parenleftbigg
1 + lnuj
v∗(˜u)j/parenrightbigg
and∂f∗
ℓ
∂c(˜u) =−λ∗(˜u).
5 Perspectives
By abstracting to a general setting of discretised error types, we established a novel type of generalisation
bound (Theorem 3) providing far richer information than existing PAC-Bayes bounds. Through our Corollary
9Under review as submission to TMLR
7 and Proposition 11, our bound inspires a training algorithm (see Appendix A) suitable for many different
learning problems, including structured output prediction (as investigated by Cantelobre et al., 2020, in
the PAC-Bayes setting), multi-task learning and learning-to-learn (see e.g.Maurer et al., 2016). We will
demonstrate these applications and our bound’s utility for real-world learning problems in an empirical
follow-up study. Note we require i.i.d. data, which in practice is frequently not the case or is hard to verify.
Further, the number of error types Mmust be finite. While in continuous scenarios it would be preferable to
be able to quantify the entire distribution of loss values without having to discretise into finitely many error
types, in the multiclass setting our framework is entirely suitable.
References
Pierre Alquier. User-friendly introduction to PAC-Bayes bounds. arXiv preprint arXiv:2110.11216 , 2021.
Amiran Ambroladze, Emilio Parrado-Hernández, and John Shawe-Taylor. Tighter PAC-Bayes bounds. In
BernhardSchölkopf, JohnC.Platt, andThomasHofmann(eds.), Advances in Neural Information Processing
Systems 19, Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems,
Vancouver, British Columbia, Canada, December 4-7, 2006 , pp. 9–16. MIT Press, 2006. URL https:
//proceedings.neurips.cc/paper/2006/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html .
Loubna Benabbou and Pascal Lang. PAC-Bayesian generalization bound for multi-class learning. In NIPS
2017 Workshop. (Almost) 50 Shades of Bayesian Learning: PAC-Bayesian trends and insights , 2017. URL
https://bguedj.github.io/nips2017/pdf/PAC-Bayes_2017_paper_3.pdf .
Felix Biggs and Benjamin Guedj. Differentiable PAC-Bayes objectives with partially aggregated neural net-
works.Entropy, 23(10):1280, 2021. doi: 10.3390/e23101280. URL https://doi.org/10.3390/e23101280 .
Felix Biggs and Benjamin Guedj. On margins and derandomisation in PAC-Bayes. In AISTATS , 2022a. URL
https://arxiv.org/abs/2107.03955 .
Felix Biggs and Benjamin Guedj. Non-vacuous generalisation bounds for shallow neural networks. arXiv
preprint arXiv:2202.01627 , 2022b.
Luc Bégin, Pascal Germain, François Laviolette, and Jean-Francis Roy. PAC-Bayesian Bounds based
on the Rényi Divergence. In Arthur Gretton and Christian C. Robert (eds.), Proceedings of the 19th
International Conference on Artificial Intelligence and Statistics , volume 51 of Proceedings of Machine
Learning Research , pp. 435–444, Cadiz, Spain, 09–11 May 2016. PMLR. URL https://proceedings.mlr.
press/v51/begin16.html .
Théophile Cantelobre, Benjamin Guedj, María Pérez-Ortiz, and John Shawe-Taylor. A pac-bayesian perspec-
tive on structured prediction with implicit loss embeddings. arXiv preprint arXiv:2012.03780 , 2020.
Olivier Catoni. A PAC-Bayesian approach to adaptive classification. preprint, 840, 2003.
Olivier Catoni. Statistical Learning Theory and Stochastic Optimization: Ecole d’Eté de Probabilités de
Saint-Flour XXXI-2001 . Springer, 2004.
Olivier Catoni. PAC-Bayesian Supervised Classification: The Thermodynamics of Statistical Learning ,
volume 56 of Institute of Mathematical Statistics (IMS) Lecture Notes - Monograph Series . Institute
of Mathematical Statistics, 2007. ISBN 9780940600720. URL https://books.google.fr/books?id=
acnaAAAAMAAJ .
Eugenio Clerico, George Deligiannidis, and Arnaud Doucet. Conditionally gaussian pac-bayes. In International
Conference on Artificial Intelligence and Statistics , pp. 2311–2329. PMLR, 2022.
Imre Csiszár. I-divergence geometry of probability distributions and minimization problems. The Annals of
Probability , pp. 146–158, 1975.
MD Donsker and SRS Varadhan. Large deviations for Markov processes and the asymptotic evaluation of
certain markov process expectations for large times. In Probabilistic Methods in Differential Equations , pp.
82–88. Springer, 1975.
10Under review as submission to TMLR
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep
(stochastic) neural networks with many more parameters than training data. In Conference on Uncertainty
in Artificial Intelligence [UAI] , 2017.
Gintare Karolina Dziugaite and Daniel M. Roy. Entropy-SGD optimizes the prior of a PAC-Bayes bound:
Generalization properties of entropy-SGD and data-dependent priors. In Jennifer G. Dy and Andreas
Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018,
Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018 , volume 80 of Proceedings of Machine Learning
Research , pp. 1376–1385. PMLR, 2018. URL http://proceedings.mlr.press/v80/dziugaite18a.html .
Gintare Karolina Dziugaite, Kyle Hsu, Waseem Gharbieh, Gabriel Arpino, and Daniel Roy. On the role of
data in PAC-Bayes. In Arindam Banerjee and Kenji Fukumizu (eds.), The 24th International Conference
on Artificial Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual Event , volume 130 of
Proceedings of Machine Learning Research , pp. 604–612. PMLR, 2021. URL http://proceedings.mlr.
press/v130/karolina-dziugaite21a.html .
Vasilii Feofanov, Emilie Devijver, and Massih-Reza Amini. Transductive bounds for the multi-class majority
vote classifier. Proceedings of the AAAI Conference on Artificial Intelligence , 33:3566–3573, 2019. doi:
10.1609/aaai.v33i01.33013566. URL https://ojs.aaai.org/index.php/AAAI/article/view/4236 .
Benjamin Guedj. A Primer on PAC-Bayesian Learning. In Proceedings of the second congress of the French
Mathematical Society , 2019. URL https://arxiv.org/abs/1901.05353 .
Sokol Koço and Cécile Capponi. On multi-class classification through the minimization of the confusion matrix
norm. In Cheng Soon Ong and Tu Bao Ho (eds.), Proceedings of the 5th Asian Conference on Machine
Learning , volume 29 of Proceedings of Machine Learning Research , pp. 277–292, Australian National
University, Canberra, Australia, 13–15 Nov 2013. PMLR. URL https://proceedings.mlr.press/v29/
Koco13.html .
François Laviolette, Emilie Morvant, Liva Ralaivola, and Jean-Francis Roy. Risk upper bounds for general
ensemble methods with an application to multiclass classification. Neurocomputing , 219:15–25, 2017. ISSN
0925-2312. doi: https://doi.org/10.1016/j.neucom.2016.09.016. URL https://www.sciencedirect.com/
science/article/pii/S0925231216310177 .
Gaël Letarte, Pascal Germain, Benjamin Guedj, and Francois Laviolette. Dichotomize and generalize: PAC-
Bayesian binary activated deep neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché
Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32 , pp. 6872–6882.
Curran Associates, Inc., 2019.
Guy Lever, François Laviolette, and John Shawe-Taylor. Distribution-dependent PAC-Bayes priors. In
International Conference on Algorithmic Learning Theory , pp. 119–133. Springer, 2010.
Guy Lever, François Laviolette, and John Shawe-Taylor. Tighter PAC-Bayes bounds through distribution-
dependent priors. Theoretical Computer Science , 473:4–28, February 2013. ISSN 0304-3975. doi: 10.1016/j.
tcs.2012.10.013. URL https://linkinghub.elsevier.com/retrieve/pii/S0304397512009346 .
Andreas Maurer. A note on the PAC-Bayesian theorem. arXiv preprint cs/0411099 , 2004.
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask representa-
tion learning. J. Mach. Learn. Res. , 17:81:1–81:32, 2016. URL http://jmlr.org/papers/v17/15-242.
html.
David A McAllester. Some PAC-Bayesian theorems. In Proceedings of the eleventh annual conference on
Computational Learning Theory , pp. 230–234. ACM, 1998.
David A McAllester. PAC-Bayesian model averaging. In Proceedings of the twelfth annual conference on
Computational Learning Theory , pp. 164–170. ACM, 1999.
11Under review as submission to TMLR
Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in
machine learning. J. Mach. Learn. Res. , 21(132):1–62, 2020.
Emilie Morvant, Sokol Koço, and Liva Ralaivola. PAC-Bayesian generalization bound on confusion matrix
for multi-class classification. In Proceedings of the 29th International Conference on Machine Learning,
ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012 . icml.cc / Omnipress, 2012. URL http:
//icml.cc/2012/papers/434.pdf .
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-Bayesian approach to spectrally-
normalizedmarginboundsforneuralnetworks. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings . OpenRe-
view.net, 2018. URL https://openreview.net/forum?id=Skz_WfbCZ .
Emilio Parrado-Hernández, Amiran Ambroladze, John Shawe-Taylor, and Shiliang Sun. PAC-Bayes bounds
with data dependent priors. J. Mach. Learn. Res. , 13:3507–3531, 2012. URL http://dl.acm.org/
citation.cfm?id=2503353 .
María Pérez-Ortiz, Omar Rivasplata, Benjamin Guedj, Matthew Gleeson, Jingyu Zhang, John Shawe-Taylor,
Miroslaw Bober, and Josef Kittler. Learning pac-bayes priors for probabilistic neural networks. arXiv
preprint arXiv:2109.10304 , 2021.
Maria Perez-Ortiz, Omar Rivasplata, John Shawe-Taylor, and Csaba Szepesvari. Tighter risk certificates
for neural networks. Journal of Machine Learning Research , 22(227):1–40, 2021. URL http://jmlr.org/
papers/v22/20-879.html .
Omar Rivasplata, Csaba Szepesvári, John Shawe-Taylor, Emilio Parrado-Hernández, and Shiliang Sun.
PAC-Bayes bounds for stable algorithms with instance-dependent priors. In Samy Bengio, Hanna M.
Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (eds.), Advances
in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing
Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada , pp. 9234–9244, 2018. URL https:
//proceedings.neurips.cc/paper/2018/hash/386854131f58a556343e056f03626e00-Abstract.html .
Matthias Seeger. PAC-Bayesian generalisation error bounds for Gaussian process classification. Journal of
Machine Learning Research , 3(Oct):233–269, 2002.
Akira Takayama and Takayama Akira. Mathematical economics . Cambridge university press, 1985.
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P. Adams, and Peter Orbanz. Non-vacuous generalization
bounds at the ImageNet scale: a PAC-Bayesian compression approach. In 7th International Conference on
Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019.
URL https://openreview.net/forum?id=BJgqqsAct7 .
12