Published in Transactions on Machine Learning Research (MM/YYYY)
Understanding convolution on graphs via energies
Francesco Di Giovanni∗fd405@cam.ac.uk
University of Cambridge
James Rowbottom∗jr908@cam.ac.uk
University of Cambridge
Benjamin P. Chamberlain
Charm Therapeutics
Thomas Markovich
Cash App
Michael M. Bronstein
University of Oxford
Reviewed on OpenReview: https: // openreview. net/ forum? id= v5ew3FPTgb
Abstract
Graph Neural Networks (GNNs) typically operate by message-passing, where the state of a
node is updated based on the information received from its neighbours. Most message-passing
models act as graph convolutions, where features are mixed by a shared, linear transformation
before being propagated over the edges. On node-classification tasks, graph convolutions have
been shown to suffer from two limitations: poor performance on heterophilic graphs, and
over-smoothing. It is common belief that both phenomena occur because such models behave
as low-pass filters, meaning that the Dirichlet energy of the features decreases along the layers
incurring a smoothing effect that ultimately makes features no longer distinguishable. In
this work, we rigorously prove that simple graph-convolutional models can actually enhance
high frequencies and even lead to an asymptotic behaviour we refer to as over-sharpening ,
opposite to over-smoothing. We do so by showing that linear graph convolutions with
symmetric weights minimize a multi-particle energy that generalizes the Dirichlet energy;
in this setting, the weight matrices induce edge-wise attraction (repulsion) through their
positive (negative) eigenvalues, thereby controlling whether the features are being smoothed
or sharpened. We also extend the analysis to non-linear GNNs, and demonstrate that some
existing time-continuous GNNs are instead always dominated by the low frequencies. Finally,
we validate our theoretical findings through ablations and real-world experiments.
1 Introduction
GraphNeural Networks (GNNs) represent a popular class of neural networks operating on graphs (Sperduti,
1993; Goller & Kuchler, 1996; Gori et al., 2005; Scarselli et al., 2008; Bruna et al., 2014; Defferrard et al.,
2016). Most GNNs follow the message-passing paradigm (Gilmer et al., 2017), where node embeddings are
computed recursively after collecting information from the 1-hop neighbours. Typically, Message Passing
Neural Networks ( MPNNs) implement convolution on graphs, where messages are first acted upon by a linear
transformation (referred to as channel-mixing ), and are then propagated over the edges by a (normalized)
adjacency matrix (Kipf & Welling, 2017; Hamilton et al., 2017; Xu et al., 2019; Bronstein et al., 2021).
While issues such as bounded expressive power (Xu et al., 2019; Morris et al., 2019) and over-squashing (Alon
& Yahav, 2021; Topping et al., 2022; Di Giovanni et al., 2023) pertain to general MPNNs, graph-convolutional
MPNNs have also been shown to suffer from additional problems more peculiar to node-classification tasks:
1Published in Transactions on Machine Learning Research (MM/YYYY)
performance on heterophilic graphs – i.e. those where adjacent nodes often have different labels – and
over-smoothing of features in the limit of many layers.
Several works have gathered evidence that graph convolutions seem to struggle on someheterophilic graphs
(Pei et al., 2020; Zhu et al., 2020; Platonov et al., 2023). A common reasoning is that these MPNNs tend to
enhance the low-frequency components of the features and discard the high-frequency ones, resulting in a
smoothing effect which is detrimental on tasks where adjacent nodes have different labels (Nt & Maehara,
2019). Accordingly, a common fix for this problem is allowing signed-message passing to revert the smoothing
effect (Bo et al., 2021; Yan et al., 2021). Nonetheless, the property that graph convolutions act as low-pass
filters is actually only proven in very specialized scenarios, which leads to an important question:
Q.1Are simple graph convolutions actually capable of enhancing the high-frequency components of the
features without explicitly modelling signed message-passing?
A positive answer to Q.1 might imply that, in the limit of many layers, simple graph convolutions can avoid
over-smoothing. Over-smoothing occurs when node features become indistinguishable in the limit of many
layers (Nt & Maehara, 2019; Oono & Suzuki, 2020). In fact, Cai & Wang (2020); Bodnar et al. (2022); Rusch
et al. (2022) argued that over-smoothing is defined by the Dirichlet energy EDir(Zhou & Schölkopf, 2005)
decaying to zero (exponentially) as the depth increases. However, these theoretical works focus on a classical
instance of GCN(Kipf & Welling, 2017) with assumptions on the non-linear activation and the singular values
of the weight matrices, thereby leaving the following questions open:
Q.2Can over-smoothing be avoided by simple graph convolutions? If so, do graph convolutions admit
asymptotic behaviours other than over-smoothing, in the limit of many layers?
time
Figure 1: Actual gradient flow dy-
namics: repulsive forces separate
heterophilic labels.Contributions and outline. Both the poor performance on het-
erophilic graphs and the issue of over-smoothing, can be characterized
in terms of a fixed energy functional (the Dirichlet energy EDir)decreas-
ing along the features F(t)computed by graph-convolutional MPNNs.
Namely, ifEDir(F(t))decreases w.r.t. the layer t, then the features
become smoother and hence likely not useful for separating nodes on
heterophilic graphs, ultimately incurring over-smoothing. But does actu-
allyEDir(F(t))decrease for all graph-convolutional models? In this work
we show that for many graph convolutions, a more general, parametric
energyEθ, which recovers EDiras a special case, decreases along the
features as we increase the number of layers. Since this energy can also
induce repulsion along the edges, we are able to provide affirmative
answers to both Q.1 and Q.2 above, thereby showing that graph con-
volutions are not bound to act as low-pass filters and over-smooth in
the limit. Namely, our contributions are:
•In Section 4 we prove that a large class of linear graph convolutions are gradient flows , meaning
that the features are evolved in the direction of steepest descent of some energy Eθ, as long as the
weight (channel-mixing) matrices are symmetric. We study Eθand show that it provides a physical
interpretation for graph convolutions as multi-particle dynamics where the weight matrices induce
attraction (repulsion) on the edges, via their positive (negative) eigenvalues.
•In light of this, we provide an affirmative answer to Q.1, proving that the class of gradient-flow MPNNs
can enhance the high-frequency components of the features (see Theorem 4.3 for the time-continuous
case and Theorem 5.1 for the discrete setting). Our analysis suggests that gradient-flow MPNNs
may be able to deal with heterophilic graphs, differently from other time-continuous MPNNs which
instead can only induce smoothing of the features (Theorem 5.4), provided that a residual connection
is available (Theorem 5.3).
2Published in Transactions on Machine Learning Research (MM/YYYY)
•In Section 5 we also answer Q.2 positively by proving that in the limit of many layers, linear
gradient-flow MPNNs admit two possible asymptotic behaviours. If the positive eigenvalues of the
weight matrices are larger than the negative ones, then we incur over-smoothing; conversely, the
phenomenon of over-sharpening occurs, where the features become dominated by the projection
onto the eigenvector of the Laplacian associated with the highest frequency (Theorem 5.1). This
result proves that asymptotic behaviours other than over-smoothing are possible, even for graph
convolutions, depending on the interactions between the spectra of the graph Laplacian and of the
weight matrices. To determine whether the underlying MPNNis smoothing/sharpening the features,
we also introduce a measure based on the normalized Dirichlet energy, of independent interest.
•In Section 6 we extend our theoretical analysis to graph convolutions with non-linear activations,
showing that despite not being gradient-flows, the same parametric energy Eθ(F(t))introduced above,
is monotonically decreasing with respect to t, thereby preserving the interpretation of the weight
matrices as edge-wise potentials inducing attraction (repulsion) via their eigenvalues (Theorem 6.1).
•Finally, in Section 7 we validate our theoretical findings through ablations and real-world experiments.
Reproducibility. Source code can be found at: https://github.com/JRowbottomGit/graff.
2 Graph convolutions and the Dirichlet energy
In this Section, we first review the message-passing paradigm, with emphasis on the class of convolutional-type
MPNNs. We then introduce the Dirichlet energy on graphs, a known functional which can be used to study
the performance of graph convolutions on heterophilic tasks and the over-smoothing phenomenon.
2.1 Preliminaries on graphs and the MPNNclass
Notations and conventions. Consider a graph G= (V,E), withn:=|V|nodes and E⊂V×Vrepresenting
the edges. We assume that Gis simple, undirected , and connected. Its adjacency matrix Ais defined as
aij= 1if(i,j)∈Eand zero otherwise. We let D=diag(d0,...,dn−1)be the degree matrix. We typically
useAto denote some (normalized) version of the adjacency matrix. We are interested in problems where the
graph has node features {fi∈Rd:i∈V}whose matrix representation is F∈Rn×d. In particular, fi∈Rdis
thei-th row (transposed) of F, while fr∈Rnis itsr-th column. Later, we also rely on the vectorization of
the feature matrix vec(F)∈Rnd, simply obtained by stacking all columns of F.
MPNNs and the ‘convolutional flavour’. In this work we investigate the dynamics of an MPNNin terms
of its smoothing or sharpening effects on the features, which are properties particularly relevant in the context
of heterophily and over-smoothing. Accordingly, we focus on node-level tasks, where each node i∈Vhas a
labelyi∈{1,...,K}that we need to predict. Message Passing Neural Networks ( MPNNs) (Gilmer et al.,
2017) compute node embeddings fi(t)at each node iand layert, using the following recursion:
fi(t+ 1) = UPt(fi(t),AGGt({{fj(t) : (i,j)∈E}})),
where AGGtis invariant to permutations. The over-smoothing phenomenon was studied for a class of MPNNs
that we refer to as graph convolutions (Bronstein et al., 2021), which essentially consist of two operations:
applying a shared linear transformation to the features ( ‘channel mixing’ ) and propagating them along the
edges (‘diffusion’ ). Namely, we consider MPNNs whose layer update can be written in matricial form as:
F(t+ 1) = F(t) +σ/parenleftbig
−F(t)Ωt+AF(t)Wt−F(0)˜Wt/parenrightbig
, (1)
where Ωt,Wt, and ˜Wtare learnable matrices in Rd×dperforming channel mixing, while σis a pointwise
nonlinearity; the message-passing matrix Ainstead, leads to the aggregation of features from adjacent nodes.
We first note how this system of equations include common instances of the MPNNclass with residual
connections . IfA=D−1/2AD−1/2andΩt=˜Wt=0, then we recover GCN(Kipf & Welling, 2017). The
case of A=D−1Aand ˜Wt=0results in GraphSAGE (Hamilton et al., 2017), while by choosing Ωt=0
andWtand ˜Wtas convex combinations with the identity we recover GCNII(Chen et al., 2020). Finally, if
A=A,Ωt=−(1 +ϵ)Wt, and ˜Wt=0, then (1) is a shallow variant of GIN(Xu et al., 2019).
3Published in Transactions on Machine Learning Research (MM/YYYY)
2.2 The Dirichlet energy on graphs
To assess whether graph convolutions induce a smoothing or sharpening behaviour over the features, we
monitor their Dirichlet energy. Consider a (feature) signal F:V→Rd, where we associate a vector fito each
nodei∈V. The graph Dirichlet energy ofFis defined by (Zhou & Schölkopf, 2005):
EDir(F) :=1
2/summationdisplay
(i,j)∈E∥(∇F)ij∥2,(∇F)ij:=fj/radicalbig
dj−fi√di,
where∇Fis thegradient ofFand is defined edge-wise. We see that EDirmeasures the smoothness of F,
meaning whether Fhas similar values across adjacent nodes.
The graph Laplacian. The (normalized) graph Laplacian is the operator ∆=I−D−1/2AD−1/2. The
Laplacian is symmetric and positive semidefinite, and its eigenvalues are referred to as (graph) frequencies
and written in ascending order as λ0≤λ1≤...≤λn−1. It is known that λ0= 0whileλn−1≤2(Chung &
Graham, 1997). The low frequencies are associated with macroscopic (coarse) information in the graph, while
the high frequencies generally describe microscopic (fine-grained) behaviour. In fact, if we let {ϕℓ}be an
orthonormal frame of eigenvectors for ∆(i.e.∆ϕℓ=λℓϕℓ), then we can rewrite the Dirichlet energy of the
features Fwith vector representation vec(F)∈Rndobtained by stacking all columns, as
EDir(F) =d/summationdisplay
r=1n−1/summationdisplay
ℓ=0λℓ(ϕ⊤
ℓfr)2= trace( F⊤∆F) = (vec( F))⊤(Id⊗∆)vec(F), (2)
where⊗denotes the Kronecker product – some properties are reviewed in Appendix A.2 – and Idis the
identity matrix in Rd×d. Features are smooth if the Dirichlet energy is small, which occurs when for each
channelr, we have large projections onto the eigenvectors ϕℓassociated with lower frequencies.
3 Heterophily and over-smoothing: overview on related work
On a broad scale, our work is related to studying GNNs as filters (Defferrard et al., 2016; Bresson & Laurent,
2017; Hammond et al., 2019; Balcilar et al., 2020; He et al., 2021) and adopts techniques similar to Cai &
Wang (2020). Our approach is also inspired by Haber & Ruthotto (2018); Chen et al. (2018); Biloš et al.
(2021), where layers of an architecture are regarded as Euler discretizations of a time-continuous dynamical
system. This direction has been studied in the context of GNNs in several flavours (Xhonneux et al., 2020;
Zang & Wang, 2020; Chamberlain et al., 2021a; Eliasof et al., 2021; Chamberlain et al., 2021b; Bodnar et al.,
2022; Rusch et al., 2022); we share this perspective in our work and expand the connection, by studying the
energy associated with GNNs and think of features as ‘particles’ in some generalized position space.
Understanding heterophily via the Dirichlet energy. A node-classification task on a graph Gis
heterophilic when adjacent nodes often have different labels. In this case the label signal has high Dirichlet
energy, meaning that enhancing the high-frequency components of the features could be useful to separate
classes – the prototypical example being the eigenvector of ∆with largest frequency λn−1, that can separate
the clusters of a bipartite graph. While MPNNs as in (1) have shown strong empirical performance on
node-classification tasks with low heterophily, it has been observed that simple convolutions on graphs could
struggle in the case of high heterophily (Pei et al., 2020; Zhu et al., 2020). Such findings have spurred a
plethora of methods that modify standard MPNNs to make them more effective on heterophilic graphs, by
either ignoring or rearranging the graph structure, or by introducing some form of signed message-passing to
‘reverse’ the dynamics and magnify the high frequencies (Bo et al., 2021; Luan et al., 2021; Lim et al., 2021;
Maurya et al., 2021; Zhang et al., 2021; Wang et al., 2022; Luan et al., 2022; Eliasof et al., 2023).
Understanding over-smoothing via the Dirichlet energy. Whether a given MPNNacts as a low-pass
filter or not, is also at the heart of the over-smoothing phenomenon, regarded as the tendency of node features
to approach the same value – up to degree rescaling – as the number of layers increase, independent of any
input information (Li et al., 2018; Oono & Suzuki, 2020). Cai & Wang (2020) formalized over-smoothing as
4Published in Transactions on Machine Learning Research (MM/YYYY)
the Dirichlet energy of the features EDir(F(t))decaying to zero as the number of layers tdiverge. A similar
approach has also been studied in Zhou et al. (2021); Bodnar et al. (2022); Rusch et al. (2022) – for a review
see Rusch et al. (2023). Although over-smoothing is now regarded as a general plague for MPNNs, it has in
fact only been proven to occur for special instances of graph convolutional equations that (i) have nonlinear
activation ReLU, (ii) have small weight matrices (as measured by their singular values), and (iii) have no
residual connections.
The general strategy. Both the problem of graph convolutions struggling on heterophilic tasks and of
over-smoothing, arise when a fixedenergy functional, in this case the Dirichlet energy, decreases along the
features computed at each layer. Namely, both these phenomena occur when, in general,d
dtEDir(F(t))≤0,
withF(t)the features at layer t. To address the questions Q.1 and Q.2 in Section 1, we study whether
there exist general energy functionals promoting dynamics more expressive than simple smoothing of the
features, that decrease along graph convolutions as in (1) . Analyzing complicated dynamical systems through
monotonicity properties of energy functionals is common in physics and geometry. Indeed, in this work we show
that the same approach sheds light on the role of the channel-mixing weight matrices for graph-convolutions,
and on the dynamics that MPNNs can generate beyond (over-)smoothing.
4Gradient-flow MPNNs: understanding convolution on graphs via parametric energies
In this Section we study when graph-convolutions (1) admit an energy functional Eθsuch thatt∝⇕⊣√∫⊔≀→Eθ(F(t))is
decreasing as tincreases. This allows us to interpret the underlying MPNN-dynamics as forces that dissipate
(or in fact, minimize) the energy Eθ. By analysingEθwe can then derive qualitative properties of the MPNN,
and determine if the features F(t)incur over-smoothing or not when tis large. Put differently, we investigate
when MPNNs as in (1) are gradient flows , a special class of dynamical systems we briefly review next.
What is a gradient flow? Consider an N-dimensional dynamical system governed by the differential
equation ˙F(t) =F(F(t))that evolves some input F(0)for timet≥0. We say that the evolution equation
is agradient flow if there existsE:RN→Rsuch thatF(F(t)) =−∇E (F(t)). In this case, since
˙E(F(t)) =−∥∇E (F(t))∥2, the energyEdecreases along the solution F(t). Gradient flows are valuable since
the existence ofEand the knowledge of its functional expression allow for a better understanding of the
underlying dynamics. To motivate our analysis of graph convolutions, we first review examples of gradient
flows on graphs that do not involve learnable weights – we discuss variational methods for image processing
(You et al., 1996; Kimmel et al., 1997) in Appendix A.1 and Appendix B.5.
4.1 Gradient flows on graphs: the non-learnable case
A prototypical gradient flow: heat equation. LetF∈Rn×dbe the feature matrix. The most common
form of diffusion process on Gis theheat equation , which consists of the system ˙fr(t) =−∆fr(t), where
fr∈Rnis ther-th entry of the features for 1≤r≤d.The heat equation is an example of gradient flow: if
we stack the columns of Fintovec(F)∈Rnd, we can rewrite the heat equation as
vec(˙F(t)) =−1
2∇EDir(vec(F(t))), (3)
whereEDir:Rnd→Ris the (graph) Dirichlet energy defined in (2). The evolution of F(t)by the heat
equation (3) decreases the Dirichlet energy EDir(F(t))and is hence a smoothing process; in the limit,
EDir(F(t))→0, which is attained by the projection of the initial state F(0)onto the null space of the
Laplacian.
Label propagation. Given a graph Gand labels{yi}onV1⊂V, assume we want to predict the labels on
V2⊂V. Zhou & Schölkopf (2005) introduced label propagation (LP) to solve this task. First, the input labels
are extended by setting yi(0) = 0for eachi∈V\V1. The labels are then updated recursively according to
the following rule, that is derived as gradient flow of an energy ELPwe want to be minimized:
˙Y(t) =−1
2∇ELP(Y(t)),ELP(Y) :=EDir(Y) +µ∥Y−Y(0)∥2. (4)
5Published in Transactions on Machine Learning Research (MM/YYYY)
The gradient flow ensures that the prediction is attained by the minimizer of ELP, which both enforces
smoothness viaEDirand penalizes deviations from the available labels (soft boundary conditions).
Motivations. Our goal amounts to extending the gradient flow formalism from the parameter-free case
to a deep learning setting which includes features. We investigate when graph convolutions as in (1) admit
(parametric) energy functionals that decrease along the features computed at each layer.
4.2 Gradient flows on graphs: the learnable case
In the spirit of Haber & Ruthotto (2018); Chen et al. (2018), we regard the family of MPNNs introduced in
(1) as the Euler discretization of a continuous dynamical system. More specifically, we can introduce a step
sizeτ∈(0,1]and rewrite the class of MPNNs as
F(t+τ)−F(t) =τσ/parenleftbig
−F(t)Ωt+AF(t)Wt−F(0)˜Wt/parenrightbig
,
which then becomes the discretization of the system of differential equations
˙F(t) =σ/parenleftbig
−F(t)Ωt+AF(t)Wt−F(0)˜Wt/parenrightbig
,
where·denotes the time derivative. Understanding the mechanism underlying this dynamical system might
shed light on the associated, discrete family of MPNNs. By this parallelism, we study both the time-continuous
and time-discrete (i.e. layers) settings, and start by focusing on the former for the rest of the section.
4.2.1 Which energies are being minimized along graph-convolutional models?
Assume that Ain (1) issymmetric and let F(0)be the input features. We introduce a parametric function
Eθ:Rnd→R, parameterised by d×dweight matrices ΩandWof the form:
Eθ(F) =/summationdisplay
i,j⟨fi,Ωfi⟩
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Eext
Ω−/summationdisplay
i,jAij⟨fi,Wfj⟩
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Epair
W+/summationdisplay
i,jφ0(F,F(0))
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Esource
φ0, (5)
Note that we can recover the energies in Section 4.1. If Ω=W=Idandφ0= 0, thenEθ=EDiras per (2),
while ifφ0is anL2-penalty, thenEθ=ELPas per (4). We can also recover harmonic energies on graphs
(see Appendix B.5). Importantly, if we choose φ0(F,F(0)) = 2/summationtext
i⟨fi,˜Wfi(0)⟩, for ˜W∈Rd×d, then we can
rewrite (see Appendix B.1)
Eθ(F) =⟨vec(F),(Ω⊗In−W⊗A)vec(F) + 2( ˜W⊗In)vec(F(0))⟩. (6)
Thegradient flow ofEθcan then be simply derived as (once we divide the gradient by 2):
˙F(t) =−1
2∇FEθ(F(t)) =−F(t)/parenleftbiggΩ+Ω⊤
2/parenrightbigg
+AF(t)/parenleftbiggW+W⊤
2/parenrightbigg
−F(0)˜W. (7)
Since Ω,Wappear in (7) in a symmetrized way, without loss of generality we can assume ΩandWto be
symmetricd×dchannel mixing matrices. Therefore, (7) simplifies as
˙F(t) =−F(t)Ω+AF(t)W−F(0)˜W. (8)
Proposition 4.1. Assume that Ghas a non-trivial edge. The linear, time-continuous MPNNs of the form
˙F(t) =−F(t)Ω+AF(t)W−F(0)˜W,
are gradient flows of the energy in (6) if and only if the weight matrices ΩandWare symmetric.
Therefore, a large class of linear MPNNs evolve the features in the direction of steepest descent of an energy,
provided that the weight matrices are symmetric. Note that despite reducing the degrees of freedom, the
symmetry of the weight matrices does not diminish their power (Hu et al., 2019).
6Published in Transactions on Machine Learning Research (MM/YYYY)
4.2.2 Attraction and repulsion: a physics-inspired framework
Thanks to the existence of an energy Eθ, we can provide a simple explanation for the dynamics induced
by gradient-flow MPNNs as pairwise forces acting among adjacent features and generating attraction and
repulsion depending on the eigenvalues of the weight matrices.
Why gradient flows? A multi-particle point of view. Below, we think of node features as particles in
Rdwith energyEθ. In (5), the first term Eext
Ωisindependent of the pairwise interactions and hence represents
an ‘external’ energy in the feature space. The second term Epair
Winstead accounts for pairwise interactions
along edges via the symmetric matrix Wand hence represents an energy associated with the graph structure.
For simplicity, we set the source term φ0to zero and write W=Θ⊤
+Θ+−Θ⊤
−Θ−, by decomposing it into
components with positive and negative eigenvalues. We can then rewrite Eθin (5) as
Eθ(F) =/summationdisplay
i,j⟨fi,(Ω−W)fi⟩
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
graph-independent+1
2/summationdisplay
i,j∥Θ+(∇F)ij∥2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
attraction−1
2/summationdisplay
i,j∥Θ−(∇F)ij∥2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
repulsion, (9)
which we have derived in Appendix B. Consider now the gradient flow (8) where features F(t)are evolved
in the direction of steepest descent of Eθ. Recall that the edge gradient (∇F(t))ijis defined edge-wise and
measures the difference between features fi(t)andfj(t)along (i,j)∈E. We note that:
(i)The channel-mixing Wencodesattractive edge-wise interactions via its positive eigenvalues since the
gradient terms∥Θ+(∇F(t))ij∥are being minimized along (8), resulting in a smoothing effect where
the edge-gradient is shrinking along the eigenvectors of Wassociated with positive eigenvalues;
(ii)The channel-mixing Wencodesrepulsive edge-wise interactions via its negative eigenvalues since the
gradient terms−∥Θ−(∇F(t))ij∥are being minimized along (8), resulting in a sharpening effect where
the edge-gradient is expanding along the eigenvectors of Wassociated with negative eigenvalues.
Next, we formalize the smoothing vs sharpening effects discussed in (i) and (ii) in a more formal way.
4.3 Low vs high frequency dominant dynamics: a new measure
Attractive forces reduce the edge gradients and are associated with smoothing effects which magnify low
frequencies, while repulsive forces increase the edge gradients and hence afford a sharpening action enhancing
the high frequencies. To determine which frequency is dominating the dynamics, we propose to monitor
thenormalized Dirichlet energy: EDir(F(t))/∥F(t)∥2. This is the Rayleigh quotient ofId⊗∆and so it
satisfies 0≤EDir(F)/∥F∥2≤λn−1(see Appendix A.2). If this Rayleigh quotient is approaching its minimum,
then the lowest frequency component is dominating, whereas if it is approaching its maximum, then the
dynamics is dominated by the highest frequencies. This allows us to introduce the following characterization,
of independent interest, to measure the frequency-response of spatial MPNNs:
Definition 4.2. Given a time-continuous (discrete) MPNNcomputing features F(t)at each time (layer) t,
we say that the MPNNisLow-Frequency-Dominant (LFD) ifEDir(F(t))/∥F(t)∥2→0fort→∞. Conversely,
we say that the MPNNisHigh-Frequency-Dominant (HFD) ifEDir(F(t))/∥F(t)∥2→λn−1ast→∞.
Note that the LFDcharacterization differs from the notion of over-smoothing introduced in Cai & Wang
(2020); Rusch et al. (2022), since it also accounts for the norm of the features. In fact, in Appendix B.2 we
derive how our formulation also captures those cases where the Dirichlet energy is not converging to zero, yet
the lowest frequency component is growing the fastest as time increases. Our notion of HFD-dynamics is also
novel since previous works typically focused on showing when graph convolutions behave as low-pass filters.
Similarly to previous works, we now focus on a class of graph-convolutions inspired by GCNand show that
even if we remove the terms Ωand ˜Wfrom (1), graph-convolutions can enhance the high frequencies and
ultimately lead to a HFDdynamics opposite to over-smoothing.
Assumption. We let A=D−1/2AD−1/2and consider the simplified gradient flows with Ω=˜W=0.
7Published in Transactions on Machine Learning Research (MM/YYYY)
Theorem 4.3. Given a continuous MPNNof the form ˙F(t) =AF(t)W, letµ0<µ 1≤...≤µd−1be the
eigenvalues of W. If|µ0|(λn−1−1)>µd−1, then for almost every F(0), the MPNNisHFD. Conversely, if
|µ0|(λn−1−1)<µd−1, then for almost every input F(0), the MPNNisLFD.
We provide convergence rates in Theorem B.3 in the Appendix. In accordance with (9), Theorem 4.3 shows
that the channel-mixing matrix Wcan generate both attractive and repulsive forces along edges, leading
to either an LFDor a HFDdynamics. In fact, the MPNNisHFDwhen Whas a negative eigenvalue µ0
sufficiently larger than the most positive one µd−1– and viceversa for the LFDcase. Therefore, Theorem 4.3
provides affirmative answers to Q.1, Q.2 in Section 1 in the time-continuous case, by showing that a simple
class of gradient flow convolutions on graphs can learn to enhance the high frequencies and lead to a HFD
dynamics where the highest frequency components dominate in the limit of many layers.
Remark. We note thatEθcan generally be negative and unbounded. With slight abuse of nomenclature,
we call them ‘energy’ since the associated MPNNs follow the direction of steepest descent of such functional.
5 The interactions between the graph and channel-mixing spectra
In this Section we consider discretized gradient flows, so that we can extend the analysis to the case of layers
of an architecture. We consider MPNNs that we derive by taking the Euler discretization of the gradient flow
equations in Theorem 4.3; given a step size τandW∈Rd×dsymmetric , we have
F(t+τ) =F(t) +τAF(t)W,F(0) =ψEN(F0), (10)
where an encoderψEN:Rn×p→Rn×dprocesses input features F0and the prediction ψDE(F(T))is produced
by adecoderψDE:Rn×d→Rn×K. Here,Kis the number of label classes, T=mτis theintegration time ,
andmis the number of layers. We note that (i) typically ψEN,ψDEare MLPs, making the entire framework
in (10) non-linear; (ii) since we have a residual connection, this is notequivalent to collapsing the dynamics
into a single layer with aggregation matrix Amas done in Wu et al. (2019) — see (31) in the Appendix.
5.1 Discrete gradient flows and spectral analysis
The gradient flow in (10) is a linear, residual GCNwith nonlinear encoder and decoder operations. Once
we vectorize the features F(t)∝⇕⊣√∫⊔≀→vec(F(t))∈Rnd, we can rewrite the update as vec(F(t+τ)) = vec(F(t)) +
τ(W⊗A) vec(F(t))(see Appendix A.2 for details). In particular, if we pick bases {ψr}⊂Rdand{ϕℓ}⊂Rn
of orthonormal eigenvectors for Wand∆respectively, we can write the features after mlayers explicitly :
vec(F(mτ)) =d−1/summationdisplay
r=0n−1/summationdisplay
ℓ=0(1 +τµr(1−λℓ))mcr,ℓ(0)ψr⊗ϕℓ, (11)
wherecr,ℓ(0) :=⟨vec(F(0)),ψr⊗ϕℓ⟩and{µr}are the eigenvalues of W. We see that the interaction of
the spectra{µr}and{λℓ}is the ‘driving’ factor for the dynamics, with positive (negative) eigenvalues of
Wmagnifying the frequencies λℓ<1(>1respectively). In fact, note that the projection of the features
onto the kernel of Wstay invariant . In the following we let µ0≤µ1≤...≤µd−1be in ascending order.
Note thatϕn−1is the Laplacian eigenvector associated with largest frequency λn−1. We formulate the result
below in terms of the following:
µd−1
λn−1−1<|µ0|<2
τ(2−λn−1). (12)
Note that if (12) holds, then µ0<0sinceλn−1−1<1whenever Gis not bipartite. The first inequality
means that the negative eigenvalues of Wdominate the positive ones (once we factor in the graph spectrum
contribution), while the second is a constraint on the step-size since if τis too large, then we no longer
approximate the gradient flow in (8). We restrict to the case where µ0,µd−1are simple eigenvalues, but
extending the results to the degenerate case is straightforward.
8Published in Transactions on Machine Learning Research (MM/YYYY)
Theorem 5.1. Consider an MPNNwith update rule F(t+τ) =F(t) +τAF(t)W, with Wsymmetric. Given
mlayers, if (12) holds, then there exists δ<1s.t. for alli∈Vwe have:
fi(mτ) = (1 +τ|µ0|(λn−1−1))m(c0,n−1(0)ϕn−1(i)·ψ0+O(δm)). (13)
Conversely, if µd−1>|µ0|(λn−1−1), then
fi(mτ) = (1 +τµd−1)m/parenleftig
cd−1,0(0)/radicaligg
di
2|E|·ψd−1+O(δm)/parenrightig
. (14)
We first note that δis reported in (32) in Appendix C.1. If (13) holds, then repulsive forces (high frequencies)
dominate since for all i∈V, we have fi(mτ)∼ϕn−1(i)·ψ0, up tolower order terms in the number of layers .
Thus as we increase the depth, any feature fi(mτ)becomes dominated by a multiple of ψ0∈Rdthat only
changes based on the value of the Laplacian eigenvector ϕn−1at nodei. Conversely, if (14) holds, then
fi(mτ)∼√di·ψd−1, meaning that the features become dominated by a multiple of ψd−1only depending on
the degree of i– which recovers the over-smoothing phenomenon (Oono & Suzuki, 2020).
Corollary 5.2. If (13) holds, then the MPNNisHFDfor almost every F(0)andF(mτ)/∥F(mτ)∥converges
toF∞s.t.∆fr
∞=λn−1fr
∞for eachr. Conversely, if (14) holds, then the MPNNisLFDfor almost every
F(0)andF(mτ)/∥F(mτ)∥converges to F∞s.t.∆fr
∞=0for eachr.
Over-smoothing and over-sharpening. Our analysis shows that (i) linear graph-convolutional equations
can induce a sharpening effect (13) (which answers Q.1 in Section 1 affirmatively); (ii) graph convolutions
can avoid over-smoothing through the negative eigenvalues of the channel-mixing by incurring the opposite
behaviour of over-sharpening (i.e.HFDbehaviour) in the limit of many layers, which provides an affirmative
answer to Q.2. As per Corollary 5.2, over-sharpening entails that the features converge, up to rescaling,
to the eigenvector of ∆associated with the largest frequency. Similarly, Corollary 5.2 implies that LFDis
a rigorous characterization of over-smoothing, given that features converge to a vector with zero Dirichlet
energy in the limit of many layers. Accordingly, both over-smoothing and over-sharpening afford a loss of
information, since the features converge to the Laplacian eigenvector with minimal or maximal Dirichlet
energy, respectively. When the number of layers is small though, the features F(mτ)also depend on the
lower-order terms of the asymptotic expansion in Theorem 5.1; whether the dynamics is LFDorHFDwill
then affect if the lower or higher frequencies have a larger contribution to the prediction.
The role of the residual connection. The following result shows that the residual connection is crucial
for the emergence of the over-sharpening ( HFD)regime:
Theorem 5.3. IfGis not bipartite, and we remove the residual connection, i.e. F(t+τ) =τAF(t)W, with
Wsymmetric, then the dynamics is LFDfor almost every F(0), independent of the spectrum of W.
We see that the residual connection enables the channel-mixing to steer the evolution towards low or high
frequencies depending on the task. If we drop the residual connection, Wis less powerful and the only
asymptotic regime is over-smoothing ( LFD), independent of the spectrum of W.
5.2 Frequency response and heterophily
Negative eigenvalues flip the edge signs. LetW=Ψdiag(µ)Ψ⊤be the eigendecomposition of
Wyielding the Fourier coefficients Z(t) =F(t)Ψ. We rewrite the discretized gradient flow F(t+τ) =
F(t) +τAF(t)Win the Fourier domain of WasZ(t+τ) =Z(t) +τAZ(t)diag(µ)and note that along the
eigenvectors of W, ifµr<0, then the dynamics is equivalent to flipping the sign of the edges. Therefore,
negative edge-weight mechanisms as those proposed in Bo et al. (2021); Yan et al. (2021) to deal with
heterophilic graphs, can in fact be simply achieved by a residual graph convolutional model where the
channel-mixing matrix Whas negative eigenvalues. We refer to (33) in the appendix for more details.
Can other ‘time-continuous’ MPNNs be HFD?A framework that cannot enhance the high frequencies
may struggle on heterophilic datasets. While by Theorem 5.1 we know that gradient-flow MPNNs can be HFD,
9Published in Transactions on Machine Learning Research (MM/YYYY)
we prove that some existing time-continuous MPNNs –CGNN(Xhonneux et al., 2020), GRAND (Chamberlain
et al., 2021a), PDE−GCN DEliasof et al. (2021) – are never HFD; in fact, in our experiments (Table 3) we
validate that these models are more vulnerable to heterophily – for an explicit statement including convergence
rates we refer to Theorem B.4.
Theorem 5.4 (Informal) .CGNN,GRANDandPDE−GCNDinduce smoothing and are never HFD.
Summary of the theoretical results. We have analysed a class of linear MPNNs that represent discrete
gradient flows of Eθ. This provides a ‘multi-particle’ interpretation for graph convolutions and sheds light
onto the dynamics they generate. We have shown that the interaction between the eigenvectors and spectra
ofWand∆is what drives the dynamics. We have also proven how such interaction ultimately leads to
two opposite asymptotic regimes referred to as over-smoothing ( LFD) and over-sharpening ( HFD). This
has allowed us to draw connections with more recent frameworks, proving that simple convolutional models
are not necessarily bound to induce smoothing among features, provided that we have a residual connection ,
and can indeed also magnify the high frequencies, a property that may be desirable on heterophilic tasks,
ultimately providing affirmative answers to Q.1 and Q.2 in Section 1.
6Extending the analysis to non-linear layers: the family of energy-dissipating MPNNs
As discussed above, analysing energies along MPNNs is a valuable approach for investigating their dynamics.
Cai & Wang (2020); Bodnar et al. (2022) showed that, under some assumptions, EDiris decreasing (expo-
nentially) along some classes of graph convolutions, implying over-smoothing – see also Rusch et al. (2022).
Consider the general family of (time-continuous) graph convolutions as in (1), with σa nonlinear activation:
˙F(t) =σ/parenleftbig
−F(t)Ω+AF(t)W−F(0)˜W/parenrightbig
. (15)
Although this is no longer a gradient flow (unless σis linear), we prove that if the weights are symmetric,
then the energyEθin (6) still decreases along (15).
Theorem 6.1. Considerσ:R→Rsatisfyingx∝⇕⊣√∫⊔≀→xσ(x)≥0. IfFsolves (15) with Ω,Wbeing symmetric,
thent∝⇕⊣√∫⊔≀→Eθ(F(t))is decreasing. If we discretize the system with step size τand letcdenote the most positive
eigenvalue of Ω⊗In−W⊗A– if no positive eigenvalues exists take c= 0– then
Eθ(F(t+τ))−Eθ(F(t))≤c∥F(t+τ)−F(t)∥2.
An important consequence of Theorem 6.1 is that for non-linear graph convolutions with symmetric weights,
the physics interpretation is preserved since the same multi-particle energy Eθin (9) dissipates along the
features. Note that in the time-discrete setting, the inequality can be interpreted as a Lipschitz regularity
result. In general, in the nonlinear case we are no longer able to derive exact results in the limit of many
layers, yet the channel-mixing Wstill induces attraction/repulsion along edges via its positive/negative
eigenvalues (see Lemma D.1). Theorem 6.1 differs from Cai & Wang (2020); Bodnar et al. (2022) in two ways:
(i) It asserts monotonicity of an energy Eθmore general than EDir, since it is parametric and in fact also able
to enhance the high frequencies; (ii) it holds for an infinite class of non-linear activations (beyond ReLU).
7 Experimental validation of the theoretical results
In this Section we validate the theoretical results through ablations and real-world experiments on node
classification tasks. First, we study a linear, gradient-flow MPNNof the form:
F(t+ 1) = F(t) +AF(t)WS,WS:= (W+W⊤)/2, (16)
where W∈Rd×dis a learnable weight matrix. Thanks to Proposition 4.1 we know that this is a discretized
gradient flow that minimizes Eθin (6). In particular, from Theorem 5.1 we derive that the positive eigenvalues
ofWSinduce attraction along the edges, while the negative eigenvalues generate repulsion. We note that
while the MPNN-update is linear, the map associating a label to each node based on its input feature, is
actually non-linear, because the encoder and decoder are, typically, non-linear MLPs. Since this family of
10Published in Transactions on Machine Learning Research (MM/YYYY)
Dataset Texas Wisconsin Cornell Film Squirrel Chameleon Citeseer Pubmed Cora
Homophily 0.11 0.21 0.30 0.22 0.22 0.23 0.74 0.80 0.81
#Nodes 183 251 183 7,600 5,201 2,277 3,327 18,717 2,708
#Edges 295 466 280 26,752 198,493 31,421 4,676 44,327 5,278
GCN 60.81 ±4.4 51.57 ±3.51 59.19 ±4.75 29.96 ±0.84 42.77 ±2.38 62.63 ±2.02 75.9 ±1.4 87.31 ±0.44 86.0 ±1.0
SGCNgf80.54 ±4.95 82.75 ±4.54 74.32 ±6.97 33.72 ±0.76 51.02 ±1.68 68.57 ±1.73 76.4 ±1.72 88.39 ±0.39 87.12 ±0.61
GCNgf84.86 ±4.22 84.12 ±2.97 77.3 ±7.85 35.13 ±0.61 50.84 ±1.92 68.27 ±1.45 76.82 ±1.59 88.49 ±0.42 87.79 ±0.93
Table 1: Comparison of GCNand the models in (16) and (17) over datasets of varying homophily.
graph convolutions amount to a gradient-flow, simplified (i.e. linear) GCNwith a residual-connection, we
adopt the notation SGCNgf(we recall that A=D−1/2AD−1/2).
By Theorem 6.1, if we ‘activate’ (16) as
F(t+ 1) = F(t) +σ/parenleftig
AF(t)WS/parenrightig
,WS:= (W+W⊤)/2 (17)
withσs.t.xσ(x)≥0, thenEθin (6) is decreasing, so that we can think of such equations as more general
‘approximate’ gradient flows. With slight abuse of notations, we refer to (17) as GCNgf, since this is just a
GCN-model with a residual connection and symmetric weights, which ensure the monotonicity of the energy.
Real-world experiments: performance on node-classification tasks. By Theorem 5.1 and The-
orem 6.1, both (16) and (17) can induce repulsion along the edges through the negative eigenvalues of
WS. Therefore, these models should be more robust to the heterophily of the graph when compared to the
classical implementation of GCN– in fact, note that a linear GCNwith symmetric weights is always LFDas
per Theorem 5.3, and that non-linear GCNis bound to over-smooth if the singular values are sufficiently
small (Oono & Suzuki, 2020; Cai & Wang, 2020). To validate this point, we run all three models on node
classification tasks defined over graphs with varying homophily (Sen et al., 2008; Rozemberczki et al., 2021;
Pei et al., 2020) (details in Appendix E). Training, validation and test splits are taken from Pei et al.
(2020) for all datasets for comparison. The results are reported in Table 1. For all the datasets shown, we
performed a simple, grid search over the space m∈{2,4,8}– recall that mis the depth – learning rate
∈{0.001,0.005}and decay∈{0.0005,0.005,0.05}. We find that both (16) and (17) surpass the performance
ofGCN, often by a significant margin, across all heterophilic datasets. We emphasize that this is in support
of our theoretical findings, since we proved that thanks to a residual connection , graph-convolutional
models enable the channel-mixing to also generate repulsion along the edges through its negative eigenvalues
hence compensating for the underlying heterophily of the graphs. In particular, the results also show that
imposing a symmetrization over the weights has no negative impact on performance.
Remark. It is possible to significantly improve the results by considering additional Ωand ˜W-terms as
in (10) and tuning, achieving strong results that often surpass more complicated benchmarks specifically
designed for heterophilic tasks. Our results then further bring about questions about the quality of these tasks
and whether graph-convolutional models really need to be augmented or replaced for heterophilic tasks. Since
the main goal of the Section amounts to validating the theory, we report these results in Appendix E .
Dataset Squirrel Chameleon Pubmed Cora
Homophily 0.22 0.23 0.8 0.81
−µd−1/µ0 0.68 0.71 2.43 9.58
EDir(F(m))/∥F(m)∥2
EDir(F(0))/∥F(0)∥20.83 0.65 0.35 0.19
Table 2: Spectral properties of the weights of SGCNgfand behaviour of
the (normalized) Dirichlet energy.Spectral analysis. To corrobo-
rateouranalysis, wealsoinvestigate
spectralpropertiesoftheweightma-
trixWS. We report statistics of the
gradient-flow MPNNin (16) after
training, over 4 datasets (two het-
erophilic ones, and two homophilic
ones). We recall that µd−1andµ0
arethemostpositiveandmostnega-
tive eigenvalues of WS, respectively.
11Published in Transactions on Machine Learning Research (MM/YYYY)
0 10 20 30 40 50 60
time0.000.250.500.751.001.251.501.75RQfilm RQ
cornell RQ
wisconsin RQ
texas RQ
squirrel RQ
chameleon RQ
Pubmed RQ
Citeseer RQ
Cora RQ
Figure 2: Rayleigh Quotient of Id⊗∆(i.e.
EDir(F(t))/∥F(t)∥2) on real graphs for SGCNgf.
0 1 2 3 4 5 6 7 8
time0.000.250.500.751.001.251.501.752.00RQSGCN_gf bipartite RQ
gcn bipartite RQFigure 3: Rayleigh quotient of Id⊗∆on a com-
plete bipartite graph for SGCNgfandGCN.
In fact, by Theorem 5.1 we know that the ratio −µd−1/µ0indicates whether the underlying dynamics is
dominated by the high frequencies (ratio smaller than 1), or by the low frequencies (ratio larger than one).
We confirm this behaviour in Table 2, where we see that the quantity −µd−1/µ0is larger on the homophilic
graphs and is instead smaller on the heterophilic graphs, where the model generally learns to enhance the
high-frequency components more. We also monitor the profile of the normalized Dirichlet energy. We see
that on homophilic graphs the low frequencies are indeed dominant since the normalized Dirichlet energy
at the final layer is much lower than the one evaluated over the input features. This validates that our
characterization of over-smoothing ( LFDdynamics) and over-sharpening ( HFDdynamics) is appropriate to
study the dynamics of GNNs.
Over-smoothing and over-sharpening. We also validate our theoretical results that linear gradient-flows
MPNNs as in (16) admit two, opposite, behaviours in the limit of many layers, depending on the eigenvalues
ofWS: (i) over-smoothing, i.e. LFDdynamics where the normalized Dirichlet energy approaches zero and (ii)
over-sharpening, i.e. HFDdynamics where the normalized Dirichlet energy approaches the largest eigenvalue
of the normalized Laplacian λn−1∈[1,2). Accordingly, we report the values of the normalized Dirichlet energy
(i.e. the Rayleigh quotient of Id⊗∆, simply denoted by RQ) over several real-world datasets; here we are
interested in validating that both over-smoothing and over-sharpening occur and that no other intermediate
case is possible. Figure 2 confirms our theoretical analysis, showing that after training, the normalized
Dirichlet energy approaches zero in 7 out of 9 cases ( LFDdynamics, i.e. over-smoothing ) whereas it converges
to its maximum, depending on the graph, on the remaining two cases ( HFDsetting, i.e. over-sharpening ),
and that no other intermediate case arises. We point out that in this experiment we study the limiting
behaviour of the underlying MPNNs and not the actual performance, which will of course degrade as we
increase the number of layers for both over-smoothing and over-sharpening. To further support our claims,
we also consider the synthetic case of a complete, bipartite graph with maximal heterophily (i.e. each node is
connected to all other nodes with different label). In this extreme case, over-sharpening does actually provide
the optimal classifier since the eigenvector of the graph Laplacian associated with largest frequency separates
the two communities perfectly; in Figure 3 we see that for (16) the dynamics is HFD(i.e. over-sharpening),
while a standard GCNover-smooths due to the lack of a residual connection as per Theorem 5.3.
8 Conclusions
Summary and the messages of the work. In this work we have shown that graph-convolutional models
can enhance the high-frequency components of the features via the negative eigenvalues of the weight matrices,
provided that there is a residual connection. In particular, we have studied a simple class of graph convolutions
that represent the gradient flow of a parametric functional Eθ. The dynamics of gradient-flow MPNNs is fully
characterized in terms of the interactions between the eigenvalues of the (symmetric) weight matrices and
those of the graph Laplacian, ultimately leading to either over-smoothing or over-sharpening, in the limit of
12Published in Transactions on Machine Learning Research (MM/YYYY)
many layers. We have also extended some of the conclusions to gradient flows activated by non-linear maps
and have finally validated our analysis through ablation studies and real-world experiments.
Our work suggests that: (i) It is possible to shed light on the dynamics of GNNs once we identify an energy
function that is decreasing along the features, thereby providing a general framework for analyzing existing
models as well as building more principled ones; (ii) graph convolutions are not simple low-pass filters but can
also learn to generate edge-wise repulsion once we have a residual connection; (iii) the role of heterophily as a
major issue for GNNs and/or the validity of benchmarks commonly adopted in node classification, are brought
into question, given that simple GCNmodels with residual connections are already on par, or better than
GNN models designed specifically to deal with heterophily; (iv) the idea that over-smoothing is the dominant
or only asymptotic behaviour for very deep MPNNs is questioned, considering that graph convolutions can
also incur over-sharpening if the negative eigenvalues of the weight matrices are sufficiently large.
Limitations and future work. We limited our attention to a class of energy functionals whose gradient
flows give rise to evolution equations of the graph-convolutional type. In future work, we plan to study
other families of energies that generalize different GNN architectures and provide new models that are
more ‘physics’-inspired; we believe this to be a powerful paradigm to design new GNNs. To the best of our
knowledge, our analysis is a first step into studying the interaction of the graph and ‘channel-mixing’ spectra;
it will be interesting to explore more general dynamics (i.e., that are neither LFDnorHFD).
Acknowledgements. We thank the anonymous reviewers for valuable suggestions and feedback.
13Published in Transactions on Machine Learning Research (MM/YYYY)
References
Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In
International Conference on Learning Representations , 2021. URL https://openreview.net/forum?id=
i80OPhOCVH2 .
Muhammet Balcilar, Guillaume Renton, Pierre Héroux, Benoit Gaüzère, Sébastien Adam, and Paul Honeine.
Analyzing the expressive power of graph neural networks in a spectral perspective. In International
Conference on Learning Representations , 2020.
Lukas Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb.com/ . Software
available from wandb.com.
Marin Biloš, Johanna Sommer, Syama Sundar Rangapuram, Tim Januschowski, and Stephan Gün-
nemann. Neural flows: Efficient alternative to neural odes. In Advances in Neural Information
Processing Systems , volume 34, 2021. URL https://proceedings.neurips.cc/paper/2021/file/
b21f9f98829dea9a48fd8aaddc1f159d-Paper.pdf .
Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. Beyond low-frequency information in graph convolutional
networks. In AAAI. AAAI Press , 2021.
Cristian Bodnar, Francesco Di Giovanni, Benjamin Paul Chamberlain, Pietro Liò, and Michael M Bronstein.
Neural sheaf diffusion: A topological perspective on heterophily and oversmoothing in gnns. arXiv preprint
arXiv:2202.04579 , 2022.
Xavier Bresson and Thomas Laurent. Residual gated graph convnets. arXiv preprint arXiv:1711.07553 , 2017.
Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veličković. Geometric deep learning: Grids,
groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478 , 2021.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected
networks on graphs. In 2nd International Conference on Learning Representations, ICLR 2014 , 2014.
Chen Cai and Yusu Wang. A note on over-smoothing for graph neural networks. arXiv preprint
arXiv:2006.13318 , 2020.
Ben Chamberlain, James Rowbottom, Maria I Gorinova, Michael Bronstein, Stefan Webb, and Emanuele
Rossi. Grand: Graph neural diffusion. In International Conference on Machine Learning , pp. 1407–1418.
PMLR, 2021a.
Benjamin Chamberlain, James Rowbottom, Davide Eynard, Francesco Di Giovanni, Xiaowen Dong, and
Michael Bronstein. Beltrami flow and neural diffusion on graphs. Advances in Neural Information Processing
Systems, 34, 2021b.
Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional
networks. In International Conference on Machine Learning , pp. 1725–1735. PMLR, 2020.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential
equations. Advances in neural information processing systems , 31, 2018.
Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank graph
neural network. In 9th International Conference on Learning Representations, ICLR 2021 , 2021. URL
https://openreview.net/forum?id=n6jl7fLxrP .
Fan RK Chung and Fan Chung Graham. Spectral graph theory . Number 92. American Mathematical Soc.,
1997.
Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs
with fast localized spectral filtering. Advances in neural information processing systems , 29, 2016.
14Published in Transactions on Machine Learning Research (MM/YYYY)
Francesco Di Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro Lio, and Michael Bronstein.
On over-squashing in message passing neural networks: The impact of width, depth, and topology. arXiv
preprint arXiv:2302.02941 , 2023.
James Eells and Joseph H Sampson. Harmonic mappings of riemannian manifolds. American journal of
mathematics , 86(1):109–160, 1964.
Moshe Eliasof, Eldad Haber, and Eran Treister. Pde-gcn: Novel architectures for graph neural networks
motivated by partial differential equations. Advances in Neural Information Processing Systems , 34, 2021.
Moshe Eliasof, Lars Ruthotto, and Eran Treister. Improving graph neural networks with learnable propagation
operators. 2023.
Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR
Workshop on Representation Learning on Graphs and Manifolds , 2019.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message
passing for quantum chemistry. In International Conference on Machine Learning , pp. 1263–1272. PMLR,
2017.
Christoph Goller and Andreas Kuchler. Learning task-dependent distributed representations by backprop-
agation through structure. In Proceedings of International Conference on Neural Networks (ICNN’96) ,
volume 1, pp. 347–352. IEEE, 1996.
Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains. In
Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005. , volume 2, pp. 729–734.
IEEE, 2005.
E. Haber and L. Ruthotto. Stable architectures for deep neural networks. Inverse Problems , 34, 2018.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances
in neural information processing systems , 30, 2017.
David K Hammond, Pierre Vandergheynst, and Rémi Gribonval. The spectral graph wavelet transform:
Fundamental theory and fast computation. In Vertex-Frequency Analysis of Graph Signals , pp. 141–175.
Springer, 2019.
Mingguo He, Zhewei Wei, Hongteng Xu, et al. Bernnet: Learning arbitrary graph spectral filters via bernstein
approximation. Advances in Neural Information Processing Systems , 34, 2021.
Shell Xu Hu, Sergey Zagoruyko, and Nikos Komodakis. Exploring weight symmetry in deep neural networks.
Computer Vision and Image Understanding , 187:102786, 2019.
Ron Kimmel, Nir Sochen, and Ravi Malladi. From high energy physics to low level vision. In International
Conference on Scale-Space Theories in Computer Vision , pp. 236–247. Springer, 1997.
Thomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph Convolutional Networks.
InProceedings of the 5th International Conference on Learning Representations , ICLR ’17, 2017. URL
https://openreview.net/forum?id=SJU4ayYgl .
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-
supervised learning. In Proceedings of the AAAI conference on artificial intelligence , volume 32, 2018.
Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar Bhalerao, and Ser Nam Lim.
Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods. Advances
in Neural Information Processing Systems , 34:20887–20902, 2021.
Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen Chang, and
Doina Precup. Is heterophily a real nightmare for graph neural networks to do node classification? arXiv
preprint arXiv:2109.05641 , 2021.
15Published in Transactions on Machine Learning Research (MM/YYYY)
Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen Chang, and
Doina Precup. Revisiting heterophily for graph neural networks. Advances in neural information processing
systems, 35:1362–1375, 2022.
Sunil Kumar Maurya, Xin Liu, and Tsuyoshi Murata. Improving graph neural networks with simple
architecture design. arXiv preprint arXiv:2105.07634 , 2021.
Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan, and
Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In AAAI Conference
on Artificial Intelligence , pp. 4602–4609. AAAI Press, 2019.
Hoang Nt and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters. arXiv
preprint arXiv:1905.09550 , 2019.
KentaOonoandTaijiSuzuki. Graphneuralnetworksexponentiallyloseexpressivepowerfornodeclassification.
InInternational Conference on Learning Representations , 2020.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS .
2019.
Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph
convolutional networks. In 8th International Conference on Learning Representations, ICLR 2020 , 2020.
Pietro Perona and Jitendra Malik. Scale-space and edge detection using anisotropic diffusion. PAMI, 12(7):
629–639, 1990.
Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A
critical look at the evaluation of gnns under heterophily: are we really making progress? arXiv preprint
arXiv:2302.11640 , 2023.
Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. Journal of
Complex Networks , 9(2):cnab014, 2021.
T Konstantin Rusch, Benjamin P Chamberlain, James Rowbottom, Siddhartha Mishra, and Michael M
Bronstein. Graph-coupled oscillator networks. In International Conference on Machine Learning , 2022.
T Konstantin Rusch, Michael M Bronstein, and Siddhartha Mishra. A survey on oversmoothing in graph
neural networks. arXiv preprint arXiv:2303.10993 , 2023.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph
neural network model. IEEE transactions on neural networks , 20(1):61–80, 2008.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective
classification in network data. AI magazine , 29(3):93–93, 2008.
Alessandro Sperduti. Encoding labeled graphs by labeling raam. Advances in Neural Information Processing
Systems, 6, 1993.
Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M Bronstein.
Understanding over-squashing and bottlenecks on graphs via curvature. International Conference on
Learning Representations , 2022.
Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio.
Graph attention networks. In International Conference on Learning Representations , 2018. URL https:
//openreview.net/forum?id=rJXMpikCZ .
16Published in Transactions on Machine Learning Research (MM/YYYY)
Yuelin Wang, Kai Yi, Xinliang Liu, Yu Guang Wang, and Shi Jin. Acmp: Allen-cahn message passing for
graph neural networks with particle phase transition. arXiv preprint arXiv:2206.05437 , 2022.
Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph
convolutional networks. In International conference on machine learning , pp. 6861–6871. PMLR, 2019.
URL http://proceedings.mlr.press/v97/wu19e.html .
Louis-Pascal Xhonneux, Meng Qu, and Jian Tang. Continuous graph neural networks. In International
Conference on Machine Learning , pp. 10432–10441. PMLR, 2020. URL http://proceedings.mlr.press/
v119/xhonneux20a.html .
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In
ICLR. OpenReview.net, 2019.
Yujun Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Danai Koutra. Two sides of the same coin:
Heterophily and oversmoothing in graph convolutional neural networks. arXiv preprint arXiv:2102.06462 ,
2021.
Yu-Li You, Wenyuan Xu, Allen Tannenbaum, and Mostafa Kaveh. Behavioral analysis of anisotropic diffusion
in image processing. IEEE Transactions on Image Processing , 5(11):1539–1553, 1996.
Chengxi Zang and Fei Wang. Neural dynamics on complex networks. In Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining , pp. 892–902, 2020.
Xitong Zhang, Yixuan He, Nathan Brugnone, Michael Perlmutter, and Matthew Hirn. Magnet: A neural
network for directed graphs. Advances in neural information processing systems , 34:27003–27015, 2021.
Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. arXiv preprint
arXiv:1909.12223 , 2019.
Dengyong Zhou and Bernhard Schölkopf. Regularization on discrete spaces. In Joint Pattern Recognition
Symposium , pp. 361–368. Springer, 2005.
Kaixiong Zhou, Xiao Huang, Daochen Zha, Rui Chen, Li Li, Soo-Hyun Choi, and Xia Hu. Dirichlet energy
constrained learning for deep graph neural networks. Advances in Neural Information Processing Systems ,
34:21834–21846, 2021.
Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily
in graph neural networks: Current limitations and effective designs. Advances in Neural Information
Processing Systems , 33:7793–7804, 2020.
17Published in Transactions on Machine Learning Research (MM/YYYY)
Overview of the appendix
To facilitate navigating the appendix, where we report several additional theoretical results, analysis of
different cases along with further experiments and ablation studies, we provide the following detailed outline.
•In Appendix A.1 we review properties of the classical Dirichlet energy on manifolds that inspired
traditional PDE variational methods for image processing whose extension to graphs and GNNs more
specifically partly constitutes one of the main motivations of our work. We also review important
elementary properties of the Kronecker product of matrices that are used throughout our proofs in
Appendix A.2. We finally comment on the choice of the normalization (and symmetrization) of the
graph Laplacian, briefly mentioning the impact of different choices.
•In Appendix B.1 we derive the energy decomposition reported in (9). In Appendix B.2 we derive
additional rigorous results to justify our characterization of LFDandHFDdynamics in Definition 4.2
along with examples. We also formalize (and prove) more quantitatively Theorem 4.3 in Theorem B.3.
In Appendix B.3 we report Theorem 5.4 including convergence rates and over-smoothing results. In
Appendix B.4 we explore the special case of Ω=Wwhich is equivalent to choosing ∆rather than
Aas message-passing matrix, hence providing new arguments as to why propagating messages using
Arather than ∆is ‘more robust’ . Finally in Appendix B.5 we formally derive an analogy between
the continuous energy used for manifolds (images) and a subset of the parametric energies in (5).
•In Appendix C we prove Theorem 5.1, Corollary 5.2, and Theorem 5.3.
•In Appendix D we prove Theorem 6.1 and an extra result confirming that even in the non-linear case
the channel-mixing Wstill induces attraction and repulsion via its spectrum, thus magnifying the
low or high frequencies respectively.
•In AppendixE wereport additional details on theevaluation along withfurther real-world experiments
and ablation studies.
Additional notations and conventions used throughout the appendix. Any graph Gis taken to be
connected . We order the eigenvalues of the graph Laplacian as 0 =λ0≤λ1≤...≤λn−1=λn−1≤2with
associated orthonormal basis of eigenvectors {ϕℓ}n−1
ℓ=0so that in particular we have ∆ϕ0=0. Moreover, given
a symmetric matrix B, we generally denote the spectrum of Bbyspec(B)and if Bis positive semi-definite
(written as B⪰0), then gap(B)denotes the positive, smallest eigenvalue ofB. Finally, if we write F(t)/∥F(t)∥
we always take the norm to be the Frobenius one and tacitly assume that the dynamics is s.t. the solution is
not zero. Without losing generality, below we always take A=D−1/2AD−1/2.
A Motivations and important preliminaries
A.1 Discussion on continuous Dirichlet energy and harmonic maps
Starting point: a geometric parallelism. To motivate a gradient-flow approach for GNNs, we start
from the continuous case. Consider a smooth map f:Rn→(Rd,h)withha constant metric represented by
H⪰0. TheDirichlet energy offis defined by
E(f,h) =1
2/integraldisplay
Rn∥∇f∥2
hdx=1
2d/summationdisplay
q,r=1n/summationdisplay
j=1/integraldisplay
Rnhqr∂jfq∂jfr(x)dx (18)
and measures the ‘smoothness’ of f. A natural approach to find minimizers of E- calledharmonic maps
- was introduced in Eells & Sampson (1964) and consists in studying the gradient flow ofE, wherein a
given map f(0) =f0is evolved according to ˙f(t) =−∇fE(f(t)). These type of evolution equations have
historically been the core of variational andPDE-based image processing ; in particular, gradient flows of the
Dirichlet energy (Kimmel et al., 1997), were shown to recover the Perona-Malik nonlinear diffusion Perona &
Malik (1990).
18Published in Transactions on Machine Learning Research (MM/YYYY)
A.2 Review of Kronecker product and properties of Laplacian kernel
Kronecker product. In this subsection we summarize a few relevant notions pertaining to the Kronecker
product of matrices, that are going to be applied throughout our spectral analysis of gradient flow equations
for GNNs in both the continuous and discrete time setting.
Given a matricial equation of the form
Y=AXB,
we can vectorize XandYby stacking columns into vec(X)andvec(Y)respectively, and rewrite the previous
system as
vec(Y) =/parenleftbig
B⊤⊗A/parenrightbig
vec(X). (19)
IfAandBare symmetric with spectra spec(A)andspec(B)respectively, then the spectrum of B⊗Ais
given by spec(A)·spec(B). Namely, if Ax=λAxandBy=λBy, forxandynon-zero vectors, then λBλA
is an eigenvalue of B⊗Awith eigenvector y⊗x:
(B⊗A)y⊗x= (λBλA)y⊗x. (20)
One can also define the Kronecker sum of matrices A∈Rn×nandB∈Rd×das
A⊕B:=A⊗Id+In⊗B, (21)
with spectrum spec(A⊕B) ={λA+λB:λA∈spec(A), λB∈spec(B)}.
Additional details on EDirand the choice of Laplacian. We recall that the classical graph Dirichlet
energyEDiris defined by
EDir(F) = trace/parenleftbig
F⊤∆F/parenrightbig
.
We can use the Kronecker product to rewrite the Dirichlet energy as
EDir(F) = vec( F)⊤(Id⊗∆)vec(F), (22)
from which we immediately derive that1
2∇vec(F)EDir(F) = (Id⊗∆)vec(F)– since ∆issymmetric – and
hence recover the gradient flow in (3) leading to the graph heat equation across each channel.
Before we further comment on the characterizations of LFDandHFDdynamics, we review the main choices
of graph Laplacian and the associated harmonic signals (i.e. how we can characterize the kernel spaces of the
given Laplacian operator). Recall that throughout the appendix we always assume that the underlying graph
Gisconnected . The symmetrically normalized Laplacian ∆=I−Ais symmetric, positive semi-definite with
harmonic space of the form (Chung & Graham, 1997)
ker(∆) := span( D1
21n:1n= (1,..., 1)⊤). (23)
Therefore, if the node features F(t)satisfy ˙F(t) =GNNθ(F(t),t)with initial condition F(0), and we assume
that over-smoothing occurs, meaning that ∆fr(t)→0fort→∞for each column 1≤r≤d, then the only
information persisting in the asymptotic regime is the degree and any dependence on the input features
is lost, as studied in Oono & Suzuki (2020); Cai & Wang (2020). A slightly different behaviour occurs if
instead of ∆, we consider the unnormalized Laplacian L=D−Awith kernel given by span(1n), meaning
that if Lfr(t)→0ast→∞for each 1≤r≤d, then any node would be embedded into a single point,
hence making any separation task impossible. The same consequence applies to the random walk Laplacian
∆RW=I−D−1A. In particular, we note that generally a row-stochastic matrix is not symmetric – if it was,
then this would in fact be doubly-stochastic – and the same applies to the random-walk Laplacian (a special
exception is given by the class of regulargraphs). In fact, in general any dynamical system governed by ∆RW
(or simply D−1A) is not the gradient flow of an energy due to the lack of symmetry, as further confirmed
below in (24).
19Published in Transactions on Machine Learning Research (MM/YYYY)
B Proofs and additional details of Section 4
B.1 Attraction vs repulsion: a physics-inspired framework
First, we show that we can rewrite the energy in (5) using the Kronecker product formalism. It suffices to
focus on the term Ω-term since the other derivations are identical. We have
/summationdisplay
i⟨fi.Ωfi⟩=/summationdisplay
ifα
iΩαβfβ
i.
On the other hand, since (Ω⊗I)vec(F) =IFΩ⊤, we also find
⟨vec(F),(Ω⊗I)vec(F) =fα
i(IFΩ⊤)α
i=fα
iΩαβfβ
i,
which shows the equivalence claimed.
Proof of Proposition 4.1. We first note that the system in (8) can be written using the Kronecker product as
vec(˙F(t)) =−(Ω⊗In)vec(F(t)) + (W⊗A)vec(F(t))−(˜W⊗In)vec(F(0)).
If this is the gradient flow of F∝⇕⊣√∫⊔≀→Eθ(F), then we would have
∇2
vec(F)Eθ(F) =Ω⊗In−W⊗A, (24)
which must be symmetric due to the Hessian of a function being symmetric. The latter means
(Ω⊤−Ω)⊗In= (W⊤−W)⊗A.
Therefore, for each components α,βwe must have
(ωαβ−ωβα)I= (wαβ−wβα)A.
IfAis non-diagonal, which happens as soon as there is a non-trivial edge in G, the equations are satisfied
only if both ΩandWaresymmetric . This shows that (8) is the gradient flow of Eθif and only if ΩandW
are symmetric .
We now rely on the spectral decomposition of Wto rewriteEθexplicitly in terms of attractive and repulsive
interactions. If we have a spectral decomposition W=Ψdiag(µ)(Ψ)⊤, we can separate the positive
eigenvalues from the negative ones and write
W=Ψdiag(µ+)Ψ⊤+Ψdiag(µ−)Ψ⊤:=W+−W−.
Since W+⪰0,W−⪰0, we can use the Choleski decomposition to write W+=Θ⊤
+Θ+andW−=
Θ⊤
−Θ−withΘ+,Θ−∈Rd×d. Equation (9) then simply follows by direct computation: namely, if we let
(D−1/2AD−1/2)ij= ¯aij, we derive
Eθ(F) =/summationdisplay
i⟨fi,Ωfi⟩−/summationdisplay
i,j¯aij⟨fi,Wfj⟩
=/summationdisplay
i⟨fi,(Ω−W)fi⟩+/summationdisplay
i⟨fi,Wfi⟩−/summationdisplay
i,j¯aij⟨Θ+fi,Θ+fj⟩+/summationdisplay
i,j¯aij⟨Θ−fi,Θ−fj⟩
=/summationdisplay
i⟨fi,(Ω−W)fi⟩+1
2/summationdisplay
i,j∥Θ+(∇F)ij∥2−1
2/summationdisplay
i,j∥Θ−(∇F)ij∥2,
where we have used that/summationtext
i,j1
di∥Θ+fi∥2=/summationtext
i∥Θ+fi∥2.
20Published in Transactions on Machine Learning Research (MM/YYYY)
B.2 Additional details on LFDandHFDcharacterizations
In this subsection we provide further details and justifications for Definition 4.2. We first prove the following
simple properties.
Lemma B.1. Assume we have a (continuous) process t∝⇕⊣√∫⊔≀→F(t)∈Rn×d, fort≥0. The following equivalent
characterizations hold:
(i)EDir(F(t))→0fort→∞if and only if ∆fr(t)→0, for 1≤r≤d.
(ii)EDir(F(t)/∥F(t)∥)→λn−1fort→∞if and only if for any sequence tj→∞there exist a subsequence
tjk→∞and a unit limit F∞– depending on the subsequence – such that ∆fr
∞=λn−1fr
∞, for
1≤r≤d.
Proof.(i) Given F(t)∈Rn×d, we can vectorize it and decompose it in the orthonormal basis {er⊗ϕℓ: 1≤
r≤d,0≤ℓ≤n−1}, with{er}d
r=1canonical basis in Rd, and write
vec(F(t)) =/summationdisplay
r,ℓcr,ℓ(t)er⊗ϕℓ, cr,ℓ(t) :=⟨vec(F(t)),er⊗ϕℓ⟩.
We can then use (22) to compute the Dirichlet energy as
EDir(F(t)) =d/summationdisplay
r=1n−1/summationdisplay
ℓ=0c2
r,ℓ(t)λℓ≡d/summationdisplay
r=1n−1/summationdisplay
ℓ=1c2
r,ℓ(t)λℓ≥gap(∆)d/summationdisplay
r=1n−1/summationdisplay
ℓ=1c2
r,ℓ(t),
where we have used that λ0= 0and that gap(∆) =λ1≤λℓfor allℓ≥1. Therefore
EDir(F(t))→0⇐⇒d/summationdisplay
r=1n−1/summationdisplay
ℓ=1c2
r,ℓ(t)→0, t→∞,
which occurs if and only if
(Id⊗∆)vec(F(t)) =d/summationdisplay
r=1n−1/summationdisplay
ℓ=1cr,ℓ(t)λℓer⊗ϕℓ→0.
(ii) The argument here is similar. Indeed we can write Q(t) =F(t)/∥F(t)∥withQ(t)a unit-norm signal.
Namely, we can vectorize it and write
vec(Q(t)) =/summationdisplay
r,ℓqr,ℓ(t)er⊗ϕℓ,/summationdisplay
r,ℓq2
r,ℓ(t) = 1.
ThenEDir(Q(t))→λn−1if and only if
/summationdisplay
r,ℓq2
r,ℓ(t)λℓ→λn−1, t→∞,
which holds if and only if
/summationdisplay
rq2
r,λn−1(t)→1
q2
r,ℓ(t)→0, ℓ :λℓ<λn−1, (25)
given the unit norm constraint. This is equivalent to the Rayleigh quotient of Id⊗∆converging to its
maximal value λn−1. When this occurs, for any sequence tj→∞we have that q2
r,ℓ(tj)≤1, meaning that
we can extract a converging subsequence that due to (25) will converge to a unit eigenvector F∞ofId⊗∆
satisfying (Id⊗∆)F∞=λn−1Q∞. Conversely assume for a contradiction that there exists a sequence
21Published in Transactions on Machine Learning Research (MM/YYYY)
tj→∞such thatEDir(F(tj)/∥F(tj)∥)<λn−1−ϵ, for someϵ>0. Then (25) fails to be satisfied along the
sequence, meaning that no subsequence converges to a unit norm eigenvector F∞ofId⊗∆with associated
eigenvalueλn−1which is a contradiction to our assumption.
Before we address the formulation of low(high)-frequency-dominated dynamics, we solve explicitly the
system ˙F(t) =AF(t)inRn×d, with some initial condition F(0). We can vectorize the equation and solve
˙ vec(F(t)) = ( Id⊗A)vec(F(t)), meaning that
vec(F(t)) =d/summationdisplay
r=1n−1/summationdisplay
ℓ=0e(1−λℓ)tcr,ℓ(0)er⊗ϕℓ, cr,ℓ(0) :=⟨vec(F(0)),er⊗ϕℓ⟩.
Consider any initial condition F(0)such that
d/summationdisplay
r=1|cr,0|=d/summationdisplay
r=1|⟨vec(F(0)),er⊗ϕ0⟩|>0,
which is satisfied for each vec(F(0))∈Rnd\U⊥, whereU⊥is the orthogonal complement of Rd⊗span(ϕ0).
SinceU⊥is a lower-dimensional subspace, its complement is dense. Accordingly for a.e. F(0), we find that
the solution satisfies
∥vec(F(t))∥2=e2t/parenleftiggd/summationdisplay
r=1c2
r,0+O(e−2gap( ∆)t)/parenrightigg
=e2t/parenleftig
∥P⊥
ker(∆)vec(F(0))∥2+O(e−2gap( ∆)t)/parenrightig
,
withP⊥
ker(∆)the projection onto Rd⊗ker(∆). We see that the norm of the solution increases exponentially,
however the dominant term is given by the projection onto the lowest frequency signal and in fact
vec(F(t))
∥vec(F(t))∥=P⊥
ker(∆)vec(F(0)) +O(e−gap(∆)t)(I−P⊥
ker(∆))vec(F(0))
/parenleftig
∥P⊥
ker(∆)vec(F(0))∥2+O(e−2gap( ∆)t)/parenrightig1
2→vec(F∞),
such that (Id⊗∆)vec(F∞) =0which means ∆fr
∞=0, for each column 1≤r≤d. Equivalently, one can
computeEDir(F(t)/∥F(t)∥)and conclude that the latter quantity converges to zero as t→∞by the very
same argument.
In fact, this motivates further the nomenclature LFDandHFD. Without loss of generality we focus now on
the high-frequency case. Assume that we have a HFDdynamicst∝⇕⊣√∫⊔≀→F(t), i.e.EDir(F(t)/∥F(t)∥)→λn−1,
then we can vectorize the solution and write vec(F(t)) =∥F(t)∥vec(Q(t)), for some time-dependent unit
vector vec(Q(t))∈Rnd:
vec(Q(t)) =/summationdisplay
r,ℓqr,ℓ(t)er⊗ϕℓ,/summationdisplay
r,ℓq2
r,ℓ(t) = 1.
By Lemma B.1 and more explicitly (25), we derive that the coefficients {qr,λn−1}associated with the
eigevenctors er⊗ϕλn−1are dominant in the evolution hence justifying the name high-frequency dominated
dynamics.
The next result provides a theoretical justification for the characterization of low (high) frequency dominated
dynamics in Definition 4.2.
Lemma B.2. Consider a dynamical system ˙F(t) = GNN θ(F(t),t), with initial condition F(0).
(i)GNNθisLFDif and only if (Id⊗∆)vec(F(t))
∥F(t)∥→0if and only if for each sequence tj→∞there
exist a subsequence tjk→∞andF∞(depending on the subsequence) s.t.F(tjk)
∥F(tjk)∥→F∞satisfying
∆fr
∞=0, for each 1≤r≤d.
22Published in Transactions on Machine Learning Research (MM/YYYY)
(ii)GNNθisHFDif and only if for each sequence tj→∞there exist a subsequence tjk→∞andF∞
(depending on the subsequence) s.t.F(tjk)
∥F(tjk)∥→F∞satisfying ∆fr
∞=λn−1fr
∞, for each 1≤r≤d.
Proof.(i) Since ∆fr(t)→0for each 1≤r≤dif and only if (Id⊗∆)vec(F(t))→0, we conclude that
the dynamics is LFDif and only if (Id⊗∆)vec(F(t))
∥F(t)∥→0due to (i) in Lemma B.1. Consider a sequence
tj→∞. Since vec(F(tj))/∥F(tj)∥is a bounded sequence we can extract a converging subsequence tjk:
vec(F(tjk))/∥F(tjk)∥→ vec(F∞). If the dynamics is LFD, then (Id⊗∆)vec(F(tjk))
∥F(tjk)∥→0and hence we
conclude that vec(F∞)∈ker(Id⊗∆). Conversely, assume that for any sequence tj→∞there exists
a subsequence tjkandF∞such thatF(tjk)
∥F(tjk)∥→F∞satisfying ∆fr
∞=0, for each 1≤r≤d. If for a
contradiction we had ε>0andtj→∞such thatEDir(F(tj)/∥F(tj)∥≥ε– forjlarge enough – then by (i)
in Lemma B.1 there exist 1≤r≤d,ℓ>0and a subsequence tjksatisfying
|⟨/parenleftbiggvec(F(tjk))
∥F(tjk)∥/parenrightbigg
,er⊗ϕℓ⟩|>δ(ε)>0,
meaning that there is no subsequence of {tjk}s.t.(Id⊗∆)vec(F(tjk))/∥F(tjk)∥→0, providing a contradiction.
(ii) This is equivalent to (ii) in Lemma B.1.
Remark. We note that in Lemma B.2 an LFDdynamics does not necessarily mean that the normalized
solution converges to the kernel of Id⊗∆– i.e. one in general has always to pass to subsequences. Indeed,
we can consider the simple example t∝⇕⊣√∫⊔≀→vec(F(t)) := cos(t)e¯r⊗ϕ0, for some 1≤¯r≤d, which satisfies
∆fr(t) =0for eachr, but it is not a convergent function due to its oscillatory nature. Same argument
applies to HFD.
We will now show that (8) can lead to a HFDdynamics. To this end, we assume that Ω=˜W=0so that (8)
becomes ˙F(t) =AF(t)W.According to (9) the negative eigenvalues of Wlead to repulsion. We show that
the latter can induce HFDdynamics as per Definition 4.2. We let Pρ−
Wbe the orthogonal projection into
the eigenspace of W⊗Aassociated with the eigenvalue ρ−:=|µ0|(λn−1−1). We recall that µ0is the most
negative eigenvalue of W, whileµd−1is the most positive eigenvalue of W. We define ϵHFDby:
ϵHFD:= min{ρ−−µd−1,|µ0|gap(λn−1I−∆),gap(|µ0|I+W)(λn−1−1)}.
Theorem B.3. Ifρ−>µd−1, then ˙F(t) =AF(t)WisHFDfor a.e. F(0)and indeed we have
EDir(F(t)) =e2tρ−/parenleftbig
λn−1∥Pρ−
WF(0)∥2+O(e−2tϵHFD)/parenrightbig
, t≥0,
andF(t)/∥F(t)∥converges to F∞∈Rn×dsuch that ∆fr
∞=λn−1fr
∞, for 1≤r≤d. If instead ρ−<µd−1
then the dynamics is LFDandF(t)/∥F(t)∥converges to F∞∈Rn×dsuch that ∆fr
∞=0, for 1≤r≤d,
exponentially fast.
Proof of Theorem B.3. Once we compute the spectrum of W⊗Avia (20), we can write the solution as –
recall that A=In−∆so we can express the eigenvalues of Ain terms of the eigenvalues of ∆:
vec(F(t)) =/summationdisplay
r,ℓeµr(1−λℓ)tcr,ℓ(0)ψr⊗ϕℓ,
withWψr=µrψr, for 0≤r≤d−1, where{ψr}ris an orthonormal basis of eigenvectors in Rd. We can
then calculate the Dirichlet energy along the solution as
EDir(F(t)) =⟨vec(F(t)),(Id⊗∆)vec(F(t))⟩=/summationdisplay
r,ℓe2µr(1−λℓ)tc2
r,ℓ(0)λℓ.
We now consider two cases:
23Published in Transactions on Machine Learning Research (MM/YYYY)
•Ifµr>0, thenµr(1−λℓ)≤µd−1.
•Ifµr<0, thenµr(1−λℓ)≤|µ0|(λn−1−1) :=ρ−, with eigenvectors ψr⊗ϕn−1for eachrs.t.
Wψr=µ0ψr– without loss of generality we can assume that λn−1is a simple eigenvalue for ∆. In
particular, if µr<0andµr(1−λℓ)<ρ−, then
µr(1−λℓ)<max{|µ0|(λn−2−1),|µ1|(λn−1−1)},
whereµ1is the second most negative eigenvalue of Wandλn−2is the second largest eigenvalue of
∆. In particular, we can write
λn−2=λn−1−gap(λn−1In−∆),|µ1|=|µ0|−gap(|µ0|Id+W). (26)
From (i) and (ii) we derive that if µr(1−λℓ)̸=ρ−, then
µr(1−λℓ)−ρ−<−min{ρ−−µd−1,ρ−−|µ0|(λn−2−1),ρ−−|µ1|(λn−1−1)}
=−min{ρ−−µd−1,|µ0|gap(λn−1I−∆),gap(|µ0|I+W)(λn−1−1)}=−ϵHFD,(27)
where we have used (26). Accordingly, if ρ−>µd−1, then
EDir(F(t)) =e2tρ−
λn−1/summationdisplay
r:µr=µ0c2
r,λn−1(0) +/summationdisplay
r,ℓ:µr(1−λℓ)̸=ρ−e2(µr(1−λℓ)−ρ−)tc2
r,ℓ(0)

=e2tρ−/parenleftbig
λn−1∥Pρ−
WF(0)∥2+O(e−2tϵHFD)/parenrightbig
.
By the same argument we can factor out the dominant term and derive the following limit for t→∞and for
a.e.F(0)sincePρ−
Wvec(F(0)) = 0only if vec(F(0))belongs to a lower dimensional subspace of Rnd:
vec(F(t))
vec(F(t))=Pρ−
Wvec(F(0)) +O(e−ϵHFDt)((I−Pρ−
W)vec(F(0)))
/parenleftbig
∥Pρ−
Wvec(F(0))∥2+O(e−2ϵHFDt)/parenrightbig1
2→Pρ−
Wvec(F(0))
∥Pρ−
Wvec(F(0))∥,
where the latter is a unit vector vec(F∞)satisfying (Id⊗∆)vec(F∞) =λn−1vec(F∞), which completes the
proof for the HFDcase.
For the opposite case the proof can be adapted without efforts as explicitly derived in the proof of Theorem 5.1.
We emphasize that the previous result includes Theorem 4.3 as a special case.
B.3 Comparison with continuous GNNs: details and proofs
Comparison with some continuous GNN models. In contrast with Theorem 4.3, we show that
three main linearized continuous GNN models are either smoothing or more generally LFD. The linearized
PDE-GCN Dmodel Eliasof et al. (2021) corresponds to choosing ˜W=0andΩ=W=K(t)⊤K(t)in (8), for
some time-dependent family t∝⇕⊣√∫⊔≀→K(t)∈Rd×d:
˙FPDE−GCN D(t) =−∆F(t)K(t)⊤K(t).
The CGNN model Xhonneux et al. (2020) can be derived from (8) by setting Ω=I−˜Ω,W=−˜W=I:
˙FCGNN (t) =−∆F(t) +F(t)˜Ω+F(0).
Finally, in linearized GRAND Chamberlain et al. (2021a) a row-stochastic matrix A(F(0))islearnedfrom
the encoding via an attention mechanism and we have
˙FGRAND (t) =−∆RWF(t) =−(I−A(F(0)))F(t).
We note that if Ais not symmetric, then GRAND is nota gradient flow.
24Published in Transactions on Machine Learning Research (MM/YYYY)
Theorem B.4. PDE−GCND,CGNNandGRAND satisfy the following:
(i)PDE−GCNDis a smoothing model: ˙EDir(FPDE−GCN D(t))≤0.
(ii)For a.e. F(0)it holds: CGNNis never HFDand if we remove the source term, then
EDir(FCGNN (t)/∥FCGNN (t)∥)≤e−gap(∆)t.
(iii) If Gis connected with self-loops, FGRAND (t)→µast→∞, withµr= mean( fr(0)),1≤r≤d.
By (ii) the source-free CGNN-evolution is LFDindependent of ˜Ω. Moreover, by (iii), over-smoothing occurs
for GRAND. On the other hand, Theorem 4.3 shows that the negative eigenvalues of Wcan make the
source-free gradient flow in (8) HFD. Experiments in Appendix E confirm that the gradient flow model
outperforms CGNN and GRAND on heterophilic graphs.
We prove the following result which covers Theorem 5.4.
Proof of Theorem B.4. We structure the proof by following the numeration in the statement.
(i) From direct computation we find
dEDir(F(t))
dt=d
dt(⟨vec(F(t)),(Id⊗∆)vec(F(t))⟩)
=−⟨vec(F(t)),(K⊤(t)K(t)⊗∆2)vec(F(t))⟩≤0,
since K⊤(t)K(t)⊗∆2⪰0. Note that we have used that (A⊗B)(C⊗D) =AC⊗BD.
(ii) We consider the dynamical system
˙FCGNN (t) =−∆F(t) +F(t)˜Ω+F(0).
We can write vec(F(t)) =/summationtext
r,ℓcr,ℓ(t)ϕ˜Ω
r⊗ϕℓ, leading to the system
˙cr,ℓ(t) = (λ˜Ω
r−λℓ)cr,ℓ(t) +cr,ℓ(0),0≤ℓ≤n−1,1≤r≤d.
We can solve explicitly the system as
cr,ℓ(t) =cr,ℓ(0)/parenleftbigg
e(λ˜Ω
r−λℓ)t/parenleftbigg
1 +1
λ˜Ωr−λℓ/parenrightbigg
−1
λ˜Ωr−λℓ/parenrightbigg
,ifλ˜Ω
r̸=λℓ
cr,ℓ(t) =cr,ℓ(0)(1 +t),otherwise.
We see now that for a.e. F(0)the projection (Id⊗ϕλn−1(ϕλn−1)⊤)vec(F(t))is never the dominant term. In
fact, if there exists rs.t.λ˜Ω
r≥λn−1, thenλ˜Ω
r−λℓ>λ˜Ω
r−λn−1, for any other non-maximal graph Laplacian
eigenvalue. It follows that there is no˜Ωs.t. the normalized solution maximizes the Rayleigh quotient of
Id⊗∆, proving that CGNN is never HFD.
If we have no source, then the CGNN equation becomes
˙F(t) =−∆F(t) +F(t)˜Ω⇐⇒ vec(˙F(t)) = ( ˜Ω⊕(−∆))vec( F(t)),
using the Kronecker sum notation in (21). It follows that we can write the vectorized solution in the basis
{ϕ˜Ω
r⊗ϕℓ}r,ℓas
vec(F(t)) =eλ˜Ω
+t
/summationdisplay
r:λ˜Ωr=λ˜Ω
+cr,0(0)ϕ˜Ω
r⊗ϕ0+O(e−gap(λ˜Ω
+Id−˜Ω)t)/summationdisplay
r:λ˜Ωr<λ˜Ω
+cr,0(0)ϕ˜Ω
r⊗ϕ0

+eλ˜Ω
+t
O(e−gap(∆)t)
/summationdisplay
r,ℓ>0cr,ℓ(0)ϕ˜Ω
r⊗ϕℓ

,
25Published in Transactions on Machine Learning Research (MM/YYYY)
meaning that the dominant term is given by the lowest frequency component and in fact, if we normalize we
findEDir(F(t)/∥F(t)∥)≤e−gap(∆)t.
(iii) Finally, we consider the dynamical system induced by linear GRAND
˙FGRAND (t) =−∆RWF(t) =−(I−A(F(0)))F(t).
Since we have no channel-mixing, without loss of generality we can assume that d= 1– one can then extend
the argument to any entry. We can use the Jordan form of Ato write the solution of the GRAND dynamical
system as
f(t) =Pdiag(eJ1t,...,eJnt)P−1f(0),
for some invertible matrix Pof eigenvectors, with
eJkt=e−(1−λA
k)t
1t···tmk−1
(mk−1)!
...
1
,
wheremkare the eigenvalue multiplicities. Since by assumption Gis connected and augmented with self-loops,
the row-stochastic attention matrix Acomputed in Chamberlain et al. (2021a) with softmax activation is
regular, meaning that there exists m∈Nsuch that (Am)ij>0for each entry (i,j). Accordingly, we can
apply Perron Theorem to derive that any eigenvalue of Ahas real part smaller than one except the eigenvalue
λA
0with multiplicity one, associated with the Perron eigenvector 1n. Accordingly, we find that each block
eJktdecays to zero as t→∞with the exception of the one eJ0tassociated with the Perron eigenvector. In
particular, the projection of f0over the Perron eigenvector is just µ1n, withµthe average of the feature
initial condition. This completes the proof.
B.4 Propagating with the Laplacian
In this subsection we briefly review the special case of (8) where Ω=W, and comment on why we generally
expect a framework where the propagation is governed by Ato be more flexible than one with −∆. IfΩ=W
and we suppress the source term i.e. ˜W=0, the gradient flow in (8) becomes
˙F(t) =−∆F(t)W. (28)
We note that once vectorized, the solution to the dynamical system can be written as
vec(F(t)) =d−1/summationdisplay
r=0n−1/summationdisplay
ℓ=0e−µrλℓtcr,ℓ(0)ψr⊗ϕℓ.
In particular, we immediately deduce the following counterpart to Theorem 4.3:
Corollary B.5. Ifspec(W)∩R−̸=∅, then (28) is HFDfor a.e. F(0).
Differently from (8) the lowest frequency component is always preserved independent of the spectrum of W.
This means that the system cannot learn eigenvalues of Wto either magnify or suppress the low-frequency
projection. In contrast, this can be done if Ω=0, or equivalently one replaces −∆withAproviding a
furtherjustification in terms of the interaction between graph spectrum and channel-mixing spectrum for why
graph-convolutional models use the normalized adjacency rather than the Laplacian for propagating messages
Kipf & Welling (2017).
B.5 Revisiting the connection with the manifold case
In (18) a constant nontrivial metric hinRdleads to the mixing of the feature channels. We adapt this idea
by considering a symmetric positive semi-definite H=W⊤WwithW∈Rd×dand using it to generalize EDir
by suitably weighting the norm of the edge gradients as
26Published in Transactions on Machine Learning Research (MM/YYYY)
EDir
W(F) :=1
2d/summationdisplay
q,r=1/summationdisplay
i/summationdisplay
j:(i,j)∈Ehqr(∇fq)ij(∇fr)ij=1
2/summationdisplay
(i,j)∈E∥W(∇F)ij∥2. (29)
We note the analogy with (18), where the sum over the nodes replaces the integration over the domain and
thej-th derivative at some point iis replaced by the gradient along the edge (i,j)∈E. We generally treat
Waslearnable weights and study the gradient flow of EDir
W:
˙F(t) =−1
2∇FEDir
W(F(t)) =−∆F(t)W⊤W. (30)
We see that (30) generalizes (3).
Proposition B.6. LetPker
Wbe the projection onto ker(W⊤W). Equation (30) is smoothing since
EDir(F(t))≤e−2tgap(W⊤W)gap(∆)∥F(0)∥2+EDir((Pker
W⊗In)vec(F(0))), t≥0.
In fact F(t)→F∞s.t.∃ϕ∞∈Rd: for eachi∈Vwe have (f∞)i=/radicalig
di
2|E|ϕ∞+Pker
Wfi(0).
Proof of Proposition B.6. We can vectorize the gradient flow system in (30) and use the spectral characteri-
zation of W⊤W⊗∆in (20) to write the solution explicitly as
vec(F(t)) =/summationdisplay
r,ℓe−(µrλℓ)tcr,ℓ(0)ψr⊗ϕℓ,
where{µr}r=spec(W⊤W)⊂R≥0with associated basis of orthonormal eigenvectors given by {ψr}r. Then
EDir(F(t)) =⟨vec(F(t)),(Id⊗∆)vec(F(t))⟩=/summationdisplay
r,ℓe−2t(µrλℓ)c2
r,ℓ(0)λℓ
=/summationdisplay
r:µr=0,ℓc2
r,ℓ(0)λℓ+/summationdisplay
r:µr>0,ℓ>0c2
r,ℓ(0)e−2t(µrλℓ)λℓ
=EDir((Pker
W⊗In)vec(F(0))) +/summationdisplay
r:µr>0,ℓ>0c2
r,ℓ(0)e−2t(µrλℓ)λℓ
≤EDir((Pker
W⊗In)vec(F(0))) +λn−1e−2tgap(W⊤W)gap(∆)∥F(0)∥2,
where we recall that Pker
Wis the projection onto ker(W⊤W)and that by convention the index ℓ= 0is
associated with the lowest graph frequency λ0= 0– by assumption Gis connected. This proves that the
dynamics is in fact smoothing. By the very same argument we find that
vec(F(t))→(Id⊗Pker)vec(F(0)) + (Pker
W⊗In)vec(F(0)), t→∞,
withPkerthe orthogonal projection onto ker∆– the other terms decay exponentially to zero. We first focus
on the first quantity, which we can write as
(Id⊗Pker)vec(F(0)) =/summationdisplay
rcr,0(0)ψr⊗ϕ0,
which has matrix representation ϕ0ϕ⊤
∞∈Rn×dwith
ϕ∞:=/summationdisplay
rcr,0(0)ψr.
By (23) we deduce that the i-th row ofϕ0ϕ⊤
∞∈Rn×dis thed-dimensional vector/radicalig
di
2|E|ϕ∞. We now focus
on the term
(Pker
W⊗In)vec(F(0)) =/summationdisplay
r:µr=0,jcr,j(0)ψr⊗ϕj
27Published in Transactions on Machine Learning Research (MM/YYYY)
which has matrix representation/summationtext
r:µr=0,jcr,j(0)ϕj(ψr)⊤. In particular, the i-th row is given by
/summationdisplay
r:µr=0,jcr,j(0)(ϕj)iψr=Pker
Wfi(0).
This completes the proof of Proposition B.6.
Proposition B.6 implies that no weight matrix Win (30)can separate the limit embeddings F∞of
nodes with same degree and same input features . In particular, we have the following characterization:
•Projections of the edge gradients (∇F)ij(0)∈Rdinto the eigenvectors of W⊤Wwith positive
eigenvalues shrinkalong the GNN and converge to zero exponentially fast as integration time (depth)
increases.
•Projections of the edge gradients (∇F)ij(0)∈Rdinto the kernel of W⊤Wstayinvariant .
IfWhas a trivial kernel, then nodes with same degrees converge to the same representation hence incurring
over-smoothing . Differently from Nt & Maehara (2019); Oono & Suzuki (2020); Cai & Wang (2020), over-
smoothing occurs independently of the spectral radius of the ‘channel-mixing’ if its eigenvalues are positive–
even for equations which lead to residual GNNs when discretized (Chen et al., 2018). According to
Proposition B.6, we do not expect (30) to succeed on heterophilic graphs where smoothing processes are
generally harmful. To deal with heterophily, one needs negative eigenvalues to generate repulsive forces
among adjacent features.
C Proofs and additional details of Section 5
We first explicitly report here the expansion of the discrete gradient flow in (10) after mlayers to further
highlight how this is not equivalent to a single linear layer with a message passing matrix Amas for S GCN
Wu et al. (2019). For simplicity we suppress the source term.
F(t+τ) =F(t) +τ(−F(t)Ω+AF(t)W)
vec(F(t+τ)) = ( Ind+τ(−Ω⊗In+W⊗A)) vec( F(t))
vec(F(mτ)) =m/summationdisplay
k=0/parenleftbiggm
k/parenrightbigg
τk(−Ω⊗In+W⊗A)kvec(F(0)) (31)
and we see how the message passing matrix Aactually enters the expansion after mlayers with each power
0≤k≤m. This is not surprising, given that we are discretizing a linear dynamical system, meaning that we
are approximating an exponential matrix.
C.1 From energy to evolution equations: exact expansion of the GNN solutions
We first address the proof of the main result.
Proof of Theorem 5.1. We consider a linear dynamical system
F(t+τ) =F(t) +τAF(t)W,
withWsymmetric. We vectorize the system and rewrite it as
vec(F(t+τ)) = ( Ind+τW⊗A)vec(F(t))
which in particular leads to
vec(F(mτ)) = ( Ind+τW⊗A)mvec(F(0)).
28Published in Transactions on Machine Learning Research (MM/YYYY)
We can then write explicitly the solution as
vec(F(mτ)) =/summationdisplay
r,ℓ(1 +τµr(1−λℓ))mcr,ℓ(0)ψr⊗ϕℓ.
We now verify that by assumption in (12) the dominant term of the solution is the projection into the
eigenspace associated with the eigenvalue ρ−=|µ0|(λn−1−1). The following argument follows the same
structure in the proof of Theorem B.3 with the extra condition given by the step-size. First, we note that for
anyrsuch thatµr>0, we have
|1 +τρ−|>|1 +τµd−1|≥|1 +τµr(1−λℓ)|
since we required ρ−>µd−1in (12). Conversely, if µr<0, then
|1 +τµr(1−λℓ)|≤max{|1 +τρ−|,|1 +τµ0|}
Assume that τ|µ0|>1, otherwise there is nothing to prove. Then |1 +τρ−|>τ|µ0|−1if and only if
τ|µ0|(2−λn−1)<2,
which is precisely the right inequality in (12). We can then argue exactly as in the proof of Theorem B.3 to
derive that for each index rsuch thatµr<0andµr̸=µ0, then
|1 +τµr(1−λℓ)|≤max{|1 +τ|µ1|(λn−1−1)|,|1 +τ|µ0|(λn−2−1)|}
withµ1andλn−2defined in (26). We can then introduce
δHFD:= max{µd−1,ρ−−|µ0|gap(λn−1I−∆),ρ−−(λn−1−1)gap(|µ0|I+W),|µ0|−2
τ}(32)
and conclude that
fi(mτ) =/summationdisplay
r,ℓ(1 +τµr(1−λℓ))mcr,ℓ(0)ϕℓ(i)ψr
= (1 +τρ−)m/parenleftig
c0,n−1(0)ϕn−1(i)·ψ0+O/parenleftig/parenleftig1 +τδHFD
1 +τρ−/parenrightigm/parenrightig/summationdisplay
ℓ,r:µr(1−λℓ)̸=ρ−cr,ℓ(0)ϕℓ(i)ψr/parenrightig
= (1 +τρ−)m(c0,n−1(0)ϕn−1(i)·ψ0+O(δm)),
which completes the proof of (13).
Conversely, if ρ−<µd−1, then the projection onto the eigenspace spanned by ψd−1⊗ϕ0is dominating the
dynamics with exponential growth (1 +τµd−1(1 + 0))m. We can then adapt the very same argument above
by factoring out the dominating term once we note that due to the choice of symmetric normalized Laplacian
∆, we haveϕ0(i) =/radicalbig
di/2|E|, which then yields (14).
We can now also address the proof of Corollary 5.2.
Proof of Corollary 5.2. Once we have the node-wise expansion we can simply compute the Rayleigh quotient
ofId⊗∆. We report the explicit details for the HFDcase since the argument for LFDextends without relevant
modifications. Using (11), we can compute the Dirichlet energy along a solution of F(t+τ) =F(t)+τAF(t)W
satisfying (12) by
EDir(F(mτ)) =/summationdisplay
r,ℓ(1 +τµr(1−λℓ))2mc2
r,ℓ(0)λℓ
= (1 +τρ−)2m/parenleftig
λn−1/summationdisplay
r:µr=µ0c2
r,λn−1(0) +O/parenleftig/parenleftig1 +τδHFD
1 +τρ−/parenrightig/parenrightig2m/summationdisplay
ℓ,r:µr(1−λℓ)̸=ρ−c2
r,ℓ(0)λℓ/parenrightig
= (1 +τρ−)2m/parenleftig
λn−1∥Pρ−
WF(0)∥2+O/parenleftig/parenleftig1 +τδHFD
1 +τρ−/parenrightig2m/parenrightig/parenrightig
,
29Published in Transactions on Machine Learning Research (MM/YYYY)
wherePρ−
Wis the orthogonal projector onto the eigenspace associated with the eigenvalue ρ−=|µ0|(λn−1−1).
In particular, since
vec(F(mτ)) = (1 +τρ−)m/parenleftbig
Pρ−
Wvec(F(0)) +O(δm)/parenrightbig
,
we find that the dynamics is HFDwith vec(F(t))/∥vec(F(t))∥converging to the unit projection of the initial
projection by Pρ−
Wprovided that such projection is not zero, which is satisfied for a.e. initial condition F(0).
Proof of Theorem 5.3. If we drop the residual connection and simply consider F(t+τ) =τAF(t)W, then
vec(F(mτ)) = (τW⊗A)mvec(F(0)).
Since Gis not bipartite, the Laplacian spectral radius satisfies λn−1<2. Therefore, for each pair of indices
(r,ℓ)we have the following bound:
|µr(1−λℓ)|≤max{µd−1,|µ0|},
and the inequality becomes strict if ℓ>0, i.e.λℓ>0. The eigenvalues µd−1andµ0are attained along the
eigenvectorsψd−1⊗ϕ0andψ0⊗ϕ0respectively. Accordingly, the dominant terms of the evolution lie in the
kernel of Id⊗∆, meaning that for any F0with non-zero projection in ker(Id⊗∆)– which is satisfied by all
initial conditions except those belonging to a lower dimensional subspace – the dynamics is LFD. In fact,
without loss of generality assume that |µ0|>µd−1, then
vec(F(mτ)) =|µ0|m/summationdisplay
r:µr=µ0(−1)mcr,0(0)ψ0⊗ϕ0
+|µ0|m/parenleftig
O(φ(m))/parenleftig
Ind−/summationdisplay
r:µr=µ0(ψ0⊗ϕ0)(ψ0⊗ϕ0)⊤/parenrightig
vec(F(0))/parenrightig
,
withφ(m)→0asm→∞, which completes the proof.
Gradient flow as spectral GNNs. We finally discuss (10) from the perspective of spectral GNNs
as in Balcilar et al. (2020). Let us assume that ˜W=0,Ω=0. If we let ∆=Ψdiag(µ)Ψ⊤be the
eigendecomposition of the graph Laplacian and {µr}be the spectrum of Wwith associated orthonormal
basis of eigenvectors given by {ψr}, and we introduce zr(t) :V→Rdefined byzr
i(t) =⟨fi(t),ψr⟩, then we
can rewrite the discretized gradient flow as
zr(t+τ) =Ψ(I+τµr(I−Λ∆))Ψ⊤zr(t) =zr(t) +τµrAzr(t),0≤r≤d−1. (33)
Accordingly, for each projection into the r-th eigenvector of W, we have a spectral function in the graph
frequency domain given by λ∝⇕⊣√∫⊔≀→1 +τµr(1−λ). Ifµr>0we have a low-pass filter while if µr<0we have a
high-pass filter. Moreover, we see that along the eigenvectors of W, ifµr<0then the dynamics is equivalent
to flipping the sign of the edge weights, which offers a direct comparison with methods proposed in Bo et al.
(2021); Yan et al. (2021) where some ‘attentive’ mechanism is proposed to learn negative edge weights based
on feature information.
The previous equation simply follows from
zr
i(t+τ) =⟨fi(t+τ),ψr⟩=⟨fi(t) +W(Af(t))i,ψr⟩
=zr
i(t) +µr/summationdisplay
j¯aijzr
j(t),
which concludes the derivation of (33).
30Published in Transactions on Machine Learning Research (MM/YYYY)
D Proofs and additional details of Section 6
Proof of Theorem 6.1. First we check that if time is continuous, then Eθin (6) is decreasing. We use the
Kronecker product formalism to rewrite the gradient ∇FEθ(F)as a vector in Rnd: explicitly, we get
1
2∇FEθ(F) = (Ω⊗In−W⊗A)vec(F) + ( ˜W⊗In)vec(F(0)).
It follows then that
1
2dEθ(F(t))
dt=1
2(∇FEθ(F(t)))⊤vec(˙F(t)) =
=/parenleftig1
2∇FEθ(F(t))/parenrightig⊤
σ/parenleftig
−1
2∇FEθ(F(t))/parenrightig
.
If we introduce the notation Z(t) =−1
2∇FEθ(F(t), then we can rewrite the derivative as
dEθ(F(t))
dt=−Z(t)⊤σ(Z(t)) =−/summationdisplay
αZ(t)ασ(Z(t)α)≤0
by assumption on σ. The discrete case follows similarly. Let us use the same notation as above so we can
write F(t+τ) =F(t) +τσ(Z(t)), with Z(t) =−1
2∇FEθ(F(t)).
Eθ(F(t+τ)) =⟨vec(F(t+τ)),(Ω⊗In−W⊗A)vec(F(t+τ)) + 2( ˜W⊗In)vec(F(0))⟩
=⟨vec(F(t)) +τσ(Z(t)),(Ω⊗In−W⊗A)vec(F(t+τ)) + 2( ˜W⊗In)vec(F(0))⟩
=⟨vec(F(t)) +τσ(Z(t)),(Ω⊗In−W⊗A) (vec( F(t) +τσ(Z(t))) + 2( ˜W⊗In)vec(F(0))⟩
=⟨vec(F(t)),(Ω⊗In−W⊗A)vec(F(t)) + 2( ˜W⊗In)vec(F(0))⟩
+τ⟨vec(F(t)),(Ω⊗In−W⊗A)σ(Z(t))⟩
+τ⟨σ(Z(t)),(Ω⊗In−W⊗A)vec(F(t) + 2( ˜W⊗In)vec(F(0))⟩
+τ2⟨σ(Z(t)),(Ω⊗In−W⊗A)σ(Z(t))⟩.
By using that Ω⊗In−W⊗Ais symmetric, we find that
Eθ(F(t+τ)) =Eθ(F(t)) + 2τ⟨σ(Z(t),(Ω⊗In−W⊗A)vec(F(t) + ( ˜W⊗In)vec(F(0))⟩
+τ2⟨1
τ(F(t+τ)−F(t)),(Ω⊗In−W⊗A)1
τ(F(t+τ)−F(t))⟩
=Eθ(F(t))−2τ⟨σ(Z(t)),Z(t)⟩+⟨F(t+τ)−F(t),(Ω⊗In−W⊗A)(F(t+τ)−F(t))⟩
≤Eθ(F(t)) +C+∥F(t+τ)−F(t))∥2,
where again we have used that Z⊤σ(Z)≥0. This completes the proof.
To further support the principle that the effects induced by Ware similar even in this non-linear setting, we
consider a simplified scenario.
Lemma D.1. If we choose Ω=W=diag(ω)withωr≤0for0≤r≤d−1and ˜W=0i.e.t∝⇕⊣√∫⊔≀→F(t)
solves the dynamical system
˙F(t) =σ(−∆F(t)diag(ω)),
withxσ(x)≥0, then the standard graph Dirichlet energy satisfies
dEDir(F(t))
dt≥0.
31Published in Transactions on Machine Learning Research (MM/YYYY)
Proof.This again simply follows from directly computing the derivative:
dEDir(F(t))
dt=1
2d
dt/parenleftigd/summationdisplay
r=1/summationdisplay
(i,j)∈E/parenleftigfr
i(t)√di−fr
j(t)/radicalbig
dj/parenrightig2/parenrightig
=d/summationdisplay
r=1/summationdisplay
i∈V(∆fr)iσ(−ωr(∆fr)i) =d/summationdisplay
r=1/summationdisplay
i∈V(∆fr)iσ(|ωr|(∆fr)i)≥0.
Important consequence: The previous Lemma implies that even with non-linear activations, negative
eigenvalues of the channel-mixing induce repulsion and indeed the solution becomes less smooth as measured
by the classical Dirichlet Energy increasing along the solution. Generalising this result to more arbitrary
choices is not immediate and we reserve this for future work.
E Additional details on experiments
E.1 Additional results on real-world tasks
General details. The graph-convolutional models in (16) and (17) are implemented in PyTorch (Paszke
et al., 2019), using PyTorch geometric Fey & Lenssen (2019) and torchdiffeq (Chen et al., 2018). Hyperpa-
rameters were tuned using wandb (Biewald, 2020) and random grid search. Experiments were run on AWS
p2.8xlarge machines, each with 8 Tesla V100-SXM2 GPUs.
We further improve our results on the node-classification tasks of by considering the following, more general,
parameterizations. We consider an instance of the gradient-flow framework, termed GRAFF, of the form:
GRAFF : F(t+τ) =F(t) +τ(−F(t)diag(ω) +AF(t)W−βF(0)), (34)
whereω∈Rdandβ∈R,Wis asymmetricd×d-matrixsharedacross layers and (node-wise) encoder and
decoder are MLPs. We descibe different implementations of Wbelow. In accordance with Theorem 6.1, we
also consider a non-linear variant (termed GRAFF NL) as
GRAFF NL:F(t+τ) =F(t) +τσ(−F(t)diag(ω) +AF(t)W−βF(0)), (35)
withσs.t.xσ(x)≥0. We note that differently from (16) and (17), these models also allow for a source
term controlled by βand a diagonal term acting on the state of a node, independent of the graph structure,
controlled by ω.
Methodology. In the experiments we rely on the following parameterizations. We implement encoder ψEN
andψDEas single linear layers or MLPs. We consider diagonally-dominant (DD)anddiagonal (D)choices for
the structure of Wthat offer explicit control over its spectrum. In the (DD)-case, we consider a W0∈Rd×d
symmetric with zero diagonal and w∈Rddefined by wα=qα/summationtext
β|W0
αβ|+rα, and set W=diag(w) +W0.
Due to the Gershgorin Theorem the eigenvalues of Wbelong to [wα−/summationtext
β|W0
αβ|,wα+/summationtext
β|W0
αβ|], so the
model ‘can’ easily re-distribute mass in the spectrum of Wviaqα,rα. This generalizes the decomposition of
Win Chen et al. (2020) providing a justification in terms of its spectrum. For (D)we take Wto be diagonal.
Real world experiments. In Table 3 we test GRAFF andGRAFF NLon the same datasets with varying
homophily adopted in Section 7 (Sen et al., 2008; Rozemberczki et al., 2021; Pei et al., 2020). We use results
provided in Yan et al. (2021, Table 1), which include GCNs models, GAT (Veličković et al., 2018), PairNorm
(Zhao & Akoglu, 2019) and models designed for heterophily (G GCN(Yan et al., 2021), Geom- GCN(Pei et al.,
2020), H2 GCN(Zhu et al., 2020) and GPRGNN (Chien et al., 2021)). For Sheaf (Bodnar et al., 2022), a
recent strong baseline with heterophily, we took the best performing variant (out of six) for each dataset. We
include continuous baselines CGNN (Xhonneux et al., 2020) and GRAND (Chamberlain et al., 2021a) to
corroborate Theorem 5.4. Training, validation and test splits are taken from Pei et al. (2020) for all datasets
32Published in Transactions on Machine Learning Research (MM/YYYY)
Texas Wisconsin Cornell Film Squirrel Chameleon Citeseer Pubmed Cora
Hom level 0.11 0.21 0.30 0.22 0.22 0.23 0.74 0.80 0.81
#Nodes 183 251 183 7,600 5,201 2,277 3,327 18,717 2,708
#Edges 295 466 280 26,752 198,493 31,421 4,676 44,327 5,278
#Classes 5 5 5 5 5 5 7 3 6
GGCN 84.86±4.55 86.86±3.29 85.68±6.63 37.54±1.56 55.17±1.58 71.14±1.84 77.14±1.45 89.15±0.37 87.95±1.05
GPRGNN 78.38±4.36 82.94±4.21 80.27±8.11 34.63±1.22 31.61±1.24 46.58±1.71 77.13±1.67 87.54±0.38 87.95±1.18
H2GCN 84.86±7.23 87.65±4.98 82.70±5.28 35.70±1.00 36.48±1.86 60.11±2.15 77.11±1.57 89.49±0.38 87.87±1.20
GCNII 77.57±3.83 80.39±3.40 77.86±3.79 37.44±1.30 38.47±1.58 63.86±3.04 77.33±1.48 90.15±0.43 88.37±1.25
Geom−GCN 66.76±2.72 64.51±3.66 60.54±3.67 31.59±1.15 38.15±0.92 60.00±2.81 78.02±1.15 89.95±0.47 85.35±1.57
PairNorm 60.27±4.34 48.43±6.14 58.92±3.15 27.40±1.24 50.44±2.04 62.74±2.82 73.59±1.47 87.53±0.44 85.79±1.01
GraphSAGE 82.43±6.14 81.18±5.56 75.95±5.01 34.23±0.99 41.61±0.74 58.73±1.68 76.04±1.30 88.45±0.50 86.90±1.04
GCN 55.14±5.16 51.76±3.06 60.54±5.30 27.32±1.10 53.43±2.01 64.82±2.24 76.50±1.36 88.42±0.50 86.98±1.27
GAT 52.16±6.63 49.41±4.09 61.89±5.05 27.44±0.89 40.72±1.55 60.26±2.50 76.55±1.23 87.30±1.10 86.33±0.48
MLP 80.81±4.75 85.29±3.31 81.89±6.40 36.53±0.70 28.77±1.56 46.21±2.99 74.02±1.90 75.69±2.00 87.16±0.37
CGNN 71.35±4.05 74.31±7.26 66.22±7.69 35.95±0.86 29.24±1.09 46.89±1.66 76.91±1.81 87.70±0.49 87.10±1.35
GRAND 75.68±7.25 79.41±3.64 82.16±7.09 35.62±1.01 40.05±1.50 54.67±2.54 76.46±1.77 89.02±0.51 87.36±0.96
Sheaf(max) 85.95±5.51 89.41±4.74 84.86±4.71 37.81±1.15 56.34±1.32 68.04±1.58 76.70±1.57 89.49±0.40 86.90±1.13
GRAFF 88 .38±4.53 88.83±3.29 84.05±6.10 37.11±1.08 58.72±0.84 71.08±1.75 77.30±1.85 90.04±0.41 88.01±1.03
GRAFF NL 86.49±4.84 87.26±2.52 77.30±3.24 35.96±0.95 59.01±1.31 71.38±1.47 76.81±1.12 89.81±0.50 87.81±1.13
Table 3: Node-classification results. Top three models are coloured by First, Second, Third
dataset Texas Wisconsin Cornell Film Squirrel Chameleon Citeseer Pubmed Cora
Homophily 0.11 0.21 0.3 0.22 0.22 0.23 0.74 0.8 0.81
Depth 2 2 2 2 4 4 2 2 4
µd−1 2.07 1.33 1.37 3.33 2.48 2.66 4.1 9.47 2.49
µ0 -3.07 -0.82 -0.76 -1.45 -3.61 -3.72 -0.96 -3.89 -0.26
mean({µr}) -0.02 0.02 0.02 0.06 -0.1 -0.1 0.13 0.15 0.11
EDir(F(0)) 13171 5367 3687 7460 68811 25772 4749 5737 1226
EDir(F(m)) 42674 6767 4932 12224 828532 398302 37168 119518 33700
EDir(F(0))/∥F(0)∥21.68 1.33 1.35 1.1 1.77 1.86 0.55 0.82 0.57
EDir(F(m))/∥F(m)∥21.67 0.71 0.91 0.71 1.47 1.21 0.16 0.29 0.11
Test accuracy (mean) 81.08 81.96 72.43 34.16 52.48 68.99 76.43 88.56 87.4
Test accuracy (std) 4.52 4.45 7.13 0.9 1.8 1.7 1.53 0.51 0.97
Table 4: SGCNgflearned spectrum
for comparison. For the Cornell dataset we take the adjacency file from before the July 2022 update of Pei
et al. (2020).
Results. GRAFF andGRAFF NLare both versions of graph convolutions with stronger ‘inductive bias’ given
by the energyEθdecreasing along the solution ; in fact, we can recover them from graph convolutions by simply
requiring that the channel-mixing is symmetric and shared across layers . Nonetheless they achieve competitive
results on all datasets often outperforming slower and more complex models. As noted in Section 7, the
improved performance is mainly due to the presence of a residual connection at each layer, which simply
enables the channel-mixing matrices to induce either attraction or repulsion, via their eigenvalues.
A word of caution though: these tasks are very sensitive to the hyperparameter tuning, making them generally
not ideal candidates to probe different models. While they suffice for our purposes, given the theoretical
nature of our work, we believe them to be at times very poor indicators of the ‘heterophilic problem’.
33Published in Transactions on Machine Learning Research (MM/YYYY)
E.2 Details on hyperparameters
Using wandb Biewald (2020) we performed a random grid search with uniform sampling of the continuous
variables. We provide the hyperparameters that achieved the best results from the random grid search in
Table 5. Input dropout and dropout are the rates applied to the encoder/decoder respectively with no dropout
applied in the ODE block . Further hyperparameters decide the use of non-linearities, batch normalisation,
parameter vector ωand source term multiplier βwhich are specified in the code.
w_style lr decay dropout input_dropout hidden time step_size
chameleon diag_dom 0.00500.0005 0.36 0.48 643.33 1
squirrel diag_dom 0.00650.0009 0.17 0.35 1282.87 1
texas diag_dom 0.00410.0354 0.33 0.39 640.6 0.5
wisconsin diag 0.00290.0318 0.37 0.37 642.1 0.5
cornell diag 0.00210.0184 0.30 0.44 642.0 1
film diag 0.00260.0130 0.48 0.42 641.5 1
Cora diag 0.00260.0413 0.34 0.53 643.0 0.25
Citeseer diag 0.00010.0274 0.22 0.51 642.0 0.5
Pubmed diag 0.00390.0003 0.42 0.41 642.6 0.5
Table 5: Selected hyperparameters for real-world datasets
E.3 Additional details on the spectral experiments
We report additional details on the spectrum of WSin (16) and on the normalized Dirichlet energy in Table 4.
We also report the distribution of the eigenvalues of the learned channel-mixing matrix WSin Figure 4.
34Published in Transactions on Machine Learning Research (MM/YYYY)
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5
W_evals010203040506070countT exas homophily: 0.11
0.75
 0.50
 0.25
 0.00 0.25 0.50 0.75 1.00
W_evals020406080countWisconsin homophily: 0.21
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5
W_evals01234567countSquirrel homophily: 0.22
1.0
 0.5
 0.0 0.5 1.0 1.5 2.0
W_evals010203040countFilm homophily: 0.22
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5
W_evals012345678countChameleon homophily: 0.23
0.75
 0.50
 0.25
 0.00 0.25 0.50 0.75 1.00
W_evals020406080100countCornell homophily: 0.3
1.0
 0.5
 0.0 0.5 1.0 1.5 2.0 2.5
W_evals0246810121416countCiteseer homophily: 0.74
2
 1
 0 1 2 3 4 5
W_evals010203040506070countPubmed homophily: 0.8
0.5
 0.0 0.5 1.0 1.5
W_evals0510152025countCora homophily: 0.81
Figure 4: Histogram of SGCNgflearned spectrum
35