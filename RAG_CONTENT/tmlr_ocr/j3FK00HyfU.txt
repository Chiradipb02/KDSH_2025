Published in Transactions on Machine Learning Research (06/2023)
The Meta-Evaluation Problem in Explainable AI:
Identifying Reliable Estimators with MetaQuantus
Anna Hedström1,6,†anna.hedstroem@tu-berlin.de
Philine Bommer1,6philine.bommer@tu-berlin.de
Kristoffer K. Wickstrøm3kristoffer.k.wickstrom@uit.no
Wojciech Samek1,2,4wojciech.samek@hhi.fraunhofer.de
Sebastian Lapuschkin4sebastian.lapuschkin@hhi.fraunhofer.de
Marina M.-C. Höhne2,3,5,6,†mhoehne@atb-potsdam.de
1Department of Electrical Engineering and Computer Science, TU Berlin
2BIFOLD – Berlin Institute for the Foundations of Learning and Data
3Department of Physics and Technology, UiT the Arctic University of Norway
4Department of Artificial Intelligence, Fraunhofer Heinrich-Hertz-Institute
5Department of Computer Science, University of Potsdam
6UMI Lab, Leibniz Institute of Agricultural Engineering and Bioeconomy e.V. (ATB)
†corresponding authors
Reviewed on OpenReview: https://openreview.net/forum?id=j3FK00HyfU
Abstract
One of the unsolved challenges in the field of Explainable AI (XAI) is determining how to
most reliably estimate the quality of an explanation method in the absence of ground truth
explanation labels. Resolving this issue is of utmost importance as the evaluation outcomes
generated by competing evaluation methods (or “quality estimators”), which aim at mea-
suring the same property of an explanation method, frequently present conflicting rankings.
Such disagreements can be challenging for practitioners to interpret, thereby complicating
their ability to select the best-performing explanation method. We address this problem
through a meta-evaluation of different quality estimators in XAI, which we define as “the
process of evaluating the evaluation method” . Our novel framework, MetaQuantus , analy-
ses two complementary performance characteristics of a quality estimator: its resilience to
noise and reactivity to randomness, thus circumventing the need for ground truth labels. We
demonstrate the effectiveness of our framework through a series of experiments, targeting
various open questions in XAI such as the selection and hyperparameter optimisation of
quality estimators. Our work is released under an open-source license1to serve as a devel-
opment tool for XAI- and Machine Learning (ML) practitioners to verify and benchmark
newly constructed quality estimators in a given explainability context. With this work,
we provide the community with clear and theoretically-grounded guidance for identifying
reliable evaluation methods, thus facilitating reproducibility in the field.
1 Introduction
Since Explainable AI (XAI) is intended to increase trust and transparency in AI systems, it is necessary to
evaluate the performance of proposed explanation methods to ensure their reliability. In the context of black-
box Machine Learning (ML) models such as neural networks (NNs)—where the input to output mapping is
not explicitly known or interpretable by a user (Benitez et al., 1997; Bellido & Fiesler, 1993; Lipton, 2018)—
there is generally an absence of ground truth explanations to understand the model’s decision. This makes
it difficult to evaluate the performance of explanation methods since the exact outcomes of explanations
1Code is available at the GitHub repository: https://github.com/annahedstroem/MetaQuantus .
1Published in Transactions on Machine Learning Research (06/2023)
Figure 1: An illustration of Meta-Evaluation through three phases: (i) Modeling, (ii) Explaining and (iii) Evaluating.
(i) A ResNet-9 model (He et al., 2016) is trained to classify digits from 0to9on Customised-MNIST dataset (Bykov
et al., 2022) (i.e., MNIST digits pasted on randomly sampled CIFAR-10 backgrounds). (ii) To understand the model’s
prediction, we use several explanation methods including Gradient (Morch et al., 1995; Baehrens et al., 2010), Inte-
grated Gradients (Sundararajan et al., 2017) and GradientShap (Lundberg & Lee, 2017), which are distinguished by
their respective colours. (iii) To evaluate the quality of the explanations, we apply different estimators of faithfulness
such asFaithfulness Correlation (FC) (Bhatt et al., 2020) and Pixel-Flipping (PF) (Bach et al., 2015), which return
a correlation coefficient and an AUC score, respectively. However, since the scores vary depending on the estima-
tor, both in range and direction, with lower or higher scores indicating more faithful explanations, interpreting the
resulting faithfulness scores remains difficult for the practitioner.
oftentimes remain unknown and thus unverifiable (Dasgupta et al., 2022). Without consensus around how
to define the quality or “correctness” of an explanation method, a variety of XAI evaluation procedures have
been proposed. These efforts most commonly involve (i) measuring the extent to which desirable properties
are fulfilled by the explanation method, e.g., through faithfulness or robustness analysis (Samek et al.,
2017; Sundararajan et al., 2017; Montavon et al., 2018; Agarwal et al., 2022a), (ii) generating well-defined,
synthetic settings where explanation labels are simulated (Yang & Kim, 2019; Arras et al., 2022; Liu et al.,
2021a) or, (iii) evaluating explanations based on visual alignment with a human prior (Smilkov et al., 2017).
Most relevant to our work is the first category of evaluation methods or “metrics” (Hedström et al., 2023),
where the goal is to estimate the quality of an attribution-based explanation method, also known as feature-
importance method. Henceforth, we refer to these evaluation methods as “quality estimators”, or simply
“estimators” of explanation quality.
The abundance of explanation methods and an ever-growing number of quality estimators, combined with
little guidance on how to use them, have caused practitioner confusion within the XAI and ML communities
Krishna et al. (2022). Strong claims of identified failure modes for explanation methods with assertions of
which methods pass or fail (Adebayo et al., 2018; Dombrowski et al., 2019; Sixt et al., 2020), followed by
rebuttals (Sundararajan & Taly, 2018; Yona & Greenfeld, 2021; Binder et al., 2022), are ever-present. To
answer the question of “which explanation method to use for a given task”, we must first be able to answer
“how to reliably define and measure the relevant qualities that an explanation method should fulfill”. While
preliminary efforts exist to address this issue (Brunke et al., 2020; Tomsett et al., 2020; Gevaert et al., 2022;
Rong et al., 2022), to the best of our knowledge, there is currently no comprehensive solution that thoroughly
evaluates the various estimators used to compare, select and reject different explanation methods in XAI.
Previous efforts at addressing this issue have been limited in scope and do not provide a thorough theoretical
motivation. With this work, we aim to fill this critical yet largely neglected research gap.
2Published in Transactions on Machine Learning Research (06/2023)
In this work, we characterise the problem of meta-evaluation, which we refer to as “the process of evaluating
the evaluation method” . This problem arises as we select and quantitatively evaluate different explanation
methods for a given model, dataset, and task—and where conflicting evaluation outcomes are produced. As
illustrated in Figure 1, we can apply various estimators to compare the explanation methods’ faithfulness,
which measures how closely the explanations align with the predictive behaviour of the model (the experi-
mental details are described in Appendix A.4). However, the estimators rank the same explanation methods
differently, e.g., the Gradient method (Morch et al., 1995; Baehrens et al., 2010) is both ranked the highest
(R=1) and the lowest (R=3) depending on the estimator used. With a disagreement in evaluation outcome
about which explanation method is superior (Krishna et al., 2022) coupled with little to no guidance on how
to identify a high-quality estimator (Wang & Wang, 2022), practitioners may unknowingly choose an infe-
rior estimator which ultimately results in a selection of an explanation method that presents a less faithful
explanation to the end user.
To address the issue of evaluation disagreement in XAI, we propose a simple yet comprehensive framework
which primary purpose is to provide an objective, independent view of the estimator’s performance by meta-
evaluating it against two failure modes: resilience to noise (NR) and reactivity to adversary (AR). Similar
to how software systems undergo vulnerability- and penetration tests before getting deployed in a larger
system, we apply this framework to stress test the estimators. If vulnerabilities in the quality estimator are
discovered, e.g., high sensitivity to noise in input or low reactivity to randomness, appropriate actions can
be taken to improve the estimators. The contribution of this work is three-fold.
•First, we provide a clear argument for why meta-evaluation of quality estimators is challenging (Sec-
tion 2.2), emphasising the importance of reliability analysis as a tool to analyse estimator behaviour.
•Second, based on these findings, we propose a framework to meta-evaluate the performance of XAI
estimators (Section 3), with a sound theoretical foundation and agnosticism towards various network
architectures and attribution-based explanation methods.
•Third, through experiments on various explanation methods, datasets, and models, we demonstrate
that our framework can solve a range of XAI-related tasks such as estimator selection and hyperpa-
rameter optimisation, generating novel insights into estimators’ behaviour (Section 6).
We find it important to point out that we have no interest in developing yet another quality estimator.
The real need in our community lies in developing meta-evaluation schemes to validate and determine the
reliability of the quality estimators that already been developed. It is surprising to us that very little effort
has so far been directed towards this important area of analysing their performance. With this work, we aim
to support the process of selecting a quality estimator in a given explainability context in order to provide
more clarity and guidance on how to effectively evaluate explanation methods.
1.1 Related Works
Despite much activity towards the development of estimators to evaluate explanation quality (Bach et al.,
2015; Sundararajan et al., 2017; Bhatt et al., 2020; Nguyen & Martinez, 2020; Rieger & Hansen, 2020; Arias-
Duart et al., 2021) and benchmarking tools (Liu et al., 2021a; Agarwal et al., 2022b; Hedström et al., 2023),
limited attention has thus far been given to analysing the performance of the estimators themselves. Only
recently, increased attention has been raised on the intricacies that come with XAI evaluation, for example,
the contributions of Brunke et al. (2020); Brocki & Chung (2022); Rong et al. (2022) emphasise the difficulty
that comes with parameterising estimators. Another issue with evaluation was brought to light by Neely
et al. (2021); Krishna et al. (2022), which revealed that explanations frequently disagree in their ranking
of features. Additionally, several independent research groups were able to identify empirical “confounders”
(Sundararajan & Taly, 2018; Kokhlikyan et al., 2021; Yona & Greenfeld, 2021; Binder et al., 2022) affecting
the well-adopted Model Parameter Randomisation test (Adebayo et al., 2018). From these publications, it
seems worryingly “easy to get it wrong” when it comes to evaluating explainable methods empirically. There
still remains a lot of ambiguity when it comes to determining what makes up a good or bad quality estimator
(Wang & Wang, 2022).
3Published in Transactions on Machine Learning Research (06/2023)
Within the scope of evaluating quality estimators, preliminary efforts exist but a unified effort is required.
Since there is little to no consensus on how to determine the true value of a quality estimator—when a new
estimator is introduced, it is often assessed based on single perspectives, e.g., by randomisation experiments
(Rieger & Hansen, 2020; Arias-Duart et al., 2021) or ranking consistency (Rong et al., 2022). All of these
mentioned works are undoubtedly steps in the right direction, but what is missing is a broader, more
comprehensive framing of what a quality estimator ought to fulfil. With this work, we aim to fill this gap.
2 Preliminaries
In the following, we derive a mathematical definition of the evaluation problem in XAI by outlining the key
elementsrequiredtoperformqualityestimationonagivenexplanationmethod. Inthesucceedingsection, we
discuss the Challenge of Unverifiability (CoU) which explains why meta-evaluation is theoretically difficult.
All notation used throughout this paper can be found in Appendix A.7.
2.1 The Evaluation Problem
Consider a supervised classification problem2where we have a black-box model fparameterised by θthat
has been trained on a given training dataset Xtr={(x1,y1),..., (xN,yN)}to map an input x∈RDto an
output class y∈{1,...,C}, with a trained functional mapping such as:
f(x;θ) = ˆy, (1)
More generally, we can define the model function f:X∝⇕⊣√∫⊔≀→Ythat maps inputs from the instance space Xto
predictions in the label space Ywithx∈Xandˆy∈Y. Let Fdenote the function space such that f∈F. To
quantitatively estimate the performance of model f, we compute the prediction error on a given test dataset
Xtewhere there exists a label yfor each prediction ˆy. To understand the reasoning of the model fbehind
a certain prediction ˆy, we can apply one of the many proposed localexplanation methods (Smilkov et al.,
2017; Zeiler & Fergus, 2014; Sundararajan et al., 2017; Selvaraju et al., 2020; Bykov et al., 2022) as follows:
Φ(x,f,ˆy;λ) =ˆe, (2)
where Φ :RD×F×Y∝⇕⊣√∫⊔≀→RDis an explanation function that is parameterised by λand which distributes an
attribution to each individual feature in xaccording to its importance, typically visualised in an explanation
map ˆe∈RD. A broad variety of explanation methods fall within the scope of Φsuch as gradient-based
(Smilkov et al., 2017; Sundararajan et al., 2017; Bykov et al., 2022), back-propagation-based (Bach et al.,
2015), model-agnostic (Zeiler & Fergus, 2014; Lundberg & Lee, 2017), local surrogate (Ribeiro et al., 2016),
attention-based (Chefer et al., 2021; Covert et al., 2022), as well as prototypical explanation methods (Si-
monyan et al., 2014). Let Edenote the space of possible explanations with respect to the model such that
Φ∈E.
Similar to how we compute the prediction error to estimate the performance of a model f, to evaluate
the quality of the explanation function Φ, we compute the explanation error, requiring a ground truth
explanatione. These labels are, however, generally not available for black-box ML models and in particular
NNs3since their inner workings are considered uninterpretable (Bellido & Fiesler, 1993; Benitez et al., 1997;
Dasguptaetal.,2022). Therefore, XAIresearchersandMLpractitionersmustresorttoindirectapproachesto
estimate the quality of a given explanation, e.g., by measuring the explanation’s relative fulfilment of certain
human-defined properties. Recent work by Hedström et al. (2023) has proposed to group these properties of
explanation quality into six categories; (a) faithfulness, (b) robustness, (c) localisation, (d) randomisation,
(e) complexity, and (f) axiomatic estimators which provide a natural framework to compare and analyse
explanation quality. A summary of these explanation quality categories can be found in Appendix A.2 (see
Equations 11-14).
2SinceclassificationtasksarecommonlyencounteredintheXAIcommunity,itischosentoillustratetheEvaluationProblem.
However, as discussed in Appendix A.1.1, our statements also apply to other prediction scenarios.
3Even in toy- or expert-annotated ground truth XAI datasets where features are known a priori (Arras et al., 2022; Yang &
Kim, 2019), the explanation labels provided may not accurately reflect the model’s decision-making process. Only with perfect
knowledge of the model can true ground truth explanation labels be obtained.
4Published in Transactions on Machine Learning Research (06/2023)
Figure 2: A visual representation of the conditional dependencies between variables in quality estimation (Equation
3). The information flows from modelling to explaining and evaluating the explanations, i.e., Ψ◦Φ◦f, which is
indicated by the direction of the arrows in the directed acyclic graph (DAG). The colours indicate if the spaces have
verifiable (black) or unverifiable outcomes (red).
We provide a generalised notation for quality estimation of attribution-based explanation methods as follows.
LetΨτ:E×RD×F×Y∝⇕⊣√∫⊔≀→Rbe a quality estimator that is parameterised by τand takes one explanation
and returns one scalar value (“quality estimate”) to indicate the quality of the explanation. The evaluation
of an explanation, i.e., quality estimation, can be written as follows:
Ψ(Φ,x,f,ˆy;τ) = ˆq, (3)
where Ψrepresents the quality estimator and the whole space of possible estimators is denoted Ψ∈O.
Mathematical descriptions of such estimators can be found in Appendix (see Equations A.3).
2.2 The Challenge of Unverifiability
The goal of quantitative XAI evaluation is to provide an objective measure of the quality of an explanation.
However, due to missing ground truth, the quantitative assessment of neural network explanations remains
non-trivial. To clarify where this difficulty arises, we represent the process of quality estimation as a directed
acyclic graph (DAG), as seen in Figure 2. Here, each node represents a random variable and the edges
represent the relationships between the variables, with the uncertainty of a parent node propagating to
its child node. We separate the nodes between verifiable- and unverifiable spaces. The verifiable spaces
are spaces where ground truth labels are available, i.e., Ω∈{{X},{F},{X,F}}and the unverifiable spaces
include spaces where there is an absence of labels, i.e., U∈{{E},{O},{E,O}}.
As indicated by the direction of the arrows, a key observation is that in quality estimation there exists a
conditional dependency between the variables of modelling, explaining and evaluating (the explanations).
This further means that since the evaluation function is applied to the results of the unverifiable explanation
function, the evaluation outcome also renders unverifiable. We refer to this phenomenon as the Challenge
of Unverifiability. Another key observation is that we cannot determine the accuracy or validity of an
estimator (i.e., whether it actually measures the intended quality) since such an assessment requires access
to ground truth labels. However, as reliability analysis does not depend on the availability of ground truth
labels, it is still possible to study the reliability of an estimator, which refers to its overall consistency (“does
this estimator produce similar results under consistent conditions?”). This can be achieved by repeatedly
measuring the evaluation outcomes that result from fixing the unverifiable parameters and functions and only
varying the elements of the verifiable spaces. In the following, we will use the distinction between verifiable-
and unverifiable spaces to systematically and controllably measure the performance of quality estimators.
3 A Meta-Evaluation Framework
While the Challenge of Unverifiability makes meta-evaluation of quality estimators challenging, it is still
possible to study the performance characteristics of an estimator through the lens of reliability. To this
end, we developed a three-step framework, which is a higher-level evaluation scheme that examines quality
estimators that themselves have been used to evaluate a particular explanation method.
5Published in Transactions on Machine Learning Research (06/2023)
3.1 Defining Failure Modes
Without ground truth information, we cannot validate or optimise the quality estimators against what we
want them to fulfil, but we can instead articulate edge-case scenarios or behaviours that we do not want
them to exhibit. For this purpose, we formulate failure modes which are described in the following.
Failure Mode 1 (Noise Resilience) .A quality estimator should be resilient to minor perturbations of its
input parameters.
Similar to the stability (or “robustness”) property of explanation functions (Agarwal et al., 2022a; Montavon
et al., 2018) and especially Lipschitz Continuity (Alvarez-Melis & Jaakkola, 2018; Yeh et al., 2019), where
smallchangesintheinputshouldonlyleadtosmallchangesintheexplanation,noiseresilience(NR)evaluates
the extent to which a quality estimator is robust towards minor perturbations of its inputs. Following our
general perturbation Definition 3 in Appendix A.2, we define a minor perturbation PM
Ωof any verifiable
space Ωas follows:
Definition 1 (Minor Perturbation) .LetPΩ(ω)be a perturbation function of ω∈Ω,ˆy=f(x;θ)be the
original prediction of the network and y′be the prediction after the perturbation. Then PΩ(ω)is minorPM
Ω,
if∀ˆy, y′∈{{f(PM
X(x);θ)},{f(x;PM
F(θ))},{f(PM
X(x);PM
F(θ))}},∃ϵ∈Rϵ≪1such that:
||ˆy−y′||p≤ϵ
For classification, we employ L 1-norm with p= 1, thus, Definition 1 states that the predicted label y′
stays unchanged after the perturbation, i.e., ˆy≈y′. Similar to works by Brunke et al. (2020); Brocki
& Chung (2022); Rong et al. (2022), we measure the vulnerability of quality estimators to variations or
“minor confounds” in the estimator. However, in contrast to these aforementioned works, we only perturb
in the verifiable space by means of measuring the change in the model decision on a sample before and after
perturbation, and thus we can control and directly measure the strength of the perturbation. Accordingly,
to quantitatively examine Failure Mode 1, we expose the estimator to perturbations with small or minor
impacts. Complementarytotestinganestimator’sresiliencetonoise, wealsoformulateasecondfailuremode
to test whether a quality estimator produces a significant change when exposed to disruptive perturbation,
i.e., randomisation to any of its inputs.
Failure Mode 2 (Adversary Reactivity) .A quality estimator should be reactive to disruptive perturbations
of its input parameters.
Previous research has noted that the estimators’ scores should be conceivably different when produced for a
random explanation (Rieger & Hansen, 2020) or a randomly initialised model (Arias-Duart et al., 2021). Our
approach is similar in that it also seeks to disrupt the explanation process. However, since we can control the
perturbation strength in the verifiable spaces, we can make more well-grounded claims about the expected
outcomes of a perturbation. Theoretically, we define disruptive perturbations PD
Ωcontrary to Definition 1.
Definition 2 (Disruptive Perturbation) .PΩ(ω)be a perturbation function of ω∈Ω,ˆy=f(x;θ)be the
original prediction of the network and y′be the prediction after the perturbation. Then PΩ(ω)is disruptive
PD
Ω, if∀ˆy, y′∈{{f(PD
X(x);θ)},{f(x;PD
F(θ))},{f(PD
X(x);PD
F(θ))}}∃ϵ∈R, ϵ≪1such that:
||ˆy−y′||p>ϵ.
In a classification context, Definition 2 implies a change in the predicted class label. Figure 3 illustrates the
main difference between minor and disruptive perturbations, which is that the decision boundary remains
uncrossed or crossed, respectively. In Appendix A.1.1, we expand the Definitions 1 and 2 to other problem
settings such as multi-label classification and also discuss how adversarial attacks relate to these definitions.
Using Definitions 1 or 2, we can generate perturbed quality estimates q′by applying a minor or disruptive
perturbation on the verifiable spaces in the input, model, or input- and model spaces simultaneously:
ˆq∈{Ψ(Φ,Pt
X(x),f,ˆy),Ψ(Φ,x,Pt
F(θ),ˆy)),Ψ(Φ,Pt
X(x),Pt
F(θ),ˆy))}, (4)
where the superscript of the perturbation function, t∈{M,D}indicates the perturbation strength. For
simplicity, we omit the hyperparameters τ,λfrom Equation 4. By repeating this perturbation (Equation 4)
6Published in Transactions on Machine Learning Research (06/2023)
Figure 3: An illustration of minor versus disruptive perturbation in the different spaces (left: X, right: F) for a
classification task. The direction of the arrows shows how the respective perturbations are realised, where blue and
red colours indicate a minor- or disruptive perturbation, respectively. The minor perturbation keeps the decision
boundary intact, either by perturbing a sample xM(left) or perturbing the model itself fD(right). The disruptive
perturbation implies that the decision boundary is crossed either through a sample xD(left) or model fD(right).
multiple times, we gather sets of perturbed estimates for meta-evaluation analysis. In the next section, we
provide a detailed description of how this analysis is performed.
3.2 Formulating Consistency Criteria
To determine whether a quality estimator appropriately circumvented a failure mode, we can measure the
similarity of its quality estimates before and after the perturbation. After a minor perturbation, when testing
for noise resilience, we would expect that the scores are similarly distributed. Conversely, for disruptive
perturbations, when testing for reactivity to adversary, we would anticipate a large response to information
annihilation of the explanation process by means of scores being dis-similarly distributed. We formalise this
idea in our Intra-Consistency (IAC) criterion as follows:
IAC =1
KK/summationdisplay
k=1d(ˆq,q′
k), (5)
where ˆqrefers to unperturbed estimates and q′
k∈RN,k= (1,...,K )is a set of perturbed quality estimates,
replicatedKtimes forNtest samples (see Equation 4) such that Q= [q′
1,...,q′
K]∈RN×K. Here,drefers to
a statistical significance measure d:RN×RN∝⇕⊣√∫⊔≀→Rthat takes a set of unperturbed- and perturbed estimates
and returns a p-value. A high p-value indicates that ˆqandq′
kare similarly distributed and a low p-value
value means that the estimates are differently distributed. Accordingly, Equation 5 returns the average
p-value across all perturbed samples over Kperturbations, with IAC ∈[0,1]. Since the nominal values of
qualityestimatorscanvaryandoftenhavelittletonosemanticmeaning, weusethenon-parametric Wilcoxon
signed-rank test (Wilcoxon, 1945) which does not carry strong assumptions about the data distribution, only
about its ranking. In addition to the intra-consistency analysis, we also measure whether quality estimators
exhibit consistent behaviour in terms of ranking. This type of inter-consistency analysis is commonly used in
Explainable AI research (Tomsett et al., 2020; Gevaert et al., 2022; Hedström et al., 2023; Rong et al., 2022)
andcomplementstheaforementionedbyinvolvingmorethanoneexplanationmethod. Let ¯Q∈RN×Ldenote
a matrix for the unperturbed estimates ˆqforLexplanation methods and ¯Q′∈RN×Lbe a matrix for the
perturbed estimates q′
k, which are both averaged over Kperturbations. We formulate the Inter-Consistency
(IEC) criterion as follows:
IEC =1
N×LN/summationdisplay
i=1L/summationdisplay
j=1Ut
i,j (6)
whereUt
i,j∈[0,1]are entries of a binary ranking agreement matrix Uthat takes quality estimates from
¯Qand ¯Q′and populates the entries according to the interpretation of ranking. Here, IEC = 1indicates
perfect ranking consistency and IEC = 0the absence of it, where IEC ∈[0,1]. The perturbation strength
is indicated in the superscript t∈{M,D}. The interpretation of ranking is different depending on the
perturbation strength, i.e., minor or disruptive. For minor perturbations, we measure if the quality estimator
7Published in Transactions on Machine Learning Research (06/2023)
Figure 4: Meta-evaluation of quality estimators is performed in three steps: (i) Perturbing, (ii) Scoring and (iii)
Integrating. (i) First, a minor or disruptive perturbation is induced depending on the failure mode, i.e., PM
Ωfor NR
andPD
Ωfor AR. (ii) Second, the estimator’s intra- and inter-consistency are calculated to assess each performance
dimension. The IAC score captures the extent that the estimator produces similar or dis-similar scores with respect
toˆqandq′
k, which is illustrated through the distribution plots, where for NR and AR, the score distributions are
overlapping and non-overlapping, respectively. The IEC score expresses ranking consistency. NR measures how
consistently the estimator ranks different explanation methods and AR calculates how consistently the perturbed
scores are lower than the unperturbed scores. (iii) In the final step, we integrate the previous steps and produce an
MC score that summarises the estimator’s performance: its resilience to noise and reactivity to adversary.
ranks different explanation methods similarly. We define UMfor minor perturbations with entries such as:
UM
i,j=/braceleftigg
1 ¯rM
j= ¯rj
0otherwise,(7)
where ¯rM=r(¯QM
i,:)with ¯QM:=¯Q′and¯r=r(¯Qi,:)arerankingvectorsgivenarankingmeasure r:RL∝⇕⊣√∫⊔≀→RL
thattakeseachrowin ¯QM
i,:and ¯Qi,:, respectivelyandsortsthevaluesindescendingorder. Eachentry ¯rM
j∈N
corresponds to integers indicating their relative rank. For example, suppose we have one sample x, three
explanation methods and their corresponding quality estimates, such as ¯QM
i,:= [0.76,0.86,0.66]. Then the
results obtained from applying rwould be ¯rM= [2,1,3]. An optimally-performing estimator would provide
the same rankings for ¯rMas¯rfor allNinputs, resulting in IEC = 1. However, as discussed in Section 6,
the reality is that many estimators often conflict with the optimal.
For disruptive perturbations, we interpret ranking consistency differently. Here, as explained in-depth in
Appendix A.1.2, we measure how consistently the quality estimator ranks estimates from ¯Qhigher than
¯QD:=¯Q′. We defineUDfor disruptive perturbations with entries such as:
UD
i,j=/braceleftigg
1¯QD
i,j<¯Qi,j
0otherwise,(8)
where the quality estimates ¯QD
i,jare generated for an explanation with respect to the same class as the one
predicted for the unperturbed estimate ¯Qi,j. For some estimators, e.g., in the robustness category, lower
values are considered better than higher values, for which we invert the comparison symbol in Equation 8.
3.3 Quantifying Meta-Consistency
To conclude the framework, we want to characterise the performance of a quality estimator with a single
Meta-Consistency (MC) score. To capture both the estimator’s resilience to noise (NR) and its reactivity to
8Published in Transactions on Machine Learning Research (06/2023)
adversary (AR), we average over the two criteria for both failure modes:
MC =/parenleftbigg1
|m∗|/parenrightbigg
m∗Tmwherem=
IACNR
IACAR
IECNR
IECAR
(9)
andm∗=14represents an optimally performing quality estimator as defined by the all-one indicator
vector. A good quality estimator should produce an MC score close to 1 as higher values indicate better
performanceonthetestedcriteria4,whereMC∈[0,1]. Anestimatorthatdemonstratesabalanceofresilience
against minor perturbations and reactivity towards disruptive perturbations—as evidenced through its score
distribution and ranking of different explanation methods—would achieve high meta-consistency scores with
ourframework. Ourproposedscorehastheadvantageofbeingbothconciseandcomprehensive, asitprovides
a summary of the performance characteristics of an estimator while also taking into account multiple criteria.
For a full overview of the framework, please see Figure 4.
4 Practical Evaluation
Within the framework of meta-evaluation, it is necessary to generate perturbed quality estimates for analysis.
To accomplish this, we developed a series of practical tests. The methodology behind these tests is simple
and thus easily extensible through the tests made available in the repository5. First, the space in which
perturbations will be applied is selected, with options being either the input or the model. Second, based on
the chosen space, an appropriate type of noise is defined. To ensure that the perturbations are meaningful
and relevant to the task at hand, the noise type should be chosen contextually with respect to the data
domain. For example, when perturbing the input space for images, we define a test as follows:
InputPerturbationTest(IPT). Applyi.i.dadditiveuniformnoisesuchthat ˆxi=x+δiwithδi∼U(α,β)
where for noise resilience, ˆxifulfills Definition 1 and for adversary reactivity, ˆxifulfills Definition 2
whereα,βhave to be chosen according to the data domain and respective failure mode (e.g., set α=
−0.001,β= 0.001for NR and α= 0.0,β= 1.0for AR). To maintain the statistics of the data distribution,
we clipα,βto the maximum and the minimum value of the test set, respectively. Moreover, when perturbing
the model space, to maintain the variance of the network, we follow an established methodology by Bykov
et al. (2022) and scale the learned weights θof the model fas follows:
Model Perturbation Test (MPT). Apply multiplicative Gaussian noise to all weights of the network,
i.e., ˆθi=θ·νiwithνi∼N (µ,Σ)whereµ= 1and for noise resilience, ˆθifulfills Definition 1 and for
adversary reactivity, ˆθifulfills Definition 2
where for Σto be consistent with either Definition 1 or 2, it is set based on the specific context of the model
and task being considered (e.g., Σ= 0.001for NR and Σ= 2.0for AR). Third, to collect sets of perturbed
quality estimates for intra- and inter-consistency analysis, we repeat the process of perturbation (as outlined
in IPT and MPT) and subsequent evaluation (using Equation 4) under Kruns. Finally, we compute the MC
score. For sanity-checking experiments of the tests, see Appendix A.5. Moreover, as a third testing scenario,
it is theoretically possible to perturb both the input- and model spaces simultaneously, i.e., PX(x),PF(θ)as
well as their respective latent spaces. This we leave for future work.
5 Experimental Setup
In this section, we give a brief account of the experimental setup, including the datasets, models, explanation
methods and estimators used in this work. Further details can be found in Appendix A.4.
4When computing intra-consistency scores for AR, we apply reverse scoring, i.e., 1−IAC AR, so that all elements in the
meta-evaluation vector (Equation 9) can be interpreted in the same way, i.e., that higher values are better.
5Code is available at the GitHub repository: https://github.com/annahedstroem/MetaQuantus .
9Published in Transactions on Machine Learning Research (06/2023)
In our experiments, we benchmark five different categories of explanation quality and within each category,
we selected two estimators as follows: Complexity (CO) (Bhatt et al., 2020), Sparseness (SP) (Chalasani
et al., 2020), Faithfulness Correlation (FC) (Bhatt et al., 2020), Pixel-Flipping (PF) (Bach et al., 2015),
Max-Sensitivity (MS) (Yeh et al., 2019), Local Lipschitz Estimate (LLE) (Alvarez-Melis & Jaakkola, 2018),
Pointing-Game (PG) (Zhang et al., 2018), Relevance Mass Accuracy (RMA) (Arras et al., 2022), Model
Parameter Randomisation Test (MPR) (Adebayo et al., 2018) and Random Logit (RL) (Sixt et al., 2020).
Each estimator evaluates explanations from a popular category of post-hoc attribution methods, including
both gradient-based- and model-agnostic techniques: Gradient (Morch et al., 1995; Baehrens et al., 2010),
Saliency (Morch et al., 1995), GradCAM (Selvaraju et al., 2020), Integrated Gradients (Sundararajan et al.,
2017),Input×Gradient (Shrikumar et al., 2016), Occlusion (Zeiler & Fergus, 2014) and GradientSHAP
(Lundberg & Lee, 2017) from which we generate explanations with respect to a sample’s predicted class.
For comparability, we normalise the explanations by dividing the attribution map by the square root of its
average second-moment estimate (Binder et al., 2022). The mathematical definitions of the estimators are
described in Appendix A.3. For estimator implementations, we use Quantus library (Hedström et al., 2023).
We use four image classification datasets for our experiments: ILSVRC-15 (i.e., ImageNet) (Russakovsky
etal.,2015), MNIST(LeCunetal.,2010), fMNIST(Xiaoetal.,2017)andcustomised-MNIST(i.e., cMINST)
(Bykov et al., 2022) with different black-box NNs such as LeNets (LeCun et al., 1998), ResNets (He et al.,
2016) and transformer-based models such as Vision Transformer (ViT) (Dosovitskiy et al., 2021) and SWIN
Transformer (Liu et al., 2021b).
6 Results
Many open questions remain in the field of XAI. In this section, we show how meta-evaluation can solve
a subset of those problems such as (i) estimator selection, (ii) optimising hyperparameters of an estima-
tor, (iii) evaluating the category convergence, i.e., the extent that estimators within the same category of
explanation quality measure the same concept and (iv) comparing estimator choice for different network
architectures including transformer-based architectures. We prioritise the topic of estimator selection in the
main manuscript and provide a detailed analysis and discussion of the experiments addressing questions (ii),
(iii) and (iv) in Appendix A.6. Instructions to reproduce the experiments can be found in the repository.
Table 1: Meta-evaluation benchmarking results with ImageNet, aggregated over 3 iterations with K= 5. IPT results
are in grey rows and MPT results are in white rows. MC denotes the averages of the MC scores over IPT and MPT.
The top-performing MC- or MC method in each explanation category, which outperforms the bottom-performing
method by at least 2 standard deviations, is underlined. Higher values are preferred for all scoring criteria.
Category Estimator MC(↑) MC(↑) IACNR(↑) IACAR(↑) IECNR(↑) IECAR(↑)
ComplexitySparseness 0.636 ±0.0550.697±0.0480.519±0.229 0.745±0.145 0.870±0.060 0.653±0.064
0.575±0.0620.588±0.1710.445±0.1640.862±0.0470.403±0.036
Complexity 0.562 ±0.0330.590±0.043 0.170±0.093 0.965±0.092 1.000±0.000 0.225±0.068
0.534±0.0230.221±0.0860.792±0.0881.000±0.0000.122±0.012
FaithfulnessFaithfulness Corr. 0.480 ±0.0800.470±0.078 0.518±0.241 0.550±0.138 0.342±0.063 0.470±0.051
0.490±0.0820.497±0.2160.586±0.1490.329±0.0740.546±0.039
Pixel-Flipping 0.698 ±0.0950.623±0.128 0.518±0.232 0.691±0.405 0.707±0.052 0.574±0.125
0.774±0.0620.515±0.2241.000±0.0000.719±0.0600.863±0.011
LocalisationPointing-Game 0.537 ±0.0430.591±0.066 0.685±0.121 0.579±0.159 0.903±0.049 0.196±0.032
0.483±0.0200.868±0.0680.102±0.0980.935±0.0260.026±0.012
Relevance Mass Acc. 0.580 ±0.0700.619±0.067 0.524±0.213 0.633±0.172 0.750±0.044 0.570±0.047
0.541±0.0740.558±0.1250.476±0.1990.767±0.0520.364±0.037
RandomisationRandom Logit 0.604 ±0.0320.644±0.063 0.481±0.268 0.631±0.217 0.939±0.035 0.524±0.083
0.564±0.0000.393±0.0000.400±0.0000.960±0.0000.505±0.000
Model Param. Rand. 0.697 ±0.0420.762±0.0410.454±0.170 0.909±0.000 0.967±0.020 0.720±0.022
0.631±0.0440.465±0.2000.563±0.0770.954±0.0170.541±0.016
RobustnessMax-Sensitivity 0.610 ±0.0650.621±0.068 0.507±0.144 0.593±0.284 0.938±0.026 0.446±0.089
0.600±0.0610.454±0.2511.000±0.0000.921±0.0200.023±0.007
Local Lipschitz Est. 0.615 ±0.0710.635±0.112 0.565±0.099 0.630±0.561 0.916±0.031 0.431±0.197
0.594±0.0300.498±0.1440.485±0.0730.895±0.0270.498±0.016
10Published in Transactions on Machine Learning Research (06/2023)
Figure 5: Left:A visualisation of MNIST benchmarking results (Table 3), in particular IAC and IEC scores for noise
resilience (x-axes) and adverse reactivity (y-axes). The colours indicate the estimator and the symbols show the test
type, i.e., IPT and MPT, respectively. Right:A comparison of averaged meta-consistency performance for different
quality estimators using MPT and IPT, aggregated over 3 iterations with K= 5, across different models { LeNets,
ResNet-9, ResNet-18 } and datasets { MNIST, fMNIST, cMNIST, ImageNet }. Higher values are preferred.
6.1 Benchmarking
As a first example, we will demonstrate how meta-evaluation can be used to select a certain quality estimator
for a given category of explanation quality. To this end, we set up a benchmarking experiment, where we
take two popular estimators from five different explanation quality categories and evaluate six explanation
methodsL={Gradient, Saliency, GradCAM, Integrated Gradients, Occlusion, GradientShap } for MNIST,
fMNIST and cMNIST datasets and evaluate three explanation methods L={Saliency, Input×Gradient,
GradientShap } for ImageNet dataset, using the MetaQuantus library. Since the choice of Lhas a minimal
influence on the MC scores (see experiments in Appendix A.5.2), we omit results from other tested sets of
explanation methods in the main manuscript.
6.1.1 Comparison of XAI Estimators
TheImageNetresultsaresummarisedinTable1. ThegreyrowsindicatetheresultsfromtheInputPerturba-
tion Test and the white rows show the results from the Model Perturbation Test. A more detailed discussion
of the results, including additional datasets, can be found in Appendix A.6.3. From Table 1, we can observe
that no tested estimator performs optimally, i.e., ∀MC<1. From column MC, which displays the averaged
MC scores (over IPT and MPT) we note that Sparseness ,Pixel-Flipping ,Relevance Mass Accuracy ,Model
Parameter Randomisation Test andLocal Lipschitz Estimate are the best-performing estimators in their
respective category for ImageNet dataset. From Figure 5 (right), we can observe that this comparison of
MC scores is generally consistent across the tested datasets, which contributes to the generalisability of our
findings. Certain categories such as localisation and randomisation exhibit higher variability with respect to
the MC score, which we further discuss in Appendix A.6.5.
6.1.2 Comparison of XAI Evaluation Categories
The meta-evaluation framework can moreover be applied to gain insights into the performance characteristics
of different estimators on a category-by-category basis. For this purpose, we represent the entries of the
meta-evaluation vector as coordinates on a 2D plane and visualise the results as an area graph (see Figure
7). By inspecting the coloured areas of the respective estimators in terms of their size and shape, we can
deduce the overall performance of both failure modes. Here, larger coloured areas imply better performance
on the different scoring criteria and the grey area indicates the area of an optimally performing quality
estimator, i.e., m∗=14. Each column of estimators represents a category of explanation quality, from left
to right: Complexity ,Faithfulness ,Localisation ,Randomisation andRobustness , which colour scheme we
apply consistently across all figures.
As seen in Figure 7 (third column), the localisation estimators exhibit a notable deficiency in terms of
adversary reactivity on the Input Perturbation Test. A low IPT score for adversary reactivity means that
the estimators are insensitive to disruptive input perturbations, as evidenced by similar score distributions
11Published in Transactions on Machine Learning Research (06/2023)
Figure7: AgraphicalrepresentationoftheImageNetbenchmarkingresults(Table1),aggregatedover3iterationswith
K= 5. Each column corresponds to a category of explanation quality, from left to right: Complexity ,Faithfulness ,
Localisation ,Randomisation andRobustness . The grey area indicates the area of an optimally performing estimator,
i.e.,m∗=14. The MC score (indicated in brackets) is averaged over MPT and IPT. Higher values are preferred.
(low IAC) and an inability to rank disruptively perturbed explanations lower than unperturbed explanations
(low IEC). Based on the definitions of these estimators (described in Equations 24-25), which include the
Pointing-Game method (Zhang et al., 2018), which evaluates explanations by verifying that the highest
attributed feature intersects with a given segmentation mask and the Relevance Mass Accuracy method
(Arras et al., 2022), which calculates the amount of explainable mass intersecting with the segmentation
mask—we would expect that these estimators perform well on this test since disrupted input usually leads
to scattered attributions. It may seem counterintuitive that these estimators lack reactivity to disruption,
however, we posit that the reason for the poor reactivity to adversary is the estimators’ inherent dependency
on the segmentation mask. If the segmentation mask (relative to the object of interest, or the input) is large
enough, high localisation scores are attainable irrespective of the “quality” of the explanation (Kohlbrenner
et al., 2020). This finding is further validated by the increase in MC scores for cMNIST dataset, which
generally has a smaller bounding box compared to ImageNet, MNIST and fMNIST (see details in Appendix
A.4), where disruption evidently has an effect, as evidenced by higher AR scores, depicted in Figure 5 (right).
Practitioners should be aware of this category’s reliance (or oversensitivity) on the segmentation mask where
relying solely or too heavily on this category in XAI evaluation may not be advisable.
Figure 6: Average MC
scores per category
over tested datasets
{ImageNet ,MNIST,
fMNIST,cMNIST }.The highest overall meta-consistency scores are obtained by the robustness and ran-
domisation categories, which can be observed in Figure 6 and by their respective
areas in Figure 7. One potential explanation for this is that the estimators in these
categories already include some element of stochasticity in their estimator definitions
(see Equations 18-19 and 20-21, respectively) which may make them more resilient
as well as reactive to perturbations. For example, both robustness estimators, i.e.,
Max-Sensitivity (Yeh et al., 2019) and Local Lipschitz Estimate (Alvarez-Melis &
Jaakkola, 2018) rely on Monte Carlo sampling-based approximation where expla-
nation methods are evaluated by examining their response to minor perturbation
of the input, aggregated over multiple runs. In the randomisation category, the
Model Parameter Randomisation Test (Adebayo et al., 2018) evaluates explanations
by increasingly perturbing the model weights and Random Logit (Sixt et al., 2020)
evaluates explanations by a random selection of an explanation of a non-target class.
As seen in Figure 6, the complexity category has the lowest overall MC scores across
the tested datasets, which includes estimators such as Sparseness (Chalasani et al.,
2020) and Complexity (Bhatt et al., 2020) that evaluate explanations by calculating their Gini coefficient
and Shannon entropy, respectively. Given the simplicity of these calculations, this outcome is not surprising.
12Published in Transactions on Machine Learning Research (06/2023)
Figure 8: A graphical representation of the ImageNet benchmarking results for different architectures aggregated
over 3 iterations with K= 5. Each column represents the estimator, from left to right: Pointing-Game ,Top-K
Intersection ,Relevance Mass Accuracy andRelevance Rank Accuracy , and each row the model type { ResNet-18 ,
ViT,SWIN}, as indicated by the labels. The grey area indicates the area of an optimally performing estimator, i.e.,
m∗=14. The MC score (indicated in brackets) is averaged over MPT and IPT. Higher values are preferred.
Another notable category of poor performance that is picked up by the meta-evaluation tests is faithfulness.
Our results, which show a lack of resilience to noise in the ranking of explanation methods (low IEC)
especially for Faithfulness Correlation (Bhatt et al., 2020), corroborate previous studies (Brunke et al., 2020;
Brocki & Chung, 2022; Rong et al., 2022) that found that faithfulness estimators may rank explanation
methods inconsistently when subjected to perturbation, such as changing the masking pixel strategy from,
for example, “uniform” to “black”. This trend is particularly evident in Figure 5 (left), where the points
belonging to the faithfulness category have notably lower IEC scores compared to the other categories of
explanationquality. Apossibleexplanationistheirwell-documentedsensitivitytoparameterisation(Tomsett
et al., 2020; Hedström et al., 2023; Rong et al., 2022).
While certain estimator categories, such as faithfulness, may present challenges such as parameterisation, it
is not advisable to disregard their evaluation. Compared to categories such as complexity which are well-
defined and simple to calculate, they may not offer as much information as categories such as faithfulness,
which can provide important insights into how the explanation- and model functions are related. Relying on
only one category to estimate explanation quality is therefore not recommended. This is especially true since
an explanation function may trade one category of explanation quality over another (Bhatt et al., 2020), for
example, an explanation that is faithful may be too complex for the user to understand. Therefore, to avoid
arriving at incomplete or incorrect conclusions about which explanation methods work (and not), it is of
utmost importance for practitioners to approach quality estimation by incorporating multiple definitions of
explanation quality.
6.1.3 Comparison of SOTA Vision Architectures
To investigate if the performance characteristics of the different quality estimators revealed by the meta-
evaluation endure across various architectural types, we conducted additional experiments with ResNet- and
transformer-based model architectures. These architectures included the Vision Transformer (ViT) (Doso-
vitskiy et al., 2021) and the SWIN Transformer (Liu et al., 2021b). For this experiment, we employed two
13Published in Transactions on Machine Learning Research (06/2023)
explanation methods L={Saliency, GradientShap }and meta-evaluated four estimators in the localisation
category, namely Pointing-Game (PG) (Zhang et al., 2018), Relevance Mass Accuracy (RMA) (Arras et al.,
2022),Relevance Rank Accuracy (RRA) (Arras et al., 2022) and Top-K Intersection (TK) (Theiner et al.,
2022). These estimators are all defined in Appendix A.3. We also conducted a comparable experiment with
the complexity estimators Sparseness (Chalasani et al., 2020) and Complexity (Bhatt et al., 2020). These
results are found in Appendix A.6.4, which yielded outcomes that are consistent with the findings presented
here.
In Figure 8, we present the meta-evaluation results as area graphs. Each column represents a localisation
estimator and each row represents a network architecture, as indicated by the labels. The performance
attributes of each estimator, when examined across different network types, display a noticeable similarity.
This suggests that the characteristics revealed through meta-evaluation analysis may be persistent across
architecture types, thereby enabling general conclusions of individual estimators. In particular, Pointing-
Game(Zhang et al., 2018) performed poorly across all model architectures, with low adverse reactivity
in the rankings (IEC) where Relevance Mass Accuracy (Arras et al., 2022) generally demonstrated higher
performance. Relevance Rank Accuracy (Arras et al., 2022) and Top-K Intersection (Theiner et al., 2022)
showed the highest scores with narrow margins, possibly due to their similarity in definitions (see Equations
26 and 27 in Appendix A.3). Overall, these results contribute to the robustness of our findings and provide
evidence demonstrating the framework’s agnosticism to the choice of network architecture.
7 Conclusion
When we neither understand the general behaviour of the explanation methods nor the estimators that we
apply to estimate their quality, we are bound to make mistakes. This problem in XAI is exacerbated by the
fact that different estimators within the same category of explanation quality may rank the same explanation
method differently (Neely et al., 2021; Krishna et al., 2022). Without an understanding of the performance
characteristics of the estimators we employ, we risk presenting inferior explanation methods to the end user.
To address the problem of meta-evaluation, we propose a novel framework for identifying reliable quality
estimators to evaluate explanation methods. We circumvent the Challenge of Unverifiability by evaluating
the estimators through the lens of reliability—through perturbing the verifiable variables of XAI evaluation
and thereafter analysing the estimator’s outcomes under different failure modes, we can get an objective
and independent characterisation of its performance. We show, in a series of experiments, how to use
the framework for estimator selection and for systematic meta-evaluation of the strengths and weaknesses
of individual estimators (Section 6.1.1), general categories of explanation quality (Section 6.1.2) as well as
architecture types (Section 6.1.3). Our findings show that (i) localisation estimators demonstrate a deficiency
in terms of adversary reactivity, possibly due to their dependency on the segmentation mask, (ii) faithfulness
estimators are inconsistent in their ranking and (iii) estimators within the randomisation- and robustness
categories demonstrate the highest performance. It is advised that XAI practitioners take into account the
limitations of various estimator categories and exercise caution when relying heavily on certain categories.
Evaluating the intrinsic value or “validity” of a quality estimator is, however, still an open and important
question to consider. It is essential to keep in mind that the reliability of an estimator does not necessarily
imply any intrinsic validity, e.g., an estimator’s theoretical soundness (Sundararajan et al., 2017; Binder
et al., 2022). As we make the meta-evaluation tests readily available, we inadvertently create the risk of the
tests being misused and the results being misinterpreted. We encourage the reader to exercise caution when
interpreting the outcome of the tests and not rush to conclusions regarding the overall reliability of quality
estimators, which should always be judged with respect to their specific experimental context.
Our experiments are limited to image classification applications. To fully demonstrate the generalisability
ofMetaQuantus , we plan to extend our experiments more broadly in the sciences and medicine and include
other explanation method types as well as other data domains such as tabular, textual and time series data.
This will require additional work to ensure that the estimators themselves support these data domains, which
will be addressed in upcoming publications.
14Published in Transactions on Machine Learning Research (06/2023)
Acknowledgments
This work was partly funded by the German Federal Ministry for Education and Research through project
Explaining 4.0 (ref. 01IS200551), BIFOLD (ref. 01IS18025A and ref. 01IS18037A), the Investitionsbank
Berlin through BerDiBa (grant no. 10174498), the European Union’s Horizon 2020 research and innovation
programme through iToBoS (grant no. 965221) as well as European Union’s Horizon 2022 research and
innovationprogramme(EUHorizonEurope)TEMA(grantno. 101093003). Itwasalsofinanciallysupported
by the Research Council of Norway (RCN), through the FRIPRO (grant no. 315029), and partially funded
by its Centre for Research-based Innovation funding scheme (Visual Intelligence, grant no. 309439) and
Consortium Partners, RCN IKTPLUSS (grant no. 303514), and the UiT Thematic Initiative “Data-Driven
Health Technology”.
References
Julius Adebayo, Justin Gilmer, Michael Muelly, Ian J. Goodfellow, Moritz Hardt, and Been Kim. Sanity
checks for saliency maps. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman,
Nicolò Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31:
Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
Montréal, Canada , pp. 9525–9536, 2018.
Chirag Agarwal, Nari Johnson, Martin Pawelczyk, Satyapriya Krishna, Eshika Saxena, Marinka Zitnik, and
Himabindu Lakkaraju. Rethinking stability for attribution-based explanations. CoRR, abs/2203.06877,
2022a.
Chirag Agarwal, Satyapriya Krishna, Eshika Saxena, Martin Pawelczyk, Nari Johnson, Isha Puri, Marinka
Zitnik, and Himabindu Lakkaraju. OpenXAI: Towards a transparent evaluation of model explanations.
InThirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track ,
2022b.
David Alvarez-Melis and Tommi S. Jaakkola. Towards robust interpretability with self-explaining neural
networks. In Samy Bengio, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett
(eds.),Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information
Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada , pp. 7786–7795, 2018.
Anna Arias-Duart, Ferran Pares, Dario Garcia-Gasulla, and Victor Gimenez-Abalos. Focus! Rating xai
methods and finding biases. CoRR, abs/2203.02928, 2021.
Leila Arras, Ahmed Osman, and Wojciech Samek. CLEVR-XAI: A benchmark dataset for the ground truth
evaluation of neural network explanations. Information Fusion , 81:14–40, 2022.
Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and
Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance
propagation. PloS one , 10(7), 2015.
David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and Klaus-Robert
Müller. How to explain individual classification decisions. J. Mach. Learn. Res. , 11:1803–1831, 2010.
Naman Bansal, Chirag Agarwal, and Anh Nguyen. SAM: the sensitivity of attribution methods to hy-
perparameters. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR
Workshops 2020, Seattle, WA, USA, June 14-19, 2020 , pp. 11–21. Computer Vision Foundation / IEEE,
2020.
I. Bellido and Emile Fiesler. Do backpropagation trained neural networks have normal weight distributions?
In Stan Gielen and Bert Kappen (eds.), ICANN ’93 , pp. 772–775, London, 1993. Springer London. ISBN
978-1-4471-2063-6.
Jose Manuel Benitez, Juan Luis Castro, and Ignacio Requena. Are artificial neural networks black boxes?
IEEE Transactions on Neural Networks and Learning Systems , 8(5):1156–1164, 1997.
15Published in Transactions on Machine Learning Research (06/2023)
Umang Bhatt, Adrian Weller, and José M. F. Moura. Evaluating and aggregating feature-based model
explanations. In Christian Bessiere (ed.), Proceedings of the Twenty-Ninth International Joint Conference
on Artificial Intelligence, IJCAI 2020 , pp. 3016–3022. ijcai.org, 2020.
Alexander Binder, Leander Weber, Sebastian Lapuschkin, Grégoire Montavon, Klaus-Robert Müller, and
Wojciech Samek. Shortcomings of top-down randomization-based sanity checks for evaluations of deep
neural network explanations. CoRR, abs/2211.12486, 2022.
Lennart Brocki and Neo Christopher Chung. Evaluation of interpretability methods and perturbation arti-
facts in deep neural networks. CoRR, abs/2203.02928, 2022.
Lukas Brunke, Prateek Agrawal, and Nikhil George. Evaluating input perturbation methods for interpreting
CNNs and saliency map comparison. In Computer Vision – ECCV 2020 Workshops , pp. 120–134. Springer
International Publishing, 2020.
Kirill Bykov, Anna Hedström, Shinichi Nakajima, and Marina M.-C. Höhne. Noisegrad - enhancing ex-
planations by introducing stochasticity to model weights. In Thirty-Sixth AAAI Conference on Artificial
Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence,
IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Vir-
tual Event, February 22 - March 1, 2022 , pp. 6132–6140. AAAI Press, 2022.
Prasad Chalasani, Jiefeng Chen, Amrita Roy Chowdhury, Xi Wu, and Somesh Jha. Concise explanations
of neural networks using adversarial training. In Proceedings of the 37th International Conference on
Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , volume 119 of Proceedings of Machine
Learning Research , pp. 1383–1391. PMLR, 2020.
Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In IEEE
Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021 , pp.
782–791. Computer Vision Foundation / IEEE, 2021.
Ian Covert, Chanwoo Kim, and Su-In Lee. Learning to estimate shapley values with vision transformers.
CoRR, abs/2206.05282, 2022.
Sanjoy Dasgupta, Nave Frost, and Michal Moshkovitz. Framework for evaluating faithfulness of local expla-
nations. In International Conference on Machine Learning , pp. 4794–4815. PMLR, 2022.
Ann-Kathrin Dombrowski, Maximilian Alber, Christopher J. Anders, Marcel Ackermann, Klaus-Robert
Müller, and Pan Kessel. Explanations can be manipulated and geometry is to blame. In Hanna M.
Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett
(eds.),Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information
Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada , pp. 13567–13578,
2019.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil
Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th Inter-
national Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 .
OpenReview.net, 2021.
Arne Gevaert, Axel-Jan Rousseau, Thijs Becker, Dirk Valkenborg, Tijl De Bie, and Yvan Saeys. Evaluating
feature attribution methods in the image domain. CoRR, abs/2202.12270, 2022.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA,
June 27-30, 2016 , pp. 770–778. IEEE Computer Society, 2016.
Anna Hedström, Leander Weber, Daniel Krakowczyk, Dilyara Bareeva, Franz Motzkus, Wojciech Samek,
Sebastian Lapuschkin, and Marina M.-C. Höhne. Quantus: An explainable ai toolkit for responsible
evaluation of neural network explanations and beyond. Journal of Machine Learning Research , 24(34):
1–11, 2023.
16Published in Transactions on Machine Learning Research (06/2023)
Maximilian Kohlbrenner, Alexander Bauer, Shinichi Nakajima, Alexander Binder, Wojciech Samek, and
Sebastian Lapuschkin. Towards best practice in explaining neural network decisions with lrp. In 2020
International Joint Conference on Neural Networks (IJCNN) , pp. 1–7. IEEE, 2020.
Narine Kokhlikyan, Vivek Miglani, Bilal Alsallakh, Miguel Martin, and Orion Reblitz-Richardson. Investi-
gating sanity checks for saliency maps with image and text classification. CoRR, abs/2106.07475, 2021.
Satyapriya Krishna, Tessa Han, Alex Gu, Javin Pombra, Shahin Jabbari, Steven Wu, and Himabindu
Lakkaraju. The disagreement problem in explainable machine learning: A practitioner’s perspective.
CoRR, abs/2202.01602, 2022.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to docu-
ment recognition. Proc. IEEE , 86(11):2278–2324, 1998.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist , 2, 2010.
Simon Letzgus, Patrick Wagner, Jonas Lederer, Wojciech Samek, Klaus-Robert Müller, and Grégoire Mon-
tavon. Toward explainable AI for regression models. CoRR, abs/2112.11407, 2021.
Zachary C. Lipton. The mythos of model interpretability. Commun. ACM , 61(10):36–43, 2018. doi: 10.114
5/3233231.
Yang Liu, Sujay Khandagale, Colin White, and Willie Neiswanger. Synthetic benchmarks for scientific
research in explainable machine learning. In Thirty-fifth Conference on Neural Information Processing
Systems Datasets and Benchmarks Track (Round 2) , 2021a.
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin
transformer: Hierarchical vision transformer using shifted windows. In 2021 IEEE/CVF International
Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021 , pp. 9992–
10002. IEEE, 2021b.
Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Isabelle Guyon,
Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman
Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pp. 4765–4774, 2017.
Grégoire Montavon, Wojciech Samek, and Klaus-Robert Müller. Methods for interpreting and understanding
deep neural networks. Digit. Signal Process. , 73:1–15, 2018.
NielsJ.S.Morch,UlrikKjems,LarsKaiHansen,ClausSvarer,IanLaw,BennyLautrup,StephenC.Strother,
and Kelly Rehm. Visualization of neural networks using saliency maps. In Proceedings of International
Conference on Neural Networks (ICNN’95), Perth, WA, Australia, November 27 - December 1, 1995 , pp.
2085–2090. IEEE, 1995.
Michael Neely, Stefan F. Schouten, Maurits J. R. Bleeker, and Ana Lucic. Order in the court: Explainable
AI methods prone to disagreement. CoRR, abs/2105.03287, 2021.
An-phi Nguyen and Maria Rodriguez Martinez. On quantitative aspects of model interpretability. CoRR,
abs/2007.07584, 2020.
Frederik Pahde, Galip Ümit Yolcu, Alexander Binder, Wojciech Samek, and Sebastian Lapuschkin. Opti-
mizing explanations by network canonization and hyperparameter search. CoRR, abs/2211.17174, 2022.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in
Neural Information Processing Systems 32 , pp. 8024–8035. Curran Associates, Inc., 2019.
17Published in Transactions on Machine Learning Research (06/2023)
Marco Túlio Ribeiro, Sameer Singh, and Carlos Guestrin. "why should I trust you?": Explaining the pre-
dictions of any classifier. In Balaji Krishnapuram, Mohak Shah, Alexander J. Smola, Charu C. Aggarwal,
Dou Shen, and Rajeev Rastogi (eds.), Proceedings of the 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016 , pp. 1135–1144.
ACM, 2016.
Laura Rieger and Lars Kai Hansen. IROF: a low resource evaluation metric for explanation methods. CoRR,
abs/2003.08747, 2020.
Yao Rong, Tobias Leemann, Vadim Borisov, Gjergji Kasneci, and Enkelejda Kasneci. A consistent and
efficient evaluation strategy for attribution methods. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song,
Csaba Szepesvári, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning,
ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA , volume 162 of Proceedings of Machine Learning
Research , pp. 18770–18795. PMLR, 2022.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale
visual recognition challenge. International Journal of Computer Vision , 115(3):211–252, 2015.
Wojciech Samek, Alexander Binder, Grégoire Montavon, Sebastian Lapuschkin, and Klaus-Robert Müller.
Evaluating the visualization of what a deep neural network has learned. IEEE Transactions on Neural
Networks and Learning Systems , 28(11):2660–2673, 2017.
Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and
Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. Int. J.
Comput. Vis. , 128(2):336–359, 2020.
Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. Not just a black box: Learning
important features through propagating activation differences. CoRR, abs/1605.01713, 2016.
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising
imageclassificationmodelsandsaliencymaps. InYoshuaBengioandYannLeCun(eds.), 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Workshop
Track Proceedings , 2014.
Leon Sixt, Maximilian Granz, and Tim Landgraf. When explanations lie: Why many modified BP attribu-
tions fail. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18
July 2020, Virtual Event , volume 119of Proceedings of Machine Learning Research , pp.9046–9057. PMLR,
2020.
Daniel Smilkov, Nikhil Thorat, BeenKim, Fernanda Viégas, and MartinWattenberg. Smoothgrad: removing
noise by adding noise. arXiv preprint arXiv:1706.03825 , 2017.
Mukund Sundararajan and Ankur Taly. A note about: Local explanation methods for deep neural networks
lack sensitivity to parameter values. CoRR, abs/1806.04205, 2018.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In Doina
Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning,
ICML 2017, Sydney, NSW, Australia, 6-11 August 2017 , volume 70 of Proceedings of Machine Learning
Research , pp. 3319–3328. PMLR, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and
Rob Fergus. Intriguing properties of neural networks. In Yoshua Bengio and Yann LeCun (eds.), 2nd
International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16,
2014, Conference Track Proceedings , 2014.
Jonas Theiner, Eric Müller-Budack, and Ralph Ewerth. Interpretable semantic photo geolocation. In
IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022, Waikoloa, HI, USA,
January 3-8, 2022 , pp. 1474–1484. IEEE, 2022.
18Published in Transactions on Machine Learning Research (06/2023)
Richard Tomsett, Dan Harborne, Supriyo Chakraborty, Prudhvi Gurram, and Alun D. Preece. Sanity checks
for saliency metrics. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The
Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI
Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February
7-12, 2020 , pp. 6021–6029. AAAI Press, 2020.
Yipei Wang and Xiaoqian Wang. A unified study of machine learning explanation evaluation metrics. CoRR,
abs/2203.14265, 2022.
Frank Wilcoxon. Individual comparisons by ranking methods. Biometrics Bulletin , 1(6):80–83, 1945. ISSN
00994987.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. CoRR, abs/1708.07747, 2017.
Mengjiao Yang and Been Kim. Benchmarking Attribution Methods with Relative Feature Importance.
CoRR, abs/1907.09701, 2019.
Chih-Kuan Yeh, Cheng-Yu Hsieh, Arun Sai Suggala, David I. Inouye, and Pradeep Ravikumar. On the
(in)fidelity and sensitivity of explanations. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer,
Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Pro-
cessing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
December 8-14, 2019, Vancouver, BC, Canada , pp. 10965–10976, 2019.
Gal Yona and Daniel Greenfeld. Revisiting sanity checks for saliency maps. CoRR, abs/2110.14297, 2021.
Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In David J.
Fleet, Tomás Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), Computer Vision - ECCV 2014 - 13th
European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I , volume 8689 of
Lecture Notes in Computer Science , pp. 818–833. Springer, 2014.
Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. Top-down
neural attention by excitation backprop. International Journal of Computer Vision , 126(10):1084–1102,
2018.
19Published in Transactions on Machine Learning Research (06/2023)
A Appendix
Inthissection,weincludeallthenecessarydetailsandinformationtosupporttheclaimsandresultspresented
in the main paper. In Section A.1, we present theoretical considerations for the meta-evaluation framework.
In Sections A.2 and A.3, we outline the mathematical definitions of explanation quality, for categories and
estimators, respectively. In Section A.4, we describe the experimental setup and in the following Section
A.5, we describe the experiments that were performed to sanity-check the framework. In Section A.6, we
provide supplementary results, in terms of additional applications and supporting experiments. We provide
a notation table at the end in Section A.7.
Broader Impact Statement
This research is important since we raise awareness and address the need for reliable evaluation methods in
the Explainable AI (XAI) community. In the XAI community, the evaluation of explanation methods has
often been neglected or clouded by the ambiguity that an absence of ground truth labels entails—yet to
foster sustainable progress in the field over time it is necessary to systematically define and meta-evaluate
the methods used to measure the quality of explanations. This research takes the first step towards this
goal by developing practical, quantifiable tools for reliable XAI evaluation. Without careful examination
of the quality of explanations, the deployment of potentially beneficial machine learning algorithms may
be hindered, preventing the full potential of AI from being realised in important areas such as healthcare,
education, finance and policy.
A.1 Theoretical Considerations
This section discusses the concept of minor and disruptive perturbations in the context of multi-label clas-
sification and regression tasks in XAI. It also addresses the potential vulnerability of the framework to
adversarial attacks and the motivation for the calculation of the IEC scoring criterion for disruptive pertur-
bations.
A.1.1 Expanding Scope and Adversarial Attacks
Multi-label classification In this work, given the popularity of image classification tasks in XAI, we
focused mostly on this application. In this application, the definitions of minor and disruptive perturbations
(asgiveninDefinitions1and2)applyto ˆyandy′whichrepresentthepredictedclassesfortrueandperturbed
samples, respectively. However, our definitions can be easily extended to other types of classification tasks,
such as multi-label classification. In this case, for a multi-label classification problem with Cclasses, the
definitions of minor and disruptive perturbations (as given in Definitions 1 and 2) would apply to binary
prediction vectors ˆy∈RCandy′∈RC, rather than single classes. The distance between the two vectors
can be denoted using the |||·|||nnotation.
Regression As is the case with many explanation methods (Letzgus et al., 2021), the extension to the
regression problem in XAI is not straightforward. Given yandy′as real-valued prediction outcomes, we
would need to adjust Definitions 1 and 2 to encompass a derivation of a proper boundary ϵ. We leave the
task of adapting the meta-evaluation framework to regression problems to future work.
Adversarial attacks Adversarial attacks are techniques used to manipulate or deceive models (Szegedy
et al., 2014) or their explanations (Dombrowski et al., 2019) by introducing perturbations to the input data
that are imperceptible to humans but results in an incorrect prediction by the model. To adversarially
attack Definition 1, it is theoretically possible to define a perturbation that maximises the strength of the
perturbation, while still remaining consistent with Definition 1, i.e., ||ˆy−y′||p> ϵ. In the same vein, to
attack Definition 2, it is theoretically possible to define a perturbation that minimises the strength of the
perturbation, while still remaining consistent with Definition 2, i.e., ||ˆy−y′||p≤ϵ. While it may be a
theoretical possibility to attack the framework through the definitions, performing such attacks would not
serve any practical purpose. This is because it contradicts the purpose of the framework, which is to assist
practitioners in selecting and developing reliable XAI methods.
20Published in Transactions on Machine Learning Research (06/2023)
A.1.2 Motivation for IEC ARcalculation
Contrary to the calculation of the inter-consistency criterion for minor perturbations, i.e., IEC NR, we cannot
motivate IEC ARbased on ranking consistency with respect to explanation methods, as disruptive perturba-
tions implicate a change in the estimator score. While the expectation of changed rankings as a result of
disruptive perturbations may appear intuitive, the imposed change on the quality estimators could theoret-
ically lead to a symmetrical change across explanation method scores, which preserves the ranking across
explanation methods. Since the behaviour of the explanation functions under disruptive perturbations lies
in the unverifiable spaces U, we cannot exclude the possibility of a symmetrical response. Accordingly, for
the calculation of IEC AR, we relax the theoretical assumptions to a ranking comparison based on scores (as
defined in Equation 7) which remain in the verifiable spaces Ω.
The assumption for the calculation of the IEC score with respect to disruptive perturbation is motivated
by the scenario of an ideal estimator, which is expected to be able to assess the true performance of an
explanation method, denoted qtrue. In the ideal scenario, the real performance varies only slightly, i.e.,
qtrue
j±ϵwould therefore define an upper estimation bound qtrue
j≈qmax
jfor each explanation method
j∈[1,...,L ]6. All estimates ¯QD
i,jresulting from the AR scenario should differ from the unperturbed quality
estimate ¯QD
i,j̸=¯Q∗
i,j. In the idealised scenario qtrue
j≈qmax
j, we argue that ¯Q∗
i,j≈qtrue
jand ¯QD
i,j<¯Q∗
i,j,
leading to Equation 8. Note, however, that in practice quality estimates are subject to larger variations which
means that the assumption qtrue
j≈qmax
jand Equation 8 might not always hold. Therefore, in practice, we
do not expect IEC AR≈1, which aligns with our results in Table 1. Nonetheless, further research on the
inter-consistency criterion under disruptive perturbations is subject to future work.
A.2 Explanation Quality: Category Definitions
In the main paper, we described how a lack of explanation ground truth labels has led to a diverse set of
interpretations of explanation quality. In the following, we provide a brief summary of the most established
categories of explanation quality, grouped into six categories; (a) faithfulness, (b) robustness, (c) randomi-
sation, (d) complexity, (e) localisation and (f) axiomatic estimators (Hedström et al., 2023). To establish
a mathematical ground for each category, we present a summarising equation. This means that all the
nuances that typically exist within a category of explanation quality are not considered. For completeness,
we, therefore, provide the exact mathematical descriptions of each of the individual estimators used in this
work in Appendix A.3.
Since many explanation categories do rely on perturbation, we define a general perturbation function on any
real-valued space S⊆RN,N∈Nin the following.
Definition 3 (Perturbation) .LetPS(s;η) :S∝⇕⊣√∫⊔≀→Sbe a perturbation function of s∈Swith parameters
η∈Rsuch that∀ˆs∈S,ˆs̸=s:
PS(s;η) =ˆs. (10)
For simplicity, we also write PS(s) :=PS(s;η), which is used in the main paper.
Faithfulness (Montavon et al., 2018; Samek et al., 2017; Bach et al., 2015; Bhatt et al., 2020; Nguyen
& Martinez, 2020) quantifies the extent that explanations follow the predictive behaviour of the model,
asserting that more important features affect model decisions more strongly. Given f,x,y′and ˆe, the
change in the model output f(x)is measured as the input features of xare manipulated based on their
attribution in ˆe. The input manipulation is defined as a perturbation function PX(x,M)withx∈Xwhere
Mis the number of input features that are perturbed. Since fdenotes a trained model with parameters θ,
for brevity, we denote f(x;θ)asf(x)where possible.
ΨF(Φ,f,x,P,M) =|(f(x)−f(PX(x,M))|. (11)
6Here, we present general theoretical considerations, but the specific claims for each estimator would require individual
proofs.
21Published in Transactions on Machine Learning Research (06/2023)
Robustness (Montavon et al., 2018; Alvarez-Melis & Jaakkola, 2018; Yeh et al., 2019; Dasgupta et al.,
2022) measures the stability of the explanation function with respect to small changes in the input, requiring
that those small perturbations in the input space ||PX(x)−x||p<ε, e.g., under an ℓpnorm constraint upper
bounded by some positive constant ε, lead to only slight changes in the explanation ||ˆe−Φ(PX(x),f,ˆy)||<ε
assuming that the model output approximately stayed the same f(x)≈f(PX(x)).
ΨRO(Φ,f,x,ˆy,ˆe,P) =||ˆe−Φ(PX(x),f,ˆy;λ)||≤ε. (12)
Randomisation (Adebayo et al., 2018; Sixt et al., 2020) measures the extent explanations deteriorate
as randomness is introduced to the quality estimation. For example, Adebayo et al. (2018) measure the
change in explanation as model parameters θare increasingly randomised, requiring large perturbations
in the parameter space of the model, i.e., PF(θ)≫εto result in large changes in the explanation, i.e.,
||ˆe−Φ(x,PF(θ),ˆy;λ)||≫ε.
ΨRA(Φ,f,x,ˆy,ˆe,P,ε) =||ˆe−Φ(x,PF(θ),ˆy;λ)||≫ε. (13)
Complexity (Bhattet al.,2020;Chalasani etal.,2020; Nguyen&Martinez, 2020)capturestheconciseness
of explanations, i.e., only a few features should be selected to explain a model prediction. The notion of
complexity differs in how it is empirically interpreted, e.g., by computing the Shannon entropy of attribution
map (Bhatt et al., 2020). Alternatively, Chalasani et al. (2020) quantifies complexity by calculating the Gini
Index of the absolute value of the attribution vector ˆewhereDis the length of the attribution vector.
ΨC(ˆe) =/summationtextD
i=1(2i−D−1)ˆei
D/summationtextD
i=1ˆei. (14)
Localisation (Theiner et al., 2022; Kohlbrenner et al., 2020; Zhang et al., 2018; Rong et al., 2022; Arias-
Duart et al., 2021) tests if the explainable evidence is centred around a region of interest, which may be
defined around an object by a bounding box, a segmentation mask or a cell within a grid. It requires
an additional segmentation mask sgt∈RD, mostly a binary mask of the input sgt
i∈{0,1}, serving as a
simulation or “proxy” of ground truth. While many variations exist, the goodness of ˆecan be defined by,
e.g., their intersection divided by their union.
ΨL(ˆe,sgt) =ˆe∩sgt
ˆe∪sgt. (15)
Axiomatic (Sundararajan et al., 2017; Nguyen & Martinez, 2020) estimators measure to what extent
an explanation fulfil some axiomatic properties such as completeness (Sundararajan et al., 2017) and
non—sensitivity (Nguyen & Martinez, 2020). Due to the ambiguity that arises when empirically evalu-
ating estimators in this category, we do not study this category in detail.
A.3 Explanation Quality: Estimator Definitions
Within each of the five categories of explanation quality used in this work; (a) faithfulness, (b) robustness,
(c) randomisation, (d) complexity and (e) localisation, we benchmarked two estimators per category in
our benchmarking experiments. In the experiments that involved comparing meta-evaluation outcomes of
estimators within the localisation category across different model architectures, we included two additional
estimators.
Faithfulness Correlation (FC) (Bhatt et al., 2020) is defined in the following:
ΨFC= corr
S∈|S|⊆d/parenleftigg/summationdisplay
i∈SΦ(x,f,ˆy;λ)i,f(x)−f/parenleftbig
x[xs=xs]/parenrightbig/parenrightigg
, (16)
where|S|⊆Dis a subset of indices of a sample x,xis the chosen baseline value and x[xs=xs]is, therefore,
the masked input, with indices chosen randomly. Since fdenotes a trained model with parameters θ, for
brevity, we denote f(x;θ)asf(x)where possible. Higher values indicate that the explanation method’s
assignment of attribution is correlated with the behaviour of the model and hence is preferred.
22Published in Transactions on Machine Learning Research (06/2023)
Pixel-Flipping (PF) (Bach et al., 2015) returns a curve of prediction scores over an iterative set of
pixel replacements, which are sorted in descending order by the highest relevant pixel in the explanation
Φ(x,f,ˆy;λ). To return one evaluation score per input sample, we calculate the area under the curve (AUC)
as follows:
ΨPF=n/summationdisplay
i=1(ˆyi+ ˆyi+1)·pi+1−pi
2(17)
wherenis the number of discrete perturbation steps, piandpi+1are the x-values for the ithand(i+ 1)th
perturbation steps and ˆyiandˆyi+1are the prediction values. For faithful explanations, a steep degradation
of prediction scores is expected when attributions are iteratively replaced in descending order. Therefore, a
lower value of AUC is indicative of better performance.
Max-Sensitivity (MS) (Yeh et al., 2019) measures the maximum sensitivity of an explanation using a
Monte Carlo sampling-based approximation. It is defined as follows:
ΨMC= max
x+δ∈Nϵ(x)≤ε/bracketleftbigg∥Φ(x,f,ˆy;λ)−Φ(x+δ,f,ˆy;λ)∥
∥x∥/bracketrightbigg
, (18)
whereεdefines the radius of a discrete, finite-sample neighborhood around each input sample x. This
neighborhood, denoted as Nϵ(x), includes all samples in the set Xthat are within a distance of εfromx.
A lower MS score is indicative of more robustness.
Local Lipschitz Estimate (LLE) (Alvarez-Melis & Jaakkola, 2018) works similarly to the Max-
Sensitivity (MS) method and estimates the Lipschitz constant of the explanation, which is a measure of
how much the explanation changes with respect to the input under slight perturbation, defined as δ. The
LLE method is defined as follows:
ΨLLE= max
x+δ∈Nϵ(x)≤ϵ∥Φ(x,f,ˆy;λ)−Φ(x+δ,f,ˆy;λ)∥2
∥x−(x+δ)∥2, (19)
where lower values indicate less change with respect to the change in input, which is desirable.
Model Parameter Randomisation Test (MPR) (Adebayo et al., 2018) measures the correlation be-
tween an explanation from a randomly parameterised model f(x;PF(θ;v)) = ˆfand the original model ffor
each separate layer vof the network. To generate one quality estimate per sample, we calculate the average
of the correlation coefficients over all the layers in the network, denoted V:
ΨMPR =1
VV/summationdisplay
v=1corr(Φv(x,f,ˆy;λ),Φv(x,ˆf,ˆy;λ)), (20)
where Φv(x,f,ˆy;λ)is the explanation generated by the original model ffor layervandΦv(x,ˆf,ˆy;λ)is the
explanation generated by the randomly parameterised model ˆffor layerv. The correlation between the two
explanations is calculated for each layer and then averaged over all layers to generate the MPR score, where
a lower correlation coefficient is desired.
Random Logit (RL) method proposed by (Sixt et al., 2020) is originally defined using the structural
similarity index ( SSIM) over the explanation of the ground truth label and an explanation of non-target
classy′. However, to make it comparable with the MPR estimator, the SSIMcalculation is replaced with
theSpearman Rank Correlation Coefficient as follows:
ΨRL=corr(Φ(x,f,ˆy;λ),Φ(x,f,y′;λ)), (21)
where Φ(x,f,ˆy;λ)is the explanation generated for the prediction ˆyandΦ(x,f,y′;λ)is the explanation
generated for a non-target class y′. Lower values indicate that the explanations are not correlated which is
desirable.
23Published in Transactions on Machine Learning Research (06/2023)
Sparseness (SP) (Chalasani et al., 2020) is a method for evaluating the sparsity of explanations and is
defined as the Gini index of the explanation. It is calculated by summing the product of the ranks of the
input features and their attributions and dividing by the sum of the attribution as follows:
ΨSP=/summationtextD
i=1(2i−D−1)·ˆei
D(D−1)/summationtextD
i=1ˆei, (22)
a higher sparseness score indicates lower complexity of the explanation ˆe, which is desirable.
Complexity (CO) (Bhatt et al., 2020) is defined using the Shannon entropy calculation which measures
the amount of uncertainty or randomness in the explanation map. It is calculated by summing the product
of the probabilities of the attributions and the logarithm of the probabilities of the attributions:
ΨCO=Ei[−ln (PΦ)] =−D/summationdisplay
i=1PΦ(i) ln (PΦ(i))
with PΦ(i) =|Φi(x,f,ˆy;λ)|/summationtext
j∈[d]|Φj(x,f,ˆy;λ)|;PΦ={PΦ(1),..., PΦ(d)},(23)
where|·|denotes the absolute value, PΦ(i)denotes the fractional contribution of feature xito the total
quantity of the attribution. A higher entropy indicates a higher level of uncertainty or randomness, i.e., a
higher complexity. A uniformly distributed attribution would have the highest possible complexity score.
Pointing-Game (PG) (Zhang et al., 2018) captures whether the feature of maximal attribution lies on
the ground truth mask, which is a binary mask indicating the true features that contribute to the model’s
output. It is defined as follows:
ΨPG=/braceleftigg
1ifarg maxiΦi(x,f,ˆy;λ)∈sgt
0otherwise(24)
where Φi(x,f,ˆy;λ)represents the ithinput feature of highest atttribution and sgt∈RDdenotes the binary
ground truth mask.
Relevance Mass Accuracy (RMA) (Arras et al., 2022) quantifies the fraction of the sum of the attri-
bution that intersects with the ground truth mask over the full explanation sum. It is defined as follows:
ΨRMA =/summationtextD
i=1Φi(x,f,ˆy;λ)·sgt,i/summationtextD
i=1Φi(x,f,ˆy;λ), (25)
where Φi(x,f,ˆy;λ)is the attribution of the ithinput feature and sgt,iis the value of the ithelement in the
ground truth mask.
Top-K Intersection (TK) (Theiner et al., 2022) extends the Pointing-Game method (Zhang et al., 2018)
by measuring the fraction of Khighest ranked attributions that lie on the binary mask sgt:
ΨTK=|s1−K|
|K|withs1−K=r1−K∩sgt, (26)
wheresKdenotes the subset of indices of explanation ˆethat corresponds to the Khighest ranked features
that intersect with sgt, withr=Rank (ˆe).
Relevance Rank Accuracy (RRA) (Arras et al., 2022) counts the number of features ranked by attri-
bution value that intersects with sgt:
ΨRRA=|sˆe|
|sgt|withsˆe=r∩sgt, (27)
wheresˆerepresents the elements of the explanation ˆethat intersect with each ithpositive element in the
ground truth mask sgt, withr=Rank (ˆe).
24Published in Transactions on Machine Learning Research (06/2023)
A.4 Experimental Setup
In this section, we describe the experimental setup more in detail, which includes the datasets, models,
explanation methods and estimators used in this work. We keep this section short since most of the methods
in the following have been widely discussed in previous works. For more details, we refer the reader to
the respective original publications. The experiments can be reproduced following the instructions in the
repository ( https://github.com/annahedstroem/MetaQuantus ).
Datasets We use four image classification datasets in the experiments: ILSVRC-15 (ImageNet) (Rus-
sakovsky et al., 2015), MNIST(LeCun et al., 2010), fMNIST (Xiao et al., 2017) and cMINST (customised-
MNIST) (Bykov et al., 2022). For MNIST and fMNIST, we randomly sample 1024test samples. We
randomly sample 384test samples from cMINST (customised-MNIST) (Bykov et al., 2022) which consists of
MNIST digits displayed on uniformly sampled CIFAR-10 (Krizhevsky et al., 2009) backgrounds. To under-
stand the real impact of State-of-the-art (SOTA), we also perform experiments on ILSVRC-15 (ImageNet)
(Russakovsky et al., 2015), using 206randomly selected test samples in batches of 50samples.
We have chosen these datasets based on the availability of segmentation masks, since the estimators within
the localisation category require such masks for computation. The bounding boxes for these datasets are
designed to enclose the object of interest. For cMNIST, the bounding box covers 25% of the input and for
MNIST and fMNIST, they cover approximately 35% (but up to 64%) of the input features. For ImageNet,
the bounding boxes vary in size depending on the class of interest.
Models The experiments are performed using different neural network models, including architectures
such as LeNets (LeCun et al., 1998) and ResNets (He et al., 2016) which contributes to the robustness of
our findings. For MNIST and fMNIST, we train LeNets to an accuracy of 98.14% and 87.44% respectively.
For the cMNIST dataset, a ResNet-9 is trained to an accuracy of 98.09%. The training of all models is
performed in a similar fashion; employing SGD optimisation with a standard cross-entropy loss, an initial
learning rate of 0.001and momentum of 0.9. All models are trained for 20epochs each. For ILSVRC-15
(Russakovsky et al., 2015), we use the ResNet-18 model (He et al., 2016) with pre-trained weights given the
ImageNet dataset, accessible via PyTorch (Paszke et al., 2019). In Appendix A.6.4, we further performed
experiments with transformers-based model architectures including Vision Transformer (ViT) (Dosovitskiy
et al., 2021) and SWIN Transformer (Liu et al., 2021b), both with pre-trained weights using PyTorch (Paszke
et al., 2019).
Explanation methods We employ explanation methods from a widely used category of post-hoc at-
tribution methods, both gradient-based and model-agnostic techniques, i.e., Gradient (Morch et al., 1995;
Baehrens et al., 2010), Saliency (Morch et al., 1995), GradCAM (Selvaraju et al., 2020), Integrated Gradients
(Sundararajan et al., 2017), Input×Gradient (Shrikumar et al., 2016), Occlusion (Zeiler & Fergus, 2014) and
GradientSHAP (Lundberg & Lee, 2017).
Inallexperiments, wegenerateexplanationsforasample’spredictedclass ˆy. Whereascertainestimatorssuch
as the Saliency explanation ignore the signs of the explanations, we refrain from taking their absolute values,
to preserve the explainable evidence in the attribution. For comparability, we normalise the explanations
prior to their evaluation using the square root of its average second-moment estimate (Binder et al., 2022),
which is defined as follows:ˆeh,w/parenleftig
1
HW/summationtext
h′,w′ˆe2
h′,w′/parenrightig1/2, (28)
where ˆeh,wis the value of the explanation map at pixel location ( h,w) andH,Wdenote the height and
width, respectively7.
7This normalisation ensures that each score in the attribution map has an average squared distance to zero that is equal to
one. Since this operation does not normalise the attributions into a fixed range, it is not meant for visualisations, rather it is
meant to preserve a quantity that is useful for the comparison of distances between different explanation methods.
25Published in Transactions on Machine Learning Research (06/2023)
Estimators We select the most established estimators within each of the five categories of explanation
quality:Complexity (CO) (Bhatt et al., 2020), Sparseness (SP) (Chalasani et al., 2020), Faithfulness Corre-
lation(FC) (Bhatt et al., 2020), Pixel-Flipping (PF) (Bach et al., 2015), Max-Sensitivity (MS) (Yeh et al.,
2019),Local Lipschitz Estimate (LLE) (Alvarez-Melis & Jaakkola, 2018), Pointing-Game (PG) (Zhang et al.,
2018),Relevance Mass Accuracy (RMA) (Arras et al., 2022), Model Parameter Randomisation Test (MPR)
(Adebayo et al., 2018) and Random Logit (RL) (Sixt et al., 2020). We have defined each of the individual
estimators mathematically in Appendix A.3.
Parameterisation For the initialisation of the different estimators, we mostly followed the recommenda-
tions as stated in the respective original publications. However, to make the estimators within a certain
explanation category as comparable as possible, alterations to certain hyperparameters were made. When
applying Pixel-Flipping (Bach et al., 2015) on image datasets, it generally becomes computationally infea-
sible to iterate over one pixel at a time. Therefore, we iterate over2∗w
DwhereDis the dimensions of the
input andwandhare the width and height of the image, respectively (which are assumed to be the same).
We also use this same value to choose the subset size for Faithfulness Correlation (Bhatt et al., 2020). For
both faithfulness estimators, as the replacement strategy for masked pixels, we use uniform sampling where
we set the lower and higher bounds to the minimum and the maximum value of the test set, respectively.
For the robustness estimators, which both are based on Monte Carlo sampling-based approximation, we let
it run for 10iterations. In the randomisation category, for comparability, we use the Spearman’s Rank Cor-
relation Coefficient to calculate the similarity between the original explanation and the explanation subject
to randomisation. A full overview of the parameterisation of the estimators can be found in the GitHub
repository https://github.com/annahedstroem/MetaQuantus .
Hardware All experiments were computed on GPUs where we used NVIDIA A100-PCIE 40GB for the
toy datasets and NVIDIA A100-PCIE 80GB and Tesla V100S-PCIE-32GB for ImageNet dataset.
A.5 Sanity-Checks of the Meta-Evaluation Framework
In this section, we conduct two sanity-checking experiments. In the first experiment, we create and meta-
evaluate adversarial estimators to demonstrate the usability of the framework in practice and highlight how
the two failure modes act complementary. In the second experiment, we examine the extent that the choice
ofL, i.e., the set of explanation methods, may influence the MC score.
A.5.1 Adversarial Estimators
To sanity-check the meta-evaluation framework, we created adversarial quality estimators that were intended
to perform poorly in a specific failure mode and thus, should indisputably fail the corresponding part of the
testing scenarios of IPT and MPT. Specifically, we created an adversarial quality estimator that, independent
of its given model, data and explanations, returns scores that are always the same (i.e., using deterministic
sampling8). As such, this estimator should ultimately fail the reactivity to adversary tests (i.e., IAC ARand
IECAR) since those tests expect a response to disruption. We denote this estimator Ψ=. We create a second
adversarial quality estimator that, independent of its inputs, returns scores that are drawn from a different
probability distribution (i.e., using stochastic sampling9). In this setup, we expect poor performance on
the noise resilience tests (i.e., IAC NRand IECNR) since these tests check that the quality estimates remain
relatively unchanged after perturbation. We denote this adversarial estimator Ψ̸=.
Table 2 summarises the outcome of this exercise, which includes the four score elements IAC NR, IACAR,
IECNRandIECAR, aggregatedfor 5iterationswith K= 10forthetwotests, IPTandMPT.Theexpectation
of the test outcome is indicated by the value in brackets after the display of the actual score, including the
8We assemble this adversarial estimator by repeatedly returning the same scores for q′as one set of uniformly sampled
scores ˆq∼U(α, β)withα= 0andβ= 1.
9Here, we sample from a normal distribution N(µ,σ2)withσ2= 1but with different means for the unperturbed- and the
perturbed estimates, respectively. For the unperturbed estimates ˆq, we sample µfrom a wide range, i.e., µ∈[−100000,−1]and
for the perturbed estimates q′, we set a narrow range with µ∈[0,1].
26Published in Transactions on Machine Learning Research (06/2023)
standard deviation. Here, a value of 0indicates that the test should fail10and any other value indicates
the desired outcome of the test to be successful. From Table 2, we note that both estimators, Ψ̸=and
Ψ=produced scores that align with the expected value. Since estimator Ψ̸=, relies on stochastic sampling,
the scores are approximate, nevertheless, the scores and expectation are close and the standard deviation
is small, indicating that the sanity checks results are stable. Overall, we can observe that the expectation
aligns with the empirical reality across the different test settings. Therefore, we conclude the sanity-checking
experiment to be passed.
Another important insight that can be drawn from Table 2 is that the two failure modes complement each
other in determining the performance of an estimator. For an estimator that is provably bad, i.e., returns
scores that are completely unrelated to the model, data and explanation methods (such as demonstrated by
Ψ̸=andΨ=), at least one of the failure modes (AR or NR) will reveal that the estimator is failing. To fully
assess the performance of an estimator, both failure modes are therefore necessary.
Table 2: The sanity-check exercise results show aggregated scores including std, over 5 iterations with K= 5. The
direction of the arrow, i.e., ↑indicates if a higher value is better. The expectation of the test outcome is indicated
by the value in brackets, after the display of the actual score.
Test Estimator IAC NR(↑) IAC AR(↑) IECNR(↑) IECAR(↑)
IPT Ψ̸= 0.015±0.023 (0.00) 0.983 ±0.011 (1.00) 0.248 ±0.004 (0.25) 0.0 ±0.0 (0.00)
Ψ= 1.0±0.0 (1.00) 0.0 ±0.0 (0.00) 1.0 ±0.0 (1.00) 0.0 ±0.0 (0.00)
MPT Ψ̸= 0.014±0.010 (0.00) 0.973 ±0.019 (1.00) 0.248 ±0.003 (0.25) 0.0 ±0.0 (0.00)
Ψ= 1.0±0.0 (1.00) 0.0 ±0.0 (0.00) 1.0 ±0.0 (1.00) 0.0 ±0.0 (0.00)
A.5.2 Dependency on L
The meta-evaluation framework is intentionally designed to take into account the set of explanation methods
given in the setup. For example, in the inter-consistency criterion (IEC) for noise resilience, we compute the
estimator’s ability to rank different explanation methods consistently when exposed to minor perturbations.
The resulting MC score of a quality estimator will, therefore, to a certain extent, be dependent on the choice
ofL: both in terms of its cardinality and how similar the explanation functions are.
To understand how the performance of our quality estimator may vary depending on the choice of L, we
conducted an experiment where we computed the MC score while enumerating various choices of L. In this
experiment, we vary both the cardinality of L, by choosing values of {2,3,4}and the methods included in the
set. We selected both model-agnostic explanation methods such as Occlusion (Zeiler & Fergus, 2014) as well
as gradient-based techniques such as GradCAM (Selvaraju et al., 2020), Integrated Gradients (Sundararajan
et al., 2017) etc. For the sets of 2explanation methods we included: { Gradient, Occlusion }, {Gradient,
Input×Gradient }, {Gradient, Saliency }, {Gradient, Input×Gradient }. For sets with 3methods: { Gradient,
Figure 9: Comparison of averaged meta-consistency performance for different quality estimators using MPT and IPT,
aggregated over 3 iterations with K= 5, across models { LeNet, ResNet } and different datasets { MNIST, fMNIST,
cMNIST } with error bars showing the standard deviation.
10The exception is the expected value of the inter-consistency score, IEC NR, for estimator Ψ̸=is not 0.0 but 0.25. This is
because, for an estimator that assigns attributions randomly, i.e., independent of the explanation method, the likelihood of the
condition ¯rM
j= ¯r⋆
jis1
L.
27Published in Transactions on Machine Learning Research (06/2023)
GradCAM, GradientSHAP }, {Gradient, Saliency, Integrated Gradients } and for 4methods: { Gradient,
Saliency, Input×Gradient, GradCAM }, {Gradient, Saliency, Occlusion, GradCAM }.
In Figure 9, we show the aggregate values for different explanation sets across the datasets separately. Here,
the error bars indicate the standard deviations. By comparing the MC scores category by category, we can
observe that the error bars from the respective estimator do generally not overlap. This means that the
choice ofLhas limited influence on the MC score, suggesting the measure’s stability.
A.6 Supplementary Experiments
In the following section, we present additional experiments conducted in the scope of this work. First, we
demonstrate that MetaQuantus can be used for additional applications in Explainable AI. Here, we include
two demonstrations, first, we show how the MC score can be employed as a target variable for optimising
the hyperparameters of an estimator and second, we demonstrate how the framework can be used to analyse
category convergence. At the end of this section, we discuss supplementary results for the benchmarking
experiment which includes an additional analysis of ranking consistency.
A.6.1 Application — Hyperparameter Optimisation
It is practically well-known and increasingly publicly recognised (Bansal et al., 2020; Brunke et al., 2020;
Brocki & Chung, 2022; Pahde et al., 2022) how difficult it can be to tune the hyperparameters in the ex-
plainability domain. Unlike in traditional ML, in XAI, we generally do not have a target variable to optimise
against. As an additional experiment, we, therefore, investigated how the meta-evaluation framework can
be useful in solving the task of selecting the best set of hyperparameters for a given estimator. For this,
we choose an estimator with relatively many parameters, that is Faithfulness Correlation (Bhatt et al.,
2020) and performed a grid-search on these using ImageNet. By exploring combinations of three baseline
strategies = [’Black’, ’Uniform’, ’Mean’] and four subset sizes = [28,52,102,128], we created 12 hyperpa-
rameter settings11. We determined the performance of each of the estimator’s parameterisation by storing
the meta-evaluation vector mand the MC score at each run. The objective of this exercise is to determine
the hyperparameter setting that optimises the performance of the estimator, i.e., its resilience to noise and
reactivity to adversary.
Figure 10: Left:The results of using the meta-evaluation framework to optimize the hyperparameters of FC (Bhatt
et al., 2020) estimator across 12 parameterisations (P1-P12) on ImageNet dataset, averaged over 3 iterations with
K= 3. The parameter setting P11 demonstrated the highest scores with small standard deviation and thus is
selected as the parameter setting. Right:The results from comparing the correlation coefficients between the meta-
evaluation vector scores for estimators within the same category versus those outside of the category, suggesting that
the estimators of the same category have more resemble with respect to its performance characteristics compared to
estimators outside.
11The parameters were combined in the following 12 settings: P1: [’Black’, 28], P2: [’Black’, 52], P3: [’Black’, 102], P4:
[’Black’, 128], P5: [’Uniform’, 28], P6: [’Uniform’, 52], P7: [’Uniform’, 102], P8: [’Uniform’, 128], P9: [’Mean’, 28], P10: [’Mean’,
52], P11: [’Mean’, 102], P12: [’Mean’, 128].
28Published in Transactions on Machine Learning Research (06/2023)
From Figure 10 (left), we can observe that P11 has the highest meta-consistency score and as such, we
recommend the associated parameter setting of “mean” as the replacement strategy with 102 features as the
subset size. In contrast to previous works that found a relatively large difference in evaluation outcomes
between different parameterisations of faithfulness estimators (Tomsett et al., 2020; Brunke et al., 2020; Rong
etal.,2022;Hedströmetal.,2023), wedetect, thatwiththeMCscore—whichprovidesamorecomprehensive
picture of the estimator’s performance—there is not a considerable variability, as depicted by the similarity
in IAC and IEC scores over P1 to P12.
A.6.2 Application — Category Convergence
The question of whether quality estimators within the same category are measuring the same underlying
concept has been of significant interest to the community (Tomsett et al., 2020; Gevaert et al., 2022). Based
on the observed similarity of estimator shapes in Figure 5—that the estimators within the same category
typically have a higher resemblance in area shapes compared to estimators outside of their categories—we
sought to employ the meta-evaluation framework to investigate whether estimators within a category exhibit
a greater level of correlation than those outside of the category. To address this question, often referred
to as “convergent validity”, the prevalent technique has been to measure intra-correlation, which simply
involves correlating the scores of different estimators within the same category. This approach, however, has
limitations, as it disregards the aspect of ranking consistency (IEC) and may not account for the fact that
scores from different estimators may have different scales and interpretations, which may skew the results.
We improve upon the current methodology proposed in Tomsett et al. (2020); Gevaert et al. (2022) by
calculating the correlation coefficient on the meta-evaluation vector mof different estimators, within- and
outside of their category as produced in the benchmarking exercise. This approach is advantageous as it:
(i) yields scores in a normalised range [0,1]and (ii) provides a more comprehensive view of the estimator’s
performance characteristics by incorporating multiple failure modes and criteria.
Figure 10 (right) presents the results of this experiment, averaged over all estimators as described in A.4.
Here, we can observe that the estimator’s performance characteristics are more similar within a category,
as seen in the higher correlation coefficient ( Spearman Rank Correlation Coefficient ) across all datasets.
These observations contrast previous works by Tomsett et al. (2020); Gevaert et al. (2022) that found a
low correlation coefficient (for faithfulness estimators in particular). We posit that this difference can be
explained by the fact that the meta-evaluation framework considers multiple failure modes and criteria of
Table 3: Meta-evaluation benchmarking results with MNIST, aggregated over 3 iterations with K= 5. IPT results
are in grey rows and MPT results are in white rows. MC denotes the averages of the MC scores over IPT and MPT.
The top-performing MC- or MC method in each explanation category, which outperforms the bottom-performing
method by at least 2 standard deviations, is underlined. Higher values are preferred for all scoring criteria.
Category Estimator MC(↑) MC(↑) IACNR(↑) IACAR(↑) IECNR(↑) IECAR(↑)
ComplexitySparseness 0.558 ±0.0280.640±0.0430.209±0.040 0.946±0.086 0.837±0.002 0.569±0.046
0.476±0.0130.929±0.0630.053±0.0140.840±0.0050.084±0.001
Complexity 0.521 ±0.0030.541±0.007 0.009±0.013 1.000±0.000 1.000±0.000 0.156±0.014
0.500±0.0000.167±0.0000.833±0.0001.000±0.0000.000±0.000
FaithfulnessFaithfulness Corr. 0.540 ±0.0150.537±0.0030.477±0.032 0.900±0.023 0.190±0.003 0.579±0.008
0.543±0.0260.500±0.1070.890±0.0050.190±0.0020.594±0.005
Pixel-Flipping 0.626 ±0.0390.609±0.0390.547±0.139 0.963±0.034 0.299±0.001 0.626±0.046
0.644±0.0380.485±0.1411.000±0.0000.294±0.0060.796±0.006
LocalisationPointing-Game 0.586 ±0.0100.672±0.0200.977±0.005 0.607±0.075 0.996±0.000 0.108±0.012
0.500±0.0001.000±0.0000.000±0.0001.000±0.0000.000±0.000
Relevance Mass Acc. 0.552 ±0.0150.613±0.022 0.258±0.062 0.793±0.023 0.846±0.001 0.553±0.032
0.491±0.0070.940±0.0190.071±0.0190.902±0.0030.051±0.000
RandomisationRandom Logit 0.666 ±0.0040.712±0.0080.360±0.041 0.969±0.010 0.937±0.003 0.581±0.006
0.620±0.0000.186±0.0000.874±0.0000.860±0.0000.562±0.000
Model Param. Rand. 0.583 ±0.0070.624±0.005 0.264±0.019 0.959±0.000 0.764±0.002 0.510±0.001
0.542±0.0100.250±0.0650.806±0.0280.647±0.0030.463±0.004
RobustnessMax-Sensitivity 0.649 ±0.0070.754±0.002 0.547±0.064 0.938±0.033 0.804±0.001 0.726±0.038
0.545±0.0120.361±0.0531.000±0.0000.806±0.0050.011±0.001
Local Lipschitz Est. 0.741 ±0.0300.726±0.026 0.484±0.091 0.935±0.088 0.736±0.002 0.750±0.034
0.756±0.0340.519±0.1180.974±0.0170.740±0.0050.789±0.006
29Published in Transactions on Machine Learning Research (06/2023)
Table 4: Meta-evaluation benchmarking results with fMNIST, aggregated over 3 iterations with K= 5. IPT results
are in grey rows and MPT results are in white rows. MC denotes the averages of the MC scores over IPT and MPT.
The top-performing MC- or MC method in each explanation category, which outperforms the bottom-performing
method by at least 2 standard deviations, is underlined. Higher values are preferred for all scoring criteria.
Category Estimator MC(↑) MC(↑) IACNR(↑) IACAR(↑) IECNR(↑) IECAR(↑)
ComplexitySparseness 0.536 ±0.0110.596±0.0120.145±0.039 0.915±0.045 0.831±0.004 0.492±0.082
0.475±0.0100.917±0.0360.070±0.0030.832±0.0030.083±0.001
Complexity 0.516 ±0.0070.532±0.014 0.050±0.047 0.990±0.027 0.999±0.000 0.086±0.028
0.500±0.0000.167±0.0000.833±0.0001.000±0.0000.000±0.000
FaithfulnessFaithfulness Corr. 0.530 ±0.0210.524±0.021 0.527±0.030 0.857±0.072 0.198±0.008 0.515±0.004
0.536±0.0210.448±0.0870.994±0.0030.196±0.0040.504±0.002
Pixel-Flipping 0.530 ±0.0210.573±0.0250.447±0.050 0.958±0.088 0.329±0.002 0.558±0.032
0.649±0.0180.453±0.0731.000±0.0000.324±0.0010.817±0.003
LocalisationPointing-Game 0.583 ±0.0050.666±0.0090.950±0.025 0.634±0.032 0.995±0.001 0.084±0.018
0.500±0.0001.000±0.0000.000±0.0001.000±0.0000.000±0.000
Relevance Mass Acc. 0.538 ±0.0230.587±0.024 0.231±0.102 0.806±0.056 0.850±0.007 0.460±0.048
0.490±0.0220.944±0.0340.067±0.0670.894±0.0030.055±0.003
RandomisationRandom Logit 0.689 ±0.0050.717±0.0100.234±0.039 1.000±0.000 0.955±0.005 0.680±0.005
0.660±0.0000.062±0.0001.000±0.0000.902±0.0000.677±0.000
Model Param. Rand. 0.570 ±0.0100.622±0.010 0.355±0.042 0.925±0.000 0.755±0.005 0.451±0.000
0.518±0.0100.098±0.0080.902±0.0450.657±0.0040.414±0.001
RobustnessMax-Sensitivity 0.639 ±0.0360.699±0.037 0.515±0.097 0.961±0.021 0.816±0.007 0.501±0.058
0.580±0.0350.504±0.1411.000±0.0000.811±0.0020.004±0.000
Local Lipschitz Est. 0.710 ±0.0220.696±0.038 0.538±0.139 0.979±0.033 0.775±0.005 0.492±0.092
0.724±0.0060.567±0.0370.896±0.0240.774±0.0010.661±0.006
Table 5: Meta-evaluation benchmarking results with cMNIST, aggregated over 3 iterations with K= 5. IPT results
are in grey rows and MPT results are in white rows. MC denotes the averages of the MC scores over IPT and MPT.
The top-performing MC- or MC method in each explanation category, which outperforms the bottom-performing
method by at least 2 standard deviations, is underlined. Higher values are preferred for all scoring criteria.
Category Estimator MC(↑) MC(↑) IACNR(↑) IACAR(↑) IECNR(↑) IECAR(↑)
ComplexitySparseness 0.616 ±0.0150.706±0.0130.352±0.061 0.989±0.017 0.814±0.001 0.670±0.016
0.525±0.0180.626±0.099 0.313±0.0280.830±0.0050.333±0.006
Complexity 0.541 ±0.0180.565±0.024 0.056±0.084 1.000±0.000 0.996±0.001 0.209±0.013
0.518±0.0130.062±0.010 0.928±0.0471.000±0.0000.080±0.005
FaithfulnessFaithfulness Corr. 0.562 ±0.0140.563±0.017 0.508±0.061 0.939±0.017 0.182±0.004 0.622±0.005
0.562±0.0100.490±0.031 0.934±0.0180.188±0.0080.634±0.012
Pixel-Flipping 0.604 ±0.0160.586±0.022 0.565±0.040 0.965±0.022 0.287±0.005 0.526±0.080
0.621±0.0100.495±0.037 0.995±0.0010.295±0.0120.701±0.002
LocalisationPointing-Game 0.687 ±0.0060.873±0.0100.967±0.000 1.000±0.000 0.997±0.000 0.527±0.040
0.502±0.001 0.995±0.0030.013±0.0030.999±0.0010.001±0.000
Relevance Mass Acc. 0.621 ±0.0110.856±0.020 0.751±0.008 0.358±0.055 1.000±0.000 0.791±0.012
0.491±0.0140.640±0.028 0.306±0.0320.796±0.0030.223±0.005
RandomisationRandom Logit 0.713 ±0.0050.723±0.0100.530±0.065 0.894±0.026 0.881±0.007 0.586±0.012
0.703±0.0000.410±0.000 0.884±0.0000.913±0.0000.606±0.000
Model Param. Rand. 0.657 ±0.0090.673±0.003 0.490±0.006 1.000±0.000 0.814±0.005 0.387±0.000
0.641±0.0160.417±0.058 1.000±0.0000.804±0.0050.344±0.004
RobustnessMax-Sensitivity 0.637 ±0.0300.690±0.035 0.494±0.069 0.972±0.045 0.687±0.004 0.606±0.050
0.583±0.0240.582±0.079 0.992±0.0020.680±0.0190.080±0.002
Local Lipschitz Est. 0.697 ±0.0200.689±0.026 0.548±0.077 0.971±0.049 0.628±0.007 0.609±0.042
0.706±0.0140.508±0.047 0.999±0.0000.630±0.0070.685±0.005
what a quality estimator should fulfil and not only one, e.g., ranking consistency (Rong et al., 2022) and
as such, give a more comprehensive answer. However, from the error bars in Figure 10, we also observe,
that the correlation coefficients are greatly varying within each group. Further research is thus necessary to
fully understand the extent to which estimators of the same explanation quality category measure the same
underlying concept.
A.6.3 Supplementary Results — Benchmarking
Similar to Table 1, we present the results of the IPT and the MPT for the MNIST, fMNIST and cMNIST
datasets in Tables 3-5, respectively. The grey rows indicate the results from the IPT and the white rows
30Published in Transactions on Machine Learning Research (06/2023)
Figure 11: A graphicalrepresentation of theMNIST benchmarking results (Table 3), aggregated over 3iterations with
K= 5. Each column corresponds to a category of explanation quality, from left to right: Complexity ,Faithfulness ,
Localisation ,Randomisation andRobustness . The grey area indicates the area of an optimally performing estimator,
i.e.,m∗=14. The MC score (indicated in brackets) is averaged over MPT and IPT. Higher values are preferred.
Figure12: AgraphicalrepresentationofthefMNISTbenchmarkingresults(Table4), aggregatedover3iterationswith
K= 5. Each column corresponds to a category of explanation quality, from left to right: Complexity ,Faithfulness ,
Localisation ,Randomisation andRobustness . The grey area indicates the area of an optimally performing estimator,
i.e.,m∗=14. The MC score (indicated in brackets) is averaged over MPT and IPT. Higher values are preferred.
show the results from the MPT. The results are consistent with those presented in the main manuscript,
both in terms of individual score criteria, estimator- and category comparison.
Similar to Figure 7, we also represent Tables 3-5 as area graphs. With an exception of slightly higher
localisation scores for cMNIST dataset (as explained in the main paper), the results as demonstrated in
Figures 11-13 are completely consistent with those findings presented in the main paper. Recall that, larger
coloured areas imply better performance on the different scoring criteria and the grey area indicates the
area of an optimally performing quality estimator, i.e., m∗=14. Each column of estimators represents a
category of explanation quality, from left to right: Complexity ,Faithfulness ,Localisation ,Randomisation
andRobustness .
We further visualise the results (as shown in Tables 1, 4 and 5) as scatter plots for ImageNet, fMNIST and
cMNIST datasets in Figure 14. In the main paper, we identified that the faithfulness category (blue points)
31Published in Transactions on Machine Learning Research (06/2023)
Figure 13: A graphical representation of the cMNIST benchmarking results (Table 5), aggregated over 3 iterations
withK= 5. Each column corresponds to a category of explanation quality, from left to right: Complexity ,Faith-
fulness,Localisation ,Randomisation andRobustness . The grey area indicates the area of an optimally performing
estimator, i.e., m∗=14. The MC score (indicated in brackets) is averaged over MPT and IPT. Higher values are
preferred.
Figure14: Asupplementaryvisualisationofthebenchmarkingresults(Tables1, 4andreftable-benchmarking-cmnist),
in particular IAC and IEC scores for noise resilience (x-axes) and adverse reactivity (y-axes). The colours indicate
the estimator and the symbols demonstrate the test: IPT and MPT, respectively. Higher values are preferred.
had a particularly low ranking consistency (IEC), which is also evident in these supplementary plots. From
Figure 14, we can moreover observe how the estimators’ scores on the respective failure modes are related.
These plots also show that a higher resilience to noise does not necessarily imply more reactivity to adversary
and vice versa—the performance characteristics of the estimators are more complex than that.
A.6.4 Supplementary Results — Comparing Neural Network Architectures
As part of our exploration of meta-evaluation outcomes across varying model architectures, including those
based on transformers, we embarked on additional benchmarking experiments. These involved comparing
estimators from both the localisation and complexity categories of explanation quality among Vision Trans-
former (Dosovitskiy et al., 2021), SWIN (Liu et al., 2021b), and ResNet-18 (He et al., 2016) models using
the ImageNet dataset (Russakovsky et al., 2015). The outcomes concerning the localisation category have
been discussed in the main manuscript 6.1.3. The results for the complexity category, including estimators
such asComplexity (CO) (Bhatt et al., 2020) and Sparseness (SP) (Chalasani et al., 2020) are discussed
below. For this experiment we used two explanation methods L=Input×Gradient, Gradients .
To analyse the results, we represent the entries of the meta-evaluation vector as coordinates on a 2D plane
and visualise the benchmarking results as area graphs, as seen in Figure 15 (left). Again, larger coloured
32Published in Transactions on Machine Learning Research (06/2023)
areas imply better performance on the different scoring criteria and the grey area indicates the area of an
optimally performing quality estimator, i.e., m∗=14. Each column represents the estimator and each row
the network architecture, as indicated by the labels of Figure 15 (left).
Figure 15: Left:A graphical representation of the ImageNet benchmarking results for the complexity category,
aggregated over 3 iterations with K= 5. Each column represents the estimator, from left to right: Sparseness
andComplexity , and each row the model type { ResNet-18 ,ViT,SWIN}, as indicated by the labels. The grey area
indicates the area of an optimally performing estimator, i.e., m∗=14. The MC score (indicated in brackets) is
averaged over MPT and IPT. Higher values are preferred. Right:A visualisation of the benchmarking results (Tables
3, 4 and 5) showing the distribution of top rankings within each category of explanation quality. There is higher
variability in scores for the IPT tests (above), while it is lower for the MPT tests (below).
As can be observed in Figure 15 (left), the areas indicating the performance profiles of each individual es-
timator across the network types demonstrate a noticeable similarity. In accordance with the observations
made in the main manuscript, the Sparseness method (Chalasani et al., 2020) demonstrates superior perfor-
mance compared to the Complexity method (Bhatt et al., 2020). Overall, this provide additional evidence
in proving the framework to be agnostic with respect to the choice of network architecture.
A.6.5 Supplementary Results — Ranking Consistency
In the main paper, we presented the average MC scores for each dataset in Figure 5, which showed that
the best-performing estimator in each category of explanation quality generally remained consistent across
tested datasets. To further explore this consistency, we considered a margin of error of 2standard deviations
applied to the MC scores and re-calculated the within-category ranking for each individual estimator in each
category and visualised the results in Figure 15 (right).
Figure 15 (right) showcases the the frequency with which the different estimators within each category
achieved the highest or the lowest average score, respectively. The color scheme, consisting of one lighter
and one darker hue for each category, is consistent with the individual estimators in previous figures. A
larger fraction of a bar with a single hue indicates more "category wins" for a specific estimator. From
Figure 15 (right), similar to observations in the main paper, we find that estimators within the localisation
and randomisation categories are prone to stronger score variability. In general, we anticipate variations in
33Published in Transactions on Machine Learning Research (06/2023)
MC scores across all categories between different datasets. This is because all quality estimators evaluate
explanation qualities, which depend on the specific dataset and network in use. Additionally, as seen in
Figure 15 (right), the score variability for IPT (above) is higher than that of MPT (below). Therefore, in the
main paper, we present the MC scores averaged over both MPT and IPT to capture a more comprehensive
view of the estimator’s performance.
A.7 Notation Table
In the following, we provide notation tables that encompasses all the notations employed throughout this
paper. The table is sorted according to the section in which each notation initially appeared.
Preliminaries
f A black-box model function that maps input xto outputy
θ The parameters of the model function f
Xtr The training dataset on which the model fis trained
Xte The test dataset on which the model fis evaluated
x An input in the instance space X
y An output class in the label space Y
ˆy A prediction made by the model f
C The number of output classes
N The number of test samples
D The dimension of the input
X The instance space
Y The label space
F The function space of all models
Φ An explanation function that maps x,f, and ˆyto an explanation map ˆe
λ The parameter of the explanation function Φ
ˆe The explanation map produced by Φ
E The space of possible explanations
Ψ A quality estimation function that takes ˆeand returns a scalar ˆqto indicate its quality
τ The parameter of the quality estimation function Ψ
ˆq A quality estimate made by the estimator Ψ
Ω The verifiable spaces of the estimator’s Ψinput parameters Ω∈{{X},{F},{X,F}}
A Meta-Evaluation Framework
U The unverifiable spaces of the estimator’s Ψinput parameters U∈{{E},{O},{E,O}}
NR The first failure mode, Noise resilience
AR The second failure mode, Adversary reactivity
y′A prediction after perturbation on the input, model or both input and model spaces
ϵ A threshold ϵ∈Rfor determining the type of perturbation
t The perturbation strength t∈{M,D}
PΩ(ω) A perturbation function of the verifiable spaces ω∈Ω
PM
X A minor perturbation function of the input space X
PM
F A minor perturbation function of the function space F
PD
X A disruptive perturbation function of the input space X
34Published in Transactions on Machine Learning Research (06/2023)
PD
F A disruptive perturbation function of the function space F
K The number of perturbations
L The set of explanation methods
ˆq The unperturbed quality estimates ˆq∈RN
q′
k The perturbed quality estimates, replicated Ktimes forNtest samples
d A statistical significance function that takes ˆqandq′
kand returns a p-value
r A ranking function that takes nominal values and returns rankings in descending order
Q A matrix of all perturbed samples over Kperturbations
¯Q A matrix for the unperturbed estimates ˆqforLexplanation methods, averaged over K
¯Q′A matrix for the perturbed estimates q′
kforLexplanation methods, averaged over K
¯QMA matrix for the perturbed estimates under minor perturbation
¯QDA matrix for the perturbed estimates under disruptive perturbation
U A binary ranking agreement matrix that takes quality estimates from ¯Qand ¯Q′and
populates the entries according to the interpretation of ranking
UMA binary ranking agreement matrix with perturbed estimates under minor perturbation
UDA binary ranking agreement matrix with perturbed estimates under disruptive pertur-
bation
m A meta-consistency vector containing the IAC and IAC scores for both failure modes
m∗An optimally performing quality estimator Ψas defined by the all-one vector 14
IAC The intra-consistency scoring criterion, with IAC ∈[0,1]
IEC The inter-consistency scoring criterion, with IEC ∈[0,1]
MC The meta-consistency score, with MC ∈[0,1]
Practical Evaluation
U The uniform distribution with parameters α,β
α The lower bound of the uniform distribution U(α,β)
β The upper bound of the uniform distribution U(α,β)
δi Additive uniform noise applied to input space such that ˆxi=x+δi
N The normal distribution with parameters µ,Σ
µ The mean of the normal distribution N(µ,Σ)
Σ The variance of the normal distribution N(µ,Σ)
νi Multiplicative Gaussian noise applied applied to model parameters such that ˆθi=θ·νi
35