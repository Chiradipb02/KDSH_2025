Published in Transactions on Machine Learning Research (02/2023)
Differentially Private Fréchet Mean on the Manifold of Sym-
metric Positive Definite (SPD) Matrices with log-Euclidean
Metric
Saiteja Utpala saitejautpala@gmail.com
UC Santa Barbara
Praneeth Vepakomma vepakom@mit.edu
MIT
Nina Miolane ninamiolane@ucsb.edu
UC Santa Barbara
Reviewed on OpenReview: https: // openreview. net/ forum? id= mAx8QqZ14f
Abstract
Differential privacy has become crucial in the real-world deployment of statistical and ma-
chine learning algorithms with rigorous privacy guarantees. The earliest statistical queries,
for which differential privacy mechanisms have been developed, were for the release of the
sample mean. In Geometric Statistics, the sample Fréchet mean represents one of the most
fundamental statistical summaries, as it generalizes the sample mean for data belonging to
nonlinear manifolds. In that spirit, the only geometric statistical query for which a differen-
tial privacy mechanism has been developed, so far, is for the release of the sample Fréchet
mean: the Riemannian Laplace mechanism was recently proposed to privatize the Fréchet
mean on complete Riemannian manifolds. In many fields, the manifold of Symmetric Pos-
itive Definite (SPD) matrices is used to model data spaces, including in medical imaging
where privacy requirements are key. We propose a novel, simple and fast mechanism -
thetangent Gaussian mechanism - to compute a differentially private Fréchet mean on the
SPD manifold endowed with the log-Euclidean Riemannian metric. We show that our new
mechanism has significantly better utility and is computationally efficient — as confirmed
by extensive experiments.
1 Introduction
Privacy-preserving computing is an active area of research which is necessitated by ethics, regulations, re-
quirements for protections of trade secrets, or possible lack of trust amongst distributed data siloes. Privacy
preservation is desired across several topologies of data sharing, be it from client devices to powerful central-
ized entities or a in peer-to-peer fashion. Mistrust in data sharing carries over not only in the sharing of raw
data but also in the sharing of results obtained from intermediate or complete computations. The need for
stringent privacy protections is often fueled by many privacy leakages and attacks that continue to happen
under various settings operating without the right level of privacy-protecting mechanisms.
In this context, differential privacy (DP) (Dwork et al., 2006; Dwork, 2008; Dwork et al., 2014; Dwork,
2006) has emerged as one of the leading mathematical definitions to ensure the preservation of privacy up
to a chosen level. Privacy-preserving mechanisms that satisfy the definition of differential privacy were
subsequently developed to privatize a wide range of statistical and machine learning computations. The
earliest queries, for which mechanisms have been proposed, were for the privatization of sample means
in statistics, computed for data lying on linear spaces. When data belong to nonlinear manifolds, the
1Published in Transactions on Machine Learning Research (02/2023)
Fréchet mean query (Fréchet, 1948) is the foundational building block of geometric statistics that needs to
be privatized. Our work proposes a new, simpler and faster, mechanism for private Fréchet means on the
manifold of symmetric positive definite (SPD) matrices endowed with log-Euclidean metric.
1.1 Motivation
Fréchet mean: a building block in geometric statistics While traditional statistics studies data that
lies onlinear spaces , geometric statistics studies data that lies on nonlinear spaces such as Riemannian man-
ifolds, affine connection spaces, or stratified spaces (Pennec et al., 2019; Miolane, 2016). Such analysis is
fruitful as data might have inherent constraints that are well captured by the geometry of a nonlinear space
Miolane et al. (2021); Myers et al. (2022). For instance, symmetric matrices constrained to have strictly
positive eigenvalues are conveniently modeled as elements of the manifold of symmetric positive definite
(SPD) matrices. Several extensions of traditional statistical analysis tools have thus been developed for the
manifold setting: regression has been generalized to geodesic regression (Fletcher, 2011; Thomas Fletcher,
2013), principal component analysis (PCA) to principal geodesic analysis or geodesic PCA (Fletcher et al.,
2004; Sommer et al., 2010; Huckemann et al., 2010), and mean shift to Riemannian mean shift clustering
(Subbarao & Meer, 2009; Caseiro et al., 2012). In each of these algorithms, the computation of the sample
Fréchet mean generalizes the computation of the sample mean , and thus represents the most fundamental
building block. The privatization of the Fréchet mean is therefore the key element required to privatize
geometric statistical queries. Privacy-preserving geometric statistics is also crucial, as one of its main appli-
cation areas is medical imaging and computational anatomy (Pennec et al., 2019; Miolane, 2016) for which
privacy requirements are often desirable.
Importance of the SPD manifold with log-Euclidean metric Symmetric positive definite (SPD)
matricesmodelawiderangeofdata, frommedicalimageswithDiffusionTensorImaging(DTI)(Basseretal.;
Pennecetal.,2006), tophysiologicalsignalswithelectroencephalography(EEG)signalsfrombrain-computer
interfaces (BCI)(Yger et al., 2016; Zanini et al., 2017; Chevallier et al., 2021), to 3D shapes (Tabia et al.,
2014) to name a few. Given their central roles for medical data where privacy is of the utmost importance
(Lotan et al., 2020; Li et al., 2005), private statistical computations on the SPD manifold are a worthy
endeavour. The SPD manifold can be equipped with different Riemannian metrics that provide elementary
operations such as distance computations. The log-Euclidean metric, originally proposed in (Arsigny et al.,
2006), has numerous advantages over another popular Riemannian metric called the affine invariant metric
(Pennec et al., 2006): (a)it is computationally faster, (b)it gives similar or better performances on several
processing and learning tasks, (c)and quite importantly, it provides a closed form expression for the Fréchet
mean - which otherwise requires solving an optimization problem.
Need for better and faster privacy mechanisms Despite its importance for the processing of a number
of (medical) data, geometric statistics currently stands understudied from the lens of differential privacy. The
very recent work by (Reimherr et al., 2021) provides the first differentially private mechanism for the Fréchet
mean. However, its utility - a measure of the mechanism’s deviation from non-privatized computations -
makes it impracticable on the manifold of SPD matrices as soon as we consider matrices of moderate size,
e.g.20×20matrices. Consequently, there is a need for better and faster privacy mechanisms on manifolds,
starting with the SPD manifold.
1.2 Related Work and Contributions
Reimherretal.(2021)werefirsttoconsiderdifferentialprivacyinmanifoldsettinganddeveloped Riemannian
Laplace mechanism by extending the standard Laplace mechanism (Dwork et al., 2014) for linear spaces to
complete Riemannian manifolds. It is based on a Laplace distribution that was originally proposed for SPD
matrices (Hajri et al., 2016) based on distance of the affine invariant metric (Pennec et al., 2006), which they
generalize to any manifold Mequipped with a distance ρ:
p(x)∝exp/parenleftbigg
−ρ(x,m)
σ/parenrightbigg
,∀x∈M (1)
2Published in Transactions on Machine Learning Research (02/2023)
wherem∈M,σ∈R>0(positive reals) are parameters of the probability density p. Reimherr et al. (2021)
show that the mechanism obtained achieves puredifferential privacy and provides an upper bound for the
expectation of its utility (a measure of the deviation from non-privatized computations) for the Fréchet mean
query. Their method is applicable to various Riemannian manifolds that satisfy some regularity conditions.
Approximate differential privacy relaxes pure differential privacy (see Section 2) but provides significantly
better utility for higher dimensions and is heavily used in real world applications Abadi et al. (2016). In the
Euclideancase, theGaussianmechanism, wherenoiseisaddedfromstandardGaussian, satisfiesapproximate
differential privacy. To this end, we make use of log Gaussian distribution Schwartzman (2016), an intrinsic
distribution on SPD matrices, for deriving approximate differentially private mechanism. This relaxation
helps us obtain better utility compared to Riemannian Laplace mechanism in terms of dimension, similar to
standard Euclidean case. We summarize our contributions are as follows.
Mechanism A DP E[ρ2(f(D),A(D))] Theoretical Results
Riemannian Laplace (Reimherr et al., 2021) Pure DP O(k4) Expectation of ρ2(f(D),A(D))
tangent Gaussian (Ours) Approx. DP O(ln(1/δ)k2)Exact Distribution of ρ2(f(D),A(D))
Table 1: Differences between existing (Reimherr et al., 2021) and proposed mechanisms for private Fréchet
mean queries on the manifold of k×kSPD matrices endowed with the log-Euclidean metric. The notation
ρ2(f(D),A(D))represents the utility with Dthe dataset, Athe mechanism under consideration, ρthe log-
Euclidean distance, fthe Fréchet mean and δquantifies approximate differential privacy.
1. We propose a new and simple mechanism - called the tangent Gaussian Mechanism - that privatizes any
statistical summary on the manifold of Symmetric Positive Definite (SPD) matrices endowed with the
log-Euclidean metric. We prove that it achieves approximate differential privacy (Th. 2).
2. When the statistical summary is the Fréchet mean, we show that our mechanism obtains significant
improvement in terms of utility over recent works - which we demonstrate theoretically, and practically for
data in higher dimensions. Further, our mechanism is computationally efficient and easily implementable.
3. We present the effectiveness of our mechanism on synthetic and real-world (medical) imaging data, the
latter being represented via their covariance descriptors. To this aim, we also prove a theoretical bound
on the radius of log-Euclidean geodesic ball with the covariance descriptor pipeline (Tuzel et al., 2006) -
required for the applicability of our mechanism (Th. 5).
Table 1 highlights the technical differences between (Reimherr et al., 2021) and our work.
2 Preliminaries and Notations
Elements of Riemannian Geometry LetMbe ad-dimensional smooth connected manifold and TpM
be its tangent space at point p∈M. ARiemannian metric gonMis a collection of inner products
gp:TpM×TpM→ Rthat vary smoothly with p. A manifoldMequipped with a Riemannian metric g
is called a Riemannian manifold. Importantly, the metric ggives a distance ρonM. Letγ: [0,1]→M
be a smooth parametrized curve on Mwith velocity vector at tdenoted as ˙γt∈Tγ(t)M. The length of
γis defined as Lγ=/integraltext1
0/radicalbig
gγ(t)( ˙γt,˙γt)dtand the distance ρbetween any two points p,q∈Mis:ρ(p,q) =
infγ:γ(0)=p,γ(1)=qLγ.
If in additionMis complete for ρ, then any two points p,q∈Mcan be joined by length-minimizing curve,
called a geodesic. We refer the reader to (Do Carmo & Flaherty Francis, 1992; Lee, 2006; Helgason, 1979)
for a detailed exposition.
Elements of Differential Privacy (DP) LetXbe an input data space and Mthe manifold under
consideration. Let f:Xn→Mbe a manifold-valued statistical summary that requires privatization with
respect to some sensitive dataset Dof sizen,i.e.D∈Xn. Two datasetsD,D′∈Xnare said to be adjacent
3Published in Transactions on Machine Learning Research (02/2023)
if they differ by at most one data point. We denote adjacency as D∼D′. Thesensitivity of the summary f
with respect to the distance ρonMis defined as:
∆ρ= sup
D∼D′ρ(f(D),f(D′)), (2)
which is the maximum amount of deviation that can occur in the output of ffor adjacent datasets.
Amechanism A:Xn→Mis a randomized algorithm that takes a dataset Das input, and outputs a
privatized version of the summary fonD. The mechanism Asatisfies (ϵ,0)differential privacy (also pure
differential privacy ) if, for all adjacent datasets D∼D′and for all measurable sets SofMthe following
holds:
P[A(D)∈S]≤exp (ϵ)P[A(D′)∈S] (3)
The intuition is that the change of a single element of the data space Xdoes not significantly alter the
output distribution of the mechanism. As a relaxation, the mechanism Asatisfies (ϵ,δ)-differential privacy
(alsoapproximate differential privacy ) if, for all adjacent datasets D∼D′and for all measurable sets Sof
M:
P[A(D)∈S]≤exp (ϵ)P[A(D′)∈S] +δ.
Intuitively, δcan be thought of as the probability of privacy failure, when Eq. equation 3 is not guaranteed.
LetpA(D)be the density of the random variable Y=A(D). Given adjacent datasets D∼D′, theprivacy
loss function ofAis defined as
ℓA,D,D′(y) = ln/parenleftbiggpA(D)(y)
pA(D′)(y)/parenrightbigg
∀y∈M, (4)
and the privacy loss random variable isLA,D,D′=ℓA,D,D′(Y)(Balle & Wang, 2018). Importantly for
our derivations, both sufficient and sufficient &necessary conditions for the mechanism Ato be (ϵ,δ)-
differentially private (DP) can be formulated in terms of LA,D,D′. The sufficient condition writes : ∀D∼
D′:P[LA,D,D′≥ϵ]≤δ=⇒Ais(ϵ,δ)-DP.The sufficient &necessary condition is: ∀D∼D′:P[LA,D,D′≥
ϵ]−exp (ϵ)P[LA,D,D′≤−ϵ]≤δ⇐⇒Ais(ϵ,δ)-DP.
Fréchet Mean When the data space Xis equal to the manifold M, we will be interested in mechanisms
that can privatize a specific statistical summary fcalled the Fréchet mean. The sample Fréchet mean X
(Fréchet, 1948) of the dataset D={X1,...Xn}on the manifoldMis defined as
X≜/braceleftigg
p|p∈arg min
q∈Mn/summationdisplay
i=1ρ2(q,Xi)/bracerightigg
,
i.e.we have in this case X=f(D)forD∈Mn. Intuitively, the Fréchet mean uses a property of the mean
on linear spaces - namely the fact that mean minimizes the sum of squared distances to the data points - as
a definition of mean on manifolds. Crucially, the Fréchet mean depends on the distance ρand therefore on
the Riemannian metric defined on M. We also note that the Fréchet mean might not always exist, and if
it exists it might not be unique – see supplementary materials. In practice, computing Xgenerally requires
optimization algorithms such as gradient descent on manifolds (Boumal, 2020).
3 Geometry of the SPD Manifold with Log Euclidean Metric
Manifold and vector space structures We now restrict Mto be the manifold of symmetric positive
definite (SPD) matrices:
SPD(k) =/braceleftbig
X∈Rk×k|XT=Xand∀u∈Rk\{0},uTXu> 0/bracerightbig
, (5)
which has dimension d=k(k+1)
2. The tangent space of the manifold SPD (k)at any point X∈SPD(k)is the
vector space of symmetric matrices SYM (k). The mathematical construct (SPD(k),+,.)is not a vector space
4Published in Transactions on Machine Learning Research (02/2023)
under element-wise addition and element-wise scalar multiplication. This can be seen from the observation
thata∈R≤0,X∈SPD(k) =⇒aX̸∈SPD(k). Instead, SPD (k)is an open cone of Rk×kand, as such,
naturally possesses a smooth manifold structure which can further be equipped with different Riemannian
metrics (Thanwerdas & Pennec, 2021). However, Arsigny et al.(Arsigny et al., 2007) showed in a surprising
result that SPD (k)can be given a vector space structure (SPD(k),⊕,⊙)via the operations ⊕,⊙defined in
Table 2, where Expm, Logm denote the matrix exponential and matrix logarithm. This fact is central for
the proofs provided in the present paper.
Operation Notation Expression
Addition X1⊕X2Expm [LogmX1+LogmX2]
Subtraction X1⊖X2Expm [LogmX1−LogmX2]
Scalar Multiplication a⊙XExpm [a.LogmX]
Table 2: Operations turning the manifold SPD (k)into a vector space. Expm and Logm denote the matrix
exponential and logarithms, respectively. X1,X2belong to SPD (k)whilea∈Ris a scalar.
Riemannian structure Arsignyet al.further define a Riemannian metric on SPD (k), called the log-
Euclidean metric , which induces the following distance:
ρLE(X1,X2) =∥LogmX1−LogmX2∥F,∀X1,X2∈SPD(k), (6)
where∥.∥Fdenotes the Frobenius norm on matrices. Importantly, the log-Euclidean metric (Arsigny et al.,
2006) gives a unique and simple closed form expression for the Fréchet mean in terms of matrix logarithm
and matrix exponential
XLE=Expm/bracketleftigg
1
nn/summationdisplay
i=1LogmXi/bracketrightigg
,
for the dataset X1,...,Xn∈SPD(k).
Maps between spaces Lastly, we present maps that will help us define the differential privacy mecha-
nism proposed in the next section. Consider the map vecd :SYM (k)→Rk(k+1)
2defined as vecd(X) =/bracketleftbig
diag(X)T,√
2 upperdiag( X)T/bracketrightbigT, where diag :SYM (k)→Rkandupperdiag : SYM (k)→Rk(k−1)
2build
vectors from the diagonal, and from the strictly upper diagonal entries, of the matrix X. The map vecdis
invertible and we denote by invvecdits inverse. Specifically, the spaces SPD (k),SYM (k)andRk(k+1)
2are
now related as follows:
SPD(k) SYM (k) Rk(k+1)
2.Logm
Expmvecd
invvecd
4 Tangent Gaussian Mechanism on SPD manifolds
We can now introduce our differential privacy mechanism for statistical summaries on the SPD (k)manifold.
Letf:Xn→SPD(k)be any SPD (k)-valued summary that needs to be privatized. The proposed mechanism
is based on the log Gaussian distribution on the SPD manifold (Schwartzman, 2016) which is defined as
follows. Consider a mean M∈SPD(k)and a tangent covariance Σ∈SPD/parenleftig
k(k+1)
2/parenrightig
. We can (i) first map
the meanMto the tangent space SYM (k)of SPD (k)at the identity using the matrix Logarithm Logm, then
(ii) to Rk(k+1)
2using the map vecdintroduced in the previous section, and (iii) consider whether the result
follows a traditional Gaussian distribution.
Definition 1 (Log Gaussian Distribution on SPD (k)(Schwartzman, 2016)) .Given a mean M∈SPD(k),
and a tangent covariance Σ∈SPD/parenleftig
k(k+1)
2/parenrightig
, we say that X∼LN (M,Σ)follows a log Gaussian distribution
5Published in Transactions on Machine Learning Research (02/2023)
Algorithm 1: tangent Gaussian Mechanism for f:Xn→SPD(k)
Inputs : DatasetDofk×kSPD matrices of size n, sigma-type∈{’classical’,’analytic’ }, ∆LEthe
log-Euclidean sensitivity of f,ϵ>0,δ∈(0,1)and additionally ϵ<1if sigma-type is
’classical’, the noise calibration subroutines CLASSIC ,ANALYTIC which take ∆LE,ϵ,δ
and provide σ.
Output: Privatef(D)
1ifsigma-type is ’classical’ thenσ=classic (∆LE,ϵ,δ);elseσ=analytic (∆LE,ϵ,δ);
2Compute non private output : fnp:=f(D)
3Compute mean of Gaussian distribution: M:= vecd[Logmfnp],M∈Rk(k+1)
2
4Sample from the Gaussian distribution in Rk(k+1)
2:N∼N(M,σ2I)
5Map sample to the SPD manifold: fp:=Expm [invvecdN]
6Return private fp
onSPD(k)ifvecd[LogmX]∼N (vecd[LogmM],Σ)follows a (regular) Gaussian distribution with mean
vecdLogmMand covariance matrix ΣonRk(k+1)
2.
The density p(X|M,Σ)is then given by
J(X)
(2π)d
2(det Σ)1
2exp/parenleftbigg
−1
2vecd(LogmX−LogmM)TΣ−1vecd(LogmX−LogmM)/parenrightbigg
whered=k(k+1)
2,J(X) =1
detX/producttext
i<jh(λi,λj),andh(λi,λj) =/braceleftigg
(logλi−logλj)λi>λj
1
λiλi=λj, withλi,λj
eigenvalues of the matrix X.
The definition of log Gaussian distribution on the SPD (k)manifold allows us to define our proposed tangent
Gaussian mechanism.
Definition 2 (tangent Gaussian Mechanism) .Consider any statistical summary f:Xn→SPD(k)on
the manifold SPD(k)equipped with log-Euclidean metric. Given σ2>0, we define the tangent Gaussian
mechanism ATG:Xn→SPD(k), as
ATG(D) =X,whereX∼LN (f(D),σ2I).
We now state our main theorem, which shows that the privacy loss of the tangent Gaussian mechanism is
normally distributed with mean and variance parametrized by the log-Euclidean distance. Proof is given in
Appendix A.2
Theorem 1 (Distribution of Privacy Loss for the tangent Gaussian Mechanism) .LetATGbe a tangent
Gaussian mechanism with variance σ2. Its privacy loss is normally distributed as
LATG,D,D′∼N/parenleftbiggρ2
LE(f(D),f(D′))
2σ2,ρ2
LE(f(D),f(D′))
σ2/parenrightbigg
.
This distribution is analogous to the distribution of the privacy loss for the Euclidean Gaussian mechanism,
but with the log-Euclidean sensitivity instead of the Euclidean sensitivity (Dwork et al., 2014; Balle &
Wang, 2018). Consequently, our theoretical analysis of the tangent Gaussian mechanism - deriving privacy
guarantees from the distribution of the privacy loss above - closely follows the steps of the analysis for the
Euclidean Gaussian case. Specifically, we can proceed in two ways with either a (1)classical approach where
sufficient conditions are used to show the mechanism is (ϵ,δ)-DP as in Dwork et al. (2014), or with an (2)
analytic approach where the utility is better by using sufficient and necessary conditions (Balle & Wang,
2018).
Theorem 2 (Privacy Guarantee of tangent Gaussian Mechanism) .Considerf:Xn→SPD(k)with log-
Euclidean sensitivity ∆LE.
6Published in Transactions on Machine Learning Research (02/2023)
1. (Classical) Given ϵ,δ∈(0,1), choosing σ= ∆ LE/radicalbig
2 ln(1.25/δ)/ϵ, makes the tangent Gaussian
mechanism (ϵ,δ)-differentially private.
2. (Analytic) Given ϵ≥0,δ∈(0,1)andΦthe cumulative distribution of the standard Gaussian,
choosing any σthat satisfies Φ(∆LE
2σ−ϵσ
∆LE)−exp(ϵ)Φ(∆LE
2σ−ϵσ
∆LE)≤δmakes the tangent Gaussian
mechanism (ϵ,δ)-differentially private.
Proofs are is given in Appendix A.3. Algorithm 1 shows the implementation of the mechanism.
5 Privatizing the Fréchet mean
In the previous section, fis any function that outputs a summary statistics on SPD (k). In this section, we
seek to privatize the Fréchet mean fof the log-Euclidean metric. We first compute its sensitivity and then
provide its utility. In what follows, Br(M) ={X|ρLE(M,X )<r}denotes an open geodesic ball of radius
0<r<∞centered at M∈SPD(k).
Theorem 3 (Sensitivity of Log-Euclidean Fréchet Mean) .Given data inBr(M)for some 0<r <∞and
M∈SPD(k),the sensitivity of the log-Euclidean Fréchet mean verifies: ∆LE≤2r
n.
Note above theorem can also obtained from (Reimherr et al., 2021, Theorem 2) by setting κ= 0. The utility
of the tangent Gaussian mechanism for a Fréchet mean query is then given below.
Theorem 4 (Utility).LetATGbe the (classical) tangent Gaussian mechanism, Br(M)a geodesic ball of
radius 0<r <∞and center M∈Mcontaining the dataset Dandfthe Fréchet mean. The utility of the
mechanism ATGis given by:
ρ2
LE(f(D),ATG(D)))∼σ2χ2
d,
E[ρ2
LE(f(D),ATG(D))] =4r2ln(1.25/δ)d
n2ϵ2withd=dim(SPD(k)) =k(k+ 1)
2,
whereχ2
drepresents the chi squared distribution with ddegree of freedoms.
Proofs of Th. 3 and Th. 4 are given in Appendix A.4. We compare these results with those of the Riemannian
Laplace mechanism (Reimherr et al., 2021), denoted ARL.
Utility: We compare the utility in terms of size kof spd matrices k×kbecause dependancy
on other factors n,ϵare same. Utility of the Riemannian Laplace mechanism has an expectation
given by E[ρ2
LE(f(D),ARL(D))] =O(k4). By contrast, our tangent Gaussian mechanism provides
E[ρ2
LE(f(D),ATG(D))] =O(ln(1/δ)k2). Hence our mechanism has significantly better utility in terms of
dimension.
Pure DP vs Approx DP : It should be noted that our privacy guarantees are weaker than Riemannian
Laplace. In practice, δis chosen to be cryptographically small and typically δ≪1/nCanonne (2021).
Theoretical Results : The authors of Reimherr et al. (2021) characterize the utility in terms of its expecta-
tionE[ρ2
LE(f(D),A(D))]. By contrast, our results yield a more complete picture, as we derive the probability
distribution of ρ2
LE(f(D),A(D)))given that we are tailoring mechanism for flat geometry of SPD matrices
with log-Euclidean metric.
6 Experiments
We use the Riemannian Laplace mechanism as the baseline and recall that this mechanism uses the Rieman-
nian Laplace distribution equation 25. Efficient sampling from the Riemannian Laplace distribution is only
discussed for (i)SPDManifold with affine-invariant metric and (ii)Hypersphere with Euclidean metric in
Reimherr et al. (2021) and we didn’t find any sampling procedure from this distribution on SPD manifold
with log-Euclidean metric in Reimherr et al. (2021); Hajri et al. (2016) and hence we used MCMC sampling
in our experiments.
7Published in Transactions on Machine Learning Research (02/2023)
10 20 30
k01020304050LE(f(),A())
=0.1
Rie-Laplace
T anG-Classical(Ours)
T anG-Analytic(Ours)
10 20 30
k0510152025
=0.2
Rie-Laplace
T anG-Classical(Ours)
T anG-Analytic(Ours)
10 20 30
k0.02.55.07.510.012.515.017.5
=0.3
Rie-Laplace
T anG-Classical(Ours)
T anG-Analytic(Ours)
10 20 30
k02468101214
=0.4
Rie-Laplace
T anG-Classical(Ours)
T anG-Analytic(Ours)
10 20 30
k01234567LE(f(),A())
=0.1
TanG-Cla(=107)
TanG-Ana(=107)
TanG-Cla(=108)
TanG-Ana(=108)
TanG-Cla(=109)
TanG-Ana(=109)
10 20 30
k0.00.51.01.52.02.53.03.54.0
=0.2
TanG-Cla(=107)
TanG-Ana(=107)
TanG-Cla(=108)
TanG-Ana(=108)
TanG-Cla(=109)
TanG-Ana(=109)
10 20 30
k0.00.51.01.52.02.5
=0.3
TanG-Cla(=107)
TanG-Ana(=107)
TanG-Cla(=108)
TanG-Ana(=108)
TanG-Cla(=109)
TanG-Ana(=109)
10 20 30
k0.000.250.500.751.001.251.501.752.00
=0.4
TanG-Cla(=107)
TanG-Ana(=107)
TanG-Cla(=108)
TanG-Ana(=108)
TanG-Cla(=109)
TanG-Ana(=109)
10 20 30
k0246810E(f(),A())
=0.1
ExtG-Analytic
T anG-Analytic
10 20 30
k02468
=0.2
ExtG-Analytic
T anG-Analytic
10 20 30
k012345678
=0.3
ExtG-Analytic
T anG-Analytic
10 20 30
k012345678
=0.4
ExtG-Analytic
T anG-Analytic
Figure 1: Utilities on synthetic data for Rie-Laplace the Riemannian Laplace mechanism (Reimherr et al.,
2021), and TanG Classical ,TanG-Cla andTanG Analytic ,TanG-Ana our proposed tangent Gaussian mech-
anisms (classical and analytic versions), and ExtG-Analytic the Extrinsic analytic gaussian mechanism for
different matrix sizes kand privacy parameter ϵ.ρLEandρEdenotes log-Euclidean and Euclidean distance
respectively. Note output of extrinsic mechanism is not a SPD matrix and hence deviation is measured in
standard Euclidean distance.
6.1 Experiments on Synthetic Datasets
The utility depends on privacy parameters (ϵ,δ), the sizekof the matrices, the dataset size nandrthe
radius of the geodesic ball containing the dataset. The utilities of the tangent Gaussian and Riemannian
Laplace mechanisms have the same dependency on n,ϵ,r, such that their differentiating parameters are δ,k.
Consequently, our experiments on synthetic data fix n,ϵ,rand varyδ,k.
We also consider Extrinsic approach suggested in Reimherr et al. (2021) where Fréchet mean is seen to be
belongingtoSymmetricmatrixandnoisefromEuclideannormaldistributionisadded, specifically AEX(D) =
X,X∼invvecd/parenleftbig
N/parenleftbig
vecd ¯XLE,σ2I/parenrightbig/parenrightbig
for appropriate σ. Ifris radius of log-Euclidean geodesic ball of data,
extrinsic sensitivity is given by ∆EX= 2(exp (r)−1)/n(Reimherr et al., 2021, Proposition 1). It should be
emphasized that resultant privatized Fréchet mean is no longer a SPDmatrix. Hence Reimherr et al. (2021)
compared deviation between private and non private Fréchet mean in the standard Euclidean norm.
8Published in Transactions on Machine Learning Research (02/2023)
We generate random k×kSPD matrices as follows: (i)generatekreal values (λ1,...,λk)uniformly in
[e−r,er],(ii)buildDthe diagonal matrix with Dii=λi, fori∈{1,...,k},(iii)generate a k×krandom
orthogonalmatrix EwiththeHaardistribution, and (iv)buildtheSPDmatrixas: X=EDET. Thisprocess
generates SPD matrices, that can be shown to belong to the geodesic ball B√
kr(I)withIthe identity matrix:
∥LogmX∥F=/radicalig/summationtextk
i=1(lnλi)2≤√
kr2=√
kr. We usen= 500andr= 1/4in our experiments and hence
∆LE≤√
k/1000.
Fig. 1 (first) compares utilities using a fixed δ= 10−6for our mechanism, a MCMC burn-in of 50,000for the
RiemannianLaplacemechanism, anddifferentvaluesof k∈{2,5,10,15,20,25,30}andϵ∈{0.1,0.2,0.3,0.4}.
Each experiment is repeated 10 times, the results are averaged and the band (µ−2σ,µ+2σ)is shown, where
µandσare the mean and standard deviation, respectively, of the associated result. The σis small for
our mechanism, and does not appear on the plots. The tangent Gaussian mechanisms (ours) yield almost
×10utility improvement for larger k, for eachϵ. Fig. 1 (middle) shows that, as expected, our utility is
not significantly impacted by different values of δ∈{10−7,10−8,10−9}. Fig. 1 (bottom) compares utilities
between Extrinsic Gaussian mechanism (analytic) and tangent Gaussian mechanism (analytic) in Euclidean
distance and shows proposed mechanism is better.
6.2 Experiments on Real-World Datasets
We run experiments on covariance descriptors of real-world images. Covariance descriptors (Tuzel et al.,
2006) have been widely used for face and person recognition (Tuzel et al., 2007; Zhang & Li, 2011; Pang
et al., 2008; Križaj et al., 2013; Ma et al., 2014; Cai et al., 2010; Zeng et al., 2015; Matsukawa et al., 2016),
action and gesture recognition (Cirujeda & Binefa, 2014; Hussein et al., 2013; Sharaf et al., 2015), 3D shape
analysis (Tabia et al., 2014; Ma et al., 2014), medical imaging (Khan et al., 2015; Cirujeda et al., 2016); and
even recently as layers in neural networks (Yu & Salzmann, 2017) - which makes them interesting data to
privatize.
LetI∈Rh×w×cbe an image of height h, widthwand withcchannels, where cis1for gray scale images
and3for RGB images. Let ϕ:Rh×w×c→Rhw×kbe a feature extractor of dimension k, i.e.ϕ(I)(x)is
ak-dimensional vector at each spatial coordinate xin the image’s domain S. Given a small η > 0, the
covariance descriptor Rη:Rh×w×c→SPD(k)associated with ϕis defined as
Rη(I) =/bracketleftigg
1
|S|/summationdisplay
x∈S(ϕ(I)(x)−µ)(ϕ(I)(x)−µ)T/bracketrightigg
+η.I,
whereµ=|S|−1/summationtext
x∈Sϕ(I)(x), andη.Iensures Rη(I)∈SPD(k)withηusuallysetto 10−6. Ourexperiments
follow (Tuzel et al., 2006; Jayasumana et al., 2015) and use the covariance descriptors associated with
the feature vector given as ϕ(I)(x) =/bracketleftig
x,y,I,|Ix|,|Iy|,|Ixx|,|Iyy|,/radicalbig
|Ix|2+|Iy|2,arctan/parenleftig
|I|x
|I|y/parenrightig/bracketrightig
, where
x= (x,y), intensities derivatives are denoted by Ix,Iy,Ixx,Iyyand we added the intensity values Ifor each
channel compared to (Tuzel et al., 2006; Jayasumana et al., 2015). For gray scale images, ϕ(I)(x)is a9-
dimensional vector that makes Rη(I)a9×9SPD matrix, while for RGB images ϕ(I)(x)is a11-dimensional
vector that makes Rη(I)a11×11SPD matrix. We are within the assumptions of Th. 4 since such covariance
descriptors belong to geodesic balls centered at I, as shown by the following theorem.
Theorem 5. LetRη(I)be the covariance descriptor associated with the feature vector ϕ(I)above.
1. IfIis a gray scale image, then ∥Logm Rη(I)∥F≤√
9 max{|lnη|,|ln(14 +η)|}.
2. IfIis a RGB image, then ∥Logm Rη(I)∥F≤√
11 max{|lnη|,|ln(16 +η)|}.
Proof is given in Appenidx A.5
6.2.1 Experiments on medical imaging data
We use images from 4 classes of the medical imaging datasets PATHMNIST (gray scale) and OctoMNIST
(RGB) from MedMNISTv2 (Yang et al., 2021), compute the 4 class-wise Fréchet means of their covariance
9Published in Transactions on Machine Learning Research (02/2023)
0.2 0.4 0.6 0.8
02468101214LE(f(),A())
N=10704
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
02468101214
N=10356
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
02468101214
N=10699
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
02468101214
N=11035
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
0.00.51.01.52.02.53.03.5LE(f(),A())
N=33734
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
02468
N=10463
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
0246810
N=8004
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
0.00.51.01.52.0
N=46276
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
Figure 2: Utilities on the private Fréchet means for different privacy parameters ϵ, and real-world datasets
of sizesN. Top: PathMNIST (RGB images yielding 11×11SPD descriptors). Bottom: OctoMNIST (gray
scale images yielding 9×9SPD matrices). Rie-Laplace is the Riemannian Laplacian mechanism (Reimherr
et al., 2021) and TanGthe tangent Gaussian mechanism for different values of δ(ours). We also show the
(µ−2σ,µ+ 2σ)band.
descriptors ( η= 10−6), which we privatize using the Riemannian Laplace and tangent Gaussian (analytical)
mechanisms. We avoid using extrinsic approach because extrinsic sensitivity is extremely high Fig. 2 shows
the utilities for different values of ϵ∈{0.1,0.3,0.5,0.7,0.9}andδ∈{10−5,10−7,10−9}. The datasets sizes
Nrange from 8000to46276images. The sensitivity of the Fréchet mean, required for the mechanisms,
is calculated using Th. 5 and Th. 3. Each experiment is repeated 10 times and averaged and the band
(µ−2σ,µ+2σ)is shown, where µandσare the mean and standard deviation, respectively, of the associated
result. Our mechanism also outperforms the Riemannian Laplace on real-world datasets, and the utility gap
is higher for smaller values of Nandϵ.
6.2.2 Experiments on standard imaging data
In this section, we perform additional experiments on standard image datasets. We choose MNIST, KMNIST
(Clanuwat et al., 2018) (gray scale images) and CIFAR10, FashionMNIST (Xiao et al., 2017) (RGB images)
as datasets. We extract images from 4 classes for each dataset and compute the corresponding class-wise
Fréchet means of their covariance descriptors (η= 10−6), which we privatize using the Riemannian Laplace
Mechanism (Reimherr et al., 2021) and our proposed mechanism tangent Gaussian (Analytic). Fig. 3 shows
the utilities for different values of ϵ∈{0.1,0.3,0.5,0.7,0.9}andδ∈{10−5,10−7,10−9}. Each experiment is
repeated 10 times, the results are averaged and the band (µ−2σ,µ+ 2σ)is shown, where µandσare the
mean and standard deviation, respectively, of the associated result. Fig. 3 illustrates the better utility of
our mechanism compared to the Riemannian Laplace mechanism.
7 Conclusion and Future Work
Differential privacy for geometric statistics and learning is at a very early stage. We proposed a tangent
Gaussian mechanism that is specific to the SPD manifold equipped with the log-Euclidean metric, and
that outperforms the only existing baseline. One limitation of our work is that the proposed mechanism
is restricted to one manifold with one specific metric. While the log-Euclidean metric is one of the most
10Published in Transactions on Machine Learning Research (02/2023)
0.2 0.4 0.6 0.8
02468101214LE(f(),A())
N=6903
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
024681012
N=7877
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
02468101214
N=6990
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
02468101214
N=7141
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
02468101214LE(f(),A())
N=7000
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
02468101214
N=7000
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
024681012
N=7000
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
02468101214
N=7000
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
05101520LE(f(),A())
N=6000
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
0510152025
N=6000
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
05101520
N=6000
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
05101520
N=6000
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
024681012LE(f(),A())
N=7000
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
024681012
N=7000
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
024681012
N=7000
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
0.2 0.4 0.6 0.8
0246810121416
N=7000
RieLaplace
TanG(=105)
TanG(=107)
TanG(=109)
Figure 3: Utilities on the private Fréchet means for different privacy parameters ϵ, and real-world datasets
of sizesN. First and Second Row: Fréchet mean from MNIST, KMNIST (Gray scale images yielding 9×9
SPD descriptors). Third and Fourth Row: Fréchet mean from CIFAR10, FashionMNIST (RGB images
yielding 11×11SPD descriptor). Rie-Laplace means the Riemannian Laplacian mechanism. TanGmeans
the tangent Gaussian Mechanism for different values of δ(ours). We also show the mean-2 ∗std, mean+2∗std
bands.
important metrics on the SPD manifold, future work should investigate how to build a Gaussian mechanism
that works on any complete Riemannian manifold. We could define such as a mechanism using a Riemannian
Gaussian distribution derived in Pennec (2006). The main challenge would be to show that the associated
procedure is (ϵ,δ)differentially private. Future work can also seek to privatize other geometric statistical
algorithms like geodesic regression or principal geodesic analysis.
11Published in Transactions on Machine Learning Research (02/2023)
References
MartinAbadi, AndyChu, IanGoodfellow, HBrendanMcMahan, IlyaMironov, KunalTalwar, andLiZhang.
Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer
and communications security , pp. 308–318, 2016. 3
Vincent Arsigny, Pierre Fillard, Xavier Pennec, and Nicholas Ayache. Log-euclidean metrics for fast and sim-
plecalculusondiffusiontensors. Magnetic Resonance in Medicine: An Official Journal of the International
Society for Magnetic Resonance in Medicine , 56(2):411–421, 2006. 2, 5
Vincent Arsigny, Pierre Fillard, Xavier Pennec, and Nicholas Ayache. Geometric means in a novel vector
spacestructureonsymmetricpositive-definitematrices. SIAM journal on matrix analysis and applications ,
29(1):328–347, 2007. 5
Borja Balle and Yu-Xiang Wang. Improving the gaussian mechanism for differential privacy: Analytical
calibration and optimal denoising. In International Conference on Machine Learning , pp. 394–403. PMLR,
2018. 4, 6, 19
P. J. Basser, J. Mattiello, and D. Le Bihan. MR diffusion tensor spectroscopy and imaging. Biophysical
Journal, 66(1):259–67. URL https://hal.archives-ouvertes.fr/hal-00349721 . 2
Nicolas Boumal. An introduction to optimization on smooth manifolds. Available online, May , 3, 2020. 4
Yinghao Cai, Valtteri Takala, and Matti Pietikainen. Matching groups of people by covariance descriptor.
In2010 20th International Conference on Pattern Recognition , pp. 2744–2747. IEEE, 2010. 9
Clément Canonne. What is delta, and what difference does it make? DifferentialPrivacy.org, 03 2021.
https://differentialprivacy.org/flavoursofdelta/ . 7
Rui Caseiro, Joao F Henriques, Pedro Martins, and Jorge Batista. Semi-intrinsic mean shift on riemannian
manifolds. In European conference on computer vision , pp. 342–355. Springer, 2012. 2
Sylvain Chevallier, Emmanuel K Kalunga, Quentin Barthélemy, and Eric Monacelli. Review of riemannian
distances and divergences, applied to ssvep-based bci. Neuroinformatics , 19(1):93–106, 2021. 2
Pol Cirujeda and Xavier Binefa. 4dcov: A nested covariance descriptor of spatio-temporal features for
gesture recognition in depth sequences. In 2014 2nd International Conference on 3D Vision , volume 1,
pp. 657–664. IEEE, 2014. 9
Pol Cirujeda, Yashin Dicente Cid, Henning Müller, Daniel Rubin, Todd A Aguilera, Billy W Loo, Maximilian
Diehn, Xavier Binefa, and Adrien Depeursinge. A 3-d riesz-covariance texture model for prediction of
nodule recurrence in lung ct. IEEE transactions on medical imaging , 35(12):2620–2630, 2016. 9
Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha.
Deep learning for classical japanese literature. arXiv preprint arXiv:1812.01718 , 2018. 10
Manfredo Perdigao Do Carmo and J Flaherty Francis. Riemannian geometry , volume 6. Springer, 1992. 3
Cynthia Dwork. Differential privacy. In Michele Bugliesi, Bart Preneel, Vladimiro Sassone, and Ingo We-
gener (eds.), Automata, Languages and Programming , pp. 1–12, Berlin, Heidelberg, 2006. Springer Berlin
Heidelberg. ISBN 978-3-540-35908-1. 1
Cynthia Dwork. Differential privacy: A survey of results. In International conference on theory and appli-
cations of models of computation , pp. 1–19. Springer, 2008. 1
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private
data analysis. In Theory of cryptography conference , pp. 265–284. Springer, 2006. 1
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Found. Trends Theor.
Comput. Sci. , 9(3-4):211–407, 2014. 1, 2, 6, 19
12Published in Transactions on Machine Learning Research (02/2023)
P Thomas Fletcher, Conglin Lu, Stephen M Pizer, and Sarang Joshi. Principal geodesic analysis for the
study of nonlinear statistics of shape. IEEE transactions on medical imaging , 23(8):995–1005, 2004. 2
Thomas Fletcher. Geodesic regression on riemannian manifolds. In Proceedings of the Third International
Workshop on Mathematical Foundations of Computational Anatomy-Geometrical and Statistical Methods
for Modelling Biological Shape Variability , pp. 75–86, 2011. 2
Maurice Fréchet. Les éléments aléatoires de nature quelconque dans un espace distancié. In Annales de
l’institut Henri Poincaré , volume 10, pp. 215–310, 1948. 2, 4
Hatem Hajri, Ioana Ilea, Salem Said, Lionel Bombrun, and Yannick Berthoumieu. Riemannian laplace
distribution on the space of symmetric positive definite matrices. Entropy, 18(3):98, 2016. 2, 7
Sigurdur Helgason. Differential geometry, Lie groups, and symmetric spaces . Academic press, 1979. 3
Stephan Huckemann, Thomas Hotz, and Axel Munk. Intrinsic shape analysis: Geodesic pca for riemannian
manifolds modulo isometric lie group actions. Statistica Sinica , pp. 1–58, 2010. 2
Mohamed E Hussein, Marwan Torki, Mohammad A Gowayyed, and Motaz El-Saban. Human action recog-
nition using a temporal hierarchy of covariance descriptors on 3d joint locations. In Twenty-third inter-
national joint conference on artificial intelligence , 2013. 9
Sadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li, and Mehrtash Harandi. Kernel
methods on riemannian manifolds with gaussian rbf kernels. IEEE transactions on pattern analysis and
machine intelligence , 37(12):2464–2477, 2015. 9
Adnan Mujahid Khan, Korsuk Sirinukunwattana, and Nasir Rajpoot. A global covariance descriptor for
nuclear atypia scoring in breast histopathology images. IEEE journal of biomedical and health informatics ,
19(5):1637–1647, 2015. 9
Janez Križaj, Vitomir Štruc, and Simon Dobrišek. Combining 3d face representations using region covariance
descriptorsandstatisticalmodels. In 2013 10th IEEE international conference and workshops on automatic
face and gesture recognition (FG) , pp. 1–7. IEEE, 2013. 9
John M Lee. Riemannian manifolds: an introduction to curvature , volume 176. Springer Science & Business
Media, 2006. 3
Mingyan Li, Radha Poovendran, and Sreeram Narayanan. Protecting patient privacy against unauthorized
release of medical images in a group communication environment. Computerized Medical Imaging and
Graphics , 29(5):367–383, 2005. 2
Eyal Lotan, Charlotte Tschider, Daniel K Sodickson, Arthur L Caplan, Mary Bruno, Ben Zhang, and
Yvonne W Lui. Medical imaging and privacy in the era of artificial intelligence: myth, fallacy, and the
future.Journal of the American College of Radiology , 17(9):1159–1162, 2020. 2
Bingpeng Ma, Yu Su, and Frederic Jurie. Covariance descriptor based on bio-inspired features for person
re-identification and face verification. Image and Vision Computing , 32(6-7):379–390, 2014. 9
Tetsu Matsukawa, Takahiro Okabe, Einoshin Suzuki, and Yoichi Sato. Hierarchical gaussian descriptor for
person re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pp. 1363–1372, 2016. 9
Nina Miolane. Geometric statistics for computational anatomy . PhD thesis, Université Côte d’Azur, 2016. 2
Nina Miolane, Nicolas Guigui, Alice Le Brigant, Johan Mathe, Benjamin Hou, Yann Thanwerdas, Stefan
Heyder, Olivier Peltre, Niklas Koep, Hadi Zaatiti, et al. Geomstats: a python package for riemannian
geometry in machine learning. Journal of Machine Learning Research , 21(223):1–9, 2020. 24
NinaMiolane, MatteoCaorsi, UmbertoLupo, MariusGuerard, NicolasGuigui, JohanMathe, YannCabanes,
Wojciech Reise, Thomas Davies, António Leitão, et al. Iclr 2021 challenge for computational geometry &
topology: Design and results. arXiv preprint arXiv:2108.09810 , 2021. 2
13Published in Transactions on Machine Learning Research (02/2023)
Adele Myers, Saiteja Utpala, Shubham Talbar, Sophia Sanborn, Christian Shewmake, Claire Donnat, Johan
Mathe, Rishi Sonthalia, Xinyue Cui, Tom Szwagier, et al. Iclr 2022 challenge for computational geometry
& topology: Design and results. In Topological, Algebraic and Geometric Learning Workshops 2022 , pp.
269–276. PMLR, 2022. 2
Yanwei Pang, Yuan Yuan, and Xuelong Li. Gabor-based region covariance matrices for face recognition.
IEEE Transactions on circuits and systems for video technology , 18(7):989–993, 2008. 9
XavierPennec. Intrinsicstatisticsonriemannianmanifolds: Basictoolsforgeometricmeasurements. Journal
of Mathematical Imaging and Vision , 25(1):127–154, 2006. 11
Xavier Pennec, Pierre Fillard, and Nicholas Ayache. A riemannian framework for tensor computing. Inter-
national Journal of computer vision , 66(1):41–66, 2006. 2
Xavier Pennec, Stefan Sommer, and Tom Fletcher. Riemannian geometric statistics in medical image anal-
ysis. Academic Press, 2019. 2
Matthew Reimherr, Karthik Bharath, and Carlos Soto. Differential privacy over riemannian manifolds.
Advances in Neural Information Processing Systems , 34, 2021. 2, 3, 7, 8, 10, 24
ChristianPRobert, GeorgeCasella, andGeorgeCasella. Monte Carlo statistical methods , volume2. Springer,
1999. 24
ArminSchwartzman. Lognormaldistributionsandgeometricaveragesofsymmetricpositivedefinitematrices.
International Statistical Review , 84(3):456–486, 2016. 3, 5
Amr Sharaf, Marwan Torki, Mohamed E Hussein, and Motaz El-Saban. Real-time multi-scale action detec-
tion from 3d skeleton data. In 2015 IEEE Winter Conference on Applications of Computer Vision , pp.
998–1005. IEEE, 2015. 9
Oleg Smirnov. Tensorflow riemopt: a library for optimization on riemannian manifolds. arXiv preprint
arXiv:2105.13921 , 2021. 24
StefanSommer,FrançoisLauze,SørenHauberg,andMadsNielsen. Manifoldvaluedstatistics,exactprincipal
geodesic analysis and the effect of linear approximations. In European conference on computer vision , pp.
43–56. Springer, 2010. 2
Raghav Subbarao and Peter Meer. Nonlinear mean shift over riemannian manifolds. International journal
of computer vision , 84(1):1–20, 2009. 2
Hedi Tabia, Hamid Laga, David Picard, and Philippe-Henri Gosselin. Covariance descriptors for 3d shape
matching and retrieval. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pp. 4185–4192, 2014. 2, 9
Yann Thanwerdas and Xavier Pennec. O (n)-invariant riemannian metrics on spd matrices. arXiv preprint
arXiv:2109.05768 , 2021. 5
P Thomas Fletcher. Geodesic regression and the theory of least squares on riemannian manifolds. Interna-
tional journal of computer vision , 105(2):171–185, 2013. 2
Oncel Tuzel, Fatih Porikli, and Peter Meer. Region covariance: A fast descriptor for detection and classifi-
cation. In European conference on computer vision , pp. 589–600. Springer, 2006. 3, 9
Oncel Tuzel, Fatih Porikli, and Peter Meer. Human detection via classification on riemannian manifolds. In
2007 IEEE Conference on Computer Vision and Pattern Recognition , pp. 1–8. IEEE, 2007. 9
Saiteja Utpala, Andi Han, Pratik Jawanpuria, and Bamdev Mishra. Rieoptax: Riemannian optimization in
jax. InOPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop) . 24
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747 , 2017. 10
14Published in Transactions on Machine Learning Research (02/2023)
Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing
Ni. Medmnist v2: A large-scale lightweight benchmark for 2d and 3d biomedical image classification.
arXiv preprint arXiv:2110.14795 , 2021. 9
Florian Yger, Maxime Berar, and Fabien Lotte. Riemannian approaches in brain-computer interfaces: a
review.IEEE Transactions on Neural Systems and Rehabilitation Engineering , 25(10):1753–1762, 2016. 2
Kaicheng Yu and Mathieu Salzmann. Second-order convolutional neural networks. arXiv preprint
arXiv:1703.06817 , 2017. 9
Paolo Zanini, Marco Congedo, Christian Jutten, Salem Said, and Yannick Berthoumieu. Transfer learning:
A riemannian geometry framework with applications to brain–computer interfaces. IEEE Transactions on
Biomedical Engineering , 65(5):1107–1116, 2017. 2
Mingyong Zeng, Zemin Wu, Chang Tian, Lei Zhang, and Lei Hu. Efficient person re-identification by hybrid
spatiogram and covariance descriptor. In Proceedings of the IEEE conference on computer vision and
pattern recognition workshops , pp. 48–56, 2015. 9
Ying Zhang and Shutao Li. Gabor-lbp based region covariance descriptor for person re-identification. In
2011 Sixth International Conference on Image and Graphics , pp. 368–371. IEEE, 2011. 9
A Proofs
Considerk∈N∗. In this supplementary material, ∥.∥L2and⟨,⟩2denote the standard Euclidean inner
product and the Euclidean norm on vectors. i.e., for all x,y∈Rk
⟨x,y⟩L2=p/summationdisplay
i=1xiyi.
∥x∥L2=/radicalbig
⟨x,x⟩L2.
Then,⟨,⟩F,∥.∥Fdenotes Frobenius inner product and Frobenius norm respectively, i.e., given A,B∈Rk×k
⟨A,B⟩F=Tr[ATB].
∥A∥F=/radicalbig
⟨A,A⟩F.
Lastly,∥.∥2denotes the spectral norm of matrices. i.e., for all A∈Rk×k
∥A∥2= sup
∥x∥L2̸=0∥Ax∥L2
∥x∥L2.
A.1 Useful Lemmas
In this section, we derive the distribution of the privacy loss. Its proof requires us to first introduce the
following definitions.
Definition 3 (Diffeomorphism and Isometry) .A diffeomorphism between two manifolds M1andM2is
an invertible smooth function whose inverse is also smooth. A diffeomorphism ϕbetween two Riemannian
manifolds (M1,g1),(M2,g2)is called an isometry if it preserves distances i.e., ρg1(p,q) =ρg2(ϕ(p),ϕ(q))
for allp,q∈M 1.
Note that Logm is a diffeomorphism from SPD (k)to SYM (k)andvecdis a diffeomorphism from SYM (k)
toRk(k+1)
2, making vecdLogm a diffeomorphism from SPD (k)toRk(k+1)
2. Importantly for our derivations in
the proofs of this Subsection, the operation vecdLogm preserves the distances – making it an isometry.
15Published in Transactions on Machine Learning Research (02/2023)
Lemma A.1 (vecdLogm is an isometry) .LetLogm :SPD(k)→SYM (k)be the matrix logarithm and
letvecd :SYM (k)→Rk(k+1)
2be defined as vecd(X) =/bracketleftbig
diag(X)T,√
2 upperdiag( X)T/bracketrightbigT. Then vecdLogm :
SPD(k)→Rk(k+1)
2is an isometry from SPD(k)equipped with the log-Euclidean metric to standard Euclidean
space Rk(k+1)
2with standard L2metric, i.e.,
ρLE(X1,X2) =ρL2(vecdLogmX1,vecdLogmX2), (7)
whereX1,X2∈SPD(k). Hence we have that
∥LogmX∥F=∥vecdLogmX∥L2. (8)
Proof.LetX1,X2be elements of SPD (k). We have:
ρ2
LE(X1,X2)
=∥LogmX1−LogmX2∥2
F
=k/summationdisplay
i,j(LogmX1−LogmX2)2
ij
=k/summationdisplay
i<j(LogmX1−LogmX2)2
ij+k/summationdisplay
i>j(LogmX1−LogmX2)2
ij+k/summationdisplay
i=j(LogmX1−LogmX2)2
ij
= 2.k/summationdisplay
i<j(LogmX1−LogmX2)2
ij+k/summationdisplay
i=j(LogmX1−LogmX2)2
ij
=/vextenddouble/vextenddouble/vextenddouble√
2 upperdiag( LogmX1−LogmX2)/vextenddouble/vextenddouble/vextenddouble2
L2+∥diag (LogmX1−LogmX2)∥2
L2
=∥vecd(LogmX1−LogmX2)∥2
L2
=∥vecdLogmX1−vecdLogmX2∥2
L2
=ρ2
L2(vecdLogmX1,vecdLogmX2).
from which we have Eq. equation 7. Eq. equation 8 follows as
∥LogmX∥F=ρLE(X,I) =ρL2(vecdLogmX,vecdLogmI) =∥vecdLogmX∥L2.
Now, we prove some useful properties of the Log Gaussian distribution, denoted LN, that we will use later.
Essentially, we show that the Log Gaussian distribution behaves “nicely" with vector space structure of
SPD(k). We recall that the vector space operations on the SPD manifold are defined as follows,
X1⊕X2=Expm [LogmX1+LogmX2]. (9)
X1⊖X2=Expm [LogmX1−LogmX2]. (10)
Lemma A.2. Takek∈N. LetIdenote the k×kidentity matrix, and consider M,C∈SPD(k),Σ∈
SPD(k(k+1)
2)andχ2
dthe chi-square distribution with ddegrees of freedom. Then:
X∼LN (I,Σ) =⇒X⊕M∼LN (M,Σ). (11)
X∼LN (I,σ2I) =⇒ ⟨LogmC,LogmX⟩F∼N(0,σ2∥LogmC∥2
F). (12)
X∼LN (I,σ2I) =⇒ ∥LogmX∥2
F∼σ2χ2
k(k+1)
2. (13)
16Published in Transactions on Machine Learning Research (02/2023)
Proof.We first recall standard properties of multivariate normal distribution. Let m,a∈RpandΣ,I∈Rp×p
then following properties hold true.
x∼N(m,Σ) =⇒a+x∼N(a+m,Σ). (14)
x∼N(m,Σ) =⇒aTx∼N(aTm,aTΣa). (15)
x∼N(0,σ2I) =⇒ ∥x∥2
L2∼σ2χ2
p. (16)
whereχ2denotes chi-square distribution. We prove the properties (a)-(c) below.
(a) Distribution of X⊕M.
vecd[Logm [X⊕M]](∗)= vecd[Logm [Expm [logX+ logM]]]
= vecd[LogmX+LogmM]
= vecd[LogmX] + vecd[LogmM]
(∗∗)∼ N (vecd[LogmM],Σ).
where in (∗)we used Eq. 9 and in (∗∗)Eq. equation 14.
(b) Distribution of ⟨LogmC,LogmX⟩F.
⟨LogmC,LogmX⟩F(∗)=⟨vecd[LogmC],vecd[LogmX]⟩L2
(∗∗)∼ N/parenleftbig
⟨vecd[LogmC],0⟩L2,vecd[LogmC]Tσ2Ivecd[LogmC]/parenrightbig
∼N(0,σ2∥vecd[LogmC]∥2
L2)
(∗)∼N (0,σ2∥LogmC∥2
F).
where we used Eq. 8 in (∗)and Eq. 15 in (∗∗).
(c) Distribution of ∥LogmX∥2
F.
∥LogmX∥2
F(∗)=∥vecd[LogmX]∥2
L2(∗∗)∼σ2χ2
k(k+1)
2.
where we used Eq. 8 in (∗)and Eq. 16 in (∗∗)withp=k(k+1)
2.
As corollary, we give equivalent reformulation of tangent Gaussian mechanism that will useful in the rest of
the proofs.
CorollaryA.3 (EquivalentReformulationoftangentGaussian) .LetATGbe a tangent Gaussian mechanism
defined as ATG(f(D)) =X, X∼LN (f(D),σ2I). Then, it is equivalently defined as:
ATG(f(D)) =f(D)⊕N,N∼LN (I,σ2I).
Proof.The proof comes from Eq. 11 of Lemma A.2.
Now, we are ready to prove the distribution of the privacy loss of the tangent Gaussian Mechanism which is
given Th. 1.
17Published in Transactions on Machine Learning Research (02/2023)
A.2 Proof of Th. 1
Theorem A.4 (Distribution of the privacy loss of the tangent Gaussian) .LetATGbe a tangent Gaussian
mechanism with variance σ2. Its privacy loss is normally distributed as
LATG,D,D′∼N/parenleftbiggρ2
LE(f(D),f(D′))
2σ2,ρ2
LE(f(D),f(D′))
σ2/parenrightbigg
.
Proof.Assume thatD,D′are adjacent datasets. Let V=f(D)⊖f(D′). Consider the privacy loss random
variableLATG,D,D′. LetY=ATG(D).
ln/parenleftbiggpATG(D)(Y)
pATG(D′)(Y)/parenrightbigg
(1)= ln/parenleftbiggpATG(D)(f(D)⊕N)
pATG(D′)(f(D)⊕N)/parenrightbigg
(2)=−1
2/bracketleftig
vecd/parenleftig
Logm (f(D)⊕N)−Logmf(D)/parenrightig/bracketrightigTI
σ2vecd/parenleftig
Logm (f(D)⊕N)−Logmf(D)/parenrightig
+1
2/bracketleftig
vecd/parenleftig
Logm (f(D)⊕N)−Logmf(D′)/parenrightig/bracketrightigTI
σ2vecd/parenleftig
Logm (f(D)⊕N)−Logmf(D′)/parenrightig
(3)=−1
2σ2/vextenddouble/vextenddouble/vextenddoublevecd/parenleftig
LogmN/parenrightig/vextenddouble/vextenddouble/vextenddouble2
L2+1
2σ2/vextenddouble/vextenddouble/vextenddoublevecd/parenleftig
Logmf(D)−Logmf(D′) +LogmN/parenrightig/vextenddouble/vextenddouble/vextenddouble2
L2
(4)=−1
2σ2/vextenddouble/vextenddouble/vextenddoublevecd/parenleftig
LogmN/parenrightig/vextenddouble/vextenddouble/vextenddouble2
L2+1
2σ2/vextenddouble/vextenddouble/vextenddoublevecd/parenleftig
Logm (V⊕N)/parenrightig/vextenddouble/vextenddouble/vextenddouble2
L2
(5)=1
2σ2/bracketleftig
∥Logm (V⊕N)∥2
F−∥LogmN∥2
F/bracketrightig
=1
2σ2/bracketleftig
∥LogmV∥2
F+ 2⟨LogmV,LogmN⟩F/bracketrightig
(6)∼1
2σ2/bracketleftig
∥LogmV∥2
F+ 2N/parenleftig
0,σ2∥LogmV∥2
F/parenrightig/bracketrightig
(7)∼N/parenleftigg
∥LogmV∥2
F
2σ2,∥LogmV∥2
F
σ2/parenrightigg
(8)∼N/parenleftbiggρ2
LE(f(D),f(D′))
2σ2,ρ2
LE(f(D),f(D′))
σ2/parenrightbigg
,
where we used following properties in each of the steps labeled above.
1. Equivalent reformulation of tangent Gaussian, Corollary. A.3.
2. Density of Log Gaussian Distribution.
3.f(D)⊕N=Expm [Logmf(D) +LogmN].
4. Logm (V⊕N) =Logmf(D)−Logmf(D′) +LogmN.
5. Isometry of the vecdoperation, Eq.8
6. Eq. 12 in Lemma. A.2.
7. standard Gaussian property, see Eq. 14.
8.∥LogmV∥2
F=∥Logmf(D)−Logmf(D′)∥2
F=ρ2
LE(f(D),f(D′)).
18Published in Transactions on Machine Learning Research (02/2023)
A.3 Proof of Th. 2
In this section we give proof of privacy guarantee of the tangent Gaussian Mechanism.
Proof.The proof proceeds similarly to the proofs referenced below, by only replacing the standard sensitivity
∆L2with respect to the Euclidean L2metric, by ∆LE:
1. (Classical). See Th. A.1 (Appendix A Page 261) in Dwork et al. (2014).
2. (Analytic). See Th. 5, Th. 8, Th. 9 (Section 3) in Balle & Wang (2018).
The fact that mechanism is manifold valued comes into play while deriving privacy loss (Taken care by
Theorem 1). Once privacy loss (which is real valued scalar random variable) is derived, going from privacy
loss to actual privacy guarantee wouldn’t be affected whether a mechanism is manifold-valued or not because
both of the above proofs entirelyrely on properties of one-dimensional euclidean Gaussian random variables.
Specifically,
1. (Classical). Directly employs tail bound of one-dimensional Gaussian variable that P[x > t ]<
σ
πexp(−t2
2σ2)
2. (Analytic). The method employs both the sufficient and necessary conditions of the (ϵ,δ)guarantee.
Additionally, the algorithm avoids using tail bounds, since they may be loose, instead uses properties
of Gaussian CDFs and employs binary search to solve analytically for σ, given (ϵ,δ). See (Balle &
Wang, 2018, Algorithm 1) and discussion therein for more details.
A.4 Proof of Th. 3 and Th. 4
In this section, we prove the sensitivity of the Fréchet Mean in Theorem. 3 and then the utility of the tangent
Gaussian Mechanism in Theorem. 4. First we give the proof of 3.
Proof.Considerk∈N,0< r <∞andM∈SPD(k)such thatBr(M)is a geodesic ball of radius rand
centerM. LetD∼D′be adjacent datasets of size n∈Nthat lie inBr(M). Without loss of generality,
we can assume that they differ only by their last data point XnandX′
n:D={X1,X2,...,Xn}and
D′={X1,X2,...,X′
n}. LetXD,XD′denote the Fréchet means of DandD′for the log-Euclidean metric,
which can be expressed in closed forms as mentioned in the main text. The log-Euclidean distance between
the Fréchet means writes:
ρLE(XD,XD′)
(∗)=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleLogm/parenleftigg
Expm/parenleftiggn/summationdisplay
i=1LogmXi
n/parenrightigg/parenrightigg
−Logm/parenleftigg
Expm/parenleftiggn−1/summationdisplay
i=1LogmXi
n+LogmX′
n
n/parenrightigg/parenrightigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F
=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn−1/summationdisplay
i=1LogmXi−1
nn−1/summationdisplay
i=1LogmXi+1
nLogmXn−1
nLogmX′
n/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F
=1
n∥LogmXn−LogmX′
n∥F
=1
nρLE(Xn,X′
n).
19Published in Transactions on Machine Learning Research (02/2023)
∆LE= sup
D∼D′ρLE(XD,XD′) = sup
D∼D′1
nρLE(Xn,X′
n)(†)
≤1
n[ρLE(Xn,M) +ρLE(M,X′
n)](‡)
≤2r
n,
where we use the closed form for the log-Euclidean Fréchet means in (∗), the triangle inequality in (†)and
assumption that data lies in Br(M)in(‡).
Proof of Th. 4 is given as follows,
Proof.Consider deviation ρ2
LE(f(D),ATG(D)))
ρ2
LE(f(D),ATG(D))) =∥Logmf(D)−LogmATG(D)∥2
F(1)=∥Logmf(D)−Logm (f(D)⊕N)∥2
F
(2)=∥LogmN∥2
F
(3)∼σ2χ2
d,
where we use the following properties at each step:
(1) Corollary. A.3.
(2)f(D)⊕N=Expm [Logmf(D) +LogmN].
(3) Eq. 13 of Lemma. A.2.
Now we derive expression for E[ρ2
LE(f(D),ATG(D))]
E[ρ2
LE(f(D),ATG(D))](1)=σ2d
(2)=2∆2
LEln(1.25/δ)d
ϵ2
(3)
≤8r2ln(1.25/δ)d
n2ϵ2.
where we use following properties at each step:
1.c∼χ2
d=⇒E[c] =di.e., expectation of chi squared distributed random variable is number of
degrees of freedom.
2.σ= ∆ LE/radicalbig
2 ln(1.25/δ)/ϵfor(ϵ,δ)-ATGfrom Th. 2.
3.∆LE≤2r
nfrom Th. 3.
A.5 Proof of Theorem 5
In this section we derive log-Euclidean geodesic radius of covariance descriptors. We first prove following
lemma that relates ||LogmX||Fin terms of lower bound on least eigenvalue and upper bound on largest
eigenvalue of X.
Lemma A.5. IfX∈SPD(k)and letλmin(X),λmax(X)be the minimum and maximum eigenvalues of X.
Ifℓ≤λmin(X)andλmax(X)≤LThen,∥LogmX∥F≤√
kmax{|lnℓ|,|lnL|}.
20Published in Transactions on Machine Learning Research (02/2023)
Proof.Consider,
∥LogmX∥F(†)
≤√
k∥LogmX∥2
=√
knmax
i=1|lnλi|
=√
kmax/braceleftig
|n
min
i=1lnλi|,|nmax
i=1lnλi|/bracerightig
(‡)=√
kmax/braceleftig
|lnn
min
i=1λi|,|lnnmax
i=1λi|/bracerightig
=√
kmax{|lnλmin|,|lnλmax|}. (17)
where (†)uses the fact that A∈Rk×k,∥A∥F≤√
k∥A∥2and(‡)uses the fact that lnis monotonically
increasing. Now, we split the derivation into two cases.
1.Caseλmin(X)≥1. Forx≥1,|lnx|is an increasing function, which gives us: |lnℓ| ≤
|lnλmin(X)|≤|lnλmax|≤|lnL|
√
kmax{|lnλmin|,|lnλmax|}≤√
k|lnL|=√
kmax{|lnℓ|,|lnL|}. (18)
2.Caseλmin(X)<1. Forx <1,|lnx|is a decreasing function: |lnλmin|≤|lnℓ|.We further split
the derivation into two sub-cases here
(a)Sub-caseλmax≥1.In this sub-case|lnλmax|≤|lnL|andlnλmin≤|lnℓ|from which we have
that
√
kmax{|lnλmin|,|lnλmax|}≤√
kmax{|lnℓ|,|lnL|}. (19)
(b)Sub-caseλmax<1.In this sub-case|lnL|≤|lnλmax|≤|lnλmin|≤|lnℓ|.
√
kmax{|lnλmin|,|lnλmax|}≤√
k|lnℓ|=√
kmax{|lnℓ|,|lnL|}. (20)
Based on Eq. 18, Eq. 19, Eq. 20 and Eq.17. We can conclude the lemma.
Lemma A.6. LetRη(I)denote the covariance descriptor for image Ifor givenη>0, which is defined as
follows ,
Rη(I) =/bracketleftigg
1
|S|/summationdisplay
x∈S(ϕ(I)(x)−µ)(ϕ(I)(x)−µ)T/bracketrightigg
+η.I,
with,
ϕ(I) =/bracketleftbigg
x,y,I,|Ix|,|Iy|,|Ixx|,|Iyy|,/radicalig
|Ix|2+|Iy|2,arctan/parenleftbigg|Ix|
|Iy|/parenrightbigg/bracketrightbigg
.
wherex,yare grid positions of Image I,Idenote pixel intensity values , |Ix|,|Iy|denotes first order intensity
derivatives and|Ixx|,|Iyy|denotes the second order intensity derivatives then following holds,
1. IfIis grayscale image, then ∥Rη(I)∥2≤12 +η.
2. IfIis RGB image then ∥Rη(I)∥2≤14 +η.
21Published in Transactions on Machine Learning Research (02/2023)
Proof.We have:
∥Rη(I)∥2=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/bracketleftigg
1
|S|/summationdisplay
x∈S(ϕ(I)(x)−µ)(ϕ(I)(x)−µ)T/bracketrightigg
+η.I/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
(1)
≤1
|S|/summationdisplay
x∈S/vextenddouble/vextenddouble(ϕ(I)(x)−µ)(ϕ(I)(x)−µ)T/vextenddouble/vextenddouble
2+∥η.I∥2
≤max
x∈S/vextenddouble/vextenddouble(ϕ(I)(x)−µ)(ϕ(I)(x)−µ)T/vextenddouble/vextenddouble
2+η
(2)= max
x∈S∥(ϕ(I)(x)−µ)∥2
L2+η
(3)
≤max
x∈S∥ϕ(I)(x)∥2
L2+η, (21)
where we used following properties in each of the steps:
1. Triangle Inequality.
2. For alla∈Rp, the spectral norm of 1-rank matrix aaTis∥a∥2
L2.
3. Consider the descriptor ϕ(I) =/bracketleftig
x,y,I,|Ix|,|Iy|,|Ixx|,|Iyy|,/radicalbig
|Ix|2+|Iy|2,arctan/parenleftig
|Ix|
|Iy|/parenrightig/bracketrightig
. Then,
ϕ(I)(x)i≥0for eachx∈Sandi∈{1,...,k}. This yields: (µ)i=/parenleftbig
|S|−1/summationtext
x∈Sϕ(I)(x)/parenrightbig
i≥0.
Hence it implies that ∥ϕ(I)(x)−µ∥2
L2≤∥ϕ(I)(x)∥2
L2.
Then, the following calculations provide an upper bound for ∥ϕ(I)(x)∥2
2. Specifically, we bound each of the
6 elements constituting the descriptor ϕ(I) =/bracketleftig
x,y,I,|Ix|,|Iy|,|Ixx|,|Iyy|,/radicalbig
|Ix|2+|Iy|2,arctan/parenleftig
|Ix|
|Iy|/parenrightig/bracketrightig
.
1. Normalized grid positions : ∀x∈S,0≤x,y≤1.
2. Pixel intensity values Cifori∈[c]:∀x∈S,0≤Ci[x]≤1.
3. Firstintensityderivatives |Ix|,|Iy|: Thefirstintensityderivativescanbeobtainedbytheconvolution
operation (denoted as ⋆):
Ix=I⋆1
4
+1 0−1
+2 0−2
+1 0−1
,Iy=I⋆1
4
+1 +2 +1
0 0 0
−1−2−1
.
Since 0≤I(x)≤1, using the definition of the convolution operation yields ∀x∈S,|Ix(x)|≤1,
|Iy(x)|≤1.
4. Second intensity derivatives |Ixx|,|Iyy|: The second intensity derivatives can be obtained by the
convolution operation (denoted as ⋆)
Ixx=I⋆1
32
+1 0−2 0 1
+4 0−8 0 4
+6 0−12 0 6
+4 0−8 0 4
+1 0−2 0 1
,Iyy=I⋆1
32
+1 +4 +6 +4 +1
0 0 0 0 0
−2−8−12−8−2
0 0 0 0 0
+1 +4 +6 +4 +1
.
Since 0≤I(x)≤1, using the definition of the convolution operation yields |Ixx(x)|≤1,|Iyy(x)|≤
1.
22Published in Transactions on Machine Learning Research (02/2023)
5. Norm of first intensity derivatives : since |Ix(x)| ≤ 1,|Iy(x)| ≤ 1we have that∀x∈
S,/radicalbig
Ix(x)2+Iy(x)2≤√
2.
6. Angle of intensity derivatives : Note that for a≥0,0≤arctana≤π
2. Hence we have that
∀x∈S,arctan/parenleftig
|Ix(x)
Iy(x)|/parenrightig
≤π
2.
These provide the following upper bounds on L2 norm of ϕ(I)(x),
for a gray scale image ,∀x∈S∥ϕ(I)(x)∥2
L2≤12, (22)
for RGB image ,∀x∈S∥ϕ(I)(x)∥2
L2≤14. (23)
The claim follows by using Eq. 22, Eq. 23 in Eq. 21
Theorem A.7 (Geodesic Radius of Covariance Descriptors) .
1. IfIis a gray scale image, then ∥Logm Rη(I)∥F≤√
9 max{|lnη|,|ln(12 +η)|}.
2. IfIis a RGB image, then ∥Logm Rη(I)∥F≤√
11 max{|lnη|,|ln(14 +η)|}.
Proof.We first note that
λmin(Rη(I)) =λmin/bracketleftigg
1
|S|/summationdisplay
x∈S(ϕ(I)(x)−µ)(ϕ(I)(x)−µ)T+η.I/bracketrightigg
(1)
≥λmin/bracketleftigg
1
|S|/summationdisplay
x∈S(ϕ(I)(x)−µ)(ϕ(I)(x)−µ)T/bracketrightigg
+λmin[ηI]
(2)
≥0 +η. (24)
where we used Weyl’s inequality for symmetric matrices in (1)andλminof positive semi definite matrix is
≥0andλmin[ηI] =ηin(2).
For gray scale images, Rη(I)produces a 9×9matrix:
∥Logm Rη(I)∥F(∗)
≤√
9 max{|lnℓ|,|lnL|}
(∗∗)=√
9 max{|lnη|,|ln(12 +η)|},
where we use Lemma. A.5 in (∗)and Eq.24(ℓ=η) and Lemma. A.6( L= 12 +η) in(∗∗)
For RGB images, Rη(I)produces a 11×11matrix:
∥Logm Rη(I)∥F(†)
≤√
11 max{|lnℓ|,|lnL|}
(‡)=√
11 max{|lnη|,|ln(14 +η)|},
where we use Lemma. A.5 in (†)and Eq.24 ( ℓ=η) and Lemma. A.6( L= 14 +η) in (‡), in a similar
fashion.
Notethatinallofourexperiments, wechoose η= 10−6andhence|lnη|≈13.8domninatesover|ln(12+η)|≈
2.5and|ln(14 +η)|≈2.6.
23Published in Transactions on Machine Learning Research (02/2023)
10 20 30
k010203040506070time in seconds
Rie-Laplace(1*104)
Rie-Laplace(3*104)
Rie-Laplace(5*104)
T anG-Analytic(Ours)
(a)
10 20 30
k01020304050LE(f(),A())
Rie-Laplace(1*104)
Rie-Laplace(3*104)
Rie-Laplace(5*104)
T anG-Classical(Ours)
T anG-Analytic(Ours) (b)
0 2 4 61.21.31.41.51.61.71.8radiuspredicted
radiusobtained
OctoMNIST(grayscale)
PathMNIST(RGB) (c)
Figure 4: (a) Computational times for Rie-Laplace (x) the Riemannian Laplace mechanism with a MCMC
burn-in ofx∈{10,000; 30,000; 50,000}(Reimherr et al., 2021), and TanG-Analytic the proposed tangent
Gaussian mechanism (analytic version). (b) Utility with varying burn-ins for Rie-Laplace . Plots (a, b) use
different matrix sizes k. Plot (c) explores if the bound from Th. 5 is tight in practice.
B Experiments
All experiments were run on Dell XPS 17 9710 Laptop which has 32GB of RAM ,11th Gen Intel(R)
Core i9-11900H @ 2.50GHz Processor. No GPUs were used in the experiments.
B.1 Implementation Details
Letk∈N,M∈SPD(k),σ>0andρLEdenotelog-Euclideandistance. TheRiemannianLaplacedistribution
with log-Euclidean distance is given by
p(X|M,σ) =1
CM,σexp/parenleftbigg
−ρLE(X,M )
σ/parenrightbigg
. (25)
Note that sampling from Eq. equation 25 requires Markov Chain Monte Carlo (MCMC) methods (Robert
et al., 1999), for which one needs to choose a proposal distribution that generates candidates on the SPD
Manifold. We choose the Log Gaussian distribution as the proposal in our experiments given its simplicity
and the fact that it is quick to sample from. In all experiments, we found that using the log Gaussian
distribution as proposal yields a stable acceptance ratio of 50% to 65%. To summarize,
1. Initialize Xcurrat a random point of the manifold SPD (k).
2. For 1→niterations
(a) Draw a candidate from X∼LN (Xcurr,σ2I).
(b) With probability exp(−ρLE(Xmean,X)/σ)/exp(−ρLE(Xcurr,X)/σ)accept the generated candi-
dateXand setXcurr=X.
The final sample is chosen based on a burn-in period of 50,000 steps. Both Riemannian Laplace and tangent
Gaussian mechanism can be easily implemented using existing libraries like geomstats Miolane et al. (2020),
tensorflow-riemopt Miolane et al. (2020); Smirnov (2021), rieoptax Utpala et al.. In all our experiments we
used geomstats Miolane et al. (2020).
B.2 Additional Experiments
We compare the times required to privatize the Fréchet mean using both mechanisms and varying k∈
{2,5,10,15,20,25,30}in Fig. 4 (a). Note that we used MCMC for Riemannian Laplace and its time depends
on the burn-in - that we choose in {10000,30000,50000}. Fork= 30, Fig. 4 (a)shows that Riemannian
Laplace mechanism takes 14sec (burn-in 10000),36sec (burn-in 30000) and 73sec (burn-in 50000) - whereas
24Published in Transactions on Machine Learning Research (02/2023)
our tangent Gaussian (Analytic) mechanism takes 1.3microsec. Fig. 4 (b)considers the effect of the burn-in
on the Riemannian Laplace’s utility and finds no significant difference for burn-ins in {10000,30000,50000}.
Fig. 4 (c)shows that the bound derived in Th. 5 is tight in practice, as illustrated by the ratio of the bound
obtained in Th.5 and the practical bound.
B.3 Code
Code is attached with Supplementary Material.
25