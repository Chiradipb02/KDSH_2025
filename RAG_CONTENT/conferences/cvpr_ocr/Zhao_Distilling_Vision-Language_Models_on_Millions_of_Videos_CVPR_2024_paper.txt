Distilling Vision-Language Models on Millions of Videos
Yue Zhao1,2∗Long Zhao1Xingyi Zhou1Jialin Wu1Chun-Te Chu1Hui Miao1Florian Schroff1
Hartwig Adam1Ting Liu1Boqing Gong1Philipp Kr ¨ahenb ¨uhl2Liangzhe Yuan1
1Google Research2University of Texas, Austin
Abstract
The recent advance in vision-language models is largely
attributed to the abundance of image-text data. We aim
to replicate this success for video-language models, but
there simply is not enough human-curated video-text data
available. We thus resort to fine-tuning a video-language
model from a strong image-language baseline with syn-
thesized instructional data. The resulting video model by
video-instruction-tuning ( VIIT ) is then used to auto-label
millions of videos to generate high-quality captions. We
show the adapted video-language model performs well on a
wide range of video-language benchmarks. For instance, it
surpasses the best prior result on open-ended NExT-QA by
2.8%. Besides, our model generates detailed descriptions
for previously unseen videos, which provide better textual
supervision than existing methods. Experiments show that
a video-language dual-encoder model contrastively trained
on these auto-generated captions is 3.8% better than the
strongest baseline that also leverages vision-language mod-
els. Our best model outperforms state-of-the-art methods
on MSR-VTT zero-shot text-to-video retrieval by 6%. As a
side product, we generate the largest video capation dataset
to date.
1. Introduction
Much progress in image understanding [15, 45, 59, 75, 81]
is fueled by large-scale high-quality image-text datasets [9,
27, 48, 51]. Despite the wide availability on the Inter-
net, annotating videos is nontrivial. For images, humans
construct most annotations within 15∼90seconds per in-
stance [27, 35]. For videos, the annotation time is 1∼2
orders of magnitude higher: it takes about 70 hours to
transcribe narratives for a 1-hour video [21, 68] and 700
hours to provide a 1-hour video with instance-level annota-
tions [12]. There have been attempts to automate such a pro-
cess by retrieving alt-text [2, 42] or transcribing text from
audio [40, 76]. However, alt-text can be irrelevant to the
∗Work done during an internship at Google Research.
In s tru c t i o n:  Give a short answer: what does the man do 
after  lifting one leg ?
Ou t p u t:  Move his hand and his body .In s tru c t i o n:  Caption the video in a short sentence :
Ou t p u t:  A man is doing Tai Chi Chuan exercise .In s tru c t i o n:  Describe the video in detail : 
Ou t p u t:  A man is standing on a beach. He's wearing a 
black shirt and black pants . He is doing a basic Tai 
Chi Chuan exercise . He is standing on one leg  and he 
is moving his hand  while talking and moving his body .There’s a man on the beach practicing martial arts .Man standing on 
the beach near 
the ocean. Man standing on 
one leg on the 
beach near the 
ocean .Man standing on 
the beach near 
the ocean. smiling businessman balancing on a surfboard  on the 
beach in slow motion 
Raw 
alt-text :
Image 
Captioner :
Video 
Captioner: 
Our model: Figure 1. Our video-language model takes a video along with any
form of instruction as input and generates text according to the in-
struction. It generates textual descriptions with multiple granular-
ities, including static appearance ,general action , and detailed
body movements . In contrast, raw alt-text can be erroneous ; im-
age captioners fail to capture the action; video captioners prefer
outputting short text. Our generated data trains a significantly bet-
ter video-language dual-encoder model. Best viewed in color.
video content; audio transcription is often misaligned with
the visual information [22]. Recent work [63] leverages ex-
isting image-based vision-language models (VLMs). How-
ever, the resulting captions are often biased towards static
scenes and lose videos’ rich temporal information.
In this paper, we propose a simple yet effective approach
to adapt an image-based VLM to video and then create
high-quality pseudo-captions on millions of videos. As a
VLM is generally composed of a visual encoder and a lan-
guage model, we propose to adapt each component sepa-
rately to better leverage the relatively small video-text cor-
pora. We first fine-tune the visual encoder on video cap-
tioning data while keeping the language component frozen.
This adapts the model to dynamic scenes while retain-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
13106
ing the diverse ability of the original language decoder.
We then fine-tune the language model on a small amount
of instruction-following data and keep the visual encoder
frozen. This is to emphasize the temporal and causal rea-
soning ability beyond scene-level description. The resulting
video-language model sees both dynamic input and motion-
focused output and is capable of generating high-quality
pseudo-captions for million-scale web-scraped videos.
Pseudo-captioning by the adapted VLM have the follow-
ing advantages. First, the captions are generally relevant to
visual content because of the maximum likelihood objec-
tive during video-captioning training. Second, our pseudo-
captions preserve temporal information in videos better
than frame-wise captions for videos [38, 63]. Third, the
instruction-tuned video-language model generates textual
descriptions with multiple granularities, including static ap-
pearance, general actions, and detailed body movements.
Finally, compared to human labeling, pseudo-captioning is
more scalable. For each video, the underlying language
model can output multiple candidate captions in parallel in a
single pass, and the annotation cost can be further improved
given advances in efficient inference techniques [31].
We evaluate the resultant VLM on a wide range of video-
language benchmarks, covering video question answering
(QA) and captioning, and observe state-of-the-art zero-
shot performance on all. For instance, it attains a 29.5%
WUPS score on open-ended NExT-QA, 2.8% better than
Flamingo-80B while using only1
16×parameters. We fur-
ther use this adapted VLM to generate video descriptions on
million-scale web-scraped videos. Qualitatively, the gener-
ated descriptions are more specific and detailed than alt-text
or image captions. To evaluate the pseudo-captions quan-
titatively, we train a CLIP-style [48] video-language dual-
encoder model using the generated descriptions. We ob-
serve a striking scaling effect on the performance with re-
spect to the size of pseudo-captioned video data, which does
not hold for alt-text alternatives. Our model also works bet-
ter than the one trained on frame-wise captions followed
by LLM summarization. Notably, the dual-encoder model
trained on 17 million web-scraped video clips with our
machine-generated descriptions achieves the state-of-the-
art performance on popular video-text retrieval and video
recognition benchmarks. For instance, the model scores
48.4% Recall@1 on MSR-VTT, 6% higher than the best
previously reported number.
2. Related Work
Synthetic data from simulators are useful to create new
datasets or augment existing ones [13] for vision tasks such
as optical flow [14], semantic segmentation [50], and 3D
vision [7]. LLM-generated text becomes a great source
for language understanding [39]. For example, Vicuna [11]
fine-tunes LLaMA [56] on user-shared conversations fromShareGPT. In the context of vision-language understanding,
generating high-quality synthetic captions for vision data
by leveraging LLMs has been shown effective in improving
multimodal datasets for VLMs [43]. VideoChatGPT [38]
uses both human-assisted and semiautomatic annotation
methods with BLIP-2 [32] and GPT-3.5 to generate high-
quality video instruction data. InternVid [63] introduces a
scalable approach to automatically construct a high-quality
video-text dataset with BLIP-2 and Vicuna. LLaV A [36]
incorporates instruction tuning to VLMs, which demon-
strates impressive multi-modal chat abilities. However,
these methods either focus on image inputs or rely on im-
age models to produce video captions, which fail to capture
correct temporal information in videos.
Vision-language models. Utilizing image-text data for pre-
training has become the default approach to tackle vision-
language tasks. Recently, VLMs based on image-text con-
trastive learning ( e.g., CLIP [48] and ALIGN [26]) attain
strong results on zero-shot retrieval and classification tasks.
Follow-up studies propose to add more pre-training objec-
tives, such as captioning loss ( e.g., CoCa [73]), to enable
VLMs to handle different downstream tasks ( e.g., image
captioning and visual QA). Parallel methods explore lever-
aging off-the-shelf pre-trained models and keep them frozen
during training. They partially freeze either vision or lan-
guage models ( e.g., PaLI [8–10] and LiT [78]) or insert new
layers between them ( e.g., Flamingo [1] and BLIP-2 [32])
so that the knowledge from frozen models can be trans-
ferred to vision and language tasks. Our work builds upon
them and tackles video inputs, a more challenging modality
involving temporal and causal reasoning of motion.
Video-language models can be adapted from image-
language models given that image-based foundation models
are pre-trained on web-scale image data. VideoCLIP [67]
leverages a pre-trained CLIP model [48] as a frame-level
feature extractor and fine-tunes video and text transformers
on video datasets. VideoCoCa [69] builds on CoCa [73] and
fine-tunes some temporal pooler layers to reason over time.
Another line of research focuses on parameter efficient tun-
ing, which is first shown effective on language model-
ing [30]. AIM [71] adapts pre-trained image models for ef-
ficient video understanding by freezing pre-trained weights
and tuning a few lightweight adapters. Furthermore, to
solve more complex video-language tasks like captioning
and QA, researchers leverage the powerful LLMs as a uni-
versal interface and adapt LLMs to consume visual tokens.
FrozenBiLM [70] leverages a frozen bi-directional lan-
guage model for video QA. VideoChat [33] and VideoChat-
GPT [38] propose a chatbot-like interface to analyze video
input. However, VideoChat only shows qualitative analy-
sis while VideoChatGPT relies on a GPT-4 for quantitative
evaluation, leading to inconsistency over time. LaViLa [80]
develops a video-language model that densely narrates for
13107
a video. However, training the narrator assumes videos to
be partially annotated. Our work takes a further step and
shows that the adapted video-language model generalizes
to million-scale unseen videos.
3. Preliminaries and Notations
We first describe preliminaries and, meanwhile, introduce
some notations facilitating the presentation of our method.
Image-based VLMs take as input an image and a text se-
quence, which is often called a prompt [4] or an instruc-
tion [64], and outputs another text sequence that follows the
prompt. Specifically, let x∈RH×W×3denote an input
image with height Hand width W,y= (s1,···, sLi)∈
{0,1}Li×|S|the instruction, and z= (z1,···, zLo)∈
{0,1}Lo×|S|the output text that are tokenized [29] into se-
quences of discrete symbols. Here Sdenotes the vocabulary
set, and LiandLoare the sequence lengths of the instruc-
tion and output, respectively.
A typical VLM has a visual encoder FVand a lan-
guage model FL. The visual encoder maps xtoNvi-
sual tokens x′=FV(x)∈RN×C. It is often instanti-
ated by a pre-trained Convolutional Network [23] or Vi-
sion Transformer [15] plus an optional projection module
in the form of Q-Former [32], Resampler [1], or attentional
pooler [73]. The language model projects an input instruc-
tionyto text tokens y′∈RLi×C, concatenates them with
the visual tokens, and emits a text sequence recursively
˜zl=FL(x′,y′,z<ℓ), where z<ℓ= [˜z0,···,˜zl−1]with ˜z0
being a special start-of-sentence token <s>.FLcan be ei-
ther an encoder-decoder-style model [49, 55], or a decoder-
only model [4]. In this paper, we train the VLM using a
captioning loss, i.e., the sum of the negative log-likelihood
of the correct word at each step:
L=−LX
ℓ=1p(zℓ|x′,y′,z<ℓ). (1)
The key to the recent success of VLMs is the abundance of
paired image-text datasets {(x,c)}. By setting y=∅or a
fixed task prompt for captioning and z=c, we can easily
scale up VLMs by training on billion-scale datasets [9, 51].
Visual instruction tuning intends to enable VLMs to tackle
tasks beyond image captioning [36]. In this case, (y,z)
can be a question-answer pair as in visual QA [20], or
more generally, any free-form instruction-answer pair. The
paired instruction-answer data are typically transformed
from a plain caption via few-shot prompting by a language
model [4, 62], i.e.(y,z) = LLM( c).
Video-text datasets. One of the main challenges in training
video-language models is the lack of video-text data. The
largest public video dataset with human-labeled textual de-
scriptions is Spoken Moments in Time (S-MiT) [41], which
has∼500K videos. Although the covered topics are diverse,
the video durations are short ( 2∼3seconds), and the cap-
tions are brief. The textual descriptions are transcribed fromaudio recordings with inevitable transcription errors. The
Video Localized Narratives (VidLN) [57] dataset captures
more complex events for longer videos ( 10∼30seconds),
but it is 10×smaller in the number of videos due to anno-
tation cost. Both lag in scale far behind existing image-text
datasets, e.g. WebLI-10B and LAION-5B. In the following
section, we present an approach to leveraging these existing
video-text datasets to efficiently adapt a pre-trained VLM
from images to videos so that we can obtain high-quality
pseudo-captions for millions of in-the-wild videos. Experi-
ments show our method yields competitive annotation qual-
ity and is more scalable than human annotation for videos.
4. Method: Adapting VLMs to Videos
We adapt an image-language model to the video domain
in two stages. In the first stage, we adapt the visual en-
coder while freezing the language component, allowing us
to leverage relatively large video-text datasets whose text is
unfortunately short and low-quality. In the second stage, we
finetune the language encoder and freeze the other model
components using a smaller video-text dataset whose text
describes the video in detail and provides diversity. We
empirically justify the advantage of this two-stage design,
which is necessary given the video-text data’s quality and
size falling behind its image-text counterpart.
4.1. Model
Our video-language model takes a sequence of frames as vi-
sual input. Let {x1,···,xT}denote the input video, where
Tis the number of frames. We pass each frame xtinto
the visual encoder FVand concatenate all output visual to-
kens, namely x′= [FV(x1),···, FV(xT)]∈RTN×C. By
doing so, we maintain the visual modeling capacity from
the image-based models [8] and keep both computation and
memory cost tractable ( O(TN2)rather than O(T2N2)).
The language model then collects the visual tokens plus in-
put instruction tokens and emits a text sequence.
Model architecture. We start with PaLI-3 [8], a state-of-
the-art VLM trained on WebLI [9] which has image-text
data only. The visual encoder is a ViT-G/14 [77] with
2B parameters. The language model follows an encoder-
decoder architecture based on UL-2 [55] with 3B parame-
ters. We feed the adapted model with 8 frames at 2 FPS and
resize the input resolution to 224×224.
4.2. Two-Stage Adaptation
Due to the scarcity of paired video-text data, we propose to
fine-tune the video-language model from the image-based
baseline in two stages: (1) visual adaptation, where we
freeze the language component while fine-tuning the visual
part with a relatively large video dataset with short captions;
and (2) language adaptation, where we instruction-tune the
13108
Image 
Encoder Language Model 
 Language Model 
Video 
Encoder 
A man is standing on a 
beach and doing a basic 
Tai Chi Chuan. A: A man moves his hand 
and his body after 
lifting one leg. A man in black suite on 
the beach. 
Generate the 
alt-text: Generate the 
alt-text: Visual 
Adaptation Language 
Adaptation 
Language Model 
Video 
Encoder 
Q: What does the 
man do after 
lifting one leg? Figure 2. Overview of adapting vision-language models to videos. In the first stage of visual adaptation on sequences of video frames,
we fine-tune the vision encoder while freezing the language model using a video dataset with captions. In the second stage of language
adaptation, we freeze the vision encoder while fine-tuning the language model using a video dataset with instruction-following data, e.g. a
question that requires temporal reasoning to answer in this example.
language component while freezing the visual part with a
smaller video dataset with detailed captions.
Visual adaptation. In the stage of visual adaptation, we
fine-tune FVwhile keeping FLfrozen using a large video
dataset with short captions {(x,c)}. We optimize Eq. (1)
by setting yto be a fixed task prompt for captioning
(“Generate the alt-text: ”) and zto be the cap-
tion. On one hand, finetuning FVenables the visual encoder
to focus more on scene dynamics rather than appearance.
On the other, freezing FLprevents the language model from
possible collapse due to simple text and repetitive patterns.
Language adaptation. In this stage, we fine-tune FLwhile
keeping FVfrozen using videos with instruction-following
data generated as follows. Given a video xand its caption
c, we first prompt an LLM to generate a question yand the
corresponding answer zwhich is inferred from the original
caption. We optimize Eq. (1) with the (x,y,z)triplets.
The video-language model’s temporal reasoning abil-
ity is highly dependent on the instruction-following data
it trains on. To this end, we design prompts to encourage
LLMs to generate causal andtemporal questions inspired
by how the NExT-QA dataset [65] is constructed. Causal
questions either explain the intention of an action that hap-
pens first or the cause of an action that occurs next. It typ-
ically follows the form of “Why did somebody do some-
thing?” or “How did something happen?”. Temporal ques-
tions ask about the temporal ordering of multiple actions.
The temporally ordered actions can either happen on a sin-
gle object or occur between multiple persons or objects. We
provide an example for illustration in Figure 3 and more de-
tails in the supplementary materials.
Inference. At inference time, we query the video-language
model by feeding sampled video frames for x, the regu-
lar task prompt for captioning for y, and a special start-
of-sentence token <s> forz= [z0]. We sample from the
distribution recursively, i.e. ˜zℓ∼p(z|x,y,˜z<ℓ)until anend-of-sentence token </s> is reached. We use nucleus
sampling [24], where we only sample from a subset of to-
kens that contain the vast majority of the probability mass
at each step, multiple times. We provide an example of cap-
tions before and after video-specific adaptation in Figure 4.
Readers can find more results in the supplementary materi-
als in§8. We observe on average 20% longer length in the
output sequence after the language adaptation while using
the same task prompt for captioning. We attribute it to the
effectiveness of instruction tuning.
5. Experiments
First, we summarize the datasets that we use in §5.1. Next,
we describe how we harness and evaluate the distilled
pseudo-captions in §5.2. We show the main results, i.e. (1)
the scaling effect of our data generation pipeline, (2) the
quality of pseudo-captions by pre-training a dual-encoder
model, and (3) the performance of the adapted video-
language model on video-language tasks in §5.3. Finally,
we discuss the effect of different components in §5.4.
5.1. Datasets
Table 1 summarizes the video datasets used in this paper,
and more details are in §9 in the supplementary material.
We categorize the datasets into four parts and describe the
adaptation data and distilled data first.
Adaptation data. We use two datasets to adapt a vision-
language model from images to videos: (1) Spoken Mo-
ments in Times (S-MiT) [41] contains 500K videos with
spoken captions. The videos are typically short ( 2∼3sec-
onds) and the transcribed captions are brief (18 words on
average per video). It has 481K/8K/3K videos for train-
ing/validation/testing. We use the training split to conduct
visual adaptation of the video-language model and eval-
uate the video captioning result by CIDEr score on the
13109
Qu e s t i o n:  What does the man do after  he fails to hold onto the tree and falls into the water ?
An sw er:  After the man falls into the water, he starts laughing .<Man>A man wearing blue shorts is  hanging while holding a wooden stick with his right hand tied 
to a string  hanging on the tree. He tries to hold onto the trunk of the tree with his left hand , 
but he fails and falls into the river , and then he starts laughing . <Boat>A white boat is moving 
in the river. <Background>In the background, there is the sound of laughing, green trees, a sky 
with clouds, a white boat, a tree, and a river. Captions: 
Temporal Reasoning: Qu e s t i o n:  Why did the guy fall off the tree swing ?
An sw er:  The guy tried to hold onto the tree trunk while on the tree swing , but it seems that he failed to 
grab onto the tree tightly enough . As a result, he fell off the tree swing. Causal Reasoning: 
Figure 3. An example of the instruction-following data. The first block shows the detailed captions used to prompt an LLM (PaLM 2 [19]
in our case), and the following two blocks show the LLM’s responses. We show the keyframes in the top block for illustration purpose and
donotuse them while prompting the LLM. Different details in text are highlighted. Best viewed in color.
there's a little castle  and a ball pit  in a backyard with a roof on it. with a straw hat  on, a young man 
jumps into the ball pit  and falls in . he is sitting in the ball pit  and he is looking around a man wearing a straw hat is playing in a colored ball pit outdoors a castle in a 
backyard a ball pit in 
front of 
trees a man wearing 
a cowboy hat 
sits in a 
ball pit a man is playing in a ball pit Raw alt-text: 
Image Captioner: 
Our method 
+ Visual adaptation: 
Our method 
+ V&L adaptation: a man is 
playing in a 
ball pit in a 
backyard a child is 
playing in a 
ball pit in a 
backyard a castle in a 
backyard 
a man is playing in a ball pit in a backyard Image Captioner 
+ LLM summarization: 
Figure 4. An example of video captions by PaLI-3 before and after video-specific adaptation. We show the keyframes on top for
illustration purposes and the generated captions in the following blocks. Different details in text are highlighted. Best viewed in color.
testing split following PaLI [8, 10]. (2) Video Localized
Narratives (VidLN) [57] annotates comprehensive events in
videos which involve multiple actors and possibly actor-
actor and actor-object interaction. The narratives are longer
(85 words on average) and are better suited to generate a
diverse instructing-following corpus. We use the training
split which has 47,776 videos from the union of OVIS [46],
Oops [17], UVO [58], and Kinetics [5] datasets, to generate
instruction-answer pairs for language adaptation.
Data with distilled pseudo-captions. We apply the re-
sultant video-language model to caption two largest-scale
webly-scraped video datasets: (1) VideoCC [42] contains
∼10M video-caption pairs from 6M unique videos. The raw
alt-text is automatically retrieved from those in the Concep-
tual Captions image-captioning dataset (CC3M) [53] based
on image similarity. ∼7.1M clips are available by the timeof our experiments. (2) InternVid [63] has ∼234M clips
from 7M videos. The original captions are synthesized from
individual frames’ captions by an LLM. We use the pub-
licly available InternVid-10M-FLT subset which has 10M
clips with top-scoring video-text similarities. We denote
the datasets processed by our method to be VideoCC+
andInternVid+. We use both datasets to pre-train a dual-
encoder model to show the usefulness of the machine-
generated video captions, explained next.
5.2. Harnessing the Distilled Pseudo-Captions
We harness and evaluate the distilled pseudo-captions
for million-scale web-scraped videos, VideoCC+and
InternVid+, using a dual-encoder model [48]. The model’s
video understanding performance is a solid indicator of
the pseudo-captions’ quality, and we show that they are of
13110
Dataset Task Size Metrics
S-MiT [41] ADP 480K (train) -
VidLN [57] ADP 47K (train) -
VideoCC [42] CP 7M/10M -
InternVid [63] CP 10M -
MSR-VTT [68] TVR 1K (val, or 1k-A ) Recall@ k
V ATEX [60] TVR 1.5K (test as in [61]) Recall@1
Kinetics-600 [6] CLS 28K (val) Accuracy
MSR-VTT [68] CAP 6.5K (train) +3K (test) CIDEr
MSR-VTT QA [66] QA 6.5K (train) +3K (test) Accuracy
ANet-Captions [28] CAP 31K (train) +14K (test) CIDEr
S-MiT [41] CAP 480K (train) +3K (test) CIDEr
ANet-QA [74] QA 32K (train) +8K (test) Accuracy
NExT-OE-QA [65] QA 37K (train) +9K (test) Wu-Palmer Similarity (WUPS)
Table 1. Dataset summary. ADP is short for adapting VLMs
while CP is for contrastive pre-training a dual-encoder model.
Evaluation tasks include text-to-video retrieval (TVR), action clas-
sification (CLS), video captioning (CAP), and video question an-
swering (QA).
higher quality than the original text in VideoCC and Intern-
Vid.
Contrastive training of a dual-encoder model. We
train a video-language dual-encoder model like CLIP [48].
Specifically, given the input video frames xand machine-
generated captions ˜c, the model applies a visual encoder
GVplus a projection head hVand a text encoder GTplus
a projection head hTin parallel to obtain the global visual
and textual embedding, respectively,
u=hV(GV(x)),v=hT(GT(˜c)). (2)
We use the InfoNCE [44] loss to train the model. Note that
we deliberately choose a different notation G(·)thanF(·)
in the VLM in §3 because the dual-encoder model does not
share any parameters with the VLM.
Model architecture. The dual-encoder model has a vi-
sion encoder and a text encoder. The video input is rep-
resented by 4 frames at 2 FPS. The vision encoder is a
Vision Transformer [15] with joint spatial-temporal atten-
tion (denoted as “ViT- st”) following [79]. We use ViT-
L/14 to report the main result and ViT-B/16 for ablation
studies if not otherwise specified. The weights are initial-
ized from CLIP [48] except that we randomly initialize the
temporal position embedding PEt∈RT×Dand add it to
the original spatial position embedding PEs∈RN×D,i.e.
PE[i,:,:] = PE t[i,None ,:]+PE s[None ,:,:]. The text en-
coder is a 12-layer GPT-like Transformer [47]. It takes as
input one video caption, tokenizes it using BPE [52], and
keeps at most 77 tokens. If a video has more than one cap-
tion, we randomly sample one of them at each time.
5.3. Main Results
We report the dual-encoder model’s text-to-video retrieval
performance (on MSR-VTT and V ATEX) and video clas-
sification accuracy (on Kinetics-600), both under the zero-
shot setting. These results are meant to evaluate the qual-
ity of the distilled video pseudo-caption data. Besides, we7 70 700 7,0003540
Number of used data in VideoCC ( ×103)MSR-VTT R@1Alt-text [42]
Img. cap. + LLM
Vid. cap. (Ours)
Figure 5. Scaling effect of video captioning. For VLM-generated
captions, the zero-shot video retrieval performance consistently
improves with respect to an increasing amount of video data. Pre-
training on retrieved alt-text quickly stagnates.
also evaluate the VLM adapted to the video domain on a
few representative video-language benchmarks following
PaLI-3 [8], including video captioning (MSR-VTT [68],
ActivityNet-Captions [28]) and video question-answering
(MSR-VTT QA [66], ActivityNet QA [74], and NExT
Open-Ended QA [65]). We enumerate the datasets involved
at the bottom of Table 1 and leave details in §9.
Distilled vs. alt-text captions at various scales. Figure 5
shows that the distilled pseudo-captions for VideoCC out-
perform VideoCC’s original Alt-text captions, by a strik-
ing margin, when the dual-encoder models trained using
different subsets of VideoCC are evaluated on the MSR-
VTT retrieval task. We find that Recall@1 quickly saturates
when training the dual-encoder model on VideoCC with alt-
text. Specifically, training with only 1% VideoCC+(∼70K)
achieves the same level of Recall@1 with training with the
whole VideoCC set ( ∼7M), indicating that the original alt-
text scales poorly. We attribute the alt-text’s inferior perfor-
mance to a compounding error of textual noise [26], spu-
rious correlation when computing visual similarities [72],
and the visual discrepancy between images and videos. In
contrast, training the dual-encoder model with the pseudo-
captions clearly exhibits a pleasant scaling effect: R@1
consistently increases with more pre-training video data.
We also include in Figure 5 the curve corresponding to the
pseudo-captions distilled from the image-language model
before it is adapted to the video domain. It almost overlaps
with the alt-text curve at the beginning and then becomes
slightly better near the end.
Distilled captions for video understanding. We con-
tinue to evaluate the distilled pseudo-captions by the cor-
responding dual-encoder model’s zero-shot performance on
text-to-video retrieval and video classification. From Ta-
ble 2, we see that the pseudo-captions distilled from our
VLM significantly improve the dual-encoder over the orig-
inal text in VideoCC and InternVid. On VideoCC, with
all other settings being the same, the dual-encoder model
trained on VideoCC+, achieves 48.2% Recall@1 on MSR-
VTT, 11.2% better than the one trained on the original Alt-
text. It also clearly surpasses the recent ViCLIP trained
13111
Method Pre-training DatasetMSR-VTT TVR V ATEX TVR Kinetics-600
R@1 R@5 R@10 R@1 R@5 R@10 Top-1 Top-5
CLIP [48] WIT 31.2 53.7 64.2 45.2 - - 55.1 79.2
CLIP4Clip [37] WIT 30.6 54.4 64.3 - - - - -
CLIP4Clip [37] WIT→VideoCC (10M) 33.7 57.9 67.9 - - - - -
InternVideo [61] WIT→Mixed (12M) 40.0 65.3 74.1 49.5 79.7 87.0 -
ViCLIP [63] WIT→WebVid (10M) 35.6 - - - - - 58.7 81.0
ViCLIP [63] WIT→InternVid (10M) 42.4 - - - - - 62.2 84.9
CLIP (ViT- st-L)WIT→VideoCC 37.0 62.1 72.5 37.7 66.9 77.2 48.6 74.8
WIT→VideoCC+(Ours ) 48.2 72.2 80.8 64.2 90.2 95.1 61.1 85.6
Absolute gain ∆ +11.2 +10.1 +8.3 +26.5 +23.3 +17.9 +12.5 +10.8
WIT→InternVid 42.5 67.0 76.8 58.7 87.0 93.0 60.7 85.0
WIT→InternVid+(Ours ) 46.3 71.5 80.3 65.2 91.3 95.5 62.7 86.2
Absolute gain ∆ +3.8 +4.5 +3.5 +6.5 +4.3 +2.5 +2.0 +1.2
WIT→VideoCC++InternVid+(Ours )48.4 73.5 81.9 65.6 91.7 95.8 62.8 86.4
Table 2. Zero-shot text-to-video retrieval performance on MSR-VTT & V ATEX and video recognition performance on Kinetics-
600 using different sources of textual descriptions. D+means that the captions in the video dataset Dare generated by our proposed
pipeline. D ∈ { VideoCC ,InternVid }in our experiments.
Method Pre-training DatasetMSR-VTT ActivityNet NExT-OE-QA
Caption QA (Acc.) Caption QA (Acc.) QA (WUPS)
Prior SOTA -18.6 16.8 15.0 25.9 26.7
DeCap [34] FrozenBiLM [70] DeCap [34] FrozenBiLM [70] Flamingo [1]
PaLI-3 8f[9] WebLI 21.3 12.7 13.8 22.9 23.2
Ours WebLI →SMiT+VidLN 48.2 24.4 31.0 29.6 29.5
Table 3. Zero-shot performance of the Video-Language Model on video-language understanding tasks. Our adapted video-language
model significantly improves over the 8-frame PaLI-3 baseline and outperforms the best reported numbers.
on InternVid, which contains 2×more unique videos than
VideoCC. On InternVid, our model trained on InternVid+
is 3.8% better than the baseline trained on the original In-
ternVid’s auto-generated captions. It is worth noting that
our adapted VLM is also “lighter-weight” compared to the
multi-scale captioning pipeline in InternVid [63], which re-
lies on both image captioning models (BLIP-2) [32] on mul-
tiple frames and an LLM to put them together. We also
highlight the zero-shot top-1 and top-5 classification accu-
racy on Kinetics-600. For instance, the dual-encoder model
trained on VideoCC+/InternVid+improves the baselines on
VideoCC/InternVid by 12.5/2.0% top-1 accuracy.
Interestingly, we notice that the model trained on
InternVid+works better on action recognition, while the
one trained on VideoCC+is better on video retrieval. This
is probably because the InternVid videos are specifically
collected based on action phrases [63], while VideoCC is
seeded from image-captioning data [42]. Since the two
datasets are complementary, combining them indeed leads
to performance gains as shown in the last row in Table 2.
Evaluating the video-language model. We compare the
adapted VLM with the baseline PaLI-3 in Table 3. We fo-
cus on the zero-shot performance where we apply the model
to the testing split of downstream tasks without any tun-
ing. This setting resembles the scenario where we gener-
ate pseudo-captions on VideoCC and InternVid, and it pro-vides us with a direct measure on well-established bench-
marks. Specifically, the greatly improved CIDEr score on
MSR-VTT and ActivityNet-Captions showcases the effec-
tiveness of adapting a VLM to the video domain. We also
see excellent zero-shot question-answering results com-
pared to PaLI-3. On the challenging open-ended NExT-
QA dataset, our model outperforms Flamingo [1] by 2.8%
(WUPS score). This gain is achieved using only1
16×of the
parameters (5B vs80B) and1
50×of training videos (0.55M
publicly available S-MiT&VidLN vs27M in-house VTP).
On MSR-VTT QA and ActivityNet QA, our adapted model
achieves 7.6% and 3.7% higher accuracy than Frozen-
BiLM [70], trained on WebVid-10M [2].
5.4. Ablation Studies
What makes captioning better? We investigate the key
to generating better captions for contrastive pre-training
video-language dual-encoder models in Table 4. The com-
parison starts from the alt-text-only baseline which achieves
37.0% text-to-video R@1 on MSR-VTT. Using frame-level
captions produced by PaLI-3 as-is increases R@1 by 2.5%.
We also attempt to merge multiple frames’ captions into a
single sentence with PaLM-2 [19] similar to the pipeline
in InternVid [63] but see marginal gain (0.3%). This re-
sult is consistent with our observation that LLMs often fail
to interpolate when key temporal information is lost in the
13112
PaLI-3 LLMAdapting VLM ( §4.2) Multi. MSR-VTT
Visual Language Samples Recall@1
37.0
✓ 39.5 (+2.5)
✓ ✓ 39.8 (+2.8)
✓ ✓ 41.7 (+4.7)
✓ ✓ ✓ 43.6 (+6.6)
✓ ✓ ✓ ✓ 44.3 (+7.3)
Table 4. The effect of using different sources of textual descrip-
tions. The captioning quality is measured by the zero-shot text-to-
video retrieval performance (Recall@1) on MSR-VTT. The first
line with no components checked refers to the alt-text baseline.
The “LLM”-column means that we use PaLM 2 [19] to summa-
rize captions from multiple frames similar to [63].
Visual Adaptation S-MiT Caption
FVSelf-training FL (CIDEr)
✗ ✓ 41.2
✓ ✗ 42.3
✓ ✓ 40.3
✓ ✓ ✗ 43.5
Table 5. Adapting vision encoder. ✓and✗denote fine-tuning
and freezing the parameters respectively. Fine-tuning the visual
part while freezing the language model yields better results.
image-level descriptions. We also encounter a trade-off be-
tween being concise but lacking diversity and being de-
tailed but vulnerable to hallucination. If we conduct visual
adaptation in PaLI-3, the resulting video captions almost
double the gain from 2.5% to 4.7%. Generating multiple
captions independently with nucleus sampling contributes
1.9%. Finally, doing language adaptation on PaLI-3 with
instruction-following data further improves R@1 by 0.7%.
How should we do visual adaptation? We study several
ways for visual adaptation in Table 5. The first option, i.e.
freezing the visual encoder FVwhile fine-tuning the lan-
guage model FL, takes inspiration from LiT [78]. This
leads to a drop of 1.1 CIDEr score compared to our de-
fault recipe, where FVis fine-tuned and FLfrozen. We
ascribe it to the visual discrepancy between images and
videos: The downstream tasks in [9, 78] are mostly still
images, the same as the large-scale pre-training data. In
contrast, the videos of our interests have unique character-
istics such as object movement, camera motion, and the re-
sultant visual degradation. We also observe a performance
drop if we fine-tune both FVandFL. This recipe may be
prone to over-fitting because the video-text dataset lacks
diversity and quantity. Finally, we show that self-training
with VideoCC pseudo-captions (details in §11.3) improves
captioning results by 1.2 CIDEr score, reaching 43.5. It
is worth noting that this number is on par with the best-
performing PaLI-X [10] which has 11×more parameters
and takes 2×more frames as input than ours.
How should we do language adaptation? We study the
effect of instruction-following data in Table 6 when do-Instruction dataMSR-VTT ActivityNet NExT-OE
Caption (CIDEr) Caption (CIDEr) QA (WUSP)
None (PaLI-3) 21.3 13.8 23.2
LLaV A 1.0 [36] 16.9 25.1 16.3
ActivityNet-Instruct [38] 30.8 34.6 11.7
Ours
+ VidLN Causal/temporal Reasoning 28.5 29.5 5.0
+ SMiT Captions 51.6 35.1 3.9
+ VidLN Short-QA 48.2 31.0 29.5
Table 6. Effect of instruction data. Our proposed instruction
data benefits the adaptation of the video-language model, reflected
by better zero-shot captioning results and QA accuracy.
ing language adaptation. We start with some representative
visual instructional tuning datasets. The first is LLaV A-
1.0 [36] with 150K instruction pairs. We find that it im-
proves the CIDEr score by 7.3 on ActivityNet Captions
but decreases by 4.4 on MSR-VTT Captions. The sec-
ond is ActivityNet-Instruct [38] with 100K instruction pairs
from ActivityNet-Captions [28]. It improves CIDEr score
on both MSR-VTT and ActivityNet Captions, indicating
that video-specific instructional-following data is essential
to video-language tasks. We then conduct an incremen-
tal study on our LLM-prompted instructional corpus on
VidLN+SMiT by adding one component at a time. First,
we fine-tune the language model with only reasoning data.
The adapted model works on par with the one fine-tuned on
ActivityNet-Instruct on ActivityNet-Captions even without
seeing ActivityNet videos, demonstrating the generalization
of our instructed data. Next, we include the captioning data
on S-MiT and see a higher CIDEr score on MSR-VTT and
ActivityNet Caption. However, both models suffer from
significant degradation in zero-shot QA accuracy. This is
expected since the answers in all existing video QA datasets
are typically short ( 1∼3words) while our instructional data
typically contains detailed reasoning (Figure 3). To mitigate
the gap, we further add QA pairs that are few-shot prompted
based on Oops-QA [57], and prepend the question with a
QA-specific task prompt (“ Answer in en: ”). The final
model restores its zero-shot question-answering ability at
the cost of a slight performance drop in captioning.
More ablations. We leave more ablations and discussions
in the supplementary materials.
6. Conclusion
We present an approach to adapting an image-based
vision-language model to videos and distilling high-quality
pseudo-captions for millions of videos. The adapted video-
language model obtains excellent zero-shot performance on
various video-language benchmarks. The pseudo-captions
yield a stronger dual-encoder model and show positive scal-
ing behavior with respect to the number of videos.
Acknowledgements. This material is based upon work in
part supported by the National Science Foundation under
Grant No. IIS-1845485.
13113
References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katherine Millican, Malcolm Reynolds, et al. Flamingo: a
visual language model for few-shot learning. In NeurIPS ,
2022. 2, 3, 7
[2] Max Bain, Arsha Nagrani, G ¨ul Varol, and Andrew Zisser-
man. Frozen in time: A joint video and image encoder for
end-to-end retrieval. In ICCV , 2021. 1, 7
[3] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas
Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A
holistic approach to semi-supervised learning. In NeurIPS ,
2019. 3
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. In NeurIPS , 2020. 3
[5] Joao Carreira and Andrew Zisserman. Quo vadis, action
recognition? a new model and the kinetics dataset. In CVPR ,
2017. 5
[6] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe
Hillier, and Andrew Zisserman. A short note about kinetics-
600. arXiv preprint arXiv:1808.01340 , 2018. 6, 1
[7] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,
Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,
Manolis Savva, Shuran Song, Hao Su, et al. Shapenet:
An information-rich 3d model repository. arXiv preprint
arXiv:1512.03012 , 2015. 2
[8] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov,
Jialin Wu, Paul V oigtlaender, Basil Mustafa, Sebastian
Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, Daniel
Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong,
Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu Sori-
cut. Pali-3 vision language models: Smaller, faster, stronger.
arXiv preprint arXiv:2310.09199 , 2023. 2, 3, 5, 6, 1
[9] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,
Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam
Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-
scaled multilingual language-image model. In ICLR , 2023.
1, 3, 7, 8
[10] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,
Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Se-
bastian Goodman, Xiao Wang, Yi Tay, et al. On scaling up a
multilingual vision and language model. In CVPR , 2024. 2,
5, 8, 1
[11] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-
hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-
hao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.
Xing. Vicuna: An open-source chatbot impressing gpt-4
with 90%* chatgpt quality, 2023. 2
[12] Ahmad Darkhalil, Dandan Shan, Bin Zhu, Jian Ma, Amlan
Kar, Richard Higgins, Sanja Fidler, David Fouhey, and Dima
Damen. Epic-kitchens visor benchmark: Video segmenta-
tions and object relations. In NeurIPS D&B , 2022. 1
[13] Celso M de Melo, Antonio Torralba, Leonidas Guibas, James
DiCarlo, Rama Chellappa, and Jessica Hodgins. Next-generation deep learning based on simulators and synthetic
data. Trends in cognitive sciences , 2022. 2
[14] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip
Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van
Der Smagt, Daniel Cremers, and Thomas Brox. Flownet:
Learning optical flow with convolutional networks. In ICCV ,
2015. 2
[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In ICLR , 2021. 1, 3,
6
[16] Haodong Duan, Yue Zhao, Yuanjun Xiong, Wentao Liu, and
Dahua Lin. Omni-sourced webly-supervised learning for
video recognition. In ECCV , 2020. 3
[17] Dave Epstein, Boyuan Chen, and Carl V ondrick. Oops! pre-
dicting unintentional action in video. In CVPR , 2020. 5
[18] Deepti Ghadiyaram, Du Tran, and Dhruv Mahajan. Large-
scale weakly-supervised pre-training for video action recog-
nition. In CVPR , 2019. 3
[19] Google. Palm 2 technical report, 2023. 5, 7, 8
[20] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-
tra, and Devi Parikh. Making the v in vqa matter: Elevating
the role of image understanding in visual question answer-
ing. In CVPR , 2017. 3
[21] Kristen Grauman, Andrew Westbury, Eugene Byrne,
Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson
Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d:
Around the world in 3,000 hours of egocentric video. In
CVPR , 2022. 1
[22] Tengda Han, Weidi Xie, and Andrew Zisserman. Temporal
alignment networks for long-term video. In CVPR , 2022. 1
[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
2016. 3
[24] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin
Choi. The curious case of neural text degeneration. In ICLR ,
2020. 4
[25] Matthew Honnibal, Ines Montani, Sofie Van Landeghem,
and Adriane Boyd. spaCy: Industrial-strength Natural Lan-
guage Processing in Python. 2020. 1
[26] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representation
learning with noisy text supervision. In ICML , 2021. 2, 6
[27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, Doll ´ar Piotr, and Gir-
shick Ross. Segment anything. In ICCV , 2023. 1
[28] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and
Juan Carlos Niebles. Dense-captioning events in videos. In
ICCV , 2017. 6, 8, 1
[29] Taku Kudo and John Richardson. Sentencepiece: A sim-
ple and language independent subword tokenizer and detok-
enizer for neural text processing. In EMNLP , 2018. 3
13114
[30] Brian Lester, Rami Al-Rfou, and Noah Constant. The power
of scale for parameter-efficient prompt tuning. In EMNLP ,
2021. 2
[31] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast
inference from transformers via speculative decoding. In
ICML , 2023. 2
[32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML ,
2023. 2, 3, 7
[33] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai
Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.
Videochat: Chat-centric video understanding. arXiv preprint
arXiv:2305.06355 , 2023. 2
[34] Wei Li, Linchao Zhu, Longyin Wen, and Yi Yang. Decap:
Decoding clip latents for zero-shot captioning via text-only
training. In ICLR , 2023. 7
[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
ECCV , 2014. 1
[36] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. In NeurIPS , 2023. 2, 3, 8
[37] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei,
Nan Duan, and Tianrui Li. Clip4clip: An empirical study of
clip for end to end video clip retrieval and captioning. Neu-
rocomputing , 2022. 7
[38] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fa-
had Shahbaz Khan. Video-chatgpt: Towards detailed video
understanding via large vision and language models. arXiv
preprint arXiv:2306.05424 , 2023. 2, 8
[39] Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. Gener-
ating training data with language models: Towards zero-shot
language understanding. In NeurIPS , 2022. 2
[40] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,
Makarand Tapaswi, Ivan Laptev, and Josef Sivic.
Howto100m: Learning a text-video embedding by watching
hundred million narrated video clips. In ICCV , 2019. 1
[41] Mathew Monfort, SouYoung Jin, Alexander Liu, David Har-
wath, Rogerio Feris, James Glass, and Aude Oliva. Spoken
moments: Learning joint audio-visual representations from
video descriptions. In CVPR , 2021. 3, 4, 6
[42] Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja
Hauth, Santiago Manen, Chen Sun, and Cordelia Schmid.
Learning audio-video modalities from image captions. In
ECCV , 2022. 1, 5, 6, 7
[43] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Se-
woong Oh, and Ludwig Schmidt. Improving multimodal
datasets with image captioning. In NeurIPS D&B , 2023. 2
[44] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-
sentation learning with contrastive predictive coding. arXiv
preprint arXiv:1807.03748 , 2018. 6
[45] OpenAI. Gpt-4v(ision) system card, 2023. 1
[46] Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu,
Xiang Bai, Serge Belongie, Alan Yuille, Philip HS Torr, and
Song Bai. Occluded video instance segmentation: A bench-
mark. IJCV , 2022. 5[47] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, Ilya Sutskever, et al. Language models are unsu-
pervised multitask learners. OpenAI blog , 2019. 6
[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML , 2021. 1, 2, 5, 6, 7
[49] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with a
unified text-to-text transformer. JMLR , 2020. 3
[50] Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen
Koltun. Playing for data: Ground truth from computer
games. In ECCV , 2016. 2
[51] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for training
next generation image-text models. In NeurIPS D&B , 2022.
1, 3
[52] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural
machine translation of rare words with subword units. In
ACL, 2016. 6
[53] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. Conceptual captions: A cleaned, hypernymed, im-
age alt-text dataset for automatic image captioning. In ACL,
2018. 5
[54] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao
Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk,
Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying
semi-supervised learning with consistency and confidence.
InNeurIPS , 2020. 3
[55] Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Ja-
son Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri,
Tal Schuster, Steven Zheng, Denny Zhou, Neil Houlsby,
and Donald Metzler. Ul2: Unifying language learning
paradigms. In ICLR , 2023. 3
[56] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efficient foundation language models.
arXiv preprint arXiv:2302.13971 , 2023. 2
[57] Paul V oigtlaender, Soravit Changpinyo, Jordi Pont-Tuset,
Radu Soricut, and Vittorio Ferrari. Connecting vision and
language with video localized narratives. In CVPR , 2023. 3,
5, 6, 8
[58] Weiyao Wang, Matt Feiszli, Heng Wang, and Du Tran.
Unidentified video objects: A benchmark for dense, open-
world segmentation. In ICCV , 2021. 5
[59] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-
iang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-
hammed, Saksham Singhal, Subhojit Som, et al. Image as
a foreign language: Beit pretraining for vision and vision-
language tasks. In CVPR , 2023. 1
[60] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang
Wang, and William Yang Wang. Vatex: A large-scale, high-
13115
quality multilingual dataset for video-and-language research.
InICCV , 2019. 6, 1
[61] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun
Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun
Wang, et al. Internvideo: General video foundation models
via generative and discriminative learning. arXiv preprint
arXiv:2212.03191 , 2022. 6, 7, 1, 2
[62] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi.
Self-instruct: Aligning language model with self generated
instructions. In ACL, 2023. 3
[63] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu,
Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu,
et al. Internvid: A large-scale video-text dataset for multi-
modal understanding and generation. In ICLR , 2024. 1, 2, 5,
6, 7, 8
[64] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu,
Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and
Quoc V Le. Finetuned language models are zero-shot learn-
ers. In ICLR , 2022. 3
[65] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua.
Next-qa: Next phase of question-answering to explaining
temporal actions. In CVPR , 2021. 4, 6, 1
[66] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,
Xiangnan He, and Yueting Zhuang. Video question answer-
ing via gradually refined attention over appearance and mo-
tion. In ACM MM , 2017. 6, 1
[67] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko,
Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and
Christoph Feichtenhofer. Videoclip: Contrastive pre-training
for zero-shot video-text understanding. In EMNLP , 2021. 2
[68] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large
video description dataset for bridging video and language. In
CVPR , 2016. 1, 6
[69] Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, So-
ham Ghosh, Yonghui Wu, and Jiahui Yu. Video-text model-
ing with zero-shot transfer from contrastive captioners. arXiv
preprint arXiv:2212.04979 , 2022. 2, 1
[70] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and
Cordelia Schmid. Zero-shot video question answering via
frozen bidirectional language models. NeurIPS , 2022. 2, 7
[71] Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen
Chen, and Mu Li. Aim: Adapting image models for efficient
video action recognition. In ICLR , 2023. 2
[72] Yu Yang, Besmira Nushi, Hamid Palangi, and Baharan
Mirzasoleiman. Mitigating spurious correlations in multi-
modal models during fine-tuning. In ICML , 2023. 6
[73] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-
jtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive
captioners are image-text foundation models. TMLR , 2022.
2, 3
[74] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting
Zhuang, and Dacheng Tao. Activitynet-qa: A dataset for
understanding complex web videos via question answering.
InAAAI , 2019. 6, 1
[75] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,Boxin Li, Chunyuan Li, et al. Florence: A new
foundation model for computer vision. arXiv preprint
arXiv:2111.11432 , 2021. 1
[76] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yan-
peng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack
Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural
script knowledge through vision and language and sound. In
CVPR , 2022. 1
[77] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu-
cas Beyer. Scaling vision transformers. In CVPR , 2022. 3
[78] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,
Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer.
LiT: Zero-shot transfer with locked-image text tuning. In
CVPR , 2022. 2, 8
[79] Yue Zhao and Philipp Kr ¨ahenb ¨uhl. Training a large video
model on a single machine in a day. arXiv preprint
arXiv:2309.16669 , 2023. 6
[80] Yue Zhao, Ishan Misra, Philipp Kr ¨ahenb ¨uhl, and Rohit Gird-
har. Learning video representations from large language
models. In CVPR , 2023. 2
[81] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp
Kr¨ahenb ¨uhl, and Ishan Misra. Detecting twenty-thousand
classes using image-level supervision. In ECCV , 2022. 1
13116
