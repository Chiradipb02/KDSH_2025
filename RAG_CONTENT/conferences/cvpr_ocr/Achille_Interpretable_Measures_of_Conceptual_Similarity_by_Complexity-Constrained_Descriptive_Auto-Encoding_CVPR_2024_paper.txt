Interpretable Measures of Conceptual Similarity by
Complexity-Constrained Descriptive Auto-Encoding
Alessandro Achille⇤Greg Ver Steeg⇤Tian Yu Liu Matthew Trager
Carson Klingenberg Stefano Soatto
AWS AI Labs
Abstract
Quantifying the degree of similarity between images is a
key copyright issue for image-based machine learning. In
legal doctrine however, determining the degree of similarity
between works requires subjective analysis, and fact-ﬁnders
(judges and juries) can demonstrate considerable variabil-
ity in these subjective judgement calls. Images that are
structurally similar can be deemed dissimilar, whereas im-
ages of completely different scenes can be deemed similar
enough to support a claim of copying. We seek to deﬁne and
compute a notion of ‘conceptual similarity’ among images
that captures high-level relations even among images that
do not share repeated elements or visually similar compo-
nents. The idea is to use a base multi-modal model to gen-
erate ‘explanations’ (captions) of visual data at increasing
levels of complexity. Then, similarity can be measured by
the length of the caption needed to discriminate between
the two images: Two highly dissimilar images can be dis-
criminated early in their description, whereas conceptually
dissimilar ones will need more detail to be distinguished.
We operationalize this deﬁnition and show that it correlates
with subjective (averaged human evaluation) assessment,
and beats existing baselines on both image-to-image and
text-to-text similarity benchmarks. Beyond just providing a
number, our method also offers interpretability by pointing
to the speciﬁc level of granularity of the description where
the source data are differentiated.
1. Introduction
Consider the two images in Fig. 1. One could say they are
similar: both portray small red Italian cars. Another could
say they are different: One is a sports car in an open space,
the other a tiny city car in an alley. Is there an objective way
of measuring the similarity among images? In some cases,
similarity judgments can be inﬂuenced by shared concepts:
in Fig. 2, two images share compelling stylistic and concep-
tual similarity, but it is difﬁcult to identify speciﬁc visual
elements they share. Yet the two were found to be legally“substantially similar” [ 37]. Can we deﬁne an objective no-
tion of ‘conceptual similarity’ among data?
There have been many attempts at deﬁning an objective
notion of similarity based on the number of bits of infor-
mation that the two samples share [ 11,25,40], but funda-
mentally they do not capture concepts (Appendix A), which
are human constructs. Since humans must play a role in
deﬁning similarity among concepts, one way to achieve ob-
jectivity is by averaging a large number of human assess-
ments. This is what large-scale neural network models do
[34]. However, contrastive-trained models measure similar-
ity based on how easily the data can be distinguished, and
any two non-identical samples can be distinguished by ran-
dom features of no conceptual relevance [ 22].
Rather than focusing on ﬁnding the features that best dis-
tinguish the two images, we focus on ﬁnding the ones that
best describe them. Then, we can measure semantic sim-
ilarity based on how well descriptions of one ﬁt the other.
If two samples are similar, a short description should apply
equally well to both. If the samples are very different, the
description of one will be a poor ﬁt for the other. The more
complex the description needed to distinguish the images,
the higher their conceptual similarity. The key idea is to fo-
cus on optimally describing individual samples, rather than
adversarially discriminating them.
Speciﬁcally, referring to Fig. 1, we generate multiple de-
scriptions of each sample in increasing level of complexity,
sorted by their coding length. Then, we measure the dif-
ference of the likelihood of each image conditioned on ei-
ther descriptions as a function of complexity. That traces
a curve that measures distance as a function of complexity.
Any two images can be distinguished by their description
at some point (the more you look, the more differences you
see). Therefore, similarity should always be relative to a de-
scription length. However, when a single number is needed
for comparison, we show that the AUC of the distance func-
tion is well aligned with human similarity assessments.
Our proposed method, which we call Complexity-
Constrained Descriptive Autoencoding, or CC:DAE, is
rooted in the idea of the Kolmogorov Structure Function,
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
11062
ComplexityItalian car brandItalian car brand, red carSingle red car, italian car brand
Conceptual DistanceFigure 1. If we describe each image at increasing levels of complexity (blue and orange text), short descriptions apply equally well to both,
as measured by their likelihood. However, as the complexity level of the description increases, a gap emerges between the likelihood under
the best common description (grey) and the likelihood under the best individual descriptions (blue and orange). For instance, at C=3 6
the best individual descriptions are “Red Fiat 500 car” and “A Ferrari” whereas the best common description is “Italian car brand” which
is not as descriptive. The gap traces two asymmetric curves that measure the conceptual difference between the images at each level of
complexity. A single number can be obtained by measuring the area under between curves.
which was introduced to differentiate semantic ‘informa-
tion’ from structureless ‘noise.’ But while Kolmogorov
used programs to describe the data, and program length as
their complexity, we use natural language descriptions and
their coding length. One may wonder whether our notion
of conceptual distance is canonical in any sense, since it
hinges on arbitrary choices, among which the use of lan-
guage, or even a particular trained language model. Indeed,
as we prove formally, no notion of conceptual similarity can
be canonical: Using results from [ 13], we show that Kol-
mogorov’s choice itself does not convey any information
about the semantics of data and more generally that no non-
trivial notion of common conceptual information can be de-
ﬁned without imposing strong restrictions on the class of
representations and encoder/decoder used. The unavoidable
need to perform subjective analysis is recognized by copy-
right legal doctrines, which provide generalized guidelines
for analyzing similarity but leave the ultimate determination
to be made on a case-by-case basis. As a result, despite ef-
forts to codify similarity into guidelines, one is left with the
impression that there are as many notions of similarity as
there are judges or juries. One of the advantages of large-
scale pre-trained models is that they aggregate content from
vast and diverse corpora and, unlike their human sources,
their biases can be measured, monitored, and calibrated.
Our method applies to general data, including any com-
bination of text and image and any choice of data-to-text en-
coder ( e.g., captioning models) and a text-to-data generative
model ( e.g., multi-modal autoregressive models or diffusion
models). It can also be used to measure similarity of data
across different modalities ( e.g., between text and images).
To evaluate the alignment between our notion of concep-tual similarity and human assessments, we use established
human-annotated similarity benchmarks, and obtain state-
of-the-art results on the STS sentence similarity bench-
marks beating all methods that have not explicitly been
trained on human annotated similarity scores. We also sur-
pass CLIP on the CxC-SIS image similarity benchmark.
2. Related Work
Using machine learning to represent conceptual informa-
tion is a long-running yet elusive goal in machine learn-
ing. For instance, the “concept bottleneck” [ 23] epitomizes
a line of work that generate restricted representations based
on human speciﬁed concepts. Our approach is more gen-
eral: we do not specify speciﬁc concepts, but only de-
mand natural language descriptions with constrained com-
plexity. Lossy compression, for example through capacity-
contrained variational auto-encoders [ 1,8,18], can be used
to distinguish semantic information in images [ 16], but the
success of these methods is largely due to inductive priors
coming from the architecture. In general, there is no reason
to expect compression alone to align with human intuition
of conceptual information, as we demonstrate in Sec. 4. An-
other line of work referring to ‘visual concept learning’ de-
ﬁnes concepts in terms of a set of generalizable properties
that then can be used in conjunction with ‘neuro-symbolic’
programs to answer novel queries [ 30], or in terms of em-
beddings that can be used to generate images [ 27].
Recent work on contrastive vision-language models [ 34]
has demonstrated that these models are surprisingly weak
at understanding semantic relationships [ 10,43], with only
modest improvements from conditional diffusion mod-
11063
els [24]. Generating natural language descriptions of im-
ages as representations was also considered by [ 7], who
ﬁnds the resulting description valuable for a visual question
answering task and more useful than the original images for
a retrieval task.
Embedding generated by auto-regressive models ﬁne-
tuned on human annotations are also frequently used to
quantify similarity [ 12,15,20,32,35,44]. Fewer works
deﬁne similarity using pre-trained auto-regressive models
without supervised ﬁne-tuning: [ 21] uses prompts to con-
dition model outputs; [ 31] evaluates similarity of two sam-
ples via the conditional probability of one given the other.
Most similar to our work, [ 28] deﬁnes semantic similarity as
the divergence between conditional distributions over future
trajectories. Interestingly, [ 28] can be considered a variant
of a special case of our method (Appendix D), which how-
ever further beneﬁts from improved interpretability.
3. CC:DAE
In this section we formalize the critical components of our
method, including the optimal description of the data using
complexity-constrained textual expressions, and the com-
putation of the ﬁt of the description, from which the con-
ceptual distance is measured.
3.1. Optimal Description
Letx2Xbe a sample and let H={h1,h2,...}be
space of hypotheses. We think of each h2Has a pos-
sible description of the sample, for example a natural lan-
guage sentence. Associated with HandXwe consider a
decoder p(x|h)which computes the conditional likelihood
of the input xgiven the description h. We refer to the neg-
ative conditional log-likelihood `(x|h)=  logp(x|h)as
the reconstruction loss of xunder hypothesis h.A k e y
quantity for us is the complexity , orcoding length ,`(h)of
an hypothesis h. By Kraft’s inequality, we can express the
number of bits needed to encode an hypothesis has a nega-
tive log-likelihood `(h)= logpcode(h)for some distribu-
tionpcode(h). Conversely, for a given p(h), a lossless com-
pression scheme whose code-lengths almost exactly match
 logp(h)can be constructed with arithmetic coding.
Example. LetXbe the space of all images and let H
be the set of all English sentences. The cost of encoding
h2Hcan be computed as the negative log-likelihood
 logpcode(h)assigned to hby a trained large language
model (LLM). The log-likelihood logp(x|h)of an image
xgiven a sentence hcan be computed using a conditional
image generation model, such as a diffusion model.
Optimal descriptions. Suppose we want to describe a sam-
plex, but we have an upper-bound `(h)Con the length
of the description hwe can use. To ﬁnd the best descrip-
tionh⇤
x(C)to use under this capacity constraint, we have to
Figure 2. These two images have similar art styles, theme, and
subject matter. On the other hand, it is difﬁcult to identify spe-
ciﬁc visual elements that appear in both images. These two images
were found to be “substantially similar” [ 37] based on the arrange-
ment of similar features in a similar way. How can we measure
how similar these images are?
solve the constrained optimization problem:
h⇤
x(C) = arg min
h2H`(x|h) (1)
s.t. logpcode(h)C,
which aims at ﬁnding the hypothesis hwith `(h)C
which minimizes the reconstruction error for x. This def-
inition however has practical limitations: since the space H
is discrete, the optimization in Eq. ( 1) cannot be performed
easily with gradient-based methods, leading to an expensive
search over the (potentially inﬁnite) hypothesis space H.
Stochastic relaxation. To simplify the problem, rather
than considering the best single hypothesis, we can search
for a distribution of hypotheses q(h)describing the image.
This corresponds to a stochastic relaxation of the problem
where we ﬁnd a low-complexity distribution of hypotheses
q⇤
x(h|C)that, on average, have good reconstruction loss:
q⇤
x(h|C) = arg min
q(h)2P(H)Eh⇠q(h)[`(x|h)] (2)
s.t.KL 
q(h)kpcode(h) 
C,
where the optimization is now over the space of distribu-
tions q(h)2P (H). When q(h)is restricted to being a
Dirac delta over a single hypothesis — i.e., q(h)= h0(h)
— we exactly recover Eq. ( 1). A natural question is in
which way the newly introduced term KL 
q(h)kp(h) 
generalizes the description length  logpcode(h)of Eq. ( 1),
aside from reducing to it in the extreme case where
q(h)is a Dirac delta. To answer, it can be shown that
KL 
q(h)kp(h) 
is the expected number of bits required
11064
to specify a fair sample of the distribution q(h)to a re-
ceiver that knows p(h)but not q(h)[17, Lemma 1.5]. For
this reason, we refer to Eq. ( 2) as a stochastic relaxation
of Eq. ( 1). The advantage of this formulation is that the
solution to the optimization problem can be efﬁciently ap-
proximated by sampling a large language model, as we will
discuss later. This formulation also connects to other frame-
works (see Sec. 4).
3.2. Conceptual Similarity
As anticipated in Section 1, given two sample x1andx2,
we want to deﬁne a notion of conceptual similarity by mea-
suring how well optimal descriptions of x1also apply to
x2— and vice versa – as the complexity of the descrip-
tion increases. To formalize this notion, let qi(h|C)denote
the family of distributions of optimal descriptions hfor the
sample xiasCvaries. We can deﬁne the function  2!1(C)
 2!1(C): = Eq1[`(x1|h)] Eq2[`(x1|h)]. (3)
which measures how well the description q2ofx2describe
x1, compared to its optimal description q1. Similarly, we
can deﬁne  1!2(C)inverting the role of x1andx2.
Conceptual Distance. We deﬁne the conceptual distance
between x1andx2at complexity level Cas
dx1,x2(C)=1
2⇣
 2!1(C)+  1!2(C)⌘
. (4)
Figure 1provides a sample illustration of the typical be-
havior of this distance. For low values of C(y-axis), the
optimal descriptions of one sample — e.g., “a red car” — is
an equally good description of both. In this case, both terms
 i!j(blue and orange curves in the plot) will be zero. As
the complexity Cincreases, the optimal description of one
sample — e.g., “a Ferrari” — is not an optimal description
for the other and the distance becomes non-zero, and will
keep increasing as the descriptions become more detailed
and speciﬁc.
AUC. Rather than being a single number, our deﬁnition of
conceptual distance is a function of the complexity Cof the
description. As discussed in Section 4, this is necessary to
address some key issues in deﬁning distances and we posit
that human perception of similarity relates to how quickly
this function grows. To capture this, when a single number
is necessary to deﬁne a distance, we use the Area Under
Curve of the graph, up to some vale Cmax.
Asymmetric distances. The distance dx1,x2(C)in Eq. ( 4)
in written in terms of two asymmetric distances , the dis-
tance  1!2ofx1from x2, and  2!1from x2tox1. While
dx1,x2(C)provides an overall measure of the distance be-
tween samples, each individual asymmetric distances can
be useful to measure directed containment relations, for in-
stance to what extent a work is being subsumed by or su-
persedes another.Interpretation as intersection. We note that, rearranging
the terms, Eq. ( 4) can also be rewritten as:
dx1,x2(C)=Eq1[`(x1|h)] + Eq2[`(x2|h)]
 Eq\[`(x1|h)+`(x2|h)] (5)
where q\(h|C)=1
2q1(h|C)+1
2q2(h|C). This compares
the reconstruction loss obtained by using different optimal
descriptions for x1andx2and the reconstruction loss ob-
tained by encoding both x1andx2using the same descrip-
tions h⇠q\(h|C). Intuitively, if the samples are similar, a
shared description equally described each image when com-
pared to picking the optimal descriptions independently.
However, as the complexity of Cincreases, q1(h|C)and
q2(h|C)will contain increasing details that describe one
sample x1(respectively x2) but not equally well the other.
In that case q\becomes suboptimal and the distance grows
accordingly.
Interpretability. An advantage of our deﬁnition of con-
ceptual distance is that the descriptions hthat have high
probability under q\provide an interpretable explanation
for why the two samples should be considered similar (e.g.,
gray captions in Figure 1). Conversely, looking at hthat
are likely under qibut not q\provides an explanation of the
unique information in qi, justifying why q1andq2should
be considered different.
4. Motivation and Discussion
Deﬁning a conceptual distance face three main challenges:
(i)Randomness dominates. Any pair of non-identical
real images has a large number of small differences
due to randomness which dominates over the few, but
important, structural similarities;
(ii)Canonical importance of properties. What proper-
ties are important for similarity is non-canonical;
(iii) Adversarial discrimination. Any two images may
be “adversarially” distinguished by simple and obvi-
ous properties (e.g., “the color of the car is different”)
but ease of discriminability should not affect similar-
ity(two pictures of the same car model should be sim-
ilar, even if the color is different).
Corresponding to these challenges there have are three main
classes of distance functions that address some of the prob-
lems, but fail to capture the others.
Contrastive learning. CLIP-like contrastive learning mod-
els by their nature rely on discriminative features that are
optimized to discriminate samples, not to describe concepts
that humans evince from said samples, thus failing (iii).
Since these representations are not trained to align with hu-
man conceptual representations, they are a poor ﬁt to mea-
sure conceptual similarity as noted in recent works [ 10,43].
Indeed, we show in Tab. 2that our method signiﬁcantly
11065
outperforms CLIP in outputting human-aligned similarity
scores for images. The method also fails (ii) since the
model learns semantically relevant properties either through
semi-supervised image-text or enforced through hard-coded
strong data augmentation.
Information theoretic distances , such as the Normalized
Compression Distance (NCD), move away from discrim-
inability and instead deﬁne a canonical notion of similarity
between samples as the ratio between the amount of shared
(algorithmic) information and the total information. How-
ever, reﬂecting (i), unique randomness in the data accounts
for most of the information in high dimensional samples.
Hence, even when samples are perceptually similar, random
information will dominate the distance and similarity of key
semantic information is lost (Appendix A).
Structure function. Deﬁning a distance for high-
dimensional data requires separating “structural” informa-
tion of the sample from the information due to randomness
and noise, raising the question of how to canonically deﬁne
what constitutes “structure” and what “noise”. Kolmogorov
proposed that describing structural information should re-
duce the reconstruction loss of a sample more quickly than
describing random details which are intrinsically incom-
pressible. This is captured by the Structure Function  x(C),
deﬁned as
 x(C)=m i n
h2H logph(x) (6)
s.t.|h|C,
which measures the reconstruction error when compressing
xusing a program h2Himplementing a computable prob-
ability distribution ph(x), as the length |h|of the program
varies. Since the optimal descriptions have to reduce the
reconstruction loss as quickly as possible, for low values
ofCthey can only contain structural information and not
random information. This reasoning can be formalized [ 39]
and allows to deﬁne a notion of algorithmic minimal suf-
ﬁcient statistic that captures structural but not the random
information. Note that this directly relates to our Eq. ( 1),
using pcode(h)=2 |h|, with the only difference that the
optimization is done over programs instead of sentences.
One of the main contributions of our work is to show that
something akin to the structure function can be used to de-
ﬁne a notion of conceptual distance, and to introduce a more
computationally feasible stochastic relaxation of the frame-
work. However, in its pure form this approach still fails. It
can be shown that algorithmic minimal sufﬁcient statistics
are trivial for most data [ 13, Corollary III.13] and, when
non trivial, they do not capture useful semantic information
but rather capture the mutual information between the sam-
ple and the halting problem [ 13, Theorem III.24]. As we
show later, this is a symptom of a more general problem
where no expressive enough class of descriptions (such asgeneral programs) can deﬁne non-trivial semantic informa-
tion. This motivates our choice to restrict descriptions to
(non-canonical) subclasses such as language sentences
Conceptual Distance. Our method attempts to tackle all
the problems (i)-(iii). First, it does not focus on discriminat-
ing samples. Rather we independently ﬁnd the descriptions
of each sample, and then evaluate them on each other. This
prevents the method from adversarially picking features that
may be good discriminators, but that are not good descrip-
tors – thus addressing (iii). However, we need to ensure that
the description will focus on structural properties of the im-
age, and not on random details. Similarly to Kolmogorov’s
Structure Function approach, we accomplish this by ﬁnd-
ing optimal descriptions under a capacity constraint which
naturally leads the distance to ignore differences due to in-
compressible randomness for small values of C.
Aside from increasing robustness to unimportant differ-
ences, focusing on the initial part of the curve as Cgrows
has other advantages. We posit that human perception of se-
mantic similarity relates to how quickly the distance func-
tion grows, rather than its asymptotic value. This motivates
our choice of using the AUC of dx1,x2(C)up to some small
value of Cto measure similarity. Indeed, in our experiments
we observe that the AUC computed up to some relatively
small value of Cis better aligned with human similarity an-
notations than the exact value at any speciﬁc C, and that
using larger values of Cdecreases alignment.
Our distance implicitly deﬁnes a separation between
structural and random components of the images. This de-
pends on their “compressibility” and is in turn dependent
on the choice of hypothesis space, encoder and decoder.
Our choice of using language as the hypothesis space, and
a particular conditional generative model, is not canonical
prompting the question of whether a more canonical choice,
like Kolmogorov’s choice of using generic programs, would
be better. However, as the following theorem shows, any
model class which is too expressive would not be able to
recover a meaningful notion of structural information.
Theorem 4.1 (No canonical deﬁnitions of structure, Ap-
pendix C).LetHbe a class of hypotheses and let p(x|h)be
the corresponding decoder. If the decoder p(x|h)is expres-
sive enough to perform perfect test-time optimization, then
all samples have the same structure, and the conceptual dis-
tance between any pair of samples is zero.
The lack of useful structure emerging from general pro-
gram classes motivated us to consider an appropriate sub-
class that could lead to a more useful, though necessarily
non-canonical as expressed in property (ii), measure of con-
ceptual similarity. Copyright doctrine acknowledges the
absence of a computable notion of similarity, and instead
relies on judges and juries to adjudicate each case. We
analogize this human decision-making process to a com-
11066
Zero-Shot Method STS-B STS12 STS13 STS14 STS15 STS16 SICK-R AvgParagonCLIP-ViTL14 [34] 65.5 67.7 68.5 58.0 67.1 73.6 68.6 67.0SimCSE-BERT [15]68.4 82.4 74.4 80.9 78.6 76.9 72.2 76.3LLaMA [38]Cond. Likelihood 44.3 20.8 51.8 38.6 56.0 50.9 56.7 45.6Meaning as Trajectories [28] 70.6 52.5 65.9 53.2 67.874.173.0 65.3Ours72.3 56.7 67.9 56.9 68.7 74.1 74.3 67.3Table 1. Text-to-Text semantic similarity benchmarks. We evaluate our method using Spearman correlation ( ⇥100) on the STS sentence
similarity benchmark. Our method outperforms all zero-shot methods based on vector embeddings. On the SICK-R dataset involving
compositional knowledge, conceptual similarity outperforms even the contrastive-trained paragon by 2.1%. This shows that our deﬁnition
of Conceptual Distance is better aligned with human judgement than standard vector embedding. It also outperforms [ 28], which can be
seen as a particular case of our method for a ﬁxed value of  =1, while greatly improving interpretability. This shows the importance of
considering different levels of capacity to recover human judgement.
putable machine-learning process, in which natural lan-
guage programs attempt to describe and distinguish the
works in question. To operationalize this notion, we re-
framed Kolmogorov complexity in Sec. 3to use natural lan-
guage as opposed to formal language programs for encod-
ing inputs. Natural language descriptions are generated by
models that aggregate subjective assessments not just of a
single judge or a small group of jurors, but from textual ex-
pressions of the millions of individual contributions used to
train large-scale models. While our approach makes a sub-
jective choice of using natural language as a representation
of the data, such a representation is naturally ﬁt to the task,
since natural language is routinely used, and arguably has
evolved, to express and communicate abstract concepts.
5. Practical distance computation
To compute our conceptual distance we need to solve the
constrained optimization problem Eq. ( 2). The solution can
easily be written using the Lagrange multiplier method (Ap-
pendix F) and leads to the family of optimal distributions:
q⇤
x(h|C)=1
Z pcode(h)p(x|h) , (7)
where Z =Epcode(h)[p(x|h) ]is the normalization factor
and = (C) 0has to satisfy the condition:
KL 
q⇤
x(h|C)kpcode(h) 
=C. (8)
Having q⇤
x(h|C)we can compute the various quantities in
the deﬁnition of the distance. However, given that q⇤
x(h|C)
is a distribution over an inﬁnite hypothesis space H, we can-
not represent it explicitly. Fortunately, all the quantities in-
volved are in the form Eq[f(h)]for some function f(h)and
can be estimated through importance sampling. Let ⇡(h)
be a proposal distribution and let h1,...,h N⇠⇡(h)be
samples from it. Then we have (Appendix F):
Eq⇤[f(h)] = E⇡h
q⇤(h)
⇡(h)f(h)i
⇡PN
i=1↵ ,if(hi)
↵ ,i:= softmax⇣
  `(x|hj) + logpcode(hj)
⇡(hj)⌘
iHence, computing the importance weights ↵ ,ifor each
sample hi(all needed quantities are available) we can easily
compute an unbiased estimator of our distance function.
Using only the encoder model Typically, we have access
to an encoder model p(h|x), which outputs description h
given the data x—e.g., a captioning model — but may not
have a corresponding generative model p(x|h)to evaluate
the likelihood of the data given the description. This prob-
lem can be circumvented. By Bayes’s rule we have
p(x|h)=p(h|x)p(x)
p(h).
The encoder model directly provides p(h|x). The likeli-
hood p(x)of the data is however is difﬁcult to evaluate for
high-dimensional data. Luckily, the role of p(x)simpliﬁes
in our deﬁnition of distance (Appendix E) so the term is
not needed. Finally, p(h)— which should not be confused
with pcode(h)— is the marginal p(h)=P
x2Xp(h|x)p(x),
which can be estimated through sampling. In practice how-
ever, we found that the simple approximation: p(h)⇡
1
2p(h|x1)+1
2p(h|x2)performs well and can be computed
for free using only the available two samples.
Choice of ⇡(h).To reduce the variance of importance sam-
pling, the proposal distribution ⇡(h)should be well aligned
with the distributions we want to compute the expectation
with, in our case q1(h|C)andq2(h|C). Recall that
qi(h|C)=1
Z pcode(h)⇣p(h|x)
p(h)⌘ (C)
which for  (C)=1 simpliﬁes to the posterior p(h|xi), and
will generally be close to it at most important values of  ,
making it a good choice for the proposal distribution. Since
we want to be able to use the same samples for both q1(h|C)
andq2(h|C), we deﬁne the proposal distribution as:
⇡(h): =1
2p(h|x1)+1
2p(h|x2).
Sampling from this distribution can be achieved easily by
samplingN
2+N
2samples from p(h|x1)andp(h|x2).
11067
Choice of pcode(h).The choice of the encoding distribution
pcode(h)can be used to align the conceptual distance with
human similarity assessments. A natural choice is to take
pcode(h)to be the likelihood assigned to the description h
by a LLM. Another choice, which is more natural from a
computational perspective is to take pcode(h)=⇡(h)in or-
der to reduce the variance of the importance sampling esti-
mation. We found the ﬁrst choice to give more interpretable
results (and we use it in our qualitative plots), whereas the
latter choice performs better on large scale benchmarks.
6. Experiments
The goal of this section is to evaluate how well our notion of
conceptual distance correlates with ground-truth human an-
notators. To this end, we empirically compare samples un-
der three different domains — text-to-text, image-to-image,
and text-to-image (cross-modal) similarity.
Model choice. To implement our proposed similarity score,
we require a model pcode(h)to compute code length of
text descriptions h, as well as conditional distributions like
p(h|x), where xcan represent text or images. For text
encoding we use pcode(h)=1
2 
p(h|x1)+ p(h|x2) 
on
benchmarks, while on qualitative experiments we use the
likelihood of houtputted by LLaMA-13B [ 38]. We also
use LLaMA-13B as conditional text encoder p(h|x), while
when xis an image we use LLaV A [ 26]. In the latter case,
images are encoded as a sequence of tokens using a vision
transformer, and then fed into a general sequence trans-
former that is trained on both image and text tokens.
Importance sampling. For comparing a pair of samples
(x1,x2), we use descriptions Hsample =H1[H2sampled
from the proposal distribution ⇡(h), where H1⇠p(h|x1)
andH2⇠p(h|x2)are the set of descriptions sampled from
x1andx2respectively. To ensure fair comparison, we use
the same sampling procedure and hyperparameter choices
on [28] and generate from each input 20descriptions of 20
tokens through multinomial sampling.
Sentence similarity. First, we establish whether CC:DAE
is able to compute a meaningful notion of conceptual sim-
ilarity between sentences which aligns with ground-truth
human annotations. Towards this end, we leverage the Se-
mantic Textual Similarity (STS) benchmark [ 2–6,9,29],
where each sentence pair ( x1,x2) is labelled with a similar-
ity score attributed by human annotators. To quantify this
alignment, we measure the Spearman correlation between
our conceptual distance and the ground-truth scores. In par-
ticular, for each input sentence x, we sample continuations
h⇠⇡(h)=1
2(p(h|x1)+p(h|x2))of the two sentences,
which we use as descriptions to estimate their distance. Our
results in Tab. 1demonstrates that our method correlates
well with human judgements. In particular, conceptual sim-Method CxC-SIS CxC-SITS Average
CLIP-ViTB/16 72.08 64.60 68.34
Cond. Likelihood - 29.46 -
Meaning as Trajectories [ 28] 81.47 67.63 74.55
Ours 81.44 67.71 74.58
Table 2. Image-Image/Image-Text Semantic similarity bench-
marks. Evaluation on human-alignment of similarity scores on the
CxC Semantic Image Similarity (SIS) and Semantic Image-Text
Similarity (SITS) benchmarks. CC:DAE outperforms CLIP, which
is trained explicitly on the multi-modal similarity task, showing
that CC:DAE better captures human intuition compared to con-
trastive methods. On CxC-SITS the method outperforms baselines
using the same backbone and, by leveraging the alignment prompt
used by [ 28], our method achieves comparable performance while
beneﬁting from enhanced interpretability.
ilarity outperforms all methods in the literature that have
not been explicitly trained on human annotations, with a
relative decrease in average error by  5.8%compared to
the next best method, which also uses the same backbone.
Moreover, our method outperforms CLIP-ViTL14, which as
been trained explicitly on contrastive objectives. Lastly, on
the SICK-R (Sentences Involving Compositional Knowl-
edge) dataset, conceptual similarity achieves state-of-the-
art performance among zero-shot methods, showing that
our method excels at representing compositional structures
present in the data.
Image similarity. Next, we want to determine whether our
conceptual distances correlates with that of human annota-
tors when used to compare images. To accomplish this, we
leverage the Crisscrossed Captions Semantic Image Simi-
larity (CxC-SIS) [ 33] benchmark containing pairs of images
annotated by humans with similarity scores ranging from 0
to 5. To evaluate our method, we measure the Spearman
correlation of conceptual distance and human annotations.
In Tab. 2, we show that our method outperforms all prior
zero-shot methods on CxC-SIS, including reducing the er-
ror 33.5%relative to CLIP. This supports the idea that our
method, based on comparing samples via descriptions, fares
better than methods based on discrimination.
Cross-modal similarity. We further investigate whether
conceptual similarity can be applied when inputs do not
come from a common modality. In particular, we compare
on the CxC-SITS Semantic Image-Text Similarity (CxC-
SITS) benchmark, which provide human annotated scores
on (image, text) pairs based on their semantic similarity.
Our experiments in Tab. 2demonstrate that our method in-
deed generalizes well to cross-modal comparisons, reduc-
ing error relative to CLIP by  8.8%. To further judge the
improvement of our method over the base capability of the
backbone, on CxC-SITS we compare against a baseline that
selects captions by directly comparing the conditional like-
11068
Figure 3. Role of prompts. Consider the three images above: which pair is most similar? This depends if we focus on content — the ﬁrst
two depict Notre-Dame, the third a boat on the Seine — or on the style/artistic technique — the ﬁrst is a photograph, the second and third
are paintings in the pointillist style. By changing the prompt (“Describe the style of the image” or “Describe the content of the image”),
the user can bias p(h|x)to focus the conceptual distance on one or the other aspect. Note that images A and B are closer under the content
prompt, but B and C are closer under the style prompt.
lihood of the text given the image. We note that this base-
line fares poorly, which is aligned with results showing that
perplexity is be a poor metric for caption alignment [ 41].
CC:DAE bypasses this limitation by autoencoding both text
and images in a common text description space. CC:DAE
also matches or outperform — both in Tabs. 1and2— the
best zero-shot method [ 28], which can be seen as a very
particular case of our method (Appendix D) while it bene-
ﬁts from enhanced interpretability. Our method also outper-
forms [ 28] and comparable baselines on the more challeng-
ing SugarCrepe benchmark (Appendix B).
Qualitative results. In Figs. 1and3we illustrate the behav-
ior of our conceptual distance. To generate richer and more
interpretable samples from the description space — and to
further highlight the trade-off between coding length Cand
distance — rather than sampling directly h⇠p(h|xi)we
use a beam search to generate effective descriptions of in-
creasing length (Appendix G). For visualization purposes,
rather than optimizing a distribution, we restrict the opti-
mization to select only the single best performing descrip-
tion among the samples for each Cand display it. In all
cases, we see that as expected the distance is zero when re-
stricted to short description (small C). As descriptions be-
come more detailed, the distance starts increasing. Each in-
crease in distance can be interpreted looking at the selected
h. While selected descriptions highlight the differences, we
can also look at the best description hexplaining both im-
ages at the same time (see Appendix) to understand what
common structure the two images contain which justiﬁes
why the distance is not larger.
Prompting. Conceptual distance is inﬂuenced by thechoice of the description space Hand the decoder p(x|h),
or equivalently the encoder p(h|x). In Sec. 4we proved that
making such non-canonical choices is inevitable. In Fig. 3
we show that freedom to modify the encoder probability
— for example through prompting — is indeed not a dis-
advantage but a helpful option. Different prompts such as
“Describe the style of the image” or “Describe the content
of the image” allow the user to easily bias the conceptual
distance to focus on the similarity axis of interest.
7. Conclusions
We introduce CC:DAE, a principled notion of conceptual
similarity which aims to explore and resolve past issues
in the deﬁnition of semantic distance between high dimen-
sional data. Emprically, the method achieves state-of-the-
art results for matching human similarity judgments without
ﬁne-tuning on human scores. We built on on Kolmogorov’s
central insight that structured information should be easier
to explain than randomness. While using general programs
as explanations does not yield acceptable results, selecting
natural language sequences as our “programs” is enough
to spotlight the properties in images and text that are rel-
evant to humans. Our general strategy can ﬂexibly incorpo-
rate other types of explanations, including visual features,
to specify aspects of conceptual similarity that are relevant
for speciﬁc contexts. In our current implementation, the
description space is limited to textual description. This pre-
vents the method from efﬁciently describing similarities de-
riving from similar visual arrangement, which is however
an important space. We leave to future works extension of
the method using more generic decoders that can take both
textual and visual features in input.
11069
References
[1]Alessandro Achille and Stefano Soatto. Information dropout:
Learning optimal representations through noisy computa-
tion. IEEE transactions on pattern analysis and machine
intelligence , 40(12):2897–2905, 2018. 2
[2]Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-
Agirre. Semeval-2012 task 6: A pilot on semantic textual
similarity. In * SEM 2012: The First Joint Conference on
Lexical and Computational Semantics–Volume 1: Proceed-
ings of the main conference and the shared task, and Volume
2: Proceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012) , pages 385–393, 2012.
7
[3]Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. * sem 2013 shared task: Semantic
textual similarity. In Second joint conference on lexical and
computational semantics (* SEM), volume 1: proceedings of
the Main conference and the shared task: semantic textual
similarity , pages 32–43, 2013.
[4]Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer,
Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mi-
halcea, German Rigau, and Janyce Wiebe. Semeval-2014
task 10: Multilingual semantic textual similarity. In Proceed-
ings of the 8th international workshop on semantic evalua-
tion (SemEval 2014) , pages 81–91, 2014.
[5]Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer,
Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Inigo
Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, et al.
Semeval-2015 task 2: Semantic textual similarity, english,
spanish and pilot on interpretability. In Proceedings of the
9th international workshop on semantic evaluation (SemEval
2015) , pages 252–263, 2015.
[6]Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor
Gonzalez Agirre, Rada Mihalcea, German Rigau Claramunt,
and Janyce Wiebe. Semeval-2016 task 1: Semantic tex-
tual similarity, monolingual and cross-lingual evaluation. In
SemEval-2016. 10th International Workshop on Semantic
Evaluation; 2016 Jun 16-17; San Diego, CA. Stroudsburg
(PA): ACL; 2016. p. 497-511. ACL (Association for Compu-
tational Linguistics), 2016. 7
[7]Maxime Bucher, St ´ephane Herbin, and Fr ´ed´eric Jurie. Se-
mantic bottleneck for computer vision tasks. In Computer
Vision–ACCV 2018: 14th Asian Conference on Computer
Vision, Perth, Australia, December 2–6, 2018, Revised Se-
lected Papers, Part II 14 , pages 695–712. Springer, 2019. 3
[8]Christopher P Burgess, Irina Higgins, Arka Pal, Loic
Matthey, Nick Watters, Guillaume Desjardins, and Alexan-
der Lerchner. Understanding disentangling in beta-vae.
arXiv preprint arXiv:1804.03599 , 2018. 2
[9]Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio,
and Lucia Specia. Semeval-2017 task 1: Semantic textual
similarity-multilingual and cross-lingual focused evaluation.
arXiv preprint arXiv:1708.00055 , 2017. 7
[10] Colin Conwell and Tomer Ullman. Testing relational un-
derstanding in text-guided image generation. arXiv preprint
arXiv:2208.00005 , 2022. 2,4[11] Thomas M Cover. Elements of information theory . John
Wiley & Sons, 1999. 1
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 3
[13] P´eter G ´acs, John T Tromp, and Paul MB Vit ´anyi. Algorith-
mic statistics. IEEE Transactions on Information Theory , 47
(6):2443–2463, 2001. 2,5,13
[14] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan
Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,
Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Dat-
acomp: In search of the next generation of multimodal
datasets. arXiv preprint arXiv:2304.14108 , 2023. 12
[15] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse:
Simple contrastive learning of sentence embeddings. arXiv
preprint arXiv:2104.08821 , 2021. 3,6
[16] Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo
Danihelka, and Daan Wierstra. Towards conceptual com-
pression. Advances In Neural Information Processing Sys-
tems, 29, 2016. 2
[17] Prahladh Harsha, Rahul Jain, David McAllester, and Jaiku-
mar Radhakrishnan. The communication complexity of cor-
relation. In Twenty-Second Annual IEEE Conference on
Computational Complexity (CCC’07) , pages 10–23. IEEE,
2007. 4
[18] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess,
Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and
Alexander Lerchner. beta-vae: Learning basic visual con-
cepts with a constrained variational framework. Interna-
tional conference on learning representations , 2016. 2
[19] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kem-
bhavi, and Ranjay Krishna. Sugarcrepe: Fixing hackable
benchmarks for vision-language compositionality. arXiv
preprint arXiv:2306.14610 , 2023. 12
[20] Ting Jiang, Jian Jiao, Shaohan Huang, Zihan Zhang, De-
qing Wang, Fuzhen Zhuang, Furu Wei, Haizhen Huang,
Denvy Deng, and Qi Zhang. Promptbert: Improving
bert sentence embeddings with prompts. arXiv preprint
arXiv:2201.04337 , 2022. 3
[21] Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing Wang,
and Fuzhen Zhuang. Scaling sentence embeddings with large
language models. arXiv preprint arXiv:2307.16645 , 2023. 3
[22] William B Johnson and J Lindenstrauss. Extensions of Lip-
shitz mapping into Hilbert space . 1984. 1
[23] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen
Mussmann, Emma Pierson, Been Kim, and Percy Liang.
Concept bottleneck models. In International conference on
machine learning , pages 5338–5348. PMLR, 2020. 2
[24] Xianghao Kong, Ollie Liu, Han Li, Dani Yogatama, and
Greg Ver Steeg. Interpretable diffusion via information de-
composition. arXiv preprint arXiv:2310.07972 , 2023. 3,13
[25] Ming Li, Paul Vit ´anyi, et al. An introduction to Kolmogorov
complexity and its applications . Springer, 2008. 1,11
[26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. arXiv preprint arXiv:2304.08485 ,
2023. 7,12
11070
[27] Nan Liu, Yilun Du, Shuang Li, Joshua B Tenenbaum, and
Antonio Torralba. Unsupervised compositional concepts dis-
covery with text-to-image generative models. arXiv preprint
arXiv:2306.05357 , 2023. 2
[28] Tian Yu Liu, Matthew Trager, Alessandro Achille, Pramu-
ditha Perera, Luca Zancato, and Stefano Soatto. Meaning
representations from trajectories in autoregressive models.
arXiv preprint arXiv:2310.18348 , 2023. 3,6,7,8,12,13,
14
[29] Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zamparelli.
Semeval-2014 task 1: Evaluation of compositional distribu-
tional semantic models on full sentences through semantic
relatedness and textual entailment. In Proceedings of the
8th international workshop on semantic evaluation (SemEval
2014) , pages 1–8, 2014. 7
[30] Lingjie Mei, Jiayuan Mao, Ziqi Wang, Chuang Gan, and
Joshua B Tenenbaum. Falcon: fast visual concept learning
by integrating images, linguistic descriptions, and concep-
tual relations. arXiv preprint arXiv:2203.16639 , 2022. 2
[31] Niklas Muennighoff. Sgpt: Gpt sentence embeddings for
semantic search. arXiv preprint arXiv:2202.08904 , 2022. 3
[32] Jianmo Ni, Gustavo Hern ´andez ´Abrego, Noah Constant, Ji
Ma, Keith B Hall, Daniel Cer, and Yinfei Yang. Sentence-
t5: Scalable sentence encoders from pre-trained text-to-text
models. arXiv preprint arXiv:2108.08877 , 2021. 3
[33] Zarana Parekh, Jason Baldridge, Daniel Cer, Austin Wa-
ters, and Yinfei Yang. Crisscrossed captions: Extended in-
tramodal and intermodal semantic similarity judgments for
ms-coco. arXiv preprint arXiv:2004.15020 , 2020. 7
[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 1,2,6,12
[35] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence
embeddings using siamese bert-networks. arXiv preprint
arXiv:1908.10084 , 2019. 3
[36] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for training
next generation image-text models. Advances in Neural In-
formation Processing Systems , 35:25278–25294, 2022. 12
[37] Steinberg v. Columbia Pictures Industries, Inc. 663 F. Supp.
706, (S.D.N.Y . 1987). 1,3
[38] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efﬁcient foundation language models.
arXiv preprint arXiv:2302.13971 , 2023. 6,7
[39] Nikolai K Vereshchagin and Paul MB Vit ´anyi. Kol-
mogorov’s structure functions and model selection. IEEE
Transactions on Information Theory , 50(12):3265–3290,
2004. 5,13[40] Paul Viola and William M Wells III. Alignment by max-
imization of mutual information. International journal of
computer vision , 24(2):137–154, 1997. 1
[41] Yequan Wang, Jiawen Deng, Aixin Sun, and Xuying Meng.
Perplexity from plm is unreliable for evaluating text quality.
arXiv preprint arXiv:2210.05892 , 2022. 8
[42] Satoshi Watanabe. Knowing and guessing: A quantitative
study of inference and information. 1969. 13
[43] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri,
Dan Jurafsky, and James Zou. When and why vision-
language models behave like bags-of-words, and what to
do about it? In The Eleventh International Conference on
Learning Representations , 2022. 2,4
[44] Yan Zhang, Ruidan He, Zuozhu Liu, Kwan Hui Lim,
and Lidong Bing. An unsupervised sentence embedding
method by mutual information maximization. arXiv preprint
arXiv:2009.12061 , 2020. 3
11071
