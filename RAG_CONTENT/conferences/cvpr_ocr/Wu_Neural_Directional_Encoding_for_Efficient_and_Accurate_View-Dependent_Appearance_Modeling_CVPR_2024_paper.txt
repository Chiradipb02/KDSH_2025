Neural Directional Encoding
for Efﬁcient and Accurate View-Dependent Appearance Modeling
Liwen Wu1*Sai Bi2Zexiang Xu2Fujun Luan2Kai Zhang2
Iliyan Georgiev2Kalyan Sunkavalli2Ravi Ramamoorthi1
1UC San Diego2Adobe Research
Abstract
Novel-view synthesis of specular objects like shiny met-
als or glossy paints remains a signiﬁcant challenge. Not
only the glossy appearance but also global illumination
effects, including reﬂections of other objects in the envi-
ronment, are critical components to faithfully reproduce a
scene. In this paper, we present Neural Directional En-
coding (NDE), a view-dependent appearance encoding of
neural radiance ﬁelds (NeRF) for rendering specular ob-
jects. NDE transfers the concept of feature-grid-based spa-
tial encoding to the angular domain, signiﬁcantly improv-
ing the ability to model high-frequency angular signals. In
contrast to previous methods that use encoding functions
with only angular input, we additionally cone-trace spa-
tial features to obtain a spatially varying directional en-
coding, which addresses the challenging interreﬂection ef-
fects. Extensive experiments on both synthetic and real
datasets show that a NeRF model with NDE (1) outper-
forms the state of the art on view synthesis of specular
objects, and (2) works with small networks to allow fast
(real-time) inference. The source code is available at:
https://github.com/lwwu2/nde
1. Introduction
Some of the most compelling appearances in our visual
world arise from specular objects like metals, plastics,
glossy paints, or silken cloth. Faithfully reproducing these
effects from photographs for novel-view synthesis requires
capturing both geometry and view-dependent appearance.
Recent neural radiance ﬁeld (NeRF) [ 37] methods have
made impressive progress on efﬁcient geometry represen-
tation and encoding using learnable spatial feature grids
[5,7,29,39,45,53]. However, modeling high-frequency
view-dependent appearance has achieved much less atten-
tion. Efﬁcient encoding of directional information is just
as important, for modeling effects such as specular high-
lights and glossy interreﬂections. In this paper, we present
a feature-grid-like neural directional encoding (NDE) that
can accurately model the appearance of shiny objects.
*This work was partially done during an internship at Adobe Research.NDE (ours) Ground truth
ENVIDR [ 26] Ref-NeRF [ 48]NDE (ours) Ground truth
0.52 FPS 0.02 FPS 75 FPS
Figure 1. Ours vs. analytical encoding. Methods like Ref-
NeRF [ 48] use an analytical function to encode viewing directions
in large MLPs, failing to model complex reﬂections (column 1-2
of the insets). Instead, we encode view-dependent effects into fea-
ture grids with better interreﬂection parameterization, successfully
reconstructing the details on the teapot and even multi-bounce re-
ﬂections of the pink ball (3rd column of the insets) with little com-
putational overhead (75 FPS on an NVIDIA 3090 GPU).
View-dependent colors in NeRFs ( e.g. [48]) are com-
monly obtained by decoding spatial features and encoded
direction. This approach necessitates a large multi-layer
perceptron (MLP) and exhibits slow convergence with ana-
lytical directional encoding functions. To that end, we bring
feature-grid-based encoding to the directional domain, rep-
resenting reﬂections from distant sources via learnable fea-
ture vectors stored on a global environment map (Sec. 4.1).
Features localize signal learning, reducing the MLP size re-
quired to model high-frequency far-ﬁeld reﬂections.
Besides far-ﬁeld reﬂections, spatially varying near-ﬁeld
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
21157
interreﬂections are also key effects in rendering glossy
objects. These effects cannot be accurately modeled by
NeRF’s spatio-angular parameterization whose directional
encoding does not depend on the position. In contrast, we
propose a novel spatio-spatial parameterization by cone-
tracing a spatial feature grid (Sec. 4.2) to encode near-ﬁeld
reﬂections. The cone tracing accumulates spatial encodings
along the queried direction and position, thus it is spatially
varying. While prior works consider only single-bounce
or diffuse interreﬂections [ 26], our representation is able to
model general multi-bounce reﬂection effects.
Overall, our neural directional encoding (NDE) achieves
both high-quality modeling of view-dependent effects and
fast evaluation. Figure 1demonstrates NDE incorpo-
rated into NeRF, showing (1) accurate rendering of spec-
ular objects—a difﬁcult challenge for the state of the art
(Sec. 5.1), and (2) high inference speed that can be pushed
to real-time without obvious quality loss (Sec. 5.2).
2. Related work
Novel-view synthesis aims to render a 3D scene from un-
seen views given a set of image captures with camera poses.
Neural radiance ﬁelds (NeRF) [ 37] has recently emerged
as a promising solution to this task, utilizing an implicit
scene representation and volume rendering to synthesize
photorealistic images. Follow-up works achieve state-of-
the-art results in this area, for unbounded scenes [ 1,59],
in-the-wild captures [ 34], and sparse- or single-view recon-
struction [ 6,14,28,46,47,51]. While the original NeRF
method [ 37] is computationally inefﬁcient, it can be visu-
alized in real-time by baking the reconstruction into voxel-
[12,15,43,57] or feature-grid-based representations (dis-
cussed below). The volumetric representation has been ex-
tended to work with signed distance ﬁelds (SDF) [ 50,55]
for better geometry acquisition, and the volume-rendering
concept has also been applied to other 3D-related tasks such
as object generation [ 4,5,27,30,42].
Feature-grid-based NeRF. NeRF’s positional encod-
ing [ 37] is a key component for the underlying multi-layer
perceptron (MLP) network to learn high-frequency spatial
and directional signals. However, the MLP size needs to
be large, which leads to slow training and inference. In-
stead, methods like NSVF [ 29] and DVGO [ 45] interpo-
late a 3D volume of learnable feature vectors to encode
the spatial signal, showing faster training and inference
with even better spatial detail. Addressing the sparsity
in typical scene geometry, later works avoid maintaining
a large dense 3D grid via volume-compression techniques
such as hash grids [ 39] and tensor factorization [ 5,7,11].
These methods are compact and scale up the feature grid to
large scenes [ 2,39] and even work with SDF-based mod-
els [25,56]. The essence of feature-grid encoding is to in-
terpolate feature vectors attached to geometry primitives,
and similar ideas have also been applied to irregular 3D
grids [ 22,44], point clouds [ 19,20,54,62], and meshes [ 8].
Operations like mip-mapping are trivial on feature grids, en-abling efﬁcient anti-aliasing and range query of NeRF mod-
els [2,16,53]—something we also leverage in this paper to
encode rough reﬂection.
Rendering specular objects. Apart from geometry,
view-dependent effects like reﬂections from rough surfaces
are a crucial component in photorealistic novel-view syn-
thesis. Reﬂections are conventionally modeled by ﬁtting
local light-ﬁeld functions [ 10,17,36]. A 4D light ﬁeld
presents more degrees of freedom than the constraints from
input images, which necessitates additional regularization
to avoid overﬁtting. Inverse-rendering approaches intro-
duce such a constraint by solving for parametric BRDFs
and lighting, then using forward rendering to reconstruct
the light ﬁeld. Spherical-basis lighting [ 60] or split-sum ap-
proximation [ 31,40] are usually used to tamper the Monte
Carlo variance of specular-reﬂection derivatives [ 3]. EN-
VIDR [ 26] and NMF [ 33] further explicitly consider global-
illumination effects by ray-tracing one or few bounces of
indirect lighting. On the other hand, Ref-NeRF [ 48] uses an
integrated directional encoding (IDE) to directly improve
NeRF’s view-dependent effects. IDE encodes the reﬂected
direction rather than viewing direction to let the network
learn an environment-map-like function and is pre-ﬁltered
to account for rough reﬂection effects. Our neural direc-
tional encoding, similar to IDE, can model general view-
dependent appearance without assuming simpliﬁed lighting
or reﬂections but with smaller computation cost.
3. Preliminaries
We assume opaque objects with diffuse and specular com-
ponents and demonstrate our directional encoding using a
surface-based model that represents a scene using a signed
distance ﬁeld (SDF) s(x)and a color ﬁeld c(x,ω)(depen-
dent on the viewing direction ω). The SDF is converted to
NeRF’s density ﬁeld σfollowing V olSDF [ 55] with a learn-
able parameter βcontrolling the boundary smoothness:
σ(x) =

1
2βexp/parenleftBig
s(x)
β/parenrightBig
ifs(x)≤0,
1
β/parenleftBig
1−1
2exp/parenleftBig
−s(x)
β/parenrightBig/parenrightBig
otherwise .(1)
The color C(x,ω)of a ray with origin xand direction ω
can thus be volume-rendered [ 35]:
C(x,ω)=/summationdisplay
iw(σ(xi))c(xi,ω),where (2)
w(σ(xi)) =/parenleftBig
1−e−σ(xi)δi/parenrightBig/productdisplay
j<ie−σ(xj)δj, (3)
withδi=∥xi−xi−1∥2andxidenoting the ithsample point
along the ray. Like Ref-NeRF [ 48], we decompose the color
cinto a diffuse color cd, specular tint ks, and specular color
csqueried in reﬂected direction ωrwith surface normal n
given by the SDF gradient:
c(x,ω) =cd(x)+ks(x)cs(x,ωr),where
ωr=reﬂect(ω,n),n=normalize (∇xs(x)).(4)
21158
!
N'6-21%E.-'D7.5321%'3D5E.3F
>6?'@20 %155O60
*.0/@200'E%3'2- /B.'1E%B'276-'4%P;516@'Q
!
/RK /R: /RN
>53'%7-2D.3FS1'3E.3F
T'D5E'
T'D5E'T'D5E'
/ 0 123(+(+-'/B.17'-'E%B2- /B.'1E%B'276-'4%PD6?'@20 Q%
!RK !RKU: !R:!
Figure 2. Pipeline of our neural directional encoding (NDE). We encode far-ﬁeld reﬂections into a cubemap and near-ﬁeld interreﬂec-
tions into a volume. Both representations store learnable feature vectors to encode direction and are mip-mapped to account for rough
reﬂections. Given a reﬂected ray, the features are combined by tracing a cone of size proportional to the surface roughness to aggregate
spatial features with cubemap features blended as the background. The result is fed into an MLP to output the specular color (Eq. ( 5)).
Here, the specular color csis decoded from an MLP that
conditions on spatial feature f(x), directional encoding H
controlled by surface roughness ρ, and the cosine term n·ω:
cs(x,ωr) =MLP(f(x),H(x,ωr,ρ(x)),n·ω).(5)
cd,ks,f,ρcome from a spatial MLP (Sec. 4.3).
Discussion on directional encoding. Previous works [ 37,
48] use an analytical function for Hdependent only on ωr
(and optionally ρ), which has several limitations: (1) the en-
coding function is ﬁxed (not learnable), and (2) the spatial
context only comes from f(x). Both require the decoder
MLP to be large to ﬁt the spatio-angular details of the spec-
ular color, which can be expensive and slow.
4. Neural directional encoding
To minimize the MLP complexity, we use a learnable neu-
ral directional encoding that also depends on the spatial
location. Speciﬁcally, our NDE encodes different types
of reﬂection by different representations, which include a
cubemap feature grid hffor far-ﬁeld reﬂections and a spa-
tial volume hnthat models near-ﬁeld interreﬂections. As
shown in Fig. 2, we compute Hby ﬁrst cone-tracing hn
accumulated along the reﬂected ray, yielding near-ﬁeld fea-
tureHn(Sec. 4.2), and blending the far-ﬁeld feature Hf
queried from hfin the same direction (Sec. 4.1):
H(x,ωr,ρ) =Hn(x,ωr,ρ)+(1−αn)Hf(ωr,ρ),(6)
whereαnis the cone-traced opacity [ 24], and both features
are mip-mapped with ρdeciding the mip level.4.1. Far­ﬁeld features
Feature-grid-based representations [ 7,29,39,45,53] speed-
up spatial signal learning by storing feature vectors in vox-
els for local signal control. Similarly, we place feature vec-
torshfat every pixel of a global cubemap to encode ideal
specular reﬂections. The cubemap is pre-ﬁltered to model
reﬂections under rough surfaces in the split-sum [ 18] style,
where the kthlevel mip-map hk
fis created by convolving the
downsampled hfusing a GGX kernel [ 49]Dwith canoni-
cal roughness ρkevenly spaced in [0,1]:
hk
f=convolution (downsample (hf,k),D(ρk)). (7)
Given the surface roughness, we perform a cubemap lookup
in the reﬂected direction and interpolate between mip levels
to get the far-ﬁeld feature:
Hf(ωr,ρ) =lerp/parenleftbigg
hk
f(ωr),hk+1
f(ωr),ρ−ρk
ρk+1−ρk/parenrightbigg
,(8)
where lerp (·)denotes linear interpolation and ρ∈[ρk,ρk+1].
The cubemap-based encoding allows signals in different
directions to be optimized independently by tuning the fea-
ture vectors. This is easier to optimize than globally solv-
ing the MLP parameters, making it more suitable to model
high-frequency details in the angular domain (Fig. 3). The
coarse level feature is a consistently ﬁltered version of the
ﬁne level, which is empirically found to be better con-
strained than using independent feature vectors at each mip
level [ 23,58].
4.2. Near­ﬁeld features
Parameterizing the specular color by a spatial and angular
feature is sufﬁcient for distant reﬂections, but lacks expres-
sivity for near-ﬁeld interreﬂections: different points query
21159
IDE small IDE large Hfsmall (ours) Ground truth
Figure 3. Our cubemap-based feature encoding requires only a
small MLP (2 layers, 64 width) to model details in mirror reﬂec-
tions (3rd image) comparable with IDE [ 48] (2nd image; 8 layers,
256 width MLP) that fails when the MLP is small (1st image).
𝐱𝝎!
𝐱𝐱!
𝐱
Spatio-angular Spatio-spatial Cone-traced
Figure 4. Spatio-spatial encoding (middle) is equivalent to the
common spatio-angular encoding (left) of mirror reﬂections, but
it captures the variation of x′across different x. The idea can be
extended to model rough reﬂections by cone tracing mip-mapped
spatial features covered by the reﬂection cone (right).
the same hf, so spatially varying components can end up
being averaged out during optimization. Our insight is that
the spatio-angular reﬂection can also be parameterized as a
spatio-spatial function of current and next bounce location
(Fig. 4). Therefore, an MLP can decode the second bounce
spatial feature with f(x)in Eq. ( 5) to get mirror reﬂections.
For rough reﬂections, we aggregate the averaged second
bounce feature under the reﬂection lobe by cone tracing [ 9]
(Fig. 4, right), which volume renders the mip-mapped spa-
tial features hnusing the mip-mapped density σnalong
the reﬂected ray x+ωrtwith mip level λi= log2(2ri)
at sample point x′
idecided by the cone’s footprint ri=√
3ρ2∥x−x′
i∥2:
Hn(x,ωr,ρ) =/summationdisplay
iwi
nhi
n,where
wi
n=w(σn(x′
i,λi)),hi
n=hn(x′
i,λi).(9)
The cone’s footprint is selected to cover the GGX lobe at x
(see supplemental document). Note that we do not use the
SDF-converted σin Eq. ( 1) as it cannot be mip-mapped;
instead, we optimize a separate σnto match σ(Sec. 4.3)
jointly with the indirect feature hn. Both are decoded from
a tri-plane [ 5]Tn, whose each 2D plane is mip-mapped
similar to Tri-MipRF [ 16]:
σn(x′
i,λi),hn(x′
i,λi)=MLP(mipmap(Tn(x′
i),λi)).(10)
The indirect rays are spatially varying, hence the cone-
traced near-ﬁeld features are spatially varying too. This has
advantages over the angular-only feature for learning inter-
reﬂections and is empirically less likely to overﬁt (Fig. 5).
This is because the same hnis traced from different rays
in training, such that the underlying representation is well-
constrained. HnandHfare similar to the foreground
Ours without Hn Ours with Hn Ground truth
Figure 5. Our cone-traced near-ﬁeld features successfully re-
construct the reﬂected spheres (2nd column) under novel views,
which are overﬁtted by the angular-only encoding (1st column).
Hash grid
Positional
Encoding
MLP
Cubemap
 MLP
Tri-plane
MLPResolution:
Levels:
Features:
Hash table:64   1
ReLU[16, 2048] 
16
2
524288
64   2
ReLU
256   8
Softplus
MLP
64   2
ReLU512 
9
16
Resolution:
Mip levels:
Features:
Resolution:
Mip levels:
Features:64 
6
16
Figure 6. Network architectures. N×Mdenotes an M-layer
MLP of width N.
and background colors in regular volume rendering, so Hf
can be naturally composited with Hnusing the opacity
αn=1−/producttext
ie−σn(x′
i,λi)δi=/summationtext
iwi
nas in Eq. ( 6).
4.3. Optimization
Figure 6shows our network architectures. Stable geometry
optimization is essential for modeling specular objects, so
we use the positional-encoded MLP from V olSDF [ 55] to
output the SDF. To reduce computation cost, a hash grid
is used to encode other spatial features ( cd,ks,ρ,f), and
all other MLPs are tiny. The representation is optimized
through the Charbonnier loss [ 1] between ground truth pixel
colorCgtand our rendering Cin tone-mapped space:
L=/summationdisplay
x,ω/radicalBig
∥Γ(C(x,ω))−Cgt(x,ω)∥2
2+0.001,(11)
whereΓis the tone-mapping function [ 40].
Occupancy-grid sampling. Eqs. ( 3) and ( 9) are acceler-
ated by an occupancy-grid estimator [ 24] to get rid of com-
putations in empty space. This is especially important for
the efﬁcient near-ﬁeld feature evaluation, since we trace a
reﬂected ray for each primary ray sample. The primal ray
rendering uses a ﬁxed ray marching step of 0.005. Follow-
ing [ 9], we choose the cone tracing step proportional to its
footprint: max(0.5ri,0.005) , and query a mip-mapped oc-
cupancy grid for the correct occupancy information.
Regularization. Given the primary samples xi, Eikonal
loss [ 55]Leikis applied to regularize the SDF, and we im-
plicitly regularize σnto matchσby encouraging the render-
ing using σnat mip level 0 to be close to the ground truth:
21160
Lσ=/summationdisplay
x,ω∥Cσ(x,ω)−Cgt(x,ω)∥2
2,where
Cσ(x,ω) =/summationdisplay
iw(σn(xi,0))˚c(xi,ω),(12)
˚□denotes stop-gradient to prevent σnaffecting appearance.
The total loss is L+0.1Leik+0.01Lσ.
Implementation details. We implement our code using
PyTorch [ 41], NerfAcc [ 24], and CUDA. The optimiza-
tion takes 400k steps using the Adam optimizer [ 21] with
0.0005 learning rate and dynamic batch size [ 39] target-
ing for 32k primary point samples. We use the scheduler
from BakedSDF [ 15] to anneal βin Eq. ( 1) for more stable
convergence. Because the SDF uses a positional-encoded
MLP, each scene still requires 10 ∼18 hours to train on an
NVIDIA 3090 GPU with 15GB GPU memory usage.
5. Experiments
We evaluate our method on view synthesis of specular ob-
jects using synthetic and real scenes. The synthetic scenes
include the Shinny Blender dataset [ 48] and the Materials
scene from the NeRF Synthetic dataset [ 37], all rendered
without background; the real scenes come from NeRO [ 31]
which contain backgrounds and reﬂections of the capturer
in the images. The rendering quality is compared in terms
of PSNR, SSIM [ 52], LPIPS [ 61], and the inference speed
in FPS is recorded on an NVIDIA 3090 GPU.
Background and capturer. For real scenes, we use a sep-
arate Instant-NGP [ 39] with coordinate contraction [ 1] to
render backgrounds. Similarly to NeRO [ 31], the reﬂection
of the capturer is encoded by blending a capturer plane fea-
turehcof opacity αcbetweenHfandHn:
H=Hn+(1−αn)(αchc+(1−αc)Hf),where
αc,hc=MLP(mipmap(Tc(u),λc))(13)
are decoded from a mip-mapped 2D feature grid Tc;u,λc
are the ray-plane intersection coordinate and the mip-level
derived from the intersection footprint. Jointly optimiz-
ing foreground and background networks can be unstable,
so we apply stabilization loss from NeRO [ 31] and mod-
ify the specular color computation for the ﬁrst 200k steps:
hf,hn,hcare sampled and decoded into colors ﬁrst, then
the colors are blended to get cs. Compared to blending the
feature and decoding, we ﬁnd the decoding-then-blending
strategy provides better geometry optimization.
5.1. View synthesis
We compare against NeRO [ 31], ENVIDR [ 26], and Ref-
NeRF [ 48] on synthetic scenes. All methods except for Ref-
NeRF use SDFs, and we evaluate NeRO after the BRDF
estimation as it shows better performance. Ideally, both
backgrounds and reﬂections from the capturer should beMethod Mat. Teapot Toaster Car Ball Coffee Helmet Mean
PSNR↑
NeRO 24.85 40.29 27.31 26.98 31.50 33.76 29.59 30.61
ENVIDR 29.51 46.14 26.63 29.88 41.03 34.45 36.98 34.95
Ref-NeRF 35.41 47.90 25.70 30.82 47.46 34.21 29.68 35.88
NDE (ours) 31.53 49.12 30.32 30.39 44.66 36.57 37.77 37.19
SSIM↑
NeRO 0.878 0.993 0.891 0.926 0.953 0.960 0.953 0.936
ENVIDR 0.971 0.999 0.955 0.972 0.997 0.984 0.993 0.982
Ref-NeRF 0.983 0.998 0.922 0.955 0.995 0.974 0.958 0.969
NDE (ours) 0.972 0.999 0.968 0.968 0.995 0.979 0.990 0.982
LPIPS↓
NeRO 0.138 0.017 0.162 0.064 0.179 0.099 0.102 0.109
ENVIDR 0.026 0.003 0.097 0.031 0.020 0.044 0.022 0.035
Ref-NeRF 0.022 0.004 0.095 0.041 0.059 0.078 0.075 0.053
NDE (ours) 0.017 0.002 0.039 0.024 0.022 0.033 0.014 0.022
Table 1. Quantitative comparison on synthetic scenes showing
our encoding (NDE) is either the best orsecond best compared to
other methods for view synthesis of specular objects.
removed when evaluating renderings of specular objects,
which is difﬁcult for the real scenes. Therefore, we only
qualitatively compare real scenes against NeRO with PSNR
computed on the foreground zoom-ins without the capturer.
Results. Overall, our method gives the best rendering
quality on synthetic scenes with quantitative results either
better or comparable with the baselines (Tab. 1). This
is because our NDE gives the most detailed modeling of
both far-ﬁeld reﬂections and interreﬂections, which also
helps improve the geometry reconstruction (Fig. 7bottom).
While ENVIDR’s SSIM is slightly better than ours in sev-
eral scenes, we not only achieve much better PSNRs (sur-
passing 2dB), but also higher LPIPS scores. The PSNR on
the Materials (Mat.) scene is worse than Ref-NeRF’s be-
cause the SDF is inefﬁcient at modeling the concave geom-
etry of the sphere base. However, our directional MLP is
much smaller (Sec. 5.2), and we still achieve perceptually
better appearance as shown in the insets of Fig. 7. The qual-
itative comparison in Fig. 8shows that NDE extends well
to real scenes, producing clearer specular reﬂections of the
complex real-world environments compared to NeRO.
Editability. The near- and far-ﬁeld features provide a nat-
ural separation of different reﬂections, allowing us to ren-
der these effects separately by excluding HforHnduring
inference (Fig. 9). Because interreﬂections are spatially en-
coded in the near-ﬁeld feature grid, an object and its ﬁrst-
bounce reﬂections can be removed by masking out both
σandσnfrom the corresponding regions (Fig. 10). This
does not work for multi-bounce reﬂections which are not
encoded on the deleted object.
5.2. Performance comparison
We compare the evaluation frames per second (FPS) on
an800×800resolution of the color network and its MLP
size (#Params.) with all baselines in Sec. 5.1on synthetic
scenes. The color MLPs include the decoder of σn,hn,cs
for our model (Fig. 6), lighting MLPs for NeRO [ 31] and
21161
ENVIDR [ 26] Ref-NeRF [ 48] NDE (ours) Ground truth ENVIDR Ref-NeRF NDE (ours) GT
Rendering
 Rendering/Normal
Figure 7. Qualitative results for synthetic scenes show our NDE successfully models the ﬁne details of reﬂections from both environment
lights (mirror sphere and car top) and other objects (glossy interreﬂections on spheres; zoom in to see the difference). Ref-NeRF tends to use
wrong geometry to fake interreﬂections (2nd column on bottom). In contrast, our encoding has sufﬁcient capacity to model interreﬂections,
which enables more accurate normals (3rd column on bottom). Mean angular error of the normal is shown in the insets.
NeRO [ 31] NDE (ours) Ground truth
Figure 8. Qualitative comparison on real scenes. Our NDE gives better reconstruction of the interreﬂections (the bear’s plate and bottom
of the vase) and detailed highlights from the environment. Numbers in the insets are image PSNR values.
21162
Far-ﬁeld reﬂections Near-ﬁeld reﬂections Combined
Far-field
Near-field
Figure 9. Reﬂection separation. We can visualize different reﬂec-
tion effects by feeding corresponding features into the network.
Figure 10. Editability of our encoding. Reﬂections from the
deleted spheres can be removed by deleting the volume of their
indirect features (bottom).
ENVIDR [ 26], and the directional MLP for Ref-NeRF [ 48].
The spatial-network evaluation is excluded to eliminate the
difference caused by different geometry representations,
network architectures, and sampling strategies. For each
method, we choose the rendering batch size that maximizes
its performance.
Results. As shown in the top half of Tab. 2, our NDE takes
a fraction of a second to evaluate, because it requires sub-
stantially smaller MLPs to infer color without hurting the
rendering. In contrast, other baselines need large MLPs to
maintain rendering quality, which prevents them to be visu-
alized in real-time.
Real-time application. It is possible to create a real-time
version of our model by converting the SDF into a mesh
through marching cubes [ 32] and baking cd,ks,ρ,finto
mesh vertices. The pixel color then can be computed us-
ing the rasterized vertex attributes and csdecoded from the
NDE, which takes only a single cubemap lookup and cone
tracing for each pixel. As a result, this process requires
about the same budget as evaluating a real-time NeRF
model [ 39,45,53]. We implement our real-time model
(NDE-RT) in WebGL and report the full rendering frame
rate (not just color evaluation) at the bottom of Tab. 2with
a real-time baseline 3DGS [ 19]. 3DGS is faster as it uses
spherical harmonics for color without network evaluation,
Ground truth Our ofﬂine model Our real-time model
Figure 11. Error near object boundaries in our real-time model
is caused by the marching-cube extraction of a triangle mesh and
its subsequent rasterization (squared error maps at the bottom).
This error does not lead to signiﬁcant qualitative differences (top).
Method FPS ↑#Params↓PSNR↑SSIM↑LPIPS↓
NeRO 0.11 454k 30.61 0.936 0.109
ENVIDR 0.55 206k 34.95 0.982 0.035
Ref-NeRF 0.08 521k 35.88 0.969 0.053
NDE (ours) 3.03 75k 37.19 0.982 0.022
3DGS 235 - 30.30 0.949 0.076
NDE-RT (ours) 66 75k 35.48 0.976 0.027
Table 2. Performance comparison. Our NDE achieves high ren-
dering quality, and its use of small MLPs enables fast color evalu-
ation and real-time rendering. We report only the evaluation time
and parameter counts of color MLPs except for 3DGS (no color
MLPs) and our NDE-RT, for which we report the total rendering
time. All metrics are averaged over the synthetic scenes in Tab. 1.
which leads to poor specular appearance reconstruction. In-
stead, our NDE-RT shows rendering quality comparable to
other baselines while achieving frame rates above 60. The
loss in PSNR is mainly due to error around object edges
which is cause by the marching-cube mesh extraction and
subsequent rasterization (Fig. 11). This error does not sig-
niﬁcantly affect the visual quality and can be resolved by
ﬁne-tuning the mesh [ 8,40].
5.3. Ablation study
Different directional encodings. In Fig. 12we com-
pare different directional encodings on the Materials scene.
IDE [ 48] (analytical) with our tiny MLP yields blurry re-
ﬂections. Interreﬂections cannot be reconstructed using
only the far-ﬁeld feature, and if we volume-render rather
than cone-trace the near-ﬁeld feature, mirror interreﬂections
can be recovered but reﬂections on rough surfaces look too
sharp. It is therefore necessary to use both the cubemap-
based far-ﬁeld feature and the cone-traced near-ﬁeld feature
to get the best specular appearance (Tab. 3).
Network architecture. Table 4shows the performance
trade-off between different network architectures of our
model on synthetic scenes. Using a smaller MLP width for
21163
Ground truth
Analytical Hf Cubemap Hf
V olume-rendered Hn Cone-traced Hn
Figure 12. Qualitative ablation of NDE components. Details
from the environment light fail to be reconstructed with an ana-
lytical encoding (mirror sphere on 2nd row). It is also necessary
to use the cone-traced near-ﬁeld feature, otherwise rough surfaces
are rendered incorrectly (grey sphere on 3rd row).
Far-ﬁeld feature Near-ﬁeld feature PSNR ↑SSIM↑LPIPS↓
Analytical - 28.54 0.944 0.029
Cubemap - 30.27 0.962 0.022
Cubemap V olume-rendered 29.31 0.951 0.034
Cubemap Cone-traced 31.53 0.972 0.017
Table 3. Ablation on directional encodings shows each compo-
nent of NDE is needed for the best rendering quality. The compar-
ison is made on the Materials scene.
Model MLP width PSNR ↑SSIM↑LPIPS↓FPS↑
Our ofﬂine64 37.19 0.982 0.022 <1
32 36.69 0.979 0.026 <1
16 36.23 0.977 0.028 <1
Our real-time64 35.48 0.976 0.027 66
32 33.97 0.971 0.034 211
16 33.71 0.969 0.036 331
Table 4. Ablation on our network architecture. Using a smaller
MLP width introduces a minor loss in rendering ﬁdelity but a no-
ticeable real-time performance boost.
the decoder of σn,hn,cshas only a slight negative impact
on the rendering quality but signiﬁcantly improves real-time
performance. The rendering quality reduction of the real-
time model is mainly caused by the error near object edges
as discussed in Sec. 5.2.
Spatial mip-mapping strategies. Besides mip-mapped
tri-plane [ 5,16], our architecture can also work with a mip-
mapped hash grid [ 39] for the near-ﬁeld feature encoding.
Similar to [ 2,25], the hash-grid mip-mapping is imple-
mented by gradually masking out ﬁne-resolution features as
the mip level increases. This results in limited model capac-
ity for rough surfaces where most of the features are maskedMat. Teapot Toaster Car Ball Coffee Helmet Mean
PSNR↑
Hash grid 30.89 49.00 29.46 30.16 43.48 34.98 37.67 36.52
Tri-plane 31.53 49.12 30.32 30.39 44.66 36.57 37.77 37.19
SSIM↑
Hash grid 0.968 0.999 0.953 0.967 0.990 0.974 0.990 0.977
Tri-plane 0.972 0.999 0.968 0.968 0.995 0.979 0.990 0.982
LPIPS↓
Hash grid 0.019 0.002 0.058 0.025 0.031 0.043 0.014 0.027
Tri-plane 0.017 0.002 0.039 0.024 0.022 0.033 0.014 0.022
Table 5. Ablation on mip-mapping strategies suggests that the
mip-mapped tri-plane represents averaged near-ﬁeld features and
density better than the mip-mapped hash grid.
ENVIDR [ 26]NDE (hash grid) NDE (MLP) Ground truth
Figure 13. Unstable geometry optimization of specular objects
prevents us from encoding the SDF using a hash grid [ 39] as it
gives incorrect surface normals (middle left). This is also the case
for other hash-grid-based methods (left).
out, such that a mip-mapped hash grid produces slightly
worse rendering than the tri-plane encoding (Tab. 5).
Limitations. Like previous works [ 26,31,48], NDE is
sensitive to the quality of the surface normal. This prevents
us from using more efﬁcient geometry representations such
as a hash grid, which tends to produce corrupted geometry
(Fig. 13). As a result, we use positional-encoded MLPs to
model the SDF, which leads to long training times and is
difﬁcult for modeling transparent objects. Meanwhile, the
editibility of our method is limited.
6. Conclusion
We have adapted feature-based NeRF encodings to the di-
rectional domain and introduced a novel spatio-spatial pa-
rameterization of view-dependent appearance. These im-
provements allow for efﬁcient modeling of complex re-
ﬂections for novel-view synthesis and could beneﬁt other
applications that model spatially varying directional sig-
nals, such as neural materials [ 13,23,58] and radiance
caching [ 38].
Acknowledgements. This work was supported in part
by NSF grants 2110409, 2100237, 2120019, ONR grant
N00014-23-1-2526, gifts from Adobe, Google, Qualcomm,
Rembrand, a Sony Research Award, as well as the Ronald
L. Graham Chair and the UC San Diego Center for Visual
Computing. Additionally, we thank Jingshen Zhu for in-
sightful discussions.
21164
References
[1] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P
Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded
anti-aliased neural radiance ﬁelds. In CVPR , 2022. 2,4,5
[2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.
Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-
based neural radiance ﬁelds. In ICCV , 2023. 2,8
[3] Yash Belhe, Bing Xu, Sai Praveen Bangaru, Ravi Ra-
mamoorthi, and Tzu-Mao Li. Importance sampling brdf
derivatives. In ACM TOG , 2024. 2
[4] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,
and Gordon Wetzstein. Pi-gan: Periodic implicit genera-
tive adversarial networks for 3d-aware image synthesis. In
CVPR , 2021. 2
[5] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,
Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J
Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efﬁ-
cient geometry-aware 3d generative adversarial networks. In
CVPR , 2022. 1,2,4,8
[6] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,
Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast general-
izable radiance ﬁeld reconstruction from multi-view stereo.
InICCV , 2021. 2
[7] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and
Hao Su. Tensorf: Tensorial radiance ﬁelds. In ECCV , 2022.
1,2,3
[8] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and An-
drea Tagliasacchi. Mobilenerf: Exploiting the polygon ras-
terization pipeline for efﬁcient neural ﬁeld rendering on mo-
bile architectures. In CVPR , 2023. 2,7
[9] Cyril Crassin, Fabrice Neyret, Miguel Sainz, Simon Green,
and Elmar Eisemann. Interactive indirect illumination using
voxel cone tracing. In Computer Graphics Forum , 2011. 4
[10] John Flynn, Michael Broxton, Paul Debevec, Matthew Du-
Vall, Graham Fyffe, Ryan Overbeck, Noah Snavely, and
Richard Tucker. Deepview: View synthesis with learned gra-
dient descent. In CVPR , 2019. 2
[11] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk
Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:
Explicit radiance ﬁelds in space, time, and appearance. In
CVPR , 2023. 2
[12] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie
Shotton, and Julien Valentin. Fastnerf: High-ﬁdelity neural
rendering at 200fps. In ICCV , 2021. 2
[13] Alban Gauthier, Robin Faury, J ´er´emy Levallois, Th ´eo
Thonat, Jean-Marc Thiery, and Tamy Boubekeur. Mipnet:
Neural normal-to-anisotropic-roughness mip mapping. In
ACM TOG , 2022. 8
[14] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M Susskind,
Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi.
Nerfdiff: Single-image view synthesis with nerf-guided dis-
tillation from 3d-aware diffusion. In ICML , 2023. 2
[15] Peter Hedman, Pratul P Srinivasan, Ben Mildenhall,
Jonathan T Barron, and Paul Debevec. Baking neural ra-
diance ﬁelds for real-time view synthesis. In ICCV , 2021. 2,
5
[16] Wenbo Hu, Yuling Wang, Lin Ma, Bangbang Yang, Lin Gao,
Xiao Liu, and Yuewen Ma. Tri-miprf: Tri-mip representation
for efﬁcient anti-aliasing neural radiance ﬁelds. In ICCV ,
2023. 2,4,8[17] Nima Khademi Kalantari, Ting-Chun Wang, and Ravi Ra-
mamoorthi. Learning-based view synthesis for light ﬁeld
cameras. In ACM TOG , 2016. 2
[18] Brian Karis. Real shading in unreal engine 4. In SIGGRAPH
2013 Course: Physically Based Shading in Theory and Prac-
tice, 2013. 3
[19] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk ¨uhler,
and George Drettakis. 3d gaussian splatting for real-time
radiance ﬁeld rendering. In ACM TOG , 2023. 2,7
[20] Leonid Keselman and Martial Hebert. Flexible techniques
for differentiable rendering with 3d gaussians. arXiv preprint
arXiv:2308.14737 , 2023. 2
[21] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 5
[22] Jonas Kulhanek and Torsten Sattler. Tetra-nerf: Represent-
ing neural radiance ﬁelds using tetrahedra. In ICCV , 2023.
2
[23] Alexandr Kuznetsov, Krishna Mullia, Zexiang Xu, Milo ˇs
Haˇsan, and Ravi Ramamoorthi. Neumip: Multi-resolution
neural materials. In ACM TOG , 2021. 3,8
[24] Ruilong Li, Matthew Tancik, and Angjoo Kanazawa. Ner-
facc: A general nerf acceleration toolbox. arXiv preprint
arXiv:2210.04847 , 2022. 3,4,5
[25] Zhaoshuo Li, Thomas M ¨uller, Alex Evans, Russell H Tay-
lor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin.
Neuralangelo: High-ﬁdelity neural surface reconstruction. In
CVPR , 2023. 2,8
[26] Ruofan Liang, Hui-Hsia Chen, Chunlin Li, Fan Chen, Sel-
vakumar Panneer, and Nandita Vijaykumar. Envidr: Implicit
differentiable renderer with neural environment lighting. In
ICCV , 2023. 1,2,5,6,7,8
[27] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,
Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution
text-to-3d content creation. In CVPR , 2023. 2
[28] Kai-En Lin, Yen-Chen Lin, Wei-Sheng Lai, Tsung-Yi Lin,
Yi-Chang Shih, and Ravi Ramamoorthi. Vision transformer
for nerf-based view synthesis from a single input image. In
WACV , 2023. 2
[29] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and
Christian Theobalt. Neural sparse voxel ﬁelds. In NeurIPS ,
2020. 1,2,3
[30] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang
Xu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh
in 45 seconds without per-shape optimization. In NeurIPS ,
2023. 2
[31] Yuan Liu, Peng Wang, Cheng Lin, Xiaoxiao Long, Jiepeng
Wang, Lingjie Liu, Taku Komura, and Wenping Wang. Nero:
Neural geometry and brdf reconstruction of reﬂective objects
from multiview images. In ACM TOG , 2023. 2,5,6,8
[32] William E Lorensen and Harvey E Cline. Marching cubes:
A high resolution 3d surface construction algorithm. In SIG-
GRAPH , 1987. 7
[33] Alexander Mai, Dor Verbin, Falko Kuester, and Sara
Fridovich-Keil. Neural microfacet ﬁelds for inverse render-
ing. In ICCV , 2023. 2
[34] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,
Jonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-
worth. Nerf in the wild: Neural radiance ﬁelds for uncon-
strained photo collections. In CVPR , 2021. 2
21165
[35] Nelson Max. Optical models for direct volume rendering.
IEEE Transactions on Visualization and Computer Graphics ,
1(2):99–108, 1995. 2
[36] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,
Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and
Abhishek Kar. Local light ﬁeld fusion: Practical view syn-
thesis with prescriptive sampling guidelines. In ACM TOG ,
2019. 2
[37] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance ﬁelds for view syn-
thesis. In ECCV , 2020. 1,2,3,5
[38] Thomas M ¨uller, Fabrice Rousselle, Jan Nov’ak, and Alexan-
der Keller. Real-time neural radiance caching for path trac-
ing. In ACM TOG , 2021. 8
[39] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a multires-
olution hash encoding. In SIGGRAPH , 2022. 1,2,3,5,7,
8
[40] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao,
Wenzheng Chen, Alex Evans, Thomas M ¨uller, and Sanja Fi-
dler. Extracting triangular 3d models, materials, and lighting
from images. In CVPR , 2022. 2,4,7
[41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An
imperative style, high-performance deep learning library. In
NeurIPS , 2019. 5
[42] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR ,
2023. 2
[43] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas
Geiger. Kilonerf: Speeding up neural radiance ﬁelds with
thousands of tiny mlps. In ICCV , 2021. 2
[44] Radu Alexandru Rosu and Sven Behnke. Permutosdf: Fast
multi-view reconstruction with implicit surfaces using per-
mutohedral lattices. In CVPR , 2023. 2
[45] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel
grid optimization: Super-fast convergence for radiance ﬁelds
reconstruction. In CVPR , 2022. 1,2,3,7
[46] Alex Trevithick and Bo Yang. Grf: Learning a general ra-
diance ﬁeld for 3d representation and rendering. In ICCV ,
2021. 2
[47] Alex Trevithick, Matthew Chan, Michael Stengel, Eric Chan,
Chao Liu, Zhiding Yu, Sameh Khamis, Manmohan Chan-
draker, Ravi Ramamoorthi, and Koki Nagano. Real-time
radiance ﬁelds for single-image portrait view synthesis. In
ACM TOG , 2023. 2
[48] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,
Jonathan T Barron, and Pratul P Srinivasan. Ref-nerf: Struc-
tured view-dependent appearance for neural radiance ﬁelds.
InCVPR , 2022. 1,2,3,4,5,6,7,8
[49] Bruce Walter, Stephen R Marschner, Hongsong Li, and Ken-
neth E Torrance. Microfacet models for refraction through
rough surfaces. In EGSR , 2007. 3
[50] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. Neus: Learning neural implicit
surfaces by volume rendering for multi-view reconstruction.
InNeurIPS , 2021. 2
[51] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P
Srinivasan, Howard Zhou, Jonathan T Barron, RicardoMartin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibr-
net: Learning multi-view image-based rendering. In CVPR ,
2021. 2
[52] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. In IEEE transactions on image process-
ing, 2004. 5
[53] Liwen Wu, Jae Yong Lee, Anand Bhattad, Yu-Xiong Wang,
and David Forsyth. Diver: Real-time and accurate neural ra-
diance ﬁelds with deterministic integration for volume ren-
dering. In CVPR , 2022. 1,2,3,7
[54] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu,
Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-
based neural radiance ﬁelds. In CVPR , 2022. 2
[55] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. V ol-
ume rendering of neural implicit surfaces. In NeuRIPS , 2021.
2,4
[56] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin,
Pratul P. Srinivasan, Richard Szeliski, Jonathan T. Barron,
and Ben Mildenhall. Bakedsdf: Meshing neural sdfs for real-
time view synthesis. In SIGGRAPH , 2023. 2
[57] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and
Angjoo Kanazawa. Plenoctrees for real-time rendering of
neural radiance ﬁelds. In ICCV , 2021. 2
[58] Tizian Zeltner, Fabrice Rousselle, Andrea Weidlich, Petrik
Clarberg, Jan Nov ´ak, Benedikt Bitterli, Alex Evans,
Tom´aˇs Davidovi ˇc, Simon Kallweit, and Aaron Lefohn.
Real-time neural appearance models. arXiv preprint
arXiv:2305.02678 , 2023. 3,8
[59] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen
Koltun. Nerf++: Analyzing and improving neural radiance
ﬁelds. arXiv preprint arXiv:2010.07492 , 2020. 2
[60] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and
Noah Snavely. Physg: Inverse rendering with spherical gaus-
sians for physics-based material editing and relighting. In
CVPR , 2021. 2
[61] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR , 2018. 5
[62] Xiaoshuai Zhang, Abhijit Kundu, Thomas Funkhouser,
Leonidas Guibas, Hao Su, and Kyle Genova. Nerﬂets: Local
radiance ﬁelds for efﬁcient structure-aware 3d scene repre-
sentation from 2d supervision. In CVPR , 2023. 2
21166
