No More Ambiguity in 360◦Room Layout via Bi-Layout Estimation
Yu-Ju Tsai1,3* Jin-Cheng Jhang2* Jingjing Zheng3Wei Wang3
Albert Y . C. Chen3Min Sun2,3Cheng-Hao Kuo3Ming-Hsuan Yang1
1UC Merced2National Tsing Hua University3Amazon
{ytsai17, myang37 }@ucmerced.edu {frank890725 }@gapp.nthu.edu.tw
{zhejingj, wweiwan, aycchen, minnsun, chkuo }@amazon.com
Abstract
Inherent ambiguity in layout annotations poses signiﬁ-
cant challenges to developing accurate 360◦room layout
estimation models. To address this issue, we propose a novel
Bi-Layout model capable of predicting two distinct layout
types. One stops at ambiguous regions, while the other ex-
tends to encompass all visible areas. Our model employs
two global context embeddings, where each embedding is
designed to capture speciﬁc contextual information for each
layout type. With our novel feature guidance module, the
image feature retrieves relevant context from these embed-
dings, generating layout-aware features for precise bi-layout
predictions. A unique property of our Bi-Layout model is its
ability to inherently detect ambiguous regions by compar-
ing the two predictions. To circumvent the need for manual
correction of ambiguous annotations during testing, we also
introduce a new metric for disambiguating ground truth
layouts. Our method demonstrates superior performance
on benchmark datasets, notably outperforming leading ap-
proaches. Speciﬁcally, on the MatterportLayout dataset, it
improves 3DIoU from 81.70% to82.57% across the full
test set and notably from 54.80% to59.97% in subsets with
signiﬁcant ambiguity.
1. Introduction
Room layout estimation from a single 360◦image has re-
ceived signiﬁcant attention due to the availability of cheap
360◦cameras and the demonstration of visually pleasing
room pop-ups. It also plays a vital role in indoor 3D scene
understanding [ 3,15,32,38] as the room layout constrains
the space where objects are placed and interact. Its perfor-
mance has improved signiﬁcantly over the years, where the
gain comes from better algorithms design [ 17,29,31,34],
and more challenging data collected [ 7,45]. Despite the
progress, the task formulation of predicting a single layout
*Equal contribution
(a) Enclosed Type Annotation
(b) Extended Type Annotation2D IoU = 0.13
2D IoU = 0.49Figure 1. Inherent ambiguity in the MatterportLayout [ 45].Blue
andGreen represent ground truth annotations and predictions from
the SoTA models, respectively. The layout boundaries are shown on
the left, and their bird’s-eye view projections are on the right. We
deﬁne two types of layout annotation: (a) enclosed type encloses
the room. (b) extended type extends to all visible areas. The dashed
lines underscore the ambiguity in the dataset labels.
given a single 360◦image has never been changed.
Annotating a single layout for each 360◦image is, in fact,
an ambiguous task. For instance, consider the two images
with openings in Fig. 1, where the ground truth (GT) anno-
tation stops at openings and encloses the nearest room in
Fig.1(a) while extending to all visible areas inside the open-
ing in Fig. 1(b). Notably, even within the same dataset, there
are variations in how opening regions are annotated. We
observe that this ambiguity issue in annotation is prevalent
across most datasets, and there is a lack of a clear deﬁnition
of how to annotate ambiguous regions. Furthermore, state-of-
the-art (SoTA) methods often overlook this ambiguity issue,
leading to inherent inaccuracy during training. As a result,
existing methods may predict in a manner opposite to the GT,
as shown in Fig. 1. In this paper, we deﬁne two main types
of layout annotations: enclosed andextended . The former
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
28056
SoTA Prediction
 Our Extended Prediction Our Enclosed PredictionFigure 2. Comparions of our Bi-Layout model and the SoTA models. Blue andGreen indicate ground truth labels and predictions,
respectively. For each image, the layout boundaries are shown on the left, while their bird’s-eye view projections are shown on the right. Our
Bi-Layout model can predict two extremely different types of layouts ( enclosed andextended ), addressing the ambiguity issue that the SoTA
methods struggle with.
stops at ambiguous regions and encloses the room, whereas
the latter extends to all visible areas inside the opening.
To address the confusion arising from the ambiguity issue
in model training, as shown in Fig. 3, we propose a novel
Bi-Layout model to simultaneously predict both enclosed
andextended layouts for each image. Our model consists of
three components: a shared feature extractor, two separate
global context embeddings, and a shared feature guidance
module. Two separate global context embeddings are learned
to encode all context information related to the correspond-
ing layout type. The shared feature guidance module guides
the fusion of the shared image feature with our two embed-
dings separately through cross-attention. Speciﬁcally, we
use the image feature as the query and each global context
embedding as the key and value. When queried by the im-
age feature, the global context embedding can inject layout
type-related context information into the image feature. This
results in enhanced alignment of the image feature with the
corresponding layout prediction type.
Our model design introduces two key innovations. First,
contrary to the standard Query-Key-Value setting employed
in DETR [ 2] and other high-level tasks [ 4,5,20,30,37,42],
where embeddings serve as queries to retrieve relevant infor-
mation from the image feature, we invert this relationship
and employ our image feature as the queries. This uncon-
ventional design allows the image feature to be guided by
our embedding, which represents the global information of a
speciﬁc layout type. To the best of our knowledge, we are the
ﬁrst to develop a query-based model for room layout estima-
tion, inherently designed for predicting multiple layouts. The
second innovation lies in the efﬁciency of our model, which
can predict two layouts with minimal additional model size
overhead. For bi-layout estimations, naive approaches either
train two distinct models with identical architectures on dif-
ferent labels or train a single model by sharing the feature
extractor but separating other components. However, the for-
mer method doubles the model size and training time, while
the latter lacks compactness and grapples with interferencein simultaneously learning two layout types. In contrast, our
model is not only the smallest, achieved by sharing both
the feature extractor and the guidance module, but it also
avoids interference issues by employing separate global con-
text embeddings to guide feature fusion for different layout
types. As shown in Fig. 2, our model is able to predict two
extremely different layouts.
We also introduce a new metric termed as disambiguate
metric to resolve ambiguities in the annotations of test data.
It calculates the Intersection over Unions (IoU) of both pre-
dicted layouts with the ground truth and selects the higher
IoU for evaluation. This is an effective way to quantitatively
measure the beneﬁt of our Bi-Layout estimation without
manually correcting ambiguous annotations during testing.
Another noteworthy feature of our Bi-Layout model is its
ability to detect ambiguous regions with reasonable precision
and recall by comparing two predictions. Our method ex-
hibits superior performance on benchmark datasets, surpass-
ing SoTA methods. On MatterportLayout [ 45], it enhances
3DIoU from 81.70% to82.57% on the entire test set and
notably from 54.80% to59.97% in subsets with substantial
ambiguity.
The main contributions of this work are:
•We clearly identify layout ambiguity issues in existing
datasets and propose a disambiguate metric to measure the
accuracy with multiple predictions effectively.
•We propose a novel Bi-Layout model that utilizes two
global context embeddings with a shared feature guidance
module to generate multiple layout predictions while keep-
ing the model compact.
•We evaluate our method with extensive experiments and
prove it outperforms SoTA methods in all settings, show-
ing that our Bi-Layout model effectively resolves the lay-
out ambiguity issues.
2. Related Work
360◦room layout estimation. In 360◦room layout es-
timation, prior methods follow the Manhattan World as-
28057
SWG Self-Attn.
Guided Cross-Attn.(c) Shared Feature Guidance Module
(b) Global Context EmbeddingSHCM ResNet
𝐼𝐸extended
𝐹𝑐
𝑃enclosed𝐸enclosed𝐹𝑙𝐹𝑔𝑒𝑥
𝐹𝑔𝑒𝑛FC 512 x 1024 x 3 (a) Feature Extractor
256x512256x512
256x512256x512256x512
𝑃extended𝐹𝑔𝑐𝑎𝑘
FC Figure 3. Our Bi-Layout network architecture. (a)Feature extractor : It processes a panoramic image Iusing ResNet-50 to extract
multi-scale features Fl, and then feed those feature into the Simpliﬁed Height Compression Module (SHCM) to produce the ﬁnal compressed
featureFc. (b)Global Context Embedding : It consists of two learnable embeddings Ek, each designed to capture and encode the contextual
information inherent in the corresponding type of layout labels. (c) Shared Feature Guidance Module : It consists of two components:
Guided Cross-Attention and SWG Self-Attention. It guides the fusion of compressed feature Fcwith the global context embedding Ekto
generate feature Fk
g(k∈[extended,enclosed]) more aligned for the corresponding layout type. Finally, we use fully connected (FC) layers
to mapFk
gto horizon-depth and room height, which are further converted to boundary layouts ( Pextended andPenclosed ).
sumption [ 6]. For instance, LayoutNet [ 44] predicts corner
and boundary probability maps directly from panoramas.
Dula-Net [ 36] predicts 2D-ﬂoor plane semantic masks from
equirectangular and perspective views of ceilings. Zou et al.
presents improved versions, LayoutNet v2, and Dula-Net
v2 [45], demonstrating enhanced performance on cuboid
datasets. Fernandez et al. adopts equirectangular convolu-
tions (EquiConvs) [ 8] for generating corner and edge proba-
bility maps. HorizonNet [ 31] and HoHoNet [ 32] simplify lay-
out estimation by employing 1D representations and employ-
ing Bi-LSTM [ 14,27] and multi-head self-attention mech-
anisms [ 33] to establish long-range dependencies. LE D2-
Net [ 34] reformulates layout estimation as predicting the
depth of walls in the horizontal direction. AtlantaNet [ 24]
predicts room layouts by combining projections of the ﬂoor
and ceiling planes. DMH-Net [ 41] transforms panorama into
cubemap [ 9] and predicts the position of intersection lines
with learnable Hough Transform Block [ 40]. LGT-Net [ 17]
employs self-attention transformers [ 33] to learn geometric
relationships and capture long-range dependencies. DOP-
Net [ 29] disentangles 1D feature by segmenting features
into orthogonal plane representation, and uses GCN [ 18] and
transformer [ 33] to reﬁne the features.
These methods are designed only to predict a single lay-
out, which often encounters challenges posed by the inherent
ambiguity in dataset labels, resulting in suboptimal perfor-
mance. In contrast, our Bi-Layout model addresses this issue
by generating two distinct layout predictions through the
innovative integration of global context embeddings and our
shared feature guidance module design.
Multiple layout hypotheses. Several prior studies [ 10,12,
13,19,25,26,28,35,39] have employed multiple hypothe-
ses in their methods for estimating room layouts. The fun-
damental concept behind these methods involves leveraging
vanishing points, edges, or other pertinent information togenerate several rays or boxes as potential layout hypothe-
ses. Through various scoring function designs, one of the
hypotheses can be selected as the prediction that best ﬁts
the room. In contrast, our method generates two distinct
layout predictions, which can also be viewed as having two
hypotheses. However, the key disparity lies in the fact that
the previous methods only have one hypothesis deﬁning the
correct geometry. On the contrary, both of our predictions
are meaningful and offer two different geometries, extended
andenclosed layouts, allowing for ﬂexibility in choosing the
suitable one based on the speciﬁc requirements of different
use cases.
Query-based vision transformer. Transformers [ 33] have
exhibited considerable efﬁcacy in various high-level com-
puter vision tasks, including object detection [ 2,43], seg-
mentation [ 4,5], tracking [ 20,42], and ﬂoorplan reconstruc-
tion [ 30,37]. The standard transformer decoder utilizes fea-
ture embeddings as queries to extract relevant features from
the image feature, which acts as both the key and value.
Unlike the standard query-based transformer, our proposed
design utilizes the image feature as the query with our global
context embedding as the key and value. This unique design
allows our model to predict two distinct layouts, a departure
from prior methods that predict only a single layout.
3. Inherent Ambiguity in Labeled Data
We systematically examine instances of low IoU in the SoTA
methods [ 17,29,34] using MatterportLayout dataset [ 45].
Our analysis identiﬁes two types of ambiguity. First, when a
enclosed type GT label is given, the SoTA methods predict re-
gions located outside of that designated room (See Fig. 1(a)).
Conversely, when a extended type GT label is given, the
SoTA methods concentrate on the room where the camera is
positioned (See Fig. 1(b)). These ﬁndings underscore inher-
ent ambiguity within the testing GT labels. Moreover, since
28058
the SoTA models will predict either enclosed orextended
types of layouts, the same ambiguity is likely to be within
the training GT labels as well. This presents a substantial
challenge for single-prediction-based SoTA methods.
4. Method
To address the ambiguity issue in the dataset labels, we in-
troduce our Bi-Layout model as shown in Fig. 3, which
can generate two types of layout predictions Pextended and
Penclosed . Our model mainly consists of three modules: fea-
ture extractor (Sec. 4.1), global context embedding (Sec. 4.2),
and shared feature guidance module (Sec. 4.3). We describe
each component and the training objectives (Sec. 4.4) used
for training our Bi-Layout model in the following sections.
4.1. Feature Extractor
The feature extractor in our Bi-Layout model is shown in
Fig.3(a). We follow previous works [ 17,29,31,34] to use
ResNet-50 [ 11] architecture to extract 2D image features Fl
of 4 different scales from the input panorama I. For each
feature scale, we modify the module from [ 31] asSimpli-
ﬁed Height Compression Module (SHCM) to compress the
features along the image height direction and generate 1D
feature of the same dimension RN×D
4, whereNis the width
of feature map and Dis the feature dimension. Finally, we
concatenate these features from different scales to generate
the ﬁnal compressed feature Fc∈RN×D, whereN= 256
andD= 512 .
In contrast to previous works [ 17,31,34] settingD=
1024 , our design reduces model parameters. By pruning our
feature representation, we enhance the model’s efﬁciency
without compromising its effectiveness. To assess this design
choice, we present detailed ablation studies in Sec. 5.5.
4.2. Global Context Embedding
Once the compressed feature Fcis extracted, we introduce a
novel and learnable embedding mechanism termed as Global
Context Embedding . This mechanism captures and encodes
the overarching contextual information in a speciﬁc layout
label, as illustrated in Fig. 3(b). We employ two learnable
embeddings Ek∈RN×Dwherek∈[extended,enclosed],
one for extended and the other one for enclosed type. Dur-
ing training, these embeddings learn and encode diverse
contextual information associated with different types of
layout annotation. Moreover, they play a vital role in provid-
ing the dataset’s global context information when queried
by the compressed image feature Fcvia cross-attention in
our shared feature guidance module (refer to Sec. 4.3). By
infusing our compressed feature Fcwith this rich layout
type-related embedding Ek, we generate diverse and mean-
ingful predictions ( Pextended andPenclosed ), each aligned with
a distinct global context of the dataset label.
SWG Self-Attn.
Guided Cross-Attn.
V
K
QV
K
Q
Sinusoidal PEGlobal Context 
Embedding 𝐸𝐾
Compressed 
Image Feature𝐹𝑐SWG Self-Attn.
Guided Cross-Attn.
QKV
Layer 0 Layer M-1𝐹𝑔𝑘
∙∙∙ Learnable PE𝐹𝑔𝑐𝑎𝑘 𝐹𝑔𝑐𝑎𝑘Figure 4. Our Shared Feature Guidance Module architecture
(SFGM). It consists of two blocks: Guided Cross-Attention and
SWG Self-Attention. The module has M= 8 layers, and the
structure of each layer is identical. Given the compressed image
featureFcand global context embedding Ek, we ﬁrst apply the
sinusoidal and learnable positional encoding, respectively. With
the compressed feature Fcas the query Qand our global context
embedding Ekas both the key Kand value V, our guided cross-
attention generates the feature Fk
gca, and it is served as QKV
inputs to SWG self-attention. This process will repeat and further
reﬁne the output feature with our global context embedding to
generate the ﬁnal guided feature Fk
g.
4.3. Shared Feature Guidance Module
Building upon the compressed image feature Fcand the
global context embeddings Ek, as shown in Fig. 3(c), we
present an innovative component called Shared Feature Guid-
ance Module (SFGM). This module can effectively guide
the fusion of the image feature with the target global context
embedding. Speciﬁcally, we share the compressed image
featureFcand use different global context embeddings Ek
(one at a time) as the inputs for our shared feature guidance
moduleSFGM(·)to generate corresponding guided feature
Fk
g∈RN×D, denoted as:
Fk
g=SFGM(Fc,Ek), k∈[extended,enclosed].(1)
Our shared feature guidance module consists of Guided
Cross-Attention and SWG Self-Attention as the building
blocks, and the details of the architecture are shown in Fig. 4.
The standard cross-attention setting in DETR [ 2] or other
high-level tasks [ 4,5,20,30,37,42] uses embeddings as the
query Q to retrieve relevant information from the correspond-
ing image feature, which acts as both the key Kand value V
in order to generate the target outputs. In our scenario, if we
adopt the standard QKV setting, the shared image feature
Fcalone may not carry sufﬁcient information to distinguish
between the two types of distinct layouts. Therefore, we
reverse this relationship and use our global context embed-
dingsEkto learn from corresponding labels, guiding the
image feature Fcto generate the desired layout types.
As shown in Fig. 4, we use the compressed feature Fc
as the query Qand our global context embedding Ekas
both the key Kand value Vin our guided cross-attention
28059
GCA(·):
Q= (Fc+PE sin)Wq,K= (Ek+PE learn)Wk,
V= (Ek+PE learn)Wv, Fk
gca=GCA(Q,K,V),(2)
whereFk
gca∈RN×Dis the output of our guided cross-
attention block. We apply different positional encoding
strategies for these features, utilizing learnable positional
encoding [ 2]PE learn∈RN×Dfor our global context
embedding Ekand sinusoidal positional encoding [ 1,21]
PE sin∈RN×Dfor the compressed image feature Fc. Each
feature is then multiplied by its respective learnable weights
Wq/k/v∈RD×D. This unique design choice enables us to
enrich the image feature Fcby effectively incorporating our
global context embedding Ek. Subsequently, this enriched
featureFk
gcais served as QKV inputs to the SWG self-
attention module [ 17] for further enhancement. As demon-
strated in [ 17], the SWG self-attention module can effec-
tively establish local and global geometric relationships
within the room layout. Then, the process of guided cross-
attention and SWG self-attention is repeated several times
to reﬁne the image feature with our context embeddings to
generate the ﬁnal guided feature Fk
g, as shown in Fig. 4.
By employing this novel cross-attention design, we
achieve an enriched and context-aware guided feature rep-
resentation Fk
gthat is subsequently utilized for generating
our Bi-Layout predictions ( Pextended andPenclosed ), each pos-
sessing distinct and valuable properties. This ﬂexibility in
our method enables us to provide diverse layout predictions
tailored to different global context embeddings Ekand input
panorama features Fc. Built on this architectural design, our
model can be compact and efﬁcient to generalize to more
label types by increasing global context embeddings Ek.
4.4. Training Objective
After obtaining the target feature Fk
g, we follow [ 17,29]
using fully connected (FC) layers to map the feature Fk
gto
horizon-depth dk={di
k}N
i=1and room height hk, whereN
is the width of the feature map. We can apply the explicit
transformation to convert the horizon depth and room height
to layout boundaries Pkon the panorama. We further convert
column-wise depth di
kinto depth normal ni
kand gradient of
normalgi
k,k∈[extended,enclosed].
The loss functions for depth, normal, gradient, and room
height are deﬁned as follows:
Lk
depth=1
N/summationdisplay
i∈N|di
k−di
gt|,Lk
normal=1
N/summationdisplay
i∈N(−ni
k·ni
gt),
Lk
gradient=1
N/summationdisplay
i∈N|gi
k−gi
gt|,Lk
height=|hk−hgt|,(3)
wheredi
gt,ni
gt,gi
gt, andhgtdenote the ground truth of depth,
normal, gradient, and room height respectively. We calculatethe L1 loss for depth loss, gradient loss, height loss, and
cosine similarity for normal loss. Our branch loss Lkis:
Lk=λdLk
depth+λnLk
normal+λgLk
gradient+λhLk
height,(4)
wherek∈[extended,enclosed]and the ﬁnal loss Ltotal=
Lextended+Lenclosed . We setλd= 0.9,λn= 0.1,λg= 0.1
andλh= 0.1for both branches to balance the model weight.
5. Experiments
We conduct our experiments on a single NVIDIA RTX 4090
GPU and implement the proposed method with PyTorch [ 22].
For training, we use a batch size of 12 and set the learning
rate to1×10−4. We select Adam as our optimizer, adhering
to its default conﬁgurations. For the data augmentation, we
use the technique proposed in [ 31], including left-right ﬂip-
ping, panoramic horizontal rotation, luminance adjustment,
and panoramic stretch.
5.1. Datasets
MatterportLayout. MatterportLayout [ 45] contains 2295
samples labeled by Zou et al. [45]. However, as we ana-
lyzed in Sec. 3, this dataset has annotation ambiguity, and
many images with ambiguity are annotated with the extended
type. Hence, we propose a semi-automatic procedure (pro-
vide the details in supplementary material) to re-annotate
enclosed type labels from the ambiguous extended ones. We
re-annotate 15% of the labels in the whole dataset. Note
that these new labels of enclosed type plus the remaining
85% of original labels will be used to train our Bi-Layout
model’s enclosed branch. In contrast, all original labels will
be used to train the extended branch. For a fair comparison
with SoTA methods, we use the original label and the same
testing split for evaluation.
ZInD. ZInD [ 7] dataset is currently the largest dataset with
room layout annotations. It provides both rawandvisible
labels, which is similar to our deﬁned enclosed andextended
types, respectively. Besides, ZInD separates the data into
simple andcomplex subsets based on whether the images
have contiguous occluded corners. Therefore, we have two
variants of ZInD in our experiments: (a) ZInD-Simple rep-
resents the simple subset and consists of 24882, 3080, and
3170 panoramas for training, validation, and testing splits.
(b) ZInD-All represents the whole dataset with 50916, 6352,
and 6352 panoramas for each split. It has complex opening
regions, resulting in more severe ambiguity issues. Therefore,
it can better evaluate the robustness of different methods for
handling the ambiguity issue. In both ZInD dataset variants,
we use the rawandvisible labels to train our enclosed andex-
tended branches, respectively, and follow the SoTA methods
to test on rawlabels for a fair comparison.
28060
(a) Full set MatterportLayout [ 45] ZInD-Simple [ 7] ZInD-All [ 7]
Method # Params 2DIoU(%) 3DIoU(%) 2DIoU(%) 3DIoU(%) 2DIoU(%) 3DIoU(%)
LED2Net [ 34] 82 M 82.37 80.05 90.20 88.34 82.31 80.28
LGT-Net [ 17] 136 M 83.52 81.11 91.77 89.95 84.07 82.09
DOP-Net [ 29] 137 M 84.11 81.70 91.94 90.13 83.92 81.87
Ours (equivalent branch) 102 M 84.56 82.05 92.07 90.25 84.90 82.96
Ours (disambiguate) 102 M 85.10 82.57 92.79 90.95 86.21 84.22
(b) Subset MatterportLayout [ 45] ZInD-Simple [ 7] ZInD-All [ 7]
Method # Params 2DIoU(%) 3DIoU(%) 2DIoU(%) 3DIoU(%) 2DIoU(%) 3DIoU(%)
LED2Net [ 34] 82 M 53.57 51.12 45.31 44.10 48.76 47.35
LGT-Net [ 17] 136 M 53.17 50.54 53.20 52.00 50.89 49.58
DOP-Net [ 29] 137 M 57.13 54.80 51.55 50.26 50.92 49.46
Ours (equivalent branch) 102 M 59.85 57.08 55.09 53.76 54.22 52.78
Ours (disambiguate) 102 M 62.81 59.97 62.10 60.63 60.20 58.53
Table 1. Full set and Subset evaluation. Equivalent branch represents the output, which is trained with the same label as baseline methods.
Disambiguate is our proposed metric.
5.2. Disambiguate Metric
If we already know the layout type of each test image, we
can use this information to select the output from the corre-
sponding branch for evaluation. However, we ﬁnd that the
test data has annotation ambiguity; even the rawlabels in
ZInD are still not exempt from this issue.
To address the above issue and demonstrate our model’s
capability to handle the annotation ambiguity, we introduce
a new metric, termed the disambiguate metric, as follows:
IoU disambiguate =S/summationdisplay
i=0argmax
Pi
k∈PIoU(Pi
gt, Pi
k), (5)
wherePi
k∈P, k∈[extended,enclosed]denotes layout
predictions from both branches and Pi
gtdenotes the ground
truth layout. We ﬁrst calculate the Intersection over Union
(IoU) between each prediction and ground truth (GT) for
each image and then select the higher IoU for averaging
all samples. This is because the higher IoU serves as the
disambiguate prediction and represents the most suitable
prediction when encountering ambiguity.
The proposed metric effectively provides a robust and
quantitative measure of how a method excels in handling
ambiguous scenarios within the dataset without necessitating
manual corrections to the ambiguous annotations. In other
words, we can use the labels provided by the original dataset
to do the evaluation.
5.3. Comparison with State­of­the­Art Methods
Evaluation settings. Since our model outputs two layouts,
we propose to compare our method with the SoTA meth-
ods in two ways. Using the equivalent branch : We use
the output from the branch trained with the same data as
other methods. Speciﬁcally, we use the output from extendedbranch for the MatterportLayout dataset and the output from
enclosed branch for ZInD to fairly compare with other meth-
ods. Using both branches with disambiguate metric : We
use the proposed disambiguate metric as deﬁned in Sec. 5.2
to evaluate the performance of our method.
Full set evaluation. We present quantitative results on
three different datasets in Table 1(a). As the SoTA meth-
ods do not experiment on the ZInD-All dataset, we retrain
all baseline models based on their ofﬁcial repositories. The
results demonstrate that our equivalent branch consistently
outperforms SoTA methods across all datasets, underscoring
the advantages of joint training with bi-layout data. Further-
more, our disambiguated results surpass these benchmarks,
afﬁrming the existence of ambiguity in the original annota-
tions. Our Bi-Layout model effectively mitigates this issue
by selecting the most appropriate prediction. Notably, in
terms of model size, our architecture, despite generating
bi-layout predictions, maintains a smaller total parameter
size compared to the SoTA models. This underscores the
efﬁciency of our design in achieving superior performance
with a more compact model.
Subset evaluation. To highlight the ambiguity issue, we
select a subset based on the failure predictions of all the
previous SoTA models [ 17,29,34]. For each SoTA model,
we ﬁnd images with the 2DIoU evaluation lower than 0.6.
Next, we combine all these failure cases among all the SoTA
models to construct the subset. Finally, the subsets consist
of11%,6%, and18% of the test data in three datasets, re-
spectively. The quantitative results in Table 1(b) reveal a
more pronounced performance gap between our method and
the SoTA models, with differences reaching up to 9.28%
in 2DIoU on the most ambiguous ZInD-All dataset. We
also provide the qualitative results in Fig. 5, where a bird’s-
28061
LGT-Net DOP -Net Ours
(a) Qualitative comparison on the MatterportLayout [ 45] dataset.LGT-Net DOP -Net Ours
(b) Qualitative comparison on the ZInd [ 7] dataset.
Figure 5. Qualitative comparison on the MatterportLayout [ 45] (top) and ZInd [ 7] datasets (bottom). Blue andGreen represent ground truth
labels and predictions, respectively. The boundaries of the room layout are on the left, and their bird’s eye view projections are on the right.
We show our disambiguate results, which effectively address the ambiguity issue, while the SoTA methods struggle with the ambiguity, as
highlighted in dashed lines.
eye view of the predictions vividly illustrates the signiﬁcant
challenges posed by ambiguity. This conﬁrms that layout
ambiguity is a key cause for low IoUs of previous methods,
and our Bi-Layout estimation is effective in addressing the
issue and performs remarkably well in this subset.
5.4. Ambiguity Detection
Our Bi-Layout model can naturally detect ambiguous re-
gions by comparing the per-column pixel difference between
two predicted layout boundaries. This per-column pixel dif-
ference can serve as our predicted conﬁdence score. If the
difference is larger, the column is more likely to be an am-
biguous region (i.e., typically an opening room structure).
We formulate the detection of ambiguous regions into a bi-
nary classiﬁcation task where GT ambiguous regions are
columns with more than 2 pixels’ difference between two
annotations of the extended andenclosed types. We predict
Figure 6. Qualitative results for ambiguity detection. Blue and
Green on the top and bottom rows represent ground truth and
predicted conﬁdence, respectively. Cyan andMegenta lines are
ourextended andenclosed type layout predictions. With these two
predictions, our model can accurately detect ambiguous regions.
columns with more than 10 pixels’ difference between pre-
dicted layouts as ambiguous regions. We test this method on
ZInD as it is the only dataset that provides both types of GT
28062
PanoramaExtended
Pred.
Enclosed
Pred.Transformer
Transformer Panorama
(a) Two distinct modelPanoramaExtended
Pred.
Enclosed
Pred.Transformer
Transformer
(b) Two distinct transformer
Shared
Guidance 
ModulePanorama
Enclosed
Pred.Extended
Pred.Extended Embed.
Enclosed Embed.
(c) Our two embedding with shared guidance moduleFigure 7. Model architecture comparison. We show the different
model architecture designs for predicting multiple layouts. Please
refer to Table 3for quantitative comparison.
labels ( i.e., raw andvisible ) in testing. Our method achieves
a reasonably high Precision of 0.82 and Recall of 0.71. More-
over, the qualitative results in Fig. 6further demonstrate that
our method can indeed detect ambiguous regions. We be-
lieve this is particularly useful for applications where the
model can highlight ambiguous regions and let the users
select suitable predictions for their use cases.
5.5. Abalation Studies
Different feature fusion designs. To fuse the information
from image features and our global context embeddings, we
conduct comprehensive ablation studies to validate the ef-
fectiveness of different designs. The fusion methods include
add, concatenation, AdaIn [ 16], and FiLM [ 23]. We further
investigate two Query-Key-Value (QKV) feature designs in
our shared feature guidance module.
The results in Table 2show that our proposed design sig-
niﬁcantly outperforms all feature fusion methods and the
standard cross-attention setting, where the global context
embedding Ekserved as query Q. The compressed image
featureFcserved as key Kand value V. This demonstrates
that our design can effectively utilize contextual informa-
tion within the embedding Ekto enhance the alignment of
the compressed feature Fcwith the corresponding layout
prediction type.
Comparions of model architectures. As shown in Fig. 7,
we compare three model architectures to predict multiple
layouts. We conduct the quantitative comparison in Table 3.
Two models : Training two models of the same architecture to
predict two layout types is the most straightforward design
(Fig. 7(a)). This architecture achieves good performance
without interference between learning two types of layout.
However, this doubles the model size and training time.
Two transformers : An alternative is to share the feature
extractor but separate the transformer and prediction head
(Fig. 7(b)). This saves model size a little, but the performance
drops signiﬁcantly as it cannot handle the interference when
learning two types of layouts simultaneously.Design Q KV 2D IoU (%) 3D IoU (%)
Add 83.79 81.26
Concat 84.17 81.44
AdaIn [ 16] 83.28 80.54
FiLM [ 23] 84.08 81.39
Standard EkFc 83.34 80.85
Ours FcEk 85.10 82.57
Table 2. Comparison of QKV feature designs. Fcis the com-
pressed feature, and Ekis our global context embedding. We
evaluate different designs of QKV features in the Shared Feature
Guidance Module on MatterportLayout [ 45] with our proposed
disambiguate metric.
Method # Params 2DIoU(%) 3DIoU(%)
Two models 272 M 85.29 82.72
Two Transformers 203 M 84.35 81.88
Ours (c = 1024) 172 M 85.25 82.76
Ours (c = 512) 102 M 85.10 82.57
Table 3. Model size and performance trade-off. In this part, we
only compare to the LGT-Net [ 17] model variances (i.e., The ﬁrst
two rows) since our model is built on top of its architecture. In
the third row, c represents the # channel of the image feature. We
conduct these quantitative evaluations on MatterportLayout [ 45]
with our proposed disambiguate metric. Our ﬁnal model strikes the
best balance between performance and parameter efﬁciency.
Our model : Our model is the smallest as we share both
the feature extractor and transformer and only separate the
lightweight prediction head (Fig. 7(c)). To reduce the in-
terference in learning two layouts, we introduce two learn-
able global context embeddings, which can inject layout
type-related context information into the image feature via
cross-attention. Therefore, our model achieves comparable
or better performance than others. In addition, there is only
a slight performance drop if we further reduce the model
size by reducing the compressed feature channel dimension
from 1024 to 512. Our ﬁnal model ( c= 512 ) balances per-
formance and parameter efﬁciency best.
6. Conclusion
We propose a novel Bi-Layout model to generate two dis-
tinct predictions, effectively resolving the layout ambiguity.
Most importantly, we introduce a novel embedding mech-
anism with a shared feature guidance module, where each
embedding is designed to learn the global context inherent
in each type of layout. Our model strikes a good balance
between model compactness and prediction accuracy with
these designs. In addition, we propose a disambiguate met-
ric to evaluate the accuracy with multiple predictions. On
MatterportLayout [ 45] and ZInD [ 7] datasets, our method
outperforms other state-of-the-art methods, especially on the
subset setting with considerable ambiguity.
28063
References
[1]Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,
and Quoc V Le. Attention augmented convolutional networks.
InICCV , 2019. 5
[2]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In ECCV , 2020. 2,3,
4,5
[3]Xiaoxue Chen, Hao Zhao, Guyue Zhou, and Ya-Qin Zhang.
Pq-transformer: Jointly parsing 3d objects and layouts from
point clouds. IEEE Robotics and Automation Letters , 2022. 1
[4]Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-
pixel classiﬁcation is not all you need for semantic segmenta-
tion. NeurIPS , 2021. 2,3,4
[5]Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander
Kirillov, and Rohit Girdhar. Masked-attention mask trans-
former for universal image segmentation. In CVPR , 2022. 2,
3,4
[6]James M Coughlan and Alan L Yuille. Manhattan world:
Compass direction from a single image by bayesian inference.
InICCV , 1999. 3
[7]Steve Cruz, Will Hutchcroft, Yuguang Li, Naji Khosravan,
Ivaylo Boyadzhiev, and Sing Bing Kang. Zillow indoor
dataset: Annotated ﬂoor plans with 360deg panoramas and
3d room layouts. In CVPR , 2021. 1,5,6,7,8
[8]Clara Fernandez-Labrador, Jose M Facil, Alejandro Perez-
Yus, C ´edric Demonceaux, Javier Civera, and Jose J Guerrero.
Corners for layout: End-to-end layout recovery from 360
images. IEEE Robotics and Automation Letters , 2020. 3
[9]Ned Greene. Environment mapping and other applications
of world projections. IEEE Computer Graphics and Applica-
tions , 1986. 3
[10] Abhinav Gupta, Martial Hebert, Takeo Kanade, and David
Blei. Estimating spatial layout of rooms using volumetric
reasoning about objects and surfaces. NeurIPS , 2010. 3
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR , 2016.
4
[12] Varsha Hedau, Derek Hoiem, and David Forsyth. Recovering
the spatial layout of cluttered rooms. In ICCV , 2009. 3
[13] Martin Hirzer, Vincent Lepetit, and PETER ROTH. Smart
hypothesis generation for efﬁcient and robust room layout
estimation. In WACV , 2020. 3
[14] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term
memory. Neural Computation , 1997. 3
[15] Siyuan Huang, Siyuan Qi, Yixin Zhu, Yinxue Xiao, Yuanlu
Xu, and Song-Chun Zhu. Holistic 3d scene parsing and re-
construction from a single rgb image. In ECCV , 2018. 1
[16] Xun Huang and Serge Belongie. Arbitrary style transfer in
real-time with adaptive instance normalization. In ICCV ,
2017. 8
[17] Zhigang Jiang, Zhongzheng Xiang, Jinhua Xu, and Ming
Zhao. Lgt-net: Indoor panoramic room layout estimation with
geometry-aware transformer network. In CVPR , 2022. 1,3,
4,5,6,8[18] Thomas N Kipf and Max Welling. Semi-supervised classi-
ﬁcation with graph convolutional networks. In ICLR , 2017.
3
[19] Arun Mallya and Svetlana Lazebnik. Learning informative
edge maps for indoor scene layout prediction. In ICCV , 2015.
3
[20] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and
Christoph Feichtenhofer. Trackformer: Multi-object tracking
with transformers. In CVPR , 2022. 2,3,4
[21] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz
Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image
transformer. In ICML , 2018. 5
[22] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An
imperative style, high-performance deep learning library.
NeurIPS , 2019. 5
[23] Ethan Perez, Florian Strub, Harm De Vries, Vincent Du-
moulin, and Aaron Courville. Film: Visual reasoning with a
general conditioning layer. In AAAI , 2018. 8
[24] Giovanni Pintore, Marco Agus, and Enrico Gobbetti. At-
lantanet: inferring the 3d indoor layout from a single 360 ◦
image beyond the manhattan world assumption. In ECCV ,
2020. 3
[25] Srikumar Ramalingam, Jaishanker K Pillai, Arpit Jain, and
Yuichi Taguchi. Manhattan junction catalogue for spatial
reasoning of indoor scenes. In CVPR , 2013. 3
[26] Yuzhuo Ren, Shangwen Li, Chen Chen, and C-C Jay Kuo.
A coarse-to-ﬁne indoor layout estimation (cﬁle) method. In
ACCV , 2017. 3
[27] Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent
neural networks. IEEE Transactions on Signal Processing ,
1997. 3
[28] Alexander G Schwing and Raquel Urtasun. Efﬁcient exact
inference for 3d indoor scene understanding. In ECCV , 2012.
3
[29] Zhijie Shen, Zishuo Zheng, Chunyu Lin, Lang Nie, Kang
Liao, Shuai Zheng, and Yao Zhao. Disentangling orthogonal
planes for indoor panoramic room layout estimation with
cross-scale distortion awareness. In CVPR , 2023. 1,3,4,5,6
[30] Jheng-Wei Su, Kuei-Yu Tung, Chi-Han Peng, Peter Wonka,
and Hung-Kuo Chu. Slibo-net: Floorplan reconstruction via
slicing box representation with local geometry regularization.
InNeurIPS , 2023. 2,3,4
[31] Cheng Sun, Chi-Wei Hsiao, Min Sun, and Hwann-Tzong
Chen. Horizonnet: Learning room layout with 1d representa-
tion and pano stretch data augmentation. In CVPR , 2019. 1,
3,4,5
[32] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Hohonet: 360
indoor holistic understanding with latent horizontal features.
InCVPR , 2021. 1,3
[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. NeurIPS , 2017. 3
[34] Fu-En Wang, Yu-Hsuan Yeh, Min Sun, Wei-Chen Chiu, and
Yi-Hsuan Tsai. Led2-net: Monocular 360deg layout estima-
tion via differentiable depth rendering. In CVPR , 2021. 1,3,
4,6
28064
[35] Huayan Wang, Stephen Gould, and Daphne Roller. Discrimi-
native learning with latent variables for cluttered indoor scene
understanding. Communications of the ACM , 2013. 3
[36] Shang-Ta Yang, Fu-En Wang, Chi-Han Peng, Peter Wonka,
Min Sun, and Hung-Kuo Chu. Dula-net: A dual-projection
network for estimating room layouts from a single rgb
panorama. In CVPR , 2019. 3
[37] Yuanwen Yue, Theodora Kontogianni, Konrad Schindler, and
Francis Engelmann. Connecting the dots: Floorplan recon-
struction using two-level queries. In CVPR , 2023. 2,3,4
[38] Cheng Zhang, Zhaopeng Cui, Yinda Zhang, Bing Zeng, Marc
Pollefeys, and Shuaicheng Liu. Holistic 3d scene understand-
ing from a single image with implicit representation. In CVPR ,
2021. 1
[39] Hao Zhao, Ming Lu, Anbang Yao, Yiwen Guo, Yurong Chen,
and Li Zhang. Physics inspired optimization on semantic
transfer features: An alternative method for room layout esti-
mation. In CVPR , 2017. 3
[40] Kai Zhao, Qi Han, Chang-Bin Zhang, Jun Xu, and Ming-Ming
Cheng. Deep hough transform for semantic line detection.
TPAMI , 2021. 3
[41] Yining Zhao, Chao Wen, Zhou Xue, and Yue Gao. 3d room
layout estimation from a cubemap of panorama image via
deep manhattan hough transform. In ECCV , 2022. 3
[42] Xingyi Zhou, Tianwei Yin, Vladlen Koltun, and Philipp
Kr¨ahenb ¨uhl. Global tracking transformers. In CVPR , 2022.
2,3,4
[43] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,
and Jifeng Dai. Deformable detr: Deformable transformers
for end-to-end object detection. In ICLR , 2021. 3
[44] Chuhang Zou, Alex Colburn, Qi Shan, and Derek Hoiem.
Layoutnet: Reconstructing the 3d room layout from a single
rgb image. In CVPR , 2018. 3
[45] Chuhang Zou, Jheng-Wei Su, Chi-Han Peng, Alex Colburn,
Qi Shan, Peter Wonka, Hung-Kuo Chu, and Derek Hoiem.
Manhattan room layout reconstruction from a single 360ˆ ◦
360◦image: A comparative study of state-of-the-art methods.
IJCV , 2021. 1,2,3,5,6,7,8
28065
