Towards High-fidelity Artistic Image Vectorization via Texture-Encapsulated
Shape Parameterization
Ye Chen1Bingbing Ni1,2∗Jinfan Liu1Xiaoyang Huang1Xuanhong Chen1,2
1Shanghai Jiao Tong University, Shanghai 200240, China
2USC-SJTU Institute of Cultural and Creative Industry
{chenye123, nibingbing, chen19910528 }@sjtu.edu.cn
Abstract
We develop a novel vectorized image representation
scheme accommodating both shape/geometry and texture
in a decoupled way, particularly tailored for reconstruction
and editing tasks of artistic/design images such as Emojis
and Cliparts. In the heart of this representation is a set
of sparsely and unevenly located 2D control points. On
one hand, these points constitute a collection of paramet-
ric/vectorized geometric primitives ( e.g., curves and closed
shapes) describing the shape characteristics of the target
image. On the other hand, local texture codes, in terms
of implicit neural network parameters, are spatially dis-
tributed into each control point, yielding local coordinate-
to-RGB mappings within the anchored region of each con-
trol point. In the meantime, a zero-shot learning algorithm
is developed to decompose an arbitrary raster image into
the above representation, for the sake of high-fidelity im-
age vectorization with convenient editing ability. Extensive
experiments on a series of image vectorization and editing
tasks well demonstrate the high accuracy offered by our
proposed method, with a significantly higher image com-
pression ratio over prior art.
1. Introduction
How to represent images is a fundamental problem in the
field of computer vision. Traditionally, images are rep-
resented by pixels stored on fixed and discrete grids ( i.e.,
raster images). One main advantage of such representation
is that, there is virtually no limit to its ability to express im-
age details with enough pixels. Nevertheless, pixel-based
representation is plagued by numerous limitations. On one
hand, it suffers from excessive redundancy due to its stor-
age on fixed grids, which inherently constrains image res-
olution, along with inevitable information loss during im-
age re-scaling. On the other hand, most importantly, pixel-
∗Corresponding author: Bingbing Ni
,
Lossy Texture Details
Lossy Pixel EditingRedundant ParametersRaster
Compact Parameters!!……,!"!#!!……!"!#,!!……!"!#"×$×3……Vector&ℎ(!)_+,-×(2-+4)3145728(1024×1024×3)
256×(2×12+4)=7168---Figure 1. Illustration of our motivation. Raster images that
store pixel values on fixed and discrete grids suffer from excessive
parameter redundancy and a high coupling between image geom-
etry and texture, resulting in challenging image editing. Vector
images represent visual concepts with vectorized geometric prim-
itives in a very compact parameter format, endowed with great
editability. However, vector images are not suitable for express-
ing image texture details. This work explores a compact vector-
ized image representation that decouples images in geometric
and texture space, tailored for high-fidelity texture-rich image vec-
torization, facilitating easy image editing.
based representation stores all the information of an image
in the RGB values of the pixels, resulting in a high coupling
between the representation of image geometry and texture.
Hence, it is challenging to edit the shape/geometry and tex-
ture of a raster image separately.
Vector images embody an alternative paradigm for rep-
resenting visual information, which describe images with
a collection of parametric/vectorized geometric primitives
(e.g., curves and closed shapes), defined as control points
in the continuous space. Compared to representing images
with fixed discrete grids, vector images present many ad-
vantages. 1) Resolution-Agnostic: Pixel-based image re-
scaling often introduces unexpected noise/artifact that is
hard to identify and remove. In contrast, modeling im-
ages with continuous parameters enables the storage and
generation of images in arbitrary resolutions without in-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
15877
formation loss. 2) Easy Editing: Vector images have ex-
plicit/explainable parameter format, facilitating convenient
image editing ability by simply modifying shape or color
parameters in a structured way. 3) Lossless Compression:
The shape primitives in vector images are typically defined
by mathematical functions, allowing for the expression of
complex shapes with a minimal number of control points.
Therefore, the number of parameters required to store a vec-
tor image is significantly lower than that of a raster image.
Unfortunately, while vector images possess indisputable
advantages in representing geometric information, they fall
short in effectively capturing the texture details found in
images, which restricts their applications to simple images
(e.g. Emojis, Fonts and Icons). As depicted in Fig. 1, cur-
rent image vectorization representations [7, 15, 17, 25, 26]
suffer from significant loss of fine texture details when con-
fronted with regions containing intricate textures. The un-
derlying reason can be attributed to the fact that traditional
vectorization representations store a single color within
each shape, rather than capturing the distribution of colors.
Consequently, this representation inherently exhibits ineffi-
ciency in conveying rich image textures.
To address both limitations, we propose a memory-
efficient texture-encapsulated shape-vectorized image
representation that decomposes an image into dual-
parameterized geometry and texture space, facilitating
high-fidelity representation of complicated image geometry
and texture as well as decoupled editing ability. Our
novel image representation features the following designs.
In a nutshell, geometry is encoded with closed B ´ezier
shapes parameterized into the coordinates of control
points, similar to contemporary image vectorization algo-
rithms [7, 15, 17, 26]. In the meantime, to represent texture,
we follow the emerging implicit neural representation
scheme [22, 29], which is parameterized into a lightweight
MLP that takes local latent encodings of coordinate
information as input and predicts local texture ( i.e. RGB
values). Drawing inspiration from distributed implicit
representations [23], these local texture codes are spatially
distributed into each control point of our geometric repre-
sentation, namely, each control point stores a latent texture
code which takes charge of the local coordinate-to-RGB
mappings within the surrounding region. To this end, an
image is parameterized to a set of coordinates of control
points assigned with corresponding latent texture codes and
a lightweight MLP shared across the whole image space,
which fully capture image characteristics including color,
texture, and geometry, in a decoupled way. Compared
to current methods that store texture parameters in fixed
grids, our shape control point anchored distributed texture
representation scheme substantially minimizes storage
overhead, while assigning more resources for describing
those more crucial local visual regions (patches).Leveraging our developed representation, we accom-
plish remarkably high-fidelity image vectorization ( i.e. pa-
rameterized reconstruction and editing) with a straight-
forward zero-shot learning algorithm based on self-
supervision. Unlike previous algorithms that rely on dif-
ferentiable rasterization to estimate gradients and optimize
control point coordinates, which often encounters large gra-
dient error, we efficiently constrain the B ´ezier shapes to
match crucial areas in image space by constructing a ge-
ometric field, thus encouraging accurate parameter fitting
and fast convergence. We extensively experiment the pro-
posed framework for a series of image vectorization tasks
including Emoji [1], Icon [2] and our developed Clipart
benchmarks. It is demonstrated that our method achieves
much better image vectorization quality with a significantly
reduced number of parametric shapes, compared to prior
art. In addition, compared to image reconstruction meth-
ods based on implicit neural representations, our method
achieves better reconstruction results with only 10×fewer
latent codes, which significantly demonstrates the effective-
ness and efficiency of our representation. Moreover, our
method can edit images by simply editing the shape and
texture parameters in a decoupled way.
2. Related Works
Image Vectorization. Image vectorization is an impor-
tant research area in computer graphics. Traditional ap-
proaches [8, 33] typically utilize a two-stage algorithm
that involves an initial step of image segmentation, fol-
lowed by targeted vectorization of the segmented regions.
In the era of deep learning, several neural network-based
methods [7, 26] are proposed inspired by the differentiable
rasterization method DiffVG [15]. Im2Vec [26] uses a
variable-complexity closed B ´ezier path as the fundamental
graphic primitive. Chen et al. [7] utilize several geometric
primitives like triangle and rectangle to fit the image. Nev-
ertheless, these methods all face challenges when it comes
to vectorizing complex images, as predicting a large num-
ber of control points simultaneously solely based on pixel
loss is very difficult. LIVE [17] is a remarkable work that
generates compact vectorized representations for complex
images with a meticulously designed layer-wise path ini-
tialization technique. Although LIVE can generate decent
vectorization results for complex images in almost any do-
main, it has inherent limitations in representing image tex-
tures due to a lack of exploration of internal color distri-
butions within closed B ´ezier shapes. As a result, it relies
on stacking local shapes to express texture details. Du et
al. [9] propose to use linear gradients to define the spa-
tially varying color within local image regions, endowed
with the ability to edit images easily in a structured way.
However, the linear gradient layers rely heavily on an in-
tricate layer configuration strategy. In this work, we in-
15878
troduce a texture-encapsulated shape-vectorized represen-
tation to capture complex image textures, achieving high-
fidelity image vectorization with a simplistic framework.
Implicit Neural Representation. Implicit neural repre-
sentation is widely applied in the field of 3D vision [3, 5,
11, 12, 21, 28], which models 3D shapes and appearances
with MLPs that map coordinates to signals. Particularly,
NeRF [22] inspires a series of outstanding works [4, 13,
16, 19, 24, 32]. Recent advances [5, 10, 20, 23] investigate
the utilization of latent representations to balance between
memory usage and computational expenses of heavy MLPs.
While implicit neural representation achieves success in
3D tasks, its applications in 2D domain [6, 14, 29–31] are
relatively unexplored. Deep Image Prior [31] is a pioneer-
ing work to represent images using deep ConvNets with
abundant neural parameters. SIREN [29] parameterizes 2D
images with MLP and novel periodic activation functions.
However, the MLPs of SIREN are highly redundant and in-
efficient, and unacceptable reconstruction errors occur as
the hidden layers are further reduced. This work proposes
an efficient and flexible representation by encapsulating la-
tent codes into crucial shape parameters, which can be op-
timized with a zero-shot learning framework.
3. Methodology
3.1. Overview
The overall framework is illustrated in Fig. 2. Our frame-
work decouples the raster image I∈RH×W×3in geo-
metric space and texture space using a texture-encapsulated
shape-vectorized representation, i.e., a parameterized shape
representation (Sec. 3.2) and a shape-anchored implicit neu-
ral texture representation (Sec. 3.3).
We utilize parameterized closed B ´ezier shapes (defined
as coordinates of control points, as implemented in [15, 17])
to represent the fundamental geometric information in the
image. We establish a geometric field based on image edge
points to constrain the coordinates of control points of all
B´ezier shapes, enabling B ´ezier shapes to efficiently cover
regions with rich geometric information in the image space.
Then we introduce a shape-anchored implicit neural tex-
ture representation to explore image textures. Specifically,
our framework assigns an optimizable latent code zto each
control point in the shape representation. Then a shape-
aware interpolation function ϕ(no parameters) and a train-
able lightweight MLP fθ(with θas its parameters) are uti-
lized to represent the texture in an implicit coordinate-to-
RGB way, where the texture information at arbitrary posi-
tion can be spatially interpolated and retrieved.
To conclude, we parameterize the input image as a set of
B´ezier shapes attached with trainable texture latents and a
lightweight MLP fθ, which can be formulated as:
I∼ {S={s1,s2, ...,sn}, θ}, (1)where ndenotes the shapes needed to represent an image.
Andsitakes the form:
si={(pi1,zi1),(pi2,zi2), ...,(pim,zim)}, (2)
where p∗= (x∗, y∗)∈R2denotes the coordinate of con-
trol points and z∗∈Rdrepresents the latent texture code
assigned to the corresponding control point. mdenotes the
number of control points for each shape. More details are
elaborated in the following sections.
3.2. Parameterized Shape Representation
We use B ´ezier shapes defined by control points to repre-
sent image geometry. Common image vectorization meth-
ods tend to optimize the control points with pixel loss by uti-
lizing the gradients approximated by differentiable renderer
(e.g., DiffVG [15]), which suffers from the issue of gradient
sparsity. In contrast, we introduce a geometric field to opti-
mize the coordinates of control points. For the input raster
image, we use the Canny operator to extract the edge points
E={ei}and generate the corresponding geometric field.
More concretely, we think that the closer a point is to the
edge of an image, the more geometric information it con-
tains. Hence, we define the geometric field as a probability
field, which describes the importance of each query point in
describing the image geometry. The ground-truth geometric
fieldGedefined by edge points can be formulated as:
Ge(q) = 1−tanh(min
i||q−ei||2
2
σ2), (3)
where q= (x, y)∈R2denotes any query coordinate in the
image space and σis used to control the range of the field.
We utilize the geometric field to optimize the coordinates
of control points of all B ´ezier shapes. Specifically, we use
control points in Sto densely sample a set of B ´ezier curve
points B={bi(t)}:
bi(t) =Bpi1pi2...pim(t), (4)
where bi(t)is a curve point of B ´ezier shape siconditioned
ontandBis the B ´ezier function. Then we compute the
predicted geometric field Gb:
Gb(q) = 1−tanh(min
i,t||q−bi(t)||2
2
σ2). (5)
We use the binary cross-entropy loss to compare the distri-
butions of GeandGc:
Lg=BCE (Gb(∗), Ge(∗)). (6)
In addition, we use the chamfer distance loss to encour-
age the B ´ezier shapes to fit the key edge points in the image:
Lc=X
b∈Bmin
e∈E∥b−e∥2
2+X
e∈Emin
b∈B∥e−b∥2
2.(7)
15879
$!%!!%!"%!#……$#%#!%#"%##………………$$%$!%$"%$#Geometric Space…Texture Space
…&$!&$#&$"&!!…&!#&!"…
""'#
""(#"#Shape-aware InterpolationControl PointsLatent Codes#Implicit RepresentationB́$zierShapes
Latent codes…
MLPTextureTexture Feature Interpolation(%,')Feature Fusion(,,.)(),*,+)#"'#Weighted Sum#"(#$%$CoordinateTexture%#=1/'Figure 2. Overview of our framework. We propose a novel texture-encapsulated shape-vectorized representation to decouple the raster
image in geometric and texture space. We utilize B ´ezier shapes defined as coordinates of control points to represent image geometry and
a shape-anchored implicit neural representation to explore image texture efficiently by distributing latent texture codes into control points.
With such representation, the texture feature at arbitrary coordinate can be spatially interpolated and fused within B ´ezier shapes and the
texture information can be retrieved through a very lightweight MLP.
!!!!!"
!#$Grid-based latent code distributionShape-anchored latent code distribution
Latent codesParams:!(ℎ$)Params:!(&)Latent codesLinear grid interpolationShape-aware interpolation
!!"!!!!!%!""!"%!"!
Figure 3. An example of the comparison between our shape-
anchored latent code distribution and general grid-based latent
code distribution. “ n” denotes shape number. Our strategy greatly
reduces storage overhead. Moreover, our shape-aware interpola-
tion restricts the interpolation process to the interior of shapes,
making the interpolation smoother and accelerating convergence
because the texture inside shapes tends to be relatively smooth.
With Eqn.(6&7), we can iteratively optimize the coordi-
nates of control points to constrain B ´ezier shapes to cover
the crucial areas with rich geometric information.
3.3. Shape-anchored Implicit Neural Texture Rep-
resentation
We use implicit neural representation in the continuous pa-
rameter domain to explore complex image texture. Follow-
ing recent progress in image reconstruction via implicit neu-
ral representation [6], we also distribute some latent codes
in the image spatial positions.
However, we abandon the usual strategy that assigns la-
tent codes to fixed and discrete grid points, which is very in-efficient and inflexible because it not only leads to storage
redundancy but also makes the training process extremely
complex (as shown in Fig. 3). For our problem, we no-
tice that there are some smooth regions in the image space
that can be fitted with only a small amount of latent codes,
while more latent codes are needed to be allocated for those
regions with sharp gradients/changes ( e.g., the edge points),
which significantly aligns with our utilization of geometric
representation in Sec. 3.2 in that texture-rich regions well
correspond to edge regions. Based on the above consid-
erations, we distribute the texture latent codes on the con-
trol points of the B ´ezier shapes in a distributed manner,
which significantly improves the utilization efficiency of la-
tent codes and reduces storage costs.
Specifically, in our texture representation, the image tex-
ture information is parameterized by a set of latent codes
stored in control points and a lightweight mapping function
(fθ: [x,z]7→c) which decodes the latent codes to texture
values (RGB), where xandzare the coordinate and the
corresponding latent vector/texture feature at query point in
the continuous image space. zis obtained via a shape-aware
interpolation method, i.e., interpolating the latent codes of
the shape that covers the query point. Assume a query point
q∈R2is within a shape si, the shape-aware interpolation
function ϕis formulated as:
ϕ(si,q) =mX
j=1wi
j(q)Pm
j=1wi
j(q)zij, (8)
with
wi
j(q) =1
∥q−pij∥2
2, (9)
where (pij,zij)∈si, as defined in Eqn. 2.
15880
For points in the image space, we adopt a divide-and-
conquer strategy to assign them with latent vectors. Specif-
ically, for points covered by B ´ezier shapes, we can obtain
their latents by interpolating the latent codes of the con-
trol points of the shapes they belong to by utilizing Eqn. 8.
For the points that are not covered by any B ´ezier shape,
we assign them a default latent vector. Note that a pixel
may be covered by several B ´ezier shapes, thus we propose
an inverse distance weighting feature fusion method (IDW-
Fusion) to fuse the latent vectors interpolated by all shapes.
We can formulate this process as:
1d,ifeSq=∅,(10) with:
wsi(q) =mPm
j=1∥q−pij∥2
2, (11)
whereeSq⊂Sdenotes the set of all shapes that cover point
q, which can be obtained with the Winding Number Algo-
rithm. The RGB value at point qcan be predicted as:
cq=fθ([q,zq]), (12)
where [·,·]means concatenation.
Compared to previous image vectorization methods that
use only one color within a B ´ezier shape, our shape-
anchored implicit neural texture representation models the
color distribution inside the B ´ezier shape efficiently. In ad-
dition, in our texture representation, the number of latent
codes is resolution-agnostic, which achieves a significant
compression of parameters and efficient texture representa-
tional capability with the content-adaptive latent code dis-
tribution.
3.4. Parameterized Representation Optimization
Given an input raster I∈RH×W×3, our task is to recon-
struct it with the above proposed parameterized representa-
tion in a zero-shot learning framework ( i.e., single-image
optimization). We have to optimize the parameters in a
self-supervised manner because we only have raster images
for optimization without any image-to-vector or image-to-
parameter labeling. Thus we have to rasterize our param-
eterized representation into a raster image ˆI∈RH×W×3,
with the same size as the input image and then measure the
pixel errors. Specifically, for each pixel [i, j]of the output
image ˆI, we query its color with our parameterized repre-
sentation by viewing it as a point in the image space with
coordinate xij= [i
H−0.5,j
W−0.5]as in [18]. Then we
can obtain the corresponding texture feature zxijand use
fθto approximate the RGB value:
ˆI[i, j] =fθ([xij,zxij]). (13)Then we can utilize the pixel-wise mean square error loss
to optimize the parameters, which is formulated as:
Lr=∥I−ˆI∥2
2. (14)
After optimization, the input image is parameterized into
decoupled geometric and texture space, facilitating high-
fidelity reconstruction with easy editing by simply editing
the shape and texture parameters in a decoupled way.
Optimization objectives. The input image is decomposed
into our parameterized representation in a zero-shot learn-
ing algorithm. As shown in Alg. 1, we firstly randomly
initialize all parameters of our representation including the
control points and corresponding latent codes of all shapes
(i.e.,S) and the MLP parameters ( i.e.,θ). Then we iter-
atively optimize all the parameters with a combination of
Lg,LcandLr, and the optimal problem of our framework
can be expressed as:
min
{S,θ}Lr+Lg+Lc. (15)
Algorithm 1: Zero-shot Representation Learning
Input : I,n,m,d,iters ;
Random Init: S,θ;// Parameters
Generate: E,Ge,Q;
// Edge points, Geometric field, All pixel points
foriin range(iters) do
B= sample curve points( S);
// pred geometric field
Gb= geometric field(B);
//eSfor all pixel points
eSQ=winding number parallel( S,Q);
// latent vector for all pixel points
zQ= latent interp(eSQ,Q);
// rasterize
ˆI=fθ(Q,zQ);
// compute loss
L=Lr(I,ˆI) +Lg(Gb, Ge) +Lc(B,E);
update parameters S,θ;
end
Output: Parameterized image representation: {S,θ}.
4. Experiments
4.1. Experimental Setups
Datasets. Current image vectorization algorithms are lim-
ited to handling images with simple geometric structures
like Emoji [1] and Icon [2], and there is a lack of explo-
ration for images with complex textures. In this paper, in
addition to comparing our method with existing methods
on commonly used Emojis and Icons, we also introduce a
Clipart Dataset consisting of 200 clipart images with com-
plex shapes, textures, and rich backgrounds, which is very
challenging for image vectorization task.
15881
n Dataset DiffVG LIVE NPA Ours
5Emoji 0.0212 0.0019 0.0049 0.0007
Icon 0.0573 0.0026 0.0093 0.0009
10Emoji 0.0092 0.0016 0.0020 0.0006
Icon 0.0285 0.0024 0.0017 0.0007
Table 1. Image vectorization results on Emoji&Icon datasets.
nmeans the number of shapes. MSE results are reported. Our
method achieves significantly better results than state-of-the-art.
Implementation Details. By default, we use four segments
for each closed B ´ezier shape, thus each shape contains 12
control points ( i.e.,m= 12 ). The number of shapes nis set
to5and128for Emoji/Icon dataset and Clipart dataset re-
spectively. Definitely, more shapes lead to better vectorized
results. The control factor σof the geometric field is set
to0.005. We set the dimension of latent codes as d= 16 ,
and the MLP fθis implemented as a linear layer with input
dimension 18and output dimension 3. With the help of a
highly parallel optimization process, we can reduce the time
it takes to optimize an image to the order of minutes.
4.2. Image Vectorization
We evaluate our texture-encapsulated shape representation
on image vectorization task by measuring the differences
between the input raster images and the rasterized vectors.
The comparison with SOTA methods ( i.e., DiffVG [15],
LIVE [17] and NPA [7]) are performed both quantitatively
and qualitatively on Emoji, Icon and Clipart dataset.
Emoji&Icon Datasets. The quantitative comparisons on
Emoji and Icon datasets are shown in Tab. 1. Our method
outperforms all previous image vectorization methods on
both benchmarks, especially when fewer B ´ezier shapes are
utilized. The results fully demonstrate the compactness
of our texture-encapsulated shape representation, especially
in the efficient utilization of shapes. Qualitative compar-
isons are shown in Fig 4. We can see that our representa-
tion achieves high-fidelity image vectorization results even
with a minimal number of shapes. Note that LIVE also
generates very compact representation, but it relies heavily
on the initialization strategy and still struggles to generate
regular shapes when dealing with shape intersections. We
also show some decoupled editing examples in Fig. 5. Our
method can easily transform geometric shapes and replace
the textures within the corresponding shapes with specific
ones like frosted glass texture.
Clipart Dataset. The Clipart dataset contains clipart im-
ages with complex texture information, which is very chal-
lenging for image vectorization task. We use this dataset
primarily to demonstrate the efficiency of our texture-
encapsulated shape representation in expressing complex
textures, especially when the number of shapes is limited.
NPA [7] is limited in its ability to simultaneously opti-
mize a large number of shape parameters only with gra-
RasterInput!=5!=10!=5!=10!=5!=10!=5!=10DiffVGLIVEOursNPAFigure 4. Qualitative comparison on Emoji&Icon datasets. n
means the number of shapes. Our method can accurately represent
the image geometry even with limited number of shapes.
fliprotate +translate translatescale-upGeometric Editing
Texture Editing
scale-uprotatenew textureadd shaperotatenew texture
Figure 5. We showcase the editability of our parameterized repre-
sentation. We can edit the image geometry and image texture in
a decoupled way simply by editing the corresponding parameters.
In the bottom row, we show the shape decompositions with the “x”
marks indicating the control points.
dients approximated by differentiable rasterization method,
which prevents it from producing acceptable results on this
dataset. Therefore, we only compare our method with
DiffVG [15] and LIVE [17]. The quantitative results are
shown in Tab. 2. Our method performs significantly better
than DiffVG and LIVE when utilizing the same number of
shapes. In addition, despite the layer-wise representation of
LIVE is highly compact, we are still able to achieve compa-
rable reconstruction results by further reducing the number
of shapes by half. The qualitative results are visualized in
Fig. 6. We can observe that our representation models the
texture details much better than other methods, and effec-
tively prevents local texture artifacts caused by the stacking
of redundant shapes.
4.3. Learning Implicit Image Representations
Considering that we utilize implicit neural representation
to parameterize images, we also make comparisons with
general implicit image representations. SIREN [29] is a
classical method to learn implicit representations for im-
ages parameterized by neural networks. In this section,
we compare our method with SIREN on the task of im-
age reconstruction with continuous representation. To com-
pare with methods that store latent codes on fixed grids,
we also utilize the framework of LIIF [6] to perform zero-
15882
!=128!=256!=128!=256!=128!=256OursLIVEDiffVGRasterInput
Figure 6. Qualitative comparison on Clipart dataset. nmeans the number of shapes. We use red boxes to emphasize the differences.
Our representation can express complex image details with a small number of shapes. Please zoom in for more details.
n Method MSE ↓LPIPS↓SSIM↑
128DiffVG [15] 0.0089 0.3741 0.7763
LIVE [17] 0.0090 0.3424 0.7916
Ours 0.0009 0.2492 0.8803
256DiffVG [15] 0.0035 0.3271 0.8092
LIVE [17] 0.0028 0.2894 0.8631
Ours 0.0006 0.2218 0.9014
Table 2. Image vectorization results on Clipart dataset. Pixel
MSE, LPIPS and SSIM are reported. nmeans the number of
shapes. The LPIPS is computed based on VGG [27]. Our method
achieves significantly better reconstruction results than state-of-
the-arts, especially when limited number of shapes are used.
shot image reconstruction. Specifically, instead of train-
ing a heavy encoder to generate latent codes, we directly
distribute randomly initialized latent codes on fixed grid
points and iteratively fit the implicit parameters (includ-
ing latent codes and an MLP-based decoding function) for
each image. We demonstrate the efficiency of our texture-
encapsulated shape representation by comparing the image
reconstruction errors and the corresponding number of pa-
rameters required for the implicit representation on the Cli-
part dataset. The results are shown in Tab. 3. We can see
that our method achieves better image reconstruction results
with a significantly higher parameter compression ratio than
other methods. When using the same order of magnitude ofparameters, our reconstruction results are obviously better
than other methods. Some visualization results are shown in
Fig. 7. For SIREN, more hidden layers lead to better recon-
struction quality, but it lacks the ability to reconstruct im-
ages with lightweight networks. For methods that store la-
tent codes on fixed grids, a large number of latent codes are
needed to achieve acceptable reconstruction results. In con-
trast, our representation captures rich image textures with
only a small number of latent codes and a very lightweight
MLP because we assign more resources for describing cru-
cial local visual regions by flexibly encapsulating texture
parameters in shape control points. Both quantitative and
qualitative results demonstrate that our method learns effi-
cient continuous representations of images.
4.4. Ablation Study
In this section, we explore the crucial designs and hyper-
parameters of our framework. For simplicity, we only use
MSE as the metric for all quantitative comparisons.
Component Analyses. We first investigate the effective-
ness of each training objective. The MSE results are shown
in Tab. 4. We see that the geometric field ( Lg) effectively
improves the reconstruction results among all datasets, and
the chamfer distance loss ( Lc) further improves reconstruc-
tion qualities in the complex Clipart dataset. The results
demonstrate that, compared to only using pixel loss, our
15883
Method MSE↓ LPIPS↓SSIM↑Codes↓Params. ↓
SIREN-1 0.0058 0.4424 0.7035 - 17152
SIREN-3 0.0020 0.2642 0.8325 - 49920
Grid-/4 0.0010 0.2602 0.8506 16384 262198
Grid-/16 0.0043 0.4896 0.6724 1024 16438
Ours 0.0009 0.2492 0.8803 1536 27702
Table 3. Comparison with general implicit image represen-
tations. “Codes” denote the number of latent codes utilized.
“Params” denote the number of parameters. “SIREN” does not
distribute latent codes. “SIREN- ∗” means the SIREN version with
∗hidden layers. In “Grid-/ ∗”, the “∗” denotes the proportion of the
spatial dimension of the original image to the grids storing latent
variables. Our representation achieves comparable reconstruction
results with a significant parameter compression. When using the
same order of magnitude of parameters, our method achieves ob-
viously higher reconstruction quality.
SIREN-1SIREN-3Grid-/4Grid-/16OursInput
Figure 7. Qualitative comparison with general implicit image
representations. Compared to SIREN and the usual strategy to
store latent codes on grids, our representation achieves clearer and
more faithful reconstruction results with fewer parameters.
Losses Emoji Icon Clipart
Lr 0.0013 0.0018 0.0043
Lr+Lg 0.0008 0.0011 0.0013
Lr+Lg+Lc0.0007 0.0009 0.0009
Table 4. Component Analyses on the training objectives. MSE
results on several datasets are reported.
method makes the shapes more accurately fit key geometric
regions by explicitly constraining the coordinates of control
points utilizing geometric information in the image.
A second experiment is conducted to explore how the
texture feature fusion method affects the image reconstruc-
tion results. More concretely, we compare our inverse dis-
tance weighting feature fusion method ( i.e., IDW-Fusion)
with the ablated version of directly adding the features inter-
polated by all shapes together ( i.e., Sum-Fusion). We also
investigate the effectiveness of attaching the coordinates of
query points to the features. The results are shown in Tab. 5.
We can observe that the IDW-Fusion method is very effec-Emoji Icon Clipart
IDW-Fusion 0.0007 0.0009 0.0009
Sum-Fusion 0.0013 0.0012 0.0016
w/o coords 0.0015 0.0020 0.0042
Table 5. Component Analyses on the feature fusion method.
MSE results on several datasets are reported.
MSE@ClipartShape Number !Latent Code Dimension "
Figure 8. Parameter Analyses on the shape number and latent
code dimension. MSE results on Clipart dataset are reported.
tive by considering the varying degrees of influence of each
shape on the query point. Notably, concatenating the fea-
tures with coordinates is effective and crucial in our method
because points in the background do not belong to any ex-
plicit shapes and the position information can serve as the
discriminative features for these points.
Parameter Analyses. We explore how shape number nand
latent code dimension daffect the reconstruction results.
The MSE results on Clipart dataset are shown in Fig. 8. We
can see that more shapes lead to better reconstruction results
and our method can achieve competitive results with only
64 shapes in the complex Clipart dataset. In addition, we
can see that higher dimensions only marginally improve the
reconstruction capability. Considering the computational
and storage costs it incurs, d= 16 is an efficient setting.
5. Conclusion
This work presents a novel vectorized image representation
to decompose images into parameterized shape and tex-
ture space. Along with our representation, we introduce
a straightforward zero-shot learning framework in a self-
supervised manner for image vectorization task. Extensive
experimental results on various benchmarks and tasks prove
that our representation achieves high-fidelity image recon-
struction with a significantly high image parameters com-
pression, endowed with convenient image editing by sim-
ply editing corresponding shape and texture parameters in a
decoupled way.
6. Acknowledgment
This work was supported by National Science Foundation
of China (U20B2072, 61976137). This work was also
partially supported by Grant YG2021ZD18 from Shang-
hai Jiaotong University Medical Engineering Cross Re-
search. This work was partially supported by STCSM
22DZ2229005.
15884
References
[1] Note emoji. https://github.com/googlefonts/
noto-emoji. Accessed: 2021-09-30. 2, 5
[2] creativestall. https : / / thenounproject . com /
creativestall/. 2, 5
[3] Matan Atzmon and Yaron Lipman. Sal: Sign agnos-
tic learning of shapes from raw data. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 2565–2574, 2020. 3
[4] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter
Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.
Mip-nerf: A multiscale representation for anti-aliasing neu-
ral radiance fields. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 5855–5864,
2021. 3
[5] Rohan Chabra, Jan E Lenssen, Eddy Ilg, Tanner Schmidt,
Julian Straub, Steven Lovegrove, and Richard Newcombe.
Deep local shapes: Learning local sdf priors for detailed 3d
reconstruction. In Computer Vision–ECCV 2020: 16th Eu-
ropean Conference, Glasgow, UK, August 23–28, 2020, Pro-
ceedings, Part XXIX 16 , pages 608–625. Springer, 2020. 3
[6] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning
continuous image representation with local implicit image
function. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 8628–8638,
2021. 3, 4, 6
[7] Ye Chen, Bingbing Ni, Xuanhong Chen, and Zhangli Hu.
Editable image geometric abstraction via neural primitive as-
sembly. In ICCV , pages 23514–23523, 2023. 2, 6
[8] James Richard Diebel. Bayesian Image Vectorization: the
probabilistic inversion of vector image rasterization . Stan-
ford University, 2008. 2
[9] Zheng-Jun Du, Liang-Fu Kang, Jianchao Tan, Yotam Gin-
gold, and Kun Xu. Image vectorization and editing via linear
gradient layer decomposition. ACM Transactions on Graph-
ics (TOG) , 42(4):1–13, 2023. 2
[10] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong
Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:
Radiance fields without neural networks. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5501–5510, 2022. 3
[11] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and
Yaron Lipman. Implicit geometric regularization for learning
shapes. arXiv preprint arXiv:2002.10099 , 2020. 3
[12] Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei
Huang, Matthias Nießner, Thomas Funkhouser, et al. Local
implicit grid representations for 3d scenes. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 6001–6010, 2020. 3
[13] Verica Lazova, Vladimir Guzov, Kyle Olszewski, Sergey
Tulyakov, and Gerard Pons-Moll. Control-nerf: Editable
feature volumes for scene rendering and manipulation. In
Proceedings of the IEEE/CVF Winter Conference on Appli-
cations of Computer Vision , pages 4340–4350, 2023. 3
[14] Jaewon Lee and Kyong Hwan Jin. Local texture estima-
tor for implicit representation function. In Proceedings ofthe IEEE/CVF conference on computer vision and pattern
recognition , pages 1929–1938, 2022. 3
[15] Tzu-Mao Li, Michal Luk ´aˇc, Micha ¨el Gharbi, and Jonathan
Ragan-Kelley. Differentiable vector graphics rasterization
for editing and learning. TOG , 39(6):1–15, 2020. 2, 3, 6, 7
[16] Jinxian Liu, Ye Chen, Bingbing Ni, Jiyao Mao, and Zhenbo
Yu. Inferring fluid dynamics via inverse rendering. arXiv
preprint arXiv:2304.04446 , 2023. 3
[17] Xu Ma, Yuqian Zhou, Xingqian Xu, Bin Sun, Valerii Filev,
Nikita Orlov, Yun Fu, and Humphrey Shi. Towards layer-
wise image vectorization. In CVPR , pages 16314–16323,
2022. 2, 3, 6, 7
[18] Xu Ma, Yuqian Zhou, Huan Wang, Can Qin, Bin Sun, Chang
Liu, and Yun Fu. Image as set of points. arXiv preprint
arXiv:2303.01494 , 2023. 5
[19] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,
Jonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-
worth. Nerf in the wild: Neural radiance fields for uncon-
strained photo collections. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 7210–7219, 2021. 3
[20] Ishit Mehta, Micha ¨el Gharbi, Connelly Barnes, Eli Shecht-
man, Ravi Ramamoorthi, and Manmohan Chandraker. Mod-
ulated periodic activations for generalizable local functional
representations. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 14214–14223,
2021. 3
[21] Mateusz Michalkiewicz, Jhony K Pontes, Dominic Jack,
Mahsa Baktashmotlagh, and Anders Eriksson. Implicit sur-
face representations as layers in neural networks. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 4743–4752, 2019. 3
[22] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. Communications of the ACM , 65(1):99–106, 2021. 2,
3
[23] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a mul-
tiresolution hash encoding. ACM Transactions on Graphics
(ToG) , 41(4):1–15, 2022. 2, 3
[24] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and
Francesc Moreno-Noguer. D-nerf: Neural radiance fields
for dynamic scenes. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
10318–10327, 2021. 3
[25] Antoine Quint. Scalable vector graphics. IEEE MultiMedia ,
10(3):99–102, 2003. 2
[26] Pradyumna Reddy, Michael Gharbi, Michal Lukac, and
Niloy J Mitra. Im2vec: Synthesizing vector graphics without
vector supervision. In CVPR , pages 7342–7351, 2021. 2
[27] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 , 2014. 7
[28] Vincent Sitzmann, Michael Zollh ¨ofer, and Gordon Wet-
zstein. Scene representation networks: Continuous 3d-
structure-aware neural scene representations. Advances in
Neural Information Processing Systems , 32, 2019. 3
15885
[29] Vincent Sitzmann, Julien Martel, Alexander Bergman, David
Lindell, and Gordon Wetzstein. Implicit neural representa-
tions with periodic activation functions. Advances in neural
information processing systems , 33:7462–7473, 2020. 2, 3,
6
[30] Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi
Schmidt, Pratul P Srinivasan, Jonathan T Barron, and Ren
Ng. Learned initializations for optimizing coordinate-based
neural representations. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 2846–2855, 2021.
[31] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
Deep image prior. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 9446–9454,
2018. 3
[32] Peng Wang, Yuan Liu, Zhaoxi Chen, Lingjie Liu, Ziwei Liu,
Taku Komura, Christian Theobalt, and Wenping Wang. F2-
nerf: Fast neural radiance field training with free camera
trajectories. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 4150–
4159, 2023. 3
[33] Tian Xia, Binbin Liao, and Yizhou Yu. Patch-based image
vectorization with automatic curvilinear feature alignment.
ACM Transactions on Graphics (TOG) , 28(5):1–10, 2009. 2
15886
