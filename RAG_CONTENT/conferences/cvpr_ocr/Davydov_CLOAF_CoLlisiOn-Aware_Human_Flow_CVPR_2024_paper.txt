CLOAF: CoLlisiOn-Aware Human Flow
Andrey Davydov*Martin Engilberge Mathieu Salzmann Pascal Fua
CVLab, EPFL
fname g.fsurname g@epfl.ch
Abstract
Even the best current algorithms for estimating body
3D shape and pose yield results that include body self-
intersections. In this paper, we present CLOAF, which
exploits the diffeomorphic nature of Ordinary Differential
Equations to eliminate such self-intersections while still im-
posing body shape constraints. We show that, unlike earlier
approaches to addressing this issue, ours completely elim-
inates the self-intersections without compromising the ac-
curacy of the reconstructions. Being differentiable, CLOAF
can be used to ﬁne-tune pose and shape estimation base-
lines to improve their overall performance and eliminate
self-intersections in their predictions. Furthermore, we
demonstrate how our CLOAF strategy can be applied to
practically any motion ﬁeld induced by the user. CLOAF
also makes it possible to edit motion to interact with the
environment without worrying about potential collision or
loss of body-shape prior.
1. Introduction
Feed-forward approaches to estimating human body 3D
shape and pose from a single image have become remark-
ably effective [6, 25, 35]. The very recent transformer-based
architecture of [10] embodies the current state-of-the-art.
It is pre-trained on 300 million images and ﬁne-tuned on
most SMPL data sets in existence. However, as good as
these methods have become, they can still produce unreal-
istic poses with substantial self-intersections of body parts,
as illustrated by Fig. 1. This is a serious issue if video-based
motion capture is to be used in ﬁelds, such as robotics or re-
alistic animation, where preventing self-intersections is of
utmost importance.
Most current approaches to addressing this issue [4, 13,
24, 28] are iterative. They penalize self-intersections explic-
itly by minimizing an interpenetration loss. This requires
explicitly detecting self-intersections and then performing
*This work was supported in part by the Swiss National Science Foun-
dation
Collision Rate (%)HMR2.0
Optim
CLOAFMethods39.2%
9.2%
0.0%Figure 1. Self-intersections in SOTA methods. Top rows.
HMR2.0 [10] ( ﬁrst row ) and PARE [19] ( second row ), two of the
best current methods, produce bodies shown in blue with self-
intersections. CLOAF removes them and generates the results
shown in gold. Bottom row. HMR 2.0 [10] recovers bodies with
self-intersections in 39.2% of frames of the 3DPW-test set. A
recent post-processing method such as [28] brings this down to
9.2%. CLOAF drops this number all the way to zero.
a separate optimization step, which makes the whole pro-
cess non-differentiable and precludes its use during train-
ing. Another approach is to eliminate self-intersections in
the training databases [26]. While all these methods help,
they do not guarantee the absence of self-collisions at infer-
ence time.
In this paper, we propose a different approach. It pre-
vents self-intersections in a differentiable manner and with-
out an explicit detection step. To this end, we rely on the
fact that if the scene ﬂow from one body to another is the
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
1176
solution of an Ordinary Differential Equation (ODE), then
there cannot be any self-intersections. Thus, given a volu-
metric representation of body shapes, we formulate an ODE
that models their deformation over time and show how it can
be solved with respect to the parameters of the body model
we use to represent humans. This means that we can pre-
vent self-intersections while imposing the proper geometric
priors on our reconstructions. In other words, our method
is able to map any motion ﬂow, even those that may seem
implausible, to a ﬂow without any self-intersections. None
of the existing methods can achieve this.
In its simplest form, our CoLlisiOn-Aware Flow
(CLOAF) method can be used to interpolate between two
non self-intersecting body representations so that the inter-
mediate body shapes are both realistic and self-intersection
free. It can also be used in a more sophisticated manner to
remove self-intersections from the output of single-frame
pose estimators, such as [10], while remaining as close as
possible to the original poses. Fig. 1 illustrates this. Be-
cause it is differentiable, CLOAF can also be integrated into
the training pipeline of a deep network to improve its per-
formance. Additionally, we demonstrate how our CLOAF
integration procedure can utilize practically any customized
motion ﬁeld to move towards a target area and model inter-
actions with surrounding objects.
In short, our contribution is to use the diffeomorphic na-
ture of ODEs to build a ﬂow-based pipeline to compute
human body trajectories without self-intersections. This
is a very generic method and, because it is differentiable,
it can be used in conjunction with any body pose estima-
tion scheme. The code will be made available at https:
//github.com/cvlab-epfl/CLOAF .
2. Related Work
Feed-Forward Pose and Shape Estimation. While re-
cent approaches to pose and shape estimation from images
have become spectacularly good [6, 10, 19, 25, 26, 35],
they still do not guarantee that the resulting body models
are self-penetration free. The latest transformer-based ar-
chitecture of [10] underwent pre-training using 300 million
images and further reﬁnement on the majority of existing
SMPL datasets, making it the current state-of-the-art. Nev-
ertheless, self-intersections can easily be found in its output,
as shown in Fig. 1.
In part, this is because most current approaches favor the
accuracy of the body reprojection in the image, potentially
at the expense of plausibility, which includes preventing
self-intersection. We are not the ﬁrst to notice this prob-
lem and attempts have been made to ﬁx it. For example,
the approach of [26] aims to make the self-contacts natural.
To this end, it generates pseudo ground-truth data that fea-
tures them. A feed-forward model trained on such data han-
dles self-contacts better than previous methods. However, itSOTA method @0 @100 @100 (w/o col.) P-MPJPE
HMR2.0 [10] 39.2 21.6 12.3 54.3
PARE [19] 36.1 24.3 11.0 50.9
TUCH [26] 23.7 11.7 8.6 55.5
EFT [16] 17.8 6.5 4.5 58.1
SPIN [20] 15.5 5.5 2.8 59.2
Table 1. Collisions in SOTA methods. We report the Col.Rate
across samples of 3DPW-test set with at least one collision (@0),
at least 100 collisions (@ 100), and at least 100 collisions among
samples of 3DPW-test that do not have collisions.
still produces a signiﬁcant number of self-intersections, as
shown in Table 1.
Collision-Aware Optimization. Since preventing feed-
forward methods from producing self-intersections is hard,
an alternative is to post-process the results to eliminate
them. For example, in SMPLify [4], limbs are modeled
as ellipsoids and inter-penetrations are explicitly penalized
with the corresponding loss. The more recent SMPLify-
X [28] uses Bounding V olume Hierarchies (BVHs) for
fast collision detection and introduces local conic 3D dis-
tance ﬁelds to penalize the penetration [3, 31]. The ﬁrst
method is simple to use but modeling body parts as el-
lipsoids is an oversimpliﬁcation that can yield unrealistic
results. The second avoids this problem but the computa-
tion of the BVHs is costly. SMPLify-DC [26] extends SM-
PLify by modeling self-contacts more precisely. Further-
more, PROX [13] introduces inter-penetration constraints
to prevent collisions between bodies and surrounding ob-
jects. COAP [24] suggests using independent body part-
aware volumetric occupancy networks. The self-collision
then can be seen as the intersection between the neighbor-
ing occupancy volumes.
In any event, none of these schemes are differentiable
with respect to the input pose estimate. Hence, they can-
not be incorporated in an end-to-end trainable pipeline.
Additionally, they do not guarantee removal of all self-
intersections because they rely on minimizing a loss. By
contrast, our ﬂow-based approach never produces self-
intersections, is fully differentiable, and can be used during
training.
Motion Field Integration. Central to our work is the
idea of integrating an ODE-based ﬁeld to prevent self-
intersections.
This concept has been explored since well before the
deep-learning era, especially for shape transfer purposes. In
[33], shape deformations are modeled as local path line in-
tegrations. This enables volume-preserving transformations
between shapes while avoiding self-intersections for practi-
cally any given input transformation. To preserve diffeo-
morphisms, NMF [12] employs a series of learnable ODE
1177
integrations to morph a spherical mesh into various shapes,
conditioned by a point cloud. It is shown that the gen-
erated meshed objects retain feasible physical properties.
MeshODE [14] and ShapeFlow [15] directly learn the vol-
umetric ﬁeld between pairs of meshed objects, then trans-
ferring one shape to another without collisions. ODE inte-
gration is also discussed in human motion modeling. Oc-
cFLow [27] models the temporal deformation sequence for
a single human subject using a ﬂow ﬁeld.
However, none of these methods exploits any data-driven
prior model. Hence, intermediate shapes are not guaranteed
to be realistic. In essence, we extend these concepts to the
problem of transitioning between human body shapes while
maintaining a valid parametric representation that preserves
realism at every step of the integration.
Inverse Kinematics. One of our main contributions is the
coupling of ODEs with a parametric model. The recovery of
the underlying pose from spatial points resembles an inverse
kinematics (IK) problem. HybriK [21] incorporates an iter-
ative IK module into the image-to-mesh recovery network
to better align the 3D keypoints with a parametric body rep-
resentation. Even though IK is solvable with iterative tech-
niques, it is usually highly restricted by the spatial rig and
the structure of the kinematic chains [1]. Instead of per-
forming exact but sometimes overly rigid IK, [7] proposes
to explicitly optimize the tangent vector using ﬁrst-order ap-
proximations of the input ﬁeld. In CLOAF, we project the
input motion onto the most plausible velocity of the para-
metric state. Since we work with velocities instead of dis-
placements, our inverse projection is exact and does not re-
quire iterating. In simple terms, we take the best of both
worlds: integrating the motion ﬂow without penetrations,
while preserving a parametric representation of the body at
every moment.
3. Method
When predicting body shape and pose in terms of a parame-
terized body model such as SMPL [22], the simplest way to
discourage self-intersections is to introduce loss functions
that make them costly [4, 28]. This can be effective but
suffers the same fate as all soft constraints: they can still
be violated. In this work, we exploit diffeomorphism, a
key property of Ordinary Differential Equations (ODEs), to
truly prevent self-intersections.
Motion Flow as an ODE. Let us consider a volume con-
taining a body deforming from a ﬁrst position B0to a sec-
ond oneB1between times t0andt1. Each point in that
volume follows a speciﬁc trajectory. Let us assume the ex-
istence of a function f!such that we can write for everypointxin the volume:
dx
dt=f!(x;tjB0;B1)8t;t0tt1; (1)
x(t0) =x0;
where x0the initial position of x, while the parameters !
control the behavior of f. Then, according to the Picard-
Lindel ¨of theorem [8], the trajectories of two initially dis-
tinct points can never intersect. This requires the right-
hand side of the ODE, the velocity ﬁeld f, to be Lipschitz
continuous with respect to x, which neural networks sat-
isfy [29, 32].
Given the formulation of Eq. 1, a start position B0, and
an end position B1, computing trajectories for points in the
volume is a classic Cauchy problem that can be solved efﬁ-
ciently using numerical solvers. This means that if we start
from a valid position B0, it will remain valid throughout
t0<t<t 1. In the remainder of this section and the exper-
iment section, we show that the function f!exists and can
be learned from data.
Introducing a Body Model. The function f!of Eq. 1
could be implemented by a neural network with weights
!. Then, given Npairs of start and end positions
f(Bi
0;Bi
1);1iNg, the weights could be learned
so that the ﬁnal body positions obtained by solving the
ODE starting from Bi
0are as close as possible to Bi
1. This
amounts to minimizing
X
ikBi
0!1 Bi
1k2; (2)
with respect to !, whereBi
0!1is the body position in the
coordinate space estimated at time t1, starting from B0att0.
Hereafter, we omit the explicit dependency of the results of
the integration on !for notational simplicity.
However, without any body shape prior, the intermedi-
ate body positions would be completely unrealistic [15, 27].
Thus, we propose to incorporate a body model, speciﬁcally
the SMPL model [22], in this formalism. To this end, we
reformulate Eq. 1 in terms of the parameters of the body
model, rather than the points xas follows.
When using the SMPL model, each 3D point xon the
body surface is parameterized by the underlying SMPL vec-
tor2Rd. For each one, we can write
x(t) =x((t));
)dx
dt=dx
dd
dt=Jd
dt; (3)
where J2R3dis the Jacobian of the SMPL transforma-
tionx(), computed given the body state (t). Injecting
this into the time derivative of Eq. 1 yields
Jd
dt=f!(x;tj0;1): (4)
1178
<latexit sha1_base64="0vQW3TPIa3lfsVAgQMq7S+gomSk=">AAAB9HicbZDLSgNBEEVr4ivGV9Slm8YguJAwE2PUjQbcuIyQFyTD0NPpJE16HnbXBELId7hxoYhbP8adf+EnOJlEUOOFhsO9VXRx3VAKjab5YaSWlldW19LrmY3Nre2d7O5eXQeRYrzGAhmopks1l8LnNRQoeTNUnHqu5A13cDPNG0OutAj8Ko5Cbnu054uuYBRjy25X+xypY50QdCwnmzPzZiKyCNYcctefkKjiZN/bnYBFHveRSap1yzJDtMdUoWCSTzLtSPOQsgHt8VaMPvW4tsfJ0RNyFDsd0g1U/HwkiftzY0w9rUeeG096FPv6bzY1/8taEXYv7LHwwwi5z2YfdSNJMCDTBkhHKM5QjmKgTIn4VsL6VFGGcU+ZpITiaeHsskQW4buEeiFvlfLFu2KufDVrA9JwAIdwDBacQxluoQI1YHAPD/AEz8bQeDRejNfZaMqY7+zDLxlvXyOhkrY=</latexit>⇥1,t1
<latexit sha1_base64="etZfg/6nQTchOp9ETPYJdX96mvM=">AAAB9HicbZDLSgNBEEVr4ivGV9Slm8YguJAwiTHqRgNuXEbIC5Jh6On0JE16HnbXBELId7hxoYhbP8adf+EnOJlEUOOFhsO9VXRxnVAKjab5YaSWlldW19LrmY3Nre2d7O5eQweRYrzOAhmolkM1l8LndRQoeStUnHqO5E1ncDPNm0OutAj8Go5Cbnm05wtXMIqxZXVqfY7UNk8I2qadzZl5MxFZhMIcctefkKhqZ9873YBFHveRSap1u2CGaI2pQsEkn2Q6keYhZQPa4+0YfepxbY2ToyfkKHa6xA1U/HwkiftzY0w9rUeeE096FPv6bzY1/8vaEboX1lj4YYTcZ7OP3EgSDMi0AdIVijOUoxgoUyK+lbA+VZRh3FMmKaF0Wjy7LJNF+C6hUcwXyvnSXSlXuZq1AWk4gEM4hgKcQwVuoQp1YHAPD/AEz8bQeDRejNfZaMqY7+zDLxlvXyCUkrQ=</latexit>⇥0,t0
<latexit sha1_base64="yOeT5kQuHJLf22a36BztoZkDGF4=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKezGGPWiAS8eEzAPSJYwO5lNxszOLjOzQljyBV48KOLVT/LmX/gJTjYR1FjQUFR1093lRZwpbdsfVmZpeWV1Lbue29jc2t7J7+41VRhLQhsk5KFse1hRzgRtaKY5bUeS4sDjtOWNrqd+655KxUJxq8cRdQM8EMxnBGsj1XUvX7CLdgq0SJw5KVx9QopaL//e7YckDqjQhGOlOo4daTfBUjPC6STXjRWNMBnhAe0YKnBAlZukh07QkVH6yA+lKaFRqv6cSHCg1DjwTGeA9VD99abif14n1v65mzARxZoKMlvkxxzpEE2/Rn0mKdF8bAgmkplbERliiYk22eTSEMonpdOLClok3yE0S0WnUizXy4Xq5SwNyMIBHMIxOHAGVbiBGjSAAIUHeIJn6856tF6s11lrxprP7MMvWG9foKeOgA==</latexit>t
<latexit sha1_base64="wy4YQgfG1/pnf1htXzWoyM+e/1k=">AAACAnicbVC7SgNBFL3rM8ZX1NJmMAhWYRNj1EYDNpYRzAOyS5idzCZDZh/MzIph2c5vsNXaTmz9EUv/wE9wdhPBGA9cOJxzL/dwnJAzqUzzw1hYXFpeWc2t5dc3Nre2Czu7LRlEgtAmCXggOg6WlDOfNhVTnHZCQbHncNp2Rlep376jQrLAv1XjkNoeHvjMZQQrLVmWh9XQceP7pMd6haJZMjOgeVKekuLlF2Ro9AqfVj8gkUd9RTiWsls2Q2XHWChGOE3yViRpiMkID2hXUx97VNpxljlBh1rpIzcQenyFMvX3RYw9KceeozfTjPKvl4r/ed1IuWd2zPwwUtQnk0duxJEKUFoA6jNBieJjTTARTGdFZIgFJkrXNPPF8ZKsk+px5eS8hubJTyetSqlcK1VvqsX6xaQcyME+HMARlOEU6nANDWgCgRAe4QmejQfjxXg13iarC8b0Zg9mYLx/AzuGmhY=</latexit>xi
<latexit sha1_base64="mvsbiNT8mEkRMDneQ1qvJanBVLc=">AAACMHicbVDLTsJAFL31ifhCXbqZSExckKZVRFyoJG5cYhQloYRMhylMmD4yMzWSpr/iR/gNbnWtK8PWlZ9gKZCgeJJJTs45N/fOsQPOpDKMD21ufmFxaTmzkl1dW9/YzG1t30k/FITWiM99UbexpJx5tKaY4rQeCIpdm9N7u3c59O8fqJDM925VP6BNF3c85jCCVSK1cmXLxaprO1E9RmfIslnHiibSY9wyC0jX9QKakm7SUNzK5Q3dSIFmiTkm+YtvSFFt5QZW2yehSz1FOJayYRqBakZYKEY4jbNWKGmASQ93aCOhHnapbEbpD2O0nyht5PgieZ5CqTo9EWFXyr5rJ8nhpfKvNxT/8xqhcsrNiHlBqKhHRouckCPlo2FdqM0EJYr3E4KJYMmtiHSxwEQlpf7aYrujTopHh8enJTRLJp3cHepmSS9eF/OV81E5kIFd2IMDMOEEKnAFVagBgSd4gVd40561d+1TG4yic9p4Zgd+Qfv6ATXVq0A=</latexit>X= x1,. . . ,xS 
<latexit sha1_base64="zxdjX52MY9ZmNbMncrQFnvsgpnc=">AAACDHicbVDLSsNAFL2pr1pf8bFzM1iEuilprVU3WnDjskJf0IQymU7aoZMHMxOxhv6C3+BW1+7Erf/g0j/wE0zTCtZ64MLhnHu5h2MHnEllGB9aamFxaXklvZpZW9/Y3NK3dxrSDwWhdeJzX7RsLClnHq0rpjhtBYJi1+a0aQ+uxn7zlgrJfK+mhgG1XNzzmMMIVrHU0fdMF6u+7UR3o5xZ61OFO8ZRR88aeSMBmieFKclefkGCakf/NLs+CV3qKcKxlO2CESgrwkIxwukoY4aSBpgMcI+2Y+phl0orStKP0GGsdJHji3g8hRL190WEXSmHrh1vjrPKv95Y/M9rh8o5syLmBaGiHpk8ckKOlI/GVaAuE5QoPowJJoLFWRHpY4GJigub+WK7o6ST0nHx5LyM5slPJ41ivlDOl25K2crFpBxIwz4cQA4KcAoVuIYq1IHAPTzCEzxrD9qL9qq9TVZT2vRmF2agvX8D27+dAQ==</latexit>x(⇥0)
<latexit sha1_base64="vFHWhVPk0TUi9SmnyziaUAYxg2s=">AAACDHicbVDLSsNAFL2pr1pf8bFzM1iEuilJrVU3WnDjskJf0JYymU7aoZMHMxOxhvyC3+BW1+7Erf/g0j/wE0zTCtZ64MLhnHu5h2P5nEllGB9aamFxaXklvZpZW9/Y3NK3d+rSCwShNeJxTzQtLClnLq0ppjht+oJix+K0YQ2vxn7jlgrJPLeqRj7tOLjvMpsRrGKpq++1HawGlh3eRbl2dUAV7ppHXT1r5I0EaJ6YU5K9/IIEla7+2e55JHCoqwjHUrZMw1edEAvFCKdRph1I6mMyxH3aiqmLHSo7YZI+Qoex0kO2J+JxFUrU3xchdqQcOVa8Oc4q/3pj8T+vFSj7rBMy1w8UdcnkkR1wpDw0rgL1mKBE8VFMMBEszorIAAtMVFzYzBfLiZJOiseFk/MSmic/ndQLebOUL94Us+WLSTmQhn04gByYcApluIYK1IDAPTzCEzxrD9qL9qq9TVZT2vRmF2agvX8D3VOdAg==</latexit>x(⇥1)<latexit sha1_base64="GU1L9y6uaNZ6sKpgJlc8RZie+AA=">AAAB/3icbVDLTgJBEOzFF+IL9ehlIjHBC1kQUS9K4sUjJrwMEDI7DDBhZncz02tCCAe/wauevRmvfopH/8BPcFkwEbGSTipV3enucnwpDNr2hxVbWl5ZXYuvJzY2t7Z3krt7VeMFmvEK86Sn6w41XAqXV1Cg5HVfc6ocyWvO4Hri1+65NsJzyzj0eUvRniu6glEMpbtmuc+RpvG4nUzZGTsCWSTZGUldfUGEUjv52ex4LFDcRSapMY2s7WNrRDUKJvk40QwM9ykb0B5vhNSlipvWKDp4TI5CpUO6ng7LRRKpvydGVBkzVE7YqSj2zV9vIv7nNQLsnrdGwvUD5C6bLuoGkqBHJt+TjtCcoRyGhDItwlsJ61NNGYYZzW1x1DjKJH+SO70okEXyk0k1l8kWMvnbfKp4OQ0H4nAAh5CGLJxBEW6gBBVgoOARnuDZerBerFfrbdoas2Yz+zAH6/0bszyYCQ==</latexit>⇥(t)
<latexit sha1_base64="tZQ4dNRpM6jZF6Sgx1+ahksJ46U=">AAACOnicbVBNT9tAEB0DLTT9wC1HLqtGlaiEIgdCSg+lKL1wBKkJkeIoWq/XYZVd2+yOkSLj/9MfwW/gWnpsbxUXDvyArh2qNsCTVvv03oxm5gWpFAY974ezsLj05OnyyrPa8xcvX626r9/0TJJpxrsskYnuB9RwKWLeRYGS91PNqQokPw4mX0r/+IxrI5L4K05TPlR0HItIMIpWGrkdP9KU5aGvKJ4EUd4vijzEgnwifqDyqPA7Yrzxz9zEc/80o+Emqb7SfT9y617Dq0AekuYdqX++gQqHI/enHyYsUzxGJqkxg6aX4jCnGgWTvKj5meEpZRM65gNLY6q4GebVrQV5Z5WQRIm2L0ZSqf935FQZM1WBrSy3Nve9UnzMG2QY7Q5zEacZ8pjNBkWZJJiQMjgSCs0ZyqkllGlhdyXshNrw0MY7NyVQRZVJa3tr52ObPCR/M+ltNZrtRuuoVd/fm4UDK7AOb2EDmvAB9uEADqELDL7BJXyHK+fC+eX8dq5npQvOXc8azMG5/QNDerBm</latexit>dXdt=f⇣X,t|,⌘
<latexit sha1_base64="juxRLDlJrWVuJw1s0LZRNJ1blRk=">AAACKHicbVDLTsJAFL3FF+ILdOlmIjFxYUiLiLpREjcuMZFHAoRMp1OYMNM2M1MiafgJt/oFfo07w9Yv8BMsBRMRTnKTk3Pvzb3n2AFnSpvmxEitrW9sbqW3Mzu7e/sH2dxhXfmhJLRGfO7Lpo0V5cyjNc00p81AUixsThv24H7abwypVMz3nvQooB2Bex5zGcE6lpptW0TuuMu62bxZMBOgZWLNSf7uGxJUuzkj03Z8EgrqacKxUi3LDHQnwlIzwuk40w4VDTAZ4B5txdTDgqpOlDw8Rqex4iDXl3F5GiXq340IC6VGwo4nBdZ99b83FVf1WqF2rzsR84JQU4/MDrkhR9pHU/fIYZISzUcxwUSy+FdE+lhiouOMFq7YYrWHc2fIAjW38zzzk6RXuihe3pTRMvlNr14sWOVC6bGUr9zOYoQ0HMMJnIEFV1CBB6hCDQhweIFXeDPejQ/j05jMRlPGfOcIFmB8/QDjAKjD</latexit>fi
<latexit sha1_base64="5R/NVfn3nZr+88SG49fq23EM+0Y=">AAACYnicbVDLSiNBFK20jo+eGU10qYtiwoCLIXQ0vhZqwI24UjAaSGdCdfXtpLD6QdVtMTT9O36NWwX3foCfYKXTAWf0QsGpc+7l3nO8RAqNjvNSsebmvy0sLi3b33/8XFmt1taudZwqDh0ey1h1PaZBigg6KFBCN1HAQk/CjXd7OtFv7kBpEUdXOE6gH7JhJALBGRpqUG27gWI8892rESDLMx9zekTdkOHIC7Lz/G/m+mw4BJXPGkupmxfN9qBadxpOUfQzaJagfvJGiroY1Cq268c8DSFCLpnWvaaTYD9jCgWXkNtuqiFh/JYNoWdgxELQ/aywmtPfhvFpECvzIqQF+3EiY6HW49AznZND9f/ahPxK66UYHPQzESUpQsSni4JUUozpJDfqCwUc5dgAxpUwt1I+YiYSNOn+s8ULv/bwx78TiS7t3E/9FOm1drZ3D/foZzBL73q70dxrtC5b9fbxNEayRDbIL7JFmmSftMkZuSAdwskDeSRP5LnyatlWzVqftlqVcqb8z8rafAeoMb1C</latexit>d⇥dt=J†dXdt<latexit sha1_base64="qsM01CW6MrAH1yliBRhp9WhV4uE=">AAACX3icbVDLSiNBFL3pcUYn42icWYmbYoKgOISOxscsfIAblw4YFdIhVFffNoXVD6pui6Hpr/Fr3OrK5fyBn2ClOwEdPVBwOOdebp3jp0oact2nmvNp5vOX2bmv9W/z3xcWG0s/zk2SaYFdkahEX/rcoJIxdkmSwstUI498hRf+9fHYv7hBbWQSn9EoxX7Er2IZSsHJSoPGvnc2ROJrtOEFqIgzWmf7bCqusw3mhZqLPKikIg+oyKejxaDRdFtuCfaetCekefgMJU4HS7W6FyQiizAmobgxvbabUj/nmqRQWNS9zGDKxTW/wp6lMY/Q9PMyZ8FWrRKwMNH2xcRK9fVGziNjRpFvJyNOQ/O/NxY/8noZhXv9XMZpRhiL6lCYKUYJG5fGAqlRkBpZwoWW9q9MDLnthWy1b6740ccZfgc3MjWTOLdVnrK9ztbm9p8d9p5M2zvfbLV3Wp2/nebRQVUjzMEK/II1aMMuHMEJnEIXBNzBPTzAY+2fM+ssOI1q1KlNdn7CGzjLL/QeuVw=</latexit>⇥(t+ t)=⇥(t)+d⇥dt t[Sampling][FieldVelocities][Parametric  Correction][ODE Solver]<latexit sha1_base64="/E2qee7BZYHFbBptoEBWtDUlS7w=">AAACK3icbVDLSkJRFN23p9lLa9jkkAQGIfeaWU1KaNLQwBeoyLnHox4898E5+0oifkbT+oK+plHRtHmf0PVqkOmCDYu192bvtWxfCo2m+W6srK6tb2zGtuLbO7t7+4nkQUV7gWK8zDzpqZpNNZfC5WUUKHnNV5w6tuRVu3836VcHXGnhuSUc+rzp0K4rOoJRDKV6o9TjSNPYsk5biZSZMSOQRWLNSOr2GyIUW0kj3mh7LHC4i0xSreuW6WNzRBUKJvk43gg09ynr0y6vh9SlDtfNUfTzmJyESpt0PBWWiyRS/26MqKP10LHDSYdiT//vTcRlvXqAnavmSLh+gNxl00OdQBL0yCQA0haKM5TDkFCmRPgrYT2qKMMwprkrtrPcw1l7IHw9s/M49ROllzvPXlznySL5Ta+SzVj5TO4hlyrcTGOEGBzBMaTBgksowD0UoQwMPHiCZ3gxXo0348P4nI6uGLOdQ5iD8fUD8MOpNw==</latexit>⇥(t1)
<latexit sha1_base64="cLDfeUfCR5t/IzA/RlDwOwNcUxo=">AAACOXicbVBNSwJRFL3Td/altWjR5pEECiEzZlabCtq0LFATVOTN800+fPPBe3ckGfw1besX9EtatotWQX+gcTTI8sCFw7n3cu85diCFRtN8NebmFxaXlldWU2vrG5tb6cx2TfuhYrzKfOmruk01l8LjVRQoeT1QnLq25Hd272rUv+tzpYXvVXAQ8JZL7z3hCEYxltrp3aZLsWs70cMw16x0OdIctq18vp3OmgUzAflPrAnJXnxCgpt2xkg1Oz4LXe4hk1TrhmUG2IqoQsEkH6aaoeYBZT16zxsx9ajLdStKHAzJQax0iOOruDwkifp7I6Ku1gPXjidH/+q/vZE4q9cI0TltRcILQuQeGx9yQknQJ6M4SEcozlAOYkKZEvGvhHWpogzj0Kau2O5sD4edvgj0xM7D2E+SXumoeHxWJv/JT3q1YsEqF0q3pezl+ThGWIE92IccWHACl3ANN1AFBkN4hCd4Nl6MN+Pd+BiPzhmTnR2YgvH1DWs/rm8=</latexit>x(⇥(t1))CLOAF integration:Starting pose(without collisions)
Target pose (with collisions)[1 step of integration]Final pose (without collisions)
<latexit sha1_base64="HyPpTCG+vE0uItFAAxeAjGHm/vM=">AAACNXicbVDLSgNBEOz1GeMrmqOXwSAoSthojHrRgBePERIVkhBmZztmyOyDmd5gCPkWr/oFfosHb+JVP8HNJoKvgoaiupvuKidU0pBtP1tT0zOzc/OphfTi0vLKamZt/coEkRZYE4EK9I3DDSrpY40kKbwJNXLPUXjtdM9H/eseaiMDv0r9EJsev/VlWwpOsdTKZBvVDhLfpt2Gi4o4ox3WyuTsvJ2A/SWFCcmdfUCCSmvNSjfcQEQe+iQUN6ZesENqDrgmKRQO043IYMhFl99iPaY+99A0B8n3Q7YVKy5rBzoun1iift8YcM+YvufEkx6njvndG4n/9eoRtY+bA+mHEaEvxofakWIUsFEUzJUaBal+TLjQMv6ViQ7XXFAc2I8rjve/hz23J0MzsXM39pOkVzzYPzwpsb/kK72r/XyhlC9eFnPl03GMkIIN2IRtKMARlOECKlADAX24hwd4tJ6sF+vVehuPTlmTnSz8gPX+CXe0rG0=</latexit>⇥(t+ t)InputOutput
<latexit sha1_base64="JmRVBsPkDhKD9d5ANdsIwnhxUJc=">AAACtHicbVFbb9MwFHYyLqPcuvGIhCwqRCuqKhmlwAMwwQviaUjrNqkJke04qbXYieyTaVWWP8G/41/wyCNO0lWM7Ui2vvOd8+ncaJEJA573y3G3bt2+c3f7Xu/+g4ePHvd3do9MXmrG5yzPcn1CieGZUHwOAjJ+UmhOJM34MT390sSPz7g2IleHsCp4KEmqRCIYAUtF/Z/B4ZIDGULkj/AHvPG8EX6FA6EgqqxT/7C/X/cCSWBJk+qbJYKYpCnXdfBZpBdRdakc2Swqq6QOqEiHG8V5vdhkhGO42DjjDkU+7jWKUQxRf+BNvNbwdeCvweDTb9TaQbTj9II4Z6XkClhGjFn4XgFhRTQIlnHbUGl4QdgpSfnCQkUkN2HVLq/GLywT4yTX9inALfuvoiLSmJWkNrOZxfwfa8ibYosSkndhJVRRAlesK5SUGYYcN5fAsdCcQbaygDAtbK+YLYkmDOy9rlSh8uYZxvGZKMx6nPNunnZ709d7b97P8HVwub2jvYk/m0y/Twf7H7s1om30FD1HQ+Sjt2gffUUHaI4Y+uM8c146Q3fmBi5zeZfqOmvNE3TFXPUX3x3Whw==</latexit>⇥(t1)=⇥(t0)+Zt1t0J†   ⇥(t)f x[⇥(t)],t|⇥(t),⇥1 dt
Figure 2. Method overview. CLOAF integrates from an initial body pose without self-intersections towards a target one that may feature
some. Every integration step involves sampling points from the body surface, calculating approximate spatial velocities, correcting these
velocities in the parametric space, and performing an integration step using the ODE solver. When the integration is complete, the body
shape is without self-intersections and its pose is taken to be the corrected pose and the output of our method.
Since the number of points Sfrom the SMPL mesh can be
taken to be much larger than the dimension of , writing
Eq. 4 for each point produces an over-constrained system
of linear equations. It can be solved in the least-squares
sense, which yields
d
dt=Jyf!(X;tj0;1); (5)
where X2R3Sis the vector formed by concatenating
the coordinates of all Spoints and Jy= (JTJ) 1JTis
the Moore-Penrose pseudo-inverse of J2R3Sd. This
computation is akin to solving an inverse kinematic prob-
lem [2, 7, 11].
We can now reformulate the ODE of Eq. 1 in terms of the
parameters of the SMPL model using Eq. 3. It becomes
d
dt=Jyf!(X();tj0;1); (6)
(t0) =  0;
where 0parameterizes the initial body pose, 1the ﬁ-
nal one, and f!approximatesdX
dt. Solving Eq. 6 yields an
evolution in the SMPL parameter space instead of the coor-
dinate space of Eq. 1. Thus, it enforces the learned shape
prior.
To train the network, we learn its weights !by adapting
the training scheme introduced above as follows. Given N
pairs of start and end poses f(i
0;i
1);1iNg, theweights are learned so that the ﬁnal body positions corre-
sponding to the parameters obtained by solving the ODE
starting from the initial i
0are as close as possible to the
body positions corresponding to the target parameters i
1.
Hence, we reformulate Eq, 2 in terms of , which induces
body points xusing Eq. 3. This amounts to minimizing
X
ikx(i
0!1) x(i
1)k2; (7)
with respect to !, where i
0!1is the body position in the
parametric space estimated at time t1, starting from 0at
t0.
Points Sampling. Solving Eq. 6 requires multiple com-
putations of the Jacobian, which means performing both
the forward and backward pass of the SMPL transforma-
tion. On modern GPUs, this can be efﬁciently parallelized.
In practice, we found that S= 1000 points is sufﬁcient to
cover the full body shape and compute meaningful Jaco-
bians. We ablate the sampling number Sin Sec. 4.5.
In [26], it was pointed out that some regions of the body,
such as the crotch or the armpits, are more prone to natu-
ral self-intersections. Furthermore, it was shown that the
SMPL blending parameters do not adequately compensate
for that, being insufﬁciently precise [4, 28]. Hence, we ex-
clude these areas from the sampling procedure.
Network Architecture and Training. The input vector
to the motion ﬁeld network f!consists of the points xand
1179
the trajectory description ft;t0;t1;0;1g. Speciﬁc input
strategies are discussed in the following paragraph. All in-
put values are extended with Fourier features [30] to bet-
ter accommodate to slight variations in the input signal, the
number of frequencies is nf= 20 . Regarding the model,
we use a variant of the ShapeFlow network architecture [15]
to implement f!. It relies on a 6-layer MLP, where the out-
put of each layer (except the last) is concatenated with an
input vector. Note that such architecture is Lipschitz contin-
uous, as required by the theory, because all the constituent
layers are Lipschitz.
To solve ODEs, we use NeuralODE [5] with the adaptive
Dormand-Prince solver. For all computations in this paper,
we use one Tensor Core GPU NVidia A100.
As our training data, we use the AMASS dataset [23] that
contains pose sequences stored in the SMPL format. During
training, we sample pairs of poses and integrate between
them. In our experiments, we found that using the absolute
timetas input does not provide any sufﬁcient information to
the model, preventing robust convergence. Instead, we use
the “time left” variable, t=t1 t; this is crucial to give
the model a sense of speed, and it signiﬁcantly improves
convergence.
At each step, the ﬁeld f!is computed at Spoints xfrom
the body surface, using Eq. 3. Following [36], points are
sampled uniformly from the mesh surface. Finally, the mo-
tion ﬁeld network can be written as
f!=f!(xj(t);1;t); (8)
where all points xare stacked together and concatenated
with the current pose (t), the target pose 1and the time
gaptthat are the same for a given body.
In our experiments, we found that the ﬂow model trained
from scratch is prone to produce unrealistic velocity values.
To avoid this, we scale the output predictions by the ex-
pected speed averaged across all the points, we see that it
substantially stabilizes the training and converges faster to
plausible ﬁelds.
Solving Eq. 6 is a two-step operation: ﬁeld estimation
followed by parametric re-projection, with the latter tak-
ing time. We found that replacing (t)by its approxi-
mation computed via linear interpolation ~(t) =  0+
t t0
t1 t0(1 0)signiﬁcantly stabilizes the training, makes
it much faster, and does not bring any detrimental effect at
inference time. This means that the model implicitly learns
a linear interpolation in the parametric space through inte-
gration in the coordinate space, where non self-intersection
is preserved.
When integrating the ﬁeld for one pair of poses we in-
duce the loss only for the ending point of the trajectory,
1, as shown in Eq. 7. We found that the model is prone
to get stuck and not move towards the target, especially in
the ﬁrst stages of training. To address this, we extend theaforementioned approximation and compute the following
“trajectory” loss for every ith pair of poses:
Li
traj=1
MMX
m=0k0!tm x(~(tm))k2; (9)
wheretm=t0+m
M(t1 t0),Mis the number of steps
in the integration, and 0!tmis the body position in the
parametric space estimated at time tm, starting from 0at
t0. Such loss signiﬁcantly helps the model to stay on the
trajectory and not diverge from the target.
4. Experiments
4.1. Datasets and Metrics
Datasets. To train the motion ﬁeld network, we utilize the
AMASS dataset [23], which contains more than 40 hours of
motion sequences in the common SMPL body representa-
tion. To evaluate the baseline methods and our approach,
we use the 3DPW [34] dataset. It contains 60 video se-
quences of various activities in the wild. All samples have a
ground-truth in the SMPL format as well. We use the com-
mon train/test split, the training part is used for unsuper-
vised ﬁne-tuning (Sec. 4.3), while the test set is a common
benchmark and is used in all our experiments for evaluation.
The COCO EFT[16] pseudo ground-truth dataset of in-the-
wild images is used for supervised part of the EFT baseline
training in Sec. 4.3.
Metrics. To evaluate the 3D pose, we use the standard 3D
Mean-Per-Joint Position Error (MPJPE, mm) and its Pro-
crustes Aligned version (P-MPJPE, mm). To assess the
smoothness of the motion, we compute the acceleration er-
ror (Accel.Err, mm=s2) between the predicted and ground-
truth 3D keypoints. This metric has been used in previous
works [6, 17, 18] to quantify the trade-off between 3D pose
accuracy and motion consistency. Since our main goal is
to eliminate self-intersections, we also report the collision
rate (Col.Rate@ C, %) computed on the 3DPW-test [34] set.
It reﬂects the proportion of samples across the dataset that
have more than Cvertices inside the body.
4.2. Eliminating Self-Intersections
As shown in Table 1, self-collisions are prevalent in the out-
put of some of the best current techniques. CLOAF can be
used to remove them.
Letbe the body shape estimate produced by a neural
network, which can contain self-intersections such as those
depicted by Fig. 1. To eliminate them in any given frame of
a video sequence, we start from a body shape estimate 0
that does not contain any and solve the ODE of Eq. 6 with
the target shape 1being . Body shapes along the result-
ing trajectory are guaranteed to be self-intersection-free and
we take the ﬁnal one to be our reﬁned estimate. Note that
1180
method MPJPE P-MPJPE Accel.Err Col.Rate@0
HMR2.0 [10] 82.0 52.7 16.1 39.2%
Opt.Cones [28] 81.3 52.4 14.3 9.2%
Opt.Cones+[28] 82.5 53.3 16.4 9.1%
Opt.Contact [26] 81.7 53.1 14.9 8.6%
COAP [24] 81.9 52.8 14.5 5.7%
CLOAF 82.3 53.2 9.4 0.0%
Table 2. Comparison against collision penalizing techniques.
Our method yields smoother motion ﬂow compared to previous
approaches and guarantees collision-free predictions. In terms of
pose estimation metrics, it is on par with existing methods. The
“+” symbol denotes that the method uses previous frames for ini-
tialization, as we do. We use 3DPW-test [34] for evaluation.
method P-MPJPE Col.Rate
EFT [16] (baseline) 58.1 6.5
+CLOAF (post-proc.) 57.9 0.0
Opt.Cones [28] 57.5 6.1
CLOAF ( no diff ) 57.8 6.3
CLOAF ( diff) 55.4 3.4
Table 3. Using the differentiability of CLOAF to ﬁne-tune a
network. This is less efﬁcient to remove self-intersections than
using CLOAF to post-process but better than the comparable base-
lines. Fine-tuning with differentiable CLOAF signiﬁcantly im-
proves the accuracy of the model. We use the 3DPW-test set [34]
for evaluation.
for this to work properly 0must be self-intersection-free.
As we work with complete video sequences, when process-
ing a frame, we take the corrected pose in the previous one
to be 0. In the case when we do not have access to se-
quences, we can use different strategies that we discuss in
Section 4.5.
In Table 2, we compare our results against those of the
baselines. For fairness, we test the baseline of [28] ini-
tialized using both, the original approach (the body esti-
mate from the current frame) and ours (the body estimated
from the neighboring frame). While the optimization-based
methods and their soft constraints cannot completely elimi-
nate the self-intersections, ours does while at the same time
providing poses that are temporally more consistent. This is
achieved at the cost of a very slight drop in reconstruction
accuracy, which is not clearly signiﬁcant because there are
some self-intersections in the so-called ground-truth data.
Fixing such errors, while actually correct, makes our results
appear to be further from the ground-truth than some incor-
rect ones.
4.3. Self-Intersection-Aware Fine-Tuning
CLOAF is differentiable with respect to the corrupted input
poses. Therefore, it can be added at the end of any pose and
shape estimation network to ﬁne-tune it in an end-to-end
manner to reduce its self-intersection rates.
We demonstrate this using the EFT pose and shape
estimation model [16] parameterized by 
. Its predic-tions ^x
can have self-intersections. We can remove
them by computing CLOAF (^x
). The differentiability of
CLOAF allows us to introduce the loss Lcloaf =k^x
 
CLOAF (^x
)k, whose computation does not require any
new annotations. We then use the 3DPW training set [34]
to reﬁne the network weights by minimizing a composite
loss that is the sum of Lcloaf and the usual supervised loss.
In Table 3, we compare our approach to two baselines.
The ﬁrst is designed for comparison against a traditional
optimization method [28]. In this scenario, the network is
ﬁne-tuned using the loss L=k^x
 ~xk, where ~xis the self-
intersection-free result of the optimization method. Since it
is not differentiable1, the gradient of the collision correcting
operation cannot be used, which hurts performance. The
second baseline involves CLOAF but with the gradient de-
tached when computing L, meaning that the right term of
the loss is only used in the computation of the loss, but no
gradient from the ODE solver is used.
All three ﬁne-tuning strategies help reduce the self-
intersection rate, but our approach (CLOAF ( diff)) signiﬁ-
cantly outperforms the optimization-based method in both
self-intersection rate and P-MPJPE. Our ablated method
with the detached gradient (CLOAF ( no diff )) is compara-
ble to the optimization-based method. This conﬁrms that
the beneﬁt of our approach lies in its differentiability.
4.4. Simpliﬁed Motion Fields
So far, we have trained the motion ﬁeld network f!of Eq. 8
to produce realistic motion ﬁelds and have constrained the
trajectories to go from a start pose towards a ﬁnal one. How-
ever, there are scenarios in which someone might wish to
use simpler ﬁelds described by rough displacements that are
deﬁned only locally. A simple example is a piecewise ﬁeld
that “moves the left arm up”. It can be deﬁned by a vec-
torf(x)6= 0 around the left arm and zero elsewhere, as
illustrated by the gold ovals in Fig. 3. Such a ﬁeld does not
satisfy the Picard-Linde ¨of theorem, which we relied in the
derivation of Eqs. 1 and 6. Nevertheless, it can be approx-
imated by one that does. Speciﬁcally, as in [33], we blend
the inner and outer regions in a Lipschitz-continuous way.
To this end, we use a B ´ezier blending function
b(r) =4X
p=0pB4
pr rin
rout rin
; (10)
whereB4
pare the Bernstein polynomials [9], rinandroutare
the thresholds for the inner and outer regions, respectively,
andpare the blending coefﬁcients, 0=1=2= 0
and3=4= 1. Then, the blended ﬁeld is split into three
1It should be noted that here we refer to the approach of [28], which is
based on iterative optimization. Its ﬁnal output is not differentiable with
respect to the initial pose, while CLOAF’s estimate is.
1181
TargetSource
TargetSourceFigure 3. Moving towards a target. CLOAF can be used to integrate practically any ﬁeld, even those that are induced locally. Here the
motion is set by the source region on the body (gold oval around the left arm) and the target point (blue dot). The ﬁeld comprises the
direction towards the target point (blue arrows) and adapts during the integration. In the top row, the source region is smaller than in the
bottow row, which affects the behavior of the ﬁeld and induces different motion.
regions as follows:
fb=8
><
>:f(x) r(x)<r in
f(x)(1 b) +0b r inr(x)rout
0 r(x)>r out(11)
This blended ﬁeld can be handled by CLOAF. We follow
the same procedure as in Sec. 3, but instead of using the
neural network to estimate the motion ﬁeld, we use fb.
We provide two examples in Fig. 3. In both cases, we
start with the same body posture, and the goal is to move the
left arm towards the blue dot, but we change the size of the
non-zero component of the ﬁeld (gold ovals). The smaller
the region, the less realistic the motion is, since fewer body
parameters are affected.
Another potential application of such customized inte-
gration is the interaction with objects. The subject must
be able to move in the ﬁeld, while not going through ob-
jects. This can be achieved by modeling the space inside
the object as a region of zero ﬁeld. When blending during
integration, the moving points get stuck in the non-moving
area, preventing further penetration.
We illustrate this in the example in Fig. 4. The target
point for both hands is located in front of the face (red dot).
As in the previous experiment, the ﬁeld exists only for se-
lected areas of the arms (gold ovals). During integration, the
local ﬁelds of the hands are blended with the zero ﬁeld of
the box, and the hands stop at the surface of the box (green
body on the right). If no constraints are imposed on the ﬁeld
(gold body in the middle), both hands successfully reach the
target point.
Figure 4. Interacting with the objects. One can build a blended
ﬁeld (Eq. 11), where non-zero component (deﬁned at arms, gold
ovals) induces the motion, while zero component impedes it. Start-
ing from the same posture ( blue body on the left ), the motion to-
wards the target ( red dot in front of the face ) is computed without
any constraints ( gold body in the middle ) and with zeroing out the
ﬁeld inside the red box ( green body on the right ). The blending
with the area empty from the ﬁeld successfully prevents penetra-
tions inside the area. Without constraints, both hands reach the
target point.
4.5. Ablation Study
Picking the Initial Body Posture. Recall from Sec-
tion 4.2 that, when solving our ODE, we took the initial
body position to be the corrected one in the previous frame.
We refer to this strategy as Successive Frames . This works
well for video sequences but there are cases where this
would be impractical, for example when dealing with single
1182
method MPJPE P-MPJPE Accel.Err
HMR2.0 [10] 82.0 52.7 16.1
Jitter 81.6 52.6 15.9
Key Poses 82.9 53.6 16.4
Successive Frames 82.3 53.2 9.4
Table 4. Picking the initial body posture. All variations of
CLOAF produce 0.0% collision rate. The Jitter strategy improves
position error, while Successive Frames variant better preserves
the overall motion smoothness. Experiments are performed on
3DPW-test set [34].
images. We have therefore explored two alternatives.
•Jitter. Having localized the self-intersections, we can de-
termine what part of the body and group of limbs are re-
sponsible for them. A straightforward solution is then to
randomly jitter these parameters until a body without self-
intersection is obtained. This method is simple yet proves
to be highly effective in most cases.
•Keyposes. We ﬁrst precompute a dictionary of poses
with no self-intersections. To this end, we subsample the
AMASS [23] pose dataset and cluster the poses using K-
means with the keypoint distance as metric. We then take
the closest neighbors to the Kcluster centers as keyposes.
Finally, given a body we wish to correct, we take the clos-
est keypose to be our starting point. We found K= 128
to be sufﬁcient to ﬁnd a suitable keypose shape in most
cases.
In Table 4, we compare these different ways to initial-
ize the CLOAF process on the same video sequences of
3DPW-test set [34]. Successive Frames denotes the ap-
proach of Section 4.2, which is clearly best at preserving
a smooth natural motion. However, the Jitter technique
yields slightly more accurate pose estimates, even surpass-
ing those of the baselines as reported in Table 2. The Key-
poses approach performs worse than the others because it
requires integration from the relatively distant body shapes.
Yet, there may be some rare cases when it is the only usable,
for example, when there is no neighboring frame without
self-intersections and jitter in the parametric space does not
yield a valid body shape.
Choosing Optimal Sampling. As discussed above, one
step of CLOAF includes the Jacobian computation with the
following solving the over-constrained system of equations.
It brings in a natural tradeoff between time and accuracy,
since the more points we sample, the more time is spent on
the Jacobian computation, but the more accurate the result
is. We explore the effect of the number of points Sused to
sample from the body shape following Eq. 5.
We show the results in Fig. 5. The running time is the
time spent only on the Jacobian computation, as all other
steps are negligible. To estimate the accuracy of the CLOAF
step, we develop a simple yet effective metric. We sam-
ple an SMPL vector 0and add a random noise of the
10 100 1000 6890
S050100150200250300Running time, msRunning Time
Relative Error
0.00.10.20.30.40.50.60.7
Rel.ErrorFigure 5. Ablation on Points Sampling. The Jacobian computa-
tion injects a tradeoff between the accuracy and the running time.
Using too many points Sfor the evaluation slows down the cal-
culations, while less do not provide enough accuracy. We choose
S= 1000 as the best tradeoff.
known amount (we use kk= 10 2) to obtain the sec-
ond posture 1=  0+. For a pair of bodies and the
sampling number S, one can compute the distance between
the bodies in the coordinate space f2R3S. As the linear
approximation of the SMPL transformation holds for such
a small, solving ^ =Jyfmust give a solution that is
close to the “ground-truth” . A more elaborate reasoning
behind the linear assumption we make is discussed in the
supplementary material. We deﬁne the relative error as
RE() =k^ k
kk: (12)
We chooseS= 1000 as a good tradeoff between the accu-
racy and the time and use it in all our experiments.
5. Conclusion
We have presented an approach to reliably eliminating self-
intersection from body shape and pose estimation by solv-
ing an ODE while imposing a body shape prior. Unlike
methods that rely on minimizing a loss function, ours guar-
antees the complete disappearance of allself-intersections.
Furthermore, it is differentiable, which means that it can
be integrated into a deep learning training pipeline. We
have shown how to exploit the differentiability of CLOAF
to ﬁne-tune networks and improve their performance while
decreasing the amount of self-intersections they produce.
Last but not least, we have demonstrated how our CLOAF
strategy can be applied to practically any customized mo-
tion ﬁeld, for example, enabling body interaction with the
environment.
In future work, we will extend CLOAF to motion gener-
ation and exploit existing motion priors.
1183
References
[1] A. Aristidou, J. Lasenby, Y . Chrysanthou, and A. Shamir. In-
verse kinematics techniques in computer graphics: A survey.
InComputer Graphics Forum . Wiley Online Library, 2018.
3
[2] P. Baerlocher and R. Boulic. An Inverse Kinematics Archi-
tecture for Enforcing an Arbitrary Number of Strict Priority
Levels. The Visual Computer , 2004. 4
[3] L. Ballan, A. Taneja, J. Gall, L. Van Gool, and M. Polle-
feys. Motion Capture of Hands in Action Using Discrimi-
native Salient Points. In European Conference on Computer
Vision , pages 640–653, 2012. 2
[4] F. Bogo, A. Kanazawa, C. Lassner, P. Gehler, J. Romero, and
M. J. Black. Keep It SMPL: Automatic Estimation of 3D
Human Pose and Shape from a Single Image. In European
Conference on Computer Vision , 2016. 1, 2, 3, 4
[5] R. Chen, Y . Rubanova, J. Bettencourt, and D. Duvenaud.
Neural Ordinary Differential Equations. In Advances in Neu-
ral Information Processing Systems , 2018. 5
[6] H. Choi, G. Moon, J.Y . Chang, and K.M. Lee. Beyond
Static Features for Temporally Consistent 3D Human Pose
and Shape from a Video. In Conference on Computer Vision
and Pattern Recognition , 2021. 1, 2, 5
[7] L. Ciccone, C. ¨Oztireli, and R.W. Sumner. Tangent-space
optimization for interactive animation control. ACM Trans-
actions on Graphics , 2019. 3, 4
[8] E.A. Coddington and N. Levinson. Theory of Ordinary Dif-
ferential Equations . Tata McGrawHill Education, 1955. 3
[9] G. Farin. Curves and Surfaces for Computer Added Geomet-
ric Design . Academic Press, 1992. 6
[10] S. Goel, G. Pavlakos, J. Rajasegaran, A. Kanazawa, and J.
Malik. Humans in 4D: Reconstructing and Tracking Hu-
mans with Transformers. In International Conference on
Computer Vision , 2023. 1, 2, 6, 8
[11] K. Grochow, S.L. Martin, A. Hertzmann, and Z. Popovic.
Style-Based Inverse Kinematics. In ACM SIGGRAPH , pages
522–531, 2004. 4
[12] K. Gupta and M. Chandraker. Neural Mesh Flow: 3D Mani-
fold Mesh Generation via Diffeomorphic Flows. In Advances
in Neural Information Processing Systems , 2020. 2
[13] M. Hassan, V . Choutas, D. Tzionas, and M. J. Black. Resolv-
ing 3D Human Pose Ambiguities with 3D Scene Constraints.
InInternational Conference on Computer Vision , 2019. 1, 2
[14] J. Huang, C.M. Jiang, B. Leng, B. Wang, and L. Guibas.
MeshODE: A Robust and Scalable Framework for Mesh De-
formation. arXiv Preprint , 2020. 3
[15] C. Jiang, J. Huang, A. Tagliasacchi, and L.J. Guibas. Shape-
ﬂow: Learnable deformation ﬂows among 3d shapes. In Ad-
vances in Neural Information Processing Systems , 2020. 3,
5
[16] H. Joo, N. Neverova, and A. Vedaldi. Exemplar Fine-Tuning
for 3D Human Pose Fitting Towards In-the-Wild 3D Human
Pose Estimation. In International Conference on 3D Vision ,
2020. 2, 5, 6
[17] A. Kanazawa, M. J. Black, D. W. Jacobs, and J. Malik. End-
To-End Recovery of Human Shape and Pose. In Conference
on Computer Vision and Pattern Recognition , 2018. 5[18] M. Kocabas, N. Athanasiou, and M. J. Black. VIBE: Video
Inference for Human Body Pose and Shape Estimation. In
Conference on Computer Vision and Pattern Recognition ,
pages 5252–5262, 2020. 5
[19] M. Kocabas, C.-H.P. Huang, O. Hilliges, and M.J. Black.
PARE: Part attention regressor for 3D human body estima-
tion. In International Conference on Computer Vision , 2021.
1, 2
[20] N. Kolotouros, G. Pavlakos, M. J. Black, and K. Daniilidis.
Learning to Reconstruct 3D Human Pose and Shape via
Model-Fitting in the Loop. In Conference on Computer Vi-
sion and Pattern Recognition , 2019. 2
[21] J. Li, C. Xu, Z. Chen, S. Bian, L. Yang, and C. Lu. Hy-
brik: A hybrid analytical-neural inverse kinematics solution
for 3d human pose and shape estimation. In Conference on
Computer Vision and Pattern Recognition , 2021. 3
[22] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M.J.
Black. SMPL: A Skinned Multi-Person Linear Model. ACM
SIGGRAPH Asia , 34(6), 2015. 3
[23] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and
M. J. Black. AMASS: Archive of Motion Capture as Surface
Shapes. In International Conference on Computer Vision ,
pages 5442–5451, 2019. 5, 8
[24] M. Mihajlovic, S. Saito, A. Bansal, M. Zollhoefer, and S.
Tang. COAP: Compositional articulated occupancy of peo-
ple. In Conference on Computer Vision and Pattern Recog-
nition , 2022. 1, 2, 6
[25] G. Moon and K.M. Lee. I2L-MeshNet: Image-to-Lixel Pre-
diction Network for Accurate 3D Human Pose and Mesh Es-
timation from a Single RGB Image. In European Conference
on Computer Vision , 2020. 1, 2
[26] L. M ¨uller, A. Osman, S. Tang, C.-H. P. Huang, and M.J.
Black. On self-contact and human pose. In Conference on
Computer Vision and Pattern Recognition , 2021. 1, 2, 4, 6
[27] M. Niemeyer, L. Mescheder, M. Oechsle, and A. Geiger. Oc-
cupancy Flow: 4D Reconstruction by Learning Particle Dy-
namics. In International Conference on Computer Vision ,
2019. 3
[28] G. Pavlakos, V . Choutas, N. Ghorbani, T. Bolkart, A. Osman,
D. Tzionas, and M.J. Black. Expressive Body Capture: 3D
Hands, Face, and Body from a Single Image. In Conference
on Computer Vision and Pattern Recognition , 2019. 1, 2, 3,
4, 6
[29] L. Smith, J. van Amersfoort, H. Huang, S. Roberts, and Y .
Gal. Can Convolutional Resnets Approximately Preserve In-
put Distances? a Frequency Analysis Perspective. In arXiv
Preprint , 2021. 3
[30] M. Tancik, P. Srinivasan, B. Mildenhall, S. Fridovich-Keil,
N. Raghavan, U. Singhal, R. Ramamoorthi, J. Barron, and
R. Ng. Fourier features let networks learn high frequency
functions in low dimensional domains. Advances in Neural
Information Processing Systems , 2020. 5
[31] D. Tzionas, L. Ballan, S. A., P. Aponte, M. Pollefeys, and
J. Gall. Capturing Hands in Action Using Discriminative
Salient Points and Physics Simulation. International Journal
of Computer Vision , 118(2):172–193, 2016. 2
1184
[32] A. Virmaux and K. Scaman. Lipschitz Regularity of Deep
Neural Networks: Analysis and Efﬁcient Estimation. In Ad-
vances in Neural Information Processing Systems , 2018. 3
[33] W. von Funck, H. Theisel, and H.-P. Seidel. Vector Field
based Shape Deformations. ACM Transactions on Graphics ,
2006. 2, 6
[34] T. von Marcard, R. Henschel, M. Black, B. Rosenhahn, and
G. Pons-Moll. Recovering Accurate 3D Human Pose in the
Wild Using IMUs and a Moving Camera. In European Con-
ference on Computer Vision , 2018. 5, 6, 8
[35] W.-L. Wei, J.-C. Lin, T.-L. Liu, and H.-Y .M. Liao. Capturing
Humans in Motion: Temporal-Attentive 3D Human Pose and
Shape Estimation from Monocular Video. In Conference on
Computer Vision and Pattern Recognition , 2022. 1, 2
[36] Y . Zhang, M.J. Black, and S. Tang. We are more than our
joints: Predicting how 3d bodies move. In Conference on
Computer Vision and Pattern Recognition , 2021. 5
1185
