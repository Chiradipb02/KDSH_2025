Differentiable Display Photometric Stereo
Seokjun Choi†Seungwoo Yoon†Giljoo Nam*Seungyong Lee†Seung-Hwan Baek†
†POSTECH*Meta Reality Labs
(c)
(b)(a)
(d)
Figure 1. We propose differentiable display photometric stereo, a method that facilitates (a) the learning of display patterns, enabling
high-quality reconstruction of surface normals using (b) a monitor and a camera. (c) Capturing a scene with the learned patterns allows for
estimating (d) high-quality surface normals.
Abstract
Photometric stereo leverages variations in illumination
conditions to reconstruct surface normals. Display photo-
metric stereo, which employs a conventional monitor as an
illumination source, has the potential to overcome limita-
tions often encountered in bulky and difficult-to-use conven-
tional setups. In this paper, we present differentiable dis-
play photometric stereo (DDPS), addressing an often over-
looked challenge in display photometric stereo: the design
of display patterns. Departing from using heuristic display
patterns, DDPS learns the display patterns that yield accu-
rate normal reconstruction for a target system in an end-
to-end manner. To this end, we propose a differentiable
framework that couples basis-illumination image formation
with analytic photometric-stereo reconstruction. The dif-
ferentiable framework facilitates the effective learning of
display patterns via auto-differentiation. Also, for training
supervision, we propose to use 3D printing for creating a
real-world training dataset, enabling accurate reconstruc-
tion on the target real-world setup. Finally, we exploit that
conventional LCD monitors emit polarized light, which al-
lows for the optical separation of diffuse and specular re-
flections when combined with a polarization camera, lead-
ing to accurate normal reconstruction. Extensive evalua-
tion of DDPS shows improved normal-reconstruction accu-
racy compared to heuristic patterns and demonstrates com-
pelling properties such as robustness to pattern initializa-
tion, calibration errors, and simplifications in image for-
mation and reconstruction.1. Introduction
Reconstructing high-quality surface normals is pivotal in
computer vision and graphics for 3D reconstruction [32,
40], relighting [36, 39], and inverse rendering [45, 52].
Among various techniques, photometric stereo [50] lever-
ages the intensity variation of a scene point under varied
illumination conditions to reconstruct normals. Photomet-
ric stereo finds its application in various imaging systems
including light stages [29, 35,49,56], handheld-flash cam-
eras [3, 10,37,52], and display-camera systems [1, 28,46].
Display photometric stereo uses monitors and cameras as
a versatile and accessible system that can be conveniently
placed on a desk [1, 28,46]. Producing diverse illumina-
tion conditions can be simply achieved by displaying mul-
tiple patterns using pixels on the display as programmable
point light sources. This convenient and intricate modu-
lation of illumination conditions significantly enlarges the
design space of illumination patterns for display photomet-
ric stereo. Nevertheless, existing approaches often rely on
heuristic display patterns, resulting in sub-optimal recon-
struction quality.
In this paper, to exploit the large design space of illumi-
nation patterns in display photometric stereo, we propose
differentiable display photometric stereo (DDPS). The key
idea is to learn display patterns that lead to improved recon-
struction of surface normals for a target system in an end-
to-end manner. To this end, we introduce a differentiable
framework that combines basis-illumination image forma-
tion and an optimization-based photometric stereo method.
This enables effective pattern learning by directly optimiz-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
11831
ing the display patterns via auto-differentiation. To com-
pute the normal-reconstruction loss for backpropagation,
we propose the use of 3D printing for creating a real-
world training dataset with known geometry. Combined
with the basis-illumination image formation, using the 3D-
printed dataset allows for efficient and realistic simulation
of relit images during end-to-end optimization. In addi-
tion, we leverage that conventional LCD monitors emit po-
larized light. Thus, using a polarization camera, we can
optically remove specular reflection that often deteriorates
photometric-stereo reconstruction.
Extensive evaluation of DDPS on diverse objects shows
that using the learned patterns significantly improves nor-
mal accuracy compared to using heuristic patterns. More-
over, DDPS exhibits robustness to pattern initialization, cal-
ibration error, and simplifications in image formation and
reconstruction, promising its practical applicability. We
will release code and data upon acceptance.
In summary, our contributions are as follows:
• Departing from using heuristic patterns for display photo-
metric stereo, we directly learn display patterns that lead
to high-quality normal reconstruction for display photo-
metric stereo in an end-to-end manner.
• For DDPS, we propose the differentiable framework con-
sisting of basis-illumination image formation and analytic
photometric-stereo reconstruction, the use of 3D-printed
objects for a training dataset, and using a polarized LCD
and a polarization camera.
• We perform extensive experiments, demonstrating the ef-
fectiveness of learned patterns, which outperforms heuris-
tic patterns, and the robustness of DDPS against various
factors including pattern initialization and calibration er-
rors.
2. Related Work
Illumination Patterns for Photometric Stereo One cru-
cial but often overlooked problem in photometric stereo
is deciding on illumination patterns, which is a set of in-
tensity distributions of light sources, so that accurate sur-
face normals can be reconstructed. A standard option is
the one-light-at-a-time (OLAT) pattern that turns on each
light source at its maximum intensity one by one [47, 54].
OLAT is typically employed when the intensity of each light
source is sufficient enough to provide light energy to be de-
tected by a camera sensor without significant noise, such
as in light stages [13]. Extending OLAT patterns with a
group of neighboring light sources increases light energy,
reducing measurement noise [8, 48]. Spherical gradient il-
lumination, designed for light stages, enables rapid acquisi-
tion of high-fidelity normals by exploiting polarization [32],
color [35], or both [16]. Complementary patterns, where
half of the lights are turned on and the other half off for
each three-dimensional axis, also enable rapid reconstruc-tion when applied to light stages and monitors [24, 28].
Wenger et al.[48] propose random binary patterns that pro-
vide high light efficiency. However, the aforementioned il-
lumination patterns are heuristically designed, which often
result in sub-optimal reconstruction accuracy and capture
efficiency. For a specific display-camera system, it is chal-
lenging to determine which display patterns would provide
high-quality photometric stereo. DDPS departs from using
heuristic patterns and instead learns display patterns for ro-
bust photometric stereo.
Illumination-optimized Systems Recent studies have in-
vestigated optimizing illumination designs for inverse ren-
dering [25, 26,33,53], active-stereo depth imaging [5],
and holographic display [41]. These approaches typically
rely on dedicated illumination modules such as LED ar-
rays, diffractive optical elements, and spatial light modu-
lators. In contrast, DDPS exploits ubiquitous LCD devices
and their polarization state for display illumination. Also,
DDPS directly applies normal reconstruction loss to illumi-
nation learning using the 3D-printed dataset, unlike previ-
ous method that employ intermediary metrics, such as lu-
mitexel prediction [25, 26,33]. Zhang et al.[53] optimize
a single illumination pattern for inverse rendering, only tar-
geting planar objects. In contrast, DDPS reconstructs sur-
face normals of general objects with complex shapes and
capable of optimizing multiple illumination patterns.
Imaging Systems for Photometric Stereo Many photo-
metric stereo systems have been proposed, including mov-
ing a point light source, such as a flashlight on a mo-
bile phone [20, 43], a DSLR camera flash [14, 17]. and
installing multiple point light sources in light stage sys-
tems [29, 35] and other custom devices [19, 24–26, 33].
Display photometric stereo exploits off-the-shelf displays
as cost-effective, versatile active-illumination modules ca-
pable of generating spatially-varying trichromatic intensity
variation [1, 11,15,18,28,31,38]. Lattas et al. [28] demon-
strated facial capture using multiple off-the-shelf monitors
and multi-view cameras with trichromatic complementary
illumination. In our paper, we build on display photomet-
ric stereo and propose to learn the display patterns to obtain
high-quality normal reconstruction.
Photometric Stereo Dataset Many datasets have been
proposed for photometric stereo [2, 30,34,42,51] for
evaluation or training photometric stereo methods. Early
datasets often relied on synthetic rendering [9, 44]. How-
ever, using synthetic datasets for a real-world target sys-
tem requires highly accurate calibration of the target
photometric-stereo system, its replication on the rendering,
and physically realistic light-transport simulation. Real-
world datasets relax these constraints by capturing real-
11832
Loss
Diffuse images
Display patterns (optimization variables)Pattern training
Rendering3D model fittingDataset acquisition
Basis capture
3D printingCaptured imagesTesting
CaptureGround-truth
normals
ReconstructionEstimated
normals
Basis images
Forward BackwardFigure 2. Overview of DDPS. DDPS consists of three stages: dataset acquisition, pattern training, and testing.
world objects under multiple point light sources [30, 42].
However, acquiring ground-truth normals of real-world ob-
jects often demands using high-quality commercial 3D
scanners. In contrast, DDPS uses 3D printing to obtain ob-
jects with known geometry. Using the 3D-printed dataset
combined with 3D model fitting allows for effectively su-
pervising the pattern learning in an end-to-end manner.
3. Overview
DDPS consists of three stages as shown in Figure 2: dataset
acquisition, pattern training, and testing. First, in the
dataset-acquisition stage, we 3D-print various objects, cap-
ture their basis-illumination images with a target display-
camera setup, and obtain ground-truth surface normal maps
via 3D model fitting. Then, in the pattern-training stage,
we learn the optimal display patterns that lead to high-
quality normal reconstruction using the real-world train-
ing dataset. To this end, we develop the differentiable
framework of basis-illumination image formation and ana-
lytic photometric-stereo reconstructor. In the testing phase,
we capture diverse real-world objects under the patterns
learned on our training dataset and reconstruct surface nor-
mals using the photometric-stereo reconstructor.
4. Polarimetric Display-Camera Imaging
Polarimetric Light Transport We first describe our
imaging system, shown in Figure 3(a). We use off-the-
shelf components: a curved 4K LCD monitor and a po-
larization camera. Linearly-polarized light is emitted from
the LCD monitor, due to the polarization-based working
principle of LCDs [12]. The light interacts with a real-
world scene, generating both specular and diffuse reflec-
tions. The specular reflection tends to maintain the polar-
ization state of light, while diffuse reflection becomes un-
polarized [7]. The polarization camera then captures the
reflected light at four different linear-polarization angles:
{Iθ}θ∈{0◦,45◦,90◦,135◦}. We then convert the captured raw
intensities {Iθ}into the linear Stokes-vector elements [12]:
s0=P
θIθ
2, s1=I0◦−I90◦, s2= 2I 45◦−I0◦,(1)
(a) Imaging system (b) Diffuse-specular
separation
LCD monitor
Polarization
camera
ObjectLinearly-polarized
light
Linearly-polarized
specular reflection
Unpolarized
diffuse reflection
Figure 3. Polarimetric imaging system. (a) Imaging system
consisting of an LCD monitor and a polarization camera. De-
composed (b) diffuse image and specular image using linearly-
polarized light emitted from the monitor.
and compute the diffuse reflection Idiffuse and specular re-
flection Ispecular :Ispecular =p
s2
1+s2
2, I diffuse =s0−
Ispecular . Hereafter, we will denote I←Idiffuse as the diffuse
image obtained by the polarimetric decomposition. Fig-
ure 3(b) shows the separated diffuse and specular images.
The diffuse image Iwill be used for photometric stereo.
Note that this diffuse-specular separation using polarized
illumination and cameras has been often used in other sys-
tems [15, 18] such as light stages. DDPS applies the same
principle to the display photometric stereo by using a con-
ventional LCD and a polarization camera.
Display Superpixels For the computational efficiency of
our end-to-end optimization, we parameterize the display
withP= 16 ×9superpixels, where each superpixel is
a group of 240×240pixels. Ablation on the superpixel
resolution can be found in the Supplemental Document.
Calibration We estimate the location of each superpixel
with respect to the camera. To this end, we develop a
mirror-based calibration method that estimates superpixel
locations by using display patterns reflected on a mirror.
We refer to the Supplemental Document for the details on
11833
the mirror-based calibration. We also calibrate the intrinsic
parameters of the camera and the non-linearity of display
intensity using standard methods [55]. Figure 7shows the
calibrated superpixel locations.
5. Dataset Creation using 3D Printing
We describe our strategy for creating a training dataset us-
ing 3D printing. This allows for easily creating a real-
world dataset with known geometry that can be used for
DDPS. Figure 4(a)&(b) show the 3D printed objects and
their ground-truth 3D models. For each training scene, we
capture raw basis images B={Bj}P
j=1, where jis the in-
dex of the basis illumination of which only j-th superpixel
is turned on with its full intensity as white color. We then
extract the silhouette mask Susing the average image of the
basis images Iavgthat present well-lit appearance for most
of the object scene points as shown in Figure 4(c). Given
the silhouette mask S, we align the ground-truth geometry
of the 3D-printed object in the scene by optimizing the pose
of the ground-truth mesh with a silhouette rendering loss:
minimize
t,r∥fs(π;t,r)−S∥2
2, (2)
where πis the known 3D model, tandrare the transla-
tion and rotation of the model. fs(·)is the differentiable
silhouette rendering function. We solve Equation (2) using
gradient descent in Mitsuba3 [23]. Once the pose param-
eters are obtained, we render the normal map with the 3D
model at the optimized pose, which serves as the ground-
truth normal map NGT, shown in Figure 4. We create 40
training scenes and 4 test scenes with ground-truth normals.
Note that even trained on the 3D-printed objects, DDPS en-
ables effective reconstruction for diverse real-world objects
as demonstrated in the results.
6. Learning Display Patterns
We learn display patterns using the 3D-printed training
dataset consisting of ground-truth normal maps NGTand
basis images B={Bj}P
j=1. We denote Kdifferent display
patterns as M={Mi}K
i=1, where the i-th display pattern
Miis modeled as an RGB intensity pattern of Psuperpix-
els:Mi∈RP×3, which is our optimization variable.
For end-to-end training of the display RGB intensity pat-
ternsM, we develop a differentiable image formation func-
tionfI(·)and a differentiable photometric-stereo method
fn(·), which are chained together via auto-differentiation.
The differentiable image formation fI(·)takes a display
pattern Miand the basis images Bof a training scene, and
simulates the captured images I={Ii}K
i=1for the display
patterns being optimized. The photometric-stereo method
fn(·)then processes the simulated captured images Ito
estimate surface normal N. Below, we describe each com-
ponent in details.
(a) 3D-printed objects (b) Rendered objects
(c) Average image 
overlayed with the 
ground -truth silhouette(d) Fitted silhouette (red) 
overlayed with the 
average image(e) Ground -truth normal 
map from the fitted 3D 
model
Figure 4. Training dataset creation with 3D printing. To learn
display patterns, we propose to use (a) 3D-printed objects that have
corresponding (b) known ground-truth 3D models. (c) We extract
the silhouette Sfrom the averaged basis images and (d) align the
ground-truth 3D models with the captured image as depicted with
the fitted silhouette in red on top of the average image. (e) We
obtain a ground-truth normal map from the fitted 3D model.
6.1. Differentiable Image Formation
For the basis images Bof a training sample, we simulate a
raw image captured under a display pattern Mias
Ii=fI(Mi,B) =PX
j=1BjMi,j, (3)
where Mi,jis the j-th superpixel RGB intensity in the dis-
play pattern Mi. For Kdisplay patterns, we synthesize
each image as
I={fI(Mi,B)}K
i=1. (4)
Figure 5 shows the overview of our image formation.
This weighted-sum formulation exploits the basis images
acquired for real-world 3D printed objects, based on light-
transport linearity in the regime of ray optics. Compared to
using variants of rendering equations as differentiable im-
age formations [5, 6], the image formation with basis im-
ages synthesizes realistic images in a computationally effi-
cient manner, comprising only a single weighted summa-
tion, serving as a memory-efficient and realistic image for-
mation suitable for end-to-end pattern learning.
6.2. Differentiable Photometric Stereo
We reconstruct surface normal Nfrom the images Icap-
tured or simulated under the display patterns M:
N=fn(I,M). (5)
Note that the images Imostly contain diffuse-reflection
components as a result of the polarimetric diffuse-specular
11834
Differentiable photometric stereoDisplay
patternsSimulated 
captured imagesEstimated surface normalsElement-wise product
Basis images of 3D-printed objectsDifferentiable image formation
Figure 5. Differentiable framework. Using 3D-printed objects as a dataset allows for simulating real-world captured images with a
differentiable image formation. We reconstruct high-fidelity surface normals using differentiable photometric stereo from the simulated
captured images.
separation described in Section 4. Using the optically-
separated diffuse image I, we develop an analytic trinoc-
ular photometric-stereo method that is independent of the
training dataset and has no training parameters. This en-
ables effective end-to-end learning of display patterns by
sorely focusing on optimizing display patterns without any
other learning variables such as neural networks.
We start by denoting the captured diffuse RGB intensity
of a camera pixel under the i-th display pattern as Ic
i, where
cis the color channel c∈ {R, G, B }. Note that dependency
on the pixel is omitted in the notation of Ic
ifor simplicity.
We denote the spatially-varying per-pixel illumination vec-
tor coming from the center of j-th superpixel on the mon-
itor to a scene point corresponding to the pixel as lj. Note
that the illumination vectors are computed considering the
different locations of the scene points. The scene points are
assumed to lie on a plane which is a fixed distance (50 cm in
our experiment) away from the camera. We then formulate
a linear equation as
I=ρ⊙MlN, (6)
where I∈R3K×1,ρ∈R3K×1, andN∈R3×1are the vec-
torized intensity, albedo, and surface normals. ⊙is Hadar-
mard product. M∈R3K×P,l∈RP×3are the matrices for
the pattern intensity and illumination directions. Note that
the only unknown variables are the surface normal Nand
the albedo ρ. Refer to the Supplemental Document for the
formulation details.
We set the albedo ρas the max intensities among cap-
tures to for numerical stability and solve for the surface nor-
malNusing the pseudo-inverse method: N←(ρ⊙Ml)†I,
where †is the pseudo-inverse operator. Figure 5shows the
reconstructed surface normals. We exploit the differentia-
bility of our analytic reconstructor for effective end-to-end
optimization of display patterns.6.3. Training
Equipped with the image formation and the reconstructor,
we learn the display patterns Mby solving an optimization
problem:
minimize
MX
B,NGTloss 
fn 
{fI(Mi,B)}K
i=1,M
, NGT
,
(7)
where loss(·) = (1 −N·NGT)/2, which is the normal-
ized cosine distance, penalizes the angular difference be-
tween the estimated and the ground-truth normals from the
3D-printed dataset, meaning that the patterns are learned on
the entire training dataset. To ensure the physically-valid
intensity range from zero to one of the display pattern M,
we apply a sigmoid function to the optimization variable:
M←sigmoid(M). We use Adam optimizer [27].
6.4. Testing
Once the display patterns are learned, we perform testing
on real-world objects. Specifically, we capture images un-
der the learned Kdisplay patterns, perform diffuse-specular
separation, and obtain diffuse image Iifor the i-th display
pattern. We then estimate surface normals using our photo-
metric stereo method:
N=fn(I). (8)
7. Assessments
We assess DDPS on diverse objects. Refer to the Supple-
mental Document for complete results.
Learned Patterns Figure 6shows the patterns learned
with DDPS. The learned patterns exhibit distinctively-
colored regions and adjusted brightness for robust normal
reconstruction. We evaluate the learned patterns regard-
ing normal-reconstruction accuracy with common heuris-
tic patterns: OLAT [47], group OLAT [8], monochromatic
11835
(a) Patterns
Mono-gradient initialization
0
1
0.5
0
1
0.5Iteration 0 Iteration 450
 Iteration 4 Iteration 0 Iteration 450 Iteration 4
(b) Estimated normals (Top row) & error maps (Bottom row)
Flat gray initialization
Figure 6. Learning process. DDPS allows the learning of dis-
play patterns for high-quality normal reconstruction, not only from
sub-optimal heuristically-designed patterns but also from flat gray
noise that does not require any prior knowledge of the imaging
system.
gradient [32], monochromatic complementary [24], trichro-
matic gradient [35], trichromatic complementary [28]. Ta-
ble 1 show that the learned patterns outperform all existing
heuristic patterns on the test dataset. We measured the av-
erage reconstruction error loss(·) on the 3D-printed test
dataset. It is worth noting that DDPS allows using ini-
tial patterns that do not require any prior knowledge of the
imaging system. That is, initialization with monochromatic
random, trichromatic random, and flat gray noise also re-
sults in competitive results.
Real-world Objects For the experiments, we used 40
training scenes containing various 3D-printed objects. Even
though increasing the number of training samples is feasi-
ble, we found that DDPS already allows for high-quality re-
construction for in-the-wild real-world objects in this con-
figuration, as shown in Figure 10. We speculate that this
capability originates from effective rendering and analytical
reconstruction without any additional training parameters as
well as supervision with the 3D-printed dataset captured by
a real setup.
Number of Patterns Since photometric stereo solves for
five unknowns (RGB diffuse albedo and surface normals),
the minimum number of patterns is set to two, providing
six measurements with the RGB channel for each. Table 2
shows that using two patterns learned by DDPS already
outperforms any tested heuristic design using four patterns,
demonstrating improved capture efficiency. Moreover, us-
ing two learned patterns is often sufficient, as shown by theIllumination Number Reconstruction error↓
patterns ofpatterns Initial Learned
OLA T 4 0.1707 0.0486
Group
OLAT 4 0.0805 0.0475
Mono-gradient 4 0.0913 0.0443
Mono-complementary 4 0.1044 0.0453
T
ri-gradient 2 0.0933 0.0512
T
ri-complementary 2 0.0923 0.0478
Flatgray 4 0.3930 0.0466
Mono-random 4 0.2533 0.0484
T
ri-random 2 0.1461 0.0476
Table 1. Comparison of display patterns without and with our end-
to-end optimization.
Number Reconstruction error↓
ofpatterns Initial Learned
2 0.1461 0.0476
3 0.1415 0.0467
4 0.1096 0.0463
5 0.1001 0.0467
Table 2. Quantitative results of reconstructed surface normals with
varying number of patterns for the trichromatic random patterns.
converged reconstruction errors.
Robustness to Simplifications For efficient end-to-end
pattern learning, DDPS has made assumptions including
light source modeling and intensity falloff in its image for-
mation and reconstruction. While the validity of these
assumptions is often critical for conventional approaches
that use synthetic training data, DDPS exhibits robustness
against such simplifications, as demonstrated in all the qual-
itative and quantitative results. This is because the learned
display patterns are optimized to achieve accurate normal
reconstruction on a real-world 3D-printed dataset, taking
into account such assumptions.
Here, we conduct additional experiments to test the ro-
bustness of DDPS. First, we evaluate DDPS under inac-
curate superpixel locations. Instead of using our mirror-
based calibration (Section 4), we manually place super-
pixels to lie at grid locations on a 3D plane, which devi-
ates from the ground-truth locations. See Figure 7. DDPS
with the inaccurate superpixel locations still provides ac-
curate normal reconstruction with the error 0.0456 compa-
rable to 0.0453 corresponding to using accurate superpixel
locations. Second, we evaluate the assumption of consis-
tent intensity with respect to distance. DDPS with and
without intensity fall-off show comparable reconstruction
errors of 0.0429 and 0.0453, indicating the robustness of
DDPS against light fall-off modeling. Third, we test test
DDPS for an object at varying depths: 40/50/80/100 cm.
Even though we assume planar scene geometry at a fixed
distance of 50 cm in our image formation, DDPS enables
accurate normal reconstruction with the corresponding er-
11836
(a) Captured scene
under white illumination
(c) Estimated normals
Inaccurate superpixel locations Accurate superpixel locations
Camera viewing directionManually-assigned
inaccurate superpixel locations
on a 3D plane
Accurate superpixel locations 
via calibration
(b) Superpixel locations with respect to the camera
(d) Learned mono-complementary patterns
Figure 7. Robustness against inaccurate superpixel locations.
We test DDPS for our calibrated curved-monitor superpixel loca-
tions, shown as (b) orange dots, and for the manually placed inac-
curate plane superpixel locations, shown as blue dots, respectively.
DDPS automatically compensates for the location error of the su-
perpixels by (d) learning display patterns for such configuration,
resulting in (c) high-quality normal maps.
rors of 0.0494/0.0417/0.0428/0.0561 for the varying depths.
That is, in that depth range, we achieve reconstruction er-
rors lower than 0.0805, which is the error using the best-
performing heuristic pattern, group OLAT for the 50cm-
distant objects. These experiments further demonstrate the
robustness of DDPS against various simplifications.
Impact of Diffuse-specular Separation In order to ac-
quire diffuse-dominant images, DDPS exploits linearly-
polarized light emitted from the monitor and the polariza-
tion camera. Figure 8 shows that the reconstructed sur-
face normals from the diffuse-dominant images obtained by
DDPS provide more accurate reconstruction than using the
images containing both diffuse and specular reflections.
Comparison with Learning-based Photometric Stereo
We compare the reconstructed normals using the learned
patterns to state-of-the-art normal-reconstruction methods
that leverage neural networks and support area light sources
compatible with our learned patterns: UniPS [21], SDM-
Diffuse + specular
Diffuse
Diffuse Diffuse + specular
Captured scene
Estimated normalsFigure 8. Impact of diffuse-specular separation. DDPS exploits
polarization for optical diffuse-specular separation, leading to ac-
curate normal reconstruction.
SDM-UniPS UniPS
Ours [Bae et al.]
Images
Estimated normals
Figure 9. Comparison to learning-based methods. DDPS with
the analytic reconstructor shows fine geometric details on the leafs,
vase, and textile, outperforming the other methods.
UniPS [22], and Bae et al. [4]. UniPS and SDM-UniPS use
multiple images under diverse unknown illumination con-
ditions. Bae et al. [4] reconstruct the normal map from a
single image. Figure 9shows that DDPS outperforms the
other methods. In particular, uncalibrated learned methods
often fail to handle out-of-distribution examples such as the
leaves in the scene. In contrast, DDPS exploits shading cue
for physically-valid and accurate normal reconstruction.
Learning-based Reconstructor DDPS uses ana-
lytic photometric stereo as a training-free and dataset-
independent module for normal reconstruction. When
we simply replace the analytic photometric stereo with a
learning-based photometric stereo, UniPS [21], the average
11837
Learned patterns with group OLAT initialization
Learned patterns with OLAT initialization
Learned patterns with mono-random initialization
(b) Reconstructed normals(a) Learned patterns and captured scenesFigure 10. Reconstruction results. We reconstruct normals of diverse objects with the learned patterns using DDPS. Note that the patterns
are learned on our 3D-printed training dataset.
reconstruction error increases from 0.0475 to 0.0951, using
the group-OLAT initialization. This degradation can be
attributed to that the backward gradient for the display
patterns does not flow as effectively due to the complex
network structure of UniPS. Also, the network is not
designed to effectively utilize complex display patterns.
Developing a learning-based photometric stereo suitable
for DDPS would be an interesting future work.
8. Discussion
First, DDPS focuses on estimating normals, leaving depth
reconstruction as a future work. Using multi-view cameras
could resolve the problem and prompt research into opti-
mizing patterns for multi-view cameras. Second, we en-
countered challenges in achieving high-speed synchroniza-
tion between the display and the camera. This could po-
tentially be circumvented with external hardware triggering,
which would facilitate the reconstruction of surface normals
for dynamic objects. Third, it would be interesting to apply
DDPS for various types of display-camera systems such as a
mobile phone. Lastly, our image formation model does not
consider shadow and global illumination, which we further
analyze in our Supplemental Document.9. Conclusion
In this paper, we presented DDPS, a method for learning
display patterns for robust display photometric stereo de-
parting from using heuristic patterns. Our differentiable
framework consisting of basis-illumination image forma-
tion and analytic photometric stereo, the use of 3D printing
for real-dataset creation, and display polarimetric separa-
tion allow for learning display patterns that leads to high-
quality normal reconstruction for diverse objects. Also,
DDPS demonstrates robustness against various simplifi-
cations in image formation, reconstruction, and calibra-
tion. We believe that DDPS takes a step towards practical
high-quality 3D reconstruction.Beyond display photometric
stereo, the principles underpinning DDPS would be applied
to a range of illumination-camera systems, including light
stages, mobile phones, and large-scale displays.
Acknowledgements This work was partly sup-
ported by Korea NRF (RS-2023-00211658,
2022R1A6A1A03052954, RS-2023-00280400), Sam-
sung Advanced Institute of Technology, Samsung Research
Funding & Incubation Center for Future Technology grant
(SRFC-IT1801-52), and Samsung Electronics.
11838
References
[1] Miika Aittala, Tim Weyrich, and Jaakko Lehtinen. Practical
svbrdf capture in the frequency domain. ACM Trans. Graph.,
32(4):110–1, 2013. 1, 2
[2] Neil Alldrin, Todd Zickler, and David Kriegman. Photo-
metric stereo with non-parametric and spatially-varying re-
flectance. In 2008 IEEE Conference on Computer Vision and
Pattern Recognition, pages 1–8. IEEE, 2008. 2
[3] Dejan Azinovi ´c, Olivier Maury, Christophe Hery, Matthias
Nießner, and Justus Thies. High-res facial appearance cap-
ture from polarized smartphone images. In IEEE Conf. Com-
put. Vis. Pattern Recog., June 2023. 1
[4] Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla. Es-
timating and exploiting the aleatoric uncertainty in surface
normal estimation. In IEEE Conf. Comput. Vis. Pattern
Recog., pages 13137–13146, 2021. 7
[5] Seung-Hwan Baek and Felix Heide. Polka lines: Learning
structured illumination and reconstruction for active stereo.
InIEEE Conf. Comput. Vis. Pattern Recog., pages 5757–
5767, 2021. 2,4
[6] Seung-Hwan Baek and Felix Heide. All-photon polarimetric
time-of-flight imaging. In IEEE Conf. Comput. Vis. Pattern
Recog., pages 17876–17885, 2022. 4
[7] Seung-Hwan Baek, Tizian Zeltner, Hyunjin Ku, Inseung
Hwang, Xin Tong, Wenzel Jakob, and Min H Kim. Image-
based acquisition and modeling of polarimetric reflectance.
ACM Trans. Graph., 39(4):139, 2020. 3
[8] Sai Bi, Stephen Lombardi, Shunsuke Saito, Tomas Simon,
Shih-En Wei, Kevyn Mcphail, Ravi Ramamoorthi, Yaser
Sheikh, and Jason Saragih. Deep relightable appearance
models for animatable faces. ACM Trans. Graph., 40(4):1–
15, 2021. 2,5
[9] Guanying Chen, Michael Waechter, Boxin Shi, Kwan-Yee K
Wong, and Yasuyuki Matsushita. What is learned in deep
uncalibrated photometric stereo? In Eur. Conf. Comput. Vis.,
pages 745–762. Springer, 2020. 2
[10] Ziang Cheng, Junxuan Li, and Hongdong Li. Wildlight: In-
the-wild inverse rendering with a flashlight. In IEEE Conf.
Comput. Vis. Pattern Recog., June 2023. 1
[11] James J Clark. Photometric stereo using lcd displays. Image
and Vision Computing, 28(4):704–714, 2010. 2
[12] Edward Collett. Field guide to polarization. Spie Belling-
ham, WA, 2005. 3
[13] Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter
Duiker, Westley Sarokin, and Mark Sagar. Acquiring the
reflectance field of a human face. In Proceedings of the
27th annual conference on Computer graphics and interac-
tive techniques, pages 145–156, 2000. 2
[14] Valentin Deschaintre, Yiming Lin, and Abhijeet Ghosh.
Deep polarization imaging for 3d shape and svbrdf acqui-
sition. In IEEE Conf. Comput. Vis. Pattern Recog., pages
15567–15576, 2021. 2
[15] Yannick Francken, Tom Cuypers, Tom Mertens, Jo Gielis,
and Philippe Bekaert. High quality mesostructure acquisi-
tion using specularities. In IEEE Conf. Comput. Vis. Pattern
Recog., pages 1–7. IEEE, 2008. 2,3
[16] Graham Fyffe and Paul Debevec. Single-shot reflectance
measurement from polarized color gradient illumination. In
IEEE Int. Conf. Comput. Photo., pages 1–10. IEEE, 2015. 2[17] Graham Fyffe, Paul Graham, Borom Tunwattanapong, Ab-
hijeet Ghosh, and Paul Debevec. Near-instant capture of
high-resolution facial geometry and reflectance. In Comput.
Graph. Forum, volume 35, pages 353–363. Wiley Online Li-
brary, 2016. 2
[18] Abhijeet Ghosh, Tongbo Chen, Pieter Peers, Cyrus A Wil-
son, and Paul Debevec. Estimating specular roughness and
anisotropy from second order spherical gradient illumina-
tion. In Comput. Graph. Forum, volume 28, pages 1161–
1170. Wiley Online Library, 2009. 2,3
[19] Vlastimil Havran, Jan Ho ˇsek, ˇS´arka N ˇemcov ´a, Jiˇr´ıˇC´ap, and
Jiˇr´ı Bittner. Lightdrum—portable light stage for accurate btf
measurement on site. Sensors, 17(3):423, 2017. 2
[20] Zhuo Hui, Kalyan Sunkavalli, Joon-Young Lee, Sunil
Hadap, Jian Wang, and Aswin C Sankaranarayanan. Re-
flectance capture using univariate sampling of brdfs. In Int.
Conf. Comput. Vis., pages 5362–5370, 2017. 2
[21] Satoshi Ikehata. Universal photometric stereo network using
global lighting contexts. In IEEE Conf. Comput. Vis. Pattern
Recog., pages 12591–12600, 2022. 7
[22] Satoshi Ikehata. Scalable, detailed and mask-free universal
photometric stereo. arXiv preprint arXiv:2303.15724, 2023.
7
[23] Wenzel Jakob, S ´ebastien Speierer, Nicolas Roussel, Merlin
Nimier-David, Delio Vicini, Tizian Zeltner, Baptiste Nicolet,
Miguel Crespo, Vincent Leroy, and Ziyi Zhang. Mitsuba 3
renderer, 2022. https://mitsuba-renderer.org. 4
[24] Christos Kampouris, Stefanos Zafeiriou, and Abhijeet
Ghosh. Diffuse-specular separation using binary spherical
gradient illumination. In EGSR (EI&I), pages 1–10, 2018. 2,
6
[25] Kaizhang Kang, Zimin Chen, Jiaping Wang, Kun Zhou, and
Hongzhi Wu. Efficient reflectance capture using an autoen-
coder. ACM Trans. Graph., 37(4):1–10, 2018. 2
[26] Kaizhang Kang, Cihui Xie, Chengan He, Mingqi Yi, Minyi
Gu, Zimin Chen, Kun Zhou, and Hongzhi Wu. Learning
efficient illumination multiplexing for joint capture of re-
flectance and shape. ACM Trans. Graph., 38(6):1–12, 2019.
2
[27] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In 3rd International Conference on
Learning Representations, ICLR 2015, San Diego, CA, USA,
May 7-9, 2015, Conference Track Proceedings, 2015. 5
[28] Alexandros Lattas, Yiming Lin, Jayanth Kannan, Ekin
Ozturk, Luca Filipi, Giuseppe Claudio Guarnera, Gaurav
Chawla, and Abhijeet Ghosh. Practical and scalable desktop-
based high-quality facial capture. In Eur. Conf. Comput. Vis.,
pages 522–537. Springer, 2022. 1,2,6
[29] Chloe LeGendre, Xueming Yu, Dai Liu, Jay Busch, An-
drew Jones, Sumanta Pattanaik, and Paul Debevec. Practi-
cal multispectral lighting reproduction. ACM Trans. Graph.,
35(4):1–11, 2016. 1,2
[30] Min Li, Zhenglong Zhou, Zhe Wu, Boxin Shi, Changyu
Diao, and Ping Tan. Multi-view photometric stereo: A ro-
bust solution and benchmark dataset for spatially varying
isotropic materials. IEEE Trans. Image Process., 29:4159–
4173, 2020. 2,3
[31] Chao Liu, Srinivasa G Narasimhan, and Artur W Dubrawski.
Near-light photometric stereo using circularly placed point
11839
light sources. In IEEE Int. Conf. Comput. Photo., pages 1–
10. IEEE, 2018. 2
[32] Wan-Chun Ma, Tim Hawkins, Pieter Peers, Charles-Felix
Chabert, Malte Weiss, and Paul Debevec. Rapid acquisition
of specular and diffuse normal maps from polarized spheri-
cal gradient illumination. In Eur. Conf. Render. Tech., pages
183–194, 2007. 1,2,6
[33] Xiaohe Ma, Kaizhang Kang, Ruisheng Zhu, Hongzhi Wu,
and Kun Zhou. Free-form scanning of non-planar appear-
ance with neural trace photography. ACM Trans. Graph.,
40(4):1–13, 2021. 2
[34] Roberto Mecca, Fotios Logothetis, Ignas Budvytis, and
Roberto Cipolla. Luces: A dataset for near-field point light
source photometric stereo. arXiv preprint arXiv:2104.13135,
2021. 2
[35] Abhimitra Meka, Christian Haene, Rohit Pandey, Michael
Zollh ¨ofer, Sean Fanello, Graham Fyffe, Adarsh Kowdle,
Xueming Yu, Jay Busch, Jason Dourgarian, et al. Deep
reflectance fields: high-quality facial reflectance field infer-
ence from color gradient illumination. ACM Trans. Graph.,
38(4):1–12, 2019. 1,2,6
[36] Abhimitra Meka, Rohit Pandey, Christian Haene, Sergio
Orts-Escolano, Peter Barnum, Philip David-Son, Daniel Er-
ickson, Yinda Zhang, Jonathan Taylor, Sofien Bouaziz, et al.
Deep relightable textures: volumetric performance capture
with neural rendering. ACM Trans. Graph., 39(6):1–21,
2020. 1
[37] Giljoo Nam, Joo Ho Lee, Diego Gutierrez, and Min H Kim.
Practical svbrdf acquisition of 3d objects with unstructured
flash photography. ACM Trans. Graph., 37(6):1–12, 2018. 1
[38] Emilie Nogue, Yiming Lin, and Abhijeet Ghosh.
Polarization-imaging surface reflectometry using near-
field display. In Eurographics Symposium on Rendering.
The Eurographics Association, volume 2, 2022. 2
[39] Rohit Pandey, Sergio Orts Escolano, Chloe Legendre, Chris-
tian Haene, Sofien Bouaziz, Christoph Rhemann, Paul De-
bevec, and Sean Fanello. Total relighting: learning to relight
portraits for background replacement. ACM Trans. Graph.,
40(4):1–21, 2021. 1
[40] Jaesik Park, Sudipta N Sinha, Yasuyuki Matsushita, Yu-
Wing Tai, and In So Kweon. Robust multiview photomet-
ric stereo using planar mesh parameterization. IEEE Trans.
Pattern Anal. Mach. Intell., 39(8):1591–1604, 2016. 1
[41] Yifan Peng, Suyeon Choi, Nitish Padmanaban, and Gordon
Wetzstein. Neural holography with camera-in-the-loop train-
ing. ACM Trans. Graph., 39(6):1–14, 2020. 2
[42] Jieji Ren, Feishi Wang, Jiahao Zhang, Qian Zheng, Mingjun
Ren, and Boxin Shi. Diligent102: A photometric stereo
benchmark dataset with controlled shape and material vari-
ation. In IEEE Conf. Comput. Vis. Pattern Recog., pages
12581–12590, 2022. 2,3
[43] J Riviere, P Peers, and A Ghosh. Mobile surface reflectom-
etry. In Comput. Graph. Forum, volume 1, pages 191–202,
2016. 2
[44] Hiroaki Santo, Masaki Samejima, Yusuke Sugano, Boxin
Shi, and Yasuyuki Matsushita. Deep photometric stereo net-
work. In Int. Conf. Comput. Vis. Worksh., pages 501–509,
2017. 2
[45] Carolin Schmitt, Simon Donne, Gernot Riegler, VladlenKoltun, and Andreas Geiger. On joint estimation of pose, ge-
ometry and svbrdf from a handheld scanner. In IEEE Conf.
Comput. Vis. Pattern Recog., pages 3493–3503, 2020. 1
[46] Soumyadip Sengupta, Brian Curless, Ira Kemelmacher-
Shlizerman, and Steven M Seitz. A light stage on every
desk. In IEEE Conf. Comput. Vis. Pattern Recog., pages
2420–2429, 2021. 1
[47] Tiancheng Sun, Zexiang Xu, Xiuming Zhang, Sean Fanello,
Christoph Rhemann, Paul Debevec, Yun-Ta Tsai, Jonathan T
Barron, and Ravi Ramamoorthi. Light stage super-
resolution: continuous high-frequency relighting. ACM
Trans. Graph., 39(6):1–12, 2020. 2,5
[48] Andreas Wenger, Andrew Gardner, Chris Tchou, Jonas
Unger, Tim Hawkins, and Paul Debevec. Perfor-
mance relighting and reflectance transformation with time-
multiplexed illumination. ACM Trans. Graph., 24(3):756–
764, 2005. 2
[49] Tim Weyrich, Wojciech Matusik, Hanspeter Pfister, Bernd
Bickel, Craig Donner, Chien Tu, Janet McAndless, Jinho
Lee, Addy Ngan, Henrik Wann Jensen, et al. Analysis of
human faces using a measurement-based skin reflectance
model. ACM Trans. Graph., 25(3):1013–1024, 2006. 1
[50] Robert J Woodham. Photometric method for determining
surface orientation from multiple images. Optical engineer-
ing, 19(1):139–144, 1980. 1
[51] Ying Xiong, Ayan Chakrabarti, Ronen Basri, Steven J
Gortler, David W Jacobs, and Todd Zickler. From shading
to local shape. IEEE transactions on pattern analysis and
machine intelligence, 37(1):67–79, 2014. 2
[52] Kai Zhang, Fujun Luan, Zhengqi Li, and Noah Snavely. Iron:
Inverse rendering by optimizing neural sdfs and materials
from photometric images. In IEEE Conf. Comput. Vis. Pat-
tern Recog., pages 5565–5574, 2022. 1
[53] Lianghao Zhang, Fangzhou Gao, Li Wang, Minjing Yu,
Jiamin Cheng, and Jiawan Zhang. Deep svbrdf estima-
tion from single image under learned planar lighting. In
ACM SIGGRAPH 2023 Conference Proceedings, pages 1–
11, 2023. 2
[54] Xiuming Zhang, Sean Fanello, Yun-Ta Tsai, Tiancheng Sun,
Tianfan Xue, Rohit Pandey, Sergio Orts-Escolano, Philip
Davidson, Christoph Rhemann, Paul Debevec, et al. Neu-
ral light transport for relighting and view synthesis. ACM
Trans. Graph., 40(1):1–17, 2021. 2
[55] Zhengyou Zhang. A flexible new technique for cam-
era calibration. IEEE Trans. Pattern Anal. Mach. Intell.,
22(11):1330–1334, 2000. 4
[56] Taotao Zhou, Kai He, Di Wu, Teng Xu, Qixuan Zhang, Kuix-
iang Shao, Wenzheng Chen, Lan Xu, and Jingyi Yi. Re-
lightable neural human assets from multi-view gradient illu-
minations. 2023. 1
11840
