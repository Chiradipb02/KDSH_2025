Interactive3D: Create What You Want by Interactive 3D Generation
Shaocong Dong1*, Lihe Ding2,4*, Zhanpeng Huang3, Zibin Wang3,
Tianfan Xue2†, Dan Xu1†
1Hong Kong University of Science and Technology2The Chinese University of Hong Kong
3SenseTime Research4Shanghai AI Laboratory
{sdongae, danxu }@cse.ust.hk, {dl023, tfxue }@ie.cuhk.edu.hk
{wangzb02, yiyuanzhang.ai }@gmail.com, {huangzhanpeng }@sensetime.com
&
(a) Deformable Drag
to add wings(b) Stretch
to enlarge wings(c) Rigid Drag
to swing wings(d) Local Semantic Editing
for burning wings
I. First Stage: Gaussian Splatting
II. Second Stage: InstantNGP(e) Combine to
dragon knight(f) Representation Converter
(g) Interactive 
Hash Refinement“A Draven Knight”“A red fire dragon”
Figure 1. Interactive 3D Generation .The first stage involves using Gaussian ellipsoids for creating a base model where users can
interact through different operations, such as deformable dragging to add features like wings, stretching to enlarge parts, rigid dragging to
reposition elements, and local semantic editing to apply specific visual effects, e.g., making wings appear aflame. The second stage shows
the conversion of this proposed Gaussian representation into an InstantNGP structure followed by an Interactive Hash Refinement process,
allowing for further detailed enhancements. It demonstrates the framework’s capability to merge and refine complex generations.
Abstract
3D object generation has undergone significant ad-
vancements, yielding high-quality results. However, fall
short of achieving precise user control, often yielding re-
sults that do not align with user expectations, thus limit-
ing their applicability. User-envisioning 3D object genera-
tion faces significant challenges in realizing its concepts us-
ing current generative models due to limited interaction ca-
pabilities. Existing methods mainly offer two approaches:
*Equal contribution.
†Corresponding author.(i) interpreting textual instructions with constrained con-
trollability, or (ii) reconstructing 3D objects from 2D im-
ages. Both of them limit customization to the confines of
the 2D reference and potentially introduce undesirable ar-
tifacts during the 3D lifting process, restricting the scope
for direct and versatile 3D modifications. In this work, we
introduce Interactive3D , an innovative framework for inter-
active 3D generation that grants users precise control over
the generative process through extensive 3D interaction ca-
pabilities. Interactive3D is constructed in two cascading
stages, utilizing distinct 3D representations. The first stage
employs Gaussian Splatting for direct user interaction, al-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
4999
lowing modifications and guidance of the generative direc-
tion at any intermediate step through (i)Adding and Re-
moving components, (ii)Deformable and Rigid Dragging,
(iii)Geometric Transformations, and (iv)Semantic Editing.
Subsequently, the Gaussian splats are transformed into In-
stantNGP . We introduce a novel (v)Interactive Hash Refine-
ment module to further add details and extract the geometry
in the second stage. Our experiments demonstrate that pro-
posed Interactive3D markedly improves the controllability
and quality of 3D generation. Our project webpage is avail-
able at https://interactive-3d.github.io/ .
1. Introduction
Recent advancements [20, 29, 30] in 2D image genera-
tion, exemplified by approaches, such as diffusion mod-
els trained on extensive text-image paired datasets (e.g.,
LAION-series [31]), have made significant strides in align-
ing generated images with textual prompts. Despite this
success, achieving precise control over image generation
to meet complex user expectations remains a severe chal-
lenge. ControlNet [38] addresses this by modifying foun-
dational 2D diffusion models with fine-tuning on specific
conditional datasets, offering a subtle control mechanism
guided by user-specific inputs.
On the other hand, 3D object generation, despite its
promising progress [27, 35], confronts more intricate chal-
lenges than those encountered in 2D image generation. Al-
though advancements have been observed from perspec-
tives, including training 3D diffusion models on direct 3D
datasets [10, 25], and lifting 2D diffusion priors to 3D rep-
resentations (e.g., NeRF [21]) via techniques like SDS loss
optimization [27], precise control over the generated ob-
jects has not been fully achieved . The reliance on initial text
prompts or 2D reference images severely limits the gener-
ation controllability and often results in lower quality. The
text prompts lack specificity to convey complex 3D designs
accurately; while the 2D reference images can inform 3D
reconstruction, they do not capture the full depth of 3D
structures, potentially leading to various unexpected arti-
facts. Moreover, personalization based on 2D images lacks
the flexibility that can be offered by direct 3D manipulation.
A straightforward idea to achieve controllable 3D gener-
ation is to adapt the ControlNet to 3D generation. However,
this strategy encounters significant obstacles: (i) the control
signals for 3D are inherently more complex, making the col-
lection of a conditioned 3D dataset exceptionally challeng-
ing when compared to the 2D paradigm; (ii) the absence of
powerful foundational models in the 3D domain, like stable
diffusion for 2D [20], impedes the possibility of developing
fine-tuning techniques at this time. These hurdles suggest
the need for a different strategy. As a result, we are inclinedto explore a novel question: can we directly integrate flexi-
ble human instructions into the 3D generation?
To address the above-identified challenges, we intro-
duce Interactive3D , a framework devised to facilitate user
interaction with intermediate outputs of the generative pro-
cess, enabling precise control over the generation and ef-
fective enhancement of generated 3D object quality. Our
approach is characterized by a two-stage process leveraging
distinct 3D representations. Specifically, the first stage uti-
lizes the SDS loss [27] to optimize a 3D object by Gaussian
Splatting [11] representation, which allows for independent
user modifications during the optimization process. The
second stage transforms the Gaussian representation into
InstantNGP [23] structures and applies proposed Interactive
Hash Refinement for detailed textures and 3D geometry.
For the first stage, the Gaussian blobs enable direct user
feedback, such as the addition and the removal of object
components by manipulating the Gaussian blobs, as il-
lustrated in the knight and dragon composition in Fig. 1,
and a dragging operation based on the principles of Drag-
GAN [26]. Users can select a source and a target point
within the 3D space, which guide the Gaussian blobs (con-
ceptualized as point clouds) from an original to a desired lo-
cation using a motion-supervision loss . Notably, our frame-
work facilitates both deformable and rigid dragging of ob-
ject elements through a rigid constraint loss , allowing users
to create new or adjust existing 3D model components.
Furthermore, our Interactive3D allows for precise selec-
tion of subsets of Gaussian blobs, to which users can apply
various transformations, e.g., the stretch operation shown
in Fig. 1 (b). By restricting the gradient flow to these sub-
sets, we enable focused optimization of chosen 3D parts us-
ing modified text prompts, without altering other areas of
the model. This strategy is termed as local semantic edit-
ing. Its capability is exemplified in Fig. 1 (d), where the
wings are edited to simulate the effect of being ablaze. To
enhance the guidance from user interactions during genera-
tion, we integrate an interactive SDS loss with an adaptive
camera zoom-in technique, significantly improving the op-
timization efficiency of the modified 3D areas.
At the beginning of the second stage, the modeled Gaus-
sian representations are transformed into InstantNGP struc-
tures through a swift NeRF distillation technique. This
strategic transformation synergies the strengths of both rep-
resentations: Gaussian blobs, while more friendly for direct
editing, face challenges in reconstructing high-quality 3D
geometry; on the other hand, InstantNGP structures excel
in providing a foundation for further geometry refinement
and mesh extraction, as depicted in Fig. 1 (f). Given that
the converted InstantNGP employs hash tables to associate
learnable features with 3D grids, we have developed an in-
novative Hash Refinement Module . This module enables
the interactive enhancement and detailing of chosen areas
5000
(c) A draven knight riding a red fire dragon
 (d) A draven knight wearing a helmet riding a red fire dragon
(a) A Genshin Impact anime girl with blue hair and dress
(b) A Genshin Impact anime girl with red hair wearing hat and dress
Figure 2. Qualitative generation results of the proposed Interactive3D. We achieve high-quality and controllable 3D generation.
within the base radiance fields, as demonstrated in Fig. 1
(g). Specifically, we begin by extracting coarse occupancy
grids from the initial radiance field. When a user selects
an area for optimization, a candidate grid set is determined
by the intersection of this area with the occupancy grids.
These grids are then categorized into multi-level sets with
finer resolutions than the original field, and multiple refine-
ment hash tables are constructed to map the selected area
to learnable refinement features, which are then fed into
lightweight MLPs to capture residual colors and densities,
concentrating detail enhancement on local surface regions.
Through our Interactive Hash Refinement module, users
can precisely control the refinement process by selecting
specific areas for optimization and adjusting the levels of
representation details, such as the hash table levels and the
capacities. Hence, users can sculpt their envisioned objects
during generation by employing the array of interactive op-
tions provided by Interactive3D .
In summary, our contributions are threefold:
•Framework Deisgn. We introduce a novel interactive 3D
generation framework, Interactive3D , which empowers
users to precisely control the 3D generation process via
direct instructions.
•3D Representations. We reveal that incorporating the
Gaussian Splatting for user interaction and the Instant-
NGP for further geometry refinement and mesh extraction
leads to higher-quality 3D generation.
•Generation Results. With precise control and effective
user feedback in the generation process, our method sig-
nificantly improves the generation quality.
2. Related Work
2.1. 3D Generative Models
3D Generation is a foundation task and has been studied
widely. Early works focus on the 3D representations in-
cluding voxels [5, 7, 17, 33, 36], point clouds [1, 22, 37],
meshes [6, 32, 39] and implicit fields [3, 19]. Recently, dif-
fusion models [8] have achieved great success in 2D content
creation [24, 28–30], and there has been substantial researchinto 3D diffusion, which has significantly improved 3D gen-
eration. These 3D diffusion models can be divided into two
directions: optimization-based [27] and feed-forward meth-
ods [10]. For the optimization-based methods, the Dream-
Fusion series [9, 13, 20, 27, 34] design an SDS loss based on
probability density distillation, which enables the use of a
2D diffusion model as a prior for optimization of a paramet-
ric image generator (e.g., rendered from NeRF [21]). They
optimize a 3D consistent radiance field via gradient descent
such that its 2D renderings from random angles achieve low
losses. These optimization-based methods can realize zero-
shot and high-quality generation with several hours of opti-
mization. Further works, such as Prolificdreamer [35] and
Fantasia3d [2], achieve higher quality in 3D content genera-
tion by modifying the SDS loss. For feed-forward methods,
PointE [25] generates a single synthetic view using a text-
to-image diffusion model, and then produces a 3D point
cloud using a second diffusion model that is conditioned
on the generated image. The follow-up work Shap-E [10]
trains a latent diffusion model on NeRF’s parameters. One-
2-3-45 [14] uses Zero-1-to-3 [15] to generate multi-view
images, which are fed into a SparseNeuS [16] for 3D gen-
eration. However, current methods [2, 9, 13, 20, 27, 34, 35]
only rely on initial text prompts or reference images to 3D
generation, restricting their controllability. In contrast, our
framework introduces user interactions into the optimiza-
tion, achieving flexible and controllable 3D generation.
2.2. Gaussian Splatting
The recent 3D representation Gaussian Splatting [11] has
revolutionized the 3D rendering field by using a set of Gaus-
sian blobs to represent the 3D scenes and splat them onto
the image plane to obtain renderings. Compared with Neu-
ral Radiance Field (NeRF), Gaussian splatting models the
3D world by explicit Gaussian blobs, which are flexible and
naturally suitable for human interactions and editing.
2.3. InstantNGP
To improve the quality and efficiency of NeRF, [23] pro-
poses InstantNGP which utilizes multi-level hash tables to
5001
map between learnable features and 3D query positions. In-
stantNGP does not handle the hash conflict explicitly and
makes the gradients to guide the optimization of features. In
this case, the surface positions obtain larger gradients dur-
ing training and dominate the feature updates. Thanks to
the adaptive feature update by hash mapping, InstantNGP
can represent the 3D world by more fine-grained grids and
easily extract 3D geometries (e.g., mesh) from the continu-
ous radiance field. However, InstantNGP is still an implicit
representation and is hard to interact with or edit.
3. The Proposed Interactive3D Approach
Achieving Interactive 3D Generation is non-trivial and
poses two core challenges: (i) we need to design an effi-
cient and effective interactive mechanism for the 3D repre-
sentation to control the generation process, and (ii) we need
to obtain high-quality 3D outputs (e.g., the mesh) aligned
with users’ expectations. As for the 3D representation,
NeRF [21] can output high-quality 3D objects, however, its
implicit representation makes it hard to incorporate interac-
tions. While the recent 3D Gaussian Splatting is naturally
suitable for interaction due to the independence and explic-
itly of Gaussian blobs, it cannot output the commonly used
mesh with high quality so far. Observing the complementar-
ity of these two representations, we design a two-stage in-
teractive 3D generation framework, where Stage I (Sec. 3.1)
adopts Gaussian Splatting to achieve flexible interactions
and Stage II (Sec. 3.2) converts the Gaussian blobs to In-
stantNGP for further geometry refinement and mesh extrac-
tion, as shown in Fig. 3. Below we first introduce the first
interactive generation stage with Gaussian Splatting.
3.1. Interaction with Gaussian Splatting
In the first stage, we represent a 3D object as a set of N
Gaussian blobs E={(ci, oi, µi,Σi)}N
i, where ci, oi, µi,Σi
represents the color, opacity, position, and the covariance
of the i-th Gaussian, respectively. To incorporate interac-
tions, we treat each Gaussian blob as a point and formulate
a point cloud set S={µi}N
i. Thanks to the flexibility of
treating Gaussian blobs as points, we can introduce various
user interactions by explicitly modifying the point set S, as
discussed in Sec. 3.1.1, Sec. 3.1.3, and Sec. 3.1.4. Once the
adjustment is finished, we adopt an interactive SDS loss to
optimize the modified parts efficiently in Sec. 3.1.5.
3.1.1 Adding and Removing Parts
To add parts, suppose we have two Gaussian blob sets E1
andE2, we can simply obtain the combined set by con-
catenating them. In practice, this interaction often happens
when the user desires to combine two objects (e.g., knight
and dragon as shown in Fig. 1).To remove parts P, we can directly delete the Gaussian
blobs within the part by:
E′=E − { ei}i∈I(P), (1)
where I(p) ={i:µi∈P}indicates the indexes of Gaus-
sian blobs in P. Importantly, the determination of whether
blobeibelongs to Pdepends on how we define the parts.
In practice, we offer two ways to select a part. The first one
is defined in 2D images. Specifically, the users can choose
a point on the rendered 2D images from Gaussian Splatting
and utilize an off-the-shell segmentor (e.g., SAM [12]) to
obtain multi-view part masks. Subsequently, we project all
the points in Sonto all masked views and obtain the blobs
with all projections within masks as the part blobs. We use
2 views by default. However, such a 2D mask-based part
selection needs to infer with a large-scale pretrained seg-
mentation network, which may be accurate but not efficient.
Furthermore, if we do not need a precise part set or we select
parts at the early stage of generation (the rendered images
may not be informative enough for SAM), we can directly
select points in 3D.
3.1.2 Geometry Transformation
Given that any interested part can be selected as discussed in
Sec. 3.1.1, we can take a further step to construct a bound-
ing box Bfor each P. In this way, all the traditional ge-
ometry transformations Tsuch as rotation, translation, and
stretching can be first applied to the part Pand then it is
concatenated with the unchanged set as follows:
Etrans=T(B(P)) +E′. (2)
3.1.3 Deformable and Rigid Dragging
Although we have achieved the part-level geometry trans-
formations, the users may prefer more flexible and di-
rect interactions. Inspired by DragGAN [26], we propose
deformable and rigid 3D Dragging operations. The de-
formable dragging regards the local structure as a plasticine
and aims to deform the geometry smoothly, e.g., dragging
new wings from the dragon’s back, while the rigid dragging
treats the local region as a rigid part and forces the local
structure unchanged. Specifically, we first select a source
point ps, a target point pt, and a local region radius r. Then,
the activated local object part can be written as:
P={ei|∥µi−ps∥2≤r}. (3)
Subsequently, we move all points with a minor offset along
the direction from pstoptbefore each optimization step:
µ′
i=µi+α(pt−ps)
∥pt−ps∥2, (4)
5002
Stage-I  Interaction with Gaussian Splatting
NeRF Distillation
Gaussian Splatting
Occupancy Grid InstantNGPInteractive Hash-Refinement
Select Interested Region Improved ResultStage-II  Refinement with InstantNGPGeometric Transformation Add / Remove Part Rigid Dragging Semantic Editing
“make it blue”
Deformable Dragging
Figure 3. The overall architecture of Interactive3D. It contains two stages with distinct 3D representations: (I)Gaussian Splatting for
flexible user interactions such as add/remove parts, geometry transformation, deformable or rigid dragging and semantic editing; (II)the
Gaussian blobs are converted to InstantNGP using NeRF distillation and fine-tuned by our Interactive Hash Refinement Module.
where αis a predefined hyper-parameter to con-
trol the movement step. Then the modified blobs
{e′
i= (ci, oi, µ′
i,Σi)}N
i=1can render images through Gaus-
sian Splatting. From the rendered images, we can compute
the SDS loss LSDSfollowing [4, 27]. Meanwhile, to encour-
age the part Pto move towards ptduring optimization, we
add a motion supervision loss following [26]:
Lmotion =X
i∈I(P)(∥µ′
i−pt∥1), (5)
where I(P)indicates a set of indexes of Gaussian blobs
inP. Then we can compute the gradient and update the
parameters of each Gaussian blob. Note that we do not need
a point tracking process as in [26] since the 3D points are
naturally tracked. Importantly, we apply densification and
pruning operations as in [11] to fill the gaps and eliminate
noises after the dragging operation.
To achieve deformable or rigid dragging, we made two
adjustments to the above process. First, for the deformable
dragging, we continuously update the activated part Pby
updating the current position of source point psas in Eq. (3),
which allows the newly generated points to be dragged,
leading to structure deformation. For the rigid dragging,
the part Pis fixed to encourage an integral movement. Sec-
ond, we introduce a rigid constraint loss to enforce the rigid
moving without the deformation:
Lrigid=X
i∈I(P)|∥ps−pi∥2− ∥p∗
s−p∗
i∥2|, (6)where p∗
sandp∗
iare the initial position of the source point
and its neighboring points, respectively. Lrigidencourages
the distance between the neighboring points and the source
point to remain unchanged, leading to integral movement.
In general, the user can flexibly combine the deformable
and rigid dragging operations by switching the above ad-
justments and accordingly modifying the hyperparameters
(e.g., the movement step αand the loss weight for Lrigid).
3.1.4 Semantic Editing
Sometimes the users may desire to interact with the gener-
ation process by simple text prompts. To achieve this, we
further propose a semantic editing operation. Specifically,
we can first select a part, and then input a new text prompt
(e.g., make the wings burning) to compute the SDS loss,
and optimize the parts correspondingly to match the newly
added semantic features. This operation can be better ac-
complished with an Interactive SDS loss discussed in the
following section.
3.1.5 Interactive SDS Loss
Once the above modifications are finished, we propose to
utilize an interactive SDS loss to efficiently optimize the
Gaussian blobs. First, observing that the user may use part-
level interactions in most cases, we do not need to render
the whole view of the object. Instead, we adopt an adap-
tive camera zoom-in strategy by putting the camera near
the modified region and computing the SDS loss using lo-
5003
cal renderings and user-modified text prompts (Sec. 3.1.4),
resulting in significantly improved optimization efficiency.
Furthermore, since we can interact with the entire genera-
tion process, we make the denoising step tin the SDS loss
adjustable for the user to adapt to different stages. For ex-
ample, in the early stage with noisier shapes, tcan be set
to a large number (e.g., 0.98), while when we aim to fine-
tune some parts, tcan be set to a small number (e.g., 0.3) to
avoid drastic changes.
3.2. Refinement with InstantNGP
Once obtaining the generated Gaussian blobs Ein Stage-I,
we can first convert it to the InstantNGP [23] representation
by a NeRF distillation operation Sec. 3.2.1 and then use the
Hash Refinement module Sec. 3.2.2 for further fine-tuning.
3.2.1 NeRF Distillation
Different from the reconstruction task, we find that the
Gaussian blobs cannot generate fine-grained 3D objects un-
der the supervision of the SDS loss, often with long trip
noises and artifacts. We attribute this to the unstable opti-
mization process for independent Gaussian blobs when su-
pervised by unstable SDS losses. In addition, extracting
meshes or other geometries from Gaussian Splatting is still
unsolved. Therefore, to further improve the quality and ex-
tract geometry, we convert the Gaussian blobs into the In-
stantNGP representation. Specifically, we adopt a simple
yet effective distillation approach by supervising random
renderings from the InstantNGP with the images rendered
by Gaussian Splatting:
Ldistill=1
MX
c∼C(θ,ϕ)∥V(F, c)− R(E, c)∥1, (7)
where cis the camera pose derived from a predefined pose
distribution C(θ, ϕ);Ris the Gaussian Splatting rendering
process; VandFare the volumetric rendering and parame-
ters of InstantNGP, respectively.
3.2.2 Interactive Hash Refinement
Upon obtaining the converted InstantNGP representation
I={F,H}, where H={Hk}L
k=1contains L-level hash
tables and F={Fk}L
k=1contains corresponding L-level
features, we aim to interactively refine Ito further improve
the generation quality. To achieve this, a simple way is to
select the unsatisfying regions and put the camera near these
areas for further refinement like what we did in Sec. 3.1.4.
However, unlike Gaussian Splatting, which can grow new
blobs with infinite resolutions and numbers, the informative
features stored in InstantNGP have limited capacity (e.g., fi-
nite grid resolution and hash table length), leading to refine-
ment bottleneck. Furthermore, InstantNGP does not handlehash conflicts so that different parts may enjoy the same
features, resulting in the degeneration of other parts when
fine-tuning one part.
To handle these problems, we propose an Interactive
Hash Refinement module, which fixes the original Instant-
NGP and adds new learnable residual features to the inter-
ested regions, overcoming the hash conflict problem by in-
troducing part-specific hash mapping. The Interactive Hash-
Refinement strategy (i) adaptively adds informative features
to different regions. For example, we can add hash tables
with more levels and features for highly complex regions,
while adding fewer residuals in regions that only need to re-
move artifacts, and the strategy can also (ii) make the model
focus on worse regions while freezing the satisfying parts
without introducing degeneration.
Specifically, we first extract the binary occupancy grids
Owith a resolution of 32×32×32from the converted
I. Given the interested region Qdefined by a center oand
radius r:
Q=
p|∥p−o∥2≤r,∀p∈R3	
. (8)
We intersect QandOto obtain a part occupancy region
Opart=O∩Q. Then we divide Opartto multi-resolution
grids and establish a part-specific multi-level hash table set
Hpartand learnable features Fpart, mapping solely from the
local region to local features, avoiding sharing information
with other object parts:
fk=(
Hpart
k(p, Fpart
k), p∈Opart
0, p / ∈Opart, (9)
where fkdenotes the mapped features at level kat position
p. Subsequently, we introduce new lightweight MLPs to
convert newly added features to residual densities and col-
ors to influence the final renderings. After that, we can use
the interactive SDS loss to optimize the local regions as dis-
cussed in Sec. 3.1.5. .
4. Experiment
4.1. Implementation Details
We build Interactive3D in two stages with different 3D
representations. We follow [4] to implement the Gaussian
Splatting-based optimization in Stage I and follow [23] to
implement InstantNGP-based refinement in Stage II. We
implement the Hash Refinement module in parallel with
CUDA to improve efficiency. The general training step for
one object is 20k (10k for Stage I and 10k for Stage II). We
conduct our experiments on NVIDIA A100 GPUs. More
details can be found in the supplementary materials.
4.2. Qualitative Results
We show some 3D generation results from Interactive3D as
well as the interaction process in Fig. 6, Fig. 11, and Fig. 4
5004
(a) w / o Interactive Hash Refinement (b) w / o Part-specific Hash Mapping (c) OursFigure 4. The effectiveness of Interactive Hash Refinement. (a) Results with the original hash table length and resolution from InstantNGP.
(b) Results with more refined hash tables and features while without part-specific hash mapping. (c) Results with complete Interactive
Hash Refinement module.
after 1000 stepsafter 1000 steps
“A robot weapon arm”“A robot hand”
(a) w/o interactive SDS loss(b) w/ interactive SDS loss
Figure 5. Ablation study of the proposed Interactive SDS loss.
(c). As shown in the first row of Fig. 6, we can interrupt
the generation process and change the pose of the generated
human, and then continue to optimize. It can be observed
that the standing 3D man is smoothly converted to a kick-
dancing man by directly modifying the Gaussian blobs. The
dancing man is then optimized by a few steps to fix some
artifacts and holes after the interaction, and it is converted to
InstantNGP for further refinement. Compared with the re-
sults directly generated from current state-of-the-art meth-
ods, our generated objects are more controllable with better
geometry and texture. For instance, ProlificDreamer has to-
tally wrong geometry and mismatches the text prompt. In
the second row of Fig. 6, we rigidly drag the head of Tyran-
nosaurus Rex from looking forward to the right direction,
achieving a DragGAN [26]-style interaction while in 3D.
In Fig. 4 (c), we show that by applying our Interactive Hash
Refinement module to a coarse 3D object, a Gundam robot
in this case, we can achieve significant textural and geomet-Table 1. Quantitative comparison on the CLIP R-Precision [10].
Method R-Precision Average Time
DreamFusion [27] 0.67 1.1h
ProlificDreamer [35] 0.83 3.4h
Ours 0.94 50min
ric improvements. It is noteworthy that the interactions can
be combined in one generation process as shown in Fig. 1.
More results can be found in the supplementary material.
4.3. Quantitative Results
Following [10], we use the CLIP R-Precision to quantita-
tively evaluate our generated results in Tab. 1. We use 50
prompts derived from Cap3D [18] and compare the results
with other methods. Interactive3D achieves the highest
CLIP R-Precision by interactive generation, which demon-
strates the strong controllability of our method. Meanwhile,
we can achieve highly efficient 3D generation, because of
the following two reasons: (i) we adopt the fast Gaussian
Splatting in Stage I and utilize its results to initialize the op-
timization in Stage II which speeds up the convergence; (ii)
the interactions are incorporated into some checkpoint of
the original optimization process without the need for extra
training steps, we achieve efficient 3D generation.
4.4. Ablation Studies
Now, we investigate the impact of the Rigid Constraint loss,
the Interactive SDS loss, and the Hash Refinement Module.
More ablations can be found in the supplementary material.
5005
“A black man in a shirt and blue and yellow striped pants dancing tap dance.”
Stage II Stage IStage I Stage II
(a) Ours (b) ProlificDreamer“A Tyrannosaurus Rex with stripes on its body.”Figure 6. Qualitative results of the rigid dragging operation, demonstrating the effectiveness and controllability of user interactions.
(a) original (b) w / o rigid constraint(c) w / rigid constraint
Figure 7. Ablation study of the proposed Rigid Constraint loss.
Effect of Rigid Constraint Loss As shown in Fig. 7, the
Rigid Constraint loss plays a vital role in ensuring the lo-
cal structure unchanged during the dragging. With this con-
straint, the local structure can be well maintained during the
dragging while eliminating this loss leads to deformation.
Effect of Interactive SDS Loss We ablate the Interac-
tive SDS loss in Fig. 5. Given a robot in the representa-
tion of Gaussian blobs, we aim to change its original arm
to other formats. The results show that we achieve better-
modified results by using the Interactive SDS loss after the
same number of training steps compared with the baseline.
Effect of Interactive Hash Refinement Module We ab-
late the Interactive Hash Refinement Module and the ef-
fectiveness of part-specific hash mapping in Fig. 4. Refer
to Fig. 4 (a), it shows that the generation quality is blurry
and lacks details with only the original SDS optimization,
and the details and artifacts cannot be improved even withextremely long optimization steps (e.g., 100k training iter-
ations). The integration of the proposed interactive refine-
ment enhances the realism of models by correcting unrea-
sonable components, such as the robot head in Fig. 4 (b).
The proposed part-specific hash mapping further advances
the texture and geometric precision of the model dramati-
cally, as shown in Fig. 4 (c). Furthermore, the part-specific
hash mapping is also important to avoid region conflicts,
leading to more fine-grained generation results.
5. Conclusion
In this paper, we proposed Interactive3D , a novel frame-
work for controllable and high-quality 3D generation. In-
teractive3D works in two main stages, using different 3D
representations. In Stage I, we use Gaussian Splatting rep-
resentation which allows users to flexibly change the model
by adding or removing parts, transforming shapes, and mak-
ing semantic editing. In Stage II, we first convert the Gaus-
sian blobs to InstantNGP by fast NeRF distillation, and then
propose a novel Interactive Hash Refinement to further im-
prove the quality and extract 3D geometry.
Acknowledgments: This work was supported in part by
Research Grants Council (RGC) of the Hong Kong SAR un-
der grant No. 26202321, HKUST startup fund No. R9253,
and CUHK Direct Grants (RCFUS) No. 4055189. We also
gratefully acknowledge the support of SenseTime.
5006
References
[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and
Leonidas Guibas. Learning representations and generative
models for 3d point clouds. In ICML , 2018. 3
[2] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-
tasia3d: Disentangling geometry and appearance for high-
quality text-to-3d content creation. In ICCV , 2023. 3
[3] Zhiqin Chen and Hao Zhang. Learning implicit fields for
generative shape modeling. In CVPR , 2019. 3
[4] Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3d using
gaussian splatting. arXiv preprint arXiv:2309.16585 , 2023.
5, 6, 2
[5] Matheus Gadelha, Subhransu Maji, and Rui Wang. 3d shape
induction from 2d views of multiple objects. In 3DV, 2017.
3
[6] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,
Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja
Fidler. Get3d: A generative model of high quality 3d tex-
tured shapes learned from images. NeurIPS , 2022. 3
[7] Philipp Henzler, Niloy J Mitra, and Tobias Ritschel. Escap-
ing plato’s cave: 3d shape from adversarial rendering. In
ICCV , 2019. 3
[8] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. NeurIPS , 2020. 3
[9] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter
Abbeel, and Ben Poole. Zero-shot text-guided object gen-
eration with dream fields. In CVPR , 2022. 3
[10] Heewoo Jun and Alex Nichol. Shap-e: Generat-
ing conditional 3d implicit functions. arXiv preprint
arXiv:2305.02463 , 2023. 2, 3, 7
[11] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk ¨uhler,
and George Drettakis. 3d gaussian splatting for real-time
radiance field rendering. ACM Transactions on Graphics , 42
(4):1–14, 2023. 2, 3, 5
[12] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. In ICCV , 2023. 4
[13] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,
Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution
text-to-3d content creation. In CVPR , 2023. 3
[14] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund
Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single
image to 3d mesh in 45 seconds without per-shape optimiza-
tion. NeurIPS , 2024. 3
[15] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-
makov, Sergey Zakharov, and Carl V ondrick. Zero-1-to-3:
Zero-shot one image to 3d object. In ICCV , 2023. 3
[16] Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and
Wenping Wang. Sparseneus: Fast generalizable neural sur-
face reconstruction from sparse views. In ECCV , 2022. 3
[17] Sebastian Lunz, Yingzhen Li, Andrew Fitzgibbon, and
Nate Kushman. Inverse graphics gan: Learning to gener-
ate 3d shapes from unstructured 2d data. arXiv preprint
arXiv:2002.12674 , 2020. 3[18] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin
Johnson. Scalable 3d captioning with pretrained models.
NeurIPS , 2024. 7
[19] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
bastian Nowozin, and Andreas Geiger. Occupancy networks:
Learning 3d reconstruction in function space. In CVPR ,
2019. 3
[20] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and
Daniel Cohen-Or. Latent-nerf for shape-guided generation of
3d shapes and textures. In CVPR , 2023. 2, 3
[21] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. Communications of the ACM , 65(1):99–106, 2021. 2,
3, 4
[22] Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka,
Niloy Mitra, and Leonidas J Guibas. Structurenet: Hierarchi-
cal graph networks for 3d shape generation. arXiv preprint
arXiv:1908.00575 , 2019. 3
[23] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a multires-
olution hash encoding. ACM Transactions on Graphics , 41
(4):102:1–102:15, 2022. 2, 3, 6
[24] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741 , 2021. 3
[25] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela
Mishkin, and Mark Chen. Point-e: A system for generat-
ing 3d point clouds from complex prompts. arXiv preprint
arXiv:2212.08751 , 2022. 2, 3
[26] Xingang Pan, Ayush Tewari, Thomas Leimk ¨uhler, Lingjie
Liu, Abhimitra Meka, and Christian Theobalt. Drag your
gan: Interactive point-based manipulation on the generative
image manifold. In ACM SIGGRAPH 2023 Conference Pro-
ceedings , 2023. 2, 4, 5, 7
[27] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR ,
2023. 2, 3, 5, 7
[28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022. 3
[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , 2022. 2
[30] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. NeurIPS , 2022. 2, 3
[31] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:
Open dataset of clip-filtered 400 million image-text pairs.
arXiv preprint arXiv:2111.02114 , 2021. 2
5007
[32] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and
Sanja Fidler. Deep marching tetrahedra: a hybrid representa-
tion for high-resolution 3d shape synthesis. NeurIPS , 2021.
3
[33] Edward J Smith and David Meger. Improved adversarial sys-
tems for 3d object generation and reconstruction. In CoRL ,
2017. 3
[34] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,
and Greg Shakhnarovich. Score jacobian chaining: Lifting
pretrained 2d diffusion models for 3d generation. In CVPR ,
2023. 3
[35] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan
Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and
diverse text-to-3d generation with variational score distilla-
tion. NeurIPS , 2024. 2, 3, 7
[36] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and
Josh Tenenbaum. Learning a probabilistic latent space of ob-
ject shapes via 3d generative-adversarial modeling. NeurIPS ,
2016. 3
[37] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge
Belongie, and Bharath Hariharan. Pointflow: 3d point cloud
generation with continuous normalizing flows. In ICCV ,
2019. 3
[38] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
ICCV , 2023. 2
[39] Yuxuan Zhang, Wenzheng Chen, Huan Ling, Jun Gao,
Yinan Zhang, Antonio Torralba, and Sanja Fidler. Im-
age gans meet differentiable rendering for inverse graph-
ics and interpretable 3d neural rendering. arXiv preprint
arXiv:2010.09125 , 2020. 3
5008
