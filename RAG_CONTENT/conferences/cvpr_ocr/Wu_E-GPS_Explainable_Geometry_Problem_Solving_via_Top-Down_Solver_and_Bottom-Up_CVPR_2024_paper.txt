E-GPS: Explainable Geometry Problem Solving via Top-Down Solver and
Bottom-Up Generator
Wenjun Wu1,3, Lingling Zhang1,3*, Jun Liu1,3, Xi Tang1,3, Yaxian Wang1,3
Shaowei Wang1,3, Qianying Wang2
1School of Computer Science and Technology, Xi’an Jiaotong University, China2Lenovo Research
3Key Laboratory of Intelligent Networks and Network Security, Ministry of Education, Xi’an, Shaanxi, China
Abstract
Geometry Problem Solving has drawn growing attention
recently due to its application prospects in intelligent ed-
ucation field. However, existing methods are still inade-
quate to meet the needs of practical application, suffering
from the following limitations: 1) explainability is not en-
sured which is essential in real teaching scenarios; 2) the
small scale and incomplete annotation of existing datasets
make it hard for model to comprehend geometric knowl-
edge. To tackle the above problems, we propose a novel
method called Explainable Geometry Problem Solving (E-
GPS). E-GPS first parses the geometric diagram and prob-
lem text into unified formal language representations. Then,
the answer and explainable reasoning and solving steps are
obtained by a Top-Down Problem Solver (TD-PS), which
innovatively solves the problem from the target and focuses
on what is needed. To alleviate the data issues, a Bottom-
Up Problem Generator (BU-PG) is devised to augment the
data set with various well-annotated constructed geome-
try problems. It enables us to train an enhanced theorem
predictor with a better grasp of theorem knowledge, which
further improves the efficiency of TD-PS. Extensive experi-
ments demonstrate that E-GPS maintains comparable solv-
ing performances with fewer steps and provides outstanding
explainability.
1. Introduction
Geometry Problem Solving (GPS) aims to obtain the an-
swer of problem based on the given geometric diagram and
textual problem description. It has drawn growing atten-
tion recently [4, 15, 22, 31] due to its application prospects
in intelligent education field in high schools [2, 20]. Dif-
ferent from general question answering (QA) tasks, GPS
requires the model to possess the abilities of symbolic ab-
straction, logical reasoning and algebraic calculation simul-
*Corresponding author. Email: zhanglling@xjtu.edu.cn
Figure 1. Output examples of two mainstream GPS methods. Case
1 and case 2 are chosen from Inter-GPS [22] and NGS [4], respec-
tively. Content with red background is seen as inexplicable.
taneously [6, 24], making it a challenging task even for large
multimodal models (LMMs) like GPT-4V [37]. Therefore,
recent works attempt to combine the procedural power of
symbolic models with the general power of neural models.
Among these, symbolic-based approaches [22, 25, 29, 32]
first parse the geometric diagram and problem text into for-
mal language representations, and then continuously pre-
dict and apply predefined theorem rules to obtain the final
answer. Neural-based approaches [4, 5, 40] tend to trans-
fer the original problem into multi-modal features, and feed
them into generative models to acquire an executable pro-
gram sequence for an answer. However, they both suffer
from the following two limitations which hinder their ap-
plication in practical scenarios.
At methodological level, previous methods are unable to
ensure basic explainability which is essential. As shown in
Fig. 1, examples of two mainstream GPS methods are listed.
The first one is a typical symbolic-based model Inter-GPS
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
13828
Dataset ScaleAnnotation
Solution Theorem Parsing
GEOS [32] 186
GEOS++ [30] 1,406
GEOS-OS [29] 2,235
Geometry3K [22] 3,002 ✓ ✓
GeoQA [4] 5,010 ✓
Table 1. Information of popular GPS datasets. Common basic an-
notation such as problem text and answer is not listed for brief.
Here, Scale refers to the number of problem pairs, Solution repre-
sents the appropriate solving steps to the answer, Theorem refers
to the explicit theorem knowledge defined in mathematical rules,
andParsing denotes problem parsing results in formal languages.
[22] that generates a theorem application sequence. It can
be seen that such solution is actually redundant and vague.
Many theorems ( e.g. 7: radius equal theorem) are unhelpful
to solving the problem but included in the sequence, which
is mostly caused by the high-level reasoning gulf between
the known information and final problem target. Moreover,
no explicit procedures on specific primitives are given for
each theorem application. As for the second neural-based
model NGS [4], how the diagram and text are parsed and
used for solving the problem is a black box. Furthermore,
the generated sequence is unreliable due to the inherent is-
sues of generative models, which makes it even more diffi-
cult to conclude precise mathematical theorem knowledge.
These drawbacks greatly reduce the interpretability, bring-
ing much confusion to the learners. Therefore, GPS models
should generate not only the correct answer, but also the
explainable reasoning and solving steps as supporting evi-
dence. It is an essential prerequisite for the employment of
solver models in real application scenarios such as teaching
assistance.
At data level, the small scale and incomplete annota-
tions of existing datasets make neural models struggle with
learning geometric knowledge. Compared with regular QA
datasets, the construction of GPS datasets is much more dif-
ficult due to the shortage of samples and tremendous labour
power demanded for high-standard annotation works. This
leads to the limited scale and incomplete annotation of ex-
isting GPS datasets, as recorded in Tab. 1. For instance,
GEOS [32] and GEOS++ [30] contains only 186 and 1,406
problems, respectively. The scale is expanded by Geome-
try3K [22] and GeoQA [4], but merely to 3,002 and 5,010
problems with annotations that are still unsatisfactory. The
appropriate problem solving sequence is absent in Geom-
etry3K, while the illustration of geometric theorems is not
considered in GeoQA. The data issue causes great troubles
for training of data-driven models and weakens their mas-
tery of geometric knowledge. It further reduces their perfor-
mances in tasks like theorem prediction and solution pro-gram generation, hence limiting the application potential of
GPS models.
To overcome the above two problems, we propose a new
method named as Explainable Geometry Problem Solving
(E-GPS). It first parses the geometric diagram and problem
text into unified formal language representations. The prob-
lem target ( e.g.Find(Line(AB)) ) is also specified dur-
ing parsing. Inspired by how human experts solve geometry
problems, starting from the target itself to seek for needed
conditions could largely narrow the reasoning gap between
the two, compared to starting from the known conditions as
previous methods did [22, 25, 32]. That is, we should focus
on what is needed instead of what could be used. To this
end, the explainable Top-Down Problem Solver (TD-PS) is
designed to transfer the task into tree-searching in a top-
down manner for solutions with two mechanisms: 1) target
decomposition decomposes the parent target into multiple
child targets according to premise conditions of a theorem
rule; 2) condition processing is responsible for checking
whether needed conditions are acquirable. In this way, all
conditions are used for a purpose, which ensures the ex-
plainability of final generated reasoning and solving steps.
To improve the efficiency of TD-PS, neural-guided theo-
rem prediction is considered to narrow the search space,
which also faces the scale and annotation issues of exist-
ing datasets. Interestingly, we find that values generated by
the forward theorem application of symbolic models could
be treated as answers with known solutions. Hence, the
Bottom-Up Problem Generator (BU-PG) is devised to con-
tinuously apply theorems on the original problem and con-
vert the geometric primitives with newly obtained values
into a series of new problem targets. It automatically aug-
ments the original data set with a large quantity of well-
annotated constructed geometry problems, making it possi-
ble for us to train an enhanced theorem predictor for TD-PS.
Our contributions can be summarized as three-folds:
• We introduce a novel top-down problem solver that gen-
erates the answer and explainable reasoning and solving
steps. To the best of our knowledge, it is the first work
that focuses on what is needed for the problem target.
• We propose a bottom-up problem generator that is able to
augment the data set with a large quantity of constructed
geometry problems with explicit solutions.
• Experiment results demonstrate that our method is able to
maintain comparable performances with fewer steps and
provide outstanding explainability.
2. Related Works
2.1. Geometry Problem Solving
Developing geometry problem solving systems has been
studied for long [11], but mostly on geometry theorem prov-
ing [7, 10, 36, 38] and problem formalization [9]. Until re-
13829
cently, research on GPS has been further promoted and can
be roughly categorized into two types: symbolic-based and
neural-based. Symbolic-based methods continuously up-
date the parsed conditions until the problem is solved. Some
work [32] focuses on selecting a most related relation set to
solve an optimization problem. Others use extracted the-
orem knowledge which is represented as horn clause rules
[30, 31] or predefined theorem rules [22, 25] to deduce new
conditions. There are also works [12, 17] paying attention
to the optimization of parsed formal language information.
Neural-based methods [4, 5] learn implicit patterns and gen-
erate an executable program to obtain a final answer. Some
works [18, 40] realize that such generative approach relies
heavily on the features extracted from raw problems, and
improve the fusion of multi-modal knowledge.
However, these works neglect the high-level reasoning
gap between the known conditions and the problem target.
It makes their models prone to introduce incorrect or re-
dundant information in the solution, weakening the explain-
ability. Moreover, for neural-based methods, the reasoning
from features to executable programs is still a black box.
2.2. Explainable Math Problem Solving
While the explainability of geometry problem solvers has
not yet been explored much, there are works investigat-
ing that in general math problem solving field. Some prior
works generate interpretable solutions of math problems by
intermediate structural results [1, 14, 16, 27, 34, 35]. Other
works propose to perform logical reasoning based on sym-
bolic language parsed from the problem [23, 28], provid-
ing a reference for the establishment of symbolic geometry
problem solving systems. Moreover, the ideas of structured
image parsing [13, 33, 41] have greatly inspired existing
methods [22, 32] on the interpretability of information ex-
traction from geometry problems. However, the explain-
ability of solutions generated by GPS solver models are still
in need of further research.
3. Methodology
In a geometry problem (D, T),Dis a geometric diagram
andTis a textual problem description which includes the
problem target ( e.g.Find AB? ). With given (D, T), ge-
ometry problem solving aims at obtaining the correct an-
swer a∗of target. For more explainable GPS, the corre-
sponding solution steps Sshould also be generated. To
achieve this goal, we design E-GPS which is illustrated in
Fig. 2. Specifically, E-GPS first transfers the raw problem
into unified formal language propositions by two parsers in
Sec. 3.1. Then the Top-Down Problem Solver is introduced
in Sec. 3.2, which generates the answer a∗as well as ex-
plainable reasoning and solving steps S. In Sec. 3.3, how
the Bottom-Up Problem Generator constructs augmented
problems is described in details. The augmented data setis then used for training an enhanced theorem predictor in
Sec. 3.4.
3.1. Geometry Problem Parsing
The content integration of geometric diagram Dand prob-
lem text Tis crucial for understanding the entire geometry
problem but faces a great modality gulf. Inspired by pre-
vious works [22, 25, 32], we utilize a diagram parser and
a text parser to transfer the modal-specific knowledge into
unified formal language representations automatically.
Diagram Parser. Following previous work [25], the ma-
ture neural-based model PGDPNet [39] is utilized to detect
the geometric primitives ( i.e.line, angle andarc) as
it has been well solved. The relations between these ge-
ometric primitives, symbols and texts in the diagram are
further generated as propositions in formal language. We
define the proposition set of DasPD, which includes the
relations between geometric primitives and values. For ex-
ample, Triangle(ABC) is a proposition describing the
combination of Line(AB) ,Line(BC) andLine(AC) .
AndEquals(Line(AB),5) is also a proposition which
establishes equation between primitive and value.
Text Parser. For more precise parsing results, we use rule-
based text parser with regular expressions [22, 32] to trans-
late the problem text. It automatically parses the content
into primitives, values and their relations. Similarly, the
text parser also generates a proposition set PT. Normally,
the problem target ( e.g.Find(Line(AB)) ) is also parsed
from problem text, defined as t∗.
Therefore, it is feasible for us to combine the visual and
textual information into proposition set P=
PD;PT	
.
We further integrate offline programs into a predefined the-
orem knowledge base set KB={k1, k2,···, kN}the same
as in [22], where each conditional rule kistands for a geom-
etry theorem and builds equations between primitives and
values. For instance, if lengths of two right sides of a right
triangle are known, length of its hypotenuse can be cal-
culated through an equation according to the Pythagorean
Theorem. Specifically, the proposition set Pcan be seen as
updated by theorem ionce the needed primitives in premise
qiofkimeet conditions and new proposition pis generated:
p←ki(qi,P), ki∈ KB , (1)
P ←p∨ P. (2)
It is noteworthy that the application of kiintroduces new
equations between primitives and values. We encapsulate
the above process into the following function:
a,P ← SolveEquations (P, t, ki). (3)
Hence, the above function always performs three tasks: 1)
apply theorem kiif it’s given; 2) solve the established equa-
tions; 3) calculate a numerical answer aof given target t.
13830
Figure 2. The overall framework of E-GPS which includes three parts: 1) the proposition set in formal language is generated by the diagram
parser and text parser; 2) TD-PS generates the answer of the problem target, as well as explainable reasoning and solving steps; 3) BU-PG
automatically augments the data set with constructed geometry problems, which is served for training the enhanced theorem predictor.
3.2. Top-Down Problem Solver
Previous works mostly attempted to solve geometry prob-
lems by starting from the known information ( e.g.Pin this
work). However, the natural reasoning gap between it and
the final target is ignored, which inevitably caused the in-
troduction of redundant or wrong steps. We find that the
gap could be narrowed if we start from the target and focus
on what is actually needed for solving it rather than what
could be used. To achieve this, the explainable Top-Down
Problem Solver (TD-PS) first reasons for the solution based
on the proposition set P, knowledge base set KBand target
t∗in reasoning stage, and outputs the answer a∗along with
explainable reasoning and solving steps Sin solving stage.
3.2.1 Reasoning Stage
Two collaborative mechanisms are implemented to first rea-
son for the solution, namely target decomposition andcon-
dition processing .
Target Decomposition. For geometry problems that re-
quire multi-step theorem applications, directly calculating
the answer of final target is not ideal. Nevertheless, once
the premise condition of a theorem is met, it is most likely
that applying this theorem would lead to an answer. Take
the geometry problem in Fig. 2 as an example, if the val-
ues of Line(BC) ,Angle(ACB) andAngle(BAC) are
known, obtaining the value of Line(AB) is promising ac-
cording to Sine Theorem. Hence, we transfer the task offinding the answer of tinto finding the values for those
primitives needed in premise condition of ki, which we call
a decomposition of target. For instance, to solve parent tar-
getFind(Line(AB)) , finding the value of Line(BC)
is transferred into Find(Line(BC)) as one of the child
targets. These child targets are further packed into a target
groupT∧
1and they should all be solved for obtaining the an-
swer of parent target. Such decomposition is actually the re-
verse process of theorem application. Moreover, alternative
decompositions are allowed such as target group T∧
2which
also leads to a solution according to the same theorem:
(Find(Line(BD)),Find(Angle(ADB)),Find(
Angle(BAD)))∧
2. Therefore, the process of target de-
composition is defined as follows:
(T∧
1,···,T∧
n)∨←TargetDecompose (t, ki),(4)
where symbol ∧represents all child targets within the group
need to have solutions simultaneously and symbol ∨de-
notes that at least one target group needs to have a solution.
The theorem ki(ki∈Kt)is selected by the following func-
tion which will be further discussed in Sec. 3.4:
Kt←TheoremPrediction (P,KB, t). (5)
Condition Processing. The logical relationship between
parent target and child targets needs to be handled carefully
to ensure the feasibility of solutions. For example in Fig. 2,
Find(Line(AC)) is solvable by applying Cosine The-
orem on Line(BC) ,Angle(ACB) , and Angle(BAC) ,
13831
where all values of primitives are known in P. We have
summarized the processing logic into the following three
rules. For parent target tand its decomposed child targets:
•treturns True if it has known numerical answer or is solv-
able by equations;
•T∧returns True if all child targets in it return True, oth-
erwise returns False;
•treturns True if any target group T∧returns True, other-
wise returns False.
In this way, the original task of solving problem target t∗is
transferred into tree-searching in a top-down manner, whose
goal is to find a path where all values of needed primitives
are known in Por can be obtained by theorem applications.
Such process is defined as reasoning. For comprehensive
and explicit explanations, child targets T∧, theorem kiand
parent target tare recorded as reasoning steps in S:
S ← S .Append (T∧, ki, t). (6)
3.2.2 Solving Stage
Once the reasoning steps of problem target t∗is confirmed,
the solving orders of theorem application and needed primi-
tives are also determined. Function SolveEquations()
is then called sequentially by the theorem executor accord-
ing to solving steps to obtain final answer a∗, which is the
solving stage. The whole process of TD-PS is displayed in
details in Algorithm 1. We claim that the explainability of
TD-PS is reflected in the following aspects:
• TD-PS generates not only the answer, but also appropriate
reasoning and solving steps Swithout redundancy, which
provides human readable solutions;
• Each step in the reasoning and solving stage has specific
target and required primitives as supporting evidences;
• Each theorem is defined by rules, which shows clear
mathematical procedures of how the theorem is applied;
• The motivation and algorithm design of TD-PS is explain-
able. It imitates the process of human solving geometry
problems to first reason and then solve.
3.3. Bottom-Up Problem Generator
Existing GPS datasets are insufficient in both quantity of
samples and annotations of solving steps, posing difficul-
ties for neural models to find patterns of complex theo-
rem applications. To alleviate this issue, the Bottom- Up
Problem Generator (BU-PG) is devised to automatically
augment any geometry problem (D, T)parsed in formal
languages with a series of constructed geometry problems
{(D, Tc
1,Sc
1),(D, Tc
2,Sc
2),···,(D, Tc
L,Sc
L)}. Each text
Tc
iincludes a new problem target tc
iand could be solved
by corresponding solving steps Sc
i.
As introduced in Sec. 3.1, KBis defined as explicit math-
ematical rules of geometry theorems. Theoretically, contin-
uously applying KBonPcould acquire many values of un-Algorithm 1 Explainable Top-Down Problem Solver
Input: Proposition set P, knowledge base KB, target t∗
Output: Numeric answer a∗, explainable solving steps S
1function Main( P,KB, t∗):
2 global KB,Pand initialize a∗=∅,S=∅, time = 0;
3 while a∗=∅andtime within restrictions do
4 F,S ←TargetDecomposition( t∗,S);
5 ifFis True then
6 forki,tiinSdo
7 ai,P ← SolveEquations( P, ti, ki);
8 return a∗,S
9 return None
10function TargetDecomposition( t,S):
11 a,P ← SolveEquations( P, t);
12 ifa̸=∅then
13 S ← S .Append( t);
14 return True,S
15 Kt←TheoremPrediction( P,KB, t);
16 forkt
iinKtdo
17 T∨←TargetDecompose( t,kt
i);
18 F,S,T∧←ConditionProcessing( T∨,S);
19 ifFis True then
20 S ← S .Append( T∧,kt
i,t);
21 return True,S
22 return False,S
23function ConditionProcessing( T,S):
24 forT∧inTdo
25 sethasSolution as True;
26 fortarget tinT∧do
27 F,S ←TargetDecomposition( t,S);
28 ifFis False then
29 S.Backward() ;
30 sethasSolution as False;
31 break
32 ifhasSolution is True then
33 return True,S,T∧
34 return False,S, None
known primitives. Therefore, given a sequence of theorems,
it is possible for the theorem executor to apply theorems one
after another and eventually obtain a numerical value of cer-
tain primitive. Such primitive is then transferred as a new
problem target tc
i, where the value can be obtained by solu-
tion path Sc
i. Similar to TD-PS, BU-PG is able to record ex-
plicit primitives and theorem in the solving steps Sc
i, which
ensures the comprehensiveness of annotation. For genera-
tion of more diverse geometry problems, we define certain
terms to control the target type ( e.g. line and angle), com-
13832
plexity ( e.g. multi-target) and difficulty ( e.g. multi-theorem
application) of the constructed geometry problems. To en-
sure that the solution is reasonable and non-redundant, ev-
ery latter theorem to be applied should cover at least one
primitive value obtained by the former theorem for prob-
lems whose difficulty is larger than 1.
With any version of KBand GPS datasets in formal lan-
guages, BU-PG is able to re-generate a new one with a large
quantity of samples. Unlike the other augmentation meth-
ods such as symbol modification and diagram flip [40], BU-
PG directly augments the multi-step theorem application
samples for high-level reasoning learning, which shows un-
deniable value to many downstream tasks. In this work, the
augmented data set aug-Geo3K is utilized to train an en-
hanced theorem predictor, which will be described next.
3.4. Enhanced Theorem Predictor
For symbolic-based models, one of the key challenges is to
predict the theorems needed for obtaining the correct an-
swer. A simple implementation is traversing the theorems
inKBuntil the problem is solved. However, when the scale
ofKBbecomes large for more complicated geometry prob-
lems in the future, such traversal method might obviously
reduce the efficiency of the solver model. Inspired by pre-
vious works [3, 21, 22], applying neural guided search to
predict theorems and speed up the search process of TD-PS
is feasible. Given the propositions P, target tandKB, the
theorem predictor aims at predicting the needed theorems
Kt={kt
1,···, kt
M}, as introduced in Eq. (5). In prac-
tice, a token sequence Et={e1,···, eM}is generated in-
stead, where eirefers to the index token of kt
iinKB. The
mature transformer-based model [19] is used to accomplish
such sequence-to-sequence (Seq2Seq) task. The optimiza-
tion goal is defined as a negative log-likelihood loss:
LM−TP=−MX
i=1logθ(ei|e1, e2,···, ei−1). (7)
We also train the model on one-step prediction (M= 1) ,
considering the difficulty of generating a multi-step se-
quence which involves high-level reasoning and intermedi-
ate values. However, sufficient well-annotated samples are
required for neural models to learn the logical correlation
between propositions and target. In previous work [22], the
pseudo-optimal theorem application sequences were con-
structed as ground-truth by random sampling in KB1, which
introduces redundant theorems. Instead, we utilize our aux-
iliary data set aug-Geo3K generated by BU-PG to train
the theorem predictor. In details, the constructed geometry
problems whose difficulty are labeled as 1 and M(M≥1)
are used as training samples for one-step and multi-step pre-
diction, respectively. In this way, both quantity and quality
of samples for theorem prediction training are guaranteed,
benefited from the generation mechanism of BU-PG.4. Experiments
4.1. Datasets
For geometry problem solving task, we mainly conduct ex-
periments on Geometry3K [22]. The theorem prediction is
further compared on the augmented data set aug-Geo3K due
to the absence of solution annotation in Geometry3K.
Geometry3K: includes 3,002 geometry problems in total
and is divided into 2,101 for training, 300 for validation and
601 for testing. Each problem is accompanied with a geo-
metric diagram, a problem text and explicit parsing anno-
tations in formal language. Problems in it cover many geo-
metric shapes, such as lines, triangles, circles, quadrilaterals
and other polygons, providing a more realistic benchmark.
aug-Geo3K: is a large scale constructed geometry prob-
lem data set augmented from Geometry3K with KB1, used
for auxiliary training and evaluation. It contains a total of
17,607 problems, where each problem is equipped with a
diagram, a problem text, the answer and corresponding ex-
plicit solutions. The problems are divided into 12,291 for
training, 1,492 for validation and 3,824 for testing accord-
ing to the original data split in Geometry3K.
4.2. Baselines and Evaluation Metrics
Baselines. We mainly compare with the methods on Geom-
etry3K. FiLM [26] is a highly effective model for visual rea-
soning on abstract images that requires multi-step process-
ing abilities. Two improved models, namely FiLM-BERT
[8] and FiLM-BART [19], are also chosen as competitors
which use better encoders. Inter-GPS [22] is a popular ap-
proach that applies predefined theorem rules until the final
problem target is solved. The recent work GeoDRL [25]
incorporates deep reinforcement learning into reasoning for
better theorem prediction based on a larger theorem set.
Metrics. For geometry problem solving, we evaluate the
model performances from two aspects: 1) accuracy: the
answer is considered correct if it is the closest to the
ground-truth; 2) average steps: the average theorem appli-
cation steps of solutions. For theorem prediction, we adopt
R@K, which shows the percentage of geometry problems
whose ground-truth theorem application sequence is con-
tained within the top-K sequences predicted by the model.
4.3. Implementation Details
For fair comparison, the same theorem knowledge base sets
KB1[22] and KB2[25] in previous works are used which
include 17 and 24 commonly used geometry theorems, re-
spectively. The solver is set to search for solutions within
5 steps. In our theorem predictor, the transformer model
is designed with 6 layers, 12 attention heads and the size
of hidden embedding layer is 768. The maximum length
of predicted sequence are set to 20. For generation of aug-
Geo3K, all 17 theorems in KB1are considered and the diffi-
13833
MethodsQuestion Type Geometric ShapeAccuracy StepsMeasure Length Area Ratio Line Triangle Quad Circle Other
Human 53.7 59.3 57.7 42.9 46.7 53.8 68.7 61.7 58.3 56.9 –
Human Expert 89.9 92.0 93.9 66.7 95.9 92.2 90.5 89.9 92.3 90.9 –
FiLM [26] 28.7 32.7 39.6 33.3 33.3 29.2 33.6 30.8 29.6 31.7 –
FiLM-BERT [8] 32.9 33.3 30.2 25.0 32.1 32.3 32.2 34.3 33.3 32.8 –
FiLM-BART [19] 32.1 33.0 35.8 50.0 34.6 32.6 37.1 30.1 37.0 33.0 –
Inter-GPS [22] 59.1 61.7 30.2 50.0 59.3 66.0 52.4 45.5 48.1 57.5 –
Inter-GPS (GT) 83.1 77.9 62.3 75.0 86.4 83.3 77.6 61.5 70.4 78.3 7.10
E-GPS (ours) 76.8 62.6 24.5 75.0 72.8 73.0 55.7 51.5 41.9 64.7 3.49∼4.19
E-GPS (GT) 83.8 80.0 66.7 63.9 87.7 85.3 79.2 65.9 56.8 79.8 3.35∼3.99
GeoDRL∗[25] 75.5 70.5 22.6 83.3 77.8 76.0 62.9 53.8 48.1 68.4 –
GeoDRL∗(GT) 86.5 93.7 75.5 100.0 87.7 93.1 90.2 78.3 77.8 89.4 2.34
E-GPS∗(ours) 78.3 67.2 27.7 72.2 76.1 75.6 59.4 55.0 51.8 67.9 1.63∼2.28
E-GPS∗(GT) 90.4 92.2 73.6 100.0 91.4 93.1 87.9 81.1 75.3 89.8 1.57∼2.18
Table 2. Comparison of geometry problem solving on Geometry3K. Methods with∗mark use the larger theorem base set KB2[25], while
the others use theorem base set KB1[22]. GT refers to using ground-truth parsing results. Best results are in bold.
MethodsOne-Step Multi-SteprSumR@1 R@3 R@5 R@1 R@3 R@5
Inter-GPS 22.1 41.8 59.5 11.8 22.3 31.7 189.2
E-GPS 61.3 82.8 88.9 49.2 72.5 80.3 435.0
Table 3. Comparison of theorem prediction on aug-Geo3K. Best
results are in bold.
culty is set to 1 and 5 for one-step and multi-step prediction,
respectively. The predictor is trained by Adam optimizer
with a 0.01 learning rate and 10 training epochs. Experi-
ments are repeated three times for more confident results.
4.4. Performance Comparison
We first compare the model performances for the regular
geometry problem solving task, and then conduct extra ex-
periments on theorem prediction.
Geometry Problem Solving. The overall accuracy, de-
tailed accuracy rates and average steps are recorded in
Tab. 2. For fair comparison, a numerical interval is recorded
for E-GPS where the actual average steps lies in, in consis-
tency with baselines [22, 25]. The first and second number
represent the average used theorems and the average solving
steps, respectively. We make the following observations:
• E-GPS shows comparable performances with the SOTA
solvers and significantly shortens the steps. Besides the
1.5% improvement of accuracy, E-GPS reduces the steps
from 7.10 in Inter-GPS to 3.35 at minimum, using KB1.
Compared to GeoDRL with KB2, the steps are further
reduced from 2.34 to 1.57 at minimum with a slight gain
of 0.4% on accuracy. The results demonstrate that E-GPS
is a more effective geometry problem solving method.
• The shortening of steps indicates the superiority of E-GPS generating better solutions. It mainly benefits from
the mechanism of E-GPS which ensures the necessity of
each step for better explainability, differed from previ-
ous methods. Moreover, obtaining more simple solutions
would also lead to fewer average steps.
• The performance gain of GeoDRL is largely due to the
introduction of the larger theorem base. Using KB2, E-
GPS also achieves comparable performance, just as com-
paring with Inter-GPS based on KB1. Further, E-GPS is
designed to be easily integrated with future version of KB
without retraining, while it costs greater for the reinforce-
ment learning method in GeoDRL to adjust to a new KB.
Theorem Prediction. It is a crucial step which reflects the
model’s mastery of geometry theorem knowledge but re-
mains challenging. For fair comparison, we choose base-
line Inter-GPS [22] which uses the same theorem predictor
settings as ours. According to Tab. 3, our theorem predic-
tor trained with the help of BU-PG shows excellent perfor-
mances with the best results over all evaluation metrics. On
one-step prediction, E-GPS achieves more than 80% at both
R@3 and R@5, obtaining a margin of 41.0% and 29.4% im-
provement compared to Inter-GPS. On the more challeng-
ing multi-step prediction which involves high-level reason-
ing, the performance gap is further widened . The results
demonstrate the effectiveness of our BU-PG for augment-
ing the original data set, which better supports the neural
models to learn the application of geometric knowledge.
4.5. Ablation Studies and Discussion
Theorem Selection Strategies. To evaluate the efficiency
of using theorem prediction during reasoning, we choose
some basic selection strategies for comparison in Tab. 4. It
shows that our enhanced theorem predictors trained by one-
13834
Strategy AccuracySteps
Application Search
Traversal 77.0 3.49∼4.00 22.3
Random 78.1 3.62∼4.12 16.1
Predict (M-step) 79.6 3.33∼3.96 12.0
Predict (1-step) 79.8 3.35∼3.99 11.5
Table 4. Comparison of different theorem selection strategies in
E-GPS on Geometry3K with KB1. Best results are in bold.
MethodsExplainability
Reasoning Solving Theorem Model
Inter-GPS ✓ ✓
NGS ✓
GeoDRL ✓ ✓ ✓
E-GPS ✓ ✓ ✓ ✓
Table 5. Comparison of explainability with geometry solvers.
step and multi-step prediction both acquire a higher accu-
racy with obviously fewer steps. Compared to the traver-
sal strategy, our predictor (1-step) even shortens the search
steps by nearly half, with a drop of 10.8 steps. Based on
previous experiments in Tab. 3, these results further demon-
strate that our enhanced theorem predictor trained with BU-
PG has a better grasp of geometric knowledge and hence
largely improves the efficiency of the solver by appropriate
theorem prediction.
Explainability. We further compare the explainability with
some baselines in Tab. 5. It can be seen that E-GPS is the
only method that is thoroughly explainable from all aspects.
It is noteworthy that although GeoDRL generates human
readable solutions after post processing, it is unable to avoid
the introduction of redundant steps from the early stage of
reasoning the same as Inter-GPS, which is caused by the
method itself. In contrast, E-GPS ensures the necessity of
each step during reasoning and thereby provides superior
explainability of both reasoning and solving steps.
Case Analysis. We select some representative results for
further analysis. Take case 1 in Fig. 3 as an example, E-GPS
is able to generate the correct answer, as well as detailed
reasoning and solving steps. The appropriate theorem ap-
plication steps and specific primitives are confirmed during
top-down reasoning, and seen as guidance for obtaining the
final answer during bottom-up solving. Nevertheless, there
are also limitations. While E-GPS chooses the Pythagoras
Theorem and Sine Theorem to acquire the length of AC, it
can be solved simply by Cosine Theorem. It is not promis-
ing for E-GPS to generate the shortest solution, which is
achievable but would take longer time. Moreover, its per-
formance is also affected by the parsing results, which is
common in symbolic-based models. It failed to reason from
Figure 3. Some typical cases from E-GPS.
the target in case 2 because the parser is confused about the
shaded region. In case 3, E-GPS is unable to give the final
solution due to the absent information specifying the region
as a trapezoidal or a parallelogram.
5. Conclusion
In this paper, we present the novel method E-GPS, which
includes a top-down solver and a bottom-up generator. The
solver starts from the problem target to reason for solutions,
and generates the final answer with solving steps. The gen-
erator uses predefined theorem rules to augment the data set
with constructed geometry problems, providing sufficient
well-annotated training samples. Experiments demonstrate
that E-GPS maintains comparable performances with fewer
steps and shows superior explainability. In the future, we
will be looking into the way of obtaining the shortest so-
lution of a problem, and constructing a larger theorem rule
base for further performance improvement.
Acknowledgments
This work was supported by National Key Research and
Development Program of China (2022YFC3303600),
National Natural Science Foundation of China (62137002,
62293550, 62293553, 62250009, 62250066, 62176209
and 62106190), “LENOVO-XJTU” Intelligent Industry
Joint Laboratory Project, Natural Science Basic Re-
search Program of Shaanxi (2023-JC-YB-593), the Youth
Innovation Team of Shaanxi Universities, Shaanxi Under-
graduate and Higher Education Teaching Reform Research
Program (Program No.23BY195), Project of China Knowl-
edge Centre for Engineering Science and Technology.
13835
References
[1] Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-
Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.
Mathqa: Towards interpretable math word problem
solving with operation-based formalisms. arXiv preprint
arXiv:1905.13319 , 2019. 3
[2] Richa Bajaj and Vidushi Sharma. Smart education with ar-
tificial intelligence based determination of learning styles.
Procedia computer science , 132:834–842, 2018. 1
[3] Mislav Balunovic, Pavol Bielik, and Martin Vechev. Learn-
ing to solve smt formulas. Advances in Neural Information
Processing Systems , 31, 2018. 6
[4] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang,
Lingbo Liu, Eric P Xing, and Liang Lin. Geoqa: A geometric
question answering benchmark towards multimodal numeri-
cal reasoning. arXiv preprint arXiv:2105.14517 , 2021. 1, 2,
3
[5] Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin,
Chongyu Chen, and Xiaodan Liang. Unigeo: Unifying ge-
ometry logical reasoning via reformulating mathematical ex-
pression. arXiv preprint arXiv:2212.02746 , 2022. 1, 3
[6] Mohan Chinnappan. Schemas and mental models in geom-
etry problem solving. Educational Studies in Mathematics ,
36:201–217, 1998. 1
[7] Shang-Ching Chou, Xiao-Shan Gao, and Jing-Zhong Zhang.
Automated generation of readable proofs with geometric in-
variants: I. multiple and shortest proof generation. Journal
of Automated Reasoning , 17(3):325–347, 1996. 2
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 6, 7
[9] Wenbin Gan and Xinguo Yu. Automatic understanding and
formalization of natural language geometry problems using
syntax-semantics models. International Journal of Inno-
vative Computing, Information and Control , 14(1):83–98,
2018. 2
[10] Wenbin Gan, Xinguo Yu, Ting Zhang, and Mingshu Wang.
Automatically proving plane geometry theorems stated by
text and diagram. International Journal of Pattern Recogni-
tion and Artificial Intelligence , 33(07):1940003, 2019. 2
[11] Herbert Gelernter, James R Hansen, and Donald W Love-
land. Empirical explorations of the geometry theorem ma-
chine. In Papers presented at the May 3-5, 1960, western
joint IRE-AIEE-ACM computer conference , pages 143–149,
1960. 2
[12] Fucheng Guo and Pengpeng Jian. A graph convolutional net-
work feature learning framework for interpretable geometry
problem solving. In 2022 International Conference on In-
telligent Education and Intelligent Research (IEIR) , pages
59–64. IEEE, 2022. 3
[13] Feng Han and Song-Chun Zhu. Bottom-up/top-down image
parsing by attribute graph grammar. In Tenth IEEE Interna-
tional Conference on Computer Vision (ICCV’05) Volume 1 ,
pages 1778–1785. IEEE, 2005. 3
[14] Yining Hong, Qing Li, Daniel Ciao, Siyuan Huang, and
Song-Chun Zhu. Learning by fixing: Solving math wordproblems with weak supervision. In Proceedings of the
AAAI conference on artificial intelligence , pages 4959–4967,
2021. 3
[15] Mark Hopkins, Ronan Le Bras, Cristian Petrescu-Prahova,
Gabriel Stanovsky, Hannaneh Hajishirzi, and Rik Koncel-
Kedziorski. Semeval-2019 task 10: math question answer-
ing. In Proceedings of the 13th International Workshop on
Semantic Evaluation , pages 893–899, 2019. 1
[16] Danqing Huang, Shuming Shi, Chin-Yew Lin, and Jian Yin.
Learning fine-grained expressions to solve math word prob-
lems. In Proceedings of the 2017 conference on empiri-
cal methods in natural language processing , pages 805–814,
2017. 3
[17] Pengpeng Jian, Fucheng Guo, Cong Pan, Yanli Wang, Yan-
grui Yang, and Yang Li. Interpretable geometry problem
solving using improved retinanet and graph convolutional
network. 2023. 3
[18] Pengpeng Jian, Fucheng Guo, Yanli Wang, and Yang Li.
Solving geometry problems via feature learning and con-
trastive learning of multimodal data. CMES-Computer Mod-
eling in Engineering & Sciences , 136(2), 2023. 3
[19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvinine-
jad, Abdelrahman Mohamed, Omer Levy, Veselin Stoy-
anov, and Luke Zettlemoyer. Bart: Denoising sequence-to-
sequence pre-training for natural language generation, trans-
lation, and comprehension. In Proceedings of the 58th An-
nual Meeting of the Association for Computational Linguis-
tics, pages 7871–7880, 2020. 6, 7
[20] Jinjiao Lin, Haitao Pu, Yibin Li, and Jian Lian. Intelligent
recommendation system for course selection in smart educa-
tion. Procedia Computer Science , 129:449–453, 2018. 1
[21] Sarah Loos, Geoffrey Irving, Christian Szegedy, and Cezary
Kaliszyk. Deep network guided proof search. arXiv preprint
arXiv:1701.06972 , 2017. 6
[22] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang,
Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable
geometry problem solving with formal language and sym-
bolic reasoning. arXiv preprint arXiv:2105.04165 , 2021. 1,
2, 3, 6, 7
[23] Takuya Matsuzaki, Takumi Ito, Hidenao Iwane, Hirokazu
Anai, and Noriko H Arai. Semantic parsing of pre-university
math problems. In Proceedings of the 55th Annual Meeting
of the Association for Computational Linguistics (Volume 1:
Long Papers) , pages 2131–2141, 2017. 3
[24] Andi Saparuddin Nur and Evy Nurvitasari. Geometry skill
analysis in problem solving reviewed from the difference of
cognitive style students junior high school. Journal of Edu-
cational Science and Technology , 3(3):204–210, 2017. 1
[25] Shuai Peng, Di Fu, Yijun Liang, Liangcai Gao, and Zhi Tang.
Geodrl: A self-learning framework for geometry problem
solving using reinforcement learning in deductive reasoning.
InFindings of the Association for Computational Linguis-
tics: ACL 2023 , pages 13468–13480, 2023. 1, 2, 3, 6, 7
[26] Ethan Perez, Florian Strub, Harm De Vries, Vincent Du-
moulin, and Aaron Courville. Film: Visual reasoning with a
general conditioning layer. In Proceedings of the AAAI con-
ference on artificial intelligence , 2018. 6, 7
13836
[27] Jinghui Qin, Lihui Lin, Xiaodan Liang, Rumin Zhang,
and Liang Lin. Semantically-aligned universal tree-
structured solver for math word problems. arXiv preprint
arXiv:2010.06823 , 2020. 3
[28] Subhro Roy and Dan Roth. Mapping to declarative knowl-
edge for word problem solving. Transactions of the Associ-
ation for Computational Linguistics , 6:159–172, 2018. 3
[29] Mrinmaya Sachan and Eric Xing. Learning to solve geom-
etry problems from natural language demonstrations in text-
books. In Proceedings of the 6th Joint Conference on Lexi-
cal and Computational Semantics (* SEM 2017) , pages 251–
261, 2017. 1, 2
[30] Mrinmaya Sachan, Kumar Dubey, and Eric Xing. From text-
books to knowledge: A case study in harvesting axiomatic
knowledge from textbooks to solve geometry problems. In
Proceedings of the 2017 Conference on Empirical Methods
in Natural Language Processing , pages 773–784, 2017. 2, 3
[31] Mrinmaya Sachan, Avinava Dubey, Eduard H Hovy, Tom M
Mitchell, Dan Roth, and Eric P Xing. Discourse in multi-
media: A case study in extracting geometry knowledge from
textbooks. Computational Linguistics , 45(4):627–665, 2020.
1, 3
[32] Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Et-
zioni, and Clint Malcolm. Solving geometry problems:
Combining text and diagram interpretation. In Proceedings
of the 2015 conference on empirical methods in natural lan-
guage processing , pages 1466–1476, 2015. 1, 2, 3
[33] Kewei Tu, Meng Meng, Mun Wai Lee, Tae Eun Choe, and
Song-Chun Zhu. Joint video and text parsing for understand-
ing events and answering queries. IEEE MultiMedia , 21(2):
42–70, 2014. 3
[34] Lei Wang, Yan Wang, Deng Cai, Dongxiang Zhang, and Xi-
aojiang Liu. Translating a math word problem to an expres-
sion tree. arXiv preprint arXiv:1811.05632 , 2018. 3
[35] Lei Wang, Dongxiang Zhang, Jipeng Zhang, Xing Xu, Lianli
Gao, Bing Tian Dai, and Heng Tao Shen. Template-based
math word problem solvers with recursive neural networks.
InProceedings of the AAAI Conference on Artificial Intelli-
gence , pages 7144–7151, 2019. 3
[36] Wu Wen-Tsun. Basic principles of mechanical theorem
proving in elementary geometries. Journal of automated
Reasoning , 2:221–252, 1986. 2
[37] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,
Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn
of lmms: Preliminary explorations with gpt-4v (ision). arXiv
preprint arXiv:2309.17421 , 9, 2023. 1
[38] Xinguo Yu, Mingshu Wang, Wenbin Gan, Bin He, and Nan
Ye. A framework for solving explicit arithmetic word prob-
lems and proving plane geometry theorems. International
Journal of Pattern Recognition and Artificial Intelligence , 33
(07):1940005, 2019. 2
[39] Ming-Liang Zhang, Fei Yin, Yi-Han Hao, and Cheng-Lin
Liu. Plane geometry diagram parsing. arXiv preprint
arXiv:2205.09363 , 2022. 3
[40] Ming-Liang Zhang, Fei Yin, and Cheng-Lin Liu. A multi-
modal neural geometric solver with textual clauses parsed
from diagram. arXiv preprint arXiv:2302.11097 , 2023. 1, 3,
6[41] Song-Chun Zhu, David Mumford, et al. A stochastic gram-
mar of images. Foundations and Trends® in Computer
Graphics and Vision , 2(4):259–362, 2007. 3
13837
