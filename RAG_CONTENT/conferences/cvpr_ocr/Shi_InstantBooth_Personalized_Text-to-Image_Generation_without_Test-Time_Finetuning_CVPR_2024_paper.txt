InstantBooth: Personalized Text-to-Image Generation without Test-Time
Finetuning
Jing Shi‚àóWei Xiong‚àóZhe Lin Hyun Joon Jung
Adobe Inc.
‚àóEqual Contribution
{jingshi,wxiong,zlin,hjung }@adobe.com
A photo of !" woman, backview, in the sunset
A photo of !" catswimming in the swimming pool
A photo of !" woman opening the arm besides the blue seaA photo of !" woman piloting a fight jetA photo of mysterious !"	woman witcher at nightA photo of !" woman in the swimming pool
A photo of !" cat in a bucketA Ukiyo-e painting of !" catA photo of !" cat as a bull dogA charcoal sketch of!" catInput5images of person
Input2images of cat
Figure 1. Personalized images generated by our model given different prompts. On both the ‚Äúperson‚Äù and ‚Äúcat‚Äù categories, our model can
generate text-aligned, identity-preserved, and high-fidelity images of the input concept. ÀÜVis a placeholder word representing the input
subject. We name our model InstantBooth as it does not require test-time model finetuning .
Abstract
Recent advances in personalized image generation have
enabled pre-trained text-to-image models to learn new con-
cepts from specific image sets. However, these methods of-
ten necessitate extensive test-time finetuning for each new
concept, leading to inefficiencies in both time and scal-
ability. To address this challenge, we introduce Instant-
Booth, an innovative approach leveraging existing text-
to-image models for instantaneous text-guided image per-
sonalization, eliminating the need for test-time finetuning.
This efficiency is achieved through two primary innova-
tions. Firstly, we utilize an image encoder that transforms
input images into a global embedding to grasp the general
concept. Secondly, we integrate new adapter layers into
* Listing order of the first two authors is random. Both Jing and Wei
have made significant contributions to the idea design, experiment imple-
mentation, result evaluation, and paper writing.the pre-trained model, enhancing its ability to capture in-
tricate identity details while maintaining language coher-
ence. Significantly, our model is trained exclusively on text-
image pairs, without reliance on concept-specific paired im-
ages. When benchmarked against existing finetuning-based
personalization techniques like DreamBooth and Textual-
Inversion, InstantBooth not only shows comparable profi-
ciency in aligning language with image, maintaining im-
age quality, and preserving the identity but also boasts
a 100-fold increase in generation speed. Project Page:
https://jshi31.github.io/InstantBooth/
1. Introduction
Recent advances in personalized image generation have en-
abled text-to-image models [2, 4, 7, 8, 10, 14, 16, 24, 27,
28, 34] to learn a new concept from a small set of images
then create images of that concept in different poses, per-
spectives, styles, and scenes. Fig. 1 shows examples gener-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
8543
ated by our model based on a specific person or cat concept
learned from the input images.
Current methods in personalized text-to-image genera-
tion face several critical challenges. First, many existing
models [5, 9, 11‚Äì13, 17, 30, 31, 36, 37] require individual
finetuning for each new concept. The image set is usually
converted to a textual token through heavy online optimiza-
tion. This process is both computationally intensive and
storage-demanding, significantly hindering scalability. Sec-
ond, these methods often struggle with identity preservation
when trying to generate images that are aligned with the tex-
tual descriptions. It is challenging to create new images that
vary in poses, viewpoints, and backgrounds, while simulta-
neously retaining the intricate details of the input concept.
To address these challenges, we propose InstantBooth,
a personalized image generation model without test-time
finetuning. To eliminate the need for test-time finetuning,
we use a concept encoder to map the set of images into
a global embedding in the textual embedding space. The
global embedding is then integrated into the original prompt
embeddings to enhance the corresponding word token, such
as ‚Äúperson‚Äù in the input prompt. This enables the model to
understand each new concept with only a single forward
pass during inference, without any optimization process.
However, global embeddings alone often fail to capture
complete identity details from input images. To enhance
identity preservation, we employ a patch encoder for de-
tailed local feature extraction. We also integrate new train-
able adapter layers [1, 20] into our pre-trained model for ad-
ditional local feature conditioning. While the identity needs
to be preserved, we observe that the patch features can affect
the language understanding so that the object pose change
or background synthesis does not always follow the text de-
scription. To address this issue, we adopt a gating mecha-
nism in the adapter layers to control the contribution of the
image patch features and the features generated from the
prompt. To further enhance the language alignment while
keeping the identity, we propose a concept token normal-
ization technique during the sampling process so that the
importance of the global concept embedding and the other
word embeddings in the sentence are re-weighted.
Notably, our model is trained exclusively on text-image
pairs, without using any concept-specific paired images.
With such a training strategy, our model is still able to
generate new images with large poses, location variations,
and diverse backgrounds, while preserving the identity and
language-image alignment, as demonstrated in Fig. 1. We
summarize our contributions below.
‚Ä¢ We propose a novel approach for personalized text-to-
image generation without test-time finetuning. Our model
can generate comparable results as test-time finetuning-
based methods like Dreambooth while being x100 faster.
‚Ä¢ We introduce concept and patch encoders to learn globalconcept embedding and rich patch embeddings from the
image set. With the proposed adapter layers and con-
cept normalization techniques, our model can gener-
ate identity-preserved, diverse, and high-quality images
while keeping the language alignment.
2. Related Work
Personalized Image Generation. Given one or more im-
ages of a concept as inputs, personalized image generation
aims at generating image variations of the given concept or
identity. Existing approaches [11, 17, 30] for this task can
be mainly categorized into two types. The first is test-time
finetuning-based methods [5, 9, 11‚Äì13, 17, 30, 31, 36, 37].
For each new concept, these methods need to learn a con-
cept token through online optimization, which is computa-
tion and storage costly.
The second is encoder-based methods [6, 15, 19, 22,
38, 39, 41], which learn image embeddings from the input
images as the representation of the concept. ELITE [38]
finetunes the pretrained parameters in the attention lay-
ers and sometimes fails to generate images with diverse
poses/viewpoints. SuTI [6] learns the concept from a mas-
sive amount of paired images generated by subject-driven
expert models. Different from these methods, our method
adopts the adapter structure to flexibly incorporate the vi-
sual signal into the model so that it can generate language-
aligned images with identity preservation. Moreover, we do
not use paired images to train our model.
Multimodal-Conditional Image Generation. Modern vi-
sual synthesis systems are usually powered by large text-
to-image foundation models [2, 4, 7, 8, 10, 16, 24, 27, 28].
Beyond text guidance, these models can also take additional
inputs from other modalities, such as image, depth, layout,
and so on. For example, Stable Diffusion V2 [29] can take
a depth image or masked image as the condition to gen-
erate images or perform inpainting. eDiff-I [2] takes im-
ages as additional style guidance for stylized image synthe-
sis. More recently, approaches built upon pre-trained text-
to-image models have been proposed for more controllable
multi-modal image generation [20, 23, 42]. ControlNet [42]
controls pre-trained diffusion models to take more input
conditions by locking the original parameters and making
a trainable copy to the newly added layers. GLIGEN [20]
extends pre-trained models to take layouts, images, or other
conditions by injecting new layers into the original model.
3. Method
3.1. Overall Framework
Fig. 2 outlines the inference pipeline of our method for gen-
erating images from image set X={xi}N
i=1(Ncan be 1
or larger) and text prompt P. We encode Xinto a global
concept embedding cgvia Concept Encoder Egand obtain
8544
Input ImagesInputPrompt: ‚ÄúPhotoof!ùëâperson‚ÄùPhotoof!ùëâpersonAdapterSelfAttnCrossAttn...AdapterSelfAttnCrossAttn
CLIP Text Encoder
ConceptEncoder
PatchEncoder
U-NetRich Patch Feature
Prompt EmbeddingsGenerated Image
FrozenTrainableSharedBackboneFigure 2. The inference pipeline of our model. Given the input images and prompt, we encode the images into a global concept embedding
using the Concept Encoder and obtain rich patch embeddings using the Patch Encoder to preserve more identity details. The prompt is
encoded into sentence embeddings and merged with the concept embedding. The prompt embeddings and the patch embeddings are taken
by the cross-attention layers and the adapter layers of the U-Net respectively to generate new images. During training, only the image
encoders and the adapter layers are trainable, the other parts are frozen. (We omit the object masks of the input images for simplicity.)
local patch embeddings cpwith Patch Encoder Ep. The
prompt Pis converted into sentence embeddings c. We then
merge cgintocto create enhanced sentence embeddings ce,
which, along with cp, are processed by the cross-attention
and adapter layers of the diffusion model‚Äôs U-Net for image
synthesis, respectively. We describe the details below.
3.2. Model Training
Training Data Creation. Our model uses Stable Diffu-
sion [29] as its backbone and is trained on many single im-
ages with their descriptions. During training, since we don‚Äôt
have concept-specific image pairs, we preprocess and aug-
ment each single image to create a condition image, using
the original one as ground truth. To clarify, during training,
we use N= 1 forX.To construct the condition image
from the original image x, we first crop out the object re-
gion, then mask out the background of the cropped image
to obtain the initial condition image x‚Ä≤=x¬∑m, where m
is the object mask. Then we perform random augmenta-
tionsAto the masked image to obtain the final condition
image x‚Ä≤=A(x‚Ä≤)to our model. Aincludes random ro-
tation (45 degrees maximum), random horizontal flipping,
and random crop. x‚Ä≤is further resized to 224√ó224resolu-
tion before feeding to the image encoders.
Prompt Construction. We modify the original prompt
to fit our model as follows. Given the prompt Pwith the
format ‚Äú... [class noun] ...‚Äù, we insert ÀÜVright before [class
noun] as a new word, where [class noun] is a coarse descrip-
tion noun of the input object category, ÀÜVis used only as a
placeholder word for the input concept to indicate the loca-
tion of the [class noun] word in the sentence . For example,
we modify the original prompt ‚ÄúA photo of a person playing
guitar‚Äù to ‚ÄúA photo of a ÀÜVwoman playing guitar‚Äù, where
‚Äúwoman‚Äù is the [class noun] for the person category. Thecoarse description noun for the person category includes
‚Äúman‚Äù, ‚Äúwoman‚Äù, ‚Äúbaby‚Äù, ‚Äúgirl‚Äù, ‚Äúboy‚Äù, ‚Äúlady‚Äù. For the
cat category, we identify ‚Äúcat‚Äù, and ‚Äúkitten‚Äù as the coarse
descriptions. We then encode the prompt into the prompt
embedding c‚ààR77√ó768using a Frozen CLIP text encoder.
Global Concept Embedding Learning. We extract the
global semantics of the condition image x‚Ä≤using the Con-
cept Encoder Ec. Specifically, Ecis composed of a pre-
trained CLIP image encoder that extracts the global image
embedding from x‚Ä≤, and a learnable linear layer that projects
the embedding to the textual embedding space to obtain the
final global embedding cg‚ààR1√ó768. Then we insert the
global embedding into the prompt embedding cto obtain
the concept-enhanced prompt embedding ce‚ààR77√ó768by
replacing the embedding of the placeholder word ÀÜVwithcg.
The enhanced prompt embedding is the input to the cross-
attention layers of the U-Net in Stable Diffusion.
Rich Patch Embedding Learning with Adapters. Using
the global concept embedding cgalone is not sufficient to
capture all identity details of the input subject. To better
preserve the identity, we use a Patch Encoder Epto encode
the images into rich patch feature tokens. Specifically, Epis
composed of a pre-trained CLIP image encoder that extracts
257 local patch tokens from x‚Ä≤*, and a learnable linear layer
that projects each token to a unified feature space to obtain
the final patch embeddings cp‚ààR257√ó768.cpcontains rich
local details of the condition image.
Since the original Stable Diffusion only takes text as the
condition, to enable the model to take images as a condi-
tion, inspired by [20], we introduce an additional adapter
layer in-between the cross-attention layer and self-attention
*The CLIP image encoder has a ViT structure with the input patch size
to be 14. The 257 tokens are composed of 1 CLS token and 256 local
tokens, each token has a shape of R1√ó1024.
8545
Figure 3. (a): Visual comparison between our model with and
without concept renormalization. (b) The average attention of each
word without concept renormalization. (c) The average attention
of each word with concept renormalization.
layer in each transformer block of the U-Net, as illustrated
in Fig. 2. The adapter layer is indeed a gated self-attention
layer formulated as:
y:=y+Œ≤¬∑tanh( Œ≥)¬∑S([y,cp]), (1)
where Sis the self-attention operator, ydenotes the visual
feature in the transformer layer, [¬∑]means concatenating the
features in the token space, Œ≥is a learnable scalar gate ini-
tialized as zero. Notably, Œ≤is an important constant be-
tween 0 and 1 to control the contribution of the image fea-
tures against the textual features. It is set to be 1 during
training to encode as much identity information from the
images as possible.
Training Objective. We use the original diffusion denois-
ing loss as our objective for each image:
L=Ez,t,c e,x‚Ä≤,Œ∑‚ààN(0,1)
‚à•Œ∑‚àíŒ∑Œ∏(zt, t, ce, x‚Ä≤)‚à•2
2
,(2)
where ztis the latent noisy image at time step tobtained
from the ground-truth image x,Œ∑is the latent noise to pre-
dict,ceis the concept-enhanced prompt embeddings, x‚Ä≤is
the condition image, and Œ∑Œ∏is the noise prediction model
with parameters Œ∏.
3.3. Model Inference
During inference, There are a few differences from the
training phase. First, we can input multiple condition im-
ages of a concept to the model rather than only 1 image. To
takeNimages as inputs, for the global concept embedding,
we average the concept embedding of each condition image
as the final concept embedding. For the patch embeddings,
we obtain the 257 tokens from each image and then con-
catenate them in the token space, forming 257√óNtokens
as the final patch embeddings. Our adapter based on the
self-attention mechanism allows for an arbitrary number of
tokens as inputs. Second, we still mask out the background
of each image but do not perform any augmentation on it.Balanced Sampling. During inference, we observe that set-
tingŒ≤= 1in Eq. 1 results in a strong identity and pose re-
construction of the input images, while the language-image
alignment is weakened. To preserve the identity while keep-
ing the language understanding, we adjust the value of Œ≤
during inference so that the adapter layer takes both the in-
formation generated from the prompt and the conditioning
images. We observe that adjusting Œ≤indeed benefits the bal-
ance between identity preservation and language alignment.
Concept Token Renormalization. We perform further
analysis on the language understanding of our model. We
observe that under certain circumstances, even if we have
adjusted the value of Œ≤for Balanced Sampling, the concept
token cgcan still dominate the cross-attention against the
other word tokens, leading to language forgetting. Fig. 3
(b) shows the cross-attention averaged over all batches, lay-
ers, and time-steps for each word when inputting the prompt
‚Äùphoto of mysterious sks woman witcher at night‚Äù, where
‚Äúsks‚Äù denotes the placeholder word ÀÜVin our implementa-
tion and Œ≤= 0.3. The attention of ‚Äúsks‚Äù is significantly
higher than the other words, while the core words such as
‚Äúnight‚Äù and ‚Äúwitcher‚Äù are assigned low attention weights,
showing a sign of language forgetting. To address this is-
sue, we renormalize the concept token cgwith a factor of
Œ±‚àà(0,1]as:cg=Œ±¬∑cg.
Since there are only linear mappings from the prompt
embeddings before calculating the cross-attention, such a
renormalization strategy is essentially equivalent to rescal-
ing the cross-attention between the concept token and the
visual tokens in the Cross-Attention layers. Fig. 3 (c) shows
the average attention for each word after our renormaliza-
tion. It can be observed that the attention of the concept to-
ken does not dominate the cross-attention anymore. The vi-
sual comparison shown in Fig. 3 (a) provides more evidence
for this observation. Without concept renormalization, the
model fails to generate the ‚Äúwitcher‚Äù style or the ‚Äúnight‚Äù
background. With renormalization, the attentions of the
nouns are more balanced, and the model successfully gener-
ates the ‚Äúwitcher‚Äù style and the ‚Äúnight‚Äù background. All the
analysis and evidence demonstrate that Concept Renormal-
ization can help to achieve a better balance between text-
image alignment and identity preservation.
4. Experiments
4.1. Dataset and Evaluation Metric
Datasets . We conduct experiments on two subject cate-
gories person andcat, and collect text-image pairs where
the images contain these object categories and the prompts
contain the related coarse category nouns as described in
section 3.2.
Then we generate the entity segmentation masks for
all the collected images using the entity segmentation
8546
¬© 2021 Adobe. All Rights Reserved. Adobe Confidential.
A photo of !ùëâ woman in New York
A photo of !ùëâwoman playing guitar in the forest
A colorful graffiti of !ùëâ woman
Aphoto of !ùëâ woman riding a bycicle under Eiffel Tower
Aphoto of !ùëâ woman in the kitchen
Input ImageTextual InversionDreamBoothELITEOursInput PromptFigure 4. The visual comparison with other methods. All methods take five images as input but we only show one image here for simplicity.
model [26] trained on high-quality entity segmentation
datasets [25]. Then we filter out images where the region
ratio of the object belonging to our target category is less
than 0.1 or larger than 0.7. We also filter out images with
multiple objects to simplify the training.
In total, we have 1.43 million text-image pairs for the
person category and 0.37 million for the cat category. We
use PPR10K [21] person dataset as our testing dataset for
the person category. PPR10K includes high-quality human
portrait photos of 1681 identities, each of which contains
multiple images of the same person. We select 50 identities
in the test split of PPR10k [21], where each selected identity
is guaranteed to have more than 5 images, and we only keep
the first 5 images in naming order as our test input.
Metrics . We quantify the identity preservation and lan-
guage alignment of each model with the metrics below.
Reconstruction evaluates whether the identity can be fully
preserved by the default prompt ‚ÄúA photo of ÀÜV[class
noun]‚Äù, where [class noun] can be a person or cat. It is
measured by the similarity of CLIP visual features between
the input image and the generated image.
Face similarity evaluates whether the identity can be fully
preserved for the person category. Since the face is the pri-mary indicator of a person‚Äôs identity, we use a strong face
detector [33] to detect faces in both the generated and the
input images of a subject. Then we extract an embedding
from each detected face with an Inception-ResnetV1 [35]
pre-trained on VGGFace2 [3]. Then we calculate the aver-
age cosine similarity between each pair of face embeddings
as the perceptual face similarity. If a face is not detected,
we penalize the similarity as 0.
Alignment measures the vision-language alignment be-
tween the input prompt and the output image, specifically,
whether the generated image follows the prompt. We mea-
sure this using the CLIP similarity between image and text
embeddings. We construct various prompts ranging from
background modifications (‚ÄúA photo of ÀÜV[class noun] on
the moon‚Äù) to style changes (‚ÄúAn oil painting of ÀÜV[class
noun]‚Äù), and a compositional prompt (‚Äú ÀÜV[class noun] shak-
ing hand with Biden‚Äù). The specific list of prompts is in the
Supplement.
4.2. Implementation Details
We use the Stable Diffusion [29] V1-4 as the pre-trained
text-to-image backbone. In all experiments with our model,
we use the infrequent word ‚Äúsks‚Äù as ÀÜVas suggested in
8547
¬© 2021 Adobe. All Rights Reserved. Adobe Confidential.
Input5images of cata photo of !ùëâcat wearing sunglasses on the beacha photo of !ùëâcat inthe swimming poola photo !ùëâcat of play with a balla photo of !ùëâcat in a bucketa watercolor painting of !ùëâcata photo !ùëâcat of on the piano
Input5images of persona photo of !ùëâwoman, backview, in the sunseta photo of !ùëâwoman opening the arm besides the seaa photo !ùëâwoman with thumb upa photo of !ùëâwoman as a doctora photo of mysterious !ùëâwoman witcher at nighta photo !ùëâwoman as a Wonder Woman
a photo of !ùëâwoman reading books in the librarya photo of !ùëâwoman driving a cara photo !ùëâwoman playing gambling machinea photo of !ùëâwoman working before a computera photo of mysterious !ùëâwoman witcher at nighta photo !ùëâwoman as a Wonder WomanInput4images of person
Input5images of cata photo of !ùëâcat standing on the boata photo of !ùëâcat jumping on the floora photo !ùëâcat on the treea photo of !ùëâcat in a bucketa watercolor painting of !ùëâcata photo !ùëâcat of on the piano
Figure 5. Personalized images generated by our model on the ‚Äú person ‚Äù and ‚Äú cat‚Äù categories.
DreamBooth [40]. Note that ‚Äúsks‚Äù is a placeholder indi-
cating the position of the concept token in the sentence, and
will be replaced by the global image embedding. For both
the Concept Encoder Ecand the Patch Encoder Ep, we use
the pre-trained CLIP image encoder as the backbone fol-
lowed by a randomly initialized linear layer. During train-
ing, only the linear layers of the image encoders and the
adapter layers are updated, and we freeze all other parts of
the model. The model trains for 320k iterations for the per-
son class and 200k iterations for the cat class, using a learn-
ing rate of 1e-6 for adapter layers and 1e-4 for the linear
layers in the visual encoders, with a batch size of 16 de-
ployed on 4 A100 GPUs. In testing, we search different
combinations of the adapter weight Œ≤and concept renor-
malization factor Œ±, and choose Œ≤= 0.3,Œ±= 0.4as the
best trade-off between vision-language alignment and iden-
tity preservation as detailed in Supplement.
4.3. Comparison to SOTA Methods
In this section, we compare our approach with Textual
Inversion (TI) [11] and DreamBooth (DB) [30], both of
which require heavy test-time finetuning, as well as with
ELITE [38], a zero-shot personalization method. We use
the official code base for Textual Inversion and ELITE, and
a third-party replication [40] for DreamBooth, where the
base text-to-image model is changed from Imagen [32] to
Aphoto of baby in the swimming poolApencil drawing of babyA painting of baby in Van Gogh style
ELITE(1 Input Image)Input
Ours(1 Input Image)
DreamBooth(5 Input Images)Figure 6. The comparison of our method with single image input.
Stable Diffusion [29]. Note that, since ELITE can accept
only a single image, we compare ELITE using a single im-
age as input, whereas all other methods use multiple images
as inputs.
Qualitative Results. The personalization results on both
person and cat categories are displayed in Fig.1 and Fig.5,
while the comparison with other methods is displayed in
Fig. 4, where our method exhibits better perceptual qual-
ity, vision-language alignment, and identity preservation
than the compared ones. Our method also supports large
pose and structure variations, such as ‚Äùriding a bicycle‚Äù and
8548
¬© 2021 Adobe. All Rights Reserved. Adobe Confidential.
InputOursTextualInversionDreamBooth
ELITEFigure 7. The visual comparison of image reconstruction given
prompt ‚ÄúA photo of ÀÜVwoman‚Äù.
‚Äùopen arms,‚Äù while maintaining the identity.
For Textual Inversion, it uses the LDM text-to-image
model, which has a weaker capacity than Stable Diffusion,
often producing blurry images and failing to accurately fol-
low the prompts. DreamBooth can generate high-quality
images; however, when the input person subject occupies a
small portion (e.g., row 2 and 4 of Fig.4), it tends to gen-
erated the person within a small portion of the image, thus
limiting the identity preservation ability. Moreover, even
when the input image contains a large portion of the per-
son object (e.g., row 5 of Fig.4), DreamBooth often only
preserves the person‚Äôs outfit but distorts the face identity.
ELITE cannot maintain the identity, but note it is not a com-
pletely fair comparison since ELITE is trained on OpenIm-
ages [18] of general categories, while ours are trained on
specific categories. In contrast, our method produces im-
ages with clearer faces and more details given a wide range
of person-size portions in the image. We suspect the reason
is our adapter layers have seen millions of different person
identities, thereby garnering a stronger prior for maintain-
ing identity than the compared test-time finetuning-based
methods.
Fig. 5 shows more visual results from our model. On
the ‚Äùperson‚Äù and ‚Äùcat‚Äù categories, our model can generate
diverse, identity-preserved, and language-aligned images.
Fig. 6 showcases our method‚Äôs performance with a single
input image, indicating the superiority of our methods over
ELITE in a single image input penalization setting.
Quantitative Results. The quantitative comparison with
other methods is shown in Tab. 1. Our method achieves bet-
ter vision-language alignment and face similarity than the
compared methods, but the reconstruction is weaker. This
can be explained by the visualization of the reconstruction
in Fig. 7. All the comparison methods tend to replicate the
pose of the foreground person, and Textual Inversion and
DreamBooth also replicate the background. Our method,
however, can reconstruct the same person in different fore-
ground poses and backgrounds. This discrepancy leads to a
lower reconstruction score for our method but does not nec-
essarily mean it is inferior in identity preservation. There-
fore, face similarity is a more robust metric, regardless of
the foreground pose and background, for measuring iden-
tity preservation, indicating that our method possesses the
best identity preservation capability.
To test our model in a full setting (i.e., the inputs are
background-masked images during testing), we also calcu-
late the alignment score and the perceptual face similarityMethods Align ‚ÜëFace Sim ‚ÜëRecon‚ÜëTime (s) ‚Üì
TI [11] 0.2556 0.1130 0.7832 ‚àº1500
DB [30] 0.3088 0.2408 0.8335 ‚àº600
ELITE [38] 0.2329 0.1873 0.7666 ‚àº6
Ours 0.3140 0.2456 0.7329 6
Ours + M 0.3135 0.2418 - 6
Table 1. Quantitative comparison in ‚Äú person ‚Äù category of TI (Tex-
tual Inversion), DB (DreamBooth), ELITE and our method. The
metric ‚ÄúAlign‚Äù is for alignment, ‚ÄúFace Sim‚Äù for face cosine simi-
larity and ‚ÄúRecon‚Äù for reconstruction. ‚ÄúM‚Äù denotes to our model
tested with masked images as input.
Methods Quality ‚ÜëAlignment ‚ÜëIdentity ‚Üë
TI [11] 2.89 3.04 2.97
DB [30] 3.50 3.50 3.55
ELITE [38] 3.14 3.08 3.08
Ours 3.53 3.58 3.55
Table 2. User study on the ‚Äú person ‚Äù category. ‚ÄúQuality‚Äù measures
the image quality ( e.g. artifact-free), ‚ÄúAlignment‚Äù measures the
vision-language alignment, and ‚ÄúIdentity‚Äù measures the identity
preservation performance.
for our model with masked images as input. The results
in Tab. 1 (last row) demonstrate that both variants of our
model achieve better vision-language alignment and iden-
tity preservation.
Moreover, since our method does not need test-time fine-
tuning, our testing time in Tab. 1 (last column) is signifi-
cantly lower than that of TI and DB methods (100x faster).
User Study. We conduct a user study to compare our
method with DreamBooth and Textual Inversion percep-
tually. For each evaluation, each user will see one input
image, one prompt, and four images generated by each
method. The user will rank each generated image from
1(worst) to 5(best) concerning its visual quality, vision-
language alignment, and identity preservation. We select
50identities of the ‚Äúperson‚Äù category where each identity is
personalized by 10prompts. 4generated images per prompt
are evaluated, resulting in 2000 unique evaluation samples.
The user study is deployed via Amazon Mechanical Turk,
where each sample will be evaluated by one user. After fil-
tering out invalid user inputs, we obtained 1094 valid eval-
uated samples. The results shown in Tab. 2 indicate that
our method outperforms the two comparison methods in all
three important aspects.
4.4. Ablation Study
We conduct a thorough ablation study across various com-
ponents and settings as follows.
W./o. train mask . To demonstrate the importance of the ob-
ject mask, we remove the object mask used during training.
W./o. patch feature . To verify the necessity of rich patch
8549
¬© 2021 Adobe. All Rights Reserved. Adobe Confidential.
w/o train maskw/o patch featurew/o adapter!ùëâbefore textencoderTune CLIP visualencoderTune UNet1 Image Test 
InputOur full model
Figure 8. The visual comparison of different ablation settings. The random noise is the same for all variants of our model. The prompts
and input images are taken from Fig. 4.
Methods Align ‚ÜëReconstruct ‚Üë
InstantBooth 0.3140 0.7329
w/o train mask 0.3127 0.7485
w/o patch feature 0.3269 0.6494
w/o adapter 0.3242 0.5468
ÀÜVbefore CLIP 0.3127 0.7495
Tune CLIP Vis Enc 0.3266 0.6425
Tune U-Net 0.3142 0.7265
1 Image Input 0.3140 0.7261
Table 3. Ablation study for various settings of our model.
features as the condition of the model, we do not use the
patch feature and only use the global [CLS] token from the
CLIP image encoder as the input to the adapter layers.
W./o. adapter . We also remove the adapter branch and study
if purely ÀÜVcan bear sufficient identity information.
ÀÜVbefore text encoder . Since our model inserts the ÀÜVto the
textual space after the CLIP text encoder, we also study the
early integration of textual and visual information, i.e., we
insert ÀÜVinto the text token space before CLIP text encoder.
Tuning CLIP visual encoder . Our standard setting freezes
both the backbone of CLIP visual encoder and only fine-
tunes the linear heads. However, since the adapter is de-
signed to extract more fine-grained content details, we also
investigate whether finetuning the backbone of CLIP visual
encoder can benefit the learning of object details.
Tuning U-Net . We also try to tune the U-Net parameters.
Single image as input . Since our model is flexible for the
number of input images, we evaluate our model using a sin-
gle image as the input image condition, i.e., N= 1.
The quantitative results are presented in Tab. 3, and the
visual results are shown in Fig. 8. We observe that all the
ablation settings result in weaker visual results than our full
setting. Without the object mask during training, the model
fails to capture accurate foreground information, and its
language-understanding ability is jeopardized by the back-
ground noise. Therefore, the model tends to retain more
background information, resulting in a higher reconstruc-
tion score but a lower alignment score.
Without the patch feature, text-image alignment be-comes slightly better, but reconstruction degrades signifi-
cantly, showing that the patch feature is crucial for identity
preservation. A similar analysis applies to the entire adapter
branch.
ForÀÜVbefore CLIP, we observe a slight trade-off between
alignment and reconstruction, but the visual results deteri-
orate. We conjecture that since the CLIP text encoder is
frozen, the identity information from ÀÜVis diffused by the
CLIP text encoder, and thus the visual details are missing.
The results of tuning the CLIP visual encoder or U-Net
indicate that tuning these modules can lead to worse identity
preservation. We suspect the reason is that we keep Œ≤=
0.3,Œ±= 0.4in all ablation settings. Training the additional
visual encoder or U-Net makes the U-Net more dependent
on the visual information, and thus such a choice of Œ≤and
Œ±may not be optimal.
The single-image finetuning version of our model
achieves the same alignment score but worse reconstruc-
tion, indicating that more input images can provide more
details of the foreground to better preserve the identity.
Nonetheless, from Fig. 6 and Fig. 8, it is impressive that
our model can achieve promising identity preservation even
with one input image.
5. Conclusion and Limitation
We present an approach that extends existing pre-trained
text-to-image diffusion models for personalized image gen-
eration without test-time finetuning. The core idea is to
convert input images into a global token for general con-
cept learning and to introduce adapter layers to incorpo-
rate rich local image representation for generating fine iden-
tity details. Extensive results demonstrate that our model
can generate language-aligned and identity-preserved im-
ages on unseen concepts with only a single forward pass.
This remarkable efficiency improvement will enable a va-
riety of practical personalization applications. While our
model exhibits strong performance and fast speed, the cur-
rent adapter structure can only take a single concept as in-
put. Investigating this issue could be our future work.
Acknowledgement. We thank Qing Liu for dataset prepa-
ration and He Zhang for object mask computation.
8550
References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katie Millican, Malcolm Reynolds, et al. Flamingo: a vi-
sual language model for few-shot learning. arXiv preprint
arXiv:2204.14198 , 2022. 2
[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,
Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,
Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image
diffusion models with an ensemble of expert denoisers. arXiv
preprint arXiv:2211.01324 , 2022. 1, 2
[3] Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and An-
drew Zisserman. Vggface2: A dataset for recognising faces
across pose and age. In 2018 13th IEEE international con-
ference on automatic face & gesture recognition (FG 2018) ,
pages 67‚Äì74. IEEE, 2018. 5
[4] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,
Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-
phy, William T Freeman, Michael Rubinstein, et al. Muse:
Text-to-image generation via masked generative transform-
ers.arXiv preprint arXiv:2301.00704 , 2023. 1, 2
[5] Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan,
Yuwei Zhou, and Wenwu Zhu. Disenbooth: Identity-
preserving disentangled tuning for subject-driven text-to-
image generation. arXiv preprint arXiv:2305.03374 , 2023.
2
[6] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui
Jia, Ming-Wei Chang, and William W Cohen. Subject-driven
text-to-image generation via apprenticeship learning. arXiv
preprint arXiv:2304.00186 , 2023. 2
[7] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,
Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,
Hongxia Yang, et al. Cogview: Mastering text-to-image gen-
eration via transformers. Advances in Neural Information
Processing Systems , 34:19822‚Äì19835, 2021. 1, 2
[8] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.
Cogview2: Faster and better text-to-image generation via
hierarchical transformers. Advances in Neural Information
Processing Systems , 35:16890‚Äì16902, 2022. 1, 2
[9] Ziyi Dong, Pengxu Wei, and Liang Lin. Drea-
martist: Towards controllable one-shot text-to-image gen-
eration via contrastive prompt-tuning. arXiv preprint
arXiv:2211.11337 , 2022. 2
[10] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,
Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-
based text-to-image generation with human priors. In Eu-
ropean Conference on Computer Vision , pages 89‚Äì106.
Springer, 2022. 1, 2
[11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-
nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-
Or. An image is worth one word: Personalizing text-to-
image generation using textual inversion. arXiv preprint
arXiv:2208.01618 , 2022. 2, 6, 7
[12] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano,
Gal Chechik, and Daniel Cohen-Or. Designing an encoder
for fast personalization of text-to-image models. arXiv
preprint arXiv:2302.12228 , 2023.[13] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar,
Dimitris Metaxas, and Feng Yang. Svdiff: Compact pa-
rameter space for diffusion fine-tuning. arXiv preprint
arXiv:2303.11305 , 2023. 2
[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems , 33:6840‚Äì6851, 2020. 1
[15] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han
Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and
Yu-Chuan Su. Taming encoder for zero fine-tuning image
customization with text-to-image diffusion models. arXiv
preprint arXiv:2304.02642 , 2023. 2
[16] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,
Eli Shechtman, Sylvain Paris, and Taesung Park. Scal-
ing up gans for text-to-image synthesis. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10124‚Äì10134, 2023. 1, 2
[17] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Shechtman, and Jun-Yan Zhu. Multi-concept customization
of text-to-image diffusion. arXiv preprint arXiv:2212.04488 ,
2022. 2
[18] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-
jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan
Popov, Matteo Malloci, Alexander Kolesnikov, et al. The
open images dataset v4: Unified image classification, object
detection, and visual relationship detection at scale. Interna-
tional Journal of Computer Vision , 128(7):1956‚Äì1981, 2020.
7
[19] Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-
diffusion: Pre-trained subject representation for control-
lable text-to-image generation and editing. arXiv preprint
arXiv:2305.14720 , 2023. 2
[20] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-
wei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.
Gligen: Open-set grounded text-to-image generation. arXiv
preprint arXiv:2301.07093 , 2023. 2, 3
[21] Jie Liang, Hui Zeng, Miaomiao Cui, Xuansong Xie, and
Lei Zhang. Ppr10k: A large-scale portrait photo retouch-
ing dataset with human-region mask and group-level consis-
tency. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 653‚Äì661, 2021.
5
[22] Yiyang Ma, Huan Yang, Wenjing Wang, Jianlong Fu, and
Jiaying Liu. Unified multi-modal latent diffusion for joint
subject and text conditional image generation. arXiv preprint
arXiv:2303.09319 , 2023. 2
[23] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-
gang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning
adapters to dig out more controllable ability for text-to-image
diffusion models. arXiv preprint arXiv:2302.08453 , 2023. 2
[24] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741 , 2021. 1, 2
[25] Lu Qi, Jason Kuen, Weidong Guo, Tiancheng Shen, Jiux-
iang Gu, Wenbo Li, Jiaya Jia, Zhe Lin, and Ming-Hsuan
8551
Yang. Fine-grained entity segmentation. arXiv preprint
arXiv:2211.05776 , 2022. 5
[26] Lu Qi, Jason Kuen, Yi Wang, Jiuxiang Gu, Hengshuang
Zhao, Philip Torr, Zhe Lin, and Jiaya Jia. Open world en-
tity segmentation. IEEE Transactions on Pattern Analysis
and Machine Intelligence , pages 1‚Äì14, 2022. 5
[27] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In International Confer-
ence on Machine Learning , pages 8821‚Äì8831. PMLR, 2021.
1, 2
[28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022. 1, 2
[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¬®orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10684‚Äì10695, 2022. 2, 3, 5, 6
[30] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. arXiv preprint arXiv:2208.12242 , 2022. 2, 6,
7
[31] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei,
Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein,
and Kfir Aberman. Hyperdreambooth: Hypernetworks for
fast personalization of text-to-image models. arXiv preprint
arXiv:2307.06949 , 2023. 2
[32] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi,
Rapha Gontijo Lopes, et al. Photorealistic text-to-image
diffusion models with deep language understanding. arXiv
preprint arXiv:2205.11487 , 2022. 6
[33] Sefik Ilkin Serengil and Alper Ozpinar. Hyperextended light-
face: A facial attribute analysis framework. In 2021 Interna-
tional Conference on Engineering and Emerging Technolo-
gies (ICEET) , pages 1‚Äì4. IEEE, 2021. 5
[34] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 1
[35] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and
Alexander Alemi. Inception-v4, inception-resnet and the im-
pact of residual connections on learning. In Proceedings of
the AAAI conference on artificial intelligence , 2017. 5
[36] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon.
Key-locked rank one editing for text-to-image personaliza-
tion. In ACM SIGGRAPH 2023 Conference Proceedings ,
pages 1‚Äì11, 2023. 2
[37] Andrey V oynov, Qinghao Chu, Daniel Cohen-Or, and Kfir
Aberman. p+: Extended textual conditioning in text-to-
image generation. arXiv preprint arXiv:2303.09522 , 2023.
2
[38] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei
Zhang, and Wangmeng Zuo. Elite: Encoding visual con-cepts into textual embeddings for customized text-to-image
generation. arXiv preprint arXiv:2302.13848 , 2023. 2, 6, 7
[39] Guangxuan Xiao, Tianwei Yin, William T Freeman, Fr ¬¥edo
Durand, and Song Han. Fastcomposer: Tuning-free multi-
subject image generation with localized attention. arXiv
preprint arXiv:2305.10431 , 2023. 2
[40] Zhisheng Xiao. Dreambooth on stable diffusion. https://
github.com/XavierXiao/Dreambooth-Stable-
Diffusion , 2022. 6
[41] Ziyang Yuan, Mingdeng Cao, Xintao Wang, Zhongang Qi,
Chun Yuan, and Ying Shan. Customnet: Zero-shot object
customization with variable-viewpoints in text-to-image dif-
fusion models. arXiv preprint arXiv:2310.19784 , 2023. 2
[42] Lvmin Zhang and Maneesh Agrawala. Adding conditional
control to text-to-image diffusion models. arXiv preprint
arXiv:2302.05543 , 2023. 2
8552
