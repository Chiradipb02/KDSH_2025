Bi-SSC: Geometric-Semantic Bidirectional Fusion for Camera-based
3D Semantic Scene Completion
Y ujie Xue*, Ruihui Li*,F a nW u†, Zhuo Tang, Kenli Li, Mingxing Duan†
College of Computer Science and Electronic Engineering, Hunan University
{xueyj, liruihui, wufan, ztang, lkl, duanmingxing }@hnu.edu.cn
Ours VoxFormer-T(CVPR2023)
 Only RGB Images As Inputs
carbicycle motocycle
truck personother-veh. bicylist
motorcyclist parkingroad sidewalk
fencebuilding vegetation
trunk poleterrain
traf.-sign
Figure 1. Given 2D image from the camera, our method is able to predict the complete 3D geometry of occluded objects and scenes.
Clearly, our method excels not only in ﬁnely reconstructing the visible region, but also achieves better completion and segmentation ofinvisible and shaded areas, such as roads and poles in shaded areas, trees in shaded areas, and ﬁne-grained proﬁles of cars overlaps.V oxFormer-T [ 25] also uses stereo depth performance for comparison, and the comparison beneﬁts are marked with boxes.
Abstract
Camera-based Semantic Scene Completion (SSC) is to
infer the full geometry of objects and scenes from only 2Dimages. The task is particularly challenging for those in-visible areas, due to the inherent occlusions and lightingambiguity. Existing works ignore the information missingor ambiguous in those shaded and occluded areas, result-ing in distorted geometric prediction. To address this issue,
we propose a novel method, Bi-SSC, bidirectional geomet-
ric semantic fusion for camera-based 3D semantic scene
completion. The key insight is to use the neighboring struc-
ture of objects in the image and the spatial differences from
different perspectives to compensate for the lack of informa-tion in occluded areas. Speciﬁcally, we introduce a spatial
sensory fusion module with multiple association attention
to improve semantic correlation in geometric distributions.This module works within single view and across stereo
views to achieve global spatial consistency. Experimentalresults demonstrate that Bi-SSC outperforms state-of-the-
art camera-based methods on SemanticKITTI, particularly
excelling in those invisible and shaded areas.
*Equal contributed.†Corresponding authors.1. Introduction
When faced with real-world objects of arbitrary shapes
and inﬁnite categories, the perception of the 3D environ-
ment is crucial for autonomous driving systems [ 11,39]. It
directly affects downstream tasks such as motion predictionand semantic map construction. However, constructing ac-curate and complete 3D information of the real world is no-
toriously difﬁcult, since factors such as viewpoint occlusion
or sparse noise.
To tackle these difﬁculties, 3D semantic scene comple-
tion (SSC) [ 41] is introduced, which formulates the prob-
lem as predicting the geometry and semantics of a scenethrough prior information. Subsequently, some excellentmethods for realizing SSC with 3D information as inputhave been proposed [ 5,6,21,46,50,61]. While LIDAR
sensors offer relatively accurate depth measurement, cam-eras, despite being more cost-effective, can provide abun-
dant visual information about the scene. The camera-based
methods are emerging as an exciting alternative to LIDAR.This trend can be evidenced by MonoScene [ 3], which
earliest approach that relied on monocular RGB images
to infer 3D voxelized semantic scenes. However, it ex-poses the vulnerability of 2d-3d transformation, which is
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
20124
inevitably subject to image occlusion and incomplete ob-
servation [ 15,24,26,33]. Such as the reconstruction of
roads obstructed by buildings is often unfeasible. In recentyears,camera-based methods [ 16,25,51] seek to overcome
this challenge through image modeling and dimensional
transformation, but their inferences about the obscured
area remain ambiguous. Additionally, although bird’s eyeview (BEV) perception provides a holistic representationof image features, and valuable support for obscured ar-eas [ 27,49]. Nevertheless, in recent works [ 20,57], the
holistic understanding of the 3D scene can hardly be recov-
ered by using monocular or binocular BEV feature maps,
especially for real-world obstacles with variable shapes.
Compared to existing methods, we consider building
ﬁne-grained 3D representations by integrating geometric
and semantic features. This design is motivated by two
factors: Firstly, images provide spatial distribution and se-mantic information of objects in the scene, the neighboring
structures within the single view can be employed to predict
the structure in the invisible region. Through geometric-semantic interaction, objects such as the pole covered by
the car in Fig. 1 can be reconstructed. Furthermore, thespatial difference across different view images can allevi-ate the occlusion issue, thus, implementing cross-fusion of
dual-view information can generate detailed and complete
3D scenes. Such as the precise outlines of overlapping ve-
hicles in Fig. 1.
After revisiting the occlusion and illumination chal-
lenges present in SSC, we propose Bi-SSC in this paper,a framework that end-to-end bi-interactive feature frame-
work. This approach utilizes two branches to preserve
global information while incorporating geometric seman-
tics to capture occlusion details. Building on the research
of LSS [ 33], we speciﬁcally develop the spatial sensory fu-
sion , which leverages multi-sensory integration and masks
to query the neighboring structure and semantic information
of visible objects. This improves the efﬁciency of trans-
ferring information from the image domain to the scenedomain. Furthermore, we propose the Cross-view Fusion
module to address the fusion bias of binocular features. By
interacting with stereo matching, this module propagatesand interacts with features from the left and right eyes, en-hancing the global representation and enabling ﬁne-grained
semantic inference within speciﬁc occluded regions. In
summary, the key contributions are as follows:
• We propose Spatial Sensory Fusion to lift image oc-
clusion region into reliable geometric and semantic infor-
mation, effectively compensate performance errors causedby occlusion.
• A method of Cross-view Fusion that propagates fea-
ture advantages to enhance scene representation.
• Experiments show that our Bi-SSC achieves state-of-
the-art results of 16.73 mIoU and 45.10 IoU on the Se-manticKITTI benchmark, outperforming all camera-only
baseline methods.
2. Related Works
Semantic Scene Completion. Semantic Scene Comple-
tion restores the complete scene by understanding the ob-jects and semantic relations, and predicting semantic infor-mation of the missing parts [ 41]. Previously, interpolation
techniques for low-level features [ 8,30] were used to ex-
tract information from the image for simple interpolation.However, these methods [ 9,18,29,30,42] are often in-
adequate when dealing with complex scenes, as they lackthe ability to comprehend the semantic information. There-fore, recent work [ 37,38,45] has started to rely on deep
learning to learn priors from large-scale datasets. Consid-ering the 3D nature of the SSC task, researchers have di-
rectly employed 3D input data to enhance algorithm perfor-
mance. Some of the studies [ 36,37,59] used point clouds
to project features in the view space, and some approaches
combine generative modeling [ 22,34] to enhance the qual-
ity of the completion results [ 10,48,55], but their focus
was solely on the scene completion task. Recently, some
research on 3D SSC [ 37,43], JS3C-Net [ 48] introduced a
point-voxel interaction module to facilitate knowledge fu-sion between semantic segmentation and the scene comple-
tion task. SSA-SC [ 50] merged the semantic information
from the segmentation branch into the completion branch.
However, the usage of extensive 3D convolutions in the 3D
methods make the model less efﬁcient and cumbersome. Inour work, we concentrate on extracting more precise geo-metric information from 2D data to aid the SSC task.
Camera-based 3D perception. The cost-effective and
easy deployment of camera-based perception has attracted
signiﬁcant attention in SSC. The rich color information inimages can extract comprehensive contextual details, assist-
ing algorithms to accurately comprehend the scene [ 7,23].
Several works have recently been proposed for SSC from
RGB image [ 16,17,25,27], such as V oxFormer [ 25], which
utilizes deformable cross-attention to align occupancy po-
sitions with multi-frame image features and subsequentlyreﬁnes voxel features through deformable self-attention.
TPVFormer [ 16] presents a transformer-based encoder that
elevates image features to 3D TPV space. However, they aresusceptible to the lack of ﬁne-grained semantic information
in voxels, resulting in inferior performance.
The BEV representation is adept at presenting the ge-
ometric conﬁguration of the scene and the distribution of
objects, enabling more effective utilization of visual infor-
mation in building the scene. To transform image featuresinto BEV features, researchers have employed the LSS [ 33]
framework along with subsequent investigations [ 14
,35,
56,60]. These studies involve projecting depth features
from images at different perspectives onto 3D space. Other
20125
outstanding studies [ 1,24,58] focus on 3D object de-
tection, such as BEVFormer [ 26], recommend a spatio-
temporal converter that aggregates features from multipleimage frames using variable attention mechanisms. ForSSC, such as OccFormer [ 57] and StereoScene [ 20] use
BEV perception to mitigate the effects of perspective trans-
formations, such that enhance spatial understanding. Re-grettably, the current BEV approaches with limited viewchallenges in understanding occluded areas for SSC.
Stereo matching based 3D perception. Driven by the
relentless advancement of deep learning, stereo-matchingmethods have enhanced their effectiveness, thereby lead-ing to substantial improvements in various 3D tasks [ 19,
40,52]. Stereo matching methods can be broadly cate-
gorized into 2D CNN-based approaches [ 44,47] and 3D
CNN-based approaches [ 4,31]. GwcNet [ 13] introduces
grouped correlations to boost feature similarity measure-
ment for more precise customer counting. Meanwhile, GA-
Net [ 54] employs a novel CNN-based feature depth aggre-
gation layer to enhance depth prediction accuracy and opti-
mize ﬁner structure and object edges. In the context of SSC
tasks, existing stereo-matching methods suffer from issues
such as non-textured regions and occlusion, which impedethe accuracy of predicted depth.
3. Methodology
3.1. Preliminary
Problem setup. With the stereo images Irgb
l,Irgb
r as in-
put, the aim is to predict the geometry and semantics of a
scene within a speciﬁc range in front. The predicted outputs
are represented using a voxel grid Y∈RH×W×Z, where
H,W,Zdenote the length, width, and height of the voxel
grid respectively. As for each voxel, it is either empty de-noted by c
0or occupied by one of the semantic classes in
C∈{c0,c1,...,c N}. Here N denotes the total number of
semantic classes. To summarize,our objective is to train amodel Y=Θ (I
rgb
l,Irgb
r)that can generate a 3D semantic
prediction Ythat closely approximates the ground truth ¯Y.
Design rationale. Motivated by visual region informa-
tion interaction and perspectives spatial difference fusion,we propose a framework for bi-interaction semantic geo-metric. Firstly, we learn neighbour structure features from
the image semantic branch and the geometric branch, to
obtain accurate features and alleviate the problems causedby occlusion. Next, leveraging the interaction between en-hanced features across stereo views to achieve global spatial
consistency, to improve voxel characterization.
Overall Architecture. We integrate semantically rich
BEV features from 2D images to construct 3D voxel fea-tures. Fig. 2 shows the overall framework of Bi-SSC. Ex-
tract 2D features from RGB images, then use the designedSpatial Sensory Fusion (SSF) to generate semantic awaregeometric features in Semantic Geometric Fusion (SGF)
module. Subsequently, these features are reﬁned and in-teractively utilized in the Cross-view Fusion (CVF) module
to propagate information across all features. Finally, the re-
sulting BEV features are up-sampled to voxel features forSSC. The speciﬁc process is outlined as follows:
• Utilizing the 2D U-Net architectures as the backbone
from RGB images, and obtaining left and right image fea-tures F
l,Fr∈RC×H×W, respectively.
• The geometric features FG∈RC×H×Wand seman-
tic features FS∈RC×H×Wthrough the corresponding
network respectively. Then use SSF to fuse FG,FSas
semantic-aware geometric features Fsag∈RDsag×H×W.
• The CVF module reﬁnes and interacts with the left
and right features, resulting in a comprehensive set ofdual-view features F
Dual∈RDd×H×W. Concurrently,
the stereo matching method generate depth features Fs∈
RDs×H×Wfor querying FDual . Ultimately, BEV features
are obtained by Mutual Interactive Aggregation (MIA) learnstereo features and reﬁned features, so that they update theirrespective advantages of different depth features.
• These features are subsequently fed into the 3D UNet
for semantic segmentation and scene completion.
The rest of this section details our innovations, the SGF
module in Sec. 3.2, the CVF module in Sec. 3.3, and the
training loss in Sec. 3.4.
3.2. Semantic Geometric Fusion Module
Mining occluding area information and reﬁning the fea-
ture are crucial for addressing camera-based SSC tasks. Theobjective of SGF is to enhance the geometric and semanticassociations to infer information about the occluded region.
Thus, we designed Spatial Sensory Fusion (SSF) into our
model inspired by Agentformer [ 53]. In the following para-
graph, we will delve into the details of this approach.
As illustrated in Fig. 3, after obtaining the image fea-
tures, we get the geometric features F
Gand semantic fea-
turesFSthrough the neural network and then they are used
as inputs to the SSF. Speciﬁcally, the geometric features aredirectly used as the query Q, and the semantic features are
used as keys K and values V . To enhance the representa-tion capabilities within the input sequences, we utilize two
sets of projections W
in,Wout∈Rl×d, to generate spa-
tial representations of inter-image and out-image seman-
tics. Then multiplied with the key to get the projected keysK
in,Kout∈Rl×d. Here, the notation l=H×W rep-
resents a sequence of feature lengths for the image, while
dcorresponds to the dimensionality of the geometric fea-
ture. This projection operation facilitates a thorough explo-
ration of the interrelationships among the input sequences,enabling a more comprehensive extraction of vital infor-mation. Following this, Q conducts separate queries into
K
inandKout to merge their dual outputs. To simulate the
20126
Semantic 
Geometric FusionInput
Stereo NetStereo Feature
3D 
U-Net
unsqueezeOutput
Cross-view Fusion
Geometric NetSemantic Net
BEVMIA
Spatial 
Sensory 
Fusion
Geometric  Feature
Spatial Sensory 
FusionSemantic FeatureFstereo
FrightFleft
Depth DistributionImage 
Encoder
FDualAttention
AttentionFrefine
Fgeometric Fstereo
Figure 2. Overall framework of Bi-SSC. Given an input stereo image, the features extracted by 2D image encoder are respectively input
to SGF and stereo network. In the SGF, our proposed SSF module is used to establish scene-level associations between geometric andsemantic features. This is followed by CVF, where the fused features from both views are interacted to provide comprehensive globalinformation. Afterward, the reﬁned features and stereo features are sent into MIA to learn from each other. The resulting BEV features areupsampled into the output space, which enables accurate occupancy segmentation for each voxel.
impact of occluded regions and enhance the robustness of
attention, we leverage an attention mask M∈Rl×d,a s
a feature intensiﬁer in the feature space, which computes
the consistency between each element Aijin the attention
weight matrix A∈Rl×dand the sequence features. Here
Aijrepresents the attention weight between the query and
two feature spaces in A. Through this operation, our model
dynamically focuses on regions with similar features, re-sulting in greater precision in semantic segmentation. We
express the process as:
M
ij=E(Ni=Nj) (1)
At this point, the output of the attention weight Aijas
A=(QKT
i)⊙Mij+(QKT
o)⊙(1−Mij) (2)
whereMijdenotes each individual element of the selection
mask, Erepresents the query function, and Ncorresponds
to the number of receptors used for processing feature as-sociation. When belonging to the same attention receptors,M
ijis set to 1, otherwise Mijis set to 0. That is, when
the rowsqkiand columns qkjof the mask Mare the same,
the attention weight Aijis computed from the mask matrix.
Notably, we use the same query to query different projec-tion spaces and complement the missing positions based onthe mask. This helps the model to pay attention to the fea-ture expression at different locations in the input sequence.
Attention Score Paims to consider more important ar-
eas to improve output, and we incorporate it into the com-putation of the attention weight matrix. This approach ex-
plicitly leverages the signiﬁcance of depth information in
reconstructing the 3D scene and adeptly adjusts the feature
weights within the attention mechanism. To achieve this,we convert each pixel value in the depth feature map into
a probabilistic form using the softmax operation. Addition-
ally, for each input image, the ﬁnal attention score is de-
termined by selecting the maximum value from each depth
dimension. The formal representation of the process is:
p=max/braceleftBigg
e
Pi
/summationtextD
j=1epj/bracerightBigg
(3)
where the softmax is applied to the entire depth dimen-
sionD, Eq. (3) represents the calculation of the weight of
each element. That is calculated by dividing the attention
score of each element by the sum of all areas. This calcula-tion effectively assigns relative importance to each element
based on its corresponding attention score, enabling seman-
tic awareness.
In the end, we update the previous attention matrix with
the attention score as follows:
Attention (Q,K,V)=softmax (A
√dk+αP)V (4)
wheredkrepresents the dimension of the query, and α
serves as the balance coefﬁcient. Thanks to the carefully
crafted SSF design, the relationship between scene-level se-
mantic geometry is effectively mined.
3.3. Cross-view Fusion Module
In our research, we introduce a novel Cross-view Fusion
module, which aligns left and right fusion features accord-ing to spatial feature similarity. Speciﬁcally, we ﬁrst mapthe left features F
sag
linto a query Ql∈RC×H×W, the
right features Fsag
r are mapped into keys Kr∈RC×H×W
20127
Query
MASK
^ŽĨƚŵĂǆ
1-MASKKey\ValueAttention Weight 
Matrix AAttention Score
Attention Map Kin
Kout
Figure 3. A illustration diagram of the SSF. Input the projection
query, and the key/value undergoes separate projections before be-ing queried. Then mask some features to simulate the effect ofan obscured area, and the resulting attention weights are calcu-
lated independently. Finally, the attention scores generated from
the query are added to obtain the attention map.
and values Vr∈RC×H×W, whereCdenotes the feature
dimension. By leveraging attention, we compute the initialfeature matching as A
lbetween features as follows:
Al(Ql,Kr,Vr)=softmax (QlKr√
C)Vr (5)
The initial right feature is subsequently incorporated into
the fusion as output FAl. Similarly, the initialized right
featureFsag
r mapping is transferred to a query Qr∈
RC×H×W, and the output FAlis mapped to the key KAl∈
RC×H×Wand values VAl∈RC×H×W. At this point, the
cross-attention of the right view with the features Alcan be
computed as:
ADual(Qr,KAl,VAl)=softmax (QrKAl√
C)VAl (6)
After several layers of attention, the dual-view features will
be updated FDual . In order to alleviate the occlusion error
of stereo matching, we combine the FDual and the stereo
featuresFsto get the reﬁned features Frefine by using SSF:
Frefine=SSF(Fs,FDual) (7)
Given the inherent bias present in left and right features,
it is imperative to utilize these features in an efﬁcient andstable manner for information exchange. Our CVF offersa mechanism to regulate the ﬂow of information, progres-sively reﬁning binocular image features.
After obtaining reﬁned features F
refine , it will mutually
enhanced with stereo features Fsand geometric features
FG. For superior aggregation, we utilize the wonderful Mu-
tual Interactive Aggregation [ 20] module. Mathematically,
the ﬁnal BEV features FBEV will be updated by the follow-
ing general equation:
FBEV=MIA(Frefine,Fs,FG) (8)Speciﬁcally, the MIA selectively ﬁlters the most reliable in-
formation from the aggregated, stereo, and geometric fea-
tures, following the standard protocol for StereoScene anal-ysis. Note that we only show the formulation of the module
for conciseness.
3.4. Loss Functions
In line with the learning objective of MonoScene [ 3] for
semantic scene completion, we employ standard semantic
loss (Lsem ) and geometric loss ( Lgeo) to provide seman-
tic and geometric supervision, respectively. Additionally,
we incorporate class weighting loss ( Lce) and binary cross-
entropy loss ( Ldepth ) to promote a sparse depth distribution.
To compute the ﬁnal training loss, we simply sum these in-
dividual losses together as:
L=Lsem+Lgeo+Lce+Ldepth (9)
4. Experiments
4.1. Experiments Setup
Dataset. We evaluated Bi-SSC on SemanticKITTI [ 2], the
KITTI Odometry Benchmark [ 12] includes 22 complex, di-
verse and challenging outdoor driving scenarios. The vox-els are generated through LIDAR scanning post-processing,
where the ground truth semantic occupancy is representedas the 256×256×32 voxel grids, and each voxel size is
0.2m×0.2m×0.2m. The voxel grid is labeled with 21 classes
(1 unknown, 1 free and 19 semantic). In the target out-
put, SemanticKITTI generates ground truth semantic voxelgrids by voxelizing a consistently registered semantic pointcloud. SemanticKITTI can use the front camera and LIDAR
points for SSC evaluation, but we use the binocular images
obtained from cam2 and cam3 as inputs, since we are think-
ing about camera-only information. Moreover, to compre-hensively evaluate the effectiveness of our model in com-plex scenarios, we introduce a SemanticKITTI-Complexdataset based on SemanticKITTI. This dataset was curated
by ﬁve researchers who carefully handpicked 300 images
from the SemanticKITTI validation set. These selected im-ages speciﬁcally emphasize challenging conditions such asoccluded areas and shaded areas, enabling a more rigorousassessment of our model’s performance.
Evaluation metrics. For quantitative evaluations, we
experimented with metrics that are widely used in the ﬁeldof SSC. We utilize IoU (Intersection over Union) to evaluate
the quality of scene completion and mIoU (mean Intersec-tion over Union) to measure the performance of semantic
segmentation, with higher values of both metrics implyingbetter performance. Note that given the speciﬁc and chal-lenging task of SSC, there is a strong interaction between
IoU and mIoU, so the desired model should have excel-
lent performance in both geometric completion and seman-
tic segmentation.
20128
Method Bi-SSC(Ours) StereoScene [20]VoxFormer-T [25]OccFormer [57]TPVFormer [16]MonoScene [3]
Input Modality Stereo Stereo Stereo Mono Mono Mono
IoU(%) 45.10 43.34 43.21 34.53 34.25 34.16
mIoU(%) 16.73 15.36 13.41 12.32 11.26 11.08
car(3.92%) 25.00 22.80 21.70 21.60 19.20 18.80
bicycle (0.03%) 1.80 3.40 1.90 1.50 1.00 0.50
motocycle (0.03%) 2.90 2.40 1.60 1.70 0.50 0.70
truck (0.16%) 6.80 2.80 3.60 1.20 3.70 3.30
other-vehicle (0.20%) 6.80 6.10 4.10 3.20 2.30 4.40
person (0.07%) 1.70 2.90 1.60 2.20 1.10 1.00
bicylist (0.07%) 3.30 2.20 1.10 1.10 2.40 1.40
motorcyclist (0.05%) 1.00 0.50 0.00 0.20 0.30 0.40
road (15.30%) 63.40 61.90 54.10 55.90 55.10 54.70
parking (1.12%) 31.70 30.70 25.10 31.50 27.40 24.80
sidewalk (11.13%) 33.30 31.20 26.90 30.30 27.20 27.10
other-grnd (0.56%) 11.20 10.70 7.30 6.50 6.50 5.70
building (14.10%) 26.60 24.20 23.50 15.70 14.80 14.40
fence (3.90%) 19.40 16.50 13.10 11.90 11.00 11.10
vegetation (39.3%) 26.10 23.80 24.40 16.80 13.90 14.90
trunk (0.51%) 10.50 8.40 8.10 3.90 2.60 2.40
terrain (9.17%) 28.9 27.00 24.20 21.30 20.40 19.50
pole (0.29%) 9.30 7.00 6.60 3.80 2.90 3.30
traf.-sign (0.08%) 8.40 7.20 5.70 3.70 1.50 2.10
Table 1. Semantic scene completion results on the SemanticKITTI [ 2] hidden test set with the state-of-the-art camera-based methods. We
signiﬁcantly outperform other methods in both IoU and mIoU, the best performing methods are marked in bold .
Implementation details. We crop RGB images to size
1280×384 and use image backbone network of Efﬁcient-NetB7 [ 3], set the input 3D feature volume size of the view
transformer to 128×128×16, with 128 channels. The gener-
ated features are upsampled to 256×256×32 for full-scale
evaluation. Unless otherwise speciﬁed, we have trainedon the SemanticKITTI dataset with 30 epochs, using theAdamW [ 28] optimizer with an initial learning rate of 1e-
4 and weight decay of 0.01. The learning rate is decayed
by a multi-step scheduler. All models are implemented onPyTorch [ 32] using a Tesla A100 GPU.
Comparison Methods. We compare the best presently
available models [ 3,16,20,25,57] that support 3D se-
mantic scene completion. Among them are camera-based
SSC methods for 2d-to-3d feature projection, such as
MonoScene [ 3], V oxFormer [ 25], OccFormer [ 57], etc.
4.2. Main Results
Quantitative Comparison. We report the performance of
Bi-SSC and RGB-inferred baselines for SemanticKITTI of-
ﬁcial benchmark (hidden test set), as shown in Tab. 1, thebest results are shown in bold. Compared to State-of-the-
art 2D methods, our method is greatly improved in terms of
geometric completion and semantic segmentation. Signif-
icantly, our approach achieved superior performance over
OccFormer, registering 4.41 mIoU (12.32 →16.73, 35.8 %)
and 10.57 IoU (34.53 →45.10, 30.61 %) increase, respec-
tively. V oxFormer-T Even using up to four temporal stereoimage pairs as inputs, our mIoU and IoU still exceeded it,
increasing by 3.32 mIoU (13.41 →16.73, 24.76 %) and 1.89
IoU (43.21 →45.10, 4.37 %), respectively. Such signiﬁcant
improvement is attributed to the fusion of geometric andsemantic features in SSF to extract information from oc-cluded regions, thereby alleviating the issue of visual blur-ring across the entire scene. For example, categories such asfence, building, and car have a lot of shielding in the scene,but they are still effectively improved. Furthermore, incomparison to StereoScene, Bi-SSC improves about +1.36mIoU/+1.76 IoU on the SemanticKITTI dataset. Bi-SSC
has the highest mIoU in almost all categories, it can be seenthat almost all the classes get effective segmentation boosts.
These results indicate that our attention module effectivelycaptures the scene geometry without resorting to a simplis-tic increase in mIoU by decreasing the IoU values.
Qualitative results. In Fig. 6, we present the visual-
ization of semantic scene completion prediction results on
the SemanticKITTI validation set using Bi-SSC. To high-
light the advantages of our method, we also include the
results of OccFormer, V oxFormer-T, and their correspond-ing ground truth values (shown in the top row). Comparedto the state-of-the-art V oxFormer-T[ 25], the spatial and se-
mantic prediction outcomes of Bi-SSC exhibit signiﬁcant
improvements. This phenomenon is particularly noticeable
in shielded areas and over extended distances. Such as inthe ﬁrst column of pictures, only our Bi-SSC is able to prop-
erly reconstruct the scene layout of the obscured road in the
20129
Ours StereoScene Inputs VoxFormer
carbicycle motocycle
truck personother-veh. bicylist
motorcyclist parkingroad sidewalk
other-grnd fencebuilding vegetation
trunk poleterrain traf.-sign
Ground 
Truth
MonoScene
Figure 4. Qualitative results from our method and others. The input image perspectives are shown at the top, and then the 3D semantic
occupancy results of Ground Truth, MonoScene [ 3], StereoScene [ 20], V oxFormer [ 25] and ours are shown in turn. Bi-SSC is able to better
complement and segment the scene layout in large-scale autopilot scenarios. Also, Bi-SSC shows satisfactory results in the completion ofsmall objects such as poles and occluded regions.
distance and the cars and trees in the shadows.
Our method outperforms MonoScene, OccFormer, and
V oxFormer in comprehending scene-level layout and oc-
cluded regions. Moreover, Bi-SSC excels in recoveringﬁne-grained structures and reasoning about interactions be-
tween neighboring semantic classes. For example, in the
complex occlusion scene in the fourth column of pictures,
our Bi-SSC can generate a more complete road extension
and accurately segment the outline of each object. These
advancements can be attributed to the effective aggregationof geometric with semantic features achieved by SSF.
4.3. Ablation Study
We conducted ablation experiments on the SemanticKITTIvalidation set to evaluate the impact of our Spatial SensoryFusion, Cross-view Fusion module, and a contrast experi-ment for complex shaded areas.
Architectural Components. Tab. 2 presents a compre-
hensive analysis of how each architectural component con-
tributes to achieving optimal results. The inclusion of the
SSF module for feature fusion demonstrates a substantialenhancement in both geometric and semantic estimation,with a notable increase of 0.5 IoU and 1.04 mIoU, respec-tively. Furthermore, Thanks to the dual view receptive ﬁeld
and features aggregation, the CVF leads to signiﬁcant im-
provements in geometric prediction (+0.87 IoU), while hav-
ing a relative impact on semantic prediction (+0.54 mIoU).Finally, the incorporation of the MIA module further con-
tributes to the overall accuracy, ultimately enhancing theperformance in both geometric and semantic estimation.
Effectiveness of SSF module. We conducted in-depth
ablation study on the SSF module to validate our designchoices. The corresponding results are presented in Tab. 3.Initially, we removed the attention score design and com-
pared it against other baseline methods. The analysis re-
vealed that the removal of the attention score led to a no-ticeable impact on semantic segmentation, as indicated by
the reduction in mIoU. This observation strongly supports
the attention score is crucial for compensating geometric
information at the scene level. Furthermore, Tab. 3 showsimproved performance by varying query projection key val-ues. It is worth noting can help improve the performance,
due to the interaction of two different sets of distinct fea-ture spaces. An interesting phenomenon is that only mIoU
decreases when we ablate the Mask, which proves that it isuseful for us to simulate the occluded area with the mask.
20130
Architecture Components IoU(%) mIoU( %)
Ours 44.88 16.39
Ours w/o SSF 44.38 15.35
Ours w/o CVF 44.01 15.85
Ours w/o MIA 44.77 16.21
Table 2. Ablation study for architecture. Results are reported on
SemanticKITTI val.
Method IoU(%) mIoU( %)
SSF 44.88 16.39
SSF w/o Attention Score 44.78 15.94
SSF w/o Dual Query 44.64 15.75
SSF w/o Mask 44.84 15.52
Attention 43.56 15.39
Table 3. Ablation study for Spatial Sensory Fusion. Our SSF
module performs best, and each block has played its role.
Method IoU(%) decline( ↓)mIoU(%) decline( ↓)
MonoScene [ 3] 37.12 36.80 (0.32) ↓ 11.50 10.65 (0.85) ↓
V oxFormer-T [ 25]44.15 44.05 ( 0.1)↓ 13.35 12.60 (0.75) ↓
StereoScene [ 20] 43.85 42.54 (1.31) ↓ 15.43 13.93 (1.50) ↓
Bi-SSC(Ours) 44.88 44.78 (0.1)↓ 16.39 16.0 (0.39 )↓
Table 4. A comparison against the state-of-the-art method in
SemanticKITTI-Complex, where our approach exhibited no sig-niﬁcant degradation in performance.
In comparison to the baseline attention mechanism, our SSF
has been proven to be an effective attention mechanism for
occluded regions in the SSC task.
Qualitative results of ablation studies. As illustrated in
Fig. 5, compared with the full pipeline (a), the V oxformer-
T (b) incorrect reconstruction of occluded roads. And re-moving SSF (c) will not learn neighboring structures (Car
and road shelter structure in the yellow box), result in roadsoccluded by buildings cannot be reconstructed (blue cir-
cle). Removing CVF module distorts the geometry of the
scene, and they both affect the details in the result. As inFig. 5 (d), the road reconstruction lacks integrity. It proves
the CVF module utilizes the spatial differences of differentviews (red box) to ﬁll in road structures.
Our superiority over others in SemanticKITTI-
Complex dataset. To ensure the validity of the experiment,
other state-of-the-art methods were tested using their pre-trained models under identical conditions. The results in
Tab. 4 demonstrate that our method surpasses other camera-
based methods. Speciﬁcally, in this challenging dataset, Bi-
SSC achieves a mIoU score of 16.0, which is 26.99 %higher
than V oxFormer-T and 14.86 %higher than StereoScene,
the most advanced methods in their respective categories.
In addition, the decrease in mIoU for our method is only
0.39, compared to a decrease of 0.75 for V oxFormer-T and a
Left Image
(a) Our Full Pipeline (b) VoxFormer-T(CVPR2023)
(d) w/o Cross-view Fusion (c) w/o Spatial Sensory Fusion
car parking road sidewalk other-grnd fence building vegetation trunk pole terrain traf.-sign
Right Image
Figure 5. Visual results from the ablation study.
Ground truth Ours VoxFormer-T
Figure 6. Qualitative results in SemanticKITTI-Complex
dataset. Our approach better captures the layout of the scene, it
reconstructs and estimates the geometry of the obscured roads andshaded areas of the car.
signiﬁcant decrease for StereoScene. This indicates that our
method exhibits better robustness in challenging conditions.More importantly, Bi-SSC demonstrates the improvements
in the area of occluded and shaded are signiﬁcant, as shown
in Fig. 6. Given the signiﬁcance of accurate prediction in
fuzzy environments, particularly in the ﬁeld of autonomous
driving, Bi-SSC should be more popular in this domain.
5. Conclusion
In this paper, we introduce Bi-SSC, an advanced camera-based framework for 3D semantic scene completion via
geometric-semantic bidirectional fusion. We propose a Spa-tial Sensory Fusion that adeptly captures ﬁne-grained fea-tures and scene-level information within two sets of imagefeature spaces. Moreover, we leverage Cross-view Fusionfor dense geometric information fusion. As a result, Bi-
SSC achieves a new SOT A performance in semantic scenecompletion on the SemanticKITTI, particularly excelling in
those invisible and shaded areas.
Acknowledgement. The work is supported by the Sci-
ence and Technology Innovation 2030 - “New Generation
Artiﬁcial Intelligence” Major Project (2021ZD40300), Na-
tional Natural Science Foundation of China (No.62202151)
and the Fundamental Research Funds for the Central Uni-
versities.
20131
References
[1] Xuyang Bai, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun
Chen, Hongbo Fu, and Chiew-Lan Tai. Transfusion: Ro-bust lidar-camera fusion for 3d object detection with trans-formers. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 1090–1099,
2022. 3
[2] Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-
zel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Se-
mantickitti: A dataset for semantic scene understanding oflidar sequences. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 9297–9307,
2019. 5, 6
[3] Anh-Quan Cao and Raoul de Charette. Monoscene: Monoc-
ular 3d semantic scene completion. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3991–4001, 2022. 1, 5, 6, 7, 8
[4] Jia-Ren Chang and Y ong-Sheng Chen. Pyramid stereo
matching network. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 5410–5418,
2018. 3
[5] Xiaokang Chen, Kwan-Y ee Lin, Chen Qian, Gang Zeng, and
Hongsheng Li. 3d sketch-aware semantic scene comple-tion via semi-supervised structure prior. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition , pages 4193–4202, 2020. 1
[6] Ran Cheng, Christopher Agia, Y uan Ren, Xinhai Li, and Liu
Bingbing. S3cnet: A sparse semantic scene completion net-work for lidar point clouds. In Conference on Robot Learn-
ing, pages 2148–2161. PMLR, 2021. 1
[7] Angela Dai, Christian Diller, and Matthias Nießner. Sg-nn:
Sparse generative neural networks for self-supervised scenecompletion of rgb-d scans. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 849–858, 2020. 2
[8] James Davis, Stephen R Marschner, Matt Garr, and Marc
Levoy. Filling holes in complex surfaces using volumet-ric diffusion. In Proceedings. First international symposium
on 3d data processing visualization and transmission , pages
428–441. IEEE, 2002. 2
[9] Raoul de Charette and Sotiris Manitsaris. 3d reconstruction
of deformable revolving object under heavy hand interaction.arXiv preprint arXiv:1908.01523 , 2019. 2
[10] Michael Firman, Oisin Mac Aodha, Simon Julier, and
Gabriel J Brostow. Structured prediction of unobserved vox-
els from a single depth image. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition ,
pages 5431–5440, 2016. 2
[11] Sourav Garg, Niko S ¨underhauf, Feras Dayoub, Douglas
Morrison, Akansel Cosgun, Gustavo Carneiro, Qi Wu, Tat-Jun Chin, Ian Reid, Stephen Gould, et al. Semanticsfor robotic mapping, perception and interaction: A survey.F oundations and Trends® in Robotics , 8(1–2):1–224, 2020.
1
[12] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmarksuite. In 2012 IEEE conference on computer vision and pat-
tern recognition , pages 3354–3361. IEEE, 2012. 5
[13] Xiaoyang Guo, Kai Y ang, Wukui Y ang, Xiaogang Wang, and
Hongsheng Li. Group-wise correlation stereo network. InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition , pages 3273–3282, 2019. 3
[14] Anthony Hu, Zak Murez, Nikhil Mohan, Sof ´ıa Dudas, Jef-
frey Hawke, Vijay Badrinarayanan, Roberto Cipolla, andAlex Kendall. Fiery: Future instance prediction in bird’s-eye view from surround monocular cameras. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-sion , pages 15273–15282, 2021. 2
[15] Junjie Huang, Guan Huang, Zheng Zhu, Y un Y e, and Dalong
Du. Bevdet: High-performance multi-camera 3d object de-
tection in bird-eye-view. arXiv preprint arXiv:2112.11790 ,
2021. 2
[16] Y uanhui Huang, Wenzhao Zheng, Y unpeng Zhang, Jie Zhou,
and Jiwen Lu. Tri-perspective view for vision-based 3d
semantic occupancy prediction. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 9223–9232, 2023. 2, 6
[17] Haoyi Jiang, Tianheng Cheng, Naiyu Gao, Haoyang Zhang,
Wenyu Liu, and Xinggang Wang. Symphonize 3d semantic
scene completion with contextual instance queries. arXiv
preprint arXiv:2306.15670 , 2023. 2
[18] Michael Kazhdan, Matthew Bolitho, and Hugues Hoppe.
Poisson surface reconstruction. In Proceedings of the fourth
Eurographics symposium on Geometry processing , page 0,
2006. 2
[19] Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter
Henry, Ryan Kennedy, Abraham Bachrach, and Adam Bry.
End-to-end learning of geometry and context for deep stereo
regression. In Proceedings of the IEEE international confer-
ence on computer vision , pages 66–75, 2017. 3
[20] Bohan Li, Y asheng Sun, Xin Jin, Wenjun Zeng, Zheng Zhu,
Xiaoefeng Wang, Y unpeng Zhang, James Okae, Hang Xiao,and Dalong Du. Stereoscene: Bev-assisted stereo match-
ing empowers 3d semantic scene completion. arXiv preprint
arXiv:2303.13959 , 2023. 2, 3, 5, 6, 7, 8
[21] Jie Li, Kai Han, Peng Wang, Y u Liu, and Xia Y uan.
Anisotropic convolutional networks for 3d semantic scene
completion. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 3351–
3359, 2020. 1
[22] Ruihui Li, Xianzhi Li, Ka-Hei Hui, and Chi-Wing Fu. Sp-
gan: Sphere-guided 3d shape generation and manipulation.
ACM Transactions on Graphics (TOG) , 40(4):1–12, 2021. 2
[23] Shichao Li and Kwang-Ting Cheng. Joint stereo 3d object
detection and implicit surface reconstruction. arXiv preprint
arXiv:2111.12924 , 2021. 2
[24] Yinhao Li, Zheng Ge, Guanyi Y u, Jinrong Y ang, Zengran
Wang, Y ukang Shi, Jianjian Sun, and Zeming Li. Bevdepth:
Acquisition of reliable depth for multi-view 3d object detec-tion. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence , pages 1477–1485, 2023. 2, 3
[25] Yiming Li, Zhiding Y u, Christopher Choy, Chaowei Xiao,
Jose M Alvarez, Sanja Fidler, Chen Feng, and Anima Anand-
20132
kumar. V oxformer: Sparse voxel transformer for camera-
based 3d semantic scene completion. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 9087–9098, 2023. 1, 2, 6, 7, 8
[26] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-
hao Sima, Tong Lu, Y u Qiao, and Jifeng Dai. Bevformer:
Learning bird’s-eye-view representation from multi-cameraimages via spatiotemporal transformers. In European con-
ference on computer vision , pages 1–18. Springer, 2022. 2,
3
[27] Zhiqi Li, Zhiding Y u, David Austin, Mingsheng Fang, Shiyi
Lan, Jan Kautz, and Jose M Alvarez. Fb-occ: 3d occupancy
prediction based on forward-backward view transformation.
arXiv preprint arXiv:2307.01492 , 2023. 2
[28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 , 2017. 6
[29] Andrew Nealen, Takeo Igarashi, Olga Sorkine, and Marc
Alexa. Laplacian mesh optimization. In Proceedings of
the 4th international conference on Computer graphics and
interactive techniques in Australasia and Southeast Asia ,
pages 381–389, 2006. 2
[30] Richard A Newcombe, Shahram Izadi, Otmar Hilliges,
David Molyneaux, David Kim, Andrew J Davison, Pushmeet
Kohi, Jamie Shotton, Steve Hodges, and Andrew Fitzgibbon.Kinectfusion: Real-time dense surface mapping and track-ing. In 2011 10th IEEE international symposium on mixed
and augmented reality , pages 127–136. Ieee, 2011. 2
[31] Haesol Park and Kyoung Mu Lee. Look wider to match im-
age patches with convolutional neural networks. IEEE Signal
Processing Letters , 24(12):1788–1792, 2016. 3
[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-
perative style, high-performance deep learning library. Ad-
vances in neural information processing systems , 32, 2019.
6
[33] Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encoding
images from arbitrary camera rigs by implicitly unproject-
ing to 3d. In Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceed-ings, Part XIV 16 , pages 194–210. Springer, 2020. 2
[34] Stefan Popov, Pablo Bauszat, and Vittorio Ferrari. Corenet:
Coherent 3d scene reconstruction from a single rgb image. In
Computer Vision–ECCV 2020: 16th European Conference,Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16 ,
pages 366–383. Springer, 2020. 2
[35] Cody Reading, Ali Harakeh, Julia Chae, and Steven L
Waslander. Categorical depth distribution network formonocular 3d object detection. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8555–8564, 2021. 2
[36] Christoph B Rist, David Schmidt, Markus Enzweiler, and
Dariu M Gavrila. Scssnet: Learning spatially-conditioned
scene segmentation on lidar point clouds. In 2020 IEEE In-
telligent V ehicles Symposium (IV) , pages 1086–1093. IEEE,
2020. 2
[37] Christoph B Rist, David Emmerichs, Markus Enzweiler, and
Dariu M Gavrila. Semantic scene completion using localdeep implicit functions on lidar data. IEEE transactions
on pattern analysis and machine intelligence , 44(10):7205–
7218, 2021. 2
[38] Luis Roldao, Raoul de Charette, and Anne V erroust-Blondet.
Lmscnet: Lightweight multiscale 3d semantic completion.In2020 International Conference on 3D Vision (3DV) , pages
111–119. IEEE, 2020. 2
[39] Luis Roldao, Raoul De Charette, and Anne V erroust-
Blondet. 3d semantic scene completion: A survey. Interna-
tional Journal of Computer Vision , 130(8):1978–2005, 2022.
1
[40] Amit Shaked and Lior Wolf. Improved stereo matching with
constant highway networks and reﬂective conﬁdence learn-ing. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 4641–4650, 2017. 3
[41] Shuran Song, Fisher Y u, Andy Zeng, Angel X Chang, Mano-
lis Savva, and Thomas Funkhouser. Semantic scene com-pletion from a single depth image. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion , pages 1746–1754, 2017. 1, 2
[42] Minhyuk Sung, Vladimir G Kim, Roland Angst, and
Leonidas Guibas. Data-driven structural priors for shape
completion. ACM Transactions on Graphics (TOG) , 34(6):
1–11, 2015. 2
[43] Jiaxiang Tang, Xiaokang Chen, Jingbo Wang, and Gang
Zeng. Not all voxels are equal: Semantic scene comple-
tion from the point-voxel perspective. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence , pages 2352–
2360, 2022. 2
[44] Vladimir Tankovich, Christian Hane, Yinda Zhang, Adarsh
Kowdle, Sean Fanello, and Soﬁen Bouaziz. Hitnet: Hierar-chical iterative tile reﬁnement network for real-time stereomatching. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 14362–
14372, 2021. 3
[45] Shun-Cheng Wu, Keisuke Tateno, Nassir Navab, and Fed-
erico Tombari. Scfusion: Real-time incremental scene recon-
struction with semantic completion. In 2020 International
Conference on 3D Vision (3DV) , pages 801–810. IEEE,
2020. 2
[46] Zhaoyang Xia, Y ouquan Liu, Xin Li, Xinge Zhu, Y uexin
Ma, Yikang Li, Y uenan Hou, and Y u Qiao. Scpnet: Se-mantic scene completion on point cloud. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 17642–17651, 2023. 1
[47] Haofei Xu and Juyong Zhang. Aanet: Adaptive aggrega-
tion network for efﬁcient stereo matching. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition , pages 1959–1968, 2020. 3
[48] Xu Y an, Jiantao Gao, Jie Li, Ruimao Zhang, Zhen Li, Rui
Huang, and Shuguang Cui. Sparse single sweep lidar pointcloud segmentation via learning contextual shape priors fromscene completion. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence , pages 3101–3109, 2021. 2
[49] Lei Y ang, Kaicheng Y u, Tao Tang, Jun Li, Kun Y uan, Li
Wang, Xinyu Zhang, and Peng Chen. Bevheight: A robustframework for vision-based roadside 3d object detection. In
20133
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 21611–21620, 2023. 2
[50] Xuemeng Y ang, Hao Zou, Xin Kong, Tianxin Huang, Y ong
Liu, Wanlong Li, Feng Wen, and Hongbo Zhang. Seman-tic segmentation-assisted scene completion for lidar pointclouds. In 2021 IEEE/RSJ International Conference on Intel-
ligent Robots and Systems (IROS) , pages 3555–3562. IEEE,
2021. 1, 2
[51] Jiawei Y ao, Chuming Li, Keqiang Sun, Yingjie Cai, Hao
Li, Wanli Ouyang, and Hongsheng Li. Ndc-scene: Boost
monocular 3d semantic scene completion in normalized de-vice coordinates space. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 9455–
9465, 2023. 2
[52] Xiaoqing Y e, Jiamao Li, Han Wang, Hexiao Huang, and Xi-
aolin Zhang. Efﬁcient stereo matching leveraging deep lo-
cal and context information. IEEE Access , 5:18745–18755,
2017. 3
[53] Y e Y uan, Xinshuo Weng, Y anglan Ou, and Kris M Kitani.
Agentformer: Agent-aware transformers for socio-temporalmulti-agent forecasting. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 9813–
9823, 2021. 3
[54] Feihu Zhang, Victor Prisacariu, Ruigang Y ang, and
Philip HS Torr. Ga-net: Guided aggregation net for end-to-
end stereo matching. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
185–194, 2019. 3
[55] Jiahui Zhang, Hao Zhao, Anbang Y ao, Y urong Chen, Li
Zhang, and Hongen Liao. Efﬁcient semantic scene comple-tion network with spatial group convolution. In Proceedings
of the European Conference on Computer Vision (ECCV) ,
pages 733–749, 2018. 2
[56] Y unpeng Zhang, Zheng Zhu, Wenzhao Zheng, Junjie Huang,
Guan Huang, Jie Zhou, and Jiwen Lu. Beverse: Uniﬁed per-
ception and prediction in birds-eye-view for vision-centricautonomous driving. arXiv preprint arXiv:2205.09743 ,
2022. 2
[57] Y unpeng Zhang, Zheng Zhu, and Dalong Du. Occformer:
Dual-path transformer for vision-based 3d semantic occu-pancy prediction. arXiv preprint arXiv:2304.05316 , 2023.
2, 3, 6
[58] Haimei Zhao, Qiming Zhang, Shanshan Zhao, Jing Zhang,
and Dacheng Tao. Bevsimdet: Simulated multi-modal distil-
lation in bird’s-eye view for multi-view 3d object detection.
arXiv preprint arXiv:2303.16818 , 2023. 3
[59] Min Zhong and Gang Zeng. Semantic point completion net-
work for 3d semantic scene completion. In ECAI 2020 , pages
2824–2831. IOS Press, 2020. 2
[60] Brady Zhou and Philipp Kr ¨ahenb ¨uhl. Cross-view transform-
ers for real-time map-view semantic segmentation. In Pro-
ceedings of the IEEE/CVF conference on computer visionand pattern recognition , pages 13760–13769, 2022. 2
[61] Hao Zou, Xuemeng Y ang, Tianxin Huang, Chujuan Zhang,
Y ong Liu, Wanlong Li, Feng Wen, and Hongbo Zhang. Up-to-down network: Fusing multi-scale context for 3d semantic
scene completion. In 2021 IEEE/RSJ International Confer-ence on Intelligent Robots and Systems (IROS) , pages 16–23.
IEEE, 2021. 1
20134
