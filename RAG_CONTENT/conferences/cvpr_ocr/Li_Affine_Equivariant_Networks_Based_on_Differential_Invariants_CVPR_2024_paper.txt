Afﬁne Equivariant Networks Based on Differential Invariants
Yikang Li1, Yeqing Qiu2,3, Yuxuan Chen1, Lingshen He1, Zhouchen Lin1,4,5*
1National Key Lab of General AI, School of Intelligence Science and Technology, Peking University
2The Chinese University of Hong Kong, Shenzhen3Shenzhen Research Institute of Big Data
4Institute for Artiﬁcial Intelligence, Peking University5Peng Cheng Laboratory
liyk18@pku.edu.cn, yeqingqiu@link.cuhk.edu.cn, edmondx.chen@gmail.com
lingshenhe@pku.edu.cn, zlin@pku.edu.cn
Abstract
Convolutional neural networks beneﬁt from translation
equivariance, achieving tremendous success. Equivariant
networks further extend this property to other transforma-
tion groups. However, most existing methods require dis-
cretization or sampling of groups, leading to increased
model sizes for larger groups, such as the afﬁne group.
In this paper, we build afﬁne equivariant networks based
on differential invariants from the viewpoint of symmetric
PDEs, without discretizing or sampling the group. To ad-
dress the division-by-zero issue arising from fractional dif-
ferential invariants of the afﬁne group, we construct a new
kind of afﬁne invariants by normalizing polynomial rela-
tive differential invariants to replace classical differential
invariants. For further ﬂexibility, we design an equivariant
layer, which can be directly integrated into convolutional
networks of various architectures. Moreover, our frame-
work for the afﬁne group is also applicable to its continu-
ous subgroups. We implement equivariant networks for the
scale group, the rotation-scale group, and the afﬁne group.
Numerical experiments demonstrate the outstanding perfor-
mance of our framework across classiﬁcation tasks involv-
ing transformations of these groups. Remarkably, under the
out-of-distribution setting, our model achieves a 3.37% im-
provement in accuracy over the main counterpart affConv
on the affNIST dataset.
1. Introduction
The success of convolutional neural networks (CNNs)
can be attributed to their utilization of translation symme-
try. This profound insight emphasizes the signiﬁcance of
incorporating symmetry priors into the design of models.
With this insight, equivariant networks extend the exploita-
tion of more symmetries, leading to great improvement
*Corresponding author.on performance and efﬁciency. Development of equivari-
ant networks begins with the approach of group convolu-
tions, which views feature maps as functions deﬁned on a
group and conducts convolution operation over the group
[3,64]. Further advancements in equivariant networks ef-
fectively achieve equivariance on the Euclidean group and
its subgroups [ 4,11,12,62,63,68,69]. However, ex-
isting methods have certain limitations when dealing with
more complicated groups. One representative group is the
afﬁne group. While group convolutions typically require
discretization of continuous groups, it becomes impracti-
cal for the afﬁne group due to its high dimension. Finzi
et al. [ 16] conduct group convolutions by sampling from
Haar measure on the group. But it relies on easy access
to Haar measure, which is unsuitable for the afﬁne group.
MacDonald et al. [ 37] overcome the limitation by comput-
ing the integral on the Lie algebra, thereby obtaining the
afﬁne equivariant model, affConv. Nevertheless, this ap-
proach still requires sampling from the group and encoun-
ters an exponential growth in memory requirements as the
number of convolutional layers increases. A recent work
[39] addresses the issue of exponential memory growth, but
it also relies on sampling based on speciﬁc measures. In
particular, sampling from the GL (n,R)-invariant measure
of positive deﬁnite matrices is infeasible, and the authors
adopt the log-normal distribution as a substitute, leading to
imperfect equivariance theoretically.
In another branch, some works adopt partial differential
operators (PDOs) to design equivariant networks [ 25,27,
50]. They achieve equivariance on Euclidean groups by im-
posing constraints on the weights of PDOs. In fact, spe-
ciﬁc functional combinations of partial derivatives remain
constant under group actions — a concept known as “dif-
ferential invariants.” Under the guidance of the differential
invariant theory, Liu et al. [ 33,34] design a shift and rota-
tionally equivariant system of learnable partial differential
equations (PDEs) with linear combinations of differential
invariants. The evolution process of PDEs can be used to
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
5546
InvariantsBlockInvariantsBlockInvariantsBlock
ConvInvariantsBlockInvariantsBlockInvariantsBlockInvariantsBlockInvariantsBlockInvariantsBlockConv
Figure 1. InvarPDEs-Net consists of iterative processes of multiple symmetric PDEs constructed with invariants. We link them by linearly
combining the output of one PDE to match the dimension of the subsequent one, which can be implemented with 1⇥1convolutions.
solve multiple vision problems. Subsequent works extend
the approach to more tasks and further develop the models
[14,47,49,78]. However, these efforts also concentrate on
equivariance of Euclidean groups, and the full potential of
differential invariants in handling more general groups has
yet to be explored.
In this paper, we construct afﬁne equivariant networks
based on differential invariants from the viewpoint of sym-
metric PDEs, without discretizing or sampling the group.
Inspired by learnable PDEs [ 33,34], we regard image data
as smooth functions on the 2D plane and model the equiv-
ariant inference process of feature extraction as an evolving
system governed by symmetric PDEs. The differential in-
variant theory reveals that, given a group G, a PDE admits
Gas a symmetry group if and only if the PDE consists of
fundamental differential invariants of the group G[42]. To
construct learnable symmetric PDEs, we can precompute
a complete set of fundamental differential invariants of the
group, and then employ multilayer perceptrons (MLPs) to
combine them into equations, leveraging the universal ap-
proximation capability of neural networks. However, dif-
ferential invariants of the afﬁne group may take the form of
fractional polynomials, potentially leading to the division-
by-zero issue in practice. Nonetheless, we notice that afﬁne
differential invariants can be represented by polynomial rel-
ative differential invariants. Building on this observation,
we propose a technique to construct a new kind of afﬁne in-
variants by normalizing polynomial relative differential in-
variants with a special norm, thus replacing the fundamental
differential invariants. These new invariants not only avoid
the division-by-zero issue but also retain more information.
To discretize the symmetric PDE (not the afﬁne group), we
approximate the temporal derivatives by forward difference
and approximate the spatial derivatives by Gaussian deriva-
tives, resulting in an iterative process that can be viewed as
a feed-forward deep equivariant network.
To equip our network with adaptability to varying chan-
nel numbers, similar to other modern networks, we sequen-
tially stack iterative processes of multiple learnable sym-metric PDEs with different dimensions. We connect them
by linearly combining the output channels of one PDE to
match channel numbers of the subsequent PDE. Thus, the
output of one PDE can serve as the input of the subse-
quent one. This approach allows us to create an equivariant
network with varying channel numbers, which consists of
multiple symmetric PDEs constructed with invariants. We
name it InvarPDEs-Net (see Figure 1). For further ﬂexibil-
ity, we extract a block from the iterative process and modify
it into an equivariant layer, offering the freedom to specify
input and output channel numbers. The layer can serve as
a drop-in replacement for convolutional networks of vari-
ous architectures. We name it InvarLayer . Our framework
for constructing equivariant networks of the afﬁne group is
also applicable to its continuous subgroups. We implement
equivariant networks for the scale group, the rotation-scale
group and the afﬁne group. Empirical experiments on clas-
siﬁcation tasks involving transformations of these groups
demonstrate the outstanding performance of our method.
We summarize our main contributions as follows:
•From the viewpoint of symmetric PDEs, we construct
afﬁne equivariant networks based on differential invari-
ants. It is the ﬁrst time that afﬁne equivariance for net-
works is achieved without discretizing or sampling the
group. Consequently, we overcome the limitation on net-
work depth encountered by affConv [ 37].
•We propose a technique to construct a new kind of afﬁne
invariants by normalizing polynomial relative differential
invariants with a special norm, which can be incorporated
into our networks and enhance numerical stability.
•For further ﬂexibility, we also design an equivariant layer,
InvarLayer, which serves as a drop-in replacement for
convolutional networks of various architectures.
•Our framework for constructing afﬁne equivariant net-
works is also applicable to its continuous subgroups. We
implement equivariant networks for three non-Euclidean
groups: the scale group, the rotation-scale group and the
afﬁne group. Extensive experiments demonstrate the out-
standing performance of our framework. Particularly, we
5547
achieve a 3.37% improvement in accuracy compared with
affConv [ 37] on the public affNIST1dataset under the
out-of-distribution setting.2
2. Related works
Currently there are two mainstream methods for con-
structing group equivariant networks. One approach stems
from Cohen and Welling [ 3], which treats feature maps as
functions deﬁned on a group. Some works extend this ap-
proach to subgroups of Euclidean groups on various do-
mains, such as rotation on the 2D plane [ 1,24,31,32],
rotation over the 3D space [ 11,12,68,69], symmetries
on spheres [ 5,9,10] and surfaces [ 6,7]. Besides, with
some proper approximations, some works utilize this ap-
proach to handle non-compact groups, such as the scale
group [ 48,54,67,70,79] and Lie groups [16,37,39].
The other approach follows the steerable CNNs framework
[4,62–64], which views feature maps as vector ﬁelds. This
approach has also been further applied to subgroups of Eu-
clidean groups on the 2D plane [ 20,58,59,71,76], the 3D
space [ 17,63], and spheres [ 13,65]. In addition, similar
to the ﬁrst approach, this approach has also been extended
to the scale group [ 19,41,53] and the rotation-scale group
[18,56,75].
Besides the above approaches, some works utilize PDOs
with learnable coefﬁcients to design equivariant neural net-
works on 2D plane [ 25,27,50]. Besides, PDOs can also
be applied to spheres [ 28,51], volumetric data [ 52] and sur-
faces [ 66]. Differential invariants, as a specialized form of
partial differential operators, hold a distinctive role in the
ﬁeld of image processing [ 22,40,45,57,60]. The equiv-
ariant method of moving frames offers an elegant tool to
derive differential invariants of a given group [ 15,43,44].
Wang et al. [ 60] provide a practical and simpliﬁed approach
for deriving relative afﬁne differential invariants. Theoret-
ical links reveal that differential invariants are closely in-
tertwined with symmetric PDEs [ 42]. Building upon this
connection, Liu et al. [ 33,34] design a shift and rotationally
equivariant system comprised of learnable PDEs with linear
combinations of fundamental differential invariants. Subse-
quently, some works apply learnable PDEs to feature learn-
ing and extensive vision tasks [ 14,78]. Some works further
develop the approach and create equivariant networks on
Euclidean groups [ 47,49]. Additionally, some researchers
draw inspiration from PDEs to design deep convolutional
networks [ 35,36].
3. Theoretical framework
In this section, we propose a new framework based on
differential invariants to achieve equivariance of the afﬁne
1https://www.cs.toronto.edu/ tijmen/affNIST/
2Our code: https://github.com/Liyk127/Afﬁne-Equivariant-Networksgroup. We also describe some extensions of the framework
and how they can be implemented.
3.1. Basic concepts and notations
To explicitly present the proposed method and theoreti-
cal derivation in the following, we ﬁrst give a preliminary
introduction to concepts involved and notations used.
Inputs and intermediate feature maps of neural networks
can be modeled as vector functions deﬁned on a continuous
domain, e.g. the 2D plane for image data. Each layer of
the network thereby can be regarded as an operator. In this
paper, we study F={u  u:X!Rn}as the set of smooth
functions deﬁned on X=R2,which are constant outside
a compact set. Given a group Gacting on X, it naturally
induces a group action on F,i.e.(g·u)(x)= u(g 1·x),
where g2G,x2X,u2F.
Equivariance indicates that the output of a mapping
transforms in accordance with transformation of the input.
Deﬁnition 1 LetGbe a group acting on function sets F
andF0. An operator  :F!F0is said to be equivariant
with respect to G, if [g·u]=g· [u],8g2G,u2F.
Transitivity is an important property of equivariance. As a
result, when equivariant operators are composed together,
they still possess equivariance.
The concept of invariants is crucial and widely applied
in various ﬁelds. Invariants extract some symmetric infor-
mation and remain constant on the orbits of group actions.
Here we give the deﬁnition of invariants below.
Deﬁnition 2 LetGbe a group acting on X, and F=
{u  u:X!Rn}be a function set deﬁned on X. An
invariant ofGis a map I:X⇥F ! Rsuch that
8u2F,x2X,g2G,
I(g·x,g·u)=I(x,u). (1)
We call I,(I1,. . . ,Ik)>ak-dimensional invariant of G,
ifI1,. . . ,Ikare invariants of G.
Invariants under operation of postcomposition maintain the
property of invariance, which can be formulated as follows:
Proposition 3 LetI:X⇥F ! Rkbe a k-dimensional
invariant, and h:Rk!Rk0be a k0-dimensional vector
function. Then h Iis ak0-dimensional invariant.
Intuitively, invariants and equivariant operators some-
how both imply symmetry of group G. In fact, we can con-
struct an equivariant operator with an invariant.
Proposition 4 LetI:X⇥F ! Rkbe a k-dimensional
invariant of G, where F={u  u:X!Rn}. View I(·,u)
as a k-dimensional function in F0={v  v:X!Rk},
and deﬁne an operator ˆI:F!F0such that
ˆI[u],I(·,u). (2)
5548
Then ˆIis equivariant.
Proof. 8u2F,g2G, x2X, we have
ˆI[g·u](x)=I(x, g·u)
=I(g 1·x,u)
=ˆI[u](g 1·x)
=(g·ˆI[u])(x).
Therefore, ˆI[g·u]=g·ˆI[u]. ⇤
It is worth noting that the equivariant operator ˆIcomposed
with a function hremains equivariant. Speciﬁcally, the op-
erator u7!h ˆI[u]is actually equivalent to u7!ˆIh[u],
where Ih,h Iis still an invariant according to Propo-
sition 3.
As a special type of invariants, a differential invariant is a
quantity involving the derivatives of functions that remains
unchanged under the prolongation of group actions.
Deﬁnition 5 Letfbe a smooth function and I(x,u),
f(x,u(x),ru(x),. . . ,rdu(x))). IfIis an invariant, we
callIad-th order differential invariant .
As we always require translation equivariance by default,
it is sufﬁcient to consider differential invariants in the
form I(x,u),f(u(x),ru(x),. . . ,rdu(x))), omitting
the term x[34,61]. According to the differential invariant
theory [ 42], there are ﬁnite independent differential invari-
ants up to the d-th order such that any d-th order differential
invariant can be expressed by these differential invariants.
We call them fundamental differential invariants .
In this paper, we focus on the afﬁne group, which is
ubiquitous in computer vision. The afﬁne group consists
of translation and invertible linear transformations. Denote
the afﬁne group as G, and any element g2Gcan be rep-
resented as g=(A,b), where A2R2⇥2is invertible and
b2R2. Then g2Gacts on R2via the following way:
g·x=Ax+b,8x2R2.
3.2. From symmetric PDE to equivariant network
Inspired by learnable PDEs, we model the process of fea-
ture extraction as the evolution process governed by PDEs
[14,33–36,78]. If the utilized PDE exhibits symmetry, the
resultant feature extraction process will inherently possess
equivariance [ 14,33,34,78].
Let˜F={˜u  ˜u:[ 0,T]⇥R2!Rn}be a set of smooth
functions involving a temporal variable t2[0,T]and a
spatial variable x2R2. We focus on high-dimensional
evolutionary PDEs in the following form:
@˜u
@t=F 
t,x,˜u,rx˜u,. . . ,rd
x˜u 
, (3)
where Fis a smooth function. We can view u(t),˜u(t,·)
as a function in F={u  u:R2!Rn}, and consider theCompute Hidden LayerFigure 2. Each iteration of the evolutionary PDE can be viewed as
a layer of the network.
group action of Gon˜u2˜Ffollowing the same way of the
group action on u(t)2F,i.e.(g·˜u)(t,x)= ˜u(t, g 1·x).
For a given symmetry group G, a PDE in the form ( 3) is
called G-symmetric as long as if ˜uis a solution, then g·˜u
is also a solution, for any g2G.
According to the differential invariant theory [ 42], the
PDE ( 3) isG-symmetric if and only if the right side of ( 3)
is a function of differential invariants. Additionally, any dif-
ferential invariant can be expressed as a function of funda-
mental differential invariants. Therefore, any G-symmetric
PDE in the form ( 3) can be written as:
@˜u
@t(t,x)=H⇣
t,I1(x,u(t)),. . . ,Ik(x,u(t))⌘
,(4)
where His a smooth function and Ii(i=1,2,. . . ,k )form a
complete set of fundamental differential invariants. Denote
IFDIas the concatenation of fundamental differential in-
variants, i.e.IFDI,(I1,. . . ,Ik)>. We can present ( 4) in
a more compact form:
@˜u
@t=H(t) ˆIFDI[u(t)], (5)
where H(t),H(t,·)is a smooth function indexed by t
with input dimension kand output dimension n, and the
deﬁnition of the operator ˆIFDIis given in ( 2).
Consider a PDE system consisting of a G-symmetric
PDE in the form ( 5) with an initial condition,
⇢@˜u
@t=H(t) ˆIFDI[u(t)],
˜u(0,x)=u0(x).(6)
We approximate the temporal derivative by forward differ-
ence to discretize the PDE and formally solve the PDE sys-
tem ( 6) by iteration. Let 0= t0<t1<. . .<t N=Tbe
a partition of the interval [0,T], and the forward scheme is
shown as follows:
u(t0)=u0, (7)
u(ti+1)=u(ti)+ ti·H(ti) ˆIFDI[u(ti)],(8)
where  ti,ti+1 ti,u(ti),˜u(ti,·). As is well known,
neural networks have universal approximation capabilities.
Theoretically, if we choose H(t)to be a neural network,
we can represent any differential invariant. In practice, we
introduce a series of parameterized multilayer perceptrons
5549
(MLPs), {h✓i,0iN 1}, whose input dimension
matches IFDIand output dimension matches u0. Conse-
quently, we have the iterative process:
u(ti+1)=u(ti)+ ti·h✓i ˆIFDI[u(ti)]. (9)
We regard each iteration as an operator  i:u(ti)7!u(ti+1)
(see Figure 2), which is equivariant. Note that equivariance
of idoes not rely on the existence and uniqueness of the
solution to the original PDE system ( 6). Furthermore, if
we replace IFDIwith general invariants, the operator  iis
still equivariant.
Utilizing transitivity of equivariance, we stack these
equivariant operators together to get a feed-forward deep
equivariant network, i.e. , N 1 ···   1  0. The
number of layers corresponds to the number Nof iterations,
and the number of channels corresponds to the dimension of
˜uin the PDE. The network takes u(t0)=u0as inputs, and
produces u(tN)as the output features. Inference of the net-
work aligns with the evolution process of the PDE. Further-
more, the network naturally incorporates the skip connec-
tion structure [ 23], which is renowned for its advantageous
impact on network optimization. Inspired by PDEs based
on invariants, we call the network InvarPDE-Net .
3.3. SupNorm normalized differential invariants
The basic version of InvarPDE-Net provides an approach
to create equivariant networks without discretizing or sam-
pling groups. However, for the afﬁne group, its fundamental
differential invariants are in the form of fractional polyno-
mials, such asuxxuyy u2
xy
u2yuxx 2uxuyuxy+u2xuyy, potentially leading
to the division-by-zero issue in practice. For instance , when
a region of an image has uniform color, the denominator
approaches or equals zero.
We notice that differential invariants of the afﬁne group
can be expressed by polynomial relative differential invari-
ants. Building upon the observation, we propose a tech-
nique to construct a new type of afﬁne invariants by nor-
malizing polynomial relative differential invariants with a
special norm, which not only avoid the division-by-zero is-
sue but also exhibit better expressive power than classical
differential invariants. To start with, we give a deﬁnition of
polynomial relative differential invariants.
Deﬁnition 6 LetGbe the afﬁne group acting on X=R2,
F={u  u:X!Rn}be the set of smooth functions,
w:G!R+be a positive multiplier, and Pbe a m-degree
homogeneous polynomial. Deﬁne J:X⇥F ! Ras
follows J(x,u),P(u(x),ru(x),. . . ,rdu(x))). We call
Jad-th order (polynomial) relative differential invariant
ofGwith weight wand degree m, if8u2F,x2X,g2
G, we have
J(g·x,g·u)=w(g)J(x,u). (10)Relative Differential Invariants Weight Degree
u 1 1
uxxuyy u2
xy 1/(det A)22
u2
yuxx 2uxuyuxy+u2
xuyy 1/(det A)23
Table 1. We present relative differential invariants of the afﬁne
group for scalar functions up to order 2. Note that any element g
in the afﬁne group can be represented as g=(A,b).
Low-order relative differential invariants of the afﬁne group
for scalar functions on R2are shown in Table 1. Unless the
weight w⌘1, a relative differential invariant is generally
not an invariant. Actually , the result of dividing two relative
differential invariants with the same weight is a differential
invariant. Butfractional polynomials may suffer from the
division-by-zero issue as mentioned before .
Next, we present a technique to construct invariants
based on relative differential invariants via normalization.
Theorem 7 LetGbe the afﬁne group acting on X=R2.
LetF={u  u:X!Rn}andF0={v  v:X!Rk}
be sets of smooth functions on X,which are constant out-
side a compact set. Deﬁne a norm on F0called SupNorm ,
kvksup,supx2Xkv(x)k1. Given a collection of rela-
tive differential invariants of Gwith weight w, denoted as
Ji:X⇥F ! R(i=1,2,. . . ,k ). Deﬁne J:X⇥F ! Rk
asJ,(J1,. . . ,Jk)>, and J(·,u)can be viewed as an
element in F0. Deﬁne I:X⇥F ! Rkas follows:
I(x,u),1
kJ(·,u)ksup·J(x,u). (11)
Then Iis ak-dimensional invariant of G.
The key to the proof of Theorem 7is that for any g2G,
we have kJ(·,g·u)ksup=w(g)kJ(·,u)ksup. A detailed
proof is provided in Supplementary Material. We call the
invariant constructed in ( 11)aSupNorm normalized dif-
ferential invariant (SNDI). As the invariant involves global
spatial information of derivatives, it is no longer a classical
differential invariant. It may contain information beyond
fundamental differential invariants.
To construct SNDIs, we start from a collection of poly-
nomial relative differential invariants. Although the selec-
tion of relative differential invariants does not affect in-
variance, a recommended practice is to encompass those
that sufﬁciently represent fundamental differential invari-
ants, thus capturing adequate information. Next, we can
normalize each relative differential invariant individually, as
Theorem 7also holds when k=1. Alternatively, we can
normalize all relative differential invariants with the same
weight and the same degree together, which preserves more
information between relative differential invariants. An ad-
ditional beneﬁt is that SNDIs derived in this way exhibit
illumination invariance, i.e.I(x,c·u)=I(x,u),8c>0.
5550
Here is an example to have a glimpse of the advantage in
expressive power of SNDIs compared with that of classical
differential invariants. Assuming uasmooth scalar func-
tion on R2,uxxuyy u2
xy
u2yuxx 2uxuyuxy+u2xuyyis the only fundamental
differential invariant of the afﬁne group up to second order
apart from the trivial one uitself. Through the newly pro-
posed method of normalization, we can obtain two SNDIs
uxxuyy u2
xy
kuxxuyy u2xyksupandu2
yuxx 2uxuyuxy+u2
xuyy
ku2yuxx 2uxuyuxy+u2xuyyksup. It is not
hard to ﬁnd that we can express the fundamental differential
invariant as the quotient of two SNDIs up to a constant mul-
tiple, but not vice versa. From another perspective, we need
to discretize functions by sampling on grid points in imple-
mentation. Given kpolynomial relative differential invari-
ants, each one can be viewed as an M⇥Mmatrix. Obtain-
ing differential invariants through division would lead to the
loss of at least M2degrees of freedom, while normalization
only sacriﬁces at most kdegrees of freedom.
In summary, SNDIs not only avoid the division-by-zero
issue but also exhibit better expressive power than classi-
cal differential invariants. Given a collection of polyno-
mial relative differential invariants, we construct SNDIs via
normalization, and concatenate them together, resulting in
a higher dimensional invariant ISNDI. With theoretical
guarantee of invariance, we can directly employ ISNDI
to replace fundamental differential invariants IFDIin (9).
Thus, each layer of InvarPDE-Net is adjusted to:
u(ti+1)=u(ti)+ ti·h✓i ˆISNDI[u(ti)]. (12)
3.4. Extensions of network architecture
The equivariant network InvarPDE-net derived from a
symmetric PDE requires the dimension of features, namely
the number of channels, to be consistent across each layer.
This is not the case for the majority of conventional net-
works. Hence, we generalize the network to accommodate
varying channel numbers while maintaining equivariance.
Note that we can stack several PDEs of the same dimen-
sion sequentially, with the output of one PDE serving as
the input of the subsequent one. Furthermore, when dealing
with PDEs of different dimensions, we can linearly com-
bine the output channels of one PDE to match the number
of channels in the subsequent PDE. Since linear combina-
tions of invariants remain invariants, the process does not
affect equivariance. This extension allows us to create an
equivariant network composed of multiple PDEs with vary-
ing channel numbers. We name the network InvarPDEs-Net
(see Figure 1), including InvarPDE-Net as a special case.
In addition, we aim to design an equivariant layer that
can be directly integrated into convolutional networks of
various architectures by replacing convolutional layers.
Such an equivariant layer will offer enhanced ﬂexibility in
its applications. A key aspect lies in the ability to freelyInvariantsHiddenLayerComputeInvariantsFigure 3. InvarLayer is an equivariant layer extracted and adapted
from the iterative process of a symmetric PDE, which allows for
free speciﬁcation of input and output channel numbers.
specify input and output channel numbers, similar to a con-
volutional layer. By observing the iterative process in ( 12),
we can adjust the output dimension of h✓i, and directly em-
ploy h✓i ˆISNDI[u(ti)]as the output of this layer, which is
still equivariant. Given input and output channel numbers,
C1andC2, we formulate the equivariant layer as follows:
uout=h✓ ˆISNDI[uin], (13)
where ISNDI is ak-dimensional SNDI, and h✓is an MLP
with input dimension kand output dimension C2. We name
the equivariant layer InvarLayer (see Figure 3). It has a sim-
ilar structure to the PDE iteration process (see Figure 2) but
without the skip connection, allowing different input and
output channel numbers.
3.5. Implementation
We establish a theoretical foundation in the continuous
setting. When it comes to implementation, in the context of
processing image data, discretization on 2D grids becomes
necessary. We employ Gaussian derivatives to estimate
derivatives by applying derivatives of a Gaussian kernel
[25,27]. For example, fx(x0)⇡PN
n=1@xG(xn; )f(xn+
x0), where G(x; )is a Gaussian kernel with standard devi-
ation  centered around 0, and xnare grid points around 0.
In the case of 2D grid points, it can be implemented using
convolutions with speciﬁc kernels.
It is important to highlight that common network com-
ponents are compatible with our approach. Proposition 3
guarantees that invariants under the operation of post-
composition maintain their invariance property. This means
BatchNorm [ 26], pointwise nonlinearities, 1⇥1Convolu-
tion, and DropOut [ 55] can all be seamlessly integrated into
our models without compromising equivariance. Pooling
can also be incorporated into the models, though it intro-
duces equivariance error to some extent. Specially, when
using global pooling, we obtain invariant features.
In the following, we will discuss the input and ultimate
output in InvarPDEs-Net, with a speciﬁc focus on image
classiﬁcation tasks. Currently, we simply replicate the im-
age data along the channel dimension multiple times until
the given number of channels is reached, which serves as
the input of the network, i.e.u(t0)=u0. For the ﬁnal out-
put of equivariant features of the network u(tN), we perform
spatial global pooling to extract a set of invariant features,
5551
matching the number of channels. Subsequently, we apply
two fully connected layers to acquire the ultimate classiﬁ-
cation result.
As for MLPs used for combining invariants in our net-
works, we apply two layer perceptrons in practice. Since
MLPs operate on the vector I(x,u)for each point xand
share weights spatially, they can be effectively implemented
using 1⇥1convolutions with the ReLU activation function.
Likewise, connections between PDEs of different dimen-
sions in InvarPDEs-Net can also be realized using 1⇥1
convolutions. As for the computation of SupNorm in con-
structing SNDIs, it can be easily implemented by applying
global Max-Pooling over the channels corresponding to rel-
ative differential invariants that are normalized together.
3.6. Discussion
Unlike existing methods for designing equivariant net-
works, our framework does notapply discretization or sam-
pling to the group. The number of channels is independent
of the dimension of the group. When the group is larger, the
number of fundamental differential invariants is bounded by
the number of derivatives, and the same holds true for poly-
nomial relative differential invariants. Therefore, the model
size does not increase as the group becomes larger. That is
why our framework can handle afﬁne equivariance.
Moreover, our framework can be extended to continuous
subgroups of the afﬁne group. Common examples include
the scale group, the shearing group, the rotation group, the
rotation-scale group and the equi-afﬁne group. To construct
equivariant networks for these groups, we simply compute
corresponding differential invariants and incorporate them
into InvarPDE-Net. If the differential invariants involve
fractions, the normalization technique is also applicable.
The network structures, InvarPDEs-Net and InvarLayer, are
compatible with these groups, and the implementation pro-
cess remains the same. Therefore, it is a uniﬁed framework
for the afﬁne group and its continuous subgroups.
4. Experiments
For empirical validation, we implement InvarPDEs-Net
and InvarLayer for three non-Euclidean groups: the scale
group, the rotation-scale group, and the afﬁne group. We
conduct classiﬁcation experiments on image datasets with
different group transformations, and refrain from using data
augmentation to emphasize the innate equivariance of net-
works.
4.1. Scale equivariance
Following previous works on scale equivariance [ 29,41,
53,79], we conduct experiments on datasets with scale vari-
ations, speciﬁcally Scale-MNIST and Scale-Fashion. We
build Scale-MNIST and Scale-Fashion by rescaling the im-
ages of the MNIST [ 30] dataset and the Fashion-MNISTModels Scale-MNIST Scale-Fashion
SiCNN [ 74] 97.53±0.12 85 .32±0.22
SI-ConvNet [ 29] 97.56±0.13 85 .16±0.14
SEVF [ 38] 97.28±0.16 84 .73±0.11
DSS [ 70] 97.34±0.13 84 .50±0.51
SS-CNN [ 19] 97.68±0.15 85 .39±0.32
SESN [ 53] 97.92±0.09 85 .93±0.28
ScDCFNet [ 79] 97.91±0.08 86 .19±0.15
SE-CNN [ 41] 97.16 87 .48
InvarPDEs-Net (Ours) 98.30 ±0.06 89.62 ±0.26
InvarLayer (Ours) 97.75±0.05 89 .50±0.15
Table 2. Test accuracy (%) on Scale-MNIST and Scale-Fashion.
All models have approximately 500k trainable parameters.
[72] dataset with the scaling factor randomly selected from
[0.3,1]. Then we reshape them back to the original size
28⇥28by zero paddings. For both datasets, we use 10k
samples for training and 50k for testing. In line with prior
works [ 41,53,79], we integrate InvarLayer into a CNN with
three convolution layers and two fully connected layers, and
ensure that both InvarPDEs-Net and InvarLayer have fewer
than 500k trainable parameters. For more details on the
models and experiments, please refer to Supplementary Ma-
terial.
Experiments are repeated for six times using datasets
generated with independent seeds. We report the mean ±
std of the test accuracy of our models in Table 2. The results
of SE-CNN on both datasets and SESN on Scale-MNIST
come from the original papers [ 41,53], and the others come
from [ 79] under the same settings. On Scale-MNIST, Invar-
Layer achieves comparable results with other models and
InvarPDEs-Net delivers the best performance. On Scale-
Fashion, InvarPDEs-Net and InvarLayer outperform other
models signiﬁcantly.
4.2. Rotation-Scale equivariance
Gao et al. [ 18] ﬁrst presented a rotation-scale equivariant
network, RST-CNN. Following [ 18], we generate datasets
RS-MNIST and RS-Fashion for evalutation. With the same
procedure, we apply rotation (uniformly in [0,2⇡]) and
rescaling (uniformly in [0.3,1]) to the images of MNIST
and Fashion-MNIST, and zero-pad them back to the origi-
nal size followed by upsizing images to 56⇥56. For both
datasets, we use 5k samples for training and 50k for test-
ing. Consistent with [ 18], we integrate InvarLayer into a
CNN with three convolution layers and two fully connected
layers, and keep the number of trainable parameters below
500k for both InvarPDEs-Net and InvarLayer.
The mean ±std of the test accuracy over six indepen-
dent trials are reported in Table 3. Compared models in-
clude RST-CNN [ 18] and other models that are equivariant
to either rotation (SFCNN [ 64] and RDCF [ 2]) or scaling
5552
Models RS-MNIST RS-Fashion
SFCNN [ 64] 89.69±0.40 75 .80±0.11
RDCF [ 2] 90.46±0.33 73 .96±0.19
SEVF [ 38] 90.29±0.37 71 .03±0.31
SESN [ 53] 90.19±0.39 72 .19±0.05
ScDCFNet [ 79] 90.40±0.09 72 .24±0.23
RST-CNN [ 18] 93.19±0.29 78 .64±0.60
InvarPDEs-Net (Ours) 95.80 ±0.09 79.48 ±0.31
InvarLayer (Ours) 93.15±0.21 74 .51±0.71
Table 3. Test accuracy (%) on RS-MNIST and RS-Fashion. All
models have approximately 500k trainable parameters. RST-CNN
is a rotation-scale equivariant network, while other compared
models are only equivariant to rotation (SFCNN and RDCF) or
scaling (SEVF, SESN, and ScDCFNet).
(SEVF [ 38], SESN [ 53], and ScDCFNet [ 79]). The results
of these models are obtained from [ 18] under the same set-
tings. On RS-MNIST, InvarPDEs-Net signiﬁcantly outper-
forms other models and InvarLayer exhibits comparable re-
sults with RST-CNN. On RS-Fashion, InvarPDEs-Net re-
mains the top-performing model, while InvarLayer delivers
relatively modest results. With minor adjustments of hy-
perparameters, InvarLayer lifts the accuracy to 93.40% on
RS-MNIST and to 76.08% on RS-Fashion. More details
about models and experiments are provided in Supplemen-
tary Material.
4.3. Afﬁne equivariance
Models Accuracy Parameters
CapsNet [ 46] 79 8 .1M
GE CapsNet [ 31] 89.10 235 K
afﬁne CapsNet [ 21] 93.21 –
RU CapsNet [ 8] 97.69 >580K
affConv [ 37] 95.08±0.31 373 K
InvarPDEs-Net (Ours) 95.72±0.12 340 K
InvarLayer (Ours) 98.45 ±0.15 365K
Table 4. Test accuracy (%) on affNIST after training on MNIST.
The ﬁrst four models are Capsule Networks that demonstrate ro-
bustness to afﬁne transformations but admit few rigorous mathe-
matical guarantees, while affConv is an afﬁne equivariant network.
As for afﬁne equivariance, the main counterpart we com-
pare with is the afﬁne equivariant model, affConv [ 37]. Fol-
lowing [ 37], we evaluate our models on the public dataset
affNIST under the out-of-distribution setting. Speciﬁcally,
we train our models on 50k non-transformed MNIST im-
ages (padded to 40⇥40) and test them on 320k afﬁne-
perturbed MNIST (affNIST) images with size 40⇥40. As
mentioned before, it is impractical to apply affConv to deepnetworks, while InvarLayer overcomes the limitation. We
use the structure of ResNet-32 for InvarLayer. For a fair
comparison, we ensure that InvarPDEs-Net and InvarLayer
both have fewer parameters than affConv ( 373k). Addi-
tional details can be found in Supplementary Material.
We present the mean ±std of test accuracy over six train-
ing runs with different random seeds in Table 4. Besides
affConv, we also list the results under the same setup from
some Capsule Networks [ 8,21,31,46], which may lack
rigorous theoretical guarantees of invariance. Although RU
CapsNet performs better than affConv, which could not be
well understood according to [ 37], our InvarLayer beats it
by a margin of 0.76%. Moreover, our InvarPDEs-net also
outperforms affConv. Additional results of the conventional
setting, training on affNIST and testing on affNIST, can be
found in Supplementary Material.
5. Conclusion
In this paper, we propose a new framework to achieve
afﬁne equivariance, a long-standing challenge in the ﬁeld
of equivariant networks. Within our framework, we con-
struct a PDE-inspired equivariant network, InvarPDEs-Net,
which showcases strong performance across extensive ex-
periments. Furthermore, for more ﬂexibility, we introduce
an equivariant layer, InvarLayer, which can serve as a drop-
in replacement for convolutional networks of various archi-
tectures. When combined with a ResNet structure, Invar-
Layer retains state-of-the-art results on the affNIST dataset.
While the performance of InvarLayer exhibits some vari-
ability in certain setups, we recognize its immense poten-
tial. We believe that further reﬁnement of the layer de-
sign based on our paradigm will elevate its capabilities to
a higher level.
Our framework is quite promising and merits further ex-
tension. It is known that differential invariants exist for Lie
groups satisfying certain regular conditions [ 42]. We con-
centrate on the afﬁne group and make differential invari-
ants applicable through the normalization technique, which
is also suitable for its subgroups. How to adapt differential
invariants of more general Lie groups into equivariant net-
works remains a future research. Additionally, besides the
2D planes considered in our work, it is worthwhile to study
the extension of our framework to other manifolds, such as
spheres and 3D spaces. Moreover, while our experiments
involve image classiﬁcation tasks, applications to a broader
range of tasks in real world can be further explored.
Acknowledgment
Z. Lin was supported by National Key R&D Pro-
gram of China (2022ZD0160300), the NSF China (No.
62276004), and the major key project of PCL, China (No.
PCL2021A12).
5553
References
[1]Erik J Bekkers, Maxime W Lafarge, Mitko Veta, Koen AJ
Eppenhof, Josien PW Pluim, and Remco Duits. Roto-
translation covariant convolutional networks for medical im-
age analysis. In International Conference on Medical Image
Computing and Computer-Assisted Intervention , 2018. 3
[2]Xiuyuan Cheng, Qiang Qiu, Robert Calderbank, and
Guillermo Sapiro. Rotdcf: Decomposition of convolutional
ﬁlters for rotation-equivariant deep networks. arXiv preprint
arXiv:1805.06846 , 2018. 7,8
[3]Taco Cohen and Max Welling. Group equivariant convolu-
tional networks. In International Conference on Machine
Learning , pages 2990–2999. PMLR, 2016. 1,3
[4]Taco S Cohen and Max Welling. Steerable CNNs. In Inter-
national Conference on Learning Representations , 2016. 1,
3
[5]Taco S Cohen, Mario Geiger, Jonas K ¨ohler, and Max
Welling. Spherical CNNs. In International Conference on
Learning Representations , 2018. 3
[6]Taco S Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max
Welling. Gauge equivariant convolutional networks and the
icosahedral CNN. In International Conference on Machine
Learning , 2019. 3
[7]Pim De Haan, Maurice Weiler, Taco S Cohen, and Max
Welling. Gauge equivariant mesh CNNs: Anisotropic con-
volutions on geometric graphs. In International Conference
on Learning Representations , 2021. 3
[8]Fabio De Sousa Ribeiro, Georgios Leontidis, and Stefanos
Kollias. Introducing routing uncertainty in capsule networks.
Advances in Neural Information Processing Systems , 33:
6490–6502, 2020. 8
[9]Micha ¨el Defferrard, Martino Milani, Fr ´ed´erick Gusset, and
Nathana ¨el Perraudin. Deepsphere: a graph-based spherical
CNN. In International Conference on Learning Representa-
tions , 2019. 3
[10] Carlos Esteves, Christine Allen-Blanchette, Ameesh Maka-
dia, and Kostas Daniilidis. Learning SO(3) equivariant rep-
resentations with spherical CNNs. In Proceedings of the Eu-
ropean Conference on Computer Vision , 2018. 3
[11] Carlos Esteves, Avneesh Sud, Zhengyi Luo, Kostas Dani-
ilidis, and Ameesh Makadia. Cross-domain 3D equivariant
image embeddings. In International Conference on Machine
Learning , 2019. 1,3
[12] Carlos Esteves, Yinshuang Xu, Christine Allen-Blanchette,
and Kostas Daniilidis. Equivariant multi-view networks. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 1568–1577, 2019. 1,3
[13] Carlos Esteves, Ameesh Makadia, and Kostas Daniilidis.
Spin-weighted spherical CNNs. Advances in Neural Infor-
mation Processing Systems , 33:8614–8625, 2020. 3
[14] Cong Fang, Zhenyu Zhao, Pan Zhou, and Zhouchen Lin.
Feature learning via partial differential equation with appli-
cations to face recognition. Pattern Recognition , 69:14–25,
2017. 2,3,4
[15] Mark Fels and Peter J Olver. Moving coframes: II. regular-
ization and theoretical foundations. Acta Applicandae Math-
ematica , 55:127–208, 1999. 3[16] Marc Finzi, Samuel Stanton, Pavel Izmailov, and An-
drew Gordon Wilson. Generalizing convolutional neural net-
works for equivariance to Lie groups on arbitrary continu-
ous data. In International Conference on Machine Learning ,
pages 3165–3176. PMLR, 2020. 1,3
[17] Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max
Welling. SE(3)-transformers: 3D roto-translation equivari-
ant attention networks. Advances in Neural Information Pro-
cessing Systems , 33:1970–1981, 2020. 3
[18] L Gao, G Lin, and W Zhu. Deformation robust roto-scale-
translation equivariant CNNs. Transactions on Machine
Learning Research , 2022. 3,7,8
[19] Rohan Ghosh and Anupam K Gupta. Scale steerable ﬁlters
for locally scale-invariant convolutional neural networks.
arXiv preprint arXiv:1906.03861 , 2019. 3,7
[20] Simon Graham, David Epstein, and Nasir Rajpoot. Dense
steerable ﬁlter CNNs for exploiting rotational symmetry in
histology images. IEEE Transactions on Medical Imaging ,
2020. 3
[21] Jindong Gu and Volker Tresp. Improving the robustness of
capsule networks to image afﬁne transformations. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 7285–7293, 2020. 8
[22] Richard Hartley and Andrew Zisserman. Multiple view ge-
ometry in computer vision . Cambridge University Press,
2003. 3
[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 770–778, 2016. 5
[24] Lingshen He, Yuxuan Chen, Yiming Dong, Yisen Wang,
Zhouchen Lin, et al. Efﬁcient equivariant network. Advances
in Neural Information Processing Systems , 34:5290–5302,
2021. 3
[25] Lingshen He, Yuxuan Chen, Zhengyang Shen, Yibo Yang,
and Zhouchen Lin. Neural ePDOs: Spatially adaptive equiv-
ariant partial differential operator based networks. In Inter-
national Conference on Learning Representations , 2022. 1,
3,6
[26] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. In International Conference on Machine Learn-
ing, pages 448–456. pmlr, 2015. 6
[27] Erik Jenner and Maurice Weiler. Steerable partial differential
operators for equivariant neural networks. In International
Conference on Learning Representations , 2021. 1,3,6
[28] Chiyu Max Jiang, Jingwei Huang, Karthik Kashinath, Philip
Marcus, Matthias Niessner, et al. Spherical CNNs on un-
structured grids. In International Conference on Learning
Representations , 2018. 3
[29] Angjoo Kanazawa, Abhishek Sharma, and David Jacobs.
Locally scale-invariant convolutional neural networks. arXiv
preprint arXiv:1412.5104 , 2014. 7
[30] Yann LeCun. The MNIST database of handwritten digits.
http://yann.lecun.com/exdb/mnist/ , 1998. 7
[31] Jan Eric Lenssen, Matthias Fey, and Pascal Libuschewski.
Group equivariant capsule networks. Advances in Neural In-
formation Processing Systems , 31, 2018. 3,8
5554
[32] Junying Li, Zichen Yang, Haifeng Liu, and Deng Cai. Deep
rotation equivariant network. Neurocomputing , 2018. 3
[33] Risheng Liu, Zhouchen Lin, Wei Zhang, and Zhixun Su.
Learning PDEs for image restoration via optimal control. In
Proceedings of the European Conference on Computer Vi-
sion, pages 115–128. Springer, 2010. 1,2,3,4
[34] Risheng Liu, Zhouchen Lin, Wei Zhang, Kewei Tang, and
Zhixun Su. Toward designing intelligent PDEs for computer
vision: an optimal control approach. Image and Vision Com-
puting , 31(1):43–56, 2013. 1,2,3,4
[35] Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong.
PDE-net: Learning PDEs from data. In International Con-
ference on Machine Learning , pages 3208–3216. PMLR,
2018. 3
[36] Zichao Long, Yiping Lu, and Bin Dong. PDE-net 2.0: Learn-
ing PDEs from data with a numeric-symbolic hybrid deep
network. Journal of Computational Physics , 399:108925,
2019. 3,4
[37] Lachlan E MacDonald, Sameera Ramasinghe, and Simon
Lucey. Enabling equivariance for arbitrary Lie groups. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 8183–8192, 2022. 1,2,
3,8
[38] Diego Marcos, Benjamin Kellenberger, Sylvain Lobry, and
Devis Tuia. Scale equivariance in CNNs with vector ﬁelds.
arXiv preprint arXiv:1807.11783 , 2018. 7,8
[39] Mircea Mironenco and Patrick Forr ´e. Lie group decomposi-
tions for equivariant neural networks. In International Con-
ference on Learning Representations , 2024. 1,3
[40] Joseph L Mundy and Andrew Zisserman. Geometric invari-
ance in computer vision . MIT Press, 1992. 3
[41] Hanieh Naderi, Leili Goli, and Shohreh Kasaei. Scale equiv-
ariant CNNs with scale steerable ﬁlters. In 2020 Interna-
tional Conference on Machine Vision and Image Processing
(MVIP) , pages 1–5. IEEE, 2020. 3,7
[42] Peter J Olver. Applications of Lie groups to differential equa-
tions . Springer Science & Business Media, 1993. 2,3,4,8
[43] Peter J Olver. Moving frames. Journal of Symbolic Compu-
tation , 36(3-4):501–512, 2003. 3
[44] Peter J Olver. Modern developments in the theory and ap-
plications of moving frames. London Math. Soc. Impact150
Stories , 1:14–50, 2015. 3
[45] Peter J Olver, Guillermo Sapiro, and Allen Tannenbaum.
Afﬁne invariant detection: edge maps, anisotropic diffusion,
and active contours. Acta Applicandae Mathematica , 59:45–
77, 1999. 3
[46] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dy-
namic routing between capsules. Advances in Neural Infor-
mation Processing Systems , 30, 2017. 8
[47] Mateus Sangalli, Samy Blusseau, Santiago Velasco-Forero,
and Jes ´us Angulo. Differential invariants for SE(2)-
equivariant networks. In 2022 IEEE International Confer-
ence on Image Processing , pages 2216–2220. IEEE, 2022.
2,3
[48] Mateus Sangalli, Samy Blusseau, Santiago Velasco-Forero,
and Jesus Angulo. Scale equivariant U-Net. In 33rd British
Machine Vision Conference , 2022. 3[49] Mateus Sangalli, Samy Blusseau, Santiago Velasco-Forero,
and Jesus Angulo. Moving frame net: SE(3)-equivariant net-
work for volumes. In NeurIPS Workshop on Symmetry and
Geometry in Neural Representations , pages 81–97. PMLR,
2023. 2,3
[50] Zhengyang Shen, Lingshen He, Zhouchen Lin, and Jinwen
Ma. PDO-eConvs: Partial differential operator based equiv-
ariant convolutions. In International Conference on Machine
Learning , pages 8697–8706. PMLR, 2020. 1,3
[51] Zhengyang Shen, Tiancheng Shen, Zhouchen Lin, and Jin-
wen Ma. PDO-eS2CNNs: Partial differential operator
based equivariant spherical CNNs. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence , pages 9585–
9593, 2021. 3
[52] Zhengyang Shen, Tao Hong, Qi She, Jinwen Ma, and
Zhouchen Lin. PDO-s3DCNNs: Partial differential opera-
tor based steerable 3D CNNs. In International Conference
on Machine Learning , pages 19827–19846. PMLR, 2022. 3
[53] Ivan Sosnovik, Michał Szmaja, and Arnold Smeulders.
Scale-equivariant steerable networks. In International Con-
ference on Learning Representations , 2019. 3,7,8
[54] Ivan Sosnovik, Artem Moskalev, and Arnold Smeulders.
How to transform kernels for scale-convolutions. In Pro-
ceedings of the IEEE/CVF International Conference on
Computer Vision , pages 1092–1097, 2021. 3
[55] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way
to prevent neural networks from overﬁtting. The Journal of
Machine Learning Research , 15(1):1929–1958, 2014. 6
[56] Zikai Sun and Thierry Blu. Empowering networks with
scale and rotation equivariance using a similarity convolu-
tion. In International Conference on Learning Representa-
tions , 2022. 3
[57] Stanley L Tuznik, Peter J Olver, and Allen Tannenbaum.
Afﬁne differential invariants for invariant feature point de-
tection. arXiv preprint arXiv:1803.01669 , 2018. 3
[58] Dian Wang, Robin Walters, Xupeng Zhu, and Robert Platt.
Equivariant qlearning in spatial action spaces. In Conference
on Robot Learning , pages 1713–1723. PMLR, 2022. 3
[59] Rui Wang, Robin Walters, and Rose Yu. Incorporating sym-
metry into deep dynamics models for improved generaliza-
tion. In International Conference on Learning Representa-
tions , 2020. 3
[60] Yuanbin Wang, Bin Zhang, and Tianshun Yao. Projective in-
variants of co-moments of 2D images. Pattern Recognition ,
43(10):3233–3242, 2010. 3
[61] Yuanbin Wang, Xingwei Wang, Bin Zhang, et al. Afﬁne dif-
ferential invariants of functions on the plane. Journal of Ap-
plied Mathematics , 2013, 2013. 4
[62] Maurice Weiler and Gabriele Cesa. General E(2)-equivariant
steerable CNNs. Advances in Neural Information Processing
Systems , 32, 2019. 1,3
[63] Maurice Weiler, Mario Geiger, Max Welling, Wouter
Boomsma, and Taco S Cohen. 3D steerable CNNs: Learn-
ing rotationally equivariant features in volumetric data. Ad-
vances in Neural Information Processing Systems , 31, 2018.
1,3
5555
[64] Maurice Weiler, Fred A Hamprecht, and Martin Storath.
Learning steerable ﬁlters for rotation equivariant CNNs. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 849–858, 2018. 1,3,7,8
[65] Ruben Wiersma, Elmar Eisemann, and Klaus Hildebrandt.
CNNs on surfaces using rotation-equivariant features. ACM
Transactions on Graphics , 39(4):92–1, 2020. 3
[66] Ruben Wiersma, Ahmad Nasikun, Elmar Eisemann, and
Klaus Hildebrandt. DeltaConv: anisotropic operators for ge-
ometric deep learning on point clouds. ACM Transactions on
Graphics , 41(4):1–10, 2022. 3
[67] Thomas Wimmer, Vladimir Golkov, Hoai Nam Dang,
Moritz Zaiss, Andreas Maier, and Daniel Cremers. Scale-
equivariant deep learning for 3D data. arXiv preprint
arXiv:2304.05864 , 2023. 3
[68] Marysia Winkels and Taco S Cohen. Pulmonary nodule de-
tection in CT scans with equivariant CNNs. Medical Image
Analysis , 2019. 1,3
[69] Daniel Worrall and Gabriel Brostow. Cubenet: Equivariance
to 3D rotation and translation. In Proceedings of the Euro-
pean Conference on Computer Vision , pages 567–584, 2018.
1,3
[70] Daniel Worrall and Max Welling. Deep scale-spaces: Equiv-
ariance over scale. Advances in Neural Information Process-
ing Systems , 32, 2019. 3,7
[71] Daniel E Worrall, Stephan J Garbin, Daniyar Turmukham-
betov, and Gabriel J Brostow. Harmonic networks: Deep
translation and rotation equivariance. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, 2017. 3
[72] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-
MNIST: a novel image dataset for benchmarking machine
learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
7
[73] Wenju Xu, Guanghui Wang, Alan Sullivan, and Ziming
Zhang. Towards learning afﬁne-invariant representations via
data-efﬁcient CNNs. In Proceedings of the IEEE/CVF Win-
ter Conference on Applications of Computer Vision , pages
904–913, 2020. 3
[74] Yichong Xu, Tianjun Xiao, Jiaxing Zhang, Kuiyuan Yang,
and Zheng Zhang. Scale-invariant convolutional neural net-
works. arXiv preprint arXiv:1411.6369 , 2014. 7
[75] Yilong Yang, Srinandan Dasmahapatra, and Sasan Mah-
moodi. Rotation-scale equivariant steerable ﬁlters. In Medi-
cal Imaging with Deep Learning , 2023. 3
[76] Linfeng Zhao, Xupeng Zhu, Lingzhi Kong, Robin Walters,
and Lawson LS Wong. Integrating symmetry into differen-
tiable planning with steerable convolutions. In International
Conference on Learning Representations , 2022. 3
[77] Yunhan Zhao, Ye Tian, Charless Fowlkes, Wei Shen, and
Alan Yuille. Resisting large data variations via introspective
transformation network. In Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision ,
pages 3080–3089, 2020. 3
[78] Zhenyu Zhao, Zhouchen Lin, and Yi Wu. A fast alternating
time-splitting approach for learning partial differential equa-
tions. Neurocomputing , 185:171–182, 2016. 2,3,4[79] Wei Zhu, Qiang Qiu, Robert Calderbank, Guillermo Sapiro,
and Xiuyuan Cheng. Scaling-translation-equivariant net-
works with decomposed convolutional ﬁlters. The Journal
of Machine Learning Research , 23(1):2958–3002, 2022. 3,
7,8
5556
