Unveiling Parts Beyond Objects:
Towards Finer-Granularity Referring Expression Segmentation
Wenxuan Wang1,2,3*Tongtian Y ue1,2∗Yisi Zhang4Longteng Guo1Xingjian He1
Xinlong Wang3Jing Liu1,2†
1Institute of Automation, Chinese Academy of Sciences (CASIA)
2School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences (UCAS)
3Beijing Academy of Artiﬁcial Intelligence (BAAI)
4University of Science and Technology Beijing (USTB)
{wangwenxuan2023@ia.ac.cn, wangxinlong@baai.ac.cn, jliu@nlpr.ia.ac.cn }
Figure 1. Classic Referring Expression Segmentation (RES) only supports expressions that indicate a single target object, e.g., (a). Com-
pared with classic RES, the proposed Multi-Granularity Referring Expression Segmentation (MRES) task supports expressions indi-
cating the speciﬁc part-level regions of target objects, e.g., part-level expressions like (b)-(e) from our newly built RefCOCOm benchmark.
Abstract
Referring expression segmentation (RES) aims at seg-
menting the foreground masks of the entities that match thedescriptive natural language expression. Previous datasetsand methods for classic RES task heavily rely on the priorassumption that one expression must refer to object-leveltargets. In this paper , we take a step further to ﬁner-grainedpart-level RES task. To promote the object-level RES tasktowards ﬁner-grained vision-language understanding, we
put forward a new multi-granularity referring expressionsegmentation (MRES) task and construct an evaluationbenchmark called RefCOCOm by manual annotations. Byemploying our automatic model-assisted data engine, we
build the largest visual grounding dataset namely MRES-32M, which comprises over 32.2M high-quality masks andcaptions on the provided 1M images. Besides, a simple yetstrong model named UniRES is designed to accomplish the
uniﬁed object-level and part-level grounding task. Exten-
sive experiments on our RefCOCOm for MRES and threedatasets ( i.e., RefCOCO(+/g)) for classic RES task demon-
*Equal contribution.
†Corresponding author.strate the superiority of our method over previous state-of-
the-art methods. To foster future research into ﬁne-grainedvisual grounding, our benchmark RefCOCOm, the MRES-
32M dataset and model UniRES will be publicly availableathttps://github.com/Rubics-Xuan/MRES .
1. Introduction
As one of the most challenging tasks in vision-language un-
derstanding, referring expression segmentation (RES) aimsto locate speciﬁc regions at the pixel level based on a de-scriptive language expression. Compared to traditional vi-sual segmentation tasks that focus on images or videos
alone, RES poses greater difﬁculties and challenges due
to the necessity of strong comprehension across modali-ties, but it can simultaneously alleviate the problem of pre-deﬁned categories in conventional object detection or seg-mentation. With the real-world scene often requiring diver-sity in target identiﬁcation, RES task holds vast potential forapplications, e.g., language-based human-object interaction
and interactive image editing.
Since the concept of RES task was initially proposed in
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
12998
Table 1. Comparison among different referring expression
datasets, including ReferIt [ 18], RefCOCO(+/g) [ 27,44], Phrase-
Cut [ 41], and our proposed MRES-32M . Part-Level: expressions
that speciﬁes various parts of the target object in the given image.
ReferIt RefCOCO(+/g) PhraseCut MRES-32M
Image Source CLEF [ 11] COCO [ 24]V G [ 21] Object365 [ 34]
Object-Level  
Part-Level  
Expression Type free free templated free
[13], various multimodal frameworks such as [ 13,19,40,
42] towards precise RES have been proposed to deal with
the challenging feature extraction and alignment problemsbetween visual and linguistic modalities. However, currentworks are limited to the scope of object-level grounding.As shown in Table 1, the RefCOCO dataset [ 44] stands
as one of the most widely used grounding benchmarks byfar, basically containing only object-level visual and tex-tual annotations. It does not take into account the part-level
grounding task, which is crucial for future multimodal mod-
els to act as intelligent agents to realize the ﬁne-grainedperception of real world. The existing grounding datasets
couldn’t support the training and evaluation of such ad-
vanced capabilities. Thus, exploring how to transcend thecurrent object-level constraints and delve into ﬁner-grainedpart-level grounding, is highly meaningful and worthy ofin-depth study, which is the primary focus of this work.
In fact, prior to this work, a few works along an-
other research line have made signiﬁcant strides towardsmore ﬁne-grained visual understanding. The demand forﬁner-grained visual understanding of objects arouses re-search community into constructing high-quality part-leveldataset. Speciﬁcally, recent years have witnessed the intro-duction of various datasets that provide ﬁne-grained part-level masks and bounding boxes annotations for either ob-jects of speciﬁc categories [ 10,33,39] or general categories
[4,12,26,32]. However, these datasets essentially corre-
spond to unimodal ( i.e., visual) downstream tasks, lacking a
deep connection between ﬁne-grained part-level masks andrich textual descriptions. To our knowledge, few of previ-ous studies have established this connection. Consequently,
there appears to be a limited availability of ﬁne-grained,large-scale vision-language datasets that facilitate part-levelcross-modality understanding, which is necessary in terms
of two aspects. Firstly, when describing an object, peoplealways naturally gravitate towards detailing part-level lo-cal features, underscoring the indispensable need for multi-modal agents to understand the part granularity. Secondly,a ﬁne-grained understanding at the part level can positivelypromote the perception of object-level targets, especiallyunder extreme conditions such as occlusion or deformation,which will consistently propel advancements in widely fo-cused object-level tasks like classic visual grounding.
Therefore, in this work, we attempt to ﬁll this impor-tant blank space that has been neglected before and move
towards ﬁner-grained part-level RES task. Speciﬁcally, topush towards ﬁner-grained vision-language understanding,we propose a new multi-granularity referring expressionsegmentation (MRES) task and an evaluation benchmarknamed RefCOCOm by manually annotating the part-leveltargets based on the previous benchmark RefCOCO withonly object-level labels. We also construct the largest-scale visual grounding dataset which is also the ﬁrst datasetto support part-level visual-textual annotations, and builda simple baseline model to accomplish the uniﬁed multi-granularity ( i.e., object-level and part-level) RES task.
Our main contributions can be summarized as follows:
• We propose a new MRES task (as shown in Fig. 1)
with corresponding benchmark RefCOCOm for evalua-
tion, pushing classic RES task towards ﬁner-granularityunderstanding of real-world scenes.
• We build a multi-grained visual grounding dataset namely
MRES-32M, which to the best of our knowledge is theﬁrst grounding dataset that supports part-level vision-language annotations and also the largest-scale visualgrounding dataset.
• To effectively unify both the object-level and part-level
RES tasks, we propose a simple yet strong model namely
UniRES, which achieves the new state-of-the-art (SOTA)performance on three object-level benchmarks for classicRES tasks and our multi-granularity RefCOCOm bench-mark for the proposed MRES task.
2. Related Work
Referring Expression Segmentation. As a challenging vi-
sual grounding task the concept of RES is ﬁrst proposedby [ 13]. Subsequent works such as [ 15,16,40,43,45]
predominantly follow a two-step pipeline of encoding thelinguistic and visual features separately and deriving thefused multimodal features from unimodal representationsfor mask prediction. In this way, the effectiveness of ob-tained multimodal representations essentially dominates themodel performance, which has been continually studied inthe following research [ 7,22,42,48,49]. Recently, there
are some works [ 28,36,46] focusing on zero-shot RES task
which has great application potential. In addition, a few re-cent works [ 14,25] have been devoted to address the limi-
tations of the existing benchmark datasets [ 18,27,41,44]
for visual grounding task. However, it’s worth noting thatprevious works primarily focus on classic object-level RESmethods and datasets, paying few attention to the vital ﬁne-
grained part-level grounding.
Part-level Visual Perception. The increasing interest in
the ﬁne-grained understanding of objects has driven the cre-ation of part-level annotated datasets across both special-
ized and general domains. Pioneering research in the formerﬁeld has introduced datasets with part-level annotations, fo-
12999
cusing on objects of particular categories like human body
parts [ 10], animal body parts [ 33] and vehicle components
[39]. In contrast, more general datasets that offer part anno-
tations for a variety of common objects include Pascal-Part[4], PartNet [ 26], and PartImageNet [ 12]. Furthermore, re-
cent work [ 6] introduces a ﬁner-grained dataset with addi-
tional instance-speciﬁc part annotations for panoptic seg-mentation task. PACO [ 32] extends the available object-
level datasets by including the annotations of part granu-larity and attribute for ﬁner-grained segmentation and de-tection tasks. Moreover, VLPart [ 35] proposes a parsing
pipeline for part segmentation and a detector that is capableof realizing the segmentation task of both open-vocabularyobjects and their part regions.
3. Multi-Granularity Grounding Benchmark
3.1. Multi-Granularity RES Task
The interaction capability of a multimodal agent directly de-pends on the visual granularity it can perceive and under-stand. However, in the current research landscape, multi-granularity visual grounding remains under-explored. Tobreak the limitations wherein visual-language alignment isconstrained to the object level, we intend to take a step for-ward towards ﬁner-grained vision-language understandingand propose the multi-granularity RES task, which requiresmodels’ proﬁciency in uniformly grounding entities acrossboth part and object levels in response to various textual ref-
erences. Within the single modality, it entails an enhance-
ment of the capacity to apprehend complex, implicit knowl-edge structures, e.g., the granularity delineated by textual
descriptions and the advanced level of pixel-wise seman-
tic comprehension. Across the vision-language modalities,ﬁner-grained references also pose new challenges for the ac-curacy and robustness of cross-modal alignment. Besides,since previous studies are mainly restricted to object-levelreferring segmentation and related benchmarks for the eval-uation of part-level grounding performance are unavailable,we build a corresponding benchmark named RefCOCOmfor performance evaluation, which will be elaborated below.
3.2. RefCOCOm Benchmark
Manual Annotation Pipeline. To facilitate easy evaluation
by community researchers without the need for any codemodiﬁcations and build the multi-granularity benchmarkwith excellent quality, we split the existing object masks intopart-level annotations through manual labeling on three
subsets ( i.e., validation, testA and testB) of the most preva-
lent RefCOCO [ 44] dataset. Initially, we establish a set of
criteria for part masks delineation and identify the speciﬁcpart categories requisite for each object. Subsequently, weemploy SAM [ 20] to procure the preliminary segmentation
results. We recruit 30 skilled annotators to reﬁne and cap-tioning these results through an online annotation tool de-
veloped by ourselves. To ensure the annotation quality, weconduct spot checks at three progress nodes of 20%, 50%,and 80% of the total workload with corrective feedback.
Figure 2. RefCOCOm benchmark statistics. (a) the number of
referring expressions per parts’ category in the log scale. (b) theword cloud highlights the head categories.
Benchmark Details. Totally, we collect 70k part refer-
ences and corresponding 26k masks. The average length
of the references is 5.1, covering 80 object and 391 part
categories, among which the number of referring expres-sions per parts’ category and the word cloud that highlightsthe head categories are both presented in Fig. 2. Mixed
with the original object-level annotations, our RefCOCOmhas a total of 34k masks and 92k references. As an en-hanced expansion of RefCOCO [ 44], our RefCOCOm puts
forward higher requirements for referential understandingand visual perception capabilities. To better match the in-tention of multi-granularity uniﬁcation, we leverage mean
Intersection-over-Union (mIoU) as evaluation metrics.
4. Multi-Granularity Grounding Dataset
4.1. Data Collection Engine
Given the intrinsic complexity of classic RES task, the asso-ciated training data necessitates extensive annotation costs
across both textual and visual domains. When the granu-larity is promoted to part-level, the annotation intricaciesare further exacerbated. We contend that the primary bot-tleneck hindering the emergence of open-world groundingis the limitation imposed by the current scalability of data.By leveraging robust foundation models for synergistic en-
hancement, we introduce an advanced data engine capableof automatically generating reliable visual grounding data.
Multi-Grained Dense Captioner. As the large language
models (LLMs) [ 5,37] continue to empower the multi-
modal domain with profound capabilities, the current scopeof large vision-language models (LVLMs) [ 1–3] has effec-
tively expanded into the grounding domain. Formally, thesemodels utilize numerical coordinates to represent bounding
boxes, allowing positional information to be seamlessly in-tegrated into the language model via word embeddings forcontextual understanding. However, the research commu-
nity’s exploration into the part granularity remains nascent.
13000
Figure 3. The illustration of our data engine for building the MRES-32M dataset. (a) We start by ﬁne-tuning an LVLM to create a capable
dense captioner, which can effectively handle captioning tasks at three levels of granularity. (b) To generate object-level grounding data,we feed images and original bounding boxes into the dense captioner and a powerful segmenter to obtain the captions and masks of various
objects. (c) We leverage the external knowledge from LLMs to decompose the existing object category annotations into a vocabulary setof part-level tags, which are sequentially fed into an open-vocabulary segmenter and our captioner to acquire the part-level annotations.
The constrained training data have resulted in the ﬁne-
grained descriptive capabilities of LVLMs being conﬁned
merely to the object-level. To fully harness the open-worldvisual knowledge acquired from extensive pretraining on
image-level and object-level data, we have devised a uni-ﬁed ﬁne-tuning scheme that equips LVLMs to operate as
dense captioners across all granularity levels, as depicted inFig. 3(a). Data for all three granularity levels are sourced
from manual annotations to ensure reliability. The inputcomprises an image and the corresponding bounding box to
be described, with bounding box coordinates normalized tointeger values within the [0,999] range. For image granu-
larity, we employ the COCO dataset [ 24], where all bboxes
are uniformly represented as (0,0),(999,999) . For object
granularity, we utilize the Visual Genome dataset [ 21]. For
part granularity, we draw upon unimodal semantic segmen-tation data [ 4,12,32] and employ a template in the form
ofP artNameX of ObjectNameY to construct dense
captions. This uniﬁed multitask training approach can besynergistic across different granularity: it allows LVLMs
to incorporate more comprehensive and detailed informa-tion to enhance part granularity descriptions. Concurrently,knowledge of part granularity assists LVLMs in generaliz-
ing knowledge within object interiors.
Model-Assisted Data Generation. For the data generation
of object-level visual grounding, we capitalize on large-scale object detection dataset Object365 [ 34] to furnish
highly reliable bounding boxes. Moreover, the rich categorylabels inherent in it ensure a comprehensive knowledge cov-erage. As illustrated in Fig. 3(b), the bounding boxes will
serve as visual prompts, which are independently sent intoboth a promptable segmenter ( i.e. segment anything model[20]) and our dense captioner to obtain the segmentation
masks and detailed semantic descriptions, respectively.
For part-level grounding data, we propose a hierarchical
annotation scheme that builds upon existing object-level an-notations, as shown in Fig. 3(c). Speciﬁcally, employing
GPT-4 [ 29] endowed with extensive external knowledge, we
decompose the objects present in a given image to gener-ate a customized part vocabulary set. This tailored vocabu-lary set is then fed into an open-vocabulary segmenter [ 35],
which yields precise part masks and bounding boxes. Thesebounding boxes are subsequently sent into our dense cap-tioner to acquire corresponding detailed captions.
Data Filtering. After completing the multi-granularity an-
notation of all images, we further introduce CLIP [ 30] for
ﬁltering. The bounding box is cropped from the originalimage, and then sent to the encoder together with the densecaption to measure the similarity. To ensure the consistency
between the visual and linguistic annotations to a great ex-tent, we retain box-caption pairs with similarity greater than0.5 as the ﬁnal annotation results.
4.2. MRES-32M Dataset Details
As listed in Table 2, existing datasets, such as the most com-
monly used benchmark dataset RefCOCO [ 44], have lim-
itations in terms of small data scale and lacking part-leveldense annotations. As one of the pioneering works for ﬁner-granularity unimodal visual comprehension, PACO [ 32]
only contains semantic tags of objects or parts, without in-
formative descriptions and visual context encompassed asreferring expressions. Summarily, we compare the pro-posed MRES-32M with existing datasets and list some
unique and signiﬁcant properties of our dataset in Table
13001
Table 2. Comparisons with previous object-level visual ground-
ing datasets and part-level segmentation datasets. # denotes thespeciﬁc number, where Cats and Avg Len denote the object/partcategories and the average length of referring expressions. “-” de-notes the corresponding part masks or captions are unavailable.
Dataset #Imgs #Objs #Parts #Cats #Avg Len
Object-Level Visual GroudingReferIt [ 18] 20K 97K – 238/– 3.2
RefCOCO [ 44] 20K 50K – 80/– 3.6
RefCOCO+ [ 44] 20K 49K – 80/– 3.5
RefCOCOg [ 27] 26K 54K – 80/– 8.4
GRES [ 25] 20K 60K – 80/– 3.7
Part-Level Segmentation & Detection
PartsIN [ 12] 24K 24K 112K 158/609 –
PascalPart [ 4] 19K 40K 363K 20/193 –
PACO [ 32] 20K 260K 641K 75/456 –
Multi-Grained Visual Grounding
MRES-32M (Ours) 1M 15.3M 16.9M 365/2299 4.6
2. Besides, we have also provided a few examples in our
MRES-32M dataset in the appendix.
Uniﬁed Multi-Granularity. In comparison to the ground-
ing counterparts, our MRES-32M is the ﬁrst visual ground-ing dataset covering both part and object granularity. Incomparison to the part-level segmentation counterparts, ourMRES-32M provides informative and unique ﬁne-graineddescription for each part mask.More Diversiﬁed Categories. Our MRES-32M is com-
posed of 365 object categories and an associated 2,299 part
categories. Compared with existing datasets, it covers awider range of multimodal knowledge and is an importantstep towards open-world understanding.Breakable Data Scales. To the best of our knowledge,
MRES-32M is the largest dataset in the current ground-ing research community. In terms of the number of im-ages and object instances, it surpasses the largest existingvisual grounding dataset RefCOCOg [ 27] by factors of 38
and 283, respectively. Meanwhile, it encompasses part in-stance counts that exceed the largest existing part semantic
segmentation dataset [ 32] by 58 times.
More Complex References. Beneﬁting from our LVLM-
based dense captioner, the reference of MRES-32M couldbe more fully combined with the visual context for entity(i.e., part and object) description. Without sticking to a
speciﬁc template, the relationships and attributes of entitiescould be highlighted in free natural language expressions.
5. Multi-Granularity RES Model
Next, we describe the proposed multi-granularity RESmodel UniRES for the uniﬁed MRES task with the refer-ring targets at both object and part granularity. Since our
original intention is to establish a simple and easy-to-followbaseline model for the proposed multi-granularity RES task,the structure of our model UniRES is designed to be simple
and clean. As shown in Fig. 4, UniRES has three major
components, which will be illustrated below.
Vision-Language Backbone. Taking account of the re-
quired attributes of both strong ability to capture the vision-language feature representations and promising scalability,we leverage the CLIP pre-trained weights of CLIP model[31], which learns transferable visual and linguistic con-
cepts from vast image-text pairs, and adopt it as the back-bone for our referring segmentation framework. The re-spective image and text encoders ( i.e. Vision Transformer
(ViT) [ 8] and Transformer [ 38] respectively) from CLIP are
utilized to effectively extract visual and linguistic features.
Query-based Grouping Design. To effectively accomplish
the proposed MRES task, it is essential to exploit both low-
level local and high-level global visual features. In order
to enhance the local-global visual representations of CLIPbackbone without introducing much additional parametersor altering model’s structure, we have incorporated 64 and8 learnable tokens (set empirically) into the ﬁrst and mid-dle layers of CLIP visual backbone. These learnable tokenstraverse the ﬁrst and second halves of the visual backbone.We expect that the ViT’s internal self-attention mechanismimplicitly serves as a manner to perform visual grouping,
obtaining representative group tokens that capture the low-
level local and high-level global features simultaneously.Based on the fact that local features spread more frag-mented, the number of appended low-level group tokens isgreater than that of high-level counterparts. Then, group to-kens from both levels are fed into a language-guided regionﬁlter (LRF) to select the language-related visual features bycross-attention mechanism, which is followed by concate-nation to fuse these expression-related visual group tokens
for subsequent vision-language decoding.
Two-Stage V-L Decoder. Now that the visual and textual
feature representations from backbone, and the expression-
related group tokens with two different levels are obtained,the two-stage mask decoder which comprises stackedTransformer layers is utilized to generate the segmentation
masks. Speciﬁcally, the ﬁrst-stage V-L decoder takes theextracted visual and textual features as input and generates
ﬁrst-stage fused multimodal representations. Subsequently,these multimodal features are further integrated with thegrouped expression-related region features of the two se-
mantic levels ( i.e., low-level & high-level) to realize further
feature enhancement, which is followed by a linear projec-tion layer to obtain the ﬁnal segmentation masks.
6. Experimental Results
To evaluate the effectiveness of our method, comprehensiveexperiments are conducted on three classic RES datasets(i.e., RefCOCO [ 44], RefCOCO+ [ 44], RefCOCOg [ 27])
and our RefCOCOm benchmark for multi-granularity RES.
13002
Figure 4. The architecture of our UniRES model as a simple baseline for the MRES task. UniRES mainly comprises three parts: the
visual and textual backbone for feature extraction, the pixel grouping design for aggregating the low-level and high-level features, and thecascaded two-stage vision-language decoder for multimodal feature fusion and the generation of segmentation masks.
6.1. Datasets
The details about the three benchmark datasets for classic
RES task can be found in the appendix.
6.2. Experimental Setup
Implementation Details. The implementation details are
provided in the appendix.
Evaluation metrics. We adopt mean Intersection-over-
Union (mIoU) and overall Intersection-over-Union (oIoU)as evaluation metrics. The mIoU measures the ratio be-tween the intersection area and the union area of the pre-dictions and labels among the test samples, while the oIoUcalculates the total intersection area over total union area.
6.3. Main Results
6.3.1 Multi-Granularity MRES Task
Comparison with SOTA Methods. To evaluate the multi-
granularity grounding performance of our UniRES as the
baseline for the proposed MRES task and previous RES
methods, we further conduct experimental comparison onour newly built RefCOCOm benchmark dataset. As pre-sented in Table 3, the classic RES methods including four
specialist models ( i.e., SeqTR [ 47], CRIS [ 40], LA VT
[42]) and two generalist models ( e.g., X-Decoder [ 48] and
SEEM [ 49]) are incorporated. For fair comparisons, we
re-implement these SOTA methods and report their perfor-mance on our RefCOCOm. It is clear that either the spe-
cialist models for classic RES task or the powerful general-
ist models perform poorly on RefCOCOm, which requiresthe crucial skills of both part-level and object-level referringsegmentation. Beneﬁting from our MRES-32M dataset,our UniRES can better master the part-level RES skills andhandle the multi-granularity RES task, achieving consider-ably higher segmentation accuracy. Due to the signiﬁcantlyhigher difﬁculty of part-level RES ( i.e. multi-granularity
RES) compared to classic RES, the absolute value of seg-
mentation accuracy is correspondingly lower. This furtheremphasizes the importance of researching ﬁner-grained partgrounding where previous SOTA methods have fallen short.Table 3. Comparison with previous SOTA methods on our Ref-
COCOm benchmark in terms of mIoU. Part and Obj & Part denotepart-only and multi-grained evaluation settings of our MRES task.
Methodsval testA testB
Part Obj & Part Part Obj & Part Part Obj & Part
Specialists
SeqTR [ 47] 13.9 28.2 12.1 22.8 18.1 34.7
CRIS [ 40] 10.6 25.4 10.1 21.2 12.9 30.0
LA VT [ 42] 15.3 29.9 13.2 24.4 18.7 35.5
Generalists
X-Decoder [ 48]16.2 29.5 13.6 23.6 20.3 33.8
SEEM [ 49] 16.1 29.4 13.6 23.4 20.4 33.9
UniRES (Ours) 19.6 34.3 16.4 27.8 25.2 41.7
Qualitative Analysis. We also conduct visual comparisons
of CRIS [ 40], LA VT [ 42] and our UniRES on the MRES
task, which is provided in appendix.
6.3.2 Classic Object-Level RES Task
Comparison with SOTA Methods. To validate the supe-
riority of our MRES-32M dataset and model UniRES, ourframework is fairly evaluated against previous SOTA meth-ods on RefCOCO [ 44], RefCOCO+ [ 44] and RefCOCOg
[27] under both zero-shot and ﬁne-tuning settings. As pre-
sented in Table 4, our method greatly outperforms previous
methods in terms of segmentation accuracy across all the
benchmark datasets. Via directly zero-shot transferring to
the downstream classic RES task after pre-training on ourMRES-32M dataset, Our UniRES model achieves a leadingzero-shot segmentation accuracy ( i.e., approximately 71%)
without any ﬁne-tuning to adapt to downstream task’s data,which is signiﬁcantly higher than all the recently proposed
zero-shot RES methods by a large margin ( i.e.,↑30-40%
mIoU). At the same time, it is worth noting that our zero-shot segmentation performance is already better than manyof previous RES methods under ﬁne-tuning setting ( e.g.,
CRIS [ 40] and ReSTR [ 19]), validating the potential of our
MRES-32M dataset and UniRES model. Furthermore, after
ﬁne-tuning on the classic RES datasets, our UniRES greatly
surpasses previous methods no matter the specialist models(e.g., LA VT [ 42] and LISA [ 22]) for classic RES or the gen-
eralist models ( e.g., X-Decoder [ 48] and SEEM [ 49]).
13003
Table 4. Comparisons with the state-of-the-art approaches on previous three classic RES benchmark datasets under both the zero-shot and
ﬁne-tuning settings. “-” denotes that the result is not provided.
MethodRefCOCO RefCOCO+ RefCOCOg
val testA testB val testA testB val test
Zero-Shot MethodsmIoURegion token [ 46](CVPR-23) 23.4 22.1 24.6 24.5 22.6 25.4 27.6 27.3
Cropping [ 46](CVPR-23) 24.8 22.6 25.7 26.3 24.1 26.5 31.9 30.9
Global-Local CLIP [ 46](CVPR-23) 26.2 24.9 26.6 27.8 25.6 27.8 33.5 33.7
SAM-CLIP [ 28](arXiv-23) 26.3 25.8 26.4 25.7 28.0 26.8 38.8 38.9
Ref-Diff [ 28](arXiv-23) 37.2 38.4 37.2 37.3 40.5 33.0 44.0 44.5
TAS [ 36](arXiv-23) 39.8 41.1 36.2 43.6 49.1 36.5 46.6 46.8
UniRES (Ours) 71.2 74.8 66.0 59.9 66.7 51.4 62.3 63.2
Fine-Tune MethodsoIoUEFNet [ 9](CVPR-21) 62.8 65.7 59.7 51.5 55.2 43.0 --
LTS [ 17](CVPR-21) 65.4 67.8 63.1 54.2 58.3 48.0 54.4 54.3
ReSTR [ 19](CVPR-22) 67.2 69.3 64.5 55.8 60.4 48.3 --
ReLA [ 25](CVPR-23) 73.8 76.5 70.2 66.0 71.0 57.7 65.0 66.0
X-Decoder [ 48](CVPR-23) -- - -- - 64.6 -
SEEM [ 49](arXiv-23) -- - -- - 65.7 -
LISA [ 22](arXiv-23) 74.9 79.1 72.3 65.1 70.8 58.1 67.9 70.6
UniRES (Ours) 77.4 80.9 74.7 69.4 76.1 61.4 69.0 71.7mIoUVLT [ 7](ICCV-21) 65.7 68.3 62.7 55.5 59.2 49.4 53.0 56.7
RefTr [ 23](NeurIPS-21) 74.3 76.8 70.9 66.8 70.6 59.4 66.6 67.4
SeqTR [ 47](ECCV-22) 71.7 73.3 69.8 63.0 66.7 59.0 65.0 65.7
CRIS [ 40](CVPR-22) 70.5 73.2 66.1 62.3 68.1 53.7 59.9 60.4
LA VT [ 42](CVPR-22) 74.5 76.9 70.9 65.8 71.0 59.2 63.3 63.6
UniRES (Ours) 79.2 81.6 76.6 73.0 78.1 65.8 71.7 73.2
6.4. Ablation Studies
We conduct ablation experiments on our RefCOCOm vali-
dation set. The tables below involves three evaluation set-
tings on our RefCOCOm, using only objects as RES targets,using only parts as RES targets, and a mixed setting thatcombines both of the above two granularity levels.
6.4.1 Ablation Study on MRES-32M Dataset
Data Granularity. We ﬁrst probe into the effect of pre-
training data’s granularity with 50% of our MRES-32M
dataset. As shown in Table 5, the baseline without pre-
training on MRES-32M dataset obtains 75.2%, 15.8% and32.0% mIoU separately on our RefCOCOm validation setunder three granularity levels. Either introducing the object-
level or part-level data from MRES-32M dataset for pre-
training consistently results in an considerable accuracy in-crease across different granularity settings. In fact, pre-training on data at a certain granularity level to improveperformance on corresponding benchmark aligns with com-mon sense. However, it is noteworthy that incorporatingpart-level data into training also enhances model perfor-mance on the only object-level RefCOCOm. This under-
scores the signiﬁcance of part-level understanding, as em-
phasized in the Sec. 1, where grasping the nuances of parts
can yield beneﬁts for object-level grounding. Besides, byjointly incorporating the training samples of both granu-
larity, our method attains 2.8% improvement against base-line under all the granularity settings on RefCOCOm, whichfully demonstrate the beneﬁt of exploiting our MRES-32Mdataset for both object-level and part-level RES tasks.
Table 5. Ablation study on the granularity of pre-training data in
our proposed MRES-32M. Object and Part denote the introductionof object-level and part-level data for pre-training.
Object PartRefCOCOm
Object-Only Part-Only Object & Part
75.2 15.8 30.5
 77.5 15.9 31.1
 75.9 18.4 32.6
  78.0 18.6 33.3
Table 6. Ablation study on the data scale of MRES-32M dataset.
RatiosRefCOCOm
Object-Only Part-Only Object & Part
0% 75.2 15.8 30.5
20% 76.8 17.3 32.0
50% 78.0 18.6 33.3
100% 79.2 19.6 34.3
Data Scale. Next, we investigate the effect of different per-
centages of training samples in our MRES-32M dataset.The results are presented in Table 6. It is obvious in Ta-
ble 6that the model performance for our MRES tasks is
13004
consistently improved with more and more employed train-
ing samples, which implicitly validates the high quality ofour built dataset. As the ratios of introduced training sam-ples continue to rise, there’s no sign of diminishing gains inmodel performance, suggesting that our framework has sig-niﬁcant potential with continually scaled up training data.
Table 7. Ablation study on the effectiveness of our MRES-32M
dataset on RES SOTA methods.
Methods MRES-32MRefCOCOm
Object-Only Part-Only Object & Part
CRIS 70.5 10.6 25.4
CRIS  73.1 15.5 29.7
LA VT 74.5 15.3 29.9
LA VT  75.7 19.3 33.2
Dataset Necessity. To prove the necessity and effective-
ness of our MRES-32M dataset for the proposed MRES
task, we take previous SOTA methods ( i.e., CRIS [ 40] and
LA VT [ 42]) on classic RES and compare the segmenta-
tion accuracy of the same models with or without pre-training on our MRES-32M. As shown in Table 7, the orig-
inal CRIS and LA VT can already well handle the classicRES task with only object-level grounding skills, but it per-forms poorly on the multi-grained RefCOCOm benchmark,which requires part-level grounding capability. In contrast,pre-training on our MRES-32M consistently leads to greatperformance gains no matter on object-only, part-only or
the multi-granularity RES tasks, because our high-quality
MRES-32M dataset can effectively enable the model tohandle the part-level visual grounding task and enhances theoriginal object-level grounding capability.
6.4.2 Ablation Study on UniRES Model
Table 8. Ablation study on the query-based grouping design.
High-Level Low-LevelRefCOCOm
Object-Only Part-Only Object & Part
74.4 14.9 29.6
 74.9 15.2 30.0
 74.9 15.4 30.1
  75.2 15.8 30.5
Besides, we additionally conduct ablation studies on the
structural design of our model UniRES. As presented in Ta-ble8, removing the simple but effective grouping design at
two different levels (this simultaneously leads to discardingthe second-stage decoder) will result in a considerable dete-rioration in model performance. Since the sequentially ap-
pended pixel grouping tokens are introduced to effectively
capture the high-level and low-level visual features for fur-ther visual feature enhancement, losing any type of thesegroup tokens will lead to a decrease in the model’s seg-
mentation accuracy on both object-level and part-level REStasks. In order to verify that the high-level and low-level
pixel grouping tokens can capture the clustering features of
the corresponding level, we also visualize the group tokensof the two levels (before being sent to LRF) for qualita-
tive analysis, where the different colors represent the vari-ous areas of the same clustering group. The visualizationresults in Fig. 5afﬁrms that our introduced low-level and
high-level group tokens respectively capture the part-levellocal features and aggregate the object-level global featureswith stronger semantics through the pixel grouping process,
which aligns with our intentions for the grouping design.
(a) Image (b) Low-Level (c) High-Level
Figure 5. Qualitative analysis for ablation study on the object-level
and part-level grouping design in our model structure. (a) the inputimage. (b) low-level group tokens. (c) high-level group tokens.
7. Conclusion and Broader Impact
In this paper, we move beyond previous works that fo-
cused solely on object-level visual grounding tasks and takea step further to ﬁner-grained part-level RES. We put for-ward a new multi-granularity referring expression segmen-tation task and establish an evaluation benchmark namedRefCOCOm by manual annotation. To advance the vi-sual grounding at both object and part levels towards ﬁner-grained vision-language understanding, we build the largestvisual grounding dataset MG-32M to date, which is also theﬁrst dataset to provide part-level vision-language annota-
tions. Furthermore, we have developed a simple yet strongmulti-grained referring segmentation model called UniRES.As a baseline for our newly proposed MRES task, UniRES
achieves the new state-of-the-art performance on both ourRefCOCOm for MRES task and three classic RES datasets.We plan to release our RefCOCOm benchmark, the MG-
32M dataset, and the UniRES model to the public, aspir-ing to foster future research in ﬁne-grained visual ground-ing tasks and to inspire new research in this direction.
Acknowledgement
We thank Yepeng Tang for the helpful discussions on thiswork, all the Image & Video Analysis Group (IV A)’s mem-
bers in CASIA for the technical support, and all the in-
sightful reviewers for the helpful suggestions. This workwas supported by the National Science and Technology Ma-
jor Project (No.2022ZD0118801), National Natural Science
Foundation of China (U21B2043, 62206279).
13005
References
[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan
Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren
Zhou. Qwen-vl: A frontier large vision-language model with
versatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 3
[2] Jun Chen, Deyao Zhu1 Xiaoqian Shen1 Xiang Li, Zechun
Liu2 Pengchuan Zhang, Raghuraman Krishnamoorthi2Vikas Chandra2 Y unyang Xiong, and Mohamed Elhoseiny.Minigpt-v2: Large language model as a uniﬁed interface
for vision-language multi-task learning. arXiv preprint
arXiv:2310.09478 , 2023.
[3] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,
Feng Zhu, and Rui Zhao. Shikra: Unleashing multi-modal llm’s referential dialogue magic. arXiv preprint
arXiv:2306.15195 , 2023. 3
[4] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fi-
dler, Raquel Urtasun, and Alan Y uille. Detect what you
can: Detecting and representing objects using holistic mod-
els and body parts. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1971–1978,
2014. 2,3,4,5
[5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Y onghao
Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source
chatbot impressing gpt-4 with 90%* chatgpt quality. See
https://vicuna. lmsys. org (accessed 14 April 2023) , 2023.
3
[6] Daan de Geus, Panagiotis Meletis, Chenyang Lu, Xiaox-
iao Wen, and Gijs Dubbelman. Part-aware panoptic seg-
mentation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 5485–
5494, 2021. 3
[7] Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang.
Vision-language transformer and query generation for refer-ring segmentation. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 16321–16330,
2021. 2,7
[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 5
[9] Guang Feng, Zhiwei Hu, Lihe Zhang, and Huchuan Lu. En-
coder fusion network with co-attention embedding for refer-ring image segmentation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 15506–15515, 2021. 7
[10] Ke Gong, Xiaodan Liang, Dongyu Zhang, Xiaohui Shen,
and Liang Lin. Look into person: Self-supervised structure-
sensitive learning and a new benchmark for human parsing.InProceedings of the IEEE conference on computer vision
and pattern recognition , pages 932–940, 2017. 2,3
[11] Michael Grubinger, Paul Clough, Henning M ¨uller, and
Thomas Deselaers. The iapr tc-12 benchmark: A new eval-
uation resource for visual information systems. In Interna-
tional workshop ontoImage , 2006. 2[12] Ju He, Shuo Yang, Shaokang Yang, Adam Kortylewski, Xi-
aoding Y uan, Jie-Neng Chen, Shuai Liu, Cheng Yang, Qi-hang Y u, and Alan Y uille. Partimagenet: A large, high-quality dataset of parts. In European Conference on Com-
puter Vision , pages 128–145. Springer, 2022. 2,3,
4,5
[13] Ronghang Hu, Marcus Rohrbach, and Trevor Darrell. Seg-
mentation from natural language expressions. In Computer
Vision–ECCV 2016: 14th European Conference, Amster-dam, The Netherlands, October 11–14, 2016, Proceedings,
P a r tI1 4 , pages 108–124. Springer, 2016. 2
[14] Y utao Hu, Qixiong Wang, Wenqi Shao, Enze Xie, Zhenguo
Li, Jungong Han, and Ping Luo. Beyond one-to-one: Re-
thinking the referring image segmentation. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-sion , pages 4067–4077, 2023. 2
[15] Zhiwei Hu, Guang Feng, Jiayu Sun, Lihe Zhang, and
Huchuan Lu. Bi-directional relationship inferring net-
work for referring image segmentation. In Proceedings of
the IEEE/CVF conference on computer vision and patternrecognition , pages 4424–4433, 2020. 2
[16] Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Y unchao
Wei, Jizhong Han, Luoqi Liu, and Bo Li. Referring im-
age segmentation via cross-modal progressive comprehen-sion. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition , pages 10488–10497,
2020. 2
[17] Ya Jing, Tao Kong, Wei Wang, Liang Wang, Lei Li, and Tie-
niu Tan. Locate then segment: A strong pipeline for referringimage segmentation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
9858–9867, 2021. 7
[18] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and
Tamara Berg. Referitgame: Referring to objects in pho-tographs of natural scenes. In Proceedings of the 2014 con-
ference on empirical methods in natural language processing
(EMNLP) , pages 787–798, 2014. 2,5
[19] Namyup Kim, Dongwon Kim, Cuiling Lan, Wenjun Zeng,
and Suha Kwak. Restr: Convolution-free referring im-age segmentation using transformers. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 18145–18154, 2022. 2,6,7
[20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. arXiv preprint arXiv:2304.02643 , 2023. 3,4
[21] Ranjay Krishna, Y uke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-tidis, Li-Jia Li, David A Shamma, et al. Visual genome:Connecting language and vision using crowdsourced dense
image annotations. International journal of computer vision ,
123:32–73, 2017. 2,4
[22] Xin Lai, Zhuotao Tian, Y ukang Chen, Yanwei Li, Y uhui
Y uan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentationvia large language model. arXiv preprint arXiv:2308.00692 ,
2023. 2,6,7
[23] Muchen Li and Leonid Sigal. Referring transformer: A
one-step approach to multi-task visual grounding. Advances
13006
in neural information processing systems , 34:19652–19664,
2021. 7
[24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,P a r tV1 3 , pages 740–755. Springer, 2014. 2,4
[25] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Gener-
alized referring expression segmentation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 23592–23601, 2023. 2,5,7
[26] Kaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna
Tripathi, Leonidas J Guibas, and Hao Su. Partnet: A large-scale benchmark for ﬁne-grained and hierarchical part-level
3d object understanding. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition ,
pages 909–918, 2019. 2,3
[27] V arun K Nagaraja, Vlad I Morariu, and Larry S Davis. Mod-
eling context between objects for referring expression under-
standing. In Computer Vision–ECCV 2016: 14th European
Conference, Amsterdam, The Netherlands, October 11–14,2016, Proceedings, Part IV 14 , pages 792–807. Springer,
2016. 2,5,6
[28] Minheng Ni, Yabo Zhang, Kailai Feng, Xiaoming Li, Yi-
wen Guo, and Wangmeng Zuo. Ref-diff: Zero-shot referringimage segmentation with generative models. arXiv preprint
arXiv:2308.16777 , 2023. 2,7
[29] R OpenAI. Gpt-4 technical report. arxiv 2303.08774. View
in Article , 2023. 4
[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 4
[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 5
[32] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi
Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Mar-quez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts
and attributes of common objects. In Proceedings of the
IEEE/CVF Conference on Computer Vision and PatternRecognition , pages 7141–7151, 2023. 2,3,4,5
[33] N Dinesh Reddy, Minh V o, and Srinivasa G Narasimhan.
Carfusion: Combining point tracking and part detection fordynamic 3d reconstruction of vehicles. In Proceedings of
the IEEE conference on computer vision and pattern recog-nition , pages 1906–1915, 2018. 2,3
[34] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang
Y u, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A
large-scale, high-quality dataset for object detection. In Pro-
ceedings of the IEEE/CVF international conference on com-
puter vision , pages 8430–8439, 2019. 2,4[35] Peize Sun, Shoufa Chen, Chenchen Zhu, Fanyi Xiao, Ping
Luo, Saining Xie, and Zhicheng Yan. Going denser
with open-vocabulary part segmentation. arXiv preprint
arXiv:2305.11173 , 2023. 3,4
[36] Y ucheng Suo, Linchao Zhu, and Yi Yang. Text augmented
spatial-aware zero-shot referring image segmentation. arXiv
preprint arXiv:2310.18049 , 2023. 2,7
[37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and ﬁne-tuned chat models. arXiv
preprint arXiv:2307.09288 , 2023. 3
[38] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 5
[39] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-
ona, and Serge Belongie. The caltech-ucsd birds-200-2011
dataset. 2011. 2,3
[40] Zhaoqing Wang, Y u Lu, Qiang Li, Xunqiang Tao, Yandong
Guo, Mingming Gong, and Tongliang Liu. Cris: Clip-
driven referring image segmentation. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 11686–11695, 2022. 2,6,7,8
[41] Chenyun Wu, Zhe Lin, Scott Cohen, Trung Bui, and
Subhransu Maji. Phrasecut: Language-based image segmen-
tation in the wild. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
10216–10225, 2020. 2
[42] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Heng-
shuang Zhao, and Philip HS Torr. Lavt: Language-aware
vision transformer for referring image segmentation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 18155–18165, 2022. 2,6,7,
8
[43] Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang.
Cross-modal self-attention network for referring image seg-
mentation. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 10502–
10511, 2019. 2
[44] Licheng Y u, Patrick Poirson, Shan Yang, Alexander C Berg,
and Tamara L Berg. Modeling context in referring expres-
sions. In Computer Vision–ECCV 2016: 14th European
Conference, Amsterdam, The Netherlands, October 11-14,2016, Proceedings, Part II 14 , pages 69–85. Springer, 2016.
2,3,4,5,6
[45] Licheng Y u, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu,
Mohit Bansal, and Tamara L Berg. Mattnet: Modular at-
tention network for referring expression comprehension. In
Proceedings of the IEEE conference on computer vision andpattern recognition , pages 1307–1315, 2018. 2
[46] Seonghoon Y u, Paul Hongsuck Seo, and Jeany Son. Zero-
shot referring image segmentation with global-local context
features. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 19456–
19465, 2023. 2,7
[47] Chaoyang Zhu, Yiyi Zhou, Y unhang Shen, Gen Luo, Xingjia
Pan, Mingbao Lin, Chao Chen, Liujuan Cao, Xiaoshuai Sun,
13007
and Rongrong Ji. Seqtr: A simple yet universal network for
visual grounding. In Computer Vision–ECCV 2022: 17th
European Conference, Tel Aviv, Israel, October 23–27, 2022,
Proceedings, Part XXXV , pages 598–615. Springer, 2022. 6,
7
[48] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li,
Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, LuY uan, et al. Generalized decoding for pixel, image, and lan-guage. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 15116–15127,
2023. 2,6,7
[49] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li,
Jianfeng Gao, and Y ong Jae Lee. Segment everything every-where all at once. arXiv preprint arXiv:2304.06718 , 2023.
2,6,7
13008
