TeMO: Towards Text-Driven 3D Stylization for Multi-Object Meshes
Xuying Zhang1, Bo-Wen Yin1, Yuming Chen1, Zheng Lin1, Yunheng Li1, Qibin Hou2,1*, Ming-Ming Cheng2,1
1VCIP, College of Computer Science, Nankai University
2NKIARI, Shenzhen Futian
https://github.com/zhangxuying1004/TeMO
Abstract
Recent progress in the text-driven 3D stylization of a sin-
gle object has been considerably promoted by CLIP-based
methods. However, the stylization of multi-object 3D scenes
is still impeded in that the image-text pairs used for pre-
training CLIP mostly consist of an object. Meanwhile, the
local details of multiple objects may be susceptible to omis-
sion due to the existing supervision manner primarily re-
lying on coarse-grained contrast of image-text pairs. To
overcome these challenges, we present a novel framework,
dubbed TeMO, to parse multi-object 3D scenes and edit
their styles under the contrast supervision at multiple levels.
We first propose a Decoupled Graph Attention (DGA) mod-
ule to distinguishably reinforce the features of 3D surface
points. Particularly, a cross-modal graph is constructed to
align the object points accurately and noun phrases decou-
pled from the 3D mesh and textual description. Then, we
develop a Cross-Grained Contrast (CGC) supervision sys-
tem, where a fine-grained loss between the words in the
textual description and the randomly rendered images are
constructed to complement the coarse-grained loss. Exten-
sive experiments show that our method can synthesize high-
quality stylized content and outperform the existing methods
over a wide range of multi-object 3D meshes.
1. Introduction
3D asset creation through stylization aims to synthesize
stylized content on the bare meshes to conform to the given
text descriptions [15, 19,25], referring images [38, 50], or
3D shapes [34, 44]. This research plays an important role in
a wide spectrum of applications, e.g.,virtual/augmented re-
ality [5, 9], gaming industries [49], and robotics [13]. More-
over, it also presents considerable potential and has attracted
increasing attention in computer vision and graphics com-
munities. Considering the ready availability and expres-
*Qibin Hou is the corresponding author.
Bare Mesh TANGO Our TeMO
two dragons ‚Äúa fire dragon and an ice dragon‚Äù
person&dragon ‚Äúan iron man and an ice dragon‚Äù
Figure 1. Visual comparisons between the existing 3D styliza-
tion methods (e.g., TANGO [15]) and our TeMO in multi-object
scenes. For a scene with multiple objects of the same/different cat-
egories, existing methods are prone to interference between differ-
ent properties of the objects, while our TeMO is able to accurately
synthesize the desired stylized content for each object.
siveness of text prompts as well as the popularity of large-
scale Contrastive Language-Image Pre-training (CLIP) [28]
model, we choose to work with text-driven 3D stylization.
Recent years have witnessed the emergence of a series
of impressive works [8, 15,23,25], aiming to drive the ad-
vancement of text-driven 3D stylization. Existing methods
usually adopt multi-layer perceptrons (MLPs) to predict the
location attribute displacements of the bare mesh under the
supervision of the contrastive loss in CLIP. We observe that
these works focus on the stylization of a single 3D object
and perform poorly on multiple objects, as shown in the
second column of Fig. 1. We argue that two inherent charac-
teristics of CLIP result in this issue: i)CLIP is mainly pre-
trained with image-text pairs mostly consisting of a single
object; ii)CLIP loss employs global representation vectors
from images and text to coarsely match these two modali-
ties, which inevitably causes the loss of local details. More-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
19531
over, the key to synthesizing desired styles for multiple 3D
objects lies in the parsing of such 3D scenes and the multi-
grained supervision for details refinement.
To simultaneously generate stylized content for multiple
3D objects, the primary step is to achieve accurate align-
ment between the objects in the 3D mesh and the target text.
However, existing methods employ global semantics of the
text to stylize a single object, which inevitably produces
noises when stylizing the objects in multi-object scenes.
To overcome this challenge, we propose to parse the 3D
scene by introducing a Decoupled Graph Attention (DGA)
module. Specifically, all noun phrases are decoupled from
the text prompt and the mesh surface points of the current
view are divided into several clusters as well. Then, a cross-
modal graph is constructed to establish connections of the
noun phrases to their corresponding object points while dis-
tancing them from the irrelevant ones. This graph enables
the accurate interaction between two interrelated modali-
ties. Finally, the surface point features of 3D objects can be
reinforced by independent cross-attention fusions with their
neighboring word nodes in the graph architecture.
Furthermore, we also design a Cross-Grained Contrast
(CGC) loss to perform comprehensive cross-modal super-
vision for the stylization of multiple objects. The goal is
to guide the network to generate more stylization details
for multiple 3D objects to match the target text. Our loss
consists of two parts, i.e.,coarse-grained contrast and fine-
grained contrast. In the former part, the text prompt is re-
garded as sentence-level supervision, which calculates the
similarity between the 2D views rendered from the stylized
3D mesh and the text prompt using the global feature vec-
tors from the CLIP model. In the latter part, we see the text
prompt from the word level, and consider the similarities
between each word of the sentence and the rendered images
of the view sets. To be specific, we produce the word rep-
resentations of the text prompt by taking the hidden states
from the text encoder of CLIP. Motivated by the recent pro-
cess in video-text retrieval [22], we calculate fine-grained
loss via the weighted summation of the element in similar-
ity vectors based on the importance of each word or image.
Based on the well-designed DGA module and CGC loss,
we propose a novel framework towards Text-Driven 3D
stylization for Multi-Object Meshes, called TeMO. To val-
idate the effectiveness of our TeMO, extensive experiments
are conducted on various multi-object 3D scenes, as shown
in the 3rdcolumn of Fig. 1. The experimental results
demonstrate our TeMO is less susceptible to interference
from multiple objects and can generate superior stylized as-
sets compared with the existing 3D stylization methods.
Our contributions can be summarized as follows:
‚Ä¢ We present a new 3D stylization framework, called
TeMO. To the best of our knowledge, it is the first at-
tempt to parse the objects in the text and 3D meshes andgenerate stylizations for multi-object scenes.
‚Ä¢ We propose a Decoupled Graph Attention (DGA) mod-
ule, which constructs a graph structure to align the sur-
face points in the multi-object mesh and the noun phrases
in the text prompt.
‚Ä¢ We design a Cross-Grained Contrast (CGC) loss, in
which the text is contrasted with the rendered images
from sentence and word levels.
2. Related Work
2.1. Text-Driven 3D Manipulation
Generating or editing 3D content according to a given
prompt is a long-standing objective in computer vision and
graphics [2, 39,42]. Among all forms of the prompt, the
text has garnered the most conspicuous gaze due to three
reasons: i) Text descriptions are readily accessible from the
existing corpus; ii) Text descriptions are particularly user-
friendly since they are easily modifiable and can effectively
express complex concepts related to stylizations; iii) The
popularity of large-scale multimodal models [16, 28] has
made achieving visual-language supervision possible.
Text2Mesh [25] proposes a neural-style field network
to predict the color and displacement of mesh vertices.
TANGO [15] proposes to disentangle the appearance style
as the spatially varying bidirectional reflectance distribu-
tion, the local geometric variation, and the lighting condi-
tion. Then, X-Mesh [23] integrates the target text guidance
by utilizing text-relevant spatial and channel-wise attention
during vertex feature extraction. Motivated by the remark-
able progress in text-driven 2D generation [29, 31], TEX-
Ture [30] and Text2Tex [7] incorporate a pre-trained depth-
aware image diffusion model to synthesize high-resolution
partial textures from multiple viewpoints progressively.
To make full use of the priors in the pre-trained 2D text-
to-image diffusion model, DreamFusion [27] introduces a
Score Distillation Sample (SDS) loss to perform text-to-3D
synthesis. With the help of SDS loss, Latent-NeRF [24]
and Fantasia3D [8] can generate 3D shapes and appear-
ances for 3D objects. Despite achieving impressive results,
these methods focus on the stylization of a single 3D object
and rarely explore multi-object scenes. CLIP-Mesh [26] at-
tempts to generate multiple 3D objects for target text. Nev-
ertheless, the resulting content is not satisfactory. In this
paper, we parse the objects described in rendered images
and text prompts, aligned by two well-designed strategies.
2.2. Attention Mechanism
The concept of the attention mechanism was initially intro-
duced in neural machine translation [1], where the weighted
summation of the candidate vectors is calculated accord-
ing to their importance scores. This technology has been
extended to a myriad of tasks, e.g., natural language pro-
19532
Ray  CastingDiffuse
Roughness
Specular
NormalLightingText
EncoderImage
Encoder
RenderingAvg
ùë•ùëù
ùëõùëùùíáùíì‚àô
ùíáùíè‚àô‚Äúa superman , a
fire dragon ,and 
an ice whale ‚Äù
‚ãÆ
‚ãÆ
‚ãÆ‚ãÆ
Loss
MeshText PromptT ‚ààRm√ódimI ‚ààRn√ódimFT‚ààRdimFI‚ààRdim
CGC
DGA
DGA
Figure 2. The overall architecture of the proposed TeMO framework. We first specify several cameras to cast rays toward the objects in
the 3D mesh scene. Then, a surface point xpand normal npcan be attained from each ray intersected with the objects. These points
and normals are fed to the attribute prediction network where the features of 3D objects are parsed and interacted with the decoupled
text features via our proposed DGA module. Meanwhile, we employ a series of spherical Gaussians to represent the lighting. Finally, a
differentiable SG render is adopted to render images, which are utilized to contrast with the text prompt by our designed CGC loss.
cessing [10, 21, 37], computer vision [14, 17,47,54], and
multi-modal learning [20, 41,48,53]. For instance, Trans-
former [37] employs the self-attention operation to estab-
lish connections between words within a sentence and uti-
lize the cross-attention mechanism to align source and tar-
get sentences. Non-local network [40] takes the lead in
introducing self-attention to computer vision and achieves
great success in video understanding and object detection.
ViT [11] treats an image as a sequence of patches and em-
ploys a Transformer encoder based on self-attention to per-
form image classification. Swin Transformer [17] intro-
duces shifted windows to enhance the local perception abil-
ity of self-attention. More recently, X-Mesh [23] designs a
text-guided dynamic attention mechanism for vertex feature
extraction of a 3D object. However, this guidance only re-
lies on a text feature vector without considering the parsing
of text and 3D scenes. In this paper, the multiple objects
decoupled from the target text and 3D mesh are aligned via
a cross-modal graph to achieve precise guidance.
2.3. Multi-modal Contrastive Learning
Contrastive learning has become an increasingly popular re-
search topic in the multi-modal community due to its ability
to align different modal representations. Based on this strat-
egy, CLIP [28] is pre-trained on an abundance of image-
text pairs, achieving great success in cross-modal super-
vision. TACo [45] presents a token-aware cascade con-
trastive learning based on the syntactic classes of words to
achieve fine-grained semantic alignment in text-video re-
trieval. Concurrently, FILIP [46] proposes comparing the
image patches with the words in the sentence. Regarding
the text-driven 3D stylization, the CLIP loss, which calcu-
lates the similarity between the image and text vectors in theembedding space of CLIP, is adopted by the vast majority
of methods. Although achieving impressive results in styl-
izing a single object, these methods cannot be well adapted
to scenes with multiple 3D objects. We argue an important
reason for this issue is the loss of local details caused by
such coarse-grained supervision. In this paper, we propose
a cross-grained supervision strategy, which considers fine-
grained and coarse-grained contrasts to achieve a more pre-
cise semantic alignment between rendered image and text.
3. Methodology
3.1. Overall Architecture
Fig. 2shows the end-to-end architecture of our TeMO
framework. Given a bare mesh and a text prompt contain-
ing multiple objects, the TeMO aims to synthesize styliza-
tion on the mesh to match the text descriptions. We employ
a set of vertices V‚ààRe√ó3and faces F‚àà {1, ..., e}u√ó1
to explicitly define the input triangle mesh, which is fixed
throughout the training. Following TANGO [15], we disen-
tangle the appearance style as the spatially varying bidirec-
tional reflectance distribution function [4, 51,52] (including
diffuse, roughness, specular), the local geometric variation
(normal map), and the lighting condition.
We start by normalizing the vertex coordinates to lie
inside a unit sphere. Then, we randomly sample points
around the mesh using Gaussian distribution as camera po-
sitions to render images. Next, we can obtain a camera ray
Rp={c+tŒΩp}from the sampled camera position cand
a pixel pin rendered images, where ŒΩpis the direction of
the ray. Further, ray casting [32] is used to seek out the
ray and mesh‚Äôs first intersection point and intersection face.
Moreover, the normal np‚ààR3of the intersection face is
19533
‚Äúa blue whale, an ice 
dragon with red eyes, 
and an iron man‚Äù
ùë•ùëù
‚Äúan iron man‚Äù‚Äúan ice dragon with red eyes‚Äù‚Äúa blue whale‚Äù
Inputs Scene Parsing Object Alignment Between Different Modalities
ùë•1
‚ãÆùë§11
‚ãÆùë§22
‚ãÆùë§24
‚ãÆùë§33ùë•ùëô
‚ãÆ
ùë•ùëù
‚ãÆ‚ãÆ
Location Nodes Word Nodes Text PromptMesh
‚Äúan ice dragon with red eyes‚Äù
‚Äúa blue whale‚Äù
‚Äúan iron man‚ÄùFigure 3. Construction pipeline of the cross-modal graph architecture in our DGA module. Note that xp, the surface point of the 3D
objects, and wij, the j-th word in the i-th noun phrase, are connected together only if they correspond to the same object.
employed as surface normal at the point xp‚ààR3.
To achieve multiview-consistent features, our TeMO is
restricted to predicting the normal displacement as a func-
tion of the location, while allowing the color materials
to be predicted as a function of both location and view-
ing direction. Therefore, our TeMO represented as MLPs
includes two branches, i.e.,normal branch fn(¬∑)and re-
flectance branch fr(¬∑). Specifically, the former is utilized
as the prediction of normal offset on the point xp, and the
latter is designed to predict surface reflectance coefficients
of the material at the location xp,i.e.,diffuse, roughness,
and specular. To synthesize high-frequency details, we also
apply the Fourier positional encoding [35] to every input.
In addition, the spherical Gaussian is employed to represent
each light intensity Li(¬∑)due to its closed-form nature and
analytical solution. Based on the attained geometric and
appearance components, each pixel color in the rendered
image can be calculated by a hemisphere renderer [15]:
Lp(ŒΩp, xp, np) =Z
‚Ñ¶Li(wi)fr(ŒΩp,wi,xp)(wi¬∑ÀÜnp)dwi,(1)
ÀÜnp=np+fn(xp, np), (2)
where ‚Ñ¶ ={wi:wi¬∑ÀÜnp‚â•0}represents the hemisphere,
wiis the incident light direction, and ÀÜnpis the estimated
normal on surface point xp.
3.2. Decoupled Graph Attention
To achieve text-drive stylization for multiple 3D objects,
the key issue that needs to be solved is the accurate align-
ment between the objects described in the text and those
in the meshes. X-Mesh [23] has incorporated text-guided
dynamic linear layers, in which the global representation
vector of the target object in the text is utilized as guid-
ance to acquire text-aware vertex features. Nevertheless,
the global vector containing information about multiple ob-
jects is prone to mutual interference and produces semantic
noises during guidance for multi-object scenes.To address this challenge, we propose to parse the ob-
jects in the text and mesh. We first extract the noun phrases
modified by adjectives or prepositional phrases from the
text using the NLTK tools [3]. Then, we employ the Gaus-
sian Mixture Model (GMM) [56] to cluster the intersection
point set {x1, ..., x p, ...} of the current ray and the mesh.
Meanwhile, we can obtain a binary map of objects in the
current view based on whether the ray intersects with the
mesh. Further, we can decouple the objects in the binary
map according to the clustered points and acquire several
binary maps of individual objects. Based on the disentan-
gled noun phrases and binary maps for multiple objects, we
can match the correct pairs by their semantic similarities.
As a result, the objects described in the text are aligned with
their corresponding objects in the mesh, which are utilized
to construct a cross-modal graph G= (V ,E), as shown in
Fig. 3. To be specific, all surface point features and word
features are considered as independent nodes to form the
node set V. For the edge set V, the link between the surface
point node and the word node will be built if the semantic
objects they belong to are the same.
Under the setting of this cross-modal graph, we can indi-
vidually perform cross-attention between the surface point
nodes and their neighboring word nodes, where the parsed
surface point features are used as queries, and the parsed
text features serve as keys and values. The enhancement of
the surface point node vi‚ààRdimcan be formulated as:
ÀÜvi=X
vj‚ààAdj(v i)Œ±ijLinear(v j), (3)
Œ±ij=eWij
P
vj‚ààAdj(v i)eWij, (4)
Wij=Linear(v i)Linear(v j)T
‚àödl, (5)
where Adj(v i)is the adjacency nodes of viandLinear(¬∑)
represents a linear transformation. With this attention
19534
mechanism, the surface point features of different objects in
the mesh can be distinguishably reinforced under the guid-
ance of the word features in the parsed text.
3.3. Cross-Grained Contrast Supervision
To guide the optimization of the neural network for 3D styl-
ization, the first step is to render the stylized 3D mesh from
multiple 2D views. Most existing methods usually employ
the visual encoder and text encoder of CLIP [28] to extract
global feature vectors for the rendered image and target text,
respectively, which are contrasted to perform cross-modal
supervision via cosine similarity:
Lcoarse =‚àíFI¬∑ FT
‚à•FI‚à•2‚à•FT‚à•2, (6)
where FI‚ààR512is the averaged feature vector of the im-
ages rendered from different views, FT‚ààR512denotes the
global feature vector of the target text, and ‚à• ¬∑ ‚à• 2represents
the Euclidean norm function.
Although achieving impressive results for stylizing a sin-
gle 3D object, these methods still have limitations in multi-
object scenes. Considering that a single feature vector still
represents a sentence describing multiple objects, the ob-
ject details may be lost in large amounts. Therefore, such a
coarse-grained contrast supervision is insufficient to guide
the neural network in synthesizing photorealistic stylized
content for multiple 3D objects.
To solve this issue, we construct a fine-grained contrast
supervision to complement the coarse-grained one. Specifi-
cally, we first calculate the correlation map, i.e.,S ‚ààRn√óm
between the word features in the text and the visual features
of the rendered images, which are also extracted from the
text encoder and visual encoder of the CLIP:
S=I ¬∑ TT
‚à•I‚à•2‚à•T ‚à• 2, (7)
where I ‚ààRn√ó512represents the features of the images
rendered from nviews, T ‚ààRm√ó512indicates the features
ofmwords in the text. Then, we normalize the correlation
matrix along the image axis and the text axis, respectively,
to retrieve the text of interest and visual components. This
process can be formulated as:
SI(i) =Pm
k=1S(i, k)
m, (8)
ST(j) =Pn
k=1S(k, j)
n. (9)
Inspired by [22], we further calculate an image-centered
fine-grained contrast score and a text-centered fine-grained
contrast score by the weighted summation of the similarityvectors, which can be formulated as follows:
LI=nX
i=1eSI(i)
Pn
k=1eSI(k)SI(i), (10)
LT=mX
j=1eST(j)
Pm
k=1eST(k)ST(j), (11)
where the weights are defined as the degree of correlation
between the central and another modality. Finally, we adopt
the average value of these two scores as the fine-grained
contrast loss, which can be defined as:
Lfine=‚àí(L I+LT)/2. (12)
The coarse-grained and fine-grained contrast supervision
complement each other to build a cross-grained contrast su-
pervision system. The former is utilized to align the global
semantic information of the target text with the 3D objects,
and the latter is used to achieve the local semantic align-
ment. This loss can be defined as:
Lcgcs=ŒªcLcoarse +ŒªfLfine, (13)
where ŒªcandŒªfare two hyper-parameters to balance the
cross-grained and the fine-grained losses, set to 1.0 and
0.33, respectively.
4. Experiments
4.1. Experiment Setup
Datasets. To examine our method across a diverse set of 3D
scenes, we first collect 3D object meshes from a variety of
sources, i.e.,COSEG [33], Thingi10K [55], Shapenet [6],
Turbo Squid [36], and ModelNet [43]. Then, we randomly
place several objects from the collected 3D set into a mesh
using Blender. Note that we down-sample the number of
meshes‚Äô vertices and faces to ensure the robustness of our
TeMO to low-quality meshes and reduce the burden of GPU
during the stylization. The meshes used in this paper con-
tain an average of 79,303 faces, 16% non-manifold edges,
0.2% non-manifold vertices, and 12% boundaries.
Implementation Details. Following the TANGO [15] net-
work, we adopt 3 linear layers with 256 dimensions to build
the normal estimation branch. In the reflectance branch, the
point features are extracted by 2 shared layers with 256 di-
mensions, followed by 3 exclusive layers to predict diffuse,
specular, and roughness. The dimension of our DGA mod-
ule is also set as 256. The word features in our DGA module
are extracted from the text encoder of CLIP, and so are the
ones in our CGC loss. We choose ViT-B/32 as the backbone
of the pre-trained CLIP model in this paper, which is consis-
tent with previous works [15, 23,25]. We also process the
rendered images with 2D augmentation strategies [12, 15]
19535
vase &candle Text Prompt: ‚Äúa cactus vase and a silver candle‚Äù Text Prompt: ‚Äúa wicker vase and a candle in jeans‚Äùcat&horse Text Prompt: ‚Äúa Garfield cat and a brown horse‚Äù Text Prompt: ‚Äúa ginger cat and an astronaut horse‚Äùperson &dragon Text Prompt: ‚Äúan iron man and an ice dragon‚Äù Text Prompt: ‚Äúa superman and a fire dragon‚Äù
Figure 4. Given the same bare mesh, our TeMO produces various stylized contents for multi-object scenes to conform to the text prompts.
before feeding them into the pre-trained CLIP model. Our
TeMO model is optimized with the AdamW [18] strategy
for 1500 iterations, where the learning rate is initialized to
5√ó10‚àí4and decayed by 0.7 every 500 iterations. The
entire training process takes about 10 minutes on a single
NVIDIA RTX 3090 GPU.
4.2. Qualitative Evaluation
We conduct visualization experiments on a wide spectrum
of multi-object scenes to verify the effectiveness of our
TeMO. However, we observe that the 3D symmetry prior
used widely in previous works [15, 25] can cause interfer-
ence between different parts during the stylization process
of multiple objects. We argue that the multiple objects of
the meshes used in this paper are randomly placed to simu-
late a real 3D scene rather than along the z-axis. To avoid
this issue, we remove this prior in our TeMO and previous
methods involved in the comparison.
Neural Stylization and Controls. We present the styliza-
tion results of our TeMO driven by different text prompts
for the same multi-object mesh in Fig. 4. As shown in the
1strow where the 3D scene is composed of a person and a
dragon, our TeMO can accurately distinguish between the
person object and the dragon object and appropriately styl-
ize different body parts for them according to the semantic
roles described in each text prompt. Meanwhile, our TeMO
also synthesizes desired stylizations for the 3D objects inthe cat-horse mesh and vase-candle mesh, as shown in the
2ndand3rdrows. These experimental results demonstrate
that our TeMO method can generate photorealistic details
with fine granularity and maintain global semantic under-
standing for the given multi-object 3D scene.
Qualitative Comparisons. We provide the visual compar-
isons of prediction results between our TeMO and previous
pioneering works in text-driven 3D object stylization, in-
cluding Text2Mesh [25], TANGO [15], and X-Mesh [23].
To ensure a fair comparison, we adopt the official imple-
mentations of these methods and also train them with the
default settings without the symmetry prior. The experi-
mental results show it is a real struggle for Text2Mesh [25]
and TANGO [15] to understand the detailed semantics of
the text prompt with multiple objects. As shown in the 1st
row of Fig. 5where the 3D scene contains two objects of
the same category, given a text prompt ‚Äúa fire dragon and an
ice dragon‚Äù, they tend to capture the ‚Äúice‚Äù property, missing
the ‚Äúfire‚Äù property. For a 3D scene containing two objects
of different categories, they are prone to mixing the proper-
ties of these objects, as shown in the 2ndrow where the text
prompt is ‚Äúa wood vase and a brick candle‚Äù. Therefore, the
stylized assets they generate for these multi-object scenes
are unsatisfactory. X-Mesh generates more accurate results
that align with the text prompts, as shown in the 1stand
2ndrows, which can be attributed to incorporating the text
vector while extracting vertex features. However, it can pro-
19536
Bare Mesh Text2Mesh [25] TANGO [15] XMesh [23] Our TeMO
(a) Mesh: two dragons; Text Prompt: ‚Äúa fire dragon and an ice dragon‚Äù.
(b) Mesh: vase &candle; Text Prompt: ‚Äúa wood vase and a brick candle‚Äù.
(c) Mesh: person &dragon &whale; Text Prompt: ‚Äúa superman, a fire dragon, and an ice whale‚Äù.
Figure 5. Visual comparisons of our TeMO with previous text-driven 3D stylization methods on several multi-object scenes, including two
objects of the same or different categories, and three different objects.
duce semantic noises due to its utilization of the text vector
containing attributes of multiple objects to process all ver-
tex features. With an increasing number of objects, it will
also encounter challenges related to comprehending text de-
tails and the alignment between the text and 3D objects. As
shown in the 3rdrow, this method still fails to generate styl-
ized assets without mixed properties. In contrast, our TeMO
equipped with 3D scene parsing and multi-grained supervi-
sion, is able to generate photorealistic stylized content for
each object in these 3D scenes to conform to the descrip-
tions in the text prompts.
4.3. Quantitative Evaluation
Objective Metric. We adopt the CLIP score to objectively
evaluate the semantic alignment achieved by our TeMO and
recent 3D stylization methods. Specifically, 8 views spaced45‚ó¶around the stylized meshes are chosen to obtain the ren-
dered 2D images. Then, the visual objects are compared
with the textual objects in CLIP‚Äôs embedding space via the
cosine function. As shown in the 2ndcolumn of Tab. 1,
our TeMO surpasses previous methods by a large margin.
These results demonstrate the superiority of our TeMO over
existing methods on multi-object stylization.
User Study. We further conduct a user study to evaluate
these 3D stylization methods subjectively. We randomly
select 10 mesh-text pairs and recruit 60 users to evaluate
the quality of the stylization assets generated by our TeMO
and previous methods. Particularly, the participants include
experts in the field and individuals without specific back-
ground knowledge. Moreover, each of them will be asked
three questions [25]: (Q1) ‚ÄúHow natural is the output re-
sults?‚Äù(Q2) ‚ÄúHow well does the output match the original
19537
Input Mesh Baseline Baseline + DGA module Baseline + CGC loss Our TeMO
Figure 6. Ablation experiments on the proposed designs of our TeMO. Mesh: two dragons; Text Prompt: ‚Äúa fire dragon and an ice dragon‚Äù.
Table 1. Quantitative comparisons of our TeMO and previous text-
driven 3D stylization methods in multi-object scenes, including an
objective alignment score (0-1) and three subjective opinion scores
(1-5). Note that the higher these metrics, the better the method.
Alignment User-Q1 User-Q2 User-Q3
Text2Mesh [25] 0.262 1.750 1.506 1.472
TANGO [15] 0.274 2.406 2.450 2.539
X-Mesh [23] 0.265 1.839 1.722 1.761
Our TeMO 0.285 3.344 3.311 3.261
content?‚Äù (Q3) ‚ÄúHow well does the output match the target
style?‚Äù, and then assign a score (1-5) to them. We report the
mean opinion scores in parentheses for each factor averaged
across all style outputs. As shown in Tab. 1, our TeMO still
outperforms other methods across all questions. Therefore,
the 3D assets generated by our method are more in line with
people‚Äôs understanding of the text prompts.
4.4. Ablation Studies
To verify the effectiveness of the proposed designs in our
TeMO, we conduct ablation studies by gradually adding
them to our baseline model, i.e.,TANGO [15]. We chose
the two-dragon mesh with the text prompt ‚Äúa fire dragon
and an ice dragon‚Äù, and the experimental results are shown
in Fig. 6. Compared to the baseline model, introducing our
DGA module enables the model to distinguish two drag-
ons, yet it falls short in endowing them with precise texture
details. Meanwhile, incorporating our CGC loss facilitates
the model to capture more semantic details, e.g.,‚Äúfire‚Äù and
‚Äúice‚Äù. Nevertheless, it fails to distinguish the two objects.
It is noteworthy that the model equipped with these two
designs together is not only capable of accurately distin-
guishing between two objects but can also synthesize high-
quality texture details for them. These experiments indicate
that our DGA module and CGC loss can effectively assist
the model in generating desired stylized content for multi-
ple 3D objects to conform to the target text.
5. Limitation and Future Work
Despite achieving excellent results on text-driven multi-
object stylization, our TeMO framework still has a few lim-itations, which can also facilitate future research:
1) 3D Symmetry Prior. As stated in Sec. 4.2, our TeMO
fails to incorporate 3D symmetry prior, whose important
role has been demonstrated by Text2Mesh [25] in promot-
ing style consistency of a single object. To generate more
photorealistic stylization assets for multi-object scenes, it
will be valuable to calculate symmetry planes for each ob-
ject and apply symmetry priors to them.
2) Diffusion Model. We observe that current diffusion
technologies struggle to generate multi-object images ac-
cording to the text prompt, which hinders the application
of diffusion-based stylization methods in multi-object 3D
scenes. We argue it would be interesting to extend the con-
cept of scene parsing to the diffusion models for the release
of their potential in multi-object editing or generation.
6. Conclusion
In this paper, we present TeMO, an innovative framework
proposing scene parsing and multi-grained cross-modal su-
pervision to achieve text-driven multi-object 3D stylization
for the first time. Specifically, we first develop a DGA mod-
ule to precisely align the objects in the 3D mesh and the text
prompt and enhance the 3D point features with the word
features belonging to the same object as them. Then, we
design a CGC loss, in which the fine-grained loss at the lo-
cal level and coarse-grained contrast loss at the global level
are both constructed and complement each other. Further,
extensive experiments are conducted to demonstrate the ef-
fectiveness and superiority of our methods over the existing
methods among a wide range of multi-object 3D scenes. We
believe it is promising to achieve content editing of multi-
ple objects in 3D scenes simultaneously, and we hope the
scene-parsing perspective provided by the proposed TeMO
framework will inspire future works.
Acknowledgments. This research was supported by NSFC
(NO. 62225604, NO. 62276145), the Fundamental Re-
search Funds for the Central Universities (Nankai Univer-
sity, 070-63223049), CAST through Young Elite Scien-
tist Sponsorship Program (No. YESS20210377). Com-
putations were supported by the Supercomputing Center of
Nankai University (NKSC).
19538
References
[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
Neural machine translation by jointly learning to align and
translate. In ICLR, 2014. 2
[2] Guirong Bai, Shizhu He, Kang Liu, and Jun Zhao. Example-
guided stylized response generation in zero-shot setting. Sci-
ence China Information Sciences, 2022. 2
[3] Edward Loper Bird, Steven and Ewan Klein. Natural lan-
guage processing with python. O‚ÄôReilly Media Inc, 2009.
4
[4] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T Bar-
ron, Ce Liu, and Hendrik Lensch. Nerd: Neural reflectance
decomposition from image collections. In IEEE ICCV, 2021.
3
[5] Arthur Caetano and Misha Sra. Arfy: A pipeline for adapting
3d scenes to augmented reality. In ACM UIST, 2022. 1
[6] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,
Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,
Manolis Savva, Shuran Song, Hao Su, et al. Shapenet:
An information-rich 3d model repository. arXiv preprint
arXiv:1512.03012, 2015. 5
[7] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey
Tulyakov, and Matthias Nie√üner. Text2tex: Text-driven tex-
ture synthesis via diffusion models. In IEEE ICCV, 2023.
2
[8] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-
tasia3d: Disentangling geometry and appearance for high-
quality text-to-3d content creation. In IEEE ICCV, 2023. 1,
2
[9] Shaoyu Chen, Budmonde Duinkharjav, Xin Sun, Li-Yi Wei,
Stefano Petrangeli, Jose Echevarria, Claudio Silva, and Qi
Sun. Instant reality: Gaze-contingent perceptual optimiza-
tion for 3d virtual reality streaming. IEEE TVCG, 2022. 1
[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional trans-
formers for language understanding. In NAACL, 2019. 3
[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In ICLR, 2021. 3
[12] Kevin Frans, Lisa Soros, and Olaf Witkowski. Clipdraw:
Exploring text-to-drawing synthesis through language-image
encoders. Advances in Neural Information Processing Sys-
tems, 2022. 5
[13] Mengdi Han, Xiaogang Guo, Xuexian Chen, Cunman Liang,
Hangbo Zhao, Qihui Zhang, Wubin Bai, Fan Zhang, Heming
Wei, Changsheng Wu, et al. Submillimeter-scale multimate-
rial terrestrial robots. Science Robotics, 2022. 1
[14] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-
works. In IEEE CVPR, 2018. 3
[15] Jiabao Lei, Yabin Zhang, Kui Jia, et al. Tango: Text-driven
photorealistic and robust 3d stylization via lighting decom-
position. Advances in Neural Information Processing Sys-
tems, 2022. 1,2,3,4,5,6,7,8
[16] Zheng Li, Xiang Li, Xinyi Fu, Xin Zhang, Weiqiang Wang,
Shuo Chen, and Jian Yang. Promptkd: Unsupervised prompt
distillation for vision-language models. arXiv preprint
arXiv:2403.02781, 2024. 2[17] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
IEEE ICCV, 2021. 3
[18] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. In ICLR, 2019. 6
[19] Haoyu Lu, Yuqi Huo, Mingyu Ding, Nanyi Fei, and Zhiwu
Lu. Cross-modal contrastive learning for generalizable and
efficient image-text retrieval. Machine Intelligence Re-
search, 2023. 1
[20] Yunpeng Luo, Jiayi Ji, Xiaoshuai Sun, Liujuan Cao,
Yongjian Wu, Feiyue Huang, Chia-Wen Lin, and Rongrong
Ji. Dual-level collaborative transformer for image caption-
ing. In AAAI, 2021. 3
[21] Minh-Thang Luong, Hieu Pham, and Christopher D Man-
ning. Effective approaches to attention-based neural machine
translation. In ACL EMNLP, 2015. 3
[22] Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang,
and Rongrong Ji. X-clip: End-to-end multi-grained con-
trastive learning for video-text retrieval. In ACM MM, 2022.
2,5
[23] Yiwei Ma, Xiaioqing Zhang, Xiaoshuai Sun, Jiayi Ji, Haowei
Wang, Guannan Jiang, Weilin Zhuang, and Rongrong Ji. X-
mesh: Towards fast and accurate text-driven 3d stylization
via dynamic textual guidance. In IEEE ICCV, 2023. 1,2,3,
4,5,6,7,8
[24] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and
Daniel Cohen-Or. Latent-nerf for shape-guided generation of
3d shapes and textures. In IEEE CVPR, 2023. 2
[25] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and
Rana Hanocka. Text2mesh: Text-driven neural stylization
for meshes. In IEEE CVPR, 2022. 1,2,5,6,7,8
[26] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,
and Tiberiu Popa. Clip-mesh: Generating textured meshes
from text using pretrained image-text models. In ACM SIG-
GRAPH, 2022. 2
[27] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR,
2023. 2
[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, 2021. 1,2,3,5
[29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125,
1(2):3, 2022. 2
[30] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes,
and Daniel Cohen-Or. Texture: Text-guided texturing of 3d
shapes. In ACM SIGGRAPH, 2023. 2
[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¬®orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In IEEE CVPR, 2022. 2
[32] Scott D Roth. Ray casting for modeling solids. Computer
graphics and image processing, 1982. 3
[33] Oana Sidi, Oliver Van Kaick, Yanir Kleiman, Hao Zhang,
and Daniel Cohen-Or. Unsupervised co-segmentation of a
set of shapes via descriptor-space spectral clustering. In
19539
ACM SIGGRAPH, 2011. 5
[34] Jia-Mu Sun, Tong Wu, and Lin Gao. Recent advances in im-
plicit representation-based 3d shape generation. Visual Intel-
ligence, 2024. 1
[35] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara
Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-
mamoorthi, Jonathan Barron, and Ren Ng. Fourier features
let networks learn high frequency functions in low dimen-
sional domains. Advances in Neural Information Processing
Systems, 2020. 4
[36] TurboSquid. Turbosquid 3d model repository. In
https://www.turbosquid.com/, 2021. 5
[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in Neural
Information Processing Systems, 2017. 3
[38] Can Wang, Menglei Chai, Mingming He, Dongdong Chen,
and Jing Liao. Clip-nerf: Text-and-image driven manipula-
tion of neural radiance fields. In IEEE CVPR, 2022. 1
[39] Wei Wang, Qiulei Dong, and Zhanyi Hu. Asppr: active
single-image piecewise planar 3d reconstruction based on
geometric priors. Science China Information Sciences, 2023.
2
[40] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-
ing He. Non-local neural networks. In IEEE CVPR, 2018.
3
[41] Mingrui Wu, Xuying Zhang, Xiaoshuai Sun, Yiyi Zhou,
Chao Chen, Jiaxin Gu, Xing Sun, and Rongrong Ji. Difnet:
Boosting visual information flow for image captioning. In
IEEE CVPR, 2022. 3
[42] Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi
Li, and Xiangyu Zhang. Lamp: Learn a motion pat-
tern for few-shot-based video generation. arXiv preprint
arXiv:2310.10769, 2023. 2
[43] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-
guang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d
shapenets: A deep representation for volumetric shapes. In
IEEE CVPR, 2015. 5
[44] Qun-Ce Xu, Tai-Jiang Mu, and Yong-Liang Yang. A survey
of deep learning-based 3d shape generation. Computational
Visual Media, 2023. 1
[45] Jianwei Yang, Yonatan Bisk, and Jianfeng Gao. Taco:
Token-aware cascade contrastive learning for video-text
alignment. In IEEE ICCV, 2021. 3
[46] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe
Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and
Chunjing Xu. Filip: Fine-grained interactive language-image
pre-training. In ICLR, 2022. 3
[47] Bowen Yin, Xuying Zhang, Qibin Hou, Bo-Yuan Sun, Deng-
Ping Fan, and Luc Van Gool. Camoformer: Masked sep-
arable attention for camouflaged object detection. arXiv
preprint arXiv:2212.06570, 2022. 3
[48] Bowen Yin, Xuying Zhang, Zhongyu Li, Li Liu, Ming-Ming
Cheng, and Qibin Hou. Dformer: Rethinking rgbd represen-
tation learning for semantic segmentation. In ICLR, 2024.
3
[49] Bo Zhang, Lizbeth Goodman, and Xiaoqing Gu. Novel 3d
contextual interactive games on a gamified virtual environ-
ment support cultural learning through collaboration amongintercultural students. SAGE Open, 2022. 1
[50] Kai Zhang, Nick Kolkin, Sai Bi, Fujun Luan, Zexiang Xu,
Eli Shechtman, and Noah Snavely. Arf: Artistic radiance
fields. In ECCV, 2022. 1
[51] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and
Noah Snavely. Physg: Inverse rendering with spherical gaus-
sians for physics-based material editing and relighting. In
IEEE CVPR, 2021. 3
[52] Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul De-
bevec, William T Freeman, and Jonathan T Barron. Nerfac-
tor: Neural factorization of shape and reflectance under an
unknown illumination. ACM TOG, 2021. 3
[53] Xuying Zhang, Xiaoshuai Sun, Yunpeng Luo, Jiayi Ji, Yiyi
Zhou, Yongjian Wu, Feiyue Huang, and Rongrong Ji. Rstnet:
Captioning with adaptive attention on visual and non-visual
words. In IEEE CVPR, 2021. 3
[54] Xuying Zhang, Bowen Yin, Zheng Lin, Qibin Hou, Deng-
Ping Fan, and Ming-Ming Cheng. Referring camouflaged
object detection. arXiv preprint arXiv:2306.07532, 2023. 3
[55] Qingnan Zhou and Alec Jacobson. Thingi10k: A
dataset of 10,000 3d-printing models. arXiv preprint
arXiv:1605.04797, 2016. 5
[56] Zoran Zivkovic. Improved adaptive gaussian mixture model
for background subtraction. In IEEE ICPR, 2004. 4
19540
