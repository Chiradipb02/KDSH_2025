Open-World Human-Object Interaction Detection via Multi-modal Prompts
Jie Yang1,2*, Bingliang Li1*, Ailing Zeng2†, Lei Zhang2, Ruimao Zhang1†
1The Chinese University of Hong Kong, Shenzhen2International Digital Economy Academy
Abstract
In this paper, we develop MP-HOI , a powerful Multi-
modal Prompt-based HOI detector designed to leverage
both textual descriptions for open-set generalization and
visual exemplars for handling high ambiguity in descrip-
tions, realizing HOI detection in the open world. Specif-
ically, it integrates visual prompts into existing language-
guided-only HOI detectors to handle situations where tex-
tual descriptions face difficulties in generalization and to
address complex scenarios with high interaction ambigu-
ity. To facilitate MP-HOI training, we build a large-scale
HOI dataset named Magic-HOI, which gathers six existing
datasets into a unified label space, forming over 186K im-
ages with 2.4K objects, 1.2K actions, and 20K HOI interac-
tions. Furthermore, to tackle the long-tail issue within the
Magic-HOI dataset, we introduce an automated pipeline for
generating realistically annotated HOI images and present
SynHOI, a high-quality synthetic HOI dataset containing
100K images. Leveraging these two datasets, MP-HOI op-
timizes the HOI task as a similarity learning process be-
tween multi-modal prompts and objects/interactions via a
unified contrastive loss, to learn generalizable and transfer-
able objects/interactions representations from large-scale
data. MP-HOI could serve as a generalist HOI detector,
surpassing the HOI vocabulary of existing expert models by
more than 30 times. Concurrently, our results demonstrate
that MP-HOI exhibits remarkable zero-shot capability in
real-world scenarios and consistently achieves a new state-
of-the-art performance across various benchmarks. Our
project homepage is available at https://MP-HOI.
github.io/ .
1. Introduction
Human-Object Interaction Detection (HOI), a core element
in human-centric vision perception tasks such as human ac-
tivity recognition [15], motion tracking [58], and anomaly
behavior detection [31], has attracted considerable attention
over the past decades. HOI primarily involves localizing
*Equal contribution.
†Corresponding author.correlated human-object 2D positions within an image and
identifying their interactions. Although numerous models
have been proposed [17, 30, 50, 55, 61, 66], deploying these
models in open-world scenarios remains a significant chal-
lenge due to their primary training on closed-set data and
limited generalization capabilities.
With the advancements in visual-linguistic models, such
as CLIP [42], recent research [30, 36, 54] attempts to intro-
duce the natural language into HOI task and transfer knowl-
edge from CLIP to recognize unseen HOI concepts. How-
ever, these methods still suffer from the following limita-
tions: (1) Limited data and category definitions : They
primarily rely on training data from a single dataset, such
as HICO-DET [7], which is built upon a relatively small set
of predefined objects (e.g., 80) and actions (e.g., 117). Con-
sequently, this narrow range of object and action categories
restricts their ability to generalize effectively. (2) An in-
herent bottleneck with textual descriptions : Even when
the model is expanded to be trained on more objects and
action categories, it still encounters a bottleneck when deal-
ing with entirely new categories that lack any relevant text
descriptions in the training data. (3) Difficulty address-
ing high-ambiguity scenarios : Current methods typically
associate a human with a specific object based solely on
a single interaction description. However, real-world situa-
tions often involve the composition of multiple interactions,
as depicted in Fig 1-(a), introducing significant ambiguity in
interaction learning. Moreover, these models may fail to de-
tect a person engaged in a specific combination of multiple
HOIs, which is essential for certain practical applications.
To tackle the above issues, we develop MP-HOI , a novel
and powerful Multi-modal Prompt-based HOI generalist
model for effective HOI detection in the open world. Within
MP-HOI , we incorporate the visual prompts into existing
language-guided-only HOI detectors to enhance the effec-
tiveness and generalization capability. Visual prompts can
act as exemplars, allowing the model to detect the same ob-
jects/interactions (single HOI or the combination of multi-
ple HOIs for the same person), and they also aid in reduc-
ing semantic ambiguity present in textual prompts. Simul-
taneously, textual prompts also provide semantic context to
enhance the understanding of visual prompts. The proposed
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
16954
OpenImages<sit at, dinning table>HCVRD<sit on, truck>SWiG-HOI<draw, pencil>HICO-DET<pet, giraffe>HAKE<hug, fire hydrant><hug, fire hydrant><pet, giraffe>SynHOI
Supplement
(a) Composition of multiple HOIs in the open world
<hold, painting brush>
<paint, wall>
<stand on, ladder>
<hold, painting brush>
<paint, wall>
<hold, painting brush>
<paint, wall>
<squat on, ground>
<hold, painting brush>
<hold, notepad>
<paint, wall>
(b) The Distribution of HOI Triplets on Magic-HOI
Rare (Instance < 10)Non-RareFigure 1. We show (a) the coexisting composited interactions within the same person in an in-the-wild (e.g., A man is squatting on the
ground, holding a painting brush, and painting the wall); (b) the long-tail distribution issue in our Magic-HOI dataset, along with another
proposed SynHOI dataset to address it.
MP-HOI optimize the HOI task as a similarity learning pro-
cess between multi-modal prompts and objects/interactions
by using a unified cross-modality contrastive loss. It
allows the model to learn the textual/visual prompts-to-
objects/interactions alignments from large-scale data.
To facilitate the effective training of our model, we build
a large-scale HOI dataset named Magic-HOI , which gath-
ers six existing datasets into a unified label space, forming
the one containing over 186Kimages with 2.4Kobjects,
1.2Kactions, and 20KHOI interactions. Furthermore, as
shown in Fig. 1-(b), we analyze the distribution of all HOI
interactions in Magic-HOI , finding a significant long-tail
distribution. Upon further investigation, we discover that
79.9%of HOI interactions have fewer than 10instances,
and over 5900 HOI categories have only a single image
available. Such imbalanced label distribution challenges
prompt-based learning, particularly visual prompts. Unfor-
tunately, our further investigation reveals that rare HOI cat-
egories are genuinely rare in real-life scenarios, making it
difficult to collect data from the internet.
Inspired by the impressive conditioned image generation
capabilities demonstrated by recent text-to-image diffusion
models [1, 43–45, 67], we present a diffusion-based auto-
matic pipeline, including the HOIPrompt design, automatic
labeling and filtering, and quality verification, designed to
systematically scale up the generation of diverse and high-
precision HOI-annotated data. Based on it, we present Syn-
HOI, a high-diversity synthetic HOI dataset with over 100K
fully annotated HOI images, which can fill in the shortcom-
ings of the long-tail MP-HOI dataset.
By leveraging the newly collected MP-HOI andSynHOI
datasets, we train the MP-HOI to be a generalist HOI de-
tector, surpassing the HOI vocabulary of existing expert
models (e.g., GEN-VLKT [30]) by more than 30times.
Through extensive experiments, we demonstrate that MP-
HOI exhibits remarkable zero-shot capability in real-worldscenarios. Furthermore, MP-HOI consistently achieves
the new state-of-the-art performance across various bench-
marks. The main contributions of this paper are threefold.
(1) We develop the first multi-modal prompts-based gen-
eralist HOI detector, called MP-HOI , which leverages both
textual descriptions for open-set generalization and visual
exemplars for handling high ambiguity in descriptions,
paving a new path to HOI detection in the open world.
(2) We build a large-scale Magic-HOI dataset for effec-
tive large-scale training, which gathers six existing datasets
into a unified semantic space. To address the long-tail is-
sue in Magic-HOI , we construct an automated pipeline for
generating realistically annotated HOI images and present
SynHOI , a high-quality synthetic HOI dataset.
(3) Extensive experimental results demonstrate that the
proposed MP-HOI exhibits remarkable zero-shot capabil-
ity in real-world scenarios and consistently achieves a new
state-of-the-art performance across various benchmarks.
2. Related Work
HOI Detection. HOI detection task primarily encompasses
three sub-problems, i.e., object detection, human-object
pairing, and interaction recognition. Previous HOI detec-
tors can generally be divided into one-stage and two-stage
paradigms. The two-stage strategy employs an off-the-shelf
detector to determine the locations and classes of objects,
followed by specially-designed modules for human-object
association and interaction recognition. Most methods are
dedicated to exploring additional feature streams to improve
interaction classification, such as the appearance stream
[11, 19, 20, 25], spatial stream [2, 26, 56], pose and body-
parts stream [14, 26, 50], semantic stream [2, 12, 33], and
graph network [37, 40, 51, 56, 62]. Instead, the one-stage
strategy detects HOI triplets in a single forward pass by as-
signing human and object proposals to predefined anchors
and then estimating potential interactions [10, 20, 29, 53].
16955
Datasets Objects Interactions HOIs Unified Images Unified Instances
HICO-DET [7] 80 117 600 37,633 117,871
SWiG-HOI [52] 1,000 407 13,520 50,861 73,499
OpenImages [23] 80 117 600 20,140 78,792
PIC [39] 80 117 600 2,736 9,503
HCVRD [68] 1,821 884 6,311 41,586 256,550
HAKE [24] 80 117 600 33,059 91,120
Magic-HOI 2,427 1,227 20,036 186,015 627,335
Table 1. Statistics of Magic-HOI that unifies six existing datasets.
Displayed numerically within the table are the counts of attributes
we have integrated from each source dataset.
Recently, the DETR-based HOI detectors [8, 21, 35, 47, 48]
have gained prominence in this paradigm, which formulate
the HOI detection task as a set prediction problem, avoid-
ing complex post-processing. In particular, many meth-
ods [4, 9, 22, 30, 36, 59–61, 65, 66] demonstrate promising
performance improvements by disentangling human-object
detection and interaction classification as two decoders in a
cascade manner. Our work builds on the transformer-based
HOI detection strategy and focuses on developing the first
multi-modal prompts-based generalist HOI detector.
Zero-shot HOI Detection. Zero-shot HOI detection has
emerged as a field aiming to identify unseen HOI triplet
categories not present in the training data. Previous re-
search [2, 14, 17–19, 33, 38, 38] has addressed this task in a
compositional manner, by disentangling the reasoning pro-
cess on actions and objects during training. This approach
enables the recognition of unseen HOI triplets during infer-
ence. With the advancements in Vision-Language Models,
such as CLIP [42], recent research [30, 36, 54] has shifted
focus toward transferring knowledge from CLIP to recog-
nize unseen HOI concepts. This shift has resulted in a no-
table performance improvement in zero-shot settings. Our
work aims to develop a HOI detector further to generalize
in open-world scenes effectively.
3. Datasets
This section introduces the proposed two datasets: 1)
Magic-HOI : A unified large-scale HOI dataset that gathers
six existing datasets to form a unified one with consistent la-
bel space; 2) SynHOI : A high-quality synthetic HOI dataset
to tackle the long-tail issue within Magic-HOI .
3.1. Magic-HOI
Existing HOI detection datasets have incorporated a wide
range of object and interaction categories. However, cur-
rent models are typically trained on separate datasets, re-
sulting in limited generalization capabilities. In light of
this, as shown in Tab. 1, we propose Magic-HOI by uni-
fying six HOI detection datasets, including HICO-DET [7],
SWiG-HOI [52], OpenImages [23], PIC [39], HCVRD [68],
HAKE [24]. To ensure the integrity of Magic-HOI , we
meticulously curate the object and verb labels across these
datasets, diligently addressing issues of label overlap and
potential data leakage for zero-shot testing. Magic-HOI
(a) Examples of HOIPrompts
<human, sandwich, make & cook>“black young woman” → “latinoteen”“urban” → “outdoor”“partial view” → “front view”{human}{scene}{shooting}HOI triplet(s)
(b) Examples of generated images and annotations
Figure 2. Illustration of a) HOIPrompts and b) how HOIPrompts
guide the text-to-image generation process to enhance diversity.
For more visualization, please refer to the Appendix.
offers three key advantages: (1) Rich synonymous de-
scriptions: Traditional datasets often use specific descrip-
tions to annotate objects and relationships. In contrast,
Magic-HOI provides a multitude of synonymous descrip-
tions across datasets. For example, it distinguishes be-
tween the object ‘person’ and ‘woman’/‘man’, as well as
the action descriptions ‘read’ and ‘look at’. This diver-
sity enhances the model’s robustness and adaptability. (2)
Large-scale data: Training large models necessitates di-
verse data. Single datasets, such as the 20Kimages in
HICO-DET, can cause the model underfitting. Magic-HOI
make a significant effort to build a large-scale dataset for ef-
fective training, containing an extensive over 186Kimages.
(3) Rich object/actions/interactions categories: Current
models are primarily trained on data with 80objects and
117verbs. However, Magic-HOI significantly broadens the
scope by offering over 2.4Kobjects, 1.2Kactions, and
20KHOI interactions. Such a large vocabulary greatly en-
hances the model’s generalization capabilities.
3.2. SynHOI
As shown in Fig. 1-(b), Magic-HOI exhibits an inherent
long-tail issue. Thus, we present a high-quality synthetic
HOI dataset called SynHOI to complement Magic-HOI . To
make the flow of dataset production scalable, we present
an automatic pipeline, including the HOIPrompt design,
automatic labeling and filtering, and quality verification,
designed to continually scale up the generation of diverse
and high-precision HOI-annotated data, as shown in Fig. 2.
Please refer to the Appendix for details.
SynHOI has three key data characteristics: (1) High-
quality data. SynHOI showcases high-quality HOI annota-
tions. First, we employ CLIPScore [16] to measure the sim-
ilarity between the synthetic images and the corresponding
HOI triplet prompts. The SynHOI dataset achieves a high
CLIPScore of 0.849, indicating a faithful reflection of the
HOI triplet information in the synthetic images. Second,
Fig. 2-(b) provides evidence of the high quality of detection
annotations in SynHOI , attributed to the effectiveness of the
SOTA detector [64] and the alignment of SynHOI with real-
16956
SD’s  Image EncoderCLIP Image EncoderText-to-Image Diffusion UNet
❄
❄Image Encoder
Scene-aware Adaptor 𝛼Human-Object Instance Decoder
Human-Object Interaction Decoder CLIP Multi-modal  Encoders
❄Human-Object Query Emerging
Scene-aware Adaptor 𝛽Input ImageRepresentative Feature EncodersTextual Prompts
HOI-levelPrompt embeddingsGated Fusion…………
FusionObject-levelPrompt embeddings
Visual Prompts……
➕A man hugs a dogA photo of a dogA photo of a person hugging a dogWord Parse
➕Human/Object BoxesFFNObject Class
HOI Class𝐅𝐱𝐅!"𝐅#𝐐{%,'}𝑸{%,'}#𝐐)𝑸)#𝐅*+,-./0𝐓1231/./0'𝐓1231/./0)Sequential DecodersMulti-Modal Prompt-based Predictor
ROI Crop
Prompt
-
to
-
Interaction
SimilarityPrompt-to-ObjectSimilarityFigure 3. Overview of MP-HOI , comprising three components: Representative Feature Encoder, Sequential Instance and Interaction
Decoders, and Multi-modal Prompt-based Predictor. Ultimately, it can leverage textual or visual prompts to detect open-world HOIs.
world data distributions. The visualization of SynHOI is
presented in the Appendix. (2) High-diversity data. Syn-
HOI exhibits high diversity, offering a wide range of visu-
ally distinct images. Fig. 2-(b) demonstrates the impact of
random variations in person’s descriptions, environments,
and photographic information within the HOIPrompts on
the diversity of synthetic images. (3) Large-scale data
with rich categories. SynHOI aligns Magic-HOI ’s cate-
gory definitions to effectively address the long-tail issue in
Magic-HOI . It consists of over 100Kimages, 130Kperson
bounding boxes, 140Kobject bounding boxes, and 240K
HOI triplet instances.
4. Methodology
4.1. The Overview of MP-HOI
MP-HOI is a multi-modal prompt-based HOI detector com-
prising three components: Representative Feature Encoder,
Sequential Instance and Interaction Decoders, and Multi-
modal Prompt-based Predictor, as shown in Fig. 3.
Given an input image x, the Representative Feature En-
coder firstly encodes it to powerful feature representations
F′, which have fused the vanilla image features Ffrom the
image encoder and the HOI-associated representations Fsd
obtained from text-to-image diffusion model [44] (refer to
Sec. 4.2 for details). Based on F′, the Sequential Instance
and Interaction Decoders are designed to decode the pair-
wise human-object representations and their corresponding
interaction representations in a sequential manner. Specifi-
cally, the instance decoder takes the input of initialized pair-
wise human-object queries Q{h,o}∈R2×N×Cand pro-
duces the updated queries Q′
{h,o}∈R2×N×C, where N
is the number of paired queries and Cindicates the chan-
nel dimension. Subsequently, we merge the human-objectqueries Q′
{h,o}, into interaction queries Qi∈RN×Cvia av-
erage pooling. The interaction decoder then receives these
interaction queries and updates them to Q′
i∈RN×C. To
enhance the interaction representations, we integrate the
global CLIP image representation into each of the interac-
tion queries Q′
i∈RN×Cvia element-wise addition.
The complete HOI prediction for an image includes hu-
man and object bounding boxes, object class, and inter-
actions class. Firstly, we employ Feed-Foreward Network
(FFN) on Q′
h,oto predict the human and object bounding
box. Secondly, we introduce the Multi-modal Prompt-based
Predictor to identify the object and interaction representa-
tions into object and interaction classes (refer to Sec. 4.3).
4.2. Representative Feature Encoder
Motivation. As illustrated in Sec. 3, even though we have
unified six datasets into Magic-HOI and introduced the syn-
thetic dataset SynHOI , compared with the amount of train-
ing data for large-scale visual-linguistic models, such as
CLIP [42] or Stable Diffusion (SD) [44], we still have
1,000times less data. We firmly believe that such mod-
els can further provide effective encoded image represen-
tations for training instance/interaction decoders. Specifi-
cally, considering that the SD model utilizes cross-attention
mechanisms between text embeddings and visual represen-
tations for text-conditioned image generation, we assume
the SD model has a substantial correlation between their
feature spaces and semantic concepts in language. To fur-
ther demonstrate this, we employ DAAM [49] to generate
pixel-level linguistic concept heatmaps based on the im-
age features extracted from the SD model. As depicted
in Fig. 4, beyond the noun concepts highlighted in prior
studies [28, 46, 57], we find that the internal representation
space of a frozen SD model is highly relevant to verb con-
16957
Person
Skateboard
RideReal image from Magic-HOI
Person
Wine Glass
HoldSythetic image from SynHOI
Figure 4. Attribution analysis of Stable Diffusion between ob-
ject/interaction texts and real/synthetic images. The visualization
solely utilizes time-step 0.
cepts and their associated contexts, which are very useful
for the HOI task.
Extract Local Semantic Association in Stable Diffusion.
Inspired by the above observations, the feature maps de-
rived from the SD model are expected to exhibit superior
semantic correspondence within local regions, establishing
an association between the semantic information of the text
and the corresponding regions within the image. Accord-
ingly, we utilize the UNet-based diffusion model to extract
the feature maps, which are not only associated with the
noun concepts but also include the verb concepts and the
corresponding contextual details. Different from using a se-
ries of denoising steps to generate an image, we directly
feed the input image xinto the UNet and perform a single
forward pass via the network. The output multi-scale im-
age features are as Fsd= SD( x,Ax), where Axdenotes
the text-related representation that corresponds to x. Typi-
cally,Axcan be obtained by utilizing a text encoder, such
as the CLIP text encoder, to encode the description of the
image. However, as a discriminative task, the description of
the HOI image is not available in the inference. To address
this, we replace it with the global feature representation of
xextracted from the CLIP image encoder, which bears sig-
nificant relevance to the textual representation.
Two Scene-aware Adaptors. Given that the CLIP image
features Fimg
cliprepresents a global feature, it inherently en-
compasses the contextual information of the overall scene.
As shown in Fig. 3, we introduce two scene-aware adap-
tors, denoted as αandβ, to project Fimg
clipinto feature
spaces more consistent with the SD model and Multi-modal
Prompt-based Predictor. Regarding the scene-aware adap-
torα, since the CLIP model is trained to align global visual
and textual representations within a latent space, the Fimg
clip
can be employed as a substitute for the textual representa-
tion. Hence, we can train an MLP to directly project Fimg
clipto
a textual space suitable for the SD model. As for the scene-
aware adaptor β, we project Fimg
clipthrough it and incorporate
the adapted version into each interaction query of Q′
i. Thisadjustment allows for the tuning of these interaction queries
to align more effectively with multi-modal prompts.
Fused Features for Better HOI Representing. Follow-
ing the previous detectors [30, 61], we also employ a com-
mon image encoder to obtain the vanilla image features F.
Then, we perform Gated Fusion to generate more informa-
tiveF′. Specifically, F′combines Fwith supplementary
features Fsdfor the interaction decoder, while retaining the
original features Ffor the instance decoder.
4.3. Multi-Modal Prompt-based Predictor
Motivation. As discussed in the introduction, existing
language-guided-only HOI detectors face challenges in
generalization when dealing with entirely new categories
that lack any relevant text descriptions in the training data.
They also struggle to address complex scenarios with high
interaction ambiguity. Therefore, we are motivated to in-
corporate visual prompts to enhance the effectiveness and
generalization capability in open-world scenarios. Visual
prompts can act as exemplars, allowing the model to detect
the same objects/interactions, and they also aid in reducing
semantic ambiguity present in textual prompts. Simultane-
ously, textual prompts provide semantic context to enhance
the understanding of visual prompts. Given that CLIP [42]
is trained to align visual and textual representations through
hundreds of millions of image-text pairs. we naturally lever-
age its pre-trained image and text encoders to encode both
textual and visual prompts.
Encoding Multi-Modal Prompts. For textual prompt in-
puts, we can parse them into instance descriptions like ”A
photo of a [object],” and interaction descriptions like ”A
photo of a [man/child/...] [verb-ing] a [object].” These sen-
tences can then be encoded to obtain object category em-
beddings To
textor interaction category embeddings Ti
text.
For visual prompt inputs, we can crop them into corre-
sponding object ROI images and HOI ROI images using
user-provided bounding boxes. These images can also be
used to obtain object category embeddings To
imgor inter-
action category embeddings Ti
img.
Multi-Modal Prompt-based Classifiers. Formally, the fi-
nal object category distributions Poand HOI category dis-
tributions Pican be calculated as,
Po= softmax( Q′
o∗To
text/img⊺) (1)
Pi= softmax( Q′
i∗Ti
text/img⊺) (2)
where Q′
o∈RN×CandQ′
i∈RN×Cdenotes object
queries and interaction queries, respectively, and softmax
indicates the row-wise softmax operation.
16958
4.3.1 Reformulate the Optimization of HOI
Cross-modal Contrastive Learning. MP-HOI reformulate
the HOI task as a similarity learning process between multi-
modal prompts and objects/interactions via a unified con-
trastive loss. Thus, it can perform multi-modal prompt-to-
object/interaction alignments for large-scale image-prompt
pairs in Magic-HOI and SynHOI . In general, given B
image-prompt pairs {(xm,promptm)}B
m=1in our datasets,
the unified contrastive loss Lccan be formulated as,
Lc=−1
BBX
m=1log(exp(S(xm,promptm)/τ)PB
n=1exp(S(xm,promptn)/τ))(3)
where S(xm,promptn)is prompt-to-image similarity be-
tween m-th image xmandn-th prompt promptn.τis a
temperature to scale the logits. Through Lc, we could ob-
tain the object contrastive loss Lo
cand the interaction con-
trastive loss Li
c, which enable the model to learn general-
izable and transferable objects/interactions representations
from the large-scale categories.
Overall Loss . Following the query-based methods [30, 61],
we employ the Hungarian algorithm to match predictions to
each ground-truth. The overall loss is computed between
the matched predictions and their corresponding ground-
truths, which includes the box regression loss Lb, the
intersection-over-union loss Lg, the object contrastive loss
Lo
c, and the interaction contrastive loss Li
c,
L=λbLb+λgLg+λo
cLo
c+λi
cLi
c, (4)
where LbandLgcontain both human and object localiza-
tion. λb,λg,λo
candλi
care used to adjust the weights of
each loss component.
4.4. Training and Inference Details
Training Details. With the unified cross-modality con-
trastive loss that we have defined, we are able to jointly op-
timize multi-modal prompts. Considering the training cost,
we employ a 50% probability to randomly select either a
visual prompt or a textual prompt for each iteration. To
use visual prompts for training, we sample two images con-
taining a person paired with the exact same single/multiple
objects and single/multiple interaction categories from the
dataset, and then choose one of them as the visual prompt.
Inference Details. Users can freely choose from available
prompts. We present quantitative results based on textual
prompts to ensure a fair comparison.
5. Experiments
5.1. Experimental Settings
Datasets and Evaluation Metrics. We evaluate our mod-
els on four benchmarks: HICO-DET [6], V-COCO [13],Method BackboneDefault
Full Rare Non-Rare
Expert Models
QPIC [48] ResNet-50 29.07 21.85 31.23
CDN [61] ResNet-50 31.44 27.39 32.64
DOQ [41] ResNet-50 33.28 29.19 34.50
IF [32] ResNet-50 33.51 30.30 34.46
GEN-VLKT [30] ResNet-50 33.75 29.25 35.10
PViC [63] ResNet-50 33.80 29.28 35.15
QAHOI [8] Swin-L 35.78 29.80 37.56
FGAHOI [35] Swin-L 37.18 30.71 39.11
PViC [63] Swin-L 43.35 42.25 43.69
Pre-trained Models
RLIPv1 [59] ResNet-50 13.92 11.20 14.73
RLIPv1 [59]†ResNet-50 30.70 24.67 32.50
RLIPv2 [60] ResNet-50 17.79 19.64 17.24
RLIPv2 [60]†ResNet-50 35.38 29.61 37.10
Generalist Models
MP-HOI -S ResNet-50 36.50 ↑2.70 35.48↑6.20 36.80↑1.65
MP-HOI -L Swin-L 44.53↑1.18 44.48↑2.23 44.55↑0.86
1RLIP V2: VG+COCO+O365, 2200K+pretraining data.
2MP-HOI :Magic-HOI +SynHOI ,286K+training data.
Table 2. Performance comparison on HICO-DET in terms of mAP
(%). †indicates the models are fully fine-tuned on HICO-DET.
The results highlighted in underlined represent state-of-the-art per-
formance among expert models, which we primarily compare to.
Method BackboneDefault
Full Rare Non-Rare
Expert Models
GEN-VLKT [30] ResNet-50 10.87 10.41 20.91
Generalist Models
MP-HOI -S ResNet-50 12.61 14.78 20.28
MP-HOI -L Swin-L 16.21 18.59 25.76
Table 3. Performance comparison on SWiG in terms of mAP (%).
Given that GEN-VLKT is one of the most powerful and represen-
tative expert models, we train it on SWiG for comparison.
SwiG [52] and HCVRD [68]. The mean Average Precision
(mAP) is used as the evaluation metric, following standard
protocols [30, 61].
Implementation Details. We implement two variant ar-
chitectures of MP-HOI :MP-HOI -S, and MP-HOI -L, where
‘S’ and ‘L’ refer to small and large, respectively. For MP-
HOI-S, we use ResNet-50 as the backbone and a six-layer
vanilla Transformer encoder [5] as the feature extractor.
Both the human-object decoder and interaction decoder are
three-layer vanilla Transformer decoders. For MP-HOI -L,
we employ Swin-L [34] as the backbone, with Transformer
decoders of six layers. The CLIP-based model and diffusion
model are frozen during training for all the settings. For a
fair comparison with previous studies, we employ the ViT-
B variant of the CLIP model within the MP-HOI -S. Con-
versely, to enhance performance in real-world scenarios, the
ViT-L variant is utilized within the MP-HOI -L.
5.2. General HOI Detection
MP-HOI is a generalist HOI detector trained on the uni-
fied dataset- Magic-HOI . Since Magic-HOI unifies multi-
16959
Method BackboneDefault
Full Rare Non-Rare
Expert Models
GEN-VLKT [30] ResNet-50 6.58 5.81 14.02
Generalist Models
MP-HOI -S ResNet-50 8.08 6.86 14.31
MP-HOI -L Swin-L 11.29 9.01 18.68
Table 4. Performance comparison on HCVRD in terms of mAP
(%). We train GEN-VLKT on HCVRD for comparison.
Method Backbone AP (Scenario 1) AP (Scenario 2)
Expert Models
IDN [27] ResNet-50 53.3 60.3
FCL [19] ResNet-50 52.4 -
SCG [62] ResNet-50 54.2 60.9
HOTR [21] ResNet-50 55.2 64.4
QPIC [48] ResNet-50 58.8 61.0
CDN [61] ResNet-50 61.7 63.8
GEN-VLKT [30] ResNet-50 62.4 64.5
Generalist Models
Zero-shot Testing
MP-HOI -S ResNet-50 37.5 44.2
10% Training Data
MP-HOI -S ResNet-50 57.7 60.2
100% Training Data
MP-HOI -S ResNet-50 66.2 67.6
Table 5. Performance comparison on V-COCO.
ple datasets, we could directly evaluate MP-HOI on these
benchmarks. We compare its performance with two types
of models: expert models and pretrained models. As there
are no standard results available like mAP on SWiG-HOI
and HCVRD datasets for comparison, we train the GEN-
VLKT [30], which is one of the representative expert mod-
els, as the baseline for our comparison.
HICO-DET. Compared with the expert models, MP-HOI
outperforms them by a significant margin under both
ResNet-50 and Swin-L backbones, notably achieving a re-
markable 6.20improvement in mAP in the rare setting.
Compared with the pre-trained model, MP-HOI also sur-
passes RLIPv2 [60], which is pretrained on over 2200K
data and fine-tuned on HICO-DET. Finally, MP-HOI -L
achieves a new SOTA performance with 44.53mAP.
SWiG-HOI & HCVRD. Compared with GEN-VLKT [30],
MP-HOI exhibits superior performance on these two
datasets that contain more HOI concepts. This highlights
MP-HOI ’s capability to serve as a generalist HOI detector.
5.3. Zero-Shot HOI Detection
We demonstrate the generalization of the model trained on
our proposed Magic-HOI andSynHOI datasets by evaluat-
ing it on the unseen dataset-V-COCO. As shown in Tab. 5,
MP-HOI exhibits excellent zero-shot performance on V-
COCO. After fine-tuning the model on just 10% of the train-
ing data, it achieves an impressive AP of 57.7, surpass-
ing some expert models that are fully trained on V-COCO.
Moreover, when utilizing the full training data, MP-HOIoutperforms all expert models by a significant margin.
5.4. Ablation Study
Two Key Representations. As described in Sec. 4.1, we in-
troduce Fsdfrom stable diffusion and Fimg
clipfrom CLIP im-
age encoder, into MP-HOI . Tab. 6 demonstrates their effec-
tiveness. Initially, we establish a baseline model following
the design of GEN-VLKT [30] while excluding the knowl-
edge distillation component. Subsequently, the incorpora-
tion of Fsdleads to significant improvements, particularly
in terms of rare mAP. This suggests that the internal fea-
tures in stable diffusion play a crucial role in enhancing the
representation of rare categories in HICO-DET. Addition-
ally, the integration of Fimg
clipfurther improves the non-rare
AP, as it aligns the queries with multi-modal prompts.
Diffusion Time Steps. We investigate the effectiveness
of different diffusion steps in extracting Fsd, similarly
to [3, 57]. Diffusion models control the noise distortion
added to the input image by varying the value of t. Sta-
ble diffusion [44] uses a total of 1000 time steps. We set t
values to 0,100,500for ablation studies. As demonstrated
in Tab. 7, the best performance is achieved when t= 0. It
is worth noting that using Fsdextracted from input images
with higher noise levels would decrease performance and
potentially impact the learning of interactions.
Two Scene-Aware Adaptors. Tab. 8 illustrates that both
scene-aware adaptors effectively facilitate Stable Diffusion
and the CLIP image encoder in extracting feature represen-
tations aligned with our MP-HOI framework and HOI task,
thereby improving performance.
Contrastive Learning. As in Sec. 4.3.1, we introduce the
object contrastive loss Lo
cand the interaction contrastive
lossLi
c. We leverage these two losses to facilitate alignment
between multi-modal prompts and objects/interactions. The
results in Tab. 9 demonstrate the effectiveness of both
loss items, particularly in improving performance on rare
classes, where the model exhibits improved discrimination
between objects and interactions.
Multi-modal Prompts. As shown in Tab. 10, integrating
visual prompts during training could enhance performance
compared to using only textual prompts. This improve-
ment is due to the inherent semantic ambiguity in textual
prompts, which visual prompts help to reduce.
The Scale of Training Data. Beyond solely training on
HICO-DET, we incorporate our Magic-HOI dataset that in-
cludes five additional datasets for training. As in Tab. 11,
this leads to improved performance and expands the range
of object and interaction categories covered. Furthermore,
the inclusion of our SynHOI dataset could also effectively
enhance performance, particularly on rare classes.
16960
FsdFimg
clipFull Rare Non-rare
31.99 29.63 32.70
✓ 32.92 31.29 33.41
✓ ✓ 34.41 31.07 35.40
Table 6. Ablation results of two key rep-
resentations on HICO-DET.Time step Full Rare Non-Rare
0 34.41 31.07 35.40
100 34.03 30.58 35.02
500 33.59 29.80 34.71
Table 7. Ablation results of different dif-
fusion time steps on HICO-DET.Adaptor αAdaptor β Full Rare Non-Rare
✓ 34.08 30.64 35.08
✓ 33.76 29.89 34.82
✓ ✓ 34.41 31.07 35.40
Table 8. Ablation results of two scene adaptors
on HICO-DET.
Lo
cLi
cFull Rare Non-Rare
34.41 31.07 35.40
✓ 34.56 31.29 35.43
✓ ✓ 34.82 31.87 35.49
Table 9. The ablation study of contrastive
losses at two levels on HICO-DET.T. V . Full Rare Non-Rare
✓ 34.82 31.87 35.49
✓ ✓ 35.18 32.16 35.57
Table 10. The ablation study of multi-modal
prompts on HICO-DET. T.andV .denotes tex-
tual prompt and visual prompt, respectively.Training Data Full Rare Non-Rare
HICO-DET 35.18 32.16 35.57
Magic-HOI 35.93 34.68 36.26
+SynHOI 36.50 35.48 36.80
Table 11. The impact of the scale of train-
ing data.
(a)
 (b)
 (c)
 (d)
 (e)
Figure 5. In-the-wild test based on arbitrary textual prompts. Each HOI triplet is represented in the same color.
(a)
 (b)
 (c)
 (d)
 (e)
Figure 6. In-the-wild test based on arbitrary visual prompts. The first row is the visual prompt (green boxes define objects and red boxes
define interactions), and the second row is the corresponding detection results. We highlight the powerful HOI detection performance, the
capability to detect single HOIs (a, b, c), as well as a specific HOI composition (d), and the flexibility to integrate the visual prompt for
interaction definition and textual prompt for object definitions (e.g., woman and phone) for the test (e).
5.5. HOI Detection in the Open World
In addition to the zero-shot experiments, we also conduct
open-world testing on arbitrary images from the Internet as
shown in Fig. 5 and Fig. 6. Our MP-HOI could leverage
arbitrary textual and visual prompts to detect HOIs. Inter-
estingly, we find MP-HOI could support extensive textual
prompts, such as Princess Diana and Prince Charles for hu-
man description in Fig. 5-(a) and D ¨oner Kebap for object
description in Fig. 5-(d). Also, as in Fig. 6, MP-HOI could
detect a specific HOI composition via visual prompts, and
has the flexibility to integrate the visual prompt for inter-
action definition and textual prompt for object definitions
(e.g., woman and phone) for the test as in Fig. 6-(e).
6. Conclusion
We introduce MP-HOI , a powerful multi-modal prompt-
based HOI detector. It leverages both textual descrip-
tions and visual exemplars to realize HOI detection in
the open world. We introduce a large-scale unified HOIdataset named Magic-HOI , and a high-quality synthetic
HOI dataset called SynHOI for effective training. Our re-
sults demonstrate that the trained MP-HOI , as a generalist
HOI detector, exhibits remarkable zero-shot capability in
real-world scenarios and consistently achieves a new state-
of-the-art performance across various benchmarks. We
hope our models and datasets can serve the community for
further research.
Acknowledgment
The work is partially supported by the Young Scientists
Fund of the National Natural Science Foundation of China
under grant No.62106154, by the Natural Science Foun-
dation of Guangdong Province, China (General Program)
under grant No.2022A1515011524, and by Shenzhen Sci-
ence and Technology Program JCYJ20220818103001002
and ZDSYS20211021111415025, and by the Guang-
dong Provincial Key Laboratory of Big Data Comput-
ing, The Chinese University of Hong Kong (Shenzhen).
16961
References
[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,
Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,
Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image
diffusion models with an ensemble of expert denoisers. arXiv
preprint arXiv:2211.01324 , 2022. 2
[2] Ankan Bansal, Sai Saketh Rambhatla, Abhinav Shrivastava,
and Rama Chellappa. Detecting human-object interactions
via functional generalization. In Proceedings of the AAAI
Conference on Artificial Intelligence , pages 10460–10469,
2020. 2, 3
[3] Dmitry Baranchuk, Ivan Rubachev, Andrey V oynov,
Valentin Khrulkov, and Artem Babenko. Label-efficient se-
mantic segmentation with diffusion models. arXiv preprint
arXiv:2112.03126 , 2021. 7
[4] Yichao Cao, Qingfei Tang, Feng Yang, Xiu Su, Shan You,
Xiaobo Lu, and Chang Xu. Re-mine, learn and reason: Ex-
ploring the cross-modal semantic correlations for language-
guided hoi detection. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 23492–
23503, 2023. 3
[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In Proceedings of the
European Conference on Computer Vision , pages 213–229.
Springer, 2020. 6
[6] Yu-Wei Chao, Yunfan Liu, Xieyang Liu, Huayi Zeng, and Jia
Deng. Learning to detect human-object interactions. In 2018
ieee winter conference on applications of computer vision
(wacv) , pages 381–389. IEEE, 2018. 6
[7] Yu-Wei Chao, Yunfan Liu, Xieyang Liu, Huayi Zeng, and
Jia Deng. Learning to detect human-object interactions. In
Proceedings of the IEEE Winter Conference on Applications
of Computer Vision , 2018. 1, 3
[8] Junwen Chen and Keiji Yanai. Qahoi: query-based an-
chors for human-object interaction detection. arXiv preprint
arXiv:2112.08647 , 2021. 3, 6
[9] Mingfei Chen, Yue Liao, Si Liu, Zhiyuan Chen, Fei Wang,
and Chen Qian. Reformulating hoi detection as adaptive
set prediction. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 9004–
9013, 2021. 3
[10] Hao-Shu Fang, Yichen Xie, Dian Shao, and Cewu Lu. Dirv:
Dense interaction region voting for end-to-end human-object
interaction detection. In Proceedings of the AAAI Conference
on Artificial Intelligence , pages 1291–1299, 2021. 2
[11] Chen Gao, Yuliang Zou, and Jia-Bin Huang. ican: Instance-
centric attention network for human-object interaction detec-
tion. arXiv preprint arXiv:1808.10437 , 2018. 2
[12] Chen Gao, Jiarui Xu, Yuliang Zou, and Jia-Bin Huang. Drg:
Dual relation graph for human-object interaction detection.
InComputer Vision–ECCV 2020: 16th European Confer-
ence, Glasgow, UK, August 23–28, 2020, Proceedings, Part
XII 16 , pages 696–712. Springer, 2020. 2
[13] Saurabh Gupta and Jitendra Malik. Visual semantic role la-
beling. arXiv preprint arXiv:1505.04474 , 2015. 6[14] Tanmay Gupta, Alexander Schwing, and Derek Hoiem. No-
frills human-object interaction detection: Factorization, lay-
out encodings, and training techniques. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 9677–9685, 2019. 2, 3
[15] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,
and Juan Carlos Niebles. Activitynet: A large-scale video
benchmark for human activity understanding. In 2015
IEEE conference on computer vision and pattern recognition
(CVPR) , pages 961–970. IEEE, 2015. 1
[16] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
and Yejin Choi. Clipscore: A reference-free evaluation met-
ric for image captioning. arXiv preprint arXiv:2104.08718 ,
2021. 3
[17] Zhi Hou, Xiaojiang Peng, Yu Qiao, and Dacheng Tao. Vi-
sual compositional learning for human-object interaction de-
tection. In Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceed-
ings, Part XV 16 , pages 584–600. Springer, 2020. 1, 3
[18] Zhi Hou, Baosheng Yu, Yu Qiao, Xiaojiang Peng, and
Dacheng Tao. Affordance transfer learning for human-object
interaction detection. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
495–504, 2021.
[19] Zhi Hou, Baosheng Yu, Yu Qiao, Xiaojiang Peng, and
Dacheng Tao. Detecting human-object interaction via fab-
ricated compositional learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 14646–14655, 2021. 2, 3, 7
[20] Bumsoo Kim, Taeho Choi, Jaewoo Kang, and Hyunwoo J
Kim. Uniondet: Union-level detector towards real-time
human-object interaction detection. In Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, Au-
gust 23–28, 2020, Proceedings, Part XV 16 , pages 498–514.
Springer, 2020. 2
[21] Bumsoo Kim, Junhyun Lee, Jaewoo Kang, Eun-Sol Kim,
and Hyunwoo J Kim. Hotr: End-to-end human-object in-
teraction detection with transformers. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 74–83, 2021. 3, 7
[22] Sanghyun Kim, Deunsol Jung, and Minsu Cho. Relational
context learning for human-object interaction detection. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 2925–2934, 2023. 3
[23] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-
jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan
Popov, Matteo Malloci, Alexander Kolesnikov, et al. The
open images dataset v4: Unified image classification, object
detection, and visual relationship detection at scale. Interna-
tional Journal of Computer Vision , 128(7):1956–1981, 2020.
3
[24] Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu,
Mingyang Chen, Ze Ma, Shiyi Wang, Hao-Shu Fang, and
Cewu Lu. Hake: Human activity knowledge engine. arXiv
preprint arXiv:1904.06539 , 2019. 3
[25] Yong-Lu Li, Siyuan Zhou, Xijie Huang, Liang Xu, Ze Ma,
Hao-Shu Fang, Yanfeng Wang, and Cewu Lu. Transfer-
able interactiveness knowledge for human-object interaction
16962
detection. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 3585–
3594, 2019. 2
[26] Yong-Lu Li, Xinpeng Liu, Han Lu, Shiyi Wang, Junqi
Liu, Jiefeng Li, and Cewu Lu. Detailed 2d-3d joint repre-
sentation for human-object interaction. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10166–10175, 2020. 2
[27] Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Yizhuo Li, and
Cewu Lu. Hoi analysis: Integrating and decomposing
human-object interaction. Advances in Neural Information
Processing Systems , 33:5011–5022, 2020. 7
[28] Ziyi Li, Qinye Zhou, Xiaoyun Zhang, Ya Zhang, Yan-
feng Wang, and Weidi Xie. Guiding text-to-image diffu-
sion model towards grounded generation. arXiv preprint
arXiv:2301.05221 , 2023. 4
[29] Yue Liao, Si Liu, Fei Wang, Yanjie Chen, Chen Qian, and Ji-
ashi Feng. Ppdm: Parallel point detection and matching for
real-time human-object interaction detection. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 482–490, 2020. 2
[30] Yue Liao, Aixi Zhang, Miao Lu, Yongliang Wang, Xiaobo
Li, and Si Liu. Gen-vlkt: Simplify association and enhance
interaction understanding for hoi detection. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 20123–20132, 2022. 1, 2, 3, 5, 6,
7
[31] Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. Fu-
ture frame prediction for anomaly detection–a new baseline.
InProceedings of the IEEE conference on computer vision
and pattern recognition , pages 6536–6545, 2018. 1
[32] Xinpeng Liu, Yong-Lu Li, Xiaoqian Wu, Yu-Wing Tai, Cewu
Lu, and Chi-Keung Tang. Interactiveness field in human-
object interactions. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
20113–20122, 2022. 6
[33] Ye Liu, Junsong Yuan, and Chang Wen Chen. Consnet:
Learning consistency graph for zero-shot human-object in-
teraction detection. In Proceedings of the 28th ACM Interna-
tional Conference on Multimedia , pages 4235–4243, 2020.
2, 3
[34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 10012–10022, 2021. 6
[35] Shuailei Ma, Yuefeng Wang, Shanze Wang, and Ying Wei.
Fgahoi: Fine-grained anchors for human-object interaction
detection. arXiv preprint arXiv:2301.04019 , 2023. 3, 6
[36] Shan Ning, Longtian Qiu, Yongfei Liu, and Xuming He.
Hoiclip: Efficient knowledge transfer for hoi detection with
vision-language models. arXiv preprint arXiv:2303.15786 ,
2023. 1, 3
[37] Jeeseung Park, Jin-Woo Park, and Jong-Seok Lee. Viplo:
Vision transformer based pose-conditioned self-loop graph
for human-object interaction detection. arXiv preprint
arXiv:2304.08114 , 2023. 2[38] Julia Peyre, Ivan Laptev, Cordelia Schmid, and Josef Sivic.
Detecting unseen visual relations using analogies. In Pro-
ceedings of the IEEE/CVF International Conference on
Computer Vision , pages 1981–1990, 2019. 3
[39] PicDataset. Picdataset.com: Artificial intelligence news to-
day, 2023. 3
[40] Siyuan Qi, Wenguan Wang, Baoxiong Jia, Jianbing Shen,
and Song-Chun Zhu. Learning human-object interactions by
graph parsing neural networks. In Proceedings of the Eu-
ropean conference on computer vision (ECCV) , pages 401–
417, 2018. 2
[41] Xian Qu, Changxing Ding, Xingao Li, Xubin Zhong,
and Dacheng Tao. Distillation using oracle queries for
transformer-based human-object interaction detection. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 19558–19567, 2022. 6
[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 1, 3, 4, 5
[43] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022. 2
[44] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10684–10695, 2022. 4, 7
[45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems , 35:36479–36494, 2022. 2
[46] Jordan Shipard, Arnold Wiliem, Kien Nguyen Thanh, Wei
Xiang, and Clinton Fookes. Diversity is definitely needed:
Improving model-agnostic zero-shot classification via stable
diffusion. 4
[47] Masato Tamura, Hiroki Ohashi, and Tomoaki Yoshinaga.
Qpic: Query-based pairwise human-object interaction detec-
tion with image-wide contextual information. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 10410–10419, 2021. 3
[48] Masato Tamura, Hiroki Ohashi, and Tomoaki Yoshinaga.
Qpic: Query-based pairwise human-object interaction detec-
tion with image-wide contextual information. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 10410–10419, 2021. 3, 6, 7
[49] Raphael Tang, Akshat Pandey, Zhiying Jiang, Gefei Yang,
Karun Kumar, Jimmy Lin, and Ferhan Ture. What the daam:
Interpreting stable diffusion using cross attention. arXiv
preprint arXiv:2210.04885 , 2022. 4
[50] Bo Wan, Desen Zhou, Yongfei Liu, Rongjie Li, and Xuming
He. Pose-aware multi-level feature network for human ob-
ject interaction detection. In Proceedings of the IEEE/CVF
16963
International Conference on Computer Vision , pages 9469–
9478, 2019. 1, 2
[51] Hai Wang, Wei-shi Zheng, and Ling Yingbiao. Contextual
heterogeneous graph network for human-object interaction
detection. In Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceed-
ings, Part XVII 16 , pages 248–264. Springer, 2020. 2
[52] Suchen Wang, Kim-Hui Yap, Henghui Ding, Jiyan Wu, Jun-
song Yuan, and Yap-Peng Tan. Discovering human interac-
tions with large-vocabulary objects via query and multi-scale
detection. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 13475–13484, 2021.
3, 6
[53] Tiancai Wang, Tong Yang, Martin Danelljan, Fahad Shahbaz
Khan, Xiangyu Zhang, and Jian Sun. Learning human-object
interaction detection using interaction points. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 4116–4125, 2020. 2
[54] Mingrui Wu, Jiaxin Gu, Yunhang Shen, Mingbao Lin, Chao
Chen, Xiaoshuai Sun, and Rongrong Ji. End-to-end zero-
shot hoi detection via vision and language knowledge distil-
lation. arXiv preprint arXiv:2204.03541 , 2022. 1, 3
[55] Xiaoqian Wu, Yong-Lu Li, Xinpeng Liu, Junyi Zhang, Yuzhe
Wu, and Cewu Lu. Mining cross-person cues for body-part
interactiveness learning in hoi detection. In Proceedings of
the European Conference on Computer Vision , pages 121–
136. Springer, 2022. 1
[56] Bingjie Xu, Yongkang Wong, Junnan Li, Qi Zhao, and Mo-
han S Kankanhalli. Learning to detect human-object interac-
tions with knowledge. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , 2019.
2
[57] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-
long Wang, and Shalini De Mello. Open-vocabulary panop-
tic segmentation with text-to-image diffusion models. arXiv
preprint arXiv:2303.04803 , 2023. 4, 7
[58] Xinyu Yi, Yuxiao Zhou, Marc Habermann, Soshi Shimada,
Vladislav Golyanik, Christian Theobalt, and Feng Xu. Phys-
ical inertial poser (pip): Physics-aware real-time human mo-
tion tracking from sparse inertial sensors. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 13167–13178, 2022. 1
[59] Hangjie Yuan, Jianwen Jiang, Samuel Albanie, Tao Feng,
Ziyuan Huang, Dong Ni, and Mingqian Tang. Rlip: Rela-
tional language-image pre-training for human-object interac-
tion detection. Advances in Neural Information Processing
Systems , 35:37416–37431, 2022. 3, 6
[60] Hangjie Yuan, Shiwei Zhang, Xiang Wang, Samuel Al-
banie, Yining Pan, Tao Feng, Jianwen Jiang, Dong Ni,
Yingya Zhang, and Deli Zhao. Rlipv2: Fast scaling of re-
lational language-image pre-training. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 21649–21661, 2023. 6, 7
[61] Aixi Zhang, Yue Liao, Si Liu, Miao Lu, Yongliang Wang,
Chen Gao, and Xiaobo Li. Mining the benefits of two-stage
and one-stage hoi detection. Advances in Neural Information
Processing Systems , 34:17209–17220, 2021. 1, 3, 5, 6, 7[62] Frederic Z Zhang, Dylan Campbell, and Stephen Gould.
Spatially conditioned graphs for detecting human-object in-
teractions. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 13319–13327, 2021.
2, 7
[63] Frederic Z Zhang, Yuhui Yuan, Dylan Campbell, Zhuoyao
Zhong, and Stephen Gould. Exploring predicate visual con-
text in detecting of human-object interactions. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 10411–10421, 2023. 6
[64] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun
Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr
with improved denoising anchor boxes for end-to-end object
detection. arXiv preprint arXiv:2203.03605 , 2022. 3
[65] Long Zhao, Liangzhe Yuan, Boqing Gong, Yin Cui, Flo-
rian Schroff, Ming-Hsuan Yang, Hartwig Adam, and Ting
Liu. Unified visual relationship detection with vision and
language models. arXiv preprint arXiv:2303.08998 , 2023. 3
[66] Desen Zhou, Zhichao Liu, Jian Wang, Leshan Wang, Tao Hu,
Errui Ding, and Jingdong Wang. Human-object interaction
detection via disentangled transformer. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 19568–19577, 2022. 1, 3
[67] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li,
Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and
Tong Sun. Towards language-free training for text-to-image
generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 17907–
17917, 2022. 2
[68] Bohan Zhuang, Qi Wu, Chunhua Shen, Ian Reid, and Anton
van den Hengel. Hcvrd: A benchmark for large-scale human-
centered visual relationship detection. In Proceedings of the
AAAI Conference on Artificial Intelligence , 2018. 3, 6
16964
