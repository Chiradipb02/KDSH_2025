Adversarial Backdoor Attack by Naturalistic Data Poisoning on Trajectory
Prediction in Autonomous Driving
Mozhgan Pourkeshavarz1Mohammad Sabokrou2Amir Rasouli1
1Noah’s Ark Lab, Huawei, Canada2Okinawa Institute of Science and Technology (OIST)
firstname.lastname@huawei.com
mohammad.sabokrou@oist.jp
Abstract
In autonomous driving, behavior prediction is funda-
mental for safe motion planning, hence the security and ro-
bustness of prediction models against adversarial attacks
are of paramount importance. We propose a novel adver-
sarial backdoor attack against trajectory prediction models
as a means of studying their potential vulnerabilities. Our
attack affects the victim at training time via naturalistic,
hence stealthy, poisoned samples crafted using a novel two-
step approach. First, the triggers are crafted by perturb-
ing the trajectory of attacking vehicle and then disguised by
transforming the scene using a bi-level optimization tech-
nique. The proposed attack does not depend on a particu-
lar model architecture and operates in a black-box manner,
thus can be effective without any knowledge of the victim
model. We conduct extensive empirical studies using state-
of-the-art prediction models on two benchmark datasets us-
ing metrics customized for trajectory prediction. We show
that the proposed attack is highly effective, as it can sig-
nificantly hinder the performance of prediction models, un-
noticeable by the victims, and efficient as it forces the vic-
tim to generate malicious behavior even under constrained
conditions. Via ablative studies, we analyze the impact of
different attack design choices followed by an evaluation of
existing defence mechanisms against the proposed attack.
1. Introduction
Trajectory prediction is one of the essential components
of autonomous driving (AD) systems necessary for safe
motion planning. Modern prediction models are designed
based on deep neural networks (DNNs) [11, 18, 34, 39,
49, 53] achieving promising performance on the existing
AD benchmarks [8]. Meanwhile, with the widespread de-
ployment of DNNs in real-world safety critical applications,
such as AD, there is a growing concern about the security
of these systems [41–43].
There is a large body of literature on adversarial attacks
Poisoning 
Training Data
AV
 AV
AV
 AV
Inference
AV
Observation Poisoned 
observationGround 
truthPrediction AV’s correct 
planAV’s altered 
planAV AtV Social 
vehicleAttack triggered MispredictionPoisoned data Disguised poisonFigure 1. Illustration of the proposed adversarial backdoor attack.
Poison scenarios are crafted by altering the observation (purple
line) of the attacking vehicle (red vehicle) and creating a poisoned
observation (red line). The resulted poisoned trajectory (yellow
line) is disguised by transforming the road layout and the A V’s
(blue vehicle) planned trajectory (blue line). The samples are then
injected in the training data of the victim. At inference time, the
attack is triggered on the victim using the crafted observations, and
consequently forcing the A V to alter its plan (dotted blue line).
and their impact on deep networks [10]. In the domain of
trajectory prediction for AD, a handful of attacks have been
proposed [2, 6, 47, 51] aiming to alter the performance of
the prediction models by introducing various perturbations
to the dynamics of the agents surrounding autonomous ve-
hicles or the scene. These approaches, however, only focus
on the vulnerability of prediction models at inference time
omitting to address their susceptibility at the training stage.
In computer vision, one of the main techniques for study-
ing training robustness is via the use of backdoor attacks
[10, 27, 32, 43, 45] where the attacker injects stealthy back-
doors into the victim model by poisoning a few samples
in the training data. The attacker achieves this by attach-
ing a trigger (i.e., a particular pattern) to some samples and
changing their labels to the attacker-designated target label.
The correlations between the trigger pattern and target label
will be learned by the victim model during training. Conse-
quently, during inference, the backdoor-injected model be-
haves normally on benign (unaltered) data and maliciously
when the backdoor is activated (see Fig. 1). The risk of
such attacks has been recognized as one of the major areas
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
14885
of concern in autonomous navigation and driving [26].
To this end, we propose a novel adversarial backdoor at-
tack designed to maliciously alter the performance of tra-
jectory prediction models in AD systems. Our attack gener-
ates naturalistic (stealthy) poison samples through a novel
disguising approach. Here, in an adversarial scheme, the
adversary first generates triggers by introducing perturba-
tions to the trajectory of the attacking vehicle and assigns a
malicious future trajectory to it. Then, to make the trigger
stealthy while preserving the attack’s effect, using the pro-
posed disguising method, the adversary conceals the gener-
ated trigger as a transformation of the road layout obtained
by a bi-level optimization technique. The proposed attack
can operate in a black-box manner and is model-agnostic,
meaning that it does not require the surrogate model (the
model used for generating poisoned samples) to have a sim-
ilar architecture compared to the victim model. To the best
of our knowledge this is the first adversarial backdoor attack
designed for trajectory prediction models.
In the proposed attack, the trigger is the attacking vehi-
cle (AtV)’s observation, which is a specific pattern designed
by the attacker. Hence, the trigger is a rare yet feasible,
i.e. realistic, trajectory pattern that can naturally be caused
by the surrounding agents of the autonomous vehicle (A V)
in real-world. In our method, the desired malicious out-
come produced by the backdoor-injected model is achieved
by intentionally inserting specific spurious correlations into
the training set, exploiting the vulnerability of the model to
learn those correlations. Therefore, the attack can be re-
garded as a method to discover the worst case predictions
for potential spurious correlations in the training data as a
means of determining the robustness of prediction models.
Contributions: 1) To the best of our knowledge, for the
first time we study the vulnerability of trajectory prediction
models from the viewpoints of data safety and security, and
robustness against potential spurious data correlations. For
this, we propose a novel adversarial backdoor attack by data
poisoning at training stage. Our model benefits from a novel
bi-level optimization technique to disguise triggers as natu-
ralistic and stealthy, hence, invisible to the victim; 2) To de-
termine the effectiveness and noticeability of the attack, we
propose modifications to the existing metrics and conduct
extensive empirical studies using state-of-the-art trajectory
prediction models on two AD benchmark datasets and high-
light the impact of our proposed attack under various con-
ditions; 3) Via performing ablation studies, we analyze the
effectiveness of the attack under different constraints, and
finally 4) we examine the capability of the existing defence
mechanisms against the proposed backdoor attack.
2. Related Works
Trajectory Prediction. The objective of trajectory predic-
tion models in AD is to forecast the future coordinates of theroad users for a given time horizon. There is a large body of
work in this domain proposing approaches based on diverse
architectural designs, including convolutional neural net-
works [4, 7, 38], graph neural networks [19, 28, 35, 37, 46],
and more recently, transformers [1, 13, 24, 25, 33, 54]. To
make predictions, these models rely on encoding contex-
tual information based on, for instance, rastersized images
[15, 20, 49] or vectorized representations [1, 13, 17, 19, 35]
capturing the scenes and often dynamics alike. The latter
representation is more dominant for encoding maps as it is
more compact and efficient.
Robustness against attacks .Given the central role of tra-
jectory prediction models in safe motion planning, their ro-
bustness to various adversarial attacks has been a major con-
cern. Recently, a number of attacks have been proposed to
study the vulnerability of these models. These attacks resort
to carefully crafted perturbations applied to agents’ trajecto-
ries [6, 47], or scene layout [2] and semantics [51]. The at-
tacks come in both types of white-box, where adversary has
access to the victim model’s parameters [6, 47] and black-
box, in which the adversary does not have access [2, 47].
The proposed attack follows the second approach, and uses
a different surrogate model than the victim to generate at-
tacks. As we will show later, in our approach, the surrogate
model does not necessarily need to have a similar architec-
ture for the attacks to be effective.
The existing attack mechanisms only address the vulner-
ability of prediction models at inference time. However,
the attacks can also occur at the training time by using ap-
proaches, such as backdoor attacks to poison the training
data. Consequently, the victim model would behave ma-
liciously when the backdoors are triggered. Such attacks
are generally harder to detect and more difficult to reverse
as they are encoded into the victim model. In this paper,
we propose a novel backdoor attack for poisoning trajectory
data. To the best of our knowledge, this is the first backdoor
attack designed for trajectory prediction in AD.
Backdoor Attack. A backdoor attack is a deep learn-
ing training-time threat that assumes the attacker can mod-
ify the training data and procedure of a given model. The
earliest works on backdoor attacks mainly focus on image
classification tasks [21] aiming to encourage malicious be-
haviours in the classifiers. However, due to the widespread
use of DNNs in the industry, especially in safety-critical ap-
plications, the backdoor attacks have also received substan-
tial attention in other fields of computer vision [10, 43, 45].
A category of backdoor attacks involves poisoning train-
ing data by injecting malicious samples with embedded trig-
gers as backdoors. Models trained on such data behave nor-
mally on clean samples (without a trigger) but will exhibit
a certain behavior on samples containing a trigger. To be
effective, the injected trigger should be unnoticeable (does
not impact the performance of the model on clean data)
14886
and stealthy (to appear realistic). In image classification,
this objective is achieved by using imperceptible pertur-
bations as backdoor triggers, consequently restricting the
differences between the triggered and clean images in ei-
ther pixel level [9, 30, 52] or latent space representation
[12, 36, 50]. Some works also rely on more explicit per-
turbations, for instance, by altering the color space [27] or
adding artificial reflections [32]. However, these methods
sacrifice the attack’s robustness and can be defeated using
common preprocessing techniques. To evaluate a backdoor
attack in classification tasks, clean accuracy (CA) and attack
success rate (ASR) are common metrics of choice. As for
the former, the backdoor-injected model’s classification ac-
curacy is measured against a clean test set and for the latter,
the accuracy is measured against a poisoned test set. Thus,
backdoor attacks with higher CA and ASR are considered
successful. However, there is a trade-off between these two
metrics, and it is often challenging to maintain high per-
formance in both. In the proposed attack, we employ a
novel bi-level optimization technique to first generate effec-
tive triggers and second to disguise them by performing dy-
namically feasible transformations to make them unnotice-
able. Furthermore, since backdoor attacks have been stud-
ied only in classification tasks, we present modifications of
the existing metrics, making them suitable for evaluating
the proposed attack in the trajectory prediction domain.
3. Methodology
3.1. Preliminary
Threat Model and Attack Requirements. We follow the
common threat model in the literature [14, 16, 48] and de-
fine two parties, the attacker and the victim . The attacker
can only manipulate the training data by injecting a small
number of poisoned samples and has no access to the victim
model and its training process. The poisoned samples are
made by implanting a trigger (a specific pattern designed
by the attacker) into the benign samples (the original data
samples) and changing the correct ground-truth to a mali-
cious one. Then, the victim, unaware of the attack, trains a
model on the poisoned data resulting in a backdoor-injected
model that learns the association between the trigger and
malicious ground-truth. Consequently, in inference time,
the backdoor-injected model generates correct predictions
given the benign samples (without the trigger) and mali-
cious predictions given the samples altered by the trigger.
To be effective, a backdoor attack should be:
Unnoticeable . When evaluated on benign samples, the
backdoor-injected model should perform similarly to the
model trained on the original data. To validate this prop-
erty, we measure the backdoor-injected model accuracy on
the benign test set using the clean accuracy (CA) metric.
Effective . For the inputs altered by the trigger (poisoned
samples) the backdoor-injected model should generate ma-licious predictions, i.e. it should behave the way the
attacker desires. To verify this property, the backdoor-
injected model is evaluated on the poisoned test set formed
by adding the trigger to the benign test set and the perfor-
mance is measured by the attack success rate (ASR) metric.
Stealthy . The injected poisoned samples should not be rec-
ognized as abnormal samples by the victim. Therefore, the
poisoned samples should 1) be perceived naturalistic , i.e.
have similar properties to the samples in the original data
and 2) appear to have correct ground-truths, or clean label
as termed in the backdoor attack literature.
Problem Formulation. The prediction modules in AD
systems forecast future trajectories of surrounding agents
according to their past observed behavior. Specifically, at
time step t, let the past trajectory of the i-th vehicle be a set
of2Dcoordinates in bird’s eye view over some observation
horizon Otime steps Xi={(xi, yi)t−O+1,···,(xi, yi)t}.
Accordingly, the objective is to predict future trajectory
Yi={(xi, yi)t+1,···,(xi, yi)t+T}, where Tis the pre-
diction horizon. The road information of the scene as an
HD map represented in the vector space is also provided.
Each scene is a matrix of stacked 2Dcoordinates consist-
ing of all lane points in the xycoordinate space where each
row represents a point (lx, ly)[2, 8]. For simplicity, in the
remainder of this paper, we refer to observations, future pre-
dictions, and lanes as X,Y, and ℓ, respectively.
3.2. Attack Overview
As illustrated in Fig. 2, our attack consists of three phases.
During the poisoning phase, given a clean (benign) sam-
ple we first generate a trigger in the form of a trajectory
of a vehicle, termed attacking vehicle (AtV). The trajec-
tory is over the stochastic observation horizon with a ma-
licious future path, such as turning towards the autonomous
vehicle (A V). We refer to samples containing AtVs as poi-
soned. These samples are made stealthy using by utilizing
a novel backdoor disguising method to generate natural-
istic trajectories with clean labels . In the training phase,
the model is trained on the poisoned dataset resulting in a
backdoor-injected model, which would behave maliciously
when given a sample occupied by a trigger.
3.3. Trigger Generation
The trigger is a specific pattern designed by the attacker to
craft a poisoned sample (Fig. 2.b) by adding a malicious be-
havior to a benign sample (Fig. 2.a). We design the trigger
based on the trajectory of the AtV , selected as the closest
vehicle to the A V , over the stochastic observation horizon.
The trigger generation is therefore viewed as a perturba-
tionδ(X), that is a minor change of the spatial coordinates
(∆x,∆y)of the AtV over the observation horizon.
To design the trigger, we use an adversarial scheme to
find the perturbation that yields the desired outcome [6, 47].
Following the past works, we assume the attacker uses a
14887
Poisoning phase Inference phaseDatasetGTObservation Malicious Observation AtV
AV Malicious GT
AV’s Plan
a. Clean sample
 c. Stealthy poisoned 
sample
Input
 Output
 b. Poisoned sample
Training phase
AV’s Prediction
AV 
mispredictsParams FrozenFigure 2. Overview of the proposed attack. At the poisoning phase, (a) given a benign sample, we select the closest vehicle to the A V with
observation Xand future trajectory Yas the attacking vehicle (AtV). (b) Next, using the trigger generation method T(.), we obtain AtV’s
perturbed observation δ(X), malicious observation acting as the trigger, and malicious future trajectory Yto craft a poisoned sample. (c)
Then, through the proposed backdoor disguising engine H(.)we apply transformations to the lanes γ(ℓ)to make the samples stealthy.
We use a surrogate model Fand have access to the dataset. The final kstealthy poisoned samples are injected into the training data. In
training phase, the victim trains a model on the poisoned dataset, resulting in a backdoor-injected model F. During inference, given an
input sample including an AtV with a malicious observation, the backdoor-injected model mispredicts the AtV’s future trajectory.
surrogate model Fwhich can be either the same as the vic-
tim model or a different one. We then define a set of con-
straints on the perturbation and an adversarial objective, e.g.
a loss function, to find the perturbation that maximizes the
attacker’s objective. We impose physical constraints on the
perturbations using a kinematic bicycle model [6] to ensure
that the altered trajectories are realistic. Then, we define the
adversarial objective as follows:
L=Ladv(Y,ˆY) +αLdyn(δ(X)), (1)
where δ(X)stands for the perturbed trajectory and Ldyn
bounds the dynamic parameters by coefficient α.Ladvde-
notes the metrics for evaluating the prediction error. For
this, we use average displacement error (ADE), final dis-
placement error (FDE) (error of the last predicted time
step), and two additional metrics, namely average deviation
towards the left and right side of the lateral direction [47].
3.4. Backdoor Disguising
The proposed backdoor disguising method aims to make the
poisoned samples stealthy (Fig. 2.b-c), thus, they are not
identifiable as abnormal. For this purpose, we disguise the
generated triggers, AtV’s altered trajectory, by transforma-
tions on the lanes γ(ℓ)under a condition Cto create a clean
label (i.e. future ground-truth that seems to be correct) for
the poisoned sample. To achieve this goal, we define the
following bi-level objective:
min
γ∈CE(X,ℓ,Y)∼Dk[L(F(δ(X), ℓ;θ(γ)),Y)]
s.t.θ(γ)∈arg min
θX
(X,ℓ,Y)∼DL(F(X, γ(ℓ);θ),Y),(2)
where XandYare AtV’s observation and future trajecto-
ries before the alteration, and δ(X),Ystand for AtV’s al-tered observation (trigger) and malicious future trajectories,
respectively. DandDkdenote the training and poisoned
sets with sizes mandk(k << m ). We set Lconsistent
with the objective Ladvused in the trigger generation step.
The condition Cis defined as a constraint on the transfor-
mation to ensure that the transformed lanes make the AtV’s
label clean. Specifically, we check whether the AtV’s ma-
licious future trajectory Yis perceived as a correct ground-
truth, e.g. no off-road, with the heading angle of trajectory
aligned with the altered lane direction.
For lane transformation function γ(.), we define a gen-
eral form of point transformation [2] described below:
˜l= (lx, ly+f(lx−r)), (3)
where ˜lis the transformed point, fis a differentiable single-
variable transformation function, and ris the reference
point that defines the starting point of the transforming area.
It should be noted that the transformation also applies to the
non-malicious trajectories that are on the transformed lanes.
To maintain the feasibility of the transformed trajecto-
ries, we determine whether the length of the trajectories is
less than the maximum feasible displacement achievable in
the given time horizon on the altered lanes [2, 22] and clip
the trajectories if their length exceeds to satisfy the check.
To circumvent the difficulty of bi-level optimization in
Eq. 2, we approximate it using gradient alignment tech-
nique [14, 16, 40] to modify the data to align the training
gradient with the gradient of some desired objective. Con-
trary to other heuristics, e.g. partial unrolling of the com-
putation graph, gradient alignment is a more stable way to
solve a bi-level problem that entails training a network in an
inner objective [14]. We define the adversarial objective as:
Latt=E(X,ℓ,Y)∼D[L(F(δ(X);θ),Y)], (4)
14888
Algorithm 1: Proposed backdoor attack
Input : Training Data D, Poisoning budget k,
Optimization steps S, Retraining factor T,
Surrogate network F
Output : Poison perturbations
1δ(X),Y← T(X,Y)// trigger generation
2randomly initialize perturbations γ(ℓ)k
i=1
3forr= 1toSdo
4 Compute A(γ, θ, δ (X),Y,Y)using Eq. 5
5 Update γ(ℓ)k
i=1with a step of signed Adam
6 ifrmod⌊S/(T+ 1)⌋= 0andr̸=Rthen
7 Poisoned training data D ←
{(Xi, γ(ℓi),Yi)}k
i=1∪ {(Xi, ℓi,Yi)}n
i=k+1
8 Retrain network FonD
9 Update network parameters F(.;θ)
10end
11return poison perturbations γ(ℓ)k
i=1
which is minimized when, given the AtV’s observed trajec-
toryδ(X), the model mispredicts the AtV’s future as a ma-
licious trajectory Y. For this, we perturb the training data
by optimizing the following alignment objective:
A= 1−∇θLtrain· ∇θLatt
∥∇θLtrain∥ · ∥∇ θLatt∥, (5)
where our goal is to find the transformation on lanes γ(ℓ)
that will make ∇θLtrainto be aligned with ∇θLattas follows:
∇θLtrain=1
mmX
i=1∇θL(F(X, γ(ℓ);θ),Yi). (6)
Given the training gradient involving the nonzero transfor-
mations ∇θLtrain, we estimate the expectation in Eq. 4 by
calculating the average adversarial loss as follows:
∇θLatt=1
kkX
i=1∇θL(F(δ(X), ℓ;θ),Yi). (7)
We first optimize Eq. 5 by fixing parameters θto calcu-
lateAthroughout the crafting process. The parameters are
trained on the benign data and used to calculate the training
∇θLtrainand adversary ∇θLattgradients. We then optimize
parameters θusing ssteps of signed Adam [3]. The com-
plete method is summarized in Algorithm 1.
Another aspect of the optimization is selection of benign
samples for trigger injection. To achieve this, we resort to
sample selection by gradient norm. More superficially, we
align the training gradient with our adversary objective and
aim to select the samples that have larger gradients since
such samples can be more potent poison instances.
3.5. Evaluating Attacks
As discussed in Sec. 2, there are two widely used metrics,
namely clean accuracy (CA) and attack success rate (ASR),for evaluating backdoor attacks. These metrics, however,
are used for discrete classification tasks and are not di-
rectly applicable to trajectory prediction. Hence, we pro-
pose modifications to these metrics, referring to them as
tASR and tCA where t stands for trajectory. For tCA, we
compute ADE or FDE of the backdoor-injected model and
clean model (the model trained with the original training
set) on the benign validation data. We compare the errors
for both models sample-wise, and if the degradation of a
sample error is less than a threshold th1, we consider that
instance a correct prediction, otherwise incorrect. tCA is
then calculated as the ratio of the correct predictions over all
predictions. Similarly, for tASR, we compute the errors for
both backdoor-injected and clean models on the poisoned
validation set and compare them. If degradation of the error
on a sample is more than a threshold th2, then the attack is
successful, otherwise it is not. tASR is then calculated as
the ratio of successful attacks over all attacks.
The way the thresholds are set is important as they
should correspond to real driving conditions. Given that
urban lanes are approximately 3.7mwide and cars are on
average 1.7mwide, a 1mdeviation is an acceptable devia-
tion for a car not to drive off-road or into another lane when
normally driving in the center of a lane [23]. Therefore, we
setth2= 1m. For clean accuracy, however, we consider a
stricter threshold of th1= 0.5mto increase the sensitivity
of this metric to slight deviations as it corresponds to the
detectability of the attacks not their success rate.
4. Experiments
For evaluation, we seek to answer the following ques-
tions: Q1: Does the proposed attack remain unnoticeable
despite its effectiveness? Q2: For a successful attack, how
many poisoned samples should be injected into the training
dataset? Q3: How many AtVs are required to successfully
launch the attack? Q4: When making poisoned samples,
does the choice of benign samples affect the attack success
rate? Q5: Is the proposed attack still effective with partial
access to the training data? Q6: Does the proposed attack
work across different representation encoding? Q7:Are the
existing defences effective against the proposed attack?
Datasets We use two widely-used large-scale autonomous
driving datasets, namely nuScenes [8] and Argoverse [5],
which are catered for trajectory prediction task. nuScenes
contains 1K driving scenes, each of which are 20slong and
annotated at 2 Hz . For this benchmark the task is to predict
6sfuture trajectory given 2sobservation. The sequences
extracted from the driving scene are split train, validation
and test sets with 32K, 8.6K and 9K instances in each re-
spectively. Argoverse consists of over 30 K driving scenar-
ios, each sampled at 10 Hz . Here, the task is to predict 3s
future trajectory of a road agent given 2sobservations. The
sequences in this data are split into train, val, and test sets
14889
Table 1. Overall attack results on different models evaluated on nuScenes and Argoverse. ADE and FDE metrics represent the performance
of the clean models without any attacks, hence smaller values are better as indicated by (↓∗). The benign/poison column shows the
performance of the backdoor-injected models on the original validation set (no attacks) and the validation set with poisoned samples,
respectively. For all metrics after attack, shown on highlighted gray area, higher values (↑)mean the attack was more successful.
DatasetSurrogate
modelBackdoor-
injected modelADE FDE
original (↓∗)benign/poison (↑)tCA(↑)tASR (↑)original (↓∗)benign/poison (↑)tCA(↑)tASR (↑)
nuScenesPGP [11] LaPred [29] 1.22 1.47/2.98 89.13 76.98 2.24 2.31/4.89 86.32 93.12
LaPred [29] PGP [11] 0.94 1.06/2.67 91.01 74.21 1.55 1.71/3.69 87.12 83.45
ArgoverseTNT [49]
HiVT [53] 0.660.82/3.54 94.86 91.00
0.961.10/5.36 96.02 92.66
MMTrans. [33] 0.75/3.67 95.30 91.11 1.04/5.21 96.88 93.12
LaneGCN [31] 0.88/3.43 94.06 90.88 1.21/5.48 95.39 92.89
HiVT [53]
TNT [49] 0.952.09/2.83 73.16 88.22
1.733.02/4.36 75.72 86.79
MMTrans. [33] 1.14/3.61 92.01 90.74 2.13/5.35 94.73 89.96
LaneGCN [31] 1.32/2.76 89.46 87.39 2.28/4.10 90.33 85.29
HiVT [53]
MMTrans. [33] 0.701.39/3.16 84.21 78.55
1.082.31/4.40 85.67 83.44
TNT [49] 1.56/2.91 80.37 77.89 2.61/4.06 83.01 80.71
LaneGCN [31] 1.49/3.05 82.40 79.99 2.48/4.14 85.06 80.31
HiVT [53]
LaneGCN [31] 0.711.09/3.02 87.00 83.65
1.081.78/3.46 88.92 84.19
TNT [49] 1.12/2.92 86.46 89.24 1.88/3.48 89.65 86.99
MMTrans. [33] 1.01/3.48 88.02 88.23 1.59/3.61 89.79 86.36
with 206K, 39K, and 78K sequences in each, respectively.
Models. On each dataset, we evaluate the state-of-the-art
(SOTA) models from the corresponding benchmark leader-
boards. For nuScenes, we choose PGP [11] and LaPred
[29] and for Argoverse, we select HiVT [53], TNT [49],
MMTransformer [33], and LaneGCN [31], which are rep-
resentative of different approaches. We use official code
released for all models, with the exception of TNT1.
Implementation. We set R= 4for solving the bi-level
optimization. For the transformation function, we use f=
γ1(1−cos (2 πγ2sx))forlx≥0and zero for the rest of the
points, where γ1andγ2determine the turn curvature and the
sharpness of the turns. We empirically choose γ1= 5.75
andγ2= 0.015for best results. We set the number of steps
in the alignment process as S= 250 .
4.1. Black-box Backdoor Attack
We evaluate the proposed attack in a black-box fashion,
meaning that we impose a restrictive condition and only
allow the attacker to have access to the training dataset
without any knowledge of the training model. Thus, in
each experiment, a different surrogate model other than the
backdoor-injected model is chosen. The experimental re-
sults are reported in Table 1.
Unnoticeable and effective. The first glance at the results
reveals the effectiveness of the proposed attack as the per-
formance of all models on both datasets has significantly
degraded on both benign and poison validation sets. The
higher clean accuracy (tCA) values, especially for top per-
forming models, such as PGP ( 91%) on nuScenes and HiVT
(95%) on Argoverse, indicate that our attack is unnoticeable
as these models learned the poison samples during the train-
ing phase without any major impact on their performance
during validation on clean data. However, once exposed to
poison samples, the models are significantly impacted, as
1The implementation used is from https://github.com/
Henry1iu/TNT-Trajectory-Predictionindicated by high attack success ratios (tASRs), in the case
of PGP and HiVT by more than 83% and93%, respectively.
High tASR values show how effective the proposed attack
is in forcing the models to generate malicious behavior at
inference time. Overall, large values of tCA and tASR met-
rics across all models indicate that the proposed attack is
unnoticeable and yet effective in altering the performance
of the victim models even without having access to their
architecture and parameters.
Accuracy vs robustness. In addition to measuring how un-
noticeable the attacks are, tCA shows the robustness of the
models to poisoned training data when evaluated under nor-
mal conditions. Hence, as shown in Table 1, there is a cor-
relation between accuracy and tCA values for top perform-
ing models. For instance, best performing models, PGP and
HiVT on both datasets with the highest accuracy ( 0.94/1.55
and0.66/0.96) also have the highest overall tCA values
(91%/87% and95%/97%). However, higher accuracy does
not necessarily translate to higher robustness. For example,
TNT with the worst overall accuracy ( 0.95/1.73) has a bet-
ter tCA ( 92%) compared to MMTransformer with higher
accuracy ( 0.70/1.08)) but lower tCA ( 80%).
Similarly for tASR, for instance, the most accurate
model on Argoverse, HiVT, is at the same time the most
vulnerable model to the proposed attack reaching the peak
value of 93%. In terms of robustness in training vs infer-
ence, TNT with the second highest tCA value on Argov-
erse, is also the second worst model in terms of tASR, as
this model is impacted significantly more by the proposed
attack compared to MMTransformer and LaneGCN.
Choice of the surrogate model. Last but not least, as
shown in Table 1, regardless of the choice of surrogate
model, the proposed attack is very successful. This is
highlighted in small fluctuations of tCA and tASR values
for each backdoor-injected model with different surrogates.
There are, however, exceptions as well. For instance, tCA
value of TNT when trained on the poison data with HiVT as
14890
Table 2. tASR and tCA metric values with different numbers of AtVs (q) as the trigger for different backdoor-injected models on the
Argoverse dataset. (↓)and (↑)show lower and higher values are better.
Backdoor-injected model
HiVT [53] TNT [49] MMTrans. [33] LaneGCN [31]
#AtV / metric tCA(↑)tASR (↑)tCA(↑)tASR (↑)tCA(↑)tASR (↑)tCA(↑)tASR (↑)
q = 1 95.30 91.11 92.01 90.74 82.40 79.99 86.46 89.24
q = 2 92.03 94.29 88.32 92.65 79.36 80.07 83.66 90.37
q = 3 86.11 95.47 86.21 95.78 68.93 84.63 75.47 91.58
q = 4 71.34 98.02 73.12 96.22 60.16 87.27 69.20 93.23
5 10 20 50
Poisoning rate (%)405060708090100 Metric (%)91.194.5 95.999.9
95.3
86.5
78.3
50.0HiVT
5 10 20 50
Poisoning rate (%)405060708090100
90.7 92.096.098.0
92.090.1
83.5
71.0TNT
5 10 20 50
Poisoning rate (%)405060708090100
80.083.188.695.9
84.2
74.1
60.1
48.1MMTrans.
5 10 20 50
Poisoning rate (%)405060708090100
89.293.297.099.0
88.0
79.3
67.2
42.1LaneGCN
Metric
tASR
tCA
Figure 3. The effectiveness of proposed attack with varying poison rates for different backdoor-injected models on Argoverse.
surrogate is significantly lower compared to other surrogate
models. This can be due to the properties of the generated
samples by HiVT that are not easily learnable for TNT.
4.2. Ablation Study
Number of AtVs. Thus far we showed the effectiveness
of the proposed attack with triggers using only a single at-
tacking vehicle (AtV). Here, we experiment with varying
numbers of AtVs in poisoned samples. More specifically,
we set the number of AtV as q={1,2,3,4}.In each, we
designate the closest qvehicles to the A V as AtVs.
As shown in Table. 2, in general, higher the number of
AtVs, the more impactful the attack is as it is evident in
the rise of tASR and fall of tCA values. This is because
the larger number of AtVs increases the likelihood that the
model learns the association between the trigger and mali-
cious label. Hence, the model becomes more vulnerable to
the attack. Once again, depending on the victim models,
the impact of the attack may vary. Overall, HiVT has the
highest drop ( 24%) in tCA for q= 4and the highest gain in
tASR, similar to MMTransformer, by approx. 7%. Among
all models, with respect to both metrics, LanceGCN is gen-
erally least impacted by increasing the number of AtVs.
Poison budget. To study the utility and specificity
of the proposed attack, we vary the poisoning rate (bud-
get)Pr. i.e. the fraction of modified training samples
to all samples. The results for different poisoning rates,
Pr={5%,10%,20%,50%}are illustrated in Fig. 3. For
each model in Table 1, we select the surrogate variation that
resulted in the highest tASR value for Pr= 5. As we can
see the proposed attack is very efficient and can succeed
with as small as 5%poison budget. As expected, poisoning
rate has direct and inverse relationship with tASR and tCA
respectively. The higher the poisoning rate is, the higher
tASR and the lower the tCA values are. The inclination of
change, however, is different for the models. For instance,
TNT has the lowest drop in tCA, with only 21%, reachingthe top spot of 71% on50% poison budget. LaneGCN, on
the other hand, has the lowest tCA for the same budget at
42.1%with the drop date of up to 46%.
Effect of sample selection. As mentioned in Sec. 3.4,
samples with larger gradients are selected to craft poison
samples. To verify this approach, we conduct an experiment
using a random selection mechanism with varying poison-
ing rates. As shown in Table 3 using the proposed gradient-
based selection approach is significantly more effective, es-
pecially for smaller poison budgets where an increase of up
to19% is achieved. The reduction in gap between different
selection mechanisms for larger budgets is expected as the
likelihood of larger gradient samples being selected in the
random procedure increases.
Full vs. partial access. In previous experiments, the
assumption was that the attacker has full access to the train-
ing data. Here, we conducted an experiment limiting the
attacker’s access to only a part of the training data. We
randomly select d%of the training dataset and launch the
proposed attack on two models, namely HiVT [53] and
MMTrans [33] with the highest and the lowest robustness
against the attacks on Argoverse according to Table 1. Here,
each model acts as the surrogate to design the attack against
the other model. Based on the results in Table 4, as ex-
pected, the effectiveness of the attack is lowered as the ac-
cess to the training data is reduced. However, the ratio
of attack’s impact degradation is significantly lowere com-
pared to the ratio of limiting data access. In the case of
d= 80% there is only a minor fluctuation in both models’
tASR ( ≈1%) and tCA ( ≈7%) . When reducing the access
to only 50%, the drop in tCA and tASR of both models do
not exceed 17% and8%, respectively. This shows that the
attack is still effective even with partial access to the data.
Note that the higher drop ratio in tCA compared to tASR is
expected as the attacks are optimized with respect to effec-
tiveness rather than being unnoticeable.
14891
Table 3. tASR metric values with different selection mechanisms for different backdoor-injected models and varying poisoning rates on
the Argoverse dataset. -G and -R stand for gradient and random selection, respectively. (↑)shows the attack is more successful.
Backdoor-injected model
HiVT [53] TNT [49] MMTrans. [33] LaneGCN [31]
Pr / metric tASR-R (↑)tASR-G (↑)tASR-R (↑)tASR-G (↑)tASR-R (↑)tASR-G (↑)tASR-R (↑)tASR-G (↑)
Pr = 5 (%) 86.12 91.11 83.67 90.74 70.23 79.99 70.69 89.24
Pr = 10 (%) 90.39 94.29 88.83 92.65 73.43 80.07 83.66 90.37
Pr = 20 (%) 92.67 95.47 92.00 95.78 79.99 84.63 86.58 91.58
Pr = 50 (%) 95.89 98.02 94.16 96.22 81.32 87.27 89.69 93.23
Table 4. Ablation study of the attacker’s partial access to the train-
ing dataset. Higher values (↑)mean the attack is more successful.
Backdoor-injected model
HiVT [53] MMTrans [33]
d / metric tCA(↑) tASR (↑) tCA(↑) tASR (↑)
d = 100 (%) 95.30 91.11 84.21 78.55
d = 80 (%) 93.63 (-1.67) 90.88 (-0.23) 78.16 (-6.05) 79.69 (+1.14)
d = 50 (%) 83.28 (-12.02) 86.13 (-4.98) 70.61 (-13.60) 72.37 (-6.18)
5 10 20 50
Poisoning rate (%)405060708090100 Metric
69.578.381.389.5
72.881.484.192.4 tCA: Trajectron++  
tCA: AgentFormer 
tASR: Trajectron++
tASR: AgentFormer
Figure 4. The attack with varying poison rates on the models with
rasterized-based map encoder on nuScenes.
Table 5. Performance with (w) and without (w/o) applying the
defence. Higher values (↑)mean the attack is more successful.
w/o defence w defence
Backdoor-injected model tCA(↑)tASR (↑) tCA(↑) tASR (↑)
HiVT [53] 95.30 91.11 93.33 (-1.97) 88.52 (-2.59)
MMTrans. [33] 84.21 78.55 81.62 (-2.59) 73.98 (-4.57)
4.3. Cross-representation Backdoor Attack
In our experiments thus far, despite architectural differ-
ences, the surrogate and victim models shared similar en-
coding mechanisms based on the vector map representation.
Here, our goal is to determine whether attacks designed us-
ing a model with vector-based map encoder can be effective
against the models consist of rasterized map encoders. To
this end, we use PGP [11] as surrogate and two state-of-
the-art models with rasterized map representation, namely
Trajectron++ [38] and AgentFormer [44], as victims. As
demonstrated in Fig. 4, the proposed attack is still as effec-
tive as before. In fact, we can observe similar trends com-
pared to vectorized surrogate-victim pairs. On Prequal to
5%and10%, we can observe high tCA and tASR values,
which point to the efficiency and effectiveness of the pro-
posed attack while staying inconspicuous. As before, by
increasing the poison budget, there is an increase in tASR
value and decline in tCA on both victim models.
4.4. Defence and Mitigation
Since the proposed attack is based on data poisoning in a
black-box setting, the defence mechanism should be de-ployed in the training time to detect poisoned samples be-
fore being fed in the victim model. Due to the stealthiness
of the proposed attacks achieved by our backdoor disguis-
ing approach, the triggers (the AtV’s malicious observation)
used to induce mispredictions during inference time are not
directly observable in the training dataset. This means that
the existing preprocessing trajectory mechanisms [6, 47]
would not be effective to mitigate the proposed attack.
Since poison samples are rare in the training data,
detection-based defences using gradient shaping methods,
which are effective against gradient alignment based attacks
can be used [23]. Following [23], during the training phase,
the gradients of the weights that are perceived abnormal are
clipped and perturbed by adding some noise to them in or-
der to mitigate the effect of poisoned samples. We experi-
mented using HiVT [53] and MMTransformer [33] models
on Argoverse with the clipping and noise values from [23]
and report the best results with the highest attack mitiga-
tion impact in Table 5. As the findings suggest, although
the defence is effective, the improvements are marginal, up
to3%in tCA and 6%in ASR. As a result, the attack stays
very effective, maintaining over 70% tASR on both models.
The reason for this is that even though the injected transfor-
mations are rare, they are realistic thanks to our disguising
method based on dynamically feasible constraints.
5. Conclusion
We proposed a novel adversarial backdoor attack as a means
of studying the vulnerability of trajectory prediction mod-
els in security-critical systems, such as autonomous driving.
Our method is based on a novel bi-objective optimization
process that generates attack triggers and effectively dis-
guises them via realistic transformations. We conducted
extensive empirical evaluations on state-of-the-art trajec-
tory prediction models on common benchmark datasets and
showed that our attack is not noticeable and significantly
effective to force victim models to generate malicious pre-
dictions. Furthermore, we conducted ablation studies high-
lighting the effectiveness of the proposed attacks under con-
strained conditions and also showed that the existing de-
fence mechanisms are not very effective in mitigating the
impact of our attacks. Our work highlighted the potential
danger of backdoor attacks in autonomous driving and the
necessity of designing more robust algorithms and defence
mechanisms to detect and mitigate the effect of such attacks.
14892
References
[1] Gorkay Aydemir, Adil Kaan Akan, and Fatma Guney.
ADAPT: Efficient multi-agent trajectory prediction with
adaptation. In ICCV , 2023. 2
[2] Mohammadhossein Bahari, Saeed Saadatnejad, Ahmad
Rahimi, Mohammad Shaverdikondori, Amir Hossein
Shahidzadeh, Seyed-Mohsen Moosavi-Dezfooli, and
Alexandre Alahi. Vehicle trajectory prediction works, but
not everywhere. In CVPR , 2022. 1, 2, 3, 4
[3] Lukas Balles and Philipp Hennig. Dissecting adam: The
sign, magnitude and variance of stochastic gradients. In
ICML , 2018. 5
[4] Mayank Bansal, Alex Krizhevsky, and Abhijit Ogale. Chauf-
feurNet: Learning to drive by imitating the best and synthe-
sizing the worst. In RSS, 2019. 2
[5] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-
ancarlo Baldan, and Oscar Beijbom. nuScenes: A multi-
modal dataset for autonomous driving. In CVPR , 2020. 5
[6] Yulong Cao, Chaowei Xiao, Anima Anandkumar, Danfei
Xu, and Marco Pavone. AdvDO: Realistic adversarial at-
tacks for trajectory prediction. In ECCV , 2022. 1, 2, 3, 4,
8
[7] Yuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir
Anguelov. MultiPath: Multiple probabilistic anchor trajec-
tory hypotheses for behavior prediction. In CoRL , 2019. 2
[8] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jag-
jeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter
Carr, Simon Lucey, Deva Ramanan, et al. Argoverse: 3D
tracking and forecasting with rich maps. In CVPR , 2019. 1,
3, 5
[9] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn
Song. Targeted backdoor attacks on deep learning systems
using data poisoning. arXiv:1712.05526 , 2017. 3
[10] Sheng-Yen Chou, Pin-Yu Chen, and Tsung-Yi Ho. How to
backdoor diffusion models? In CVPR , 2023. 1, 2
[11] Nachiket Deo, Eric Wolff, and Oscar Beijbom. Multimodal
trajectory prediction conditioned on lane-graph traversals. In
CoRL , 2022. 1, 6, 8
[12] Khoa Doan, Yingjie Lao, and Ping Li. Backdoor attack with
imperceptible input and latent modification. In NeurIPS ,
2021. 3
[13] Shaoheng Fang, Zi Wang, Yiqi Zhong, Junhao Ge, and Si-
heng Chen. TBP-Former: Learning temporal bird’s-eye-
view pyramid for joint perception and prediction in vision-
centric autonomous driving. In CVPR , 2023. 2
[14] Liam Fowl, Ping-yeh Chiang, Micah Goldblum, Jonas Geip-
ing, Arpit Bansal, Wojtek Czaja, and Tom Goldstein. Pre-
venting unauthorized use of proprietary data: Poisoning for
secure dataset release. arXiv:2103.02683 , 2021. 3, 4
[15] Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir
Anguelov, Congcong Li, and Cordelia Schmid. VectorNet:
Encoding HD maps and agent dynamics from vectorized rep-
resentation. In CVPR , 2020. 2
[16] Jonas Geiping, Liam H Fowl, W. Ronny Huang, Wojciech
Czaja, Gavin Taylor, Michael Moeller, and Tom Goldstein.Witches’ brew: Industrial scale data poisoning via gradient
matching. In ICLR , 2021. 3, 4
[17] Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bog-
dan Stanciulescu, and Fabien Moutarde. GOHOME: Graph-
oriented heatmap output for future motion estimation. In
ICRA , 2022. 2
[18] Roger Girgis, Florian Golemo, Felipe Codevilla, Martin
Weiss, Jim Aldon D’Souza, Samira Ebrahimi Kahou, Fe-
lix Heide, and Christopher Pal. Latent variable sequential
set transformers for joint multi-agent motion prediction. In
ICLR , 2022. 1
[19] Daniel Grimm, Philip Sch ¨orner, Moritz Dreßler, and J.-
Marius Z ¨ollner. Holistic graph-based motion prediction. In
ICRA , 2023. 2
[20] Junru Gu, Chen Sun, and Hang Zhao. DenseTNT: End-to-
end trajectory prediction from dense goal sets. In ICCV ,
2021. 2
[21] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth
Garg. Badnets: Evaluating backdooring attacks on deep neu-
ral networks. IEEE Access , pages 47230–47244, 2019. 2
[22] David Halliday, Robert Resnick, and Jearl Walker. Funda-
mentals of Physics . John Wiley & Sons, 2013. 4
[23] Sanghyun Hong, Varun Chandrasekaran, Yi ˘gitcan Kaya, Tu-
dor Dumitras ¸, and Nicolas Papernot. On the effectiveness
of mitigating data poisoning attacks with gradient shaping.
arXiv:2002.11497 , 2020. 5, 8
[24] Zhiyu Huang, Xiaoyu Mo, and Chen Lv. Multi-modal mo-
tion prediction with transformer-based neural network for
autonomous driving. In ICRA , 2022. 2
[25] Chiyu “Max” Jiang, Andre Cornman, Cheolho Park, Ben-
jamin Sapp, Yin Zhou, and Dragomir Anguelov. Motion-
Diffuser: Controllable multi-agent motion prediction using
diffusion. In CVPR , 2023. 2
[26] Wenbo Jiang, Hongwei Li, Sen Liu, Xizhao Luo, and Rongx-
ing Lu. Poisoning and evasion attacks against deep learning
algorithms in autonomous vehicles. Transactions on Vehicu-
lar Technology , 2020. 2
[27] Wenbo Jiang, Hongwei Li, Guowen Xu, and Tianwei Zhang.
Color backdoor: A robust poisoning attack in color space. In
CVPR , 2023. 1, 3
[28] Siddhesh Khandelwal, William Qi, Jagjeet Singh, Andrew
Hartnett, and Deva Ramanan. What-if motion prediction for
autonomous driving. arXiv:2008.10587 , 2020. 2
[29] ByeoungDo Kim, Seong Hyeon Park, Seokhwan Lee, Elbek
Khoshimjonov, Dongsuk Kum, Junsoo Kim, Jeong Soo Kim,
and Jun Won Choi. LaPred: Lane-aware prediction of multi-
modal future trajectories of dynamic agents. In CVPR , 2021.
6
[30] Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin
Zhu, and Xinpeng Zhang. Invisible backdoor attacks on
deep neural networks via steganography and regularization.
Transactions on Dependable and Secure Computing , 2020.
3
[31] Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song
Feng, and Raquel Urtasun. Learning lane graph representa-
tions for motion forecasting. In ECCV , 2020. 6, 7, 8
14893
[32] Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Re-
flection backdoor: A natural backdoor attack on deep neural
networks. In ECCV , 2020. 1, 3
[33] Yicheng Liu, Jinghuai Zhang, Liangji Fang, Qinhong Jiang,
and Bolei Zhou. Multimodal motion prediction with stacked
transformers. In CVPR , 2021. 2, 6, 7, 8
[34] Daehee Park, Hobin Ryu, Yunseo Yang, Jegyeong Cho, Ji-
won Kim, and Kuk-Jin Yoon. Leveraging future relationship
reasoning for vehicle trajectory prediction. In ICLR , 2023. 1
[35] Mozhgan Pourkeshavarz, Changhe Chen, and Amir Ra-
souli. Learn TAROT with MENTOR: A meta-learned self-
supervised approach for trajectory prediction. In ICCV ,
2023. 2
[36] Yankun Ren, Longfei Li, and Jun Zhou. Simtrojan: Stealthy
backdoor attack. In ICIP , 2021. 3
[37] Luke Rowe, Martin Ethier, Eli-Henry Dykhne, and
Krzysztof Czarnecki. FJMP: Factorized joint multi-agent
motion prediction over learned directed acyclic interaction
graphs. In CVPR , 2023. 2
[38] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and
Marco Pavone. Trajectron++: Dynamically-feasible trajec-
tory forecasting with heterogeneous data. In ECCV , 2020. 2,
8
[39] Haoran Song, Di Luan, Wenchao Ding, Michael Y Wang,
and Qifeng Chen. Learning to predict vehicle trajectories
with model-based planning. In CoRL , 2022. 1
[40] Hossein Souri, Liam Fowl, Rama Chellappa, Micah Gold-
blum, and Tom Goldstein. Sleeper agent: Scalable hidden
trigger backdoors for neural networks trained from scratch.
InNeurIPS , 2022. 4
[41] Chenyu Yi, Siyuan Yang, Haoliang Li, Yap-peng Tan, and
Alex Kot. Benchmarking the robustness of spatial-temporal
models against corruptions. In NeurIPS , 2021. 1
[42] Yi Yu, Wenhan Yang, Yap-Peng Tan, and Alex C Kot. To-
wards robust rain removal against adversarial attacks: A
comprehensive benchmark analysis and beyond. In CVPR ,
2022.
[43] Yi Yu, Yufei Wang, Wenhan Yang, Shijian Lu, Yap-Peng
Tan, and Alex C Kot. Backdoor attacks against deep im-
age compression via adaptive frequency trigger. In CVPR ,
2023. 1, 2
[44] Ye Yuan, Xinshuo Weng, Yanglan Ou, and Kris M Kitani.
AgentFormer: Agent-aware transformers for socio-temporal
multi-agent forecasting. In ICCV , 2021. 8
[45] Zenghui Yuan, Pan Zhou, Kai Zou, and Yu Cheng. You are
catching my attention: Are vision transformers bad learners
under backdoor attacks? In CVPR , 2023. 1, 2
[46] Wenyuan Zeng, Ming Liang, Renjie Liao, and Raquel Ur-
tasun. LaneRCNN: Distributed representations for graph-
centric motion forecasting. In IROS , 2021. 2
[47] Qingzhao Zhang, Shengtuo Hu, Jiachen Sun, Qi Alfred
Chen, and Z Morley Mao. On adversarial robustness of tra-
jectory prediction for autonomous vehicles. In CVPR , 2022.
1, 2, 3, 4, 8
[48] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset
condensation with gradient matching. In ICLR , 2021. 3[49] Hang Zhao, Jiyang Gao, Tian Lan, Chen Sun, Ben Sapp,
Balakrishnan Varadarajan, Yue Shen, Yi Shen, Yuning Chai,
Cordelia Schmid, et al. TNT: Target-driven trajectory pre-
diction. In CoRL , 2021. 1, 2, 6, 7, 8
[50] Zhendong Zhao, Xiaojun Chen, Yuexin Xuan, Ye Dong,
Dakui Wang, and Kaitai Liang. DEFEAT: Deep hidden fea-
ture backdoor attacks by imperceptible perturbation and la-
tent representation constraints. In CVPR , 2022. 3
[51] Zhihao Zheng, Xiaowen Ying, Zhen Yao, and Mooi Choo
Chuah. Robustness of trajectory prediction models under
map-based attacks. In WACV , 2023. 1, 2
[52] Haoti Zhong, Cong Liao, Anna Cinzia Squicciarini, Sencun
Zhu, and David Miller. Backdoor embedding in convolu-
tional neural network models via invisible perturbation. In
Proceedings of the Tenth ACM Conference on Data and Ap-
plication Security and Privacy , 2020. 3
[53] Zikang Zhou, Luyao Ye, Jianping Wang, Kui Wu, and Kejie
Lu. HiVT: Hierarchical vector transformer for multi-agent
motion prediction. In CVPR , 2022. 1, 6, 7, 8
[54] Zikang Zhou, Jianping Wang, Yung-Hui Li, and Yu-Kai
Huang. Query-centric trajectory prediction. In CVPR , 2023.
2
14894
