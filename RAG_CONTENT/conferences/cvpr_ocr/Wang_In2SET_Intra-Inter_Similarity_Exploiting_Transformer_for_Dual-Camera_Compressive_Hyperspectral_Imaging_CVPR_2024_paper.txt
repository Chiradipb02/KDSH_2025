In2SET: Intra-Inter Similarity Exploiting Transformer for
Dual-Camera Compressive Hyperspectral Imaging
Xin Wang1Lizhi Wang1* Xiangtian Ma1Maoqing Zhang1Lin Zhu1Hua Huang2
1Beijing Institute of Technology2Beijing Normal University
Abstract
Dual-camera compressive hyperspectral imaging (DC-
CHI) offers the capability to reconstruct 3D hyperspec-
tral image (HSI) by fusing compressive and panchromatic
(PAN) image, which has shown great potential for snap-
shot hyperspectral imaging in practice. In this paper, we in-
troduce a novel DCCHI reconstruction network, intra-inter
similarity exploiting Transformer (In2SET). Our key insight
is to make full use of the PAN image to assist the recon-
struction. To this end, we propose to use the intra-similarity
within the PAN image as a proxy for approximating the
intra-similarity in the original HSI, thereby offering an en-
hanced content prior for more accurate HSI reconstruction.
Furthermore, we propose to use the inter-similarity to align
the features between HSI and PAN images, thereby main-
taining semantic consistency between the two modalities
during the reconstruction process. By integrating In2SET
into a PAN-guided deep unrolling (PGDU) framework, our
method substantially enhances the spatial-spectral ﬁdelity
and detail of the reconstructed images, providing a more
comprehensive and accurate depiction of the scene. Ex-
periments conducted on both real and simulated datasets
demonstrate that our approach consistently outperforms ex-
isting state-of-the-art methods in terms of reconstruction
quality and computational complexity. The code is avail-
able athttps://github.com/2JONAS/In2SET .
1. Introduction
Hyperspectral imaging collects and processes scene infor-
mation by dividing the whole spectrum into tens or hun-
dreds of bands [ 15,29,57]. Owing to the capability of de-
tailed scene representation, this technique has been widely
adopted in many ﬁelds, e.g., medical diagnosis, health
care, remote sensing [ 5,11,28,37], and various com-
puter vision tasks, e.g., recognition, classiﬁcation, segmen-
tation [ 26,35,49].
The hyperspectral image (HSI) inherently possesses a
*Corresponding Author: Lizhi Wang (wanglizhi@bit.edu.cn)
(a) In tra -Similarity
PAN image
(b) Inter -Similarity
PAN image Reconstructed HSIReconstructed HSI
Shallow Semantic FeaturesSpatial Attention Maps
1
2
1
2Extract
ExtractApply
ExtractWeak Consistency
Strong Consistency1
2Scorer
Scorer
Figure 1. Illustration of the proposed In2SET method for hyper-
spectral image reconstruction. (a) Intra-Similarity: extraction and
application of spatial attention maps from the PAN image to en-
hance the spatial resolution of the reconstructed HSI. (b) Inter-
Similarity: utilization of semantic features from the HSI and PAN
image, scored by their consistency, to inform and reﬁne the recon-
struction of HSI.
three-dimensional (3D) structure, comprising two dimen-
sions of spatial information and one dimension of spec-
tral information. Consequently, existing 2D imaging sen-
sors lack the capability to directly capture this complete
3D signal. With the rapid progress in computational pho-
tography, snapshot spectral imagers have been developed
to enable hyperspectral imaging in dynamic scenes [ 12,
25,53]. The coded aperture snapshot spectral imaging
(CASSI) [ 2,19,38], serving as a representative prototype
of snapshot spectral imagers, captures one compressive im-
age that encodes the spectral information by leveraging a
decorated optical system. Inspired by multi-modal image
fusion techniques, dual-camera compressive hyperspectral
imaging (DCCHI) [ 39,40,43] upgrades the original CASSI
by adding a panchromatic (PAN) camera to supply spatial
information lost in CASSI. As a result, DCCHI can achieve
more accurate reconstructions of HSI by fusing the com-
pressive and PAN images compared to CASSI, which has
shown great potential for snapshot hyperspectral imaging
in practice.
The signiﬁcant challenge of DCCHI lies in the HSI re-
construction method, where the key factor that determines
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
24881
the reconstruction performance is the ability to model HSI
priors. Early methods mainly rely on general image priors
such as smoothness[ 4,39,51], sparsity[ 17,41,52], low-
rank[ 18,30,56], and non-local similarity[ 20,42], which
ignore the continuous and complete spatial characteristics
of PAN images, a feature that could provide stronger image
priors.
In recent years, some PAN-guided methods like
ANSR [ 42,45], PFusion [ 21], and PIDS [ 13] have utilized
PAN image as prior information to guide the reconstruc-
tion of HSI. While these approaches have demonstrated
merit, their performance is limited by the following two
reasons: ﬁrstly, current PAN-guided methods rely on hand-
crafted prior constraints, which lack a precise depiction of
the intrinsic structures of HSI and often require manual
parameter tuning. Consequently, when dealing with com-
plex scenes, these methods tend to produce degraded recon-
struction quality. Secondly, current PAN-guided methods
typically model inter-modality correlations between PAN
and HSI images from a single perspective, such as using
edge information from the PAN images to constrain the
HSI edges [ 13] or employing PAN image content to en-
rich HSI spatial detail [ 21]. However, HSI exhibits multi-
faceted consistency with PAN images across various aspects
such as structure, texture, luminance, contrast, and seman-
tic content. Relying on one constraint may not fully ex-
ploit the comprehensive spatial-semantic guidance offered
by the PAN image. In summary, accurately leveraging PAN
image-guided prior modeling for HSI reconstruction is the
key to further improving reconstruction quality.
In this paper, we propose an intra-inter similarity ex-
ploiting Transformer (In2SET) for DCCHI. The proposed
In2SET module is integrated into a PAN-guided deep un-
rolling (PGDU) framework for reconstruction. The PGDU
makes use of a multi-scale feature pyramid extracted from
the PAN image as guidance. This feature pyramid provides
hierarchical representations of the PAN at different resolu-
tions, allowing the network to capture both global context
and ﬁne details, resulting in improved spatial ﬁdelity and
detail in the reconstructed images.
Our key insight is to make full use of the PAN image to
assist the reconstruction. The In2SET is based on two ob-
servations: (1) given the large redundancy in both the spa-
tial and the spectral dimensions, intra-similarity exist ubiq-
uitously within HSI. Since the original HSI is not available
in practice, intra-similarity needs to be computed based on
intermediate reconstruction results. However, relying on
these intermediate results for intra-similarity calculations
introduces unreliability. Fortunately, the PAN image can
be regarded as the integral of all spectral bands and pro-
vides a natural reference for estimating the intra-similarity
in the original HSI. Thus, we propose to employ the intra-
similarity in the PAN image as an approximation proxy forthe intra-similarity in the original HSI, which provides an
enhanced content prior for HSI reconstruction. (2) Since
the HSI and the PAN image describe the same scenes, inter-
similarity should exist between the HSI and the PAN image,
considering semantic information. Inter-similarity, serving
as a scorer, assigns higher weights to features in areas of
the HSI that are more similar to the corresponding areas in
the PAN image, thereby improving feature representation
in those areas and minimizing uncertainty and risk in the
reconstruction process. This capability enables the recon-
struction of the highly ill-posed inverse problem in a more
conﬁdently supervised manner, providing new contextual
information for HSI reconstruction.
In a nutshell, this work integrates intra-similarity and
inter-similarity with the advanced Transformer mechanism,
speciﬁcally for DCCHI, resulting in high-ﬁdelity HSI re-
construction. The main contributions of this work are as
follows:
• We propose the PGDU, which employs the feature
pyramid from the PAN image to guide the reconstruc-
tion of the HSI.
• We propose the In2SET, which employs a novel at-
tention mechanism, speciﬁcally designed to concur-
rently capture both intra-similarity and inter-similarity
between spectral and PAN images.
• Experiments on real and simulated data demonstrate
that our method consistently outperforms state-of-the-
art approaches.
2. Related Work
This section provides an overview of the advancements in
HSI reconstruction, focusing on three pivotal areas: hand-
crafted HSI priors modeling, data-driven HSI priors model-
ing, and PAN-guided HSI reconstruction.
2.1. Hand­Crafted HSI Priors Modeling
Early HSI reconstruction relied heavily on hand-crafted pri-
ors. Total Variation (TV) regularization [ 39,54,55] utilized
the piecewise smoothness of images to constrain TV , albeit
at the risk of suppressing important high-frequency struc-
tural details. Sparse representation techniques [ 3,41,52]
have been employed, focusing on constructing dictionaries
to model image sparsity. Low-rank priors [ 30,47,56] have
been applied, designed to capture contextual information in
high-dimensional data. However, these hand-crafted meth-
ods often required extensive manual parameter tuning and
struggled to represent complex data scenarios effectively.
2.2. Data­Driven HSI Priors Modeling
The advent of deep learning has revolutionized the ﬁeld of
HSI reconstruction. Models such as HyperReconNet [ 44],
λ-net [ 33], TSA-Net [ 31], DNU [ 46], DGSMP [ 23] and
24882
PnP-DIP-HSI [ 32] have markedly improved reconstruction
quality by modeling HSI data prior knowledge. Further-
more, the introduction of Transformer-based methods like
MST [ 7], CST [ 6], DAU [ 9], RDLUF [ 16], and PADUT [ 27]
has led to even more signiﬁcant advances in reconstruction
performance. Beneﬁting from the attention mechanism,
Transformers can efﬁciently model both spatial and spec-
tral similarities within HSI data, enabling a more ﬂexible
handling of input data and providing a richer feature repre-
sentation for HSI reconstruction.
2.3. PAN­Guided HSI Reconstruction
The integration of PAN guidance in HSI reconstruction rep-
resents a signiﬁcant stride in addressing the inherent lim-
itations of traditional CASSI. ANSR[ 42,45] proposed an
adaptive non-local sparse representation model, guided by
the PAN image. PFusion[ 21] leveraged RGB measurements
for spatial coefﬁcient estimation and CASSI measurements
for spectral basis acquisition. It features a patch process-
ing strategy to enhance the spectral low-rank property, op-
timizing the model efﬁciently without iterative methods or
a spectral sensing matrix. PIDS[ 13] utilized the RGB mea-
surement as a prior image to enhance semantic correspon-
dence between HSI and the PAN image. These methods
have marked signiﬁcant advancements in the ﬁeld. How-
ever, they often require manual parameter tuning and fail to
leverage the rich spatial context and detailed features em-
bedded within the PAN image, suggesting potential areas
for further improvements in reconstruction.
Our motivation is to utilize the spatial semantic infor-
mation provided by the PAN image to improve the HSI re-
construction result in DCCHI. We aim to develop a method
that combines the strengths of PAN guidance to model the
intra-inter similarity for more accurate reconstruction.
3. PGDU
3.1. DCCHI Forward Model
DCCHI consists of a beam splitter, a CASSI branch, and
a PAN camera branch, as shown in Figure 2. The beam
splitter divides the incident light from the scene into two
equal-intensity parts. One part of the light is captured by
CASSI, which undergoes spatial modulation and spectral
modulation to form the CASSI measurement. The other part
of the light is directly captured by the PAN camera.
LetX ∈RH×W×Crepresent the spectral image of the
target scene, where H,W, andCare the height, width,
and number of spectral bands, respectively. The spatial and
spectral modulation in CASSI is given by
Yc=C/summationdisplay
c=1shift(φ⊙X[:,:,c])+Vc, (1)
Scene
PAN 
MeasurementCASSI
MeasurementObjective
LensCoded
ApertureRelay
LensPrism
Objective
LensSensor
Sensor
CASSI
Pan Camera
Figure 2. The dual-camera compressive hyperspectral imaging
system.
whereφis the transmission intensity of the coded aperture,
represented as φ∈RH×W,shift(·)indicates the spectral
modulation caused by the dispersion prism, Ycis the im-
age captured by the CASSI branch, and Vcis regarded as
Gaussian noise in CASSI.
To simplify to a matrix-vector form, the imaging model
of the CASSI branch is represented as
yc=Φcx+vc, (2)
whereΦcdenotes the imaging model of the CASSI branch
sensing matrix, ycis the vector form of Yc,xis the vector
form ofX, andvcis the vector form of Vc.
Correspondingly, the imaging model of the PAN camera
branch is described by
yp=Φpx+vp, (3)
whereΦpdenotes the imaging model of the PAN camera
branch sensing matrix, determined by the spectral response
of the sensor, ycis the vector form of PAN measurement,
andvpis regarded as Gaussian noise in the PAN measure-
ment. To connect the imaging models to the subsequent
discussion, we deﬁne y,Φ, andvas
y=/bracketleftbiggyc
yp/bracketrightbigg
,Φ=/bracketleftbiggΦc
Φp/bracketrightbigg
,v=/bracketleftbiggvc
vp/bracketrightbigg
. (4)
Using the above deﬁnitions, the imaging model can be
expressed as
y=Φx+v, (5)
whereΦis a sensing matrix in RM×Nthat encodes the sys-
tem mapping from the original scene xto the measurements
y.y∈RM×1,x∈RN×1, andΦ∈RM×N, The dimen-
sionsMandNare deﬁned as follows: Mis calculated as
H(W+d(C−1))+HW andNis calculated as HWC ,
wheredrepresents the dispersion step size. It is important
to note that in the context of HSI reconstruction, Mis typi-
cally much smaller than N, highlighting that the HSI recon-
struction task in DCCHI is an ill-posed problem.
24883
InitialNet
 Guided Feature Extractor (GFE)
Conv3×3
Conv3×3Down
Conv3×3
Conv3×3DownConv7×7
Conv3×3
Guided Feature Pyramid
CG Dkz1kz
Stage 1k1kx1k 1k



CG D0z1z
Stage 11x11(,)μσ
(,,)pcyy Φ
pyCG D1KzKz
StageKKxKK
Figure 3. Overview of PGDU. The InitialNet initiates the process with compressive measurements and sensing matrix, followed by a
series of stages each containing a conjugate gradient (CG) block and an In2SET denoiser.
3.2. Reconstruction Framework
Compared to learn a brute-force mapping between HSI x
and measurement y, model-based methods have potential in
compressive imaging due to fully considering the imaging
model. Model-based methods usually fomulate HSI recon-
struction as a Bayesian inference challenge, solving Eq. ( 5)
under a uniﬁed Maximum A Posteriori (MAP) framework.
Mathematically, the optimization problem for HSI recon-
struction could be expressed as
ˆx= argmin
x1
2∥y−Φx∥2
2+ηJ(x), (6)
whereJ(x)represents regularization. The parameter ηis a
regularization coefﬁcient, controlling the trade-off between
data ﬁdelity and the regularization term imposed by J(x).
To solve the optimization problem shown in Eq. ( 6), we
design the PGDU based on the physical structure of DC-
CHI, as shown in Figure 3. Utilizing the half quadratic split-
ting (HQS) method, we decompose the optimization prob-
lem into two sub-problems: the data ﬁdelity term, as shown
in Eq. ( 7); and the prior term, as shown in Eq. ( 8).
xk+1= argmin
x∥y−Φx∥2
2+µk+1/vextenddouble/vextenddoublezk−x/vextenddouble/vextenddouble2
2,(7)
zk+1= argmin
z1
2µk+1/vextenddouble/vextenddoublez−xk+1/vextenddouble/vextenddouble2
2+ηk+1J(z),(8)
the superscript kindicates the stage index number, and zis
an auxiliary variable introduced to facilitate the optimiza-
tion. The parameter µk+1is a relaxation parameter that gov-
erns the alignment between the current estimate xand the
auxiliary variable z. The parameter ηk+1is a stage-speciﬁc
parameter for the regularization coefﬁcient η.
The data ﬁdelity term can be directly calculated in closed
form, as demonstrated in Eq. ( 9).
xk+1=/parenleftBig
ΦTΦ+µI/parenrightBig−1/parenleftBig
ΦTy+µk+1zk/parenrightBig
,(9)in the context of HSI reconstruction in CASSI, the matrix
ΦTΦis strictly diagonal. However, with the addition of the
PAN camera branch in DCCHI, the matrix ΦTΦbecomes
only diagonally dominant, and thus a closed-form solution
cannot be explicitly calculated.
Given the non-diagonal form of ΦTΦ, we employ the
conjugate gradient (CG) method [ 34] for the efﬁcient res-
olution of the data ﬁdelity term. The CG method is well
suited for this problem due to its effectiveness in solving
large-scale, sparse linear systems.
To fully explore the guidance of the PAN image ypon
prior term, we design a PAN-guided Gaussian denoiser to
estimatezk+1. This is formulated as Eq. ( 10):
zk+1= Denoiser k+1/parenleftbig
xk+1,GFE(yp),σk+1/parenrightbig
,(10)
whereGFE(yp)serves as structural guidance for the de-
noising, which incorporates multi-scale spatial details from
the high-resolution ypdata into the denoiser, aiming to re-
ﬁne the ﬁdelity of the reconstructed HSI by adding contex-
tual information. The Gaussian noise level of the denoiser,
denoted as σk+1, is equal to/radicalbig
ηk+1/µk+1. Theµk+1and
σk+1are the elements of vectors µandσ, which are in-
ferred by the InitialNet of PGDU.
The feature extraction from ypis delineated as follows:
within the unrolling framework, the feature representation
ofypis extracted singularly and iteratively shared across
multiple stages. A stack of convolutional blocks is utilized
to harvest feature maps at varying resolutions, assembling a
guided feature pyramid [G1,G2,G3]. The guided feature
pyramid is integral to the subsequent denoising stage, aid-
ing in the preservation of intricate spatial details during the
denoising.
24884
Conv3×3
In2ABConv1×1
ConcatConv3×3
In2AB
Down
In2AB
Down
In2AB
Up
In2AB
Up
Conv1×1
Concat
(a) In2SET+
Inter Similarity
Intra Similarity
MHA-C
MHA-SCNPL NPL
Norm
FFN(b) In2AB
R S × Reshape Softmax Matrix ProductElement-wise 
ProductCConcatenateCRW
RR
R×
× RS(d) MHA-C/SMHA-C
MHA-S
 (e) CRW
CosineLinear
(c) NPL
PartitionNorm
Linear1kx1kz11(, )kkx
1G
2G1G
2G
3GX
G,,11 1QKV
,22QK2V1X
2X2QinterX
3K3V
2QintraX
intraX






1Q
2Q




1K
2K
1V
2VˆBCN
ˆBNC
ˆBNC
ˆBCN
ˆBCN
ˆBNCˆˆBCC
BNN
1X
2XoutX
Figure 4. Diagram of In2SET architecture. (a) The U-shaped In2SET structure. (b) In2AB, consisting of two normalization layers, an
intra-similarity attention module, an inter-similarity attention module, and an FFN layer. (c) The components of NPL. (d) The multi-
head self-attention in channel (MHA-C) and multi-head cross-attention in spatial (MHA-S). (e) The cosine similarity reweighting (CRW)
mechanism.
4. In2SET
4.1. Overall Architecture
In this section, we introduce the denoiser In2SET within
the PGDU framework. As shown in Figure 4(a), the pro-
posed In2SET adopts a U-shaped architecture [ 36], involv-
ing the intra-inter attention block (In2AB) as the founda-
tional units. This architecture employs multi-scale feature
extraction coupled with skip connections between the en-
coder and decoder, enhancing the efﬁciency in information
processing and feature extraction. To be speciﬁc, the de-
noiser initializes feature map Xfrom the spectral image
xk+1and the noise level σk+1, passing through a convo-
lutional layer with a kernel size of 3×3. The input fed into
each In2AB consists of guided feature Gi, feature maps
extracted from the PAN image, and HSI feature X. The
denoised image zk+1is the sum of xk+1and the output
of the last In2AB, calculated by a convolutional layer with
a kernel size of 3×3. The In2AB structure is shown in
Figure 4(b). Our In2AB module includes two normaliza-
tion layers, one intra-similarity attention module, one inter-
similarity attention module, and one feed-forward fully con-
nected (FFN) layer. In the following, we introduce the detail
of the intra-similarity and inter-similarity.
4.2. Intra­Similarity Attention
Our intra-similarity attention module is grounded on two
key principles: ﬁrstly, for exploring intra-spatial similarity,the comprehensive and continuous spatial data provided by
the PAN image are invaluable. This allows the spatial sim-
ilarity in the PAN image to serve as an effective proxy for
approximating the spatial similarity of the HSI. Secondly,
in addressing intra-spectral similarity, analyzing the recon-
structed HSI becomes signiﬁcantly more effective, given the
PAN image’s deﬁciency in spectral color information.
As shown in Figure 4(b), our intra-similarity attention
module includes two branches: multi-head self-attention in
channel (MHA-C) and multi-head cross-attention in spatial
(MHA-S), representing the exploration of spectral and spa-
tial self-similarity, respectively. The MHA-C is based on
the work in MST [ 7], which has been demonstrated to ef-
fectively explore spectral self-similarity.
The input features XandGhave dimensions RH×W×C
andRH×W×ˆC, respectively, where ˆC=C
2. After normal-
ization and partitioning, the dimensions become RB×N×C
andRB×N×ˆC. In the ﬁrst and last layers of HSAB within
In2SET,B=HW
M2andN=M2. For the remain-
ing layers, dimensions are recalibrated to B=M2and
N=HW
M2. Following linear projection as shown in Eq. ( 11)
and Eq. ( 12).
Q1,K1,V1,V2=LQ1,K1,V1,V2(X), (11)
Q2,K2=LQ2,K2(G), (12)
whereXis projected to Q1,K1,V1∈RB×ˆC×Nand
24885
Method GFLOPsSceneAverage01 02 03 04 05 06 07 08 09 10
PFusion-RGB [ 21] -40.09 38.84 38.70 46.65 32.07 37.12 39.74 36.75 34.52 35.53 38.00
0.979 0.968 0.966 0.936 0.980 0.980 0.964 0.965 0.931 0.979 0.965
PIDS-RGB [ 13] -42.09 40.08 41.50 48.55 40.05 39.00 36.63 37.02 38.82 38.64 40.24
0.983 0.949 0.968 0.989 0.982 0.974 0.940 0.948 0.953 0.980 0.967
TV-DC [ 39] -35.81 33.22 31.07 40.11 33.32 34.62 31.09 32.31 29.36 33.84 33.47
0.947 0.885 0.879 0.947 0.944 0.943 0.885 0.916 0.862 0.953 0.910
PIDS-DC [ 13] -39.82 37.07 37.72 46.78 37.45 37.74 32.90 31.66 34.35 38.58 37.41
0.977 0.921 0.950 0.978 0.973 0.963 0.896 0.915 0.902 0.972 0.945
BiSRNet-DC [ 10] 1.3335.02 34.13 31.50 35.88 33.70 35.58 32.31 32.73 31.37 34.48 33.67
0.945 0.914 0.883 0.895 0.935 0.925 0.900 0.903 0.899 0.936 0.914
CST-DC [ 6] 25.4037.44 38.91 36.79 42.27 36.57 38.91 36.87 35.91 35.87 37.93 37.75
0.975 0.978 0.969 0.983 0.982 0.984 0.967 0.980 0.973 0.990 0.978
HDNet-DC [ 22] 144.3138.06 39.79 38.21 42.79 37.22 39.26 37.41 36.51 36.64 37.52 38.34
0.976 0.982 0.973 0.983 0.983 0.986 0.968 0.982 0.976 0.988 0.980
MST-DC [ 7] 25.7738.38 40.45 37.63 42.88 37.72 39.66 37.56 37.40 37.86 38.10 38.76
0.976 0.976 0.966 0.975 0.978 0.976 0.965 0.971 0.971 0.981 0.974
MST++-DC [ 8] 17.6938.38 40.47 37.70 43.88 37.75 39.42 37.48 37.38 38.82 39.04 39.03
0.977 0.980 0.968 0.984 0.983 0.985 0.964 0.982 0.980 0.989 0.980
DAUHST-DC-2stg [ 9] 16.7940.78 43.13 41.73 47.09 39.84 40.90 39.75 38.98 41.29 40.04 40.22
0.983 0.987 0.980 0.990 0.987 0.986 0.976 0.981 0.983 0.988 0.983
DAUHST-DC-3stg [ 9] 24.7040.22 43.52 41.74 47.07 38.81 40.16 39.86 38.21 40.63 39.32 40.95
0.983 0.989 0.981 0.993 0.985 0.987 0.978 0.981 0.983 0.990 0.985
DAUHST-DC-5stg [ 9] 40.5140.74 44.00 41.58 46.84 39.66 40.89 40.21 38.72 39.98 40.10 41.27
0.984 0.989 0.981 0.991 0.986 0.987 0.979 0.983 0.982 0.989 0.916
DAUHST-DC-9stg [ 9] 72.1141.59 45.19 43.47 48.92 40.27 41.17 40.73 40.11 43.50 41.33 42.62
0.985 0.991 0.984 0.993 0.988 0.988 0.979 0.986 0.988 0.990 0.987
40.33 42.30 40.34 47.24 39.42 40.61 39.46 38.42 40.37 39.96 40.84In2SET-2stg (Ours) 14.350.983 0.985 0.977 0.991 0.986 0.986 0.975 0.979 0.981 0.988 0.983
40.78 43.13 41.73 47.09 39.84 40.90 39.75 38.98 41.29 40.04 41.35In2SET-3stg (Ours) 20.790.983 0.987 0.980 0.990 0.987 0.986 0.976 0.981 0.983 0.988 0.984
41.13 44.43 42.74 47.29 40.33 40.95 40.49 39.15 42.07 39.44 41.80In2SET-5stg (Ours) 33.660.985 0.990 0.983 0.993 0.988 0.987 0.979 0.982 0.985 0.987 0.985
42.56 46.42 44.55 50.63 42.01 42.49 41.59 40.53 43.83 42.33 43.69In2SET-9stg (Ours) 59.400.989 0.994 0.986 0.996 0.992 0.991 0.983 0.989 0.990 0.994 0.990
Table 1. Comparison of In2SET with SOTA DCCHI methods across 10 simulated scenes, including FLOPS, PSNR (upper entry), and
SSIM (lower entry). “Method-RGB” indicates the use of RGB observations, while “Method-DC” denotes grayscale observations in PAN
images. It is noted that “Method-DC” is modiﬁed from other HSI reconstruction method for the DCCHI reconstruction task.
V2∈RB×N×ˆC.Gis projected to Q2,K2∈RB×N×ˆC.
The computations of MHA-C and MHA-S can be repre-
sented by Eq. ( 13) and Eq. ( 14),
X1= Softmax/parenleftbiggQ1KT
1√dh1+P1/parenrightbigg
V1, (13)
X2= Softmax/parenleftbiggQ2KT
2√dh2+P2/parenrightbigg
V2, (14)
where√dh1and√dh2are scalars as deﬁned in [ 9],P1∈
RB×ˆC×ˆC, andP2∈RB×N×Ndenote learnable positional
encodings.
The output of intra-attention Xintra can be obtained by
Eq. ( 15).
Xintra= Concat( X1,X2). (15)
4.3. Inter­Similarity Attention
We establish our inter-similarity attention module based on
one principle: the shallow semantic features of the PAN im-
age and the corresponding HSI area are similar, includingconsistency in texture, shape features, and low-level image
attributes (brightness and contrast). The credibility of the
reconstructed area is directly proportional to this consis-
tency. Inter-similarity attention explores cross-modal simi-
larity using cosine similarity reweighting (CRW).
After exploiting intra-similarity, the output of the intra-
similarity attention module Xintra is projected to obtain
K3andV3,
K3,V3=LK3,V3(Xintra), (16)
whereK3∈RB×N×ˆCandV3∈RB×N×C.
Then, the computations of CRW can be represented as
follows:
Xinter=Q2·K3
∥Q2∥∥K3∥⊙V3, (17)
where the cosine similarity computed along the last dimen-
sion between Q2andK3quantiﬁes the feature alignment
in corresponding areas. A higher similarity score reﬂects
a stronger feature correspondence, which, in turn, informs
the enhanced weighting of V3, improving the ﬁdelity of the
region being reconstructed.
24886
5. Experiments
In this section, we begin by introducing detailing the net-
work training process. Subsequently, we evaluate the per-
formance of In2SET on both simulation and real-world DC-
CHI datasets. Then, we validate the effectiveness of the
intra-similarity block using statistical simulation data. Fi-
nally, we perform an ablation study to demonstrate the ef-
fectiveness of our proposed method.
Simulation Dataset. Two distinct datasets are used:
CA VE [ 50] and KAIST [ 14]. The CA VE dataset is made up
of 32 hyperspectral images with 512×512spatial resolu-
tion. The KAIST dataset contains 30 hyperspectral images,
each with a larger spatial dimension of 2704×3376 . The
CA VE dataset is used as the training data, while a subset of
10 scenes crop from the KAIST dataset is used for the eval-
uation phase, in accordance with the protocols established
in references [ 7,9,22,23,31].
Implementation . Our method is implemented with Py-
torch and trained with Adam [ 24] optimizer for 300 epochs.
During training, the learning rate is 4×10−4using the co-
sine annealing scheduler, and the loss function for network
training is L1loss. We randomly extract patches from 3D
HSI cubes to serve as training samples. The dimensions of
these patches are 256×256×28for the simulation experi-
ment and 350×260×26for the real-world experiment. In
simulation imaging model, we conﬁgure a dispersion shift
stepdof 2, directing the dispersion to the right. In real-
word imaging model, the dispersion shift step d is set to 1,
with the dispersion oriented upwards.
5.1. Simulation Data Results
The reconstruction quality of hyperspectral images are as-
sessed using peak signal-to-noise ratio (PSNR) and struc-
tural similarity index (SSIM) [ 48] metrics.
The Table 1clearly shows that In2SET excels in all test
scenarios, particularly in In2SET-9stg, achieving the best
performance across all scenes with an average PSNR of
43.69 dB and a SSIM of 0.990. This signiﬁcantly sur-
passes model optimization methods like TV-DC and PIDS-
DC. Compared to deep learning-based methods, such as
various stages of DAUHST-DC [ 9], In2SET demonstrates
remarkable superiority. In2SET-3stg achieves performance
comparable to that of DAUHST-DC-5stg [ 9], but with only
51.3% of the cost, amounting to 20.78 GFLOPs compared
to 40.50 GFLOPs.
To facilitate a direct visual quality assessment, we ana-
lyzed the distinction between Ground Truth (GT) and the
reconstructions produced by various open-source methods.
Figure 5presents reconstruction results for selected spectral
bands. These results are generated from two model-based
approaches, TV-DC [ 39] and PIDS-DC [ 13], ﬁve end-to-
end network algorithms, BiSRNet-DC [ 10], CST-DC [ 6],
HDNet-DC [ 22], MST-DC [ 7], MST++-DC [ 8], as well asRMSE Correlation PSNR(dB)
CA VE 0.034 0.999 36.28
KAIST 0.025 0.996 35.74
ICVL 0.076 0.997 31.10
Table 2. Comparison of correlation maps between HSI and PAN
image, including average Root Mean Squared Error (RMSE), av-
erage correlation, and PSNR.
Baseline CRW MHA-C MHA-S PSNR(dB) SSIM GFLOPs
✓ 37.23 0.944 9.87
✓ ✓ 40.05 0.980 11.19
✓ ✓ ✓ 40.46 0.982 14.06
✓ ✓ ✓ ✓ 40.84 0.983 14.35
Table 3. Break-down ablation study on individual components of
the proposed method.
Methods CG-1 CG-2 CG-5 CG-10
PSNR(dB) 40.42 40.66 40.84 41.05
SSIM 0.982 0.983 0.983 0.984
FPS 26.45 22.79 19.16 13.99
Table 4. Ablation study on iterative performance of conjugate gra-
dient descent for data item.
from two deep unfolding methods, namely DAUHST-DC-
9stg [ 9] and our In2SET-9stg. The comparison underlines
the exceptional ability of our In2SET approach to maintain
spatial and spectral ﬁdelity.
We suggest that the intra similarity in PAN image
can effectively approximates that in HSI. To support this,
we experiment with three public datasets: CA VE [ 50],
KAIST [ 14] and ICVL [ 1]. We compute spatial correla-
tion maps for each scene, comparing the correlation map of
the HSI with that of the corresponding PAN image. The
results of this comparison are displayed in Table 2, indi-
cating a high degree of similarity between the intra-spatial
correlations in PAN images and HSI. More visual results
are presented in the supplementary materials.
5.2. Real Data Results
In this research, we used a real-world DCCHI measurement
Ninja , taken from publicly available data as detailed in ref-
erence [ 42]. Figure 6illustrates the reconstruction results
for four spectral bands in this scene, using various DCCHI
reconstruction algorithms. The comparison highlights the
superior image restoration quality of our model over other
methods, validating its effectiveness and reliability in real-
world applications.
5.3. Ablation Study
To verify the effectiveness of our proposed method, we con-
ducted ablation studies for the In2SET method. All evalua-
tions are conducted on simulated datasets.
Break-down Ablation. The break-down study, as de-
lineated in Table 3, offers a detailed examination of how
each component within our proposed method inﬂuences re-
construction performance. Starting from a baseline derived
by removing all ablated components from In2SET-2stg, the
24887
DCCHI Measurement RGB
Spectral Density Curves GTIn2SET-9stg 
(Ours)DAUHST-
DC-9stgMST++-DC MST-DC HDNet-DC CST-DC BiSRNet-DC PIDS-DC TV-DC450 475 500 525 550 575 600 625 650
Wavelength(nm)0.050.100.150.200.25DensityGround Truth
In2SET-9stg,corr:0.9994
DAUHST-DC-9stg,corr:0.9983
MST++-DC,corr:0.9947
MST-DC,corr:0.9895
HDNet-DC,corr:0.9988
CST-DC,corr:0.9994
BiSRNet-DC,corr:0.9541
PIDS-DC,corr:0.8228
TV-DC,corr:0.9552
Figure 5. Comparative reconstruction results of different reconstruction methods for Scene 3 from the KAIST dataset at spectral bands
476.5nm, 536.5nm, 584.5nm, and 625.0nm. The spectral density curves are plotted from the blue region in the colorchecker.
481.5nm 544.0nm 594.5nm 614.5nm
TV-DC
PIDS-DC
DAUHST
-DC-2
In2SET-2
Figure 6. Visualization of reconstruction performance across dif-
ferent spectral bands on real data. The spectral bands selected for
comparison are 481.5nm, 544.0nm, 594.5nm, and 614.5nm.
model achieves a PSNR of 37.23 dB. The addition of the
CRW component leads to a signiﬁcant increase in PSNR
by +2.82 dB, albeit at an increased computational cost of
+1.32 GFLOPs. The subsequent integration of MHA-C fur-
ther elevates the PSNR by +0.41 dB. The ﬁnal inclusion of
MHA-S brings an additional PSNR gain of +0.38 dB. These
step-by-step enhancements highlight the critical role each
component plays in enhancing the overall capability of the
model for HSI reconstruction, demonstrating a progressive
improvement in performance with each added component.Ablation Study of CG Iterations. Table 4presents a
comparative analysis of PSNR, SSIM, and Frames Per Sec-
ond (FPS) metrics across varying numbers of CG iterations
for HSI reconstruction. The CG-1, which conducts a single
iteration, essentially operates as a vanilla gradient descent.
These experiments are conducted on a system equipped
with a TITAN Xp GPU (12GB) and an Intel (R) Xeon (R)
Platinum 8358P CPU @ 2.60GHz.
There is a clear trend of improvement in PSNR as the
number of CG iterations increases from 1 to 10, with val-
ues escalating from 40.42 dB to 41.05 dB. However, an in-
crease in iterations from CG-1 to CG-10 leads to a decrease
in FPS from 26.45 dB to 13.99 dB, indicating a trade-off
between reconstruction quality and computational speed. A
notable observation is that CG-5 offers an optimal balance
between time efﬁciency and performance, making it the pre-
ferred choice for our In2SET network.
6. Conclusion
In this paper, we propose the In2SET for DCCHI re-
construction. Our method maximizes PAN image utility
for HSI reconstruction by: (1) approximating HSI intra-
similarity using PAN image, addressing the unreliability of
direct computation from intermediate results. (2) Leverag-
ing inter-similarity between HSI and PAN images to accu-
rately reconstruct regions, providing cues for uncertain ar-
eas. Integrating In2SET into a PGDU framework allowed
us to substantially enhance the spatial-spectral ﬁdelity and
detail of reconstructed images. Experiments on real and
simulated datasets show that our method consistently out-
performs the state-of-the-art while maintaining lower com-
putational complexity.
Acknowledgments This work is partially supported by
the National Natural Science Foundation of China under
grants 62322204, 62131003, 62072038 and 62302041.
24888
References
[1] Boaz Arad and Ohad Ben-Shahar. Sparse recovery of hyper-
spectral signal from natural rgb images. In European Con-
ference on Computer Vision , pages 19–34, 2016. 7
[2] Gonzalo R Arce, David J Brady, Lawrence Carin, Henry
Arguello, and David S Kittle. Compressive coded aperture
spectral imaging: An introduction. IEEE Signal Processing
Magazine , 31(1):105–115, 2013. 1
[3] Henry Arguello, Hoover Rueda, Yuehao Wu, Dennis W
Prather, and Gonzalo R Arce. Higher-order computational
model for coded aperture spectral imaging. Applied Optics ,
52(10):D12–D21, 2013. 2
[4] Jos ´e M Bioucas-Dias and M ´ario AT Figueiredo. A new twist:
Two-step iterative shrinkage/thresholding algorithms for im-
age restoration. IEEE Transactions on Image Processing , 16
(12):2992–3004, 2007. 2
[5] Jos ´e M Bioucas-Dias, Antonio Plaza, Gustavo Camps-Valls,
Paul Scheunders, Nasser Nasrabadi, and Jocelyn Chanussot.
Hyperspectral remote sensing data analysis and future chal-
lenges. IEEE Geoscience and Remote Sensing Magazine , 1
(2):6–36, 2013. 1
[6] Yuanhao Cai, Jing Lin, Xiaowan Hu, Haoqian Wang, Xin
Yuan, Yulun Zhang, Radu Timofte, and Luc Van Gool.
Coarse-to-ﬁne sparse transformer for hyperspectral image re-
construction. In European Conference on Computer Vision ,
pages 686–704, 2022. 3,6,7
[7] Yuanhao Cai, Jing Lin, Xiaowan Hu, Haoqian Wang, Xin
Yuan, Yulun Zhang, Radu Timofte, and Luc Van Gool.
Mask-guided spectral-wise transformer for efﬁcient hyper-
spectral image reconstruction. In IEEE Conference on Com-
puter Vision and Pattern Recognition , pages 17502–17511,
2022. 3,5,6,7
[8] Yuanhao Cai, Jing Lin, Zudi Lin, Haoqian Wang, Yulun
Zhang, Hanspeter Pﬁster, Radu Timofte, and Luc Van Gool.
Mst++: Multi-stage spectral-wise transformer for efﬁcient
spectral reconstruction. In IEEE Conference on Computer
Vision and Pattern Recognition Workshop , pages 745–755,
2022. 6,7
[9] Yuanhao Cai, Jing Lin, Haoqian Wang, Xin Yuan, Henghui
Ding, Yulun Zhang, Radu Timofte, and Luc Van Gool.
Degradation-aware unfolding half-shufﬂe transformer for
spectral compressive imaging. In Advances in Neural Infor-
mation Processing Systems , pages 37749–37761, 2022. 3,6,
7
[10] Yuanhao Cai, Yuxin Zheng, Jing Lin, Xin Yuan, Yulun
Zhang, and Haoqian Wang. Binarized spectral compressive
imaging. In Advances in Neural Information Processing Sys-
tems, pages 38335–38346, 2023. 6,7
[11] Mihaela A Calin, Sorin V Parasca, Dan Savastru, and Dragos
Manea. Hyperspectral imaging in the medical ﬁeld: Present
and future. Applied Spectroscopy Reviews , 49(6):435–447,
2014. 1
[12] Xun Cao, Tao Yue, Xing Lin, Stephen Lin, Xin Yuan, Qiong-
hai Dai, Lawrence Carin, and David J Brady. Computational
snapshot multispectral cameras: Toward dynamic capture ofthe spectral world. IEEE Signal Processing Magazine , 33
(5):95–108, 2016. 1
[13] Yurong Chen, Yaonan Wang, and Hui Zhang. Prior image
guided snapshot compressive spectral imaging. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence , 2023.
2,3,6,7
[14] Inchang Choi, Daniel S Jeon, Giljoo Nam, Diego Gutierrez,
and Min H Kim. High-quality hyperspectral reconstruction
using a spectral prior. ACM Transactions on Graphics , 36
(6):1–13, 2017. 7
[15] Weisheng Dong, Huan Wang, Fangfang Wu, Guangming
Shi, and Xin Li. Deep spatial–spectral representation learn-
ing for hyperspectral image denoising. IEEE Transactions
on Computational Imaging , 5(4):635–648, 2019. 1
[16] Yubo Dong, Dahua Gao, Tian Qiu, Yuyan Li, Minxi Yang,
and Guangming Shi. Residual degradation learning unfold-
ing framework with mixing priors across spectral and spa-
tial for compressive spectral imaging. In IEEE Conference
on Computer Vision and Pattern Recognition , pages 22262–
22271, 2023. 3
[17] M ´ario AT Figueiredo, Robert D Nowak, and Stephen J
Wright. Gradient projection for sparse reconstruction: Ap-
plication to compressed sensing and other inverse problems.
IEEE Journal of Selected Topics in Signal Processing , 1(4):
586–597, 2007. 2
[18] Ying Fu, Yinqiang Zheng, Imari Sato, and Yoichi Sato. Ex-
ploiting spectral-spatial correlation for coded hyperspectral
image restoration. In IEEE Conference on Computer Vision
and Pattern Recognition , pages 3727–3736, 2016. 2
[19] Michael E Gehm, Renu John, David J Brady, Rebecca M
Willett, and Timothy J Schulz. Single-shot compressive
spectral imaging with a dual-disperser architecture. Optics
Express , 15(21):14013–14027, 2007. 1
[20] Wei He, Quanming Yao, Chao Li, Naoto Yokoya, and Qibin
Zhao. Non-local meets global: An integrated paradigm for
hyperspectral denoising. In IEEE Conference on Computer
Vision and Pattern Recognition , pages 6868–6877, 2019. 2
[21] Wei He, Naoto Yokoya, and Xin Yuan. Fast hyperspec-
tral image recovery of dual-camera compressive hyperspec-
tral imaging via non-iterative subspace-based fusion. IEEE
Transactions on Image Processing , 30:7170–7183, 2021. 2,
3,6
[22] Xiaowan Hu, Yuanhao Cai, Jing Lin, Haoqian Wang, Xin
Yuan, Yulun Zhang, Radu Timofte, and Luc Van Gool. Hd-
net: High-resolution dual-domain learning for spectral com-
pressive imaging. In IEEE Conference on Computer Vision
and Pattern Recognition , pages 17542–17551, 2022. 6,7
[23] Tao Huang, Weisheng Dong, Xin Yuan, Jinjian Wu, and
Guangming Shi. Deep gaussian scale mixture prior for spec-
tral compressive imaging. In IEEE Conference on Computer
Vision and Pattern Recognition , pages 16216–16225, 2021.
2,7
[24] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint , 2014. 7
[25] Edmund Y Lam. Computational photography with plenop-
tic camera and light ﬁeld capture: Tutorial. Journal of the
Optical Society of America A , 32(11):2021–2032, 2015. 1
24889
[26] Jun Li, Jos ´e M Bioucas-Dias, and Antonio Plaza. Hyper-
spectral image segmentation using a new bayesian approach
with active learning. IEEE Transactions on Geoscience and
Remote Sensing , 49(10):3947–3960, 2011. 1
[27] Miaoyu Li, Ying Fu, Ji Liu, and Yulun Zhang. Pixel adaptive
deep unfolding transformer for hyperspectral image recon-
struction. In IEEE International Conference on Computer
Vision , pages 12959–12968, 2023. 3
[28] Robert P Lin, Brian R Dennis, Gordon J Hurford, DM Smith,
Alex Zehnder, PR Harvey, David W Curtis, Dave Pankow,
Paul Turin, M Bester, et al. The reuven ramaty high-energy
solar spectroscopic imager (rhessi). In The Reuven Ramaty
High-Energy Solar Spectroscopic Imager (RHESSI): Mis-
sion Description and Early Results , pages 3–32, 2003. 1
[29] Xing Lin, Yebin Liu, Jiamin Wu, and Qionghai Dai. Spatial-
spectral encoded compressive hyperspectral imaging. ACM
Transactions on Graphics , 33(6):1–11, 2014. 1
[30] Yang Liu, Xin Yuan, Jinli Suo, David Brady, and Qionghai
Dai. Rank minimization for snapshot compressive imaging.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence , 41(12):2990–3006, 2018. 2
[31] Ziyi Meng, Jiawei Ma, and Xin Yuan. End-to-end low
cost compressive spectral imaging with spatial-spectral self-
attention. In European Conference on Computer Vision ,
pages 187–204, 2020. 2,7
[32] Ziyi Meng, Zhenming Yu, Kun Xu, and Xin Yuan. Self-
supervised neural networks for spectral snapshot compres-
sive imaging. In IEEE International Conference on Com-
puter Vision , pages 2622–2631, 2021. 3
[33] Xin Miao, Xin Yuan, Yunchen Pu, and Vassilis Athitsos.
Lambda-net: Reconstruct hyperspectral images from a snap-
shot measurement. In IEEE International Conference on
Computer Vision , pages 4059–4069, 2019. 2
[34] John L Nazareth. Conjugate gradient method. Wiley Interdis-
ciplinary Reviews: Computational Statistics , 1(3):348–353,
2009. 4
[35] Zhihong Pan, Glenn Healey, Manish Prasad, and Bruce
Tromberg. Face recognition in hyperspectral images. IEEE
Transactions on Pattern Analysis and Machine Intelligence ,
25(12):1552–1560, 2003. 1
[36] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
InMedical Image Computing and Computer-Assisted Inter-
vention , pages 234–241, 2015. 5
[37] Michal Shimoni, Rob Haelterman, and Christiaan Perneel.
Hypersectral imaging for military and security applications:
Combining myriad processing and sensing techniques. IEEE
Geoscience and Remote Sensing Magazine , 7(2):101–117,
2019. 1
[38] Ashwin Wagadarikar, Renu John, Rebecca Willett, and
David Brady. Single disperser design for coded aperture
snapshot spectral imaging. Applied Optics , 47(10):B44–
B51, 2008. 1
[39] Lizhi Wang, Zhiwei Xiong, Dahua Gao, Guangming Shi, and
Feng Wu. Dual-camera design for coded aperture snapshot
spectral imaging. Applied Optics , 54(4):848–858, 2015. 1,
2,6,7[40] Lizhi Wang, Zhiwei Xiong, Dahua Gao, Guangming Shi,
Wenjun Zeng, and Feng Wu. High-speed hyperspectral video
acquisition with a dual-camera architecture. In IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
4942–4950, 2015. 1
[41] Lizhi Wang, Zhiwei Xiong, Guangming Shi, Wenjun Zeng,
and Feng Wu. Simultaneous depth and spectral imaging with
a cross-modal stereo system. IEEE Transactions on Circuits
and Systems for Video Technology , 28(3):812–817, 2016. 2
[42] Lizhi Wang, Zhiwei Xiong, Guangming Shi, Feng Wu,
and Wenjun Zeng. Adaptive nonlocal sparse representation
for dual-camera compressive hyperspectral imaging. IEEE
Transactions on Pattern Analysis and Machine Intelligence ,
39(10):2104–2111, 2017. 2,3,7
[43] Lizhi Wang, Zhiwei Xiong, Hua Huang, Guangming Shi,
Feng Wu, and Wenjun Zeng. High-speed hyperspectral video
acquisition by combining nyquist and compressive sampling.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence , 41(4):857–870, 2018. 1
[44] Lizhi Wang, Tao Zhang, Ying Fu, and Hua Huang. Hyper-
reconnet: Joint coded aperture optimization and image re-
construction for compressive hyperspectral imaging. IEEE
Transactions on Image Processing , 28(5):2257–2270, 2018.
2
[45] Lizhi Wang, Chen Sun, Ying Fu, Min H Kim, and Hua
Huang. Hyperspectral image reconstruction using a deep
spatial-spectral prior. In IEEE Conference on Computer Vi-
sion and Pattern Recognition , pages 8032–8041, 2019. 2,
3
[46] Lizhi Wang, Chen Sun, Maoqing Zhang, Ying Fu, and Hua
Huang. Dnu: deep non-local unrolling for computational
spectral imaging. In IEEE Conference on Computer Vision
and Pattern Recognition , pages 1661–1671, 2020. 2
[47] Lizhi Wang, Shipeng Zhang, and Hua Huang. Adaptive
dimension-discriminative low-rank tensor recovery for com-
putational hyperspectral imaging. International Journal of
Computer Vision , 129(10):2907–2926, 2021. 2
[48] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE Transactions on Image Process-
ing, 13(4):600–612, 2004. 7
[49] Qingsong Xu, Xin Yuan, Chaojun Ouyang, and Yue Zeng.
Attention-based pyramid network for segmentation and clas-
siﬁcation of high-resolution and hyperspectral remote sens-
ing images. Remote Sensing , 12(21):3501, 2020. 1
[50] Fumihito Yasuma, Tomoo Mitsunaga, Daisuke Iso, and
Shree K Nayar. Generalized assorted pixel camera: Post-
capture control of resolution, dynamic range, and spectrum.
IEEE Transactions on Image Processing , 19(9):2241–2253,
2010. 7
[51] Xin Yuan. Generalized alternating projection based total
variation minimization for compressive sensing. In IEEE
International Conference on Image Processing , pages 2539–
2543, 2016. 2
[52] Xin Yuan, Tsung-Han Tsai, Ruoyu Zhu, Patrick Llull, David
Brady, and Lawrence Carin. Compressive hyperspectral
imaging with side information. IEEE Journal of Selected
Topics in Signal Processing , 9(6):964–976, 2015. 2
24890
[53] Xin Yuan, David J Brady, and Aggelos K Katsaggelos. Snap-
shot compressive imaging: Theory, algorithms, and appli-
cations. IEEE Signal Processing Magazine , 38(2):65–88,
2021. 1
[54] Maoqing Zhang, Lizhi Wang, Lei Zhang, and Hua Huang.
Compressive hyperspectral imaging with non-zero mean
noise. Optics Express , 27(13):17449–17462, 2019. 2
[55] Shipeng Zhang, Hua Huang, and Ying Fu. Fast parallel
implementation of dual-camera compressive hyperspectral
imaging system. IEEE Transactions on Circuits and Systems
for Video Technology , 29(11):3404–3414, 2018. 2
[56] Shipeng Zhang, Lizhi Wang, Ying Fu, Xiaoming Zhong, and
Hua Huang. Computational hyperspectral imaging based on
dimension-discriminative low-rank tensor recovery. In IEEE
International Conference on Computer Vision , pages 10183–
10192, 2019. 2
[57] Weihang Zhang, Jinli Suo, Kaiming Dong, Lianglong Li,
Xin Yuan, Chengquan Pei, and Qionghai Dai. Handheld
snapshot multi-spectral camera at tens-of-megapixel resolu-
tion. Nature Communications , 14(1):5043, 2023. 1
24891
