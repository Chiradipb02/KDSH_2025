Dynamic Adapter Meets Prompt Tuning:
Parameter-Efficient Transfer Learning for Point Cloud Analysis
Xin Zhou∗1, Dingkang Liang∗1, Wei Xu1, Xingkui Zhu1, Yihan Xu1, Zhikang Zou2, Xiang Bai†1
1Huazhong University of Science and Technology, {xzhou03,dkliang,xbai }@hust.edu.cn
2Baidu Inc., China
Transformer layer
𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑡𝑡Transformer layer
(a) Existing Adapter tuning (b) Existing Prompt tuningAdapter𝑺𝑺𝒎𝒎
(c) Our Dynamic Adapter with 
Internal Prompt tuningAdapter𝑺𝑺𝒅𝒅
: Tunable parameters : Frozen parameters 𝑺𝑺𝒎𝒎: Manual scale 𝑺𝑺𝒅𝒅: Dynamic scalePointMLPPointNeXt
Point -MAE
Point -BERTRECON-DAPT
 (Ours )RECON
Point -BERT -
DAPT ( Ours )
Point -BERT -
IDPT  RECON-IDPT
828384858687888990
0 15 30 45Overall Acc. (%)
Tunable parameters (M)
(d) The comparison of several methods on the 
ScanObjectNN (Hardest part) dataset… 𝑇𝑇𝑃𝑃𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑃𝑃𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑃𝑃𝑇𝑇𝑇𝑇𝑇𝑇… 𝑇𝑇𝑃𝑃𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑃𝑃𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑃𝑃𝑇𝑇𝑇𝑇𝑇𝑇
… 𝑇𝑇𝑃𝑃𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑃𝑃𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑃𝑃𝑇𝑇𝑇𝑇𝑇𝑇𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑡𝑡 … 𝑇𝑇𝑃𝑃𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑃𝑃𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑃𝑃𝑇𝑇𝑇𝑇𝑇𝑇
… 𝑇𝑇𝑃𝑃𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑃𝑃𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑃𝑃𝑇𝑇𝑇𝑇𝑇𝑇… 𝑇𝑇𝑃𝑃𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑃𝑃𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑃𝑃𝑇𝑇𝑇𝑇𝑇𝑇 𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑡𝑡
Transformer layer
 Transformer layer
Transformer layer
Transformer layer
Figure 1. (a) Adapter tuning utilizes additional residual blocks with manual scale. (b) Prompt tuning usually introduces extra random
initialized prompts into the input space. (c) The proposed Dynamic Adapter generates a dynamic scale for each token and seamlessly
integrates with Prompt tuning. (d) The proposed method achieves an ideal trade-off between tunable parameters and performance.
Abstract
Point cloud analysis has achieved outstanding perfor-
mance by transferring point cloud pre-trained models.
However, existing methods for model adaptation usually up-
date all model parameters, i.e., full fine-tuning paradigm,
which is inefficient as it relies on high computational costs
(e.g., training GPU memory) and massive storage space.
In this paper, we aim to study parameter-efficient trans-
fer learning for point cloud analysis with an ideal trade-
off between task performance and parameter efficiency. To
achieve this goal, we freeze the parameters of the default
pre-trained models and then propose the Dynamic Adapter,
which generates a dynamic scale for each token, consid-
ering the token significance to the downstream task. We
further seamlessly integrate Dynamic Adapter with Prompt
Tuning (DAPT) by constructing Internal Prompts, captur-
ing the instance-specific features for interaction. Extensive
experiments conducted on five challenging datasets demon-
strate that the proposed DAPT achieves superior perfor-
mance compared to the full fine-tuning counterparts while
significantly reducing the trainable parameters and training
GPU memory by 95% and 35%, respectively. Code is avail-
able at https://github.com/LMD0311/DAPT .
* Equal contribution. †Corresponding author.1. Introduction
3D point cloud analysis, a crucial computer vision task, has
a broad range of applications, such as autonomous driv-
ing [6, 24, 30] and 3D reconstruction [3, 10, 34]. The in-
trinsic irregularity and sparsity of point clouds pose signifi-
cant challenges. In the past few years, various deep learning
methods [7, 22, 38] have appeared, yielding substantial ad-
vancements in this domain.
To generate robust 3D general representations, using
self-supervised learning with vision transformers [9] to pre-
train models is a recent hot topic, such as mask mod-
eling [13, 35] and contrast learning [39, 40]. Further
fine-tuning the entire parameters of the pre-trained mod-
els on downstream datasets usually leads to faster conver-
gence and significant performance improvements. How-
ever, such a fine-tuning strategy is sub-optimal for point
cloud analysis, as reflected in the following aspects: 1)
Fine-tuning the entire backbone might suffer catastrophic
forgetting and break the rich prior knowledge embedded in
the pre-training. 2) Fine-tuning for each point cloud analy-
sis dataset requires a separate weights copy, and the storage
space overhead may becomes a burden as the number of
datasets increases. 3) The computational cost requirements
escalate dramatically, especially for larger batch sizes, lead-
ing to a substantial increase in GPU memory usage, limiting
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
14707
its accessibility for researchers with weak hardware.
To alleviate these problems, we turn our attention to
Parameter-Efficient Transfer Learning (PETL), an alterna-
tive fine-tuning strategy that has achieved marked progress
in the Natural Language Processing (NLP) domain. Current
PETL methods seek to exploit the representational prior in
large language models (LLM) by fixing most of the param-
eters and adjusting only a selected few. Two pioneering
PETL paradigms of NLP are Adapter tuning [12, 16, 17]
and Prompt tuning [23, 25], where the former usually intro-
duces a lightweight network for each Attention/FFN block
(Fig. 1(a)) and the latter inserts some external learnable pa-
rameters called prompts to the input of each transformer
layer (Fig. 1(b)). Both paradigms can obtain performance
on par with or surpass the full fine-tuning counterpart while
significantly reducing the trainable parameters.
However, we empirically find that directly using the
Adapter/Prompt tuning cannot achieve satisfactory results
in point cloud analysis. We argue the main reasons are
twofold: 1)One key aspect of Adapter tuning is the scale,
which is used to adjust the features to match the down-
stream tasks better, as shown in Fig. 1(a). Existing meth-
ods [4, 12, 19, 31] often require manual scale setting as a
crucial hyper-parameter, while the value remains constant
during inference. This static scale may struggle to adapt
to point clouds with complex geometry structures and non-
uniform distribution. As a result, such challenges demand
a more dynamic and adaptable tuning approach. Simply
applying a static scale overlooks the unique characters of
each point cloud. 2)The Prompt tuning usually adds exter-
nal random initialized prompts that often lack relevance to
the point clouds, as shown in Fig. 1(b). This misalignment
makes the network optimization process challenging.
Considering these reasons, in this paper, we propose the
Dynamic Adapter and seamlessly integrate it with Prompt
Tuning, namely ‘DAPT’. As shown in Fig. 1(c), we intro-
duce an extremely lightweight module to generate a unique
scale for each token for Adapter tuning. Such an opera-
tor can make sure the Adapter dynamically adjusts each to-
ken by considering the significance score and matches the
complex 3D samples better. Besides, we propose Internal
Prompt tuning, which utilizes the Dynamic Adapter to pro-
duce a series of prompts instead of using extra random ini-
tialized prompts. Through the Internal Prompt tuning, each
prompt will better capture the instance-specific features and
be easy to optimize, helping the model capture the global
perspective of point clouds.
Extensive experiments on diverse point cloud datasets
in different settings demonstrate the effectiveness of our
method. In particular, as shown in Fig. 1(d), our DAPT
can be simply merged into the state-of-the-art pre-trained
models (e.g., Point-BERT [52], R ECON[39]) and can sig-
nificantly reduce the trainable parameters by 95% and saveGPU memory by up to 35% compared with the full fine-
tuning paradigm, while achieving similar or even higher
performance, e.g., 2.36% accuracy improvement on Point-
BERT under ScanObjectNN PB 50RS variant.
Our major contributions can be summarized as follows:
• We reveal the limitation of existing parameter-efficient
transfer learning (Adapter tuning and Prompt tuning) of
NLP in point cloud analysis.
• We propose the Dynamic Adapter, which generates a
unique scale for each token to dynamically adjust features
by considering the significance score.
• Based on the Dynamic Adapter, we introduce the Inter-
nal Prompt tuning, where we use the Dynamic Adapter
to construct the prompt instead of using extra random ini-
tialized prompts.
2. Related Work
2.1. Pre-training on Point Cloud
Transformers have been widely used in 3D vision [2, 15, 49,
56, 58], and pre-training on 3D datasets is receiving signif-
icant interest. There are two main pretext tasks paradigms
for 3D pre-training, including contrastive learning [1, 48]
and mask modeling [27, 28, 52]. Contrastive learning-based
methods usually contrast between the different views or in-
stances, such as PointContrast [48] and CrossPoint [1]. Be-
sides, Poursaeed et al. [36] defined the estimation of ro-
tation angles as an auxiliary task for contrastive learning.
The mask modeling [35, 55, 57] typically relies on autoen-
coders to learn the latent features of the data by reconstruct-
ing the original input. For example, Point-BERT [52] gen-
erates discrete point tokens containing meaningful local in-
formation from masked tokens. Point-MAE [35] uses an au-
toencoder to learn high-level latent features from unmasked
patches, aiming to reconstruct the masked point patches.
ACT [8] employs an autoencoder as the cross-modal teacher
to guide the 3D representation learning. R ECONand R E-
CON++ [39, 40] learns through ensemble distillation from
both generative modeling teachers and single/cross-modal
contrastive teachers.
The above methods usually fine-tune the entire back-
bone to adopt the pre-trained model to the downstream 3D
tasks effectively, achieving promising performance. How-
ever, full fine-tuning is inefficient and may break the well-
trained knowledge. In this paper, our focus is on efficiently
transferring pre-trained 3D models to downstream tasks.
2.2. Parameter-Efficient Transfer Learning
The continuous expansion of pre-trained models demands
significant computational resources and consumes consid-
erable storage during fine-tuning. To address these chal-
lenges, researchers in the NLP and 2D computer vision do-
main have explored PETL methods:
14708
Prompt tuning [18, 23] usually adds extra information to
the model by introducing latent tokens (prompts) to the task
input, enhancing the model’s behavior during fine-tuning.
Building on prompt tuning, Prefix tuning [25] concatenates
tunable prefix vectors to the keys and values of the Atten-
tion at every layer. Adapter tuning methods [4, 16] insert
lightweight modules named Adapter into the FFN in a com-
plementary approach, allowing for incremental model re-
finements. LoRA [17] further adopts a low-rank approxima-
tion to update weight matrices in the Attention. Building on
these foundations, several variations have been proposed,
including varying the placement of Adapters [12, 26, 59],
empowering the network to select the best method combi-
nation [33], and implementing strategies to minimize the
trainable parameters [19–21, 51].
So far, only one method, IDPT [54] focuses on
parameter-efficient transfer learning on point cloud analy-
sis. IDPT extends the Prompt tuning with a DGCNN [45]
to extract instance-aware prompts for model fine-tuning in-
stead of using static prompts. Unlike IDPT, we propose the
Dynamic Adapter and seamlessly integrate it with Prompt
Tuning, which significantly reduces the tunable parameters
and achieves impressive performance.
3. Preliminary
In this section, we revisit the vision transformer and intro-
duce Adapter tuning and Prompt tuning as preliminary.
3.1. Vision Transformer
In a vision transformer (ViT) [9], an image is divided into
Nnon-overlapping patches and transferred to a 1D se-
quence. The sequence is further processed by L-layer trans-
former blocks, along with a classification token. The in-
putx∈R(N+1)×d, where dis the embedding dimen-
sion, is first transformed to queries Q∈R(N+1)×d, keys
K∈R(N+1)×dand values V∈R(N+1)×d. After that, we
can calculate the self-attention with an attention layer.
The output of the attention layer will be sent to an FFN
module with residual bypasses to extract information in
channels. The transformer block can be written as below:
x′
i= Attention (LN ( xi−1)) +xi−1,
xi= FFN (LN ( x′
i)) +x′
i,(1)
where xiis the output of i-th block, LN is layer norm. Re-
cently, some work [35, 52] has adapted the ViT for point
cloud analysis and utilized self-supervision for pre-training.
In ViT, the Attention and FFN components contain most pa-
rameters. It would be reasonable to fix them for PETL, but
doing so would damage performance.
3.2. Parameter-Efficient Fine-tuning
Adapter tuning [4, 12, 16] is a module that acts as a bot-
tleneck and inserts a few parameters into the transformer. ItTable 1. The overall accuracy (%) for Adapter and Prompt tun-
ing strategies on three variants of ScanObjectNN [43] is reported.
‘#TP’ means the number of tunable parameters. Linear probing
indicates head-tuned only.
Tuning Strategy #TP(M) OBJ BG OBJ ONLY PB T50 RS
Point-MAE [35] 22.1 90.02 88.29 85.18
Linear probing 0.3 87.26 (-2.76 )84.85 (-3.44 )75.99 (-9.19 )
+ Adapter [16] 0.9 89.50 (-0.52 )88.64 (+0.35 )83.93 (-1.25 )
+ VPT [18] 0.4 87.26 (-2.76 )87.09 (-1.20 )81.09 (-4.09 )
includes a downward projection to decrease the feature di-
mension, a non-linear function, and an upward projection to
project back to the original dimension. Specifically, given
the input as x∈R(N+1)×d, the output is calculated by
Output = Sm× 
ϕ 
xWT
d
WT
u
, (2)
where Wd∈Rr×d,ϕ(·),Wu∈Rd×r(r≪d), and Sm
represent the down-projection weight, non-linear function,
up-projection weight, and scale, respectively. Note that Sm
is a key hyper-parameter that needs to be manually set in
existing methods [4, 12, 19, 31].
Prompt tuning [18, 23, 25] creates random initialized
tokens as prompts. These prompts are then incorporated
into the input of a transformer block or attention layer,
where they interact with the original tokens through self-
attention. In the fine-tuning process, the weights of the
backbone network remain frozen, and only the prompts are
updated. Given classification token Tcls∈R1×dand patch
tokens T∈RN×d, we can represent the inserted prompts
asPi={Pk
i∈R1×d|k∈N,1≤k≤n}, where n
is the number of prompts. The output of i-th layer ( Li(·))
xi∈R(N+n+1)×dare obtained by
xi=Li([Tcls;Pi;T]). (3)
However, while these tuning strategies achieve promis-
ing results in NLP and 2D vision, they lack targeted design
for 3D. As shown in Tab. 1, when it comes to the hardest
(i.e., PB T50 RS) variant of ScanObjectNN [43], Adapter
and VPT have an accuracy gap of 1.73% and 4.09% when
compared to full fine-tuning, respectively. Thus, designing
a parameter-efficient fine-tuning method that performs well
on 3D data is meaningful.
4. Methodology
Fig. 2(a) illustrates an overview of the proposed DAPT,
which consists of three key components: a Task-agnostic
Feature Transform Strategy (TFTS) for feature regulation,
a Dynamic Adapter for feature modification, and an Inter-
nal Prompt selected from the output of Dynamic Adapter.
The specifics of our DAPT will be discussed below.
14709
…
Patch encoderTFTSTransformer layer…Transformer layer…Transformer layerDownstream head
Dynamic 
Adapter
𝑻𝑻0 𝑻𝑻𝑁𝑁 𝑻𝑻𝑐𝑐𝑐𝑐𝑐𝑐𝑻𝑻0 𝑻𝑻𝑁𝑁 𝑻𝑻1 𝑻𝑻𝑐𝑐𝑐𝑐𝑐𝑐𝑻𝑻𝑐𝑐𝑐𝑐𝑐𝑐
𝑖𝑖−𝑡𝑡ℎ
1−𝑡𝑡ℎ
0−𝑡𝑡ℎ𝑷𝑷0𝑷𝑷0𝑷𝑷𝑖𝑖… … 𝑻𝑻0𝑻𝑻1𝑻𝑻𝑁𝑁
…𝑻𝑻0 𝑻𝑻𝑁𝑁 𝑻𝑻𝑐𝑐𝑐𝑐𝑐𝑐Layer normTFTSAttentionTFTSFFNTFTS… 𝑻𝑻0 𝑻𝑻𝑁𝑁 𝑻𝑻1 𝑻𝑻𝑐𝑐𝑐𝑐𝑐𝑐𝑷𝑷0
(a) The overall of our methodFFN
GELU
(b)Dynamic Adapter with internal promptInput
tokens 𝑻𝑻
ReLU𝑊𝑊𝑑𝑑 𝑊𝑊𝑢𝑢
𝑊𝑊𝑐𝑐
Layer normTFTS𝑷𝑷𝑖𝑖 TFTS
𝑆𝑆𝑑𝑑
Dynamic 
scaleInternal 
Prompt
Tunable  
parameters
Frozen  
parameters
20.4
16.0
10.4
2610141822
16 32 64 128 256 512GPU Memory (G)
Batch sizeIDPT (ICCV 23)
Full fine-tuning
DAPT (Ours)
(c) The comparison of training GPU memory
Figure 2. (a) The pipeline of our DAPT. During training, we fix the entire backbone, only fine-tuning the newly added parameters. (b) The
detail of the Dynamic Adapter with Internal Prompt. (c) Comparison of GPU memory usage among full fine-tuning, IDPT [54] and ours.
4.1. Task-agnostic Feature Transform Strategy
3D pre-training models, such as masked autoencoders [35,
57] or contrastive learning [1, 39], have demonstrated their
effectiveness in acquiring a generic feature representation
space. However, these representations are often task-
agnostic, thus requiring fine-tuning for downstream tasks.
Hence, we introduce a frustratingly simple Task-agnostic
Feature Transform Strategy (TFTS), adding two small
learnable factors to transform the model’s features [26].
Given the input x∈R(N+1)×d, the output y∈R(N+1)×d
is calculated as follows:
y=γ×x+β, (4)
where γ,β∈R1×dare learnable factors. Such a simple
strategy effectively converts features to match downstream
representations of 3D data, resulting in considerable perfor-
mance improvements while only introducing an additional
0.5% parameters to the baselines.
4.2. Dynamic Adapter
Adapter tuning [4, 12, 16] from NLP and 2D vision usually
involves manual scale, significantly affecting the adapted
model’s performance. Furthermore, this scale must be ad-
justed manually as a hyper-parameter for each specific task,
which is time-consuming and inconvenient. To address this
problem, we proposed the Dynamic Adapter, comprising
two projection layers: a downward projection with parame-
tersWd∈Rr×dand an upward projection with parameters
Wu∈Rd×r, as shown in Fig. 2(b).
The two projection layers are connected by a GELU
function. We adopt a parallel MLP to generate dynamic
scaleSdbased on variant point cloud features. Specifically,we utilize a scoring weight matrix Ws∈R1×dto calculate
a scale Sdfor each token. With xas the input, our dynamic
scale can be expressed as
Sd= ReLU 
xWT
s
. (5)
Note that we utilize the ReLU to select the positive scale
and set the rest as zero, as we think only significant tokens
need to be adjusted, and it should depend on the unique
characters of each point cloud. In general, the output of our
Dynamic Adapter x′can be described as
x′=Sd× 
GELU 
xWT
d
WT
u
. (6)
To mitigate the influence of Adapter outputs during the ini-
tial stages of model training, we initialize Wuto zero.
4.3. Internal Prompt Tuning
In this subsection, we introduce how to combine our Dy-
namic Adapter with Prompt tuning. The existing Prompt
tuning methods [18, 23, 25] usually adopt external random
initialized embedding (prompts) to each layer input of the
Transformer. Such random initialization cannot match well
to the point cloud. In addition, during the testing phase,
the prompts are static, i.e., they cannot generate instance-
specific representations for each point cloud.
To this end, we leverage the Dynamic Adapter to gen-
erate the prompt derived from the original model’s internal
output using the proposed Dynamic Adapter, called Internal
Prompt tuning. Specifically, Dynamic Adapter operates on
raw tokens, allowing our prompt to incorporate the model’s
pre-training knowledge effectively. In practice, we empiri-
cally average pool the output of Dynamic Adapter after ap-
plying a GELU activation. Then, we utilized TFTS to ob-
tain a Prompt and concatenated it between the classification
14710
Algorithm 1 Dynamic Adapter and Internal Prompt.
# An example of Dynamic Adapter and Internal Prompt
import torch.nn as nn
import torch.nn.functional as F
class DynamicAdapter():
def __init__(self, dim, r):
super().__init__()
self.norm = nn.LayerNorm(dim)
self.scale = nn.Linear(dim, 1)
self.down_proj = nn.Linear(dim, r)
self.up_proj = nn.Linear(r, dim)
self.active = nn.GELU()
with torch.no_grad():
nn.init.zeros_(self.up_proj.weight)
nn.init.zeros_(self.up_proj.bias)
def forward(self, x):
# Dynamic Adapter
x = self.norm(x)
dynamic_scale = F.relu(self.scale(x))
out = self.down_proj(x)
out = self.active(out)
out = self.up_proj(out)
out = out *dynamic_scale
# Generate Internal Prompt
internal_prompt = self.active(out).mean(dim=1)
return out, internal_prompt
tokenTcls∈R1×dand patch tokens T∈RN×d. The out-
put of the i-th transformer layer xican be expressed as
xi=Li([Tcls;P0, . . . ,Pi−1;T]). (7)
Our Dynamic Adapter adds a residual embedding ( ∆xk)
for each token ( xk), acting as a local perspective without
modeling other point tokens (i.e., ∆xkonly comes from
xk). By pooling all outputs of the Dynamic Adapter, our
internal prompts (P∆xk) can serve as a global perspec-
tive and interact directly with each point tokens xk+ ∆xk,
enabling access to the cumulative information of the en-
tire sequence. Besides, the bi-directional interaction in
self-attention strengthens the representation of the Dynamic
Adapter, further improving the performance.
Please refer to Algorithm 1 for more detailed informa-
tion on Dynamic Adapter and Internal Prompt.
4.4. Analysis
Tunable parameters. The proposed DAPT keeps the entire
backbone frozen and only tunes the newly inserted param-
eters, effectively reducing the number of tunable parame-
ters. Specifically, the TFTS introduces only two parameters
inR1×dfor each operation. Each extra Dynamic Adapter
module adds 2dfrom layer norm, dfromWs,2×r×dfrom
WdandWu(bias ignored) as tunable parameters, where
the intermediate dimension ris relatively small compared
tod. Our Internal Prompt tuning can propose a prompt by
pooling and TFTS without introducing other parameters. In
summary, we only need to fine-tune fewer than 5% of the
parameters to achieve comparable results.
Training memory. The proposed approach can consis-
tently reduce training memory as the batch size increases.As shown in Fig. 2(c), with 512 batch size, our DAPT sig-
nificantly reduces GPU memory1usage by 35% and 49%
compared to full fine-tuning and IDPT [54] (the recent
SOTA PETL method for point cloud), respectively.
Storage. The proposed DAPT significantly reduces the
storage cost. After being fine-tuned on Mpoint cloud
datasets, only one copy of the original pre-trained model’s
full parameters and Mdataset-specific trainable parameters
need to be stored, saving up to 95% of storage space. Thus,
even if storage is not a current concern, DAPT presents a
promising approach for adapting the expansion of increas-
ingly larger models in 3D vision.
5. Experiments
5.1. Implement Details
To ensure a fair comparison, we employed identical exper-
imental settings to the default fine-tuning method for each
baseline. This entails freezing the weights of the pre-trained
point cloud backbone and solely updating the newly in-
serted parameters during training. All experiments are con-
ducted on a single GeForce RTX 4090.
5.2. 3D Classification
Real-World Object Classification. ScanObjectNN [43] is
a highly challenging 3D dataset covering ∼15K real-world
objects across 15categories. These objects consist of in-
door scene data obtained by scanning, often characterized
by cluttered backgrounds and occlusion caused by other ob-
jects. As shown in Tab. 2, we conducted experiments on
three variants of ScanObjectNN (i.e., OBJ BG, OBJ ONLY ,
and PB T50 RS), each with increasing complexity. We
utilize various baselines such as Point-BERT [52], Point-
MAE [35], and R ECON[39] to demonstrate the effective-
ness and generalization of our approach. It is worth noting
that our DAPT meets or exceeds the performance of full
fine-tuning in most cases, while only tuning less than 5%
of the parameters. Especially, DAPT achieved increases of
3.62%, 1.55%, and 2.36% in the three ScanObjectNN vari-
ants on Point-BERT. Furthermore, we surpass IDPT [54],
the most recent SOTA method for 3D efficient fine-tuning,
in almost every variant for different baselines, with 35%
fewer tunable parameters. A similar phenomenon is also
observed in Point-MAE and R ECON. Moreover, our DAPT
has only a slight increase in FLOPs compared to the full
fine-tuning and is quite competitive with IDPT.
Synthetic Object Classification. ModelNet40 [47] con-
tains12,311pristine 3D CAD models across 40categories.
Every point cloud is complete, uniform, and noise-free.
Due to the voting strategy [29] being time-consuming, we
prioritize reporting overall Accuracy without voting. As
shown in Tab. 2, although the performance in ModelNet40
1The gradient checkpoint [5] is already being used to save memory.
14711
Table 2. Classification on three variants of the ScanObjectNN [43] and the ModelNet40 [47], including the number of trainable parameters
and overall accuracy (OA). All methods utilize the default data argumentation as the baseline.∗denotes reproduced results. We report
ScanObjectNN results without voting. ModelNet40 results are without and with voting, referred to (-/-).
Method Reference Tunable params. (M) FLOPs (G)ScanObjectNN ModelNet40
OBJ BG OBJ ONLY PB T50 RS Points Num. OA (%)
Supervised Learning Only
PointNet [37] CVPR 17 3.5 0.5 73.3 79.2 68.0 1k - / 89.2
PointNet++ [38] NeurIPS 17 1.5 1.7 82.3 84.3 77.9 1k - / 90.7
DGCNN [45] TOG 19 1.8 2.4 82.8 86.2 78.1 1k - / 92.9
MVTN [11] ICCV 21 11.2 43.7 - - 82.8 1k - / 93.8
PointNeXt [41] NeurIPS 22 1.4 1.6 - - 87.7 1k - / 94.0
PointMLP [32] ICLR 22 13.2 31.4 - - 85.4 1k - / 94.5
RepSurf-U [42] CVPR 22 1.5 0.8 - - 84.3 1k - / 94.4
ADS [14] ICCV 23 - - - - 87.5 1k - / 95.1
Self-Supervised Representation Learning (Full fine-tuning)
OcCo [44] ICCV 21 22.1 4.8 84.85 85.54 78.79 1k - / 92.1
Point-BERT [52] CVPR 22 22.1 4.8 87.43 88.12 83.07 1k - / 93.2
MaskPoint [28] ECCV 22 22.1 - 89.70 89.30 84.60 1k - / 93.8
Point-MAE [35] ECCV 22 22.1 4.8 90.02 88.29 85.18 1k - / 93.8
Point-M2AE [57] NeurIPS 22 15.3 3.6 91.22 88.81 86.43 1k - / 94.0
ACT [8] ICLR 23 22.1 4.8 93.29 91.91 88.21 1k - / 93.7
RECON[39] ICML 23 43.6 5.3 94.15 93.12 89.73 1k - / 93.9
Self-Supervised Representation Learning (Efficient fine-tuning)
Point-BERT [52] (baseline) CVPR 22 22.1 (100%) 4.8 87.43 88.12 83.07 1k 92.7 / 93.2
+ IDPT [54] ICCV 23 1.7 (7.69%) 7.2 88.12 (+0.69 )88.30 (+0.18 )83.69 (+0.62 ) 1k 92.6 (-0.1)/ 93.4 (+0.2)
+ DAPT (ours) - 1.1(4.97%) 5.0 91.05 (+3.62 )89.67 (+1.55 )85.43 (+2.36 ) 1k 93.1 (+0.4)/ 93.6 (+0.4)
Point-MAE [35] (baseline) ECCV 22 22.1 (100%) 4.8 90.02 88.29 85.18 1k 93.2 / 93.8
+ IDPT [54] ICCV 23 1.7 (7.69%) 7.2 91.22 (+1.20 )90.02 (+1.73 )84.94 (-0.24 ) 1k 93.3 (+0.1)/ 94.4 (+0.6)
+ DAPT (ours) - 1.1(4.97%) 5.0 90.88 (+0.86 )90.19 (+1.90 )85.08 (-0.10 ) 1k 93.5 (+0.3)/ 94.0 (+0.2)
RECON[39] (baseline2) ICML 23 22.1 (100%) 4.8 94.32 92.77 90.01 1k 92.5 / 93.0
+ IDPT∗[54] ICCV 23 1.7 (7.69%) 7.2 93.29 (-1.03 )91.57 (-1.20 )87.27 (-2.74 ) 1k 93.4 (+0.9)/ 93.5 (+0.5)
+ DAPT (ours) - 1.1(4.97%) 5.0 94.32 (0.00)92.43 (-0.34 )89.38 (-0.63 ) 1k 93.5 (+1.0)/ 94.1 (+1.1)
is quite saturated, our method consistently achieves perfor-
mance gains compared to different baselines and IDPT [54].
Specifically, R ECONcombined with our DAPT achieves
1.0% improvement and significantly reduces tunable pa-
rameters. Even after voting, our DAPT remains the leader.
Few-shot Learning. We further conduct few-shot exper-
iments on ModelNet40 [47] to prove our few-shot transfer
learning ability. Consistent with previous works [35, 52],
we employ the n-way, m-shot configuration. As shown in
Tab. 3, it can be observed that DAPT achieved performance
enhancements in most cases compared to full fine-tuning
and IDPT, indicating its effectiveness in few-shot learning.
5.3. Part Segmentation
Part segmentation is challenging to predict a more detailed
class label for every point. We evaluate the effectiveness of
DAPT on ShapeNetPart [50], which includes 16,881sam-
ples from 16categories. Following previous works [8, 35],
we incorporate Dynamic Adapter into the 2nd,6th, and 10th
layers, generating an Internal Prompt to feed into the in-
put of the 3rd,7th, and 11th layers and employing them asglobal features. As shown in Tab. 4, our DAPT achieves
competitive results on Inst. mIoU and notable improve-
ments on Cls. mIoU compared with IDPT [54]. Keep in
mind that the increase in tunable parameters mainly comes
from the head part, but we still reduce the tunable parame-
ters and gain improvements compared with IDPT.
5.4. Compared with Other PETL Methods
To further prove the effectiveness of our proposed method,
we compare DAPT with several PETL approaches from
NLP and 2D vision. As shown in Tab. 5, we select the
most challenging PB T50 RS variant with Point-MAE as
the baseline. Note that we have carefully tuned the hyper-
parameters of these methods to attain optimal performance.
While they show different performance improvements com-
pared to linear probing, there is still a considerable gap com-
pared to full fine-tuning. Although these methods excel in
NLP or 2D, they still struggle to adapt to 3D data due to
2We modify the structure by loading a pre-trained R ECONweight into
Point-MAE as a single-modal fine-tuning approach to reproduce similar
results as the original paper.
14712
Table 3. Few-shot learning on ModelNet40 [47]. Overall accuracy
(%)±the standard deviation (%) without voting is reported.
Methods Reference5-way 10-way
10-shot 20-shot 10-shot 20-shot
with Self-Supervised Representation Learning (Full fine-tuning)
OcCo [44] ICCV 21 94.0 ±3.6 95.9 ±2.3 89.4 ±5.1 92.4 ±4.6
Point-BERT [52] CVPR 22 94.6 ±3.1 96.3 ±2.7 91.0 ±5.4 92.7 ±5.1
MaskPoint [28] ECCV 22 95.0 ±3.7 97.2 ±1.7 91.4 ±4.0 93.4 ±3.5
Point-MAE [35] ECCV 22 96.3 ±2.5 97.8 ±1.8 92.6 ±4.1 95.0 ±3.0
Point-M2AE [57] NeurIPS 22 96.8 ±1.8 98.3 ±1.4 92.3 ±4.5 95.0 ±3.0
ACT [8] ICLR 23 96.8 ±2.3 98.0 ±1.4 93.3 ±4.0 95.6 ±2.8
RECON[39] ICML 23 97.3 ±1.9 98.9 ±3.9 93.3 ±3.9 95.8 ±3.0
with Self-Supervised Representation Learning (Efficient fine-tuning)
Point-BERT [52] (baseline) CVPR 22 94.6 ±3.1 96.3 ±2.7 91.0 ±5.4 92.7 ±5.1
+ IDPT [54] ICCV 23 96.0±1.797.2±2.6 91.9 ±4.4 93.6 ±3.5
+ DAPT ( ours ) - 95.8±2.197.3±1.392.2±4.394.2±3.4
Point-MAE [35] (baseline) ECCV 22 96.3 ±2.5 97.8 ±1.8 92.6 ±4.1 95.0 ±3.0
+ IDPT [54] ICCV 23 97.3±2.1 97.9 ±1.1 92.8 ±4.1 95.4 ±2.9
+ DAPT ( ours ) - 96.8±1.898.0±1.093.0±3.595.5±3.2
Table 4. Part segmentation on the ShapeNetPart [50]. The mIoU
for all classes (Cls.) and for all instances (Inst.) are reported. #TP
represents the tunable parameters.∗denotes reproduced results.
Methods Reference #TP (M) Cls. mIoU (%) Inst. mIoU (%)
Supervised Learning Only
PointNet [37] CVPR 17 - 80.39 83.7
PointNet++ [38] NeurIPS 17 - 81.85 85.1
DGCNN [45] TOG 19 - 82.33 85.2
APES [46] CVPR 23 - 83.67 85.8
Self-Supervised Representation Learning (Full fine-tuning)
OcCo [44] ICCV 21 27.09 83.42 85.1
MaskPoint [28] ECCV 22 - 84.60 86.0
Point-BERT [52] CVPR 22 27.09 84.11 85.6
Point-MAE [35] ECCV 22 27.06 84.19 86.1
ACT [8] ICLR 23 27.06 84.66 86.1
Self-Supervised Representation Learning (Efficient fine-tuning)
Point-BERT [52] (baseline) CVPR 22 27.06 84.11 85.6
+ IDPT∗[54] ICCV 23 5.69 83.50 85.3
+ DAPT ( ours ) - 5.65 83.83 85.5
Point-MAE [35] (baseline) ECCV 22 27.06 84.19 86.1
+ IDPT [54] ICCV 23 5.69 83.79 85.7
+ DAPT ( ours ) - 5.65 84.01 85.7
RECON[39] (baseline2) ICML 23 27.06 84.52 86.1
+ IDPT∗[54] ICCV 23 5.69 83.66 85.7
+ DAPT ( ours ) - 5.65 83.87 85.7
missing point clouds and other complexities. It is worth not-
ing that our method surpasses the IDPT while reducing the
number of tunable parameters by 35% and achieving a bet-
ter trade-off between tunable parameters and performance.
5.5. Ablation Study
We conducted ablation studies on the most challenging
PBT50 RS variant to investigate the rationalization and ef-
fectiveness of the design of our proposed DAPT and then
fine-tuned on Point-MAE as the baseline.
Analysis on each component. We conduct experiments
to prove the effectiveness of the proposed components of
our DAPT and display the results in Tab. 6. When no ad-
ditional inserted components exist, the method is the sameTable 5. Comparisons of parameter efficient transfer learning
methods from NLP and 2D Vision on the hardest variant of
ScanObjectNN [43]. Overall accuracy (%) without voting is re-
ported. #TP represents the tunable parameters.
Method Reference #TP (M) PB T50 RS
Point-MAE [28] ECCV 22 22.1 85.18
Linear probing - 0.3 75.99
+ Adapter [16] ICML 19 0.9 83.93
+ Perfix tuning [25] ACL 21 0.7 77.72
+ BitFit [53] ACL 21 0.3 82.62
+ LoRA [17] ICLR 22 0.9 81.74
+ VPT-Deep [18] ECCV 22 0.4 81.09
+ AdaptFormer [4] NeurIPS 22 0.9 83.45
+ SSF [26] NeurIPS 22 0.4 82.58
+ IDPT [54] ICCV 23 1.7 84.94
+ DAPT ( ours ) - 1.1 85.08
Table 6. The effect of each component of our DAPT. The tun-
able parameters (#TP) and the overall accuracy (%) on the hardest
variant of ScanObjectNN [43] are reported. In. and Ex. indicate
Internal and External respectively.
TFTS DA In. Prompt Ex. Prompt #TP (M) PB T50 RS
Full fine-tuning 22.1 85.18
Linear Probing 0.27 75.99
✔ 0.37 83.17
✔ 0.88 84.25
✔ ✔ 0.99 84.80
✔ ✔ ✔ 1.08 84.70
✔ ✔ ✔ 1.09 85.08
as Linear Probing, which only achieves 75.99% overall ac-
curacy. By adopting our TFTS, we can achieve a substan-
tial 7.18% performance increase while only adding 0.1M
tunable parameters. To examine the effectiveness of our
proposed Dynamic Adapter, we exclusively use Dynamic
Adapter and achieve an 8.26% performance gain. These
results indicate that TFTS and Dynamic Adapter are pow-
erful parameter-efficient transfer learning modules. We fur-
ther combine the above two components to gain an 84.80%
performance. We also externally replaced the Internal
Prompt, using a Xavier uniform initializer [18] mentioned
in VPT [18]. Compared to our Internal Prompt, the Ex-
ternal Prompt shows a 0.38% performance fall and even a
0.1% performance drop compared with no Prompt tuning,
demonstrating the effectiveness of Internal Prompts.
Analysis on token selections for head inputs. We ex-
amined the influence of the input tokens of the downstream
task head, including the classification token, the pooling of
our prompt tokens, and the pooling of point patch tokens.
As depicted in Fig. 3, we selected three standard token se-
lection methods. We observe that the best performance is
attained when all three features are included: Tcls, prompt
tokens, and point patch tokens. Comparing the use of Tcls
only and both Tclsand pooling of Patch tokens, using all the
features resulted in a 4.13% and 0.66% performance gain.
Analysis on dimension rin Dynamic Adapter. One
of our keys to reducing tunable parameters is selecting di-
mension rin Dynamic Adapter. As shown in Tab. 7a, we
14713
Table 7. Ablation on hyper-parameters and settings of DAPT, including dimension r, settings of dynamic scale and inserted layers. The
tunable parameters (#TP) and the overall accuracy (%) on the hardest variant of ScanObjectNN [43] are reported.
(a) Ablation on rof Dynamic Adapter.
Dimension r#TP (M) PB T50 RS
8 0.57 84.39
16 0.64 84.52
32 0.79 84.18
64 1.09 85.08
72 1.16 84.46(b) Ablation on scale Sdin Dynamic Adapter.
Scale #TP (M) PB T50 RS
1.0 1.08 84.59
5.0 1.08 84.42
10.0 1.08 84.11
Dynamic scale w/o ReLU 1.09 84.73
Dynamic scale 1.09 85.08(c) Ablation on the inserted layers.
Layers #TP (M) PB T50 RS
1→3 0.62 80.67
1→6 0.78 82.48
1→9 0.93 83.97
4→12 0.93 81.75
1→12 1.09 85.08
(c) 𝑻𝑻𝑐𝑐𝑐𝑐𝑐𝑐+ Prompt + Patch pooling  (a) 𝑻𝑻𝑐𝑐𝑐𝑐𝑐𝑐Last Transformer layerDownstream Task Head
Prompts Patch tokens𝑻𝑻𝑐𝑐𝑐𝑐𝑐𝑐
(b) 𝑻𝑻𝑐𝑐𝑐𝑐𝑐𝑐+ Patch pooling  
80.9584.4285.08
80818283848586
(a) (b) (c)Overall Acc. (%)… …
Last Transformer layerDownstream Task Head 𝑻𝑻𝑐𝑐𝑐𝑐𝑐𝑐… …
Last Transformer layerDownstream Task Head 𝑻𝑻𝑐𝑐𝑐𝑐𝑐𝑐… …Prompts Patch tokens
Prompts Patch tokens
Figure 3. Effect of different inputs for downstream task head. We
conduct experiments on the hardest variant (i.e., PB T50 RS) of
ScanObjectNN [43] with Point-MAE [35] baseline.
conduct experiments on various values of rand find that
when r= 64 , we can achieve the best trade-off between
tunable parameters and performance.
Analysis on scale Sdin Dynamic Adapter. It is cru-
cial to obtain a dynamic scale during inference due to the
complexity of point clouds. As indicated in Tab. 7b, fixing
the scale to 1.0yields the best result but decreases by 0.49%
compared to our dynamic scale. We also perform a variation
of dynamic scale, removing the ReLU, and observe a 0.35%
decrease, which is still superior to the fixed scale. In addi-
tion, we displayed the average value of Sdand the ratio of
adjusted tokens in each layer by choosing three samples in
Fig. 4. While their trends are similar, various samples differ
across different layers, demonstrating that our method can
dynamically generate scale for different samples to over-
come the complex geometry structures of point clouds.
Analysis on inserted layers for Dynamic Adapter and
Internal Prompts. One way to further reduce the tunable
parameters is to select only a portion of the layer to insert
our components. However, as shown in Tab. 7c, we observe
that fine-tuning only shallow blocks and fine-tuning only
deep blocks leads to different performance decline levels.
The potential reason may be the difficulty in determining
which layer is more important when the network processes
the diverse point cloud. To address this issue, we incorpo-
0.51.01.52.02.53.03.5
1 2 3 4 5 6 7 8 9 10 11 12The avg. value of Sd
Layer
Bin
Chair
Door20406080100
1 2 3 4 5 6 7 8 9 10 11 12The ratio of adjusted tokens (%)
LayerFigure 4. A visual results of the average value of Sdand the
ratio of adjusted tokens for each layer across three samples in
OBJ ONLY variant of ScanObjectNN [43].
rate our proposed components for each layer and utilize the
dynamic scale to regulate the extent of feature adjustment.
6. Conclusion and Limitation
In this study, we propose a simple yet effective parameter-
efficient transfer learning strategy named DAPT for point
cloud analysis. The proposed DAPT freezes the param-
eters of the pre-trained backbones and leverages the Dy-
namic Adapter to adjust each token dynamically by con-
sidering the significance. In addition, we further use Dy-
namic Adapter to implement Internal Prompt tuning, better
capturing the instance-specific feature. Our method pro-
vides a practical solution to reduce storage cost require-
ments without compromising performance. One limitation
is that whether our method could perform well in more com-
plex tasks such as 3D object detection and generation is still
unclear, which is our future work.
Acknowledgement. This work was supported in part by
the NSFC (No.62225603), in part by the Hubei Key R&D
Program (No.2022BAA078), in part by the Taihu Lake In-
novation Fund for Future Technology (HUST:2023-A-1),
and in part by the National Undergraduate Training Projects
for Innovation and Entrepreneurship (202310487020).
14714
References
[1] Mohamed Afham, Isuru Dissanayake, Dinithi Dissanayake,
Amaya Dharmasiri, Kanchana Thilakarathna, and Ranga Ro-
drigo. Crosspoint: Self-supervised cross-modal contrastive
learning for 3d point cloud understanding. In Proc. of IEEE
Intl. Conf. on Computer Vision and Pattern Recognition ,
2022. 2, 4
[2] Xuyang Bai, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun
Chen, Hongbo Fu, and Chiew-Lan Tai. Transfusion: Robust
lidar-camera fusion for 3d object detection with transform-
ers. In Proc. of IEEE Intl. Conf. on Computer Vision and
Pattern Recognition , 2022. 2
[3] Guangyan Chen, Meiling Wang, Li Yuan, Yi Yang, and
Yufeng Yue. Rethinking point cloud registration as masking
and reconstruction. In Porc. of IEEE Intl. Conf. on Computer
Vision , 2023. 1
[4] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang,
Yibing Song, Jue Wang, and Ping Luo. Adaptformer: Adapt-
ing vision transformers for scalable visual recognition. In
Proc. of Advances in Neural Information Processing Sys-
tems, 2022. 2, 3, 4, 7
[5] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
Training deep nets with sublinear memory cost. arXiv
preprint arXiv:1604.06174 , 2016. 5
[6] Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, and
Jiaya Jia. V oxelnext: Fully sparse voxelnet for 3d object de-
tection and tracking. In Proc. of IEEE Intl. Conf. on Com-
puter Vision and Pattern Recognition , 2023. 1
[7] Silin Cheng, Xiwu Chen, Xinwei He, Zhe Liu, and Xiang
Bai. Pra-net: Point relation-aware network for 3d point cloud
analysis. IEEE Transactions on Image Processing , 2021. 1
[8] Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jian-
jian Sun, Zheng Ge, Li Yi, and Kaisheng Ma. Autoencoders
as cross-modal teachers: Can pretrained 2d image transform-
ers help 3d representation learning? In Proc. of Intl. Conf.
on Learning Representations , 2022. 2, 6, 7
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In Proc. of Intl. Conf.
on Learning Representations , 2021. 1, 3
[10] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set
generation network for 3d object reconstruction from a single
image. In Proc. of IEEE Intl. Conf. on Computer Vision and
Pattern Recognition , 2017. 1
[11] Abdullah Hamdi, Silvio Giancola, and Bernard Ghanem.
Mvtn: Multi-view transformation network for 3d shape
recognition. In Porc. of IEEE Intl. Conf. on Computer Vi-
sion, 2021. 6
[12] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-
Kirkpatrick, and Graham Neubig. Towards a unified view of
parameter-efficient transfer learning. In Proc. of Intl. Conf.
on Learning Representations , 2021. 2, 3, 4
[13] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scalablevision learners. In Proc. of IEEE Intl. Conf. on Computer
Vision and Pattern Recognition , 2022. 1
[14] Cheng-Yao Hong, Yu-Ying Chou, and Tyng-Luh Liu. Atten-
tion discriminant sampling for point clouds. In Porc. of IEEE
Intl. Conf. on Computer Vision , 2023. 6
[15] Jinghua Hou, Zhe Liu, Dingkang Liang, Zhikang Zou, Xiao-
qing Ye, and Xiang Bai. Query-based temporal fusion with
explicit motion for 3d object detection. In Proc. of Advances
in Neural Information Processing Systems , 2024. 2
[16] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona
Attariyan, and Sylvain Gelly. Parameter-efficient transfer
learning for nlp. In Proc. of Intl. Conf. on Machine Learning ,
2019. 2, 3, 4, 7
[17] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank
adaptation of large language models. In Proc. of Intl. Conf.
on Learning Representations , 2021. 2, 3, 7
[18] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,
Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-
sual prompt tuning. In Proc. of European Conference on
Computer Vision , 2022. 3, 4, 7
[19] Shibo Jie and Zhi-Hong Deng. Fact: Factor-tuning for
lightweight adaptation on vision transformer. In Proc. of the
AAAI Conf. on Artificial Intelligence , 2023. 2, 3
[20] Shibo Jie, Haoqing Wang, and Zhi-Hong Deng. Revisiting
the parameter efficiency of adapters from the perspective of
precision redundancy. In Porc. of IEEE Intl. Conf. on Com-
puter Vision , 2023.
[21] Rabeeh Karimi Mahabadi, James Henderson, and Sebastian
Ruder. Compacter: Efficient low-rank hypercomplex adapter
layers. In Proc. of Advances in Neural Information Process-
ing Systems , 2021. 3
[22] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang
Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified trans-
former for 3d point cloud segmentation. In Proc. of IEEE
Intl. Conf. on Computer Vision and Pattern Recognition ,
2022. 1
[23] Brian Lester, Rami Al-Rfou, and Noah Constant. The power
of scale for parameter-efficient prompt tuning. In Proc. of
Conf. on Empirical Methods in Natural Language Process-
ing, 2021. 2, 3, 4
[24] Jinyu Li, Chenxu Luo, and Xiaodong Yang. Pillarnext: Re-
thinking network designs for 3d object detection in lidar
point clouds. In Proc. of IEEE Intl. Conf. on Computer Vi-
sion and Pattern Recognition , 2023. 1
[25] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
continuous prompts for generation. In Annual Meeting of the
Association for Computational Linguistics , 2021. 2, 3, 4, 7
[26] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao
Wang. Scaling & shifting your features: A new baseline
for efficient model tuning. In Proc. of Advances in Neural
Information Processing Systems , 2022. 3, 4, 7
[27] Dingkang Liang, Xin Zhou, Xinyu Wang, Xingkui Zhu, Wei
Xu, Zhikang Zou, Xiaoqing Ye, and Xiang Bai. Point-
mamba: A simple state space model for point cloud analysis.
arXiv preprint arXiv:2402.10739 , 2024. 2
14715
[28] Haotian Liu, Mu Cai, and Yong Jae Lee. Masked discrimi-
nation for self-supervised learning on point clouds. In Proc.
of European Conference on Computer Vision , 2022. 2, 6, 7
[29] Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong
Pan. Relation-shape convolutional neural network for point
cloud analysis. In Proc. of IEEE Intl. Conf. on Computer
Vision and Pattern Recognition , 2019. 5
[30] Zhe Liu, Tengteng Huang, Bingling Li, Xiwu Chen, Xi
Wang, and Xiang Bai. Epnet++: Cascade bi-directional fu-
sion for multi-modal 3d object detection. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 2022. 1
[31] Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai
Sun, and Rongrong Ji. Cheap and quick: Efficient vision-
language instruction tuning for large language models. In
Proc. of Advances in Neural Information Processing Sys-
tems, 2023. 2, 3
[32] Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, and Yun Fu. Re-
thinking network design and local geometry in point cloud:
A simple residual mlp framework. In Proc. of Intl. Conf. on
Learning Representations , 2022. 6
[33] Yuning Mao, Lambert Mathias, Rui Hou, Amjad Alma-
hairi, Hao Ma, Jiawei Han, Scott Yih, and Madian Khabsa.
Unipelt: A unified framework for parameter-efficient lan-
guage model tuning. In Annual Meeting of the Association
for Computational Linguistics , 2022. 3
[34] Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shub-
ham Tulsiani. Autosdf: Shape priors for 3d completion, re-
construction and generation. In Proc. of IEEE Intl. Conf. on
Computer Vision and Pattern Recognition , 2022. 1
[35] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu,
Yonghong Tian, and Li Yuan. Masked autoencoders for point
cloud self-supervised learning. In Proc. of European Confer-
ence on Computer Vision , 2022. 1, 2, 3, 4, 5, 6, 7, 8
[36] Omid Poursaeed, Tianxing Jiang, Han Qiao, Nayun Xu, and
Vladimir G Kim. Self-supervised learning of point clouds
via orientation estimation. In Intl. Conf. on 3D Vision . IEEE,
2020. 2
[37] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classification
and segmentation. In Proc. of IEEE Intl. Conf. on Computer
Vision and Pattern Recognition , 2017. 6, 7
[38] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. In Proc. of Advances in Neural
Information Processing Systems , 2017. 1, 6, 7
[39] Zekun Qi, Runpei Dong, Guofan Fan, Zheng Ge, Xiangyu
Zhang, Kaisheng Ma, and Li Yi. Contrast with reconstruct:
Contrastive 3d representation learning guided by generative
pretraining. In Proc. of Intl. Conf. on Machine Learning ,
2023. 1, 2, 4, 5, 6, 7
[40] Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng,
Chunrui Han, Zheng Ge, Li Yi, and Kaisheng Ma. Shapellm:
Universal 3d object understanding for embodied interaction.
arXiv preprint arXiv:2402.17766 , 2024. 1, 2
[41] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai,
Hasan Hammoud, Mohamed Elhoseiny, and Bernard
Ghanem. Pointnext: Revisiting pointnet++ with improved
training and scaling strategies. In Proc. of Advances in Neu-
ral Information Processing Systems , 2022. 6[42] Haoxi Ran, Jun Liu, and Chengjie Wang. Surface represen-
tation for point clouds. In Proc. of IEEE Intl. Conf. on Com-
puter Vision and Pattern Recognition , 2022. 6
[43] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua,
Thanh Nguyen, and Sai-Kit Yeung. Revisiting point cloud
classification: A new benchmark dataset and classification
model on real-world data. In Porc. of IEEE Intl. Conf. on
Computer Vision , 2019. 3, 5, 6, 7, 8
[44] Hanchen Wang, Qi Liu, Xiangyu Yue, Joan Lasenby, and
Matt J Kusner. Unsupervised point cloud pre-training via
occlusion completion. In Porc. of IEEE Intl. Conf. on Com-
puter Vision , 2021. 6, 7
[45] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma,
Michael M Bronstein, and Justin M Solomon. Dynamic
graph cnn for learning on point clouds. ACM Transactions
ON Graphics , 2019. 3, 6, 7
[46] Chengzhi Wu, Junwei Zheng, Julius Pfrommer, and J ¨urgen
Beyerer. Attention-based point cloud edge sampling. In
Proc. of IEEE Intl. Conf. on Computer Vision and Pattern
Recognition , 2023. 7
[47] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-
guang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d
shapenets: A deep representation for volumetric shapes. In
Proc. of IEEE Intl. Conf. on Computer Vision and Pattern
Recognition , 2015. 5, 6, 7
[48] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas
Guibas, and Or Litany. Pointcontrast: Unsupervised pre-
training for 3d point cloud understanding. In Proc. of Eu-
ropean Conference on Computer Vision , 2020. 2
[49] Kaixin Xiong, Shi Gong, Xiaoqing Ye, Xiao Tan, Ji Wan,
Errui Ding, Jingdong Wang, and Xiang Bai. Cape: Camera
view position embedding for multi-view 3d object detection.
InProc. of IEEE Intl. Conf. on Computer Vision and Pattern
Recognition , 2023. 2
[50] Li Yi, Vladimir G Kim, Duygu Ceylan, I-Chao Shen,
Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Shef-
fer, and Leonidas Guibas. A scalable active framework for
region annotation in 3d shape collections. ACM Transactions
ON Graphics , 2016. 6, 7
[51] Dongshuo Yin, Yiran Yang, Zhechao Wang, Hongfeng Yu,
Kaiwen Wei, and Xian Sun. 1% vs 100%: Parameter-
efficient low rank adapter for dense predictions. In Proc.
of IEEE Intl. Conf. on Computer Vision and Pattern Recog-
nition , 2023. 3
[52] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie
Zhou, and Jiwen Lu. Point-bert: Pre-training 3d point cloud
transformers with masked point modeling. In Proc. of IEEE
Intl. Conf. on Computer Vision and Pattern Recognition ,
2022. 2, 3, 5, 6, 7
[53] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit:
Simple parameter-efficient fine-tuning for transformer-based
masked language-models. In Annual Meeting of the Associ-
ation for Computational Linguistics , 2022. 7
[54] Yaohua Zha, Jinpeng Wang, Tao Dai, Bin Chen, Zhi Wang,
and Shu-Tao Xia. Instance-aware dynamic prompt tuning for
pre-trained point cloud models. In Porc. of IEEE Intl. Conf.
on Computer Vision , 2023. 3, 4, 5, 6, 7
14716
[55] Yaohua Zha, Huizhen Ji, Jinmin Li, Rongsheng Li, Tao Dai,
Bin Chen, Zhi Wang, and Shu-Tao Xia. Towards compact
3d representations via point feature enhancement masked au-
toencoders. In Proc. of the AAAI Conf. on Artificial Intelli-
gence , 2024. 2
[56] Dingyuan Zhang, Dingkang Liang, Zhikang Zou, Jingyu Li,
Xiaoqing Ye, Zhe Liu, Xiao Tan, and Xiang Bai. A sim-
ple vision transformer for weakly semi-supervised 3d object
detection. In Porc. of IEEE Intl. Conf. on Computer Vision ,
2023. 2
[57] Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bin
Zhao, Dong Wang, Yu Qiao, and Hongsheng Li. Point-m2ae:multi-scale masked autoencoders for hierarchical point cloud
pre-training. In Proc. of Advances in Neural Information
Processing Systems , 2022. 2, 4, 6, 7
[58] Zixiang Zhou, Xiangchen Zhao, Yu Wang, Panqu Wang, and
Hassan Foroosh. Centerformer: Center-based transformer
for 3d object detection. In Proc. of European Conference on
Computer Vision , 2022. 2
[59] Yaoming Zhu, Jiangtao Feng, Chengqi Zhao, Mingxuan
Wang, and Lei Li. Counter-interference adapter for multi-
lingual machine translation. In Proc. of Conf. on Empirical
Methods in Natural Language Processing , 2021. 3
14717
