ZONE : Zero-Shot Instruction-Guided Local Editing
Shanglin Li1*, Bohan Zeng1*, Yutang Feng1*, Sicheng Gao1Xiuhui Liu1, Jiaming Liu2,
Lin Li2, Xu Tang2, Yao Hu2, Jianzhuang Liu4, Baochang Zhang1,3,5†
1Beihang University2Xiaohongshu Inc3Nanchang Institute of Technology, China
4Shenzhen Institute of Advanced Technology, China5Zhongguancun Laboratory, China
(Change)(a) Editing Results(b) Comparisons 
Null Text InversionInstructPix2Pix
Blended Latent DiffusionText2LIVEOriginalZONE (Ours)Wear the dog a hat.“Make his tie blue” 
Make the corgi red.Get off his hat.
+ Edited Layer+ Edited Layer
+ Edited LayerZONEUndo all changes.(Add)
(Remove)
Difference MapDifference MapInstruction
Figure 1. We propose ZONE , a zero-shot instruction-guided local editing approach. Our key idea is to edit and locate precise editing
regions in an image with intuitive textual instructions. We demonstrate a multi-turn editing example in (a) and compare the difference
maps between the edited image and the original image in (b) to highlight our method’s ability for local editing.
Abstract
Recent advances in vision-language models like Stable
Diffusion have shown remarkable power in creative image
synthesis and editing. However, most existing text-to-image
editing methods encounter two obstacles: First, the text
prompt needs to be carefully crafted to achieve good results,
which is not intuitive or user-friendly. Second, they are in-
sensitive to local edits and can irreversibly affect non-edited
regions, leaving obvious editing traces. To tackle these
problems, we propose a Zero-shot instructiON-guided lo-
cal image Editing approach, termed ZONE . We first convert
the editing intent from the user-provided instruction (e.g.,
“make his tie blue”) into specific image editing regions
through InstructPix2Pix. We then propose a Region-IoU
*These authors contributed equally.
†Corresponding author: bczhang@buaa.edu.cnscheme for precise image layer extraction from an off-the-
shelf segment model. We further develop an edge smoother
based on FFT for seamless blending between the layer and
the image. Our method allows for arbitrary manipulation
of a specific region with a single instruction while preserv-
ing the rest. Extensive experiments demonstrate that our
ZONE achieves remarkable local editing results and user-
friendliness, outperforming state-of-the-art methods. Code
is available at https://github.com/lsl001006/
ZONE .
1. Introduction
Large-scale vision-language models, such as Stable Diffu-
sion [42], DALL·E 2 [41], and Imagen [45], have revolu-
tionized text-guided image editing by bridging the gap be-
tween natural language and image content. Trained on vast
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
6254
visual and textual data, these methods harness generative
power to manipulate appearance and style in natural im-
ages, offering a wide array of possibilities for enhancing
and manipulating images in domains such as photography,
advertising, and social media. These advancements have
opened up new possibilities for text-guided image editing,
making it increasingly important in various applications.
State-of-the-art (SOTA) image generative techniques
[36, 41, 42, 53] predominantly concentrate on stylization,
where the desired appearance is determined by a reference
image or textual description, often leading to global image
alterations [25, 29, 47]. However, these methods often lack
straightforward local editing capabilities, and the precise
localization of these edits typically needs additional input
guidance, such as segmentation masks [1, 13, 32], making
text-driven editing cumbersome and potentially limiting its
scope. Recent description-guided works1like Prompt-to-
Prompt [14], DiffEdit [8], and Text2LIVE [3] make note-
worthy contributions to mask-free local edits, but they ei-
ther require complex textual descriptions ( e.g., Prompt-
to-Prompt requires word-to-word alignment between the
source image caption and the edited image caption, and
DiffEdit uses query and reference prompts) or need to spec-
ify the edited object ( e.g., Text2LIVE asks for multiple
prompts), which are not user friendly. Instruction-guided
editing methods2[5, 10, 55, 58] present more elegant char-
acteristics in this regard. They eliminate the need for image-
anchored descriptions, requiring only descriptions of the de-
sired edits ( e.g., “make it snowy”), which facilitates con-
cise and intuitive expression. However, these methods suf-
fer from the over-edit problem, potentially distorting high-
frequency details in non-edited regions (see Fig. 1 (b)).
To tackle these problems, we propose ZONE , aZero-shot
instructi ON-guided local image Editing approach. ZONE
provides a more flexible and creative way to manipulate real
images with layers.
Specifically, we leverage the pretrained instruction-
guided model, InstructPix2Pix (IP2P) [5], for image edit-
ing. By exploring the attention mechanism of IP2P, we un-
cover the implicit associations between the editing locations
and user-provided instructions in instruction-guided mod-
els. This allows us to identify the locations of the edited
objects in instructions without the need for extra specifica-
tion ( e.g., Stable Diffusion-based methods have to specify
the tokens of the objects to edit). We further enhance this
capability by proposing a Region-IoU scheme in conjunc-
tion with SAM [28], ensuring the mask refinement of the
edited image layer. Our ZONE allows arbitrary image edit-
ing actions like “add”, “remove”, and “change”, all accom-
plished with intuitive instructions. Additionally, ZONE sup-
ports multi-turn local editing without affecting non-edited
1In this paper, we call them description-guided diffusion models.
2In this paper, we call them instruction-guided diffusion models.regions, empowering high-fidelity local editing without any
training or fine-tuning. Comprehensive experiments and
user studies demonstrate that ZONE achieves remarkable re-
sults and user-friendliness in local image editing, outper-
forming existing SOTA methods.
To summarize, we make the following key contributions:
• We propose ZONE , a zero-shot image local editing
method that enables users to edit localized regions of both
real and synthetic images with simple instructions. ZONE
preserves non-edited regions without loss and allows ar-
bitrary manipulation of edited image layers.
• We reveal and exploit the different attention mechanisms
between IP2P and Stable Diffusion when processing user
instructions for image editing, with intuitive visual com-
parisons.
• We present a novel Region-IoU scheme and incorporate
it with SAM for effective edited region refinement, and
introduce a Fourier transform-based edge smoother to re-
duce the artifacts when compositing the image layers.
• Comprehensive experiments and user studies demon-
strate that ZONE achieves high-fidelity local editing re-
sults without any auxiliary prompts, outperforming SOTA
methods in photorealism and content preservation.
2. Related Work
2.1. Generative Models for Image Manipulation
Image manipulation is a fundamental process within the
realm of computer vision, involving altering images with
the aid of additional conditions like textual prompts, la-
bels, masks, or reference images. Two mainstream editing
methods include Generative Adversarial Networks (GANs)
and Diffusion Models (DMs). Typical image manipulation
tasks comprise image-to-image translation [7, 20, 26, 44,
47, 52, 59], super-resolution [12, 21, 30, 54], inpainting
[19, 32, 39, 42], colorization [4, 31, 35, 51], and more. Al-
though GAN-based methods excel when dealing with care-
fully curated data, they struggle with extensive and hetero-
geneous datasets [22, 23, 34]. To enhance generative ex-
pressiveness, [16, 17, 42, 48, 49] utilize DMs to achieve
high-quality generation over diverse datasets. Recent re-
search has yielded promising generation outcomes through
the training or fine-tuning of large-scale text-to-image mod-
els [5, 18, 24, 33, 36, 41, 45, 53], as well as by har-
nessing CLIP [40] embeddings to guide image manipula-
tion using textual prompts [9, 25, 29]. Some prior works
[1, 2, 14, 38, 50] also demonstrate the zero-shot editing
capability of pretrained DMs. Similarly, our method ex-
tensively exploits a pretrained DM’s generative capability
to facilitate diverse and stylized image editing. However,
we uniquely explore the implicit relationship between the
DM’s editing regions during generation and the whole user
instructions, enabling fine-grained layer-specific position-
6255
ing.
2.2. Localized Image Editing
Several recent works have made attempts at localized im-
age editing. Blend Diffusion [1] proposes a mask-guided
method by blending edited regions with the other parts of
the image at different noise levels along the diffusion pro-
cess. Text2LIVE [3] introduces an RGBA layer generation
approach with a CLIP-supervised generator for perform-
ing edits of objects in real images and videos. Prompt-
to-Prompt [14] controls the spatial layouts of the image
corresponding to the words in the prompt through cross-
attention modification, enabling local edits by modifying
textual prompts. Pix2Pix-Zero [38] preserves the structure
of the original image with cross-attention guidance and ap-
plies an edit-direction embedding to make changes to local-
ized objects. Instruction-based editing methods like IP2P
[5] and MagicBrush [55] are trained or finetuned on triplet
datasets to realize intuitive high-quality image editing based
on user-provided instructions. PAIR-diffusion [13] allows
editing the structure and appearance of each masked part in
the original image independently. While these methods pro-
duce impressive results within their specific applications,
they compromise on local image editing: instruction-guided
methods [5, 55] and attention-based modifications [14, 38]
introduce artifacts to non-edited regions, mask-based meth-
ods [1, 13] add complexity to user interactions, and CLIP-
based methods [3, 38] sacrifice the flexibility of natural lan-
guage editing. In contrast, our ZONE requires only a single
instruction to achieve high-fidelity local image editing with
an image layer.
2.3. Instruction-Guided Editing
Despite the significant progress of text-to-image models,
most require detailed textual descriptions [36, 41–43, 45]
to convey the desired image content, often falling short
of user expectations for image editing. In contrast, direct
instruction-guided modifications of target regions/attributes
offer a more intuitive and convenient approach, such as
“make the girl smile” and “give him a ball.” Recent ad-
vancements in instruction-guided editing and generation
[5, 10, 37, 55, 57, 58] have made notable progress. For in-
stance, IP2P [5] employs GPT-3 [6] and Prompt-to-Prompt
[14] to synthesize an instruction-editing dataset, utilizes a
pretrained Stable Diffusion model [42] for weight initializa-
tion, and trains a diffusion model specialized in instruction-
guided editing. MagicBrush [55] fine-tunes IP2P using a
real image dataset, thereby demonstrating a superior per-
formance in instruction-guided editing. In this paper, we
aim to leverage the instruction-editing capability of these
pretrained instruction-guided diffusion models to eliminate
the need for additional masks in previous local editing ap-
proaches [1, 2, 36], enabling flexible and high-fidelity localediting based on a single user-provided instruction.
3. Preliminaries
Diffusion Models. Diffusion models [16, 46, 48] are
probabilistic generative models founded on two comple-
mentary stochastic processes: diffusion anddenoising . The
diffusion process progressively adds different amounts of
Gaussian noise to a clean image x0towards Gaussian dis-
tribution xT∼ N (0, I)inTtimesteps: xt=√αtx0+√1−αtϵ, where αtdefines the level of noise, and ϵ∼
N(0, I).
In the denoising process, a neural network ϵθis de-
signed to predict the noise ϵforxtto get a “cleaner”
image gradually. This process is achieved by minimiz-
ing the denoising objective: L=Ex0,t,ϵ∥ϵ−ϵθ(xt, t)∥2
2.
Rombach et al. [42] introduce a latent diffusion model
(LDM), which speeds up both processes by reducing images
into a lower-dimensional latent space utilizing a variational
auto-encoder [27]. This advancement has underpinned the
achievements of Stable Diffusion, serving as the fundamen-
tal model for many diffusion-based works.
InstructPix2Pix. InstructPix2Pix [5] (IP2P) is a pioneer-
ing conditional diffusion model that edits images from user-
provided instructions. Specifically, IP2P constructs an in-
struction dataset to fine-tune the pretrained Stable Diffu-
sion. Given a target image x, an image condition cI, and a
textual instruction condition cT, IP2P projects xto the latent
z=E(x)with a pretrained encoder E, and then fine-tunes
Stable Diffusion by minimizing the following objective:
L=
EE(x),E(cI),cT,ϵ∼N (0,1),t[∥ϵ−ϵθ(zt, t,E(cI), cT))∥2
2],
(1)
where the denoising network ϵθaccepts two input condi-
tions and predicts the noise ϵ. IP2P also finds it beneficial to
perform classifier-free guidance [15] concerning both con-
ditions, thus controlling the strength of edit by image guid-
ance scale sIand instruction guidance scale sT:
˜ϵθ(zt, cI, cT) =ϵθ(zt,∅,∅)
+sI·(ϵθ(zt, cI,∅)−ϵθ(zt,∅,∅))
+sT·(ϵθ(zt, cI, cT)−ϵθ(zt, cI,∅)).
(2)
At inference time, IP2P can modify an image with a user-
provided instruction and trade-off the generated sample ac-
cording to the strengths of the guidance image and the edit
instruction through sIandsT.
4. Method
Overview of ZONE .We aim to make localized edits on an
image with simple instructions. As depicted in Fig. 1 (a),
6256
(2) Mask Refinement
“Make the dog a golden statue”(1) Instruction-Guided Localization
Generated Flaws
Fusion
SAMRegion-IoU
(3) Layer Blending
Edge Smoother
Text tokens𝑧!𝑧!"#QKVQKV𝑇!𝑇"𝑇#
Description-guided𝑇#𝑇$𝑇%Cross Attention MapsCross Attention Maps
Instruction-guidedDenoising NetworkQKVCross-AttentionNoisy Latent𝑇!𝑇"𝑇#
(a)(b)
ActionClassifier
𝑧!𝑧!"#∗𝑧!"#'IP2PMagicBrush
Fuse & Upscale(c)
Max…Token-awareEdit-awareSeg 1Seg2Seg N…SAMSeg i∩
Seg i∪
allrIoUN= rIoUrIoU2rIoU1Segk
Region-IoUdilateedgeEdge Smoother
⊖
Close&Fill
(1)(2)
(3)
Trainable
Frozen
…timestep
Merge
𝑧!"#ActionClassifier
⨀⨀𝑓,𝑓"#⨀
𝑓,
𝑔𝑒
CompositeOperation
Composite
ActionClassifier
Fused IP2PFinal edited result
“”“”“”∅Figure 2. Overview of ZONE . (a) Three modules in ZONE . (b) The distinct difference between description-guided and instruction-guided
diffusion models on cross-attention. The former usually follows a token-aware format, while the latter is edit-aware .∅denotes the
unconditional embeddings for null input. (c) Implementation details of the modules shown in (a).
such edits include performing three primary actions: (i)
“add”: add an object to the image without specifying lo-
cation with user-provided masks; (ii) “remove”: remove the
object in the scene; (iii) “change”: change the style ( i.e.,
texture) of an existing object or replace the object with an-
other object. Additionally, our method allows high-fidelity
multi-turn edits with a series of instructions.
As outlined in Fig. 2 (a), our approach consists of the
following steps: First, we train an action classifier for steer-
ing different editing requirements and concurrently gener-
ate and position the editing region using a fused IP2P, as
detailed in Section 4.2 and Fig. 2 (c). Second, we devise a
mask refinement module for an edited image layer in Sec-
tion 4.3. Finally, in Section 4.4, we propose an FFT-based
edge smoother for seamless blending of the edited image
layer with the original image.
4.1. Problem Statement
Given an RGB image IG∈R3×H×Wand a textual instruc-
tionTI, we aim to locate and edit image regions following
TIand maintain the original non-edited regions. Inspired by
Text2LIVE [3], we extract an edited layer ILwith color and
opacity that are composited over IG. As opposed to previ-
ous works [1, 3, 14, 38], we neither rely on any user-defined
mask nor need non-intuitive prompt engineering, realizingprecise local editing and seamless layer blending.
4.2. Instruction-Guided Localization
Many local editing methods require users to explicitly spec-
ify the object they want to edit with a prompt or a mask
[1, 3, 8, 38]. This is not intuitive and often requires a certain
learning cost. Our approach locates and edits the implicitly
designated object from the user’s instruction. For example,
a user-provided instruction like “make her old” can implic-
itly convey the user’s editing intent to modify the woman in
the scene ( locate ) by making her appear older ( edit).
As shown in Fig. 2 (b), our key finding is that the oper-
ational mechanisms of instruction-guided and description-
guided diffusion models on cross-attention exhibit a dis-
tinct difference. Specifically, we empirically demonstrate
that: (i) a description-guided model displays a token-aware
characteristic on its cross-attention maps, associating each
input text token with a corresponding spatial structure; (ii)
an instruction-guided model’s cross-attention maps with
unconditional embeddings share similar spatial features,
demonstrating an edit-aware characteristic, being respon-
sive to the overall editing intent.
Given a noisy latent ztand a textual embedding cT, the
denoising UNet ϵθpredicts the noise ϵat each timestep
t. The generation is conditioned on the textual prompt TI
6257
IP2PSDGivethesheepahat</start-of-text><end-of-text/>
</start-of-text>Makethedogagoldenstatue<end-of-text/>IP2PSD
Text Tokens:
Text Tokens:∅
∅Figure 3. Cross-attention map difference. We average the cross-
attention maps among all timesteps for each sample. IP2P shows
consistency in the overall editing intent with unconditional embed-
dings∅, while Stable Diffusion (SD) demonstrates a one-to-one
correspondence with text tokens.
by computing cross-attention between the textual embed-
dingcTand the spatial features ϕ(zt), and updates ϕ(zt)as
ˆϕ(zt):
M= Softmax(QKT
√
d),ˆϕ(zt) =M·V, (3)
where the query Q=WQϕ(zt), key K=WKcT, and
value V=WVcTare obtained with linear projections WQ,
WK, andWV.M∈RH′×W′×Lcontains Lcross-attention
maps that are correlated to the similarity between Qand
K. Typically H′andW′are1/32of the original image
sizeHandWin Stable Diffusion. For the description-
guided Stable Diffusion model, each token corresponds to
its specific attention map Mlwith text embeddings, where
l∈ {1,2, . . . , L }. For the instruction-guided IP2P, we find
its attention maps share a uniform characteristic with un-
conditional embeddings, concentrated directly at the edited
location without token specification, as visualized in Fig. 3.
Based on this finding, we devise a simple yet effec-
tive localization module that semantically locates the edited
region with instruction TI. Specifically, we first collect
the attention maps of the denoising model of IP2P from
all timesteps of the denoising process. Then, we aver-
age and resize the maps to obtain averaged attention maps
MA∈RH×W×L. We observe that the first attention map
M1
Aprimarily emphasizes the attention weights of the non-
edited region. Subsequent attention maps M2,...,L
A shift at-
tention towards the edited region. Therefore, we subtract
the last cross-attention map from the first attention map and
binarize each pixel (m, n)with a fixed threshold Tto high-
light the edited region and mitigate the background noise:
Mb(m, n) =(
1,ifM1
A(m, n)− ML
A(m, n)< T,
0,others,
(4)where Tis empirically set to 128. This yields a rough,
noise-filtered edited region mask Mbmost related to TI
(see Fig. 2 (a)).
Moreover, we find that IP2P performs not as well as
MagicBrush in the “remove” editing but preserves better
object identity in terms of “add” and “change”. Therefore,
we design a fused IP2P module with a trainable action clas-
sifierAI. As illustrated in Fig. 2 (c), we lock the weights
of both IP2P and MagicBrush and use a pretrained action
classifier AIto steer the denoising process based on TI:
zt−1= (z∗
t−1+β·z′
t−1)/(1 +β), (5)
where z∗
t−1andz′
t−1are the denoised latents by IP2P and
MagicBrush, respectively. βis a hyperparameter to control
the guidance strength of MagicBrush on IP2P, empirically
set to 0.2ifAI(TI)is classified to “remove” and 0.01for
other actions. This module generates a globally edited im-
ageIstyaccording to TI.Istyserves as the canvas, from
which the edited region is cropped out to form a separate
image layer in the following steps.
4.3. Mask Refinement
The location mask MbandIstyobtained in Section 4.2 are
insufficient for precise local editing, since Mbonly indi-
cates the general location of the edited region, as illustrated
in Fig. 2 (a). An intuitive and effective mask refinement
method is to use an off-the-shelf segmentation model. We
leverage the Segment Anything Model (SAM) [28] to gen-
erate precise masks of the canvas Istyat various levels.
However, we do not use SAM’s preset point or box prompts
for segmentation selection, because these prompts could po-
tentially lead to misselection or omission of SAM’s seg-
mentation results due to IP2P’s over-edit problem (which is
also reflected in Mb, seeIstyandMbin Fig. 2 (a)), result-
ing in a final mask that does not accurately reflect TI’s edit-
ing intention. Therefore, we propose a Region-IoU (rIoU)
scheme to obtain the accurate segmentation mask.
As depicted in Fig. 2 (c), by sending Istyto SAM, we
extract all the possible instance segments S={Sj}N
j=1.
Note that Scontains the segments from all levels of SAM’s
segmentation. We define rIoU R(j)as:
R(j) =area(Sj∩ M b)
area(Sj∪ M b), j= 1,2, . . . , N. (6)
Ifk= arg max
j=1,2,...,N{R(j)}, then we obtain the refined mask
Mf=Sk. One example is shown in Fig. 2 (a) or (c).
4.4. Layer Blending
After the mask refinement, we obtain an edited image layer
I′
L=Isty⊙ M f, which retains the color information of
Istywithin the region where Mf= 1, with the rest being
6258
Type Methods L1↓ L2↓ LPIPS ↓CLIP-I ↑CLIP-T ↑
Description-guidedDiffEdit [8] 0.0426 0.0099 0.1695 0.8947 0.2815
Text2LIVE [3] 0.0511 0.0075 0.2176 0.9075 0.3062
Pix2Pix-Zero [38] 0.1198 0.0342 0.4375 0.7679 0.2701
Instruction-guidedInstructPix2Pix [5] 0.0945 0.0274 0.2816 0.9089 0.2907
MagicBrush [55] 0.0919 0.0378 0.2903 0.8959 0.2939
ZONE (Ours) 0.0146 0.0061 0.0441 0.9688 0.2969
Table 1. Quantitative evaluation. We use L1 and L2 to gauge pixel-level structural similarity, LPIPS and CLIP-I to evaluate image quality,
and CLIP-T to assess text-image semantic similarity. The best and the second best results are marked in bold and underline , respectively.
(a) w/ 𝑒(b) w/o 𝑒“Wear him a hat.”
Figure 4. Visualization and ablation. The first 4 columns show
the intermediate results related to the edge smoother. The last col-
umn compares the final edited results with and without the edge
smoother.
transparent. A na ¨ıve way to get the final edited result ICis
to stitch I′
Land the original image IGat pixel-level. This
fundamentally tackles the over-edit problem encountered in
instruction-guided methods for local editing. Nevertheless,
directly pasting I′
Lback to IGmay result in noticeable ar-
tifacts, such as jagged edges and incomplete coverage of
the edited region in the original image, as indicated by the
yellow arrows in Fig. 4 (b).
We tackle this problem by designing a novel edge
smoother with Fast Fourier Transform (FFT). Given the
original image IG, the canvas Isty, and the refined lo-
cation mask Mf, we first dilate MftoMdto incorpo-
rate more edge information in Istythat may not be in-
cluded in I′
L. Then we get the dilated edited image layer
IL,d=Isty⊙ M dand the dilated original image layer
IG,d=IG⊙M d, as shown in the second column of Fig. 4.
The edge smoother eis defined by:
e(IL,d,IG,d) =g(f−1(H(f(IL,d))− H(f(IG,d)))),(7)
where gis a composition of binarization and morphological
closing and filling functions, fandf−1represent FFT and
inverse FFT, respectively, and His an ideal low-pass filter:
H(fs) =(
fs(c),if∥c−c0∥2≤D0,
0, if∥c−c0∥2> D 0,(8)
where fs∈RH×Wis the frequency spectrum of the imagetransformed by f,cis the coordinate in fs,c0is the cen-
ter coordinate of fs, and D0is set empirically to 200for a
512×512image. We use the edge smoother eto get the
final mask M∗
f.
As shown in the second column of Fig. 4, we observe that
bothIG,dandIL,dshare similar low-frequency characteris-
tics on non-edited regions ( e.g., background), but they hold
different low-frequency characteristics on the edited regions
(e.g., hat and the shadow below it). Therefore, we can ex-
clude the non-edited regions and retain the edited regions
by subtracting the low-frequency components between IL,d
andIG,din the frequency domain: ds=H(f(IL,d))−
H(f(IG,d))and invert it back to the image domain to get
the difference mask Mdm=f−1(ds). The final mask
M∗
fis then obtained by M∗
f=g(Mdm) =e(IL,d,IG,d).
Finally, we get the final edited image layer ILbyIL=
Isty⊙ M∗
f, and the final edited result ICis acquired by
compositing IGandIL. The intuitive visualization of these
intermediate results are shown in Fig. 4.
The implementation details and more discussions can be
found in the supplementary material.
5. Experiments
5.1. Experimental Setup
Baselines. We conduct comprehensive experiments for
the local editing task by comparing ZONE with five state-
of-the-art image editing methods that are capable of local
editing: Text2LIVE [3],DiffEdit [8],IP2P [5],Pix2Pix-Zero
[38], and MagicBrush [55]. The implementation of these
methods can be found in the supplementary material.
Datasets. We randomly select and annotate 100 samples
for evaluation, including 60 real images from the Internet
and 40 synthetic images. To ensure the representativeness
of the evaluation, we consider the diversity of scenes and
objects in the sample selection. In particular, we divide the
test set into three categories: 32 images for “add”, 54 for
“change”, and 14 for “remove” actions. All these 100 im-
ages are listed in the supplementary material.
Evaluation Metrics. Following [5, 55], we perform qual-
itative and quantitative comparisons using a variety of eval-
6259
ChangeRemoveAdd
Figure 5. Qualitative comparison. We compare the editing efficacy of our ZONE with existing SOTA methods. The instructions (or
instructions that are equivalent to the descriptions required by some baselines) used for editing are written below each row of the images.
uation metrics. Learned Perceptual Image Patch Similarity
(LPIPS) [56] is used to quantify the perceptual similarity
between the original and edited image. CLIP text-image
similarity (CLIP-T) [11] is employed to assess the align-
ment between the edited image and its corresponding cap-
tion, and CLIP image similarity (CLIP-I) is used to evaluate
the layout similarity and semantic correlation between the
edited image and the original image, serving as a reliable
indicator of the edited image’s quality. We also use L1 and
L2 distances for pixel-level difference comparison.
5.2. Comparisons
Quantitative Evaluation. As shown in Table 1, we mea-
sure the models with the five metrics. The quantitative re-sults indicate the following: (i) Our method significantly
outperforms our counterparts on metrics related to im-
age structure and quality, implying the efficacy of ZONE ’s
preservation of the non-edited regions. (ii) Text2LIVE
performs best on CLIP-T, but the qualitative comparison
in Fig. 5 does not support this result. We surmise that
Text2LIVE performs better on this metric potentially due
to its direct supervision by CLIP.
To quantify the stability of the edits, we divide the test set
into three action groups: “change”, “add”, and “remove”.
We then test the CLIP-I and CLIP-T metrics for each model
and plot the CLIP curves in Fig. 6, where the performances
of the same method on these actions are connected with
lines of the same color. Our interpretation is as follows:
6260
RemoveAddChangeAction LegendBetterFigure 6. Stability analysis. We categorize the test set into three
actions (“Remove”, “Add”, and “Change”) and calculate their re-
spective CLIP-I and CLIP-T values. Our method achieves the best
quality-stability trade-off for all actions.
Input ImageOursInstructPix2PixText2LIVE
” Make the basket full of red apples”
Figure 7. Detailed comparison. We show a zoomed-in sample
where ZONE effectively resolves the over-edit problem.
first, the shorter the projection of the line on the axis, the
higher the semantic stability (i.e.,maintaining similar per-
formances under different editing instructions) of the image
editing; second, if the curve is closer to the upper right cor-
ner, it indicates that the method’s editing quality is more
superior. Our method achieves the best trade-off between
quality andstability , demonstrating strong editing stability
and representativeness.
Qualitative Comparsion. In Fig. 5, we illustrate the edit-
ing results for the baselines and our method. We select six
sets of images (including synthetic and real images) and
group them based on actions. Our ZONE shows precise lo-
cal editing capability while preserving the remaining pixels,
this is especially important when there are perceptually im-
portant high-frequency details, such as faces, textures, or
texts. A zoomed-in comparison is shown in Fig. 7. Both
InstructPix2Pix and Text2LIVE introduce distortions to the
non-edited areas during the editing process. For instance,
InstructPix2Pix distorts the nearby clock and paints the or-
ange outside of the basket red. In comparison, Text2LIVE
maintains a better structure but generates a “barrel” of ap-
ples and introduces an obvious foggy effect to the image.Methods SR (%) UPR (%)
DiffEdit [8] 27.1 ±2.7 8.8
Text2LIVE [3] 33.0 ±3.2 17.3
Pix2Pix-Zero [38] 19.2 ±3.7 10.4
InstructPix2Pix [5] 59.8 ±3.1 18.9
MagicBrush [55] 50.2 ±2.9 18.0
ZONE (Ours) 69.4±3.5 26.6
Table 2. Human evaluation. OurZONE obtains the highest suc-
cess rate (SR) and user preference rate (UPR).
Our method, however, can clearly distinguish between the
edited region and the non-edited regions, demonstrating the
best local editing efficacy.
5.3. Human Evaluation
Due to the lack of an effective metric to measure editing ef-
fects (mainly due to the absence of ground truth images af-
ter editing), the metrics mentioned in Section 5.1 alone are
not sufficient to demonstrate the superiority of our method
over existing ones. To further validate the editing effects
ofZONE , in addition to the visual comparison in Fig. 5, we
also conduct a human evaluation to calculate the success
rate (SR) and user preference rate (UPR) of the edited im-
ages with the editing instructions. Table 2 shows a consis-
tent preference for our method by users, as well as a domi-
nant success rate over other methods.
Please refer to our supplementary material for more vi-
sualizations and details of this user study.
6. Conclusion
We present ZONE , a zero-shot instruction-guided local im-
age editing approach, which leverages the localization ca-
pability within the pre-trained instruction-guided diffusion
models. Our approach innovatively utilizes the editing in-
tent regions inherent in the instructions, rather than focus-
ing on individual tokens, eliminating the need for specific
guidance. By integrating the Region-IoU scheme and FFT-
based edge smoother with a pretrained segmentation model,
ZONE effectively realizes precise local editing. Comprehen-
sive experiments and user studies further demonstrate the
superiority of ZONE over SOTA methods.
Acknowledgements. The work was supported by
the National Key Research and Development Program of
China (2023YFC3300029), Zhejiang Provincial Natural
Science Foundation of China (LD24F020007), Beijing Nat-
ural Science Foundation (L223024), National Natural Sci-
ence Foundation of China (62076016), “One Thousand
Plan” projects in Jiangxi Province (Jxsg2023102268), Bei-
jing Municipal Science & Technology Commission, Ad-
ministrative Commission of Zhongguancun Science Park
(Z231100005923035).
6261
References
[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended
diffusion for text-driven editing of natural images. In CVPR ,
2022. 2, 3, 4
[2] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended
latent diffusion. TOG , 2023. 2, 3
[3] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-
ten, and Tali Dekel. Text2live: Text-driven layered image
and video editing. In ECCV , 2022. 2, 3, 4, 6, 8
[4] Marc G ´orriz Blanch, Marta Mrak, Alan F Smeaton, and
Noel E O’Connor. End-to-end conditional gan-based archi-
tectures for image colourisation. In MMSPW , 2019. 2
[5] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structpix2pix: Learning to follow image editing instructions.
InCVPR , 2023. 2, 3, 6, 8
[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. In NeurIPS , 2020. 3
[7] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha,
Sunghun Kim, and Jaegul Choo. Stargan: Unified genera-
tive adversarial networks for multi-domain image-to-image
translation. In CVPR , 2018. 2
[8] Guillaume Couairon, Jakob Verbeek, Holger Schwenk,
and Matthieu Cord. Diffedit: Diffusion-based seman-
tic image editing with mask guidance. arXiv preprint
arXiv:2210.11427 , 2022. 2, 4, 6, 8
[9] Katherine Crowson, Stella Biderman, Daniel Kornis,
Dashiell Stander, Eric Hallahan, Louis Castricato, and Ed-
ward Raff. Vqgan-clip: Open domain image generation and
editing with natural language guidance. In ECCV , 2022. 2
[10] Alaaeldin El-Nouby, Shikhar Sharma, Hannes Schulz, De-
von Hjelm, Layla El Asri, Samira Ebrahimi Kahou, Yoshua
Bengio, and Graham W Taylor. Tell, draw, and repeat: Gen-
erating and modifying images based on continual linguistic
instruction. In CVPR , 2019. 2, 3
[11] Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano,
Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip-
guided domain adaptation of image generators. TOG , 2022.
7
[12] Sicheng Gao, Xuhui Liu, Bohan Zeng, Sheng Xu, Yan-
jing Li, Xiaoyan Luo, Jianzhuang Liu, Xiantong Zhen, and
Baochang Zhang. Implicit diffusion models for continuous
super-resolution. In CVPR , 2023. 2
[13] Vidit Goel, Elia Peruzzo, Yifan Jiang, Dejia Xu, Nicu
Sebe, Trevor Darrell, Zhangyang Wang, and Humphrey Shi.
Pair-diffusion: Object-level image editing with structure-
and-appearance paired diffusion models. arXiv preprint
arXiv:2303.17546 , 2023. 2, 3
[14] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
age editing with cross attention control. arXiv preprint
arXiv:2208.01626 , 2022. 2, 3, 4
[15] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. arXiv preprint arXiv:2207.12598 , 2022. 3
[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In NeurIPS , 2020. 2, 3[17] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,
Mohammad Norouzi, and Tim Salimans. Cascaded diffusion
models for high fidelity image generation. JMLR , 2022. 2
[18] Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi
Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen,
and Liangliang Cao. Diffusion model-based image editing:
A survey. arXiv preprint arXiv:2402.17525 , 2024. 2
[19] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa.
Globally and locally consistent image completion. TOG ,
2017. 2
[20] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adver-
sarial networks. In CVPR , 2017. 2
[21] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,
Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up
gans for text-to-image synthesis. In CVPR , 2023. 2
[22] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks. In
CVPR , 2019. 2
[23] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improving
the image quality of stylegan. In CVPR , 2020. 2
[24] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen
Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:
Text-based real image editing with diffusion models. In
CVPR , 2023. 2
[25] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Dif-
fusionclip: Text-guided diffusion models for robust image
manipulation. In CVPR , 2022. 2
[26] Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee,
and Jiwon Kim. Learning to discover cross-domain relations
with generative adversarial networks. In ICML , 2017. 2
[27] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114 , 2013. 3
[28] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. arXiv preprint arXiv:2304.02643 , 2023. 2, 5
[29] Gihyun Kwon and Jong Chul Ye. Clipstyler: Image style
transfer with a single text condition. In CVPR , 2022. 2
[30] Christian Ledig, Lucas Theis, Ferenc Husz ´ar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-
realistic single image super-resolution using a generative ad-
versarial network. In CVPR , 2017. 2
[31] Jianxin Lin, Peng Xiao, Yijun Wang, Rongju Zhang, and Xi-
angxiang Zeng. Diffcolor: Toward high fidelity text-guided
image colorization with diffusion models. arXiv preprint
arXiv:2308.01655 , 2023. 2
[32] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher
Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting
using denoising diffusion probabilistic models. In CVPR ,
2022. 2
[33] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided
image synthesis and editing with stochastic differential equa-
tions. arXiv preprint arXiv:2108.01073 , 2021. 2
6262
[34] Ron Mokady, Omer Tov, Michal Yarom, Oran Lang, Inbar
Mosseri, Tali Dekel, Daniel Cohen-Or, and Michal Irani.
Self-distilled stylegan: Towards generation from internet
photos. In SIGGRAPH , 2022. 2
[35] Kamyar Nazeri, Eric Ng, and Mehran Ebrahimi. Image col-
orization using generative adversarial networks. In AMDO ,
2018. 2
[36] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,
Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya
Sutskever, and Mark Chen. Glide: Towards photorealis-
tic image generation and editing with text-guided diffusion
models. In ICML , 2022. 2, 3
[37] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al. Training lan-
guage models to follow instructions with human feedback.
InNeurIPS , 2022. 3
[38] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun
Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image
translation. In SIGGRAPH , 2023. 2, 3, 4, 6, 8
[39] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A Efros. Context encoders: Feature
learning by inpainting. In CVPR , 2016. 2
[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML , 2021. 2
[41] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022. 1, 2, 3
[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , 2022. 1, 2, 3
[43] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In CVPR , 2023. 3
[44] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,
Jonathan Ho, Tim Salimans, David Fleet, and Mohammad
Norouzi. Palette: Image-to-image diffusion models. In SIG-
GRAPH , 2022. 2
[45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. In NeurIPS , 2022. 1, 2, 3
[46] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In ICML , 2015. 3
[47] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro
Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang,
Glenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image
generation in any style. arXiv preprint arXiv:2306.00983 ,
2023. 2[48] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 2, 3
[49] Yang Song and Stefano Ermon. Generative modeling by es-
timating gradients of the data distribution. In NeurIPS , 2019.
2
[50] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali
Dekel. Plug-and-play diffusion features for text-driven
image-to-image translation. In CVPR , 2023. 2
[51] Hanzhang Wang, Deming Zhai, Xianming Liu, Junjun Jiang,
and Wen Gao. Unsupervised deep exemplar colorization via
pyramid dual non-local attention. TIP, 2023. 2
[52] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong
Chen, Qifeng Chen, and Fang Wen. Pretraining is all
you need for image-to-image translation. arXiv preprint
arXiv:2205.12952 , 2022. 2
[53] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-
jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-
fei Yang, Burcu Karagol Ayan, et al. Scaling autoregres-
sive models for content-rich text-to-image generation. arXiv
preprint arXiv:2206.10789 , 2022. 2
[54] Zongsheng Yue, Jianyi Wang, and Chen Change
Loy. Resshift: Efficient diffusion model for image
super-resolution by residual shifting. arXiv preprint
arXiv:2307.12348 , 2023. 2
[55] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su.
Magicbrush: A manually annotated dataset for instruction-
guided image editing. arXiv preprint arXiv:2306.10012 ,
2023. 2, 3, 6, 8
[56] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR , 2018. 7
[57] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-
Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio
Savarese, Stefano Ermon, et al. Hive: Harnessing human
feedback for instructional visual editing. arXiv preprint
arXiv:2303.09618 , 2023. 3
[58] Tianhao Zhang, Hung-Yu Tseng, Lu Jiang, Weilong Yang,
Honglak Lee, and Irfan Essa. Text as neural operator: Image
manipulation by text instruction. In ACMMM , 2021. 2, 3
[59] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In ICCV , 2017. 2
6263
