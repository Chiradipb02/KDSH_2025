From-Ground-To-Objects: Coarse-to-Fine Self-supervised Monocular Depth
Estimation of Dynamic Objects with Ground Contact Prior
Jaeho Moon Juan Luis Gonzalez Bello Byeongjun Kwon Munchurl Kim
Korea Advanced Institute of Science and Technology
{jaeho.moon, juanluisgb, kbj2738, mkimee }@kaist.ac.kr
https://kaist-viclab.github.io/From_Ground_To_Objects_site/
Abstract
Self-supervised monocular depth estimation (DE) is an
approach to learning depth without costly depth ground
truths. However, it often struggles with moving objects that
violate the static scene assumption during training. To ad-
dress this issue, we introduce a coarse-to-fine training strat-
egy leveraging the ground contacting prior based on the ob-
servation that most moving objects in outdoor scenes con-
tact the ground. In the coarse training stage, we exclude
the objects in dynamic classes from the reprojection loss
calculation to avoid inaccurate depth learning. To provide
precise supervision on the depth of the objects, we present a
novel Ground-contacting-prior Disparity Smoothness Loss
(GDS-Loss) that encourages a DE network to align the
depth of the objects with their ground-contacting points.
Subsequently, in the fine training stage, we refine the DE
network to learn the detailed depth of the objects from the
reprojection loss, while ensuring accurate DE on the mov-
ing object regions by employing our regularization loss with
a cost-volume-based weighting factor. Our overall coarse-
to-fine training strategy can easily be integrated with ex-
isting DE methods without any modifications, significantly
enhancing DE performance on challenging Cityscapes and
KITTI datasets, especially in the moving object regions.
1. Introduction
Recent neural-network-based methods for depth estimation
(DE) from 2D images have shown promising performance
as the demand for 3D information grows with navigation,
robotics, and AR/VR applications. While supervised meth-
ods [2, 10, 29, 33, 45] utilize expensive and sparse ground
truth (GT) depth for training, self-supervised methods learn
depth by minimizing photometric errors (or reprojection
loss) between the target frame and the warped frames from
stereo images [11, 12, 14, 15, 48, 50] or consecutive frames
in monocular videos [13, 17, 51, 54].
MonoViT [54] 
Ours-MonoViT
Depth Map Error Map
Point Cloud Reconstruction
Depth Map Error Map
Point Cloud ReconstructionImage
 Ground Truth
0.01.0
0.01.0ܽଵ: 0.661
ܽଵ: 0.913ݏܾܽ _݈݁ݎ :0.422
ݏܾܽ _݈݁ݎ :0.095Figure 1. The effect of our coarse-to-fine self-supervised DE train-
ing strategy applied to MonoViT [54]. In the error maps, blue in-
dicates small errors, and red indicates large errors.
In the pipeline of self-supervised depth learning for
monocular videos, a DE network often learns inaccurate
depth of moving objects that break the static scene as-
sumption but still minimize the reprojection loss for self-
supervision [13, 17, 18, 27]. To handle the moving object
problem, the ‘automasking’ [13] is proposed to exclude the
static pixels in sequential frames from the reprojection loss
calculation. More recent work suggested to estimate 3D ob-
ject motions [22, 30–32] or to disentangle object motion for
cost volume construction [9]. However, these methods con-
front inherent ambiguities in learning the depth and motion
of the objects without precise supervision.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10519
In this paper, we introduce a ground contacting prior to
effectively resolve the depth ambiguity of moving objects
in a self-supervised manner. The ground contacting prior is
based on an observation that most objects classified as dy-
namic in outdoor scenes, such as cars, bicycles, or pedes-
trians, invariably tend to make contact with the ground,
thereby sharing similar depth at their ground contact points.
Leveraging this crucial insight, we introduce a coarse-
to-fine training strategy for self-supervised monocular DE.
In the initial coarse training stage, we train a DE network
by employing the novel Ground-contacting-prior Disparity
Smoothness Loss (GDS-Loss) that aligns the depth of ob-
jects in dynamic classes with the depth of their ground con-
tact points. Also, we exclude the objects from the repro-
jection loss computation to avoid learning inaccuracies in
DE. We further fine-tune the DE network in the fine training
stage for learning to capture detailed depth over the surfaces
of objects in dynamic classes by employing the reprojection
loss for the whole image region. We introduce a regulariza-
tion loss that allows the DE network to learn the detailed
depth of non-moving objects by the reprojection loss while
avoiding inaccurate learning of DE on the moving object
regions with a cost-volume-based weighting factor.
As shown in Fig. 1, MonoViT [54] predicts inaccurate
depth that make cars float (green box) or sunken under the
ground (red box) in the reconstructed point cloud. On the
other hand, MonoViT trained with our coarse-to-fine train-
ing strategy, denoted as Ours-MonoViT, predicts the accu-
rate depth of the cars, so that they are appropriately standing
on the ground in the reconstructed point cloud. In summary,
our contributions are three-fold:
• We, for the first time, propose to utilize the ground con-
tacting prior as a self-supervision for DE on objects in dy-
namic classes, presenting the novel Ground-contacting-
prior Disparity Smoothness Loss (GDS-Loss).
• We introduce a regularization loss with a cost-volume-
based weighting factor, which allows fine-tunning with
the reprojection loss while ensuring adherence to the
ground contacting prior on the moving object regions.
• Our coarse-to-fine training strategy can easily be in-
tegrated into the existing DE networks, boosting their
DE performance to achieve state-of-the-art results on the
challenging Cityscapes and Kitti datasets.
2. Related Work
2.1. Self-supervised Monocular Depth Estimation
Since measuring the GT depth is expensive, especially in
outdoor environments, self-supervised learning of DE has
been intensively studied [7, 11–13, 15, 20, 40, 43, 47, 51,
55]. Without the GT depth, neural networks (NN) have been
adopted to learn depth from stereo images [14, 41, 42, 50,57] or sequential video frames [4, 18, 25, 46, 53] by mini-
mizing the photometric errors, or reprojection loss, between
a target view and the warped reference view based on the
predicted depth. Since the usage of stereo images inherently
has limitations such as requirements of synchronized binoc-
ular cameras and calibration, self-supervised learning from
monocular videos for DE has been actively investigated.
From monocular videos, NN-based methods learn from
structure-from-motion techniques. Zhou et al. [55] incor-
porated two separate networks to learn camera ego-motion
and depth simultaneously from monocular videos. Godard
et al. proposed a CNN-based architecture, Monodepth2
[13], which handles occlusion problems based on the min-
imum reprojection loss. Various approaches have been
explored: ranging from improving network architectures,
[1, 17, 24, 37, 52, 54], using semantics-guidance [25, 27],
to employing sequential frames in test time by utilizing the
cost volumes [9, 19, 51].
Recently, Petrovai et al. [40] suggested a two-stage train-
ing pipeline to transfer the knowledge of the high-resolution
DE as explicit pseudo labels into the low-resolution DE
training of a student network. This process involves fil-
tering out noisy pseudo depth labels by assessing 3D con-
sistency across sequential frames. In contrast, our coarse-
to-fine training strategy guides a DE network to learn the
depth of objects in dynamic classes based on the ground
contacting prior (GDS-Loss) in the coarse stage and to fur-
ther refine them in the fine stage. Our approach employs a
regularization loss in the fine stage to regularize the repro-
jection loss for enhancing the detailed depth predictions in
static object regions while encouraging the precise learning
of DE on the moving object regions with a cost-volume-
based weighting factor.
2.2. Handling Moving Objects
Since the self-supervised monocular DE pipelines learn
depth and camera ego-motion from consecutive frames un-
der the rigid scene assumption, independently moving ob-
jects induce them to learn erroneous depth. To overcome
this problem, Monodepth2 [13] utilized automasking that
excludes pixels static in sequential frames, but it is only ca-
pable of masking out the objects moving at the same speed
and in the same direction as the camera from the calculation
of the reprojection loss. Although other approaches sug-
gested filtering out the images with moving objects from
training scenes [18], and masking out the detected mov-
ing objects in photometric loss computation [27], discern-
ing whether an object is moving or not involves implicit
uncertainty, which may lead to inaccurate DE performance.
Recent work [3, 9, 16, 22, 30–32] attempted to learn
depth by decoupling object motion from camera ego-
motion, exploiting a regularization term for 3D transla-
tion fields [32], or predicting individual motions of objects
10520
by utilizing off-the-shelf segmentation models [3, 30, 31].
Feng et al. [9] suggested an occlusion-aware cost volume
for multi-frame DE by adopting a cycle-consistency loss.
However, without precise supervision of depth or motion of
moving objects, these methods often yield inferior results,
since the reprojection loss can be minimized even with in-
accurate depth and object motion predictions. In contrast,
our proposed GDS-Loss provides precise depth supervision
on the objects classified as dynamic classes, regardless of
their moving speed and directions based on the ground point
they stand, which is ground contacting prior. Moreover, our
method can easily be integrated with existing DE methods
by utilizing instance segmentation masks only during train-
ing without necessitating of training an auxiliary object mo-
tion estimation network.
3. Method
3.1. Self-supervised Monocular Depth Estimation
In this subsection, we briefly describe the conventional self-
supervised monocular depth estimation (DE) pipeline in a
self-contained manner. Given two consequent frames Itand
It′at time tandt′, respectively, from a monocular video, a
DE network θdepth predicts a depth map Dtfor target image
It, as given by
Dt=θdepth(It). (1)
Also, a pose estimation network θpose predicts a camera
ego-motion Tt→t′from IttoIt′. We can obtain a synthe-
sized frame Iw
t′by warping It′with the projected coordi-
nates based on the estimated depth Dt, pose Tt→t′and the
camera intrinsic matrix K, as given by
Iw
t′=DW(It′, proj (Dt, Tt→t′, K)), (2)
where proj denotes the projection of the 3D transformed
points and DW indicates the differentiable warping process
[23]. Then the reprojection loss Lrepdefined as the photo-
metric errors between Iw
t′andIt, consisting of L1 and SSIM
[49] error terms weighted by α, as given by
Lrep=α·L1(It, Iw
t′) + (1 −α)·SSIM (It, Iw
t′).(3)
In addition, an edge-aware smoothness loss Lsm[12] on
the disparity dt(inverse of depth Dt) aims to encourage lo-
cally smooth depth maps while preserving the edges in the
images, as is given by
Lsm=|∂xˆdt|e−|∂xIt|+|∂yˆdt|e−|∂yIt|, (4)
where ˆdtis the normalized disparity by the mean of dt.
Specifically, each term in Eq. 4 consists of the disparity
gradient exponentially weighted by the negative horizontal
or vertical gradient of It. As the color gradient ∂xIt(or
∂yIt) decreases, Lsmgets increased which guides θdepth topredict smooth depth in homogeneous regions. On the other
hand, as ∂xIt(or∂yIt) increases, Lsmdecreases to induce
abrupt boundaries in the depth map. Through the training,
θdepth andθpose learn DE and camera ego-motion, respec-
tively, by minimizing both LrepandLsm, as given by
Ltotal=Lrep+Lsm. (5)
3.2. Proposed Self-supervised DE Pipeline
Fig. 2 illustrates our coarse-to-fine training strategy for
self-supervised monocular DE. As depicted in the ‘Coarse
Training Stage’ (also in Sec. 3.2.1), DE network θ1
depth
learns the depth of objects in dynamic classes from our pro-
posed Ground-contacting-prior Disparity Smoothness Loss
LGDS (Eq. 9) that provides precise supervision. The re-
gion of objects in dynamic classes are masked out from the
calculation of Lrep(Eq. 6) to avoid inaccurate learning of
DE. After the ‘Coarse Training Stage’, the trained DE net-
work θ1
depth and pose estimation network θpose are fixed
and denoted as θ1∗
depth andθ∗
pose, respectively. In the ‘Fine
Training Stage’ (also in Sec. 3.2.2), θ2
depth , initialized as
the trained θ1
depth , is further finetuned to capture detailed
depth over the surfaces of objects in dynamic classes by ap-
plying an unmasked Lrepon the whole image. To prevent
θ2
depth from learning inaccurate DE on moving object re-
gions, we introduce our regularization loss LREG (Eq. 14)
with a cost-volume-based weighting factor λcv(Eq. 13).
3.2.1 Coarse Training Stage with Ground Contacting
Prior
Excluding objects in dynamic classes from Lrepcalcula-
tion. From the conventional self-supervised monocular DE
pipeline in Sec. 3.1, camera ego-motion is only considered
for calculating Lrep. Even though objects have independent
motions that often differ from the camera, Lrepcan still
be minimized via the estimated camera motion. As a re-
sult, the DE network often fails to predict the precise depth
of the objects, as shown in Fig. 1. In this coarse training
stage, instead of adopting erroneous supervision from Lrep,
we mask objects that belong to dynamic classes, referred to
as objects in dynamic classes, such as cars, bicycles, and
pedestrians from calculating Lrepwhether the objects are
moving or not. Using a pre-computed instance segmenta-
tion map for input image It, we define the binary dynamic
instance mask as Mt, valued 1 for the region of objects in
dynamic classes and 0 otherwise. The per-pixel masked re-
projection loss Lm
repis defined as
Lm
rep= (1−Mt)Lrep, (6)
so that the objects in dynamic classes can effectively be ex-
cluded from the calculation of Lrep, as also depicted in the
‘Coarse Training Stage’ of Fig. 2.
10521
Figure 2. Overview of our coarse-to-fine training strategy. In the coarse training stage, a DE network learns the depth of static region from
the masked reprojection loss Lm
repand the depth of the objects in dynamic classes from our GDS-Loss LGDS . In the fine training stage, we
further refine the DE network with unmasked Lrepwhile regularizing to ensure consistent depth prediction with the reliable DE network
trained in the coarse stage by employing our regularization loss LREG with a cost volume-based weighting factor λcv.
Leveraging the ground contacting prior for supervi-
sion. A predominant cause of imprecise depth prediction
for moving objects is insufficient supervision in the self-
supervised learning of DE. To address this issue, we intro-
duce the ground contacting prior, rooted in the observation
that most objects in dynamic classes such as cars, bicycles,
and pedestrians, are in contact with the ground, resulting in
similar depth with the depth of ground point they touch.
Based on this, we introduce a novel Ground-contacting-
prior Disparity Smoothness Loss (GDS-Loss) that can pre-
cisely supervise the depth of objects in dynamic classes.
Our GDS-Loss is designed to guide DE networks for consis-
tent depth predictions between the objects and their contact-
ing ground locations. As the ground is static, the DE net-
works can accurately predict the depth of the ground, which
can be utilized for strong self-supervision on the objects.
Ground-contacting-prior Disparity Smoothness Loss.
Our GDS-Loss is implemented by extending the conven-
tional edge-aware disparity smoothness loss (Eq. 4). Our
GDS-Loss focuses on the vertical gradient of disparity
|∂yˆdt|to induce consistency of DE in the region between
objects in dynamic classes and their contacting ground. For-
mally, for ˆdt∈Rn×m,|∂yˆdt|represents the disparity differ-
ence between two vertically adjacent pixel locations, (i, j)
and its bottom neighbor (i+ 1, j), which is expressed as:
|∂yˆdt|(i, j) =|ˆdt(i, j)−ˆdt(i+ 1, j)|, (7)where |∂yˆdt| ∈R(n−1)×m. In order to enforce consistency
of depth between the objects in dynamic classes and their
ground points of contact, we introduce a ground-contacting-
prior mask Mgrgiven as
Mgr(i, j) =γ·Mt(i, j) + (1 −Mt(i, j)) (8)
where Mgr∈R(n−1)×mandγis a weighting parame-
ter for Mgr, empirically setting to 100. For the location
(i, j)in the object region, we let |∂yˆdt|be highly weighted
byMgr(i, j) = γ, so that the disparities are enforced
to be consistent with their neighboring pixels in a down-
ward direction. On the other hand, |∂yˆdt|is weighted the
same as in Eq. 4 in the background region because of
Mgr(i, j) = 1 . We additionally modify the color gradi-
ents|∂yIt|and|∂xIt|to guide consistent depth prediction
inside the regions of objects in dynamic classes by replac-
ingItwithIm
t= (1−Mt)Itto remove the effect from the
color edges. Our GDS-Loss is defined as:
LGDS =|∂xˆdt|e−|∂xIm
t|+|∂yˆdt|Mgre−|∂yIm
t|,(9)
so that it induces smooth depth inside the object regions
while aligning them with their contacting ground points.
Note that only |∂yˆdt|is weighted by Mgr.
Finally, our total loss function L1
total in the coarse train-
ing stage with the masked reprojection loss and GDS-Loss
is now defined by
L1
total=Lm
rep+βLGDS. (10)
10522
Figure 3. Our fine training stage further refines to capture the de-
tailed depth of the objects in dynamic classes.
3.2.2 Fine Training Stage
Refinement of the depth estimation network. The initial
DE network θ1
depth , trained using the GDS-Loss (Eq. 9) in
the coarse training stage, effectively aligns the depth of the
objects in dynamic classes with their ground contact points.
Nonetheless, it may struggle to capture the detailed depth of
objects when their surfaces vary vertically or if their bottom
parts are occluded by other (closer to the camera) objects,
leading to homogeneous depth prediction on both objects.
Thus, we introduce a fine training stage for θ2
depth to learn
the complex depth of the objects in dynamic classes by ap-
plying an Lrep(Eq. 3) for the whole image region. Note
thatθ2
depth is initialized as the trained θ1
depth weights and
the fixed θ1∗
depth provides moderate supervision in this fine
training stage. Despite Lrepis critical to induce DE net-
works to learn the detailed depth of non-moving objects,
such as parked cars, that follow the rigid scene assumption,
it is prone to guide θ2
depth to learn the inaccurate DE on
moving object regions, as discussed in Sec. 3.2.1.
Regularization with respect to θ1∗
depth .To regularize
θ2
depth from learning inaccurate DE, we propose to guide
θ2
depth to predict consistent depth with reliable depth predic-
tions of θ1∗
depth that is trained based on the ground contact-
ing prior. We introduce a per-pixel depth regularization loss
that guides θ2
depth to learn detailed depth from Lrepwhile
maintaining alignment with θ1∗
depth , which is expressed by
L0
REG =max(|D1
t−D2
t|, δD), (11)
where D1
tandD2
trepresent the depth predictions of θ1∗
depth
andθ2
depth for target frame It, respectively. δDis a hy-
perparameter for an allowable depth difference between
θ2
depth andθ1∗
depth . Note that L0
REG is activated only when
the depth difference |D1
t−D2
t|exceeds δD, while allow-
ingθ2
depth to learn fine details without regularization when
|D1
t−D2
t|is minor. However, our experiments indicate that
the vanilla L0
REG fails to regularize when current |D1
t−D2
t|
is small but quickly becomes larger by Lrepin the moving
object regions, so that θ2
depth learns inaccurate DE.
Cost-volume-based weighting factor. To enhance reg-
ularization in the moving object regions, we introduce acost-volume-based approach that can identify the region by
analyzing the depth discrepancy between D1
tand a cost-
volume-induced depth. This approach is feasible because of
the reliable depth prediction D1
tinduced by the ground con-
tacting prior. The multi-view cost volume is constructed by
the L1 distances between the target frame Itand the warped
neighbor frame I′
tto the position of Itwith the depth can-
didates ranging from DmintoDmax, following [51]. From
the cost volume, the depth for each pixel that minimizes
the L1 distance among depth candidates, which is the cost-
volume-induced depth Dcv, is expressed as:
Dcv
t= arg min
Dk∈{1,...,32}(|DW(I′
t, proj (Dk, Tt→t′, K))−It|),
(12)
where Dkrepresents a k-th quantized depth candidate in the
range of [ D1=Dmin, D32=Dmax].DminandDmaxare
determined by the minimum and maximum values of D1
t,
respectively. By leveraging the difference between the cost-
volume-induced depth Dcv
tandD1
t, we define a pixel-wise
cost-volume-based-weighting factor as
λcv=max|D1
t−Dcv
t|
δD,1
, (13)
where λcvis equal to the normalized depth difference by
δD, being always greater than or equal to 1. In order to
regularize when |D1
t−Dcv
t|is larger than δDin the moving
object regions regardless of their current depth predictions,
we additionally mask δDwithµcv= [λcv= 1] where [·]is
an Iverson bracket. By employing λcvandµcvintoL0
REG
in Eq. 11, our proposed pixel-wise regularization loss is
defined as
LREG =λcv·max(|D1
t−D2
t|, µcvδD), (14)
where δDis empirically set to 5% of Dmax in our experi-
ments. The regions of moving objects have large |D1
t−Dcv
t|
values, so LREG regularizes the θ2
depth learning toward
maintaining the consistency between D1
tandD2
t. In con-
trast, the static regions which often have small values of
|D1
t−Dcv
t|and|D1
t−D2
t|can learn detailed depth without
the regularization. Since λcvpenalizes more on the region
that differs a lot between Dcvand the reliable D1
t,LREG is
a more stable regularization than the approach that directly
mask out pixels determined as moving from Lrep. Our total
lossL2
total in the fine training stage is defined as,
L2
total=Lrep+ρLREG. (15)
Note L1
total andL2
total are averaged over all image pix-
els and batches for training. As shown in Fig. 3, MonoViT
[54] trained with the coarse training stage predicts consis-
tent depth between the bus and the front car in the red box,
so the bus has a higher error than the car. On the other hand,
our fine training stage guides Our-MonoViT to predict the
accurate depth of the occluded bus with low error.
10523
D Methods O.N. Sem Resolutions abs rel ↓ sq rel ↓ rmse↓ logrmse↓ a1↑ a2↑ a3↑CityscapesStruct2Depth [3] ✓ Tr 128×416 0.145 1.737 7.280 0.205 0.813 0.942 0.976
Gordon et al. [16] ✓ Tr 128×416 0.127 1.330 6.960 0.195 0.830 0.947 0.981
Li et al. [32] ✓ 128×416 0.119 1.290 6.980 0.190 0.846 0.952 0.982
Monodepth2 [13] 128×416 0.129 1.569 6.876 0.187 0.849 0.957 0.983
Ours-Monodepth2 Tr 128×416 0.110 1.179 6.390 0.169 0.881 0.968 0.989
DynamicDepth †[9] Tr/Te 128×416 0.103 1.000 5.867 0.157 0.895 0.974 0.991
MonoViT [54] 128×416 0.114 1.238 6.589 0.174 0.860 0.965 0.990
Ours-MonoViT Tr 128×416 0.096 0.930 5.806 0.152 0.905 0.976 0.992
Lee et al. [31] ✓ Tr 256×832 0.116 1.214 6.695 0.186 0.852 0.951 0.982
InstaDM [30] ✓ Tr 256×832 0.111 1.158 6.437 0.182 0.868 0.961 0.983
Monodepth2 [13] 192×640 0.125 1.474 6.688 0.180 0.865 0.964 0.988
Ours-Monodepth2 Tr 192×640 0.102 1.024 6.015 0.159 0.896 0.973 0.990
HR-Depth [37] 192×640 0.120 1.253 6.714 0.179 0.857 0.963 0.988
Ours-HR-Depth Tr 192×640 0.100 1.010 5.998 0.157 0.896 0.974 0.991
RM-Depth [22] ✓ 192×640 0.100 0.839 5.774 0.154 0.895 0.976 0.993
CADepth [52] 192×640 0.124 1.278 6.771 0.183 0.862 0.962 0.986
Ours-CADepth Tr 192×640 0.097 0.966 5.646 0.150 0.907 0.978 0.992
MonoViT [54] 192×640 0.106 1.098 6.071 0.160 0.881 0.974 0.991
Ours-MonoViT Tr 192×640 0.088 0.795 5.368 0.140 0.920 0.981 0.994KITTIStruct2Depth [3] ✓ Tr 192×640 0.141 1.026 5.291 0.215 0.816 0.945 0.979
Li et al. [32] ✓ 192×640 0.130 0.950 5.138 0.209 0.843 0.948 0.978
Gordon et al. [16] ✓ Tr 192×640 0.128 0.959 5.230 0.212 0.845 0.947 0.976
SGDepth [27] Tr 192×640 0.117 0.907 4.844 0.196 0.875 0.958 0.980
Lee et al. [31] ✓ Tr 256×832 0.114 0.876 4.715 0.191 0.872 0.955 0.981
InstaDM [30] ✓ Tr 256×832 0.112 0.777 4.772 0.191 0.872 0.959 0.982
Monodepth2 [13] 192×640 0.115 0.903 4.863 0.193 0.877 0.959 0.981
Ours-Monodepth2 Tr 192×640 0.112 0.866 4.766 0.190 0.879 0.960 0.982
PackNet [17] 192×640 0.111 0.785 4.601 0.189 0.878 0.960 0.982
Johnston et al. [24] 192×640 0.111 0.941 4.817 0.189 0.885 0.961 0.981
HR-Depth [37] 192×640 0.109 0.792 4.632 0.185 0.884 0.962 0.983
Ours-HR-Depth Tr 192×640 0.108 0.775 4.614 0.184 0.886 0.962 0.983
FSRE-Depth [25] Tr 192×640 0.105 0.708 4.546 0.182 0.886 0.964 0.983
Petrovai et al. [40] 192×640 0.106 0.751 4.485 0.180 0.885 0.964 0.984
RM-Depth [22] ✓ 192×640 0.108 0.710 4.513 0.183 0.883 0.964 0.983
CADepth [52] 192×640 0.105 0.769 4.535 0.181 0.892 0.964 0.983
Ours-CADepth Tr 192×640 0.103 0.730 4.427 0.179 0.895 0.966 0.984
DynamicDepth †[9] Tr/Te 192×640 0.096 0.720 4.458 0.175 0.897 0.964 0.984
MonoViT [54] 192×640 0.099 0.708 4.372 0.175 0.900 0.967 0.984
Ours-MonoViT Tr 192×640 0.096 0.696 4.327 0.174 0.904 0.968 0.985
Table 1. Performance comparison of self-supervised DE methods on Cityscapes and KITTI. ‘D’ column: Test dataset. †: multiple test
frames. A checkmark on the ‘O.N.’: utilization of an additional object motion estimation network. ‘Tr’/‘Te’ in ‘Sem’ column: semantic
information usage in training/testing. Metrics with ↓are the lower, the better, while metrics with ↑are the higher, the better.
Figure 4. Examples of estimated depth maps and error maps in Cityscapes [6]. (a), (b), and (c) indicate MonoViT [54], DynamicDepth [9],
and Ours-MonoViT, respectively. In the error maps, blue indicates small errors, and red indicates large errors.
10524
4. Experiments
4.1. Datasets
Experiments are conducted on the widely adopted
Cityscapes [6] and KITTI [38]. Cityscapes [6] contains
a considerable amount of moving objects in the training
dataset where self-supervised DE methods often suffer from
the moving object problem. Following [51, 55], we use
69,731 training images (pre-processed with the scripts from
[55]), and 1,525 testing images. For the DE performance
on the regions of the objects in dynamic classes, we utilize
masks provided by [9]. We also train the DE networks on
the KITTI [38] dataset which contains 39,180 training and
4,424 validation images and evaluate on the Eigen split [8]
which contains 697 images. For testing in both datasets, the
estimated depth maps are normalized using median scaling
within the depth range from 0 mto 80 m.
4.2. Implementation Details
Our coarse-to-fine training strategy is integrated with the
monocular DE networks including Monodepth2 [13], HR-
Depth [37] with ResNet-18 based encoder [21], CADepth
[52] with ResNet-50 based encoder [21] and MonoViT [54]
with MPViT-small based encoder, all pre-trained on Ima-
geNet [28]. The DE network and the pose estimation net-
work (with ResNet-18 [21] based encoder) are trained for
15 epochs in the coarse training stage and the DE network
is refined for an additional 5 epochs in the fine training
stage. Our overall training strategy including the cost vol-
ume computation (Eq. 12) with the fixed θ1∗
depth in the fine
stage increased the total training time by about 11.1% (from
13.5h to 15h) on an NVIDIA RTX 3090 GPU for train-
ing Monodepth2 [13] in Cityscapes. The learning rates for
ResNet-based models are set as 1×10−4for the coarse
stage and 1×10−5for the fine stage, while Ours-MonoViT
starts from 5×10−5and decaying exponentially by a factor
of 0.9 for 20 epochs. The overall pipeline is implemented
in PyTorch [39] and trained with the Adam [26] optimizer
with a batch size of 12. The loss weights αof the L1 loss
(Eq. 3), βof the GDS-Loss (Eq. 10), and ρofLREG (Eq.
15) are empirically set to 0.85, 0.001 and 0.1, respectively.
The masks of objects in dynamic classes for training are
pre-computed by the Mask2Former [5] instance segmenta-
tion network with the backbone of Swin-S [36], pre-trained
from COCO [34] dataset.
4.3. Cityscapes Results
Table 1 shows the performance comparison of self-
supervised DE methods on Cityscapes and KITTI datasets.
Especially on Cityscapes dataset which contains a substan-
tial amount of moving objects, our training strategy signif-
icantly improves the DE performance of existing DE meth-
ods including Monodepth2 [13], HR-Depth [37], CADepthMethods Resolutions abs rel ↓sq rel ↓rmse↓logrmse↓a1↑
Monodepth2 [13] 128×416 0.159 1.937 6.363 0.201 0.816
MonoViT [54] 128×416 0.142 1.536 5.130 0.185 0.820
Ours-Monodepth2 128×416 0.136 1.238 4.791 0.176 0.864
DynamicDepth [9] 128×416 0.129 1.273 4.626 0.168 0.862
Ours-MonoViT 128×416 0.109 0.888 4.243 0.151 0.898
InstaDM [30] 256×832 0.139 1.698 5.760 0.181 0.859
Monodepth2 [13] 192×640 0.185 2.432 5.919 0.218 0.794
HR-Depth [37] 192×640 0.165 2.144 5.720 0.198 0.811
CADepth [52] 192×640 0.143 1.278 4.997 0.184 0.825
MonoViT [54] 192×640 0.149 1.508 5.340 0.190 0.817
Ours-Monodepth2 192×640 0.119 1.044 4.270 0.157 0.881
Ours-HR-Depth 192×640 0.117 1.015 4.297 0.154 0.883
Ours-CADepth 192×640 0.116 1.161 4.370 0.154 0.892
Ours-MonoViT 192×640 0.098 0.674 3.688 0.135 0.914
Table 2. Performance comparison of self-supervised DE methods
for objects in dynamic classes on Cityscapes [6].
[52] and MonoViT [54], At a resolution of 192×640,
the aforementioned methods are enhanced by incorporat-
ing our training strategy with large margins of 18.4%,
16.7%, 21.8%, and 17.0% in the absrel metric. Particularly,
Ours-Monodepth2 outperforms other methods [37, 52, 54]
that are trained without our training strategy. Also, Ours-
CADepth and Ours-MonoViT outperform RM-Depth [22]
without requiring additional object motion estimation net-
works. Notably, in the 128×416resolution, our single-
frame-based Ours-MonoViT outperforms the multi-frame-
based SOTA DynamicDepth [9], which relies on segmenta-
tion masks in test time. In Table 2, we compare the DE per-
formance on the regions of objects in dynamic classes. Our
coarse-to-fine training strategy significantly enhances the
DE performance of existing DE methods [13, 37, 52, 54].
Especially in a resolution of 128×416, Ours-MonoViT out-
performs DynamicDepth [9] in all metrics.
Also, in Fig. 4, MonoViT [54] and DynamicDepth [9]
predict erroneous depth on a car (denoted in a red box) and
a pedestrian (denoted in a green box) resulting in high errors
in the error maps due to the lack of supervision for depth
of moving objects. In contrast, Ours-MonoViT estimates
accurate depth especially on the objects (the cars and the
pedestrian) regardless of their moving speed and directions,
while preserving details in depth maps.
4.4. KITTI Results
We further assess the efficacy of our coarse-to-fine training
strategy on the frequently employed KITTI [38] dataset. As
can be seen in Table 1, the performances of Monodepth2
[13], HR-Depth [37], CADepth [52] and MonoViT [54] are
also improved with our training strategy integrated. Al-
though KITTI has less amount of moving objects in the
training scenes, our coarse-to-fine training strategy consis-
tently boost the DE performance of existing DE methods.
Also, Ours-MonoViT outperforms the current state-of-the-
art multi-frame-based method, DynamicDepth [9].
10525
ExpLm
repLGDS abs rel ↓sq rel ↓rmse↓logrmse↓a1↑
(a)WIR 0.125 1.474 6.688 0.180 0.865
DOR 0.185 2.432 5.919 0.218 0.794
(b)✓WIR 0.119 1.492 6.610 0.179 0.877
DOR 0.160 2.143 5.503 0.203 0.813
(c)✓ ✓WIR 0.104 1.097 6.034 0.160 0.895
DOR 0.125 1.185 4.459 0.161 0.876
Table 3. Ablation study on coarse training stage by training Mon-
odepth2 [13] on Cityscapes [6]. WIR and DOR indicate the per-
formance measures over the whole image regions and over the re-
gions of objects in dynamic classes only, respectively.
Exp. Methods abs rel ↓sq rel ↓rmse↓logrmse↓a1↑
(d)No WIR 0.145 2.868 7.395 0.201 0.856
regularization DOR 0.303 10.533 8.839 0.281 0.764
(e) Filtering [40]WIR 0.124 1.620 6.739 0.187 0.868
DOR 0.214 4.372 7.218 0.257 0.769
(f)L0
REG (Eq. 11)WIR 0.117 1.358 6.422 0.176 0.873
DOR 0.181 2.594 6.047 0.218 0.791
(g)LREG (Eq. 14)WIR 0.102 1.024 6.015 0.159 0.896
DOR 0.119 1.044 4.270 0.157 0.881
Table 4. Ablation study on fine training stage.
5. Ablation Study
We demonstrate the ablation study to verify the effective-
ness of our coarse-to-fine training strategy by incrementally
applying the proposed contributions to Monodepth2 [13]
and compare DE performance in Cityscapes [6] dataset.
Coarse Training Stage. In Table 3, from Exp-(a) to (c),
we train the DE network for 15 epochs to show the effec-
tiveness of our proposed masked reprojection loss Lm
repand
GDS-Loss LGDS .
Exp-(a) - Original Monodepth2 [13]: As shown in Table
3, the automasking technique fails to guide the Monodepth2
[13] model to learn the accurate depth of moving objects
since it is only capable of excluding the objects moving at
the same speed and direction as the camera.
Exp-(b) - Monodepth2 + Lm
rep: Simply excluding the
objects in dynamic classes from Lrepshows marginal im-
provement by avoiding inaccurate learning of depth from
Lrep. However, it is not sufficient to guide precise learning
of depth due to the lack of supervision in the object regions.
Exp-(c) - Monodepth2 + Lm
rep+LGDS : Additional
adoption of our GDS-Loss (Eq. 9) not only enhances the
overall performance but also drastically improves the DE
quality on the regions of objects in dynamic classes, since
our GDS-Loss LGDS provides precise depth supervision for
the objects based on the ground contacting prior.
Fine Training Stage. In Table 4, from Exp-(d) to (g),
we show the effectiveness of our regularization loss LREG
when the DE network θ2
depth , initially trained in Exp-(c), isfurther refined with the reprojection loss Lrepto capture the
detailed depth of objects in dynamic classes.
Exp-(d) - Training with no regularization: Although
θ2
depth is initially trained in the coarse stage, the DE per-
formance shows degradation due to the inaccurate guidance
from Lrepin the moving object regions.
Exp-(e) - Usage of the filtering scheme in [40]: We mask
out the pixels from Lrep, which are identified to be incon-
sistent in the 3D space by thresholding the difference be-
tween the depth of sequential frames predicted by θ1∗
depth .
Although this approach is effective in filtering out noisy
pixels from pseudo labels in [40], it struggles to effectively
exclude moving objects from Lrepwith thresholding so that
θ2
depth suffers from inaccurate learning from Lrepunfiltered
moving object regions.
Exp-(f) - Initial regularization loss L0
REG (Eq. 11): Al-
though L0
REG moderately guides θ2
depth to predict consis-
tent depth with θ1
depth , it struggles to effectively regular-
ize the inaccurate supervision from Lrep, especially on the
moving object regions.
Exp-(g) - Our regularization loss LREG (Eq. 14) with
λcv(Eq. 13): Since λcveffectively penalizes the moving
object regions, θ2
depth successfully improves the DE per-
formance with elaborating the depth details of the objects
in dynamic classes. Moreover, the approach of weighting
based on the depth discrepancy relative to the reliable θ1∗
depth
is shown to be more stable than filtering pixels from Lrep
that involves implicit uncertainty by discerning whether an
object is moving or not.
6. Conclusion
We address a challenging moving object problem in self-
supervised monocular depth estimation. For this, our pro-
posed coarse-to-fine training strategy provides precise su-
pervision on the depth of moving objects by firstly incorpo-
rating the ground-contacting-prior-based self-supervision ,
which is Ground-contacting-prior Disparity Smoothness
Loss (GDS-Loss). The subsequent stage refines the DE net-
work to capture the detailed depth of the objects in dynamic
classes under well-defined constraints from our regulariza-
tion loss with a cost-volume-based weighting factor. The
extensive experimental results show that our training strat-
egy is very well harmonized with existing DE methods to
effectively handle such moving objects. Moreover, Ours-
MonoViT yields the SOTA depth estimation performance
on both KITTI and Cityscapes datasets.
Acknowledgements
This work was supported by IITP grant funded by the Korea
government (MSIT) (No. RS2022-00144444, Deep Learn-
ing Based Visual Representational Learning and Rendering
of Static and Dynamic Scenes).
10526
References
[1] Jinwoo Bae, Sungho Moon, and Sunghoon Im. Deep digging
into the generalization of self-supervised monocular depth
estimation. In Proceedings of the AAAI conference on artifi-
cial intelligence , pages 187–196, 2023. 2
[2] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.
Adabins: Depth estimation using adaptive bins. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 4009–4018, 2021. 1
[3] Vincent Casser, Soeren Pirk, Reza Mahjourian, and Anelia
Angelova. Depth prediction without the sensors: Leveraging
structure for unsupervised learning from monocular videos.
InProceedings of the AAAI Conference on Artificial Intelli-
gence , pages 8001–8008, 2019. 2, 3, 6
[4] Yuhua Chen, Cordelia Schmid, and Cristian Sminchis-
escu. Self-supervised learning with geometric constraints in
monocular video: Connecting flow, depth, and camera. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 7063–7072, 2019. 2
[5] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-
der Kirillov, and Rohit Girdhar. Masked-attention mask
transformer for universal image segmentation. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 1290–1299, 2022. 7, 3, 4
[6] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 3213–3223, 2016. 6, 7, 8, 1, 2, 3, 4
[7] Tom van Dijk and Guido de Croon. How do neural networks
see depth in single images? In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 2183–
2191, 2019. 2
[8] David Eigen and Rob Fergus. Predicting depth, surface nor-
mals and semantic labels with a common multi-scale con-
volutional architecture. In Proceedings of the IEEE inter-
national conference on computer vision , pages 2650–2658,
2015. 7
[9] Ziyue Feng, Liang Yang, Longlong Jing, Haiyan Wang,
YingLi Tian, and Bing Li. Disentangling object motion and
occlusion for unsupervised multi-frame monocular depth. In
Computer Vision–ECCV 2022: 17th European Conference,
Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part
XXXII , pages 228–244. Springer, 2022. 1, 2, 3, 6, 7
[10] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-
manghelich, and Dacheng Tao. Deep ordinal regression net-
work for monocular depth estimation. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 2002–2011, 2018. 1
[11] Ravi Garg, Vijay Kumar Bg, Gustavo Carneiro, and Ian Reid.
Unsupervised cnn for single view depth estimation: Geom-
etry to the rescue. In Computer Vision–ECCV 2016: 14th
European Conference, Amsterdam, The Netherlands, Octo-
ber 11-14, 2016, Proceedings, Part VIII 14 , pages 740–756.
Springer, 2016. 1, 2[12] Cl ´ement Godard, Oisin Mac Aodha, and Gabriel J Bros-
tow. Unsupervised monocular depth estimation with left-
right consistency. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 270–279,
2017. 1, 3
[13] Cl ´ement Godard, Oisin Mac Aodha, Michael Firman, and
Gabriel J Brostow. Digging into self-supervised monocular
depth estimation. In Proceedings of the IEEE International
Conference on Computer Vision , pages 3828–3838, 2019. 1,
2, 6, 7, 8, 3, 4, 5
[14] Juan Luis Gonzalez and Munchurl Kim. Plade-net: To-
wards pixel-level accuracy for self-supervised single-view
depth estimation with neural positional encoding and dis-
tilled matting loss. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
6851–6860, 2021. 1, 2
[15] Juan Luis GonzalezBello and Munchurl Kim. Forget about
the lidar: Self-supervised depth estimators with med proba-
bility volumes. Advances in Neural Information Processing
Systems , 33:12626–12637, 2020. 1, 2
[16] Ariel Gordon, Hanhan Li, Rico Jonschkowski, and Anelia
Angelova. Depth from videos in the wild: Unsupervised
monocular depth learning from unknown cameras. In Pro-
ceedings of the IEEE/CVF International Conference on
Computer Vision , pages 8977–8986, 2019. 2, 6
[17] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raven-
tos, and Adrien Gaidon. 3d packing for self-supervised
monocular depth estimation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , 2020. 1, 2, 6, 4
[18] Vitor Guizilini, Rui Hou, Jie Li, Rares Ambrus, and Adrien
Gaidon. Semantically-guided representation learning for
self-supervised monocular depth. In International Confer-
ence on Learning Representations , 2020. 1, 2
[19] Vitor Guizilini, Rares ,Ambrus ,, Dian Chen, Sergey Zakharov,
and Adrien Gaidon. Multi-frame self-supervised depth with
transformers. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 160–
170, 2022. 2
[20] Wencheng Han, Junbo Yin, Xiaogang Jin, Xiangdong Dai,
and Jianbing Shen. Brnet: Exploring comprehensive features
for monocular depth estimation. In European Conference on
Computer Vision , pages 586–602. Springer, 2022. 2
[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 7
[22] Tak-Wai Hui. Rm-depth: Unsupervised learning of recur-
rent monocular depth in dynamic scenes. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 1675–1684, 2022. 1, 2, 6, 7
[23] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.
Spatial transformer networks. Advances in neural informa-
tion processing systems , 28, 2015. 3
[24] Adrian Johnston and Gustavo Carneiro. Self-supervised
monocular trained depth estimation using self-attention and
discrete disparity volume. In Proceedings of the ieee/cvf con-
10527
ference on computer vision and pattern recognition , pages
4756–4765, 2020. 2, 6
[25] Hyunyoung Jung, Eunhyeok Park, and Sungjoo Yoo.
Fine-grained semantics-aware representation enhancement
for self-supervised monocular depth estimation. CoRR ,
abs/2108.08829, 2021. 2, 6
[26] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. CoRR , abs/1412.6980, 2014. 7
[27] Marvin Klingner, Jan-Aike Term ¨ohlen, Jonas Mikolajczyk,
and Tim Fingscheidt. Self-supervised monocular depth es-
timation: Solving the dynamic object problem by semantic
guidance. In Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceed-
ings, Part XX 16 , pages 582–600. Springer, 2020. 1, 2, 6
[28] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classification with deep convolutional neural net-
works. Communications of the ACM , 60(6):84–90, 2017. 7
[29] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and
Il Hong Suh. From big to small: Multi-scale local planar
guidance for monocular depth estimation. arXiv preprint
arXiv:1907.10326 , 2019. 1
[30] Seokju Lee, Sunghoon Im, Stephen Lin, and In So Kweon.
Learning monocular depth in dynamic scenes via instance-
aware projection consistency. In Proceedings of the AAAI
Conference on Artificial Intelligence , pages 1863–1872,
2021. 1, 2, 3, 6, 7
[31] Seokju Lee, Francois Rameau, Fei Pan, and In So Kweon.
Attentive and contrastive learning for joint depth and mo-
tion field estimation. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 4862–4871,
2021. 3, 6
[32] Hanhan Li, Ariel Gordon, Hang Zhao, Vincent Casser, and
Anelia Angelova. Unsupervised monocular depth learning
in dynamic scenes. In Conference on Robot Learning , pages
1908–1917. PMLR, 2021. 1, 2, 6
[33] Rui Li, Dong Gong, Wei Yin, Hao Chen, Yu Zhu, Kaix-
uan Wang, Xiaozhi Chen, Jinqiu Sun, and Yanning Zhang.
Learning to fuse monocular and multi-view cues for multi-
frame depth estimation in dynamic scenes. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 21539–21548, 2023. 1
[34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13 , pages 740–755. Springer, 2014. 7
[35] Yuxuan Liu, Yuan Yixuan, and Ming Liu. Ground-aware
monocular 3d object detection for autonomous driving. IEEE
Robotics and Automation Letters , 6(2):919–926, 2021. 1
[36] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 10012–10022, 2021. 7
[37] Xiaoyang Lyu, Liang Liu, Mengmeng Wang, Xin Kong,
Lina Liu, Yong Liu, Xinxin Chen, and Yi Yuan. Hr-depth:high resolution self-supervised monocular depth estimation.
CoRR abs/2012.07356 , 2020. 2, 6, 7, 3, 4, 5
[38] Moritz Menze and Andreas Geiger. Object scene flow for
autonomous vehicles. In Conference on Computer Vision
and Pattern Recognition (CVPR) , 2015. 7, 1, 2, 3, 4
[39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-
perative style, high-performance deep learning library. Ad-
vances in neural information processing systems , 32, 2019.
7
[40] Andra Petrovai and Sergiu Nedevschi. Exploiting pseudo
labels in a self-supervised learning framework for im-
proved monocular depth estimation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 1578–1588, 2022. 2, 6, 8
[41] Andrea Pilzer, Stephane Lathuiliere, Nicu Sebe, and Elisa
Ricci. Refine and distill: Exploiting cycle-inconsistency and
knowledge distillation for unsupervised monocular depth
estimation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 9768–
9777, 2019. 2
[42] Matteo Poggi, Fabio Tosi, and Stefano Mattoccia. Learning
monocular depth estimation with unsupervised trinocular as-
sumptions. In 2018 International conference on 3d vision
(3DV) , pages 324–333. IEEE, 2018. 2
[43] Matteo Poggi, Filippo Aleotti, Fabio Tosi, and Stefano Mat-
toccia. On the uncertainty of self-supervised monocular
depth estimation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
3227–3237, 2020. 2
[44] Zequn Qin and Xi Li. Monoground: Detecting monocular 3d
objects from the ground. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 3793–3802, 2022. 1
[45] Ren ´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-
sion transformers for dense prediction. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 12179–12188, 2021. 1
[46] Chang Shu, Kun Yu, Zhixiang Duan, and Kuiyuan Yang.
Feature-metric loss for self-supervised learning of depth and
egomotion. In European Conference on Computer Vision ,
pages 572–588. Springer, 2020. 2
[47] Kunal Swami, Amrit Muduli, Uttam Gurram, and Pankaj Ba-
jpai. Do what you can, with what you have: Scale-aware and
high quality monocular depth estimation without real world
labels. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 988–997, 2022.
2
[48] Ruoyu Wang, Zehao Yu, and Shenghua Gao. Planedepth:
Self-supervised depth estimation via orthogonal planes. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 21425–21434, 2023. 1
[49] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P.
Simoncelli. Image quality assessment: from error visibility
to structural similarity. IEEE Trans. Image Process. , 13(4):
600–612, 2004. 3
10528
[50] Jamie Watson, Michael Firman, Gabriel J Brostow, and
Daniyar Turmukhambetov. Self-supervised monocular depth
hints. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 2162–2171, 2019. 1, 2
[51] Jamie Watson, Oisin Mac Aodha, Victor Prisacariu, Gabriel
Brostow, and Michael Firman. The temporal opportunist:
Self-supervised multi-frame monocular depth. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 1164–1174, 2021. 1, 2,
5, 7
[52] Jiaxing Yan, Hong Zhao, Penghui Bu, and YuSheng Jin.
Channel-wise attention-based network for self-supervised
monocular depth estimation. In 2021 International Confer-
ence on 3D vision (3DV) , pages 464–473. IEEE, 2021. 2, 6,
7, 3, 4, 5
[53] Huangying Zhan, Ravi Garg, Chamara Saroj Weerasekera,
Kejie Li, Harsh Agarwal, and Ian Reid. Unsupervised learn-
ing of monocular depth estimation and visual odometry with
deep feature reconstruction. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
340–349, 2018. 2
[54] Chaoqiang Zhao, Youmin Zhang, Matteo Poggi, Fabio Tosi,
Xianda Guo, Zheng Zhu, Guan Huang, Yang Tang, and Ste-
fano Mattoccia. Monovit: Self-supervised monocular depth
estimation with a vision transformer. In International Con-
ference on 3D Vision , 2022. 1, 2, 5, 6, 7, 3, 4
[55] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G
Lowe. Unsupervised learning of depth and ego-motion from
video. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 1851–1858, 2017. 2,
7
[56] Yunsong Zhou, Quan Liu, Hongzi Zhu, Yunzhe Li, Shan
Chang, and Minyi Guo. Mogde: Boosting mobile monocular
3d object detection with ground depth estimation. Advances
in Neural Information Processing Systems , 35:2033–2045,
2022. 1
[57] Zhengming Zhou and Qiulei Dong. Self-distilled feature ag-
gregation for self-supervised monocular depth estimation. In
European Conference on Computer Vision , pages 709–726.
Springer, 2022. 2
10529
