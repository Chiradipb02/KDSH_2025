FreeControl: Training-Free Spatial Control of Any Text-to-Image Diffusion
Model with Any Condition
Sicheng Mo†*, Fangzhou Mu§*, Kuan Heng Lin†, Yanli Liu‡, Bochen Guan‡, Yin Li§, Bolei Zhou†
†University of California, Los Angeles,§University of Wisconsin-Madison,‡Innopeak Technology, Inc
Input Condition ControlNet Input Condition
Canny edge “An avocado chair, oil painting”
Segmentation mask “Cartoon of living room” “Modern living room ”
Human pose “Person, outside” “Robot, on the grass”
Depth map “A bear, with an Eiffel Tower in the background”FreeControl FreeControl
Mesh “A huge building in the shape of cup, with city in background” (b)
 Point cloud “Sunshine, railway” “Winter, railway” (a)
Figure 1. Training-free conditional control of Stable Diffusion [44]. (a) FreeControl enables zero-shot control of pretrained text-to-
image diffusion models given various input control conditions. (b) Compared to ControlNet [59], FreeControl achieves a good balance
between spatial and image-text alignment, especially when facing a conflict between the guidance image and text description. Additionally,
FreeControl supports several condition types ( e.g., 2D projections of point clouds and meshes in the bottom row), where it is difficult to
construct training pairs.
Abstract
Recent approaches such as ControlNet [59] offer users
fine-grained spatial control over text-to-image (T2I) diffu-
sion models. However, auxiliary modules have to be trained
for each spatial condition type, model architecture, and
checkpoint, putting them at odds with the diverse intents and
preferences a human designer would like to convey to the AI
models during the content creation process. In this work,
we present FreeControl, a training-free approach for con-
trollable T2I generation that supports multiple conditions,
architectures, and checkpoints simultaneously. FreeCon-
* indicates equal contributiontrol enforces structure guidance to facilitate the global
alignment with a guidance image, and appearance guid-
ance to collect visual details from images generated with-
out control. Extensive qualitative and quantitative exper-
iments demonstrate the superior performance of FreeCon-
trol across a variety of pre-trained T2I models. In partic-
ular, FreeControl enables convenient training-free control
over many different architectures and checkpoints, allows
the challenging input conditions on which most of the ex-
isting training-free methods fail, and achieves competitive
synthesis quality compared to training-based approaches.
Project page: https://genforce.github.io/freecontrol/.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
7465
1. Introduction
Text-to-image (T2I) diffusion models [4, 42] have achieved
tremendous success in high-quality image synthesis, yet a
text description alone is far from enough for users to con-
vey their preferences and intents for content creation. Re-
cent advances such as ControlNet [59] enable spatial con-
trol of pretrained T2I diffusion models, allowing users to
specify the desired image composition by providing a guid-
ance image ( e.g., depth map, human pose) alongside the text
description. Despite their superior generation results, these
methods [6, 30, 33, 55, 59, 62] require training an additional
module specific to each spatial condition type. Consider-
ing the large space of control signals, constantly evolving
model architectures, and a growing number of customized
model checkpoints ( e.g., Stable Diffusion [44] fine-tuned
for Disney characters or user-specified objects [24, 46]), this
repetitive training on every new model and condition type
is costly and uneconomical.
Besides the high training cost and poor scalability, con-
trollable T2I diffusion methods face drawbacks that stem
from their training scheme: they are trained to output a tar-
get image given a spatially-aligned control condition com-
puted from the same image using an off-the-shelf model
(e.g., MiDaS [43] for depth maps, OpenPose [10] for hu-
man poses). This limits the use of many desired control
signals that are difficult to infer from an image ( e.g., mesh,
point cloud). Further, the trained models tend to prioritize
spatial condition over text description, likely because the
close spatial alignment of input-output image pairs exposes
a shortcut. This is illustrated in Figure 1(b), where there is a
conflict between the guidance image and text prompt ( e.g.,
an edge map of a sofa chair vs. “an avocado chair”).
To address the aforementioned limitations, we present
FreeControl, a versatile training-free method for control-
lable T2I diffusion. Our key motivation is that feature maps
in T2I models during the generation process already capture
the spatial structure and local appearance described in the
input text. By modeling the subspace of these features, we
can effectively steer the generation process towards a sim-
ilar structure expressed in the guidance image, while pre-
serving the appearance of the concept in the input text. To
this end, FreeControl includes an analysis stage and a syn-
thesis stage. In the analysis stage, FreeControl queries a
T2I model to generate as few as one seed image and then
constructs a linear feature subspace from the generated im-
ages. In the synthesis stage, FreeControl employs guidance
in the subspace to facilitate structure alignment with a guid-
ance image, as well as appearance alignment between im-
ages generated with and without control.
FreeControl offers significant strength over training-
based methods by eliminating the need for additional train-
ing on a pretrained T2I model, while adeptly adhering to
concepts outlined in the text description. It supports awide range of control conditions, model architectures and
customized checkpoints, achieves high-quality image gen-
eration with robust controllability in comparison to prior
training-free methods [20, 31, 37, 53], and can be read-
ily adapted for text-guided image-to-image translation. We
conduct extensive qualitative and quantitative experiments
and demonstrate the superior performance of our method.
Notably, FreeControl excels at challenging control condi-
tions on which prior training-free methods fail. In the mean-
time, it attains competitive image synthesis quality com-
pared to training-based methods while providing stronger
image-text alignment and supporting a broader set of con-
trol signals.
Our contributions . (1) We present FreeControl, a novel
method for training-free controllable T2I generation via
modeling the linear subspace of intermediate diffusion fea-
tures and employing guidance in this subspace during the
generation process. (2) Our method presents the first uni-
versal training-free solution that supports multiple control
conditions (sketch, normal map, depth map, edge map, hu-
man pose, segmentation mask, natural image and beyond),
model architectures ( e.g., SD 1.5, 2.1, and SD-XL 1.0), and
customized checkpoints ( e.g., using DreamBooth [46] and
LoRA [24]). (3) Our method demonstrates superior results
in comparison to previous training-free methods ( e.g., Plug-
and-Play [53]) and achieves comparable performance with
prior training-based approaches ( e.g., ControlNet [59]).
2. Related Work
Text-to-image diffusion. Diffusion models [22, 49, 51]
bring a breakthrough in text-to-image (T2I) generation. T2I
diffusion models formulate image generation as an iter-
ative denoising task guided by a text prompt. Denois-
ing is conditioned on textual embeddings produced by lan-
guage encoders [40, 41] and is performed either in pixel
space [7, 34, 42, 48] or latent space [19, 39, 44], followed
by cascaded super-resolution [23] or latent-to-image decod-
ing [16] for high-resolution image synthesis. Several recent
works show that the internal representations of T2I diffu-
sion models capture mid/high-level semantic concepts, and
thus can be repurposed for image recognition tasks [28, 58].
Our work builds upon this intuition and exploits the feature
space of T2I models to guide the generation process.
Controllable T2I diffusion. It is challenging to convey hu-
man preferences and intents through text description alone.
Several methods thus instrument pre-trained T2I models
to take an additional input condition by learning auxiliary
modules on paired data [6, 30, 33, 55, 59, 62]. One signif-
icant drawback of this training-based approach is the cost
of repeated training for every control signal type, model
architecture, and model checkpoint. On the other hand,
training-free methods leverage attention weights and fea-
7466
tures inside a pre-trained T2I model for the control of ob-
ject size, shape, appearance and location [9, 15, 18, 38, 57].
However, these methods only take coarse conditions such as
bounding boxes to achieve precise control over object pose
and scene composition. Different from all the prior works,
FreeControl is a training-free approach to controllable T2I
diffusion that supports any spatial condition, model archi-
tecture, and checkpoint within a unified framework.
Image-to-image translation with T2I diffusion. Control-
ling T2I diffusion becomes an image-to-image translation
(I2I) task [25] when the control signal is an image. I2I meth-
ods map an image from its source domain to a target do-
main while preserving the underlying structure [25, 36, 47].
T2I diffusion enables I2I methods to specify target domains
using text. Text-driven I2I is often posed as conditional
generation [8, 26, 33, 59, 61, 62]. These methods fine-
tune a pretrained model to condition it on an input image.
Alternatively, recent training-free methods perform zero-
shot image translation [20, 31, 37, 53] and is most rele-
vant to our work. This is achieved by inverting the input
image [32, 50, 56], followed by manipulating the attention
weights and features throughout the diffusion process. A
key limitation of these methods is they require the input
to have rich textures, and hence they fall short when con-
verting abstract layouts ( e.g. depth) to realistic image. By
contrast, our method attends to semantic image structure by
decomposing features into principal components, thereby it
supports a wide range of modalities as layout specifications.
Customized T2I diffusion. Model customization is a key
use case of T2I diffusion in visual content creation. By
fine-tuning a pretrained model on images of custom ob-
jects or styles, several methods [5, 17, 27, 46] bind a dedi-
cated token to each concept and insert them in text prompts
for customized generation. Amid the growing number of
customized models being built and shared by content cre-
ators [2, 3], FreeControl offers a scalable framework for
zero-shot control of any model with any spatial condition.
3. Preliminary
Diffusion sampling. Image generation with a pre-trained
T2I diffusion model amounts to iteratively removing noise
from an initial Gaussian noise image xT[22]. This sam-
pling process is governed by a learned denoising network
ϵθconditioned on a text prompt c. At a sampling step t,
a cleaner image xt−1is obtained by subtracting from xta
noise component ϵt=ϵθ(xt;t,c). Alternatively, ϵθcan be
seen as approximating the score function for the marginal
distributions ptscaled by a noise schedule σt[51]:
ϵθ(xt;t,c)≈ −σt∇xtlogpt(xt|c). (1)
Guidance. The update rule in Equation 1 may be altered by
a time-dependent energy function g(xt;t, y)through guid-
Figure 2. Visualization of feature subspace given by PCA. Keys
from the first self-attention in the U-Net decoder are obtained via
DDIM inversion [50] for five images in different styles and modal-
ities ( top:person ;bottom :bedroom ), and subsequently un-
dergo PCA. The top three principal components (pseudo-colored
in RGB) provide a clear separation of semantic components.
ance (with strength s) [14, 15] so as to condition diffusion
sampling on auxiliary information y(e.g., class labels):
ˆϵθ(xt;t,c) =ϵθ(xt;t,c)−s g(xt;t, y). (2)
In practice, gmay be realized as classifiers [14] or CLIP
scores [34], or defined using bounding boxes [12, 57], atten-
tion maps [18, 37] or any measurable object properties [15].
Attentions in ϵθ.A standard choice for ϵθis a U-Net [45]
with self- and cross-attentions [54] at multiple resolutions.
Conceptually, self-attentions model interactions among spa-
tial locations within an image, whereas cross-attentions re-
late spatial locations to tokens in a text prompt. These two
attention mechanisms complement one another and jointly
control the layout of a generated image [9, 18, 38, 53].
4. Training-Free Control of T2I Models
FreeControl is a unified framework for zero-shot control-
lable T2I diffusion. Given a text prompt cand a guidance
image Igof any modality, FreeControl directs a pre-trained
T2I diffusion model ϵθto comply with cwhile also respect-
ing the semantic structure provided by Igthroughout the
sampling process of an output image I.
Our key finding is that the leading principal components
of self-attention block features inside a pre-trained ϵθpro-
vide a strong and surprisingly consistent representation of
semantic structure across a broad spectrum of image modal-
ities (see Figure 2 for examples). To this end, we introduce
structure guidance to help draft the structural template of
Iunder the guidance of Ig. To texture this template with
7467
“A photo of a Lego man giving a lecture”
Guiding Branch
xgtDDIM Inversionxgt−1¯xt¯xT¯xt−1Generation BranchxtxTxt−1Appearance GuidanceStructure Guidance…Bt=[]Update
Output Sample  with control
Output Sample without controlSynthesis StageCopy(b)Analysis Stage(a)“A photo of [man], with a background”
Fst=[…PCA(])
p(Nb+1)tp(c)tDropped…p(1)tp(2)tp(3)tp(Nb)tSemantic bases
…Seed Images
Semantic Bases for [man]…Input ConditionPrompt
…Bt=[]…
…Figure 3. Method overview. (a) In the analysis stage, FreeControl generates seed images for a target concept ( e.g.,man) using a pretrained
diffusion model and performs PCA on their diffusion features to obtain a linear subspace as semantic basis. (b) In the synthesis stage,
FreeControl employs structure guidance in this subspace to enforce structure alignment with the input condition. In the meantime, it
applies appearance guidance to facilitate appearance transfer from a sibling image generated using the same seed without structure control.
the content and style described by c, we further devise ap-
pearance guidance to borrow appearance details from ¯I, a
sibling of Igenerated without altering the diffusion process.
Ultimately, Imimics the structure of Igwith its content and
style similar to ¯I.
Method overview. FreeControl is a two-stage method as il-
lustrated in Figure 3. It begins with an analysis stage, where
diffusion features of seed images undergo principal com-
ponent analysis (PCA), with the leading PCs forming the
time-dependent bases Btas our semantic structure repre-
sentation .Igsubsequently undergoes DDIM inversion [50]
with its diffusion features projected onto Bt, yielding their
semantic coordinates Sg
t. In the synthesis stage, structure
guidance encourages Ito develop the same semantic struc-
ture as Igby attracting SttoSg
t. In the meantime, appear-
ance guidance promotes appearance similarity between I
and¯Iby penalizing the difference in their feature statistics.
4.1. Semantic Structure Representation
Zero-shot spatial control of T2I diffusion demands a unified
representation of semantic image structure that is invariant
to image modalities. Recent work has discovered that self-
attention features ( i.e., keys and queries) of self-supervised
Vision Transformers [52] and T2I diffusion models [9] are
strong descriptors of image structure. Based on these find-
ings, we hypothesize that manipulating self-attention fea-
tures is key to controllable T2I diffusion.
A na ¨ıve approach derived from PnP [53] is to directly
inject the self-attention weights (equivalently the features)
ofIginto the diffusion process of I. Unfortunately, this ap-
proach introduces appearance leakage ; that is, not only thestructure of Igis carried over but also traces of appearance
details. As seen in Figure 6, appearance leakage is partic-
ularly problematic when IgandIare different modalities
(e.g., depth vs. natural images), common for controllable
generation.
Towards disentangling image structure and appearance,
we draw inspiration from Transformer feature visualiza-
tion [35, 53] and perform PCA on self-attention features
of semantically similar images. Our key observation is that
the leading PCs form a semantic basis ; It exhibits a strong
correlation with object pose, shape, and scene composition
across diverse image modalities. In the following, we lever-
age this basis as our semantic structure representation and
explain how to obtain such bases in the analysis stage.
4.2. Analysis Stage
Seed images. We begin by collecting Nsimages that share
the target concept with c. These seed images {Is}are gener-
ated with ϵθusing a text prompt ˜cmodified from c. Specifi-
cally, ˜cinserts the concept tokens into a template that is in-
tentionally kept generic ( e.g., “A photo of [] with
background. ”). Importantly, this allows {Is}to cover
diverse object shape, pose, and appearance as well as image
composition and style, which is key to the expressiveness of
semantic bases . We study the choice of Nsin Section 5.2.
Semantic basis. We apply DDIM sampling [50] to generate
{Is}and obtain time-dependent diffusion features {Fs
t}of
sizeNs×C×H×Wfrom ϵθ. This yields Ns×H×
Wdistinct feature vectors, on which we perform PCA to
obtain the time-dependent semantic bases Btas the first Nb
7468
SD 1.5 SD-XL 1.0 SD 2.1 Prompt ConditionSketch Normal map Depth map Canny edge Seg mask Human pose M-LSD line HED edge Image
“a turtle in the 
sea”“a bronze 
horse in a 
museum”“A cute 
Stormtrooper's 
lecture”“A man, in the 
home”“A storied 
building, 
modern”“A clean 
sneaker being 
displayed”“A house, 
behind a river, 
raining”“A man, 
standing on a 
beach”“A Lego 
bedroom”Figure 4. Qualitative comparison of controllable T2I diffusion. FreeControl supports a suite of control signals and three major versions
of Stable Diffusion. The generated images closely follow the text prompts while exhibiting strong spatial alignment with the input images.
principal components:
Bt= [p(1)
t,p(2)
t, ...,p(Nb)
t]∼PCA({Fs
t}) (3)
Intuitively, Btspan semantic spaces Stthat connect dif-
ferent image modalities, allowing the propagation of image
structure from IgtoIin the synthesis stage. We study the
choice of FtandNbin Section 5.2 and Section B.
Basis reuse. Once computed, Btcan be reused for the same
text prompt or shared by prompts with related concepts. The
cost of basis construction can thus be amortized over multi-
ple runs of the synthesis stage.
4.3. Synthesis Stage
The generation of Iis conditioned on Igthrough guidance.
As a first step, we express the semantic structure of Igwith
respect to the semantic bases Bt.
Inversion of Ig.We perform DDIM inversion [50] on Ig
to obtain the diffusion features Fg
tof size C×H×Wand
project them onto Btto obtain their semantic coordinates
Sg
tof size Nb×H×W. For local control of foreground
structure, we further derive a mask M(sizeH×W) from
cross-attention maps of the concept tokens [18]. Mis set to
1(sizeH×W) for global control.
We are now ready to generate Iwith structure guidance
to control its underlying semantic structure.
Structure guidance. At each denoising step t, we obtain
the semantic coordinates Stby projecting the diffusion fea-turesFtfromϵθontoBt. Our energy function gsfor struc-
ture guidance can then be expressed as
gs(St;Sg
t,M) =P
i,jmij∥[st]ij−[sg
t]ij∥2
2P
i,jmij| {z }
forward guidance
+w·P
i,j(1−mij)∥max([ st]ij−τt,0)∥2
2P
i,j(1−mij)
| {z }
backward guidance,
where iandjare spatial indices for St,Sg
tandM, and w
is the balancing weight. The thresholds τtare defined as
τt= max
i,js.t. mij=0[sg
t]ij (4)
withmax taken per channel. Loosely speaking, [st]ij>τt
indicates the presence of foreground structure. Intuitively,
theforward term guides the structure of Ito align with Igin
the foreground, whereas the backward term, effective when
M̸=1, helps carve out the foreground by suppressing spu-
rious structure in the background.
While structure guidance drives Ito form the same se-
mantic structure as Ig, we found that it also amplifies low-
frequency textures, producing cartoony images that lack ap-
pearance details. To fix this problem, we apply appearance
guidance to borrow texture from ¯I, a sibling image of Igen-
erated from the same noisy latent with the same seed yet
without structure guidance.
7469
Input Condition FreeControl Input Condition FreeControl
Triangular  
mesh“A sketch of a bunny”“A cartoon bunny,  
on the grass”Wireframe
“A teapot, 
 from China”“An uncolored syderolife 
teapot”
Point cloud 
(w/ part seg.)“A plane,  
in the sky”“A plane on the runway, 
landing”Point cloud“A SUV car,  
on the road”“A small wooden SUV 
car”
Face mesh 
(3D MM)“A man,  
with short hair”“An ID photo of Benedict 
Cumberbatch”
Facial Landmarks 
(MediaPipe)“A man,  
in a suit”“A woman, blonde hair, in 
nature”
Body mesh 
(AMASS)“A man, in a suit,  
on the beach”“A monkey, 
in the snow”
Humanoid“Man, with sword and 
shield, in the river”“Large robot, with 
weapon, over a city”
“library,  
England”“library,  
cartoon”Blender 
viewport
“A house,  
on the grass”“A gingerbread house, in 
the snow”AutoCAD 
viewport
Metadrive 
Simulator“A black Jeep car, on the 
road, raining, back”“A Ferrari car, sunshine, 
back”
Metadrive 
Simulator“Road, bird-view, in the 
city, buildings”“Road, bird-view, in the 
snow, woods”Input Condition FreeControlFigure 5. Qualitative results for more diverse control conditions. FreeControl supports challenging control conditions not possible with
training-based methods. These include 2D projections of common graphics primitives, domain-specific shape models (point cloud, body
mesh, and humanoid) , graphics software viewports (Blender and AutoCAD) , and simulated driving environments (Metadrive) .
Appearance representation. Inspired by DSG [15], we
represent image appearance as {v(k)
t}Na≤Nb
k=1, the weighted
spatial means of diffusion features Ft:
v(k)
t=P
i,jσ([s(k)
t]ij)[ft]ij
P
i,jσ([s(k)
t]ij), (5)
where iandjare spatial indices for StandFt,kis channel
index for [st]i,j, and σis the sigmoid function. We repur-
poseStas weights so that different v(k)
t’s encode appear-
ance of distinct semantic components. We calculate {v(k)
t}
and{¯v(k)
t}respectively for Iand¯Iat each timestep t.
Appearance guidance. Our energy function gafor appear-
ance guidance can then be expressed as
ga({v(k)
t};{¯v(k)
t}) =PNa
k=1∥v(k)
t−¯v(k)
t∥2
2
Na.(6)
It penalizes difference in the appearance representations and
thus facilitates appearance transfer from ¯ItoI.
Guiding the generation process. Finally, we arrive at our
modified score estimate ˆϵtby including structure and ap-
pearance guidance alongside classifier-free guidance [21]:
ˆϵt= (1+ s)ϵθ(xt;t,c)−s ϵθ(xt;t,∅)+λsgs+λaga,(7)where s,λsandλaare the respective guidance strengths,
and∅denotes the null token input.
5. Experiments and Results
We report extensive qualitative and quantitative results to
demonstrate the effectiveness and generality of our ap-
proach for zero-shot controllable T2I diffusion. We present
additional results on text-guided image-to-image translation
and provide ablation studies on key method components.
5.1. Controllable T2I Diffusion
Baselines. ControlNet [59] and T2I-Adapter [33] learn an
auxiliary module to condition a pretrained diffusion model
on a guidance image. One such module is learned for each
condition type. Uni-ControlNet [62] instead learns adapters
shared by all condition types for all-in-one control. Dif-
ferent from these training-based methods, SDEdit [31] adds
noise to a guidance image and subsequently denoises it with
a pretrained diffusion model for guided image synthesis.
Prompt-to-Prompt (P2P) [20] and Plug-and-Play (PnP) [53]
manipulate attention weights and features inside pretrained
diffusion models for zero-shot image editing. We compare
our method with these strong baselines in our experiments.
7470
“An embroidery of a penguin”“A cartoon of a jeep”“An origami of a cello”“A sculpture of a castle”Condition
ControlNet
T2I-Adapter
Uni-ControlNet
Ours
Plug-and-Play
P2P
SDEdit-.85
SDEdit-.75Training-based MethodsTraining-free MethodsFigure 6. Qualitative comparison on controllable T2I diffusion. FreeControl achieves competitive spatial control and superior image-
text alignment in comparison to training-based methods. It also escapes the appearance leakage problem manifested by the training-free
baselines, producing high-quality images with rich content and appearance faithful to the text prompt.
MethodCanny HED Sketch Depth Normal
Self-Sim ↓CLIP↑LPIPS ↑Self-Sim ↓CLIP↑LPIPS ↑Self-Sim ↓CLIP↑LPIPS ↑Self-Sim ↓CLIP↑LPIPS ↑Self-Sim ↓CLIP↑LPIPS ↑
ControlNet [59] 0.042 0.300 0.665 0.040 0.291 0.609 0.070 0.314 0.668 0.058 0.306 0.645 0.079 0.304 0.637
T2I-Adapter 0.052 0.290 0.689 - - - 0.096 0.290 0.648 0.071 0.314 0.673 - - -
Uni-ControlNet 0.044 0.295 0.539 0.050 0.301 0.553 0.050 0.301 0.553 0.061 0.303 0.636 - - -
SDEdit-0.75 [31] 0.108 0.306 0.582 0.123 0.288 0.375 0.135 0.281 0.361 0.153 0.294 0.327 0.128 0.284 0.456
SDEdit-0.85 [31] 0.139 0.319 0.670 0.153 0.305 0.485 0.139 0.300 0.485 0.165 0.304 0.384 0.147 0.298 0.512
P2P [20] 0.078 0.253 0.298 0.112 0.253 0.194 0.194 0.251 0.096 0.142 0.248 0.167 0.100 0.249 0.198
PNP [53] 0.074 0.282 0.417 0.098 0.286 0.271 0.158 0.267 0.221 0.126 0.287 0.268 0.107 0.286 0.347
Ours 0.080 0.322 0.724 0.078 0.321 0.561 0.090 0.322 0.611 0.090 0.321 0.576 0.086 0.322 0.642
Table 1. Quantitative results on controllable T2I diffusion. FreeControl consistently outperforms all training-free baselines in structure
preservation, image-text alignment and appearance diversity as measured by Self-similarity distance, CLIP score and LPIPS distance. It
achieves competitive structure and appearance scores with the training-based baselines while demonstrate stronger image-text alignment.
Experiment setup. Similar to ControlNet [59], we report
qualitative results on eight condition types (sketch, normal,
depth, Canny edge, M-LSD line, HED edge, segmentation
mask, and human pose). We further employ several previ-
ously unseen control signals as input conditions (Figure 5),
and combine our method with all major versions of Stable
Diffusion (1.5, 2.1, and XL 1.0) to study its generalization
on diffusion model architectures.
For a fair comparison with the baselines, we adapt the
ImageNet-R-TI2I dataset from PnP [53] as our benchmark
dataset. It contains 30 images from 10 object categories.
Each image is associated with five text prompts originally
for the evaluation of text-guided image-to-image transla-
tion. We convert the images into their respective Canny
edge, HED edge, sketch, depth map, and normal map fol-
lowing ControlNet [59], and subsequently use them as input
conditions for all methods in our experiments.
Evaluation metrics. We report three widely adopted met-
rics for quantitative evaluation; Self-similarity distance [52]
measures the structural similarity of two images in the fea-
ture space of DINO-ViT [11]. A smaller distance suggests
better structure preservation. Similar to [53], we report self-similarity between the generated image and the dataset im-
age that produces the input condition. CLIP score [40] mea-
sures image-text alignment in the CLIP embedding space.
A higher CLIP score indicates a stronger semantic match
between the text prompt and the generated image. LPIPS
distance [60] measures the appearance deviation of the gen-
erated image from the input condition. Images with richer
appearance details yield higher LPIPS score.
Implementation details. We adopt keys from the first self-
attention in the U-Net decoder as the features Ft. We run
DDIM sampling on Ns= 20 seed images for 200steps to
obtain bases of size Nb= 64 . In the synthesis stage, we
run DDIM inversion on Igfor1000 steps, and sample Iand
¯Iby running 200steps of DDIM sampling. Structure and
appearance guidance are applied in the first 120steps. λs∈
[400,1000] ,λa= 0.2λs, and Na= 2in all experiments.
Qualitative results. As shown in Figure 4, FreeControl is
able to recognize diverse semantic structures from all condi-
tion modalities used by ControlNet [59]. It produces high-
quality images in close alignment with both the text prompts
and spatial conditions. Importantly, it generalizes well on
all major versions of Stable Diffusion, enabling effortless
7471
upgrade to future model architectures without retraining.
In Figure 5, we present additional results for condition
types not possible with previous methods. FreeControl gen-
eralizes well across challenging condition types for which
constructing training pairs is difficult. In particular, it en-
ables superior conditional control with common graphics
primitives ( e.g., mesh and point cloud), domain-specific
shape models ( e.g., face and body meshes), graphics soft-
ware viewports ( e.g., Blender [13] and AutoCAD [1]),
and simulated driving environments ( e.g., MetaDrive [29]),
thereby providing an appealing solution to visual design
preview and sim2real.
Comparison with baselines. Figure 6 and Table 1 com-
pare our methods to the baselines. Despite stronger struc-
ture preservation ( i.e., small self-similarity distances), the
training-based methods at times struggle to follow the text
prompt ( e.g.embroidery for ControlNet and origami for all
baselines) and yield worse CLIP scores. The loss of text
control is a common issue in training-based methods due to
modifications made to the pretrained models. Our method
is training-free, hence retaining strong text conditioning.
In contrast, training-free baselines are prone to appear-
ance leakage, where the appearance of condition images
is leaked to generated images, resulting in worse LIPIS
scores. This is because the generated image shares latent
states (SDEdit) or diffusion features (P2P & PnP) with the
condition. For example, all baselines inherit the texture-less
background in the embroidery example and the foreground
shading in the castle example. Our method instead decou-
ples structure and appearance, thereby avoiding the leakage.
FreeControl PnP Pix2Pix-zero P2P+NTI
Pre-processing 127.00 0 1236.00 0
Inversion 25.36 31.96 32.57 87.51
Sampling 23.95 10.09 33.03 11.51
Total 176.31 42.05 1301.60 99.02
Table 2. Runtime for training-free methods
Inference efficiency. We further study the inference cost
of our method in comparison to training-free baselines.
Table 2 reports the average inference time using a single
Nvidia A6000 GPU. The inference has three stages: (1)
Pre-processing stage , where category-level information is
extracted (analysis stage in FreeControl and the computa-
tion of edit direction in Pix2Pix-zero) ; (2) Inversion stage ,
for extracting the image-level latent representation from the
input condition; and (3) Sampling stage , for generating the
target image. FreeControl is slower than PnP ( 4.2×) and
P2P ( 1.8×), yet much faster than Pix2Pix-zero ( 0.14×).
When considering the reused basis and thus only counting
inversion and inference time, FreeControl can achieve 1.1×
that of PnP, 0.5×that of P2P, and 0.75×that of Pix2Pix-
zero, yet still generate diverse images.5.2. Ablation Study
Effect of guidance. As seen in Figure 7, structure guid-
ance is responsible for structure alignment ( −gsvs. Ours).
Appearance guidance alone has no impact on generation in
the absence of structure guidance ( −gavs.−gs,−ga). It
only becomes active after image structure has shaped up, in
which case it facilitates appearance transfer ( −gavs. Ours).
Choice of diffusion features Ft.Figure 8 compares results
using self-attention keys, queries, values, and their preced-
ing Conv features from up block.[1,2] in the U-Net decoder.
It reveals that up block.1 in general carries more structural
cues than up block.2, whereas keys better disentangle se-
mantic components than the other features.
Figure 7. Ablation on guidance effect. Top: “leather shoes” ;
Bottom: “cat, in the desert” .gsandgastand for structure and
appearance guidance, respectively.
Figure 8. Ablation on feature choice. Keys from self-attention of
upblock.1 in the U-Net decoder expose the strongest controllabil-
ity. PCA visualization of the features are in the insets.
6. Conclusion
We present FreeControl, a training-free method for spa-
tial control of any T2I diffusion model with any condi-
tion. FreeControl exploits the feature space of pretrained
T2I models, facilitates convenient control over many archi-
tectures and checkpoints, allows various challenging input
conditions on which most of the existing training-free meth-
ods fail, and achieves competitive synthesis quality with
training-based approaches. One limitation is that FreeCon-
torl relies on the DDIM inversion process to extract interme-
diate features of the guidance image and compute additional
gradients during the synthesis stage, resulting in increased
inference time. We hope our findings and analysis can shed
light on controllable visual content creation.
Acknowledgement : This work was partially supported by
NSF grant IIS-2339769, and by grants from McPherson Eye
Research Institute and VCGRE at UW Madison.
7472
References
[1] Autocad. https://www.autodesk.com/
products/autocad . 8
[2] Civitai. https://civitai.com/ . 3
[3] Hugging face. https://huggingface.co/ . 3
[4] Midjourney. https://www.midjourney.com .
2
[5] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel
Cohen-Or, and Dani Lischinski. Break-a-scene: Ex-
tracting multiple concepts from a single image. In
SIGGRAPH Asia , 2023. 3
[6] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal
Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski,
Ohad Fried, and Xi Yin. Spatext: Spatio-textual repre-
sentation for controllable image generation. In CVPR ,
2023. 2
[7] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash
Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala,
Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ed-
iffi: Text-to-image diffusion models with an ensemble
of expert denoisers. arXiv preprint arXiv:2211.01324 ,
2022. 2
[8] Tim Brooks, Aleksander Holynski, and Alexei A
Efros. Instructpix2pix: Learning to follow image edit-
ing instructions. In CVPR , 2023. 3
[9] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying
Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl:
Tuning-free mutual self-attention control for consis-
tent image synthesis and editing. In ICCV , 2023. 3,
4
[10] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser
Sheikh. Realtime multi-person 2d pose estimation us-
ing part affinity fields. In CVPR , 2017. 2
[11] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e
J´egou, Julien Mairal, Piotr Bojanowski, and Armand
Joulin. Emerging properties in self-supervised vision
transformers. In ICCV , 2021. 7
[12] Minghao Chen, Iro Laina, and Andrea Vedaldi.
Training-free layout control with cross-attention guid-
ance. arXiv preprint arXiv:2304.03373 , 2023. 3
[13] Blender Online Community. Blender - a 3D modelling
and rendering package . Blender Foundation, Stichting
Blender Foundation, Amsterdam, 2018. 8
[14] Prafulla Dhariwal and Alexander Nichol. Diffusion
models beat gans on image synthesis. In NeurIPS ,
2021. 3
[15] Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros,
and Aleksander Holynski. Diffusion self-guidance for
controllable image generation. In NeurIPS , 2023. 3, 6
[16] Patrick Esser, Robin Rombach, and Bjorn Ommer.
Taming transformers for high-resolution image syn-
thesis. In CVPR , 2021. 2[17] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-
nik, Amit H Bermano, Gal Chechik, and Daniel
Cohen-Or. An image is worth one word: Personal-
izing text-to-image generation using textual inversion.
InICLR , 2023. 3
[18] Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-Bin
Huang. Expressive text-to-image generation with rich
text. In ICCV , 2023. 3, 5
[19] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen,
Bo Zhang, Dongdong Chen, Lu Yuan, and Baining
Guo. Vector quantized diffusion model for text-to-
image synthesis. In CVPR , 2022. 2
[20] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aber-
man, Yael Pritch, and Daniel Cohen-Or. Prompt-to-
prompt image editing with cross attention control. In
ICLR , 2023. 2, 3, 6, 7
[21] Jonathan Ho and Tim Salimans. Classifier-free dif-
fusion guidance. arXiv preprint arXiv:2207.12598 ,
2022. 6
[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising
diffusion probabilistic models. In NeurIPS , 2020. 2, 3
[23] Jonathan Ho, Chitwan Saharia, William Chan, David J
Fleet, Mohammad Norouzi, and Tim Salimans. Cas-
caded diffusion models for high fidelity image gener-
ation. JMLR , 2022. 2
[24] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. LoRA: Low-rank adaptation of large
language models. In ICLR , 2022. 2
[25] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and
Alexei A Efros. Image-to-image translation with con-
ditional adversarial networks. In CVPR , 2017. 3
[26] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov,
Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal
Irani. Imagic: Text-based real image editing with dif-
fusion models. In CVPR , 2023. 3
[27] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Shechtman, and Jun-Yan Zhu. Multi-concept cus-
tomization of text-to-image diffusion. In CVPR , 2023.
3
[28] Alexander C. Li, Mihir Prabhudesai, Shivam Dug-
gal, Ellis Brown, and Deepak Pathak. Your diffusion
model is secretly a zero-shot classifier. In ICCV , 2023.
2
[29] Quanyi Li, Zhenghao Peng, Lan Feng, Qihang Zhang,
Zhenghai Xue, and Bolei Zhou. Metadrive: Compos-
ing diverse driving scenarios for generalizable rein-
forcement learning. TPAMI , 2022. 8
[30] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou
Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and
Yong Jae Lee. Gligen: Open-set grounded text-to-
image generation. In CVPR , 2023. 2
7473
[31] Chenlin Meng, Yutong He, Yang Song, Jiaming Song,
Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit:
Guided image synthesis and editing with stochastic
differential equations. In ICLR , 2022. 2, 3, 6, 7
[32] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch,
and Daniel Cohen-Or. Null-text inversion for editing
real images using guided diffusion models. In CVPR ,
2023. 3
[33] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang,
Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-
adapter: Learning adapters to dig out more control-
lable ability for text-to-image diffusion models. arXiv
preprint arXiv:2302.08453 , 2023. 2, 3, 6
[34] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh,
Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya
Sutskever, and Mark Chen. Glide: Towards photore-
alistic image generation and editing with text-guided
diffusion models. In ICML , 2022. 2, 3
[35] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni,
Huy V o, Marc Szafraniec, Vasil Khalidov, Pierre Fer-
nandez, Daniel Haziza, Francisco Massa, Alaaeldin
El-Nouby, et al. Dinov2: Learning robust vi-
sual features without supervision. arXiv preprint
arXiv:2304.07193 , 2023. 4
[36] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and
Jun-Yan Zhu. Semantic image synthesis with
spatially-adaptive normalization. In CVPR , 2019. 3
[37] Gaurav Parmar, Krishna Kumar Singh, Richard
Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu.
Zero-shot image-to-image translation. In SIGGRAPH ,
2023. 2, 3
[38] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar
Averbuch-Elor, and Daniel Cohen-Or. Localizing
object-level shape variations with text-to-image diffu-
sion models. In ICCV , 2023. 3
[39] Dustin Podell, Zion English, Kyle Lacey, Andreas
Blattmann, Tim Dockhorn, Jonas M ¨uller, Joe Penna,
and Robin Rombach. Sdxl: Improving latent diffusion
models for high-resolution image synthesis. arXiv
preprint arXiv:2307.01952 , 2023. 2
[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. Learning transferable visual models from nat-
ural language supervision. In ICML , 2021. 2, 7
[41] Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. Exploring the limits of trans-
fer learning with a unified text-to-text transformer.
JMLR , 2020. 2
[42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol,
Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv
preprint arXiv:2204.06125 , 2022. 2
[43] Ren ´e Ranftl, Katrin Lasinger, David Hafner, Kon-
rad Schindler, and Vladlen Koltun. Towards robust
monocular depth estimation: Mixing datasets for zero-
shot cross-dataset transfer. TPAMI , 2020. 2
[44] Robin Rombach, Andreas Blattmann, Dominik
Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-
resolution image synthesis with latent diffusion mod-
els. In CVPR , 2022. 1, 2
[45] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
U-net: Convolutional networks for biomedical image
segmentation. In MICCAI , 2015. 3
[46] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael
Pritch, Michael Rubinstein, and Kfir Aberman.
Dreambooth: Fine tuning text-to-image diffusion
models for subject-driven generation. In CVPR , 2023.
2, 3
[47] Chitwan Saharia, William Chan, Huiwen Chang,
Chris Lee, Jonathan Ho, Tim Salimans, David Fleet,
and Mohammad Norouzi. Palette: Image-to-image
diffusion models. In SIGGRAPH , 2022. 3
[48] Chitwan Saharia, William Chan, Saurabh Saxena,
Lala Li, Jay Whang, Emily L Denton, Kam-
yar Ghasemipour, Raphael Gontijo Lopes, Burcu
Karagol Ayan, Tim Salimans, et al. Photorealistic text-
to-image diffusion models with deep language under-
standing. In NeurIPS , 2022. 2
[49] Jascha Sohl-Dickstein, Eric Weiss, Niru Mah-
eswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In
ICML , 2015. 2
[50] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. In ICLR , 2021.
3, 4, 5
[51] Yang Song, Jascha Sohl-Dickstein, Diederik P
Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through
stochastic differential equations. In ICLR , 2021. 2,
3
[52] Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali
Dekel. Splicing vit features for semantic appearance
transfer. In CVPR , 2022. 4, 7
[53] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali
Dekel. Plug-and-play diffusion features for text-driven
image-to-image translation. In CVPR , 2023. 2, 3, 4,
6, 7
[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need.
InNeurIPS , 2017. 3
[55] Andrey V oynov, Kfir Aberman, and Daniel Cohen-
7474
Or. Sketch-guided text-to-image diffusion models. In
SIGGRAPH , 2023. 2
[56] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict:
Exact diffusion inversion via coupled transformations.
InCVPR , 2023. 3
[57] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu,
Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou.
Boxdiff: Text-to-image synthesis with training-free
box-constrained diffusion. In ICCV , 2023. 3
[58] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon,
Xiaolong Wang, and Shalini De Mello. Open-
vocabulary panoptic segmentation with text-to-image
diffusion models. In CVPR , 2023. 2
[59] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.
Adding conditional control to text-to-image diffusion
models. In ICCV , 2023. 1, 2, 3, 6, 7
[60] Richard Zhang, Phillip Isola, Alexei A Efros, Eli
Shechtman, and Oliver Wang. The unreasonable ef-
fectiveness of deep features as a perceptual metric. In
CVPR , 2018. 7
[61] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dim-
itris N Metaxas, and Jian Ren. Sine: Single image
editing with text-to-image diffusion models. In CVPR ,
2023. 3
[62] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jian-
min Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K
Wong. Uni-controlnet: All-in-one control to text-to-
image diffusion models. In NeurIPS , 2023. 2, 3, 6
7475
