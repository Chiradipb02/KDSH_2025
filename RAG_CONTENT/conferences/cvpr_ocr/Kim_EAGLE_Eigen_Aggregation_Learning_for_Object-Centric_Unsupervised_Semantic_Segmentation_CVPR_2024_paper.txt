EAGLE: Eigen Aggregation Learning for Object-Centric
Unsupervised Semantic Segmentation
Chanyoung Kim*Woojung Han*Dayun Ju Seong Jae Hwang‚Ä†
Yonsei University
{chanyoung, dnwjddl, juda0707, seongjae }@yonsei.ac.kr
Abstract
Semantic segmentation has innately relied on extensive
pixel-level annotated data, leading to the emergence of un-
supervised methodologies. Among them, leveraging self-
supervised Vision Transformers for unsupervised semantic
segmentation (USS) has been making steady progress with
expressive deep features. Yet, for semantically segment-
ing images with complex objects, a predominant challenge
remains: the lack of explicit object-level semantic encod-
ing in patch-level features. This technical limitation often
leads to inadequate segmentation of complex objects with
diverse structures. To address this gap, we present a novel
approach, EAGLE , which emphasizes object-centric repre-
sentation learning for unsupervised semantic segmentation.
Specifically, we introduce EiCue, a spectral technique pro-
viding semantic and structural cues through an eigenbasis
derived from the semantic similarity matrix of deep image
features and color affinity from an image. Further, by incor-
porating our object-centric contrastive loss with EiCue, we
guide our model to learn object-level representations with
intra- and inter-image object-feature consistency, thereby
enhancing semantic accuracy. Extensive experiments on
COCO-Stuff, Cityscapes, and Potsdam-3 datasets demon-
strate the state-of-the-art USS results of EAGLE with accu-
rate and consistent semantic segmentation across complex
scenes.
1. Introduction
Semantic segmentation plays a pivotal role in modern
vision, fundamentally advancing an array of diverse ar-
eas including medical imaging [21, 40], autonomous driv-
ing [14, 46], and remote sensing imagery [12, 28]. Never-
theless, its reliance on labeled data, while common across
nearly all vision tasks, is especially problematic due to the
laborious and time-consuming process of pixel-level anno-
*Equal contribution
‚Ä†Corresponding author
Project Page: https://micv-yonsei.github.io/eagle2024/
Input ImageSTEGOHPOursLabelEiCue
Input ImageEigen Aggregation Module
EigenvectorsSeg.HeadProj.Head
PrototypesObjNCE LossSeg. ResultClusterHead
EiCueClusterHead
(a) Overview of EAGLE
(b) Visualizing EAGLE & Baselines
ùìïFigure 1. We introduce EAGLE ,EigenAGgregation LEarning
for object-centric unsupervised semantic segmentation. (a)We
first leverage the aggregated eigenvectors, named EiCue, to ob-
tain the semantic structure knowledge of object segments in an im-
age. Based on both semantic and structural cues from the EiCue,
we compute object-centric contrastive loss to learn object-level se-
mantic representation. (b)A visual comparison between EAGLE
and other methods. Our object-level semantic segmentation results
robustly identify objects with complex semantics (e.g., blanket
with vivid stripe patterns) by exploiting strong semantic structure
cues from EiCue.
tation. In response to this challenge, various studies in se-
mantic segmentation tasks have drifted away from relying
solely on human-labeled annotations by exploring weakly-
supervised [1, 23, 26, 39, 48], semi-supervised [2, 27, 37],
andunsupervised semantic segmentation (USS) methodolo-
gies [8, 15, 16, 20, 22, 36, 43, 52].
Among these learning schemes, the unsupervised ap-
proach of USS clearly stands as the most challenging case.
Specifically, compared to the classical unsupervised seg-
mentation methods (e.g., K-means clustering) which pro-
duce segments without explicit semantics, USS additionally
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
3523
aims to derive semantically consistent local features (e.g.,
patch-level features) that aid the further class assignment
post-steps via clustering and the Hungarian matching algo-
rithm. That is, semantically plausible local features result in
accurate semantic segmentation results (e.g., Fig. 1b), but in
USS, this must be achieved without any labels.
Despite the glaring challenge, steady progress has been
shown in USS. For example, initial pioneering works have
emerged to maximize the mutual information across the
two different views of a single image [20, 36]. Recently,
network-based techniques such as STEGO [15] have fo-
cused on deriving patch-level semantic features with a self-
supervised pretrained model [6], showing a significant im-
provement compared to previous methods [8, 20,52]. How-
ever, while these methodologies have advanced USS, unre-
solved shortcomings still remain.
In particular, the recent network-based methods often
leverage a self-supervised Vision Transformer (ViT) to
learn patch-level features. While their patch-level fea-
tures proved to be useful for further USS inference steps
(e.g., K-means), the underlying object-level semantics are
not explicitly imposed in these patch-level features. To
grasp the ‚Äúobject-level semantics‚Äù, consider an example of
ablanket object as shown in Fig. 1b second row. As with
any object, blanket may easily appear with varying col-
ors and textures across different images. Without proper
object-level semantics, features corresponding to varying
regions of blanket may result in vastly different feature
representations. Ideally, though, the features corresponding
to all kinds of blanket should be mapped to similar fea-
tures, namely, object-level semantics. Thus, without care-
fully imposed object-level semantics, complex objects with
diverse structures and shapes may easily be partitioned into
multiple segments with wrong class labels or be merged
with nearby segments of different class labels. Thus, in
USS, an immense effort must be paid to learn the local fea-
tures (e.g., patch-level) with strong object-level semantics.
Our object-centric representation learning for USS aims
to capture such object-level semantics. Specifically, we first
need a semantic or structure cue in the object-centric view.
Several previous works utilized clustering methods such as
K-means or superpixel to obtain semantic cues [19], how-
ever, they mainly fixated on the generic image patterns, not
the object‚Äôs semantic or structural representation. Here, we
propose EiCue which provides semantic and structural cues
of objects via eigenbasis. Specifically, we utilize the seman-
tic similarity matrix obtained from the projected deep im-
age features obtained from ViT [6, 13] and the color affinity
matrix of the image to construct the graph Laplacian. The
corresponding eigenbasis captures the underlying semantic
structures of objects [29, 55], providing soft guidance to the
subsequent object-level feature refinement step.
Recall that accurate object-level semantics of an objectmust be consistent across images. Our object-centric con-
trastive learning framework explicitly imposes these traits
with a novel object-level contrastive loss. Specifically,
based on the object cues from EiCue, we derive learnable
prototypes for each object which enables intra- and inter-
image object-feature consistency. Through this comprehen-
sive learning process, our model effectively captures the
inherent structures within images, allowing it to precisely
identify semantically plausible object representations, the
key to advancing modern feature-based USS.
Contributions. Our main contributions are as follows:
‚Ä¢ We propose EiCue, using a learnable graph Laplacian, to
acquire a more profound understanding of the underlying
semantics and structural details within images.
‚Ä¢ We design an object-centric contrastive learning frame-
work that capitalizes on the spectral basis of EiCue to
construct robust object-level feature representations.
‚Ä¢ We demonstrate that our EAGLE achieves state-of-the-
art performance on unsupervised semantic segmentation,
supported by a series of comprehensive experiments.
2. Related Work
2.1. Unsupervised Semantic Segmentation
Semantic segmentation plays a crucial role in vision by
assigning distinct class labels to pixels. Yet, while the seg-
mentation performance strongly correlates with the label
quality, acquiring precise pixel-level ground truth labels is
a challenge on its own, especially for images with com-
plex structures. This naturally led to numerous attempts
to perform semantic segmentation in an unsupervised man-
ner [8, 15,16,20,22,36,43,52], that is, with no labels.
For instance, early works such as IIC [20] and AC [36]
utilized mutual information, while subsequent approaches
like InfoSeg [16] and PiCIE [8] integrated diverse features
for enhanced pixel learning. Recent studies have adopted
self-supervised, pretrained ViT models like DINO [6] for
top-down feature extraction. Namely, STEGO [15] demon-
strated a major step forward by distilling unsupervised fea-
tures into discrete semantic labels with the DINO backbone.
HP [43] interestingly utilizes contrastive learning to en-
hance semantic correlations among patch-level regions, but
this patch-level (local) refinement holds little object-level
understanding.
2.2. Spectral Techniques for Segmentation
Predating the aforementioned methods for semantic seg-
mentation, spectral techniques have long been offering in-
sights into diverse segmentation challenges in vision. Span-
ning some early pioneering works [30, 34,38,45] to con-
temporary efforts [3, 11,24,32,44], these techniques share
a common aim: to exploit the intrinsic spectral signatures
embedded within image regions. These graph-theoretic ap-
proaches are methodologically influenced by the affinity
3524
PrototypesPositive Pair
Negative Pair
Z‚®Ä
‚®Ä
‚®Ä
‚®Ä‚®Ç
‡∑©Z‚Ñíobjùê±‚Üíùê±
‚Ñíobj‡∑§ùê±‚Üí‡∑§ùê±‚Ñíscùê±‚Üí‡∑§ùê±
‚Ñísc‡∑§ùê±‚Üíùê±‚Ñínceùê±‚Üí‡∑§ùê±
‚Ñínce‡∑§ùê±‚Üíùê±‚Ñínceùê±
‡∑§ùê±
=‚Ñínceùê±‚Üí‡∑§ùê±+‚Ñínce‡∑§ùê±‚Üíùê±Eigen Aggregation Module
‡∑©Z
ZŒ¶ùëôZ
‡∑©ZŒ¶
‡∑©AcolorEigenvectors
ClusterLaplacian
Matrix‡∑©AsegEigenvectorsLaplacian
MatrixAseg
‡∑©Œ¶ùëô‚Ä≤
‚®Ç
‚®Ä: Inner Product
: Hadamard Product
Cluste rInference
CRF
Cluster
Proj .  Head Proj .  HeadAugmented Image Normal ImageImage
Encoder Kùê±
‡∑§ùê±
S
Image
Encoder
‡∑©K ‡∑®S
Seg. Head Seg. Head‚ä§K
‚ä§Acolor
‡∑©K‚®ÇEigen Aggregation ModuleùúÖsim
‡∑§ùúÖsim‡∑©‚Ñ≥eicue‚Ñ≥eicueFigure 2. The pipeline of EAGLE. Leveraging the Laplacian matrix, which integrates hierarchically projected image key features and
color affinity, the model exploits eigenvector clustering to capture object-level perspective cues defined as Meicueand ÀúMeicue. Distilling
knowledge from Meicue, our model further adopts an object-centric contrastive loss, utilizing the projected feature ZandÀúZ. The learnable
prototype Œ¶assigned from ZandÀúZ, acts as a singular anchor that contrasts positive objects and negative objects. Our object-centric
contrastive loss is computed in two distinct manners: intra(L obj)- and inter(L sc)-image to ensure semantic consistency.
matrix quality, which gave rise to recent methods utiliz-
ing the network features from the pretrained deep models.
For instance, Deep Spectral Methods [33] builds power-
ful Laplacian eigenvectors from the feature affinity matrix,
while EigenFunction [10] exploits the network-based learn-
able eigenfunctions to produce spectral embeddings. De-
spite steadily discovering the effectiveness of spectral meth-
ods on deep features for capturing complex object struc-
tures, their object-level semantics still require additional
methodological efforts, e.g., contrastive learning.
2.3. Object-centric Contrastive Learning
Contrastive learning approaches aim to maximize feature
similarities between similar units while minimizing them
between dissimilar ones. In the task of semantic segmen-
tation, patch-level representation learning [35, 49,51] is
widely used. However, this approach tends to overempha-
size fine details while neglecting high-level concepts (i.e.,
semantic relations between objects). This leads object-level
contrastive learning methods [17, 41,42,47,50,53,54]
to focus on balancing detailed perception with an object-
centric view, identifying objects in an unsupervised man-
ner. For instance, MaskContrast [47] and COMUS [53]
use unsupervised saliency to make pixel embeddings, while
Odin [18] and DetCon [17] utilize K-means clustering and
heuristic masks for sample generation, respectively. Re-
fining this, SlotCon [50] assigned pixels to learn slots for
semantic representation, and DINOSAUR [41] further im-
proved it by reconstructing self-supervised pretrained fea-
tures in the decoder, instead of the original inputs. However,
these methods [41, 50] rely solely on slots, potentially over-
looking high-level image features. In contrast, our approach
distills knowledge from clustered eigenvectors derived froma similarity matrix-based Laplacian capturing their object
semantic relationships.
3. Methods
As we begin describing our full pipeline shown in Fig. 2,
let us first cover the core USS framework based on pre-
trained models as in prior works [15, 43].
3.1. Preliminary
Unlabeled Images. Our approach is built exclusively
upon a set of images, without any annotations, denoted as
X={xb}B
b=1, where Bis the number of training im-
ages within a mini-batch. We also utilize a photometric
augmentation strategy Pto obtain an augmented image set
ÀúX={Àú xb}B
b=1=P(X).
Pretrained Features K.Then, for each input image xb,
we use a self-supervised pretrained vision transformer [6]
as an image encoder Fto obtain hierarchical attention key
features from the last three blocks as KL‚àí2=FL‚àí2(xb),
KL‚àí1=FL‚àí1(xb),KL=FL(xb), where L‚àí2,L‚àí1,L
is the third-to-last layer, the second-to-last layer, and the last
layer, respectively. Then, we concatenate them into a single
attention tensor K= [K L‚àí2;KL‚àí1;KL]‚ààRH√óW√óDK.
Similarly, we apply the same procedure for the augmented
image Àúxband obtain its attention tensor ÀúK‚ààRH√óW√óDK.
Semantic Features S.Although Kcontains some struc-
tural information about the objects based on the attention
mechanism, this is known for insufficient semantic infor-
mation to be considered for direct inference. Thus, for fur-
ther feature refinement, we compute the semantic features
S=SŒ∏(K)‚ààRH√óW√óDSandÀúS=SŒ∏(ÀúK)‚ààRH√óW√óDS,
where SŒ∏:RH√óW√óDK‚ÜíRH√óW√óDSis a learnable non-
linear segmentation head. For brevity, the total number of
3525
patches, denoted as H√óW, will be referred to as N.
Inference. During the inference time, given a new image,
its semantic feature Sbecomes the basis of further cluster-
ing for the final semantic segmentation output with conven-
tional evaluation setups such as the K-means clustering and
linear probing. Thus, as with prior pretrained feature-based
USS works [15, 43], training SŒ∏to output strong semantic
features Sin an unsupervised manner is the basic frame-
work of contemporary USS frameworks. We next describe
the remainder of the pipeline in Fig. 2which corresponds
to our methodological contributions for producing power-
fulobject-level semantic features.
3.2. EiCue via the Eigen Aggregation Module
Intuition tells us that the ‚Äúsemantically plausible‚Äù object-
level segments are groups of pixels precisely capturing the
object structure, even under complex structural variance.
For instance, a car segment must contain all of its parts
including the windshield, doors, wheels, etc. which may
all appear in different shapes and views. However, without
pixel-level annotations that provide object-level semantics,
this becomes an extremely challenging task of inferring the
underlying structure with zero object-level structural prior.
From this realization, our model EAGLE first aims to de-
rive a strong yet simple semantic structural cue, namely,
EiCue, based on the eigenbasis of the feature similarity
matrix as illustrated in Fig. 3. Specifically, we use the
well-known Spectral Clustering [7, 34,45] to obtain un-
supervised feature representations that capture the under-
lying non-linear structures for handling data with complex
patterns. This classically operates only in the color space
but may easily extend to utilize the similarity matrix con-
structed from any features. We observed that such a spectral
method becomes especially useful for complex real-world
images as in Fig. 4.
EiCue Construction. Let us describe the process of con-
structing EiCue in detail as shown in Fig. 3. The overall
framework generally follows the vanilla spectral clustering:
(1) from an adjacency matrix A, (2) construct the graph
Laplacian L, and (3) perform the eigendecomposition on L
to derive the eigenbasis Vfrom which the eigenfeatures are
used for the clustering. We describe each step below.
3.2.1 Adjacency Matrix Construction
Our adjacency matrix consists of two components: (1)
color affinity matrix and (2) semantic similarity matrix.
(I) Color Affinity Matrix Acolor: The color affinity matrix
leverages the RGB values of the image x. The color affinity
matrix is computed by the color distance. It utilizes the Eu-
clidean distance between patches, where pandqare specific
patch positions within the image. Here, ¬®x‚ààRH√óW√ó3de-
notes a resized version of x, scaled from its original image
resolution to patch resolution, to ensure compatibility with
the dimensions of other adjacency matrices. The resulting
Seg. 
Head K S
Transformer Encoder‚®ÇAttention 
Key FeatureTransformer Block
‚Ä¶Cluster 
Head +
0
L-2
L-1
LTransformer Block
Transformer BlockTransformer Block‚Ä¶
 Aùê¨ùêûùê† Generation ProcessEiCueInput Image ùê±
ConcatenateAùê¨ùêûùê†
Aùêúùê®ùê•ùê®ùê´ Aùê¨ùêûùê† Lùê¨ùê≤ùê¶ $ùêï
‚®Ç: Inner Product‚ä§SFigure 3. An illustration of the EiCue generation process. From
the input image, both color affinity matrix Acolorand semantic sim-
ilarity matrix Asegare derived, which are combined to form the
Laplacian Lsym. An eigenvector subset ÀÜVofLsymare clustered to
produce EiCue.
color affinity matrix, Acolor‚ààRN√óNthus captures the pair-
wise relationship between the patches based on the colors.
Specifically, we use the RBF kernel as the distance func-
tionAcolor(p, q) = exp/parenleftbig
‚àí‚à•¬®x(p)‚àí¬®x(q)‚à•2/2œÉc2/parenrightbig
where
œÉc>0is a free hyperparameter. Further, to ensure that
only nearby patches influence each other‚Äôs affinity values,
we hard-constrain the maximum distance of the patch pairs
such that we only compute the affinity between the patch
pairs with a predefined spatial distance.
(II) Semantic Similarity Matrix Aseg: The semantic sim-
ilarity matrix, denoted as Aseg‚ààRN√óN, is formed by the
product of tensor Sand its transpose S‚ä§. Tensor Sis de-
rived by hierarchically concatenating key attention features
from the last three layers of a pretrained vision transformer,
as processed through the segmentation head SŒ∏.
(III) Adjacency Matrix A: The final adjacency matrix
Ais the sum of Acolor andAseg:A=Acolor+Aseg,
which is also applicable to ÀúA. Our adjacency matrix amal-
gamates the high-level color information and the network-
based deep features to characterize semantic-wise relations.
The use of the image-based Acolor preserves the image‚Äôs
structural integrity and also complements the contextual in-
formation of the image. Following this, the incorporation of
the learnable tensor Sfor the Asegfurther strengthens this
aspect, enhancing the semantic interpretation of the object
without compromising the structural integrity and serving
as a vital cue for our learning process.
3.2.2 Eigendecomposition
To construct EiCue based on A, a Laplacian matrix is
created. Formally, the Laplacian Matrix is expressed as
L=D‚àíA, where Dis the degree matrix of Adefined
asD(i, i ) =/summationtextN
j=1A(i, j ). In our approach, we utilize
the normalized Laplacian matrix for its enhanced cluster-
ing capabilities. The symmetric normalized Laplacian ma-
trixLsymare defined as Lsym=D‚àí1
2LD‚àí1
2. Then, via
eigendecomposition on Lsym, the eigenbasis V‚ààRN√óN
is computed, where each column corresponds to a unique
3526
eigenvector. We then extract keigenvectors correspond-
ing to the ksmallest eigenvalues and concatenate them into
ÀÜV‚ààRN√ókwhere the ithrow corresponds to the kdimen-
sional eigenfeature of the ithpatch.
3.2.3 Differentiable Eigen Clustering
After obtaining eigenvectors ÀÜV, we perform the eigen-
vectors clustering process and extract the EiCue denoted as
Meicue‚ààRN. To cluster eigenvectors, we leverage a mini-
batch K-means algorithm based on cosine distance [31] be-
tween ÀÜVandC, denoted as P=ÀÜVC. Centers of clusters
C‚ààRk√óCare composed of learnable parameters. To learn
C, we further trained with a loss defined as follows:
Lx
eig=‚àí1
NNX
i=1CX
c=1Œ®icPic
, (1)
where Cdenotes pre-defined number of classes, Œ® :=
softmax(P) andPicandŒ®icrepresents the ithpatch and
thecthcluster number of PandŒ®. We apply same pro-
cedure to augmented image Àú xto getLÀú x
eig. By minimizing
Leig=1
2(Lx
eig+LÀú x
eig), we can obtain centers of clusters that
enable more effective clustering. Then we obtain EiCue as
Meicue(i) =argmax
c
Pic‚àílogCX
c‚Ä≤=1exp(P ic‚Ä≤)
.(2)
As the precision of cluster centroids improves, EiCue fa-
cilitates the mapping of patch ito its corresponding ob-
ject based on semantic structure. This serves as a mean-
ingful cue to stress semantic distinctions between different
objects, thereby enhancing the discriminative power of the
feature embeddings.
Remark. While similar to previous work [33] in using
eigendecomposition, our approach differs by enhancing fea-
ture vectors Swith a trainable segmentation head, unlike
their reliance on static vectors (i.e., K). Our method en-
hances Slearnable and adaptable via differentiable eigen
clustering, allowing the graph Laplacian and object seman-
tics to evolve. This dynamic integration of EiCue into the
learning process distinctly separates our methodology from
prior applications.
3.3. EiCue-based ObjNCELoss
For a successful semantic segmentation task, it is im-
portant not only to classify the class of each pixel accu-
rately but also to aggregate object representation and cre-
ate a segmentation map that reflects object semantic repre-
sentations. From this perspective, learning relationships in
an object-centric view is especially crucial in semantic seg-
mentation tasks. To capture the complex relationships be-
tween objects, our approach incorporates an object-centric
contrastive learning strategy, named ObjNCELoss, guided
by EiCue. This strategy is designed to refine the discrimina-
tive capabilities of feature embeddings S, emphasizing the
Input Image 1stEigenvector 2nd Eigenvector 3rd EigenvectorFigure 4. Visualizing eigenvectors derived from Sin the Eigen
Aggregation Module. These eigenvectors not only distinguish dif-
ferent objects but also identify semantically related areas, high-
lighting how EiCue captures object semantics and boundaries ef-
fectively.
distinctions among various object semantics. Before pro-
ceeding, we map both the projected feature Z‚ààRN√óDZ
andÀúZ‚ààRN√óDZ, using the linear projection head ZŒæ, de-
rived from the reshaped S‚ààRN√óDSandÀúS‚ààRN√óDS,
respectively. While the actual dimension sizes of DSand
DZare kept the same, we use different notations for ease of
explanation.
3.3.1 Object-wise Prototypes
To extract the representative object level semantic fea-
tures from projected feature Z, we construct adaptable pro-
totypes Œ¶lbased on the object lin aforementioned EiCue.
As we describe next, semantically representative prototypes
become the anchors for either pulling objects with similar
semantics while pushing away the different ones.
Let us describe how Œ¶is derived, which represents ob-
ject semantics from Z. We first update the object-wise pro-
totypes through the projected feature Zand a given Meicue,
derived from the clustered eigenbasis. Formally, for each
object lobtained from Meicue, the mask Mlis defined as
Ml(i) = 1 ifMeicue(i) = l,and0otherwise, where irep-
resents each position in Meicue. Then, applying the mask
Mlto the projected feature tensor ZgivesZl=Z‚äôMl,
where ‚äôdenotes the Hadamard product and Zlrepresents
a collection of feature representations from Zcorrespond-
ing to object l. Next, we compute medoid to select a single
vector from Zl, which then becomes the prototype Œ¶l. Let
Ilbe the set of indices where M(i‚ààIl)
l= 1to only consider
the indices of object l.Z(i)
lindicates the i-th feature vector
ofZl. Then, the prototype Œ¶lfrom the masked tensor Zlis
Œ¶l=Z(m‚àó)
l form‚àó= argmin
m‚ààIlX
i‚ààIlZ(m)
l‚àíZ(i)
l
2.(3)
Thus, Œ¶lacts as the semantic vector of object l, serving as
an anchor for the following object-centric contrastive loss.
3527
Label Input Image STEGO HP Ours
(a) COCO -Stuff (b) Cityscapes
Figure 5. A qualitative comparison of the (a) COCO-Stuff [4] and (b) Cityscapes [9] datasets trained using ViT-S/8 and ViT-B/8 as a
backbone, respectively. The comparison included previous state-of-the-art USS approaches, STEGO [15], HP [43], and ours.
3.3.2 Object-centric Contrastive Loss
Once we compute prototypes, we then step towards
object-centric contrastive loss between prototypes Œ¶and
feature vectors Z. Specifically, we compute object-centric
contrastive loss defined as follows:
Lx‚Üíx
obj=1
NNX
i=1w(i)
obj"
‚àílog 
exp((Z(i)
l¬∑Œ¶l)/œÑ)PC
j=1,jÃ∏=lexp((Z j¬∑Œ¶l)/œÑ)!#
,
(4)
where Cis the total number of unique predicted objects in
Meicue,(¬∑)denotes the cosine similarity, and œÑ > 0is the
temperature scalar. To emphasize the influence of feature
vectors with high similarity and direct the model‚Äôs focus
toward them, we weigh the loss based on the similarity in-
formation between vectors. The weight w(i)
objis defined as
w(i)
obj= (/summationtextN
j=1Ksim(i, j))/N, where Ksim‚ààRN√óNrepre-
sents the similarity matrix defined as Ksim=KK‚ä§.
While Eq. (4) aggregates the object-level features based
on the EiCue assignment, we note that another kind of ro-
bust consistency could be cleverly imposed with our photo-
metric augmented image Àúx. That is, since the photometric
augmentation does not apply structural changes, the aug-
mented image Àúxandxare structurally identical, allowing
us to make the following important assumption: the vec-
tors in the same positions of ZandÀúZshould have similar
object-level semantics. This assumption ultimately allows
us to create a new masked ÀúZ(Fig. 2,ÀúZin green box) of Àúx
based on Meicueofx. Thus, we apply the contrastive loss to
the augmented image Àúx, based on the prototypes Œ¶from the
non-augmented image xto guide the model to learn global
semantic consistency. To illustrate this concept, our seman-
tic consistency contrastive loss is defined as
Lx‚ÜíÀúx
sc=1
NNX
i=1w(i)
obj"
‚àílog 
exp((ÀúZ(i)
l¬∑Œ¶l)/œÑ)PC
j=1,jÃ∏=lexp((ÀúZj¬∑Œ¶l)/œÑ)!#
,
(5)where ÀúZ(i)
lnotes the i-th feature vector of projected feature
ÀúZfor object l. Concretely, we can formulate our object-
centric contrastive loss as Lx‚ÜíÀúx
nce =ŒªobjLx‚Üíx
obj+ŒªscLx‚ÜíÀúx
sc,
where 0< Œª obj<1and0< Œª sc<1are hyperparameters
that adjust the strength of each loss. Since the loss function
Lx‚ÜíÀúx
nce is asymmetric, we also take into account the opposite
case as LÀúx‚Üíx
nce=ŒªobjLÀúx‚ÜíÀúx
obj+ŒªscLÀúx‚Üíx
sc. Therefore, the final
object-centric contrastive loss function (ObjNCELoss) that
we optimize is as follows:
Lx‚ÜîÀúx
nce=Lx‚ÜíÀúx
nce+LÀúx‚Üíx
nce. (6)
3.4. Total Objective
To enhance the stability of the training process from the
outset, we additionally employ a correspondence distilla-
tion loss [15], Lcorr(see Supp D.1. for a detailed explana-
tion). In total, we minimize the following objective Ltotal:
Ltotal=ŒªnceLx‚ÜîÀúx
nce+ (1‚àíŒªnce)Lcorr+ŒªeigLeig, (7)
where 0‚â¶Œªnce‚â¶1and0‚â¶Œªeig‚â¶1are hyperparameters.
Here, Œªncestarts from zero and increases rapidly, indicating
the growing influence of Lx‚ÜîÀúx
nce during training.
4. Experiments
In this section, we first discuss the implementation de-
tails, including dataset configuration, evaluation protocols,
and detailed experimental settings. Then, we evaluate our
proposed method, EAGLE, both qualitatively and quantita-
tively while making a fair comparison with existing state-
of-the-art methods. We also demonstrate the effectiveness
of our proposed method through an ablation study. See the
supplementary material for additional details.
4.1. Experimental Settings
Implementation Details. We use DINO [6] pretrained vi-
sion transformer Fwhich is kept frozen during the training
process as in the prior works [15, 43]. The training sets are
3528
Input Image EiCue
 K-means
 Label
Figure 6. Comparison between K-means and EiCue. The bot-
tom row presents EiCue, highlighting its superior ability to cap-
ture subtle structural intricacies and understand deeper semantic
relationships, which is not as effectively achieved by K-means.
resized and five-cropped to 244√ó244. For segmentation
headSŒ∏, we use two layers of MLP with ReLU [15, 43],
and for projection head ZŒæwe constructed a single lin-
ear layer [43]. All backbones employed an embedding di-
mension DSandDZof 512. For the EiCue, we extract
4 eigenvectors from the eigenbasis V. In the inference
stage, we post-process the segmentation map with Dense-
CRF [15, 25,43]. See supplement for more details.
Datasets. We evaluate on (1) COCO-Stuff [4], (2)
Cityscapes [9], and (3) Potsdam-3 [20] datasets, in line with
methodologies established in prior works [8, 15,20,43]. (1)
The COCO-Stuff dataset is composed of its detailed pixel-
level annotations, facilitating comprehensive various object
understanding, while (2) Cityscapes presents diverse urban
street scenes. (3) The Potsdam-3 dataset is composed of
satellite imagery. Following the class selection protocols
from previous studies [8, 15,20,43], we use 27 classes
from both COCO-Stuff and Cityscapes. For Potsdam-3, we
use all 3 classes (see supplement for result).
Evaluation Details. To align with established benchmarks,
we adopt the evaluation protocols of prior works [15, 43].
Our evaluation includes (1) a linear probe, assessing rep-
resentational quality with a supervised linear layer on the
unsupervised model, and (2) clustering through semantic
segmentation via minibatch K-means based on cosine dis-
tance [31], without ground truth, compared against it using
Hungarian matching. We measure performance using pixel
accuracy (Acc.) and mean Intersection over Union (mIoU).
4.2. Evaluation Results
Here, we carefully compare our proposed method to ex-
isting USS works in both qualitative and quantitative ways.
We mainly set up two representative baselines [15, 43] from
the literature which share the same evaluation protocols.Table 1. Quantitative results on the COCO-Stuff dataset [4].
Method BackboneUnsupervised Linear
Acc. mIoU Acc. mIoU
DC [5] R18+FPN 19.9 - - -
MDC [5] R18+FPN 32.2 9.8 48.6 13.3
IIC [20] R18+FPN 21.8 6.7 44.5 8.4
PiCIE [8] R18+FPN 48.1 13.8 54.2 13.9
PiCIE+H [8] R18+FPN 50.0 14.4 54.8 14.8
SlotCon [50] R50 42.4 18.3 - -
DINO [6] ViT-S/16 22.0 8.0 50.3 18.1
+ STEGO [15] ViT-S/16 52.5 23.7 70.6 34.5
+ HP [43] ViT-S/16 54.5 24.3 74.1 39.1
+EAGLE (Ours) ViT-S/16 60.1 24.4 75.2 42.5
DINO [6] ViT-S/8 28.7 11.3 68.6 33.9
+ TransFGU [52] ViT-S/8 52.7 17.5 - -
+ STEGO [15] ViT-S/8 48.3 24.5 74.4 38.3
+ HP [43] ViT-S/8 57.2 24.6 75.6 42.7
+EAGLE (Ours) ViT-S/8 64.2 27.2 76.8 43.9
Table 2. Quantitative results on the Cityscapes dataset [9].
Method BackboneUnsupervised Linear
Acc. mIoU Acc. mIoU
MDC [5] R18+FPN 40.7 7.1 - -
IIC [20] R18+FPN 47.9 6.4 - -
PiCIE [8] R18+FPN 65.5 12.3 - -
DINO [6] ViT-S/8 34.5 10.9 84.6 22.8
+ TransFGU [52] ViT-S/8 77.9 16.8 - -
+ HP [43] ViT-S/8 80.1 18.4 91.2 30.6
+EAGLE (Ours) ViT-S/8 81.8 19.7 91.2 33.1
DINO [6] ViT-B/8 43.6 11.8 84.2 23.0
+ STEGO [15] ViT-B/8 73.2 21.0 90.3 26.8
+ HP [43] ViT-B/8 79.5 18.4 90.9 33.0
+EAGLE (Ours) ViT-B/8 79.4 22.1 91.4 33.4
Quantitative Evaluation: COCO-Stuff. In Table 1, our
EAGLE method sets new benchmarks on the COCO-Stuff
dataset. (I)With the ViT-S/8 backbone, EAGLE showcases
substantial improvements over existing methods in unsuper-
vised accuracy, with gains of +15.9 over STEGO [15] and
+7.0 over HP [43]. The unsupervised mIoU of EAGLE also
significantly outperforms other methods: +2.7 over STEGO
and+2.6 over HP. The linear accuracy and mIoU of EAGLE
both bring notable improvements over STEGO (+2.4 Acc.
and+5.6 mIoU) and HP (+1.2 Acc. and +1.2 mIoU). Com-
pared to SlotCon [50], which also emphasizes object-level
representations, our model excels with a +21.8 and+8.9 in
unsupervised mIoU and accuracy respectively. (II)With the
ViT-S/16 backbone, EAGLE maintains its dominance, gain-
ing+7.6 over STEGO and +5.6 over HP in unsupervised
Acc. The linear accuracy and mIoU of EAGLE outperforms
STEGO (+4.6 Acc. and +8.0 mIoU) and HP (+1.1 Acc. and
+3.4 mIoU) as well.
Quantitative Evaluation: Cityscapes. As shown in Ta-
ble2, our evaluations on the Cityscapes dataset show that
3529
Table 3. Ablation results on the COCO-Stuff dataset [4].
Exp.Lcorrx‚ÜíÀúx Àúx‚ÜíxMeicueMkmUnsupervised
# LobjLscLobjLsc Acc. mIoU
1‚úì 46.9 21.8
2‚úì ‚úì ‚úì ‚úì 59.3 23.2
3‚úì ‚úì ‚úì ‚úì 62.1 25.1
4‚úì ‚úì ‚úì ‚úì 61.6 24.8
5‚úì ‚úì ‚úì ‚úì 62.9 26.1
6‚úì ‚úì ‚úì ‚úì ‚úì ‚úì 55.1 17.0
7‚úì ‚úì‚úì‚úì‚úì‚úì 64.2 27.2
EAGLE notably excels in both ViT-S/8 and ViT-B/8 back-
bones. (I)For the ViT-S/8 backbone, EAGLE has achieved
significant unsupervised performance over STEGO ( +3.9
Acc. and +2.9 mIoU) and HP ( +1.7 Acc. and +1.3 mIoU).
(II)For the ViT-B/8 backbone, EAGLE significantly im-
proves both unsupervised Acc. and mIoU. The Cityscapes
dataset innately exhibits highly imbalanced pixel-level class
distributions, like the predominance of sky overtraffic
light pixels, typically forces a trade-off between Acc.
and mIoU [43], as seen with STEGO and HP excelling in
each metric respectively. However, EAGLE effectively bal-
ances these competing metrics, showcasing strong perfor-
mance in both areas despite such challenges.
Qualitative Analysis. In Fig. 5, we also qualitatively com-
pare our method to previous state-of-the-art models [15, 43]
on the COCO-Stuff and Cityscapes datasets trained us-
ing ViT-S/8 and ViT-B/8 backbone, respectively. Our
approach outperforms baselines by accurately segment-
ing objects and preserving details, unlike STEGO which
tends to segment multiple elements within a single object
furniture orroad , and HP neglects certain small ob-
jectssports(kite) ortraffic sign . Our model,
however, is trained at the object level with an understanding
of the structure of the image, which not only comprehends
the overall layout but also ensures no objects are missed.
4.3. Ablation Study
We further analyze our model with ablation studies and
discuss the results based on the full ablation results in Ta-
ble 3 denoted as Exp. #1 to Exp. #7. We primarily con-
ducted our experiments using the COCO-Stuff dataset us-
ing the DINO pretrained ViT-S/8 model. For more details,
please refer to the supplementary material.
Effect of EiCue. We validate the effectiveness
of EiCue ( Meicue) by comparing the performance of
our EiCue-enhanced method (Exp. #7) against a K-
means ( Mkm) approach (Exp. #6) in Table 3. The EiCue
result shows a notable improvement, capturing fine struc-
tural details that K-means misses. Fig. 6 visually demon-
strates how EAGLE better identifies object semantics and
structures compared to K-means.
ObjNCE Loss. Table 3 shows how different loss compo-
nents affect performance. The full model (Exp. #7) outper-
forms others, highlighting the effectiveness of combining
27.243.9
1020304050
(A)(B)(C)(D)(E)Unsup. mIoULinear mIoU00.010.020.030.040.05
123456789101112131415(a) Hierarchical Layers(b) Eigengap(%)Eigenvalue
ùíåFigure 7. (a)Analysis of hierarchical attention with the following
layer combinations (layer numbers in square brackets): (A): [1-6-
12], (B): [12], (C): [11-12], (D): [10-11-12], and (E): [9-10-11-
12]. (b)Analysis of eigengap to identify the optimal kfor eigen-
basis clustering, selected at the dashed line with maximal eigengap
(i.e., the gap between two consecutive eigenvalues).
all components. Notably, using Lobjalone (Exp. #3) signif-
icantly improves upon the baseline, underlining the impor-
tance of object-focused representation. The inclusion of Lsc
further refines quality, as evidenced by comparing Exp. #3
with Exp. #7. Additionally, the combined use of both Lnce
directions (Exp. #7) shows a synergistic effect over using
them individually (Exp. #4 and Exp. #5).
Combination of Hierarchical Attention and Eigengap.
In Fig. 7a, we present results from using various combina-
tions of hierarchical attention. The combination of the third-
to-last, second-to-last, and last layers from 12-layer archi-
tecture, demonstrated the best performance since the layers
closer to the end better capture the spatial information of
the image. For optimal eigenbasis clustering, we conduct
eigengap analysis in Fig. 7b. Since we choose kat the point
where the eigengap is maximized, we have selected k= 4.
5. Conclusion
In this study, we present EAGLE , a novel method that
addresses the persistent challenges in semantic segmenta-
tion with a focus on collecting semantic pairs through an
object-centric lens. Through empirical analysis using a
series of datasets, EAGLE showcases a remarkable capa-
bility to leverage the Laplacian matrix constructed from
attention-projected features and fortified by an object-level
prototype contrastive loss, which guarantees the accurate
association of objects with their corresponding semantic
pairs. Pioneering in utilizing dual advanced techniques, this
method marks a substantial advance in addressing the con-
straints of patch-level representation learning found in pre-
vious research. Consequently, EAGLE emerges as a power-
ful framework for encapsulating the semantic and structural
intricacies of images in contexts devoid of labels.
Acknowledgement. This work was supported in part by the National Research
Foundation of Korea (NRF) Grant funded by the Korean Government through the
Ministry of Science and ICT (MSIT) under Grant RS-2023-00219019, and Institute
of Information & Communications Technology Planning & Evaluation (IITP) Grant
funded by MSIT (No. 2020-0-01361, Artificial Intelligence Graduate School Pro-
gram (Yonsei University)).
3530
References
[1] Jiwoon Ahn and Suha Kwak. Learning pixel-level semantic
affinity with image-level supervision for weakly supervised
semantic segmentation. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
4981‚Äì4990, 2018. 1
[2] Inigo Alonso, Alberto Sabater, David Ferstl, Luis Monte-
sano, and Ana C Murillo. Semi-supervised semantic seg-
mentation with pixel-level contrastive learning from a class-
wise memory bank. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pages 8219‚Äì8228,
2021. 1
[3] Mirko Paolo Barbato, Paolo Napoletano, Flavio Piccoli, and
Raimondo Schettini. Unsupervised segmentation of hyper-
spectral remote sensing images with superpixels. Remote
Sensing Applications: Society and Environment, 28:100823,
2022. 2
[4] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-
stuff: Thing and stuff classes in context. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1209‚Äì1218, 2018. 6,7,8
[5] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and
Matthijs Douze. Deep clustering for unsupervised learning
of visual features. In Proceedings of the European confer-
ence on computer vision (ECCV), pages 132‚Äì149, 2018. 7
[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ¬¥e J¬¥egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 9650‚Äì9660, 2021. 2,3,6,7
[7] Jeff Cheeger. A lower bound for the smallest eigenvalue
of the laplacian. In Problems in Analysis: A Symposium
in Honor of Salomon Bochner (PMS-31), pages 195‚Äì200.
Princeton University Press, 2015. 4
[8] Jang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath
Hariharan. Picie: Unsupervised semantic segmentation us-
ing invariance and equivariance in clustering. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, pages 16794‚Äì16804, 2021. 1,2,7
[9] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3213‚Äì3223, 2016. 6,7
[10] Zhijie Deng and Yucen Luo. Learning neural eigenfunctions
for unsupervised semantic segmentation. In Proceedings of
the IEEE/CVF International Conference on Computer Vision
(ICCV), pages 551‚Äì561, 2023. 3
[11] Nameirakpam Dhanachandra, Khumanthem Manglem, and
Yambem Jina Chanu. Image segmentation using k-means
clustering algorithm and subtractive clustering algorithm.
Procedia Computer Science, 54:764‚Äì771, 2015. 2
[12] Lei Ding, Hao Tang, and Lorenzo Bruzzone. Lanet: Local
attention embedding to improve the semantic segmentation
of remote sensing images. IEEE Transactions on Geoscience
and Remote Sensing, 59(1):426‚Äì435, 2020. 1[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. ICLR, 2021. 2
[14] Di Feng, Christian Haase-Sch ¬®utz, Lars Rosenbaum, Heinz
Hertlein, Claudius Glaeser, Fabian Timm, Werner Wies-
beck, and Klaus Dietmayer. Deep multi-modal object de-
tection and semantic segmentation for autonomous driving:
Datasets, methods, and challenges. IEEE Transactions on
Intelligent Transportation Systems, 22(3):1341‚Äì1360, 2020.
1
[15] Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah
Snavely, and William T. Freeman. Unsupervised semantic
segmentation by distilling feature correspondences. In Inter-
national Conference on Learning Representations, 2022. 1,
2,3,4,6,7,8
[16] Robert Harb and Patrick Kn ¬®obelreiter. Infoseg: Unsuper-
vised semantic image segmentation with mutual information
maximization. In DAGM German Conference on Pattern
Recognition, pages 18‚Äì32. Springer, 2021. 1,2
[17] Olivier J H ¬¥enaff, Skanda Koppula, Jean-Baptiste Alayrac,
Aaron Van den Oord, Oriol Vinyals, and Joao Carreira.
Efficient visual pretraining with contrastive detection. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 10086‚Äì10096, 2021. 3
[18] Olivier J H ¬¥enaff, Skanda Koppula, Evan Shelhamer, Daniel
Zoran, Andrew Jaegle, Andrew Zisserman, Jo Àúao Carreira,
and Relja Arandjelovi ¬¥c. Object discovery and representa-
tion networks. In European Conference on Computer Vision,
pages 123‚Äì143. Springer, 2022. 3
[19] Jyh-Jing Hwang, Stella X Yu, Jianbo Shi, Maxwell D
Collins, Tien-Ju Yang, Xiao Zhang, and Liang-Chieh Chen.
Segsort: Segmentation by discriminative sorting of seg-
ments. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision, pages 7334‚Äì7344, 2019. 2
[20] Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant in-
formation clustering for unsupervised image classification
and segmentation. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pages 9865‚Äì9874,
2019. 1,2,7
[21] Feng Jiang, Aleksei Grigorev, Seungmin Rho, Zhihong Tian,
YunSheng Fu, Worku Jifara, Khan Adil, and Shaohui Liu.
Medical image semantic segmentation based on deep learn-
ing. Neural Computing and Applications, 29:1257‚Äì1265,
2018. 1
[22] Robin Karlsson, Tomoki Hayashi, Keisuke Fujii, Alexan-
der Carballo, Kento Ohtani, and Kazuya Takeda. Improv-
ing dense representation learning by superpixelization and
contrasting cluster assignment. In British Machine Vision
Conference, 2021. 1,2
[23] Hoel Kervadec, Jose Dolz, Meng Tang, Eric Granger, Yuri
Boykov, and Ismail Ben Ayed. Constrained-cnn losses for
weakly supervised segmentation. Medical Image Analysis,
54:88‚Äì99, 2019. 1
[24] Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and
Hamed Pirsiavash. Mean shift for self-supervised learning.
3531
InProceedings of the IEEE/CVF International Conference
on Computer Vision, pages 10326‚Äì10335, 2021. 2
[25] Philipp Kr ¬®ahenb ¬®uhl and Vladlen Koltun. Efficient inference
in fully connected crfs with gaussian edge potentials. Ad-
vances in Neural Information Processing Systems, 24, 2011.
7
[26] Hyeokjun Kweon, Sung-Hoon Yoon, and Kuk-Jin Yoon.
Weakly supervised semantic segmentation via adversarial
learning of classifier and reconstructor. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 11329‚Äì11339, 2023. 1
[27] Xin Lai, Zhuotao Tian, Li Jiang, Shu Liu, Hengshuang Zhao,
Liwei Wang, and Jiaya Jia. Semi-supervised semantic seg-
mentation with directional context-aware consistency. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, pages 1205‚Äì1214, 2021. 1
[28] Rui Li, Shunyi Zheng, Ce Zhang, Chenxi Duan, Jianlin Su,
Libo Wang, and Peter M Atkinson. Multiattention network
for semantic segmentation of fine-resolution remote sensing
images. IEEE Transactions on Geoscience and Remote Sens-
ing, 60:1‚Äì13, 2021. 1
[29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 10012‚Äì10022, 2021. 2
[30] Ming Luo, Yu-Fei Ma, and Hong-Jiang Zhang. A spatial
constrained k-means approach to image segmentation. In
Fourth International Conference on Information, Communi-
cations and Signal Processing, 2003 and the Fourth Pacific
Rim Conference on Multimedia. Proceedings of the 2003
Joint, pages 738‚Äì742. IEEE, 2003. 2
[31] James MacQueen et al. Some methods for classification and
analysis of multivariate observations. In Proceedings of the
Fifth Berkeley Symposium on Mathematical Statistics and
Probability, pages 281‚Äì297. Oakland, CA, USA, 1967. 5,
7
[32] R Manavalan and K Thangavel. Trus image segmentation us-
ing morphological operators and dbscan clustering. In 2011
World Congress on Information and Communication Tech-
nologies, pages 898‚Äì903. IEEE, 2011. 2
[33] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and
Andrea Vedaldi. Deep spectral methods: A surprisingly
strong baseline for unsupervised semantic segmentation and
localization. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 8364‚Äì
8375, 2022. 3,5
[34] Andrew Ng, Michael Jordan, and Yair Weiss. On spectral
clustering: Analysis and an algorithm. Advances in Neural
Information Processing Systems, 14, 2001. 2,4
[35] Pedro O O Pinheiro, Amjad Almahairi, Ryan Benmalek, Flo-
rian Golemo, and Aaron C Courville. Unsupervised learning
of dense visual representations. Advances in Neural Infor-
mation Processing Systems, 33:4489‚Äì4500, 2020. 3
[36] Yassine Ouali, C ¬¥eline Hudelot, and Myriam Tami. Autore-
gressive unsupervised image segmentation. In European
Conference on Computer Vision, pages 142‚Äì158. Springer,
2020. 1,2[37] Yassine Ouali, C ¬¥eline Hudelot, and Myriam Tami. Semi-
supervised semantic segmentation with cross-consistency
training. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 12674‚Äì
12684, 2020. 1
[38] Thrasyvoulos N Pappas and Nikil S Jayant. An adaptive clus-
tering algorithm for image segmentation. In International
Conference on Acoustics, Speech, and Signal Processing,,
pages 1667‚Äì1670. IEEE, 1989. 2
[39] Shenghai Rong, Bohai Tu, Zilei Wang, and Junjie Li.
Boundary-enhanced co-training for weakly supervised se-
mantic segmentation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
19574‚Äì19584, 2023. 1
[40] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In International Conference on Medical Image Com-
puting and Computer-Assisted Intervention, pages 234‚Äì241.
Springer, 2015. 1
[41] Maximilian Seitzer, Max Horn, Andrii Zadaianchuk, Do-
minik Zietlow, Tianjun Xiao, Carl-Johann Simon-Gabriel,
Tong He, Zheng Zhang, Bernhard Sch ¬®olkopf, Thomas Brox,
and Francesco Locatello. Bridging the gap to real-world
object-centric learning. In The Eleventh International Con-
ference on Learning Representations, 2023. 3
[42] Ramprasaath R Selvaraju, Karan Desai, Justin Johnson, and
Nikhil Naik. Casting your model: Learning to localize im-
proves self-supervised representations. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 11058‚Äì11067, 2021. 3
[43] Hyun Seok Seong, WonJun Moon, SuBeen Lee, and Jae-Pil
Heo. Leveraging hidden positives for unsupervised semantic
segmentation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 19540‚Äì
19549, 2023. 1,2,3,4,6,7,8
[44] Jianbing Shen, Xiaopeng Hao, Zhiyuan Liang, Yu Liu, Wen-
guan Wang, and Ling Shao. Real-time superpixel segmenta-
tion by dbscan clustering algorithm. IEEE Transactions on
Image Processing, 25(12):5933‚Äì5942, 2016. 2
[45] Jianbo Shi and Jitendra Malik. Normalized cuts and image
segmentation. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 22(8):888‚Äì905, 2000. 2,4
[46] Marvin Teichmann, Michael Weber, Marius Zoellner,
Roberto Cipolla, and Raquel Urtasun. Multinet: Real-time
joint semantic reasoning for autonomous driving. In 2018
IEEE Intelligent Vehicles Symposium (IV), pages 1013‚Äì1020.
IEEE, 2018. 1
[47] Wouter Van Gansbeke, Simon Vandenhende, Stamatios
Georgoulis, and Luc Van Gool. Unsupervised semantic seg-
mentation by contrasting object mask proposals. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision, pages 10052‚Äì10062, 2021. 3
[48] Alexander Vezhnevets and Joachim M Buhmann. Towards
weakly supervised semantic segmentation by means of mul-
tiple instance and multitask learning. In 2010 IEEE Com-
puter Society Conference on Computer Vision and Pattern
Recognition, pages 3249‚Äì3256. IEEE, 2010. 1
3532
[49] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong,
and Lei Li. Dense contrastive learning for self-supervised
visual pre-training. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
3024‚Äì3033, 2021. 3
[50] Xin Wen, Bingchen Zhao, Anlin Zheng, Xiangyu Zhang, and
Xiaojuan Qi. Self-supervised visual representation learning
with semantic grouping. In Advances in Neural Information
Processing Systems, 2022. 3,7
[51] Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen
Lin, and Han Hu. Propagate yourself: Exploring pixel-level
consistency for unsupervised visual representation learning.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 16684‚Äì16693, 2021.
3
[52] Zhaoyuan Yin, Pichao Wang, Fan Wang, Xianzhe Xu, Han-
ling Zhang, Hao Li, and Rong Jin. Transfgu: a top-down ap-
proach to fine-grained unsupervised semantic segmentation.
InEuropean Conference on Computer Vision, pages 73‚Äì89.
Springer, 2022. 1,2,7
[53] Andrii Zadaianchuk, Matthaeus Kleindessner, Yi Zhu,
Francesco Locatello, and Thomas Brox. Unsupervised se-
mantic segmentation with self-supervised object-centric rep-
resentations. In The Eleventh International Conference on
Learning Representations, 2023. 3
[54] Xiao Zhang and Michael Maire. Self-supervised visual rep-
resentation learning from hierarchical grouping. Advances
in Neural Information Processing Systems, 33:16579‚Äì16590,
2020. 3
[55] Xiaosong Zhang, Yunjie Tian, Lingxi Xie, Wei Huang, Qi
Dai, Qixiang Ye, and Qi Tian. Hivit: A simpler and more
efficient design of hierarchical vision transformer. In The
Eleventh International Conference on Learning Representa-
tions, 2023. 2
3533
