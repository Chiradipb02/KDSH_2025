Relightful Harmonization: Lighting-aware Portrait Background Replacement
Mengwei Ren12∗Wei Xiong2Jae Shin Yoon2Zhixin Shu2
Jianming Zhang2HyunJoon Jung2Guido Gerig1He Zhang2
1New York University2Adobe
Figure 1. Relightful Harmonization on four real-world images. Each set shows a direct composition ( upper left ) of the foreground subject
onto a new backgound ( lower left ), and our harmonized result ( right ) that accounts for both lighting andcolor .
Abstract
Portrait harmonization aims to composite a subject into
a new background, adjusting its lighting and color to en-
sure harmony with the background scene. Existing harmo-
nization techniques often only focus on adjusting the global
color and brightness of the foreground and ignore crucial
illumination cues from the background such as apparent
lighting direction, leading to unrealistic compositions. We
introduce Relightful Harmonization, a lighting-aware diffu-
sion model designed to seamlessly harmonize sophisticated
lighting effect for the foreground portrait using any back-
ground image. Our approach unfolds in three stages. First,
we introduce a lighting representation module that allows
our diffusion model to encode lighting information from tar-
get image background. Second, we introduce an alignment
network that aligns lighting features learned from image
background with lighting features learned from panorama
*Work done during an internship at Adobe.environment maps, which is a complete representation for
scene illumination. Last, to further boost the photorealism
of the proposed method, we introduce a novel data simula-
tion pipeline that generates synthetic training pairs from a
diverse range of natural images, which are used to reﬁne
the model. Our method outperforms existing benchmarks
in visual ﬁdelity and lighting coherence, showing superior
generalization in real-world testing scenarios, highlighting
its versatility and practicality.
1. Introduction
Portrait harmonization [ 59,63] stands as a crucial element
in both photography and creative image editing, seeking
to seamlessly composite a subject into a new background
while maintaining realism and aesthetic uniformity in terms
of lighting and color. The process initiates with the seg-
mentation of the subject from its original image, followed
by the composition into a new background. To enhance vi-
sual consistency, the subsequent step entails meticulous ad-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
6452
justments to the foreground, aligning it with the new back-
ground—considering factors such as color, brightness, sat-
uration, and lighting conditions. The manual effort could
be labor-intensive, particularly when dealing with intricate
lighting scenarios in portraiture.
There are two principal sets of methods that could auto-
matically adjust the foreground to match the background for
human portraits: (1) image harmonization techniques, and
(2) portrait relighting methods. Harmonization-based meth-
ods [ 5,7,8,14–16,22,26,29,30,56,61,67,68] aim to
match the color statistics of the foreground with those of the
background, by adjusting the foreground color tone, con-
trast, and illumination. Yet, they often overlook the light-
ing characteristics and leave the foreground illumination ef-
fects unchanged, such as the lighting direction and shadows,
potentially resulting in an unnatural appearance when the
background has distinct lighting conditions. For instance,
compositing a person photographed under a top-down light
into a sunset scene might make the composite non-realistic
to the human eye. On the other hand, recent work on portrait
relighting [ 33,38,42,55,64,69,72,76,79] are designed
for matching the lighting of the subject towards a new en-
vironment by using the paired training data acquired with
the light stage system [ 10]. Nevertheless, current relighting
methods often require dynamic range (HDR) panorama en-
vironment maps [ 42] during training and inference, which
are not always feasible to acquire, especially in casual pho-
tography settings. In this work, we explore the possibil-
ity of generating realistic and lighting-aware composition
images in a straightforward harmonization set up. Given
a foreground image (with its corresponding alpha mask)
and an arbitrary background image, we propose a uniﬁed
and end-to-end framework that encompasses both color and
lighting harmonization. We approach the task through a
conditional generative framework, leveraging a pretrained
diffusion model [ 3,8], and develop a three-stage training
pipeline.
In the ﬁrst phase, we conduct Lighting-aware Diffusion
Training to integrate explicit lighting conditioning into a
pretrained diffusion model. This involves a lighting repre-
sentation learning module that derives lighting conditions
from a selected background image. The resulting light-
ing information is then integrated into the diffusion UNet
backbone to guide the generative process. The training is
performed on a pairwise relighting-specialized light stage
dataset to effectively capture the lighting dynamics.
Given the challenge of accurately inferring environ-
mental lighting from a single background image, which
is inherently an ill-posed problem, we employ paired en-
vironment maps to augment the physical plausibility of
our background-derived lighting representation. This is
achieved through a second stage of Lighting Representa-
tion Alignment , designed to align the lighting representa-tion extracted from background images with that learned
from their corresponding panorama environment maps.
Finally, we perform Finetuning for Photorealism on an
expanded dataset to improve the photorealism of the harmo-
nization. We propose a novel data synthesis pipeline using
our initially trained model to create additional data from
natural images. Notably, once trained, our pipeline does
notrely on any external environment maps, which greatly
empowers the proposed framework for ﬂexible background
replacement and portrait harmonization.
Our contributions are threefold. (1) We enable the light-
ing effects to be encoded in a pretrained image-conditioned
latent diffusion model by incorporating a spatial lighting
feature extraction and conditioning module to the diffu-
sion backbone. The background-extracted lighting repre-
sentation is further aligned with the feature extracted from
panorama environment map to ensure better physical plau-
sibility. (2) We use our model as a data augmenter and pro-
pose a novel data simulation pipeline to synthesize training
pairs from natural images. The model is then reﬁned with
the enlarged dataset to further boost the photorealism of the
results. (3) Compared with existing harmonization and re-
lighting methods, our pipeline demonstrates improvements
of the harmonized results in both lighting coherence visual
ﬁdelity, providing a versatile solution for real-world portrait
harmonization in a variety of settings.
2. Related Works
Image Harmonization aims to rectify color, contrast,
and style differences between foreground and back-
ground to ensure natural and consistent composition.
In deep learning, this task is approached as an end-to-
end image-to-image translation problem [ 5,7,8,14–
16,22,30,56,57,61,67,80], where the network is trained
to predict a harmonized image from the input composite.
Pixel-aligned datasets are created by altering foreground
color in real images with pre-designed [ 7] or learned [ 41]
augmentations. Yet, existing harmonization methods
primarily focus on global color adjustment, overlooking the
subtle but important discrepancies between foreground and
background lighting, i.e., direction, intensity, and shadow.
This can lead to a harmonized image that, despite matching
colors, still appears unnatural due to mismatched lighting
conditions. Therefore, we postulate that enhancing the
lighting-awareness of harmonization models is a vital yet
underexplored area for natural and realistic composition.
Portrait Relighting Recent advancements in portrait re-
lighting has been driven by deep learning methods [ 33,34,
38,42,44,55,64,69,72,76,79]. They leverage super-
vised training with the paired training data acquired with
the light stage system [ 10]. These methods require a tar-
get HDR enviroment map as the external input source, and
6453
Bg-conditioned ModelII. Lighting Alignment
Bg
Composite
InputRendered 
Target ( Tgt)Environment 
map ( Env)Rendered 
Background ( Bg)
Synthetic Dataset
III. FinetuningforPhotorealism
Synthetic 
Background ( S-Bg )Synthetic 
Input (S-Input)UNetenvFinal Model
Input Tgt UNetbg
Bg
Real 
Target ( R-Tgt ) 
Env-conditioned Model
Env Input Tgt UNetenv
Pretrained
 S-BgS-InputI. Lighting-aware Diffusion
Light Stage Rendered DatasetR-Tgt
Figure 2. The Pipeline of Relightful Harmonization. In Stage I, a lighting representation module is integrated into the diffusion model,
conditioning the generation on lighting information encoded from the background image, trained with a light stage dataset for relighting
(lower left ). Stage II aligns lighting features derived from the background with the environment map for enhanced physical accuracy.
Finally, Stage III reﬁnes the model on a real image dataset ( lower right ) obtained via a novel data simulation pipeline.
typically involve the intermediate prediction of surface nor-
mals, albedo, and/or a set of diffuse and specular maps with
ground truth supervision. However, reliance on HDR maps
for background replacement and harmonization tasks poses
signiﬁcant limitations on their applicability in everyday sce-
narios where HDR maps cannot be easily captured along-
side [ 42]. In our framework, we do not require any ex-
ternal input sources except a single target background im-
age. Furthermore, many current relighting systems employ
multistage frameworks and/or heavily rely on external pack-
ages [ 12]. The accuracy and performance of these systems
are consequently contingent on the precision of each indi-
vidual stage, making the overall process complicated and
prone to errors propagated through these intermediate steps.
Additionally, the datasets commonly employed for training
are rendered from limited light stage illumination acqui-
sition, which means the target images utilized during the
training phase are not captured in real-world conditions but
are rendered composites. Therefore, its generalization on
unseen images, as well as the photorealism when applied to
arbitrary background replacement tasks remain unclear.
Diffusion Models [11,19,51,52] have signiﬁcantly ad-
vanced the image and video synthesis quality [ 18,20,24,
25,45,47]. Image-conditioned diffusion models [ 48,62,
73] typically take an image as additional input to perform
an image-conditioned generation such as image enhance-
ment [ 23,25,28,46], harmonization [ 6,31,32] and trans-
lation [ 27,40,50,62,73,78], typically trained on task-
speciﬁc pairwise data. Recently, the application of pre-
trained text-to-image diffusion models [ 39,47,49] has been
extended to image editing tasks [ 1–4,9,13,17,21,35–37,
53,54,58,60,65,77]. These models leverage text-image
correlations to perform context modiﬁcations in image edit-
ing, such as ‘adding a sunset’ with InstructPix2Pix [ 3],which is loosely connected to our lighting-aware set up.
However, text-based editing does not incorporate spatial in-
formation, thus lacking ﬁnegrained control to the model. In-
stead, we propose to use a spatial lighting representation as
the new ‘instruction’ that guide the diffusion model to per-
form lighting-aware editing.
3. Method
We aim to develop a conditional diffusion model that pro-
cesses a composite image (along with its alpha mask) as
the input, conditioned on the target background, and pro-
duce color andlighting harmonized output. To do so, we
develop a three-stage training strategy presented in Fig. 2.
Stage I: Lighting-aware Diffusion Training: We build
our model on a pretrained diffusion model [ 3] and enable
its lighting awareness by attaching a lighting representa-
tion learning branch to encode lighting information from
the background image, which is then injected into the UNet
backbone as illustrated in Fig. 2-I. The training is conducted
with relighting-specialized light stage rendered dataset, as
shown on the bottom left of Fig. 2-I.
Stage II: Lighting Representation Alignment: As one
of our of goals is to enable lighting-aware portrait harmo-
nization without relying on environment maps during infer-
ence, we propose a representation alignment step (Fig. 2-II)
to adapt the lighting representation extracted from a back-
ground image towards the learned representation from its
environment map. We assume the aligned representation is
more robust and physically plausible.
Stage III: Finetuning for Photorealism: In the third stage,
we ﬁnetune our model using high-quality pixel-aligned
training pairs from natural images, where these paired
datasets are generated via a novel data synthesis pipeline
(Fig. 2-III) using the stage 2 model as a data augmenter.
6454
3.1. Lighting­aware Diffusion Training
Our lighting-aware diffusion model learns to generate a har-
monized image given the composite input, conditioned on a
lighting representation extracted from the target background
image. Adhering to established practice for training relight-
ing models [ 42,55,75], we assume access to the light stage
rendered training dataset. A training tuple includes the input
image (alongside its alpha mask), the target background, the
target environment map, as well as the target image. An ex-
ample of training data is shown on the bottom left of Fig. 2.
Formally, we represent a rendered image sample as xa
i,
indicating a portrait image of subject iilluminated under the
lighting condition a. The corresponding environment map
is denoted as za
i, and the background image ya
iforxa
iis
generated by projecting the HDR map with a speciﬁed ﬁeld
of view and resolution. Subject masks miare obtained us-
ing methods described in [ 70,71].
Lighting Conditioning: We further modify the diffusion
backbone to incorporate explicit lighting conditions. As
depicted in Fig. 2-I, a lighting conditioning branch is in-
tegrated atop the UNet backbone, injecting a lighting repre-
sentation fencoded from the target background image by
a CNNF, at multiple scales within the UNet. The condi-
tioning mechanism is designed in a similar fashion as [ 73]
where conditional feature maps are added to the UNet fea-
tures at respective resolution within the encoder.
With the light stage dataset, we postulate that the light-
ing representation fcan be learned from the pairwise train-
ing. Speciﬁcally, a training tuple from the same subject i
is sampled as (xa,m,yb,xb). Noise is progressively added
to the target image xbuntil time step t, resulting in a noisy
imagexb
t. The UNet, denoted as Ubg, is conditioned on
the background-extracted lighting feature Fbg(yb), and is
trained to predict the noise ϵ, with the following objective:
LD=Exa,yb,xb
0,t,ϵ/bracketleftBig/vextenddouble/vextenddoubleϵ−Ubg(xb
t,t,xa,Fbg(yb))/vextenddouble/vextenddouble2
2/bracketrightBig
(1)
whereϵ∼ N(0,1). At this stage, we initilize the weights
of the UNet backbone from [ 3], and jointly train both UNet
and the conditioning branch.
3.2. Lighting Representation Alignment
Given that a background image is a partial projection of the
environment map which encapsulates panoramic lighting
information (see example on bottom left of Fig. 2), we sus-
pect that the lighting cues learned from an environment map
will be inherently more comprehensive than those from fbg,
implying that an environment-conditioned model could po-
tentially offer more physically plausible relighting under the
same training scenario. This is further empirically veriﬁed
in our ablation detailed in Sec. 4.5. However, in real-world
photography, environment maps are usually not co-acquired
which poses practical challenges, limiting the applicabilityof environment map dependent models. To circumvent this,
we align the lighting representation extracted from a back-
ground image with features derived from its ground truth
environment map, ultimately enabling effective portrait har-
monization with just a single background image.
As shown in Fig. 2II., we ﬁrst pretrain an environ-
ment map conditioned harmonization model (in green box)
to generate a ground truth environment map-derived light-
ing representation. The model architecture is identical to
the background-based model in Fig. 2I, while substituting
the Stage I input condition from the background image to
its corresponding environment map. It is trained with the
same light stage dataset, under a denoising loss analogous
to Eq. 1, where the background lighting feature fbgis re-
placed with an environment map-derived feature Fenv(zb).
Then, we freeze the environment-conditioned model and
introduce an alignment network Fbg→envthat calibrates the
background-extracted lighting representation to align with
its environment map equivalent. We formulate such a pro-
cess as an inverse problem that can be learned with a net-
workFbg→env, under a supervised loss. For a training tu-
ple(xa,m,yb,zb,xb), the alignment network takes fbg=
Fbg(yb)as input, and maps it with the alignment network to
f∗
bg. The environment extracted feature fenv=Fenv(zb)is
utilized as the ground truth, and we use a L1objective:
f∗=Fbg→env(Fbg(yb)),
LA=Eyb,zb/bracketleftBig/vextenddouble/vextenddoubleFenv(zb)−f∗
bg/vextenddouble/vextenddouble
1/bracketrightBig
. (2)
During this phase, we update only Fbg→env, while freez-
ing the other networks. We assume that this alignment en-
hances the background-derived lighting representation to
more accurately encode the environmental lighting, which
is empirically veriﬁed where the aligned feature maps re-
ﬂect more global illumination information (Fig. 7). Once
trained, we integrate the aligned feature extraction and con-
ditioning into Uenv, formulating our ﬁnal model in Fig. 2-III.
3.3. Finetuning for Photorealism
The light stage dataset serves as a valuable resource for
learning lighting representations, providing physically con-
strainted relighting pairs. However, it is essential to rec-
ognize that this dataset is compositional by nature. It uses
backgrounds projected from environment maps, combined
with relit foreground subjects, to create composites that
serve as ground truth for the diffusion model. However,
these composites differ from real photographs, leading to
potential concerns about the photorealism of the model’s
outputs. Moreover, due to the cost of light stage data acqui-
sition, it restricts the number and diversity of the subjects
that can be collected. The diversity of the background im-
ages is also bounded by the scale of available environment
maps during rendering (a few thousands). These limitations
6455
Model from
Stage I and IIGenerative 
Inpainting
Synthetic InputReal Image Synthetic Background Foreground Mask
Random sampled  
background / env. map
Synthetic DatasetInput Background Target
Figure 3. Data synthesis pipeline. Given a real image, and in-
paint the subject region, we obtain a synthetic background. The
foreground lighting is then altered with our model trained in Stage
I/II, to create an input image with distinct lighting. Two example
pairs are shown on the lower right.
could affect the model’s generalization ability to real im-
ages, and the capacity to produce realistic and varied light-
ing effects. Therefore, we propose a third stage (Fig. 2III)
that ﬁnetunes our ﬁnal model for improved photorealism.
We introduce a novel data synthesis pipeline that cre-
ates pairwise training pairs from natural images, to ensure
that the ground truth for ﬁnetuning the diffusion model re-
mains realimages. As depicted in Fig. 3, the process starts
with a portrait photograph, from which we extract the fore-
ground mask [ 70,71]. We then inpaint the foreground re-
gion [ 66] with text guidance ‘clear background’ to create a
clean background image for the real image, which can serve
as the condition input for the training. Next, the lighting of
the foreground subject(s) is altered by running our trained
model from stage I/II with a randomly chosen background
image or environment map as the condition. This produces
a synthetic input image with distinct foreground lighting
and color compared to the target image. Two sets of gener-
ated training tuples are displayed at bottom right of Fig. 3.
Once we obtain a sufﬁcient number of synthetic data, we
combine the original light stage dataset with the synthetic
data to reﬁne our model. During this stage, we freeze the
lighting representation extraction and conditioning branch
and only ﬁnetune the UNet backbone to reﬁne the synthesis
quality while maintaining the learned lighting plausibility.
Once trained, our ﬁnal model in Fig. 2III is used to perform
portrait harmonization given arbitrary background images,
eliminating the need for environment maps.
4. Experiments
4.1. Setup and Metrics
Three testing scenarios are created for evaluation: (1) 500
Light stage rendered test pairs to evaluate the lighting accu-
racy; (2) 200 natural image test pairs, synthetically created
from real images using our data synthesis pipeline, to assess
the lighting realism; and most importantly, (3) Real-world
portraits combined with arbitrary backgrounds, examiningthe model’s generalizability and adaptability in real-life
scenarios. For (1) and (2), we also quantify the results
with common metrics MSE, SSIM, PSNR and LPIPS [ 74].
To benchmark, we compare Relightful Harmonization with
both established harmonization methods INR [5],PCT [14],
Harmonizer [26] andPIH [61], and relighting method
TR[42]. We also construct a relighting baseline with a
transformer architecture and trained it with light stage
data. More details are provided in the appendix. Note that
relighting methods are applied only on the light stage test
set as they are not applicable without HDR maps.
4.2. Implementation Details
Our model is implemented in PyTorch [ 43] using8×80GB
A100 at512×512resolutions, with 96 batch size. In stage
I, we initialize UNet from the pretrained weights of the In-
structPix2Pix [ 3] checkpoint. In the ﬁrst and second stage,
we use in total 400 ktraining image pairs, rendered from
a arbitrary combination of 100 unique light stage subjects,
and 2908 HDR environment maps. We also randomly ro-
tate the HDR maps and use various FoVs to increase the
diversity of the background. In the third stage, we train the
network with additional 200 kpairs of images synthesized
from natural images. We set learning rate to 5e−5. More
details are provided in the appendix.
4.3. Benchmark Results
In Figs. 4aand4b, we present visual comparisons of our
method with selected benchmarks across on both test sets.
Our approach performs better in adjusting both the fore-
ground color and lighting, aligning more closely with the
ground truth on the right. Quantitative results in Table 1
further demonstrate the beneﬁt of our methods. Fig. 4c
showcases test results on natural images. While harmoniza-
tion methods adjust for color, the composition still lacks ﬁ-
delity due to counterintuitive lighting on the composed im-
age. For example, the strong cast shadow on the foreground
in the ﬁrst row, and opposite lighting directions between
foreground/background in the second and third rows. Full
visual results with all methods are provided in appendix.
We further conducted a user study to verify the visual
plausibility of our method on the real world testing set.
Given pairwise comparison between our method and each
baseline, we asked Amazon Mechanical Turk raters to se-
lect the better harmonized image from a given pair of re-
sults sample from 70 image. The results on 1750 ratings
are collected, and we report the fraction of times that raters
preferred our results over the baseline method in Table. 2.
4.4. Arbitrary Portrait Background Replacement
We further test our model on various in-the-wild portraits
by compositing them onto arbitrary natural backgrounds.
6456
Composite TR Harmonizer Ours GT
(a) Comparison on the light stage test set with [ 42] and [ 26].
Composite PIH Harmonizer Ours GT
(b) Comparison on the natural image synthetic test set with [ 26,61].
Composite PIH Harmonizer Ours
(c) Comparison with [ 61] and [ 26] on real test images. Our method more
effectively harmonizes incoherent foreground lighting and shadow.
Figure 4. Visual comparisons with benchmark methods.
Table 1. Quantitative results on both light stage test set and the natural image test set. The relighing methods including TR [ 42] and the
transformer baseline require HDR maps during the inference and are thus non-applicable on the natural image test set.
MethodLight stage test set Natural image test set
MSE↓ PSNR↑ SSIM↑ LPIPS↓ MSE↓ PSNR↑ SSIM↑ LPIPS↓
TR [42] 0.044 (0.057) 15.889 (4.318) 0.757 (0.087) 0.354 (0.092) N/A N/A N/A N/A
Transformer* 0.026 (0.021) 17.259 (3.715) 0.742 (0.096) 0.337 (0.095) N/A N/A N/A N/A
INR [ 5] 0.016 (0.014) 19.147 (3.353) 0.823 (0.081) 0.327 (0.083) 0.009 (0.005) 21.566 (2.943) 0.904 (0.038) 0.113 (0.031)
Harmonizer [ 26] 0.015 (0.011) 19.304 (2.980) 0.822 (0.077) 0.338 (0.087) 0.010 (0.007) 21.419 (3.506) 0.905 (0.039) 0.108 (0.032)
PCT [ 14] 0.020 (0.016) 18.339 (3.454) 0.808 (0.093) 0.408 (0.082) 0.014 (0.010) 19.647 (3.279) 0.898 (0.038) 0.147 (0.039)
PIH [ 61] 0.018 (0.015) 18.865 (3.087) 0.807 (0.087) 0.330 (0.089) 0.010 (0.007) 21.147 (3.097) 0.901 (0.038) 0.112 (0.033)
Ours 0.012 (0.010) 20.527 (3.136) 0.848 (0.076) 0.159 (0.058) 0.005 (0.004) 23.562 (2.830) 0.913 (0.034) 0.097 (0.044)
Table 2. User preference. Each value represents the fraction of
times that raters preferred our results than the baseline method.
Method PIH [ 61] INR [ 5] PCT [ 14] Harmonizer [ 26]
Preference Rate 0.713 0.702 0.845 0.639
Lighting Plausibility: We start by evaluating the lighting
plausibility and visual ﬁdelity by replacing the backgrounds
with strong lighting indications, like sunlight. Example re-
sults are shown in Fig. 5a. As can be seen, the lighting
tone and direction in the harmonized foreground matched
the background effectively. We further vertically ﬂip the
backgrounds on the right side, and observe a corresponding
change in the lighting effects as expected.
Shadow Plausibility: We then examine how our model
handles shadows. Fig. 5bdepicts the test cases on input sub-
jects with prominent shadows. When compositing and har-
monizing them onto background images with more ambient
lighting, our model is able to remove the strong shadow and
estimate a shadow-free output, while adapting to the back-
ground light. Conversely, in Fig. 5c, under backgrounds in
daytime scenes with potential overhead sunlight, our model
generates visually plausible self-occlusion shadows.Creative Background Replacement: Our method allows
for creative background replacement for portrait images. In
Fig. 5d(right ), we create a sequence of background crops
from one panorama image with a sliding window, so that the
major light source consistently changes from left to right.
We observe visually consistent lighting changes on the fore-
ground. Similarly, Fig. 5d(left) shows the harmonization
results by placing a subject into different timelapse video
frames at different timepoints, resulting in a series of por-
trait images that effectively mimicked a timelapse effect.
Reference-based Harmonization: Our pipeline also ex-
tends to reference-based harmonization, where users can in-
tegrate their portrait photos with scenes from a chosen ref-
erence portrait image. This is achieved by ﬁrst inpainting
the reference portrait to generate a background image, onto
which the desired foreground is composited. Our method
ensures that the ﬁnal output matches the tone and lighting
of the original scene. Fig. 5eshowcases examples where
our results effectively align with the tone and lighting of the
reference image, displayed on the top left.
6457
(a) In cases where the target backgrounds offer clear lighting cues, our method generates visually convincing lighting effects . Additionally, upon
ﬂipping the background, we note consistent and appropriate adjustments to the lighting direction in the output.
(b) Our method effectively neutralizes pronounced shadows in the input while accommodating the ambient lighting of the background.
(c) When applied to backgrounds with intense lighting conditions, e.g., with overhead sunlight, our method casts plausible self-occlusion shadows .
(d) Our method consistently adjusts lighting when applied to moving backgrounds with temporally (left) and spatially (right) changing lighting direc-
tions.
(e) Our approach allows for reference-based harmonization tasks. This involves removing the subject from the reference image ( upper left ) to create a
background ( lower left ) for composition. The harmonized results ( right ) achieve lighting effects closely resembling those in the reference.
Figure 5. Real-world testing results under different scenarios to examine the lighting and shadow effects. For each pair of results in row
(a)-(c), we display the composite image ( left) and the harmonized image ( right ). In row (d), we omit the composition for better visibility.
Full visualization is provided in the Appendix.
6458
Table 3. Ablation to verify the effects of lighting conditioning (‘Cond’), alignment (‘Align’), and ﬁnetuning (‘Finetune’).
Model Cond Align FinetuneLight stage test set Natural image test set
MSE↓ PSNR↑ SSIM↑ LPIPS↓ MSE↓ PSNR↑ SSIM↑ LPIPS↓
0 - - - 0.018 (0.015) 18.768 (3.531) 0.815 (0.085) 0.188 (0.070) 0.012 (0.008) 20.158 (3.024) 0.866 (0.045) 0.103 (0.028)
1 Bg - - 0.014 (0.012) 19.748 (3.161) 0.835 (0.084) 0.168 (0.063) 0.013 (0.008) 19.787 (2.684) 0.864 (0.042) 0.106 (0.028)
2 Env - - 0.009 (0.009) 21.626 (3.162) 0.866 (0.077) 0.148 (0.056) 0.013 (0.008) 19.824 (2.634) 0.863 (0.045) 0.105 (0.028)
3 Bg ✓ - 0.012 (0.009) 20.439 (2.987) 0.842 (0.077) 0.163 (0.059) 0.012 (0.007) 20.006 (2.586) 0.866 (0.044) 0.106 (0.028)
4 Bg ✓ ✓ 0.012 (0.010) 20.527 (3.136) 0.848 (0.076) 0.159 (0.058) 0.005 (0.004) 23.562 (2.830) 0.913 (0.035) 0.097 (0.044)
Composite Model 1 Model 2 Model 3 Model 4
Figure 6. Example testing results from our ablation. Model 1 to Model 4 correspond to the conﬁgurations in Table 3.
Bg Env ∥fbg∥2∥fenv∥2∥fbg→env∥2
Figure 7. The L2norm of learned lighting representations.
The aligned background-derived feature on the right matches the
panorama much closer, indicating a better lighting representation.
4.5. Ablation
As our proposed method involves multiple stages, we con-
ducted an ablation study to isolate individual and collec-
tive effects. We deﬁne the base Model#0 as a baseline
diffusion model without lighting conditioning. Model#1
andModel#2 incorporate lighting conditioning via back-
ground and environment map, respectively. Model#3 fur-
ther introduces the alignment module and Model#4 is our
ﬁnal model that includes the ﬁnetuning. Table 3illustrates
the quantitative performance of each model conﬁguration
on both light stage and natural image test sets. We include
more visual comparisons in the appendix.
Lighting-conditioning: Upon integrating lighting condi-
tioning into Model#0 , we observe notable improvements
across all metrics on light stage data with both background
conditioned Model#1 and environmental conditioned
Model#2 .Model#2 further outperforms Model#1 , ver-
ifying our assumption that an environmental map facilitates
a more accurate encoding of lighting.Lighting Representation Alignment: InModel#3 , we
introduce the embedding alignment. The improvements
fromModel#1 toModel#3 validated the beneﬁts of uti-
lizing an additional adaptation step to extract robust lighting
representation from a single background image. As shown
in Fig. 7, the right column representing the aligned feature
norms, shows a visual convergence towards the features ex-
tracted from the environment map.
Finetuning for Photorealism: Model#4 is a ﬁnetuned
version ofModel#3 with the newly synthesized data. We
observe signiﬁcant improvements in particular on the nat-
ural image test set, verifying our assumption that the ﬁne-
tuning boosts the photorealism on natural images. As can
be also seen in examples in Fig. 6, whileModel#2 and
Model#3 estimate plausible lighting direction, the lighting
effects are much less realistic than the ﬁnetuned Model#4 .
5. Conclusion
We present Relightful Harmonization, a novel lighting-
aware diffusion model to blend advanced lighting effects
into foreground portraits when compositing onto diverse
background images. Limitations exist including a resolu-
tion cap of 512x512, potentially affecting facial detail, es-
pecially in smaller faces. Also, subtle variations may occur
in the subject’s clothing and skin tones. Detailed analyses
and additional failure cases are included in the appendix.
6. Acknowledgement
We are grateful for Yannick Hold-Geoffroy who provides
the panorama environment maps and set up the rendering
scripts. We thank Chaowei Company for the support of
light stage dataset. We thank David Futschik for running
the testing on Total Relighting. We thank Scott Cohen for
the inspiration for our project name.
6459
References
[1] Yuval Alaluf, Daniel Garibi, Or Patashnik, Hadar Averbuch-
Elor, and Daniel Cohen-Or. Cross-image attention for zero-
shot appearance transfer. arXiv preprint arXiv:2311.03335 ,
2023. 3
[2] Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas
Struppek, Patrick Schramowski, and Kristian Kersting. Sega:
Instructing diffusion using semantic dimensions. arXiv
preprint arXiv:2301.12247 , 2023.
[3] Tim Brooks, Aleksander Holynski, and Alexei A. Efros.
Instructpix2pix: Learning to follow image editing instruc-
tions. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 18392–
18402, 2023. 2,3,4,5
[4] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xi-
aohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mu-
tual self-attention control for consistent image synthesis and
editing. arXiv preprint arXiv:2304.08465 , 2023. 3
[5] Jianqi Chen, Yilan Zhang, Zhengxia Zou, Keyan Chen, and
Zhenwei Shi. Dense pixel-to-pixel harmonization via con-
tinuous image representation. IEEE Transactions on Circuits
and Systems for Video Technology , pages 1–1, 2023. 2,5,6
[6] Jianqi Chen, Zhengxia Zou, Yilan Zhang, Keyan Chen, and
Zhenwei Shi. Zero-shot image harmonization with genera-
tive model prior, 2023. 3
[7] Wenyan Cong, Jianfu Zhang, Li Niu, Liu Liu, Zhixin Ling,
Weiyuan Li, and Liqing Zhang. Dovenet: Deep image
harmonization via domain veriﬁcation. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 8394–8403, 2020. 2
[8] Wenyan Cong, Xinhao Tao, Li Niu, Jing Liang, Xuesong
Gao, Qihao Sun, and Liqing Zhang. High-resolution im-
age harmonization via collaborative dual transformations. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 18470–18479, 2022. 2
[9] Guillaume Couairon, Jakob Verbeek, Holger Schwenk,
and Matthieu Cord. Diffedit: Diffusion-based seman-
tic image editing with mask guidance. arXiv preprint
arXiv:2210.11427 , 2022. 3
[10] Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter
Duiker, Westley Sarokin, and Mark Sagar. Acquiring the
reﬂectance ﬁeld of a human face. In Proceedings of the
27th annual conference on Computer graphics and interac-
tive techniques , pages 145–156, 2000. 2
[11] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in Neural Informa-
tion Processing Systems , 34:8780–8794, 2021. 3
[12] Yao Feng, Haiwen Feng, Michael J Black, and Timo Bolkart.
Learning an animatable detailed 3d face model from in-the-
wild images. ACM Transactions on Graphics (ToG) , 40(4):
1–13, 2021. 3
[13] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang,
Yinfei Yang, and Zhe Gan. Guiding instruction-based im-
age editing via multimodal large language models. arXiv
preprint arXiv:2309.17102 , 2023. 3
[14] Julian Jorge Andrade Guerreiro, Mitsuru Nakazawa, and
Bj¨orn Stenger. Pct-net: Full resolution image harmonizationusing pixel-wise color transformations. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 5917–5926, 2023. 2,5,6
[15] Zonghui Guo, Dongsheng Guo, Haiyong Zheng, Zhaorui
Gu, Bing Zheng, and Junyu Dong. Image harmonization
with transformer. In Proceedings of the IEEE/CVF interna-
tional conference on computer vision , pages 14870–14879,
2021.
[16] Zonghui Guo, Haiyong Zheng, Yufeng Jiang, Zhaorui Gu,
and Bing Zheng. Intrinsic image harmonization. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 16367–16376, 2021. 2
[17] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kﬁr Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
age editing with cross attention control. arXiv preprint
arXiv:2208.01626 , 2022. 3
[18] Jonathan Ho and Tim Salimans. Classiﬁer-free diffusion
guidance. In NeurIPS 2021 Workshop on Deep Generative
Models and Downstream Applications , 2021. 3
[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems , 33:6840–6851, 2020. 3
[20] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William
Chan, Mohammad Norouzi, and David J Fleet. Video dif-
fusion models. arXiv:2204.03458 , 2022. 3
[21] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer
Michaeli. An edit friendly ddpm noise space: Inversion and
manipulations. arXiv preprint arXiv:2304.06140 , 2023. 3
[22] Yifan Jiang, He Zhang, Jianming Zhang, Yilin Wang, Zhe
Lin, Kalyan Sunkavalli, Simon Chen, Sohrab Amirghodsi,
Sarah Kong, and Zhangyang Wang. Ssh: A self-supervised
framework for image harmonization. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 4832–4841, 2021. 2
[23] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming
Song. Denoising diffusion restoration models. In Advances
in Neural Information Processing Systems , 2022. 3
[24] Bahjat Kawar, Roy Ganz, and Michael Elad. Enhancing
diffusion-based image synthesis with robust classiﬁer guid-
ance. arXiv preprint arXiv:2208.08664 , 2022. 3
[25] Bahjat Kawar, Jiaming Song, Stefano Ermon, and Michael
Elad. Jpeg artifact correction using denoising diffusion
restoration models. arXiv preprint arXiv:2209.11888 , 2022.
3
[26] Zhanghan Ke, Chunyi Sun, Lei Zhu, Ke Xu, and Ryn-
son W.H. Lau. Harmonizer: Learning to perform white-box
image and video harmonization. In European Conference on
Computer Vision (ECCV) , 2022. 2,5,6
[27] Gihyun Kwon and Jong Chul Ye. Diffusion-based image
translation using disentangled style and content representa-
tion. arXiv preprint arXiv:2209.15264 , 2022. 3
[28] Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun
Feng, Zhihai Xu, Qi Li, and Yueting Chen. Srdiff: Single
image super-resolution with diffusion probabilistic models.
Neurocomputing , 479:47–59, 2022. 3
[29] Jingtang Liang, Xiaodong Cun, and Chi-Man Pun.
Spatial-separated curve rendering network for efﬁcient
6460
and high-resolution image harmonization. arXiv preprint
arXiv:2109.05750 , 2021. 2
[30] Lingxiao Lu, Jiangtong Li, Junyan Cao, Li Niu, and Liqing
Zhang. Painterly image harmonization using diffusion
model. In Proceedings of the 31st ACM International Con-
ference on Multimedia , pages 233–241, 2023. 2
[31] Lingxiao Lu, Jiangtong Li, Junyan Cao, Li Niu, and Liqing
Zhang. Painterly image harmonization using diffusion
model. In Proceedings of the 31st ACM International Con-
ference on Multimedia . ACM, 2023. 3
[32] Shilin Lu, Yanzhu Liu, and Adams Wai-Kin Kong. Tf-icon:
Diffusion-based training-free cross-domain image composi-
tion. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision , pages 2294–2305, 2023. 3
[33] Yiqun Mei, He Zhang, Xuaner Zhang, Jianming Zhang,
Zhixin Shu, Yilin Wang, Zijun Wei, Shi Yan, HyunJoon
Jung, and Vishal M. Patel. Lightpainter: Interactive por-
trait relighting with freehand scribble. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 195–205, 2023. 2
[34] Yiqun Mei, Yu Zeng, He Zhang, Zhixin Shu, Xuaner Zhang,
Sai Bi, Jianming Zhang, HyunJoon Jung, and Vishal M Patel.
Holo-relighting: Controllable volumetric portrait relighting
from a single image. arXiv preprint arXiv:2403.09632 , 2024.
2
[35] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-
Yan Zhu, and Stefano Ermon. Sdedit: Image synthesis and
editing with stochastic differential equations. arXiv preprint
arXiv:2108.01073 , 2021. 3
[36] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki
Tanaka. Negative-prompt inversion: Fast image inversion
for editing with text-guided diffusion models. arXiv preprint
arXiv:2305.16807 , 2023.
[37] Ron Mokady, Amir Hertz, Kﬁr Aberman, Yael Pritch,
and Daniel Cohen-Or. Null-text inversion for editing real
images using guided diffusion models. arXiv preprint
arXiv:2211.09794 , 2022. 3
[38] Thomas Nestmeyer, Jean-Franc ¸ois Lalonde, Iain Matthews,
and Andreas Lehrmann. Learning physics-guided face re-
lighting under directional light. In CVPR , pages 5124–5133,
2020. 2
[39] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741 , 2021. 3
[40] Shen Nie, Hanzhong Allan Guo, Cheng Lu, Yuhao Zhou,
Chenyu Zheng, and Chongxuan Li. The blessing of random-
ness: Sde beats ode in general diffusion-based image editing.
arXiv preprint arXiv:2311.01410 , 2023. 3
[41] Li Niu, Junyan Cao, Wenyan Cong, and Liqing Zhang.
Deep image harmonization with learnable augmentation. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 7482–7491, 2023. 2
[42] Rohit Pandey, Sergio Orts Escolano, Chloe Legendre, Chris-
tian Haene, Soﬁen Bouaziz, Christoph Rhemann, Paul De-
bevec, and Sean Fanello. Total relighting: learning to relightportraits for background replacement. ACM Transactions on
Graphics (TOG) , 40(4):1–21, 2021. 2,3,4,5,6
[43] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-
perative style, high-performance deep learning library. Ad-
vances in neural information processing systems , 32, 2019.
5
[44] Puntawat Ponglertnapakorn, Nontawat Tritrong, and Supa-
sorn Suwajanakorn. Difareli: Diffusion face relighting.
2023. 2
[45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022. 3
[46] Mengwei Ren, Mauricio Delbracio, Hossein Talebi, Guido
Gerig, and Peyman Milanfar. Multiscale structure guided
diffusion for image deblurring. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 10721–10733, 2023. 3
[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 10684–10695, 2022. 3
[48] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,
Jonathan Ho, Tim Salimans, David Fleet, and Mohammad
Norouzi. Palette: Image-to-image diffusion models. In
ACM SIGGRAPH 2022 Conference Proceedings , pages 1–
10, 2022. 3
[49] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi,
Rapha Gontijo Lopes, et al. Photorealistic text-to-image
diffusion models with deep language understanding. arXiv
preprint arXiv:2205.11487 , 2022. 3
[50] Hiroshi Sasaki, Chris G Willcocks, and Toby P Breckon.
Unit-ddpm: Unpaired image translation with denois-
ing diffusion probabilistic models. arXiv preprint
arXiv:2104.05358 , 2021. 3
[51] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International Confer-
ence on Machine Learning , pages 2256–2265. PMLR, 2015.
3
[52] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In International Conference
on Learning Representations , 2021. 3
[53] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price,
Jianming Zhang, Soo Ye Kim, and Daniel Aliaga. Object-
stitch: Object compositing with diffusion model. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 18310–18319, 2023. 3
[54] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price,
Jianming Zhang, Soo Ye Kim, He Zhang, Wei Xiong, and
Daniel Aliaga. Imprint: Generative object compositing by
learning identity-preserving representation. arXiv preprint
arXiv:2403.10701 , 2024. 3
6461
[55] Tiancheng Sun, Jonathan T Barron, Yun-Ta Tsai, Zexiang
Xu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay
Busch, Paul Debevec, and Ravi Ramamoorthi. Single image
portrait relighting. ACM Transactions on Graphics (TOG) ,
38(4):1–12, 2019. 2,4
[56] Linfeng Tan, Jiangtong Li, Li Niu, and Liqing Zhang. Deep
image harmonization in dual color spaces. In Proceedings
of the 31st ACM International Conference on Multimedia ,
pages 2159–2167, 2023. 2
[57] Yi-Hsuan Tsai, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli,
Xin Lu, and Ming-Hsuan Yang. Deep image harmonization.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 3789–3797, 2017. 2
[58] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali
Dekel. Plug-and-play diffusion features for text-driven
image-to-image translation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 1921–1930, 2023. 3
[59] Jeya Maria Jose Valanarasu, He Zhang, Jianming Zhang,
Yilin Wang, Zhe Lin, Jose Echevarria, Yinglan Ma, Zijun
Wei, Kalyan Sunkavalli, and Vishal M Patel. Interactive por-
trait harmonization. arXiv preprint arXiv:2203.08216 , 2022.
1
[60] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Ex-
act diffusion inversion via coupled transformations. arXiv
preprint arXiv:2211.12446 , 2022. 3
[61] Ke Wang, Micha ¨el Gharbi, He Zhang, Zhihao Xia, and Eli
Shechtman. Semi-supervised parametric real-world image
harmonization. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 5927–
5936, 2023. 2,5,6
[62] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong
Chen, Qifeng Chen, and Fang Wen. Pretraining is all
you need for image-to-image translation. arXiv preprint
arXiv:2205.12952 , 2022. 3
[63] Yijiang Wang, Yuqi Li, Chong Wang, and Xulun Ye. Harmo-
nized portrait-background image composition. In Computer
Graphics Forum , page e14921. Wiley Online Library, 2023.
1
[64] Zhibo Wang, Xin Yu, Ming Lu, Quan Wang, Chen Qian, and
Feng Xu. Single image portrait relighting via explicit mul-
tiple reﬂectance channel modeling. ACM TOG , 39(6):1–13,
2020. 2
[65] Chen Henry Wu and Fernando De la Torre. A latent space
of stochastic diffusion models for zero-shot image editing
and guidance. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 7378–7387, 2023. 3
[66] Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun
Zhang. Smartbrush: Text and shape guided object inpainting
with diffusion model. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
22428–22437, 2023. 5
[67] Yazhou Xing, Yu Li, Xintao Wang, Ye Zhu, and Qifeng
Chen. Composite photograph harmonization with complete
background cues. In Proceedings of the 30th ACM interna-
tional conference on multimedia , pages 2296–2304, 2022. 2
[68] Su Xue, Aseem Agarwala, Julie Dorsey, and Holly Rush-
meier. Understanding and improving the realism of imagecomposites. ACM Transactions on graphics (TOG) , 31(4):
1–10, 2012. 2
[69] Yu-Ying Yeh, Koki Nagano, Sameh Khamis, Jan Kautz,
Ming-Yu Liu, and Ting-Chun Wang. Learning to relight
portrait images via a virtual light stage and synthetic-to-real
adaptation. ACM TOG , 2022. 2
[70] Qihang Yu, Jianming Zhang, He Zhang, Yilin Wang, Zhe
Lin, Ning Xu, Yutong Bai, and Alan Yuille. Mask guided
matting via progressive reﬁnement network. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , pages 1154–1163, 2021. 4,5
[71] He Zhang, Jianming Zhang, Federico Perazzi, Zhe Lin, and
Vishal M. Patel. Deep image compositing. In Proceedings of
the IEEE/CVF Winter Conference on Applications of Com-
puter Vision (WACV) , pages 365–374, 2021. 4,5
[72] Longwen Zhang, Qixuan Zhang, Minye Wu, Jingyi Yu, and
Lan Xu. Neural video portrait relighting in real-time via con-
sistency modeling. In ICCV , pages 802–812, 2021. 2
[73] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3836–3847, 2023. 3,4
[74] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , 2018. 5
[75] Xuaner Zhang, Jonathan T Barron, Yun-Ta Tsai, Rohit
Pandey, Xiuming Zhang, Ren Ng, and David E Jacobs. Por-
trait shadow manipulation. ACM Transactions on Graphics
(TOG) , 39(4):78–1, 2020. 4
[76] Xiuming Zhang, Sean Fanello, Yun-Ta Tsai, Tiancheng Sun,
Tianfan Xue, Rohit Pandey, Sergio Orts-Escolano, Philip
Davidson, Christoph Rhemann, Paul Debevec, et al. Neu-
ral light transport for relighting and view synthesis. ACM
TOG , 40(1):1–17, 2021. 2
[77] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris N
Metaxas, and Jian Ren. Sine: Single image editing with text-
to-image diffusion models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 6027–6037, 2023. 3
[78] Min Zhao, Fan Bao, Chongxuan Li, and Jun Zhu. Egsde:
Unpaired image-to-image translation via energy-guided
stochastic differential equations. Advances in Neural Infor-
mation Processing Systems , 35:3609–3623, 2022. 3
[79] Hao Zhou, Sunil Hadap, Kalyan Sunkavalli, and David W Ja-
cobs. Deep single-image portrait relighting. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 7194–7202, 2019. 2
[80] Jun-Yan Zhu, Philipp Krahenbuhl, Eli Shechtman, and
Alexei A Efros. Learning a discriminative model for the
perception of realism in composite images. In Proceedings
of the IEEE International Conference on Computer Vision ,
pages 3943–3951, 2015. 2
6462
