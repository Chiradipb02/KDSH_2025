AnyScene: Customized Image Synthesis with Composited Foreground
Ruidong Chen1†Lanjun Wang1†Weizhi Nie1Yongdong Zhang2An-An Liu1∗
1Tianjin University2University of Science and Technology of China
Abstract
Recent advancements in text-to-image technology have
significantly advanced the field of image customization.
Among various applications, the task of customizing diverse
scenes for user-specified composited elements holds great
application value but has not been extensively explored.
Addressing this gap, we propose AnyScene , a specialized
framework designed to create varied scenes from compos-
ited foreground using textual prompts. AnyScene addresses
the primary challenges inherent in existing methods, partic-
ularly scene disharmony due to a lack of foreground seman-
tic understanding and distortion of foreground elements.
Specifically, we develop a foreground injection module that
guides a pre-trained diffusion model to generate cohesive
scenes in visual harmony with the provided foreground. To
enhance robust generation, we implement a layout control
strategy that prevents distortions of foreground elements.
Furthermore, an efficient image blending mechanism seam-
lessly reintegrates foreground details into the generated
scenes, producing outputs with overall visual harmony and
precise foreground details. In addition, we propose a new
benchmark and a series of quantitative metrics to evaluate
this proposed image customization task. Extensive exper-
imental results demonstrate the effectiveness of AnyScene,
which confirms its potential in various applications.
1. Introduction
Recently, text-to-image (T2I) synthesis models [24, 28, 30,
34] have seen rapid advancements and gained significant
attention due to their ability to generate high-fidelity im-
ages from textual prompts. Among the various applications
that harness T2I technology to enhance design efforts, gen-
erating diverse scenes tailored to specific composited fore-
grounds is valuable. It is effective in image editing tasks
demanding background alteration and creation, such as sit-
uating e-commerce products in customized scenes, depict-
ing objects in various environmental settings, etc. Despite
its clear utility, this task of “customizing diverse scenes for
1†Equal contribution.
2∗Corresponding author: An-An Liu (anan0422@gmail.com).
Figure 1. Our proposed AnyScene is capable of synthesizing high-
quality scenes that align with textual prompts based on the given
foregrounds. Compared to previous alternative methods [42, 45],
AnyScene provides precise control over the introduced foreground
and generates visually harmonious images.
composited foreground” has not been extensively explored.
Currently, several alternative methods [42, 45] can
achieve this task. One such method is employing the canny
edge as a control condition [23, 45], combined with over-
laying the original foreground onto the generated image,
often requiring manual adjustments to achieve visual har-
mony. Another strategy involves inpainting models [30, 42]
that construct backgrounds by painting around the exterior
of a masked foreground area. Furthermore, in cases where
the foreground comprises only one or two specific objects,
subject-driven techniques [17, 19, 32] have shown profi-
ciency in generating customized images.
However, these methods have limitations in practical ap-
plications. The method of directly controlling the fore-
ground edge [45] does not consider the visual context, lead-
ing to scenes that lack harmony (e.g. Fig. 2(a)). More-
over, inpainting-based approaches [30, 42], though capa-
ble of creating cohesive scenes, are commonly trained un-
der a random area recovering paradigm, which may distort
the foreground elements or cause unintended regeneration
of elements from foreground descriptions (e.g. Fig. 2(b)).
Finally, subject-driven methods [17, 19, 32] often compro-
mise high-level details of objects (e.g. Fig. 2(c)), leading
to significant distortions of details such as logos, text, etc,
which is problematic for the applications like commodity
poster design. Meanwhile, their limited capacity to com-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
8724
Figure 2. The visualized examples of challenges faced by current
alternatives [17, 42, 45] with the proposed task.
pose multiple objects into a complex scene also restricts
their applicability.
To address these limitations, we present AnyScene , a
customized image synthesizing framework tailored explic-
itly for “customizing diverse scenes for composited fore-
ground” . Firstly, facing the challenge of generating a visu-
ally harmonious scene based on the specific foreground and
text prompt, we establish a training paradigm that recon-
structs the overall scene considering the given foreground
elements and propose the foreground injection module to
guide pre-trained latent diffusion models in producing the
images of overall scenes that cohesively incorporate the
specific foreground. Secondly, to minimize severe distor-
tion of the foreground and prevent redundant generation of
foreground elements during synthesis, we implement the
layout control strategy, which ensures that foreground in-
formation remains confined to its designated areas during
model inference. Finally, acknowledging that generated im-
ages often fail to preserve intricate visual details of specific
objects due to perceptual compression [9] inherent in latent
diffusion models [30], it is necessary to recover the original
details of the foreground onto the generated images. Unlike
alternative methods [42, 45] that typically directly overlay
the foreground on the generated image, we employ an im-
age blending mechanism to achieve a visually harmonious
integration, considering the lighting and color conditions of
the generated images.
We conduct extensive experiments to evaluate the gen-
erative capability of AnyScene on the proposed task. We
propose a benchmark and a series of evaluation metrics
to assess the generative capacity of methods on this task.
Additionally, we perform a detailed visualization analysis
to compare the results produced by different approaches.
These experiments conclusively demonstrate the effective-
ness of AnyScene in synthesizing high-quality customized
images.
2. Related Work
2.1. Text-to-Image Synthesis
Text-to-image (T2I) synthesis techniques aim to gener-
ate images from given textual descriptions. Early re-
search [29, 40, 43, 44] employed Generative AdversarialNetworks (GANs) [11] on text-image datasets. These initial
methods suffered from training instability[1, 37] and a lack
of high-quality datasets, which posed challenges in produc-
ing high-fidelity images. Subsequently, models based on
autoregressive architectures [7, 8, 10, 27, 38, 41], trained
on expansive datasets [35, 36], have made notable strides
in generation quality, significantly advancing T2I synthesis
research with their enhanced generative capabilities.
Recently, the study of denoising diffusion probabilistic
models (DDPMs) [6, 14, 15] has significantly advanced
the field of T2I technology. DDPMs utilize a process that
progressively adds random noise to the original image and
then iteratively removes this noise during inference to pro-
duce various high-quality images. Pioneering works such
as [24, 30, 33, 34] have built powerful T2I models based on
DDPMs, achieving remarkable improvements in the fidelity
and clarity of the synthesized images. This progress has
led to a range of innovations in image customization works,
leveraging the capabilities of pre-trained diffusion models
for practical applications, such as subject-driven genera-
tion [17, 19, 32, 39] and text-driven image editing [2, 12].
2.2. Controling Text-to-Image Diffusion Models
With the rapid advancement of Text-to-Image (T2I) diffu-
sion models, recent studies have significantly advanced in
precisely controlling the generative content of these models.
In order to incorporate specific visual references into gener-
ative models, works such as [19, 21, 39] utilize CLIP-based
visual adapters to introduce visual concepts from reference
images into the image generation process. For layout con-
trol, some studies [23, 45] introduce additional conditions
like sketches, depth maps, or canny edges to pre-trained
diffusion models, constraining models to generate images
conforming to these layouts. Additionally, attention control
methods [5, 12] have achieved image editing or layout guid-
ing capabilities without the need for any training by editing
cross-attention maps between text tokens and intermediate
visual features during the model’s inference process.
3. Preliminary
We adopt the Stable Diffusion model [30] for its proven
stability and efficacy in text-to-image synthesis tasks. Our
method aims to utilize the powerful generative capabili-
ties of the pre-trained diffusion model to customize diverse
scenes based on the given foregrounds guided by textual
prompts. Before introducing our method, it is essential first
to discuss the mechanism of Stable Diffusion, which forms
the preliminary basis of our method.
The Stable Diffusion is based on the latent diffusion
model [30]. It operates diffusion and denoise processes
in an autoencoder’s latent space. Formally, it uses a pre-
trained encoder Eto compress images xinto smaller “latent
images” zfor stabilized training. Then, the model features
8725
Figure 3. The overall framework of our proposed AnyScene, which is designed to synthesize diverse scenes with user-specified foreground
elements. a) AnyScene begins with gaining a composited foreground and text prompts for the target scene. b) Then, the Foreground
Inject Module(Sec. 4.1), alongside a Layout Control Strategy(Sec. 4.2), guides a pre-trained diffusion model to generate the scene
contextually. c) Finally, the generated scene undergoes a Foreground Blending(Sec. 4.3) process to recover the foreground details, resulting
in a harmoniously synthesized final image.
a denoising U-Net [31] Eto predict the reverse diffusion
process in the latent space. A key aspect of the model is
its conditioning on textual inputs through a CLIP [26] text
encoder τθthat translates the text condition yinto an inter-
mediate form τθ(y), which is integrated into the model via
a cross-attention mechanism in the U-Net layers:
Attn( Q, K, V ) = SoftmaxQKT
√
d
V, (1)
where Q=WQ·φ(zt), K=WK·τθ(y), V=WV·τθ(y),
andφ(zt)represents the hidden states in U-Net with ztas
the diffused latent representation at time t. The training ob-
jective of the latent diffusion model is to predict the noise ϵ
added to the latent image representation, as formulated in:
Ldenoising =Ez,t,y,ϵ∼N(0,1)h
∥ϵ−ϵθ(zt, t, τθ(y))∥2
2i
,(2)
where ϵis the added noise for the diffusion process, ϵθindi-
cates the U-Net’s parameters which are used to predict this
noise conditioned with text y.
During inference, the model starts with a random Gaus-
sian noise sample zTand iteratively denoises it over Tsteps
to arrive at the estimated initial code ˆz0. Finally, the image
decoder D(·)is applied to reconstruct this latent code ˆz0
into the final image ˆxwith high fidelity.
4. Methodology
This study focuses on the task of “ customizing diverse
scenes for composited foreground ”. As illustrated in Fig. 3
(a), this task has two inputs: 1) the specific foreground ele-
ments onto the blank canvas denoted cf; 2) a text prompt ofthe target scene, denoted y. Given these inputs, the goal of
our proposed task consists of the following requirements.
• Generate scenes that are visually cohesive with the pro-
vided foreground and textual description;
• Prevent severe distortion and redundant generation of
foreground elements during scene creation;
• Preserve the precise details of the given foreground and
blend it into the generated image naturally.
Based on these requirements, we introduce AnyScene, as
illustrated in Fig. 3. We propose a comprehensive frame-
work based on three modules designed to address each of
the points above, ultimately synthesizing images that align
with user input and uphold overall visual harmony and the
accuracy of the original foreground details.
4.1. Foreground Injection Module
We propose the Foreground Injection Module Fcbased on
the ControlNet [45] architecture to introduce the foreground
information into the pre-trained diffusion model. Fcem-
ploys the same encoder blocks as the diffusion U-Net to
extract detailed information from cfand learns the correla-
tion between the input text yand the foreground elements.
By adding the learned intermediate features into the diffu-
sion model’s latent space, Fcguides the frozen diffusion
models to generate a cohesive overall scene with the input
foreground and text prompt.
To facilitate the training of the Foreground Injection
Module Fc, we construct the training data, utilizing the
foreground image cfwith its target scene description yto
reconstruct the original image xand learn the module’s
foreground integration capabilities. During training, we ap-
ply random color enhancements to the input foregrounds,
8726
encouraging Fcto guide the diffusion model to generate vi-
sually harmonious scenes, even with variable foreground
colors and lighting conditions. To ensure effective fore-
ground integration, we apply the following three parts of
training objectives for Fc.
Denoising Loss. We adopt the standard denoising loss from
latent diffusion models [15, 30]. During the denoising train-
ing phase of latent diffusion models, the denoising loss aims
to reconstruct the initial latent code z0from the diffused
codeztin timestep tby predicting the added noise. In our
cases, the denoising loss can be formulated as:
Ldenoising =Ez0,t,y,c f,ϵ∼N (0,1)
∥ϵ−ϵθ(zt, t, τθ(y), Fc(cf))∥2
2
,
(3)
where the ϵis the random sampling noise to simulate the
diffusion process, we leverage the U-Net’s parameters ϵθto
predict this added noise in conditions with yandcf.
Content Loss. We apply a pixel-level foreground content
loss to enhance the color and texture learning with the given
foreground elements. At timestep t, we reverse the diffused
latent ztto their estimated initial state ˆzt, which is formu-
lated as: ˆzt= 
zt−√1−¯αtϵθ(zt, t, y, c f)
/√¯αt, where
¯αt=Qt
s=1αs, αs= 1−βsandβsrepresents forward
process variances [15]. From this, the reconstructed esti-
mated image is denoted as ˆxt=D(ˆzt), where D(·)is the
image decoder as defined in Sec. 3. The Content Loss is
then applied using the foreground binary mask Mto align
the reconstructed and original images within the foreground
region:
Lcontent =∥x0−ˆxt∥2
2⊙M. (4)
With this training objective, Fccan effectively learn the
color and texture information of the given foreground. Par-
ticularly, incorporating the color enhancement applied on
the foreground elements during training, the introduction of
this loss helps adjust the foreground colors according to the
overall generated scene in response to various lighting and
color conditions, thereby improving the visual harmony of
the generated scene.
Edge Gradient Loss. In conventional image processing,
identifying foreground boundaries often relies on detect-
ing changes in color gradients within the image. These
changes are typically pronounced at the edges of foreground
elements due to variations in lighting, color, and depth of
field. Inspired by this, we propose an edge gradient loss,
which encourages Fcto generate soft and realistic fore-
ground boundaries by learning the gradient changes of fore-
ground boundaries in real images. To this end, we generate
an edge mask Medge=dilate (M)−erode (M), which is
obtained by subtracting the eroded mask from the dilated
mask to capture the morphological edges of the foreground.
Subsequently, we utilize the Laplacian operator to calcu-
late the color transition gradient in the RGB channels of the
original image x0and the estimated image ˆxt. The formulafor aligning gradients at the edges between the two images
is as follows:
Lgradient =∥∇x0− ∇ˆxt∥2
2⊙Medge, (5)
where ∇denotes the Laplacian operator. Incorporating this
loss enables Fcto refine image generation from two as-
pects: 1) clarify the edges around the foreground elements;
2) align edge gradients to preserve the foreground shape,
which could minimize distortions in the generated images.
Training Strategies. Building upon the previously intro-
duced concepts, the Foreground Injection Module Fcis
trained using the foundational denoising loss in conjunction
with the newly proposed losses. Recognizing that the model
cannot reconstruct the original image in the early stages
of denoising, we adjust our training strategy accordingly.
Specifically, we apply Lgradient exclusively at timesteps
t < αT , where Tdenotes the total number of diffusion
timesteps and α∈[0,1]is an adjustment parameter, de-
faulting to 0.25. Consequently, the overall training objec-
tive of the network is formulated as follows:
Ltotal=(
Ldenoising +Lcontent +Lgradient ,ift < αT
Ldenoising +Lcontent , otherwise.
(6)
A more detailed training strategy is discussed in the experi-
mental section(Sec 5.1).
4.2. Foreground Layout Control
Through training the Fcin the proposed paradigm, it can
achieve precise foreground injection into the generation
process. However, due to the lack of semantic understand-
ing of the foreground introduced in the frozen diffusion
model, challenges such as distortion and redundant genera-
tion of the foreground elements may still arise. To address
this challenge, we propose a foreground layout control strat-
egy applied during the iterative denoising steps of model
inference to avoid such errors.
As discussed in previous works [5, 12, 39], in cross-
attention based text-to-image diffusion models, the cross-
attention map between the word tokens and latent spatial
patches is calculated over the visual features ( Q) and word
embeddings ( K), as referenced in Eq.(1). In timestep t,
for a text sequence of length N, the cross-attention map
can be denoted as At∈Rp×p×N, with pindicating the
resolution of the map. Specifically, At[i, j, n ]denotes the
probability that the n-th token conveys information to the
(i, j)-th spatial patch of the intermediate feature map, re-
flecting the extent of influence each word has on the image
patch. Therefore, observing cross-attention maps in incor-
rectly generated examples is crucial to addressing the issue
of generation errors with foreground elements.
As illustrated in Fig. 4, during the default sampling pro-
cess, it is observed that although the foreground informa-
tion has been injected into the diffusion model, the attention
8727
Figure 4. In the failure cases, the semantic information of “backpack” extends beyond the designated foreground region, resulting in the
distortion of foreground elements and their redundant generation within the synthesized image. Our proposed foreground layout control
strategy capably avoids such failures by effectively confining the foreground semantics to the appropriate region.
maps of words associated with foreground elements still
probably extend beyond their intended region. To deal with
this issue, we apply the foreground mask Mto these expan-
sive attention maps. Specifically, for words designated by
the user as related to foreground elements, we identify their
positions in the input prompt as K={k1, k2, . . . k n}. The
cross-attention editing strategy can then be written as:
A∗
tk=At[:,:, k]⊙(M↓p), k∈K (7)
where ↓indicates the down-sampling operation on the mask
Mto the resolution p∈ {64,32,16}, matching the resolu-
tion of Atat different network levels. This control strategy
ensures that the semantic information of the foreground el-
ements remains confined within the masked area, as shown
in Fig. 4, thus facilitating controllable preservation of the
foreground layout. Moreover, since this strategy operates
during the sampling steps, it is independent of the model’s
training phase. This allows for its direct application to mod-
els already trained, providing an adaptable control mecha-
nism for the practical application of our method.
4.3. Foreground Blending
To reintegrate the original foreground details into the gen-
erated scenes, current alternatives merely paste the fore-
ground over the mask M, which may lead to issues with
border harmony and disregard the scene’s overall color tone.
Addressing these limitations, a viable solution is the appli-
cation of image blending techniques like Poisson Blend-
ing [25], which can smoothen intensity transitions in the
pasting area, thus reducing artifacts. Poisson Blending
achieves harmonized blending by formulating a guidance
vector field as:
min
fZZ
Ω|∇f− ∇g|2with f|∂Ω=f∗|∂Ω. (8)
where ∇represents a gradient operator. The main purpose
is to calculate the final blending image funder the condi-
tion that the foreground boundary ∂Ωof generated scene f∗remains unchanged while ensuring the gradient of fin the
foreground region Ωclosely approximates the gradient of
foreground g. Utilizing an eroded mask Meroded , we rein-
tegrate the intricate inner details of the foreground into the
generated image. Finally, it outputs the synthesized image
with overall visual harmony and precise details of the fore-
ground elements.
5. Experiment
5.1. Implementations Details
Training Dataset. We construct a training dataset com-
prising {foreground image, scene image, text prompt }data
pairs for training the foreground injection module. The
training images are sourced from FFHQ [16, 39], COCO [4,
22], and OpenImage [18] datasets. We utilize the segmenta-
tion annotations in these datasets to extract foreground ob-
jects from images, which were then recombined to form
composited foregrounds. Additionally, we employ BLIP-
2 [20] to generate scene descriptions for data lacking textual
annotations. We collect 240K training pairs from various
categories to train our network.
Data Augmentation. Throughout our training process,
we apply random data augmentations to the foreground
elements to enhance the ability of the Foreground Injec-
tion Module Fcto generate visually harmonious scenes
under various lighting, color, and clarity conditions. Us-
ing the Albumentations library [3], we introduce random
color(RGBshift), contrast, and gamma adjustments, creat-
ing diverse foreground augmentations.
Framework Training. We develop our framework us-
ing Stable Diffusion v1.5 [30] as the base model, with all
model parameters frozen. Particularly, we start training
our network from the weights of community ControlNet-
Inpaint [45] checkpoint. Although it cannot be directly ap-
plied to the proposed scene generation task (it fails to in-
paint large blank areas like the entire background), its scene
completion capability trained on random masks can accel-
8728
Figure 5. Qualitative comparisons on the proposed benchmark, which shows the generation qualities conditioned with the foreground from
the real scene. To demonstrate the scene generation capabilities of the generative model without introducing a foreground, we also display
and compare the results generated by default versions of Stable Diffusion.
erate the training efficiency of our proposed framework.
Training was conducted on two Nvidia A800 GPUs, with
a batch size of 16, at an image resolution of 512×512and
a learning rate of 1×10−5. Employing the AdamW opti-
mizer, the training was completed over 40 hours, finishing
100K training steps.
5.2. Evaluation Settings
To comprehensively evaluate the efficacy of the AnyScene
on the task of “customizing diverse scenes for composited
foreground” , we devise an evaluation benchmark. We use
the same method as for constructing the training data to
form 3,108 pairs of composited foreground as well as tar-
get scene descriptions on COCO [22], which is specifically
curated to evaluate the ability of models to synthesize target
scenes from given foregrounds and scene descriptions.
We employ three classic metrics to evaluate the quanti-
tative quality of the synthesized images, including FID [13]
(Fr´echet Inception Distance) for visual quality, CLIP-
Score [26] for text-visual consistency, and LAION Aes-
thetic [36] for the aesthetic quality of generated images.
5.3. Comparison Method
To comprehensively evaluate the generation quality and ap-
plications with our approach, comparative analyses were
conducted against three distinct types of generative models:Alternatives for the Proposed Task. Specifically, it in-
cludes two methods. 1) Canny Edge: the Canny Edge ver-
sion of ControlNet [45]. We extract the canny edges of the
input foreground and use them to guide the overall scene
generation. 2) Inpainting Anything [30, 42]: a method that
employs an inpainting version of Stable Diffusion to replace
the background with a given foreground. We treat these
methods as the primary comparative method to compare the
generative capabilities through quantitative evaluation and
visualization of results.
Reference-based Subject-Driven Method. Methods in
this category use a single image as a visual reference to
synthesize new scenes with text prompts. We employ the
zero-shot setting of BLIP-Diffusion [19] for comparison to
evaluate how our method compares to this technique in the
proposed task.
Finetuning-based Subject-Driven Method. This ap-
proach entails finetuning the model with multiple images of
the same subject to learn visual concepts and generate new
scenes. We employed DreamBooth [32] and CustomDif-
fusion [17], finetuning each with 4 to 10 images for ev-
ery foreground element, composing them to generate cus-
tomized images.
Since the subject-driven methods can customize scenes
for specific objects, we compare the image quality gener-
ated by these methods with our approach in the image qual-
8729
Figure 6. Qualitative comparisons by simulating the user’s combination of specific objects to form the foreground, where the words marked
blue in the input text are user-specified foreground-related words, and we use this information to control the foreground layout using the
proposed foreground control strategy.
Table 1. The quantitative evaluation results with the proposed
benchmark.
FID (↓) CLIP-S ( ↑) Aesthetic ( ↑)
Stable Diffusion [30] 26.82 14.19 6.02
Canny Edge [45] 24.07 14.64 5.56
Inpaint Anything [42] 16.52 14.72 5.83
AnyScene (ours) 16.09 15.18 5.94
ity analysis with specific foreground objects and discuss
their differences in applications.
5.4. Comparison Experiments
5.4.1 Quantitative Comparisons
The quantitative comparative results are detailed in Table.1.
As a baseline, we include the performance of default Sta-
ble Diffusion, which generates images solely from text
prompts. The results in the table indicate that AnyScene
surpasses the alternative methods across both metrics. Re-
garding the aesthetic metric of the generated images, our re-
sults more closely align with the aesthetic evaluation result
of the default Stable Diffusion, demonstrating our method’s
effective utilization of the pre-trained model’s capabilities
to synthesize high-quality images based on the given fore-
ground.5.4.2 Qualitative Comparisons
Fig. 5 showcases the generated images in our proposed
benchmark, which evaluates the model’s ability to cus-
tomize images for real scenes. Meanwhile, in Fig. 6, we
simulate user actions of compositing elements to obtain
foregrounds and incorporate the foreground control strategy
for generation.
Ours vs. alternative methods. As shown in the visual-
ization results. ControlNet based on Canny Edge can only
correctly generate scenes when the foreground elements
are clear and only a single element. Inpainting Anything
can produce images with roughly reasonable semantics but
faces issues like foreground distortion, especially when the
foreground elements are complex, leading to a significant
decline in visual harmony. In contrast, our proposed method
exhibits a more precise understanding of foreground seman-
tics, thereby achieving superior visual effects and synthesiz-
ing scenes that are both accurate and of higher image qual-
ity.
Ours vs. subject-driven methods. As depicted in Fig. 6,
we compare our method with subject-driven approaches
to highlight differences in application focus. BLIP-
Diffusion[19], relying only on a single reference image,
struggles to preserve the details of the foreground in the
generation results. Meanwhile, DreamBooth [32] and Cus-
8730
Figure 7. By applying the proposed losses, the visual quality of
the synthesized image can be enhanced in object texture fidelity
and overall scene harmony.
Figure 8. the visualization of applying the foreground layout con-
trol strategy to eliminate two types of errors.
tomDiffusion [17] achieve favorable visual quality by gen-
erating individual elements in customized scenes. How-
ever, they struggle to cohesively combine multiple visual
concepts, resulting in loss of the foreground elements and
significant detail distortion. Although our proposed frame-
work does not provide extensive variability in foreground
elements, its robust layout control and superior scene syn-
thesis quality still make it well-suited for a wide range of
practical applications.
5.5. Module Analysis
In this section, we discuss the core components of the pro-
posed AnyScene to validate their respective effectiveness.
Training Losses. In this study, the Foreground Injection
Module is trained to seamlessly integrate foreground infor-
mation into the pre-trained diffusion model. We incorporate
two additional losses to aid the network’s training alongside
the standard denoising loss. As depicted in Fig. 7, visual
comparisons highlight the distinct outcomes when employ-
ing different loss functions. The baseline solely using the
denoising loss shows slight edge and color distortions in
the generated images. Incorporating content loss improves
the network’s capability to accurately capture texture de-
tails and preserve the true colors of the foreground textures.
Besides, employing gradient loss contributes to generating
more realistic foreground edges while curbing shape dis-
tortions in foreground elements by constraining gradient
changes at their edges. Combining these two losses’ ad-
vantages, AnyScene can generate structurally accurate and
Figure 9. the visualization of the effectiveness of employing fore-
ground blending.
visually harmonious customized scene images.
Foreground Control Strategy. We conduct visualization
experiments to assess the efficacy of the foreground control
strategy, particularly its impact on the surrounding scenes
after modifying the attention map. Fig. 8 shows the scene
changes before and after strategy application with consis-
tent input noise. Results indicate that our strategy elimi-
nates severe foreground distortions and redundant elements
without degrading overall scene quality. Instead, it en-
hances contextual coherence in the eliminated areas, im-
proving the visual integrity of the generated images.
Effect of Foreground Blending. Fig. 9 demonstrates the
impact of applying foreground blending. In this case,
AnyScene receives a foreground image and the textual
prompt “with sunset” to generate a preliminary correspond-
ing scene. The foreground blending module is then uti-
lized to integrate the details from foreground elements. As
the figure indicates, directly using “Copy & Paste” results
in noticeable color discrepancies and edge disharmony. In
contrast, employing the image blending technique can con-
sider the generated scene’s color and lighting conditions and
achieve a visually harmonious final synthesized image.
6. Conclusion
This paper presents AnyScene, a tailored image synthe-
sis framework for customizing scenes to specific compos-
ited foregrounds. Firstly, we propose the foreground in-
jection module, which effectively guides the pre-trained
diffusion model to generate harmonious scenes with the
given foreground. Then, we develop the foreground lay-
out control strategy to ensure robust scene generation and
the foreground blending mechanism to preserve foreground
details. Our extensive experiments validate the effective-
ness of AnyScene with the proposed task, highlighting its
versatility and potential for a wide range of applications.
Acknowledgements
This work is supported by the National Natural Science
Foundation of China under grants U21B2024, 62202329,
and 62272337.
8731
References
[1] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale gan training for high fidelity natural image synthesis.
arXiv preprint arXiv:1809.11096 , 2018. 2
[2] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structpix2pix: Learning to follow image editing instructions.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 18392–18402, 2023.
2
[3] Alexander V . Buslaev, Alex Parinov, Eugene Khvedchenya,
Vladimir I. Iglovikov, and Alexandr A Kalinin. Albu-
mentations: fast and flexible image augmentations. ArXiv ,
abs/1809.06839, 2018. 5
[4] Holger Caesar, Jasper R. R. Uijlings, and Vittorio Fer-
rari. Coco-stuff: Thing and stuff classes in context. 2018
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 1209–1218, 2016. 5
[5] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and
Daniel Cohen-Or. Attend-and-excite: Attention-based se-
mantic guidance for text-to-image diffusion models. ACM
Transactions on Graphics (TOG) , 42(4):1–10, 2023. 2, 4
[6] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems , 34:8780–8794, 2021. 2
[7] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,
Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,
Hongxia Yang, et al. Cogview: Mastering text-to-image gen-
eration via transformers. Advances in Neural Information
Processing Systems , 34:19822–19835, 2021. 2
[8] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
transformers for high-resolution image synthesis. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 12873–12883, 2021. 2
[9] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
transformers for high-resolution image synthesis. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 12873–12883, 2021. 2
[10] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,
Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-
based text-to-image generation with human priors. In Eu-
ropean Conference on Computer Vision , pages 89–106.
Springer, 2022. 2
[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. Advances in
neural information processing systems , 27, 2014. 2
[12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
age editing with cross attention control. arXiv preprint
arXiv:2208.01626 , 2022. 2, 4
[13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In Neural Information Processing Systems , 2017. 6
[14] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. arXiv preprint arXiv:2207.12598 , 2022. 2[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840–6851, 2020. 2, 4
[16] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 4401–4410, 2019. 5
[17] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Shechtman, and Jun-Yan Zhu. Multi-concept customization
of text-to-image diffusion. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 1931–1941, 2023. 1, 2, 6, 8
[18] Alina Kuznetsova, Hassan Rom, Neil Gordon Alldrin, Jasper
R. R. Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Ka-
mali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov,
Tom Duerig, and Vittorio Ferrari. The open images dataset
v4. International Journal of Computer Vision , 128:1956 –
1981, 2018. 5
[19] Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-
diffusion: Pre-trained subject representation for control-
lable text-to-image generation and editing. arXiv preprint
arXiv:2305.14720 , 2023. 1, 2, 6, 7
[20] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. arXiv
preprint arXiv:2301.12597 , 2023. 5
[21] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-
wei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.
Gligen: Open-set grounded text-to-image generation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 22511–22521, 2023. 2
[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13 , pages 740–755. Springer, 2014. 5, 6
[23] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-
gang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning
adapters to dig out more controllable ability for text-to-image
diffusion models. arXiv preprint arXiv:2302.08453 , 2023. 1,
2
[24] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741 , 2021. 1, 2
[25] Patrick P ´erez, Michel Gangnet, and Andrew Blake. Poisson
image editing. In Seminal Graphics Papers: Pushing the
Boundaries, Volume 2 , pages 577–582. 2023. 5
[26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 3, 6
[27] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.
8732
Zero-shot text-to-image generation. In International Confer-
ence on Machine Learning , pages 8821–8831. PMLR, 2021.
2
[28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022. 1
[29] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee. Generative ad-
versarial text to image synthesis. In International conference
on machine learning , pages 1060–1069. PMLR, 2016. 2
[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684–10695, 2022. 1, 2, 4, 5, 6, 7
[31] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III
18, pages 234–241. Springer, 2015. 3
[32] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 22500–
22510, 2023. 1, 2, 6, 7
[33] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems , 35:36479–36494, 2022. 2
[34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems , 35:36479–36494, 2022. 1, 2
[35] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:
Open dataset of clip-filtered 400 million image-text pairs.
arXiv preprint arXiv:2111.02114 , 2021. 2
[36] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for training
next generation image-text models. Advances in Neural In-
formation Processing Systems , 35:25278–25294, 2022. 2,
6
[37] Akash Srivastava, Lazar Valkov, Chris Russell, Michael U
Gutmann, and Charles Sutton. Veegan: Reducing mode col-
lapse in gans using implicit variational learning. Advances
in neural information processing systems , 30, 2017. 2
[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 2
[39] Guangxuan Xiao, Tianwei Yin, William T Freeman, Fr ´edo
Durand, and Song Han. Fastcomposer: Tuning-free multi-
subject image generation with localized attention. arXiv
preprint arXiv:2305.10431 , 2023. 2, 4, 5
[40] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,
Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-
grained text to image generation with attentional generative
adversarial networks. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 1316–
1324, 2018. 2
[41] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-
jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-
fei Yang, Burcu Karagol Ayan, et al. Scaling autoregres-
sive models for content-rich text-to-image generation. arXiv
preprint arXiv:2206.10789 , 2(3):5, 2022. 2
[42] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin
Jin, Wenjun Zeng, and Zhibo Chen. Inpaint anything:
Segment anything meets image inpainting. arXiv preprint
arXiv:2304.06790 , 2023. 1, 2, 6, 7
[43] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-
gang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-
gan: Text to photo-realistic image synthesis with stacked
generative adversarial networks. In Proceedings of the IEEE
international conference on computer vision , pages 5907–
5915, 2017. 2
[44] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-
gang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-
gan++: Realistic image synthesis with stacked generative ad-
versarial networks. IEEE transactions on pattern analysis
and machine intelligence , 41(8):1947–1962, 2018. 2
[45] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3836–3847, 2023. 1, 2, 3, 5, 6, 7
8733
