MonoDiff : Monocular 3D Object Detection and
Pose Estimation with Diffusion Models
Yasiru Ranasinghe, Deepti Hegde, and Vishal M. Patel
Johns Hopkins University, Baltimore, USA
{dranasi1, dhegde1, vpatel36 }@jhu.edu
Abstract
3D object detection and pose estimation from a single-
view image is challenging due to the high uncertainty
caused by the absence of 3D perception. As a solution,
recent monocular 3D detection methods leverage addi-
tional modalities, such as stereo image pairs and LiDAR
point clouds, to enhance image features at the expense
of additional annotation costs. We propose using diffu-
sion models to learn effective representations for monoc-
ular 3D detection without additional modalities or training
data. We present MonoDiff, a novel framework that em-
ploys the reverse diffusion process to estimate 3D bound-
ing box and orientation. But, considering the variability
in bounding box sizes along different dimensions, it is inef-
fective to sample noise from a standard Gaussian distribu-
tion. Hence, we adopt a Gaussian mixture model to sam-
ple noise during the forward diffusion process and initial-
ize the reverse diffusion process. Furthermore, since the
diffusion model generates the 3D parameters for a given
object image, we leverage 2D detection information to pro-
vide additional supervision by maintaining the correspon-
dence between 3D/2D projection. Finally, depending on
the signal-to-noise ratio, we incorporate a dynamic weight-
ing scheme to account for the level of uncertainty in the
supervision by projection at different timesteps. MonoDiff
outperforms current state-of-the-art monocular 3D detec-
tion methods on the KITTI and Waymo benchmarks without
additional depth priors. MonoDiff project is available at:
https://dylran.github.io/monodiff.github.io.
1. Introduction
Research on monocular 3D object detection is currently a
focal point in various fields, such as autonomous driving
[8, 36], robotic navigation [29, 44], and beyond [32]. The
objective is to generate 3D bounding box parameters based
on the identification of objects in 2D images [8, 43, 64, 72].
Previous studies have extracted 3D information for ob-
Figure 1. Comparison between existing architectures for effective
representation learners for monocular 3D object detection. CMKD
[28] uses LiDAR data, and MonoNeRD [74] estimates the stereo
and depth image during training. MonoDiff uses monocular im-
ages with diffusion models as effective representation learners.
ject poses using 2D/3D constraints and geometric priors.
These constraints typically necessitate additional annota-
tions [8, 24] or the employment of Computer-Aided Design
models [5, 47]. Alternatively, some earlier approaches use
pseudo-LiDAR from depth estimates [50, 65, 71] or inte-
grate image features with depth maps as a precursor for the
3D detection model. Lately, the monocular 3D detection
research has focused on generating corresponding bird’s-
eye-view (BEV) representations [11, 53, 77, 78] from 2D
images to work with pre-trained 3D detectors.
Following the improvement from generating meaningful
representations, recent methods have demonstrated the ef-
fectiveness of leveraging additional training data or modal-
ities, as illustrated in Fig. 1, for inferring 3D information
[25, 38, 64]. While these additional modalities help learn
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10659
effective representations, their inclusion burdens the cost of
data acquisition and annotation. Notably, recently proposed
denoising diffusion probabilistic models [26, 66], known as
diffusion models, have emerged as proficient representation
learners for discriminative tasks [18, 60]. The forward dif-
fusion process in these models is conceptualized as an aug-
mentation technique, contributing to more effective repre-
sentation learning than a conventional single forward pass
network.
Up until recently, diffusion models [26, 66] have ex-
hibited superior performance in learning data distributions
for generative tasks, outperforming GANs and achieving
state-of-the-art results [2, 15]. Capitalizing on their suc-
cess in generative tasks, diffusion models have found ap-
plication in various image-to-image applications, includ-
ing super-resolution [62], inpainting [45], image segmen-
tation [1, 2, 22], and image deblurring [59]. Building on
the demonstrated effectiveness of diffusion models as rep-
resentation learners for various computer vision challenges
[2], several contemporary approaches have adopted diffu-
sion models for other perception tasks like 2D object de-
tection [6], crowd analysis [57], and human pose estima-
tion [18]. Consequently, we investigate the role of diffusion
models in monocular 3D object detection and pose estima-
tion, focusing on their ability to elevate 2D detections to 3D
parameters.
MonoDiff conceptualizes the 3D detection and pose es-
timation of an object through a reverse diffusion process,
wherein a distribution characterized by high variance un-
dergoes a progressive transformation towards one marked
by low variance. Due to the high variation in bounding box
dimension, the uncertainty along different axes will differ.
Thus, the estimates will not necessarily converge to the nor-
mal distribution (i.e., zero mean and unit variance Gaussian)
after completing the forward process. The standard diffu-
sion process is thus not the most appropriate to model the
uncertainty or initialize the starting bounding box distribu-
tion for 3D localization and pose estimation tasks.
To address this, we model the latent distributions of the
reverse process using Gaussian Mixture Models (GMM) to
account for different uncertainty levels along different di-
mensions. Furthermore, we use 2D bounding box informa-
tion to supervise the localization and orientation estimates
from the reverse diffusion process using the corresponding
constraints of 3D-2D projection. Lastly, we use a signal-to-
noise ratio-based weighting scheme on 2D/3D projection
supervision to account for the uncertainty levels at different
timesteps.
The contributions of the paper are:
• We present MonoDiff, a novel detection framework that
leverages the distribution learning ability of diffusion
models to enable accurate 3D perception from a single
image.• We present a GMM-based initialization for the reverse
diffusion process instead of a normal distribution to re-
solve the uncertainty variation along different 3D local-
ization and orientation parameters.
• We experiment on both the KITTI-3D detection bench-
mark [20] and the Waymo Open Dataset [68]. Our exper-
imental results showcase the effectiveness of MonoDiff,
surpassing the state-of-the-art methodologies without ad-
ditional modalities.
2. Related work
Monocular 3D object detection is designed to establish a
transformation between the camera sensor input and 3D at-
tributes, as outlined by Fang et al. [17]. In contrast to stereo
methodologies [10, 39, 40, 75] that rely on dual RGB cam-
eras, monocular systems operate with a sole single-view in-
put. Initially, Mousavian et al. [48] proposed a technique
involving the regression of relatively stable 3D parameters
based on 2D detections. More recently, MonoJSG [42] in-
troduces a method that utilizes a joint semantic and geo-
metric cost volume to mitigate the inherent challenges of
ill-posedness in monocular 3D object detection. Further-
more, MonoGround [55] suggests incorporating a ground
plane as an additional depth estimation source without ne-
cessitating extra data or modalities. Recent advancements
in monocular 3D detection also integrate geometric proper-
ties [13, 43, 80] to effectively address the challenges asso-
ciated with the ill-posed nature of the task.
Pose estimation within 3D object detection systems is
concerned with accurately determining the orientation of in-
stances. Various solutions, both closed-form and iterative,
assume correspondences between 2D keypoints in the im-
age and a 3D object model, as discussed in [4, 35]. Alter-
natively, some approaches involve constructing 3D models
for object instances and then identifying the 3D pose in the
image that best aligns with the model [19, 61]. Addressing
images with multiple instances, architectures akin to Fast-
RCNN were utilized in [3, 7, 9, 30, 31], where the region-
of-interest features captured instance appearance, and a
classification head provided pose predictions.
Pseudo-auxiliary feature-based methods utilize addi-
tional data sources or modalities such as LiDAR, BEV , and
stereo information during training to establish priors for
the detection model. Pseudo-LiDAR-based 3D detectors
[11, 46, 54, 71] derive benefits from both emulating the
representation of LiDAR data during inference and lever-
aging the accurate 3D information provided by LiDAR dur-
ing training. Typically, these methods involve transform-
ing 2D images into intermediate 3D representations, such
as pseudo-point clouds through depth estimators [70, 71].
Subsequently, LiDAR-based methods are applied to these
representations. Simultaneously, MonoNeRD [74] incorpo-
rates stereo images during training and builds a pipeline to
10660
Figure 2. (1) We train a 2D detector to localize objects on the images. (2) We estimate the parameters of the Gaussian Mixture Model
(ΦGMM ) to generate supervisory signals ( ϕ1, ϕ2,···, ϕT) from the ground truths for the denoising network ( f). The localized object
features ( ψ) are generated using a feature encoder ( Ψ) and provided to fas the conditioning. The estimates ( ˆϕt) offare saved at each
time step to learn the reverse diffusion process. (3) Loss propagation to train the pipeline. Ldiff is computed between ˆϕtandϕtfor
{T−1,···,0}. At each time step, an estimate ˆϕt
0forϕ0is produced using ˆϕtaccording to the equation to compute the loss ( Lt
2D)
between 2D bounding box information and the projection of estimated 3D bounding box. Then Lt
2Dis scaled using λtat each time step
and accumulated to compute Lreproj .(4) During the inference process, objects are first localized and then passed through Ψandfto
estimate the 3D bounding box.
recover the depth map and the right RGB image in addition
to the 3D bounding boxes. For that, Xu et al. [74] intro-
duce scene modeling to generate 3D representations akin to
Neural Radiance Fields. Across all these approaches, the
overarching goal is to produce complementary features us-
ing monocular images to enhance 3D object detection and
pose estimation. In this study, we leverage diffusion mod-
els to extract features at different timesteps, reducing the
need for additional training data while augmenting the size
of features learned through noise augmentation.
2.1. Diffusion models for perception
Diffusion models have proven to be a potent methodol-
ogy for learning a data distribution suitable for sampling.
Originally introduced DDPMs [66] to generate images or
for image-to-image translation, have undergone recent ad-
vancements, particularly in terms of improved inference
speed [26, 49, 67]. Previous research has delved into apply-
ing diffusion models across diverse generative tasks, rang-
ing from image inpainting [45] to text generation [41].
Pioneering the integration of diffusion models into ob-
ject detection, DiffusionDet [6] addresses the 2D objectdetection problem by denoising random boxes into object
bounding boxes through the diffusion process. Building
on this concept, DiffRef3D [33] extends the application
of diffusion models to 3D perception tasks, employing Li-
DAR point clouds instead of images. Additionally, diffu-
sion models have found diverse real-time applications, in-
cluding human pose estimation [21, 27, 76], crowd analysis
[14, 57], and segmentation [56, 73]. In this context, we in-
vestigate the utilization of diffusion models to address the
challenges of 3D detection and pose estimation within the
framework of our MonoDiff.
3. MonoDiff
We aim to generate accurate 3D coordinates and poses with
diffusion models. We condition the denoising process on
the image features of objects localized by a fixed 2D detec-
tor, and the diffusion process is formulated as the iterative
noising of a vector with the 3D coordinates and angles into
a Gaussian distribution. The components of our proposed
MonoDiff pipeline along with loss propagation and infer-
ence are illustrated in Fig. 2.
10661
3.1. Background
The diffusion model has two processes: the forward and the
reverse process.
The forward process is the approximate posterior
q(x1:T|x0)modeled by a Markov chain that gradually trans-
forms the original data distribution to a normal distribu-
tionN(0, I)by adding Gaussian noise to the original data.
At each degradation step, the noise is sampled from a
predefined parametrized noise schedule depending on the
timestep t. At each step t, the noise is incrementally added
to the signal according to
q(xt|xt−1) :=N(xt;p
1−βtxt−1, βtI).
This formulation allows for the sampling of degraded sam-
ples at any given timestep in closed form by
q(xt|x0) :=N 
xt;√¯αtx0,(1−¯αt)I
,
where αt:= 1−βtand¯αt:=Qt
s=1αs.
The reverse process in the standard diffusion models, first,
a realization is sampled from a normal Gaussian distribu-
tion. It is then iterated through the denoising network to
transverse to the data distribution. We refer the readers to
[26] for more details.
3.2. MonoDiff forward diffusion process
The uncertainty along different dimensions differs in simi-
lar object categories, and the geometric structure and ori-
entation of bounding boxes are different across classes.
Hence, the target distribution of the forward diffusion pro-
cess does not have to converge to a normal distribution, i.e.,
a Gaussian distribution with a zero mean and a unit vari-
ance, as the means and variances of different dimensions
will differ. If we initialize from a N(0,I), this overlooks
the difference in variance between the dimensions. Hence,
we don’t enforce the distribution to converge to N(0,I)in
the forward diffusion process. Further, using a normal dis-
tribution to initialize the inference process is not the most
suitable, as a single distribution does not account for these
variations. Also, N(0,I)initialization represents random
boxes without any regard for the object. This is similar to
DiffusionDet [6] and DiffBev [79], which trains a separate
decoder to predict the box parameters from the image fea-
tures instead of the denoising network.
In MonoDiff, we use a mixture of Gaussians (GMM)
[37] to define the initial bounding boxes’ parameters’ (di-
mensions and orientations) distribution, ensuring the com-
patibility with the Gaussian assumption inherent in the for-
mulation of DDPMs. Utilizing a GMM for initialization
offers the advantage of enhancing the inference speed of
the network. This is achieved by sampling the starting
latent variable from a distribution containing information
about bounding boxes instead of a random initialization.
Moreover, initializing and noise sampling from the GMM
contribute to a more efficient implementation of DDPMs
by constraining the range of latent variable values at eachtimestep. Then, the set of parameters ( ΦGMM ) is optimized
as follows using the Expectation-Maximization algorithm:
argmax
ΦGMMYN
n=1XK
k=1πkN(ϕn
0|µk,Σk),
where ϕn
0s are ground truth bounding box parameters, and
πk,µk, and Σkare the prior, mean, and covariance of the
individual Gaussian component.
Subsequently, we degrade the ground truth bounding
boxes by adding noise from the GMM-based initialization
provided by ΦGMM . To approximate ΦGMM at the end of
the forward diffusion process, we modify the forward orig-
inal DDPM diffusion equation following [23] as follows:
ϕt=µ+√αt(ϕ0−µ) +p
(1−αt)·ϵ. (1)
where ϕtis a sample from the latent distribution Φtat the tth
timestep. Then, µis the mean of the selected Gaussian com-
ponent, and noise ( ϵ) is sampled from a zero mean Gaussian
distribution but with covariance equivalent to that of the se-
lected Gaussian component.
Since we approximate the ΦGMM at the end of the for-
ward diffusion process, we denote it as ΦT. Then, we
choose a Gaussian component according to the πkdistri-
bution. Note that, according to Eq. (1), at the end of the for-
ward diffusion process, the sampled point ϕTwill be from
the selected Gaussian component as the effect of ϕ0fades
away since αTgoes to zero. During training, we can as-
sign the ground truth boxes to the components in the GMM.
These assignments are used as additional labels during the
training of the 2D detector to give the best cluster assign-
ment during testing.
3.3. MonoDiff reverse diffusion process
In the reverse process, the denoising network is employed to
estimate latent samples at each timestep, enabling the deter-
mination of 3D coordinates and orientation as a determinis-
tic distribution derived from the initial distribution ΦT. The
optimization of the denoising network involves leveraging
intermediate distributions to learn the reverse diffusion pro-
cess effectively. For a sampled ϕtfrom Φt, the denoising
network f, conditioned on image features and the diffusion
step, reconstructs ˆϕt−1from ϕtusing the formulation:
ˆϕt−1=f(ϕt, ψ, t), t∈ {1, ..., T}.
to learn the propagation of distributions.
3.4. 2D Reprojection Regulation
To enhance supervision in 3D localization and pose estima-
tion, we incorporate information from the 2D bounding box
of the object of interest. Since the estimated 3D bounding
box of the localized object can be projected onto the image
plane, the projection should fall within its 2D bounding box.
Though these two do not have to overlap, we can expect a
snug fit between the projected bounding box and the ground
truth bounding box.
10662
To project the 3D bounding box, we need the rotation
matrix Rand the translation vector T. However, since the
diffusion pipeline operates on localized objects, we can as-
sume the Tas the origin and shift the 2D ground truth in-
formation accordingly. Consequently, the coordinates of the
3D bounding box can be expressed using the dimensions of
the bounding box. Next, we estimate the 2D bounding box
using the projected 3D bounding box vertices and compare
it with the ground truth 2D box. The reprojection provides
additional supervision since the projected bounding box is
a function of RandD, estimated by the diffusion pipeline.
3.5. Choice of generative parameters
We generate the rotation matrix Rand produce the dimen-
sion matrix Dusing the reverse process for parameter esti-
mation of the bounding boxes. To solve for T, we identify
the vector that gives the closest projection to the initial 2D
bounding box from the detector. More details are provided
in the supplementary on solving for translation. The range
of values probable for box dimensions is small compared
to the translation vector, as the 3D box size does not vary
depending on the position of the camera coordinate system.
This is desirable for learning the distribution with diffusion
models [26].
3.6. Loss function
To build the connections between 3D representations and
2D observations, we optimize the parameters of the denois-
ing network fdiscussed in Sec. 3.3. The overall objective
is expressed as a composite loss function consisting of dif-
fusion loss ( Ldiff) and reprojection loss ( Lreproj ).
Diffusion loss. The denoising network parameters are op-
timized to produce ˆϕi
t−1from ϕi
tin a single forward pass.
Unlike the usual objective where the network is trained to
estimate the amount of noise in the ˆϕi
t−1, we formulate the
diffusion loss Ldiffas follows:
Ldiff=TX
t=1NX
i=1∥f(ϕi
t, ψi, t)−ϕi
t−1∥2
2,
following preliminary work on DDPMs [26, 67].
Reprojection loss. The reprojection of the 3D bounding
box should tightly fit into the 2D detection window, and we
ensure this by using the bounding box and IoU losses from
the 2D detection network. We compute the loss across all
timesteps in the diffusion loss, and the reprojection loss is
written as:
Lreproj=TX
t=1NX
i=1λbboxLti
bbox+λiouLti
iou,
where Lti
bboxandLti
iouare the independent loss of each sam-
ple at each timestep. More on computing Lti
bbox andLti
iou
are provided in the supplementary document.
Weighting factor. The 3D localization and orientation esti-
mates at the initial timesteps of the reverse process become
more uncertain as the signal-to-noise ratio increases as thereverse process progresses. In order to account for the un-
certainty at different signal-to-noise ratio stages, we adopt
a weighting factor as follows:
λt=(1−βt)(1−¯αt)/βt
(p+SNR (t))γ,
where SNR (t)=¯αt
1−¯αtandpandγare hyperparameters
following [12]. With the introduced weighting scheme, we
modify the reprojection loss as follows:
Lreproj=TX
t=1λtNX
i=1λbboxLti
bbox+λiouLti
iou.
Training. In the training stage, we take the monocular im-
ages and compute the distributions for DandRfrom the
training set. Subsequently, we compute ΦTas a GMM to
initialize the noise sampling procedure. Then, for each sam-
pled ground truth 3D bounding box ϕi
0we generate a set of
intermediary samples
ϕi
t;t∈ {1, ..., T}	
using Eq. (1).
Next, we pass the monocular image of the object corre-
sponding to the ground truth ϕi
0through the feature encoder
Ψand extract the features ψias the condition to the diffu-
sion process. Finally, the denoising network is optimized
using the composite loss function
Lf=Ldiff+λreprojLreproj,
where λreproj is a hyperparameter to scale Lreproj .
Inference. During testing, we first detect the 2D bounding
boxes or the objects of interest using a 2D detector. Then,
we initialize ϕTfor each object of interest and extract the
features for the monocular image. We perform the reverse
process by recursively feeding to the denoising network to
estimate ϕ0. Once we generate the DandRmatrices using
the reverse process, we follow the procedure explained in
Sec. 3.5 to compute the localization and orientation infor-
mation in the camera coordinate system.
4. Experiments
4.1. Benchmarks and metrics
KITTI. The KITTI-3D detection [20] benchmark has a
trainval set and a testset with 7,481 images and 7,518 im-
ages, respectively. To train MonoDiff for the KITTI dataset,
we employ the train-val split used in [9]. Accordingly, we
split the trainval set as 3,712 training images and 3,769 val-
idation images. To evaluate MonoDiff performance on the
KITTI dataset, we use the 3D detection criterion with a 0.7
threshold and report AP 3D(R40) [74].
Waymo. The Waymo Open Dataset [68] provides 798 train
and 202 valsequences. We adopt the performance reporting
criterion of CaDDN [58] for a fair comparison with existing
methods. We train MonoDiff on 51,564 samples acquired
solely from the front camera. We report the numbers on
three ranges at 30m, 50m, and infinity, as well as on two
difficulty levels. Performance on the validation set is mea-
sured using the official evaluation with 3D IoU criterion at
0.5 threshold. The numbers are reported for mean average
10663
Table 1. AOS and AP 3Dperformance on KITTI testset. The best andsecond-best figures are in color. The performance metrics for the
other methods are reported from the respective published results.
Methods VenueAOS AP3D
Average Easy Moderate Hard Average Easy Moderate Hard
Car
D4LCN [16] CVPR’20 78.66 90.01 82.08 63.90 12.63 16.65 11.72 9.51
CaDDN [58] CVPR’21 68.37 78.28 67.31 59.52 14.68 19.17 13.41 11.46
DDMP-3D [69] CVPR’21 77.58 90.73 80.20 61.82 14.10 19.71 12.78 9.80
MonoRCNN [63] ICCV’21 81.70 91.90 86.48 66.71 13.68 18.36 12.65 10.03
MonoJSG [42] CVPR’22 86.88 92.64 85.00 83.00 18.16 24.69 16.14 13.64
LPCG [51] ECCV’22 91.29 96.68 93.26 83.94 19.58 25.56 17.80 15.38
DID-M3D [52] ECCV’22 88.45 94.20 90.55 80.61 18.15 24.40 16.29 13.75
MonoNerd [74] ICCV’23 85.58 94.24 86.13 76.38 18.50 22.75 17.13 15.63
MonoDiff 91.54 97.16 93.72 83.75 23.12 30.18 21.02 18.16
Pedestrian
D4LCN [16] CVPR’20 36.35 46.73 33.62 28.71 3.60 4.55 3.42 2.83
CaDDN [58] CVPR’21 19.12 24.45 17.13 15.79 9.26 12.87 8.14 6.76
DDMP-3D [69] CVPR’21 35.97 46.12 33.35 28.45 3.83 4.93 3.55 3.01
MonoRCNN [63] ICCV’21 43.99 55.19 42.59 34.18 8.11 11.21 7.28 5.85
MonoJSG [42] CVPR’22 35.82 44.88 32.30 30.27 8.44 11.94 7.36 6.03
LPCG [51] ECCV’22 43.94 56.60 39.79 35.42 8.11 10.82 7.33 6.18
DID-M3D [52] ECCV’22 37.60 46.78 36.37 29.66 8.43 11.78 7.44 6.08
MonoNerd [74] ICCV’23 22.44 28.43 20.54 18.36 9.49 13.20 8.26 7.02
MonoDiff 46.00 58.25 43.14 36.62 9.91 13.51 8.94 7.28
Cyclist
D4LCN [16] CVPR’20 35.57 48.03 31.70 26.99 1.83 2.45 1.67 1.36
CaDDN [58] CVPR’21 22.56 30.35 19.96 17.38 4.57 7.00 3.41 3.30
DDMP-3D [69] CVPR’21 33.95 46.42 29.53 25.91 3.00 4.18 2.50 2.32
MonoRCNN [63] ICCV’21 42.43 54.93 39.89 32.48 2.03 2.89 1.67 1.54
MonoJSG [42] CVPR’22 38.71 49.31 33.36 33.46 5.08 8.03 3.87 3.33
LPCG [51] ECCV’22 49.20 63.07 45.24 39.28 4.97 6.98 4.38 3.56
DID-M3D [52] ECCV’22 40.63 51.38 38.15 32.35 5.05 7.82 3.95 3.37
MonoNerd [74] ICCV’23 22.99 30.64 20.13 18.19 3.14 4.79 2.48 2.16
MonoDiff 52.42 67.21 48.34 41.70 5.55 8.52 4.35 3.78
Table 2. 3D mAP and 3D mAPH pereformance on Waymo valset. The best andsecond-best figures are in color. The performance metrics
for the other methods are reported from the respective published results.
Methods Venue3D mAP 3D mAPH
Overall 0 - 30m 30 - 50m 50m - ∞ Overall 0 - 30m 30 - 50m 50m - ∞LEVEL 1CaDDN [58] CVPR’21 17.54 45.00 9.24 0.64 17.31 44.46 9.11 0.62
MonoJSG [42] CVPR’22 5.65 20.86 3.91 0.97 5.47 20.26 3.79 0.92
LPCG [51] ECCV’22 6.23 18.39 3.44 0.19 6.09 18.03 3.33 0.17
CMKD [28] ECCV’22 14.69 38.67 6.26 0.40 14.59 38.44 6.20 0.38
DID-M3D [52] ECCV’22 20.66 40.92 15.63 5.35 20.47 40.60 15.48 5.24
MonoNerd [74] ICCV’23 31.18 61.11 26.08 6.60 30.70 60.28 25.71 6.47
MonoDiff 32.28 63.94 25.91 7.51 31.49 62.13 25.47 7.34LEVEL 2CaDDN [58] CVPR’21 16.51 44.87 8.99 0.58 16.28 44.33 8.86 0.55
MonoJSG [42] CVPR’22 5.34 20.79 3.79 0.85 5.17 20.19 3.67 0.82
LPCG [51] ECCV’22 5.84 18.33 3.34 0.17 5.70 17.97 3.23 0.15
CMKD [28] ECCV’22 12.99 38.17 5.77 0.38 12.90 37.95 5.71 0.35
DID-M3D [52] ECCV’22 19.37 40.77 15.18 4.69 19.19 40.46 15.04 4.59
MonoNerd [74] ICCV’23 29.29 60.91 25.36 5.77 28.84 60.08 25.00 5.66
MonoDiff 30.73 63.86 25.28 6.43 30.48 62.92 24.86 6.29
precision and mean average precision weighted by heading
annotated as mAP and mAPH, respectively.
4.2. Implementation details
Training details. We implemented MonoDiff using the Py-
Torch framework and performed the experiments with fourNVIDIA A6000 GPUs. We sample four ground truth points
per iteration and perform the forward diffusion process for
100 steps. We estimate GMM using five Gaussian compo-
nents and use DDIM [67] to improve inference speed. The
denoising network is trained for 100 epochs with 256×256
images. To produce image features, we use the ResNet-34
10664
Figure 3. Qualitative results corresponding to MonoDiff.
architecture (pre-trained on ImageNet) as the context en-
coder Ψ. We use an AdamW optimizer with a fixed learning
rate 1e-4 and a linear warm-up schedule over ten epochs.
The 2D detector is trained using AdamW [34] optimizer
with a batch size of four and momentum factors 0.9 and
0.999 for β1andβ2. On KITTI, we use a 2e-3 learning rate
for 75 epochs and then 15 epochs with a 5e-4 learning rate.
On Waymo, we train with a learning rate of 1e-3 for the first
20 epochs and a learning rate of 1e-4 for the last ten epochs.
In both experiments, we set the weight decay factor to 1e-4.
Hyperparameters. Hyperparameters for the loss function
areλbbox,λiou,λt, and λreproj . We set λreproj at 5e-2.
Then the parameters γ, and pinλtare fixed at 0.5 and 1,
respectively following [12]. We use 1 for λbbox and 0.02
forλioufor the reprojection loss. Hyperparameters are the
same for all experiments. More details are provided in the
supplementary document.
4.3. Main results
KITTI testset results are tabulated in Tab. 1 where with-
out additional modalities and training data MonoDiff out-
performs previous methods. We outperform the most recent
SOTA method MonoNeRD [74], which uses stereo images
during training. We boost the performance ( AOS/AP 3D)
from 86.13/17.13 to 93.72/21.02 under moderate setting
and from 76.38/15.63 to 83.75/18.13 under hard setting for
the Car object category, respectively.Waymo valset results are tabulated in Tab. 2 for 3D ob-
jection detection from the official evaluation. MonoD-
iff achieves competitive results 32.28/31.49 compared to
MonoNeRD 31.18/30.70 on 3DmAP/3DmAPH without
using additional data.
Qualitative results are presented in Fig. 3 for 3D localiza-
tion and pose estimation generation with diffusion models.
4.4. Ablation studies
We conduct ablation experiments on KITTI valset to vali-
date the impact of each design in our method.
Diffusion process impact. To demonstrate the impact of
performing the 3D detection as a generative process, we
consider two baselines: (1) Baseline 1 , which mirrors the
architecture of MonoDiff but a single step inference. (2)
Baseline 2 , where the model architecture from Baseline 1
matches the computational complexity of MonoDiff by it-
erating. The baselines and MonoDiff results are presented
in Tab. 3, and the former is inferior to the latter.
Table 3. Ablation study for diffusion pipeline.
MethodAOS AP 3D
Easy / Moderate / Hard
Baseline 1 92.90 / 88.75 / 76.76 27.74 / 18.25 / 15.88
Baseline 2 93.11 / 88.23 / 77.96 27.86 / 18.47 / 15.53
MonoDiff 98.46 / 94.72 / 85.75 32.18 / 22.02 / 19.84
MonoDiff component analysis. As tabulated in Tab. 4, we
begin with the standard diffusion model and sample noise
from the normal distribution. G,F,R, and Win Tab. 4
represent GMM initialization, including feature encoder for
conditioning, 2D projection supervision, and scaling with
the weighting factor, respectively. The standard diffusion
model performs better than Baselines 1 and 2, promot-
ing diffusion models (generative models) for discriminative
tasks in addition to Tab. 3. According to Tab. 4, initializing
with the GMM model of ΦTimproves orientation perfor-
mance by 3%, while detection performance has improved
by∼2%. Then, using 2D bounding box information dur-
Table 4. Ablation study for different components.
G F R WAOS AP 3D
Easy / Moderate / Hard
✗ ✗ ✗ ✗ 94.32 / 89.12 / 78.58 28.44 / 19.99 / 16.52
✓ ✗ ✗ ✗ 97.27 / 93.11 / 83.69 31.10 / 21.44 / 18.89
✓ ✗ ✓ ✗ 97.64 / 93.61 / 84.33 31.44 / 21.62 / 19.18
✓ ✓ ✓ ✗ 98.05 / 94.17 / 85.04 31.81 / 21.82 / 19.51
✓ ✓ ✓ ✓ 98.46 / 94.72 / 85.75 32.18 / 22.02 / 19.84
ing training has improved the performance of our MonoDiff
even though using a 2D detector prohibits end-to-end train-
ing. Likewise, using monocular image features instead of
the image is conducive to performing the reverse diffusion
process even though the feature encoder introduces an extra
computational cost. However, since there is only a single
10665
pass through the feature encoder, the additional computa-
tion head is negligible compared to the computational cost
of the entire reverse process. Finally, adopting the weight-
ing scheme has gained marginal improvements at no ex-
pense.
Mixture model ablation. We vary the size
ofΦGMM to test the improvement from the
GMM and report the numbers in Tab. 5.
Table 5. Ablation study for ΦGMM size
#AOS AP 3D
Easy / Moderate / Hard
395.61 / 90.86 / 80.81 29.60 / 20.62 / 17.56
597.27 / 93.11 / 83.69 31.10 / 21.44 / 18.89
897.46 / 93.37 / 84.02 31.27 / 21.53 / 19.04Furthermore,
we discarded
the remaining
components of
our MonoDiff
to compare
fairly with
the standard
diffusion forward process. While increasing the number of
components improves the performance, it burdens memory
and time during training. Moreover, as the size of the
GMM increases, the performance tends to saturate at an
unnecessary expense. Hence, we chose five Gaussian
components to be optimal compute ΦGMM . We do not
consider the single Gaussian case as it is equivalent to the
standard diffusion model implementation.
Generalization with backbones and detectors. In this
part, we conduct experiments on the generalization ability
of our MonoDiff using different 2D detectors and feature
encoders. For the 2D detectors, we use off-the-shelf detec-
tors, and for feature encoders, we use the popular backbones
in object detection for comparison. The running speed and
Table 6. Ablation study for different detectors and backbones.
Speed FLOPS AOS AP 3D
(fps) (G) Easy / Moderate / HardDetectorYOLOv7 14.1 54.7 98.75 / 94.33 / 85.51 32.06 / 22.12 / 19.67
FasterRCNN 3.8 35.2 98.26 / 95.18 / 85.94 32.44 / 22.08 / 19.28
CenterNet 11.7 8.7 98.46 / 94.72 / 85.75 32.18 / 22.02 / 19.84
RetinaNet 7.3 17.3 97.59 / 95.29 / 85.79 32.35 / 21.95 / 19.73BackboneEfficientNet-b3 5.9 9.5 98.07 / 94.67 / 84.40 32.44 / 21.87 / 18.67
EfficientNet-b5 3.6 23.5 98.14 / 94.75 / 84.32 32.35 / 21.94 / 18.59
ResNet-34 11.7 3.5 98.46 / 94.72 / 85.75 32.18 / 22.02 / 19.84
ResNet-50 8.2 4.5 98.15 / 94.97 / 85.88 31.94 / 22.16 / 19.14
MobileNet 16.2 0.4 96.59 / 94.65 / 84.46 30.73 / 21.85 / 18.72
memory are tested on a single NVIDIA RTX A6000 GPU
on KITTI val. We compare the performance of our MonoD-
iff, including running speed, operation, AOS , and AP3D.
The results are shown in Tab. 6. According to Tab. 6, the
difference in performance for various 2D detectors and fea-
ture encoders is not very significant, though the inference
speed could vary significantly. Furthermore, in some cases,
light models outperform heavy models, especially for the
‘Easy’ class, while heavy models marginally perform bet-
ter for the ‘Moderate’ and ‘Hard’ classes. Nonetheless, the
proposed diffusion pipeline can be adapted to different ar-
chitectures depending on the practical requirements.
Inference Speed and performance. We tabulate the in-Table 7. Ablation study for inference speed and performance.
Speed FLOPS AOS AP 3D
(fps) (G) Easy / Moderate / Hard
LPCG 33.4 16.7 96.68 /93.26 /83.94 25.56 / 17.80 / 15.38
CMKD 10.1 9.8 95.07 / 89.81 / 83.24 28.55 /18.69 /16.77
MonoNeRD 12.5 7.2 94.24 / 86.13 / 76.38 22.75 / 17.13 / 15.63
MonoRCNN 14.3 8.5 91.90 / 86.48 / 66.71 18.36 / 12.65 / 10.03
MonoDiff 11.7 14.1 97.16 /93.72 /83.75 30.18 /21.02 /18.16
Figure 4. Infernce speed vs AOS performance. Corresponding
diffusion steps are shown near the data point.
ference speed (FPS) of state-of-the-art monocular 3D de-
tection methods against the performance on KITTI testin
Tab. 7 along with MonoDiff results. Moreover, the FPS
of our model satisfies most real-time requirements consid-
ering other state-of-the-art monocular 3D detection meth-
ods. Additionally, we considered the effect of reverse dif-
fusion timesteps and the trade-off between inference speed
and performance. The results are illustrated in Fig. 4 for
the orientation task. The final number of reverse diffusion
steps was selected where the FPS gain is significant with a
negligible drop in performance (elbow method).
5. Conclusion
We proposed a novel framework that handles monocular 3D
detection and pose estimation as a generative task. MonoD-
iff allows learning effective representations using diffu-
sion models without additional modalities and training data.
Also, by introducing a GMM-based initialization, MonoD-
iff improved the inference speed and performance of diffu-
sion models for object detection and pose estimation. Fur-
thermore, the MonoDiff pipeline generalizes well to dif-
ferent detectors and backbones while meeting the real-time
performance of other state-of-the-art methods.
Acknowledgments
This research was sponsored by the Army Research Labo-
ratory and was accomplished under Cooperative Agreement
Number W911NF-21-2-0211.
10666
References
[1] Tomer Amit, Tal Shaharbany, Eliya Nachmani, and Lior
Wolf. Segdiff: Image segmentation with diffusion proba-
bilistic models. arXiv preprint arXiv:2112.00390 , 2021. 2
[2] Dmitry Baranchuk, Ivan Rubachev, Andrey V oynov,
Valentin Khrulkov, and Artem Babenko. Label-efficient se-
mantic segmentation with diffusion models. arXiv preprint
arXiv:2112.03126 , 2021. 2
[3] Markus Braun, Qing Rao, Yikang Wang, and Fabian Flohr.
Pose-rcnn: Joint object detection and pose estimation using
3d object proposals. In 2016 IEEE 19th International Con-
ference on Intelligent Transportation Systems (ITSC) , pages
1546–1551. IEEE, 2016. 2
[4] LE Carvalho and Aldo von Wangenheim. 3d object recogni-
tion and classification: a systematic literature review. Pattern
Analysis and Applications , 22:1243–1292, 2019. 2
[5] Florian Chabot, Mohamed Chaouch, Jaonary Rabarisoa,
C´eline Teuli `ere, and Thierry Chateau. Deep manta: A
coarse-to-fine many-task network for joint 2d and 3d vehicle
analysis from monocular image. In CVPR , pages 2040–2049,
2017. 1
[6] Shoufa Chen, Peize Sun, Yibing Song, and Ping Luo. Dif-
fusiondet: Diffusion model for object detection. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 19830–19843, 2023. 2, 3, 4
[7] Xiaozhi Chen, Kaustav Kundu, Yukun Zhu, Andrew G
Berneshawi, Huimin Ma, Sanja Fidler, and Raquel Urtasun.
3d object proposals for accurate object class detection. Ad-
vances in neural information processing systems , 28, 2015.
2
[8] Xiaozhi Chen, Kaustav Kundu, Ziyu Zhang, Huimin Ma,
Sanja Fidler, and Raquel Urtasun. Monocular 3d object
detection for autonomous driving. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 2147–2156, 2016. 1
[9] Xiaozhi Chen, Kaustav Kundu, Yukun Zhu, Huimin Ma,
Sanja Fidler, and Raquel Urtasun. 3d object proposals us-
ing stereo imagery for accurate object class detection. IEEE
transactions on pattern analysis and machine intelligence ,
40(5):1259–1272, 2017. 2, 5
[10] Yilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Dsgn:
Deep stereo geometry network for 3d object detection. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 12536–12545, 2020. 2
[11] Yi-Nan Chen, Hang Dai, and Yong Ding. Pseudo-stereo for
monocular 3d object detection in autonomous driving. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 887–897, 2022. 1, 2
[12] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon
Kim, Hyunwoo Kim, and Sungroh Yoon. Perception pri-
oritized training of diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 11472–11481, 2022. 5, 7
[13] Zhiyu Chong, Xinzhu Ma, Hong Zhang, Yuxin Yue, Haojie
Li, Zhihui Wang, and Wanli Ouyang. Monodistill: Learn-
ing spatial features for monocular 3d object detection. arXiv
preprint arXiv:2201.10830 , 2022. 2[14] Adriano D’Alessandro, Ali Mahdavi-Amiri, and Ghassan
Hamarneh. Syrac: Synthesize, rank, and count. arXiv
preprint arXiv:2310.01662 , 2023. 3
[15] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems , 34:8780–8794, 2021. 2
[16] Mingyu Ding, Yuqi Huo, Hongwei Yi, Zhe Wang, Jianping
Shi, Zhiwu Lu, and Ping Luo. Learning depth-guided con-
volutions for monocular 3d object detection. In Proceedings
of the IEEE/CVF Conference on computer vision and pattern
recognition workshops , pages 1000–1001, 2020. 6
[17] Zhicheng Fang, Xiaoran Chen, Yuhua Chen, and Luc Van
Gool. Towards good practice for cnn-based monocular depth
estimation. In Proceedings of the IEEE/CVF Winter Confer-
ence on Applications of Computer Vision , pages 1091–1100,
2020. 2
[18] Runyang Feng, Yixing Gao, Tze Ho Elden Tse, Xueqing
Ma, and Hyung Jin Chang. Diffpose: Spatiotemporal dif-
fusion model for video-based human pose estimation. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 14861–14872, 2023. 2
[19] Vittorio Ferrari, Tinne Tuytelaars, and Luc Van Gool. Si-
multaneous object recognition and segmentation from single
or multiple model views. International journal of computer
vision , 67(2):159–188, 2006. 2
[20] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In 2012 IEEE conference on computer vision and pat-
tern recognition , pages 3354–3361. IEEE, 2012. 2, 5
[21] Jia Gong, Lin Geng Foo, Zhipeng Fan, Qiuhong Ke, Hos-
sein Rahmani, and Jun Liu. Diffpose: Toward more reliable
3d pose estimation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
13041–13051, 2023. 3
[22] Zhangxuan Gu, Haoxing Chen, Zhuoer Xu, Jun Lan,
Changhua Meng, and Weiqiang Wang. Diffusioninst: Dif-
fusion model for instance segmentation. arXiv preprint
arXiv:2212.02773 , 2022. 2
[23] Xizewen Han, Huangjie Zheng, and Mingyuan Zhou. Card:
Classification and regression diffusion models. Advances in
Neural Information Processing Systems , 35:18100–18115,
2022. 4
[24] Tong He and Stefano Soatto. Mono3d++: Monocular 3d ve-
hicle detection with two-scale 3d hypotheses and task priors.
InProceedings of the AAAI Conference on Artificial Intelli-
gence , pages 8409–8416, 2019. 1
[25] Jonas Heylen, Mark De Wolf, Bruno Dawagne, Marc Proes-
mans, Luc Van Gool, Wim Abbeloos, Hazem Abdelkawy,
and Daniel Olmeda Reino. Monocinis: Camera independent
monocular 3d object detection using instance segmentation.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 923–934, 2021. 1
[26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840–6851, 2020. 2, 3, 4, 5
[27] Karl Holmquist and Bastian Wandt. Diffpose: Multi-
hypothesis human pose estimation using diffusion models.
10667
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 15977–15987, 2023. 3
[28] Yu Hong, Hang Dai, and Yong Ding. Cross-modality knowl-
edge distillation network for monocular 3d object detection.
InEuropean Conference on Computer Vision , pages 87–104.
Springer, 2022. 1, 6
[29] Peiyun Hu, Jason Ziglar, David Held, and Deva Ramanan.
What you see is what you get: Exploiting visibility for 3d ob-
ject detection. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 11001–
11009, 2020. 1
[30] Siyuan Huang, Yixin Chen, Tao Yuan, Siyuan Qi, Yixin Zhu,
and Song-Chun Zhu. Perspectivenet: 3d object detection
from a single rgb image via perspective points. Advances
in neural information processing systems , 32, 2019. 2
[31] Lei Ke, Shichao Li, Yanan Sun, Yu-Wing Tai, and Chi-
Keung Tang. Gsnet: Joint vehicle pose and shape recon-
struction with geometrical and scene-aware supervision. In
Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part XV
16, pages 515–532. Springer, 2020. 2
[32] Seong-heum Kim and Youngbae Hwang. A survey on deep
learning based methods and datasets for monocular 3d object
detection. Electronics , 10(4):517, 2021. 1
[33] Se-Ho Kim, Inyong Koo, Inyoung Lee, Byeongjun Park, and
Changick Kim. Diffref3d: A diffusion-based proposal re-
finement framework for 3d object detection. arXiv preprint
arXiv:2310.16349 , 2023. 3
[34] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In 3rd International Conference on
Learning Representations, ICLR 2015, San Diego, CA, USA,
May 7-9, 2015, Conference Track Proceedings , 2015. 7
[35] Vincent Lepetit, Francesc Moreno-Noguer, and Pascal Fua.
Ep n p: An accurate o (n) solution to the p n p problem.
International journal of computer vision , 81:155–166, 2009.
2
[36] Buyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng, and
Xiaogang Wang. Gs3d: An efficient 3d object detec-
tion framework for autonomous driving. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 1019–1028, 2019. 1
[37] Jonathan Li and Andrew Barron. Mixture density estima-
tion. Advances in neural information processing systems ,
12, 1999. 4
[38] Peixuan Li and Huaici Zhao. Monocular 3d detection with
geometric constraint embedding and semi-supervised train-
ing. IEEE Robotics and Automation Letters , 6(3):5565–
5572, 2021. 1
[39] Peiliang Li, Tong Qin, et al. Stereo vision-based semantic
3d object and ego-motion tracking for autonomous driving.
InProceedings of the European Conference on Computer Vi-
sion (ECCV) , pages 646–661, 2018. 2
[40] Peiliang Li, Xiaozhi Chen, and Shaojie Shen. Stereo r-cnn
based 3d object detection for autonomous driving. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 7644–7652, 2019. 2[41] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang,
and Tatsunori B Hashimoto. Diffusion-lm improves control-
lable text generation. Advances in Neural Information Pro-
cessing Systems , 35:4328–4343, 2022. 3
[42] Qing Lian, Peiliang Li, and Xiaozhi Chen. Monojsg: Joint
semantic and geometric cost volume for monocular 3d ob-
ject detection. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 1070–
1079, 2022. 2, 6
[43] Qing Lian, Botao Ye, Ruijia Xu, Weilong Yao, and Tong
Zhang. Exploring geometric consistency for monocular 3d
object detection. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages
1685–1694, 2022. 1, 2
[44] Zechen Liu, Zizhang Wu, and Roland T ´oth. Smoke: Single-
stage monocular 3d object detection via keypoint estimation.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition Workshops , pages 996–997,
2020. 1
[45] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher
Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting
using denoising diffusion probabilistic models. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 11461–11471, 2022. 2, 3
[46] Xinzhu Ma, Shinan Liu, Zhiyi Xia, Hongwen Zhang, Xingyu
Zeng, and Wanli Ouyang. Rethinking pseudo-lidar repre-
sentation. In Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceed-
ings, Part XIII 16 , pages 311–327. Springer, 2020. 2
[47] Fabian Manhardt, Wadim Kehl, and Adrien Gaidon. Roi-
10d: Monocular lifting of 2d detection to 6d pose and metric
shape. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 2069–2078, 2019. 1
[48] Arsalan Mousavian, Dragomir Anguelov, John Flynn, and
Jana Kosecka. 3d bounding box estimation using deep learn-
ing and geometry. In Proceedings of the IEEE conference
on Computer Vision and Pattern Recognition , pages 7074–
7082, 2017. 2
[49] Alexander Quinn Nichol and Prafulla Dhariwal. Improved
denoising diffusion probabilistic models. In International
Conference on Machine Learning , pages 8162–8171. PMLR,
2021. 3
[50] Dennis Park, Rares Ambrus, Vitor Guizilini, Jie Li, and
Adrien Gaidon. Is pseudo-lidar needed for monocular 3d
object detection? In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 3142–3152,
2021. 1
[51] Liang Peng, Fei Liu, Zhengxu Yu, Senbo Yan, Dan Deng,
Zheng Yang, Haifeng Liu, and Deng Cai. Lidar point cloud
guided monocular 3d object detection. In European Confer-
ence on Computer Vision , pages 123–139. Springer, 2022.
6
[52] Liang Peng, Xiaopei Wu, Zheng Yang, Haifeng Liu, and
Deng Cai. Did-m3d: Decoupling instance depth for monoc-
ular 3d object detection. In European Conference on Com-
puter Vision , pages 71–88. Springer, 2022. 6
[53] Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encoding
images from arbitrary camera rigs by implicitly unproject-
10668
ing to 3d. In Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceed-
ings, Part XIV 16 , pages 194–210. Springer, 2020. 1
[54] Rui Qian, Divyansh Garg, Yan Wang, Yurong You, Serge
Belongie, Bharath Hariharan, Mark Campbell, Kilian Q
Weinberger, and Wei-Lun Chao. End-to-end pseudo-lidar
for image-based 3d object detection. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5881–5890, 2020. 2
[55] Zequn Qin and Xi Li. Monoground: Detecting monocular 3d
objects from the ground. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 3793–3802, 2022. 2
[56] Aimon Rahman, Jeya Maria Jose Valanarasu, Ilker Haci-
haliloglu, and Vishal M Patel. Ambiguous medical image
segmentation using diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 11536–11546, 2023. 3
[57] Yasiru Ranasinghe, Nithin Gopalakrishnan Nair, Wele
Gedara Chaminda Bandara, and Vishal M Patel. Diffuse-
denoise-count: Accurate crowd-counting with diffusion
models. arXiv preprint arXiv:2303.12790 , 2023. 2, 3
[58] Cody Reading, Ali Harakeh, Julia Chae, and Steven L
Waslander. Categorical depth distribution network for
monocular 3d object detection. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8555–8564, 2021. 5, 6
[59] Mengwei Ren, Mauricio Delbracio, Hossein Talebi, Guido
Gerig, and Peyman Milanfar. Image deblurring with
domain generalizable diffusion models. arXiv preprint
arXiv:2212.01789 , 2022. 2
[60] C ´edric Rommel, Eduardo Valle, Micka ¨el Chen, Souhaiel
Khalfaoui, Renaud Marlet, Matthieu Cord, and Patrick
P´erez. Diffhpe: Robust, coherent 3d human pose lifting with
diffusion. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 3220–3229, 2023. 2
[61] Fred Rothganger, Svetlana Lazebnik, Cordelia Schmid, and
Jean Ponce. 3d object modeling and recognition using lo-
cal affine-invariant image descriptors and multi-view spatial
constraints. International journal of computer vision , 66:
231–259, 2006. 2
[62] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems , 35:36479–36494, 2022. 2
[63] Xuepeng Shi, Qi Ye, Xiaozhi Chen, Chuangrong Chen,
Zhixiang Chen, and Tae-Kyun Kim. Geometry-based dis-
tance decomposition for monocular 3d object detection. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 15172–15181, 2021. 6
[64] Andrea Simonelli, Samuel Rota Bulo, Lorenzo Porzi,
Manuel L ´opez-Antequera, and Peter Kontschieder. Disen-
tangling monocular 3d object detection. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 1991–1999, 2019. 1[65] Andrea Simonelli, Samuel Rota Bulo, Lorenzo Porzi, Peter
Kontschieder, and Elisa Ricci. Are we missing confidence in
pseudo-lidar methods for monocular 3d object detection? In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3225–3233, 2021. 1
[66] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International confer-
ence on machine learning , pages 2256–2265. PMLR, 2015.
2, 3
[67] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In 9th International Con-
ference on Learning Representations, ICLR 2021, Virtual
Event, Austria, May 3-7, 2021 . OpenReview.net, 2021. 3,
5, 6
[68] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien
Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,
Yuning Chai, Benjamin Caine, et al. Scalability in perception
for autonomous driving: Waymo open dataset. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 2446–2454, 2020. 2, 5
[69] Li Wang, Liang Du, Xiaoqing Ye, Yanwei Fu, Guodong
Guo, Xiangyang Xue, Jianfeng Feng, and Li Zhang. Depth-
conditioned dynamic message propagation for monocular 3d
object detection. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages
454–463, 2021. 6
[70] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hari-
haran, Mark Campbell, and Kilian Q Weinberger. Pseudo-
lidar from visual depth estimation: Bridging the gap in 3d
object detection for autonomous driving. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8445–8453, 2019. 2
[71] Xinshuo Weng and Kris Kitani. Monocular 3d object de-
tection with pseudo-lidar point cloud. In Proceedings of
the IEEE/CVF International Conference on Computer Vision
Workshops , pages 0–0, 2019. 1, 2
[72] Di Wu, Zhaoyong Zhuang, Canqun Xiang, Wenbin Zou,
and Xia Li. 6d-vnet: End-to-end 6-dof vehicle pose es-
timation from monocular rgb images. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops , pages 0–0, 2019. 1
[73] Junde Wu, Rao Fu, Huihui Fang, Yu Zhang, Yehui Yang,
Haoyi Xiong, Huiying Liu, and Yanwu Xu. Medsegdiff:
Medical image segmentation with diffusion probabilistic
model. arXiv preprint arXiv:2211.00611 , 2022. 3
[74] Junkai Xu, Liang Peng, Haoran Cheng, Hao Li, Wei Qian,
Ke Li, Wenxiao Wang, and Deng Cai. Mononerd: Nerf-
like representations for monocular 3d object detection. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 6814–6824, 2023. 1, 2, 3, 5, 6, 7
[75] Zhenbo Xu, Wei Zhang, Xiaoqing Ye, Xiao Tan, Wei Yang,
Shilei Wen, Errui Ding, Ajin Meng, and Liusheng Huang.
Zoomnet: Part-aware adaptive zooming neural network for
3d object detection. In Proceedings of the AAAI Conference
on Artificial Intelligence , pages 12557–12564, 2020. 2
[76] Jieming Zhou, Tong Zhang, Zeeshan Hayder, Lars Petersson,
and Mehrtash Harandi. Diff3dhpe: A diffusion model for 3d
10669
human pose estimation. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 2092–
2102, 2023. 3
[77] Zheyuan Zhou, Liang Du, Xiaoqing Ye, Zhikang Zou, Xiao
Tan, Li Zhang, Xiangyang Xue, and Jianfeng Feng. Sgm3d:
Stereo guided monocular 3d object detection. IEEE Robotics
and Automation Letters , 7(4):10478–10485, 2022. 1
[78] Minghan Zhu, Songan Zhang, Yuanxin Zhong, Pingping Lu,
Huei Peng, and John Lenneman. Monocular 3d vehicle de-
tection using uncalibrated traffic cameras through homogra-
phy. In 2021 IEEE/RSJ International Conference on Intel-
ligent Robots and Systems (IROS) , pages 3814–3821. IEEE,
2021. 1
[79] Jiayu Zou, Zheng Zhu, Yun Ye, and Xingang Wang. Diffbev:
Conditional diffusion model for bird’s eye view perception.
arXiv preprint arXiv:2303.08333 , 2023. 4
[80] Zhikang Zou, Xiaoqing Ye, Liang Du, Xianhui Cheng, Xiao
Tan, Li Zhang, Jianfeng Feng, Xiangyang Xue, and Er-
rui Ding. The devil is in the task: Exploiting reciprocal
appearance-localization features for monocular 3d object de-
tection. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 2713–2722, 2021. 2
10670
