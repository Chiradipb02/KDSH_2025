MESA: Matching Everything by Segmenting Anything
Yesheng Zhang Xu Zhao⇤
Department of Automation, Shanghai Jiao Tong University
Abstract
Feature matching is a crucial task in the ﬁeld of com-
puter vision, which involves ﬁnding correspondences be-
tween images. Previous studies achieve remarkable perfor-
mance using learning-based feature comparison. However,
the pervasive presence of matching redundancy between im-
ages gives rise to unnecessary and error-prone computa-
tions in these methods, imposing limitations on their ac-
curacy. To address this issue, we propose MESA, a novel
approach to establish precise area (orregion ) matches for
efﬁcient matching redundancy reduction. MESA ﬁrst lever-
ages the advanced image understanding capability of SAM,
a state-of-the-art foundation model for image segmentation,
to obtain image areas with implicit semantic. Then, a multi-
relational graph is proposed to model the spatial structure
of these areas and construct their scale hierarchy. Based on
graphical models derived from the graph, the area match-
ing is reformulated as an energy minimization task and ef-
fectively resolved. Extensive experiments demonstrate that
MESA yields substantial precision improvement for multi-
ple point matchers in indoor and outdoor downstream tasks,
e.g.+13.61% for DKM in indoor pose estimation.
1. Introduction
Feature matching aims at establishing correspondences be-
tween images, which is vital in a broad range of applica-
tions, such as SLAM [ 5], SfM [ 39] and visual localiza-
tion [ 38]. However, achieving exact point matches is still
a signiﬁcant challenge due to the presence of matching
noises [ 33], including scale variations, viewpoint and illu-
mination changes, repetitive patterns, and poor texturing.
The underlying goal of feature matching is to ﬁnd corre-
spondences for points in the source image from a highly re-
dundant candidate set, i.e.all the points of the target image.
Therefore, feature matching essentially involves reducing
thematching redundancy , which is caused by troublesome
matching noises. Recent years have witnessed signiﬁcant
⇤Corresponding author: Xu Zhao.
Figure 1. The matching redundancy reduction in MESA.
High-level image understanding enables efﬁcient matching re-
dundancy reduction, allowing for precise point matching by dense
feature comparison . Therefore, MESA effectively reduces the
matching redundancy by area matching based on SAM [ 22] seg-
mentation, signiﬁcantly improving the accuracy of DKM [ 17].
advancements in learning-based methods for feature match-
ing. Especially the semi-dense [ 8,40] and dense meth-
ods [ 17], which search for matches densely by deep feature
comparison, obtain an impressive precision gap over sparse
methods [ 14]. However, as they heavily rely on dense fea-
ture comparison of the entire image, limited input resolution
and error-prone computation in matching-redundant parts
of images decrease their accuracy. Thus, effective matching
redundancy reduction is urgently required for them.
On the other hand, most matching redundancy can be ef-
fectively and efﬁciently identiﬁed through high-level image
understanding and only strongly correlated local areas (also
known as regions ) need dense feature comparison to deter-
mine precise matches. (Fig. 1). Therefore, recent meth-
ods [ 9,15] perform learning-based overlap segmentation
before matching. However, implicit learning needs non-
reusable computational overhead and the redundancy still
exists in the overlap. To address these issues, some work
turn to explicit semantic prior [ 18,52]. Unlike manually
specifying a topic number to group feature patches [ 18],
Area to Point Matching (A2PM) framework proposed by
SGAM [ 52] provides a more intuitive way to reduce match-
ing redundancy. Speciﬁcally, this framework establishes
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
20217
semantic area matches ﬁrst where matching redundancy is
largely removed, and then obtains accurate correspondences
within these areas utilizing an state-of-the-art point matcher.
Nevertheless, SGAM relies on semantic segmentation to
determine area matches. Its performance, thus, decreases
when encountering inexact semantic labeling and semantic
ambiguity [ 52]. Furthermore, the beneﬁts of A2PM can-
not be extrapolated to more general scenes due to the close-
set semantic labels. Hence, reducing matching redundancy
through semantic segmentation suffers from impracticality.
Recently, the Segment Anything Model (SAM) [ 22] has
gained notable attention from the research community due
to its exceptional performance and versatility, which has
the potential to be the basic front-end of many tasks [ 32,
49,50]. This suggests that the foundation model can accu-
rately comprehend image contents across various domains.
Taking inspiration from this, we realize that the image un-
derstanding from the foundation model can be leveraged to
reduce matching redundancy. Thus, we propose to establish
area matches based on SAM segmentation to overcome lim-
itations of SGAM [ 52]. Similar to the semantic segmenta-
tion, the image segmentation also provides multiple areas in
images, but without semantic labels attached to these areas.
However, the general object perception of SAM ensures that
its segmentation results inherently contain implicit seman-
tic information. In other words, a complete semantic en-
tity is always segmented as an independent area by SAM.
Hence, matching these areas effectively reduces matching
redundancy and promotes accurate point matching within
areas [ 52]. Furthermore, the absence of explicit semantics
alleviate issues of inaccurate area matching caused by er-
roneous labeling. The generic limitations due to semantic
granularity are also overcome. Nevertheless, area matching
can not be simply achieved by semantic labels but requires
other approaches under this situation.
In this work, we propose Matching Everything by Seg-
menting Anything (MESA, Fig. 2), a method for precise
area matching from SAM segmentation. MESA focuses on
two main aspects :constructing area relations andﬁnding
area matches based on these relations . Since SAM areas
only provide local information, matching them indepen-
dently can lead to inaccurate results, especially in scenes
with scale variation and repetitiveness. To address this,
MESA ﬁrst constructs a novel graph structure, named Area
Graph (AG), to model the global context of areas for sub-
sequent precise matching. The AG takes areas as nodes and
connects them with two types of edges: undirected edges
for adjacent areas and directed edges for areas that include
one another. Both edges capture global information, and
the latter also enables the construction of hierarchy struc-
tures in AG similar to [ 43] for efﬁcient matching. After-
wards, MESA performs area matching by deriving two dis-
tinct graphical models from the AG: Area Markov Ran-dom Field (AMRF) and Area Bayesian Network (ABN).
The AMRF involves global-informative edges in AG, thus
allowing global-consistent area matching through energy
minimization on the graph. The ABN, furthermore, is pro-
posed to facilitate the graph energy calculation, through the
hierarchy structure of AG. Speciﬁcally, the graph energy is
determined based on the similarity and spatial relation be-
tween areas, making this energy minimization effectively
solvable through the Graph Cut [23]. The area similar-
ity calculation is accomplished by a novel learning-based
model and accelerated by the ABN. Finally, we propose a
global matching energy to reﬁne accurate area matches, ad-
dressing the issue of multiple solutions in Graph Cut , lead-
ing to effective matching redundancy reduction.
Our method makes several contributions. 1)In pursuit
of effective matching redundancy reduction, we propose
MESA to achieve accurate area matching from results of
SAM, an advanced image segmentation foundation model.
MESA realizes the beneﬁt of A2PM framework in a more
practical manner, leading to precise feature matching. 2)
We introduce a multi-relational graph, AG, to model the
spatial structure and scale hierarchy of image areas, con-
tributing to outstanding area matching. 3)We propose
graphical area matching by converting AG into AMRF and
ABN. The AMRF is employed to formulate the area match-
ing as an energy minimization, which is effectively solvable
through the Graph Cut . The energy calculation is efﬁciently
performed by the property of ABN and the proposed learn-
ing area similarity model. 4)MESA is evaluated in indoor
and outdoor downstream tasks of feature matching, obtain-
ing remarkable improvements for multiple point matchers.
2. Related Work
Sparse, Semi-Dense and Dense Matching. There are three
types of feature matching methods: sparse, semi-dense and
dense. Classical sparse feature matching [ 31] involves de-
tecting and describing keypoints in images, followed by
keypoint matching. The learning counterpart of this frame-
work utilizes neural networks to perform feature detec-
tion [ 2,14], description [ 16,36,53] or matching [ 37,51].
To avoid the detection failure in sparse methods, semi-
dense methods [ 20,26,40] are proposed, also known as
the detector-free methods. These methods [ 8,42,43] per-
form dense feature matching over the entire image and then
select conﬁdent matches, which achieve impressive match-
ing precision. Dense matching methods [ 17,35,44] out-
put a dense warp with conﬁdence map for the image pair.
Recent DKM [ 17] reformulates this framework as a Gaus-
sian Process and achieves state-of-the-art performance. Our
method, however, focus on area matching between images,
which can be combined with both semi-dense and dense
point matching methods to increase their precision.
Matching Redundancy Reduction. Matching redundancy
20218
: Inclusion EdgeEach Source Area Node in Sepcific Level ��∗
Multi-Relational Area Graphs
Image SegmentationArea Bayesian NetworkArea Markov Random FieldSource Area Node
�푟푔 � �(�)�푟푔   �(�)Energy Minimization FormulationAMRF
Graph Cut Solution
Graph Energy Calculation���푟푔   ��Global Energy Minimization
Area to Point Matching
CNN
CNN
�0,1�1,0푆  �0,�1Learning Area Similarity CalculationSiamese
Area Image0Area Image1
XArea Matching
: Self Attention Layer
 Segment Anything Model
 Point MatcherPoint MatchingArea Matching Result1223
554
6789
CrossAttention: Area Node
�0�1�2�3�0�1�2�3: Adjacency EdgeNode EnergyEdge Energy�0�1��1
��1�푠푟 0
Figure 2. Overview of MESA. Based on ∂SAM segmentation , we ﬁrst construct ∑Area Graph s. Then the graph is turned to two graphical
models based on its two different edges. Through ∏Area Markov Random Field , area matching is formulated as an πEnergy Minimization .
Then, leveraging ∫Area Bayesian Network and our ∫Learning Area Similarity Calculation ,ªGraph Energy can be efﬁciently calculated.
Therefore, ºGraph Cut is utilized to obtain putative area matches. Finally, ΩGlobal Energy Minimization determines the best area match,
which serves as the input of subsequent point matcher for precise feature matching, following the æArea to Point Matching framework [ 52].
is evident in non-overlapping areas between images, moti-
vating several works [ 9,12,20] to focus on covisible areas
extraction. They predict overlaps between images by iter-
ative matching [ 20] or overlap segmentation [ 9,12]. Nev-
ertheless, matching redundancy still exists in the overlap-
ping area, when it comes to detailed point matching. Top-
icFM [ 18] proposes to divide image contents into topics and
then restrict matching to the same topic to avoid redundant
computation. The learning-based topic inference, however,
is implicit, suffering from lack of versatility and generalize
issues. SGAM [ 52] explicitly ﬁnds area matches between
images, which are fed into point matchers for redundancy
reduction. The Area to Point Matching framework is simple
yet effective. Our method further builds on its advantages.
Area to Point Matching. Establishing area matches is an
effective way to reduce the matching redundancy. Thus,
SGAM [ 52] proposes the Area to Point Matching (A2PM)
framework. This framework obtains area matches as the in-
put of point matchers, which possess higher resolution and
less matching noise than original images, contributing to ac-
curate point matching. However, SGAM heavily relies on
explicit semantic prior, leading to performance sensitivity
of area matching to semantic labeling precision and scene
semantic granularity. In contrast, MESA utilizes pure im-
age segmentation for area matching, which is more practical
and remedies drawbacks associated with explicit semantic.
3. Area Graph
In this section, we introduce the concept of an Area
Graph (AG) and explain how to construct the AG from the
SAM [ 22] results of an image. The main reason to adopt the
AG is that direct area matching on SAM results is unaccept-
able, as global information is ignored in independent areas.
Also, ﬁxed area sizes hinder robust point matching underscale changes. Therefore, AG is proposed to capture the
global structural information of these areas and construct
scale hierarchy for them, contributing to accurate and ro-
bust area matching.
3.1. Area Graph Deﬁnition
The AG ( G=hV,Ei) takes image areas as nodes and
contains two kinds of edges to model inter-area relations
(Fig. 3), thus making it a multi-relational graph [1]. The
graph nodes include both areas provided by SAM and ar-
eas generated by graph completion (cf. Sec. 3.2). We di-
vide these areas into Llevels according to their sizes, cor-
responding to different image scales, to serve as the foun-
dation of area scale hierarchy. On the other hand, the graph
edges ( E=EinSEadj) represent two relations between ar-
eas,i.e.inclusion ( Ein) and adjacency ( Eadj). The inclu-
sion edge ein2Einis directed, pointing from an area to
one of its containing areas. It forms a hierarchical con-
nection between graph nodes, enabling robust and efﬁcient
area matching especially under scale changes. The adja-
cency edge eadj2Eadjis undirected, indicating the areas
it connects share common parts but without the larger one
including the smaller one. This edge captures the spatial
relations between areas. By the above two edges, AG mod-
els both the spatial and scale structure of image areas, thus
contributing to exact area matching.
3.2. Area Graph Construction
The construction of AG includes collecting areas as nodes
and connecting them by proper edges. Firstly, not all SAM
areas can function as nodes, since some are too small or
have extreme aspect ratios, rendering them unsuitable for
point matching. Thus, Area Pre-processing is performed
ﬁrst to obtain initial graph nodes. We then approach the
20219
Figure 3. The proposed Area Graph. The graph nodes (circles
with masks representing rectangle areas) includes both areas from
SAM results (white boundaries) and our graph completion algo-
rithm (black boundaries). They are divided into four levels ac-
cording to their sizes. The adjacency edges (dashed lines) and
inclusion edges (arrows) connect all nodes. Only adjacency edges
within the same level are shown for better view.
edge construction as a Graph Link Prediction problem [ 34].
Afterwards, the preliminary AG is formed, but it still lacks
matching efﬁciency and scale robustness. Thus, we pro-
pose the Graph Completion algorithm, which generates ad-
ditional nodes and edges to construct the scale hierarchy.
Area Pre-processing. To ﬁlter unsuitable areas, we set two
criteria: the acceptable minimal area size ( Ts) and maxi-
mum area aspect ratio ( Tr). Any area that has smaller size
thanTsor larger aspect ratio than Tr, gets screened out. The
remaining areas are added into the candidate areas set. For
each ﬁltered area, we fuse it with its nearest neighbor area
in the candidate set. We repeat the ﬁltering and fusion on
the candidate set until no areas get screened out. Then, we
assign a level lato each candidate area abased on its size,
by setting Lsize thresholds ( {TLi  i2[0,L 1]}):
la=i  TLiWa⇥Ha<TL i+1. (1)
The size level is the basis of scale hierarchy in AG.
Graph Link Prediction. The edge construction is treated
as a link prediction problem [ 34]. Given two area nodes
(vi,vj), the edge between them ( eij) can be predicted ac-
cording to the spatial relation of their corresponding ar-
eas ( ai,aj). This approach adopts the ratio of the over-
lap size (Oij) tothe minimum area size between two areas
( =Oij/min( Wi⇥Hi,Wj⇥Hj)) as the score function:
eij28
><
>:Ein, >= h
Eadj, l< < h
?,  l, (2)
where  l, hare predeﬁned thresholds.
Graph Completion. Initial AG is achieved by connecting
all processed nodes with different edges. However, since
SAM inherently produces areas containing complete en-
tity, there are few inclusion relations among areas. Con-
sequently, initial AG lacks the scale hierarchy, which re-
duces its robustness at scale variations and makes access-
ing nodes inefﬁcient. To address the issue, we propose the
Graph Completion algorithm, which generates additional
nodes and edges to ultimately construct a tree structure in
the original graph. The core of this algorithm is to gen-
erate parent nodes for each orphan node, i.e.the node hasno parent node in the next higher level, from small to large
size levels. The algorithm begins at the smallest level, col-
lects all orphan nodes, and clusters them based on their cen-
ter coordinates. Nodes in the same cluster have their cor-
responding areas fused with their nearest neighbors. It is
noteworthy that these generated areas containing multiple
objects preserve the internal implicit semantic. Based on
our level thresholds, the resulting areas correspond to new
higher level nodes. If a node remains single after cluster-
ing, we increase its area size to the next level to allow for
potential parent nodes. We repeat the above operations on
each level and connect generated nodes to others by suitable
edges. More details can be found in the Supp. Mat.
4. Graphical Area Matching
In this section, we describe our area matching method,
which formulates area matching on the graph, utilizing
two graphical models derived from the AG. Given two
AGs ( G0,G1) of the input image pair ( I0,I1) and one area
(a0
src2I0) corresponding to node v0
src2G0(termed as
thesource node ), area matching involves ﬁnding the node
v1
j2G1with the highest probability of matching its corre-
sponding area a1
jto the source node area a0
src. However,
treating this problem as independent node matching is in-
adequate, as which disregards global information from ad-
jacency and inclusion relations between areas modeled in
AG by its two edges. Meanwhile, these two edges respec-
tively convert AG into two graphical models, i.e.Markov
Random Fields (undirected edges) and Bayesian Network
(directed edges). Thus, the area matching can be naturally
formulated inside a framework of graphical model.
4.1. Area Markov Random Field
The adjacency edge in AG implies the correlation between
probabilities of two nodes matching the source node. By
considering the general adjacency relation, which includes
the inclusion relations as adjacency too, the G1is trans-
formed into an undirected graph. Next, random variables
(x) are introduced for all nodes to indicate their matching
status with the source node. The binary variable xi2xis
equal to 1when v1
imatches v0
srcand0otherwise. There-
fore, Area Markov Random Field (AMRF, G1
M=hV,Eadji)
is obtained and area matching can be performed by max-
imizing the joint probability distribution over the AMRF:
arg max
xP(x). (3)
Based on the Hammersley-Clifford theorem [ 10], the prob-
ability distribution deﬁned by AMRF belongs to the Boltz-
mann distribution , which is an exponential of negative en-
ergy function ( P(x)=e x p (  E(x))). Therefore, the area
matching can be formulated as an energy minimization.
arg min
xE(x). (4)
20220
Figure 4. Learning area similarity. The area similarity calcu-
lation is formed as the patch-level classiﬁcation. We predict the
probability of each patch in one area appearing on the other to
construct activity maps. The similarity is obtained by the product
of activity expectations, contributing to our exact area matching.
The energy can be divided into two parts, i.e.the energy of
nodes ( EV) and edges ( EE), based on the graph structure.
E(x)=X
iEV(xi)+ X
(i,j)2NEE(xi,xj),(5)
where  is a parameter balancing the terms and Nis the set
of all pairs of neighboring nodes. For each graph node v1
i,
its energy is expected to be low when its matching probabil-
ity is high, which can be reﬂected by the apparent similarity
(Sa0srca1
i) between a0
srcanda1
i.
EV(xi)=|xi Sa0srca1
i|. (6)
The edge energy aims to penalize all neighbors with differ-
ent labels, and the Potts model [ 46](T) would be a justiﬁ-
able choice. To better reﬂect the spatial relation, the Potts
interactions are speciﬁed by IoU[47] of neighboring areas.
EE(xi,xj)=IoU(a1
i,a1
j)·T(xi6=xj). (7)
Function T(·)is1if the argument is true and 0otherwise.
Finally, the area matching is formulated as an binary label-
ing energy minimization. By carefully deﬁning the energy
function, the energy minimization problem in Eq. ( 4) is ef-
ﬁciently solvable via the Graph Cut algorithm [ 4,23]. The
obtained minimum cut of the graph G1
Mis the matched node
set ({v1
h  h2H}). Although the set may contain more
than one area node, the best matching result can be achieved
from this set by our reﬁnement algorithm (cf. Sec. 4.4).
4.2. Learning Area Similarity
The proposed Graph Cut solution relies on graph energy
calculations for both nodes and edges. Unlike easily avail-
able area pair IoUs for EE, determining the area apparent
similarity for EVis not straightforward. Thus, we turn to
the learning-based framework, inspired by recent successes
of learning models in point matching [ 8,21,40]. One sim-
ple idea is to calculate the correlation of learning descriptors
of two areas [ 21] as the area similarity. However, the de-
scriptor correlation is too rough for accurate area matchingand lacks ﬁne-grained interpretability. To overcome these
issues, we decompose the area similarity calculation into
two patch-level classiﬁcation problems as shown in Fig. 4.
Speciﬁcally, for each image in the area image pair
{aj  j2{0,1}}reshaped to the same size, we perform bi-
nary classiﬁcation for each 1/8·Waj⇥1/8·Hajimage
patch pj
i(where iis the index of patch and jis the index
of area image ) in it, computing the probability of pj
iap-
pearing on the other area image, termed as the patch ac-
tivity  j
i. To accomplish the classiﬁcation, we ﬁrst extract
patch-wise features from each area image using a Siamese
CNN [ 24]. Then we update these patch features via self and
cross-attention with normalization [ 45], resulting in patch
activities. Utilizing these patch activities, we construct an
activity map (  j
m={ j
i  j2{0,1}}i,where iis the index
of element in this set ) for each area image. When two areas
are ideally matched, the corresponding 3D point of every
pixel in one area is projected onto the other area. Hence,
all the patch activities of both areas should be closed to 1,
revealing the area similarity can be represented by the prod-
uct of expectations of two activity maps, EX( i
m)= i,j,
where i6=j,i, j2{0,1}.
Sim a0,a1=EX( 0
m)⇥EX( 1
m)= 0,1⇥ 1,0.(8)
The training details of this learning model are described in
the Supp. Mat.
4.3. Area Bayesian Network
Although the Graph Cut can be performed in polynomial
time [ 4], the dense energy calculation over G1
Mis time con-
suming. Furthermore, due to the scale hierarchy in AG,
this dense calculation is highly redundant. In particular, if
the area a0
srcis not matched to a1
j, it won’t be matched to
any children area of a1
j. This observation reveals the condi-
tional independence in the similarity calculation, which in-
volves inclusion edges in G1, thus turning G1to a Bayesian
Network ( G1
B)[3]. Therefore, the redundancy in the sim-
ilarity calculation can be reduced. In practice, we calcu-
late the dense similarities by constructing a similarity ma-
trixMS2R|V0|⇥|V1|. Note notall similarities in MSneed
calculation, but any similarity can be accessed in MS.W e
ﬁrst calculate similarities related to all source nodes. Sub-
sequent calculations are saved in MSas well. For MS[i, j]
that has not been calculated, we achieve it by our learning
model (cf. Sec. 4.2):
MS[i, j]=Sima0
i,a1
j. (9)
IfMS[i, j]<T as, all children nodes {v0
h  h2ch0(i)}and
{v1
c  c2ch1(j)}ofv0
iandv1
jare found from G0
BandG1
B,
where chj(i)is the index set of children indices of node vj
i
from Gj
B. Based on the conditional independence, we have:
20221
MS[h, k]=0 ,8(h, k)2ch0(i)⇥ch1(j). (10)
This operation effectively reduce the times of similarity cal-
culation, leading to more efﬁcient area matching.
4.4. Global Matching Energy Minimization
The minimum cut {v1
h  h2H}achieved through the Graph
Cutmay contain more than one area node, which means fur-
ther reﬁnement is necessary to obtain the best area match.
Moreover, the aforementioned graphical area matching, i.e.
ﬁnding the corresponding area node in G1forv0
src2G0,
only considers the structure information in G1and ignores
the structure of G0. To overcome this limitation, we propose
a global matching energy EGfor each candidate node v1
h,
which consists of four parts.
EG(v1
h)=1
Z(µ·Eself(v1
h)+↵·Eparent (v1
h)
+ ·Echildren (v1
h)+ ·Eneighbour (v1
h)),(11)
where µ,↵, and are weights to balance the terms; Zis
the partition function. The Eself(v1
h)is the energy related
to matching probability between v0
srcandv1
h:
Eself(v1
h)=|1 Sima0src,a1
h|. (12)
TheEparent (v1
h)is the energy related to matching probabil-
ity between the parent node pairs of v0
srcandv1
h:
Eparent (v1
h)=
min{|1 Sim a0u,a1r|  u2p0(src),r2p1(h)},(13)
where pi(j)is the index set of parent nodes of vi
jinGi.
This energy is the minimum matching energy among all
parent node pairs of v1
handv0
src. Same as the Eparent , the
Echildren andEneighbour are the energy parts of children
and neighbour node matching pairs. Afterwards, the best
area match v1
h⇤in the set can be found by minimize EG:
h⇤= arg min
h2HEG(v1
h). (14)
If the EG(v1
h⇤)>T Emax(a threshold parameter), the
source area node v0
srcis considered to have no matches. To
further improve the accuracy of ﬁnal match, we set an en-
ergy range threshold TErto collect all the candidates within
a certain energy range.
{v1
¯h  |EG(v1
¯h) EG(v1
h⇤)|TEr,¯h2H}. (15)
Then the ﬁnal area match is achieved by fusing v1
h⇤and all
candidates {v1
¯h}¯hwith EGas weights, similar to [ 21]. This
reﬁnement completely considers the structure information
of both G0andG1and achieves exact area matches.Method AOR "AMP@ 0.6"AMP@ 0.7"AMP@ 0.8"AreaNum "
SEEM [ 54]+SGAM [ 52]†52.96 49.39 29.88 12.32 2.66
w/GAM ( =0.3) 57.74 53.86 33.61 13.13 1.21
w/GAM ( =1.0) 60.59 59.43 36.29 15.27 1.55
w/GAM ( =5.0) 60.32 59.34 36.55 15.27 2.23
MESA 67.98 80.09 57.74 22.73 3.47
w/GAM ( =0.3) 68.99 81.11 60.75 25.77 1.97
w/GAM ( =1.0) 68.78 81.07 60.06 25.21 3.25
w/GAM ( =5.0) 68.51 80.84 59.22 23.87 3.35
†SGAM with only semantic area matching activated.
Table 1. Area matching results on ScanNet1500. We compare
the area matching performance between SGAM and MESA, com-
bined with GAM [ 52] under various  settings. Results are high-
lighted as ﬁrst,second and third for both series respectively.
5. Experiments
5.1. Area Matching
Since accurate area matching is the prerequisite for the pre-
cise point matching, we evaluate our method for this task on
ScanNet1500 [ 11] benchmark.
Experimental setup. TheTEmaxis0.35. The input size is
320⇥320for our learning model. Other scene-independent
parameter settings and implementation details can be found
in our Supp. Mat. The previous area matching approach,
SGAM [ 52] with SEEM [ 54] providing the semantic seg-
mentation input, is compared with our method MESA. The
impacts of Geometric Area Matching (GAM) in [ 52] to both
methods are also investigated, whose parameter  reﬂects
the strictness of outlier rejection. We employ the Area Over-
lap Ratio (AOR, %) and Area Matching Precision under
three thresholds (AMP@ 0.6/0.7/0.8, %) [ 52] as metrics.
The average area number per image (AreaNum) is reported.
Results. In Tab. 1, our MESA outperforms SGAM by
a large margin ( 67.98vs.60.59in AOR). The poor area
matching performance of SGAM is mainly caused by the
inaccurate semantic labeling of SEEM, which is avoided in
MESA. MESA can also achieve more areas than SGAM,
favouring downstream tasks by more point matches. Al-
though GAM brings signiﬁcant improvement for SGAM, it
achieves less enhancement on MESA, because more precise
area matching is obtained by MESA. We empirically adopt
GAM w/  =1.0for MESA in the following experiments.
5.2. Indoor Pose Estimation
MESA is also evaluated using the ScanNet1500 benchmark
for indoor pose estimation. The benchmark is challenging
due to the poor texture and large view variances in scenes.
Experimental setup. The parameters of MESA are the
same as our area matching experiment in Sec. 5.1.W e
adopt the A2PM framework [ 52] and combine four ﬁne-
tuned point matchers with our method, i.e. LoFTR [ 40],
ASpan [ 8], QuadT [ 43] and DKM [ 17], to demonstrate our
performance and compare with recent methods [ 6,19,21].
The cropped area images are resized to be square with the
same side length as the long side of the default size of point
20222
Pose estimation AUCScanNet1500 benchmarkAUC@5 "AUC@10 "AUC@20 "SP [14]+SGMNet [7]ICCV’2115.4 32.1 48.3SiLK [19]ICCV’2318.0 34.4 50.4PATS [21]CVPR’2326.0 46.9 64.3CasMTR [6]ICCV’2327.1 47.0 64.4ASpan [8]ECCV’2225.6 46.0 63.3SEEM [54]+SGAMASpan [52]arxiv’2327.5+7.42%48.0+4.35%65.3+3.16%MESAASpan27.5+7.42%48.2+4.35%65.0+2.69%QuadT [43]ICLR’2224.9 44.7 61.8SEEM [54]+SGAMQuadT [52]arxiv’2325.5+2.53%46.0+2.95%63.4+2.59%MESAQuadT28.7+15.26%46.5+4.03%61.9+0.16%LoFTR [40]CVPR’2122.1 40.8 57.6SEEM [54]+SGAMLoFTR [52]arxiv’2323.4+5.88%41.8+2.45%58.7+1.91%MESALoFTR22.9+3.62%41.8+2.45%58.4+1.39%DKM [17]CVPR’2329.4 50.7 68.3SEEM [54]+SGAMDKM [52]arxiv’2330.6+4.12%52.3+3.10%69.3+1.48%MESADKM33.4+13.61%55.0+8.48%72.0+5.42%Table 2. Relative pose estimation results (%) on ScanNet1500
benchmark. Thebestandsecond results are highlighted.
matchers. Following [ 40], the pose estimation AUC at var-
ious thresholds are used as metrics. More implementation
details can be found in our Supp. Mat.
Results. As we can see in Tab. 2, MESA signiﬁcantly
increases the pose accuracy for all baselines. Due to the
presence of more severe matching redundancy in indoor
scenes, our approach gains more noticeable enhancement,
e.g.+15.26% in AUC@ 5 for QuadT, compared to out-
door scenes in Tab. 3. The improvement obtained by
MESA for semi-dense matchers are comparable to those
of SGAM [ 52], because the coarse-to-ﬁne pipeline of these
methods makes them insensitive to AOR. However, MESA
brings a larger improvement for DKM [ 17] than SGAM
(13.61% vs.4.12%) and boosts the precision to a new state-
of-the-art , proving the precise area matching by MESA ben-
eﬁts the performance of dense matching.
5.3. Outdoor Pose Estimation
We use the Megadepth1500 [ 27] which consists of 1500
outdoor image pairs to evaluate the effectiveness of our
method for relative pose estimation in outdoor scenes.
Experimental setup. The parameter TEmaxis set as 0.3.
MESA is combined with four baselines to compare with re-
cent methods [ 18,29,48]. We also compare with another
covisible-area matching method, OETR [ 9], combined with
our post-processing. The area images are cropped and re-
sized as input of point matcher, same as the indoor experi-
ment. The pose estimation AUC is adopted as the metric.
Results. In Tab. 3, MESA achieves impressive precision
improvement for semi-dense baselines, up to +7.58% in
AUC@ 5 . The improvement for the dense method is not
as notable as that for semi-dense methods, because lots of
repetitiveness in MegaDepth leads to hard area matching,
decreasing the accuracy of DKM. However, MESA DKM
does set a new state-of-the-art for outdoor pose estima-
tion. MESA also achieves better results than OETR by ﬁnd-Pose estimation AUCMegaDepth1500 benchmark
AUC@5 "AUC@10 "AUC@20 "
LightGlue [ 29]ICCV’23 49.9 67.0 80.1
SiLK [ 19]ICCV’23 43.8 57.7 68.6
ASTR [ 48]CVPR’23 58.4 73.1 83.8
TopicFM [ 18]AAAI’23 54.1 70.1 81.6
CasMTR [ 6]ICCV’23 59.1 74.3 84.8
ASpan [ 8]ECCV’22 55.4 71.6 83.1
OETR [ 9]+ASpan AAAI’22 56.2 +1.44% 72.3 +0.98% 82.8  0.36%
MESA ASpan 58.4 +5.42% 74.1 +3.49% 84.8 +2.05%
QuadT [ 43]ICLR’22 54.6 70.5 82.2
OETR+QuadT AAAI’22 55.4 +1.47% 71.2 +0.99% 82.1  0.01%
MESA QuadT 57.9 +6.04% 73.1 +3.69% 83.9 +2.07%
LoFTR [ 40]CVPR’21 52.8 69.2 81.2
OETR+LoFTR AAAI’22 54.8 +3.79% 70.3 +1.59% 81.8 +0.73%
MESA LoFTR 56.8 +7.58% 72.2 +4.34% 83.3 +2.59%
DKM [ 17]CVPR’23 60.4 74.9 85.1
OETR+DKM AAAI’22 60.6 +0.33% 75.1 +0.27% 84.9  0.24%
MESA DKM 61.1 +1.16% 75.6 +0.93% 85.6 +0.59%
Table 3. Relative pose estimation results (%) on MegaDepth-
1500 benchmark. Thebestandsecond results are highlighted.
ing more detailed area matches, whereas OETR solely esti-
mates covisible areas.
5.4. Visual Odometry
We also evaluate our method on the visual odometry us-
ing the KITTI360 [ 28] dataset, which densely estimates the
camera motion in the self-driving scene. We select four se-
quences with few moving objects from the total dataset.
Experimental setup. The parameter TEmaxis set as 0.25.
The area image size is 480⇥480. Our method is combined
with four baselines [ 8,17,40,43] to demonstrate its effec-
tiveness. All baselines are trained on ScanNet [ 11]. Follow-
ing [25], we report the relative pose errors (RPE), including
the rotational error ( Rerr) and translation error ( terr).
Results Tab. 4shows that our method achieves promi-
nent precision improvement for DKM [ 17], ASpan [ 8] and
LoFTR [ 40] in all sequences. For QuadT [ 43], its perfor-
mances increases in most cases with the aid of MESA. Two
cases of accuracy declines may caused by its quadtree at-
tention mechanism [ 43], which evenly splits the area image
leading to destruction of semantic integrity, especially in the
driving scene with strong symmetry. Our method combined
with DKM achieves the best performance, proving the ben-
eﬁts of MESA for the dense matching method.
5.5. Ablation Study
To evaluate the effectiveness of our design, we conduct a
comprehensive ablation study for components of MESA.
Area Graph Construction. To justify the AG of MESA,
we adopt a naive approach to match areas, which is com-
paring area similarity densely (CSD). In particular, we ﬁrst
select areas with proper size from all SAM areas of two im-
20223
MethodSeq. 00 Seq. 02 Seq. 05 Seq. 06Rerr#terr#Rerr#terr#Rerr#terr#Rerr#terr#DKM [17]0.0460.5690.1020.6830.0480.6230.0500.533MESADKM0.0390.4990.0510.6110.0410.5460.0440.488ASpan [8]0.1522.4120.1492.3580.1542.5230.1461.887MESAASpan0.1302.0790.1412.3270.1312.2270.1321.671QuadT [43]0.1452.3070.1442.3220.1352.2690.1371.762MESAQuadT0.1342.3050.1402.4200.1232.1310.1351.915LoFTR [40]0.1322.0230.1241.9130.1171.9300.1261.572MESALoFTR0.1161.7030.1141.6580.1091.6540.1201.493Table 4. Visual odometry results on KITTI360. The original
results of baselines and better orworse results achieved by our
method are highlighted. All baselines are trained on ScanNet [ 11].
Method AOR "AMP@0.6 "PoseAUC@5 "AreaNum "
MESA ASpan ( Ours ) 72.75 89.09 27.50 3.47
w/CSD 69.23 84.21 26.78 2.73
w/DesSim. [21] 63.71 62.91 26.05 2.65
w/SEEMSeg. [54] 70.58 85.52 26.18 1.67
w/arg min Eself 70.98 87.56 26.96 2.90
Table 5. Ablation study. Three variants of MESA ASpan are
evaluated for area matching and pose estimation on the Scan-
Net1500 to demonstrate the importance of various components.
ages. The similarity of each area to all areas in the other
images is then calculated and area matches with the great-
est similarity is obtained. The comparison results are sum-
marised in Tab. 5. As AG can generate more proper areas
for matching, MESA w/ CSD gets less area matches. Thus,
the area and point matching performance is also decreased
by CSD. Moreover, CSD results in a signiﬁcant increase in
time of area matching ( ⇠⇥10slower than MESA), due to
its inefﬁcient dense comparison.
Area Similarity Calculation. In contrast to our classiﬁ-
cation formulation for area similarity calculation, another
straightforward method [ 21] involves calculating the dis-
tance between learning descriptors of areas. Thus, we
replace our learning similarity with descriptor similarity
in [21](DesSim ) and conduct experiments in ScanNet to in-
vestigate the impact. The results are summarized in Tab. 5,
including the area number per image, area matching and
pose estimation performance. Overall, the performance of
DesSim experiences a noticeable decline, due to poor area
matching precision, indicating the effectiveness and impor-
tance of proposed learning similarity calculation.
Image Segmentation Source. We relay on SAM to achieve
areas with implicit semantic, whose outstanding segmen-
tation precision and versatility contribute to our leading
matching performance. However, areas can also be ob-
tained from other segmentation methods. Therefore, to
measure the impact of different segmentation sources, we
exchange the segmentation input from SAM [ 22] with that
from SEEM [ 54](SEEMSeg. ) and evaluate the perfor-
mances. In Tab. 5, MESA with SEEMSeg. gets a slight pre-
cision decline and fewer areas compared with SAM, lead-
ing to decreased pose estimation results. These results in-
Figure 5. The qualitative comparison of Global Energy Reﬁne-
ment. As AG structures of both images are considered by EG,
objects with the same apparent can be distinguished according to
their neighbors, which are mismatched by arg min Eself, reveal-
ing the robustness of arg min EGunder repetitive patterns.
dicates that the advanced segmentation favors our methods.
Notably, MESA with SEEMSeg. also achieves slight im-
provement for ASpan, proving the effectiveness of MESA.
Global Energy Reﬁnement. After Graph Cut , the pro-
posed global matching energy for the ﬁnal area matching
reﬁnement considers structures of both AGs of the input im-
age pair. To show the importance of this dual-consideration,
we replace the global energy with naive Eselfin Eq. ( 12)
(arg min Eself) and evaluate the performance. In Tab. 5,
the reﬁnement relying on Eselfproduces decreased area
matching precision and a subsequent decline in pose esti-
mation performance, due to inaccurate area matches espe-
cially under repetitiveness. The qualitative results shown in
Fig.5further indicate the better robustness of global energy
under repetitiveness due to dual graph structure capture.
6. Conclusion
In this paper, we propose MESA as an approach for accu-
rate, robust, and practical area matching, aiming at effec-
tive feature matching redundancy reduction. To this end,
MESA ﬁrst utilizes the high-level image understanding ca-
pability of SAM, an advanced foundation model for image
segmentation, to obtain informative image areas. Then, we
propose a novel Area Graph (AG) to model the spatial struc-
ture and scale hierarchy of image areas. By formulating
the area matching on graphical models converted by AG,
we adopt the Graph Cut algorithm and a proposed learning
area similarity model to establish area matches. We further
consider the AG structures for both images to introduce the
global matching energy reﬁnement, which enables robust
and exact area matching, promoting precise point match-
ing. In extensive experiments, MESA achieves impressive
area matching accuracy and signiﬁcantly improves feature
matching performance for various point matching methods.
Acknowledgement. This work has been funded in part by
the NSFC grants 62176156, and SJTU Trans-med Awards
Research 20220101.
20224
References
[1]Ivana Balazevic, Carl Allen, and Timothy Hospedales.
Multi-relational poincar ´e graph embeddings. Advances in
Neural Information Processing Systems , 32, 2019. 3
[2]Axel Barroso-Laguna, Edgar Riba, Daniel Ponsa, and Krys-
tian Mikolajczyk. Key. net: Keypoint detection by hand-
crafted and learned cnn ﬁlters. Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 5836–
5844, 2019. 2
[3]Christopher M Bishop and Nasser M Nasrabadi. Pattern
recognition and machine learning . Springer, 2006. 5
[4]Y . Boykov, O. Veksler, and R. Zabih. Fast approximate en-
ergy minimization via graph cuts. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence , 23(11):1222–1239,
2001. 5
[5]Carlos Campos, Richard Elvira, Juan J G ´omez Rodr ´ıguez,
Jos´e MM Montiel, and Juan D Tard ´os. Orb-slam3: An accu-
rate open-source library for visual, visual–inertial, and mul-
timap slam. IEEE Transactions on Robotics , 2021. 1
[6]Chenjie Cao and Yanwei Fu. Improving transformer-based
image matching by cascaded capturing spatially informative
keypoints. ICCV , 2023. 6,7,3
[7]Hongkai Chen, Zixin Luo, Jiahui Zhang, Lei Zhou, Xuyang
Bai, Zeyu Hu, Chiew-Lan Tai, and Long Quan. Learn-
ing to match features with seeded graph matching network.
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 6301–6310, 2021. 7
[8]Hongkai Chen, Zixin Luo, Lei Zhou, Yurun Tian, Mingmin
Zhen, Tian Fang, David McKinnon, Yanghai Tsin, and Long
Quan. Aspanformer: Detector-free image matching with
adaptive span transformer. ECCV , pages 20–36, 2022. 1,
2,5,6,7,8,3
[9]Ying Chen and et al. Guide local feature matching by overlap
estimation. AAAI , 36(1):365–373, 2022. 1,3,7
[10] Peter Clifford. Markov random ﬁelds in statistics. Disorder
in physical systems: A volume in honour of John M. Ham-
mersley , pages 19–32, 1990. 4
[11] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nießner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes. Proc.
Computer Vision and Pattern Recognition (CVPR), IEEE ,
2017. 6,7,8,1,3,4
[12] Kun Dai, Tao Xie, Ke Wang, Zhiqiang Jiang, Ruifeng Li,
and Lijun Zhao. Oamatcher: An overlapping areas-based
network for accurate local feature matching. arXiv preprint
arXiv:2302.05846 , 2023. 3
[13] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan
Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner,
Joan Puigcerver, Robert Geirhos, Ibrahim M Alabdul-
mohsin, et al. Patch n’pack: Navit, a vision transformer for
any aspect ratio and resolution. Advances in Neural Infor-
mation Processing Systems , 36, 2024. 3
[14] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-
novich. Superpoint: Self-supervised interest point detec-
tion and description. Proceedings of the IEEE conference on
computer vision and pattern recognition workshops , pages
224–236, 2018. 1,2,7[15] Ying Chen Dihe Huang, Jianlin Liu Yong Liu, Wen-
long Wu Shang Xu, Fan Tang Yikang Ding, and Chengjie
Wang. Adaptive assignment for geometry aware local fea-
ture matching. CVPR , 2023. 1
[16] Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Polle-
feys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-net:
A trainable cnn for joint description and detection of local
features. Proceedings of the ieee/cvf conference on computer
vision and pattern recognition , pages 8092–8101, 2019. 2
[17] Johan Edstedt, Ioannis Athanasiadis, M ˚arten Wadenb ¨ack,
and Michael Felsberg. DKM: Dense kernelized feature
matching for geometry estimation. CVPR , 2023. 1,2,6,
7,8,3,4,5
[18] Khang Truong Giang, Soohwan Song, and Sungho Jo. Top-
icfm: Robust and interpretable topic-assisted feature match-
ing.AAAI , 37(2):2447–2455, 2023. 1,3,7
[19] Pierre Gleize, Weiyao Wang, and Matt Feiszli. Silk–simple
learned keypoints. ICCV , 2023. 6,7
[20] Wei Jiang, Eduard Trulls, Jan Hosang, Andrea Tagliasac-
chi, and Kwang Moo Yi. Cotr: Correspondence transformer
for matching across images. Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 6207–
6217, 2021. 2,3,4,7
[21] Yijin Li Junjie Ni, Hujun Bao Zhaoyang Huang, Hong-
sheng Li, and Guofeng Zhang Zhaopeng Cui. Pats: Patch
area transportation with subdivision for local feature match-
ing.CVPR , 2023. 5,6,7,8,4
[22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C. Berg, Wan-Yen Lo, Piotr Doll ´ar, and
Ross Girshick. Segment anything. ICCV , 2023. 1,2,3,
8,5
[23] Vladimir Kolmogorov and Ramin Zabin. What energy func-
tions can be minimized via graph cuts? IEEE transactions on
pattern analysis and machine intelligence , 26(2):147–159,
2004. 2,5
[24] Laura Leal-Taix ´e, Cristian Canton-Ferrer, and Konrad
Schindler. Learning by tracking: Siamese cnn for robust tar-
get association. In Proceedings of the IEEE conference on
computer vision and pattern recognition workshops , pages
33–40, 2016. 5
[25] Shenghao Li, Qunfei Zhao, and Zeyang Xia. Sparse-to-local-
dense matching for geometry-guided correspondence esti-
mation. IEEE Transactions on Image Processing , 32:3536–
3551, 2023. 7
[26] Xinghui Li, Kai Han, Shuda Li, and Victor Prisacariu. Dual-
resolution correspondence networks. Advances in Neural In-
formation Processing Systems , 33:17346–17357, 2020. 2
[27] Zhengqi Li and Noah Snavely. Megadepth: Learning single-
view depth prediction from internet photos. Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 2041–2050, 2018. 7,1
[28] Yiyi Liao, Jun Xie, and Andreas Geiger. Kitti-360: A novel
dataset and benchmarks for urban scene understanding in 2d
and 3d. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 2022. 7
20225
[29] Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Polle-
feys. LightGlue: Local feature matching at light speed.
ICCV , 2023. 7
[30] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. ICLR , 2019. 2
[31] David G Lowe. Distinctive image features from scale-
invariant keypoints. International journal of computer vi-
sion, 60(2):91–110, 2004. 2
[32] Jun Ma and Bo Wang. Segment anything in medical images.
arXiv preprint arXiv:2304.12306 , 2023. 2
[33] Jiayi Ma, Xingyu Jiang, Aoxiang Fan, Junjun Jiang, and
Junchi Yan. Image matching from handcrafted to deep fea-
tures: A survey. International Journal of Computer Vision ,
129(1):23–79, 2021. 1
[34] V´ıctor Mart ´ınez, Fernando Berzal, and Juan-Carlos Cubero.
A survey of link prediction in complex networks. ACM Com-
put. Surv. , 49(4), 2016. 4
[35] Iaroslav Melekhov, Aleksei Tiulpin, Torsten Sattler, Marc
Pollefeys, Esa Rahtu, and Juho Kannala. Dgc-net: Dense
geometric correspondence network. 2019 IEEE Winter Con-
ference on Applications of Computer Vision (WACV) , pages
1034–1042, 2019. 2
[36] Jerome Revaud, Cesar De Souza, Martin Humenberger, and
Philippe Weinzaepfel. R2d2: Reliable and repeatable detec-
tor and descriptor. Advances in neural information process-
ing systems , 32, 2019. 2
[37] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,
and Andrew Rabinovich. Superglue: Learning feature
matching with graph neural networks. Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , 2020. 2
[38] Paul-Edouard Sarlin, Ajaykumar Unagar, Mans Larsson,
Hugo Germain, Carl Toft, Viktor Larsson, Marc Pollefeys,
Vincent Lepetit, Lars Hammarstrand, Fredrik Kahl, et al.
Back to the feature: Learning robust camera localization
from pixels to pose. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
3247–3257, 2021. 1
[39] Johannes L Schonberger and Jan-Michael Frahm. Structure-
from-motion revisited. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
4104–4113, 2016. 1
[40] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and
Xiaowei Zhou. LoFTR: Detector-free local feature matching
with transformers. CVPR , 2021. 1,2,5,6,7,8,3,4
[41] MA Syakur, BK Khotimah, EMS Rochman, and Budi Dwi
Satoto. Integration k-means clustering method and elbow
method for identiﬁcation of the best customer proﬁle cluster.
InIOP conference series: materials science and engineer-
ing, page 012017. IOP Publishing, 2018. 1
[42] Dongli Tan, Jiang-Jiang Liu, Xingyu Chen, Chao Chen,
Ruixin Zhang, Yunhang Shen, Shouhong Ding, and Ron-
grong Ji. Eco-tr: Efﬁcient correspondences ﬁnding via
coarse-to-ﬁne reﬁnement. European Conference on Com-
puter Vision , pages 317–334, 2022. 2
[43] Shitao Tang, Jiahui Zhang, Siyu Zhu, and Ping Tan. Quadtree
attention for vision transformers. ICLR , 2022. 2,6,7,8[44] Prune Truong, Martin Danelljan, and Radu Timofte. Glu-
net: Global-local universal network for dense ﬂow and cor-
respondences. Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 6258–6268,
2020. 2
[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 5
[46] Fa-Yueh Wu. The potts model. Reviews of modern physics ,
54(1):235, 1982. 5
[47] Jiahui Yu, Yuning Jiang, Zhangyang Wang, Zhimin Cao, and
Thomas Huang. Unitbox: An advanced object detection net-
work. In Proceedings of the 24th ACM international confer-
ence on Multimedia , pages 516–520, 2016. 5
[48] Jiahuan Yu, Jiahao Chang, Jianfeng He, Tianzhu Zhang,
Jiyang Yu, and Wu Feng. ASTR: Adaptive spot-guided trans-
former for consistent local feature matching. CVPR , 2023.
7
[49] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin
Jin, Wenjun Zeng, and Zhibo Chen. Inpaint anything:
Segment anything meets image inpainting. arXiv preprint
arXiv:2304.06790 , 2023. 2
[50] Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim,
Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong.
Faster segment anything: Towards lightweight sam for mo-
bile applications. arXiv preprint arXiv:2306.14289 , 2023.
2
[51] Jiahui Zhang, Dawei Sun, Zixin Luo, Anbang Yao, Lei
Zhou, Tianwei Shen, Yurong Chen, Long Quan, and Hon-
gen Liao. Learning two-view correspondences and geometry
using order-aware network. ICCV , 2019. 2
[52] Yesheng Zhang, Xu Zhao, and Dahong Qian. Searching
from area to point: A hierarchical framework for semantic-
geometric combined feature matching. arXiv , 2023. 1,2,3,
6,7,5
[53] Xiaoming Zhao, Xingming Wu, Jinyu Miao, Weihai Chen,
Peter CY Chen, and Zhengguo Li. Alike: Accurate and
lightweight keypoint detection and descriptor extraction.
IEEE Transactions on Multimedia , 2022. 2
[54] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li,
Jianfeng Gao, and Yong Jae Lee. Segment everything every-
where all at once. Advances in neural information processing
systems , 2023. 6,7,8,5
20226
