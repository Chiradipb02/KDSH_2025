Friendly Sharpness-Aware Minimization
Tao Li1Pan Zhou2Zhengbao He1Xinwen Cheng1Xiaolin Huang1
1Department of Automation, Shanghai Jiao Tong University, Shanghai, China
2Singapore Management University, Singapore
{li.tao,lstefanie,xinwencheng,xiaolinhuang }@sjtu.edu.cn panzhou@smu.edu.sg
Abstract
Sharpness-Aware Minimization (SAM) has been instru-
mental in improving deep neural network training by min-
imizing both training loss and loss sharpness. Despite the
practical success, the mechanisms behind SAM’s general-
ization enhancements remain elusive, limiting its progress
in deep learning optimization. In this work, we investigate
SAM’s core components for generalization improvement
and introduce “Friendly-SAM” (F-SAM) to further enhance
SAM’s generalization. Our investigation reveals the key
role of batch-specific stochastic gradient noise within the
adversarial perturbation, i.e., the current minibatch gra-
dient, which significantly influences SAM’s generalization
performance. By decomposing the adversarial perturba-
tion in SAM into full gradient and stochastic gradient noise
components, we discover that relying solely on the full gra-
dient component degrades generalization while excluding it
leads to improved performance. The possible reason lies
in the full gradient component’s increase in sharpness loss
for the entire dataset, creating inconsistencies with the sub-
sequent sharpness minimization step solely on the current
minibatch data. Inspired by these insights, F-SAM aims to
mitigate the negative effects of the full gradient component.
It removes the full gradient estimated by an exponentially
moving average (EMA) of historical stochastic gradients,
and then leverages stochastic gradient noise for improved
generalization. Moreover, we provide theoretical valida-
tion for the EMA approximation and prove the convergence
of F-SAM on non-convex problems. Extensive experiments
demonstrate the superior generalization performance and
robustness of F-SAM over vanilla SAM. Code is available
athttps://github.com/nblt/F-SAM .
1. Introduction
Deep neural networks (DNNs) have achieved remarkable
performance in various vision and language processing
tasks [1–3]. A critical factor contributing to their success
is the choice of optimization algorithms [4–9], designed to
SAM SAM-full SAM-noise777879808182T est accuracy (%)80.88
78.0281.21ResNet-18 on CIFAR-100Figure 1. Investigation on SAM’s adversarial perturbation direc-
tion. We decompose the minibatch gradient ∇BL(w)into two
components: the full gradient component and the remaining batch-
specific stochastic gradient noise. Solely using the full gradient
component leads to a dramatic generalization degradation, while
only using the noise component enhances the generalization.
efficiently optimize DNN model parameters. To achieve
better performance, it is often desirable for an optimizer
to converge to flat minima, characterized by uniformly low
loss values, as opposed to sharp minima with abrupt loss
changes, as the former typically results in better general-
ization [10–13]. However, the complex landscape of over-
parameterized DNNs, featuring numerous sharp minima,
presents challenges for optimizers in real applications.
To address this challenge, Sharpness-Aware Minimiza-
tion (SAM) [14] is proposed to simultaneously minimize
the training loss and the loss sharpness, enabling it to
identify flat minima associated with improved generaliza-
tion performance. For each training iteration, given the
minibatch training loss LB(w)parameterized by w, SAM
achieves this by adversarially computing a adversarial per-
turbation ϵsto maximize the training loss LB(w+ϵs). It
then minimizes the loss of this perturbed objective via one
updating step of a base optimizer, such as SGD [4]. In
practice, to efficiently compute ϵs, SAM takes a linear ap-
proximation of the loss objective, and employs the mini-
batch gradient ∇LB(w)as the accent direction for search-
ingϵs=ρ·norm (∇LB(w)), where norm (x) =x/∥x∥2
andρ >0is a radius. This approach enables SAM to seek
models situated in neighborhoods characterized by consis-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
5631
tently low loss values, resulting in better generalization
when used in conjunction with base optimizers such as SGD
and Adam [5].
Despite SAM’s practical success, the underlying mech-
anism responsible for its empirical generalization improve-
ments remains limited [15, 16]. This open challenge hin-
ders developing new and more advanced deep learning op-
timization algorithms in a principle way. To address this
issue, we undertake a comprehensive exploration of SAM’s
core components contributing to its generalization improve-
ment. Subsequently, we introduce a new variant of SAM,
which offers a simple yet effective approach to further en-
hance the generalization performance in conjunction with
popular base optimizers.
First, our investigations reveal that the batch-specific
stochastic gradient noise present in the minibatch gra-
dient∇LB(w)plays a crucial role in SAM’s general-
ization performance. Specifically, as shown in Fig. 1,
we decompose the perturbation direction ∇LB(w)into
two orthogonal components: the full gradient compo-
nent Proj∇L(w)∇LB(w), representing the projection of
∇LB(w)onto the full gradient direction ∇L(w), and the
stochastic gradient noise component Proj⊤
∇L(w)∇LB(w),
denoting the residual projection. Surprisingly, we empiri-
cally find that as shown in Fig. 1, using only the full gradient
component for perturbation significantly degrades SAM’s
generalization, which is also observed by [15]. Conversely,
excluding the full gradient component leads to improved
generalization performance. This observation suggests that
the effectiveness of SAM primarily arises from the pres-
ence of the stochastic gradient noise component within the
minibatch gradient ∇LB(w). The reason behind this phe-
nomenon may be that the full gradient component in the per-
turbation increases the sharpness loss of the entire dataset,
leading to inconsistencies with the subsequent sharpness
minimization step which only uses the current minibatch
data to minimize training loss value and its sharpness. See
more discussion in Sec. 4.
Next, building on this insight, we introduce a straight-
forward yet highly effective modification to SAM, referred
to as “Friendly-SAM” or F-SAM. F-SAM mitigates the
undesirable effects of the full gradient component within
the minibatch gradient and leverages the beneficial role of
batch-specific stochastic gradient noise to further enhance
SAM’s generalization performance. Since removing the full
gradient component in the perturbation step can relieve the
adversarial perturbation impact on other data points except
for current minibatch data, the adversarial perturbation in
F-SAM is more “friendly” to other data points compared
with vanilla SAM, which helps the adversarial perturba-
tion and the following minimization perform on the same
minibatch data and improves sharpness minimization con-
sistency. Moreover, such friendliness enables F-SAM to beless sensitive to the choice of perturbation radius as dis-
cussed in Sec. 6.3. For efficiency, F-SAM approximates the
computationally expensive full gradient through an expo-
nentially moving average (EMA) of the historical stochastic
gradients and computes the stochastic gradient noise as the
perturbation direction. Moreover, we also provide theoreti-
cal validation of the EMA approximation to the full gradient
and prove the convergence of F-SAM on non-convex prob-
lems. Our main contribution can be summarized as follows:
• We take an in-depth investigation into the key component
in SAM’s adversarial perturbation and identify that the
stochastic gradient noise plays the most significant role.
• We propose a novel variant of SAM called F-SAM by
eliminating the undesired full gradient component and
harnessing the beneficial stochastic gradient noise for ad-
versarial perturbation to enhance generalization.
• Extensive experimental results demonstrate that F-SAM
improves SAM’s generalization, aids in training, and ex-
hibits better robustness across different perturbation radii.
2. Related Work
Sharp Minima and Generalization. The connection be-
tween the flatness of local minima and generalization has
received a rich body of studies [10–13, 17–19]. Recently,
many works have tried to improve the model generaliza-
tion by seeking flat minima [14, 17, 20–25]. For example,
Chaudhari et al. [17] proposed Entropy-SGD that actively
searches for flat regions by minimizing local entropy. Wen
et al. [20] designed SmoothOut framework to smooth out
the sharp minima and obtain generalization improvements.
Notably, sharpness-aware minimization (SAM) [14] pro-
vides a generic training scheme for seeking flat minima by
formulating the optimization as a min-max problem and en-
courage parameters sitting in neighborhoods with uniformly
low loss, achieving state-of-the-art generalization improve-
ments across various tasks. Later, a line of works improves
SAM from the perspective of the neighborhood’s geomet-
ric measure [26, 27], surrogate loss function [28], friendly
adversary [29], and training efficiency [30–34].
Understanding SAM. Despite the empirical success of
SAM across various tasks, a deep understanding of its gen-
eralization performance is still limited. Foret et al. [14]
explained the success of SAM via using a PAC-Bayes gen-
eralization bound. However, they did not analyze the key
components of SAM that contributes to its generalization
improvement. Andriushchenko et al. [15] investigated the
implicit bias of SAM for diagonal linear networks. Wen et
al. [35] demonstrated that SAM minimizes the top eigen-
value of Hessian in the full-batch setting and thus improves
the flatness of the minima. Chen et al. [16] studied SAM
on the non-smooth convolutional ReLU networks, and ex-
plained its success because of its ability to prevent noise
5632
SGD SAM SAM-full96.096.296.496.696.8T est accuracy (%)ResNet-18 on CIFAR-10
AdamW SAM SAM-full92.692.893.0Deit-S on OxfordIIITPet
SGD SAM SAM-full78.079.581.0T est accuracy (%)ResNet-18 on CIFAR-100
AdamW SAM SAM-full73.073.574.074.575.0Deit-S on Flower-102(a)
SGD SAMSAM-dbSAM-noise96.096.5T est accuracy (%)ResNet-18 on CIFAR-10
SGD SAMSAM-dbSAM-noise95.095.5VGG16-BN on CIFAR-10
SGD SAMSAM-dbSAM-noise78.079.581.0T est accuracy (%)ResNet-18 on CIFAR-100
SGD SAMSAM-dbSAM-noise75.576.076.5VGG16-BN on CIFAR-100 (b)
2 4 6 8 10
Perturbation step batch size (x128)79.580.080.581.0T est accuracy(%)
ResNet-18 on CIFAR-100 (c)
Figure 2. Performance comparison of different versions of SAM with SGD/AdamW as its base optimizer. In (a), SAM-full denotes
the version of SAM using the full gradient component as the perturbation. In (b), SAM-db represents the SAM using different random-
selected data batch for perturbation and its following minimization step. (c) compares SAM using different minibatch size to compute the
perturbation but always fixing minibatch size of 128 for the following minimization step.
learning. While previous works primarily focus on simpli-
fied networks or ideal loss objectives, we aim to deepen the
understanding of SAM by undertaking a deep investigation
into the key components that contribute to its practical gen-
eralization improvement.
3. Preliminaries
SAM. Letf(x;w)parameterized by w∈Rdbe a neu-
ral network, and L(f(xi;w),yi)(Li(w)for short) denote
the loss to measure the discrepancy between the prediction
f(xi;w)and the ground-truth label yi. Given a dataset
S= (xi,yi)n
i=1i.i.d. drawn from a data distribution D, the
empirical training loss is often defined as
L(w) =1
nXn
i=1Li(w). (1)
To solve this problem, one often uses SGD or Adam to opti-
mize it. To further improve the generalization performance,
SAM [14] aims to minimize the worst-case loss within a de-
fined neighborhood for guiding the training process towards
flat minima. The objective function of SAM is given by:
LSAM(w) = max ∥ϵ∥2≤ρL(w+ϵ), (2)
where ρdenotes the neighborhood radius. Eqn. (2) shows
that SAM seeks to minimize the maximum loss over the
neighborhood surrounding the current weight w. By doing
so, SAM encourages the optimization process to converge
to flat minima which often enjoys better generalization per-
formance [10–13].
To efficiently optimize LSAM(w), SAM first approxi-
mates Eqn. (2) via first-order expansion and compute the
adversarial perturbation ϵsas follows:
ϵs≈arg max∥ϵ∥2≤ρϵ⊤∇wL(w) =ρ·norm (∇wL(w)).
(3)where norm (x) =x/∥x∥2. Subsequently, one can com-
pute the gradient at the perturbed point w+ϵs, and then use
the updating step of the base SGD optimizer to update
wt+1=wt−γ∇LB(w)|wt+ϵt. (4)
Other base optimizers, e.g., Adam, can also be used to up-
date the model parameters in Eqn. (4).
Assumptions. Before delving into our analysis, we first
make some standard assumptions in stochastic optimization
[15, 34, 36, 37] that will be used in our theoretical analysis.
Assumption 1 (β-smoothness) .Assume the loss function
L:Rd7→Rto be β-smooth. There exists β >0such that
∥∇L(w)− ∇L(v)∥2≤β∥w−v∥2,∀w,v∈Rd.(5)
Assumption 2 (Bounded variance) .There exists a constant
M > 0for any data batch Bsuch that
E
∥∇LB(w)− ∇L(w)∥2
2
≤M,∀w∈Rd. (6)
Assumption 3 (Bounded gradient) .There exists G >0for
any data batch Bsuch that
E[∥∇LB(w)∥2]≤G,∀w∈Rd. (7)
4. Empirical Analysis of SAM
Here we conduct a set of experiments to identify the ef-
fective components in the adversarial perturbation that con-
tributes to SAM’s improved generalization performance.
To this end, we decompose SAM’s perturbation direction
∇LB(w)in Eqn. (3) into two orthogonal components. The
first one is the full gradient component Proj∇L(w)∇LB(w)
which is the projection onto the full gradient direction, i.e.,
Proj∇L(w)∇LB(w) = cos( ∇L(w), LB(w))∇L(w),
5633
where cos(·,·)denotes the cosine similarity function. An-
other one is the residual projection, i.e., the batch-specific
stochastic gradient noise component which is defined as
Proj⊤
∇L(w)∇LB(w) =∇LB(w)−Proj∇L(w)∇LB(w).
(8)
In the following, we will investigate the effects of these two
orthogonal components to the performance of SAM.
4.1. Effect of Full Gradient Component
Here we investigate the impact of the full gradient compo-
nent on the generalization performance of SAM. Specifi-
cally, for each training iteration, we set the perturbation ϵs
asϵs=ρ·norm (Proj∇L(w)∇LB(w))where norm (x) =
x/∥x∥2, and then follow the minimization step in vanilla
SAM to minimize the perturbed training loss. For brevity,
this modified SAM version is called “SAM-full” since it
uses the full gradient direction as the perturbation direc-
tion. Then we follow the standard training settings of SAM
and compare the performance of SGD/AdamW, SAM, and
SAM-full in training from scratch and transfer learning
tasks. See more training details in Appendix A5.
Fig. 2a (a) summarizes the results. One can observe that
SAM significantly outperforms the base optimizer in most
scenarios, except for the Flower-102 dataset [38], where the
training data are very limited (see Sec. 6.2 for more dis-
cussion). However, as shown in Fig. 2a (a), SAM-full in-
deed impairs the performance of base optimizers, such as
SGD base optimizer on the CIFAR-10, CIFAR-100 [39],
and OxFordIIITPet datasets [40], and even severely hurt
the performance of AdamW base optimizer on the Flower-
102 dataset, of which the training sample number is small.
This implies that 1) utilizing the full gradient compo-
nentProj∇L(w)∇LB(w)as the perturbation direction ac-
tually does not improve generalization and can even have
negative effects; 2) the effectiveness of SAM primarily
arises from the presence of stochastic gradient noise com-
ponent Proj⊤
∇L(w)∇LB(w)within the minibatch gradient
∇LB(w)in adversarial perturbation.
To provide more evidence, we further modify the per-
turbation step in SAM by increasing the minibatch size in
Eqn. (3) to compute the adversarial perturbation . As illus-
trated in Fig. 2c, we use a minibatch B′with size k×128
to compute the perturbation in SAM by following Eqn. (3),
where kis selected from {1,2,3,···,10}. However, we
maintain the use of a minibatch Bof size 128 for the second
minimization step of standard SAM in Eqn. (4). Here we
ensure the sample set Bin the second minimization belong
to the sample set B′in the first perturbation step, namely,
B ⊆ B′, which allows to better observe the effects of large
minibatch to the performance of SAM. For the perturbation
ϵs=ρ·norm∇LB′(w), its direction gradually aligns more
closely with the full gradient direction along with incrementofk. Consequently, as the minibatch size grows, the con-
tribution of the full gradient component in the perturbation
step becomes stronger, while the residual stochastic gradi-
ent noise component weakens. This analysis helps to re-
veal the perturbation effects of the full gradient direction to
SAM’s performance.
By observing Fig. 2c, one can find that when mini-
batch size of B′in the first adversarial perturbation step
increases, the classification accuracy of SAM gradually de-
creases when using ResNet18 on CIFAR-100 dataset. See
more similar experimental results in Appendix A5. All
these results together indicate that strengthening the full
gradient component in the perturbation step indeed impairs
the generalization performance of vanilla SAM.
4.2. Effect of Stochastic Gradient Noise Component
To delve into the role of the batch-specific stochastic gra-
dient noise in the generalization improvement of SAM,
we first replace the stochastic gradient noise component of
the current minibatch data with another random minibatch.
That is, for each training iteration, we use two different
minibatch data of the same size to do the first adversarial
perturbation step (3) and the second minimization step (4).
We term this modified version “SAM-db”. As illustrated in
Fig. 2b, we observe that for the four training settings, SAM-
db with SGD as its base optimizer often does not make sig-
nificant improvements in generalization compared to SGD.
Note that the adversarial perturbation in SAM-db contains
a similar full gradient component as vanilla SAM (in terms
of expectation), but the key difference lies in replacing the
stochastic gradient noise component from the current mini-
batch data with another random minibatch. However, this
substitution ultimately hinders generalization. Therefore,
we conclude that the stochastic gradient noise associated
with the minibatch in decent step in perturbation plays a
pivotal role in improving the generalization of SAM.
To further solidify this observation, we modify SAM by
setting the adversarial perturbation ϵsin vanilla SAM as
ϵs=ρ·norm (Proj⊤
∇L(w)∇LB(w)). Note, in this mod-
ification, we compute the full gradient for each iteration
and then follow Eqn. (8) to calculate the stochastic gradi-
ent noise Proj⊤
∇L(w)∇LB(w). We refer to this modified
version as “SAM-noise”. As illustrated in Fig. 2b, one can
observe that SAM-noise not only restores the generalization
performance of SAM but even exhibits a notable enhance-
ment. This result highlights that exclusively utilizing the
stochastic gradient noise component as SAM’s perturbation
can further enhance its generalization ability, thereby inspir-
ing our F-SAM algorithm in the next section.
5. The F-SAM Algorithm
In this section, we will present our proposed F-SAM algo-
rithm. We begin by introducing the efficient estimation of
5634
the stochastic gradient noise component. We then describe
the algorithmic steps of our proposed F-SAM. Finally, we
formulate the loss objective of our F-SAM algorithm and
highlight the distinctions from vanilla SAM.
5.1. Estimation of Stochastic Gradient Noise Com-
ponent
From the empirical results in Sec. 4, we know that
the full gradient component Proj∇L(w)∇LB(w)in the
perturbation impairs the performance of SAM, while
the batch-specific stochastic gradient noise component
Proj⊤
∇L(w)∇LB(w)is essential to improve performance.
Accordingly, it is natural to remove the full gradient compo-
nent and only use the stochastic gradient noise component.
To this end, we first reformulate the batch-specific
stochastic gradient noise component in Eqn. (8) as
Proj⊤
∇L(w)∇LB(w)=∇LB(w)−σ∇L(w).(9)
where σ= cos( ∇L(w),∇LB(w)).From Eqn. (9), one can
observe that to compute Proj⊤
∇L(w)∇LB(w), we need to
compute the minibatch gradient ∇LB(w)and also the full
gradient ∇L(w)at each training iteration. For ∇LB(w), it
is also computed in vanilla SAM and thus does not bring
extra computation overhead. Regarding the full gradient
∇L(w), it is computed on the whole dataset, and thus is
computationally prohibitive in practice.
To address this issue, we resort to estimating ∇L(w)
with an exponentially moving average (EMA) which accu-
mulates the historical minibatch gradients as follows:
mt=λmt−1+ (1−λ)∇LBt(wt), (10)
where λ >0is a hyper-parameter. This approach enables
us to approximate the full gradient ∇L(wt)with minimal
additional computational overhead. We also prove that mt
is a good estimation to the full gradient ∇L(wt)as shown
in following Theorem 1 with its proof in Appendix A1.
Theorem 1. Suppose Assumption 1, 2, and 3 hold. Assume
that SAM uses SGD as the base optimizer with a learning
rateγto update the model parameter in Eqn. (4). Then
by setting λ= 1−Cγ2/3, after T > C′γ−2/3training
iterations, with probability 1−δ, we have
ΦT=∥mT− ∇L(wT)∥2≤ O 
γ1
3β1
3G1
3M1
3log 1
δ
,
where CandC′are two universal constants.
Theorem 1 shows that the error bound Φtbetween the
full gradient ∇L(wt)and its EMA estimation mtis at the
order of O(γ1/3). On the non-convex problem, learning rate
γis often set as O(1/√
T)to ensure convergence as shown
in our Theorem 2 in Section 5.4. This implies that the error
bound ΦtisO(T−1/6)and thus is small since the trainingiteration Tis often large. So mtis a good estimation the
full gradient ∇L(wt).
In this way, the stochastic gradient noise component in
Eqn. (8) is approximated as
Proj⊤
∇L(wt)∇LB(wt)≈gProj⊤
∇L(wt)∇LB(wt),(11)
where gProj⊤
∇L(wt)∇LB(wt) =∇LB(wt)−σtmtand
σt= cos( mt,∇LB(wt)).In the real network training, the
training frameworks, e.g., PyTorch, often uses propagation
to update mtand∇LB(wt)from layer to layer. So one
needs to wait until the training framework finishes the up-
dating of all layers, and then compute σtwhich is not effi-
cient, especially for modern over-parameterized networks.
Moreover, both mtand∇LB(wt)are high dimensional,
their cosine computation can be unstable due to the possible
noises in the approximation mt. So to improve the training
efficiency and stability, we set σtas a constant σ, and obtain
gProj⊤
∇L(wt)∇LB(wt) =∇LB(wt)−σmt. (12)
The experimental results in Section 6 show that this practi-
cal setting achieves satisfactory performance. In theory, we
also prove that this practical setting is validated as shown in
the following Lemma 1 with its proof in Appendix A2.
Lemma 1. Assuming mtis a unbiased estimator to the full
gradient ∇L(wt). With σ= 1, we have
ED
gProj⊤
∇L(wt)∇LB(wt),∇L(wt)E
= 0. (13)
Lemma 1 shows that gProj⊤
∇L(wt)∇LB(wt)is orthogo-
nal to the full gradient ∇L(wt)in expectation when σ= 1.
So using gProj⊤
∇L(wt)∇LB(wt)as the perturbation direc-
tion can remove the effects of the full gradient ∇L(wt). As
Theorem 1 shows that mtis good estimation to full gradient
∇L(wt), the assumption in Lemma 1 is not restrictive.
5.2. Algorithmic Steps of F-SAM
After estimating the stochastic gradient noise component
in Eqn. (12), one can use it to replace the adversarial per-
turbation direction in vanilla SAM, and straightforwardly
develop our proposed F-SAM. Specifically, for the t-th
training iteration, following the algorithmic steps in vanilla
SAM, we first define the perturbation ϵas follows:
ϵt=ρ·norm gProj⊤
∇L(wt)∇LB(wt)
, (14)
where ρis the radius. Then, following SAM, we use SGD as
its base optimizer, and updates the model parameter wtby
using Eqn. (4) in Section 3. We summarize the algorithmic
steps of F-SAM with SGD as base optimizer in Algorithm
1. One can also use other base optimizers, e.g., AdamW, to
update the model parameters in Eqn. (4).
5635
Algorithm 1: F-SAM algorithm
Input: Loss function L(w), training datasets
S={(xi,yi)}n
i=1, minibatch size b,
neighborhood size ρ, learning rate γ,
momentum factor λ, projection constant σ
Output: Trained weight w
1Initialize w0,t←0,m−1=0;
2while not converged do
3 Sample a minibatch data Bof size bfromS;
4gt=∇LB(wt);
5mt=λmt−1+ (1−λ)gt;
6 Compute adversarial adversarial perturbation:
7 ϵt=ρdt
∥dt∥where dt=gt−σmt;
8 Compute the gradient approximation g:
9 g′
t← ∇LB(wt+ϵt);
10 Update wusing gradient descent:
11 wt+1←wt−γg′
t;
12 t←t+ 1;
13return wt.
5.3. A Friendly Perspective of the Loss Objective
Here we provide an intuitive understanding on the differ-
ence between SAM and F-SAM. For a minibatch data B,
SAM solves the maximization problem to seek adversarial
perturbation: ϵSAM
s= arg max∥ϵ∥2≤ρLB(w+ϵ).In con-
trast, by observing gProj⊤
∇L(wt)∇LB(wt)in Eqn. (12), F-
SAM indeed computes the adversarial perturbation by max-
imizing the current minibatch loss while minimizing the
loss on the entire dataset:
ϵF-SAM
s = arg max
∥ϵ∥2≤ρLB(w+ϵ)−σLD(w+ϵ).
Then combining the following minimization problem, one
can write F-SAM’s bi-level optimization problem as
min
wEB[LB(w+ϵs)]
s.t.ϵs= arg max∥ϵ∥2≤ρLB(w+ϵ)−σLD(w+ϵ).
The insight behind is that F-SAM aims to find an adversarial
perturbation ϵsthat increases the loss sharpness of the cur-
rent minibatch data while minimizing the impact on the loss
sharpness of the other data points as much as possible. Then
F-SAM minimizes the loss value and sharpness of current
minibatch data. So the adversarial perturbation in F-SAM
is “friendly” to other data points, yielding consistent sharp-
ness minimization. Moreover, this friendliness enables high
robustness of F-SAM to the perturbation radius as shown in
Sec. 6.3. But for vanilla SAM, its adversarial perturbation
increases the loss sharpness of current minibatch data andother data points but minimizes the loss value and sharp-
ness of only current minibatch data. So in vanilla SAM, the
second minimization step indeed does not consider the loss
sharpness increment caused by its first adversarial pertur-
bation step. This inconsistency may impair the sharpness
optimization and thus limit the generalization.
5.4. Convergence Analysis
In this section, we analyze the convergence properties of
the F-SAM algorithm under non-convex setting. We follow
basic assumptions that are standard in convergence analysis
of stochastic optimization [15, 36, 37].
Theorem 2. Assume Assumption 1 and 2 hold. Assume that
SAM uses SGD as the base optimizer with a learning rate
γto update the model parameter in Eqn. (4). By setting
learning rate γ=γ0√
T≤1/βand the perturbation radius
ρt=ρ0√
t, we have
1
TTX
t=1E∥∇L(wt)∥2≤2∆
γ0√
T+Θ√
T+Π log T√
T,(15)
where ∆ =E[L(w0)−L(w∗)]with the optimal solution
w∗toL(w),Θ = 2 βMγ 0+ρ2
0β3γ0, and Π =ρ2
0β2.
See the proof in Appendix A3. For non-convex stochastic
optimization, Theorem 2 shows that F-SAM has the conver-
gence rate O(logT/√
T)and share the same convergence
speed as SAM. But F-SAM enjoys better generalization per-
formance than SAM as shown in Section 6.
6. Numerical Experiments
Here we test F-SAM on various tasks and network architec-
tures under two popular settings: 1) training from scratch,
and 2) transfer learning by fine-tuning pretrained models.
Moreover, ablation study and additional experiments show
more insights on F-SAM.
6.1. Training From Scratch
CIFAR. We first evaluate over CIFAR-10/100 datasets
[39]. The network architectures we considered include
VGG16-BN [41], ResNet-18 [42], WRN-28-10 [43] and
PyramidNet-110 [44]. The first three models are trained
for 200 epochs while for PyramidNet-110 we train for 300
epochs. We set the initial learning rate as 0.05 with a cosine
learning rate schedule. The momentum and weight decay
are set to 0.9 and 0.0005 for SGD, respectively. SAM and
variants adopt the same setting except that the weight de-
cay is set to 0.001 following [29, 45]. We apply standard
random horizontal flipping, cropping, normalization, and
cutout augmentation [46]. For SAM, we set the perturba-
tion radius ρas 0.1 and 0.2 for CIFAR-10 and CIFAR-100
[29, 45]. For ASAM, we adopt the recommend ρas 2 and
5636
CIFAR-10 SGD ASAM FisherSAM SAM F-SAM (ours)
VGG16-BN 94.96 ±0.15 95.57 ±0.05 95.55 ±0.08 95.42 ±0.05 95.62 ±0.11
ResNet-18 96.25 ±0.06 96.63 ±0.15 96.72 ±0.03 96.58 ±0.10 96.75 ±0.09
WRN-28-10 97.08 ±0.16 97.64 ±0.13 97.46 ±0.18 97.32 ±0.11 97.53 ±0.11
PyramidNet-110 97.63 ±0.09 97.82 ±0.07 97.64 ±0.09 97.70 ±0.10 97.84 ±0.05
Table 1. Test accuracy (%) comparison of various neural networks on CIFAR-10.
CIFAR-100 SGD ASAM FisherSAM SAM F-SAM (ours)
VGG16-BN 75.43 ±0.19 76.27 ±0.35 76.90 ±0.37 76.63 ±0.20 77.08 ±0.17
ResNet-18 77.90 ±0.07 81.68 ±0.12 80.99 ±0.13 80.88 ±0.10 81.29 ±0.12
WRN-28-10 81.71 ±0.13 84.99 ±0.22 84.91 ±0.07 84.88 ±0.10 85.16 ±0.07
PyramidNet-110 84.65 ±0.11 86.47 ±0.09 86.53 ±0.07 86.55 ±0.08 86.70 ±0.14
Table 2. Test accuracy (%) comparison of various neural networks on CIFAR-100.
4 for CIFAR-10 and CIFAR-100 [26]. For FisherSAM, we
adopt the recommended ρ= 0.1. For F-SAM, we use the
same ρas SAM, and tune λ={0.6,0.9,0.95}although all
parameters outperforms SAM. We find that λ= 0.9works
best for WRN-28-10 and PyramidNet-110, while λ= 0.6
achieves the best for others. We simply set σ= 1 for all
the following experiments. Experiments are repeated over
3 independent trials.
CIFAR-10 ASAM F-ASAM (ours)
ResNet-18 96.63±0.15 96.77±0.11
WRN-28-10 97.64±0.13 97.73±0.04
CIFAR-100 ASAM F-ASAM (ours)
ResNet-18 81.68±0.12 81.82±0.14
WRN-28-10 84.99±0.22 85.24±0.16
Table 3. Results on integration with ASAM.
In Tab. 1 and 2, we observe that F-SAM achieves 0.1 to
0.2 accuracy gain on CIFAR-10 and 0.2 to 0.4 on CIFAR-
100 in all tested scenarios. Moreover, F-SAM outperforms
SAM’s adaptive variants in most cases, and in Tab. 3, we in-
tegrate our friendly approach to ASAM (see Appendix A4)
and again observe consistent improvement.
ImageNet. Next, we investigate the performance of F-
SAM on larger scale datasets by training ResNet-50 [42]
on ImageNet [47] from scratch. We set the training epochs
to 90 with batch size 128, weight decay 0.0001, and mo-
mentum 0.9. We set the initial learning rate as 0.05 with
a cosine schedule [29]. We apply basic data preprocess-
ing and augmentation [48]. We set ρ= 0.075for all three
SAM and variants and λ= 0.95for F-SAM. In Tab. 4, we
observe that F-SAM outperforms SAM and ASAM and of-
fers an accuracy gain of 0.21 over SAM. This validates the
effectiveness of F-SAM on large-scale problems.
ImageNet SGD ASAM SAM F-SAM
ResNet-50 76.62±0.1277.10±0.1477.14±0.1677.35±0.12
Table 4. Test accuracy (%) comparison on ImageNet.6.2. Transfer Learning
One of the fascinating training pipelines for modern DNNs
is transfer learning, i.e., first training a model on a large
datasets and then easily and quickly adapting to novel tar-
get datasets by fine-tuning. In this subsection, we evaluate
the performance of transfer learning for F-SAM. We use a
Deit-small model [49] pre-trained on ImageNet (with pub-
lic available checkpoints1). We use AdamW [6] as base op-
timizer and train the model for 10 epochs with batch size
128, weight decay 10−5and initial learning rate of 10−4.
We adopt ρ= 0.075for SAM and F-SAM. The results are
shown in Tab. 5. We observe that F-SAM consistently out-
performs SAM and AdamW by an accuracy gain of 0.1 to
0.7. An intriguing observation is that on small Flowers102
dataset [38] (with 1020 images), AdamW even outperforms
SAM. We attribute this to the fact that when the dataset is
small, the full gradient component within the minibatch gra-
dient is more prominent, and its detrimental impact on con-
vergence and generalization becomes more evident.
Datasets AdamW SAM F-SAM (ours)
CIFAR-10 98.10±0.10 98.27±0.05 98.43±0.07
CIFAR-100 88.44±0.10 89.10±0.11 89.49±0.12
Standford Cars 74.78±0.09 75.39±0.05 75.82±0.14
OxfordIIITPet 92.42±0.43 92.70±0.26 92.90±0.23
Flowers102 74.82±0.36 74.38±0.24 75.15±0.35
Table 5. Results on transfer learning by fine-tuning.
6.3. Additional Studies
6.3.1 Robustness to Label Noise
Since previous works have shown that SAM is robust to la-
bel noise, in this subsection, we also test the performance of
F-SAM in the presence of symmetric label noise by random
flipping. The training settings are the same as in Sec. 6.1.
From the results in Tab. 6, we observe that F-SAM con-
sistently improves the performance from SAM, confirming
1https://github.com/facebookresearch/deit
5637
its improved generalization. Notably, such improvement is
more obvious when the noise rate is large (i.e. 60%-80%).
This is perhaps because when the data label is wrong, the
harmfulness of increasing the sharpness of other batch data
is more prominent, even posing challenges to convergence.
On severe noise ratio of 80%, we find that vanilla SAM
even suffers a collapse with the original setting, which is
also reported by Foret et al. [14], and our F-SAM relieves
it, achieving a remarkable accuracy gain of 28%.
Rates 20% 60% 70% 80%
SGD 87.05±0.0652.21±0.2339.31±0.1327.86±0.53
SAM 95.27±0.0990.08±0.0984.89±0.1031.73±3.37
F-SAM 95.42±0.0890.47±0.0586.48±0.0759.39±1.77
Table 6. Results under label noise on CIFAR-10 with ResNet-18.
6.3.2 Robustness to Perturbation Radius
One deficiency of SAM is its sensitivity to the perturba-
tion radius, which may require choosing different radii for
optimal performance on different datasets. Especially, an
excessively large perturbation radius can result in a sharp
decrease in generalization performance. We attribute a por-
tion of this over-sensitivity to the impact of the adversarial
perturbation derived from the current minibatch data on the
other data samples. Such an impact can be more obvious
when the perturbation radius increases as the magnitude of
the full gradient component grows correspondingly. In our
F-SAM, such impact on other data samples is eliminated as
much as possible, and thus, we can expect that it is more ro-
bust to the choices of the perturbation radius. In Fig. 3, we
plot the performance of SAM and F-SAM under different
perturbation radii. We can observe that F-SAM is much less
sensitive to ρthan SAM. Especially on larger perturbation
radius, the performance gain of F-SAM over SAM is more
significant. For example, when ρis set to 2x larger than the
optimal on CIFAR100, SAM’s performance suffers a sharp
drop from 80.88% to78.83%, but F-SAM still maintains
a good performance of 80.30%. We further compare the
training curves of SAM and F-SAM under different pertur-
bation radii in Appendix A6, showing that F-SAM greatly
facilitates training on large radius. This confirms F-SAM’s
improved robustness to different perturbation radii.
6.3.3 Results on Different Batch Sizes
As the main claim of this paper is that the full gradient
component existing in SAM’s adversarial perturbation does
not contribute to generalization and may have detrimen-
tal effects. As the training batch size increases, the corre-
sponding full gradient component existing in the minibatch
gradient can be strengthened. Thus, the improvement of
F-SAM from removing the full gradient component could
be more obvious. In Fig. 4, we compare the performance
of SGD, SAM, and our F-SAM under batch sizes from
0.05 0.10 0.15 0.20
Perturbation radius 
96.396.496.596.696.796.8T est accuracy (%)
ResNet-18 on CIFAR-10
F-SAM
SAM
0.1 0.2 0.3 0.4
Perturbation radius 
79.079.580.080.581.081.5T est accuracy (%)
ResNet-18 on CIFAR-100
F-SAM
SAMFigure 3. Results under different perturbation radii ρ.
128 512 2048
Batch size b74767880T est accuracy (%)
ResNet-18 on CIFAR-100
SGD
SAM
F-SAM
Figure 4. Performance comparison with different batch sizes.
{128,512,2048}while keeping other hyper-parameters un-
changed. We observe that the performance gain of F-
SAM over SAM increases significantly as the batch size in-
creases. This further validates our findings and supports the
effectiveness of removing the full gradient component.
Connection to m-sharpness. We would like to relate our
removing the full gradient component to the intriguing m-
sharpness in SAM, which is defined as the search for an ad-
versarial perturbation that maximizes the sum of losses over
sub-batches of mtraining points [14]. This phenomenon
emphasizes the importance of using a small mto effectively
improve generalization [14, 15, 50]. It is worth noting that
by utilizing a small sub-batch, one implicitly suppresses the
effects of the full gradient component. Hence, our findings
can provide new insights into understanding m-sharpness.
7. Conclusion
In this paper, we conduct an in-depth investigation into the
core components of SAM’s generalization. By decompos-
ing the minibatch gradient into two orthogonal components,
we discover that the full gradient component in adversar-
ial perturbation contributes minimally to generalization and
may even have detrimental effects, while the stochastic gra-
dient noise component plays a crucial role in enhancing
generalization. We then propose a new variant of SAM
called F-SAM to eliminate the undesirable effects of the
full gradient component. Extensive experiments across var-
ious tasks demonstrate that F-SAM significantly improves
the robustness and generalization performance of SAM.
Acknowledgement This work was supported by National
Key Research Development Project (2023YFF1104202),
National Natural Science Foundation of China (62376155),
Shanghai Municipal Science and Technology Research Pro-
gram (22511105600) and Major Project (2021SHZDZX0102).
Pan Zhou was supported by the Singapore Ministry of Ed-
ucation (MOE) Academic Research Fund (AcRF) Tier 1 grant.
5638
References
[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 1
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. In Advances in Neural
Information Processing Systems (NeurIPS) , 2020.
[3] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (CVPR) , 2021. 1
[4] H. Robbins and S. Monro. A stochastic approximation
method. The Annals of Mathematical Statistics , 22(3):400–
407, 1951. 1
[5] Diederik P. Kingma and Jimmy Lei Ba. Adam: A method
for stochastic optimization. In International Conference on
Learning Representations (ICLR) , 2015. 2
[6] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 , 2017. 7
[7] Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C
Tatikonda, Nicha Dvornek, Xenophon Papademetris, and
James Duncan. Adabelief optimizer: Adapting stepsizes by
the belief in observed gradients. In Advances in Neural In-
formation Processing Systems (NeurIPS) , 2020.
[8] Xingyu Xie, Pan Zhou, Huan Li, Zhouchen Lin, and
Shuicheng Yan. Adan: Adaptive nesterov momentum al-
gorithm for faster optimizing deep models. arXiv preprint
arXiv:2208.06677 , 2022.
[9] Pan Zhou, Xingyu Xie, and YAN Shuicheng. Win: Weight-
decay-integrated nesterov acceleration for adaptive gradient
algorithms. In International Conference on Learning Repre-
sentations (ICLR) , 2023. 1
[10] Sepp Hochreiter and J ¨urgen Schmidhuber. Flat minima. Neu-
ral computation , 1997. 1, 2, 3
[11] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua
Bengio. Sharp minima can generalize for deep nets. In In-
ternational Conference on Machine Learning (ICML) , 2017.
[12] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and
Tom Goldstein. Visualizing the loss landscape of neural
nets. In Advances in Neural Information Processing Systems
(NeurIPS) , 2018.
[13] Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven
Chu Hong Hoi, et al. Towards theoretically understand-
ing why sgd generalizes better than adam in deep learn-
ing. Advances in Neural Information Processing Systems ,
33:21285–21296, 2020. 1, 2, 3
[14] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam
Neyshabur. Sharpness-aware minimization for efficiently
improving generalization. In International Conference on
Learning Representations (ICLR) , 2020. 1, 2, 3, 8[15] Maksym Andriushchenko and Nicolas Flammarion. Towards
understanding sharpness-aware minimization. In Interna-
tional Conference on Machine Learning (ICML) , 2022. 2,
3, 6, 8
[16] Zixiang Chen, Junkai Zhang, Yiwen Kou, Xiangning Chen,
Cho-Jui Hsieh, and Quanquan Gu. Why does sharpness-
aware minimization generalize better than sgd? arXiv
preprint arXiv:2310.07269 , 2023. 2
[17] Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann
LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes,
Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing
gradient descent into wide valleys. In International Confer-
ence on Learning Representations (ICLR) , 2017. 2
[18] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal,
Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-
batch training for deep learning: Generalization gap and
sharp minima. In International Conference on Learning Rep-
resentations (ICLR) , 2017.
[19] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry
Vetrov, and Andrew Gordon Wilson. Averaging weights
leads to wider optima and better generalization. arXiv
preprint arXiv:1803.05407 , 2018. 2
[20] Wei Wen, Yandan Wang, Feng Yan, Cong Xu, Chunpeng
Wu, Yiran Chen, and Hai Li. Smoothout: Smoothing out
sharp minima to improve generalization in deep learning.
arXiv preprint arXiv:1805.07898 , 2018. 2
[21] Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Nor-
malized flat minima: Exploring scale invariant definition of
flat minima for neural networks using pac-bayesian analy-
sis. In International Conference on Machine Learning , pages
9636–9647. PMLR, 2020.
[22] Yaowei Zheng, Richong Zhang, and Yongyi Mao. Regu-
larizing neural networks via adversarial model perturbation.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) , 2021.
[23] Devansh Bisla, Jing Wang, and Anna Choromanska. Low-
pass filtering sgd for recovering flat optima in the deep learn-
ing optimization landscape. In International Conference on
Artificial Intelligence and Statistics (AISTATIS) , 2022.
[24] Tao Li, Weihao Yan, Zehao Lei, Yingwen Wu, Kun Fang,
Ming Yang, and Xiaolin Huang. Efficient generalization im-
provement guided by random weight perturbation. arXiv
preprint arXiv:2211.11489 , 2022.
[25] Tao Li, Weihao Yan, Qinghua Tao, Zehao Lei, Yingwen Wu,
Kun Fang, Mingzhen He, and Xiaolin Huang. Revisiting
random weight perturbation for efficiently improving gener-
alization. In OPT 2023: Optimization for Machine Learning ,
2023. 2
[26] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and
In Kwon Choi. Asam: Adaptive sharpness-aware minimiza-
tion for scale-invariant learning of deep neural networks.
InInternational Conference on Machine Learning (ICML) ,
2021. 2, 7
[27] Minyoung Kim, Da Li, Shell X Hu, and Timothy
Hospedales. Fisher sam: Information geometry and sharp-
ness aware minimisation. In International Conference on
Machine Learning (ICML) , 2022. 2
5639
[28] Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui,
Hartwig Adam, Nicha Dvornek, Sekhar Tatikonda, James
Duncan, and Ting Liu. Surrogate gap minimization improves
sharpness-aware training. In International Conference on
Learning Representations (ICLR) , 2022. 2
[29] Bingcong Li and Georgios B Giannakis. Enhancing
sharpness-aware optimization through variance suppression.
arXiv preprint arXiv:2309.15639 , 2023. 2, 6, 7
[30] Jiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Lian-
gli Zhen, Rick Siow Mong Goh, and Vincent YF Tan. Effi-
cient sharpness-aware minimization for improved training of
neural networks. In International Conference on Learning
Representations (ICLR) , 2022. 2
[31] Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and
Yang You. Towards efficient and scalable sharpness-aware
minimization. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision (CVPR) , 2022.
[32] Jiawei Du, Daquan Zhou, Jiashi Feng, Vincent YF Tan, and
Joey Tianyi Zhou. Sharpness-aware training for free. In Ad-
vances in Neural Information Processing Systems (NeurIPS) ,
2022.
[33] Yang Zhao, Hao Zhang, and Xiuyuan Hu. SS-SAM:
Stochastic scheduled sharpness-aware minimization for ef-
ficiently training deep neural networks. arXiv preprint
arXiv:2203.09962 , 2022.
[34] Weisen Jiang, Hansi Yang, Yu Zhang, and James Kwok.
An adaptive policy to employ sharpness-aware minimiza-
tion. In International Conference on Learning Representa-
tions (ICLR) , 2023. 2, 3
[35] Kaiyue Wen, Tengyu Ma, and Zhiyuan Li. How sharpness-
aware minimization minimizes sharpness? In International
Conference on Learning Representations (ICLR) , 2023. 2
[36] Saeed Ghadimi and Guanghui Lan. Stochastic first-and
zeroth-order methods for nonconvex stochastic program-
ming. SIAM Journal on Optimization , 2013. 3, 6
[37] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear con-
vergence of gradient and proximal-gradient methods under
the polyak-łojasiewicz condition. In Joint European Con-
ference on Machine Learning and Knowledge Discovery in
Databases , 2016. 3, 6
[38] Maria-Elena Nilsback and Andrew Zisserman. Automated
flower classification over a large number of classes. In In-
dian conference on computer vision, graphics & image pro-
cessing . IEEE, 2008. 4, 7
[39] Alex Krizhevsky and Geoffrey Hinton. Learning multiple
layers of features from tiny images. Technical Report , 2009.
4, 6
[40] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and
CV Jawahar. Cats and dogs. In IEEE conference on computer
vision and pattern recognition (ICCV) , 2012. 4
[41] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 , 2014. 6
[42] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , 2016. 6, 7[43] Sergey Zagoruyko and Nikos Komodakis. Wide residual
networks. In Richard C. Wilson, Edwin R. Hancock, and
William A. P. Smith, editors, British Machine Vision Confer-
ence (BMVC) , 2016. 6
[44] Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyra-
midal residual networks. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
2017. 6
[45] Peng Mi, Li Shen, Tianhe Ren, Yiyi Zhou, Xiaoshuai Sun,
Rongrong Ji, and Dacheng Tao. Make sharpness-aware mini-
mization stronger: A sparsified perturbation approach. arXiv
preprint arXiv:2210.05177 , 2022. 6
[46] Terrance DeVries and Graham W Taylor. Improved regular-
ization of convolutional neural networks with cutout. arXiv
preprint arXiv:1708.04552 , 2017. 6
[47] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2009. 7
[48] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. 2017. 7
[49] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herv ´e J´egou. Training
data-efficient image transformers & distillation through at-
tention. In International Conference on Machine Learning
(ICML) , 2021. 7
[50] Kayhan Behdin, Qingquan Song, Aman Gupta, David
Durfee, Ayan Acharya, Sathiya Keerthi, and Rahul
Mazumder. Improved deep neural network generaliza-
tion using m-sharpness-aware minimization. arXiv preprint
arXiv:2212.04343 , 2022. 8
5640
