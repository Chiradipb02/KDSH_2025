Privacy-preserving Optics for Enhancing Protection in Face De-identification
Jhon Lopez1,2, Carlos Hinojosa2,∗, Henry Arguello1,†, Bernard Ghanem2,†
1Universidad Industrial de Santander;2King Abdullah University of Science and Technology (KAUST)
https://carloshinojosa.me/project/privacy-face-deid/
Abstract
The modern surge in camera usage alongside
widespread computer vision technology applications
poses significant privacy and security concerns. Current
artificial intelligence (AI) technologies aid in recognizing
relevant events and assisting in daily tasks in homes, of-
fices, hospitals, etc. The need to access or process personal
information for these purposes raises privacy concerns.
While software-level solutions like face de-identification
provide a good privacy/utility trade-off, they present
vulnerabilities to sniffing attacks. In this paper, we propose
a hardware-level face de-identification method to solve this
vulnerability. Specifically, our approach first learns an
optical encoder along with a regression model to obtain a
face heatmap while hiding the face identity from the source
image. We also propose an anonymization framework that
generates a new face using the privacy-preserving image,
face heatmap, and a reference face image from a public
dataset as input. We validate our approach with extensive
simulations and hardware experiments.
1. Introduction
The widespread use of cameras, coupled with the ubiqui-
tous application of computer vision technology across var-
ious facets of our daily lives, has led to a significant and
increasing concern about privacy and security. Specifically,
current AI technology enables systems to recognize rele-
vant events and assist us in daily activities in homes, offices,
hospitals, etc. However, how can we ensure this technology
protects our privacy, especially when it needs to access or
process our personal information? This rising concern over
data privacy has led international entities like the European
Union to establish regulations for ensuring privacy [43].
Different studies have explored methods and applica-
tions for protecting privacy in computer vision. In general,
these studies can be divided into software and hardware-
level protection. Most methods operate at the software-
level since they directly work on already acquired high-
fidelity images. Prior approaches rely on domain knowl-
*Project lead; †Equal PI contribution.
InternetTraditional Methods
Server
Internet
(a)
(b)Our Proposed Method
Server?Source
Traditional Camera
Source
Our Camera
Output
Output
AdversaryAdversary
Acquired 
ImageAcquired 
Image
Optimized Phase MaskAttack
AttackFigure 1. Traditional vs. our proposed pipeline. (a) The tradi-
tional face de-identification pipeline is vulnerable to adversarial
attacks ( /unlock). (b) We propose to close the security gap and enhance
protection by learning privacy-preserving optics ( ὑ2).
edge and hand-crafted approaches, such as pixelation, blur-
ring, and face/object replacement, to protect sensitive infor-
mation [24, 36, 52]. However, traditional anonymization
methods, such as face blurring, change the image seman-
tics, resulting in a significant detection performance drop
in downstream tasks such as face detection [15] and track-
ing [44]. Recent studies have shown that de-identifying
people in images, i.e., removing the identification character-
istics and generating virtual faces to replace these identities,
maintains the utility for downstream tasks [3, 14, 29, 48].
Unfortunately, these software-level solutions are vulner-
able to adversarial attacks that can get direct access to
the original privacy-sensitive images before the vision al-
gorithms process them (see Fig. 1 (a)). Consequently,
hardware-level privacy protection approaches are consid-
ered more secure, since they rely on the optical system to
add an extra layer of security by removing sensitive data
during image acquisition. Prior hardware-level methods in-
clude the use of low-resolution cameras [38], defocus lenses
[34], and depth cameras [41] to perform different computer
vision tasks like human action recognition and pose estima-
tion. Recent approaches employ the end-to-end and joint
optimization of the optics and vision task to promote pri-
vacy [12, 42]. With such approaches, it is possible to learn
a phase mask [40] to modulate the incident light field to fil-
ter out privacy-sensitive information before image capture.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
12120
Paper Contribution. Among the privacy-preserving so-
lutions, face de-identification using generative adversarial
networks (GAN) provides the best privacy/utility trade-off.
However, current face de-identification methods operate at
the software-level, introducing a security gap between im-
age acquisition and algorithms (see Fig. 1 (a)). In this paper,
we develop a hardware-level face de-identification approach
to close the security gap as shown in Fig. 1 (b). In our study,
we observe that the learned optics filter out high-frequency
information from the image to protect privacy and promote
face misrecognition; however, low-frequency information
is preserved. Therefore, we first perform end-to-end train-
ing of our privacy-preserving computational camera (a.k.a
optical encoder) and a heatmap regression model to obtain
a face heatmap that conceals privacy-sensitive information
from the source image. Then, we train a GAN using the
acquired privacy-preserving image, the corresponding face
heatmap, and a reference face image from a public dataset
to generate a new synthetic face. The acquired privacy-
preserving image and the face heatmap conceal the true
identity of the source but provide general information that
allows our model to understand the global geometry (e.g.,
head position). On the other hand, the given reference im-
age is used to extract the specific style for the new face. We
summarize our contributions as follows: (i)We introduce a
full privacy-preserving framework for face de-identification
that addresses the security gap between image acquisition
and algorithms in traditional approaches. (ii)Our approach
consists of two integrated stages: First, we jointly optimize
the lens of a computational camera to encode the source
images such that the true identity of the person is con-
cealed, but important features are preserved to perform face
heatmap regression. Second, we use the acquired privacy-
preserving images, the corresponding face heatmaps, and
reference images from a public dataset to train a GAN to
generate a new face. (iii)We propose a facial expression
consistency loss and use an LPIPS-based loss term to im-
prove quality and promote the preservation of detailed ex-
pressions (like smiles) in the generated images. (iv)We
perform extensive simulations on FFHQ and CelebA-HQ
datasets to validate our proposed approach. Furthermore,
we build a prototype camera and perform hardware experi-
ments that match simulations.
2. Related Work
We categorize previous efforts in privacy-preserving vision
into two groups: software-level andhardware-level protec-
tion, where the latter is considered more robust to attacks.
2.1. Software-level Privacy Protection
In the literature, most privacy-preserving vision approaches
work at the software level. Specifically, such methods onlyperform software-level processing on high-resolution im-
ages acquired by traditional cameras.
Non-generative Approaches. Traditional methods lever-
age domain knowledge and use hand-crafted techniques
such as blurring, mosaicing, masking, pixelation, and
face/object replacement to protect sensitive information
[1, 21, 24, 33, 36, 52]. These methods can successfully con-
ceal the identity of a person and private sensitive objects on
an image when the target to protect is known beforehand.
However, they also could introduce artifacts harming the
performance of downstream visual tasks [15].
GAN-based Face De-identification. On the other hand,
face de-identification using GAN provides a better pri-
vacy/utility trade-off than other approaches. These meth-
ods focus on generating virtual faces to replace the orig-
inal identity, preserving privacy and avoiding being recog-
nized by unauthorized users (adversaries/hackers) and com-
puter vision systems. Several methods in the literature
[3, 6, 14, 23, 27, 45, 48] are built on top of state-of-the-
art GAN networks like StyleGAN2 [18] and StarGAN2 [8]
given their impressive ability to capture the distribution of
the training samples and then generate good-looking face
images. Among prior works, DeepPrivacy [14] first extracts
the sparse facial key points from the image and masks the
face region with a constant value; then employs a Style-
GAN2 generator to in-paint a randomly generated face, pre-
serving the contextual and pose information. Similarly,
CIAGAN [29] also masks the source face region and uses a
conditional GAN to perform conditional identity swapping
by employing an identity discriminator to force the gener-
ator to synthesize a new face on the masked region. Most
recent approaches aim to improve the ability of the models
to generate better quality and natural-looking faces while
keeping similar privacy preservation results as CIAGAN
and DeepPrivacy [3, 23, 27, 48]. For example, Barattin et al.
[3] proposes to directly optimize the latent space to ensure
the identity is distant enough from the original image while
preserving some facial attributes. While these software-
level approaches preserve privacy in the final application,
the acquired images are not protected. This security gap
makes these methods vulnerable to adversarial attacks that
can get direct access to the original privacy-sensitive image
captures before the algorithms process them.
2.2. Hardware-level Privacy Protection
These approaches are considered more secure since they
rely on the optical system to add an extra layer of security
that removes sensitive data during image acquisition.
Fixed Optics. Prior methods include the use of low-
resolution cameras to capture images and videos, avoiding
the unwanted leak of identity information from human sub-
jects [37, 38]. Similarly, authors in [34, 35] propose optical
designs based on a defocusing lens to filter or block sensi-
12121
tive information directly from the incident light field before
sensor measurement acquisition and perform K-anonymity
to protect privacy. In particular, they show how to select a
defocus blur that provides a certain level of privacy over a
working region within the sensor size limits; however, only
using optical defocus for privacy may be susceptible to re-
verse engineering attacks [12, 13].
Learning Optics. Instead of using fixed optics privacy-
preserving cameras to acquire the data and then train al-
gorithms, the privacy/utility trade-off can be improved at
the hardware-level by approaches that jointly optimize the
optics and vision algorithms to directly filter sensitive data
and enhance utility in downstream tasks [12, 40]. Most re-
cent approaches learn a phase mask [12, 46] to modulate the
incident light field and filter out privacy-sensitive informa-
tion before image acquisition. This idea has been applied in
different tasks, such as human pose estimation [12], human
action recognition [13], image captioning [2], and passive
depth estimation [42]. Our proposed framework, developed
in the next section, uses privacy-preserving cameras to im-
prove privacy and close the security gap while preserving
the utility given by face de-identification methods. To the
best of our knowledge, this is the first work that learns optics
for the face de-identification task. Note that our framework
can be used with fixed optics cameras, e.g., low-resolution
cameras; however, learning the optics provides better con-
trol over privacy and utility, as we show in Section 4.
3. Proposed Methodology
We are interested in using privacy-preserving images to add
an extra layer of security to the face de-identification task.
Previous approaches [2, 12, 13] learn optics for a specific
task, and the utility of the acquired privacy-preserving im-
ages is limited only to this task. To overcome this limita-
tion, we propose to combine optics learning with a genera-
tive model to maximize utility in downstream tasks. Specif-
ically, the output of our framework is a realistic face image
that maintains the anonymity of the original person.
In general, our framework (Fig. 2) consists of two parts:
optics learning and new face generation. In the first part,
our strategy is to jointly optimize the camera optics and a
heatmap regression network to obtain a face heatmap while
ensuring privacy protection. Our key observation is that
we can learn the camera lens to degrade the image such
that the high-frequency information is filtered out, conceal-
ing the subject identity while preserving important features
to obtain a face heatmap. We can then leverage the face
heatmap to infer the global geometry information of the
original (source) image like head pose and eyes, nose, and
mouth position. In the second part, our main goal is to learn
the parameters of a conditional GAN to generate a new face
identity from the input privacy-preserving image, the face
heatmap, and a reference image.3.1. Optics Learning
The main goal of the optical component in our proposed
approach (Fig. 2) is to design a phase mask to visually dis-
tort images by removing the high-frequency spatial infor-
mation and hence conceal privacy-sensitive attributes while
preserving useful information to extract a face heatmap.
Image Formation Model and Phase Mask Learning. We
adopt a similar strategy as previous works in [12, 13, 40] to
couple the modeling and design of two essential operators
in the imaging system: wave propagation and phase modu-
lation. We model the image acquisition process by defining
the point spread function (PSF) in terms of the lens sur-
face profile. This allows emulating the propagation of the
wavefront and training the parameters of the refractive lens.
Considering the Fresnel approximation and paraxial regime
[10] for incoherent illumination, we describe the PSF as:
H(u′, v′) =|F−1{F{P(u, v)·W(u, v)} ·T(fu, fv)}|2,(1)
where P(u, v) = Pt(u, v)·Pϕ(u, v),W(u, v)is
the incoming wavefront, T(·)represents the trans-
fer function with (fu, fv)as the spatial frequencies,
Pϕ(u, v) = exp( −ikϕ(u, v))with ϕ(u, v)as the phase
mask and k= 2π/λ as the wavenumber, Pt(u, v) =
exp 
−ik
2d(u2+v2)
denotes the light wave propagation
phase with das the object-lens distance, F{·} denotes the
2D Fourier transform, and (u′, v′)is the spatial coordi-
nate on the camera plane. The values of ϕ(·)are modeled
via Zernike polynomials with ϕ(u, v) =R(√
u2+v2)·
cos (arctan ( v/u)), where R(·)represents the radial poly-
nomial function [22]. To train the phase mask values using
our model, we discretize the phase mask ϕ(·)as:
ϕ=pX
j=1βjQj, (2)
where Qjdenotes the j-th Zernike polynomial in Noll no-
tation, and βjis the corresponding coefficient [4]. Each
Zernike polynomial describes a wavefront aberration [22];
hence the phase mask ϕis formed by the linear combination
ofpaberrations. In this regard, the optical element parame-
terized by ϕcan be seen as an optical encoder, where the co-
efficients β={βj}p
j=1determine the data transformation.
Therefore, our end-to-end training finds the coefficients β
that provide the maximum visual distortion of the scene but
allow extracting relevant features to obtain a face heatmap
via a regression network.
The sensing process of our camera can be modeled as
a shift-invariant convolution operation between the source
image (scene) and the PSF as:
y=C(H∗x) +η, (3)
where xrepresents the discrete color source image of the
person we want to protect; Hdenotes the discretized ver-
sion of the PSF [10] in Eq. (1); ηrepresents Gaussian
noise in the sensor, and C(·)is the camera response func-
tion, which we assume linear. Please refer to our supple-
mentary for more details on our light propagation model.
12122
Camera PSF
Optimized
Phase Mask
Optical ComponentReference
(0, 1)Domain 
Coding
(0, 1)Domain 
Coding
Heatmap
Output
Acquired 
Image
Style Encoder Generator
Heatmap Regression Network DiscriminatorEncoder
Real Fake Real FakeDomain 0 Domain 1Reference
Source
Figure 2. Our proposed face de-identification framework . We first jointly optimize the camera optics and a heatmap regression network
to obtain a face heatmap and conceal source image identity. Then, we train a GAN network to generate a new face identity using the
acquired image, the obtained heatmap, and a reference image. We leverage global geometry information (e.g. face pose and eyes, nose,
mouth position) from the acquired image and the heatmap while the style is extracted from the reference image.
Heatmap Regression Network. We adopt the same imple-
mentation as in [8], which is an adaptation from the Face
Alignment Network (FAN) [5] and the adaptive wing-based
heatmap [47]. This network consists of stacked Hourglass
(HG) [32] layers and coordinate convolutions (CoordConv)
[26], which encode coordinate information as additional
channels before the convolution operation. Given an image
ycaptured by our camera, we use the regression network U
to obtain a heatmap m=U(y)as shown in Fig. 2. This
heatmap fits the eyes, nose, mouth, and chin region and re-
trieves geometry information about the face.
3.2. Generative Network
To perform the face image generation, we use the genera-
tor, style encoder, and discriminator network architectures
from StarGAN2 [8]. Considering ω∈Ωas an arbitrary do-
main and given a privacy-preserving image ycaptured by
our camera as in Eq. 3, we first use an inversion encoder
Eto map yto the latent space ¯y=E(y). Then, we want
the generator Gto translate yinto a new image G(¯y,s,m)
reflecting a domain-specific style code sprovided by the
style encoder Sand the pose and face geometry informa-
tion provided by the heatmap m. We extract the style code
s=Sω(r)from a publicly available reference image rcor-
responding to a specific domain ω. Note that using different
reference images produces different style codes s. Specifi-
cally,srepresents the style of a particular reference image r
and domain ω; hence it is not necessary to directly provide
ωto the generator Gand allows Gto synthesize images of
all domains reflecting the style of r. Finally, the discrimina-
torDis a multi-task discriminator [25, 31], which consists
of multiple output branches. Each branch Dωlearns a bi-nary classification determining whether an image ris a real
image of its domain ωor a fake image G(¯y,s,m)produced
byG. Note that, unlike StarGAN2, we use the reference im-
agerin the discriminator instead of the source image xto
avoid privacy information leakage.
3.3. Training Objectives
We divide our training into two stages. In the first stage, we
learn the coefficients of Zernike polynomials βand the pa-
rameters of the heatmap regression network to produce im-
ages that visually conceal the identity of the person while
allowing to extract a face heatmap. For the second stage,
we freeze the camera and regression model and only train
the generative network to translate the acquired privacy-
preserving image to a new face using the extracted heatmap
and a reference face image.
3.3.1 Stage I: Optical Design
This stage aims to extract the face heatmap information
fromyby learning the optics. As the starting point of our
training, we assume an aberration-free lens and use a pre-
trained regression model U.
Optics Loss Function. To encourage image degradation,
we minimize the difference of the acquired image ywith the
original image x. Also, for easy and faster convergence, we
promote symmetric and low frequencies using a predefined
defocus PSF Hfas a regularizer. Specifically, assuming
we acquire yusing Eq. 3 and the images are normalized
between [0,1], we define the loss function for our camera
lens optimization as:
Loptics =Ex
1− ∥x−y∥2
2+α1∥H−Hf∥2
, (4)
where ∥ · ∥2
2is the standard mean squared error (MSE),
andα1is a hyperparameter. Note that Hfis a predefined
12123
PSF whose parameters are frozen during training. See our
supplementary document for details on our regularizer Hf.
Heatmap Regression Loss Function . After initializing
Uwith the pre-trained weights [47], we aim to obtain a
heatmap from ythat closely resembles the heatmap ob-
tained from x; therefore, we use the following loss:
Lhmap =Ex[∥U(y)− U∗(x)∥1], (5)
where we use the heatmap U∗(x)as ground truth and U∗is
a freeze pre-trained regression model.
Full Objective. We finetune the camera model and the re-
gression network Uend-to-end to gradually distort the op-
tics and learn to extract the heatmap from the yusing the
following full objective function:
min
C,ULoptics +α2Lhmap, (6)
where α2is a hyperparameter to control the similarity be-
tween HandHf.
3.3.2 Stage II: Generative Network
Once we train the lens and the regression network parame-
ters, we now train our generative network using the follow-
ing objectives. Note that the first four losses are adopted
from StarGAN2 [8], while the last two losses are specifi-
cally proposed to improve the generation when using our
privacy-preserving image y.
Adversarial Objective . During training, we randomly se-
lect a reference image rand a target domain ˜ω∈Ω, from
which we compute a target style code ˜s=S˜ω(r). The
generator Greceives the latent code ¯y=E(y)and the
maskm=U(y)extracted from the image yand the style
code ˜sas inputs and is trained to produce an output image
G(¯y,m,˜s), guided by an adversarial loss function:
Ladv=Ey,ω[logDω(r)]
+Ey,˜ω,r[log(1 − D ˜ω(G(¯y,m,˜s)))], (7)
where Dω(·)denotes the output of the discriminator Dcor-
responding to the domain ω.
Style Reconstruction . To ensure that the generator Geffec-
tively incorporates the style code ˜sin the image generation
process, we employ a style reconstruction loss:
Lsty=Ey,˜ω,r[∥˜s− S ˜ω(G(¯y,m,˜s))∥1]. (8)
Style Diversification. To further enable the generator to
produce diverse images, we explicitly regularize Gwith the
diversity sensitive loss [28]:
Lds=Ey,˜ω,r1,r2[∥G(¯y,m,˜s1)− G(¯y,m,˜s2)∥1],(9)
where the target style codes ˜s1and˜s2are produced by S
using two reference images r1andr2.
Preserving Source Characteristics. To ensure that the
domain-invariant attributes (such as pose) of the input
source image xare accurately maintained in the generated
image G(¯y,m,˜s), we utilize the cycle consistency loss that
leverages the information from the mask extracted from the
privacy-preserving image y.
Lcyc=Ey,ω,˜ω,r[∥y− G(ˆy,ˆm,ˆs)∥1], (10)where ˆy=E(G(¯y,m,˜s)),ˆm=U∗(G(¯y,m,˜s)),ˆs=
Sω(y)is the estimated style code of the input privacy-
preserving image y, and ωcorresponds to the original do-
main of y, which is the same as the source image x.
LPIPS. We employ an LPIPS loss term [51] to improve the
generated image quality and more effectively reflect the ref-
erence image face attributes.
LLPIPS =Ey,˜ω,r[∥Q(r)− Q(G(¯y,m,˜s))∥2], (11)
where Qis the pre-trained perceptual feature extractor.
Facial Expression Consistency. Since the high-frequency
information is missing in the privacy-preserving image y,
it is challenging to preserve small expressions like smiles,
eyes blinking, or small mouth movements in the generated
image. Therefore, we propose a facial expression consis-
tency loss as follows:
Lexpr=Ex[∥m∗⊙x−m⊙y∥1], (12)
where ⊙denotes the element-wise product, and m∗=
U∗(x)is the heatmap obtained from the source image x.
Full Objective. Our full objective function can be summa-
rized as follows:
min
G,S,Emax
D
Ladv+λstyLsty−λdsLds+λcycLcyc
+λLPIPS LLPIPS +λexprLexpr
, (13)
where λsty, λds,λcyc,λLPIPS , and λexpr are hyperpa-
rameters for each term. See our supplementary document
for training details of Stage I and Stage II, and results when
using latent vectors instead of reference images, as used in
StarGAN2, to generate the style codes.
4. Experimental Results
We extensively evaluate the performance of our proposed
face de-identification approach using standard metrics and
evaluation protocols and compare it with other methods.
Dataset. We train our proposed model on two widely used
datasets: CelebA-HQ [16] and FFHQ [17]. We separate the
images of the CelebA-HQ dataset into two domains (male
and female) as in [8]. We show our main results and ab-
lation studies with the CelebA-HQ dataset using the same
train/eval set split provided by [8]. To quantitatively com-
pare with other methods, we follow the common practice
of using the FFQH dataset for training and the CelebA-
HQ dataset for evaluation [23, 48]. For a fair comparison,
all images are resized to 256×256resolution for training,
which is the same resolution used in the baselines.
Metrics. Following previous works [6, 48], we quanti-
tatively evaluate the identity protection given by our face
de-identification approach by measuring the ℓ2distance
(DIS) of embedding vectors from the de-identified and
original faces extracted by a pre-trained face recognition
model. Specifically, we employ the Face Recognition li-
brary (FR) [9] and FaceNet [39], which is pre-trained on
two public datasets: CASIA-Webface [49] and VGGFace2
12124
Components DIS ↑ Landmarks ↓Bounding Box ↓FID↓H L E FR CASIA VGGFace2 MtCNN Dlib MtCNN Dlib
× × × 0.851 1.132 1.313 5.434 6.473 13.667 8.008 34.388
✓× × 0.847 1.162 1.350 5.309 6.260 12.988 7.654 36.056
✓ ✓ × 0.847 1.174 1.359 4.692 5.683 11.818 7.037 29.338
✓×✓ 0.664 1.037 1.199 1.884 2.081 4.694 4.365 24.998
✓ ✓ ✓ 0.697 1.073 1.255 2.022 2.259 4.727 4.985 24.972
Table 1. Quantitative results of the ablation studies . The best
result for each pre-trained face recognition model is in bold, and
the second-best result is underlined.
[7]. We further evaluate the utility ( ↑) of our generated im-
ages and the preservation of facial attributes from the source
image ( ↓), such as face orientation, lips position, and other
attributes, using the Dlib library [20] and MtCNN network
[50]. Specifically, we estimate the facial landmarks between
the original and de-identified faces and measure their simi-
larity using the ℓ1norm. Additionally, we estimate the fa-
cial bounding box to evaluate whether the face is located
in the correct position and size. Finally, we use Fr ´echet in-
ception distance (FID ↓) to measure the distance between
the generated and real distribution. Since our proposed ap-
proach uses reference-guided synthesis, each source image
is transformed using 10 reference images randomly sam-
pled from the test set and we report the average value.
4.1. Ablation Studies
We conducted a series of experiments to investigate the
contribution of using the face heatmap (H), LLPIPS (L),
andLexpr (E) loss functions on the performance of our
method. Table 1 shows the results of our ablation stud-
ies. In the table, a checkmark ( ✓) denotes the inclusion,
and a red cross (×) denotes the exclusion of specific com-
ponents. From the table, we observe that there is an im-
provement in all metrics when face heatmaps (H ✓) are
used. This supports our hypothesis that integrating a face
heatmap in our framework effectively maintains geometric
information of the face (such as head pose) from the source
image. Since we obtain the face heatmap directly from
the privacy-preserving image, the source image privacy is
still preserved. Similarly, we observe that the inclusion of
theLLPIPS (L✓) andLexpr (E✓) significantly improves
all the metrics, especially the FID and facial Landmarks,
demonstrating their contribution to our framework to pre-
serve detailed expressions (like smiles) in the generated im-
ages. Note that we input the reference image instead of the
source image to our discriminator. We conducted experi-
ments using the source image instead of the reference in the
discriminator. The results show an improvement in the FID;
however, the face anonymization metrics (DIS) decrease
significantly, which suggests that the face information from
the source image is leaked to the generator. Hence, we only
use the reference image in our discriminator to maximize
privacy preservation. See our supplementary document for
results when using the source image in the discriminator.Method DeepPrivacy CIAGAN FIT IDeudemon RiDDLE Ours-LR Ours
FR↑ 0.783 0.671 0.812 0.819 0.776 0.795 0.734
CASIA ↑ 1.091 0.919 1.207 1.228 1.033 1.123 1.071
VGGFace2 ↑ 1.187 1.085 1.224 1.233 1.129 1.294 1.251
Table 2. Quantitative Results - DIS . We compare our method
against SOTA face de-identification methods on the CelebA-HQ
dataset using the DIS metrics (FR, CASIA, VGGFace2).
Method CIAGAN FIT DeepPrivacy RiDDLE Ours-LR Ours
FID↓ 32.611 30.331 23.713 15.389 34.161 29.218
FD↑MtCNN 0.992 1.000 1.000 1.000 1.000 1.000
Dlib 0.937 0.984 0.980 0.991 0.994 0.991
BB↓MtCNN 20.387 7.879 4.654 3. 824 5.358 5.314
Dlib 15.476 4.218 2.685 1.700 4.694 4.821
LM↓MtCNN 8.042 3.572 3.280 1.674 2.392 2.298
Dlib 8.930 4.047 2.896 1.512 2.749 2.716
Table 3. Quantitative Results - Detection. We compare
our method against SOTA face de-identification methods on the
CelebA-HQ dataset using the Face Detection (FD), Bounding Box
(BB), and Landmarks (LM) metrics.
4.2. Comparison with other methods
Quantitative Results. We quantitatively compare our
framework with several state-of-the-art (SOTA) face de-
identification approaches, including: CIAGAN [30], Face
Identity Transformer (FIT) [11], DeepPrivacy [14], IDeude-
mon [48], and RiDDLE [23]. We also compare the per-
formance of our proposed framework when using a low-
resolution (LR) camera instead of learning the lens. Specif-
ically, we simulate an LR camera (fixed optics parameters)
and perform the two-stage training described in section 3
without using the Eq. 4. Table 2 reports the results for
all the methods evaluated on the CelebA-HQ dataset using
theℓ2distance (DIS) metrics. As observed, our approach
achieved the best performance in the VGGFace2 metric and
a similar performance in the other metrics. Also, Ours ap-
proach using a low-resolution camera (Ours-LR) performs
better than our learned lens (Ours); however, LR is suscep-
tible to reverse engineering attacks as shown in section 4.3.
Additionally, we also compare our method against SOTA
using face detection (FD), bounding box detection (BB),
and landmark (LM) metrics in Table 3. The results show
that our approach does not outperform some SOTA meth-
ods, such as RiDDLE. The main reason is the significant
loss of high-frequency information on the acquired privacy-
preserving images, which makes it challenging to generate
images that preserve detailed expressions and face geom-
etry from the source image. Although including our pro-
posed loss functions and using heatmaps helps mitigate this
problem, a trade-off exists between achieved image degra-
dation and the generative capabilities of our framework.
However, even with such loss of information, our method
outperforms other methods, such as CIAGAN and FIT.
Qualitative Results . Figure 3 shows generated faces us-
ing our proposed framework. In the first row, we show the
source image, which is the input of our privacy-preserving
12125
Reference
Acquired
Source
Figure 3. Qualitative results . We qualitatively evaluate our pro-
posed face de-identification method using the CelebA-HQ dataset.
Ours
Source
CIAGAN
FIT
DeepPrivacy
RiDDLE
Figure 4. Qualitative Results . We compare our method against
SOTA methods: CIAGAN, FIT, DeepPrivacy, and RIDDLE.
camera. The image acquired by our camera is shown in the
second row. Our generative network generates new faces
using the acquired privacy-preserving image and different
reference images shown in the first column. We observe
that the generated images have the style extracted from
the reference images and face geometry of the source im-
ages. Furthermore, we qualitatively compare our approach
with SOTA methods in Fig. 4 using the CelebA-HQ dataset.
While our generated faces might not achieve the same level
of realism as those produced by methods like RiDDLE, they
effectively fulfill our primary goal of preserving privacy.
4.3. Robustness to Deconvolution
Assuming that an attacker performs an adversarial attack
and successfully captures the privacy-preserving image (see
Fig. 1 (b)). Then, the attacker could use a deconvolution
Source
Low-Resolution
21.89
23.75
Our Optimized Lens
15.69
12.02Figure 5. Robustness to Deconvolution. We use the SOTA dif-
fusion model in [19] to recover the person identity. We show the
PSNR between the original and recovered face in each image.
method to recover the identity of the person. To test the ro-
bustness of our optimized lens to deconvolution attacks, we
use a SOTA diffusion model (DDRM) proposed by Kawar
et al. [19] to recover the original image. DDRM is trained
to recover images in multiple scenarios, such as superres-
olution, inpainting, colorization, and deblurring. We apply
the pre-trained DDRM model on privacy-preserving images
captured by our optimized lens and a low-resolution camera
with16×16sensor size. As shown in Fig. 5, DDRM can re-
cover the identity of the person from the low-resolution with
high accuracy but fails to recover the face from the privacy-
preserving image acquired by our optimized lens. Similar to
previous works [12, 13], we also investigate the case where
an attacker has direct access to the camera; hence, it can
get the PSF by imaging a point of source light and perform
the attack using non-blind deconvolution methods ( e.g., the
Wiener Filter). See our supplementary document for results
on non-blind deconvolution attacks.
4.4. Human Evaluation for Privacy Protection
We conduct a human evaluation to assess the effectiveness
of privacy protection given by our approach under four dif-
ferent scenarios. In the first scenario, we randomly se-
lect five original face images alongside their correspond-
ing “low-resolution” and “blurred” image versions. Here,
the term “blurred” refers to the privacy-preserving images
acquired through our optimized lens; then, we display the
images to 106 individuals and ask them to judge to what
degree they believe that the identity of the person is pro-
tected considering the following options: Not Protected ,
Slightly Protected ,Moderately Protected , and Highly Pro-
tected . We found that only 12.76% of the surveyed people
consider that the “low-resolution” face image is Highly Pro-
tected while 56.00% consider that the “blurred” face image
isHighly Protected . In the second scenario, we generate a
new face image with our face de-identification framework
and show it alongside the other five face images, where one
of them corresponds to the current person we want to pro-
tect (source image). Then, we asked the 106 individuals to
identify the source image. The responses show that 91.82%
of the people fail to identify the source image, showing the
12126
Camera 2
Objetive Lens
Camera 1
Lens 1
Phase Mask
Real PSF
Beam Spliter
Deformable Mirror
Lens 3
Lens 2
RiDDLE
Ours Proposed Face De-identi ﬁcationFigure 6. Proof-of-concept optical system . (Top) Experimen-
tal hardware setup for our privacy-preserving approach. (Bottom)
Qualitative results on images acquired by our prototype camera
and comparison with the RiDDLE method.
effectiveness of our method. Refer to our supplementary
document for details about our human evaluation study.
4.5. Hardware Experiments
We developed a real camera prototype in the optical lab-
oratory to validate our proposed approach; see Fig. 6
(top). Our prototype includes an additional side infor-
mation branch to obtain the original images without pri-
vacy protection, which are used as ground truth to esti-
mate DIS metrics. We emulate the lens designed with our
framework using a deformable Thorlabs mirror (DMH40-
P01) in a 4f optical system built with 50mm achromatic
lenses and two Canon EOS M50 mirrorless cameras as sen-
sors. Due to the deformable mirror limitations, we are re-
stricted to using only p= 15 Zernike coefficients. There-
fore, considering this limitation, we first simulate the first
stage in our proposed framework (Sec. 3.3.1) to obtain
the simulated coefficients. Then, we load the learned co-
efficients to the deformable mirror and calibrate the optical
setup. After calibration, we obtained the following learned
Zernike coefficients: {β1, β2, β3= 0.0, β4=−0.83, β5=
0.0, β6=−0.31, β7= 0 .08, β8=−0.69, β9=
0.0, β10= 0.0, β11= 0.0, β12=−0.67, β13= 0.0, β14=
0.18, β15= 0.0}. We acquired 17 short face videos, each
lasting between 15 to 30 seconds, to capture the same per-
son with different facial expressions and head positions. Af-
ter preprocessing, we obtain 3700 face images, which we
split into two sets of 3000 and700images for finetuning and
testing, respectively. Specifically, we finetune the Heatmap
Regression Network Uand the Generator Gof our model
to adapt them to our acquired dataset. To evaluate the ef-MethodDIS↑
FR CASIA VGGFace2
RiDDLE 0.798 0.958 1.224
Ours-LR 0.808 1.141 1.310
Ours 0.792 1.156 1.334
Table 4. Quantitative results - Prototype camera. We quanti-
tatively evaluate the performance of our proposed approach using
the privacy-preserving data acquired by our prototype camera and
compare it against RiDDLE, which uses traditional images.
fectiveness of our proposed method on the data obtained in
our lab, we use the DIS metric to realize a comparison with
the SOTA RiDDLE model [23] over the collected testing
set, see Tab. 4. Furthermore, we also show some quali-
tative comparisons in Fig. 6 (bottom). These visual and
quantitative results demonstrate that our low-resolution ap-
proach and proposed learned camera are effective for face
anonymization in real-world scenarios. Please refer to our
supplementary material for a qualitative comparison when
using a low-resolution camera with data acquired in our lab.
5. Conclusion
We presented a novel hardware-level face de-identification
approach that proposes a solution to close the security
gap between image acquisition and algorithms in tradi-
tional software-level methods. To ensure training stability
and efficiency, we propose a two-stage optimization pro-
cess. First, we leverage optics learning to capture privacy-
preserving images while preserving facial features for face
heatmap regression. Then, we employ a generative adver-
sarial network to generate new synthetic faces that preserve
facial expression from the source image but style and face
appearance from a reference image with no privacy concern.
Specifically, our proposed approach offers two key advan-
tages over existing methods as Hardware-Level Security
mitigating the vulnerability to sniffing attacks that can com-
promise privacy protection, and High-Quality Face Gener-
ation using our proposed generative framework for privacy-
preserving images acquired with our camera.
Limitations. In our prototype camera, the deformable mir-
ror is the main limitation, which only uses 15 Zernike Poly-
nomials, limiting the scene’s level of distortion. Addition-
ally, generating faces using only our privacy-preserving im-
ages is still challenging for the generative model; hence,
some artifacts could appear in the generated faces.
Acknowledgment. This work was supported by the King
Abdullah University of Science and Technology (KAUST)
Office of Sponsored Research through the Visual Com-
puting Center (VCC) funding, the SDAIA-KAUST Cen-
ter of Excellence in Data Science and Artificial Intelli-
gence (SDAIA-KAUST AI), and the Universidad Industrial
de Santander (UIS), Colombia under the research projects
VIE-3735 and VIE-3924.
12127
References
[1] Shafiq Ahmad, Pietro Morerio, and Alessio Del Bue. Person
re-identification without identification via event anonymiza-
tion. In IEEE International Conference on Computer Vision
(ICCV) , 2023. 2
[2] Paula Arguello, Jhon Lopez, Carlos Hinojosa, and Henry Ar-
guello. Optics lens design for privacy-preserving scene cap-
tioning. In IEEE International Conference on Image Pro-
cessing (ICIP) . IEEE, 2022. 3
[3] Simone Barattin, Christos Tzelepis, Ioannis Patras, and Nicu
Sebe. Attribute-preserving face dataset anonymization via
latent code optimization. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , 2023. 1, 2
[4] Max Born and Emil Wolf. Principles of optics: electromag-
netic theory of propagation, interference and diffraction of
light. Elsevier, 2013. 3
[5] Adrian Bulat and Georgios Tzimiropoulos. How far are we
from solving the 2d & 3d face alignment problem? (and a
dataset of 230,000 3d facial landmarks). In IEEE Interna-
tional Conference on Computer Vision (ICCV) , 2017. 4
[6] Jingyi Cao, Bo Liu, Yunqian Wen, Rong Xie, and Li Song.
Personalized and invertible face de-identification by disen-
tangled identity information manipulation. In IEEE Inter-
national Conference on Computer Vision (ICCV) , 2021. 2,
5
[7] Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and An-
drew Zisserman. Vggface2: A dataset for recognising faces
across pose and age. In 13th IEEE international conference
on automatic face & gesture recognition . IEEE, 2018. 6
[8] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha.
Stargan v2: Diverse image synthesis for multiple domains.
InIEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR) , 2020. 2, 4, 5
[9] Adam Geitgey. Face recognition. https://github.
com/ageitgey/face_recognition , 2018. 5
[10] Joseph W Goodman. Introduction to Fourier optics . Macmil-
lan Learning, 4 edition, 2017. 3
[11] Xiuye Gu, Weixin Luo, Michael S Ryoo, and Yong Jae Lee.
Password-conditioned anonymization and deanonymization
with face identity transformers. In European Conference on
Computer Vision (ECCV) . Springer, 2020. 6
[12] Carlos Hinojosa, Juan Carlos Niebles, and Henry Arguello.
Learning privacy-preserving optics for human pose estima-
tion. In IEEE International Conference on Computer Vision
(ICCV) , 2021. 1, 3, 7
[13] Carlos Hinojosa, Miguel Marquez, Henry Arguello, Ehsan
Adeli, Li Fei-Fei, and Juan Carlos Niebles. Privhar: Recog-
nizing human actions from privacy-preserving lens. In Eu-
ropean Conference on Computer Vision (ECCV) . Springer,
2022. 3, 7
[14] H ˚akon Hukkel ˚as, Rudolf Mester, and Frank Lindseth.
Deepprivacy: A generative adversarial network for face
anonymization. In International symposium on visual com-
puting . Springer, 2019. 1, 2, 6
[15] Baowei Jiang, Bing Bai, Haozhe Lin, Yu Wang, Yuchen Guo,
and Lu Fang. Dartblur: Privacy preservation with detectionartifact suppression. In IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , 2023. 1, 2
[16] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of gans for improved quality, stability,
and variation. arXiv preprint arXiv:1710.10196 , 2017. 5
[17] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , 2019. 5
[18] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improving
the image quality of stylegan. In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2020. 2
[19] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming
Song. Denoising diffusion restoration models. Advances in
Neural Information Processing Systems (NeurIPS) , 2022. 7
[20] Vahid Kazemi and Josephine Sullivan. One millisecond face
alignment with an ensemble of regression trees. In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR) , 2014. 6
[21] Sudhakar Kumawat and Hajime Nagahara. Privacy-
preserving action recognition via motion difference quan-
tization. In European Conference on Computer Vision
(ECCV) . Springer, 2022. 2
[22] Vasudevan Lakshminarayanan and Andre Fleck. Zernike
polynomials: a guide. Journal of Modern Optics , 2011. 3
[23] Dongze Li, Wei Wang, Kang Zhao, Jing Dong, and Tie-
niu Tan. Riddle: Reversible and diversified de-identification
with latent encryptor. In IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , 2023. 2, 5, 6, 8
[24] Chi Liu, Tianqing Zhu, Jun Zhang, and Wanlei Zhou. Pri-
vacy intelligence: A survey on image privacy in online social
networks. ACM Computing Surveys , 2022. 1, 2
[25] Ming-Yu Liu, Xun Huang, Arun Mallya, Tero Karras, Timo
Aila, Jaakko Lehtinen, and Jan Kautz. Few-shot unsuper-
vised image-to-image translation. In IEEE International
Conference on Computer Vision (ICCV) , 2019. 4
[26] Rosanne Liu, Joel Lehman, Piero Molino, Felipe Pet-
roski Such, Eric Frank, Alex Sergeev, and Jason Yosinski.
An intriguing failing of convolutional neural networks and
the coordconv solution. Advances in neural information pro-
cessing systems (NeurIPS) , 2018. 4
[27] Yuchen Luo, Junwei Zhu, Keke He, Wenqing Chu, Ying
Tai, Chengjie Wang, and Junchi Yan. Styleface: Towards
identity-disentangled face generation on megapixels. In Eu-
ropean Conference on Computer Vision (ECCV) . Springer,
2022. 2
[28] Qi Mao, Hsin-Ying Lee, Hung-Yu Tseng, Siwei Ma, and
Ming-Hsuan Yang. Mode seeking generative adversarial net-
works for diverse image synthesis. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) , 2019. 5
[29] Maxim Maximov, Ismail Elezi, and Laura Leal-Taixe. Cia-
gan: Conditional identity anonymization generative adver-
sarial networks. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) , 2020. 1, 2
[30] Maxim Maximov, Ismail Elezi, and Laura Leal-Taix ´e. Cia-
gan: Conditional identity anonymization generative adver-
12128
sarial networks. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) , 2020. 6
[31] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin.
Which training methods for GANs do actually converge?
Inthe 35th International Conference on Machine Learning .
PMLR, 2018. 4
[32] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-
glass networks for human pose estimation. In Computer
Vision–ECCV 2016: 14th European Conference. Proceed-
ings, Part VIII 14 . Springer, 2016. 4
[33] Tribhuvanesh Orekondy, Mario Fritz, and Bernt Schiele.
Connecting pixels to privacy and utility: Automatic redac-
tion of private information in images. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) , 2018.
2
[34] Francesco Pittaluga and Sanjeev J Koppal. Privacy preserv-
ing optics for miniature vision sensors. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) , 2015.
1, 2
[35] Francesco Pittaluga and Sanjeev Jagannatha Koppal. Pre-
capture privacy for small vision sensors. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence (TPAMI) ,
2016. 2
[36] Siddharth Ravi, Pau Climent-P ´erez, and Francisco Florez-
Revuelta. A review on visual privacy preservation techniques
for active and assisted living. Multimedia Tools and Appli-
cations , 2023. 1, 2
[37] Michael Ryoo, Brandon Rothrock, Charles Fleming, and
Hyun Jong Yang. Privacy-preserving human activity recog-
nition from extreme low resolution. In AAAI conference on
artificial intelligence , 2017. 2
[38] Michael Ryoo, Kiyoon Kim, and Hyun Yang. Extreme low
resolution activity recognition with multi-siamese embed-
ding learning. In AAAI conference on artificial intelligence ,
2018. 1, 2
[39] Florian Schroff, Dmitry Kalenichenko, and James Philbin.
Facenet: A unified embedding for face recognition and clus-
tering. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , 2015. 5
[40] Vincent Sitzmann, Steven Diamond, Yifan Peng, Xiong Dun,
Stephen Boyd, Wolfgang Heidrich, Felix Heide, and Gor-
don Wetzstein. End-to-end optimization of optics and image
processing for achromatic extended depth of field and super-
resolution imaging. ACM Transactions on Graphics (TOG) ,
2018. 1, 3
[41] Vinkle Srivastav, Afshin Gangi, and Nicolas Padoy. Hu-
man pose estimation on privacy-preserving low-resolution
depth images. In International conference on medical im-
age computing and computer-assisted intervention (MIC-
CAI) . Springer, 2019. 1
[42] Zaid Tasneem, Giovanni Milione, Yi-Hsuan Tsai, Xiang
Yu, Ashok Veeraraghavan, Manmohan Chandraker, and
Francesco Pittaluga. Learning phase mask for privacy-
preserving passive depth estimation. In European Confer-
ence on Computer Vision (ECCV) . Springer, 2022. 1, 3
[43] Razvan Viorescu et al. 2018 reform of eu data protection
rules. European Journal of Law and Public Administration ,
2017. 1[44] Paul V oigtlaender, Michael Krause, Aljosa Osep, Jonathon
Luiten, Berin Balachandar Gnana Sekar, Andreas Geiger,
and Bastian Leibe. Mots: Multi-object tracking and segmen-
tation. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , 2019. 1
[45] Hui-Po Wang, Tribhuvanesh Orekondy, and Mario Fritz. In-
foscrub: Towards attribute privacy by targeted obfuscation.
InIEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR) , 2021. 2
[46] Ping Wang, Lishun Wang, and Xin Yuan. Deep optics for
video snapshot compressive imaging. In IEEE International
Conference on Computer Vision (ICCV) , 2023. 3
[47] Xinyao Wang, Liefeng Bo, and Li Fuxin. Adaptive wing
loss for robust face alignment via heatmap regression. In The
IEEE International Conference on Computer Vision (ICCV) ,
2019. 4, 5
[48] Yunqian Wen, Bo Liu, Jingyi Cao, Rong Xie, and Li Song.
Divide and conquer: a two-step method for high quality face
de-identification with model explainability. In IEEE Interna-
tional Conference on Computer Vision (ICCV) , 2023. 1, 2,
5, 6
[49] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learn-
ing face representation from scratch. arXiv preprint
arXiv:1411.7923 , 2014. 5
[50] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao.
Joint face detection and alignment using multitask cascaded
convolutional networks. IEEE signal processing letters ,
2016. 6
[51] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) , 2018. 5
[52] Ruoyu Zhao, Yushu Zhang, Tao Wang, Wenying Wen, Yong
Xiang, and Xiaochun Cao. Visual content privacy protection:
A survey. arXiv preprint arXiv:2303.16552 , 2023. 1, 2
12129
