Bridging the Gap: A Uniﬁed Video Comprehension Framework
for Moment Retrieval and Highlight Detection
Yicheng Xiao1†, Zhuoyan Luo1†
Yong Liu1, Yue Ma1, Hengwei Bian2, Yatai Ji1, Yujiu Yang1B, Xiu Li1B
1Tsinghua Shenzhen International Graduate School, Tsinghua University
2Carnegie Mellon University
Abstract
Video Moment Retrieval (MR) and Highlight Detection
(HD) have attracted signiﬁcant attention due to the grow-
ing demand for video analysis. Recent approaches treat MR
and HD as similar video grounding problems and address
them together with transformer-based architecture. How-
ever, we observe that the emphasis of MR and HD differs,
with one necessitating the perception of local relationships
and the other prioritizing the understanding of global con-
texts. Consequently, the lack of task-speciﬁc design will in-
evitably lead to limitations in associating the intrinsic spe-
cialty of two tasks. To tackle the issue, we propose a Uniﬁed
Video COM prehension framework (UVCOM) to bridge the
gap and jointly solve MR and HD effectively. By performing
progressive integration on intra and inter-modality across
multi-granularity, UVCOM achieves the comprehensive un-
derstanding in processing a video. Moreover, we present
multi-aspect contrastive learning to consolidate the local
relation modeling and global knowledge accumulation via
well aligned multi-modal space. Extensive experiments on
QVHighlights, Charades-STA, TACoS, YouTube Highlights
and TVSum datasets demonstrate the effectiveness and ra-
tionality of UVCOM which outperforms the state-of-the-art
methods by a remarkable margin. Code is available at
https://github.com/EasonXiao-888/UVCOM .
1. Introduction
Video has emerged as a highly favored multi-medium for-
mat on the internet with its diverse content. This signiﬁ-
cant surge in online video encourages users to adjust their
strategies for accessing desired video contents. Instead
of spending time-consuming efforts inspecting the whole
video, they are more inclined to directly obtain particu-
lar clips of interest through language descriptions. This
†Equal contribution.
BCorresponding author ( {yang.yujiu, li.xiu }@sz.tsinghua.edu.cn).
Query:A video collection of wonderful places to visit
HeadHeadTransformerEncoderTransformerEncoderVideoTextVideoTextEncoderMultiModalFusionEncoder0s
SaliencyScore150sMomentSpan
01MomentRetrievalHighlightDetectionFigure 1. Illustration of the intrinsic characteristics of Moment
Retrieval and Highlight Detection. We visualize the attention
map of the same video under two tasks. The attention map for MR
takes on strip-like patterns, indicating the emphasis of local rela-
tions. In contrast, it is in band-like format for HD, which signiﬁes
the focus on global information.
shift in user preference gives rise to two signiﬁcant research
topics: Video Moment Retrieval [ 8,26,40,60,61], fo-
cuses on locating the speciﬁc moment, and Highlight Detec-
tion [ 1,12,52,56,58], is dedicated to identifying segments
of high saliency.
Actually, it is apparent that two tasks share many com-
mon characteristics, e.g., identifying relevant video seg-
ments in response to textual expressions. In light of the
above, Lei et.al. [21] ﬁrst proposes a novel dataset named
QVHighlights and a basic framework called Moment-
DETR to jointly solve both tasks. UMT [ 27] incorporates
extra audio modality and QD-DETR [ 31] produces text-
query dependent video representation to achieve better per-
formance. The above-mentioned methods simply model
MR and HD as a multi-task problem and mainly concen-
trate on utilizing non-speciﬁc strategy to solve them. In
particular, they all adopt a straightforward way to train
and optimize both tasks together with general design, e.g.,
transformer-based models. However, we revisit the charac-
teristics of MR/HD and discover that there exists a gap be-
tween them as illustrated in Fig. 1. which leads to the chal-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
18709
lenge in consistent performance on both tasks, i.e., achiev-
ing precise moment localization and accurate highlight-ness
estimation simultaneously.
Therefore, we consider that the design of framework
should follow two principles to alleviate the above weak-
ness: 1) Local Relation Activation: MR necessitates the
understanding of local relationships within the video to ac-
curately localize speciﬁc segments. 2) Global Knowledge
Accumulation: The objective of HD is to ﬁt the saliency
distribution of the entire video, emphasizing the impor-
tance of global context (in Fig. 1). Based on the princi-
ples, we propose a Uniﬁed Video Comprehension Frame-
work (UVCOM) to seamlessly integrate the emphasis of
MR and HD, which effectively bridges the gap and achieves
great performance on both tasks consistently. Speciﬁcally,
we ﬁrst design a novel Comprehensive Integration Mod-
ule (CIM) to progressively facilitate the integration on in-
tra and inter-modality across multi-granularity. CIM ﬁrst
efforts to propagate the aggregated semantic phrases from
the text into the visual feature to realize local relationship
perception. Then, it accumulates global information from
video by utilizing the moment-awareness feature as an in-
termediary. With a comprehensive view of the entire video,
CIM facilitates the understanding of particular intervals and
highlight contents, which is beneﬁcial to identify the de-
sired moment and non-related ones. Furthermore, we intro-
duce a multi-aspect contrastive learning which incorporates
clip-text alignment to consolidate the local relation model-
ing, and video-linguistic discrimination to enhance the qual-
ity of accumulated global information.
We conduct extensive experiments on ﬁve popular
MR/HD benchmarks to validate the effectiveness of our
framework and the results show that UVCOM notably out-
performs existing methods for all benchmarks.
Overall, our contributions are summarized as follows:
•Based on our investigation into the emphasis of Moment
Retrieval and Highlight Detection, we present two prin-
ciples for framework design. Guided by them, we pro-
pose a Uniﬁed Video Comprehension Framework called
UVCOM to effectively bridge the gap between two tasks.
•In UVCOM, a Comprehensive Integration Module (CIM)
is designed to perform progressive intra and inter-
modality interaction across multi-granularity, which
achieves locality perception of temporal and multi-modal
relationships as well as global knowledge accumulation
of the entire video.
•Without bells and whistles, our method outperforms all
existing state-of-the-art methods by a remarkable margin,
e.g.,+5.97% in R1@0.7 for MR than UniVTG [ 24] on
TACoS [ 37] and +3.31% in HIT@1 for HD than QD-
DETR [ 31] on QVHighlights [ 21].2. Related Work
Moment Retrieval. Moment Retrieval is a task that aims
at retrieving the target moment, i.e., one [ 8] or many [ 20]
continuous intervals in a video given the text descrip-
tion. Generally, the model will focus more on the re-
lationship across adjacent frames for better localization.
Previous works retrieve video intervals into two perspec-
tives: proposal-based and proposal-free. The proposal-
based methods [ 8,9,15,44,50] follow the propose-then-
rank pipeline, where they ﬁrst generate candidate propos-
als then rank them based on matching scores. Liu et
al.[25] and Hendricks et al. [15] utilize sliding windows
to scan the entire video for candidate proposals genera-
tion and calculate the similarity with textual embedding
for selection. On the other hand, the proposal-free meth-
ods [10,22,32,34,45,59,62] directly regress the start and
end timestamp via video-text interaction. Yuan et al. [59]
and Mun et al. [32] generate the temporal coordinates of
sentence by multi-modal co-attention mechanism. Further-
more, Rodriguez et al. [34] utilizes a simple dynamic ﬁlter
instead of cross attention to match video and text embed-
ding.
Highlight Detection. Highlight Detection aims to iden-
tify highlights or important segments with high potential
appeal in a video. Compared with Moment Retrieval, It
is necessary for the model to associate the whole video
content for ﬁtting saliency distribution of each clip. Many
prior works [ 12,52,56,56,58] adopt ranking formula-
tion where they rank the important segments with higher
score. Video2Gif [ 12] trains a generic highlight predictor
to produce GIF from videos. Rochan et al. [39] designs
a task-speciﬁc highlight detectors to automatically create
highlights from the user history. Recently, Badamdorj et
al.[1] elaborates on fusing visual and audio content to gen-
erate better video representations.
MR and HD share many similar properties. Moment-
DETR [ 21] puts forward a novel dataset which ﬁrst includes
two tasks and provides a simple DETR-based [ 2] network.
To improve the query quality, UMT [ 27] proposes to adopt
audio, visual and text content for query generation. Fur-
thermore, QD-DETR [ 31] exploits the textual information
by involving video-text pair negative relationship learning,
achieving greater performance. However, previous meth-
ods simply train and optimize two tasks without considering
the different emphasis of each task. To address this issue,
we propose a novel uniﬁed framework UVCOM that effec-
tively associates the speciality of MR and HD to achieve
comprehensive understanding.
3. Method
Given a video of Lclips {v1,v2,...,v l}and a textual ex-
pression of Nwords {e1,e2,...,e n}, the goal of MR is
18710
…MRDecoderDual Branches Intra-Modality Aggregation……RBF EM-AttClip-to-Moment BranchWord-to-Phrase BranchLocal Relation Perceptionvideo-levelGlobalKnowledge Accumulation…HDProjectionMulti-TaskPredictionHeads
01MRHDTransformerEncoder…………RBF EM-AttBMRWTemporalConvBlockComprehensiveIntegrationModule
Early-FusionMan and womanare dancing together
clip-levelword-levelMomentlevel
……Figure 2. Overview of UVCOM. Based on the exploration of MR and HD, we propose a uniﬁed video comprehension framework guided
by the design principles. Speciﬁcally, the model takes a video with language description as input. After encoding and early-fusion process,
we design a Comprehensive Integration Module (CIM) to achieve subsequent progressive integration on intra and inter-modality across
multi-granularity. Finally, the multi-task heads output the moment spans for MR and saliency scores for HD.
to localize the most relevant moment with the center coor-
dinate and duration, while HL is to generate the saliency
score distribution for the whole video.
3.1. Visual-Text Encoding
Visual Encoder. Following previous works [ 21,24,27,
31], we utilize the pretrained backbone, e.g., SlowFast [ 6],
video encoder of CLIP [ 36] and I3D [ 3] to extract visual
features Fv2RL⇥Dof the video. Note that D demotes the
channel.
Language Encoder. Simultaneously, text encoder of
CLIP is adopted to encode the linguistic expression into the
textual embedding Ft2RN⇥D.
With the visual and textual features, we apply a bidirec-
tional transformer-based encoder [ 13,29,55] to perform
the early fusion. It coarsely encodes features in different
modalities and outputs preliminary aligned visual and tex-
tual representations.
3.2. Comprehensive Integration Module
After getting the visual and textual representations, we de-
sign a Comprehensive Integration Module (CIM) to per-
form progressive intra and inter-modality integration across
multi-granularity. Speciﬁcally, we leverage Expectation-
Maximum (EM) Attention [ 23] on associating inner-
modality content to generate the moment-wise visual fea-
tures and phrase-wise textual features, respectively. Then
we propose Local Relation Perception (LRP) module to
unify temporal relationship modeling and inter-modality fu-
sion, which reformulates the temporal and modality inter-
connection to enhance the locality perception. Finally, we
utilize a standard encoder to produce the video-wise feature
by integrating the correlation between moment and clip-
wise visual features.
Dual Branches Intra-Modality Aggregation. A video
usually contains more than one event and irrelevant back-ground scenes. The same scenario happens in textual de-
scriptions where insigniﬁcant words and unconstrained ex-
pressions may cause potential ambiguity. To tackle the
problem, we propose to utilize RBF-kernel based EM At-
tention [ 17,23] to aggregate the clip/word-level features.
As shown in Fig. 2, it is a dual-branches structure. The clip-
to-moment branch aims at incorporating the relationship of
each clip to enhance the desired event representations while
suppressing the background noise. Meanwhile, the word-
to-phrase branch is to emphasize the referred moment de-
scription by accumulating contextual information.
Speciﬁcally, we ﬁt the distribution of FvandFtby a sep-
arated Gaussian Mixture Model [ 38] to generate the com-
pact moment and phrase-level representations via the cen-
troid of Gaussians. Taking Fvas an example, we utilize a
linear superposition of nvGaussians to capture the statistics
offi
v2RD(thei-th snippet of Fv):
p(fi
v)=nvX
k=1zv
kN(fi
v|µk,⌃k), (1)
where zv
k2R,µk2RDand⌃k2RD⇥Ddenote the
weight, mean and covariance of k-th Gaussian basis for the
clip-to-moment branch. We substitute the covariance with
an identity matrix Ifor simpliﬁcation and employ the ra-
dial basis function (RBF Kernel) K(fi
v,µk)to estimate the
posterior probability N(fi
v|µk,I):
K(fi
v,µk)=exp(    fi
v µk  2
2), (2)
where  > 0is an adjustable hyper-parameter to control
the distribution. Afterwards, at t-th iteration, we update the
weight Z(t)2RL⇥nvin the E Step and re-estimate µ(t)2
Rnv⇥Din the M step, which can be formulated as:
µ(t)= Norm 1(Z(t))TFv,t2{1,...,T }.(3)
Furthermore, in contrast to conventional cluster methods
that only involve iterative update, the initialized means µ(0)
18711
…
Phrase-levelTextFeatureConv1DKernelSize1Conv1DKernelSize3Conv1DKernelSize1Affinity……Affinity…Iteration!…Iteration"+×
+++××+Clip-levelVisualFeatureMultipleIterations×
Figure 3. Illustration of Local Relation Perception (LRP)
module. We ﬁrst process the visual feature Fvwith a Conv1D
Block. Then we develop a Bidirectional Modality Random Walk
(BMRW) algorithm to exploit the power of ﬁne-grained multi-
modal interaction. The afﬁnity Zis generates by scaled dot prod-
uct:Z= zF0(0)
v(F(0)
p)>.
we set are learnable. Therefore, they can effectively capture
the feature distribution of the dataset through the standard
back-propagation.
After titerations, we obtain the ﬁne-grained moment-
wise representation Fmfrom µ(t)which fully aggregates
the contextual information. Similarly, we operate the above
steps on word-to-phrase branch to generate the phrase-level
linguistic feature Fp2Rnt⇥D, where ntindicates the num-
ber of Gaussian basis in word-to-phrase branch.
Local Relation Perception. Previous methods [ 21,27,
31] directly perform cross-modal fusion between clip and
word-level features, disregarding the temporal relation and
valuable semantic interaction across different granularities.
Without the information from adjacent clips, the simple and
coarse clip-word fusion will easily deviate the model from
focusing on the relevant boundary clips, causing incorrect
localization. To address the aforementioned weakness, we
design a Local Relation Perception (LRP) module to ex-
cavate both temporal and inter-modality relationships. As
shown in Fig. 2, we ﬁrst utilize a temporal convolution
block to improve the locality perception of clip-level fea-
tures, which can be formulated as:
F0
v= Conv ( Fv)+Fv. (4)
Since simply incorporate clip-level relation may introduce
local redundancy, we leverage ﬁne-grained inter-modal in-
teraction to re-calibrate the attention for activating the rel-
evant moments. Intuitively, a straightforward approach isto utilize cross-attentive mechanism [ 27,31] to perform
inter-modal interaction. Nevertheless, the complex sce-
nario in an untrimmed video, e.g., footage transitions and
irrelevant events, will increase the likelihood of attention
drift which leads to the undesirable local activation. More-
over, although phrase-wise linguistic features specify re-
ferred moment description and alleviate the impact of noise
in contrast to word-wise one, it may potentially contribute
to attention drift due to the irrelevant accumulated words.
Therefore, inspired by [ 11,17,33], we design a bidirec-
tional modality random walk (BMRW) algorithm to miti-
gate the mentioned drawbacks and fully exploit the power
of the ﬁne-grained multi-modal interaction. It propagates
the textual prior into the visual features for highlighting the
corresponding local context and suppressing unrelated ones.
Simultaneously, linguistic features are reﬁned through the
incorporation of updated visual content. As shown in Fig. 3,
there are multiple iterations in BMRW where two modali-
ties features learn collaboratively in visual-linguistic shared
embedding space until convergence.
Formally, we ﬁrst deﬁne the F0v,Fpas initial features
F0(0)
v,F(0)
pat0-th iteration and formulate afﬁnity Zby
scaled dot product: Z= zF0(0)
v(F(0)
p)>, where  zis the
scaling factor. At t-th iteration, the phrase-wise linguistic
feature F(t)
pis updated by the original feature F(0)
pand the
visual output F0(t 1)
vfrom previous iteration:
F(t)
p=!Norm1( Z)>F0(t 1)
v+( 1  !)F(0)
p,(5)
Subsequently, it is projected into the temporal-awareness
feature F0(t)
v:
F0(t)
v=!ZF(t)
p+( 1  !)F0(0)
v, (6)
where !2(0,1)is the factor which controls the degree of
modalities fusion. Then, we substitute F(t)
pinto Eq. ( 6) to
derive the iterative update formula of F0(t)
v:
F0(t)
v=(!2A)tF0(0)
v+(1 !)t 1X
i=0(!2A)i(!ZF(0)
p+F0(0)
v),
(7)
where Adenotes ZNorm1( Z)>. Intuitively, the moment-
speciﬁc regions of visual features can be fully activated by
the guidance of textual features after multiple iterations.
Moreover, to avoid the potential issue of unexpected gradi-
ent and high computation cost, we use an approximate infer-
ence function based on Neumann Series [ 30] when t!1:
F0(1)
v=( 1  !)(I !2A) 1(!ZF(0)
p+F0(0)
v).(8)
In this manner, the model realizes a synergistic tem-
poral and inter-modality relation integration and generates
a more comprehensive visual representation Fnew
v,i.e.,
F0(1)
vin Eq. ( 8).
18712
Global Knowledge Accumulation. As illustrated in
Fig. 1, Highlight Detection prioritizes global information
of videos. QD-DETR [ 31] uses a saliency token to cap-
ture general information. However, the input-agonist design
might cause the inferior perception of the text-related inter-
vals due to the non-referential search area. To mitigate the
concern, we propose to use the moment-aware feature as
intermediate guidance to accumulate the global knowledge
of a video. Speciﬁcally, we derive the most relevant snip-
petF0mby measuring the similarity between the moment-
wise Fmand phrase-wise embeddings Fp. Then, a stack of
transformer encoder layers [ 46] are utilized to excavate the
correlation between F0mandFnew
v. The overall process is:
Fg
v,Fl
v=Encoder (Concat [F0
m,Fnew
v]). (9)
Consequently, the semantic snippet is obliged to focus on
the referred moment and suppress the non-target response,
which eventually produces the video-wise feature Fg
v2
R1⇥D. In addition, Fl
v2RL⇥Dis greatly enriched by the
supplement of global information.
3.3. Multi-Aspect Contrastive Learning
As discussed in Sec. 3.2, CIM can better accomplish lo-
cal relation enhancement in temporal and inter-modality as
well as global knowledge accumulation of a video. It is an-
ticipated that the explicit supervision of each objective will
further consolidate the effectiveness. To this end, we intro-
duce multi-aspect contrastive learning in two folds:
Clip-Text Alignment. This loss bridges the semantic gap
between the textual expression and the clip-level features,
which further improves the quality of local relation model-
ing. Speciﬁcally, we ﬁrst average Ftto get the sentence-
level textual embedding F0t2R1⇥Dand then measure the
relevance with clip-level visual representation Fnew
v:
Sct=Fnew
v·F0>
t
kFnewvk · kF0tk. (10)
Finally, we compute the contrastive loss by matrix multipli-
cation:
Lcta= LogSoftmax ( Sct)·Gct, (11)
where Gctis annotated to 1for relevant clips and 0for oth-
ers.
Video-Linguist Discrimination. It aims at constructing
the ﬁne-grained multi-modal joint space where video-level
visual feature {Fg
v(i)}B
i=1closens relevant sentence-level
textual representation {F0t(i)}B
i=1while distances unrelated
ones within a batch B. Similar to [ 28,36,51], the whole
process can be formulated as:
Lvld= BX
i=1Logexp⇣
Fg
v(i)·F0>
t(i)⌘
PB
j=1exp⇣
Fg
v(i)·F0>
t(j)⌘.(12)3.4. Prediction Heads and Loss Function
Multi-Task Prediction Heads. As depicted in Fig. 2,
there are two simple heads built on top of the Comprehen-
sive Integration Module for Moment Retrieval and High-
light Detection respectively. Similar to [ 18,21,31], Mo-
ment Retrieval Head comprises a standard transformer de-
coder [ 14,47,65] where we leverage Ftas the query to
generate a series of moment spans Pm. Highlight Detection
Head consists of two groups of single fully-connected layer
for linear projection. Accordingly, we get the prediction
saliency scores Ps2RL⇥1:
Ps=Fg
vw>
g·Fl
vw>
lp
d, (13)
where wgandwl2Rd⇥Dare learnable weights.
Total Loss. We supervise our framework by four groups
of training objective functions. For MR, L1loss and GIoU
loss are adopted to measure the disparity between GT mo-
ment Gmand prediction spans Pm:
LMR= gIoULgIoU(Pm,Gm)+ L1LL1(Pm,Gm).
(14)
Moreover, the loss functions for HD consist of margin rank-
ing loss Lmargin and rank-aware loss Lrankfollowing [ 31].
Both losses work in tandem to ensure the predicted saliency
scores Psconform to the ground truth scores Gs:
LHD= HD[Lmargin (Ps,Gs)+Lrank(Ps,Gs)](15)
Inspired by [ 5,31], we involve hard samples into training
process for diversifying the formulations of local and global
relationships of different video-text pairs. Brieﬂy, we cate-
gorize the lowest relevance between video and text as hard
samples and suppress their saliency scores Phard
sduring
training:
Lhard=  hardLog(1  Phard
s) (16)
In addition, the objective of multi-aspect contrastive learn-
ing promotes semantic associations between text descrip-
tions and visual contents of multi-granularity:
Lcon= ctaLcta+ vldLvld. (17)
Generally, the total loss is expressed as:
Ltotal=LHD+LMR+Lhard+Lcon. (18)
The above are hyper-parameters for balancing the losses.
4. Experiments
4.1. Datasets and Evaluation Metrics
Datasets. We evaluate our model on ﬁve prevalent
MR/HD benchmarks: QVHighlights [ 21], Charades-
STA [ 7], TaCoS [ 37], TVSum [ 42] and YouTube High-
lights [ 43]. Due to the space limitation, the details of each
datasets are included in the supplementary material.
18713
MR HD
R1 mAP  Very Good Method
@0.5@0.7@0.5@0.75Avg. mAP HIT@ 1
M-DETR [ 21] 52.89 33 .02 54 .82 29 .40 30 .73 35 .69 55 .60
UMT †[27] 56.23 41 .18 53 .83 37 .01 36 .12 38 .18 59 .99
UniVTG [ 24] 58.86 40 .86 57 .60 35 .59 35 .47 38 .20 60 .96
MH-DETR [ 54] 60.05 42 .28 60 .75 38 .13 38 .38 38 .22 60 .51
QD-DETR †[31]63.06 45 .10 63 .04 40 .14 0 .19 39 .04 62 .87
EaTR [ 18] 61.36 45 .79 61 .86 41 .91 41 .74 37 .15 58 .65
UVCOM 63.5547.4763.3742.6743.1839.7464.20
UVCOM † 63.8148.7064.4744.0143.2739.7964.79
With ASR Captions Pretrain
M-DETR [ 21] 59.78 40 .33 60 .51 35 .36 36 .14 37 .43 60 .17
UMT [ 27] 60.83 43 .26 57 .33 39 .12 38 .08 39 .12 62 .39
QD-DETR [ 31] 64.10 46 .10 64 .30 40 .50 40 .62 38 .52 62 .27
UVCOM 64.5348.3164.7843.6543.8039.9865.58
Table 1. Jointly MR and HD results on QVHighlights test split.
†indicates training with audio modality. With ASR Caption Pre-
train denotes models pretrained on ASR captions [ 21].
MethodCharades-STA TACoS
R1@0.5 R1@0.7 R1@0.5 R1@0.7
2D TAN [ 64] 46.02 27 .50 27.99 12 .92
VSLNet [ 62] 42.69 24 .14 23.54 13 .15
M-DETR [ 21] 53.63 31 .37 24.67 11 .97
QD-DETR [ 31] 57.31 32 .55 ––
UniVTG [ 24] 58.01 35 .65 34.97 17 .35
UVCOM 59.25 36.64 36.39 23.32
Table 2. MR results on Charades-STA test split and TACoS
test split . The pre-extracted features are from SlowFast [ 6] and
CLIP [ 36].
Metrics. Following [ 2,21,27], we measure the perfor-
mance of our model by the same criteria for QVhighlights,
Charades-STA, TACoS, YouTube Highlights and TVSum.
For descriptions of the metrics corresponding to datasets,
please see the supplementary material.
4.2. Implementation Details
Pre-extracted Features. For a fair comparison, we take
the same features of video, text and audio from corre-
sponding pretrained feature extractors, e.g., SlowFast [ 6],
CLIP [ 36], PANN [ 19]. For more details please refer to
supplementary material.
Training Settings. Our model is trained with AdamW op-
timizer where the learning rate is 1⇥10 4and weight decay
is1⇥10 4by default. The encoder of Global Knowledge
Accumulation and the decoder of Moment Retrieval Head
compose of three layers of transformer blocks. The coefﬁ-
cients for losses are set to  cta=0.5, hard=1, vld=
0.5, HD=1, gIoU=1, L1= 10 in default. Due to
space limitations, please see the supplementary material for
more training details.4.3. Main Result
QVHighlights. We compare our method to previous
methods on QVHighlights in Tab. 1. Beneﬁting from the
comprehensive understanding of the video, our UVCOM
achieves new state-of-the-art performance on different set-
tings and shows a signiﬁcant margin across all met-
rics. Speciﬁcally, our approach outperforms EaTR [ 18] by
2.25% on the average of all metrics. Incorporating with
video and audio modality, UVCOM yields a clear improve-
ment of 3.6%in R1@0.7, 4%in mAP@0.75 for MR and
2%in HID@1 for HD compared to QD-DETR [ 31]. Fur-
thermore, with ASR caption pretraining, UVCOM achieves
the greatest performance on more stringent metrics, e.g.,
43.8%in Avg. mAP for MR and 39.98% in Avg. mAP
for HD, demonstrating the effectiveness of our method.
Charades-STA & TACoS. In order to evaluate the per-
formance of our method in precise moment localization,
we report the results on Charades-STA and TACoS bench-
marks. As depicted in Tab. 2, UVCOM outperforms QD-
DETR [ 31] by about 4%R1@0.7 using SlowFast and CLIP
features in Charades-STA dataset while boosts 6%R1@0.7
than UniVTG [ 24] in TaCoS. It is worth noting that we also
validate our model surpasses the existing SOTA methods
using VGG features (see in supplementary material).
YouTube Highlights & TVSum. For Video Highlight
Detection, we conduct experiments on TVSum and
YouTube Highlights. Considering the fact that the scale
and scoring criteria of TVSum is small and inconsistent,
our method gains incoherently among domains. However,
in Tab. 4, it still boost an improvement of 1.3%in Avg.
mAP compared with the SOTA methods. As shown in
Tab.3, our method achieves 76.4% and 77.4% in Avg. mAP
without audio source under different settings. Note that the
features used in UniVTG [ 24] and UMT [ 27] on YouTube
Highlights are different. Therefore, we follow the same pro-
tocol of each for a fair comparison.
4.4. Ablation Study
In this section, we conduct a series of analysis experiments
on the val split of QVHighlights benchmark and train the
model from scratch without audio modality.
Component Analysis. We ﬁrst verify the effectiveness
of the proposed Comprehensive Integration Module (CIM)
and Multi-Aspect Contrastive Learning (MCL). As illus-
trated in Tab. 5, both of them brings improvement and
their combination contributes to better performance , i.e.,
+5.71% in Avg. mAP, which demonstrates the effective-
ness of the comprehensive understanding. To further in-
vestigate the validity of three modules involved in CIM,
we provide additional experiments on Dual Branches Intra-
Modality Aggregation (DBIA), Local Relation Perception
18714
Method Dog Gym. Par. Ska. Ski. Sur. Avg.
GIFs [ 12] 30.83 3 .55 4 .05 5 .43 2 .85 4 .14 6 .4
LSVM [ 43] 60.04 1 .06 1 .06 2 .03 6 .06 1 .05 3 .6
LIM-S [ 52] 57.94 1 .76 7 .05 7 .84 8 .66 5 .15 6 .4
SL-Module [ 53]70.85 3 .27 7 .27 2 .56 6 .17 6 .26 9 .3
MINI-Net †[16]58.26 1 .77 0 .27 2 .25 8 .76 5 .16 4 .4
TCG †[57] 55.46 2 .77 0 .96 9 .16 0 .15 9 .86 3 .0
Joint-V A †[1] 64.57 1 .98 0 .86 2 .07 3 .27 8 .37 1 .8
UMT †[27] 65.97 5 .28 1 .67 1 .87 2 .38 2 .77 4 .9
UniVTG [ 24] 71.87 6 .57 3 .97 3 .37 3 .28 2 .27 5 .2
UVCOM173.877.175.775.374.082.776.4
UVCOM266.577.482.878.774.284.677.4
Table 3. HD results of mAP on YouTube HL. †de-
notes using audio modality. 1and2indicate using the
same visual and textual features of UniVTG and UMT.Method VT VU GA MS PK PR FM BK BT DS Avg.
sLSTM [ 63] 41.14 6 .24 6 .34 7 .74 4 .84 6 .14 5 .24 0 .64 7 .14 5 .54 5 .1
LIM-S [ 52] 55.94 2 .96 1 .25 4 .06 0 .44 7 .54 3 .26 6 .36 9 .16 2 .65 6 .3
Trailer [ 48] 61.35 4 .66 5 .76 0 .85 9 .17 0 .15 8 .26 4 .76 5 .66 8 .16 2 .8
SL-Module [ 53]86.56 8 .77 4 .98 6 .279.06 3 .25 8 .97 2 .67 8 .96 4 .07 3 .3
MINI-Net †[16]80.66 8 .37 8 .28 1 .87 8 .16 5 .85 7 .87 5 .08 0 .26 5 .57 3 .2
TCG †[57] 85.07 1 .48 1 .97 8 .68 0 .27 5 .57 1 .67 7 .37 8 .66 8 .17 6 .8
Joint-V A †[1] 83.75 7 .37 8 .58 6 .18 0 .16 9 .27 0 .07 3 .097.467.57 6 .3
UniVTG [ 24] 83.98 5 .18 9 .080.18 4 .68 1 .47 0 .99 1 .773.56 9 .38 1 .0
UMT †[27] 87.58 1 .58 8 .27 8 .88 1 .587.076.08 6 .98 4 .479.683.1
QD-DETR [ 31]88.287.485.68 5 .08 5 .886.97 6 .491.38 9 .273.78 5 .0
UVCOM 87.691.691.486.786.986.976.992.387.475.686.3
Table 4. HD results of Top-5 mAP on TVSum. †denotes using audio modal-
ity. The 2-nd performance values are highlighted by underline .
CIM MCLMR HD
R1 R1 mAPmAP HIT@1@0.5 @0.7 Avg.
61.55 44 .84 40 .08 37 .10 62 .0
X 62.84 48 .77 43 .63 9 .33 62 .97
X 60.77 44 .06 40 .48 38 .81 62 .06
XX 65.10 51 .81 45 .79 40 .03 63 .29
Table 5. Effectiveness of the proposed modules.DBIA LRP GKAMR HD
R1 R1 mAPmAP HIT@1@0.5 @0.7 Avg.
60.77 44 .06 40 .48 38 .81 62 .06
X62.32 46 .71 41 .03 38 .73 62 .58
X 62.06 46 .45 41 .42 38 .57 62 .45
XX 63.74 49 .16 43 .45 39 .54 64.26
XX 64.71 50 .04 3 .69 39 .69 63 .16
XX 64.84 50 .04 4 .02 39 .58 64 .13
XXX 65.10 51 .81 45 .79 40 .03 63.29
Table 6. Effects of the components designed of proposed CIM module.
MethodMR HD
R1 R1 mAPmAP HIT@1@0.5 @0.7 Avg.
Average 63.48 49 .87 44 .10 39 .81 63 .16
K-Means 62.58 48 .39 43 .13 39 .47 62 .26
EM-Att 64.32 50 .26 44 .49 39 .82 64.0
EM-Att † 65.10 51 .81 45 .79 40 .03 63.29
Table 7. Impact of various aggregation methods. †indicates
the EM Attention module with RBF kernel.MethodMR HD
R1 R1 mAPmAP HIT@1@0.5 @0.7 Avg.
Cross Attention 63.03 49 .87 43 .79 39 .63 63.94
BMRW 65.10 51 .81 45 .79 40 .03 63.29
Table 8. Comparison of different modality interaction strategies.
(LRP) and Global Knowledge Accumulation (GKA). As
shown in Tab. 6, since GKA facilitate the understanding
of global context, the ablation of it leads to inferior per-
formance on HD, i.e., 1.1%in HIT@1. Moreover, LRP
brings a clear improvement of +2.34% in Avg. mAP on
MR, proving the enhancement on locality perception.
Aggregation Method. We study the impacts on various
aggregation methods utilized in DBIA module. As illus-
trated in Tab. 7, we believe the superiority of our RBF ker-
nel based EM-Attention derives from two aspects: 1) Com-
pared with “Average” and K-Means, our method enhances
the desired moment representation while suppresses noises.
2) RBF kernel maps features into a high-dimensional latent
space while modeling the relationship within it, which is
beneﬁcial for the subsequent aggregation.
Modality Interaction Strategy. We investigate the ef-
fects of different modality interaction strategies in Lo-
cal Relation Perception. As shown in Tab. 8, replacingBMRW by cross attention mechanism results in 2%perfor-
mance degradation, which demonstrates the effectiveness
of BMRW. Furthermore, we provide visualization of fea-
tures to prove the rationality of LRP. It can be seen in Fig. 5
that the utilization of cross-attentive mechanism leads to the
emergence of attention drift. In contrast, through iterative
multi-modal learning in shared space, BMRW mitigates the
issue, thereby facilitating more precise localization. More-
over, LRP achieves the local relation perception evidenced
by clearer strip-like attention patterns in Fig. 5.
Grounding Consistency. Beneﬁting from the task-
speciﬁc design, our method yields greater consistency in
the joint solution of MR and HD. To quantify the perfor-
mance coherence, on one hand, we count the videos with
accurate hightlight-ness estimation ( mAP HD>0.8) and
calculate MR mAP for those videos as shown in Fig. 6(a).
On the other hand, we measure the HD mAP and quantities
of videos with precise moment spans ( mAP MR>0.8) as
18715
SaliencyScore60s0s12sQuery:Underwater views of whale sharks and people swimming with them54s
00.51024681012141618202224262830323436384042444648505254565860TimeOurPredictionQDPredictionGround-TruthOurPrediction:QDPrediction:Ground-Truth:MomentSpan
Figure 4. Visullization comparison on MR and HD. QD indicates previous state-of-the-art method QD-DETR [ 31]
Query:The old statues have butterflies on them
CrossAttentionLRPModuleFigure 5. Illustration of different modality interaction strate-
gies. The red bounding box indicates the relevant interval and the
white bounding box denotes the start and end clips.
shown in Fig. 6(b). The results demonstrate that UVCOM
effectively bridges the gap between two tasks for which our
method is superior on all statistics, i.e., MR and HD preci-
sion as well as quantity.
4.5. Qualitative Results
As shown in Fig. 4, The local-global enhancement and com-
prehensive understanding allows our method to accurately
model the saliency distribution and localize timestamps of
the moment precisely. Comparatively, without the explicit
association of characteristics of two tasks, QD-DETR [ 31]
struggles to handle simultaneously in complex scenarios.
5. Conclusion
In light of the different emphasis on MR and HD, we
propose a uniﬁed video comprehension framework called
UVCOM under the guidance of design principles to effec-
tively bridge the gap between two tasks. By performing pro-
gressive intra and inter-modality interaction across multi-
granularity, UVCOM achieves locality perception of tem-Figure 6. Illustration of grounding consistency of MR and HD.
(a) indicates the videos collected by mAP HD>0.8. (b) indicates
the videos collected by mAP MR>0.8.
poral and multi-modal relationship as well as global knowl-
edge accumulation of the entire video. Moreover, we in-
troduce multi-aspect contrastive learning to provide the ex-
plicit supervision of above two objectives. Extensive stud-
ies validate our model’s comprehensive understanding of
videos and show our UVCOM remarkably outperforms the
existing state-of-the-art methods.
Limitations. Since we just use a simple way to handle au-
dio features instead of speciﬁc design, we think that the ex-
plicit design for audio features is an interesting future direc-
tion.
Acknowledgements
This work is supported by National Key R&D Pro-
gram of China (Grant No. 2020AAA0108303), Shen-
zhen Science and Technology Project (Grant No.
JCYJ20200109143041798), Shenzhen Stable Support-
ing Program (WDZC20200820200655001) and partly
supported by the National Natural Science Foundation of
China (Grant No. U1903213) and the Shenzhen Science
and Technology Program (JSGG20220831093004008).
18716
References
[1]Taivanbat Badamdorj, Mrigank Rochan, Yang Wang, and Li
Cheng. Joint visual and audio learning for video highlight
detection. In ICCV , pages 8107–8117, 2021. 1,2,7
[2]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-
to-end object detection with transformers. In ECCV , pages
213–229, 2020. 2,6
[3]Joao Carreira and Andrew Zisserman. Quo vadis, action
recognition? a new model and the kinetics dataset. In CVPR ,
pages 6299–6308, 2017. 3,1
[4]Shaoxiang Chen and Yu-Gang Jiang. Semantic proposal for
activity localization in videos via sentence query. In AAAI ,
pages 8199–8206, 2019. 1
[5]Qi Dong, Shaogang Gong, and Xiatian Zhu. Class rectiﬁ-
cation hard mining for imbalanced deep learning. In ICCV ,
pages 1851–1860, 2017. 5
[6]Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and
Kaiming He. Slowfast networks for video recognition. In
ICCV , pages 6201–6210, 2019. 3,6,1
[7]Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia.
Tall: Temporal activity localization via language query. In
ICCV , pages 5267–5275, 2017. 5,2
[8]Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia.
TALL: temporal activity localization via language query. In
ICCV , pages 5277–5285, 2017. 1,2
[9]Kuofeng Gao, Yang Bai, Jindong Gu, Yong Yang, and Shu-
Tao Xia. Backdoor defense via adaptively splitting poi-
soned dataset. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 4005–
4014, 2023. 2
[10] Kuofeng Gao, Yang Bai, Jindong Gu, Shu-Tao Xia, Philip
Torr, Zhifeng Li, and Wei Liu. Inducing high energy-latency
of large vision-language models with verbose images. In
ICLR , 2024. 2
[11] Leo Grady. Random walks for image segmentation. IEEE
Trans. Pattern Anal. Mach. Intell. , 28(11):1768–1783, 2006.
4
[12] Michael Gygli, Yale Song, and Liangliang Cao. Video2gif:
Automatic generation of animated gifs from video. In CVPR ,
pages 1001–1009, 2016. 1,2,7
[13] Chunming He, Kai Li, Yachao Zhang, Guoxia Xu, Longxi-
ang Tang, Yulun Zhang, Zhenhua Guo, and Xiu Li. Weakly-
supervised concealed object segmentation with sam-based
pseudo labeling and multi-scale feature grouping. NeurIPS ,
36, 2024. 3
[14] Chunming He, Kai Li, Yachao Zhang, Yulun Zhang, Zhen-
hua Guo, Xiu Li, Martin Danelljan, and Fisher Yu. Strategic
preys make acute predators: Enhancing camouﬂaged object
detectors by generating camouﬂaged objects. In ICLR , 2024.
5
[15] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef
Sivic, Trevor Darrell, and Bryan C. Russell. Localizing mo-
ments in video with temporal language. In EMNLP , pages
1380–1390, 2018. 2[16] Fa-Ting Hong, Xuanteng Huang, Weihong Li, and Wei-Shi
Zheng. Mini-net: Multiple instance ranking network for
video highlight detection. In ECCV , pages 345–360, 2020. 7
[17] Linjiang Huang, Liang Wang, and Hongsheng Li. Weakly
supervised temporal action localization via representative
snippet knowledge propagation. In CVPR , pages 3262–3271,
2022. 3,4
[18] Jinhyun Jang, Jungin Park, Jin Kim, Hyeongjun Kwon,
and Kwanghoon Sohn. Knowing where to focus: Event-
aware transformer for video grounding. arxiv preprint arXiv:
2308.06947 , 2023. 5,6
[19] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,
Wenwu Wang, and Mark D Plumbley. Panns: Large-scale
pretrained audio neural networks for audio pattern recogni-
tion. IEEE ACM Trans. Audio Speech Lang. Process. , 28:
2880–2894, 2020. 6,1
[20] Jie Lei, Licheng Yu, Tamara L. Berg, and Mohit Bansal.
TVR: A large-scale dataset for video-subtitle moment re-
trieval. In ECCV , pages 447–463, 2020. 2
[21] Jie Lei, Tamara L. Berg, and Mohit Bansal. Qvhighlights:
Detecting moments and highlights in videos via natural lan-
guage queries. arXiv preprint arXiv: 2107.09609 , 2021. 1,
2,3,4,5,6
[22] Kun Li, Dan Guo, and Meng Wang. Proposal-free video
grounding with contextual pyramid network. In AAAI , pages
1902–1910, 2021. 2
[23] Xia Li, Zhisheng Zhong, Jianlong Wu, Yibo Yang, Zhouchen
Lin, and Hong Liu. Expectation-maximization attention net-
works for semantic segmentation. In ICCV , pages 9166–
9175, 2019. 3
[24] Kevin Qinghong Lin, Pengchuan Zhang, Joya Chen, Shra-
man Pramanick, Difei Gao, Alex Jinpeng Wang, Rui
Yan, and Mike Zheng Shou. Univtg: Towards uniﬁed
video-language temporal grounding. arXiv preprint arXiv:
2307.16715 , 2023. 2,3,6,7
[25] Meng Liu, Xiang Wang, Liqiang Nie, Xiangnan He, Bao-
quan Chen, and Tat-Seng Chua. Attentive moment retrieval
in videos. In SIGIR , pages 15–24, 2018. 2
[26] Meng Liu, Xiang Wang, Liqiang Nie, Qi Tian, Baoquan
Chen, and Tat-Seng Chua. Cross-modal moment localiza-
tion in videos. In ACM MM , pages 843–851, 2018. 1
[27] Ye Liu, Siyuan Li, Yang Wu, Chang Wen Chen, Ying Shan,
and Xiaohu Qie. UMT: uniﬁed multi-modal transformers
for joint video moment retrieval and highlight detection. In
CVPR , pages 3032–3041, 2022. 1,2,3,4,6,7
[28] Zhuoyan Luo, Yicheng Xiao, Yong Liu, Shuyan Li, Yi-
tong Wang, Yansong Tang, Xiu Li, and Yujiu Yang. Soc:
Semantic-assisted object cluster for referring video object
segmentation. NeurIPS , 36, 2024. 5
[29] Yue Ma, Yali Wang, Yue Wu, Ziyu Lyu, Siran Chen, Xiu
Li, and Yu Qiao. Visual knowledge graph for human action
reasoning in videos. In ACM MM , pages 4132–4141, 2022.
3
[30] Carl D Meyer and Ian Stewart. Matrix analysis and applied
linear algebra . SIAM, 2023. 4,1
[31] WonJun Moon, Sangeek Hyun, Sanguk Park, Dongchan
Park, and Jae-Pil Heo. Query - dependent video representa-
18717
tion for moment retrieval and highlight detection. In CVPR ,
2023. 1,2,3,4,5,6,7,8
[32] Jonghwan Mun, Minsu Cho, and Bohyung Han. Local-
global video-text interactions for temporal grounding. In
CVPR , pages 10807–10816, 2020. 2
[33] Giannis Nikolentzos and Michalis Vazirgiannis. Random
walk graph neural networks. In NeurIPS , 2020. 4
[34] Cristian Rodriguez Opazo, Edison Marrese-Taylor, Fate-
meh Sadat Saleh, Hongdong Li, and Stephen Gould.
Proposal-free temporal moment localization of a natural-
language query in video using guided attention. In WACV ,
pages 2453–2462, 2020. 2
[35] Jeffrey Pennington, Richard Socher, and Christopher D Man-
ning. Glove: Global vectors for word representation. In
EMNLP , pages 1532–1543, 2014. 1
[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In ICML , pages
8748–8763, 2021. 3,5,6,1
[37] Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel,
Stefan Thater, Bernt Schiele, and Manfred Pinkal. Ground-
ing action descriptions in videos. Trans. Assoc. Comput. Lin-
guistics , 1:25–36, 2013. 2,5
[38] Sylvia Richardson and Peter J Green. On bayesian analysis
of mixtures with an unknown number of components (with
discussion). Journal of the Royal Statistical Society Series
B: Statistical Methodology , 59(4):731–792, 1997. 3
[39] Mrigank Rochan, Mahesh Kumar Krishna Reddy, Linwei
Ye, and Yang Wang. Adaptive video highlight detection by
learning from user history. In ECCV , pages 261–278, 2020.
2
[40] Erica K Shimomoto, Edison Marrese-Taylor, Hiroya Taka-
mura, Ichiro Kobayashi, Hideki Nakayama, and Yusuke
Miyao. Towards parameter-efﬁcient integration of pre-
trained language models in temporal video grounding. arXiv
preprint arXiv:2209.13359 , 2022. 1
[41] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In ICLR ,
2015. 1
[42] Yale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro
Jaimes. Tvsum: Summarizing web videos using titles. In
CVPR , pages 5179–5187, 2015. 5
[43] Min Sun, Ali Farhadi, and Steve Seitz. Ranking domain-
speciﬁc highlights by analyzing edited videos. In ECCV ,
pages 787–802, 2014. 5,7
[44] Xin Sun, Xuan Wang, Jialin Gao, Qiong Liu, and Xi Zhou.
You need to read again: Multi-granularity perception net-
work for moment retrieval in videos. In SIGIR , pages 1022–
1032, 2022. 2
[45] Haoyu Tang, Jihua Zhu, Meng Liu, Zan Gao, and Zhiyong
Cheng. Frame-wise cross-modal matching for video moment
retrieval. TMM , 24:1338–1349, 2022. 2
[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. NeurIPS , 30, 2017. 5[47] Jiangshan Wang, Yifan Pu, Yizeng Han, Jiayi Guo, Yiru
Wang, Xiu Li, and Gao Huang. Gra: Detecting oriented
objects through group-wise rotating and attention. arXiv
preprint arXiv:2403.11127 , 2024. 5
[48] Lezi Wang, Dong Liu, Rohit Puri, and Dimitris N Metaxas.
Learning trailer moments in full-length movies with co-
contrastive attention. In ECCV , pages 300–316, 2020. 7
[49] Weining Wang, Yan Huang, and Liang Wang. Language-
driven temporal activity localization: A semantic matching
reinforcement learning model. In CVPR , pages 334–343,
2019. 1
[50] Shaoning Xiao, Long Chen, Songyang Zhang, Wei Ji, Jian
Shao, Lu Ye, and Jun Xiao. Boundary proposal network
for two-stage natural language video localization. In AAAI ,
pages 2986–2994, 2021. 2
[51] Yicheng Xiao, Yue Ma, Shuyan Li, Hantao Zhou, Ran Liao,
and Xiu Li. Semanticac: semantics-assisted framework for
audio classiﬁcation. In ICASSP , pages 1–5. IEEE, 2023. 5
[52] Bo Xiong, Yannis Kalantidis, Deepti Ghadiyaram, and Kris-
ten Grauman. Less is more: Learning highlight detection
from video duration. In CVPR , pages 1258–1267, 2019. 1,
2,7
[53] Minghao Xu, Hang Wang, Bingbing Ni, Riheng Zhu, Zhen-
bang Sun, and Changhu Wang. Cross-category video high-
light detection via set-based learning. In ICCV , pages 7950–
7959, 2021. 7
[54] Yifang Xu, Yunzhuo Sun, Yang Li, Yilei Shi, Xiaoxiang
Zhu, and Sidan Du. Mh-detr: Video moment and high-
light detection with cross-modal transformer. arXiv preprint
arXiv:2305.00355 , 2023. 6
[55] Zunnan Xu, Zhihong Chen, Yong Zhang, Yibing Song, Xi-
ang Wan, and Guanbin Li. Bridging vision and language
encoders: Parameter-efﬁcient tuning for referring image seg-
mentation. In ICCV , pages 17503–17512, 2023. 3
[56] Ting Yao, Tao Mei, and Yong Rui. Highlight detection with
pairwise deep ranking for ﬁrst-person video summarization.
InCVPR , pages 982–990, 2016. 1,2
[57] Qinghao Ye, Xiyue Shen, Yuan Gao, Zirui Wang, Qi Bi, Ping
Li, and Guang Yang. Temporal cue guided video highlight
detection with low-rank audio-visual fusion. In ICCV , pages
7950–7959, 2021. 7
[58] Youngjae Yu, Sangho Lee, Joonil Na, Jaeyun Kang, and
Gunhee Kim. A deep ranking model for spatio-temporal
highlight detection from a 360 video. In AAAI , pages 7525–
7533, 2018. 1,2
[59] Yitian Yuan, Tao Mei, and Wenwu Zhu. To ﬁnd where you
talk: Temporal sentence localization in video with attention
based location regression. In AAAI , pages 9159–9166, 2019.
2
[60] Yawen Zeng, Ning Han, Keyu Pan, and Qin Jin. Temporally
language grounding with multi-modal multi-prompt tuning.
TMM , 2023. 1
[61] Da Zhang, Xiyang Dai, Xin Wang, Yuan-Fang Wang, and
Larry S. Davis. MAN: moment alignment network for natu-
ral language moment retrieval via iterative graph adjustment.
InCVPR , pages 1247–1257, 2019. 1
18718
[62] Hao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou.
Span-based localizing network for natural language video lo-
calization. In ACL, pages 6543–6554, 2020. 2,6
[63] Ke Zhang, Wei-Lun Chao, Fei Sha, and Kristen Grauman.
Video summarization with long short-term memory. In
ECCV , pages 766–782, 2016. 7
[64] Songyang Zhang, Houwen Peng, Jianlong Fu, and Jiebo Luo.
Learning 2d temporal adjacent networks for moment local-
ization with natural language. In AAAI , pages 12870–12877,
2020. 6,1
[65] Hantao Zhou, Rui Yang, Yachao Zhang, Haoran Duan,
Yawen Huang, Runze Hu, Xiu Li, and Yefeng Zheng. Uni-
head: unifying multi-perception for detection heads. arXiv
preprint arXiv:2309.13242 , 2023. 5
18719
