Learning to navigate efficiently and precisely in real environments
Guillaume Bono, Herv ´e Poirier, Leonid Antsfeld, Gianluca Monaci, Boris Chidlovskii, Christian Wolf
NA VER LABS Europe, Meylan, France
{firstname }.{lastname }@naverlabs.com
Abstract
In the context of autonomous navigation of terrestrial robots,
the creation of realistic models for agent dynamics and sens-
ing is a widespread habit in the robotics literature and in
commercial applications, where they are used for model
based control and/or for localization and mapping. The
more recent Embodied AI literature, on the other hand, fo-
cuses on modular or end-to-end agents trained in simulators
like Habitat or AI-Thor, where the emphasis is put on photo-
realistic rendering and scene diversity, but high-fidelity robot
motion is assigned a less privileged role. The resulting
sim2real gap significantly impacts transfer of the trained
models to real robotic platforms. In this work we explore
end-to-end training of agents in simulation in settings which
minimize the sim2real gap both, in sensing and in actuation.
Our agent directly predicts (discretized) velocity commands,
which are maintained through closed-loop control in the real
robot. The behavior of the real robot (including the under-
lying low-level controller) is identified and simulated in a
modified Habitat simulator. Noise models for odometry and
localization further contribute in lowering the sim2real gap.
We evaluate on real navigation scenarios, explore different
localization and point goal calculation methods and report
significant gains in performance and robustness compared
to prior work.
1. Introduction
Point goal navigation of terrestrial robots in indoor buildings
has traditionally been addressed in the robotics community
with mapping and planning [ 6,30,50], which led to solu-
tions capable of operating on robots in real environments.
The field of computer vision and embodied AI has addressed
this problem through large-scale machine learning in sim-
ulated 3D environments from reward with RL [ 24,34] or
with imitation learning [ 14]. Learning from large-scale data
allows the agent to pick up more complex regularities, to
process more subtle cues and therefore (in principle) to be
more robust, to exploit data regularities to infer hidden and
occluded information, and generally to learn more powerful
Training/sim (ours)Test/realxt1212xt+1
Training/sim (classical)xt12xt+1Policy: ∆πOnboardctrl∆φ333ms/3 Hz(Unknown)Policy: ∆πDyn. model∆φ333ms/3 Hz33ms/30 HzPolicy: ∆πconstant333ms/3 HzvelocitiesPiecewise
Figure 1. Efficient navigation with policies end-to-end trained in
3D photorealistic simulators requires closing the sim2real gap in
sensing andactuation. Efficiency demands that the robot continues
to move during decision taking (as opposed to stopping for each
sensing operation), and this requires a realistic motion model in
simulation allowing the agent to internally anticipate its future state.
This requirement is exacerbated by the delay between sensing
①and actuation ②caused by the computational complexity of
high-capacity deep networks (visual encoders, policy). To model
realistic motion while training in simulation, we create a 2ndorder
dynamical model running with higher frequency, which models
the robot and its low-level closed-loop controller. We identify the
model from real data and add it to the Habitat [44] Simulator.
decision rules. In this context, the specific task of point goal
navigation (navigation to coordinates) is now sometimes
considered “solved” in the literature [ 38], incorrectly, as we
argue. While the machine learning and computer vision
community turns its attention towards the exciting goal of
integrating language models into navigation [ 15,23], we
think that further improvements are required to make trained
agents perform reliably in real environments with sufficient
speed.
Experiments and evaluations of trained models in real
environments and the impact of the sim2real gap do exist in
the Embodied AI literature [ 11,20,43], but they are rare and
were performed in restricted environments. Known models
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
17837
are trained in 3D simulators like Habitat [ 44] or AI-Thor
[26], which are realistic in their perception aspects, but lack
the accurate motion models of physical agents which the
robotics community uses for articulated robots, for instance,
where forward and inverse kinetics are often modeled and
identified. This results in a large sim2real gap in motion char-
acteristics, which is either ignored or addressed by discrete
motion commands. The latter strategy teleports the agent
in simulation for each discrete action and executes these
actions on the physical robot by low-level control on posi-
tions instead of velocities, stopping the robot between agent
decisions and leading to extremely inefficient navigation and
overly long episodes [43].
In this work we go beyond current models for terrestrial
navigation by realistically modeling both actuation and sens-
ing of real agents in simulation, and we successfully transfer
trained models to a real robotic platform. On the actuation
side, we take agent decisions as discretized linear and an-
gular velocities and predict them asynchronously while the
agent is in motion. The resulting delay between sensing
and actuation (see Figure 1) requires the agent to anticipate
motion, which it learns in simulation from a realistic motion
model. To this end, we create a second-order dynamical
model, which approximates the physical properties of the
real robot together with its closed-loop low-level controller.
We identify this model from real robot rollouts and expert
actions. The model is integrated into the Habitat simulator as
an intermediate solution between the standard instantaneous
motion model and a full-blown computationally-expensive
physics simulation based on forces, masses, frictions etc.
On the sensing side, we combine visual and Lidar obser-
vations with different ways of communicating the navigation
(point) goal to the agent, and different estimates of robot
pose relative to a reference frame, combining two different
types of localization: first, dead reckoning from wheel en-
coders, and secondly, external localization based on Monte
Carlo methods from Lidar input. We perform extensive ex-
periments and ablations on a real navigation platform and
quantify the impact of the motion model, action spaces, sens-
ing capabilities and the way we provide goal coordinates
on navigation performance, showing that end-to-end trained
agents can robustly navigate from visual input when their
motion has been correctly modelled in simulation.
2. Related Work
Visual navigation — navigation has been classically solved
in robotics using mapping and planning [ 7,31,32], which re-
quires solutions for mapping and localization [ 6,28,50], for
planning [ 27,46] and for low-level control [ 19,42]. These
methods depend on accurate sensor models, filtering, dynam-
ical models and optimization. End-to-end trained models
directly map input to actions and are typically trained with
RL [ 24,35] or imitation learning [ 14]. They learn repre-sentations such as flat recurrent states, occupancy maps [ 9],
semantic maps [ 8], latent metric maps [ 4,21,37], topologi-
cal maps [ 3,10,47], self-attention [ 12,16,18,41], implicit
representations [ 33] or maximizing navigability directly with
a blind agent optimizing a collected representation [ 5]. Our
proposed method is end-to-end trained and features recurrent
memory, but benefits from an additional motion model in
simulation.
Sim2real transfer — The sim2real gap can compromise
strong performance achieved in simulation when agents are
tested in the real-world [ 22,29], as perception and control
policies often do not generalize well to real robots due to in-
accuracies in modelling, simplifications and biases. To close
the gap, domain randomization methods [ 39,49] treat the
discrepancy between the domains as variability in the simu-
lation parameters. Alternatively, domain adaptation methods
learn an invariant mapping for matching distributions be-
tween the simulator and the robot environment. Examples
include transfer of visuo-motor policies by adversarial learn-
ing [ 55], adapting dynamics in RL [ 17], lowering fidelity
simulation [ 52] and adapting object recognition to new do-
mains [ 56]. Bi-directional adaptation is proposed in [ 51];
Recently, Chattopadhyay et al. [11] benchmarked the robust-
ness of embodied agents to visual and dynamics corruptions.
Kadian et al. [25] investigated sim2real predictability of
Habitat-Sim [ 44] for PointGoal navigation and proposed a
new metric to quantify it, called Sim-vs-Real Correlation Co-
efficient (SRCC). The PointGoal task on real robots is also
evaluated in [ 43]. To reach competitive performance with-
out modelling any sim2real transfer, the agent is pre-trained
on a wide variety of environments and then fine-tuned on a
simulated version of the target environment.
Motion models — are usually kept very simple when train-
ing visual navigation policies. Several works use a discrete
high-level action space in simulation [ 8,9,20] and rely on
dedicated controllers to translate these discrete actions into
robot controls when transferring to the real world. Alter-
native approaches predict continuous actions [ 13,54] or
waypoints [ 2,48], that again are subsequently mapped to
robot controls when executing the policies in real. Some-
what closer to our approach, Yokoyama et al. [53] adopt a
discretized linear and angular velocity space, and introduce
a new metric, SCT, that uses a simplified “unicycle-cart” dy-
namical model to estimate the “optimal” episode completion
time computed by rapidly exploring random trees (RRT).
However the unicycle model is only used for evaluation and
it is not deployed for training nor testing: as in previous
works, the commands are mapped to robot controls using
handcrafted heuristics. In contrast to previous approaches,
we include the dynamical model of the robot in the training
loop to learn complex dependencies between the navigation
policy and the robot physical properties.
17838
vGRUdg0πatlˆgthtRGB imgStatic goal vectorˆprtˆpatIntegrated relative pose estimateAbsolute pose estimateLidar scanu
ItStgtLPPOAux supervision
Previous actionat−1eFigure 2. The agent uses a recurrent policy with a static point goal
g0as input, i.e. the goal is constant and given wrt. to the initial
reference frame. During training, the estimation of the dynamic
point goal ˆgtis supervised from privileged information.
3. End-to-end training with realistic sensing
We propose a method for training agents which (in principle)
could address a variety of visual navigation problems, and
without loss of generality, focusing on sim2real transfer
and efficiency, we formalize the problem as a PointGoal
task: an agent receives sensor observations at each time step
tand must take actions atto reach a goal position given
as polar coordinates. The method is not restricted to any
specific observations, in our experiments the agent receives
RGB images It∈R3×H×Wand a Lidar-like vector of ranges
St∈RK, where Kis the number of laser rays.
The proposed agent is fully trained and therefore does not
require to localize itself on a map for analytical path planning.
However, even in this very permissive setting the application
itself requires to communicate the goal to the agent in some
form of coordinate system, which requires localization at
least initially with respect to the goal (in ImageGoal settings
this would be replaced by the goal image). We can classically
differentiate between three different forms:
Absolute PointGoal — the goal vector ga= [ga
x,ga
y]T
is given in a pre-defined agent-independent absolute refer-
ence frame. This requires access to a global localization
system.
Dynamic PointGoal — the goal vector gtis provided with
respect to the current agent pose and therefore updated at
each time instant t. This requires an external localization
system providing estimates which are then transformed
into the egocentric reference frame.
Static PointGoal — the goal vector is provided with re-
spect to the starting position at t=0of the robot, and not
updated with agent motion (= g0). This requires the agent
to learn this update itself, and therefore some form of in-
ternal localization or integration of odometry information.
We target the Static PointGoal case, which does not require
localization beyond the initial relative vector. However, we
also argue that some form of localization and/or odometry
information is useful, which will assist the dynamics infor-mation inferred from (visual) sensor readings. Furthermore,
during training it will allow the agent to learn a useful latent
representation in its hidden state and an associated dynamics
in this latent space. We therefore provide the agent with two
different localization components:
ˆpa
t— an estimate of agent pose with respect to the initial
agent position, asynchronously delivered and with low
frequency, around 0.5-1 fps. At test, this signal is
delivered by external localization, in our case from a
Lidar-based AMCL module (see Section 5).
ˆpr
t— an estimate coming from onboard odometry, which
we integrate over time to provide an estimate situated
in the same reference frame as ˆpa
t, i.e. with respect to
the initial agent position.
The characteristics of the two localization signals are differ-
ent: while ˆpr
tcorresponds to a dead reckoning process and
is subject to drift, its relative noise is lower. The signal ˆpa
tis
not subject to drift but suffers from larger errors in certain
regions, due to registration errors or absence of structure.
The agent policy is recurrent, based on a hidden memory
vector htupdated over time from input of all sensor readings
It,St,pr
t,ˆpa
t, the goal g0and the previous action at−1,
ht=d(ht−1, v(It), u(St),g0,ˆpr
t,ˆpa
t, e(at−1)),(1)
at=π(ht), (2)
ˆgt=l(ht), (3)
where dis a GRU with hidden state ht,vis a visual encoder
of the image It(in our case, a ResNet-18); uis a 1D-CNN
encoding the scan St;eis a set of action embedding vectors;
πis a linear policy. Other inputs go through dedicated MLPs,
not named to lighten notations. To help the agent deal with
the noisy relative and absolute pose estimates, we put an in-
ductive bias for localization on htthrough an auxiliary head
lpredicting the dynamic goal compass ˆgt, supervised during
training from the simulated agent’s ground truth pose, as
shown in Figure 2. In the following paragraphs we describe
the two localization systems and the noise models we use to
simulate them during training in simulation. Section 4 then
introduces the dynamical model used during training.
Integrated relative localization — the pose estimate ˆpr
t
can be obtained with any commercially available odometry
system. In our experiments we integrate readings from wheel
encoders. During simulation and training, ˆpr
tis estimated by
sampling planar noise [ϵx, ϵθ]⊺from a multivariate Gaussian
distribution N(0.01
0
,10−410−4
10−410−3
)which gets added
to the step-wise motion of the agent, leading to the accumu-
lation of drift over time.
Absolute localization — the pose estimate ˆpa
tis ob-
tained with the standard ROS 2 package for Adaptive Monte
Carlo Localization (AMCL) [ 50], a KLD-sampling ap-
proach requiring a pre-computed, static map based on par-
ticle filtering. During simulation and training, ˆpr
tis es-
17839
Training in Simulation
Test onReal robotsDeploy
g0at+
Diff. driveLeft and right motor speed set valuesnatvrtvltgoal
Second order dynamical systemat
Physical robot3D simulatorActuation noise
Filtered + integrated odometry (IMU + encoders)Global localization Fig. 2ˆprtˆpatRGB imgLidar scan
ItStg0goalFig. 2
ItStRGB imgLidar scanSystem identificationfromrecordedtrajectoriesOdometry and absolute pose wrt. to starting position.Noisy privileged informationˆprtˆpat
30 HzState update (3 Hz)
t+2t(333ms/3 Hz)∆π(333ms/3 Hz)∆πt+1Onboard closed loop control (PID), unknown freq.Policy/decision taking loop:τ+1τ…Figure 3. Training visual navigation with realistic motion models: we train an end-to-end agent in simulation (top) subject to two different
simulation loops: a slower loop at 3Hz (indexed by t) renders visual observations and takes agent decisions, while a faster loop at 30 Hz
(indexed by τ) simulates physics. Physics is approximated with a 2ndorder model identified from real robot rollouts (bottom) and includes
the robot physics as well as the behavior of the closed-loop control of the differential drive (neither onboard control algorithm nor control
frequency need to be known). Operations in the intervals are pipelined, eg. sensing occurs at each time step, as does agent forward pass etc.
The agent architecture is detailed in Figure 2.
timated by sampling planar noise from another Gaussian
N(0,Diag [0.03,0.03,0.05]), but this time it gets directly
added to the ground truth pose of the robot, thus modeling
imprecise localization (eg. due to Lidar registration failures)
without drift accumulation. In addition, we model the po-
tential unreliability of such absolute pose estimates by only
providing a new observation to the agent every few steps,
at an irregular period governed by a uniform distribution
U(8,12). This potentially allows to plug-in low-frequency
visual localization.
4. A dynamical model of realistic robot motion
One of our main motivations is to be able to use a trained
model to navigate efficiently , i.e. fast and reliably. Stopping
the robot between predicted actions is therefore not an option,
and we train the model to predict linear and angular velocity
commands in parallel to its physical motion, as illustrated
in Figure 1. This requires the agent to anticipate motion
to predict an action atrelative to the next (yet unobserved)
state at t, exacerbated by the delay between sensing at time
tand actuation at t+1, during which computation through
the high-capacity visual encoder and policy happens. This
motion depends on the physical characteristics of the robot,
as shown in Figure 3 (bottom): the linear and angular veloc-ity commands predicted by the neural policy πare translated
into “set values” for left and right motor speeds of the differ-
ential drive system with a deterministic function and used as
targets by the closed loop control algorithm, typically a PID
(Proportional Integral Derivative Controller ), attempting
to reach and maintain the predicted speed between agent
decisions. The behavior depends on the control algorithm
but also on the physical properties of the robot, like its mass
and resulting inertia, power and torque curves of the motors,
friction etc. We decrease the actuation sim2real gap as as
much as possible by integrating a model of realistic robot
behavior into the Habitat [ 44] platform, shown in the top part
of Figure 3. We approximate the robot’s motion behavior by
modeling the combined behavior of both the robot and the
control algorithm implemented on the real platform as an
asymmetric second-order dynamic system. The parameters
of this low-level loop are estimated from recordings of the
real robot platform with a system identification algorithm.
Classically, a sequential decision process can be mod-
eled as a POMDP ⟨X,U, T,O, O, R⟩. In pure 2D navi-
gation settings with a fixed goal in a single static scene,
the environment maintains a (hidden) state xt∈ X con-
sisting in the pose of the agent, i.e. position and orienta-
tion,xt= [xt, yt, θt]. In most settings in the literature
17840
Figure 5. Robustness: Top: The ROS2 baseline fails when obsta-
cles are thin at the height of the single Lidar ray. The robot doesn’t
see the stool, collides, drags it away. The red circles
 show the
position of the moved stool. Bottom: our agent is surprisingly
robust and allows navigation very close to fine structures.
based on 3D photo-realistic simulators like Habitat [ 44],
the state update in simulation ignores physical properties
of the robot (mass/inertia, friction, accelerations, etc.), as
it is implemented through “teleportation” of the robot to a
new pose. In that case, the environment transition decom-
poses into a discrete state update xt+1=T(xt, ut)and
an observation ot+1=O(xt+1), The discrete action space
Umotion ={FORWARD 25cm ,TURN LEFT 10◦,TURN RIGHT
10◦andSTOP}frequently chosen in the navigation literature
results in a halting and jerky robot motion.
Adding realism — To be able to smooth robot trajecto-
ries, we need to model accelerations. We extend the agent
state as:
xt= [xt, yt, θt, vt, wt,˙vt,˙wt], (4)
where xt, yt, θtare the absolute position and orientation of
the robot in the plane of the scene (in m, m and rad); vt, wt
are the linear (forward) and angular velocities of the robot (in
m/s and rad/s); ˙vt; ˙wtare the linear and angular accelerations
of the robot (in m/s2and rad/s2).
Figure 4. The action space :
28 actions, 4 choices of lin-
ear velocities ∈[0,1]m/s, 7
choices for angular vel. ∈
[−3,3]rad/s. Arrows show
the effect on pose of action
held for2
3sec.The proposed agent predicts
a pair⟨v∗
t, w∗
t⟩of velocity com-
mands, chosen from a discrete ac-
tion space Uvelresulting from the
cartesian product of linear and
angular spaces {0,0.3,0.6,1} ×
{−3,−2,−1,0, . . . ,3}, as shown
in Figure 4. To simplify no-
tations, we will use ut=
⟨v∗
t, w∗
t⟩ ∈ U .
One possibility to model real-
istic robot behavior is to design a
full dynamical model of the robot
in the form of a set of differen-
tial equations, identify its param-
eters, discretize it, and simulate
it in Habitat, together with run-
ning an exact copy of the control algorithm used on the
Figure 6. Dynamic obstacles: our agent stops and circumvents
people, achieved through a highly negative collision reward term.
physical robot with exactly the same parameters and control
frequency, which is a stringent constraint. Instead, we opted
for a more flexible solution and we approximate the com-
bined behavior of the physical robot and its control algorithm
by a second order dynamical model [ 36]. We decompose in-
teractions with the simulator into two different loops running
at different frequencies:
•Visual observations, simulator state updates and agent
decisions (Equations (1) to (3)) are produced in a slower
loop, indexed by letter tin the subsequent notation, and
run at f∗= 3Hz in our experiments.
•Between two subsequent steps of the slower loop, a faster
loop models physical motion of the robot, without ren-
dering observations. At the end of this loop, the simula-
tor state is updated, and a visual observation is returned.
Steps in this faster loop are indexed by a second index τ
in the subsequent notation, xt,τ.
In-between two environment steps t−1andt, we simulate
dynamics at frequency fϕ= 30 > f∗. The number of phys-
ical sub-time steps per environment time step is therefore
Kϕ=l
fϕ
f∗m
= 10 , their duration is ∆ϕ=1
fϕ= 33 ms. The
physical loop running between two environment steps can
be formalized as the following set of state update equations,
ot=O(xt−1). Observation
xt−1,0=xt−1 Init
˙vt,τ+1= ˙vt,τ+ ∆ϕ(fv
t,τ2δv
t,τ−2ζv
t,τfv
t,τ˙vt,τ)Upd. acc.
˙wt,τ+1= ˙wt,τ+ ∆ϕ(fw
t,τ2δw
t,τ−2ζw
t,τfw
t,τ˙wt,τ)
vt,τ+1=vt,τ+ ∆ϕvt,τ+1 Upd. vel.
wt,τ+1=wt,τ+ ∆ϕwt,τ+1
θt,τ+1=θt,τ+ ∆ϕwt,τ+1 Upd. pose
xt,τ+1=xt,τ+ ∆ϕvt,τ+1cosθt,τ+1
yt,τ+1=yt,τ+ ∆ϕvt,τ+1sinθt,τ+1
xt=xt−1,K Final state
(5)
In these equations, f.are natural frequencies and damping
coefficients of asymmetric 2ndorder dynamic models for
linear and angular motion. At each step τ, they are chosen
from identified values given command errors as follows:
δv
t,τ=v∗
t−vt,τ
⟨fv
t,τ, ζv
t,τ⟩=(
⟨fv↑, ζv↑⟩ifδv
t,τ·vt,τ>0(acceleration)
⟨fv↓, ζv↓⟩otherwise (deceleration)
(6)
17841
and similarly for angular model parameters ⟨fw, ζw⟩. Veloc-
ity and acceleration are also clipped to identified maximum
values.
System identification — the model has 8 parameters,
fv↑, ζv↑, fv↓, ζv↓, fw↑,ζw↑, fw↓,ζw↓, which we iden-
tify from trajectories of a real robot controlled with pre-
computed trajectories exploring the command space, see the
supplementary material for more details.
Training — the model is trained with RL, in particular
PPO [ 45], and with a reward inspired by [ 11],rt= R·
Isuccess−∆Geo
t−λ−C·Icollision , where R=2.5,∆Geo
tis the
gain in geodesic distance to the goal, a slack cost λ=0.01
encourages efficiency, and a cost C= 0.1penalizes each
collision without ending the episode.
Recovery behavior — similar to what is done in classi-
cal analytical planning, we added a rudimentary recovery
behavior on top of the trained agent: if the agent is notified
of an obstacle and does not move for five seconds, it will
move backward at 0.2 m/s for two seconds.
5. Experimental Results
Experimental setup — we trained the agent in the Habitat
simulator [ 44] on the 800 scenes of the HM3D Dataset [ 40]
for 200M env. steps. Real robot experiments have been per-
formed on a Naver Rookie robot, which came equipped with
various sensors. We added an additional front facing RGB
camera and capture images with a resolution of 1280×720
resized to 256×144. In our experiments, the Lidar scan S
is not taken from an onboard Lidar, but simulated from the
4RealSense depth cameras which are installed close to the
floor and oriented in 4 different directions. We use multiple
scan lines of the cameras and fuse them into a single ray,
details are given in the supplementary material. We did,
however, add a single plane Lidar to the robot (which did
not come equipped with one) and used it for localization
only (see section 3). All processing has been done onboard,
thanks to a Nvidia Jetson AGX Orin we added, with an
integrated Nvidia Ampere GPU. It handles pre-processing
of visual observations and network forward pass in around
70ms. The exact network architecture of the policy is given
in the supplementary material.
Evaluation — evaluating sim2real transfer is inherently
difficult, as it would optimally require to evaluate all agent
variants and ablations on a real physical robot and on a high
number of episodes. We opted for a three-way strategy, all
tables are color-coded with numbers corresponding to one of
the three following settings: (i) “Real” experiments evalu-
ate the agent trained in simulation on the real Naver Rookie
robot. It is the only form of evaluation which correctly
estimates navigation performance in a real world scenario,
but for practical reasons we limit it to a restricted number
of 20 episodes in a large office environment shown in Fig.
7.(ii) “Simulation (+dyn. model)”) is a setting in simula-tion (Habitat), which allows large-scale evaluation on a large
number of unseen environments and episodes, the HM3D
validation set. Most importantly, during evaluation the sim-
ulator uses the identified dynamical model and therefore
realistic motion, even for baselines which do not have access
to one during training. Similar to the “Real” setting, this al-
lows to evaluate the impact of not modeling realistic motion
during training. (iii) “ Simulation (train domain)” evalu-
ates in simulation with the same action space an agent variant
uses during training, i.e. there is no sim2real gap at all. This
setting evaluates the difficulty of the simulated task, which
might be a severe approximation of the task in real condi-
tions. High performance in this setting is not necessarily
indicative of high performance in a real environment.
We evaluate on two different sets of episodes:
HM3D/2.5k consists of 2500 episodes in the HM3D val-
idation scenes, used in simulation only. Office/20 consists
of 20 episodes in the targeted office building, Figure 7. They
are used for evaluation in both, real world and simulation.
Metrics — Navigation performance is evaluated by suc-
cess rate (SR), i.e., fraction of episodes terminated within
a distance of <0.2m to the goal by the agent calling the
⟨v=0, w=0⟩action enough time to cancel its velocity, and
SPL [ 1], i.e., SR weighted by the optimality of the path,
SPL=1
NPN
i=1Isuccessℓ∗
i
max( ℓi,ℓ∗
i),where ℓiis the agent path
length and ℓ∗
ithe GT path length.
SPL is limited in its ability to properly evaluate agents
with complex dynamics. Success Weighted by Completion
Time (SCT) [ 53] explicitly takes the agent’s dynamics model
into consideration, and aims to accurately capture how well
the agent has approximated the fastest navigation behavior.
It is defined as SCT=1
NPN
i=1Sit∗
i
max( ci,t∗
i),where ciis the
agent’s completion time in episode i, and t∗
iis the shortest
possible amount of time it takes to reach the goal point from
the start point while circumventing obstacles based on the
agent’s dynamics. To simplify implementation, we use a
lower bound on t∗taking into account linear dynamics along
straight shortest path segments.
Checkpoints have been chosen as follows: performance in
Real is given with checkpoints selected as the ones provid-
ing max SR in simulation . Performance in Simu lation
is given on the last checkpoint.
5.1. Results
Agent behavior — the agent is surprisingly robust and does
not collide with the infrastructure even though it can quite
closely approach it navigating around it, even if the obstacles
are thin and light. Examples are given in Figure 5 (top), but
they are best viewed through the video in the supplementary
material . This is in stark contrast to the ROS 2 based planner,
which uses a 2D Map constructed with the onboard Lidar:
thin and light structures do not show up on the map or a
filtered, often because the larger part of the obstacle is not
17842
Figure 7. The 20 trajectories ofOffice/20 are distributed over 5 plots and superimposed on the map, color coded. If an episode fails, the
remaining distance is shown as a straight black line from the last position to the goal. The agent is variant βof Table 4.
Method +dyn Ref. Sim (+dyn.) HM3D/2.5k Sim (+dyn.) Office/20 Real Office/20
& action space (train) SR(%) SPL(%) SCT(%) SR(%) SPL(%) SCT(%) SR(%) SPL(%) SCT(%)
(a) Position, disc(4) ✗ [43] 58.2 48.2 16.8 35.0 30.3 13.2 15.0 11.8 2.4
(b) Velocity, disc(28) ✗ [53] 0.8 0.1 0.1 0.0 0.0 0.0 20.0 8.7 2.6
(c) Velocity, disc(28) ✓ (Ours) 97.4 82.2 51.0 100.0 78.8 59.9 40.0 28.9 10.1
Table 1. Impact of training with a realistic dynamical model: we compare end-to-end trained models using position commands (a) as in
[43] and, discretized velocity commands without a dynamical model (considering constant velocity) as in [ 53], and our proposed method (c).
The references [43, 53] are cited for their action space and motion handling, but they have different agent architectures.
Method Sim(train) HM3D/2.5k Sim(+dyn) HM3D/2.5k
SR% SPL% SCT% SR% SPL% SCT%
(a)Pos, disc(4) 92.7 81.7 30.3 58.2 48.2 16.8
(b)Vel, disc(28) 98.0 74.2 58.9 0.8 0.1 0.1
(c)Vel, disc(28) 97.4 82.2 51.0 97.4 82.2 51.0
Table 2. Difficulty of tasks given action spaces and motion mod-
els: we evaluate the baseline model variants in the same simulated
environment in which they have been trained. High performance
does not necessarily transfer to real. Letters are variants in Table 1.
in the height of the single Lidar ray. Collisions are frequent,
examples are given in Figure 5 (bottom). The agent is also
capable of avoiding dynamic obstacles like people in spite
of not having seen them in simulation (cf. Figure 6), which
we conjecture is due to a high collision avoidance weight in
the reward.
Sim2real gap and memory — one interesting finding we
would like to share is that the raw base agent sometimes tends
to get discouraged from initially being blocked in a situation,
for instance if the passage through a door is not optimal and
requires correction. The initial variants of the agent seemed
to abandon relatively quickly and started searching for a
different trajectory, circumventing the passage way obstacle
altogether. We conjecture that this stems from the fact that
such blockings are rarely seen in simulation and the agent is
trained to “write off” this area quickly, storing in its recurrent
memory that this path is blocked. All our real experiments
are therefore performed with a version, which resets its re-
current state hi(eq. (1)) every 10 seconds, leading to less
frequent abandoning. Future work will address this problem
more directly, for instance by simulating blocking situations
during training.Influence of motion model — Table 1 compares results
of different agents trained with different action spaces and
with or without realistic motion during simulation. We can
see that the impact of training the agent with the correct
motion model in simulation is tremendous. The agent in line
(b) uses the same action space, but no dynamical model is
used in simulation, which means that changes in velocity are
instantaneous and velocities are constant between decisions.
The behavior is unexploitable, the agent is disoriented and
keeps crashing into its environment. Line (a) corresponds
to a position controlled agent with action space {FORWARD
25cm ,TURN LEFT 10◦,TURN RIGHT 10◦,STOP}. After
training, it is adapted to motion commands by calculating
the corresponding velocity commands given the decision
frequency of 3 Hz. It has somewhat acceptable (but low)
performance in simulation, although it does not dispose of
a motion model during training. This fact that rotating and
linear motion are separated and cannot occur in the same
time simplifies the problem and leads to some success in
simulation, but this behavior does not transfer well to the real
robot. The agent with the identified motion model achieves
near perfect SR in simulation, which shows that the model
can learn to anticipate motion and (internal latent) future
state prediction correctly. When transferred to the real robot,
performance is the best among all agents, but still leaves
room much room for improvement.
Difficulty of the tasks — Table 2 compares the same
three agents also in simulation when validated with the
same setting they have been trained on (action space, motion
model or absence of). While this comparison is not at all
indicative of the performance of these agents on a real robot,
it provides evidence of the difficulty of the task in simulation.
17843
Point Goal ˆpr
tˆpa
tSim (+dyn) Office/20 Real Office/20
SR% SPL% SCT% SR% SPL% SCT%
gt ✗✗40.0 34.5 27.4 35.0 23.6 5.2
g0 ✓✓70.0 51.9 36.9 50.0 37.5 11.4
g0+ superv. ✓✓100.0 78.8 59.9 40.0 28.9 10.1
g0+ superv. ✓✗70.0 51.9 36.9 25.0 16.7 3.0
Table 3. Localization and PointGoal calculation : we compare
the impact of the presence of external localization to the agent,
and the point goal sources: dynamical point goal (through external
localization) with static point goal w/ and w/o supervision. All
agents are variant (c) from Table 1.
Method Real Office/20 Real Office/20-alt
SR% SPL% SCT% SR% SPL% SCT%
(α)ROS 2 (fused) 80.0 69.5 26.2 90.0 70.5 27.0
(β)Ours (finetuned) 55.0 40.4 11.2 60.0 42.0 9.7
Table 4. Comparisons — we compare with the ROS 2 NavStack,
which has access to the 2D occupancy map beforehand, localizes
itself with the single ray Lidar scan and AMCL (Monte Carlo local-
ization), uses the fused laserscan for obstacle avoidance, followed
by shortest path planning. Metrics do no measure collisions ,
which are much higher for the ROS 2 planner. To be comparable,
our method is finetuned with RL on the Matterport scan of the same
building. We also add an experiment on the same office building
with a different furniture arrangement, Office/20-alt .
Surprisingly, the performances are very close: the additional
burden the motion model puts on the learning problem itself
can be handled very well by the agent.
Goal vector calculation — As explained in Section 3,
our base agent receives the static point goal g0, ie. a vector
with respect to the initial reference frame at time t=0, which
is not updated. Table 3 compares this agent with two other
variants. A version where supervision is removed during
training, which surprisingly shows good performance. We
conjecture that the additional supervision learns integration
of odometry which might not transfer well enough from sim-
ulation, indicating insufficient noise modeling in simulation.
This will be addressed in future work. In another variant the
dynamic point goal gtis provided at each time step in the
agent’s egocentric frame. It is calculated from an external
localization source, noisy in simulation. Letting the agent
itself take care of the point goal integration seems to be the
better solution.
Comparison with a map based planner — Table 4
compares the method with a ROS 2 based planner, which
uses a 2D occupancy map constructed beforehand (not on
the fly). To be comparable, we finetuned our agent on a
Matterport scan of the same building. While the ROS 2 based
planner is still more efficient in terms of SR and time (SCT),
it requires many experiments to finetune its parameters. For
example, low values for the inflation radius will produce
many collisions with tables. But when this parameter is too
1
 0 1 2 3
x (m)4
3
2
1
0y (m)
Real
Sim(+dyn.)0 5 10 15 20 25
t (s)0.00.20.40.6v (m/s)
0 5 10 15 20 25
t (s)1.0
0.5
0.00.51.0w (rad/s)Command
Real
Sim(+dyn.)Figure 8. Dynamical sim2real gap: we compared recorded trajec-
tories to simulated rollouts obtained with the same actions.
high, the planner cannot find any path through doors and
corridors.
Rearrangement — we test the impact of rearranging
furniture significantly in the Office environment (see the
supp.mat. for pictures) and show the effect in Table 4, block
Office/20-alt . The differences are neglectable.
Dynamical sim2real gap — Figure 8 compares real robot
trajectories obtained by the agent on a small map of 4m×4m
with rollouts of the motion model in simulation with the
same actions, indicating very small drift.
lin+3
lin+2
lin+1
lin+0ang+3
ang+2
ang+1
ang+0
ang-1
ang-2
ang-30%10%20%30%40%50%
Figure 9. Distribution of actions ,
Office/20 ,Sim, Real.Actions taken —
Figure 9 shows his-
tograms of the actions
taken by the action in
simulation and in real
onOffice/20 . There
is a clear preference
for certain combina-
tions of linear and an-
gular velocity. Dis-
tributions in sim and
real mostly match, ex-
cept for a tendency to
turn in-place in real, which could be explained by obstacle
mis-detections due to scan range noise.
6. Conclusion
We have presented an end-to-end trained method for swift
and precise navigation which can robustly avoid even thin
and finely structured obstacles. This is achieved with train-
ing in simulation only by adding a realistic motion model
identified from recorded trajectories from a real robot. The
method has been extensively evaluated in simulation as well
as on a real robotic platform, where we assessed the im-
pact of the motion model, the action space, and the way
how a point goal is calculated and provided to the agent.
The method is robust, future work will focus on closed-loop
adaptation of dynamics and sensor noise estimation.
17844
References
[1]Peter Anderson, Angel X. Chang, Devendra Singh Chaplot,
Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana
Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva,
and Amir Roshan Zamir. On evaluation of embodied naviga-
tion agents. arXiv preprint , 2018. 6
[2]Somil Bansal, Varun Tolani, Saurabh Gupta, Jitendra Malik,
and Claire Tomlin. Combining optimal control and learning
for visual navigation in novel environments. In CoRL , 2020.
2
[3]Edward Beeching, Jilles Dibangoye, Olivier Simonin, and
Christian Wolf. Learning to reason on uncertain topological
maps. In ECCV , 2020. 2
[4]Edward Beeching, Jilles Dibangoye, Olivier Simonin, and
Christian Wolf. Egomap: Projective mapping and structured
egocentric memory for deep RL. In ECML-PKDD , 2020. 2
[5]Guillaume Bono, Leonid Antsfeld, Assem Sadek, Gianluca
Monaci, and Christian Wolf. Learning with a Mole: Trans-
ferable latent spatial representations for navigation without
reconstruction. In ICLR , 2024. 2
[6]Guillaume Bresson, Zayed Alsayed, Li Yu, and S ´ebastien
Glaser. Simultaneous localization and mapping: A survey of
current trends in autonomous driving. IEEE Transactions on
Intelligent Vehicles , 2017. 1, 2
[7]Wolfram Burgard, Armin B Cremers, Dieter Fox, Dirk
H¨ahnel, Gerhard Lakemeyer, Dirk Schulz, Walter Steiner,
and Sebastian Thrun. The interactive museum tour-guide
robot. In Aaai/iaai , pages 11–18, 1998. 2
[8]Devendra Singh Chaplot, Dhiraj Gandhi, Abhinav Gupta, and
Ruslan Salakhutdinov. Object goal navigation using goal-
oriented semantic exploration. In NeurIPS , 2020. 2
[9]Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Ab-
hinav Gupta, and Ruslan Salakhutdinov. Learning to explore
using active neural slam. In ICLR , 2020. 2
[10] Devendra Singh Chaplot, Ruslan Salakhutdinov, Abhinav
Gupta, and Saurabh Gupta. Neural topological slam for visual
navigation. In CVPR , 2020. 2
[11] Prithvijit Chattopadhyay, Judy Hoffman, Roozbeh Mottaghi,
and Aniruddha Kembhavi. Robustnav: Towards benchmark-
ing robustness in embodied navigation. CoRR , 2106.04531,
2021. 1, 2, 6
[12] Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi,
Cordelia Schmid, and Ivan Laptev. Think Global, Act Lo-
cal: Dual-scale Graph Transformer for Vision-and-Language
Navigation. arXiv:2202.11742 , 2022. 2
[13] Jinyoung Choi, Kyungsik Park, Minsu Kim, and Sangok Seok.
Deep reinforcement learning of navigation in a complex and
crowded environment with a limited field of view. In ICRA ,
2019. 2
[14] Yiming Ding, Carlos Florensa, Pieter Abbeel, and Mariano
Phielipp. Goal-conditioned imitation learning. In NeurIPS ,
2019. 1, 2
[15] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch,
Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan
Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen
Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine,
Vincent Vanhoucke, Karol Hausman, Marc Toussaint, KlausGreff, Andy Zeng, Igor Mordatch, and Pete Florence. PALM-
E: An Embodied Multimodal Language Model. In ICML ,
2023. 1
[16] Heming Du, Xin Yu, and Liang Zheng. Vtnet: Visual trans-
former network for object goal navigation. arXiv preprint
arXiv:2105.09447 , 2021. 2
[17] Benjamin Eysenbach, Shreyas Chaudhari, Swapnil Asawa,
Sergey Levine, and Ruslan Salakhutdinov. Off-dynamics
reinforcement learning: Training for transfer with domain
classifiers. In ICLR , 2021. 2
[18] Kuan Fang, Alexander Toshev, Li Fei-Fei, and Silvio Savarese.
Scene memory transformer for embodied agents in long-
horizon tasks. In CVPR , 2019. 2
[19] Dieter Fox, Wolfram Burgard, and Sebastian Thrun. The
dynamic window approach to collision avoidance. IEEE
Robotics & Automation Magazine , 4(1):23–33, 1997. 2
[20] Theophile Gervet, Soumith Chintala, Dhruv Batra, Jitendra
Malik, and Devendra Singh Chaplot. Navigating to objects in
the real world. Science Robotics , 8(79), 2023. 1, 2
[21] Jo˜ao F. Henriques and Andrea Vedaldi. Mapnet: An allo-
centric spatial memory for mapping environments. In CVPR ,
2018. 2
[22] Sebastian H ¨ofer, Kostas E. Bekris, Ankur Handa, Juan
Camilo Gamboa Higuera, Florian Golemo, Melissa Mozifian,
Christopher G. Atkeson, Dieter Fox, Ken Goldberg, John
Leonard, C. Karen Liu, Jan Peters, Shuran Song, Peter Welin-
der, and Martha White. Perspectives on sim2real transfer for
robotics: A summary of the R: SS 2020 workshop, 2020. 2
[23] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch,
Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jack-
son, Linda Luu, Sergey Levine, Karol Hausman, and Brian
Ichter. Inner Monologue: Embodied Reasoning through Plan-
ning with Language Models. In CoRL , 2022. 1
[24] Max Jaderberg, V olodymyr Mnih, Wojciech Marian Czar-
necki, Tom Schaul, Joel Z. Leibo, David Silver, and Koray
Kavukcuoglu. Reinforcement learning with unsupervised
auxiliary tasks. In ICLR , 2017. 1, 2
[25] Abhishek Kadian, Joanne Truong, Aaron Gokaslan, Alexan-
der Clegg, Erik Wijmans, Stefan Lee, Manolis Savva, Sonia
Chernova, and Dhruv Batra. Sim2Real Predictivity: Does
Evaluation in Simulation Predict Real-World Performance?
IEEE Robotics and Automation Letters , 5(4):6670–6677,
2020. 2
[26] Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu,
Abhinav Gupta, and Ali Farhadi. AI2-THOR: An Interactive
3D Environment for Visual AI. CoRR , 1712.05474, 2017. 2
[27] Kurt Konolige. A gradient method for realtime robot control.
InIROS , 2000. 2
[28] Mathieu Labb ´e and Fran c ¸ois Michaud. RTAB-Map as an
open-source lidar and visual simultaneous localization and
mapping library for large-scale and long-term online opera-
tion. Journal of Field Robotics , 36(2):416–446, 2019. 2
[29] Chen Liu, Kiran Lekkala, and Laurent Itti. World model
based sim2real transfer for visual navigation. In NeurIPS
Robot Learning Workshop , 2023. 2
17845
[30] Iker Lluvia, Elena Lazkano, and Ander Ansuategi. Active
Mapping and Robot Exploration: A Survey. Sensors , 21(7):
2445, 2021. 1
[31] Steve Macenski, Francisco Mart ´ın, Ruffin White, and
Jonatan Gin ´es Clavero. The marathon 2: A navigation system.
InIROS , 2020. 2
[32] Eitan Marder-Eppstein, Eric Berger, Tully Foote, Brian
Gerkey, and Kurt Konolige. The office marathon: Robust
navigation in an indoor office environment. In ICRA , 2010. 2
[33] Pierre Marza, Laetitia Matignon, Olivier Simonin, and Chris-
tian Wolf. Multi-Object Navigation with dynamically learned
neural implicit representations. In ICCV , 2023. 2
[34] Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer,
Andy Ballard, Andrea Banino, Misha Denil, Ross Goroshin,
Laurent Sifre, Koray Kavukcuoglu, Dharshan Kumaran, and
Raia Hadsell. Learning to navigate in complex environments.
InICLR , 2017. 1
[35] Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer,
Andy Ballard, Andrea Banino, Misha Denil, Ross Goroshin,
Laurent Sifre, Koray Kavukcuoglu, Dharshan Kumaran, and
Raia Hadsell. Learning to navigate in complex environments.
InICLR , 2017. 2
[36] Katsuhiko Ogata. Modern Control Engineering . Prentice
Hall, 2010. 5
[37] Emilio Parisotto and Ruslan Salakhutdinov. Neural map:
Structured memory for deep reinforcement learning. In ICLR ,
2018. 2
[38] Ruslan Partsey, Erik Wijmans, Naoki Yokoyama, Oles Dobo-
sevych, Dhruv Batra, and Oleksandr Maksymets. Is mapping
necessary for realistic pointgoal navigation? In CVPR , 2022.
1
[39] Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba,
and Pieter Abbeel. Sim-to-real transfer of robotic control with
dynamics randomization. In ICRA , 2018. 2
[40] Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wi-
jmans, Oleksandr Maksymets, Alexander Clegg, John M
Turner, Eric Undersander, Wojciech Galuba, Andrew West-
bury, Angel X Chang, Manolis Savva, Yili Zhao, and Dhruv
Batra. Habitat-matterport 3D dataset (HM3D): 1000 large-
scale 3d environments for embodied AI. In NeurIPS Datasets
and Benchmarks Track , 2021. 6
[41] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez
Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai
Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springen-
berg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards,
Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals,
Mahyar Bordbar, and Nando de Freitas. A Generalist Agent.
arXiv:2205.06175 , 2022. arXiv: 2205.06175. 2
[42] Christoph R ¨osmann, Frank Hoffmann, and Torsten Bertram.
Timed-elastic-bands for time-optimal point-to-point nonlinear
model predictive control. In European Control Conference
(ECC) , 2015. 2
[43] Assem Sadek, Guillaume Bono, Boris Chidlovskii, and Chris-
tian Wolf. An in-depth experimental study of sensor usage
and visual reasoning of robots navigating in real environments.
InICRA , 2022. 1, 2, 7
[44] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,
Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, JiaLiu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv
Batra. Habitat: A platform for embodied ai research. In ICCV ,
2019. 1, 2, 4, 5, 6
[45] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-
ford, and Oleg Klimov. Proximal policy optimization algo-
rithms. arXiv preprint , 2017. 6
[46] James A Sethian. A fast marching level set method for mono-
tonically advancing fronts. PNAS , 93(4):1591–1595, 1996.
2
[47] Dhruv Shah and Sergey Levine. ViKiNG: Vision-
Based Kilometer-Scale Navigation with Geographic Hints.
arXiv:2202.11271 , 2022. 2
[48] Dhruv Shah, Ajay Sridhar, Nitish Dashora, Kyle Stachowicz,
Kevin Black, Noriaki Hirose, and Sergey Levine. ViNT: A
foundation model for visual navigation. In CoRL , 2023. 2
[49] Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei
Bai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke.
Sim-to-real: Learning agile locomotion for quadruped robots.
InRSS, 2018. 2
[50] Sebastian Thrun, Wolfram Burgard, and Dieter Fox. Proba-
bilistic robotics, 2005. 1, 2, 3
[51] Joanne Truong, Sonia Chernova, and Dhruv Batra. Bi-
directional Domain Adaptation for Sim2Real Transfer of Em-
bodied Navigation Agents. IEEE Robotics and Automation
Letters , 6(2), 2021. 2
[52] Joanne Truong, Max Rudolph, Naoki Yokoyama, Sonia Cher-
nova, Dhruv Batra, and Akshara Rai. Rethinking sim2real:
Lower fidelity simulation leads to higher sim2real transfer in
navigation. In CoRL , pages 859–870, 2022. 2
[53] Naoki Yokoyama, Sehoon Ha, and Dhruv Batra. Success
weighted by completion time: A dynamics-aware evaluation
criteria for embodied navigation. In IROS , 2021. 2, 6, 7
[54] Naoki Yokoyama, Qian Luo, Dhruv Batra, and Sehoon Ha.
Learning robust agents for visual navigation in dynamic envi-
ronments: The winning entry of igibson challenge 2021. In
IROS , pages 77–83, 2022. 2
[55] Fangyi Zhang, J ¨urgen Leitner, Zongyuan Ge, Michael Mil-
ford, and Peter Corke. Adversarial discriminative sim-to-real
transfer of visuo-motor policies. Int. J. Robotics Res. , 38
(10-11), 2019. 2
[56] Xinge Zhu, Jiangmiao Pang, Ceyuan Yang, Jianping Shi, and
Dahua Lin. Adapting object detectors via selective cross-
domain alignment. In CVPR , 2019. 2
17846
