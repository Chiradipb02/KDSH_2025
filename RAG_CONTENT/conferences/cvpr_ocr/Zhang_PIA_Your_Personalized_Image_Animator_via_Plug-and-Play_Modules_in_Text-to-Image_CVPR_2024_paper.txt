PIA: Your Personalized Image Animator
via Plug-and-Play Modules in Text-to-Image Models
Yiming Zhang1,2,*Zhening Xing1,*Yanhong Zeng1,‚Ä†Youqing Fang1Kai Chen1,‚Ä†
1Shanghai Artificial Intelligence Laboratory2Dalian University of Technology
https://github.com/open-mmlab/PIA
Figure 1. Given an elaborated image generated by a personalized text-to-image model, the proposed Personalized Image Animator ( PIA)
animates it with realistic motions according to different text prompts while preserving the original distinct styles and high-fidelity details.
We recommend using Abode Arobat and clicking the images to play the animation clips. [Best viewed in color with zoom-in]
Abstract
Recent advancements in personalized text-to-image
(T2I) models have revolutionized content creation, empow-
ering non-experts to generate stunning images with unique
styles. While promising, animating these personalized im-
ages with realistic motions poses significant challenges in
preserving distinct styles, high-fidelity details, and achiev-
ing motion controllability by text. In this paper, we present
PIA, aPersonalized Image Animator that excels in align-
ing with condition images, achieving motion controllabil-
ity by text, and the compatibility with various personalized
T2I models without specific tuning. To achieve these goals,
PIA builds upon a base T2I model with well-trained tempo-
‚àódenotes equal contribution,‚Ä†denotes corresponding author.ral alignment layers, allowing for the seamless transforma-
tion of any personalized T2I model into an image animation
model. A key component of PIA is the introduction of the
condition module, which takes as inputs the condition frame
and inter-frame affinity. This module leverages the affinity
hint to transfer appearance information from the condition
frame to individual frames in the latent space. This design
mitigates the challenges of appearance-related frame align-
ment within PIA and allows for a stronger focus on align-
ing with motion-related guidance. To address the lack of
a benchmark for this field, we introduce AnimateBench , a
comprehensive benchmark comprising diverse personalized
T2I models, curated images, and motion-related prompts.
We show extensive evaluations and applications on Ani-
mateBench to verify the superiority of PIA.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
7747
1. Introduction
Recent democratization of text-to-image (T2I) generation
has seen incredible progress by the growing Stable Diffu-
sion community [1, 30] and the explosion of personalized
models [18, 31]. Numerous AI artists and enthusiasts cre-
ate, share, and use personalized models on model-sharing
platforms such as Civitai [6] and Hugging Face [19], turn-
ing imagination into images at an affordable cost (e.g., a
laptop with an RTX3080). However, there is still limited
research exploring the realm of infusing these unique per-
sonalized images with dynamic motions [4, 12, 37].
We are intrigued by the potential of animating person-
alized images, given their distinctive styles and impressive
visual quality. Moreover, the incorporation of text prompts
to guide the animation process can significantly streamline
various creative endeavors within the vibrant personalized
community, minimizing the required effort. While promis-
ing, this task presents two distinct challenges: image align-
ment with the detailed personalized image and achieving
motion controllability through text prompt. While recent
advancements in training large-scale text-to-video (T2V)
models have enabled users to generate videos based on text
or images [37], these approaches struggle to preserve the
unique styles and high-fidelity details of conditional images
due to the absence of personalized domain knowledge. To
address this limitation, researchers have developed temporal
alignment layers [4, 12]. These plug-and-play modules ef-
fectively transform personalized T2I models into T2V mod-
els while maintaining their distinct styles [4, 12]. Despite
numerous attempts to adapt these personalized T2V mod-
els for image animation [41, 42], the integrated temporal
alignment layers mainly focus on aligning the appearance
of individually generated frames in latent space to ensure
smooth video outcomes. As a result, they find it hard to
animate images according to motion-related text prompts.
To address the aforementioned limitations, we introduce
PIA, aPersonalized Image Animator that excels in both im-
age alignment and motion controllability. First, we choose
to build our framework upon a base T2I model ( i.e., Sta-
ble Diffusion [30]) and incorporate well-established tempo-
ral alignment layers following previous works [12]. This
approach allows PIA to effectively leverage personalized
domain knowledge by seamlessly replacing the base T2I
model with any personalized T2I model during inference.
Second, to empower our framework with image animation
ability, we introduce a trainable condition module into the
input layer of the T2I model. This module plays a crucial
role in generating each frame of the animation by taking the
conditional image and the inter-frame affinity between the
current frame and the conditional frame as inputs. Through
this design, the condition module effectively borrows ap-
pearance features from the conditional frame, guided by
the affinity hint, resulting in improved alignment with thecondition image. Finally, we fine-tune the condition mod-
ule and the temporal alignment layers jointly while keep-
ing the base T2I model fixed. Leveraging the enhanced
appearance-related alignment facilitated by the condition
module within the input layer, the temporal alignment lay-
ers can shift their focus towards motion-related alignment,
leading to improved motion controllability. Importantly, we
demonstrate the effortless generalizability of PIA‚Äôs anima-
tion capability by replacing the T2I model with any other
personalized model. This flexibility empowers users to an-
imate their elaborated personal images using text prompts
while preserving distinct features and high-fidelity details.
In summary, PIA presents a powerful solution for per-
sonalized image animation, offering superior image align-
ment, motion controllability by text, and the flexibility to
integrate various personalized models. This comprehensive
approach ensures an engaging and customizable animation
experience for users. To address the lack of a benchmark in
personalized image animation, we introduce a new bench-
mark called AnimateBench , which comprises various per-
sonalized T2Is, curated images, and tailored motion-related
prompts. To validate the effectiveness of PIA, we conducted
extensive quantitative and qualitative evaluations using An-
imateBench. These evaluations provide a robust assessment
of the capabilities and performance of our approach.
2. Related Work
Single Image Animation. Enhancing still pictures with
motions has attracted decades of attention and efforts in the
research field [3, 5, 8, 17, 34, 35, 39]. Chuang et al. present
a pioneer work for animating a variety of photographs and
paintings [5]. Specifically, the still scene is segmented into
a series of layers by a human user and each layer is ren-
dered with a stochastic motion texture to form the final an-
imated video. With the success of deep learning, more re-
cent works get rid of the manual segmentation and can syn-
thesize more natural motion textures [3, 8, 17, 34, 35, 39].
Endo et al. propose learning and predicting the motion and
appearance separately using two types of DNN models for
animating landscape [8]. Holynski et al. learns an image-to-
image translation network to encode motion priors of natu-
ral scenes with continuous fluid [17]. Bertiche et al . fo-
cus on learning garment motion dynamics for blowing the
dressed humans under the wind [3]. Xiao et al. propose au-
tomatically segmenting hair wisps and animating hair wisps
with a wisp-aware animation module with pleasing motions
[39]. Some works focus on synthesizing natural lip motion
for talking heads based on audio [34, 35]. Recent works
also have been significantly prompted by the powerful dif-
fusion models [21, 23, 30]. Mahapatra et al. transfer the
estimated optical flow from real images to artistic paintings
by the concept of twin image synthesis with a pre-trained
text-to-image diffusion model [23]. Li et al . leverages a
7748
Res + SACATAPrompt: ‚ÄúA dog running‚Äù
Aligned frames
(b) U-Net Block and Temporal Alignment layer(TA)
ùëß!
Prompt: ‚ÄúA dog running‚Äù
Iterative denoising: ùëß!‚Üíùëß!"#U-Net BlockTA
U-Net BlockTA
U-Net BlockTA
ùë§$%&'ùë§
ùëß!(&=‚Ñù)√ó(,-#-,)√ó/√ó0ùë†#:)ùë†#,‚Ä¶ùë†)
√óùêπ‚Ñ∞ùêº(a) The overview of Personalized Image Animator(PIA)
Individualframes
Frozen weights
Res: ResBlockCA: Cross-AttentionSA: Self-AttentionTrainable weightsFigure 2. Personalized Image Animator (PIA) . As shown in (a), PIA consists of a text-to-image (T2I) model, well-trained temporal
alignment layers (TA), and a new condition module Wcond responsible for encoding the condition image zIand inter-frame affinity s1:F.
In particular, the T2I model consists of U-Net blocks, including a ResBlock (Res) [14], a self-attention layer (SA), and a cross-attention
layer (CA), as depicted in (b). During training, the condition module learns to leverage the affinity hints and incorporate appearance
information from the condition images, facilitating image alignment and enabling a stronger emphasis on motion-related alignment.
latent diffusion model for modeling natural oscillating mo-
tion such as flowers and candles blowing in the wind [21].
Despite the significant progress, these animation techniques
can only synthesize specific types of content and motion
such as time-lapse videos, and facial and body animation.
In contrast, our model, PIA, can animate the image without
segmenting the region of interest and drive the image with
any motion through a text prompt.
Text-to-video Synthesis. The synthesis of images and
videos is a classic and important topic that has received
significant research attention by decades [11, 15, 20, 43].
Recent large text-to-image models have shown remarkable
progress by enabling diverse and high-fidelity image syn-
thesis based on a text prompt written in natural language
[26, 28‚Äì30, 33, 44]. Most of text-to-video (T2V) generation
models are built upon the well-trained text-to-image mod-
els and can synthesize smooth videos [4, 12]. Some video
latent diffusion models (VLDMs) have shown promising re-
sults in synthesizing natural videos [4, 9, 12, 13, 16, 22, 38,
40]. Despite numerous attempts that try to adapt these T2V
models for image animation, it remains a challenge to align
with the conditional image and control the image animation
by text [32, 42]. Among them, we highlight AnimateDiff as
a pioneer work that aims at training a plug-and-play motion
module on large-scale video datasets [12]. After training, it
can transfer the learned realistic motion priors to new do-
mains by inserting the motion module into customized T2I
models. Compared with AnimateDiff, our proposed PIA
empowers any text-to-image models into an image anima-
tor, which can animate personalized images with better im-
age fidelity and motion controllability.3. PIA: Your Personalized Image Animator
To animate an image Igenerated by a personalized text-to-
image model, our target is to generate a video clip V=
{v1, . . . , vF}with a length of Fframes, where the first
frame v1should resemble condition image I, and the gener-
ated motion should follow the text prompt c. To achieve this
goal, we propose PIA, which aims at training plug-and-play
models that is able to turn any text-to-image personalized
models into an image animator.
As shown in Fig. 2, PIA consists of a text-to-image
model, well-trained temporal alignment layers, and a new
condition module. First, the condition module takes as input
the condition frame Iand the inter-frame affinity s1:F. The
condition module can transfer appearance information to
each frame according to the affinity hints, leading to a bet-
ter appearance alignment within the input layers. Second,
the condition module and the temporal layers are trained
jointly while maintaining the T2I model frozen. Such a de-
sign encourages the temporal alignment layers to shift focus
toward motion-related alignment. Finally, PIA can be com-
bined with any personalized model by replacing the base
T2I model. We introduce more in this section.
3.1. Preliminaries
Stable Diffusion. Stable Diffusion (SD) is one of the
most popular large-scale open-source text-to-image mod-
els and has a well-developed community [30]. SD has four
main components: a encoder E, a decoder D, a U-Net de-
noiser œµŒ∏, and a pretrained CLIP-based [27] text encoder T.
In the training stage, the input image xis first encoded to the
latent code z0=E(x). Then z0is perturbed via pre-defined
7749
diffusion process [7, 15, 24]:
zt=‚àö¬ØŒ±tz0+‚àö
1‚àí¬ØŒ±tœµt, œµt‚àº N(0, I) (1)
where timestep t‚àº U[1, T], and ¬ØŒ±tis the noise strength at
t. Then the denoising U-Net œµis trained to predict the added
noise with the following loss:
L=Ex,œµ‚àºN(0,1),t,c[‚à•œµ‚àíœµŒ∏(zt, t,T(c))‚à•] (2)
where cis the corresponding prompt for x0.
Temporal Alignment Layer. Recent T2V methods have
demonstrated that incorporating attention layers [36] into
SD in the temporal dimension enables the alignment of ap-
pearance and motion information across frames, thereby
transforming the T2I model into a T2V model [4, 12, 38].
We take a diffusion feature with a shape of f‚ààRF,C,H,W
as an example, where Fdenotes the frame length, Cde-
notes the number of channels, and HandWdenote the
spatial dimensions. The temporal alignment layer begins
by permuting the shape of the feature to ((H, W ), F, C ),
and conducts the temporal attention as follows,
fout=Softmax (QK‚ä∫
‚àöc)¬∑V, (3)
where Q=WQz,K=WKz,V=WVzare projec-
tion operations. Through such a design, the temporal align-
ment layers are trained to align both appearance and motion
information across all the frames simultaneously. Despite
some promising results, a trade-off arises between appear-
ance consistency and motion controllability in the generated
videos. In our work, PIA introduces a condition module for
appearance alignment and enables the temporal alignment
layers to shift focus toward motion-related alignment.
3.2. Plug-and-Play Modules for Animation
PIA consists of a base text-to-image model, temporal align-
ment layers, and a condition module. It takes as input a con-
dition frame and inter-frame affinities to generate videos.
As illustrated in Fig. 3-(a), in a typical T2V model, the
initial latent for each frame is independently sampled from
noise and synthesized individually, except for the temporal
attention operations across all frames by the temporal align-
ment layers. As a result, temporal alignment layers play a
key role as the only frame-aware modules, and it needs to
devote much effort to simultaneously learning motion prior
and aligning appearance consistency across frames. Such a
design often leads to both inconsistent appearance identities
for each frame and limited motion control by text prompt.
Inter-frame Affinity. To overcome the challenge of
aligning both appearance and motion across all the frames
simultaneously, we propose to explicitly encode the appear-
ance into the latent from the condition frame Iwith affin-
ity hint, i.e.zI=E(I). During training, we calculate the
(a)Vanilla Personalized T2V: w/o CM (b) PIA: w/ CM U-Net BlocksConvIN+ CMùë§!"#$ùë§ConvINùë§U-Net BlocksTAùëß%ùëß%ùëß&‚äïùë†TAIndividualframesAligned framesIndividualframesAligned framesFigure 3. Illustration of the condition module (CM). A vanilla
personalized T2V model (shown in (a)) needs to align both the
appearance and motion of individual frames simultaneously. PIA
with CM (shown in (b)) can borrow appearance information from
condition image zIwith affinity hints s, easing the challenge of
both appearance and motion alignment. We use the color and strip
to denote appearance and motion, respectively.
affinity score based on the training data. Given a video
clipv1:Fand its first frame v1as the condition frame, we
first calculate the L1 distance between each frame viand
the condition frame v1in HSV space, which is denoted as
di. Next, we apply this operation to all frames of video
clips in the dataset and find the maximum distance value
dmax. We normalize the distance dito[0,1]viadmax. Fi-
nally, the affinity score for each frame can be calculated by
si= 1‚àídi/dmax√ó(smax‚àísmin), where smaxandsminare
hyperparameters to scale affinity scores to specific ranges.
To align the shape of siwithzI, we expand it to a single-
channel feature map with the same size as latent code ( i.e.
[1√óh√ów]). After training, users can manually set the
value of sito animate images with varying motion scales,
providing customization in the output.
Condition Module. To encode the additional inputs of
the condition image and the inter-frame affinity based on the
text-to-image U-Net, we introduce a lightweight learnable
single-layer convolution as the condition module without
compromising the original functionality. Given condition
inputs zI‚äïsi, where ‚äïdenotes concatenation operation,
we encode it with the condition module Wcond and add the
encoded feature to the output of the first convolution layer.
As shown in Fig. 3-(b), the incorporation of the condition
module leads to enhanced consistency of the appearance in-
formation in the output of the first convolutional layer com-
pared to the original model. This makes subsequent tem-
poral alignment layers shift focus toward aligning motion-
related guidance. This operation is equivalent to concate-
nating Wcond andW, the weight of the original convolu-
tion block, at the first dimension. To insert the newly added
weight without hurting the performance of the pre-trained
model, we apply zero-initialization [42] to Wcond.
7750
3.3. Training and inference
We inherit the pre-training of domain adapter and temporal
alignment layers from AnimateDiff [12]. Specifically, the
domain adapter pre-training is conducted on images from
the video dataset to prevent the temporal alignment layers
from learning the low visual quality of the video dataset.
Then, the temporal alignment layers are trained on a video
dataset to learn realistic motion prior. Finally, we insert the
condition module into the first convolution layer in U-Net
and initialize the weights as zero. During training, given a
sampled video clip v1:F, we designate the first frame as the
condition frame, i.e.I:=v0. We jointly trained the Wcond
and the temporal alignment layers while keeping the other
parameters fixed as follows,
L=Ev1:F,œµ‚àºN(0,I),t,c[‚à•œµ‚àíœµŒ∏(z1:F
t, z0
0, s1:F, t,T(c))||2
2],
(4)
where cis the text prompt, z0
0is the clean latent of the
condition frame and z1:F
tis the perturbed latent.
During training, to retain the model‚Äôs text-to-video ca-
pability, we randomly drop inputs with a 20% probability,
effectively transforming the training task into a T2V task.
Specifically, we replace both s1:FandzIwith zeros. When
calculating the affinity score, we set smax andsminto be
1 and 0.2, respectively. Both single frame and video data
training are conducted on WebVid dataset [2]. The num-
ber of frames Fis fixed as 16 during training. After train-
ing, given an image Igenerated by a personalized model,
we replace the domain adapter with the personalized model
and fix the temporal alignment layers and condition module.
The inter-frame affinity sserves as a user input to control
the magnitude of motion in generated videos.
4. Experiments
4.1. AnimateBench
AnimateBench. Existing benchmarks are restricted to
specific domains like human faces, fluid elements, etc. To
this end, we introduce AnimateBench for comparisons in
the field of personalized image animation. AnimateBench
contains images in different domains and multiple prompts
to test the text-based image animation ability. Specifically,
we evaluated 105 personalized cases, which contain dif-
ferent contents, styles, and concepts. These personalized
cases are generated from seven different personalized text-
to-image models, which are used to evaluate the domain
generalization ability. We use five images generated by each
model for a comprehensive comparison. In addition, we
carefully elaborate three motion-related prompts for each
image to evaluate the motion controllability by the text of
different approaches. Specifically, the motion-related text
prompts typically describe the following motions that the
image probably happens within a single short shot.
Figure 4. An example of AnimateBench . The images in Ani-
mateBench are carefully crafted using a set of collected person-
alized text-to-image models. Each image has three carefully de-
signed prompts, describing the following motions that the image
probably happens within a single short shot.
Evaluation Metrics. Our quantitative comparison evalu-
ates animation quality including image alignment andtext
alignment , following previous works [12, 38]. Specifically,
the CLIP score is calculated by computing the cosine sim-
ilarity between different embeddings, e.g., text and image.
Therefore, we can use the CLIP score to evaluate both text
and input image similarity with videos.
‚Ä¢Image Alignment We compute the cosine similarity be-
tween input image embeddings and each video frame em-
beddings to evaluate the image-video alignment.
‚Ä¢Text Alignment We also compute the cosine similarity
between text embeddings and frame embeddings to eval-
uate the text-video alignment.
Baselines. We carefully choose the most recent and com-
petitive approaches for personalized image animation with
their brief introduction as follows,
‚Ä¢VideoComposer [37] can generate controllable videos
using different conditions such as text, image, motion
vector, or a combination of them.
‚Ä¢AnimateDiff [12] is a milestone work that largely facil-
itates the development of personalized video generation
by learning motion prior. We extend it for the application
of image animation following previous best practices by
using ControlNet [42] and image prompt adapter [41].
‚Ä¢Pika Labs [25] and Gen2 [10] are two of the most pop-
ular commercial tools for video generation, which are
widely recognized as state-of-the-art in image animation.
4.2. Comparisons with State-of-the-Art
We compare our method with state-of-the-art for personal-
ized image animation on AnimateBench. The performance
of these models was evaluated in various terms of text align-
ment, and image alignment.
Qualitative Comparison. We qualitatively compared
PIA with the most recent animation methods, including
7751
Figure 5. Qualitative comparison with state-of-the art approaches. Compared with other methods, PIA shows excellent motion con-
trollability and strong image alignment. Specifically, in the first case, PIA generates a ‚Äúwalking‚Äù motion for the toy bear (in its feet), while
other methods can only remain static frames, showing a lack of motion controllability. In the second case, PIA adds a new element, i.e.,
fire, with realistic motion. We show more video cases in supplementary materials due to the file size limit of the main paper.
AnimateDiff-ControlNet, Gen-2, Pika Labs, and Video-
Composer. Visual results can be found in Fig. 5. It can
be observed that the videos generated by PIA exhibit better
responses to motion-related keywords in the prompts and
better preservation of details from the input image content.
VideoComposer produces video clips with poor text align-ment and incoherent motion. Animatediff-ControlNet, on
the other hand, generates videos with smooth motion but
loses image details. Gen2 and Pika Labs are prone to gener-
ate videos with high fidelity but respond less to the prompt.
Quantitative Evaluation. To test both the personalized
cases in AnimateBench and open-domain cases in WebVid
7752
MethodsAnimateBench WebVid User Study
image text image text image text
VideoComposer [37] 90.10 66.17 85.03 75.52 0.180 0.110
AnimateDiff [12] 89.72 68.41 72.82 72.86 0.295 0.220
PIA (ours) 93.44 68.74 85.11 76.59 0.525 0.670
Table 1. Quantitative comparison with state-of-the-art approaches.
[2] for a comprehensive comparison, we calculate the CLIP
Score to compare text alignment and image alignment. We
conclude the results in Table 1. The results demonstrate that
PIA achieves the best CLIP Scores, indicating strong text-
video alignment and high image fidelity.
To conduct a user study, we generate videos with differ-
ent methods and set up twenty questions in total. Note that,
all the cases are selected randomly for fair testing. In each
trial, we ask the users to choose the video that matches the
text or image best. Finally, we conclude the preference rate
to evaluate the performance of all the methods in Tab. 1.
4.3. Analysis
In this section, we present some interesting applications and
conduct ablative study. Specifically, we first introduce three
abilities of PIA, which are motion control by text prompt,
motion magnitude controllability and style transfer. We
show visual results for each application. Then we com-
pare two different frame affinities to clarify the effective-
ness. Details of different constructions of frame affinity will
be explained in the following.
Motion Control by Text Prompt. The condition module
enables PIA to borrow appearance from the condition frame
so that temporal alignment layers can focus more on mo-
tion generation. These designs enable PIA to perform better
image and text alignment in generated videos. The strong
prompt control ability of PIA can be found in Fig. 6. Dur-
ing the test, we observed that a simple prompt design pro-
duces great visual effects, such as fire and lightning. PIA
can generate fancy results by adding new elements to the
generated videos with smooth motion quality based on text
prompts. Unlike Pika Labs or Gen-2 which are prone to
generate small motion while ignoring text prompt, PIA gen-
erates video with more text alignment motion.
Motion Magnitude Controllability. PIA can control the
magnitude of motion by adjusting the frame affinity. Specif-
ically, we first make statistics of the frame affinity across the
whole video training dataset. Then, according to the statis-
tical results, we roughly divide the range of frame affinity
into three groups and find three corresponding sets of inter-
frame affinity values. Finally, during inference, given the
same condition image and text prompt, PIA can achieve
videos with different motion magnitudes by using differ-
ent sets of inter-frame affinity inputs. As shown in Fig. 7,
Prompt: Fireworks , 1castle
Prompt: 1castle is on fire
Prompt: Lightning struck the castleFigure 6. Motion Control by Text Prompt. PIA can effectively
capture the motion-related guidance in text prompt and add realis-
tic related motion in the results.
Light Middle Large
Generated Videos
Prompt: Jumping , a golden labrador, ‚Ä¶‚Ä¶
Figure 7. Motion Magnitude Controllability. PIA enables users
to adjust the motion magnitude as light, middle, or large by setting
the input affinity information as different values.
the frame affinity gradually decreases from top to bottom,
indicating an increasing magnitude of motion. From the re-
sults, it can be observed that the dog can jump with different
magnitudes, demonstrating the effectiveness of the frame
affinity in controlling motion magnitude. In short, by us-
ing different frame affinity during the inference stage, users
can flexibly control the motion magnitude of the generated
video, leading to a more controllable animation.
Style Transfer. So far, we have discussed using models
and input images in the same domain to generate videos.
7753
Figure 8. Style transfer. Given the same input image and text
prompt, PIA can generate different styles of videos like style
transfer by using different personalized text-to-image models ( e.g.,
RCNZ 3d Cartoon and ToonYou in this case).
However, we have found interesting facts that if the two
have different domains, such as providing a real-style im-
age and using an anime-style model to generate motion, PIA
can achieve style transfer effects. As shown in Fig. 8, frame
affinity helps generate videos with smooth style transfer be-
tween image style and model style. This feature of PIA
allows users to have a better personalized experience.
Ablation Study on Frame Affinity. During training, we
calculate the similarity between the condition frame and
each frame as the affinity hints. To verify the effectiveness
of the inter-frame affinity design, we conduct an ablation
study by comparing it with one-hot frame affinity. Specif-
ically, one-hot frame affinity is constructed by setting 1 in
the condition frame and setting 0 in the other frames. Such
a design can indicate the position of the condition frame.
We evaluate two methods on the AnimateBench. One-hot
frame affinity obtains the image CLIP score of 210.9 , while
the inter-frame affinity reaches 225.9 . Moreover, we show
the visual result of video frames generated by these two de-
signs in Fig. 9. We can find that frame affinity constructed
by similarity allows the model to align with the condition
frame better. Note that in different constructions, we use
the same pattern of affinity in training and inference.
Limitation. First, we have observed that videos generated
by PIA exhibit color discrepancy when applied to images
with significantly different styles from the training data, as
shown in Fig. 10. We hypothesize the color discrepancy
is caused by training on the WebVid dataset [2]. WebVid
consists mainly of real-world recordings, characterized by
frames with motion blur and compression artifacts [2, 12].
Hence, the quality and diversity of these frames are lower
than image datasets that include professional photography
and artistic paintings [4, 12]. Training with this noticeable
quality domain gap can inevitably limit the animation of
complex images with significant stylistic differences.
One-hot Affinity
Prompt: Strong waves hitting the lighthouse, ‚Ä¶Figure 9. Ablation study on the design of inter-frame affinity.
Compared with the one-hot condition design, the design of inter-
frame affinity in PIA shows better image alignment.
Figure 10. Limitation. When the input image is far from the
training dataset domain, PIA is prone to generate videos with sig-
nificant shifts of color.
Secondly, in personalized image generation, users rely
on specific trigger words in text prompts to evoke desired
styles. However, during image animation, if these trigger
words are absent from the animation prompts, significant
color inconsistencies may arise. We further speculate that
training the model on a wider range of video data encom-
passing diverse styles and content can ease this issue. Ad-
ditionally, ensuring the inclusion of complete trigger words
during inference could help mitigate this phenomenon.
5. Conclusions
In this paper, we introduce PIA a powerful solution for per-
sonalized image animation. Our method shows excellent
image alignment and motion controllability, ensuring text-
based customized animation for users. Furthermore, we
construct an animation benchmark named AnimateBench to
evaluate the animation performance of PIA and other meth-
ods. Experimental results demonstrate that PIA performs
excellent on image animation tasks and shows various in-
teresting extended applications.
6. Acknowledgement
This project is supported by the National Key R&D Pro-
gram of China (No. 2022ZD0161600)
7754
References
[1] AUTOMATIC1111, 2023.
https://github.com/AUTOMATIC1111/stable-diffusion-
webui. 2
[2] Max Bain, Arsha Nagrani, G ¬®ul Varol, and Andrew Zisser-
man. Frozen in time: A joint video and image encoder for
end-to-end retrieval. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 1728‚Äì1738,
2021. 5, 7, 8
[3] Hugo Bertiche, Niloy J Mitra, Kuldeep Kulkarni, Chun-
Hao P Huang, Tuanfeng Y Wang, Meysam Madadi, Sergio
Escalera, and Duygu Ceylan. Blowing in the wind: Cyclenet
for human cinemagraphs from still images. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 459‚Äì468, 2023. 2
[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-
horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.
Align your latents: High-resolution video synthesis with la-
tent diffusion models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
22563‚Äì22575, 2023. 2, 3, 4, 8
[5] Yung-Yu Chuang, Dan B Goldman, Ke Colin Zheng, Brian
Curless, David H Salesin, and Richard Szeliski. Animat-
ing pictures with stochastic motion textures. In ACM SIG-
GRAPH 2005 Papers , pages 853‚Äì860, 2005. 2
[6] CIVITAI, 2022. https://civitai.com/. 2
[7] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems , 34:8780‚Äì8794, 2021. 4
[8] Yuki Endo, Yoshihiro Kanamori, and Shigeru Kuriyama. An-
imating landscape: self-supervised learning of decoupled
motion and appearance for single-image video synthesis.
ACM Transactions on Graphics (TOG) , 38(6):1‚Äì19, 2019.
2
[9] Patrick Esser, Johnathan Chiu, Parmida Atighehchian,
Jonathan Granskog, and Anastasis Germanidis. Structure
and content-guided video synthesis with diffusion models.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 7346‚Äì7356, 2023. 3
[10] Gen2, 2023. https://research.runwayml.com/gen2. 5
[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial networks. Commu-
nications of the ACM , 63(11):139‚Äì144, 2020. 3
[12] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu
Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your
personalized text-to-image diffusion models without specific
tuning. arXiv preprint arXiv:2307.04725 , 2023. 2, 3, 4, 5, 7,
8
[13] William Harvey, Saeid Naderiparizi, Vaden Masrani, Chris-
tian Weilbach, and Frank Wood. Flexible diffusion modeling
of long videos. Advances in Neural Information Processing
Systems , 35:27953‚Äì27965, 2022. 3
[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770‚Äì778, 2016. 3[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840‚Äì6851, 2020. 3, 4
[16] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William
Chan, Mohammad Norouzi, and David J. Fleet. Video diffu-
sion models, 2022. 3
[17] Aleksander Holynski, Brian L Curless, Steven M Seitz, and
Richard Szeliski. Animating pictures with eulerian mo-
tion fields. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 5810‚Äì
5819, 2021. 2
[18] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-
rank adaptation of large language models. In International
Conference on Learning Representations , 2021. 2
[19] Huggingface, 2022. https://huggingface.co/. 2
[20] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 4401‚Äì4410, 2019. 3
[21] Zhengqi Li, Richard Tucker, Noah Snavely, and Aleksander
Holynski. Generative image dynamics. arXiv preprint
arXiv:2309.07906 , 2023. 2, 3
[22] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,
Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and
Tieniu Tan. Videofusion: Decomposed diffusion mod-
els for high-quality video generation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10209‚Äì10218, 2023. 3
[23] Aniruddha Mahapatra, Aliaksandr Siarohin, Hsin-Ying Lee,
Sergey Tulyakov, and Jun-Yan Zhu. Text-guided synthesis
of eulerian cinemagraphs. SIGGRAPH ASIA , 2023. 2
[24] Alexander Quinn Nichol and Prafulla Dhariwal. Improved
denoising diffusion probabilistic models. In International
Conference on Machine Learning , pages 8162‚Äì8171. PMLR,
2021. 4
[25] Pika Labs, 2023. https://www.pika.art/. 5
[26] Dustin Podell, Zion English, Kyle Lacey, Andreas
Blattmann, Tim Dockhorn, Jonas M ¬®uller, Joe Penna, and
Robin Rombach. Sdxl: improving latent diffusion mod-
els for high-resolution image synthesis. arXiv preprint
arXiv:2307.01952 , 2023. 3
[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748‚Äì8763. PMLR, 2021. 3
[28] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In International Confer-
ence on Machine Learning , pages 8821‚Äì8831. PMLR, 2021.
3
[29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022.
7755
[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¬®orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684‚Äì10695, 2022. 2, 3
[31] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 22500‚Äì
22510, 2023. 2
[32] s9roll7, 2023. https://github.com/s9roll7/animatediff-cli-
prompt-travel. 3
[33] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems , 35:36479‚Äì36494, 2022. 3
[34] Shuai Shen, Wanhua Li, Zheng Zhu, Yueqi Duan, Jie Zhou,
and Jiwen Lu. Learning dynamic facial radiance fields for
few-shot talking head synthesis. In European Conference on
Computer Vision , pages 666‚Äì682. Springer, 2022. 2
[35] Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li, Zheng
Zhu, Jie Zhou, and Jiwen Lu. Difftalk: Crafting diffusion
models for generalized audio-driven portraits animation. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 1982‚Äì1991, 2023. 2
[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 4
[37] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,
Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,
and Jingren Zhou. Videocomposer: Compositional video
synthesis with motion controllability. arXiv preprint
arXiv:2306.02018 , 2023. 2, 5, 7
[38] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian
Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu
Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning
of image diffusion models for text-to-video generation. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 7623‚Äì7633, 2023. 3, 4, 5
[39] Wenpeng Xiao, Wentao Liu, Yitong Wang, Bernard Ghanem,
and Bing Li. Automatic animation of hair blowing in still
portrait photos. In ICCV , 2023. 2
[40] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. Dif-
fusion probabilistic modeling for video generation. Entropy ,
25(10):1469, 2023. 3
[41] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-
adapter: Text compatible image prompt adapter for text-to-
image diffusion models. arXiv preprint arxiv:2308.06721 ,
2023. 2, 5
[42] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3836‚Äì3847, 2023. 2, 3, 4, 5[43] Heliang Zheng, Jianlong Fu, Yanhong Zeng, Jiebo Luo, and
Zheng-Jun Zha. Learning semantic-aware normalization for
generative adversarial networks. Advances in Neural Infor-
mation Processing Systems , 33:21853‚Äì21864, 2020. 3
[44] Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan,
and Kai Chen. A task is worth one word: Learning with task
prompts for high-quality versatile image inpainting. arXiv
preprint arXiv:2312.03594 , 2023. 3
7756
