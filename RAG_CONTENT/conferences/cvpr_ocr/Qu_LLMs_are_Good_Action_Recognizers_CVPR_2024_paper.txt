LLMs are Good Action Recognizers
Haoxuan Qu1Yujun Cai2Jun Liu1†
1Singapore University of Technology and Design2Nanyang Technological University
haoxuan qu@mymail.sutd.edu.sg, yujun001@e.ntu.edu.sg, jun liu@sutd.edu.sg
Abstract
Skeleton-based action recognition has attracted lots of
research attention. Recently, to build an accurate skeleton-
based action recognizer, a variety of works have been pro-
posed. Among them, some works use large model architec-
tures as backbones of their recognizers to boost the skeleton
data representation capability, while some other works pre-
train their recognizers on external data to enrich the knowl-
edge. In this work, we observe that large language models
which have been extensively used in various natural lan-
guage processing tasks generally hold both large model ar-
chitectures and rich implicit knowledge. Motivated by this,
we propose a novel LLM-AR framework, in which we in-
vestigate treating the LargeLanguage Model as an Action
Recognizer. In our framework, we propose a linguistic pro-
jection process to project each input action signal (i.e., each
skeleton sequence) into its “sentence format” (i.e., an “ac-
tion sentence”). Moreover, we also incorporate our frame-
work with several designs to further facilitate this linguistic
projection process. Extensive experiments demonstrate the
efficacy of our proposed framework.
1. Introduction
Human action recognition aims to categorize the actions
performed by humans into a pre-defined list of classes. It
is relevant to a variety of applications, such as human-
computer interaction [37], intelligent surveillance [36], and
virtual reality [13]. In the past few years, skeleton-based
action recognition [3, 5, 6, 12, 53, 54, 62, 70] has received
a lot of research attention with the notice that skeleton is
a succinct yet informative representation of human behav-
iors. Yet, despite the considerable progress, skeleton-based
action recognition still remains a challenging task [78], and
to build a more accurate skeleton-based action recognizer,
various recent works have been proposed from different per-
spectives. Among them, some recent works [15, 59] pro-
† Corresponding author
Figure 1. Overview of our proposed LLM-AR framework. In
our framework, given an input action signal, we first perform a
linguistic projection process to acquire the corresponding “action
sentence”. We then perform action recognition via the large lan-
guage model with its pre-trained weights untouched to keep its
pre-learned rich knowledge.
posed to utilize large model architectures (such as trans-
formers) as the backbone of their action recognizers to
achieve stronger representation capability and capture more
subtle differences among different actions. On the other
hand, some other works [72, 80] proposed to pre-train their
action recognizers on external data in order for their action
recognizers to handle this task with richer knowledge.
Recently, large language models such as GPT [2] and
LLaMA [56] have become quite popular and have been ex-
tensively applied in handling various human languages. For
example, having been pre-trained over various human lan-
guages and learned common language-related characteris-
tics, large language models can be very effectively and ef-
ficiently adapted to even new human languages unseen dur-
ing pre-training [49]. Moreover, people have also treated
large language models as interpreters in code interpretation
[14] and generators in essay generation [22], and found that
large language models can handle these tasks effectively.
Motivated by this, in this work we are wondering, if we
can also treat the large language model as an action recog-
nizer in skeleton-based human action recognition? In gen-
eral, large language models hold some characteristics that
are useful for action recognition. Specifically, pre-trained
over a tremendously large corpus [2, 56] that generally con-
tains very rich human-centric behavior descriptions, large
language models that hold large model architectures could
naturally contain rich implicit knowledge [46, 50] of human
behaviors, and hold strong capability in handling different
inputs. Thus, it can be very promising if we treat the large
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
18395
language model as a human action recognizer.
However, despite the success of large language models
in handling various human languages, it can be challenging
to treat them as action recognizers. This is because, while
large language models typically take sentences in human
languages as input instructions, the input signal of the ac-
tion recognizer (i.e., the skeleton sequence) is not in a “sen-
tence format” that is “friendly” to large language models.
To handle this issue, given a large language model initial-
ized with its pre-trained weights, one potential way is to
adjust the weights of the full model to fit the non-linguistic
input action signals. Recently, several works [25, 38] have
been proposed to fine-tune large language models and ad-
just their pre-trained weights to handle non-language tasks.
However, as shown in the previous study [64], the adjust-
ment of the large language model’s pre-trained weights can
hurt its generalizability and lead to the loss of its pre-learned
rich knowledge. This is clearly undesirable here, as we hope
the large language model to be knowledgeable in order for
it to be an accurate action recognizer.
Taking this into account, in this work, we aim to har-
ness the large language model as an action recognizer, and
at the same time keep its pre-trained weights untouched to
preserve its pre-learned rich knowledge. Specifically, we
propose a novel action recognition framework named Large
Language Model as an Action Recognizer ( LLM-AR ). As
shown in Fig. 1, to perform action recognition, our LLM-
AR framework first involves a linguistic projection process
to project the input action signal into its “sentence format”,
that can be “friendly” and compatible to the large language
model pre-trained over human sentences. Then the large
language model takes in this “sentence format” of the action
signal together with a human sentence as the instruction to
predict the name of the action. In the rest of this work, for
simplicity, we call the action signal in its “sentence format”
an “action sentence”.
In our framework, to project the action signals into “ac-
tion sentences” (i.e., performing linguistic projection), we
incorporate several designs. Basically, to perform such a
process, we observe that, every sentence in a language gen-
erally can be regarded as a sequence consisting of discrete
word tokens. Inspired by both this observation and previous
works [75, 77], in LLM-AR, we first learn an action-based
vector quantized variational autoencoder (VQ-V AE) model
to project each action signal into a sequence of discrete to-
kens.
However, while we can basically represent each “action
sentence” by a sequence of discrete tokens, it is not nec-
essary that any discrete token sequence can represent an
“action sentence” well. In fact, to enable a large language
model to become an accurate action recognizer based on
the “action sentences”, besides being discrete sequences
of tokens, ideally, the “action sentences” also need to ful-fill some requirements as discussed below. Firstly, since
large language models are generally pre-trained over cor-
pus consisting of sentences in human languages, to facili-
tate large language models in taking “action sentences” as
instructions together with sentences in human languages,
these “action sentences” should be “like” sentences in hu-
man languages so as to be more friendly to the large lan-
guage model. On the other hand, “action sentences” should
still maintain good representations of their original action
signals in order for the large language model to accurately
perform action recognition based on them.
To better meet the above requirements in the linguistic
projection process, we further incorporate our framework
with two designs below. Firstly, to make the “action sen-
tences” more “like” sentences in human languages, we get
inspiration from previous linguistic and natural language
processing works [27, 47, 48, 55, 82] which show that, lan-
guages as the communication tools of human beings often
contain human inductive biases. Thus, we propose to incor-
porate our framework with a learning strategy to regular-
ize the projected “action sentences” to follow human induc-
tive biases as well. Secondly, to make the projected “action
sentence” a good representation of its original action sig-
nal, motivated by the human skeleton’s tree-like nature [63]
and hyperbolic space’s superior ability in representing tree
structures [18], and inspired by previous works [33, 51], we
further incorporate our action-based VQ-V AE model with a
hyperbolic codebook.
Once we finish the learning of the linguistic projection
process, we perform low-rank adaptation (LoRA) [23] on
the large language model to let the model understand the
projected “action sentences”. Note that since the pre-trained
weights of the large language model are untouched through-
out the LoRA process, we can preserve the pre-learned rich
knowledge in the large language model and thus utilize the
large language model conveniently in our framework.
The contributions of our work are as follows. 1) We pro-
pose a novel action recognition framework named LLM-
AR. To the best of our knowledge, this work is the first ex-
ploration on treating a large language model with its pre-
trained weights untouched as an action recognizer. 2) We
introduce several designs in our framework to facilitate the
linguistic projection process and produce the “action sen-
tences”. 3) LLM-AR achieves state-of-the-art performance
on the evaluated benchmarks.
2. Related Work
Human Action Recognition. For tackling human action
recognition, most of the existing methods can be roughly
categorized into two groups: RGB-based methods [58, 60]
and skeleton-based methods [5, 6, 53, 54, 70].
As skeleton sequences can represent human behaviors
in a succinct yet informative manner, skeleton-based ac-
18396
tion recognition has received lots of research attention [3–
7, 12, 15, 17, 21, 28, 32, 39, 40, 53, 54, 59, 62, 65–
68, 70, 73, 76, 80]. In the early days, different works have
been proposed to use different network architectures to han-
dle skeleton-based action recognition. Liu et al. [39] pro-
posed to perform skeleton-based action recognition through
a spatial-temporal LSTM architecture. Ke et al. [28] pro-
posed to transform skeleton sequences into grey images and
process these images through a CNN network. As time
passed, GCN tends to be a popular network architecture
[5, 7, 8, 34, 54, 65, 67, 70]. Yan et al. [70] proposed
ST-GCN that made the first attempt in performing skeleton-
based action recognition using a GCN architecture. After
that, various different GCN-based methods have been fur-
ther proposed, such as AS-GCN [34], 2s-AGCN [54], Shift-
GCN [7], and CTR-GCN [5]. More recently, a number of
works [15, 59, 76, 80] further proposed to perform skeleton-
based action recognition via training a model with a large
transformer architecture in an end-to-end manner. Some of
such methods include 3Mformer [59] and UPS [15].
Different from these approaches, in this work, from a
novel perspective, we investigate how to instruct the large
language model to perform skeleton-based action recogni-
tion while keeping its pre-trained weights untouched to pre-
serve its pre-learned rich knowledge. To achieve this, we
involve our framework with a novel linguistic projection
process to project each input action signal into an “action
sentence”. Specifically, we incorporate several designs in
the linguistic projection process to make the projected “ac-
tion sentences” more “like” sentences of human languages
so as to make them more friendly to large language models,
and meanwhile keep the “action sentence” a good represen-
tation of the input action signal.
Large Language Models. Recently, a variety of large lan-
guage models have been proposed, such as GPT [2] and
LLaMA [56]. Pre-trained over a tremendous number of
word tokens, these large language models with large model
architectures have been shown to contain very rich knowl-
edge [46, 50]. Consequently, large language models have
been extensively explored in various tasks [14, 16, 19, 22,
26, 77, 79], such as language translation [26], essay gen-
eration [22], and code interpretation [14]. In this work, we
design a novel framework treating the large language model
as a human action recognizer leveraging our designed lin-
guistic projection process.
3. Proposed Method
Given a skeleton sequence as the input action signal, the
goal of action recognition is to predict its corresponding ac-
tion class. Recently, holding large model architectures and
containing very rich knowledge, pre-trained large language
models have been shown to be powerful in handling sen-
tences of human languages, and thus have become usefultools in many natural language processing tasks. Inspired
by this, in this work, we aim to leverage the large language
model as an effective action recognizer. To achieve this, we
propose a novel framework LLM-AR . Specifically, LLM-
AR first performs a linguistic projection process to project
the input action signal (i.e., the skeleton sequence) into an
“action sentence”. After that, LLM-AR passes the “action
sentence” into the large language model to derive the corre-
sponding human action. Below, we first describe how LLM-
AR performs the linguistic projection process, and then in-
troduce the overall training and testing scheme of LLM-AR.
3.1. Linguistic Projection
In our framework, to enable the large language model to
perform action recognition, we first perform a linguistic
projection process to project each input action signal into
an “action sentence”. To learn to perform such a projection,
with the observation that each sentence in human languages
is essentially a sequence of discrete word tokens, motivate
by previous works [75, 77], we first learn an action-based
VQ-V AE model to project each input action signal into a
sequence of discrete tokens. We then further (1) involve
the above learning process with a human-inductive-biases-
guided learning strategy to make the projected “action sen-
tences” more “like” sentences in human languages, and (2)
incorporate the action-based VQ-V AE model with a hyper-
bolic codebook to keep “action sentences” good representa-
tions of the action signals.
Action-based VQ-V AE Model. VQ-V AE models [51,
57, 74] have been popularly used in converting an image
into a discrete token sequence. Inspired by this, in our
framework, to convert the input action signal into a se-
quence of discrete tokens, we first learn an action-based
VQ-V AE model.
Specifically, similar to the architecture of previous VQ-
V AE models [57, 75, 77], our action-based VQ-V AE model
involves an encoder E, a decoder D, and a codebook C
consisting of Ulearnable tokens (i.e., C={cu}U
u=1where
cu∈Rdu). We here keep duto be the same as the dimen-
sion of the word token in the used large language model,
and keep Uan even number. Among the aforementioned
three components of the model, given an input action signal
s1:V, where Vrepresents the action length, the encoder E
first encodes the action signal through 1D convolutions over
the time dimension into a sequence of latent features f1:W,
where fw∈RduandWrepresents the length of the latent
feature sequence. After this, to discretize the latent features
f1:W, a quantization operation is performed to replace each
feature fwwith its nearest token in the codebook as:
fd
w= argmin
cu∈C 
dist(fw, cu)
(1)
where fd
wis the discrete version of fw, and dist(·,·)rep-
resents a distance function. Finally, the decoder aims to
18397
recover the original action signal s1:Vfrom the sequence of
discrete latent features (tokens) fd
1:W. Through the above
process of encoding, quantization, and decoding, we can
project an action signal into a sequence of informative dis-
crete tokens fd
1:W. The more detailed architecture of this
action-based VQ-V AE model is provided in supplementary.
Human-inductive-biases-guided Learning Strategy.
Above we learn to project each input action signal into
a sequence of discrete tokens. To enable the learned to-
ken sequences to be more friendly to the large language
model, here we aim to make these sequences more “like”
sentences in human languages. To achieve this, we get in-
spiration from massive existing studies [27, 47, 48, 55, 82]
which show that, human languages as tools created for hu-
man communication naturally contain human inductive bi-
ases. Besides, it is also further shown by the previous study
[47] that, inputs following human inductive biases are more
friendly to large language models. Taking these into con-
sideration, we aim to optimize the set of learned token se-
quences to also follow human inductive biases, so that these
sequences can be more “like” human sentences and thus be-
come more friendly to be used by large language models.
Below, we first introduce the human inductive biases that
are generally recognized as being present in human lan-
guages. Such biases include: (a) a human language natu-
rally follows the Zipf’s law [47, 48, 82], and (b) a human
language is generally context-sensitive [27, 47, 55]. After
introducing these human inductive biases, we then describe
our proposed human-inductive-biases-guided learning strat-
egy.
As for the Zipf’s law in (a), during daily communications
of human beings, there naturally exist some words that are
used more commonly and some words more rarely. Zipf’s
law intuitively represents this imbalanced usage frequency
of word tokens in human languages. Specifically, Zipf’s law
states that, in a human language, the i-th most commonly
used word has its usage frequency roughly proportional to:
1
(i+β)α(2)
where α≈1andβ≈2.7[82]. The reason why Zipf’s
law consistently appears across different human languages
is also theoretically analyzed by various works [9, 43] from
different perspectives, such as from the evolution of human
communications.
With respect to the context-sensitivity of human lan-
guages in (b), intuitively, the context-sensitivity refers to the
inductive bias that when people formulate their sentences,
they often do not use each word token independently. In-
stead, their usage of different tokens in formulating a sen-
tence is often correlated. Note that, while human languages
are generally believed by linguists to be context-sensitive,
how to explicitly represent context-sensitivity remains a dif-
ficult problem. Here, inspired by [47] in its way of repre-senting context-sensitivity, we represent this human induc-
tive bias as follows. Specifically, given the codebook C
consisting of Utokens where Uis an even number, dur-
ing initializing C, we first randomly split the codebook into
two halves (i.e., {1, ...,U
2}and{1 +U
2, ..., U}). Next, we
regard each token cuin the first half (i.e., u∈ {1, ...,U
2})
and the token cu+U
2in the second half to be a pair of corre-
lated tokens. As shown in [47], such a pairing mechanism is
a simple yet very effective way of representing the context-
sensitivity bias in human languages.
To optimize the set of learned token sequences (i.e., “ac-
tion sentences”) {fd
1:W}to also follow the above biases like
human languages, we then aim to (a) regularize the set of
“action sentences” to follow Zipf’s law, as well as (b) reg-
ularize each “action sentence” to be formulated using more
correlated word tokens so that the formulated “action sen-
tences” can better follow the context-sensitivity bias. To
achieve this, we design a human-inductive-biases-guided
learning strategy that consists of the following steps. (1)
Given a batch of action signals {sb
1:V}B
b=1with batch size
B, we first encode these signals through the encoder Eof
the action-based VQ-V AE model to get their corresponding
latent features {fb
1:W}B
b=1. (2) After that, during discretiz-
ing the latent features using tokens {cu}U
u=1in the code-
book C, we measure the token usage of each sequence of
latent features fb
1:Win a differentiable way via the Gumbel
Softmax trick [24] as:
tb=WX
w=1db
w,where db
w= Gumble Softmax( −dist(fb
w, cu))
(3)
where Gumble Softmax( ·)is the Gumble Softmax trick,
anddb
wis the one-hot vector with length Uas the output
of the Gumble Softmax trick. Besides, tbis a vector with
length U, and the value of the u-th element of tbrepresents
the number of times token cuis used in discretizing fb
1:W
and formulating its corresponding “action sentence”. (3)
Next, denoting DZipf the Zipf distribution with α= 1and
β= 2.7, we regularize the set of “action sentences” to fol-
low Zipf’s law through LZipf as:
LZipf =JS(Dfreq∥DZipf),where Dfreq=sort(PB
b=1tb)
B×W
(4)
where sort(·)is the sorting operation used to order tokens
based on their usage frequency, Dfreq is the distribution
representing the token usage frequency, and JS(·∥·)repre-
sents the JS divergence between two distributions. (4) At
the same time, we encourage the “action sentences” to fol-
low the context-sensitivity bias and use more correlated to-
kens via Lcontext as:
Lcontext = 1−PB
b=1Corr (tb)
B×W(5)
where Corr (tb)leverages min pooling over every pair of
18398
Figure 2. Overview of the action-based VQ-V AE model with the hyperbolic codebook CHincorporated. Given a batch of input action
signals {sb
1:V}B
b=1, to optimize the action-based VQ-V AE model, {sb
1:V}B
b=1are first fed to the encoder Eto get the corresponding latent
features {fb
1:W}B
b=1. Next, to leverage the hyperbolic codebook CHthat can serve as a good representation of the tree-like human skeletons
to perform quantization, {fb
1:W}B
b=1are projected into the hyperbolic space via the process of E-to-H projection. After that, the quantization
is performed in the hyperbolic space using dist B(efw, cu)defined in Eq. 9 as the distance function. Finally, after quantization, the discrete
version of the latent features are passed back into the Euclidean space via the process of H-to-E projection to reconstruct the input action
signals through the decoder D.
elements in tb(e.g., the pair of u-th element and (u+U
2)-th
element) to calculate the number of correlated tokens used
in discretizing fb
1:W(more details about this method to mea-
sureCorr (tb)are provided in supplementary). Note that
via minimizing Lcontext , we encourage more correlated to-
kens to be used in formulating “action sentences”. (5) Fi-
nally, we formulate the loss function Lhuman that we use to
perform our human-inductive-biases-guided strategy as:
Lhuman =LZipf+Lcontext (6)
Via incorporating the above learning strategy into the learn-
ing process of the action-based VQ-V AE model, we can reg-
ularize the “action sentences” to better follow the human in-
ductive biases that are present in human languages and thus
make them more “like” sentences in human languages.
Above we inject human inductive biases into “action
sentences”. Here, we notice that, besides inductive biases
that can be explicitly described, human languages can also
present other implicit characteristics. Thus, to enable “ac-
tion sentences” to be more friendly to large language mod-
els which are generally pre-trained over various human lan-
guages, we aim to further incorporate implicit character-
istics into “action sentences” as well. Specifically, since
large language models such as LLaMA typically use word
tokens {cLLM}stored in its first layer to formulate sen-
tences in human languages, we here further align (1) the
tokens {cu}U
u=1in codebook Cthat are used to formulate
the “action sentences” with (2) the word tokens {cLLM}
used in the large language model. To achieve such an align-
ment, we leverage Maximum Mean Discrepancy (MMD)
[20] as an effective feature alignment technique to measure
the discrepancy between {cu}U
u=1and{cLLM}. Specifi-
cally, the less MMD( {cu}U
u=1,{cLLM})is,{cu}U
u=1and
{cLLM}can be regarded as more aligned. We then incor-
porate this alignment procedure into our proposed strategyvia rewriting Lhuman in Eq. 6 as:
Lhuman =LZipf+Lcontext + MMD( {cu}U
u=1,{cLLM})
(7)
By incorporating this alignment procedure into the previ-
ously mentioned learning strategy, we can then lead the for-
mulated “action sentences” {fd
1:W}to be more friendly to
be used by large language models.
Hyperbolic Codebook. Besides making the “action sen-
tences” more friendly to the large language model, we also
aim to keep them good representations of the original in-
put action signals (i.e., the skeleton sequences). Motivated
by the tree-like nature of human skeletons [63], we aim to
make the word tokens in the “action sentences” to well rep-
resent such a structure. To achieve this, inspired by the
superior capability of the hyperbolic space in embedding
tree structures [18] and motivated by previous VQ-V AE
works [33, 51], we here further incorporate our action-based
VQ-V AE model with a hyperbolic codebook utilizing the
Poincar ´e ball model [18, 45]. The Poincar ´e ball model is
an isometric model that can represent the hyperbolic space.
Formally, the n-dimensional Poincar ´e ball model is defined
as(Bn
c, gc
x), where Bn
c={x∈Rn:c∥x∥<1},gc
x=
(γc
x)2Inis the Riemannian metric tensor, γc
x=2
1−c∥x∥2is
the conformal factor, Inis the Euclidean metric tensor, and
cis the curvature.
Denoting the hyperbolic codebook CH={cu}U
u=1,
where cu∈Bduc, we make the following three changes
to incorporate CHinto our action-based VQ-V AE model.
We also illustrate where these changes take place in our
action-based VQ-V AE model in Fig. 2. (1) Euclidean-to-
Hyperbolic (E-to-H) Projection. Given a latent feature fw
which is originally in the Euclidean space, to convert it into
a discrete token using a hyperbolic codebook, we first need
to project fwinto the hyperbolic space. Specifically, fol-
lowing [18], we perform such a projection through the ex-
18399
ponential map expc
0(·)as:
efw= expc
0(fw) =tanh (√c∥fw∥)fw√c∥fw∥(8)
whereefwis the projected feature of fwin the hyperbolic
space. (2) Hyperbolic Distance Calculation. Given the
projected feature efw, to perform quantization of efwbased
on the tokens in CH, we need a distance function defined
in the hyperbolic space. Here, for simplicity, we use the
popularly used geodesic/induced distance [18, 29]. Denote
this distance as distB(·,·).distB(efw, cu)between efwand
cucan be defined as:
distB(efw, cu) =arccosh 
1 + 2∥efw−cu∥2
(1− ∥efw∥2)(1− ∥cu∥2)
(9)
(3) Hyperbolic-to-Euclidean (H-to-E) Projection. After
quantization, we need to project efd
w(the discrete version
ofefw) back into the Euclidean space in order for it to be
passed to the decoder D. To achieve this, we follow [18] to
leverage the logarithmic map logc
0(·)as:
fd
w= logc
0(efd
w) =arctanh (√c∥efd
w∥)efd
w√c∥efdw∥(10)
where fd
wrepresents the discrete version of the feature pro-
jected back in the Euclidean space. With the above changes
made, we can seamlessly involve the hyperbolic codebook
CHinto our action-based VQ-V AE model, and make the
projected “action sentence” a better representation of the
input action signal.
3.2. Overall Training and Testing
Above we describe how we perform the linguistic projec-
tion process in our framework to project each input action
signal into an “action sentence”. Below, we introduce the
overall training and testing scheme of our framework.
Training. The training process of our framework con-
sists of the following two stages: (1) optimizing the action-
based VQ-V AE model incorporated with the hyperbolic
codebook CHto acquire the ability to project each action
signal s1:Vinto its corresponding “action sentence” efd
1:W;
and (2) perform low-rank adaptation (LoRA) [23] on the
large language model for it to better understand the pro-
jected “action sentences”.
At the first stage, to optimize the action-based VQ-V AE
model, given a batch of action signals, we first follow pre-
vious VQ-V AE works [57, 75, 77] to use the following loss
functions, including the reconstruction loss, the embedding
loss, and the commitment loss. Among them, the recon-
struction loss Lreis designed to regularize the difference
between the input action signal and the action signal recon-
structed from the decoder D, the embedding loss Lembed
is designed to learn the tokens in the codebook, and the
commitment loss Lcommit is designed to encourage eachof the encoded latent features to stay close to its discrete
version. Formally speaking, denoting {sb
1:V}B
b=1a batch of
input action signals, {ˆsb
1:V}B
b=1the corresponding batch of
reconstructed action signals, {fb
1:W}B
b=1the latent features
encoded from {sb
1:V}B
b=1, and{fd,b
1:W}B
b=1the discrete ver-
sions of these latent features, the above three loss functions
can be written as:
Lre=Lsmooth
1 ({sb
1:V}B
b=1,{ˆsb
1:V}B
b=1)
Lembed =∥sg({fb
1:W}B
b=1)− {fd,b
1:W}B
b=1∥2
Lcommit =∥{fb
1:W}B
b=1−sg({fd,b
1:W}B
b=1)∥2(11)
where Lsmooth
1 (·)represents the L1 smooth loss and sg(·)is
the stop-gradient operation that is used to prevent the gradi-
ent from flowing through its operand. In addition, to make
the learned “action sentences” more like sentences in hu-
man languages, we also incorporate the above three loss
functions with Lhuman defined in Eq. 7. Overall, we can
write the total loss Ltotal for the first training stage as:
Ltotal=Lre+Lembed +ω1Lcommit +ω2Lhuman (12)
where ω1andω2are weighting hyperparameters.
We then perform LoRA [23] on the large language model
in the second stage in order for it to better understand the
“action sentences” while keeping the pre-trained weights of
the model untouched. Specifically, for each training sample
consisting of an input action signal and its corresponding
ground-truth action, we perform the following steps. (1)
We first project the action signal into an “action sentence”
of discrete tokens through the action-based VQ-V AE
model learned in the first stage. (2) We then instruct the
large language model to act as an action recognizer via
a simple instruction as: “ Given a sequence of
action tokens [tokens], please predict
the corresponding action. ”, where [tokens]
represent the word tokens of the “action sentence” derived
in step (1). (3) Finally, during LoRA process, we pass
the above instruction into the large language model and
encourage the similarity between the tokens tppredicted
by the large language model and the tokens tgrepresenting
the ground-truth action as:
LLoRA =Lce(tp, tg) (13)
where Lce(·,·)represents the cross-entropy loss.
Testing. During testing, for each testing action signal,
we first use the learned action-based VQ-V AE model to de-
rive its corresponding “action sentence”. After that, we use
the same instruction as in step (2) above to instruct the large
language model to predict the corresponding action.
4. Experiments
To evaluate the efficacy of our framework, we conduct
experiments on 4 datasets including NTU RGB+D, NTU
RGB+D 120, Toyota Smarthome, and UA V-Human.
18400
4.1. Datasets
NTU RGB+D [52] is a large-scale dataset popularly used in
human action recognition. It contains around 56k skeleton
sequences from 60 activity classes. On this dataset, follow-
ing [52], we evaluate our method under the Cross-Subject
(X-Sub) and Cross-View (X-View) evaluation protocols.
NTU RGB+D 120 [41] is an extension of the NTU RGB+D
dataset. It consists of more than 114k skeleton sequences
across 120 activity classes. Following [41], we evaluate our
method on this dataset under the Cross-Subject (X-Sub) and
Cross-Setup (X-Set) evaluation protocols.
Toyota Smarthome [10] contains 16,115 video samples
over 31 activity classes. On this dataset, we use the skeleton
sequences pre-processed by [71] and we follow it to evalu-
ate our method under the Cross-Subject (X-Sub) and two
Cross-View (X-View1 & X-View2) evaluation protocols.
UA V-Human [35] is a dataset that is captured by unmanned
aerial vehicles (UA V). It contains more than 20k skeleton
sequences over 155 activity classes, and it is collected from
119 distinct subjects. Following [35], we use 89 subjects
for training and 30 subjects for testing.
4.2. Implementation Details
We conduct our main experiments on Nvidia V100 GPU
and we use LLaMA-13B [56] as the large language model.
During the training process of the action-based VQ-V AE
model, we optimize the model for 300,000 iterations using
the AdamW [42] optimizer with an initial learning rate of
2e-4. We set the batch size Bto 256, the number of tokens
Uto 512, and the curvature cof the hyperbolic codebook to
1. Additionally, we set the dimension of each word token du
to the same size (5120) as the word token in LLaMA-13B.
Moreover, we set w1to 0.02 following previous VQ-V AE
works [44, 75] and set w2to 0.2. Besides, we set the length
(W) of each sequence of latent features to be a quarter of the
length ( V) of its corresponding input action signal. Dur-
ing the LoRA process, we develop our code based on the
Github repo [1], set the number of iterations to 75,000, and
use the AdamW optimizer with an initial learning rate of
3e-3. Moreover, we set the batch size to 256, partitioned
into micro-batches of 4. Besides, we set the two hyperpa-
rameters of the LoRA process, i.e., rLoRA andαLoRA , to
64 and 16 respectively.
4.3. Comparison with State-of-the-art Methods
On NTU RGB+D and NTU RGB+D 120 datasets, fol-
lowing the experimental setting of recent works [12, 80],
we only use the joint modality of human skeletons dur-
ing our experiments. We report the results in Tab. 1. As
shown, compared to existing skeleton-based action recogni-
tion methods, our method consistently achieves the best per-
formance across all the evaluation protocols. This demon-
strates the effectiveness of our method. Besides, we alsoTable 1. Performance comparison on the NTU RGB+D and NTU
RGB+D 120 datasets.
MethodNTU RGB+D NTU RGB+D 120
X-Sub X-View X-Sub X-Set
ST-GCN [70] 85.7 92.4 82.1 84.5
Shift-GCN [7] 87.8 95.1 80.9 83.2
InfoGCN [8] 89.8 95.2 85.1 86.3
PoseC3D [11] 93.7 96.5 85.9 89.7
FR-Head [78] 90.3 95.3 85.5 87.3
Koopman [62] 90.2 95.2 85.7 87.4
GAP [65] 90.2 95.6 85.5 87.0
HD-GCN [31] 90.6 95.7 85.7 87.3
STC-Net [30] 91.0 96.2 86.2 88.0
DSTformer [80] 93.0 97.2 - -
SkeleTR [12] 94.8 97.7 87.8 88.3
Ours 95.0 98.4 88.7 91.5
Table 2. Performance comparison on the
Toyota Smarthome dataset.
Method X-Sub X-View1 X-View2
2S-AGCN [54] 58.8 32.2 57.9
SSTA-PRS [71] 62.1 22.8 54.0
UNIK [72] 62.1 33.4 63.6
ML-STGNet [81] 64.6 29.9 63.5
Ours 67.0 36.1 66.6Table 3. Performance
comparison on the
UA V-Human dataset.
Method X-Sub
ST-GCN [69] 30.3
2S-AGCN [54] 34.8
Shift-GCN [7] 38.0
ACFL [61] 44.2
Ours 46.3
report results on the Toyota Smarthome dataset in Tab. 2,
and on the UA V-Human dataset in Tab. 3. As shown, our
method consistently achieves the best performance on these
datasets. This further shows the efficacy of our method.
4.4. Ablation Studies
We conduct extensive ablation experiments on the X-Set
protocol of the NTU RGB+D 120 dataset. More ablation
studies such as experiments w.r.t. hyperparameters are
in supplementary.
Table 4. Evaluation on
the human-inductive-biases-
guided learning strategy.
Method Accuracy
w/o biases 87.6
w/oLZipf 89.9
w/oLcontext 89.8
w/o MMD alignment 90.3
LLM-AR 91.5Impact of the human-
inductive-biases-guided
learning strategy. In our
framework, to lead the for-
mulated “action sentences”
to be more friendly to large
language models, we design
a human-inductive-biases-
guided learning strategy consisting of three components (as
shown in Eq. 7). To evaluate the efficacy of this strategy,
we test four variants. In the first variant ( w/o biases ), we
remove the whole strategy (i.e., all its three components
LZipf,Lcontext , and MMD alignment) from the learning
process. In the second variant ( w/oLZipf), we still utilize
the strategy but remove its LZipf component. Moreover,
in the third variant ( w/oLcontext ), we remove the Lcontext
component from the strategy, whereas in the fourth
variant ( w/o MMD alignment ), we remove the MMD
alignment component. As shown in Tab. 4, compared to
18401
our framework, the performance of the first variant drops
significantly. This shows the importance of formulating
“action sentences” like sentences in human languages.
Moreover, our framework also outperforms all the other
three variants. This further shows the effectiveness of all
the three components of our proposed learning strategy.
Table 5. Evaluation on dis-
cretizing latent features into
“action sentences”.
Method Accuracy
w/o discretization 83.4
with discretization 91.5Impact of discretizing latent
features into “action sen-
tences”. In our framework,
we discretize the encoded
latent features to formulate
“action sentences” consisting
of discrete word tokens ( with discretization ). To valid
this design, we test a variant. In this variant ( w/o dis-
cretization ), instead of discretizing the encoded latent
features into “action sentences”, we directly pass these
continuous features to the intermediate layers of the large
language model. As shown in Tab. 5, our framework with
discretization outperforms this variant. This shows the
advantage of discretizing latent features into “action sen-
tences”, which are more “like” human sentences consisting
of discrete word tokens, and thus are more friendly to the
large language model pre-trained over human sentences.
Table 6. Evalua-
tion on the hyper-
bolic codebook CH.
Method Accuracy
w/oCH 89.7
withCH 91.5Impact of the hyperbolic codebook
CH.In our framework, we incorpo-
rate our action-based VQ-V AE model
with a hyperbolic codebook CH(with
CH). To validate the efficacy of CH,
we test a variant ( w/oCH) in which
the codebook is set up in the Euclidean instead of hyper-
bolic space. As shown in Tab. 6, our framework involving
CHperforms better than this variant. This shows the effi-
cacy of CHin the hyperbolic space, which can facilitate the
“action sentences” in representing the tree-like-structured
input action signals better.
Table 7. Evaluation
on the LoRA process.
Method Accuracy
All tuning 79.6
LLM-AR 91.5Impact of the LoRA process. In
our framework, to make the large
language model understand the “ac-
tion sentences” while keeping its pre-
trained weights untouched to pre-
serve its rich pre-learned knowledge, we tune the large
language model through a LoRA process. To validate
this scheme, here we also test a variant ( all tuning ) on
A100 GPU. In this variant, during tuning, after initializ-
ing the large language model with its pre-trained weights,
all the parameters of the model will undergo gradient up-
dates. As shown in Tab. 7, our framework achieves much
better performance than this variant. This shows the su-
periority of our framework in choosing to perform tun-
ing via LoRA, which enables the large language model’s
pre-trained weights to be untouched and maintains its pre-
learned rich knowledge.Table 8. Evalua-
tion on unseen activ-
ity classes.
Method Accuracy
All tuning 37.7
LLM-AR 62.4Evaluation on unseen activity
classes. In the main experiments,
following [31, 52, 70], we evaluate
our framework on activity classes
that have been seen during training.
Here, inspired by that large language
models naturally could contain rich knowledge beyond the
training activity classes used in our experiments, we are
curious, assuming we have a list of testing activity classes
unseen during training, can we also use our framework to
perform action recognition on these classes? To answer
this question, we first build a new evaluation protocol
for unseen activity classes based on the NTU RGB+D
120 dataset. Under this new protocol, during each time
of evaluation, we randomly select 3 classes to form the
unseen class list (i.e., the list of testing classes), and use
the remaining classes as the classes seen during training
(i.e., the training classes). Besides, we instruct the large
language model as: “Given a sequence of action tokens
[tokens], please predict the corresponding action from
[list].”, where [tokens] represent the word tokens of the
“action sentence” and [list] represents the unseen class
list. We then perform the above evaluation for five times
and report the average performance. As shown in Tab. 8,
even testing on activity classes unseen during training, our
framework can still achieve a relatively good performance,
while the afore-defined all tuning variant performs much
worse. This can be analyzed as, large language models
could contain rich pre-learned knowledge w.r.t. the list of
unseen classes. Thus, our framework that maintains such
rich knowledge can still perform promising recognition on
these unseen classes, while the all tuning variant that can
lose amount of pre-learned knowledge of the large language
model would yield a much worse performance. This further
shows the advantage of our framework in maintaining the
pre-learned rich knowledge of the large language model.
5. Conclusion
In this paper, we have proposed a novel action recognition
framework LLM-AR. In LLM-AR, we treat the large lan-
guage model as an action recognizer, and instruct the large
language model to perform action recognition using its con-
tained rich knowledge. Specifically, to lead the input action
signals (i.e., the skeleton sequences) to be more friendly to
the large language model, we first propose a linguistic pro-
jection process to project each action signal into an “action
sentence”. Moreover, we also introduce several designs to
further facilitate this process. Our framework consistently
achieves SOTA performance across different benchmarks.
Acknowledgement. This project is supported by the Min-
istry of Education, Singapore, under the AcRF Tier 2
Projects (MOE-T2EP20222-0009 and MOE-T2EP20123-
0014).
18402
References
[1] Lit-llama. https://github.com/Lightning-AI/
lit-llama . 7
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in neural in-
formation processing systems , 33:1877–1901, 2020. 1, 3
[3] Dongqi Cai, Yangyuxuan Kang, Anbang Yao, and Yurong
Chen. Ske2grid: Skeleton-to-grid representation learning for
action recognition. 2023. 1, 3
[4] Jinghong Chen, Chong Zhao, Qicong Wang, and Hongy-
ing Meng. Hmanet: Hyperbolic manifold aware network
for skeleton-based action recognition. IEEE Transactions on
Cognitive and Developmental Systems , 2022.
[5] Yuxin Chen, Ziqi Zhang, Chunfeng Yuan, Bing Li, Ying
Deng, and Weiming Hu. Channel-wise topology refinement
graph convolution for skeleton-based action recognition. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 13359–13368, 2021. 1, 2, 3
[6] Ke Cheng, Yifan Zhang, Congqi Cao, Lei Shi, Jian Cheng,
and Hanqing Lu. Decoupling gcn with dropgraph module
for skeleton-based action recognition. In Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, Au-
gust 23–28, 2020, Proceedings, Part XXIV 16 , pages 536–
553. Springer, 2020. 1, 2
[7] Ke Cheng, Yifan Zhang, Xiangyu He, Weihan Chen, Jian
Cheng, and Hanqing Lu. Skeleton-based action recognition
with shift graph convolutional network. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 183–192, 2020. 3, 7
[8] Hyung-gun Chi, Myoung Hoon Ha, Seunggeun Chi,
Sang Wan Lee, Qixing Huang, and Karthik Ramani. In-
fogcn: Representation learning for human skeleton-based
action recognition. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
20186–20196, 2022. 3, 7
[9] Bernat Corominas-Murtra, Jordi Fortuny, and Ricard V Sol ´e.
Emergence of zipf’s law in the evolution of communication.
Physical Review E , 83(3):036115, 2011. 4
[10] Srijan Das, Rui Dai, Michal Koperski, Luca Minci-
ullo, Lorenzo Garattoni, Francois Bremond, and Gianpiero
Francesca. Toyota smarthome: Real-world activities of daily
living. In Proceedings of the IEEE/CVF international con-
ference on computer vision , pages 833–842, 2019. 7
[11] Haodong Duan, Yue Zhao, Kai Chen, Dahua Lin, and Bo
Dai. Revisiting skeleton-based action recognition. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 2969–2978, 2022. 7
[12] Haodong Duan, Mingze Xu, Bing Shuai, Davide Mod-
olo, Zhuowen Tu, Joseph Tighe, and Alessandro Bergamo.
Skeletr: Towards skeleton-based action recognition in the
wild. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 13634–13644, 2023. 1,
3, 7
[13] Abassin Sourou Fangbemi, Bin Liu, Neng Hai Yu, and Yanx-
iang Zhang. Efficient human action recognition interface foraugmented and virtual reality applications based on binary
descriptor. In Augmented Reality, Virtual Reality, and Com-
puter Graphics: 5th International Conference, AVR 2018,
Otranto, Italy, June 24–27, 2018, Proceedings, Part I 5 ,
pages 252–260. Springer, 2018. 1
[14] Faisal Firdous, Saimul Bashir, Syed Zoofa Rufai, and San-
jeev Kumar. Openai chatgpt as a logical interpreter of code.
In2023 2nd International Conference on Edge Computing
and Applications (ICECAA) , pages 1192–1197. IEEE, 2023.
1, 3
[15] Lin Geng Foo, Tianjiao Li, Hossein Rahmani, Qiuhong Ke,
and Jun Liu. Unified pose sequence modeling. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 13019–13030, 2023. 1, 3
[16] Lin Geng Foo, Hossein Rahmani, and Jun Liu. Ai-generated
content (aigc) for various data modalities: A survey, 2023. 3
[17] Luca Franco, Paolo Mandica, Bharti Munjal, and Fabio
Galasso. Hyperbolic self-paced learning for self-supervised
skeleton-based action representations. In The Eleventh In-
ternational Conference on Learning Representations , 2023.
3
[18] Octavian Ganea, Gary B ´ecigneul, and Thomas Hofmann.
Hyperbolic neural networks. Advances in neural informa-
tion processing systems , 31, 2018. 2, 5, 6
[19] Jia Gong, Lin Geng Foo, Yixuan He, Hossein Rahmani, and
Jun Liu. Llms are good sign language translators. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , 2024. 3
[20] Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard
Sch¨olkopf, and Alex Smola. A kernel method for the two-
sample-problem. Advances in neural information processing
systems , 19, 2006. 5
[21] Ryo Hachiuma, Fumiaki Sato, and Taiki Sekii. Unified
keypoint-based action recognition framework via structured
keypoint pooling. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
22962–22971, 2023. 3
[22] Steffen Herbold, Annette Hautli-Janisz, Ute Heuer, Zlata
Kikteva, and Alexander Trautsch. Ai, write an essay for me:
A large-scale comparison of human-written versus chatgpt-
generated essays. arXiv preprint arXiv:2304.14276 , 2023. 1,
3
[23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021. 2, 6
[24] Eric Jang, Shixiang Gu, and Ben Poole. Categorical
reparameterization with gumbel-softmax. arXiv preprint
arXiv:1611.01144 , 2016. 4
[25] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and
Tao Chen. Motiongpt: Human motion as a foreign language.
arXiv preprint arXiv:2306.14795 , 2023. 2
[26] Wenxiang Jiao, Wenxuan Wang, JT Huang, Xing Wang, and
ZP Tu. Is chatgpt a good translator? yes with gpt-4 as the
engine. arXiv preprint arXiv:2301.08745 , 2023. 3
[27] Aravind K Joshi, K Vijay Shanker, and David Weir. The con-
vergence of mildly context-sensitive grammar formalisms.
Technical Reports (CIS) , page 539, 1990. 2, 4
18403
[28] Qiuhong Ke, Mohammed Bennamoun, Senjian An, Ferdous
Sohel, and Farid Boussaid. A new representation of skele-
ton sequences for 3d action recognition. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 3288–3297, 2017. 3
[29] Valentin Khrulkov, Leyla Mirvakhabova, Evgeniya Usti-
nova, Ivan Oseledets, and Victor Lempitsky. Hyperbolic
image embeddings. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
6418–6428, 2020. 6
[30] Jungho Lee, Minhyeok Lee, Suhwan Cho, Sungmin Woo,
Sungjun Jang, and Sangyoun Lee. Leveraging spatio-
temporal dependency for skeleton-based action recognition.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 10255–10264, 2023. 7
[31] Jungho Lee, Minhyeok Lee, Dogyoon Lee, and Sangyoun
Lee. Hierarchically decomposed graph convolutional net-
works for skeleton-based action recognition. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 10444–10453, 2023. 7, 8
[32] Zikang Leng, Hyeokhyen Kwon, and Thomas Pl ¨otz. On the
benefit of generative foundation models for human activity
recognition. arXiv preprint arXiv:2310.12085 , 2023. 3
[33] Lei Li, Tingting Liu, Chengyu Wang, Minghui Qiu, Cen
Chen, Ming Gao, and Aoying Zhou. Resizing codebook of
vector quantization without retraining. Multimedia Systems ,
pages 1–14, 2023. 2, 5
[34] Maosen Li, Siheng Chen, Xu Chen, Ya Zhang, Yanfeng
Wang, and Qi Tian. Actional-structural graph convolutional
networks for skeleton-based action recognition. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 3595–3603, 2019. 3
[35] Tianjiao Li, Jun Liu, Wei Zhang, Yun Ni, Wenqian Wang,
and Zhiheng Li. Uav-human: A large benchmark for human
behavior understanding with unmanned aerial vehicles. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 16266–16275, 2021. 7
[36] Weiyao Lin, Ming-Ting Sun, Radha Poovendran, and
Zhengyou Zhang. Activity recognition using a combination
of category components and local models for video surveil-
lance. IEEE Transactions on Circuits and Systems for Video
Technology , 18(8):1128–1139, 2008. 1
[37] Hong Liu, Qinqin He, and Mengyuan Liu. Human action
recognition using adaptive hierarchical depth motion maps
and gabor filter. In 2017 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP) , pages
1432–1436. IEEE, 2017. 1
[38] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. arXiv preprint arXiv:2304.08485 ,
2023. 2
[39] Jun Liu, Amir Shahroudy, Dong Xu, and Gang Wang.
Spatio-temporal lstm with trust gates for 3d human action
recognition. In Computer Vision–ECCV 2016: 14th Euro-
pean Conference, Amsterdam, The Netherlands, October 11-
14, 2016, Proceedings, Part III 14 , pages 816–833. Springer,
2016. 3
[40] Jun Liu, Gang Wang, Ping Hu, Ling-Yu Duan, and Alex C
Kot. Global context-aware attention lstm networks for 3daction recognition. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1647–1656,
2017. 3
[41] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang,
Ling-Yu Duan, and Alex C Kot. Ntu rgb+ d 120: A large-
scale benchmark for 3d human activity understanding. IEEE
transactions on pattern analysis and machine intelligence ,
42(10):2684–2701, 2019. 7
[42] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 , 2017. 7
[43] Benoit Mandelbrot. Structure formelle des textes et commu-
nication: Deux ´etudes par. Word , 10(1):1–27, 1954. 4
[44] Evonne Ng, Sanjay Subramanian, Dan Klein, Angjoo
Kanazawa, Trevor Darrell, and Shiry Ginosar. Can language
models learn to listen? In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 10083–
10093, 2023. 7
[45] Maximillian Nickel and Douwe Kiela. Poincar ´e embeddings
for learning hierarchical representations. Advances in neural
information processing systems , 30, 2017. 5
[46] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al. Training lan-
guage models to follow instructions with human feedback.
Advances in Neural Information Processing Systems , 35:
27730–27744, 2022. 1, 3
[47] Isabel Papadimitriou and Dan Jurafsky. Pretrain on just
structure: Understanding linguistic inductive biases using
transfer learning. arXiv preprint arXiv:2304.13060 , 2023.
2, 4
[48] Steven T Piantadosi. Zipf’s word frequency law in natural
language: A critical review and future directions. Psycho-
nomic bulletin & review , 21:1112–1130, 2014. 2, 4
[49] Yiwei Qin, Graham Neubig, and Pengfei Liu. Searching for
effective multilingual fine-tuning methods: A case study in
summarization, 2022. 1
[50] Haoxuan Qu, Xiaofei Hui, Yujun Cai, and Jun Liu.
Lmc: Large model collaboration with cross-assessment for
training-free open-set object recognition. Advances in Neu-
ral Information Processing Systems , 36, 2024. 1, 3
[51] Acquaviva Sam. Hyperbolic vq-vaes. 2, 3, 5
[52] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang.
Ntu rgb+ d: A large scale dataset for 3d human activity anal-
ysis. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 1010–1019, 2016. 7, 8
[53] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu.
Skeleton-based action recognition with directed graph neu-
ral networks. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 7912–7921,
2019. 1, 2, 3
[54] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu. Two-
stream adaptive graph convolutional networks for skeleton-
based action recognition. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition ,
pages 12026–12035, 2019. 1, 2, 3, 7
[55] Stuart M Shieber. Evidence against the context-freeness of
natural language. In The Formal complexity of natural lan-
guage , pages 320–334. Springer, 1985. 2, 4
18404
[56] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efficient foundation language models.
arXiv preprint arXiv:2302.13971 , 2023. 1, 3, 7
[57] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete
representation learning. Advances in neural information pro-
cessing systems , 30, 2017. 3, 6
[58] Vivek Veeriah, Naifan Zhuang, and Guo-Jun Qi. Differen-
tial recurrent neural networks for action recognition. In Pro-
ceedings of the IEEE international conference on computer
vision , pages 4041–4049, 2015. 2
[59] Lei Wang and Piotr Koniusz. 3mformer: Multi-order multi-
mode transformer for skeletal action recognition. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 5620–5631, 2023. 1, 3
[60] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua
Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment net-
works: Towards good practices for deep action recognition.
InEuropean conference on computer vision , pages 20–36.
Springer, 2016. 2
[61] Xuanhan Wang, Yan Dai, Lianli Gao, and Jingkuan Song.
Skeleton-based action recognition via adaptive cross-form
learning. In Proceedings of the 30th ACM International Con-
ference on Multimedia , pages 1670–1678, 2022. 7
[62] Xinghan Wang, Xin Xu, and Yadong Mu. Neural koop-
man pooling: Control-inspired temporal dynamics encod-
ing for skeleton-based action recognition. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10597–10607, 2023. 1, 3, 7
[63] Shenghua Wei, Yonghong Song, and Yuanlin Zhang. Human
skeleton tree recurrent neural network with joint relative mo-
tion feature for skeleton based action recognition. In 2017
IEEE international conference on image processing (ICIP) ,
pages 91–95. IEEE, 2017. 2, 5
[64] Rongxiang Weng, Wen Sen Cheng, and Min Zhang. G-
tuning: Improving generalization of pre-trained language
models with generative adversarial network. In Findings of
the Association for Computational Linguistics: ACL 2023 ,
pages 4747–4755, 2023. 2
[65] Wangmeng Xiang, Chao Li, Yuxuan Zhou, Biao Wang,
and Lei Zhang. Generative action description prompts for
skeleton-based action recognition. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 10276–10285, 2023. 3, 7
[66] Wentian Xin, Qiguang Miao, Yi Liu, Ruyi Liu, Chi-Man
Pun, and Cheng Shi. Skeleton mixformer: Multivariate
topology representation for skeleton-based action recogni-
tion. In Proceedings of the 31st ACM International Con-
ference on Multimedia , pages 2211–2220, 2023.
[67] Haojun Xu, Yan Gao, Zheng Hui, Jie Li, and Xinbo
Gao. Language knowledge-assisted representation learn-
ing for skeleton-based action recognition. arXiv preprint
arXiv:2305.12398 , 2023. 3
[68] Kailin Xu, Fanfan Ye, Qiaoyong Zhong, and Di Xie.
Topology-aware convolutional neural network for efficient
skeleton-based action recognition. In Proceedings of theAAAI Conference on Artificial Intelligence , pages 2866–
2874, 2022. 3
[69] Hongliang Yan, Yukang Ding, Peihua Li, Qilong Wang,
Yong Xu, and Wangmeng Zuo. Mind the class weight bias:
Weighted maximum mean discrepancy for unsupervised do-
main adaptation. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 2272–2281,
2017. 7
[70] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial tempo-
ral graph convolutional networks for skeleton-based action
recognition. In Proceedings of the AAAI conference on arti-
ficial intelligence , 2018. 1, 2, 3, 7, 8
[71] Di Yang, Rui Dai, Yaohui Wang, Rupayan Mallick, Luca
Minciullo, Gianpiero Francesca, and Francois Bremond. Se-
lective spatio-temporal aggregation based pose refinement
system: Towards understanding human activities in real-
world videos. In Proceedings of the IEEE/CVF Winter Con-
ference on Applications of Computer Vision , pages 2363–
2372, 2021. 7
[72] Di Yang, Yaohui Wang, Antitza Dantcheva, Lorenzo Garat-
toni, Gianpiero Francesca, and Franc ¸ois Br ´emond. Unik:
A unified framework for real-world skeleton-based action
recognition. arXiv preprint arXiv:2107.08580 , 2021. 1, 7
[73] Jiahang Zhang, Lilang Lin, and Jiaying Liu. Hierarchi-
cal consistent contrastive learning for skeleton-based action
recognition with growing augmentations. In Proceedings of
the AAAI Conference on Artificial Intelligence , pages 3427–
3435, 2023. 3
[74] Jiahui Zhang, Fangneng Zhan, Christian Theobalt, and Shi-
jian Lu. Regularized vector quantization for tokenized im-
age synthesis. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 18467–
18476, 2023. 3
[75] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli
Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi
Shen. T2m-gpt: Generating human motion from textual
descriptions with discrete representations. arXiv preprint
arXiv:2301.06052 , 2023. 2, 3, 6, 7
[76] Yuhan Zhang, Bo Wu, Wen Li, Lixin Duan, and Chuang Gan.
Stst: Spatial-temporal specialized transformer for skeleton-
based action recognition. In Proceedings of the 29th ACM
International Conference on Multimedia , pages 3229–3237,
2021. 3
[77] Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu
Chen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. Mo-
tiongpt: Finetuned llms are general-purpose motion genera-
tors. arXiv preprint arXiv:2306.10900 , 2023. 2, 3, 6
[78] Huanyu Zhou, Qingjie Liu, and Yunhong Wang. Learn-
ing discriminative representations for skeleton based action
recognition. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , 2023. 1, 7
[79] Lanyun Zhu, Tianrun Chen, Deyi Ji, Jieping Ye, and Jun Liu.
Llafs: When large-language models meet few-shot segmen-
tation. arXiv preprint arXiv:2311.16926 , 2023. 3
[80] Wentao Zhu, Xiaoxuan Ma, Zhaoyang Liu, Libin Liu, Wayne
Wu, and Yizhou Wang. Motionbert: A unified perspective
on learning human motion representations. In Proceedings
18405
of the IEEE/CVF International Conference on Computer Vi-
sion, 2023. 1, 3, 7
[81] Yisheng Zhu, Hui Shuai, Guangcan Liu, and Qingshan
Liu. Multilevel spatial–temporal excited graph network for
skeleton-based action recognition. IEEE Transactions on Im-
age Processing , 32:496–508, 2022. 7
[82] G. K. Zipf. The psycho-biology of language, 1935. 2, 4
18406
