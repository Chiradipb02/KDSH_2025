Domain-Rectifying Adapter for Cross-Domain Few-Shot Segmentation
Jiapeng Su1*, Qi Fan2*, Wenjie Pei1†, Guangming Lu1, Fanglin Chen1
1Harbin Institute of Technology, Shenzhen2Nanjing University
MattSu@163.com, fanqics@gmail.com, wenjiecoder@outlook.com,
luguangm@hit.edu.cn, chenfanglin@hit.edu.cn
Abstract
Few-shot semantic segmentation (FSS) has achieved
great success on segmenting objects of novel classes, sup-
ported by only a few annotated samples. However, exist-
ing FSS methods often underperform in the presence of
domain shifts, especially when encountering new domain
styles that are unseen during training. It is suboptimal to
directly adapt or generalize the entire model to new do-
mains in the few-shot scenario. Instead, our key idea is
to adapt a small adapter for rectifying diverse target do-
main styles to the source domain. Consequently, the rec-
tified target domain features can fittingly benefit from the
well-optimized source domain segmentation model, which
is intently trained on sufficient source domain data. Train-
ing domain-rectifying adapter requires sufficiently diverse
target domains. We thus propose a novel local-global style
perturbation method to simulate diverse potential target
domains by perturbating the feature channel statistics of
the individual images and collective statistics of the entire
source domain, respectively. Additionally, we propose a
cyclic domain alignment module to facilitate the adapter
effectively rectifying domains using a reverse domain rec-
tification supervision. The adapter is trained to rectify the
image features from diverse synthesized target domains to
align with the source domain. During testing on target do-
mains, we start by rectifying the image features and then
conduct few-shot segmentation on the domain-rectified fea-
tures. Extensive experiments demonstrate the effectiveness
of our method, achieving promising results on cross-domain
few-shot semantic segmentation tasks. Our code is avail-
able at https://github.com/Matt-Su/DR-Adapter.
1. Introduction
Benefiting from well-established large-scale datasets [1,
24], numerous semantic segmentation methods [5, 30, 36]
*Both authors contributed equally.
†Corresponding author.
test
(b) Adapt the entire model to 
new domainsModeltrain & adapt
test
(a) Train and test the model 
       on the same domaintrain
ModelSD MultipleSD
(c) Train an adapter to rectify diverse target domains (ours)train & 
adaptrectify trainsimulate
rectify & test
adapter Model
TDSDTD
SIMD DiverseSD
Target domain TDSDSource domain Simulated target domain SIMDFigure 1. The comparison of our method with other approaches.
(a) Traditional few-shot segmentation (FSS) methods train and
test the model on the same domain. (b) Most domain general-
ization (DG) methods leverages multiple source domains to train
and adapt the large-parameter model simultaneously. (c) In con-
trast to conventional DG methods, we propose using a lightweight
adapter as a substitute. This adapter is designed to adapt to vari-
ous domain data, thereby decoupling domain adaptation from the
source domain training process.
have undergone rapid development in recent years. How-
ever, obtaining enough labeled data is still a challenging
and resource-intensive process, particularly for tasks like in-
stance and semantic segmentation. Unlike machine learning
approaches, human capacity to recognize novel concepts
from limited examples fuels considerable research interest.
Hence, few-shot segmentation (FSS) is proposed to meet
this challenge, developing a network that generalizes to new
domains with limited annotated data.
Nonetheless, most existing few-shot segmentation meth-
ods [12, 21, 31, 32, 34, 41, 51, 53, 54] often exhibit subpar
performance when confronted with domain shifts [25, 37,
50]. The cross-domain few-shot segmentation (CD-FSS) is
thus proposed for generalizing few-shot segmentation mod-
els from the source domain to other domains [20, 47]. CD-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
24036
FSS trains the model solely on the source domain, and gen-
eralizes the trained model to segment object of novel classes
in a separate target domain, supported by few-shot samples.
Domain adaptation(DA) and domain generalization(DG)
are closely related to cross-domain few-shot segmentation.
However, DA methods require unlabeled training data from
the target domain. DG aims to generalize models trained
in the source domain to various unseen domains, often re-
quiring extensive training data from multiple source do-
mains. Consequently, DA/DG methods typically adapt
the entire model to new domains, leveraging substantial
domain-specific data. Similarly, most existing CD-FSS
methods adapt the entire model to target domains. However,
in few-shot learning, the scarcity of training data can lead to
overfitting when directly adapting the entire model. Rather
than generalizing the entire model, our approach focuses on
adapting a compact adapter to rectify diverse target domain
features to align with the source domain. Once rectified to
the source domain, target domain features can effectively
utilize the well-trained source domain segmentation model,
which is intently optimized using extensive source domain
data. Figure 1 shows the difference among our method and
conventional FSS and domain generalization methods.
Training a domain-rectifying adapter requires extensive
data of diverse target domains. The straightforward feature-
level domain synthesis method can effectively generate di-
verse potential target domains by randomly perturbing fea-
ture channel statistics. We can diversify the synthesized
domain styles by increasing the magnitude of perturbation
noises. However, as shown in Figure 2, some feature chan-
nels in individual images exhibit very low activation val-
ues. These small feature channel statistic values result in the
corresponding channels suffering from limited style synthe-
sis. Merely increasing the perturbation noises may lead to
model collapse, where highly activated channels are exces-
sively perturbed. Consequently, we propose a novel local-
global style perturbation method to generate diverse poten-
tial target domain styles. Our local style perturbation mod-
ule generates new domains by perturbing the feature chan-
nel statistics of individual images, similar to DG methods.
Our global style perturbation module effectively diversifies
the synthesized styles by leveraging the collective feature
statistics of the entire source domain. Dataset-level feature
statistics are estimated through momentum updating on the
entire source domain dataset. Our local and global style
perturbation modules collaboratively generate diverse and
meaningful domain styles.
The perturbed feature channel statistics represent diverse
potential styles, which are then input into the adapter to
train the domain-rectifying adapter. The adapter predicts
two rectification vectors to rectify the perturbed feature
channel statistics to their original values. Additionally, we
propose a cyclic domain alignment module to assist the
0 5 10 15 20 25 30
Channel Index0.00.10.20.30.4Channel Statistic ValuesChannel Statistics Comparison
Average statistic
Individual sample statisticFigure 2. We show the feature channel statistic of an individual
sample’s statistic and the average statistic across the dataset on the
pretrained backbone at stage 1. The average statistics exhibit a
smoother profile compared to that of an individual sample, allow-
ing for the application of more substantial noise to the feature with
the smoother statistics.
adapter in learning to effectively rectify diverse domain
styles to align with the source domain. Once rectified, the
feature channel statistics will collaborate with the normal-
ized feature map to train the segmentation model. During
inference, we can directly use the domain-rectifying adapter
to align the image features with the source domain and then
input them into the well-trained source domain model for
segmentation. In summary, our contributions are:
• We introduce a novel domain-rectifying method for cross-
domain few-shot segmentation, employing a compact
adapter to align diverse target domain features with the
source domain, mitigating overfitting in limited training
data scenarios.
• We propose a unique local-global style perturbation mod-
ule that generates diverse target domain styles by per-
turbing feature channel statistics at both local and global
scales, enhancing model adaptability to various target do-
mains.
• To enhance domain adaptation, we introduce a cyclic
domain alignment loss that helps the domain-rectifying
adapter align diverse domain styles with the source do-
main.
2. Related Work
2.1. Few-Shot Segmentation
Few-shot semantic segmentation [26–29, 41, 46, 49, 53,
57], using a limited number of labeled support images, pre-
dicts dense masks for query images. Previous methods pri-
marily adopted a metric-based paradigm [8], improved in
various ways, and fell into two main categories: prototype-
based and matching-based approaches. Motivated by Pro-
totypicalNet [40], prototype-based methods extract proto-
types from support images to guide query object segmen-
tation. Most studies concentrate on effectively utilizing
limited support images to obtain more representative pro-
24037
totypes. Recent studies [52, 54] emphasize that a sin-
gle prototype often fails to represent an entire object ade-
quately. To address this, methods such as ASGNet [21] and
PRMMs [41] explore using multiple prototypes to represent
the overall target.
On the other hand, matching-based methods [31, 32, 43]
concatenate support and query features, subsequently in-
putting the concatenated feature map into CNN or trans-
former networks. This process explores the dense corre-
spondence between query images and support prototypes.
Recently, researches [35, 39] has focused on leveraging
pixel-to-pixel similarity maps for effective support proto-
type generation and query feature enhancement.
2.2. Domain Generalization
Domain Generalization (DG) targets at generalizing models
to diverse target domains, particularly when target domain
data is inaccessible during training. Existing domain gener-
alization methods fall into two categories: learning domain-
invariant feature representations from multiple source do-
mains [9, 14, 33, 45] and generating diverse samples via
data or feature augmentation [4, 38, 44, 58]. The core idea
of learning domain-invariant features is to leverage vari-
ous source domains to learn a robust feature representation.
Data or feature augmentation aims to increase the diversity
of training samples to simulate diverse new domains.
Domain generalization is particularly challenging in
few-shot settings, as the target domain substantially differs
from the source domains in both domain style and class con-
tent. Unlike popular DG methods generalizing the entire
model, we train a small adapter to rectify the target domain
data into the source domain style for model generalization.
2.3. Cross-domain Few-Shot segmentation
Recently, cross-domain few-shot segmentation has received
increasing attention. PATNet [20] proposes a feature trans-
formation layer to map query and support features from any
domain into a domain-agnostic feature space. RestNet [17]
addresses the intra-domain knowledge preservation prob-
lem in CD-FSS. RD [47] employs a memory bank to re-
store the meta-knowledge of the source domain to augment
the target domain data. Unlike previous CD-FSS methhods,
our method directly learns two rectification parameters for
effective domain adaptation, eliminating the needs of restor-
ing source domain styles.
3. Methodology
Problem Setting Cross-Domain Few-Shot Segmentation
(CD-FSS) aims to apply the source domain trained few-shot
segmentation models to diverse target domains. The CD-
FSS model is typically trained using episode-based meta-
learning paradigm [11, 43]. The training and testing data
both consist of thousands of randomly sampled episodes,including Ksupport samples and one query image. The
model first extracts the support prototype and query feature
from each training episode, and then performs pixel-wise
feature matching between the support prototype and query
feature to predict the query mask. The support prototype
is typically a feature vector aggregating the object features
of all support images. Once trained, the model is directly
applied to various target domains.
Method Overview Our key idea is to train a adapter to
rectify diverse target domain styles to the source domain,
and leverage the well-trained source domain segmentation
model to process the rectified target domain features for ac-
curate few-shot segmentation. The crux is to align diverse
potential target domain distributions to the source domain
distribution. To train the domain-rectifying adapter, we thus
synthesize various target domain styles by perturbing the
feature channel statistics of the source domain training im-
ages. And the adapter is trained to rectify the synthesized
feature styles to the source domain style. During inference,
the adapter can be directly applied on the target domain fea-
tures to rectify their domain styles, and the subsequent seg-
mentation model can process the rectified support and query
features for few-shot segmentation. he overall framework of
our approach is illustrated in Figure 3.
3.1. Local Domain Perturbation
Previous works [13, 59] show that perturbing feature chan-
nel statistics can effectively synthesize diverse domain
styles and meanwhile preserves the image contents. We
thus synthesize various domain styles by injecting gaussian
noises into feature channel statistics of source domain im-
ages.
Given a feature map Fo∈ RB×C×H×W, we first com-
pute the feature channel statistics, i.e., mean µoand vari-
anceσoalong each channel dimension:
µo(Fo) =1
HWHX
h=1WX
w=1Fo, (1)
σo(Fo) =vuut1
HWHX
h=1WX
w=1(Fo−µo(Fo))2+ϵ, (2)
where µo, σo∈ RB×C,ϵis a small constant for numerical
stability, B,C,H, andWrepresent the batch size, channel
dimension, height, and width of the feature map.
Then, we leverage two perturbation factors αandβto
control the gaussian noise injection process for µoandσo.
The noise vectors, sharing the same dimension as µoandσo,
are used to compute the perturbed mean µpand variance σp:
µp= (1 + α)µo,
σp= (1 + β)σo.(3)
24038
μs   σsIndividual 
style
N(0,0.75)
momentum 
update
Average 
style
Pglobal<0.5Plocal<0.5
Source Domain
Episode
blockx-1
Adapterμrect,σrect...
blockx
(Plocal>0.5) ∩ (Pglobal>0.5)
μa   σa
...
p(x) Rectificaiton ModulePerturbation 
Module
Perturbed Statistics
 Perturbed Features
 Original Statistics
 Convolution LayerFigure 3. Overview of our cross-domain few-shot segmentation approach. Our method consists of two modules: a feature perturbation
module and a feature rectification module. The former is used to generate simulated domain features, while the latter trains the adapter by
restoring the features to their original states. During the perturbation process, we employ both local and global perturbations, controlled by
two different probabilities Pto decide if a feature is perturbed. Note that when both probabilities exceed 0.5, the entire backbone undergoes
standard training. During testing, we treat target domain features as perturbed features and directly rectify them using the adapter.
We can obtain the perturbed feature map Fpby replacing
the feature channel statistics {µo, σo}of the original feature
mapFowith the perturbed channel statistics {µp, σp}using
the Adaptive Instance Normalization formula [16]:
Fp=σpFo−µo
σo+µp. (4)
Within each episode, the support and query features
share the same perturbation factors. The above equations
can be further simplified to the following expression:
Fp= (1 + β)Fo+ (α−β)µo. (5)
We call this feature channel statistic perturbation method
as local domain perturbation, as it is enabled on individual
images with probability Plocal.
3.2. Global Domain Perturbation
We need to bound the local domain perturbation to prevent
potential training collapse caused by the aggressive pertur-
bation noises. However, insufficient domain perturbation
may lead the domain-rectifying adapter to underperform
when encountering new domain styles. The local domain
perturbation method is trapped in the stability and perfor-
mance dilemma. We thus propose a novel global domain
perturbation by leveraging the global style statistics of the
entire dataset to facilitate the domain style synthesis. Thedataset’s global style statistics exhibit better perturbation
stability when leveraging aggressive perturbation noises to
synthesize meaningful target domain styles for sufficient
style diversity.
We first compute the feature channel statistics µofor in-
dividual images and then progressively update the global
style statistics through momentum updating:
µdatum =λµdatum + (1−λ)µo, (6)
where λis the momentum updating factor. Then we can per-
form the global domain perturbation by replacing the image
feature channel statistics µoin equation 5 with the global
style statistics µdatum. This global domain perturbation is
randomly enabled with probability Pglobal.
3.3. Domain Rectification Module
The domain rectification module leverages an domain-
rectifying adapter to rectify the target domain feature chan-
nel statistics to the source domain. The adapter takes as
input the perturbed features and predicts two rectification
vectors {αrect, βrect}to rectify the feature channel statis-
tics of the perturbed feature map Fpas the rectified feature
channel statistics {µrect, σrect}.
µrect= (1 + αrect)µp,
σrect= (1 + βrect)σp.(7)
24039
Simulated 
domainsP
PR
R
'
rectFoF
rectFLcycLalign
Source 
domainsFigure 4. The process of cycle alignment, where ’P’ denotes per-
turbation and ’R’ stands for rectification.
Then we leverage the AdaIN function to generate the rec-
tified feature map Frectbased on the perturbed feature map
Fpand the rectified feature channel statistics {µrect, σrect}:
Frect= (1 + βrect)σpFp−µp
σp+ (1 + αrect)µp,(8)
which can be further simplified as:
Frect= (1 + βrect) Fp+ (αrect−βrect)µp. (9)
We expect the adapter can adaptively predict the rectifi-
cation factors {αrect, βrect}to rectify the perturbed features
corresponding to diverse potential target domains. Conse-
quently, during inference, we can leverage the adapter to
rectify the target domain features to the source domain,
and the rectified features can fittingly benefit from the well-
trained source domain model for satisfactory few-shot seg-
mentation results.
3.4. Cyclic Domain Alignment
Our goal is enabling the adapter to rectify the perturbed fea-
tures back to the source domain space. Insufficient supervi-
sion during this process may lead the adapter to rectify the
features into an unknown space. Therefore, in addition to
utilizing the standard Binary Cross-Entropy (BCE) loss for
supervision, we propose incorporation of a cyclic alignment
loss to constrain the adapter.
After obtaining the rectified feature Frect, we further
perturb the Frectwith the same noise αandβto get a new
perturbed feature Fp
rect. This perturbed image Fp
rectis then
input into the adapter for a reverse rectification, resulting in
F′
rect. If the adapter can map features back to the source do-
main space, the style of F′
rectshould closely match that of
Fo. The cycle process is shown in figure 4. Consequently,
we align the statistics between original feature and the cycli-
cally rectified feature:
Lcyc=1
CX
c(|µ(Fo)−µ(F′
rect)|
+|σ(Fo)−σ(F′
rect)|).(10)We add constraint to the statistics between FoandFrect:
Lalign=1
CX
c(|µ(Fo)−µ(Frect)|
+|σ(Fo)−σ(Frect)|).(11)
We optimize the model with the final loss L:
L=LBCE +Lcyc+Lalign (12)
4. Experiments
4.1. Datasets
Following [20], we validate our methods on the cross-
domain few-shot segmentation (CD-FSS) benchmark. This
benchmark includes images and pixel-level annotations
from the FSS-1000 [22], DeepGlobe [7], ISIC2018 [6, 42],
and Chest X-ray datasets [3, 18]. These datasets range from
natural to medical images, providing sufficient domain di-
versity. We train models on the natural image dataset PAS-
CAL VOC 2012 [10] with SBD [15] augmentation and eval-
uate models on the CD-FSS benchmark.
FSS-1000 [22] is a dataset designed for few-shot seg-
mentation, containing 1,000 different categories of natural
objects and scenes, with each category comprising 10 an-
notated images. We evaluate models on the official test set
with 2,400 images.
Deepglobe [7] is a complex Geographic Information
System (GIS) dataset, containing satellite images with cat-
egories of urban, agriculture, rangeland, forest, water, bar-
ren, and unknown. For testing, we follow the processing
in [20] to divide each image into six patches and filtering
out the ’unknown’ category, and evaluate models on the re-
sulting 5,666 test images and their corresponding masks.
ISIC2018 [6, 42] is used for skin lesion analysis, con-
taining numerous skin images with associated segmentation
labels. We evaluate models on the official training set fol-
lowing the common practice [6], using a uniform resolution
of 512×512 pixels, comprising a total of 2,596 test images.
Chest X-ray [3, 18] is an X-ray image dataset for tuber-
culosis detection, containing X-ray images of Tuberculosis
cases as well as images from normal cases. We downsam-
ple the original image resolution to 1024×1024 pixels for
testing.
4.2. Implementation Details
We utilize the train set of PASCAL VOC dataset as the
source domain training set. During training, we employ
SSP [12] with the ResNet-50 backbone as the baseline
model. We first train the baseline model on the whole train-
ing set, and then train our method with additional 5 epochs
using a batch size of 8. We use SGD to optimize our model,
with a 0.9 momentum and an initial learning rate of 1e-3.
To reduce memory consumption and accelerate the training
24040
Table 1. Mean-IoU of 1-way 1-shot and 5-shot results of tradictional few-shot approaches and cross-domain few-shot method on the four
CD-FSS benchmark.Bold denotes the best performance among all methods.
MethodsDeepglobe ISIC Chest X-ray FSS-1000 Average
1-shot 5-shot 1-shot 5-shot 1-shot 5-shot 1-shot 5-shot 1-shot 5-shot
Few-shot Segmentation Methods
PGNet [52]
PANet [46]
CaNet [53]
RPMMs [48]
PFENet [41]
RePRI [2]
HSNet [32]
SSP [12]10.73
36.55
22.32
12.99
16.88
25.03
29.65
40.4812.36
45.43
23.07
13.47
18.01
27.41
35.08
49.6621.86
25.29
25.16
18.02
23.50
23.27
31.20
35.0921.25
33.99
28.22
20.04
23.83
26.23
35.10
44.9633.95
57.75
28.35
30.11
27.22
65.08
51.88
74.2327.96
69.31
28.62
30.82
27.57
65.48
54.36
80.5162.42
69.15
70.67
65.12
70.87
70.96
77.53
79.0362.74
71.68
72.03
67.06
70.52
74.23
80.99
80.5632.24
47.19
36.63
31.56
34.62
46.09
47.57
57.2031.08
55.10
37.99
32.85
34.98
48.34
51.38
63.92
Cross-domain Few-shot Segmentation Methods
PATNet [20]
Ours37.89
41.2942.97
50.1241.16
40.7753.58
48.8766.61
82.3570.20
82.3178.59
79.0581.23
80.4056.06
60.8661.99
65.42
process, we resize both query and support images to 400 ×
400. We apply our two domain perturbation modules into
the first three layers of ResNet. For local perturbations, we
use the Gaussian noise with a mean of zero and a standard
deviation of 0.75, while for global perturbations, we used
the Gaussian noise with a mean of zero and a standard de-
viation of one. All models are evaluated using the mean
Intersection Over Union (mIOU).
4.3. Comparison Experiments
In Table 1, we present a comparison between our method
and other approaches, including traditional few-shot seg-
mentation methods and existing cross-domain few-shot seg-
mentation methods. Traditional few-shot segmentation
methods usually underperform in cross-domain scenarios
due to the large domain gap between the train and test
data. While our approach effectively reduces the domain
gap and improves the segmentation performance. This per-
formance improvement is particularly notable in Chest X-
rays, where our 1-shot and 5-shot performance surpasses
the PATNet [20] by 15.74% and 12.11%, respectively. In
Deepglobe, the improvement is 3.4%(1-shot) and 7.15%(5-
shot). For FSS-1000, our model achieves comparable per-
formance to PATNet, because the domain gap is small.
We also follow the same setting of RD [47] to train
our model on VOC and evaluate models on SUIM. Table
2 shows our method performs much better than RD.
We present some qualitative results of our proposed
model for 1-way 1-shot segmentation in Fig. 5. These re-
sults indicate that our method improves the generalization
ability of traditional few-shot models, attributing to its ca-
pability of aligning various domains to the source domain.Table 2. Mean-IoU of 1-way 1-shot results of our method follow-
ing the same setting of RD[47].
split-0 split-1 split-2 split-3 Average
RD[47] 35.20 33.40 34.30 36.00 34.70
Ours 40.60 38.18 41.53 40.72 40.25
Support Query Baseline Ours
Figure 5. Qualitative results of our model and baseline in 1-way
1-shot setting on challenging scenarios with large domain gap.
4.4. More Analysis
We conduct extensive ablation experiments to demonstrate
and analyze the effectiveness of our approach.
4.4.1 Ablation Studies
We conduct comprehensive ablation experiments to evalu-
ate the effectiveness of our proposed components.
24041
0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0
Noise Variance54555657585960mIoU
The impact of noise variance on local vs. global perturbations.
local perturbation
global perturbationFigure 6. We demonstrate the trend of global and local perturba-
tions under different Gaussian noise variances.
Table 3. The effects of each module within the baseline, namely
the Perturbation module, Rectification module, and Cyclic Align-
ment Loss, are demonstrated.
Perturbation Rectification Cyclic Alignment mean-IoU
57.20
✓ 58.45
✓ 57.80
✓ ✓ 59.17
✓ ✓ ✓ 60.86
Table 4. Results of using our Cyclic Alignment Loss.
BCE loss + cyclic loss + align loss + cyclic & align loss
57.65 58.56 59.12 60.86
Impact of Noise Variance on Perturbations. Figure 6
illustrates the effects of Gaussian noise with varying vari-
ances in local and global perturbations. Local perturbations
suffer from performance degradation with slightly higher
noise levels, whereas global perturbations withstand larger
noise levels with minimal performance impact, suggesting
greater stability. Thus, in our method, we set Gaussian noise
variances at 0.75 for local and 1 for global perturbations to
broaden the simulated domain range and improve the do-
main generalizability.
Impact of Each Component. Table 3 illustrates the ef-
fectiveness of each module in the model. Integration of all
modules results in 3.66% performance improvement com-
pared to the SSP [12] baseline. Importantly, the feature
perturbation and rectification processes complement each
other: perturbation simulates features across domains, and
rectification aligns these features back to the source domain
space. Solely using feature perturbation degrades the model
to the domain generalization approach similar to NP [13].
Additionally, our cyclic alignment loss is indispensable as
it ensures the unification of images from various domains to
the source domain.
Points of same color refer to the same sample
Average distance between original and perturbed features     1.91
Average distance between original and perturbed features     1.24
Points of same color refer to the same sample
Average distance between original and perturbed features     0.58
Average distance between original and perturbed features     0.31Figure 7. Visual analysis (t-SNE) of channel-wise means(top) and
variations(bottom).
Table 5. Results of feature perturbation methods.
local style global style Both perturbation
mean-IoU 59.81 59.17 60.86
Table 4 shows the ablation analysis on Cyclic Alignment
Loss. Both the alignment loss and cyclic loss can improve
the performance.
Table 5 compares the impact of local and global styles
in feature perturbation, showing that their combination im-
proves model performance attributing to a wider range of
domain simulation. Using the channel-wise means and vari-
ances as features, the t-SNE(Figure 7) shows that the per-
turbed features are rectified to be closer to the original fea-
tures, demonstrating our model’s effectiveness.
Impact of Noise Types. We choose the popular Gaus-
sian distribution to generate random noises, which has been
widely used by other works ( e.g., Mixstyle, DSU and NP).
Perturbing feature statistics with random noises can effec-
tively synthesize diverse domain styles, while the noise type
is not essential. Table 6 shows that our method is insensitive
to the noise types, performing well with Beta, and Uniform
noises. Note that our novel Local-Global Domain Perturba-
tion and Cyclic Domain Alignment can largely improve the
domain style synthesis diversity for all kinds of noise.
More adapters. Table 7 shows that applying multiple
adapters can further improve performance.
24042
Table 6. Results of using different types of noise.
Noise Type Gaussian (1,0.75) Beta (3,4) Uniform (-1,1)
mIoU 60.86 60.78 60.00
Table 7. Results of using one/two adapters within a single stage.
FSS Chest Deepglobe ISIC Average
One adapter 79.05 82.35 41.29 40.77 60.86
Two adapters 79.25 83.04 41.74 41.63 61.41
Table 8. Comparison to domain adaption and domain generaliza-
tion approaches under 1-shot setting. We use same baseline with
different methods to ensure fair comparison.
Method FSS Chest Deepglobe ISIC Average
Baseline(SSP [12]) 79.03 74.23 40.48 35.09 57.20
AdaIN [16] 78.89 74.23 41.85 34.36 57.33
Mixstyle [59] 79.24 76.63 41.05 35.98 58.21
DSU [23] 78.99 77.83 41.19 36.64 58.66
NP [13] 78.98 76.44 41.83 37.87 58.78
Ours 79.05 82.35 41.29 40.77 60.86
4.4.2 Comparion with Domain Transfer Methods
We compare our method against traditional domain adap-
tation (DA) and domain generalization (DG) approaches to
validate our method’s effectiveness. For a fair comparison,
all categories in the PASCAL VOC were used for training
in both DA and DG methods. We evaluate models in the
1-shot setting on the CD-FSS benchmark.
Domain Adaptation. We adopt the classical AdaIN [16]
method to train four models for the four test datasets. Dur-
ing training, we randomly sample images from the test
dataset and extract their feature channel statistics in the low-
level feature map. And then the AdaIN is applied to replace
the feature channel statistics of the train image with the ex-
tracted statistics from the test dataset.
Domain Generalization. We employ the Mixstyle [59],
DSU [23] and NP [13] methods for comparison. These ap-
proaches also involves perturbing feature statistics, but they
only perform local perturbations and lack a feature rectifi-
cation process.
Table 8 shows that our method performs much better
than DA and DG methods in cross-domain few-shot seg-
mentation.
4.4.3 Applying SAM in CD-FSS
The recent released large-scale SAM [19] model has greatly
advanced image segmentation, demonstrating remarkable
zero-shot segmentation capabilities. However, SAM cannot
be directly applied to cross-domain few-shot segmentation.
Thus we evaluate PerSAM [56] to compare our method to
the SAM-based method in cross-domain few-shot segmen-Table 9. The result of directly applying PerSAM to cross-domain
few-shot segmentation.
FSS Chest Deepglobe ISIC Average
PerSAM [56] 79.65 31.12 33.39 21.27 41.35
Ours 79.05 82.35 41.29 40.77 60.86
Table 10. Applying our method to transformers can further en-
hance the model’s performance in cross-domain tasks.
FSS Chest Deepglobe ISIC Average
FPTrans [55] 78.92 80.49 39.21 47.79 61.60
FPTrans + ours 78.63 82.74 40.32 49.43 62.78
tation. PerSAM is a training-free method. It adapts SAM
into the one-shot setting by using support images as the
prompt input to segment target objects in query images. Ta-
ble 9 shows that our method performs much better than Per-
SAM in cross-domain few-shot segmentation.
4.4.4 Extension to Transformer
In Table 10, we show the results of applying our method
within FPTrans[55], which leverages support sample proto-
types as prompts and Vision Transformer (ViT) as the back-
bone. Applying our method to the lower-level blocks of ViT
improves performance in cross-domain datasets.
5. Conclusion
In this paper, we propose a method to effectively bridge the
domain gap between different datasets by aligning the tar-
get domain with the source domain space. During train-
ing, we train a unified adapter by using simulated perturbed
features. In the inference stage, we consider target domain
images as a form of perturbed images for the direct recti-
fication. Furthermore,we introduce both local and global
perturbations to ensure significant style changes, not only
based on individual sample but also on the overall style of
the dataset. We utilize a cyclic alignment loss to ensure
the alignment between the source and target domains for
model optimization. We conduct extensive experiments to
validate the effectiveness of the proposed framework on var-
ious cross-domain segmentation tasks and achieve state-of-
the-art (SOTA) results on multiple benchmarks.
Acknowledgement. This work was supported in
part by the National Natural Science Foundation of
China (U2013210, 62372133), Guangdong Basic
and Applied Basic Research Foundation under Grant
(Grant No. 2022A1515010306, 2024A1515011706),
Shenzhen Fundamental Research Program (Grant No.
JCYJ20220818102415032), and Shenzhen Key Technical
Project (Grant NO. KJZD20230923115117033).
24043
References
[1] Rodrigo Benenson, Stefan Popov, and Vittorio Ferrari.
Large-scale interactive object segmentation with human an-
notators. In CVPR , 2019. 1
[2] Malik Boudiaf, Hoel Kervadec, Ziko Imtiaz Masud, Pablo
Piantanida, Ismail Ben Ayed, and Jose Dolz. Few-shot seg-
mentation without meta-learning: A good transductive infer-
ence is all you need? In CVPR , 2021. 6
[3] Sema Candemir, Stefan Jaeger, Kannappan Palaniappan,
Jonathan P Musco, Rahul K Singh, Zhiyun Xue, Alexandros
Karargyris, Sameer Antani, George Thoma, and Clement J
McDonald. Lung segmentation in chest radiographs using
anatomical atlases with nonrigid registration. IEEE Transac-
tions on Medical Imaging , 2013. 5
[4] Fabio M Carlucci, Antonio D’Innocente, Silvia Bucci, Bar-
bara Caputo, and Tatiana Tommasi. Domain generalization
by solving jigsaw puzzles. In CVPR , 2019. 3
[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic im-
age segmentation with deep convolutional nets, atrous con-
volution, and fully connected crfs. Transactions on Pattern
Analysis and Machine Intelligence , 2017. 1
[6] Noel Codella, Veronica Rotemberg, Philipp Tschandl,
M Emre Celebi, Stephen Dusza, David Gutman, Brian
Helba, Aadi Kalloo, Konstantinos Liopyris, Michael
Marchetti, et al. Skin lesion analysis toward melanoma
detection 2018: A challenge hosted by the interna-
tional skin imaging collaboration (isic). arXiv preprint
arXiv:1902.03368 , 2019. 5
[7] Ilke Demir, Krzysztof Koperski, David Lindenbaum, Guan
Pang, Jing Huang, Saikat Basu, Forest Hughes, Devis Tuia,
and Ramesh Raskar. Deepglobe 2018: A challenge to parse
the earth through satellite images. In CVPR , 2018. 5
[8] Nanqing Dong and Eric P Xing. Few-shot semantic segmen-
tation with prototype learning. In BMVC , 2018. 2
[9] Yingjun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong
Zhen, Cees GM Snoek, and Ling Shao. Learning to learn
with variational information bottleneck for domain general-
ization. In ECCV , 2020. 3
[10] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge. IJCV , 2010. 5
[11] Qi Fan, Wei Zhuo, Chi-Keung Tang, and Yu-Wing Tai. Few-
shot object detection with attention-rpn and multi-relation
detector. In CVPR , 2020. 3
[12] Qi Fan, Wenjie Pei, Yu-Wing Tai, and Chi-Keung Tang. Self-
support few-shot semantic segmentation. In ECCV , 2022. 1,
5, 6, 7, 8
[13] Qi Fan, Mattia Segu, Yu-Wing Tai, Fisher Yu, Chi-Keung
Tang, Bernt Schiele, and Dengxin Dai. Towards robust ob-
ject detection invariant to real-world domain shifts. In ICLR ,
2022. 3, 7, 8
[14] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang,
and David Balduzzi. Domain generalization for object recog-
nition with multi-task autoencoders. In ICCV , 2015. 3
[15] Bharath Hariharan, Pablo Arbel ´aez, Lubomir Bourdev,Subhransu Maji, and Jitendra Malik. Semantic contours from
inverse detectors. In ICCV , 2011. 5
[16] Xun Huang and Serge Belongie. Arbitrary style transfer in
real-time with adaptive instance normalization. In CVPR ,
2017. 4, 8
[17] Xinyang Huang, Chuang Zhu, and Wenkai Chen. Restnet:
Boosting cross-domain few-shot segmentation with residual
transformation network. In BMVC , 2023. 3
[18] Stefan Jaeger, Alexandros Karargyris, Sema Candemir, Les
Folio, Jenifer Siegelman, Fiona Callaghan, Zhiyun Xue,
Kannappan Palaniappan, Rahul K Singh, Sameer Antani,
et al. Automatic tuberculosis screening using chest radio-
graphs. IEEE Transactions on Medical Imaging , 2013. 5
[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. In ICCV , 2023. 8
[20] Shuo Lei, Xuchao Zhang, Jianfeng He, Fanglan Chen,
Bowen Du, and Chang-Tien Lu. Cross-domain few-shot se-
mantic segmentation. In ECCV , 2022. 1, 3, 5, 6
[21] Gen Li, Varun Jampani, Laura Sevilla-Lara, Deqing Sun,
Jonghyun Kim, and Joongkyu Kim. Adaptive prototype
learning and allocation for few-shot segmentation. In CVPR ,
2021. 1, 3
[22] Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and
Chi-Keung Tang. Fss-1000: A 1000-class dataset for few-
shot segmentation. In CVPR , 2020. 5
[23] Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying
Shan, and Ling-Yu Duan. Uncertainty modeling for out-of-
distribution generalization. In ICLR , 2022. 8
[24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
ECCV , 2014. 1
[25] Bingyu Liu, Zhen Zhao, Zhenpeng Li, Jianan Jiang, Yuhong
Guo, and Jieping Ye. Feature transformation ensemble
model with batch spectral regularization for cross-domain
few-shot classification. arXiv preprint arXiv:2005.08463 ,
2020. 1
[26] Lizhao Liu, Junyi Cao, Minqian Liu, Yong Guo, Qi Chen,
and Mingkui Tan. Dynamic extension nets for few-shot se-
mantic segmentation. In ACMMM , 2020. 2
[27] Weide Liu, Chi Zhang, Guosheng Lin, and Fayao Liu. Cr-
net: Cross-reference networks for few-shot segmentation. In
CVPR , 2020.
[28] Yongfei Liu, Xiangyi Zhang, Songyang Zhang, and Xum-
ing He. Part-aware prototype network for few-shot semantic
segmentation. In ECCV , 2020.
[29] Yuanwei Liu, Nian Liu, Qinglong Cao, Xiwen Yao, Junwei
Han, and Ling Shao. Learning non-target knowledge for few-
shot semantic segmentation. In CVPR , 2022. 2
[30] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In
CVPR , 2015. 1
[31] Zhihe Lu, Sen He, Xiatian Zhu, Li Zhang, Yi-Zhe Song, and
Tao Xiang. Simpler is better: Few-shot semantic segmenta-
tion with classifier weight transformer. In ICCV , 2021. 1,
3
24044
[32] Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrela-
tion squeeze for few-shot segmentation. In ICCV , 2021. 1,
3, 6
[33] Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gi-
anfranco Doretto. Unified deep supervised domain adapta-
tion and generalization. In ICCV , 2017. 3
[34] Khoi Nguyen and Sinisa Todorovic. Feature weighting and
boosting for few-shot segmentation. In ICCV , 2019. 1
[35] Bohao Peng, Zhuotao Tian, Xiaoyang Wu, Chengyao Wang,
Shu Liu, Jingyong Su, and Jiaya Jia. Hierarchical dense
correlation distillation for few-shot segmentation. In CVPR ,
2023. 3
[36] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
InMICCAI , 2015. 1
[37] Kuniaki Saito, Donghyun Kim, Piotr Teterwak, Stan
Sclaroff, Trevor Darrell, and Kate Saenko. Tune it the right
way: Unsupervised validation of domain adaptation via soft
neighborhood density. In ICCV , 2021. 1
[38] Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Sid-
dhartha Chaudhuri, Preethi Jyothi, and Sunita Sarawagi.
Generalizing across domains via cross-gradient training.
ICLR , 2018. 3
[39] Xinyu Shi, Dong Wei, Yu Zhang, Donghuan Lu, Munan
Ning, Jiashun Chen, Kai Ma, and Yefeng Zheng. Dense
cross-query-and-support attention weighted mask aggrega-
tion for few-shot segmentation. In ECCV , 2022. 3
[40] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical
networks for few-shot learning. NeurIPS , 2017. 2
[41] Zhuotao Tian, Hengshuang Zhao, Michelle Shu, Zhicheng
Yang, Ruiyu Li, and Jiaya Jia. Prior guided feature enrich-
ment network for few-shot segmentation. TPAMI , 2020. 1,
2, 3, 6
[42] Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The
ham10000 dataset, a large collection of multi-source der-
matoscopic images of common pigmented skin lesions. Sci-
entific data , 2018. 5
[43] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan
Wierstra, et al. Matching networks for one shot learning.
NeurIPS , 2016. 3
[44] Riccardo V olpi, Hongseok Namkoong, Ozan Sener, John C
Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to
unseen domains via adversarial data augmentation. NeurIPS ,
2018. 3
[45] Haohan Wang, Zexue He, Zachary C Lipton, and Eric P
Xing. Learning robust representations by projecting superfi-
cial statistics out. In ICLR , 2019. 3
[46] Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou,
and Jiashi Feng. Panet: Few-shot image semantic segmenta-
tion with prototype alignment. In ICCV , 2019. 2, 6
[47] Wenjian Wang, Lijuan Duan, Yuxi Wang, Qing En, Jun-
song Fan, and Zhaoxiang Zhang. Remember the differ-
ence: Cross-domain few-shot semantic segmentation via
meta-memory transfer. In CVPR , 2022. 1, 3, 6
[48] Boyu Yang, Chang Liu, Bohao Li, Jianbin Jiao, and Qixiang
Ye. Prototype mixture models for few-shot semantic seg-
mentation. In ECCV , 2020. 6[49] Lihe Yang, Wei Zhuo, Lei Qi, Yinghuan Shi, and Yang Gao.
Mining latent classes for few-shot segmentation. In ICCV ,
2021. 2
[50] Xiangyu Yue, Zangwei Zheng, Shanghang Zhang, Yang
Gao, Trevor Darrell, Kurt Keutzer, and Alberto Sangiovanni
Vincentelli. Prototypical cross-domain self-supervised learn-
ing for few-shot unsupervised domain adaptation. In CVPR ,
2021. 1
[51] Bingfeng Zhang, Jimin Xiao, and Terry Qin. Self-guided and
cross-guided learning for few-shot segmentation. In CVPR ,
2021. 1
[52] Chi Zhang, Guosheng Lin, Fayao Liu, Jiushuang Guo,
Qingyao Wu, and Rui Yao. Pyramid graph networks with
connection attentions for region-based one-shot semantic
segmentation. In ICCV , 2019. 3, 6
[53] Chi Zhang, Guosheng Lin, Fayao Liu, Rui Yao, and Chunhua
Shen. Canet: Class-agnostic segmentation networks with it-
erative refinement and attentive few-shot learning. In CVPR ,
2019. 1, 2, 6
[54] Gengwei Zhang, Guoliang Kang, Yi Yang, and Yunchao
Wei. Few-shot segmentation via cycle-consistent trans-
former. NeurIPS , 2021. 1, 3
[55] Jian-Wei Zhang, Yifan Sun, Yi Yang, and Wei Chen. Feature-
proxy transformer for few-shot segmentation. NeurIPS ,
2022. 8
[56] Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junt-
ing Pan, Hao Dong, Peng Gao, and Hongsheng Li. Person-
alize segment anything model with one shot. arXiv preprint
arXiv:2305.03048 , 2023. 8
[57] Xiaolin Zhang, Yunchao Wei, Yi Yang, and Thomas S
Huang. Sg-one: Similarity guidance network for one-shot
semantic segmentation. Transactions on Cybernetics , 2020.
2
[58] Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao
Xiang. Learning to generate novel domains for domain gen-
eralization. In ECCV , 2020. 3
[59] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do-
main generalization with mixstyle. In ICLR , 2021. 3, 8
24045
