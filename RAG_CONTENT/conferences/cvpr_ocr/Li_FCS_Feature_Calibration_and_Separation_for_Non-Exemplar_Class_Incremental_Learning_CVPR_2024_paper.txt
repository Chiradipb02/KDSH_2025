FCS: Feature Calibration and Separation for Non-Exemplar Class Incremental
Learning
Qiwei Li1, 2, Y uxin Peng1, Jiahuan Zhou1*
1Wangxuan Institute of Computer Technology, Peking University, Beijing 100871, China
2School of Electronics Engineering and Computer Science, Peking University
{lqw, pengyuxin, jiahuanzhou }@pku.edu.cn
Abstract
Non-Exemplar Class Incremental Learning (NECIL) in-
volves learning a classiﬁcation model on a sequence ofdata without access to exemplars from previously encoun-tered old classes. Such a stringent constraint always leadsto catastrophic forgetting of the learned knowledge. Cur-rently, existing methods either employ knowledge distilla-tion techniques or preserved class prototypes to sustainprior knowledge. However , two critical issues still per-sist. On the one hand, as the model is continually updated,the preserved prototypes of old classes will inevitably de-rive from the suitable location in the feature space of thenew model. On the other hand, due to the lack of exem-plars, the features of new classes will take the place ofsimilar old classes which breaks the classiﬁcation bound-ary. T o address these challenges, we propose a FeatureCalibration and Separation (FCS) method for NECIL. Ourapproach comprises a Feature Calibration Network (FCN)that adapts prototypes of old classes to the new modelvia optimal transport learning, approximating the drift ofprototypes caused by model evolution. Additionally, wealso propose a Prototype-Involved Contrastive Loss (PIC)that enhances feature separation among different classes.Speciﬁcally, to mitigate the boundary distortion arisingfrom the interplay of classes from different learning stages,prototypes are involved in pushing the feature of new classesaway from the old classes. Extensive experiments on threedatasets with different settings have demonstrated the su-periority of our FCS method against the state-of-the-artclass incremental learning approaches. Code is availableathttps://github.com/zhoujiahuan1991/CVPR2024-FCS .
1. Introduction
As a milestone research task in computer vision, image
classiﬁcation has consistently attracted substantial attention
*Corresponding authorFeature of old classextracted by old model
Feature of old classextracted by new modelMaintained prototypesClassification boundary Feature of new classextracted by new model
Calibrated prototypesFeature Calibration
Feature Separation
࢚ࢌି૚
Unconstrained model 
changeLack of old 
featureMitigate by retaining 
prototypes
Feature space 
overlapMitigate by 
contrastive learningMitigate by 
knowledge distillation
Mitigate by
prototype calibrationInconsistent between 
evolving model 
and fixed prototypes Model for stage ࢚െ1
࢚ࢍି૚
Classification head࢕࢚ࢍ
࢔࢚ࢍModel for stage ࢚
࢚ࢌ
Four aspects of catastroph ic forgetting in NECIL
(1) Change of  feature extractor ሺ࢚ࢌି૚ǡ࢚ࢌሻ (2) Change of classification head ሺ࢚ࢍି૚ǡ࢕࢚ࢍሻ
(3) Sub-optimal interaction between ࢚ࢌand ࢕࢚ࢍ( 4) Intersection between ࢕࢚ࢍand ࢔࢚ࢍParadigm of non-exemplar cla ss incremental learning (NECIL)
Figure 1. Four main aspects of forgetting in NECIL. Existing
methods mostly focus on retaining knowledge by knowledge dis-tillation and prototypes. However, the sub-optimal interaction be-tween the feature extractor and classiﬁcation head as well as theintersection between classiﬁcation heads may also cause catas-trophic forgetting. So we propose a prototype calibration networkand a prototype-involved contrastive loss to handle this issue.
over time [ 6,7,20]. Conventional deep learning-based
models are designed to learn from static data [ 12,18],
assuming that the entire training data of all classes areavailable at once. When dealing with dynamic and evolv-ing data streams, the performance of previously learned
classes severely degrades, leading to a phenomenon known
ascatastrophic forgetting [8]. To handle this issue, inspired
by the natural way that humans continually acquire knowl-edge throughout their lives, Incremental Learning (IL) [ 40]
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
28495
has been investigated recently. A popular solution to IL
aims to retain historical exemplars to replay past knowledgewhen training on current data [ 1,3,11]. However, they not
only raise critical concerns about data privacy but also resultin substantial storage and training consumption. Therefore,a more practical but challenging IL scenario where no pre-vious exemplars can be accessed is considered in this pa-per, named as Non-Exemplar Class Incremental Learning
(NECIL ). In this setting, the issue of catastrophic forgetting
becomes more severe due to the absence of explicit priorknowledge.
Existing NECIL methods [ 21,42,43] predominantly
rely on knowledge distillation to transfer the knowledgefrom old models to new ones (Fig. 1(1)) or memorize a
set of prototypes of previously learned classes for knowl-edge preservation (Fig. 1(2)). Although the above efforts
can mitigate catastrophic forgetting to some extent, the per-formance is limited by two crucial challenges. As illustratedin Fig. 1(3), with the incremental learning of new classes,
the preserved prototypes of old classes will inevitably driftin the feature space of the new model and can no longer ac-curately represent the discriminative characteristics of thoseold classes. Though the few existing works [ 38] propose to
estimate the feature changes across different IL stages, theystill omit the changes within one IL stage. Moreover, with-out the guidance of historical exemplars from old classes,the features of new classes will occupy and squeeze thespace akin to old classes (Fig. 1(4)). Consequently, the
overlap between old and new classes disrupts the classiﬁ-cation boundary and results in knowledge forgetting. Asshown in Fig. 2, there is a noticeable accuracy decline when
simultaneously employing classiﬁcation heads for both oldand new classes, in comparison to using them individually.
To address the aforementioned challenges, we propose a
novel NECIL method focusing on Feature Calibration andSeparation (FCS) during IL stages. The designed FCS con-
sists of a Feature Calibration Network (FCN) that adapts
historical prototypes to the appropriate locations in the fea-ture space of the new model, and a Prototype-Involved Con-trastive loss (PIC) that separates the features of old and newclasses to handle the deterioration caused by feature over-lap. Speciﬁcally, motivated by the well-known Optimal
Transport theory [ 25], our proposed FCN plays an impor-
tant role in bridging the feature spaces of the old and newmodels. By treating the features of new data extracted byold and new models as source and target distributions, atransport plan is learned by minimizing the transport cost ofaligning features from source to target distribution. There-fore, FCN leverages such a transport plan to calibrate thelocation of historical prototypes in the feature space of thenew model, as well as alleviate the drift issue. Addition-ally, PIC is designed to tackle the distortion of classiﬁca-tion boundaries caused by feature overlap. Different from
Classify old classes by ݃௢Classify new classes by ݃௡Classify all classes by ݃௢and ݃௡
New class Old class 
Accuracy of old classesTop-1 Acc(%)
Number of tasks Number of tasksOnly ݃௢
݃௢and ݃௡
Accuracy of new classesTop-1 Acc(%) Only ݃௡
݃௢and ݃௡
Number of tasks
Figure 2. A veriﬁcation experiment on CIFAR-100 shows the
intersection of classiﬁcation boundaries of classes from differentstages leads to a serve decrease in performance. g
oandgnare
classiﬁcation heads related to old and new classes.
existing methods [ 41,42] that prototypes are just used to
train the classiﬁcation heads, we treat the calibrated pro-totypes, via our FCN, as negative references to push newclasses away from old ones. Moreover, the proposed PICloss can also discriminatively separate features among new
classes to further improve the IL performance of the new
model.
In summary, the main contributions of this paper are
three-fold: (1) A novel Feature Calibration Network is pro-posed to appropriately adapt historical prototypes to the fea-ture space of the new model, mitigating catastrophic forget-ting issues caused by feature drift. (2) A Prototype-InvolvedContrastive loss is introduced to further alleviate the forget-ting caused by the feature overlap across different IL stages.(3) Extensive experiments on various benchmarks have ver-iﬁed the superiority of our method against the state-of-the-art approaches in different settings.
2. Related Work
2.1. Class Incremental Learning
Existing CIL methods can be mainly categorized into threegroups: rehearsal-based, regularization-based, and networkarchitecture-based. Rehearsal-based methods [ 22,23,26,
27] focused on retaining representative data from previous
stages and adopting knowledge distillation to extract andtransfer knowledge acquired from previous stages to thecurrent model. Regularization-based approaches [ 16,30–
32] were designed to stabilize model parameters by con-
trolling feature adjustments, thus alleviating the tendency toforget. Network architecture-based models [ 14,33,34,39]
dynamically adjusted the network structures or design spe-ciﬁc parameters for different stages to adapt to the evolv-ing data stream. Despite the substantial advancements
achieved by the aforementioned methods, rehearsal-basedmethods and most of the regularization-based and network
28496
࣮ࣦ
ࣦ௖௢ࣦ௣௥௢ି࣮
ࣦ௖௘ࣦ௞ௗ
Image of Stage t
Augmented Image 
of Stage tWithout Grad
With Grad
Prototypes ષ࢚ି૚Feature Calibration Network Feature Space of New Model
࢚ࢌ
ClassifierPrototype Calibration (a) Training on Stage t (b) Prot otype Calibration after TrainingPrototype-Involved 
Contrastive LossPrototypes ષ࢚ି૚Prototypes ષ࢚
Old Model
New ModelNew Model
Figure 3. The overall pipeline of our proposed FCS model. (a) In the t-th IL stage, a feature calibration network is learned to transfer
preserved prototypes Ωt−1to the feature space of the new model, and a prototype-involved contrastive loss is introduced to separate
features from different classes. (b) After the training stage, the calibrated prototypes of previous classes Ωt−1, and calculated prototypes
of new classes form the new prototypes set Ωt.
architecture-based methods require data storage, potentially
raising concerns about data privacy.
2.2. Non-Exemplar Class Incremental Learning
Recently, Non-Exemplar Class Incremental Learning(NECIL) characterizes a particularly challenging scenariowhere exemplars from previous classes are unavailable, Theabsence of previous data further exacerbates the issue of
catastrophic forgetting. V arious NECIL methods have been
proposed to address this issue. [ 9,10,21] introduced a
knowledge distillation loss between the outputs of modelsat different IL stages to resist forgetting. [ 5,37] aimed
to train a generator to replay old knowledge by generat-ing exemplars, but their performance highly relied on thehigh quality of generated data and the sequentially updatedgenerator also faced the problem of catastrophic forgetting.From the perspective of model parameters, [ 24,43] tackled
forgetting by freezing parts of the model parameters to re-duce the impact of knowledge updating from different ILstages. Though these works mitigate forgetting effectively,
their ability to acquire new knowledge is severely limited.
Recent works [ 35,36] adopt only a small number of pa-
rameters to prompt the model but highly rely on large-scalepre-trained models. [ 41,42] propose label and prototype
augmentation to effectively retain past knowledge, however,the preserved prototypes will inevitably drift in the feature
space of the new model, leading to knowledge forgetting.Although [ 38] tries to adapt the prototypes by interpolat-
ing the feature drift of new data extracted by old and newmodels after each IL stage, it simply omits the fact that thefeature space of the new model is continuously changingwithin one IL stage. As a result, their ability to handle catas-trophic forgetting is still limited. Moreover, the above ap-proaches overlook the overlap between features of classesfrom different stages which also causes knowledge forget-ting according to our observation.3. Problem Formulation and Analysis
3.1. Problem Formulation
In the task of NECIL, a data stream consists of Tstages
denoted as D={Dt}T
t=1 come in sequence to incre-
mentally train the model. Each dataset Dt={Xt,Yt}
consists of input data set Xt={xt,j}nt
j=1and label set
Yt={yt,j∈C t}nt
j=1, wherentis the number of data in
staget,xt,jrepresents the j-th image and Ctis the label
set. To be noticed, labels of different stages are disjoint,
that isCi∩C j=∅(i/negationslash=j).In staget, the model consists
of a feature extractor ft:Rh×w×3→Rdand a classiﬁca-
tion head gt:Rd→Rlt, wheredis the feature dimension
andlt=/summationtextt
j=1|Cj|is the number of classes that have been
learned. The predicted label of input image xcan be ob-
tained by argmaxgt◦ft(x).
3.2. Forgetting Analysis
In this section, we ﬁrst analyze the potential reasons for for-
getting in NECIL and clarify our motivation. The model ofstagetcan be represented as θ
t=gt◦ft=[gn
t,go
t]◦ft,
wheregn
t:Rd→R|Ct|andgo
t:Rd→Rct−1are clas-
siﬁcation head of new and old classes respectively. Mean-
while, the learned model of stage t−1is represented as
θt−1=gt−1◦ft−1. As shown in Fig. 1, during stage t,w e
observed that the catastrophic forgetting could be caused bythe following four aspects:
Change of Feature Extractor (f
t−1,ft)in Fig. 1(1):
The feature extractor ft, acquired during the stage twill be
inevitably different from its predecessor ft−1. Furthermore,
the lack of historical data can exacerbate this phenomenon,possibly rendering f
tunsuitable for data encountered in
earlier stages. To tackle this problem, existing methodsoften employ various knowledge distillation losses to pre-serve knowledge from the previous model. For example,PASS [ 42] seeks to ameliorate this by minimizing the Eu-
28497
clidean distance between features extracted by the old and
new model:
Lkd=/bardblft(x)−ft−1(x)/bardbl2. (1)
Change of Classiﬁcation Head (gt−1,go
t)in Fig. 1(2):
Similar to the change of feature extractor, the classiﬁcation
head for old classes gt−1, will inevitably face disruptions
due to the absence of data from preceding classes. To ad-dress this, recent methods propose to maintain a small num-ber of prototypes of previous classes Ω
t−1during training.
Speciﬁcally, the prototypes are augmented and then used totrain the classiﬁcation head to maintain old knowledge.
L
pro=Lce/parenleftbig
gt(Aug(Ω t−1)),Y/prime
t−1/parenrightbig
, (2)
whereAug denotes the prototype augmentation, Ωt−1de-
notes the prototype of previous t−1tasks,Y/prime
t−1denotes the
class labels of prototypes and Lceis cross-entropy loss.
While previous research has primarily concentrated on
the ﬁrst two aspects, we identify two additional factors withthe potential to cause severe forgetting:
Sub-optimal Interaction between f
tandgo
tin
Fig. 1(3): Though various knowledge distillation losses are
proposed to mitigate the change of feature extractor, the fea-ture space of new model f
twill inevitably diverge from the
old oneft−1. Therefore, the prototypes of the old classes
maintained by the new model will drift from those of the oldmodel. This mismatch can disrupt the prototypes’ ability toaccurately represent the old classes, consequently impairingthe capacity of classiﬁcation head g
o
t. To address this con-
cern, we introduce a Feature Calibration Network (FCN)that transports prototypes to the feature space of the newmodel, which alleviates the feature drift that arises due tomodel transitions.
Intersection between g
o
tandgn
tin Fig. 1(4): Since only
Dtcan be accessed in stage t, the feature of current data in
Dtmight take the place of similar historical data from D1
toDt−1. This feature overlap across distinct training stages
introduces a potential for classiﬁcation boundary breaches,
subsequently leading to performance deterioration. We of-
fer a tangible demonstration of this issue in Fig. 2.W e
can observe a clear reduction in accuracy when employingclassiﬁcation heads for both old and new classes concur-rently, as opposed to using them individually. This conspic-uous decrease shows the pronounced impact of classiﬁca-tion boundary intersection between classes from differentstages. To effectively tackle this challenge, we introduce aPrototype-Involved Contrastive loss (PIC) which separatesprototypes of old classes and features of new classes to re-duce the mutual inﬂuence of classiﬁcation boundaries.4. The Proposed Method
4.1. Feature Calibration Network (FCN)
As mentioned above, we demonstrate that directly utilizingprototypes extracted by the old model in the feature spaceof the new model leads to sub-optimal performance. Denotethe feature space of old and new model as F
t−1andFt, and
the probability distribution of the feature as P∈P(Ft−1),
Q∈P(Ft). Our goal is obtaining a transport plan, T, that
maps the distribution PtoQwith the lowest error, which
is also called the problem of optimal transport [ 25]. The
Monge’s formulation of optimal transport can be formed as
Cost(Ft−1,Ft)= i n f
TP=Q/integraldisplay
Ftc(x,T(x))dP(x), (3)
whereT:Ft−1→F tis the transport plan that transports
the feature of source space to target space and c(x,T(x))
is the cost of transporting xtoT(x). During a training
step of the IL stage t, the model is fed with a batch of data
{Xt,Yt}={xj,yj}nb
j=1sampled from Dtwith batch size
nb. We can get the feature of xj, extracted by the old model
and the new model as ft−1(xj)andft(xj). Then the Eq. ( 3)
can be approximated in a discrete form:
Cost(Ft−1,Ft)= i n f
TP=Q1
nbnb/summationdisplay
j=1c/parenleftBig
ft−1(xj),T/parenleftbig
ft−1(xj)/parenrightbig/parenrightBig
.
(4)
For the cost function c, the feature of a certain xjextracted
byft−1should be mapped to the related feature ft(xj),s o
we set the cost function as follows:
c/parenleftBig
ft−1(xj),T/parenleftbig
ft−1(xj)/parenrightbig/parenrightBig
=/bardblT(ft−1(xj))−ft(xj)/bardbl2.
(5)
In contrast to previous methods that solve the optimal trans-
port problem between two ﬁxed sets of samples, our ap-proach implements the transport plan using a neural net-work. This network is optimized through a loss functionthat minimizes the cost, Cost(F
t−1,Ft), during the train-
ing process.
LT=C o s t (Ft−1,Ft). (6)
Then the learned transport plan, T, serves as the feature
calibration network that transfers prototypes to the featurespace of the new model (Fig. 3).
During the incremental training stage t, we have the pro-
totypes of previous classes Ω
t−1, these prototypes are trans-
ferred to the feature space of the new model before trainingthe classiﬁcation head. So compared to the previous classi-ﬁcation loss of prototypes in Eq. ( 2), our loss function is:
L
pro−T=Lce/parenleftBig
gt/parenleftbig
T(Aug(Ω t−1))/parenrightbig
,Y/prime
t−1/parenrightBig
. (7)
After the training of IL stage t, prototypes of new classes in
staget,ωt, can be maintained as the mean of feature in each
28498
class. Then the prototypes of the previous tstages isΩt=
T(Ωt−1)∪ωt, which is consist of calibrated prototypes,
T(Ωt−1), and prototypes of new classes, ωt.
4.2. Prototype-Involved Contrastive loss (PIC)
Another aspect of knowledge forgetting is the overlap of
similar classes across distinct IL stages which will disruptthe established classiﬁcation boundaries, leading to a de-cline in performance. To address this challenge, we intro-duce a PIC (Fig. 3) that mitigates the feature overlap from
two aspects: separating new classes to leave more roomfor future updating and pushing new classes away from old
classes. Firstly, inspired by contrastive learning [ 13] which
is effective in clustering similar features, we adopt a super-vised contrastive loss [ 15] to compress the features of each
class, thus allowing for greater ﬂexibility in accommodat-ing future classes. To simplify notation, the training stage
tis omitted in this section. Given a batch of data with in-
dexI, we augment each data xand get a query view x
qand
a key view xk, then the supervised contrastive loss can be
expressed as :
Lco=/summationdisplay
i∈I−1
|S(i)|/summationdisplay
p∈S(i)logexp(zq
i·zk
p/τ)/summationtext
a∈Iexp(zq
i·zka/τ),(8)
whereS(i)is the set of index that have the same class label
as image xi,zq
i=f(xq
i)andzk
i=f(xk
i)mean the fea-
ture of the query view and key view of data xiextracted by
feature extractor f,τis a scalar temperature parameter.
Secondly, after the initial training stage, we have the
maintained prototypes which partly represent the featuresof previous classes. To fully utilize the knowledge con-tained in prototypes, prototypes are treated as the featurewith different classes from training samples, then the super-
vised contrastive loss is:
L
co=/summationdisplay
i∈I−1
|S(i)|/summationdisplay
p∈S(i)logexp(zq
i·zk
p/τ)/summationtext
a∈I∪IΩexp(zq
i·zka/τ),
(9)
whereIΩis the index set of prototypes.
By leveraging prototype-involved contrastive loss, in-
stances of the same classes are pulled closer together. Si-multaneously, instances are pushed apart not only from dis-similar classes but also from prototypes of previous classes.This approach allows the model to leave more room for fu-ture classes and separate the features of different classes,mitigating the forgetting induced by the intersection of clas-siﬁcation boundaries.
4.3. Overall Optimization
For the optimization of our method, a classical cross-entropy loss L
ceis ﬁrst used for backbone training. As dis-
cussed above, our analysis sheds light on four distinct facetsof forgetting, leading to adopting different losses to address
them individually. We explore the wildly recognized knowl-edge distillation loss L
kd(Eq. ( 1)) and the prototype clas-
siﬁcation loss Lpro (Eq. ( 2)) as existing methods do [ 42].
Then, the proposed calibration network learning loss LT
(Eq. ( 6)) is used to learn the FCN that can transfer proto-
types of old classes to the feature space of the new model.Building upon this transformation, we replace the proto-type inL
pro with the calibrated ones and get our prototype
classiﬁcation loss Lpro−T (Eq. ( 7)). Finally, a prototype-
involved contrastive loss Lco(Eq. ( 9)) is adopted to mitigate
the feature overlap issue. The overall optimization loss canbe represented as:
L=L
ce+αLkd+βLpro−T+γLT+λLco, (10)
whereα,β,γandλare the weighting parameters that bal-
ance different components.
5. Experiments
5.1. Experiment Settings
5.1.1 Datasets
We conduct evaluations of our proposed FCS model on
three public datasets, CIFAR-100 [ 17], TinyImageNet [ 19],
and ImageNet-Subset [ 29]. CIFAR-100 comprises 100
classes, with 500 train images and 100 test images for each
class. TinyImageNet comprises 200 classes, with 500 trainimages and 50 test images for each class. ImageNet-Subset
is a subset of ImageNet containing 100 classes, with 1300train images and 50 test images for each class. We followthe conventional NECIL setting [ 42] to build incremental
settings. Speciﬁcally, for CIFAR-100, the model is trained
on 50, 50, and 40 classes and subsequently trained for 5, 10,
and 20 IL stages. For TinyImageNet, the model is trainedon 100 classes and subsequently trained for 5, 10, and 20IL stages. For ImageNet-Subset, the model is trained on 50classes and subsequently trained for 10 IL stages.
5.1.2 Comparison Methods
Our FCS method is compared with various state-of-the-
art NECIL methods including LwF [ 21], PASS [ 42],
IL2A [ 41], SSRE [ 43], R-DFCIL [ 9], EDG [ 10], and
FeTrIL [ 24]. Furthermore, we also compare with two
exemplar-based CIL methods, iCaRL [ 28] and EEIL [ 2],
and the memorize size is set to 20 per class. Moreover, twospecial experimental settings, Joint-Train and Fine-Tuneare also included. Joint-Train means all data are used fortraining at once serving as the upper bound result. Fine-Tune means directly ﬁne-tuning the model without any anti-forgetting algorithms.
28499
MethodsCIFAR-100 TinyImageNet ImageNet-Subset
5 stages 10 stages 20 stages 5 stages 10 stages 20 stages 10 stages
Joint-Train 77.31 77.31 77.31 54.17 54.17 54.17 80.36
Fine-Tune 9.01 4.76 3.28 7.09 3.67 2.04 4.64
LwF [ 21] 24.01 16.52 14.66 14.73 7.60 3.11 13.70
iCaRL-CNN [ 28] 47.79 42.15 40.10 24.78 20.02 15.15 39.54
iCaRL-NME [ 28] 54.96 48.51 46.14 30.47 25.56 18.48 46.90
EEIL [ 2] 50.21 47.60 42.23 35.00 33.67 27.64 -
PASS [ 42] 56.40 50.69 46.93 42.52 40.27 34.80 54.50
IL2A [ 41] 53.93 45.76 44.24 39.53 36.55 30.02 -
R-DFCIL ‡[9] 54.79 50.00 37.02 40.78 37.89 31.99 52.92
SSRE‡[43] 56.97 56.57 51.92 41.45 41.18 41.03 59.32
EDG‡[10] 56.03 54.31 49.32 38.10 37.99 34.85 -
FeTrIL [ 24] 58.12 57.64 52.48 42.92 42.41 41.33 61.22
FCS (Ours) 62.13 60.39 58.36 46.04 44.95 42.57 61.76
Table 1. Comparison of top-1 accuracy with different incremental learning methods on various dataset settings. ‡represents results
reported by their original paper.
MethodsCIFAR-100
5 stages 10 stages 20 stages
LwF [ 21] 50.80 55.00 57.95
iCaRL-CNN [ 28] 42.80 46.50 51.00
iCaRL-NME [ 28] 27.80 31.90 28.80
EEIL [ 2] 23.36 26.65 32.40
PASS [ 42] 19.64 26.61 27.80
IL2A [ 41] 28.54 39.29 41.27
SSRE [ 43] 18.37 19.48 19.00
EDG [ 10] 21.93 23.76 24.71
FeTrIL [ 24] 17.20 18.80 23.40
FCS (Ours) 12.20 16.70 15.90
Table 2. Results of average forgetting (lower is better).
5.1.3 Evaluation Metrics
Following previous works [ 42], we use Accuracy and Av-
erage F orgetting [4] for evaluation. Accuracy is the av-
erage accuracy of all the classes that have already beenlearned. Average forgetting calculates the average per-formance degradation of different tasks during incremen-
tal learning, which can estimate the forgetting of previous
tasks.
5.1.4 Implementation Details
We use the widely adopted ResNet-18 as our backbone [ 12]
and train it from scratch. The parameters are optimizedby an Adam optimizer with an initial learning rate of 1e-3 and weight decay of 2e-4. The model is trained for 100epochs and the learning rate is decayed by 0.1 after every45 epochs. We set the batch size to 64 and the input is aug-mented following [ 41,42]. The feature calibration networkis implemented with a linear layer which is initialized with
an identity matrix and zero bias. We set the weighting pa-rameters of different losses as α=1 0,β=1 0,γ=1
andλ=0.1for the setting with 5,10 incremental stages,
λ=0.03for 20 incremental learning stages and ImageNet-
Subset dataset. All experiments are implemented with Py-Torch on a single NVIDIA 4090 GPU.
5.2. Comparison with SOTA
Main Results. Tab. 1shows the results of ﬁnal accuracy.
Across various scenarios, our approach signiﬁcantly outper-forms both previous non-exemplar methods and classicalexemplar-based methods. We achieve performance gainsof 4.01%, 2.75%, 4.69% on CIFAR-100, 3.12%, 2.54%,1.24% on TinyImageNet, and 0.54% on ImageNet-Subset.It is worth noting that methods using knowledge distillationprototypes (e.g., PASS, IL2A) experience substantial accu-racy degradation, 9.47%, 9.69% on CIFAR-100 and 7.72%,
9.51% on TinyImageNet, as the number of stages increases
from 5 to 20. In contrast, our results demonstrate a com-paratively mild performance reduction of 4.96% and 3.47%respectively. This resilience is attributed to the adaptabil-ity of our calibrated prototypes to evolving models andthe efﬁcacy of our prototype-involved contrastive loss inmitigating feature overlap. Notably, on ImageNet-Subset,our method only outperforms the frozen backbone method(FeTrIL) by 0.54%. This is because the FeTrIL freezesthe backbone, thereby effectively preserving the knowledgeof feature extractors from forgetting when applied to largedatasets. However, the knowledge acquisition capabilityof FeTrIL is highly restricted, leading to inferior resultson CIFAR-100 and TinyImageNet. Additionally, we alsoprovide the results of average forgetting on CIFAR-100 inTab. 2. It can be observed that the average forgetting of
28500
Figure 4. Complete classiﬁcation accuracy of each stage on CIFAR-100.
our method is the lowest, demonstrating the superior anti-
forgetting capability of our method.
Accuracy Curve. To present our results in detail, we
show the accuracy of our method on the CIFAR-100 datasetin Fig. 4. Notably, with similar accuracy for the initial
stage, our method achieves the best results across subse-quent stages. This observation underscores that our methodstrikes a better balance between knowledge forgetting andacquisition.
Confusion Matrix. In Fig. 5, we present the confusion
matrix of different method on CIFAR-100. Our method sur-passes existing methods in correctly predicting classes from
the early stages (the upper left of the matrix). This is be-cause the prototypes calibrated by FCN can better repre-
sent the features of old classes in the feature space of thenew model thus retaining more knowledge. Furthermore,the PIC serves to mitigate the interference between old andnew classes, also contributing to this improvement.
5.3. Ablation Study
Results Analysis. To elucidate the effectiveness of FCS, we
conduct extensive experiments on the CIFAR-100 dataset.Our method comprises two components: the feature cali-
bration network and the prototype-involved contrastive loss.
The results presented in Tab. 3substantiate the following
observations: (1) The baseline of our method achieves com-parable results with the SOTA methods, showcasing the po-tential of augmenting model training with the techniquesproposed by [ 41,42]. This conﬂuence of strategies en-
hances the learning of more generalized features, leading toan overall performance improvement. (2) The incorporationof the FCN improves the results of baseline with a margin
of (1.26%, 2.40%, 3.83%). This gain can be attributed tothe FCN which learns the transfer function between the fea-ture spaces of the old and new model. The calibrated pro-
totypes can better represent the feature of historical data inthe feature space of the new model, thus maintaining more
knowledge to resist forgetting. Notably, the improvementbrought by the FCN increases as the stages increase from 5MethodCIFAR-100
5 stages 10 stages 20 stages
Base 60.51 57.33 54.48
Base+FCN 61.77 59.73 58.31
Base+FCN+PIC 62.13 60.39 58.36
Table 3. Ablation study of different components.
to 20. This phenomenon is attributed to the accumulation ofmodel changes throughout the learning process, where ourmethod can effectively mitigate this problem, achieving bet-
ter results. (3) Remarkably, using the FCN and PIC togetherachieves the best results. This combined approach achieves
an improvement of (0.36%, 0.66%, 0.05%) over the exclu-sive utilization of FCN. This gain can be attributed to PIC’sability to separate the features of similar classes from dif-ferent stages and reduce damage to classiﬁcation bound-
aries. Simultaneously, FCN also contributes by endowing
the model with more appropriate and adaptable prototypes.
Ablation Study of FCN. In Tab. 4, we show the results
of employing different architectures for the feature calibra-tion network (FCN). We implement FCN with three dif-ferent networks. Speciﬁcally, [512,512] represents a linear
layer with an input dimension of 512 and an output dimen-sion of 512. [512,D,512] represents two linear layers with
the input dimension of 512, Dand output dimension of D,
512 respectively.
Results show that a single linear layer achieves the best
performance. This can be attributed to the linear layer’s ca-pability to effectively capture feature drift between modelswhile being relatively easier to learn. The use of a singlelinear layer also ensures the preservation of linear separableproperties, which facilitates the learning of linear classiﬁ-cation. Consequently, we choose this single layer as thearchitecture for our FCN.
Effectiveness of FCN. To further clarify the efﬁcacy of
FCN, we visualize the average Euclidean distance betweenthe maintained prototypes and the appropriate prototypes
28501
Figure 5. The confusion matrix of different methods on CIFAR-100.
Figure 6. Average Euclidean distance between the maintained pro-
totypes and the appropriate prototypes (extracted by new model)
at each stage on CIFAR-100 and TinyImageNet datasets.
FCNCIFAR-100
5 stages 10 stages 20 stages
[512,1024,512] 61.97 60.06 56.06
[512,512,512] 60.77 59.12 56.57
[512,512] 62.13 60.39 58.36
Table 4. Ablation study of FCN architecture.
(extracted by the new model) at each stage in the setting
of 20 stages on CIFAR-100 and TinyImageNet in Fig. 6.
We can observe that the distance of our methods is lower
than the baseline method. This phenomenon indicates thatFCN can effectively transfer the prototypes from the featurespace of the old model to the new model and alleviate theknowledge forgetting brought by the drift of feature space.
Effectiveness of PIC. To show the efﬁcacy of PIC, in
Fig. 7, we present a visualization of the classiﬁcation ac-
curacy for both old classes (left) and new classes (middle).Notably, the adoption of PIC improves the accuracy of oldand new classes across a spectrum of stages. This improve-ment can be attributed to PIC’s capability to separate fea-tures from different classes, thereby reducing their intersec-tion. To further analyze this ability, we show the averageperformance degradation caused by the intersection of clas-siﬁcation boundaries between classes from different stagesFigure 7. Top-1 accuracy of the old classes (left), new classes
(middle), and the accuracy degradation caused by the intersectionof classiﬁcation boundaries between classes from different stages(right).
(right). Average performance degradation is calculated as
the degradation of using the classiﬁcation head solely forold and new classes in contrast to their combined deploy-
ment (lower is better). The results show that PIC can mit-igate such degradation, demonstrating its efﬁcacy in mini-mizing the conﬂuence of classiﬁcation boundaries.
6. Conclusion
In this paper, we introduce a Feature Calibration and Sepa-ration (FCS) method to tackle the challenging non-exemplarclass incremental learning (NECIL) task. Our proposed
FCS is composed of a novel Feature Calibration Network(FCN) and a speciﬁc Prototype-Involved Contrastive Loss(PIC). In detail, motivated by the optimal transport theory,
FCN learns a transfer function between the feature spaces ofthe old and new models to calibrate the drift of the preserved
prototypes. Moreover, the PIC loss is designed to fully uti-lize the knowledge of prototypes by contrastive learning toseparate classes from different IL stages away from each
other, further enhancing the generalization capacity and dis-criminative ability of the proposed method. Extensive ex-periments on various datasets present the superiority of our
FCS method.Acknowledgment. This work was supported by the Na-
tional Natural Science Foundation of China (62376011,61925201, 62132001).
28502
References
[1] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Y oshua Ben-
gio. Gradient based sample selection for online continuallearning. Advances in neural information processing sys-
tems , 32, 2019. 2
[2] Francisco M Castro, Manuel J Mar ´ın-Jim ´enez, Nicol ´as Guil,
Cordelia Schmid, and Karteek Alahari. End-to-end incre-mental learning. In Proceedings of the European Conference
on Computer Vision , 2018. 5,6
[3] Sungmin Cha, Sungjun Cho, Dasol Hwang, Sunwon Hong,
Moontae Lee, and Taesup Moon. Rebalancing batch nor-malization for exemplar-based class-incremental learning. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition , 2023. 2
[4] Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajan-
than, and Philip HS Torr. Riemannian walk for incremen-tal learning: Understanding forgetting and intransigence. In
Proceedings of the European Conference on Computer Vi-
sion , 2018. 6
[5] Y ulai Cong, Miaoyun Zhao, Jianqiao Li, Sijia Wang, and
Lawrence Carin. Gan memory with no forgetting. Advances
in neural information processing systems , 33, 2020. 3
[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical im-
age database. In Proceedings of the European Conference
on Computer Vision , 2009. 1
[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 1
[8] Robert M French. Catastrophic forgetting in connectionist
networks. Trends in cognitive sciences , 3(4):128–135, 1999.
1
[9] Qiankun Gao, Chen Zhao, Bernard Ghanem, and Jian Zhang.
R-dfcil: Relation-guided representation learning for data-
free class incremental learning. In Proceedings of the Eu-
ropean Conference on Computer Vision , 2022. 3,5,6
[10] Zhi Gao, Chen Xu, Feng Li, Y unde Jia, Mehrtash Harandi,
and Y uwei Wu. Exploring data geometry for continual learn-ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , 2023. 3,5,6
[11] Bing Han, Feifei Zhao, Yi Zeng, Wenxuan Pan, and Guobin
Shen. Enhancing efﬁcient continual learning with dynamicstructure development of spiking neural networks. In Pro-
ceedings of the Thirty-Second International Joint Confer-ence on Artiﬁcial Intelligence, IJCAI-23 , pages 2993–3001.
International Joint Conferences on Artiﬁcial Intelligence Or-
ganization, 2023. Main Track. 2
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition , 2016. 1,6
[13] Kaiming He, Haoqi Fan, Y uxin Wu, Saining Xie, and Ross
Girshick. Momentum contrast for unsupervised visual rep-resentation learning. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition , 2020.
5
[14] Zhiyuan Hu, Y unsheng Li, Jiancheng Lyu, Dashan Gao, and
Nuno V asconcelos. Dense network expansion for class in-cremental learning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , 2023.
2
[15] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,
Y onglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, andDilip Krishnan. Supervised contrastive learning. Advances
in neural information processing systems , 33, 2020. 5
[16] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel
V eness, Guillaume Desjardins, Andrei A Rusu, KieranMilan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neu-ral networks. Proceedings of the national academy of sci-
ences , 114, 2017. 2
[17] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009. 5
[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-works. Advances in neural information processing systems ,
25, 2012. 1
[19] Y a Le and Xuan Y ang. Tiny imagenet visual recognition
challenge. CS 231N , 7(7):3, 2015. 5
[20] Y ann LeCun, Bernhard Boser, John Denker, Donnie Hen-
derson, Richard Howard, Wayne Hubbard, and LawrenceJackel. Handwritten digit recognition with a back-propagation network. Advances in neural information pro-
cessing systems , 2, 1989. 1
[21] Zhizhong Li and Derek Hoiem. Learning without forgetting.
IEEE transactions on pattern analysis and machine intelli-gence , 40, 2017. 2,3,5,6
[22] Y aoyao Liu, Bernt Schiele, and Qianru Sun. Rmm: Re-
inforced memory management for class-incremental learn-ing. Advances in neural information processing systems , 34,
2021. 2
[23] Zilin Luo, Y aoyao Liu, Bernt Schiele, and Qianru
Sun. Class-incremental exemplar compression for class-incremental learning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
11371–11380, 2023. 2
[24] Gr ´egoire Petit, Adrian Popescu, Hugo Schindler, David Pi-
card, and Bertrand Delezoide. Fetril: Feature translation forexemplar-free class-incremental learning. In Proceedings of
the IEEE/CVF Winter Conference on Applications of Com-puter Vision , 2023. 3,5,6
[25] Gabriel Peyr ´e, Marco Cuturi, et al. Computational optimal
transport: With applications to data science. F oundations
and Trends® in Machine Learning , 11, 2019. 2,4
[26] Ameya Prabhu, Philip HS Torr, and Puneet K Dokania.
Gdumb: A simple approach that questions our progress in
continual learning. In Proceedings of the European Confer-
ence on Computer Vision , 2020. 2
[27] Daiqing Qi, Handong Zhao, and Sheng Li. Better genera-
tive replay for continual federated learning. arXiv preprint
arXiv:2302.13001 , 2023. 2
28503
[28] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg
Sperl, and Christoph H Lampert. icarl: Incremental classiﬁer
and representation learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
2017. 5,6
[29] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,Aditya Khosla, Michael Bernstein, et al. Imagenet largescale visual recognition challenge. International journal of
computer vision , 115:211–252, 2015. 5
[30] James Smith, Y en-Chang Hsu, Jonathan Balloch, Yilin Shen,
Hongxia Jin, and Zsolt Kira. Always be dreaming: A new ap-proach for data-free class-incremental learning. In Proceed-
ings of the IEEE/CVF International Conference on Com-puter Vision , 2021. 2
[31] Zhicheng Sun, Y adong Mu, and Gang Hua. Regularizing
second-order inﬂuences for continual learning. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition , 2023.
[32] Marco Toldo and Mete Ozay. Bring evanescent representa-
tions to life in lifelong class incremental learning. In Pro-
ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition , 2022. 2
[33] Fu-Y un Wang, Da-Wei Zhou, Liu Liu, Han-Jia Y e, Y atao
Bian, De-Chuan Zhan, and Peilin Zhao. Beef: Bi-compatibleclass-incremental learning via energy-based expansion andfusion. In The Eleventh International Conference on Learn-
ing Representations , 2022. 2
[34] Fu-Y un Wang, Da-Wei Zhou, Han-Jia Y e, and De-Chuan
Zhan. Foster: Feature boosting and compression for class-incremental learning. In Proceedings of the European Con-
ference on Computer Vision , 2022. 2
[35] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun,
Han Zhang, Chen-Y u Lee, Xiaoqi Ren, Guolong Su, Vin-cent Perot, Jennifer Dy, et al. Dualprompt: Complementaryprompting for rehearsal-free continual learning. In Proceed-
ings of the European conference on Computer Vision , 2022.
3
[36] Zifeng Wang, Zizhao Zhang, Chen-Y u Lee, Han Zhang,
Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jen-
nifer Dy, and Tomas Pﬁster. Learning to prompt for con-tinual learning. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , 2022. 3
[37] Fei Y e and Adrian G Bors. Learning latent representations
across multiple data domains using lifelong vaegan. In Pro-
ceedings of the European Conference on Computer Vision ,
2020. 3
[38] Lu Y u, Bartlomiej Twardowski, Xialei Liu, Luis Herranz,
Kai Wang, Y ongmei Cheng, Shangling Jui, and Joost van deWeijer. Semantic drift compensation for class-incrementallearning. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , 2020. 2,3
[39] Da-Wei Zhou, Qi-Wei Wang, Han-Jia Y e, and De-
Chuan Zhan. A model or 603 exemplars: Towardsmemory-efﬁcient class-incremental learning. arXiv preprint
arXiv:2205.13218 , 2022. 2[40] Da-Wei Zhou, Qi-Wei Wang, Zhi-Hong Qi, Han-Jia Y e, De-
Chuan Zhan, and Ziwei Liu. Deep class-incremental learn-
ing: A survey. arXiv preprint arXiv:2302.03648 , 2023. 1
[41] Fei Zhu, Zhen Cheng, Xu-yao Zhang, and Cheng-lin Liu.
Class-incremental learning via dual augmentation. Advances
in neural information processing systems , 34, 2021. 2,3,5,
6,7
[42] Fei Zhu, Xu-Y ao Zhang, Chuang Wang, Fei Yin, and Cheng-
Lin Liu. Prototype augmentation and self-supervision for
incremental learning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , 2021.
2,3,5,6,7
[43] Kai Zhu, Wei Zhai, Y ang Cao, Jiebo Luo, and Zheng-
Jun Zha. Self-sustaining representation expansion for non-exemplar class-incremental learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition , 2022. 2,3,5,6
28504
