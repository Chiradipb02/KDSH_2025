TiNO-Edit: Ti mestep and N oise O ptimization
for Robust Diffusion-Based Image Edit ing
Sherry X Chen*1, Yaron Vaxman2, Elad Ben Baruch2, David Asulin2, Aviad Moreshet2,
Kuo-Chin Lien3, Misha Sra1, and Pradeep Sen1
1.University of California, Santa Barbara2.Cloudinary3.Layer AI
Abstract
Despite many attempts to leverage pre-trained text-to-
image models (T2I) like Stable Diffusion (SD) [25] for con-
trollable image editing, producing good predictable results
remains a challenge. Previous approaches have focused
on either fine-tuning pre-trained T2I models on specific
datasets to generate certain kinds of images (e.g., with a
specific object or person), or on optimizing the weights,
text prompts, and/or learning features for each input im-
age in an attempt to coax the image generator to produce
the desired result. However, these approaches all have
shortcomings and fail to produce good results in a pre-
dictable and controllable manner. To address this problem,
we present TiNO-Edit, an SD-based method that focuses on
optimizing the noise patterns and diffusion timesteps dur-
ing editing, something previously unexplored in the liter-
ature. With this simple change, we are able to generate
results that both better align with the original images and
reflect the desired result. Furthermore, we propose a set
of new loss functions that operate in the latent domain of
SD, greatly speeding up the optimization when compared
to prior losses, which operate in the pixel domain. Our
method can be easily applied to variations of SD includ-
ing Textual Inversion [13] and DreamBooth [27] that en-
code new concepts and incorporate them into the edited re-
sults. We present a host of image-editing capabilities en-
abled by our approach. Our code is publicly available at
https://github.com/SherryXTChen/TiNO-Edit.
1. Introduction
Computer-generated image synthesis has been studied
for decades for its wide range of applications includ-
ing content creation, marketing and advertising, visual-
ization/simulation, entertainment, and storytelling. Re-
*Corresponding author email: xchen774@ucsb.eduPure text-guided & Reference-guided image editing
Original Ref w/o ref w/ref
“sandwich” →“cak
e”
+“a
magenta hat”
Str
oke-guided image editing
Original User input Result Original User input Result
+“a
sunflower” +“curtains”
Image
composition
Original User input Result Original User input Result
+“a scarecrow” “white” →“yellow plaid”
Image editing with DreamBooth (DB) or Textual Inversion (TI)
Original DB Result Original TI Result
“photo” →
⟨concept⟩ “sky”→ ⟨concept⟩
Figure
1.Overview of capabilities enabled by TiNO-Edit.
TiNO-Edit offers various image-editing capabilities and can be run
with DreamBooth (DB) [27] or Textual Inversion (TI) [13]. By
leveraging diffusion timestep and noise optimization techniques,
it can generate realistic and high quality outputs.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
6337
cent advances in diffusion-based text-to-image (T2I) gen-
erations [10, 19, 23,25,30] models have allowed users to
specify, modify, and enhance images in novel ways using
text-based prompts or other inputs [35]. Still, these often
give undesired results, so the main question is how to use
these models in an artistic workflow to enable controllable
image editing that can produce the desired results.
To this end, researchers have fine-tuned existing model
backbones on new datasets to accommodate specific use
cases, including conditioning image editing on instruc-
tions [3], using images with inpainting masks [34] or
bounding boxes [17], and including other visual infor-
mation such as edges, segmentation maps, and depth
maps [35]. Another line of work has focused on opti-
mizing weights [6, 36], intermediate attention/feature lay-
ers [22, 31], or inputs [21, 33] of existing T2I backbones
with respect to each image the user wants to edit to try and
produce the desired result.
In this last thrust, optimizing text-prompt inputs has
gained the most attention. We have seen considerable work
on prompt engineering [25] which studies the effect of dif-
ferent (positive or negative) text prompts, including opti-
mizing the text prompt encoding in the CLIP domain [24]
as well as adjusting the weights of text prompts [33]. How-
ever, as shown in the examples in this paper, these previous
approaches still leave a lot to be desired when it comes to
high-quality image editing.
Despite previous work that attempts to optimize input
parameters to the diffusion model, we observe that there
are two other kinds of inputs whose optimization has not
been thoroughly explored yet: the noise used to corrupt the
input images, and the diffusion timesteps. For noisy im-
ages, most work either inverts/reconstructs them with re-
spect to the noise scheduler they use with the diffusion
backbones [21, 22,31,32] or simply distorts the original
image based on a pre-chosen distortion level [1, 8,20],
which is determined by the starting diffusion timestep. Al-
though preliminary research has been conducted to explore
how this affects the similarity between the output and the
original image [1, 8,20], these observations have yet to be
incorporated into the methods in an automated manner, so
it still requires extensive trial-and-error for users to find the
optimal settings. Furthermore, the remaining timestep val-
ues are fixed based on method hyper-parameters.
Given the important role noisy input images and
timesteps play in the diffusion-based image synthesis pro-
cess, we argue that they should also be optimized based on
the image editing objective. To this end, we present TiNO-
Edit, an optimization-based method that is built up of one
T2I backbone, namely Stable Diffusion (SD) [25], to auto-
matically find the best level of noise that should be added
and removed from the original image to achieve the best
results. We design a set of loss functions that operate inthe latent domain of SD to save computational time and
resources. More importantly, TiNO-Edit supports a line
of image editing capabilities (Fig. 1), including pure text-
guided image editing, image editing guided by reference
images, stroke-based image editing, and image composi-
tion. It can also be applied with variations of SD including
Textual Inversion [13] and DreamBooth [27] that encode
new concepts in order to generate outputs with concepts that
may otherwise be hard to describe, which is a capability that
has been largely overseen.
In summary, our contributions include:
• A novel SD algorithm for image editing that supports var-
ious image-editing capabilities.
• A new set of loss functions that save computational time
and resources.
• A novel image editing capability of incorporating new
concepts from variations of SD
2. Related Work
2.1. Text-to-image diffusion-based models
Diffusion models have shown unprecedented quality in im-
age generation tasks and become the backbone of many
text-to-image (T2I) applications [3, 21,22,31–36]. These
models use text prompts as inputs to the diffusion model
to condition the reverse denoising process. Like the gen-
eral diffusion models, T2I diffusion-based models are able
to take Gaussian noise and denoise it using text prompts to
generate images that align with those text prompts.
2.2. Text-guided diffusion-based image editing with
pre-trained models
T2I diffusion-based models have drawn a lot of attention
with their high-quality synthesis results. One line of work
takes these pre-trained models as backbones and fine-tunes
them to perform specific image editing tasks. For exam-
ple, ControlNet [35] takes a copy of the U-Net [26] encoder
component of SD [25] and attaches it back to the SD back-
bone to accept visual information including image edge
maps, depth maps, segmentation maps, etc. This attached
encoder is then fine-tuned to encode that information and
guide the generated image output accordingly. Although
ControlNet primarily focuses on T2I applications, other
work built on it supports text-guided image editing [14].
On the other hand, InstructPix2Pix [3] fine-tunes SD to
accept instructions via text prompts to perform image edit-
ing. They tailor a dataset of image pairs and corresponding
instructions that reflect changes in the image pairs, using the
T2I backbone and a large language model GPT-3 [4]. Sim-
ilarly, Paint by Example [34] is an exemplar-based image
editing method via inpainting, where they attach a CLIP-
based classification model on SD to accept image examples
that will be used to fill the masked region of an input im-
6338
age. The pre-trained SD backbone is then fine-tuned to be
conditioned on these examples.
Another approach centers on optimizing weights [6, 36],
intermediate attention/feature layers [22, 31], or inputs [21,
33] for each specific image. Similar to the previously
mentioned methods, these approaches also need a specific
dataset. The dataset may be formed from a single image,
such as its patches [36]. The model takes in the noisified
patches as well as their positions with respect to the origi-
nal image and learns to denoise those patches. During in-
ference, both models (before and after fine-tuning) are used
to denoise the input. Subsequently, the outputs from each
stage are combined through diffusion (reverse denoising)
steps to generate the final image.
Intermediate layers of pre-trained models can be used to
guide image editing processes. Existing work [11, 22, 31]
has looked into various aspects including cross and self-
attention maps, as well as activation functions and their ef-
fects on object location, shape, size, and appearance in the
generated results. Again, image inversion may be applied
with respect to the original image [31] to learn relevant in-
formation of intermediate layers, which can then be injected
in the denoising process when editing this image [31] or be
compared with the ones with respect to the edited results
and optimize the latter to be aligned with the former [22].
Finally, inputs of T2I models include text prompts, in-
put noise, and timesteps, all of which may be optimized for
more desirable editing outcomes. Image editing work that
utilizes input optimization usually starts from image inver-
sion (reconstructing images by representing them in the do-
main of the generative model), which involves finding the
reverse denoising path that will lead to the input image [21].
Among all inputs of T2I models, prompts have gained a
fair amount of attention as they have a lot of power over
the semantics of generative images and, thus are very rele-
vant to image editing results. In the context of SD, prompts
can either be positive to describe what the generated images
should contain or negative, to describe undesirable visuals
that should be excluded from the outputs (e.g. “low quality,
jpeg artifacts, ugly...”). Optimization can be performed on
both positive [33] or negative prompts [21].
Our optimization-based method falls into the above cat-
egory. In contrast to prior work, our method maintains both
positive and negative prompts while focusing on optimizing
input noise and timesteps instead to get the corresponding
edits entailed by the prompts.
3. Preliminaries
In this section, we first provide an overview of Diffusion
Denoising Implicit Model (DDIM) [30] and Stable Diffu-
sion (SD) [25] algorithms, which our method is based on,
followed by an experiment that gives us insight into the ef-
fect of various components in them on edited results.
Figure 2. Effect of starting timestep and noise on image editing.
Suppose we want to change the cat in the left image to a dog,
we can input this image and the target prompt “a photo of a dog”
to Stable Diffusion (SD) Img2Img [15, 25], along with random
Gaussian noise Nand a starting time T∈[0,1]to produce results
such as those shown in the grid on the right. Here, we vary T(fixed
per column) and N(fixed per row). As Tincreases, the output
matches the target prompt better, but it also diverges more from
the original image in terms of composition and pose. Furthermore,
different random noise inputs can lead to different visual features.
DDIMs are generative models that produce natural im-
ages from noise iteratively. During training, DDIMs model
a forward diffusion (FD) process transforming images to
noise in Stimesteps, denoted as {tk=k·1
S, k∈[0, S]},
where the corresponding noisified image Ikper timestep
from a pristine image Iis
Ik=FDk(I, N) =p
α(tk)·I+p
1−α(tk)·N, (1)
where noise N∼ N (0,I)andα(·)is a pre-defined diffu-
sion function. Starting from ˜Ik=Ik, DDIMs then model
a reverse diffusion (RD) that remove noise iteratively and
recover intermediate images as
˜Ik−1=RDk,k−1(˜Ik,˜Nk)
=p
α(tk−1) ˜Ik−p
1−α(tk)·˜Nkp
α(tk)!
+p
1−α(tk−1)·˜Nk,(2)
, where ˜Nk=UNet DDIM(˜Ik, tk)is the predicted noise from
a denoising UNet UNet DDIM [26].
The same can be applied to Stable Diffusion (SD) [25]
with two key differences. First, SD performs diffusion steps
in a latent space of a Variational Auto-Encoder (V AE) [12]
with an encoder V AE encand a decoder V AE dec. The latent
representation of IisL=V AE enc(I). Secondly, the denois-
ing UNet in SD is conditioned on an additional text prompt
p, where ˜Nk=UNet SD(˜Lk, tk, p), which enables its text-
to-image synthesis functionality.
SD Img2Img variant [15] can edit images with text.
Given an image Iand time T∈[0,1],Iis first noisi-
fied with respect to timestep max tk≤Ttkusing Eq. 1 and
then denoised. The idea behind this, first introduced in
6339
Figure 3. Optimization parameters. We find optimization pa-
rameters by studying the SD denoising process. The output ˜L0is
only affected by the timesteps tk(k∈[1, K]) and the noisy la-
tent image input ˜Lkfor each of the Kdenoising steps. Note we
are assuming that the learning models are all fixed (denoted by the
snowflake symbol) and that the number of timesteps Kis a con-
stant. ˜Lkcan then be traced back through Kiterations to the ini-
tial latent image input ˜LKthat is computed from starting timestep
tK=Tand the Gaussian noise N. Hence, we can achieve our
goal by simply optimizing Nand time steps tkfor all k∈[1, K].
SDEdit [20], is that the less we distort the input, the more
likely we are to generate a result similar to it.
However, this raises the question as to what the value
ofTshould be. To give us insight into the problem, we
can run a simple experiment, shown in Fig. 2. As we can
see, changes in TandNcan lead to drastically different re-
sults, which motivates our timestep and noise optimization
approach explained in the next section.
4. TiNO-Edit
Rather than finding the optimal values manually as in pre-
vious approaches, TiNO-Edit aims to automate the process
by optimizing the variable parameters related to NandT
so that they produce desired results. From the SD denoising
process (Fig. 3), we see that the output result ˜L0is only af-
fected by the timesteps tk(k∈[1, K]) and the input Gaus-
sian noise N, assume all models are fixed and the number
of steps Kis a constant. We optimize them, starting from
tk=k·T
KandN(0,I), with respect to our loss functions
defined in Sec. 4.2 via gradient descent with the Jacobian
from UNet SD. We optimizes all tk’s so they can be non-
uniformly spaced and is more flexible than only optimizing
Tand/or k. The updated tk’s are used in the next optimiza-
tion step as the timestep inputs to the UNet for the reverse
diffusion process. Note that we don’t optimize α(tk)be-
cause doing so along with changing tkwill break the de-
pendency between the two learnt by the models and may
cause generation quality degrade.
The inputs to our method include the input image I,
the target prompt p, the input image description/prompt
pOand additional inputs A={I∗, Ma,∗ ∈ { r, s, c}}
where a, r, s, c denote the additional image and mask in-
put to the editing capabilities of adding objects, reference-
guided image editing, stroked-guided image editing, and
image composition if applicable.
Figure 4. Training LatentCLIP. Our LatentCLIP visual encoder
(LatentCLIP vis) is a copy of a pre-trained CLIP image encoder
(CLIP vis) [24], except the first convolution layer is replaced to ac-
commodate for taking the latent vector V AE enc(I)[12] as input and
output the image feature fL. The entire LatentCLIP visual en-
coder is unfrozen (indicated by the fire symbol, as opposed to the
snowflake symbol which means the model is frozen) and is trained
to minimize the cosine difference between fLandf, which is the
image feature of Ifrom the CLIP image encoder.
4.1. Masking mechanism
During the editing process, it does not make sense to add
noise or denoise regions that we want to maintain. So
we design a masking mechanism MSK (I, pO, p,A)to lo-
cate the editing region Mfrom aforementioned inputs. To
replace objects in pure text-guided image editing, M=
CLIPSeg (I, o), where CLIPSeg (·,·)[18] computes regions
inIthat correspond to objects {o:o∈pO, o /∈p}. To per-
form style transfer, Mij= 1for all pixel locations (i, j). To
add objects for pure text-guided or reference-guided image
editing, M=Ma∈ A. For stroke-guided image editing or
image composition, Mij= 1−δ(Iij,(I∗)ij)using the Kro-
necker delta function ( δ(x, y) = 1 ifx=y, otherwise 0).
4.2. Optimization loss functions
To achieve the desired output ˜I0or its latent representation
˜L0, we need to design a set of optimization functions that
captures key characteristics the output should have with re-
spect to the original image Ior its latent representation L.
First, we want to enforce semantic similarity between the
two with a CLIP-based[24] semantic loss function. But in-
stead of operating it in the pixel domain like previous meth-
ods [9, 33], we have designed our own CLIP visual encoder
that operates in the latent domain called LatentCLIP. This
greatly speed up the optimization as the latent domain is
much more compacted than the pixel domain.
LatentCLIP visual encoder LatentCLIP vistakes a latent
image and is trained to output the same feature as its pixel
domain counterpart from the original CLIP visual encoder
CLIP vis(Fig. 4), where the model training objective is
1−cos(LatentCLIP vis(L),CLIP vis(I)). (3)
LatentCLIP visis initialized from a pretrained CLIP vis,
which the first convolution layer replaced to accommodate
for the dimension of latent images.
To incorporate LatentCLIP visand the CLIP text
encoder CLIP text(·)[7, 24] to the semantic loss
6340
Algorithm 1 TiNO-Edit
1:function TINO-E DIT(I, pO, p,A)
2: K∈Z+,T∈[0,1]
3: tk←k·T
K, k∈[0, K]
4: N∼ N(0,I)
5: L←V AE enc(I)
6: M=MSK (I, pO, p,A)
7: forw←1toWdo
8: ˜LK←FDK(L, N) ▷Eq. 1
9: ˜LK←˜LKJM+LJ(1−M)
10: fork←Kto1do
11: ˜Nk←UNet SD(˜Lk, tk, p)
12: ˜Lk−1←RDk,k−1(˜Lk,˜Nk) ▷Eq. 2
13: ˜Lk−1←˜Lk−1JM+LJ(1−M)
14: end for
15: argmin
N,tkLtotal(L,˜L0, pO, p,A) ▷Eq. 8
16: end for
17: return V AE dec(˜L0)
18:end function
function, we may calculate the cosine distance be-
tween the output feature and the prompt feature
cos(LatentCLIP vis(˜L0),CLIP text(p))[24], the distance
between the original prompt and the target prompt in CLIP
text domain CLIP text(pO)−CLIP text(p), as well as the
distance between the original image and the output in CLIP
image domain (LatentCLIP vis(L)−LatentCLIP vis(˜L0)) [33]
if we adopt directly from these prior methods.
However, all of them rely on the assumption that the im-
age encoder and the text encoder are from the same CLIP
pre-trained model without any modifications or fine-tuning
with the same feature dimensions. This prevent different
variations of SD and new concepts learned through Dream-
Booth [27] or Textual Inversion [13] outside what the origi-
nal CLIP text encoder to be incorporated in the loss. To this
end, we design a new semantic loss function that minimizes
the difference between the cosine distance between origi-
nal/target images in the CLIP image domain and the one
between original/target prompt in the CLIP text domain as
Lsem(L,˜L0, pO, p)
= cos( LatentCLIP vis(L),LatentCLIP vis(˜L0))
−cos(CLIP text(pO),CLIP text(p)),(4)
where CLIP textmay include new concepts learned through
DreamBooth [27] or Textual Inversion [13].
Similarly, to ensure semantic similarity to the latent rep-
resentation of the reference image Lr=V AE enc(Ir)for
reference-guided image editing, we define
Lref(L, Lr) = cos( LatentCLIP vis(L),LatentCLIP vis(Lr).
(5)
Figure 5. Compounded image editing. We present a com-
pounded image-editing workflow by applying our method repeat-
edly on a single image. For each step, the user can perform any
of the supported editing operations. Any additional information
such as inputs including masks, reference images, user strokes,
user-composed images, and concept images used to train custom
concepts are shown next to the corresponding arrows.
Next, we want to enforce visual similarity between Land
˜L0. Following the same idea in LatentCLIP, we designed
LatentVGG, a VGG encoder [29] that operates in the latent
domain, which is initialized and trained along with a pre-
trained VGG encoder. The training objective is
∥LatentVGG (L),VGG (I)∥1. (6)
After training, our perceptual loss is
Lperc(L,˜L0) =∥LatentVGG (L)−LatentVGG (˜L0)∥1.
(7)
To put everything together, our final loss function is
Ltotal(L,˜L0, pO, p,A)
=λsem· Lsem(L,˜L0, pO, p)
+λref· Lref(L, Lr)
+λperc· Lperc(L,˜L0),(8)
where λsem= 1,λperc= 0.5, and λref= 1if applicable.
In the end, TiNO-Edit (Alg. 1) optimize Nand all tk’s
for a pre-defined Wnumber of optimization steps with re-
spect to Ltotal, each of which consists of Ktimesteps as
one complete denoising process.
5. Experiments
TiNO-Edit uses SD v2.1 [25] with K= 10 denoising steps,
starting timestep value T= 0.75, and the number of op-
6341
Original T2L DD P2P0 SINE EDICT IP2P PnP NTI Ours
“a cat” →“a dog”
“roses” →“sunflowers”
+“a bracelet”
+“a magenta hat”
“oil painting ”→ “watercolor”
“3d
rendering” →“crochet”
Figure 6. Pure text-guided image editing comparison. We compare with the following baselines: Text2LIVE (T2L) [2], Disentangle-
mentDiffusion (DD) [33], Pix2Pix-Zero (P2P0) [22], SINE [36], EDICT [32], InstructPix2Pix (IP2P) [3], Plug-and-Play (PnP) [31], and
Null-text Inversion (NTI) [21]. The user indicates the desired editing via text, where the part that reflects the edit is shown below each row.
timization steps W= 50. To use LatentCLIP and La-
tentVGG for our loss function, each model has been trained
on a subset of LAION-5B [28] datasets for 105iterations
with a batch size of 16. We use the AdamW [16] optimizer
with a learning rate of lr=1e-5.
We use two AdamW [16] optimizers, one for tk’s with
lr= 1 and the other for Nwithlr= 0.005. Our method
can also run on DreamBooth (DB) [27] or Textual Inversion
(TI) [13], where we use the same SD and noise scheduler
DB/TI is trained with to avoid performance degradation.
5.1. Compounded Image Editing
We present a compounded image editing workflow by ap-
plying our method repeatedly on a single image with dif-
ferent operations(Fig. 5). For each step, the user can per-
form any of the supported editing operations, which gives
the user creative control over how they want to edit images.5.2. Qualitative comparisons
Pure text-guided image editing. We compare TiNO-
Edit with other text-guided image editing baselines, in-
cluding Text2LIVE (T2L) [2], DisentanglementDiffu-
sion (DD) [33], Pix2Pix-Zero (P2P0) [22], SINE [36],
EDICT [32], InstructPix2Pix (IP2P) [3], Plug-and-Play
(PnP) [31], and Null-text Inversion (NTI) [21] and test vari-
ous use cases (Fig. 6). While some baselines generate good
results in sevaeral scenarios (e.g., EDICT rows 2, 4; NTI
rows 2, 3, 5), many still pose various issues, including in-
cohesive edit (T2L rows 3, 4, 5; DD row 4), failing to add
new objects (P2P0, EDICT, PnP for row 3), concept leak-
ing (IP2P row 2: image becomes yellow; row 3: shirt is
magenta-colored; NTI row 4: man gets a hat texture), and
altering other attributes (P2P0, SINE, PnP row 4; NTI row
6). On the other hand, our method is robust across use cases.
6342
Original Mask Ref VCT GLIGEN PbE Ours
Figure
7.Comparison with reference-guided image editing
baselines. We compare our method with Visual Concept Trans-
lator (VCT) [6], GLIGEN [17], and Paint-by-Example (PbE) [34],
where the masks (Mask) indicate edit regions where the object in
the reference image (Ref) should be included.
Original Strokes BLD 0.5 BLD 0.75 SDE 0.5 SDE 0.75 Ours
+“boots”
“chandelier” →“stained
glass chandelier”
+“an
apple tree”
“hair”→“blue
curly hair”
+“a
teddy bear”
+“a
gift box”
Figure 8. Stroke-based image editing and image composition
baseline comparison. We compare to Blended Latent Diffusion
(BLD) [1] and SDEdit (SDE) [20] for stroke-guided image editing
(row 1-3) and image composition (row 4-6). BLD and SDEdit
both accept a starting timestep Tas input, and we test two values
0.5 and 0.75 (shown next to the method names).
Reference-guided image editing. For reference-guided
image editing, we compare our method with Visual Concept
Translator (VCT) [6], GLIGEN [17], and Paint-by-Example
(PbE) [34] (Fig. 7). our method is better at preserving de-
tails (a dog with its tongue sticking out, white stripes on
the hat) when maintaining other image regions. In con-
trast, VCT failed to add objects, GLIGEN does not com-T2L
DD P2P0 SINE EDICT IP2P PnP NTI Ours
CLIP-T 0.299
0.316 0.271 0.314 0.327 0.315 0.321 0.316 0.328
CLIP-I 0.879
0.853 0.793 0.912 0.898 0.845 0.835 0.843 0.924
DINO-I 0.864
0.862 0.776 0.805 0.838 0.828 0.756 0.799 0.874
T
able 1. Pure text-guided image editing qualitative evaluation.
VCT
GLIGEN PbE Ours
CLIP-T 0.267
0.295 0.300 0.311
CLIP-I 0.899
0.814 0.841 0.928
CLIP-I r 0.537
0.616 0.570 0.674
DINO-I 0.919 0.770
0.813 0.869
Table 2. Reference-guided image editing qualitative evaluation
BLD
0.5 BLD 0.75 SDEdit 0.5 SDEdit 0.75 Ours
CLIP-T 0.298 0.287
0.295 0.295 0.298
CLIP-I 0.930
0.924 0.897 0.953 0.959
CLIP-I s 0.956
0.927 0.986 0.956 0.987
DINO-I 0.953
0.959 0.961 0.973 0.978
Table 3. Stroke-guided image editing qualitative evaluation
BLD
0.5 BLD 0.75 SDEdit 0.5 SDEdit 0.75 Ours
CLIP-T 0.273
0.273 0.294 0.267 0.298
CLIP-I 0.955
0.950 0.903 0.937 0.958
CLIP-I c 0.943
0.945 0.965 0.964 0.978
DINO-I 0.977
0.975 0.972 0.973 0.983
Table 4. Image composition qualitative evaluation
pose edited objects to the original images (dog standing on
the sofa) and PbE generates objects that are similar to the
references at a high level without keeping their details.
Stroke-guided image editing. We compare to Blended La-
tent Diffusion (BLD) [1] and SDEdit (SDE) [20] for stroke-
guided image editing (row 1-3 of Fig. 8), both of which take
a starting timestep as input. BLD largely generates results
that do not align with the strokes and the same problem ap-
pears in SDEdit. On the other hand, TiNO-Edit can generate
objects from strokes that blend well with the original.
Image composition. We also compare with BLD and
SDEdit for image composition use cases (Fig. 8row 4-6),
where TiNO-Edit refine user-composed images and gener-
ate cohesive and nature-looking outputs (Fig. 8) such as
making the teddy bear sit on the sofa rather than floating
in front of it. Again, BLD changes the user input dras-
tically. SDEdit with the starting timestep of 0.75 largely
omits the new object that should be added to the results.
While SDEdit with the starting timestep of 0.5 generates
better results, the output may not look cohesive.
5.3. Quantitative Comparisons
We created a test set of 200 samples in the form of
(I, p O, p, Ir, Is, Ic), where we ran each method for each
sample and report the following popular metrics averaged
across the test set ( Tabs. 1to4):
• CLIP-T: cos(CLIPvis( ˜I),CLIPtext(p))
• CLIP-I: cos(CLIP vis(˜I),CLIP vis(I))
• CLIP-I ∗:cos(CLIP vis(˜I),CLIP vis(I∗)),I∗∈ {Ir, Is, Ic}
• DINO-I: cos(DINO vis(˜I),DINO vis(I))
6343
where ˜Iis the output, DINO visrefers to DINO ViT [5]
which can be used to evaluate image perceptual alignment.
Our method outperforms other methods in most metrics, in-
cluding DINO-I, a non-CLIP-based metric not used during
training. In Tab. 2, VCT scores the highest in DINO-I as
its outputs are very similar to the original images without
respecting the target prompts/images (Fig. 7), which is also
supported by the fact that it gets the worst CLIP-T score.
5.4. Ablation studies
Change in timesteps and noise throughout optimization.
We look at how tk’s and N change across W optimization
steps and average the values across our test set in Fig. 9.
Timesteps earlier in the diffusion process are changed more
compared to timesteps later in the diffusion process because
the former has a larger effect on the editing result. The
range and the mean of Nis quite stable over time because
they are optimized with respect to a lower learning rate and
Figure
9.Changes in timesteps and noise through optimization.
We plot the values of tk’s alongside the minimum, maximum, and
mean of Nacross Woptimization steps. This indicates that N
remains within the noise distribution that SD has been learned,
preventing potential degradation of the output image.
const tk
const N
M=1
W=0
W=10 W=20 W=30 W=40 W=50
Figure 10. Effect of optimizing timesteps, noise, and using the
masking mechanism. Optimizing both tk’s and Nleads to opti-
mize results across different number of optimization steps.Original w/ latent w/ pixel Concept
w/ latent w/ pixel
“
Audrey Hepburn” →“Marilyn Monroe” “Audrey Hepburn” → ⟨concept⟩
Figure 11. Effect of LatentCLIP and LatentVGG on image
editing quality and custom concepts. Compared with the orig-
inal CLIP [24] and VGG [29] that operate in pixel spaces (“w/
pixel”), using our LatentCLIP and LatentVGG (“w/ latent”) is not
only more efficient but leads to better quality outputs.
handle more local changes with respect to the changing tk’s.
Stability in Nalso means that we don’t need to worry about
Ngoing outside the noise distribution SD has learnt and
cause performance degrade.
Effect of optimizing timesteps, noise, and using the
masking mechanism. We first study the effect of optimiz-
ing timesteps, noise, and using the masking mechanism by
generating results with different optimization configuration
with a fixed seed (Fig. 10). As we can see, TiNO-Edit is the
most robust across various W. Keeping tk’s constant causes
the outputs to degrade over time because when the timestep
values are sub-optimal, Nhas to change more drastically
to over-compensate for our loss function, which pushes it
away from the Gaussian noise distribution that SD learns.
Omitting optimizing Nhas a smaller effect on the results
with larger Wbut is more prone to artifacts (the shadow
around the coat collar). Moreover, when Wis small, we
may get drastically different images compared to the origi-
nal. Lastly, when the editing region is not constrained by the
mask M, the global structure of the image may get changed
entirely. This suggest that optimizing both tk’s and Nis
necessary for optimal results.
Effect of LatentCLIP and LatentVGG. We study the ef-
fect of using LatentCLIP and LatentVGG compared to us-
ing the original CLIP [24] and VGG [29] on a NVIDIA
A6000. In terms of image editing quality, using our Latent-
CLIP and LatentVGG leads to high-quality image editing
results with better alignment with the original image and
less artifacts (Fig. 11row 1) while performing better with
custom concepts in Stable Diffusion variants such as Tex-
tual Inversion [13] (Fig. 11row 2).
6. Conclusion
In this paper, we have presented TiNO-Edit, an image
editing method with pre-trained Stable Diffusion models
by optimizing diffusion timesteps and input noise. We
have designed a set of loss functions that greatly reduce
the computation resources required for optimization.
Comparing TiNO-Edit with various task-specifically
baselines, our method is able to generalize across
specific image editing tasks and produce superior results.
6344
References
[1] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended
latent diffusion. ACM Transactions on Graphics (TOG), 42
(4):1–11, 2023. 2,7
[2] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-
ten, and Tali Dekel. Text2LIVE: Text-driven layered image
and video editing. In European Conference on Computer
Vision (ECCV), pages 707–723. Springer, 2022. 6
[3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structPix2Pix: Learning to follow image editing instructions.
InConference on Computer Vision and Pattern Recognition
(CVPR), pages 18392–18402, 2023. 2,6
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in Neural
Information Processing Systems (NeurIPS), 33:1877–1901,
2020. 2
[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In In-
ternational Conference on Computer Vision (ICCV), 2021.
8
[6] Bin Cheng, Zuhao Liu, Yunbo Peng, and Yue Lin. General
image-to-image translation with one-shot image guidance. In
International Conference on Computer Vision (ICCV), pages
22736–22746, 2023. 2,3,7
[7] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell
Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-
mann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scal-
ing laws for contrastive language-image learning. In Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 2818–2829, 2023. 4
[8] Guillaume Couairon, Jakob Verbeek, Holger Schwenk,
and Matthieu Cord. DiffEdit: Diffusion-based seman-
tic image editing with mask guidance. arXiv preprint
arXiv:2210.11427, 2022. 2
[9] Katherine Crowson, Stella Biderman, Daniel Kornis,
Dashiell Stander, Eric Hallahan, Louis Castricato, and Ed-
ward Raff. VQGAN-CLIP: Open domain image genera-
tion and editing with natural language guidance. In Euro-
pean Conference on Computer Vision (ECCV), pages 88–
105. Springer, 2022. 4
[10] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat GANs on image synthesis. Advances in Neural Infor-
mation Processing Systems (NeurIPS), 34:8780–8794, 2021.
2
[11] Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, and
Aleksander Holynski. Diffusion self-guidance for control-
lable image generation. arXiv preprint arXiv:2306.00986,
2023. 3
[12] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
transformers for high-resolution image synthesis. In Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 12873–12883, 2021. 3,4
[13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,
Amit H Bermano, Gal Chechik, and Daniel Cohen-Or.An Image is Worth One Word: Personalizing text-to-
image generation using textual inversion. arXiv preprint
arXiv:2208.01618, 2022. 1,2,5,6,8
[14] Shanghua Gao, Zhijie Lin, Xingyu Xie, Pan Zhou, Ming-
Ming Cheng, and Shuicheng Yan. EditAnything: Empow-
ering unparalleled flexibility in image editing and gener-
ation. In ACM International Conference on Multimedia
(ACMMM), 2023. 2
[15] HuggingFace. HuggingFace Stable diffusion image-
to-image, 2023. https : / / huggingface . co /
docs / diffusers / api / pipelines / stable _
diffusion/img2img. 3
[16] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980,
2014. 6
[17] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-
wei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.
GLIGEN: Open-set grounded text-to-image generation. In
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 22511–22521, 2023. 2,7
[18] Timo L ¨uddecke and Alexander Ecker. Image segmentation
using text and image prompts. In Conference on Computer
Vision and Pattern Recognition (CVPR), pages 7086–7096,
2022. 4
[19] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang
Zhao. Latent consistency models: Synthesizing high-
resolution images with few-step inference. arXiv preprint
arXiv:2310.04378, 2023. 2
[20] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided
image synthesis and editing with stochastic differential equa-
tions. In International Conference on Learning Representa-
tions (ICLR), 2022. 2,4,7
[21] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and
Daniel Cohen-Or. Null-text inversion for editing real images
using guided diffusion models. In Conference on Computer
Vision and Pattern Recognition (CVPR), pages 6038–6047,
2023. 2,3,6
[22] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun
Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image
translation. In SIGGRAPH, pages 1–11, 2023. 2,3,6
[23] Dustin Podell, Zion English, Kyle Lacey, Andreas
Blattmann, Tim Dockhorn, Jonas M ¨uller, Joe Penna, and
Robin Rombach. SDXL: Improving latent diffusion mod-
els for high-resolution image synthesis. arXiv preprint
arXiv:2307.01952, 2023. 2
[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In International Conference on Machine Learning
(ICML), pages 8748–8763. PMLR, 2021. 2,4,5,8
[25] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 10684–
10695, 2022. 1,2,3,5
6345
[26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
Net: Convolutional networks for biomedical image segmen-
tation. In Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III
18, pages 234–241. Springer, 2015. 2,3
[27] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In Conference on Computer Vision and Pattern
Recognition (CVPR), pages 22500–22510, 2023. 1,2,5,6
[28] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. LAION-5B: An open large-scale dataset for train-
ing next generation image-text models. Advances in Neu-
ral Information Processing Systems (NeurIPS), 35:25278–
25294, 2022. 6
[29] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014. 5,8
[30] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502, 2020. 2,3
[31] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali
Dekel. Plug-and-play diffusion features for text-driven
image-to-image translation. In Conference on Computer
Vision and Pattern Recognition (CVPR), pages 1921–1930,
2023. 2,3,6
[32] Bram Wallace, Akash Gokul, and Nikhil Naik. EDICT:
Exact diffusion inversion via coupled transformations. In
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 22532–22541, 2023. 2,6
[33] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale,
Trung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu
Chang. Uncovering the disentanglement capability in text-
to-image diffusion models. In Conference on Computer
Vision and Pattern Recognition (CVPR), pages 1900–1910,
2023. 2,3,4,5,6
[34] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin
Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by Ex-
ample: Exemplar-based image editing with diffusion mod-
els. In Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 18381–18391, 2023. 2,7
[35] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In In-
ternational Conference on Computer Vision (ICCV), pages
3836–3847, 2023. 2
[36] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris N
Metaxas, and Jian Ren. SINE: Single image editing with
text-to-image diffusion models. In Conference on Computer
Vision and Pattern Recognition (CVPR), pages 6027–6037,
2023. 2,3,6
6346
