Parameter Efﬁcient Self-Supervised Geospatial Domain Adaptation
Linus Scheibenreif1Michael Mommert2,1Damian Borth1
1University of St. Gallen, Switzerland2Stuttgart University of Applied Sciences, Germany
{linus.scheibenreif, damian.borth }@unisg.ch michael.mommert@hft-stuttgart.de
Abstract
As large-scale foundation models become publicly avail-
able for different domains, efﬁciently adapting them to indi-
vidual downstream applications and additional data modal-
ities has turned into a central challenge. For example, foun-
dation models for geospatial and satellite remote sensing
applications are commonly trained on large optical RGB or
multi-spectral datasets, although data from a wide variety
of heterogeneous sensors are available in the remote sens-
ing domain. This leads to signiﬁcant discrepancies between
pre-training and downstream target data distributions for
many important applications. Fine-tuning large founda-
tion models to bridge that gap incurs high computational
cost and can be infeasible when target datasets are small.
In this paper, we address the question of how large, pre-
trained foundational transformer models can be efﬁciently
adapted to downstream remote sensing tasks involving dif-
ferent data modalities or limited dataset size. We present a
self-supervised adaptation method that boosts downstream
linear evaluation accuracy of different foundation models
by4-6%(absolute) across 8 remote sensing datasets while
outperforming full ﬁne-tuning when training only 1-2%of
the model parameters. Our method signiﬁcantly improves
label efﬁciency and increases few-shot accuracy by 6-10%
on different datasets1.
1. Introduction
Remote sensing data, such as satellite imagery and aerial
photographs, have become ubiquitously available in recent
years. Governmental programs such as Landsat [ 44] and
Copernicus [ 11] produce vast amounts of high quality data
and make them publicly available. Important environmen-
tal and societal problems can now be addressed by ap-
plying methods from computer vision to remote sensing
data [ 41]. These include the monitoring of biodiversity [ 27],
extraction of socioeconomic indicators [ 45] or the estima-
tion of greenhouse gas emissions [ 31]. Public remote sens-
1Code available at github.com/HSG-AIML/GDAFigure 1. Average performance (colored bars) and number of
trainable parameters (gray bars) for different visual foundation
models across 8 remote sensing datasets. Our Scaled Low-Rank
(SLR) adapter method achieves signiﬁcant performance improve-
ments across datasets and models with no (SLR Linear) and as
little as 1-2%(SLR ﬁne-tuned) additional trainable parameters.
ing archives provide high quality data with global cover-
age, while private ﬂeets of smaller satellites already pro-
vide data at high spatial and temporal resolutions, which
enables applications that depend on timely observations or
high resolution imagery, such as mapping of natural dis-
asters [ 18], monitoring of marine trafﬁc [ 12], or precision
agriculture [ 34].
The remote sensing ﬁeld is characterized by a high het-
erogeneity of available data sources. Most satellite instru-
ments are custom-designed to monitor speciﬁc phenom-
ena enabling the satellite’s scientiﬁc or commercial mis-
sion. Commonly collected data modalities include optical
data such as RGB images, multi-spectral data (e.g., near-
infrared or short-wave infrared), hyperspectral data, or syn-
thetic aperture radar (SAR). Accordingly, computer vision
approaches for remote sensing data are highly fragmented
into specialized sub-ﬁelds deﬁned by the different modali-
ties or the application of interest (see [ 49] for a review).
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
27841
0. Foundation Model Training
ℒ!"#Foundation Model
ℒ	!"#
Foundation Model
2. Self-supervised Domain Adaptation
ℒ$#
Foundation Model
3. Supervised Task AdaptationTask Head
Pre-trainingDatasetSARNIRHyperSpectralMulti-spectral1. ModelAdaptationFoundation Model+Source Domain(e.g., Optical RGB)
Unlabeled Data
Labeled DataTarget Domain(e.g., SAR)
Figure 2. Overview of our parameter-efﬁcient continual pre-training framework. An existing pre-trained visual foundation model (0) is
modiﬁed with SLR adapters (1). Then, the adapters are trained in a self-supervised way on the target domain (2) before the model is
ﬁne-tuned for the target task in a supervised manner (3). The contributions of this work lie in steps 1–3.
With the success of self-supervised learning, the idea of
general foundation models that can be leveraged for many
different tasks has gained traction in the computer vision
ﬁeld [ 4]. Large pre-trained models have become a stan-
dard component in computer vision pipelines for classiﬁ-
cation [ 13], segmentation [ 19] or object-detection [ 47]. In
large part, the success of these approaches is due to the
ubiquitous nature of optical RGB data in computer vision.
Deep neural networks trained on large optical RGB datasets
such as ImageNet [ 29] have shown to be robust to lower-
level differences between individual optical camera sensors.
This robustness enables the transfer of the pre-trained mod-
els to unseen imagery with similar characteristics. Similar
results are possible in natural language processing, where
unsupervised pre-training on large amounts of textual data
yields strong performance on diverse tasks [ 5].
A number of foundation models for remote sensing
data haven recently been proposed [ 8,23,28]. Most of
these models follow the computer vision approach and are
pre-trained on optical RGB imagery, albeit collected from
satellites or airborne observing platforms. However, opti-
cal RGB data corresponds only to a small fraction of the
commonly used data modalities in the remote sensing do-
main [ 38]. Foundation models promise large beneﬁts in
remote sensing, where labeled datasets are small and the
acquisition of labels can be very expensive. To date, sig-
niﬁcant potential for remote sensing foundation models re-
mains for data modalities beyond optical RGB data. First
steps haven been made to include other data modalities
such as multi-spectral [ 8] or SAR [ 30] data in the pre-
training progress, but remote sensing foundation models re-
main limited in their ability to adapt to downstream tasks
utilizing unseen modalities. Without zero- or few-shot ca-pabilities on modalities other than optical data, expensive
ﬁne-tuning protocols have to be employed, resulting in the
re-training of large foundation models for datasets involv-
ing new modalities. This requires large amounts of labeled
samples to adapt the model and comes with high computa-
tional cost.
In this paper, we present a new approach for adapting
large remote sensing foundation models to novel tasks and
modalities in a computationally efﬁcient way. Our method
introduces Scaled Low-Rank (SLR) adapters with a small
number of parameters to add new data modalities to a pre-
trained foundation model. Through self-supervised learn-
ing on unlabeled data of the target domain, these additional
parameters allow the model to adapt to the characteristics
of the new data modality, while the pre-trained parameters
are kept ﬁxed. This approach helps to generalize remote
sensing foundation models beyond their pre-training data
modalities while fully leveraging their existing capabilities.
The SLR adapters enable parameters-efﬁcient and label-
efﬁcient supervised training for new downstream tasks.
The contributions of our work are as follows:
•We present SLR adapters, a parameter efﬁcient domain
adaptation method to utilize visual foundation models on
new data modalities.
•We introduce a self-supervised continual pre-training
framework to optimize SLR adapters on unlabeled data
from new domains.
•Our empirical results demonstrate strong performance
of the proposed method. SLR adapters drastically re-
duce the memory footprint, outperform ﬁne-tuning of all
model parameters, and signiﬁcantly improve performance
in few-shot scenarios.
27842
2. Related Work
Computer Vision for Remote Sensing Large amounts
of remote sensing data are routinely collected by a wide
variety of heterogeneous sensors aboard satellites and air-
borne vehicles [ 38]. Much of this data, e.g., optical or radar
observations, can be represented as imagery and be pro-
cessed with computer vision methods [ 41]. This enables
applications ranging from monitoring of biodiversity [ 27] or
wildlife conservation [ 40] to the estimation of demographic
parameters [ 45] or industrial air pollution [ 31]. While unla-
beled remote sensing data is automatically acquired every-
day, producing high quality labeled datasets is difﬁcult for
many important applications [ 3].
Geospatial Foundation Models Foundational models are
large general purpose models that are typically trained
with self-supervised learning [ 4]. To solve speciﬁc prob-
lems, the pre-trained models are then adapted to the target
task with supervised ﬁne-tuning. A common pre-training
technique for foundation models is masked-autoencoding
(MAE) [ 13], which is used to train transformer models by
reconstructing masked portions of the input data. After ex-
tensive training on large datasets, MAE produces strong
general-purpose visual features [ 13]. A number of recent
works adapt the MAE framework to the characteristics of
remote sensing data: SatMAE [ 8] introduces specialized
encodings for the temporal or multi-spectral dimension of
satellite imagery and explores different masking schemes
for self-supervised training. Scale-MAE [ 28] explicitly
addresses the problem of varying ground sampling dis-
tance (GSD) of different remote sensing sensors, and pro-
poses a GSD-aware positional encoding scheme. Masked-
autoencoding has also been used with other remote sens-
ing modalities such as SAR [ 43] or hyperspectral data [ 32].
Other approaches for geospatial foundation models have
used contrastive learning between satellite observations at
different points in time [ 22,23] or data modalities [ 30]. An-
other line of work emphasizes the advantages of hierarchi-
cal pre-training approaches [ 28] for remote sensing founda-
tion models [ 24].
Parameter Efﬁcient Finetuning General purpose founda-
tion models can be adapted for speciﬁc tasks by supervised
ﬁne-tuning on labeled datasets [ 4]. This approach leverages
the model’s pre-trained representations and adapts them to
the characteristics of the target data and task. However, the
standard ﬁne-tuning process is very costly, as every parame-
ter of the large foundation model has to be re-trained, which
is computationally expensive, has a large memory footprint,
and requires a sizeable labeled target dataset. Recently, this
process has been simpliﬁed by limiting the number of pa-
rameters that are trained during the ﬁne-tuning stage [ 16].
Parameter efﬁcient ﬁne-tuning approaches train only the
bias parameters [ 48], add additional trainable adapter mod-
ules [ 14,16] such as low-rank matrices [ 17] between the!!"!!!	#!!"!!##$%(!!)
!!!!""!!$	#LNMSA!"#SLRLN+++MSA!"#SLR
Figure 3. Transformer block (left) with SLR adapters (right). We
add individual SLR adapters to linear transformations in the qkv
projection and mlplayers of the transformer block. SLR freezes
the original transform Wiand introduces trainable scaling param-
eters s1,2
iand low-rank matrices W1,2
i.
pre-trained layers, or rescale activations with learnable vec-
tors [ 20]. Originally proposed for supervised ﬁne-tuning
of large-language models, these techniques can reach per-
formance on-par with ﬁne-tuning of the entire model while
drastically reducing the memory requirements for individ-
ual downstream tasks.
Geospatial Domain Adaptation Domain shifts due to
changes in acquisition region, time, sensor or environmen-
tal conditions are a common problem in geospatial machine
learning (see [ 39] for a review). Unlike most of the general
domain adaptation literature, we focus on adapting unsuper-
vised foundation models to new modalities and targets.
3. Method
This section describes our proposed method for efﬁciently
adapting visual foundation models for new remote sensing
tasks. We ﬁrst introduce the proposed continual pre-training
framework and then the scaled low-rank adapters. Together,
these components facilitate a data-efﬁcient and compute-
efﬁcient adaptation of the foundation model to the target
domain and task.
Preliminaries We investigate the scenario when
a reconstruction-based visual foundation model
f✓:RC⇥H⇥W!RC⇥H⇥W, trained on a dataset from
the source domain Ds, should be transferred to a dataset
from the target domain Dt. We focus on vision transformer
(ViT) [ 10] based foundation models trained with masked
autoencoding (MAE) [ 13]. For self-supervised training, the
unlabeled input imagery xs⇠D swith xs2RC⇥H⇥W
27843
is split into nnon-overlapping patches p2Rn⇥P2Cof
patch size P. The patches are then embedded with a
linear transform ft:Rn⇥P2C!Rn⇥Dto create tokens
of dimension D. For each image xs, a binary mask m
randomly drops a fraction dof the tokens. The remain-
der of the tokens is then processed by a ViT encoder
f✓e:R(1 d)·n⇥D!R(1 d)·n⇥D. After the encoder, MAE
models introduce learnable [MASK] tokens at the positions
of dropped tokens to recover the original sequence length
n. A transformer [ 42] decoder maps the tokens back into
pixel space f✓d:Rn⇥D!RC⇥H⇥Wto compute the
reconstruction loss LMAE:
LMAE=1
d·nX
imi·(xi f✓(xi))2(1)
3.1. Parameter Efﬁcient Continual Pre-training
To adapt the model to data xt⇠D tfrom the unseen target
domain, we introduce scaled low-rank (SLR) adapters f✓ada
to the foundation model f✓. The resulting model f✓ f✓adais
then trained with unlabeled samples xi2D tfrom the target
dataset. During that process, the pre-trained parameters ✓
are kept ﬁxed and only the parameters of the adapters ✓ada
are optimized with stochastic gradient descent. In practice,
we use a masked autoencoding objective LMAEto train the
adapter parameters in a self-supervised fashion.
This makes it possible to leverage all available unlabeled
data samples xiof the target domain. Masked autoencoding
reduces the computational costs for continual pre-training
as the majority of the patches from each image is dropped
in the forward pass, reducing the input sequence length by
the factor d. This has signiﬁcant advantages for transformer
models, as their computational complexity is quadratic in
the number of input tokens. During the backward pass,
we only perform gradient updates for the adapters, as all
other parameters are ﬁxed. Training of the adapters primes
the foundation model for new types of remote sensing data,
even new data modalities, and facilitates successive training
for a target task with limited labeled samples. After adap-
tation to the target domain , the foundation model is then
adapted to the target task(e.g., classiﬁcation) by supervised
ﬁne-tuning of the pre-trained adapter parameters. Based on
our setup and experience, the proposed method reduces the
number of parameters that are trained in the continual pre-
training an ﬁne-tuning stages by about two orders of mag-
nitude for commonly used MAE visual foundation models
when compared to the standard ﬁne-tuning approach.
3.2. Scaled Low Rank Adapters
The scaled low-rank (SLR) adapters are designed to aug-
ment the linear transformations Wiin a pre-trained trans-
former foundation model in a parameter efﬁcient way, while
maintaining as much capacity as possible. ViTs stack mul-tiple blocks bof multi-head self-attention (MSA) and multi-
layer perceptrons (MLP) with layer normalization (LN) [ 2]:
z0
b=M S A b(LN1
b(zb)) +zb
zb+1= MLP b(LN2
b(z0
b)) +z0
b(2)
Notably, the MSA computes query, key and value
representations of the inputs through a linear projection
fqkv:RD!R3D. Similarly, the feed-forward layers of
the MLP consist of linear projections fmlp:RD!RD.
These operations contain the majority of trainable parame-
ters in ViT models.
We propose SLR adapters that scale activations zi 1
from the preceding layer element-wise with a learnable vec-
tors1
i2RD. The resulting rescaled feature vector is
then passed through the original linear transform (s1
i 
zi)Wi. Inspired by low-rank adaptation methods [ 17],
SLR uses symmetric low-rank matrices W1
i2RD⇥r
andW2
i2Rr⇥Dto process the scaled input features
((s1
i z)W1
i)W2
i. Finally, the adapter adds the features
from the original transform and those from the low-rank
matrices and multiplies them with a second scaling vector
s2
i. Using an SLR adapter (see Fig. 3), the linear transform
f(zi)=ziWiturns into:
fada(zi)=s2
i⇥
(s1
i zi)Wi+( (s1
i zi)W1
i)W2
i⇤
(3)
We add instances of this adapter to the linear transforms of
the MLP and MSA layers throughout the MAE encoder and
decoder. The scaling parameters s1,2
iare initialized with
vectors of ones, and the low-rank matrices W1
i⇠N and
W2
iwith zeros [ 17]. The SLR adapter consist of 2·D·r+
2·Dparameters. We choose r⌧D, making the adapter
signiﬁcantly smaller than the original linear transform Wi
with D2parameters. Depending on model architecture and
bottleneck size r, this introduces ⇡1%of the original model
size as additional parameters (see Sec. 4).
3.3. Supervised Task Adaptation
SLR adapters are added throughout the pre-trained founda-
tion model and trained with masked autoencoding on the
target domain Dt. We then transfer the model to the target
task of interest T. The model encoder f✓eand its adapter
parameters are combined with a task-speciﬁc head f✓task.
The resulting model can then be trained in a supervised way
with labeled data (xt,yt)from the target domain (e.g., for
ak-way classiﬁcation task with a linear head Wh2RD⇥k
and cross-entropy objective). We investigate supervised
task adaptation settings with different efﬁciency and per-
formance trade-offs (see Sec. 4).
4. Experiments & Results
Datasets We evaluate our experiments on 8 different re-
mote sensing datasets (see Tab. 2): EuroSAT [ 15], RE-
SISC45 [ 6], UCMerced[ 46], FireRisk [ 33], TreeSatAI [ 1],
27844
MAE SatMAE ScaleMAEDatasetLinear SLR Lin.SLR FT FTLinear SLR Lin.SLR FT FTLinear SLR Lin.SLR FT FTEuroSAT93.27 96.6198.66 98.8292.00 94.5398.21 97.7994.52 95.6998.65 98.73RESISC4577.90 87.0893.84 95.1681.80 84.0292.57 93.3986.28 87.4092.87 95.12FireRisk37.89 41.7852.23 49.1738.27 38.1450.80 51.2141.05 41.8652.63 51.45TreeSatAI23.05 38.6957.66 53.7821.33 29.5555.56 50.9923.48 37.1553.97 52.58EuroSAT-SAR77.95 84.2287.00 86.4671.83 79.6687.17 86.8673.53 82.7386.44 78.49BENGE-S1-C34.81 42.1442.80 46.8235.23 35.6045.14 44.7735.35 37.0245.59 45.07BENGE-S1-S68.06 69.7469.84 69.0066.57 68.4570.63 68.2768.10 68.4469.05 67.23UCMerced94.74 98.3598.43 98.5095.44 95.9598.81 96.6595.18 96.6797.60 96.20Average63.46 69.8375.06 74.7162.81 65.7474.86 73.7464.69 68.3774.60 73.11Table 1. Classiﬁcation and segmentation accuracy of different visual foundation models across 8 remote sensing datasets. Linear : Linear
evaluation of the pre-trained model. SLR Linear : Linear evaluation after self-supervised training of SLR adapters. SLR FT : Supervised
training of the SLR adapters after self-supervised pre-training of SLR adapters. FT: Fine-tuning of the full foundation model.
Dataset # Samples Modality GSD
EuroSAT [ 15] 27k Multispec. 10m
RESISC45 [ 6] 31k RGB 0.2-30m
FireRisk [ 33] 91k RGB 1m
TreeSatAI [ 1] 50k multiple 0.2-10m
EuroSAT-SAR [ 43] 27k SAR 10m
BENGE-8k [ 25] 8k multiple 10m
UCMerced [ 46] 2.1k RGB 0.3m
Table 2. Overview of the remote sensing datasets used in this
work. Each dataset is used to learn a supervised classiﬁcation
and/or segmentation task. The datasets have between 2-90ksam-
ples and combine observations from different modalities that vary
in their ground-sampling distance (GSD).
BENGE-8k [ 25] classiﬁcation, BENGE-8k segmentation
and EuroSAT-SAR [ 43]. These datasets contain different
data modalities such as RGB and multi-spectral imaging,
SAR polarimetry and others at ground-sampling distances
between 30 cm and30 m per pixel. Detailed information on
the speciﬁc downstream task trained on each dataset (clas-
siﬁcation or segmentation) and the corresponding target are
presented in the supplemental material. When available, we
use the dataset splits deﬁned by the torchgeo library [ 35].
Implementation Details To use foundation models for
the downstream classiﬁcation and segmentation tasks, we
append a linear layer or a convolutional layer, respectively,
to the model encoder. We use the AdamW [ 21] optimizer
for training and reduce the learning rate by a factor of 10
when the validation loss plateaus. The number of train-
ing steps is ﬁxed for each dataset, and we report the test
performance of the checkpoint with the lowest validation
loss. Images are re-sized to 2242pixels and 75% of to-
kens are masked during the self-supervised SLR adapter
pre-training. For few-shot experiments we report the aver-
age performance and standard-deviation across three train-ing runs on few-shot samples chosen with different random
seeds. Samples are chosen with replacement if kis greater
than the total number of samples for a class in the dataset.
Further details on datasets and training procedures can be
found in the supplemental material.
Foundation Models The SLR adapter method can be ap-
plied to any neural network model with dense layers for
continual pre-training and efﬁcient ﬁne-tuning. In our ex-
periments, we use ViT-L models with pre-trained weights
from three visual foundation models: MAE , the vanilla
masked autoencoder [ 13], pre-trained on ImageNet [ 29].
SatMAE , a geospatial foundation model [ 8] with special-
ized positional embeddings for remote sensing data modal-
ities. And Scale-MAE , a geospatial foundation model [ 28]
with positional embeddings that are invariant to changes in
the ground-sampling distance of remote sensing data. Both
geospatial foundational models were pre-trained on optical
remote sensing data [ 7].
Evaluation Settings We evaluate the quality of self-
supervised domain adaptation with our method with dif-
ferent supervised downstream training settings on the tar-
get domain. These approaches offer different trade-offs be-
tween label-efﬁciency and computational cost ranging from
standard linear evaluation toﬁne-tuning , which serve as
our baselines.
SLR Linear : To evaluate the quality of representations
from a foundation model after adaptation to the target do-
main with self-supervised SLR adapter training, we ﬁx all
model parameters and train only the (linear) task head. This
incurs the same training cost as conventional linear probing.
SLR Scaling : The design of the SLR adapters facilitates
a parameter-efﬁcient ﬁne-tuning method. Instead of re-
training all parameters of the model, we ﬁx the original
foundation model, as well as the parameters of the low-rank
matrices, after the self-supervised domain adaptation stage.
27845
InputMaskMAEMAE-SLREuroSATUCMercedEuroSAT-SAR
Figure 4. Masked autoencoder reconstruction examples. The ﬁrst
two columns show the original image and the masked version, re-
spectively. The last two columns show the reconstructions from a
masked autoencoder trained on ImageNet and after self-supervised
domain adaptation with SLR adapters. Training with adapters re-
duces the reconstruction loss from 1.35to0.87in the top row, 0.62
to0.59in the middle row, and 0.17to0.14in the bottom row.
We then train all scaling vectors s1,2
{i}and normalization pa-
rameters together with the task head.
SLR Fine-tuning : In this setting, all parameters of the SLR
adapter and normalization parameters are trained along with
the task head, while the parameters of the original founda-
tion model remain ﬁxed.
Self-supervised Domain Adaptation We evaluate the
performance of our method when transferring visual foun-
dation models to different new modalities (see Tab. 1). To
that end, a set of SLR adapters for each dataset is added
to the foundation models, as detailed in Section 3. Self-
supervised training on the target dataset improves the mod-
els’ reconstruction capacities and recovers data modality-
speciﬁc details (see Fig. 4). In supervised downstream
experiments, we ﬁnd that SLR adaptation improves the
resulting data representations across datasets and modali-
ties (see Tab. 1). For the MAE foundation model, SLR
adapters improve the average linear evaluation accuracy
from 63.46% to 69.83% (+ 6.37% absolute improvement)
across all datasets (see Fig. 1). Similarly, for SatMAE and
Scale-MAE linear evaluation accuracy improves +2.93%
and+3.68%, respectively. In the ﬁne-tuning setup, our
method outperforms ﬁne-tuning of the entire model on
most dataset and foundation model combinations. On aver-
age, SLR adapter ﬁne-tuning improves model accuracy by
+0.35% for MAE, +1.12% for SatMAE and +1.49% for
Scale-MAE over ﬁne-tuning of the full model.
To evaluate the degree to which self-supervised trainingMethod Params k= 10 k= 100
Linear Eval. 10k 75±0.589±0.5
SLR Linear 10k 74±0.292±0.5
SLR Scale 0.5M 87±0.696±0.1
SLR FT 7.3M 88±2.096±0.1
Fine-tune 304M 82±2.095±0.4
Table 3. Few-shot results with SatMAE on EuroSAT.
Method Params k= 10 k= 100
Linear Eval. 10k 63±0.863±0.2
SLR Linear 10k 71±2.975±0.1
SLR Scale 0.5M 74±3.077±0.3
SLR FT 7.3M 72±3.082±1.0
Fine-tune 303M 64±1.677±3.0
Table 4. Few-shot results with MAE on EuroSAT-SAR.
of SLR adapters captures the beneﬁts that continual pre-
training can provide [ 24], we perform continual pre-training
of all model parameters for the RESISC45 and EuroSAT-
SAR datasets. On both datasets, SLR adapters reach >98%
of the accuracy achieved by full continual pre-training (see
Sec. 9in the Supplementary Material).
Few-shot Learning We investigate the label efﬁciency of
our method with few-shot experiments on the EuroSAT (see
Tab. 3) and EuroSAT-SAR (see Tab. 4) datasets. Fine-
tuning of the SLR adapters outperforms full-model ﬁne-
tuning on both modalities, as well as for different labeled
dataset sizes. In a k-shot experiment, we randomly select k
samples for every class from the training set. With k=10,
SLR adapter ﬁne-tuning improves land-cover classiﬁcation
performance by +6% and+8% on EuroSAT and EuroSAT-
SAR, respectively, compared with ﬁne-tuning of the full
model. To further reduce the number of trainable param-
eters, we also ﬁx the low-rank matrices W1,2
iafter self-
supervised adapter training ( SLR Scale ). In this setting,
only the task head and the scaling parameters are trained
with labeled data. This approach results in the best k=10
performance on EuroSAT-SAR over all tested approaches.
It outperforms full ﬁne-tuning on EuroSAT by +5% and by
+10% on EuroSAT-SAR. Only 0.5M of the 303M param-
eters of the model ( ⇡0.2%) are optimized in this setting,
which limits the risk of overﬁtting and improves label ef-
ﬁciency. Even at k=100 labeled samples per class, SLR
scale outperforms full ﬁne-tuning of the model on EuroSAT
(+1%) and achieves the same accuracy at lower variance
across random seeds on EuroSAT-SAR ( 77±0.3%).
27846
Figure 5. Classiﬁcation accuracy of the MAE foundation model
with SLR adapters. SLR pre-trained : Self-supervised pre-
training of adapters on the target dataset. SLR from scratch :
Random initialization of adapter parameters and supervised train-
ing on the target data.
Ablation on Self-supervised Adapter Training In this
experiment, we investigate the value of self-supervised
adapter training on the target domain (see Fig. 5). To
that end, we compare the SLR adapters initialized by self-
supervised pre-training with adapters trained from random
initialization on the supervised target task. We ﬁnd that
self-supervised pre-training helps to stabilize the super-
vised training phase, facilitates faster convergence, and ulti-
mately leads to higher performance on the target task. This
advantage is more pronounced when the dataset for pre-
training and the shift from source to target domain is larger.
With k=10labeled samples per class, self-supervised SLR
training improves classiﬁcation performance by +6% on
BENGE-8k and by +7% on EuroSAT over randomly ini-
tialized adapters. On the EuroSAT-SAR dataset, we ﬁnd
an improvement of +16% . For a fair evaluation, the same
number of samples for self-supervised and supervised train-
ing of the SLR adapters is used in this experiment. How-
ever, our self-supervised adapter training method is not lim-
ited by the number of available labeled samples. It also has
lower computational costs than supervised ﬁne-tuning, as
75% of each input image is dropped for reconstruction and
not processed by the model encoder.
Segmentation We ﬁne-tune SLR adapters on the
BENGE-8k Sentinel-1 SAR dataset with land-cover masks
(see Fig. 6). For SatMAE, using SLR adapters improves
performance from 66.57% to68.45% with a frozen
encoder ( +1.88%) and up to 70.63% when training the
SLR adapters along with the segmentation head. Thiscorresponds to a +2.36%improvement over ﬁne-tuning of
the full model.
Adapter Design We compare our SLR adapters
with other parameter-efﬁcient training methods:
Method Accuracy
BitFit [ 48] 80.34±2.4
(IA)3[20] 76.72±3.5
Norm tuning [ 9]79.00±3.1
LoRA [ 17] 85.86±0.3
SLR (ours) 87.14±0.1
Table 5. Performance of different pa-
rameter efﬁcient ﬁne-tuning methods
when adapting an ImageNet MAE to
SAR data (EuroSAT-SAR).LoRA [ 17] adds
low-rank ma-
trices to the
model, (IA)3[20]
re-scales interme-
diate activations,
BitFit [ 48] only
trains bias pa-
rameters, and
NormTuning [ 9]
modulates the
normalization
layers. When
necessary, we
slightly adapt the original methods to ensure a fair eval-
uation. LoRA matrices are added to the same linear
transforms where we place our SLR adapters (i.e., to both,
the MSA and MLP blocks of the transformer). (IA)3
activation scaling is applied to the query, key and value
projections of the foundation model. The results indicate
that SLR adapters achieve the best performance when
adapting a visual foundation model from RGB to the SAR
data modality (see Tab. 5). Additionally, SLR is designed
to combine domain adaptation and few-shot capabilities
through its scaling parameters.
Parameter Efﬁciency In our experiments, we use ViT-
L encoder models, which consist of 303M parameters in
the standard setting. For linear evaluation, we add a lin-
ear classiﬁer, introducing D·nc+ncadditional param-
eters, where Dis the model’s embedding dimension and
ncis the number of classes for the task at hand. Our SLR
adapters introduce 2·D·r+2·Dparameters for each lin-
ear projection in the model. For ViT-L, we add 194adapters
throughout the model. Using different bottleneck values
r2{8,16}based on the target data modality, gives an
additional 3.9M or 7.1M parameters in total. This corre-
sponds to ⇡1%and⇡2%of the model’s total parameters,
respectively. Using SLR adapters thus drastically reduces
the storage requirements when training models for multi-
ple data modalities compared to individual ﬁne-tuning of
the full model for each data modality. In the linear eval-
uation setting, the SLR adapters signiﬁcantly improve per-
formance across data modalities, while introducing negligi-
ble computational overhead for training and inference. In
our experiments, we ﬁnd no signiﬁcant difference in train-
ing time between standard linear evaluation and SLR linear
evaluation (using a single NVIDIA Tesla V100 GPU).
27847
InputGround TruthSatMAEFTSatMAE-SLR
Figure 6. Segmentation examples on Sentinel-1 SAR imagery
from the BENGE-8k dataset. Predictions for 8 different land-
cover classes obtained with SatMAE combined with a fully con-
volutional segmentation head. Comparison between full SatMAE
ﬁne-tuning and ﬁne-tuning of SLR adapters.
5. Discussion
We investigate the problem of adapting visual foundation
models to remote sensing downstream tasks involving dif-
ferent data modalities. The large number of different het-
erogeneous data modalities in the remote sensing domain
makes it difﬁcult to design foundation models that can read-
ily adapt to any of them. Furthermore, domain adaptation
might also be necessary within a single data modality to ac-
commodate different sensor types, due different spatial or
spectral resolutions and other effects. Recently, a number
of data modality-speciﬁc foundation models have been pre-
sented. In this work, we take an orthogonal approach and
propose to explicitly adapt an existing foundation model for
the data modality of interest with a self-supervised recon-
struction objective. To alleviate the computational cost and
to enable supervised training on small target datasets, we re-
duce the trainable parts of the model to a fraction of its total
parameter count for the adaptation process. We focus on
ViT foundation models trained with masked-autoencoding,
but the proposed approach is generally applicable to neural
networks with dense layers and not speciﬁc to geospatial or
computer vision data modalities. For example, our method
could easily be applied in other domains with multi-modal
data, as, for instance, encountered in autonomous driving or
robotics scenarios.
Parameter efﬁcient training methods make it possible
to train and store large numbers of modality-speciﬁc or
task-speciﬁc models that are derived from the same foun-dation model with small memory footprint. More gener-
ally, utilizing exchangable adapters to introduce specialized
knowledge into a general foundation is a ﬁrst step towards
a more modular deep learning framework where models
for speciﬁc problems are created by combining individ-
ual pre-trained building blocks. Such a framework would
be well suited for the geospatial computer vision domain,
where training independent foundation models for each data
modality of interest would incur huge computational cost.
Broader Impact and Limitations Advances in geospa-
tial foundation models can improve our understanding of
geophysical variables and improve estimates of socioeco-
nomic indicators. This contributes to ﬁelds such as environ-
mental sciences or public policy. In particular, applications
where little labeled data is available stand to beneﬁt from
these developments. As our abilities to collect and anal-
yse remote sensing data improve, we need to be mindful
of implications on surveillance technology and individual
privacy rights. This work focuses on static remote sensing
data without direct observations of people or their individ-
ual activities. The proposed method is limited by public
access to a pre-trained foundation model and performance
might degrade when the difference between source and tar-
get data distribution gets very large (e.g., adaptation of a
vision foundation model to audio data).
6. Conclusion
Geospatial foundation models promise to simplify the anal-
ysis of remote sensing imagery by providing a strong, task
agnostic starting point for building specialized deep learn-
ing models. We still face signiﬁcant challenges when ap-
plying foundation models on data modalities that were not
seen during the pre-training stage. The standard solution to
this problem, i.e., supervised ﬁne-tuning on the target task,
incurs high computational cost and fails altogether on small
datasets. In this work, we show that self-supervised train-
ing of a small number of additional adapter parameters suf-
ﬁces to adapt foundation models to new remote sensing data
modalities. This provides a resource-efﬁcient way to apply
existing large visual models on new remote sensing tasks
with small labeled datasets, or in settings where computa-
tional constraints prevent ﬁne-tuning of the full model. The
presented method represents a memory-efﬁcient improve-
ment over ﬁne-tuning of the full model. We demonstrate
improved performance across different data modalities and
target tasks and strongly outperform existing approaches in
few-shot learning scenarios. We believe that these results
will be also valuable beyond the analysis of remote sens-
ing data in any setting where visual foundation models are
applied across different data modalities.
27848
References
[1]Steve Ahlswede, Christian Schulz, Christiano Gava, Patrick
Helber, Benjamin Bischke, Michael F ¨orster, Florencia Arias,
J¨orn Hees, Beg ¨um Demir, and Birgit Kleinschmit. TreeSatAI
Benchmark Archive: A Multi-sensor, Multi-label Dataset for
Tree Species Classiﬁcation in Remote Sensing. Earth System
Science Data Discussions , 2022:1–22, 2022. 4,5,1
[2]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. Layer Normalization. arXiv preprint arXiv:1607.06450 ,
2016. 4
[3]Favyen Bastani, Piper Wolters, Ritwik Gupta, Joe Ferdi-
nando, and Aniruddha Kembhavi. SatlasPretrain: A Large-
Scale Dataset for Remote Sensing Image Understanding. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 16772–16782, 2023. 3
[4]Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Alt-
man, Simran Arora, Sydney von Arx, Michael S Bernstein,
Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al.
On the Opportunities and Risks of Foundation Models. arXiv
preprint arXiv:2108.07258 , 2021. 2,3
[5]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage Models are Few-shot Learners. Advances in Neural
Information Processing Systems , 33:1877–1901, 2020. 2
[6]Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote Sens-
ing Image Scene Classiﬁcation: Benchmark and State of the
Art. Proceedings of the IEEE , 105(10):1865–1883, 2017. 4,
5,1
[7]Gordon Christie, Neil Fendley, James Wilson, and Ryan
Mukherjee. Functional Map of the World. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 6172–6180, 2018. 5
[8]Yezhen Cong, Samar Khanna, Chenlin Meng, Patrick Liu,
Erik Rozi, Yutong He, Marshall Burke, David Lobell, and
Stefano Ermon. SatMAE: Pre-training Transformers for
Temporal and Multi-spectral Satellite Imagery. Advances in
Neural Information Processing Systems , 35:197–211, 2022.
2,3,5
[9]Harm De Vries, Florian Strub, J ´er´emie Mary, Hugo
Larochelle, Olivier Pietquin, and Aaron C Courville. Mod-
ulating Early Visual Processing by Language. Advances in
Neural Information Processing Systems , 30, 2017. 7
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An Image is Worth 16x16 Words: Trans-
formers for Image Recognition at Scale. In International
Conference on Learning Representations , 2020. 3
[11] Ferran Gascon, Catherine Bouzinac, Olivier Th ´epaut, Math-
ieu Jung, Benjamin Francesconi, J ´erˆome Louis, Vincent
Lonjou, Bruno Lafrance, St ´ephane Massera, Ang ´elique
Gaudel-Vacaresse, et al. Copernicus Sentinel-2A calibration
and products validation status. Remote Sensing , 9(6):584,
2017. 1
[12] Harm Greidanus and Naouma Kourti. Findings of the
DECLIMS Project—Detection and Classiﬁcation of MarineTrafﬁc from Space. Proceedings of SEASAR 2006 , pages
23–26, 2006. 1
[13] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked Autoencoders are Scal-
able Vision Learners. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
16000–16009, 2022. 2,3,5
[14] Xuehai He, Chunyuan Li, Pengchuan Zhang, Jianwei Yang,
and Xin Eric Wang. Parameter-efﬁcient Model Adaptation
for Vision Transformers. In Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence , pages 817–825, 2023. 3
[15] Patrick Helber, Benjamin Bischke, Andreas Dengel, and
Damian Borth. EuroSAT: A Novel Dataset and Deep Learn-
ing Benchmark for Land Use and Land Cover Classiﬁcation.
IEEE Journal of Selected Topics in Applied Earth Observa-
tions and Remote Sensing , 12(7):2217–2226, 2019. 4,5,1
[16] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona
Attariyan, and Sylvain Gelly. Parameter-efﬁcient Transfer
Learning for NLP. In International Conference on Machine
Learning , pages 2790–2799. PMLR, 2019. 3
[17] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-
Rank Adaptation of Large Language Models. In Interna-
tional Conference on Learning Representations , 2022. 3,4,
7
[18] Karen E Joyce, Stella E Belliss, Sergey V Samsonov,
Stephen J McNeill, and Phil J Glassey. A Review of the Sta-
tus of Satellite Remote Sensing and Image Processing Tech-
niques for Mapping Natural Hazards and Disasters. Progress
in Physical Geography , 33(2):183–207, 2009. 1
[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment Any-
thing. arXiv preprint arXiv:2304.02643 , 2023. 2
[20] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta,
Tenghao Huang, Mohit Bansal, and Colin A Raffel. Few-
shot Parameter-efﬁcient Fine-tuning is Better and Cheaper
than In-context Learning. Advances in Neural Information
Processing Systems , 35:1950–1965, 2022. 3,7
[21] Ilya Loshchilov and Frank Hutter. Decoupled Weight De-
cay Regularization. In International Conference on Learning
Representations , 2018. 5
[22] Utkarsh Mall, Bharath Hariharan, and Kavita Bala. Change-
Aware Sampling and Contrastive Learning for Satellite Im-
ages. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 5261–5270,
2023. 3
[23] Oscar Ma ˜nas, Alexandre Lacoste, Xavier Gir ´o-i Nieto,
David Vazquez, and Pau Rodriguez. Seasonal Contrast:
Unsupervised Pre-training from Uncurated Remote Sensing
Data. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 9414–9423, 2021. 2,3
[24] Mat´ıas Mendieta, Boran Han, Xingjian Shi, Yi Zhu, and
Chen Chen. Towards Geospatial Foundation Models via
Continual Pretraining. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 16806–
16816, 2023. 3,6
27849
[25] Michael Mommert, Nicolas Kesseli, Jo ¨elle Hanna, Linus
Scheibenreif, Damian Borth, and Beg ¨um Demir. Ben-Ge:
Extending BigEarthNet with Geographical and Environmen-
tal Data. In IGARSS 2023-2023 IEEE International Geo-
science and Remote Sensing Symposium , pages 1016–1019.
IEEE, 2023. 5,1
[26] Maxim Neumann, Andre Susano Pinto, Xiaohua Zhai, and
Neil Houlsby. In-domain Representation Learning for Re-
mote Sensing. arXiv preprint arXiv:1911.06721 , 2019. 1
[27] Omiros Pantazis, Gabriel J Brostow, Kate E Jones, and Oisin
Mac Aodha. Focus on the Positives: Self-supervised Learn-
ing for Biodiversity Monitoring. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 10583–10592, 2021. 1,3
[28] Colorado J Reed, Xiangyu Yue, Ani Nrusimha, Sayna
Ebrahimi, Vivek Vijaykumar, Richard Mao, Bo Li, Shang-
hang Zhang, Devin Guillory, Sean Metzger, et al. Self-
supervised Pretraining Improves Self-supervised Pretrain-
ing. In Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision , pages 2584–2594, 2022. 2,
3,5
[29] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. ImageNet Large
Scale Visual Recognition Challenge. International Journal
of Computer Vision , 115:211–252, 2015. 2,5
[30] Linus Scheibenreif, Jo ¨elle Hanna, Michael Mommert, and
Damian Borth. Self-supervised Vision Transformers for
Land-cover Segmentation and Classiﬁcation. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition Workshops , pages 1422–1431, 2022. 2,3
[31] Linus Scheibenreif, Michael Mommert, and Damian Borth.
Toward Global Estimation of Ground-Level NO2 Pollution
with Deep Learning and Remote Sensing. IEEE Transac-
tions on Geoscience and Remote Sensing , 60:1–14, 2022. 1,
3
[32] Linus Scheibenreif, Michael Mommert, and Damian Borth.
Masked Vision Transformers for Hyperspectral Image Clas-
siﬁcation. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition Workshops , pages
2165–2175, 2023. 3
[33] Shuchang Shen, Sachith Seneviratne, Xinye Wanyan, and
Michael Kirley. FireRisk: A Remote Sensing Dataset
for Fire Risk Assessment with Benchmarks Using Su-
pervised and Self-supervised Learning. arXiv preprint
arXiv:2303.07035 , 2023. 4,5,1
[34] Rajendra P Sishodia, Ram L Ray, and Sudhir K Singh. Ap-
plications of Remote Sensing in Precision Agriculture: A
Review. Remote Sensing , 12(19):3136, 2020. 1
[35] Adam J Stewart, Caleb Robinson, Isaac A Corley, Anthony
Ortiz, Juan M Lavista Ferres, and Arindam Banerjee. Torch-
geo: Deep Learning with Geospatial Data. In Proceedings
of the 30th International Conference on Advances in Geo-
graphic Information Systems , pages 1–12, 2022. 5
[36] Gencer Sumbul, Marcela Charfuelan, Beg ¨um Demir, and
V olker Markl. BigEarthNet: A Large-Scale Benchmark
Archive for Remote Sensing Image Understanding. InIGARSS 2019-2019 IEEE International Geoscience and Re-
mote Sensing Symposium , pages 5901–5904. IEEE, 2019. 1
[37] Gencer Sumbul, Arne De Wall, Tristan Kreuziger, Filipe
Marcelino, Hugo Costa, Pedro Benevides, Mario Caetano,
Beg¨um Demir, and V olker Markl. BigEarthNet-MM: A
large-scale, Multimodal, Multilabel Benchmark Archive for
Remote Sensing Image Classiﬁcation and Retrieval. IEEE
Geoscience and Remote Sensing Magazine , 9(3):174–180,
2021. 1
[38] Charles Toth and Grzegorz J ´o´zk´ow. Remote Sensing Plat-
forms and Sensors: A Survey. ISPRS Journal of Photogram-
metry and Remote Sensing , 115:22–36, 2016. 2,3
[39] Devis Tuia, Claudio Persello, and Lorenzo Bruzzone. Do-
main adaptation for the classiﬁcation of remote sensing data:
An overview of recent advances. IEEE geoscience and re-
mote sensing magazine , 4(2):41–57, 2016. 3
[40] Devis Tuia, Benjamin Kellenberger, Sara Beery, Blair R
Costelloe, Silvia Zufﬁ, Benjamin Risse, Alexander Mathis,
Mackenzie W Mathis, Frank van Langevelde, Tilo
Burghardt, et al. Perspectives in Machine Learning for
Wildlife Conservation. Nature Communications , 13(1):792,
2022. 3
[41] Devis Tuia, Konrad Schindler, Beg ¨um Demir, Gustau
Camps-Valls, Xiao Xiang Zhu, Mrinalini Kochupillai, Sa ˇso
Dˇzeroski, Jan N van Rijn, Holger H Hoos, Fabio Del Frate,
et al. Artiﬁcial Intelligence to Advance Earth Observation:
A Perspective. arXiv preprint arXiv:2305.08413 , 2023. 1,3
[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is All You Need. Advances in Neural
Information Processing Systems , 30, 2017. 4
[43] Yi Wang, Hugo Hern ´andez Hern ´andez, Conrad M Albrecht,
and Xiao Xiang Zhu. Feature Guided Masked Autoen-
coder for Self-supervised Learning in Remote Sensing. arXiv
preprint arXiv:2310.18653 , 2023. 3,5,1
[44] Michael A Wulder, David P Roy, V olker C Radeloff,
Thomas R Loveland, Martha C Anderson, David M Johnson,
Sean Healey, Zhe Zhu, Theodore A Scambos, Nima Pahle-
van, et al. Fifty Years of Landsat Science and Impacts. Re-
mote Sensing of Environment , 280:113195, 2022. 1
[45] Michael Xie, Neal Jean, Marshall Burke, David Lobell, and
Stefano Ermon. Transfer Learning from Deep Features for
Remote Sensing and Poverty Mapping. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence , 2016. 1,3
[46] Yi Yang and Shawn Newsam. Bag-of-visual-words and Spa-
tial Extensions for Land-use Classiﬁcation. In Proceedings
of the 18th SIGSPATIAL International Conference on Ad-
vances in Geographic Information Systems , pages 270–279,
2010. 4,5,1
[47] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,
Boxin Li, Chunyuan Li, et al. Florence: A new
Foundation Model for Computer Vision. arXiv preprint
arXiv:2111.11432 , 2021. 2
[48] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bit-
Fit: Simple Parameter-efﬁcient Fine-tuning for Transformer-
based Masked Language-models. In Proceedings of the 60th
27850
Annual Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers) , pages 1–9, 2022. 3,7
[49] Xiao Xiang Zhu, Devis Tuia, Lichao Mou, Gui-Song Xia,
Liangpei Zhang, Feng Xu, and Friedrich Fraundorfer. Deep
learning in Remote Sensing: A Comprehensive Review and
List of Resources. IEEE Geoscience and Remote Sensing
Magazine , 5(4):8–36, 2017. 1
27851
