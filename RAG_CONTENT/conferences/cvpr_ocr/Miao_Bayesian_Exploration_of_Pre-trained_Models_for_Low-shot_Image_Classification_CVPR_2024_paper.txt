Bayesian Exploration of Pre-trained Models for Low-shot Image Classification
Yibo Miao1, Yu Lei1, Feng Zhou2*, Zhijie Deng1*
1Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University
2Center for Applied Statistics and School of Statistics, Renmin University of China
{miaoyibo, tony-lei, zhijied }@sjtu.edu.cn, feng.zhou@ruc.edu.cn
Abstract
Low-shot image classification is a fundamental task in
computer vision, and the emergence of large-scale vision-
language models such as CLIP has greatly advanced the
forefront of research in this field. However, most existing
CLIP-based methods lack the flexibility to effectively incor-
porate other pre-trained models that encompass knowledge
distinct from CLIP . To bridge the gap, this work proposes
a simple and effective probabilistic model ensemble frame-
work based on Gaussian processes, which have previously
demonstrated remarkable efficacy in processing small data.
We achieve the integration of prior knowledge by specify-
ing the mean function with CLIP and the kernel function
with an ensemble of deep kernels built upon various pre-
trained models. By regressing the classification label di-
rectly, our framework enables analytical inference, straight-
forward uncertainty quantification, and principled hyper-
parameter tuning. Through extensive experiments on stan-
dard benchmarks, we demonstrate that our method consis-
tently outperforms competitive ensemble baselines regard-
ing predictive performance. Additionally, we assess the ro-
bustness of our method and the quality of the yielded un-
certainty estimates on out-of-distribution datasets. We also
illustrate that our method, despite relying on label regres-
sion, still enjoys superior model calibration compared to
most deterministic baselines.
1. Introduction
The past few years have witnessed the trend of training
large-scale foundation models to serve as infrastructures for
processing images, texts, and multi-modal data [3, 7, 12,
20, 24, 44]. The increasing availability of off-the-shelf pre-
trained models is changing the standard practice for solving
specific downstream tasks for AI practitioners. One funda-
mental application in vision is adapting pre-trained models
for low-shot image classification. This eliminates the need
*Corresponding authors.for massive labeled data as in traditional cases, helps initiate
the data annotation process, and supports the construction
of complex recognition systems, among other advantages.
Fine-tuning and linear probing are typical approaches
for pre-trained models-based low-shot image classifica-
tion [5, 7, 8, 27]. Recently, vision-language models, e.g.,
CLIP [44], have significantly advanced zero-shot classifi-
cation where the image and semantics of interest are pro-
jected into a structured hidden space for nearest neighbor-
based classification. Nevertheless, the few-shot CLIP with
linear probing shows inferior results [44]. To address
this, researchers have put considerable effort into develop-
ing novel CLIP-based few-shot learning pipelines involving
techniques such as prompting learning [59], image-guided
prompt generation [43, 58], adapter tuning [15, 56], etc.
Despite relatively good results, existing CLIP-based
methods usually lose the flexibility to incorporate other
pre-trained models that may contain complementary prior
information. CaFo [57] is a seminal work that explores
constructing few-shot predictors using pre-trained models
other than CLIP and demonstrates outperforming effective-
ness. However, the ensemble weights in CaFo are de-
termined heuristically, and the learning requires extensive
hyper-parameter tuning. Furthermore, as a deterministic
method, CaFo is likely to overfit the few-shot training data
and cannot provide accurate uncertainty estimates. These
challenges are particularly troublesome in situations with
limited data and high-risk domains.
This paper aims to assemble CLIP and other pre-trained
models in a more principled probabilistic manner. Given
that previous studies usually deploy a linear classification
head on top of the pre-trained models, we focus on its
Bayesian counterpart, i.e., a Gaussian process (GP) [53].
GP is an ideal model for low-shot image classification due
to its effectiveness with small data. To incorporate prior
knowledge from various pre-trained models, we suggest
defining the prior kernel as a combination of deep kernels
associated with various pre-trained models. Noting that the
prior mean implicitly corresponds to a model that makes
predictions without seeing any data, we specify it with the
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
23849
well-performing zero-shot CLIP classifier.
Such a modeling can address overfitting and result in
calibrated post-data uncertainty arising from posterior in-
ference. Further, the Bayesian framework allows for the
use of principled objectives for hyper-parameter tuning. For
example, we can use the marginal likelihood or predictive
likelihood of the GP model for hyper-parameter tuning fol-
lowing common practice.
We begin by assessing the predictive performance of
our proposed method on standard low-shot image classifi-
cation benchmarks and observe superior or competitive re-
sults compared to a variety of ensemble baselines. To eval-
uate the generalization capability of our method, we test the
trained models on natural out-of-distribution (OOD) data
and find that our method achieves outperforming results. In
addition, our method has the potential to yield calibrated
uncertainty estimates for OOD data. We further assess the
model calibration by inspecting Expected Calibration Error
(ECE) [16] and its more robust variant, Thresholded Adap-
tive Calibration Error (TACE) [39]. We also offer thorough
ablation studies to better understand the proposed method.
2. Related Works
Zero/few-shot classification. Few-shot classification
means making classifications based on a limited number
of observations, and the zero-shot one requires the trained
model to adapt to the new task without any observation.
Meta-learning has demonstrated its potential as a viable ap-
proach for zero/few-shot learning [47, 50]. Recently, bene-
fiting from the learning on web-scale data, large pre-trained
vision-language models like CLIP have demonstrated im-
pressive performance in zero/few-shot image classification.
Since then, continual effort has been made to better adapt
CLIP to downstream few-shot tasks [15, 17, 51, 56–59]. In
particular, CoOp [59] optimizes a collection of learnable
prompt tokens for few-shot adaptation. Tip-Adapter [56]
augments the zero-shot CLIP classifier with a linear key-
cache cache model to further enhance the classification per-
formance. CaFo [57] supplements Tip-Adapter with one
further linear key-cache cache model for knowledge in-
tegration. Although effective, the deterministic nature of
these methods makes them tend to overfit the few-shot train-
ing data and struggle to estimate predictive uncertainty.
Pre-trained models in vision and beyond. We have
witnessed the change of model architectures in vision from
VGG [48] and ResNet [18] to ViT [13] and Swin Trans-
former [33]. The dominant learning paradigm has under-
gone a transformation, where pre-training models on ex-
tensive datasets and then utilizing them for downstream
tasks via fine-tuning [19] has become a widespread prac-
tice. MoCo [9] and DINO [5] are recent representative
pre-trained models, enjoying the ability to generate high-
quality representations. Visual pre-trained models are in-strumental in achieving state-of-the-art performance on di-
verse downstream tasks such as object detection [32], se-
mantic segmentation [6], and so on. Recently, visual-
language pre-training has achieved impressive success by
learning from massive image-text pairs gathered from the
internet [24, 31, 44], demonstrating astonishing perfor-
mance on various downstream vision and language tasks.
We have reached the consensus that pre-trained models
can serve as containers of valuable prior knowledge, but
a proper mechanism for effective knowledge integration is
under-explored, which is alleviated by this work.
Deep Gaussian processes. GPs are a well-studied and
powerful probabilistic tool in machine learning [53]. They
share a deep connection with neural networks (NNs) with
infinite width [23, 30, 36]. There exists an interesting corre-
spondence among linear regression, Bayesian linear regres-
sion, and GP regression, with the last one often preferred
in low-data regimes. GPs have been successfully used to
solve classification problems based on approximations [37].
However, GPs built on classic kernels lack the inductive
bias carried by NNs. To address this, deep kernel learn-
ing (DKL) [4, 54] has been proposed to leverage deep NNs
for nonlinear data projection, which is then fed to classic
kernels. In the context of few-shot learning, deterministic
methods with a linear classification head face challenges of
overfitting to the training set and are unable to accurately
quantify uncertainty. This limitation restricts their applica-
bility in high-risk domains. In contrast, GPs offer a viable
remedy to these pathologies.
3. Preliminary
This section reviews the basics of GP regression and
deep kernel learning. We use D={xi,yi}N
i=1to denote
a dataset with xi∈ X ⊂ RLandyi∈RCas the L-dim in-
puts and C-dim targets respectively. Let X={xi}N
i=1and
Y={yi}N
i=1represent the training data. Let XvalandYval
represent the validation data (can be split from the training
data) and X∗={x∗
i}M
i=1represent the test data.
3.1. From Deterministic to Bayesian
To deal with the learning problem on the above dataset, it
is common practice to train a deterministic model f:X →
RCusing maximum likelihood estimation or maximum a
posteriori principle. Despite effectiveness, the approach can
suffer from detrimental overfitting and struggle to reason
about model uncertainty appropriately. These issues are ex-
acerbated when only limited data is available.
Practitioners can turn to Bayesian learning approaches to
address such issues. In Bayesian learning, a prior distribu-
tion over model parameters is introduced, and the Bayesian
posterior is (approximately) computed. Then, we compute
the posterior predictive distribution to predict for a new da-
tum, where all likely model specifications are considered.
23850
The uncertainty can be quantified by certain statistics that
capture the degree of variation in that distribution.
3.2. Gaussian Process Regression
GP regression is an extensively studied function-space
Bayesian model [52]. It enjoys exact Bayesian inference
and non-parametric flexibility, allowing for a high degree
of freedom in kernel specification to adapt the model to var-
ious types of nonlinear data. Consequently, it is often the
preferred choice for small- tomedium- sized datasets.
Specifically, GP regression usually deploys a prior in the
following formula:
f(x)∼ GP (m(x), k(x,x′)), (1)
where m(x)indicates the mean function and k(x,x′)de-
notes the kernel (covariance) function that describes the
similarity among data points. Assume additive isotropic
Gaussian noise on the function output, which corresponds
to a Gaussian likelihood y(x)|f(x)∼ N(y(x);f(x), σ2I)
where σ2is the noise variance. The predictive distribution
of the function evaluations f∗on new data points X∗is:
f∗|X∗,X,Y∼ N(E[f∗],cov(f∗)), (2)
where
E[f∗] :=mX∗+kX∗,X[kX,X+σ2I]−1(Y−mX),
cov(f∗) :=kX∗,X∗−kX∗,X[kX,X+σ2I]−1kX,X∗,(3)
mX∈RN×CandkX,X∈RN×Nrepresent the evaluation
ofm(·)andk(·,·)on the training data Xrespectively. Other
matrices are defined similarly. Unlike parametric models
such as NNs, GP makes predictions for new data by re-
ferring to the training samples, similar to how humans ap-
proach the task.
This model can be readily adapted to tackle classifica-
tion problems by treating the one-hot labels as regression
targets, which is known as the label regression [29, 30, 42].
The label regression design enables analytical expressions
for both evidence and posterior, making the classifier com-
putationally efficient and easy to implement.
The GP regression also offers analytical objectives for
tuning parameters (denoted as α; including σ2and others
in the definition of mandk). One typical choice is the log
marginal likelihood:
logp(Y|X,α)∝ −[trace(( Y−mX)⊤(kX,X
+σ2I)−1(Y−mX)) +Clog|kX,X+σ2I|],(4)
which corresponds to the summation of the log marginal
likelihood of Cindependent 1-dim GP regressions. Yet, it is
shown that this objective can be negatively correlated with
the generalization [25, 34]. Given this, a more proper ob-
jective can be logp(Yval|Xval,X,Y,α), i.e., the predictive
likelihood on extra validation data (Xval,Yval). It also takes
the form of Gaussian log densities.3.3. Deep Kernel Learning
In DKL, a θ-parameterized deep NN gθ:X → RD
is typically used to transform the input data xinto hidden
features gθ(x). The kernel is then defined as:
k(x,x′) :=˜k(gθ(x), gθ(x′)), (5)
where ˜kis a base kernel, such as the popular radial basis
function (RBF) kernel or polynomial kernel.
To make the NN parameters better suited for the data
at hand, DKL treats them as hyper-parameters of the GP
model and optimizes them to maximize the marginal like-
lihood. However, the large number of hyper-parameters
makes the optimization time-consuming and increases the
risk of overfitting [40]. It can even underperform a standard
deterministic NN in some toy cases.
4. Methodology
This section explores a Bayesian approach to assem-
ble CLIP with other pre-trained models for low-shot image
classification. Given the discussion above, we take the GP
regression as the modeling framework. We then elaborate
on how to integrate various pre-trained models into it. We
provide an overview of our method in Fig. 1.
4.1. Design of Kernel
Utilizing an NN-based feature extractor to define the ker-
nel function aids to incorporate informative inductive bias
into GP, which is essential for processing complex data such
as images and texts. However, conventional approaches like
DKL suffer from the pathology that all involved NN param-
eters are required to be carefully tuned. Considering that
pre-trained models can yield representations that are gener-
ally applicable to a wide range of applications, we propose
to alternatively use pre-trained models to define deep ker-
nels and then perform an adaptive combination. By doing
this, the number of hyper-parameters in the GP is reduced
significantly, and the prior knowledge encoded by various
pre-trained models is effectively integrated.
Specifically, assuming access to Kpre-trained models
g(i):X →RD(i), i= 1, . . . , K ,1we define the following
independent kernels:
k(i)(x,x′) :=˜k(l(i)◦g(i)(x),l(i)◦g(i)(x′)),(6)
where ◦denotes the element-wise product and l(i)∈RD(i)
+
is a learnable vector used to boost flexibility, e.g., when the
base kernel ˜kis the RBF kernel, l(i)defines the learnable
length-scales for it. We can also enforce a constraint where
1We omit the dependency of these models on their parameters because
we regard them as fixed models and do not perform fine-tuning. We assume
anL2normalization at the end of each model unless specified otherwise.
23851
⋯⋯𝑓∼𝒢𝒫,
CatDogDog(Fairly certain)
Pre-trainedvision model 1Zero-shot predictorAssembled kernel
Pre-trainedvision model K
Pre-trainedvision-language model 
Low-shot training dataTest imagesPosterior predictive distributionShark(Uncertain)Figure 1. Overview of our method. We leverage a GP regressor to tackle the low-shot image classification problem. To integrate knowledge
from CLIP and other pre-trained models, we use them to specify the GP mean and kernel. The label is determined by the mean, and the
uncertainty estimate is determined by the variance.
all elements in l(i)have the same value, and the final learn-
ing outcomes are slightly impacted. By summing up these
kernels, we get the final kernel:
k(x,x′) :=KX
i=1k(i)(x,x′). (7)
The learnable hyper-parameters enable an easy, automatic
adaptation of the kernel to specific data.
4.2. Design of Mean
In essence, the prior mean m(·)refers to a function mak-
ing predictions before seeing any data, i.e., a zero-shot pre-
dictor. Traditionally, m(·)is set to zero for simplicity. How-
ever, as shown in Sec. 5.5, this can lead to poor generaliza-
tion performance in low-shot image classification tasks in
practice. This suggests that it is necessary to incorporate
effective prior knowledge of m(·)into the GP.
Interestingly, a similar phenomenon has been reported in
the literature, where the linear probe CLIP using few-shot
data performs much worse than zero-shot CLIP [44]. This is
because the knowledge in the zero-shot CLIP classifier has
not been effectively integrated into the few-shot learners.
With these insights, we make a simple yet significant im-
provement to our GP model. We set the mean function m(·)
to the zero-shot linear classifier in CLIP, which has demon-
strated strong performance. Concretely, let g:X → RD
denotes CLIP’s image encoder, and w∈RD×Cdenotes
the weight of the zero-shot linear classifier composed of em-
beddings of the text descriptions of the Cclasses of interest.
Our prior mean takes the following form:
m(x) :=γsoftmax( τg(x)⊤w), (8)
where τ, γ∈R+denote the introduced learnable tempera-
ture and scale respectively. Notably, we use a softmax oper-
ation to obtain classification probabilities directly because
we formulate the classification problem as a regression one.Algorithm 1 Leverage Gaussian processes to assemble pre-
trained models for low-shot image classification
1:Input: Number of optimization steps T, training data
X,Y, validation data Xval,Yval, test data X∗, hyper-
parameters α.
2:Output: Predictions ( Y∗) ofX∗andcov(f∗).
3:fort= 1→Tdo
4: Obtain E[fval]andcov(fval)ofXvalvia Eq. (3);
5: Calculate logp(Yval|Xval,X,Y,α)and estimate its
gradients w.r.t. α;
6: Update αby one-step gradient ascent;
7:Obtain E[f∗]andcov(f∗)ofX∗via Eq. (3);
8:Y∗= argmax( E[f∗]);
4.3. Learning
Using the classification likelihood for data fitting is also
viable. However, doing so naturally disrupts conjugacy,
leading to the inability to estimate the posterior in a closed
form [1, 53]. Therefore, we advocate label regression for
its computational efficiency and ease of implementation. It
also allows us to revert to analytical expressions for both the
evidence and the posterior.
One extra merit of label regression is that it enables
the tractable marginalization of data likelihood, so we can
perform hyper-parameter tuning more easily. Let α:=
{σ2,l(1), . . . ,l(K), τ, γ}denote all hyper-parameters in our
method. We optimize them by maximizing the aforemen-
tioned log marginal likelihood or log predictive likelihood
to make them suitable for the data. In the low-shot learning
scenario, the dataset size is small, allowing us to compute
the kernel matrix, its inversion, and its determinant with
minimal cost. Using an Adam optimizer [26], convergence
is usually rapid, typically within 100optimization steps. We
depict the overall algorithmic procedure in Algorithm 1.
23852
Shot 1 2 4 8 16
Ens-LP 40.25 ±0.09 49.79 ±0.07 57.42 ±0.06 62.28 ±0.09 66.31 ±0.13
Ens-LP†61.77±0.13 64.10 ±0.35 65.89 ±0.33 67.59 ±0.06 69.83 ±0.27
Ens-CaFo 62.09 ±0.13 63.67 ±0.19 64.96 ±0.06 66.57 ±0.42 68.78 ±0.25
Ours 63.07±0.07 65.17 ±0.23 67.50 ±0.06 69.31 ±0.08 70.77 ±0.07
Table 1. Comparison with ensemble baselines of low-shot classification accuracy (%) on ImageNet.
4.4. Uncertainty Quantification
As per convention, we utilize the diagonal elements of
cov(f∗)(outlined in Eq. (3)) to quantify the predictive un-
certainty of our model on the test data points. This informa-
tion enables us to refrain from making predictions on data
with high uncertainty and, instead, implement other conser-
vative fallback strategies to handle such situations. More-
over, we can leverage this information to identify OOD
samples since they typically exhibit greater uncertainty than
in-distribution data.
5. Experiments
We first demonstrate that our method achieves competi-
tive low-shot performance on diverse and prevalent bench-
marks. Subsequently, we illustrate how our uncertainty es-
timates can identify OOD samples and validate the calibra-
tion of the learning outcomes. We also conduct ablation
studies on our method and provide analyses.
5.1. Experimental Setup
Datasets. Following CaFo [57], we conduct exper-
iments on image classification datasets including Ima-
geNet [11] and 10 other widely-used ones: Stanford-
Cars [28], UCF101 [49], Caltech101 [14], Flowers102 [38],
SUN397 [55], DTD [10], EuroSAT [21], FGVCAir-
craft [35], OxfordPets [41], and Food101 [2]. We follow
CaFo to train the model with 1, 2, 4, 8, and 16 shots of
training data and test on the entire test set.
Pre-trained models. Unless specified otherwise, we use
the ResNet-50 version of CLIP. Besides, we consider the
model trained by MoCo with ResNet-50 architecture, and
that trained by DINO with ResNet-50 architecture for en-
semble due to their popularity. We clarify that other pre-
trained models are readily applicable to our framework.
Baselines. To validate that our ensemble strategy is non-
trivial, we build three baselines for comparison: (1) Ens-LP,
short for the ensemble of linear probing, where we apply
linear probing to each pre-trained model and take the aver-
age of their output probabilities for prediction, (2) Ens-LP†,
where the zero-shot CLIP classifier is further integrated into
Ens-LP, and (3) Ens-CaFo, where we generalize the original
CaFo approach to assemble multiple pre-trained models by
fusing logits. Notably, the original CaFo approach uses im-ages generated by DALL·E [45] for data augmentation. We
do not use this strategy for all results reported in our paper.
Training protocols. For the hyper-parameters, we ini-
tialize the noise variance σ2= 0.01, the scale γ= 1, and
the temperature τ= 100 . The learnable length-scales l
of the kernel are randomly initialized. We can also con-
strain all elements in lto have the same value, and the
results are slightly impacted.2Notably, since optimizing
hyper-parameters using predictive likelihood requires a val-
idation split, which is not feasible under 1-shot setting of
ImageNet, we instead utilize marginal likelihood to opti-
mize the hyper-parameters in that case. When using the
predictive likelihood, there is an equal 1:1 ratio between the
training and validation splits. On other datasets, following
CaFo [57], we tune the hyper-parameters by the official val-
idation sets. We perform hyper-parameter optimization for
100 steps with an Adam optimizer with a learning rate of
0.01(a cosine decay is adopted). The optimization is low-
cost, e.g., only requiring about 4 minutes on a single RTX-
3090 under the 16-shot ImageNet setting. We follow CaFo
to construct the zero-shot CLIP classifier. We report the av-
erage results over three random runs.
5.2. Predictive Performance
We first evaluate the low-shot classification performance
on the ImageNet benchmark. The results are presented in
Tab. 1. As shown, our method outperforms other baselines
with clear margins. The less favorable results of the base-
lines underscore the inherent challenges in amalgamating
knowledge from multiple pre-trained models. The merits
of our method are more prominent for medium-sized train-
ing data (e.g., the 4 and 8 shots). It is worth noting that
the performance difference between Ens-LP and Ens-LP†is
quite significant, which further underscores the importance
of introducing the zero-shot CLIP-based classifier.
To further evidence the generality and superiority of our
model, we conduct experiments on ten other popular bench-
marks across various domains, with the results reported in
Fig. 2. As shown, our method surpasses or is on par with
the competing baselines on most benchmarks.
2For example, on the 16-shot ImageNet, the accuracy is 70.77 %when
lis set as a learnable vector and 70.42 %when constraining all elements in
lto have the same value.
23853
Figure 2. Comparison of low-shot classification accuracy (%) on the ten popular benchmarks.
Shot 1 2 4 8 16
Linear-probe 22.17 31.90 41.20 49.52 56.13
CoOp 57.15 57.81 59.99 61.56 62.95
CLIP-Adapter 61.20 61.52 61.84 62.68 63.59
VT-CLIP 60.53 61.29 62.02 62.81 63.92
Tip-Adapter-F 61.32 61.69 62.52 64.00 65.51
CALIP-FS 61.35 62.03 63.13 64.11 65.81
Ours 63.07 65.17 67.50 69.31 70.77
Table 2. Comparison with leading methods of low-shot classifica-
tion accuracy (%) on ImageNet.
We also compare our method to leading CLIP-based
low-shot learners, including CLIP-Adapter [15], Tip-
Adapter-F [56], CoOp [59], and CALIP-FS [17] on Ima-
geNet [11]. All these methods use the CLIP model with
ResNet-50 architecture, the same as ours. The results in
Tab. 2 show that our method consistently achieves higher
accuracy than the leading approaches, which indicates the
necessity of assembling complementary prior knowledge
from various pre-trained models for low-shot classification.
5.3. Evaluation on OOD Data
We next evaluate the robustness and the quality of uncer-
tainty estimates of our method on OOD data.
OOD robustness. We use our model trained on 16-
shot ImageNet to evaluate OOD samples from ImageNet-
V2 [46] and ImageNet-Sketch [22]. ImageNet-v2 is an Im-
ageNet test set collected using the original labeling proto-
col, with 10 samples per class. ImageNet-Sketch shares the
same classes as ImageNet, but all images are sketches. AsDatasetsSource Target
ImageNet -V2 -Sketch
Ens-CaFo 68.53 59.62 36.12
Ens-LP 66.37 55.08 24.76
Ens-LP†70.13 59.86 34.66
Ours 70.77 61.30 36.58
Table 3. Test accuracy (%) on OOD datasets.
shown in Tab. 3, our model exhibits superior OOD robust-
ness compared to the ensemble baselines on both ImageNet-
V2 and ImageNet-Sketch.
Quality of uncertainty estimates. We then assess the
quality of our uncertainty estimates on the above OOD
datasets. We collect the predictive uncertainty estimates
yielded by our model for both in-distribution data points
and OOD ones and depict the histogram in Fig. 3, where
the results of the baselines are also included. As the base-
lines are deterministic, we take one minus the prediction
confidence as their uncertainty estimate. As implied by
the histograms, our model does not regard ImageNet-V2
as OOD data, which aligns with the fact that the distribu-
tion of ImageNet-V2 is as similar as possible to the original
ImageNet [43]. On the other hand, our method can clearly
identify the OOD ImageNet-Sketch dataset. For the other
three baselines, while we can also observe that the differ-
ences between ImageNet and ImageNet-V2 are smaller than
those between ImageNet and ImageNet-Sketch, the mani-
festations of these properties are not as pronounced as in
our approach. We also provide additional illustrative fig-
ures demonstrating the utilization of uncertainty estimates,
23854
/uni00000013/uni00000011/uni00000013/uni00000015 /uni00000013/uni00000011/uni00000013/uni00000017 /uni00000013/uni00000011/uni00000013/uni00000019 /uni00000013/uni00000011/uni00000013/uni0000001b /uni00000013/uni00000011/uni00000014/uni00000013 /uni00000013/uni00000011/uni00000014/uni00000015
/uni00000038/uni00000051/uni00000046/uni00000048/uni00000055/uni00000057/uni00000044/uni0000004c/uni00000051/uni00000057/uni0000005c/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000017/uni00000013/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057
/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057/uni00000010/uni00000039/uni00000015
/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057/uni00000010/uni00000036/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b(a) Ours
/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013
/uni00000038/uni00000051/uni00000046/uni00000048/uni00000055/uni00000057/uni00000044/uni0000004c/uni00000051/uni00000057/uni0000005c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000016/uni00000011/uni00000013/uni00000016/uni00000011/uni00000018/uni00000017/uni00000011/uni00000013/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057
/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057/uni00000010/uni00000039/uni00000015
/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057/uni00000010/uni00000036/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b (b) Ens-CaFo.
/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013
/uni00000038/uni00000051/uni00000046/uni00000048/uni00000055/uni00000057/uni00000044/uni0000004c/uni00000051/uni00000057/uni0000005c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000016/uni00000011/uni00000013/uni00000016/uni00000011/uni00000018/uni00000017/uni00000011/uni00000013/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057
/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057/uni00000010/uni00000039/uni00000015
/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057/uni00000010/uni00000036/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b (c) Ens-LP.
/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013
/uni00000038/uni00000051/uni00000046/uni00000048/uni00000055/uni00000057/uni00000044/uni0000004c/uni00000051/uni00000057/uni0000005c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000016/uni00000011/uni00000013/uni00000016/uni00000011/uni00000018/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057
/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057/uni00000010/uni00000039/uni00000015
/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057/uni00000010/uni00000036/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b (d) Ens-LP†.
Figure 3. Histogram for uncertainty estimates. We evaluate different ensemble methods on ImageNet, ImageNet-V2, and Imagenet-Sketch.
/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013
/uni00000026/uni00000052/uni00000051/uni00000049/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000026/uni00000044/uni0000004f/uni0000004c/uni00000045/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000004a/uni00000044/uni00000053
(a) Ours
/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013
/uni00000026/uni00000052/uni00000051/uni00000049/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000026/uni00000044/uni0000004f/uni0000004c/uni00000045/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000004a/uni00000044/uni00000053 (b) Ens-LP
/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013
/uni00000026/uni00000052/uni00000051/uni00000049/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000026/uni00000044/uni0000004f/uni0000004c/uni00000045/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000004a/uni00000044/uni00000053 (c) Ens-LP†
/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013
/uni00000026/uni00000052/uni00000051/uni00000049/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000026/uni00000044/uni0000004f/uni0000004c/uni00000045/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000004a/uni00000044/uni00000053(d) Ens-CaFo
Figure 4. Realibility diagrams of the four ensemble methods.
shown in the Appendix.
We further quantitatively estimate the effectiveness of
the uncertainty estimates by using them to distinguish
ImageNet-Sketch from ImageNet. The AUROCs for our
method, Ens-LP, Ens-LP†, and Ens-CaFo to distinguish be-
tween ImageNet and ImageNet-Sketch are 0.8545, 0.8249,
0.8253, and 0.7546 respectively. These results align with
our previous analyses and validate the superior reliability of
our uncertainty estimates.
5.4. Model Calibration
We then evaluate the model calibration of the proposed
method by the ECE metric [16]. For our method, we take
the maximum element in E[f∗]as predictive confidence to
calculate ECE. The results are presented in Tab. 4, and our
model is slightly worse than Ens-CaFo. However, accord-
ing to [39], ECE leaves ambiguity in both its binning imple-
mentation and the calibration computation for multi-class
scenarios. Its robust variant, TACE [39], can be a better al-
ternative. As shown in Tab. 4, our method enjoys the best
TACE compared to all baselines.
We further present the reliability diagrams of the four
methods in Fig. 4. The model calibration is good if the re-
liability diagram is close to the diagonal. As shown, com-
pared to Ens-LP and Ens-LP†, our method and Ens-CaFoMethod Ens-LP Ens-LP†Ens-CaFo Ours
ECE 0.1489 0.1858 0.0577 0.0786
TACE 0.0462 0.0477 0.0545 0.0169
Table 4. ECE and TACE of the four ensemble methods. All exper-
iments are conducted on ImageNet.
are more well calibrated. Besides, our method, Ens-LP, and
Ens-LP†tend to be underconfident, and the Ens-CaFo tends
to be overconfident. Combining the results in Tab. 4 and
Fig. 4 yields the conclusion that our method enjoys good
model calibration.
5.5. Ablation Study
In this section, we offer ablation studies for our method,
including an examination of the mean and kernel of the GP,
an investigation into how optimization objectives impact the
results, and some visualization results.
GP mean. We investigate the impact of the GP mean
on the final results in Fig. 5a. As shown, when the mean
function equals zero or a learnable vector, prior knowledge
is not incorporated into the GP model, and the final few-shot
classification performance is unsatisfactory.
GP base kernel. We then delve into the specification
23855
/uni00000014/uni00000015 /uni00000017 /uni0000001b /uni00000014/uni00000019
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000018/uni00000019/uni00000018/uni0000001b/uni00000019/uni00000013/uni00000019/uni00000015/uni00000019/uni00000017/uni00000019/uni00000019/uni00000019/uni0000001b/uni0000001a/uni00000013/uni0000001a/uni00000015/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000026/uni00000052/uni00000051/uni00000056/uni00000057/uni00000044/uni00000051/uni00000057/uni00000003/uni0000005d/uni00000048/uni00000055/uni00000052
/uni0000002f/uni00000048/uni00000044/uni00000055/uni00000051/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000003/uni00000059/uni00000048/uni00000046/uni00000057/uni00000052/uni00000055
/uni0000003d/uni00000048/uni00000055/uni00000052/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057/uni00000003/uni00000053/uni00000055/uni00000048/uni00000047/uni0000004c/uni00000046/uni00000057/uni00000052/uni00000055(a)
/uni00000014/uni00000015 /uni00000017 /uni0000001b /uni00000014/uni00000019
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000018/uni0000001b/uni00000019/uni00000013/uni00000019/uni00000015/uni00000019/uni00000017/uni00000019/uni00000019/uni00000019/uni0000001b/uni0000001a/uni00000013/uni0000001a/uni00000015/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000035/uni00000025/uni00000029
/uni0000002f/uni00000044/uni00000053/uni0000004f/uni00000044/uni00000046/uni0000004c/uni00000044/uni00000051
/uni00000030/uni00000044/uni00000057/uni00000048 /uni00000055/uni00000051
 (b)
/uni00000014/uni00000015 /uni00000017 /uni0000001b /uni00000014/uni00000019
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000018/uni00000019/uni00000018/uni0000001b/uni00000019/uni00000013/uni00000019/uni00000015/uni00000019/uni00000017/uni00000019/uni00000019/uni00000019/uni0000001b/uni0000001a/uni00000013/uni0000001a/uni00000015/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000026/uni0000002f/uni0000002c/uni00000033
/uni00000026/uni0000002f/uni0000002c/uni00000033/uni0000000e/uni00000027/uni0000002c/uni00000031/uni00000032
/uni00000026/uni0000002f/uni0000002c/uni00000033/uni0000000e/uni00000030/uni00000052/uni00000026/uni00000052
/uni00000026/uni0000002f/uni0000002c/uni00000033/uni0000000e/uni00000027/uni0000002c/uni00000031/uni00000032/uni0000000e/uni00000030/uni00000052/uni00000026/uni00000052
/uni00000026/uni0000002f/uni0000002c/uni00000033/uni0000000e/uni00000027/uni0000002c/uni00000031/uni00000032/uni0000000e/uni00000030/uni00000052/uni00000026/uni00000052/uni0000000e/uni00000036/uni0000004c/uni00000050/uni00000026/uni0000002f/uni00000035 (c)
/uni00000014/uni00000015 /uni00000017 /uni0000001b /uni00000014/uni00000019
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000019/uni00000013/uni00000019/uni00000015/uni00000019/uni00000017/uni00000019/uni00000019/uni00000019/uni0000001b/uni0000001a/uni00000013/uni0000001a/uni00000015/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni0000002f/uni00000052/uni0000004a/uni00000003/uni00000030/uni00000044/uni00000055/uni0000004a/uni0000004c/uni00000051/uni00000044/uni0000004f/uni00000003/uni0000002f/uni0000004c/uni0000004e/uni00000048/uni0000004f/uni0000004c/uni0000004b/uni00000052/uni00000052/uni00000047
/uni0000002f/uni00000052/uni0000004a/uni00000003/uni00000033/uni00000055/uni00000048/uni00000047/uni0000004c/uni00000046/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni0000002f/uni0000004c/uni0000004e/uni00000048/uni0000004f/uni0000004c/uni0000004b/uni00000052/uni00000052/uni00000047 (d)
Figure 5. Ablation studies on (a) GP mean, (b) GP base kernel, (c) Pre-trained model, and (d) hyper-parameter optimization objective. All
experiments are conducted on ImageNet.
of the GP base kernel. The GP base kernel takes the RBF
formula, we also explore other formulas of base kernels,
e.g., the Laplacian kernel and Mat ´ern kernel. The results
on ImageNet are presented in Fig. 5b. It is evident that the
RBF kernel performs best.
Pre-trained model. Further, we test using various pre-
trained models to specify the GP kernel. The results on Im-
ageNet are shown in Fig. 5c. It is evident that integrating
multiple sources of prior knowledge provided by different
pre-trained models leads to substantial advantages. We ob-
serve that assembling three pre-trained models already pro-
vides comprehensive prior knowledge of ImageNet, and ad-
ditional integration of pre-trained models like SimCLR [7]
does not significantly improve performance.
Objective. As pointed out, using the marginal likelihood
can potentially result in underfitting or overfitting [25, 34].
Thus, we advocate the predictive likelihood for tuning
hyper-parameters. We perform an empirical study on the
objective for hyper-parameter optimization in Fig. 5d. The
results clearly echo such an argument and support the use
of predictive likelihood for hyper-parameter optimization.
Visualization of the prior kernels. In Fig. 6, we illus-
trate the data similarities given by the prior deep kernels de-
fined with various pre-trained models. The data points are
randomly sampled from ImageNet. The results reflect that
distinct prior knowledge regarding data similarities is em-
bedded in these models, and through the kernel ensemble
approach, our method can achieve knowledge integration.
6. Conclusion
This work presents a simple and effective Bayesian ap-
proach for low-shot image classification. We develop a GP
framework to flexibly incorporate diverse prior knowledge
from pre-trained models. Extensive experiments showcase
the superiority and strong generalization capabilities of our
method. More importantly, we demonstrate that the uncer-
tainty given by our method is well-calibrated. Our method
will likely enable intriguing applications such as OOD de-
(a) CLIP
 (b) DINO
(c) MoCo
 (d) Ensemble
Figure 6. Visualization of prior kernel similarities.
tection by leveraging the uncertainty estimates. Overall,
our study demonstrates the exceptional power of Bayesian
methods in the large model era and aids in paving the path
for future algorithmic improvements in low-shot learning.
Acknowledgments
This work was supported by NSF of China (No.
62306176, 62106121), Natural Science Foundation of
Shanghai (No. 23ZR1428700), the Key Research and
Development Program of Shandong Province, China
(No. 2023CXGC010112), CCF-Baichuan-Ebtech Founda-
tion Model Fund, and the MOE Project of Key Research In-
stitute of Humanities and Social Sciences (22JJD110001).
23856
References
[1] Christopher M Bishop and Nasser M Nasrabadi. Pattern
recognition and machine learning . Springer, 2006. 4
[2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.
Food-101–mining discriminative components with random
forests. In Computer Vision–ECCV 2014: 13th European
Conference, Zurich, Switzerland, September 6-12, 2014,
Proceedings, Part VI 13 , pages 446–461. Springer, 2014. 5
[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in neural in-
formation processing systems , 33:1877–1901, 2020. 1
[4] Roberto Calandra, Jan Peters, Carl Edward Rasmussen, and
Marc Peter Deisenroth. Manifold gaussian processes for re-
gression. In 2016 International joint conference on neural
networks (IJCNN) , pages 3338–3345. IEEE, 2016. 2
[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In Pro-
ceedings of the IEEE/CVF international conference on com-
puter vision , pages 9650–9660, 2021. 1, 2
[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. IEEE transactions on pattern
analysis and machine intelligence , 40(4):834–848, 2017. 2
[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In International conference on ma-
chine learning , pages 1597–1607. PMLR, 2020. 1, 8
[8] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad
Norouzi, and Geoffrey E Hinton. Big self-supervised mod-
els are strong semi-supervised learners. Advances in neural
information processing systems , 33:22243–22255, 2020. 1
[9] Xinlei Chen, Saining Xie, and Kaiming He. An empiri-
cal study of training self-supervised vision transformers. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 9640–9649, 2021. 2
[10] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy
Mohamed, and Andrea Vedaldi. Describing textures in the
wild. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 3606–3613, 2014. 5
[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248–255. Ieee, 2009. 5, 6
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 1
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 2[14] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener-
ative visual models from few training examples: An incre-
mental bayesian approach tested on 101 object categories. In
2004 conference on computer vision and pattern recognition
workshop , pages 178–178. IEEE, 2004. 5
[15] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao
Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao.
Clip-adapter: Better vision-language models with feature
adapters. arXiv preprint arXiv:2110.04544 , 2021. 1, 2, 6
[16] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger.
On calibration of modern neural networks. In International
conference on machine learning , pages 1321–1330. PMLR,
2017. 2, 7
[17] Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzheng Ma, Xu-
peng Miao, Xuming He, and Bin Cui. Calip: Zero-shot
enhancement of clip with parameter-free attention. arXiv
preprint arXiv:2209.14169 , 2022. 2, 6
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 2
[19] Kaiming He, Ross Girshick, and Piotr Doll ´ar. Rethinking im-
agenet pre-training. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 4918–4927,
2019. 2
[20] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 16000–
16009, 2022. 1
[21] Patrick Helber, Benjamin Bischke, Andreas Dengel, and
Damian Borth. Eurosat: A novel dataset and deep learning
benchmark for land use and land cover classification. IEEE
Journal of Selected Topics in Applied Earth Observations
and Remote Sensing , 12(7):2217–2226, 2019. 5
[22] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-
hardt, and Dawn Song. Natural adversarial examples. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 15262–15271, 2021. 6
[23] Arthur Jacot, Franck Gabriel, and Cl ´ement Hongler. Neu-
ral tangent kernel: Convergence and generalization in neural
networks. Advances in neural information processing sys-
tems, 31, 2018. 2
[24] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representa-
tion learning with noisy text supervision. In International
Conference on Machine Learning , pages 4904–4916. PMLR,
2021. 1, 2
[25] Tianjun Ke, Haoqun Cao, Zenan Ling, and Feng Zhou.
Revisiting logistic-softmax likelihood in bayesian meta-
learning for few-shot classification. In Thirty-seventh Con-
ference on Neural Information Processing Systems , 2023. 3,
8
[26] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 4
23857
[27] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan
Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.
Big transfer (bit): General visual representation learning. In
Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part V 16 ,
pages 491–507. Springer, 2020. 1
[28] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
3d object representations for fine-grained categorization. In
Proceedings of the IEEE international conference on com-
puter vision workshops , pages 554–561, 2013. 5
[29] Malte Kuss. Gaussian process models for robust regression,
classification, and reinforcement learning . PhD thesis, ech-
nische Universit ¨at Darmstadt Darmstadt, Germany, 2006. 3
[30] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S
Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein.
Deep neural networks as gaussian processes. arXiv preprint
arXiv:1711.00165 , 2017. 2, 3
[31] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. arXiv
preprint arXiv:2301.12597 , 2023. 2
[32] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Doll ´ar. Focal loss for dense object detection. In Pro-
ceedings of the IEEE international conference on computer
vision , pages 2980–2988, 2017. 2
[33] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 10012–10022, 2021. 2
[34] Sanae Lotfi, Pavel Izmailov, Gregory Benton, Micah Gold-
blum, and Andrew Gordon Wilson. Bayesian model selec-
tion, the marginal likelihood, and generalization. In Inter-
national Conference on Machine Learning , pages 14223–
14247. PMLR, 2022. 3, 8
[35] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew
Blaschko, and Andrea Vedaldi. Fine-grained visual classi-
fication of aircraft. arXiv preprint arXiv:1306.5151 , 2013.
5
[36] Radford M Neal. Bayesian learning for neural networks,
1996. 2
[37] Hannes Nickisch and Carl Edward Rasmussen. Approxima-
tions for binary gaussian process classification. Journal of
Machine Learning Research , 9(Oct):2035–2078, 2008. 2
[38] Maria-Elena Nilsback and Andrew Zisserman. Automated
flower classification over a large number of classes. In 2008
Sixth Indian Conference on Computer Vision, Graphics &
Image Processing , pages 722–729. IEEE, 2008. 5
[39] Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang,
Ghassen Jerfel, and Dustin Tran. Measuring calibration in
deep learning. In CVPR workshops , 2019. 2, 7
[40] Sebastian W Ober, Carl E Rasmussen, and Mark van der
Wilk. The promises and pitfalls of deep kernel learning.
InUncertainty in Artificial Intelligence , pages 1206–1216.
PMLR, 2021. 3
[41] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and
CV Jawahar. Cats and dogs. In 2012 IEEE conference oncomputer vision and pattern recognition , pages 3498–3505.
IEEE, 2012. 5
[42] Massimiliano Patacchiola, Jack Turner, Elliot J Crowley,
Michael O’Boyle, and Amos J Storkey. Bayesian meta-
learning for the few-shot setting via deep kernels. Advances
in Neural Information Processing Systems , 33:16108–16118,
2020. 3
[43] Longtian Qiu, Renrui Zhang, Ziyu Guo, Ziyao Zeng, Yafeng
Li, and Guangnan Zhang. Vt-clip: Enhancing vision-
language models with visual-guided texts. arXiv preprint
arXiv:2112.02399 , 2021. 1, 6
[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 1, 2, 4
[45] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In International Confer-
ence on Machine Learning , pages 8821–8831. PMLR, 2021.
5
[46] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and
Vaishaal Shankar. Do imagenet classifiers generalize to im-
agenet? In International conference on machine learning ,
pages 5389–5400. PMLR, 2019. 6
[47] J ¨urgen Schmidhuber. Evolutionary principles in self-
referential learning, or on learning how to learn: the meta-
meta-... hook . PhD thesis, Technische Universit ¨at M ¨unchen,
1987. 2
[48] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 , 2014. 2
[49] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.
Ucf101: A dataset of 101 human actions classes from videos
in the wild. arXiv preprint arXiv:1212.0402 , 2012. 5
[50] Sebastian Thrun and Lorien Pratt. Learning to learn: Intro-
duction and overview. Learning to learn , pages 3–17, 1998.
2
[51] Vishaal Udandarao, Ankush Gupta, and Samuel Albanie.
Sus-x: Training-free name-only transfer of vision-language
models. arXiv preprint arXiv:2211.16198 , 2022. 2
[52] Christopher Williams and Carl Rasmussen. Gaussian pro-
cesses for regression. Advances in neural information pro-
cessing systems , 8, 1995. 3
[53] Christopher KI Williams and Carl Edward Rasmussen.
Gaussian processes for machine learning . MIT press Cam-
bridge, MA, 2006. 1, 2, 4
[54] Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov,
and Eric P Xing. Deep kernel learning. In Artificial intelli-
gence and statistics , pages 370–378. PMLR, 2016. 2
[55] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,
and Antonio Torralba. Sun database: Large-scale scene
recognition from abbey to zoo. In 2010 IEEE computer so-
ciety conference on computer vision and pattern recognition ,
pages 3485–3492. IEEE, 2010. 5
23858
[56] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao,
Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.
Tip-adapter: Training-free clip-adapter for better vision-
language modeling. arXiv preprint arXiv:2111.03930 , 2021.
1, 2, 6
[57] Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Han-
qiu Deng, Hongsheng Li, Yu Qiao, and Peng Gao. Prompt,
generate, then cache: Cascade of foundation models makes
strong few-shot learners. arXiv preprint arXiv:2303.02151 ,
2023. 1, 2, 5
[58] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
Liu. Conditional prompt learning for vision-language mod-
els. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 16816–16825,
2022. 1
[59] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
Liu. Learning to prompt for vision-language models. In-
ternational Journal of Computer Vision , 130(9):2337–2348,
2022. 1, 2, 6
23859
