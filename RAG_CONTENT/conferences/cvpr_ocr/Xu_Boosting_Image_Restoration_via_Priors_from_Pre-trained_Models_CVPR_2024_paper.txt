Boosting Image Restoration via Priors from Pre-trained Models
Xiaogang Xu1,2,3Shu Kong5,6,7Tao Hu3,8Zhe Liu1*Hujun Bao1,4
1Zhejiang Lab2CUHK3RealityEdge4Zhejiang University5University of Macau
6Institute of Collaborative Innovation7Texas A&M University8National University of Singapore
xiaogangxu00@gmail.com, skong@um.edu.mo, yihouxiang@gmail.com
zhe.liu@zhejianglab.com, bao@cad.zju.edu.cn
Abstract
Pre-trained models with large-scale training data, such
as CLIP and Stable Diffusion, have demonstrated remark-
able performance in various high-level computer vision
tasks such as image understanding and generation from
language descriptions. Yet, their potential for low-level
tasks such as image restoration remains relatively unex-
plored. In this paper, we explore such models to enhance
image restoration. As off-the-shelf features (OSF) from pre-
trained models do not directly serve image restoration, we
propose to learn an additional lightweight module called
Pre-Train-Guided Refinement Module (PTG-RM) to refine
restoration results of a target restoration network with OSF .
PTG-RM consists of two components, Pre-Train-Guided
Spatial-Varying Enhancement (PTG-SVE), and Pre-Train-
Guided Channel-Spatial Attention (PTG-CSA). PTG-SVE
enables optimal short- and long-range neural operations,
while PTG-CSA enhances spatial-channel attention for
restoration-related learning. Extensive experiments demon-
strate that PTG-RM, with its compact size (<1M parame-
ters), effectively enhances restoration performance of var-
ious models across different tasks, including low-light en-
hancement, deraining, deblurring, and denoising.
1. Introduction
Image restoration plays a vital role in real-world scenar-
ios, aiming to reconstruct high-quality images by elimi-
nating degradations. It has broad applications in various
fields, such as denoising [43, 44] and low-light enhance-
ment [41, 42] for improving smartphone-captured pho-
tos. While effective restoration networks have been pro-
posed [19, 52], the inherently ill-posed nature of image
restoration makes it challenging to achieve significant im-
provements by merely modifying network structures. Sim-
ply increasing model parameters does not guarantee better
*Corresponding author.
MPRNet(CVPR2021)MPRNet +Ours
Uformer(CVPR2022)Uformer +OursRestormer
(CVPR2022)Restormer +Ours
39.639.84040.240.4
0.957 0.96 0.963 0.966UHD(AAAI2023)UHD +Ours
URetinex
(CVPR2022)URetinex +Ours
SNR
(CVPR2022)SNR +Ours
1921232527
0.7 0.75 0.8 0.85 0.9SPAIR(ICCV2021)SPAIR +OursRestormer (CVPR2022)Restormer +Ours
3333.634.234.835.4
0.935 0.94 0.945 0.95 0.955
MPRNet(CVPR2021)MPRNet +OursRestormer
(CVPR2022)Restormer +Ours
32.432.73333.333.6
0.956 0.96 0.964 0.968SSIM SSIM
SSIM SSIMDeraining on Test2800 Low -Light Enhancement on LOL -real
Motion Deblurring on GoPro Real -Image Denoising on SIDD
PSNRPSNR
PSNRPSNRFigure 1. Our method leverages pre-trained models, such as CLIP
and Stable Diffusion, and significantly improves image restoration
across various tasks. More results on different tasks/models can be
seen in experiments. Pre-trained models are involved during the
training and not required during the inference.
results, as the model may tend to overfit to the training data.
Restoration performance relies on strong image pri-
ors, such as the novel level of denoising [38] or the
blur kernel in deblurring [14, 50]. However, estimating
these priors is challenging, especially with real-world data.
Some approaches utilize physical variables as priors, like
depth information [46] and semantic features [1, 36,41]
derived from pre-trained networks. Nevertheless, these
physical variables are not robust enough since the dense
depth/semantic prediction networks do not have sufficient
generalization ability among different scenes in restoration
tasks. As a result, employing them requires complex and
specific mechanisms, limiting their applicability across var-
ious tasks. In this paper, we propose a novel approach that
extracts degradation-related information from pre-trained
models (with various training objectives) exposed to dif-
ferent degradation during pre-training, all without requiring
explicit annotations.
Motivation. Two types of pre-trained models may
contain degradation-related information during training:
restoration models, and pre-trained models on large-scale
data (e.g., CLIP [27], BLIP [16], and BLIP2 [17]). Using
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
2900
Initial	RestorationRefined Restoration
Using a pre-trained model 𝒢 to boost image restoration
CLIP
Stable DiffusionBLIP2….
Low-Light 
EnhancementDeraining
Motion 
DeblurringDefocus Deblurring
Gaussian Denoising
 Real 
Denoising
ℛ- ℛ.PTG -CSA
Restoration Model ℱPretrained Model 𝒢 Pre-Training Guided Refinement Module (PTG- RM)
PTG -SVE
𝑓̅𝑓%𝑓
Degraded Image 𝐼 !𝑔Figure 2. We present a lightweight plugin, pre-training guided refining module (PTG-RM), to leverage pre-trained models for enhancing
image restoration. The desired prior is the OFS G(Id). It has two components, PTG spatial varying enhancement (PTG-SVE), and PTG
channel-spatial attention (PTG-CSA). Fig. 3depicts their details. Our PTG-RM significantly improves restoration in various tasks as listed
in the top-right (see quantitative results previewed in Fig. 1).
the former is evident, but models trained with some types
of degradation may not effectively help restore images with
other types of degradation. Using the latter remains un-
explored. CLIP-IQA [33] finds that CLIP features contain
degradation-related information and so be useful for image
assessment, while no restoration approaches have been pro-
posed yet. Existing pre-trained multi-modality models may
have been trained on various degraded images. Presumably,
restoration-related annotations are unavailable during pre-
training, their resulted features likely contain valuable in-
formation for image restoration. The key is to leverage such
information to help the target restoration learning. How-
ever, the heterogeneity of pre-trained models and restora-
tion models poses difficulties in using the off-the-shelf fea-
tures extracted from pre-trained models.
Technical novelty. We introduce a novel pre-training
guided refinement module (PTG-RM) that leverages off-
the-shelf features (OSF) computed by a pre-trained model
Gto improve image restoration tasks. The PTG-RM Ris
a lightweight plugin (Fig. 2) (additional Rhas<1M pa-
rameters in total). PTG-RM enables us to determine op-
timal operation ranges and spatial-channel attention, thus
facilitating image restoration. It takes as input the initially
enhanced image from F, the input image, and its OSF ex-
tracted by a pre-trained model. It is trained with F(using
the same loss as F) and adaptively enhances it. PTG-RM
Rconsists of two components: Pre-Train-Guided Spatial
Varying Enhancement (PTG-SVE), and Pre-Train-Guided
Channel-Spatial Attention (PTG-CSA).
PTG-SVE employs spatial-varying operations to refine
the initially enhanced results differently from region to re-
gion. Unlike previous methods [42] that rely on fixed ref-
erences to determine optimal operation ranges, we establish
a spatial-aware learnable mapping for OSF and utilize the
mapped features as spatial-wise guidance. This adaptivelyfuses the features extracted from short- and long-range op-
erations, allowing different regions to be refined appropria-
tel and yielding more effective enhancement.
Following PTG-SVE, PTG-CSA further enhances the re-
sults by formulating effective channel- and spatial-attention
with OSF. We note that different areas may require varying
degrees of feature correctness via the attention mechanism.
Hence, we propose to generate spatial-varying convolution
kernels to synthesize the spatial weights. Our approach tai-
lors the attention process to different regions.
Contributions. We make three major contributions.
• We present a novel and general method that leverages pre-
trained models to enhance various restoration tasks. Our
work opens up possibilities for improving performance
across various domains.
• We propose a novel paradigm that leverages pre-trained
priors to formulate effective neural operation ranges and
attention mechanisms.
• We validate our method through extensive experiments on
different datasets, networks, and tasks, and show remark-
able improvements over prior methods (cf. Fig. 1).
2. Related Work
Image Priors for Restoration. Different restoration tasks
demand distinct image priors, such as noise levels for de-
noising and blurring kernels for deblurring. Due to the
ill-posed nature of restoration, estimating priors is diffi-
cult. In real-world scenarios, these priors are typically in-
tertwined, adding further complexity to the restoration pro-
cess. Recent literature introduces several methods to im-
prove restoration by leveraging multi-modal maps as unified
priors. These methods predominantly rely on pre-computed
physical multi-modal maps. For instance, SKF [41] uses
semantic maps to optimize the feature space for low-light
enhancement. SMG [46] employs a generative framework
2901
t o i nt e gr at e e d g e, d e pt h, a n d s e m a nti c i nf or m ati o n, e n h a n c- 
i n g t h e i niti al a p p e ar a n c e m o d eli n g f or l o w-li g ht s c e n ari o s. 
A d diti o n all y , s o m e a p pr o a c h e s u s e N e ar-I nfr ar e d ( NI R) i n- 
f or m ati o n t o r e ﬁ n e i m a gi n g r e s ult s [ 1 2 ,3 2 ]. T h e s e pri or s 
ar e al s o a p pli e d t o ot h er r e st or ati o n t a s k s, s u c h a s i m a g e 
d e n oi si n g [ 2 0 ] a n d d er ai ni n g [ 1 8 ]. H o w e v er , ali g ni n g t h e s e 
pri or s wit h t h e i n p ut i m a g e c a n b e c h all e n gi n g, a n d err or s 
i n t h e pri or s m a y a d v er s el y i m p a ct p erf or m a n c e. Diff er- 
e nt fr o m e xi sti n g w or k s, w e pr o p o s e t o l e v er a g e pr e-tr ai n e d 
m o d el s a s pri or s t o e n h a n c e i m a g e r e st or ati o n. 
P r e- Tr ai n e d M o d el s f o r D o w n st r e a m T a s k s. R e- 
c e ntl y , a s eri e s of pr e-tr ai n e d m o d el s wit h l ar g e- s c al e tr ai n- 
i n g d at a s et s h a v e e m er g e d, p arti c ul arl y i n t h e f or m of 
m ulti- m o d al m o d el s s u c h a s C LI P [ 2 7 ], B LI P [ 1 6 ], a n d 
B LI P 2 [ 1 7 ]. T h e f e at ur e s p a c e l e ar n e d b y t h e s e m o d el s of- 
f er s ri c h k n o wl e d g e t h at c a n b e n e ﬁt v ari o u s t a s k s. W hil e 
pr e vi o u s w or k h a s d e m o n str at e d t h e eff e cti v e n e s s of C LI P 
i n hi g h-l e v el t a s k s li k e z er o- s h ot cl a s si ﬁ c ati o n [ 6,5 3 ], i m- 
a g e e diti n g [ 4,2 5 ], o p e n- w orl d s e g m e nt ati o n [ 3 9 ,6 0 ], a n d 
3 D cl a s si ﬁ c ati o n [ 4 7 ,5 9 ], it s p ot e nti al f or ai di n g l o w-l e v el 
r e st or ati o n t a s k s r e m ai n s u n e x pl or e d. O nl y t h e c a p a bilit y 
of e m pl o yi n g s u c h f or i m a g e q u alit y a s s e s s m e nt, a s d e m o n- 
str at e d i n C LI P-I Q A, h a s b e e n e x pl or e d. We pr o p o s e a g e n- 
er al fr a m e w or k t o l e v er a g e pr e-tr ai n e d m o d el s t o i m pr o v e 
v ari o u s r e st or ati o n t a s k s. 
3. M et h o d s 
B a c k g r o u n d. L et Idr e pr e s e nt a d e gr a d e d i m a g e, a n d Ic
d e n ot e t h e c orr e s p o n di n g gr o u n d-tr ut h ( wit h o ut d e gr a d a- 
ti o n). A r e st or ati o n n et w or k F pr o d u c e s r e st or e d i m a g e 
ˆIc=F(Id). D e s pit e t h e e xi st e n c e of v ari o u s eff e cti v e n et- 
w or k str u ct ur e s Ft h at h a v e b e e n pr o p o s e d, t h er e ar e c urr e nt 
u p p er b o u n d s i n t h e s e t a s k s. Br e a ki n g t hr o u g h t h e s e b o u n d s 
oft e n r e q uir e s d e si g ni n g m or e c o m pl e x n et w or k s a n d tr ai n- 
i n g str at e gi e s, w hi c h c a n b e ar d u o u s. A d diti o n all y , i n n o v a- 
ti o n s i n n et w or k ar c hit e ct ur e or tr ai ni n g str at e gi e s f or o n e 
t a s k mi g ht n ot tr a n sl at e t o a n ot h er . W hil e diff er e nt pri or s g
h a v e b e e n i ntr o d u c e d i nt o t h e r e st or ati o n pr o c e s s, i n cl u di n g 
i m a g e a n d p h y si c al pri or s, e sti m ati n g t h e s e pri or s i s dif ﬁ- 
c ult. 
M oti v ati o n. We h y p ot h e si z e t h at t h e pri or gc a n b e ef- 
f e cti v el y r e pr e s e nt e d a s t h e f e at ur e e xtr a ct e d fr o m v ari o u s 
pr e-tr ai n e d m o d el s G, a s g=G(Id). N ot e t h at Gi s t y pi c all y 
n ot tr ai n e d wit h r e st or ati o n t ar g et s b ut mi g ht h a v e b e e n e x- 
p o s e d t o i m a g e s wit h di v er s e d e gr a d ati o n s. S o it i s li k el y 
t o l e ar n u s ef ul i nf or m ati o n t o h el p i m a g e r e st or ati o n. We 
pr o p o s e a n o v el a p pr o a c h t h at u s e s gt o i m pr o v e t h e i ni- 
ti al r e st or ati o n b y F, e v e n if t h e s e n et w or k s h a v e alr e a d y 
r e a c h e d t h eir c urr e nt u p p er b o u n d s. 
C h all e n g e. U si n g gt o a s si st Fi s n o n-tri vi al. Pri m aril y , 
t h e f e at ur e gi s n ot i n h er e ntl y ali g n e d wit h t h e r e st or ati o n 
t a s k s b e c a u s e t h e y mi g ht r e pr e s e nt diff er e nt a s p e ct s. F or i n st a n c e, f e at ur e s fr o m C LI P f o c u s m or e o n s e m a nti c i nf or- 
m ati o n, m a ki n g dir e ct ali g n m e nt t o r e st or ati o n c h all e n gi n g. 
M or e o v er , t h e s e pri or s e x hi bit v ar yi n g s h a p e s, s u c h a s t h e 
o n e- di m e n si o n al ( 1 D) f e at ur e s fr o m t h e C LI P m o d el, w hil e 
t h e f e at ur e s i n Far e t y pi c all y 2 D. T o r e c o n cil e t h e di s cr e p- 
a n ci e s i n b ot h r e pr e s e nt ati o n a n d s h a p e, w e pr o p o s e a r e- 
ﬁ n e m e nt m o d ul e R t o r e ﬁ n e t h e i niti al r e st or ati o n b y F.
T hi s eli mi n at e s t h e n e e d t o ali g n gt o di sti n ct f e at ur e s of F
a n d all o w s f or a u ni ﬁ e d 1 D r e pr e s e nt ati o n f or g. F urt h er- 
m or e, w e i ntr o d u c e a n o v el a p pr o a c h t o utili z e gt o f or m u- 
l at e o pti m al n e ur al o p er ati n g r a n g e s vi a a n eff e cti v e att e n- 
ti o n m e c h a ni s m i n R. T hi s i m pli citl y di still s r e st or ati o n- 
r el at e d i nf or m ati o n, eff e cti v el y b o o sti n g t h e ﬁ n al p erf or- 
m a n c e. 
3. 1. O v e r vi e w of R e ﬁ n e m e nt M o d ul e 
Fi g. 2d e pi ct s t h e r e st or ati o n pi p eli n e u si n g o ur m et h o d. 
Gi v e n a n i n p ut i m a g e Id, w e h a v e a n i niti al r e st or ati o n r e- 
s ult a s ˆIc= F(Id). We ai m t o r e ﬁ n e t h e r e s ult u si n g 
t h e pr o p o s e d pr e-tr ai ni n g g ui d e d r e ﬁ n e m e nt m o d ul e ( P T G- 
R M) R, r e s ulti n g i n ¯Ic=R(ˆIc, I d, g ). T h e k e y of t hi s a p- 
pr o a c h i s t o di still r e st or ati o n-r el at e d i nf or m ati o n fr o m t h e 
pri or g.
R i s a si m pl e e n c o d er- d e c o d er str u ct ur e. T h e e n c o d er 
a n d d e c o d er of R ar e d e n ot e d a s R ea n d R d, r e s p e c- 
ti v el y . T o e n s ur e li g ht w ei g ht i m pl e m e nt ati o n, di still ati o n 
o c c ur s i n t h e l at e nt s p a c e, a v oi di n g t h e n e e d t o ali g n gwit h 
r e st or ati o n-r el at e d f e at ur e s. T h e l at e nt f e at ur e fi s d eri v e d 
t hr o u g h a c o m p ari s o n b et w e e n t h e i niti al e n h a n c e d r e s ult s 
a n d t h e ori gi n al i n p ut i m a g e s, gi v e n a s f= R e(ˆIc⊕Id),
w h er e ⊕d e n ot e s t h e c o n c at e n ati o n o p er ati o n. T h e r e s ult- 
i n g fi s i n Rh×w×c, wit h h,w, a n d cr e pr e s e nti n g f e a- 
t ur e h ei g ht, wi dt h, a n d c h a n n el n u m b er , r e s p e cti v el y . T h e 
pri or s ar e u s e d i n f urt h er l e ar ni n g t h e l at e nt f e at ur e a s 
¯f=C(A(f, g ), g ), w h er e Aa n d Cr e pr e s e nt t h e Pr e- Tr ai n- 
G ui d e d S p ati al- Var yi n g E n h a n c e m e nt ( P T G- S V E) a n d Pr e- 
Tr ai n- G ui d e d C h a n n el- S p ati al Att e nti o n ( P T G- C S A) m o d- 
ul e s, r e s p e cti v el y . T h e ﬁ n al e n h a n c e m e nt i s o bt ai n e d fr o m 
t h e d e c o d er a s [Im, I r] = R d(ˆf), c o m pri si n g t w o c o m p o- 
n e nt s. T h e ﬁr st c o m p o n e nt, Im, r e pr e s e nt s t h e c orr e cti o n 
m a s k u s e d t o miti g at e err or s i n t h e i niti al e n h a n c e m e nt r e- 
s ult s. T h e s e c o n d c o m p o n e nt, Ir, i s t h e r e si d u al r e ﬁ n e m e nt 
t h at a d dr e s s e s artif a ct s a n d a d d s a d diti o n al d et ail s. T h e ﬁ n al 
r e s ult i s d e n ot e d a s 
¯Ic=Id+ ( ˆIc−Id)×Im+Ir. ( 1) 
3. 2. P r e- Tr ai n- G ui d e d S p ati al- V a r yi n g O p e r ati o n s 
I n P T G- S V E, w e ar g u e t h at g= G(Id)m a y c o nt ai n i nf or- 
m ati o n r e ﬂ e cti n g t h e pi x el-l e v el i m a g e q u alit y of Id. F or ar- 
e a s wit h p o or q u alit y , l o n g-r a n g e o p er ati o n s ar e u s e d t o c a p- 
t ur e n o n-l o c al f e at ur e s, w hil e r e gi o n s wit h r el ati v el y g o o d 
q u alit y pri oriti z e l o c al f e at ur e s f or a c c ur at e r e st or ati o n. 
2902
P o oli n g 𝒫 ×∗
(a) Pre -Trai n -G ui de d S patial -Var yi n g E n ha nce me nt  𝒜P ositi o nal E m be d di n g  𝒮!
( b) Pre -Trai n -G ui de d C ha n nel -S patial Atte nti o n 𝒞ℛ!𝑀"
𝑀#ℛ$
%𝑓#ℛ%̅𝑓ℛ"O S F  𝑔
Feat ure  fr o m  ℛ&: 𝑓ℛ'𝑀S patial -var yi n g Wei g hts 
O S F  𝑔%𝑓O ut p ut 
Feat ure P ositi o nal 
E m be d di n g  𝒮! ℛ(
ℛ#S patial Atte nti o n Ma p 
E n ha nce d 
Feat ure S patial -var yi n g  
Ker nels  𝐶( ×
%𝑓"S h ort -ra n ge  
Feat ure  𝑓"
L o n g -ra n ge  
Feat ure  𝑓'
Fi g ure 3. T he pi peli ne of P T G- S V E a n d P T G- C S A. I n P T G- S V E, we use t he lear na ble s patial e m be d di n g Sm, O S F g, a n d i n p ut feat ure f
t o a da pti vel y f or m ulate s patial wei g hts ( M, E q. 2) f or f usi n g s h ort- a n d l o n g-ra n ge pr ocesse d feat ures ( fsa n d fl) via o perati o ns Rsa n d 
Rl, yiel di n g ˆf( E q. 3). I n P T G- C S A, O S F gc o n diti o ns c ha n nel atte nti o n Mcf or ˆft hr o u g h Rc( E q. 4). A d diti o nall y, gc o m bi nes wit h 
lear na ble s patial re prese ntati o n Sca n d ˆft o ge nerate s patial atte nti o n ma p Ms, usi n g s patial- wise c o n v ol uti o ns Cp( o btai ne d via Rp) t o 
deri ve ˆMst hat is f urt her pr ocesse d wit h Ro( E qs. 5a n d 6). C ha n nel- a n d s patial-atte nti o n o ut p uts ( ˆfca n d ˆfs) mer ge via Rft o e n ha nce 
feat ure ¯f( E q. 7). 
I n Fi g. 3, t he pri mar y o bjecti ve is t o pre dict t he o pti mal 
ne ural o perati o n ra n ge f or eac h l ocati o n of t he feat ure ma p 
f, w hic h we refer t o as t he “ra n ge sc ore ma p”, de n ote d as 
M. To e ns ure a ge neral Rwit h u ni ﬁe d 1 D pri ors gfr o m 
vari o us m o dels, we pr o p ose a d di n g l ocati o n-a ware e m be d- 
di n gs f or t he pri ors, t here b y a da pti vel y disc o veri n g q ual- 
it y i nf or mati o n f or differe nt pi xels. Let S={(x, y )|x∈
[ 1 , w ], y ∈[ 1 , h ]}re prese nt t he 2 D c o or di nate ma p wit h di- 
me nsi o ns h×w×2. We use a p ositi o n e m be d di n g m o d ule P
t o ge nerate s patial re prese ntati o n, de n ote d as Sm=P(S),
w here S ∈ Rh×w×c. F urt her m ore, t o deter mi ne t he a d- 
mire d ne ural o perati o n ra n ge f or eac h l ocati o n of f, we use 
a lear na ble ma p pi n g f u ncti o n Tmt o tra nsf or m t he pri ors t o 
a n ot her s pace t hat ca n m ore effecti vel y deci de t he o pti mal 
ra n ge. To o btai n M, we use a ra n ge-lear ni n g m o d ule Rm,
w hic h ta kes t he e nc o der’s feat ure f, t he pre-trai ne d pri or g,
a n d t he s patial re prese ntati o n Smas i n p uts. T he pr oce d ure 
is de n ote d as 
M=Rm(f⊕ T m(g)⊕ S m). ( 2) 
F oll o wi n g [ 4 2 ], we use C N N f or t he s h ort-ra n ge o per- 
ati o n, de n ote d as Rs, a n d tra nsf or mer f or t he l o n g-ra n ge 
o perati o n, re prese nte d as Rl. S peci ﬁcall y, we e m pl o y t he 
Rest or mer bac k b o ne f or Rla n d Res Net f or Rs. S u p p ose 
t he feat ures after t he s h ort- a n d l o n g-ra n ge o perati o n are fs
a n d fl, res pecti vel y. We ca n o btai n t he re ﬁ ne d feat ure ˆfas 
fs=Rs(f), f l=Rl(f),ˆf=M×fs+( 1 −M)×fl.( 3) 
T he pre vi o us a p pr oac h [ 4 2 ] relies o n pre-c o m p ute d S N R 
val ues, w hic h ma y n ot al wa ys be acc urate a n d ca n fail t o 
e n ha nce res ults, es peciall y w he n t he i nitial res ults fr o m F
ha ve reac he d t heir u p per b o u n d. I n c o ntrast, o ur sc ore ra n ge 
ma p is lear ne d o nli ne base d o n t he i n p ut i ma ge, rest orati o n- 
relate d pri ors, a n d e x plicit s patial feat ures t hat are lear na ble. 
T his ﬂe xi bilit y all o ws us t o ha n dle vari o us sit uati o ns, re- 
s ulti n g i n better perf or ma nce a n d ge neralizati o n (as de m o n- 
strate d i n t he a blati o n st u d y). 3. 3. Pre- Tr ai n- G ui de d Atte nti o n 
As s h o w n i n Fi g. 3, we f urt her i ntr o d uce a li g ht wei g ht c o m- 
p o ne nt t hat utilizes pre-trai ne d pri ors gt o create a n effecti ve 
atte nti o n mec ha nis m i n R. O pti mizi n g t he feat ure atte nti o n 
i n Ris cr ucial f or effecti vel y i de ntif yi n g hel pf ul feat ures 
t o e n ha nce t he i nitial res ults ˆIc. T his i n v ol ves b ot h s patial- 
le vel a n d c ha n nel-le vel atte nti o ns. T he hi d de n rest orati o n- 
relate d i nf or mati o n i n gca n be disc o vere d b y usi n g gt o 
i m pr o ve t he rest orati o n feat ures i n Rc o n diti o ne d o n t he m. 
We be gi n b y f or m ulati n g t he atte nti o n c o m p utati o n at t he 
c ha n nel le vel. We i ntr o d uce a ma p pi n g f u ncti o n Tct o tra ns- 
f or m gi nt o t he atte nti o n- pre dicti o n s pace, a n d utilize t he 
c ha n nel atte nti o n c o m p utati o n m o d ule Rc. T he f or m ula- 
ti o n of t he c ha n nel atte nti o n is 
Mc=Rc(O(ˆf)⊕ T c(g)) ,ˆfc=ˆf×Mc, ( 4) 
w here Ois t he p o oli n g o perati o n, a n d Mc∈Rc.
As f or t he s patial-atte nti o n c o m p utati o n, we utilize t he 
1 D pre-trai ne d pri or gt o pre dict l ocati o n- wise atte nti o n 
base d o n t he feat ure distri b uti o n of eac h l ocati o n i n ˆf. Si m- 
pl y usi n g t he s patial l ocati o n i nf or mati o n, as s h o w n i n E q. 2,
res ults i n eac h pi xel’s feat ure c o nsi deri n g a si milar c o n di- 
ti o n f or nei g h b ori n g feat ures, li miti n g t he eli mi nati o n of 
s patial artifacts. T heref ore, we pr o p ose a n alter nati ve strat- 
e g y b y pre dicti n g t he ne ural o perati o n para meters f or eac h 
l ocati o n, o pti mizi n g t he s patial atte nti o n base d o n t he var y- 
i n g l ocati o n- wise feat ure distri b uti o n. We de n ote t he s patial 
atte nti o n c o m p utati o n m o d ule as Rp, a n d ﬁrst f or m ulate t he 
l ocati o n- wise c o n v ol uti o n ma p, as 
Cp=Rp(ˆf, Tc(g),Sc), ( 5) 
w here t he o btai ne d c o n v ol uti o n  ma p Cp ∈
Rh×w×(kh×kw×c),kha n d kware t he c o n v ol uti o n ker- 
nel size, a n d Scis a n ot her lear na ble p ositi o n e m be d di n g 
here. T he o btai ne d c o n v ol uti o n ma ps ca n be utilize d t o 
o pti mize t he feat ure, a n d s patial atte nti o n ca n be o btai ne d 
as 
ˆMs=ˆf∗ C p, M s=Ro(ˆMs), ( 6) 
2903
D at asets Met h o ds Ori gi n al + O urs-c + O urs- b + O urs-s + O urs-r + O urs-f 
P S N R S SI M P S N R S SI M P S N R S SI M P S N R S SI M P S N R S SI M P S N R S SI M 
L O L U H D 1 9. 8 7 0. 7 0 6 2 2. 9 1 ( + 3. 0 4 ) 0. 7 6 7 ( + 6. 1 )2 1. 8 3 ( + 1. 9 6 ) 0. 7 3 2 ( + 2. 6 )2 2. 3 5 ( + 2. 4 8 ) 0. 7 5 8 ( + 5. 2 )2 1. 7 1 ( + 1. 8 4 ) 0. 7 3 7 ( + 3. 1 )2 2. 7 4 ( + 2. 8 7 ) 0. 7 6 4 ( + 5. 8 )
U Reti ne x 2 1. 1 6 0. 8 4 0 2 4. 7 0 ( + 3. 5 4 ) 0. 8 7 8 ( + 3. 8 )2 3. 5 7 ( + 2. 4 1 ) 0. 8 6 9 ( + 2. 9 )2 4. 2 3 ( + 3. 0 7 ) 0. 8 6 6 ( + 2. 6 )2 3. 9 9 ( + 2. 8 3 ) 0. 8 6 2 ( + 2. 2 )2 4. 5 6 ( + 3. 4 0 ) 0. 8 7 0 ( + 3. 0 )
S N R 2 1. 4 8 0. 8 4 9 2 5. 5 0 ( + 4. 0 2 ) 0. 8 9 2 ( + 4. 3 )2 5. 6 1 ( + 4. 1 3 ) 0. 8 9 1 ( + 4. 2 )2 5. 1 9 ( + 3. 7 1 ) 0. 8 7 4 ( + 2. 5 )2 5. 2 4 ( + 3. 7 6 ) 0. 8 8 7 ( + 3. 8 )2 4. 9 0 ( + 3. 4 2 ) 0. 8 8 8 ( + 3. 9 )
SI D U H D 2 0. 4 6 0. 6 1 4 2 0. 9 9 ( + 0. 5 3 ) 0. 6 1 6 ( + 0. 2 )2 1. 0 6 ( + 0. 6 0 ) 0. 6 1 9 ( + 0. 5 )2 2. 3 4 ( + 1. 8 8 ) 0. 6 2 5 ( + 1. 1 )2 1. 1 1 ( + 0. 6 5 ) 0. 6 1 8 ( + 0. 4 )2 1. 0 8 ( + 0. 6 2 ) 0. 6 1 9 ( + 0. 5 )
U Reti ne x 2 1. 5 6 0. 6 1 9 2 2. 3 4 ( + 0. 7 8 ) 0. 6 2 3 ( + 0. 4 )2 2. 0 2 ( + 0. 4 6 ) 0. 6 2 1 ( + 0. 2 )2 2. 2 1 ( + 0. 6 5 ) 0. 6 2 3 ( + 0. 4 )2 2. 1 7 ( + 0. 6 1 ) 0. 6 2 5 ( + 0. 6 )2 2. 4 0 ( + 0. 8 4 ) 0. 6 2 6 ( + 0. 7 )
S N R 2 2. 8 7 0. 6 2 5 2 3. 3 4 ( + 0. 4 7 ) 0. 6 3 0 ( + 0. 5 )2 3. 1 5 ( + 0. 2 8 ) 0. 6 2 7 ( + 0. 2 )2 3. 0 8 ( + 0. 2 1 ) 0. 6 3 1 ( + 0. 6 )2 3. 0 6 ( + 0. 1 9 ) 0. 6 3 2 ( + 0. 7 )2 3. 1 7 ( + 0. 3 0 ) 0. 6 3 6 ( + 1. 1 )
Ta ble 1. C o m paris o ns o n L O L-real a n d SI D dataset. −c,−b,−s, a n d −rrefer t o usi n g C LI P , B LI P 2, Sta ble Diff usi o n, a n d rest orati o n 
m o dels trai ne d o n S D S D, res pecti vel y. −fde n otes a p pl yi n g re ﬁ ne me nt o n t he feat ures of F. ( +) i n dicates i m pr o ve me nts f or P S N R a n d 
S SI M ( x 1 0 0 ) .
Met h o ds S N R + S K F + S M G + S M G( de p) + O urs-c 
P S N R 2 1. 4 8 2 3. 0 5 2 4. 8 4 2 4. 1 2 2 5. 5 0 
S SI M 0. 8 4 9 0. 8 5 3 0. 8 8 0 0. 8 5 1 0. 8 9 2 
Met h o ds U Reti ne x + S K F + S M G + S M G( de p) + O urs-c 
P S N R 2 1. 1 6 2 3. 5 1 2 3. 7 4 2 3. 2 5 2 4. 7 0 
S SI M 0. 8 4 0 0. 8 5 6 0. 8 5 2 0. 8 4 9 0. 8 7 8 
+ Para ms 0 2. 1 5 M 1 6. 7 6 M 1 6. 7 6 M 0. 6 7 M 
Ta ble 2.  Q ua ntitati ve c o m paris o n o n t he L O L-real dataset. 
+ Para ms mea ns t he a d diti o nal para meter n u m ber c o m pare d wit h 
ori gi nal F.
w here ∗is t he c o n v ol uti o n o perati o n f or eac h l ocati o n, a n d 
Rois a n ot her lear na ble o perati o n w hic h ma p ps t he feat ure 
c ha n nel ct o 1, eli mi nati n g t he i n ﬂ ue nce fr o m t he c ha n nel- 
le vel de pe n de nc y. F urt her, t he feat ure after s patial atte nti o n 
ca n be descri be d as ˆfs=ˆf×Ms.
T he feat ures after s patial a n d c ha n nel atte nti o ns ca n be 
mer ge d via a f usi o n m o d ule as 
¯f=Rf(ˆfc⊕ˆfs), ( 7) 
w here Rfde n otes t he f usi n m o d ule. T he o btai ne d feat ure 
¯fca n be pr ocesse d via a dec o der Rdt o o btai n t he resi d ual 
re ﬁ ne me nt Ira n d t he mas k Imas i n dicate d i n E q. 1.
3. 4. L oss F u ncti o n 
O ur desi g ne d Rca n be j oi ntl y trai ne d wit h t he m o del F.
S u p p ose t he paire d gr o u n d tr ut h f or t he i n p ut i ma ge Idis 
Ic, a n d t he l oss f u ncti o n f or t he m o del Fis de n ote d as 
Lg(ˆIc,Ic)(is us uall y t he rec o nstr ucti o n l oss i n t he pi xel 
le vel or perce pt ual l oss, a n d ca n als o be t he u ns u per vise d 
l oss), t he n t he l oss f u ncti o n f or t he re ﬁ ne me nt m o d ule ca n 
be writte n as Lg(¯Ic,Ic). I n s u m mar y, t he o verall l oss is 
Lg(ˆIc,Ic) + λ1Lg(¯Ic,Ic), ( 8) 
w here λ1is t he l oss wei g ht a n d re mai ns r o b ust acr oss var- 
i o us tas ks a n d net w or ks (i n o ur e x peri me nts, λ1is al wa ys 
set as 1). 
4. E x peri me nts 
We ﬁrst i ntr o d uce tas ks a n d datasets use d i n e x peri me nts, 
f oll o we d b y a detaile d a nal ysis of o ur met h d usi n g l o w-li g ht 
i ma ge e n ha nce me nt as a n e xa m ple. We als o de m o nstrate 
t he effecti ve ness of o ur met h o d o n ot her tas ks. I n p ut S N R S N R + O urs Gr o u n d-tr ut h 
I n p ut S N R S N R + O urs Gr o u n d-tr ut h 
Fi g ure 4. C o m paris o ns o n L O L-real (t o p) a n d SI D ( b ott o m). Re- 
s ults wit h “ O urs” ha ve less n oise a n d clearer visi bilit y. 
4. 1. Tas ks a n d D at asets 
F or l o w-li g ht e n ha nce me nt, we use t he SI D [ 5] a n d L O L- 
real [ 4 9 ] datasets. F or derai ni n g, we use t he Rai n 1 3 K [ 5 2 ]
dataset f or trai ni n g a n d test o n Rai n 1 0 0 H [ 4 8 ], 
Rai n 1 0 0 L [ 4 8 ], Test 1 0 0 [ 5 5 ], Test 1 2 0 0 [ 5 4 ], a n d 
Test 2 8 0 0 [ 8] datasets. F or ga ussia n de n oisi n g, we use 
t w o setti n gs: s y nt hetic n oise o n Set 1 2 [ 5 6 ], B S D 6 8 [ 2 3 ], 
C B S D 6 8 [ 2 3 ], K o da k [ 7], Mc Master [ 5 8 ], a n d Ur- 
ba n 1 0 0 [ 1 0 ]; a n d real- w orl d de n oisi n g o n SI D D [ 2]. 
F or si n gle-i ma ge m oti o n de bl urri n g, we use t he G o- 
Pr o [ 2 4 ] dataset f or trai ni n g a n d e val uate o n s y nt hetic 
datasets ( G o Pr o [ 2 4 ], HI D E [ 3 0 ]) a n d real- w orl d datasets 
( Real Bl ur- R [ 2 8 ], Real Bl ur-J [ 2 8 ]). F or def oc us de bl ur- 
ri n g, we use t he D P D D [ 3] trai ni n g data a n d test o n t he 
E B D B [ 1 3 ] a n d J N B [ 3 1 ] datasets. 
4. 2. L o w-li g ht I m a ge E n h a nce me nt 
C o m p aris o n. We c h o ose c urre nt S O T A l o w-li g ht i ma ge 
e n ha nce me nt met h o ds as t he baseli nes ( U H D [ 3 5 ], U Re- 
ti ne x [ 4 0 ], S N R [ 4 2 ]), a n d a p pl y o ur re ﬁ ne me nt m o d ule f or 
t hese baseli nes t o see if t heir perf or ma nce ca n be i m pr o ve d. 
T he pri ors are c h ose n fr o m t he C LI P [ 2 7 ], B LI P 2 [ 1 7 ], 
Sta ble Diff usi o n [ 2 9 ], a n d pre-trai ne d rest orati o n m o dels 
(trai ne d o n a n ot her dataset, as S D S D [ 3 4 ,4 5 ]). We de n ote 
t hese res ults as −c,−b,−s, a n d −r, res pecti vel y. I n Ta- 
ble 1, we o bser ve t hat c o m bi ni n g t hese pri ors wit h o ur re- 
ﬁ ne me nt m o d ule si g ni ﬁca ntl y i m pr o ves t he perf or ma nce of 
t he baseli nes. A d diti o nall y, Fi g. 4pr o vi des vis ual c o m par- 
is o ns. 
M ore o ver, we c o n d ucte d a n e x peri me nt b y a d di n g t he 
re ﬁ ne me nt m o d ule t o t he i nter me diate la yer of F, re ﬁ n- 
2904
LOL-real SID
URetine x SNR URetine x SNR
PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM
w/oSP,
with CA and SA 23.45 0.868 24.25 0.886 21.98 0.619 23.02 0.620
with SP
, w/o CA, with SA 22.10 0.856 24.05 0.875 22.05 0.623 22.93 0.624
with SP
and CA, w/o SA 23.76 0.850 23.86 0.879 21.92 0.620 23.07 0.621
LargeRw/o
SP/CA/SA 22.74 0.857 24.51 0.881 22.06 0.621 23.04 0.627
w/oPosition
Embedding S23.66 0.843 24.13 0.874 22.13 0.620 22.92 0.622
SNR V
alue as Mask 22.66 0.855 24.77 0.887 22.01 0.617 22.94 0.627
Use1D
Priors via Con. 23.01 0.853 23.83 0.878 22.07 0.622 22.93 0.628
Use2D
Priors via Con. 22.68 0.862 24.11 0.880 22.08 0.618 23.06 0.625
FullSetting 24.70 0.878 25.50 0.892 22.34 0.623 23.34 0.630
Table 3. Ablation study results. We adopt CLIP as the pre-trained
model. “SP” denote PTG-SVE, “CA” and “SA” denote spatial-
and channel attentions in PTG-CSA. Con. means Concatenation.
Datasets LOL-real SID
Methods ZeroDCE RU
AS SCI ZeroDCE RU
AS SCI
PSNR 18.06 18.37 20.28 18.08 18.44 19.09
SSIM 0.580 0.723 0.752 0.576 0.581 0.585
Methods +Ours-c +Ours-c +Ours-c +Ours-c +Ours-c +Ours-c
PSNR 18.79 19.53 21.62 18.65 18.93 19.61
SSIM 0.614 0.747 0.781 0.593 0.590 0.598
Table 4. Quantitative comparison on the LOL-real and SID dataset
for unsupervised methods. We adopt CLIP as the pre-trained
model here.
Method PSNR ↑SSIM↑PSNR ↑SSIM↑PSNR ↑SSIM↑
Test100 Rain100H Rain100L
SPAIR 30.35 0.909 30.95 0.892 36.93 0.969
SPAIR+Ours-c 30.62 0.917 31.20 0.901 37.26 0.973
Restormer 32.00 0.923 31.46 0.904 38.99 0.978
Restormer+Ours-c 32.30 0.934 31.77 0.913 39.27 0.985
Test2800 Test1200 Average
SP
AIR 33.34 0.936 33.04 0.922 32.91 0.926
SPAIR+Ours-c 33.58 0.942 33.35 0.924 33.16 0.932
Restormer 34.18 0.944 33.19 0.926 33.96 0.935
Restormer+Ours-c 34.47 0.951 33.48 0.929 34.24 0.943
Table 5. Image deraining results.
ing features of the target model. The refinement module
is added to the deepest feature layer for efficiency, produc-
ing the residual feature map and the mask information for
refinement. These results are denoted as −f. The improve-
ment achieved by this operation is also evident as displayed
in Table 1.
Comparison with Other Priors. Some works, such as
SKF [41] and SMG [46], utilize additional information like
semantic maps, edge maps, and depth maps to enhance low-
light image enhancement results. However, these methods
require supervision with paired multi-modal information,
whereas our method does not. Additionally, as shown in
Table 2, our approach achieves better performance improve-
ment for a given target model. Notably, the improvements
achieved by other methods are based on large additional pa-
rameters, while our approach only uses a lightweight refine-
ment module <1M.
Ablation Study: Ablation of Different Components. We
first set experiments by deleting different components from
our framework, including PTG-SVE (abbreviated as “SP”),
and spatial-channel attentions with priors that are abbrevi-
Input
 Ground-truth
Restormer
 Restormer+Ours
Figure 5. Visual comparison on Rain100H showing the effects of
our strategy.
MethodGoPr o HIDE RealBlur -R RealBlur -J
PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM
MPRNet 32.66 0.959 30.96 0.939 35.99 0.952 28.70 0.873
MPRNet+Ours-c 32.87 0.964 31.19 0.943 36.25 0.960 28.98 0.881
Restormer 32.92 0.961 31.22 0.942 36.19 0.957 28.96 0.879
Restormer+Ours-c 33.18 0.966 31.51 0.950 36.47 0.962 29.21 0.883
Table 6. Single-image motion deblurring results.
ated as “CA” and “SA”, respectively. As shown in Table 3,
deleting any component will lead to a performance drop.
We conduct experiments without SP, CA, or SA to ana-
lyze whether additional parameters or priors take a promi-
nent role. The short-range and long-range results are fused
via a simple sum, and the spatial-channel attention is con-
ducted using only the features themselves. Additionally,
we increase the feature channel number fourfold to add
more parameters. The results, denoted as “Large Rw/o
SP/CA/SA” in Table 3, are still lower than our full setting,
indicating the effectiveness of our proposed approach over
simply increasing parameters.
In addition, we perform an experiment by removing the
learnable position embeddings SmandSc, denoted as “w/o
Position Embedding for Priors” in Table 3. This comparison
highlights the importance of using spatial-aware representa-
tions for the pre-trained features.
Ablation Study: SNR Value as Mask. In comparison
to previous methods that directly use the SNR value as
the mask to fuse the short- and long-range results, our
approach utilizes pre-trained priors to automatically dis-
cover restoration-related information and formulate the fu-
sion mask adaptively. In this ablation study, we demon-
strate that our strategy outperforms the direct SNR-based
approach, as shown in Table 3.
Ablation Study: Alternatives of Using Priors. In this
study, we demonstrate the difficulty of directly aligning pri-
ors to the restoration features. We conduct an experiment
where the priors are concatenated with the features in the re-
finement module to implement different components. How-
ever, the improvement obtained with this direct approach
2905
Input
 Ground-truth
Restormer
 Restormer+Ours
Figure 6. Visual comparison on HIDE.
Input
 Ground-truth
GRL
 GRL+Ours
Figure 7. Visual comparison on single-image defocus deblurring.
is not as significant as our proposed method, as shown in
Table 3. This is because the different features are hetero-
geneous with the restoration features, even when the priors
are adopted as 2D feature maps. This study highlights the
importance of our novel strategy of employing these priors.
Rfor Unsupervised Approach. Different from existing
refinement methods that need supervision for learning the
additional features (e.g., SKF needs the semantic ground
truth of the normal-light data, SMG needs the depth and
edge information of the normal-light data), our approach
does not require the feature of the normal-light data dur-
ing both training and inference. We only need the feature
that is extracted from Idwith the pre-trained model Gdur-
ing the training. Also, the loss function for training the re-
finement module can be set the same as that of the target
model. Thus, the unsupervised training of the target model
can also be adopted in our framework. As shown in Table 4,
our method can successfully improve the performance of
various unsupervised low-light image enhancement meth-
ods with different unsupervised loss terms, including En-
GAN [11], ZeroDCE [9], RUAS [21], and SCI [22].
4.3. Other Restoration Tasks
In this section, we conduct experiments using CLIP as the
pre-trained model (−c). CLIP is chosen for its efficiency
and convenience compared to other pre-trained models.MethodIndoor Scenes Outdoor Scenes Combined
PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS
IFANS 28.11 0.861 0.179 22.76 0.720 0.254 25.37 0.789 0.217
IF
ANS+Ours-c 28.32 0.870 0.171 23.08 0.727 0.248 25.72 0.795 0.213
Restormer S 28.87 0.882 0.145 23.24 0.743 0.209 25.98 0.811 0.178
Restormer S+Ours-c 29.17 0.890 0.141 23.43 0.749 0.206 26.13 0.816 0.165
GRL S-B 29.06 0.886 0.139 23.45 0.761 0.196 26.18 0.822 0.168
GRL S-B
+Ours-c 29.30 0.894 0.133 23.67 0.768 0.189 26.45 0.828 0.161
IFAND 28.66 0.868 0.172 23.46 0.743 0.240 25.99 0.804 0.207
IF
AND+Ours-c 28.94 0.875 0.167 23.70 0.748 0.235 26.20 0.811 0.203
Restormer D 29.48 0.895 0.134 23.97 0.773 0.175 26.66 0.833 0.155
Restormer D+Ours-c 29.79 0.902 0.131 24.23 0.778 0.155 26.89 0.840 0.153
GRL D-B 29.83 0.903 0.114 24.39 0.795 0.150 27.04 0.847 0.133
GRL D-B+Ours-c 29.96 0.911 0.110 24.62 0.803 0.145 27.27 0.855 0.128
Table 7. Defocus deblurring comparisons on the DPDD testset
(containing 37 indoor and 39 outdoor scenes). S:single-image
defocus deblurring. D:dual-pixel defocus deblurring.
MethodSet12 BSD68 Urban100
σ=15 σ=25 σ=50 σ=15 σ=25 σ=50 σ=15 σ=25 σ=50
DRUNet 33.25 30.94 27.90 31.91 29.48 26.59 33.44 31.11 27.96
DR
UNet+Ours-c 33.51 31.18 28.27 32.20 29.73 26.84 33.65 31.34 28.16
Restormer 33.35 31.04 28.01 31.95 29.51 26.62 33.67 31.39 28.33
Restormer+Ours-c 33.57 31.28 28.36 32.11 29.78 26.91 33.96 31.67 28.58
Restormer 33.42 31.08 28.00 31.96 29.52 26.62 33.79 31.46 28.29
Restormer+Ours-c 33.70 31.29 28.35 32.24 29.81 26.86 33.97 31.73 28.58
GRL-B 33.47 31.12 28.03 32.00 29.54 26.60 34.09 31.80 28.59
GRL-B+Ours-c 33.74 31.30 28.37 32.29 29.76 26.91 34.22 31.95 28.74
Table 8. Gaussian grayscale image denoising comparisons. Top
super rows: learning a single model to handle various noise levels.
Bottom super rows: training a separate model for each noise level.
Deraining. For deraining tasks, we use SOTA methods
such as SPAIR [26] and Restormer [52] as baselines. We
compute PSNR/SSIM values using the Y channel in the
YCbCr color space, similar to existing methods. Table 5
demonstrates that our approach improves the performance
of these existing methods and consistently achieves signif-
icant performance gains across all five datasets. The quali-
tative comparison results are shown in Fig. 5.
Motion Deblurring. We analyze our approach for deblur-
ring tasks on synthetic datasets (GoPro, HIDE) and real-
world datasets (RealBlur-R, RealBlur-J). The baselines in-
clude MPRNet [51] and Restormer [52]. Table 6demon-
strates that our approach improves the performance of all
these methods on all four benchmark datasets. Although
the enhanced network is trained only on the GoPro dataset,
it shows more robust generalization to other datasets. Qual-
itative comparisons are shown in Fig. 6, further supporting
our claim.
Defocus Deblurring. Table 7presents the image fidelity
scores of SOTA approaches on the DPDD dataset [3], in-
cluding IFAN [15], Restormer [52], and GRL [19]. Our re-
finement module achieves significant performance improve-
ment for these SOTA schemes in both single-image and
dual-pixel defocus deblurring settings across all scene cate-
gories. The qualitative results are depicted in Fig. 7.
Gaussian Denoising. We conduct denoising experiments
on synthetic benchmark datasets with additive white Gaus-
sian noise. We choose DRUNet [57], Restormer [52], and
2906
MethodCBSD68 Kodak24 McMaster Urban100
σ=15σ=25σ=50σ=15σ=25σ=50σ=15σ=25σ=50σ=15σ=25σ=50
DRUNet 34.30 31.69 28.51 35.31 32.89 29.86 35.40 33.14 30.08 34.81 32.60 29.61
+Ours-c 34.54 31.97 28.76 35.58 33.15 29.97 35.71 33.50 30.25 35.10 32.82 29.83
Restormer 34.39 31.78 28.59 35.44 33.02 30.00 35.55 33.31 30.29 35.06 32.91 30.02
+Ours-c 34.63 32.04 28.88 35.65 33.26 30.15 35.86 33.64 30.63 35.26 33.22 30.21
Restormer 34.40 31.79 28.60 35.47 33.04 30.01 35.61 33.34 30.30 35.13 32.96 30.02
+Ours-c 34.76 32.05 28.94 35.72 33.27 30.21 35.80 33.63 30.55 35.32 33.14 30.27
GRL-B 34.45 31.82 28.62 35.43 33.02 29.93 35.73 33.46 30.36 35.54 33.35 30.46
+Ours-c 34.73 32.07 28.90 35.71 33.24 30.18 35.96 33.75 30.62 35.70 33.57 30.64
Table 9. Gaussian color image denoising. Equivalent notation
meanings (top and bottom rows) as those in Table 8.
Dataset Method MPRNet MPRNet
+ Ours-cUformer
Uformer
+ Ours-cRestormer Restormer
+ Ours-c
SIDD PSNR ↑ 39.71 39.93 39.77 39.94 40.02 40.22
SSIM↑ 0.958 0.961 0.959 0.962 0.960 0.965
Table 10. Real image denoising on the SIDD dataset.
GRL [19] as baselines, which are SOTA approaches in de-
noising. Tables 8and9present PSNR scores of different
approaches on grayscale and color image denoising, respec-
tively, for noise levels of 15, 25, and 50. We evaluate two
experimental settings: (1) learning a single model to handle
various noise levels and (2) learning separate models for
each noise level. Our method achieves significant perfor-
mance enhancement for all these methods under both exper-
imental settings on different datasets and noise levels. The
visual results are shown in Fig. 8, showing the effectiveness
of our strategy.
Real Denoising. We also conduct denoising experi-
ments on the real-world SIDD dataset, with MPRNet [51],
Uformer [37], and Restormer [52] as baselines. Table 10
demonstrates that our refinement method improves both
PSNR and SSIM metrics. Notably, on the SIDD dataset,
our refinement enables the SOTA approach Restormer to
achieve a PSNR surpassing 40.2 dB. The visual compari-
son is shown in Fig. 8.
User Study. Furthermore, we conducted a large-scale user
study with an A/B test strategy involving 80 participants.
Each participant is asked to simultaneously see two restored
results, i.e., baseline and baseline+ours, and gauge which
one is better. As shown in Fig. 9, the results combined with
our strategy are more preferred by the participants.
5. Conclusion
In this work, we explore the utilization of features from a
pre-trained model to enhance the performance of a restora-
tion model. By unifying the shapes of the pre-trained fea-
tures, we introduce a novel refinement module PTG-RM
that employs PTG-SVE and PTG-CSA mechanisms. Un-
like existing strategies, we focus on formulating optimal
operation ranges and attention strategies guided by the pre-
trained features. The extensive experiments conducted on
various tasks, datasets, and networks demonstrate the ef-
fectiveness and generalization ability of our approach. We
Input
 Ground-truth
GRL
 GRL+Ours
Input
 Ground-truth
 Restormer
 +Ours
Figure 8. Visual comparisons on Kodak (top) and SIDD (bottom).
0.2438 
0.7563 
GRL GRL+Ours0.3313 
0.6688 
GRL GRL+Ours0.3688 
0.6313 
Uformer Uformer+Ours0.1375 
0.8625 
Restormer Restormer+Ours
0.3125 
0.6875 
Restormer Restormer+OursLow-light Enhancement
DerainingMotion Deblurring
Single-Image 
Defocus DeblurringDual-Pixel Defocus 
Deblurring0.1688 
0.8313 
GRL GRL+Ours
Gaussian DenoisingReal-Image Denoising0.2625 
0.7375 
SCI SCI+Ours
Low-light Enhancement0.1563 
0.8438 
SNR SNR+Ours
Figure 9. The user study results show that our strategy can effec-
tively improve the performance of restoration approaches in terms
of human subjective evaluation.
believe that our proposed principle of discovering hidden
useful information in pre-trained models can be applicable
to other domains as well.
Limitation and Future Work. While our proposed strat-
egy has exhibited significant effects in enhancing the perfor-
mance of diverse restoration networks across various archi-
tectures with its lightweight module, the extent of improve-
ment appears to vary across different experiments. Some in-
stances showcase noticeable enhancement, while others do
not. Such differences correlate with the capacity of the tar-
get network and the difficulty/complexity of the target task.
In future endeavors, we intend to delve into more effective
approaches that specifically aid target restoration tasks. We
aim to employ a tailored distillation framework to derive
refined restoration feature priors, ultimately making signifi-
cant strides beyond existing upper boundaries. We also aim
to develop corresponding technical products.
Acknowledgements. This work is supported by
the Natural Science Foundation of Zhejiang Pvovince,
China, under No. LD24F020002. SK is partially
supported by University of Macau (SRG2023-00044-
FST).
2907
References
[1] Andreas Aakerberg, Anders S Johansen, Kamal Nasrollahi,
and Thomas B Moeslund. Semantic segmentation guided
real-world super-resolution. In WACV, 2022. 1
[2] Abdelrahman Abdelhamed, Stephen Lin, and Michael S
Brown. A high-quality denoising dataset for smartphone
cameras. In CVPR, 2018. 5
[3] Abdullah Abuolaim and Michael S Brown. Defocus deblur-
ring using dual-pixel data. In ECCV, 2020. 5,7
[4] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended
diffusion for text-driven editing of natural images. In CVPR,
2022. 3
[5] Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun.
Learning to see in the dark. In CVPR, 2018. 5
[6] Sepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei
Shu. Zero-shot out-of-distribution detection based on the
pre-trained model clip. In AAAI, 2022. 3
[7] Rich Franzen. Kodak lossless true color image suite. http:
//r0k.us/graphics/kodak/, 1999. Online accessed
24 Oct 2021. 5
[8] Xueyang Fu, Jiabin Huang, Delu Zeng, Yue Huang, Xinghao
Ding, and John Paisley. Removing rain from single images
via a deep detail network. In CVPR, 2017. 5
[9] Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy,
Junhui Hou, Sam Kwong, and Runmin Cong. Zero-reference
deep curve estimation for low-light image enhancement. In
CVPR, 2020. 7
[10] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single
image super-resolution from transformed self-exemplars. In
CVPR, 2015. 5
[11] Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang,
Xiaohui Shen, Jianchao Yang, Pan Zhou, and Zhangyang
Wang. Enlightengan: Deep light enhancement without
paired supervision. TIP, 2021. 7
[12] Shuangping Jin, Bingbing Yu, Minhao Jing, Yi Zhou, Jiajun
Liang, and Renhe Ji. Darkvisionnet: Low-light imaging via
rgb-nir fusion with deep inconsistency prior. In AAAI, 2022.
3
[13] Ali Karaali and Claudio Rosito Jung. Edge-based defocus
blur estimation with adaptive scale selection. TIP, 2017. 5
[14] Shu Kong and Charless Fowlkes. Image reconstruction
with predictive filter flow. arXiv preprint arXiv:1811.11482,
2018. 1
[15] Junyong Lee, Hyeongseok Son, Jaesung Rim, Sunghyun
Cho, and Seungyong Lee. Iterative filter adaptive network
for single image defocus deblurring. In CVPR, 2021. 7
[16] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation. In ICML,
2022. 1,3
[17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. arXiv
preprint, 2023. 1,3,5
[18] Yi Li, Yi Chang, Changfeng Yu, and Luxin Yan. Close the
loop: a unified bottom-up and top-down paradigm for joint
image deraining and segmentation. In AAAI, 2022. 3[19] Yawei Li, Yuchen Fan, Xiaoyu Xiang, Denis Demandolx,
Rakesh Ranjan, Radu Timofte, and Luc Van Gool. Effi-
cient and explicit modelling of image hierarchies for image
restoration. In CVPR, 2023. 1,7,8
[20] Ding Liu, Bihan Wen, Xianming Liu, Zhangyang Wang, and
Thomas S Huang. When image denoising meets high-level
vision tasks: A deep learning approach. In IJCAI, 2018. 3
[21] Risheng Liu, Long Ma, Jiaao Zhang, Xin Fan, and Zhongx-
uan Luo. Retinex-inspired unrolling with cooperative prior
architecture search for low-light image enhancement. In
CVPR, 2021. 7
[22] Long Ma, Tengyu Ma, Risheng Liu, Xin Fan, and Zhongx-
uan Luo. Toward fast, flexible, and robust low-light image
enhancement. In CVPR, 2022. 7
[23] David Martin, Charless Fowlkes, Doron Tal, and Jitendra
Malik. A database of human segmented natural images
and its application to evaluating segmentation algorithms and
measuring ecological statistics. In ICCV, 2001. 5
[24] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep
multi-scale convolutional neural network for dynamic scene
deblurring. In CVPR, 2017. 5
[25] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,
and Dani Lischinski. Styleclip: Text-driven manipulation of
stylegan imagery. In ICCV, 2021. 3
[26] Kuldeep Purohit, Maitreya Suin, AN Rajagopalan, and
Vishnu Naresh Boddeti. Spatially-adaptive image restoration
using distortion-guided networks. In ICCV, 2021. 7
[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, 2021. 1,3,5
[28] Jaesung Rim, Haeyun Lee, Jucheol Won, and Sunghyun Cho.
Real-world blur dataset for learning and benchmarking de-
blurring algorithms. In ECCV, 2020. 5
[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR, 2022. 5
[30] Ziyi Shen, Wenguan Wang, Xiankai Lu, Jianbing Shen,
Haibin Ling, Tingfa Xu, and Ling Shao. Human-aware mo-
tion deblurring. In ICCV, 2019. 5
[31] Jianping Shi, Li Xu, and Jiaya Jia. Just noticeable defocus
blur detection and estimation. In CVPR, 2015. 5
[32] Renjie Wan, Boxin Shi, Wenhan Yang, Bihan Wen, Ling-Yu
Duan, and Alex C Kot. Purifying low-light images via near-
infrared enlightened image. TMM, 2022. 3
[33] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Ex-
ploring clip for assessing the look and feel of images. In
AAAI, 2023. 2
[34] Ruixing Wang, Xiaogang Xu, Chi-Wing Fu, Jiangbo Lu, Bei
Yu, and Jiaya Jia. Seeing dynamic scene in the dark: A high-
quality video dataset with mechatronic alignment. In ICCV,
2021. 5
[35] Tao Wang, Kaihao Zhang, Tianrun Shen, Wenhan Luo, Bjorn
Stenger, and Tong Lu. Ultra-high-definition low-light image
enhancement: A benchmark and transformer-based method.
InAAAI, 2023. 5
2908
[36] Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy.
Recovering realistic texture in image super-resolution by
deep spatial feature transform. In CVPR, 2018. 1
[37] Zhendong Wang, Xiaodong Cun, Jianmin Bao, and
Jianzhuang Liu. Uformer: A general u-shaped transformer
for image restoration. In CVPR, 2022. 8
[38] Zejin Wang, Jiazheng Liu, Guoqing Li, and Hua Han.
Blind2unblind: Self-supervised image denoising with visi-
ble blind spots. In CVPR, 2022. 1
[39] Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong
Guo, Mingming Gong, and Tongliang Liu. Cris: Clip-driven
referring image segmentation. In CVPR, 2022. 3
[40] Wenhui Wu, Jian Weng, Pingping Zhang, Xu Wang, Wen-
han Yang, and Jianmin Jiang. Uretinex-net: Retinex-based
deep unfolding network for low-light image enhancement. In
CVPR, 2022. 5
[41] Yuhui Wu, Chen Pan, Guoqing Wang, Yang Yang, Jiwei Wei,
Chongyi Li, and Heng Tao Shen. Learning semantic-aware
knowledge guidance for low-light image enhancement. In
CVPR, 2023. 1,2,6
[42] Xiaogang Xu, Ruixing Wang, Chi-Wing Fu, and Jiaya Jia.
Snr-aware low-light image enhancement. In CVPR, 2022. 1,
2,4,5
[43] Xiaogang Xu, Yitong Yu, Nianjuan Jiang, Jiangbo Lu, Bei
Yu, and Jiaya Jia. Pvdd: A practical video denoising dataset
with real-world dynamic scenes. arXiv preprint, 2022. 1
[44] Xiaogang Xu, Hengshuang Zhao, Philip Torr, and Jiaya Jia.
General adversarial defense against black-box attacks via
pixel level and feature level distribution alignments. arXiv
preprint, 2022. 1
[45] Xiaogang Xu, Ruixing Wang, Chi-Wing Fu, and Jiaya Jia.
Deep parametric 3d filters for joint video denoising and illu-
mination enhancement in video super resolution. In AAAI,
2023. 5
[46] Xiaogang Xu, Ruixing Wang, and Jiangbo Lu. Low-light
image enhancement via structure modeling and guidance. In
CVPR, 2023. 1,2,6
[47] Le Xue, Mingfei Gao, Chen Xing, Roberto Mart ´ın-Mart ´ın,
Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles,
and Silvio Savarese. Ulip: Learning unified representation
of language, image and point cloud for 3d understanding. In
CVPR, 2023. 3
[48] Wenhan Yang, Robby T Tan, Jiashi Feng, Jiaying Liu, Zong-
ming Guo, and Shuicheng Yan. Deep joint rain detection and
removal from a single image. In CVPR, 2017. 5
[49] Wenhan Yang, Wenjing Wang, Haofeng Huang, Shiqi Wang,
and Jiaying Liu. Sparse gradient regularized deep Retinex
network for robust low-light image enhancement. TIP, 2021.
5
[50] Yan Yang, Liyuan Pan, Liu Liu, and Miaomiao Liu. K3dn:
Disparity-aware kernel estimation for dual-pixel defocus de-
blurring. In CVPR, 2023. 1
[51] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar
Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling
Shao. Multi-stage progressive image restoration. In CVPR,
2021. 7,8[52] Syed Waqas Zamir, Aditya Arora, Salman Khan, Mu-
nawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang.
Restormer: Efficient transformer for high-resolution image
restoration. In CVPR, 2022. 1,5,7,8
[53] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,
Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer.
Lit: Zero-shot transfer with locked-image text tuning. In
CVPR, 2022. 3
[54] He Zhang and Vishal M Patel. Density-aware single image
de-raining using a multi-stream dense network. In CVPR,
2018. 5
[55] He Zhang, Vishwanath Sindagi, and Vishal M Patel. Im-
age de-raining using a conditional generative adversarial net-
work. TCSVT, 2019. 5
[56] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and
Lei Zhang. Beyond a gaussian denoiser: Residual learning
of deep cnn for image denoising. TIP, 2017. 5
[57] Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc
Van Gool, and Radu Timofte. Plug-and-play image restora-
tion with deep denoiser prior. TPAMI, 2021. 7
[58] Lei Zhang, Xiaolin Wu, Antoni Buades, and Xin Li. Color
demosaicking by local directional interpolation and nonlocal
adaptive thresholding. JEI, 2011. 5
[59] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xu-
peng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng
Li. Pointclip: Point cloud understanding by clip. In CVPR,
2022. 3
[60] Ziqin Zhou, Yinjie Lei, Bowen Zhang, Lingqiao Liu, and
Yifan Liu. Zegclip: Towards adapting clip for zero-shot se-
mantic segmentation. In CVPR, 2023. 3
2909
