Diffusion-FOF: Single-view Clothed Human Reconstruction
via Diffusion-based Fourier Occupancy Field
Yuanzhen Li, Fei Luo∗, Chunxia Xiao*
School of Computer Science, Wuhan University, China
yuanzhen@whu.edu.cn, luofei@whu.edu.cn, cxxiao@whu.edu.cn
Abstract
Reconstructing a clothed human from a single-view im-
age has several challenging issues, including flexibly repre-
senting various body shapes and poses, estimating complete
3D geometry and consistent texture, and achieving more
fine-grained details. To address them, we propose a new
diffusion-based Fourier occupancy field method to improve
the human representing ability and the geometry generat-
ing ability. First, we estimate the back-view image from the
given reference image by incorporating a style consistency
constraint. Then, we extract multi-scale features of the two
images as conditional and design a diffusion model to gen-
erate the Fourier occupancy field in the wavelet domain.
We refine the initial estimated Fourier occupancy field with
image features as conditions to improve the geometric ac-
curacy. Finally, the reference and estimated back-view im-
ages are mapped onto the human model, creating a textured
clothed human model. Substantial experiments are con-
ducted, and the experimental results show that our method
outperforms the state-of-the-art methods in geometry and
texture reconstruction performance.
1. Introduction
3D human reconstruction has wide applications in ed-
ucation, entertainment, and AR/VR [4, 5, 20]. Traditional
methods can reconstruct a human’s geometry and texture
by extracting adequate information from multi-view images
[10, 23, 41]. However, multiple-view images for a human
are not available in many scenarios. For example, photos
of people in the smartphone’s album are usually shot on the
single-view. Reconstructing a clothed human from a single-
view image is a challenging task [24, 27]. Several issues
need to be further dealt with, including flexibly representing
various shapes and poses, robustly estimating complete 3D
geometry and consistent texture appearance, and achieving
more fine-grained details.
*Chunxia Xiao and Fei Luo are co-corresponding authors
 Input        Mesh  Mesh with texture
Figure 1. The 3D reconstruction example of our method for a
single-view clothed human image.
Properly representing a human is one important factor,
as it determines the scale of body shapes and poses that
can be reconstructed. The widely used parametric model
SMPL [21] is based on skinning and blend shapes and is
learned from thousands of 3D body scans. Researchers
introduced clothing displacement to SMPL to reconstruct
clothing details [1, 2]. These methods have limitations
in reconstructing loose-fitting clothes such as dresses and
robes. The visual hull [24], the depth [11, 19], and the
voxel [35, 47] have been also explored. They require sig-
nificant computational resources. Besides explicit represen-
tation methods, researchers proposed implicit neural repre-
sentation methods [27, 28], using the network to predict the
geometry and texture of spatial sample points. Recently,
Feng et al. [9] proposed a novel 3D geometric representa-
tion called Fourier occupancy field (FOF), representing 3D
geometry as a multi-channel image. However, FOF directly
utilizes a 2D Convolutional Neural Network (CNN) to es-
timate the occupancy field and loses high-frequency infor-
mation. Such weakness may lead to geometric distortion or
overly smoothing.
The powerful generative model is another important
factor in determining how well the reconstruction results
can approximate real humans. Diffusion model [8] has
substantially succeeded in several computer vision tasks
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
9525
[26, 33, 44]. We propose to combine the generation ca-
pability of the image-conditioned diffusion model and the
flexibility of the Fourier occupancy field representation. In
particular, we introduce the wavelet mechanism to handle
the weaknesses of the diffusion model and the Fourier oc-
cupancy field. Briefly, the wavelet transform decomposes
the high-frequency and low-frequency information, facili-
tating learning intricate details. The spatial dimensions of
wavelet subbands are 1/4 of the original image, reducing
computational resources and inference time.
We propose a novel single-view clothed human recon-
struction method via a diffusion-based Fourier occupancy
field. It takes several steps to realize reconstruction. First,
we estimate the back-view image to provide more human
prior knowledge. Current methods have not considered
the texture consistency between the back-view and refer-
ence images. When wearing a black jacket with a white
T-shirt, the clothing on the back view will likely be black.
Thus, we introduce a style consistency constraint between
the predicted back-view and reference images. Then, we
extract multi-scale features of the reference image and the
estimated back-view image. We design a diffusion model to
generate the Fourier occupancy field in the wavelet domain
based on the image features as conditional. Meanwhile, we
refine the initial estimated Fourier occupancy field with im-
age features as conditions to improve the geometric accu-
racy. Finally, the reference and estimated back-view images
are mapped onto the model, producing the final textured
clothed human model. Extensive comparative experiments
and ablation studies verify the effectiveness of our method.
In summary, the main contributions of our method are
summarized as follows:
•We propose a wavelet-based FOF diffusion model to re-
construct 3D clothed human from a single image. The
wavelet transform facilitates the explicit learning of de-
tailed information and reduces computational time.
•During the back-view image prediction, we introduce a
style consistency constraint between the predicted back-
view image and the reference image to enhance the style
consistency of the texture.
•In the geometric prediction, we add the predicted back-
view image as conditional, providing more human prior
information to the geometric prediction network.
2. Related Work
Single-view human reconstruction. Only with 2D in-
formation of a single-view image, reconstructing the 3D hu-
man is inherently ill-posed. Some works introduced addi-
tional assumptions or prior knowledge. Previous research
has proposed effective parametric models [3, 21] of the hu-
man body, which utilizes statistical methods to reduce thevariations in human body shape and pose to a compact set
of parameters. With the rapid development of deep learn-
ing, methods [17, 18] attempt to estimate and regress the
model parameters from a single image by deep neural net-
works. Human-parameterized models lack 3D details of
clothes, hairstyles, and adornments. To generate the cloth-
ing details, methods [1,2] added offsets on the top of param-
eterized model vertices. Although these methods can re-
construct certain clothing details, they often face challenges
when dealing with loose-fitting garments, such as robes and
dresses.
To address the constraints of loose-fitting clothing re-
construction, various 3D human representations have been
explored, including the visual hull [24], the double depth
maps [11, 13], and the voxel [36]. However, these methods
need high memory requirements, limiting the spatial reso-
lution of shape estimation.
To reconstruct high-resolution 3D clothed humans, Saito
et al. [27] proposed a pixel-aligned implicit function for re-
constructing 3D human geometry and texture called PIFu.
They used neural networks to extract 2D image features and
designed a deep implicit function to estimate the geometry
and texture of 3D sampling points. PIFuHD [28] designed
a coarse-to-fine framework to reconstruct high-resolution
3D human geometric with the normal as prior knowledge.
These two methods could generate distorted shapes in com-
plex poses due to the lack of regularization. To improve
geometric stability, methods [40, 42, 46] utilized human pa-
rameterized models as prior information to guide implicit
function learning. However, inaccuracies in parameterized
model estimation would decrease reconstruction accuracy.
Recently, Feng et al. [9] proposed a novel 3D represen-
tation, the Fourier occupancy field, which converts the 3D
occupancy value into a 2D vector field using Fourier se-
ries expansion. Different from [9], we aim to generate the
Fourier occupancy field from the view of generative model-
ing implemented by a wavelet-based diffusion model.
Denoising diffusion model. The Diffusion model is a
generative model that employs an iterative denoising pro-
cess to generate high-quality results of target tasks [14]. It
has demonstrated superior performance in various computer
vision tasks. In 2D computer vision applications, it has been
used for tasks such as image generation [33, 34], inpaint-
ing [25], and super-resolution [26]. In 2.5D computer vision
tasks, the diffusion model has been applied to monocular
depth estimation [15, 29] and depth refinement [30]. In the
realm of 3D content generation, it finds application in tasks
such as 3D shape generation [6, 48] and completion [7, 49].
3. Method
The pipeline of our method is shown in Figure 2. Given
a single human image Ia, we first predict the back-view im-
ageI′
b. Then, we design a wavelet-based diffusion model to
9526
共享权
重共享权重
共享权重
Input                                                  
(a) Back- view image prediction                                              (b) Mesh reconstruction                   Textured mesh
Tsteps (sampling) 
IWTc𝐴𝐴𝑉𝑉1𝑉𝑉2𝑉𝑉3
FOFFOF refinement
ConcatenationcImage encoderEncoder Decoder
Input                                  PIFu GTA                                                    Ours
Figure 2. Overview of our method. Given the human image Ia, we first predict the back-view image I′
b. Then, we design a wavelet-based
diffusion model to generate the Fourier occupancy field based on the two images as conditional. We transform the Fourier occupancy field
to the 3D mesh. Finally, the two images IaandI′
bare projected back to the 3D mesh, generating the textured model.
generate the Fourier occupancy field based on the two im-
ages as conditional. We transform the Fourier occupancy
field to occupancy values and use the marching cubes [22]
algorithm to generate the 3D mesh. Finally, the two im-
agesIaandI′
bare mapped onto the human model through
rasterization, creating the textured model.
3.1. Preliminaries
Fourier occupancy field. It encodes the 3D object ge-
ometry to a 2D vector field [9]. Each pixel (x,y)on the
image Icorresponds to a line in 3D space and can be used
as an occupancy function f(z)represent. The occupancy
function f(z)satisfies the Dirichlet conditions and can be
expanded as a convergent Fourier series:
f(z) =a0
2+∞X
n=1(ancos (nzπ) +bnsin (nzπ)).(1)
where {an}and{bn}are coefficients of basis functions
{cos(nx)}and{sin(nx)}, respectively. Approximate f(z)
by a subspace spanned by the first 2N + 1 basis functions:
f(z) = b⊤(z)c, (2)
where b(z) = [1 /2,cos(z) ,sin(z) , ...,cos(Nz),sin(Nz)]⊤
is the vector of the first 2N+1basis functions spanning the
approximation subspace, and c= [a0, a1, b1, ..., a N, bN]⊤
is the 2N+ 1 Fourier coefficient vectors. This procedure
can be extended to cover the entire xy-plane: F(x,y,z) =
b⊤(z)C(x,y), and Ccalled Fourier occupancy field (FOF).
In our experiment, Nis settled as 15.
When FOF is estimated, the approximate occupancy
field can be extracted according to Eq. 2. Then, we can use
the marching cubes algorithm [22] to extract the 3D mesh
from the occupancy field.
Wavelet transform. It is a classic technique exten-
sively used in image compression. It is applied to sepa-
rate the original image’s low-frequency approximation and
high-frequency details. The low sub-band corresponds to adown-sampled version resembling the original image, and
the high sub-bands capture local statistical information on
vertical, horizontal, and diagonal edges. Haar wavelet [12]
is a simple wavelet transform, including discrete wavelet
transform (DWT) and inverse wavelet transform (IWT).
We use Haar wavelet to decompose the Fourier occu-
pancy field C∈RH×W×(2N+1)into four wavelet sub-
bands{A, V 1, V2, V3} ∈RH
2×W
2×(2N+1), representing the
average of the source image and high-frequency informa-
tion in the vertical, horizontal, and diagonal directions, re-
spectively.
Diffusion model. It is divided into forward diffusion and
inverse denoising phases [14]. The forward process is fixed
to a Markov chain that gradually adds Gaussian noise to the
target training data z0=z:
q(z1:T) =QT
t=1q(zt|zt−1), (3)
q(zt|zt−1) =N(zt;p
1−βtzt−1, βtI), (4)
where βtis a variance schedule that increases from β0= 0
toβT= 1 and controls how much noise is added in each
step. We can obtain the sampling ztat even arbitrary time
steptfrom z0:
q(zt|z0) =N(zt;√¯αtz0,(1−¯αt)I), (5)
where ¯αt=Qt
s=0αs=Qt
s=0(1−βs). The reverse process
aims to derive the posterior distribution for the less noisy
image zt−1given the more noisy image ztusing the denois-
ing network fθ:
pθ(z0:T) =p(zT)QT
t=1pθ(zt−1|zt), (6)
pθ(zt−1|zt) =N(zt−1;µθ(zt, t), σ2
tI), (7)
where p(zT) =N(zT;0,I)is the random sampling of
Gaussian noise, µθ(zt, t)andσ2
tare the mean and variance
of the parametric denoising model, respectively. The ob-
jective is to minimize the distance between a true denoising
distribution q(zt|zt−1)and the parameterized pθ(zt−1|zt)
through Kullback-Leibler (KL) divergence. DDPM [14]
utilizes the network to predict the noise ϵ.
9527
3.2. Back-view Image Prediction
In the back-view image prediction, we propose a style
consistency constraint between the predicted and reference
images. We utilize a Siamese network training strategy to
train the back-view image prediction network.
Given the human image Ia, we use a deep neural network
to estimate the back-view image I′
b. We train the network
using the L1 loss between the estimated image I′
band the
ground truth Ib:
L1(I′
b, Ib) =∥I′
b−Ib∥1. (8)
There is a significant correlation between the back-view
and front clothes of the human. For example, when a person
wears a black jacket with a white T-shirt, the clothing on the
back side will be black. Consequently, the predicted back-
view image I′
band the reference image Iahave consistent
style (texture). Inspired by the image style transfer [16], we
propose a style consistency constraint between the predicted
back-view image I′
band the reference image Ia:
Ls(I′
b, Ia) =∥G(Φ1(I′
b))−G(Φ1(Ia))∥2
2, (9)
where G(.)denotes the Gram matrix, and Φ1represents the
latent space feature extracted from the first layer of the pre-
trained VGG19 model [31], which is effective at capturing
low-level color features of the reconstructed texture.
From the above analysis, the predicted back-view image
I′
bcorrelates with the ground truth Iband the reference im-
ageIa. To enhance the effectiveness of network training,
we adopt a Siamese network training strategy to train the
back-view image prediction network.
共享权
重
sharing weights
共享权
重
Figure 3. Training framework of the back-view image estimation.
The training framework is shown in Figure 3. The twin
networks have the same parameters and sharing weights.
During testing, only one network is used to predict the back-
view image. We input a pair of images IaandIbinto the
twin networks and output their back-view image I′
band
I′
a, respectively. We establish constraints between the es-
timated and input images to train the network.In summary, we train the back-view image prediction
network with the following loss:
Lcolor=λ1(L1(I′
b, Ib) +L1(I′
a, Ia))+
λ2(Ls(I′
b, Ia) +Ls(I′
a, Ib)),(10)
where the hyper-parameters λ1= 1.0andλ2= 0.8control
the relative weights of the different terms. We use HR-Net
[37] as the back-view image prediction network.
3.3. Wavelet-based Diffusion FOF Prediction
The training framework is shown in Figure 4. We learn
the conditional denoising process pθ(z0:T|z0, x)with the
reference image Iaand the predicted back-view image I′
b
as the condition x. In our work, the Eq. 6 is modified to:
pθ(z0:T|x) =p(zT)QT
t=1pθ(zt−1|zt, x). (11)
As our task involves optimizing Fourier occupancy
fields, we train the denoising network fθto predict z0in-
stead of the noise ϵ. Once trained, at generation time, the
model fθcan then approximate the mean µθ(zt, t)of the
posterior pθ(zt−1|zt, x)as:
µθ(zt, t)≈1√αt
zt−1−αt
1−¯αt 
zt−√¯αtfθ(zt, t)
.
(12)
Thus, we can obtain the less noisy image zt−1from the
noisy image ztby sampling the approximate posterior in
each generation step.
We use DWT to decompose the FOF Cinto four wavelet
sub-bands as the training data z0in the diffusion model. At
even arbitrary time step t, the sampling ztis constructed
by adding Gaussian noise to the ground truth z0in the for-
ward diffusion. The inverse denoising is to predict z0with
the condition x. We estimate the initial FOF in the wavelet
domain and further refine it in the pixel domain.
Firstly, we design an image encoder module to extract
multi-scale feature representations from the reference im-
ages Iaand predicted back-view image I′
b. We use the
low-resolution image feature as the condition xin the con-
ditional diffusion model. Then, we concatenate zt,xand
tand feed them into the denoising network fθ, outputs
wavelet coefficient ˜z0. The wavelet coefficient ˜z0is con-
verted into the FOF Cinitusing IWT. Finally, We design a
refinement module to refine the initial predicted FOF Cinit
with the high-resolution image feature as guidance and out-
put a perfect FOF Crefine .
We formulate three objective functions to train the model
Lgeometric =Ldiffu(˜z0, z0) +Linit(Cinit, Cgt)+
Lrefine(Crefine, Cgt).(13)
The loss Ldiffu(˜z0, z0)represents the difference between the
ground truth z0and predicted ˜z0:
Ldiffu(˜z0, z0) =∥˜z0−z0∥1. (14)
9528
DWT
共享
权重
…
 c c cImage encoder 
IWT
FOF refinement…… ……Forward diffusion 
Inverse diffusion 
Concatenate
Image featurec
…
Figure 4. The training framework of the FOF prediction. FOF Cis decomposed into sub-bands z0using DWT. At even time step t,
ztis obtained from z0through forward diffusion. Multi-scale features of the images IaandI′
bare extracted using an image encoder
network, with low-resolution features as condition x. The concatenation of zt,x, and tis input into the denoising network fθ, output
ˆz0. Subsequently, IWT is applied to ˆz0to convert it into FOF. Finally, a refinement module enhances the predicted FOF guided by high-
resolution features. Three loss functions, Ldiffu,Linit, andLrefine , are designed to optimize the network.
The loss Linit(Cinit, Cgt)represents the difference between
the ground truth Cgt(C)and the initial predicted FOF Cinit:
Linit(Cinit, Cgt) =∥Cinit−Cgt∥1. (15)
The loss Lrefine(Crefine, Cgt)represents the difference be-
tween the ground truth Cgtand the refined FOF Crefine :
Lrefine(Crefine, Cgt) =∥Crefine−Cgt∥1. (16)
Figure 5 presents the details of the image encoder net-
work and refinement network. CBAM [38] is a lightweight
attention module. We use the U-Net architecture [8] as our
denoise network fθ.
In the inference process, given a human image and the
predicted back-view image, we first extract the image fea-
ture as the condition. Then, we randomly sample the noisy
map˜zTfrom the Gaussian distribution ˜zT∼ N (0,I)and
iteratively predict ˜z0in the wavelet domain. We employ the
DDIM update rule [32] for the sampling process. Finally,
we use the inverse wavelet transform (IWT) to transform ˜z0
to the FOF and refine it with the image feature. We pro-
vide the pseudo-code of the geometric training procedure
and test procedure in the supplementary material.
Inputllllllllllc
Image encoder
Images
Predicted 
FOFRefined 
FOF
Downsample+conv3x3+BN+ReLUConv3x3+BN+ReLU
CBAMRefinement network
Figure 5. Image encoder network and refinement network.Table 1. Quantitative comparison of texture on the 2K2K dataset.
↑means the larger the better while ↓means the smaller the better.
Methods PSNR↑SSIM↑LPIPS↓
SiCloPe [24] 17.579 0.844 0.281
Ours 26.362 0.929 0.142
PIFu [27] 14.334 0.821 0.269
GTA [45] 18.352 0.842 0.236
Ours 25.252 0.925 0.157
(a) (b) (c) (d) (a) (b) (c) (d)
Figure 6. Qualitative comparison of back-view image estimation.
(a) Input image. (b) Ground Truth. (c) SiCloPe [24]. (d) Ours.
4. Experiments
4.1. Experimental Settings
Datasets. We utilize three public 3D clothed human
datasets: Thuman2.0 [43], 2K2K [13], and CLOTH4D [50],
for training and evaluating our method. We randomly select
400 scans from Thuman2.0 and 1000 scans from 2K2K for
our training dataset. The test set comprises 100 scans from
Thuman2.0, 300 from 2K2K, and 300 from CLOTH4D. For
each model, we render the FOF [9], front image, and back-
view image as a training pair at each viewpoint of 300 dif-
ferent viewpoints around it utilizing the weak perspective
camera model. Each render image resolution is 512×512.
Metrics. We evaluate texture performance using three
metrics: peak signal-to-noise ratio (PSNR), structural sim-
ilarity index (SSIM) and learned perceptual image patch
9529
Input  PIFu      GTA  OursFigure 7. Qualitative comparison of geometric and texture with methods PIFu [27] and GTA [45].
Table 2. Quantitative evaluation of geometric on the Thuman2.0, 2k2k, and CLOTH4D datasets (cm). The best and second-best results are
highlighted in bold and underlined on each dataset, respectively.
THuman2.0 2K2K CLOTH4D
Methods Chamfer ↓ P2S↓ Normal ↓Chamfer ↓ P2S↓ Normal ↓Chamfer ↓ P2S↓ Normal ↓
PIFu [27] 3.224 2.802 3.476 2.232 2.210 3.212 2.124 2.132 3.124
PIFuHD [28] 2.952 2.240 2.698 1.820 1.745 2.362 1.726 1.732 2.235
ICON [40] 2.378 1.820 2.792 1.142 1.012 2.242 1.114 1.198 1.854
FOF [9] 2.474 1.817 2.532 1.132 1.093 2.101 1.218 1.195 1.723
D-IF [42] 2.368 1.820 2.724 1.134 0.912 2.125 1.124 1.012 1.863
ECON [39] 2.312 1.878 2.584 1.174 1.192 1.885 1.132 1.098 1.684
2K2K [13] 2.523 1.914 2.532 1.129 1.107 1.754 1.146 1.132 1.647
GTA [45] 2.298 1.793 2.475 1.130 1.103 1.791 1.102 0.963 1.594
Ours 2.131 1.635 2.031 0.934 0.872 1.502 0.906 0.927 1.517
similarity (LPIPS). We use Chamfer distance and P2S
distance [9] to evaluate the geometric quality, compar-
ing reconstructed meshes with ground truth. We also
measure L1 normal error between normal images from
both meshes by rotating the camera at four fixed angles
{0◦,90◦,180◦,270◦}relative to the input view.
4.2. Comparison with the State-of-the-art Methods
Texture evaluation. We compare our method with
methods: SiCloPe [24], PIFu [27], and GTA [45]. SiC-
loPe estimates the back-view image; thus, we compare the
predicted back-view image. PIFu and GTA use the implicit
function to estimate the color of 3D points. To evaluate our
results, we render textured meshes by rotating the camera at
four fixed angles ( 0◦,90◦,180◦,270◦) relative to the input
view and compare the rendered images.
Table 1 presents the quantitative comparison results.When evaluating the estimated back-view image, our result
surpasses SiCloPe. Our method has also achieved the best
results when evaluating rendered images. Figure 6 presents
visual comparison results with method SiCloPe, where our
results exhibit greater clarity. Figure 7 presents visual com-
parison results with methods PIFu and GTA. Our texture
and geometric present greater clarity and realism in invis-
ible areas. Quantitative and qualitative experiments verify
the effectiveness of our texture estimation method.
Geometric evaluation. We compare our method with
methods: PIFu [27], PIFuHD [28], ICON [40], D-IF [42],
FOF [9], ECON [39], 2K2K [13], and GTA [45]. Table 2
reports the quantitative comparison results. We can see that
our method excels over the compared methods on the three
quantitative metrics.
Figure 8 and Figure 9 show qualitative comparison re-
sults. In Figure 8, the two input images are from the 2K2K
9530
(a) (b) (c) (d) (e ) (f) (g) (h)
Input  Ground Truth  PIFuHD D-IF  GTA  ECON  FOF  OursFigure 8. Qualitative comparison of geometric on the 2K2K dataset with state-of-the-art single-view human reconstruction methods:
PIFuHD [28], D-IF [42], GTA [45], ECON [39], and FOF [9].
(a)                    (b)                        (c)                        (d)                            (e)              (f)                          (g)                       
Input              PIFuHD D-IF                          GTA                           ECON                         FOF            Ours
Figure 9. Qualitative comparison of geometric in-the-wild of images with state-of-the-art single-view human reconstruction methods:
PIFuHD [28], D-IF [42], GTA [45], ECON [39], and FOF [9].
dataset. Our method can accurately reconstruct the geom-
etry and capture more realistic wrinkle details. Figure 9
shows two examples of in-the-wild images. The first im-
age is captured using a camera under natural lighting condi-
tions; the second is from the Internet. Our method achieves
outstanding results in reconstructing loose-fitting clothing
and recovering more reasonable details in invisible areas.
Figure 9 validates our method has robustness and general-
ization. From both quantitative and qualitative results, our
geometric reconstruction method is effective.4.3. Ablation Study
Ablation on back-view image estimation. To validate
the effectiveness of the proposed style loss Lsand Siamese
network training strategy, we design the following three
variants: (1) loss L1: L1 loss between the predicted back-
view image and the ground truth back-view image. (2) loss
(L1+Ls): loss L1combines with the style consistency loss
Lsbetween the predicted back-view image and the input
image. (3) loss ( L1+Ls) with Siamese: loss ( L1+Ls) com-
bines with the Siamese network training strategy.
9531
We train the three variants and evaluate the results on the
2K2K dataset. The comparison results of these three vari-
ants are summarized in Table 3. The results show that the
style loss Lsand Siamese network training strategy are two
important factors for our back-view image prediction. We
also present two qualitative results in Figure 10. Compared
to only using the L1loss, incorporating the style loss Ls
yields more reasonable results.
Table 3. Ablation study on back-view image estimation.
Methods PSNR↑SSIM↑LPIPS↓
loss (L1) 18.427 0.886 0.247
loss (L1+Ls) 24.476 0.920 0.174
loss (L1+Ls) with Siamese 26.362 0.929 0.142
(a) (b) (c) (a) (b) (c)
Figure 10. Ablation study for style loss Lsin the back-view image
prediction. (a) Input image, (b) w/o style loss. (c) w style loss.
Ablation on geometric. To validate the effectiveness
of the predicted back-view image I′
b, diffusion model, and
wavelet transform, we design the following eight variants:
(1) FOF-M1: baseline method with the reference image Ia
as input. (2) FOF-M2: baseline method with IaandI′
bas in-
put. (3) FOF-M3: FOF-M2 combines with refinement mod-
ule. (4) Ours-W1: training on the diffusion model with Ia
as condition. (5) Ours-W2: training on the diffusion model
withIaandI′
bas condition. (6) Ours-M1: training on the
wavelet diffusion model with Iaas condition. (7) Ours-M2:
training on the wavelet diffusion model with IaandI′
bas
condition. (8) Ours-M3: Ours-M2 combines with refine-
ment module.
We train the eight variants and evaluate the results. The
comparison results are summarized in Table 4. The results
confirm that the predicted back-view image I′
b, the diffusion
model, and the wavelet transform are three important fac-
tors for our geometric prediction. We also present qualita-
tive results in Figure 11. The geometric quality of FOF-M2
is higher than that of FOF-M1, indicating that the back-view
image contributes beneficially to geometric reconstruction.
Compared to the baseline method, our approach achieves
higher geometric accuracy in reconstruction, particularly in
capturing details such as the scarves and facial features.Table 4. Ablation study on 3D reconstruction (cm).
THuman2.0 2K2K
Methods Chamfer ↓P2S↓Normal ↓Chamfer ↓P2S↓Normal ↓
FOF-M1 2.474 1.817 2.532 1.132 1.093 2.101
FOF-M2 2.467 1.808 2.528 1.127 1.085 1.984
FOF-M3 2.462 1.803 2.522 1.122 1.078 1.971
Ours-W1 2.245 1.700 2.224 0.981 0.916 1.705
Ours-W2 2.243 1.695 2.221 0.978 0.912 1.700
Ours-M1 2.241 1.690 2.219 0.972 0.907 1.694
Ours-M2 2.138 1.642 2.114 0.942 0.887 1.584
Ours-M3 2.131 1.635 2.031 0.934 0.872 1.502
Input   FOF-M1 FOF-M2 FOF-M3 
back-view       Ours -M1   Ours -M2    Ours -M3 
Figure 11. Qualitative comparison with PIFu [27].
Limitation. Our method can reconstruct the 3D clothed
human model from a single image. However, the recon-
structed geometric accuracy may be lower when the human
self-occlusion is severe in the image.
5. Conclusion
We have proposed a diffusion model to reconstruct a 3D
human model from a single image. First, we proposed a
style consistency constraint between the back-view and ref-
erence images to effectively predict the back-view image.
We then proposed a wavelet-based diffusion model in the
geometric prediction to generate the FOF conditional on the
two images. The two images are mapped onto the human
model, creating a textured clothed human model. Experi-
mental results indicated that our texture estimation and ge-
ometric reconstruction methods are effective.
6. Acknowledgments
This work is partially supported by the National Natu-
ral Science Foundation of China (No. 61972298 and No.
62372336) and Wuhan University-Huawei GeoInformatices
Innovation Lab.
9532
References
[1] Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar,
Christian Theobalt, and Gerard Pons-Moll. Learning to re-
construct people in clothing from a single rgb camera. In
CVPR , pages 1175–1186, 2019. 1, 2
[2] Thiemo Alldieck, Gerard Pons-Moll, Christian Theobalt,
and Marcus Magnor. Tex2shape: Detailed full human body
geometry from a single image. In ICCV , 2019. 1, 2
[3] Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Se-
bastian Thrun, Jim Rodgers, and James Davis. Scape: Shape
completion and animation of people. ACM Transactions on
Graphics , 24(3):408–416, 2005. 2
[4] Zhongyun Bao, Chengjiang Long, Gang Fu, Daquan Liu,
Yuanzhen Li, Jiaming Wu, and Chunxia Xiao. Deep image-
based illumination harmonization. In CVPR , pages 18542–
18551, 2022. 1
[5] Tuo Cao, Wenxiao Zhang, Yanping Fu, Shengjie Zheng, Fei
Luo, and Chunxia Xiao. Dgecn++: A depth-guided edge
convolutional network for end-to-end 6d pose estimation via
attention mechanism. IEEE Transactions on Circuits and
Systems for Video Technology , 2023. 1
[6] Gene Chou, Yuval Bahat, and Felix Heide. Diffusion-sdf:
Conditional generative modeling of signed distance func-
tions. In ICCV , 2023. 2
[7] Ruihang Chu, Enze Xie, Shentong Mo, Zhenguo Li,
Matthias Nießner, Chi-Wing Fu, and Jiaya Jia. Diffcomplete:
Diffusion-based generative 3d shape completion. arXiv
preprint arXiv:2306.16329 , 2023. 2
[8] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. In NeurIPS , pages 8780–8794,
2021. 1, 5
[9] Qiaojun Feng, Yebin Liu, Yu-Kun Lai, Jingyu Yang, and Kun
Li. Fof: Learning fourier occupancy field for monocular real-
time human reconstruction. In NeurIPS , 2022. 1, 2, 3, 5, 6,
7
[10] Yanping Fu, Qingan Yan, Jie Liao, and Chunxia Xiao. Joint
texture and geometry optimization for rgb-d reconstruction.
InCVPR , pages 5950–5959, 2020. 1
[11] Valentin Gabeur, Jean-S ´ebastien Franco, Xavier Martin,
Cordelia Schmid, and Gregory Rogez. Moulding humans:
Non-parametric 3d human shape estimation from single im-
ages. In ICCV , pages 2232–2241, 2019. 1, 2
[12] Alfred Haar. Zur theorie der orthogonalen funktionensys-
teme . Georg-August-Universitat, Gottingen, 1909. 3
[13] Sang-Hun Han, Min-Gyu Park, Ju Hong Yoon, Ju-Mi Kang,
Young-Jae Park, and Hae-Gon Jeon. High-fidelity 3d hu-
man digitization from single 2k resolution images. In CVPR ,
2023. 2, 5, 6
[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. In NeurIPS , pages 6840–6851.
2020. 2, 3
[15] Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu,
Zhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo. Ddp:
Diffusion model for dense visual prediction. arXiv preprint
arXiv:2303.17559 , 2023. 2[16] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
losses for real-time style transfer and super-resolution. In
ECCV , pages 694–711, 2016. 4
[17] Angjoo Kanazawa, Michael J Black, David W Jacobs, and
Jitendra Malik. End-to-end recovery of human shape and
pose. In CVPR , pages 7122–7131, 2018. 2
[18] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and
Kostas Daniilidis. Learning to reconstruct 3d human pose
and shape via model-fitting in the loop. In ICCV , pages
2252–2261, 2019. 2
[19] Yuanzhen Li, Fei Luo, and Chunxia Xiao. Monocular human
depth estimation with 3d motion flow and surface normals.
The Visual Computer , 39(8):3701–3713, 2023. 1
[20] Daquan Liu, Chengjiang Long, Hongpan Zhang, Hanning
Yu, Xinzhi Dong, and Chunxia Xiao. Arshadowgan: Shadow
generative adversarial network for augmented reality in sin-
gle light scenes. In CVPR , pages 8139–8148, 2020. 1
[21] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J. Black. Smpl: A skinned multi-
person linear model. ACM Transactions on Graphics , 34(6),
2015. 1, 2
[22] William E. Lorensen and Harvey E. Cline. Marching cubes:
A high resolution 3d surface construction algorithm. ACM
SIGGRAPH Computer Graphics , pages 163–169, 1987. 3
[23] Fei Luo, Yongqiong Zhu, Yanping Fu, Huajian Zhou,
Zezheng Chen, and Chunxia Xiao. Sparse rgb-d images cre-
ate a real thing: a flexible voxel based 3d reconstruction
pipeline for single object. Visual Informatics , 7(1):66–76,
2023. 1
[24] Ryota Natsume, Shunsuke Saito, Zeng Huang, Weikai Chen,
Chongyang Ma, Hao Li, and Shigeo Morishima. Siclope:
Silhouette-based clothed people. In CVPR , pages 4475–
4485, 2019. 1, 2, 5, 6
[25] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,
Jonathan Ho, Tim Salimans, David Fleet, and Mohammad
Norouzi. Palette: Image-to-image diffusion models. In ACM
SIGGRAPH Conference Proceedings , pages 1–10, 2022. 2
[26] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sal-
imans, David J Fleet, and Mohammad Norouzi. Image
super-resolution via iterative refinement. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 45(4):4713–
4726, 2022. 2
[27] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-
ishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned
implicit function for high-resolution clothed human digitiza-
tion. In ICCV , pages 2304–2314, 2019. 1, 2, 5, 6, 8
[28] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul
Joo. Pifuhd: Multi-level pixel-aligned implicit function for
high-resolution 3d human digitization. In CVPR , pages 84–
93, 2020. 1, 2, 6, 7
[29] Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, and
David J Fleet. Monocular depth estimation using diffusion
models. arXiv preprint arXiv:2302.14816 , 2023. 2
[30] Ruizhi Shao, Zerong Zheng, Hongwen Zhang, Jingxiang
Sun, and Yebin Liu. Diffustereo: High quality human recon-
struction via diffusion-based stereo using sparse cameras. In
ECCV , 2022. 2
9533
[31] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 , 2014. 4
[32] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 5
[33] Yang Song and Stefano Ermon. Generative modeling by es-
timating gradients of the data distribution. In NeurIPS , 2019.
2
[34] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. arXiv preprint arXiv:2011.13456 , 2020. 2
[35] Gul Varol, Duygu Ceylan, Bryan Russell, Jimei Yang, Ersin
Yumer, Ivan Laptev, and Cordelia Schmid. Bodynet: V olu-
metric inference of 3d human body shapes. In ECCV , 2018.
1
[36] Gul Varol, Duygu Ceylan, Bryan Russell, Jimei Yang, Ersin
Yumer, Ivan Laptev, and Cordelia Schmid. Bodynet: V olu-
metric inference of 3d human body shapes. In ECCV , pages
20–36, 2018. 2
[37] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,
Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui
Tan, Xinggang Wang, Wenyu Liu, and Bin Xiao. Deep
high-resolution representation learning for visual recogni-
tion. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 43(10):3349–3364, 2021. 4
[38] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So
Kweon. Cbam: Convolutional block attention module. In
ECCV , pages 3–19, 2018. 5
[39] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and
Michael J. Black. ECON: Explicit Clothed humans Opti-
mized via Normal integration. In CVPR , 2023. 6, 7
[40] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J
Black. Icon: implicit clothed humans obtained from normals.
InCVPR , pages 13286–13296, 2022. 2, 6
[41] Long Yang, Qingan Yan, Yanping Fu, and Chunxia Xiao.
Surface reconstruction via fusing sparse-sequence of depth
images. IEEE Transactions on Visualization and Computer
Graphics , 24(2):1190–1203, 2017. 1
[42] Xueting Yang, Yihao Luo, Yuliang Xiu, Wei Wang, Hao Xu,
and Zhaoxin Fan. D-IF: Uncertainty-aware Human Digiti-
zation via Implicit Distribution Field. In ICCV , 2023. 2, 6,
7
[43] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qiong-
hai Dai, and Yebin Liu. Function4d: Real-time human vol-
umetric capture from very sparse consumer rgbd sensors. In
CVPR , 2021. 5
[44] Zongsheng Yue and Chen Change Loy. Difface: Blind face
restoration with diffused error contraction. arXiv preprint
arXiv:2212.06512 , 2022. 2
[45] Zechuan Zhang, Li Sun, Zongxin Yang, Ling Chen, and
Yi Yang. Global-correlated 3d-decoupling transformer for
clothed avatar reconstruction. In NeurIPS , 2023. 5, 6, 7
[46] Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai. Pamir:
Parametric model-conditioned implicit representation for
image-based human reconstruction. IEEE Transactionson Pattern Analysis and Machine Intelligence , 44(6):3170–
3184, 2022. 2
[47] Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, and
Yebin Liu. Deephuman: 3d human reconstruction from a
single image. In ICCV , pages 7739–7749, 2019. 1
[48] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape genera-
tion and completion through point-voxel diffusion. In ICCV ,
pages 5826–5835, 2021. 2
[49] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape genera-
tion and completion through point-voxel diffusion. In ICCV ,
pages 5826–5835, 2021. 2
[50] Xingxing Zou, Xintong Han, and Waikeung Wong. Cloth4d:
A dataset for clothed human reconstruction. In CVPR , pages
12847–12857, 2023. 5
9534
