OVER-NA V: Elevating Iterative Vision-and-Language Navigation with
Open-Vocabulary Detection and StructurEd Representation
Ganlong Zhao1,2Guanbin Li2,3*Weikai Chen4Yizhou Yu1∗
1The University of Hong Kong2Sun Yat-sen University
3GuangDong Province Key Laboratory of Information Security Technology
4Digital Content Technology Center, Tencent Games
zhaogl@connect.hku.hk, liguanbin@mail.sysu.edu.cn, chenwk891@gmail.com, yizhouy@acm.org
Abstract
Recent advances in Iterative Vision-and-Language Nav-
igation (IVLN) introduce a more meaningful and practical
paradigm of VLN by maintaining the agent’s memory across
tours of scenes. Although the long-term memory aligns bet-
ter with the persistent nature of the VLN task, it poses more
challenges on how to utilize the highly unstructured navi-
gation memory with extremely sparse supervision. Towards
this end, we propose OVER-NAV , which aims to go over and
beyond the current arts of IVLN techniques. In particular,
we propose to incorporate LLMs and open-vocabulary de-
tectors to distill key information and establish correspon-
dence between multi-modal signals. Such a mechanism in-
troduces reliable cross-modal supervision and enables on-
the-fly generalization to unseen scenes without the need of
extra annotation and re-training. To fully exploit the inter-
preted navigation data, we further introduce a structured
representation, coded Omnigraph, to effectively integrate
multi-modal information along the tour. Accompanied with
a novel omnigraph fusion mechanism, OVER-NAV is able to
extract the most relevant knowledge from omnigraph for a
more accurate navigating action. In addition, OVER-NAV
seamlessly supports both discrete and continuous environ-
ments under a unified framework. We demonstrate the su-
periority of OVER-NAV in extensive experiments.
1. Introduction
Vision-and-Language Navigation (VLN) [1] aims to build
intelligent agents that can follow natural language instruc-
tions to navigate in the unseen environments. However,
existing VLN benchmarks eliminate the agent’s memory
*Corresponding authors are Guanbin Li and Yizhou Yu. This work was
supported in part by the National Natural Science Foundation of China
(NO. 62322608), in part by the CAAI-MindSpore Open Fund, developed
on OpenI Community.
Close-
Vocabulary:Annotate objects
chair, table, cushion, cabinet, 
shelving, sink, dresser, plant, bed, 
sofa, counter, fireplaceTrain Seg Models Build Maps
e.g., RedNetSegmentation Map 
(Fixed Class Num)
OursExtra Module 
in VLN Agent
Open-Vocabulary 
DetectionFlexible and Variant-
Length Keyword MapReuse Instruction 
Processing Module
Episode 2: Go straight 
through the kitchen. Wait 
near the tableand chairs.
Turn left and exit the 
room. Cross the hall to 
the sitting room . Turn left 
and enter the bedroom 
on your left . Wait near 
the bed.
Viewpoint Ground Truth Path Navigation Path
Oracle Goal Path Oracle Start Path bed/kitchen: Close/Open Set KeywordFigure 1. Top: Example of a two-episode tour. The agent first
navigates the environment following the instruction of episode 1
(Yellow). Then the agent is directed to the ground truth goal as
Oracle Goal phase (Red). Later the agent travels to the start point
of episode 2 in the Oracle Start phase (Blue). Finally, the agent
navigates the environment following the next instruction (Yellow).
Bottom: Comparison between previous methods and ours. Close-
vocabulary methods require extra annotation and training efforts
to provide segmentation results in navigation, and the agent is lim-
ited to a close set of categories building segmentation maps. Our
method proposed an open-vocabulary-based omnigraph which is
more flexible for various keywords and circumstances.
upon the start of every episode, failing to leverage the vi-
sual observations and the iteratively built maps collected by
the physical robots. Recent work on Iterative Vision-and-
Language Navigation (IVLN) [25] introduces a more mean-
ingful and practical paradigm that orders the episodic tasks
in VLN as tours and allows the agents to utilize memory to
achieve better navigation performance.
We demonstrate a typical procedure of an IVLN task in
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
16296
Figure 1. An IVLN agent receives a ordered sequence of
instructions that fulfill a tour of the target scene. Each tour
consists of individual episodes, each guided by a language
instruction. When the agent completes the navigation of an
instruction, it is teleoperated by an oracle to the correct goal
location, and then translated to the start of the next episode.
Hence, each episode is composed of three parts: naviga-
tion, oracle goal, and oracle start. In the navigation phase,
the agent receives the environment observations along the
pathPIfollowing the given instruction I. In the oracle
goal/start phase, no instruction is present except the obser-
vations along the oracle start path POSand the oracle goal
pathPOG. Thus the tour history of each episode is repre-
sented as {POS,(I, PI), POG}.
Although the memory-retaining strategy aligns well with
the persistent nature of the VLN tasks, it poses additional
challenges on how to fully exploit the unstructured nav-
igation history of previous episodes. First, it remains
formidable to interpret the multi-modal information that
spans a number of domains including language (instruc-
tion), vision (visual measurements), time, and location
(physical movement), without any explicit supervision. The
only weak supervision existing in the tour history is the cor-
respondence between the instruction Iand the navigation
pathPI. However, such correspondences are coarse and
ambiguous as the visual observations along the path may
not correspond to the order of the objects and actions ap-
peared in the instruction. Moreover, PIcould deviate from
the correct path due to the erroneous decisions made along
the navigation. Therefore, obtaining reliable supervision
over the navigation history and establishing faithful corre-
spondence between the multi-modal data are the keys to the
success of IVLN tasks.
In the ideal scenarios where all the multi-modal data can
be thoroughly interpreted, the second challenge arises in
structurizing the extensive memory so that it can be effec-
tively utilized under the IVLN framework. Prior works [25]
show that a naive stacking of the history data for feature rep-
resentation would lead to inferior performance. Addition-
ally, a well structured memory should be general enough to
accommodate varying settings in IVLN tasks, e.g. the dis-
crete [1, 25] and continuous environments [24, 25] which
are conventionally tackled with distinct strategies.
To address the above challenges, we present OVER-
NA V, a novel framework that strives to go over and beyond
the current arts of IVLN solutions. To combat with the lack
of explicit supervisions, we propose to incorporate Large
Language Models (LLMs) and Open-V ocabulary Detection
(OVD) to extract key information from the unattended data
flow. Specifically, while LLMs are leveraged to identify
keywords from the language instructions, the OVD detec-
tors are employed to build the correspondence between the
keywords and the visual observations. Such a strategy is ca-pable of providing distilled supervision which is critical to
understanding the unordered navigation data. As the OVD
detector can scale up to novel categories, OVER-NA V is
able to generalize to unseen scenes on the fly without the
need of extra annotation and re-training, offering great flex-
ibility over the methods using closed-set detectors [3, 25].
To better harness the extracted supervisory signals, we
further introduce a structured representation, coded Omni-
graph , to integrate multi-modal information along the tour.
By leveraging a novel fusion mechanism, omnigraph can ef-
ficiently collect the pertinent knowledge for a more accurate
navigation action. Moreover, the omnigraph representation
can seamlessly support both discrete and continuous envi-
ronments under a unified framework. We extensively eval-
uate OVER-NA V on a number of challenging benchmarks.
The experimental results demonstrate the superiority of our
method over the state-of-the-art approaches. In summary,
our contributions are:
• A novel framework dubbed OVER-NA V, that, for the
first time, incorporates LLMs and OVD into the IVLN
paradigms to distill reliable and generalizable supervision
signals from the unordered navigation data.
• A structured and general representation called Omnigraph
that facilitates the utilization of multi-modal knowledge
and can be generalized to different VLN settings.
• Superior performance on the IVLN tasks in both discrete
and continuous environments.
2. Related Works
Vision-and-Language Navigation Vision-and-Language
Navigation (VLN) [1, 10, 11, 15, 16, 21, 26, 28, 30, 36, 41]
requires an agent with the ability to navigate a never-before-
seen environment following a natural language instruction
that describes the ground truth navigation path. There are
two major settings in VLN benchmarks, discrete [1, 26, 36]
and continuous environments [24, 40]. In the discrete set-
ting, the VLN agent is limited to changing position and ori-
entation by discrete amounts or predefined options, while
the continuous setting provides a continuous range for the
agent’s action. Iterative VLN [25] evaluates the agent in
persistent environments, and the agent needs to utilize prior
experience in the environment for better performance.
Persistent Environment and Iterative VLN The increas-
ing amount and improved quality of 3D scene datasets [4,
39], and the high-performance navigation environment sim-
ulation platform [23, 31, 40, 44] significantly promote the
development of navigation tasks, and make it possible to
study the long-horizon tasks in persistent environments
such as visual navigation [42, 43], multi-object naviga-
tion [42], visual room rearrangement [43], courier task [33],
multi-target embodied QA [45], scene exploration and ob-
ject search [9]. Iterative VLN [25] further studies VLN in
16297
persistent environments and enriches the long-horizon vi-
sual navigation problem with natural language and linguis-
tic information. Previous IVLN studies demonstrate that
structured memory [3, 5, 7, 12, 14, 27, 35, 38, 42] is es-
sential for IVLN agents. TourHAMT [25] tries to mitigate
the problem in discrete environments by adding the history
embedding from previous episodes to the current episode
but fails to improve the performance. MAP-CMA [25] con-
structs semantic and occupancy maps [3] from the point
cloud built by fine-tuned RedNet [19] and depth images
for action prediction. However, point cloud construction
requires depth sensors, and the RedNet is close-vocabulary
and only limited to thirteen kinds of objects. This requires
extra data collection labor for fine-tuning and prevents the
agent from scaling to unseen environments and complicated
concepts. TourHAMT/MAP-CMA can only be applied to
discrete/continuous environments. Despite MAP-CMA’s
success, it is hard to transfer its solution to discrete environ-
ments as semantic maps require continuous observations.
Open-Vocabulary Detection Open-vocabulary detection
(OVD) aims to train object detectors beyond recognizing
only base categories present in training labels and expand
the vocabulary to detect novel categories. With the de-
velopment of powerful language encoders [22] and con-
trastive image-text training [18, 37, 47], recent works trans-
fer the language capabilities of these models to OVD [13,
20, 29, 46, 48, 49]. ViLD [13] distills the knowledge from a
pretrained open-vocabulary image classification model into
a two-stage detector following a teacher-student training
paradigm. MDETR [20] and GLIP [29] take a single text
query for the image then formulate detection as the phrase
grounding problem. Owl-ViT [32] combines Vision Trans-
former [8], contrastive image-text pre-training [37] and end-
to-end detection fine-tuning. We study OVD in persistent
environments thus promoting the development of VLN.
3. OVER-NA V
The framework of our proposed method is depicted in Fig 2.
Large Language Models (LLMs) extract keywords from the
instruction of each episode. As the agent traverses the en-
vironment, it sends the keywords and observations along its
path to the OVD detector for detection. The detection re-
sults are subsequently stored in the omnigraph, which is es-
tablished and maintained to memorize the observed portion
of the current environment. The omnigraph generates the
keyword input for the agent’s action prediction. This input
encapsulates the information of an ego-centric map, thereby
aiding the prediction process.
3.1. OVD-based Omnigraph Construction
The OVER-NA V aims to construct an organized omnigraph
that allows the IVLN agent to memorize and utilize the his-tory information from previous episodes in the same tour.
It incorporates three distinct processes: keyword extraction,
keyword-panoramic detection, and omnigraph construction.
Keyword Extraction The IVLN agent receives an instruc-
tion each episode in the tour. A typical navigation instruc-
tion can be decomposed into two parts: milestones and ac-
tions. The milestones (hereinafter referred to as keywords)
are related to the scene where the navigation task is issued.
Conversely, actions are likely episodic and related to the
agent’s current orientation. For instance, consider the in-
struction “Head past the dining table and turn left towards
the kitchen” , the terms dining table andkitchen serve as
milestones, enabling the agent to discern different direc-
tions and movements ( e.g.,head past ,turn left ) and verify
the accuracy of previous actions. Thus the keywords pro-
vide the agent with a condensed understanding of the scene.
We propose employing Large Language Mod-
els (LLMs) [2, 34] for keyword extraction. The system
prompt of GPT is provided in supplementary material. The
GPT is fed instructions following the format defined in
the prompt. Subsequently, the LLM responds to the query
with the appropriately formatted keywords. One of the key
benefits of using LLMs for keyword extraction lies in their
flexibility because the keywords in instructions can vary
greatly in length and category. For instance, keywords like
“counter with the blue top ” carry significant attributes for
the agent’s reference, and LLMs can effectively identify
such keywords with their strong in-context learning ability.
Keyword-Panoramic Detection After keyword extraction,
the agent extracts a keyword set KI={ki}N
i=1withNkey-
words from each instruction I. Throughout the navigation
process of instruction I, the agent constantly observes the
environment and captures images from its current position
and orientation. Following the common practice of some
VLN models, e.g., HAMT [6], we assume that the agent
can acquire panoramic photos Ppano of the surrounding en-
vironment. This can be achieved by equipping the agent
with a panoramic camera or rotating the agent 360◦.
Upon reaching a position and capturing panoramic im-
ages, the agent can perform open-vocabulary detection us-
ing the keyword set KIand the image Ppano. This posi-
tion is subsequently recorded as a viewpoint in the agent’s
memory. The OVD detector Dgenerates a set of detec-
tion boxes {Bi, li, ci}M
i=1as detection results, where each
boxBiis with a label liand a confidence score ci. Each
boxBicontains four values (xmin, ymin, xmax, ymax)to
locate the object in the panoramic image. These values can
be used to calculate the relative heading of the detected ob-
ject concerning the agent’s orientation. This can then be
transformed into the absolute heading hiin the coordinate
system of the current environment by deducting the agent’s
heading. Thus, the detection results stored in the agent’s
memory for the current viewpoint are {Bi, li, ci, hi}M
i=1.
16298
Go straight, pass the piano and the 
pictures on the wall then turn left and 
head down the hallway then turn right and 
stop by the bed.
LLM
piano wall picture hallway
AgentOmnigraphInstruction
Prompt
……
Keywords
Open-Vocabulary 
Detector
Panoramic View
pianopicture
hallway
picture
Detection Resultsviewpoint
current viewpoint
connectivity
tag attribute
piano: (𝑐௜,ℎ௜)
picture: (𝑐௜,ℎ௜)
hallway: (𝑐௜,ℎ௜)
……
neighbour set
violin
picture
……bathtub
……
desk
……
Memory Updatepiano
picture
……piano
Picture
violin
……
Action
Positional Keywords InputNeighbouring Set
Keywords Filtering
piano
Picture
violin
……
couch
bed
bedroom
……couch
bed
bedroom
……piano
picture
hallway
……
piano
picture
……
Distance & ViewIndex
Assignment
piano: (𝑑଴,ℎ଴)
picture: (𝑑ଵ,ℎଵ)
hallway: (𝑑ଶ,ℎଶ)
couch: (𝑑ଷ,ℎଷ)
bed: (𝑑ସ,ℎସ)
bedroom: (𝑑ହ,ℎହ)
violin: (𝑑଺,ℎ଺)
……
Omnigraph FusionFigure 2. The overview of our proposed method. The instruction is sent to LLMs with the prompt to obtain keywords. The open-vocabulary
detector receives the keywords and the panoramic view at the current position, and sends the detection results to the agent. With the
detection results containing the distribution of detected objects, e.g., heading and confidence, the agent maintains the omnigraph that stores
the information of visited viewpoints in previous episodes. Each viewpoint is tagged with keywords and distribution information. For
inference, the omnigraph first collects the neighboring viewpoints and filters their keywords, then fuses the keywords with corresponding
positional information, e.g., heading and confidence. Finally, the resulting positional keyword inputs are sent to the agent for prediction.
In each episode, the agent performs the detection with
KIandPpano at every viewpoint along the navigation
path. Each viewpoint along the path is tagged with detec-
tion boxes, labels, confidence, and absolute heading, which
summarize the properties of viewpoints in the scene. We
will further elaborate the details, e.g., the location of view-
points, in Sec. 3.2.1 and Sec. 3.2.2.
Omnigraph Construction After keyword extraction and
keyword-panoramic detection, all visited viewpoints are
tagged with keywords. We then construct the omnigraph
as a graph whose nodes are the viewpoints with keywords
and edges are the connectivity between viewpoints. The
omnigraph organizes the viewpoint, delineates the structure
of the current scene, and outlines the distribution of various
objects identified through panoramic keyword detection. It
can be readily applied to both discrete and continuous en-
vironments. Furthermore, the open-vocabulary omnigraph
can handle a wide array of keywords, and also empowers
the model to discern the intrinsic relationship between key-
words. For example, if the agent determines the relationship
between the living room and the television , it can make an
informed decision when instructed to go to the living room ,
even if only the television is present in the omnigraph.
Specifically, when the agent arrives at a new viewpoint
afrom a previous viewpoint b, and obtains the detection re-
sults, it incorporates the viewpoint ainto the graph as a new
node, and adds the undirected edge ⟨a, b⟩to the omnigraph.
Ifais the starting point, only ais added to the graph. During
the navigation, the omnigraph is incrementally refined in
two ways: i) the discovery of viewpoints updates the nodesand edges, ii) when the agent arrives at a previously vis-
ited viewpoint discovered in an earlier episode Ebin a new
episode Ea, the keyword-panoramic detection will generate
fresh detection results with the new instruction IEa, which
might have different keywords from the previous instruction
IEb. The new detection results are then employed to update
the keywords associated with this viewpoint.
3.2. Omnigraph Fusion
After the omnigraph construction, the agent needs to exploit
the information in the omnigraph at each step during its nav-
igation. However, directly feeding the entire omnigraph to
the agent is computationally intensive and inefficient as the
agent only navigates a small area of the entire scene and the
number of detection boxes might be overwhelmingly large.
Hence, we fuse the information of the omnigraph within a
local and ego-centric subgraph and send it to the agent. We
reuse the agent’s instruction encoder to extract the embed-
dings of keywords in omnigraph subgraph, which not only
eliminates the cost of the extra module but also preserves
the semantic consistency between instruction embeddings
and keyword embeddings. Then the extra information for
each keyword is fused to enrich the keyword embeddings.
Here we discuss the omnigraph fusion in discrete and con-
tinuous environments respectively. The overviews of the
two agents are provided in supplementary materials.
3.2.1 OVER-NA V for Discrete Environments
In discrete VLN, the environment is represented as a set of
pre-defined viewpoints, and the agent can navigate through
16299
the connections between these viewpoints. Following pre-
vious studies, we integrate OVER-NA V into the History
Aware Multimodal Transformer (HAMT) [6] framework to
work with discrete environments.
HAMT [6] trains the agent with both text-modal and
vision-modal inputs. HAMT agent encodes the input in-
struction W= (w1, w2, ..., w L)withLwords as instruction
embeddings X= (xCLS, x1, x2, ..., x L)using BERT, and
encodes the panoramic observation Ot= (vo
1, vo
2, ..., vo
K)
at step twithKdifferent views as observation embeddings
Ot= (o1, o2, ..., o K, ostop). HAMT proposes hierarchi-
cal history encoding to encode the navigation history Ht
at step t, which consists of all the past panoramic observa-
tionsO1,...,t−1and performed actions a1,...,t−1before step
t. The hierarchical history encoding produces history em-
bedding Ht= (hCLS, h1, ..., h t−1). Then the history em-
bedding and observation embedding are concatenated as vi-
sion modality, while instruction embedding serves as text
modality. The cross-modal transformer fuses them and out-
puts the probability distribution of selecting different views
of observation Otas the predicted action.
Since the discrete environment is already discretized into
viewpoints, the OVER-NA V agent performs object detec-
tion at each new viewpoint encountered, i.e., at every step.
For an agent positioned at viewpoint p, we prepare the om-
nigraph input using the following steps:
1. Neighbours Identification. Given distance dn, we collect
the neighboring viewpoints that are reachable from the
current position within dnsteps as the neighbor set.
2. Inner-viewpoint Detection Box Filtering. For each
neighbor, we filter out most of the detection boxes so
that each keyword detected in this neighbor is associated
with only one detection box with the highest detection
confidence ci.
3. Distance & View Index Assignment. For keyword kof
neighbouring viewpoint v, we assign the distance dk
vbe-
tween vand the current position to k, and the view in-
dexhk
vwhich is the direction of the next move the agent
should take to go to vfrom the current position to k.
Please note that both dk
vandhk
vare discrete rather than
continuous, which makes the next step possible.
4. Cross-viewpoint Keyword Filtering. For those keywords
that appear in more than one neighbour and thus have
multiple dk
vandhk
v, we select the most frequent dk
vand
hk
vas final attributes for k.
After these procedures, the agent obtains a set of dis-
tinct keywords and each keyword has a distance dk
vand a
view index hk
v. Following HAMT, we use BERT to ex-
tract the embeddings of keywords, then use the [CLS] to-
ken embeddings ECLS as the representation of each key-
word. The view index hk
vis converted to 4-dimension em-
beddings Ehkv= (sin θ,cosθ,sinϕ,cosϕ)where θandϕ
are the heading and elevation of view index hv. Therefore,the fused keyword embedding is computed as:
Ek=LN(WCLSECLS) +LN(WhkvEhkv) +Edkv,(1)
where Edkvis the distance embedding, WCLS andWhkvare
learnable weights, and LNis layer normalization. Then all
embeddings of keywords are concatenated:
Emap= Concat( Ek1, Ek2, ..., E kN), (2)
where Nis the number of keywords and Ekiis the fused
keyword embedding of the i-th closest keyword kito the
agent’s current position.
The embedding Emap can be viewed as the text-modal
context that indicates the position of every detected object in
the memory. Therefore, we concatenate Emapto instruction
embeddings Xand send them to the agent as text modality.
3.2.2 OVER-NA V for Continuous Environments
VLN in continuous environments discards the pre-defined
viewpoints for navigation and situates the agent in contin-
uous environments with low-level actions. Following pre-
vious studies, we apply our OVER-NA V to continuous en-
vironments by incorporating OVER-NA V into a variant of
Cross-Modal Attention [24], MAP-CMA [25].
MAP-CMA [25] is based on CMA [24], a common base-
line in recent works. CMA is an end-to-end recurrent model
that predicts actions from RGBD observations, the instruc-
tion and previous actions. CMA uses two recurrent net-
works, one for visual history tracking, and the other for
instruction and visual features. MAP-CMA replace the
RGBD observations with occupancy maps and semantic
maps, which are built by using inverse pinhole camera pro-
jection model to 3D pointclouds. MAP-CMA use a Red-
Net [19] feature encoder to form a semantic pointcloud,
which is fine-tuned on thirteen common labels.
The application of OVER-NA V to MAP-CMA is simi-
lar to HAMT, except that OVER-NA V needs to decide the
position of viewpoints, i.e., where the keyword-panoramic
detection should be performed. For an agent at position p,
we prepare the omnigraph input with the following steps:
1. Viewpoint Discovery. Given discovery threshold dvpas
a hyper-parameter, when the agent arrives at a position
whose distances to all recorded viewpoints in the mem-
ory are larger than dvp, this position is registered as a
new viewpoint vand the agent stores the observations
Pv
pano. Different from Discrete VLN, we perform the
keyword-panoramic detection with the stored observa-
tions Pv
pano at viewpoint vand keywords from current
instruction when the agent arrives at a position whose
distance to vis less than the hyper-parameter viewpoint
detection threshold ddet.ddet< dvp.
2. Neighbours Identification. Given a hyper-parameter dn,
we collect the neighboring viewpoints whose distance to
the current position is less than dn.
16300
Val-Seen Val-Unseen
# Model PH TH PHI IW TL NE ↓ OS↑nDTW↑SR↑ SPL↑t-nDTW ↑ TL NE ↓ OS↑nDTW↑SR↑ SPL↑t-nDTW ↑
1HAMT 10.1±0.14.2±0.170±171±163±161±158±1 9.4±0.14.7±0.064±166±056±054±050±0
2TourHAMT ✓ ✓ ✓ ✓ 9.4±0.45.8±0.156±159±045±143±145±0 10.0±0.26.2±0.152±252±039±136±032±1
3 ✓ ✓ ✓ 10.5±0.36.0±0.260±158±145±243±242±1 10.9±0.26.8±0.254±151±138±134±131±1
4 ✓ ✓ 10.6±0.36.0±0.161±158±145±142±142±1 10.3±0.36.7±0.252±150±138±134±129±1
5 ✓ 10.9±0.36.1±0.160±258±145±142±141±0 11.0±0.66.7±0.152±251±038±034±028±1
6Ours 9.9±0.13.7±0.170±073±165±163±162±0 9.4±0.14.1±0.166±169±060±157±055±1
Table 1. The comparison between our method, HAMT and TourHAMT on IR2R. PH: previous episodes’ history; TH: trainable history
encoder; PHI: previous history identifier; IW: inflection weighting. TourHAMT fails to outperform HAMT, while our method achieves
significant improvements in both val-seen and val-unseen datasets. We run each experiment 3 times and report metrics as ¯x±σ¯x.
Val-Seen Val-Unseen
# Model TL NE ↓ OS↑nDTW↑SR↑ SPL↑t-nDTW ↑ TL NE ↓ OS↑nDTW↑SR↑ SPL↑t-nDTW ↑
1CMA 7.8±0.48.8±0.627±342±318±317±339±1 7.5±0.38.8±0.226±144±119±118±138±2
2TourCMA 8.0±0.48.2±0.930±244±220±319±240±1 7.8±0.19.0±0.226±142±118±017±136±1
3PoolCMA 7.2±0.59.1±0.424±441±217±416±237±2 7.3±0.29.0±0.323±142±116±115±036±2
4PoolEndCMA 7.6±0.88.9±0.927±342±318±417±238±2 6.9±0.28.7±0.225±244±118±116±138±2
5MAP-CMA 9.4 6.4 48 56 39 36 52 8.5 6.8 44 54 35 32 47
6Ours 9.5±0.95.8±0.949±459±239±236±256±2 8.8±0.66.5±0.245±256±135±133±150±2
Table 2. The performance of our method on IR2R-CE. The experiment results of all previous methods are copied from [25]. Our method
and MAP-CMA both use inferred semantics and iterative map construction. Our method gains 4% and 3% t-nDTW improvement on
val-seen and val-unseen datasets respectively. We run each experiment 3 times and report metrics as ¯x±σ¯x.
3. Inner-viewpoint Detection Box Filtering. This part is the
same as Sec. 3.2.1.
4. Distance & Heading Assignment. Similar to Sec. 3.2.1,
we assign heading hk
vand distance dk
vto each keyword.
In this case, dk
vandhk
vare continuous.
5. Cross-viewpoint Keyword Filtering. We select the de-
tection box with the highest score for each keyword.
Similar to Sec. 3.2.1, the agent receives a set of distinct
keywords and each keyword has a heading hk
vand distance
dk
v. We use the instruction encoder of MAP-CMA to ex-
tract the embedding of every keyword. Then we fuse the
keyword with heading and distance embeddings using lin-
ear transformation. Finally, we perform the attention opera-
tion with the instruction embedding as query and the fused
keyword embedding as key/value. The operation result of
the attention is sent to the model for action prediction.
4. Experiments
Following previous studies [25], we present the experiment
results on IR2R and IR2R-CE in this section.
Dataset IR2R [25] is an iterative version of R2R [1] dataset,
a common evaluation benchmark for Vision-and-Language
Navigation in discrete environments collected from Matter-
port3D [4]. Same as R2R, IR2R is split into training/val-
seen/val-unseen set. IR2R training set contains 14025
episodes from 61 scenes, and each scene has 3 tours. IR2R
val-seen and val-unseen set has 53/11 scenes, 1011/2349
episodes, and 159/33 tours respectively. IR2R-CE [25] is
the iterative version of R2R-CE [24] benchmark, whichtransforms the instruction and path in R2R to continuous
form and uses Habitat [40] for environment simulation.
Evaluation Metric Following previous studies [25], we use
t-nDTW [25] to evaluate the performance of different meth-
ods in IVLN. t-nDTW is the tour-version of normalized dy-
namic time warping (nDTW) [17], which is a common met-
ric for evaluation in VLN by measuring the normalized sim-
ilarity between the ground truth path and the agent’s navi-
gation path. Please refer to more details in [25].
Implementation Details We implement our method on
[25] and retain all the hyper-parameters of VLN agents, in-
cluding HAMT and MAP-CMA. The LLM we use for key-
word extraction is GPT-3.5 [2], and the prompts are pro-
vided in supplementary materials. For OVD detection, we
use OWL-ViT [32] with ViT-L/14 backbone [8]. For dis-
crete environments, we set neighbour distance dnas 3. For
continuous environments, we set neighbour distance dnas
7, discovery threshold dvpas 1, and detection threshold ddet
as 0.25. All experiments are conducted on two RTX-3090
GPUs with 24GB memory. We train the model on one GPU
and run the detection model on the other. Please refer to
supplementary materials for details.
Experiment Result The experiment results on IR2R and
IR2R-CE are shown in Table 1 and Table 2. In Table 1, we
compare our method to HAMT baseline and four different
variants of TourHAMT. The performance of TourHAMT
is copied from [25]. For each model, we present Total
Length (TL), Navigation Error (NE), Oracle Success (OS),
nDTW, Success Rate (SR), and Success weighted by Path
16301
Val-Seen Val-Unseen
# Model TL NE ↓ OS↑nDTW↑SR↑ SPL↑t-nDTW ↑ TL NE ↓ OS↑nDTW↑SR↑ SPL↑t-nDTW ↑
1Baseline 10.1±0.14.2±0.170±171±163±161±158±1 9.4±0.14.7±0.064±166±056±054±050±0
6Type-I 10.1±0.23.8±0.170±173±165±162±161±1 9.7±0.34.2±0.166±168±059±156±052±1
6Type-II 10.2±0.33.7±0.171±173±165±162±162±0 9.8±0.24.2±0.266±168±159±156±053±1
6Ours 9.9±0.13.7±0.170±073±165±163±162±0 9.4±0.14.1±0.166±169±060±157±055±1
6Type-III 9.9±0.23.6±0.170±074±165±063±163±0 9.4±0.24.0±0.068±169±161±158±156±1
Table 3. The ablation of open-vocabulary keywords. Type-I, Type-II and Type-III adopt different keyword strategies compared to our
method. Type-I limits the keywords to 12 categories and obtains minimal improvement among these models. Type-II retains the attributes
of the 12 keywords and slightly improves the performance of Type-I. Ours retains all open-vocabulary keywords and further gains 2%.
Type-III’s memory contains the detection results in all viewpoints for the keywords of the current instruction in advance, thus achieving
the best performance.
Val-Seen Val-Unseen
# Model nDTW↑t-nDTW ↑ nDTW↑t-nDTW ↑
1Ours ( dn=1) 73.0 62.1 67.5 53.4
2Ours ( dn=2) 73.3 62.6 68.1 54.0
3Ours ( dn=3) 73.0 62.3 68.7 54.9
4Ours ( dn=4) 72.1 61.2 68.1 54.4
5Ours ( dn=5) 71.9 61.2 68.0 54.2
Table 4. The performance of our method on IR2R with different
neighbour distances dn, which determines the size of neighbour-
ing set. Our method achieves the best performance with dn= 3.
Val-Seen Val-Unseen
# Model DISTANCE VIEW INDEX nDTW↑t-nDTW ↑ nDTW↑t-nDTW ↑
1Ours 70.7 58.6 66.1 50.6
2Ours ✓ 72.2 61.3 67.2 51.3
3Ours ✓ 72.5 62.0 68.2 53.4
4Ours ✓ ✓ 73.0 62.3 68.7 54.9
Table 5. The ablation of omnigraph information. We remove
the distance dvand viewindex hvfor all keywords. Both dis-
tance dvand viewindex hvcontribute to the performance improve-
ment, which indicates the importance of positional information
and structured memory.
Length (SPL) besides t-nDTW. These six metrics are calcu-
lated on the episode level. t-nDTW is the tour-based metric
for IVLN evaluation. As shown in Table 1, TourHAMT fails
to improve the performance using tour navigation history,
while our method gains 4% and 5% improvement in val-
seen and val-unseen set respectively. Moreover, our method
achieves better performance in episode-level metrics, in-
cluding SPL, nDTW, SR, OS and NE. Note that our method
is based on HAMT, the superior performance demonstrate
the benefit of utilizing the history of previous episodes.
Table 2 shows the performance in continuous environ-
ments. We compare our method to CMA [24] and its three
variants, TourCMA, PoolCMA and PoolEndCMA [25].
We also compare our method to MAP-CMA, a strong
baseline for IVLN-CE. Our method gains 4% and 3% t-
nDTW improvement in val-seen and val-unseen set respec-
tively, while achieving better or comparable performance inepisode-level metrics. Note that the performance of MAP-
CMA [25] is copied from the original paper. Our method is
implemented on MAP-CMA with the same experiment set-
ting, i.e., inferred semantics and iterative map construction,
on which MAP-CMA achieves its best performance.
5. Ablation Study
5.1. Ablation on Open-Vocabulary Keywords
To demonstrate the superiority of the open-vocabulary key-
words in our method, here we ablate the Open-V ocabulary
strategy in the three variants of our method in Table 3.
Type-I Following previous studies[3], we collect and fo-
cus on the 12 most common object categories in R2R en-
vironment: {chair, table, cushion, cabinet, shelving, sink,
dresser, plant, bed, sofa, counter, fireplace }(sorted in de-
scending order by number of object instances). Type-I
discards the Keyword Extraction stage and performs the
Keyword-Panoramic Detection with the above 12 categories
as keywords. In this case, the OVD detector in our method
detects the objects in the observations in a close-vocabulary
way, i.e., all detection boxes sent to the agent’s memory are
limited to the 12 categories above.
Type-II Compared to the original method, Type-II only
sends the detection boxes with labels that contain at least
one of the 12 most common objects above. For exam-
ple, “marble kitchen counter” contains “counter” while
“kitchen island” does not contain any class names above.
Thus only “marble kitchen counter” will be sent to the de-
tector and then to the agent’s memory. In contrast, Type-I
only has “counter” in its keywords.
Type-III Type-III shows the performance of models with
oracle keyword detections. After training the agent, we
evaluate the agent twice on the validation seen/unseen
dataset and do not clear the agent’s memory after the first
evaluation. Thus the agent’s memory contains the detec-
tion result of the first evaluation which shares the same key-
words to the second evaluation. Type-III simulates the sce-
narios where all the keywords in the evaluation have been
detected and saved to the memory, while the parameters of
16302
Episode 5
bedroom door
doorway
bedroomroom
kitchen
black furniturebedroom door
room
bedroom
bathroomkitchen 
doorway
room
bathroom
Indoor dining table
kitchenmirror
hallway bedroom door(a) Omnigraph visualization after 5 episodes in a 100-episode tour.
Episode 5
couch
painting
doorroom
doorway
bedroom
closed closet
bed
room
sink
kitchen area
black furnituresink
indoor dinning table
kitchen
dining table
dining room
patio doorrug
sink
grey couchbathroom
doorbedroom
couch
black furnituregray rug
bedroom
large framed artwork
table
doorway
entrance door
large framed artwork
entrance tablesink
dinning room area
pantrymirror
hallway
clock
hallway
shelfkitchen
pantry
fridgewashroom
open door
sink
dining table
kitchen
sinkcloset
door
hallway (b) Omnigraph visualization after 50 episodes in a 100-episode tour.
Figure 3. The visualization of Omnigraph in our method during a 100-episode tour. As the tour proceeds, the omnigraph becomes larger
with more viewpoints and more connections. The keywords attached to viewpoints become more precise and diverse. We show 3 keywords
for each viewpoint at most and omit the extra information (e.g., heading) for simplicity.
the agent are not updated.
Our method outperforms Type-I and Type-II models on
both Val-Seen and Val-Unseen datasets. The difference be-
tween the four models indicates the performance contri-
bution of open-vocabulary detection. Note that Type-III
achieves the best performance because its memory contains
the exact keywords of every episode in advance.
5.2. Ablation on Structured Memory
To demonstrate the effectiveness of our structured memory,
we ablate the two important attributes for each keyword in
omnigraph, distance dvand view index hv. The experi-
ments are conducted in the discrete environment, i.e., IR2R.
Specifically, OVER-NA V fuses distance dvand view in-
dexhvwith each keyword. dvindicates the distance be-
tween the agent’s current position and the viewpoint where
the keyword is detected, while hvrepresents the direction
the agent should move towards to go to the viewpoint where
the keyword is detected. Table 5 shows the ablation exper-
iment results. It can be observed that both attributes, dis-
tance dvand view index hvcontribute to the performance
improvement. The ablation of each attribute will decrease
the performance, and the view index hvis more important
than distance dv, because view index hvindicates which
direction the agent should move towards more clearly.
5.3. Sensitivity for Hyper-Parameter Depth dn
Hyper-parameter depth dnis the distance threshold for
neighbour identification as described in Section 3. Neigh-
bour identification only collects the viewpoints whose dis-
tances to the agent’s current position are less than threshold
dn, and sends the collected viewpoints to the following pro-
cedures. Here we analyze the sensitivity for dn. The perfor-
mance of our method with different dnon IR2R is shown in
Table 4. The best performance is achieved when dn= 3.5.4. Visualization
We visualize the omnigraph in Fig. 3 on IR2R. The tour
shown in Fig. 3 contains 100 episodes in the same scene,
and Fig. 3a and Fig. 3b shows the omnigraph after 5
episodes and 50 episodes respectively. The dots/lines indi-
cate the viewpoint and connectivity between them respec-
tively. The red dots/lines are the viewpoints/connections
visited by the agent in previous episodes, while the white
dots/lines are unvisited. We show 3 keywords for some
viewpoints at most and omit the extra information (e.g.,
heading) for simplicity. As shown in Fig. 3a, the agent nav-
igates a large portion of the scene (66% of all viewpoints)
with a small number of episodes (5% of the instructions).
The keywords from the 5 instructions are limited, but we
can still observe that some open-vocabulary keywords are
correctly detected, e.g., black furniture and indoor dining
table. In Fig. 3b, most of the viewpoints are recorded in
the omnigraph as well as the connectivity. The keywords
attached to the viewpoints are more diverse and precise,
which enhances the representability of the omnigraph and
provides more accurate information for the agent.
6. Conclusion
We propose an open-vocabulary-based method, OVER-
NA V for Iterative Vision-Language Navigation. OVER-
NA V incorporates LLMs and an Open-V ocabulary
detector to construct an omnigraph, which consists
of viewpoints and connections with keywords to de-
scribe the distribution of key objects that are important
for the agent’s navigation. Extensive experiments in
both discrete and continuous environments demonstrate
that omnigraph is a superior and more general struc-
tured memory to memorize and describe the navigation
scene, enabling the agent to utilize the navigation his-
tory of previous episodes in IVLN for better performance.
16303
References
[1] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark
Johnson, Niko S ¨underhauf, Ian Reid, Stephen Gould, and
Anton Van Den Hengel. Vision-and-language navigation: In-
terpreting visually-grounded navigation instructions in real
environments. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 3674–
3683, 2018. 1, 2, 6
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in neural in-
formation processing systems , 33:1877–1901, 2020. 3, 6
[3] Vincent Cartillier, Zhile Ren, Neha Jain, Stefan Lee, Irfan
Essa, and Dhruv Batra. Semantic mapnet: Building allo-
centric semantic maps and representations from egocentric
views. In Proceedings of the AAAI Conference on Artificial
Intelligence , pages 964–972, 2021. 2, 3, 7
[4] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-
ber, Matthias Niebner, Manolis Savva, Shuran Song, Andy
Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d
data in indoor environments. In 2017 International Confer-
ence on 3D Vision (3DV) , pages 667–676. IEEE Computer
Society, 2017. 2, 6
[5] Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta,
Abhinav Gupta, and Ruslan Salakhutdinov. Learning to ex-
plore using active neural slam. In International Conference
on Learning Representations , 2020. 3
[6] Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, and
Ivan Laptev. History aware multimodal transformer for
vision-and-language navigation. Advances in neural infor-
mation processing systems , 34:5834–5847, 2021. 3, 5
[7] Tao Chen, Saurabh Gupta, and Abhinav Gupta. Learning
exploration policies for navigation. In International Confer-
ence on Learning Representations , 2019. 3
[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In International Con-
ference on Learning Representations , 2021. 3, 6
[9] Kuan Fang, Alexander Toshev, Li Fei-Fei, and Silvio
Savarese. Scene memory transformer for embodied agents
in long-horizon tasks. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
538–547, 2019. 2
[10] Daniel Fried, Ronghang Hu, V olkan Cirik, Anna Rohrbach,
Jacob Andreas, Louis-Philippe Morency, Taylor Berg-
Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell.
Speaker-follower models for vision-and-language naviga-
tion. Advances in Neural Information Processing Systems ,
31, 2018. 2
[11] Chen Gao, Xingyu Peng, Mi Yan, He Wang, Lirong Yang,
Haibing Ren, Hongsheng Li, and Si Liu. Adaptive zone-
aware hierarchical planner for vision-language navigation.InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 14911–14920, 2023.
2
[12] Georgios Georgakis, Bernadette Bucher, Karl Schmeck-
peper, Siddharth Singh, and Kostas Daniilidis. Learning to
map for active semantic goal navigation. In International
Conference on Learning Representations , 2022. 3
[13] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
Open-vocabulary object detection via vision and language
knowledge distillation. In International Conference on
Learning Representations , 2022. 3
[14] Joao F Henriques and Andrea Vedaldi. Mapnet: An allocen-
tric spatial memory for mapping environments. In proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 8476–8484, 2018. 3
[15] Jingyang Huo, Qiang Sun, Boyan Jiang, Haitao Lin, and
Yanwei Fu. Geovln: Learning geometry-enhanced visual
representation with slot attention for vision-and-language
navigation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 23212–
23221, 2023. 2
[16] Minyoung Hwang, Jaeyeon Jeong, Minsoo Kim, Yoonseon
Oh, and Songhwai Oh. Meta-explore: Exploratory hier-
archical vision-and-language navigation using scene object
spectrum grounding. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
6683–6693, 2023. 2
[17] Gabriel Ilharco, Vihan Jain, Alexander Ku, Eugene Ie, and
Jason Baldridge. General evaluation for instruction condi-
tioned navigation using dynamic time warping. NeurIPS Vi-
sually Grounded Interaction and Language (ViGIL) Work-
shop , 2019. 6
[18] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representa-
tion learning with noisy text supervision. In International
conference on machine learning , pages 4904–4916. PMLR,
2021. 3
[19] Jindong Jiang, Lunan Zheng, Fei Luo, and Zhijun Zhang.
Rednet: Residual encoder-decoder network for indoor rgb-
d semantic segmentation. arXiv preprint arXiv:1806.01054 ,
2018. 3, 5
[20] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel
Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-
modulated detection for end-to-end multi-modal understand-
ing. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision , pages 1780–1790, 2021. 3
[21] Aishwarya Kamath, Peter Anderson, Su Wang, Jing Yu
Koh, Alexander Ku, Austin Waters, Yinfei Yang, Jason
Baldridge, and Zarana Parekh. A new path: Scaling vision-
and-language navigation with synthetic instructions and imi-
tation learning. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 10813–
10823, 2023. 2
[22] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina
Toutanova. Bert: Pre-training of deep bidirectional trans-
formers for language understanding. In Proceedings of
NAACL-HLT , pages 4171–4186, 2019. 3
16304
[23] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt,
Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani,
Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d
environment for visual ai. arXiv preprint arXiv:1712.05474 ,
2017. 2
[24] Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra,
and Stefan Lee. Beyond the nav-graph: Vision-and-language
navigation in continuous environments. In Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, Au-
gust 23–28, 2020, Proceedings, Part XXVIII 16 , pages 104–
120. Springer, 2020. 2, 5, 6, 7
[25] Jacob Krantz, Shurjo Banerjee, Wang Zhu, Jason Corso,
Peter Anderson, Stefan Lee, and Jesse Thomason. Iter-
ative vision-and-language navigation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 14921–14930, 2023. 1, 2, 3, 5, 6, 7
[26] Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and
Jason Baldridge. Room-across-room: Multilingual vision-
and-language navigation with dense spatiotemporal ground-
ing. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) , pages
4392–4412, 2020. 2
[27] Obin Kwon, Jeongho Park, and Songhwai Oh. Renderable
neural radiance map for visual navigation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 9099–9108, 2023. 3
[28] Jialu Li and Mohit Bansal. Improving vision-and-language
navigation by generating future-view image semantics. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 10803–10812, 2023. 2
[29] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-
wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu
Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded
language-image pre-training. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10965–10975, 2022. 3
[30] Xiangyang Li, Zihan Wang, Jiahao Yang, Yaowei Wang,
and Shuqiang Jiang. Kerm: Knowledge enhanced reason-
ing for vision-and-language navigation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 2583–2592, 2023. 2
[31] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo,
Michelle Lu, Kier Storey, Miles Macklin, David Hoeller,
Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac
gym: High performance gpu based physics simulation for
robot learning. In Thirty-fifth Conference on Neural Infor-
mation Processing Systems Datasets and Benchmarks Track ,
2021. 2
[32] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim
Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh
Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran
Shen, et al. Simple open-vocabulary object detection with
vision transformers. In European Conference on Computer
Vision , pages 728–755. Springer, 2022. 3, 6
[33] Piotr Mirowski, Matt Grimes, Mateusz Malinowski,
Karl Moritz Hermann, Keith Anderson, Denis Teplyashin,
Karen Simonyan, Andrew Zisserman, Raia Hadsell, et al.Learning to navigate in cities without a map. Advances in
neural information processing systems , 31, 2018. 2
[34] OpenAI. Gpt-4 technical report, 2023. 3
[35] Benjamin Planche, Xuejian Rong, Ziyan Wu, Srikrishna
Karanam, Harald Kosch, YingLi Tian, Jan Ernst, and An-
dreas Hutter. Incremental scene synthesis. Advances in Neu-
ral Information Processing Systems , 32, 2019. 3
[36] Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang,
William Yang Wang, Chunhua Shen, and Anton van den
Hengel. Reverie: Remote embodied visual referring ex-
pression in real indoor environments. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 9982–9991, 2020. 2
[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 3
[38] Santhosh K Ramakrishnan, Ziad Al-Halah, and Kristen
Grauman. Occupancy anticipation for efficient exploration
and navigation. In Computer Vision–ECCV 2020: 16th Eu-
ropean Conference, Glasgow, UK, August 23–28, 2020, Pro-
ceedings, Part V 16 , pages 400–418. Springer, 2020. 3
[39] Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wi-
jmans, Oleksandr Maksymets, Alexander Clegg, John M
Turner, Eric Undersander, Wojciech Galuba, Andrew West-
bury, Angel X Chang, et al. Habitat-matterport 3d dataset
(hm3d): 1000 large-scale 3d environments for embodied ai.
InThirty-fifth Conference on Neural Information Processing
Systems Datasets and Benchmarks Track , 2021. 2
[40] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,
Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia
Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A
platform for embodied ai research. In Proceedings of
the IEEE/CVF international conference on computer vision ,
pages 9339–9347, 2019. 2, 6
[41] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao,
Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and
Lei Zhang. Reinforced cross-modal matching and self-
supervised imitation learning for vision-language navigation.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 6629–6638, 2019. 2
[42] Saim Wani, Shivansh Patel, Unnat Jain, Angel Chang, and
Manolis Savva. Multion: Benchmarking semantic map
memory using multi-object navigation. Advances in Neural
Information Processing Systems , 33:9700–9712, 2020. 2, 3
[43] Luca Weihs, Matt Deitke, Aniruddha Kembhavi, and
Roozbeh Mottaghi. Visual room rearrangement. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 5922–5931, 2021. 2
[44] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra
Malik, and Silvio Savarese. Gibson env: Real-world percep-
tion for embodied agents. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
9068–9079, 2018. 2
[45] Licheng Yu, Xinlei Chen, Georgia Gkioxari, Mohit Bansal,
Tamara L Berg, and Dhruv Batra. Multi-target embodied
16305
question answering. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
6309–6318, 2019. 2
[46] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-
Fu Chang. Open-vocabulary object detection using captions.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 14393–14402, 2021.
3
[47] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,
Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer.
Lit: Zero-shot transfer with locked-image text tuning. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 18123–18133, 2022. 3
[48] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chun-
yuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou,
Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-
based language-image pretraining. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 16793–16803, 2022. 3
[49] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp
Kr¨ahenb ¨uhl, and Ishan Misra. Detecting twenty-thousand
classes using image-level supervision. In European Confer-
ence on Computer Vision , pages 350–368. Springer, 2022.
3
16306
