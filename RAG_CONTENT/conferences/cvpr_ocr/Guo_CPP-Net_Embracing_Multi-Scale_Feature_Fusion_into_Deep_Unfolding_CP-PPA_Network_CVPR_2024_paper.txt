CPP-Net: Embracing Multi-Scale Feature Fusion into Deep Unfolding CP-PPA
Network for Compressive Sensing
Zhen Guo
Northwestern Polytechnical University
Xi‚Äôan 710072, China
guozhen2022@mail.nwpu.edu.cnHongping Gan*
Northwestern Polytechnical University
Xi‚Äôan 710072, China
ganhongping@nwpu.edu.cn
Abstract
In the domain of compressive sensing (CS), deep unfold-
ing networks (DUNs) have garnered attention for their good
performance and certain degree of interpretability rooted
in CS domain, achieved by marrying traditional optimiza-
tion solvers with deep networks. However, current DUNs
are ill-suited for the intricate task of capturing fine-grained
image details, leading to perceptible distortions and blurri-
ness in reconstructed images, particularly at low CS ratios,
e.g., 0.10 and below. In this paper, we propose CPP-Net, a
novel deep unfolding CS framework, inspired by the primal-
dual hybrid strategy of the Chambolle and Pock Proximal
Point Algorithm (CP-PPA). First, we derive three iteration
submodules, X(k),V(k)andY(k), by incorporating cus-
tomized deep learning modules to solve the sparse basis
related proximal operator within CP-PPA. Second, we de-
sign the Dual Path Fusion Block (DPFB) to adeptly extract
and fuse multi-scale feature information, enhancing sensi-
tivity to feature information at different scales and improv-
ing detail reconstruction. Third, we introduce the Itera-
tion Fusion Strategy (IFS) to effectively weight the fusion of
outputs from diverse reconstruction stages, maximizing the
utilization of feature information and mitigating the infor-
mation loss during reconstruction stages. Extensive experi-
ments demonstrate that CPP-Net effectively reduces distor-
tion and blurriness while preserving richer image details,
outperforming current state-of-the-art methods. Codes are
available at https://github.com/ICSResearch/
CPP-Net .
1. Introduction
Compressive Sensing (CS) stands as a pivotal signal pro-
cessing technique, facilitating the recovery of sparse or
compressible signals from significantly fewer measure-
ments than required by the Shannon-Nyquist sampling the-
*Corresponding author.
27.01/0.9570/0.0334 18.26/0.4156/0.1830 17.28/0.2631/0.2482PSNR/SSIM/LPIPS
 26.13 /0.8145 /0.2063 25.13 /0.7779 /0.2384 27.34/0.8396/0.1027
DGUNet+ 
(CVPR 2022)OCTUF 
(CVPR 2023)CPP -Net 
(Our Method)Original
Image
PSNR/SSIM/LPIPS
Figure 1. The PSNR (dB), SSIM and LPIPS [45] performance
comparison of DGUNet+[24], OCTUF [31] and our CPP-Net
at a CS ratio of 0.10. Our proposed CPP-Net outperforms other
methods with higher image quality and improved human percep-
tion quality.
orem [5]. This capability accelerates signal acquisition,
leading to a substantial reduction in sampling costs and data
storage requirements. Furthermore, its advent has spurred
advancements in various research domains, encompassing
signal processing [4] in diverse areas such as applied math-
ematics [39], computational imaging [9, 10, 23, 34], com-
puter science [49], etc. These explorations have profoundly
propelled the development of CS theory, giving rising to
various practical applications, including but not limited to
natural image CS [20, 40, 41] and CS magnetic resonance
imaging (CS-MRI) [22, 33, 36].
The CS sampling process can be expressed as b=Œ¶x,
where x‚ààRNdenotes the original signal, Œ¶‚ààRM√óN
(M‚â™N) signifies the sampling matrix, b‚ààRMrepre-
sents the measurements derived from x, and the CS ratio œÑ
is defined asM
N. AsM‚â™N, the primary objective of CS
is to recover xfrom this underdetermined system. Conven-
tional CS methods are designed to address the Basis Pursuit
De-Noising (BPDN) problem for signal recovery, mathe-
matically represented as follows:
min
xŒª‚à•Œ®x‚à•‚Ñì1+1
2‚à•Œ¶x‚àíb‚à•2
‚Ñì2, (1)
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
25086
CS Sampling
bMeasurements
x
P(0)Original Image Reconstructed Image
A(0)
A(k)P(1)
A(1)
Stage 1P(k-1)
A(k-1)
Stage kP(k)
A(K-1)P(K-1)P(K)
A(K)
 
Reconstruction Stages
x^ ^ x^ P(k)={x(k), y(k)}
A(k)={x(k), x(k), x(k), x(k)}0 1 2 3 A(k)={x(k), x(k), x(k), x(k)}0 1 2 3P(k)={x(k), y(k)}
A(k)={x(k), x(k), x(k), x(k)}0 1 2 3
Stage  K
x--x-ùìïvec(¬∑ )
 
InitializationFigure 2. Overall architecture of our proposed CPP-Net, which consists of Kreconstruction stages. xis the original image, the function
Fvec(¬∑)divides the images into non-overlapping blocks and then flattens them into vectors, xrepresents the obtained vectorized image
blocks, bis the measurements, P(k)andA(k)represent the set of iteration auxiliary variables, and ÀÜxis the output reconstructed image.
where Œªis the regularization parameter, Œ®‚ààRN√óNde-
notes the orthogonal sparse basis, ‚à• ¬∑ ‚à• ‚Ñì1denotes the ‚Ñì1-
norm, and ‚à• ¬∑ ‚à•‚Ñì2denotes the Euclidean norm.
Traditional optimization algorithms [2, 3, 12, 15] aim
to discover solutions that not only align with the measure-
ments but also encourage sparsity in the sparse domain.
These approaches excels at handling problems with sparse
constraints and offers robust theoretical foundations. How-
ever, they often demand high computational complexity and
require manual parameter selection and tuning for various
datasets. In recent years, deep learning (DL) has been in-
troduced into the field of image CS, giving rise to pure
DL-based CS methods [13, 14, 21, 25, 28, 32, 37, 48],
which employ Convolutional Neural Networks (CNNs) or
Transformers to learn the mapping relationship between
measurements and original images. However, these pure
DL-based methods remain a black box with limited trans-
parency and interpretability, particularly within the CS do-
main, constraining their broader utility and further devel-
opment. To bridge the gap between DL and traditional
optimization algorithms, deep unfolding networks (DUNs)
[29, 31, 36, 41, 43, 46] incorporate DL architectures into
the iterative steps of optimization algorithms. This combi-
nation not only brings the benefits of rapid reconstruction
and adaptive learning of parameters, but also introduces a
degree of interpretability rooted in the CS domain.
However, existing DUNs struggle to capture fine-grained
image details, resulting in noticeable degradation, including
distortions and blurriness, especially at low CS ratios, e.g.,
0.10 or lower. This is primarily due to two key challenges:
‚Ä¢Inadequate extraction and fusion of multi-scale fea-
ture information : Current DUNs often focus on op-
timizing images at high CS ratios, and are inadequate
in extracting and fusing feature information at various
scales from images, resulting in a lack of sensitivity to
fine-grained image features. This deficiency compro-
mises their ability to process fine-grained details accu-
rately, such as subtle textures, edges, and intricate details
when measurements are limited.
‚Ä¢Information fusion deficiency of different reconstruc-
tion stages : DUNs typically consist of multiple iterative
reconstruction stages responsible for extracting and pro-
cessing specific image information. However, existingmethods often struggle to balance and fuse this informa-
tion effectively, resulting in information loss and confu-
sion. This flawed information fusion strategy can impact
DUNs‚Äô performance, as different stages of information
may have varying importance and contributions to the fi-
nal reconstruction.
Furthermore, image details are pivotal for image content
recognition and analysis. Particularly at low CS ratios, en-
hancing image details becomes especially vital to fully ex-
ploit the advantages of CS, such as resource-efficient trans-
mission and storage. Consequently, the reconstruction of
image details at low CS ratios is a vital and challenging task
to ensure high-quality image perception.
To address these challenges, we propose CPP-Net, a
novel deep unfolding framework inspired by the Cham-
bolle and Pock Proximal Point Algorithm (CP-PPA), as il-
lustrated in Fig. 2. By integrating a customized DL mod-
ule into the iterative optimization solvers of CP-PPA to ad-
dress the problem of sparse basis related to proximal op-
erator, we derive and establish the deep unfolding CP-PPA
framework, comprising three iteration submodules: X(k),
V(k), andY(k). Subsequently, building upon the proposed
framework, we design the Dual Path Fusion Block (DPFB),
which employs up and down sampling as well as Multi-
Scale Transformer Block (MSTB) to proficiently extract
and fuse multi-scale feature information. Additionally, we
introduce the Iteration Fusion Strategy (IFS) by concate-
nating outputs from different reconstruction stages in the
channel dimension and applying attentional weighting for
adaptive fusion. The main contributions are summarized as
follows:
‚Ä¢ We propose CPP-Net, a novel deep unfolding framework
for CS, drawing inspiration from the primal-dual hybrid
strategy of CP-PPA, achieving superior reconstructed im-
age and human perception quality.
‚Ä¢ We design the Dual Path Fusion Block (DPFB) as an ex-
tension of the sparse basis structure associated with the
proximal operator within CPP-Net to adeptly extract and
fuse multi-scale feature information, enhancing sensitiv-
ity at different scales and improving detail reconstruction.
‚Ä¢ We introduce the Iteration Fusion Strategy (IFS) to ef-
fectively weight the fusion of information from different
reconstruction stages in CPP-Net, maximizing feature uti-
25087
b
y(0)
v(0)x(0)
x(0)
3x(0)
3x(0)
2x(0)
2x(0)
1x(0)
1x(0)Initialization
0
A(k)={x(k), x(k), x(k), x(k)}0 1 2 3 A(k)={x(k), x(k), x(k), x(k)}0 1 2 3Measurements
: Point -wise
Addition
rC√óH/r√óW/r rC√óH/r√óW/rC√óH√ó1 C√óH√ó1
C√ó1√óW C√ó1√óWC√óH√óW C√óH√óWC√óH√ó1 C√óH√ó1
C√ó1√óWC√ó1√óWC√óH√óW C√óH√óW
C√óH√óW C√óH√óW
C√óH√óW C√óH√óWC√óH√óW C√óH√óWC√óH√óW C√óH√óW
rC√óH/r√ó1 rC√óH/r√ó1
rC√ó1√óW/r rC√ó1√óW/rC/8√óH√óW C/8√óH√óW C√óH√óW C√óH√óW
(b) MSTBDCAB
Conv2 √ó2
LNDCAB
Conv2 √ó2
LN
DCAB
Conv2 √ó2
LN
GAP HGAP W GAP W
 GAP W
GAP H GAP H
 GAP HGAP W GAP W
 GAP W
DConv3x3DConv3x3
DConv3x3
GELUGELU
GELU
Conv1x1Conv1x1
Conv1x1
Conv1x1Conv1x1
Conv1x1
GELUGELU
GELU
Conv1x1Conv1x1
Conv1x1Conv1x1Conv1x1
Conv1x1
GELUGELU
GELU
Conv1x1Conv1x1
Conv1x1
SigmoidSigmoid
Sigmoid SigmoidSigmoid
Sigmoid
Reconstruction Stages
A(0)A(1)
A(k-1)A(K-1)
A(k)
x(0)
0x(0)
0x(1)
0x(1)
0 x(k)
0x(k)
0 x(K)
0x(K)
0
Reconstructed  
Image
x^ ^ x^ CC
CCCC
CCCCx(0)
x(1)x(k)x(K)
DCAB
Conv1 √ó1
DCAB
Conv1 √ó1DCAB
Conv1 √ó1
DCAB
Conv1 √ó1
DCAB
Conv1 √ó1
DCAB
Conv1 √ó1
Œ¶ùìïvec(v(0)) - b
DCABConv1 √ó1
DCABConv1 √ó1
DCABConv1 √ó1
Conv2 √ó2
LNConv2 √ó2
LN
Conv2 √ó2
LN
DCAB
Conv2 √ó2
LNDCAB
Conv2 √ó2
LN
DCAB
Conv2 √ó2
LN
Stage 1
 Stage k
CC
Stage K
CC
C√óH√óW C√óH√óWrC√óH/r√óW/rC√óH√óW C√óH√óW
MSCAMSCA DCABDCAB LNLNLNLN
LNLNMSCA DCAB LNLN
LN
MSCA DCAB LNLN
LNMSCAMSCA DCABDCAB LNLNLNLN
LNLN
MSCA DCAB LNLN
LN
 or
x(0)
C√óH√óW C√óH√óW
2C√óH/2√óW/2 2C√óH/2√óW/2
4C√óH/4√óW/4 4C√óH/4√óW/4
8C√óH/8√óW/8 8C√óH/8√óW/81√óH√óW 1√óH√óW
ùìïpre
ùìïdown 1
ùìïdown 2
ùìïdown 3
MSCA
Q2Q2
K1K1VV
Q1Q1K2K2C√óH√óW C√óH√óWùìïpost
RR
RRC√óH√óW C√óH√óW
x(k-1)
3x(k-1)
3x(k-1)
2x(k-1)
2x(k-1)
1x(k-1)
1x(k-1)x(k-1)
x(k)
x(k)
3x(k)
3x(k)
2x(k)
2x(k)
1x(k)
1(a) DPFB
C√óH√óW C√óH√óW
2C√óH/2√óW/2 2C√óH/2√óW/2
4C√óH/4√óW/4 4C√óH/4√óW/4
8C√óH/8√óW/8 8C√óH/8√óW/84C√óH/4√óW/4 4C√óH/4√óW/42C√óH/2√óW/2 2C√óH/2√óW/2C√óH√óW C√óH√óW
C√óH√óW C√óH√óW1√óH√óW 1√óH√óWx(k-1)
0x(k-1)
0
x(k)
0x(k)
0
C√ó1√ó1 C√ó1√ó1
LN
ConvT2 √ó2DCAB
LN
ConvT2 √ó2DCAB
LN
ConvT2 √ó2DCAB
LN
ConvT2 √ó2DCAB
LN
ConvT2 √ó2DCAB
LN
ConvT2 √ó2DCABDCAB
Conv2 √ó2
LNDCAB
Conv2 √ó2
LN
DCAB
Conv2 √ó2
LN
DCAB
Conv2 √ó2
LNDCAB
Conv2 √ó2
LN
DCAB
Conv2 √ó2
LN
DCAB
MSTB
MSTBMSTB
MSTB
MSTBMSTB
MSTB
Conv1x1Conv1x1
Conv1x1
GELUGELU
GELU
Conv1x1Conv1x1
Conv1x1
SigmoidSigmoid
Sigmoid
Œ∑(k)Œª(k)GAPDCAB
Conv1 √ó1
DCAB
DCAB
Conv1 √ó1DCAB
Conv1 √ó1
DCAB
DCAB
Conv1 √ó1
DCAB
Conv1 √ó1
DCAB
DCAB
Conv1 √ó1
STResSTRes
STRes
ConvT2 √ó2
LN
DCABConvT2 √ó2
LN
DCAB
ConvT2 √ó2
LN
DCABDCAB
Conv2 √ó2
LNDCAB
Conv2 √ó2
LN
DCAB
Conv2 √ó2
LNSS
Conv1 √ó1
DConv3 √ó3Conv1 √ó1
DConv3 √ó3
Conv1 √ó1
DConv3 √ó3Conv1 √ó1
DConv3 √ó3Conv1 √ó1
DConv3 √ó3
Conv1 √ó1
DConv3 √ó3
√óŒº(k)
yÃÉ(k)
4C√óH√óW 4C√óH√óWC√óH√óW C√óH√óWC√óH√óW C√óH√óW
C√ó1√ó1 C√ó1√ó1C/8√ó1√ó1 C/8√ó1√ó1
C√ó1√ó1 C√ó1√ó1
C√óH√óW C√óH√óWDConv3x3DConv3x3
LNLN
Conv1x1Conv1x1
GELUGELU
Conv1x1Conv1x1DConv3x3
LN
Conv1x1
GELU
Conv1x1
DConv3x3
LN
Conv1x1
GELU
Conv1x1
GAPGAP
Conv1x1Conv1x1
GELUGELU
Conv1x1Conv1x1
SigmoidSigmoidGAP
Conv1x1
GELU
Conv1x1
Sigmoid
GAP
Conv1x1
GELU
Conv1x1
Sigmoid(c) DCAB
**
Reconstructed  
MRI Image
A(k)x(k-1)x(k)
A(k-1)
y(k-1)y(k)
v(k)
 2x(k) - x(k-1)2x(k) - x(k-1)
2x(k) - x(k-1)
Œ¥(k)(Œ¶ùìïvec(¬∑ ) - b)
DPFBDPFB
DPFB
(¬∑ )/(1+Œ¥(k))
P(k)={x(k), y(k)}P(0)P(1)P(k-1)P(k)P(K-1)
CC
ùìïÃÉvec(Œ¶  b)‚ä§
ùìïÃÉvec(Œ¶  b)‚ä§
ùìïÃÉvec(Œ¶ (¬∑ ))ùìïÃÉvec(Œ¶ (¬∑ ))
ùìïÃÉvec(Œ¶ (¬∑ ))‚ä§ùìïÃÉvec(Œ¶ (¬∑ ))
ùìïÃÉvec(Œ¶ (¬∑ ))‚ä§
¬∑¬∑: Point -wise
Subtraction
: Point -wise
Subtraction
: Multiplication
: Multiplication
: Hadamard 
Product¬∑¬∑¬∑
¬∑
: Hadamard 
Product¬∑¬∑
¬∑: Matrix 
Multiplication
: Matrix 
Multiplication
: ReshapeRRR
R
: ReshapeRR
R: Concatenation
CC
: Concatenation
C
****
: Soft 
Threshloding
SS
: Soft 
Threshloding
SFigure 3. Details of our proposed CPP-Net. We present the details of initialization and reconstruction stages in the first row. The second
row demonstrates the details of components in CPP-Net, i.e., illustrations of (a) Dual Path Fusion Block (DPFB) and Soft Thresholding
Residual connection (STRes); (b) Multi-Scale Transformer Block (MSTB) and Multi-Scale Cross Attention (MSCA); (c) Depth-wise
Channel Attention Block (DCAB).
lization and enhancing image detail reconstruction while
reducing information loss during reconstruction stages.
Extensive experiments demonstrate that our proposed CPP-
Net outperforms current state-of-the-art (SOTA) methods
by preserving richer image details, reducing distortion and
blurring, leading to higher image quality and improved hu-
man perception quality.
2. Related Works
2.1. Traditional Chambolle and Pock Proximal
Point Algorithm
The Chambolle and Pock Proximal Point Algorithm (CP-
PPA) [6, 16, 17] is a straightforward yet powerful optimiza-
tion algorithm that utilizes an enhanced primal-dual hybrid
approach [50], offering convergence and efficiency. Con-
sider the following saddle-point problem:
min
x‚ààRNmax
y‚ààRMŒò(x,y) :=Œ∏1(x)‚àíy‚ä§Œ¶x‚àíŒ∏2(y),(2)
where Œ¶‚ààRM√óN. The CP-PPA solves this problem
through following iterative steps:
x(k+1)= arg min
xŒò(x,y(k)) +Œ±
2‚à•x‚àíx(k)‚à•2
‚Ñì2,(3)v(k+1)= 2x(k+1)‚àíx(k), (4)
y(k+1)= arg max
yŒò(v(k+1),y)‚àíŒ≤
2‚à•y‚àíy(k)‚à•2
‚Ñì2,(5)
where Œ±andŒ≤are regularization parameters.
CP-PPA addresses both the primal and dual prob-
lems alternately and iteratively, and achieves faster con-
vergence compared to the widely-used Iterative Shrink-
age/Thresholding Algorithm (ISTA) [2, 6], which solely op-
timizes the primal problem. Furthermore, CP-PPA provides
stronger convergence guarantees, increasing its potential to
theoretically attain globally optimal solutions [17]. How-
ever, as a traditional iterative optimization algorithm, CP-
PPA has its limitations. Its performance heavily relies on
parameter settings, particularly the choice of Œ±andŒ≤. In-
accurate parameter choices can lead to slow convergence or
system instability [17]. Additionally, for the problem out-
lined in Eq. (1), a persistent challenge remains in the se-
lection of an appropriate sparse basis Œ®due to the diverse
content and characteristics of images [1].
2.2. Deep Unfolding Networks
The core idea of DUNs is to unfold the traditional itera-
tive optimization algorithms into the form of deep networks,
25088
making it an end-to-end trainable model. DUNs offer the
combined advantages of rapid reconstruction and adaptive
learning of parameters, while introducing interpretability
rooted in the CS domain.
Since Zhang and Ghanem proposed ISTA-Net [41],
based on the famous ISTA algorithm (a special case
of Proximal Gradient Descent (PGD) algorithm for ‚Ñì1-
regularization term), these are many ISTA-based network
variants have been gradually proposed for CS, including
OPINE-Net+[42], TransCS [27], OCTUF [31], DGUNet+
[24], and others [7, 8, 24, 29, 30]. Similarly, Yang et al.
proposed ADMM-CSNet [36] based on the Alternating Di-
rection Method of Multipliers (ADMM) [3]; Zhang et al.
proposed AMP-Net [46] inspired by the Approximate Mes-
sage Passing (AMP) [12] for CS, and so on. Nevertheless,
current DUNs usually neglect the enhancement of details at
different scales in images at low CS ratios, and at the same
time are insufficient for the transfer and utilization of infor-
mation from different stages, resulting in significant degra-
dation, e.g., distortions and blurriness, in reconstructed im-
ages. For example, OCTUF [31] utilizes cross-attention
mechanisms to optimize the image but disregards the finer
image details of various scales, leading to substantial areas
of blurring. Similarly, while DGUNet+[24] utilizes down-
sampling for encoding and up-sampling for decoding to un-
fold PGD, it lacks effective fusion of feature information
across different scales. This deficiency results in notice-
able distortions in the reconstructed images. In this paper,
we propose CPP-Net, a novel deep unfolding framework
inspired by the CP-PPA with efficient multi-scale feature
fusion to achieve precise image detail reconstruction.
3. Proposed Method
The details of our CPP-Net are illustrated in Fig. 3. CPP-
Net includes one initialization module and Kreconstruction
stages, with the default setting of eight stages. Each stage
represents one iteration in the CP-PPA, and consists of three
iteration submodules ( X(k),V(k)andY(k)) and one DPFB.
3.1. Initialization Module
As shown in Fig. 3, initialization module responds to an
initial estimate P(0)from the measurements and initialize
the iteration parameters A(0).P(k)contains the iterative
auxiliary variables ( x(k)andy(k)), while A(k)contains the
downsampling results at different scales ( x(k)
0,x(k)
1,x(k)
2
andx(k)
3) in the k-th reconstruction stages. In order to fa-
cilitate convolutional operations and iterative computations,
we use Fvec(¬∑)andeFvec(¬∑)to denote the chunking and flat-
tening functions as well as their inverse processes, respec-
tively.3.2. Reconstruction Stages
Iteration Submodules . We introduce a DL-based opera-
tor, denoted as D(¬∑), to replace Œ®, which offers enhanced
adaptability to various images and a high degree of self-
adjustment without the need for manual parameter tun-
ing. We define two functions: f(x) = Œª‚à•D(x)‚à•‚Ñì1and
h(z) =1
2‚à•z‚àíb‚à•2
‚Ñì2. Therefore, we reformulate the Eq. (1)
as a saddle-point problem, expressed as:
min
x‚ààRNmax
y‚ààRMŒò(x,y) :=f(x) +y‚ä§Œ¶x‚àíh‚àó(y).(6)
Here, h‚àóis the convex conjugate of the convex lower-
semicontinuous function h:
h‚àó(y) = sup
z‚ààRMz‚ä§y‚àí1
2‚à•z‚àíb‚à•2
‚Ñì2. (7)
By alternately iterating and optimizing the primal and dual
problems, and independently learning regularization terms
Œ±,Œ≤andŒªfor each iteration, iterative steps can be derived
as follows:
x(k)= arg min
xŒò(x,y(k‚àí1))+Œ±(k)
2‚à•x‚àíx(k‚àí1)‚à•2
‚Ñì2,(8)
v(k)= 2x(k)‚àíx(k‚àí1), (9)
y(k)= arg max
yŒò(v(k),y)‚àíŒ≤(k)
2‚à•y‚àíy(k‚àí1)‚à•2
‚Ñì2.(10)
Since the constant term is independent of the solution, the
solution for Eq. (8) can be expressed as:
x(k)= arg min
xf(x) + (y(k‚àí1))‚ä§Œ¶x+1
2Œ∑(k)‚à•x‚àíx(k‚àí1)‚à•2
‚Ñì2
= arg min
xf(x) +1
2Œ∑(k)‚à•x‚àíx(k‚àí1)+Œ∑(k)Œ¶‚ä§y(k‚àí1)‚à•2
‚Ñì2,
(11)
where Œ∑(k)= 1/Œ±(k).x(k)can be obtained using the soft
thresholding function denoted as S(¬∑), a special proximal
operator for ‚Ñì1-norm:
x(k)=eD(SŒª(k)Œ∑(k)(D(x(k‚àí1)‚àíŒ∑(k)Œ¶‚ä§y(k‚àí1)))),(12)
where
Sœµ(x) = sgn( x)‚àómax(|x| ‚àíœµ,0), (13)
andeD(¬∑)represents the left inverse of D(¬∑). Similarly, solv-
ing for y(k)in Eq. (10) is equivalent to:
y(k)= arg min
yh‚àó(y)‚àíy‚ä§Œ¶v(k)+1
2Œ¥(k)‚à•y‚àíy(k‚àí1)‚à•2
‚Ñì2
= arg min
yh‚àó(y) +1
2Œ¥(k)‚à•y‚àíy(k‚àí1)‚àíŒ¥(k)Œ¶v(k)‚à•2
‚Ñì2
= proxŒ¥(k)h‚àó(y(k‚àí1)+Œ¥(k)Œ¶v(k))
=1
1 +Œ¥(k)(y(k‚àí1)+Œ¥(k)(Œ¶v(k)‚àíb)),
(14)
25089
where prox(¬∑)denotes the proximal operator and Œ¥(k)=
1/Œ≤(k).
In summary, for the sake of convenience in iterative and
convolutional operations, we can express the explicit solu-
tions for the iteration submodules X(k),V(k), andY(k)in
thek-th reconstruction stages as follows:
Ô£±
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥X(k):x(k)=eD(SŒª(k)Œ∑(k)(D(
x(k‚àí1)‚àíŒ∑(k)eFvec(Œ¶‚ä§y(k‚àí1))))),
V(k):v(k)= 2x(k)‚àíx(k‚àí1),
Y(k):y(k)=1
1 +Œ¥(k)(y(k‚àí1)+Œ¥(k)(Œ¶Fvec(v(k))‚àíb)).
(15)
The convergence analysis of the proposed framework is pre-
sented in the Supplementary Material (Sec. 2). The essence
of our CPP-Net resides in the structure eD(S(D(¬∑))). There-
fore, we design the DPFB as an extension of eD(S(D(¬∑)))to
enhance CPP-Net‚Äôs capability for characterizing informa-
tion and to achieve effective extraction and fusion of multi-
scale features. DPFB consists mainly of DCABs, MSTBs
and STRes. The specific details are presented below.
Depth-wise Channel Attention Block (DCAB) . We in-
troduce the DCAB as the fundamental building block to en-
hance the feature extraction and attentional weighting capa-
bilities of our CPP-Net. As illustrated in Fig. 3(c), DCAB
begins with a 3√ó3depth-wise convolution ( DConv 3) to
extract spatial information, followed by a layer normaliza-
tion (LN) to enhance stability. It then employs two 1√ó1
convolutions ( Conv 1) to expand and reduce the channel
count, with a Gaussian error linear units (GELU) activation
in between. Additionally, DCAB incorporates channel at-
tention block inspired by [18] to highlight informative fea-
tures while suppressing less relevant ones across channels
to achieve adaptive weighting.
Multi-Scale Transformer Block (MSTB) . We design
the MSTB, a composition of LN, Multi-Scale Cross At-
tention (MSCA) and DCAB, to improve the aggregation
and fusion of feature information across various scales. As
shown in Fig. 3(b), MSTB takes two inputs of varying
scales, applies LN to each, and then processes them through
the MSCA. Following the residual connection, the feature
undergoes further refinement via the DCAB. Assuming that
x(k)‚ààRC√óH√óWandx(k)
i‚ààRrC√óH/r√óW/rrepresent the
inputs to the MSCA, the process unfolds as follows: Ini-
tially, x(k)undergoes Global Average Pooling (GAP) along
both the width ( GAP W:RC√óH√óW‚ÜíRC√óH√ó1) and
height ( GAP H:RC√óH√óW‚ÜíRC√ó1√óW) dimensions, pro-
ducing Query Q2and Key K1, respectively. Similarly, x(k)
i
generates the corresponding Q1andK2after reshaping.
Moreover, Vis derived from x(k)by applying a depth-wise
3√ó3convolution, followed by a GELU activation, and fi-
nally a 1√ó1convolution. The process can be representedAlgorithm 1 CPP-Net
Input: Original image x, sampling matrix Œ¶, block size B,
the number of reconstruction stages K.
Output: Final reconstructed image ÀÜx.
1:Sampling: x=Fvec(x),b=Œ¶x.
2:Initialization: k= 0,x(0)=eFvec(Œ¶‚ä§b),v(0)=
x(0),y(0)=Œ¶Fvec(v(0))‚àíb,x(0)
0=Fpre(x(0)),
x(0)
1=Fdown 1(x(0)
0),x(0)
2=Fdown 2(x(0)
1),x(0)
3=
Fdown 3(x(0)
2),A(0)={x(0)
0,x(0)
1,x(0)
2,x(0)
3}.
3:Reconstruction:
4:while k < K do
5: k‚Üêk+ 1;
6:ey(k‚àí1)=eFvec(Œ¶‚ä§y(k‚àí1));
7: x(k),A(k)‚ÜêDPFB(k)(x(k‚àí1),ey(k‚àí1),A(k‚àí1));
8: v(k)= 2x(k)‚àíx(k‚àí1);
9: y(k)=1
1 +Œ¥(k)(y(k‚àí1)+Œ¥(k)(Œ¶Fvec(v(k))‚àíb));
10:end while
11:ÀÜx=Fpost(Concat( {x(k)}K
k=0,{x(k)
0}K
k=0)).
as follows:Ô£±
Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£≥Q2= GAP W(x(k)),K1= GAP H(x(k)),
Q1= R(GAP W(x(k)
i)),K2= R(GAP H(x(k)
i)),
V= Conv 1(GELU(DConv 3(x(k)))).(16)
Here, R(¬∑)denotes the reshape function. Subsequently,
the Queries and Keys generated by these two inputs un-
dergo the matrix multiplication, resulting in two matrices
inRC√óH√óW. These matrices are then subjected to channel
downscaling and upscaling through two 1√ó1convolutions,
followed by the Sigmoid function to obtain the correspond-
ing attention weights. The MSCA output, denoted as ex(k),
is computed by the Hadamard product of Vwith the ob-
tained attention weights:
ex(k)= SG( Q1‚äóK1)‚äôSG(Q2‚äóK2)‚äôV, (17)
where SG(¬∑) = Sigmoid(Conv 1(GELU(Conv 1(¬∑)))).
Dual Path Fusion Block (DPFB) . Our approach prior-
itizes the efficient extraction and fusion of multi-scale fea-
tures to enhance performance of details reconstruction. To
achieve this, we design the DPFB as the cornerstone of our
CPP-Net framework, as illustrated in Fig. 3(a). The DPFB
begins with an initial step that includes a 1√ó1convolu-
tion and a 3√ó3depth-wise convolution to increase channel
count. It then downsamples the feature maps using 2√ó2
convolutions with a stride of 2 ( Conv 2), which reduces the
feature maps‚Äô size by half while doubling the channel count,
facilitating efficient capture of multi-scale features. Further-
more, the DPFB effectively fuses multi-scale information
25090
Table 1. Average PSNR (dB)/SSIM performance comparisons of recent CS methods on various datasets at different CS ratios.
Datasets CS RatiosISTA-Net+
(CVPR 2018)AMP-Net
(TIP 2021)CASNet
(TIP 2022)DGUNet+
(CVPR 2022)FSOINet
(ICASSP 2022)TransCS
(TIP 2022)AutoBCS
(TCYB 2023)CSformer
(TIP 2023)DPC-DUN
(TIP 2023)OCTUF
(CVPR 2023)TCS-Net
(TCI 2023)CPP-Net
(Our Method)
General100 [11]0.01 19.00/0.4700 22.71/0.6282 23.48/0.6480 22.86/0.6190 23.27/0.6363 21.66/0.5415 22.24/0.6164 23.35/0.6394 19.95/0.5363 23.31/0.6346 22.58/0.5978 24.17/0.6593
0.04 23.76/0.6549 26.96/0.7695 28.50/0.8171 27.92/0.8078 28.39/0.8135 27.25/0.7843 27.10/0.7964 27.81/0.7986 26.61/0.7531 28.35/0.8122 26.57/0.7712 29.04/0.8284
0.10 28.54/0.8104 30.82/0.8722 32.78/0.9099 32.41/0.9073 32.70/0.9085 31.39/0.8918 30.76/0.8927 31.60/0.8880 31.17/0.8716 32.77/0.9084 29.90/0.8748 33.16/0.9159
0.25 34.32/0.9250 36.01/0.9508 38.07/0.9657 37.55/0.9645 38.13/0.9660 37.08/0.9604 35.92/0.9581 36.51/0.9558 36.50/0.9481 38.26/0.9666 34.63/0.9504 38.32/0.9674
Avg. 26.41/0.7151 29.13/0.8052 30.71/0.8352 30.19/0.8247 30.62/0.8311 29.35/0.7945 29.01/0.8159 29.82/0.8205 28.56/0.7773 30.67/0.8305 28.42/0.7986 31.17/0.8428
LIVE29 [26]0.01 19.15/0.4279 22.07/0.5448 22.31/0.5523 22.27/0.5503 22.37/0.5502 21.48/0.4968 21.46/0.5347 22.33/0.5516 19.81/0.4764 22.31/0.5458 22.01/0.5245 22.60/0.5567
0.04 22.02/0.5625 24.89/0.6857 25.27/0.7136 25.32/0.7168 25.34/0.7150 24.87/0.6942 24.55/0.6991 25.27/0.7048 23.70/0.6470 25.26/0.7113 24.71/0.6904 25.52/0.7211
0.10 25.05/0.7104 27.69/0.8067 28.16/0.8338 28.27/0.8379 28.23/0.8359 27.65/0.8195 27.07/0.8184 27.80/0.8148 26.83/0.7812 28.23/0.8345 27.21/0.8165 28.46/0.8414
0.25 29.42/0.8648 32.01/0.9183 32.55/0.9303 32.56/0.9323 32.66/0.9326 32.05/0.9251 31.21/0.9227 31.82/0.9196 31.17/0.9008 32.69/0.9327 31.10/0.9208 32.82/0.9343
Avg. 23.91/0.6414 26.67/0.7389 27.07/0.7575 27.11/0.7593 27.15/0.7584 26.51/0.7339 26.07/0.7437 26.81/0.7477 25.38/0.7014 27.12/0.7561 26.26/0.7381 27.35/0.7634
McM18 [44]0.01 19.99/0.4942 23.78/0.6426 24.23/0.6538 23.05/0.6267 24.10/0.6464 22.81/0.5736 23.26/0.6248 23.66/0.6526 21.10/0.5553 23.87/0.6409 23.63/0.6144 24.57/0.6614
0.04 24.27/0.6577 27.90/0.7879 28.48/0.8166 28.16/0.8091 28.50/0.8157 27.94/0.7976 27.54/0.8002 28.12/0.8030 26.51/0.7539 28.33/0.8120 27.54/0.7907 28.97/0.8277
0.10 28.54/0.8104 31.68/0.8860 32.47/0.9100 32.32/0.9070 32.47/0.9097 31.88/0.8998 31.13/0.8973 31.69/0.8907 30.67/0.8701 32.49/0.9093 30.97/0.8913 32.97/0.9159
0.25 33.99/0.9237 36.88/0.9560 37.77/0.9659 37.74/0.9655 37.85/0.9663 37.27/0.9627 36.25/0.9608 36.60/0.9570 35.86/0.9462 37.93/0.9667 35.89/0.9579 38.08/0.9676
Avg. 26.70/0.7215 30.06/0.8181 30.74/0.8366 30.32/0.8271 30.73/0.8345 29.98/0.8084 29.55/0.8208 30.02/0.8258 28.54/0.7814 30.66/0.8322 29.51/0.8136 31.15/0.8432
OST300 [35]0.01 19.36/0.4208 22.31/0.5288 22.47/0.5338 22.36/0.5306 22.49/0.5335 21.67/0.4826 21.65/0.5176 22.48/0.5299 20.12/0.4645 22.46/0.5298 22.28/0.5127 22.76/0.5400
0.04 22.06/0.5475 24.92/0.6651 25.15/0.6911 25.24/0.6973 25.25/0.6953 24.86/0.6756 24.51/0.6769 25.19/0.6843 23.61/0.6249 25.19/0.6910 24.74/0.6728 25.39/0.7001
0.10 24.78/0.6896 27.35/0.7859 27.66/0.8124 27.84/0.8187 27.75/0.8159 27.31/0.8018 26.87/0.7991 27.53/0.7950 26.25/0.7561 27.77/0.8148 27.04/0.8000 27.93/0.8207
0.25 28.53/0.8433 31.06/0.9009 31.35/0.9135 31.53/0.9170 31.55/0.9171 31.07/0.9096 30.52/0.9083 31.05/0.9038 29.93/0.8792 31.60/0.9175 30.55/0.9084 31.67/0.9185
Avg. 23.68/0.6253 26.41/0.7202 26.66/0.7377 26.74/0.7409 26.76/0.7405 26.23/0.7174 25.89/0.7255 26.56/0.7283 24.98/0.6812 26.76/0.7383 26.15/0.7235 26.94/0.7448
Set11 [21]0.01 17.45/0.4131 20.20/0.5581 21.76/0.6019 22.15/0.6114 21.73/0.5937 20.15/0.5066 19.63/0.5605 21.86/0.6071 18.03/0.4601 21.75/0.5934 21.09/0.5505 22.19/0.6135
0.04 21.56/0.6240 25.26/0.7722 26.25/0.8118 26.83/0.8230 26.37/0.8119 25.41/0.7883 24.73/0.7871 26.41/0.8058 24.38/0.7498 26.45/0.8126 25.46/0.7863 27.23/0.8337
0.10 26.49/0.8036 29.40/0.8779 30.29/0.9005 30.93/0.9088 30.44/0.9018 29.54/0.8877 28.44/0.8827 30.09/0.8925 29.42/0.8801 30.70/0.9030 29.04/0.8834 31.27/0.9135
0.25 32.44/0.9237 34.63/0.9481 35.65/0.9592 36.18/0.9616 35.80/0.9595 35.06/0.9548 33.56/0.9481 34.99/0.9534 34.75/0.9483 36.10/0.9604 33.94/0.9508 36.35/0.9631
Avg. 24.49/0.6911 27.37/0.7891 28.49/0.8184 29.02/0.8262 28.59/0.8167 27.54/0.7844 26.59/0.7946 28.34/0.8147 26.65/0.7596 28.75/0.8174 27.38/0.7928 29.26/0.8310
Set14 [38]0.01 18.22/0.4014 21.64/0.5433 22.03/0.5600 21.86/0.5409 22.00/0.5538 20.91/0.4853 20.93/0.5343 22.07/0.5493 19.04/0.4551 21.94/0.5500 21.64/0.5219 22.52/0.5694
0.04 22.08/0.5708 25.50/0.7007 26.04/0.7330 25.88/0.7250 26.08/0.7324 25.50/0.7133 25.07/0.7153 25.87/0.7160 24.32/0.6630 26.04/0.7302 25.25/0.7073 26.54/0.7440
0.10 26.00/0.7289 28.77/0.8183 29.37/0.8467 29.34/0.8455 29.35/0.8451 28.81/0.8343 28.00/0.8286 28.79/0.8214 28.03/0.7950 29.47/0.8454 28.19/0.8283 29.93/0.8537
0.25 30.62/0.8700 33.21/0.9144 33.95/0.9308 33.70/0.9294 34.05/0.9309 33.37/0.9244 32.14/0.9203 32.95/0.9174 32.78/0.9023 34.18/0.9312 32.23/0.9206 34.41/0.9336
Avg. 24.23/0.6428 27.28/0.7442 27.85/0.7676 27.70/0.7602 27.87/0.7656 27.15/0.7393 26.54/0.7496 27.42/0.7510 26.04/0.7039 27.91/0.7642 26.83/0.7445 28.35/0.7752
Urban100 [19]0.01 16.67/0.3734 19.62/0.5025 20.08/0.5366 20.15/0.5335 19.87/0.5223 18.98/0.4398 19.23/0.4991 20.14/0.5298 17.31/0.4216 19.88/0.5167 19.61/0.4945 20.55/0.5554
0.04 19.66/0.5370 22.81/0.6825 23.73/0.7412 24.05/0.7478 23.69/0.7376 23.27/0.7117 22.50/0.7029 24.03/0.7377 22.36/0.6768 23.68/0.7329 22.93/0.7036 24.66/0.7691
0.10 23.51/0.7201 26.04/0.8151 27.40/0.8606 28.01/0.8709 27.53/0.8627 26.77/0.8418 25.36/0.8242 27.30/0.8483 26.96/0.8361 27.79/0.8621 25.87/0.8291 28.49/0.8801
0.25 28.91/0.8834 30.89/0.9202 32.19/0.9396 32.77/0.9452 32.62/0.9430 31.77/0.9332 29.60/0.9187 31.83/0.9347 32.36/0.9323 32.99/0.9445 30.13/0.9241 33.38/0.9485
Avg. 22.19/0.6285 24.84/0.7301 25.85/0.7695 26.25/0.7744 25.93/0.7664 25.20/0.7316 24.17/0.7362 25.83/0.7626 24.75/0.7167 26.09/0.7641 24.64/0.7378 26.77/0.7883
through two pathways: one involves up-sampling the ac-
quired multi-scale features using a 2√ó2transposed con-
volution ( ConvT 2) with a stride of 2 and residual connec-
tions, while the other employs three MSTBs for weighted
feature fusion at different scales. The outputs from these
two pathways, along with the feature maps from the pre-
vious stage ( x(k‚àí1)
0 ), are concatenated along the channel
dimension. Subsequently, the DPFB employs DCABs and
1√ó1convolutions to achieve effective attentional weighted
fusion, resulting in two output feature maps: x(k)
0andx(k).
We also introduce the Soft Thresholding Residual connec-
tion (STRes), inspired by [47], as an adaptive method for
threshold adjustment.
Iteration Fusion Strategy (IFS) . Upon completing K
reconstruction stages, in order to maximize the utilization
of output features at each stage while reducing information
loss across stages, we perform the following steps: First, we
concatenate the outputs x(k)andx(k)
0from each stage along
the channel dimension. Then we employ the DCABs and
1√ó1convolutions to achieve attentional weighted feature
fusion across iterative reconstruction stages, contributing to
the final output image denoted as ÀÜx:
ÀÜx=Fpost(Concat( {x(k)}K
k=0,{x(k)
0}K
k=0)), (18)
where Concat denotes the concatenation along the channel
dimension. In summary, the proposed CPP-Net is described
in Algorithm 1.3.3. Loss Function
We define the training set of images as {xi}n
i=1, where nis
the number of training images. To quantify the dissimilarity
between the reconstructed image ÀÜxiand the corresponding
ground truth image xi, we employ the ‚Ñì2-loss. The sam-
pling matrix Œ¶is jointly learned through the successive re-
construction stages. Consequently, the loss function is for-
mulated as follows:
L(Œò) =1
nnX
i=1‚à•ÀÜxi‚àíxi‚à•2
‚Ñì2, (19)
where Œòrepresents the set of learnable parameters for our
proposed CPP-Net.
4. Experiments
4.1. Qualitative Evaluation
The best and second-best results in the tables are high-
lighted in red and blue colors, respectively. As detailed in
Tab. 1, our CPP-Net consistently outperforms all competing
methods across various cases, excelling in both PSNR and
SSIM. For example, on General100 at a CS ratio of 0.04,
CPP-Net surpasses AutoBCS [13], CSformer [37], DPC-
DUN [30], OCTUF [31], and TCS-Net [14] by approxi-
mately 1.94 dB (7.16%), 1.23 dB (4.42%), 2.43 dB (9.13%),
0.69 dB (2.43%), and 2.47 dB (9.30%) in terms of PSNR,
respectively. Similarly, when considering SSIM, CPP-Net
25091
DGUNet+
20.67/0.6206/0.2101Original Image
CS ratio = 0.10CSNet+
19.83/0.5848/0.2535ISTA -Net+
18.83/0.4968/0.2678
TCS-Net
20.07/0.5898/0.2435Local Image
PSNR/SSIM/LPIPSCPP -Net
21.68/0.6653/0.1638OCTUF
21.10/0.6388/0.1799AutoBCS
20.01/0.5970/0.2356TransCS
20.50/0.6144/0.2181DPC -DUN
20.26/0.5716/0.2029
CSformer
20.65/0.6216/0.2228
TCS-Net
15.38/0.5967/0.3297Local Image
PSNR/SSIM/LPIPSCPP -Net
17.48/0.7469/0.1615OCTUF
16.27/0.6638/0.2603AutoBCS
15.41/0.6480/0.2507TransCS
15.42/0.5848/0.3357DPC -DUN
13.65/0.5108/0.3514CSformer
16.22/0.6712/0.2402DGUNet+
16.78/0.6974/0.2302Original Image
CS ratio = 0.04CSNet+
14.99/0.5488/0.3683ISTA -Net+
12.44/0.3683/0.4288
OPINE -Net+
20.64/0.6082/0.2167
AMP -Net
20.45/0.6094/0.2263
CASNet
21.03/0.6341/0.2067
OPINE -Net+
15.14/0.5816/0.3322AMP -Net
15.90/0.6394/0.2832CASNet
16.21/0.6898/0.2315FSOINet
16.50/0.6905/0.2389FSOINet
21.22/0.6411/0.1940
Figure 4. Comparisons of visual and corresponding PSNR (dB)/SSIM/LPIPS performance at CS ratios of 0.10 and 0.04. The arrows point
to details in the reconstructed image for better comparison.
leads by around 0.0320 (4.02%), 0.0298 (3.73%), 0.0753
(10.00%), 0.0162 (1.99%), and 0.0572 (7.42%). Further-
more, visual comparisons of the reconstructed image details
are presented in Fig. 4. These comparisons reveal that im-
ages reconstructed by CPP-Net exhibit a notable reduction
in block artifacts, a common issue observed in ISTA-Net+
[41], CSNet+[28], DGUNet+[24], TransCS [27], CS-
former, and TCS-Net. Moreover, these competing methods,
e.g., AMP-Net [46], CASNet [7], FSOINet [8], DGUNet+,
DPC-DUN, CSformer and OCTUF, exhibit susceptibility
to blurring and the omission of fine image details, leading
to compromised visual perception and overall image qual-
ity. Conversely, CPP-Net‚Äôs reconstructed images showcase
a superior preservation of fine image details, thereby de-
livering an enhanced visual experience, higher human per-
ception quality and overall image quality. Furthermore, de-
tailed experimental settings and more comparisons can be
found in Supplementary Material (Sec. 3.1 and Sec. 3.2).
4.2. Ablation Studies
In this part, we conducted the ablation studies to investigate
the impact of the number of stages and the effectiveness of
the different components of our proposed CPP-Net.
Table 2. The impact of different number of iterative reconstruction
stages in our CPP-Net on Set14 at a CS ratio of 0.25.
Stages 3 5 7 8 (default) 9
PSNR/SSIM 33.65/0.9301 34.21/0.9326 34.27/0.9332 34.41/0.9336 34.46/0.9338
Number of stages . We investigated the impact of thenumber of reconstruction stages in CPP-Net, specifically 3,
5, 7, 8 (the default setting) and 9 stages, to explore the ben-
efits associated with different stage counts. As shown in
Tab. 2, we observe a direct correlation between the number
of stages and the performance, highlighting the efficacy of
our iterative network design. As a result, we choose eight
stages as the default configuration, as it strikes a balanced
compromise between model complexity and performance.
Table 3. Ablation studies of different components in CPP-Net.
‚úîand‚úòrepresent the inclusion and exclusion of the component,
respectively. DS represents the downsampling operations, US is
the upsampling operations, RB denotes the residual block with two
3√ó3convolutions and GELU in between, and Shared denotes the
parameters sharing in the reconstruction stages.
Cases STRes DS US DCAB RB MSTB IFS Shared
Net-1 ‚úò ‚úò ‚úò ‚úò ‚úò ‚úò ‚úî ‚úò
Net-2 ‚úî ‚úò ‚úò ‚úò ‚úò ‚úò ‚úî ‚úò
Net-3 ‚úî ‚úî ‚úî ‚úò ‚úî ‚úò ‚úî ‚úò
Net-4 ‚úî ‚úî ‚úî ‚úî ‚úò ‚úò ‚úî ‚úò
Net-5 ‚úî ‚úî ‚úî ‚úî ‚úò ‚úî ‚úò ‚úò
Net-6 ‚úî ‚úî ‚úî ‚úî ‚úò ‚úî ‚úî ‚úî
CPP-Net ‚úî ‚úî ‚úî ‚úî ‚úò ‚úî ‚úî ‚úò
Different components . We conducted ablation experi-
ments to assess the individual components within our CPP-
Net. The experimental cases are detailed in Tab. 3, and the
corresponding results are summarized in Tab. 4. Our re-
sults show that the inclusion of DPFB ( i.e., STRes, DS, US,
DCAB, MSTB) and IFS leads to progressive improvements
of our method‚Äôs performance. The default CPP-Net out-
25092
performs other cases and alternatives, achieving the highest
PSNR and SSIM, while Net-5 ranks second except on Ur-
ban100.
Table 4. Comparisons of average PSNR (dB)/SSIM, parameter
counts and FLOPs for different methods at a CS ratio of 0.10.
Methods Set11 Set14 LIVE29 General100 Urban100 Param. FLOPs
Net-1 30.15/0.8955 28.95/0.8382 27.66/0.8228 32.10/0.9041 26.39/0.8373 1.31 76.17
Net-2 30.21/0.8969 28.98/0.8388 27.70/0.8236 32.16/0.9052 26.45/0.8390 1.34 76.19
Net-3 30.74/0.9063 29.44/0.8480 28.12/0.8349 32.55/0.9113 27.39/0.8621 21.61 243.52
Net-4 31.21/0.9132 29.83/0.8529 28.43/0.8404 33.12/0.9155 28.42/0.8784 11.69 114.82
Net-5 31.24/0.9134 29.88/0.8533 28.44/0.8409 33.13/0.9158 28.36/0.8779 9.13 114.88
Net-6 30.75/0.9064 29.50/0.8486 28.14/0.8353 32.57/0.9119 27.36/0.8623 2.60 153.47
CPP-Net 31.27/0.9135 29.93/0.8537 28.46/0.8414 33.16/0.9159 28.49/0.8801 12.31 153.47
CASNet 30.29/0.9005 29.37/0.8467 28.16/0.8338 32.78/0.9099 27.40/0.8606 16.90 205.24
DGUNet+30.93/0.9088 29.34/0.8455 28.27/0.8379 32.41/0.9073 28.01/0.8709 6.81 97.79
FSOINet 30.44/0.9018 29.35/0.8451 28.23/0.8359 32.70/0.9085 27.53/0.8627 0.64 17.19
TransCS 29.54/0.8877 28.81/0.8343 27.65/0.8195 31.39/0.8918 26.77/0.8418 1.49 25.86
AutoBCS 28.44/0.8827 28.00/0.8286 27.07/0.8184 30.76/0.8927 25.36/0.8242 2.01 20.11
CSformer 30.09/0.8925 28.79/0.8214 27.80/0.8148 31.60/0.8880 27.30/0.8483 6.65 21.04
DPC-DUN 29.42/0.8801 28.03/0.7950 26.83/0.7812 31.17/0.8716 26.96/0.8361 1.64 65.54
OCTUF 30.70/0.9030 29.47/0.8454 28.23/0.8345 32.77/0.9084 27.79/0.8621 0.40 21.51
TCS-Net 29.04/0.8834 28.19/0.8283 27.21/0.8165 29.90/0.8748 25.87/0.8291 0.52 7.03
4.3. Performance under Noise
We evaluated the performance of our CPP-Net under differ-
ent levels of noise to show the robustness of our method. We
first added Gaussian noise to the images of Set11 and then
evaluated the performance of our CPP-Net under noisy im-
ages. As shown in Fig. 5, the performance of each method
decreases as the variance increases, but CPP-Net still out-
performs the other methods at all tested Gaussian noise lev-
els. Besides, performance under salt-and-pepper noise is
presented in the Supplementary Material (Sec. 3.3).
5. Extension to CS-MRI
Table 5. Average PSNR (dB)/SSIM performance comparisons of
recent CS-MRI methods on Brain dataset using Pseudo Radial
masks at different CS ratios.
Methods 0.05 0.10 0.20 Avg.
Zero-filled 24.20/0.5417 26.81/0.6030 30.41/0.7229 27.14/0.6225
DC-CNN [25] 30.81/0.8370 34.33/0.8957 38.43/0.9467 34.52/0.8931
RDN [32] 30.95/0.8421 34.38/0.8998 38.47/0.9474 34.60/0.8964
ISTA-Net+[41] 31.28/0.8547 34.62/0.9035 38.57/0.9478 34.82/0.9020
CDDN [48] 31.58/0.8513 34.67/0.9014 38.65/0.9476 34.97/0.9001
ADMM-CSNet [36] 31.37/0.8608 34.45/0.8985 38.52/0.9471 34.78/0.9021
HiTDUN [43] 32.72/0.8770 35.71/0.9179 39.27/0.9529 35.90/0.9159
CPP-Net 34.87/0.9176 36.61/0.9318 39.67/0.9559 37.05/0.9351
We expanded CPP-Net into the domain of CS-MRI, aim-
ing to reconstruct magnetic resonance images from partial
Fourier data. In this context, the sampling matrix Œ¶is
defined by a partial Fourier transform matrix, specifically
Œ¶=SF. Here, Srepresents a sub-sampling mask, and
Fcorresponds to the Discrete Fourier Transform (DFT). To
assess the performance, we utilized the same set of training
and testing brain images as used in previous works [36, 41],
using Pseudo Radial masks as S. The results are summa-
rized in Tab. 5. Notably, our CPP-Net consistently outper-
CS ratio = 0.10 CS ratio = 0.04Figure 5. Comparisons of PSNR (dB) performance on Set11 at CS
ratios of 0.04 and 0.10 under different Gaussian noise levels.
CDDN
26.00/0.8114/0.0786ADMM -CSNet
27.53/0.8013/0.0834HiTDUN
27.87/0.8109/0.0835CPP -Net
28.79/0.8533/0.0745RDN
27.29/0.7960/0.0907DC-CNN
26.84/0.7780/0.0950Original Image
CS ratio = 0.10
Local Image
PSNR/SSIM/LPIPS
ISTA -Net+
27.39/0.8022/0.0867Zero -filled
22.44/0.5879/0.1977
Figure 6. Comparisons of visual and corresponding PSNR
(dB)/SSIM/LPIPS performance on Brain dataset using Pseudo Ra-
dial masks at a CS ratio of 0.10. The arrows point to details in the
reconstructed image for better comparison.
forms other methods across all tested CS ratios. This supe-
rior performance is further illustrated in Fig. 6, emphasizing
the CPP-Net‚Äôs capacity to capture richer details and pro-
vide superior image quality compared to alternative meth-
ods. Besides, additional CS-MRI experiments are presented
in the Supplementary Material (Sec. 3.4).
6. Conclusion
In this paper, we propose CPP-Net, a novel deep unfold-
ing framework inspired by CP-PPA. We then enhance the
framework in two key aspects: with DPFB for multi-scale
feature extraction and fusion, and IFS for information-
weighted fusion across iterative reconstruction stages. Ex-
tensive experiments validate CPP-Net‚Äôs superior perfor-
mance in preserving image details beyond current SOTA
methods. Future work aims to extend CPP-Net to diverse
tasks, e.g., image denoising, deraining and super-resolution.
Acknowledgement
This work was supported by the National Natural
Science Foundation of China under Grant 62101455.
25093
References
[1] Arya Bangun, Arash Behboodi, and Rudolf Mathar. Sens-
ing matrix design and sparse recovery on the sphere and the
rotation group. IEEE Trans. Signal Process. , 68:1439‚Äì1454,
2020. 3
[2] Amir Beck and Marc Teboulle. A fast iterative shrinkage-
thresholding algorithm for linear inverse problems. SIAM J.
Imaging Sci. , 2(1):183‚Äì202, 2009. 2, 3
[3] Stephen Boyd. Distributed optimization and statistical
learning via the alternating direction method of multipliers.
Found. Trends Mach. Learn. , 3(1):1‚Äì122, 2010. 2, 4
[4] Emmanuel J Cand `es and Yaniv Plan. A probabilistic and
RIPless theory of compressed sensing. IEEE Trans. Intell.
Transp. Syst. , 57(11):7235‚Äì7254, 2011. 1
[5] Emmanuel J Cand `es and Michael B Wakin. An introduction
to compressive sampling. IEEE Signal Process. Mag. , 25(2):
21‚Äì30, 2008. 1
[6] Antonin Chambolle and Thomas Pock. A First-order primal-
dual algorithm for convex problems with applications to
imaging. J. Math Imaging Vis. , 40(1):120‚Äì145, 2011. 3
[7] Bin Chen and Jian Zhang. Content-aware scalable deep
compressed sensing. IEEE Trans. Image Process. , 31:5412‚Äì
5426, 2022. 4, 7
[8] Wenjun Chen, Chunling Yang, and Xin Yang. FSOINET:
Feature-space optimization-inspired network for image com-
pressive sensing. In IEEE Int. Conf. Acoust. Speech Signal
Process. , pages 2460‚Äì2464, 2022. 4, 7
[9] Ziheng Cheng, Bo Chen, Ruiying Lu, Zhengjue Wang, Hao
Zhang, Ziyi Meng, and Xin Yuan. Recurrent neural net-
works for snapshot compressive imaging. IEEE Trans. Pat-
tern Anal. Mach. Intell. , 45(2):2264‚Äì2281, 2022. 1
[10] Chao Deng, Yuanlong Zhang, Yifeng Mao, Jingtao Fan, Jinli
Suo, Zhili Zhang, and Qionghai Dai. Sinusoidal sampling
enhanced compressive camera for high speed imaging. IEEE
Trans. Pattern Anal. Mach. Intell. , 43(4):1380‚Äì1393, 2019.
1
[11] Chao Dong, Chen Change Loy, and Xiaoou Tang. Acceler-
ating the super-resolution convolutional neural network. In
Eur. Conf. Comput. Vis. , pages 391‚Äì407. 2016. 6
[12] David L. Donoho, Arian Maleki, and Andrea Montanari.
Message-passing algorithms for compressed sensing. Natl.
Acad. Sci. , 106(45):18914‚Äì18919, 2009. 2, 4
[13] Hongping Gan, Yang Gao, Chunyi Liu, Haiwei Chen, Tao
Zhang, and Feng Liu. AutoBCS: Block-based image com-
pressive sensing with data-driven acquisition and nonitera-
tive reconstruction. IEEE Trans. Cybern. , 53(4):2558‚Äì2571,
2023. 2, 6
[14] Hongping Gan, Minghe Shen, Yi Hua, Chunyan Ma, and Tao
Zhang. From patch to pixel: A transformer-based hierarchi-
cal framework for compressive image sensing. IEEE Trans.
Comput. Imaging , 9:133‚Äì146, 2023. 2, 6
[15] Tom Goldstein and Stanley Osher. The split bregman method
for‚Ñì1-regularized problems. SIAM J. Imaging Sci. , 2(2):
323‚Äì343, 2009. 2
[16] Guoyong Gu, Bingsheng He, and Xiaoming Yuan. Cus-
tomized proximal point algorithms for linearly constrainedconvex minimization and saddle-point problems: A unified
approach. Comput. Optim. Appl. , 59(1):135‚Äì161, 2014. 3
[17] Bingsheng He and Xiaoming Yuan. Convergence analysis
of primal-dual algorithms for a saddle-point problem: From
contraction perspective. SIAM J. Imaging Sci. , 5(1):119‚Äì
149, 2012. 3
[18] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-
works. In IEEE Conf. Comput. Vis. Pattern Recognit. , pages
7132‚Äì7141, 2018. 5
[19] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Sin-
gle image super-resolution from transformed self-exemplars.
InIEEE Conf. Comput. Vis. Pattern Recognit. , pages 5197‚Äì
5206, 2015. 6
[20] Kuldeep Kulkarni and Pavan Turaga. Reconstruction-free ac-
tion inference from compressive imagers. IEEE Trans. Pat-
tern Anal. Mach. Intell. , 38(4):772‚Äì784, 2015. 1
[21] Kuldeep Kulkarni, Suhas Lohit, Pavan Turaga, Ronan Ker-
viche, and Amit Ashok. ReconNet: Non-iterative reconstruc-
tion of images from compressively sensed measurements. In
IEEE Conf. Comput. Vis. Pattern Recognit. , pages 449‚Äì458,
2016. 2, 6
[22] Dong Liang, Jing Cheng, Ziwen Ke, and Leslie Ying. Deep
magnetic resonance image reconstruction: Inverse problems
meet neural networks. IEEE Signal Process. Mag. , 37(1):
141‚Äì151, 2020. 1
[23] Yang Liu, Xin Yuan, Jinli Suo, David J Brady, and Qionghai
Dai. Rank minimization for snapshot compressive imaging.
IEEE Trans. Pattern Anal. Mach. Intell. , 41(12):2990‚Äì3006,
2018. 1
[24] Chong Mou, Qian Wang, and Jian Zhang. Deep general-
ized unfolding networks for image restoration. In IEEE/CVF
Conf. Comput. Vis. Pattern Recognit. , pages 17378‚Äì17389,
2022. 1, 4, 7
[25] Jo Schlemper, Jose Caballero, Joseph V . Hajnal, Anthony N.
Price, and Daniel Rueckert. A deep cascade of convolu-
tional neural networks for dynamic MR image reconstruc-
tion. IEEE Trans. Med. Imag. , 37(2):491‚Äì503, 2018. 2, 8
[26] H.R. Sheikh, M.F. Sabir, and A.C. Bovik. A statistical eval-
uation of recent full reference image quality assessment al-
gorithms. IEEE Trans. Image Process. , 15(11):3440‚Äì3451,
2006. 6
[27] Minghe Shen, Hongping Gan, Chao Ning, Yi Hua, and Tao
Zhang. TransCS: A transformer-based hybrid architecture
for image compressed sensing. IEEE Trans. Image Process. ,
31:6991‚Äì7005, 2022. 4, 7
[28] Wuzhen Shi, Feng Jiang, Shaohui Liu, and Debin Zhao. Im-
age compressed sensing using convolutional neural network.
IEEE Trans. Image Process. , 29:375‚Äì388, 2020. 2, 7
[29] Jiechong Song, Bin Chen, and Jian Zhang. Deep memory-
augmented proximal unrolling network for compressive
sensing. Int. J. Comput. Vis. , 131(6):1477‚Äì1496, 2023. 2,
4
[30] Jiechong Song, Bin Chen, and Jian Zhang. Dynamic path-
controllable deep unfolding network for compressive sens-
ing. IEEE Trans. Image Process. , 32:2202‚Äì2214, 2023. 4,
6
25094
[31] Jiechong Song, Chong Mou, Shiqi Wang, Siwei Ma, and Jian
Zhang. Optimization-inspired cross-attention transformer
for compressive sensing. In IEEE/CVF Conf. Comput. Vis.
Pattern Recognit. , 2023. 1, 2, 4, 6
[32] Liyan Sun, Zhiwen Fan, Yue Huang, Xinghao Ding, and
John Paisley. Compressed sensing MRI using a recursive
dilated network. In AAAI Conf. Artif. Intell. , 2018. 2, 8
[33] Jonathan I Tamir, Frank Ong, Suma Anand, Ekin Karasan,
Ke Wang, and Michael Lustig. Computational MRI with
physics-based constraints: Application to multicontrast and
quantitative imaging. IEEE Signal Process. Mag. , 37(1):94‚Äì
104, 2020. 1
[34] Lizhi Wang, Zhiwei Xiong, Guangming Shi, Feng Wu,
and Wenjun Zeng. Adaptive nonlocal sparse representation
for dual-camera compressive hyperspectral imaging. IEEE
Trans. Pattern Anal. Mach. Intell. , 39(10):2104‚Äì2111, 2016.
1
[35] Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy.
Recovering realistic texture in image super-resolution by
deep spatial feature transform. In IEEE/CVF Conf. Comput.
Vis. Pattern Recognit. , pages 606‚Äì615, 2018. 6
[36] Yan Yang, Jian Sun, Huibin Li, and Zongben Xu. ADMM-
CSNet: A deep learning approach for image compressive
sensing. IEEE Trans. Pattern Anal. Mach. Intell. , 42(3):521‚Äì
538, 2020. 1, 2, 4, 8
[37] Dongjie Ye, Zhangkai Ni, Hanli Wang, Jian Zhang, Shiqi
Wang, and Sam Kwong. CSformer: Bridging convolution
and transformer for compressive sensing. IEEE Trans. Image
Process. , 32:2827‚Äì2842, 2023. 2, 6
[38] Roman Zeyde, Michael Elad, and Matan Protter. On single
image scale-up using sparse-representations. In Proc. Int.
Conf. Curves Surfaces , pages 711‚Äì730, 2012. 6
[39] Zhiyuan Zha, Xin Yuan, Bihan Wen, Jiantao Zhou, Jiachao
Zhang, and Ce Zhu. A benchmark for sparse coding: When
group sparsity meets rank minimization. IEEE Trans. Image
Process. , 29:5094‚Äì5109, 2020. 1
[40] Zhiyuan Zha, Bihan Wen, Xin Yuan, Saiprasad Ravishankar,
Jiantao Zhou, and Ce Zhu. Learning nonlocal sparse and
low-rank models for image compressive sensing: Nonlocal
sparse and low-rank modeling. IEEE Signal Process. Mag. ,
40(1):32‚Äì44, 2023. 1
[41] Jian Zhang and Bernard Ghanem. ISTA-Net: Interpretable
optimization-inspired deep network for image compressive
sensing. In IEEE Conf. Comput. Vis. Pattern Recognit. , pages
1828‚Äì1837, 2018. 1, 2, 4, 7, 8
[42] Jian Zhang, Chen Zhao, and Wen Gao. Optimization-
inspired compact deep compressive sensing. IEEE J. Sel.
Top. Signal Process. , 14(4):765‚Äì774, 2020. 4
[43] Jian Zhang, Zhenyu Zhang, Jingfen Xie, and Yongbing
Zhang. High-throughput deep unfolding network for com-
pressive sensing MRI. IEEE J. Sel. Top. Signal Process. , 16
(4):750‚Äì761, 2022. 2, 8
[44] Lei Zhang, Xiaolin Wu, Antoni Buades, and Xin Li. Color
demosaicking by local directional interpolation and nonlocal
adaptive thresholding. J. Electron. Imaging , 20(2):023016,
2011. 6
[45] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness ofdeep features as a perceptual metric. In IEEE Conf. Comput.
Vis. Pattern Recognit. , pages 586‚Äì595, 2018. 1
[46] Zhonghao Zhang, Yipeng Liu, Jiani Liu, Fei Wen, and Ce
Zhu. AMP-Net: Denoising-based deep unfolding for com-
pressive image sensing. IEEE Trans. Image Process. , 30:
1487‚Äì1500, 2021. 2, 4, 7
[47] Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang,
and Michael Pecht. Deep residual shrinkage networks for
fault diagnosis. IEEE Trans. Ind. Inform. , 16(7):4681‚Äì4690,
2020. 6
[48] Hao Zheng, Faming Fang, and Guixu Zhang. Cascaded di-
lated dense network with two-step data consistency for MRI
reconstruction. In Adv. Neural Inform. Process. Syst. , 2019.
2, 8
[49] Ziyang Zheng, Wenrui Dai, Duoduo Xue, Chenglin Li, Junni
Zou, and Hongkai Xiong. Hybrid ISTA: Unfolding ISTA
with convergence guarantees using free-form deep neural
networks. IEEE Trans. Pattern Anal. Mach. Intell. , 45(3):
3226‚Äì3244, 2023. 1
[50] Mingqiang Zhu and Tony Chan. An efficient primal-dual hy-
brid gradient algorithm for total variation image restoration.
Ucla Cam Report , 34:8‚Äì34, 2008. 3
25095
