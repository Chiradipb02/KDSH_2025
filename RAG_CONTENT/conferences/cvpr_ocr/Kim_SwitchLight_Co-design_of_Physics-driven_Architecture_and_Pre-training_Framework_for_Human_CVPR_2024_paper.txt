SwitchLight: Co-design of Physics-driven Architecture and
Pre-training Framework for Human Portrait Relighting
Hoon Kim1Minje Jang1Wonjun Yoon1Jisoo Lee1Donghyun Na1Sanghyun Woo2
1Beeble AI2New York University
Figure 1. Be Anywhere at Any Time. SwitchLight processes a human portrait by decomposing it into detailed intrinsic components, and
re-renders the image under a designated target illumination, ensuring a seamless composition of the subject into any new environment.
Abstract
We introduce a co-designed approach for human portrait
relighting that combines a physics-guided architecture with
a pre-training framework. Drawing on the Cook-Torrance
reflectance model, we have meticulously configured the ar-
chitecture design to precisely simulate light-surface inter-
actions. Furthermore, to overcome the limitation of scarce
high-quality lightstage data, we have developed a self-
supervised pre-training strategy. This novel combination of
accurate physical modeling and expanded training dataset
establishes a new benchmark in relighting realism.
*All authors contributed equally to this work
*https://www.beeble.ai1. Introduction
Relighting is more than an aesthetic tool; it unlocks infi-
nite narrative possibilities and enables seamless integration
of subjects into diverse environments (see Fig. 1). This ad-
vancement resonates with our innate desire to transcend the
physical constraints of space and time, while also providing
tangible solutions to practical challenges in digital content
creation. It is particularly transformative in virtual (VR)
and augmented reality (AR) applications, where relighting
facilitates the real-time adaptation of lighting, ensuring that
users and digital elements coexist naturally within any en-
vironment, offering a next level of telepresence.
In this work, we focus on human portrait relighting.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
25096
While the relighting task fundamentally demands an in-
depth understanding of geometry, material properties, and
illumination, the challenge is more compounded when ad-
dressing human subjects, due to the unique characteristics
of skin surfaces as well as the diverse textures and re-
flectance properties of a wide array of clothing, hairstyles,
and accessories. These elements interact in complex ways,
necessitating advanced algorithms capable of simulating the
subtle interplay of light with these varied surfaces.
Currently, the most promising approach involves the use
of deep neural networks trained on pairs of high-quality
relit portrait images and their corresponding intrinsic at-
tributes, which are sourced from a light stage setup [10].
Initial efforts approached the relighting process as a ‘black
box’ [44, 47], without delving into the underlying mech-
anisms. Later advancements adopted a physics-guided
model design, incorporating the explicit modeling of im-
age intrinsics and image formation physics [31]. Pandey
et al. [33] proposed the Total Relight (TR) architecture,
also physics-guided, which decomposes an input image into
surface normals and albedo maps, and performs relighting
based on the Phong specular reflectance model. The TR
architecture has become foundational model for image re-
lighting, with most recent and advanced architectures build-
ing upon its principle [22, 30, 51].
Following the physics-guided approach, our contribution
lies in a co-design of architecture with a self-supervised
pre-training framework. First, our architecture evolves to-
wards a more accurate physical model by integrating the
Cook-Torrance specular reflectance model [8], represent-
ing a notable advancement from the empirical Phong spec-
ular model [36] employed in the Total Relight architec-
ture. The Cook-Torrance model adeptly simulates light
interactions with surface microfacets, accounting for spa-
tially varying roughness and reflectivity. Secondly, our pre-
training framework scales the learning process beyond the
typically hard-to-obtain lightstage data. By revisiting the
masked autoencoder (MAE) framework [18], we adept it
for the task of relighting. These modifications are crafted to
address the unique challenges posed by this task, enabling
our model to learn from unlabelled data and refine its ability
to produce realistic relit portraits during fine-tuning. To the
best of our knowledge, this is the first time applying self-
supervised pre-training specifically to the relighting task.
To summarize, our contribution is twofold. Firstly, by
enhancing the physical reflectance model, we have intro-
duced a new level of realism in the output. Secondly, by
adopting self-supervised learning, we have expanded the
scale of the training data and enhanced the expression of
lighting in diverse real-world scenarios. Collectively, these
advancements have led SwitchLight framework to achieve
a new state-of-the-art in human portrait relighting.2. Related Work
Human Portrait Relighting is an ill-posed problem due to
its under-constrained nature. To tackle this, earlier meth-
ods incorporated 3D facial priors [43], exploited image in-
trinsics [3, 39], or framed the task as a style transfer [42].
Light stage techniques [48] offer a more powerful solu-
tion by recording subject’s reflectance fields under varying
lighting conditions [10, 13], though they are labor-intensive
and require specialized equipment. A promising alternative
has emerged with deep learning, utilizing neural networks
trained on light stage data. Sun et al. [44] pioneered this
approach, but their method had limitations in representing
non-Lambertian effects. This was improved upon by Nest-
meyer et al. [31], who integrated rendering physics into net-
work design, albeit limited to directional light. Building
upon this, Pandey et al. [33] incorporated the Phong re-
flection model and a high dynamic range (HDR) lighting
map [9] into their network, enabling a more accurate rep-
resentation of global illumination. Simultaneously, efforts
have been made to explore portrait relighting without light
stage data [19, 20, 41, 46, 54]. Moreover, introduction of
NeRF [7] and diffusion-based [37] models has opened new
avenues in the field. However, networks trained with light-
stage data maintain superior accuracy and realism, thanks to
physics-based composited relight image training pairs and
precise ground truth image intrinsics [55].
Our work furthers this domain by integrating the Cook-
Torrance model into our network design, shifting from the
empirical Phong model to a more physics-based approach,
thereby enhancing the realism and detail in relit images.
Self-supervised Pre-training has become a standard train-
ing scheme in the development of large language models
like BERT [11] and GPT [38], and is increasingly influential
in vision models, aiming to replicate the ‘BERT moment’.
This approach typically involves pre-training on extensive
unlabeled data, followed by fine-tuning on specific tasks.
While early efforts in vision models focused on simple pre-
text tasks [12, 16, 32, 35, 52], the field has evolved through
stages like contrastive learning [5, 17] and masked image
modeling [2, 18, 49]. However, the primary focus has re-
mained on visual recognition, with less attention to other
domains. Exceptions include low-level image processing
tasks [4, 6, 26, 29] using the vision transformer [14].
Our research takes a different route, focusing on human
portrait relighting—a complex challenge of manipulating il-
lumination in the image. This direction is crucial because
acquiring accurate ground truth data, especially from light
stage, is both expensive and difficult. We modify the MAE
framework [18], previously successful in robust image rep-
resentation learning and developing locality biases [34], to
suit the unique requirements of effective relighting.
25097
3. SwitchLight
We introduce SwitchLight, a state-of-the-art framework for
human portrait relighting, with its architectural overview
presented in Fig. 2. We first provide foundational concepts
in Sec. 3.1, and define the problem in Sec. 3.2. This is fol-
lowed by detailing the architecture in Sec. 3.3, and lastly,
we describe the loss functions used in Sec. 3.4.
3.1. Preliminaries
In this section, vectors n,v,l, andhare denoted as unit
vectors. Specifically, nrepresents the surface normal, vis
the view direction, lis the incident light direction, and his
the half-vector computed from landv. The dot product is
clamped between [0..1], indicated by ⟨·⟩.
Image Rendering. The primary goal of image rendering
is to create a visual representation that accurately simulates
the interactions between light and surfaces. These complex
interactions are encapsulated by the rendering equation:
Lo(v) =Z
Ωf(v,l)Li(l)⟨n·l⟩dl (1)
where Lo(v)denotes the radiance, or the light intensity per-
ceived by the observer in direction v. It is the cumulative
result of incident lights Li(l)from all possible directions
over the hemisphere, Ω, centered around the surface nor-
mal, denoted as n. Central to this equation lies the Bidirec-
tional Reflectance Distribution Function (BRDF), denoted
asf(v,l), describing the surface’s reflection characteristics.
BRDF Composition. The BRDF, represented by f(v,l),
describes how light is reflected at an opaque surface. It is
composed of two major components: diffuse reflection ( fd)
and specular reflection ( fs):
f(v,l) =fd(v,l) +fs(v,l) (2)
A surface intrinsically exhibits both diffuse and specular re-
flections. The diffuse component uniformly scatters light,
ensuring consistent illumination regardless of the viewing
angle. In contrast, the specular component is viewing angle-
dependent, producing shiny highlights that are crucial for
achieving a photorealistic effect.
Lambertian Diffuse Reflectance. Lambertian re-
flectance, a standard model for diffuse reflection, describes
a uniform light scatter irrespective of the viewing angle.
This ensures a consistent appearance in brightness:
fd(v,l) =σ
π[const.] (3)
Here, σis the albedo , indicating the intrinsic color and
brightness of the surface.Cook-Torrance Specular Reflectance. The Cook-
Torrance model, based on microfacet theory, represents
surfaces as a myriad of tiny, mirror-like facets. It incor-
porates a roughness parameter α, which allows precise
rendering of surface specular reflectance:
fs(v,l) =D(h, α)G(v,l, α)F(v,h, f0)
4⟨n·l⟩⟨n·v⟩(4)
In this model, Dis the microfacet distribution function, de-
scribing the orientation of the microfacets relative to the
half-vector h,Gis the geometric attenuation factor, ac-
counting for the shadowing and masking of microfacets,
andFis the Fresnel term, calculating the reflectance varia-
tion depending on the viewing angle, where f0is the surface
Fresnel reflectivity at normal incidence. A lower αvalue
implies a smoother surface with sharper specular highlights,
whereas a higher αvalue indicates a rougher surface, result-
ing in more diffused reflections. By adjusting α, the Cook-
Torrance model can depict a range of specular reflections.
Image Formation. Upon the base rendering equation, we
include the diffuse and specular components of the BRDF
and derive a unified formula:
Lo(v) =Z
Ω(fd(v,l) +fs(v,l))E(l)⟨n·l⟩dl(5)
where E(l)denotes the incident environmental lighting.
This formula represents the core principle that an image is
a product of interplay between the BRDF and lighting. To
further clarify this concept, we introduce a rendering func-
tionR, succinctly modeling the process of image formation:
I=R(n, σ, α, f 0|{z}
surface attributes, E|{z}
lighting) (6)
It is important to note that since the BRDF is a function of
surface properties, as detailed in Eqn. 3 and 4, we can now
clearly understand that image formation is essentially gov-
erned by the interaction of surface attributes and lighting.
3.2. Problem Formulation
Image Relighting. Given the image formation model
above, our goal is to manipulate the lighting of an exist-
ing image. This involves two main steps: inverse rendering
and re-rendering under target illumination, both driven by
neural networks. For a given source image Isrcand target
illumination Etgt, the process is delineated as:
Inverse Rendering. (n, σ, α, f 0, Esrc) =U(Isrc)
Rendering with Target Light. Itgt=R(n, σ, α, f 0, Etgt)
During the inverse rendering step, the function Uunravels
the intrinsic properties of Isrc. In the subsequent relighting
25098
Figure 2. SwitchLight Architecture. The input source image is decomposed into normal map, lighting ,diffuse andspecular components.
Given these intrinsics, images are re-rendered under target lighting. The architecture integrates the Cook-Torrance reflection model; the
final output combines physically-based predictions with neural network enhancements for realistic portrait relighting.
step, the derived intrinsic properties along with new illu-
mination Etgtare employed by the rendering function Rto
generate the target relit image Itgt.
3.3. Architecture
Our architecture systematically executes the two primary
stages outlined in our problem formulation. The first stage
involves extracting intrinsic properties from the source im-
ageIsrc. For this purpose, we employ a matting net-
work [25, 27, 40] to accurately separate the foreground.
This extracted image is then processed by our inverse ren-
dering network U, which infers normal, albedo, roughness,
reflectivity, and source lighting. Subsequently, the sec-
ond stage involves re-rendering the image under new target
lighting conditions. To achieve this, the acquired intrinsics,
along with the target lighting Etgt, are fed into our relighting
network R, producing the relit image Itgt.
Normal Net. The network takes the source image Isrc∈
RH×W×3and generates a normal map ˆN. Each pixel in
this map contains a unit normal vector ˆ n, indicating the ori-
entation of the corresponding surface point.
Illum Net. The network infers the lighting conditions in
the given image captured in an HDRI format. Specifically,
it computes the convolved HDRIs :
Ep
src(l′) =Z
ΩEsrc(l)|{z}
HDRI⟨l′·l⟩p
|{z}
Phong lobedl (7)
In this equation, Esrc∈RHHDRI×WHDRI×3is the original source
HDRI map, with lindicating spherical directions in the
HDRI space RHHDRI×WHDRI. The term ⟨l′·l⟩prepresents
the Phong reflectance lobe with shininess exponents p∈
{1,16,32,64}, which incorporates various specular terms.
Consequently, it is expressed in a multi-dimensional ten-
sor form as R4×HcHDRI×WcHDRI×HHDRI×WHDRI. Finally, Ep
src∈R4×HcHDRI×WcHDRI×3is the convolved HDRI. In this work, we
set the resolution of HDRI and convolved HDRI at 32 ×64
and 64 ×128, respectively, and we apply convolution on
light source coordinates.
The network employs a cross-attention mechanism at its
core, where predefined Phong reflectance lobes serve as
queries, and the original image acts as both keys and val-
ues. Within this setup, the convolved HDRI maps are syn-
thesized by integrating image information into the Phong
reflectance lobe representation. Specifically, our model uti-
lizes bottleneck features from the Normal Net as a compact
image representation. Our approach simplifies the complex
task of HDRI reconstruction by instead focusing on estimat-
ing interactions with known surface reflective properties.
Diffuse Net. Estimating albedo is challenging due to the
ambiguities in surface color and material properties, further
complicated by shadow effetcs. To address this, we priori-
tize the inference of source diffuse render ,Isrc,diff:
Lsrc,odiff(v) =σ
π|{z}
diffuse BRDFZ
ΩEsrc(l)⟨n·l⟩dl
| {z }
diffuse shading(8)
Our key insight is that the diffuse render closely resembles
the original image, which simplifies the model learning pro-
cess. It captures surface color after removing specular re-
flections, such as shine or gloss, contrasting with albedo that
represents the true surface color unaffected by lighting and
shadows. The network takes a source image Isrc, concate-
nated with its diffuse shading, to produce the diffuse render.
As in Eqn. 7, the diffuse shading, ˆE1
src(ˆn), is derived using
the predicted normals, ˆn, and the predicted lighting map,
ˆE1
src, with a Phong exponent of 1 for the diffuse term. The
albedo map ˆAis then computed by dividing the predicted
diffuse render by its diffuse shading:
ˆσ
π=ˆLsrc,odiff(v)
ˆE1src(ˆn)(9)
25099
Figure 3. Render Net Overview. Utilizing extracted image intrin-
sics, it employs the Cook-Torrance model for initial relighting and
a neural network for enhanced refinement, producing high-fidelity
relit images through a synergistic computational approach.
We have empirically validated that it significantly enhances
albedo prediction across a range of real-world scenarios.
Specular Net. The network infers surface attributes asso-
ciated with the Cook-Torrance specular elements, specifi-
cally, the roughness αandFresnel reflectivity f0. It uses a
source image, predicted normal, and albedo maps as inputs.
Render Net. The network utilizes extracted intrinsic sur-
face attributes to produce the target relit images . It gen-
erates two types of relit images, as shown in Fig. 3. The
first type adheres to the physically-based rendering (PBR)
principles of the Cook-Torrance model. This involves com-
puting diffuse and specular renders under the target illumi-
nation using Eqn. 3 and Eqn. 4. These renders are combined
to form the PBR render, ˆIPBR
tgt, as:
ˆLPBR
tgt,o(v) =ˆLtgt,odiff(v) +ˆLtgt,ospec(v) (10)
The second type of relit image is the result of a neural net-
work process. This enhances the PBR render, capturing
finer details that the Cook-Torrance model might miss. It
employs the albedo, along with the diffuse and specular ren-
ders from the Cook-Torrance model, to infer a more refined
target relit image, termed the neural render, ˆINeural
tgt . The
qualitative improvements achieved through this neural en-
hancement are illustrated in Fig. 4.
3.4. Objectives
We supervise both intrinsic image attributes and relit im-
ages using their corresponding ground truths, obtained from
the lightstage. We employ a combination of reconstruction,
perceptual [23], adversarial [21], and specular [33] losses.
Reconstruction Loss L=ℓ1(X,ˆX).It measures the
pixel-level differences between the ground truth Xand its
prediction ˆX. This loss is applied across different attributes,
including normal map ˆN, convolved source HDRI maps
ˆEp
src, source diffuse render ˆIsrc,diff, albedo map ˆA, and both
Figure 4. Neural Render Enhancement. Using the Cook-
Torrance model, diffuse and specular renders are computed, which
are then composited into a physically-based rendering. Subse-
quently, a neural network enhances this PBR render, improving
aspects such as brightness and specular details.
types of target relit images ˆIPBR
tgtandˆINeural
tgt . The attributes
like the roughness αand Fresnel reflectivity f0are also im-
plicitly learned when this loss applied to the PBR render.
Perceptual Loss Lvgg=ℓ2(VGG (X),VGG (ˆX)).It cap-
tures high-level feature differences based on a VGG-
network feature comparison. We apply this loss to the
source diffuse render, albedo, and target relit images.
Adversarial Loss Ladv=GAN (X,ˆX).It promotes re-
alism in the generated images by fooling a discriminator
network. This loss is applied to the target relit images. We
employ a PatchGAN architecture, with detailed specifica-
tions provided in the supplementary material.
Specular Loss Lspec=ℓ1(X⊙ˆS,ˆX⊙ˆS).It enhances
the specular highlights in the relit images. Specifically, we
utilize the predicted specular render ˆS:=ˆIPBR
tgt,specderived
from the Cook-Torrance physical model, to weigh the ℓ1
reconstruction loss. Here, ⊙denotes the element-wise mul-
tiplication. This loss is applied to the neural render.
Final Loss. The SwitchLight is trained in an end-to-end
manner using the weighted sum of the above losses:
Lrelight= 10· Lnormal + 10· LsrcHDRI+ 0.2· Lsrcdiff
−+ 0.2· Lalbedo+ 0.2· LPBR+ 0.2· LNeural
−+Lvggsrcdiff+Lvggalbedo+LvggPBR+LvggNeural
−+Ladv PBR+Ladv Neural+ 0.2· LspecNeural.(11)
25100
Figure 5. Dynamic Masking Strategies. We have generalized the
MAE masks to include overlapping patches of varying sizes, as
well as outpainting and free-form masks.
We empirically determined the weighting coefficients.
4. Multi-Masked Autoencoder Pre-training
We introduce the Multi-Masked Autoencoder (MMAE), a
self-supervised pre-training framework designed to enhance
feature representations in relighting models. It aims to im-
prove output quality without relying on additional, costly
light stage data. Building upon the MAE framework [18],
MMAE capitalizes on the inherent learning of crucial im-
age features like structure, color, and texture, which are es-
sential for relighting tasks. However, adapting MAE to our
specific needs poses several non-trivial challenges. Firstly,
MAE is primarily designed for vision transformers [14],
while our focus is on a UNet, a convolution-based architec-
ture. This convolutional structure, with its hierarchical na-
ture and aggressive pooling, is known to simplify the MAE
task, necessitating careful adaptation [49]. Further, the hy-
perparameters of MAE, particularly the fixed mask size and
ratio, are also specific to vision transformers. These fac-
tors could introduce biases during training and hinder the
model to recognize image features at various scales. More-
over, MAE relies solely on masked region reconstruction
loss, limiting the model to understand the global coherence
of the reconstructed region in relation to its visible context.
To address these challenges effectively, we have devel-
oped two key strategies within the MMAE framework:
Dynamic Masking. MMAE eliminates two key hyperpa-
rameters, mask size and ratio, by introducing a variety of
mask types to generalize the MAE. These types, which
include overlapping patches of various sizes, outpainting
masks [45], and free-form masks [28] (see Fig.5), each con-
tribute to the model’s versatility. With the ability to handle
challenging masked regions, MMAE achieves a more com-
prehensive understanding of image properties.
Generative Target. In addition to its structural advance-
ments, MMAE incorporates a new loss function strat-
egy. We have adopted perceptual [23] and adversarial
losses [21], along with the original reconstruction loss. As
a result, MMAE is equipped not only to reconstruct miss-
ing image parts but also to ensure synthesis capabilities and
their seamless integration with the original context. In prac-
tice, the weights for the three losses are equally set.
We pre-train the entire UNet architecture using MMAE,
and, unlike MAE, we retain the decoder and fine-tune theentire model on relighting ground truths.
5. Data
We constructed the OLAT (One Light at a Time) dataset us-
ing a light stage [10, 48] equipped with 137 programmable
LED lights and 7 frontal-viewing cameras. Our dataset
comprises images of 287 subjects, with each subject be-
ing captured in up to 15 different poses, resulting in a to-
tal of 29,705 OLAT sequences. We sourced HDRI dataset
from several publicly available archives. Specifically, we
acquired 559 HDRI maps from Polyhaven, 76 from Noah
Witchell, 364 from HDRMAPS, 129 from iHDRI, and
34 from eisklotz. In addition, we incorporated synthetic
HDRIs created using the method proposed in [30]. During
training, HDRIs are randomly selected with equal probabil-
ity from either real-world or synthetic collections.
We produced training pairs by projecting the sampled
source and target lighting maps onto the reflectance fields
of the OLAT images [10]. To derive the ground truth in-
trinsics, we applied the photometric stereo method [50] and
obtained normal and albedo maps.
6. Experiments
This section details our experimental results. We begin with
a comparative evaluation of our method against state-of-
the-art approaches using the OLAT dataset. We also em-
ploy images from the FFHQ–test [24] for user studies. For
qualitative analysis, we utilize copyright-free portrait im-
ages from Pexels [1]. Additionally, we conduct ablation
studies to validate the efficacy of our pre-training frame-
work and architectural design choice. Subsequently, we de-
tail the additional features and conclude by discussing its
limitations. Our evaluation uses the OLAT test set, com-
prising 35 subjects and 11 lighting environments, ensuring
no overlap with the train set.
Evaluation metrics. We employ several key metrics for
evaluating the prediction accuracy; Mean Absolute Error
(MAE ), Mean Squared Error ( MSE ), Structural Similar-
ity Index Measure ( SSIM ) and Learned Perceptual Image
Patch Similarity ( LPIPS ). While these metrics offer valu-
able quantitative insights, they do not fully capture the sub-
tleties of visual quality enhancement. Therefore, we em-
phasize the importance of qualitative evaluations to gain a
comprehensive understanding of model performance.
Baselines. We compared our approach with three state-
of-the-art baselines: Single Image Portrait Relighting
(SIPR ) [44], which uses a single neural network for re-
lighting; Total Relight ( TR) [33], employing multiple neu-
ral networks that incorporate the Phong reflectance model;
andLumos [51], a TR adaptation for synthetic datasets.
Due to the lack of publicly available code or model from
these methods, we either integrated their techniques into our
25101
MAE↓MSE↓SSIM↑LPIPS ↓
SIPR [44] 0.1715 0.0748 0.8432 3.648
TR [33] 0.1643 0.0658 0.8465 3.425
Ours 0.1023 0.0275 0.9002 2.137
Ours (w. MMAE) 0.0933 0.0235 0.9051 2.059
Table 1. Quantitative Evaluation on the OLAT test set.
Lumos [51] TR [33] Ours
Consistent Lighting 0.0478 0.1852 0.7671
Facial Details 0.2022 0.2602 0.5376
Similar Identity 0.1741 0.2440 0.5819
Table 2. User Study on the FFHQ test set.
Figure 6. Impact of Pre-training. The fine details such as specu-
lar highlights, skin tones, and shadows are notably improved.
framework or requested the original authors to process our
inputs with their models and share the results.
Quantitative Comparisons. The results in Table. 1 shows
our method outperforming SIPR and TR baselines, demon-
strating the significance of incorporating advanced render-
ing physics and reflectance models. The transition from
SIPR to TR emphasizes the value of physics-based design,
while the shift from TR to our approach underscores the im-
portance of transitioning from the empirical Phong model to
the more accurate Cook-Torrance model. Additionally, pre-
training contributes to further enhancements, as evidenced
by the improved image details, depicted in Fig 6.
Qualitative Comparisons. Our relighting method exhibits
several key advantages over previous approaches, as show-
cased in Fig. 7. It effectively harmonizes light direction and
softness, avoiding harsh highlights and inaccurate lighting
that are commonly observed in other methods. A notable
strength of our approach lies in its ability to capture high-
frequency details like specular highlights and hard shadows.
Additionally, as shown in the second row, it preserves fa-
Figure 7. Qualitative Comparison on the Pexels images [1].
cial details and identity, ensuring high fidelity to the sub-
ject’s original features and mitigating common distortions
seen in previous approaches. Moreover, our approach ex-
cels in handling skin tones, producing natural and accurate
results under various lighting conditions. This is clearly
demonstrated in the fourth row, where our method contrasts
sharply with the over-saturated or pale tones from previous
methods. Finally, the nuanced treatment of hair is high-
lighted in the sixth row, where our approach maintains luster
and detail, unlike the flattened effect typical in other meth-
ods. More qualitative results are available in our supple-
mentary video demonstration.
User Study. We conducted a human subjective test to eval-
uate the visual quality of relighitng results, summarized in
Table. 2. In each test case, workers were presented with
an input image and an environment map. They were asked
to compare the relighting results from three methods–Ours,
Lumos, and TR–based on three criteria: 1) consistency of
lighting with the environment map, 2) preservation of fa-
cial details, and 3) maintenance of the original identity. To
ensure unbiased evaluations, the order of the methods pre-
sented was randomized. To aid in understanding the con-
cept of consistent lighting, relit balls were displayed along-
side the images. The study included a total of 256 images,
consisting of 32 portraits each illuminated with 8 different
HDRIs. Each worker was tasked with selecting the best im-
age for each specific criterion, randomly assessing 30 sam-
ples. A total of 47 workers participated in the study. The
results indicate a strong preference for our results over the
baseline methods across all evaluated metrics.
25102
Method MAE ↓MSE↓SSIM↑LPIPS ↓
Pre-trainingMAE [18] 0.0952 0.0242 0.9007 2.096
MMAE 0.0933 0.0235 0.9051 2.059
DiffuseNetAlbedo 0.1053 0.0295 0.8985 2.197
Diff Render 0.1023 0.0275 0.9002 2.137
Table 3. Ablation Studies on the OLAT test set.
Figure 8. Ablation on DiffuseNet. Our approach successfully
infers the albedo on various surfaces (skin, teeth, and accessories).
Ablation Studies. We analyze our two major design
choices in Table. 3: the MMAE pre-training framework and
DiffuseNet. The MMAE, which integrates dynamic mask-
ing with generative objectives, outperforms MAE. This su-
periority is mainly due to the incorporation of challenging
masks and global coherence objectives, enabling the model
to learn richer features during pre-training. Furthermore,
our method of predicting diffuse render demonstrates su-
periority over direct albedo prediction. Firstly, we see it
simplifies the learning process, as predicting diffuse render
is more closely related to the original image. Secondly, our
approach effectively distinguishes between the influences of
illumination (diffuse shading) and surface properties (dif-
fuse render). This distinction is crucial for accurately mod-
eling the intrinsic color of surfaces, as it enables indepen-
dent and precise evaluation of each element (see Eqn. 9). In
contrast, methods that predict albedo directly often struggle
to differentiate between these factors, leading to significant
inaccuracies in color constancy, as shown in Fig. 8.
Applications. We present two applications using predicted
intrinsics in Fig. 9. First, real-time PBR via Cook-Torrance
Figure 9. Applications. We showcase additional features of
SwitchLight, powered by the diverse intrinsics features.
Figure 10. Limitations. The model faces challenges in removing
strong shadows, misinterpreting reflective surfaces like sunglasses,
and inaccurately predicting albedo for face paint.
components in Three.js graphics library. Second, switching
the lighting environment between the source and reference
images. Further details are in the supplementary video.
Limitations. We identified a few failure cases in Fig. 10.
First, our model struggles with neutralizing strong shad-
ows, which leads to inaccurate facial geometry and resid-
ual shadow artifacts in both albedo and relit images. Incor-
porating shadow augmentation techniques [15, 53] during
training could mitigate this issue. Second, the model in-
correctly interprets reflective surfaces, such as sunglasses,
as opaque objects in the normal image. This error prevents
the model from properly removing reflective highlights in
the albedo and relit images. Lastly, the model inaccurately
predicts the albedo for face paint. Implementing a semantic
mask [51] to distinguish different semantic regions sepa-
rately from the skin could help resolving these issues.
7. Conclusion
We introduce SwitchLight, an architecture based on
Cook-Torrance rendering physics, enhanced with a self-
supervised pre-training framework. This co-designed ap-
proach significantly outperforms previous models. Our fu-
ture plans include scaling the current model beyond images
to encompass video and 3D data. We hope our proposal
serve as a new foundational model for relighting tasks.
25103
References
[1] Pexels. https://www.pexels.com . 6, 7
[2] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:
Bert pre-training of image transformers. arXiv preprint
arXiv:2106.08254 , 2021. 2
[3] Jonathan T Barron and Jitendra Malik. Shape, illumination,
and reflectance from shading. IEEE transactions on pattern
analysis and machine intelligence , 37(8):1670–1687, 2014.
2
[4] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping
Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and
Wen Gao. Pre-trained image processing transformer. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 12299–12310, 2021. 2
[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In International conference on ma-
chine learning , pages 1597–1607. PMLR, 2020. 2
[6] Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao,
and Chao Dong. Activating more pixels in image super-
resolution transformer. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 22367–22377, 2023. 2
[7] Zhaoxi Chen and Ziwei Liu. Relighting4d: Neural re-
lightable human from videos. In European Conference on
Computer Vision , pages 606–623. Springer, 2022. 2
[8] Robert L Cook and Kenneth E. Torrance. A reflectance
model for computer graphics. ACM Transactions on Graph-
ics (ToG) , 1(1):7–24, 1982. 2
[9] Paul Debevec. Rendering synthetic objects into real scenes:
Bridging traditional and image-based graphics with global
illumination and high dynamic range photography. In Acm
siggraph 2008 classes , pages 1–10. 2008. 2
[10] Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter
Duiker, Westley Sarokin, and Mark Sagar. Acquiring the
reflectance field of a human face. In Proceedings of the
27th annual conference on Computer graphics and interac-
tive techniques , pages 145–156, 2000. 2, 6
[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 2
[12] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper-
vised visual representation learning by context prediction. In
Proceedings of the IEEE international conference on com-
puter vision , pages 1422–1430, 2015. 2
[13] Julie Dorsey, James Arvo, and Donald Greenberg. Interac-
tive design of complex time dependent lighting. IEEE Com-
puter Graphics and Applications , 15(2):26–36, 1995. 2
[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 2, 6
[15] David Futschik, Kelvin Ritland, James Vecore, Sean Fanello,
Sergio Orts-Escolano, Brian Curless, Daniel S `ykora, and Ro-hit Pandey. Controllable light diffusion for portraits. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 8412–8421, 2023. 8
[16] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un-
supervised representation learning by predicting image rota-
tions. arXiv preprint arXiv:1803.07728 , 2018. 2
[17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross
Girshick. Momentum contrast for unsupervised visual rep-
resentation learning. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
9729–9738, 2020. 2
[18] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 16000–
16009, 2022. 2, 6, 8
[19] Andrew Hou, Ze Zhang, Michel Sarkis, Ning Bi, Yiying
Tong, and Xiaoming Liu. Towards high fidelity face relight-
ing with realistic shadows. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 14719–14728, 2021. 2
[20] Andrew Hou, Michel Sarkis, Ning Bi, Yiying Tong, and
Xiaoming Liu. Face relighting with geometrically consis-
tent shadows. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 4217–
4226, 2022. 2
[21] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adver-
sarial networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1125–1134,
2017. 5, 6
[22] Chaonan Ji, Tao Yu, Kaiwen Guo, Jingxin Liu, and Yebin
Liu. Geometry-aware single-image full-body human relight-
ing. In European Conference on Computer Vision , pages
388–405. Springer, 2022. 2
[23] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Percep-
tual losses for real-time style transfer and super-resolution.
InComputer Vision–ECCV 2016: 14th European Confer-
ence, Amsterdam, The Netherlands, October 11-14, 2016,
Proceedings, Part II 14 , pages 694–711. Springer, 2016. 5,
6
[24] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 4401–4410, 2019. 6
[25] Taehun Kim, Kunhee Kim, Joonyeong Lee, Dongmin Cha,
Jiho Lee, and Daijin Kim. Revisiting image pyramid struc-
ture for high resolution salient object detection. In Pro-
ceedings of the Asian Conference on Computer Vision , pages
108–124, 2022. 4
[26] Wenbo Li, Xin Lu, Shengju Qian, Jiangbo Lu, Xiangyu
Zhang, and Jiaya Jia. On efficient transformer-based
image pre-training for low-level vision. arXiv preprint
arXiv:2112.10175 , 2021. 2
[27] Shanchuan Lin, Andrey Ryabtsev, Soumyadip Sengupta,
Brian L Curless, Steven M Seitz, and Ira Kemelmacher-
Shlizerman. Real-time high-resolution background matting.
25104
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 8762–8771, 2021. 4
[28] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang,
Andrew Tao, and Bryan Catanzaro. Image inpainting for ir-
regular holes using partial convolutions. In Proceedings of
the European conference on computer vision (ECCV) , pages
85–100, 2018. 6
[29] Yihao Liu, Jingwen He, Jinjin Gu, Xiangtao Kong, Yu Qiao,
and Chao Dong. Degae: A new pretraining paradigm for
low-level vision. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages
23292–23303, 2023. 2
[30] Yiqun Mei, He Zhang, Xuaner Zhang, Jianming Zhang,
Zhixin Shu, Yilin Wang, Zijun Wei, Shi Yan, HyunJoon
Jung, and Vishal M Patel. Lightpainter: Interactive por-
trait relighting with freehand scribble. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 195–205, 2023. 2, 6
[31] Thomas Nestmeyer, Jean-Franc ¸ois Lalonde, Iain Matthews,
and Andreas Lehrmann. Learning physics-guided face re-
lighting under directional light. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5124–5133, 2020. 2
[32] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of
visual representations by solving jigsaw puzzles. In Euro-
pean conference on computer vision , pages 69–84. Springer,
2016. 2
[33] Rohit Pandey, Sergio Orts Escolano, Chloe Legendre, Chris-
tian Haene, Sofien Bouaziz, Christoph Rhemann, Paul De-
bevec, and Sean Fanello. Total relighting: learning to relight
portraits for background replacement. ACM Transactions on
Graphics (TOG) , 40(4):1–21, 2021. 2, 5, 6, 7
[34] Namuk Park, Wonjae Kim, Byeongho Heo, Taekyung Kim,
and Sangdoo Yun. What do self-supervised vision transform-
ers learn? arXiv preprint arXiv:2305.00729 , 2023. 2
[35] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A Efros. Context encoders: Feature
learning by inpainting. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
2536–2544, 2016. 2
[36] Bui Tuong Phong. Illumination for computer generated pic-
tures. In Seminal graphics: pioneering efforts that shaped
the field , pages 95–101. 1998. 2
[37] Puntawat Ponglertnapakorn, Nontawat Tritrong, and Supa-
sorn Suwajanakorn. Difareli: Diffusion face relighting.
arXiv preprint arXiv:2304.09479 , 2023. 2
[38] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. Improving language understanding by gen-
erative pre-training. 2018. 2
[39] Soumyadip Sengupta, Angjoo Kanazawa, Carlos D Castillo,
and David W Jacobs. Sfsnet: Learning shape, reflectance
and illuminance of facesin the wild’. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 6296–6305, 2018. 2
[40] Soumyadip Sengupta, Vivek Jayaram, Brian Curless,
Steven M Seitz, and Ira Kemelmacher-Shlizerman. Back-
ground matting: The world is your green screen. In Pro-ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 2291–2300, 2020. 4
[41] Soumyadip Sengupta, Brian Curless, Ira Kemelmacher-
Shlizerman, and Steven M Seitz. A light stage on every desk.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 2420–2429, 2021. 2
[42] YiChang Shih, Sylvain Paris, Connelly Barnes, William T
Freeman, and Fr ´edo Durand. Style transfer for headshot por-
traits. 2014. 2
[43] Zhixin Shu, Sunil Hadap, Eli Shechtman, Kalyan Sunkavalli,
Sylvain Paris, and Dimitris Samaras. Portrait lighting trans-
fer using a mass transport approach. ACM Transactions on
Graphics (TOG) , 36(4):1, 2017. 2
[44] Tiancheng Sun, Jonathan T Barron, Yun-Ta Tsai, Zexiang
Xu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay
Busch, Paul Debevec, and Ravi Ramamoorthi. Single image
portrait relighting. ACM Transactions on Graphics (TOG) ,
38(4):1–12, 2019. 2, 6, 7
[45] Piotr Teterwak, Aaron Sarna, Dilip Krishnan, Aaron
Maschinot, David Belanger, Ce Liu, and William T Free-
man. Boundless: Generative adversarial networks for image
extension. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 10521–10530, 2019.
6
[46] Yifan Wang, Aleksander Holynski, Xiuming Zhang, and
Xuaner Zhang. Sunstage: Portrait reconstruction and re-
lighting using the sun as a light stage. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 20792–20802, 2023. 2
[47] Zhibo Wang, Xin Yu, Ming Lu, Quan Wang, Chen Qian, and
Feng Xu. Single image portrait relighting via explicit mul-
tiple reflectance channel modeling. ACM Transactions on
Graphics (TOG) , 39(6):1–13, 2020. 2
[48] Andreas Wenger, Andrew Gardner, Chris Tchou, Jonas
Unger, Tim Hawkins, and Paul Debevec. Perfor-
mance relighting and reflectance transformation with time-
multiplexed illumination. ACM Transactions on Graphics
(TOG) , 24(3):756–764, 2005. 2, 6
[49] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei
Chen, Zhuang Liu, In So Kweon, and Saining Xie. Con-
vnext v2: Co-designing and scaling convnets with masked
autoencoders. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 16133–
16142, 2023. 2, 6
[50] Robert J Woodham. Photometric method for determining
surface orientation from multiple images. Optical engineer-
ing, 19(1):139–144, 1980. 6
[51] Yu-Ying Yeh, Koki Nagano, Sameh Khamis, Jan Kautz,
Ming-Yu Liu, and Ting-Chun Wang. Learning to relight
portrait images via a virtual light stage and synthetic-to-real
adaptation. ACM Transactions on Graphics (TOG) , 41(6):
1–21, 2022. 2, 6, 7, 8
[52] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful
image colorization. In Computer Vision–ECCV 2016: 14th
European Conference, Amsterdam, The Netherlands, Octo-
ber 11-14, 2016, Proceedings, Part III 14 , pages 649–666.
Springer, 2016. 2
25105
[53] Xuaner Zhang, Jonathan T Barron, Yun-Ta Tsai, Rohit
Pandey, Xiuming Zhang, Ren Ng, and David E Jacobs. Por-
trait shadow manipulation. ACM Transactions on Graphics
(TOG) , 39(4):78–1, 2020. 8
[54] Hao Zhou, Sunil Hadap, Kalyan Sunkavalli, and David W
Jacobs. Deep single-image portrait relighting. In Proceed-
ings of the IEEE/CVF international conference on computer
vision , pages 7194–7202, 2019. 2
[55] Taotao Zhou, Kai He, Di Wu, Teng Xu, Qixuan Zhang, Kuix-
iang Shao, Wenzheng Chen, Lan Xu, and Jingyi Yu. Re-
lightable neural human assets from multi-view gradient il-
luminations. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 4315–
4327, 2023. 2
25106
