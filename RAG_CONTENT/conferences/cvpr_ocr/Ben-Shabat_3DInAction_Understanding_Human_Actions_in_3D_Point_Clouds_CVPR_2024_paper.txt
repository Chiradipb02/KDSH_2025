3DInAction: Understanding Human Actions in 3D Point Clouds
Yizhak Ben-Shabat1;2Oren Shrout2Stephen Gould1
1Australian National University2Technion, Israel Institute of Technology
sitzikbs@technion.ac.il, shrout.oren@campus.technion.ac.il, stephen.gould@anu.edu.au
https://github.com/sitzikbs/3dincaction
Figure 1. t-patches for action recognition. We propose a new representation for dynamic 3D point clouds. Termed t-patches, these
are locally evolving point cloud sets aggregated over time. Learning features over t-patches provides an improved temporal point cloud
representation for action understanding.
Abstract
We propose a novel method for 3D point cloud ac-
tion recognition. Understanding human actions in RGB
videos has been widely studied in recent years, however,
its 3D point cloud counterpart remains under-explored de-
spite the clear value that 3D information may bring. This
is mostly due to the inherent limitation of the point cloud
data modality—lack of structure, permutation invariance,
and varying number of points—which makes it difﬁcult to
learn a spatio-temporal representation. To address this lim-
itation, we propose the 3DinAction pipeline that ﬁrst esti-
mates patches moving in time (t-patches) as a key build-
ing block, alongside a hierarchical architecture that learns
an informative spatio-temporal representation. We show
that our method achieves improved performance on existing
datasets, including DFAUST and IKEA ASM. Code is pub-
licly available at https://github.com/sitzikbs/3dincaction.
1. Introduction
In this paper, we address the task of action recognition
from 3D point cloud sequences. We propose a novel
pipeline wherein points are grouped into temporally evolv-ing patches that capture discriminative action dynamics.
Our work is motivated by the massive growth of online me-
dia, mobile and surveillance cameras that have enabled the
computer vision community to develop many data-driven
action-recognition methods [5, 12,26,31], most of which
rely on RGB video data. Recently, commodity 3D sensors
are gaining increased momentum, however, the 3D point
cloud modality for action recognition has yet been under-
exploited due to the scarcity of 3D action-labeled data.
In many cases, a pure RGB video-based inference may
not be enough and incorporating other modalities like ge-
ometry is required. This is especially necessary for safety
critical applications such as autonomous systems, where
redundancy is crucial, or in scenarios where the video is
heavily degraded (e.g., due to poor lighting). Some ap-
proaches incorporate geometrical information implicitly,
e.g., through intermediate pose estimation [7]. This often
entails extra steps that require more time and resources and
is still limited to video input. Therefore a more explicit ap-
proach is desirable.
3D sensors provide an alternative modality in the form of
point clouds sampled on the environment. Despite the vast
research on 3D vision and learning, even static 3D point
cloud datasets are signiﬁcantly smaller than their RGB im-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
19978
age counterparts due to difﬁculties in collecting and label-
ing. 3D point cloud sequence databases are even smaller,
making it more difﬁcult to learn a meaningful 3D action
representation. Furthermore, learning a point cloud repre-
sentation still remains an active research ﬁeld because point
clouds are unstructured, unordered, and may contain a vary-
ing number of points. Learning a temporal point cloud rep-
resentation is even more challenging since, unlike pixels,
there is no one-to-one point correspondence through time.
We address these challenges and propose the 3DinAc-
tion pipeline for 3D point cloud action recognition. In our
pipeline, we ﬁrst extract local temporal point patches (t-
patches) that reﬂect a point region’s motion in time, see
Figure 1. We then learn a t-patch representation using a
novel hierarchical architecture that incorporates spatial fea-
tures in the temporal domain. We ﬁnally get an action pre-
diction for each frame in a sequence by aggregating mul-
tiple t-patch representations. This pipeline overcomes the
need for ground truth point temporal correspondence, grid
structure, point order, and a ﬁxed number of points in each
frame. Intuitively, patches reﬂect local surface deformation
and are more robust to point correspondence errors.
We conduct extended experiments to evaluate the perfor-
mance of our approach compared to existing SoTA methods
and show that 3DinAction provides signiﬁcant performance
gains of 13% and7%in accuracy on DFAUST and IKEA
ASM, respectively.
The key contributions of our work are as follows:
• A novel representation for dynamically evolving local
point cloud sets termed t-patches.
• A hierarchical architecture that produces an informa-
tive spatio-temporal representation for sequences of point
clouds.
2. Related Work
Learning 3D point cloud representations. Point clouds
pose a challenge for neural networks due to their unstruc-
tured and point-wise unordered nature. To address these
challenges, several approaches have been proposed. Point-
Net [23, 24] uses permutation-invariant operators, such as
pointwise MLPs and pooling layers, to aggregate features
across a point set. Some approaches construct a graph
from the point set. DGCNN [34] applies message pass-
ing and performs graph convolutions on kNN graphs, KC-
Net [29] uses kernel correlation and graph pooling, and
Kd-Networks [15] apply multiplicative transformations and
share the parameters based on the subdivisions imposed by
kd-trees. Alternatively, the structure can be imposed using
a grid of voxels [22, 36], or a grid of Gaussians in 3Dm-
FVNet [1]. Another alternative avoids the structure by using
Transformer’s attention mechanism [17, 37] . For a compre-
hensive survey of point cloud architectures please see [14].
Recently, various factors that can impact the training ofdifferent architectures have been investigated [13, 25]. This
includes exploring data augmentation strategies and loss
functions that are not speciﬁc to a particular architecture.
The results of this study showed that older PointNet-based
architectures [23, 24] can perform comparably to newer ar-
chitectures with minor changes.
All of the above methods deal with static, single-frame,
or single-shape point clouds. In this work, the input
is a temporal point cloud where a representation for a
short sequence is required and point correspondence be-
tween frames is unknown. Therefore extending existing ap-
proaches is not trivial.
Learning temporal 3D point cloud representations.
Temporal point clouds have not been as extensively stud-
ied as their static counterparts, in particular for action
recognition. Meteornet [21] processes 4D points using a
PointNet++architecture where they appended a temporal
dimension to the spatial coordinates. PSTNet [10, 11] pro-
posed spatio-temporal convolutions and utilized some of
the temporal consistency for action recognition. Similarly,
P4Transformer [8] uses 4D convolutions and a transformer
for capturing appearance and motion via self-attention. In
a follow-up work PST-Transformer [9] employs a video
level of self-attention in search for similar points across en-
tire videos and so encodes spatio-temporal structure. Some
works attempt to alleviate the full supervision requirement
for 3D action recognition. These include self-supervised
features learning [32] by predicting temporal order from a
large unlabeled dataset and ﬁne-tuning on a smaller anno-
tated datasets and unsupervised skeleton colorization [35].
Additional supervised approaches include MinkowskiNet
[6] that uses a 4D spatio-temporal CNN after converting
the point clouds to an occupancy grid, 3DV [33] that en-
codes 3D motion information from depth videos into a com-
pact voxel set, and Kinet [38] that implicitly encoded fea-
ture level dynamics in feature space by unrolling the normal
solver of ST-surfaces.
The above methods, perform a single classiﬁcation per
clip. In this paper, we focus on a related, and more chllang-
ing, task that requires a prediction per-frame. We propose
to convert the point cloud representation into t-patches and
use an MLP based hierarchical architecture to get the spatio-
temporal representation.
3D action understanding datasets. One of the major driv-
ing forces behind the success of learning-based approaches
is the availability of annotated data. For the task of 3D point
cloud action recognition, there is currently no designated
standard dataset, however, some existing datasets may be
extended. The CAD 60 and CAD 120 [16, 30] datasets in-
clude 60 and 120 long-term activity videos of 12 and 10
classes respectively (e.g., making cereal, microwave food).
These datasets provide raw RGB, skeletons, and depth data
however its small scale and long-term focus limit its effec-
19979
tiveness. The NTU RGB+D 60 [28] and NTU RGB+D 120
[20] provide56K and114K clips containing 60 and 120
actions classes respectively, e.g., taking off a jacket, taking
a selﬁe. They provide three different simultaneous RGB
views, IR and depth streams as well as 3D skeletons. While
these datasets can be considered large-scale, their contrived
nature makes recent skeleton-based methods (e.g., [7]) per-
form well, making a prior-free approach difﬁcult to justify.
The MSR-Action3D dataset [19] includes 20 action classes
performed by 10 subjects for a total of 567 depth map se-
quences, collected using a Kinect v1 device (23K frames).
The sequences in this dataset are very short and therefore
using it to evaluate learning-based approaches provides a
limited indication of generalization. The above datasets
provide per clip action annotations.
Some datasets inherently provide per-frame annotations.
The IKEA ASM dataset [2] provides 371 videos clipped
into 31K clips. It contains 33 action classes related to fur-
niture assembly, annotated per frame. This dataset provides
several modalities including three RGB views, and Depth.
It is an extremely challenging dataset since the human as-
sembler is often occluded and presents very unique assem-
bly poses. It is also very imbalanced since different assem-
bly actions have different duration and may repeat multi-
ple times within the same assembly. Although it was de-
signed for video action recognition, its challenges are the
core reasons for choosing to extend it to the point cloud
action recognition task. The DFAUST dataset [3] provides
high-resolution 4D scans of human subjects in motion. It
includes 14 action categories with over 100 dynamic scans
of 10 subjects (1:1 male-to-female ratio) with varying body
shapes represented as registrations of aligned meshes, there-
fore an extension to our task is straightforward. One partic-
ularly important feature of this dataset is the GT point corre-
spondences throughout the sequence i.e. it is possible to fol-
low each point’s movement through time. While this dataset
is not as large-scale as others, it provides ground truth infor-
mation (correspondence) that most other collected datasets
do not. Therefore, we extend this dataset to 3D point cloud
action recognition and use it as a testbed for many ablation
studies (see Section 4.4).
3. 3DinAction pipeline
Our 3DinAction pipeline is illustrated in Figure 2. Given
a temporal sequence of 3D point clouds we ﬁrst extract a
set of t-patches (Section 3.1). We then feed the t-patches
into a hierarchical neural network (Section 3.2) to produce
a per-frame high dimensional feature vector representation.
Finally, the feature vectors are fed into a classiﬁer to obtain
per-frame predictions. The proposed approach is prior-free
(no skeleton extraction required) and therefore general and
can be used on different action-understanding datasets.3.1. t-patches
LetS=fxj2R3jj= 1;:::;Ngdenote a 3D point
cloud withNpoints. In the classic (static) setting, a patch
	qis extracted around some query point xq. For example,
the patch 	qmay be constructed by ﬁnding the k-nearest
neighbors of xqinS.
In our temporal setting we are given a sequence of point
cloudsS=fS0;:::;STgcomposed of point cloud frames
St=fxt
jjj= 1;:::;Ntg. Here the superscript tis
used to denote the index of the point cloud in the sequence.
Instead of extracting a patch within a single frame, we allow
patches to extend temporally, and denote them as t-patches.
Deﬁnition 3.1 A t-patchPqis a sequence of point sets in-
dexed by a query point x0
qand jointly moving in time de-
ﬁned by a pointwise mapping function between patches in
consecutive frames. Mathematically, Pq=h	t
qiT
t=0where
	0
qis the initial (static) patch and 	t
q= (	t 1
q)is the
patch at time twhere is a pointwise mapping function.
In practice, it is difﬁcult to ﬁnd a reliable mapping func-
tion. Therefore we propose a simpliﬁed formulation that,
for a given query point x0
q, ﬁrst extracts a patch for the
ﬁrst frame 	0
qand then iteratively extracts corresponding
patches for the next frames (iterating over time), by us-
ing the closest point in the next frame as the new query
point. More formally, we specify  !	0
q,	0
q,  !	t
q=
knn(xt 1
q;St)andxt
q=nn(xt 1
q;St)fort= 1;:::;T .
Hereknn is theknearest neighbor and nnis nearest neigh-
bor. Then, the simpliﬁed t-patch formulation is given by
  !Pq=h  !	t
qjt= 0;:::;Ti (1)
See Figure 3left for an illustration of the t-patch extrac-
tion process. Note that if ground truth correspondence is
available knncan be swapped back to . However, this
does not guarantee improved performance.
Temporal t-patch collapse. The simpliﬁed formulation
of extracting t-patches inherently suffers from the problem
of two or more t-patches collapsing into having the same
points after a certain frame. We call this scenario t-patch
temporal collapse. Temporal collapse can happen when-
everxt
q=xt
pforx0
q6=x0
p. The main issue with tem-
poral collapse is the reduction in point coverage as time
progresses, i.e. the patches covering the last point cloud
have signiﬁcant overlaps and therefore include fewer points
than the ﬁrst frame and so missing vital data. An illustra-
tion of the t-patch collapse problem is available in Figure 3
(right). To mitigate this issue, we propose two solutions.
First, adding small noise to each iteration’s query points, i.e.  !	t
q=knn(xt
q+;St+1)whereN(;2)is a small
Gaussian noise. Second, we propose to construct t-patches
from the ﬁrst to last frame but also in reverse, initializing
19980
Figure 2. 3DinAction pipeline. Given a sequence of point clouds, a set of t-patches is extracted. The t-patches are fed into a neural
network to output an embedding vector. This is done hierarchically until ﬁnally the global t-patch vectors are pooled to get a per-frame
point cloud embedding which is then fed into a classiﬁer to output an action prediction per frame.
Figure 3. t-patch construction and collapse. Illustration of t-
patch construction (left) and collapse (right). Starting from an ori-
gin point x0
qwe ﬁnd the nearest neighbours in the next frame it-
eratively to construct the t-patch subset (non-black points). A col-
lapse happens when two different origin points, x0
qandx0
p, have
the same nearest neighbour at some time step, 	3
p= 	3
qhere.
with	0
qand	T
q, respectively. We name this variation bidi-
rectional t-patches. More formally bidirectional t-patches
are given by,
 !P= [
q  !Pq!
[ [
p  Pp!
(2)
where  Ppis deﬁned similarly to  !Pqbut in the reverse
direction, i.e.,  	T
p,	T
pand  	t
p=knn(xt+1
p;St)fort=T 1;:::; 0. Here, the ﬁnal set of t-patches is com-
posed of an equal number of t-patches from both directions.
3.2. Hierarchical architecture
The proposed architecture is composed of lconsecutive t-
patch modules. Each module receives a point cloud se-
quenceSas input. The sequence is fed into a t-patch ex-
tractor where it undergoes subsampling and t-patch extrac-
tion, forming ~SlandPlrespectively. Then, the t-patches
are fed into t-patch Net, a network that computes a high-
dimensional feature vector flfor each t-patch, parametrized
byl. The subsampled sequence ~Sland its corresponding
t-patch features flare then fed into the next t-patch mod-
ule. These modules form a hierarchy in the sense that each
module receives as input a sparser point cloud with a higher
dimensional feature vector representing each point (encod-
ing both spatial and temporal information). Note that both
the t-patch points and their features are fed into t-patch Net.
t-patch extractor. We ﬁrst subsample the ﬁrst frame
in the point cloud sequence S0using farthest point
sampling (FPS) to form a set of Mquery points
~S0=fx0
j2FPS (S0;M)g. The set ~S0is used to form
the t-patches. Subsampling is required since computing a
t-patch for each point is inefﬁcient and unnecessary due to
overlaps. After subsampling, we extract Mt-patches us-
ing Equation 2 where q2~S0. The extractor operates on
both 3D points and their corresponding features (for mod-
19981
ules deeper in the hierarchy).
Model architecture and t-patch net. The t-patch net-
work computes a high dimensional representation for each
t-patch. The t-patch Net architecture is composed of sev-
eral MLP layers operating on the non-temporal dimensions
(sharing weights across points) followed by a convolutional
layer operating on both the temporal and feature dimen-
sions. Note that the network weights are also shared across
t-patches. The output of each t-patch Net is a vector for
each frame. The ﬁnal frame representation is obtained by
aggregating all of the t-patch features using a max pooling
operation i.e.maxpool Ml(f3). This representation is then
fed into a classiﬁer consisting of three fully connected lay-
ers with temporal smoothing and softmax to output the ﬁnal
action prediction. To train the network we use the same
losses of RGB based approaches [2, 5] which include a per-
frame prediction cross entropy loss and a per-sequence pre-
diction cross entropy loss (summed and weighted evenly)
Ltotal=Lframe +Lseq. For full details see supplemental.
4. Experiments
We evaluate the performance of our approach on three
datasets. The results show that the 3DinAction pipeline out-
performs all baselines in DFAUST [3] and IKEA ASM [2]
and is comparable in MSR-Action 3D [19]. We then con-
duct an ablation study for selecting parameters and t-patch
extraction method showing that adding jitter and bidirec-
tional t-patches is beneﬁcial. Finally, we report time per-
formance and show the tradeoff between performance and
inference time. For more results and experiments, see sup-
plemental material.
Baselines and evaluation metrics. For evaluation, we re-
port several standard metrics [4]: the top1 and top3 frame-
wise accuracy are the de facto standard for action classi-
ﬁcation. We compute it by summing the number of cor-
rectly classiﬁed frames and dividing by the total number of
frames in each video and then averaging over all videos in
the test set. Additionally, since some of the datasets are im-
balanced and may contain different actions for each frame
in a clip, we also report the macro-recall by separately com-
puting recall for each category and then averaging (macro).
Finally, we report the mean average precision (mAP) since
all untrimmed videos contain multiple action labels.
For DFAUST and IKEA ASM we report static methods
PointNet [23], PointNet++[24], and Set Transformer [18]
by applying them on each point cloud frame individually.
Additionally, we report temporal methods like PSTNet [10]
and also implemented a temporal smoothing version of
each static method (PoinNet+TS, Pointnet+++TS, and Set
Transformer+TS respectively) by learning the weights of a
convolutional layer over the temporal dimension. Tempo-
ral smoothing aims to provide a naive baseline for utiliz-
ing temporal information in addition to spatial information.Note that in all experiments, unless otherwise speciﬁed, our
method uses the simpliﬁed formulation with jitter and bidi-
rectional t-patches.
4.1. Experiments on DFAUST dataset
We extend the DFAUST dataset for the task of action recog-
nition and show that the proposed approach outperforms
other methods (see Table 1).
DFAUST dataset [3]. We extended the DFAUST dataset to
our task by subdividing it into clips of 64 frames with train
and test human subjects. The split was constructed so no
subject will appear in both training and test set as well as
guarantee that all actions appear in both. The train and test
sets contain 76 full-length sequences (395 clips, and 25K
frames) and 53 sequences (313 clips, and 20K frames) re-
spectively. Each point cloud frame contains 6890 points.
These points are mesh vertices and therefore the density
varies greatly (e.g., very dense on the face, hands, and feet
and sparser on the legs). For all baselines, we sampled a set
of 1024 points using the farthest point sampling algorithm
to provide a more uniform set of points. For this dataset,
all frames in a clip have the same label. Note that not all
actions are performed by all subjects. For the full action list
and dataset statistics, see the supplemental.
Results. The results, reported in Table 1, show that our pro-
posed approach outperforms all baselines by a large mar-
gin. It also shows that temporal smoothing boosts perfor-
mance signiﬁcantly for all static baselines. Additionally, to
explore the inﬂuence of our simpliﬁed knn-based tempo-
ral point mapping, we used the GT point correspondence
to match the consecutive t-patch origin points and report
the results as another baseline (Ours + GT corr). The re-
sults show that there is a mAP performance gain with GT
correspondence, however, it is limited. Note that in most
datasets, this GT correspondence is not available. Finally,
we also experimented with a Transformer architecture to
process the t-patch learned representations and show that
it does not provide additional performance boost. This may
be attributed to the dataset size.
Insight. We extended the GradCam [27] approach for our
3DinAction pipeline. Using this approach we get a score
per point in each t-patch proportional to its inﬂuence on
classifying the frame to a given target class. The results in
Figure 4show that, as expected, our approach learns mean-
ingful representations since the most prominent regions are
the ones with the informative motion. For example, in the
Jumping jacks action (top row) the hands are most promi-
nent as they are making a large and distinct motion.
4.2. Experiments on IKEA ASM dataset
IKEA ASM dataset [2]. This dataset consists of 371
videos (3M frames) of people assembling IKEA furniture
in different indoor environments. It was collected using a
19982
MethodFrame acc.
top 1 top 3 mAP
3DmFVNet [1] 60.86 87.68 0.7171
PointNet [23] 65.67 86.44 0.7161
PointNet++[24] 58.51 88.28 0.5842
Set Transformer [18] 52.27 81.98 0.6209
PoinNet [23] + TS 74.10 94.00 0.7863
PointNet++[24] + TS 67.88 86.21 0.7563
Set Transformer [18] + TS 62.95 90.33 0.7322
PSTNet [10] 50.70 78.28 0.6490
Ours + GT corr 77.67 95.38 0.8762
Ours + Transformer 77.09 93.7 77.49
Ours 87.26 99.26 0.8616
Table 1. Action recognition results on DFAUST. Reporting
frame-wise accuracy and mean average precision. Ours outper-
forms all baselines by a large margin.
Figure
4.3DinAction GradCAM scores. The proposed 3Din-
Action pipeline learns meaningful representations for prominent
regions. The presented actions are jumping jacks (top row), hips
(middle row), and knees (bottom row). The columns represent
progressing time steps from left to right. Colormap indicates high
GradCAM scores in red and low scores in blue.
Kinect V2 camera and provides camera parameters to re-
construct point clouds in camera coordinates. It provides
action annotation for each frame (33 classes). It is a highly
challenging dataset for two main reasons: (1) It is highlyimbalanced since some actions have a long duration and
occur multiple times in each video (e.g., spin leg) and some
are shorter and sparser (ﬂip tabletop). (2) The assembly mo-
tion includes a lot of self-occlusion as well as subtle move-
ments. The train/test split consists of 254 and 117 full se-
quences respectively. The split is environment-based (i.e.
in the test set there is no environment that appeared in the
training set). The assembly videos have an average of 
2735 frames per video. The point clouds provided in this
dataset are aligned to the camera coordinate frame, posing
a challenge for methods that are sensitive to rotations since
the camera moves between different scans.
Results. The results on the IKEA ASM dataset are reported
in Table 2. The results show that the proposed 3DinAction
pipeline provides a signiﬁcant performance boost over static
approaches and their temporally smooth variants. Addition-
ally, as expected, PointNet and Set Transformer are heavily
affected by the variations in coordinate frames. PointNet++
on the other hand performs better since it uses local coor-
dinate frames for each local region. All methods show an
improved mAP when using the temporally smooth variant
with degradation in frame-wise accuracy due to the dataset
imbalance. For this dataset, the top1 metric is not always in-
dicative of the quality of performance because a high top1 is
directly correlated with many frames classiﬁed as the most
common class. Additionally, we compare to pose-based
methods reported in [2] and show that the proposed ap-
proach also outperforms these baselines. See supplemen-
tary material for confusion matrices.
t-patch intuition and visualization. In Figure 5we visual-
ize the t-patches for the ﬂip table action in the TV Bench as-
sembly. A set of selected t-patches are highlighted in color
demonstrating different types of t-patches and their spatio-
temporal changes. The blue is on the moving TV Bench
assembly, it moves rigidly with the assembly. The maroon
is on the moving person’s arm, it exhibits nonrigid motion
and deformations through time. The tealis on the static ta-
ble surface containing some of the TV Bench’s points in the
ﬁrst frame but remains static when it moves since its origin
query point is on the table. The green is on the static carpet,
remaining approximately the same through time. Note that
the RGB images are for visualization purposes and are not
used in our pipeline.
4.3. Experiments on MSR-Action3D dataset
For this dataset, the task is to predict a single class for a
sequence of frames (unlike the other datasets where a per-
frame prediction is required). To that end, we replace our
classiﬁer with a single fully connected layer and max pooled
the results over the temporal domain (similar to [10]). The
results, reported in Table 3, show that all SoTA methods,
including the proposed approach, exhibit very similar per-
formance. This is mainly attributed to the small scale of the
19983
Figure 5. IKEA ASM example with t-patches. The ﬂip table action for the TV Bench assembly is visualization including the RGB image
(top), and a grayscale 3D point cloud with t-patches (bottom). t-patches are highlighted in color. The blue is on the moving TV Bench
assembly, maroon is on the moving persons arm, tealis on the static table surface, and green is on the colorful static carpet.
MethodFrame acc.
top 1 top 3 macro mAP
PointNet [23] 4.20 19.86 5.76 0.0346
PointNet++[24] 45.97 70.10 29.48 0.1187
Set Transformer [18] 14.96 57.12 13.16 0.0299
PoinNet [23] + TS 6.00 19.48 5.14 0.0804
PointNet++[24] +TS 27.84 60.64 27.72 0.2024
Set Transformer [18] + TS 9.54 36.50 10.74 0.1471
PSTNet [10] 17.94 52.24 17.14 0.2016
Human Pose HCN [2] 39.15 65.37 28.18 0.2232
Human Pose ST-GCN [2] 43.4 66.29 26.54 0.1856
Ours without BD 45.16 72.83 35.06 0.2932
Ours 52.91 75.03 38.84 0.2875
Table 2. Action classiﬁcation on IKEA ASM. The proposed ap-
proach provides a signiﬁcant performance boost over other static
and dynamic approaches, including the temporal smoothing (TS).
dataset and the lack of diversity in the action classes. Fur-
thermore, we witnessed that the main performance gap is
for frames and sequences where the action is indistinguish-
able (e.g., ﬁrst few frames of a sequence where no distin-
guishable action commenced).
4.4. Ablation study
t-patch extraction. We studied the t-patch extraction
method and its effects on action recognition on a noisy ver-
sion of the DFAUST dataset. The results reported in Ta-
ble 4, show the signiﬁcance of the t-patch collapse problem
and the effectiveness of adding small jitter and bidirectional
t-patches to overcome it. In the DFAUST dataset, ﬁnd-
ing the nearest neighbor between frames provides a 96%
correspondence accuracy (small motion between frames).# frames
Method 4 8 12 16 24
PSTNet [10] 81.14 83.50 87.88 89.90 91.20
P4Transformer [8] 80.13 83.17 87.54 89.56 90.94
PST-Transformer [9] 81.14 83.97 88.15 91.98 93.73
Kinet [38] 79.80 83.84 88.53 91.92 93.27
Ours 80.47 86.20 88.22 90.57 92.23
Table 3. MSR-Action3D classiﬁcation results. Reporting classi-
ﬁcation accuracy for clips of different lengths. Results show that
all methods are comparable since this dataset’s scale is limited.
Therefore, in this experiment, we augment the dataset once
by adding small Gaussian noise to each point in the dataset
(= 0:01), decreasing the correspondence accuracy to
62:4% and introducing multiple t-patch collapse instances
as well as increasing the classiﬁcation difﬁculty.
Several variants of the t-patch extraction were explored.
The ﬁrst variation (GT) incorporates the ground truth cor-
respondence into the t-patch extraction. Using this method,
there is no t-patch collapse since there is a one-to-one map-
ping between frames. We expected this to produce an up-
per bound on the performance, however, surprisingly the
results show that this variation is actually inferior to the
proposed t-patch approach. We attribute this to the pro-
posed t-patch extraction inherent augmentation caused by
the downsampling and nearest neighbor point jitter. We
then continue to explore the proposed approaches for deal-
ing with t-patch collapse which include jitter, i.e. adding
small noise to each point before ﬁnding its nearest neighbor
in the next frame, and the bidirectional t-patches that extract
patches both from the ﬁrst to the last frame and from the last
to the ﬁrst frame. The results show that adding jitter is al-
19984
Frame acc.
Data GT Jitter BD top 1 top 3 mAP
clean3 7 7 77.67 95.38 0.8762
7 7 7 74.73 92.14 0.8097
7 3 7 80.49 96.61 0.9023
7 3 3 87.26 99.26 0.8616
noisy3 7 7 76.08 95.50 0.9013
7 7 7 66.74 93.76 0.7626
7 3 7 81.83 98.97 0.9220
7 3 3 80.03 97.57 0.8975
Table 4. t-patch collapse ablation on DFAUST. Exploring adding
(1) GT - ground truth correspondences, (2) jitter - small Gaussian
noise in t-patch construction, and (3) BD - bidirectional t-patches.
Frame acc.
n k top 1 top 3 mAP
256 16 76.96 97.54 0.8430
512 16 80.03 97.57 0.8975
1024 16 77.30 97.88 0.8507
512 8 76.87 96.21 0.7557
512 32 77.91 96.60 0.7453
Table 5. t-patch parameters ablation. Results for the number
of neighboring points in a patch kand number of downsampled
points nshow that the method is robust.
ways beneﬁcial and provides a boost in performance. The
bidirectional t-patches improve accuracy performance sig-
niﬁcantly when the data is clean and are comparable when
the data is noisy. Note that in both dataset variations, the
degradation due to temporal t-patch collapse is low com-
pared to Kinect-based scan data, therefore the bidirectional
beneﬁts are not fully utilized.
t-patch parameters. The core parameters for t-patch ex-
traction are the number of neighbors to extract (k ) and the
number of points to subsample (n). Here there is a trade-
off between complexity and performance i.e. whenkandn
are small, the input to the model is small accordingly but
the overall coverage is reduced and therefore performance
is lower. We explored their inﬂuence on the noisy DFAUST
dataset and report the results in Table 5. The results show
that the method is fairly robust to the selection of these pa-
rameters, producing comparable results for all. The best
performance was obtained for n= 512;k = 16. Surpris-
ingly, the performance slightly degrades when increasing k
andnbeyond these values. This is likely due to the increase
in model size, which easily overﬁts on a dataset of this size.
Time and parameters. We report the time performance
and the number of parameters of several baselines in Ta-
ble 6. The results show the tradeoff between performance
and time, i.e. the temporal approaches exhibit longer pro-Method Time [ms] # parameters
PointNet [23] 64.49 3.5M
PointNet++[24] 23.35 1.5M
PSTNet [10] 185.92 8.3M
Ours t-patch extraction 180.65 0
Ours feature computation 12.50 9.8M
Ours classiﬁer 0.36 1.1M
Ours 193.51 10.9M
Table 6. Time and parameters. Temporal methods have more
parameters and take longer. 3DinAction time is mostly used to
extract t-patches.
cessing times and more parameters while performing better.
For the proposed approach, we break down the timing of in-
dividual components, namely the t-patch extraction, feature
computation, and classiﬁer. The results show that the pro-
posed approach is comparable to PSTNet in time while hav-
ing more parameters. Interestingly, most of the time is used
for extracting the t-patches and not for feature extraction or
classiﬁcation. This is attributed to the farthest point sam-
pling and the sequential knnsearch, both of which could be
further optimized for speed. Note that results are average of
50 runs, each with a batch of 4 and 1024 points per frame.
Limitations. Since the simpliﬁed formulation of t-patch
construction uses knn, it is sensitive to variations in point
densities. A t-patch in a sparse region will occupy a larger
volume than a t-patch in a dense region. We use FPS to mit-
igate this, however, other approaches can be used e.g., using
neighbors in a ﬁxed radius. Another limitation is data with
a very low frame rate or very fast motion since this breaks
the assumption that points in consecutive frames are close
to each other, and will cause inconsistent t-patch motion.
5. Conclusions
We introduced the 3DinAction pipeline, a novel method for
3D point cloud action recognition. It showed that the cre-
ation of temporal patches is beneﬁcial for ﬁnding informa-
tive spatio-temporal point representations. 3DinAction has
demonstrated a performance boost over SoTA methods.
This work opens many interesting future directions of
research. These include trying to learn the t-patch construc-
tion instead of the knnselection, imposing stronger tem-
poral structure based on preexisting knowledge and bias
(e.g., sceneﬂow or tracking), and exploring using multi-
modal inputs with this representation (e.g., RGB or text).
Acknowledgement. This project has received funding from the
European Union’s Horizon 2020 research and innovation pro-
gramme under the Marie Sklodowska-Curie grant agreement No
893465. We also thank the Microsoft for Azure Credits and
NVIDIA Academic Hardware Grant Program for providing high-
speed A5000 GPU.
19985
References
[1] Yizhak Ben-Shabat, Michael Lindenbaum, and Anath
Fischer. 3DMFV: Three-dimensional point cloud clas-
siﬁcation in real-time using convolutional neural net-
works. RAL, 3:3145–3152, 2018. 2,6
[2] Yizhak Ben-Shabat, Xin Yu, Fatemeh Saleh, Dylan
Campbell, Cristian Rodriguez-Opazo, Hongdong Li,
and Stephen Gould. The ikea asm dataset: Under-
standing people assembling furniture through actions,
objects and pose. In Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vi-
sion, pages 847–859, 2021. 3,5,6,7
[3] Federica Bogo, Javier Romero, Gerard Pons-Moll,
and Michael J. Black. Dynamic FAUST: Registering
human bodies in motion. In IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR), July 2017. 3,
5
[4] Fabian Caba Heilbron, Victor Escorcia, Bernard
Ghanem, and Juan Carlos Niebles. Activitynet: A
large-scale video benchmark for human activity un-
derstanding. In Proceedings of the ieee conference on
computer vision and pattern recognition, pages 961–
970, 2015. 5
[5] Joao Carreira and Andrew Zisserman. Quo vadis,
action recognition? a new model and the kinet-
ics dataset. In proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages
6299–6308, 2017. 1,5
[6] Christopher Choy, JunYoung Gwak, and Silvio
Savarese. 4d spatio-temporal convnets: Minkowski
convolutional neural networks. In Proceedings of the
IEEE/CVF conference on computer vision and pattern
recognition, pages 3075–3084, 2019. 2
[7] Haodong Duan, Yue Zhao, Kai Chen, Dahua Lin, and
Bo Dai. Revisiting skeleton-based action recognition.
InProceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 2969–
2978, 2022. 1,3
[8] Hehe Fan, Yi Yang, and Mohan Kankanhalli. Point 4d
transformer networks for spatio-temporal modeling in
point cloud videos. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recogni-
tion, pages 14204–14213, 2021. 2,7
[9] Hehe Fan, Yi Yang, and Mohan Kankanhalli. Point
spatio-temporal transformer networks for point cloud
video modeling. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 45(2):2181–2192, 2022.
2,7
[10] Hehe Fan, Xin Yu, Yuhang Ding, Yi Yang, and Mo-
han Kankanhalli. Pstnet: Point spatio-temporal con-
volution on point cloud sequences. In International
Conference on Learning Representations, 2021. 2,5,
6,7,8[11] Hehe Fan, Xin Yu, Yi Yang, and Mohan Kankanhalli.
Deep hierarchical representation of point cloud videos
via spatio-temporal decomposition. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
44(12):9918–9930, 2021. 2
[12] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik,
and Kaiming He. Slowfast networks for video recog-
nition. In Proceedings of the IEEE/CVF interna-
tional conference on computer vision, pages 6202–
6211, 2019. 1
[13] Ankit Goyal, Hei Law, Bowei Liu, Alejandro Newell,
and Jia Deng. Revisiting point cloud shape classiﬁca-
tion with a simple and effective baseline. In Interna-
tional Conference on Machine Learning, pages 3809–
3820. PMLR, 2021. 2
[14] Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu,
Li Liu, and Mohammed Bennamoun. Deep learning
for 3d point clouds: A survey. PAMI, 2020. 2
[15] Roman Klokov and Victor Lempitsky. Escape from
cells: Deep kd-networks for the recognition of 3d
point cloud models. In Proceedings of the IEEE inter-
national conference on computer vision, pages 863–
872, 2017. 2
[16] Hema Swetha Koppula, Rudhir Gupta, and Ashutosh
Saxena. Learning human activities and object affor-
dances from rgb-d videos. The International Journal
of Robotics Research, 32(8):951–970, 2013. 2
[17] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam
Kosiorek, Seungjin Choi, and Yee Whye Teh.
Set transformer: A framework for attention-based
permutation-invariant neural networks. In Interna-
tional conference on machine learning, pages 3744–
3753. PMLR, 2019. 2
[18] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam
Kosiorek, Seungjin Choi, and Yee Whye Teh.
Set transformer: A framework for attention-based
permutation-invariant neural networks. In Interna-
tional conference on machine learning, pages 3744–
3753. PMLR, 2019. 5,6,7
[19] Wanqing Li, Zhengyou Zhang, and Zicheng Liu. Ac-
tion recognition based on a bag of 3d points. In 2010
IEEE computer society conference on computer vi-
sion and pattern recognition-workshops , pages 9–14.
IEEE, 2010. 3,5
[20] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang
Wang, Ling-Yu Duan, and Alex C Kot. Ntu rgb+ d
120: A large-scale benchmark for 3d human activity
understanding. IEEE transactions on pattern analysis
and machine intelligence, 42(10):2684–2701, 2019. 3
[21] Xingyu Liu, Mengyuan Yan, and Jeannette Bohg. Me-
teornet: Deep learning on dynamic 3d point cloud se-
quences. In Proceedings of the IEEE/CVF Interna-
19986
tional Conference on Computer Vision, pages 9246–
9255, 2019. 2
[22] Daniel Maturana and Sebastian Scherer. V oxnet: A
3d convolutional neural network for real-time object
recognition. In 2015 IEEE/RSJ international confer-
ence on intelligent robots and systems (IROS), pages
922–928. IEEE, 2015. 2
[23] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J
Guibas. Pointnet: Deep learning on point sets for
3d classiﬁcation and segmentation. In IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR),
pages 652–660, 2017. 2,5,6,7,8
[24] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learn-
ing on point sets in a metric space. In NeurIPS, vol-
ume 30, 2017. 2,5,6,7,8
[25] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai,
Hasan Abed Al Kader Hammoud, Mohamed Elho-
seiny, and Bernard Ghanem. Pointnext: Revisiting
pointnet++ with improved training and scaling strate-
gies. arXiv preprint arXiv:2206.04670, 2022. 2
[26] Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-
temporal representation with pseudo-3d residual net-
works. In The IEEE International Conference on
Computer Vision (ICCV), Oct 2017. 1
[27] Ramprasaath R Selvaraju, Michael Cogswell, Ab-
hishek Das, Ramakrishna Vedantam, Devi Parikh, and
Dhruv Batra. Grad-cam: Visual explanations from
deep networks via gradient-based localization. In
Proceedings of the IEEE international conference on
computer vision, pages 618–626, 2017. 5
[28] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang
Wang. Ntu rgb+ d: A large scale dataset for 3d human
activity analysis. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition,
pages 1010–1019, 2016. 3
[29] Yiru Shen, Chen Feng, Yaoqing Yang, and Dong Tian.
Mining point cloud local structures by kernel correla-
tion and graph pooling. In IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR), pages 4548–
4557, 2018. 2
[30] Jaeyong Sung, Colin Ponce, Bart Selman, and
Ashutosh Saxena. Unstructured human activity detec-
tion from rgbd images. In 2012 IEEE international
conference on robotics and automation, pages 842–
849. IEEE, 2012. 2
[31] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Tor-
resani, and Manohar Paluri. Learning spatiotemporal
features with 3d convolutional networks. In Proceed-
ings of the IEEE international conference on computer
vision, pages 4489–4497, 2015. 1
[32] Haiyan Wang, Liang Yang, Xuejian Rong, Jinglun
Feng, and Yingli Tian. Self-supervised 4d spatio-temporal feature learning via order prediction of se-
quential point cloud clips. In Proceedings of the
IEEE/CVF Winter Conference on Applications of
Computer Vision, pages 3762–3771, 2021. 2
[33] Yancheng Wang, Yang Xiao, Fu Xiong, Wenxiang
Jiang, Zhiguo Cao, Joey Tianyi Zhou, and Junsong
Yuan. 3dv: 3d dynamic voxel for action recognition
in depth video. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition,
pages 511–520, 2020. 2
[34] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma,
Michael M Bronstein, and Justin M Solomon. Dy-
namic graph cnn for learning on point clouds. Acm
Transactions On Graphics (tog), 38:1–12, 2019. 2
[35] Siyuan Yang, Jun Liu, Shijian Lu, Meng Hwa Er, and
Alex C Kot. Skeleton cloud colorization for unsu-
pervised 3d action representation learning. In Pro-
ceedings of the IEEE/CVF International Conference
on Computer Vision, pages 13423–13433, 2021. 2
[36] Cheng Zhang, Haocheng Wan, Shengqiang Liu,
Xinyi Shen, and Zizhao Wu. Pvt: Point-voxel
transformer for 3d deep learning. arXiv preprint
arXiv:2108.06076, 2021. 2
[37] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip H.S.
Torr, and Vladlen Koltun. Point transformer. In Pro-
ceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV), pages 16259–16268, Oc-
tober 2021. 2
[38] Jia-Xing Zhong, Kaichen Zhou, Qingyong Hu, Bing
Wang, Niki Trigoni, and Andrew Markham. No pain,
big gain: classify dynamic point cloud sequences with
static models by ﬁtting feature-level space-time sur-
faces. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages
8510–8520, 2022. 2,7
19987
