MAPSeg: Uniﬁed Unsupervised Domain Adaptation for Heterogeneous Medical
Image Segmentation Based on 3D Masked Autoencoding and Pseudo-Labeling
Xuzhe Zhang1y, Yuhao Wu2y, Elsa Angelini1;3, Ang Li4, Jia Guo1, Jerod M. Rasmussen5,
Thomas G. O’Connor6, Pathik D. Wadhwa5, Andrea Parolin Jackowski7,
Hai Li2, Jonathan Posner2, Andrew F. Laine1z, Yun Wang2;8z
1Columbia University2Duke University3T´el´ecom Paris, LTCI, Institut Polytechnique de Paris
4University of Maryland, College Park5University of California, Irvine6University of Rochester
7Universidade Federal de S ˜ao Paulo8Emory University
Abstract
Robust segmentation is critical for deriving quantita-
tive measures from large-scale, multi-center, and longitu-
dinal medical scans. Manually annotating medical scans,
however, is expensive and labor-intensive and may not al-
ways be available in every domain. Unsupervised domain
adaptation (UDA) is a well-studied technique that allevi-
ates this label-scarcity problem by leveraging available la-
bels from another domain. In this study, we introduce
Masked Autoencoding and Pseudo-Labeling Segmentation
(MAPSeg), a uniﬁed UDA framework with great versatil-
ity and superior performance for heterogeneous and vol-
umetric medical image segmentation. To the best of our
knowledge, this is the ﬁrst study that systematically re-
views and develops a framework to tackle four different do-
main shifts in medical image segmentation. More impor-
tantly, MAPSeg is the ﬁrst framework that can be applied
tocentralized ,federated , and test-time UDA while main-
taining comparable performance. We compare MAPSeg
with previous state-of-the-art methods on a private infant
brain MRI dataset and a public cardiac CT-MRI dataset,
and MAPSeg outperforms others by a large margin (10.5
Dice improvement on the private MRI dataset and 5.7 on
the public CT-MRI dataset). MAPSeg poses great practical
value and can be applied to real-world problems. GitHub:
https://github.com/XuzheZ/MAPSeg/ .
1. Introduction
Quantitative measures from medical scans serve as
biomarkers for various types of medical research and clin-
ical practice. For instance, neurodevelopmental studies
yCo-ﬁrst authors.
zCo-senior supervising authors.
(a).
Centralized UDA
SSource Domain
TTarget Domain
Centralized Server
S TTest-time UDA
Source-only Training
S
Test-time FinetuningTFederated UDA
Communication Efficiency
Privacy PerformanceCommunication Efficiency
Privacy PerformanceCommunication Efficiency
Privacy PerformanceCloud Server
Client ServersT TS
...
Pretrained
Encoder3D Multi-scale 
MAECentralized UDA: 
      Centralized and Synchronous MPL            Sec.3.4
Test-time UDA:
      Decentralized and Asynchronous MPL       Sec.3.6Federated UDA:
      Decentralized and Synchronous MPL Sec.3.5
(b).
Cross-modality
Cross-site
Cross-sequence
Cross-ageFigure 1. (a). Illustrations of four different domain shifts in med-
ical images. (b). Overview of different UDA settings and how
MAPSeg can ﬁt into different scenarios.
utilize metrics such as brain volume and cortex thick-
ness/surface area from infant brain magnetic resonance
imaging (MRI) to investigate the early brain development
and neurodevelopmental disorders [2, 11, 23, 56]. There-
fore, robust segmentation of medical images acquired from
large-scale, multi-center, and longitudinal studies is desired,
yet often challenged by the domain shifts across differ-
ent imaging techniques and even within a single modality
(Fig.1a). For example, computed tomography (CT) and
MRI provide markedly different signals for the same struc-
ture ( e.g., cardiac regions, Fig.1a). MRI, a widely adopted
radiation-free imaging technique, bears various types of in-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
5851
herent heterogeneity, including cross-sequence ( e.g., dis-
tinct contrasts for the same tissue in T1/T2 sequences) and
cross-site ( e.g., contrast of the same tissue in the same se-
quence varies with acquisition scanner and setup). More-
over, subject-dependent physiological changes also lead to
domain shift. For example, contrasts of white matter and
grey matter vary while the human brain undergoes signiﬁ-
cant growth and expansion within both cortical and subcor-
tical regions during early postnatal years [19], which con-
tributes to the cross-age domain shift (Fig.1a).
The prevalent heterogeneities in medical images lead to
suboptimal performance when deep neural networks trained
in one source domain are applied to another target domain.
To address this challenge, we introduce a uniﬁed unsuper-
vised domain adaptation (UDA) framework for volumet-
ric and heterogeneous medical image segmentation, named
Masked Autoencoding and Pseudo-Labeling Segmentation
(MAPSeg ). To the best of our knowledge, MAPSeg is the
ﬁrst framework that can be used in centralized ,federated ,
andtest-time UDA for volumetric medical image segmen-
tation while maintaining comparable performance. This
versatility is particularly advantageous in the ﬁeld of med-
ical image segmentation, where data sharing is restricted
and annotations are expensive. While centralized UDA
delivers the best performance in most cases, the strict re-
quirement of co-located data limits its application in multi-
institutional studies due to regulations such as the Health
Insurance Portability and Accountability Act (HIPAA) and
EU General Data Protection Regulation (GDPR) [35, 61].
MAPSeg circumvents this restriction with federated and
test-time adaptation, enabling clinical and research collab-
oration across different medical centers. In contrast, some
previous studies, despite showing promising results in one
scenario, may become infeasible or suffer signiﬁcant perfor-
mance drop in others due to the requirement for co-located
data or synchronous adaptation.
In addition, we conduct extensive experiments on a
private infant brain MRI dataset, which includes expert-
provided annotations, to evaluate MAPSeg on cross-
sequence, cross-site, and cross-age adaptation tasks.
MAPSeg is also compared with previously reported state-
of-the-art (SOTA) results on a public cardiac CT !MRI
segmentation task. MAPSeg consistently outperforms pre-
vious SOTA methods by a large margin (10.5 Dice improve-
ment on the private MRI dataset and 5.7 on the public CT-
MRI dataset in the centralized UDA setting). While previ-
ous studies have separately explored one of the abovemen-
tioned domain shifts [12, 57, 70], they may not generalize
to others. For example, cross-age domain shift is mainly
composed of changes in brain size and contrast, and meth-
ods based on image-translation fail to handle it as they also
change the size when translating data from target domain to
source domain, leading to segmentation errors. We system-atically evaluate MAPSeg across various domain shifts and
imaging modalities, demonstrating its consistent and gener-
alizable effectiveness.
Moreover, in all three UDA settings, MAPSeg does not
rely on any target labels for model validation and selection.
On the contrary, some previous studies on cardiac CT !
MRI segmentation [3, 4] validate and select the best model
using labeled target data, which may not be readily available
in real-world problems. We demonstrate that MAPSeg sur-
passes the previous SOTA results without using any target
label for validation, and the performance drop between us-
ing and without using target label is minor (0.9 mean Dice).
This further justiﬁes its practical value in real-world med-
ical image segmentation tasks. The contributions of this
study are multi-fold:
1. We propose MAPSeg, a uniﬁed UDA framework capa-
ble of handling various domain shifts in medical image
segmentation.
2. MAPSeg is suitable for universal UDA scenarios, sug-
gesting its versatility and practical value for real-world
problems.
3. MAPSeg is extensively evaluated on both private and
public datasets, outperforming previous SOTA methods
by a large margin. We conduct detailed ablation studies
to investigate the impact of each component of MAPSeg.
2. Related Work
2.1. Masked Image Modeling
Masked image modeling (MIM) represents a category of
methods that learn representations from corrupted or in-
complete images [7, 13, 26, 51], and can naturally serve
as a pretext task for self-supervised learning. For exam-
ple, masked autoencoder (MAE) trains an encoder by re-
constructing missing regions from a masked image input
and has demonstrated improved generalization and per-
formance in downstream tasks [24, 36, 38, 59, 65–67].
MAPSeg heavily relies on MIM, leveraging MAE and
masked pseudo-labeling (MPL), to achieve versatile UDA.
2.2. Pseudo-Labeling
Pseudo-labeling facilitates learning from limited or imper-
fect data and is prevalent in semi- and self-supervised learn-
ing [32, 40, 53]. Consistency regularization is widely used
in pseudo-label learning [39, 54], which is a scheme that
forces the model to output consistent prediction for inputs
with different degrees of perturbation ( e.g., weakly- and
strongly-augmented images). Mean Teacher [60], a teacher-
student framework that generates pseudo-labels from the
teacher model (which is a temporal ensembling of the stu-
dent model), is also a common strategy. In this work, we
utilize the teacher model to generate pseudo labels based on
complete images and guide the learning of student model
5852
on masked images.
2.3. Unsupervised Domain Adaptation
Discrepancy minimization, adversarial learning, and
pseudo-labeling are the three main directions explored in
UDA. Previous studies have explored minimizing the dis-
crepancy between source and target domains within differ-
ent spaces, such as input [3, 27, 70], feature [14–17, 25, 47],
and output spaces [33, 63], and they sometimes overlap with
approaches base on adversarial learning as the supervisory
signal to align two distributions may come from statisti-
cal distance metrics [20, 46] or a discriminator model [15,
27, 63]. Meanwhile, self-training with pseudo-label is also
a prevalent technique [72, 73, 76] and has shown signiﬁ-
cant improvement on natural image segmentation [29–31].
Hoyer et al. [31] proposed masked image consistency as a
plug-in to improve previous UDA baselines. In contrast,
MAPSeg leverages the synergy between MAE and MPL,
and employs MPL as a standalone component for various
scenarios.
In this study, we exploit the vanilla pseudo-labeling
with three straightforward yet crucial measures to stabi-
lize the training. We hypothesize that random masking is
an ideal strong perturbation for consistency regularization
in pseudo-labeling, and the model pretrained via MAE can
be efﬁciently adapted to infer semantics of missing regions
from visible patches. This hypothesis is justiﬁed in Sec. 4.3.
In addition, we leverage the anatomical distribution prior in
medical images and make predictions jointly based on local
and global contexts, which also help mitigate the pseudo-
label drifts. We demonstrate the superior performance and
versatility of MAPSeg in different UDA scenarios in the fol-
lowing sections.
2.4. Federated Learning
Federated Learning (FL) is a distributed learning paradigm
that aims to train models on decentralized data [49]. FL has
attracted great attention in the research community in the
last few years and numerous works have focused on the key
challenges raised by FL such as data/system heterogene-
ity [58] and communication/computation efﬁciency [41].
By virtue of keeping privacy-sensitive medical data local,
FL has been adopted for various medical image analysis
tasks [22]. Sheller et al . [55] pioneered FL for brain tu-
mor segmentation on multimodal brain scans in a multi-
institutional collaboration and showed its promising perfor-
mance compared to centralized training. Yang et al. [68]
proposed a federated semi-supervised learning framework
for COVID-19 detection that relaxed the requirement for
all clients to have access to ground truth annotations. Fed-
Mix [64] further alleviated the necessity for all clients to
possess dense pixel-level labels, allowing users with weak
bounding-box labels or even image-level class labels tocollaboratively train a segmentation model. In contrast,
MAPSeg assumes all clients have completely unlabeled
data when extended to federated UDA scenario. Mush-
taqet al . [50] proposed a Federated Alternate Training
(FAT) scheme that leverages both labeled and unlabeled
data silos. It employs mixup [71] and pseudo-labeling to
enable self-supervised learning on the unlabeled partici-
pants. MAPSeg, on the other hand, adopts masked pseudo-
labeling and global-local feature collaboration for adapting
to unlabeled target domains. Yao et al. [69] introduced the
federated multi-target domain adaptation problem and a so-
lution termed DualAdapt . It decouples the local-classiﬁer
adaptation with client-side self-supervised learning from
the feature alignment via server-side mixup and adversar-
ial training. MAPSeg addresses the same federated multi-
target UDA problem, and we compare our results to those
ofFAT andDualAdapt in Sec. 4.3.
2.5. Test-Time UDA
While federated UDA eases the constraint of centralized
data, its learning paradigm still requires synchronous learn-
ing across server and clients. Test-time UDA [5, 10, 18,
25, 37, 44] assumes the unavailability of source-domain
data when adapting to target domains. This assumption sig-
niﬁcantly limits the applicability of methods based on im-
age translation, adversarial learning, and feature distribu-
tion alignment which require simultaneous access to both
source and target data. Gandelsman et al. [18] explored us-
ing MAE retraining during test-time to improve classiﬁca-
tion without employing pseudo labeling. Chen et al . [5]
proposed using prototype and uncertainty estimation for
denoised pseudo labeling of 2D fundus images. Karani
et al. [37] designed a 2D denoising autoencoder to reﬁne
pseudo labels. He et al . [25] employed AE during test-
time to align source and target feature distributions by min-
imizing AE reconstruction loss. We demonstrate that, with
slight performance drop on source domain, MAPSeg can be
extended to test-time UDA with comparable performance to
that of centralized UDA on target domain (Sec. 4.3).
3. Methods
3.1. Preliminary
In this section, we introduce each component of MAPSeg
(Fig.2) and how MAPSeg can serve as a uniﬁed solution
to centralized, federated, and test-time UDA (Fig.1b). We
deploy MAPSeg for domain adaptative 3D segmentation of
heterogeneous medical images and it consists of three com-
ponents: (1) 3D masked multi-scale autoencoding for self-
supervised pre-training, (2) 3D masked pseudo-labeling for
domain adaptive self-training, and (3) global-local feature
collaboration to fuse global and local contexts for the ﬁnal
segmentation task. The hybrid cross-entropy and Dice loss
5853
(Eq.1) is often adopted for regular supervised segmentation
training, and we employ it as the basic component of the
objective functions for MAPSeg:
Lseg(^y;y) = 1
nX
iX
jyi;jlog(^yi;j) 2Py^y+Py+P^y+
(1)
wherendenotes the number of pixels, yi;jand^yi;jrepre-
sent the ground truth label and predicted probability for the
ith pixel to belong to the jth class, and is used to prevent
zero-division.
In the following sections, notations are deﬁned as: xand
yindicate the original image and label of the randomly sam-
pled local patch; XandYrefer to downsampled global scan
and label; the subscripts sandtrefer to the source and tar-
get domains, respectively; the superscript Mindicates the
image is masked ( e.g.,xM
trefers to a masked local patch
from the target domain).
3.2. 3D Multi-Scale Masked Autoencoder (MAE)
In this study, we propose a 3D variant of MAE using a 3D
CNN backbone (Fig.2a). The detailed conﬁguration can be
found in Appendix Sec. 1.1. Training is jointly performed
on two image sources with identical size ( 963voxels): local
patchesxrandomly sampled from the volumetric scan, and
the whole scan downsampled to the same size, denoted as
X. BothxandXare masked before feeding into the MAE:
xis divided into non-overlapping 3D sub-patches with size
83, of which 70% are masked out randomly based on a uni-
form distribution (Fig.2a); The same procedure is applied
toXwith patch size 43since it contains a larger ﬁeld-of-
view (FOV). The masked versions of xandXare denoted
asxMandXM, respectively. We train the MAE encoder
and decoder to reconstruct x=X based onxM=XMusing
mean squared error on the masked-out regions as the objec-
tive function.
3.3. 3D Masked Pseudo-Labeling (MPL)
MPL uses a teacher-student framework which is a standard
strategy in semi-/self-supervised learning [21, 60] to pro-
vide stable pseudo labels on an unlabeled target domain
during training. After MAE pre-training, we keep the MAE
encodergand append a segmentation decoder hto build the
segmentation model f=hg(Fig.2b-c). Given an input
imagexsand labelysfrom the source domain and an in-
put imagextfrom the target domain, the teacher model f
takes as input the target image xtand generates pseudo la-
belsf(xt), with gradient detached. The student model f
is then optimized by minimizing the segmentation loss be-
tween the predictions of xM
t/xM
sandf(xt)/ys, which can
be formulated as:
LMPL =LSeg(f(xM
t);f(xt)) +LSeg(f(xM
s);ys)
(2)whereis the weight of source prediction and set as 0.5.
The teacher model’s parameters are then updated during
training via exponential moving average (EMA) based on
the student model’s parameters [60].
t+1 t+ (1 )t; (3)
wheretandt+ 1 indicate training iterations and is the
EMA update weight. For model initialized from the large-
scale MAE pretraining, we set as 0.999 during the ﬁrst
1,000 steps and 0.9999 afterwards. For model pretrained on
small-scale source and target datasets ( e.g., only dozens of
scans), we set as 0.99 during the ﬁrst 1,000 steps, 0.999
during the next 2,000 steps, and 0.9999 for the remaining
training. The teacher model fis initialized with student
model’s parameters after some warm-up training ( e.g.,
1,000 iterations) on the source-domain data.
3.4. 3D Global-Local Collaboration (GLC)
Directly applying MPL for UDA segmentation with large
domain shift ( e.g., cross-modality/sequence) may lead to
unreliable pseudo-label and disrupt the training. There-
fore, we design a GLC module (Fig.2c) to improve pseudo-
labeling by leveraging the spatial global-local contextual
relations induced by the inherent anatomical distribution
prior in medical images. With the image encoder pre-
trained to extract image features at both local and global
levels during multi-scale MAE, we take advantage of the
global-local contextual relations by concatenating local and
global semantic features in the latent space and make pre-
diction based on the fused features. We differ from previ-
ous study [8] by only applying GLC on the output of the
encoderginstead of all layers to save computation cost and
employing a different regularization to prevent segmenta-
tion decoder from predicting solely based on local features.
In GLC, a binary mask Mis used to indicate the cor-
responding location of the local patch xinside the down-
sampled global volume X. The encoder gtakes as input x
andXand generates the local latent feature loc=g(x)
as well as cropped and resized global latent feature glo=
upsample (Mg(X)), whereindicates cropping g(X)
based onMfollowed by upsampling to match the spatial
size ofloc. Therefore, segmenting a local patch xcan be
rewritten as f(x) =h(locglo), whereis the con-
catenation along channel dimension (Fig.2c). In addition,
fis also trained on downsampled global volume Xwith
LSeg(f(X);Y)), in which the global latent feature g(X)
is duplicated and f(X) =h(g(X)g(X)), to prevent
model from solely relying on local semantic features and
encourage the encoder to extract meaningful semantic fea-
tures from both local and global levels.
We also add a regularization term between the locand
gloto maintain their similarity following [8]. Instead of
theL2regularization used in [8], we maximize the cosine
5854
(a) 3D Multi-scale Masked Autoencoding (MAE, Sec. 3.2)
V olumetric ScanEncoder Decoder
Downsampled
Global ScanSub-volume
Source Domain
Target Domain
GT Label
Pseudo
Label
EMADetach(b) 3D Masked Pseudo Labeling (MPL, Sec. 3.3) (c) 3D Global-local Collaboration (GLC, Sec. 3.4)
Corresponding location of local patch in global scan
Segmentation
DecoderPrediction
of Local Patch
Concatenated
and
Local Patch
Downsampled 
Global ScanLatent 
Representations
ResizeCrop
Cosine SimilarityFigure 2. Components of the proposed MAPSeg framework. (a) 3D multi-scale masked autoencoding. (b) 3D masked pseudo labeling in
source and target domains. (c) 3D Global-local collaboration.
similarity between the locandgloas:
Lcos(x;X) = 1 locglo
max(klock2;kglok2;)(4)
whereis used to prevent zero-division. The loss function
for GLC calculated on the source data is formulated as:
LS
GLC =(LSeg(f(Xs);Ys) +LSeg(f(XM
s);Ys))
+(Lcos(xs;Xs) +Lcos(xM
s;XM
s)) (5)
whereandare the weights of the auxiliary global loss
and cosine similarity, and set as = 0:05and= 0:025
in our experiments. Similarly, the GLC loss is also calcu-
lated on the target data based on pseudo-label f(Xt)and
formulated as:
LT
GLC = 2LSeg(f(XM
t);f(Xt)) + 2Lcos(xM
t;XM
t)
(6)
Therefore, the overall loss function of GLC is:
LGLC =LS
GLC+LT
GLC (7)
With the regular fully-supervised segmentation loss on
source dataLFSS =LSeg(f(xs);ys), whereis de-
ﬁned as in Eq.2, the overall objective function Lfor cen-
tralized UDA is formulated as:
L=LFSS+LMPL +LGLC (8)
It is clear that Eq.8 requires centralized and synchronous ac-
cess to source and target data. In the section 3.5 and 3.6, we
demonstrate how MAPSeg can be adapted to federated (de-
centralized and synchronous access to data) and test-time
(decentralized and asynchronous access to data) UDA sce-
narios.
3.5. Extension to Federated UDA
In reality, labeled source-domain data and unlabeled target-
domain data are often collected at different sites. We con-
sider a practical scenario where a server ( e.g. a major hos-
pital) hosts potentially large amount of both labeled and un-
labeled scans, and distributed clients ( e.g. clinics or imag-
ing sites) possess only unlabeled images. This is an under-
explored scenario as FL typically assumes either fully orpartially labeled data from all clients. We extend MAPSeg
to solve this federated multi-target UDA problem according
to the details in Algorithm 1 of Appendix Sec. 1.2. Specif-
ically, the server updates the student model fby minimiz-
ing the loss for the labeled source-domain data DS:
Ls=(Lseg(f(xs);ys) +Lseg(f(xM
s);ys))
+(Lseg(f(Xs);Ys) +Lseg(f(XM
s);Ys))
+(Lcos(xs;Xs) +Lcos(xM
s;XM
s)) (9)
The clients update the student model fby minimizing the
loss for its own unlabeled target-domain data Dk
T:
Lu=(Lseg(f(xM
t);f(xt)) +Lseg(f(xt);f(xt)))
+(Lseg(f(XM
t);f(Xt)) +Lseg(f(Xt);f(Xt)))
+(Lcos(xt;Xt) +Lcos(xM
t;XM
t)) (10)
Comparing to the centralized UDA loss (Eq.8), we decom-
pose it into two components: fully supervised loss for server
training (Eq.9) and self-supervised loss for client updates
(Eq.10), which avoids the need for centralized data. After
each local update, each client sends the EMA teacher model
parametersto the server for aggregation following typical
federated averaging[49].
3.6. Extension to Test-time UDA
Test-time UDA often involves two separate stages of train-
ing, including the source-only training at one center and the
target-only ﬁnetuning at another site. In the federated UDA
setting, Eq.9 and Eq.10 are jointly used to update the server
model through synchronous federated averaging after each
round. We can further ease the constraint of synchronous
communication between source and target sites by train-
ingfon the source data using Eq.9 for some ( e.g. 1,000)
warm-up steps before distributing the model parameters 
to the target site for initializing the teacher model f. On
the target site, fprovides stable pseudo-labels to guide
the self-supervised training with Eq.10 and is updated by
the EMA of following Eq.3. We ﬁnd that in this asyn-
chronous setting MAPSeg still performs well on the target-
5855
domain data, albeit with a minor performance tradeoff on
the source-domain data (see Tab.3).
3.7. Implementation Details
Model architecture and implementation. We implement
the encoder backbone gusing 3D-ResNet-like CNN. The
segmentation decoder his adapted from DeepLabV3 [6].
The framework is implemented using PyTorch. More de-
tails of the model and the training procedure are provided
in Appendix Sec. 1.1 and 1.2.
Selecting the best model. For choosing the best model dur-
ing training, some studies choose to train for ﬁxed iterations
and use the last checkpoint. On the other hand, some of the
previous UDA studies [3, 4] face a dilemma in selecting
the best model during training by validating against a hold-
out portion of target-domain labels, which is unrealistic as
UDA assumes full absence of target labels. We demonstrate
that MPL not only provides an efﬁcient pathway to domain
adaptative segmentation but also serves as an indicator of
how well the model is being adapted to the target domain.
We validate the model after each epoch and the best model
is selected based on the score: Score =DiceSrc 0:5
LSeg(f(xM
t);f(xt)), where DiceSrcis the Dice score on
source-domain validation set and LSeg(f(xM
t);f(xt))is
the mean ofLSeg(f(xM
t);f(xt))during the last training
epoch. From Eq.1, it is clear that lim^y!yLseg(^y;y) =
 1, therefore, Score has an upper bound of 1:5. We
demonstrate in Tab.4 that the difference between valida-
tion using target labels versus Score is acceptable (81.2 vs.
80.3). Even without accessing target labels for validation,
MAPSeg still surpasses the previous SOTA results that use
target labels for validation. It is worth noting that we only
use target labels for validation in Tab.4 for a fair comparison
with previously reported results; other results presented use
Score for validation by default. For federated and test-time
UDA, Score = LSeg(f(xM
t);f(xt)).
4. Experiments and Results
4.1. Datasets
Brain MRI Datasets. We include 2,421 (1,163 T1w)
brain MRI scans acquired from newborn to toddler in this
study. Among them, 2,306 are unannotated scans dedi-
cated for the 3D multi-scale MAE pretraining. These MRI
scans are acquired from multiple sites with different se-
quence parametrization and scanner types. All scans are
preprocessed with skull stripping [28] and bias-ﬁeld correc-
tion [62]. These MRI brain scans were acquired worldwide,
and detailed descriptions can be found in Appendix Sec.
1.4.
To evaluate cross-sequence/site/age UDA segmentation
for seven subcortical regions ( i.e., hippocampus (HC),
amygdala (AD), caudate (CD), putamen (PT), pallidumTable 1. Performance of centralized UDA on brain MRI segmen-
tation.
Cross-Sequence
MethodDice(%)"
HC AD CD PT PD TM AB Avg
AdvEnt[63] 56.7 52.7 66.7 66.1 61.8 74.1 40.1 59.8
DAFormer[29] 40.5 53.3 62.2 64.7 45.9 61.8 39.9 52.6
HRDA[30] 42.6 37.7 66.5 71.9 0.0 67.6 0.3 40.9
MIC[31] 40.3 47.0 72.5 52.9 0.0 62.1 0.0 39.3
DAR-UNet[70] 61.3 65.2 76.7 75.8 68.1 82.0 48.4 68.2
MAPSeg (Ours) 70.3 73.2 81.4 83.9 76.5 89.6 69.2 77.7
Cross-Site
MethodDice(%)"
HC AD CD PT PD TM AB Avg
AdvEnt[63] 27.1 6.7 21.0 23.1 12.5 36.0 20.5 21.0
DAFormer[29] 40.0 45.8 75.3 70.0 68.4 64.0 51.3 59.3
HRDA[30] 30.9 44.3 80.8 79.8 66.4 83.0 53.4 62.7
MIC[31] 48.1 36.2 67.7 82.8 69.5 66.8 52.3 60.5
DAR-UNet[70] 51.9 43.6 69.8 55.2 55.5 81.2 45.8 57.6
MAPSeg (Ours) 70.0 53.5 85.6 85.4 67.9 88.1 61.4 73.1
Cross-Age
MethodDice(%)"
HC AD CD PT PD TM AB Avg
AdvEnt[63] 58.7 54.1 44.0 63.8 56.9 78.0 30.9 55.2
DAFormer[29] 30.2 65.7 72.7 55.8 38.4 88.8 57.3 58.4
HRDA[30] 48.6 66.6 81.9 67.7 35.7 74.1 56.0 61.5
MIC[31] 61.3 66.0 80.9 73.4 44.3 76.1 51.0 64.7
DAR-UNet[70] 58.8 56.3 64.4 64.5 53.6 82.6 28.6 58.8
MAPSeg (Ours) 75.8 76.7 83.1 71.4 58.2 90.7 70.1 75.2
(PD), thalamus (TM), and accumbens (AB)), our analysis
include manual segmentation of 115 scans. They com-
prise independent subjects from the BCP cohort ( BCP50 )
with private expert segmentation for both T1w and T2w
scans (acquired from 0 to 24 months postnatal age); 5 new-
born scans from the ECHO cohort ( ECHO5 ) with private
expert segmentation; and 10 newborn scans from the M-
CRIB project ( MCRIB10 ) with publicly available segmen-
tation [1].
Cardiac CT-MRI Dataset. Following the previous stud-
ies [3, 4], we include 40 independent scans (20 CT and 20
MRI) of cardiac regions from Multi-Modality Whole Heart
Segmentation (MMWHS) Challenge 2017 dataset [48, 74,
75] with ground truth labels of ascending aorta (AA),
left atrium blood cavity (LAC), left ventricle blood cavity
(LVC), and myocardium of the left ventricle (MYO). Simi-
larly, we apply bias-ﬁeld correction to the MRI scans.
4.2. Dataset Partition
Pretraining. For multi-scale MAE pretraining on brain
MRI scans, we have four models pretrained on different
amounts of data to investigate the inﬂuence of pretrain-
ing data size. The model pretrained on large-scale data
takes advantage of all 2,306 unannotated scans introduced
in Sec. 4.1. Since there is no overlapping with the annotated
scans, the pretrained model can be directly applied to all
downstream UDA tasks ( i.e., cross-site/age/sequence). We
also pretrain the model solely relying on source and target
training data of each task.
For multi-scale MAE pretraining on cardiac CT-MRI
5856
scans, the model is only pretrained on training scans of
source (16 CT scans) and target (16 MRI scans) domains,
following the partition adopted by previous studies.
Cross-Sequence UDA segmentation of brain. The model
is trained on T1w MRI scans (source domain) and tested
on T2w MRI scans (target domain). The BCP50 dataset is
randomly split into two non-overlapping subsets of 25 sub-
jects per each. The model is trained on T1w scans of the
ﬁrst group (source domain 18 scans for training and 7 for
validation) and T2w scans of the second group (target do-
main 15 for training and 10 for testing). The best validation
model is then applied to the T2w testing scans.
Cross-Site UDA segmentation of brain. The model is
trained on a single site ( BCP50 , source domain) and tested
on two other sites ( MCRIB10 andECHO5 , target domains).
Utilizing 50 T2w MRI scans from BCP as the source do-
main, we randomly select 40 scans for training and 10 for
validation. Six scans from MCRIB10 and three scans from
ECHO5 are used for UDA training, and remaining scans are
used for testing.
Cross-Age UDA segmentation of brain. We also conduct
experiments in cross-age segmentation using longitudinal
scans from BCP50 . We set the 24 T2w MRI scans of 12-
24 month-old infant as the source domain and 14 T2w MRI
scans of 0-6 month-old infants as the target domain. For the
source domain, 19 scans are randomly sampled for training
and remaining 5 scans are used for validation. For the target
domain, 8 scans are used for UDA training and 6 scans are
used for testing.
Cross-Modality UDA segmentation of cardiac. For the
cardiac scans, for a fair comparison, we follow the same
partition employed by the previous studies. We set CT as
the source domain and MRI as the target domain, and use
16 CT scans and 16 MRI scans for training, 4 CT scans for
validation, and the remaining 4 MRI scans for testing.
4.3. Results
Centralized Domain Adaptation. To assess MAPSeg’s
performance in different UDA tasks for infant brain MRI
segmentation, we compare it with methods utilizing adver-
sarial entropy minimization [63], image translation [70],
and pseudo-labeling [29–31]. The results are reported
in Tab.1. MAPSeg consistently outperforms its counter-
parts across all tasks. DAR-UNet ranks second in the
cross-sequence task but shows degraded performance in
others, partially due to translation error (details in Ap-
pendix). Among pseudo-labeling approaches, HRDA and
MIC achieve the second best performance in cross-site and
cross-age tasks, respectively. However, they fail to segment
pallidum and accumbens in the cross-sequence task. A ma-
jor challenge here is the small size of subcortical regions
(accounting for approximately 2% of overall voxels) and
signiﬁcant class imbalance ( e.g., thalamus comprises aboutTable 2. Performance of federated UDA on brain MRI segmenta-
tion.
MethodDice(%)"
Cross-Sequence Cross-Site Cross-Age
FAT[50] 27.6 63.8 69.0
DualAdapt[69] 28.4 66.1 54.8
Fed-MAPSeg (ours) 69.9 73.6 71.0
Table 3. Comparison between centralized and test-time UDA on
brain MRI segmentation. Performance of source domain are re-
ported on source validation set.
TaskCentralized UDA Test-time UDASource TargetSource Target Source Target
X-seq 84.0 77.7 79.2 75.9 -4.8 -1.8
X-age 85.8 75.2 84.2 72.9 -1.6 -2.3
X-site 85.7 73.1 79.9 70.3 -5.8 -2.8
0.8% of overall voxels, while accumbens accounts for only
0.03%). This imbalance poses a signiﬁcant challenge for
previous pseudo-labeling methods. Additional visualiza-
tions and discussions are available in Appendix Sec 1.7.
Federated Domain Adaptation. To evaluate our frame-
work in the federated domain adaptation setting, we des-
ignate the labeled source-domain dataset as the server
dataset and the unlabeled target-domain datasets as the
client datasets. In the cross-sequence setting, the 25 T1w
scans of the ﬁrst group are considered as the server dataset,
and the 25 T2w scans of the second group are split roughly
equally into three disjoint client datasets. In the cross-site
setting, the BCP50 is considered as the server dataset, and
theECHO5 andMCRIB10 naturally serve as two different
client datasets. In the cross-age setting, we treat the scans
from the ﬁrst age group as the server dataset, and split the
scans from the second age group equally into two client
datasets.
We compare our Fed-MAPSeg with two other related
work, FAT [50] and DualAdapt [69]. To our best knowl-
edge, there is no direct comparison from the literature that
addresses this challenging federated multi-target unsuper-
vised domain adaptation for 3D medical image segmen-
tation. FAT [50] proposes an alternating training scheme
between the labeled and unlabeled data silos and adopts
a mixup approach to augment the unlabeled input data
for self-supervised learning with pseudo-labels. Dual-
Adapt [69] considers a similar single-source to multi-target
unsupervised domain adaptation setting, except that it only
reports segmentation performance for 2D image datasets
such as the DomainNet [52] and CrossCity [9]. Implemen-
tation details for our Fed-MAPSeg as well as the baselines
are included in Appendix Sec. 1.3. We report our results in
Tab.2. Fed-MAPSeg not only outperforms the two baselines
by a large margin (esp. in the the cross-sequence setting), it
also maintains a fairly close performance compared to the
centralized UDA.
5857
Table 4. Performance of centralized UDA on cardiact CT !MRI
segmentation. Underline indicates the target labels are not used
for validation.
Cardiac CT!MRI segmentation
MethodDice(%)"
AA LAC LVC MYO Avg
PnP-AdaNet[15] 43.7 47.0 77.7 48.6 54.3
SIFA-V1[3] 67.0 60.7 75.1 45.8 62.1
SIFA-V2[4] 65.3 62.3 78.9 47.3 63.4
DAFormer[29] 75.2 59.4 72.0 57.1 65.9
MPSCL[45] 62.8 76.1 80.5 55.1 68.6
MA-UDA[34] 71.0 67.4 77.5 57.1 68.7
SE-ASA[17] 68.3 74.6 81.0 55.9 69.9
FSUDA-V1[42] 62.4 72.1 81.2 66.5 70.6
PUFT[14] 69.3 77.4 83.0 63.6 73.3
SDUDA[12] 72.8 79.3 82.3 64.7 74.8
FSUDA-V2[43] 72.5 78.6 82.6 68.4 75.5
MAPSeg (Ours)78.5 81.8 92.1 68.8 80.3
78.2 81.8 92.9 72.0 81.2
Table 5. Ablation studies of MAPSeg components on cross-
sequence brain MRI segmentation.
Components Performance
MAE GLC MPL Dice(%)"
31.6
X 51.3
X 53.0
X 39.5
X X 59.0
X X 71.3
X X 75.3
X X X 77.7
Test-Time Domain Adaptation. We further extend
MAPSeg to Test-time UDA, and the results for different
tasks are reported in Tab.3. With decentralized data and
asynchronous training, MAPSeg still performs very well in
all tasks, with performance drop smaller than 3% in the
target domain. However, we observe a slightly more per-
formance degradation in the source domain (Tab.3), par-
ticularly in cross-sequence and cross-site tasks, suggesting
that the model suffers from forgetting of the source domain
knowledge during test-time UDA.
Cross-Modality Segmentation of Cardiac. To evaluate
the generalizability of MAPSeg, we further conduct exper-
iment for cross-modality cardiac segmentation and the re-
sults are reported in Tab.4. MAPSeg surpasses all previ-
ously reported results. Results of MRI !CT segmentation
can be found in Appendix Sec. 1.5.
Ablation Studies. To further investigate each compo-
nent of MAPSeg, we conduct ablation studies focusing on
MAE, GLC, MPL, masking ratio, masking patch size of lo-
cal patch, and pretraining data size in the context of cross-
sequence segmentation. From Tab.5, it is clear that directly
applying MPL only brings a minor improvement, suggest-
ing using MPL alone suffers from pseudo-label drifts. By
incorporating GLC to leverage global-local contexts, MPL
Figure 3. Ablation studies on masking ratio, patch size, and pre-
train data. Experiments on masking ratio and patch size are con-
ducted on cross-sequence task.
yields better results. MAE pretraining signiﬁcantly boosts
the performance from using MPL alone (39.5 to 75.3), justi-
fying MAE and MPL are complementary parts in MAPSeg.
Combining MAE, MPL, and GLC together yields the opti-
mal performance.
The impact of masking ratio and local patch size is re-
ported in Fig.3. The masking ratio and patch size remain the
same in MAE and MPL. The results indicate that MAPSeg
is more sensitive to patch size. A patch size of 4 or 16
decreases the performance signiﬁcantly. For the masking
ratio, MAPSeg achieves optimal performance when 70%
of the regions are masked out. Additionally, we evaluate
model’s performance using only source and target training
data (<50 scans) for MAE pretraining, much fewer than the
large-scale pretraining ( >2,000 scans). This suggests that,
even with dozens of scans involved in MAE, MAPSeg still
delivers comparable performance. Another beneﬁt of large-
scale pretraining is its immediate applicability to new target
domains; the pretrained encoders can be directly employed
for MPL, bypassing the need for training from scratch. Ad-
ditional analyses about sensitivity to other hyperparameters
can be found in Appendix Sec. 1.6.
5. Conclusions
In this paper, we introduce the MAPSeg framework as a uni-
ﬁed UDA framework that works on centralized, federated,
and test-time UDA scenarios. We evaluate it under multiple
domain shift and adaptation settings, and it outperforms all
the baselines in all scenarios. We conduct extensive ablation
study to demonstrate the effectiveness of each component.
6. Acknowledgements
This work was supported by NIH grants R00HD103912
(Y .W.), R01HL121270 (R.G.B. & A.F.L.), R01MH121070
(J.P. & A.P.J.), and NSF grant CNS-2112562 (H.L.), as well
as by Duke Science and Technology (Y .W. & H.L.).
5858
References
[1] Bonnie Alexander, Andrea L Murray, Wai Yen Loh, Lil-
lian G Matthews, Chris Adamson, Richard Beare, Jian Chen,
Claire E Kelly, Sandra Rees, Simon K Warﬁeld, et al. A new
neonatal cortical and subcortical brain atlas: the melbourne
children’s regional infant brain (m-crib) atlas. Neuroimage ,
147:841–851, 2017.
[2] Danielle A Baribeau, Annie Dupuis, Tara A Paton, Christo-
pher Hammill, Stephen W Scherer, Russell J Schachar,
Paul D Arnold, Peter Szatmari, Rob Nicolson, Stelios Geor-
giades, et al. Structural neuroimaging correlates of social
deﬁcits are similar in autism spectrum disorder and attention-
deﬁcit/hyperactivity disorder: analysis from the pond net-
work. Translational psychiatry , 9(1):72, 2019.
[3] Cheng Chen, Qi Dou, Hao Chen, Jing Qin, and Pheng-Ann
Heng. Synergistic image and feature adaptation: Towards
cross-modality domain adaptation for medical image seg-
mentation. Proceedings of the AAAI Conference on Artiﬁcial
Intelligence , 33(01):865–872, 2019.
[4] Cheng Chen, Qi Dou, Hao Chen, Jing Qin, and Pheng Ann
Heng. Unsupervised bidirectional cross-modality adaptation
via deeply synergistic image and feature alignment for med-
ical image segmentation. IEEE Transactions on Medical
Imaging , 39(7):2494–2505, 2020.
[5] Cheng Chen, Quande Liu, Yueming Jin, Qi Dou, and Pheng-
Ann Heng. Source-free domain adaptive fundus image seg-
mentation with denoised pseudo-labeling. In Medical Image
Computing and Computer Assisted Intervention–MICCAI
2021: 24th International Conference, Strasbourg, France,
September 27–October 1, 2021, Proceedings, Part V 24 ,
pages 225–235. Springer, 2021.
[6] Liang-Chieh Chen, George Papandreou, Florian Schroff, and
Hartwig Adam. Rethinking atrous convolution for seman-
tic image segmentation. arXiv preprint arXiv:1706.05587 ,
2017.
[7] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-
woo Jun, David Luan, and Ilya Sutskever. Generative pre-
training from pixels. In Proceedings of the 37th Interna-
tional Conference on Machine Learning , pages 1691–1703.
PMLR, 2020.
[8] Wuyang Chen, Ziyu Jiang, Zhangyang Wang, Kexin Cui,
and Xiaoning Qian. Collaborative global-local networks for
memory-efﬁcient segmentation of ultra-high resolution im-
ages. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2019.
[9] Y . Chen, W. Chen, Y . Chen, B. Tsai, Y . Wang, and M.
Sun. No more discrimination: Cross city adaptation of road
scene segmenters. In 2017 IEEE International Conference
on Computer Vision (ICCV) , pages 2011–2020, Los Alami-
tos, CA, USA, 2017. IEEE Computer Society.
[10] Boris Chidlovskii, Stephane Clinchant, and Gabriela Csurka.
Domain adaptation in the absence of source domain data. In
Proceedings of the 22nd ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining , page
451–460, New York, NY , USA, 2016. Association for Com-
puting Machinery.[11] K Guadalupe Cruz, Yi Ning Leow, Nhat Minh Le, Elie
Adam, Raﬁq Huda, and Mriganka Sur. Cortical-subcortical
interactions in goal-directed behavior. Physiological reviews ,
103(1):347–389, 2023.
[12] Zhiming Cui, Changjian Li, Zhixu Du, Nenglun Chen,
Guodong Wei, Runnan Chen, Lei Yang, Dinggang Shen,
and Wenping Wang. Structure-driven unsupervised domain
adaptation for cross-modality cardiac segmentation. IEEE
Transactions on Medical Imaging , 40(12):3604–3616, 2021.
[13] Carl Doersch, Abhinav Gupta, and Alexei A. Efros. Un-
supervised visual representation learning by context predic-
tion. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV) , 2015.
[14] Shunjie Dong, Zixuan Pan, Yu Fu, Dongwei Xu, Kuangyu
Shi, Qianqian Yang, Yiyu Shi, and Cheng Zhuo. Partial un-
balanced feature transport for cross-modality cardiac image
segmentation. IEEE Transactions on Medical Imaging , 42
(6):1758–1773, 2023.
[15] Qi Dou, Cheng Ouyang, Cheng Chen, Hao Chen, Ben
Glocker, Xiahai Zhuang, and Pheng-Ann Heng. Pnp-adanet:
Plug-and-play adversarial domain adaptation network at un-
paired cross-modality cardiac segmentation. IEEE Access ,
7:99065–99076, 2019.
[16] Liang Du, Jingang Tan, Hongye Yang, Jianfeng Feng, Xi-
angyang Xue, Qibao Zheng, Xiaoqing Ye, and Xiaolin
Zhang. Ssf-dan: Separated semantic feature based domain
adaptation network for semantic segmentation. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision (ICCV) , 2019.
[17] Wei Feng, Lie Ju, Lin Wang, Kaimin Song, Xin Zhao, and
Zongyuan Ge. Unsupervised domain adaptation for medi-
cal image segmentation by selective entropy constraints and
adaptive semantic alignment. Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence , 37(1):623–631, 2023.
[18] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei Efros.
Test-time training with masked autoencoders. In Advances
in Neural Information Processing Systems , pages 29374–
29385. Curran Associates, Inc., 2022.
[19] John H Gilmore, Rebecca C Knickmeyer, and Wei Gao.
Imaging structural and functional brain development in early
childhood. Nature Reviews Neuroscience , 19(3):123–137,
2018.
[20] Yves Grandvalet and Yoshua Bengio. Semi-supervised
learning by entropy minimization. In Advances in Neural
Information Processing Systems . MIT Press, 2004.
[21] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin
Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch,
Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-
laghi Azar, et al. Bootstrap your own latent-a new approach
to self-supervised learning. Advances in neural information
processing systems , 33:21271–21284, 2020.
[22] Hao Guan and Mingxia Liu. Federated learning for medical
image analysis: A survey. CoRR , abs/2306.05980, 2023.
[23] Heather Cody Hazlett, Hongbin Gu, Brent C Munsell,
Sun Hyung Kim, Martin Styner, Jason J Wolff, Jed T Elison,
Meghan R Swanson, Hongtu Zhu, Kelly N Botteron, et al.
Early brain development in infants at high risk for autism
spectrum disorder. Nature , 542(7641):348–351, 2017.
5859
[24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
16000–16009, 2022.
[25] Yufan He, Aaron Carass, Lianrui Zuo, Blake E Dewey, and
Jerry L Prince. Autoencoder based self-supervised test-time
adaptation for medical image analysis. Medical image anal-
ysis, 72:102136, 2021.
[26] Olivier Henaff. Data-efﬁcient image recognition with con-
trastive predictive coding. In Proceedings of the 37th In-
ternational Conference on Machine Learning , pages 4182–
4192. PMLR, 2020.
[27] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu,
Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell.
CyCADA: Cycle-consistent adversarial domain adaptation.
InProceedings of the 35th International Conference on Ma-
chine Learning , pages 1989–1998. PMLR, 2018.
[28] Andrew Hoopes, Jocelyn S Mora, Adrian V Dalca, Bruce
Fischl, and Malte Hoffmann. Synthstrip: skull-stripping for
any brain image. NeuroImage , 260:119474, 2022.
[29] Lukas Hoyer, Dengxin Dai, and Luc Van Gool. Daformer:
Improving network architectures and training strategies for
domain-adaptive semantic segmentation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 9924–9935, 2022.
[30] Lukas Hoyer, Dengxin Dai, and Luc Van Gool. Hrda:
Context-aware high-resolution domain-adaptive semantic
segmentation. In Computer Vision – ECCV 2022 , pages 372–
391, Cham, 2022. Springer Nature Switzerland.
[31] Lukas Hoyer, Dengxin Dai, Haoran Wang, and Luc
Van Gool. Mic: Masked image consistency for context-
enhanced domain adaptation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 11721–11732, 2023.
[32] Zijian Hu, Zhengyu Yang, Xuefeng Hu, and Ram Neva-
tia. Simple: Similar pseudo label exploitation for semi-
supervised classiﬁcation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 15099–15108, 2021.
[33] Jiaxing Huang, Shijian Lu, Dayan Guan, and Xiaobing
Zhang. Contextual-relation consistent domain adaptation for
semantic segmentation. In Computer Vision – ECCV 2020 ,
pages 705–722, Cham, 2020. Springer International Publish-
ing.
[34] Wen Ji and Albert C. S. Chung. Unsupervised domain adap-
tation for medical image segmentation using transformer
with meta attention. IEEE Transactions on Medical Imag-
ing, pages 1–1, 2023.
[35] Peter Kairouz, H. Brendan McMahan, Brendan Avent,
Aur´elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista
Bonawitz, Zachary Charles, Graham Cormode, Rachel Cum-
mings, Rafael G. L. D’Oliveira, Hubert Eichner, Salim El
Rouayheb, David Evans, Josh Gardner, Zachary Garrett,
Adri `a Gasc ´on, Badih Ghazi, Phillip B. Gibbons, Marco
Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan
Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi,Gauri Joshi, Mikhail Khodak, Jakub Konecn ´y, Aleksandra
Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancr `ede Le-
point, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard
Nock, Ayfer ¨Ozg¨ur, Rasmus Pagh, Hang Qi, Daniel Ramage,
Ramesh Raskar, Mariana Raykova, Dawn Song, Weikang
Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha
Suresh, Florian Tram `er, Praneeth Vepakomma, Jianyu Wang,
Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and
Sen Zhao. Advances and open problems in federated learn-
ing. Foundations and Trends Rin Machine Learning , 14
(1–2):1–210, 2021.
[36] Ioannis Kakogeorgiou, Spyros Gidaris, Bill Psomas, Yan-
nis Avrithis, Andrei Bursuc, Konstantinos Karantzalos, and
Nikos Komodakis. What to hide from your students:
Attention-guided masked image modeling. In Computer Vi-
sion – ECCV 2022 , pages 300–318, Cham, 2022. Springer
Nature Switzerland.
[37] Neerav Karani, Ertunc Erdil, Krishna Chaitanya, and Ender
Konukoglu. Test-time adaptable neural networks for robust
medical image segmentation. Medical Image Analysis , 68:
101907, 2021.
[38] Xiangwen Kong and Xiangyu Zhang. Understanding masked
image modeling via learning occlusion invariant feature. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , pages 6241–6251,
2023.
[39] Samuli Laine and Timo Aila. Temporal ensembling for
semi-supervised learning. arXiv preprint arXiv:1610.02242 ,
2016.
[40] Dong-Hyun Lee et al. Pseudo-label: The simple and efﬁ-
cient semi-supervised learning method for deep neural net-
works. In Workshop on challenges in representation learn-
ing, ICML , page 896. Atlanta, 2013.
[41] Ang Li, Jingwei Sun, Xiao Zeng, Mi Zhang, Hai Li, and Yi-
ran Chen. Fedmask: Joint computation and communication-
efﬁcient personalized federated learning via heterogeneous
masking. In Proceedings of the 19th ACM Conference on
Embedded Networked Sensor Systems , page 42–55, New
York, NY , USA, 2021. Association for Computing Machin-
ery.
[42] Shaolei Liu, Siqi Yin, Linhao Qu, and Manning Wang. Re-
ducing domain gap in frequency and spatial domain for
cross-modality domain adaptation on medical image seg-
mentation. Proceedings of the AAAI Conference on Artiﬁcial
Intelligence , 37(2):1719–1727, 2023.
[43] Shaolei Liu, Siqi Yin, Linhao Qu, Manning Wang, and Zhi-
jian Song. A structure-aware framework of unsupervised
cross-modality domain adaptation via frequency and spatial
knowledge distillation. IEEE Transactions on Medical Imag-
ing, pages 1–1, 2023.
[44] Yuang Liu, Wei Zhang, and Jun Wang. Source-free domain
adaptation for semantic segmentation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 1215–1224, 2021.
[45] Zhizhe Liu, Zhenfeng Zhu, Shuai Zheng, Yang Liu, Jiayu
Zhou, and Yao Zhao. Margin preserving self-paced con-
trastive learning towards domain adaptation for medical im-
5860
age segmentation. IEEE Journal of Biomedical and Health
Informatics , 26(2):638–647, 2022.
[46] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jor-
dan. Learning transferable features with deep adaptation net-
works. In Proceedings of the 32nd International Conference
on Machine Learning , pages 97–105, Lille, France, 2015.
PMLR.
[47] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I
Jordan. Unsupervised domain adaptation with residual trans-
fer networks. In Advances in Neural Information Processing
Systems . Curran Associates, Inc., 2016.
[48] Xinzhe Luo and Xiahai Zhuang. X-metric: An n-
dimensional information-theoretic framework for groupwise
registration and deep combined computing. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence , 45(7):
9206–9224, 2023.
[49] Brendan McMahan, Eider Moore, Daniel Ramage, Seth
Hampson, and Blaise Aguera y Arcas. Communication-
efﬁcient learning of deep networks from decentralized data.
InArtiﬁcial intelligence and statistics , pages 1273–1282.
PMLR, 2017.
[50] Erum Mushtaq, Yavuz Faruk Bakman, Jie Ding, and Salman
Avestimehr. Federated alternate training (fat): Leveraging
unannotated data silos in federated segmentation for medical
imaging. In 20th IEEE International Symposium on Biomed-
ical Imaging, ISBI 2023, Cartagena, Colombia, April 18-21,
2023 , pages 1–5. IEEE, 2023.
[51] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A. Efros. Context encoders: Feature
learning by inpainting. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
2016.
[52] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate
Saenko, and Bo Wang. Moment matching for multi-source
domain adaptation. In Proceedings of the IEEE International
Conference on Computer Vision , pages 1406–1415, 2019.
[53] Andra Petrovai and Sergiu Nedevschi. Exploiting pseudo
labels in a self-supervised learning framework for im-
proved monocular depth estimation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 1578–1588, 2022.
[54] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen.
Regularization with stochastic transformations and perturba-
tions for deep semi-supervised learning. In Advances in Neu-
ral Information Processing Systems . Curran Associates, Inc.,
2016.
[55] Micah J. Sheller, G. Anthony Reina, Brandon Edwards, Ja-
son Martin, and Spyridon Bakas. Multi-institutional deep
learning modeling without sharing patient data: A feasi-
bility study on brain tumor segmentation. In Brainlesion:
Glioma, Multiple Sclerosis, Stroke and Traumatic Brain In-
juries , pages 92–104, Cham, 2019. Springer International
Publishing.
[56] Mark D Shen, Meghan R Swanson, Jason J Wolff, Jed T Eli-
son, Jessica B Girault, Sun Hyung Kim, Rachel G Smith,
Michael M Graves, Leigh Anne H Weisenfeld, Lisa Flake,
et al. Subcortical brain development in autism and fragile xsyndrome: evidence for dynamic, age-and disorder-speciﬁc
trajectories in infancy. American Journal of Psychiatry , 179
(8):562–572, 2022.
[57] Yue Sun, Kun Gao, Zhengwang Wu, Guannan Li, Xiaopeng
Zong, Zhihao Lei, Ying Wei, Jun Ma, Xiaoping Yang, Xue
Feng, Li Zhao, Trung Le Phan, Jitae Shin, Tao Zhong, Yu
Zhang, Lequan Yu, Caizi Li, Ramesh Basnet, M. Omair
Ahmad, M. N. S. Swamy, Wenao Ma, Qi Dou, Toan Duc
Bui, Camilo Bermudez Noguera, Bennett Landman, Ian H.
Gotlib, Kathryn L. Humphreys, Sarah Shultz, Longchuan Li,
Sijie Niu, Weili Lin, Valerie Jewells, Dinggang Shen, Gang
Li, and Li Wang. Multi-site infant brain segmentation al-
gorithms: The iseg-2019 challenge. IEEE Transactions on
Medical Imaging , 40(5):1363–1376, 2021.
[58] Minxue Tang, Xuefei Ning, Yitu Wang, Jingwei Sun, Yu
Wang, Hai Li, and Yiran Chen. Fedcor: Correlation-based
active client selection strategy for heterogeneous federated
learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pages
10102–10111, 2022.
[59] Yucheng Tang, Dong Yang, Wenqi Li, Holger R. Roth,
Bennett Landman, Daguang Xu, Vishwesh Nath, and Ali
Hatamizadeh. Self-supervised pre-training of swin trans-
formers for 3d medical image analysis. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 20730–20740, 2022.
[60] Antti Tarvainen and Harri Valpola. Mean teachers are better
role models: Weight-averaged consistency targets improve
semi-supervised deep learning results. In Advances in Neu-
ral Information Processing Systems . Curran Associates, Inc.,
2017.
[61] Nguyen Binh Truong, Kai Sun, Siyao Wang, Florian Guitton,
and Yike Guo. Privacy preservation in federated learning: In-
sights from the GDPR perspective. CoRR , abs/2011.05411,
2020.
[62] Nicholas J Tustison, Brian B Avants, Philip A Cook, Yuanjie
Zheng, Alexander Egan, Paul A Yushkevich, and James C
Gee. N4itk: improved n3 bias correction. IEEE transactions
on medical imaging , 29(6):1310–1320, 2010.
[63] T. Vu, H. Jain, M. Bucher, M. Cord, and P. Perez. Advent:
Adversarial entropy minimization for domain adaptation in
semantic segmentation. In 2019 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pages
2512–2521, Los Alamitos, CA, USA, 2019. IEEE Computer
Society.
[64] Jeffry Wicaksana, Zengqiang Yan, Dong Zhang, Xijie
Huang, Huimin Wu, Xin Yang, and Kwang-Ting Cheng.
Fedmix: Mixed supervised federated learning for medical
image segmentation. IEEE Transactions on Medical Imag-
ing, 42(7):1955–1968, 2023.
[65] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin
Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple
framework for masked image modeling. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 9653–9663, 2022.
[66] Zhenda Xie, Zigang Geng, Jingcheng Hu, Zheng Zhang, Han
Hu, and Yue Cao. Revealing the dark secrets of masked im-
age modeling. In Proceedings of the IEEE/CVF Conference
5861
on Computer Vision and Pattern Recognition (CVPR) , pages
14475–14485, 2023.
[67] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Yixuan
Wei, Qi Dai, and Han Hu. On data scaling in masked im-
age modeling. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
10365–10374, 2023.
[68] Dong Yang, Ziyue Xu, Wenqi Li, Andriy Myronenko, Hol-
ger R. Roth, Stephanie Harmon, Sheng Xu, Baris Turkbey,
Evrim Turkbey, Xiaosong Wang, Wentao Zhu, Gianpaolo
Carraﬁello, Francesca Patella, Maurizio Cariati, Hirofumi
Obinata, Hitoshi Mori, Kaku Tamura, Peng An, Bradford J.
Wood, and Daguang Xu. Federated semi-supervised learn-
ing for covid region segmentation in chest ct using multi-
national data from china, italy, japan. Medical Image Analy-
sis, 70:101992, 2021.
[69] Chun-Han Yao, Boqing Gong, Hang Qi, Yin Cui, Yukun
Zhu, and Ming-Hsuan Yang. Federated multi-target domain
adaptation. In 2022 IEEE/CVF Winter Conference on Ap-
plications of Computer Vision (WACV) , pages 1081–1090,
2022.
[70] Kai Yao, Zixian Su, Kaizhu Huang, Xi Yang, Jie Sun, Amir
Hussain, and Frans Coenen. A novel 3d unsupervised do-
main adaptation framework for cross-modality medical im-
age segmentation. IEEE Journal of Biomedical and Health
Informatics , 26(10):4976–4986, 2022.
[71] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and
David Lopez-Paz. mixup: Beyond empirical risk minimiza-
tion. In International Conference on Learning Representa-
tions , 2018.
[72] Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang,
and Fang Wen. Prototypical pseudo label denoising and tar-
get structure learning for domain adaptive semantic segmen-
tation. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 12414–
12424, 2021.
[73] Zhedong Zheng and Yi Yang. Rectifying pseudo label learn-
ing via uncertainty estimation for domain adaptive seman-
tic segmentation. International Journal of Computer Vision ,
129(4):1106–1120, 2021.
[74] Xiahai Zhuang. Multivariate mixture model for myocardial
segmentation combining multi-source images. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence , 41
(12):2933–2946, 2019.
[75] Xiahai Zhuang and Juan Shen. Multi-scale patch and multi-
modality atlases for whole heart segmentation of mri. Medi-
cal Image Analysis , 31:77–87, 2016.
[76] Yang Zou, Zhiding Yu, Xiaofeng Liu, B.V .K. Vijaya Kumar,
and Jinsong Wang. Conﬁdence regularized self-training. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) , 2019.
5862
