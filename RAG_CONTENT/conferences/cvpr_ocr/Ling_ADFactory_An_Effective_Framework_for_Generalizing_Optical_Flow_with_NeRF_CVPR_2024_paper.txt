ADFactory: An Effective Framework for Generalizing Optical Flow with NeRF
Han Ling1Quansen Sun1*Yinghui Sun2‚Ä†Xian Xu2Xingfeng Li1
1Nanjing University of Science and Technology2Southeast University
321106010190@njust.edu.cn
Abstract
A significant challenge facing current optical flow meth-
ods is the difficulty in generalizing them well to the real
world. This is mainly due to the lack of large-scale real-
world datasets, and existing self-supervised methods are
limited by indirect loss and occlusions, resulting in fuzzy
outcomes. To address this challenge, we introduce a novel
optical flow training framework: automatic data factory
(ADF). ADF only requires RGB images as input to effec-
tively train the optical flow network on the target data do-
main. Specifically, we use advanced NeRF technology to
reconstruct scenes from photo groups collected by a monoc-
ular camera, and then calculate optical flow labels between
camera pose pairs based on the rendering results. To elimi-
nate erroneous labels caused by defects in the scene recon-
structed by NeRF , we screened the generated labels from
multiple aspects, such as optical flow matching accuracy,
radiation field confidence, and depth consistency. The fil-
tered labels can be directly used for network supervision.
Experimentally, the generalization ability of ADF on KITTI
surpasses existing self-supervised optical flow and monoc-
ular scene flow algorithms. In addition, ADF achieves
impressive results in real-world zero-point generalization
evaluations and surpasses most supervised methods1.
1. Introduction
Optical flow aims to estimate the motion of each pixel on
the image plane between two consecutive frames. It is one
of the oldest problems in computer vision and has essen-
tial applications in autonomous driving[10, 38, 39], video
understanding[11], and human action recognition[1, 18]. In
recent years, deep learning methods[31, 33] have become
the mainstream solution for optical flow and achieved per-
formance far superior to traditional handcrafted methods [6]
in benchmark tests[9, 27].
Compared to traditional methods, deep optical flow
methods can learn robust matching features and contextual
*Corresponding author
‚Ä†Corresponding author
1Code: https://github.com/HanLingsgjk/UnifiedGeneralizationfeatures from large-scale data, as well as infer the opti-
cal flow of occluded parts[14]. However, acquiring real-
world optical flow datasets is challenging. It requires addi-
tional sensors such as LiDAR, GPS, IMU, and much man-
ual annotations[27], which has been one of the significant
limitations of applying optical flow methods[44]. Although
recent works in synthetic datasets and self-supervised learn-
ing have alleviated this limitation to some extent, they are
still subject to constraints in domain transferability[9, 25,
26], lighting variations, and occlusions[15, 17].
In this era where data is the gold mine, massive training
datasets are a necessary factor for the success of powerful
models such as ChatGPT[8] and SAM[16]. However, how
to quickly and cost-effectively mine high-quality training
data from the ‚Äògold mines‚Äô, and fully tap into the enormous
potential of deep networks remains a challenge to be ex-
plored [48]. This paper proposes a novel automated data
factory (ADF) to address this issue. ADF utilizes only pho-
tos taken by a single camera as input, and does not require
manual intervention or ground-truth labels. It can quickly
generate massive datasets at a meagre cost, training various
geometric matching tasks. Specifically, ADF mainly con-
sists of two parts: data generation and data filtering.
Data generation: We build a data generator based on
the latest radiation rendering technology (NeRF)[5]. Firstly,
based on NeRF, a high-resolution scene is reconstructed
from photos taken by a monocular camera. Then, by ren-
dering a depth map and randomly generating new camera
pose pairs, we can calculate the corresponding optical flow
results between poses. In addition, to alleviate the prob-
lem that NeRF can only reconstruct static scenes with high
quality, we also added randomly shaped foreground floaters
based on the Bezier curve.
Data filtering: In NeRF reconstruction, it is inevitable
to encounter areas of reconstruction failure, resulting in fur-
ther generation of erroneous training data. Therefore, con-
structing a comprehensive data filtering mechanism is es-
sential. Before us, the only evaluation metric used was
ambient occlusion (AO) [34, 35]. However, AO can only
be used to check whether floating objects are between the
viewpoint and the target depth, which cannot be widely used
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
20591
Scale-flow (Ours) Scale-flow MDFlow GMFlow
Self-supervised : Trained by our 
automatic data factory (ADF)Supervised: Trained by existing 
synthetic and real-world datasetsSelf-supervised: Trained by 
indirect photometric loss
ùë•
ùë¶Color mapping of 
optical flowFigure 1. Zero-Shot Generalization Results in Real World. On the top is Scale-flow using our automatic data factory (ADF) scheme
to estimate optical flow results in real-world scenarios. Below is a comparison with existing advanced supervised and self-supervised
methods. ADF shows unprecedented accuracy and clarity. Moreover, it requires no manual intervention and only utilizes photos captured
by a monocular camera to train optical flow tasks.
for NeRF methods and downstream tasks. In this paper, we
have designed practical evaluation metrics such as recon-
struction stability, depth consistency, and optical flow vi-
sual consistency based on the characteristics of the NeRF
method. Experiments have shown that the combination of
these metrics improves the overall performance of the opti-
cal flow method by at least 20%.
The filtered optical flow results can be directly used to
train complex geometric matching tasks[21, 22, 32, 42‚Äì44].
As shown in Fig.1, the optical flow model trained on ADF-
generated data achieves stunning real-world generalization
effects. We believe that ADF is an important step in mak-
ing optical flow methods widely applicable in real-world
scenarios and a stepping stone to building matching large
models. In this paper, we only used about 300 collected
scenarios, which has made the ADF-supervised optical flow
methods superior in the real-world zero-shot generalization
to self-supervised and supervised models trained on exist-
ing datasets [9, 25, 27].
Our contributions can be summarized as follows:
‚Ä¢ A new mode for self-supervised learning of optical flow
algorithms based on NeRF.
‚Ä¢ A fully automatic optical flow label generation pipeline
ADF, which only requires images taken by a monocular
camera as input to generate massive high quality datasets.
‚Ä¢ We collected and created an optical flow dataset ADF58
containing over 300 scenes and 58800 images. Experi-
mental results demonstrate that ADF58 achieves superior
zero-shot generalization capability in optical flow task
compared to existing datasets.
2. Preliminaries
In this section, we mainly introduce the components of the
NeRF method used in this paper and analyze how to render
the most critical depth values.
We use the Zip-NeRF[5] to represent 3D scenes and im-age generation. This is currently one of the most advanced
NeRF methods in terms of performance, which can achieve
anti-aliasing and robust reconstruction results. Secondly, it
uses hash voxel networks to store color and density features.
This explicit expression not only accelerates convergence
speed, but also naturally conforms to multi-view geometric
structure consistency better than implicit expression.
Scene representation: Zip-NeRF‚Äôs neural radiation
field is composed of the hash voxel field in instant-
NGP[29], mapping the distance interval Ti= [ti, ti+1)on
a rayr(t) =o+tdinto a set of color features c‚àà[0,1]3
and volume density œÉ‚ààR+, where oanddare the origin
and direction of the ray, respectively, tis the distance from
the origin along the ray direction. It can be formulated as:
(œÉi,ci) = MLP Œ∏(Œ≥(r(Ti))),‚àÄTi‚ààt (1)
Here, MLP Œ∏is an shallow MLP with parameters Œ∏,r(Ti)
maps the sampling points in the conical frustum corre-
sponding to Ti,Œ≥is the coding function, and tis the set
of all intervals on the ray that are included in the rendering.
Volume rendering: With these volume densities and
colors, we can calculate the RGB values Ccorresponding
to the rays based on the volume rendering formula:
C(r ,t)=X
iwici (2)
wi=
1‚àíe‚àíœÉi(ti+1‚àíti)
Ei (3)
Ei=e‚àíP
j<iœÉj(tj+1‚àítj)(4)
Where Eiis the accumulated transmittance along the ray
from viewpoint to ti, 
1‚àíe‚àíœÉi(ti+1‚àíti)
is the opacity at
Ti. By construction, the sum of the weights wion a ray is
always less than or equal to 1, and when the ray points to an
opaque surface, the sum of the weights wiapproaches 1 [4].
20592
ABC
Rendering RGB
Rendering Depth
Midpoint Depth0.50.5
 A1
ùë°‡Ø°‡Æ∫
B0.51
ùë°‡Ø°‡Æª
C
Ray Distance ùê≠1
ùë°‡Ø°‡Æº‡∑çùë§‡Øú
‡ØúNormalized ùë§Figure 2. Midpoint Depth vs. Rendering Depth. Left: Rendered
RGB image and two different depths. Right: The weights won
three different rays: well-trained ray B, ray C with incorrect sur-
faces, and ray A with potential multiple surfaces. We found that
rendering depth in A and C is clearly incorrect, as the weighted
depth of the wrong surface interfered with the final result, espe-
cially when there were incorrect weights at the far end of the ray.
And the midpoint depth can reduce the interference of these erro-
neous surfaces.
Usually, the scene depth calculation formula corresponding
to a ray can be written as:
z(r,t)=X
iwitmid, t mid=(ti+ti+1)
2(5)
Here, z(r,t)is the rendering depth of ray r. As the sum
ofwialways approaches 1. We can also consider Eq (5) as
calculating the expected value, and wiis the probability of
the existence of an object surface at position ti.
Midpoint depth: Although most NeRF-based genera-
tive methods[34, 35, 46, 47] use the depth described in Eq
(5), it is unreasonable in Zip-NeRF. As mentioned in the
previous chapter, its definition is the expected distance be-
tween all surfaces on the ray. So, when there are multiple
surfaces or incorrect surfaces on a ray (due to insufficient
NeRF reconstruction), it is highly likely to generate errors.
In this paper, we use midpoint depth as the rendering
depth output, which is defined as follows:
z(r,t)=tn,nX
iwi= 0.5 (6)
Due to the anti-aliasing characteristics of Zip-NeRF, the
sum of weight won a well-trained ray should appear as a
smoothed step function, as shown in Fig.2 B. At this point,
the midpoint distance is close to the peak of w(Locationwith the highest potential surface probability). As shown in
Fig.2 A and C, when there are potential multiple surfaces
or incorrect surfaces on the ray, the midpoint depth can also
avoid small errors and approach the correct depth.
3. Method
Fig.3 shows our data factory‚Äôs workflow, which is divided
into two stages. Firstly, we construct a neural radiation field
based on static scene multi-view images users collected, and
generate preliminary optical flow results. Then filter out the
unqualified results. The filtered optical flow results can be
directly used for network training.
3.1. Optical flow generation
After giving the camera pose pair PiandPj, we can ob-
tain the RGB images Ii, Ijand the corresponding depth
Zi, Zjbased on NeRF rendering. Among them Ii(u, v) =
C(r ,t),Zi(u, v) =z(r,t),(u, v)is the pixel plane co-
ordinate corresponding to ray (r,t),Pis the 3√ó4trans-
formation matrix from the camera coordinate system to the
world coordinate system.
Based on the Ziand the camera pose Pi, Pj, we can cal-
culate the position of the pixel pi(u, v) = ( u, v,1)in the
first frame to corresponding pi‚Ä≤(u, v) = ( u‚Ä≤, v‚Ä≤,1)in the
second frame, as follows:
pi‚Ä≤(u, v) =K‚àí1Zi(u, v)Kpi(u, v)PiPj‚àí1
Zi‚Ä≤(u, v)(7)
fi‚Üíj=pi‚Ä≤‚àípi (8)
Where fi‚Üíjis the optical flow between frames iandj,
Kis the camera‚Äôs internal reference matrix to convert pixel
points into the camera coordinate system, Zi‚Ä≤(u, v)is the
depth of piin frame jcalculated by projection. For ease of
expression, we have moved it to the right side.
Dynamic foreground: Because Zip-NeRF can only
model static backgrounds. To make up for this weakness,
we use Bezier curves to form randomly shaped slices and
make perspective changes between two frames to enhance
the foreground part of the generated data, as shown in Fig.3.
In addition to the optical flow results, based on Ziand
Zj, we can also calculate scene flow datasets. For more
information on datasets and foreground masks, please refer
tosupplementary materials .
3.2. Data Credibility
During shooting, the environment may change and the
shooting perspective may not be perfect, which can result
in artifacts in the data generated by NeRF. To address this
issue, we require a reliable filtering mechanism to screen
out unqualified data. In previous studies[35], researchers
20593
Collect photos from multiple perspectives
COLMAP
camera posesZip-NeRF
trainingùëÉ‡Øú ùëÉ‡Øù
The camera pose 
pairs to be rendered
Rendering
Add random dynamic 
foreground
Optical flow ùëì‡Øú‚Üí‡Øù
Also have ùëç‡Øú, ùëç‡Øù,ùëÄ‡Ø¢‡Øñ‡Øñ ‚Ä¶ 1. Preliminary Generation of Training Data
2. Data Filtering
ùêº‡Øù
Optical flow ùëì‡Øú‚Üí‡ØùWrapping
ùêº‡Øú·á≤
ùêº‡Øú
A. Structural 
Similarity Error
ùëç‡ØúùëÉ‡Øú ùëÉ‡Øù
Wrapping and get coordinates 
from the perspective of ùëó
 ùëç‡Øù‡Øú·á≤
Zip-NeRF
rendering by 
coordinatesùëç‡Øù‡Øú·á≤‡µÜùëç‡Øú·á≤
ùëç‡Øù‡Øú·á≤‡µÖùëç‡Øú·á≤
C. Deep ConsistenceZip-NeRF
rendering by 
camera poseRenderingùëÉ‡Øú
B. Radiation Field 
Confidence‚ã∞
The weight distribution 
of each ray 
ùëç‡Øú·á≤ùêº‡Øú
ùêº‡ØùFigure 3. Data Factory Workflow. On the top: Training NeRF using a sequence of photos collected by a monocular camera and rendering
preliminary optical flow labels. Below: Conduct confidence checks on the rendered data and labels, eliminating non-conforming parts.
have used ambient occlusion (AO) to measure the credibil-
ity of generated data. The AO can be calculated for a ray
using the following formula:
AO =m‚àí1X
iwi, t m=Zi(u, v) (9)
The meaning of AO is the probability of the presence of
surfaces from the rendering depth Zi(u, v)to the viewpoint.
However, AOis not applicable in anti-aliasing methods be-
cause whas been smoothed, so even ordinary rays will have
highAOvalues. In this article, we only use AOto calculate
occlusion mask Moccbetween two frames. At this time, we
useZi‚Ä≤in Eq.7 to replace the rendering depth. The specific
calculation process is as follows:
AOocc=n‚àí1X
jwj, t n=Zi‚Ä≤(u, v) (10)
Mocc(u, v) =(
0ifAOocc> th
1otherwise(11)
Where AOoccis the probability of the presence of a sur-
face between the viewpoint and the depth Zi‚Ä≤(u, v),Mocc
is the occlusion mask between two frames, and in experi-
ments, this generally set to 0.3.
In the following section, we propose different credibility
masks to filter the generated data of NeRF from three per-
spectives: optical flow matching accuracy, NeRF rendering
stability, and depth consistency. Combining these indicators
ùëÄ‡Ø¢‡Øñ‡Øñ ùëÄ‡Ø¶‡Ø¶‡Øú‡Ø†  ùë§ùëñùë°‚Ñé  ùëÄ‡Ø¢‡Øñ‡ØñùëÄ‡Ø¶‡Ø¶‡Øú‡Ø† ùêº‡Øú·á≤ùêº‡Øú ùêº‡Øù
0 1 0.25 0.5 0.75Color mapping of 
filter mask ùëÄ‡Ø¶‡Ø¶‡Øú‡Ø†Figure 4. SSIM Inspection of Optical Flow Labels. We got
Ii‚Ä≤by warping IjtoIibased on optical flow, and calculated the
Mssim between Ii‚Ä≤andIi, where the darker the color, the smaller
the error. Notice that the part in the red box has artifacts due to
occlusion. When using Mssim as the evaluation standard, we fil-
tered out these artifacts using an occlusion mask Moccto reflect
the matching quality of the optical flow truly.
can find generated data labels that conform to geometric re-
lationships.
Optical flow matching accuracy: As shown in Fig.4,
to evaluate the accuracy of generating optical flow fi‚Üíj,
we used the optical flow results to warp the second frame
image Ijto the first frame to obtain Ii‚Ä≤. Then we calculated
the corresponding structural similarity masks:
Mssim= 1‚àíSSIM( Ii, Ii‚Ä≤) (12)
20594
‡∑çùë§‡Øú
‡ØúNormalized ùë§
0.9
0.90.1
0.1
0 1 0.25 0.5 0.75Rendered RGB
BinaryùëÄ‡Øñ‡Ø¢‡Ø°‡Øô   ·à∫ùëÄ‡Øñ‡Ø¢‡Ø°‡Øô‡µê0.3·àªùë°‡Øü‡Æ∫ùë°‡Øõ‡Æ∫
ùë°‡Øü‡Æªùë°‡Øõ‡ÆªA
A
BB
Ray Distance ùê≠ùëÄ‡Øñ‡Ø¢‡Ø°‡ØôFigure 5. Visualization of Radiation Field Confidence. Left:
RGB image rendered by neural field, Mconf confidence mask, and
binarized Mconf. Right: A represents the weight distribution of
rays in dark areas without texture, and B represents the weight
distribution of rays in rich texture areas. The Mconf we proposed
easily segmented those areas where the radiation field is difficult
to reconstruct based on the ray weight distribution.
where SSIM [40] is the Structural Similarity Index Mea-
sure. Mssim can also be used as an evaluation indicator of
optical flow. In this case, it needs to be used in conjunction
with the Moccmask to eliminate artifacts caused by occlu-
sion, as shown in Fig.4.
Radiation field confidence: In NeRF generative ap-
plication scenarios, the quality of neural field reconstruc-
tion directly determines the method‚Äôs overall performance.
However, the only available AO indicators are not widely
applicable for various methods and scenarios. In order to
overcome the above difficulties, we propose a new metric
in this section to measure the quality of neural field recon-
struction, called radiation field confidence (RFC).
As shown in Fig.5B, when the neural field is well trained,
the weight distribution of rays should be clustered at a cen-
tral point, and the weight integration should appear as a step
function, indicating that the radiation field has an exact sur-
face on that ray. Moreover, we observed that such rays of-
ten correspond to areas with rich textures and appropriate
brightness. On the contrary, it is difficult for the radiation
field to learn the correct surface position for areas that are
too dark or textureless.
Based on the above observations, we proposed Mconf
(RFC) to describe the reconstruction quality of the neural
field, as follows:
Mconf(u, v) =th‚àítl
th+tl(13)lX
iwi=thlow,hX
iwi=thhigh (14)
Where thlowandthhigh are the boundary thresholds,
which are set to 0.1 and 0.9 in the experiments of this pa-
per,tlandthare the depths in the ray direction when the
boundary threshold is reached.
Mconf revealed the dispersion of weight distribution on
a ray, with a range of (0,1). The larger the value of Mconf,
the lower the reconstruction quality of the radiation field.
As shown in Fig.5, Mconf can effectively identify the parts
of where the radiation field is challenging to reconstruct.
Geometry consistency: Inspired by Bian[7], we calcu-
lated the geometric consistency mask of rendering depth
under multiple perspectives. This is a core component for
scene flow and depth estimation tasks. Although this paper
focuses on optical flow applications, we hope the data fac-
tory proposed can be further applied to a broader range of
downstream tasks.
Specifically, as shown in Fig.3, given the rendering depth
Ziof the first frame and the pose Pi, Pjbetween the two
perspectives. We can calculate the coordinates pi‚Ä≤and depth
mapZi‚Ä≤inPjcorresponding to Pibased on Eq.7, and then
use NeRF rendering to obtain the depth value Zi‚Ä≤
jof the cor-
responding coordinates pi‚Ä≤. The final geometric consistency
is as follows:
Mdc=Zi‚Ä≤
j‚àíZi‚Ä≤
Zi‚Ä≤
j+Zi‚Ä≤(15)
Where the value range of Mdcis (0,1). The larger the
value of Mdc, the lower the depth consistency.
3.3. NeRF-Supervised Training
After calculating the optical flow value fi‚Üíjbetween two
frames, we filter the optical flow results based on the mask
Mconf,Mssim, and Mdc. In this paper, we simply set a
threshold to filter out those unqualified points, and the final
optical flow label fgt
i‚Üíjused for training is as follows:
fgt
i‚Üíj=Mth1
confMth2
ssimMth3
dcfi‚Üíj (16)
Mth=(
1ifM < th
0otherwise(17)
Points with fgt
i‚Üíjequal to zero do not participate in the
loss calculation.
4. Experimental
In this section, we first introduce the implementation details
of the dataset and experiment and then discuss the experi-
mental results.
20595
Table 1. Ablation Study. The best results in ablation studies are bolded. Flepeis the average end-to-end optical flow error, Flallis the
optical flow outlier rate (errors greater than 3 pixels or greater than 5% are considered outliers)
K15 Midd-A K12
Method Training Data Mconf Mssim MdcMfFlepe Fall Flepe Fall Flepe Fall
RAFT[33]4.9 17.34 0.313 0.222 2.04 9.18
ADF15 (ours)‚úì 4.62 15.71 0.368 0.184 1.81 7.45
‚úì ‚úì 4.42 18.28 0.201 0.186 1.77 9.59
‚úì ‚úì 4.4 15.7 0.257 0.129 1.72 7.76
‚úì ‚úì 4.19 14.83 0.3 0.231 1.73 8.12
‚úì ‚úì ‚úì ‚úì 4.19 14.15 0.28 0.175 1.62 6.8
Sintel[9] 8.5 18.31 0.23 0.22 2.38 8.01
Flyingthings3D[25] 8.2 23.42 0.24 0.17 3.14 14.16
FlyingChairs2[25] 10.87 36.86 0.46 0.79 4.2 26
Scale-flow[20] ADF58 (ours) ‚úì ‚úì ‚úì ‚úì 3.88 13.36 0.218 0.122 1.59 6.97
RAFT ADF58 (ours) ‚úì ‚úì ‚úì ‚úì 4.17 13.9 0.223 0.138 1.59 6.43
Generate dataset We have gathered and created a se-
ries of 300 scenes, featuring both indoor still life and out-
door courtyard scenes and ensured complete stillness. For
each scenario, we train for 30,000 steps with a batch size
of 10240 based on Zip-NeRF. Afterwards, we generate ap-
proximately 200 camera pose pairs per scene (maintaining
orientation towards the center of the scene) to produce an
optical flow dataset and corresponding masks. In the end,
the training set consists of a total of 58800 image pairs
(ADF58).
Deep optical flow training: We use RAFT[33] as the
main architecture for ablation experiments and optical flow
experiments evaluation, as it is the backbone network of
most advanced methods. Hence, the evaluation on it has
better representativeness. Of course, we also trained a
stronger normalized scene flow baseline Scale-flow[20] to
evaluate the effectiveness of ADF in more advanced meth-
ods. All methods are trained with a batch size of 6 and
a crop size of 384√ó768, where we trained 200k iterations
for RAFT and 300k iterations for Scale-flow. All training
is done from scratch without using any synthesized dataset
for pre-training.
Datasets: In the evaluation, we used 194 images from
KITTI 2012 training set[12] (K12), 3 static scenes from
Middlebury[3] (Midd-A), and 200 images from KITTI 2015
training set[27] (K15). Around 30k KITTI raw data (Kr)
and 4k KITTI multi-frame (Km) images were used for train-
ing in other self-supervised methods[17, 24]. In addition,
commonly used optical flow pre-trained synthetic datasets
Sintel[9] (S), Flyingthings3D[25] (T), and Flyingchair2[25]
(C2) were also included in the comparison. To facilitate the
ablation experiment, we separated a subset of 15600 images
(ADF15) from the complete ADF58.4.1. Ablation Study
In this section, we first studied the effectiveness of vari-
ous masks proposed in this article. Then, we compared
the datasets ADF58 and ADF15 produced by our data fac-
tory with the commonly used synthetic dataset. Except for
ADF58, all datasets only train 100K iterations.
Use of masks: We simply set a threshold to filter out
unqualified points. Specifically, points with Mconf greater
than 0.3 will be excluded, those with Mssim greater than
0.1 will be excluded, and those with Mdcgreater than 0.01
will be excluded. In our experiments, the number of added
motion foreground Mfis 2.
Mask analysis: Tab.1 shows the experimental results
of training RAFT using different masks. Firstly, we found
that the training effect after using masks always improves in
most cases. Specifically, after adding Mfas a sports fore-
ground, the performance in the KITTI dataset was signifi-
cantly improved, but there was a decrease in Midd-A. This
is easy to understand because Mfenhances the network‚Äôs
perception of motion prospects, while Midd-A is all static
scenes. Secondly, we found that Mconf had the most sig-
nificant performance improvement among all masks, which
proves that our motivation to evaluate dataset quality from
the perspective of reconstruction stability is effective. Fi-
nally, RAFT achieved optimal results after using all masks,
which proves that the various masks we proposed are com-
patible. Although this article only proves its effectiveness
by applying masks through simple thresholding, it is neces-
sary to use masks more finely in subsequent work, such as
calculating the credibility of points by combining masks in
different tasks.
Comparison with synthetic datasets: For fairness,
we compared ADF15 with the commonly used synthetic
20596
datasets Sintel, Flyingthings3D and Flyingchairs2. All of
them were trained using the same training parameters based
on RAFT. Tab.1 shows the comparison results, and ADF15
is far ahead in almost all indicators, thanks to its data do-
main being closer to real world scenario. It has been proven
that our NeRF data factory scheme can serve as a substi-
tute and supplement to existing synthetic datasets for pre-
training matching tasks.
Number of scenes and stronger baseline: We at-
tempted to use more scenarios for training, and as expected,
ADF58 improved all metrics compared to ADF15, repre-
senting a key advantage of our work: ADF can quickly
expand the photos taken casually into the dataset and en-
hance the method‚Äôs performance. In addition, we also tested
a stronger normalized scene flow method, Scale-flow, and
achieved better performance than RAFT, which proves that
our method is also effective on a stronger baseline.
Table 2. Evaluation of Normalized Scene Flow. Above is the
depth method that only uses monocular images, and below is the
traditional method that uses stereo images.
Method Training data FlepeMid error Time/s
Scale-flow ADF58(ours) 3.46 149.83 0.2
Expansion[45] Kr 6.56 348 0.2
SMMSF[13] Km+Kr 7.92 288.99 0.063
OSF[28] K15 - 115 300
PRSM[37] K15 - 124 3000
4.2. Normalized Scene Flow
This section compares the Scale-flow trained on ADF58
with other monocular self-supervised scene flow meth-
ods. For fairness, we use the same evaluation criteria as
Yang[45]. Specifically, using 40 images in K15 for evalua-
tion, the metrics include depth change rate œÑand end-to-end
optical flow error Flepe. The calculation of depth change
rate loss Mid error is as follows:
Mid error =||log(œÑ)‚àílog(œÑGT)||1¬∑104(18)
Where œÑ=z2/z1,z1is the depth of point in the first
frame, and z2is the depth of the corresponding point in the
second frame.
We first compared our method with the state-of-the-art
monocular self-supervised scene flow method SMMSF[13].
Our Scale-flow with ADF58 outperformed them by a large
margin (149.83 vs. 288.99). Because too complex mod-
els often have greater training difficulty, the architecture of
general unsupervised methods tends to favour models with
small parameter numbers, which is also why SMMSF is
smaller and faster. Unlike them, our data factory solution
can fully train more complex and more extensive models.We also compare with traditional methods[28, 37] that
use stereo images as input. They decompose the image into
rigid blocks, and iteratively optimize the 3D motion of the
rigid blocks based on the rigid assumption and regulariza-
tion terms. As shown in Tab.2, although traditional methods
use additional sensors, we have achieved accuracy close to
theirs, and the speed is much faster (0.2s vs. 300s).
4.3. Self-supervised Optical Flow
In this section, we compare with several state-of-the-
art self-supervised optical flow methods: MDFlow[17],
UPFlow[24], and UFlow[15]. Because MDFlow only pro-
vides fast mode weights, the MDFlow in our paper defaults
to MDFlow-fast.
Table 3. Evaluation of Self-supervised Optical Flow. Top is
the zero-shot generalization result, and below is the result of fine-
tuning using target domain data.
K15 K12
Method Training data Flepe FallFlepe Fall
MDFlow[17] Sintel 10.05 23.12 3.49 12.17
Scale-flow ADF58(ours) 3.88 13.36 1.59 6.97
RAFT ADF58(ours) 4.17 13.9 1.59 6.43
UFlow[15] Km+Kr 2.71 - 1.68 -
MDFlow[17] Km 4.44 12.3 1.83 6.8
UPFlow[24] Km+Kr 2.45 - 1.27 -
SMMSF[13] Km+Kr 6.04 18.81 - -
As shown in the upper part of Tab.3, our method has
achieved absolute advantages over the self-supervised al-
gorithm in the case of zero-shot generalization. Even after
the self-supervised ways are fine-tuned in the target domain,
the generalization performance of ADF is still competitive
compared with these fine-tuned methods, as shown in the
lower part of Tab.3.
It is not difficult to find that ADF performs weakly in the
KITTI evaluation of driving scenarios compared to the stun-
ning performance in daily scenarios. There are two main
reasons for this: firstly, for safety reasons, we did not shoot
road scenes and add in ADF58; secondly, due to the inher-
ent defects of Zip-NeRF, it is impossible to reconstruct the
reflective and transparent surfaces in cars (see more in sup-
plementary materials ).
4.4. Zero-shot Generalization in Real Word
In this section, we test our algorithm in real-world daily sce-
narios, which is also the original intention of this work: to
enable the massive number of excellent optical flow algo-
rithms and scene flow algorithms to be truly applied to our
daily production and life.
20597
Evaluation method: As there is currently no large-scale
real-world optical flow dataset, we have adopted an indirect
evaluation method to evaluate optical flow accuracy based
on SSIM and photometric loss. Assuming fis the optical
flow between frames IiandIj, in order to evaluate its ac-
curacy, we project Ijframes onto Iiframes based on fto
obtain Ii‚Ä≤.The specific calculation method is as follows:
Sloss=Mocc¬∑Mssim (19)
Ploss=Mocc¬∑ |Ii‚àíIi‚Ä≤| (20)
Where SlossandPlossrepresent SSIM loss and photo-
metric loss, respectively, while Moccrepresents the occlu-
sion mask between two frames, used to eliminate artifacts
caused by occlusion.
Evaluation dataset: We chose DA VIS[30] as the eval-
uation dataset because it contains many dynamic prospects
and a wide distribution of data domains. Moreover, it has
a foreground mask that can be used to calculate Mocc. Fi-
nally, approximately 3,400 image pairs were included in the
evaluation.
Table 4. Evaluation of Zero-shot Generalization in Real-world. fg
andbgrepresent the foreground and background.
Method Training data Sfg
lossSbg
lossPfg
lossPbg
loss
Scale-flow ADF58(ours) 0.21 0.09 11.56 4.13
RAFT ADF58(ours) 0.22 0.09 11.69 4.19
GMFlow[44] ADF58(ours) 0.26 0.11 12.73 4.82
Scale-flow KITTI-ft 0.26 0.10 13.62 4.55
RAFT KITTI-ft 0.29 0.11 15.15 4.82
GMFlow KITTI-ft 0.31 0.15 15.81 5.97
We mainly compare the performance differences under
different supervision methods. As shown in Tab.4, KITTI-ft
represents the commonly used fine-tuning training process,
which involves pre-training on extensive synthetic data such
as Sintel and Flyingthings3D, then fine-tuning on the real-
world dataset KITTI. ADF58 represents our self-supervised
data factory solution. RAFT, Scale-flow and GMFlow ex-
hibits a significant difference in generalization ability under
different training methods ( 0.22 vs. 0.29; 0.21 vs. 0.26;
0.26 vs. 0.31). This proves that our data factory scheme
has better generalization performance in real-world scenar-
ios compared to the previous supervised training mode.
We have visualized the comparison between ADF, su-
pervised, and self-supervised methods in the demo folder
of the supplementary materials . Welcome to check.
5. Limitation and Improvement
In this section, we discuss the limitations and potential im-
provement methods of our method.
Scale_flow  NDF  
00021_sfg_0.8265_sbg_0.9420
Scale_flow KITTI00021_sfg_0.8223_sbg_0.9355
Scale-flow (ADF58) Scale-flow (KITTI-ft)
MDFlowImage 1 Image 2Figure 6. Optical Flow and Object Flow: . Observing the parts
in coloured boxes, the methods trained by ADF are more faithful
to the flow of light, while existing supervised methods (KITTI-ft)
often tend to learn object flows with semantics. In addition, due to
the lack of training samples, supervised methods often have poorer
generalization in the real world.
Optical flow and object flow: Observing the car in
Fig.6, we found that our method has more details compared
to the supervised learning method. For example, the rotat-
ing wheels in the red box, neglected transparent glass in the
orange box, and shadow in the black box. This is because
our method is based on NeRF, so the results are more faith-
ful to the light flow. In existing supervised learning[27], op-
tical flow is defined as object flow, which tracks the surface
of objects in space and introduces semantic information to
erase some details, such as rotating car wheels and trans-
parent windows. In future work, we believe that we can try
to bring our data factory closer to supervised learning by
introducing a big semantic model[19].
NeRF: The Zip-NeRF used in this paper cannot recon-
struct reflective surfaces such as glass and water surfaces,
and is limited to static scenes. These limitations directly
prevent us from creating fine-tuning scenarios based on
KITTI raw data. Fortunately, thanks to the rapid develop-
ment of NeRF technology[2, 23, 36, 41, 46], this problem
has been partially resolved.
6. Conclusion
We propose a groundbreaking optical flow training method:
automated data factory (ADF), which utilizes scenes gen-
erated by NeRF to train deep optical flow networks with-
out requiring manual annotation or expensive equipment.
ADF can generate almost infinite high-quality optical flow
datasets from ordinary monocular cameras, and directly use
them for network training. This approach results in excel-
lent real-world zero generalization performance, surpassing
most self-supervised and supervised methods. Our work
can help many excellent optical flow algorithms perform
better in the real world, making them applicable to produc-
tion and life!
Acknowledgements This research was supported by the
National Natural Science Foundation of China (Grant nos.
62372235).
20598
References
[1] Muhammad Haseeb Arshad, Muhammad Bilal, and Abdul-
lah Gani. Human activity recognition: Review, taxonomy
and open challenges. Sensors , 22(17):6463, 2022. 1
[2] Benjamin Attal, Eliot Laidlaw, Aaron Gokaslan, Changil
Kim, Christian Richardt, James Tompkin, and Matthew
O‚ÄôToole. T ¬®orf: Time-of-flight radiance fields for dynamic
scene view synthesis. Advances in neural information pro-
cessing systems , 34:26289‚Äì26301, 2021. 8
[3] Simon Baker, Daniel Scharstein, JP Lewis, Stefan Roth,
Michael J Black, and Richard Szeliski. A database and eval-
uation methodology for optical flow. International journal
of computer vision (IJCV) , 92:1‚Äì31, 2011. 6
[4] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P
Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded
anti-aliased neural radiance fields. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition (CVPR) , pages 5470‚Äì5479, 2022. 2
[5] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P
Srinivasan, and Peter Hedman. Zip-nerf: Anti-
aliased grid-based neural radiance fields. arXiv preprint
arXiv:2304.06706 , 2023. 1, 2
[6] Steven S. Beauchemin and John L. Barron. The computation
of optical flow. ACM computing surveys (CSUR) , 27(3):433‚Äì
466, 1995. 1
[7] Jia-Wang Bian, Huangying Zhan, Naiyan Wang, Zhichao
Li, Le Zhang, Chunhua Shen, Ming-Ming Cheng, and Ian
Reid. Unsupervised scale-consistent depth learning from
video. International Journal of Computer Vision (IJCV) , 129
(9):2548‚Äì2564, 2021. 5
[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in neural in-
formation processing systems , 33:1877‚Äì1901, 2020. 1
[9] Daniel J Butler, Jonas Wulff, Garrett B Stanley, and
Michael J Black. A naturalistic open source movie for op-
tical flow evaluation. In European Conference on Computer
Vision (ECCV) , pages 611‚Äì625, 2012. 1, 2, 6
[10] Linda Capito, Umit Ozguner, and Keith Redmill. Optical
flow based visual potential field for autonomous driving. In
IEEE Intelligent Vehicles Symposium (IV) , pages 885‚Äì891.
IEEE, 2020. 1
[11] Lijie Fan, Wenbing Huang, Chuang Gan, Stefano Ermon,
Boqing Gong, and Junzhou Huang. End-to-end learning of
motion representation for video understanding. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 6016‚Äì6025, 2018. 1
[12] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition (CVPR) , pages 3354‚Äì
3361, 2012. 6
[13] Junhwa Hur and Stefan Roth. Self-supervised multi-frame
monocular scene flow. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
2684‚Äì2694, 2021. 7[14] Shihao Jiang, Dylan Campbell, Yao Lu, Hongdong Li, and
Richard Hartley. Learning to estimate hidden motions with
global motion aggregation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 9772‚Äì9781, 2021. 1
[15] Rico Jonschkowski, Austin Stone, Jonathan T Barron, Ariel
Gordon, Kurt Konolige, and Anelia Angelova. What matters
in unsupervised optical flow. In European Conference on
Computer Vision (ECCV) , pages 557‚Äì572, 2020. 1, 7
[16] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. arXiv preprint arXiv:2304.02643 , 2023. 1
[17] Lingtong Kong and Jie Yang. Mdflow: Unsupervised opti-
cal flow learning by reliable mutual knowledge distillation.
IEEE Transactions on Circuits and Systems for Video Tech-
nology (TCSVT) , 33(2):677‚Äì688, 2022. 1, 6, 7
[18] Kanokphan Lertniphonphan, Supavadee Aramvith, and Tha-
narat H Chalidabhongse. Human action recognition us-
ing direction histograms of optical flow. In International
Symposium on Communications & Information Technologies
(ISCIT) , pages 574‚Äì579, 2011. 1
[19] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu,
Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao.
Semantic-sam: Segment and recognize anything at any gran-
ularity. arXiv preprint arXiv:2307.04767 , 2023. 8
[20] Han Ling, Quansen Sun, Zhenwen Ren, Yazhou Liu,
Hongyuan Wang, and Zichen Wang. Scale-flow: Estimat-
ing 3d motion from video. In Proceedings of the 30th ACM
International Conference on Multimedia (ACMMM) , pages
6530‚Äì6538, 2022. 6
[21] Han Ling, Yinghui Sun, Quansen Sun, and Zhenwen Ren.
Learning optical expansion from scale matching. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition (CVPR) , pages 5445‚Äì5454, 2023. 2
[22] Haisong Liu, Tao Lu, Yihui Xu, Jia Liu, Wenjie Li, and Lijun
Chen. Camliflow: Bidirectional camera-lidar fusion for joint
optical flow and scene flow estimation. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition (CVPR) , pages 5791‚Äì5801, 2022. 2
[23] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and
Deva Ramanan. Dynamic 3d gaussians: Tracking
by persistent dynamic view synthesis. arXiv preprint
arXiv:2308.09713 , 2023. 8
[24] Kunming Luo, Chuan Wang, Shuaicheng Liu, Haoqiang Fan,
Jue Wang, and Jian Sun. Upflow: Upsampling pyramid
for unsupervised optical flow learning. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition (CVPR) , pages 1045‚Äì1054, 2021. 6, 7
[25] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,
Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A
large dataset to train convolutional networks for disparity,
optical flow, and scene flow estimation. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition (CVPR) , pages 4040‚Äì4048, 2016. 1, 2, 6
[26] Lukas Mehl, Jenny Schmalfuss, Azin Jahedi, Yaroslava Nali-
vayko, and Andr ¬¥es Bruhn. Spring: A high-resolution high-
detail dataset and benchmark for scene flow, optical flow and
20599
stereo. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition (CVPR) , pages 4981‚Äì
4991, 2023. 1
[27] Moritz Menze, Christian Heipke, and Andreas Geiger. Joint
3d estimation of vehicles and scene flow. In ISPRS Workshop
on Image Sequence Analysis (ISA) , pages 427‚Äì434, 2015. 1,
2, 6, 8
[28] Moritz Menze, Christian Heipke, and Andreas Geiger. Ob-
ject scene flow. ISPRS Journal of Photogrammetry and Re-
mote Sensing , 140:60‚Äì76, 2018. 7
[29] Thomas M ¬®uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a mul-
tiresolution hash encoding. ACM Transactions on Graphics
(ToG) , 41(4):1‚Äì15, 2022. 2
[30] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-
bel¬¥aez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017
davis challenge on video object segmentation. arXiv preprint
arXiv:1704.00675 , 2017. 8
[31] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.
Pwc-net: Cnns for optical flow using pyramid, warping, and
cost volume. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
8934‚Äì8943, 2018. 1
[32] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and
Xiaowei Zhou. Loftr: Detector-free local feature matching
with transformers. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition (CVPR) ,
pages 8922‚Äì8931, 2021. 2
[33] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field
transforms for optical flow. In European Conference on
Computer Vision (ECCV) , pages 402‚Äì419, 2020. 1, 6
[34] Fabio Tosi, Alessio Tonioni, Daniele De Gregorio, and Mat-
teo Poggi. Nerf-supervised deep stereo. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition (CVPR) , pages 855‚Äì866, 2023. 1, 3
[35] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt,
and Federico Tombari. Sparf: Neural radiance fields from
sparse and noisy poses. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition
(CVPR) , pages 4190‚Äì4200, 2023. 1, 3
[36] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,
Jonathan T Barron, and Pratul P Srinivasan. Ref-nerf: Struc-
tured view-dependent appearance for neural radiance fields.
InProceedings of the IEEE/CVF conference on computer
vision and pattern recognition (CVPR) , pages 5481‚Äì5490,
2022. 8
[37] Christoph V ogel, Konrad Schindler, and Stefan Roth. 3d
scene flow estimation with a piecewise rigid scene model.
International Journal of Computer Vision , 115:1‚Äì28, 2015.
7
[38] Hengli Wang, Peide Cai, Rui Fan, Yuxiang Sun, and Ming
Liu. End-to-end interactive prediction and planning with op-
tical flow distillation for autonomous driving. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 2229‚Äì2238, 2021. 1
[39] Hengli Wang, Peide Cai, Yuxiang Sun, Lujia Wang, and
Ming Liu. Learning interpretable end-to-end vision-basedmotion planning for autonomous driving with optical flow
distillation. In IEEE International Conference on Robotics
and Automation (ICRA) , pages 13731‚Äì13737. IEEE, 2021. 1
[40] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE transactions on image processing ,
13(4):600‚Äì612, 2004. 5
[41] Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, For-
rester Cole, and Cengiz Oztireli. DÀÜ 2nerf: Self-supervised
decoupling of dynamic and static objects from a monocular
video. Advances in Neural Information Processing Systems ,
35:32653‚Äì32666, 2022. 8
[42] Gangwei Xu, Xianqi Wang, Xiaohuan Ding, and Xin Yang.
Iterative geometry encoding volume for stereo matching. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 21919‚Äì21928, 2023. 2
[43] Gangwei Xu, Yun Wang, Junda Cheng, Jinhui Tang, and Xin
Yang. Accurate and efficient stereo matching via attention
concatenation volume. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence , 2023.
[44] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and
Dacheng Tao. Gmflow: Learning optical flow via global
matching. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition (CVPR) , pages
8121‚Äì8130, 2022. 1, 2, 8
[45] Gengshan Yang and Deva Ramanan. Upgrading optical flow
to 3d scene flow through optical expansion. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition (CVPR) , pages 1334‚Äì1343, 2020. 7
[46] Jiawei Yang, Boris Ivanovic, Or Litany, Xinshuo Weng, Se-
ung Wook Kim, Boyi Li, Tong Che, Danfei Xu, Sanja Fidler,
Marco Pavone, and Yue Wang. Emernerf: Emergent spatial-
temporal scene decomposition via self-supervision. arXiv
preprint arXiv:2311.02077 , 2023. 3, 8
[47] Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Mani-
vasagam, Wei-Chiu Ma, Anqi Joyce Yang, and Raquel Urta-
sun. Unisim: A neural closed-loop sensor simulator. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition (CVPR) , pages 1389‚Äì1399, 2023. 3
[48] Zhiqi Zhang, Pan Ji, Nitin Bansal, Changjiang Cai, Qingan
Yan, Xiangyu Xu, and Yi Xu. Clip-flow: Contrastive learn-
ing by {s}emi-supervised iterative pseudo {l}abeling for
optical flow estimation. arXiv preprint arXiv:2210.14383 ,
2022. 1
20600
