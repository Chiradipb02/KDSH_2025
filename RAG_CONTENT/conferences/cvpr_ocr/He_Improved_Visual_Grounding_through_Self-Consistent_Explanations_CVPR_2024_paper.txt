Improved Visual Grounding through Self-Consistent Explanations
Ruozhen He1Paola Cascante-Bonilla1Ziyan Yang1Alexander C. Berg2Vicente Ordonez1
1Rice University2University of California, Irvine
catherine.he@rice.edu, pc51@rice.edu, zy47@rice.edu, bergac@uci.edu, vicenteor@rice.edu
Abstract
Vision-and-language models trained to match images
with text can be combined with visual explanation meth-ods to point to the locations of speciÔ¨Åc objects in an im-
age. Our work shows that the localization ‚Äì‚Äúgrounding‚Äù‚Äì
abilities of these models can be further improved by Ô¨Åne-tuning for self-consistent visual explanations. We proposea strategy for augmenting existing text-image datasets withparaphrases using a large language model, and SelfEQ,a weakly-supervised strategy on visual explanation maps
for paraphrases that encourages self-consistency. SpeciÔ¨Å-
cally, for an input textual phrase, we attempt to generatea paraphrase and Ô¨Ånetune the model so that the phraseand paraphrase map to the same region in the image. Weposit that this both expands the vocabulary that the modelis able to handle, and improves the quality of the object
locations highlighted by gradient-based visual explanation
methods (e.g. GradCAM). We demonstrate that SelfEQ im-proves performance on Flickr30k, ReferIt, and RefCOCO+over a strong baseline method and several prior works. Par-ticularly, comparing to other methods that do not use any
type of box annotations, we obtain 84.07% on Flickr30k
(an absolute improvement of 4.69%), 67.40% on ReferIt (anabsolute improvement of 7.68%), and 75.10%, 55.49% onRefCOCO+ test sets A and B respectively (an absolute im-provement of 3.74% on average).
1. Introduction
Vision-and-language models that are trained to associateimages with text have shown to be effective for many tasksand benchmarks [ 21,27,31,41], including object detec-
tion [ 18,54] and image segmentation [ 15,36,50]. Since
these models are typically trained with in-the-wild datafrom the web, they can handle a wide range of vocab-ulary for objects as long as they are well represented in
the training data. These models are often remarkably ac-curate [ 16,22,28,47] even without tuning them to per-
form well in any particular downstream task [ 14,32,52].
The ALBEF model [ 27] was particularly capable of visual

  

  





	
Figure 1. Previous models can localize the word frisbee , but strug-
gle with equivalent but more uncommon referents such as disc .A
model tuned with our proposed SelfEQ objective encourages con-sistent visual explanations for paraphrased prompts and performswell on both examples. SelfEQ not only enables a larger workingvocabulary but also improves overall localization performance.
‚Äúgrounding‚Äù ‚Äì or in other words ‚Äì the ability to localize
objects in images by simply using it in conjunction with avisual explanation method such as GradCAM [ 43]. This ca-
pability is particularly remarkable given that this model wasonly supervised with images and text but no object locationannotations of any type.
In order to improve the ability of vision-and-language
models to perform localization, many methods have in-corporated further Ô¨Ånetuning with either box or segment
annotations, or rely on pretrained object detectors or boxproposal networks [ 5,12,19,23,30,51]. Our work in-
stead aims to improve the localization capabilities of modelstrained only on image-text pairs through weak supervision.But, how can we improve the ability of a model to localizeobjects without access to object location annotations? Con-sider the example in Figure 1where a model is tasked with
pointing to the location of the object frisbee in this image.
The baseline model succeeds at Ô¨Ånding the object but is un-successful at locating the object when prompted with the
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
13095
equivalent but more generic name disc . Regardless of the
ability for the base model to Ô¨Ånd either of these, the visualexplanations for these two prompts should be the same sincethe query refers to the very same object in both cases. Ourwork exploits this property by Ô¨Årst generating paraphrasesusing a large language model and then proposing a weakly-supervised Self -consistency EQuivalence Tuning (SelfEQ)
objective that encourages consistent visual explanations be-tween paraphrased input text pairs that refer to the sameobject or region in a given image.
Given a base pre-trained vision-and-language model
purely trained on image-text pairs such as ALBEF [ 27],
SelfEQ tunes the model so that for a given input imageand text pair, the visual attention map extracted using Grad-CAM [ 43] produces a similar visual attention map when
provided with the same image and a text paraphrase. Fig-ure 2provides an overview of our method. Another contri-
bution of our work consists in exploiting a large language
model (LLM) to automatically generate paraphrases for ex-isting datasets such as Visual Genome [ 26] that contains
textual descriptions of individual objects and regions, orMS-COCO [ 33] and CC3M [ 46] that contain global im-
age descriptions. We Ô¨Ånd that SelfEQ not only expandsthe vocabulary of objects that the base model is able to lo-calize but more importantly, improves the visual groundingcapabilities of the model on standard benchmarks such asreferring expression comprehension on the ReferIt bench-mark [ 24] and region-phrase grounding in the Flickr30K
Entities benchmark [ 40]. In summary, our key contributions
are as follows:
‚Ä¢ We design a novel objective, SelfEQ, to encourage vision-
and-language models to generate self-consistent visualexplanations for equivalent text phrases, thereby improv-ing grounding capabilities while expanding the workingvocabulary of the model.
‚Ä¢ We propose to prompt large language models for generat-
ing paraphrased image descriptions of individual objectsor regions. Particularly, we adopt Vicuna-13B [ 6] and de-
sign text prompts to obtain high quality paraphrases.
‚Ä¢ We demonstrate the effectiveness of our method by out-
performing previous methods, leading to 4.69% improve-
ment on Flickr30k, 7.68% improvement on ReferIt, and
3.74% improvement on RefCOCO+.
Finally, we plan to release our code, generated para-
phrases and model checkpoints upon publication.
2. Related Work
Our work is related to previous methods on visual ground-ing, especially those that are trained under weak supervisionunderstood as without the use of bounding box or segmentannotations and relying only on image-text pairs. From atechnical perspective our work is related to methods that op-timize visual explanations to improve the underlying model.Visual grounding consists of localizing an input textual
description in an image. Supervised methods are pro-vided with text-image pairs and corresponding boundingboxes [ 8,9,12,23,51]. Other supervised methods lever-
age pretrained object detectors to obtain a region of interestand then identify the region that aligns most closely undertheir textual representations [ 5,7,17,19,35,48]. In both
cases, these methods use some form of box supervision dur-ing pre-training or at test time by relying on a pre-trainedobject detector. In contrast, our work focuses exclusively inthe scenario where no bounding boxes or segment annota-tions are available at any stage.
Weakly-Supervised Grounding. Our setup is similar to that
of Arbelle et al [ 3] where no box annotations or object de-
tectors are used for grounding. This work proposes Ground-
ing by Separation (GbS) where a model is trained on ran-domly alpha-blended pairs of images and the goal is to sep-arate them conditioned on text prompts. Our method insteadrelies on data augmentation on the text side and while ourmethod shows favorable results, our contribution is orthog-onal. Shaharabany et al [ 45] builds a weakly-supervised
phrase grounding model by creating a large amount of databy combining region boxes with a BLIP captioning model.Later work by Shaharabany and Wolf [ 44] employs layer-
wise relevance propagation [ 38] to integrate relevancy and
gradient information with the scores computed from each
attention head in the transformer layers [ 4], or residual con-
nections [ 1]. Our work compares favorably or on par with
these methods but since we rely on gradient-based explana-tions, our work does not require making any modiÔ¨Åcationsto the base network.
Visual Explanation Tuning. Related to our method is the
strategy used by ALBEF [ 27] where the model is only
supervised on image-text pairs and performs groundingthrough GradCAM [ 43], a gradient-based visual explana-
tion method that outputs a heatmap indicating spatial rele-vance. Earlier, Xiao et al [ 49] used a similar strategy but fur-
ther optimized gradient-based explanations using structuralconstraints derived from text. Recently, AMC [ 51] follows
this strategy but further adds box supervision on the outputmaps using a margin-based loss. We adopt ALBEF [ 27]a s
our base model and also adopt a gradient-based explanation
strategy but unlike AMC [ 51] we do not rely on box anno-
tations for tuning this model and use our proposed SelfEQobjective instead. Javed et al [ 20] proposed an objective
function that encourages consistent representations in em-bedding space for the same input prompt on different im-ages. In contrast SelfEQ encourages consistent visual ex-planations for different prompts on the same image by rely-ing on automatically generated paraphrases. Akbari et al [ 2]
also optimizes attention maps but in their formulation themodel backbone is modiÔ¨Åed to explicitly incorporate atten-tion instead of relying on gradient-based attention maps.
13096
‹∂‡Øò
V‚Äúa choo choo ‚Äù‡¢ì‡¢è‡¢ô‡°∏ ‡µí›á‡¢ö‡¢ô‡¢â‡°∏ T 
V‚Äúa train‚Äù
‹©‡Øò‹©
ﬂ∂‡Øß
ﬂ∂‡Øßﬂ∂‡Ø©
ﬂ∂‡Ø©ﬂ∂‡Øô
ﬂ∂‡ØôRoI Mask
Figure 2. Overview of our proposed weakly-supervised Self -
consistency EQuivalence tuning objective. We input image-text
and image-paraphrase pairs /angbracketleftV,T/angbracketrightand/angbracketleftV,Te/angbracketrightto our base pre-
trained vision-and-language model. We then obtain gradient-based visual explanations /angbracketleftG,G
e/angbracketrightand compute a similarity loss
between them. We also deÔ¨Åne an overlapping region of inter-
est mask and encourage the model to predict consistently high
saliency scores within this mask for each input pair.
3. Method
We start from a base vision-language model composed of
a text encoder œÜt, an image encoder œÜv, and a multimodal
fusion encoder œÜf, along with a dataset Dto Ô¨Ånetune this
model consisting of image-text pairs /angbracketleftT,V/angbracketright. Section 3.1,
introduces the training objectives for the base model whichwe also adopt for Ô¨Ånetuning our baseline and are also usedin conjunction with SelfEQ to Ô¨Ånetune our Ô¨Ånal model. Sec-
tion 3.2 introduces in detail SelfEQ, our self-consistency
equivalence tuning objective that assumes the existence ofparaphrases T
efor each input image-text pair /angbracketleftT,V/angbracketright, and
Section 3.3 describes our approach for automatically gen-
erating paraphrases Teusing LLM-prompting. Figure 2
presents an overview of our proposed method.
3.1. Base Model: Preliminaries
Our base vision-and-language model is ALBEF [ 27] which
relies on three widely used objectives for visual and textualrepresentation learning: image-text matching, masked lan-guage modeling and a contrastive loss. We describe themhere brieÔ¨Çy as they are also re-used during Ô¨Åne-tuning.
Image-Text Matching Loss (ITM). This loss is calcu-
lated using the output of the [CLS] token to predict if the
input image and the input text are matching or not and isdeÔ¨Åned as follows:
Litm=E(V,T )‚àºDH/parenleftBig
/vectory,œÜcls
f(œÜv(V),œÜt(T))/parenrightBig
, (1)
where/vectorydenotes a two-dimensional one-hot vector, indicat-
ing whether the sample /angbracketleftV,T/angbracketrightconstitutes a match, œÜcls
frep-
resents a linear layer followed by a softmax function, andHis the cross entropy loss function.Masking Language Modeling Loss (MLM). This loss
has been applied for various vision-language pretrainingmodels [ 5,27,29,34]. It integrates the contextual text and
the input image to infer masked words in the input text. Af-ter utilizing a linear layer and a softmax activation functionœÜ
m
fto individual output embeddings, the objective is ex-
pressed as:
Lmlm=E(V,T‚àím)‚àºDH/parenleftbig/vectortm,œÜm
f/parenleftbig
œÜv(V),œÜt/parenleftbig
T‚àím/parenrightbig/parenrightbig/parenrightbig
,(2)
where the one-hot vector /vectortmdenotes the masked token, and
T‚àímrepresents the input masked text.
Image-Text Contrastive Loss (ITC). It improves the
alignment between visual and textual representations by
bringing closer the representations for corresponding text-
image pairs relative to text-image pairs that do not corre-spond. This objective can be deÔ¨Åned as follows:
Litc=E(V,T )‚àºD1
2/bracketleftbigg
H/parenleftbigg
/vectory,exp (œÜv(V)¬∑œÜt(T))/œÑ/summationtextB
b=1s(V,T b)/parenrightbigg
+H/parenleftbigg
/vectory,exp (œÜt(T)¬∑œÜv(V))/œÑ/summationtextB
b=1s(T,V b)/parenrightbigg/bracketrightbigg
,(3)
whereBis the number of negative sample pairs, and œÑis a
temperature parameter for the softmax function.
The training objective for the base model is a combina-
tion of the previous three loss functions:
Lvl=Litm+Lmlm+Litc. (4)
This loss Lvlwill also be used to tune our baseline model.
3.2. Self-Consistency Equivalence Tuning
SelfEQ assumes that the model has access to paraphrases
Tefor each input image-text pair /angbracketleftV,T/angbracketrightor, in practice, for a
subset of those samples. Therefore we assume a Ô¨ÅnetuningdatasetD
/primewith triplets /angbracketleftV,T,Te/angbracketrightsuch that Teexists for
a corresponding input text T. The Ô¨Årst step to deÔ¨Åne our
SelfEQ objective is to generate the explanation heatmaps(i.e., attention maps) through GradCAM [ 43] conditioned
on the input text. We extract intermediate feature maps from
the multimodal interactive encoder œÜ
ffor input pairs /angbracketleftV,T/angbracketright
and/angbracketleftV,Te/angbracketrightas follows:
F=œÜ(œÜv(V),œÜt(T)),Fe=œÜ(œÜv(V),œÜt(Te)), (5)
whereœÜdenotes the feature map extraction operation. We
then proceed to calculate the gradient of FandFerelated
to the image-text matching score Litm. This computation
yields the attention maps for the original text and para-phrased text, referred to as GandG
e, respectively:
G=R e L U/parenleftBig
F‚äô/triangleinvH/parenleftBig
/vectory,œÜcls
f(œÜv(V),œÜt(T))/parenrightBig/parenrightBig
,
Ge=R e L U/parenleftBig
Fe‚äô/triangleinvH/parenleftBig
/vectory,œÜcls
f(œÜv(V),œÜt(Te))/parenrightBig/parenrightBig
.(6)
13097
Our SelfEQ tuning is based on the premise that if a
vision-language model is identiÔ¨Åed as self-consistent, theattention maps produced for both the text and its equivalentparaphrase should yield nearly identical results. To achievethis, we Ô¨Årst apply a simple mean squared error loss over theproduced heatmaps so that their /lscript
2distance is minimized
and thus become more similar.
Lsim=E(V,T,Te)‚àºD/prime/bracketleftbigg1
N/summationdisplay
i,j(Gi,j‚àíGe
i,j)2/bracketrightbigg
. (7)
Nevertheless, while minimizing a sum of pixel-wise dis-
tances contributes to self-consistency, without a regulariza-tion term this loss can easily fall into a trivial solution. Forinstance, it could lead to attention maps with uniformly
negative or positive predictions, or just really small val-
ues. To address this limitation, we propose to integratethese heatmaps by deÔ¨Åning a Region of Interest (RoI) mask.This mask is designed to preserve regions within the atten-tion maps that possibly contain correct predictions. Our ap-proach hinges on the observation that, despite the predic-
tions of equivalent textual inputs being inconsistent, some-
times regions with large values or regions that overlap be-tween the two heatmaps tend to be correct. As such, we as-sume that if the sum of attention scores at a given position(i,j)exceeds a certain threshold k, it is likely indicative of
a correct prediction. We formalize the condition as follows:
Mi,j=/braceleftBigg
1,(Gi,j+Ge
i,j)‚â•k
0,(Gi,j+Ge
i,j)<k. (8)
The attention maps within RoI masks are obtained by
element-wise multiplication as follows:
R=G‚äôM, Re=Ge‚äôM. (9)
The integration of RoI masks allows us to use equiva-
lent texts for mutual supervision, reÔ¨Åning and improvingthe accuracy and providing regularization for the previouslydeÔ¨Åned distance-based loss. Moreover, it could potentiallyaddress errors owing to unknown or less common wordsby working vocabulary expansion. Presuming one of thetextual expressions is known and correctly understood, themodel could extrapolate the meaning of the other equivalentexpression via weak supervision. To implement it, we Ô¨Årstcompute the mean Œº
RoI,Œºe
RoI and the standard deviation
œÉRoI,œÉe
RoI within the RoI as follows:
ŒºRoI=/summationtext
i,jRi,j/summationtext
i,jMi,j,Œºe
RoI=/summationtext
i,jRe
i,j/summationtext
i,jMi,j, (10)
œÉRoI=/radicalBigg/summationtext
i,jMi,j¬∑(Ri,j‚àíŒºRoI)2
/summationtext
i,jMi,j,
œÉe
RoI=/radicalBigg/summationtext
i,jMi,j¬∑(Re
i,j‚àíŒºe
RoI)2
/summationtext
i,jMi,j.(11)We propose a consistency loss ( Lcst), expecting the
RoI regions of attention maps to achieve consistently highscores, further reinforcing self-consistency, accuracy, andpotential working vocabulary expansion. This objective isformulated as follows:
Lcst=E(V,T,Te)‚àºD/prime/bracketleftbigg
œÉRoI+œÉe
RoI+
max(0,k/2‚àíŒºRoI)+m a x ( 0 ,k/2‚àíŒºe
RoI)/bracketrightbigg
.(12)
Finally, the objective of our self-consistency equivalence
tuning is expressed as:
LSelfEQ =Lsim+Œª¬∑Lcst, (13)
whereŒªis a hyperparameter to control the relative inÔ¨Çuence
of each loss.
3.3. Self-Consistency Data Augmentation
In this section we deÔ¨Åne a function Fthat can automatically
map input textual phrases Tas paraphrases Tewithout the
need to rely on human annotations such that Te‚àºF (T).
We achieve this goal through a two-level prompting ap-
proach using a large language model which we describe indetail below.
Phrase Chunking: Using our Ô¨Årst-level prompts, we aim
to augment the original text using phrase chunking to en-courage global captions to concentrate on more speciÔ¨Åc re-gions. Visual grounding seeks to localize objects in imagesbased on textual inputs. In contrast, global captions usuallydescribe the entire image, typically describing several ob-jects. While training on global captions could be beneÔ¨Åcialfor learning cross-modal information, it may lead the modelto predict a broader region ( i.e., global context) rather than a
speciÔ¨Åc region. Phrase chunking ( i.e., shallow parsing [ 55])
aims to identify continuous sequences of tokens represent-ing syntactic units, enabling the extraction of phrases fromunstructured text. We leverage an LLM to segment globalcaptions into object-centric short phrases. During training,we use these image-chunk pairs instead of global captions,effectively guiding the model attention toward localized re-gions of interest. We refer the reader to the supplementary
material for prompting details and generated examples.
Paraphrase Generation: Our SelfEQ approach involves
feeding the model with pairs of textual descriptions that re-fer to the same underlying concept, with the expectation thatthe model can make similar predictions for these equivalentdescription pairs /angbracketleftT,T
e/angbracketright. We augment our dataset by trans-
forming the region-based captions ( i.e., text that only refers
to a region in the image) and the object-centric short phraseswe obtained from phrase chunking into equivalent para-
phrases referring to the same concept through our second-level LLM-prompts.
13098
					

   	
   




		 
		


 
Figure 3. Two samples from our LLM-prompt for paraphrase gen-
eration. The Ô¨Årst set showcases an example of a region-based cap-tion, and the second set shows a non-sentence phrase. Qis the
query text and Ais the expected answer.
There are many ways to paraphrase, including substi-
tuting words, altering sentence structures, and rewritingsentences based on semantics. However, considering thatself-consistency in vision and language is relatively under-explored, we adopt a straightforward strategy: Replacing
the primary object in the sentence while retaining all other
attributes. This strategy yields several beneÔ¨Åts. First, it pro-vides a consistent context, which serves as a reference forthe model to identify equivalent descriptions. This enablesthe equivalent relationships of paraphrases to be learned in-tuitively. Second, it simpliÔ¨Åes prompt designing and post-processing by detecting the primary object and generatingits synonym.
To generate paraphrases for the dataset consisting of
region-based captions, we select four textual descriptionsin which the primary noun plays different syntactic roles.We further select two non-sentence phrases as examplesof query texts in our prompts. We show an example ofa region-based caption and a non-sentence phrase in Fig-ure 3. To design our prompt, we identify the primary ob-
ject in the query text Q. Then we use WordNet [ 37]t oo b -
tain synonyms automatically and further remove inaccurateor invalid words. We add Ato indicate the expected an-
swer and include other relationships such as antonym, hy-
pernym and meronym to provide richer contexts for LLM
in-context learning. Additional prompting details and para-
phrase samples are provided in the supplementary material.
This two-level prompt-based LLM augmentation ap-
proach ensures that our model is exposed to textual inputs
that share the same concept while varying in linguistic rep-
resentation, thereby promoting self-consistency and work-ing vocabulary expansion.
4. Experimental Settings
Training. We use ALBEF [ 27] as our base model in
all our experiments, given its reported off-the-shelf visualgrounding performance via GradCAM [ 43]. ALBEF com-
bines a ViT-B [ 11] model for encoding images and a BERT-
base [ 10] model for encoding text. It is pre-trained on a
range of datasets, including ImageNet-1K [ 42], Conceptual
Captions [ 46], SBU Captions [ 39], MS-COCO [ 33], andMethod Training Flickr30k ReferItBox SupervisionAlign2Ground [ 7] VG-boxes 71.00 -
12-in-1 [ 35] VG-boxes 76.40 -
InfoGround [ 19] VG-boxes 76.74 -
VMRM [ 12] VG-boxes 81.11 -
AMC [ 51] VG-boxes 86.59 73.17Without Box SupervisionTD [ 56] VG 42.40 31.97
SSS [ 20] VG 49.10 39.98
MG-BiLSTM [ 2] VG 57.91 62.76
MG-ELMo [ 2] VG 60.08 60.01
GbS [ 3] VG 73.39 62.24
g[45] VG 75.63 65.95
g++ [ 44] VG 79.95 70.25
SelfEQ (ours) VG 81.90 67.40
FCVC [ 13] MS-COCO 29.03 33.52
MG-BiLSTM [ 2] MS-COCO 53.29 47.89
MG-ELMo [ 2] MS-COCO 61.66 47.52
GbS [ 3] MS-COCO 74.50 49.26
g[45] MS-COCO 75.43 61.03
g++ [ 44] MS-COCO 78.10 61.53
SelfEQ (ours) MS-COCO 84.07 62.75
Table 1. Visual Grounding results on two benchmarks using point-
ing game accuracy with two training datasets. SelfEQ yieldsgenerally the best overall performance among weakly-supervisedmethods, and comes second to g++ on the ReferIt benchmarkwhen trained using VG. We also show at the top the results of
methods using additional box supervision from Visual Genome
(VG) either directly or through an object detector.
Visual Genome (VG) [ 26] excluding box annotations. We
Ô¨Ånetune ALBEF with image-text pairs from VG and MS-
COCO without any type of box supervision (i.e., no bound-ing boxes or object detectors), following prior work [ 2].
We further leverage Vicuna-13B [ 6] as our LLM-prompting
model to generate the object-centric short phrases (via shal-low parsing or chunking) and the equivalent paraphrases
for our self-consistency data augmentation. Additionally,we validate the effectiveness of our SelfEQ tuning and self-
consistency data augmentation method by training on a pre-processed subset of the Conceptual Captions 3M (CC3M)dataset [ 46], which contains many noisy or unaligned web-
crawled AltText-image pairs. With this subset, we achieve
an absolute improvement of 2.15% on Flickr30k, 3.32% on
ReferIt, and 1.33% on RefCOCO+; refer to the supplemen-
tary material for detailed CC3M experiments.
Evaluation. We conduct evaluations using Flickr30k [ 40]
and ReferIt [ 24] under pointing game accuracy following
previous weakly-supervised visual grounding works [ 2,3].
To underscore the competitive edge of our method, we also
13099
Method Box SupervisionRefCOCO+
Test A Test B
InfoGround [ 19] Yes 39.80 41.11
VMRM [ 12] Yes 58.87 50.32
AMC [ 51]Y e s 80.34 64.55
ALBEF [ 27] No 69.35 53.77
SelfEQ (ours) No 75.10 55.49
Table 2. Results on RefCOCO+ pointing game accuracy. SelfEQ
shows signiÔ¨Åcant improvements over off-the-shelf ALBEF andcompetitive results compared to box-supervised methods.
present its performance on RefCOCO+ [ 53], a challenging
benchmark more commonly used for testing box-supervisedmethods [ 7,12,19,35,51].
4.1. Implementation Details
Our experiments are conducted on a single computing node
with 8 NVIDIA A40 GPUs. During the training phase, in-put images are resized to 256√ó256 and augmented with hor-
izontal Ô¨Çipping, color jittering, and random grayscale con-version. We set up an Adam optimizer [ 25] with a learning
rate of 1e-5 and a batch size of 448 across all experiments.We empirically set the RoI threshold kto 0.8 and the hy-
perparameter Œªto 1.0. For training with raw image-text
pairs from the datasets, we employ the vision-language ob-jectiveL
vl(Sec. 3.1), while for the subset with equivalent
paraphrases, we use our self-consistency equivalence tun-ing objective L
SelfEQ (Sec. 3.2) and corresponding vision-
language objective Le
vl. The composite objective function
is given by L=Œ±¬∑Lvl+( 1‚àíŒ±)¬∑(LSelfEQ +Le
vl), where
Œ±is initially set to 0 and increments to 1, remaining con-
stant after the second epoch. Our hyperparameter values
and schedules were determined empirically on a small vali-
dation subset.
5. Experimental Results
Our resulting model obtains the best performance on thetask of weakly-supervised visual grounding compared tomost methods under this setting and is comparable to sev-eral prior works that rely on some of box supervision.Moreover, our qualitative results show that our method can
handle paraphrases and a larger working vocabulary withoutthe needed to increment the training dataset signiÔ¨Åcantly.
Flickr30k and ReferIt. We evaluate the effectiveness of
our proposed SelfEQ method in Table 1, demonstrating its
substantial lead over GradCAM-based weakly-supervisedapproaches. Our self-consistency equivalent tuning adaptswell for both region-based ( i.e.,, VG) and global-based
(i.e.,, COCO) image-text pairs, yielding a performance gain
of 4.69% on Flickr30k and 7.68% on ReferIt, compared toImage >& D K∆µ∆å∆ê
Text: ‚Äúblue thermos very bottom‚ÄùText: ‚Äútrees on the right‚Äù
Text: ‚Äúwhite bldg‚Äù
Text: ‚Äúkangaroo furthest away facing right‚Äù
Text: ‚Äúperson right corner‚Äù
Image >& D K∆µ∆å∆ê
Image >& D K∆µ∆å∆ê
Image >& D K∆µ∆å∆ê
Image >& D K∆µ∆å∆ê
Figure 4. Qualitative results of our method in challenging visual
grounding scenarios compared to prior works. On top of each rowwe show the reference text, the Ô¨Årst column shows the image, thenwe show our base model ALBEF, the SotA box-supervised methodAMC, and Ô¨Ånally we show results with our method SelfEQ.
our base model ALBEF (see Ô¨Årst row in Table 3). Notably,
our method outperforms almost all box-supervised meth-ods on Flickr30k [ 7,12,19,35]. In the weakly-supervised
setting, our method only comes second on ReferIt com-
pared to g++[ 44] when trained on Visual Genome image-
text pairs. This method leverages a custom architectureto produce a mask and uses heatmap supervision from aCLIP [ 41] model as pseudo-labels during training. We
posit that our contribution is orthogonal and our approachwould likely also beneÔ¨Åt from similar supervision, since
CLIP is trained in a much larger image-text pair dataset.Despite differences, our method still obtains higher per-formance when trained on MS-COCO and the best per-
formance compared to all weakly-supervised methods onFlickr30K region-phrase grounding.
RefCOCO+. RefCOCO+ [ 53] serves as a rigorous bench-
mark for visual grounding, typically used to evaluate box-supervised techniques. In Table 2, we present the per-
13100
Data ObjectiveRefCOCO+Flickr30k ReferIt
Test A Test B
- Lvl 69.35 53.77 79.38 59.72
T Lvl 72.30 54.22 78.75 65.86
T+TeLvl 71.55 53.51 78.05 64.57
T+TeLSelfEQ 75.10 55.49 81.90 67.40
Table 3. Ablation studies on different ways to utilize extra equiv-
alent paraphrased data. The Ô¨Årst row is off-the-shelf ALBEF per-formance before tuning. Tdenotes the textual captions from the
dataset, and T
ecorresponds to the associated equivalent para-
phrases.Lvlis the vision-language objective, and LSelfEQ is our
self-consistency equivalence tuning objective.
formance of our weakly-supervised method (VG trained)
against box-supervised methods. Our results indicate thatour approach is competitive without reliance on any formof box annotations and signiÔ¨Åcantly improves over the baseALBEF model.
Visual Grounding Analysis. Figure 4provides qualita-
tive results of our method in challenging scenarios, includ-ing occluded objects (row 1), small objects within complexscenes (row 2), objects partially shown in the corner of theimage (row 3), multiple similar objects (row 4), and abbre-viated text inputs (row 5). Our self-consistency equivalencytuning approach exhibits substantial improvements in thegrounding capability of the base ALBEF [ 27] model. Re-
markably, our approach even outperforms the state-of-the-art box-supervised method AMC [ 51] in multiple scenarios.
Self-Consistency Analysis. Figure 5demonstrates qualita-
tive results for the self-consistency capability across differ-ent equivalent paraphrases, encompassing terminology (row
1), synonym substitutions (row 2), and regional slang com-
bining with different sentence structures (row 3). Althoughother methods succeed in localizing certain phrases, theydemonstrate inconsistencies for the equivalent paraphrases.In contrast, our model Ô¨Ånetuned with SelfEQ effectivelyestablishes connections between semantically equivalentparaphrases, thereby enhancing the model self-consistencyability and potentially expanding its working vocabulary.
5.1. Ablation Studies
Data Quantity. We assess the impact of the data quantity of
our generated paraphrases and compare our tuning strategyagainst standard vision-language objectives. We randomlysample portions of data from VG by 10% associated withour augmented equivalent paraphrases three times. To com-pare, we use the vision-language objective with VG text-image pairs as baselines. In Figure 6, we show the mean
and standard deviation pointing game accuracy. The perfor-mance of the base vision-language objective does not ex-hibit a steady improvement with more text-image pairs. Al-Text: ‚Äúwater ‚Äù
Equivalent Paraphrase: ‚ÄúH2O ‚Äù
ImageALBEF AMC Ours
ALBEF AMC Ours
Text: ‚Äúright light ‚Äù
Equivalent Paraphrase: ‚Äúright illuminator ‚Äù
ImageALBEF AMC Ours
ALBEF AMC Ours
Text: ‚Äúan umbrella‚Äù
Equivalent Paraphrase: ‚Äúthere is a brolly in the image ‚Äù
ImageALBEF AMC Ours
ALBEF AMC Ours
Figure 5. Qualitative results across equivalent paraphrases among
different methods. For each image, we show a caption referringto an object in the Ô¨Årst row and an equivalent paraphrase in thesecond row. Each column shows the results of ALBEF, the SotAbox-supervised method AMC, and our SelfEQ method.
though ReferIt performance increases, the performance on
RefCOCO+ Test A remains mostly unchanged. Addition-ally, the performance on Flickr30k notably decreases, andthere is a mixed effect on RefCOCO+ Test B, with half ofthe accuracy falling below the off-the-shelf ALBEF perfor-mance of 53.77%.
In contrast, SelfEQ consistently leads to performance en-
hancements with more equivalent paraphrases. Clear up-
ward trends are observed on Flickr30k, ReferIt, and Ref-
COCO+ Test A as more data with corresponding equiva-lent paraphrases are added, meanwhile the gaps between thebaselines generally widen. Notably, SelfEQ tuning main-tains performance gains on Flickr30k, whereas the base-lines performances drop. Although the trend on RefCOCO+
Test B is not consistently increasing, it is essential to em-phasize that RefCOCO+ Test B is only a subset, and SelfEQ
illustrates more stable and effective tuning performance onit, compared to the base vision-language objective.
These observations indicate that more equivalent para-
phrases connecting with associated text phrases enable the
13101
10 20 30 40 50 60 70 80 90100
Data Amount (%)78.579.079.580.080.581.081.582.0Pointing Game Accuracy (%)Flickr30k
Baseline
SelfEQ (ours)
10 20 30 40 50 60 70 80 90100
Data Amount (%)64.064.565.065.566.066.567.067.568.0Pointing Game Accuracy (%)ReferIt
Baseline
SelfEQ (ours)
10 20 30 40 50 60 70 80 90100
Data Amount (%)71.572.072.573.073.574.074.575.0Pointing Game Accuracy (%)RefCOCO+ Test A
Baseline
SelfEQ (ours)
10 20 30 40 50 60 70 80 90100
Data Amount (%)52.553.053.554.054.555.055.556.0Pointing Game Accuracy (%)RefCOCO+ Test B
Baseline
SelfEQ (ours)
Figure 6. Tuning performance with different data quantities on Flickr30k, ReferIt, RefCOCO+ Test A and Test B. The purple and cyan
lines represent SelfEQ (ours) and ALBEF baseline losses (vision-language objective), respectively. We show the impact of progressivelyaugmenting captions via LLM-prompting for generating equivalent paraphrases tuned with our SelfEQ objective. Best viewed in color.
Format Objective Flickr30k ReferIt
- Lvl 79.38 59.72
C Lvl 79.90 60.64
C LSelfEQ 81.28 62.04
P Lvl 81.18 61.18
P LSelfEQ 84.07 62.75
Table 4. Comparisons on data augmentation strategy for global-
based captions in MS-COCO with or without the paraphrases. Cis
the global-based captions from MS-COCO, and Pis our Vicuna-
13B processed object-centric phrases separately. The Ô¨Årst row isthe off-the-shelf ALBEF performance before tuning.
model to acquire more valuable information during tuning.
SelfEQ proves to be an effective and robust strategy for con-sistently improving performance with our generated para-
phrases. With increased self-consistency augmented data,SelfEQ guides the model toward better grounding perfor-mance by enhancing its self-consistency capabilities.
Our method generates equivalent paraphrases for self-
consistency enhancement, but it also contributes additionaldata for training. To ascertain the speciÔ¨Åc impact of ourSelfEQ tuning strategy, we run a control experiment onthis variable. As shown in Table 3, we assess the model‚Äôs
performance when equivalent paraphrases are integratedas regular image-text pairs with vision-language objectives
(Sec. 3.1). This comparison reveals that merely augment-
ing the dataset with extra image-paraphrase pairs, withoutforming explicit linkages between the original text and itsparaphrases, does not yield performance improvements.
Data Augmentation. For global-based captions in MS-
COCO, we preprocess the captions Cto object-centric short
phrasesPvia LLM-prompting. As shown in Table 4, tuning
with phrases Pleads to better performance, beneÔ¨Åting both
the vision-language objective ( L
vl) and our self-consistency
equivalence tuning objective ( LSelfEQ ). This improvement
is probably attributed to short phrases allowing the model tofocus on a speciÔ¨Åc region rather than the entire scene, align-ing more closely with the objective of visual grounding. By
utilizing equivalent paraphrases with our SelfEQ objective(row 3 and 5), phrase chunking helps SelfEQ even more,
indicating the important role of equivalent paraphrases inpromoting self-consistency and grounding ability.LsimLcstRefCOCO+Flickr30k ReferIt
Test A Test B
/check 66.42 47.21 68.26 55.96
/check 73.33 55.88 80.94 66.57
/check/check 75.10 55.49 81.90 67.40
Table 5. Ablation studies on objective components of self-
consistency equivalence tuning objective LSelfEQ .
Objective. Table 5evaluates each component within our
self-consistency equivalence tuning objective. The Lsim
loss targets pixel-wise similarity, ensuring that maps for a
caption and its equivalent paraphrase are identical. How-ever, focusing solely on pixel-level similarity may neglectthe precise spatial positioning of objects. To address this,theL
cstloss is proposed to identify the most likely cor-
rect object positions within the two maps ( i.e., RoI). It
then encourages the model to yield consistently high atten-tion scores within the RoI. By integrating both L
sim and
Lcst, self-consistency equivalence tuning objective fosters
the model to not only align global similarities but also topinpoint accurate object locations through the mutual su-pervision provided by a caption and its paraphrase, thereby
enhancing self-consistency and accuracy.
6. Conclusion
We propose a novel weakly-supervised tuning approach
coupled with a data augmentation strategy to enhance thelocalization capabilities of an image-text supervised modelthrough self-consistency. Using an open-source LLM, weexpand a dataset with equivalent paraphrases tailored to beobject-centric. The augmented data is used to Ô¨Ånetune abase model employing our novel self-consistency equiva-lence tuning objective. Our approach has been rigorouslyvalidated across pretraining on diverse datasets, rangingfrom region-based captions ( i.e., VG) to global-based cap-
tions ( i.e., COCO). Our method achieves superior and self-
consistent performance on three benchmarks and is evencompetitive with some box-supervised methods.
Acknowledgments. This work was partially funded by
NSF Award #2201710 and a Ken Kennedy Institute‚Äôs SLBPhD Fellowship.
13102
References
[1] Samira Abnar and Willem Zuidema. Quantifying atten-
tion Ô¨Çow in transformers. arXiv preprint arXiv:2005.00928 ,
2020. 2
[2] Hassan Akbari, Svebor Karaman, Surabhi Bhargava, Brian
Chen, Carl V ondrick, and Shih-Fu Chang. Multi-level multi-
modal common semantic space for image-phrase grounding.InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 12476‚Äì12486, 2019. 2,
5
[3] Assaf Arbelle, Sivan Doveh, Amit Alfassy, Joseph Shtok,
Guy Lev, Eli Schwartz, Hilde Kuehne, Hila Barak Levi,Prasanna Sattigeri, Rameswar Panda, et al. Detector-freeweakly supervised grounding by separation. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-sion , pages 1801‚Äì1812, 2021. 2,5
[4] Hila Chefer, Shir Gur, and Lior Wolf. Transformer inter-
pretability beyond attention visualization. In Proceedings of
the IEEE/CVF conference on computer vision and patternrecognition , pages 782‚Äì791, 2021. 2
[5] Yen-Chun Chen, Linjie Li, Licheng Y u, Ahmed El Kholy,
Faisal Ahmed, Zhe Gan, Y u Cheng, and Jingjing Liu. Uniter:Universal image-text representation learning. In European
conference on computer vision , pages 104‚Äì120. Springer,
2020. 1,2,3
[6] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-
hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Y ong-hao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P .Xing. Vicuna: An open-source chatbot impressing gpt-4with 90%* chatgpt quality, 2023. 2,5
[7] Samyak Datta, Karan Sikka, Anirban Roy, Karuna Ahuja,
Devi Parikh, and Ajay Divakaran. Align2ground: Weaklysupervised phrase grounding guided by image-caption align-ment. In Proceedings of the IEEE/CVF international con-
ference on computer vision , pages 2601‚Äì2610, 2019. 2,5,
6
[8] Chaorui Deng, Qi Wu, Qingyao Wu, Fuyuan Hu, Fan Lyu,
and Mingkui Tan. Visual grounding via accumulated atten-tion. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 7746‚Äì7755, 2018. 2
[9] Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang
Zhou, and Houqiang Li. Transvg: End-to-end visual ground-ing with transformers. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 1769‚Äì
1779, 2021. 2
[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectionaltransformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 5
[11] Al exey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 5
[12] Zi-Yi Dou and Nanyun Peng. Improving pre-trained vision-
and-language embeddings for phrase grounding. In Proceed-ings of the 2021 Conference on Empirical Methods in Natu-
ral Language Processing , pages 6362‚Äì6371, 2021. 1,2,5,
6
[13] Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K Sri-
vastava, Li Deng, Piotr Doll ¬¥ar, Jianfeng Gao, Xiaodong He,
Margaret Mitchell, John C Platt, et al. From captions to vi-sual concepts and back. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
1473‚Äì1482, 2015. 5
[14] Y uting Gao, Jinfeng Liu, Zihan Xu, Jun Zhang, Ke Li, and
Chunhua Shen. Pyramidclip: Hierarchical feature align-ment for vision-language model pretraining. arXiv preprint
arXiv:2204.14095 , 2022. 1
[15] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scal-
ing open-vocabulary image segmentation with image-levellabels. In European Conference on Computer Vision , pages
540‚Äì557. Springer, 2022. 1
[16] Shashank Goel, Hritik Bansal, Sumit Bhatia, Ryan A
Rossi, Vishwa Vinay, and Aditya Grover. Cyclip: Cycliccontrastive language-image pretraining. arXiv preprint
arXiv:2205.14459 , 2022. 1
[17] Eyal Gomel, Tal Shaharbany, and Lior Wolf. Box-based re-
Ô¨Ånement for weakly supervised and unsupervised localiza-
tion tasks. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 16044‚Äì16054, 2023.
2
[18] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
Open-vocabulary object detection via vision and languageknowledge distillation. arXiv preprint arXiv:2104.13921 ,
2021. 1
[19] Tanmay Gupta, Arash V ahdat, Gal Chechik, Xiaodong Yang,
Jan Kautz, and Derek Hoiem. Contrastive learning forweakly supervised phrase grounding. In European Confer-
ence on Computer Vision , pages 752‚Äì768. Springer, 2020. 1,
2,5,6
[20] Syed Ashar Javed, Shreyas Saxena, and Vineet Gandhi.
Learning unsupervised visual grounding through semanticself-supervision. Twenty-Eighth International Joint Confer-
ence on ArtiÔ¨Åcial Intelligence (IJCAI) , 2019. 2,5
[21] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Y un-Hsuan Sung, Zhen Li, and TomDuerig. Scaling up visual and vision-language representa-tion learning with noisy text supervision. In International
conference on machine learning , pages 4904‚Äì4916. PMLR,
2021. 1
[22] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc V . Le, Y un-Hsuan Sung, Zhen Li, and TomDuerig. Scaling up visual and vision-language representationlearning with noisy text supervision. In International Con-
ference on Machine Learning , 2021. 1
[23] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel
Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understand-ing. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision , pages 1780‚Äì1790, 2021. 1,2
[24] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and
Tamara Berg. Referitgame: Referring to objects in pho-
13103
tographs of natural scenes. In Proceedings of the 2014 con-
ference on empirical methods in natural language processing
(EMNLP) , pages 787‚Äì798, 2014. 2,5
[25] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 6
[26] Ranjay Krishna, Y uke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-tidis, Li-Jia Li, David A Shamma, et al. Visual genome:Connecting language and vision using crowdsourced denseimage annotations. International journal of computer vision ,
123:32‚Äì73, 2017. 2,5
[27] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,
ShaÔ¨Åq Joty, Caiming Xiong, and Steven Chu Hong Hoi.Align before fuse: Vision and language representation learn-ing with momentum distillation. Advances in Neural Infor-
mation Processing Systems , 34, 2021. 1,2,3,5,6,7
[28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for uni-Ô¨Åed vision-language understanding and generation. arXiv
preprint arXiv:2201.12086 , 2022. 1
[29] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,
and Kai-Wei Chang. Visualbert: A simple and perfor-mant baseline for vision and language. arXiv preprint
arXiv:1908.03557 , 2019. 3
[30] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-
wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, LuY uan, Lei Zhang, Jenq-Neng Hwang, et al. Groundedlanguage-image pre-training. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10965‚Äì10975, 2022. 1
[31] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-
wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, LuY uan, Lei Zhang, Jenq-Neng Hwang, et al. Groundedlanguage-image pre-training. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10965‚Äì10975, 2022. 1
[32] Yangguang Li, Feng Liang, Lichen Zhao, Y ufeng Cui, Wanli
Ouyang, Jing Shao, Fengwei Y u, and Junjie Yan. Su-
pervision exists everywhere: A data efÔ¨Åcient contrastive
language-image pre-training paradigm. arXiv preprint
arXiv:2110.05208 , 2021. 1
[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ¬¥ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision‚ÄìECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,P a r tV1 3 , pages 740‚Äì755. Springer, 2014. 2,5
[34] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:
Pretraining task-agnostic visiolinguistic representations forvision-and-language tasks. Advances in neural information
processing systems , 32, 2019. 3
[35] Jiasen Lu, V edanuj Goswami, Marcus Rohrbach, Devi
Parikh, and Stefan Lee. 12-in-1: Multi-task vision andlanguage representation learning. In Proceedings of the
IEEE/CVF conference on computer vision and patternrecognition , pages 10437‚Äì10446, 2020. 2,5,6[36] Timo L ¬®uddecke and Alexander Ecker. Image segmenta-
tion using text and image prompts. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition , pages 7086‚Äì7096, 2022. 1
[37] George A Miller. Wordnet: a lexical database for english.
Communications of the ACM , 38(11):39‚Äì41, 1995. 5
[38] Gr ¬¥egoire Montavon, Sebastian Lapuschkin, Alexander
Binder, Wojciech Samek, and Klaus-Robert M ¬®uller. Ex-
plaining nonlinear classiÔ¨Åcation decisions with deep taylordecomposition. Pattern recognition , 65:211‚Äì222, 2017. 2
[39] Vicente Ordonez, Girish Kulkarni, and Tamara Berg.
Im2text: Describing images using 1 million captioned pho-tographs. Advances in neural information processing sys-
tems , 24, 2011. 5
[40] Bryan A Plummer, Liwei Wang, Chris M Cervantes,
Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-nik. Flickr30k entities: Collecting region-to-phrase corre-spondences for richer image-to-sentence models. In Pro-
ceedings of the IEEE international conference on computer
vision , pages 2641‚Äì2649, 2015. 2,5
[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, GretchenKrueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In Proceedings
of the 38th International Conference on Machine Learning ,
pages 8748‚Äì8763. PMLR, 2021. 1,6
[42] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,Aditya Khosla, Michael Bernstein, et al. Imagenet largescale visual recognition challenge. International journal of
computer vision , 115:211‚Äì252, 2015. 5
[43] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,
Ramakrishna V edantam, Devi Parikh, and Dhruv Batra.
Grad-cam: Visual explanations from deep networks via
gradient-based localization. In Proceedings of the IEEE in-
ternational conference on computer vision , pages 618‚Äì626,
2017. 1,2,3,5
[44] Tal Shaharabany and Lior Wolf. Similarity maps for self-
training weakly-supervised phrase grounding. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition , pages 6925‚Äì6934, 2023. 2,5,6
[45] Tal Shaharabany, Y oad Tewel, and Lior Wolf. What is
where by looking: Weakly-supervised open-world phrase-grounding without text inputs. Advances in Neural Informa-
tion Processing Systems , 35:28222‚Äì28237, 2022. 2,5
[46] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. Conceptual captions: A cleaned, hypernymed, im-age alt-text dataset for automatic image captioning. In Pro-
ceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (V olume 1: Long Papers) , pages
2556‚Äì2565, 2018. 2,5
[47] Amanpreet Singh, Ronghang Hu, V edanuj Goswami, Guil-
laume Couairon, Wojciech Galuba, Marcus Rohrbach, andDouwe Kiela. Flava: A foundational language and visionalignment model. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
15638‚Äì15650, 2022. 1
13104
[48] Josiah Wang and Lucia Specia. Phrase localization without
paired training examples. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 4663‚Äì
4672, 2019. 2
[49] Fanyi Xiao, Leonid Sigal, and Y ong Jae Lee. Weakly-
supervised visual grounding of phrases with linguistic struc-tures. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 5945‚Äì5954, 2017. 2
[50] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon,
Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit:Semantic segmentation emerges from text supervision. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition , pages 18134‚Äì18144, 2022. 1
[51] Ziyan Yang, Kushal KaÔ¨Çe, Franck Dernoncourt, and Vi-
cente Ordonez. Improving visual grounding by encouragingconsistent gradient-based explanations. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition , pages 19165‚Äì19174, 2023. 1,2,5,6,7
[52] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe
Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, andChunjing Xu. Filip: Fine-grained interactive language-imagepre-training. arXiv preprint arXiv:2111.07783 , 2021. 1
[53] Licheng Y u, Patrick Poirson, Shan Yang, Alexander C Berg,
and Tamara L Berg. Modeling context in referring expres-sions. In Computer Vision‚ÄìECCV 2016: 14th European
Conference, Amsterdam, The Netherlands, October 11-14,
2016, Proceedings, Part II 14 , pages 69‚Äì85. Springer, 2016.
6
[54] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-
Fu Chang. Open-vocabulary object detection using captions.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 14393‚Äì14402, 2021.
1
[55] Feifei Zhai, Saloni Potdar, Bing Xiang, and Bowen Zhou.
Neural models for sequence chunking. In Proceedings of the
AAAI conference on artiÔ¨Åcial intelligence , 2017. 4
[56] Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan
Brandt, Xiaohui Shen, and Stan Sclaroff. Top-down neu-ral attention by excitation backprop. International Journal
of Computer Vision , 126(10):1084‚Äì1102, 2018. 5
13105
