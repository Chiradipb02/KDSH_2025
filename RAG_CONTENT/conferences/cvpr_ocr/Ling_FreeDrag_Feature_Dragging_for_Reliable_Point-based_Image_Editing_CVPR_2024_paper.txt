FreeDrag: Feature Dragging for Reliable Point-based Image Editing
Pengyang Ling1* Lin Chen1,2* Pan Zhang2Huaian Chen1†Yi Jin1†Jinjin Zheng1
1University of Science and Technology of China2Shanghai AI Laboratory
{lpyang27, chlin }@mail.ustc.edu.cn {anchen, jinyi08, jjzheng }@ustc.edu.cn zhangpan@pjlab.org.cn
Image & Edit DragGAN FreeDrag
 Image & Edit DragDiffusion FreeDrag
Figure 1. The comparison between the feature-centric FreeDrag and point-based DragGAN [ 33] and DragDiffusion[ 43]. Given an image
input, users can assign handle points ( redpoints) and target points ( blue points) to force the semantic positions of the handle points to reach
corresponding target points, and optional mask can also be provided by users to assign editing region.
Abstract
To serve the intricate and varied demands of image edit-
ing, precise and ﬂexible manipulation in image content is
indispensable. Recently, Drag-based editing methods have
gained impressive performance. However, these methods
predominantly center on point dragging, resulting in two
noteworthy drawbacks, namely “miss tracking”, where dif-
ﬁculties arise in accurately tracking the predetermined han-
dle points, and “ambiguous tracking”, where tracked points
are potentially positioned in wrong regions that closely re-
semble the handle points. To address the above issues,
we propose FreeDrag , a feature dragging methodology de-
signed to free the burden on point tracking. The Free-
Drag incorporates two key designs, i.e., template feature
via adaptive updating and line search with backtracking,
the former improves the stability against drastic content
change by elaborately controlling the feature updating scale
after each dragging, while the latter alleviates the misguid-
ance from similar points by actively restricting the search
area in a line. These two technologies together contribute
to a more stable semantic dragging with higher efﬁciency.Comprehensive experimental results substantiate that our
approach signiﬁcantly outperforms pre-existing methodolo-
gies, offering reliable point-based editing even in various
complex scenarios.
1. Introduction
The domain of image editing utilizing generative models
has gained substantial attention and witnessed remarkable
advancements in recent years [ 10,14,24,31,36,38]. In or-
der to effectively address the intricate and diverse demands
of image editing in real-world applications, it becomes im-
perative to achieve precise and ﬂexible manipulation of im-
age content. Consequently, researchers have proposed two
primary categories of methodologies in this domain: (1)
harnessing prior 3D models [ 8,12,46] or manual annota-
tions [ 2,17,26,34,42] to enhance control over generative
models, and (2) employing textual guidance for conditional
generative models [ 37,39,41]. Nevertheless, the former
category of methodologies often encounters challenges in
generalizing to novel assets, while the latter category ex-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
6860
hibits limitations in terms of precision and ﬂexibility when
it comes to spatial attribute editing.
To tackle these aforementioned limitations, a recent pio-
neering study, known as DragGAN [ 33], has emerged as a
remarkable contribution in the realm of precise image edit-
ing. This work has garnered signiﬁcant attention, primarily
due to its interactive point-based editing capability, termed
“drag” editing, which enables users to exert precise control
over the editing process by specifying pairs of handle and
target points on the given image. The DragGAN frame-
work introduces a two-step iterative process: (i) a motion
supervision step, which directs the handle points to migrate
towards their corresponding target positions, and (ii) a point
tracking step, which consistently tracks the relocated handle
points’ positions. In each iteration, the points derived from
the current iteration necessitate supervision from points of
the last iteration and are subsequently tracked for the next
iteration. We categorize this type of method, exempliﬁed by
DragGAN and its variant [ 43], as point dragging solutions.
Notwithstanding the praiseworthy achievements exhib-
ited by point dragging solution, there exist several issues.
One issue is miss tracking , whereby point dragging en-
counters difﬁculty in effectively tracking the desired han-
dle points. This issue arises particularly in highly curved
regions with a large perceptual path length, as observed in
latent space [ 21]. In such cases, the optimized image under-
goes drastic changes, leading to handle points in subsequent
iterations being positioned outside the intended search re-
gion. Additionally, in certain scenarios, miss tracking leads
to the disappearance of handle points, as shown in Figure
2. It is important to note that during miss tracking, the cu-
mulative error in the motion supervision step increases pro-
gressively as iterations proceed, owing to the misalignment
of tracked features. Another issue that arises is ambiguous
tracking , where the tracked points are situated within other
regions that bear resemblance to the handle points. This
predicament emerges when there are areas in the image that
possess similar features to the intended handle points, lead-
ing to ambiguity in the tracking process. (see Figure 3).
This issue introduces a potential challenge as it can mis-
guide the motion supervision process in subsequent itera-
tions, leading to inaccurate or misleading directions.
To remedy the aforementioned issues, we propose Free-
Drag , a feature dragging solution for interactive point-
based image editing. To address the miss tracking issue,
we introduce a template feature that is maintained for each
handle point to supervise the movements during the itera-
tive process. This template feature is implemented as an ex-
ponential moving average feature that dynamically adjusts
its weights based on the errors encountered in each itera-
tion. Even when miss tracking occurs in a speciﬁc iteration,
the maintained template feature remains intact, preventing
the optimized image from undergoing drastic changes. Tohandle the ambiguous tracking issue, we propose the line
search with backtracking. Line search restricts the move-
ments along a speciﬁc line connecting the original handle
point and the corresponding target point. This constraint
effectively reduces the presence of ambiguous points and
minimizes the potential misguidance of the movement di-
rection in subsequent iterations. Moreover, the backtrack-
ing mechanism enables prompt adjustment for motion plan
by effectively discriminating abnormal motion, thereby en-
hancing the reliability of the total movement process. In
light of the fact that the points in each iteration undergo
supervision from template features and do not necessitate
exacting tracking precision, we classify our approach as a
feature dragging solution. To summarize, our key contribu-
tions are as follows:
• We propose FreeDrag , a feature dragging solution for re-
liable point-based image editing that incorporates adap-
tive template features and line search with backtracking,
marking a signiﬁcant advancement in the ﬁeld of ﬂexible
and precise image editing.
• We propose FreeDragBench, a new evaluation dataset
with 2251 handmade dragging instructions that are tai-
lored for GAN-based dragging editing, equipped with a
new metric, which measures the editing accuracy of a pair
of symmetrical dragging instructions.
2. Related Work
2.1. Generative Adversarial Networks
Generative adversarial networks (GANs)[ 13] have main-
tained the dominant position in image generation for an ex-
tended period. Classical unconditional GANs [ 6], are de-
vised to learn the mapping function from low-dimension
random variables to realistic images that conform to the
distribution of training datasets. Typically, the Style-
GAN architecture [ 21–23,30], which employs a mapping
network for low-dimension representation disentanglement
and a synthesis network for photorealistic image genera-
tion, has made signiﬁcant success in both generation qual-
ity and ﬂexible style manipulation. Meanwhile, conditional
GANs have been developed to enable versatile applications
by infusing additional conditions, such as segmentation
maps[ 19,35], aerial photo[ 48], degraded images[ 9,18,50],
and 3D variables [ 7,11].
2.2. Diffusion Models
The emerging diffusion models [ 15,44], which conduct
gradual denoising procedures from Gaussian noises to natu-
ral images, have recently sparked a strong wave of more po-
tent image synthesis. Based on its promising generation ca-
pability, a series of versatile methods [ 3,20,25,47,51] are
developed to exceed the performance peaks of various gen-
eration tasks. Typically, Rombach et al. propose the Latent
6861
Diffusion Model (LDM)[ 40], which employs a pre-trained
auto-encoder for perceptual compression and then performs
high-quality sample in latent space, bringing a substantial
advancement in high-resolution image synthesis.
2.3. Point­based Image Editing
Given an image, interactive image editing aims to mod-
ify certain image content in response to speciﬁc user in-
put, such as text instructions [ 4,28,29,53], region mask
[27], and reference images [ 5,49]. The uniqueness of point-
based image editing lies in that the user input is a set of
point coordinates, and the generative models are expected to
achieve precise image content manipulation to match the in-
tent of users. For instance, Endo [ 10] devises a latent trans-
former architecture to learn the mapping between two latent
codes in StyleGAN. However, this framework necessitates
the aid of a pre-trained optical ﬂow network and demands a
training procedure tailored for each model, which limits its
practicability. Later, DragGAN [ 33] garners considerable
attention with remarkable performance, which performs a
cycle of point tracking and motion supervision in the fea-
ture map to persistently force the handle point to move to
the target point. This simple framework achieves impres-
sive performance and attracts subsequent works [ 32,43] for
better combination with the popular diffusion models.
Generally, the GAN-based dragging approaches achieve
superior dragging compared to diffusion-based approaches
but exhibit inferior real image inversion. The GAN-based
approaches beneﬁt from the attribute disentanglement of
StyleGAN, enhancing dragging capability. However, its
generative quality and real image inversion ability are com-
paratively limited. In contrast, diffusion models achieve
higher generative quality and superior real image inversion.
Nevertheless, it encounters challenges in balancing point
manipulation and appearance preservation due to the inter-
twined feature map, and demands more processing time.
3. Motivation
Given a set of nhandle points {p1,p2,p3...,pn}and a cor-
responding set of ntarget points {t1,t2,t3...,tn}, the objec-
tive of point-based dragging is to displace pito its respective
ti. Illustrated in Fig. 4, the widely adopted DragGAN [ 33]
accomplishes this objective through two sequential steps in
each motion: (i) Motion Supervision, wherein the current
handle point is consistently directed towards its target point
by leveraging the feature of itself. (ii) Point Tracking, in-
volving the search for the handle point in the proximity of
the handle point from the last motion. Denoting the initial
feature map as F0, the tracked handle point pk
ifor thek-th
motion possesses the most similar feature to F0(p0
i)in the
2D tracking area centered at pk−1
i.
While the point dragging pipeline depicted in Fig. 4
presents a promising solution for point-based image edit-
Originl k+5 k-th Step Originlk+5 k-th Step
Figure 2. Miss tracking of DragGAN [ 33] due to the drastic
change in layout (ﬁrst and second rows) and the disappearance
of handle points (third and last rows).
Image & Edit DragGAN
 k-th Step
 FreeDrag
Figure 3. Ambiguous tracking in DragGAN [ 33] due to the exis-
tence of similar structures.
it
0
ip1
ip2
ip3
ipPoint Tracking
Motion SupervisionPoint Tracking
Motion Supervision
Figure 4. Concept illustration of point dragging pipeline. pk
i
denotes the tracked position of i-th handle point in k-th motion
(p0
i=pi), andtiindicates the corresponding i-th target point.
ing, it is noted that it frequently encounters challenges, in-
cluding handle point loss, imprecise editing, and distorted
image generation in certain scenarios. These issues are at-
tributed to the intrinsic instability of point dragging, encom-
passing miss tracking and ambiguous tracking. (i) Miss
Tracking: This occurs in situations where point dragging
encounters difﬁculty in effectively tracking the designated
handle points. Given the presence of highly curved regions
with substantial perceptual path lengths, as discerned in la-
tent space [ 21], the optimized image undergoes signiﬁcant
alterations following motion supervision. Consequently, the
handle point pk+1
ideviates outside the intended search re-
gion ofpk
i, as shown in Figure 2, leading to miss tracking
in the point tracking step. In speciﬁc scenarios, pk+1
imay
completely vanish from the entire feature map, exempliﬁed
6862
Feature Dragging
Adaptive Updating
Line SearchFeature Dragging
Adaptive Updating
Line Search
) (1
i rh F) (2
i rh F) (3
i rh F
1
ih1
1h
1
3h
1
2h3
ih
2
ihk
ih1+k
ih
) (1+k
ik
ih h
k
ih1+k
ih0
ip
0
ip
0
ipit
it
it
(a) (b) (c)it
0
ip0
1p
0
2p
0
3p1
2p1t
2t3t
2
iTFigure 5. Illustration of proposed feature dragging pipeline. hk
idenotes the searched point in k-th drag, which lies in the line formed by p0
i
andti, andTk
idenotes the corresponding template feature. (a) Concept of feature dragging. (b) The coupling movement under multiple
points dragging. (c) The visualization of Eq. 9.
by the disappeared glasses in Figure 2. It is imperative to
underscore that during miss tracking, the cumulative error
in the motion supervision step progressively ampliﬁes with
iterations due to the misalignment of tracked features. (ii)
Ambiguous Tracking: This occurs when the tracked points
are positioned within other regions that bear resemblance to
the handle points. This challenge arises when there are ar-
eas in the image exhibiting features similar to the intended
handle points, such as the blue boundary lines and horse’s
hooves in Figure 3, which may misdirect the motion super-
vision process in subsequent iterations, resulting in inaccu-
rate or misleading directional adjustments.
4. Methodology
In light of the instability associated with point dragging,
which heavily depends on accurate point tracking in each
step, we introduce a feature dragging approach termed
FreeDrag , as illustrated in Fig. 5(a). Here, hk
irepresents
the target position in the k-th drag, and Fr(hk
i)signiﬁes the
feature aggregate centered at hk
iwith a radius rin the fea-
ture map F, which can be expressed as:
Fr(hk
i) =/summationdisplay
qi∈Ω(hk
i,r)F(qi). (1)
Here,Ω(hk
i,r)denotes the square patch centered at hk
iwith
a side length of 2r. In thek-th drag, we promote hk
ito be the
carrier of Tk
iby compelling the feature aggregate Fr(hk
i)to
closely align with the template feature Tk
i(as depicted by
thered line in Fig. 5(a)), i.e.,
Ldrag=n/summationdisplay
i=1/vextenddouble/vextenddoubleFr(hk
i)−Tk
i/vextenddouble/vextenddouble
1. (2)
In order to facilitate high-quality feature dragging, multi-
ple optimization steps are performed from the same position
hk
i, with consistent supervision as deﬁned in Eq. 2.
The template feature undergoes adaptive updating ac-
cording the quality of each dragging, as detailed in Section
4.1. This updated template feature guides the feature of the
handle point in the subsequent dragging. By gradually com-
pellinghk
ito approach ti, the template feature effectivelytransitions to the ﬁnal ti, indirectly encouraging the handle
point to move towards the ultimate position. Additionally,
we enforce constraints on hk
iand iterate to update the sub-
sequent handle point hk+1
ialong the line extending from p0
i
toti(as illustrated by the blue line in Fig. 5(a)). This ap-
proach not only provides a reliable movement direction but
also signiﬁcantly reduces the risk of misguidance arising
from potential similar points.
4.1. Template Features via Adaptive Updating
Concerning the template feature, it necessitates retaining
the feature of the initial handle point on one hand, while
on the other hand, it should undergo updates to accommo-
date reasonable geometric and appearance changes in each
dragging. Accordingly, we introduce an adaptive updating
strategy that permits a ﬂexible updating scale, enabling the
template feature to undergo few updates in chaotic situa-
tions and more updates in ﬁne conditions. Speciﬁcally, the
adaptive updating strategy for the template feature is formu-
lated as follows:
Tk+1
i=λk
i·Fr(hk
i)+(1−λk
i)·Tk
i. (3)
Here,λk
irepresents the coefﬁcient controlling the updat-
ing scale of the template feature in the k-th dragging. For
consistency, we speciﬁcally deﬁne λ0
i= 0,h0
i=p0
i, and
T0
i=Fr(p0
i). Intuitively, for the k-th dragging, a smaller
λk
iis employed for low-quality feature dragging. This aids
in maintaining Tk+1
i relatively constant in chaotic situa-
tions. Conversely, a larger λk
iis utilized for high-quality
feature dragging, promoting sufﬁcient updating of Tk+1
i in
ﬁne conditions.
For simplicity, the feature discrepancy of between
Fr(hk
i)andTk
iis denoted as L(i,k). Since Eq. 2is reused
in multiple optimization steps for each feature dragging,
we deﬁne L(i,k)at the initial/end optimization step in each
dragging as Lin
(i,k)andLen
(i,k), respectively. Accordingly.
Lin
(i,k)controls the difﬁculty of k-th feature dragging from
Tk
itoFr(hk
i), and a larger Lin
(i,k)indicates more arduous
challenge for feature dragging. While Len
(i,k)reﬂects the
quality of each feature dragging, i,e, a smaller Len
(i,k)means
6863
fewer discrepancy between Tk
iandFr(hk
i)at the last op-
timization step, which implies higher quality feature drag-
ging from Tk
itoFr(hk
i). Therefore, the adaptive coefﬁcient
λk
iin Eq. 3is devised as:
λk
i= (1+exp(α·(Len
(i,k)−β)))−1, (4)
whereαandβdenote two positive constants, and exp(·)
represents the exponential function. Given a hyperparam-
eterl, we determine αandβby considering the following
typical scenarios: (i) the well-optimized case, where we set
Len
(i,k)= 0.2·lwithλ= 0.5; and (ii) the ill-optimized case,
where we set Len
(i,k)= 0.8·lwithλ= 0.1,i.e.,
0.5 = (1+ exp(α·(0.2·l−β)))−1, (5)
0.1 = (1+ exp(α·(0.8·l−β)))−1. (6)
Solving the equation yields α= ln(9)/(0.6·l)andβ=
0.2·l. It is noteworthy that we impose a constraint on the
maximum value of λto mitigate the potential impact of in-
correct updating.
4.2. Line Search with Backtracking
For the target position hk
iin thek-th dragging, we contem-
plate its localization from two perspectives: i) Reliable mo-
tion direction; ii) Appropriate feature discrepancy at the be-
ginning of each drag, denoted as Lin
(i,k). A too small value of
Lin
(i,k)fails to furnish adequate discrepancy in Eq. 2for gra-
dient optimization, while an excessively large Lin
(i,k)height-
ens the risk of unsuccessful feature dragging.
From the ﬁrst goal, illustrated in Fig. 5(a), we con-
strainthk
ito the line extending from p0
itoti. This ap-
proach not only ensures a reliable movement direction but
also markedly diminishes the risk of misguidance stemming
from potential similar points. As for the second goal, point
localization is conducted based on both feature discrepancy
and motion distance, expressed as:
hk+1
i=S(hk
i,ti,Tk+1
i,d,l) (7)
= argmin
qi∈π(hk
i,ti,d)/vextenddouble/vextenddouble∥Fr(qi)−Tk+1
i∥1−l/vextenddouble/vextenddouble
1,(8)
wherelanddare two hyperparameters that control initial
feature distance Lin
(i,k)and maximum single movement dis-
tance, respectively, and π(hk
i,ti,d)represents the point set,
which includes hk
i+j·ti−hk
i
|ti−hk
i|2withj= 0.1·d,0.2·d,...,d .
Additionally, as depicted in Fig. 5(b), during the joint
optimization of multiple points dragging, the motion direc-
tion of a speciﬁc point may be inﬂuenced by the overall
trend. This can result in the handle point deviating from
the target point in certain steps. For instance, in compari-
son top0
2, the handle point p1
2is farther away from h1
2. To
address this issue, we integrate a backtracking mechanism
to identify such abnormal movements, facilitating prompt
adjustments for the subsequent dragging plan. Concretely,backtracking is implemented by introducing two additional
options for the dragging plan: frozen and fallback the point,
which can be expressed as:
hk+1
i=

S(hk
i,ti,Tk+1
i,d,l), if Len
(i,k)≤0.5·l
hk
i, elif Len
(i,k)≤Lin
(i,k)
S(hk
i−d·ti−hk
i
∥ti−hk
i∥2,ti,Tk+1
i,2d,0),else(9)
For better comprehension, Eq. 9has been visually rep-
resented in Fig. 5(c). To elaborate, the ﬁrst scenario corre-
sponds to a normal high-quality optimization, where hk+1
i
closer to tiis assigned for further movement (depicted by
the blue line in Fig. 5(c)). The second scenario corresponds
to insufﬁcient feature dragging, where hk
iis reused as tk+1
i
for continued feature dragging towards the same point. In
the exceptional case, i.e.,Len
(i,k)> max/braceleftBig
0.5·l,Lin
(i,k)/bracerightBig
,
we setl= 0 and double the search range (illustrated by
the yellow line in Fig. 5(c)) to immediately locate the point
closest to the template feature Tk+1
i, promptly avoiding de-
terioration.
4.3. Termination Signal
For each feature dragging towards hk
i, the maximum op-
timization step of each feature dragging is set as 5. To
enhance efﬁciency, we pause the optimization process if
Len
(i,k)already falls below 0.5·l. The ﬁnal termination
signal is obtained by determining if the remaining distance
||hk
i−ti||2≤2.
4.4. Directional Editing
If the optional binary mask is provided by users, the mask
loss can be obtained as:
Lmask=∥(F0−F)⊙(1−M)∥1, (10)
whereF0denotes the initial feature without any dragging,
and⊙is the element-wise multiplication. The total training
loss can be expressed as:
Ltotal=Ldrag+γ·Lmask. (11)
whereγis the hyperparameter for loss balance.
5. Experiments
Since the proposed feature dragging pipeline is constructed
based on the feature map, thus it can be effortlessly im-
plemented on StyleGAN2 models [ 22] and latent diffusion
models[ 40] by extracting corresponding feature maps.
5.1. Implementation Details
Parameter rin Eq. 1is set as 3, and parameter γin
Eq. 11is set as 10. For StyleGAN2 models, the feature
map is extracted after the 6th block and the optimization
for latent code is conducted in the extended W+space[ 1].
We setl= 0.4andd= 4 for elephant and lion mod-
els that are observed to likely perform larger movement
6864
in a single optimization step, and l= 0.3andd= 3 for
other StyleGAN2 models. For diffusion models, following
DragDiffusion[ 43], we ﬁne-tune a LoRA [ 16] with rank of
16 on the UNet parameters for each image, which is used for
both image inversion and dragging editing, and the feature
map is extracted from the U-Net. We also replace the fea-
ture map with diffusion latent in Eq. 10to keep consistent
with DragDiffusion. The parameters landdare empirically
set as 1 and 5 in diffusion models, respectively. To reﬂect
the performance of different dragging pipelines themselves,
FreeDrag and DragDiffusion utilize the same LoRA param-
eters for the same image. To fully capture the potential of
each method, the max step is set as 300 for all methods.
5.2. Dataset Construction
Since there is no public dataset to evaluate the drag-based
editing in StyleGAN2, we propose FreeDragBench, which
is the ﬁrst dataset customized for GAN-based dragging edit-
ing. As presented in Table 1, FreeDragBench consists of
600 images randomly generated by ﬁve different Style-
GAN2 models, equipped with 2251 dragging instructions
tailored for image content (including the editing in the pose,
size, position, etc.), as shown in Fig. 6.
Furthermore, since the ground-truth corresponding to
dragging instruction is not available, we propose a new met-
ric to measure the accuracy of dragging editing, i.e., the
Content Consistency under Symmetrical Dragging (CCSD).
To be speciﬁc, as depicted Fig. 7, we reuse the reverse side
of the original dragging instruction to construct a symmet-
rical dragging instruction pair and measure the content con-
sistency under this symmetrical dragging instruction pair.
To avoid penalizing stochastic elements with no effect on
perception, LPIPS[ 52] is used for similarity measurement.
A low CCSD value requires accurate dragging in symmet-
rical editing, which could be used as an effective measure-
ment metric in the absence of ground-truth.
5.3. Qualitative Evaluation
As depicted in Fig. 9, FreeDrag successfully avoids the ab-
normal disappearance of handle points ( e.g., the vanished
eyes in the human face, and the mouth of cartoon character
and cat), showcasing its superiority in ﬁne-detail editing.
Meanwhile, FreeDrag achieves better stability against dras-
tic content distortions (see the eye of the horse), steadily
attaining the editing intent. Moreover, FreeDrag exhibits
better robustness in handling similar points, resulting in re-
liable and precise dragging editing, as demonstrated in the
examples of the third row. Additionally, FreeDrag effec-
tively mitigates the potential misguidance during optimiza-
tion steps, leading to more natural and coherent editing re-
sults, as observed in the last row in Fig. 9.
For image editing with the combination of diffusion
models, FreeDrag also attains impressive performance. AsCategory Face Cat Car Horse Elephant
Image number 200 100 100 100 100
Instruction number 1068 406 337 227 213
Table 1. Statistic of images and instructions of FreeDragBench.
Figure 6. Several examples in the proposed FreeDragBench.
Drag
Calculate Similarity via LPIPSReverse Points Drag
Figure 7. Visualization of the proposed CCSD metric.
Segmentation of original image Drawn segmentation Segmentation of EditGAN result
Original image Result of EditGANDragging instruction
Result of FreeDrag
Figure 8. Comparison with EditGAN[ 26] in editing accuracy.
shown in Fig. 10, FreeDrag outperforms DragDiffusion in
both editing accuracy (see the examples from the ﬁrst to
third columns) and structure preservation (see the examples
from the fourth to last columns), thus achieving superior
quality of point-based dragging editing.
Additionally, we further conduct a comparison with
EditGAN[ 26], which performs ﬁne-grained editing by
drawing object-level masks. As shown in Fig. 8, FreeDrag
better follows editing instructions.
5.4. Quantitative Evaluation
For quantitative evaluation, we implement comparison
with DragGAN and DragDiffusion in FreeDragBench and
DragBench[ 43], respectively. Speciﬁcally, for the compari-
son in FreeDragBench, we use FID and the proposed CCSD
to evaluate the image quality and editing accuracy, respec-
tively. For DragBench that owns images with varying res-
olution, we follow the setting in DragDiffusion[ 43],i.e.,
Mean Distance (MD) for dragging accuracy measurement
and LPIPS [ 52] for image ﬁdelity evaluation. The mean
6865
Image & Edit DragGAN FreeDrag Image & Edit DragGAN FreeDrag Image & Edit DragGAN FreeDrag Image & Edit DragGAN FreeDrag
Figure 9. Demonstration of the edited results of FreeDrag and DragGAN[ 32] in eight different StyleGAN2 models.
Category Face Cat Car Horse Elephant Time
Metric FID CCSD FID CCSD FID CCSD FID CCSD FID CCSD Second
DragGAN[ 33] 38.07 0.83 19.14 0.56 36.36 0.73 21.90 1.20 11.17 1.19 8.26
FreeDrag 29.50 0.35 15.67 0.23 33.50 0.37 21.18 0.68 10.86 0.82 2.74
Table 2. Quantitative evaluation on FreeDragBench. A lower FID score indicates better ﬁdelity in single dragging editing, while lower
CCSD(×10)scores imply higher accuracy in two symmetrical dragging editing. The time is calculated on Face category.
DragBench MD ↓LPIPS(×10)↓Time (Sec) ↓
DragDiffusion[ 43] 38.76 1.38 71.77
FreeDrag 33.49 1.02 63.62
Table 3. Quantitative evaluation on DragBench. The time con-
sumption is computed on DragBench which only includes the
dragging process because a ﬁne-tuned LoRA can be used for mul-
tiple image editing with different dragging instructions.
distance is obtained by calculating the corresponding rela-
tionship of points between the original image and the edited
image based on DIFT[ 45].
As presented in Table 2, FreeDrag consistently attains
higher scores in all categories, which further validates its
superiority in achieving precise dragging editing and bet-
ter image ﬁdelity preservation. Moreover, it can be ob-
served that FreeDrag gains signiﬁcant improvement in time
consumption, which can be attributed to that the proposed
line search effectively alleviates the interference of similarMetric w/o updating w/o backtracking Ours
CCSD(×10) 0.82 0.52 0.35
Table 4. Quantitative ablation on human face model.
points and thus successfully avoids unrewarding dragging
steps, allowing for higher efﬁciency.
For the quantitative evaluation in diffusion models, we
utilize the public DragBench dataset [ 43] that is customized
for diffusion-based dragging evaluation. The results of
DragDiffusion and FreeDrag are presented in Table. 3. It
is observed that FreeDrag outperforms DragDiffusion with
higher dragging accuracy and lower time-consumption, im-
plying a promising potential for versatile applications.
5.5. Ablation Study
The parameters landddetermine the initial feature discrep-
ancy and maximum single movement distance, thus con-
trolling the style of total dragging editing. Speciﬁcally, a
too small lordimplies a more conservative editing strat-
6866
Image & Edit DragDiffusion FreeDrag
Figure 10. Demonstration of real image editing results of FreeDrag and DragDiffusion[ 32].
(a) (b) (c) (d)
Figure 11. The edited results by using different parameters. (a)
Original images with dragging instructions. (b) Edited results with
{l= 0.15,d= 1.5}. (c) Edited results with {l= 0.3,d= 3}. (d)
Edited results with {l= 0.45,d= 4.5}.
Image & Edit w/o updating FreeDrag w/o backtracking Image & Edit w/o updating FreeDrag w/o backtracking
Figure 12. Illustration of the effect of adaptive updating strategy
in template feature and backtracking mechanism in line search.
egy, which prefers small motion and refuses large updating
scale, thus failing to reach the target point in limited opti-
mization steps, as shown in Fig. 11(b). In contrast, a too
largelordmeans a more impulsive editing strategy, which
appears to accept large updating scale and larger movement
distance and thus increases the risk of coarse feature up-
dating, resulting in damage to editing accuracy, as can be
observed in Fig. 11(d).
Furthermore, we assign λ= 0 in Eq. 3to obtain a sta-
tionary template feature to evaluate the effect of adaptive
updating strategy and adopt Eq. 7rather than Eq. 9to eval-uate the effect of backtracking mechanism. As can be ob-
served in Fig. 12, both of them play necessary roles for
better editing quality. The quantitative ablation in Table 4
also validates their necessity.
6. Conclusion
In this work, we propose FreeDrag, a novel feature drag-
ging framework for reliable point-based image editing. By
incorporating an adaptive template feature, FreeDrag allows
for ﬂexible control in the scale of each feature updating,
which contributes to stronger stability under drastic content
change, resulting in a better immunity against point miss-
ing. Meanwhile, the established line search with backtrack-
ing effectively mitigates the misguidance caused by simi-
lar points and allows timely adjustment for motion plan by
effectively discriminating abnormal motion, leading to re-
liable and continuous movements towards the ﬁnal target
point. Extensive experiments demonstrate the reliability of
FreeDrag in precise semantic dragging and stable structure
preservation, indicating superior editing quality.
Acknowledgement. This work is supported in part by the
Postdoctoral Fellowship Program of CPSF GZB20230713,
in part by the Anhui Provincial Key Research and De-
velopment Plan 202304a05020072, in part by the Fun-
damental Research Funds for the Central Universities
WK2090000065, and in part by the Anhui Provincial Nat-
ural Science Foundation 2308085QF226. This work is
partially supported by the National Key R&D Program of
China (2022ZD0160201), and Shanghai Artiﬁcial Intelli-
gence Laboratory.
6867
References
[1] Rameen Abdal, Yipeng Qin, and Peter Wonka. Im-
age2stylegan: How to embed images into the stylegan latent
space? In Proceedings of the IEEE/CVF international con-
ference on computer vision , pages 4432–4441, 2019. 5
[2] Rameen Abdal, Peihao Zhu, Niloy J Mitra, and Peter Wonka.
Styleﬂow: Attribute-conditioned exploration of stylegan-
generated images using conditional continuous normalizing
ﬂows. ACM Transactions on Graphics (ToG) , 40(3):1–21,
2021. 1
[3] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended
latent diffusion. ACM Transactions on Graphics (TOG) , 42
(4):1–11, 2023. 2
[4] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structpix2pix: Learning to follow image editing instructions.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 18392–18402, 2023.
3
[5] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao,
and Hengshuang Zhao. Anydoor: Zero-shot object-level im-
age customization. arXiv preprint arXiv:2307.09481 , 2023.
3
[6] Antonia Creswell, Tom White, Vincent Dumoulin, Kai
Arulkumaran, Biswa Sengupta, and Anil A Bharath. Gen-
erative adversarial networks: An overview. IEEE signal pro-
cessing magazine , 35(1):53–65, 2018. 2
[7] Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, and Xin
Tong. Disentangled and controllable face image genera-
tion via 3d imitative-contrastive learning. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 5154–5163, 2020. 2
[8] Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, and Xin
Tong. Disentangled and controllable face image genera-
tion via 3d imitative-contrastive learning. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 5154–5163, 2020. 1
[9] Yu Dong, Yihao Liu, He Zhang, Shifeng Chen, and Yu
Qiao. Fd-gan: Generative adversarial networks with fusion-
discriminator for single image dehazing. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence , pages 10729–
10736, 2020. 2
[10] Yuki Endo. User-controllable latent transformer for stylegan
image layout editing. In Computer Graphics Forum , pages
395–406. Wiley Online Library, 2022. 1,3
[11] Partha Ghosh, Pravir Singh Gupta, Roy Uziel, Anurag Ran-
jan, Michael J Black, and Timo Bolkart. Gif: Generative
interpretable faces. In 2020 International Conference on 3D
Vision (3DV) , pages 868–878. IEEE, 2020. 2
[12] Partha Ghosh, Pravir Singh Gupta, Roy Uziel, Anurag Ran-
jan, Michael J Black, and Timo Bolkart. Gif: Generative
interpretable faces. In 2020 International Conference on 3D
Vision (3DV) , pages 868–878. IEEE, 2020. 1
[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. Advances in
neural information processing systems , 27, 2014. 2[14] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kﬁr Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
age editing with cross attention control. arXiv preprint
arXiv:2208.01626 , 2022. 1
[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840–6851, 2020. 2
[16] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021. 6
[17] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adver-
sarial networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1125–1134,
2017. 1
[18] Yeying Jin, Wenhan Yang, and Robby T Tan. Unsupervised
night image enhancement: When layer decomposition meets
light-effects suppression. In European Conference on Com-
puter Vision , pages 404–421. Springer, 2022. 2
[19] Chanyong Jung, Gihyun Kwon, and Jong Chul Ye. Ex-
ploring patch-wise semantic relation for contrastive learn-
ing in image-to-image translation tasks. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 18260–18269, 2022. 2
[20] Animesh Karnewar, Andrea Vedaldi, David Novotny, and
Niloy J Mitra. Holodiffusion: Training a 3d diffusion model
using 2d images. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
18423–18433, 2023. 2
[21] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 4401–4410, 2019. 2,3
[22] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improv-
ing the image quality of stylegan. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 8110–8119, 2020. 5
[23] Tero Karras, Miika Aittala, Samuli Laine, Erik H ¨ark¨onen,
Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free
generative adversarial networks. Advances in Neural Infor-
mation Processing Systems , 34:852–863, 2021. 2
[24] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen
Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:
Text-based real image editing with diffusion models. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 6007–6017, 2023. 1
[25] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Dif-
fusionclip: Text-guided diffusion models for robust image
manipulation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 2426–
2435, 2022. 2
[26] Huan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim,
Antonio Torralba, and Sanja Fidler. Editgan: High-precision
semantic image editing. Advances in Neural Information
Processing Systems , 34:16331–16345, 2021. 1,6
6868
[27] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher
Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting
using denoising diffusion probabilistic models. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 11461–11471, 2022. 3
[28] Yueming Lyu, Tianwei Lin, Fu Li, Dongliang He, Jing
Dong, and Tieniu Tan. Deltaedit: Exploring text-free train-
ing for text-driven image manipulation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 6894–6903, 2023. 3
[29] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided
image synthesis and editing with stochastic differential equa-
tions. arXiv preprint arXiv:2108.01073 , 2021. 3
[30] Ron Mokady, Omer Tov, Michal Yarom, Oran Lang, Inbar
Mosseri, Tali Dekel, Daniel Cohen-Or, and Michal Irani.
Self-distilled stylegan: Towards generation from internet
photos. In ACM SIGGRAPH 2022 Conference Proceedings ,
pages 1–9, 2022. 2
[31] Ron Mokady, Amir Hertz, Kﬁr Aberman, Yael Pritch, and
Daniel Cohen-Or. Null-text inversion for editing real im-
ages using guided diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 6038–6047, 2023. 1
[32] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and
Jian Zhang. Dragondiffusion: Enabling drag-style manipula-
tion on diffusion models. arXiv preprint arXiv:2307.02421 ,
2023. 3,7,8
[33] Xingang Pan, Ayush Tewari, Thomas Leimk ¨uhler, Lingjie
Liu, Abhimitra Meka, and Christian Theobalt. Drag your
GAN: Interactive point-based manipulation on the generative
image manifold. arXiv preprint arXiv:2305.10973 , 2023. 1,
2,3,7
[34] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan
Zhu. Semantic image synthesis with spatially-adaptive nor-
malization. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 2337–2346,
2019. 1
[35] Taesung Park, Alexei A Efros, Richard Zhang, and Jun-
Yan Zhu. Contrastive learning for unpaired image-to-image
translation. In Computer Vision–ECCV 2020: 16th Euro-
pean Conference, Glasgow, UK, August 23–28, 2020, Pro-
ceedings, Part IX 16 , pages 319–345. Springer, 2020. 2
[36] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun
Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image
translation. arXiv preprint arXiv:2302.03027 , 2023. 1
[37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022. 1
[38] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel
Cohen-Or. Pivotal tuning for latent-based editing of real im-
ages. ACM Transactions on graphics (TOG) , 42(1):1–13,
2022. 1
[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings ofthe IEEE/CVF conference on computer vision and pattern
recognition , pages 10684–10695, 2022. 1
[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684–10695, 2022. 3,5
[41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems , 35:36479–36494, 2022. 1
[42] Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. In-
terpreting the latent space of gans for semantic face editing.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 9243–9252, 2020. 1
[43] Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vin-
cent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffu-
sion models for interactive point-based image editing. arXiv
preprint arXiv:2306.14435 , 2023. 1,2,3,6,7
[44] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 2
[45] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng
Phoo, and Bharath Hariharan. Emergent correspondence
from image diffusion. arXiv preprint arXiv:2306.03881 ,
2023. 7
[46] Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian
Bernard, Hans-Peter Seidel, Patrick P ´erez, Michael Zoll-
hofer, and Christian Theobalt. Stylerig: Rigging style-
gan for 3d control over portrait images. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 6142–6151, 2020. 1
[47] Xingqian Xu, Zhangyang Wang, Gong Zhang, Kai Wang,
and Humphrey Shi. Versatile diffusion: Text, images and
variations all in one diffusion model. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 7754–7765, 2023. 2
[48] Yanwu Xu, Shaoan Xie, Wenhao Wu, Kun Zhang, Mingming
Gong, and Kayhan Batmanghelich. Maximum spatial pertur-
bation consistency for unpaired image-to-image translation.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 18311–18320, 2022.
2
[49] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin
Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by
example: Exemplar-based image editing with diffusion mod-
els. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 18381–18391,
2023. 3
[50] Yuntong Ye, Changfeng Yu, Yi Chang, Lin Zhu, Xi-le Zhao,
Luxin Yan, and Yonghong Tian. Unsupervised deraining:
Where contrastive learning meets self-similarity. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 5821–5830, 2022. 2
[51] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
6869
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3836–3847, 2023. 2
[52] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 586–595, 2018. 6
[53] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris N
Metaxas, and Jian Ren. Sine: Single image editing with text-
to-image diffusion models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 6027–6037, 2023. 3
6870
