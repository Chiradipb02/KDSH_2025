A Simple Recipe for Language-guided Domain Generalized Segmentation
Mohammad Fahes1Tuan-Hung Vu1,2Andrei Bursuc1,2Patrick P ´erez3Raoul de Charette1
1Inria2Valeo.ai3Kyutai
https://astra-vision.github.io/FAMix
Abstract
Generalization to new domains not seen during training
is one of the long-standing challenges in deploying neu-
ral networks in real-world applications. Existing gener-
alization techniques either necessitate external images for
augmentation, and/or aim at learning invariant represen-
tations by imposing various alignment constraints. Large-
scale pretraining has recently shown promising generaliza-
tion capabilities, along with the potential of binding differ-
ent modalities. For instance, the advent of vision-language
models like CLIP has opened the doorway for vision models
to exploit the textual modality. In this paper, we introduce
a simple framework for generalizing semantic segmentation
networks by employing language as the source of random-
ization. Our recipe comprises three key ingredients: (i) the
preservation of the intrinsic CLIP robustness through mini-
mal fine-tuning, (ii) language-driven local style augmenta-
tion, and (iii) randomization by locally mixing the source
and augmented styles during training. Extensive experi-
ments report state-of-the-art results on various generaliza-
tion benchmarks. Code is accessible on the project page1.
1. Introduction
A prominent challenge associated with deep neural net-
works is their constrained capacity to generalize when con-
fronted with shifts in data distribution. This limitation is
rooted in the assumption of data being independent and
identically distributed, a presumption that frequently proves
unrealistic in real-world scenarios. For instance, in safety-
critical applications like autonomous driving, it is impera-
tive for a segmentation model to exhibit resilient generaliza-
tion capabilities when dealing with alterations in lighting,
variations in weather conditions, and shifts in geographic
location, among other considerations.
To address this challenge, domain adaptation [13, 18, 33,
34, 47, 48] has emerged; its core principle revolves around
1https://astra-vision.github.io/FAMixaligning the distributions of both the source and target do-
mains. However, DA hinges on having access to target data,
which may not always be available. Even when accessible,
this data might not encompass the full spectrum of distribu-
tions encountered in diverse real-world scenarios. Domain
generalization [31, 32, 49, 52, 62, 63] overcomes this lim-
itation by enhancing the robustness of models to arbitrary
and previously unseen domains.
The training of segmentation networks is often backed
by large-scale pretraining as initialization for the feature
representation. Until now, to the best of our knowledge, do-
main generalization for semantic segmentation (DGSS) net-
works [7, 19, 23, 24, 29, 36, 37, 51, 53, 58] are pretrained
with ImageNet [9]. The underlying concept is to transfer
the representations from the upstream task of classification
to the downstream task of segmentation.
Lately, contrastive language image pretraining (CLIP)
[22, 39, 55, 56] has demonstrated that transferable visual
representations could be learned from the sole supervi-
sion of loose natural language descriptions at very large
scale. Subsequently, a plethora of applications have been
proposed using CLIP [39], including zero-shot semantic
segmentation [30, 59], image editing [27], transfer learn-
ing [10, 40], open-vocabulary object detection [16], few-
shot learning [64, 65] etc. A recent line of research pro-
poses fine-tuning techniques to preserve the robustness of
CLIP under distribution shift [15, 26, 45, 50], but they are
limited to classification.
In this paper, we aim at answering the following ques-
tion: How to leverage CLIP pretraining for enhanced do-
main generalization for semantic segmentation? The moti-
vation for rethinking DGSS with CLIP is twofold. On one
hand, distribution robustness is a notable characteristic of
CLIP [12]. On the other hand, the language modality of-
fers an extra source of information compared to unimodal
pretrained models.
A direct comparison of training two segmentation mod-
els under identical conditions but with different pretrain-
ing, i.e. ImageNet vs. CLIP, shows that CLIP pretraining
does not yield promising results. Indeed, Tab. 1 shows that
fine-tuning CLIP-initialized network performs worse than
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
23428
Pretraining C B M S AN AS AR AF Mean
ImageNet 29.04 32.17 34.26 29.87 4.36 22.38 28.34 26.76 25.90
CLIP 16.81 16.31 17.80 27.10 2.95 8.58 14.35 13.61 14.69
Table 1. Comparison of ImageNet and CLIP pretraining
for out-of-distribution semantic segmentation. The network is
DeepLabv3+ with ResNet-50 as backbone. The models are trained
on GTA V and the performance (mIoU %) is reported on Cityscapes
(C), BDD-100K ( B), Mapillary ( M), Synthia ( S), and ACDC Night
(AN), Snow ( AS), Rain ( AR) and Fog ( AF).
its ImageNet counterpart on out-of-distribution (OOD) data.
This raises doubts about the suitability of CLIP pretrain-
ing for DGSS and indicates that it is more prone to overfit-
tingthe source distribution at the expense of degrading its
original distributional robustness properties. Note that both
models converge and achieve similar results on in-domain
data. More details are provided in Appendix A.
This paper shows that we can prevent such behavior with
a simple recipe involving minimal fine-tuning, language-
driven style augmentation, and mixing. Our approach is
coined FAMix, for Freeze, Augment and Mix .
It was recently argued that fine-tuning might distort the
pretrained representations and negatively affect OOD gen-
eralization [26]. To maintain the integrity of the represen-
tation, one extreme approach is to entirely freeze the back-
bone. However, this can undermine representation adapt-
ability and lead to subpar OOD generalization. As a middle-
ground strategy balancing adaptation and feature preser-
vation, we suggest minimal fine-tuning of the backbone,
where a substantial portion remains frozen, and only the fi-
nal layers undergo fine-tuning.
For generalization, we show that rethinking
MixStyle [62] leads to significant performance gains.
As illustrated in Fig. 1, we mix the statistics of the original
source features with augmented statistics mined using
language. This helps explore styles beyond the source
distribution at training time without using additional image.
We summarize our contributions as follows:
• We propose a simple framework for DGSS based on min-
imal fine-tuning of the backbone and language-driven
style augmentation. To the best of our knowledge, we
are the first to study DGSS with CLIP pretraining.
• We propose language-driven class-wise local style aug-
mentation. We mine class-specific local statistics using
prompts that express random styles and names of patch-
wise dominant classes. During training, randomization is
performed through patch-wise style mixing of the source
and mined styles.
• We conduct careful ablations to show the effectiveness
of FAMix. Our framework outperforms state-of-the-art
approaches in single and multi-source DGSS settings.
Mixed statistics
Source statistics
Augmented statisticsFigure 1. Mixing strategies. (Left) MixStyle [62] consists of
a linear mixing between the feature statistics of the source do-
main(s) Ssamples. ( Right ) We apply an augmentation A(.)on
the source domain statistics, then perform linear mixing between
original and augmented statistics. Intuitively, this enlarges the sup-
port of the training distribution by leveraging statistics beyond the
source domain(s), as well as discovering intermediate domains.
A(.)could be a language-driven or Gaussian noise augmentation,
and we show that the former leads to better generalization results.
2. Related works
Domain generalization (DG). The goal of DG is to train,
from a single or multiple source domains, models that per-
form well under arbitrary domain shifts. The DG litera-
ture spans a broad range of approaches, including adversar-
ial learning [32, 57], meta-learning [4, 38], data augmen-
tation [60–62] and domain-invariant representation learn-
ing [1, 3, 7, 25]. We refer the reader to [49, 63] for com-
prehensive surveys on DG.
Domain generalization with CLIP. CLIP [39] exhibits
a remarkable distributional robustness [12]. Nevertheless,
fine-tuning comes at the expense of sacrificing generaliza-
tion. Kumar et al. [26] observe that full fine-tuning can
distort the pretrained representation, and propose a two-
stage strategy, consisting of training a linear probe with
a frozen feature extractor, then fine-tuning both. Worts-
man et al. [50] propose ensembling the weights of zero-
shot and fine-tuned models. Goyal et al. [15] show that
preserving the pretraining paradigm ( i.e. contrastive learn-
ing) during the adaptation to the downstream task improves
both in-domain (ID) and OOD performance without multi-
step fine-tuning or weight ensembling. CLIPood [45] in-
troduces margin metric softmax training objective and Beta
moving average for optimization to handle both open-class
and open-domain at test time. On the other hand, distri-
butional robustness could be improved by training a small
amount of parameters on top of a frozen CLIP backbone in
a teacher-student manner [21, 28]. Other works show that
specialized prompt ensembling and/or image ensembling
strategies [2, 14] coupled with label augmentation using the
WordNet hierarchy improve robustness in classification.
Domain Generalized Semantic Segmentation. DGSS
methods could be categorized into three main groups: nor-
malization methods, domain randomization (DR) and in-
23429
building 
road sky 
PIN
Local Style MiningStyle bank
Training
...AdaINrandom style
extract styleMixsave style
Augment
Freeze
Segmentercosine
distance
loss......
1 per classCosmic voyage style roadPop art popularity style skyUrban grit style vegetationRetro futurism style buildingcross
entropyvegetation 
Figure 2. Overall process of FAMix. FAMix consists of two steps. ( Left)Local style mining consists of dividing the low-level feature
activations into patches, which are used for style mining using Prompt-driven Instance Normalization (PIN) [10]. Specifically, for each
patch, the dominant class is queried from the ground truth, and the mined style is added to corresponding class-specific style bank.
(Right )Training the segmentation network is performed with minimal fine-tuning of the backbone. At each iteration, the low-level feature
activations are viewed as grids of patches. For each patch, the dominant class is queried using the ground truth, then a style is sampled from
the corresponding style bank. Style randomization is performed by normalizing each patch in the grid by its statistics, and transferring the
new style which is a mixing between the original style and the sampled one. The network is trained using only a cross-entropy loss.
variant representation learning. Normalization methods aim
at removing style contribution from the representation. For
instance, IBN-Net [36] shows that Instance Normalization
(IN) makes the representation invariant to variations in the
scene appearance (e.g., change of colors, illumination, etc.),
and that combining IN and batch normalization (BN) helps
the synthetic-to-real generalization. SAN & SAW [37] pro-
poses semantic-aware feature normalization and whitening,
while RobustNet [7] proposes an instance selective whiten-
ing loss, where only feature covariances that are sensitive to
photometric transformations are whitened. DR aims instead
at diversifying the data during training. Some methods use
additional data for DR. For example, WildNet [29] uses Im-
ageNet [9] data for content and style extension learning,
while TLDR [24] proposes learning texture from random
style images. Other methods like SiamDoGe [51] perform
DR solely by data augmentation, using a Siamese [6] struc-
ture. Finally in the invariant representation learning group,
SPC-Net [19] builds a representation space based on style
and semantic projection and clustering, and SHADE [58]
regularizes the training with a style consistency loss and a
retrospection consistency loss.
3. Method
FAMix proposes an effective recipe for DGSS through the
blending of simple ingredients. It consists of two stages (see
Fig. 2): (i) Local style mining from language (Sec. 3.2);
(ii) Training of a segmentation network with minimal fine-
tuning and local style mixing (Sec. 3.3). In Fig. 2 and in the
following, CLIP-I1 denotes the stem layers andLayer1
of CLIP image encoder, CLIP-I2 the remaining layers ex-
cluding the attention pooling, and CLIP-T the text encoder.We start with some preliminary background knowledge,
introducing AdaIN and PIN which are essential to our work.
3.1. Preliminaries
Adaptive Instance Normalization (AdaIN). For a feature
mapf∈Rh×w×c, AdaIN [20] shows that the channel-wise
mean µ∈Rcand standard deviation σ∈Rccapture in-
formation about the style of the input image, allowing style
transfer between images. Hence, stylizing a source feature
fswith an arbitrary target style (µ(ft), σ(ft))reads:
AdaIN (fs,ft) =σ(ft)fs−µ(fs)
σ(fs)
+µ(ft), (1)
withµ(·)andσ(·)the mean and standard deviation of input
feature; multiplications and additions being element-wise.
Prompt-driven Instance Normalization (PIN). PIN was
introduced for prompt-driven zero-shot domain adaptation
in PØDA [10]. It replaces the target style (µ(ft), σ(ft))in
AdaIN (1) with two optimizable variables (µ,σ)guided
by a single prompt in natural language. The rationale is to
leverage a frozen CLIP [39] to mine visual styles from the
prompt representation in the shared space. Given a prompt
Pand a feature map fs, PIN reads as:
PIN (P)(fs) =σfs−µ(fs)
σ(fs)
+µ, (2)
where µandσare optimized using gradient descent, such
that the cosine distance between the visual feature represen-
tation and the prompt representation is minimized.
Different from PØDA which mines styles globally with
a predetermined prompt describing the target domain, we
make use of PIN to mine class-specific styles using local
patches of the features, leveraging random style prompts .
23430
Further, we show the effectiveness of incorporating the class
name in the prompt for better style mining.
3.2. Local Style Mining
Our approach is to leverage PIN to mine class-specific style
banks that used for feature augmentation when training
FAMix. Given a set of cropped images Is, we encode them
using CLIP-I1 to get a set of low-level features Fs. Each
batch bof features fs∈ F sis cropped into mpatches, result-
ing in b×mpatches fp, associated ground-truth annotation
yp, of size h/√m×w/√m×c.
We aim at populating Kstyle banks, Kbeing the total
number of classes. For a feature patch fp, we compute the
dominant class from the corresponding label patch yp, and
get its name tpfrom the predefined classes in the training
dataset. Given a set of prompts describing random styles
R, the target prompt Ppis formed by concatenating a ran-
domly sampled style prompt rfromRandtp(e.g.,retro
futurism style building ). We show in the experi-
ments (Sec. 4.4) that our method is not very sensitive to the
prompt design, yet our prompt construction works best.
The idea is to mine proxy domains and explore interme-
diate ones in a class-aware manner (as detailed in Sec. 3.3),
which makes our work fundamentally different from [10],
that steers features towards a particular target style and cor-
responding domain, and better suited to generalization.
To handle the class imbalance problem, we simply select
one feature patch fpper class among the total b×mpatches,
as shown in Fig. 2. Consequently, we apply PIN (2) to op-
timize the local styles to match the representations of their
corresponding prompts, and use the mined styles to popu-
late the corresponding style banks. The complete procedure
is outlined in Algorithm 1.
The resulting style banks {T(1),···,T(K)}are used for
domain randomization during training.
3.3. Training FAMix
Style randomization. During training, randomly cropped
images Isare encoded into fsusing CLIP-I1 . Each batch
of feature maps fsis viewed as a grid of mpatches, with-
out cropping them. For each patch fs(ij)within the grid,
the dominant class cp(ij)is queried using the corresponding
ground truth patch ys(ij), and a style is randomly sampled
from the corresponding mined bank T(cp(ij)). We then ap-
ply patch-wise convex combination ( i.e., style mixing) of
the original style of the patch and the mined style. Specif-
ically, for an arbitrary patch fs(ij), our local style mixing
reads:
µmix←(1−α)µ(f(ij)
s) +αµ(ij)(3)
σmix←(1−α)σ(f(ij)
s) +ασ(ij), (4)
with(µ(ij),σ(ij))∈ T(c(ij)
p)andα∈[0,1]c.Algorithm 1: Local Style Mining.
Input : SetFsof source features batches.
Label set YsinDs.
Set of random prompts Rand class names C.
Param : Number of patches m.
Number of classes K.
Output: Ksets{T(1),···,T(K)}of class-wise
augmented statistics.
1{T(1),···,T(K)} ← ∅
2foreach (fs∈ F s,ys∈ Y s)do
3{yp} ←crop-patch (ys, m)
4{cp},{Pp},{fp} ← ∅
5 foreach yp∈ {yp}do
6 cp←get-dominant-class (yp)
7 ifcpnot in{cp}then
8 {cp} ←cp
9 {Pp} ←concat (sample (R),get-name (cp))
10 {fp} ←fp
11 end
12 end
13 µ(cp),σ(cp),f′
p←PIN (Pp)(fp)
14 T(cp)← T(cp)∪ {(µ(cp),σ(cp))}
15end
Algorithm 2: Training FAMix.
Input : SetFsof source features batches.
Label set YsinDs.
Ksets{T(1),···,T(K)}of class-wise
augmented statistics.
Param: Number of patches m.
1foreach (fs∈ F s,ys∈ Y s)do
2 α∼Beta (0.1,0.1)
3 for(i, j)∈[1,√m]×[1,√m]do
4 c(ij)
p←get-dominant-class (y(ij)
s)
5 µ(ij),σ(ij)←sample (T(c(ij)
p))
6 µmix←(1−α).µ(f(ij)
s) +α.µ(ij)
7 σmix←(1−α).σ(f(ij)
s) +α.σ(ij)
8 f(ij)
s←AdaIN (f(ij)
s, µmix, σmix)
9 end
10 ˜ ys←CLIP-I2 (fs)
11 Loss =cross-entropy (˜ ys,ys)
12end
As shown in Fig. 1, our style mixing strategy differs
from [62] which applies a linear interpolation between
styles extracted from the images of a limited set of source
domain(s) assumed to be available for training. Here, we
view the mined styles as variations of multiple proxy tar-
get domains defined by the prompts. Training is conducted
over all the paths in the feature space between the source
and proxy domains without requiring any additional image
during training other than the one from source.
23431
Method arch. C B M S AN AS AR AF Mean
RobustNet [7]
RN5036.58 35.20 40.33 28.30 6.32 29.97 33.02 32.56 30.29
SAN & SAW [37] 39.75 37.34 41.86 30.79 - - - - -
Pin the memory [23] 41.00 34.60 37.40 27.08 3.84 5.51 5.89 7.27 20.32
SHADE [58] 44.65 39.28 43.34 28.41 8.18 30.38 35.44 36.87 33.32
SiamDoGe [51] 42.96 37.54 40.64 28.34 10.60 30.71 35.84 36.45 32.89
DPCL [53] 44.87 40.21 46.74 - - - - - -
SPC-Net [19] 44.10 40.46 45.51 - - - - - -
NP [11] 40.62 35.56 38.92 27.65 - - - - -
WildNet* [29] 44.62 38.42 46.09 31.34 8.27 30.29 36.32 35.39 33.84
TLDR* [24] 46.51 42.58 46.18 30.57 13.13 36.02 38.89 40.58 36.81
FAMix (ours) 48.15 45.61 52.11 34.23 14.96 37.09 38.66 40.25 38.88
SAN & SAW [37]
RN10145.33 41.18 40.77 31.84 - - - - -
SHADE†[58] 46.66 43.66 45.50 31.58 7.58 32.48 36.90 36.69 35.13
WildNet* [29] 45.79 41.73 47.08 32.51 - - - - -
TLDR* [24] 47.58 44.88 48.80 33.14 - - - - -
FAMix (ours) 49.47 46.40 51.97 36.72 19.89 41.38 40.91 42.15 41.11
Table 2. Single-source DGSS trained on G.Performance (mIoU
%) of FAMix compared to other DGSS methods trained on Gand
evaluated on C,S,M,S,Afor ResNet-50 (‘RN50’) and ResNet-
101 (‘RN101’) backbone architecture (‘arch.’). * indicates the use
of extra-data. †indicates the use of the full data for training. We
emphasize best and second best results.
Style transfer is applied through AdaIN (1). Only the
standard cross-entropy loss between the ground truth ysand
the prediction ˜ ysis applied for training the network. Algo-
rithm 2 shows the training steps of FAMix.
Minimal fine-tuning. During training, we fine-tune only
the last few layers of the backbone. Subsequently, we ex-
amine various alternatives and show that the minimal extent
of fine-tuning is the crucial factor in witnessing the effec-
tiveness of our local style mixing strategy.
Previous works [11, 36, 62] suggest that shallow feature
statistics capture style information while deeper features en-
code semantic content. Consequently, some DGSS methods
focus on learning style-agnostic representations [7, 36, 37],
but this can compromise the expressiveness of the represen-
tation and suppress content information. In contrast, our
intuition is to retain these identified traits by introducing
variability to the shallow features through augmentation and
mixing. Simultaneously, we guide the network to learn in-
variant high-level representations by training the final layers
of the backbone with a label-preserving assumption, using
a standard cross-entropy loss.
4. Experiments
4.1. Experimental setup
Synthetic datasets. GTA V [41] and SYNTHIA [42] are
used as synthetic datasets. GTA V consists of 24 966 images
split into 12 403 images for training, 6 382 for validation
and 6 181 for testing. SYNTHIA consists of 9 400 images:
6 580 for training and 2 820 for validation. GTA V and SYN-
THIA are denoted by GandS, respectively.
Real datasets. Cityscapes [8], BDD-100K [54], and Map-
illary [35] contain 2 975, 7 000, and 18 000 images for train-ing and 500, 1 000, and 2 000 images for validation, respec-
tively. ACDC [44] is a dataset of driving scenes in adverse
conditions: night, snow, rain and fog with respectively 106,
100, 100 and 100 images in the validation sets. C,B, andM
denote Cityscapes, BDD-100K and Mapillary, respectively;
AN,AS,ARandAFdenote night, snow, rain and fog subsets
of ACDC, respectively.
Implementation details. Following previous works [7,
19, 23, 24, 29, 37, 51, 53, 58], we adopt DeepLabv3+ [5] as
segmentation model. ResNet-50 and ResNet-101 [17], ini-
tialized with CLIP pretrained weights, are used in our ex-
periments as backbones. Specifically, we remove the atten-
tion pooling layer and add a randomly initialized decoder
head. The output stride is 16. Single-source and multi-
source models are trained respectively for 40Kand60K
iterations with a batch size of 8. The training images are
cropped to 768×768. Stochastic Gradient Descent (SGD)
with a momentum of 0.9and weight decay of 10−4is used
as optimizer. Polynomial decay with a power of 0.9is used,
with an initial learning rate of 10−1for the classifier and
10−2for the backbone. We use color jittering and horizon-
tal flip as data augmentation. Label smoothing regulariza-
tion [46] is adopted. For style mining, Layer1 features are
divided into 9patches. Each patch is resized to 56×56,
corresponding to the dimensions of Layer1 features for an
input image of size 224×224(i.e. the input dimension of
CLIP). We use ImageNet templates2for each prompt.
Evaluation metric. We evaluate our models on the valida-
tion sets of the unseen target domains with mean Intersec-
tion over Union (mIoU%) of the 19shared semantic classes.
For each experiment, we report the average of three runs .
4.2. Comparison with DGSS methods
Single-source DGSS. We compare FAMix with state-of-
the-art DGSS methods under the single-source setting.
Training on GTA V ( G)as source, Tab. 2 reports models
trained with either ResNet-50 or ResNet-101 backbones.
The unseen target datasets are C,B,M,S, and the four sub-
sets of A. Tab. 2 shows that our method significantly out-
performs all the baselines on all the datasets for both back-
bones. We note that WildNet [29] and TLDR [24] use extra-
data, while SHADE [58] uses the full Gdataset (24,966
images) for training with ResNet-101. Class-wise perfor-
mances are reported in Appendix B.
Training on Cityscapes ( C)as source, Tab. 3 reports per-
formance with ResNet-50 backbone. The unseen target
datasets are B,M,G, andS. The table shows that our method
outperforms the baseline in average, and is competitive to
SOTA on GandM.
Multi-source DGSS. We also show the effectiveness of
FAMix in the multi-source setting, training on G+Sand
2https://github.com/openai/CLIP/
23432
Method B M G S Mean
RobustNet [7] 50.73 58.64 45.00 26.20 45.14
Pin the memory [23] 46.78 55.10 - - -
SiamDoGe [51] 51.53 59.00 45.08 26.67 45.57
WildNet* [29] 50.94 58.79 47.01 27.95 46.17
DPCL [53] 52.29 - 46.00 26.60 -
FAMix (ours) 54.07 58.72 45.12 32.67 47.65
Table 3. Single-source DGSS trained on C.Performance (mIoU
%) of FAMix compared to other DGSS methods trained on Cand
evaluated on B,M,GandSfor ResNet-50 backbone. * indicates
the use of extra-data. We emphasize best and second best results.
Method C B M Mean
RobustNet [7] 37.69 34.09 38.49 36.76
Pin the memory [23] 44.51 38.07 42.70 41.76
SHADE [58] 47.43 40.30 47.60 45.11
SPC-Net [19] 46.36 43.18 48.23 45.92
TLDR* [24] 48.83 42.58 47.80 46.40
FAMix (ours) 49.41 45.51 51.61 48.84
Table 4. Multi-source DGSS. Performance (mIoU %) of FAMix
compared to other DGSS methods trained on G+Sand evaluated
onC,B,Mfor ResNet-50 backbone. * indicates the use of extra-
data. We emphasize best and second best results.
evaluating on C,BandM. The results reported in Tab. 4 for
ResNet-50 backbone outperform state-of-the-art.
Qualitative results. We visually compare the segmenta-
tion results with Pin the memory [23], SHADE [58] and
WildNet [29] in Fig. 3. FAMix clearly outperforms other
DGSS methods on “stuff” ( e.g., road and sky) and “things”
(e.g., bicycle and bus) classes.
4.3. Decoder-Probing Fine-Tuning (DP-FT)
Kumar et al. [26] show that standard fine-tuning may distort
the pretrained feature representation, leading to degraded
OOD performances for classification. Consequently, they
propose a two-step training strategy: (1) Training a linear
probe (LP) on top of the frozen backbone features, (2) Fine-
tuning (FT) both the linear probe and the backbone. In-
spired by it, Saito et al. [43] apply the same strategy for
object detection, which is referred to as Decoder-probing
Fine-tuning (DP-FT). They observe that DP-FT improves
over DP depending on the architecture. We hypothesize
that the effect is also dependent on the pretraining paradigm
and the downstream task. As observed in Tab. 1, CLIP
might remarkably overfit the source domain when fine-
tuned. In Tab. 5, we compare fine-tuning (FT), decoder-
probing (DP) and DP-FT. DP brings improvements over FT
since it completely preserves the pretrained representation.
Yet, DP major drawback lies in its limitation to adapt fea-
tures for the downstream task, resulting in suboptimal re-
sults. Surprisingly, DP-FT largely falls behind DP, meaningMethod C B M S AN AS AR AF Mean
FT 16.81 16.31 17.80 27.10 2.95 8.58 14.35 13.61 14.69
DP 34.13 37.67 42.21 29.10 10.71 26.26 29.47 30.40 29.99
DP-FT 25.62 21.71 26.39 31.45 4.22 18.26 20.07 20.85 21.07
FAMix (ours) 48.15 45.61 52.11 34.23 14.96 37.09 38.66 40.25 38.88
Table 5. FAMix vs. DP-FT . Performance (mIoU%) of
FAMix compared to Fine-tuning (FT), Decoder-probing (DP) and
Decoder-probing Fine-tuning (DP-FT). We use here ResNet-50,
trained on G. We emphasize best and second best results.
Freeze Augment Mix C B M S AN AS AR AF Mean
✗ ✗ ✗ 16.81 16.31 17.80 27.10 2.95 8.58 14.35 13.61 14.69
✗ ✓ ✗22.48 26.05 24.15 25.40 4.83 17.61 22.86 19.75 20.39
✗ ✗ ✓ 20.07 21.24 22.91 26.52 1.28 14.99 22.09 20.51 18.70
✗ ✓ ✓ 27.53 26.59 26.27 26.91 4.90 18.91 25.60 22.14 22.36
✓ ✗ ✗ 37.83 38.88 44.24 31.93 12.41 29.59 31.56 33.05 32.44
✓ ✓ ✗36.65 35.73 37.32 30.44 14.72 34.65 34.91 38.98 32.93
✓ ✗✓ 43.43 43.79 48.19 33.70 11.32 35.55 36.15 38.19 36.29
✓ ✓ ✓ 48.15 45.61 52.11 34.23 14.96 37.09 38.66 40.25 38.88
Table 6. Ablation of FAMix components. Performance
(mIoU %) after removing one or more components of FAMix.
that the learned features over-specialize to the source do-
main distribution even with a “decoder warm-up”.
The results advocate for the need of specific strategies to
preserve CLIP robustness for semantic segmentation. This
need emerges from the additional gap between pretraining
(i.e. aligning object-level and language representations) and
fine-tuning ( i.e. supervised pixel classification).
4.4. Ablation studies
We conduct all the ablations on a ResNet-50 backbone with
GTA V ( G) as source dataset.
Removing ingredients from the recipe. FAMix is based
on minimal fine-tuning of the backbone ( i.e., Freeze), style
augmentation and mixing. We show in Tab. 6 that the
best generalization results are only obtained when com-
bining the three ingredients. Specifically, when the back-
bone is fine-tuned ( i.e., Freeze ✗), the performances are
largely harmed. When minimal fine-tuning is performed
(i.e., Freeze ✓), we argue that the augmentations are too
strong to be applied without style mixing; the latter brings
both effects of domain interpolation and use of the original
statistics. Subsequently, when style mixing is not applied
(i.e. Freeze ✓, Augment ✓, Mix ✗), the use of mined styles
brings mostly no improvement on OOD segmentation com-
pared to training without augmentation ( i.e. Freeze ✓, Aug-
ment ✗, Mix ✗). Note that for Freeze ✓, Augment ✓, Mix
✗, the line 8 in Algorithm 2 becomes:
f(ij)
s←AdaIN (f(ij)
s,µ(ij),σ(ij)) (5)
Our style mixing is different from MixStyle [62] for being
applied: (1) patch-wise and (2) between original styles of
23433
Image GT PIN the mem. [23] SHADE [58] WildNet [29] FAMix
C
B
M
Road Sidewalk Building Wall Fence Pole Traffic light Traffic sign Vegetation Terrain
Sky Person Rider Car Truck Bus Train Motorbike Bicycle n/a
Figure 3. Qualitative results. Columns 1-2 : Image and ground truth (GT), Columns 3-4-5 : DGSS methods results, Column 6 : Our results.
The models are trained on Gwith ResNet-50 backbone.
RCP RSP CN C B M S AN AS AR AF Mean
✓45.99 43.71 50.48 34.75 15.22 35.09 34.92 38.17 37.29
✓ 46.10 44.24 48.90 33.62 13.39 35.99 36.68 39.86 37.35
✓ 45.64 44.59 49.13 33.64 15.33 37.32 35.98 38.85 37.56
✓ ✓ 47.83 44.83 50.38 34.27 14.43 37.07 37.07 38.76 38.08
✓✓48.15 45.61 52.11 34.23 14.96 37.09 38.66 40.25 38.88
Table 7. Ablation on the prompt construction. Perfor-
mance (mIoU %) for different prompt constructions. RCP, RSP
and CN refer to <random character prompt >,<random
style prompt >and<class name >, respectively.
the source data and augmented versions of them. Note that
the case (Freeze ✓, Augment ✗, Mix✓) could be seen as a
variant of MixStyle, yet applied locally and class-wise. Our
complete recipe is proved to be significantly more effective
with a boost of ≈+6mean mIoU w.r.t. the baseline of
training without augmentation and mixing.
Prompt construction. Tab. 7 reports results when ablat-
ing the prompt construction. In FAMix, the final prompt
is derived by concatenating <random style prompt >
and<class name >; removing either of those leads to
inferior results. Interestingly, replacing the style prompt
by random characters – e.g. “ioscjspa” – does not signifi-
cantly degrade the performance. In certain aspects, using
random prompts still induces a randomization effect within
the FAMix framework. However, meaningful prompts still
consistently lead to the best results.
Number of style prompts. FAMix uses a set Rof random
style prompts which are concatenated with the class names;
Ris formed by querying ChatGPT3using <give me 20
prompts of 2 to 5 words describing random
image styles >. The output prompts are provided in
Appendix C. Fig. 4a shows that the size of Rhas a marginal
impact on FAMix performance. Yet, the mIoU scores on C,
B,MandARare higher for |R|= 20 compared to |R|= 1
and almost equal for the other datasets.
3https://chat.openai.com/
(a) Number of prompts
 (b) Effect of layer freezing
Figure 4. Ablation of prompt set and freezing strategy. (a) Per-
formance (mIoU %) on test datasets w.r.t. the number of random
style prompts in R. (b) Effect of freezing layers reporting on x-
axis the last frozen layer. For example, ‘L3’ means freezing L1,
L2 and L3. ‘L4’ ’ indicates that the Layer4 is partially frozen.
The low sensitivity of the performance to the size of R
could be explained by two factors. First, mining even from
a single prompt results in different style variations as the
optimization starts from different anchor points in the latent
space, as argued in [10]. Second, mixing style between the
source and the mined proxy domains is the crucial factor
making the network explore intermediate domains during
training. This does not contradict the effect of our prompt
construction which leads to the best results (Tab. 7).
Local vs. global style mining. To highlight the effect of
our class-wise local style mining, we perform an ablation
replacing it with global style mining. Specifically, the same
set of <random style prompt >are used, though be-
ing concatenated with <driving >as a global description
instead of local class name. Intuitively, local style mining
and mixing induces richer style variations and more contrast
among patches. The results in Tab. 8 show the effective-
ness of our local style mining and mixing strategy, bringing
about 3mIoU improvement on G→C.
What to mix? LetS=SK
k=1S(k)andT=SK
k=1T(k)
the sets of class-wise source and augmented features, re-
spectively. In FAMix training, for an arbitrary patch f(ij)
s,
23434
Syle mining C B M S AN AS AR AF Mean
global w/“street view” 45.51 45.12 50.40 33.65 14.59 36.92 37.38 40.53 38.01
“urban scene” 46.59 45.38 51.33 33.67 14.42 35.96 37.30 40.52 38.15
“roadscape” 45.49 45.55 50.63 33.66 14.77 36.75 37.07 40.33 38.03
“commute snapshot” 45.39 45.08 50.50 33.68 13.65 36.63 37.93 40.92 37.97
“driving” 45.06 44.98 50.67 33.36 14.84 35.11 36.21 39.52 37.47
local 48.15 45.61 52.11 34.23 14.96 37.09 38.66 40.25 38.88
Table 8. Ablation on style mining. Global style mining consists
of mining one style per feature map, using <random style
prompt >+<global class >as prompt.
Syle mining C B M S AN AS AR AF Mean
S 43.43 43.79 48.19 33.70 11.32 35.55 36.15 38.19 36.29
S ∪ T 44.76 45.59 50.78 34.05 13.67 36.92 37.18 38.13 37.64
T(ours) 48.15 45.61 52.11 34.23 14.96 37.09 38.66 40.25 38.88
Table 9. Ablation on the sets used for mixing. The styles (µ,σ)
used in (3) and (4) are sampled either from SorS ∪ T orT.
style mixing is performed between the original source
statistics and statistics sampled from the augmented set ( i.e.,
(µ(ij),σ(ij))∈ T(c(ij)
p), see (3) and (4)). In class-wise
vanilla MixStyle, (µ(ij),σ(ij))∈ S(c(ij)
p). In Tab. 9, we
show that sampling (µ(ij),σ(ij))fromS(c(ij)
p)∪ T(c(ij)
p)
does not lead to better generalization, despite sampling
from a set with twice the cardinality. This supports our mix-
ing strategy visualized in Fig. 1. Intuitively, sampling from
S ∪ T could be viewed as applying either MixStyle or our
mixing with a probability p= 0.5.
Minimal fine-tuning. We argue for minimal fine-tuning
as a compromise between pretrained feature preservation
and adaptation. Fig. 4b shows an increasing OOD gener-
alization trend with more freezing. Interestingly, only fine-
tuning the last layers of the last convolutional block (where
the dilation is applied) achieves the best results. When train-
ing on Cityscapes, we observed that freezing all the layers
except Layer4 achieves the best results.
4.5. Does FAMix require language?
Inspired by the observation that target statistics deviate
around the source ones in real cases [11], we conduct an
experiment where we replace language-driven style mining
by noise perturbation. The same procedure of FAMix is
kept: (i) Features are divided into patches, perturbed with
noise and then saved into a style bank based on the domi-
nant class; (ii) During training, patch-wise style mixing of
original and perturbed styles is performed.
Different from Fan et al . [11], who perform a pertur-
bation on the feature statistics using a normal distribu-
tion with pre-defined parameters, we experiment perturba-
tion with different magnitudes of noise controlled by the
signal-to-noise ratio (SNR). Consider the mean of a patch
µ∈Rcas a signal, the goal is to perturb it with some noiseSNR C B M S AN AS AR AF Mean
Baseline 37.83 38.88 44.24 31.93 12.41 29.59 31.56 33.05 32.44
5 28.78 29.24 30.32 21.67 12.60 24.00 25.95 25.87 24.80
10 40.09 39.50 43.45 29.09 13.36 33.47 33.11 36.17 33.53
15 45.02 44.16 48.63 32.96 14.55 36.09 35.99 40.96 37.30
20 45.52 44.29 49.26 33.45 12.40 35.96 36.52 38.60 37.00
25 44.82 44.26 48.54 33.30 11.38 34.51 35.46 37.61 36.24
30 43.07 43.80 48.31 33.47 12.33 35.05 35.58 38.10 36.21
∞ 43.43 43.79 48.19 33.70 11.32 35.55 36.15 38.19 36.29
MixStyle [62] 40.97 42.04 48.36 33.15 13.14 31.26 34.94 38.12 35.25
Prompts 48.15 45.61 52.11 34.23 14.96 37.09 38.66 40.25 38.88
Table 10. Noise vs prompt-driven augmentation. The prompt-
driven augmentation in FAMix is replaced by random noise with
different levels defined by SNR. We also include vanilla MixStyle.
The prompt-driven strategy is superior.
nµ∈Rc. The SNR dBbetween ∥µ∥and∥nµ∥is defined
asSNR dB= 20 log10(∥µ∥/∥nµ∥). Given µ,SNR dB, and
n∼ N(0, I), where I∈Rc×cis the identity matrix, the
noise is computed as nµ= 10−SNR
20∥µ∥
∥n∥n. We add µ+nµ
to the style bank corresponding to the dominant class in the
patch. The same applies to σ∈Rc. The results of training
for different noise levels are in Tab. 10. Using language as
source of randomization outperforms any noise level. The
baseline corresponds to the case where no augmentation nor
mixing are performed (See Tab. 6, Freeze ✓, Augment ✗,
Mix✗). SNR= ∞could be seen as a variant of MixStyle,
applied class-wise to patches (See Tab. 6, Freeze ✓, Aug-
ment ✗, Mix✓). The vanilla MixStyle gets inferior results.
Besides lower OOD performance, one more disadvan-
tage of noise augmentation compared to our language-
driven augmentation is the need to select a value for the
SNR, for which the optimal value might vary depending on
the target domain encountered at the test time.
5. Conclusion
We presented FAMix, a simple recipe for domain general-
ized semantic segmentation with CLIP pretraining. We pro-
posed to locally mix the styles of source features with their
augmented counterparts obtained using language prompts.
Combined with minimal fine-tuning, FAMix significantly
outperforms the state-of-the-art approaches. Extensive ex-
periments showcase the effectiveness of our framework.
We hope that FAMix will serve as a strong baseline in fu-
ture works, exploring the potential of leveraging large-scale
vision-language models for perception tasks.
Acknowledgment. This work was partially funded by French project
SIGHT (ANR-20-CE23-0016) and was supported by ELSA - European
Lighthouse on Secure and Safe AI funded by the European Union under
grant agreement No. 101070617. It was performed using HPC resources
from GENCI–IDRIS (Grant AD011014477).
23435
References
[1] Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-
Christophe Gagnon-Audet, Yoshua Bengio, Ioannis
Mitliagkas, and Irina Rish. Invariance principle meets infor-
mation bottleneck for out-of-distribution generalization. In
NeurIPS , 2021. 2
[2] James Urquhart Allingham, Jie Ren, Michael W Dusenberry,
Xiuye Gu, Yin Cui, Dustin Tran, Jeremiah Zhe Liu, and Bal-
aji Lakshminarayanan. A simple zero-shot prompt weighting
technique to improve prompt ensembling in text-image mod-
els. In ICML , 2023. 2
[3] Martin Arjovsky, L ´eon Bottou, Ishaan Gulrajani, and David
Lopez-Paz. Invariant risk minimization. arXiv preprint
arXiv:1907.02893 , 2019. 2
[4] Yogesh Balaji, Swami Sankaranarayanan, and Rama Chel-
lappa. Metareg: Towards domain generalization using meta-
regularization. In NeurIPS , 2018. 2
[5] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian
Schroff, and Hartwig Adam. Encoder-decoder with atrous
separable convolution for semantic image segmentation. In
ECCV , 2018. 5
[6] Xinlei Chen and Kaiming He. Exploring simple siamese rep-
resentation learning. In CVPR , 2021. 3
[7] Sungha Choi, Sanghun Jung, Huiwon Yun, Joanne T Kim,
Seungryong Kim, and Jaegul Choo. Robustnet: Improving
domain generalization in urban-scene segmentation via in-
stance selective whitening. In CVPR , 2021. 1, 2, 3, 5, 6
[8] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In CVPR ,
2016. 5
[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR , 2009. 1, 3
[10] Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick
P´erez, and Raoul de Charette. Poda: Prompt-driven zero-
shot domain adaptation. In ICCV , 2023. 1, 3, 4, 7
[11] Qi Fan, Mattia Segu, Yu-Wing Tai, Fisher Yu, Chi-Keung
Tang, Bernt Schiele, and Dengxin Dai. Towards robust ob-
ject detection invariant to real-world domain shifts. In ICLR ,
2023. 5, 8
[12] Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan,
Vaishaal Shankar, Achal Dave, and Ludwig Schmidt. Data
determines distributional robustness in contrastive language
image pre-training (clip). In ICML , 2022. 1, 2
[13] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas-
cal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario
Marchand, and Victor Lempitsky. Domain-adversarial train-
ing of neural networks. JMLR , 2016. 1
[14] Yunhao Ge, Jie Ren, Andrew Gallagher, Yuxiao Wang,
Ming-Hsuan Yang, Hartwig Adam, Laurent Itti, Balaji Lak-
shminarayanan, and Jiaping Zhao. Improving zero-shot gen-
eralization and robustness of multi-modal models. In CVPR ,
2023. 2
[15] Sachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter,
and Aditi Raghunathan. Finetune like you pretrain: Im-proved finetuning of zero-shot vision models. In CVPR ,
2023. 1, 2
[16] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
Open-vocabulary object detection via vision and language
knowledge distillation. In ICLR , 2022. 1
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
2016. 5
[18] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu,
Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell.
Cycada: Cycle-consistent adversarial domain adaptation. In
ICML , 2018. 1
[19] Wei Huang, Chang Chen, Yong Li, Jiacheng Li, Cheng Li,
Fenglong Song, Youliang Yan, and Zhiwei Xiong. Style pro-
jected clustering for domain generalized semantic segmenta-
tion. In CVPR , 2023. 1, 3, 5, 6
[20] Xun Huang and Serge Belongie. Arbitrary style transfer in
real-time with adaptive instance normalization. In ICCV ,
2017. 3
[21] Nishant Jain, Harkirat Behl, Yogesh Singh Rawat, and Vib-
hav Vineet. Efficiently robustify pre-trained models. In
ICCV , 2023. 2
[22] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representation
learning with noisy text supervision. In ICML , 2021. 1
[23] Jin Kim, Jiyoung Lee, Jungin Park, Dongbo Min, and
Kwanghoon Sohn. Pin the memory: Learning to generalize
semantic segmentation. In CVPR , 2022. 1, 5, 6, 7
[24] Sunghwan Kim, Dae-hwan Kim, and Hoseong Kim. Tex-
ture learning domain randomization for domain generalized
segmentation. In ICCV , 2023. 1, 3, 5, 6
[25] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen,
Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi
Le Priol, and Aaron Courville. Out-of-distribution gener-
alization via risk extrapolation (rex). In ICML , 2021. 2
[26] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones,
Tengyu Ma, and Percy Liang. Fine-tuning can distort pre-
trained features and underperform out-of-distribution. In
ICLR , 2022. 1, 2, 6
[27] Gihyun Kwon and Jong Chul Ye. Clipstyler: Image style
transfer with a single text condition. In CVPR , 2022. 1
[28] Clement Laroudie, Andrei Bursuc, Mai Lan Ha, and Gianni
Franchi. Improving clip robustness with knowledge distil-
lation and self-training. arXiv preprint arXiv:2309.10361 ,
2023. 2
[29] Suhyeon Lee, Hongje Seong, Seongwon Lee, and Euntai
Kim. Wildnet: Learning domain generalized semantic seg-
mentation from the wild. In CVPR , 2022. 1, 3, 5, 6, 7
[30] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen
Koltun, and Rene Ranftl. Language-driven semantic seg-
mentation. In ICLR , 2022. 1
[31] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot.
Domain generalization with adversarial feature learning. In
CVPR , 2018. 1
[32] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang
Liu, Kun Zhang, and Dacheng Tao. Deep domain generaliza-
23436
tion via conditional invariant adversarial networks. In ECCV ,
2018. 1, 2
[33] Yunsheng Li, Lu Yuan, and Nuno Vasconcelos. Bidirectional
learning for domain adaptation of semantic segmentation. In
CVPR , 2019. 1
[34] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and
Michael I Jordan. Conditional adversarial domain adapta-
tion. In NeurIPS , 2018. 1
[35] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and
Peter Kontschieder. The mapillary vistas dataset for semantic
understanding of street scenes. In ICCV , 2017. 5
[36] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two
at once: Enhancing learning and generalization capacities
via ibn-net. In ECCV , 2018. 1, 3, 5
[37] Duo Peng, Yinjie Lei, Munawar Hayat, Yulan Guo, and Wen
Li. Semantic-aware domain generalized segmentation. In
CVPR , 2022. 1, 3, 5
[38] Fengchun Qiao, Long Zhao, and Xi Peng. Learning to learn
single domain generalization. In CVPR , 2020. 2
[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML , 2021. 1, 2, 3
[40] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong
Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu.
Denseclip: Language-guided dense prediction with context-
aware prompting. In CVPR , 2022. 1
[41] Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen
Koltun. Playing for data: Ground truth from computer
games. In ECCV , 2016. 5
[42] German Ros, Laura Sellart, Joanna Materzynska, David
Vazquez, and Antonio M Lopez. The synthia dataset: A large
collection of synthetic images for semantic segmentation of
urban scenes. In CVPR , 2016. 5
[43] Kuniaki Saito, Donghyun Kim, Piotr Teterwak, Rogerio
Feris, and Kate Saenko. Mind the backbone: Minimiz-
ing backbone distortion for robust object detection. arXiv
preprint arXiv:2303.14744 , 2023. 6
[44] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. ACDC:
The adverse conditions dataset with correspondences for se-
mantic driving scene understanding. In ICCV , 2021. 5
[45] Yang Shu, Xingzhuo Guo, Jialong Wu, Ximei Wang, Jianmin
Wang, and Mingsheng Long. Clipood: Generalizing clip to
out-of-distributions. In ICML , 2023. 1, 2
[46] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception ar-
chitecture for computer vision. In CVPR , 2016. 5
[47] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell.
Adversarial discriminative domain adaptation. In CVPR ,
2017. 1
[48] Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu
Cord, and Patrick P ´erez. Advent: Adversarial entropy mini-
mization for domain adaptation in semantic segmentation. In
CVPR , 2019. 1
[49] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang,
Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and PhilipYu. Generalizing to unseen domains: A survey on domain
generalization. T-KDE , 2022. 1, 2
[50] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim,
Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gon-
tijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok
Namkoong, et al. Robust fine-tuning of zero-shot models.
InCVPR , 2022. 1, 2
[51] Zhenyao Wu, Xinyi Wu, Xiaoping Zhang, Lili Ju, and Song
Wang. Siamdoge: Domain generalizable semantic segmen-
tation using siamese network. In ECCV , 2022. 1, 3, 5, 6
[52] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, and
Qi Tian. A fourier-based framework for domain generaliza-
tion. In CVPR , 2021. 1
[53] Liwei Yang, Xiang Gu, and Jian Sun. Generalized seman-
tic segmentation by self-supervised source domain projec-
tion and multi-level contrastive learning. In AAAI , 2023. 1,
5, 6
[54] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying
Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Dar-
rell. Bdd100k: A diverse driving dataset for heterogeneous
multitask learning. In CVPR , 2020. 5
[55] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,
Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer.
LiT: Zero-shot transfer with locked-image text tuning. In
CVPR , 2022. 1
[56] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and
Lucas Beyer. Sigmoid loss for language image pre-training.
InICCV , 2023. 1
[57] Shanshan Zhao, Mingming Gong, Tongliang Liu, Huan Fu,
and Dacheng Tao. Domain generalization via entropy regu-
larization. In NeurIPS , 2020. 2
[58] Yuyang Zhao, Zhun Zhong, Na Zhao, Nicu Sebe, and
Gim Hee Lee. Style-hallucinated dual consistency learning
for domain generalized semantic segmentation. In ECCV ,
2022. 1, 3, 5, 6, 7
[59] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free
dense labels from clip. In ECCV , 2022. 1
[60] Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao
Xiang. Deep domain-adversarial image generation for do-
main generalisation. In AAAI , 2020. 2
[61] Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao
Xiang. Learning to generate novel domains for domain gen-
eralization. In ECCV , 2020.
[62] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do-
main generalization with mixstyle. In ICLR , 2021. 1, 2, 4, 5,
6, 8
[63] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and
Chen Change Loy. Domain generalization: A survey.
TPAMI , 2022. 1, 2
[64] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Zi-
wei Liu. Conditional prompt learning for vision-language
models. In CVPR , 2022. 1
[65] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
Liu. Learning to prompt for vision-language models. IJCV ,
2022. 1
23437
