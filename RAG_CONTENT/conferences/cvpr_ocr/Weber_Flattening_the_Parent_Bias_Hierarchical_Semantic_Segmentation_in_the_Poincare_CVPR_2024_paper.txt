Flattening the Parent Bias:
Hierarchical Semantic Segmentation in the Poincar ´e Ball
Simon Weber1,2Barıs ¸ Z ¨ong¨ur1Nikita Araslanov1,2Daniel Cremers1,2
1Technical University of Munich2Munich Center for Machine Learning
hyperplane
dA
E
dB
E
(a) Embeddings in Euclidean space
gyroplanedA
H
dB
H
(b) Embeddings in Poincar ´e ball
Figure 1. Core idea. Class embeddings in the Euclidean space (a)exhibit non-uniform properties of the separation margin: the average
distance of a pixel embedding of one class to the decision boundaries of the other classes varies substantially ( e.g.dA
E> dB
E). This creates
an implicit parent bias in hierarchical segmentation, which prefers grouping one set of classes over the other, in terms of the parent-level
segmentation accuracy. In contrast, in hyperbolic space characterized by the Poincar ´e ball (b), the separation margins between the class
embeddings are more uniform, e.g. the hyperbolic distance of embeddings AandBof two different classes to the decision boundary (a
gyroplane) of another class is approximately equal, dA
H≈dB
H. This may explain the strong generalization of the parent-level predictions,
observed in practice, in terms of the segmentation accuracy and calibration quality.
Abstract
Hierarchy is a natural representation of semantic tax-
onomies, including the ones routinely used in image seg-
mentation. Indeed, recent work on semantic segmentation
reports improved accuracy from supervised training lever-
aging hierarchical label structures. Encouraged by these
results, we revisit the fundamental assumptions behind that
work. We postulate and then empirically verify that the rea-
sons for the observed improvement in segmentation accu-
racy may be entirely unrelated to the use of the semantic
hierarchy. To demonstrate this, we design a range of cross-
domain experiments with a representative hierarchical ap-
proach. We find that on the new testing domains, a flat (non-
hierarchical) segmentation network, in which the parents
are inferred from the children, has superior segmentation
accuracy to the hierarchical approach across the board.
Complementing these findings and inspired by the intrinsic
properties of hyperbolic spaces, we study a more principled
approach to hierarchical segmentation using the Poincar ´eball model. The hyperbolic representation largely outper-
forms the previous (Euclidean) hierarchical approach as
well and is on par with our flat Euclidean baseline in terms
of segmentation accuracy. However, it additionally exhibits
surprisingly strong calibration quality of the parent nodes
in the semantic hierarchy, especially on the more challeng-
ing domains. Our combined analysis suggests that the es-
tablished practice of hierarchical segmentation may be lim-
ited to in-domain settings, whereas flat classifiers general-
ize substantially better, especially if they are modeled in the
hyperbolic space.
1. Introduction
Semantic knowledge is inherently structured, and orga-
nizing it in a hierarchy is both natural and expressive. Un-
surprisingly, hierarchical representations play an important
role in computer vision [36, 37, 39, 59, 60, 62]. For in-
Project code: https://github.com/tum-vision/hierahyp
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
28223
stance, we may want to assign multiple labels to each pixel
in the image, rather than a single one, to encode ancestral
relations ( e.g. a “car” is also a “vehicle” and a “means of
transport”). Adhering to a tree-based label hierarchy, this
pixelwise classification task defines the so-called hierarchi-
calsemantic segmentation and is the subject of this work.
In the literature, recent work addresses this problem as
a supervised multi-label classification task [34, 35]. In this
formulation, the terminal leaf nodes and the internal nodes
are modeled with individual one- vs-all classifiers. Remark-
ably, the empirical results of this approach appear to even
exceed the standard supervised formulation (which only
considers the leaf categories) in the evaluation of segmenta-
tion accuracy over the leaf categories themselves . Such an
effect cannot be explained from the perspective of a learning
algorithm, for which the semantic grouping of leaf nodes
into parent classes is meaningless.
As our first step, we examine this phenomenon and re-
veal limited generalization of a state-of-the-art method for
hierarchical semantic segmentation [34]. Rather surpris-
ingly, we find that a flatclassifier, which only learns to clas-
sify the child (leaf) categories, largely outperforms the more
sophisticated prior art on both the child and parent classes .
We formulate the sufficiency of flat classifiers under the ex-
isting formulation of the hierarchical semantic segmenta-
tion, which establishes the link between model calibration
and accuracy on the hierarchical prediction task.
Moving forward, we identify an inherent bias of flat clas-
sifiers toward particular groupings of child categories into
parent meta-classes, as illustrated in Fig. 1. Specifically,
the Euclidean distance between a decision boundary of one
class and the class embeddings of the other categories is
non-uniform. However, classification errors of deep clas-
sifiers tend to occur near decision boundaries.1This im-
plies that defining a parent class comprising the two classes
with the lowest separation margin will tend to produce a
lower error rate in the parent-level classification, if we were
to combine two classes with the largest margin of separa-
tion. To mitigate this parent bias , we would like the deci-
sion boundary of any class to be equidistant to the embed-
dings of other classes. While additional regularization may
be necessary to achieve this in the Euclidean space, we find
that hyperbolic spaces provide such capacity naturally. We
embed pixel features in the Poincar ´e ball instead of the Eu-
clidean space, which allows us to alleviate the parent bias
and achieve a notable improvement in segmentation accu-
racy and calibration on the parent-level classification task.
We summarize our contributions as follows. (i)We re-
veal limited generalization of prior work on the hierarchical
semantic segmentation task. (ii)Through a systematic anal-
ysis, we establish the sufficiency of flat classifiers for this
1This is evidenced by the imperfect, yet fairly high calibration quality
of segmentation networks, as we will also show in the experiments.task, which in Euclidean embedding space, however, may
suffer from suboptimal accuracy on the parent classes. (iii)
We show that the intrinsic properties of hyperbolic space,
the Poincar ´e ball model, allow for mitigating this bias. (iv)
We experimentally confirm the strong generalization of the
Poincar ´e ball model, in terms of segmentation accuracy and
calibration, especially on parent categories.
2. Related Work
Research on semantic segmentation spans numerous
problem domains, including deep network architectures
[10, 13, 46, 51], training objectives [5, 12, 52] and strate-
gies [38, 41, 44]. Here, we are specifically interested in hi-
erarchical semantic segmentation and, more generally, the
hierarchical classification problem. Therefore, our review
of related literature below will focus only on these aspects,
and we refer interested readers to surveys for a comprehen-
sive overview of semantic segmentation research [40].
Hierarchical classification with tree-like taxonomies. A
multi-label classification problem is considered hierarchi-
cal if the label assignment must respect a pre-defined hi-
erarchy [25, 29]. Hierarchical classifiers may be catego-
rized into flat, local and global approaches. Flat classifiers
only model the leaf nodes, thus completely ignoring the
class hierarchy. Following the tree structure in the bottom-
up fashion, one can infer the parent label from the predic-
tions of its children. By contrast, global (or “big-bang”)
methods explicitly represent each node in the tree, for ex-
ample with a one- vs-all classifier per node [30]. Local ap-
proaches solve a number of smaller classification problems
using only the local information available at each node or
tree level [19, 32]. The success of these strategies appears
to be domain-specific. However, it is notable that flat classi-
fiers are generally seen as competitive baselines [4, 55, 61]
– the conclusion reached in our study too. Learning individ-
ual classifiers for internal (non-leaf) nodes in the hierarchy
may reduce semantically critical prediction errors [6, 22].
As a remark, hierarchical prediction has been the sub-
ject of research on problems in natural language processing
and bionformatics [29], where it is not uncommon to have
large taxonomies – in the order of tens or hundreds of thou-
sands of labels [45, 58]. By contrast, a typical size of the
label space in computer vision is substantially smaller [18],
especially for dense tasks, such as semantic segmentation
considered here ( e.g. up to 30 in Cityscapes [14]).
Hierarchical semantic segmentation. Considering a hi-
erarchy of image segments is a classic concept in computer
vision. Hierarchical image parsing helps to improve robust-
ness to (self-)occlusions and to variation in object scale of
early object recognition systems [2, 49, 54, 67]. Similar to
conditional random fields (CRFs) [16], deep networks can
also benefit from hierarchical representations for advanced
contextual reasoning [24, 50, 63].
28224
In contrast to the earlier work, where the hierarchy plays
a facilitating role, training deep semantic segmentation net-
works producing a hierarchical label structure is relatively
recent [34, 35]. HSSN [34], which we extensively use in our
analysis, formalizes the hierarchical prediction task with
auxiliary “parent” logits. Note that this implies a training
objective with more decision boundaries to learn than in the
standard (child-only) case, since each parent logit requires
a one- vs-all classifier. In a follow-up work, LogicSeg [35]
formulates Boolean rules describing the hierarchical con-
straints and maps them to a differentiable loss using fuzzy
logic. While somewhat elegant, this approach does not im-
prove over HSSN empirically in a significant way.
Hyperbolic computer vision. Deep learning on hyper-
bolic manifolds is still in its nascent stage [40]. In contrast
to the Euclidean space, hyperbolic spaces possess proper-
ties lending themselves well to embedding hierarchical rep-
resentations with minimal distortion [8, 43, 48]. Previous
research concentrated on generalizing the Euclidean models
operating on the hyperbolic manifold, in terms of network
models [20, 23, 56] and training specifics [27]. Exploit-
ing the properties of the hyperbolic embedding space has
been of primary interest in (self-supervised) metric learning
[28, 53]. Against the backdrop of this work, semantic seg-
mentation has been studied rather marginally. In a seminal
work in this domain, Atigh et al. [3] learn pixel embeddings
on the Poincar ´e ball and reach competitive segmentation ac-
curacy w.r.t. the more established Euclidean formulation.
Concurrently, Franco et al. [21] report the correlation of the
embedding norm with uncertainty in the context of active
segmentation learning.
Overall, despite the recent progress, the benefits of the
hyperbolic representation for semantic segmentation remain
unclear. Our study of hierarchical semantic segmentation
exemplifies some compelling advantages of the Poincar ´e
ball model, both theoretically and experimentally.
3. The problem and motivation
Let us revisit the formulation of the hierarchical semantic
segmentation problem from previous work [34, 35], which
we follow in our study. Our label space is defined as the
union of semantic categories at multiple levels of the se-
mantic hierarchy, S:=∪nSn, where S0defines the leaf
classes. Learning a semantic segmentation model with the
finest label space, S0, reduces the problem to the conven-
tional supervised scenario [10, 51], since it defines the gran-
ularity limit set by the available annotation in a given bench-
mark. In addition to S0, we construct S1by defining “meta-
classes”, which semantically agglomerate one or more cat-
egories from S0into a common parent class. In Cityscapes
[14], for example, one defines a parent class “Human” com-
prising child classes “Person” and “Rider”. While one could
create deep hierarchical structures, the limited annotation
Root
Human
PersonVehicle
Rider CarRoot
Human
PersonVehicle
Rider Car
(a) Generating a non-semantic label hierarchy – an example.
mIoU mAcc aAcc02040608010080.2886.2796.01
80.286.1895.92
79.8685.6995.9
Tree-A Tree-B Tree-C
(b) Segmentation accuracy does not change between semantic (Tree-A)
and non-semantic hierarchies (Tree-B, Tree-C).
Figure 2. Training with non-semantic hierarchies. We train se-
mantic segmentation models with non-semantic trees. (a)For ex-
ample, we define class “Person” as a parent of a “vehicle”, which
is clearly semantically meaningless. (b)Non-semantic hierarchies
(“Tree-B” and “Tree-C”) do not affect the segmentation accuracy
of semantic hierarchy (“Tree-A”) in a significant way.
in semantic segmentation only allows for hierarchies up to
n= 2 – more rarely n= 3, in practice [34]. With the la-
bel hierarchy thus defined, our goal now is to maximize the
segmentation accuracy ( e.g. mIoU or mean pixel accuracy),
evaluated separately for each level of the tree.
Empirical observations from previous work [34, 35] sug-
gest that defining the meta-classes, e.g.S1, improve the ac-
curacy of the children categories S0.What explains this
phenomenon? After all, the only additional supervision
signal in our new hierarchical formulation is the semantic
proximity of some classes in S0. However, this does not im-
mediately render the learning problem easier. In fact, since
we add an additional classification problem over categories
inS1, optimization may become even more difficult.
The hypothesis that the semantic proximity between
child categories provides a complementary supervision sig-
nal asks for empirical validation. We design meta-classes in
a semantically meaningless fashion and train a DeepLabv3+
with ResNet-101 backbone [10] following HSSN training
objective [34] on Cityscapes train [14]. For example, we
define “Car” as a child of “Human”, and “Terrain” as a
sibling of “Sky”. Observing the results in Fig. 2b on
Cityscapes val, we conclude that such semantically inco-
herent definitions of new meta-classes do not have much
effect on the segmentation accuracy of the leaf categories
(i.e. the standard 19 semantic classes in Cityscapes [14],
which remained unchanged). This experiment suggests that
the empirical benefits reported by Li et al. [34, 35] may not
be related to the semantic definition of the label hierarchy.
28225
3.1. Flat classifiers are strong baselines
As discussed in Sec. 2, flatclassifiers offer a reason-
able sanity check regardless of the problem domain. In-
deed, we found that the HSSN model exhibits a strong
bias towards the specific traffic scenes of the training do-
main (Cityscapes), while performing poorly on the novel
domains. By contrast, a flat classifier consistently outper-
forms HSSN on the test datasets ( cf. Tab. 2.). These results
may not strike us as surprising and are in line with our in-
tuition developed in the previous section. A less expected
result, however, is that we also observed superior accuracy
of flat classifiers on the parent categories. Recall that we
represent the semantic hierarchical relations as a fixed sta-
tionary tree, defined by three properties: (i)Every level of
the label hierarchy forms a categorical distribution. (ii)A
prediction at a terminal node determines the complete hier-
archical label (due to the uniqueness of the path to the root).
(iii)The label hierarchy remains unchanged in training and
testing. As we are dealing with a closed-world taxonomy, it
is straightforward that the conditional probability of a par-
ent class can be expressed with only the conditional proba-
bilities of its children, which we formalize as follows:
Proposition 1. Letybe the parent class and mydenote the
children of that class in image I. Given a stationary tree
defined above, the optimal class posterior p(ˆy=y| I)for
a parent node ywith child class posterior p( ˆmy=my| I)
is given by
p(ˆy=y|I) =X
mp(ˆy=y|m)p( ˆm=m| I)
=X
myp( ˆmy=my| I).(1)
Note that prior work on hierarchical semantic segmen-
tation [34, 35] independently models the parent posterior
p(ˆy=y| I). The above proposition states the sufficiency
of predicting the child nodes independently from the parent
nodes, and then inferring the parent posterior with Eq. (1).
3.2. The parent bias in Euclidean space
Class embeddings produced in the Euclidean space tend
to produce a multi-modal distribution, where each class
forms an independent mode (a cluster). The modes exhibit
a particular rank-based arrangement and we can loosely es-
tablish that, for example, class “A” is closer to class “B”,
in terms of the Euclidean distance (see supp. material for
empirical support). Since typical classification errors oc-
cur at the decision boundaries, the spatial proximity of two
class embeddings presents an inherent bias of that embed-
ding space. For instance, if classes “A” and “B” are close
in the embedding space and end up in different parent cat-
egories, this will lead to suboptimal accuracy on the parent
level. Without any prior on the parent taxonomy, which istask-specific and would lead to the parent bias, we can en-
courage the modes of our class embeddings to be approxi-
mately equidistant. In the Euclidean space, this would re-
quire additional regularization. However, it can emerge nat-
urally in hyperbolic space – in the Poincar ´e ball model.
4. From Euclidean to hyperbolic geometry
Segmentation in Euclidean space. Let us formalize the
training process of a deep network for semantic segmenta-
tion in Euclidean space. Given an image I ∈RH×W×3,
we would like to predict label l∈ S for each pixel i∈
{1, ..., HW }, where Sis the set of |S|class labels. An en-
coder fθwith parameters θmapsIto a set of pixel feature
embeddings X= (xi)HW
i=1=f(I)∈RHW×d. In the last
layer, for each pixel i, we obtain a segmentation map, mod-
eled as the class posterior,
p(ˆl=l|xi)∝exp(a⊤
lxi+bl). (2)
Here, {(al, bl)}|S|
l=1are classifier parameters and define
hyperplanes for each class l∈ S in the Euclidean
space. During training, we jointly optimize for Θ :=
θ,{(al, bl)}|S|
l=1	
with backpropagation, minimizing the
(expected) cross-entropy loss for each pixel i,
min
ΘEI
−logp(ˆl=l∗
i|xi)
, (3)
where l∗
iis the ground-truth label for pixel i.
By analogy with the Euclidean setup, the per-pixel clas-
sification in the hyperbolic space involves gyroplanes ( cf.
Fig. 1), defined by offsets and normals [23]. Let us revisit
some basic notions of hyperbolic geometry.
4.1. The Poincar ´e ball model
Poincar ´e Ball and Exponential Map. Hyperbolic ge-
ometry can be expressed with different conformal models
[9]. We operate on the Poincar ´e ball (Dn
c, gDn
c)with−c
denoting the negative curvature, and gDn
cbeing the Rieman-
nian metric associated with the manifold Dn
c={x∈Rn|
c∥x∥<1}.gDn
ccan be linked to the Euclidian metric tensor
gE=Invia:
gDn
c= (λc
x)2gE=2
1−c∥x∥22
gE. (4)
The exponential map between the Euclidean space Rn
and the Poincar ´e ballDn
cwith anchor vis defined as:
Expc
v(x) =v⊕c
tanh(√cλc
v∥x∥
2)x√c∥x∥
,(5)
where ⊕cis the M ¨obius hyperbolic addition defined as:
v⊕cw=(1 + 2 c⟨v, w⟩+c∥w∥2)v+ (1−c∥v∥2)w
1 + 2 c⟨v, w⟩+c2∥v∥2∥w∥2,
(6)
28226
for all v, w∈Dn
c. For simplicity, vis set to the origin 0and
then the considered exponential map is:
Expc
0(x) = tanh(√c∥x∥)x√c∥x∥. (7)
Hyperbolic Distance. The hyperbolic distance between x
andzon the Poincar ´e ball is given by:
dH(x, z) =arcosh
1 + 2∥x−z∥2
(1− ∥x∥2)(1− ∥z∥2)
.(8)
Hyperbolic Multinomial Logistic Regression (MLR).
Given a hyperbolic vector handKclasses, Ganea et al. [23]
provide a geometric interpretation of the hyperbolic MLR
by defining gyroplanes as:
Hc
k={h∈Dn
c| ⟨−rk⊕ch, ak⟩= 0}, (9)
where k∈ {1, ..., K}andrkandakare respectively the gy-
roplane offset and the normal associated with class k. The
hyperbolic distance between hand the gyroplane of class k
is given by:
dH(h, Hc
y) =1√carcsinh2√c|⟨−ry⊕ch, wy⟩|
(1−c∥−ry⊕ch∥2)∥wy∥
.
(10)
Based on this distance, we define the hyperbolic logit:
ζy(h) :=λc
ry∥wy∥
√carcsinh2√c⟨−ry⊕ch, wy⟩
(1−c∥−ry⊕ch∥2)∥wy∥
,
(11)
and the likelihood,
p(ˆy:=y|h)∝exp(ζy(h)). (12)
4.2. Calibrated segmentation in the Poincar ´e ball
Following Atigh et al. [3], we perform the per-pixel clas-
sification in the hyperbolic space. We project Xonto the
Poincar ´e ball with the mapping Expc
0(·)to get the hyper-
bolic embedding H∈DH×W×n. Then, we optimize the
obtained likelihood ( cf. Eq. (12)) with the standard cross-
entropy loss. Once the classification is performed in the
Poincar ´e ball, we link the hyperbolic logits (Eq. (11)) to
the confidence of the prediction, by analogy with Euclidean
networks [26]. To our knowledge, while this extension of
Euclidean calibration to hyperbolic networks is straightfor-
ward, we are the first to present its experimental analysis.
Nevertheless, the analogy with Euclidean space has its lim-
itations. Focusing on the hyperbolic distance, we demon-
strate its concave property w.r.t. Euclidean distance. This
property allows us to establish a distinguishing feature of
the hyperbolic space in modeling inter-class relationships.
4.3. From concavity to flattening bias
On the hyperbolic distance. Hyperbolic geometry natu-
rally embeds hierarchical structure [48]. However, we argue
 0 2 4 6 8 10 12 14
 0  2  4  6  8  10  12  14f(x)
xf(x) =x
f(x) =arcosh (1 +αx2)
Figure 3. Hyperbolic distance exhibits strict concavity w.r.t.
Euclidean distance. Observe (assuming α= 1 for simplicity)
that the hyperbolic distance has sublinear (logarithmic) growth. In
practical terms, this implies that the difference in the distance be-
tween two hyperbolic representations as xincreases will diminish,
while it remains constant in Euclidean space.
here that the hyperbolic space lends itself well also for flat
classification. We observe that during training, the hyper-
bolic embeddings of the same class tend to be pushed to the
periphery of the Poincar ´e ball and onto the same side of the
associated gyroplane [3]. Studying inter-class hyperbolic
distance ( cf. Eq. (10)), we further find that embeddings of
one class are approximately equidistant to the embeddings
of any other class. This implies that, in contrast to the Eu-
clidean space, there is no parent bias in the Poincar ´e ball,
i.e. there is no preference for a specific grouping of child
categories into parents. The following proposition provides
the formal argument explaining our observation:
Proposition 2. The hyperbolic distance between two em-
beddings h1, h2is strictly concave in the Euclidean dis-
tance between h1andh2.
Proof. We can write the hyperbolic distance dHbetween
two embeddings in the Poincar ´e ball as a function of the
Euclidean distance dE(h1, h2). The derivative of dHwith
respect to dE(h1, h2)is then
2
p
(1− ∥h1∥2)(1− ∥h2∥2)q
1 +d2
E
(1−∥h1∥2)(1−∥h2∥2),
(13)
which is strictly decreasing in dE.
Fig. 3 illustrates this proposition. Given the concave
property of the hyperbolic distance, we postulate that the
Poincar ´e ball formulation facilitates distance uniformity be-
tween classes. Since the hyperbolic embeddings are pushed
to the border of the Poincar ´e ball during training, the norms
∥h1∥and∥h2∥in Eq. (8) are close to 1. Therefore, we op-
erate at the high-end spectrum of the domain in Fig. 3 (see
supp. material for further details and empirical support).
28227
DatasetMethodcwECE
Level 1 Level 0CityscapesHSSN∗[34] 0.90 – 5.97 –
HSSN [34] 0.79 0.79 6.85 5.09
Flat-Euc (ours) 0.52 0.60 4.40 3.97
Flat-Hyp (ours) 0.66 0.73 4.35 4.09MapillaryHSSN∗[34] 4.26 – 17.39 –
HSSN [34] 4.47 4.35 18.44 15.59
Flat-Euc (ours) 2.84 3.62 11.0311.26
Flat-Hyp (ours) 2.88 3.38 11.44 11.42IDDHSSN∗[34] 7.85 – 17.74 –
HSSN [34] 7.86 7.56 19.39 17.24
Flat-Euc (ours) 5.94 7.24 12.77 13.43
Flat-Hyp (ours) 6.05 6.27 13.3710.87ACDCHSSN∗[34] 8.12 – 23.54 –
HSSN [34] 6.78 8.40 21.18 20.92
Flat-Euc (ours) 6.55 5.96 16.63 16.49
Flat-Hyp (ours) 6.34 6.03 17.5812.64BDDHSSN∗[34] 8.68 – 26.37 –
HSSN [34] 8.82 6.77 29.13 23.71
Flat-Euc (ours) 7.40 7.05 21.44 20.95
Flat-Hyp (ours) 6.55 6.09 20.7118.58WilddashHSSN∗[34] 14.28 – 32.32 –
HSSN [34] 14.95 11.48 33.21 28.23
Flat-Euc (ours) 13.53 13.16 23.25 23.93
Flat-Hyp (ours) 9.5210.04 20.1718.45
Table 1. Calibration quality (cwECE). We train
DeepLabv3+/ResNet-101 and OCRNet/HRNet-W48 on
Cityscapes (train) and report the results for six datasets.
HSSN∗corresponds to the pretrained model provided by the
authors, only available for DeepLabV3+.
Consequently, changes in the class embeddings have a di-
minished effect on the distance in the Poincar ´e ball, whereas
in the Euclidean space, this relationship is linear. Demon-
strating a practical consequence, the next section experi-
mentally confirms that the hyperbolic space leads to strong
parent-level generalization in semantic segmentation.
5. Experiments
Datasets. Different from prior work, we perform our
analysis by testing the models on 6 datasets: Cityscapes
[15], Mapillary [42], IDD [57], ACDC [47], BDD [64] and
Wilddash [66]. Note that since we train our models on
Cityscapes, the datasets with a larger visual domain shift
(ACDC, BDD and Wilddash) are more challenging.
Metrics. We compare the models in terms of segmen-
tation accuracy and calibration quality, on both the child
and parent nodes (19 and 7 classes, respectively). To evalu-
ate the accuracy, we use the mean Intersection-over-Union
(mIoU), the mean accuracy over classes (mAcc) and the av-
erage pixel accuracy (aAcc). We follow Kull et al. [33] to
derive a calibration metric by comparing the difference be-
tween the confidence and accuracy of class predictions. Wereport the class-wise expected calibration error (cwECE).
Models. We use DeepLabV3+ with ResNet-101 backbone
[11] and OCRNet with HRNet-W48 backbone [65], with
the backbones pre-trained on ImageNet [17]. We train each
model on Cityscapes train with fine annotations [15] for
80K iterations. Leveraging the child class posterior prob-
abilities (Level 0), flat models compute the parent posterior
(Level 1) using Eq. (1).
Implementation details. For a fair comparison, we fol-
low HSSN [34] to set the training hyper-parameters. For
the hyperbolic networks, we use the optimization method
in [23]. We optimize the offsets with Riemannian SGD [7],
and set the learning rate to 0.0001 . The normals are op-
timized with SGD in the Euclidean space, with a learning
rate0.001. The projection onto the Poincar ´e ball uses the
Geoopt library [31], setting curvature to c= 1.
5.1. Quantitative results
Tab. 1 and Tab. 2 report the calibration quality and the
segmentation accuracy, respectively, for the hierarchical
training (HSSN [34]), and the flat Euclidean (Flat-Euc) and
hyperbolic models (Flat-Hyp). To ensure a fair comparison,
we train all models with a consistent codebase and train-
ing schedule. For reference, we also report the results for
HSSN∗with DeepLabV3+ using the weights provided by
the authors [1]. A pre-trained model for OCRNet is not
available. Level 1 and Level 0 refer to the parent (classes
S1) and child (classes S0) nodes in the hierarchical tree, re-
spectively. In the context of our study, we are particularly
interested in Level 1.
Calibration quality. Referencing Tab. 1, we inspect the
calibration quality of DeepLabV3+/ResNet-101 (shaded in
red). For child nodes (Level 0), the flat classifiers exhibit
calibration quality on par with or better than HSSN. For par-
ent nodes (Level 1), Flat-Hyp is better calibrated than Flat-
Euc on challenging datasets ACDC/BDD/Wilddash (-0.21/-
0.85/-4.01). Notably, the gap grows toward the most chal-
lenging testbeds, BDD and Wilddash. The Poincar ´e ball
model outperforms HSSN on five datasets, and, notably, for
the most challenging datasets – BDD and Wilddash (-2.27/-
5.43 w.r.t. HSSN).
Similarly for OCRNet/HRNet-W48 (shaded in blue,
cf. Tab. 1), for child nodes (Level 0), the Poincar ´e ball
is better calibrated by a large margin for the datasets
with a large domain shift, IDD/ACDC/BDD/Wilddash (-
2.56/-3.85/-2.37/-5.48 w.r.t. Flat-Euc; -6.37/-8.28/-5.13/-
9.78 w.r.t. HSSN). For parent nodes (Level 1), Flat-Hyp
is better calibrated than its competitors for datasets Map-
illary/IDD/BDD/Wilddash (-0.24/-0.97/-1.04/-3.12 w.r.t.
Flat-Euc; -0.97/-1.29/-0.68/-1.44 w.r.t. HSSN).
Segmentation accuracy. Let us examine the segmen-
tation accuracy of DeepLabV3+/ResNet-101 in Tab. 2.
For child nodes (Level 0), flat classifiers Flat-Euc
28228
DatasetMethodLevel 1 Level 0
mIoU mAcc aAcc mIoU mAcc aAccCityscapesHSSN∗[34] 90.82 – 94.92 – 97.35 – 81.62 – 87.90 – 96.16 –
HSSN [34] 90.68 90.27 94.47 94.25 97.29 97.20 80.3080.21 86.11 86.62 95.93 95.98
Flat-Euc (ours) 90.9690.57 95.2494.80 97.6497.52 80.89 79.82 87.5387.34 96.5696.27
Flat-Hyp (ours) 90.91 90.47 95.08 94.67 97.60 97.48 80.36 79.28 86.83 86.80 96.41 96.26MapillaryHSSN∗[34] 83.26 – 89.37 – 94.70 – 62.32 – 72.53 – 90.37 –
HSSN [34] 81.89 79.09 88.35 86.19 93.77 91.91 59.34 59.09 70.37 70.62 89.53 87.54
Flat-Euc (ours) 83.11 80.27 90.73 89.14 93.98 93.7 63.9460.47 76.4474.62 90.47 90.12
Flat-Hyp (ours) 81.8781.89 90.1589.23 93.4194.9 60.34 58.66 74.88 73.55 89.6090.96IDDHSSN∗[34] 79.03 – 84.37 – 94.46 – 58.33 – 70.05 – 91.57 –
HSSN [34] 78.52 77.69 84.00 83.01 94.45 94.29 55.21 58.14 67.37 67.21 90.57 91.79
Flat-Euc (ours) 81.27 79.30 87.08 85.03 95.29 94.90 61.64 58.50 73.70 71.65 92.22 91.89
Flat-Hyp (ours) 80.9879.83 86.7085.25 95.5495.27 58.7659.01 71.5572.88 91.9792.14ACDCHSSN∗[34] 65.56 – 78.60 – 86.62 – 42.97 – 57.33 – 81.62 –
HSSN [34] 73.45 69.43 82.00 78.78 90.64 87.41 52.71 49.20 62.62 61.40 84.95 82.10
Flat-Euc (ours) 75.1871.29 83.8683.47 91.8090.38 54.3451.90 66.75 64.56 86.7985.99
Flat-Hyp (ours) 72.90 70.88 83.66 82.58 91.78 90.20 47.72 49.31 62.4667.19 85.37 85.85BDDHSSN∗[34] 73.53 – 82.25 – 89.60 – 48.32 – 60.54 – 86.76 –
HSSN [34] 74.28 73.66 81.88 81.10 90.11 90.08 47.75 48.22 57.96 60.18 86.85 87.26
Flat-Euc (ours) 76.27 74.47 84.64 83.52 91.22 90.62 51.5450.04 64.58 63.12 88.52 88.05
Flat-Hyp (ours) 76.4976.62 85.1284.34 91.9192.12 49.64 49.60 63.2164.37 89.0089.31WilddashHSSN∗[34] 57.20 – 71.42 – 76.85 – 36.55 – 50.61 – 71.62 –
HSSN [34] 58.60 59.80 71.65 71.22 76.49 79.74 37.07 39.01 50.03 50.86 71.12 74.42
Flat-Euc (ours) 58.98 57.11 74.15 72.64 76.75 75.74 39.3839.97 55.97 54.31 71.84 70.84
Flat-Hyp (ours) 62.5362.52 77.0275.77 81.9181.88 40.57 39.48 57.1757.57 76.2976.35
Table 2. Segmentation accuracy (mIoU, mAcc). We train DeepLabv3+/ResNet-101 and OCRNet/HRNet-W48 on Cityscapes train and
test them on six datasets. HSSN∗corresponds to the pretrained model given by the authors, only available for DeepLabV3+.
and Flat-Hyp substantially outperform HSSN on Mapil-
lary/IDD/ACDC/BDD/Wilddash, in terms of mAcc, aAcc
and mIoU. For parent nodes (Level 1), the Poincar ´e ball
model outperforms the Euclidean model and HSSN, in
terms of aAcc for Mapillary/IDD/BDD/Wilddash. For
the most challenging datasets, BDD/Wilddash, Flat-
Hyp clearly reaches the best results in terms of mIoU
(+0.22/+3.55 w.r.t. Flat-Euc; +2.21/+3.93 w.r.t. HSSN)
and mAcc (+0.48/+2.87 w.r.t. Flat-Euc, +3.24/+5.37
w.r.t. HSSN). Even on the less challenging datasets
(Cityscapes/Mapillary/IDD/ACDC), where Flat-Euc out-
performs the hyperbolic model on the parent-level predic-
tions, the difference in mIoU between the child and parent
nodes in the Euclidean and the hyperbolic model reduces
(from -0.53/-3.60/-2.88/-6.62 to -0.07/-1.24/-0.29/-2.28) in
favor of the hyperbolic model. This observation supports
our analytical analysis of the parent bias in Sec. 4.3.
Similarly for OCRNet/HRNet-W48, we ob-
serve that Flat-Hyp shows the best mAcc for
IDD/ACDC/BDD/Wilddash on Level 0, by a significant
margin. For parent nodes (Level 1), Flat-Hyp outperforms
hierarchical training and Flat-Euc in terms of aAcc on
Mapillary/IDD/BDD/Wilddash. For the most challenging
datasets, Flat-Hyp also clearly outperforms its competi-
tors in terms of mIoU (+2.15/+5.41 w.r.t. the Flat-Euc;+2.96/+2.72 w.r.t. HSSN, on BDD/Wilddash) and mAcc
(+0.82/+3.13 w.r.t. Flat-Euc; +3.24/+4.55 w.r.t. HSSN,
on BDD/Wilddash). Similarly to DeepLabV3+, on less
challenging datasets, Cityscapes/Mapillary/IDD/ACDC,
the difference in mIoU scores from child to parent levels is
larger for Flat-Hyp. Unlike DeepLabV3+, the hyperbolic
model outperforms the Euclidean model (+1.62) on the
Mapillary dataset.
5.2. Qualitative results
In Fig. 4, we visualize an example of semantic segmen-
tation on Level 1, comparing HSSN to our Euclidean and
hyperbolic networks. We observe that HSSN mislabels
parts of the “Building” as a “Vehicle”. Notably, the confi-
dence of this incorrect prediction is high. By contrast, both
Euclidean and the hyperbolic networks largely predict the
“Building” correctly, although with higher uncertainty than
HSSN. Rather curiously, the hyperbolic network exhibits a
spatially smoother confidence map, which suggests a higher
leverage of spatial correlations in the Poincar ´e ball model.
5.3. Discussion
Overall, the empirical results of segmentation accuracy
and calibration strongly support our analysis in Sec. 3 and
Sec. 4.1. On the one hand, the hierarchical training in HSSN
28229
Figure 4. Qualitative results on Wilddash. We show parent-level segmentation results for HSSN (left), Flat-Euc (middle), and Flat-Hyp
(right) models by using label predictions (top), pixel-level accuracy maps (middle row), and confidence maps (bottom).
leads to poor performance on novel domains, presumably
due to the large bias from the training on Cityscapes. By
contrast, our flat classifiers, be they Euclidean or hyper-
bolic, are much more resilient to the domain shift, both
in terms of calibration quality and segmentation accuracy,
which is in line with our intuitive analysis in Sec. 3. The hy-
perbolic model exhibits a larger gap between the accuracy
of the child and the parent nodes, compared to the Euclidean
model. In cases, where both have comparable scores for
child nodes, the hyperbolic model significantly outperforms
the Euclidean model for parent nodes, and similarly for cal-
ibration. This suggests the advantage of the Poincar ´e ball
in flattening the Euclidean parent bias, as we conjectured in
Sec. 4.3. Importantly, these conclusions are consistent for
both DeepLabV3+ and OCRNet.
Limitations. We have only evaluated on datasets with a
strong focus on traffic scenes. This limitation comes from
the lack of datasets with other taxonomic structures and of
comparable visual complexity. Our analysis also focuses on
the Poincar ´e ball, although this is not the only possible re-
alization of the hyperbolic space. An extension of our study
to other conformal models, such as the Lorentz model, of-
fers exciting avenues for future research.6. Conclusion
Our empirical investigations show that semantically
meaningful hierarchical relations might not be the primary
driver of the reported improvements in segmentation ac-
curacy. A contrario , our cross-domain experiments reveal
that flat segmentation networks, in which the parent cat-
egories are inferred from children, outperform hierarchi-
cal approaches consistently, for parent nodes most notably.
However, we show that flat classifiers may suffer from the
parent bias in the Euclidean space. By contrast, the Poincar ´e
ball model exhibits more uniform properties between class
representations. We demonstrate that a flat hyperbolic clas-
sifier coupled with a straightforward bottom-up inference
generalizes surprisingly well across unseen test domains.
It tends to outperform, for parent categories, the Euclidean
representation in terms of the segmentation accuracy and
calibration on the more challenging datasets. To our knowl-
edge, our work is also the first empirical analysis of dense
calibration with hyperbolic networks. We hope that our
study will encourage future efforts toward more accurate
and calibrated semantic segmentation models, extending
beyond the currently mainstream Euclidean representation.
Acknowledgement. This work was supported by the ERC Ad-
vanced Grant SIMULACRON.
28230
References
[1]https : / / github . com / lingorX / HieraSeg /
tree/main/Pytorch . [Online; accessed 28-March-
2024]. 6
[2] Pablo Arbelaez, Michael Maire, Charless C. Fowlkes, and
Jitendra Malik. Contour detection and hierarchical image
segmentation. IEEE TPAMI , 33(5):898–916, 2011. 2
[3] Mina Ghadimi Atigh, Julian Schoep, Erman Acar, Nanne van
Noord, and Pascal Mettes. Hyperbolic image segmentation.
InCVPR , 2022. 3, 5
[4] Rohit Babbar, Ioannis Partalas, ´Eric Gaussier, and Massih-
Reza Amini. On flat versus hierarchical classification in
large-scale taxonomies. In NIPS , 2013. 2
[5] Maxim Berman, Amal Rannen Triki, and Matthew B.
Blaschko. The Lov ´asz-Softmax loss: A tractable surrogate
for the optimization of the intersection-over-union measure
in neural networks. In CVPR , 2018. 2
[6] Luca Bertinetto, Romain M ¨uller, Konstantinos Tertikas, Sina
Samangooei, and Nicholas A. Lord. Making better mistakes:
Leveraging class hierarchies with deep networks. In CVPR ,
2020. 2
[7] Silvere Bonnabel. Stochastic gradient descent on riemannian
manifolds. IEEE Transactions on Automatic Control , 58(9):
2217–2229, 2013. 6
[8] Martin R Bridson and Andr ´e Haefliger. Metric spaces of
non-positive curvature . Springer Science & Business Media,
2013. 3
[9] James W. Cannon, William J. Floyd, Walter R. Parry, and et
al. Hyperbolic geometry. In Flavors of Geometry , 1997. 4
[10] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian
Schroff, and Hartwig Adam. Encoder-decoder with atrous
separable convolution for semantic image segmentation. In
ECCV , 2018. 2, 3
[11] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian
Schroff, and Hartwig Adam. Encoder-decoder with atrous
separable convolution for semantic image segmentation. In
ECCV , 2018. 6
[12] Bowen Cheng, Ross B. Girshick, Piotr Doll ´ar, Alexander C.
Berg, and Alexander Kirillov. Boundary iou: Improving
object-centric image segmentation evaluation. In CVPR ,
2021. 2
[13] Bowen Cheng, Alexander G. Schwing, and Alexander Kir-
illov. Per-pixel classification is not all you need for semantic
segmentation. In NeurIPS , 2021. 2
[14] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In CVPR ,
2016. 2, 3
[15] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In CVPR ,
2016. 6
[16] Marius Cordts, Timo Rehfeld, Markus Enzweiler, Uwe
Franke, and Stefan Roth. Tree-structured models for efficientmulti-cue scene labeling. IEEE TPAMI , 39(7):1444–1454,
2017. 2
[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. ImageNet: A large-scale hierarchical image
database. In CVPR , 2009. 6
[18] Ivica Dimitrovski, Dragi Kocev, Suzana Loskovska, and
Saso Dzeroski. Hierarchical annotation of medical images.
Pattern Recognit. , 44(10-11):2436–2449, 2011. 2
[19] Roman Eisner, Brett Poulin, Duane Szafron, Paul Lu, and
Russell Greiner. Improving protein function prediction us-
ing the hierarchical structure of the gene ontology. In IEEE
CIBCB , 2005. 2
[20] Aleksandr Ermolov, Leyla Mirvakhabova, Valentin
Khrulkov, Nicu Sebe, and Ivan V . Oseledets. Hyper-
bolic vision transformers: Combining improvements in
metric learning. In CVPR , 2022. 3
[21] Luca Franco, Paolo Mandica, Konstantinos Kallidromitis,
Devin Guillory, Yu-Teng Li, and Fabio Galasso. Hyper-
bolic active learning for semantic segmentation under do-
main shift. arXiv:2306.11180 [cs.CV] , 2023. 3
[22] Andrea Frome, Gregory S. Corrado, Jonathon Shlens, Samy
Bengio, Jeffrey Dean, Marc’Aurelio Ranzato, and Tom ´as
Mikolov. Devise: A deep visual-semantic embedding model.
InNIPS , 2013. 2
[23] Octavian-Eugen Ganea, Gary B ´ecigneul, and Thomas Hof-
mann. Hyperbolic neural networks. In NIPS , 2018. 3, 4, 5,
6
[24] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In CVPR , 2014. 2
[25] Allan D Gordon. A review of hierarchical classification.
Journal of the Royal Statistical Society: Series A (General) ,
150(2):119–137, 1987. 2
[26] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger.
On calibration of modern neural networks. In CVPR , 2016.
5
[27] Yunhui Guo, Xudong Wang, Yubei Chen, and Stella X. Yu.
Clipped hyperbolic classifiers are super-hyperbolic classi-
fiers. In CVPR , 2022. 3
[28] Joy Hsu, Jeffrey Gu, Gong Her Wu, Wah Chiu, and Ser-
ena Yeung. Capturing implicit hierarchical structure in 3D
biomedical images with self-supervised hyperbolic represen-
tations. In NeurIPS , 2021. 3
[29] Carlos Nascimento Silla Jr. and Alex Alves Freitas. A sur-
vey of hierarchical classification across different application
domains. Data Min. Knowl. Discov. , 22(1-2):31–72, 2011. 2
[30] Svetlana Kiritchenko, Stan Matwin, Richard Nock, and
A. Fazel Famili. Learning and evaluation in the presence
of class hierarchies: Application to text categorization. In
Adv. in Art. Intell. , 2006. 2
[31] Max Kochurov, Rasul Karimov, and Serge Kozlukov.
Geoopt: Riemannian optimization in pytorch, 2020. 6
[32] Daphne Koller and Mehran Sahami. Hierarchically classify-
ing documents using very few words. In ICML , 1997. 2
[33] Meelis Kull, Miquel Perello Nieto, Markus K ¨angsepp,
Telmo Silva Filho, Hao Song, and Peter Flach. Beyond tem-
perature scaling: Obtaining well-calibrated multi-class prob-
abilities with dirichlet calibration. NIPS , 32, 2019. 6
28231
[34] Liulei Li, Tianfei Zhou, Wenguan Wang, Jianwu Li, and Yi
Yang. Deep hierarchical semantic segmentation. In CVPR ,
2022. 2, 3, 4, 6, 7
[35] Liulei Li, Wenguan Wang, and Yi Yang. LogicSeg: Parsing
visual semantics with neural logic learning and reasoning. In
ICCV , 2023. 2, 3, 4
[36] Zhiheng Li, Wenxuan Bao, Jiayang Zheng, and Chenliang
Xu. Deep grouping model for unified perceptual parsing. In
CVPR , 2020. 1
[37] Xiaodan Liang, Hongfei Zhou, and Eric Xing. Dynamic-
structured semantic propagation network. In CVPR , 2018.
1
[38] Pauline Luc, Camille Couprie, Soumith Chintala, and Jakob
Verbeek. Semantic segmentation using adversarial networks.
arXiv:1611.08408 [cs.CV] , 2016. 2
[39] Panagiotis Meletis and Gijs Dubbelman. Training of con-
volutional networks on multiple heterogeneous datasets for
street scene semantic segmentation. In IEEE IV , 2018. 1
[40] Pascal Mettes, Mina Ghadimi Atigh, Martin Keller-Ressel,
Jeffrey Gu, and Serena Yeung. Hyperbolic deep learning
in computer vision: A survey. arXiv:2305.06611 [cs.CV] ,
2023. 2, 3
[41] Sudhanshu Mittal, Maxim Tatarchenko, and Thomas Brox.
Semi-supervised semantic segmentation with high- and low-
level consistency. IEEE TPAMI , 43(4):1369–1379, 2021. 2
[42] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and
Peter Kontschieder. The mapillary vistas dataset for semantic
understanding of street scenes. In ICCV , pages 4990–4999,
2017. 6
[43] Maximilian Nickel and Douwe Kiela. Poincar ´e embeddings
for learning hierarchical representations. In NIPS , 2017. 3
[44] Yassine Ouali, C ´eline Hudelot, and Myriam Tami. Semi-
supervised semantic segmentation with cross-consistency
training. In CVPR , 2020. 2
[45] Ioannis Partalas, Aris Kosmopoulos, Nicolas Baskiotis,
Thierry Arti `eres, George Paliouras, ´Eric Gaussier, Ion An-
droutsopoulos, Massih-Reza Amini, and Patrick Gallinari.
LSHTC: A benchmark for large-scale text classification.
arXiv:1503.08581 [cs.IR] , 2015. 2
[46] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
InMICCAI , 2015. 2
[47] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. ACDC:
The adverse conditions dataset with correspondences for se-
mantic driving scene understanding. In ICCV , 2021. 6
[48] Frederic Sala, Christopher De Sa, Albert Gu, and Christo-
pher R ´e. Representation tradeoffs for hyperbolic embed-
dings. In ICML , 2018. 3, 5
[49] Paul Schnitzspan, Mario Fritz, Stefan Roth, and Bernt
Schiele. Discriminative structure learning of hierarchical
representations for object detection. In CVPR , 2009. 2
[50] Abhishek Sharma, Oncel Tuzel, and David W. Jacobs. Deep
hierarchical parsing for semantic segmentation. In CVPR ,
2015. 2
[51] Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. IEEE
TPAMI , 39(4):640–651, 2017. 2, 3[52] Carole H. Sudre, Wenqi Li, Tom Vercauteren, S ´ebastien
Ourselin, and M. Jorge Cardoso. Generalised dice overlap
as a deep learning loss function for highly unbalanced seg-
mentations. In MICCAI Workshops , 2017. 2
[53] Didac Suris, Ruoshi Liu, and Carl V ondrick. Learning the
predictability of the future. In CVPR , 2021. 3
[54] Jasper R. R. Uijlings, Koen E. A. van de Sande, Theo Gevers,
and Arnold W. M. Smeulders. Selective search for object
recognition. IJCV , 104(2):154–171, 2013. 2
[55] Jack Valmadre. Hierarchical classification at multiple oper-
ating points. In NeurIPS , 2022. 2
[56] Max van Spengler, Erwin Berkhout, and Pascal Mettes.
Poincare resnet. In ICCV , 2023. 3
[57] Girish Varma, Anbumani Subramanian, Anoop Namboodiri,
Manmohan Chandraker, and CV Jawahar. Idd: A dataset
for exploring problems of autonomous navigation in uncon-
strained environments. In WACV , 2019. 6
[58] Celine Vens, Jan Struyf, Leander Schietgat, Saso Dzeroski,
and Hendrik Blockeel. Decision trees for hierarchical multi-
label classification. Mach. Learn. , 73(2):185–214, 2008. 2
[59] Wenguan Wang, Zhijie Zhang, Siyuan Qi, Jianbing Shen,
Yanwei Pang, and Ling Shao. Learning compositional neural
information fusion for human parsing. In ICCV , 2019. 1
[60] Wenguan Wang, Hailong Zhu, Jifeng Dai, Yanwei Pang,
Jianbing Shen, and Ling Shao. Hierarchical human parsing
with typed part-relation reasoning. In CVPR , 2020. 1
[61] Xiaolin Wang and Bao-Liang Lu. Flatten hierarchies for
large-scale hierarchical text categorization. In ICDIM , 2010.
2
[62] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and
Jian Sun. Unified perceptual parsing for scene understand-
ing. In ECCV , 2018. 1
[63] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon,
Thomas M. Breuel, Jan Kautz, and Xiaolong Wang.
Groupvit: Semantic segmentation emerges from text super-
vision. In CVPR , 2022. 2
[64] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying
Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Dar-
rell. BDD100K: A diverse driving dataset for heterogeneous
multitask learning. In CVPR , 2020. 6
[65] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-
contextual representations for semantic segmentation. In
ECCV , 2020. 6
[66] Oliver Zendel, Katrin Honauer, Markus Murschitz, Daniel
Steininger, and Gustavo Fernandez Dominguez. Wilddash-
creating hazard-aware benchmarks. In ECCV , 2018. 6
[67] Long Zhu, Yuanhao Chen, Alan L. Yuille, and William T.
Freeman. Latent hierarchical structural learning for object
detection. In CVPR , 2010. 2
28232
