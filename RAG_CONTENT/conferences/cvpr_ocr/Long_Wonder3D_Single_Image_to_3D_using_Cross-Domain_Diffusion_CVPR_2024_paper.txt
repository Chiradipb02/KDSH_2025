Wonder3D: Single Image to 3D using Cross-Domain Diffusion
Xiaoxiao Long1,3∗, Yuan-Chen Guo2∗‡, Cheng Lin1†, Yuan Liu1, Zhiyang Dou1
Lingjie Liu4, Yuexin Ma5, Song-Hai Zhang2, Marc Habermann6, Christian Theobalt6, Wenping Wang7†
1The University of Hong Kong2Tsinghua University3V AST
4University of Pennsylvania5Shanghai Tech University6MPI Informatik7Texas A&M University
∗Core contributions†Corresponding authors‡Intern at V AST
https://www.xxlong.site/Wonder3D/
Input images Generated multi -view normal maps and color images Textured meshes
Figure 1. Wonder3D reconstructs highly-detailed textured meshes from a single-view image in only 2∼3minutes. Wonder3D first
generates consistent multi-view normal maps with corresponding color images via a cross-domain diffusion model, and then leverages
a novel normal fusion method to achieve fast and high-quality reconstruction. Compared to the prior works, Wonder3D has achieved a
leading-level of geometric details with high efficiency.
1
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
9970
Abstract
In this work, we introduce Wonder3D , a novel method
for efficiently generating high-fidelity textured meshes from
single-view images. Recent methods based on Score Dis-
tillation Sampling (SDS) have shown the potential to re-
cover 3D geometry from 2D diffusion priors, but they typ-
ically suffer from time-consuming per-shape optimization
and inconsistent geometry. In contrast, certain works di-
rectly produce 3D information via fast network inferences,
but their results are often of low quality and lack geomet-
ric details. To holistically improve the quality, consistency,
and efficiency of single-view reconstruction tasks, we pro-
pose a cross-domain diffusion model that generates multi-
view normal maps and the corresponding color images. To
ensure the consistency of generation, we employ a multi-
view cross-domain attention mechanism that facilitates in-
formation exchange across views and modalities. Lastly, we
introduce a geometry-aware normal fusion algorithm that
extracts high-quality surfaces from the multi-view 2D rep-
resentations in only 2∼3minutes. Our extensive evalu-
ations demonstrate that our method achieves high-quality
reconstruction results, robust generalization, and good effi-
ciency compared to prior works.
1. Introduction
Reconstructing 3D geometry from a single image [13, 26,
33, 38, 40, 42, 45] stands as a fundamental task in com-
puter graphics and 3D computer vision, benefiting a wide
range of versatile applications such as novel view synthe-
sis [7, 25, 35], 3D content creation [36, 48], and robotics
grasping [29, 79]. However, this task is notably challenging
since it is ill-posed and demands the ability to discern the
3D geometry of both visible and invisible parts. This ability
requires extensive knowledge of the 3D world.
Recently, the field of 3D generation has experienced
rapid and flourishing development with the introduction of
diffusion models. A growing body of research [5, 31, 47,
63, 67], such as DreamField [24], DreamFusion [47], and
Magic3D [31], resort to distilling prior knowledge of 2D
image diffusion models or vision language models to create
3D models from text or images via Score Distillation Sam-
pling (SDS) [47]. Despite their compelling results, these
methods suffer from two main limitations: efficiency and
consistency . The per-shape optimization process typically
entails tens of thousands of iterations, involving full-image
volume rendering and inferences of the diffusion models.
Consequently, it often consumes tens of minutes or even
hours on per-shape optimization. Moreover, the 2D prior
model operates by considering only a single view at each
iteration and strives to make every view resemble the input
image. This often results in the generation of 3D shapesexhibiting inconsistencies, thus, often leading to the gen-
eration of 3D shapes with inconsistencies such as multiple
faces (i.e., the Janus problem [47]).
There exists another group of works that endeavor to di-
rectly produce 3D geometries like point clouds [41, 45, 75,
80], meshes [16, 37], neural fields [1, 4, 8, 15, 17, 21, 26–
28, 44, 46, 65, 76] via network inference to avoid time-
consuming per-shape optimization. Most of them attempt
to train 3D generative diffusion models from scratch on 3D
assets. However, due to the limited size of publicly avail-
able 3D datasets, these methods demonstrate poor general-
izability , most of which can only generate shapes on spe-
cific categories.
More recently, several methods have emerged that di-
rectly generate multi-view 2D images, with representative
works including SyncDreamer [36] and MVDream [55]. By
enhancing the multi-view consistency of image generation,
these methods can recover 3D shapes from the generated
multi-view images. Our method also adopts a multi-view
generation scheme to favor the flexibility and efficiency of
2D representations. However, due to only relying on color
images, the fidelity of the generated shapes is not well-
maintained, and they struggle to recover geometric details
or come with enormous computational costs.
To better address the issues of fidelity, consistency, gen-
eralizability and efficiency in the aforementioned works,
in this paper, we introduce a new approach to the task
of single-view 3D reconstruction by generating multi-view
consistent normal maps and their corresponding color im-
ages with a cross-domain diffusion model. The key idea is
to extend diffusion frameworks to model the joint distribu-
tion of two different domains, i.e., normals and colors. The
normal map characterizes the undulations and variations
presented on the surface of the shape, thus encoding rich
detailed geometric information. This allows for the high-
fidelity extraction of 3D geometry from 2D normal maps.
Adopting the 2D representations enables Wonder3D to be
built on the pre-trained Stable Diffusion model [49], where
strong priors facilitate zero-shot generalization ability.
The following technical designs of Wonder3D make it a
robust and efficient tool to create 3D shapes from single im-
ages. 1) Cross-domain switcher . The introduced domain
switcher allows the diffusion model to generate either nor-
mal maps or color images without significantly modifying
the original model. 2) Cross-domain attentions . We fur-
ther leverage cross-domain attention mechanisms to assist
in the information exchange between the two domains, ulti-
mately improving consistency and quality. This mechanism
facilitates information perception across different domains,
enabling our method to recover high-fidelity geometry. 3)
Geometry-aware normal fusion . In order to stably extract
surfaces from the generated views, we propose a geometry-
aware normal fusion algorithm that is robust to subtle inac-
9971
CLIPGeometry
Fusion
Normal
 Color
⨁
⋯
⋯
Cameras Domain switcher
Single RGB image Generated normals and colorsTextured meshUNet
Figure 2. Overview of Wonder3D . Given a single image, Wonder3D takes the input image, the text embedding produced by CLIP
model [49], the camera parameters of multiple views, and a domain switcher as conditioning to generate consistent multi-view normal
maps and color images. Subsequently, Wonder3D employs an innovative normal fusion algorithm to robustly reconstruct high-quality 3D
geometry from the 2D representations, yielding high-fidelity textured meshes.
curate generations and capable of reconstructing clean and
high-quality geometries (see Figure 1).
We conduct extensive experiments on the Google
Scanned Object dataset [14] and various 2D images with
different styles. The experiments validate that Wonder3D
achieves a leading level of geometric details with high ef-
ficiency and strong generalization among current zero-shot
single-view reconstruction methods.
2. Related Works
2.1. 2D Diffusion Models for 3D Generation
Recent compelling successes in 2D diffusion models [9,
22, 51] and large vision language models (e.g., CLIP
model [49]) provide new possibilities for generating 3D as-
sets using the strong priors of 2D diffusion models. Pi-
oneering works DreamFusion [47] and SJC [63] propose
to distill a 2D text-to-image generation model to generate
3D shapes from texts, and many follow-up works follow
such per-shape optimization scheme. For the task of text-
to-3D [2, 5, 6, 23, 31, 52, 53, 61, 67, 69, 73, 82] or image-
to-3D synthesis [42, 48, 50, 54, 58, 71], these methods typ-
ically optimize a 3D representation (i.e., NeRF, mesh, or
SDF), and then leverage neural rendering to generate 2D
images from various viewpoints. The images are then fed
into the 2D diffusion models or CLIP model for calculat-
ing SDS [47] losses, which guide the 3D shape optimiza-
tion. However, most of these methods always suffer from
low efficiency and multi-face problem, where a per-shape
optimization consumes tens of minutes and the optimized
geometry tends to produce multiple faces due to the lack of
explicit 3D supervision.
2.2. 3D Generative Models
Instead of performing a time-consuming per-shape opti-
mization guided by 2D diffusion models, some works at-
tempt to directly train 3D diffusion models based on vari-
ous 3D representations, like point clouds [41, 45, 75, 80],meshes [16, 37], neural fields [1, 4, 8, 15, 17, 21, 26–
28, 44, 46, 65, 76] However, due to the limited size of pub-
lic available 3D assets dataset, most of the works have only
been validated on limited categories of shapes, and how to
scale up on large datasets is still an open problem. On the
contrary, our method adopts 2D representations and, thus,
can be built upon the 2D diffusion models [51] whose pre-
trained priors significantly facilitate zero-shot generaliza-
tion ability.
2.3. Multi-view Diffusion Models
To generate consistent multi-view images, some efforts [3,
11, 18, 30, 34, 57, 59, 60, 62, 68, 70, 72, 74, 77, 81] are
made to extend 2D diffusion models from single-view im-
ages to multi-view images. However, most of these meth-
ods focus on image generation and are not designed for 3D
reconstruction. The recent works Viewset Diffusion [57],
SyncDreamer [36], and MVDream [55] share a similar idea
to produce consistent multi-view color images via attention
layers. However, unlike that normal maps explicitly encode
geometric information, reconstruction from color images
always suffers from texture ambiguity, and, thus, they either
struggle to recover geometric details or require huge com-
putational costs. SyncDreamer [36] requires dense views
for 3D reconstruction but still suffers from low-quality ge-
ometry and blurring textures. MVDream [55] still resorts
to a time-consuming optimization using SDS loss for 3D
reconstruction, and its multi-view distillation scheme re-
quires 1.5 hours. In contrast, our method can reconstruct
high-quality textured meshes in just 2 minutes.
3. Problem Formulation
3.1. Diffusion Models
Diffusion models [22, 56] are first proposed to gradually re-
cover images from a specifically designed degradation pro-
cess, where a forward Markov chain and a Reverse Markov
chain are adopted. The forward process will be iteratively
applied to the target image zuntil the image becomes com-
9972
plete Gaussian noise at the end. On the contrary, the reverse
chain then is employed to iteratively denoise the corrupted
image, i.e., recovering zt−1fromztby predicting the added
random noise ϵ. The readers can refer to [22, 56] for more
details about image diffusion models.
3.2. The Distribution of 3D Assets
Unlike that prior works adopt 3D representations like point
clouds, tri-planes, or neural radiance fields, we propose that
the distribution of 3D assets, denoted as pa(z), can be mod-
eled as a joint distribution of its corresponding 2D multi-
view normal maps and color images. Specifically, given a
set of cameras {π1,π2,···,πK}and a conditional input
image y, we have
pa(z) =pnc 
n1:K, x1:K|y
, (1)
where pncis the distribution of the normal maps n1:Kand
color images x1:Kobserved from a 3D asset conditioned on
an image y. For simplicity, we omit the symbol yfor this
equation in the following discussions. Therefore, our goal
is to learn a model fthat synthesizes multiple normal maps
and color images of a set of camera poses denoted as
(n1:K, x1:K) =f(y,π1:K). (2)
Finally, we can formulate this cross-domain joint distri-
bution as a Markov chain within the diffusion scheme:
p
n(1:K)
T, x(1:K)
TY
tpnc
n(1:K)
t−1, x(1:K)
t−1|n(1:K)
t, x(1:K)
t
,
(3)
where p
n(1:K)
T, x(1:K)
T
are Gaussian noises. Our key
problem is to characterize the distribution pnc, so that we
can sample from this Markov chain to generate normal
maps and images.
4. Method
As per our problem formulation in Section 3.2, we propose
a multi-view cross-domain diffusion scheme, which oper-
ates on two distinct domains to generate multi-view consis-
tent normal maps and color images. The overview of our
method is presented in Figure 2. First, our method adopts
a multi-view diffusion scheme to generate multi-view nor-
mal maps and color images, and enforces the consistency
across different views using multi-view attentions (see Sec-
tion 4.1). Second, our proposed domain switcher allows the
diffusion model to operate on more than one domain. A
cross-domain attention is proposed to propagate informa-
tion between the normal domain and color image domain
ensuring geometric and visual coherence between the two
domains (see Section 4.2). Finally, our novel geometry-
aware normal fusion reconstructs the high-quality geometry
and appearance from the multi-view 2D normal and color
images (see Section 4.3).4.1. Consistent Multi-view Generation
The prior 2D diffusion models [33, 49] generate each image
separately, so that the resulting images are not geometrically
and visually consistent across different views. To enhance
consistency among different views, similar to prior works
such as SyncDreamer [36] and MVDream [55], we utilize
attention mechanism to facilitate information propagation
across different views, implicitly encoding multi-view de-
pendencies (as illustrated in Figure 3).
This is achieved by extending the original self-attention
layers to be global-aware, allowing connections to other
views within the attention layers. Keys and values from dif-
ferent views are connected to each other to facilitate the ex-
change of information. By sharing information across dif-
ferent views within the attention layers, the diffusion model
perceives multi-view correlation and is capable of generat-
ing consistent multi-view color images and normal maps.
4.2. Cross-Domain Diffusion
Our model is built upon pre-trained stable diffusion mod-
els [49] to leverage its strong priors. Given that current 2D
diffusion models [33, 49] are designed for a single domain,
the main challenge lies in how to effectively extend stable
diffusion models to support multi-domain operations.
Naive Solutions. To achieve this goal, we explore several
possible designs. A straightforward solution is to add four
more channels to the output of the UNet module represent-
ing the extra domain. Therefore, the diffusion model can
simultaneously output normals and color image domains.
However, we notice that such a design suffers from low
convergence speed and poor generalization. This is because
the channel expansion may perturb the pre-trained weights
of stable diffusion models and therefore cause catastrophic
model forgetting.
Revisiting Eq. 1, it is possible to factor the joint distribu-
tion into two conditional distributions:
qa(z) =qn(n1:K)·qc 
x1:K|n1:K
. (4)
This equation suggests an alternative solution where we
could initially train a diffusion model to generate normal
maps and then train another diffusion model to produce
color images, conditioning on the generated normal maps
(or vice versa). Nonetheless, the implementation of this
two-stage framework introduces certain complications. It
not only substantially increases the computational cost but
also results in performance degradation. Please refer to Sec-
tion 5.6 for an in-depth discussion.
Domain Switcher. To overcome these difficulties men-
tioned above, we design a cross-domain diffusion scheme
via a domain switcher , denoted as s. The switcher sis
a one-dimensional vector serving as labels for distinct do-
mains, and we further feed the switcher into the diffusion
9973
ResBlock
Multi -view 
SelfAttention
Cross -domain
SelfAttention
CrossAttention
Multi -view normals /colors
Paired normal & colorFigure 3. The illustration of the structure of the multi-view cross-
domain transformer block.
model as an extra input. Therefore, the formulation of Eq. 2
can be extended as:
n1:K, x1:K=f(y,π1:K, sn), f(y,π1:K, sc). (5)
The domain switcher sis first encoded via positional en-
coding [43] and subsequently concatenated with the time
embedding. This combined representation is then injected
into the UNet of the stable diffusion models. Interestingly,
experiments show that this subtle modification does not sig-
nificantly alter the pre-trained priors. As a result, it allows
for fast convergence and robust generalization, without re-
quiring substantial changes to the stable diffusion models.
Cross-domain Attention. Using the proposed domain
switcher, the diffusion model can generate two different do-
mains. However, it is important to note that for a single
view, there is no guarantee that the generated color image
and the normal map will be geometrically consistent. To ad-
dress this issue and ensure the consistency between the gen-
erated normal maps and color images, we introduce a cross-
domain attention mechanism to facilitate the exchange of
information between the two domains. This mechanism
aims to ensure that the generated outputs align well in terms
of geometry and appearance.
The cross-domain attention layer maintains the same
structure as the original self-attention layer and is integrated
before the cross-attention layer in each transformer block of
the UNet, as depicted in Figure 3. In the cross-domain at-
tention layer, the keys and values from the normal and color
image domains are combined and processed through atten-
tion operations. This design ensures that the generations of
color images and normal maps are closely correlated, thus
promoting geometric consistency between the two domains.
4.3. Textured Mesh Extraction
To extract explicit 3D geometry from 2D normal maps and
color images, we optimize a neural implicit signed distance
field (SDF) to amalgamate all 2D generated data. Nonethe-
less, adopting existing SDF-based reconstruction methods,
such as NeuS [64], proves unviable. These methods weretailored for real-captured images and necessitate dense in-
put views. In contrast, our generated views are relatively
sparse, and the generated normal maps and color images
may exhibit subtle inaccurate predictions of some pixels.
Regrettably, these errors accumulate during the geometry
optimization, leading to distorted geometries, outliers, and
incompleteness. To overcome the challenges above, we pro-
pose a novel geometric-aware optimization scheme.
Optimization Objectives. With the obtained normal maps
G0:Nand color images H0:N, we first leverage segmenta-
tion models to segment the object masks M0:Nfrom the
normal maps or color images. Then we optimize SDF field
with such an objective function:
L=Lnormal +Lrgb+Lmask
+Reik+Rsparse +Rsmooth ,(6)
where Lnormal denotes the normal loss term that will be
discussed later, Lrgbdenotes a MSE loss term that calcu-
lates the errors between rendered colors ˆhkand generated
colors hk,Lmask denotes a binary cross-entropy loss term
that calculating errors between the rendered mask ˆmkand
the generated mask mk,Reikdenotes eikonal regularization
term that encourages the magnitude of the SDF gradients
to be unit length, Rsparse denotes a sparsity regularization
term that avoid floaters of SDF, and Rsmooth denotes a 3D
smoothness regularization term that enforces the SDF gra-
dients to be smooth in 3D space (see details in the supp.).
Geometry-aware Normal Loss. Thanks to the differen-
tiable nature of SDF representation, we can easily extract
normal values ˆgof the optimized SDF via calculating the
gradients of SDF. We maximize the similarity of the nor-
mal of SDF ˆgand our generated normal gto provide 3D
geometric supervision. To tolerate trivial inaccuracies of
the generated normals from different views, we introduce a
geometry-aware normal loss:
Lnormal =1PwkX
wk·(1−cos (ˆgk, gk)) (7)
where we measure the error between the normal of SDF
ˆgkand the generated normal gkfor the kthsampled ray,
cos(·,·)denotes cosine function, and wkis a geometric-
aware weight defined as
wk=(
0, cos (vk,gk)> ϵ
exp (|cos (vk,gk)|),cos (vk,gk)≤ϵ.(8)
Here exp(·)denotes exponential function, |·|denotes abso-
lute function, ϵis a negative threshold closing to zero, and
we measure the cosine value of the angle between the gen-
erated normal gkand the kthray’s viewing direction vk.
9974
Input images (a) Ours (b) SyncDreamer (c) Zero123Figure 4. The qualitative comparisons with baseline models on synthesized multi-view color images. We present the picked best results of
Zero123 [33] after multiple runs.
Figure 5. The qualitative comparisons with baseline methods on GSO [14] dataset in terms of the reconstructed textured meshes.
The design rationale behind this approach lies in the ori-
entation of normals, which are deliberately set to face out-
ward, while the viewing direction is inward-facing. This
configuration ensures that the angle between the normal
vector and the viewing ray remains not less than 90◦. A de-
viation from this criterion would imply inaccuracies in the
generated normals. Furthermore, rather than treating nor-
mals from different views equally, we introduce a weighting
mechanism. We assign higher weights to normals that form
larger angles with the viewing rays. This prioritization en-
hances the accuracy of our geometric supervision process.
Outlier Dropping. Instead of directly summing up the
color errors of all sampled rays at each iteration, we first sortthese errors in a descending order and then discard the top
largest errors according to a predefined percentage. This en-
ables the optimization process to circumvent the outlier re-
gions with poor quality or inconsistency, and instead lever-
age the regularization capability of MLP to learn the geom-
etry. This strategy effectively eliminates inaccurate isolated
surfaces and distorted textures.
5. Experiments
5.1. Implementation Details
We train our model on the LVIS subset of the Objaverse
dataset [10], which comprises approximately 30k+ objects
9975
(c) Sequential models
(normal model →color model)Input images (d) Sequential models
(color model →normal model)
(a) Cross -domain model
w/ cross -domain attention
(b) Cross -domain model
w/o cross -domain attention
Figure 6. Ablation studies on different cross-domain diffusion schemes.
following a cleanup process. To create the rendered multi-
view dataset, we first normalized each object to be centered
and of unit scale. Then we render normal maps and color
images from six views, including the front, back, left, right,
front-right, and front-left views, using Blenderproc [12].
We fine-tune our model starting from the Stable Diffu-
sion Image Variations Model, which has previously been
fine-tuned with image conditions. During fine-tuning, we
use a reduced image size of 256 ×256 and a total batch
size of 512 for training. The fine-tuning process involves
training the model for 30,000 steps. This entire training pro-
cedure typically requires approximately 3 days on a cluster
of 8 Nvidia Tesla A800 GPUs. To reconstruct 3D geometry
from the 2D representations, our normal fusion method is
built on the instant-NGP SDF reconstruction method [19].
5.2. Baselines
We adopt Zero123 [33], RealFusion [42], Magic123 [48],
One-2-3-45 [32], Point-E [45], Shap-E [26] and a recent
work SyncDreamer [36] as baseline methods. Given an
input image, Zero123 [33] is capable of generating novel
views of arbitrary viewpoints, and it can be incorporated
with SDS loss [47] for 3D reconstruction (we adopt the im-
plementation of ThreeStudio [20]). RealFusion [42] and
Magic123 [48] leverage Stable Diffusion [51] and SDS loss
for single-view reconstruction. One-2-3-45 [32] directly
predict SDFs via SparseNeuS [39] by taking the generated
multiple images of Zero123 [33]. Point-E [45] and Shap-
E [26] are 3D generative models trained on a large inter-
nal OpenAI 3D dataset, both of which are able to convert
a single-view image into a point cloud or an implicit repre-
sentation. SyncDreamer[36] generates multi-view consis-
tent images from a single image for deriving 3D geometry.
5.3. Evaluation Protocol
Evaluation Datasets. Following prior research [33, 36], we
adopt the Google Scanned Object dataset [14] for our eval-Method Chamfer Dist. ↓V olume IoU ↑
Realfusion [42] 0.0819 0.2741
Magic123 [48] 0.0516 0.4528
One-2-3-45 [32] 0.0629 0.4086
Point-E [45] 0.0426 0.2875
Shap-E [26] 0.0436 0.3584
Zero123 [33] 0.0339 0.5035
SyncDreamer [36] 0.0261 0.5421
Ours 0.0199 0.6244
Table 1. Quantitative comparison with baseline methods. We re-
port Chamfer Distance and V olume IoU on the GSO [14] dataset.
uation, which includes a wide variety of common every-
day objects. Our evaluation dataset matches that of Sync-
Dreamer [36], comprising 30 objects that span from every-
day items to animals. For each object in the evaluation set,
we render an image with a size of 256×256, which serves
as the input. Additionally, we include some images with di-
verse styles collected from the internet or text2image mod-
els in our evaluation.
Metrics. To evaluate the quality of the single-view re-
constructions, we adopt two metrics: Chamfer Distances
(CD) and V olume IoU between ground-truth shapes and
reconstructed shapes. Since different methods adopt vari-
ous canonical systems, we first align the generated shapes
to the ground-truth shapes before calculating the two met-
rics. Moreover, we adopt the metrics PSNR, SSIM [66] and
LPIPS [78] for evaluating the generated color images.
5.4. Single View Reconstruction
We evaluate the quality of the reconstructions of differ-
ent methods. The quantitative results are summarized in
Table 1, and the qualitative comparisons are presented in
Fig. 5. Shap-E [26] tends to produce incomplete and dis-
torted meshes. SyncDreamer [36] generates shapes that
are roughly aligned with the input image but lack detailed
geometries, and the texture quality is subpar. One-2-3-
45 [32] attempts to reconstruct meshes from the multiview-
9976
Input image Baseline Baseline + Outlier -dropping Baseline + Geo -aware normal Full model
Figure 7. Ablation study on the strategies in the mesh extraction module: geometry-aware normal loss and outlier-dropping strategy.
Method PSNR ↑SSIM↑LPIPS↓
Realfusion [42] 15.26 0.722 0.283
Zero123 [33] 18.93 0.779 0.166
SyncDreamer [36] 20.05 0.798 0.146
Ours 26.07 0.924 0.065
Table 2. The quantitative comparison in novel view synthesis. We
report PSNR, SSIM [66], LPIPS [78] on the GSO [14] dataset.
inconsistent outputs of Zero123 [33]. While it can cap-
ture coarse geometries, it loses important details in the pro-
cess. In comparison, our method stands out by achieving
the highest quality, both in terms of geometry and textures.
5.5. Novel View Synthesis
We evaluate the quality of novel view synthesis for dif-
ferent methods. The quantitative results are presented in
Table 2, and the qualitative results can be found in Fig-
ure 4. Zero123 [33] produces visually reasonable images,
but they lack multi-view consistency since it operates on
each view independently. Although SyncDreamer [33] in-
troduces a volume attention scheme to enhance the consis-
tency of multi-view images, their model is sensitive to the
elevation degrees of the input images and tends to produce
unreasonable results. In contrast, our method generates im-
ages that not only exhibit semantic consistency with the in-
put image but also maintain high consistency across multi-
ple views in terms of both colors and geometry.
5.6. Discussions
In this section, we perform a series of experiments to vali-
date the effectiveness of the designs in our method.
Cross-Domain Diffusion. To validate the effectiveness
of our proposed cross-domain diffusion scheme, we study
the following settings: (a) cross-domain model with cross-
domain attention; (b) cross-domain model without cross-
domain attention; (c) sequential models rgb-to-normal: first
train a multi-view color diffusion model then train a multi-
view normal diffusion model conditioned on the previously
generated color images; (d) sequential models normal-to-
rgb: first train a multi-view normal diffusion model then
train a multi-view color diffusion model conditioned on the
previously generated normal images.As shown in (a) and (b) of Figure 6, it’s evident that
the cross-domain attentions significantly enhance the con-
sistency between color images and normals, particularly in
terms of the detailed geometries of objects like the ice-
cream and the sculpture. From (c) and (d) of Figure 6,
while the normals and color images generated by sequen-
tial models maintain some consistency, their results suffer
from performance drops, like color shifts of (c) and wrong
geometries of (d).
Normal Fusion. To assess the efficacy of our normal fusion
algorithm, we conducted experiments using the complex
lion model, which is rich in geometric details, as illustrated
in Figure 7. The baseline model’s surfaces exhibited numer-
ous holes and noises. Utilizing either the geometry-aware
normal loss or the outlier-dropping loss helps mitigate the
noisy surfaces. Finally, combining both strategies yields the
best performance, resulting in clean surfaces while preserv-
ing detailed geometries.
6. Conclusions and Future Works
In this paper, we present Wonder3D , an innovative ap-
proach designed for efficiently generating high-fidelity tex-
tured meshes from single-view images. Experimental re-
sults demonstrate that our method upholds good efficiency
and robust generalization, and delivers high-quality geome-
try. However, limited by computational resources, the cur-
rent implementation of Wonder3D only produces normals
and color images from six views. This limited number of
views makes it challenging for our method to accurately re-
construct objects with very thin structures and severe occlu-
sions. To address this issue, Wonder3D may benefit from
leveraging more efficient multi-view attention mechanisms
to handle a greater number of views effectively.
Acknowledgements
This research is partially supported by the Innovation and
Technology Commission of the HKSAR Government un-
der the InnoHK initiative and Ref. T45-205/21-N of Hong
Kong RGC. Song-Hai Zhang is supported by the Na-
tional Key Research and Development Program of China
(No. 2023YFF0905104), the Natural Science Foundation
of China (No. 62132012), and Tsinghua-Tencent Joint Lab-
oratory for Internet Innovation Technology.
9977
References
[1] Titas Anciukevi ˇcius, Zexiang Xu, Matthew Fisher, Paul Hen-
derson, Hakan Bilen, Niloy J Mitra, and Paul Guerrero. Ren-
derdiffusion: Image diffusion for 3d reconstruction, inpaint-
ing and generation. In CVPR , 2023. 2, 3
[2] Mohammadreza Armandpour, Huangjie Zheng, Ali
Sadeghian, Amir Sadeghian, and Mingyuan Zhou. Re-
imagine the negative prompt algorithm: Transform 2d
diffusion into 3d, alleviate janus problem and beyond. arXiv
preprint arXiv:2304.04968 , 2023. 3
[3] Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W
Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini
De Mello, Tero Karras, and Gordon Wetzstein. Generative
novel view synthesis with 3d-aware diffusion models. In
ICCV , 2023. 3
[4] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen
Tu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf:
A unified approach to 3d generation and reconstruction. In
ICCV , 2023. 2, 3
[5] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.
Fantasia3d: Disentangling geometry and appearance for
high-quality text-to-3d content creation. arXiv preprint
arXiv:2303.13873 , 2023. 2, 3
[6] Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai,
Gang Yu, Lei Yang, and Guosheng Lin. It3d: Improved text-
to-3d generation with explicit view synthesis. arXiv preprint
arXiv:2308.11473 , 2023. 3
[7] Kai Cheng, Xiaoxiao Long, Kaizhi Yang, Yao Yao, Wei Yin,
Yuexin Ma, Wenping Wang, and Xuejin Chen. Gaussianpro:
3d gaussian splatting with progressive propagation. arXiv
preprint arXiv:2402.14650 , 2024. 2
[8] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexan-
der G Schwing, and Liang-Yan Gui. Sdfusion: Multimodal
3d shape completion, reconstruction, and generation. In
CVPR , 2023. 2, 3
[9] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu,
and Mubarak Shah. Diffusion models in vision: A survey.
T-PAMI , 2023. 3
[10] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,
Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana
Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:
A universe of annotated 3d objects. In CVPR , 2023. 6
[11] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan,
Yin Zhou, Leonidas Guibas, Dragomir Anguelov, et al.
Nerdi: Single-view nerf synthesis with language-guided dif-
fusion as general image priors. In CVPR , 2023. 3
[12] Maximilian Denninger, Dominik Winkelbauer, Martin Sun-
dermeyer, Wout Boerdijk, Markus Knauer, Klaus H. Strobl,
Matthias Humt, and Rudolph Triebel. Blenderproc2: A
procedural pipeline for photorealistic rendering. Journal of
Open Source Software , 8(82):4901, 2023. 7
[13] Zhiyang Dou, Qingxuan Wu, Cheng Lin, Zeyu Cao,
Qiangqiang Wu, Weilin Wan, Taku Komura, and Wenping
Wang. Tore: Token reduction for efficient human mesh re-
covery with transformer. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 15143–
15155, 2023. 2[14] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kin-
man, Ryan Hickman, Krista Reymann, Thomas B McHugh,
and Vincent Vanhoucke. Google scanned objects: A high-
quality dataset of 3d scanned household items. In ICRA ,
2022. 3, 6, 7, 8
[15] Ziya Erkoc ¸, Fangchang Ma, Qi Shan, Matthias Nießner,
and Angela Dai. Hyperdiffusion: Generating implicit
neural fields with weight-space diffusion. arXiv preprint
arXiv:2303.17015 , 2023. 2, 3
[16] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,
Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja
Fidler. Get3d: A generative model of high quality 3d tex-
tured shapes learned from images. NeurIPS , 2022. 2, 3
[17] Jiatao Gu, Qingzhe Gao, Shuangfei Zhai, Baoquan Chen,
Lingjie Liu, and Josh Susskind. Learning controllable 3d
diffusion models from single-view images. arXiv preprint
arXiv:2304.06700 , 2023. 2, 3
[18] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M Susskind,
Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi.
Nerfdiff: Single-image view synthesis with nerf-guided dis-
tillation from 3d-aware diffusion. In ICML , 2023. 3
[19] Yuan-Chen Guo. Instant neural surface reconstruction, 2022.
https://github.com/bennyguo/instant-nsr-pl. 7
[20] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian
Laforte, Vikram V oleti, Guan Luo, Chia-Hao Chen, Zi-
Xin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang.
threestudio: A unified framework for 3d content generation.
https://github.com/threestudio-project/
threestudio , 2023. 7
[21] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Bar-
las O ˘guz. 3dgen: Triplane latent diffusion for textured mesh
generation. arXiv preprint arXiv:2303.05371 , 2023. 2, 3
[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In NeurIPS , 2020. 3, 4
[23] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-
Jun Zha, and Lei Zhang. Dreamtime: An improved optimiza-
tion strategy for text-to-3d content creation. arXiv preprint
arXiv:2306.12422 , 2023. 3
[24] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter
Abbeel, and Ben Poole. Zero-shot text-guided object genera-
tion with dream fields. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
867–876, 2022. 2
[25] Yingwenqi Jiang, Jiadong Tu, Yuan Liu, Xifeng Gao, Xiaox-
iao Long, Wenping Wang, and Yuexin Ma. Gaussianshader:
3d gaussian splatting with shading functions for reflective
surfaces. arXiv preprint arXiv:2311.17977 , 2023. 2
[26] Heewoo Jun and Alex Nichol. Shap-e: Generat-
ing conditional 3d implicit functions. arXiv preprint
arXiv:2305.02463 , 2023. 2, 3, 7
[27] Animesh Karnewar, Niloy J Mitra, Andrea Vedaldi, and
David Novotny. Holofusion: Towards photo-realistic 3d gen-
erative modeling. In ICCV , 2023.
[28] Seung Wook Kim, Bradley Brown, Kangxue Yin, Karsten
Kreis, Katja Schwarz, Daiqing Li, Robin Rombach, Antonio
Torralba, and Sanja Fidler. Neuralfield-ldm: Scene gener-
ation with hierarchical latent diffusion models. In CVPR ,
2023. 2, 3
9978
[29] Kilian Kleeberger, Richard Bormann, Werner Kraus, and
Marco F Huber. A survey on learning-based robotic grasp-
ing. Current Robotics Reports , 1:239–249, 2020. 2
[30] Jiabao Lei, Jiapeng Tang, and Kui Jia. Generative scene syn-
thesis via incremental view inpainting using rgbd diffusion
models. In CVPR , 2022. 3
[31] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,
Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution
text-to-3d content creation. In CVPR , 2023. 2, 3
[32] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang
Xu, and Hao Su. One-2-3-45: Any single image to 3d mesh
in 45 seconds without per-shape optimization. arXiv preprint
arXiv:2306.16928 , 2023. 7
[33] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-
makov, Sergey Zakharov, and Carl V ondrick. Zero-1-to-3:
Zero-shot one image to 3d object. In ICCV , 2023. 2, 4, 6, 7,
8
[34] Xinhang Liu, Shiu-hong Kao, Jiaben Chen, Yu-Wing Tai,
and Chi-Keung Tang. Deceptive-nerf: Enhancing nerf recon-
struction using pseudo-observations from diffusion models.
arXiv preprint arXiv:2305.15171 , 2023. 3
[35] Yuan Liu, Sida Peng, Lingjie Liu, Qianqian Wang, Peng
Wang, Christian Theobalt, Xiaowei Zhou, and Wenping
Wang. Neural rays for occlusion-aware image-based render-
ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 7824–7833,
2022. 2
[36] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie
Liu, Taku Komura, and Wenping Wang. Syncdreamer: Gen-
erating multiview-consistent images from a single-view im-
age. arXiv preprint arXiv:2309.03453 , 2023. 2, 3, 4, 7, 8
[37] Zhen Liu, Yao Feng, Michael J Black, Derek
Nowrouzezahrai, Liam Paull, and Weiyang Liu. Meshdiffu-
sion: Score-based generative 3d mesh modeling. In ICLR ,
2023. 2, 3
[38] Xiaoxiao Long, Cheng Lin, Lingjie Liu, Wei Li, Christian
Theobalt, Ruigang Yang, and Wenping Wang. Adaptive sur-
face normal constraint for depth estimation. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion (ICCV) , pages 12849–12858, 2021. 2
[39] Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and
Wenping Wang. Sparseneus: Fast generalizable neural sur-
face reconstruction from sparse views. In European Confer-
ence on Computer Vision , pages 210–227. Springer, 2022.
7
[40] Xiaoxiao Long, Yuhang Zheng, Yupeng Zheng, Beiwen
Tian, Cheng Lin, Lingjie Liu, Hao Zhao, Guyue Zhou, and
Wenping Wang. Adaptive surface normal constraint for ge-
ometric estimation from monocular images. arXiv preprint
arXiv:2402.05869 , 2024. 2
[41] Shitong Luo and Wei Hu. Diffusion probabilistic models for
3d point cloud generation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 2837–2845, 2021. 2, 3
[42] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and
Andrea Vedaldi. Realfusion: 360deg reconstruction of any
object from a single image. In CVPR , 2023. 2, 3, 7, 8[43] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. In ECCV , 2020. 5
[44] Norman M ¨uller, Yawar Siddiqui, Lorenzo Porzi,
Samuel Rota Bulo, Peter Kontschieder, and Matthias
Nießner. Diffrf: Rendering-guided 3d radiance field
diffusion. In CVPR , 2023. 2, 3
[45] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela
Mishkin, and Mark Chen. Point-e: A system for generat-
ing 3d point clouds from complex prompts. arXiv preprint
arXiv:2212.08751 , 2022. 2, 3, 7
[46] Evangelos Ntavelis, Aliaksandr Siarohin, Kyle Olszewski,
Chaoyang Wang, Luc Van Gool, and Sergey Tulyakov.
Autodecoding latent 3d diffusion models. arXiv preprint
arXiv:2307.05445 , 2023. 2, 3
[47] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR ,
2023. 2, 3, 7
[48] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,
Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-
rokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123:
One image to high-quality 3d object generation using both
2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843 ,
2023. 2, 3, 7
[49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML , 2021. 2, 3, 4
[50] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,
Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aber-
man, Michael Rubinstein, Jonathan Barron, et al. Dream-
booth3d: Subject-driven text-to-3d generation. arXiv
preprint arXiv:2303.13508 , 2023. 3
[51] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , 2022. 3, 7
[52] Hoigi Seo, Hayeon Kim, Gwanghyun Kim, and Se Young
Chun. Ditto-nerf: Diffusion-based iterative text to omni-
directional 3d model. arXiv preprint arXiv:2304.02827 ,
2023. 3
[53] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon
Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee,
and Seungryong Kim. Let 2d diffusion model know 3d-
consistency for robust text-to-3d generation. arXiv preprint
arXiv:2303.07937 , 2023. 3
[54] Qiuhong Shen, Xingyi Yang, and Xinchao Wang. Anything-
3d: Towards single-view anything reconstruction in the wild.
arXiv preprint arXiv:2304.10261 , 2023. 3
[55] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,
and Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-
eration. arXiv preprint arXiv:2308.16512 , 2023. 2, 3, 4
[56] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In ICML , 2015. 3, 4
9979
[57] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea
Vedaldi. Viewset diffusion:(0-) image-conditioned 3d gener-
ative models from 2d data. arXiv preprint arXiv:2306.07881 ,
2023. 3
[58] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,
Lizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d
creation from a single image with diffusion prior. In ICCV ,
2023. 3
[59] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and
Yasutaka Furukawa. Mvdiffusion: Enabling holistic multi-
view image generation with correspondence-aware diffusion.
arXiv preprint arXiv:2307.01097 , 2023. 3
[60] Ayush Tewari, Tianwei Yin, George Cazenavette, Semon
Rezchikov, Joshua B Tenenbaum, Fr ´edo Durand, William T
Freeman, and Vincent Sitzmann. Diffusion with forward
models: Solving stochastic inverse problems without direct
supervision. arXiv preprint arXiv:2306.11719 , 2023. 3
[61] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,
Michael Niemeyer, and Federico Tombari. Textmesh: Gen-
eration of realistic 3d meshes from text prompts. arXiv
preprint arXiv:2304.12439 , 2023. 3
[62] Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, Jia-
Bin Huang, and Johannes Kopf. Consistent view synthesis
with pose-guided diffusion models. In CVPR , 2023. 3
[63] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,
and Greg Shakhnarovich. Score jacobian chaining: Lifting
pretrained 2d diffusion models for 3d generation. In CVPR ,
2023. 2, 3
[64] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. Neus: Learning neural implicit
surfaces by volume rendering for multi-view reconstruction.
InNeurIPS , 2021. 5
[65] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin
Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang
Wen, Qifeng Chen, et al. Rodin: A generative model for
sculpting 3d digital avatars using diffusion. In CVPR , 2023.
2, 3
[66] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P
Simoncelli. Image quality assessment: from error visibility
to structural similarity. TIP, 2004. 7, 8
[67] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan
Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and
diverse text-to-3d generation with variational score distilla-
tion. arXiv preprint arXiv:2305.16213 , 2023. 2, 3
[68] Daniel Watson, William Chan, Ricardo Martin-Brualla,
Jonathan Ho, Andrea Tagliasacchi, and Mohammad
Norouzi. Novel view synthesis with diffusion models. arXiv
preprint arXiv:2210.04628 , 2022. 3
[69] Jinbo Wu, Xiaobo Gao, Xing Liu, Zhengyang Shen, Chen
Zhao, Haocheng Feng, Jingtuo Liu, and Errui Ding. Hd-
fusion: Detailed text-to-3d generation leveraging multiple
noise estimation. arXiv preprint arXiv:2307.16183 , 2023.
3
[70] Jianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin
Tong. 3d-aware image generation using 2d diffusion mod-
els.arXiv preprint arXiv:2303.17905 , 2023. 3
[71] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,
and Zhangyang Wang. Neurallift-360: Lifting an in-the-wild2d photo to a 3d object with 360 views. arXiv e-prints , pages
arXiv–2211, 2022. 3
[72] Paul Yoo, Jiaxian Guo, Yutaka Matsuo, and Shixiang Shane
Gu. Dreamsparse: Escaping from plato’s cave with 2d frozen
diffusion model given sparse views. CoRR , 2023. 3
[73] Chaohui Yu, Qiang Zhou, Jingliang Li, Zhe Zhang, Zhibin
Wang, and Fan Wang. Points-to-3d: Bridging the gap be-
tween sparse points and shape-controllable text-to-3d gener-
ation. arXiv preprint arXiv:2307.13908 , 2023. 3
[74] Jason J. Yu, Fereshteh Forghani, Konstantinos G. Derpanis,
and Marcus A. Brubaker. Long-term photometric consistent
novel view synthesis with diffusion models. In ICCV , 2023.
3
[75] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic,
Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent
point diffusion models for 3d shape generation. In NeurIPS ,
2022. 2, 3
[76] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter
Wonka. 3dshape2vecset: A 3d shape representation for neu-
ral fields and generative diffusion models. In SIGGRAPH ,
2023. 2, 3
[77] Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing
Liao. Text2nerf: Text-driven 3d scene generation with neural
radiance fields. arXiv preprint arXiv:2305.11588 , 2023. 3
[78] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR , 2018. 7, 8
[79] Yuhang Zheng, Xiangyu Chen, Yupeng Zheng, Songen Gu,
Runyi Yang, Bu Jin, Pengfei Li, Chengliang Zhong, Zeng-
mao Wang, Lina Liu, et al. Gaussiangrasper: 3d lan-
guage gaussian splatting for open-vocabulary robotic grasp-
ing. arXiv preprint arXiv:2403.09637 , 2024. 2
[80] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation
and completion through point-voxel diffusion. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 5826–5835, 2021. 2, 3
[81] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Dis-
tilling view-conditioned diffusion for 3d reconstruction. In
CVPR , 2023. 3
[82] Joseph Zhu and Peiye Zhuang. Hifa: High-fidelity text-
to-3d with advanced diffusion guidance. arXiv preprint
arXiv:2305.18766 , 2023. 3
9980
