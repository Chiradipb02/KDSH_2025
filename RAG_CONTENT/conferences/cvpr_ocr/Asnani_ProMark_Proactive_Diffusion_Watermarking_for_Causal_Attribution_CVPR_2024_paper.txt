ProMark: Proactive Diffusion Watermarking for Causal Attribution
Vishal Asnani1,2John Collomosse1,3Tu Bui3Xiaoming Liu2Shruti Agarwal1
1Adobe Research,2Michigan State University,3University of Surrey
{asnanivi,liuxm}@msu.edu {collomos,shragarw}@adobe.com t.v.bui@surrey.ac.uk
Abstract
Generative AI (GenAI) is transforming creative work-
flows through the capability to synthesize and manipulate
images via high-level prompts. Yet creatives are not well
supported to receive recognition or reward for the use of
their content in GenAI training. To this end, we propose
ProMark, a causal attribution technique to attribute a syn-
thetically generated image to its training data concepts like
objects, motifs, templates, artists, or styles. The concept
information is proactively embedded into the input train-
ing images using imperceptible watermarks, and the diffu-
sion models (unconditional or conditional) are trained to
retain the corresponding watermarks in generated images.
We show that we can embed as many as 216unique water-
marks into the training data, and each training image can
contain more than one watermark. ProMark can maintain
image quality whilst outperforming correlation-based attri-
bution. Finally, several qualitative examples are presented,
providing the confidence that the presence of the watermark
conveys a causative relationship between training data and
synthetic images.
1. Introduction
GenAI is able to create high-fidelity synthetic images span-
ning diverse concepts, largely due to advances in diffusion
models, e.g. DDPM [18], DDIM [23], LDM [28]. GenAI
models, particularly diffusion models, have been shown to
closely adopt and sometimes directly memorize the style
and the content of different training images – defined as
“concepts” in the training data [11, 21]. This leads to con-
cerns from creatives whose work has been used to train
GenAI. Concerns focus upon the lack of a means for attribu-
tion, e.g. recognition or citation, of synthetic images to the
training data used to create them and extend even to calls for
a compensation mechanism (financial, reputational, or oth-
erwise) for GenAI’s derivative use of concepts in training
images contributed by creatives.
We refer to this problem as concept attribution – the abil-
ity to attribute generated images to the training concept/s
which have most directly influenced their creation. Several
Figure 1. Causative vs. correlation-based matching for concept
attribution. ProMark identifies the training data most responsible
for a synthetic image (‘attribution’). Correlation-based matching
doesn’t always perform the data attribution properly. We propose
ProMark, which is a proactive approach involving adding water-
marks to training data and recovering them from the synthetic im-
age to perform attribution in a causative way.
passive techniques have recently been proposed to solve the
attribution problem [5, 30, 34]. These approaches use vi-
sual correlation between the generated image and the train-
ing images for attribution. Whilst they vary in their method
and rationale for learning the similarity embedding – all use
some forms of contrastive training to learn a metric space
for visual correlation.
We argue that although correlation can provide visually
intuitive results, a measure of similarity is not a causative
answer to whether certain training data is responsible for the
generation of an image or not. Further, correlation-based
techniques can identify close matches with images that were
not even present in the training data.
Keeping this in mind, we explore an intriguing field
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10802
of research which is developing around proactive water-
marking methodologies [3, 29, 33, 37], that employ sig-
nals, termed templates to encrypt input images before feed-
ing them into the network. These works have integrated
and subsequently retrieved templates to bolster the perfor-
mance of the problem at hand. Inspired by these works,
we introduce ProMark, a proactive watermarking-based ap-
proach for GenAI models to perform concept attribution in
a causative way. The technical contributions of ProMark are
three-fold:
1. Causal vs. Correlation-based Attribution. ProMark
performs causal attribution of synthetic images to the pre-
defined concepts in the training images that influenced the
generation. Unlike prior works that visually correlate syn-
thetic images with training data, we make no assumption
that visual similarity approximates causation. ProMark ties
watermarks to training images and scans for the watermarks
in the generated images, enabling us to demonstrate rather
than approximate/imply causation. This provides confi-
dence in grounding downstream decisions such as legal at-
tribution or payments to creators.
2. Multiple Orthogonal Attributions. We propose to use
orthogonal invisible watermarks to proactively embed at-
tribution information into the input training data and add
a BCE loss during the training of diffusion models to re-
tain the corresponding watermarks in the generated images.
We show that ProMark causatively attributes as many as
216unique training-data concepts like objects, scenes, tem-
plates, motifs, and style, where the generated images can
simultaneously express one or two orthogonal concepts.
3. Flexible Attributions. ProMark can be used for train-
ing conditional or unconditional diffusion models and even
finetuning a pre-trained model for only a few iterations. We
show that ProMark’s causative approach achieves higher
accuracy than correlation-based attribution over five di-
verse datasets (Sec. 4.1): Adobe Stock, ImageNet, LSUN,
Wikiart, and BAM while preserving synthetic image quality
due to the imperceptibility of the watermarks.
Fig. 1presents our scenario, where synthetic image(s)
are attributed back to the most influential GenAI training
images. Correlation-based techniques [5, 34] try to match
the high-level image structure or style. Here, the green-
lizard synthetic image is matched to a generic green image
without a lizard [5]. With ProMark’s causative approach,
the presence of the green-lizard watermark in the synthetic
image will correctly indicate the influence of the similarly
watermarked concept group of lizard training images.
2. Related Works
Passive Concept Attribution. Concept attribution differs
from model [8] or camera [12] attribution in that the task is
to determine the responsible training data for a given gen-
eration. Existing concept attribution techniques are passive
– they do not actively modify the GenAI model or training
data but instead, measure the visual similarity (under someTable 1. Comparison of ProMark with prior works. Uniquely,
we perform causative attribution using proactive watermarking to
attribute multiple concepts. [Keys: emb.: embedding, obj.: object,
own.: ownership, sem.: semantic, sty.: style, wat.: watermark]
Method Scheme T
ask Match #
Class Multiple Attrib
ution
type type attrib
ution type
[
30] passi
ve attrib
ution emb
. - ✕ sty
.
[
5] passi
ve attrib
ution emb
. - ✕ obj.
[
34] passi
ve attrib
ution emb
.693 ✕ sty
., obj.
[
17] passi
ve detect w
at. 2 ✕ -
[
22] passi
ve detect w
at. 2 ✕ -
[
14] passi
ve detect w
at. 2 ✕ -
[
33] proacti
ve detect w
at. 2 - -
[
1] proacti
ve detect w
at. 2 - -
[
3] proacti
velocalization w
at. 2 - -
[
2] proacti
veobj.
detect - 90 - -
sty
., obj.ProMark proacti
veattrib
ution w
at. 216✓o
wn., sem.
definition) of synthetic images and training data to quan-
tify attribution for each training image. EKILA [5] pro-
poses patch-based perceptual hashing (visual fingerprinting
[6, 24]) to match query patches to the training data for attri-
bution. Wang et al. [34] propose Attribution by Customiza-
tion (AbC), calibrating embeddings like CLIP, DINO, etc.
for the attribution task using images generated from “cus-
tomized” diffusion models in the training loop. Both [5] and
[34] also explored ALADIN [30] for style attribution; a fea-
ture for fine-grained style similarity. All these approaches
approximate causation by visual correlation within a con-
trastively trained embedding. They are passive approaches
that take the image as an attribute by correlating between
generated and training images. Instead, our approach is a
proactive scheme that adds a watermark to training images
and performs attribution in a causal manner (Tab. 1).
Leave-One-Out Training. Early works retrained models
holding out training data to determine its influence [16, 20].
Whilst causal, the lengthy training required for GenAI mod-
els is not practical for our image attribution task.
Proactive Schemes. Proactive schemes involve adding
a signal/perturbation onto the input images to benefit dif-
ferent tasks like deepfake tagging [33], deepfake detec-
tion [1], manipulation localization [3], object detection [2],
etc. Some works [29, 37] disrupt the output of the gener-
ative models by adding perturbations to the training data.
Alexandre et al. [31] tackles the problem of training dataset
attribution by using fixed signals for every data type. These
prior works successfully demonstrate the use of watermarks
to classify the content of the AI-generated images proac-
tively. We extend the idea of proactive watermarking to per-
form the task of causal attribution of AI-generated images
to influential training data concepts. Watermarking has not
been used to trace attribution in GenAI before.
Watermarking of GenAI Models. It is an active research
to watermark AI-generated images for the purpose of pri-
vacy protection. Fernandez et al. [17] fine-tune the LDM’s
10803
Figure 2. Overview of ProMark . We show the training and inference procedure for our proposed method. Our training pipeline involves
two stages, image encryption and generative model training. We convert the bit-sequences to spatial watermarks ( W), which are then
added to the corresponding concept images ( X) to make them encrypted ( XW). The generative model is then trained with the encrypted
images using the LDM supervision. During training, we recover the added watermark using the secret decoder ( DS) and apply the BCE
supervision to perform attribution. To sample newly generated images, we use a Gaussian noise and recover the bit-sequences using the
secret decoder to attribute them to different concepts. Best viewed in color.
decoder to condition on a bit sequence, embedding it in
images for AI-generated image detection. Kirchenbauer et
al. [19] propose a watermarking method for language mod-
els by pre-selecting random tokens and subtly influencing
their use during word generation. Zhao et al . [39] use
a watermarking scheme for text-to-image diffusion mod-
els, while Liu et al. [22] verify watermarks by pre-defined
prompts. [14, 25] add a watermark for detecting copyright
infringement. Asnani et al. [4] reverse engineer a finger-
print left by various GenAI models to further use it for re-
covering the network and training parameters of these mod-
els [4, 36]. Finally, Cao et al. [10] adds an invisible wa-
termark for protecting diffusion models which are used to
generate audio modality. Most of these works have used wa-
termarking for protecting diffusion models, which enables
them to add just one watermark onto the data. In contrast,
we propose to add multiple watermarks to the training data
and to a single image, which is a more challenging task than
embedding a universal watermark.
3. Method
3.1. Background
Diffusion Models. Diffusion models learn a data distribu-
tionp(X),where X∈Rh×w×3is the input image. They
do this by iteratively reducing the noise in a variable that
initially follows a normal distribution. This can be viewed
as learning the reverse steps of a fixed Markov Chain with
a length of T. Recently, LDM [28] is proposed to convert
images to their latent representation for faster training in a
lower dimensional space than the pixel space. The imageis converted to and from the latent space by a pretrained
autoencoder consisting of an encoder z=EL(X)and a de-
coder XR=DL(z), where zis the latent code and XR
is the reconstructed image. The trainable denoising module
of the LDM is ϵθ(zt, t);t= 1...T, where ϵθis trained to
predict the denoised latent code ˆzfrom its noised version
zt. This objective function can be defined as follows:
LLDM =EEL(X),ϵ∼N (0,1),t
||ϵ−ϵθ(zt, t)||2
2
, (1)
where ϵis the noise added at step t.
Image Encryption. Proactive works [1–3] have shown
performance gain on various tasks by proactively transform-
ing the input training images Xwith a watermark, result-
ing in an encrypted image. This watermark is either fixed
or learned, depending on the task at hand. Similar to prior
proactive works, our image encryption is of the form:
XW=T(X;W) =X+m×R(W, h, w ),(2)
where Tis the transformation, Wis the spatial watermark,
XWis the encrypted image, mis the watermark strength,
andR(.)resizes Wto the input resolution (h, w).
We use the state-of-the-art watermarking technique RoS-
teALS [9] to compute the spatial watermarks for encryption
due to its robustness to image transformation and general-
ization (the watermark is independent of content of the input
image). RoSteALS is designed to embed a secret of length
b-bits into an image using robust and imperceptible water-
marking. It comprises of a secret encoder ES(s), which
converts the bit-secret s∈ {0,1}binto a latent code offset
zo. It is then added to the latent code of an autoencoder
zw=z+zo. This modified latent code zwis then used to
10804
reconstruct a watermarked image via autoencoder decoder.
Finally, a secret decoder, denoted by DS(XW), takes the
watermarked images as input and predicts bit-sequence ˆs.
3.2. Problem Definition
LetC={c1, c2, . . . , c N}be a set of Ndistinct concepts
within a dataset that is used for training a GenAI model for
image synthesis. The problem of concept attribution can be
formulated as follows:
Given a synthetic image XSgenerated by a GenAI
model, the objective of concept attribution is to accurately
associate XSto a concept ci∈ C that significantly influ-
enced the generation of XS.
We aim to find a mapping f:XS→cisuch that
c∗
i= arg max
ci∈Cf(XS, ci), (3)
where c∗
irepresents the concept most strongly attributed to
image XS.
3.3. Overview
The pipeline of ProMark is shown in Fig. 2. The principle is
simple: if a specific watermark unique to a training concept
can be detected from a generated image, it indicates that the
generative model relies on that concept in the generation
process. Thus, ProMark involves two steps: training data
encryption via watermarks and generative model training
with watermarked images.
To watermark the training data, the dataset is first di-
vided into Ngroups, where each group corresponds to a
unique concept that needs attribution. These concepts can
be semantic ( e.g. objects, scenes, motifs or stock image
templates) or abstract ( e.g. stylistic, ownership info). Each
training image in a group is encoded with a unique water-
mark without significantly altering the image’s perceptibil-
ity. Once the training images are watermarked, they are
used to train the generative model. As the model trains,
it learns to generate images based on the encrypted training
images. Ideally, the generated images would have traces of
watermarks corresponding to concepts they’re derived from.
During inference, ProMark conforms to whether a gen-
erated image is derived from a particular training concept
by identifying the unique watermark of that concept within
the image. Through the careful use of unique watermarks,
we can trace back and causally attribute generated images
to their origin in the training dataset.
3.4. Training
During training, our algorithm is composed of two stages:
image encryption and generative model training. We now
describe each of these stages in detail.
Image Encryption. The training data is first divided
intoNconcepts, and images in each partition are en-
crypted using a fixed spatial watermark Wj∈Rh×w(j∈0,1,2, ..., N ). Each noise Wjis ab-dim bit-sequence
(secret) sj={pj1, pj2, ..., p jb}where pji∈ {0,1}.
In order to compute the watermark Wjfrom the bit-
sequence sj, we encrypt 100random images with sjus-
ing pretrained RoSteALS secret encoder ES(.)which takes
b= 160 length secret as input. From these encrypted im-
ages, we obtain 100noise residuals by subtracting the en-
crypted images from the originals, which are averaged to
compute the watermark Wjas:
Wj=1
100100X
i=1(Xi− ES(Xi,sj)). (4)
The above process is defined as spatial noise conversion
in Fig. 2. The averaging of noise residuals across differ-
ent images reduces the image content in the watermark and
makes the watermark independent of any specific image.
Additionally, the generated watermarks are orthogonal due
to different bits for all sj, ensuring distinguishability from
each other. With the generated watermarks, each training
image is encrypted using Eq. (2) with one of the Nwater-
marks that correspond to the concept the image belongs to.
Generative Model Training. Using the encrypted data,
we train the LDM’s denoising module ϵθ(.)using the ob-
jective function (Eq. (1)), where ztis the noised version of:
z=EL(XWj) =EL(T(X;Wj)), (5)
i.e., the input latent codes zare generated using the en-
crypted images XWjforj∈ {0,1,2...., N}.
However, we found that only using LDM loss is insuffi-
cient to successfully learn the connection between the con-
ceptual content and its associated watermark. This gap in
learning presents a significant hurdle, as the primary aim
is to trace back generated images to their respective train-
ing concepts via the watermark. To tackle this, an auxiliary
supervision is introduced to LDM’s training,
LBCE(sj,ˆs) =−1
bbX
i=1[pjilog(ˆpi)+(1−pji) log(1 −ˆpi)],
(6)
where LBCE(.)is the binary cross-entropy (BCE) between
the actual bit-sequence sjassociated with watermark Wj
and the predicted bit-sequence ˆs. The denoised latent code
ˆzis then decoded using the autoencoder DL(.), and the em-
bedded secret ˆsis predicted by the secret decoder DS(.)as:
ˆs=DS(DL(ˆz)). (7)
By employing BCE, the model is guided to minimize the
difference between the predicted watermark and the embed-
ded watermark, hence improving the model’s ability to as-
sociate watermarks with their respective concepts. Finally,
our objective is to minimize the loss Lattr=LLDM +
αLBCE during training, where α= 2for our experiments.
10805
3.5. Inference
After the LDM learns to associate the watermarks with con-
cepts, we use random Gaussian noise to sample the newly
generated images from the model. While the diffusion
model creates these new images, it also embeds a water-
mark within them. Each watermark maps to a distinctive
orthogonal bit-sequence associated with a specific training
concept, serving as a covert signature for attribution.
To attribute the generated images and ascertain which
training concept influenced them, we predict the secret em-
bedded by the LDM in the generated images (see Eq. (7)).
Given a predicted binary bit-sequence, ˆs={ˆp1,ˆp2, ...,ˆpb}
and all the input bit-sequences sjforj∈0,1,2..., N , we
define the attribution function, f, in Eq. (3) as:
f(ˆs,sj) =bX
k=1[ˆpk=pjk], (8)
where [ˆpk=pik]acts as an indicator function, returning 1
if the condition is true, i.e., the bits are identical, and 0oth-
erwise. Consequently, we assign the predicted bit sequence
to the concept whose bit sequence it most closely mirrors
— that is, the concept j∗for which f(ˆs,sj∗)is maximized:
j∗= arg max
j∈{1,2,...,N}f(ˆs,sj). (9)
In other words, the concept whose watermark is most
closely aligned with the generated image’s watermark is
deemed to be the influencing source behind the generated
image.
3.6. Multiple Watermarks
In prior image attribution works, an image is usually at-
tributed to a single concept ( e.g. image content or image
style). However, in real-world scenarios, an image may en-
capsulate multiple concepts. This observation brings forth
a pertinent question: “Is it possible to use multiple water-
marks for multi-concept attribution within a single image?”
In this paper, we propose a novel approach to per-
form multi-concept attribution by embedding multiple wa-
termarks into a single image. In our preliminary experi-
ments, we restrict our focus to the addition of two water-
marks. To achieve this, we divide the image into two halves
and resize each watermark to fit the respective halves. This
ensures that each half of the image carries distinct water-
mark information pertaining to a specific concept.
For the input RGB image X,{Wi,Wj}are the water-
marks for two secrets {si,sj}, we formulate the new trans-
formation Tas:
T(X;Wi,Wj) =n
Xleft,Xrighto
=n
(X(:,0 :w
2,:) +R(Wi, h,w
2)),
(X(:,w
2:w,:) +R(Wj, h,w
2)o
,Table 2. Comparison with prior works for unconditional diffusion
model on various datasets. [Keys: str.: strength]
Str. Attribution Accuracy (%) ↑Method(%) Stock LSUN Wiki-A Wiki-S ImageNet
ALADIN [30] - 99.86 46.27 48.95 33.25 9.25
CLIP [27] - 75.67 87.13 77.58 60.84 60.12
AbC-CLIP [34] - 78.49 87.39 77.23 60.43 62.83
SSCD [26] - 99.63 73.26 69.51 50.37 37.32
EKILA [5] - 99.37 70.60 51.23 37.06 38.00
30 100 95.12 97.45 98.12 83.06ProMark100 100 100 100 100 91 .07
where {.}is the horizontal concatenation. The loss func-
tion uses the two predicted secrets ( ˆs1andˆs2) from the two
halves of the generated image, defined as:
Lattr=LLDM +α(LBCE(si,ˆs1) +LBCE(sj,ˆs2)).
4. Experiments
4.1. Unconditional Diffusion Model
In this section, we train multiple versions of unconditional
diffusion models [28] to demonstrate that ProMark can be
used to attribute a variety of concepts in the training data.
In each case, the model is trained starting from random ini-
tialization of LDM weights. Described next are details of
the datasets and evaluation protocols.
Datasets We use 5datasets spanning attribution categories
like image templates, scenes, objects, styles, and artists. For
each dataset, we consider the dataset classes as our attribu-
tion categories. For each class in a dataset, we use 90% im-
ages for training, and 10% for evaluation, unless specified
otherwise.
1. Stock: We collect images from Adobe Stock, compris-
ing of near-duplicate image clusters like templates, sym-
bols, icons, etc. An example image from some clusters is
shown in the supplementary. We use 100such clusters,
each with 2Kimages.
2. LSUN: The LSUN dataset [38] comprises 10scene cat-
egories, such as bedrooms and kitchens. It’s commonly
used for scene classification, training generative models
like GANs, and anomaly detection. Same as the Stock
dataset, we use 2Kimages per class.
3. Wiki-S: The WikiArt dataset [32] is a collection of fine
art images spanning various styles and artists. We use
the28style classes with 580average images per class.
4. Wiki-A: From the WikiArt dataset [32] we also use the
23artist classes with 2,112average images per class.
5. ImageNet: We use the ImageNet dataset [15] which
comprises of 1million images across 1Kclasses. For
this dataset, we use the standard validation set with 50K
for evaluation and the remaining images for training.
Evaluation Protocol For all datasets, the concept attribu-
tion performance is tested on the held-out data as follows.
For a held-out image, we first encrypt it with the concept’s
watermark. Then using the latent code of the encrypted im-
age, we noise it till a randomly assigned timestamp and ap-
10806
Figure 3. Example training and newly sampled images of dif-
ferent datasets for the corresponding classes. We observe a sim-
ilar content in the inference image compared with the training im-
age of the predicted class.
ply our trained diffusion model to reverse back to the ini-
tial timestamp with the estimated noise. The denoised la-
tent code is then decoded using the autoencoder DL(.), and
the embedded secret is predicted using the secret decoder
DS(.). Using Eq. (9), we compute the predicted concept
and calculate the accuracy using the ground-truth concept.
Results Shown in Tab. 2is the attribution accuracy of
ProMark at two watermark strengths i.e.100% and30%
which is set by variable min Eq. (2). ProMark outper-
forms prior works, achieving near-perfect accuracy on all
the datasets when the watermark strength is 100%. How-
ever, the watermark introduces visual artifacts [9] if the
watermark strength is full. Therefore, we decrease the
watermark strength to 30% before adding it to the train-
ing data (see Sec. 4.5for ablation on watermark strength).
Even though our performance drops at a lower watermark
strength, we still outperform the prior works. This shows
that our causal approach can be used to attribute a variety of
concepts in the training data with an accuracy higher than
the prior passive approaches.
Fig. 3(rows 1-5) shows the qualitative examples of the
newly sampled images from each of the trained models. For
each model, we sample the images using random Gaussian
noise until we have images for every concept. The con-
cept for each image is predicted using the secret embedded
in the generated images. Shown in each row of Fig. 3are
three training images (columns 1-3) and three sampled im-
ages from the corresponding concepts (columns 4-6). This
shows that ProMark makes the diffusion model embed the
corresponding watermark for the class of the generated im-
Figure 4. Visual results of prior embedding-based works. We
show the image of the closest matched embedding for each method
on ImageNet. We highlight images green for correct attribution,
otherwise red. Embedding-based works do not always attribute to
the correct concept.
age, thereby demonstrating the usefulness of our approach.
Shown in Fig. 4 are the nearest images retrieved using
the embedding-based methods (row (2)-(6)) for the query
images from the ImageNet(row (1)). For each image re-
trieval, we highlight the correct/incorrect attribution using a
green/red box. As we can see, the correlation-based prior
techniques rely on visual similarity between the query and
the retrieved images, ignoring the concept. However, for
each query image, ProMark predicts the correct concept
corresponding to the query image (Fig. 3).
4.2. Multiple Watermarks
We evaluate the effectiveness of ProMark for multi-concept
attribution. As before, an unconditional diffusion model is
trained starting from random initialization, and each image
in the training data is encrypted with two watermarks as
outlined in Sec. 3.6.
Dataset For this experiment, we use the BAM dataset [35],
comprising contemporary artwork sourced from Behance,
a platform hosting millions of portfolios by professionals
and artists. This dataset uniquely categorizes each image
into two label types: media and content. It encompasses 7
distinct labels for media and 9for content, culminating in a
diverse set of 63label pairs, with 4,593average images in
these label pairs. For each class pair, we use 90% data for
10807
Table 3. Multi-concept attribution comparison with baselines.
Strength Attrib
ution Accuracy (%) ↑Method(%) Media Content Combined
ALADIN
[30] - 42.16 41.25 34.97
CLIP
[27] - 46.71 45.12 42.36
AbC-CLIP
[34] - 52.12 51.56 46.23
SSCD
[26] - 47.06 46.09 40.61
EKILA
[5] - 43.72 43.58 37.09
ProMark
(single) 30 - - 97.73
30 91.33 89.21 84.66ProMark
(multi)100 95.61 93.31 90.12
training and 10% for held-out evaluation.
Results The same evaluation is performed as described
in Sec. 4.1, except the accuracy is now computed for two
concepts instead of one. Shown in Tab. 3is the attribu-
tion accuracy for the two concepts individually and simul-
taneously. To benchmark the effectiveness of ProMark,
we also compare against baselines, where ProMark outper-
forms baselines, achieving a combined attribution accuracy
of90.12% as compared to 46.61% for AbC-CLIP [34] (At-
tribution by Customization, fine-tuning of CLIP). We be-
lieve our findings substantiate that ProMark can be extended
to a scenario where the generated images are composed
of several unique concepts from the training images. For
ablation, we train ProMark with 7×8classes, with each
pair of media and content as an individual concept. Pro-
Mark is able to achieve 97.73% attribution accuracy for
single-concept, higher than the performance achieved for
multi-concept case i.e.90.12%. However, single concept
approach is not scalable when the number of concepts in an
image increases, as the number of watermarks would grow
exponentially (7 ×8vs.7 + 8). Therefore, transitioning to
a multi-concept scenario is more appropriate for real-world
scenarios, where scalability and practicality are crucial.
In the final row of Fig. 3, we present qualitative exam-
ples of newly sampled images from the model trained on the
BAM dataset. Observations indicate that these sampled im-
ages successfully adopt both media and content correspond-
ing to training images of the same concept. This provides
empirical evidence of ProMark’s effectiveness in facilitat-
ing multi-concept attribution.
4.3. Number of Concepts
AI models leverage large-scale image datasets [7, 18,23,
28], encompassing a broad spectrum of concepts. This
diversity necessitates concept attribution methods that can
maintain high performance across numerous concepts. In
this context, we test ProMark with an exponentially in-
creasing number of concepts. Our dataset comprises Adobe
Stock images with near duplicate image templates (used
as concepts). As we escalate the number of concepts, we
concurrently reduce the per-concept image count, only 24
images per concept for 216concepts, see the red curve
of Fig. 5(a) for image count. This is done to obtain bal-Table 4. Comparison with different baselines for the conditional
model trained on ImageNet dataset.
Strength Attrib
ution Accuracy (%) ↑Method(%) Held-out
data Ne
w images
ALADIN
[30] - 9.25 0.18
CLIP
[27] - 60.12 41.01
AbC-CLIP
[34] - 62.83 50.19
SSCD
[26] - 37.32 30.10
EKILA
[5] - 38.00 29.06
30 91.24 87.30ProMark100 95.60 90.13
anced
image distribution and also to challenge ProMark’s
robustness.
The outcomes, depicted in Fig. 5(a) red curve, indi-
cate an anticipated decline in ProMark’s efficacy in line
with the increase in the number of concepts, reducing from
100% attribution accuracy for 10concepts (chance accu-
racy10%) to 82% for216concepts (chance accuracy 1.5e-
3%). This reduction in attribution accuracy is correlated
with the reduction in bit-secret accuracy (green curve) for
every predicted secret, indicating poor watermark recov-
ery due to the increased confusion between the watermarks.
Notwithstanding the increased difficulty, ProMark demon-
strates commendable performance, underscoring its poten-
tial in real-world applications.
4.4. Conditional Diffusion Model
As the diffusion models are usually trained with conditions
to guide generation, we also evaluate using the conditional
LDM model [28]. For this, we fine-tune a model pretrained
of the ImageNet dataset (see Sec. 4.1), where the 1000 Im-
ageNet classes are used as model conditions and also as the
1000 concepts.
Evaluation Protocol In addition to the evaluation on the
held-out data (see Sec. 4.1), we also perform the quantita-
tive evaluation on the newly sampled images as follows. We
use the labels of the ImageNet dataset as conditions to sam-
ple10K images (10 images per label). Using these labels
as the ground-truth concept for a newly sampled image, we
compute the accuracy of the concept predicted by the em-
bedded watermark in the generated images.
Results The accuracies for held-out and newly sampled im-
ages are shown in Tab. 4. The performance on the held-out
dataset for the conditional model improves compared to the
unconditional models as the label conditions provide im-
proved supervision for correct watermarks. ProMark also
outperforms prior embedding-based works by a large mar-
gin on both held-out and newly sampled images. The at-
tribution accuracy on the new images, however, is less than
the held-out data. We hypothesize that it is because newly
sampled images may contain more than one concept and
can be more confusing to attribute. The high accuracy, even
for newly sampled images, suggests that ProMark exhibits
higher generalizability to unseen synthetic images.
10808
Figure 5. Ablation experiments: We show the results for ablating multiple parameters of ProMark. (a) Number of concepts, (b) watermark
strength, and (c) number of images per concept.
4.5. Ablation Study
For the ablation experiment, we use Stock dataset with a
varying number of concepts, and we train unconditional
LDM models from random initialization.
Strength of Watermark. The hyperparameter min Eq. (2)
modulates the intensity of the watermark applied to the
training images, ensuring encrypted images retain high
quality. We systematically alter mto examine its impact on
the LDM’s performance and the Peak Signal-to-Noise Ratio
(PSNR) of the output images with reference to the held-out
encrypted images. Fig. 5(b) shows that attribution accuracy
improves with increased m, plateauing beyond a threshold
of0.5. The discernible compromise in image quality, as ev-
idenced by the inverse relationship between intensity and
PSNR, can be attributed to the use of fixed watermarks ob-
tained using RoSteALS [9], which is originally optimized
for robustness. In light of this, we select an optimal wa-
termark strength of 0.3, which balances between perfor-
mance and PSNR. We measured the FID between original
and newly sampled images from a pretrained ImageNet con-
ditional model (trained without watermark) and ProMark
model (trained with watermark), which is 13.28 and17.63
respectively. This small increment shows negligible quality
loss in the generated images due to ProMark.
Number of Images Per Concept. To ascertain the op-
timal number of images required per concept for effective
watermark learning, we ablate by fixing the number of con-
cepts to 500 and varying the number of images used to
train the LDM. Fig. 5(c) reveals that performance drops by
2.5% when image count per concept is reduced from 700to
10. Remarkably, the general efficacy of ProMark remains
consistently high, suggesting a low sensitivity to the image
count per concept. These results demonstrate that ProMark
can successfully learn watermarks with as few as 10images
per concept, highlighting its efficiency and potential for ap-
plications with limited data availability.
Framework Design. ProMark employs BCE loss to in-
struct the LDM model in the accurate embedding of bit-
sequence watermarks within generated images. The attribu-
tion performance degrades to 2%when BCE loss is not used
as compared to 100% in Tab. 2. This shows that removing
BCE loss significantly impairs the LDM’s performance, un-
derscoring the necessity of this supervision in helping LDMembed watermarks effectively.
Also, ProMark incorporates a secret decoder to retrieve
secret bit-sequence from synthesized images, rendering the
process contingent upon the pretrained secret decoder. In
contrast, prior works [1–3] recover watermarks by training
a dedicated decoder with the main model in an end-to-end
fashion. To ablate this alternative approach, we train a stan-
dard decoder along with LDM by optimizing for the co-
sine similarity between the embedded and extracted water-
marks. We see a degradation in performance from 100%
to80.56%, indicating that the pretrained secret decoder is a
better choice for our approach. This is due to the increased
complexity of predicting watermarks of resolution 2562as
compared to 160-bit sequence from the encrypted images.
5. Conclusion
We introduce a novel proactive watermarking-based ap-
proach, ProMark, for causal attribution. We use prede-
fined training concepts like styles, scenes, objects, motifs,
etc. to attribute the influence of training data on gener-
ated images. We show ProMark’s is effective across var-
ious datasets and model types, maintaining image qual-
ity while providing more accurate attribution on a large
number of concepts. Our approach can also be extended
to multi-concept attribution by embedding multiple water-
marks onto the image. Finally, for each experiment, our ap-
proach achieves a higher attribution accuracy than the prior
passive approaches. Such attribution offers opportunities to
recognize and reward creative contributions to generative
AI, underpinning new models for value creation in the fu-
ture creative economy [13].
Limitations. In evaluating ProMark, we note a trade-off
between image quality and attribution accuracy, which may
need us to learn watermarks for attribution task. Our model
is currently trained with predefined concepts and further re-
search is needed on training paradigm when new concepts
are introduced. While we use orthogonal watermarks for
varied concepts like motifs and styles, this may not accu-
rately reflect the interrelated nature of some concepts, sug-
gesting another opportunity for future research. Finally, our
results are specific to the LDM, and extending this approach
to other GenAI models could provide a better understanding
of ProMark’s effectiveness.
10809
References
[1] Vishal Asnani, Xi Yin, Tal Hassner, Sijia Liu, and Xiaom-
ing Liu. Proactive image manipulation detection. In CVPR,
2022. 2,3,8
[2] Vishal Asnani, Abhinav Kumar, Suya You, and Xiaom-
ing Liu. PrObeD: Proactive object detection wrapper. In
NeurIPS, 2023. 2
[3] Vishal Asnani, Xi Yin, Tal Hassner, and Xiaoming Liu.
MaLP: Manipulation localization using a proactive scheme.
InCVPR, 2023. 2,3,8
[4] Vishal Asnani, Xi Yin, Tal Hassner, and Xiaoming Liu. Re-
verse engineering of generative models: Inferring model hy-
perparameters from generated images. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 45(12):15477–
15493, 2023. 3
[5] Kar Balan, Shruti Agarwal, Simon Jenni, Andy Parsons, An-
drew Gilbert, and John Collomosse. EKILA: Synthetic me-
dia provenance and attribution for generative art. In CVPR,
2023. 1,2,5,7
[6] Alex Black, Tu Bui, Hailin Jin, Vishy Swaminathan, and
John Collomosse. Deep image comparator: Learning to vi-
sualize editorial change. In CVPR WMF, 2021. 2
[7] Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas
M¨uller, and Bj ¨orn Ommer. Retrieval-augmented diffusion
models. NeurIPS, 2022. 7
[8] Tu Bui, Ning Yu, and John Collomosse. RepMix: Represen-
tation mixing for robust attribution of synthesized images. In
ECCV, 2022. 2
[9] Tu Bui, Shruti Agarwal, Ning Yu, and John Collomosse.
RoSteALS: Robust steganography using autoencoder latent
space. In CVPR, 2023. 3,6,8
[10] Xirong Cao, Xiang Li, Divyesh Jadav, Yanzhao Wu, Zhehui
Chen, Chen Zeng, and Wenqi Wei. Invisible watermarking
for audio generation diffusion models. In TPS-ISA, 2023. 3
[11] Nicholas Carlini, Chang Liu, ´Ulfar Erlingsson, Jernej Kos,
and Dawn Song. The secret sharer: Evaluating and testing
unintended memorization in neural networks. In USENIX,
2019. 1
[12] Chang Chen, Zhiwei Xiong, Xiaoming Liu, and Feng Wu.
Camera trace erasing. In CVPR, 2020. 2
[13] John Collomosse and Andy Parsons. To Authenticity, and
Beyond! Building safe and fair generative AI upon the three
pillars of provenance. IEEE Computer Graphics and Appli-
cations, 2024. 8
[14] Yingqian Cui, Jie Ren, Han Xu, Pengfei He, Hui Liu, Lichao
Sun, and Jiliang Tang. DiffusionShield: A watermark for
copyright protection against generative diffusion models.
arXiv preprint arXiv:2306.04642, 2023. 2,3
[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, 2009. 5
[16] Vitaly Feldman and Chiyuan Zhang. What neural networks
memorize and why: Discovering the long tail via influence
estimation. In Proc. NeurIPS, 2020. 2
[17] Pierre Fernandez, Guillaume Couairon, Herv ´e J´egou,
Matthijs Douze, and Teddy Furon. The stable signature:
Rooting watermarks in latent diffusion models. In ICCV,
2023. 2[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. NeurIPS, 2020. 1,7
[19] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan
Katz, Ian Miers, and Tom Goldstein. A watermark for large
language models. In ICML, 2023. 3
[20] Pang Koh and Percy Liang. Understanding black-box pre-
dictions via influence functions. In Proc. ICML, 2017. 2
[21] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli
Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating con-
cepts in text-to-image diffusion models. In ICCV, 2023. 1
[22] Yugeng Liu, Zheng Li, Michael Backes, Yun Shen, and
Yang Zhang. Watermarking diffusion model. arXiv preprint
arXiv:2305.12502, 2023. 2,3
[23] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik
Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.
On distillation of guided diffusion models. In CVPR, 2023.
1,7
[24] Eric Nguyen, Tu Bui, Vishy Swaminathan, and John Collo-
mosse. OSCAR-Net: Object-centric scene graph attention
for image attribution. In ICCV, 2021. 2
[25] Sen Peng, Yufei Chen, Cong Wang, and Xiaohua Jia.
Protecting the intellectual property of diffusion models
by the watermark diffusion process. arXiv preprint
arXiv:2306.03436, 2023. 3
[26] Ed Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya
Goyal, and Matthijs Douze. A self-supervised descriptor for
image copy detection. In CVPR, 2022. 5,7
[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, 2021. 5,7
[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR, 2022. 1,3,5,
7
[29] Nataniel Ruiz, Sarah Adel Bargal, and Stan Sclaroff. Dis-
rupting deepfakes: Adversarial attacks against conditional
image translation networks and facial manipulation systems.
InECCVW, 2020. 2
[30] Dan Ruta, Saeid Motiian, Baldo Faieta, Zhe Lin, Hailin Jin,
Alex Filipkowski, Andrew Gilbert, and John Collomosse.
ALADIN: All layer adaptive instance normalization for fine-
grained style similarity. In ICCV, 2021. 1,2,5,7
[31] Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid,
and Herv ´e J´egou. Radioactive data: tracing through training.
InICML, 2020. 2
[32] Wei Ren Tan, Chee Seng Chan, Hernan Aguirre, and Kiyoshi
Tanaka. Improved ArtGAN for conditional synthesis of nat-
ural image and artwork. IEEE Transactions on Image Pro-
cessing, 28(1):394–409, 2019. 5
[33] Run Wang, Felix Juefei-Xu, Meng Luo, Yang Liu, and Lina
Wang. FakeTagger: Robust safeguards against deepfake dis-
semination via provenance tracking. In ACM MM, 2021. 2
[34] Sheng-Yu Wang, Alexei A Efros, Jun-Yan Zhu, and Richard
Zhang. Evaluating data attribution for text-to-image models.
InICCV, 2023. 1,2,5,7
[35] Michael J Wilber, Chen Fang, Hailin Jin, Aaron Hertzmann,
John Collomosse, and Serge Belongie. BAM! the behance
10810
artistic media dataset for recognition beyond photography.
InICCV, 2017. 6
[36] Yuguang Yao, Xiao Guo, Vishal Asnani, Yifan Gong,
Jiancheng Liu, Xue Lin, Xiaoming Liu, and Sijia Liu. Re-
verse engineering of deceptions on machine- and human-
centric attacks. Foundations and Trends in Privacy and Se-
curity, 2024. 3
[37] Chin-Yuan Yeh, Hsi-Wen Chen, Shang-Lun Tsai, and Sheng-
De Wang. Disrupting image-translation-based deepfake al-
gorithms with adversarial attacks. In WACVW, 2020. 2
[38] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas
Funkhouser, and Jianxiong Xiao. LSUN: Construction of
a large-scale image dataset using deep learning with humans
in the loop. arXiv preprint arXiv:1506.03365, 2015. 5
[39] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Ngai-
Man Cheung, and Min Lin. A recipe for watermarking dif-
fusion models. arXiv preprint arXiv:2303.10137, 2023. 3
10811
