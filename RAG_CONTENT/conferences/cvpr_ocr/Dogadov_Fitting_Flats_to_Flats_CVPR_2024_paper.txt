Fitting Flats to Flats
Gabriel Dogadov Ugo Finnendahl Marc Alexa
TU Berlin, Computer Graphics Group
cg.tu-berlin.de
Abstract
Affine subspaces of Euclidean spaces are also referred
to as flats. A standard task in computer vision, or more
generally in engineering and applied sciences, is fitting a
flat to a set of points, which is commonly solved using the
PCA. We generalize this technique to enable fitting a flat to
a set of other flats, possibly of varying dimensions, based
on representing the flats as squared distance fields. Com-
pared to previous approaches such as Riemannian centers
of mass in the manifold of affine Grassmannians, our ap-
proach is conceptually much simpler and computationally
more efficient, yet offers desirable properties such as re-
specting symmetries and being equivariant to rigid trans-
formations, leading to more intuitive and useful results in
practice. We demonstrate these claims in a number of syn-
thetic experiments and a multi-view reconstruction task of
line-like objects.
1. Introduction
Affine subspaces of Euclidean spaces, also called flats,
are the basic entities in affine geometry, and are founda-
tional building blocks for computations in computer vi-
sion, computer graphics, and generally all engineering dis-
ciplines. To motivate this work with a concrete example,
notice that a point observed in a calibrated camera gives
rise to a line in 3-space. Observing the same point in several
cameras allows recovering its position by fitting a point to
the collection of lines. Minimizing the squared distances to
the lines only requires solving a linear system. Now imag-
ine the features observed in the images are lines, giving rise
to planes in each camera view. We would like to fit a line to
the set of observed planes. Surprisingly, despite the prob-
lem appearing almost identical, there is no established ’stan-
dard’ solution to this problem.
Conversely, we often have to work with samples (obser-
vations) in some Euclidean space. The PCA [29] is arguably
the standard way to fit a line or plane to the observation if
they are points in space – but what do we do if the observa-
tions are lines or planes?Approaching these problems in a systematic way com-
monly leads to Grassmannians , the manifold of linear sub-
spaces. Similar to how homogeneous coordinates of Eu-
clidean space lead to a representation of projective space,
Grassmannians can be projectivized and then represent
flats. The first and likely best-known example are Pl ¨ucker
coordinates for affine lines in R3[5, 16, 27]. One may argue
that the resulting spaces are not strictly representing flats, as
they also contain ideal elements, i.e., flats ’at infinity’. This
is rectified in the explicit construction of a Grassmannian
of affine subspaces [23]. While the manifold of the for-
mer construction is represented by the Klein quadric (and
its generalizations), the latter is described by orthogonality
conditions, a so-called Stiefel manifold (we review several
representations of flats in Sec. 2). It has been argued [22]
that computation on Stiefel manifolds is more convenient
and better established [1, 2, 8]. Still, the mathematical so-
phistication and complexity of necessary computations for
something as mundane as the mean of a set of affine lines
or planes in 3D, let alone fitting flats to flats of different di-
mensions, is baffling when one compares it to the simplicity
of fitting a point or fitting to points.
In this work, we provide a simple framework, based on
representing flats as squared distance fields (see Sec. 2 for
details on the representation and the relation to other repre-
sentations). As we derive in Sec. 4, it allows fitting a flat of
desired dimension kto a given set of flats of arbitrary and
possibly varying dimension. Importantly, we show that it
is equivariant under rigid transformations, the isometries of
affine space. Unlike Pl ¨ucker coordinates and related repre-
sentations, the representation is unoriented , which together
with equivariance immediately implies that both angles as
well as distances are bisected by least-squares fits to two
flats. To our knowledge, this is the first method with these
very natural properties. In addition, the necessary compu-
tations are similar to fitting flats to points with the PCA,
with the most complex operation being the eigendecompo-
sition of a symmetric PSD matrix. An interpretation of the
procedure as projected means in ambient space leads to ex-
tensions to Lpmeans, enabling more robust fitting in the
presence of outliers using the L1-norm.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
5439
We evaluate the properties of our approach in compari-
son to Riemannian means in the space of affine Grassman-
nians on synthetic examples in low dimension (Sec. 5). We
verify the predicted properties of both classes of methods.
In addition, we demonstrate the method at the example of
reconstructing line-like objects from multiple views.
The basic approach leaves much room for applying more
advanced statistical methods, some of which we discuss in
Sec. 6.
2. Background: Representations of flats
In the following, we assume the ambient space is Rd(un-
less otherwise noted). A k-flat is ak-dimensional affine
subspace of the ambient space. In contrast, a k-plane is a
linear subspace. Every k-plane is a k-flat, but a k-flat is
ak-plane only if it passes through the origin. For d= 3,
flats can be points ( k= 0 ), lines ( k= 1 ), or planes
(k= 2). A (d−1)-flat in Rdis called hyperplane . The
set of all k-planes in Rdis the Grassmannian of linear
subspaces Gr(k, d), a smooth compact manifold of dimen-
sionk(d−k). Similarly, the set of all k-flats is called
theGrassmannian of affine subspaces or short the affine
Grassmannian Graff( k, d)[19], a smooth (k+ 1)( d−k)-
dimensional manifold. While the Grassmannian, represen-
tations, and computational methods are well established, the
affine Grassmannian is a more recent development [22, 23],
but created interest for applications such as image classifi-
cation [30] or LiDAR registration [24].
The orthogonal complement of ak-plane P ⊆Rdis a
linear subspace denoted P⊥and contains all vectors that are
orthogonal to P. The dimension of P⊥is the co-dimension
¯k=d−kofP.
In the following, we describe several representations of
flats, i.e., elements of Graff( k, d), needed for the ensuing
discussion of existing and new techniques for least squares
fitting flats to flats.
Parameter form A matrix A∈Rd×kwith full column
rank represents a linear k-space in Rd. A flat Fcan be
represented by additionally specifying a displacement b∈
Rd:
F=
x∈Rd:x=Ay+b,y∈Rk	
. (1)
We ask that the columns of Aare orthonormal and that
ATb=0. This implies that bis the vector to the point
closest to the origin and is unique. Under this condition,
the representation yin Eq. (1) has been referred to as or-
thogonal affine coordinates [22, 23]. We may arbitrarily
change the basis of these coordinates with an orthogonal
transformation O∈Rk×kwithout affecting the flat. So
(AO,b),OTO=Idescribes the set of equivalent repre-
sentations of F.Stiefel and Grassmann-Pl ¨ucker coordinates The affine
Grassmannian in parameter form can be embedded into a
(standard) Grassmannian one dimension higher, similar to
Euclidean points being represented as lines in homogeneous
coordinates: given a k-flat(A,b), it is treated as a k+ 1-
plane in Rd+1spanned by (AT,0)Tand(bT,1)T. Normal-
izing the last basis vector leads to the (homogeneous) Stiefel
coordinates [11, 22] for a k-flatF:
Y=A b /p
1 +∥b∥2
0T1/p
1 +∥b∥2
. (2)
This representation of a flat still admits orthogonal trans-
formations that map span(Y)intospan(Y). To reduce
the degrees of freedom, one may represent the parallelo-
tope spanned by the basis of this linear space as the exterior
product of the basis vectors. This construction is known as
thePl¨ucker embedding [27]. Intuitively, one may think of
the representation as the signed (hyper)areas of the shadows
of the parallelotope onto all k-planes spanned by the axes of
a fixed coordinate system. These (hyper)areas are indepen-
dent of the choice of the vectors A′spanning the parallelo-
tope, with the only degree of freedom being the total signed
volume of the parallelotope. This suggests that the result-
ing representation consisting of d
k
elements is unique up
to scale for a given flat. In general, d
k
>(k+1)(d−k), so
only a subset of the coordinates corresponds to flats. This
subset is described as the intersection of quadratic surfaces,
theGrassmann-Pl ¨ucker relations . The particular case of
affine lines ( k=1) in(d=3)-space of this representation is
well known as Pl¨ucker coordinates1.
Normal form We may also represent the flat Fas the
intersection of ¯khyperplanes, each defined as nT
ix=ci.
Writing the normal vectors nias rows of a matrix N∈
R¯k×dand the offsets as a vector c∈R¯k, the flat is repre-
sented as:
F=
x∈Rd:Nx=c	
. (3)
Note that the row space of Nis the orthogonal complement
of the column space of AinRd. As before, we ask that the
basis is chosen orthonormal. Under this condition, the vec-
torccontains the signed distances of the hyperplanes to the
origin (see Fig. 1 (left) for an illustration). Note that equal-
ity in Eq. (3) is preserved for any change of basis O∈R¯k×¯k
and that the rows of ON remain orthonormal if Ois orthog-
onal. Thus, in normal form, the set (ON,Oc)describes the
same flat F.
1The original formulation of Pl ¨ucker coordinates starts from two points
on the line in homogeneous coordinates – the construction starting from
Stiefel coordinates mentioned above is equivalent, as both span the same
linear space.
5440
a1 n1b|c1|
−2 −1 0 1 2−2−1012
xy
q(x)
FFigure 1. Left: The squared distance between a point (black) and
a line (red) corresponds to the sum of squared distances to two
orthogonal hyperplanes (blue and green) whose intersection is the
line. Right: The connection between the squared distance field q
of a line in R2, its parametric representation (a1,b)and normal
representation (n1, c1).
Squared distance function The normal representation of
Fyields the distances to the hyperplanes for any x∈Rdas
Nx−c. We can get the squared distance by computing the
the squared norm of this vector:
d2
F(x) = (Nx−c)T(Nx−c) =xTNTNx−2cTNx+cTc.
(4)
Note that the squared distance field is unaffected by start-
ing from a different orthogonal normal frame ON and cor-
responding distance vector Oc: the resulting distances are
O(Nx−c)and the inner product OTO=Icancels. This
means the symmetric positive semi-definite (PSD) matrix
Q=NTN, the vector r=−NTcand the scalar s=cTc
uniquely describe a k-flat as
F=
x∈Rd:d2
F(x) =xTQx+ 2rTx+s= 0	
.(5)
Any non-zero scalar multiple of the triple (Q,r, s)of the
squared distance field has the same zero-set, so we may in-
terpret this representation as a homogeneous coordinate for
k-flats. Similar to Pl ¨ucker-Grassmann coordinates, only a
subset of symmetric matrices Q, vectors r, and scalars s
correspond to k-flats. Note that Qcontains the bases en-
coded by AandNas eigenspaces corresponding to the
eigenvalues 0and1. SoQis characterized by the spec-
trum consisting of the eigenvalues 0with multiplicity kand
1with multiplicity ¯k.
The vector rhas been constructed as −NTc, so it is a lin-
ear combination of the normals and has to be an eigenvector
ofQwith eigenvalue 1.
Since Qis PSD, the zero set is equivalent to the local
minima of the squared distance field and could be computed
by setting the gradient 2Qx+ 2rto zero. This allows re-
covering sfrom (Q,r)by asking that the squared distances
evaluate to zero for the minima, yielding
s=rTQ+r=rTQr=rTr, (6)where we have exploited that Qis its own pesudo-inverse
under the assumptions on its spectrum and that ris an eigen-
vector with eigenvalue one. Since sis redundant, we can
represent flats by the pair (Q,r). We summarize its relation
to the standard parameter form ( A,b)below:
(Q,r) = (I−AAT,−b) ( A,b) = (ker( Q),−r).
(7)
Fig. 1 (right) illustrates the connection to both standard and
normal forms.
3. Related work: Riemannian centers of mass
Recall that the Grassmannian is a smooth compact mani-
fold. Let {Pi}represent k-planes, then least-squares fitting
ak-plane to this data implies
m= arg min
P∈Gr(k,d)X
id2(P,Pi), (8)
where dis a metric. In fact, this is a way of computing a
mean in Riemannian manifolds and has been aptly termed
Riemannian center of mass [17], now often referred to as
Karcher mean . Note that by embedding Graff( k, d)into
Gr(k+ 1, d+ 1) , we can use the same approach for fit-
ting a k-flat to given k-flats. In the following, we will first
explain the metric we have found to be commonly used in
our context, and then how to extend the basic ideas to linear
subspaces of varying dimensions.
Principal angles and metric The concept of angles be-
tween two lines (in the plane or in space), two planes or a
line and a plane in space can be generalized to flats of any
dimension. Given a k-plane represented by A1and an l-
plane represented by A2and assuming k≤l, the principal
angles are a set of kmutual angles 0≤θ1≤. . .≤θk≤π
2
that are defined recursively as:
θi:= min
cos−1|uTv|
∥u∥∥v∥u∈span( A1),
v∈span( A2),uTuj=0,
vTvj=0,
∀j∈{1,...,i−1}
.
(9)
The vectors (ui,vi)forming the principal angles are the
principal vectors . The principal angles and vectors can
be computed via a (reduced) Singular Value Decomposi-
tion (SVD) [4]. When decomposing AT
1A2=UΣVT
withΣ= diag( σ1, . . . , σ k), the principal vectors are the
columns of A1UandA2V, respectively, and the corre-
sponding principal angles are given by θi= cos−1(σi).
A commonly used metric on Gr(k, d)constructed from the
principal angles isPk
i=1θ2
i1/2
.
While one can compute principal angles between flats in
the very same way, the angles lack information about the
displacement between the flats if they are not intersecting
5441
(e.g., consider two skew lines in 3-space). The distance
due to rotation of the linear subspaces and translation of the
points closest to the origin are automatically reconciled by
using the Stiefel coordinates introduced above. The result-
ing principal angles in this embedding have been referred to
asaffine principal angles [22]. Moreover, optimization as
necessary here for computing the mean is well-understood
in Stiefel manifolds [2], and we detail the computation of
means next.
Computing the mean in Stiefel coordinates Let flats Fi
be given and represented in Stiefel coordinates Yi. For
Stiefel coordinate Y, the gradient of the sum of squared
distances in Eq. (8) is given by [18]
−mX
i=1exp−1
Y(Yi), (10)
with exp−1
Y(X)denoting the derivative of the geodesic
that connects YandX. As the affine principal angles,
similar to principal angles for planes as in Eq. (9), are
also defined for flats of different dimensions, geodesic dis-
tances between flats of different dimensions and their gra-
dient can be computed in a similar manner as described
in [22, 36]. The gradient can be exploited to minimize the
sum of squared geodesic distances in an iterative minimiza-
tion scheme such as gradient descent or Newton’s method
on Stiefel manifolds [1, 2, 8]. This essentially requires re-
orthogonalization of Yafter each descent step.
In our implementation, we used a variable step size gra-
dient descent scheme but avoided computing the Hessian.
For further information on the gradient computation, our
adaptation to flats of different dimensions, and the orthog-
onalization scheme we found to yield the best results, we
refer to the supplementary material.
4. Method
Given a set of flats {Fi}, we want to fit a flat ˆFwith
fixed dimension kin the least-squares sense. We start by
revisiting the case of fitting a point, i.e., the case k= 0, and
then extend the approach to k >0.
Fitting a point The sum of squared distances to flats from
an arbitrary point xcan be written as the sum of the squared
distance fields to the flats:
σ2(x) =X
id2
Fi(x∗) (11)
=xT X
iQi!
x+ 2 X
iri!
x+X
isi
=xTQ∗x+ 2r∗x+s∗. (12)Algorithm 1: Closest Flat (Mean-SDF)
Input : Flats (of arbitrary dimension) as squared
distance fields (Q1,r1), . . . , (Qm,rm)
Output: k-Flat in parameter form (A,b)
begin
Q∗←Pm
i=1Qi
r∗←Pm
i=1ri
UDUT←Q∗// Eigendecomposition,
see Eq. 14 for conventions
A←[u1, . . . ,uk]// Eq. 15
Q∗+←UD+UT
b←(I−AAT)Q∗+r∗
end
The minimal value is attained at a critical point and can be
found by setting the gradient to zero, yielding the necessary
condition Q∗ˆx=r∗. If this sum of squared distances has a
unique minimum, the linear system has a unique solution.
Notice that simply adding the representations of the flats
is quite convenient, particularly in applications where flats
are being added dynamically: this is commonly exploited
in computer graphics for modeling with piecewise planar
surfaces. By associating squared distances with the planar
polygons, it is easy to measure changes to the shape relative
to the original shape, most prominently used in surface sim-
plification [10]. Conversely, the same concept can be used
for fitting planes to sample points [38].
Fitting discretely sampled flats Now let us consider the
casek= 1, an affine line. Naturally, one would ask that the
sum of squared distances over all points on the line are min-
imized. This leads to an indefinite integral that takes on a
finite value only in degenerate cases. It is illuminating, how-
ever, to start with a finite number mof samples of the line:
represent the line in parameter form as ya+bwith∥a∥= 1
and sample it in parametric locations {yj}. We assume that
these locations are mean-unbiased, i.e.P
jyj= 0. The
sum of squared distances summed up over all samples is
X
y2
jaTQ∗a+mbTQ∗b+ 2mbTr∗+ms∗,(13)
where we have already removed all terms with the factorP
jyj= 0. We notice that we can independently optimize
foraandb. Forawe are looking to minimize caTQawith
positive c=P
jy2
jsubject to ∥a∥= 1. This means ais the
eigenvector of Q∗corresponding to the smallest eigenvalue.
The positive constant chas no consequences. For bwe find
that it has to satisfy Q∗b=r∗(regardless of the sampling
{yj}). The resulting pair (a,b)describes the desired line,
but is not yet in standard form, because bis not necessarily
orthogonal to a. This can be achieved by replacing bwith
b−(aTb)a.
5442
It is straightforward to extend this analysis to k-flats for
k > 1. We find that the basis Ahas to be chosen as the
eigenvectors corresponding to the smallest keigenvalues of
Q∗. To make this concrete, let
Q∗=UDUT,UTU=I,
D= diag( λ1, . . . , λ d),0≤λ0≤. . .≤λd−1(14)
thenAconsists of the first kcolumns of U:
U= [u1, . . . ,ud] =⇒A= [u1, . . . ,uk] (15)
The condition Q∗b=r∗is independent of kand bringing
it into standard parameter form leads to
b= (I−AAT)Q∗+r∗. (16)
Note that this solution is independent of the number of sam-
ples and their distribution. In fact, it extends to any weight-
ing of the point samples as long as the weighted mean is
still zero. With appropriate weighting, we could make sure
that the constant cremains finite for infinite sampling, in-
cluding a dense sampling of the line. This means the choice
for(A,b)described in Eqs. (15) and (16) solve the fitting
problem for flats in the least-squares sense. Alg. 1 summa-
rizes the described procedure in pseudocode.
Complexity Assuming we start from flats given in param-
eter form (Ai,bi)building the matrices Qi=I−AAT
isO(kd2)for each flat. Summing up Q∗andr∗is lin-
ear in the number of flats and the number of coefficients in
the matrices and vectors. The most complex operation is
the necessary eigendecomposition of Q∗, which is O(d3).
We implement this using known closed-form solutions for
d= 2,3[7] and a symmetric QR algorithm [12] for d >3.
Overall, the complexity of fitting a k-flat in this way to a set
of arbitrary flats is identical to the complexity of fitting flats
to points using the PCA. This appears to be quite natural,
yet it is remarkable that existing solutions (to our knowl-
edge) are significantly more involved and much slower in
practice (see also Sec. 5).
Uniqueness and degeneracies All steps in the above fit-
ting procedure are uniquely determined, except for selecting
the eigenvectors Acorresponding to the smallest keigen-
values of Q∗. This step assumes that λkis strictly smaller
thanλk+1.
The dependence of the spectrum of the sum of matri-
ces on the spectra of the summands is generally quite in-
volved [9, 20], but it is worth pointing out some special
cases in our context: (1) If all input flats are points, then
Q∗also represents a point, i.e., all its eigenvalues are iden-
tical. So while our approach works for arbitrary k-flats, it
fails for the special case k= 0for all flats (which is exactlywhat the PCA solves). (2) Two orthogonal lines in R2, three
mutually orthogonal planes in R3, or more generally dmu-
tually orthogonal d−1flats in Rdresult in an isotropic
squared distance field, so all eigenvalues of Q∗are identi-
cal. (3) Similar statements can be made for subspaces, lead-
ing to parts of the spectrum being isotropic. For example,
two non-intersecting lines in R3with orthogonal directions
uniquely define a plane, but every line in the plane spanned
by the directions is an equally good fit.
Equivariance It seems very natural to ask that least
squares fitting is isometry invariant . The isometries of
affine spaces are rigid transformations and reflections. This
means we expect that if the input is rigidly transformed and
reflected, the fitted flat undergoes the same transformation
– it is equivariant to these transformations.
This property is not difficult to show for the procedure
above. Instead of x, we plug in the transformed point
Rx+t, where Ris an orthogonal transformation in Rd
andt∈Rda translation. What we find is that in the sum
of squared distances Eq. (13), the term caTQ∗atransforms
intocaTRTQ∗Ra, indicating that the direction vector ahas
to be rotated by R, as expected. For the terms involving b
we get (b+t)TQ∗(b+t) + 2( b+t)Tr∗and setting the
gradient w.r.t. bto zero yields Q∗(b+t) =−r∗, again, as
expected. Note that the standard form for bas computed in
Eq. (16) is not necessarily translated by t, as the projection
ontoAis removed.
It seems worth pointing out that, as natural as this prop-
erty may appear, the Riemannian center on the affine Grass-
mannian varies with the translation of the coordinate sys-
tem. We demonstrate this with examples in the supplemen-
tary material.
Elementary properties At least for two k-flats, we can
formulate what we expect for their mean: if the two flats
intersect, the mean should be the bisector; and if they are
parallel, the mean should be parallel and have the same dis-
tance to both of them.
In fact, both properties directly follow from the fact that
the computations are symmetric in the inputs and equivari-
ant to translations and reflections. To see this, translate the
flats so that they are symmetric w.r.t. the origin. Then reflec-
tion at the origin has no effect on the input, so the output has
to respect the symmetry as well.
Interpretation as projected mean Consider the fitting as
starting from a set of flats represented by the pairs (Qi,ri)
and generating the mean in this form as (ˆQ,ˆr). In this set-
ting, we may interpret the procedure as: first, compute the
mean in the ambient space of symmetric PSD matrices and
Euclidean vectors and, second, project onto the manifold of
matrices and vectors representing k-flats.
5443
Clearly, (Q∗,r∗)arise from the set of Qi,rias a mean
(modulo irrelevant scale factors). Given Q∗, we constructed
Abased on the eigenvector corresponding to the smallest
eigenvectors. This suggests that the normal space Ncon-
tains the eigenvectors corresponding to the largest eigen-
vectors. Or, in other words, the fitted flat is represented
byˆQwith the same eigenspace as Q∗, but the ksmallest
eigenvalues mapped to zero and the remaining eigenvalues
mapped to one. Recall the decomposition of Q∗in Eq. (14),
then we get
ˆQ=Udiag(0 , . . . , 0|{z}
ktimes,1, . . . , 1)UT. (17)
Andˆris related to bin Eq. (16) simply by ˆr=−b=
−ˆQQ∗+r∗.
We want to show that this mapping is an orthogonal pro-
jection. For the PSD matrix, we claim that any unitarily
invariant norm in the space of symmetric PSD matrices is
minimized. To see this, consider minimizing ∥Q∗−X∥
among symmetric matrices Xhaving the desired spectrum
(0T
k,1T
¯k). IfXhas this spectrum then so does X′=UXUT
so we get the equivalent minimization problem
arg min
λ(X′)=(0T
k,1T
¯k)∥D−X′∥,D=UTQ∗U. (18)
Mirsky [28] has shown that for any matrix the norm of the
difference is not smaller than the norm difference of the
sorted singular values. For symmetric PSD matrices, the
singular values are the eigenvalues. So we have
∥D−X′∥ ≥ ∥D−diag(0T
k,1T
¯k)∥, (19)
showing that the best solution for X′is the diagonal matrix,
and we find X=UTX′Uas constructed.
Forˆr, we see that it minimizes ∥ˆr+ˆQQ∗+r∗∥.
Means in other norms The interpretation of the least-
squares fitting as first computing the L2-mean in the space
of matrix and vector coefficients of the representation
(Q,r)and then projecting back onto the manifold of k-
flats suggest a simple approximation of other means: we can
compute any Lpmean in the space of the matrix and vector
computations and then project. This reduces the problem
of computing Lpmeans to the well-understood problem of
doing so in Euclidean spaces.
We particularly consider the case of the L1-norm, or ge-
ometric median, as it is known to be robust against outliers.
For this, we use Weiszfeld’s algorithm [34], which is essen-
tially an iterative re-weighted least squares method. As we
show in Sec. 5, it indeed shows robustness to outliers, and
is not only significantly simpler to implement than Rieman-
nianLpcenters of mass [3], but also preserves the equivari-
ance properties that are missing from the methods based on
Stiefel coordinates.5. Experiments
We demonstrate in the following experiments that our
method is both faster and yields more useful results com-
pared to optimization in the affine Grassmannian [22], and
provide a brief experiment in a realistic application sce-
nario for reconstructing line-like objects from more than
two views. All results reported are based on implementa-
tions in C++ using the Eigen library [15] for numerical lin-
ear algebra. Running times were gathered on a computer
with an Intel i5-13600K CPU and 32GB RAM.
Data generation We randomly generate a target k-flat
F∗in standard form (A∗,b∗). Then, we generate mflats
of dimension lby sampling n≥drandom points xi=
b∗+A∗yi+ξiin a sample interval of yi∈[−10,10]kand
displacing them with additive, zero-mean Gaussian noise
ξi∼ N (0, σ2I). Subsequently, PCA is used to fit an l-flat
to the nsampled points. The noise on the sample points
is intended to simulate measurement and calibration errors
from the real world. This procedure is repeated until mflats
{F1, . . . ,Fm}are generated as observations to reconstruct
F∗.
Iterative approach on the Grassmannian As a compar-
ison to our method, we optimize the objective in Eq. (8)
using gradient descent on the Stiefel manifold. We use a
combination of the Frobenius norm of the gradient, the dif-
ference between two consecutive iterates, and the number
of iterations as the stopping criteria.
Our method We use our method from Eq. (15) and
Eq. (16) in two variations. In the first one, we compute
Q∗andr∗as the arithmetic mean of the respective matrices
{Qi}and{ri}(Mean-SDF). In the second one, we compute
Q∗andr∗as the median of the respective matrices using
Weiszfeld’s algorithm, where we use the Frobenius norm
for computing Q∗and the Euclidean norm for r∗(Median-
SDF). Although the projection is identical to the previous
case, with this method, we may get more stable results in
the presence of outliers.
Metrics To compare the reconstructed flat F′to the orig-
inalF∗, we use their principal angle(s) θ1, . . . , θ kand their
least-squares distance [14]
dmin(F∗,F′) = min
x∗∈F∗,x′∈F′∥x∗−x′∥ (20)
as separate metrics.
Efficiency Fig. 2 shows the average running time of our
method and the method from [22] over ten trials on a
log scale. For each dimension of the ambient space d∈
5444
4 6 8 10 12 14 16 18 20
Dimension of ambient space d10−210−1100101102Running time [ms]Dimensions
kin=kout= 1
kin=d−1, kout= 1
kin=kout=d−1Method
Graff
Mean-SDF (ours)
Median-SDF (ours)
Figure 2. Running time comparison between our methods and the
iterative optimization on the affine Grassmannian (Graff) for dif-
ferent dimensions of flats and ambient space but constant input
sizem= 20 and noise level σ= 0.2.
Figure 3. Comparison between the mean and the median estima-
tion of Q∗andr∗using our method. A line (red) that is the inter-
section of four planes (yellow) is reconstructed while having a fifth
plane as an outlier (purple). On the left, only the displacement of
the outlier is off. On the right, both orientation and displacement
are off. The Mean-SDF method yields the blue line, while the
green line is the output of the Median-SDF method.
{4, . . . , 20}, three different experiments were conducted:
(1) reconstructing a line from other lines ( kin=kout= 1),
(2) reconstructing a line from hyperplanes ( kin=d−1,
kout= 1), and lastly (3) reconstructing a hyperplane from
hyperplanes ( kin=kout=d−1). In every case, we chose
the number of input flats as m=d. A limit of 200 itera-
tions was set for the iterative optimization. It is to be ex-
pected that our method is significantly faster than the itera-
tive computation of the Riemannian center, and our results
clearly show that this is the case. The running time of our
method is similar for all cases irrespective of the dimension
of the input and output flats, which is, to some degree, ex-
pected, as our method mainly involves the diagonalization
of ad×dmatrix whose dimensionality only depends on the
dimension of the ambient space. We note that the dimen-
sion of the ambient space dis likely to be less pronounced
for a larger number of samples m, as then the assembly of
Q∗,r∗might dominate the computation.
Accuracy Tab. 1 shows distances of a reconstructed affine
line from planes in R3. While we are aware that, in this par-Table 1. The minimum distance dminand (principal) angle θ
between a target line F∗and its reconstruction F′inR3from
m= 20 planes, averaged over ten trials. Top: varying noise levels
σwith no outliers. Bottom: varying percentage of outliers with
constant noise level σ= 0.2. The best result for each condition is
highlighted in bold , the second best is underlined .
Graff Mean-SDF Median-SDF
↓dmin ↓θ↓dmin ↓θ↓dmin ↓θ
Noise σ
0.5 0.2174 0.0402 0.1292 0.0381 0.1192 0.0434
1.0 0.3411 0.1112 0.2417 0.0913 0.1870 0.0817
1.5 0.3953 0.3240 0.3297 0.1357 0.2697 0.1252
2.0 0.4167 0.3914 0.4843 0.1045 0.2854 0.1095
2.5 0.5946 0.4817 0.4599 0.1374 0.3200 0.1339
Outlier %
0.0 0.2325 0.0212 0.0227 0.0190 0.0929 0.0177
0.1 0.1801 0.0242 2.6684 0.0438 0.1274 0.0423
0.2 0.1892 0.0189 6.6070 0.0696 0.0919 0.0518
0.3 0.2944 0.0387 8.2614 0.0951 0.1650 0.0822
0.4 0.2891 0.0325 12.4167 0.1880 0.2195 0.1764
0.5 0.3917 0.0568 15.0596 0.2536 0.3857 0.2232
ticular scenario, other techniques could have been used, it
still serves to illustrate the difference between our method
and computations in the affine Grassmannian, which we
have found to be consistent across dimensions of flats and
ambient dimension.
The upper half shows results for experiments with only
Gaussian noise added. For the remaining rows in Tab. 1,
we have added outliers to the data (see Fig. 3). It is evident
that our Mean-SDF variant is not robust to outliers, espe-
cially in terms of displacement, in which the results appear
to be worse than the iterative method. Yet the Median-SDF
variant recovers from this loss of robustness, especially in
terms of the translation distance, even with a high propor-
tion of outliers, and provides the best overall results. The
difference between our two variants is visualized in Fig. 3.
Application: Archery A possible application is line re-
construction in multiple-view geometry. Imagine a line-like
object being extracted in more than 2 registered cameras.
For each camera, the line in screen space creates an instance
of a plane in world-space, containing the line in 3-space.
These planes are expected to intersect in the common line,
but due to noise in calibration and registration of the cam-
era as well as in the feature extraction in the discrete im-
ages this will not be the case. The standard approach to this
problem is to represent the planes as a matrix Mby stack-
ing the normal representation Ni(see Sec. 2) in rows and
apply the SVD to find the best rank-2 approximation with
respect to the Frobenius norm. The kernel of the resulting
rank-reduced matrix represents the line. We note that the
5445
Figure 4. Multiple views of an archery target. Given the camera parameters, the re-projected arrows form planes in world space that do
not intersect due to detection and calibration errors. Our method can be used to find a good representative. The original arrow is colored in
red, given a bad calibrated camera our result is colored in blue and the rank minimization approach [16] is colored in green.
properties of this approach in the presence of noise are not
clear, because the normal representation is not unique and,
as a consequence, it is not clear what is being minimized.
In our method, it is clear that the least-squares distance of
the reconstructed line to the planes is being minimized, as
we showed that this happens for any sampling of the liner.
We compare the two approaches using a synthetic
archery example, where we try to reconstruct the arrows
in 3D space. A scene is rendered from four different
perspectives and the cameras are calibrated with moder-
ate noise being introduced. We then ’shoot’ random ar-
rows onto the target and detect the features in the image
spaces of the cameras. As expected, the results for the rank-
minimization [16] depends on the choice of representation
of the planes. Fig. 4 shows the results for a typical case,
comparing to our results. While our method naturally yields
consistent reconstruction very close to ground truth, rank-
minimization is significantly off in some cases.
6. Discussion
The method for least-squared fitting of flats to flats based
on squared distance functions, as simple as it is, exhibits
various nice properties that are not present in other, seem-
ingly more principled approaches such as the manifold of
affine Grassmannians. We are unaware of any construc-
tion that offers the desired equivariance to the isometries
of affine space for means or fitting procedures of flats in
arbitrary dimensions (without shifting the input to com-
pensate for the missing equivariance to translations [24]).
We briefly comment below on a possible similar construc-
tion using Grassmann-Pl ¨ucker coordinates, and then men-
tion some further use cases and investigations.
Interpolation in Grassmann-Pl ¨ucker coordinates
Since Pl ¨ucker coordinates are unique up to scale, one might
want to try, similar to the interpretation of our approach,
computing the mean of the coefficients and then projecting
back onto the space described by the Grassmann-Pl ¨ucker
relations.
Taking the mean of the coefficient requires selecting an
appropriate scale. In general, this might be difficult [21]and typically introduces bias for noisy data. In our con-
text, it is not difficult to see that taking the mean leads to
consistent results if the basis taken on the flat is orthonor-
mal, i.e., if the coefficients are constructed from the Stiefel
coordinates as described in Sec. 2. This method still suf-
fers from several drawbacks: (1) Grassmann-Pl ¨ucker coor-
dinates represent oriented spaces [31]. This means the line
constructed from point band direction ais not the same as
the one constructed from band−a. Taking the mean de-
pends on the choice of orientation. (2) The ’projection’ onto
coefficients satisfying the Grassmann-Pl ¨ucker relations is,
in general, significantly more involved than computing the
eigendecomposition [13]. (3) The Grassmann-Pl ¨ucker co-
ordinates naturally arise as antisymmetric rank- ktensors. It
is unclear (to us) how one could perform computations in-
volving different dimensions, although admittedly Schubert
varieties may be used [26], similar to the construction for
affine Grassmannians.
Outlook The PCA fitting flats to points may be consid-
ered the simplest statistical analysis of a set of point-like ob-
servations. This can be extended to fitting several flats such
as in clustering methods or, more generally, describing data
using mixture models [6, 32, 33, 37]. All of these meth-
ods may be generalized to work with flats as the observed
samples, using the methods we have introduced here.
As described, our method can be re-interpreted as tak-
ing the mean in ambient space and projecting, and in this
way used similar to methods in Euclidean spaces. This
suggests other methods for improving robustness to out-
liers could be used, as well as considering the L∞-norm,
whose minimization yields the center of the smallest enclos-
ing ball [35]. This approach would be considerably simpler
than computing geodesic disks enclosing a set of flats [25].
Acknowledgements
This work was funded by the European Research Coun-
cil (ERC) under the European Union’s Horizon 2020
research and innovation program (Grant agreement No.
101055448, ERC Advanced Grand EMERGE).
5446
References
[1] Pierre-Antoine Absil, Robert Mahony, and Rodolphe Sepul-
chre. Riemannian geometry of grassmann manifolds with a
view on algorithmic computation. Acta Applicandae Mathe-
matica , 80:199–220, 2004. 1, 4
[2] Pierre-Antoine Absil, Robert Mahony, and Rodolphe Sepul-
chre. Optimization algorithms on matrix manifolds . Prince-
ton University Press, 2008. 1, 4
[3] Bijan Afsari. Riemannian lp center of mass: existence,
uniqueness, and convexity. Proceedings of the American
Mathematical Society , 139(2):655–673, 2011. 6
[4] ˚Ake Bj ¨orck and Gene H Golub. Numerical methods for
computing angles between linear subspaces. Mathematics
of Computation , 27(123):579–594, 1973. 3
[5] James F. Blinn. A homogeneous formulation for lines in
3 space. SIGGRAPH Comput. Graph. , 11(2):237–241, jul
1977. 1
[6] Paul S Bradley and Olvi L Mangasarian. K-plane clustering.
Journal of Global optimization , 16:23–32, 2000. 8
[7] Charles-Alban Deledalle, Loic Denis, Sonia Tabti, and Flo-
rence Tupin. Closed-form expressions of the eigen decom-
position of 2 x 2 and 3 x 3 hermitian matrices. Technical
report, 2017. 5
[8] Alan Edelman, Tom ´as A Arias, and Steven T Smith. The ge-
ometry of algorithms with orthogonality constraints. SIAM
journal on Matrix Analysis and Applications , 20(2):303–
353, 1998. 1, 4
[9] William Fulton. Eigenvalues of sums of hermitian matrices.
S´eminaire Bourbaki , 40:255–269, 1998. 5
[10] Michael Garland and Paul S Heckbert. Surface simplification
using quadric error metrics. In SIGGRAPH ’97 , pages 209–
216, 1997. 4
[11] Israel M. Gelfand, Mikhail M. Kapranov, and Andrei V .
Zelevinsky. Discriminants, Resultants, and Multidimen-
sional Determinants . Birkh ¨auser Boston, 1994. 2
[12] Gene H Golub and Charles F Van Loan. Matrix computa-
tions . JHU press, 2013. 5
[13] P. Griffiths and J. Harris. Principles of Algebraic Geometry .
Wiley Classics Library. Wiley, 2014. 8
[14] J ¨urgen Gross and G ¨otz Trenkler. On the least squares dis-
tance between affine subspaces. Linear Algebra and its Ap-
plications , 237-238:269–276, 1996. 6
[15] Ga ¨el Guennebaud, Beno ˆıt Jacob, et al. Eigen v3.
http://eigen.tuxfamily.org, 2010. 6
[16] Richard Hartley and Andrew Zisserman. Multiple View Ge-
ometry in Computer Vision . Cambridge University Press,
USA, 2 edition, 2003. 1, 8
[17] Hermann Karcher. Riemannian center of mass and mollifier
smoothing. Communications on pure and applied mathemat-
ics, 30(5):509–541, 1977. 3
[18] Hermann Karcher. Riemannian center of mass and so called
karcher mean, 2014. 4
[19] Daniel A Klain and Gian-Carlo Rota. Introduction to geo-
metric probability . Cambridge University Press, 1997. 2
[20] Allen Knutson and Terence Tao. Honeycombs and sums of
hermitian matrices. Notices Amer. Math. Soc , 48(2), 2001. 5
[21] Vincent Lesueur and Vincent Nozick. Least square for
grassmann-cayley agelbra in homogeneous coordinates. In
Fay Huang and Akihiro Sugimoto, editors, Image andVideo Technology – PSIVT 2013 Workshops , pages 133–144,
Berlin, Heidelberg, 2014. Springer Berlin Heidelberg. 8
[22] Lek-Heng Lim, Ken Sze-Wai Wong, and Ke Ye. Numeri-
cal algorithms on the affine grassmannian. SIAM Journal on
Matrix Analysis and Applications , 40(2):371–393, 2019. 1,
2, 4, 6
[23] Lek-Heng Lim, Ken Sze-Wai Wong, and Ke Ye. The grass-
mannian of affine subspaces. Foundations of Computational
Mathematics , 21(2):537–574, 2021. 1, 2
[24] Parker C. Lusk, Devarth Parikh, and Jonathan P. How.
Graffmatch: Global matching of 3d lines and planes for wide
baseline lidar registration. IEEE Robotics and Automation
Letters , 8(2):632–639, 2023. 2, 8
[25] Tim Marrinan, P-A Absil, and Nicolas Gillis. On a minimum
enclosing ball of a collection of linear subspaces. Linear
Algebra and its Applications , 625:248–278, 2021. 8
[26] Ezra Miller and Bernd Sturmfels. Matrix schubert vari-
eties. In Combinatorial Commutative Algebra , pages 289–
310. Springer New York, New York, NY , 2005. 8
[27] Ezra Miller and Bernd Sturmfels. Pl ¨ucker coordinates.
InCombinatorial Commutative Algebra , pages 273–288.
Springer New York, New York, NY , 2005. 1, 2
[28] L. Mirsky. Symmetric gauge functions and unitarily invariant
norms. The Quarterly Journal of Mathematics , 11(1):50–59,
01 1960. 6
[29] Karl Pearson. On lines and planes of closest fit to systems of
points in space. The London, Edinburgh, and Dublin philo-
sophical magazine and journal of science , 2(11):559–572,
1901. 1
[30] Krishan Sharma and Renu Rameshan. Image set classifi-
cation using a distance-based kernel over affine grassmann
manifold. IEEE Transactions on Neural Networks and
Learning Systems , 32(3):1082–1095, 2021. 2
[31] Jorge Stolfi. Oriented projective geometry: a framework for
geometric computations . PhD thesis, Stanford University,
1995. 8
[32] Paul Tseng. Nearest q-flat to m points. Journal of Optimiza-
tion Theory and Applications , 105:249–252, 2000. 8
[33] Rene Vidal, Yi Ma, and Shankar Sastry. Generalized prin-
cipal component analysis (gpca). IEEE Trans. Pattern Anal.
Mach. Intell. , 27(12):1945–1959, 2005. 8
[34] Endre Weiszfeld and Frank Plastria. On the point for which
the sum of the distances to n given points is minimum. An-
nals of Operations Research , 167(1):7–41, 2009. 6
[35] Emo Welzl. Smallest enclosing disks (balls and ellipsoids).
InNew Results and New Trends in Computer Science: Graz,
Austria, June 20–21, 1991 Proceedings , pages 359–370.
Springer, 2005. 8
[36] Ke Ye and Lek-Heng Lim. Schubert varieties and distances
between subspaces of different dimensions. SIAM Journal on
Matrix Analysis and Applications , 37(3):1176–1197, 2016. 4
[37] Teng Zhang, Arthur Szlam, and Gilad Lerman. Median k-
flats for hybrid linear modeling with many outliers. In 12th
International Conference on Computer Vision Workshops,
ICCV Workshops , pages 234–241. IEEE, 2009. 8
[38] Tong Zhao, Laurent Bus ´e, David Cohen-Steiner, Tamy
Boubekeur, Jean-Marc Thiery, and Pierre Alliez. Variational
shape reconstruction via quadric error metrics. In ACM SIG-
GRAPH 2023 Conference Proc. , pages 1–10, 2023. 4
5447
