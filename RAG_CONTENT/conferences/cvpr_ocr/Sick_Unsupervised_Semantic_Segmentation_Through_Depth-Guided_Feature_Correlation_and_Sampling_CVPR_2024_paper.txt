Unsupervised Semantic Segmentation
Through Depth-Guided Feature Correlation and Sampling
Leon Sick
Ulm UniversityDominik Engel
Ulm UniversityPedro Hermosilla
TU ViennaTimo Ropinski
Ulm University
Abstract
Traditionally, training neural networks to perform se-
mantic segmentation requires expensive human-made an-
notations. But more recently, advances in the field of un-
supervised learning have made significant progress on this
issue and towards closing the gap to supervised algorithms.
To achieve this, semantic knowledge is distilled by learning
to correlate randomly sampled features from images across
an entire dataset. In this work, we build upon these ad-
vances by incorporating information about the structure of
the scene into the training process through the use of depth
information. We achieve this by (1) learning depth-feature
correlation by spatially correlating the feature maps with
the depth maps to induce knowledge about the structure of
the scene and (2) exploiting farthest-point sampling to more
effectively select relevant features by utilizing 3D sampling
techniques on depth information of the scene. Finally, we
demonstrate the effectiveness of our technical contributions
through extensive experimentation and present significant
improvements in performance across multiple benchmark
datasets.
1. Introduction
Semantic segmentation plays a critical role in many of
today’s vision systems in a multitude of domains. These
include, among others, autonomous driving [15], medical
applications [33, 39], and many more [9, 27, 41, 45]. Un-
til recently, the main body of research in this area was fo-
cused on supervised models that require a large amount
pixel-level annotations for training. Not only is sourcing
this image data often a labor intensive process, but also an-
notating the large datasets required for good performance
comes at a high price. Several benchmark datasets re-
port their annotation times. For example, the MS COCO
dataset [28] required more than 28K hours of human anno-
tation for around 164K images, and annotating a single im-
age in the Cityscapes dataset [11] took 1.5 hours on average.These costs have triggered the advent of unsupervised se-
mantic segmentation [10, 16, 20, 35], which aims to remove
the need for labeled training data in order to train segmen-
tation models. Recently, work by Hamilton et al. [16] has
accelerated the progress towards removing the need for la-
bels to achieve good results on semantic segmentation tasks.
Their model, STEGO, uses a DINO-pretrained [7] Vision
Transformer (ViT) [13] to extract features that are then dis-
tilled across the entire dataset to learn semantically rele-
vant features, using a contrastive learning approach. The
to-be-distilled features are sampled randomly from feature
maps produced from the same image, k-NN matched im-
ages as well as other negative images. Seong et al . [35]
build on this process by trying to identify features that are
most relevant to the model by discovering hidden positives.
Their work exposes an inefficiency of random sampling in
STEGO as hidden positives sampling leads to significant
improvements. However, both approaches only operate on
the pixel space and therefore fail to take into account the
spatial layout of the scene. Not only do we humans perceive
the world in 3D, but also previous work [5, 8, 18, 37, 38, 44]
has shown that supervised semantic segmentation can bene-
fit greatly from spatial information during training. Inspired
by these observations, we propose to incorporate spatial in-
formation in the form of depth maps into the STEGO train-
ing process. Depth is considered a product of vision and
does not provide a labeled training signal. To obtain depth
information for the benchmark image datasets in our evalua-
tions, we make use of an off-the-shelf zero-shot monocular
depth estimator to obtain spatial information of the scene.
This allows us to incorporate the depth information without
the need for human annotations or sensor ground truth.
With our method, DepthG , we propose to (1)guide the
model to learn a rough spatial layout of the scene, since
we hypothesize this will aid the network in differentiating
objects much better. We achieve this by extending the con-
trastive process to the spatial dimension: We do not limit
the model to learning only Feature-Feature Correlations, but
alsoDepth-Feature Correlations . Through this process, the
model is guided towards pulling apart the features with high
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
3637
Feature SpaceDepth SpaceGuideFigure 1. Guiding the feature space for unsupervised segmen-
tation with depth information. Our intuition behind the proposed
approach is simple: For locations in the 3D space with a low dis-
tance, we guide the model to map their features closer together.
Vice versa, the features are learned to be drawn apart in feature
space if their distance in the metric space is large.
distances in 3D space, as well as mapping them closer to-
gether if their distance is low depth space. Figure 1 visual-
izes this process.
With the information about the spatial layout of the scene
present, we furthermore propose to (2)spatially inform our
feature sampling process by utilizing Farthest-Point Sam-
pling (FPS) [14, 31] on the depth map, which equally sam-
ples scenes in 3D. We show these techniques in combination
make our approach to unsupervised segmentation highly ef-
fective, since in our evaluations across multiple benchmark
datasets, we demonstrate state-of-the-art performance. We
include a short video presenting our method as part of the
supplementary materials. To the best of our knowledge,
we are the first to propose a mechanism to incorporate 3D
knowledge of the scene into unsupervised learning for 2D
images without encoding depth maps as part of the network
input. This design aspect of our method alleviates the risk
of the model developing an input dependency. Therefore,
our approach does not rely on depth information during in-
ference and is not affected by availability or quality of such
depth information.2. Related Work
2.1. Unsupervised Semantic Segmentation
Recent works [10, 16, 20, 35] have attempted to tackle
semantic segmentation without the use of human annota-
tions. Ji et al . [20] propose IIC, a method that aims to
maximize the mutual information between different aug-
mented versions of an image. PiCIE, published by Cho et
al. [10], introduces an inductive bias based on the invariance
to photometric transformations and equivariance to geomet-
ric manipulations. DINO [7] often serves as a critical com-
ponent to unsupervised segmentation algorithms, since the
self-supervised pre-trained ViT can produce semantically
relevant features. Recent work by Seitzer et al. [34] builds
upon this ability by training a model with slot attention [30]
to reconstruct the feature maps produced by DINO from the
different slots. The features of their object-centric model
are clustered with k-means [29] where each slot is associ-
ated with a cluster. In their 2021 work, Hamilton et al. [16]
have also built upon DINO features by introducing a fea-
ture distillation process with features from the same image,
k-NN retrieved examples as well as random other images
from the dataset. Their learned representations are finally
clustered and refined with a CRF [22] for semantic segmen-
tation. While STEGO’s feature selection process is random,
Seong et al. [35] introduce a more effective sampling strat-
egy by discovering hidden positives. During training, they
form task-agnostic and task-specific feature pools. For an
anchor feature, they then compute the maximum similarity
to any of the pool features and sample locations in the image
with greater similarity than the determined value. A more
detailed introduction to STEGO is provided in Section 3.1.
2.2. Depth For Semantic Segmentation
Previous research [5, 17, 18, 38, 40, 43] has sought to in-
corporate depth for semantic segmentation in different set-
tings. Wang et al. [38] propose to use depth for adapting
segmentation models to new data domains. Their method
adds depth estimation as an auxiliary task to strengthen the
prediction of segmentation tasks. Furthermore, they ap-
proximate the pixel-wise adaption difficulty from source to
target domain through the use of depth decoders. Work by
Hoyer et al . [18] explores three further strategies of how
depth can be useful for segmentation. First, they propose
using a shared backbone to share learning features for seg-
mentation and self-supervised depth estimation, similar to
Wang et al. [38]. Second, they use depth maps to introduce
a data augmentation that is informed by the structure of the
scene. And lastly, they detail the integration of depth into
an active learning loop as part of a student-teacher setup.
Another work by Hou et al. [17] incorporates depth infor-
mation into a pre-training algorithm with the aim to learn
better representations for semantic segmentation. In their
3638
work, Mask3D, they propose to incorporate depth in the
form of a 3D prior by formulating a reconstruction task that
operates on masked RGB and depth patches, enabling them
to learn more useful features for semantic segmentation.
3. Method
In the following, we detail our proposed method for
guiding unsupervised segmentation with depth information.
An overview of our approach is presented in Figure 2.
3.1. Preliminary
Our approach builds upon work by Hamilton et al. [16].
In their work, each image is 5-cropped and k-NN corre-
spondences between these images are calculated using the
DINO ViT [7]. Generally, STEGO uses a feature extractor
F:R3×Hin×Win→RC×H×Wwith input image height Hin
and width Win, to calculate a feature map f∈RC×H×W
with height H, width Wand feature dimension Cfrom
the input image. These features are then further encoded
by a segmentation head S:RC×H×W→RQ×H×Wto
calculate the code space s∈RQ×H×Wwith code di-
mension Q. With the goal of forming compact clusters
and amplifying the correlation of the learned features, let
fi:=F(xi)andfj:=F(xj)be feature maps for a given
input pair of xiandxj, which are then used to calculate
si:=S(fi)andsj:=S(fj)from the segmentation head
S. In practice, STEGO samples N2vectors from the fea-
ture map during training. Hamilton et al. [16] introduce the
concept of constructing the feature correspondence tensor
F∈RH×W×H×Was follows:
Fhw,uv =fhw
i·fuv
j
∥fhw
i∥∥fuv
j∥(1)
where·denotes the dot product. Using the same formula we
obtainSusing si, sj. Consequently, the feature correlation
loss is defined as:
LCorr:=−X
hw,uv(Fhw,uv−b) max(Shw,uv ,0) (2)
where bis a scalar bias hyperparameter. Empirical evalua-
tions from STEGO have shown that applying spatial center-
ing to the feature correlation loss along with zero-clamping
further improves performance [16]. These correlations are
calculated for two crops from the same image ( Lself) and
one from a different but similar image, determined by the
k-NN correspondence pre-processing ( Lknn). Finally, nega-
tive images are sampled randomly ( Lrandom ). The final loss
is a weighted sum of the different losses where each of them
has their individual weight λi:
LSTEGO =λselfLself+λknnLknn+λrandomLrandom (3)After training, the inferred feature maps for a test image
are clustered using k-means and refined with a conditional
random field (CRF) [22].
3.2. Depth Map Generation
Since in many cases, depth information about the scene
is not readily available, we make use of recent progress
in the field of monocular depth estimation [1–3, 25, 32] to
obtain depth maps from RGB images. Recently, methods
from this field have made significant advances in zero-shot
depth estimation, i.e. predicting depth values for scenes
from data domains not seen during training. This property
makes them especially suitable for our method, since it en-
ables us to obtain high-quality depth predictions for a wide
variety of data domains without ever re-training the depth
network. This property also limits the computational cost
for our method. For our method, we experiment with dif-
ferent state-of-the-art monocular depth estimators, detailed
in Section 5, and found ZoeDepth [3] to perform best in our
evaluations. Given a cropped RGB image xi, we use the
monocular depth estimator Mtogether with average pool-
ing to predict depth di∈[0,1]H×Wat feature resolution:
di=pool(M(xi)) (4)
The average pooling operation is used to match the dimen-
sions of the feature map, which is required to sample non-
overlapping locations at the patch resolution.
3.3. Depth-Feature Correlation Loss
With our Depth-Feature Correlation loss, we aim to en-
force spatial consistency in the feature map by transferring
the distances from the depth map to the latent feature space.
In contrastive learning, the network is incentivized to de-
crease the distance in feature space for similar instances,
therefore learning to map their latent representations closer
together. Likewise, different instances are drawn further
apart in feature distance. We assume the same concept
to be true in 3D space: The spatial distance between two
points from the same depth plateau is smaller, while the
distance between a point in the foreground and one in the
background is larger. Since, in both spaces, the concept of
measuring difference is represented by the distance between
two points, we propose to align them through our concept
ofDepth-Feature Correlation : For large distances in the 3D
space, we encourage the network to produce vectors that are
further apart, and vice versa. With this, we induce the model
with knowledge about the spatial structure of the scene, en-
abling it to better differentiate between objects. To achieve
this we construct the depth correspondence tensor similar
to the feature correspondence from Equation 1. The depth
correspondence tensor Dis computed from the depths of
two different image crops as follows:
Dhw,uv =dhw
iduv
j, (5)
3639
XXXX
FPSSampleXXSampleXXXX
FPS
324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431CVPR#7164CVPR#7164CVPR 2024 Submission #7164. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.
XXXX
FPSSampleFPSXXSampleXXFFSSXXLSTEGO+DepthG
Figure 2.Overview of the DepthG training process.After 5-cropping the image, each crop is encoded by the DINO-pretrained ViTFtooutput a feature map. Using farthest-point-sampling (FPS), we sample the 3D space equally and convert the coordinates to select samplesin the feature map. The sampled features are further transformed by the segmentation headS. For both feature maps, the correlationtensor is computed. Following, we sample the depth map at the coordinates obtained by FPS and compute a correlation tensor in the samefashion. Finally, we compute our depth-feature correlation loss and combine it with the feature distillation loss from STEGO. We guidethe model to learn depth-feature correlation for crops of the same image, while the feature distallation loss is also applied to k-NN-selectedand random images.Letu=d(xi)andv=d(yi)be the depth maps ob-tained for two different crops. The depth maps representthe estimated depths at each pixel of the respective image.We construct the depth correspondence tensorD, deﬁnedas follows:Dhw,ij=uhwvij,(5)where(h, w)and(i, j)represent the pixel positions in thedepth mapsuandvrespectively. Together with the zeroclamping, our depth-feature correlation loss is deﬁned as:LDepthG:= Xhw,ij(Dhw,ij bDepthG) max(Shw,ij,0)(6)whereDhw,ijrepresents the depth correlation tensor,bDepthGis the bias for our loss term, andShw,ijrepresents the fea-ture correlation tensor computed from the output featuresof the segmentation headS. By also using zero-clamping,we limit erroneous learning signals that aim to draw apartinstances of the same class if they have large spatial differ-ences.With this, we extend the STEGO loss so it can be formu-lated as follows:LSTEGO+DepthG=LSTEGO+ DepthGLDepthG(7)with depth-feature correlation weight DepthG. By induc-ing depth knowledge during trainingwithoutencoding thedepth maps as part of the model input, our model can pre-dict on RGB images with its distilled knowledge and doesrely on depth to be available at test time.3.4. Depth-Guided Feature SamplingWe also aim to make the feature sampling process in-formed by the spatial laypout of the scene. To performsampling in the depth space, we transform the down-sampled depth mapd(xi)into a point cloud with points{p1,p2,. . . ,pn}. On this point cloud, we apply farthestpoint sampling (FPS), in an iterative fashion by alwaysselecting the next pointpijas the point with the maxi-mum distance in 3D space with respect to the rest of points{pi1,pi2,. . . ,pij 1}. After having sampledN2points, weend up with a set of samples{pi1,pi2,. . . ,piN2}which areconsequently converted two 2D sampling indices for thefeature mapsfandg. In contrast to the data-agnostic ran-dom sampling applied in STEGO, our feature selection pro-cess takes into account the geometry of the input scene andmore equally covers the spatial structure. This more equalsampling of depth space further increases the effectivenessof our depth feature correlation loss due to the increase di-versity in selected 3D locations.3.5. Guidance SchedulingWhile our depth-feature correlation loss is effective atenriching the model’s learning process with spatial infor-mation of the scene, we aim to alleviate the potential of itinterfering the learning of feature correlations during modeltraining. We hypothesize that our model most greatly bene-ﬁts from depth information towards the beginning of train-ing when its only knowledge is encoded in the featuresmaps output by the frozen ViT backbone. To give it a head4an active learning loop as part of a student-teacher setup.Another work by Houet al.[17] incorporates depth infor-mation into a pre-training algorithm with the aim to learnbetter representations for semantic segmentation. In theirwork, Mask3D, they propose to incorporate depth in theform of a 3D prior by formulating a reconstruction task thatoperates on masked RGB and depth patches, enabling themto learn more useful features for semantic segmentation.3. MethodIn the following, we detail our proposed method forguiding unsupervised segmentation with depth information.An overview of our approach is presented in Figure2.3.1. PreliminaryOur approach builds upon work by Hamiltonet al.[16].In their work, each image is 5-cropped and k-NN corre-spondences between these images are calculated using theDINO ViT [7]. Generally, STEGO uses a feature extractorF:R3⇥Hin⇥Win!RC⇥H⇥Wwith input image heightHinand widthWin, to calculate a feature mapf2RC⇥H⇥Wwith heightH, widthWand feature dimensionCfromthe input image. These features are then further encodedby a segmentation headS:RC⇥H⇥W!RQ⇥H⇥Wtocalculate the code spaces2RQ⇥H⇥Wwith code di-mensionQ. With the goal of forming compact clustersand amplifying the correlation of the learned features, letfi:=F(xi)andfj:=F(xj)be feature maps for a giveninput pair ofxiandxj, which are then used to calculatesi:=S(fi)andsj:=S(fj)from the segmentation headS. In practice, STEGO samplesN2vectors from the fea-ture map during training. Hamiltonet al.[16] introduce theconcept of constructing the feature correspondence tensorF2RH⇥W⇥H⇥Was follows:Fhw,uv=fhwi·fuvjkfhwikkfuvjk(1)where·denotes the dot product. Using the same formula weobtainSusingsi,sj. Consequently, the feature correlationloss is deﬁned as:LCorr:= Xhw,uv(Fhw,uv b) max(Shw,uv,0)(2)wherebis a scalar bias hyperparameter. Empirical evalua-tions from STEGO have shown that applying spatial center-ing to the feature correlation loss along with zero-clampingfurther improves performance [16]. These correlations arecalculated for two crops from the same image (Lself) andone from a different but similar image, determined by thek-NN correspondence pre-processing (Lknn). Finally, nega-tive images are sampled randomly (Lrandom). The ﬁnal lossis a weighted sum of the different losses where each of themhas their individual weight i:LSTEGO= selfLself+ knnLknn+ randomLrandom(3)After training, the inferred feature maps for a test imageare clustered using k-means and reﬁned with a conditionalrandom ﬁeld (CRF) [22].3.2. Depth Map GenerationSince in many cases, depth information about the sceneis not readily available, we make use of recent progressin the ﬁeld of monocular depth estimation [1–3,25,32] toobtain depth maps from RGB images. Recently, methodsfrom this ﬁeld have made signiﬁcant advances in zero-shotdepth estimation, i.e. predicting depth values for scenesfrom data domains not seen during training. This propertymakes them especially suitable for our method, since it en-ables us to obtain high-quality depth predictions for a widevariety of data domains without ever re-training the depthnetwork. This property also limits the computational costfor our method. For our method, we experiment with dif-ferent state-of-the-art monocular depth estimators, detailedin Section5, and found ZoeDepth [3] to perform best in ourevaluations. Given a cropped RGB imagexi, we use themonocular depth estimatorMtogether with average pool-ing to predict depthdi2[0,1]H⇥Wat feature resolution:di=pool(M(xi))(4)The average pooling operation is used to match the dimen-sions of the feature map, which is required to sample non-overlapping locations at the patch resolution.3.3. Depth-Feature Correlation LossWith ourDepth-Feature Correlationloss, we aim to en-force spatial consistency in the feature map by transferringthe distances from the depth map to the latent feature space.In contrastive learning, the network is incentivized to de-crease the distance in feature space for similar instances,therefore learning to map their latent representations closertogether. Likewise, different instances are drawn furtherapart in feature distance. We assume the same conceptto be true in 3D space: The spatial distance between twopoints from the same depth plateau is smaller, while thedistance between a point in the foreground and one in thebackground is larger. Since, in both spaces, the concept ofmeasuring difference is represented by the distance betweentwo points, we propose to align them through our conceptofDepth-Feature Correlation: For large distances in the 3Dspace, we encourage the network to produce vectors that arefurther apart, and vice versa. With this, we induce the modelwith knowledge about the spatial structure of the scene, en-abling it to better differentiate between objects. To achievean active learning loop as part of a student-teacher setup.Another work by Houet al.[17] incorporates depth infor-mation into a pre-training algorithm with the aim to learnbetter representations for semantic segmentation. In theirwork, Mask3D, they propose to incorporate depth in theform of a 3D prior by formulating a reconstruction task thatoperates on masked RGB and depth patches, enabling themto learn more useful features for semantic segmentation.3. MethodIn the following, we detail our proposed method forguiding unsupervised segmentation with depth information.An overview of our approach is presented in Figure2.3.1. PreliminaryOur approach builds upon work by Hamiltonet al.[16].In their work, each image is 5-cropped and k-NN corre-spondences between these images are calculated using theDINO ViT [7]. Generally, STEGO uses a feature extractorF:R3⇥Hin⇥Win!RC⇥H⇥Wwith input image heightHinand widthWin, to calculate a feature mapf2RC⇥H⇥Wwith heightH, widthWand feature dimensionCfromthe input image. These features are then further encodedby a segmentation headS:RC⇥H⇥W!RQ⇥H⇥Wtocalculate the code spaces2RQ⇥H⇥Wwith code di-mensionQ. With the goal of forming compact clustersand amplifying the correlation of the learned features, letfi:=F(xi)andfj:=F(xj)be feature maps for a giveninput pair ofxiandxj, which are then used to calculatesi:=S(fi)andsj:=S(fj)from the segmentation headS. In practice, STEGO samplesN2vectors from the fea-ture map during training. Hamiltonet al.[16] introduce theconcept of constructing the feature correspondence tensorF2RH⇥W⇥H⇥Was follows:Fhw,uv=fhwi·fuvjkfhwikkfuvjk(1)where·denotes the dot product. Using the same formula weobtainSusingsi,sj. Consequently, the feature correlationloss is deﬁned as:LCorr:= Xhw,uv(Fhw,uv b) max(Shw,uv,0)(2)wherebis a scalar bias hyperparameter. Empirical evalua-tions from STEGO have shown that applying spatial center-ing to the feature correlation loss along with zero-clampingfurther improves performance [16]. These correlations arecalculated for two crops from the same image (Lself) andone from a different but similar image, determined by thek-NN correspondence pre-processing (Lknn). Finally, nega-tive images are sampled randomly (Lrandom). The ﬁnal lossis a weighted sum of the different losses where each of themhas their individual weight i:LSTEGO= selfLself+ knnLknn+ randomLrandom(3)After training, the inferred feature maps for a test imageare clustered using k-means and reﬁned with a conditionalrandom ﬁeld (CRF) [22].3.2. Depth Map GenerationSince in many cases, depth information about the sceneis not readily available, we make use of recent progressin the ﬁeld of monocular depth estimation [1–3,25,32] toobtain depth maps from RGB images. Recently, methodsfrom this ﬁeld have made signiﬁcant advances in zero-shotdepth estimation, i.e. predicting depth values for scenesfrom data domains not seen during training. This propertymakes them especially suitable for our method, since it en-ables us to obtain high-quality depth predictions for a widevariety of data domains without ever re-training the depthnetwork. This property also limits the computational costfor our method. For our method, we experiment with dif-ferent state-of-the-art monocular depth estimators, detailedin Section5, and found ZoeDepth [3] to perform best in ourevaluations. Given a cropped RGB imagexi, we use themonocular depth estimatorMtogether with average pool-ing to predict depthdi2[0,1]H⇥Wat feature resolution:di=pool(M(xi))(4)The average pooling operation is used to match the dimen-sions of the feature map, which is required to sample non-overlapping locations at the patch resolution.3.3. Depth-Feature Correlation LossWith ourDepth-Feature Correlationloss, we aim to en-force spatial consistency in the feature map by transferringthe distances from the depth map to the latent feature space.In contrastive learning, the network is incentivized to de-crease the distance in feature space for similar instances,therefore learning to map their latent representations closertogether. Likewise, different instances are drawn furtherapart in feature distance. We assume the same conceptto be true in 3D space: The spatial distance between twopoints from the same depth plateau is smaller, while thedistance between a point in the foreground and one in thebackground is larger. Since, in both spaces, the concept ofmeasuring difference is represented by the distance betweentwo points, we propose to align them through our conceptofDepth-Feature Correlation: For large distances in the 3Dspace, we encourage the network to produce vectors that arefurther apart, and vice versa. With this, we induce the modelwith knowledge about the spatial structure of the scene, en-abling it to better differentiate between objects. To achievean active learning loop as part of a student-teacher setup.Another work by Houet al.[17] incorporates depth infor-mation into a pre-training algorithm with the aim to learnbetter representations for semantic segmentation. In theirwork, Mask3D, they propose to incorporate depth in theform of a 3D prior by formulating a reconstruction task thatoperates on masked RGB and depth patches, enabling themto learn more useful features for semantic segmentation.3. MethodIn the following, we detail our proposed method forguiding unsupervised segmentation with depth information.An overview of our approach is presented in Figure2.3.1. PreliminaryOur approach builds upon work by Hamiltonet al.[16].In their work, each image is 5-cropped and k-NN corre-spondences between these images are calculated using theDINO ViT [7]. Generally, STEGO uses a feature extractorF:R3⇥Hin⇥Win!RC⇥H⇥Wwith input image heightHinand widthWin, to calculate a feature mapf2RC⇥H⇥Wwith heightH, widthWand feature dimensionCfromthe input image. These features are then further encodedby a segmentation headS:RC⇥H⇥W!RQ⇥H⇥Wtocalculate the code spaces2RQ⇥H⇥Wwith code di-mensionQ. With the goal of forming compact clustersand amplifying the correlation of the learned features, letfi:=F(xi)andfj:=F(xj)be feature maps for a giveninput pair ofxiandxj, which are then used to calculatesi:=S(fi)andsj:=S(fj)from the segmentation headS. In practice, STEGO samplesN2vectors from the fea-ture map during training. Hamiltonet al.[16] introduce theconcept of constructing the feature correspondence tensorF2RH⇥W⇥H⇥Was follows:Fhw,uv=fhwi·fuvjkfhwikkfuvjk(1)where·denotes the dot product. Using the same formula weobtainSusingsi,sj. Consequently, the feature correlationloss is deﬁned as:LCorr:= Xhw,uv(Fhw,uv b) max(Shw,uv,0)(2)wherebis a scalar bias hyperparameter. Empirical evalua-tions from STEGO have shown that applying spatial center-ing to the feature correlation loss along with zero-clampingfurther improves performance [16]. These correlations arecalculated for two crops from the same image (Lself) andone from a different but similar image, determined by thek-NN correspondence pre-processing (Lknn). Finally, nega-tive images are sampled randomly (Lrandom). The ﬁnal lossis a weighted sum of the different losses where each of themhas their individual weight i:LSTEGO= selfLself+ knnLknn+ randomLrandom(3)After training, the inferred feature maps for a test imageare clustered using k-means and reﬁned with a conditionalrandom ﬁeld (CRF) [22].3.2. Depth Map GenerationSince in many cases, depth information about the sceneis not readily available, we make use of recent progressin the ﬁeld of monocular depth estimation [1–3,25,32] toobtain depth maps from RGB images. Recently, methodsfrom this ﬁeld have made signiﬁcant advances in zero-shotdepth estimation, i.e. predicting depth values for scenesfrom data domains not seen during training. This propertymakes them especially suitable for our method, since it en-ables us to obtain high-quality depth predictions for a widevariety of data domains without ever re-training the depthnetwork. This property also limits the computational costfor our method. For our method, we experiment with dif-ferent state-of-the-art monocular depth estimators, detailedin Section5, and found ZoeDepth [3] to perform best in ourevaluations. Given a cropped RGB imagexi, we use themonocular depth estimatorMtogether with average pool-ing to predict depthdi2[0,1]H⇥Wat feature resolution:di=pool(M(xi))(4)The average pooling operation is used to match the dimen-sions of the feature map, which is required to sample non-overlapping locations at the patch resolution.3.3. Depth-Feature Correlation LossWith ourDepth-Feature Correlationloss, we aim to en-force spatial consistency in the feature map by transferringthe distances from the depth map to the latent feature space.In contrastive learning, the network is incentivized to de-crease the distance in feature space for similar instances,therefore learning to map their latent representations closertogether. Likewise, different instances are drawn furtherapart in feature distance. We assume the same conceptto be true in 3D space: The spatial distance between twopoints from the same depth plateau is smaller, while thedistance between a point in the foreground and one in thebackground is larger. Since, in both spaces, the concept ofmeasuring difference is represented by the distance betweentwo points, we propose to align them through our conceptofDepth-Feature Correlation: For large distances in the 3Dspace, we encourage the network to produce vectors that arefurther apart, and vice versa. With this, we induce the modelwith knowledge about the spatial structure of the scene, en-abling it to better differentiate between objects. To achievean active learning loop as part of a student-teacher setup.Another work by Houet al.[17] incorporates depth infor-mation into a pre-training algorithm with the aim to learnbetter representations for semantic segmentation. In theirwork, Mask3D, they propose to incorporate depth in theform of a 3D prior by formulating a reconstruction task thatoperates on masked RGB and depth patches, enabling themto learn more useful features for semantic segmentation.3. MethodIn the following, we detail our proposed method forguiding unsupervised segmentation with depth information.An overview of our approach is presented in Figure2.3.1. PreliminaryOur approach builds upon work by Hamiltonet al.[16].In their work, each image is 5-cropped and k-NN corre-spondences between these images are calculated using theDINO ViT [7]. Generally, STEGO uses a feature extractorF:R3⇥Hin⇥Win!RC⇥H⇥Wwith input image heightHinand widthWin, to calculate a feature mapf2RC⇥H⇥Wwith heightH, widthWand feature dimensionCfromthe input image. These features are then further encodedby a segmentation headS:RC⇥H⇥W!RQ⇥H⇥Wtocalculate the code spaces2RQ⇥H⇥Wwith code di-mensionQ. With the goal of forming compact clustersand amplifying the correlation of the learned features, letfi:=F(xi)andfj:=F(xj)be feature maps for a giveninput pair ofxiandxj, which are then used to calculatesi:=S(fi)andsj:=S(fj)from the segmentation headS. In practice, STEGO samplesN2vectors from the fea-ture map during training. Hamiltonet al.[16] introduce theconcept of constructing the feature correspondence tensorF2RH⇥W⇥H⇥Was follows:Fhw,uv=fhwi·fuvjkfhwikkfuvjk(1)where·denotes the dot product. Using the same formula weobtainSusingsi,sj. Consequently, the feature correlationloss is deﬁned as:LCorr:= Xhw,uv(Fhw,uv b) max(Shw,uv,0)(2)wherebis a scalar bias hyperparameter. Empirical evalua-tions from STEGO have shown that applying spatial center-ing to the feature correlation loss along with zero-clampingfurther improves performance [16]. These correlations arecalculated for two crops from the same image (Lself) andone from a different but similar image, determined by thek-NN correspondence pre-processing (Lknn). Finally, nega-tive images are sampled randomly (Lrandom). The ﬁnal lossis a weighted sum of the different losses where each of themhas their individual weight i:LSTEGO= selfLself+ knnLknn+ randomLrandom(3)After training, the inferred feature maps for a test imageare clustered using k-means and reﬁned with a conditionalrandom ﬁeld (CRF) [22].3.2. Depth Map GenerationSince in many cases, depth information about the sceneis not readily available, we make use of recent progressin the ﬁeld of monocular depth estimation [1–3,25,32] toobtain depth maps from RGB images. Recently, methodsfrom this ﬁeld have made signiﬁcant advances in zero-shotdepth estimation, i.e. predicting depth values for scenesfrom data domains not seen during training. This propertymakes them especially suitable for our method, since it en-ables us to obtain high-quality depth predictions for a widevariety of data domains without ever re-training the depthnetwork. This property also limits the computational costfor our method. For our method, we experiment with dif-ferent state-of-the-art monocular depth estimators, detailedin Section5, and found ZoeDepth [3] to perform best in ourevaluations. Given a cropped RGB imagexi, we use themonocular depth estimatorMtogether with average pool-ing to predict depthdi2[0,1]H⇥Wat feature resolution:di=pool(M(xi))(4)The average pooling operation is used to match the dimen-sions of the feature map, which is required to sample non-overlapping locations at the patch resolution.3.3. Depth-Feature Correlation LossWith ourDepth-Feature Correlationloss, we aim to en-force spatial consistency in the feature map by transferringthe distances from the depth map to the latent feature space.In contrastive learning, the network is incentivized to de-crease the distance in feature space for similar instances,therefore learning to map their latent representations closertogether. Likewise, different instances are drawn furtherapart in feature distance. We assume the same conceptto be true in 3D space: The spatial distance between twopoints from the same depth plateau is smaller, while thedistance between a point in the foreground and one in thebackground is larger. Since, in both spaces, the concept ofmeasuring difference is represented by the distance betweentwo points, we propose to align them through our conceptofDepth-Feature Correlation: For large distances in the 3Dspace, we encourage the network to produce vectors that arefurther apart, and vice versa. With this, we induce the modelwith knowledge about the spatial structure of the scene, en-abling it to better differentiate between objects. To achievean active learning loop as part of a student-teacher setup.Another work by Houet al.[17] incorporates depth infor-mation into a pre-training algorithm with the aim to learnbetter representations for semantic segmentation. In theirwork, Mask3D, they propose to incorporate depth in theform of a 3D prior by formulating a reconstruction task thatoperates on masked RGB and depth patches, enabling themto learn more useful features for semantic segmentation.3. MethodIn the following, we detail our proposed method forguiding unsupervised segmentation with depth information.An overview of our approach is presented in Figure2.3.1. PreliminaryOur approach builds upon work by Hamiltonet al.[16].In their work, each image is 5-cropped and k-NN corre-spondences between these images are calculated using theDINO ViT [7]. Generally, STEGO uses a feature extractorF:R3⇥Hin⇥Win!RC⇥H⇥Wwith input image heightHinand widthWin, to calculate a feature mapf2RC⇥H⇥Wwith heightH, widthWand feature dimensionCfromthe input image. These features are then further encodedby a segmentation headS:RC⇥H⇥W!RQ⇥H⇥Wtocalculate the code spaces2RQ⇥H⇥Wwith code di-mensionQ. With the goal of forming compact clustersand amplifying the correlation of the learned features, letfi:=F(xi)andfj:=F(xj)be feature maps for a giveninput pair ofxiandxj, which are then used to calculatesi:=S(fi)andsj:=S(fj)from the segmentation headS. In practice, STEGO samplesN2vectors from the fea-ture map during training. Hamiltonet al.[16] introduce theconcept of constructing the feature correspondence tensorF2RH⇥W⇥H⇥Was follows:Fhw,uv=fhwi·fuvjkfhwikkfuvjk(1)where·denotes the dot product. Using the same formula weobtainSusingsi,sj. Consequently, the feature correlationloss is deﬁned as:LCorr:= Xhw,uv(Fhw,uv b) max(Shw,uv,0)(2)wherebis a scalar bias hyperparameter. Empirical evalua-tions from STEGO have shown that applying spatial center-ing to the feature correlation loss along with zero-clampingfurther improves performance [16]. These correlations arecalculated for two crops from the same image (Lself) andone from a different but similar image, determined by thek-NN correspondence pre-processing (Lknn). Finally, nega-tive images are sampled randomly (Lrandom). The ﬁnal lossis a weighted sum of the different losses where each of themhas their individual weight i:LSTEGO= selfLself+ knnLknn+ randomLrandom(3)After training, the inferred feature maps for a test imageare clustered using k-means and reﬁned with a conditionalrandom ﬁeld (CRF) [22].3.2. Depth Map GenerationSince in many cases, depth information about the sceneis not readily available, we make use of recent progressin the ﬁeld of monocular depth estimation [1–3,25,32] toobtain depth maps from RGB images. Recently, methodsfrom this ﬁeld have made signiﬁcant advances in zero-shotdepth estimation, i.e. predicting depth values for scenesfrom data domains not seen during training. This propertymakes them especially suitable for our method, since it en-ables us to obtain high-quality depth predictions for a widevariety of data domains without ever re-training the depthnetwork. This property also limits the computational costfor our method. For our method, we experiment with dif-ferent state-of-the-art monocular depth estimators, detailedin Section5, and found ZoeDepth [3] to perform best in ourevaluations. Given a cropped RGB imagexi, we use themonocular depth estimatorMtogether with average pool-ing to predict depthdi2[0,1]H⇥Wat feature resolution:di=pool(M(xi))(4)The average pooling operation is used to match the dimen-sions of the feature map, which is required to sample non-overlapping locations at the patch resolution.3.3. Depth-Feature Correlation LossWith ourDepth-Feature Correlationloss, we aim to en-force spatial consistency in the feature map by transferringthe distances from the depth map to the latent feature space.In contrastive learning, the network is incentivized to de-crease the distance in feature space for similar instances,therefore learning to map their latent representations closertogether. Likewise, different instances are drawn furtherapart in feature distance. We assume the same conceptto be true in 3D space: The spatial distance between twopoints from the same depth plateau is smaller, while thedistance between a point in the foreground and one in thebackground is larger. Since, in both spaces, the concept ofmeasuring difference is represented by the distance betweentwo points, we propose to align them through our conceptofDepth-Feature Correlation: For large distances in the 3Dspace, we encourage the network to produce vectors that arefurther apart, and vice versa. With this, we induce the modelwith knowledge about the spatial structure of the scene, en-abling it to better differentiate between objects. To achievean active learning loop as part of a student-teacher setup.Another work by Houet al.[17] incorporates depth infor-mation into a pre-training algorithm with the aim to learnbetter representations for semantic segmentation. In theirwork, Mask3D, they propose to incorporate depth in theform of a 3D prior by formulating a reconstruction task thatoperates on masked RGB and depth patches, enabling themto learn more useful features for semantic segmentation.3. MethodIn the following, we detail our proposed method forguiding unsupervised segmentation with depth information.An overview of our approach is presented in Figure2.3.1. PreliminaryOur approach builds upon work by Hamiltonet al.[16].In their work, each image is 5-cropped and k-NN corre-spondences between these images are calculated using theDINO ViT [7]. Generally, STEGO uses a feature extractorF:R3⇥Hin⇥Win!RC⇥H⇥Wwith input image heightHinand widthWin, to calculate a feature mapf2RC⇥H⇥Wwith heightH, widthWand feature dimensionCfromthe input image. These features are then further encodedby a segmentation headS:RC⇥H⇥W!RQ⇥H⇥Wtocalculate the code spaces2RQ⇥H⇥Wwith code di-mensionQ. With the goal of forming compact clustersand amplifying the correlation of the learned features, letfi:=F(xi)andfj:=F(xj)be feature maps for a giveninput pair ofxiandxj, which are then used to calculatesi:=S(fi)andsj:=S(fj)from the segmentation headS. In practice, STEGO samplesN2vectors from the fea-ture map during training. Hamiltonet al.[16] introduce theconcept of constructing the feature correspondence tensorF2RH⇥W⇥H⇥Was follows:Fhw,uv=fhwi·fuvjkfhwikkfuvjk(1)where·denotes the dot product. Using the same formula weobtainSusingsi,sj. Consequently, the feature correlationloss is deﬁned as:LCorr:= Xhw,uv(Fhw,uv b) max(Shw,uv,0)(2)wherebis a scalar bias hyperparameter. Empirical evalua-tions from STEGO have shown that applying spatial center-ing to the feature correlation loss along with zero-clampingfurther improves performance [16]. These correlations arecalculated for two crops from the same image (Lself) andone from a different but similar image, determined by thek-NN correspondence pre-processing (Lknn). Finally, nega-tive images are sampled randomly (Lrandom). The ﬁnal lossis a weighted sum of the different losses where each of themhas their individual weight i:LSTEGO= selfLself+ knnLknn+ randomLrandom(3)After training, the inferred feature maps for a test imageare clustered using k-means and reﬁned with a conditionalrandom ﬁeld (CRF) [22].3.2. Depth Map GenerationSince in many cases, depth information about the sceneis not readily available, we make use of recent progressin the ﬁeld of monocular depth estimation [1–3,25,32] toobtain depth maps from RGB images. Recently, methodsfrom this ﬁeld have made signiﬁcant advances in zero-shotdepth estimation, i.e. predicting depth values for scenesfrom data domains not seen during training. This propertymakes them especially suitable for our method, since it en-ables us to obtain high-quality depth predictions for a widevariety of data domains without ever re-training the depthnetwork. This property also limits the computational costfor our method. For our method, we experiment with dif-ferent state-of-the-art monocular depth estimators, detailedin Section5, and found ZoeDepth [3] to perform best in ourevaluations. Given a cropped RGB imagexi, we use themonocular depth estimatorMtogether with average pool-ing to predict depthdi2[0,1]H⇥Wat feature resolution:di=pool(M(xi))(4)The average pooling operation is used to match the dimen-sions of the feature map, which is required to sample non-overlapping locations at the patch resolution.3.3. Depth-Feature Correlation LossWith ourDepth-Feature Correlationloss, we aim to en-force spatial consistency in the feature map by transferringthe distances from the depth map to the latent feature space.In contrastive learning, the network is incentivized to de-crease the distance in feature space for similar instances,therefore learning to map their latent representations closertogether. Likewise, different instances are drawn furtherapart in feature distance. We assume the same conceptto be true in 3D space: The spatial distance between twopoints from the same depth plateau is smaller, while thedistance between a point in the foreground and one in thebackground is larger. Since, in both spaces, the concept ofmeasuring difference is represented by the distance betweentwo points, we propose to align them through our conceptofDepth-Feature Correlation: For large distances in the 3Dspace, we encourage the network to produce vectors that arefurther apart, and vice versa. With this, we induce the modelwith knowledge about the spatial structure of the scene, en-abling it to better differentiate between objects. To achievean active learning loop as part of a student-teacher setup.Another work by Houet al.[17] incorporates depth infor-mation into a pre-training algorithm with the aim to learnbetter representations for semantic segmentation. In theirwork, Mask3D, they propose to incorporate depth in theform of a 3D prior by formulating a reconstruction task thatoperates on masked RGB and depth patches, enabling themto learn more useful features for semantic segmentation.3. MethodIn the following, we detail our proposed method forguiding unsupervised segmentation with depth information.An overview of our approach is presented in Figure2.3.1. PreliminaryOur approach builds upon work by Hamiltonet al.[16].In their work, each image is 5-cropped and k-NN corre-spondences between these images are calculated using theDINO ViT [7]. Generally, STEGO uses a feature extractorF:R3⇥Hin⇥Win!RC⇥H⇥Wwith input image heightHinand widthWin, to calculate a feature mapf2RC⇥H⇥Wwith heightH, widthWand feature dimensionCfromthe input image. These features are then further encodedby a segmentation headS:RC⇥H⇥W!RQ⇥H⇥Wtocalculate the code spaces2RQ⇥H⇥Wwith code di-mensionQ. With the goal of forming compact clustersand amplifying the correlation of the learned features, letfi:=F(xi)andfj:=F(xj)be feature maps for a giveninput pair ofxiandxj, which are then used to calculatesi:=S(fi)andsj:=S(fj)from the segmentation headS. In practice, STEGO samplesN2vectors from the fea-ture map during training. Hamiltonet al.[16] introduce theconcept of constructing the feature correspondence tensorF2RH⇥W⇥H⇥Was follows:Fhw,uv=fhwi·fuvjkfhwikkfuvjk(1)where·denotes the dot product. Using the same formula weobtainSusingsi,sj. Consequently, the feature correlationloss is deﬁned as:LCorr:= Xhw,uv(Fhw,uv b) max(Shw,uv,0)(2)wherebis a scalar bias hyperparameter. Empirical evalua-tions from STEGO have shown that applying spatial center-ing to the feature correlation loss along with zero-clampingfurther improves performance [16]. These correlations arecalculated for two crops from the same image (Lself) andone from a different but similar image, determined by thek-NN correspondence pre-processing (Lknn). Finally, nega-tive images are sampled randomly (Lrandom). The ﬁnal lossis a weighted sum of the different losses where each of themhas their individual weight i:LSTEGO= selfLself+ knnLknn+ randomLrandom(3)After training, the inferred feature maps for a test imageare clustered using k-means and reﬁned with a conditionalrandom ﬁeld (CRF) [22].3.2. Depth Map GenerationSince in many cases, depth information about the sceneis not readily available, we make use of recent progressin the ﬁeld of monocular depth estimation [1–3,25,32] toobtain depth maps from RGB images. Recently, methodsfrom this ﬁeld have made signiﬁcant advances in zero-shotdepth estimation, i.e. predicting depth values for scenesfrom data domains not seen during training. This propertymakes them especially suitable for our method, since it en-ables us to obtain high-quality depth predictions for a widevariety of data domains without ever re-training the depthnetwork. This property also limits the computational costfor our method. For our method, we experiment with dif-ferent state-of-the-art monocular depth estimators, detailedin Section5, and found ZoeDepth [3] to perform best in ourevaluations. Given a cropped RGB imagexi, we use themonocular depth estimatorMtogether with average pool-ing to predict depthdi2[0,1]H⇥Wat feature resolution:di=pool(M(xi))(4)The average pooling operation is used to match the dimen-sions of the feature map, which is required to sample non-overlapping locations at the patch resolution.3.3. Depth-Feature Correlation LossWith ourDepth-Feature Correlationloss, we aim to en-force spatial consistency in the feature map by transferringthe distances from the depth map to the latent feature space.In contrastive learning, the network is incentivized to de-crease the distance in feature space for similar instances,therefore learning to map their latent representations closertogether. Likewise, different instances are drawn furtherapart in feature distance. We assume the same conceptto be true in 3D space: The spatial distance between twopoints from the same depth plateau is smaller, while thedistance between a point in the foreground and one in thebackground is larger. Since, in both spaces, the concept ofmeasuring difference is represented by the distance betweentwo points, we propose to align them through our conceptofDepth-Feature Correlation: For large distances in the 3Dspace, we encourage the network to produce vectors that arefurther apart, and vice versa. With this, we induce the modelwith knowledge about the spatial structure of the scene, en-abling it to better differentiate between objects. To achievean active learning loop as part of a student-teacher setup.Another work by Houet al.[17] incorporates depth infor-mation into a pre-training algorithm with the aim to learnbetter representations for semantic segmentation. In theirwork, Mask3D, they propose to incorporate depth in theform of a 3D prior by formulating a reconstruction task thatoperates on masked RGB and depth patches, enabling themto learn more useful features for semantic segmentation.3. MethodIn the following, we detail our proposed method forguiding unsupervised segmentation with depth information.An overview of our approach is presented in Figure2.3.1. PreliminaryOur approach builds upon work by Hamiltonet al.[16].In their work, each image is 5-cropped and k-NN corre-spondences between these images are calculated using theDINO ViT [7]. Generally, STEGO uses a feature extractorF:R3⇥Hin⇥Win!RC⇥H⇥Wwith input image heightHinand widthWin, to calculate a feature mapf2RC⇥H⇥Wwith heightH, widthWand feature dimensionCfromthe input image. These features are then further encodedby a segmentation headS:RC⇥H⇥W!RQ⇥H⇥Wtocalculate the code spaces2RQ⇥H⇥Wwith code di-mensionQ. With the goal of forming compact clustersand amplifying the correlation of the learned features, letfi:=F(xi)andfj:=F(xj)be feature maps for a giveninput pair ofxiandxj, which are then used to calculatesi:=S(fi)andsj:=S(fj)from the segmentation headS. In practice, STEGO samplesN2vectors from the fea-ture map during training. Hamiltonet al.[16] introduce theconcept of constructing the feature correspondence tensorF2RH⇥W⇥H⇥Was follows:Fhw,uv=fhwi·fuvjkfhwikkfuvjk(1)where·denotes the dot product. Using the same formula weobtainSusingsi,sj. Consequently, the feature correlationloss is deﬁned as:LCorr:= Xhw,uv(Fhw,uv b) max(Shw,uv,0)(2)wherebis a scalar bias hyperparameter. Empirical evalua-tions from STEGO have shown that applying spatial center-ing to the feature correlation loss along with zero-clampingfurther improves performance [16]. These correlations arecalculated for two crops from the same image (Lself) andone from a different but similar image, determined by thek-NN correspondence pre-processing (Lknn). Finally, nega-tive images are sampled randomly (Lrandom). The ﬁnal lossis a weighted sum of the different losses where each of themhas their individual weight i:LSTEGO= selfLself+ knnLknn+ randomLrandom(3)After training, the inferred feature maps for a test imageare clustered using k-means and reﬁned with a conditionalrandom ﬁeld (CRF) [22].3.2. Depth Map GenerationSince in many cases, depth information about the sceneis not readily available, we make use of recent progressin the ﬁeld of monocular depth estimation [1–3,25,32] toobtain depth maps from RGB images. Recently, methodsfrom this ﬁeld have made signiﬁcant advances in zero-shotdepth estimation, i.e. predicting depth values for scenesfrom data domains not seen during training. This propertymakes them especially suitable for our method, since it en-ables us to obtain high-quality depth predictions for a widevariety of data domains without ever re-training the depthnetwork. This property also limits the computational costfor our method. For our method, we experiment with dif-ferent state-of-the-art monocular depth estimators, detailedin Section5, and found ZoeDepth [3] to perform best in ourevaluations. Given a cropped RGB imagexi, we use themonocular depth estimatorMtogether with average pool-ing to predict depthdi2[0,1]H⇥Wat feature resolution:di=pool(M(xi))(4)The average pooling operation is used to match the dimen-sions of the feature map, which is required to sample non-overlapping locations at the patch resolution.3.3. Depth-Feature Correlation LossWith ourDepth-Feature Correlationloss, we aim to en-force spatial consistency in the feature map by transferringthe distances from the depth map to the latent feature space.In contrastive learning, the network is incentivized to de-crease the distance in feature space for similar instances,therefore learning to map their latent representations closertogether. Likewise, different instances are drawn furtherapart in feature distance. We assume the same conceptto be true in 3D space: The spatial distance between twopoints from the same depth plateau is smaller, while thedistance between a point in the foreground and one in thebackground is larger. Since, in both spaces, the concept ofmeasuring difference is represented by the distance betweentwo points, we propose to align them through our conceptofDepth-Feature Correlation: For large distances in the 3Dspace, we encourage the network to produce vectors that arefurther apart, and vice versa. With this, we induce the modelwith knowledge about the spatial structure of the scene, en-abling it to better differentiate between objects. To achieve
XXXX
FPSSampleXXSampleXXXXFPS
324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431CVPR#7164CVPR#7164CVPR 2024 Submission #7164. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.
XXXX
FPSSampleFPSXXSampleXXFFSSXXLSTEGO+DepthG
Figure 2.Overview of the DepthG training process.After 5-cropping the image, each crop is encoded by the DINO-pretrained ViTFtooutput a feature map. Using farthest-point-sampling (FPS), we sample the 3D space equally and convert the coordinates to select samplesin the feature map. The sampled features are further transformed by the segmentation headS. For both feature maps, the correlationtensor is computed. Following, we sample the depth map at the coordinates obtained by FPS and compute a correlation tensor in the samefashion. Finally, we compute our depth-feature correlation loss and combine it with the feature distillation loss from STEGO. We guidethe model to learn depth-feature correlation for crops of the same image, while the feature distallation loss is also applied to k-NN-selectedand random images.Letu=d(xi)andv=d(yi)be the depth maps ob-tained for two different crops. The depth maps representthe estimated depths at each pixel of the respective image.We construct the depth correspondence tensorD, deﬁnedas follows:Dhw,ij=uhwvij,(5)where(h, w)and(i, j)represent the pixel positions in thedepth mapsuandvrespectively. Together with the zeroclamping, our depth-feature correlation loss is deﬁned as:LDepthG:= Xhw,ij(Dhw,ij bDepthG) max(Shw,ij,0)(6)whereDhw,ijrepresents the depth correlation tensor,bDepthGis the bias for our loss term, andShw,ijrepresents the fea-ture correlation tensor computed from the output featuresof the segmentation headS. By also using zero-clamping,we limit erroneous learning signals that aim to draw apartinstances of the same class if they have large spatial differ-ences.With this, we extend the STEGO loss so it can be formu-lated as follows:LSTEGO+DepthG=LSTEGO+ DepthGLDepthG(7)with depth-feature correlation weight DepthG. By induc-ing depth knowledge during trainingwithoutencoding thedepth maps as part of the model input, our model can pre-dict on RGB images with its distilled knowledge and doesrely on depth to be available at test time.3.4. Depth-Guided Feature SamplingWe also aim to make the feature sampling process in-formed by the spatial laypout of the scene. To performsampling in the depth space, we transform the down-sampled depth mapd(xi)into a point cloud with points{p1,p2,. . . ,pn}. On this point cloud, we apply farthestpoint sampling (FPS), in an iterative fashion by alwaysselecting the next pointpijas the point with the maxi-mum distance in 3D space with respect to the rest of points{pi1,pi2,. . . ,pij 1}. After having sampledN2points, weend up with a set of samples{pi1,pi2,. . . ,piN2}which areconsequently converted two 2D sampling indices for thefeature mapsfandg. In contrast to the data-agnostic ran-dom sampling applied in STEGO, our feature selection pro-cess takes into account the geometry of the input scene andmore equally covers the spatial structure. This more equalsampling of depth space further increases the effectivenessof our depth feature correlation loss due to the increase di-versity in selected 3D locations.3.5. Guidance SchedulingWhile our depth-feature correlation loss is effective atenriching the model’s learning process with spatial infor-mation of the scene, we aim to alleviate the potential of itinterfering the learning of feature correlations during modeltraining. We hypothesize that our model most greatly bene-ﬁts from depth information towards the beginning of train-ing when its only knowledge is encoded in the featuresmaps output by the frozen ViT backbone. To give it a head4Figure 2.Overview of the DepthG training process.After 5-cropping the image, each crop is encoded by the DINO-pretrained ViTFtooutput a feature map. Using farthest point sampling (FPS), we sample the 3D space equally and convert the coordinates to select samplesin the feature map. The sampled features are further transformed by the segmentation headS. For both feature maps, the correlationtensor is computed. Following, we sample the depth map at the coordinates obtained by FPS and compute a correlation tensor in the samefashion. Finally, we compute ourDepth-Feature Correlationloss and combine it with the feature distillation loss from STEGO. We guidethe model to learn depth-feature correlation for crops of the same image, while the feature distillation loss is also applied to k-NN-selectedand random images.this we construct the depth correspondence tensor similarto the feature correspondence from Equation1. The depthcorrespondence tensorDis computed from the depths oftwo different image crops as follows:Dhw,uv=dhwiduvj,(5)where(h, w)and(u, v)represent the pixel positions in thedepth mapsdianddjrespectively. Together with the zeroclamping, ourDepth-Feature Correlationloss is deﬁned as:LDepthG= Xhw,uv(Dhw,uv bDepthG) max(Shw,uv,0)(6)whereDhw,uvrepresents the depth correlation tensor,bDepthGis the bias for our loss term, andShw,uvrepresentsthe feature correlation tensor computed from the output fea-tures of the segmentation headS. By also using zero-clamping, we limit erroneous learning signals that aim todraw apart instances of the same class if they have largespatial differences. With this, we extend the STEGO loss soit can be formulated as follows:LSTEGO+DepthG=LSTEGO+ DepthGLDepthG(7)withDepth-Feature Correlationweight DepthG. By induc-ing depth knowledge during trainingwithoutencoding thedepth maps as part of the model input, our model can predictspatially informed segmentations on RGB images with itsdistilled knowledge and does not rely on depth to be avail-able at test time.3.4. Depth-Guided Feature SamplingWe also aim to make the feature sampling process in-formed by the spatial layout of the scene. To perform sam-pling in the 3D space, we transform the downsampled depthmapd(xi)into a point cloud with points{p1,p2,. . . ,pn}.On this point cloud, we apply farthest point sampling(FPS) [14], in an iterative fashion by always selecting thenext pointpkas the point with the maximum distancein 3D space with respect to the already sampled points{p1,p2,. . . ,pk 1}. After having sampledN2points, weend up with a set of samples{p1,p2,. . . ,pN2}which areconsequently converted to 2D sampling indices for the fea-ture mapsfandg. In contrast to the data-agnostic randomsampling applied in STEGO, our feature selection processtakes into account the geometry of the input scene and cov-ers the spatial structure more equally. In our ablations inSection5, we show this scene coverage from FPS of thedepth space further increases the effectiveness of ourDepthFeature Correlationloss, due to the increased diversity inselected 3D locations. We show a visual comparison be-tween random and farthest point sampling in Figure5.3.5. Guidance SchedulingWhile ourDepth-Feature Correlationloss is effective atenriching the model’s learning process with spatial infor-mation of the scene, we aim to alleviate the danger of itinterfering with the learning of feature correlations duringmodel training. We hypothesize that our model most greatlybeneﬁts from depth information in the beginning of trainingwhen its only knowledge is encoded in the features maps by
XXXX
FPSSampleXXSampleXXXXFPS
324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431CVPR#7164CVPR#7164CVPR 2024 Submission #7164. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.
XXXX
FPSSampleFPSXXSampleXXFFSSXXLSTEGO+DepthG
Figure 2.Overview of the DepthG training process.After 5-cropping the image, each crop is encoded by the DINO-pretrained ViTFtooutput a feature map. Using farthest-point-sampling (FPS), we sample the 3D space equally and convert the coordinates to select samplesin the feature map. The sampled features are further transformed by the segmentation headS. For both feature maps, the correlationtensor is computed. Following, we sample the depth map at the coordinates obtained by FPS and compute a correlation tensor in the samefashion. Finally, we compute our depth-feature correlation loss and combine it with the feature distillation loss from STEGO. We guidethe model to learn depth-feature correlation for crops of the same image, while the feature distallation loss is also applied to k-NN-selectedand random images.Letu=d(xi)andv=d(yi)be the depth maps ob-tained for two different crops. The depth maps representthe estimated depths at each pixel of the respective image.We construct the depth correspondence tensorD, deﬁnedas follows:Dhw,ij=uhwvij,(5)where(h, w)and(i, j)represent the pixel positions in thedepth mapsuandvrespectively. Together with the zeroclamping, our depth-feature correlation loss is deﬁned as:LDepthG:= Xhw,ij(Dhw,ij bDepthG) max(Shw,ij,0)(6)whereDhw,ijrepresents the depth correlation tensor,bDepthGis the bias for our loss term, andShw,ijrepresents the fea-ture correlation tensor computed from the output featuresof the segmentation headS. By also using zero-clamping,we limit erroneous learning signals that aim to draw apartinstances of the same class if they have large spatial differ-ences.With this, we extend the STEGO loss so it can be formu-lated as follows:LSTEGO+DepthG=LSTEGO+ DepthGLDepthG(7)with depth-feature correlation weight DepthG. By induc-ing depth knowledge during trainingwithoutencoding thedepth maps as part of the model input, our model can pre-dict on RGB images with its distilled knowledge and doesrely on depth to be available at test time.3.4. Depth-Guided Feature SamplingWe also aim to make the feature sampling process in-formed by the spatial laypout of the scene. To performsampling in the depth space, we transform the down-sampled depth mapd(xi)into a point cloud with points{p1,p2,. . . ,pn}. On this point cloud, we apply farthestpoint sampling (FPS), in an iterative fashion by alwaysselecting the next pointpijas the point with the maxi-mum distance in 3D space with respect to the rest of points{pi1,pi2,. . . ,pij 1}. After having sampledN2points, weend up with a set of samples{pi1,pi2,. . . ,piN2}which areconsequently converted two 2D sampling indices for thefeature mapsfandg. In contrast to the data-agnostic ran-dom sampling applied in STEGO, our feature selection pro-cess takes into account the geometry of the input scene andmore equally covers the spatial structure. This more equalsampling of depth space further increases the effectivenessof our depth feature correlation loss due to the increase di-versity in selected 3D locations.3.5. Guidance SchedulingWhile our depth-feature correlation loss is effective atenriching the model’s learning process with spatial infor-mation of the scene, we aim to alleviate the potential of itinterfering the learning of feature correlations during modeltraining. We hypothesize that our model most greatly bene-ﬁts from depth information towards the beginning of train-ing when its only knowledge is encoded in the featuresmaps output by the frozen ViT backbone. To give it a head4Figure 2.Overview of the DepthG training process.After 5-cropping the image, each crop is encoded by the DINO-pretrained ViTFtooutput a feature map. Using farthest point sampling (FPS), we sample the 3D space equally and convert the coordinates to select samplesin the feature map. The sampled features are further transformed by the segmentation headS. For both feature maps, the correlationtensor is computed. Following, we sample the depth map at the coordinates obtained by FPS and compute a correlation tensor in the samefashion. Finally, we compute ourDepth-Feature Correlationloss and combine it with the feature distillation loss from STEGO. We guidethe model to learn depth-feature correlation for crops of the same image, while the feature distillation loss is also applied to k-NN-selectedand random images.this we construct the depth correspondence tensor similarto the feature correspondence from Equation1. The depthcorrespondence tensorDis computed from the depths oftwo different image crops as follows:Dhw,uv=dhwiduvj,(5)where(h, w)and(u, v)represent the pixel positions in thedepth mapsdianddjrespectively. Together with the zeroclamping, ourDepth-Feature Correlationloss is deﬁned as:LDepthG= Xhw,uv(Dhw,uv bDepthG) max(Shw,uv,0)(6)whereDhw,uvrepresents the depth correlation tensor,bDepthGis the bias for our loss term, andShw,uvrepresentsthe feature correlation tensor computed from the output fea-tures of the segmentation headS. By also using zero-clamping, we limit erroneous learning signals that aim todraw apart instances of the same class if they have largespatial differences. With this, we extend the STEGO loss soit can be formulated as follows:LSTEGO+DepthG=LSTEGO+ DepthGLDepthG(7)withDepth-Feature Correlationweight DepthG. By induc-ing depth knowledge during trainingwithoutencoding thedepth maps as part of the model input, our model can predictspatially informed segmentations on RGB images with itsdistilled knowledge and does not rely on depth to be avail-able at test time.3.4. Depth-Guided Feature SamplingWe also aim to make the feature sampling process in-formed by the spatial layout of the scene. To perform sam-pling in the 3D space, we transform the downsampled depthmapd(xi)into a point cloud with points{p1,p2,. . . ,pn}.On this point cloud, we apply farthest point sampling(FPS) [14], in an iterative fashion by always selecting thenext pointpkas the point with the maximum distancein 3D space with respect to the already sampled points{p1,p2,. . . ,pk 1}. After having sampledN2points, weend up with a set of samples{p1,p2,. . . ,pN2}which areconsequently converted to 2D sampling indices for the fea-ture mapsfandg. In contrast to the data-agnostic randomsampling applied in STEGO, our feature selection processtakes into account the geometry of the input scene and cov-ers the spatial structure more equally. In our ablations inSection5, we show this scene coverage from FPS of thedepth space further increases the effectiveness of ourDepthFeature Correlationloss, due to the increased diversity inselected 3D locations. We show a visual comparison be-tween random and farthest point sampling in Figure5.3.5. Guidance SchedulingWhile ourDepth-Feature Correlationloss is effective atenriching the model’s learning process with spatial infor-mation of the scene, we aim to alleviate the danger of itinterfering with the learning of feature correlations duringmodel training. We hypothesize that our model most greatlybeneﬁts from depth information in the beginning of trainingwhen its only knowledge is encoded in the features maps by
XXXX
FPSSampleXXSampleXXXXFPS
324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431CVPR#7164CVPR#7164CVPR 2024 Submission #7164. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.
XXXX
FPSSampleFPSXXSampleXXFFSSXXLSTEGO+DepthG
Figure 2.Overview of the DepthG training process.After 5-cropping the image, each crop is encoded by the DINO-pretrained ViTFtooutput a feature map. Using farthest-point-sampling (FPS), we sample the 3D space equally and convert the coordinates to select samplesin the feature map. The sampled features are further transformed by the segmentation headS. For both feature maps, the correlationtensor is computed. Following, we sample the depth map at the coordinates obtained by FPS and compute a correlation tensor in the samefashion. Finally, we compute our depth-feature correlation loss and combine it with the feature distillation loss from STEGO. We guidethe model to learn depth-feature correlation for crops of the same image, while the feature distallation loss is also applied to k-NN-selectedand random images.Letu=d(xi)andv=d(yi)be the depth maps ob-tained for two different crops. The depth maps representthe estimated depths at each pixel of the respective image.We construct the depth correspondence tensorD, deﬁnedas follows:Dhw,ij=uhwvij,(5)where(h, w)and(i, j)represent the pixel positions in thedepth mapsuandvrespectively. Together with the zeroclamping, our depth-feature correlation loss is deﬁned as:LDepthG:= Xhw,ij(Dhw,ij bDepthG) max(Shw,ij,0)(6)whereDhw,ijrepresents the depth correlation tensor,bDepthGis the bias for our loss term, andShw,ijrepresents the fea-ture correlation tensor computed from the output featuresof the segmentation headS. By also using zero-clamping,we limit erroneous learning signals that aim to draw apartinstances of the same class if they have large spatial differ-ences.With this, we extend the STEGO loss so it can be formu-lated as follows:LSTEGO+DepthG=LSTEGO+ DepthGLDepthG(7)with depth-feature correlation weight DepthG. By induc-ing depth knowledge during trainingwithoutencoding thedepth maps as part of the model input, our model can pre-dict on RGB images with its distilled knowledge and doesrely on depth to be available at test time.3.4. Depth-Guided Feature SamplingWe also aim to make the feature sampling process in-formed by the spatial laypout of the scene. To performsampling in the depth space, we transform the down-sampled depth mapd(xi)into a point cloud with points{p1,p2,. . . ,pn}. On this point cloud, we apply farthestpoint sampling (FPS), in an iterative fashion by alwaysselecting the next pointpijas the point with the maxi-mum distance in 3D space with respect to the rest of points{pi1,pi2,. . . ,pij 1}. After having sampledN2points, weend up with a set of samples{pi1,pi2,. . . ,piN2}which areconsequently converted two 2D sampling indices for thefeature mapsfandg. In contrast to the data-agnostic ran-dom sampling applied in STEGO, our feature selection pro-cess takes into account the geometry of the input scene andmore equally covers the spatial structure. This more equalsampling of depth space further increases the effectivenessof our depth feature correlation loss due to the increase di-versity in selected 3D locations.3.5. Guidance SchedulingWhile our depth-feature correlation loss is effective atenriching the model’s learning process with spatial infor-mation of the scene, we aim to alleviate the potential of itinterfering the learning of feature correlations during modeltraining. We hypothesize that our model most greatly bene-ﬁts from depth information towards the beginning of train-ing when its only knowledge is encoded in the featuresmaps output by the frozen ViT backbone. To give it a head4Figure 2.Overview of the DepthG training process.After 5-cropping the image, each crop is encoded by the DINO-pretrained ViTFtooutput a feature map. Using farthest point sampling (FPS), we sample the 3D space equally and convert the coordinates to select samplesin the feature map. The sampled features are further transformed by the segmentation headS. For both feature maps, the correlationtensor is computed. Following, we sample the depth map at the coordinates obtained by FPS and compute a correlation tensor in the samefashion. Finally, we compute ourDepth-Feature Correlationloss and combine it with the feature distillation loss from STEGO. We guidethe model to learn depth-feature correlation for crops of the same image, while the feature distillation loss is also applied to k-NN-selectedand random images.this we construct the depth correspondence tensor similarto the feature correspondence from Equation1. The depthcorrespondence tensorDis computed from the depths oftwo different image crops as follows:Dhw,uv=dhwiduvj,(5)where(h, w)and(u, v)represent the pixel positions in thedepth mapsdianddjrespectively. Together with the zeroclamping, ourDepth-Feature Correlationloss is deﬁned as:LDepthG= Xhw,uv(Dhw,uv bDepthG) max(Shw,uv,0)(6)whereDhw,uvrepresents the depth correlation tensor,bDepthGis the bias for our loss term, andShw,uvrepresentsthe feature correlation tensor computed from the output fea-tures of the segmentation headS. By also using zero-clamping, we limit erroneous learning signals that aim todraw apart instances of the same class if they have largespatial differences. With this, we extend the STEGO loss soit can be formulated as follows:LSTEGO+DepthG=LSTEGO+ DepthGLDepthG(7)withDepth-Feature Correlationweight DepthG. By induc-ing depth knowledge during trainingwithoutencoding thedepth maps as part of the model input, our model can predictspatially informed segmentations on RGB images with itsdistilled knowledge and does not rely on depth to be avail-able at test time.3.4. Depth-Guided Feature SamplingWe also aim to make the feature sampling process in-formed by the spatial layout of the scene. To perform sam-pling in the 3D space, we transform the downsampled depthmapd(xi)into a point cloud with points{p1,p2,. . . ,pn}.On this point cloud, we apply farthest point sampling(FPS) [14], in an iterative fashion by always selecting thenext pointpkas the point with the maximum distancein 3D space with respect to the already sampled points{p1,p2,. . . ,pk 1}. After having sampledN2points, weend up with a set of samples{p1,p2,. . . ,pN2}which areconsequently converted to 2D sampling indices for the fea-ture mapsfandg. In contrast to the data-agnostic randomsampling applied in STEGO, our feature selection processtakes into account the geometry of the input scene and cov-ers the spatial structure more equally. In our ablations inSection5, we show this scene coverage from FPS of thedepth space further increases the effectiveness of ourDepthFeature Correlationloss, due to the increased diversity inselected 3D locations. We show a visual comparison be-tween random and farthest point sampling in Figure5.3.5. Guidance SchedulingWhile ourDepth-Feature Correlationloss is effective atenriching the model’s learning process with spatial infor-mation of the scene, we aim to alleviate the danger of itinterfering with the learning of feature correlations duringmodel training. We hypothesize that our model most greatlybeneﬁts from depth information in the beginning of trainingwhen its only knowledge is encoded in the features maps byFigure 2. Overview of the DepthG training process. After 5-cropping the image, each crop is encoded by the DINO-pretrained ViT Fto
output a feature map. Using farthest point sampling (FPS), we sample the 3D space equally and convert the coordinates to select samples
in the feature map. The sampled features are further transformed by the segmentation head S. For both feature maps, the correlation
tensor is computed. Following, we sample the depth map at the coordinates obtained by FPS and compute a correlation tensor in the same
fashion. Finally, we compute our Depth-Feature Correlation loss and combine it with the feature distillation loss from STEGO. We guide
the model to learn depth-feature correlation for crops of the same image, while the feature distillation loss is also applied to k-NN-selected
and random images.
where (h, w)and(u, v)represent the pixel positions in the
depth maps dianddjrespectively. Together with the zero
clamping, our Depth-Feature Correlation loss is defined as:
LDepthG =−X
hw,uv(Dhw,uv−bDepthG ) max(Shw,uv ,0)
(6)
whereDhw,uv represents the depth correlation tensor,
bDepthG is the bias for our loss term, and Shw,uv represents
the feature correlation tensor computed from the output fea-
tures of the segmentation head S. By also using zero-
clamping, we limit erroneous learning signals that aim to
draw apart instances of the same class if they have large
spatial differences. With this, we extend the STEGO loss so
it can be formulated as follows:
LSTEGO+DepthG =LSTEGO +λDepthGLDepthG (7)
with Depth-Feature Correlation weight λDepthG . By induc-
ing depth knowledge during training without encoding the
depth maps as part of the model input, our model can predict
spatially informed segmentations on RGB images with its
distilled knowledge and does not rely on depth to be avail-
able at test time.
3.4. Depth-Guided Feature Sampling
We also aim to make the feature sampling process in-
formed by the spatial layout of the scene. To perform sam-
pling in the 3D space, we transform the downsampled depth
mapd(xi)into a point cloud with points {p1, p2, ..., p n}.
On this point cloud, we apply farthest point sampling(FPS) [14], in an iterative fashion by always selecting the
next point pkas the point with the maximum distance
in 3D space with respect to the already sampled points
{p1, p2, ..., p k−1}. After having sampled N2points, we
end up with a set of samples {p1, p2, ..., p N2}which are
consequently converted to 2D sampling indices for the fea-
ture maps fandg. In contrast to the data-agnostic random
sampling applied in STEGO, our feature selection process
takes into account the geometry of the input scene and cov-
ers the spatial structure more equally. In our ablations in
Section 5, we show this scene coverage from FPS of the
depth space further increases the effectiveness of our Depth
Feature Correlation loss, due to the increased diversity in
selected 3D locations. We show a visual comparison be-
tween random and farthest point sampling in Figure 5.
3.5. Guidance Scheduling
While our Depth-Feature Correlation loss is effective at
enriching the model’s learning process with spatial infor-
mation of the scene, we aim to alleviate the danger of it
interfering with the learning of feature correlations during
model training. We hypothesize that our model most greatly
benefits from depth information in the beginning of training
when its only knowledge is encoded in the features maps by
the frozen ViT backbone. To give it a head start, we increase
the weight of our Depth-Feature Correlation loss in the be-
ginning and gradually decrease its influence during training.
Vice versa, the distillation process in the feature space will
increasingly emphasised as the training progresses. In this
way, the network builds upon the already learned rough spa-
3640
OriginalLabelAttention LHP3D-LHP
Figure 3. Local Hidden Positives. We visualize the use of depth
and attention maps for local hidden positives. For this visualiza-
tion, we sample the respective propagation maps at the yellow
patch in the center of the crops. We observe the depth map to
have sharper borders and more consistent propagation values. We
experiment with both propagation strategies in Section 5.
tial structure of the scene achieved through our depth guid-
ance. Therefore, we implement an exponential decay of the
weight λDepthG and bias bDepthG of our loss component. We
ablate on the use of guidance scheduling in the appendix.
3.6. Local Hidden Positives In 3D Space
We further explore the combination of our method with
Hidden Positives [35], which also builds upon STEGO.
As we demonstrate as part of our ablation on the individ-
ual influence of our contributions in Section 5, our feature
sampling method, implemented through farthest point sam-
pling, is integral to the effectiveness of DepthG . Therefore,
we decide not to replace it with the global hidden positives
sampling from Seong et al. [35]. Instead, we implement a
depth-informed variant of local hidden positives, 3D-LHP ,
where the loss for an individual patch is propagated to eight
neighboring patches proportionally to their attention values
obtained from the feature extractor. We modify this strat-
egy by instead propagating the learning signal to the closest
patches in 3D space proportional to their relative distances,
using the point cloud described in Section 3.4. As visual-
ized in Figure 3, we observe that our depth-based propaga-
tion map has sharper and more consistent surfaces. To prop-
agate the learning signal to the selected patches, we follow
Hidden Positives and mix the features coming from the seg-
mentation head Sin proportion to their propagation values
(point distances). The mixed representations are then fed
through an additional projection head P. We then calculate
LSTEGO+DepthG again for the produced output features and
combine it with the loss from the segmentation head output.
4. Experiments
Datasets and Models. We conduct experiments on the
COCO-Stuff [4], Cityscapes [11] and Potsdam-3 datasets.
COCO-Stuff contains a wide variety of real-world scenes.
In our evaluation, we follow [16, 20, 35] and provide re-sults on the coarse class split, COCO-Stuff 27. In contrast,
Cityscapes contains traffic scenes from 50 cities from a
driver-like viewpoint. Lastly, the Potsdam-3 dataset is com-
posed of aerial, top-down images of the city of Potsdam. We
use the DINO [7] backbones ViT-Small (ViT-S) and ViT-
Base (ViT-B), which were pre-trained in a self-supervised
manner on ImageNet-1k [12]. We choose the models with
a patch size of 8×8, since they have been shown to per-
form best for semantic segmentation due to the higher res-
olution of the resulting feature maps [16, 35]. In the result
tables, we refer to DepthG models trained with our Depth-
Feature Correlation loss and FPS as Ours . Models that uti-
lize 3D-LHP propagation with depth maps in addition are
displayed as Ours w/ 3D-LHP . We compare our approach
against competing methods which were never trained with
human annotations, i.e. human labels or language supervi-
sion, neither in the feature extractor nor the segmentation
training.
Evaluation Protocols. Similar to STEGO and related
work [16, 35], we evaluate our models in the unsupervised,
clustering-based setting, as well as linear probing. Since
the output of our model is a pixel-level map of features and
not class labels, these features are consequently clustered.
Following, the pseudo-labeled clusters are aligned with the
ground truth labels through Hungarian matching [23, 24]
across the entire validation dataset. To perform linear prob-
ing, an additional linear layer is added on top of the model
and trained with cross-entropy loss to learn classification of
the features.
Setting Unsupervised Linear
Method Model Acc. mIoU Acc. mIoU
IIC [20] R18+FPN 21.8 6.7 44.5 8.4
PiCIE [10] R18+FPN 48.1 13.8 54.2 13.9
PiCIE+H [10] R18+FPN 50.0 14.4 54.8 14.8
DINO [7] ViT-S/8 28.7 11.3 68.6 33.9
ACSeg [26] ViT-S/8 16.4 - - -
TransFGU [42] ViT-S/8 17.5 52.7 - -
STEGO + HP [35] ViT-S/8 57.2 24.6 75.6 42.7
STEGO [16] ViT-S/8 48.3 24.5 74.4 38.3
+Ours ViT-S/8 56.3 25.6 73.7 38.9
+Ours w/ 3D-LHP ViT-S/8 55.1 26.7 73.9 37.8
DINO [7, 16] ViT-B/8 30.5 9.6 66.8 29.4
DINOSAUR [34]* ViT-B/8 44.9 24.0 - -
STEGO [16] ViT-B/8 56.9 28.2 76.1 41.0
+Ours ViT-B/8 58.6 29.0 75.5 41.6
Table 1. Evaluation on COCO-Stuff 27. We report results on
COCO-Stuff with 27 high-level classes. Overall, our method out-
performs STEGO and HP on unsupervised segmentation with the
ViT-B/8, while showing competitive performance for the ViT-S/8.
*Results obtained without post-processing optimization.
3641
4.1. COCO-Stuff
We present our evaluation on COCO-Stuff 27 in Table 1.
For the ViT-S/8, our experiments show that Ours is able to
improve upon STEGO in most metrics, with improved un-
supervised accuracy by +8.0% and unsupervised mIoU in-
creased by +1.1% .Ours w/ 3D-LHP further increases this
mIoU delta to +1.8% , highlighting the effectiveness of our
3D-information propagation strategy in combination with
DepthG . When comparing our approach to Hidden Posi-
tives, a method with more computational overhead, for the
ViT-S/8, we show competitive performance for unsuper-
vised accuracy and outperform their approach by +1.0%
on unsupervised mIoU with Ours and+1.7% with Ours w/
3D-LHP . When using the DINO ViT-B/8 encoder, our ap-
proach again outperforms STEGO, as well as all other pre-
sented methods on unsupervised metrics. Most notably, we
are able to increase the unsupervised mIoU by +0.8% .
4.2. Cityscapes
We further evaluate our approach on the Cityscapes
dataset [11], consisting of various scenes from 50 differ-
ent cities. We follow the training setting from STEGO and,
contrary to all other datasets, do not sample point-wise but
use the full feature map for our learning process along with
the full depth map. As can be seen in Table 2, our method
significantly outperforms STEGO as well as Hidden Pos-
itives. For unsupervised mIoU, while Hidden Positives
decreased performance compared to STEGO, we observe
that our approach to achieves a +2.1% increase. Similarly,
we report state-of-the-art performance in accuracy, building
upon Hidden Positives’ already impressive improvements
over STEGO and outperforming it by +2.1% .
4.3. Potsdam
Our model is further evaluated on the Potsdam-3 dataset,
containing aerial images of the German city of Potsdam.
Contrary to the other benchmarks, which contain images in
a first-person perspective, Potsdam-3 contains only birds-
eye-view images, a perspective that is considered out-of-
distribution for the monocular depth estimator. Despite this
inherent limitation of our approach for aerial data, we are
able to demonstrate relatively commendable performance
in Table 3 by improving STEGO’s performance and report-
ing state-of-the-art performance for the ViT-S backbone.
In contrast, Hidden Positives [35] use a ViT-B/8, and with
roughly twice the parameters as ours, reach an accuracy of
82.4%. We present a visual overview of the predicted Pots-
dam depth maps in the appendix.
4.4. Qualitative Results
We present qualitative results of our method in Figure 4
and compare with segmentation maps from STEGO. OnMethod Model U. Acc U. mIoU
IIC [20] R18+FPN 47.9 6.4
PiCIE [10] R18+FPN 65.6 12.3
DINO [7] ViT-B/8 43.6 11.8
STEGO + HP [35] ViT-B/8 79.5 18.4
STEGO [16] ViT-B/8 73.2 21.0
+Ours ViT-B/8 81.6 23.1
Table 2. Results on Cityscapes. We report unsupervised accuracy
and mIoU on Cityscapes. Our method outperforms both STEGO
variants by substantial margins. Notably, our method is the first to
improve upon unsupervised mIoU.
Method Model U. Acc.
CC [19] VGG11 63.9
DeepCluster [6] VGG11 41.7
IIC [20] VGG11 65.1
DINO [7, 21] ViT-S/8 71.3
STEGO [16, 21] ViT-S/8 77.0
+Ours ViT-S/8 80.4
+Ours w/ 3D-LHP ViT-S/8 80.4
Table 3. Results on Potsdam. We report unsupervised accuracy
on the Potsdam dataset. Our method is able to improve upon
STEGO. We hypothesize that with a zero-shot depth estimator
more suitable for aerial images, the results for our method could
further improve.
Method U. mIoU. U. Acc
STEGO [16] 24.5 48.3
+ Depth-Feature Correlation (1) 24.7 51.2
+ FPS (2) 24.6 49.1
+ 3D-LHP (3) 25.2 48.5
+Ours (1, 2) 25.6 56.3
+Ours w/ 3D-LHP (1, 2, 3) 26.7 55.1
Table 4. Effect of our contributions. We compare our individual
contributions and the combination of all contributions.
multiple occasions, our depth guidance reduces erroneous
predictions from the model caused by visual irritations in
the pixel space. In the example of the boy with the base-
ball bat in Figure 4a, false classifications from STEGO are
caused by shadows on the ground. Our model is able to cor-
rect this. Furthermore, it goes beyond the noisy label and
also correctly classifies the glimpse of a plant that can be
seen through a hole in the background. This is an indication
that our model does not overfit to the depth map, since this
visual cue is only observable in the pixel space, but not the
depth map.
3642
OriginalDepthLabelSTEGOOurs
(a) COCO-Stuff
OriginalDepthLabelSTEGOOurs
 (b) Cityscapes
Figure 4. Qualitative results. We show qualitative differences for plain STEGO compared to STEGO with our depth guidance, using
ViT-S models for COCO and ViT-B for Cityscapes. Where STEGO struggles to differentiate instances, our model is able to correct this
and successfully separates them for segmentation. In the case of the building in (a), our method alleviates visual irritations from the pixel
space and corrects the segmentation of the building. In (b), our model is able to better handle visual inconsistencies from shadows.
5. Ablations
Individual Influence. We investigate the effect of our
technical contributions on training our model with a ViT-
S/8 backbone on COCO-Stuff 27. Our observations in Ta-
ble 4 show that our Depth-Feature Correlation loss itself al-
ready improves the performance of STEGO. This improve-
ment is further increased through the use of FPS, which
enables us to sample the depth space more meaningfully
and therefore encourages more diversity in the depth cor-
relation tensor Dhw,uv . Intuitively, this sampling diver-
sity significantly amplifies our Depth-Feature Correlation
for aligning the feature space with the depth space. We
provide a visual comparison to random sampling in Fig-
ure 5 and additional illustrations in the appendix. FPS re-
trieves more diverse locations and specifically selects lo-
cations with depth discontinuities. This naturally benefits
theDepth-Feature Correlation to learn sharper edges in this
area for the output segmentation. Adding local hidden pos-
itives with depth maps further increases the unsupervised
mIoU, while slightly lowering the accuracy.
Source Of Depth Maps. We investigate the effect of dif-
ferent monocular depth estimators to generate the depth
maps used to train our model. In our experiments,
we consider three options: The previously mentioned
ZoeDepth [3], Kick Back & Relax (KBR) [36] which uses
self-supervision to learn depth from Slow-TV videos, as
well as MiDaS [32], the base model to ZoeDepth. Em-
pirically, ZoeDepth produces the most accurate zero-shotmonodepth results across indoor and outdoor datasets, fol-
lowed by MiDaS and KBR [3,36]. We provide a qualitative
comparison in the appendix. To evaluate the influence of
the depth map quality on the performance of our model, we
first generate depth maps for COCO-Stuff 27 using each of
the introduced models. We then train our model with a ViT-
S/8 backbone and report the results in Table 5. We observe
that depth maps from ZoeDepth work best with our method,
while the model trained with MiDaS depth has an edge over
the KBR counterpart.
OriginalDepth
(a) Random Sampling
OriginalDepth
 (b) Farthest Point Sampling
Figure 5. Random vs. Farthest Point Sampling. We observe
that random sampling can miss entire structures like trees in the
first top and the plane in the bottom row. In contrast, our method
meaningfully samples the depth space and selects locations across
the different structures and at depth edges. We show further illus-
trations of FPS in the appendix.
3643
OriginalDepthLabelSTEGOOurs
Figure 6. Failure cases. We show cases where our model fails to
correctly segment and classify the scene. The top row is a prime
example where the difference in depth is correctly distilled, though
the model fails to correctly classify the snow region.
Signal Propagation With Local Hidden Positives. We
ablate the implementation of LHP along with different
propagation strategies. As described in Section 3.6, our
method takes advantage of the depth information of the
scene to propagate the learning signal to patches which are
nearby in 3D space. We further implement utilizing the at-
tention map from the DINO backbone and propagate pro-
portionally to their values i.e., the approach utilized in Hid-
den Positives [35]. While, in Table 6, we show that our ap-
proach benefits from signal propagation with 3D-LHP, ap-
plying LHP with Attention leads to lower performance. We
speculate 3D-LHP is more suitable for our approach since,
for a given location, the signal from our Depth-Feature Cor-
relation loss is calculated w.r.t. the depth at this sample.
Propagating to locations with the same depth does not cor-
rupt this signal, though this can happen with LHP (Atten-
tion), since it does not consider depth for propagation.
Computational Cost. Our method only leads to an in-
significant increase in runtime versus the baseline STEGO
model, since we solely guide the loss as well as the fea-
ture sampling and only for Ours w/ 3D-LHP add an addi-
tional small segmentation head. In contrast, the competing
method Hidden Positives [35] relies on a computationally
more expensive process to select features and introduces an
additional segmentation head to fill their task-specific fea-
ture pool. To keep our computational overhead low, we
make use of a pre-trained monocular depth estimation net-
work with impressive zero-shot capabilities. We consider
task specific training of the depth estimator not a necessity,
since the model has zero-shot capabilities that generalize
well to different scenes and domains. Therefore, in our ex-
periments on a diverse array of scenes, we do not re-train
the depth estimator, and consider the additional computa-
tional cost for generating the depth maps to be negligible.Method Trained with U. Acc. U. mIoU
ZoeDepth [3] Sensor Depth 56.3 25.6
MiDaS [32] Sensor Depth 53.0 25.0
KBR [36] Self-Supervision 50.6 23.1
Table 5. Different depth map sources. We experiment with dif-
ferent monocular depth estimators which were trained with either
sensor depth or self-supervision. Overall, the model trained with
depth maps from ZoeDepth performs best on COCO-Stuff 27.
LHP Propagation Strategy U. Acc. U. mIoU
✗ - 56.3 25.6
✓ 3D-LHP (Depth) 55.1 26.7
✓ LHP (Attention) 52.6 24.5
Table 6. LHP Ablation. We compare the use of depth and atten-
tion for local hidden positives. We find that using depth improves
unsupervised mIoU, while we find that using attention does not
improve our method.
6. Limitations
While we have demonstrated our method’s effectiveness
for many real-world cases, our method’s applicability is
limited in settings unsuitable for depth estimation, such as
slices of CT scans and other medical data domains. Fur-
thermore, the experiments on Potsdam-3 have shown, our
method can improve unsupervised semantic segmentation
despite suboptimal viewing perspectives for the monocu-
lar depth estimator. We assume the scenario of aerial im-
ages represents a rare case where our method would profit
from a domain-specific monocular depth estimator. We also
present failure cases of our model in Figure 6.
7. Conclusion & Future Work
In this work, we have presented a novel method to in-
duce spatial knowledge of the scene into our model for un-
supervised semantic segmentation. We have proposed to
correlate the feature space with the depth space and use
the 3D information to more meaningfully sample features
in a spatially informed way. Furthermore, we have demon-
strated that these contributions produce state-of-the-art per-
formance on many real-world datasets and thus foster the
progress in unsupervised segmentation. The applicability
of our approach for other tasks is further to be explored,
since we hypothesize it can be useful beyond unsupervised
segmentation as part of other contrastive processes. We
consider this to be a promising direction for future work.
Furthermore, it remains to be investigated what kind of in-
formation could be useful in domains where depth is not an
obviously meaningful signal, like medical data.
3644
References
[1] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.
Adabins: Depth estimation using adaptive bins. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 4009–4018, 2021. 3
[2] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.
Localbins: Improving depth estimation by learning local dis-
tributions. In European Conference on Computer Vision ,
pages 480–496. Springer, 2022. 3
[3] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter
Wonka, and Matthias M ¨uller. Zoedepth: Zero-shot trans-
fer by combining relative and metric depth. arXiv preprint
arXiv:2302.12288 , 2023. 3, 7, 8
[4] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-
stuff: Thing and stuff classes in context. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 1209–1218, 2018. 5
[5] Adriano Cardace, Luca De Luigi, Pierluigi Zama Ramirez,
Samuele Salti, and Luigi Di Stefano. Plugging self-
supervised monocular depth into unsupervised domain adap-
tation for semantic segmentation. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer
Vision , pages 1129–1139, 2022. 1, 2
[6] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and
Matthijs Douze. Deep clustering for unsupervised learning
of visual features. In Proceedings of the European confer-
ence on computer vision (ECCV) , pages 132–149, 2018. 6
[7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In Pro-
ceedings of the IEEE/CVF international conference on com-
puter vision , pages 9650–9660, 2021. 1, 2, 3, 5, 6
[8] Po-Yi Chen, Alexander H Liu, Yen-Cheng Liu, and Yu-
Chiang Frank Wang. Towards scene understanding: Un-
supervised monocular depth estimation with semantic-aware
representation. In Proceedings of the IEEE/CVF Conference
on computer vision and pattern recognition , pages 2624–
2632, 2019. 1
[9] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexan-
der Kirillov, and Rohit Girdhar. Masked-attention mask
transformer for universal image segmentation. 2022. 1
[10] Jang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath
Hariharan. Picie: Unsupervised semantic segmentation us-
ing invariance and equivariance in clustering. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 16794–16804, 2021. 1, 2, 5, 6
[11] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 3213–3223, 2016. 1, 5, 6
[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248–255. Ieee, 2009. 5[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In International Con-
ference on Learning Representations , 2020. 1
[14] Yuval Eldar, Michael Lindenbaum, Moshe Porat, and
Yehoshua Y Zeevi. The farthest point strategy for progres-
sive image sampling. IEEE Transactions on Image Process-
ing, 6(9):1305–1315, 1997. 2, 4
[15] Di Feng, Christian Haase-Sch ¨utz, Lars Rosenbaum, Heinz
Hertlein, Claudius Glaeser, Fabian Timm, Werner Wies-
beck, and Klaus Dietmayer. Deep multi-modal object de-
tection and semantic segmentation for autonomous driving:
Datasets, methods, and challenges. IEEE Transactions on
Intelligent Transportation Systems , 22(3):1341–1360, 2020.
1
[16] Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah
Snavely, and William T Freeman. Unsupervised semantic
segmentation by distilling feature correspondences. In Inter-
national Conference on Learning Representations , 2021. 1,
2, 3, 5, 6
[17] Ji Hou, Xiaoliang Dai, Zijian He, Angela Dai, and Matthias
Nießner. Mask3d: Pre-training 2d vision transformers by
learning masked 3d priors. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 13510–13519, 2023. 2
[18] Lukas Hoyer, Dengxin Dai, Yuhua Chen, Adrian Koring,
Suman Saha, and Luc Van Gool. Three ways to improve se-
mantic segmentation with self-supervised depth estimation.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 11130–11140, 2021.
1, 2
[19] Phillip Isola, Daniel Zoran, Dilip Krishnan, and Edward H
Adelson. Learning visual groups from co-occurrences in
space and time. arXiv preprint arXiv:1511.06811 , 2015. 6
[20] Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant in-
formation clustering for unsupervised image classification
and segmentation. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 9865–9874,
2019. 1, 2, 5, 6
[21] Alexander Koenig, Maximilian Schambach, and Johannes
Otterbach. Uncovering the inner workings of stego for
safe unsupervised semantic segmentation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshop , pages 3788–3797, 2023. 6
[22] Philipp Kr ¨ahenb ¨uhl and Vladlen Koltun. Efficient inference
in fully connected crfs with gaussian edge potentials. Ad-
vances in neural information processing systems , 24, 2011.
2, 3
[23] Harold W Kuhn. The hungarian method for the assignment
problem. Naval research logistics quarterly , 2(1-2):83–97,
1955. 5
[24] Harold W Kuhn. Variants of the hungarian method for
assignment problems. Naval research logistics quarterly ,
3(4):253–258, 1956. 5
[25] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and
Il Hong Suh. From big to small: Multi-scale local planar
3645
guidance for monocular depth estimation. arXiv preprint
arXiv:1907.10326 , 2019. 3
[26] Kehan Li, Zhennan Wang, Zesen Cheng, Runyi Yu, Yian
Zhao, Guoli Song, Chang Liu, Li Yuan, and Jie Chen. Ac-
seg: Adaptive conceptualization for unsupervised semantic
segmentation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 7162–
7172, 2023. 5
[27] Chen Liang-Chieh, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan Yuille. Semantic image segmenta-
tion with deep convolutional nets and fully connected crfs.
InInternational Conference on Learning Representations ,
2015. 1
[28] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
European conference on computer vision , pages 740–755.
Springer, 2014. 1
[29] Stuart Lloyd. Least squares quantization in pcm. IEEE trans-
actions on information theory , 28(2):129–137, 1982. 2
[30] Francesco Locatello, Dirk Weissenborn, Thomas Un-
terthiner, Aravindh Mahendran, Georg Heigold, Jakob
Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-
centric learning with slot attention. Advances in Neural In-
formation Processing Systems , 33:11525–11538, 2020. 2
[31] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. Advances in neural information
processing systems , 30, 2017. 2
[32] Ren ´e Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. IEEE transactions on pattern analysis and machine
intelligence , 44(3):1623–1637, 2020. 3, 7, 8
[33] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III
18, pages 234–241. Springer, 2015. 1
[34] M Seitzer, M Horn, A Zadaianchuk, D Zietlow, T Xiao, C
Simon-Gabriel, T He, Z Zhang, B Sch ¨olkopf, Thomas Brox,
et al. Bridging the gap to real-world object-centric learn-
ing. In International Conference on Learning Representa-
tions (ICLR) , 2023. 2, 5
[35] Hyun Seok Seong, WonJun Moon, SuBeen Lee, and Jae-Pil
Heo. Leveraging hidden positives for unsupervised semantic
segmentation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 19540–
19549, 2023. 1, 2, 5, 6, 8
[36] Jaime Spencer, Chris Russell, Simon Hadfield, and Richard
Bowden. Kick back & relax: Learning to reconstruct the
world by watching slowtv. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , 2023. 7, 8
[37] Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu
Cord, and Patrick P ´erez. Dada: Depth-aware domain adap-
tation in semantic segmentation. In Proceedings of theIEEE/CVF International Conference on Computer Vision ,
pages 7364–7373, 2019. 1
[38] Qin Wang, Dengxin Dai, Lukas Hoyer, Luc Van Gool, and
Olga Fink. Domain adaptive semantic segmentation with
self-supervised depth estimation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 8515–8525, 2021. 1, 2
[39] Risheng Wang, Tao Lei, Ruixia Cui, Bingtao Zhang, Hongy-
ing Meng, and Asoke K Nandi. Medical image segmenta-
tion using deep learning: A survey. IET Image Processing ,
16(5):1243–1267, 2022. 1
[40] Yikai Wang, Xinghao Chen, Lele Cao, Wenbing Huang,
Fuchun Sun, and Yunhe Wang. Multimodal token fusion for
vision transformers. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
12186–12195, 2022. 2
[41] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,
Jose M Alvarez, and Ping Luo. Segformer: Simple and
efficient design for semantic segmentation with transform-
ers. Advances in Neural Information Processing Systems ,
34:12077–12090, 2021. 1
[42] Zhaoyuan Yin, Pichao Wang, Fan Wang, Xianzhe Xu, Han-
ling Zhang, Hao Li, and Rong Jin. Transfgu: a top-down ap-
proach to fine-grained unsupervised semantic segmentation.
InEuropean conference on computer vision , pages 73–89.
Springer, 2022. 5
[43] Xiaowen Ying and Mooi Choo Chuah. Uctnet: Uncertainty-
aware cross-modal transformer network for indoor rgb-d se-
mantic segmentation. In European Conference on Computer
Vision , pages 20–37. Springer, 2022. 2
[44] Zhenyu Zhang, Zhen Cui, Chunyan Xu, Yan Yan, Nicu Sebe,
and Jian Yang. Pattern-affinitive propagation across depth,
surface normal and semantic segmentation. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 4106–4115, 2019. 1
[45] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,
Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao
Xiang, Philip HS Torr, et al. Rethinking semantic segmen-
tation from a sequence-to-sequence perspective with trans-
formers. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 6881–6890,
2021. 1
3646
