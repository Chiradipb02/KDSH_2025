VMINer: Versatile Multi-view Inverse Rendering
with Near- and Far-field Light Sources
Fan Fei1,2,4Jiajun Tang1,2,4Ping Tan3,4Boxin Shi1,2#
1National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University
2National Engineering Research Center of Visual Technology, School of Computer Science, Peking University
3Hong Kong University of Science and Technology4Light Illusions
Abstract
This paper introduces a versatile multi-view inverse ren-
dering framework with near- and far-field light sources.
Tackling the fundamental challenge of inherent ambiguity
in inverse rendering, our framework adopts a lightweight
yet inclusive lighting model for different near- and far-field
lights, thus is able to make use of input images under varied
lighting conditions available during capture. It leverages
observations under each lighting to disentangle the intrin-
sic geometry and material from the external lighting, using
both neural radiance field rendering and physically-based
surface rendering on the 3D implicit fields. After training,
the reconstructed scene is extracted to a textured triangle
mesh for seamless integration into industrial rendering soft-
ware for various applications. Quantitatively and qualita-
tively tested on synthetic and real-world scenes, our method
shows superiority to state-of-the-art multi-view inverse ren-
dering methods in both speed and quality.
1. Introduction
The reconstruction of 3D scenes from multi-view RGB im-
agery has experienced significant advancements following
the development of Neural Radiance Fields (NeRF) [19],
and both the speed and quality of the reconstruction have
reached an unprecedented level [18, 20]. Despite these im-
provements, a common limitation is to represent only the
radiance field, which is a complex product of the external
lighting interacting with the intrinsic geometry and material
of the scene [11]. The entangled nature of this representa-
tion generally hampers the ability to accurately render the
scene under unseen lighting conditions, because the influ-
ence of the original lighting is embedded within the newly
rendered scene. Recent approaches [24, 41,46] have in-
corporated inverse rendering [22] to separate material prop-
erties and lighting effects, extending the application of the
reconstruction beyond novel view synthesis to novel sce-
narios involving relighting and material editing [2]. Some
#Corresponding author. E-mail: shiboxin@pku.edu.cn.
Input Multi -view Images under 
Varied Lighting ConditionsGeometry and Material 
Reconstruction
Mesh Extraction Applications
Light 1
Flash off…
Light 2 Light 1
Flash on
Surface
NormalDiffuse
Albedo
Specular
Roughness
Triangle Mesh Texture
Figure 1. Given multi-view images under possibly varied light-
ing, our method leverages all present lighting conditions to recon-
struct scene geometry and material disentangled with lighting. The
trained fields are extracted to textured meshes for seamless integra-
tion into industrial renderers for various applications.
methods [2, 6,9,21,32] have advanced to the extraction of
detailed triangle meshes with material UV textures, which
can serve as economical and lifelike 3D models for gaming
and cinematography industries [9, 21], thereby marking a
transformative step in digital asset creation.
Inverse rendering presents several fundamental chal-
lenges, one of them being its severe inherent ambigu-
ity [41]. To combat this, multi-view inverse rendering
methodologies usually reduce the ambiguity by imposing
various constraints on each scene component. These meth-
ods can be categorized based on the assumption on the
amount and types of lighting conditions present in the input
images (Tab. 1). The majority of methods [9, 21,24,35,36,
39,41,42] assumes the imagery to be captured under one
fixed lighting condition (rows 1, 2). However, in such sce-
narios, only one case of scene appearances among all pos-
sible cases under different lighting is observed, causing se-
verelighting-material ambiguity , posing considerable dif-
ficulties to material estimation. In a bid to alleviate this am-
biguity, recent techniques [2, 6,10,12,23,32] utilize input
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
11800
Table 1. A summary of multi-view inverse rendering methods based on their assumptions and usages of input lighting conditions.
Methods
# of far-field light # of near-field light Material optimization Lighting-material ambiguity Capture workload
(1)
[24,39,41,42] Single NoneOnly from single Strong Low
(2)
NeILF [35] Single 5D incident light field
(3)
[2,10,12,32]aMultiple None Only from far-field Moderate High
(4)
WildLight [6] Single Single Only from near-field Moderate Low
(5)
OursSingle SingleFrom near- and far-fieldWeak Low
Multiple
Single or multiple Almost none High
aThese methods also support input images under single far-field lighting. In that case, they belong to single far-field lighting methods (row 1).
images under varied lighting to discern materials from light-
ing (rows 3, 4). This demands more complex lighting mod-
els. Existing varied-lighting methods [2, 10,12,23,32] typ-
ically model multiple, but only far-field, lighting conditions
to maintain manageable unknown lighting parameters. This
means throughout the capture, the lighting setups should be
largely changed by either moving the objects to reconstruct
or adapting far-field light sources, causing increased work-
load to the data collection process. Alternatively, near-field
light sources, such as flashlights, can function as easily con-
trollable light sources for disambiguation. WildLight [6]
suggests the use of a camera-collocated flashlight in addi-
tion to the ambient lighting for inverse rendering. Never-
theless, it limits itself by using only the isolated appearance
under the flashlight for material estimation, not harness-
ing available observations under the ambient lighting and
degrading the quality of its estimated material (row 4).
Addressing the challenges and restrictions previously
outlined, this paper introduces VMINer, a Versatile Multi-
view Inverse rendering framework with Near- and far-field
light sources (row 5). The distinctive feature of our frame-
work is that it uses a lightweight yet inclusive lighting
model for different far- and near-field light sources and ef-
ficiently leverages available observations under each light-
ing to disentangle lighting from other components, mak-
ing the most of whatever lighting conditions are at hand
to boost the practicality and quality of the reconstruction,
as shown in Fig. 1. Although our method accepts the sim-
plest setting of inputs under single far-field lighting, a more
effective compromise is to employ an additional flashlight
to enhance quality without overly complicating the capture
process. The ideal scenario for the highest quality recon-
struction would involve capturing the scene under varied
far-field lighting and using a flashlight.
Nonetheless, implementing such a versatile framework
is far from straightforward. The method has to render
the scene under different lighting conditions, including
spatially-varying and changing ones to represent possibly
moving near-field lights. We propose to solve the prob-
lem by modeling near-field lights as point sources in 3D
space with adjustable positions and intensities, which could
beanisotropic and not necessarily camera-aligned (e.g.,stationary desk lamps). This representation effectively ap-
proximates a broad range of common near-field lights and
is easy for spatially-consistent editing and optimization.
We use 3D implicit fields to model the scenes’ radiance,
shape, and material and minimize the error of the neu-
ral or physically-based re-rendered scene appearance under
each input lighting. To account for multiple lighting situ-
ations and moving near-field lights, we integrate specially
tailored multi-layer perceptrons (MLPs) that additionally
process light directions and embeddings. Post-training, the
fields are converted into a textured triangle mesh, ready
forseamless integration into industrial rendering software
like Blender [1], facilitating a broad spectrum of applica-
tions. Our extensive experiments, covering both synthetic
and real-world scenes, demonstrate that VMINer surpasses
prior methods quantitatively and qualitatively.
2. Related Work
Preliminary. The rendering equation [11] models the re-
flected radiance from any point on a surface as a result of
an intricate surface integral. This integral comprises con-
tributions from three fundamental scene components: light-
ing, material, and geometry. The equation is expressed as:
Lo(x,ωo) =Z
ΩLi(x,ωi)|{z}
lightingf(x,ωi,ωo)|{z}
material(ωi·n)+
|{z}
geometrydωi.(1)
In this equation, Lo(x,ωo)represents the radiance reflected
in the outgoing direction ωofrom a surface point xin 3D
space. Ωdenotes the unit hemisphere encompassing all in-
cident directions ωiwithωi·n>0.Li(x,ωi)is the in-
cident radiance at xfrom direction ωi, and f(x,ωi,ωo)is
the bidirectional reflectance distribution function (BRDF),
denoting the ratio of light reflected along ωoatxfromωi.
The term (ωi·n)+, where x+≜max(x, 0), signifies the co-
sine weakening factor. The incident lighting Li(x,ωi)may
originate from other points within the scene, necessitating
recursive evaluation of this integral.
Multi-view Inverse Rendering. Multi-view inverse ren-
dering techniques [2–4, 6,9,10,12,21,23,24,34–36, 38–
43] generally employ implicit or explicit 3D fields to model
the scene’s geometry and material, while also estimating
11801
Input Multi -view Images under 
Varied Lighting Conditions
Physically -based Surface Rendering (Sec 3.3) Versatile Lighting M odel (Sec. 3.1)
Camera ray :
𝐫𝐫(𝑡𝑡) =𝐨𝐨+𝑡𝑡𝐝𝐝𝐱𝐱
encmat𝐯𝐯
encgeo
𝜷𝜷𝐧𝐧
𝐳𝐳𝜕𝜕
𝑠𝑠Eq. (4)Eq. (5)encdir
Eq. (6)Far-field:
 𝝃𝝃𝑖𝑖𝑖𝑖,𝝁𝝁𝑖𝑖𝑖𝑖,𝜆𝜆𝑖𝑖𝑖𝑖
Near -field:
 𝐩𝐩𝑖𝑖,𝐡𝐡𝑖𝑖𝐿𝐿near𝑖𝑖(𝐱𝐱,𝝎𝝎,𝐩𝐩𝑖𝑖)𝐿𝐿far𝑖𝑖(𝝎𝝎)Eq. (2)
Eq. (3)
Eq. (8)𝐜𝐜near𝑖𝑖𝐂𝐂rf,near𝑖𝑖𝐂𝐂rf𝐜𝐜far𝑖𝑖Eq. (7)𝐂𝐂rf,far𝑖𝑖𝐿𝐿dfar𝑖𝑖(𝐱𝐱,𝝎𝝎)
𝐿𝐿dnear𝑖𝑖(𝐱𝐱,𝝎𝝎,𝐩𝐩𝑖𝑖)
Eq. (13)Eq. (12), (14)
𝐿𝐿ind∗𝑖𝑖(𝐱𝐱,𝝎𝝎)𝐂𝐂pb,dfar𝑖𝑖
𝐂𝐂pb,dnear𝑖𝑖𝐂𝐂pb,ind∗𝑖𝑖𝐂𝐂pb,far𝑖𝑖
𝐂𝐂pb,near𝑖𝑖
Eq. (16)𝐂𝐂pb
𝑪𝑪gtVolume 
Rendering
Eq. (9), (10)ℒconℒpb
ℒrf
ℒeik
ℒsmoEq. (15)
near on/offfar indexencdir(𝐯𝐯)𝐅𝐅far𝑖𝑖
𝐅𝐅near𝑖𝑖𝐜𝐜{far,near }𝑖𝑖near on/offfar index
near on/offfar indexSecondary
Ray Tracing
Radiance Field Rendering (Sec 3.3)encdir(𝐯𝐯)𝜷𝜷 𝐫𝐫(𝑡𝑡)
Monte Carlo Integration
Eq. (11)
: input : variable : refer to another entry : trainable operator / MLP : untrainable operator : element -wise multiplication / add
𝐩𝐩𝑖𝑖𝐿𝐿near𝑖𝑖Geometry, Material, and Radiance  Fields (Sec. 3.2)     Figure 2. Bottom-left: VMINer takes as input multi-view RGB images with foreground masks under possibly varied far- and near-field
lighting. The total number of lighting conditions and their types should be given. Each image is illuminated by one far-field lighting whose
index is “far index”, and near-field lights whose on/off states are “near on/off”. Top-left: VMINer models each far-field lighting (far i) and
near-field lighting (near i) separately as parametric models, from which incoming radiance is queried at any position xand direction ω.
Bottom-mid: It utilizes 3D implicit representations for scene geometry, material, and radiance. Four MLPs process xand view direction v
encoded by different encoders enc{geo,mat,dir} with light embeddings F{far,near}ito get the SDF s, the neural appearance descriptor z, the
BRDF parameters β, and the radiance c{far,near}iunder each lighting. Bottom-right: It then uses radiance field rendering to re-render the
appearance Crf,{near,far}iseparately under each lighting condition, added up to Crfaccording to the per-image lighting condition. Top-
right: Physically-based surface rendering uses Monte Carlo integration to evaluate both direct illumination Ld{near,far} iwith secondary
visibility and indirect illumination Lind{near,far} i. The rendered appearances are aggregated to Cpb.C{rf,pb} are compared with the
observation Cgtto train the scene model. Crf,{near,far}iare also used as additional supervision signals for Cpb,{near,far}i.
lighting to recreate the scene’s appearance. They optimize
each scene component from scratch by aligning the ren-
dered with the observed appearance, while avoiding poten-
tial local minima, which, although do not faithfully repre-
sent the scene, can replicate the observed input. Address-
ing this inherently ambiguous nature of inverse rendering,
these methods typically impose constraints on each scene
component. For geometry and material, assumptions of
known geometry [35, 36,46] are made, along with lever-
aging the recovered shape from radiance fields [6, 10,41],
adopting Lambertian or uniform materials [23, 39], con-
trolling BRDF parameter smoothness [10], or integrating a
learned BRDF latent space [41]. Regarding lighting, many
approaches presuppose known lighting conditions [24, 40]
or fixed lighting shared across input images [35, 38,41,42].
More recent techniques [2, 6,10,12,23,32] have turned
to using images with varied lighting conditions to mitigate
the lighting-material ambiguity. They typically accept in-
put images that can be grouped into sets, each illuminated
by one distinct and fixed far-field lighting. WildLight [6]
exceptionally employs a camera-collocated flashlight along
with fixed ambient lighting. However, it does not model the
ambient lighting and thus its material property estimation
relies exclusively on flashlight observations, where the light
and view direction always coincide [5], limiting its BRDF
estimation ability. Our VMINer, in contrast, accommodatesinput images under single or multiple far-field lighting con-
ditions with near-field light sources and leverages each of
them to enhance the quality of the estimated material.
Lighting Models. Spatially-uniform (SU) lighting mod-
els rely on the assumption that the scene’s lighting orig-
inates from a distant source, modeled as spherical Gaus-
sians (SGs) [2, 10], spherical harmonics (SH) [12, 23],
or MLPs [32]. In contrast, spatially-varying (SV) light-
ing models acknowledge the presence of near-field light
sources, causing different locations to receive different inci-
dent lighting. They include parametric 3D lights [7, 17,36,
37], outgoing light fields such as volumetric SGs [31] and
neural out-of-view lighting volumes [45], and incident light
fields like SV environment maps [47], SVSGs [16], and
neural incident light fields (NeILF) [35, 38]. Our VMINer
integrates distant lights as SGs and near-field lights as point
lights with adjustable position and intensity. It is less com-
plex compared to other SV lighting models, facilitating spa-
tially consistent editing and optimization, while being able
to approximate common lighting setups in real life.
3. Proposed Method
As illustrated in Fig. 2, VMINer reconstructs the scene
lighting (Sec. 3.1) and 3D fields (Sec. 3.2) from input im-
ages using differentiable rendering (Sec. 3.3). The training
scheme and loss functions are described in Sec. 3.4.
11802
3.1. Versatile Lighting Model
This subsection models the lighting term in Eq. (1) and cor-
responds to the top-left part of Fig. 2.
VMINer harnesses the diversity of lighting conditions
present in the input RGB images to disentangle lighting ef-
fects. The lighting model plays a crucial role here: 1) Since
lighting conditions are unknown initially, they must be op-
timized alongside other scene parameters using physically-
based rendering during training. 2) Different lighting se-
tups, each compatible with the model, are necessary dur-
ing the capture phase to achieve this goal. Given these pre-
requisites, it is essential to design a lighting model that ac-
commodates common far-field and near-field light sources,
while being lightweight enough for efficient and robust op-
timization and physically-based rendering. With these con-
siderations, we propose a versatile lighting model that is
both inclusive and lightweight. This representation models
far-field and near-field lighting separately as below.
Far-field Lighting. In our model, each far-field light-
ing condition i, where ibelongs to the set NNfar≜
{1, . . . , N far}andNfardenotes the number of far-field
lighting, is represented using 128-lobe SGs [28] commonly
adopted in existing far-field lighting techniques [33, 39,42].
For each lobe jof lighting i, there are six parameters: lobe
axisξij∈S2, lobe sharpness λij∈R+, and lobe RGB
amplitude µij∈R3
+. The incident radiance from far-field
lighting ialong direction ωat any 3D position xis calcu-
lated using the formula (ignoring visibility for now):
Lfari(ω) =128X
j=1cijµijeλi(ω·ξij−1), (2)
where cij=λij/(2π(1−e−2λij))acts as the normalization
factor related to roughness. Notably, xdoes not appear as
an input to Lfari, reflecting the nature of distant lighting.
Near-field Lighting. Each near-field lighting i, included
in the set NNnearwithNnear representing the number of
near-field light sources, is modeled as a point light. These
point lights can have moving positions pi∈R3and ex-
hibit anisotropic radiation characterized by lth-order (l can
be0,1,or2) SH coefficients hi∈R3×(l+1)2. Our method
accommodates two types of near-field lighting: camera-
collocated lights, positioned at the camera ray origin for
each image, and stationary lights, which remain fixed across
all images. We observe that in neural radiance field ren-
dering, the radiance under a stationary near-field light, es-
pecially when the light is active in all images, can be chal-
lenging to distinguish from radiance under far-field lighting.
Also, the radiance from these two sources aids in material
estimation in a similar way. Therefore, in practical applica-
tions, we favor using a moving flashlight as the near-field
light to provide unique information for material estimation.
The incident radiance from near-field lighting iat a 3D po-sition xis computed as (also ignoring visibility for now):
Lneari(x,ω,pi) =

SH(ω ;hi)
∥pi−x∥2
2ifω=pi−x
∥pi−x∥2
0 otherwise,(3)
where SH(ω ;h)calculates the SH at direction ωwith co-
efficients h, and 1/∥pi−x∥2
2signifies the inverse-square
lighting attenuation for point lights. Here we include pias
an input because it may be set differently across images. It
is important to note that with respect to the incident direc-
tionω,Lfaris a continuous function, while Lnearis a dis-
crete function, being non-zero only in a single direction. As
a result, to render appearances under direct lighting, Monte
Carlo integration is essential for far-field lighting, while a
simple multiplication suffices for near-field lighting. Fur-
ther details about this process are provided in Sec. 3.3.
3.2. Geometry, Material, and Radiance Fields
This subsection models the material and geometry terms in
Eq. (1) and corresponds to the bottom-mid part of Fig. 2.
Geometry. Multi-view reconstruction methodologies
generally hinge on two geometry representations: volume
density [2, 10,19,41] and the signed distance function
(SDF) [33, 39,42,44] fields. We choose the SDF field
for our geometry representation due to its clearly defined
surface at the zero-level isosurface, which simplifies and
enhances the post-training mesh extraction process. Our
approach utilizes implicit fields as MLPs over explicit
structures like voxel grids for their compactness and
flexibility. For each 3D position x, a multi-resolution hash
grid [20] is first employed for positional encoding, yielding
a feature vector encgeo(x)∈R16. The geometry MLP
Mgeothen predicts the SDF value at x:
s(x),z(x) = M geo(x,encgeo(x)), (4)
where s∈Rrepresents the signed distance (positive outside
the surface, negative inside), and z∈R13is a descriptor of
local appearance. To maintain differentiability in rendering
and to aid in geometry optimization, during training we ap-
ply NeuS [29] techniques to transform SDF values along
rays into volume densities. The surface normal n∈S2is
derived as the gradient of the SDF swith respect to x:
n(x) =∂s
∂x∂s
∂x
2. (5)
Mgeoonly approximates a strict SDF field that has∂s
∂x
2=
1, so we normalize nto ensure it is a unit vector.
Material. For accurate scene material recovery using
physically-based re-rendering, it is imperative to model the
material in such a way that the renderer can query the BRDF
at any point on the surface. Considering that material prop-
erties are independent of lighting and view directions, we
represent the scene’s material through an implicit 3D field,
much like the SDF field utilized for geometry. To achieve
11803
this, we employ another multi-resolution hash grid encmat
for positional encoding. We adopt the simplified GGX
BRDF model [27] with a fixed fresnel parameter to help al-
leviate ambiguity. The material MLP, Mmat, is tasked with
predicting the SVBRDF parameters β(x):
β(x) = M mat(x,z,n,encmat(x)). (6)
Radiance. Although our method does not strictly require
a radiance field for rendering scene appearance – given
that appearance could be rendered solely using physically-
based surface rendering (PBR) – we find that incorporating
a radiance field at the start of training significantly eases
the optimization of scene geometry and appearance. PBR,
while physically accurate, tends to introduce instability and
slow down the training process due to its inherent ambi-
guity and computational intensity. Moreover, the radiance
field-rendered results can serve as supplementary supervi-
sion, aiding PBR in better separating scene appearance un-
der each lighting condition and thus further diminishing
lighting-material ambiguity. Hence, we deploy radiance
MLPs to predict view-dependent and lighting-dependent ra-
diance at each position. For each lighting condition i, a light
embedding F{far,near}i∈R16is utilized. The view direc-
tionv∈S2is processed through directional encoding that
projects it onto the coefficients of the 3rd-order SH basis,
yielding encdir(v)∈R42=16[20]. The far-field radiance
MLP Mfarcomputes the outgoing radiance cfarifrom posi-
tionxalong view direction vunder far-field lighting i:
cfari(x,v) = M far(x,z,n,encdir(v),Ffari), (7)
where we incorporate the surface normal nas an additional
MLP input, following Instant-NSR [44] and WildLight [6],
as this has shown to aid shape recovery.
The near-field radiance MLP differs slightly, as near-
field lighting can move in different images, thus the cur-
rent light position piis a necessary input. Additionally,
the radiance under near-field point light takes the form of
the rendering equation Eq. (1) without the integral, allow-
ing the incoming radiance and cosine term to be explicitly
included. We follow ReNe [25] to input the relative direc-
tion of the point light ωi=pi−x
∥pi−x∥2instead of the absolute
position piinto the MLP. Therefore, the outgoing neural
radiance cneariunder near-field lighting iis calculated as:
cneari(x,v,pi) = M near(x,z,n,encdir(v),Fneari,ωi)
⊗Lneari(x,ωi)
∥pi−x∥2
2(ωi·n)+, (8)
where ⊗denotes element-wise multiplication. On the right
side, the second term denotes the incident radiance attenu-
ated by square distance, the third term denotes the cosine
weakening factor, and Mnearaccounts for other factors like
reflectance and visibility. Owing to the instability of these
terms at the training’s outset, we use an annealing strategy,as recommended in prior works [44], gradually replacing
default values (e.g., 1 for incident radiance) with those op-
timized during training, to ease through the process.
3.3. Differentiable Rendering
To simplify the notation, we omit piand the lighting in-
dexiin the equations within this subsection. Our approach
optimizes the lighting alongside the implicit fields primar-
ily by reducing the discrepancy between the observed and
differentiably re-rendered appearances. The re-rendering is
accomplished using two types of renderer: a neural radiance
field renderer (Fig. 2bottom-right) and a physically-based
surface renderer (Fig. 2top-right; Eq. (1)). We handle each
lighting condition separately in rendering, and then aggre-
gate the results based on per-image lighting condition.
Volume Rendering for SDF Field. Consider a camera
rayr(t) = o+td(t > 0), with o∈R3as its origin and
d∈S2as its direction. We incorporate importance sam-
pling using an occupancy grid [14] to get N≤1024 points
along this ray, denoted as r(ti), i∈NN. Utilizing tech-
niques from NeuS [29], we transform the signed distances
s(r(t i))into discrete opacity values as follows:
αi=Φb(s(r(t i)))−Φb(s(r(t i+1)))
Φb(s(r(t i)))+
, (9)
where Φb(s) = 1 /(1 + e−bs)is the cumulative opacity dis-
tribution function. The parameter bis trainable and tends
to increase during training, focusing opacity more narrowly
around the surface where s= 0. The accumulated transmit-
tance is then calculated as Ti=Qi−1
j=1(1−αj).
Neural Radiance Field Rendering. The color Crfof a
camera ray under either far- or near-field lighting condi-
tions, as rendered through the neural radiance field, is de-
termined by accumulating the radiance along the ray’s path:
Crf,{far,near} (o,d) =NX
i=1Tiαic{far,near} (r(ti),−d). (10)
This formula sums the radiance from each sampled point to
yield the color of the ray under the specified lighting.
Physically-based Rendering. Our method employs dif-
ferentiable surface rendering, modeling the observed radi-
ance as reflected from a single surface point. We com-
pute the depth tof this surface point similarly to Eq. (10):
t=PN
i=1Tiαiti. Accumulation along the ray also ap-
plies to the BRDF parameters βand the surface normal
n. For each input lighting, we render the appearance that
models secondary visibility and indirect illumination. The
rendering method under direct illumination differs for far-
and near-field lighting. For direct far-field lighting Ldfar,
we employ Monte Carlo integration to evaluate the render-
ing equation (Eq. (1)) as the incident light comes from all
11804
directions:
Cpb,dfar (o,d) = Lo(r(t),−d)
=1
SSX
s=1Ldfar(r(t), ωs)f(r(t), ωs,−d)(ω s·n)+
p(ωs),(11)
where Sis the number of sampled directions, ωsa sam-
pled incident direction, and p(ωs)the probability density
function (PDF) for the sampled direction. We use multi-
ple importance sampling [26] to combine evaluations from
different strategies: BRDF importance sampling [13], SG
lighting importance sampling [33], and cosine importance
sampling. we follow differentiable rendering works [15] to
use a small S= 20, as stochastic gradient descent handles
noisy gradients. The incident radiance, now taking the visi-
bility of light sources into account, is given by:
Ldfar(r(t), ωs) =Lfar(ωs)V(r(t), ωs), (12)
withV(r(t), ωs)∈[0,1]representing visibility, or the in-
verse of the accumulated opacity along a secondary ray
rsec(t′) =r(t) + t′ωs,(t′>0), computed in a manner
akin to Eq. (10). For direct near-field lighting Ldnear , the
integral in Eq. (1) simplifies to a multiplication, as the light
originates from a single direction:
Cpb,dnear (o,d) = Ldnear(r(t), ω)f(r(t), ω,−d)(ω ·n)+,
(13)
where ω= (p−r(t))/∥p−r(t)∥2is the direction of in-
cident light, and Ldnear(r(t), ω)is the potentially occluded
incident radiance, computed similarly to Eq. (12).
Indirect illumination Lind, relevant to both far-field and
near-field lighting, considers radiance reflected from the
scene itself, potentially from all directions, thus neces-
sitating Monte Carlo integration. The neural radiance
fieldCrfsubstitutes multi-bounce path tracing for indi-
rect illumination, given its encapsulation of the scene
radiance under infinite lighting bounces. The radiance
Cpb,ind{far,near} from indirect illumination is evaluated
similarly to Eq. (12), but with the Ldfarreplaced by the in-
direct lighting Lind{far,near} :
Lind∗(r(t), ωs) =c∗(x′,−ωs)(1−V(r(t), ωs)), (14)
where x′is the intersection of the secondary ray rsec(t′)
with the scene geometry. The radiance from indirect illu-
mination is also computed for each lighting condition sep-
arately, then combined with direct illumination to yield the
complete PBR radiance for the camera ray under a specific
lighting condition:
Cpb,{far,near} =Cpb,d{far,near} +Cpb,ind{far,near} .(15)
3.4. Training
Training Schemes. Our training process is divided into
two sequential stages. In the first stage, the focus is on
training the geometry and radiance field without employ-ing PBR. The objectives are: 1) to recover the scene’s ge-
ometry, 2) to distinguish the appearance under each light-
ing condition, a necessity due to the potential presence of
multiple light sources in one image, and 3) to utilize the re-
constructed radiance field for indirect illumination. In the
second stage, we train the material field and scene lighting
using PBR, aiming to estimate material properties.
Loss Functions. We average the loss functions across
batches of camera rays. During the first stage, our loss func-
tion compares the total re-rendered radiance under all light-
ing conditions present in an image – including a far-field
light iand any active near-field lights – with the ground-
truth (GT) color Cgtfrom the input observation. This com-
parison trains the geometry and radiance field:
Lrf=∥Crf,far i+X
neariis onCrf,near i−Cgt∥2
2.(16)
The Eikonal loss [8] is applied to the gradients of the SDF
values sfor geometric regularization:
Leik=NX
j=1Tjαj∂s(r(tj))
∂r(tj)
2−12
, (17)
A silhouette loss using GT alpha αgtfrom the foreground
mask and a normal smoothness loss [41] aid shape recovery:
Lsil= ((1−TN+1)−αgt)2. (18)
Lns=NX
j=1Tjαj(n(r(t j))−n(r(t j) +ϵ))2, (19)
where ϵ∈R3is a small random perturbation. The total loss
for stage one is Lrf+λeikLeik+λsilLsil+λnsLns, with
λ∗representing the loss weights. In stage two, PBR colors
replace radiance field-rendered colors for comparison with
Cgt, and a novel self-consistency loss is introduced between
neural and PBR radiance under each lighting for additional
supervision. The direct supervision loss Lpbmirrors Lrf,
withCrf,{far,near} ireplaced by Cpb,{far,near} i. The per-
lighting self-consistency loss Lconis defined as:
Lcon=N{far,near}X
i=1Crf,{far,near} i−Cpb,{far,near} i2
2.(20)
This loss is crucial for discerning contributions from
each lighting in images with multiple active light sources,
thereby reducing ambiguity and improving material estima-
tion. A material smoothness loss Lmsis used similar to Lns.
The total loss for stage two is Lpb+λconLcon+λmsLms.
Our model is trained on an NVIDIA RTX 3090 GPU for
a total of 40,000 steps. The first 30,000 steps of stage one
take about 20 minutes, while the subsequent 10,000 steps
of stage two require about 40 minutes. Post-training, we
follow a procedure similar to WildLight [6] to extract the
fields into textured meshes, which can be easily integrated
into industry-standard rendering software like Blender [1],
11805
Table 2. Quantitative comparison results with state-of-the-art methods averaged on 6 synthetic scenes. We show results of surface normal,
diffuse albedo, view synthesis RGB, free-viewpoint (FV) relit RGB, the specular reflection part of FV relit RGB, and training time on a
single RTX 3090 GPU. We mark the best and the second best results in each column. ↑(↓) means bigger (smaller) is better.
MethodInput lighting Normal Albedo V
iew synthesis FV relit FV relit (spec)Time
conditions MAngE↓ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑
(1)TensoIR [
10]
Single far-field17.66 26.48 0.921 29.48 0.912 28.18 0.901 28.30 0.861 300 mins
(2) NVDiffRecMC [9] 16.24 26.52 0.915 27.13 0.913 26.61 0.901 25.57 0.836 150 mins
(3) Ours 12.39 24.50 0.882 28.20 0.934 27.46 0.921 27.56 0.871 45 mins
(4)WildLight [
6] Single far-field + 11.49 28.86 0.940 29.87 0.929 30.44 0.930 27.71 0.863 1440 mins
(5) Ours Flashlight 10.89 31.62 0.953 32.09 0.953 32.00 0.953 30.75 0.906 60mins
(6)TensoIR [
10]Two far-field16.24 27.18 0.929 29.68 0.912 28.66 0.902 28.46 0.863 300 mins
(7) Ours 11.70 26.07 0.902 29.59 0.942 29.27 0.934 29.09 0.890 45 mins
(8)Ours Two
far + Single near 10.79 32.04 0.957 32.10 0.950 32.38 0.954 31.40 0.910 60
mins
enabling fast and high-quality rendering suitable for various
applications. Check the supplement for more details.
4. Experiments
Our experiments involve a comprehensive comparison
with state-of-the-art methods (Sec. 4.2), ablation stud-
ies (Sec. 4.3), and evaluations on both synthetic datasets
(Sec. 4.1) and real-world images (Sec. 4.4). The supplement
shows further data creation details and more experiments.
4.1. Synthetic Datasets
We collect 6 synthetic scenes comprising a variety of scene
geometries and materials. For training, each scene is ren-
dered under four distinct lighting setups: 1) single far-field
light, 2) single far-field light with a camera-collocated flash-
light, 3) two far-field lights, and 4) two far-field lights with
a near-field light source. We render 100 training images
per setting using random viewpoints and per-image light-
ing conditions. The far-field lighting from the first setting is
used in subsequent settings, with the last two sharing their
far-field lighting. For testing, 200 images under one com-
mon far-field lighting are rendered to assess novel view syn-
thesis under seen lighting, and another 200 under unseen
far-field lighting to evaluate free-viewpoint (FV) relighting,
which largely depends on the geometry and material esti-
mation quality. This methodology provides a relatively fair
comparison across methods with different abilities regard-
ing supported input lighting conditions, trained under simi-
lar conditions and tested all using the same sets.
4.2. Comparison with State-of-the-art Methods
We compare our method with leading multi-view inverse
rendering methods under each input lighting scenario. For
single far-field lighting inputs (Tab. 1row 1), we compare
with NVDiffRecMC [9] and TensoIR [10] (single-lighting
input). For multiple far-field lighting scenarios (Tab. 1row
3), we compare with TensoIR [10]. For setups involving a
far-field light and a flashlight (Tab. 1row 4), we comparewith WildLight [6]. We use Blender [1] for high-quality
relit results for our method, WildLight [6], and NVD-
iffRecMC [9], importing trained SDF and material fields
as textured meshes. Since TensoIR [10] employs density-
based geometry and cannot extract high-quality meshes, we
utilize its differentiable renderer for relighting.
Our quantitative comparison employs mean angular er-
ror (MAngE), peak signal-to-noise ratio (PSNR), and struc-
tural similarity (SSIM) [30] metrics for surface normal, dif-
fuse albedo, view synthesis RGB, free-viewpoint (FV) relit
RGB, and specular reflection of FV relit RGB. We follow
NeRFactor [41] in assuming albedo and lighting brightness
as scale-invariant, necessitating metric comparisons using
corresponding scales. For visualization and metrics, we
scale each RGB channel of albedo and relit images from all
methods by a global scale to minimize mean squared error
against ground truth.
The quantitative results are detailed in Tab. 2. We ob-
serve that when only input images under single far-field
lighting are available (rows 1-3), our method gives com-
parable results as TensoIR [10] and NVDiffRecMC [9].
Adding a flashlight (rows 4, 5) significantly helps in geom-
etry and material estimation (comparing rows 3, 5), where
the greatest improvement is made regarding the diffuse
albedo. Also, our method clearly surpasses WildLight [6]
in both quality and speed in this setting. Leveraging another
far-field lighting condition (rows 6, 7) brings similar effects
as using a flashlight, but generally with a lower degree of
improvement. Further adding a flashlight (row 8) gives the
best results among all settings and methods, but the gain is
marginal compared to the setting of single far-field lighting
with a flashlight (comparing rows 5, 8).
Fig.3shows the qualitative comparison on two synthetic
scenes: L EGO (left) with detailed geometry, and T ROOPER
(right) with highly-reflective SV materials. The results
show that our method predicts more detailed and faithful
shape, diffuse albedo, and specular parameters that could
produce accurate and realistic relit results under unseen
11806
Albedo Normal Relit Relit (spec)GT NVDiffRecMC WildLight TensoIR Ours
Albedo Normal Relit Relit (spec)GT NVDiffRecMC WildLight TensoIR Ours
Figure 3. Comparison with state-of-the-art methods. We test NVDiffRecMC [9] (single far-field lighting in input), TensoIR [10] (two
far-field lighting in input), and WildLight [6] and our method (single far-field lighting with a flashlight in input) on two synthetic scenes.
The intensity of the specular reflection is multiplied by 3×for the visualization purpose. Please zoom in for details.
Table 3. Ablation study on our method using inputs under single
far-field lighting with a flashlight.
ModelNormal Albedo FV Relit
Relit (spec)
MAngE↓ PSNR↑ PSNR↑ PSNR↑
(1) w/o Lcon 8.452 27.70 29.85
29.08
(2) Mod. cnear 8.572 27.79 30.03 28.87
(3) w/o encmat 8.263 27.79 30.07 28.92
(4) Full model 8.298 28.05 30.19 29.36
lighting. In contrast, WildLight [6] tends to overly smooth
geometry and material. TensoIR [10] can not handle highly-
reflective surfaces, and lighting is baked into its predicted
diffuse albedo for TROOPER .
4.3. Ablation Studies
In Tab. 3, we quantitatively evaluate ablation models on a
subset of our synthetic scenes: 1) the model excluding the
self-consistency loss Lcon, 2) the model where the output of
the near-field radiance MLP is not explicitly multiplied by
the cosine weakening factor and incident radiance (Eq. (8)),
3) the model with encmat(x)replaced by encgeo(x). The
results show that all the above techniques enhance the ge-
ometry and materials estimated by our method.
4.4. Real-world Results
Fig. 4shows the results of our method on two real-world
scenes captured under single far-field lighting and a flash-
light. Our method faithfully recovers the geometry and ma-
terial of the objects and produces realistic relit results.
5. Conclusion
We introduce a versatile multi-view inverse rendering
framework, distinguished by its ability to leverage input im-
InputNormal
AlbedoRelit
(spec)InputNormal
AlbedoRelit
(spec)RelitRelitFigure 4. The results of our method on two real-world scenes, us-
ing multi-view images under one ambient lighting and a flashlight.
The specular reflection intensity is increased for visualization.
ages under varied far- and near-field light sources available
in capture for better geometry and material estimation.
Limitations. VMINer does not model the background, ne-
cessitating a foreground mask for each image. It also does
not consider unknown tone-mapping curves applied dur-
ing the image signal processing (ISP) stage, using a fixed
curve with γ= 2.2 to the computed linear radiance. It
depends on distinguishing the contribution from each light-
ing, which implies that, for optimal results, near-field light
sources should be switched on/off during capture.
Acknowledgments
This work is supported by the National Natural Science
Foundation of China (Grand No. 62136001, 62088102).
11807
References
[1] Blender Foundation. The Blender project - free and open
3D creation software. https://www.blender.org.
Accessed: 2023-11-07. 2,6,7
[2] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Bar-
ron, Ce Liu, and Hendrik P. A. Lensch. NeRD: Neural re-
flectance decomposition from image collections. In Proc.
of IEEE/CVF International Conference on Computer Vision
(ICCV), 2021. 1,2,3,4
[3] Mark Boss, Varun Jampani, Raphael Braun, Ce Liu,
Jonathan T. Barron, and Hendrik P. A. Lensch. Neural-
PIL: Neural pre-integrated lighting for reflectance decompo-
sition. In Proc. of Neural Information Processing Systems
(NeurIPS), 2021.
[4] Mark Boss, Andreas Engelhardt, Abhishek Kar, Yuanzhen
Li, Deqing Sun, Jonathan T. Barron, Hendrik P. A. Lensch,
and Varun Jampani. SAMURAI: shape and material from
unconstrained real-world arbitrary image collections. In
Proc. of Neural Information Processing Systems (NeurIPS),
2022. 2
[5] Brent Burley. Physically-based shading at disney. In SIG-
GRAPH 2012 Courses, 2012. 3
[6] Ziang Cheng, Junxuan Li, and Hongdong Li. WildLight:
In-the-wild inverse rendering with a flashlight. In Proc. of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2023. 1,2,3,5,6,7,8
[7] Marc-Andr ´e Gardner, Yannick Hold-Geoffroy, Kalyan
Sunkavalli, Christian Gagn ´e, and Jean-Franc ¸ois Lalonde.
Deep parametric indoor lighting estimation. In Proc. of
IEEE/CVF International Conference on Computer Vision
(ICCV), 2019. 3
[8] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and
Yaron Lipman. Implicit geometric regularization for learn-
ing shapes. In Proc. of International Conference on Machine
Learning (ICML), 2020. 6
[9] Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg.
Shape, light, and material decomposition from images us-
ing Monte Carlo rendering and denoising. In Proc. of Neural
Information Processing Systems (NeurIPS), 2022. 1,2,7,8
[10] Haian Jin, Isabella Liu, Peijia Xu, Xiaoshuai Zhang, Song-
fang Han, Sai Bi, Xiaowei Zhou, Zexiang Xu, and Hao
Su. TensoIR: Tensorial inverse rendering. In Proc. of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2023. 1,2,3,4,7,8
[11] James T. Kajiya. The rendering equation. In Proc. of the
ACM SIGGRAPH Conference and Exhibition On Computer
Graphics and Interactive Techniques (SIGGRAPH), 1986. 1,
2
[12] Zhengfei Kuang, Kyle Olszewski, Menglei Chai, Zeng
Huang, Panos Achlioptas, and Sergey Tulyakov. NeROIC:
neural rendering of objects from online image collections.
ACM Transactions on Graphics (TOG), 41(4):56:1–56:12,
2022. 1,2,3
[13] Eric P Lafortune and Yves D Willems. Using the mod-
ified phong reflectance model for physically based render-
ing. Katholieke Universiteit Leuven. Departement Comput-
erwetenschappen, 1994. 6[14] Ruilong Li, Hang Gao, Matthew Tancik, and Angjoo
Kanazawa. NerfAcc: Efficient sampling accelerates NeRFs.
arXiv preprint arXiv:2305.04966, 2023. 5
[15] Tzu-Mao Li, Miika Aittala, Fr ´edo Durand, and Jaakko Lehti-
nen. Differentiable Monte Carlo ray tracing through edge
sampling. ACM Transactions on Graphics (TOG), 37(6):
222, 2018. 6
[16] Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi,
Kalyan Sunkavalli, and Manmohan Chandraker. Inverse ren-
dering for complex indoor scenes: Shape, spatially-varying
lighting and SVBRDF from a single image. In Proc. of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2020. 3
[17] Zhengqin Li, Jia Shi, Sai Bi, Rui Zhu, Kalyan Sunkavalli,
Milos Hasan, Zexiang Xu, Ravi Ramamoorthi, and Manmo-
han Chandraker. Physically-based editing of indoor scene
lighting from a single image. In Proc. of European Confer-
ence on Computer Vision (ECCV), 2022. 3
[18] Zhaoshuo Li, Thomas M ¨uller, Alex Evans, Russell H. Tay-
lor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin.
Neuralangelo: High-fidelity neural surface reconstruction. In
Proc. of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), 2023. 1
[19] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:
Representing scenes as neural radiance fields for view syn-
thesis. In Proc. of European Conference on Computer Vision
(ECCV), 2020. 1,4
[20] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a mul-
tiresolution hash encoding. ACM Transactions on Graphics
(TOG), 41(4):102:1–102:15, 2022. 1,4,5
[21] Jacob Munkberg, Wenzheng Chen, Jon Hasselgren, Alex
Evans, Tianchang Shen, Thomas M ¨uller, Jun Gao, and Sanja
Fidler. Extracting triangular 3D models, materials, and light-
ing from images. In Proc. of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), 2022. 1,
2
[22] Ravi Ramamoorthi and Pat Hanrahan. A signal-processing
framework for inverse rendering. In Proc. of the ACM SIG-
GRAPH Conference and Exhibition On Computer Graphics
and Interactive Techniques (SIGGRAPH), 2001. 1
[23] Viktor Rudnev, Mohamed Elgharib, William A. P. Smith,
Lingjie Liu, Vladislav Golyanik, and Christian Theobalt.
NeRF for outdoor scene relighting. In Proc. of European
Conference on Computer Vision (ECCV), 2022. 1,2,3
[24] Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang,
Matthew Tancik, Ben Mildenhall, and Jonathan T. Barron.
NeRV: Neural reflectance and visibility fields for relighting
and view synthesis. In Proc. of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), 2021.
1,2,3
[25] Marco Toschi, Riccardo De Matteo, Riccardo Spezialetti,
Daniele De Gregorio, Luigi Di Stefano, and Samuele Salti.
ReLight My NeRF: A dataset for novel view synthesis and
relighting of real world objects. In Proc. of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), 2023. 5
11808
[26] Eric Veach and Leonidas J. Guibas. Optimally combining
sampling techniques for Monte Carlo rendering. In Proc. of
the ACM SIGGRAPH Conference and Exhibition On Com-
puter Graphics and Interactive Techniques (SIGGRAPH),
1995. 6
[27] Bruce Walter, Stephen R. Marschner, Hongsong Li, and Ken-
neth E. Torrance. Microfacet models for refraction through
rough surfaces. In Proc. of the Eurographics Symposium on
Rendering Techniques, 2007. 5
[28] Jiaping Wang, Peiran Ren, Minmin Gong, John M. Sny-
der, and Baining Guo. All-frequency rendering of dynamic,
spatially-varying reflectance. ACM Transactions on Graph-
ics (TOG), 28(5):133, 2009. 4
[29] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. NeuS: Learning neural im-
plicit surfaces by volume rendering for multi-view recon-
struction. In Proc. of Neural Information Processing Systems
(NeurIPS), 2021. 4,5
[30] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P.
Simoncelli. Image quality assessment: from error visibility
to structural similarity. IEEE Transactions on Image Pro-
cessing (TIP), 13(4):600–612, 2004. 7
[31] Zian Wang, Jonah Philion, Sanja Fidler, and Jan Kautz.
Learning indoor inverse rendering with 3D spatially-varying
lighting. In Proc. of IEEE/CVF International Conference on
Computer Vision (ICCV), 2021. 3
[32] Zian Wang, Tianchang Shen, Jun Gao, Shengyu Huang,
Jacob Munkberg, Jon Hasselgren, Zan Gojcic, Wenzheng
Chen, and Sanja Fidler. Neural fields meet explicit geomet-
ric representations for inverse rendering of urban scenes. In
Proc. of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), 2023. 1,2,3
[33] Haoqian Wu, Zhipeng Hu, Lincheng Li, Yongqiang Zhang,
Changjie Fan, and Xin Yu. NeFII: Inverse rendering for
reflectance decomposition with near-field indirect illumina-
tion. In Proc. of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), 2023. 4,6
[34] Wenqi Yang, Guanying Chen, Chaofeng Chen, Zhenfang
Chen, and Kwan-Yee K. Wong. PS-NeRF: Neural inverse
rendering for multi-view photometric stereo. In Proc. of Eu-
ropean Conference on Computer Vision (ECCV), 2022. 2
[35] Yao Yao, Jingyang Zhang, Jingbo Liu, Yihang Qu, Tian
Fang, David McKinnon, Yanghai Tsin, and Long Quan.
NeILF: Neural incident light field for physically-based ma-
terial estimation. In Proc. of European Conference on Com-
puter Vision (ECCV), 2022. 1,2,3
[36] Bohan Yu, Siqi Yang, Xuanning Cui, Siyan Dong, Baoquan
Chen, and Boxin Shi. MILO: multi-bounce inverse rendering
for indoor scene with light-emitting objects. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence (TPAMI),
45(8):10129–10142, 2023. 1,2,3
[37] Chong Zeng, Guojun Chen, Yue Dong, Pieter Peers, Hongzhi
Wu, and Xin Tong. Relighting neural radiance fields with
shadow and highlight hints. In Proc. of the ACM SIGGRAPH
Conference and Exhibition On Computer Graphics and In-
teractive Techniques (SIGGRAPH), 2023. 3
[38] Jingyang Zhang, Yao Yao, Shiwei Li, Jingbo Liu, Tian Fang,
David McKinnon, Yanghai Tsin, and Long Quan. NeILF++:Inter-reflectable light fields for geometry and material esti-
mation. In Proc. of IEEE/CVF International Conference on
Computer Vision (ICCV), 2023. 2,3
[39] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and
Noah Snavely. PhySG: Inverse rendering with spherical
gaussians for physics-based material editing and relighting.
InProc. of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), 2021. 1,2,3,4
[40] Kai Zhang, Fujun Luan, Zhengqi Li, and Noah Snavely.
IRON: inverse rendering by optimizing neural SDFs and ma-
terials from photometric images. In Proc. of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), 2022. 3
[41] Xiuming Zhang, Pratul P. Srinivasan, Boyang Deng, Paul E.
Debevec, William T. Freeman, and Jonathan T. Barron. NeR-
Factor: neural factorization of shape and reflectance under
an unknown illumination. ACM Transactions on Graphics
(TOG), 2021. 1,2,3,4,6,7
[42] Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei
Jia, and Xiaowei Zhou. Modeling indirect illumination for
inverse rendering. In Proc. of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), 2022. 1,
2,3,4
[43] Youjia Zhang, Teng Xu, Junqing Yu, Yuteng Ye, Yanqing
Jing, Junle Wang, Jingyi Yu, and Wei Yang. NeMF: In-
verse volume rendering with neural microflake field. In Proc.
of IEEE/CVF International Conference on Computer Vision
(ICCV), 2023. 2
[44] Fuqiang Zhao, Yuheng Jiang, Kaixin Yao, Jiakai Zhang, Liao
Wang, Haizhao Dai, Yuhui Zhong, Yingliang Zhang, Minye
Wu, Lan Xu, and Jingyi Yu. Human performance modeling
and rendering via neural animated mesh. ACM Transactions
on Graphics (TOG), 41(6):235:1–235:17, 2022. 4,5
[45] Jingsen Zhu, Fujun Luan, Yuchi Huo, Zihao Lin, Zhihua
Zhong, Dianbing Xi, Rui Wang, Hujun Bao, Jiaxiang Zheng,
and Rui Tang. Learning-based inverse rendering of complex
indoor scenes with differentiable Monte Carlo raytracing. In
Proc. of the ACM SIGGRAPH Conference and Exhibition on
Computer Graphics and Interactive Techniques in Asia (SIG-
GRAPH Asia), 2022. 3
[46] Jingsen Zhu, Yuchi Huo, Qi Ye, Fujun Luan, Jifan Li, Di-
anbing Xi, Lisha Wang, Rui Tang, Wei Hua, Hujun Bao,
and Rui Wang. I2-SDF: Intrinsic indoor scene reconstruc-
tion and editing via raytracing in neural SDFs. In Proc. of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2023. 1,3
[47] Yongjie Zhu, Yinda Zhang, Si Li, and Boxin Shi. Spatially-
varying outdoor lighting estimation from intrinsics. In Proc.
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2021. 3
11809
