4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling
Sherwin Bahmani1,2Ivan Skorokhodov3,4Victor Rong1,2Gordon Wetzstein5Leonidas Guibas5
Peter Wonka3Sergey Tulyakov4Jeong Joon Park6Andrea Tagliasacchi1,7,8David B. Lindell1,2
1University of Toronto2Vector Institute3KAUST4Snap Inc.5Stanford University6University of Michigan7SFU8Google
"a space shtllne natichgir" "a coycymgne pnavgir a motw sel"
lgwevgewpygil
Figure 1. Text-to-4D Synthesis. We present 4D-fy, a technique that synthesizes 4D (i.e., dynamic 3D) scenes from a text prompt. We show
scenes generated from two text prompts for different viewpoints (vertical dimension) at different time steps (horizontal dimension). Video
results can be viewed on our website: https://sherwinbahmani.github.io/4dfy .
Abstract
Recent breakthroughs in text-to-4D generation rely on
pre-trained text-to-image and text-to-video models to gener-
ate dynamic 3D scenes. However, current text-to-4D meth-
ods face a three-way tradeoff between the quality of scene
appearance, 3D structure, and motion. For example, text-
to-image models and their 3D-aware variants are trained
on internet-scale image datasets and can be used to pro-
duce scenes with realistic appearance and 3D structure—but
no motion. Text-to-video models are trained on relatively
smaller video datasets and can produce scenes with motion,
but poorer appearance and 3D structure. While these mod-
els have complementary strengths, they also have opposing
weaknesses, making it difficult to combine them in a way
that alleviates this three-way tradeoff. Here, we introduce
hybrid score distillation sampling, an alternating optimiza-
tion procedure that blends supervision signals from multiple
pre-trained diffusion models and incorporates benefits of
each for high-fidelity text-to-4D generation. Using hybrid
SDS, we demonstrate synthesis of 4D scenes with compelling
appearance, 3D structure, and motion.1. Introduction
The advent of internet-scale image–text datasets [ 54] and
advances in diffusion models [ 20,58,60] have led to new
capabilities in stable, high-fidelity image generation from
text prompts [ 6,51,52]. Recent methods have also shown
that large-scale text-to-image or text-to-video [ 56] diffusion
models learn useful priors for 3D [ 25,44] and 4D scene
generation [ 57]. Our work focuses on text-to-4D scene gen-
eration (Fig. 1), which promises exciting new capabilities
for applications in augmented and virtual reality, computer
animation, and industrial design.
Current techniques for generating 3D or 4D scenes from
text prompts typically iteratively optimize a representation
of the scene using supervisory signals from a diffusion
model [ 44,67,71]. Specifically, these methods render an
image of a 3D scene, add noise to the rendered image, use a
pre-trained diffusion model to denoise the rendered image,
and estimate gradients used to update the 3D representa-
tion [ 44,67]. This procedure, known as score distillation
sampling (SDS) [ 44], underpins most recent methods for
text-conditioned scene generation.
Using SDS for text-to-4D generation requires navigating
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
7996
Table 1. Text-to-4D models face a tradeoff between the quality
of appearance, 3D structure, and motion depending on the type
of generative model used for score distillation sampling (SDS):
text-to-image (T2I), 3D-aware T2I, or, text-to-video (T2V).
SDS model appearance 3D structure motion
T2I [6, 51, 52, 79] high low N/A
3D-aware T2I [29, 55] medium high N/A
T2V [7, 21, 56, 69, 72] low low high
Our method medium high medium
a three-way tradeoff between the quality of appearance, 3D
structure, and motion (see Table 1); existing techniques ob-
tain satisfactory results in just one or two of these categories.
For example, while SDS produces images that appear real-
istic when rendering a generated scene from any particular
viewpoint, inspecting multiple viewpoints can reveal that the
scene has several faces or heads, replicated appendages, or
other incorrectly repeated 3D structures—an issue now re-
ferred to as the “Janus problem” [ 55].1One way to improve
3D structure is to use SDS with a 3D-aware diffusion model
that is trained to generate images from different camera
viewpoints [ 33]. But 3D-aware models sacrifice appearance
quality as they require fine-tuning on synthetic datasets of
posed images [ 55]. Incorporating motion into a scene using
SDS with a text-to-video model [ 69] typically degrades the
appearance relative to static scenes generated with text-to-
image models, which are more realistic (see Fig. 2). While
different types of diffusion models thus have complemen-
tary qualities, they also have opposing weaknesses (Table 1).
Therefore, it is not trivial to combine them in a way that
yields text-to-4D generation with high-quality appearance,
3D structure, and motion.
Here, we propose a method for text-to-4D scene genera-
tion that alleviates this three-way tradeoff using hybrid SDS ,
an alternating optimization scheme that blends gradient up-
dates from multiple pre-trained diffusion models and syn-
thesizes 4D scenes using the best qualities of each. The
method consists of three stages of optimization: (1) we use
a 3D-aware text-to-image model [ 55] to generate an initial
static 3D scene (without the Janus problem); (2) we continue
the optimization by blending in alternating supervision with
variational SDS [ 71] and a text-to-image model to improve
appearance; (3) we blend in alternating supervision using
video SDS with a text-to-video model [ 69] to add motion to
the scene. By smoothly incorporating supervisory signals
from these three diffusion models throughout the training
process, we achieve text-driven 4D scene generation with
state-of-the-art quality in terms of appearance, 3D structure,
and motion. Overall we provide the following contributions.
•We introduce hybrid SDS, a technique that extracts desir-
1Referring to the two-faced Roman god of beginnings and endings.
text-to-img"ecg pgt lygnih" 
bitd g sgyyccg ro" vbimmih" 
ih g looyccg segr rghpih"ctext-to-vireoFigure 2. Comparing text-to-image and text-to-video models.
Rendered frames from Stable Diffusion version 2.1 (top; text-to-
image) [ 1] and Zeroscope version 2 (bottom; text-to-video) [ 3]
show significant disparity in appearance, with the text-to-image
model appearing far more realistic.
able qualities from multiple pre-trained diffusion models
and alleviates a tradeoff between appearance, 3D structure,
and motion in text-to-4D scene generation.
•We provide a quantitative and qualitative evaluation of the
method, and we explore the three-way tradeoff space with
ablation studies to facilitate future research.
•We demonstrate text-to-4D generation based on open-
source pretrained models and will make all codes and
evaluation procedures publicly available.
•We present state-of-the-art results for the task of text-to-4D
generation.
2. Related Work
Our method is related to techniques from multiple areas of
generative modeling, including text-to-image, text-to-video,
and text-to-3D models. For more extensive discussions of
related works, we refer readers to a recent state-of-the-art
report on diffusion models [43].
Text-to-image generation. Methods for generating im-
ages from text prompts are a relatively new innovation, first
demonstrated using generative adversarial networks [ 49,73,
80]. The problem itself is also related to other methods for
text-based image retrieval [ 34] or image-conditioned text
generation [ 61,77]. More recently, models trained on text–
image datasets with billions of samples [ 54] have become
the state of the art for this task [51].
Diffusion models [ 20,59] are a popular architecture for
generative modeling on large-scale datasets, and autoregres-
sive models have also shown promising results [ 47,78].
Typically, these methods exploit a pretrained text encoder,
such as CLIP [ 46], to encode the text prompt into a feature
7997
vector used to condition the diffusion model [ 40,48]. In
diffusion models, high-resolution (i.e., megapixel) image
generation is achieved by applying repeated upsampling lay-
ers [22,48] or performing diffusion in the lower-resolution
latent space of an autoencoder and then decoding the result
to recover an image at the nominal resolution [ 16,51]. Our
work incorporates two open-source text-to-image diffusion
models: Stable Diffusion [ 51] and MVDream [ 55] (a recent
3D-aware diffusion model) to enable 4D scene generation.
Text-to-video generation. Our work relies on the burgeon-
ing field of video generation via diffusion models, an area
that is somewhat constrained by the limited scale of video
datasets. To counteract this, methods often utilize a hy-
brid training approach on both image and video datasets,
such as WebVid-10M [ 5], HD-VG-130M [ 69], or HD-VILA-
100M [ 74]. Recent approaches in this field typically em-
ploy variations of pixel-space upsampling (both in space
and time) [ 21] or latent space upsampling to improve spatial
and temporal resolution [ 17,18,70,83]. Autoregressive
models distinguish themselves by their ability to generate
videos of varying lengths [ 65]. Further improvements in
video synthesis have been achieved by finetuning pre-trained
text-to-image diffusion models on video data [ 7,56,72],
or separating the content and motion generation process by
using an initial image frame as a starting point [ 17,72]. De-
spite recent advances in text-to-video synthesis, the fidelity
of generated videos still lags behind that of static image gen-
eration (see Fig. 2) and so they perform poorly when used
directly with SDS for text-to-4D generation. Instead, our
work leverages an open-source latent space text-to-video dif-
fusion model called Zeroscope [ 3] (extended from the Mod-
elscope architecture [ 68]) together with other pre-trained,
open-source diffusion models using hybrid SDS.
Text-to-3D generation. Early methods for text-to-3D gen-
eration relied on parsers to convert input text to a seman-
tic representation and synthesized scenes from an object
database [ 4,10,12]. Later, automated, data-driven methods
used multi-modal datasets [ 11], and pre-trained models, such
as CLIP [ 46], to edit or stylize an input 3D mesh [ 14,26]
or a radiance field [ 66]. More recently, CLIP-based super-
vision enabled synthesis of entire 3D scenes [ 25,53], and
these techniques evolved into the most recent approaches,
which optimize a mesh or radiance field based on SDS
supervision [ 30,44,71]. The quality of their 3D struc-
tures has been improved by applying diffusion models that
consider multiple viewpoints [ 31,33,55]. Alternatively,
recent advancements have seen a shift towards using dif-
fusion or transformer models to transform an input 2D
image into a 3D representation for novel-view synthesis
[9,15,35,45,62,63,76]. Still, these techniques do not yet
support generating 4D scenes.
Our work is most closely related to Make-A-Video3D (MA V3D) [ 57], a recent method for text-to-4D
generation that integrates SDS-based supervision in two
separate stages: first with a text-to-image model and subse-
quently with a text-to-video model. Similar to MA V3D, we
aim to generate dynamic 3D scenes; however, our approach
uses hybrid SDS, which allows gradient updates from multi-
ple models to be smoothly blended together in an alternating
optimization. Our approach generates high-quality dynamic
3D scenes and does not suffer from Janus problems.
Concurrent works. Concurrent works on text-to-4D [ 32,
82], image-to-4D [ 50,81,82], and video-to-4D [ 27,41,75]
similarly use recent diffusion models for 4D generation.
3. Method
Our approach for text-to-4D generation builds upon a hash-
encoding-based neural representation [ 39] that implicitly
decomposes the scene into static and dynamic feature
grids [ 64]. In this section we overview our representation for
4D neural rendering and describe the optimization procedure
based on hybrid SDS (see Fig. 3).
3.1. 4D Neural Rendering
V olumetric neural rendering methods represent a scene using
a neural representation to parameterize the attenuation and
emission of light at every point in 3D space [ 36,38]. We can
use such a representation to render an image by casting a
ray from the camera center of projection, through each pixel
location, and into the scene. For sampled points along the
rayµ∈R3, we query a neural representation to retrieve
a volumetric density τ∈R+and color c∈R3
+, which
describe attenuation and emission of light, respectively, at a
particular point. Then, the resulting density and color sam-
ples are alpha-composited to recover the color of a rendered
pixelCas
C=X
iwici, wi=αY
j<i(1−αj), (1)
where αi= 1−e−τi∥µi−µi+1∥. We query the neural rep-
resentation using an additional input time variable t, which
enables modeling time-varying density and color.
We illustrate the neural representation in Fig. 3; it consists
of two multi-resolution hash tables to disentangle static and
dynamic scene modeling.
Following Müller et al. [ 39], the static hash table stores
learnable feature vectors that are indexed by a voxel-lookup
and hashing operation and decoded into density and color
using two small multilayer perceptrons (MLPs). Concretely,
we consider the neural representation
Nθ:µ, t→τ,c (2)
withθ={θstatic, θdynamic , θMLP}denoting all learnable param-
eters from the static and dynamic hash tables and the MLPs.
7998
multi-rsonlutinh 
acoa shdn4ihD
yb hsurcl rsh4srsrapIri4 odnrs 4iotillctinh ocmplihD
"a monket eaignc a danbt ral "
uolume
lenbelgnc
add noisemendemed iga/esvbideos
cakt,mo,a/al e/madienlsw aflemnalin/ celueen di33-sion godefsT2IV()
V()
V(y
otcDs I
otcDs IIslalik 4T2)
hash lacfe
dynagik 442)
hash lacfeotcDs IIIcfend in s-,embision
Figure 3. Overview . A 4D radiance field is parameterized using a neural representation with a static and dynamic multiscale hash table of
features. Images and videos are rendered from the representation using volume rendering, and we supervise the representation using hybrid
score distillation sampling—a technique that combines gradients from multiple types of pre-trained diffusion models. In the first stage of
training we use gradients ∇θL3Dfrom a 3D-aware text-to-image model (3D-T2I) to iteratively optimize a representation without the Janus
problem. Next, we blend in gradient supervision using variational SDS with a text-to-image model (T2I) to improve the appearance (i.e., we
alternate supervision between ∇θLIMGand∇θL3D). In the last stage we incorporate gradients ( ∇θLVID) from a text-to-video model (T2V)
to add motion to the scene, and we update the scene using the other models in an alternating fashion.
For a given µ, we query the static hash table by identifying
the closest voxel at each scale 1≤s≤S. Then, we trilinearly
interpolate the feature values from the voxel vertices after
retrieving them from the hash table. Retrieved features from
each scale are concatenated as fstatic=f(1)
static⊕···⊕ f(S)
static. We
follow the same procedure to query the dynamic hash table
given ( µ,t), except we use quadrilinear interpolation to inter-
polate feature values. The resulting features from the static
and dynamic hash tables are added as f=fstatic+fdynamic .
We do not model view-dependent effects in the feature en-
coding. Finally, we decode density and color as MLPτ(f)
and MLP c(f), respectively.
3.2. Hybrid Score Distillation Sampling
We leverage the 4D representation along with SDS to create
dynamic 3D scenes from a text prompt. Our hybrid approach
incorporates three different flavors of SDS that are smoothly
merged during an alternating optimization procedure to im-
prove the structure and quality of the 4D model:
1.SDS applied to a 3D-aware text-to-image diffusion model
to optimize a static scene without the Janus problem.
2.Variational score distillation sampling (VSD; a modi-
fied version of SDS [71]) using a standard text-to-image
model [ 51] to improve the appearance of the static scene.
3.Video SDS using a text-to-video model [ 69], which ex-
tends SDS to multiple video frames and adds motion to
the scene.
In the following, we describe each type of SDS and how it
used for text-to-4D generation.
3D-aware scene optimization. We first consider optimiz-
ing a static scene using SDS with a 3D-aware text-to-image
diffusion model [ 55]. The diffusion model is pre-trained
using a stochastic forward process that slowly adds Gaus-
sian noise to multiview images xover timesteps 0≤td≤Td.With increasing td, the process yields noisy images ztdthat,
attd=Td, are close to zero-mean Gaussian. After train-
ing, the model reverses this process to add structure to the
noisy images. It predicts ˆ xϕ(ztd;td,y,T), which approx-
imates the output of an optimal denoiser at each timestep
td, conditioned on a text embedding y[48,51,52] and
the camera extrinsics Tcorresponding to each image. In
practice, text-to-image diffusion models typically predict
the noise content ϵϕrather than the denoised image ˆ xϕ.
But note that the denoised image can still be obtained as
ˆ xϕ(ztd;td,y,T)∝ztd−ϵϕ(ztd;td,y,T), i.e., by subtract-
ing the predicted noise from the noisy image [ 20]. We im-
plement 3D-aware SDS by rendering multiple images xθ
from the neural representation, adding noise ϵ, and using
the 3D-aware diffusion model [ 55] to predict the noise ϵϕ
using classifier-free guidance [ 19]. To update the parameters
θof the neural representation, we use the 3D-aware SDS
gradient:
∇θL3D=Etd,ϵ,T
w(td) (ϵϕ(ztd;td,y,T)−ϵ)∂xθ
∂θ
,
(3)
where w(td)is a weighting function that depends on the
diffusion timestep, and we add a stop gradient to the output
of the diffusion model [ 55]. Intuitively, the SDS loss queries
the diffusion model to see how it adds structure to an image,
then this information is used to backpropagate gradients to
the scene representation.
Improving appearance using VSD. We incorporate an ad-
ditional loss term based on VSD [ 71] to improve the appear-
ance of images rendered from the scene. This term uses a
pre-trained text-to-image model [ 51] along with a finetun-
ing scheme that improves image quality over the 3D-aware
text-to-image model alone. We follow Wang et al. [ 71] and
augment the standard SDS gradient with the output of an
7999
additional text-to-image diffusion model that is finetuned
using a low-rank adaptation [ 24], during scene optimization.
Specifically, we have
∇θLIMG=Etd,ϵ,Th
w(td)
ϵϕ(ztd;td,y)−ϵ′
ϕ(ztd;td,y,T)
∂xθ
∂θi
,
(4)
where ϵ′
ϕis the noise predicted using a finetuned version of
the diffusion model that incorporates additional conditioning
from the camera extrinsics T; here, we let ztdrepresent
a noisy version of a single image rendered from Nθ. The
model is finetuned using the standard diffusion objective
min
θEtd,ϵ,T
∥ϵ′
ϕ(ztd;td,y,T)−ϵ∥2
2
. (5)
Note that, different from the original description
of VSD [ 71], we find we can omit the simultaneous op-
timization over multiple scene samples (i.e. the variational
component of [ 71]), which reduces memory requirements
without significantly degrading appearance.
Adding motion with Video SDS. Last, we use supervision
from a text-to-video diffusion model [ 69] to add motion
to the generated scene. This procedure extends the origi-
nal SDS gradient by incorporating structure added by the dif-
fusion model to all noisy video frames [ 57]. The video SDS
gradient is given as
∇θLVID=Etd,ϵ
w(td) (ϵϕ(ztd;td,y)−ϵ)∂Xθ
∂θ
.(6)
To simplify notation, we re-use ϵϕandϵto here repre-
sent the predicted and actual noise for each video frame,
and we let Xθbe a collection of Vvideo frames Xθ=
[x(1)
θ, . . . ,x(V)
θ]Trendered from the representation.
Optimization procedure – Algorithm 1. We optimize the
4D representation in three stages that smoothly blend super-
vision in alternating steps from (1) 3D-aware SDS, (2) VSD,
and (3) video SDS.
Stage 1. In the first stage of optimization, we update
Nθusing gradients from 3D-aware SDS until convergence.
Since this stage focuses on optimizing a static scene, we
freeze (i.e. do not update) the parameters of the dynamic
hash table fdynamic and only update the static hash table and
decoder MLP. We set the total number of first-stage itera-
tions Nstage-1 to match that of Shi et al. [55], which allows
the optimization to proceed until there are no distinguishable
changes in the rendered scene from one iteration to the next.
Stage 2. Next, we add VSD gradients using an alternat-
ing optimization procedure. At each iteration, we randomly
select to update the model using ∇θL3Dor∇θLIMGwith
probability P3DandPIMG. We continue this alternating op-
timization for Nstage-2 iterations, until convergence. As we
show in the next section, this stage of optimization results in
improved appearance compared to using ∇θL3Dalone while
also being free of the Janus problem.Algorithm 1 Hybrid Score Distillation Sampling
Require:
Nθ ▷4D neural representation
Nstage-1 ,Nstage-2 ,Nstage-3 ▷iterations for each stage
P3D,PIMG ▷update probabilities
∇θL3D,∇θLIMG,∇θLVID ▷SDS grads. (Eqs. 3, 4, 6)
1://Stage 1
2:freeze dynamic hash map ( θdynamic )
3:foriter in Nstage-1 do ▷3D update
4: grad =n
∇θL3D
5: UPDATE (grad )
6://Stage 2
7:foriter in Nstage-2 do ▷3D or IMG update
8: grad =(
∇θL3D,with probability P3D
∇θLIMG,otherwise
9: UPDATE (grad )
10://Stage 3
11:decrease learning rate of static hash map ( θstatic)
12:foriter in Nstage-3 do ▷3D, IMG, or VID update
13: grad =

∇θL3D,with probability P3D
∇θLIMG,with probability P3D·PIMG
∇θLVID,otherwise
14: ifVID, unfreeze θdynamic
15: UPDATE (grad )
16:procedure UPDATE (grad )
17: x← N θ ▷render images (Eq. 1)
18: take gradient step on grad ▷optimize Nθ
19: ifIMG, take finetuning step (Eq. 5)
20:end procedure
Stage 3. Last, we update the representation using a com-
bination of all gradient updates. Specifically, we randomly
select to update the model at each iteration using ∇θL3D,
∇θLIMG, or∇θLVIDwith probability P3D,P3D·PIMG, and
1−P3D·PIMG, respectively. Since we now aim to incorporate
motion into the representation, we unfreeze the parameters
of the dynamic hash table during the update with ∇θLVID
but keep them frozen for updates using the text-to-image
models. We also decrease the learning rate of the static hash
table to preserve the high-quality appearance from the previ-
ous stage. We repeat the alternating optimization in the final
stage until convergence, which we find occurs consistently
within Nstage-3 iterations. Overall, hybrid SDS effectively
combines the strengths of each pre-trained diffusion model
while avoiding quality degradations that result from naively
combining gradients from each model.
3.3. Implementation
We implement hybrid SDS based on the threestudio
framework [ 2], which includes implementations of MV-
8000
"a panda dancinv"
eiwoptinm
mihw dwpmAV3D4- f-slr
"a uibewfl Auhantid fltbtm /f_lippinv a ctin"Figure 4. Text-to-4D Comparison. We compare against MA V3D [ 57], and observe our approach obtains significantly higher quality results.
Dream [ 55] (for 3D-aware text-to-image diffusion and SDS),
ProlificDreamer [ 71] with Stable Diffusion [ 51] (text-to-
image diffusion and VSD), and we implement the video
SDS updates using Zeroscope [3, 69].
Hyperparameter values. We initialize the 4D neural repre-
sentation following [ 30,44] and add an offset to the density
predicted by the network in the center of the scene to pro-
mote object-centric reconstructions. We set the learning rates
for the static hash map to 0.01, for the dynamic hash map to
0.01, and for the MLP to 0.001. We drop the learning rate for
the static hash map to 0.0001 before the last stage to focus
the gradient updates on the dynamic hash map. The values
ofNstage-1 ,Nstage-2 , andNstage-3 are set to 10000, 10000, and
100000, respectively. We set the probabilities for hybrid
SDS to P3D= 0.5andPIMG= 0.5for a reasonable tradeoff
with respect to appearance, 3D structure, and motion.
Rendering. Each of the diffusion models has a different
native resolution, so we render images from Nθaccordingly.
We render four images from different camera positions for
the 3D-aware SDS at the native (256 ×256 pixel) resolution
of the 3D-aware text-to-image model. The VSD updateis computed by rendering a 256 ×256 image and bilinearly
upsampling the image to the native resolution of Stable Diffu-
sion (512 ×512). Finally, the video SDS update is computed
by rendering 16 video frames at 160 ×288 resolution and
upsampling to the native 320 ×576 resolution of Zeroscope.
4. Experiments
4.1. Metrics
We assess our method using CLIP Score [ 42] and a user study.
We compare our model against MA V3D for 28 prompts and
against our ablations for a subset of 5 prompts. Current
text-to-4D models are costly to train, and many researchers
in academia do not have access to the scale of resources
available to large tech companies. Hence, we only used a
subset due to computational limitations. To promote future
research in this field, we open source the evaluation protocol
for the user study along the code: https://github.
com/sherwinbahmani/4dfy .
CLIP Score. CLIP Score [ 42] evaluates the correlation
between a text prompt and an image. Specifically, this cor-
8001
Table 2. Quantitative results. We compare our method against
MA V3D and variations of 4D-fy with different loss terms or back-
bone architectures (i.e., with HexPlane [ 8]). The methods are eval-
uated in terms of CLIP Score (CLIP) and human preference based
on appearance quality (AQ), 3D structure quality (SQ), motion
quality (MQ), text alignment (TA), and overall preference (Overall).
The numbers reported for human preference are the percentages of
users who voted for our method over the corresponding method in
head-to-head comparisons.
Human Preference
Method CLIP AQ SQ MQ TA Overall
MA V3D [57] 33.9 92% 89% 41% 52% 67%
4D-fy 34.2 — —
Ablation Study
4D-fy 35.0 — —
w/o∇θL3D/IMG 29.3 100% 100% 78% 86% 94%
w/o∇θL3D 35.1 88% 89% 95% 92% 91%
w/o∇θLIMG 34.5 70% 68% 68% 69% 70%
w/o hybrid SDS 33.8 100% 100% 78% 88% 95%
w/ HexPlane 34.5 95% 92% 90% 92% 95%
responds to the cosine similarity between textual CLIP [ 46]
embedding and visual CLIP [ 46] embedding. The score is
bound between 0 and 100, where 100 is best. We calculate
the CLIP score for MA V3D using the same procedure we use
for our method. Specifically, for each input text prompt, we
render a video using the same camera trajectory as MA V3D,
i.e., moving around the scene in azimuth with a fixed el-
evation angle. Subsequently, we score each video frame
with CLIP ViT-B/32 and average the scores over all frames
and text prompts to derive the final CLIP score.
User study. We conduct qualitative comparisons between
our method and the baseline, MA V3D, by surveying 26 hu-
man evaluators. We use the same head-to-head comparison
model as the user survey conducted by MA V3D. Specifically,
we present text prompts alongside the corresponding outputs
of our method and the baseline method in random order.
Evaluators are requested to specify their overall preference
for a video, as well as evaluate four specific properties: ap-
pearance quality, 3D structure quality, motion quality, and
text alignment. In Table 2, we report the percentage of users
who prefer each method overall and based on each of the
four properties. We conduct χ2-tests to evaluate statistical
significance at the p <0.05level. Further details on the user
study are included in the supplementary.
4.2. Results
We visualize spatio-temporal renderings along with depth
maps in comparison to MA V3D in Fig. 4. Although both
methods can synthesize 4D scenes, MA V3D noticeably lacks
detail. In contrast, our method produces realistic renderings
across space and time. We report quantitative metrics inTable 2. In terms of CLIP Score and overall preference in
the user study 4D-fy outperforms MA V3D. Users indicated a
statistically significant preference towards 4D-fy compared
to MA V3D in terms of appearance quality, 3D structure
quality, text alignment, and overall preference. They rated
the motion quality roughly on par with MA V3D, which used
a propriety text-to-video model. For example, overall, 67%
of users prefer our method over 33% for MA V3D.
4.3. Ablations
We provide an in-depth analysis motivating our hybrid SDS
training scheme by ablating each component and evaluating
the use of a 4D neural representation more similar to that of
MA V3D. We provide ablations in Table 2 and in Fig. 5.
Image guidance (w/o ∇θL3D/IMG ).Technically, learning a
dynamic 3D scene solely from a text-to-video model without
text-to-image guidance is possible. To demonstrate the draw-
backs of this approach, we present results where we skip
the first two stages and directly train the model with text-to-
video guidance only. This corresponds to setting P3D= 0
andPIMG= 0. Our experiments reveal that the text-to-video
model fails to provide realistic 3D structure and high-quality
appearance for generating a dynamic 3D scene.
3D-aware guidance (w/o ∇θL3D).We find that using a
3D-aware diffusion model is crucial for generating realistic
3D structures. If we remove the 3D-aware diffusion model,
i.e., by setting P3D= 0, we can generate scenes with similar
motion and high-quality appearance, but the 3D structure is
degraded. This is evident for both scenes in Fig. 5.
VSD guidance (w/o ∇θLIMG).We find that VSD helps
provide a realistic scene appearance; if we disable it during
scene generation, i.e., PIMG= 0, there are some negative
effects. For example in Fig. 5, the ice cream cone in the
bucket (top row) is more detailed, and the dog’s face (bottom
row) is sharper (please zoom in).
Hybrid SDS. To illustrate the impact of our hybrid SDS
approach we disable image guidance after the second stage
by setting P3D= 0 andPIMG= 0 for the third stage only.
This aligns with the MA V3D training scheme, where a static
model is pre-trained with text-to-image and subsequently
fine-tuned with text-to-video. Our quantitative and qualita-
tive analysis shows that this approach results in degraded
appearance and 3D structure. We find that incorporating
text-to-image, 3D-aware text-to-image, and text-to-video
via hybrid SDS in the final optimization stage preserves a
realistic appearance and high-quality 3D structure.
Backbone architecture. Finally, we ablate the hash-grid-
based 4D representation by replacing it with the HexPlane
[8,13] architecture. This representation similarly disen-
tangles static and dynamic scene components and can be
readily integrated into our pipeline. The HexPlane approach
8002
timeviewpni"tab ybyd pb"gb ebti"r ise skebma ab gnr kigi"r b /hbteynbkgawSn wSn wSn wSn Ddykig xPx wS le4-fb"e 4P-fd
Figure 5. Ablation study. We assess the qualitative impact of removing gradient updates from different models during optimization. Our
method without image guidance ( ∇θL3D/IMG ) does not produce realistic appearance and 3D structure. Removing the 3D-aware guidance
(∇θL3D) generates high-quality appearance but low-quality 3D structure. Our approach without VSD ( ∇θLIMG) reduces the appearance
quality. Hybrid SDS is crucial for appearance and 3D structure, while using HexPlane reduces the appearance quality. Best viewed digitally.
fails to match the appearance quality of the hash-grid-based
representation. MA V3D uses HexPlane but implements a
multi-scale variant with a large 5-layer decoding MLP fea-
turing 128 hidden units. We could not re-implement this
approach as the model does not fit on an 80 GB A100 GPU.
To allow for a fair comparison, we instead increased the ca-
pacity of HexPlane to match the memory consumption of our
hash-grid-based representation. We expect that increasing
the capacity of HexPlane and longer training times could
lead to similar results as our representation.
5. Conclusion
Our method synthesizes high-quality 4D scenes from text
prompts using a novel hybrid score distillation sampling
procedure. Our work alleviates a three-way tradeoff between
appearance, 3D structure, and motion and is the first to build
on open-source models. We will release the code to facilitate
future research in text-to-4D generation.
Limitations. Although our method produces compelling
dynamic 3D scenes, there are several limitations and av-
enues for future work. First, the complexity of motion in our
scenes is limited to simple movements. We believe that our
method will directly benefit from future progress in text-to-
video generation, as current text-to-video models suffer from
low-quality renderings and unrealistic motion. Another wayto improve motion could be exploiting recently proposed
dynamic representations, e.g., dynamic 3D Gaussians [ 37].
Moreover, current metrics in text-to-3D generation are not
sufficient, as they mainly rely on image-based metrics and
user studies. Designing more sophisticated 3D and 4D met-
rics is an important direction for future work. Lastly, generat-
ing each scene takes a significant amount of time. Concurrent
text-to-3D works [ 23,28] alleviate this problem by training
a large-scale model on 3D data, allowing generation within
seconds. Incorporating our hybrid optimization procedure to
blend between large-scale pre-training on 2D, 3D, and video
data could enable fast text-to-4D generation.
Ethics Statement. We condemn the application of our
method for creating realistic fake content intended to harm
specific entities or propagate misinformation.
6. Acknowledgements
This work was supported by the Natural Sciences and Engi-
neering Research Council of Canada (NSERC) Discovery
Grant program, the Digital Research Alliance of Canada,
and by the Advanced Research Computing at Simon Fraser
University. It was also supported in part by ARL grant
W911NF-21-2-0104, a Vannevar Bush Faculty Fellowship,
a gift from the Adobe Corporation, a PECASE by the ARO,
NSF award 1839974, Stanford HAI, and a Samsung GRO.
8003
References
[1]Stable Diffusion version 2. https://github.com/
Stability-AI/stablediffusion . Accessed: 2023-
10-31. 2
[2]Threestudio Github page. https://github.com/
threestudio-project/threestudio . Accessed:
2023-10-31. 5
[3]Zeroscope text-to-video model. https://huggingface.
co/cerspense/zeroscope_v2_576w . Accessed:
2023-10-31. 2, 3, 6
[4]Giovanni Adorni and Mauro Di Manzo. Natural language
input for scene generation. In Proc. EACL , 1983. 3
[5]Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman.
Frozen in time: A joint video and image encoder for end-to-
end retrieval. In Proc. ICCV , 2021. 3
[6]Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Ji-
aming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli
Laine, Bryan Catanzaro, et al. eDiff-I: Text-to-image dif-
fusion models with an ensemble of expert denoisers. arXiv
preprint arXiv:2211.01324 , 2022. 1, 2
[7]Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-
horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.
Align your latents: High-resolution video synthesis with la-
tent diffusion models. In Proc. CVPR , 2023. 2, 3
[8]Ang Cao and Justin Johnson. HexPlane: A fast representation
for dynamic scenes. In Proc. CVPR , 2023. 7
[9]Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W
Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini
De Mello, Tero Karras, and Gordon Wetzstein. Generative
novel view synthesis with 3D-aware diffusion models. In
Proc. ICCV , 2023. 3
[10] Angel Chang, Manolis Savva, and Christopher D Manning.
Learning spatial knowledge for text to 3D scene generation.
InProc. EMNLP , 2014. 3
[11] Kevin Chen, Christopher B Choy, Manolis Savva, Angel X
Chang, Thomas Funkhouser, and Silvio Savarese. Text2shape:
Generating shapes from natural language by learning joint
embeddings. In Proc. ACCV , 2018. 3
[12] Bob Coyne and Richard Sproat. Wordseye: An automatic
text-to-scene conversion system. In Proc. SIGGRAPH , 2001.
3
[13] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk War-
burg, Benjamin Recht, and Angjoo Kanazawa. K-planes:
Explicit radiance fields in space, time, and appearance. In
Proc. CVPR , 2023. 7
[14] William Gao, Noam Aigerman, Thibault Groueix, V ova Kim,
and Rana Hanocka. Textdeformer: Geometry manipulation
using text guidance. In Proc. SIGGRAPH , 2023. 3
[15] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M Susskind,
Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi.
Nerfdiff: Single-image view synthesis with Nerf-guided dis-
tillation from 3d-aware diffusion. In Proc. ICML , 2023. 3
[16] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang,
Dongdong Chen, Lu Yuan, and Baining Guo. Vector quan-
tized diffusion model for text-to-image synthesis. In Proc.
CVPR , 2022. 3[17] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao,
Dahua Lin, and Bo Dai. Animatediff: Animate your person-
alized text-to-image diffusion models without specific tuning.
arXiv preprint arXiv:2307.04725 , 2023. 3
[18] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and
Qifeng Chen. Latent video diffusion models for high-fidelity
video generation with arbitrary lengths. arXiv preprint
arXiv:2211.13221 , 2022. 3
[19] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. In Proc. NeurIPS Workshop on Deep Generative
Models , 2021. 4
[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. In Proc. NeurIPS , 2020. 1, 2,
4
[21] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole,
Mohammad Norouzi, David J Fleet, et al. Imagen video:
High definition video generation with diffusion models. arXiv
preprint arXiv:2210.02303 , 2022. 2, 3
[22] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,
Mohammad Norouzi, and Tim Salimans. Cascaded diffusion
models for high fidelity image generation. The Journal of
Machine Learning Research , 23(1):2249–2281, 2022. 3
[23] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou,
Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao
Tan. Lrm: Large reconstruction model for single image to 3D.
arXiv preprint arXiv:2311.04400 , 2023. 8
[24] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank
adaptation of large language models. In Proc. ICLR , 2021. 5
[25] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel,
and Ben Poole. Zero-shot text-guided object generation with
dream fields. In Proc. CVPR , 2022. 1, 3
[26] Nikolay Jetchev. Clipmatrix: Text-controlled creation of 3D
textured meshes. arXiv preprint arXiv:2109.12922 , 2021. 3
[27] Yanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, and Yao Yao.
Consistent4D: Consistent 360◦dynamic object generation
from monocular video. arXiv preprint arXiv:2311.02848 ,
2023. 3
[28] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun
Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg
Shakhnarovich, and Sai Bi. Instant3D: Fast text-to-3D with
sparse-view generation and large reconstruction model. arXiv
preprint arXiv:2311.06214 , 2023. 8
[29] Weiyu Li, Rui Chen, Xuelin Chen, and Ping Tan. Sweet-
dreamer: Aligning geometric priors in 2D diffusion for con-
sistent text-to-3D. arXiv preprint arXiv:2310.02596 , 2023.
2
[30] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-
Yu Liu, and Tsung-Yi Lin. Magic3D: High-resolution text-to-
3D content creation. In Proc. CVPR , 2023. 3, 6
[31] Yukang Lin, Haonan Han, Chaoqun Gong, Zunnan Xu,
Yachao Zhang, and Xiu Li. Consistent123: One image to
highly consistent 3D asset using case-aware diffusion priors.
arXiv preprint arXiv:2309.17261 , 2023. 3
8004
[32] Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler,
and Karsten Kreis. Align your gaussians: Text-to-4D with
dynamic 4D gaussians and composed diffusion models. arXiv
preprint arXiv:2312.13763 , 2023. 3
[33] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov,
Sergey Zakharov, and Carl V ondrick. Zero-1-to-3: Zero-shot
one image to 3d object. In Proc. ICCV , 2023. 2, 3
[34] Ying Liu, Dengsheng Zhang, Guojun Lu, and Wei-Ying Ma.
A survey of content-based image retrieval with high-level
semantics. Pattern Recognition , 40(1):262–282, 2007. 2
[35] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie
Liu, Taku Komura, and Wenping Wang. Syncdreamer: Gener-
ating multiview-consistent images from a single-view image.
arXiv preprint arXiv:2309.03453 , 2023. 3
[36] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel
Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural
volumes: Learning dynamic renderable volumes from images.
ACM Trans. Graph. , 2019. 3
[37] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva
Ramanan. Dynamic 3D Gaussians: Tracking by persistent
dynamic view synthesis. arXiv preprint arXiv:2308.09713 ,
2023. 8
[38] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:
Representing scenes as neural radiance fields for view syn-
thesis. Communications of the ACM , 65(1):99–106, 2021.
3
[39] Thomas Müller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a multires-
olution hash encoding. ACM Trans. Graph. , 2022. 3
[40] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. GLIDE: Towards photorealistic image genera-
tion and editing with text-guided diffusion models. In Proc.
ICML , 2022. 3
[41] Zijie Pan, Zeyu Yang, Xiatian Zhu, and Li Zhang. Fast dy-
namic 3D object generation from a single-view video. arXiv
preprint arXiv:2401.08742 , 2024. 3
[42] Dong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell,
and Anna Rohrbach. Benchmark for compositional text-to-
image synthesis. In Proc. NeurIPS , 2021. 6
[43] Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman,
Jonathan T Barron, Amit H Bermano, Eric Ryan Chan, Tali
Dekel, Aleksander Holynski, Angjoo Kanazawa, et al. State
of the art on diffusion models for visual computing. arXiv
preprint arXiv:2310.07204 , 2023. 2
[44] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall.
DreamFusion: Text-to-3D using 2D diffusion. In Proc. ICLR ,
2023. 1, 3, 6
[45] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Ali-
aksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov,
Peter Wonka, Sergey Tulyakov, et al. Magic123: One image
to high-quality 3D object generation using both 2D and 3D
diffusion priors. arXiv preprint arXiv:2306.17843 , 2023. 3
[46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, GretchenKrueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In Proc. ICML ,
2021. 2, 3, 7
[47] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In Proc. ICML , 2021. 2
[48] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with CLIP latents. arXiv preprint arXiv:2204.06125 ,
2022. 3, 4
[49] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee. Generative adver-
sarial text to image synthesis. In Proc. ICML , 2016. 2
[50] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao,
Gang Zeng, and Ziwei Liu. DreamGaussian4D: Generative
4D Gaussian splatting. arXiv preprint arXiv:2312.17142 ,
2023. 3
[51] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bjorn Ommer. High-resolution image
synthesis with latent diffusion models. In Proc. CVPR , 2022.
1, 2, 3, 4, 6
[52] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,
Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael
Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Pho-
torealistic text-to-image diffusion models with deep language
understanding. Proc. NeurIPS , 2022. 1, 2, 4
[53] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang,
Chin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malek-
shan. Clip-forge: Towards zero-shot text-to-shape generation.
InProc. CVPR , 2022. 3
[54] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes,
Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.
Laion-5b: An open large-scale dataset for training next gener-
ation image-text models. Proc. NeurIPS , 2022. 1, 2
[55] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,
and Xiao Yang. MVDream: Multi-view diffusion for 3d
generation. arXiv preprint arXiv:2308.16512 , 2023. 2, 3, 4,
5, 6
[56] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
Oran Gafni, et al. Make-a-video: Text-to-video generation
without text-video data. arXiv preprint arXiv:2209.14792 ,
2022. 1, 2, 3
[57] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual,
Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea
Vedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dy-
namic scene generation. In Proc. ICML , 2023. 1, 3, 5, 6,
7
[58] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In Proc. ICML , 2015. 1
[59] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising
diffusion implicit models. Proc. ICLR , 2021. 2
[60] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equations.
InProc. ICLR , 2021. 1
8005
[61] Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Silvia
Cascianelli, Giuseppe Fiameni, and Rita Cucchiara. From
show to tell: A survey on deep learning-based image caption-
ing. IEEE Trans. Pattern Anal. Mach. Intell. , 2022. 2
[62] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,
Lizhuang Ma, and Dong Chen. Make-it-3D: High-fidelity
3D creation from a single image with diffusion prior. arXiv
preprint arXiv:2303.14184 , 2023. 3
[63] Ayush Tewari, Tianwei Yin, George Cazenavette, Semon
Rezchikov, Joshua B Tenenbaum, Frédo Durand, William T
Freeman, and Vincent Sitzmann. Diffusion with forward
models: Solving stochastic inverse problems without direct
supervision. arXiv preprint arXiv:2306.11719 , 2023. 3
[64] Haithem Turki, Jason Y Zhang, Francesco Ferroni, and Deva
Ramanan. Suds: Scalable urban dynamic scenes. In Proc
CVPR , 2023. 3
[65] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kinder-
mans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar,
Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki:
Variable length video generation from open domain textual
description. arXiv preprint arXiv:2210.02399 , 2022. 3
[66] Can Wang, Menglei Chai, Mingming He, Dongdong Chen,
and Jing Liao. Clip-NeRF: Text-and-image driven manipula-
tion of neural radiance fields. In Proc. CVPR , 2022. 3
[67] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,
and Greg Shakhnarovich. Score Jacobian Chaining: Lifting
pretrained 2D diffusion models for 3D generation. In Proc.
CVPR , 2023. 1
[68] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,
Xiang Wang, and Shiwei Zhang. Modelscope text-to-video
technical report. arXiv preprint arXiv:2308.06571 , 2023. 3
[69] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen
Zhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap atten-
tion in spatiotemporal diffusions for text-to-video generation.
arXiv preprint arXiv:2305.10874 , 2023. 2, 3, 4, 5, 6
[70] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,
Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jin-
gren Zhou. Videocomposer: Compositional video synthesis
with motion controllability. arXiv preprint arXiv:2306.02018 ,
2023. 3
[71] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan
Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and
diverse text-to-3D generation with variational score distilla-
tion. Proc. NeurIPS , 2023. 1, 2, 3, 4, 5, 6
[72] Ruiqi Wu, , Liangyu Chen, Tong Yang, Chunle Guo, Chongyi
Li, and Xiangyu Zhang. Lamp: Learn a motion pat-
tern for few-shot-based video generation. arXiv preprint
arXiv:2310.10769 , 2023. 2, 3
[73] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe
Gan, Xiaolei Huang, and Xiaodong He. AttnGAN: Fine-
grained text to image generation with attentional generative
adversarial networks. In Proc. CVPR , 2018. 2
[74] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun,
Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Ad-
vancing high-resolution video-language representation with
large-scale video transcriptions. In Proc. CVPR , 2022. 3[75] Yuyang Yin, Dejia Xu, Zhangyang Wang, Yao Zhao, and
Yunchao Wei. 4DGen: Grounded 4D content genera-
tion with spatial-temporal consistency. arXiv preprint
arXiv:2312.17225 , 2023. 3
[76] Paul Yoo, Jiaxian Guo, Yutaka Matsuo, and Shixiang Shane
Gu. Dreamsparse: Escaping from plato’s cave with
2d diffusion model given sparse views. arXiv preprint
arXiv:2306.03414 , 2023. 3
[77] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and
Jiebo Luo. Image captioning with semantic attention. In Proc.
CVPR , 2016. 2
[78] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan
Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei
Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana
Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui
Wu. Scaling autoregressive models for content-rich text-to-
image generation. arXiv preprint arXiv:2206.10789 , 2022.
2
[79] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller,
Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian
Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-
modal models: Pretraining and instruction tuning. arXiv
preprint arXiv:2309.02591 , 2023. 2
[80] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-
gang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-
GAN: Text to photo-realistic image synthesis with stacked
generative adversarial networks. In Proc. ICCV , 2017. 2
[81] Yuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhen-
guo Li, and Gim Hee Lee. Animate124: Animating one image
to 4D dynamic scene. arXiv preprint arXiv:2311.14603 , 2023.
3
[82] Yufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Otmar
Hilliges, and Shalini De Mello. A unified approach for
text-and image-guided 4D scene generation. arXiv preprint
arXiv:2311.16854 , 2023. 3
[83] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,
Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video
generation with latent diffusion models. arXiv preprint
arXiv:2211.11018 , 2022. 3
8006
