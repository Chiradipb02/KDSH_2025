CrossKD: Cross-Head Knowledge Distillation for Object Detection
Jiabao Wang1*, Yuming Chen1∗, Zhaohui Zheng1, Xiang Li2,1, Ming-Ming Cheng2,1, Qibin Hou2,1†
1VCIP, College of Computer Science, Nankai University
2NKIARI, Shenzhen Futian
https://github.com/jbwang1997/CrossKD
Abstract
Knowledge Distillation (KD) has been validated as an
effective model compression technique for learning com-
pact object detectors. Existing state-of-the-art KD methods
for object detection are mostly based on feature imitation.
In this paper, we present a general and effective prediction
mimicking distillation scheme, called CrossKD, which de-
livers the intermediate features of the student’s detection
head to the teacher’s detection head. The resulting cross-
head predictions are then forced to mimic the teacher’s pre-
dictions. This manner relieves the student’s head from re-
ceiving contradictory supervision signals from the annota-
tions and the teacher’s predictions, greatly improving the
student’s detection performance. Moreover, as mimicking
the teacher’s predictions is the target of KD, CrossKD of-
fers more task-oriented information in contrast with feature
imitation. On MS COCO, with only prediction mimicking
losses applied, our CrossKD boosts the average precision
of GFL ResNet-50 with 1× training schedule from 40.2 to
43.7, outperforming all existing KD methods. In addition,
our method also works well when distilling detectors with
heterogeneous backbones.
1. Introduction
Knowledge Distillation (KD), serving as a model compres-
sion technique, has been deeply studied in object detec-
tion [5, 13, 29, 31, 55,59,60,73,74] and has received
excellent performance recently. According to the distil-
lation position of the detectors, existing KD methods can
be roughly classified into two categories: prediction mim-
icking and feature imitation. Prediction mimicking (See
Fig. 1(a)) was first proposed in [24], which points out that
the smooth distribution of the teacher’s predictions is more
comfortable for the student to learn than the Dirac distribu-
*Equal contribution.
†Corresponding author.
Figure 1. Comparisons between conventional KD methods and
our CrossKD. Rather than explicitly enforcing the consistency
between the intermediate feature maps or the predictions of the
teacher-student pair, CrossKD implicitly builds the connection be-
tween the heads of the teacher-student pair to improve the distilla-
tion efficiency.
tion of the ground truths. In other words, prediction mim-
icking forces the student to resemble the prediction distri-
bution of the teacher. Differently, feature imitation (See
Fig. 1(b)) follows the idea proposed in FitNet [53], which
argues that intermediate features contain more information
than the predictions from the teacher. It aims to enforce the
feature consistency between the teacher-student pair.
Prediction mimicking plays a vital role in distilling ob-
ject detection models. However, it has been observed to be
inefficient than feature imitation for a long time. Recently,
Zheng et al. [73] proposed a localization distillation (LD)
method that improves prediction mimicking by transferring
localization knowledge, which pushes the prediction mim-
icking to a new level. Despite just catching up with the ad-
vanced feature imitation methods, e.g., PKD [5], LD shows
that prediction mimicking has the ability to transfer task-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
16520
(a
) ground-truth (c) prediction mimicking (b) teacher predictions (d) CrossKD
0.
000.501.00Figure 2. Visualizations of the classification predictions from the GFL [35]. (a) and (b) are ground truth and distillation targets. (c) and (d)
are the classification outputs predicted by models training with conventional prediction mimicking and proposed CrossKD. In the green
circled areas, the distillation targets predicted by the teacher have a large discrepancy with the ground-truth targets assigned to the student.
prediction mimicking forces the student to mimic the teacher, while CrossKD can smooth the mimicking process.
specific knowledge, which benefits the student from the or-
thogonal aspect to feature imitation. This motivates us to
further explore and improve prediction mimicking.
Through investigation, we observe that conventional pre-
diction mimicking may suffer from a conflict between the
ground-truth targets from the student’s assigner and the dis-
tillation targets predicted from the teacher. When train-
ing a detector with prediction mimicking, the student’s pre-
dictions are forced to mimic both the ground-truth targets
and the teacher’s predictions simultaneously. However, the
distillation targets predicted by the teacher usually have
a large discrepancy with the ground-truth targets assigned
to the student. As shown in Fig. 2(a) and Fig. 2(b), the
teacher produces class probabilities in the green circled ar-
eas, which conflicts with the ground-truth targets assigned
to the student. As a result, the student detector experiences
a contradictory learning process during distillation, which
seriously interferes with optimization.
To alleviate the above conflict, previous prediction mim-
icking methods [13, 19,73] tend to conduct the distillation
within regions containing mediate teacher-student discrep-
ancies. However, we argue that the heavily uncertain re-
gions generally accommodate more information that is ben-
eficial to the student. In this paper, we present a novel
cross-head knowledge distillation pipeline, abbreviated as
CrossKD. As illustrated in Fig. 1(c), We propose to feed the
intermediate features from the head of the student to that of
the teacher, yielding the cross-head predictions. Then, the
KD operations can be conducted between the new cross-
head predictions and the teacher’s predictions.
Despite its simplicity, CrossKD offers the following two
main advantages. First, since both the cross-head predic-
tions and the teacher’s predictions are produced by sharing
part of the teacher’s detection head, the cross-head predic-
tions are relatively consistent with the teacher’s predictions.
This relieves the discrepancy between the teacher-student
pair and enhances the training stability of prediction mim-
icking. In addition, as mimicking the teacher’s predictionsis the target of KD, CrossKD is theoretically optimal and
can offer more task-oriented information compared with
feature imitation. Both advantages enable our CrossKD to
efficiently distill knowledge from the teacher’s predictions
and hence result in even better performance than previous
state-of-the-art feature imitation methods.
Without bells and whistles, our method can significantly
boost the performance of the student detector, achieving a
faster training convergence. Comprehensive experiments on
the COCO [40] dataset are conducted in this paper to elabo-
rate the effectiveness of CrossKD. Specifically, with only
prediction mimicking losses applied, CrossKD achieves
43.7 AP on GFL with 1× training schedule, which is 3.5
AP higher than the baseline, surpassing all previous state-
of-the-art object detection KD methods. Moreover, exper-
iments also indicate our CrossKD is orthogonal to feature
imitation methods. By combining CrossKD with the state-
of-the-art feature imitation method, like PKD [5], we fur-
ther achieve 43.9 AP on GFL. Furthermore, we also show
that our method can be used to distill detectors with hetero-
geneous backbones and performs better than other methods.
2. Related Work
2.1. Object Detection
Object detection is one of the most fundamental computer
vision tasks, which requires recognizing and localizing ob-
jects simultaneously. Modern object detectors can be briefly
divided into two categories: one-stage [3, 10,11,35,39,
50,56,69] detectors and two-stage [8, 17,18,20,21,
38,51,57,72] detectors. Among them, one-stage detec-
tors, also known as dense detectors, have emerged as the
mainstream trend in detection due to their excellent speed-
accuracy trade-off.
Dense object detectors have received great attention
since YOLOv1 [48]. Typically, YOLO series detectors [2,
16,43,48–50] attempt to balance the model size and their
accuracy to meet the requirement of real-world applica-
tions. Anchor-free detectors [27, 56,75] attempt to dis-
16521
card the design of anchor boxes to avoid time-consuming
box operations and cumbersome hyper-parameter tuning.
Dynamic label assignment methods [15, 46, 69] are pro-
posed to better define the positive and negative samples
for model learning. GFL [34, 35] introduces Quality Fo-
cal Loss (QFL) and a Distribution-Guided Quality Pre-
dictor to increase the consistency between the classifica-
tion score and the localization quality. It also models the
bounding box representation as a probability distribution
so that it can capture the localization ambiguity of the box
edges. Recently, attributing to the strong ability of the trans-
former block to encode expressive features, DETR fam-
ily [4, 6, 30, 41, 44, 67, 76] has become a new trend in
the object detection community.
2.2. Knowledge Distillation for Object Detection
Knowledge Distillation (KD) is an effective technique to
transfer knowledge from a large-scale teacher model to a
small-scale student model. It has been widely studied in
the classification task [12, 23, 26, 36, 37, 45, 47, 53, 62,
66, 70, 71], but it is still challenging to distill detection
models because of the extreme background ratio. The pi-
oneer work [7] proposes the first distillation framework
for object detection by simply combining feature imitation
and prediction mimicking. Since then, feature imitation
has attracted more and more research attention. Typically,
some works [13, 25, 33, 60] focus on selecting effective
distillation regions for better feature imitation, while other
works [19, 31, 74] aim to weight the imitation loss better.
There are also methods [5, 64, 65, 68] attempting to de-
sign new teacher-student consistency functions, aiming to
explore more consistency information or release the strict
limit of the MSE loss.
As the earliest distillation strategy proposed in [24], pre-
diction mimicking plays a vital role in classification distilla-
tion. Recently, some improved prediction mimicking meth-
ods have been proposed to adapt to object detection. For
example, Rank Mimicking [31] regards the score rank of
the teacher as a kind of knowledge and aims to force the
student to rank instances as the teacher. LD [73] proposes
to distill the localization distribution of bounding box [35]
to transfer localization knowledge. In this paper, we con-
struct a CrossKD pipeline which separates detection and
distillation into different heads to alleviate the target con-
flict problem of prediction mimicking. It’s worth noting
that HEAD [58] delivers the student features to an indepen-
dent assistant head to bridge the gap between heterogeneous
teacher-student pairs. In contrast, we observe that sim-
ply delivering the student feature to the teacher is effective
enough to achieve SOTA results. This makes our method
quite concise and different from HEAD. Our method is also
related to [1, 28, 32, 63], but all of them aim to distill clas-
sification models and are not tailored for object detection.
/uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018
/uni00000037/uni0000004b/uni00000055/uni00000048/uni00000056/uni0000004b/uni00000052/uni0000004f/uni00000047/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013 /uni00000035/uni00000048/uni00000057/uni0000004c/uni00000051/uni00000044/uni00000031/uni00000048/uni00000057 /uni00000024/uni00000037/uni00000036/uni00000036 /uni0000002a/uni00000029/uni0000002fFigure 3. Statistics of the target conflict degree between stu-
dent (GFL-R50) and teacher (GFL-R101, ATSS-R101, RetinaNet-
R101). X-axis is the teacher-student discrepancy threshold for
conflict areas. Y-axis represents the ratios of the target conflict
areas to the positive areas.
3. Methodology
3.1. Analysis of the Target Conflict Problem
Target conflict is a common issue confronted in conven-
tional prediction mimicking methods. In contrast to the
classification task, which assigns a specific category to ev-
ery image, the labels in advanced detectors are usually dy-
namically assigned and not deterministic. Typically, detec-
tors depend on a hand-crafted principle, i.e., assigner, to
determine the label in each location. In most cases, detec-
tors cannot reproduce the assigner’s labels exactly, which
results in a conflict between the teacher-student targets in
KD. Furthermore, the inconsistency of the assigners of the
student and teacher in real-world scenarios extends the dis-
tance between the ground-truth and distillation targets.
To quantitatively measure the degree of target conflict,
we statistic the ratios of conflict areas to the positive areas
under different teacher-student discrepancy in the COCO
minival dataset and report the results in Fig. 3. As we can
see, even if both the teacher (ATSS [69] and GFL [35])
and student (GFL) have the same label assignment strategy,
there are still numerous locations that have a discrepancy
larger than 0.5 between the ground-truth and distillation tar-
gets, respectively. When we use a teacher with a different
assigner (RetinaNet) to distill the student (GFL), the con-
flict areas increases by a large margin. More experiments
in Sec. 4.5 also demonstrate that the target conflict problem
severely hinders the performance of prediction mimicking.
Despite the large influence of target conflict, this prob-
lem has been neglected for a long time in previous predic-
tion mimicking methods [24, 31]. These methods intend
to directly minimize the discrepancy between the teacher-
student predictions. Its objective can be described as:
LKD=1
|S|X
r∈RS(r)Dpred(ps(r),pt(r)), (1)
where psandptare the prediction vectors generated by the
detection heads of the student and the teacher, respectively.
16522
Backbone
(w/ neck)
Backbone
(w/ neck)
 
 
ℒ
 
̂
 
 
 
ℒ
   
Teacher
Student
Student forward path
Teacher forward path
Distillation backprop path
Features
Supervised backprop path
Free
Frozen
Head
Head
Head
Head
Head
Head
Head
Head
 
1
 
+
1
 
 
1
 
+
1
 
 
  Figure 4. Overall framework of the proposed CrossKD. For a given teacher-student pair, CrossKD first delivers the intermediate features of
the student into the teacher layers and generates the cross-head predictions ˆps. Then, distillation losses are calculated between the original
teacher’s predictions and the cross-head predictions of the student. In back-propagation, the gradients with respect to the detection loss
normally pass through the student detection head, while the distillation gradients propagate through the frozen teacher layers.
Dpred(·)refers to the loss function calculating the discrep-
ancy between psandpt,e.g., KL Divergence [24] for clas-
sification, L1 Loss [7] and LD [73] for regression. S(·)is
the region selection principle which produces a weight at
each position rin the entire image region R.
It’s worth noting that S(·), to a certain extent, can al-
leviate the target conflict problem by down-weighting the
regions with large teacher-student discrepancies. However,
the heavily uncertain regions usually accommodate more
information benefits for the student than undisputed areas.
Ignoring those regions may have a large impact on the effec-
tiveness of prediction mimicking methods. Consequently,
to push the envelope of prediction mimicking, it is neces-
sary to handle the target conflict problem gracefully instead
of directly down-weighting.
3.2. Cross-Head Knowledge Distillation
As described in Sec. 3.1, we observe that directly mim-
icking the predictions of the teacher confronts the tar-
get conflict problem, which hinders prediction mimicking
achieving promising performance. To alleviate this prob-
lem, we present a novel Cross-head Knowledge Distillation
(CrossKD) in this section. The overall framework is illus-
trated in Fig. 4. Like many previous prediction mimicking
methods, our CrossKD performs the distillation process on
the predictions. Differently, CrossKD delivers the interme-
diate features of the student to the teacher’s detection head
and generates cross-head predictions to conduct distillation.
Given a dense detector, like RetinaNet [39], each detec-
tion head usually consists of a sequence of convolutional
layers, represented as {Ci}. For simplicity, we suppose
there are totally nconvolutional layers in each detection
head (e.g., 5 in RetinaNet with 4 hidden layers and 1 pre-
diction layer). We use fi, i∈ {1,2,···, n−1}to denote
the feature maps produced by Ciandf0the input featuremaps of C1. The predictions pare generated by the last
convolutional layer Cn. Thus, for a given teacher-student
pair, the predictions of the teacher and the student can be
represented as ptandps, respectively.
Besides the original predictions from the teacher and the
student, CrossKD additionally delivers the student’s inter-
mediate features fs
i, i∈ {1,2,···, n−1}toCt
i+1, the
(i+1)-th convolutional layer of the teacher’s detection head,
resulting in the cross-head predictions ˆps. Given ˆps, instead
of computing the KD loss between psandpt, we propose to
use the KD loss between the cross-head predictions ˆpsand
the original predictions of the teacher ptas the objective of
our CrossKD, which is described as follows:
LCrossKD =1
|S|X
r∈RS(r)Dpred(ˆps(r),pt(r)), (2)
where S(·)and|S|are the region selection principle and
the normalization factor. Instead of designing complicated
S(·), we equally conduct distillation between ˆpsandpt
over the entire prediction map. Specifically, S(·)is a con-
stant function with the value of 1 in our CrossKD. Accord-
ing to the different tasks of each branch (e.g., classification
or regression), we perform different types of Dpred(·)to ef-
fectively deliver task-specific knowledge to the student.
By performing CrossKD, the detection loss and the dis-
tillation loss are separately applied to different branches. As
illustrated in Fig. 4, the gradients of the detection loss pass
through the entire head of the student, while the gradients of
distillation loss propagate through the frozen teacher layers
to the latent features of the student, which heuristically in-
creases the consistency between the teacher and the student.
Compared to directly closing the predictions between the
teacher-student pair, CrossKD allows part of the student’s
detection head to be only relative with detection losses, re-
16523
Table 1. Effectiveness of applying CrossKD at different positions.
The index irepresents the intermediate features used as input in
the cross-head branches. ‘LD’ means the direct application of
prediction mimicking on the student’s head with LD [73]. The
teacher-student pair is GFL with ResNet-50 and ResNet-18 back-
bones. We can see that i= 3 yields the best performance in this
experiment.
i AP AP 50 AP75 APS APM APL
- 35.8 53.1 38.2 18.9 38.9 47.9
0 38.2 55.6 41.3 20.2 41.9 50.9
1 38.3 55.8 41.1 20.8 42.1 49.8
2 38.6 56.2 41.5 20.8 42.7 50.7
3 38.7 56.3 41.6 21.1 42.2 51.1
4 38.2 55.7 41.2 20.3 41.9 50.2
LD 37.8 55.5 40.5 20.0 41.4 49.5
sulting in a better optimization towards ground-truth targets.
Quantitative analysis is presented in our experiment section.
3.3. Optimization Objectives
The overall loss for training can be formulated as the
weighted sum of the detection loss and the distillation loss,
written as:
L=Lcls(ps
cls,pgt
cls) +Lreg(ps
reg,pgt
reg)
+Lcls
CrossKD (ˆps
cls,pt
cls) +Lreg
CrossKD (ˆps
reg,pt
reg),(3)
where LclsandLregstand for the detection losses which
are calculated between the student predictions ps
cls,ps
regand
their corresponding ground truth targets pgt
cls,pgt
reg. The ad-
ditional CrossKD losses are represented as Lcls
CrossKD and
Lreg
CrossKD , which are performed between the cross-head pre-
dictions ˆps
cls,ˆps
regand the teacher’s predictions pt
cls,pt
reg.
We apply different distance functions Dpredto transfer
task-specific information in different branches. In the clas-
sification branch, we regard the classification scores pre-
dicted by the teacher as the soft labels and directly use Qual-
ity Focal Loss (QFL) proposed in GFL [35] to pull close the
teacher-student distance. As for regression, there are mainly
two types of regression forms presenting in dense detec-
tors. The first regression form directly regresses the bound-
ing boxes from the anchor boxes (e.g., RetinaNet [39],
ATSS [69]) or points (e.g., FCOS [56]). In this case, we
directly use GIoU [52] as Dpred. In the other situation, the
regression form predicts a vector to represent the distribu-
tion of box location (e.g., GFL [35]), which contains richer
information than the Dirac distribution of the bounding box
representation. To efficiently distill the knowledge of loca-
tion distribution, we employ KL divergence, like LD [73],
to transfer localization knowledge. More details about the
loss functions are given in the supplementary materials.Table 2. Comparisons between feature imitation and CrossKD.
we choose advanced PKD to represent feature imitation and apply
PKD to different positions to compare with CrossKD fairly. Here,
‘neck’ means performing PKD on the FPN neck. ’cls’ and ’reg’
indicate applying PKD to the classification branch and the regres-
sion, respectively. The teacher-student pair is GFL with ResNet-50
and ResNet-18 backbones.
Methods AP AP 50 AP75 APS APM APL
- 35.8 53.1 38.2 18.9 38.9 47.9
PKD:neck 38.0 55.0 41.2 19.6 41.5 50.2
PKD:cls 37.5 54.9 40.5 19.5 41.1 50.5
PKD:reg 37.2 54.0 40.2 19.0 40.9 50.0
PKD:cls+reg 37.3 54.3 40.0 19.2 41.1 49.8
CrossKD 38.7 56.3 41.6 21.1 42.2 51.1
4. Experiments
4.1. Implement Details
We evaluate the proposed method on the large-scale MS
COCO [40] benchmark as done in most previous works. To
ensure consistency with the standard practice, we use the
trainval135k set (115 Kimages) for training and the minival
set (5 Kimages) for validation. For evaluation, the standard
COCO-style measurement, i.e., Average Precision (AP), is
used. We also report mAP with IoU thresholds of 0.5 and
0.75, as well as AP for small, medium, and large objects.
Our proposed method, CrossKD, is implemented under the
MMDetection [9] framework in Python. For a fair compar-
ison, all experiments are developed using 8 Nvidia V100
GPUs with a minibatch of two images per GPU. Unless
otherwise stated, all the hyper-parameters follow the default
settings of the corresponding student model for both train-
ing and testing.
4.2. Method Analysis
To investigate the effectiveness of our method, we conduct
extensive ablation experiments based on GFL [35]. If not
specified, we use GFL with the ResNet-50 backbone [22]
as the teacher detector and use the ResNet-18 backbone in
the student detector. The accuracy of the teacher and the
student are 40.2 AP and 35.8 AP, respectively. All experi-
ments follow the default 1 ×training schedule (12 epochs).
Positions to apply CrossKD. As described in Sec. 3.2,
CrossKD delivers the i-th intermediate feature of the stu-
dent to part of the teacher’s head. Here, we conduct dis-
tillation on both classification and box regression branches.
When i= 0, CrossKD directly feeds the student’s FPN fea-
tures into the teacher’s head. In this case, the entire stu-
dent’s head is only supervised by the detection loss, and no
distillation loss is involved. As igradually increases, more
layers of the student’s head are jointly affected by the detec-
16524
0.000.25 0.751.00
0.50  Feature Imitation (PKD ) CrossKD GradientReg Branch ClsBranchFeature Imitation (PKD ) CrossKD
Figure 5. Visualizations of the gradients w.r.t feature imitation and CrossKD. The visualization demonstrates that our CrossKD guided by
prediction mimicking can effectively focus on the potentially valuable regions.
Table 3. Effectiveness of CrossKD on different branches. We sep-
arately apply CrossKD on the classification (cls) and regression
(reg) branches. The teacher-student pair is GFL with ResNet-50
and ResNet-18 backbones.
cls regLD CrossKD
AP AP 50 AP75 AP AP 50 AP75
✓ 37.3 55.2 40.0 37.7 55.6 40.2
✓ 36.8 53.8 39.6 37.2 54.0 40.0
✓ ✓ 37.8 55.4 40.5 38.7 56.3 41.6
tion loss and the distillation loss. When i=n, our method
degrades to the original prediction mimicking, where the
distillation loss will be directly performed between the two
predictions of the teacher-student pair.
In Tab. 1, we report the results of performing CrossKD
on different intermediate features. One can see that our
CrossKD can improve the distillation performance for all
the choices of i. Notably, when using the 3-rd intermediate
features, CrossKD reaches the best performance of 38.7 AP,
which is 0.9 AP higher than the recent state-of-the-art pre-
diction mimicking method LD [73]. This suggests that not
all layers in the student’s head need to be isolated from the
influence of the distillation loss. Therefore, we use i= 3as
the default setting in all subsequent experiments.
CrossKD v.s. Feature Imitation. We compare CrossKD
with the advanced feature imitation method PKD [5]. For a
fair comparison, we perform PKD on the same positions as
our CrossKD, including FPN features and the third layer of
detection heads. The results are reported in Tab. 2. It can
be seen that PKD can achieve 38.0 AP when it is applied
between the FPN features of the teacher-student pair. On
the detection head, PKD even shows a performance drop.
In contrast, our CrossKD achieves 38.7 AP, which is 0.7
AP higher than PKD applied on the FPN features.
To further investigate the advantage of CrossKD, we vi-
sualize the gradients on the latent features of the detectionTable 4. Collective effect of CrossKD and prediction mimicking.
The teacher-student pair is GFL with ResNet-50 and ResNet-18
backbones.
CrossKD LD AP AP 50 AP75 APSAPM APL
- - 35.8 53.1 38.2 18.9 38.9 47.9
✓ 38.7 56.3 41.6 21.1 42.2 51.1
✓ 37.8 55.5 40.5 20.0 41.4 49.5
✓ ✓ 38.1 55.6 40.9 20.4 41.6 51.1
head, as shown in Fig. 5. As illustrated, the gradients gen-
erated by PKD have a large and wide impact on the entire
feature maps, which is inefficient and not targeted. On the
contrary, the gradients generated by CrossKD can focus on
potential semantic areas with objects of interest.
CrossKD v.s. Prediction Mimicking. We begin by sep-
arately performing prediction mimicking and CrossKD on
the classification and box regression branches. The results
are reported in Tab. 3. One can see that replacing predic-
tion mimicking with CrossKD leads to a stable performance
gain regardless of classification or regression branches.
Specifically, prediction mimicking produces 37.3 AP and
36.8 AP on the classification and regression branches, re-
spectively, while CrossKD yields 37.7 AP and 37.2 AP, rep-
resenting a consistent improvement over the corresponding
results of prediction mimicking. If KD is performed on the
two branches, our method can still outperform prediction
mimicking by +0.9 AP. Moreover, we further evaluate the
collective effect of prediction mimicking and CrossKD, as
shown in Tab. 4. Intriguingly, we observe that using both
prediction mimicking and CrossKD together yields a final
result of 38.1 AP, which is even lower than the result of
using CrossKD alone. We believe that this is because the
prediction mimicking introduces the target conflict problem
again, which makes the student model struggle to learn.
In addition, we visualize the statistical variation during
16525
1234567891011122.02.53.03.54.04.5
1234567891011121.601.651.701.751.801.851.90
w/o…distillation Prediction…Mimicking CrossKD123456789101112283032343638
(a) Avg. L1distance between psandpt(b) Avg. L1distance between psandpgt(c) AP
Figure 6. Visualization for the variation of statistics during training. (a) Curves of average L1distance between student predictions psand
teacher’s pt. (b) Curves of average L1distance between student predictions psand positive ground truth targets pgt. (c) Curves of Average
Precision (AP). All curves are evaluated on the COCO minival set. X-axis refers to the epoch number. Y-axis in (a) and (b) indicate the
average L1distance, while in (c) means the value of AP.
Table 5. Comparison with state-of-the-art detection KD meth-
ods on COCO. * denotes results are referenced from LD [73] and
PKD [5]. All results are evaluated on the COCO minival set.
Method AP AP 50AP75APSAPMAPL
GFL-R101 (T) 44.9 63.1 49.0 28.0 49.1 57.2
GFL-R50 (S) 40.2 58.4 43.3 23.3 44.0 52.2
FitNets* [53] 40.7 (0.5↑) 58.6 44.0 23.7 44.4 53.2
Inside GT Box* 40.7 (0.5↑) 58.6 44.2 23.1 44.5 53.5
Defeat* [19] 40.8 (0.6↑) 58.6 44.2 24.3 44.6 53.7
Main Region* [73] 41.1 (0.9↑) 58.7 44.4 24.1 44.6 53.6
Fine-Grained* [61] 41.1 (0.9↑) 58.8 44.8 23.3 45.4 53.1
FGD [64] 41.3 (1.1↑) 58.8 44.8 24.5 45.6 53.0
GID* [13] 41.5 (1.3↑) 59.6 45.2 24.3 45.7 53.6
SKD [14] 42.3 (2.1↑) 60.2 45.9 24.4 46.7 55.6
LD [73] 43.0 (2.8↑) 61.6 46.6 25.5 47.0 55.8
PKD* [5] 43.3 (3.1↑) 61.3 46.9 25.2 47.9 56.2
CrossKD 43.7 (3.5↑) 62.1 47.4 26.9 48.0 56.2
CrossKD+PKD 43.9 (3.7↑) 62.0 47.7 26.4 48.5 57.0
training to conduct further analysis on CrossKD and predic-
tion mimicking. We first calculate the L1distances between
the student’s predictions psand the teacher’s predictions
pt, as well as the ground-truth targets pgtat each epoch.
As plotted in Fig. 6(a), the distance L1(ps,pt)can be re-
duced significantly by our CrossKD, while it is reasonable
for the prediction mimicking to achieve the lowest distance
as the distillation is directly imposed on ps. However, as the
optimization target conflict exists, the prediction mimick-
ing involves a contradictory optimization process, thereby
generally yielding a larger distance L1(ps,pgt)than our
CrossKD, as shown in Fig. 6(b). In Fig. 6(c), our method
shows a faster training process and achieves the best perfor-
mance of 37.8 AP.Table 6. CrossKD for detectors with homogeneous backbones.
Teacher detectors use ResNet-101 as the backbone, while the stu-
dents use ResNet-50 as the backbone. All results are evaluated on
the COCO minival set.
Student Methods AP AP 50AP75APSAPMAPL
RetinaNet [39]R101 38.9 58.0 41.5 21.0 32.8 52.4
R50 37.4 56.7 39.6 20.0 40.7 49.7
CrossKD 39.7 58.9 42.5 22.4 43.6 52.8
FCOS [56]R101 40.8 60.0 44.0 24.2 44.3 52.4
R50 38.5 57.7 41.0 21.9 42.8 48.6
CrossKD 41.3 60.6 44.2 25.1 45.5 52.4
ATSS [69]R101 41.5 59.9 45.2 24.2 45.9 53.3
R50 39.4 57.6 42.8 23.6 42.9 50.3
CrossKD 41.8 60.1 45.4 24.9 45.9 54.2
4.3. Comparison with SOTA KD Methods
In this section, we evaluate various state-of-the-art object
detection KD methods on the GFL [35] framework and
fairly compare them with our proposed CrossKD. We use
ResNet-101 as the backbone for the teacher detector, which
is trained with a 2× schedule and multi-scale augmenta-
tion. For the student detector, we adopt the ResNet-50 back-
bone. We train the student with the 1× schedule. The pre-
trained checkpoint of the teacher is directly borrowed from
the MMDetection[9] model zoo.
We report all results in Tab. 5. As we can see, at the
same condition, CrossKD can achieve 43.7 AP without bells
and whistles, which improves the accuracy of the student
by 3.5 AP, outperforming all other state-of-the-art methods.
Notably, CrossKD surpasses the advanced feature imitation
method PKD by 0.4 AP and surpasses the advanced predic-
tion mimicking method LD by 0.7 AP, demonstrating the
effectiveness of CrossKD. In addition, we also observe that
16526
Table 7. CrossKD for teacher-student pairs with different label
assigners. All results are evaluated on the COCO minival set.
Methods AP AP 50AP75APSAPMAPL
GFL-R50 (S) 40.2 58.4 43.3 23.3 44.0 52.2
ATSS[69]-R101 (T) 41.5 59.9 45.2 24.2 45.9 53.3
KD 39.7 57.9 42.8 21.8 44.2 51.5
CrossKD 42.1 60.5 45.7 24.5 46.3 54.5
GFL-R50 (S) 40.2 58.4 43.3 23.3 44.0 52.2
Retinanet[39]-R101 (T) 38.9 58.0 41.5 21.0 32.8 52.4
KD 30.3 49.2 31.2 20.0 38.1 34.4
CrossKD 41.2 59.4 44.8 24.0 45.1 53.5
CrossKD is also orthogonal to the feature imitation meth-
ods. With the help of PKD, CrossKD achieves the high-
est results of 43.9 AP, achieving an improvement of 3.7 AP
compared to the baseline.
4.4. CrossKD on Different Detectors
Besides performing CrossKD on GFL, we select three com-
monly used detectors, i.e., RetinaNet[39], FCOS [56], and
ATSS [69], to investigate the effectiveness of CrossKD. We
strictly follow the student settings for training and refer-
ence the teacher and student results from the MMDetection
model zoo. The results are presented in Tab. 6. As shown
in Tab. 6, CrossKD significantly boosts the performance of
all three types of detectors. Specifically, RetinaNet, FCOS,
and ATSS with our CrossKD achieve 39.7 AP, 41.3 AP, and
41.8 AP, respectively, which are 2.3 AP, 2.8 AP, and 2.4 AP
higher than their corresponding baselines. All results after
distillation even outperform the original teachers, indicating
that CrossKD can work well on different dense detectors.
4.5. Distillation under Severe Target Conflict
In this subsection, we perform prediction mimicking and
our CrossKD between detectors with different assigners to
explore the effectiveness of CrossKD against the target con-
flict problem. As shown in Tab. 7, the target conflict prob-
lem has a large impact on the optimization of the student,
leading to an inferior performance. Specifically, predic-
tion mimicking reduces the AP to 30.3 with the teacher as
RetinaNet which has a different assigner with GFL. Fur-
thermore, even if the ATSS has the same assigner as GFL,
the student’s AP is only distilled to 39.7, falling below the
performance without KD. In contrast, CrossKD can still sig-
nificantly improve the student’s accuracy even if existing a
large discrepancies between the ground-truth and distilla-
tion targets. CrossKD boosts the accuracy of GFL-R50 to
42.1 (+1.9 AP) when applying ATSS as the teacher. Even
guided by a weak teacher ReitnaNet, CrossKD still im-
proves the performance of GFL-R50 to 41.2 AP, 1.0 AP
higher than the baseline. This demonstrates how robust ourTable 8. CrossKD for other detector pairs with Heterogeneous
Backbones. For convenience, only the backbone lists below, where
‘SwinT’ refers to RetinaNet with a tiny version of Swin Trans-
former [42]. All results are evaluated on the COCO minival set.
Methods AP AP 50AP75APSAPMAPL
SwinT (T) [42] 37.3 57.5 39.9 22.7 41.0 49.6
ResNet-50 (S) 36.5 55.4 39.1 20.4 40.3 48.1
PKD 37.2 56.7 39.5 21.2 41.2 49.7
CrossKD 38.0 58.1 40.5 23.1 41.8 49.7
ResNet-50 (T) 36.5 55.4 39.1 20.4 40.3 48.1
MobileNetv2 (S) [54] 30.9 48.7 32.5 16.3 33.5 41.9
PKD 33.2 51.3 35.0 16.5 36.6 46.5
CrossKD 34.1 52.7 36.5 18.8 37.1 45.4
CrossKD is when confronting severe target conflict.
4.6. Distillation between Heterogeneous Backbones
In this subsection, we explore the ability of our CrossKD for
distilling the heterogeneous students. We perform knowl-
edge distillation on RetinaNet [39] with different back-
bone networks and compare our method with the recent
state-of-the-art method PKD [5]. Specifically, we choose
two typical heterogeneous backbones, i.e., the transformer
backbone Swin-T [42] and the lightweight backbone Mo-
bileNetv2 [54]. All the detectors are trained for 12 epochs
with a single-scale strategy. The results are presented in
Tab. 8. We can see when distilling knowledge from Swin-T,
CrossKD reaches 38.0 AP (+1.5 AP), outperforming PKD
by 0.8 AP. CrossKD also improves the results of RetinaNet
with the MoblieNetv2 backbone to 34.1 AP, which is 3.2
AP higher than the baseline and surpasses PKD by 0.9 AP.
5. Conclusions and Discussions
In this paper, we introduce CrossKD, a novel KD method
designed to enhance the performance of dense object detec-
tors. CrossKD transfers the intermediate features from the
student’s head to that of the teacher to produce the cross-
head predictions for distillation, an efficient way to alleviate
the conflict between the supervised and distillation targets.
Our results have shown that CrossKD can improve the dis-
tillation efficiency and achieve state-of-the-art performance.
In the future, we will further extend our method to other rel-
evant fields, e.g. 3D object detection.
Acknowledgments. This research was supported by NSFC
(NO. 62225604, NO. 62276145), the Fundamental Re-
search Funds for the Central Universities (Nankai Univer-
sity, 070-63223049), CAST through Young Elite Scien-
tist Sponsorship Program (No. YESS20210377). Com-
putations were supported by the Supercomputing Center of
Nankai University (NKSC).
16527
References
[1] Haoli Bai, Jiaxiang Wu, Irwin King, and Michael Lyu. Few
shot network compression via cross distillation. In Proceed-
ings of the AAAI Conference on Artificial Intelligence, 2020.
[2] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-
Yuan Mark Liao. Yolov4: Optimal speed and accuracy of
object detection. arXiv preprint arXiv:2004.10934, 2020.
[3] Qinglong Cao, Zhengqin Xu, Yuantian Chen, Chao Ma, and
Xiaokang Yang. Domain-controlled prompt learning. arXiv
preprint arXiv:2310.07730, 2023.
[4] Qinglong Cao, Zhengqin Xu, Yuntian Chen, Chao Ma, and
Xiaokang Yang. Domain prompt learning with quaternion
networks. arXiv preprint arXiv:2312.08878, 2023.
[5] Weihan Cao, Yifan Zhang, Jianfei Gao, Anda Cheng, Ke
Cheng, and Jian Cheng. Pkd: General distillation framework
for object detectors via pearson correlation coefficient. In
Advances in Neural Information Processing Systems, 2022.
[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, Au-
gust 23–28, 2020, Proceedings, Part I 16, pages 213–229.
Springer, 2020.
[7] Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Man-
mohan Chandraker. Learning efficient object detection mod-
els with knowledge distillation. Advances in neural informa-
tion processing systems, 30, 2017.
[8] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao
Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi,
Wanli Ouyang, Chen Change Loy, and Dahua Lin. Hybrid
task cascade for instance segmentation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
[9] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu
Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,
Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tian-
heng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu,
Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,
Chen Change Loy, and Dahua Lin. MMDetection: Open
mmlab detection toolbox and benchmark. arXiv preprint
arXiv:1906.07155, 2019.
[10] Shaoyu Chen, Tianheng Cheng, Jiemin Fang, Qian Zhang,
Yuan Li, Wenyu Liu, and Xinggang Wang. Tinydet: accu-
rately detecting small objects within 1 gflops. Science China
Information Sciences, 66(1):119102, 2023.
[11] Yuming Chen, Xinbin Yuan, Ruiqi Wu, Jiabao Wang, Qibin
Hou, and Ming-Ming Cheng. Yolo-ms: Rethinking multi-
scale representation learning for real-time object detection,
2023.
[12] Jang Hyun Cho and Bharath Hariharan. On the efficacy of
knowledge distillation. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision (ICCV), October
2019.
[13] Xing Dai, Zeren Jiang, Zhao Wu, Yiping Bao, Zhicheng
Wang, Si Liu, and Erjin Zhou. General instance distilla-
tion for object detection. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 7842–7851, June 2021.[14] Philip De Rijk, Lukas Schneider, Marius Cordts, and Dariu
Gavrila. Structural knowledge distillation for object detec-
tion. In Advances in Neural Information Processing Systems,
2022.
[15] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R. Scott,
and Weilin Huang. Tood: Task-aligned one-stage object de-
tection. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision (ICCV), pages 3510–3519, Oc-
tober 2021.
[16] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian
Sun. Yolox: Exceeding yolo series in 2021. arXiv preprint
arXiv:2107.08430, 2021.
[17] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE Inter-
national Conference on Computer Vision (ICCV), December
2015.
[18] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detec-
tion and semantic segmentation. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), June 2014.
[19] Jianyuan Guo, Kai Han, Yunhe Wang, Han Wu, Xinghao
Chen, Chunjing Xu, and Chang Xu. Distilling object detec-
tors via decoupled features. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 2154–2164, June 2021.
[20] Jianyuan Guo, Kai Han, Han Wu, Chao Zhang, Xinghao
Chen, Chunjing Xu, Chang Xu, and Yunhe Wang. Positive-
unlabeled data purification in the wild for object detection.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 2653–2662, 2021.
[21] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-
shick. Mask r-cnn. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV), Oct 2017.
[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
[23] Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, No-
jun Kwak, and Jin Young Choi. A comprehensive overhaul
of feature distillation. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision (ICCV), October
2019.
[24] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling
the knowledge in a neural network (2015). arXiv preprint
arXiv:1503.02531, 2, 2015.
[25] Zihao Jia, Shengkun Sun, Guangcan Liu, and Bo Liu. Mssd:
multi-scale self-distillation for object detection. Visual Intel-
ligence, 2(1):8, 2024.
[26] Jangho Kim, SeongUk Park, and Nojun Kwak. Paraphras-
ing complex network: Network compression via factor trans-
fer.Advances in neural information processing systems, 31,
2018.
[27] Tao Kong, Fuchun Sun, Huaping Liu, Yuning Jiang, Lei Li,
and Jianbo Shi. Foveabox: Beyound anchor-based object de-
tection. IEEE Transactions on Image Processing, 29:7389–
7398, 2020.
[28] Animesh Koratana, Daniel Kang, Peter Bailis, and Matei Za-
haria. LIT: Learned intermediate representation training for
model compression. In International Conference on Learn-
16528
ing Representations, 2019.
[29] Yuqing Lan, Yao Duan, Chenyi Liu, Chenyang Zhu, Yueshan
Xiong, Hui Huang, and Kai Xu. Arm3d: Attention-based re-
lation module for indoor 3d object detection. Computational
Visual Media, 8(3):395–414, 2022.
[30] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M. Ni,
and Lei Zhang. Dn-detr: Accelerate detr training by intro-
ducing query denoising. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 13619–13627, June 2022.
[31] Gang Li, Xiang Li, Yujie Wang, Shanshan Zhang, Yichao
Wu, and Ding Liang. Knowledge distillation for object de-
tection via rank mimicking and prediction-guided feature im-
itation. In Proceedings of the AAAI Conference on Artificial
Intelligence, pages 1306–1313, 2022.
[32] Guilin Li, Junlei Zhang, Yunhe Wang, Chuanjian Liu,
Matthias Tan, Yunfeng Lin, Wei Zhang, Jiashi Feng, and
Tong Zhang. Residual distillation: Towards portable deep
neural networks without shortcuts. Advances in Neural In-
formation Processing Systems, 33:8935–8946, 2020.
[33] Quanquan Li, Shengying Jin, and Junjie Yan. Mimicking
very efficient network for object detection. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
[34] Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang,
and Jian Yang. Generalized focal loss v2: Learning reliable
localization quality estimation for dense object detection. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR), pages 11632–11641,
June 2021.
[35] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu,
Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss:
Learning qualified and distributed bounding boxes for dense
object detection. In H. Larochelle, M. Ranzato, R. Hadsell,
M.F. Balcan, and H. Lin, editors, Advances in Neural Infor-
mation Processing Systems, volume 33, pages 21002–21012.
Curran Associates, Inc., 2020.
[36] Zheng Li, Xiang Li, Lingfeng Yang, Borui Zhao, Renjie
Song, Lei Luo, Jun Li, and Jian Yang. Curriculum tempera-
ture for knowledge distillation. In Proceedings of the AAAI
Conference on Artificial Intelligence, number 2, pages 1504–
1512, 2023.
[37] Zheng Li, Jingwen Ye, Mingli Song, Ying Huang, and Zhi-
geng Pan. Online knowledge distillation for efficient pose
estimation. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 11740–11750, 2021.
[38] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature pyramid
networks for object detection. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
[39] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Dollar. Focal loss for dense object detection. In Pro-
ceedings of the IEEE International Conference on Computer
Vision (ICCV), Oct 2017.
[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13, pages 740–755. Springer, 2014.
[41] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi,
Hang Su, Jun Zhu, and Lei Zhang. DAB-DETR: Dynamic
anchor boxes are better queries for DETR. In International
Conference on Learning Representations, 2022.
[42] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,
Zheng Zhang, Stephen Lin, and Baining Guo. Swin trans-
former: Hierarchical vision transformer using shifted win-
dows. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision (ICCV), pages 10012–10022,
October 2021.
[43] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou,
Yudong Wang, Yanyi Liu, Shilong Zhang, and Kai Chen.
Rtmdet: An empirical study of designing real-time object
detectors. arXiv preprint arXiv:2212.07784, 2022.
[44] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng,
Houqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang.
Conditional detr for fast training convergence. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision (ICCV), pages 3651–3660, October 2021.
[45] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir
Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Im-
proved knowledge distillation via teacher assistant. In Pro-
ceedings of the AAAI conference on artificial intelligence,
pages 5191–5198, 2020.
[46] Chuong H. Nguyen, Thuy C. Nguyen, Tuan N. Tang, and
Nam L.H. Phan. Improving object detection by label as-
signment distillation. In Proceedings of the IEEE/CVF Win-
ter Conference on Applications of Computer Vision (WACV),
pages 1005–1014, January 2022.
[47] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho.
Relational knowledge distillation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
[48] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Unified, real-time object de-
tection. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
[49] Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster,
stronger. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), July 2017.
[50] Joseph Redmon and Ali Farhadi. Yolov3: An incremental
improvement. arXiv preprint arXiv:1804.02767, 2018.
[51] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In C. Cortes, N. Lawrence, D. Lee, M.
Sugiyama, and R. Garnett, editors, Advances in Neural Infor-
mation Processing Systems, volume 28. Curran Associates,
Inc., 2015.
[52] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir
Sadeghian, Ian Reid, and Silvio Savarese. Generalized in-
tersection over union: A metric and a loss for bounding box
regression. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), June
2019.
[53] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets:
Hints for thin deep nets. arXiv preprint arXiv:1412.6550,
16529
2014.
[54] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
residuals and linear bottlenecks. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
[55] Lin Song, Jin-Fu Yang, Qing-Zhen Shang, and Ming-Ai Li.
Dense face network: A dense face detector based on global
context and visual attention mechanism. Machine Intelli-
gence Research, 19(3):247–256, 2022.
[56] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos:
Fully convolutional one-stage object detection. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision (ICCV), October 2019.
[57] Jiaqi Wang, Kai Chen, Shuo Yang, Chen Change Loy, and
Dahua Lin. Region proposal by guided anchoring. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
[58] Luting Wang, Xiaojie Li, Yue Liao, Zeren Jiang, Jianlong
Wu, Fei Wang, Chen Qian, and Si Liu. Head: Hetero-assists
distillation for heterogeneous object detectors. In Computer
Vision–ECCV 2022: 17th European Conference, Tel Aviv, Is-
rael, October 23–27, 2022, Proceedings, Part IX, pages 314–
331. Springer, 2022.
[59] Luequan Wang, Hongbin Xu, and Wenxiong Kang. Mv-
contrast: Unsupervised pretraining for multi-view 3d object
recognition. Machine Intelligence Research, 20(6):872–883,
2023.
[60] Tao Wang, Li Yuan, Xiaopeng Zhang, and Jiashi Feng. Dis-
tilling object detectors with fine-grained feature imitation. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR), June 2019.
[61] Tao Wang, Li Yuan, Xiaopeng Zhang, and Jiashi Feng. Dis-
tilling object detectors with fine-grained feature imitation. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR), June 2019.
[62] Chenglin Yang, Lingxi Xie, Chi Su, and Alan L. Yuille.
Snapshot distillation: Teacher-student optimization in one
generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), June
2019.
[63] Jing Yang, Brais Martinez, Adrian Bulat, Georgios Tz-
imiropoulos, et al. Knowledge distillation via softmax re-
gression representation learning. In International Confer-
ence on Learning Representations, 2021.
[64] Zhendong Yang, Zhe Li, Xiaohu Jiang, Yuan Gong, Ze-
huan Yuan, Danpei Zhao, and Chun Yuan. Focal and global
knowledge distillation for detectors. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 4643–4652, June 2022.
[65] Lewei Yao, Renjie Pi, Hang Xu, Wei Zhang, Zhenguo Li,
and Tong Zhang. G-detkd: Towards general distillation
framework for object detectors via contrastive and semantic-
guided feature imitation. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV), pages
3591–3600, October 2021.
[66] Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A
gift from knowledge distillation: Fast optimization, network
minimization and transfer learning. In Proceedings of theIEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), July 2017.
[67] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun
Zhu, Lionel Ni, and Heung-Yeung Shum. DINO: DETR with
improved denoising anchor boxes for end-to-end object de-
tection. In The Eleventh International Conference on Learn-
ing Representations, 2023.
[68] Linfeng Zhang and Kaisheng Ma. Improve object detec-
tion with feature-based knowledge distillation: Towards ac-
curate and efficient detectors. In International Conference
on Learning Representations, 2021.
[69] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and
Stan Z. Li. Bridging the gap between anchor-based and
anchor-free detection via adaptive training sample selection.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), June 2020.
[70] Ying Zhang, Tao Xiang, Timothy M. Hospedales, and
Huchuan Lu. Deep mutual learning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), June 2018.
[71] Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun
Liang. Decoupled knowledge distillation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 11953–11962, June 2022.
[72] Wangbo Zhao, Jiasheng Tang, Yizeng Han, Yibing Song, Kai
Wang, Gao Huang, Fan Wang, and Yang You. Dynamic tun-
ing towards parameter and inference efficiency for vit adap-
tation, 2024.
[73] Zhaohui Zheng, Rongguang Ye, Ping Wang, Dongwei Ren,
Wangmeng Zuo, Qibin Hou, and Ming-Ming Cheng. Local-
ization distillation for dense object detection. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 9407–9416, June 2022.
[74] Du Zhixing, Rui Zhang, Ming Chang, Shaoli Liu, Tianshi
Chen, Yunji Chen, et al. Distilling object detectors with fea-
ture richness. Advances in Neural Information Processing
Systems, 34:5213–5224, 2021.
[75] Chenchen Zhu, Yihui He, and Marios Savvides. Feature se-
lective anchor-free module for single-shot object detection.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
[76] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,
and Jifeng Dai. Deformable {detr}: Deformable transform-
ers for end-to-end object detection. In International Confer-
ence on Learning Representations, 2021.
16530
