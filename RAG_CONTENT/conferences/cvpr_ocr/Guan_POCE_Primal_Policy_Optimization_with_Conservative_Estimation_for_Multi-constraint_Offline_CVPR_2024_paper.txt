POCE: Primal Policy Optimization with Conservative Estimation for
Multi-constraint Offline Reinforcement Learning
Jiayi Guan1†, Li Shen2†, Ao Zhou1, Lusong Li2,Han Hu3,
Xiaodong He2, Guang Chen1‡, Changjun Jiang1
1Tongji University,2JD Explore Academy,3Beijing Institute of Technology
Abstract
Multi-constraint offline reinforcement learning (RL)
promises to learn policies that satisfy both cumulative and
state-wise costs from offline datasets. This arrangement
provides an effective approach for the widespread appli-
cation of RL in high-risk scenarios where both cumula-
tive and state-wise costs need to be considered simulta-
neously. However, previously constrained offline RL algo-
rithms are primarily designed to handle single-constraint
problems related to cumulative cost, which faces challenges
when addressing multi-constraint tasks that involve both
cumulative and state-wise costs. In this work, we pro-
pose a novel Primal policy Optimization with Conservative
Estimation algorithm (POCE) to address the problem of
multi-constraint offline RL. Concretely, we reframe the ob-
jective of multi-constraint offline RL by introducing the con-
cept of Maximum Markov Decision Processes (MMDP).
Subsequently, we present a primal policy optimization al-
gorithm to confront the multi-constraint problems, which
improves the stability and convergence speed of model
training. Furthermore, we propose a conditional Bell-
man operator to estimate cumulative and state-wise Q-
values, reducing the extrapolation error caused by out-of-
distribution (OOD) actions. Finally, extensive experiments
demonstrate that the POCE algorithm achieves competitive
performance across multiple experimental tasks, particu-
larly outperforming baseline algorithms in terms of safety.
Our code is available at github.POCE.
1. Introduction
Reinforcement learning (RL) have achieved remarkable
achievements in the domains of policy games [4, 19, 42],
robotics [2, 17, 39, 50], and recommendation systems [5, 7].
However, safety concerns remain a primary challenge to the
†Equal Contribution;
‡Corresponding author: guangchen@tongji.edu.cn.real-world deployment of RL, particularly in scenarios with
high safety requirements [15, 21, 38, 53]. These scenarios
demand attention to both cumulative and state-wise safety
constraints. For example, in the context of autonomous
driving, the objective is not only to mitigate instances of
prolonged hazardous driving but also to prevent immediate
collisions [16, 33, 46, 58]. Multi-constraint offline RL is
a promising and potentially effective approach to address
the aforementioned issues. It learns policies that satisfy
both cumulative and state-wise safety constraints from pre-
collected offline datasets.
Currently, several works leverage the framework of Con-
strained Markov Decision Processes (CMDP) to model and
address the problem of learning cost-constrained strategies
under offline dataset, achieving promising results in han-
dling tasks with cumulative cost constraints [22, 30, 32, 44].
However, these approaches face challenges when dealing
with problems that involve constraints on both cumulative
and state-wise cost simultaneously [6, 13, 30]. Additionally,
for the aforementioned multi-constraint problems, meth-
ods such as the Lagrange multiplier and penalty function
method are commonly adopted [12, 24, 31, 36]. These
methods are sensitive to initial values, which leads to un-
stable model training. Moreover, they introduce additional
penalty coefficients or Lagrange multipliers, increasing the
cost of model hyperparameter tuning. On the other hand,
the extrapolation error caused by out-of-distribution (OOD)
actions in offline settings also significantly affects the per-
formance of optimization algorithms [14, 29, 54]. There-
fore, this work focuses on solving multi-constraint op-
timization problems involving cumulative and state-wise
costs in the offline setting.
To solve the aforementioned multi-constraint optimiza-
tion problem in the offline setting, we propose a novel
Primal policy Optimization with Conservative Estimation
algorithm (POCE). Concretely, we introduce the Maximum
Markov Decision Process to represent the state-wise cost.
Based on this, we redefine a novel multi-constrained of-
fline RL objective for tasks that involve both cumulative
and state-wise costs. Subsequently, we propose a pri-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
26243
mal policy optimization approach to address the multi-
constrained offline RL task. Additionally, we present a con-
ditional Bellman operator to mitigate the extrapolation error
of the cumulative and state-wise cost Q-values caused by
OOD actions. Finally, extensive experiments demonstrate
that the POCE algorithm achieves competitive performance
across multiple experimental tasks, particularly outperform-
ing baseline algorithms in terms of safety.
The main contributions of this work are listed as follows:
• To the best of my knowledge, we are the first to formu-
late a multi-constrained offline RL objective to address
the policy optimization problem involving cumulative and
state-wise costs under offline settings.
• We introduce a primal policy optimization method to ad-
dress the multi-constrained optimization problem, which
improves the stability of the algorithm and reduces the
tuning cost associated with the introduction of additional
hyperparameters.
• We propose a novel conditional Bellman operator to ad-
dress the Q-value iteration problem for cumulative and
state-wise costs, This operator ensures that the cost Q-
values are not underestimated while guaranteeing conver-
gence to a unique fixed point.
• Extensive comparisons and ablation experiments demon-
strate that the POCE algorithm delivers competitive per-
formance, particularly in terms of safety.
2. Related Work
In this section, we extensively discuss the related work on
multi-constrained offline RL. We primarily focus on two as-
pects: constrained RL and constrained offline RL.
Constrained RL is an approach to address constrained pol-
icy optimization problems by introducing costs correspond-
ing to rewards based on standard RL. Currently, there are
numerous constrained RL algorithms based on the CMDP
framework for handling cumulative costs [11, 38, 48].
Among them, the algorithms based on the primal-dual
methods have been widely applied to solve constrained RL
problems, such as PDO [9], RCPO [37], and CVPO [25].
Furthermore, there also have been successful attempts in ad-
dressing constrained RL tasks using second-order approxi-
mation [1, 49] and two-step projection methods [47, 59], re-
sulting in excellent performance. Additionally, some works
have combined commonly used constrained methods such
as interior-point [24], conditional value-at-risk [8, 34, 52],
and penalty function methods [55] to solve constrained RL
tasks. On the other hand, some studies focus on the state-
wise cost [60, 61], such as USL [56], which achieves state-
wise cost constraints through hierarchical implementation.
Additionally, some works model state-wise cost as Gaus-
sian processes and satisfy state-wise constraints through
planning with nearby states [40, 41].
Constrained Offline RL is to solve the problem of policyoptimization with cost constraints under an offline dataset.
After the proposal of a constrained offline RL algorithm
with constrained penalties [44], several algorithms have
been developed to achieve cost-constrained policy learn-
ing from offline datasets using different methods. Some
works combine stationary distributions and linear program-
ming to solve for the optimal stationary distribution, thereby
enabling the learning of policies that satisfy cumulative
cost constraints from offline dataset [22, 32]. Addition-
ally, a method based on variational inference and pes-
simistic conservative estimation has been used to address
constrained offline RL tasks [13]. On the other hand, some
works have addressed the optimization problem of satisfy-
ing cumulative costs under offline data from the perspec-
tive of sequential decision-making using the Transformer
model [27, 51, 57].
In summary, our work is distinct from existing studies
in two main aspects, which also represent the core chal-
lenges we meet. Firstly, our work addresses the multi-
constrained optimization problem of simultaneous handling
of cumulative and state-wise costs in high-risk scenarios.
Secondly, our work deals with the multi-constrained opti-
mization problem in the offline setting. To the best of my
knowledge, this is the first attempt to simultaneously handle
cumulative state-wise costs in the offline setting.
3. Preliminaries
In this section, we present the fundamental concepts and
background of constrained RL and constrained offline RL.
Subsequently, we rethink the existing works and highlight
the purpose and significance of our work.
3.1. Constrained RL and Constrained Offline RL
CMDP provides a theoretical framework to solve con-
strained RL problems [3]. It is defined as a tuple (S,A, C,
P, r, ρ 0, γ), where S ∈Rnis the state space, A ∈Rm
is the action space, P:S × A × S → [0,1]is the
transition kernel, which specifies the transition probability
p(st+1|st, at)from state stto state st+1under the action
at,r:S × A → Rrepresents the reward function, Cis
the set of costs {ci:S × A → R+, i= 1,2,···, m}for
violating mconstraints, γ∈[0,1)is the discount factor,
andρ0:S → [0,1]is the distribution of initial states.
The policy πis a probability distribution mapping the state
stto the action at. We use shorthand rt=r(st, at)and
ci,t=ci(st, at)for simplicity. The common objective of
constrained RL is to maximize the cumulative reward while
satisfying the cumulative cost constraint.
π∗=arg max
πEτ∼π"∞X
t=0γtrt#
,s.t.Eτ∼π"∞X
t=0γtci,t#
≤¯ci,(1)
where the τ={s0, a0,···} ∼ πdenotes the trajectory. ¯ci
is the cost threshold of the i-th cumulative cost constraint.
26244
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.004681012Reward
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Step(           ) 1e7246810CostCMDP method
SCMDP method
Cost limit
0.0 0.2 0.4 0.6 0.8 1.0
State-wise Cost0.00.20.40.60.81.01.2Frequencies1e4
P0. 8=2.47%P0. 6=10.53%P0. 4=26.73%
0.40 0.45 0.50 0.55 0.60 0.65 0.700.40.50.60.7
C1>0.4
C2>0.6
C3>0.8
Agent
* Step(           ) 1e7*Figure 1. The left two sub-figures illustrate the reward and cost curves under the cumulative cost constraint and state-wise cost constraint
method for the PointGoal task. The third sub-figure illustrates the state-wise cost violation rate under different state-wise cost thresholds
when satisfying the cumulative cost constraint using the cumulative cost constraint algorithm. The last subfigure depicts the region range
of different state-wise costs. The threshold for cumulative cost is set as ¯ci= 5.
On the other hand, to ensure that the agent satisfies
cost constraints at each time step during the execution of
the policy, a State-wise Constrained Markov Decision Pro-
cess (SCMDP) is proposed [61]. Unlike the cumulative cost
constraint in CMDP mentioned earlier, SCMDP requires
the agent to satisfy the cost constraint in every state. The
objective of SCMDP be formulated as follows:
π∗= arg max
πEτ∼π"∞X
t=0γtrt#
,
s.t.∀t≥0,Est∼p(·|st,at),at∼π(·|st)(ci,t)≤¯Di.(2)
where trepresents the t-th step, and ¯Didenotes the cost
threshold of the i-th sate-wise cost constraint.
Although the aforementioned online-constrained RL
methods can ensure the safety of the agent during the testing
and deployment phases, their safety during the training pro-
cess faces challenges. Constrained offline RL is a method of
learning a policy that satisfies cost constraints from offline
datasets without interacting with the environment [22, 32].
This paradigm not only addresses the safety concerns dur-
ing the testing and deployment phases but also ensures the
safety of the training process.
3.2. Rethinking to Constrained RL
We rethink the application requirements of RL algorithms
in real-world scenarios. In the field of autonomous driving,
we allow intelligent agent vehicles to exceed the speed limit
or change lanes within a limited timeframe, but we do not
permit long-term speeding or crossing lane lines, nor do we
allow any collisions. Similarly, in the domain of quantita-
tive investing, it is necessary to consider not only the overall
investment limit but also restrictions on individual invest-
ment amounts. These cases require us to simultaneously
consider cumulative and state-wise costs.
We conduct experiments on the constraint RL algorithms
of CMDP and SCMDP in the typical experimental scenario
PointGoal . The experimental results are shown in Fig. 1.
The results shown in Fig. 1 indicate that in the constraint RL
for handling cumulative costs, the probability of the agent’s
state-wise cost exceeding 0.6 is more than 10% when the
cumulative cost constraint is satisfied. This suggests thatalthough the constraint RL algorithm for addressing cumu-
lative costs can ensure that the agent satisfies the cumulative
cost constraint, it is hard to guarantee that the state-wise
cost remains within a controllable range. Additionally, we
can also observe from the results in Fig. 1 that the constraint
RL algorithm for handling state-wise costs does not ensure
that all states satisfy the cost constraint, and it leads to a
significant loss of reward.
In summary, the current individual treatment of cumu-
lative and state-wise costs in constraint RL does not ade-
quately meet the requirements of application scenarios that
involve both cumulative and state-wise costs. Inspired by
CMDP, SCMDP, and offline RL, we propose a novel multi-
constraint offline RL task for addressing the simultaneous
treatment of cumulative and state-wise costs in RL applica-
tion scenarios.
4. Methodology
In this section, we provide a detailed exposition of pri-
mal policy optimization with conservative estimation algo-
rithm for multi-constraint offline RL. Firstly, we define the
multi-constraint offline RL task and reframe the objective
of multi-constraint offline RL by introducing the concept of
maximizing the MDP (MMDP). Based on this, we propose
a primal policy optimization with conservative estimation
algorithm for multi-constraint offline RL. Additionally, we
propose a conditional Bellman operator to handle the ex-
trapolation error in cost Q-values caused by OOD actions.
Definition 1 In the offline setting, the multi-constraint safe
RL objective that encompasses cumulative and state-wise
cost constraints is as follows:
π∗= arg max
πEτ∼π"∞X
t=0γtr(st, at)#
,
s.t.Eτ∼π"∞X
t=0γtci(st, at)#
≤¯ci,
∀t≥0,Est∼p(·|st,at),at∼π(·|st)[ci(st, at)]≤¯Di.(3)
4.1. Multi-constraint Offline Safe RL
Based on the analysis and discussion in Section 3.2, it is
known that existing constrained RL algorithms meet chal-
26245
lenges when dealing with tasks that involve both cumula-
tive and stat-wise cost constraints. Therefore, we define a
multi-constraint offline RL to address the problem of simul-
taneously incorporating cumulative and state-wise costs in
the offline setting.
We find that directly addressing the state-wise cost con-
straint as shown in Eq. (3) is extremely challenging. In-
spired by SCPO [61], we transform the state-wise cost into
a state cost increment. Concretely, we extend the objec-
tive of multi-constraint offline RL defined in Definition 1
by introducing a set of maximum state-wise costs Mi,tand
state-wise cost increments Di,t. Additionally, we obtain an
augmented state ˆst= (st, Mi,t)by supplementing the state
stwith the maximum state cost Mi,t. The maximum state-
wise costs and state-wise cost increments are expressed as:
Di(ˆst, at) = max {ci(st, at)−Mi,t,0}, (4)
Mi,t=k=t−1X
k=0Di(ˆsk, ak), (5)
where Mi,t=Mi(ˆst, at)represents the maximum state-
wise cost of the i-th cost at step t, and Di,t=Di(ˆst, at)
represents the increment of the state-wise of the i-th cost
at step t. The initial values of the maximum state-wise
cost and the increment of the state-wise cost are defined as
Mi,0= 0 andDi(ˆs0, a0) = ci(s0, a0). Combining the
maximum state-wise cost and the increment of the state-
wise cost as defined in Eq. (4) and (5), we derive Lemma 4.1
based on Definition 1. Due to the space limitation, proofs
and discussions are in Appendix A.1 .
Lemma 4.1 The objective of the multi-constraint offline RL
with cumulative and state-wise costs can be formulated as:
π∗= arg max
πEτ∼π"∞X
t=0γtr(ˆst, at)#
,
s.t.Eτ∼π"∞X
t=0γtci(ˆst, at)#
≤¯ci,Eτ∼π"∞X
t=0Di(ˆst, at)#
≤¯Di.(6)
where r(ˆst, at)≜r(st, at)andci(ˆst, at)≜ci(st, at).
We define the Q-value for reward on policy πas
Qr(ˆs, a) =Eτ∼π[P∞
t=0γtr(ˆst, at)|ˆs0= ˆs, a0=a],
and the Q-value of cost is defined similarly. Additionally,
we define the Q-vlaue for cost increments on policy πas
QDi(ˆs, a) =Eτ∼π[P∞
t=0Di(ˆst, at)|ˆs0= ˆs, a0=a].
Through theoretical derivation, we discovered an inherent
connection between the state-wise and cumulative cost con-
straint, as mentioned in Remark 4.2. Proofs and discussions
are in Appendix A.2 .
Remark 4.2 When the state-wise cost satisfies the cost
constraint Eτ∼π[P∞
t=0Di(ˆst, at)]≤¯Diand the cost
threshold of state-wise cost satisfies the condition ¯Di≤
(1−γ)¯ci, then the cumulative cost also satisfies the cost
constraint Eτ∼π[P∞
t=0γtci(ˆst, at)]≤¯ci.4.2. Primal Policy Optimization
Due to the difficulty in directly handling the multi-
constrained maximization reward objective stated in
Lemma 4.1 under continuous state and action spaces, we
are inspired by DDPG [35] and rewrite Eq. (6) as:
π∗= arg max
πEˆs∼D,a∼π
Qr
ϕr(ˆs, a)
,
s.t.Eˆs∼D
a∼πh
Qci
ϕci(ˆs, a)i
≤¯ci,Eˆs∼D
a∼πh
QDi
ϕDi(ˆs, a)i
≤¯Di,(7)
where Qr
ϕr,Qci
ϕci, and QDi
ϕDirepresent the parameterized
reward Q-value, cumulative and state-wise cost Q-value.
For the multi-constrained optimization as shown in Eq. 7,
the common practice is to apply the Lagrange multiplier
method to transform the constrained problem into an uncon-
strained one [23, 37]. However, this conventional primal-
dual method is sensitive to initial values and learning rates,
resulting in significant costs during hyperparameter tuning.
Inspired by CRPO [45], we directly optimize the aforemen-
tioned multi-constrained problem using the policy gradient
of the primal problem based on stochastic approximation
theory. This method avoids the training instability and tun-
ing cost caused by additional Lagrange multipliers. On the
other hand, OOD actions in offline settings have a signifi-
cant impact on the accuracy of Q-value estimation. There-
fore, to mitigate the extrapolation errors caused by OOD ac-
tions, we employ a conservative estimation method to esti-
mate the Q-values of reward, cumulative cost and state-wise
cost separately.
Based on the above analysis, we propose a Primal pol-
icyOptimization with Conservative Estimation (POCE) for
multi-constraints offline RL tasks. We divide the POCE al-
gorithm into two steps: Q-value estimation and policy up-
date. In the Q-value estimation step, we present a conserva-
tive estimation method to estimate the Q-values of reward,
cumulative cost, and state-wise cost separately. The spe-
cific estimation methods are detailed in Section 4.3. In
the policy update step, we evaluate whether the current
policy satisfies the cost constraints based on the Q-values
of cumulative and state-wise cost. Subsequently, we up-
date the policy by maximizing the reward or minimizing
the cost. Concretely, we evaluate whether the cumula-
tive cost Eˆs∼D,a∼πh
Qci
ϕci(ˆs, a)i
≤¯ciand state-wise cost
Eˆs∼D,a∼πh
QDi
ϕDi(ˆs, a)i
≤¯Disatisfy the constraints. If
both constraints are satisfied, the constrained problem de-
scribed in Eq. 7 is transformed into an unconstrained prob-
lem of maximizing the reward. If either constraint is not sat-
isfied, the constrained problem in Eq. 7 is transformed into
minimizing either the cumulative or state-wise cost. There-
fore, the objective of policy updating can be expressed as:
L(θ) =

arg max
θE
Qr
ϕr(ˆs, πθ(a|ˆs))
,(πθ∈πs)
arg min
θEh
Qχ
ϕχ(ˆs, πθ(a|ˆs))i
,(πθ/∈πs), (8)
26246
where πsrepresents the safety policy, πθ∈πsindicates
that both the cumulative and state-wise cost Q-values under
the current policy πθmeet the constraints, and πθ/∈πs
indicates that at least one cost Q-value under the policy πθ
does not satisfy the constraints. The χis either ciorDi.
Note that if both constraints are not satisfied, we choose
either one and minimize its cost.
4.3. Conservatively Estimate for Q-values
Due to the fact that the POCE algorithm evaluates whether
the cost meets the constraints and computes the policy gra-
dients are both based on the Q-values, the accuracy of
Q-value estimation directly affects the algorithm’s perfor-
mance. Therefore, in the Q-value estimation step, we are
committed to providing accurate Q-values. Considering
that the experience Bellman iteration always selects action-
state pairs with the maximum Q-values, the erroneous esti-
mation of Q-values for OOD actions is difficult to correct
in the offline setting, which leads to the overestimation of
reward Q-values. To address the issue of reward Q-value
overestimation caused by OOD actions, we follow the pes-
simistic estimation approach of CQL [20]. We incorporate
a penalty term for the marginal distribution of unseen action
based on the experience Bellman iteration and maximize the
Q-value of the current policy to achieve a conservative esti-
mation of the reward Q-values. Then, the loss function for
the reward Q-value iteration can be expressed as follows:
L(ϕr)=arg min1
2Eˆs,a∼D
Qr
ϕr(ˆs, a)−ˆTBˆQr(ˆs, a)2
+κ
E ˆs∼D
a∼πM(·|ˆs)Qr
ϕr(ˆs, a)−Eˆs∼D
a∼ˆπθ(·|s)Qr
ϕr(ˆs, a)
,(9)
where κis a tradeoff factor. Eq. (9) utilizes the empirical
Bellman operator ˆTBinstead of the actual Bellman Operator
TB.πMis the marginal distribution of unseen actions.
On the other hand, to prevent unsafe actions during pol-
icy execution, we need to avoid underestimating the cost
Q-values of action-state pairs while ensuring that the esti-
mated cost Q-values do not significantly deviate from the
true Q-values. Obviously, our estimation method for re-
ward Q-values is no longer applicable to the estimation of
Q-values for cumulative and state-wise costs. Inspired by
MCQ [28], we propose a conditional Bellman operator for
iterative estimation of cost Q-values. Concretely, when an
action belongs to the in-distribution actions, we still employ
the Bellman operator for Q-value iteration. However, when
an action belongs to the OOD actions, we provide a pseudo
target for the iterated Q-values. Then, the conditional Bell-
man iteration equation can be written as:
TCBQ(ˆs, a)=

χ(ˆs, a)+γEˆs′max
a′Q(ˆs′, a′),(a∈πβ(a|ˆs))
max
ˆa∼πβQ(ˆs, πβ(a|ˆs)), (a /∈πβ(a|ˆs)),(10)Algorithm 1: POCE
Input: Dataset D={(ˆs, a, r, c i, Di,ˆs′, d)i}n
i=0
Output: Policy network parameters θ;
Parameters for the reward Q-values ϕr,ϕt
r, ;
Parameters for the cumulative cost Q-values ϕci,ϕt
ci;
Parameters for the incremental cost Q-values ϕDi,ϕt
Di;
Parameters of the behavioral policy network ω;
1foreach batch do
2 Sample a batch of transitions (ˆs, a, r, ˆs′, ci, Di, d)
from the buffer D;
// Q-value Estimation Step:
3 Update the behavioral policy ωvia the Eq. (12);
4 Update the reward Q-values ϕrvia the Eq. (9);
5 Update the cumulative and incremental cost Q-values
ϕciandϕDivia the Eq. (11);
// Policy Update Step:
6 Compute the cost constraint:
¯Qci(s, a) = max (s,a)∼DQci
ϕci(s, a),
¯QDi(s, a) = max (s,a)∼DQDi
ϕDi(s, a);
7 if¯Qci(s, a)≤¯ciand¯QDi(s, a)≤¯Dithen
8 Take policy updates toward maximize reward
θ←arg max θE
Qr
ϕr(ˆs, πθ(a|ˆs))
;
9 else
10 Take policy updates to minimize cost
θ←arg min θEh
Qχ
ϕχ(ˆs, πθ(a|ˆs))i
;
11 ϕt
r←τϕr+ (1−τ)ϕt
r;
12 ϕt
ci←τϕci+ (1−τ)ϕt
ci;
13 ϕt
Di←τϕDi+ (1−τ)ϕt
Di;
where πβis the behavioral policy of the sample data. The
χrepresents the cost cior cost increment Di. Note that an
action is considered an in-distribution action if it belongs
to the behavioral policy of the sample data; otherwise, it is
considered an OOD action.
Proposition 4.3 Within the scope of behavioral policies
πβ, the conditional Bellman operator TCBis aγ-contractive
operator under the L+∞norm, and any initial Q-value can
converge to a unique fixed point through TCB.
Proposition 4.4 As the unique fixed point of the conditional
Bellman operator, QTCBis bounded within the range of be-
havioral policies πβ, with QTCB∈[Qπβ, Qπ∗
β]. Here, Qπβ
is the Q-value of the behavioral policy, and Qπ∗
βis the Q-
value of the optimal policy.
Proposition 4.3 demonstrates that the Q-values converge to
a unique fixed point through conditional Bellman iteration.
Proposition 4.4 indicates that within the scope of supported
behavioral policies, the Q-values obtained through condi-
tional Bellman iteration does not exhibit substantial under-
estimation nor significant deviation from the true values.
The proofs and discussions of Proposition 4.3 and 4.4 are
26247
inAppendix A.3 andAppendix A.4. Based on the con-
ditional Bellman operator, we constructed an iterative loss
function for the Q-values of cumulative costs. Concretely,
we sampled OOD actions for state susing the current pol-
icy and assigned pseudo targets to the OOD actions under
statesbased on the current Q-value network.
L(ϕχ)=arg min λEˆs,a∼D
Qχ
ϕχ(ˆs, a)−ˆTBˆQχ(ˆs, a)2
+ (1−λ)Eˆs∼D,ao∼πh
(Qχ
ϕχ(ˆs, ao)−y)2i
,(11)
where λis a hyperparameter that balances the in-
distribution and OOD actions. The χis either ci
orDiThe Pseudo targets for OOD actions y=
E{ˆai}N∼πβ[max ˆa∼{ˆai}NQ(ˆs,ˆa)].
4.4. Practical Algorithm
To facilitate the understanding of the implementation pro-
cess of the POCE algorithm, we provide a detailed explana-
tion of a practical instance of the POCE algorithm. The
pseudo-code for the POCE algorithm is shown in Algo-
rithm 1. In the Q-value estimation step, we employ conser-
vative estimation methods to update the Q-values of reward,
cumulative cost, and state-wise cost, respectively. Addition-
ally, since the conditional Bellman operator requires the be-
havioral policy πβof the sample data, which is unknown
in the offline setting, we utilize the learned behavioral pol-
icyˆπβfrom a conditional variational autoencoder (CV AE)
as a substitute for the behavioral policy πβ. Then, the loss
function for the CV AE can be written as:
L(ω) = arg min E(ˆs,a)∼D,z∼Eω(ˆs,a)
(a−Dω(ˆs, z))2
+DKL(z||N(0, I))
.(12)
In the policy update step, we evaluate whether the cost con-
straints are satisfied and update the policy network. Due to
the limited number of action-sate pairs we collect, it is chal-
lenging to ensure that we sample action-state pairs with high
accumulated or state-wise costs on every trajectory during
the evaluation of cost constraints. Therefore, we scale the
evaluation condition using the maximum Q-value when as-
sessing whether the cost constraints are met.
5. Experimental Evaluation
In this section, we conduct comprehensive comparative ex-
periments between POCE and previous offline safe RL
methods using datasets from a range of domains encom-
passing diverse action spaces and observation dimensions.
5.1. Task and Baseline
Task and Dataset. To assess the performance of POCE
in various tasks across different domains, we selected three
widely adopted tasks [18, 26] from the Safety-gym and Mu-
joco domains as experimental tasks in this work. Con-
cretely, we chose the PointGoal andCarGoal tasks in thesafety-gym domain. These tasks require the agent to nav-
igate the Point orCar through interference and avoid haz-
ardous obstacles to reach a target location. Additionally,
we select the AntVelocity task in the Mujoco domain which
requires the Antagent to walk or run within a specified ve-
locity range.
0.00 0.25 0.50 0.75 1.000510RewardPointGoal( =0.8)
0.00 0.25 0.50 0.75 1.0050010001500AntVelocity( =0.8)
0.00 0.25 0.50 0.75 1.002.55.07.510.012.5CostCCRR CPQ-Lag Cost limit
0.00 0.25 0.50 0.75 1.00051015
0.00 0.25 0.50 0.75 1.00
Step( *        ) 1e50.00.20.40.6Cost Rate (%)
0.00 0.25 0.50 0.75 1.000.00.51.01.52.0POCE BCQ-Lag
Step( *        ) 1e5
Figure 2. The figure depicts the reward and cumulative cost curves
and the curve representing the violation rate of state costs. The
shaded areas on the curves represent the variance obtained from
online testing conducted with three random seeds. The safety fac-
tor for this experiment is φ= 0.8, and the cumulative cost thresh-
old is set at ¯ci= 5. In the PointGoal task, the state-wise cost
threshold is set to ¯Di= 0.8, while in the AntVelocity task, the
state-wise cost threshold is ¯Di= 0.2.
To the best of my knowledge, there is currently no stan-
dardized dataset for multi-constraint offline RL. To facili-
tate further research and reproducibility of this work, we re-
vise the standard experimental scenarios of constrained RL
by adjusting the cost values in the environment to conform
to the range of ci∈[0,1]rather than ci∈ {0,1}. Moreover,
drawing inspiration from work [13], we introduce a safety
factor φto differentiate the sample data of different behav-
iors. Subsequently, we collect diverse sample data based on
different behaviors. The cost allocation for each task and
the methods of diverse data collection are in Appendix B.1 .
Baselines. Due to the lack of constrained offline RL al-
gorithms that can simultaneously handle both cumulative
and state-wise cost constraints. Therefore, we enhance
existing constrained offline RL methods to develop multi-
constraint offline RL algorithms as the experimental base-
lines. BCQ-Lag is an improved constrained offline RL
algorithm that combines the Lagrange multiplier method
with the BCQ [10] algorithm. We introduce two Lagrange
multipliers to handle cumulative and state-wise costs re-
26248
Table 1. The performance of the baseline and POCE algorithms is evaluated across samples with varying safety factors. The results from
experiments involving 20 episodes are conducted with 3 random seeds. The data with a shaded background indicated the satisfaction
of cumulative cost constraints, while the data highlighted in bold represents the highest reward return obtained under the satisfaction of
cumulative cost constraints. The state-wise cost threshold for this experiment is ¯Di= 0.8, and the cumulative cost threshold is ¯ci= 5.
Method MetricsPointGoal CarGoalMean
φ=0.8 φ=0.6 φ=0.4 φ=0.8 φ=0.6 φ=0.4
BCQ-LagReward ↑ 11.27±0.24 11.18 ±0.28 11.03 ±0.11 12.63±0.56 12.41 ±0.26 12.18 ±0.38 11.78
Cost↓ 6.05±0.64 6.38 ±0.47 6.80 ±0.59 4.84±0.51 5.09 ±1.11 5.30 ±0.76 5.74
Cost rate(%) ↓ 0.20±0.10 0.20 ±0.12 0.28 ±0.15 0.15±0.08 0.16 ±0.08 0.16 ±0.06 0.19
CCRRReward ↑ 11.67±0.24 11.44 ±0.22 11.34 ±0.17 12.57±0.80 12.30±0.61 12.21 ±0.05 11.92
Cost↓ 6.11±2.34 6.45 ±2.64 6.80 ±1.48 4.16±0.85 4.89±1.04 5.26 ±0.79 5.61
Cost rate(%) ↓ 0.21±0.13 0.27 ±0.16 0.31 ±0.17 0.15±0.07 0.15±0.09 0.16 ±0.12 0.21
CPQ-LagReward ↑ 11.18±2.53 10.97 ±0.26 10.90 ±0.43 14.66 ±0.44 14.17 ±0.53 13.94 ±0.42 12.64
Cost↓ 7.94±1.46 6.24 ±1.38 6.60 ±2.00 8.59 ±1.87 7.28 ±1.10 7.97 ±0.69 7.44
Cost rate↓ 0.32±0.18 0.22 ±1.41 0.26 ±0.18 0.38 ±0.13 0.24 ±0.15 0.26 ±0.12 0.28
POCEReward ↑ 12.74±0.26 12.62±0.13 12.54±0.19 13.58±0.78 13.45±0.51 13.41±0.64 13.06
Cost↓ 4.37±0.78 4.76±0.89 4.93±0.83 3.43±0.65 4.56±0.62 5.00 ±0.43 4.51
Cost rate(%) ↓ 0.09±0.04 0.12±0.05 0.14±0.06 0.04±0.01 0.06±0.05 0.11 ±0.04 0.09
spectively, enabling the optimization of multi-constrained
policies in an offline setting. Similarly, we introduce
two Lagrange multipliers based on CRR [43] to obtain an
improved multi-constrained offline RL algorithm CCRR.
CPQ-Lag is an extension of the constrained offline RL al-
gorithm CPQ [44] that introduces additional Lagrange mul-
tipliers to handle the state-wise cost Q-values, thus enabling
the implementation of a multi-constraint offline RL method.
5.2. Performance Comparison Experiment
Performance on various tasks and behavioral samples.
To evaluate the performance of the POCE algorithm across
various tasks and behavioral samples, we conducted a com-
prehensive evaluation. This evaluation involved comparing
the performance of the POCE with the baseline algorithms
in three tasks: PointGoal ,CarGoal , and AntVelocity . Fur-
thermore, we also examined the algorithm’s performance
across samples with different safety factors φ. Fig. 2 illus-
trates the reward, cumulative cost, and state cost violation
rate curves for the baseline algorithm and the POCE algo-
rithm in the PointGoal task. From the illustrated results, it
can be observed that compared to the baseline algorithm,
POCE not only constrains the cumulative cost within the
cost threshold range but also achieves a lower state-wise
cost violation rate. Furthermore, it provides competitive re-
wards return. The results presented in Table 1 demonstrate
that the POCE algorithm consistently ensures that the pol-
icy satisfies cumulative cost constraints across samples with
different safety factors while also delivering competitive re-
ward returns. Notably, in the PointGoal task, the POCE out-
performs the baseline algorithms in terms of both reward
and cost-effectiveness. The above results indicate that the
POCE algorithm can consistently ensure policy compliance
with cumulative cost constraints across samples with differ-
ent behaviors while providing competitive reward returns
and state-wise cost management.
Performance on different state-wise cost thresholds. Toevaluate the performance of the POCE algorithm under dif-
ferent state-wise cost thresholds, we conducted comparative
experiments between the POCE algorithm and the baseline
algorithm at multiple state-wise cost thresholds. The results
in Table 2 demonstrate that POCE provides competitive re-
ward returns while ensuring the satisfaction of cumulative
cost constraints under different state-wise cost thresholds.
Moreover, the mean values of reward and cumulative costs
for both tasks are superior to those of the baseline algorithm.
Additionally, from the results in the table, it is evident that
lowering the state-wise cost threshold leads to a decrease
in cumulative costs. This observation is consistent with the
viewpoint expressed in Remark 4.2. The analysis of the
above results reveals that the POCE algorithm ensures com-
pliance with cost constraints for cumulative costs under dif-
ferent state-wise cost thresholds, while also providing com-
petitive rewards and state-wise cost violation rates.
5.3. Ablation Experiment
The parameters safety factors φand state-wise cost
thresholds ¯Di.To assess the sensitivity of the POCE algo-
rithm to different safety factors φand state-wise cost thresh-
olds¯ci, we tested the performance of the POCE algorithm
at different state-wise cost thresholds ¯ciusing samples with
varying safety factors φin the PointGoal task. Fig. 3 dis-
plays the mean performance of the POCE algorithm at dif-
ferent state-wise cost thresholds with samples of varying
safety factors in the PointGoal task. From the results in the
figure, it can be observed that higher safety factors in the
sample or higher state-wise cost thresholds result in larger
reward returns. Conversely, lower safety factors in the sam-
ple or lower state-wise cost thresholds lead a higher state-
wise cost violation rates. Additionally, higher safety factors
φin the sample data or lower state-wise cost thresholds ¯ci
result in lower cumulative costs. From the above analysis, it
is evident that higher safety factors in the sample data lead
to higher rewards, along with lower cumulative costs and
26249
Table 2. The performance of the baseline and POCE algorithms is assessed under various state-wise cost thresholds. The results from
experiments involving 20 episodes are conducted with three random seeds. The data with a shaded background indicated the satisfaction
of cumulative cost constraints, while the data highlighted in bold represents the highest reward return obtained under the satisfaction of
cumulative cost constraints. The safety factor for the experiment is φ= 0.8, and the cumulative cost threshold is consistent at ¯ci= 5.
Method MetricsPointGoal CarGoalMean
¯Di=0.8 ¯Di=0.6 ¯Di=0.4 ¯Di=0.8 ¯Di=0.6 ¯Di=0.4
BCQ-LagReward ↑ 11.27±0.24 11.13 ±0.34 11.06±0.20 12.63±0.56 12.40±0.46 11.28±1.17 11.63
Cost↓ 6.05±0.64 5.85 ±1.97 4.65±0.94 4.84±0.51 4.08±0.46 3.95±0.56 4.90
Cost rate(%) ↓ 0.20±0.10 1.01 ±0.49 1.48±0.65 0.15±0.08 0.38±0.18 1.03±0.82 0.71
CCRRReward ↑ 11.67±0.24 11.50 ±0.20 11.30 ±0.20 12.57±0.80 12.23±0.73 12.16±0.76 11.91
Cost↓ 6.11±2.34 5.95 ±2.37 5.29 ±2.37 4.16±0.85 4.19±0.85 4.04±0.92 4.96
Cost rate↓ 0.21±0.13 1.29 ±1.08 1.72 ±1.13 0.15±0.07 0.47±0.30 1.10±0.68 0.82
CPQ-LagReward ↑ 11.18±2.53 10.97 ±1.64 10.59 ±1.26 14.66 ±0.44 14.11 ±0.31 13.73 ±0.25 12.54
Cost↓ 7.94±1.46 7.27 ±2.61 6.65 ±3.31 8.59 ±1.87 7.92 ±1.03 5.99 ±1.18 7.39
Cost rate (%) ↓ 0.32±0.18 2.13 ±1.57 2.98 ±1.38 0.38 ±0.13 0.92 ±0.52 2.03 ±1.47 1.46
POCEReward ↑ 12.74±0.26 12.58±0.16 12.49±0.24 13.58±0.78 13.41±0.50 13.28±0.51 13.01
Cost↓ 4.37±0.78 4.15±0.37 4.07±0.67 3.43±0.65 3.33±0.56 3.24±0.75 3.77
Cost rate (%) ↓ 0.09±0.04 0.24±0.16 0.98±0.75 0.04±0.01 0.20±0.09 0.80±0.35 0.39
cost violation rates. On the other hand, lower state-wise
cost thresholds result in higher cost violation rates, while
leading to lower rewards and cumulative costs.
0.8 0.7 0.6 0.5
0.8 0.7 0.6 0.5Di
12.74 12.71 12.62 12.58
12.63 12.56 12.53 12.55
12.58 12.52 12.49 12.44
12.51 12.45 12.46 12.37Reward
0.8 0.7 0.6 0.5
0.8 0.7 0.6 0.54.37 4.58 4.76 4.85
4.32 4.43 4.68 4.77
4.15 4.39 4.52 4.62
4.12 4.25 4.48 4.57Cost
0.8 0.7 0.6 0.5
0.8 0.7 0.6 0.50.09 0.10 0.12 0.14
0.19 0.20 0.20 0.21
0.24 0.25 0.28 0.35
0.51 0.54 0.57 0.59Cost Ratio(%)
12.412.512.612.7
4.24.44.64.8
0.10.20.30.40.5
Figure 3. The mean values of rewards, cumulative costs, and stat-
wise cost violation rates for the POCE algorithm under varying
safety factors and state-wise cost thresholds in the PointGoal task.
The values shown in the figure represent the means of 20 episodes
tested independently with three different random seeds.
0.0 0.2 0.4 0.6 0.8 1.010
010Reward
0.0 0.2 0.4 0.6 0.8 1.0
Step(*         ) 1e501020 Cost POCE_Qr  
0.0 0.2 0.4 0.6 0.8 1.00.00.51.0 Cost Rate (%)
Step(*         ) 1e5 Step(*         ) 1e5POCE  Cost limit  POCE_Qx  
Figure 4. The reward, cumulative cost, and state-wise cost viola-
tion rate curves of the POCE algorithm and the POCE algorithm
with conservative Q-value estimation removed in the PointGoal
task. The shadowed curves represent the mean and variance of the
test results for three different random seeds.
Conservative estimation of rewards and costs. We eval-
uate the impact of conservative estimation of POCE al-
gorithm by removing the conservative estimation methods
for reward and cost Q-values. POCE-Qr represents the
POCE algorithm with the removal of conservative reward
Q-values, while POCE-Qx represents the POCE algorithm
with the removal of conservative cumulative cost Q-values
and state cost Q-values based on conditional Bellman es-
timation. Fig. 4 displays the reward, cumulative cost, and
state-wise cost violation rate curves for the POCE algorithm
and the POCE algorithm with conservative Q-value estima-tion removed in the PointGoal task. From the figure, it can
be observed that the reward of POCE-Qr is significantly
lower compared to POCE, while the reward of POCE-Qx is
slightly higher than POCE. However, POCE-Qx fails to sat-
isfy the cumulative cost constraint, and the state-wise cost
violation rate also noticeably increases. This indicates that
conservative estimation of reward Q-values can improve the
reward performance of the POCE algorithm, while con-
servative estimation of cost Q-values based on conditional
Bellman implementation can prevent underestimation of Q-
values for action-state pairs, thereby ensuring safety.
6. Conclusion
In this work, we propose a novel policy optimization al-
gorithm with conservative estimation for multi-constrained
offline reinforcement learning. Concretely, we first rethink
the requirements of constraint RL in real-world applica-
tions and redefine the objectives of multi-constrained of-
fline RL tasks by introducing the MMDP. Then, we present
a primal policy optimization method to address the multi-
constrained optimization problem. Additionally, we pro-
pose the conditional Bellman operator to achieve a conser-
vative estimation of cumulative cost and state-wise cost. Fi-
nally, extensive experiments demonstrate that the POCE al-
gorithm provides competitive performance, particularly in
terms of safety.
Acknowledgment: This work is supported by STI
2030-Major Projects (No.2021ZD0201405), in part by
the National Natural Science Foundation of China (No.
62372329), in part by the National Key Research and De-
velopment Program of China (No.2021YFB2501104), in
part by Shanghai Rising Star Program (No.21QC1400900),
in part by Tongji-Qomolo Autonomous Driving Commer-
cial Vehicle Joint Lab Project, and in part by Xiaomi Young
Talents Program. We thank Long Yang and Yiqin Yang for
the insightful discussion.
26250
References
[1] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel.
Constrained policy optimization. In International conference
on machine learning , pages 22–31. PMLR, 2017. 2
[2] Rishabh Agarwal, Dale Schuurmans, and Mohammad
Norouzi. An optimistic perspective on offline reinforcement
learning. In International Conference on Machine Learning ,
pages 104–114. PMLR, 2020. 1
[3] Eitan Altman. Constrained Markov decision processes:
stochastic modeling . Routledge, 1999. 2
[4] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh
Song. Uncertainty-based offline reinforcement learning with
diversified q-ensemble. Advances in Neural Information
Processing Systems , 34, 2021. 1
[5] Xueying Bai, Jian Guan, and Hongning Wang. A model-
based reinforcement learning with adversarial training for
online recommendation. In Advances in Neural Information
Processing Systems . Curran Associates, Inc., 2019. 1
[6] Steven Bohez, Abbas Abdolmaleki, Michael Neunert, Jonas
Buchli, Nicolas Heess, and Raia Hadsell. Value con-
strained model-free continuous control. arXiv preprint
arXiv:1902.04623 , 2019. 1
[7] Xu Chen, Yali Du, Long Xia, and Jun Wang. Reinforcement
recommendation with user multi-aspect preference. In Pro-
ceedings of the Web Conference 2021 , pages 425–435, 2021.
1
[8] Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone.
Risk-sensitive and robust decision-making: a cvar optimiza-
tion approach. Advances in neural information processing
systems , 28, 2015. 2
[9] Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson,
and Marco Pavone. Risk-constrained reinforcement learning
with percentile risk criteria. The Journal of Machine Learn-
ing Research , 18(1):6070–6120, 2017. 2
[10] Scott Fujimoto, David Meger, and Doina Precup. Off-policy
deep reinforcement learning without exploration. In Inter-
national conference on machine learning , pages 2052–2062.
PMLR, 2019. 6
[11] Javier Garcıa and Fernando Fern ´andez. A comprehensive
survey on safe reinforcement learning. Journal of Machine
Learning Research , 16(1):1437–1480, 2015. 2
[12] Shangding Gu, Jakub Grudzien Kuba, Yuanpei Chen, Yali
Du, Long Yang, Alois Knoll, and Yaodong Yang. Safe multi-
agent reinforcement learning for multi-robot control. Artifi-
cial Intelligence , 319:103905, 2023. 1
[13] Jiayi Guan, Guang Chen, Jiaming Ji, Long Yang, Ao Zhou,
Zhijun Li, and changjun jiang. VOCE: Variational optimiza-
tion with conservative estimation for offline safe reinforce-
ment learning. In Thirty-seventh Conference on Neural In-
formation Processing Systems , 2023. 1, 2, 6
[14] Jiayi Guan, Shangding Gu, Zhijun Li, Jing Hou, Yiqin Yang,
Guang Chen, and Changjun Jiang. Uac: Offline reinforce-
ment learning with uncertain action constraint. IEEE Trans-
actions on Cognitive and Developmental Systems , 2023. 1
[15] Jing Hou, Guang Chen, Zhijun Li, Wei He, Shangding Gu,
Alois Knoll, and Changjun Jiang. Hybrid residual multiex-
pert reinforcement learning for spatial scheduling of high-density parking lots. IEEE Transactions on Cybernetics ,
pages 1–13, 2023. 1
[16] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,
Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai
Wang, et al. Planning-oriented autonomous driving. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 17853–17862, 2023. 1
[17] Stephen James, Kentaro Wada, Tristan Laidlow, and An-
drew J Davison. Coarse-to-fine q-attention: Efficient learn-
ing for visual robotic manipulation via discretisation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 13739–13748, 2022. 1
[18] Jiaming Ji, Jiayi Zhou, Borong Zhang, Juntao Dai, Xuehai
Pan, Ruiyang Sun, Weidong Huang, Yiran Geng, Mickel Liu,
and Yaodong Yang. Omnisafe: An infrastructure for accel-
erating safe reinforcement learning research. arXiv preprint
arXiv:2305.09304 , 2023. 6
[19] Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism
provably efficient for offline rl? In International Conference
on Machine Learning , pages 5084–5096. PMLR, 2021. 1
[20] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey
Levine. Conservative q-learning for offline reinforcement
learning. Advances in Neural Information Processing Sys-
tems, 33:1179–1191, 2020. 5
[21] Hoang Le, Cameron V oloshin, and Yisong Yue. Batch policy
learning under constraints. In International Conference on
Machine Learning , pages 3703–3712. PMLR, 2019. 1
[22] Jongmin Lee, Wonseok Jeon, Byungjun Lee, Joelle Pineau,
and Kee-Eung Kim. Optidice: Offline policy optimization
via stationary distribution correction estimation. In Interna-
tional Conference on Machine Learning , pages 6120–6130.
PMLR, 2021. 1, 2, 3
[23] Qingkai Liang, Fanyu Que, and Eytan Modiano. Acceler-
ated primal-dual policy optimization for safe reinforcement
learning. arXiv preprint arXiv:1802.06480 , 2018. 4
[24] Yongshuai Liu, Jiaxin Ding, and Xin Liu. Ipo: Interior-point
policy optimization under constraints. In Proceedings of the
AAAI conference on artificial intelligence , pages 4940–4947,
2020. 1, 2
[25] Zuxin Liu, Zhepeng Cen, Vladislav Isenbaev, Wei Liu,
Steven Wu, Bo Li, and Ding Zhao. Constrained variational
policy optimization for safe reinforcement learning. In In-
ternational Conference on Machine Learning , pages 13644–
13668. PMLR, 2022. 2
[26] Zuxin Liu, Zijian Guo, Haohong Lin, Yihang Yao, Ji-
acheng Zhu, Zhepeng Cen, Hanjiang Hu, Wenhao Yu,
Tingnan Zhang, Jie Tan, et al. Datasets and benchmarks
for offline safe reinforcement learning. arXiv preprint
arXiv:2306.09303 , 2023. 6
[27] Zuxin Liu, Zijian Guo, Yihang Yao, Zhepeng Cen, Wenhao
Yu, Tingnan Zhang, and Ding Zhao. Constrained decision
transformer for offline safe reinforcement learning. arXiv
preprint arXiv:2302.07351 , 2023. 2
[28] Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu. Mildly
conservative q-learning for offline reinforcement learning.
Advances in Neural Information Processing Systems , 35:
1711–1724, 2022. 5
26251
[29] Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir
Nachum, and Shixiang Gu. Deployment-efficient reinforce-
ment learning via model-based offline optimization. In In-
ternational Conference on Learning Representations , 2021.
1
[30] R ´emi Munos, Tom Stepleton, Anna Harutyunyan, and Marc
Bellemare. Safe and efficient off-policy reinforcement learn-
ing. Advances in neural information processing systems , 29,
2016. 1
[31] Santiago Paternain, Miguel Calvo-Fullana, Luiz FO Cha-
mon, and Alejandro Ribeiro. Safe policies for reinforcement
learning via primal-dual methods. IEEE Transactions on Au-
tomatic Control , 68(3):1321–1336, 2022. 1
[32] Nicholas Polosky, Bruno C Da Silva, Madalina Fiterau, and
Jithin Jagannath. Constrained offline policy optimization.
InInternational Conference on Machine Learning , pages
17801–17810. PMLR, 2022. 1, 2, 3
[33] Aditya Prakash, Kashyap Chitta, and Andreas Geiger. Multi-
modal fusion transformer for end-to-end autonomous driv-
ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 7077–7087,
2021. 1
[34] LA Prashanth. Policy gradients for cvar-constrained mdps.
InInternational Conference on Algorithmic Learning The-
ory, pages 155–169. Springer, 2014. 2
[35] David Silver, Guy Lever, Nicolas Heess, Thomas Degris,
Daan Wierstra, and Martin Riedmiller. Deterministic policy
gradient algorithms. In International conference on machine
learning , pages 387–395. Pmlr, 2014. 4
[36] Adam Stooke, Joshua Achiam, and Pieter Abbeel. Respon-
sive safety in reinforcement learning by pid lagrangian meth-
ods. In International Conference on Machine Learning ,
pages 9133–9143. PMLR, 2020. 1
[37] Chen Tessler, Daniel J. Mankowitz, and Shie Mannor. Re-
ward constrained policy optimization. In International Con-
ference on Learning Representations , 2019. 2, 4
[38] Garrett Thomas, Yuping Luo, and Tengyu Ma. Safe rein-
forcement learning by imagining the near future. Advances
in Neural Information Processing Systems , 34:13859–13869,
2021. 1, 2
[39] Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde.
End-to-end model-free reinforcement learning for urban
driving using implicit affordances. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 7153–7162, 2020. 1
[40] Akifumi Wachi and Yanan Sui. Safe reinforcement learning
in constrained markov decision processes. In International
Conference on Machine Learning , pages 9797–9806. PMLR,
2020. 2
[41] Akifumi Wachi, Yanan Sui, Yisong Yue, and Masahiro Ono.
Safe exploration and optimization of constrained mdps using
gaussian processes. In Proceedings of the AAAI Conference
on Artificial Intelligence , 2018. 2
[42] Xudong Wang, Long Lian, and Stella X Yu. Unsupervised
visual attention and invariance for reinforcement learning.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 6677–6687, 2021. 1[43] Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S
Merel, Jost Tobias Springenberg, Scott E Reed, Bobak
Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess,
et al. Critic regularized regression. Advances in Neural In-
formation Processing Systems , 33:7768–7778, 2020. 7
[44] Haoran Xu, Xianyuan Zhan, and Xiangyu Zhu. Constraints
penalized q-learning for safe offline reinforcement learning.
Proceedings of the AAAI Conference on Artificial Intelli-
gence , 36(8):8753–8760, 2022. 1, 2, 7
[45] Tengyu Xu, Yingbin Liang, and Guanghui Lan. Crpo: A
new approach for safe reinforcement learning with conver-
gence guarantee. In International Conference on Machine
Learning , pages 11480–11491. PMLR, 2021. 4
[46] Xintao Yan, Zhengxia Zou, Shuo Feng, Haojie Zhu, Haowei
Sun, and Henry X Liu. Learning naturalistic driving envi-
ronment with statistical realism. Nature Communications ,
14(1):2037, 2023. 1
[47] Long Yang, Jiaming Ji, Juntao Dai, Linrui Zhang, Binbin
Zhou, Pengfei Li, Yaodong Yang, and Gang Pan. Con-
strained update projection approach to safe policy optimiza-
tion. arXiv preprint arXiv:2209.07089 , 2022. 2
[48] Qisong Yang, Thiago D Sim ˜ao, Simon H Tindemans, and
Matthijs TJ Spaan. Wcsac: Worst-case soft actor critic for
safety-constrained reinforcement learning. In Proceedings of
the AAAI Conference on Artificial Intelligence , pages 10639–
10646, 2021. 2
[49] Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and
Peter J Ramadge. Projection-based constrained policy opti-
mization. arXiv preprint arXiv:2010.03152 , 2020. 2
[50] Yijun Yang, Jing Jiang, Tianyi Zhou, Jie Ma, and Yuhui
Shi. Pareto policy pool for model-based offline reinforce-
ment learning. In International Conference on Learning
Representations , 2021. 1
[51] Yijun Yang, Tianyi Zhou, Jing Jiang, Guodong Long, and
Yuhui Shi. Continual task allocation in meta-policy network
via sparse prompting. In International Conference on Ma-
chine Learning , pages 39623–39638. PMLR, 2023. 2
[52] Chengyang Ying, Xinning Zhou, Hang Su, Dong Yan, Ning
Chen, and Jun Zhu. Towards safe reinforcement learning
via constraining conditional value-at-risk. arXiv preprint
arXiv:2206.04436 , 2022. 2
[53] Ming Yu, Zhuoran Yang, Mladen Kolar, and Zhaoran Wang.
Convergent policy optimization for safe reinforcement learn-
ing.Advances in Neural Information Processing Systems , 32,
2019. 1
[54] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon,
James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma.
Mopo: Model-based offline policy optimization. Advances
in Neural Information Processing Systems , 33:14129–14142,
2020. 1
[55] Linrui Zhang, Li Shen, Long Yang, Shixiang Chen, Bo Yuan,
Xueqian Wang, and Dacheng Tao. Penalized proximal policy
optimization for safe reinforcement learning. arXiv preprint
arXiv:2205.11814 , 2022. 2
[56] Linrui Zhang, Qin Zhang, Li Shen, Bo Yuan, Xueqian Wang,
and Dacheng Tao. Evaluating model-free reinforcement
learning toward safety-critical tasks. Proceedings of the
26252
AAAI Conference on Artificial Intelligence , 37(12):15313–
15321, 2023. 2
[57] Qin Zhang, Linrui Zhang, Haoran Xu, Li Shen, Bowen
Wang, Yongzhe Chang, Xueqian Wang, Bo Yuan, and
Dacheng Tao. Saformer: A conditional sequence model-
ing approach to offline safe reinforcement learning. arXiv
preprint arXiv:2301.12203 , 2023. 2
[58] Ruiqi Zhang, Jing Hou, Guang Chen, Zhijun Li, Jianxiao
Chen, and Alois Knoll. Residual policy learning facilitates
efficient model-free autonomous racing. IEEE Robotics and
Automation Letters , 7(4):11625–11632, 2022. 1
[59] Yiming Zhang, Quan Vuong, and Keith Ross. First order
constrained optimization in policy space. Advances in Neu-
ral Information Processing Systems , 33:15338–15349, 2020.
2
[60] Weiye Zhao, Rui Chen, Yifan Sun, Tianhao Wei, and
Changliu Liu. State-wise constrained policy optimization.
arXiv preprint arXiv:2306.12594 , 2023. 2
[61] Weiye Zhao, Tairan He, Rui Chen, Tianhao Wei, and
Changliu Liu. State-wise safe reinforcement learning: A sur-
vey. arXiv preprint arXiv:2302.03122 , 2023. 2, 3, 4
26253
