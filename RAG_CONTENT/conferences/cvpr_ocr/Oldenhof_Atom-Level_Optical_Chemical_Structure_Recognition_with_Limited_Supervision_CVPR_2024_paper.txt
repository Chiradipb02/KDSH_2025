Atom-Level Optical Chemical Structure Recognition with Limited Supervision
Martijn Oldenhof1Edward De Brouwer1,2Adam Arany1Yves Moreau1
1ESAT - STADIUS, KU Leuven, Belgium
2Yale University, USA
{martijn.oldenhof, edward.debrouwer, adam.arany, yves.moreau }@esat.kuleuven.be
Abstract
Identifying the chemical structure from a graphical rep-
resentation, or image, of a molecule is a challenging pattern
recognition task that would greatly beneﬁt drug develop-
ment. Yet, existing methods for chemical structure recogni-
tion do not typically generalize well, and show diminished
effectiveness when confronted with domains where data is
sparse, or costly to generate, such as hand-drawn molecule
images. To address this limitation, we propose a new chem-
ical structure recognition tool that delivers state-of-the-art
performance and can adapt to new domains with a limited
number of data samples and supervision. Unlike previ-
ous approaches, our method provides atom-level localiza-
tion, and can therefore segment the image into the different
atoms and bonds. Our model is the ﬁrst model to perform
OCSR with atom-level entity detection with only SMILES
supervision. Through rigorous and extensive benchmark-
ing, we demonstrate the preeminence of our chemical struc-
ture recognition approach in terms of data efﬁciency, accu-
racy, and atom-level entity prediction.
1. Introduction
Molecules and chemical reactions represent the tokens of
the language of chemistry, which underlies applications
such as drug or new materials discovery. Molecules can be
represented by a molecular formula ( e.g.,C8H10N4O2), or
preferably by a more detailed structural formula—a graph-
ical representation showcasing the spatial arrangement of
atoms in the molecule. Isomers, molecules sharing the same
molecular formulas but differing in spatial atom arrange-
ment, typically exhibit distinct chemical and physical prop-
erties (as illustrated in Supplementary Material (SM) Sec-
tion8). Structural molecular formulas are thus ubiquitous
in chemistry publications, lab notes, patents, or text books.
This prevalence motivates the development of automatic
pipelines to perform chemical structure recognition, pars-
ing structural formulas from images. Such ability promises
more efﬁcient scientiﬁc literature browsing, automatic labnotes transcription, or chemical data mining, among others.
Recent advances in computer vision have allowed the
development of several chemical structure recognition
tools [ 5,19,24]. These tools can be classiﬁed into molecu-
lar graph predictions methods and atom-level entity predic-
tion methods. Molecular graph prediction methods only use
limited image annotation, such as SMILES, a serial nota-
tion of a molecule [ 5,24,33], and only predict the molecu-
lar graph. In contrast, atom-level entity prediction methods
leverage richer image annotations for training the model,
such as atom-level entity localization ( i.e., individual atoms
and bonds are annotated in the original image) [ 19,23].
These methods predict the molecular graph as well as the
localization of the different components of the molecule in
the original image. Figure 1illustrates the different types of
predictions for these two categories of models.
Previous research has shown that atom-level entity pre-
diction methods typically enjoy better training sample efﬁ-
ciency, requiring less images for achieving the same level
of performance [ 14]. This class of methods is also more
interpretable. Atom-level entity annotation can indeed help
identify the atoms that will be part of new chemical bond
in a reaction, and can also facilitate human evaluation and
correction when necessary, opening the way for synergis-
tic human-in-the-loop training strategies [ 23]. Nevertheless,
these advantages are compensated by the necessity to pro-
vide rich image annotation in the training data. Unfortu-
nately, such supervision is often unavailable in many data
domains, such as hand-drawn images. Yet, hand-drawn im-
ages represent a prevalent format in chemical notations and
sketches. The strict dependency of existing atom-level en-
tity prediction methods on rich image annotation thus pre-
vents their deployment to crucial data domains.
Our research addresses these limitations by introducing
a state-of-the-art chemical structure recognition tool, which
(1) predicts a molecular graph from images, (2) provides
atom-level localization in the original image, and (3) adapts
to new data domain with a limited number of data samples
and supervision. Our architecture relies on a object detec-
tion backbone coupled with a graph construction strategy
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
17669
BrNNOInput imagePredicted atom-level annotationsPredicted molecular graphFigure 1. Problem setup. Our chemical structure recognition method takes an input image and predict atom-level entities predictions
(atoms, bonds, charges, and stereocenters). This rich annotation can then be used to construct the molecular graph. Atom-level entity
prediction models, like ours, predict both the atom-level annotations and molecular graph from the image. In contrast, molecular graph
predictions models only predict the molecular graph and do not provide any localization in the original image.
that is pretrained on synthetic data, where the localization
of each atom-level entity is known. We then leverage the
atom-level entity localization, coupled with a efﬁcient self-
relabeling strategy, to aptly transfer to new domains where
no localization is available (typically only image-SMILES
pairs are available). This results in a state-of-the-art and
highly data efﬁcient architecture, as demonstrated by our
rigorous benchmarking.
Key contributions : (1) We propose a novel frame-
work for chemical structure recognition that predicts atom-
level localizations trained on a target domain with only
SMILES supervision. (2) We show that our method re-
sults in state-of-the-art performance on challenging hand-
drawn molecule images, with a remarkable data efﬁciency.
(3) We release a new curated dataset containing hand-drawn
molecules with atom-level annotations.
Our implementation is available on Github:
https://github.com/molden/atomlenz
2. Background
Our work builds upon the chemical structure recognition
literature and takes an object detection approach for solv-
ing this task. To enable ﬁne-tuning of the model where no
atom-level annotations are present, we leverage advances in
weakly supervised object detection.
2.1. Chemical structure recognition
Optimal chemical structure recognition consists in inferring
the structural formulae of a chemical compound based on
an image representation of it. The large majority of exist-
ing methods performing this task take the image as input,
and predict the SMILES (simpliﬁed molecular-input line-
entry system) representation of the molecule [ 33]. SMILES
consists of strings of ASCII characters that are obtained
by printing the chemical symbols encountered in a depth-
ﬁrst tree traversal of the molecular graph. This serial nota-tion provides, at ﬁrst sight, a convenient representation for
training machine learning models, while encoding geomet-
ric information about the molecular graph. This justiﬁed
the popularity of SMILES-based chemical structure recog-
nition models [ 5,24].
Nevertheless, SMILES do not provide a natural chemi-
cal representation and do not readily encode the geometric
properties of the molecules. This hampers the trainability
of the underlying machine learning model [ 14]. This limi-
tation motivated the development of methods predicting the
molecular graph and capable of leveraging richer image an-
notations, such as atom-level localizations [ 19,23]. Our
work belongs to this category and therefore inherits these
strengths. However, we extend previous approaches by pro-
viding a mechanism to ﬁne-tune the model to new data do-
mains where only SMILES annotations are available.
2.2. Object detection
Our architecture draws heavily on the literature on objec-
tion detection in images [ 9,10,26,27,37], which under-
lies a wide array of high-level machine learning applica-
tions [ 3,11,12,31]. We refer to [ 38] for a recent review
of the ﬁeld. Training object detection models typically re-
quires comprehensive image annotations, such as the pre-
cise coordinates of bounding boxes and the associated la-
bels for every object contained within each image. How-
ever, and crucially for our application, these annotations are
not consistently accessible within certain domains of inter-
est. This scarcity of detailed annotations has spurred the
development of weakly supervised object detection meth-
ods.
2.3. Weakly supervised object detection
This category of methods enables the training of object de-
tection models without the necessity for precise bounding-
box annotations. As a result, these approaches can be di-
17670
rectly applied to target domains where such annotations
are unavailable. Diverse variations of Weakly Supervised
Object Detection (WSOD) architectures have emerged, re-
lying on a range of implementations, including Multiple
Instance Learning (MIL) [ 16,29] and Class Activation
Map (CAM) [ 2,36] approaches. Other advanced WSOD
techniques incorporate knowledge transfer from a source
domain [ 6,15,21,32,35]. Among these approaches,
ProbKT [ 21] distinguishes itself as a versatile method, rely-
ing on probabilistic reasoning, which offers the capacity to
train atom-level localization models using chemical back-
ground information obtained from SMILES and logical rea-
soning. Our architecture leverages this approach.
3. Method
Our architecture is composed of four high-level modules:
(1) an object detection backbone, which is trained on richly
annotated images with atom-level entities, (2) a molecular
graph constructor that assembles a molecular graph from
the set of atom-level predictions, (3) a weakly supervised
training scheme that enables ﬁne-tuning the model on new
domains without rich annotations. Additionally, we design
a chemically informed combination of experts, ChemEx-
pert, that can further boost the prediction performance. The
weakly supervised training scheme of the object detection
backbone is visualized in Figure 2.
3.1. Object detection backbone
At the core of our architecture lies an object detection model
that is responsible for detecting and labeling atom-level en-
tities in the image. The objects in the image are there-
fore the atom-level entities such as atoms or bonds. While
many object detection methods exist and can be used inter-
changeably in our architecture, we used the Faster RCNN
model [ 27] in our experiments. It is fast to train, robust, and
simultaneously localizes and classiﬁes all objects in a sin-
gle step. The object detection backbone is trained by min-
imizing a multi-task loss Lmixing a multi-class log loss
Lclsapplied on the predicted class ˆcof the objects and a
regression loss Lregapplied to the predicted bounding box
coordinates ˆb=(ˆbx,ˆby,ˆbw,ˆbh)of the objects.
L=Lcls(c,ˆc)+Lreg(b,ˆb) (1)
Bonds, atoms, or charges are graphically very differ-
ent. To account for this heterogeneity, we train four distinct
object detection models, each tailored to a speciﬁc atom-
level entity: atoms ( Oa), bonds ( Ob), charge objects ( Oc),
and stereocenters ( Os). A stereocenter, also known as a
stereogenic center, refers to an atom within a molecule that
carries groups in a way that exchanging any two of these
groups results in a stereoisomer [ 17]. More details about
stereochemistry can be found in SM Section 8.Each type of atom-level entity comprises multiple
classes cthat the object detection backbone aims at label-
ing (via Lcls). For instance, the bond object encompasses
categories such as single bond, double bond, triple bond,
aromatic bond, dashed bond, and wedged bond. Illustra-
tions of different atom-level entity types and classes can be
found in SM Section 10.
3.2. Molecular graph constructor
The output of the object detection backbone is a list of de-
tected atom-level entities in the image, along with their pre-
dicted label and position. The objective of the molecular
graph constructor is to assemble a chemically sound molec-
ular graph from this list of predictions. This graph can then
be easily converted to a SMILES format.
The output graph G(V,E)is composed of a set of ver-
tices V, corresponding to atoms, and edges Ecorrespond-
ing to bonds. Each vertex and edge has a label ( e.g., a node
can be a carbon atom with a positive charge, an edge can be
a double bond). Algorithm 1outlines the series of steps in-
volved in constructing the molecular graph from atom-level
entity predictions Oa(atoms), Ob(bonds), Oc(charges),
andOs(stereocenters). It proceeds in four steps: (1) a ﬁl-
tering step, (2) a node creation step, (3) an edge creation
step, and (4) a validation step.
Theﬁltering step ﬁlters atoms from Oathat are severely
overlapping on the image. When multiple atom objects
show an Intersection over Union (IoU) score exceeding a
speciﬁed threshold, only the object with the highest score is
retained.
In the node creation step , we ﬁrst attach charges to atom
objects. Overlapping atom and charge objects exceeding
a speciﬁc IoU threshold are then merged. The function
checkCharges is responsible for determining which atom
objects should carry a charge. A similar procedure is sub-
sequently applied to identify atoms functioning as stereo-
centers, utilizing checkStereoChem for this purpose. The
list of all atom objects, along with their potentially assigned
charges or stereocenters are then added to the list of graph
vertices.
In the edge creation step , we iterate over all bond ob-
jects and evaluate which vertices (atoms) overlap with these
bonds, with the function checkEdge . If only two candi-
date atoms are identiﬁed, the algorithm proceeds to add the
edge to the graph. However, when more than two overlap-
ping candidates emerge, the algorithm selects the two most
probable ones, factoring in the orientation of the edge and
the atoms involved.
Lastly, the validation step identiﬁes potential chemistry-
related issues through ChemistryProblems and endeav-
ors to resolve them via SolveChemistryProblems to en-
sure the prediction of a chemically valid molecular graph.
For instance, each chemical element is assigned a valence
17671
Object detection backboneSMILES:C1=NN=C(O1)CBr
Error
CorrectionBackpropagation
RetrainingProbKT*EditCorrectionRelabelingFigure 2. Weakly supervised training . The weakly supervised training data set consists of images of molecular depictions paired with
SMILES. The input image is used by the object detection backbone to predict the atom-level entities while the SMILES is used by ProbKT*
and the edit-correction scheme. In the ﬁrst phase, ProbKT* will perform backpropagation to update the object detection backbone using
probabilistic reasoning. In the second phase, both ProbKT* and the edit-correction mechanism will generate pseudo-labels for the atom-
level entities, which are used to retrain the object detection backbone.
number, indicating the atom’s capability to establish bonds
with other atoms. If our algorithm detects within the out-
put graph containing atoms with more bonds than their va-
lence numbers permit, the SolveChemistryProblems func-
tion would attempt to remove bonds iteratively until a graph
is formed without valence errors.
To maintain conciseness in our experiments and re-
sults, we refer to the combination of the object detection
backbone and the molecular graph constructor as Atom-
Lenz (ATOM-Level ENtity localiZer). Further information
regarding the subroutines used in the molecular graph con-
structor is available in SM Section 9.
3.3. Weakly supervised training
Our architecture uses an object detection backbone to pre-
dict atom-level entities, which requires rich image annota-
tions, such bounding boxes for every object type, including
atoms, bonds, charges, and stereocenters within the images.
While such a level of supervision can be obtained syntheti-
cally with tools like RDKit [ 1], it is usually not available in
real-world target domains, such as hand-drawn images. In
such domains, only SMILES are typically available. To en-
able the ﬁne-tuning of the object detection backbone with
only SMILES information, we use a weakly supervised
training mechanism that combines (1) a probabilistic log-
ical reasoning module that allows to differentiate through
the object detection backbone with only weak supervision,
and (2) a graph edit-correction mechanism that allows ﬁne-
tuning on less frequent atoms and bonds. A graphical out-
line of the weakly supervised training procedure is given in
Figure 2.Algorithm 1: Molecular graph constructor
Input: Atom-level predictions Oa,Ob,Oc,Os
Result: Graph G(V,E), vertices Vand edges E
1˜Oa=ﬁlterAtoms( Oa); //filtering step
2V=[] ; //vertices of graph
/*node creation step */
3foroain˜Oado
4 oa
c=checkCharges( Oc,oa)
5 oa
c,s=checkStereoChem( Os,oa
c)
6 V.appendAtom( oa
c,s)
7end
8E=[] ; //edges of graph
/*edge creation step */
9forobinObdo
10 candAtoms = checkEdge( V,ob)
11 iflen(candAtoms) == 2 then
12 E.appendBond( ob,candAtoms)
13 end
14 iflen(candAtoms) >2then
15 ﬁlteredAtoms = ﬁlterCands(candAtoms)
E.appendBond( ob,ﬁlteredAtoms)
16 end
17end
/*validation step */
18ifChemistryProblems( G(V,E)) == True then
19 G(V,E)=
SolveChemistryProblems( G(V,E))
20end
17672
Backpropagation with weak supervision
To update the weights of the object detection backbone with
only SMILES supervision, we use the ProbKT [ 21] frame-
work. This weakly supervised domain adaption technique
uses probabilistic programming for ﬁne-tuning object de-
tection models with a wide range of supervision signals,
and is thus particularly suited for our application. In our
experiments, we used ProbKT⇤, a computationally efﬁcient
variant of ProbKT that relies on Hungarian matching.
ProbKT⇤allows differentiating through the object de-
tection backbone with only SMILES supervision. For bet-
ter performance, it also includes a relabeling mechanism,
where conﬁdent predictions are used as new atom-level an-
notation of the target domain images. This strategy effec-
tively creates a richly annotated dataset that can be used to
ﬁne-tune the object detection backbone directly.
Edit-correction mechanism
While ProbKT⇤is generally effective at performing weakly
supervised domain adaptation, it fails when dealing with
rare atoms or bonds types. We therefore combine ProbKT⇤
with a new edit-correction mechanism [ 20] designed to de-
tect and rectify minor errors in model predictions. Based
on the SMILES, one can generate a reference true graph,
although not aligned on the original image. The edit-
correction mechanism solves an optimization problem that
aims at ﬁnding the smallest edit on the predicted graph such
that the true and corrected graphs are isomorphic. While
this optimization would be intractable in general, focusing
on small edits makes it computationally feasible. If such a
correction is found, it is used to annotate the image which
can then be used to ﬁne-tune the object detection backbone.
Combined weakly supervised training
When ﬁne-tuning on a new target domain, we proceed by it-
eratively applying ProbKT⇤and the edit-correction scheme.
In practice, we start with a few iterations of ProbKT⇤.W e
then use multiple iterations of the edit-correction scheme
until the validation performance stops improving. For sake
of conciseness in our experiments results, we abbreviate the
combination of both approaches as EditKT*.
3.4. ChemExpert: Combination of experts
For the ﬁnal prediction of our architecture, we propose to
use a combination of experts, which is constrained by chem-
ical soundness of the model predictions. We call this mod-
ule ChemExpert. It relies on a list of chemical structure
recognition tools, ordered by the user’s preference in terms
of the predictions. The ﬁrst tool serves as the most trusted
model. At inference time, ChemExpert iteratively checks
the validity of the prediction of each model in the list. Ifa chemical issue is identiﬁed, the agent evaluates the next
model in the list. The module returns the prediction of the
ﬁrst model with no chemical issues detected. This strategy
enables us to incorporate predictions from additional tools
alongside those generated by our core model, thereby im-
proving predictive performance. In practice, we use a com-
bination of DECIMER [ 25] and our approach.
4. Datasets
4.1. Synthetically generated dataset
For the pretraining of the object detection models, we gen-
erate images synthetically using RdKit [ 1] and Indigo [ 22]
paired with bounding boxes delineating all objects within,
including atoms, bonds, charges, and stereocenters, simi-
larly to what is used in other chemical structure recogni-
tion tools [ 19,23]. Speciﬁcally, we collect approximately
214,000 chemical compounds in SMILES format from the
ChEMBL [ 8] database. To enhance the method’s resilience
to stylistic variations, we introduce variability in elements
such as fonts, font sizes, line widths, and the spacing be-
tween multiple bonds during image generation. More de-
tails on this dataset can be found in SM Section 7.
4.2. Hand-drawn images datasets
To facilitate the training, ﬁne-tuning, and testing of our
models on hand-drawn images, we meticulously curate
multiple datasets. We begin with the dataset introduced by
Brinkhaus et al. [ 4], which consists of hand-drawn chemical
depictions matched with their corresponding SMILES rep-
resentations. This dataset is partitioned into 4,070 samples
for training and validation purposes, along with an addi-
tional 1,018 samples for testing. These sets are referred to
as the hand-drawn training set and the hand-drawn test set .
In addition to this primary test set, we incorporate an
extra test dataset of 614 hand-drawn chemical depictions
sourced from Weir et al. [ 34], which we call the ChemPix
test set , to further evaluate the performance of the models
on hand-drawn images.
4.3. Atom localization dataset
To assess our models’ capability for object localization, we
employ a synthetically generated dataset using RdKit [ 1]
provided by Oldenhof et al. [ 21]. This dataset encompasses
1,000 images depicting chemical structures, each meticu-
lously annotated with bounding boxes outlining the posi-
tions and corresponding classes of all the atoms within the
molecules. Example images for each test dataset are shown
in Figure 3and SM Section 7.
5. Experiments and Results
Our experiments investigate the performance of our ap-
proach on hand-drawn images of chemical structures, as
17673
(a) Hand-drawn test set
 (b) ChemPix test set
(c) Atom Localization test set, labeled with
bounding boxes, here illustrated with different
color for every atom type.
Figure 3. Image samples from the dataset. Different example samples for the different datasets used in experiments. The hand-drawn
and ChemPix datasets are used to assess the domain adaptation and out-of-domain performance. The atom localization dataset is used for
testing object localization.
this domain suffers from limited data availability and has
been shown to be a weak point of existing tools. We eval-
uate our architecture and state-of-the-art baselines on four
distinct fronts: (1) molecular recognition on the new target
domain ( i.e.,only predicting the SMILES), (2) atom-level
entity localization, (3) training efﬁciency (when retrained
from scratch), and (4) model evaluation per atom and bond
type.
Baselines We compare our architecture with the follow-
ing baselines. DECIMER [24] is an image-transformer
approach trained on more than 400 million synthetically
generated data samples. The authors of DECIMER have
also introduced a version speciﬁcally tailored for hand-
drawn images (DECIMER ﬁne-tuned [ 25]). Although it
is trained on synthetically generated images, the training
dataset of this version mimics the style of hand-drawn im-
ages more closely. Img2Mol [5] integrates a deep convo-
lutional neural network trained on molecule depictions (11
million synthetically generated images) with a pretrained
decoder. MolScribe [23] and ChemGrapher [19] em-
ploy atom-level entity localization annotations in their train-
ing process on synthetically generated images. These are
the only baselines that can predict atom-level annotations,
alongside the SMILES predictions. ChemGrapher is trained
on 114,000 generated images and MolScribe on 1 million
generated images. Lastly, OSRA [7] is a non-trainable,
rule-based approach.
5.1. Performance of molecular recognition on hand-
drawn images
We compare the performance of our approach with the base-
lines on the hand-drawn and ChemPix dataset. Results are
given in Table 1. To assess the impact of our ﬁne-tuning
strategy, we evaluate three versions of our architecture.
The ﬁrst is a version trained on the synthetic dataset but
not ﬁne-tuned to the new hand-drawn dataset (AtomLenz).The second is ﬁne-tuned to the hand-drawn dataset using
EditKT⇤. The third is ChemExpert, combining DECIMER
ﬁne-tuned and AtomLenz. Performance of other combina-
tions in ChemExpert are reported in SM Section 11.
We assess the molecular structure prediction perfor-
mance using accuracy and Tanimoto similarity. Tanimoto
similarity ( T)[30], a widely used metric for quantifying
molecular similarity, to assess the resemblance between the
model’s predictions and the actual molecular graphs. Tani-
moto similarity values range from 0 to 1, with higher values
indicating greater similarity. A Tanimoto similarity of 1 in-
dicates that the structural descriptors are identical or that
they are matching ‘on-bits’ in a binary ﬁngerprint. The bi-
nary ﬁngerprint employed to measure the Tanimoto similar-
ity is the Extended-connectivity ﬁngerprint [ 28] with radius
3 (ECFP6) and ﬁngerprint length of 2048. More details on
the calculation of the ECFP6 ﬁngerprint and other ﬁnger-
prints can be found in SM Section 11.
Our tables report both the accuracy, computed by count-
ing the instances where the predicted structures have iden-
tical structural ECFP6 descriptors (denoted by a Tanimoto
similarity of 1) and the average Tanimoto similarity. Addi-
tional measured metrics can be found in SM Section 11.
For both datasets hand-drawn test set andChemPix test
set, our ChemExpert performs best. This demonstrates that
the combination of our approach with other baselines results
in state-of-the-art performance. We further appreciate a sig-
niﬁcant increase in performance from EditKT⇤, compared
to the non-ﬁne-tuned version, highlighting the effectiveness
of our ﬁne-tuning approach.
5.2. Performance of atom-level localization
In Table 2, we assess the performance of the different meth-
ods in terms of their atom-level localization abilities. We
employ a test set from Oldenhof et al. [ 21], which com-
prises images of chemical representations along with the
corresponding atom objects. We use two evaluation met-
17674
hand-drawn test set ChemPix test set
Method Acc.( T=1)T Acc.( T=1)T
DECIMER (v2.2.0)[ 24] 0.295 0.451 0.05 0.1
DECIMER ﬁne-tuned(v2.2.0)[ 25] 0.622 0.727 0.508 0.643
Img2Mol[ 5] 0.084 0.275 0.015 0.084
MolScribe[ 23] 0.102 0.288 0.269 0.417
ChemGrapher[ 19] 0.002 0.065 0.187 0.286
OSRA[ 7] 0.006 0.065 0.047 0.071
AtomLenz 0.009 0.087 0.054 0.064
AtomLenz+EditKT* 0.338 0.484 0.484 0.605
ChemExpert([ 25],AtomLenz+EditKT*) 0.635 0.749 0.518 0.655
Table 1. Benchmark results on target domain (hand-drawn images test set) and out of domain ChemPix test set. Both the accuracy,
computed by counting the instances where the predicted structures have identical structural ECFP6 descriptors (denoted by a Tanimoto ( T)
similarity of 1) and the average Tanimoto similarity ( T) are reported.
rics: the count accuracy (which can be evaluated without
bounding box predictions), and the mean average precision
(mAP) localization of the bounding boxes. The atoms count
accuracy measures the ability to predict the correct num-
ber of atom types in each image. The average precision
is computed as the weighted mean of precisions at various
Intersect over Union (IoU) thresholds, with the weight re-
ﬂecting the increase in recall from the previous threshold.
Mean Average Precision represents the average of AP val-
ues across each class. We use the rather low IoU thresholds
of[0.05,0.1,0.15,0.2,0.25,0.3,0.35]in our experiments,
considering the relatively small size of the bounding boxes
of interest (see Figure 5c), where signiﬁcant overlap with
the true bounding boxes is not anticipated. Methods that do
not provide any form of localization are marked with n/a in
the table.
In Table 2, we observe the commendable localization
performance of our pretrained backbone model (AtomLenz)
and also note the high counting accuracy of DECIMER.
We posit that both outcomes may be attributed to the na-
ture of the test dataset, aligning with the characteristics of
the images used for training both DECIMER and our core
pretrained model (AtomLenz). Additionally, we note that
DECIMER was trained on 2000 times more images than
our architecture, and does not provide any localization.
5.3. Training efﬁciency
Baseline architectures were trained on signiﬁcantly larger
number of images than our model. Molscribe uses 4 times
more samples, while DECIMER uses a staggering 2000
times more. In Table 3, we evaluate the sample complex-
ity of the different methods by retraining them from scratch
on the same small training dataset, which mimics limited
data availability scenarios. We use the hand-drawn train-
ing set (4,070 data samples), enriched with atom-level en-Method Count Acc. mAP
DECIMER (v2.2.0)[ 24] 0.973 n/a
DECIMER ﬁne-tuned (v2.2.0)[ 24] 0.97 n/a
Img2Mol[ 5] 0.929 n/a
MolScribe[ 23] 0.829 0.008
ChemGrapher[ 19] 0.248 0.002
OSRA[ 7] 0.255 n/a
AtomLenz 0.602 0.801
Table 2. Benchmark results on object (atom) detection test set to
compare localization performance.
tity localization annotations generated using EditKT* as the
training dataset. We observe that the methods that lever-
age these atom-level entity annotations tend to fare better
(ChemGrapher [ 19] and MolScribe [ 23]) than the ones us-
ing SMILES as only supervision signal (DECIMER [ 24]
and Img2Mol [ 5]). Our approach signiﬁcantly outperforms
all baselines at this task, highlighting the remarkable data
efﬁciency of our architecture. These ﬁndings align with
those in the work of Hormazabal et al. [ 13], where the au-
thor concluded that the use of atom-level entity annotations
can enhance data efﬁciency during training. The hand-
drawn images utilized in this experiment, along with the
corresponding bounding box labels for 1417 images, we re-
lease as a novel annotated dataset. More info in the SM
Section 7.
5.4. Fine-grained model evaluation
Additionally, we conduct a detailed performance analysis
of the most effective models from Table 1, presented in
Figure 4. This ﬁgure showcases the count accuracies per
atom or bond type. For each speciﬁc type, we identify im-
17675
Method Acc.( T=1) T
DECIMER (v2.2.0)[ 24] 0.001 0.039
Img2Mol[ 5] 0.0 0.0867
MolScribe[ 23] 0.013 0.0865
ChemGrapher[ 19] 0.004 0.067
AtomLenz 0.338 0.484
Table 3. All methods are retrained from scratch on same training
dataset (4070 samples of hand-drawn images) to asses data efﬁ-
ciency. Benchmark results on hand-drawn images test set. Both
the accuracy, computed by counting the instances where the pre-
dicted structures have identical structural ECFP6 descriptors (de-
noted by a Tanimoto ( T) similarity of 1) and the average Tanimoto
similarity ( T) are reported.
ages featuring that particular atom or bond type, then exam-
ine the predictions made by the methods on these images.
Subsequently, we calculate the count accuracies for the pre-
dicted objects of the speciﬁc type within these images. For
instance, when analyzing the ’triple bond’ type, we select
test images where at least one triple bond is depicted in
the molecule and evaluate whether the method accurately
predicts the correct number of triple bonds in the resulting
molecular graph.
The plot in Figure 4exhibits distinct patterns between
‘AtomLenz+EditKT* ’ and Decimer ﬁne-tuned [ 25]. For
example ‘AtomLenz+EditKT* ’ performs better on images
with Chlorine (Cl), Fluorine (F), and Phosphorus(P) com-
pared to Decimer ﬁne-tuned but worse on bonds. This vari-
ability may clarify why combining both predictions into
ChemExpert leads to improved performance, as errors tend
to occur on different samples, and the two approaches com-
plement each other. The same analysis is performed for the
ChemPix dataset in the SM section 11in Figure 11.
6. Conclusion
This study has undertaken a comprehensive evaluation of
various methods for chemical structure recognition, with a
primary focus on the challenging domain of hand-drawn
images. Our ﬁndings reveal insights into the strengths
and limitations of existing tools and provide a compelling
case for the efﬁcacy of our approach. We showed that our
method fares competitively despite a lower number of train-
ing samples, and resulted in state-of-the-art performance
when combined with previous approaches. Our experi-
ments highlighted our method’s proﬁciency in precisely lo-
calizing atom-level entities, a feature notably lacking in
many existing tools. Importantly, we showed that our ar-
chitecture is remarkably more data-efﬁcient than previous
models
Despite these improvements in chemical structure recog-
Figure 4. Count accuracies per type over images if type is present
in image for hand-drawn test set. We observe errors of ’Atom-
Lenz+EditKT*’ and ’DECIMER ﬁne-tuned’ tend to occur on dif-
ferent samples. Combining both approaches in ChemExpert im-
proves performance.
nition, reliably predicting the molecular structure from
hand-drawn remains a challenge, and higher prediction per-
formance would be required for a wide adoption of these
tools. We hope that the release of our curated hand-drawn
molecules images dataset, with detailed atom-level annota-
tions, to the community will contribute to the development
of more efﬁcient and reliable tools.
Acknowledgments
AA, MO and YM are funded by (1) Research Coun-
cil KU Leuven: Symbiosis 4 (C14/22/125), Sym-
biosis3 (C14/18/092); (2) Federated cloud-based Ar-
tiﬁcial Intelligence-driven platform for liquid biopsy
analyses (C3/20/100); (3) CELSA - Active Learning
(CELSA/21/019); (4) European Union’s Horizon 2020
research and innovation programme under the Marie
Skłodowska-Curie grant agreement No. 956832; (5) Flem-
ish Government (FWO: SBO (S003422N), Elixir Bel-
gium (I002819N), SB and Postdoctoral grants: S003422N,
1SB2721N, 1S98819N, 12Y5623N) and (6) VLAIO PM:
Augmenting Therapeutic Effectiveness through Novel An-
alytics (HBC.2019.2528); (7) YM, AA, and MO are afﬁl-
iated to Leuven.AI and received funding from the Flem-
ish Government (AI Research Program). Computational re-
sources and services used in this work were partly provided
by the VSC (Flemish Supercomputer Center), funded by
the Research Foundation - Flanders (FWO) and the Flemish
Government – department EWI.
17676
References
[1]Rdkit: Open-source cheminformatics. accessed on
01.02.2022. 4,5,1
[2]Wonho Bae, Junhyug Noh, and Gunhee Kim. Rethinking
class activation mapping for weakly supervised object local-
ization. In European Conference on Computer Vision , pages
618–634. Springer, 2020. 3
[3]Aseem Behl, Omid Hosseini Jafari, Siva
Karthik Mustikovela, Hassan Abu Alhaija, Carsten Rother,
and Andreas Geiger. Bounding boxes, segmentations and
object coordinates: How important is recognition for 3d
scene ﬂow estimation in autonomous driving scenarios?
InProceedings of the IEEE International Conference on
Computer Vision , pages 2574–2583, 2017. 2
[4]Henning Otto Brinkhaus, Achim Zielesny, Christoph Stein-
beck, and Kohulan Rajan. Decimer - hand-drawn molecule
images dataset, 2022. 5,1
[5]Djork-Arn ´e Clevert, Tuan Le, Robin Winter, and Flori-
ane Montanari. Img2mol–accurate smiles recognition from
molecular graphical depictions. Chemical science , 12(42):
14174–14181, 2021. 1,2,6,7,8
[6]Thomas Deselaers, Bogdan Alexe, and Vittorio Ferrari.
Weakly supervised localization and learning with generic
knowledge. International journal of computer vision , 100
(3):275–293, 2012. 3
[7]Igor V Filippov and Marc C Nicklaus. Optical structure
recognition software to recover chemical information: Osra,
an open source solution, 2009. 6,7,8
[8]Anna Gaulton, Anne Hersey, Micha łNowotka, A Patricia
Bento, Jon Chambers, David Mendez, Prudence Mutowo,
Francis Atkinson, Louisa J Bellis, Elena Cibri ´an-Uhalte,
et al. The chembl database in 2017. Nucleic acids research ,
45(D1):D945–D954, 2017. 5,1
[9]Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-
national conference on computer vision , pages 1440–1448,
2015. 2
[10] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
580–587, 2014. 2
[11] Eleonora Giunchiglia, Mihaela C ˘at˘alina Stoian, Salman
Khan, Fabio Cuzzolin, and Thomas Lukasiewicz. Road-r:
The autonomous driving dataset with logical requirements.
arXiv preprint arXiv:2210.01597 , 2022. 2
[12] Ibtihaal M Hameed, Sadiq H Abdulhussain, and Basheera M
Mahmmod. Content-based image retrieval: A review of re-
cent trends. Cogent Engineering , 8(1):1927469, 2021. 2
[13] Rodrigo Hormazabal, Changyoung Park, Soonyoung Lee,
Sehui Han, Yeonsik Jo, Jaewan Lee, Ahra Jo, Seung Hwan
Kim, Jaegul Choo, Moontae Lee, et al. Cede: A collection
of expert-curated datasets with atom-level entity annotations
for optical chemical structure recognition. 7
[14] Rodrigo Hormazabal, Changyoung Park, Soonyoung Lee,
Sehui Han, Yeonsik Jo, Jaewan Lee, Ahra Jo, Seung Hwan
Kim, Jaegul Choo, Moontae Lee, et al. Cede: A collection
of expert-curated datasets with atom-level entity annotationsfor optical chemical structure recognition. Advances in Neu-
ral Information Processing Systems , 35:27114–27126, 2022.
1,2
[15] Naoto Inoue, Ryosuke Furuta, Toshihiko Yamasaki, and Kiy-
oharu Aizawa. Cross-domain weakly-supervised object de-
tection through progressive domain adaptation. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 5001–5009, 2018. 3
[16] Dong Li, Jia-Bin Huang, Yali Li, Shengjin Wang, and Ming-
Hsuan Yang. Weakly supervised object localization with
progressive domain adaptation. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition ,
pages 3512–3520, 2016. 3
[17] Kurt Mislow and Jay Siegel. Stereoisomerism and local chi-
rality. Journal of the American Chemical Society , 106(11):
3319–3328, 1984. 3
[18] H. L. Morgan. The generation of a unique machine descrip-
tion for chemical structures-a technique developed at chemi-
cal abstracts service. Journal of Chemical Documentation ,5
(2):107–113, 1965. 6
[19] Martijn Oldenhof, Adam Arany, Yves Moreau, and Jaak
Simm. Chemgrapher: optical graph recognition of chemical
compounds by deep learning. Journal of chemical informa-
tion and modeling , 60(10):4506–4517, 2020. 1,2,5,6,7,
8
[20] Martijn Oldenhof, Adam Arany, Yves Moreau, and Jaak
Simm. Self-labeling of fully mediating representations by
graph alignment. In Benelux Conference on Artiﬁcial Intel-
ligence , pages 46–65. Springer, 2021. 5
[21] Martijn Oldenhof, Adam Arany, Yves Moreau, and Edward
De Brouwer. Weakly supervised knowledge transfer with
probabilistic logical reasoning for object detection. In The
Eleventh International Conference on Learning Representa-
tions (ICLR) , 2023. 3,5,6
[22] Dmitry Pavlov, Mikhail Rybalkin, Boris Karulin, Mikhail
Kozhevnikov, Alexey Savelyev, and A Churinov. Indigo:
universal cheminformatics api. Journal of cheminformatics ,
3(Suppl 1):P4, 2011. 5,1
[23] Yujie Qian, Jiang Guo, Zhengkai Tu, Zhening Li, Connor W
Coley, and Regina Barzilay. Molscribe: Robust molecular
structure recognition with image-to-graph generation. Jour-
nal of Chemical Information and Modeling , 63(7):1925–
1934, 2023. 1,2,5,6,7,8
[24] Kohulan Rajan, Achim Zielesny, and Christoph Steinbeck.
Decimer: towards deep learning for chemical image recog-
nition. Journal of Cheminformatics , 12(1):1–9, 2020. 1,2,
6,7,8
[25] Kohulan Rajan et al. Decimer.ai: an open platform for auto-
mated optical chemical structure identiﬁcation, segmentation
and recognition in scientiﬁc publications. Nat Commun , 14
(5045), 2023. 5,6,7,8
[26] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Uniﬁed, real-time object de-
tection. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 779–788, 2016. 2
[27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
17677
proposal networks. Advances in neural information process-
ing systems , 28, 2015. 2,3
[28] David Rogers and Mathew Hahn. Extended-connectivity ﬁn-
gerprints. Journal of Chemical Information and Modeling ,
50(5):742–754, 2010. PMID: 20426451. 6
[29] Hyun Oh Song, Ross Girshick, Stefanie Jegelka, Julien
Mairal, Zaid Harchaoui, and Trevor Darrell. On learning to
localize objects with minimal supervision. In International
Conference on Machine Learning , pages 1611–1619. PMLR,
2014. 3
[30] Taffee T Tanimoto. Elementary mathematical theory of clas-
siﬁcation and prediction. 1958. 6
[31] Matteo Tomei, Marcella Cornia, Lorenzo Baraldi, and Rita
Cucchiara. Art2real: Unfolding the reality of artworks via
semantically-aware image-to-image translation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 5849–5859, 2019. 2
[32] Jasper Uijlings, Stefan Popov, and Vittorio Ferrari. Revis-
iting knowledge transfer for training object class detectors.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 1101–1110, 2018. 3
[33] David Weininger. Smiles, a chemical language and informa-
tion system. 1. introduction to methodology and encoding
rules. Journal of chemical information and computer sci-
ences , 28(1):31–36, 1988. 1,2
[34] Hayley Weir, Keiran Thompson, Amelia Woodward, Ben-
jamin Choi, Augustin Braun, and Todd J. Mart ´ınez.
Chempix: automated recognition of hand-drawn hydrocar-
bon structures using deep learning. Chem. Sci. , 12:10622–
10633, 2021. 5
[35] Yuanyi Zhong, Jianfeng Wang, Jian Peng, and Lei Zhang.
Boosting weakly supervised object detection with progres-
sive knowledge transfer. In European conference on com-
puter vision , pages 615–631. Springer, 2020. 3
[36] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,
and Antonio Torralba. Learning deep features for discrimina-
tive localization. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 2921–2929,
2016. 3
[37] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang
Wang, and Jifeng Dai. Deformable detr: Deformable trans-
formers for end-to-end object detection. arXiv preprint
arXiv:2010.04159 , 2020. 2
[38] Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, and
Jieping Ye. Object detection in 20 years: A survey. Proceed-
ings of the IEEE , 2023. 2
17678
