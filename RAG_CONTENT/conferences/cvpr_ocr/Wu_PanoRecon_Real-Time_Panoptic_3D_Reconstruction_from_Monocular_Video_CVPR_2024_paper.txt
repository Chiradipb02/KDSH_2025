PanoRecon: Real-Time Panoptic 3D Reconstruction from Monocular Video
Dong Wu1Zike Y an2Hongbin Zha1
1National Key Lab of GAI, School of IST
PKU-SenseTime Joint Lab of MV
Peking University
2AIR, Tsinghua University
riserwu@stu.pku.edu.cn, yanzike@air.tsinghua.edu.cn, zha@cis.pku.edu.cn
Frame 1200 Frame 3000 Frame 4500
Online 3D Panoptic Reconstructionwall
floor
cabinet
bed
chair
sofa
tabledoor
window
bookshelf
picture
counter
desk
curtain
Online Instance Map
Online Semantic MapOnline Geometry Map
refrige.
s. curtain
toilet
sink
bathtub
otherfur.
Figure 1. We present PanoRecon, which realizes an online reconstruction at the level of stuff and things with only monocular video as
input. The system runs in real-time, and performs online 3D geometry reconstruction as well as dense semantic labeling for all map regionand segmentation of individual things .
Abstract
We introduce the Panoptic 3D Reconstruction task, a
uniﬁed and holistic scene understanding task for a monoc-ular video. And we present PanoRecon - a novel frame-
work to address this new task, which realizes an online ge-
ometry reconstruction alone with dense semantic and in-stance labeling. Speciﬁcally, PanoRecon incrementally per-forms panoptic 3D reconstruction for each video fragmentconsisting of multiple consecutive key frames, from a vol-umetric feature representation using feed-forward neuralnetworks. We adopt a depth-guided back-projection strat-egy to sparse and purify the volumetric feature represen-tation. We further introduce a voxel clustering module toget object instances in each local fragment, and then de-sign a tracking and fusion algorithm for the integration ofinstances from different fragments to ensure temporal co-herence. Such design enables our PanoRecon to yield acoherent and accurate panoptic 3D reconstruction. Exper-iments on ScanNetV2 demonstrate a very competitive ge-ometry reconstruction result compared with state-of-the-artreconstruction methods, as well as promising 3D panoptic
segmentation result with only RGB input, while being real-time. Code is available at: http s:// github.com/
Riser6/PanoRecon .
1. Introduction
3D scene understanding from a posed monocular video is
a fundamental task of 3D computer vision and robotics.The understanding ability to infer the underlying geomet-
ric structures and recognize object with semantics imme-diately, is critical towards many downstream applications,such as Augmented and Virtual Reality(AR and VR), inte-
rior modeling and human-robot interaction.
Recently, signiﬁcant advances have been made in 3D
geometric reconstruction. Many depth-fusion methods [ 1–
5] and feature-fusion methods [ 6–12] are proposed, each
with its own strengths(see Sec. 2.3). Although the water-
tight reconstruction of whole scene can be produced, dif-ferent objects in the scene are unable to be decoupled dueto lack of the ability to distinguish instance. On the other
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
21507
hand, many algorithms are proposed to perform semantic
segmentation [ 13–16] or instance segmentation [ 17–22]i n
3D scene but require dedicated depth sensors, which aremore expensive, less compact than cameras. Thus we aimto view the three separate tasks as a new uniﬁed task, called
panoptic 3D reconstruction . For panoptic 3D reconstruc-
tion, we aim to recover the surface geometry of the scenefrom a monocular video and also assign semantic and in-stance label for geometry elements. Following the task for-mat of 2D panoptic segmentation [ 23], for ”stuff’ elements
that refer the structural regions of similar texture or mate-rial such as wall, ﬂoor, the instance ID is often ignored, butfor ”things” elements that denote countable objects such aschair, table, both semantic label and instance ID need to bedistinguished(see Fig. 1).
To address this panoptic 3D reconstruction task, we pro-
pose a novel framework that jointly infers geometric struc-ture, semantic label and instance id from a monocular video,called PanoRecon. As illustrated in Fig 2, PanoRecon in-
crementally performs 3D geometric reconstruction and 3Dpanoptic segmentation(consists of 3D semantic segmenta-
tion and 3D instance segmentation) in a view-independent
3D feature volume. Given a posed monocular video, wesuccessively split it into multiple non-overlapping frag-ments. Then in order to form a sparse and valid 3D fea-ture volume of each fragment, we adopt a depth-guided
back-projection strategy to reduce erroneous feature allo-cation(detailed in Sec. 3.1). After getting the fused 3D fea-
ture volume, we introduce a V oxel-wise Prediction Networkto decode semantic and geometric primitive for each voxelin the feature volume(detailed in Sec. 3.2). With the hy-
brid primitives, we introduce a voxel clustering module to
get instance detection of each local fragment, and then a
Tracking and Fusion module is designed for the integrationof instances fragment by fragment to ensure global consis-tency(detailed in Sec. 3.3), yielding the ﬁnal panoptic re-
construction.
The contributions of our work can be summarized as fol-
lows:
• We introduce the task of Panoptic 3D Reconstruction
from posed monocular video, which aims for holistic 3D
scene understanding by jointly reasoning scene geometryand instance-level semantics.
• We propose a novel system, PanoRecon, which realizes
coherent while high-detailed geometry reconstruction aswell as reasonable panoptic segmentation of the scene,
and run in real-time.
• The experimental results on ScanNetV2 [ 24] show that
PanoRecon achieves competitive geometry reconstruc-tion quality compared with state-of-the-art methods andquite considerable 3D panoptic segmentation results inabsence of depth input.2. Related Work
2.1. Semantic SLAM and Neural Field
Recent developments in deep learning have also enabledthe integration of rich semantic information within Simulta-neous Localization and Mapping (SLAM). [ 25–29] Com-
bine the semantic segmentation network with SLAM sys-tem can incrementally compute semantic 3D map of the
environment, but no instance information is included. Asthe pioneer of object-level SLAM, SLAM++ [ 30] repre-
sents the scene with known CAD object models. Node-
SLAM [ 31] and DSP-SLAM[ 32] take this direction of us-
ing a category-speciﬁc learnt shape prior to build the sceneinstead of CAD model. [ 33–38] drop the requirement for
prior shape knowledge and instead take advantage of 2Dinstance segmentation masks to obtain object-level scenemap. Our work will further get rid of the dependence of2D semantic mask prior, and directly make prediction for3D map at one stage. In addition, most of these systemsrely on the RGBD sensor data input [ 26,27,30–37], some
of them focus more on pose estimation but do not recoverthe continuous scene surface geometry [ 25,27,38]. Our
method takes only RGB image sequence as input while re-
covering surface geometry and instance-level semantics of
the environment.
Neural ﬁelds have recently been used as a ﬂexible repre-
sentation of the whole scene [ 39–41]. SemanticNeRF [ 42]
and iLabel [ 43] reveal the coherence properties of neural
ﬁelds via adding semantic output channel, but they do notexplicitly model each semantic entity’s geometry. PNF [ 44]
and Panoptic NeRF [ 45] take a set of images, 3D bounding
primitives and 2D predictions as input, and can render RGBcolor and panoptic segmentation map at arbitrary view. Torepresent multiple objects, [ 46–48] take pre-computed in-
stance masks as additional input and conditioned objectrepresentation with learnable activation code, but all these
methods are still trained ofﬂine. vMAP [ 49] represents
each object by a small MLP and takes depth as additionalinput, which can detect and optimise object instances on-the-ﬂy. However, these neural ﬁeld methods rely on time-consuming per-scene optimization, while our method usesa feed-forward neural network to directly predict scene ge-
ometry and instance-level semantics in real-time.
2.2. Panoptic Segmentation
Kirillov et al. [ 23] proposes the task of 2D panoptic segmen-
tation, establishing a uniﬁed, holistic scene understandingtask for a single RGB image. This uniﬁed task aim to as-sign each pixel in 2D image with an semantic label and aninstance ID. The semantic labels can be divided into twoparts: ’stuff’ and ’thing’, where ’stuff’ labels do not dis-
tinguish instance ID. VPSNet [ 50] proposes and explores a
new video extension of this uniﬁed task, called video panop-
21508
tic segmentation(VPS). [ 51–53] present depth-aware VPS
network by introducing additional depth information. Incontrast to these existing methods, we try to directly infer-ence the instance-level semantics in the 3D space, and aremore concerned with the global results rather than the re-sults of individual frames.
Another line of works take RGBD sequences or point
cloud as input and output the semantic label of 3D inputdata. Broadly speaking, there are two kinds of approachesto solve this problem: (1) Predict semantics of 2D imageusing 2D segmentation network [ 54–56] and back-project
the semantic label to 3D data [ 27,36,57]. (2) Directly
inference the semantic labels in the 3D space. [ 13–16]
take voxelized point clouds as input and then apply 3D con-volution on the voxel grid, which output the class seman-tics of each voxel regardless of the instance ID. In orderto produce instance-level semantics of per-voxel, proposal-
based methods [ 17,18] and grouping-based methods [ 19–
22] are presented to detect object instances. Different from
these works, we propose a baseline to a relatively untouchedproblem of 3D panoptic segmentation without depth sensor.
2.3. 3D Reconstruction
There has been a long history of researches on multi-viewstereo reconstruction [ 1,58], which pose 3D reconstruc-
tion as per-pixel depth estimation. Recent works [ 2–5]e x -
tend the classical MVS with neural network to construct3D cost volume with multi-view features, which is used toregress the dense depth maps. Although these methods haveshown strong results, performing especially well on recov-ering highly detailed geometry, a known drawback is thatthe estimation of each depth map is independent, so con-tinuity across different frames is not constrained, and thisoften leads to artifacts.
An alternative method has been proposed to address 3D
reconstruction in Altas [ 6], which proposed direct predic-
tion of TSDF and label volume from back-projected image
features with a 3D CNN. [ 7,9–11] adopt a sparse 3D CNN
to perform feature volume-based reconstruction fragmentby fragment for better efﬁciency and scalability, and utilizea GRU to fuse fragment feature volumes over time for bet-ter coherence. The main advantage of these method is that
the 3D CNN can learn to produce smooth and consistent
surfaces. However, the geometric reconstruction with thesemethods still remains coarse. In order to recover more ac-curate surfaces, [ 9,12] propose to use multi-view depth es-
timates as guidance to enhance the scene representation andproduce high-resolution predictions. Moreover, althoughthese methods can produce watertight reconstruction of thewhole scene, the scene entities can not be split to differentobjects due to lack of instance-level information.
We share a similar pipeline with PlanarRecon [ 59],
where we recover panoptic watertight mesh instead of low-polygonal geometry of planar instances. [ 60,61] address
the panoptic 3D scene reconstruction task from a single im-age rather than monocular video. Our work will combinethe advantages of multi-view stereo method and volumetric-based method, achieving highly detailed and coherent re-construction, and for the ﬁrst time show an instance-levelreconstructed map from a posed monocular video.
3. Methods
Given a sequence of monocular video frames {It}and their
camera poses {ξt}∈SE(3) provided by a SLAM system,
our goal is to incrementally reconstruct the underlying geo-
metric structures and recognize objects as well as semantics.
To achieve online reconstruction, we sequentially select
suitable key frames from the incoming image stream fol-lowing [ 7]. A new incoming frame is selected as key frame
if the camera motion is greater than a predeﬁned thresh-old [ 7]. We then split the key frame stream into multiple
non-overlapping fragments F
i={Ii,j}N
j=1, each of which
consists of Nconsecutive key frames. With these frag-
ments, we then perform online panoptic 3D reconstruction.
An overview of our proposed method is shown in Fig
2. We divide our framework into three major compo-
nents, namely Depth-guided Feature Allocation and Fu-sion, V oxel-wise Prediction Network, and Instance Detec-tion, Tracking and Fusion. Below, we will introduce thesecomponents in detail.
3.1. Depth-guided Feature Allocation and Fusion
Overview. The task of panoptic 3D reconstruction re-
quires coherent geometry recovered from the whole scene,
also detailed reconstruction of foreground objects. So we
choose to utilize the high-detailed depth prior from an MVSnetwork to guide the volxel feature allocation and fusion.As MVS is a well-studied problem, we leverage an off-the-shelf method [ 5] due to its outstanding efﬁciency and accu-
racy.
Feature Allocation. Unlike most existing volume-based
methods that allocate dense feature volume along each ray,our method allocates sparse feature volumes from locationsonly where the surface is likely located according to thedepth prior, similar to [ 9,10]. Given a predicted depth
map and 2D feature which extracted by a 2D CNN fromkey frame image, our method back-projects the 2D featureonly to the voxels within a predeﬁned distance /triangledfrom the
corresponding estimated depth surface along the ray. This
simple and direct approach can not only sparse the featurevolume for efﬁciency, but also avoid the problem of irrel-evant 2D feature ﬁlling due to the occlusion of foregroundobjects. An illustration of this module is included in sup-plementary material.
21509
Ds^ͲŐƵŝĚĞĚ
&ĞĂƚƵƌĞ
ůůŽĐĂƚŝŽŶsŽǆĞůͲǁŝƐĞ
WƌĞĚŝĐƚŝŽŶ
EĞƚǁŽƌŬ
/ŶƐƚĂŶĐĞ
ĞƚĞĐƚŝŽŶ
dƌĂĐŬŝŶŐĂŶĚ
&ƵƐŝŽŶ
Input:
Posed  ImagesMVS
Network2D CNN
3D CNN
and
GRU
Output: 
Online Panoptic MapLocal Fragment
Instance SetGlobal
Instance Set
Figure 2. PanoRecon Framework. PanoRecon back-projects the 2D image features into a fragment bound volume Fiwith the guidance
of MVS depth estimation, and gradually processes the feature volume in a coarse-to-ﬁne paradigm with the 3D CNN and GRU, thenforms a temporal coherent feature volume. A voxel-wise prediction network will be used to decode geometric and semantic primitives{ˆO
i,k,ˆTi,k,ˆSi,k,/triangleˆxi,k}for each voxel in this fragment. Then the hybrid primitives {ˆSi,k,/triangleˆxi,k+xi,k}are sent to the instance detection
module to obtain the instance objects Oi={Oi,m}in this fragment. The tracking and fusion module matches the current instances Oi
with the global instances Og
i−1from all previous fragments, and then performs fusion for matched instances as well as initialization for
new instances, yielding the current global panoptic reconstruction.
Mean-shiftGNN Matching
Initialize FusionVoxel Center
Shift Vector
Semantic Class
Instance Object
No match
Figure 3. 2D illustration of Instance Detection, Tracking and
Fusion. We use different shapes to represent different semantic
classes, and use different colors to distinguish instances. The red
dot means the coordinate center of each voxel.
Feature Fusion. After back-projecting view-dependent
2D features into 3D volume, the view-independent featurevolume is obtained by directly averaging the features fromdifferent views. Thanks to the guidance of depth prior, wedo not encouter the noise problem caused by the fusion ofirrelevant feature, mentioned in [ 8]. Following [ 7], we use
3D sparse convolution to efﬁciently process the feature vol-ume and adopt the Gated Recurrent Unit(GRU) to performfeature fusion between current-fragment reconstruction andpreviously reconstructed global volume for temporal coher-ence, in a coarse-to-ﬁne paradigm.3.2. Voxel-wise Prediction Network
Overview. After obtaining the general 3D feature volume,
there are four branches applied to decode the voxel-wiseproperties of geometric and semantic for the panoptic 3Dreconstruction task.
Occupancy prediction branch. Given a voxel-wise fea-
ture, the occupancy prediction branch can predict the per-voxel occupancy score ˆO
k. The voxel whose occupancy
score is lower than a predeﬁned threshold θwill be spar-
siﬁed. The supervision of this branch is deﬁned as cross-entropy(BCE) between the predicted occupancy score andthe ground-truth occupancy values.
TSDF prediction branch. After the occupancy analy-
sis, our model will obtain a set of features of occupiedvoxels. We apply a simple MLP for regressing TSDF
ˆT
k. The TSDF loss between the TSDF prediction and the
groundtruth TSDF is deﬁned as: LT=|/lscript(ˆTk)−/lscript(Tk)|,
where/lscript(x)=sgn(x)log(|x|+1) is the log scale function
andsgn(·)is the sign function.
Semantic prediction branch. Paralleled with the TSDF
prediction branch. We apply an additional MLP to producesemantic scores of classes we considered for each occupiedvoxel. The class with highest score will be be regraded asthe predicted semantic label ˆS
k. The cross-entropy loss is
used to train this branch.
Offset prediction branch. With the semantic predictions,
our model can distinguish voxels of different semanticclasses, but still may fail to separate two instances who have
21510
the same semantic label, especially when they’re close to-
gether. Inspired by [ 62], we also estimate the voxel-wise
center shift vector /triangleˆxk, which represents the offset from
each voxel to its instance center. The instance center is de-ﬁned as the coordinate mean of all voxels belong to thisinstance. The predicted 3D shift vector /triangleˆx
kis supervised
by L1 loss.
3.3. Instance Detection, Tracking and Fusion
Overview. The panoptic 3D reconstruction task requires
instance-level reconstruction rather than voxel-level recon-struction. Hence, we perform a clustering-based instancesegmentation scheme to detect all potential instance ob-jectsO
i={Oi,m}in each fragment Fi. Meanwhile,
we maintain a set of global instance object reconstructionO
g
i−1={Og
i−1,n}from previous fragments. In order to as-
sociate instance detections between OiandOg
i−1and inte-
grate instance detections from different fragments to obtaina globally consistent 3D instance object reconstruction, weintroduce an instance-level tracking and fusion module. Anillustration of this procedure is shown in Fig. 3.
Voxel clustering. Once we have geometric and semantic
primitives for each occupied voxel, we group the voxels toform instance in each local fragment F
i. With the estimated
shift vector /triangleˆxk∈R3, we shift every voxel xktoward
its instance center, making the voxels of the same instancespatially closer to each other, deﬁned as:
x/prime
k=xk+/triangleˆxk (1)
Then, we ignore the voxels belong to the background
(stuff region) and use the predicted semantic label ˆSto split
voxels of different foreground classes. With the shifted co-ordinatex
/prime
k, we adopt a simple yet efﬁcient mean-shift clus-
tering algorithm [ 63] to perform intra-class instance seg-
mentation. Then, we get the instance detection sets Oiin
current fragment Fi.
Instance tracking. In order to get the correspondences
between two sets of object instances, OiandOg
i−1, we ﬁrst
compute the similarity S(m,n)between instance Oi,m∈
Oiand every instance Og
i−1,n∈Og
i−1. We initialize in-
stance embedding ei,m of each instance Oi,m by average
pooling all voxel features of this instance. Inspired by
[59,64], we resort to a GNN with attention mechanism [ 65]
to allow message passing between different instances, andthen get augmented embedding vectors ¯e
i,m and¯eg
i−1,n,o f
Oi,m andOg
i−1,nrespectively. The similarity between them
is formulated as:
S(m,n)=/angbracketleftbig
¯ei,m,¯eg
i−1,n/angbracketrightbig
(2)Then we use a differentiable Sinkhorn algorithm [ 66,67]t o
solve the optimal matching M∗:
M∗=a r gm i n
M/summationdisplay
m,nM(m,n)S(m,n) (3)
WhereM(m,n)∈{0,1}. We will include the loss func-
tion of this module in the supplementary material.
Integration to global map. After instance tracking, there
are three possible cases, as shown in Fig. 3. If some global
instances in Og
i−1do not ﬁnd their correspondences since
they may not be visible in current fragment Fi, we will keep
these instances unchanged. In addition, for a local instanceinO
imay not ﬁnd its correspondence in global set, it could
be a new instance observed for the ﬁrst time, so we initializethis new instance to O
g
i. Lastly, if two instances, Oi,m and
Og
i−1,nhave been successfully matched, we will integrate
them into a global instance Og
i,nand adopt a GRU module
to fuse their instance embeddings:
eg
i,n=G R U ( ¯eg
i−1,n,¯ei,m) (4)
where¯eg
i−1,nand¯ei,m are used as the hidden state and input
to the GRU module.
3.4. Implementation Details
Our 2D CNN architecture is a Feature Pyramid Net-work [ 68] using EfﬁcientNetV2-S [ 69] as backbone, and
we use torchsparse [ 70] for 3D sparse convolution. Our
network is trained using Adam optimizer on four NvidiaRTX 3090 GRU for 60 epochs. Following [ 7], we use three
coarse-to-ﬁne layers and set the voxel size of each layer as16cm, 8cm, 4cm respectively. The predeﬁned distance /triangled
is set to be 32cm. The TSDF truncation distance is set asthree times of the voxel size for each layer. The occupancypruning threshold is set to 0.
4. Experiments
4.1. Dataset, Metrics and Baseline.
Dataset. We perform our experiments on Scan-
Net(V2) [ 24]. ScanNet consists of 1613 scans across
807 distinct scenes with RGB images, groundtruth depths,camera poses, surface reconstructions and instance-levelsemantic segmentations. Following previous works [ 6,7],
we use the ofﬁcial train/val/test split to train and evaluate
our method in order to make a fair comparison.
Metrics. We follow the 3D geometry metrics used in [ 6,
7] to evaluate the 3D geometry reconstruction performance
of our approach. Among these 3D metrics, we regard theF-score as the most important metric to measure 3D ge-
ometry reconstruction quality since both the accuracy and
21511
completeness of the reconstruction are taken into account.
Inspired by [ 6], we transfer the semantic and instance la-
bel from the predicted mesh to the groundtruth mesh us-ing nearest neighbor lookup on each vertices. The stan-dard mIoU, mAP@50 and mAP@25 are used to evaluate
the prediction of semantic label and instance label respec-tively. The detailed deﬁnitions of all these metrics above
are included in the supplementary material.
Baseline. For the evaluation of 3D geometry reconstruc-
tion, we compare our method with multi-view depth esti-mation methods [ 1–5] and end-to-end reconstruction meth-
ods [ 6–9]. Among them, NeuralRecon [ 68] and Zuo et
al. [ 9] are two end-to-end incremental volumetric recon-
struction methods that are the most relevant ones to ourapproach. In order to evaluate the prediction of semanticlabel , we compare our method with some prior methodsthat include depth as input [ 14–16,24,36,71–73], as well
as Atlas [ 6] using only RGB images as input, as we do. As
for the evaluation of instance label prediction, in addition
to some previous works with 3D input [ 17,74–78], we also
compare with an original but classic algorithm [ 56], which
also include only 2D images as input.
4.2. Results
Evaluation of 3D Geometry Reconstruction. The ex-
perimental results of 3D reconstruction on ScanNet datasetare shown in Tab. 1. Our method outperforms existing on-
line feature fusion methods [ 7,9], and slightly better than
the state of the art depth fusion method [ 5] in terms of F-
score metric. The advantage on F-score is due to the fact
that our method have achieved a good balance between theaccuracy and completeness. With the assistance of MVSdepth, our method can recover more complete and detailedgeometry than the pure feature fusion method [ 7]. Thanks
to the de-noise ability of feature fusion, our method can re-cover more coherent and accurate geometry than [ 5]. The
qualitative comparison and analysis are shown in Fig. 4.
Our method presents more complete and coherent recon-struction of the whole scene while preserving many details
of foreground objects. Moreover, we additionally conductevaluation of geometry reconstruction at instance level, andprovide average metrics for each semantic category in the
supplementary material.
Evaluation of 3D Panoptic Segmentation. The quanti-
tative results of 3D semantic segmentation on ScanNetV2are reported in Tab. 2. Despite the unfair setup since we
use only RGB data, our method is still surprisingly com-petitive with(even beats) some prior works with 3D inputdata. In the same case with only RGB images as input,
our method outperforms Altas by a large margin(+18.4) interms of mIoU. As the qualitative comparison shown inMethod Online Comp ↓Acc↓Recall ↑Prec↑F-score ↑Depth FusionCOLMAP [ 1] - 6.9 13.5 0.634 0.505 0.558
MVDNet [ 2]- 4.0 24.0 0.831 0.208 0.329
DPSNet [ 3] - 4.5 28.4 0.793 0.223 0.344
GPMVS [ 4] - 10.5 19.1 0.423 0.339 0.373
SimRec [ 5] - 6.2 10.1 0.636 0.536 0.577Feature FusionAtlas [ 6] x 8.3 10.1 0.566 0.600 0.579
V ortx [ 8]x 8.1 6.2 0.605 0.689 0.643
NeuRec [ 7]/check 13.7 5.6 0.470 0.678 0.553
Zuo et al. [ 9]/check 11.0 5.8 0.505 0.665 0.572
Ours /check 8.9 6.4 0.530 0.656 0.584
Table 1. Quantitative Result of 3D Geometry Reconstruction
on ScanNetV2 test set. We highlight the best results for Depth
Fusion, Ofﬂine Feature Fusion and Online Feature Fusion meth-ods in green ,magenta , and cyan , respectively. The experimental
results of Simplerecon [ 5] are reproduced with the ofﬁcial code-
base and published weights. Other results come from [ 9].
Fig. 5, the advance of performance is due to the improve-
ment of both reconstruction quality and segmentation accu-
racy. It is worth mentioning that due to the setting of online
panoptic reconstruction, our method do not have the accessto full batch data with global context, which is extremelychallenging for geometry reconstruction and semantic seg-mentation. In addition, we also report the 3D instancesegmentation results in Tab. 3. In absence of depth sen-
sor and post process procedure, such as Non-max suppres-sion(NMS), our method still achieves decent performanceof instance segmentation. The quantitative result of 3D in-stance segmentation is illustrated in the Fig. 2 of the sup-plementary material, our method is able to accurately recon-struct while successfully splitting the 3D scene into multipleinstance objects.
Method with Depth mIoU ↑
ScanNet [ 24] /check 30.6
PointNet++ [ 14] /check 33.9
SPLA TNet [ 71] /check 39.3
3DMV [ 72] /check 48.4
SegFusion [ 73] /check 51.5
PanopticFusion [ 36] /check 52.9
SparseConvNet [ 15] /check 72.5
MinkowskiNet [ 16] /check 73.4
Altas [ 6] x 34.0
Ours x 52.4
Table 2. Quantitative Result of 3D Semantic Segmentation on
ScanNetV2 test set. The experiment results of other methods are
borrowed from Altas [ 6] and the ScanNetV2 benchmark [ 24].
Ablation Study of different designs of choices. In
Tab. 4, we conduct ablation experiments to verify the effect
of different designs of choices. We experiment with dif-ferent number of keyframes in each fragment(row (a), (b))rather than the default 9 views(row (f)), and replace Mnas-
21512
SimpleRecon NeuralRecon Ours GroundTruth
Figure 4. Qualitative Result of 3D Geometry Reconstruction on ScanNet val set. Since SimpleRecon relies on the non-learnable depth
fusion method, it generates artifacts and duplicate surfaces (highlighted with the yellow boxes). With the incorporation of MVS depth, our
geometry reconstruction results are more complete while preserving more details of foreground objects(highlighted with the red boxes)
compared with NeuralRecon.
Method with Depth AP50 ↑ AP25 ↑
SGPN [ 74] /check 0.143 0.390
ASIS [ 75] /check 0.199 0.422
Gspn [ 76] /check 0.306 0.544
3D-SIS [ 17] /check 0.382 0.558
SegGroup [ 77] /check 0.445 0.637
PBNet [ 78] /check 0.747 0.825
MaskRCNN [ 56] x 0.058 0.261
Ours x 0.227 0.484
Table 3. Quantitative Result of 3D Instance Segmentation on
ScanNetV2 test set. The experiment results of other methods are
borrowed from the ScanNetV2 benchmark [ 24].
Net [ 79] as backbone(row (c)) instead of EfﬁcientNetV2-
S(row (d). We also conduct an ablation study to verify the
effect of depth guidance strategy(row (d), (e), (g)). By com-
paring the row (d) and row (f), it is obvious that introducingthe strategy of depth-guided feature allocation brings con-siderable improvement to all metrics. We also attempt to
adopt the traditional TSDF-Fusion to form TSDF volume,and then enhance the feature volume by concatenating withthe TSDF V olume(row(e)). But this strategy does not bringthe expected improvement, so we abandon it in our ﬁnalscheme. There is a signiﬁcant improvement of geometricquality by directly using gt depth as input without any ﬁne-tuning or re-training(row (g)), which indicates our methodcan beneﬁt from more advanced depth estimation models.In addition, the ablation of tracking and fusion module isdetailed in supplementary material.
Runtime analysis. We conduct runtime evaluation on a
desktop computer equipped with Intel i9-12900KS CPUand NVIDIA RTX3090 GPU. Our method takes an averageof 700 ms to process one fragment including 9 key frames,which composed of 430ms for MVS Network, 30 ms for2D CNN, 137 ms for 3D Network(consists of 3D CNN,GRU and voxel-wise prediction network), and 103 ms for
21513
Atlas Ours Ground Truth
Figure 5. Qualitative Result of 3D Semantic Segmentation on ScanNetV2 val set. Our method produces more accurate geometry
reconstruction and semantic labeling than Atlas, though Atlas have the access to full batch data with global context.
Method Recall ↑ Prec↑ F-score ↑ mIoU ↑
(a) 7 views. 0.580 0.680 0.623 54.3
(b) 11 views. 0.552 0.666 0.600 52.5
(c) MnasNet. 0.551 0.655 0.596 52.6
(d) Ours w/o guide proj. 0.548 0.661 0.597 53.6
(e) Ours with tsdf agu. 0.582 0.679 0.624 53.7
(f) Ours 0.593 0.681 0.631 54.2
(g) with depth sensor. 0.761 0.874 0.810 55.5
Table 4. Ablation of different designs of choices on ScanNetV2
val set.
instance detection, tracking and fusion. Since keyframes
are created at a far lower frequency than the framerate, ourmodel still achieves a real-time panoptic reconstruction of
12.85 key frames per second (KFPS).
5. Conclusion
In this work, we introduce the task of panoptic 3D recon-struction from a posed monocular video. This uniﬁed taskaims to obtain the holistic understanding of global scene
consisting of geometric reconstruction, 3D semantic seg-mentation and 3D instance segmentation. We also presenta novel system, PanoRecon, for real-time panotic 3D re-construction. The main idea of PanoRecon is to use thevolumetric feature representation to perform geometry re-construction and panoptic segmentation fragment by frag-ment. Experiments show that PanoRecon achieves compet-itive geometry reconstruction compared with the state-of-the-art methods and promising 3D panoptic segmentation,while running in real-time. We hope this work will help topush forward research towards more comprehensive holis-tic scene understanding and introduce new algorithm chal-lenges and additional insights to this area.
Acknowledgement
We thank anonymous reviewers and AC for their fruitfulcomments and suggestions. This work is supported byNSFC (U22A2061, 62176010), and 230601GP0004.
21514
References
[1] Johannes L Sch ¨onberger, Enliang Zheng, Jan-Michael
Frahm, and Marc Pollefeys. Pixelwise view selection forunstructured multi-view stereo. In Computer Vision–ECCV
2016: 14th European Conference, Amsterdam, The Nether-lands, October 11-14, 2016, Proceedings, Part III 14 , pages
501–518. Springer, 2016. 1,3,6
[2] Kaixuan Wang and Shaojie Shen. Mvdepthnet: Real-time
multiview depth estimation neural network. In 2018 Interna-
tional conference on 3d vision (3DV) , pages 248–257. IEEE,
2018. 3,6
[3] Sunghoon Im, Hae-Gon Jeon, Stephen Lin, and In So
Kweon. Dpsnet: End-to-end deep plane sweep stereo. arXiv
preprint arXiv:1905.00538 , 2019. 6
[4] Y uxin Hou, Juho Kannala, and Arno Solin. Multi-view
stereo by temporal nonparametric fusion. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-sion , pages 2651–2660, 2019. 6
[5] Mohamed Sayed, John Gibson, Jamie Watson, Victor
Prisacariu, Michael Firman, and Cl ´ement Godard. Simplere-
con: 3d reconstruction without 3d convolutions. In European
Conference on Computer Vision , pages 1–19. Springer, 2022.
1,3,6
[6] Zak Murez, Tarrence V an As, James Bartolozzi, Ayan Sinha,
Vijay Badrinarayanan, and Andrew Rabinovich. Atlas: End-to-end 3d scene reconstruction from posed images. In Com-
puter Vision–ECCV 2020: 16th European Conference, Glas-gow, UK, August 23–28, 2020, Proceedings, Part VII 16 ,
pages 414–431. Springer, 2020. 1,3,5,6
[7] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and
Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruc-tion from monocular video. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 15598–15607, 2021. 3,4,5,6
[8] Noah Stier, Alexander Rich, Pradeep Sen, and Tobias
H¨ollerer. V ortx: V olumetric 3d reconstruction with trans-
formers for voxelwise view selection and fusion. In 2021 In-
ternational Conference on 3D Vision (3DV) , pages 320–330.
IEEE, 2021. 4,6
[9] Xingxing Zuo, Nan Y ang, Nathaniel Merrill, Binbin Xu, and
Stefan Leutenegger. Incremental dense reconstruction from
monocular video with guided sparse feature volume fusion.IEEE Robotics and Automation Letters , 2023. 3,6
[10] Jihong Ju, Ching Wei Tseng, Oleksandr Bailo, Georgi Dikov,
and Mohsen Ghafoorian. Dg-recon: Depth-guided neural3d scene reconstruction. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 18184–
18194, 2023. 3
[11] Huiyu Gao, Wei Mao, and Miaomiao Liu. Visfusion:
Visibility-aware online 3d scene reconstruction from videos.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 17317–17326, 2023.
3
[12] Noah Stier, Anurag Ranjan, Alex Colburn, Y ajie Y an, Liang
Y ang, Fangchang Ma, and Baptiste Angles. Finerecon:Depth-aware feed-forward network for detailed 3d recon-struction. arXiv preprint arXiv:2304.01480 , 2023. 1,3[13] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classiﬁcationand segmentation. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 652–660,
2017. 2,3
[14] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learning onpoint sets in a metric space. Advances in neural information
processing systems , 30, 2017. 6
[15] Benjamin Graham, Martin Engelcke, and Laurens V an
Der Maaten. 3d semantic segmentation with submani-
fold sparse convolutional networks. In Proceedings of the
IEEE conference on computer vision and pattern recogni-tion , pages 9224–9232, 2018. 6
[16] Christopher Choy, JunY oung Gwak, and Silvio Savarese. 4d
spatio-temporal convnets: Minkowski convolutional neuralnetworks. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 3075–3084,
2019. 2,3,6
[17] Ji Hou, Angela Dai, and Matthias Nießner. 3d-sis: 3d se-
mantic instance segmentation of rgb-d scans. In Proceedings
of the IEEE/CVF conference on computer vision and patternrecognition , pages 4421–4430, 2019. 2,3,6,7
[18] Bo Y ang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen
Wang, Andrew Markham, and Niki Trigoni. Learning ob-ject bounding boxes for 3d instance segmentation on pointclouds. Advances in neural information processing systems ,
32, 2019. 3
[19] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg:
Occupancy-aware 3d instance segmentation. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 2940–2949, 2020. 3
[20] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-
Wing Fu, and Jiaya Jia. Pointgroup: Dual-set pointgrouping for 3d instance segmentation. In Proceedings of
the IEEE/CVF conference on computer vision and Patternrecognition , pages 4867–4876, 2020.
[21] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and
Xinggang Wang. Hierarchical aggregation for 3d instancesegmentation. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 15467–15476,
2021.
[22] Leyao Liu, Tian Zheng, Y un-Jou Lin, Kai Ni, and Lu Fang.
Ins-conv: Incremental sparse convolution for online 3d seg-mentation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 18975–
18984, 2022. 2,3
[23] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten
Rother, and Piotr Doll ´ar. Panoptic segmentation. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 9404–9413, 2019. 2
[24] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nießner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes. InProceedings of the IEEE conference on computer vision andpattern recognition , pages 5828–5839, 2017. 2,5,6,7
[25] Abhijit Kundu, Yin Li, Frank Dellaert, Fuxin Li, and
James M Rehg. Joint semantic segmentation and 3d re-
21515
construction from monocular video. In Computer Vision–
ECCV 2014: 13th European Conference, Zurich, Switzer-
land, September 6-12, 2014, Proceedings, Part VI 13 , pages
703–718. Springer, 2014. 2
[26] Christian Hane, Christopher Zach, Andrea Cohen, Roland
Angst, and Marc Pollefeys. Joint 3d scene reconstruction andclass segmentation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 97–104,
2013. 2
[27] John McCormac, Ankur Handa, Andrew Davison, and Ste-
fan Leutenegger. Semanticfusion: Dense 3d semantic map-ping with convolutional neural networks. In 2017 IEEE In-
ternational Conference on Robotics and automation (ICRA) ,
pages 4628–4635. IEEE, 2017. 2,3
[28] Shuaifeng Zhi, Michael Bloesch, Stefan Leutenegger, and
Andrew J Davison. Scenecode: Monocular dense semanticreconstruction using learned encoded scene representations.InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 11776–11785, 2019.
[29] Antoni Rosinol, Marcus Abate, Y un Chang, and Luca Car-
lone. Kimera: an open-source library for real-time metric-semantic localization and mapping. In 2020 IEEE Inter-
national Conference on Robotics and Automation (ICRA) ,
pages 1689–1696. IEEE, 2020. 2
[30] Renato F Salas-Moreno, Richard A Newcombe, Hauke
Strasdat, Paul HJ Kelly, and Andrew J Davison. Slam++: Si-multaneous localisation and mapping at the level of objects.InProceedings of the IEEE conference on computer vision
and pattern recognition , pages 1352–1359, 2013. 2
[31] Edgar Sucar, Kentaro Wada, and Andrew Davison.
Nodeslam: Neural object descriptors for multi-view shapereconstruction. In 2020 International Conference on 3D Vi-
sion (3DV) , pages 949–958. IEEE, 2020. 2
[32] Jingwen Wang, Martin R ¨unz, and Lourdes Agapito. Dsp-
slam: Object oriented slam with deep shape priors. In 2021
International Conference on 3D Vision (3DV) , pages 1362–
1371. IEEE, 2021. 2
[33] Martin R ¨unz and Lourdes Agapito. Co-fusion: Real-time
segmentation, tracking and fusion of multiple objects. In2017 IEEE International Conference on Robotics and Au-tomation (ICRA) , pages 4471–4478. IEEE, 2017. 2
[34] John McCormac, Ronald Clark, Michael Bloesch, Andrew
Davison, and Stefan Leutenegger. Fusion++: V olumetricobject-level slam. In 2018 international conference on 3D
vision (3DV) , pages 32–41. IEEE, 2018.
[35] Binbin Xu, Wenbin Li, Dimos Tzoumanikas, Michael
Bloesch, Andrew Davison, and Stefan Leutenegger. Mid-
fusion: Octree-based object-level multi-instance dynamicslam. In 2019 International Conference on Robotics and Au-
tomation (ICRA) , pages 5231–5237. IEEE, 2019.
[36] Gaku Narita, Takashi Seno, Tomoya Ishikawa, and Y ohsuke
Kaji. Panopticfusion: Online volumetric semantic mappingat the level of stuff and things. In 2019 IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems (IROS) ,
pages 4205–4212. IEEE, 2019. 3,6
[37] Leevi Raivio and Esa Rahtu. Online panoptic 3d recon-
struction as a linear assignment problem. In InternationalConference on Image Analysis and Processing , pages 39–50.
Springer, 2022. 2
[38] Weicai Y e, Xinyue Lan, Shuo Chen, Y uhang Ming,
Xingyuan Y u, Hujun Bao, Zhaopeng Cui, and Guofeng
Zhang. Pvo: Panoptic visual odometry. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition , pages 9579–9589, 2023. 2
[39] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
bastian Nowozin, and Andreas Geiger. Occupancy networks:
Learning 3d reconstruction in function space. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 4460–4470, 2019. 2
[40] Jeong Joon Park, Peter Florence, Julian Straub, Richard
Newcombe, and Steven L ovegrove. Deepsdf: Learning con-
tinuous signed distance functions for shape representation.InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 165–174, 2019.
[41] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance ﬁelds for view syn-thesis. Communications of the ACM , 65(1):99–106, 2021.
2
[42] Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and An-
drew J Davison. In-place scene labelling and understandingwith implicit scene representation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 15838–15847, 2021. 2
[43] Shuaifeng Zhi, Edgar Sucar, Andre Mouton, Iain Haughton,
Tristan Laidlow, and Andrew J Davison. ilabel: Interactiveneural scene labelling. arXiv preprint arXiv:2111.14637 ,
2021. 2
[44] Abhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi, Car-
oline Pantofaru, Leonidas J Guibas, Andrea Tagliasacchi,
Frank Dellaert, and Thomas Funkhouser. Panoptic neural
ﬁelds: A semantic object-aware neural scene representation.InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 12871–12881, 2022.
2
[45] Xiao Fu, Shangzhan Zhang, Tianrun Chen, Yichong Lu,
Lanyun Zhu, Xiaowei Zhou, Andreas Geiger, and Yiyi Liao.Panoptic nerf: 3d-to-2d label transfer for panoptic urbanscene segmentation. In 2022 International Conference on
3D Vision (3DV) , pages 1–11. IEEE, 2022. 2
[46] Bangbang Y ang, Yinda Zhang, Yinghao Xu, Yijin Li, Han
Zhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui.Learning object-compositional neural radiance ﬁeld for ed-itable scene rendering. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 13779–
13788, 2021. 2
[47] Qianyi Wu, Xian Liu, Y uedong Chen, Kejie Li, Chuanxia
Zheng, Jianfei Cai, and Jianmin Zheng. Object-compositional neural implicit surfaces. In European Confer-
ence on Computer Vision , pages 197–213. Springer, 2022.
[48] Qianyi Wu, Kaisiyuan Wang, Kejie Li, Jianmin Zheng, and
Jianfei Cai. Objectsdf++: Improved object-compositionalneural implicit surfaces. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 21764–
21774, 2023. 2
21516
[49] Xin Kong, Shikun Liu, Marwan Taher, and Andrew J Davi-
son. vmap: V ectorised object mapping for neural ﬁeld slam.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 952–961, 2023. 2
[50] Dahun Kim, Sanghyun Woo, Joon-Y oung Lee, and In So
Kweon. Video panoptic segmentation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition , pages 9859–9868, 2020. 2
[51] Siyuan Qiao, Y ukun Zhu, Hartwig Adam, Alan Y uille, and
Liang-Chieh Chen. Vip-deeplab: Learning visual perceptionwith depth-aware video panoptic segmentation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition , pages 3997–4008, 2021. 3
[52] Haobo Y uan, Xiangtai Li, Yibo Y ang, Guangliang Cheng,
Jing Zhang, Y unhai Tong, Lefei Zhang, and Dacheng Tao.Polyphonicformer: uniﬁed query learning for depth-awarevideo panoptic segmentation. In European Conference on
Computer Vision , pages 582–599. Springer, 2022.
[53] Junwen He, Yifan Wang, Lijun Wang, Huchuan Lu, Bin Luo,
Jun-Y an He, Jin-Peng Lan, Yifeng Geng, and Xuansong Xie.Towards deeply uniﬁed depth-aware panoptic segmentation
with bi-directional guidance learning. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 4111–4121, 2023. 3
[54] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.
Learning deconvolution network for semantic segmentation.InProceedings of the IEEE international conference on com-
puter vision , pages 1520–1528, 2015. 3
[55] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid scene parsing network. InProceedings of the IEEE conference on computer vision andpattern recognition , pages 2881–2890, 2017.
[56] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-
shick. Mask r-cnn. In Proceedings of the IEEE international
conference on computer vision , pages 2961–2969, 2017. 3,
6,7
[57] Margarita Grinvald, Fadri Furrer, Tonci Novkovic, Jen Jen
Chung, Cesar Cadena, Roland Siegwart, and Juan Nieto.V olumetric instance-aware semantic mapping and 3d ob-ject discovery. IEEE Robotics and Automation Letters ,
4(3):3037–3044, 2019. 3
[58] Y asutaka Furukawa, Carlos Hern ´andez, et al. Multi-view
stereo: A tutorial. F oundations and Trends® in Computer
Graphics and Vision , 9(1-2):1–148, 2015. 3
[59] Yiming Xie, Matheus Gadelha, Fengting Y ang, Xiaowei
Zhou, and Huaizu Jiang. Planarrecon: Real-time 3d plane
detection and reconstruction from posed monocular videos.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 6219–6228, 2022. 3,
5
[60] Manuel Dahnert, Ji Hou, Matthias Nießner, and Angela
Dai. Panoptic 3d scene reconstruction from a single rgb im-
age. Advances in Neural Information Processing Systems ,
34:8282–8293, 2021. 3
[61] Tao Chu, Pan Zhang, Qiong Liu, and Jiaqi Wang. Buol:
A bottom-up framework with occupancy-aware lifting forpanoptic 3d scene reconstruction from a single image. InProceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 4937–4946, 2023. 3
[62] Charles R Qi, Or Litany, Kaiming He, and Leonidas J
Guibas. Deep hough voting for 3d object detection in point
clouds. In proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 9277–9286, 2019. 5
[63] Dorin Comaniciu and Peter Meer. Mean shift analysis and
applications. In Proceedings of the seventh IEEE inter-
national conference on computer vision
, volume 2, pages
1197–1203. IEEE, 1999. 5
[64] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,
and Andrew Rabinovich. Superglue: Learning featurematching with graph neural networks. In Proceedings of
the IEEE/CVF conference on computer vision and patternrecognition , pages 4938–4947, 2020. 5
[65] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 5
[66] Richard Sinkhorn and Paul Knopp. Concerning nonnegative
matrices and doubly stochastic matrices. Paciﬁc Journal of
Mathematics , 21(2):343–348, 1967. 5
[67] Marco Cuturi. Sinkhorn distances: Lightspeed computation
of optimal transport. Advances in neural information pro-
cessing systems , 26, 2013. 5
[68] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature pyra-mid networks for object detection. In Proceedings of the
IEEE conference on computer vision and pattern recogni-tion , pages 2117–2125, 2017. 5,6
[69] Mingxing Tan and Quoc Le. Efﬁcientnetv2: Smaller models
and faster training. In International conference on machine
learning , pages 10096–10106. PMLR, 2021. 5
[70] Haotian Tang, Zhijian Liu, Shengyu Zhao, Y ujun Lin, Ji Lin,
Hanrui Wang, and Song Han. Searching efﬁcient 3d architec-tures with sparse point-voxel convolution. In European con-
ference on computer vision , pages 685–702. Springer, 2020.
5
[71] Hang Su, V arun Jampani, Deqing Sun, Subhransu Maji,
Evangelos Kalogerakis, Ming-Hsuan Y ang, and Jan Kautz.Splatnet: Sparse lattice networks for point cloud processing.InProceedings of the IEEE conference on computer vision
and pattern recognition , pages 2530–2539, 2018. 6
[72] Angela Dai and Matthias Nießner. 3dmv: Joint 3d-multi-
view prediction for 3d semantic scene segmentation. In Pro-
ceedings of the European Conference on Computer Vision(ECCV) , pages 452–468, 2018. 6
[73] Davide Menini, Suryansh Kumar, Martin R Oswald, Erik
Sandstr ¨om, Cristian Sminchisescu, and Luc V an Gool. A
real-time online learning framework for joint 3d reconstruc-
tion and semantic segmentation of indoor scenes. IEEE
Robotics and Automation Letters , 7(2):1332–1339, 2021. 6
[74] Weiyue Wang, Ronald Y u, Qiangui Huang, and Ulrich Neu-
mann. Sgpn: Similarity group proposal network for 3dpoint cloud instance segmentation. In Proceedings of the
IEEE conference on computer vision and pattern recogni-tion , pages 2569–2578, 2018. 6,7
21517
[75] Xinlong Wang, Shu Liu, Xiaoyong Shen, Chunhua Shen, and
Jiaya Jia. Associatively segmenting instances and semantics
in point clouds. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 4096–
4105, 2019. 7
[76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J
Guibas. Gspn: Generative shape proposal network for 3dinstance segmentation in point cloud. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition , pages 3947–3956, 2019. 7
[77] An Tao, Y ueqi Duan, Yi Wei, Jiwen Lu, and Jie Zhou. Seg-
group: Seg-level supervision for 3d instance and seman-tic segmentation. IEEE Transactions on Image Processing ,
31:4952–4965, 2022. 7
[78] Weiguang Zhao, Y uyao Y an, Chaolong Y ang, Jianan Y e,
Xi Y ang, and Kaizhu Huang. Divide and conquer: 3d pointcloud instance segmentation with point-wise binarization. InProceedings of the IEEE/CVF International Conference onComputer Vision , pages 562–571, 2023. 6,7
[79] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay V asudevan,
Mark Sandler, Andrew Howard, and Quoc V Le. Mnas-net: Platform-aware neural architecture search for mobile.InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 2820–2828, 2019. 7
21518
