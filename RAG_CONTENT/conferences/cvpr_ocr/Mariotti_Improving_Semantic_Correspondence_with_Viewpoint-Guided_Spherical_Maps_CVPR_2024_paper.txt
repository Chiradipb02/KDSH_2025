Improving Semantic Correspondence with Viewpoint-Guided Spherical Maps
Octave Mariotti Oisin Mac Aodha Hakan Bilen
University of Edinburgh
groups.inf.ed.ac.uk/vico/research/sphericalmaps
Abstract
Recent self-supervised models produce visual features
that are not only effective at encoding image-level, but
also pixel-level, semantics. They have been reported to
obtain impressive results for dense visual semantic corre-
spondence estimation, even outperforming fully-supervised
methods. Nevertheless, these models still fail in the pres-
ence of challenging image characteristics such as symme-
tries and repeated parts. To address these limitations, we
propose a new semantic correspondence estimation method
that supplements state-of-the-art self-supervised features
with 3D understanding via a weak geometric spherical
prior. Compared to more involved 3D pipelines, our model
provides a simple and effective way of injecting informa-
tive geometric priors into the learned representation while
requiring only weak viewpoint information. We also pro-
pose a new evaluation metric that better accounts for re-
peated part and symmetry-induced mistakes. We show that
our method succeeds in distinguishing between symmetric
views and repeated parts across many object categories in
the challenging SPair-71k dataset and also in generalizing
to previously unseen classes in the AwA dataset.
1. Introduction
Semantic correspondence (SC) estimation aims to find local
regions that correspond to the same semantic entities across
a collection of images, where each image contains a dif-
ferent instance of the same object category [30]. SC has
been studied in both supervised [9, 26, 59] and unsuper-
vised [2, 50] settings. Thanks to the recent progress in self-
supervised learning (SSL) [6, 40, 60], recent SSL-based ap-
proaches have been shown to obtain strong performance on
multiple benchmarks [1, 2, 54]. However, they often act as
simple part detectors, without having any 3D understanding
of object extent, or relative location of object parts. As these
SSL methods are typically trained using only 2D-based ob-
jectives, they are not able to learn 3D-aware representations,
and often converge to similar features for object parts that
share appearance but not fine-grained semantics.
Input
collapsed sidesDINOv2 Ours
distinct sides
distinct parts collapsed partsFigure 1. Features from self-supervised methods such as DI-
NOv2 [40] have been used to discover parts and regions of objects.
However, these features fail to correctly distinguish (i) object sym-
metries ,e.g. the left and right side of the car have the same fea-
tures and (ii) individual parts ,e.g. the wheels are represented by
the same features irrespective of their location on the car. Our ap-
proach through use of a weak geometric spherical prior addresses
these issues. Note, we use the 3D PCA projection of the features
for DINOv2 and our learned spherical mapping for our method.
The two main failure modes of current methods are de-
picted in Fig. 1. First, they are often confused by the sym-
metries that objects can exhibit ( e.g. left vs. right side of a
car), producing reflected versions of the feature maps when
presented with the two different sides of a symmetric ob-
ject. Second, they predict similar features for similar look-
ing parts ( e.g. the front left vs. back left wheel of a car), and
fail to discriminate between part instances. Such symme-
tries and repeated structures are ubiquitous in both human-
made and naturally occurring object categories.
We posit that these shortcomings can be addressed by en-
forcing an explicit 3D structure into correspondence learn-
ing, allowing the model to disambiguate multiple repeated
parts, and accounting for the fact that some object parts
might not be visible in all images. Indeed, scene-based cor-
respondence pipelines use geometric verification in order
to guarantee robustness in their matches [44], while other
works have shown that detailed 3D meshes can be used to
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
19521
discover category-based correspondences through a render-
and-compare pipeline [29, 62]. However, these approaches
typically require 3D mesh supervision which is difficult and
costly to obtain at scale.
Motivated by the limitations, and the promise of the re-
cent SSL representations [40], we propose a novel method
that incorporates the missing 3D geometry into object rep-
resentations. We make use of a weak 3D prior by represent-
ing object features on the surface of a sphere which encodes
both geometry and appearance information. Our method
consistently maps features corresponding to the same se-
mantic parts ( e.g. left headlight of a car) in images of dif-
ferent object instances to the same spherical coordinates,
hence enabling a 3D aware and accurate correspondence.
However, learning such a mapping without the ground-truth
3D supervision is nontrivial because of repeated parts and
object symmetries. To this end, we propose to use much
weaker supervision via coarse camera viewpoint annota-
tions to help separate these confusing features during train-
ing. We couple this with geometric constraints to prevent
collapsed representations for repeated parts. As a result,
our method learns to separate object parts based on their
position and spatial configuration in an image.
Furthermore, we point out that the standard evaluation
metric for SC, Percentage of Correct Keypoints (PCK), is
not able to properly evaluate symmetry-related failures as
it only considers keypoints that are simultaneously visible
in both the source and target image. For instance, a sys-
tem predicting high similarity between a front left wheel in
the source and front right wheel in the target is unlikely to
be penalized for it, as the target image is unlikely to contain
both a front left andfront right wheel due to self-occlusions.
In such cases PCK will simply not evaluate what the near-
est neighbor of the source wheel is. Rather than simple
accuracy-based metric, we argue that average precision is a
more robust scoring mechanism and propose a new metric,
Keypoint Average Precision (KAP), to account for potential
spurious matches.
Our contributions are: i) a novel SC approach that learns
to separate visually similar parts by mapping them to differ-
ent location on a sphere, ii) a set of simple geometric con-
straints that force separation of visually similar parts during
training, and iii) a new evaluation protocol that better ac-
counts for the failure cases of current methods.
2. Related work
Image matching. Visual correspondences were first ex-
plored as exact matches between different views of the same
scene with applications to stereo vision [24, 38], object de-
tection [14, 33], structure from motion [20, 44, 52], and
SLAM [12, 19]. Correspondences can also be computed
across pixels in the frames of a video, which is referred to
as optical flow [5, 7]. Traditionally they are obtained by
computing hand-crafted visual descriptors [11, 32, 43].Semantic correspondence (SC). In SC estimation [30],
the goal is not only to find matches across different views
of the exact same object instance, but also across differ-
ent instances of the same object category. While ear-
lier work [30] match two images through densely sampled
SIFT [32] features, later works extended pairwise corre-
spondence learning to a global matching problem [61] and
established region correspondences using object proposals
and their geometric relations over dense descriptors [17].
Subsequently, CNNs trained with only image-level super-
vision have been shown to discover correspondences [31],
paving the way for many deep learning correspondence
systems [10, 18, 26, 59]. Typical correspondence-specific
CNN architectures rely on aggregating features along the
depth of a network [35, 53, 59] or use 4D convolutions span-
ning two images simultaneously [41]. Recently, transform-
ers have been shown to be particularly suited to correspon-
dence discovery thanks to their attention mechanism [9].
While the early works in unsupervised correspondence
discovery [2, 27, 50] use on the convolutional representa-
tions, recent methods [1, 47, 54, 57] build on the success
of transformer-based self-supervised representations [6, 40,
60] to discover salient and robust features, which subse-
quent works have applied to detect semantic correspon-
dences successfully, often surpassing supervised methods.
The features from generative image models have also been
used in SC [22, 49, 57]. Two closely related works [16, 39]
propose to create category-wide representations by aligning
self-supervised feature maps. Unlike our approach, these
works are limited to 2D, and find correspondences between
images without knowing the underlying 3D structure of the
world which renders them incapable of distinguishing dif-
ferent sides of objects ( e.g. left vs. right side of a car). A
concurrent work [58] also identifies geometry-related issues
in SSL-based correspondence but targets a supervised train-
ing regime where our approach does not necessitate key-
point supervision.
3D priors for correspondence. In literature, 3D priors
have been used for SC, mainly in the form of annotated
meshes [8, 29, 37, 46, 62], where annotations can either be
from image to mesh, or between meshes. [29, 37] in partic-
ular use a dense parametrization of the mesh surface similar
to our sphere. When no mesh is available, precise camera
pose can be used to recover sparse correspondences through
constrained geometric optimization [48] or by training a
neural field [56]. In comparison, our approach requires no
meshes and only very coarse viewpoint supervision.
Alternatively, it is possible to estimate 3D models of ob-
ject categories from image supervision alone [3, 15, 23, 25,
36, 55], from which correspondences are extracted. How-
ever, this is very challenging by itself, requiring complex
pipelines capable of modeling accurate mesh deformations,
pose, and rendering, which are all failure-prone.
19522
3. Method
3.1. Problem statement
General formulation. Given an unordered monocular
image collection, we wish to learn a mapping f(I, x) =z
that takes in an RGB image I: Λ→R3,Λ⊂R2and
a pixel location x∈Λto an n-dimensional embedding
z∈ Z ⊂ Rn. In particular, we would like fto map seman-
ticconcepts from different instances of the same category
(e.g. center of front-right wheel of a car) to the same point
in the embedding space Z. Formally, f(I, x) =f(I′, x′),
when the pixels xinI, and x′inI′correspond to the same
semantic part and instance. In practice, the correspond-
ing location for (I, x)inI′can be obtained by finding the
nearest neighbor of f(I, x)in the embedding space, i.e.
arg minx′dZ(f(I, x), f(I′, x′)), where dZis a distance
function over Z. For simplicity, we will use f(I)to refer to
the feature map obtained by evaluating fon all pixels.
An ideal embedding function should be general enough
as to not only match related semantic parts across different
object instances ( e.g. match the front-left wheel across two
different car models) but also specific enough to distinguish
between different part instances within the same object in-
stance ( e.g. front-left wheel vs. front-right wheel). Learning
this from image-based self-supervision alone is a challeng-
ing task due to the visual similarity of repeated parts and
self-occlusion. As a result, it is quite challenging for cor-
respondence estimators to learn that cars have four distinct
wheels from single images without explicit supervision.
Spherical mapping. Instead, it is beneficial to endow a
correspondence estimator with priors about the underly-
ing 3D structure of objects. To this end, previous meth-
ods [29, 62] use detailed 3D meshes via a render-and-
compare approach. However, they rely on the presence
of groundtruth 3D meshes and the precise viewpoint in-
formation. Instead, following [50, 51], we use a simpli-
fied 3D structure, specifically a sphere S2⊂R3(denoted
asSfor simplicity) based on the assumption that the ob-
ject categories of interest are approximately homeomorphic
to spheres ( i.e. objects without holes). While this may at
first appear particularly ill-suited for some categories, e.g.
bicycles, the geometric simplicity of Sprovides a conve-
nient parameterization of an object’s surface by allowing
easy computation of geometric constraints to separate vi-
sual and semantic similarity. In particular, Sprovides a
simple geodesic distance through the dot product, making
it straightforward to push different semantic parts apart,
which we use to distinguish repeated object features. For-
mally, we are interested in building a spherical mapping
fS(I, x)that maps pixels in Ito the surface of S. Unlike
an arbitrary ndimensional space, Sforms an object-centric
specific coordinate system where each region ideally repre-sents a distinct semantic part andinstance on the surface of
the object.
3.2. Learning formulation
In the absence of ground truth correspondences, ensuring
the semantic consistency of our spherical mapping function
is very challenging. [50] enforces correspondence between
image pairs through synthetically augmenting training im-
ages via thinplate splines. Unfortunately, such augmenta-
tions do not produce realistic 3D transformations such as
3D rotations which can result in the self-occlusion of parts.
Instead, similar to [16, 39], we formulate the learning objec-
tive as one of aligning features from a pretrained SSL model
ϕ: Λ→ Z ϕbetween different instances of an object cat-
egory. For simplicity, we will refer to the SSL embedding
spaceZϕsimply as Zin the rest of the text.
While those approaches use flat atlases R2→ Z , we
build a spherical prototype SZ:S→ Z that maps a point
on the surface of the sphere s∈S2to a feature vector
z∈ Z. This formulation has two important benefits. First,
the input domain of SZis simply S, meaning that it does not
depend on I, and it is shared across all images, constituting
a category-based map. Second, and in contrast to [16, 39],
because we chose the output of fSto lie on the surface of
a sphere, we can use it to enforce 3D priors during train-
ing (Sec. 3.3).
Intuitively, for a given category, the spherical prototype
SZshould be able to reconstruct any self-supervised feature
mapϕ(I)when queried on the coordinates predicted by fS,
i.e.SZ(fS(I, x))must closely approximate ϕ(I, x)for any
I, x. Even if SZencodes a perfect category prototype, it
will be unable to encode information about the image back-
ground. Hence we use instance masks M(I)at training
time to restrict the loss computation to only the pixels on
an object’s surface. In practice, these can be obtained using
pretrained segmentation models [21]. Thus, given a training
image Iand a pixel location x, our primary training objec-
tive to learn the parameters of fSandSZcan be formulated
as the minimization of
Lrec=1
|Λ|X
x∈ΛM(I, x)×Γ (ϕ(I, x), SZ(fS(I, x))),
(1)
where Γis the cosine distance between the two inputs.
Our spherical prototypes are implemented via a neural net-
work that maps points in R3toZ. Importantly, each point
s∈S2⊂R3must be mapped independently, meaning that
it should correspond to a unique point z∈ Z, irrespective
of which instance it comes from or where in the image this
part is located. To model multiple categories, and to en-
courage information sharing between closely related ones,
we use a single network conditioned on a category embed-
ding. In practice, SZis a visual transformer [13] without
19523
Features
Coordinates
Input Image SSL Features Spherical CoordinatesFeature Reconstruction Loss
Category-conditioned
Spherical PrototypeSSL 
BackboneSpherical 
Mapper
Coordinate
QueryFigure 2. Overview of our semantic correspondence estimation approach. We begin by extracting features from a frozen self-supervised
backbone, and further use them to predict spherical coordinates via a learned module. Each predicted point is used to query a jointly
learned prototype, providing the supervision signal (Sec. 3.2). The sphere is used to enforce weak geometric priors (Sec. 3.3). During
inference, SSL features are combined with spherical coordinates (Sec. 3.4). A blue outline indicates a fixed module, while orange indicate
learned parameters.
self-attention, but with cross-attention between image to-
kens and a single category token cwhich is a one hot em-
bedding of the category. As learning fSfrom scratch can be
challenging, we further leverage the SSL backbone by sim-
ply adding a prediction head f′
Sto it to produce spherical
maps, i.e.fS=f′
S◦ϕ. An overview of our architecture is
presented in Fig. 2.
3.3. Enforcing geometric priors
The above spherical reconstruction formulation alone of-
fers little benefit over matching the self-supervised features
fromϕdirectly, especially considering the low dimensional-
ity of the intermediate spherical space, which is not guaran-
teed to be well-behaved. However, this spherical structure
enables the computation of simple geometric priors that can
be enforced during training.
Viewpoint regularization. Many object categories ex-
hibit some form of symmetry which can make it particularly
challenging for image-based models to distinguish between
different sides of an object, creating spurious correlations
between distinct parts ( e.g. see cars in Fig. 1). One poten-
tial way of addressing this challenge, without using dense
groundtruth correspondence, is to exploit viewpoint infor-
mation of the object instance.
An interesting property of using a spherical prototype
to represent an object category is that it can easily be used
to infer an object’s viewpoint with respect to the cam-
era. We posit that the average coordinate of a spheri-
cal map of an image, which we present as µ(fS(I)) =
1/|Λ|P
x∈ΛfS(I, x), can be viewed as a coarse approxi-
mation of the camera viewpoint under which the object is
seen, e.g. right side views should be approximately mapped
to the right side of the sphere, and left side views to the
left. Intuitively, thinking of fSas a mapping from image
pixels to the object’s surface, the visible parts of an object
should roughly be those closest to the camera, while the
others should be hidden by self-occlusion.
By using the groundtruth camera viewpoint vI, we en-force the average direction of the spherical map µ(fS(I))
to align with vI. When this is satisfied, symmetric views
must be mapped to different parts of the sphere. While pre-
cise viewpoint information is hard to obtain, we observe it
is sufficient to use coarse relative viewpoint supervision be-
tween images. In practice, viewpoints are discretized into a
small number of bins, and vIis represented by the median
value of the corresponding bin. Then, we simply enforce
that the dot product between viewpoints from two images
equals that of their average spherical maps,
Lvp=X
I,I′||vI·vI′−µ(fS(I))·µ(fS(I′))||2. (2)
Relative distance loss. As noted earlier, current self-
supervised extracted features can confuse repeated object
repeated parts. Depending on the target goal, this can be
desirable, if the parts do indeed belong to the same part cat-
egory. However, this is detrimental for dense correspon-
dence estimation as the parts may actually be different part
instances, e.g. left vs. right leg. It can be hard to distin-
guish between these based on their appearance alone and
additional geometric context is required.
One trivial way of encoding this context is by making
the assumption that similar parts that appear in different lo-
cations on an image ( i.e. not beside each other) are in fact
different instances of the same semantic part. Therefore, we
would like to enforce the property on our predicted spheri-
cal maps that distant pixels in the image should be mapped
far apart on the sphere . Due to scale variation, distance on
the image plane is hard to link to distances on the surface
of the sphere. However, we can reformulate this property as
a relative distance comparison using a triplet. Given three
points a,b, and con an image I, and their corresponding
positions sa=fS(I, a), sb, andscon the sphere, the order-
ing between distances should be preserved, that is:
||a−b|| ≤ || a−c|| ⇐⇒ Γ(sa, sb)≤Γ(sa, sc).(3)
In reality, this does not hold for distances along an object’s
surface, which the sphere is supposed to represent, partly
19524
sc sasb
b
ca sasb
n ucSpherical Coordinates
ubRelative Distance Loss Orientation Loss
scFigure 3. Illustration of our geometry losses LrdandLo. The left
image shows a spherical map from which a triplet of points is sam-
pled.Lrd: as the anchor patch ais closer to the positive bon the
image compared with the negative c, its corresponding position sa
on the sphere must also be closer to sbthan sc.Lo: after project-
ingsbandscto the plane tangent to the sphere at sa, we ensure
orientation is preserved by enforcing positive colinearity between
ub×ucand the normal vector n.
due to perspective. For example, in the case of a long train
seen from a near frontal view, parts at the back of the train
might appear very close to the front in the image, even
though they are very far in 3D space. Nevertheless, we find
that this is still an effective prior for discouraging repeated
part instances from being mapped to the same location.
During training, point triplets (a, b, c )are randomly sam-
pled among pixels that belong to the object. Among each
such triplet, we chose an anchor location anc=a, a posi-
tive location pos= arg minx∈{b,c}||a−x||, and a negative
location neg= arg maxx∈{b,c}||a−x||(see Fig. 3). We
can then define a triplet margin loss as:
Lrd= max(Γ( fS(I, anc), fS(I, pos))
−Γ(fS(I, anc), fS(I, neg)) +δ,0),(4)
where δis a hyperparameter which specifies the margin. A
useful side product of this formulation is that it encourages
the spherical maps to be smooth, as strong discontinuities
would produce a large anchor to positive distance.
Orientation loss. A final advantage of using Sas a latent
space is that it is an orientable 2D space, just like images.
Therefore, enforcing that fSpreserves orientation prevents
spurious matches between symmetric views. Intuitively, if
a point aappears on the right side of bin the image, then sa
must appear in the same side of sb. Given a point triplet as
defined earlier, this can be enforced by making sure that the
determinant of the image triplet and sphere triplet positively
correlate. Formally, defining Paas the orthogonal projec-
tion to the plane tangent to SatsaandubasPa(sb)−sa,
we have dI=det(b−a, c−a)anddS=det(ub, uc), and
wantsign(dI) =sign(dS)(see Fig. 3).
As objects can have complex geometry that our model
does not have access to, there exists no exact relation link-
ing the two determinants together, as for instance perspec-
tive distortions might alter angles. However, we can as-sume that image triplets of large determinants should also
have large determinant on the sphere, as a change of sign in
determinant indicates the relative orientation on the sphere
has been inverted. In practice, we randomly sample image
triplets (a, b, c )and select those whose image determinant
dIis higher than a threshold dτ= 0.7, while swapping b
andcifdIis negative. Then, we enforce the determinant of
the corresponding sphere triplet to be at least dτ,
Lo=(
0 ifdI< dτ
max(dτ−dS,0) ifdI≥dτ.(5)
3.4. Correspondence via combined representations
During inference, it is possible to directly query the spheri-
cal maps fSof two images to obtain correspondences using
the cosine distance. That is, for an image pair I,I′and a
query location qonI, its location in I′can be computed as
p∗= arg min
pΓ(fS(I, q), fS(I′, p)). (6)
While this protocol is sound, it comes with two important
drawbacks. First, fSproduces spherical maps for the whole
image, including the background, meaning segmentation
masks would be required at inference time to prevent the
emergence of spurious matches. Second, the spherical map
is designed to be a smooth parameterization of the object
surface, making it susceptible to missing fine details. A
slightly incorrect mapping fS(I, q) +ϵhas a high proba-
bility of finding a higher match in a nearby region of the
correct match fS(I′, p∗)if both maps are smooth. In com-
parison, SSL feature maps can exhibit strict feature separa-
tion, e.g. wheel features are very different from other nearby
non-wheel features, making it unlikely that a wheel pixel
finds a nearest neighbor among nearby non-wheel pixels.
To mitigate this issue, we make use of the feature maps
that are already computed by the self-supervised back-
bone network ϕ, in a similar way to supervised corre-
spondence approaches that aggregate features from a net-
work [35, 53, 59]. Existing literature has shown that DINO
features are particularly effective at differentiating between
the foreground and background [1], removing the need for
inference-time segmentation. To leverage these DINO fea-
tures, we reformulate Eq. (6) as the combination of self-
supervised features and spherical locations:
p∗= arg minp(1−α) Γ(ϕ(I, q), ϕ(I′, p))
+αΓ(fS(I, q), fS(I′, p)),(7)
where αis a hyperparameter that balances each term.
4. Experiments
Implementation details. We build our models on DINOv1-
B/8 [6] and DINOv2-B/14 [40] backbones. Training masks
19525
♂planeὫ2 /crow Ὢ2/wine-bottleὨC /car ὀ8 ⑁
 ὁ5 ὀE ♂¶otorcycle/walking
 Ὠ6 /tv avgCustomCATS [9] 52.0 34.7 72.2 34.3 49.9 57.5 43.6 66.5 24.4 63.2 56.5 52.0 42.6 41.7 43.0 33.6 72.6 58.0 49.9
MMNet+FCN [59] 55.9 37.0 65.0 35.4 50.0 63.9 45.7 62.8 28.7 65.0 54.7 51.6 38.5 34.6 41.7 36.3 77.7 62.5 50.4
SCorrSan [26] 57.1 40.3 78.3 38.1 51.8 57.8 47.1 67.9 25.2 71.3 63.9 49.3 45.3 49.8 48.8 40.3 77.7 69.7 54.4DINOv1DINOv1 [6] 44.3 26.8 57.6 22.0 29.3 32.8 19.7 54.0 14.9 40.1 39.3 29.3 29.0 37.0 20.0 28.2 40.6 21.1 32.6
ASIC [16] 57.9 25.2 68.1 24.7 35.4 28.4 30.9 54.8 21.6 45.0 47.2 39.9 26.2 48.8 14.5 24.5 49.0 24.6 37.0
Ours 47.1 26.0 70.9 21.8 37.5 34.9 32.4 60.0 23.2 53.6 48.5 42.5 28.3 42.7 21.1 41.9 39.7 41.7 39.7SDDIFT [49] 63.5 54.5 80.8 34.5 46.2 52.7 48.3 77.7 39.0 76.0 54.9 61.3 53.3 46.0 57.8 57.1 71.1 63.4 57.7
SD [57] 63.1 55.6 80.2 33.8 44.9 49.3 47.8 74.4 38.4 70.8 53.7 61.1 54.4 55.0 54.8 53.5 65.0 53.3 56.1DINOv2DINOv2 [40] 72.7 62.0 85.2 41.3 40.4 52.3 51.5 71.1 36.2 67.1 64.6 67.6 61.0 68.2 30.7 62.0 54.3 24.2 56.2
DINOv2 + SD [57] 73.0 64.1 86.4 40.7 52.9 55.0 53.8 78.6 45.5 77.3 64.7 69.7 63.3 69.2 58.4 67.6 66.2 53.5 63.3
Ours (sphere only) 46.7 28.8 66.3 33.0 36.5 66.6 59.1 74.9 25.4 65.7 50.1 52.7 27.1 13.7 15.8 46.6 73.5 36.7 45.5
Ours 76.9 61.2 85.9 42.1 48.4 73.3 67.2 80.0 46.3 80.2 66.7 71.2 66.0 63.9 36.2 68.6 67.8 42.2 63.6
Ours + SD 74.8 64.5 87.1 45.6 52.7 77.8 71.4 82.4 47.7 82.0 67.3 73.9 67.6 60.0 49.9 69.8 78.5 59.1 67.3
Table 1. Keypoint matching scores on SPair-71k evaluated using PCK@0.1 with macro -averaging for the summary scores. We present our
approach using DINOv1 [6] features (middle rows) and DINOv2 [40] features (bottom rows). In both cases, we improve over the DINO
only baselines and are superior to fully supervised methods (top rows). Bold entries are best per category and underlined are second-best.
are obtained using a pretrained Mask R-CNN model [21].
Our spherical mapper consists of a linear dimension reduc-
tion layer halving the number of features, a visual trans-
former block with self-attention, and a final linear layer with
an output dimension of three. The resulting feature map is
then normalized pixel-wise so that its output values lies on
the surface of S2. The dimensionality reduction layer, while
not strictly necessary, is helpful considering the small num-
ber of images used during training, while the transformer
block helps in disentangling sides and repeated parts by al-
lowing global reasoning between image patches.
The final loss term is computed as L=Lrec+λrdLrd+
λoLo+λvpLvp. We set λrd=λo= 0.3, while λvp=
0.1is set slightly lower as it can have the detrimental effect
of pulling all predictions towards the average over image
pixels - cf. Eq. (2). The relative distance margin is set to δ=
0.5to encourage feature separation. To perform matching
in Eq. (7), αis set to 0.2so that we leverage the highly
discriminative DINO features, while using the sphere as a
symmetry and repetition-breaking mechanism. Our model
is trained for a total of 200 epochs using Adam [28].
Datasets. Following existing work [16, 26, 57], we eval-
uate our method primarily on the Spair-71k dataset [34],
which contains images from 18 different object categories.
The evaluation set contains between 600 and 900 image
pairs, each of which are annotated with the keypoints that
are visible in both images. We train our model on the Spair-
71k training split on all categories simultaneously, without
using any keypoint annotations. Additionally, we evaluate
on the AwA-pose dataset [4], which contains images from
35 quadruped categories, annotated with keypoints. We also
present results by training on the Freiburg cars dataset [45].
This dataset was collected by densely sampling approxi-
mately 100 images each around 48 different car instances.
The higher image count, coupled with 360odense sampling,
enables us to learn a more detailed spherical prototype.4.1. Keypoint average precision
While issues with symmetries are clear in Fig. 4, we argue
that the PCK metric fails to account for them as it is only
computed between keypoints that appear in both the source
and target image. Hence, models that predict high similarity
between two visually similar but semantically different key-
points might not be penalized for it. PCK can partly account
for repetition-related mistakes if the parts appear simultane-
ously in both images, but it does not penalize symmetry-
related issues, like mistaking the left and right side of a
car, as the corresponding keypoints do not appear simul-
taneously. PCK was initially proposed for simpler cases
where keypoints were assumed to be always be visible, e.g.
faces or different instances in the same pose. However, as
benchmarks become more challenging, a more robust pro-
tocol should be used. To illustrate how much these failure
cases impact SSL models, we propose an alternative eval-
uation metric, Keypoint Average Precision - KAP@ κ. It is
similar to PCK@ κ, but accounts for these errors by eval-
uating average precision instead of accuracy and penalizes
methods that predict matches when none exist.
When performing evaluation on a pair of images, instead
of restricting it to keypoints that simultaneously appear in
both images, we consider allkeypoints that appear in the
source, irrespective of whether they are also visible in the
target image. For each such keypoint, if it appears in the
target image, we extract the highest similarity value within
radius κof the ground-truth and label it as positive. We also
extract the highest similarity value outside the radius and
label it as negative, penalizing overly high predictions on
wrong, possibly repeated parts. If the source keypoint does
not appear on the target image, we simply extract the high-
est similarity prediction across the whole target image and
label it as negative to penalize incorrectly high similarity
when none should exist. This effectively reformulates the
task as a binary classification problem, where the embed-
19526
Image
 DINOv2
 SD
 DINOv2+SD
 Sphere
Figure 4. Qualitative comparison of dense correspondence maps. For DINOv2, SD, and DINO+SD features we perform PCA on the
segmented object features independently for each category, then visualize the three main components. Note that the SD and DINO+SD
features are not completely equivalent to the ones used to compute matches, but are provided here for illustration. Spherical maps from
fS(Sphere) for our approach are visualized directly. Our spherical maps correctly identify the different sides of objects, whereas other
features fail to capture these differences.
ding of each source keypoint has to be close to, and only
to, the corresponding target keypoint. Finally, we compute
the average precision over all the extracted pairs, and report
mean average precision per-category.
4.2. SPair-71k results
We first evaluate on the task of keypoint transfer between
pairs of images using Spair-71k [34]. We present quan-
titative per-category results in Tab. 1, using the standard
PCK metric along with macro -averaged scores as the num-
ber of per-category pairs varies greatly. We split the mod-
els according to their backbone: custom supervised mod-
els, DINOv1-based, SD-based, and DINOv2-based. A
striking preliminary observation is the effectiveness of DI-
NOv2, surpassing supervised approaches on most cate-
gories. While its success can partially be attributed to evalu-
ation biases [2], this demonstrates the effectiveness of large
scale learned self-supervised features.
Using a DINOv1 backbone, our model improves over its
backbone on most categories, and over ASIC [16] on av-
erage. When combined with the DINOv2 backbone, our
model provides improvements on all but two categories. A
clear pattern can be identified, with strongest improvements
being observed over blob-like symmetric objects with re-
peated parts (+21.0 on bus and +15.7 on car), while spheri-
cal mapping hinders performance on highly deformable cat-
egories (-4.3 on person), or high-genus objects for which a
sphere is a poor surface prior (-0.8 on bicycle). Our model
performs slightly better compared to DINO+SD [57], while
requiring a fraction of the computational cost at inference
time. Finally, following [57] we evaluate adding Stable Dif-fusion (SD) [42] features to our method at inference time,
which yields further improvements. This illustrates that
spherical maps and SD features capture different facets of
the correspondence problem.
We show the results of evaluating correspondences with
KAP in Tab. 2. We observe particularly large discrepancies
between PCK and KAP in highly symmetric objects with
repeated parts, e.g. buses and cars, and the ranking between
our model and DINOv2 is reversed on humans, possibly due
to DINOv2’s symmetric predictions on left and right limbs.
All models across all categories obtain lower results when
using KAP highlight the symmetry issues they are facing. A
limitation of KAP is that it might still count correct repeated
parts mistakes if their distance in the image is less than κ.
Nonetheless, we argue that KAP is more informative than
PCK and is just as efficient to evaluate.
The qualitative visualization shown in Fig. 4 illustrates
these improved results. Our model correctly maps the two
opposite sides of the cars to different sphere regions, while
other feature maps look extremely similar, even when look-
ing at two opposite sides of an object, with the notable ex-
ception of SD on birds. The fact that such similarities exist
between opposite views suggest that the utility of baselines
such as DINOv2 would be quite limited in real scenarios.
4.3. Additional evaluation
Animals with Attributes (AwA). To further evaluate our
model, we also report results on the AwA-pose dataset
from [4], which contains images from 35 different cat-
egories of quadruped. While our spherical prototype
needs category information during training to reconstruct
19527
♂planeὫ2 /crow Ὢ2/wine-bottleὨC /car ὀ8 ⑁
 ὁ5 ὀE ♂¶otorcycle/walking
 Ὠ6 /tv avg
DINOv2 [40] 53.5 54.0 60.2 35.5 44.4 36.3 31.7 61.3 37.4 54.7 52.5 51.5 48.8 48.2 37.8 44.1 47.4 38.2 46.5
SD [57] 44.4 48.5 54.5 31.5 45.2 32.7 30.0 68.4 35.8 55.2 47.9 48.1 44.8 42.3 44.5 39.2 52.7 51.2 45.4
DINOv2 + SD [57] 52.0 55.9 59.2 34.7 49.0 36.0 32.5 70.3 39.8 59.8 53.1 52.4 50.6 50.4 47.8 46.2 53.3 49.8 49.6
Ours (sphere only) 38.4 34.2 53.9 33.0 37.9 49.7 43.4 71.7 29.8 57.1 45.8 42.5 32.4 27.0 29.5 37.1 57.4 36.0 42.1
Ours 60.7 51.2 63.1 38.4 45.0 55.9 45.7 69.7 40.4 63.2 54.8 54.3 51.2 48.7 38.8 47.9 55.5 42.2 51.5
Ours + SD 58.9 54.2 62.2 39.6 46.6 54.5 47.1 76.2 40.9 65.3 57.3 56.1 54.2 47.4 43.7 49.4 62.4 52.0 53.8
Table 2. Keypoint matching scores on SPair-71k evaluated using KAP@0.1 with macro-averaging for the summary scores.
category-specific features, only the sphere mapper is used
during inference, meaning it can be readily applied to un-
seen categories. Five quadruped classes (cat, cow, dog,
horse, and sheep) appear in both SPair and AwA-pose, and
are therefore seen during training, but the rest are com-
pletely new for our sphere mapper. Nonetheless, results
shown in Tab. 3 show that the sphere mapper generalizes
well to unseen categories. We attribute this to the assis-
tance from the strong category-agnostic features from DI-
NOv2. Once again, the improvement of using our spherical
method is much more apparent using KAP, i.e. the PCK im-
provement over DINOv2 alone is 2.2 when adding SD and
2.6 when adding spheres, while these become 1.8 and 3.9
respectively with KAP.
Dv2 SD Dv2+SD Ours Ours+SD
PCK@0.1 65.9 56.0 68.1 68.7 69.8
KAP@0.1 55.0 50.7 56.8 58.9 60.6
Table 3. Average scores when evaluating on AwA-pose using a
random subset of 200 pairs per category. ‘Ours’ denotes our model
trained on SPair-71k with a DINOv2 backbone.
Model ablation. We also analyze the impact of removing
each individual loss term when training our model in an ab-
lation study. The results in Tab. 4 indicate that all variants
except the no Lrdone do not significantly improve over the
DINOv2 baseline as they also fail to identify opposite sides
of objects and repeated parts.
DINOv2 no Lvp noLrd noLofull model
56.2 58.6 61.2 56.0 63.6
Table 4. Average PCK@0.1 scores when training and testing on
SPair-71k using different ablated versions of our approach.
Inference speed. The results in Tab. 1 demonstrate how ef-
fective the addition of Stable Diffusion features are. How-
ever, computing correspondences through diffusion requires
adding noise to images, running iterative diffusion, and per-
forming joint decomposition between the source and target
features. As a result, its inference speed is much slower
compared to using DINOv2 features. In comparison, our
approach only requires a few extra layers on top of DINOv2
and has negligible additional overhead. Timing results in
Tab. 5 illustrate that adding diffusion-extracted features re-
duces throughput by an order of magnitude.DINOv2 SD DINOv2+SD Ours Ours+SD
3.4 0.38 0.35 3.3 0.34
Table 5. Descriptor computation throughput in pairs/second at in-
ference time on a single A5000 GPU, where higher is better.
4.4. Limitations
While our approach results in significant improvements
over the DINOv2 baseline for many categories (see Tab. 1),
there are still some limitations. For example, we do not
perform as well on a small number of categories that can-
not be well approximated by a sphere ( e.g. humans). Fur-
thermore, the very low-dimensional spherical encoding on
its own is not as effective at discriminating parts and sub-
parts and thus needs to be combined with self-supervised
features. However, as we already densely compute these
features at inference time for each test image, there is no
additional cost in combining them with our spherical map-
ping when evaluating correspondence. Finally, our method
relies on additional supervision at training time in the form
of automatically computed segmentation masks and weak
camera pose supervision. However, results in the supple-
mentary material suggest that coarse camera viewpoints are
enough to obtain good performance.
5. Conclusion
We presented a new approach for semantic correspondence
estimation that is robust to issues resulting from object sym-
metries and repeated part instances. In addition, we pro-
posed a new evaluation protocol that is more sensitive to
these issues. Our approach leverages recent advances in the
self-supervised learning of discriminative image features
and combines them with a weak geometric spherical prior.
By using weak pose supervision to represent objects cate-
gories on the surface of spheres, we are able to enforce sim-
ple geometric constraints during training that result in more
geometric and semantically consistent representations. Re-
sults on the task of semantic keypoint matching between
images from different object instances, a fundamental task
in 3D computer vision, demonstrate that our approach alone
is superior to existing strong baselines and can be combined
with existing methods to further boost performance.
Acknowledgments. This project was supported by the EP-
SRC Visual AI grant EP/T028572/1.
19528
References
[1] Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel.
Deep vit features as dense visual descriptors. ECCV Work-
shop - What is Motion For? , 2022. 1, 2, 5
[2] Mehmet Ayg ¨un and Oisin Mac Aodha. Demystifying un-
supervised semantic correspondence estimation. In ECCV ,
2022. 1, 2, 7
[3] Mehmet Ayg ¨un and Oisin Mac Aodha. SAOR: Single-View
Articulated Object Reconstruction. In CVPR , 2024. 2
[4] Prianka Banik, Lin Li, and Xishuang Dong. A novel dataset
for keypoint detection of quadruped animals from images.
arXiv:2108.13958 , 2021. 6, 7
[5] Steven S. Beauchemin and John L. Barron. The computation
of optical flow. ACM computing surveys , 1995. 2
[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In
ICCV , 2021. 1, 2, 5, 6
[7] Zhuoyuan Chen, Hailin Jin, Zhe Lin, Scott Cohen, and Ying
Wu. Large displacement optical flow from nearest neighbor
fields. In CVPR , 2013. 2
[8] An-Chieh Cheng, Xueting Li, Min Sun, Ming-Hsuan Yang,
and Sifei Liu. Learning 3d dense correspondence via canon-
ical point autoencoder. NeurIPS , 2021. 2
[9] Seokju Cho, Sunghwan Hong, Sangryul Jeon, Yunsung Lee,
Kwanghoon Sohn, and Seungryong Kim. Cats: Cost ag-
gregation transformers for visual correspondence. NeurIPS ,
2021. 1, 2, 6
[10] Christopher B Choy, JunYoung Gwak, Silvio Savarese, and
Manmohan Chandraker. Universal correspondence network.
NeurIPS , 2016. 2
[11] Navneet Dalal and Bill Triggs. Histograms of oriented gra-
dients for human detection. In CVPR , 2005. 2
[12] Andrew J Davison, Ian D Reid, Nicholas D Molton, and
Olivier Stasse. Monoslam: Real-time single camera slam.
PAMI , 2007. 2
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In ICLR , 2021. 3
[14] Vittorio Ferrari, Tinne Tuytelaars, and Luc Van Gool. Simul-
taneous object recognition and segmentation from single or
multiple model views. IJCV , 2006. 2
[15] Shubham Goel, Angjoo Kanazawa, and Jitendra Malik.
Shape and viewpoint without keypoints. In ECCV , 2020. 2
[16] Kamal Gupta, Varun Jampani, Carlos Esteves, Abhinav Shri-
vastava, Ameesh Makadia, Noah Snavely, and Abhishek Kar.
Asic: Aligning sparse in-the-wild image collections. In
ICCV , 2023. 2, 3, 6, 7
[17] Bumsub Ham, Minsu Cho, Cordelia Schmid, and Jean
Ponce. Proposal flow: Semantic correspondences from ob-
ject proposals. PAMI , 2017. 2
[18] Kai Han, Rafael S Rezende, Bumsub Ham, Kwan-Yee K
Wong, Minsu Cho, Cordelia Schmid, and Jean Ponce. Sc-
net: Learning semantic correspondence. In ICCV , 2017. 2[19] Christopher G Harris and JM Pike. 3d positional integration
from image sequences. Image and Vision Computing , 1988.
2
[20] Richard Hartley and Andrew Zisserman. Multiple view ge-
ometry in computer vision . Cambridge university press,
2003. 2
[21] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-
shick. Mask r-cnn. In ICCV , 2017. 3, 6
[22] Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack,
Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi.
Unsupervised semantic correspondence using stable diffu-
sion. In NeurIPS , 2023. 2
[23] Paul Henderson and Vittorio Ferrari. Learning to generate
and reconstruct 3d meshes with only 2d supervision. In
BMVC , 2018. 2
[24] Heiko Hirschmuller. Accurate and efficient stereo processing
by semi-global matching and mutual information. In CVPR ,
2005. 2
[25] Tao Hu, Liwei Wang, Xiaogang Xu, Shu Liu, and Jiaya Jia.
Self-supervised 3d mesh reconstruction from single images.
InCVPR , 2021. 2
[26] Shuaiyi Huang, Luyu Yang, Bo He, Songyang Zhang, Xum-
ing He, and Abhinav Shrivastava. Learning semantic corre-
spondence with sparse annotations. In ECCV , 2022. 1, 2,
6
[27] Tomas Jakab, Ankush Gupta, Hakan Bilen, and Andrea
Vedaldi. Unsupervised learning of object landmarks through
conditional image generation. NeurIPS , 2018. 2
[28] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In ICLR , 2015. 6
[29] Nilesh Kulkarni, Abhinav Gupta, and Shubham Tulsiani.
Canonical surface mapping via geometric cycle consistency.
InICCV , 2019. 2, 3
[30] Ce Liu, Jenny Yuen, Antonio Torralba, Josef Sivic, and
William T Freeman. Sift flow: Dense correspondence across
different scenes. In ECCV , 2008. 1, 2
[31] Jonathan L Long, Ning Zhang, and Trevor Darrell. Do con-
vnets learn correspondence? NeurIPS , 2014. 2
[32] David G Lowe. Object recognition from local scale-invariant
features. In ICCV , 1999. 2
[33] David G Lowe. Distinctive image features from scale-
invariant keypoints. IJCV , 2004. 2
[34] Juhong Min, Jongmin Lee, Jean Ponce, and Minsu Cho.
Spair-71k: A large-scale benchmark for semantic correspon-
dence. arXiv:1908.10543 , 2019. 6, 7
[35] Juhong Min, Jongmin Lee, Jean Ponce, and Minsu Cho.
Learning to compose hypercolumns for visual correspon-
dence. In ECCV , 2020. 2, 5
[36] Tom Monnier, Matthew Fisher, Alexei A Efros, and Mathieu
Aubry. Share with thy neighbors: Single-view reconstruction
by cross-instance consistency. In ECCV , 2022. 2
[37] Natalia Neverova, David Novotny, Marc Szafraniec, Vasil
Khalidov, Patrick Labatut, and Andrea Vedaldi. Continuous
surface embeddings. In NeurIPS , 2020. 2
[38] H K Nishihara. Prism: A practical realtime imaging stereo
matcher. In Intelligent Robots: 3rd International Conference
on Robot Vision and Sensory Controls , 1984. 2
19529
[39] Dolev Ofri-Amar, Michal Geyer, Yoni Kasten, and Tali
Dekel. Neural congealing: Aligning images to a joint se-
mantic atlas. In CVPR , 2023. 2, 3
[40] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy
V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.
Dinov2: Learning robust visual features without supervision.
arXiv:2304.07193 , 2023. 1, 2, 5, 6, 8
[41] Ignacio Rocco, Mircea Cimpoi, Relja Arandjelovi ´c, Akihiko
Torii, Tomas Pajdla, and Josef Sivic. Neighbourhood con-
sensus networks. NeurIPS , 2018. 2
[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , 2022. 7
[43] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary
Bradski. Orb: An efficient alternative to sift or surf. In ICCV ,
2011. 2
[44] Johannes L Schonberger and Jan-Michael Frahm. Structure-
from-motion revisited. In CVPR , 2016. 1, 2
[45] Nima Sedaghat and Thomas Brox. Unsupervised generation
of a viewpoint annotated car dataset from videos. In ICCV ,
2015. 6
[46] Roman Shapovalov, David Novotny, Benjamin Graham,
Patrick Labatut, and Andrea Vedaldi. Densepose 3d: Lift-
ing canonical surface maps of articulated objects to the third
dimension. In ICCV , 2021. 2
[47] Aleksandar Shtedritski, Andrea Vedaldi, and Christian Rup-
precht. Learning universal semantic correspondences with
no supervision and automatic data curation. In ICCV Work-
shops , 2023. 2
[48] Supasorn Suwajanakorn, Noah Snavely, Jonathan J Tomp-
son, and Mohammad Norouzi. Discovery of latent 3d key-
points via end-to-end geometric reasoning. NeurIPS , 2018.
2
[49] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng
Phoo, and Bharath Hariharan. Emergent correspondence
from image diffusion. In NeurIPS , 2023. 2, 6
[50] James Thewlis, Hakan Bilen, and Andrea Vedaldi. Unsuper-
vised learning of object frames by dense equivariant image
labelling. NeurIPS , 2017. 1, 2, 3
[51] James Thewlis, Samuel Albanie, Hakan Bilen, and Andrea
Vedaldi. Unsupervised learning of landmarks by descriptor
vector exchange. In ICCV , 2019. 3
[52] Philip HS Torr and Andrew Zisserman. Feature based meth-
ods for structure and motion estimation. In International
workshop on vision algorithms , 1999. 2
[53] Nikolai Ufer and Bjorn Ommer. Deep semantic feature
matching. In CVPR , 2017. 2, 5
[54] Matthew Walmer, Saksham Suri, Kamal Gupta, and Abhi-
nav Shrivastava. Teaching matters: Investigating the role of
supervision in vision transformers. In CVPR , 2023. 1, 2
[55] Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rup-
precht, and Andrea Vedaldi. MagicPony: Learning articu-
lated 3d animals in the wild. In ICCV , 2023. 2
[56] Lin Yen-Chen, Pete Florence, Jonathan T Barron, Tsung-Yi
Lin, Alberto Rodriguez, and Phillip Isola. Nerf-supervision:
Learning dense object descriptors from neural radiance
fields. In ICRA , 2022. 2[57] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Pola-
nia Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan
Yang. A tale of two features: Stable diffusion complements
dino for zero-shot semantic correspondence. In NeurIPS ,
2023. 2, 6, 7, 8
[58] Junyi Zhang, Charles Herrmann, Junhwa Hur, Eric Chen,
Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. Telling
left from right: Identifying geometry-aware semantic corre-
spondence. In CVPR , 2024. 2
[59] Dongyang Zhao, Ziyang Song, Zhenghao Ji, Gangming
Zhao, Weifeng Ge, and Yizhou Yu. Multi-scale matching
networks for semantic correspondence. In ICCV , 2021. 1, 2,
5, 6
[60] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang
Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training
with online tokenizer. In ICLR , 2022. 1, 2
[61] Tinghui Zhou, Yong Jae Lee, Stella X Yu, and Alyosha A
Efros. Flowweb: Joint image set alignment by weaving con-
sistent, pixel-wise correspondences. In CVPR , 2015. 2
[62] Tinghui Zhou, Philipp Krahenbuhl, Mathieu Aubry, Qixing
Huang, and Alexei A Efros. Learning dense correspondence
via 3d-guided cycle consistency. In CVPR , 2016. 2, 3
19530
