BSNet: Box-Supervised Simulation-assisted Mean Teacher for 3D Instance
Segmentation
Jiahao Lu1, Jiacheng Deng1, Tianzhu Zhang1,2,*
1University of Science and Technology of China,2Deep Space Exploration Lab
{lujiahao, dengjc}@mail.ustc.edu.cn, tzzhang@ustc.edu.cn
Abstract
3D instance segmentation (3DIS) is a crucial task, but
point-level annotations are tedious in fully supervised set-
tings. Thus, using bounding boxes (bboxes) as annota-
tions has shown great potential. The current mainstream
approach is a two-step process, involving the generation
of pseudo-labels from box annotations and the training of
a 3DIS network with the pseudo-labels. However, due
to the presence of intersections among bboxes, not every
point has a determined instance label, especially in over-
lapping areas. To generate higher quality pseudo-labels
and achieve more precise weakly supervised 3DIS results,
we propose the Box-Supervised Simulation-assisted Mean
Teacher for 3D Instance Segmentation (BSNet), which de-
vises a novel pseudo-labeler called Simulation-assisted
Transformer. The labeler consists of two main components.
The ﬁrst is Simulation-assisted Mean Teacher, which intro-
duces Mean Teacher for the ﬁrst time in this task and con-
structs simulated samples to assist the labeler in acquir-
ing prior knowledge about overlapping areas. To better
model local-global structure, we also propose Local-Global
Aware Attention as the decoder for teacher and student la-
belers. Extensive experiments conducted on the ScanNetV2
and S3DIS datasets verify the superiority of our designs.
Code is available at https://github.com/peoplelu/BSNet .
1. Introduction
3D instance segmentation is a fundamental task in 3D scene
understanding, primarily focused on predicting masks and
categories for every foreground object within a scene. Cur-
rent instance segmentation methods are mainly in fully su-
pervised settings [ 25,30,34,36,37] and achieve commend-
able results. However, the time-consuming nature of point-
level annotations poses a signiﬁcant challenge. In contrast,
annotating instances with 3D bboxes (object-level) is no-
tably easier, requiring only the annotations for center points
and dimensions (length, width, height). Nevertheless, a no-
*Corresponding Author
Global modeling
Local modeling
Global modeling…
…Local modeling
Simulation
generation
Weight 
Initialization
Strong 
supervisionWeak
supervision(b) (c)Heuristics Gaussian Process Ours
Real sample SimulatedsampleNon-overlapping 
area
Overlapping sample Non-overlapping 
areaOverlapping 
area
(a)
GT
 GT
Real sceneSplit
Mean Teacher Base Model
Simulation-assisted Mean TeacherPredictions Predictions Predictions PredictionsFigure 1. (a) The visualization of an overlapping sample. (b) The
proposed Simulation-assisted Mean Teacher helps the labeler ac-
quire prior knowledge from simulated samples. (c) Our method
improves local-global structure modeling of overlapping samples
to generate better pseudo-labels (especially in yellow circles).
table limitation stems from the use of bboxes, which cannot
capture the detailed shape or geometry of objects. Conse-
quently, bridging the gap between object-level and point-
level annotations remains a challenge.
To solve the above challenge, existing methods [ 8,11,
35] conduct several explorations. Box2Mask [ 8] parame-
terizes bboxes and utilizes them as labels. However, due to
bbox overlap, some point clouds may exist within multiple
bboxes, introducing ambiguity in point-object assignments.
As illustrated in Figure 1(a), instance labels for point clouds
in non-overlapping areas are determined as they only belong
to one bbox. In contrast, overlapping areas are governed by
two different bboxes, resulting in indeterminate instance la-
bels. Consequently, the point-wise predicted bboxes cannot
be reliably used for clustering. To better address the ambi-
guity in overlapping areas, WISGP [ 11] employs straight-
forward heuristics based on local structure modeling. Con-
cretely, for each indeterminate point, WISGP selects the
most common label from its neighboring points as its la-
bel. On the other hand, Gapro [ 29] uses Gaussian Process
(GP) [ 35] to train individual overlapping samples, model-
ing global structure by ﬁtting the similarity relationships
between all points into a Gaussian distribution. Then Gapro
computes posterior probability to achieve binary classiﬁca-
tion for overlapping areas.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
20374
Based on the preceding discussion, we identify two key
issues in bbox-supervised 3DIS: 1) How to generate labels
for overlapping areas? Using Mean Teacher [ 3,10,20,48]
to generate and continuously optimize pseudo-labels for
overlapping areas is an effective way. This paradigm al-
lows for the online update of pseudo-labels during the train-
ing process, continuously transferring knowledge from a
teacher network to a student network. Besides, the teacher
network employs Exponential Moving Average (EMA) to
integrate information from historical students, providing
more stable learning targets for the student network. How-
ever, given that non-overlapping areas contain a single ob-
ject with a clear structure, while overlapping areas involve
two intertwined objects, there is a signiﬁcant disparity in
complexity between them. Hence, it is difﬁcult to infer ac-
curate pseudo-labels for overlapping areas solely according
to non-overlapping area labels. To tackle this issue, given
the abundance of non-overlapping bboxes in the dataset
with deﬁnite labels, we can construct simulated overlapping
samples using these bboxes. As illustrated in Figure 1(b),
we can train a base model on these simulated samples, and
transfer this model to real datasets. By this way, the infor-
mation loss resulting from the absence of labels in overlap-
ping areas can be compensated and higher quality pseudo-
labels can be predicted. 2) How to better model structure of
overlapping samples? As illustrated in Figure 1(c), current
methods [ 11,29] either tend to focus on local structure mod-
eling, bringing dedicated local structure representations like
WISGP [ 11], or emphasize global relationship modeling,
results in a more effective connection between overlapping
areas and non-overlapping areas like Gapro [ 29]. Both types
of modeling are crucial, but there is currently no approach
effectively integrating these two aspects. Consequently, it
is essential to devise a universal network proﬁcient in ex-
tracting local structure features efﬁciently while fostering
interactions between overlapping and non-overlapping ar-
eas, yielding more precise pseudo-labels.
To achieve the above goals, we introduce a novel
pseudo-labeler called Simulation-assisted Transformer
(SAFormer), which is trained based on an innovative
training strategy called Simulation-assisted Mean Teacher
(SMT) and incorporates a special decoder called Local-
Global Aware Attention (LGA). In order to solve the ﬁrst
problem in the previous paragraph, we introduce the SMT.
Concretely, student network is directly supervised with def-
inite instance labels for non-overlapping areas, and for
overlapping areas, pseudo-labels generated by teacher net-
work are used as supervision. As to teacher network, we
use EMA to updates its parameters. This approach yields
more accurate predictions for overlapping areas compared
to classical statistical methods like GP [ 35]. Furthermore,
to address the challenge of suboptimal pseudo-label qual-
ity, we generate simulated overlapping samples using non-overlapping bboxes. And these simulated samples are used
to train a base model, producing weights that serve as the
initialization for teacher and student networks. This funda-
mentally equips the network with the ability to distinguish
overlapping areas, and the higher quality pseudo-labels gen-
erated by the teacher network aid in the rapid training of
the Mean Teacher. Additionally, when applied to multi-
ple datasets, only a brief ﬁnetuning is required instead of
retraining the pre-trained weights. Taking S3DIS [ 2] as
an example, the model’s training time decreases from 42
hours to just 1.7 hours. As to the second problem , we in-
troduce the LGA. Concretely, we ﬁrst initialize two learn-
able queries, with each representing one of the two fore-
ground instances. We then employ local-structure atten-
tion to effectively model the local structure of each in-
stance and aggregate structural relationships within each in-
stance into a holistic representation through queries. Sub-
sequently, by employing global-context attention, we facil-
itate the aggregation of global information, especially inter-
actions between the two foreground instances and interac-
tions between overlapping areas and non-overlapping areas.
Through this design, we can effectively model category,
structure, and contextual information adaptively. Addition-
ally, we can leverage the response values of the overlapping
area points to the queries to remove background points from
the overlapping areas.
In summary, the main contributions of this work are as
follows: (i) We propose a weakly supervised 3D instance
segmentation method called BSNet, which uses bboxes as
annotations and devises a novel pseudo-labeler. (ii) We de-
sign a pioneering pseudo-labeler called SAFormer, which
for the ﬁrst time incorporates the deep neural network and
the Mean Teacher paradigm, and innovatively constructs
simulated samples to facilitate training. Besides, with the
help of LGA, SAFormer can accurately predict pseudo-
labels for overlapping areas, thus achieving precise weakly
supervised 3DIS results. (iii) Extensive experimental re-
sults on two standard benchmarks, ScanNetV2 [ 9] and
S3DIS [ 2], verify the superiority of our designs.
2. Related Work
In this section, we brieﬂy overview related works on 3D
instance segmentation, weakly supervised 3D instance seg-
mentation and the Mean Teacher paradigm.
3D Instance Segmentation. 3D instance segmentation
is a fundamental task for 3D scene understanding, which
can be categorized into proposal-based, grouping-based
and transformer-based methods. Proposal-based meth-
ods [ 12,23,45,46] extract 3D bboxes and utilize a mask
learning branch to predict the object mask inside each box.
Grouping-based methods [ 5,19,21,30,37,42,50] predict
semantic categories and geometric offsets for each point,
and then employ clustering algorithms to group the points
20375
Pseudo-labelingSimulatedSamples
EMA UpdateN×6Teacher Labeler
Student LabelerAug.Backbone
Backbone
Backbone LGA
Weight Initialization
Overlapping Non-overlapping
N×6Flipping
Jittering
Elastic Distortion
RotationN×C
N×CN×C
Pseudo-labels
Threshold
Real SamplesN×6
GT
GT
Real Scenes Simulation 
generationⅡ ⅠFramework Process to Generate SAFormer
Labeler3D Bounding 
Boxes
Pseudo Labels
SAFormer 3D Instance
Segmentation Network 
Point Cloud
Soft Supervision
Predictions
Aug. Samples
Local-structure 
Attention
Global-context 
AttentionLGALocal-structure 
Attention
Global-context 
AttentionLGALocal-structure
Attention
Global-context 
AttentionLGAFigure 2. (I) The overall framework of our method BSNet. (II) The total process to generate an outstanding pseudo-labeler
SAFormer. BSNet is a novel two-step method consisting of generating pseudo instance labels by SPFormer and using the pseudo instance
labels to train a 3D instance segmentation network.
into instances. Transformer-based methods [ 25,34,36] are
the state-of-the-art paradigm, where queries are used to rep-
resent instances and global information is aggregated into
queries through a transformer decoder [ 4,6,7]. Although
these fully supervised methods have achieved superior per-
formance, they still have signiﬁcant limitations in practice
due to the time-consuming point-wise instance annotation.
Therefore, some weakly supervised instance segmentation
methods have been proposed to alleviate this problem.
Weakly Supervised 3D Instance Segmentation. The cur-
rent weakly supervised 3D instance segmentation meth-
ods [ 8,11,17,29,43] can be divided into two categories:
sparse points as annotations and 3D bboxes as annotations.
Sparse-point annotation methods [ 17,43] primarily utilize
sparsely labeled point clouds as supervision to train the net-
work. In comparison to methods using sparse points as an-
notations, 3D bboxes provide richer instance information
such as category and shape size, enabling the network to
better handle instance segmentation tasks. Box2mask [ 8]
uses bboxes as supervision, allowing the network to pre-
dict bbox for each individual point. WISGP [ 11] leverages
3D local geometric information to generate point-level la-
bels from bbox annotations. Gapro [ 29] employs GP [ 35] to
model the global similarity relationships between overlap-
ping and non-overlapping regions. However, these meth-
ods have relatively simple modeling of structure in overlap-
ping samples and do not effectively incorporate category,
structure, and contextual information. In contrast, our pro-
posed Local-Global Aware Attention enhances the capacity
to model both local structures and global relationships.
Mean Teacher Paradigm. The Mean Teacher paradigmhas been widely researched in various tasks, such as UDA
for semantic segmentation [ 1,18,47], semi-supervised ob-
ject detection [ 26,40,44], weakly supervised object de-
tection [ 39,41], UDA for object detection [ 3,20], and
UDA for person ReID [ 13,16,48]. This paradigm helps
to avoid the interative self-training complicated multi-stage
training process. SoftTeacher [ 44] introduces the ﬁrst end-
to-end pseudo labeling framework in semi-supervised ob-
ject detection, gradually improving the quality of pseudo
labels during a curriculum. To mitigate the issue of low-
quality pseudo-labels, CMT [ 3] identiﬁes the alignment and
synergy between Mean Teacher and contrastive learning.
UNRN [ 48] proposes the estimation and exploitation of the
credibility of assigned pseudo-labels for each sample, re-
ducing the impact of noisy pseudo-labels generated by the
teacher network. Based on the above research, we intro-
duce a Simulation-assisted Mean Teacher approach, which
employs the Mean Teacher paradigm to generate stable
pseudo-labels in real-time and constructs simulated samples
to assist the network in acquiring prior knowledge about
overlapping areas.
3. Method
3.1. Overview
As illustrated in Figure 2(I), the framework of our method
begins by generating pseudo object masks for instances in
the training set based on bbox annotations. Subsequently,
these pseudo object masks are employed to train a 3DIS
network. Throughout the entire process, the most crit-
ical step is to generate an outstanding pseudo-labeler to
predict pseudo-labels for overlapping areas, as shown in
20376
Figure 2(II). In the generation process, two distinct de-
signs stand out. The ﬁrst one is the adoption of a unique
training strategy called Simulation-assisted Mean Teacher
(SMT), which can be divided into two steps: Simulated
Sample Generation in Section 3.2.1 and Mean Teacher Ap-
proach in Section 3.2.3 . The second one is a novel de-
coder named Local-Global Aware Attention (LGA) in Sec-
tion3.2.2 . First, we generate simulated samples using non-
overlapping bboxes from real datasets. These simulated
samples are then used to train a labeler ϕ. Subsequently,
we utilize the weights of ϕto initialize the teacher labeler
ϕtand the student labeler ϕs. Finally, we ﬁnetune the la-
belers ϕs, ϕtusing the Mean Teacher approach to generate
pseudo-labels in overlapping areas. The resulting labeler ϕt
is denoted as SAFormer. After obtaining the pseudo-labels,
we employ them for soft supervision of the 3DIS network.
3.2. Process to Generate SAFormer
We develop a novel pseudo-labeler called SAFormer, which
accurately predicts labels for overlapping areas, leading to
precise results in bbox-supervised 3DIS. Next, we will se-
quentially introduce the generation process.
3.2.1 Simulated Sample Generation
The abundant non-overlapping bboxes in ScanNetV2 [ 9]
with deﬁnite instance labels allow us to generate simulated
overlapping samples. As illustrated in Figure 3, we begin by
extracting real overlapping samples Oand non-overlapping
objects Pfrom real scenes. Subsequently, we conduct an
analysis of the class distribution and spatial distribution
within these real overlapping samples. To be more speciﬁc,
we determine which class pairs make up overlapping sam-
ples, counting the number nof samples for each class pair
and calculate the mean µand variance σof the distances be-
tween the center points of each class pair. After obtaining
the statistical data, we commence the simulation of the dis-
tribution. Firstly, we perform sampling of class pairs based
on the distribution of n. Assuming the sampled class pair
is denoted as (a, b), we then uniformly sample one object
point cloud for each of the classes aandbfrom the set P.
After obtaining these two point clouds, we perform gaus-
sian sampling based on the corresponding µandσto ob-
tain a distance d, representing the distance between the two
point clouds. Finally, for the sake of simplicity, we directly
translate one of the point clouds along the X or Y-axis by
the distance d. It is worth noting that, before performing
the distance translation, we align the center points of the
point cloud pairs.
To better maintain physical plausibility, we make scene
adjustments based on the following two principles [ 32]: 1)
gravity: objects should not ﬂoat in the air; 2) collision: these
two objects should not exhibit any collision. Speciﬁc de-
Category 
distributionCalculate 
distribution 
Spatial 
distribution Split
Split
No-overlapping objects P
Real overlapping samples O
 Simulatedistribution 
Gravity, collisionconstrainReal scenes
Add background noise points
Simulated samples S
Figure 3. The process of generating simulated samples. There
are numerous non-overlapping objects ( P) with deﬁnite instance
labels in real scenes. We can generate simulated samples ( S) based
on the distribution of real overlapping samples ( O) and the physi-
cal plausibility.
tails are covered in the supplementary materials. Another
purpose of designing the collision constraint is to verify
whether two objects can be matched when constructing sim-
ulated samples. Concretely, after multiple (with an upper
limit ofM) distance samplings and collision constraint cor-
rections, some object pairs may still fail to generate over-
lapping regions. Such pairs are subsequently excluded from
use. Finally, considering that real overlapping samples con-
tain background noise points, we add an appropriate num-
ber of floor points to represent the presence of background
noise points.
During the aforementioned process, we have obtained
simulated overlapping samples S. Currently, we utilize
these samples to train a labeler ϕ. The labeler ϕprimarily
consists of two components: a lightweight 3D-UNet based
on sparse convolution [ 14,15,22] and LGA. In next section,
we provide a detailed introduction of LGA.
3.2.2 Local-Global Aware Attention
As shown in Figure 4, LGA mainly contains local-structure
attention and global-context attention. Assuming that the
input point cloud consists of Npoints, with each point con-
taining position coordinates (x, y, z )and color information
(r, g, b ). First, we input the point cloud into a lightweight
3D-UNet to obtain point-level features F. Subsequently,
following SPFormer [ 36], we aggregate the point-level fea-
tures Finto superpoint-level features Fsupusing average
pooling. Next, we initialize two learnable queries Q1, Q2,
representing two foreground instances respectively. To bet-
ter model local structure, we separately employ the self-
attention layer and the feed-forward layer within the non-
overlapping areas of different instances. This approach en-
sures that each local region interacts with similar regions
belonging to the same instance, signiﬁcantly enhancing
the discriminative and representational capabilities of local
structures. Speciﬁcally, we concatenate Fsup, 1withQ1and
Fsup, 2withQ2to form Fv,1, Fv,2, and then input them sep-
20377
Superpoint-level FeaturesSuperpoint-level Features Superpoint-level Features Figure 4. The Local-Global Aware Attention. Two foreground
queries are input into local-structure attention and global-context
attention to generate corresponding masks. S1, S2represent non-
overlapping areas. S3represents overlapping areas.
arately into the self-attention layer, just as follows:
F′
v,i= Softmax(QKT
√
C)V, i= 1,2, (1)
where Q=Fv,iWq,K=Fv,iWk,V=Fv,iWv,
andWq, Wk, Wvdenote the linear transform matrices for
queries, keys, and values, respectively. Finally we input F′
v
into the feed-forward layer to obtain F′′
v.
After modeling the local structures within each non-
overlapping area S1, S2, and aggregating these structures
into a holistic representation through foreground queries,
we need to incorporate global information. The speciﬁc ap-
proach involves concatenating the features F′′
v,1, F′′
v,2, and
Fsup, 3. Then, we input this concatenated features into the
self-attention layer and the feed-forward layer. Through
this method, we can model relationships between non-
overlapping and overlapping areas, between the two fore-
ground instances, and aggregate these global relationships
intoQ1andQ2, respectively. Finally, to classify the over-
lapping areas, we obtain the masks Mins,1, Mins,2for the
two objects by calculating the dot product between Fsup, 3
andQ1,Q2. The ﬁnal mask is obtained through the Sig-
moid function followed by a threshold of 0.5:
Mi= Sigmoid( Mins,i)>0.5, i= 1,2. (2)
Since M1andM2represent two different foreground object
masks, for areas where both M1andM2are not activated,
we classify them as background areas. This approach nat-
urally helps the labeler ﬁlter out background points, which
is an improvement over Gapro [ 29], as Gapro overlooks the
presence of background points. Furthermore, to better as-
sist the labeler in learning uniﬁed knowledge for the same
class, we add a class prediction head.
For training on the simulated overlapping samples S,
since the instance labels are complete, we directly use the
shared losses from SPFormer [ 36] and Mask3D [ 34]:
Ltotal _sim=λ1Lcls+λ2Lbce+λ3Ldice, (3)where λ1, λ2, λ3are hyperparameters, Lclsis the cross-
entropy loss, Lbceis the binary cross-entropy loss, Ldice
is the dice loss [ 27].
3.2.3 Mean Teacher Approach
During the above process, the labeler ϕhas learned prior
knowledge about overlapping scenes through training on
the simulated samples. Subsequently, we used the pre-
trained weights as the initial weights for both the teacher
labeler ϕtand the student labeler ϕs. Then, we apply data
augmentation to the real overlapping samples, including
ﬂipping, jittering, elastic distortion, and so on. The origi-
nal samples are fed into the teacher labeler, while the aug-
mented samples are input into the student labeler. Since the
labels for non-overlapping areas in real samples are known,
we can directly supervise these areas. As for the overlap-
ping areas, to better leverage the predictions of the teacher
labeler, we select high-conﬁdence pseudo-labels for the
overlapping areas based on a ﬁxed threshold τ. The teacher
labeler updates its parameters using the EMA technique.
With this design, the teacher labeler can continuously up-
date pseudo-labels online and transfer knowledge to the stu-
dent. Simultaneously, the student labeler can employ EMA
to transmit the acquired knowledge back to the teacher. Go-
ing a step further, with the initialization weights obtained
through simulation, the teacher labeler gains the ability to
distinguish overlapping areas. It can generate higher qual-
ity pseudo-labels, accelerating the Mean Teacher’s training
speed. Finally, the well-trained teacher labeler ϕtis referred
to as SAFormer, which is used to generate ﬁnal pseudo-
labels for overlapping regions.
For ﬁnetuning on the real samples, we adopt a weakly
supervised approach. The speciﬁc approach can be divided
into two parts. First, for non-overlapping areas, where the
labels are known but only partial labels of the complete ob-
jects, we only supervise the non-overlapping areas.:
M′=QsFT
s,sup, 1∪2, (4)
where Qsrepresents the instance queries of the student la-
beler, 1∪2represents the union of S1andS2,
Lsup=λ2Lbce(M′, M′
gt) +λ3Ldice(M′, M′
gt). (5)
Next, for overlapping areas, we obtain high-conﬁdence
pseudo-labels based on a threshold τfrom the teacher la-
beler and solely supervise the overlapping areas that have
corresponding pseudo-labels:
M′′
ps=QtFT
t,sup, 3⊙(QtFT
t,sup, 3> τ), (6)
M′′=QsFT
s,sup, 3⊙(QtFT
t,sup, 3> τ), (7)
where ⊙represents hadamard product, Qtrepresents the
instance queries of the teacher labeler, M′′
psrepresents the
pseudo-labels of overlapping areas,
Lunsup =λ2Lbce(M′′, M′′
ps) +λ3Ldice(M′′, M′′
ps). (8)
20378
The total loss for real samples is:
Ltotal _real=λ1Lcls+Lsup+Lunsup . (9)
3.3. Training a 3DIS Network
Due to the fact that the instance labels for points within non-
overlapping bboxes and non-overlapping areas of overlap-
ping bboxes are deﬁnite, we can combine these determined
instance labels with the pseudo-labels obtained through
SAFormer. Then the combined labels are used to train a
3DIS network. It’s worth noting that since the predicted
pseudo-label values ∈[0,1], which reﬂect conﬁdence, em-
ploying a soft supervision is a better choice. Assuming
there are Kinstances, the pseudo masks M∈[0,1]K×N,
L′
bce=PK
i=1PN
j=1Lbce(Mpred,i,j , Mi,j)∗Mi,j
PK
i=1PN
j=1Mi,j, (10)
where Mpred represents the results predicted by the 3DIS
network. The total soft loss is:
Ltotal _soft=cλ1Lcls+cλ2L′
bce+cλ3Ldice+Lnet, (11)
wherecλ1,cλ2,cλ3are hyperparameters speciﬁc to different
3DIS networks, and Lnetare loss functions unique to dif-
ferent 3DIS networks.
4. Experiments
4.1. Experimental Setup
Datasets and metrics. We conduct our experiments on
ScanNetV2 [ 9] and S3DIS [ 2] datasets. ScanNetV2 in-
cludes 1,613 scenes with 18 instance categories. Among
them, 1,201 scenes are used for training, 312 scenes are
used for validation, and 100 scenes are used for test. S3DIS
is a large-scale indoor dataset collected from six different
areas, which contains 272 scenes with 13 instance cate-
gories. Following previous works [ 29,37], we train on Area
1, 2, 3, 4, 6 and evaluate on Area 5. AP@25 and AP@50
represent the average precision scores with IoU thresholds
25% and 50%, and mAP represents the average of all the
APs with IoU thresholds ranging from 50% to 95% with a
step size of 5%. On ScanNetV2, we report mAP, AP@50
and AP@25. Moreover, we also report the Box AP@50 and
AP@25 results following Gapro [ 29]. On S3DIS, we report
mAP and AP@50.
Implementation details. The whole method BSNet
is trained on a single RTX3090. As to the training setting
of the pseudo-labeler SAFormer, ﬁrst we train 100 epochs
on simulated samples with a batch size of 64, which takes
about 6 hours. Next, we ﬁntune 5 epochs on real samples of
ScannetV2 training set with a batch size of 64, which takes
about 90 minutes. During inference, it takes approximately
10 minutes to generate pseudo-labels for the entire training
set. As to S3DIS, it takes about 100 minutes for ﬁntuningTable 1. Comparison on ScanNetV2 validation set. %full indi-
cates the percentage of the current method’s performance com-
pared to its corresponding fully supervised method. ISBNet †
refers that we use the pseudo-labels generated by "Box2Mask [ 8]:
assign points to smaller box" to supervise ISBNet [ 30].
Method Sup. mAP %full AP@50 %full AP@25
Mask3D [ 34]
Mask55.2 - 73.7 - 83.5
PointGroup [ 19] 34.8 - 51.7 - 71.3
SSTNet [ 21] 49.4 - 64.3 - 74.0
ISBNet [ 30] 54.5 - 73.1 - 82.5
SPFormer [ 36] 56.3 - 73.9 - 82.9
CSC [ 17]Point15.9 28.8% 28.9 39.2% 49.6
PointContrast [ 43] 27.8 50.4% 47.1 63.9% 64.5
Box2Mask(stand-alone) [ 8]
Box39.1 - 59.7 - 71.8
ISBNet † 41.8 76.7% 64.8 88.6% -
WISGP [ 11] + PointGroup 31.3 89.9% 50.2 97.1% 64.9
WISGP + SSTNet 35.2 71.3% 56.9 88.5% 70.2
GaPro [ 29] + ISBNet 50.6 92.8% 69.1 94.5% 79.3
GaPro + SPFormer 51.1 90.8% 70.4 95.3% 79.9
Ours + ISBNet 52.8 96.9% 71.6 97.9% 82.6
Ours + SPFormer 53.3 94.7% 72.7 98.4% 83.4
with 5 epochs and 10 minutes to generate pseudo-labels.
Given that our pseudo-labeler only needs to be trained once
on the simulated samples when applied to multiple datasets,
so the more datasets we apply, the more efﬁcient the method
is. As to the backbone of SAFormer, we use a lightweight
3D-UNet based on sparse convolution [ 14,15,22] with 3
blocks and 32 media channels. At last, we tune the hyper-
parameters M, τ, λ 1, λ2, λ3as 8, 0.9, 2, 5, 2.
4.2. Comparison with state-of-the-art methods
ScanNetV2. As shown in Table 1, we compare our ap-
proach with existing state-of-the-art methods on the Scan-
NetV2 validation set. Attributed to the innovative con-
struction of simulated samples by SMT and the capability
of LGA to model local and global information, our pro-
posed SAFormer can generate higher-quality pseudo-labels
to supervise the 3DIS network. Consequently, our box-
supervised 3DIS method outperforms other methods by a
signiﬁcant margin in terms of mAP, AP@50 and AP@25.
It is worth emphasizing that our results can achieve 95% in
terms of mAP when compared to the corresponding fully
supervised methods. This signiﬁes a substantial improve-
ment over previous approaches, which typically achieves
only about 90% performance. To vividly illustrate the dif-
ferences between our method and others, we visualize the
qualitative results of pseudo-labels in Figure 5. From the
regions highlighted in yellow circles, it is evident that our
method can generate more accurate pseudo-labels for over-
lapping areas.
S3DIS. We evaluate our method on S3DIS using Area 5
in Table 2. Our proposed method achieves superior perfor-
mance compared to previous methods, with large margins
in both mAP and AP@50, demonstrating the effectiveness
20379
Input Box2Mask’s Ours Gapro’s GT labelsFigure 5. Qualitative results on ScanNetV2 training set. Our
approach produces highly accurate pseudo instance masks, partic-
ularly in overlapping areas (yellow circles).
Table 2. Comparison on S3DIS on Area 5. Box2Mask* repre-
sents the results of Box2Mask [ 8] reproduced by Gapro [ 29] on
the S3DIS dataset based on their public code.
Method Sup. mAP %full AP@50 %full
Mask3D [ 34]
Mask56.6 - 68.4 -
PointGroup [ 19] - - 57.8 -
SSTNet [ 21] 42.7 - 59.3 -
SoftGroup [ 21] 51.6 - 66.1 -
ISBNet [ 30] 54.0 - 65.8 -
Box2Mask* [ 8]
Box43.6 - 54.6 -
WISGP [ 11] + PointGroup 33.5 - 46.8 81.0%
WISGP + SSTNet 37.2 87.1% 51.0 86.0%
GaPro [ 29] + SoftGroup 47.0 91.1% 62.1 93.9%
GaPro + ISBNet 50.5 93.5% 61.2 93.0%
Ours + SoftGroup 51.4 99.6% 62.8 95.0%
Ours + ISBNet 53.0 98.1% 64.3 97.7%
and generalization of our method.
4.3. Ablation Study
The following experiments in Table 4are conducted on IS-
BNet on the validation set of ScanNetV2, while the others
are performed on the training set of ScanNetV2.
Comparison on pseudo-labels. Firstly, we use the met-
ric mAcc to evaluate the quality of pseudo-labels in over-
lapping areas. Assuming that the predicted pseudo-labels
of overlapping areas are P, the GT of overlapping areas are
Pgt, there are Noverlapping areas, there are Mipoints in
overlapping area i, and Iis the indicator function,
mAcc =1
NNX
i=1PMi
j=1I(Pi,j==Pgt
i,j)
Mi. (12)
The higher mAcc represents the better quality of pseudo-
labels. With the help of mAcc, we can explore the impact
of different techniques for handling overlapping areas.
As depicted in Table 3, setting B represents the current
state-of-the-art technique for handling overlapping areas,
and its performance is signiﬁcantly higher than setting A.
We compare our method SAFormer with these methods and
conduct an ablation study of each component in setting C.
In setting C0, attributed to the neural network’s strong ﬁt-
ting capability, even without utilizing our proposed SMT
and LGA, our base performance still surpasses the currentTable 3. Quality of pseudo-labels in overlapping areas. Base
refers to utilizing a 3D-UNet and a mask and classiﬁcation head.
LA, GA, MT, SSG represent Local-structure Attention, Global-
context Attention, Mean Teacher, Simulated Sample Generation.
Handling of overlapping areas mAcc
A: Box2Mask: assign points to smaller box 24.1
B: Gapro: GP classiﬁcation with superpoints 38.1
C0: Base 41.5
C1: Ours (LA) 48.1
C2: Ours (GA) 43.5
C3: Ours (LA + GA) 52.5
C4: Ours (LA + GA + MT) 55.3
C5: Ours (LA + GA + MT + SSG) 59.6
Table 4. Effect of our method’s components. Our pseudo-
labels: the pseudo-labels generated by our proposed pseudo-
labeler SAFormer. Soft loss: the soft loss proposed in Section 3.3.
Our pseudo-labels Soft loss mAP AP@50 AP@25
7 7 38.1 59.1 72.7
3 7 52.3 71.2 82.1
3 3 52.8 71.6 82.6
state-of-the-art method Gapro. In setting C1, we directly
train the labeler with a backbone and LA on the real scenes.
The results show an improvement of 10.0 in mAcc com-
pared to Gapro, indicating that deep neural networks can ac-
curately predict overlapping area labels through dedicated
local structure modeling and the accumulation of multiple
samples. In setting C2, we replace LA with GA, resulting
in a 5.4 increase in mAcc compared to Gapro. The results
suggest the importance of global information, particularly
the interaction between the two foreground instances and
between overlapping areas and non-overlapping areas. In
setting C3, we add the design of GA based on C1, resulting
in a 4.4 improvement in mAcc. From the results of C1, C2,
and C3, we can conclude that local structure modeling and
global relationship modeling complement each other. Good
local structures form the basis for modeling global relation-
ships, and modeling global relationships can better unleash
the potential of good local structures. In setting C4, to pro-
vide stable pseudo-labels for overlapping areas and facili-
tate information transfer between teacher and student label-
ers, we add MT. The improved performance in mAcc proves
its effectiveness. Finally, to help the labeler gain the ability
to distinguish overlapping areas, we add SSG in C5. This
enables the labeler to predict higher quality pseudo-labels
and achieve faster training speed, as shown in Table 7.
Effect of our method’s components. Table 4shows
3DIS results with different components. In the ﬁrst row, we
evaluate the approach of ignoring overlapping areas during
training and only using the determined regions as pseudo-
labels. The second row showcases the efﬁcacy of the
pseudo-labels produced by our proposed labeler SAFormer,
resulting in a 14.2 improvement in mAP. In the last row,
to validate the impact of the soft loss, we conduct a cor-
responding ablation experiment and achieve a performance
20380
Table 5. Effect of different pseudo-label utilization meth-
ods. Base refers that pseudo-labels are directly used to train the
3DIS network. Iterative self-training refers that updating pseudo-
labels ofﬂine after each training round and then using the updated
pseudo-labels to further optimize the labeler. After multiple itera-
tions, the latest pseudo-labels are used to train the 3DIS network.
Method mAcc
Base 52.5
Iterative self-training 52.6
Mean Teacher 55.3
Table 6. Effect of different steps in SSG. SD, GCC, ABP rep-
resent simulating distribution, gravity-collision constrain, adding
background points respectively.
SD GCC ABP mAcc
3 7 7 58.5
3 3 7 59.3
3 3 3 59.6
Figure 6. Qualitative visualization results of our SSG.
boost of 0.5 in mAP.
Effect of different pseudo-label utilization methods.
As shown in Table 5, we observe that iterative self-
training contributes minimally to performance improve-
ment, whereas Mean Teacher results in a 2.8 increase in
mAcc. The ﬁndings highlight that Mean Teacher can gener-
ate higher quality pseudo-labels by facilitating information
transfer between the student and teacher labeler.
Effect of different steps in SSG. Table 6illustrates that
as the simulated overlapping samples become more realis-
tic, the quality of pseudo-labels is getting better. It’s worth
noting that adding background points results in a 0.3 in-
crease in mAcc. This is partly because it makes the samples
more realistic. On the other hand, it is because our designed
mask activation using sigmoid function can naturally ﬁlter
out background points. In order to illustrate the generation
process more vividly, we visualize the qualitative results in
Figure 6. It is shown that the generated simulated samples
successfully combine the individual 3D shapes in a mean-
ingful way.
Effect of SSG. As shown in Table 7, with the assistance
of SSG, the labeler can predict higher quality pseudo-labels.
Moreover, owing to the labeler’s initialization with simu-
lated samples, the teacher labeler can furnish more stable
and accurate pseudo-labels in the early stages of training,
thereby expediting the overall training process.
Effect of a class head. Based on Table 8, it can be de-
duced that the incorporation of a class head helps the labeler
acquire uniﬁed representations for the same class, resultingTable 7. Effect of SSG to MT.
Setting Training time(h) mAcc
w/o SSG 40 55.3
w SSG 1.5 59.6Table 8. Effect of a class head.
Setting mAcc
w/o class head 59.2
w class head 59.6
Table 9. Comparison of parameters and training time. Trep-
resents the total training time, which includes the time to generate
pseudo-labels and the time to train 3DIS network with the pseudo-
labels.bPrepresents the pseudo-labeler parameters, Prepresents
the corresponding 3DIS network parameters, and %full denotes
the proportion of bPtoP.
Method T(h) bP(M) P(M) %full
Gapro + ISBNet 150 - 30.7 -
Gapro + SPFormer 80 - 17.6 -
Ours + ISBNet 72 2.4 30.7 7.8%
Ours + SPFormer 37 2.4 17.6 13.6%
in more precise pseudo-labels.
4.4. Parameters and Training Time Analysis
Table 9reports the parameters and the training time on
ScanNetV2 training set. For a fair comparison, the reported
training time is measured on the same device. Our pseudo-
labeler utilizes only about 10% of the corresponding 3DIS
network parameters, making it very lightweight. And in
terms of time, it is less than half of Gapro’s. This can be
attributed to different self-training ways and different ob-
jects to which self-training is applied. Gapro performs it-
erative self-training on pseudo-labeler and 3DIS network,
while our method performs Mean Teacher self-training only
on pseudo-labeler. Therefore, our method not only elimi-
nates the high time cost caused by repeated training of the
3DIS network, but also greatly alleviates the training time
of Mean Teacher through the design of SMT.
5. Conclusion
In this paper, we propose the Box-Supervised Simulation-
assisted Mean Teacher for 3D Instance Segmentation,
which devises a novel pseudo-labeler called SAFormer. To
the best of our knowledge, SAFormer is the ﬁrst labeler
incorporating the deep neural network and Mean Teacher
in this task, and innovatively constructs simulated samples
to facilitate training. Furthermore, the well-designed trans-
former decoder LGA effectively models local structures and
global relationships of point clouds. Extensive experiments
conducted on two widely used box-supervised 3D instance
segmentation benchmarks demonstrate the superior perfor-
mance of our method.
6. Acknowledgements
This work was partially supported by the Youth Innova-
tion Promotion Association CAS 2018166, Anhui Provin-
cial Natural Science Foundation (Grant 2308085QF222)
and National Defense Basic Scientiﬁc Research program
(JCKY2021601B013).
20381
References
[1]Nikita Araslanov and Stefan Roth. Self-supervised augmen-
tation consistency for adapting semantic segmentation. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 15384–15394, 2021. 3
[2]Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioan-
nis Brilakis, Martin Fischer, and Silvio Savarese. 3d seman-
tic parsing of large-scale indoor spaces. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 1534–1543, 2016. 2,6
[3]Shengcao Cao, Dhiraj Joshi, Liang-Yan Gui, and Yu-Xiong
Wang. Contrastive mean teacher for domain adaptive ob-
ject detectors. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 23839–
23848, 2023. 2,3
[4]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, Au-
gust 23–28, 2020, Proceedings, Part I 16 , pages 213–229.
Springer, 2020. 3
[5]Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and
Xinggang Wang. Hierarchical aggregation for 3d instance
segmentation. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 15467–15476,
2021. 2
[6]Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-
pixel classiﬁcation is not all you need for semantic segmen-
tation. Advances in Neural Information Processing Systems ,
34:17864–17875, 2021. 3
[7]Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-
der Kirillov, and Rohit Girdhar. Masked-attention mask
transformer for universal image segmentation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 1290–1299, 2022. 3
[8]Julian Chibane, Francis Engelmann, Tuan Anh Tran, and
Gerard Pons-Moll. Box2mask: Weakly supervised 3d se-
mantic instance segmentation using bounding boxes. In
European Conference on Computer Vision , pages 681–699.
Springer, 2022. 1,3,6,7
[9]Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nießner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 5828–5839, 2017. 2,4,6
[10] Jiacheng Deng, Chuxin Wang, Jiahao Lu, Jianfeng He,
Tianzhu Zhang, Jiyang Yu, and Zhe Zhang. Se-ornet:
Self-ensembling orientation-aware network for unsupervised
point cloud shape correspondence. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5364–5373, 2023. 2
[11] Heming Du, Xin Yu, Farookh Hussain, Mohammad Ali
Armin, Lars Petersson, and Weihao Li. Weakly-supervised
point cloud instance segmentation with geometric priors. In
Proceedings of the IEEE/CVF Winter Conference on Appli-
cations of Computer Vision , pages 4271–4280, 2023. 1,2,3,
6,7[12] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian
Leibe, and Matthias Nießner. 3d-mpa: Multi-proposal ag-
gregation for 3d semantic instance segmentation. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 9031–9040, 2020. 2
[13] Yixiao Ge, Dapeng Chen, and Hongsheng Li. Mutual
mean-teaching: Pseudo label reﬁnery for unsupervised do-
main adaptation on person re-identiﬁcation. arXiv preprint
arXiv:2001.01526 , 2020. 3
[14] Benjamin Graham and Laurens Van der Maaten. Sub-
manifold sparse convolutional networks. arXiv preprint
arXiv:1706.01307 , 2017. 4,6
[15] Benjamin Graham, Martin Engelcke, and Laurens Van
Der Maaten. 3d semantic segmentation with submani-
fold sparse convolutional networks. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 9224–9232, 2018. 4,6
[16] Jian Han, Ya-Li Li, and Shengjin Wang. Delving into proba-
bilistic uncertainty for unsupervised domain adaptive person
re-identiﬁcation. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence , pages 790–798, 2022. 3
[17] Ji Hou, Benjamin Graham, Matthias Nießner, and Saining
Xie. Exploring data-efﬁcient 3d scene understanding with
contrastive scene contexts. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 15587–15597, 2021. 3,6
[18] Lukas Hoyer, Dengxin Dai, and Luc Van Gool. Daformer:
Improving network architectures and training strategies for
domain-adaptive semantic segmentation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 9924–9935, 2022. 3
[19] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-
Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point
grouping for 3d instance segmentation. In Proceedings of
the IEEE/CVF conference on computer vision and Pattern
recognition , pages 4867–4876, 2020. 2,6,7
[20] Yu-Jhe Li, Xiaoliang Dai, Chih-Yao Ma, Yen-Cheng Liu,
Kan Chen, Bichen Wu, Zijian He, Kris Kitani, and Peter Va-
jda. Cross-domain adaptive teacher for object detection. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 7581–7590, 2022. 2,
3
[21] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and
Kui Jia. Instance segmentation in 3d scenes using semantic
superpoint tree networks. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 2783–
2792, 2021. 2,6,7
[22] Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen,
and Marianna Pensky. Sparse convolutional neural networks.
InProceedings of the IEEE conference on computer vision
and pattern recognition , pages 806–814, 2015. 4,6
[23] Shih-Hung Liu, Shang-Yi Yu, Shao-Chi Wu, Hwann-Tzong
Chen, and Tyng-Luh Liu. Learning gaussian instance seg-
mentation in point clouds. arXiv preprint arXiv:2007.09860 ,
2020. 2
[24] Ze Liu, Zheng Zhang, Yue Cao, Han Hu, and Xin Tong.
Group-free 3d object detection via transformers. In Proceed-
20382
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 2949–2958, 2021.
[25] Jiahao Lu, Jiacheng Deng, Chuxin Wang, Jianfeng He, and
Tianzhu Zhang. Query reﬁnement transformer for 3d in-
stance segmentation. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 18516–
18526, 2023. 1,3
[26] Peng Mi, Jianghang Lin, Yiyi Zhou, Yunhang Shen, Gen
Luo, Xiaoshuai Sun, Liujuan Cao, Rongrong Fu, Qiang Xu,
and Rongrong Ji. Active teacher for semi-supervised ob-
ject detection. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 14482–
14491, 2022. 3
[27] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.
V-net: Fully convolutional neural networks for volumetric
medical image segmentation. In 2016 fourth international
conference on 3D vision (3DV) , pages 565–571. Ieee, 2016.
5
[28] Ishan Misra, Rohit Girdhar, and Armand Joulin. An end-to-
end transformer model for 3d object detection. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 2906–2917, 2021.
[29] Tuan Duc Ngo, Binh-Son Hua, and Khoi Nguyen. Gapro:
Box-supervised 3d point cloud instance segmentation using
gaussian processes as pseudo labelers. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 17794–17803, 2023. 1,2,3,5,6,7
[30] Tuan Duc Ngo, Binh-Son Hua, and Khoi Nguyen. Isbnet: a
3d point cloud instance segmentation network with instance-
aware sampling and box-aware dynamic convolution. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 13550–13559, 2023. 1,2,6,
7
[31] Charles R Qi, Or Litany, Kaiming He, and Leonidas J
Guibas. Deep hough voting for 3d object detection in point
clouds. In proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 9277–9286, 2019.
[32] Yongming Rao, Benlin Liu, Yi Wei, Jiwen Lu, Cho-Jui
Hsieh, and Jie Zhou. Randomrooms: Unsupervised pre-
training from synthetic shapes and randomized layouts for
3d object detection. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 3283–3292,
2021. 4
[33] Danila Rukhovich, Anna Vorontsova, and Anton Konushin.
Fcaf3d: Fully convolutional anchor-free 3d object detection.
InEuropean Conference on Computer Vision , pages 477–
493. Springer, 2022.
[34] Jonas Schult, Francis Engelmann, Alexander Hermans, Or
Litany, Siyu Tang, and Bastian Leibe. Mask3d: Mask trans-
former for 3d semantic instance segmentation. In 2023
IEEE International Conference on Robotics and Automation
(ICRA) , pages 8216–8223. IEEE, 2023. 1,3,5,6,7
[35] Myung-Ok Shin, Gyu-Min Oh, Seong-Woo Kim, and Seung-
Woo Seo. Real-time and accurate segmentation of 3-d point
clouds based on gaussian process regression. IEEE Trans-
actions on Intelligent Transportation Systems , 18(12):3363–
3377, 2017. 1,2,3[36] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu.
Superpoint transformer for 3d scene instance segmentation.
InProceedings of the AAAI Conference on Artiﬁcial Intelli-
gence , pages 2393–2401, 2023. 1,3,4,5,6
[37] Thang Vu, Kookhoi Kim, Tung M Luu, Thanh Nguyen, and
Chang D Yoo. Softgroup for 3d instance segmentation on
point clouds. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 2708–
2717, 2022. 1,2,6
[38] Haiyang Wang, Shaocong Dong, Shaoshuai Shi, Aoxue Li,
Jianan Li, Zhenguo Li, Liwei Wang, et al. Cagroup3d: Class-
aware grouping for 3d object detection on point clouds.
Advances in Neural Information Processing Systems , 35:
29975–29988, 2022.
[39] Pei Wang, Zhaowei Cai, Hao Yang, Gurumurthy Swami-
nathan, Nuno Vasconcelos, Bernt Schiele, and Stefano
Soatto. Omni-detr: Omni-supervised object detection with
transformers. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 9367–
9376, 2022. 3
[40] Xinjiang Wang, Xingyi Yang, Shilong Zhang, Yijiang Li,
Litong Feng, Shijie Fang, Chengqi Lyu, Kai Chen, and
Wayne Zhang. Consistent-teacher: Towards reducing incon-
sistent pseudo-targets in semi-supervised object detection. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 3240–3249, 2023. 3
[41] Yuting Wang, Velibor Ilic, Jiatong Li, Branislav Kisa ˇcanin,
and Vladimir Pavlovic. Alwod: Active learning for weakly-
supervised object detection. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 6459–
6469, 2023. 3
[42] Yizheng Wu, Min Shi, Shuaiyuan Du, Hao Lu, Zhiguo Cao,
and Weicai Zhong. 3d instances as 1d kernels. In Computer
Vision–ECCV 2022: 17th European Conference, Tel Aviv, Is-
rael, October 23–27, 2022, Proceedings, Part XXIX , pages
235–252. Springer, 2022. 2
[43] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas
Guibas, and Or Litany. Pointcontrast: Unsupervised pre-
training for 3d point cloud understanding. In Computer
Vision–ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part III 16 , pages
574–591. Springer, 2020. 3,6
[44] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan
Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to-
end semi-supervised object detection with soft teacher. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3060–3069, 2021. 3
[45] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen
Wang, Andrew Markham, and Niki Trigoni. Learning ob-
ject bounding boxes for 3d instance segmentation on point
clouds. Advances in neural information processing systems ,
32, 2019. 2
[46] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J
Guibas. Gspn: Generative shape proposal network for 3d
instance segmentation in point cloud. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3947–3956, 2019. 2
20383
[47] Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang,
and Fang Wen. Prototypical pseudo label denoising and tar-
get structure learning for domain adaptive semantic segmen-
tation. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition , pages 12414–12424,
2021. 3
[48] Kecheng Zheng, Cuiling Lan, Wenjun Zeng, Zhizheng
Zhang, and Zheng-Jun Zha. Exploiting sample uncertainty
for domain adaptive person re-identiﬁcation. In Proceed-
ings of the AAAI Conference on Artiﬁcial Intelligence , pages
3538–3546, 2021. 2,3
[49] Yu Zheng, Yueqi Duan, Jiwen Lu, Jie Zhou, and Qi Tian. Hy-
perdet3d: Learning a scene-conditioned 3d object detector.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 5585–5594, 2022.
[50] Min Zhong, Xinghao Chen, Xiaokang Chen, Gang Zeng, and
Yunhe Wang. Maskgroup: Hierarchical point grouping and
masking for 3d instance segmentation. In 2022 IEEE Inter-
national Conference on Multimedia and Expo (ICME) , pages
1–6. IEEE, 2022. 2
20384
