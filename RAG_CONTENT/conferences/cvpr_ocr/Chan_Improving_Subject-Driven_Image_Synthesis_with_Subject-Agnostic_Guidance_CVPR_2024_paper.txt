Improving Subject-Driven Image Synthesis with Subject-Agnostic Guidance
Kelvin C.K. Chan Yang Zhao Xuhui Jia Ming-Hsuan Yang Huisheng Wang
Google
S*, Monet style w/ SAG 
S*, swimming in front 
of EiÔ¨Äel T ower, V an  
Gogh starry night style 
w/o SAG 
S* in a b asket , 
at a beach 
S* w ith a cloudy night 
sky and a m oon 
S*, P ixar mov ie 
Reference 
Figure 1. Addressing Content Ignorance. Given user-provided subject images, a part of the content speciÔ¨Åed in the text prompt (high-
lighted in blue ) are overlooked. Our Subject-Agnostic Guidance (SAG) aligns the output more closely with both the target subject and text
prompt. Here Sdenotes a pseudo-word, with its text embedding replaced by a learnable subject embedding.
Abstract
In subject-driven text-to-image synthesis, the synthesis
process tends to be heavily inÔ¨Çuenced by the reference im-
ages provided by users, often overlooking crucial attributes
detailed in the text prompt. In this work, we propose
Subject-Agnostic Guidance (SAG) , a simple yet effective
solution to remedy the problem. We show that through
constructing a subject-agnostic condition and applying our
proposed dual classiÔ¨Åer-free guidance, one could obtain
outputs consistent with both the given subject and input text
prompts. We validate the efÔ¨Åcacy of our approach through
both optimization-based and encoder-based methods. Ad-
ditionally, we demonstrate its applicability in second-order
customization methods, where an encoder-based model is
Ô¨Åne-tuned with DreamBooth. Our approach is conceptually
simple and requires only minimal code modiÔ¨Åcations, but
leads to substantial quality improvements, as evidenced by
our evaluations and user studies.1. Introduction
Subject-driven text-to-image synthesis focuses on generat-
ing diverse image samples, conditioned on user-given text
descriptions and subject images. This domain has witnessed
a surge of interest and signiÔ¨Åcant advancements in recent
years. Optimization-based methods [16, 37, 41] tackle the
problem by overÔ¨Åtting pre-trained text-to-image synthesis
models [36, 38] and text tokens to the given subject. Re-
cently, encoder-based approaches [10, 24, 49] propose to
train auxiliary encoders to generate subject embeddings, by-
passing the necessity of per-subject optimization.
In the aforementioned approaches, both the embeddings
and networks are intentionally tailored to closely Ô¨Åt the tar-
get subject. As a consequence, these learnable conditions
tend to dominate the synthesis process, often obscuring the
attributes speciÔ¨Åed in the text prompt. For instance, as
shown in Fig. 1, when employing S1alongside the style
1Sdenotes a pseudo-word, where its embedding is substituted by a
learnable subject embedding.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
6733
description Monet style , the desired style is not appro-
priately synthesized. Such observations underscore that the
network struggles to prioritize key content in the existence
of learnable components. To address the content ignorance
issue, existing solutions modify the training process through
additional regularization [37, 49], leading to improved per-
formance.
In this work, we present Subject-Agnostic Guidance
(SAG) , an approach that diverges from traditional method-
ologies. Our strategy emphasizes attending to subject-
agnostic attributes by diminishing the inÔ¨Çuence of subject-
speciÔ¨Åc attributes, accomplished using classiÔ¨Åer-free guid-
ance. Differing from standard classiÔ¨Åer-free guidance [19],
our method incorporates a subject-agnostic condition2.
Subsequently, our proposed Dual ClassiÔ¨Åer-Free Guidance
(DCFG) is employed to enhance attention directed towards
subject-agnostic attributes. Crucially, motivated by the ob-
servation that structures are constructed during early iter-
ations [12, 22], we temporarily replace the subject-aware
condition with a subject-agnostic condition at the begin-
ning of the iteration process. Following the construction
of coarse image structures, the original subject-aware con-
dition is reintroduced to reÔ¨Åne customized details.
Our SAG is elegant in both design and implementa-
tion, seamlessly blending with existing methods. We show-
case the efÔ¨Åcacy of SAG using both optimization-based
and encoder-based approaches. Furthermore, we delve
into its applicability in second-order customization, with
an encoder-based model Ô¨Åne-tuned via DreamBooth [37].
Qualitative and quantitative evaluations as well as user feed-
back verify our robustness, succinctness, and versatility.
In the evolving realm of subject-driven text-to-image
synthesis, challenges have emerged due to over-tailored
embeddings and networks. These often inherit crucial at-
tributes. While existing solutions modify training to ad-
dress these issues, our novel Subject-Agnostic Guidance
(SAG) provides a distinct approach. Seamlessly integrating
with prevalent methods, SAG emphasizes a more balanced
synthesis process. Its effectiveness is demonstrated through
various methodologies and supported by user feedback.
2. Related Work
Diffusion Model for Text-To-Image Synthesis. Typically,
given natural language descriptions, a text encoder such as
CLIP [33] or T5 [34] is employed to derive the text embed-
ding. This embedding is then fed into the diffusion model
for the generation phase. Earlier approaches [35] operated
directly within the high-resolution image space for gener-
ation. While these methods yielded promising outcomes,
the direct iteration in high-resolution space poses signiÔ¨Å-
cant computational challenges. In light of these constraints,
2The construction of this condition varies based on the speciÔ¨Åc cus-
tomization approach used.considerable efforts have been devoted to enhancing gener-
ation efÔ¨Åciency. For instance, Imagen [38] employs a multi-
stage diffusion model. It starts by synthesizing a 6464
resolution image based on the input text prompt and sub-
sequently employs a series of super-resolution modules to
increase the resolution to 10241024 . BeneÔ¨Åting from op-
timized architectures in the super-resolution stages, this cas-
caded approach considerably reduces computational over-
head compared to direct high-resolution image synthesis.
Latent Diffusion [36] transitions the generation process to a
low-resolution feature space to improve efÔ¨Åciency. Initially,
a V AE [26] or VQGAN [15, 45] is pre-trained. During train-
ing, images are encoded into low-resolution features using
the pre-trained encoder, and the diffusion model aims to re-
construct these encoded features. In the inference stage, the
trained diffusion model produces a feature which is subse-
quently decoded using the pre-trained module to render the
Ô¨Ånal output image.
Subject-Driven Image Synthesis. Subject-driven text-to-
image synthesis [1, 8, 9, 18, 21, 27‚Äì29, 31, 40, 43, 47] is
a sub-branch of text-to-image synthesis [3, 6, 14, 25, 36,
38, 50] with an additional requirement that the primary at-
tributes in the output aligns with the subjects provided by
the user. Existing research [16, 17, 37, 44] has demon-
strated that subject information can be encoded as a subject-
aware embedding through test-time optimization, given sev-
eral reference images. For instance, Textual Inversion [16]
leverages pre-trained synthesis networks and optimizes a
special token while keeping the network static. Dream-
Booth [37] shares a similar premise but also Ô¨Åne-tunes the
network to enhance subject consistency. To bypass test-
time optimization, which restricts instant feedback, recent
studies [10, 24] advocate the use of an encoder to encap-
sulate subject information. However, despite advancements
in both quality and speed, the encoded subject information
often dominates the synthesis process, resulting in inade-
quately capture of subject information . In this study, we
introduce Subject-Agnostic Guidance (SAG) to rectify this
challenge. Our SAG focuses on enhancing subject-agnostic
attributes, diminishing the inÔ¨Çuence of subject-speciÔ¨Åc ele-
ments through our dual classiÔ¨Åer-free guidance. We illus-
trate that SAG not only enhances consistency to the input
captions but also maintains Ô¨Ådelity to the subject.
3. Methodology
In this work, we introduce an intuitive and effective method
to enhance content alignment. We Ô¨Årst provide the back-
ground for our approach, followed by the discussion of our
method ‚Äì Subject-Agnostic Guidance .
6734
S*, C u bism style. 
Weak CFG 
Null CFG 
(n ull embedding) A dog , C u bism style. Figure 2. Overview of SAG. Given a subject-aware embed-
ding, we Ô¨Årst construct a subject-agnostic embedding. These em-
beddings are subsequently used in our dual classiÔ¨Åer-free guid-
ance (DCFG), which consists of weak classiÔ¨Åer-free guidance and
null-classiÔ¨Åer-free guidance. Null CFG adopts a constant weight
(Eqn. 2) and Weak CFG adopts a variable weight (Eqn. 3).
3.1. Preliminaries
3.1.1 Diffusion Model
The diffusion process transforms a data distribution to a
Gaussian noise distribution by iteratively adding noise. Dif-
fusion model is a class of generative models that invert
the diffusion process through iterative denoising. Extended
from the original unconditional model [20], recent works
demonstrate huge success by conditioning diffusion models
on various modalities, including text [7, 23, 39], segmenta-
tion [2, 22, 32], and many more [30, 42, 48].
Letx0be the input image, and cbe the condition. During
training, a noisy image xtis obtained by adding Gaussian
noise ttox0. The network is trained to predict the added
noise, given the noisy image and condition as input. It is
generally optimized with a single denoising objective:
Ld=jj(xt;c) tjj2
2; (1)
where tthe noise added to the input image, and (xt;c)
corresponds to the noise estimated by the network. Here
xtandcrefer to the noisy image and condition, re-
spectively. During inference, the process starts with a
pure Gaussian noise xT0, and the trained network is iter-
atively applied to obtain a series of intermediate outputs
fxT0 1;xT0 2;;x0g, where x0is the Ô¨Ånal output.
3.1.2 ClassiÔ¨Åer-Free Guidance
Similar to classiÔ¨Åer guidance [13], classiÔ¨Åer-free guidance
is designed to trade between image quality and diversity,
but without the need of a classiÔ¨Åer. It is widely adopted in
existing works [46, 51].
During training, an unconditional diffusion model is
jointly trained by randomly replacing the input condition
cby a null condition . Once trained, during each iterationt, a weighted sum of the conditional output and the uncon-
ditional output is computed:
~t= (1 + w)(xt;c) w(xt;): (2)
In general, a larger wproduces better quality, whereas a
smaller wyields greater diversity.
3.2. Subject-Agnostic Guidance
In this section, we introduce the concept of Subject-
Agnostic Guidance (SAG) . The essence of SAG is anchored
in formulating a subject-agnostic embedding based on the
inputs provided by users. The embedding is then used in our
dual classiÔ¨Åer-free guidance (DCFG) in generating outputs
that align with both the subject and text prompt. We delve
into the details of constructing subject-agnostic embeddings
in Sec. 3.2.1, and discuss our dual classiÔ¨Åer-free guidance
in Sec. 3.2.2.
3.2.1 Subject-Agnostic Embeddings
The construction of subject-agnostic embeddings depends
on the choice of methods. Existing approaches generally
fall into two categories: Learnable Text Token andSepa-
rate Subject Embedding . In this section, we discuss the
construction of subject-agnostic embeddings in these two
approaches.
Learnable Text Token. Given images of a reference sub-
ject, the learnable text token approach derives a token em-
bedding that captures the identity of the subject, either
through Ô¨Åne-tuning [16, 47] or by using an encoder [1, 49].
The resultant token embedding, combined with the token
embedding of the text description, is processed by text en-
coders such as CLIP [33] and T5 [34] to produce a subject-
aware embedding.
To construct a subject-agnostic embedding, we replace
the derived token embedding with one from a general de-
scription of the subject. This strategy ensures that the syn-
thesis process is not dominated by any adaptable compo-
nents, thereby allowing the model to focus attention on the
attributes speciÔ¨Åed in the text prompt.
Letcbe the text condition containing the learnable token
S. We deÔ¨Åne a subject-agnostic condition c0by replacing
the token Sby a generic descriptor. For example, assuming
the target subject is a dog and
c=A pencil sketch of S
we construct c0as
c0=A pencil sketch of a dog
The generic descriptor is chosen as a noun describing the
subject.
6735
Separate Subject Embedding. Instead of encoding the
subject identity to a learnable text token, the separate sub-
ject embedding approach [10, 24] adopts an independent
embedding. This embedding is then integrated into the net-
work via auxiliary operations. For instance, Jia et al. [24]
employ the CLIP image encoder to encapsulate the subject
information into an embedding, which is then injected to
Imagen [38] using cross attention.
To construct the subject-agnostic embedding, we opt for
a direct method ‚Äì setting both the subject embedding and its
corresponding attention mask to zero. This disables atten-
tion to the subject, directing focus towards subject-agnostic
information.
3.2.2 Dual ClassiÔ¨Åer-Free Guidance
In this section, we introduce the Dual ClassiÔ¨Åer-Free Guid-
ance (DCFG) , designed primarily to address the issue of
content ignorance by attenuating the subject-aware condi-
tion. Our DCFG requires no modiÔ¨Åcations of the training
process. It simply requires the application of an additional
classiÔ¨Åer-free guidance using the subject-aware condition c
and the subject-agnostic condition c0. The derived feature
is subsequently merged with the null condition within a
conventional classiÔ¨Åer-free guidance.
Weak ClassiÔ¨Åer-Free Guidance. Given the subject-aware
condition cand the subject-agnostic condition c0, we Ô¨Årst
perform classiÔ¨Åer-free guidance using candc0. Incorporat-
ingc0into the synthesis process directs the generation to-
wards subject-agnostic content, representing a weaker ver-
sion of the desired generation. When subject information is
absent, the model more effectively creates the correct out-
line and structure, generating outputs that align with both
the subject and text description.
Differing from the conventional classiÔ¨Åer-free guidance,
where the guidance weight woften remains constant dur-
ing the denoising process, we implement a time-varying
scheme to enhance performance. Building on the obser-
vation that earlier iterations emphasize structure construc-
tion [12, 22], we highlight the subject-agnostic condition
during the initial phases. SpeciÔ¨Åcally, we adopt a time-
varying weighting strategy, suppressing subject information
in the early stages:
t= (1 + wt)(xt;c) wt(xt;c0); (3)
where wtdenotes the guidance weight, similar to win
Eqn. 2. Since a larger wtcorresponds to a larger contribu-
tion from c,wtis devised as a non-increasing function with
respect to the iteration t. In this work, we Ô¨Ånd that a simple
piecewise constant scheme sufÔ¨Åces to produce promising
results:
wt=(
r if0tT;
 1 ifT < t1:(4)Here 0T1andr  1are pre-determined constants,
which will be ablated in Sec. 5. Essentially, in the early
stages ( i.e., when t1), we use solely the subject-agnostic
condition to establish the structure and outline of the out-
put. The subject information is integrated in the subsequent
stages.
Null ClassiÔ¨Åer-Free Guidance. The null classiÔ¨Åer-free
guidance is identical to the conventional classiÔ¨Åer-free guid-
ance, leveraging the null condition to encourage diversity.
We adopt a constant guidance weight throughout itera-
tions. SpeciÔ¨Åcally, the output tof the weak-classiÔ¨Åer-free
guidance is used in place of (xt;c)in the conventional
classiÔ¨Åer-free guidance (Eqn. 2):
~t= (1 + w)t w(xt;): (5)
4. Experiments
To validate the efÔ¨Åcacy of SAG, we conduct experi-
ments across multiple approaches, namely Textual In-
version [16] (optimization-based), ELITE [49] (encoder-
based), SuTI [10] (encoder-based), and DreamSuTI [10]
(second-order).
4.1. ELITE
First, we examine the performance improvement when ap-
plying SAG to ELITE [49]. In this study, we simplify its
architecture by using only the global mapping branch. The
settings are as follows:
Training. To promote the learning of subject informa-
tion, we create a domain-speciÔ¨Åc ( e.g., animals) text-image
dataset where the text caption incorporates the specialized
token. SpeciÔ¨Åcally, we gather images from a pre-deÔ¨Åned
category and employ straightforward templates such as
A photo of Sfor the corresponding captions. During
training, the token corresponding to Sis substituted with
the output of the encoder. The condition is subsequently fed
into the text encoder.
As discussed in concurrent work [24], text prompts gen-
erated using templates and captioning models [11] have in-
herent limits to their diversity. Moreover, training within
narrow domains may harm generation diversity. To coun-
teract this, we employ a general-domain dataset containing
detailed text descriptions for regularization. Training on a
broad array of text captions ensures the model retains its
text-understanding abilities.
During the training phase, the domain-speciÔ¨Åc and
general-domain datasets are sampled with probabilities
p1and (1 p), respectively. Given that the general-
domain dataset serves primarily for regularization, we al-
locate a higher value to p, greater than 0.5, emphasizing
subject encoding.
6736
Reference Textual Inversion DreamBooth ELITE ELITE-SAG (ours) 
S* in front of S tatue of Liberty, sunset, U k iyo-e 
Stable Diffusion 
(Text-to-Image) 
S* in Hong K ong, sunset 
S* in a garden, Georges S eurat pointillism style 
S* next to T okyo tower, oil painting 
Figure 3. SAG on ELITE [49]. Our ELITE-SAG produces outputs that are more faithful to text prompts while still preserving subject
identity. For Stable Diffusion, we generate pure text-to-image results by substituting ‚Äú S‚Äù with ‚Äú A dog ‚Äù or ‚ÄúA cat ‚Äù.
Since the subject-agnostic condition c0is also natural
language, no modiÔ¨Åcation to the original denoising objec-
tive (Eqn. 1) is needed. Additionally, we adopt a regulariza-
tion to the learnable token [49] by constraining its `2-norm.
The effective training loss is:
L=Ld+jjsjj2; (6)
where sdenotes the output of the subject encoder. The re-
maining part of the training is identical to the training of
conventional text-to-image networks.
Inference. For each input image, we use the encoder to map
the target subject into a text token. This learnable token is
then combined with the text description to form the input
condition c. The subject-agnostic condition is c0is then
constructed following the process discussed in Sec. 3.2.1.
Starting from random Gaussian noise xT, the Ô¨Åne-tuned
network iteratively denoises the intermediate outputs. In-stead of applying the conventional classiÔ¨Åer-free guidance,
our SAG is employed.
Implementation. We adopt the pre-trained Stable Diffu-
sion [36] as the synthesis network, which uses CLIP [33]
as the text encoder. For the subject encoder, we use the
CLIP image encoder and a three-layer MLP to obtain the
learnable token. During training, only the cross-attention
layers in Stable Diffusion and the MLP are trained, all other
weights are being Ô¨Åxed.
We use an internal text-image dataset for training. To
construct the domain-speciÔ¨Åc dataset, we extract images
containing dogs andcats from the meta-dataset. The re-
maining part is used as our general-domain dataset. The
dataset mixing ratio is 0:1. The proposed method is imple-
mented in JAX [4]. The detailed experimental settings will
be discussed in the supplementary material.
Comparison. We compare our modiÔ¨Åed model, ELITE-
6737
References w/o SAG w/ SAG References A S* at a beach. 
w/o SAG w/ SAG A S* in front of EiÔ¨Äel T ower. 
Figure 4. SAG on Textual Inversion [16]. Our SAG improves text alignment without sacriÔ¨Åcing the identity of the subject.
Table 1. Quantitative Comparison . Our ELITE-SAG yields im-
proved performance in both text and subject alignment.
Methods CLIP-T"CLIP-I"DINO"
DreamBooth [37] 0.315 0.785 0.651
Textual Inversion [16] 0.339 0.751 0.571
ELITE [49] 0.342 0.751 0.586
ELITE-SAG (ours) 0.344 0.790 0.671
Table 2. User Study . Across all three compared methods, the
majority of raters favor the results produced by our approach.
% Prefer Ours Subject Align. Text Align. Quality
DreamBooth [37] 52% 68% 60%
Textual Inversion [16] 64% 76% 84%
ELITE [49] 56% 80% 76%
SAG, with three existing works: DreamBooth [37], Textual
Inversion [16], and ELITE [49]. In this section, we assume
the existence of only one reference image. As illustrated in
Fig. 3, while Stable Diffusion exhibits high text alignment,
the compared methods often fall short in generating results
faithful to text prompts in the presence of additional sub-
ject images. In contrast, with our SAG, outputs adhering
to both text captions and reference subjects are consistently
generated.
We also conduct a quantitative comparison as presented
in Table 1, utilizing CLIP [33] and DINO [5] scores. Specif-
ically, the image feature similarities of CLIP [33] and DINO
underscore that SAG enhances subject Ô¨Ådelity, while the
text feature similarity indicates that SAG improves text
alignment. Furthermore, our user study depicted in Table 2
reveals that more than half of the raters prefer our method
when compared to the aforementioned methods, thereby
corroborating the effectiveness of SAG.
4.2. Textual Inversion
Textual Inversion [16] is an optimization-based method for
customization. For each given subject, Textual Inversion
learns a text token to represent the subject. As discussed in
Sec. 3.2.1, the subject-agnostic embedding is generated by
replacing the learned special token by a generic description.Then, the conventional CFG is replaced by our SAG. The
remaining generation pipeline remains unchanged.
As illustrated in Fig. 4, the absence of SAG leads to gen-
eration dominated by the optimized text token, resulting in
suboptimal text alignment. Conversely, the incorporation of
SAG enables the model to produce outputs that align more
closely with the text description, while preserving the iden-
tity of the subject.
4.3. SuTI
Unlike ELITE, which encodes subject information into a
text token, SuTI [10] employs an encoder-based approach
that leverages a distinct subject embedding. This embed-
ding is then fed to the generation network through indepen-
dent cross-attention layers. As discussed in Sec. 3.2.1, the
subject-agnostic condition, denoted as c0, is simply con-
structed by setting the subject embedding to zero.
As illustrated in Fig. 5, without SAG, the model suc-
cessfully preserves the identity of the individual provided in
the reference images, yet the text alignment is inadequate.
SpeciÔ¨Åcally, the styles are unsatisfactory across all outputs.
In contrast, employing SAG and suppressing the subject
information during initial iterations signiÔ¨Åcantly enhances
text alignment. Consequently, the outputs exhibit both high
identity preservation and improved text alignment.
4.4. DreamSuTI
DreamSuTI [10] is a second-order method that Ô¨Åne-tunes
SuTI using DreamBooth [37] for compositional customiza-
tion. In this section, we Ô¨Åne-tune SuTI with a provided style
image to achieve simultaneous customization of style and
subject. The subject-agnostic embedding is generated using
the same method as in SuTI.
As depicted in Fig. 6, in the presence of subject images,
the outputs are dominated by the subject, resulting in a lack
of style Ô¨Ådelity. In contrast, when applying SAG, the sub-
ject is suppressed during the early stages of generation, ef-
fectively leading to enhanced style generation.
6738
Prompt w/o SAG w/ SAG 
P ixel art of face Ô¨Ågures, 
attractive faces, edgy, cool, 
extreme detail, t iny pixels, 
very coherent, v ibrant colors Prompt w/o SAG w/ SAG A bronze face, looks like 
bronze statue or 
bronze Ô¨Ågure, Ô¨Åne details. 
w/o SAG w/ SAG Beaut iful steampunk face 
wearing mask, 
d & d, f antasy, intricate, 
e legant, digital painting, 
matte, sharp focus, 
illustration, hearthstone, 
gradient background, hdr 
8k. 
Prompt w/o SAG Prompt w/o SAG w/ SAG P ortrait photography of a 
f ace, city, 35mm, night time, 
blue green, pink and gold, 
heavy bokeh. 
w/ SAG 
Figure 5. SAG on SuTI [10]. When applying SAG on SuTI, the subject is discarded during initial iterations, yielding outputs with
markedly improved text alignment. Reference images are not provided to protect privacy.
References w/o SAG 
 w/ SAG 
A beer can  in watercolor painting  style References References w/o SAG w/ SAG A v ase  in k id line dra wing  style 
w/o SAG w/ SAG A berry bowl  in 3D rendering  style 
References w/o SAG References w/o SAG w/ SAG A grey sloth plushie  in watercolor painting  style 
w/ SAG 
Figure 6. SAG on DreamSuTI [10]. Even after Ô¨Åne-tuning with DreamBooth to adapt to the speciÔ¨Åed style, the generated results tend
to be dominated by the subjects, leading to an inadequate style-alignment. Our SAG addresses this issue by diminishing the inÔ¨Çuence of
subjects, thereby ensuring outputs that are well-aligned with both the text, subject, and style.
5. Ablations
Guidance Timing. The hyper-parameter Tplays an im-
portant role in controlling the contribution of the subject
embedding. An illustration employing DreamSuTI is pro-
vided in Fig. 7. With r= 0, adopting a smaller Tresults
in a stronger suppression of the subject embedding, thereby
promoting a better text-alignment ( i.e., style-alignment in
this example). A gradual increment in Tfacilitates a transi-
tion from style alignment to subject alignment.Guidance Weight. While a default value of r= 0(i.e., em-
ploying only the subject-aware condition in later iterations)
performs well generally, decreasing rfacilitates the utiliza-
tion of the subject-agnostic condition in subsequent itera-
tions, thereby further enhancing content faithfulness. As
depicted in Fig. 8, the inclusion of subject-agnostic condi-
tions signiÔ¨Åcantly improves the style alignment of Dream-
SuTI. Since no re-training is required, the values of Tandr
can be dynamically adjusted based on user preference.
6739
References T = 1.0 T = 0.99 T = 0.95 T = 0.9 T = 0.85 T = 0.0 
Subject 
Alignment Style 
Alignment 
Figure 7. Guidance Timing. As an example, when Ô¨Åne-tuning SuTI [10] to a given style using DreamBooth [37], our SAG facilitates a
transition from subject-centric alignment to style-centric alignment. Here r= 0is used.
References r = 0.0 r = -0.2 r = -0.4 r = -0.6 r = -0.8 r = -1.0 
Subject 
Alignment Style 
Alignment 
Figure 8. Guidance Weight. The guidance weight rcan be leveraged to enhance content faithfulness further. For instance, lowering r
results in improved style alignment in DreamSuTI. Here T= 0:9is used.
6. Limitation and Societal Impact
Limitation. While our SAG signiÔ¨Åcantly enhances con-
tent alignment compared to existing methods, the quality
of outputs is inherently constrained by the underlying gen-
eration model. Hence, it may still exhibit suboptimal per-
formance for uncommon content that challenges the gener-
ation model. However, this limitation can be mitigated by
incorporating a more robust synthesis network, a direction
we aim to explore in our future work.
Societal Impact. This project targets at improving content
alignment in customized synthesis, which holds the poten-
tial for misuse by malicious entities aiming to mislead the
public. Future investigations in this domain should duly
consider these ethical implications. Moreover, ensuing ef-
forts to develop mechanisms for detecting images generated
by such models emerge as a critical avenue to foster the safeadvancement of generative models.
7. Conclusion
Subject-driven text-to-image synthesis has witnessed no-
table progress in recent years. However, overcoming the
problem of content ignorance remains a signiÔ¨Åcant chal-
lenge. As shown in this work, this problem signiÔ¨Åcantly
limits the diversity of the generation. Rather than introduc-
ing complex modules, we propose a straightforward yet ef-
fective method to address this issue. Our Subject-Agnostic
Guidance demonstrates how a balance between content con-
sistency and subject Ô¨Ådelity can be achieved using a subject-
agnostic condition. The proposed method enables users to
generate customized and diverse scenes without modifying
the training process, making it adaptable across various ex-
isting approaches.
6740
References
[1] Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel
Cohen-Or, Ariel Shamir, and Amit H Bermano. Domain-
agnostic tuning-encoder for fast personalization of text-to-
image models. arXiv preprint arXiv:2307.06925 , 2023. 2,
3
[2] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.
MultiDiffusion: Fusing diffusion paths for controlled image
generation. In ICML , 2023. 3
[3] James Betker, Gabriel Goh, Li Jing, Brooks Tim, Jianfeng
Wan, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,
Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu,
and Yunxin Jiao. Improving image generation with better
captions. Technical Report , 2023. 2
[4] James Bradbury, Roy Frostig, Peter Hawkins,
Matthew James Johnson, Chris Leary, Dougal Maclau-
rin, George Necula, Adam Paszke, Jake VanderPlas, Skye
Wanderman-Milne, and Qiao Zhang. JAX: composable
transformations of Python+NumPy programs, 2018. 5
[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ¬¥e J¬¥egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In
ICCV , 2021. 6
[6] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,
Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-
phy, William T Freeman, Michael Rubinstein, et al. Muse:
Text-to-image generation via masked generative transform-
ers.arXiv preprint arXiv:2301.00704 , 2023. 2
[7] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and
Daniel Cohen-Or. Attend-and-Excite: Attention-based se-
mantic guidance for text-to-image diffusion models. In SIG-
GRAPH , 2023. 3
[8] Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan,
Yuwei Zhou, and Wenwu Zhu. DisenBooth: Identity-
preserving disentangled tuning for subject-driven text-to-
image generation. arXiv preprint arXiv:2305.03374 , 2023.
2
[9] Li Chen, Mengyi Zhao, Yiheng Liu, Mingxu Ding,
Yangyang Song, Shizun Wang, Xu Wang, Hao Yang, Jing
Liu, Kang Du, et al. PhotoVerse: Tuning-free image
customization with text-to-image diffusion models. arXiv
preprint arXiv:2309.05793 , 2023. 2
[10] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui
Jia, Ming-Wei Chang, and William W Cohen. Subject-
driven text-to-image generation via apprenticeship learning.
InNeurIPS , 2023. 1, 2, 4, 6, 7, 8
[11] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,
Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam
Grycner, Basil Mustafa, Lucas Beyer, et al. PaLI: A jointly-
scaled multilingual language-image model. In ICLR , 2023.
4
[12] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon
Kim, Hyunwoo Kim, and Sungroh Yoon. Perception pri-
oritized training of diffusion models. In CVPR , 2022. 2, 4
[13] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat GANs on image synthesis. In NeurIPS , 2021. 3[14] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.
CogView2: Faster and better text-to-image generation via hi-
erarchical transformers. In NeurIPS , 2022. 2
[15] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
transformers for high-resolution image synthesis. In CVPR ,
2021. 2
[16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,
Chechik Gal Bermano, Amit H., and Daniel Cohen-Or. An
image is worth one word: Personalizing text-to-image gen-
eration using textual inversion. In ICLR , 2023. 1, 2, 3, 4,
6
[17] Inhwa Han, Serin Yang, Taesung Kwon, and Jong Chul Ye.
Highly personalized text embedding for image manipulation
by stable diffusion. arXiv preprint arXiv:2303.08767 , 2023.
2
[18] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar,
Dimitris Metaxas, and Feng Yang. SVDiff: Compact pa-
rameter space for diffusion Ô¨Åne-tuning. In ICCV , 2023. 2
[19] Jonathan Ho and Tim Salimans. ClassiÔ¨Åer-free diffusion
guidance. arXiv preprint arXiv:2207.12598 , 2022. 2
[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In NeurIPS , 2020. 3
[21] Hexiang Hu, Kelvin C.K. Chan, Yu-Chuan Su, Wenhu Chen,
Yandong Li, Kihyuk Sohn, Yang Zhao, Xue Ben, Boqing
Gong, William Cohen, Chang Ming-Wei, and Xuhui Jia.
Instruct-Imagen: Image generation with multi-modal in-
struction. In CVPR , 2024. 2
[22] Ziqi Huang, Kelvin C.K. Chan, Yuming Jiang, and Ziwei
Liu. Collaborative diffusion for multi-modal face generation
and editing. In CVPR , 2023. 2, 3, 4
[23] Ziqi Huang, Tianxiang Wu, Yuming Jiang, Kelvin C.K.
Chan, and Ziwei Liu. ReVersion: Diffusion-based relation
inversion from images. arXiv preprint arXiv:2303.13495 ,
2023. 3
[24] Xuhui Jia, Yang Zhao, Kelvin C.K. Chan, Yandong Li, Han
Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and
Yu-Chuan Su. Taming encoder for zero Ô¨Åne-tuning image
customization with text-to-image diffusion models. arXiv
preprint arXiv:2304.02642 , 2023. 1, 2, 4
[25] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,
Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up
GANs for text-to-image synthesis. In CVPR , 2023. 2
[26] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. In ICLR , 2014. 2
[27] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Shechtman, and Jun-Yan Zhu. Multi-concept customization
of text-to-image diffusion. In CVPR , 2023. 2
[28] Dongxu Li, Junnan Li, and Steven CH Hoi. BLIP-
Diffusion: Pre-trained subject representation for control-
lable text-to-image generation and editing. arXiv preprint
arXiv:2305.14720 , 2023.
[29] Xiaoming Li, Xinyu Hou, and Chen Change Loy. When
StyleGAN meets stable diffusion: A W+adapter for per-
sonalized image generation. In CVPR , 2024. 2
[30] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-
wei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.
GLIGEN: Open-set grounded text-to-image generation. In
CVPR , 2023. 3
6741
[31] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai
Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang
Cao. Cones 2: Customizable image synthesis with multiple
subjects. arXiv preprint arXiv:2305.19327 , 2023. 2
[32] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-
gang Qi, Ying Shan, and Xiaohu Qie. T2I-Adapter: Learning
adapters to dig out more controllable ability for text-to-image
diffusion models. arXiv preprint arXiv:2302.08453 , 2023. 3
[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In ICML , 2021.
2, 3, 5, 6
[34] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with a
uniÔ¨Åed text-to-text transformer. JMLR , 2020. 2, 3
[35] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with CLIP latents. arXiv preprint arXiv:2204.06125 ,
2022. 2
[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¬®orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , 2022. 1, 2, 5
[37] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and KÔ¨År Aberman. DreamBooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In CVPR , 2023. 1, 2, 6, 8
[38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha
Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet,
and Mohammad Norouzi. Photorealistic text-to-image diffu-
sion models with deep language understanding. In NeruIPS ,
2022. 1, 2, 4
[39] Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer,
Oran Gafni, Eliya Nachmani, and Yaniv Taigman. KNN-
Diffusion: Image generation via large-scale retrieval. In
ICLR , 2023. 3
[40] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-
Booth: Personalized text-to-image generation without test-
time Ô¨Ånetuning. arXiv preprint arXiv:2304.03411 , 2023. 2
[41] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro
Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang,
Glenn Entis, Yuanzhen Li, et al. StyleDrop: Text-to-image
generation in any style. arXiv preprint arXiv:2306.00983 ,
2023. 1
[42] Yu-Chuan Su, Kelvin C.K. Chan, Yandong Li, Yang Zhao,
Han Zhang, Boqing Gong, Huisheng Wang, and Xuhui Jia.
Identity encoder for personalized diffusion. arXiv preprint
arXiv:2304.07429 , 2023. 3
[43] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon.
Key-locked rank one editing for text-to-image personaliza-
tion. In SIGGRAPH , 2023. 2
[44] Dani Valevski, Danny Wasserman, Yossi Matias, and Yaniv
Leviathan. Face0: Instantaneously conditioning a text-to-image model on a face. arXiv preprint arXiv:2306.06638 ,
2023. 2
[45] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete
representation learning. In NeurIPS , 2017. 2
[46] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-
dermans, Hernan Moraldo, Han Zhang, Mohammad Taghi
Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.
Phenaki: Variable length video generation from open domain
textual description. In ICLR , 2023. 3
[47] Andrey V oynov, Qinghao Chu, Daniel Cohen-Or, and KÔ¨År
Aberman. P+: Extended textual conditioning in text-to-
image generation. arXiv preprint arXiv:2303.09522 , 2023.
2, 3
[48] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin C.K.
Chan, and Chen Change Loy. Exploiting diffusion prior
for real-world image super-resolution. arXiv preprint
arXiv:2305.07015 , 2023. 3
[49] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei
Zhang, and Wangmeng Zuo. ELITE: Encoding visual con-
cepts into textual embeddings for customized text-to-image
generation. In ICCV , 2023. 1, 2, 3, 4, 5, 6
[50] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-
jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-
fei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive
models for content-rich text-to-image generation. TMLR ,
2022. 2
[51] Lvmin Zhang and Maneesh Agrawala. Adding conditional
control to text-to-image diffusion models. In ICCV , 2023. 3
6742
