Training Diffusion Models Towards Diverse Image Generation with
Reinforcement Learning
Zichen Miao1, Jiang Wang2, Ze Wang1, Zhengyuan Yang2, Lijuan Wang2, Qiang Qiu1, Zicheng Liu3
Purdue University1Microsoft Corporation2Advanced Micro Devices, Inc.3
{miaoz, zewang, qqiu }@purdue.edu {jiangwang, zhengyang, lijuanw }@microsoft.com zicheliu@amd.com
Abstract
Diffusion models have demonstrated unprecedented ca-
pabilities in image generation. Yet, they incorporate and
amplify the data bias (e.g., gender, age) from the orig-
inal training set, limiting the diversity of generated im-
ages. In this paper, we propose a diversity-oriented fine-
tuning method using reinforcement learning (RL) for diffu-
sion models under the guidance of an image-set-based re-
ward function. Specifically, the proposed reward function,
denoted as Diversity Reward, utilizes a set of generated im-
ages to evaluate the coverage of the current generative dis-
tribution w.r.t. the reference distribution, represented by
a set of unbiased images. Built on top of the probabilis-
tic method of distribution discrepancy estimation, Diver-
sity Reward can measure the relative distribution gap with
a small set of images efficiently. We further formulate the
diffusion process as a multi-step decision-making problem
(MDP) and apply policy gradient methods to fine-tune diffu-
sion models by maximizing the Diversity Reward. The pro-
posed rewards are validated on a post-sampling selection
task, where a subset of the most diverse images are selected
based on Diversity Reward values. We also show the effec-
tiveness of our RL fine-tuning framework on enhancing the
diversity of image generation with different types of diffu-
sion models, including class-conditional models and text-
conditional models, e.g., StableDiffusion.
1. Introduction
Generative modeling on different modalities has been an
active research area [5, 8, 34]. Empowered by large-scale
models and datasets, the diffusion model develops remark-
able capabilities in modeling high-dimensional, complex
distributions, which enables photo-realistic conditional im-
age generation under various types of guidance, e.g., classi-
fier guidance [8] and textual guidance [29].
However, diffusion models inherit and amplify data bias
from the large-scale, uncurated training data [3], showing
undesired behaviors under specific conditions. For instance,
Figure 1. Illustration of non-diverse & biased images generated
with both class-conditional and text-conditional diffusion models.
Our diversity-oriented RL fine-tuning framework can tune both
models for diverse & unbiased image generation.
text-to-image diffusion models show significant gender bias
for particular occupations. Given the text prompt, ‘a photo
of a programmer’, StableDiffusion [29] generates almost all
males, whereas female programmers are dramatically un-
derrepresented, which is shown in Figure 1. This ‘condi-
tional mode collapse’ also happens in class-conditional dif-
fusion models [8], where generated images show high uni-
formity under high guidance scales.
To advance diverse class-conditional image generation,
[27] retrains diffusion models with a re-weighted loss func-
tion, while [33] proposes a classifier-based diversity guid-
ance. Meanwhile, both text prompt engineering [9, 26] and
text prompt tuning [40] methods have been proposed for
unbiased text-to-image generation. However, they all adopt
task-specific designs that lack the generalizability to other
types of diffusion models.
In this paper, we propose a general fine-tuning frame-
work to further enhance the generation diversity of diffusion
models under the guidance of a small set of unbiased refer-
ence images. Specifically, we design a class of reward func-
tions, denoted as Diversity Reward , that estimate the cover-
age of the generative distribution w.r.t. the reference dis-
tribution, where each population is represented by a small
set of images. To fine-tune diffusion models with Diversity
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10844
Reward , we apply reinforcement learning (RL) as a versa-
tile optimization framework. Note that our RL fine-tuning
framework does not depend on specific types of diffusion
models and thus is generalizable.
Reward function design poses as the core of RL. Un-
like ImageReward [39] adopted by [5, 10], where the re-
ward is predicted by a reward model based on a single im-
age, our Diversity Reward measures the overlap between the
generative and the reference populations based on a set of
samples. We explore several designs of Diversity Reward .
First, we adopt the principal distribution distance measure-
ment, Maximum Mean Discrepancy [13] (MMD), as one
instantiation of Diversity Reward . It estimates the distribu-
tion distance with a few samples from each population, and
we propose to use the negative MMD value to indicate the
coverage of the current generative distribution. Moreover,
formulating the groups of images as a Gaussian Process [38]
in the feature space, we apply mutual information to mea-
sure the dependency between the generated and reference
image random variables.
To provide more dense supervision to RL fine-tuning, we
further estimate the contribution of each generated image
among the group with the marginal utility [23]. In this way,
we assign each image a distinct reward value. By maximiz-
ing the proposed Diversity Reward , RL fine-tuning expands
the model coverage and mitigates the discrepancy between
the generative and reference distributions.
We first validate the effectiveness of Diversity Reward
with a post-sampling selection task, where we try to select
the most diverse subsets with the proposed reward func-
tion. We then evaluate our diversity-oriented RL fine-tuning
framework on both class-conditional and text-conditional
diffusion models.
We summarize our contribution as follows:
• We propose theoretically supported Diversity Reward for
estimating the diversity of diffusion models with small
sets of generated and reference images.
• We propose to use the marginal utility to estimate the re-
ward for each image from Diversity Reward .
• We propose a general RL fine-tuning framework
equipped with Diversity Reward to further enhance the
generative diversity of diffusion models.
2. Related Works
2.1. Diffusion Models
Diffusion models [14, 34] gradually transform multi-mode
distributions into the standard Gaussian. By learning each
step and constructing reverse processes, they show state-of-
the-art performances in image synthesis [8, 14, 29]. Among
them, conditional diffusion models [8, 29] have shown the
ability to generate photo-realistic images with high corre-
spondence to given conditions. Specifically, [8] equips dif-
fusion models with an additional classifier, leading to high-fidelity class-conditional image generation. While text-
conditional diffusion models generate images based on text
prompts [2, 24, 30, 41]. For instance, StableDiffusion, built
on top of latent diffusion models(LDM) [29], augments
the diffusion models with the text encoder of CLIP [28],
enabling generating high-quality images aligned with the
given text prompts. However, both show a lack of diver-
sity in conditional image generation, especially under high
guidance scales. We propose a diversity-oriented RL fine-
tuning method to mitigate this issue.
2.2. Unbiased Image Generation
Mitigating bias in deep models has been explored inten-
sively in discriminative models [36, 37]. Yet, there are lim-
ited works on diverse generative modeling [7, 19], where
most of them focus on GAN-based models. Several recent
works have been proposed for learning debiased diffusion
models. [33] propose hardness score guidance to sample
images from low-density regions with class-conditional dif-
fusion models, while [27] adopts a re-weighted loss to train
diffusion models that generate data from long-tailed classes.
For text-conditional diffusion models, [3] proposed to diver-
sify model outputs by ethical intervention, and [9] proposed
to directly add attribute words to the prompt. Yet, these
prompt engineering methods have limitations such as being
ambiguous, and not always generating diverse images reli-
ably. [40] apply prompt tuning to learn inclusive prompts
with a small set of unbiased images, yet it needs to man-
ually specify the diversity ratio during generation. Differ-
ent from these works, our RL fine-tuning framework works
with both class and text conditional diffusion models that
automatically fit the diversity ratio in the reference images.
2.3. RL Fine-tuning Diffusion Models
Inspired by tuning large language models with reinforce-
ment learning (RL) [25], several works have been proposed
to formulate the diffusion model as a multi-step decision-
making problem and apply RL methods for various objec-
tives. [10] introduces a policy gradient method for training
diffusion models for a more efficient sampler. [5, 11] build
upon [10, 21] to better align text-to-image diffusion mod-
els to human preferences using a policy gradient algorithm,
where they both adopt a single-image-based reward func-
tion [39]. Different from these works, we first propose an
image-set-based reward, i.e., Diversity Reward , that mea-
sures the gap between generative and reference distribution
efficiently. It further enables the diversity-oriented RL fine-
tuning for diffusion models.
3. Preliminary
3.1. Diffusion Models
As proposed in [14, 34], diffusion models diffuse the
original data distribution on x0with a sequential forward
10845
Figure 2. Illustration of the proposed general diversity-oriented
RL fine-tuning. The proposed Diversity Reward uses a pair of
sets of generated images and diverse reference images to assess
the diversity of the current model. After estimating the individual
reward for each image, we apply RL to maximize reward values.
Our method has no model-specific design and is generalizable to a
broad range of diffusion models.
Markov process in Ttotal steps. By using a transition dis-
tribution q(xt|xt−1)in each step t, which iteratively adds
noise to the data, diffusion models gradually destruct the
original data distribution p(x0|c)(cdenotes the condition)
to standard Gaussian p(xT|c) =N(0,I). To sample from
random noise xT, [14] parameterizes a reverse Markovian
process with neural networks,
pθ(xt−1|xt, c) =N(xt−1;µθ(xt−1, t, c), σ2
tI),(1)
where θis the parameters of the neural network. Starting
from random noise xT, diffusion models create a denoising
trajectory {xT, xT−1, ..., x 0}, and obtains sample x0in the
end.
The optimization of θis performed by minimizing the
KL divergence between q(xt−1|xt)andpθ(xt−1|xt), which
can be represented by the following objective,
L(θ) =Ex0,c,t,x t∼q(xt|x0)[||ˆµ(x0, t)−µθ(xt, c, t)||2],(2)
where the posterior mean of the forward diffusion process
ˆµ(x0, t)is a linear combination of x0andxt.
3.2. Fine-tuning Diffusion Model with Reinforce-
ment Learning
Fine-tuning a pre-trained diffusion model with reinforce-
ment learning (RL) can be achieved by optimizing the ob-
jective below,
L=Ec,x0∼pθ(x0|c)[r(x0, c)], (3)
where pθ(x0|c)denotes the sampling distribution, and
r(x0, c)denotes the reward defined over image x0and thecondition c. [5, 10] formulate the reverse diffusion process
as the following Markov decision process (MDP),
st≜{xt, c, t}, at≜xt−1
π(at|st)≜pθ(xt−1|xt, c),
P(st+1|st, at)≜{δ(xt−1), δ(c), δ(t−1)}
R(st, at)≜ 1{t= 0} ·r(x0, c),(4)
where st, at,denote state, action, πis the policy, and δ(x)
denotes the Dirac distribution where all probability mass
concentrates on x. In this formulation, the diffusion model
serves as the policy network and parameterizes the Markov
transition kernel P(st+1|st, at). The trajectory consists of
Ttimesteps, ending with a sampled image x0.
This formulation further enables the estimation of pol-
icy gradients. With access to both likelihood and gradients
of likelihood, we follow the formulation in [5, 10], which
makes direct Monte Carlo estimates of ∇θL, by sampling,
and then perform parameter update. Specifically, we adopt
the importance sampling estimator [16], and substitute the
per-step return with the final reward r(x0, c),
∇θL=E"TX
t=0pθ(xt−1|xt, c)
pθ′(xt−1|xt, c)∇θlogpθ(xt−1|xt, c)r(x0, c)#
(5)
in which the expectation is taken over the trajectories sam-
pled with pθ′, i.e., the previous sampler. The estimator be-
comes less accurate if pθ′deviates too much from pθ. We
adopt the trust region [31] for regularizing the change of θ
w.r.t. θ′, which in practice we adopt the clipping proposed
in proximal policy optimization [32].
4. Method
In this section, we first describe the proposed Diversity Re-
ward , which estimates the discrepancy between the genera-
tive and the reference distributions defined by a small set of
diverse images. We then show how the proposed reward can
be seamlessly incorporated into the diffusion model fine-
tuning with reinforcement learning (RL) towards diverse
image generation.
4.1. Diversity Reward
We first present our formulation and show several design
choices for Diversity Reward . Current rewards for fine-
tuning diffusion models are mostly single-image reward.
For instance, ImageReward [39], proposed for enhancing
text-image alignment, evaluates behaviors of the diffusion
model with a single image. However, it is intractable to de-
sign a reward function based on a single image to evaluate
the diversity. We propose to use a set of generated images to
characterize the distribution and propose a reward function
that measures the overlap between the current generative
10846
distribution and the reference distribution using two sets of
sampled images.
Problem Formulation. Given the generative distribution
of the diffusion model pθ(x0|c)and the reference distribu-
tionpr(xr), we propose the Diversity Reward to measure
the overlap, which can also be represented as negative dis-
tance between pθandpr,−D(pθ, pr). MMD [13] is a prin-
cipal method in probabilistic theory to estimate D(pθ, pr)
given a few samples from each population. Formally, we
denote Mgenerated images sampled from the diffusion
model under condition casXg={x1
0,g, x2
0,g, ..., xM
0,g},
where xm
0,gi.i.d.∼pθ(x0|c), and Nreference images as
Xr={x1
r, x2
r, ..., xN
r}sampled from the reference distri-
bution, xn
ri.i.d.∼pr(xr). MMD then estimates D(pθ, pr)in
the feature spaces of network ϕwith a measure d,
rD(Xg;Xr, ϕ) =−d(Zg,Zr)≈ −D (pθ, pr),(6)
where Zg,Zrdenotes the transformed deep features of
Xg,Xrwith model ϕ(·), and d(·,·)is a distance measure
in the feature space. Specifically, Zg={Z1
g, Z2
g, ..., ZM
g},
Zm
g=ϕ(xm
0,g), andZr={Z1
r, Z2
r, ..., ZN
r},Zn
r=ϕ(xn
r).
Alternatively, we can also view the Diversity Reward
with a measure of the dependency between random vari-
ables, x0,gandxr, where we can naturally adopt mutual in-
formation as a proxy measurememt. As the relationship be-
tween image samples are better characterized in the feature
space of ϕ, we compute the mutual information of Zg,Zr,
and our Diversity Reward is defined as,
rD(Xg;Xr, ϕ) =I(Zg,Zr) (7)
We also explore another non-differentiable design of the
Diversity Reward based on the Recall metric [20], detailed
in Appendix A.
4.1.1 Choices of ϕ
We propose two choices of ϕ(·); one is the pre-trained In-
ceptionV3 [35] for class-conditional diffusion models, and
the other is the pre-trained visual encoder of CLIP [28] for
text-to-image diffusion models.
4.1.2 Selection of Reference Images
We select Xrwith a small set of diverse images in a
task-specific manner. For instance, we randomly select
around 50 images per class from the training set for class-
conditional generation, and select a set of images with bal-
anced attributes, e.g., 25 photos for people w/ & w/o eye-
glasses.
Figure 3. Illustration of the proposed individual reward, where
the reward for a single image is estimated based on the Diversity
Reward of two image sets.
4.1.3 Specific Designs of Diversity Reward
MMD. Maximum mean discrepancy (MMD) can be writ-
ten as
DF(P, Q) = sup
f∈FEPf(X)−EQf(Y), (8)
in which Fdenotes the witness function. We propose to
exploit MMD as a Diversity Reward that characterizes the
coverage of pθw.r.t. pr. Specifically, with kernel function
Kϕin the feature space of ϕ, we represent the MMD reward
as,
DF(pθ, pr) =||Exm
0,g∼pθf∗(xm
0,g)−Exnr∼prf∗(xn
r)||2,
(9)
where the witness function is defined as,
f∗(x) =Exm
0,g∼pθKϕ(xm
0,g, x)−Exnr∼prKϕ(xn
r, x),(10)
andKϕ(x, y) =K(ϕ(x), ϕ(y)) =K(Zx, Zy).
Note that with a small batch of generated and real sam-
ples, i.e., Xg,Xr, we adopt an unbiased estimation of
DF(Pg, Pr)withd(Zg,Zr), which is the L2norm between
the mean of kernel matrices. As the reward function is max-
imized in RL, we take the negative MMD distance as our
MMD diversity reward,
rMMD
D(Xg;Xr, ϕ) =−1
M(M−1)MX
i̸=jKϕ(xi
0,g, xj
0,g)
−1
N(N−1)NX
i̸=jKϕ(xi
r, xj
r) +2
MNMX
i=1NX
i=1Kϕ(xi
0,g, xj
r).
(11)
We use the polynomial kernel as the kernel function K.
Mutual Information. Mutual information of Zg,Zrcan
be written as,
I(Zg,Zr) =H(Zg) +H(Zr)−H(Zg,Zr), (12)
10847
where H(X)denotes the entropy of X. AsP(Zg), P(Zr)
are intractable, an approximation to I(Zg,Zr)with only
a few samples is required. [4] have proposed an effective
method to estimate mutual information with samples, yet
an additional neural network is needed to be trained, which
can induce extra computational and time costs. Instead, we
propose to model Zg,Zrwith Gaussian Process (GP). By
modeling the covariance (Cov) of Zg,Zrusing the kernel
function K,
Cov(Zm
g, Zn
r) =Cov(m, n) =K(Zm
g, Zn
r), (13)
we can represent the entropy of H(Z)with,
H(Z) =1
2ln|Cov(Z, Z)|+D
2ln(1 +π
2), (14)
where |A|is the determinant of matrix A.
By removing constants, Equation 12 can be represented
as
d(Zg,Zr) =1
2ln|Cov(Zg,Zg)|−1
2ln|Cov(Zg,r,Zg,r)|,
(15)
where Zg,ris the concatenation of Zg,Zr. In terms of the
kernel function K, we select the RBF kernel [38]. We de-
note this design of rDas,
rGP-MI
D (Xg;Xr, ϕ) = ln|Cov(Zg,Zg)|
|Cov(Zg,r,Zg,r)|. (16)
4.2. Diversity-Oriented RL Fine-tuning
4.2.1 Individual Reward
Note that the diversity rewards we propose in the previ-
ous section are based on a set of generated images Xg=
{x1
0,g, x2
0,g, ..., xM
0,g}. In other words, for all Mchains of
states and actions, the diffusion model, i.e., policy net-
work, only receives a single reward. This sparse reward
issue can cause unstable RL learning and sub-optimal re-
sults [1, 12, 32].
To generate dense reward values, i.e., assigning each
member in Xgwith a reward, we propose to adopt the
marginal utility in submodular set cover problem [23] to
estimate the individual contribution of xm
0,gto the overall
diversity reward. Specifically, the marginal utility of ew.r.t.
a subset Sis expressed as,
F(e|S) =F(S∪ {e})−F(S), (17)
where Fis a function defined over sets. It is required that
Fis monotone, i.e., F(e|S)≥0,∀e∈V\SandS⊂V,
where Vis the whole set. As rMMD
D andrMI
Dsatisfy thiscondition, we propose individual rewards ˆrDas follows,
ˆrGP-MI
D (xm
0,g;Xr, ϕ)
=rGP-MI
D (Xg;Xr, ϕ)−rGP-MI
D (Xg\ {xm
0,g};Xr, ϕ)
ˆrMMD
D(xm
0,g;Xr, ϕ)
=rMMD
D(Xg;Xr, ϕ)−rMMD
D(Xg\ {xm
0,g};Xr, ϕ)
(18)
4.2.2 RL fine-tuning with Diversity Rewards
Applying our proposed diversity rewards to RL fine-tuning
introduced in Section 3.2, we now can tune diffusion mod-
els towards diverse conditional image generation. Specifi-
cally, the objective function used in RL fine-tuning for di-
versity (RLD) now becomes,
LRLD =Ec,x1
0,g,...,xM
0,g∼p(x|c)[X
mrD(xm
0,g)]. (19)
Note that we compute the individual reward and
have rD(xm
0,g) = ˆ rGP-MI
D (xm
0,g;Xr, ϕ)orrD(xm
0,g) =
ˆrMMD
D(xm
0,g;Xr, ϕ).
To optimize Equation 19, we adopt the policy gradient
in Equation 5 which supports multiple steps of parameter
updates,
∇θLRLD =
E"MX
m=1TX
t=0pθ(xm
t−1|xm
t, c)
pθ′(xm
t−1|xm
t, c)∇θlogpθ(xm
t−1|xm
t, c)r(xm
g,0, c)#
.
(20)
We summarize the proposed diversity-oriented RL fine-
tuning in Algorithm 1.
5. Experiments
In this section, we present experiments to validate each
component of the proposed method. We first validate the ef-
fectiveness of the proposed reward functions in Section 5.1.
Algorithm 1 Diversity-Oriented Diffusion Models with RL
Fine-tuning.
Require: pre-trained diffusion model pθpre, reference im-
ages with each condition c:{Xr|c}, diversity reward
function rD(Xg;Xr, ϕ).
Initialize pθ=pθpre
while θnot converged do
For each condition c∼p(c), sample Mimages
Xg|c={x1
0,g, ..., xM
0,g}, xm
0,g∼pθ(x0|c), together
with their intermediate states xm
1:T,g.
Compute diversity rewards with the selected reward
function for each condition c,rD(Xg;Xr, ϕ, c).
Assign reward to xm
0,gwith individual reward (18).
TakeEinrounds of policy gradient steps with (20).
end while
return Fine-tuned diffusion model pθ.
10848
(a) Baseline Model
 (b) RL Fine-tuned Model w/ rMMD
D
(c) Reference images
Figure 4. Comparisons with baseline model on ImageNet-128x128 generation (high guidance scale s= 4.0). Reference images are
randomly selected from the training set of ImageNet. Our method fine-tunes the class-conditional model to generate images with higher
visual diversity, e.g., diverse backgrounds, guided by the diverse reference images.
In Section 5.2 and Section 5.3, we demonstrate that, equip-
ping with the proposed reward functions, our reinforce-
ment learning (RL) framework can fine-tune both class-
conditional and text-conditional diffusion models to gener-
ate diversified images guided by small reference sets.
5.1. Diversity Reward Evaluation
Post-Sampling Selection. Given a set of Ogenerated im-
ages under condition c, we aim at selecting a subset of
Mimages that have the maximum diversity. Specifically,
on the 128x128 ImageNet class-conditional generation, we
first sample 100K images from the pre-trained diffusion
model [8]. We view the sampled images as 10K image sets,
where each set of O= 10 images is sampled from the same
(class) condition c. As for subset selection, we evaluate the
reward values for all O
M
possible subsets and select the
one with the largest reward. As for the Xrin computing
rewards, we randomly select 50 images of the same class c
from the ImageNet training set. We then evaluate the FID,
Precision, and Recall of the selected 50K images.
Reward Evaluation Results. We first present the quanti-
tative results of post-sampling selection in Table 1. Com-
pared with the baseline random selection, all of the pro-
posed rewards show their effectiveness in selecting the di-
Table 1. Reward Evaluation on ImageNet 128x128 post-sampling
selection task. Compared with random selection, subsets selected
with our maximum Diversity Reward achieve higher diversity
(higher Recall and lower FID), while the ones selected with mini-
mum reward have lower diversity (lower Recall and higher FID).
Select Criterion Recall Precision FID
Baseline (Random) 40.22 85.77 8.10
w/ max MMD 47.66 83.26 6.22
w/ max GP-MI 45.31 84.14 6.81
w/ min MMD 27.69 92.39 14.94
w/ min GP-MI 31.24 88.26 10.32Table 2. Results on ImageNet-128x128 class-conditional genera-
tion. Our method improves the diversity (Recall) of baseline mod-
els under various guidance scales.
Method Recall Precision FID
Baseline ( s= 4.0)36.15 82.96 24.49
Ours (GP-MI) 45.31 81.72 23.81
Ours (MMD) 47.66 83.26 23.42
Baseline s= 3.0 40.35 82.10 23.00
Ours (GP-MI) 46.31 81.74 22.31
Ours (MMD) 49.31 81.26 22.08
Baseline s= 2.0 45.95 79.20 21.19
Ours (GP-MI) 47.83 79.05 20.95
Ours (MMD) 49.30 78.82 20.48
Baseline s= 1.0 55.10 74.16 19.86
Ours (GP-MI) 56.13 73.52 19.49
Ours (MMD) 59.63 74.84 19.19
verse subset in the post-sampling selection task. Specifi-
cally, subsets selected with max-reward criteria have higher
Recall than the baseline random selection, which in turn im-
proves the overall FID by a large margin. On the other hand,
subsets selected with the min-reward criteria show low di-
versity compared with baseline selection, where both Re-
call and FID drop significantly. The results above show that
proposed rewards are effective indicators in measuring the
overlap between XgandXr. We provide more results in-
cluding visualizations in Appendix B.
5.2. Diverse Class-Conditional Image Generation
In this section, we validate our method with class-
conditional diffusion models. On both ImageNet [18]
and CIFAR-10/100 [17], we show that our RL fine-tuning
framework with the proposed rewards can adapt diffusion
models to generate images with enhanced diversity.
Diverse Fine-tuning on ImageNet. We fine-tune the pre-
trained ImageNet 128x128 conditional diffusion model and
guided classifier proposed in [8] with RL and the diversity
rewards. Note that we only fine-tune the first 100 classes
10849
(a) Baseline generation, all ’Young’ people without ’Eyeglasses’
 (b) Unbiased reference images for attributes ’Young’ and ’Eyeglasses’
(c) Unbiased generation for attribute ’Eyeglasses’
 (d) Unbiased generation for attribute ’Young’
Figure 5. RL Fine-tuning on StableDiffusion [29] for unbiased face generations. The text prompt used is, ’a headshot of a person’.
Reference Images are selected from CelebA [22], which have equal numbers of images with positive and negative attributes, e.g., 25
images w/ glasses, 25 images w/o glasses. Baseline generations are biased as they are young people without glasses. Our method tunes the
SD to generate unbiased facial images under attributes ’Eyeglasses’ and ’Young’ respectively.
of ImageNet for efficiency consideration. For Xr, we ran-
domly select N= 50 images per class from the training
set as in Section 5.1. As for evaluation, we select 20 non-
overlapped images from each class as the real-image set
with 2K images, and generate 5K images from the fine-
tuned model. We then evaluate the Precision, Recall [20],
and FID to measure both the diversity and quality of gener-
ated images. In RL fine-tuning, we adopt the LoRA [15] to
the attention layers of both the diffusion model and guided
classifier. We set M= 5, learning rate to 1e−4, and fine-
tune for 100 epochs. More implementation details are pro-
vided in Appendix C.
We present the quantitative results of RL fine-tuning
on multiple guidance scales, s={1.0,2.0,3.0,4.0}. As
shown in Table 2, RL fine-tuned models with all the pro-
posed rewards show superior performances in all guidance
scales. Specifically, compared with baseline models, fine-
tuned models show higher Recall, which in turn leads to
improvement in FID. We also provide visualizations in Fig-
ure 4, where images from the fine-tuned model, following
the guidance of the reference set, are more diverse com-
pared to the baseline generation results.
Diverse Fine-tuning on CIFAR. To compare with other
benchmarks, we provide RL fine-tuning results on CI-
FAR [17]. Following [27], we first obtain pre-trained
DDPM models on both CIFAR-10 and CIFAR-100. Com-
pared with [27], where diverse diffusion models are re-
trained from scratch, our method only fine-tunes a small
portion of the parameters in a more efficient way. As fortheXr, we also randomly select 50 images per class from
the training set. More implementation details are provided
in Appendix C. As shown in Table 3, our method achieves
comparable results with [27], despite being guided by only
a small reference set and with much fewer parameters.
5.3. Unbiased Text-Conditional Image Generation
In this section, we show how our method can improve di-
versity in text-to-image (T2I) generation.
5.3.1 Experimental Settings.
We adopt the settings in [40], and conduct experiments on
tuning StableDiffusion [29] (SD) toward diverse face gen-
eration under the guidance of a small set of reference im-
agesXr. Specifically, we leverage our method to fine-tune
T2I diffusion models to generate face images with a uni-
form distribution in a set of attributes, such as gender, skin
tone, and age. For example, we tune SD to generate equal
Table 3. Results on CIFAR-10/100. Our method achieves compa-
rable performances with task-specific retraining method [27].
Dataset Method Recall Fs IS FID
CIFAR100DDPM 0.65 0.97 13.65 3.11
CBDM [27] 0.67 0.97 14.03 2.72
Ours (GP-MI) 0.67 0.97 13.97 2.83
Ours (MMD) 0.69 0.97 13.87 2.70
CIFAR10DDPM 0.64 0.99 9.80 3.16
CBDM [27] 0.65 0.99 9.63 3.03
Ours (GP-MI) 0.65 0.99 9.71 3.04
Ours (MMD) 0.67 0.99 9.68 2.98
10850
Method Dmale
KLDpale skin
KLDyoung
KLDeyeglass
KLDmustache
KLDsmile
KL
SD [29] 0.343 0.308 0.578 0.375 0.111 0.134
EI [3] 0.143 0.644 0.423 0.531 0.693 0.189
HPS [9] 1e-5 2.8e-3 0.027 0.371 0.241 4.4e-3
PD [7] 0.322 0.165 0.131 0.272 0.063 0.146
CD [19] 0.309 0.074 0.284 0.301 0.246 0.469
ITI-GEN [40] 1×10−60 1×10−42×10−44.5×10−41.0×10−3
Ours (MMD) 1×10−31×10−32×10−46×10−41×10−39×10−4
Ours (GP-MI) 2×10−45×10−42×10−61.5×10−43×10−48×10−4
Figure 6. (Table) Comparison with baselines on unbiased face generation with StableDiffusion. Our method achieves competitive results
with baseline text prompt engineering & tuning methods while being a generalizable framework. (Figure) Ablation experiments on the
number of generated and reference images used in evaluating Diversity Reward .
numbers of faces with and without eyeglasses. We adopt
the proposed two diversity rewards with RL fine-tuning, and
apply LoRA [15] adaptation to SD. The text prompt for tun-
ing is set as ‘a headshot of a person’. More implementation
details are provided in Appendix D.
Reference Images. We select Xrfrom CelebA [22],
which contains facial images annotated with 40 attributes.
Specifically, we randomly select 50 images for each binary
attribute, with 25 from the positive attribute set and the rest
of 25 from the negative attribute set. For instance, for at-
tribute ’Eyeglasses’, we select 25 reference images of faces
with eyeglasses, and 25 reference images of faces without
glasses, as shown in Figure 5b.
Baselines and Evaluation Metrics. We compare our
method with the following methods. 1)SD [29] 2) Ethical
Intervention (EI) [3] 3) Hard Prompt Searching (HPS) [9] 4)
Prompts Debiasing (PD) [7] 5) Custom Diffusion (CD) [19]
6) ITI-GEN [40]. As for the evaluation, we sample 200 im-
ages from the fine-tuned SD and use the CLIP [28] to pre-
dict the attribute for each generated image. For attribute a,
this gives an attribute distribution of generated images Pa
gen.
Following [6, 7, 40], we adopt KL divergence to measure
the discrepancy between Pa
genand the targeted uniform at-
tribute distribution Pa
uni, i.e.,DKL[Pa
gen||Pa
uni]. We provide
more details on evaluation in Appendix D.
5.3.2 Results.
As shown in Figure 6 (Table), our RL fine-tuning frame-
work achieves better performance than most of the base-
lines. As for the comparison with ITI-GEN [40], a di-
verse image generation method specifically designed for
StableDiffusion, our method achieves comparable perfor-
mances while having the property of being generalizable
to other diffusion models. We provide more results with
other attributes in Appendix D. Moreover, we present vi-
sualizations in Figure 5. As shown in Figure 5c & 5d, SDfine-tuned with our method can generate images with uni-
form distributions for attributes ‘Eyeglasses’ and ‘Young’,
following the guidance of Xr, which greatly enhances the
diversity over the baseline generation as in Figure 5a.
5.4. Ablation Experiments
In this section, we study the effects of the size of generated
images Mand reference images Nused to evaluate the re-
ward. Specifically, we adopt the ImageNet experiment as in
Section 5.2. More details are provided in Appendix E
5.4.1 Choice of M
We test M= 2,3,5,7,10, and set N= 50 . As shown
in Figure 6 (Top Figure), using small numbers of generated
images ( M= 2,3) leads to degraded results, as it can be
hard to represent the distribution with 2,3 samples. On the
other hand, we find that M= 5can achieve reasonable re-
sults while increasing Mfurther can not improve the results
even with more cost induced.
5.4.2 Choice of N
We test N= 5,30,50,70,100, and set M= 5. More ex-
perimental details are provided in Appendix E. As shown in
Figure 6 (Bottom Figure), N≥30reference images lead to
reasonable results, while N= 5 images can cause perfor-
mance degradation.
6. Conclusion
We present a general diversity-oriented fine-tuning frame-
work for diffusion models with reinforcement learn-
ing (RL), under the guidance of a small set of diverse refer-
ence images. We propose several designs of Diversity Re-
ward with theoretical justification, and design an RL fine-
tuning framework that adapts the generative distribution of
diffusion models to align with the diverse reference distri-
bution. The effectiveness of the proposed reward functions
and the overall RL fine-tuning framework is validated on
multiple datasets with various diffusion models.
10851
References
[1] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas
Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh
Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hind-
sight experience replay. Advances in neural information pro-
cessing systems , 30, 2017. 5
[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,
Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,
Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image
diffusion models with an ensemble of expert denoisers. arXiv
preprint arXiv:2211.01324 , 2022. 2
[3] Hritik Bansal, Da Yin, Masoud Monajatipoor, and Kai-Wei
Chang. How well can text-to-image generative models un-
derstand ethical natural language interventions? arXiv
preprint arXiv:2210.15230 , 2022. 1, 2, 8
[4] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar,
Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R Devon
Hjelm. Mine: mutual information neural estimation. arXiv
preprint arXiv:1801.04062 , 2018. 5
[5] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and
Sergey Levine. Training diffusion models with reinforce-
ment learning. arXiv preprint arXiv:2305.13301 , 2023. 1, 2,
3
[6] Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-
eval: Probing the reasoning skills and social biases of
text-to-image generative transformers. arXiv preprint
arXiv:2202.04053 , 2(4):5, 2022. 8
[7] Ching-Yao Chuang, Varun Jampani, Yuanzhen Li, Anto-
nio Torralba, and Stefanie Jegelka. Debiasing vision-
language models via biased prompts. arXiv preprint
arXiv:2302.00070 , 2023. 2, 8
[8] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems , 34:8780–8794, 2021. 1, 2, 6
[9] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,
Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,
Hongxia Yang, et al. Cogview: Mastering text-to-image gen-
eration via transformers. Advances in Neural Information
Processing Systems , 34:19822–19835, 2021. 1, 2, 8
[10] Ying Fan and Kangwook Lee. Optimizing ddpm sampling
with shortcut fine-tuning. arXiv preprint arXiv:2301.13362 ,
2023. 2, 3
[11] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu,
Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Moham-
mad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok:
Reinforcement learning for fine-tuning text-to-image diffu-
sion models. arXiv preprint arXiv:2305.16381 , 2023. 2
[12] Scott Fujimoto, Herke Hoof, and David Meger. Address-
ing function approximation error in actor-critic methods. In
International conference on machine learning , pages 1587–
1596. PMLR, 2018. 5
[13] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch,
Bernhard Sch ¨olkopf, and Alexander Smola. A kernel method
for the two-sample-problem. Journal of Machine Learning
Research , 13:723–773, 2012. 2, 4, 1
[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840–6851, 2020. 2, 3[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021. 7, 8, 1, 2
[16] Sham Kakade and John Langford. Approximately optimal
approximate reinforcement learning. In Proceedings of the
Nineteenth International Conference on Machine Learning ,
pages 267–274, 2002. 3
[17] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009. 6, 7
[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
Imagenet classification with deep convolutional neural net-
works. In Advances in Neural Information Processing Sys-
tems, 2012. 6
[19] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Shechtman, and Jun-Yan Zhu. Multi-concept customization
of text-to-image diffusion. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 1931–1941, 2023. 2, 8
[20] Tuomas Kynk ¨a¨anniemi, Tero Karras, Samuli Laine, Jaakko
Lehtinen, and Timo Aila. Improved precision and recall met-
ric for assessing generative models. Advances in Neural In-
formation Processing Systems , 32, 2019. 4, 7, 1
[21] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins,
Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad
Ghavamzadeh, and Shixiang Shane Gu. Aligning text-
to-image models using human feedback. arXiv preprint
arXiv:2302.12192 , 2023. 2
[22] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild. In Proceedings of
the IEEE international conference on computer vision , pages
3730–3738, 2015. 7, 8
[23] Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec.
Coresets for data-efficient training of machine learning mod-
els. In International Conference on Machine Learning , pages
6950–6960. PMLR, 2020. 2, 5
[24] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741 , 2021. 2
[25] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al. Training lan-
guage models to follow instructions with human feedback.
Advances in Neural Information Processing Systems , 35:
27730–27744, 2022. 2
[26] Vitali Petsiuk, Alexander E Siemenn, Saisamrit Surbehera,
Zad Chin, Keith Tyser, Gregory Hunter, Arvind Raghavan,
Yann Hicke, Bryan A Plummer, Ori Kerret, et al. Human
evaluation of text-to-image models on a multi-task bench-
mark. arXiv preprint arXiv:2211.12112 , 2022. 1
[27] Yiming Qin, Huangjie Zheng, Jiangchao Yao, Mingyuan
Zhou, and Ya Zhang. Class-balancing diffusion models. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 18434–18443, 2023. 1,
2, 7
10852
[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 2, 4, 8
[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684–10695, 2022. 1, 2, 7, 8
[30] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems , 35:36479–36494, 2022. 2
[31] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jor-
dan, and Philipp Moritz. Trust region policy optimization. In
International conference on machine learning , pages 1889–
1897. PMLR, 2015. 3
[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-
ford, and Oleg Klimov. Proximal policy optimization algo-
rithms. arXiv preprint arXiv:1707.06347 , 2017. 3, 5
[33] Vikash Sehwag, Caner Hazirbas, Albert Gordo, Firat Oz-
genel, and Cristian Canton. Generating high fidelity data
from low-density regions using diffusion models. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 11492–11501, 2022. 1, 2
[34] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International confer-
ence on machine learning , pages 2256–2265. PMLR, 2015.
1, 2
[35] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception archi-
tecture for computer vision. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
2818–2826, 2016. 4, 1, 2
[36] Tianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang,
and Vicente Ordonez. Balanced datasets are not enough:
Estimating and mitigating gender bias in deep image rep-
resentations. In Proceedings of the IEEE/CVF international
conference on computer vision , pages 5310–5319, 2019. 2
[37] Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle
Genova, Prem Nair, Kenji Hata, and Olga Russakovsky. To-
wards fairness in visual recognition: Effective strategies for
bias mitigation. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 8919–
8928, 2020. 2
[38] Christopher KI Williams and Carl Edward Rasmussen.
Gaussian processes for machine learning . MIT press Cam-
bridge, MA, 2006. 2, 5, 1
[39] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai
Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagere-
ward: Learning and evaluating human preferences for text-
to-image generation. arXiv preprint arXiv:2304.05977 ,
2023. 2, 3[40] Cheng Zhang, Xuanbai Chen, Siqi Chai, Chen Henry Wu,
Dmitry Lagun, Thabo Beeler, and Fernando De la Torre. Iti-
gen: Inclusive text-to-image generation. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 3969–3980, 2023. 1, 2, 7, 8
[41] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang,
and In So Kweon. Text-to-image diffusion model in gener-
ative ai: A survey. arXiv preprint arXiv:2303.07909 , 2023.
2
10853
