SCoFT: Self-Contrastive Fine-Tuning for Equitable Image Generation
Zhixuan Liu1Peter Schaldenbrand1Beverley-Claire Okogwu1Wenxuan Peng2
Youngsik Yun3Andrew Hundt1Jihie Kim3Jean Oh1*
1Carnegie Mellon University2Nanyang Technological University3Dongguk University
⇤Corresponding author
Original	Stable	DiffusionOriginal	Stable	Diffusion(a)“Photo	of	a	traditional	building,	in	[Culture]”
(b)“Two	people	wearing	traditional	clothing,	in	[Culture]”SCoFT	(Ours)SCoFT	(Ours)AmericanCultureNigerian	CultureKorean	CultureMexican	CultureChinese	CultureIndian	Culture
StereotypeMisrepresen-tation
Figure 1. Comparison between Stable Diffusion with and without our proposed ﬁne-tuning approach, SCoFT, on our proposed CCUB
dataset. Stable Diffusion perpetuates harmful stereotypes that assume dirty buildings are representative of some nations, and often generates
regionally irrelevant designs. By contrast, our approach decreases stereotypes and improves cultural relevance of generated images.
Abstract
Accurate representation in media is known to improve
the well-being of the people who consume it. Genera-
tive image models trained on large web-crawled datasets
such as LAION are known to produce images with harm-
ful stereotypes and misrepresentations of cultures. We im-
prove inclusive representation in generated images by (1)
engaging with communities to collect a culturally repre-
sentative dataset that we call the Cross-Cultural Under-
standing Benchmark (CCUB) and (2) proposing a novel
Self-Contrastive Fine-Tuning (SCoFT, pronounced /s ˆoft/)
method that leverages the model’s known biases to self-
improve. SCoFT is designed to prevent overﬁtting on small
datasets, encode only high-level information from the data,and shift the generated distribution away from misrepresen-
tations encoded in a pretrained model. Our user study con-
ducted on 51 participants from 5 different countries based
on their self-selected national cultural afﬁliation shows that
ﬁne-tuning on CCUB consistently generates images with
higher cultural relevance and fewer stereotypes when com-
pared to the Stable Diffusion baseline, which is further im-
proved with our SCoFT technique. Resources and code are
athttps://ariannaliu.github.io/SCoFT .
1. Introduction
Representation matters. In media, studies repeatedly show
that representation affects the well-being of its viewers [ 8,
12,53]. Representation can positively affect viewers by
providing them with role models that they identify with,
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10822
but it can also negatively affect viewers by creating harm-
ful, stereotypical understandings of people and culture [ 7].
When people are accurately represented in media, it al-
lows people to properly understand cultures without harm-
ful stereotypes forming [ 11,37]. To date, unfortunately,
many media-generating AI models show poor representa-
tion in their results [ 32,39] and have been deployed for
cases with negative impacts on various groups, such as non-
consensual images [ 34]. Such issues stem from every stage
of an ML system’s lifecycle [ 56], with key steps being al-
gorithms and the large training datasets gathered by crawl-
ing the Internet with inadequate ﬁltering supervision. They
contain or amplify malignant stereotypes and ethnic slurs,
among other problematic content [ 4], and impact a range
of applications [ 22]. Researchers have shown that large
datasets such as LAION-400M [ 51] used to train many text-
to-image synthesis models, including Stable Diffusion [ 46],
center the Global North [ 4,5,32] and struggle to accurately
depict cultures from the Global South as shown in Figure 1.
To ensure models better represent culture and more ac-
curately represent the world, we introduce a new task of
culturally-aware image synthesis with the aim of address-
ing representation in image generation: generating visual
content that is perceived to be more representative of na-
tional cultural contexts. Our overarching goal is to improve
the well-being of viewers of the AI-generated images with
particular attention to those who are from a selection of
groups marginalized by existing methods.
Our research question is, how can effective, existing text-
to-image models be improved to become more culturally
representative and thus less offensive? Since it may be in-
feasible to vet billions of training examples for accurate cul-
tural content, we hypothesize that a small dataset that is
veritably representative of a culture can be used to prime
pre-trained text-to-image models to guide the model toward
more culturally accurate content creation. To verify the hy-
pothesis, we collected a dataset of image and caption pairs
for 5 cultures. For each culture, data was collected by peo-
ple who self-selectedly afﬁliated with that culture as they
are the people who properly understand it and are most af-
fected by its misrepresentations. We call this the Cross-
Cultural Understanding Benchmark (CCUB) dataset which
comprises 150 - 200 images for each culture each with a
manually written caption as shown in Figure 2.
To encode the culturally representative information in
CCUB into a pre-trained model, we propose to ﬁne-tune the
model with the new dataset. Existing ﬁne-tuning techniques
work well for low-level adaptations such as style changes
or introducing new characters to models [ 21], but we show
that these methods struggle to encode high-level, complex
concepts such as culture. Additionally, ﬁne-tuning on small
datasets, such as CCUB, can lead to overﬁtting.
Unlike concept editing tasks [ 15,16] with speciﬁc im-age editing directions, depicting cultural accuracy remains
more abstract and challenging. We propose a novel ﬁne-
tuning approach, Self-Contrastive Fine-Tuning (SCoFT,
pronounced /s ˆoft/), to address these issues. SCoFT lever-
ages the pre-trained model’s cultural misrepresentations
against itself. We harness the intrinsic biases of large pre-
trained models as a rich source of counterexamples; shift-
ing away from these biases gives the model direction to-
wards more accurate cultural concepts. Image samples from
the pre-trained model are used as negative examples, and
CCUB images are used as positive examples, to train the
model to discern subtle differences. We de-noise latent
codes in several iterations, project them into the pixel space,
and then compute the contrastive loss. The loss is back-
propagated to the diffusion model UNet and optimized to
push generated samples towards the positive distribution
and away from the negative. This is all done in a percep-
tual feature space so that the model learns more high-level,
complex features from the images.
To evaluate our results we recruited participants who
identify as being a member of the cultural communities in
the CCUB dataset to rank images generated by Stable Dif-
fusion with or without the proposed self-contrastive ﬁne-
tuning on CCUB. Fine-tuning on CCUB was found to de-
crease offensiveness and increase the cultural relevance of
generated results based on 51 participants across ﬁve cul-
tures and 766 image comparisons. Our proposed SCoFT
approach further improved these results. We share the ﬁnd-
ings from our experiments to provide a basis for an im-
portant aspect of AI-generated imagery: that cultural infor-
mation should be accurately presented and celebrated equi-
tably. Our contributions are as follows:
1.The introduction of culturally-aware text-to-image syn-
thesis as a valuable task within text-to-image synthesis;
2.The Cross-Cultural Understanding Benchmark (CCUB)
dataset of culturally representative image-text pairs
across 5 countries; and
3.Self-Contrastive Fine-Tuning (SCoFT), a novel tech-
nique for encoding high-level information into a pre-
trained model using small datasets.
2. Related Work
Cultural Datasets Various efforts have been made to eval-
uate and document the impacts of datasets[ 23,50,60]. Dol-
lar Street [ 45] aimed to capture accurate demographic infor-
mation based on socioeconomic features, such as everyday
household items and monthly income of 63 countries world-
wide. However, this dataset offers less diverse scenarios, as
most of its images are indoor views with limited cultural
features. Likewise, the WIT [ 55] and MLM [ 1] strive for
cultural representation but use Wikipedia/WikiData sources
for images that are not representative of all aspects of cul-
ture and are over-saturated with old, streotypical images.
Other works, capturing the idea of a diverse dataset, aim
10823
to reduce stereotypical bias through self-curation [ 10] or
cultural-driven methods [ 17,36,38], inspiring our data col-
lection methodology.
The MaRVL dataset [ 30], for example, was curated by
people who identify as afﬁliated with one of several partic-
ular cultures, MaRVL was developed to mitigate biases for
reasoning tasks covering popular concepts and is unsuitable
for text-to-image synthesis. Our dataset was also designed
to cover a diverse sample of cultures and engage with peo-
ple who are native, but it is speciﬁcally curated for vision-
language tasks and diverse cultural categories.
Fine-Tuning Text-to-Image Models Fine-tuning pre-
trained text-to-image synthesis models with a new dataset
is an approach to encode additional new simple concepts
and content into the model [ 21,27,31,41,48]. But culture
is a complex, high-level concept that poses many challenges
when attempting to ﬁne-tune a model to understand it.
Fine-Tuning on Cultural Data. Prior work in adapting
pre-trained models to be culturally relevant found some suc-
cess using millions of culturally relevant text-image pairs,
as in ERNIE-ViLG 2.0. [ 13] and Japanese Stable Diffu-
sion [ 54]. The size of these training datasets leads to bet-
ter cultural representations of Japan and China, but they are
not easy to deploy universally, as these approaches require
millions of training examples which cannot possibly be met
for cultures with less internet presence. Besides, these
datasets are so large that it is infeasible to 100% vet them
for harmful and stereotypical information [ 43]. We propose
a ﬁne-tuning technique that adapts pre-trained models to
learn complex, elusive concepts, namely culture, from small
datasets.
Fine-Tuning Stable Diffusion in the Pixel Space. Latent
diffusion models are customarily trained in the latent space,
however, the latent codes can be decoded into images dif-
ferentiably. Multiple works compute losses on the decoded
images to optimize over the input latent code [ 59] or the de-
coder weights [ 2]. DiffusionCLIP [ 25] optimizes UNet pa-
rameters using losses computed on image outputs, however,
this is performed on a non-latent diffusion model. High pa-
rameter count in latent diffusion models complicates gradi-
ent recording through multiple UNet passes, hindering de-
coding into pixel space. We propose a novel method to re-
duce the computation graph size, facilitating tractable back-
propagation of loss through Stable Diffusion.
Perceptual Similarity. Perceptual metrics, such as
LPIPS [ 62], and recent foundation models for perceptual
similarity [ 14,29,40,42], have been shown to align more
closely with human perception than pixel space Euclidean
distance [ 49]. Perceptual similarity ignores low-level dif-
ferences and captures high-level details, which are more
important for complex concepts such as cultures. To our
knowledge, no other work has trained a latent diffusion
model using perceptual loss, likely due to the technical chal-Nine	cultural	categoriesExperienced	Residentsof	Target	Culture“Korean	barbecue	grilling	meat”“a	Korean	couple	in	hanbok”“the	Eight	Gates	in	the	Fortress	Wall	of	Seoul”“Woman	in	costume	for	the	Day	of	the	Dead	holiday”“Puebla	Cholula	Church	and	Popocatepetl	Volcano	in	Mexico”“a	plate	of	beef	tacos	with	beans	and	rice	on	the	side”CCUBKoreanData	ExamplesCCUB	Mexican	Data	Examples
CCUBDatasetFigure 2. CCUB dataset cultural image and description samples.
lenges of back-propagating loss through the diffusion pro-
cess and latent decoder, which we address in this paper.
3. CCUB Dataset
Can a CCUB tame a LAION? As opposed to the
LAION [ 52] dataset which scraped images and captions
from the internet with minimal supervision leading to a
prominence in harmful content, our CCUB dataset was col-
lected by hand by the people most affected by cultural mis-
representations in text-to-image synthesis.
Following the deﬁnition of culture in [ 18], [30], and [ 24],
nine categories are used to represent cultural elements in our
dataset: architecture (interior and exterior), city, clothing,
dance music and visual arts, food and drink, nature, people
and action, religion and festival, utensils and tools. The
categories are further divided into traditional and modern to
reﬂect how cultural characteristics change over time.
For each culture, we recruited at least 5 people with
at least 5 years of experience living in one of 5 countries
(China, Korea, India, Mexico, and Nigeria) to each provide
20-30 images and captions. The images were collected ei-
ther by collecting image links from Google searches or the
collectors’ own photographs. Fig. 2has selected samples of
our CCUB dataset. More details are in supplement Sec. 9.
4. Method
Given our CCUB dataset, our objective is to alter Stable
Diffusion to have a more accurate understanding of a given
culture. We next offer a brief background on training latent
diffusion models (Sec. 4.1), a modiﬁcation to a regulariza-
tion loss to prevent over-ﬁtting (Sec. 4.2), a novel approach
to computing perceptual loss on decoded images (Sec. 4.3),
and a method to contrastively use Stable Diffusion’s mis-
representations of culture to reﬁne itself (Sec. 4.4).SCoFT.
Our full ﬁne-tuning approach, SCoFT, is a weighted sum of
all of the following loss functions:
4.1. Latent Diffusion Model Loss
Diffusion models are latent variable models that sample
from a desired distribution by reversing a forward noising
process. The latent noise at any timestep tis given by
zt=p↵tz0+p1 ↵t✏, where z0is the latent variable
10824
Stable	Diffusion	UNet
Iterative	Denoising	ProcessVAEDecoder(Frozen)
!!!!"#Denoising	with	Classifier-free	Guidance!!!!!!"#!$Record	first	gradientRecord	random	gradientPixel	SpaceStable	Diffusion	ControlNet(Frozen)
!"!!
!"Self-ContrastivePerceptual	LossCCUB	ImageDepth	Estimation
Latent	Space
ℒ#ℒ$ℒ%&'ℒ'
Figure 3. SCoFT Overview. A conventional ﬁne-tuning loss, LLDM, and memorization penalty loss, LM, are computed in the Stable
Diffusion latent space using images and captions from our CCUB dataset. After 20 denoising steps, the latent space is decoded. Perceptual
features are extracted from the generated image and compared contrastively to CCUB images as positive and non-ﬁne-tuned Stable Diffu-
sion images as negative examples to form our Self-Contrastive Perceptual Loss, LC.
encoded from the real image and ↵tis the strength of the
gaussian noise ✏. We are interested in the pretrained denois-
ing network ✏✓(zt,c,t), which denoises the noisy latent to
getzt 1conditioned on the text c. The training objective
for the denoiser ✏✓is minimizing the noise prediction:
LLDM(z,c)=E✏,z,c,t[wtk✏ ✏✓(zt,c,t)k](1)
where wtis the weight to control the noise schedules. A
pretrained model can be further trained, or ﬁne-tuned, using
the original objective with a new dataset. We use Low-Rank
Adaptation (LoRA) [ 21] to reduce memory demands and
over-ﬁtting by training new low-rank weight matrices.
4.2. Memorization Loss
Despite CCUB’s rich cultural content, its size still remains
small compared to LAION. The challenges of language
drift and decreased output diversity are prevalent issues en-
countered during this few-shot ﬁne-tuning [ 47]. Moreover,
CCUB’s text captions can be highly speciﬁc, as shown in
Fig.2. Fine-tuning solely on the CCUB dataset using Equa-
tion 1may lead to the undesirable outcome of reproduc-
ing training data as shown in Fig. 6. Our approach is in-
spired by [ 26], which proposed a model-based concept ab-
lation by letting the model memorize the mapping between
newly generated anchor images x?andc⇤, except we focus
on preventing memorization during ﬁne-tuning on a small
dataset (e.g., CCUB). We harness the property of BLIP au-
tomatic caption cbliponxccub, which is the naive version of
our cultural text prompt cccub, to regularize the model out-
puts conditioned on cccub, as shown in Fig. 6. To achieve
this, we introduce a memorization penalty loss leveraging
BLIP [ 28] generated captions of CCUB images. We utilize
multiple BLIP captions {cblip}to regularize the one-on-onemapping between CCUB images and cultural captions:
LM(xccub,cccub,cblip)=E✏,t[k✏(xccub,cccub,t)
 Ei[✏(xccub,ci
blip,t).sg()] k],(2)
where .sg() stands for a stop-gradient operation of the cur-
rent network to reduce memory cost.
4.3. Perceptual Loss
LLDM andLMare loss functions that operate on the la-
tent codes within a diffusion model. Operating directly on
these latent codes is ideal for ﬁne-tuning models for adding
simple concepts such as adding a character’s appearance to
the model. For adding more complex, abstract concepts,
like culture, we propose to decode the latent space into the
pixel space in order to utilize pre-trained perceptual sim-
ilarity models. We propose to use a perceptual loss, LP,
which is computed as the difference in extracted perceptual
features between the decoded, generated image, ˆx, and an
image from a training set, x:
LP(ˆx, x)=Eˆx,x[S(ˆx, x;f✓)] (3)
where Sis some perceptual similarity function and f✓is a
pretrained feature extractor.
Backpropagation through sampling. State-of-the-art per-
ceptual models typically process inputs in the pixel space.
In contrast, Stable Diffusion is ﬁne-tuned in a latent space.
To fulﬁll our objective function, an intuitive strategy en-
tails iteratively denoising latent features and then decoding
them back into the pixel space for use with perceptual mod-
els. Concurrent work [ 9] denoises the stable diffusion latent
from Gaussian noise into the pixel space based solely on
text prompts and by optimizing the Stable Diffusion UNet
10825
with a reward function computed using the decoded image.
Instead, our approach starts from the latent code at timestep
t:zt=p↵tz0+p1 ↵t✏, making it coupled with the Sta-
ble Diffusion ﬁne-tuning process. We utilize classiﬁer-free
guidance, iteratively denoising the latent code according to
(1 + w(t))✏(zt,cccub,t) w(t)✏(xt,?,t), where w(t)is
the guidance scale and ?is a null text embedding. This en-
ables the model to generate images conditioned on cultural
text prompts and unveils its cultural understanding, as illus-
trated in Fig. 3. In practice, we denoise ztfor 20 timesteps.
Starting from zt, our method ensures that the denoised im-
age aligns with the same pose and structure as the original
training image x0. This facilitates a more meaningful com-
parison for perceptual loss and subsequent self-contrastive
perceptual loss, revealing cultural differences.
Directly backpropagating through the multiple UNet de-
noising iterations, the latent space decoder, and the percep-
tual model incurs signiﬁcant memory and time costs. To
address this, we selectively record the gradient on a sin-
gle denoising step and employ stopgrad on other denois-
ing steps. Our ﬁndings indicate that recording the gradient
from the ﬁrst step has the most signiﬁcant impact on re-
ﬁning the model’s cultural understanding. Further compar-
isons on gradient recording and perceptual model backbone
are detailed in Sec. 6.
4.4. Self-Contrastive Perceptual Loss
To further improve Perceptual Loss, we raise an intriguing
question: Can Stable Diffusion leverage its intrinsic biases
to reﬁne its own? We seek to leverage the model’s prior of
its cultural understanding and propose a contrastive learn-
ing approach. Utilizing our CCUB dataset, we designate
the positive examples to be {x+|xccub}representing pre-
ferred features. To unveil the cultural biases within Stable
Diffusion, we employ images generated by the model itself
as negative samples.
It is imperative to ensure that the generated negative
samples share a high-level similarity with positive samples,
such as pose and structure, thereby emphasizing that the
primary distinctions lie in the diffusion model’s percep-
tion of cultural features. We achieve this by incorporat-
ing a pre-trained ControlNet [ 61] module ⇥c, conditioned
on the estimated depth of {x+}, into Stable Diffusion.
As depicted in Fig. 3, negative examples are obtained as
{x 
i|⇥c(D(x+),c)}, where Dis the MiDaS [ 6,44] depth
estimator, and crepresents cblipfollowed by a cultural suf-
ﬁx (e.g., “in Korea”). In practice, we generate 5 negative
examples for each positive example, then utilize DreamSim
to ﬁlter out false negatives similar to the positive instances.
To enhance the cultural ﬁne-tuning process, our ob-
jective is to ensure that images generated by the current
model, denoted as ˆx0
✓thave closer perceptual distances to
positive examples and farther distances from negative ex-
amples. This reinforces the model’s alignment with pre-ferred cultural features, distancing itself from undesirable
biases in negative examples, indicated by: S(ˆx0
✓t,x+;f✓)>
S(ˆx0
✓t,x 
i;f✓), where Sis some perceptual similarity func-
tion and f✓is a pre-trained feature extractor. Thus, we for-
mulate this objective, which we call Self-Contrastive Per-
ceptual Loss, using triplet loss:
LC(ˆx, x+,x )=Eˆx,x+,x [max( S(ˆx, x+;f✓)
  S(ˆx, x ;f✓)+m,0)](4)
where  denotes the weights on negative examples, mis
the constant margin between positive and negative exam-
ples, and f✓is a feature extractor. We evaluate a variety of
state-of-the-art perceptual embeddings and report compari-
son results in Section 6.
5. Experiments
User Survey. Our goal of improving the cultural percep-
tion of generated images is a subjective metric largely de-
termined by members of a given identity group. To eval-
uate our performance on this criterion, we recruited peo-
ple with at least 5 years of cultural experience in each of
the 5 countries with survey questions speciﬁc to their self-
selected national cultural afﬁliation. A single page of the
survey form provides one description (prompt) and one im-
age made by four different generators using a common ran-
dom seed, for four total images. We compare four image
generators: Stable Diffusion against three ﬁne-tuned ab-
lations. All ﬁne-tunings were performed with the CCUB
dataset using LLDM along with one or more proposed loss
functions, see Tab. 1. For example, SCoFT+M is Stable
Diffusion ﬁne-tuned on CCUB using the sum of LLDMand
LMas a loss function. Each survey page has a total of four
survey items (rows that participants respond to) to rank im-
ages on (a) Description and Image Alignment, (b) Cultural
Representation, (c) Stereotypes, and (d) Offensiveness. Par-
ticipants rank the set of randomly ordered images from best
to worst image once for each item. An image labeled rank
1 signiﬁes both best aligned and least offensive, while rank
4 is least aligned and most offensive.
We quantitatively estimate the subjective perceived per-
formance of each method with Matrix Mean-Subsequence-
Reduced (MMSR) [ 33] model in crowd-kit [ 57], an estab-
lished algorithm [ 35] for noisy label aggregation, followed
by a weighted majority vote to aggregate labels across
workers, and then a simple majority vote aggregating labels
into rankings, thus MMSR+V ote (see Supp Sec. 11.2).
Automatic Metrics. In addition to the user survey, we use
Kernel Inception Distance (KID) [ 3] and CLIP Score [ 20]
to evaluate the quality of generated images. For automatic
evaluation to ablate SCoFT, we use 10 test prompts for each
culture, generating 20 images for each prompt.
10826
Stable	DiffusionSCoFT+MSCoFT+MPSCoFT+MPCNigerian	Culture
Mexico	CultureIndian	Culture
Chinese	Culture
Korean	Culture
Figure 4. Qualitative comparison of our SCoFT model ablated and compared to Stable Diffusion without ﬁne-tuning.
Automatic Metrics User Survey Results - Average Rank
Model Name LMLPLCKID- #
CCUBKID- #
COCOCLIP- "
ScoreBest
DescribedMost Culturally
RepresentativeLeast
StereotypicalLeast
Offensive
Stable Diffusion[ 46] 30.355 4.396 0.813 3.09 3.07 2.98 3.07
SCoFT+M X 22.643 4.711 0.802 2.57 2.56 2.66 2.59
SCoFT+MP XX 21.360 4.936 0.800 2.33 2.35 2.34 2.30
SCoFT+MPC XXX 19.621 4.819 0.799 1.83 1.84 1.91 1.78
Table 1. We compare our SCoFT ablations to Stable Diffusion using automatic metrics (Sec. 6) and a user survey (Sec. 5). Values in the
user survey results report average ranking of images across all ﬁve cultures where lower rankings indicate better results (KID is ⇥103)
6. Results
Qualitative Comparison. We qualitatively compare our
SCoFT model versus the original Stable Diffusion in Fig. 1.
SCoFT guides Stable Diffusion away from generating
stereotypes and misrepresentations of culture. For example,
many of the Stable Diffusion results for a “Photo of a tra-
ditional building, in ...” depict disheveled structures, which
promote a harmful stereotype that some cultures are poor or
simple, whereas SCoFT promotes more accurate and less
stereotypical buildings for each nation. To investigate the
effects of each loss function within SCoFT we also qual-
itatively compare each ablation in Fig. 4. We tend to see
the SCoFT models modernize generated images, which de-
creases harmful stereotypes.
User Survey Results. 51 survey participants from ﬁve
countries ranked images across four ablations by respond-
ing to each of the four survey items in Sec. 5on freshly
generated images. We had 13 Korean, 11 Chinese, 10 In-dian, 9 Nigerian, and 7 Mexican participants. The average
participant rankings are in Table 1.
We also ran the MMSR [ 33] noisy data labeling al-
gorithm across all responses (see Sec. 5, Supp. Sec.
11.2), ﬁnding a participant consensus ranking of: (1)
SCoFT+MPC (2) SCoFT+MP, (3) SCoFT+M, and ﬁnally
(4) Generic Stable Diffusion. MMSR found that partic-
pants reached an identical consensus when separately rank-
ing each ablation with respect each of the four survey items.
MMSR also found a participant consensus in which every
country individually agreed with the ranking above, with
the exception of India, which swapped (1) SCoFT+MP and
(2) SCoFT+MPC.
We convert the rankings into binary comparisons by iso-
lating two ablations and comparing their rankings. This
way, we can compare the effects of each of the loss func-
tions of SCoFT. SCoFT+M was ranked less offensive than
Stable Diffusion 63% of the time, SCoFT+MP was less of-
10827
Figure 5. Violin plot of participant rankings across the survey
items and countries. A wider strip means more answers with that
value. Each new loss in our ablation study improved the rankings,
and SCoFT+MPC is best. (Rank 1 is the best; 4, the worst)
fensive than SCoFT+M 56% of the time, and SCoFT+MPC
was less offensive than SCoFT+MP 62% of the time. We
see that each loss function contributed signiﬁcantly to de-
creasing the offensiveness of generated results, and this
trend continued for the other three survey items. We note
that the initial addition of ﬁne-tuning and the contrastive
loss produced more dramatic improvements in SCoFT com-
pared to adding perceptual loss.
We compare whole distributions of the rankings in Fig. 5.
Across survey items, we see very similar distributions. For
example, Stable Diffusion images were very commonly
ranked fourth for both Stereotypes and Cultural Representa-
tion. Participants in the Chinese and Korean surveys ranked
images with less variance than participants in the Indian and
Mexican surveys. This is potentially due to a difference in
the number of participants for each survey.
Automatic Metric Results. The automated evaluation re-
sults in Table 1show that our proposed approach achieves
the highest KID score on the CCUB test set, indicating that
the ﬁne-tuned model is able to generate images with a sim-
ilar quality to the culturally curated data. In contrast, the
original Stable Diffusion model scores highest in the CLIP
Score and KID score on the MS COCO dataset. This result
is not surprising as the CLIP model is itself known to be
biased [ 4] in ways shared with Stable Diffusion [ 32], where
the number of measurable outputs people perceive as hatred
scales with the training set [ 5]. CLIP biases have also been
quantitatively shown to be passed on to downstream ap-
plications [ 22]. Human evaluators favor our SCoFT+MPC
method over Stable Diffusion, suggesting CLIP-Score’s in-
adequacy in assessing cultural competence.
Perceptual Backbones and Gradient Recording. SCoFT
uses a perceptual backbone to compare images in feature
spaces rather than pixel space to avoid overﬁtting to train-
!!!"#:	“a	woman	is	offering	Chinese	tea	to	another	Chinese	woman	in	cheongsam”!#$%&:	two	girls	sitting	at	a	table,	in	China
Memorizing	and	Overfitting
Correct	and	Non-memorizing
conditioned	on	!!!"#?!!"#Figure 6. Top-right: Fine-tuning Stable Diffusion on CCUB data
using only a conventional loss ( LLDM, Sec. 4.1) leads to overﬁt-
ting on CCUB captions. Bottom-right: Adding memorization loss
(LM, Sec. 4.2) prevents overﬁtting with small datasets by ensuring
images generated by general captions ( cblip) are similar to those
generated using CCUB’s cultural captions.
ing images. We test several backbones to extract image fea-
tures, including the output of CLIP [ 42] convolutional lay-
ers [58], and the last layer of DreamSim [ 14], DINOv2 [ 40],
BLIP2 [ 29]. For each comparison, we generate 200 im-
ages and calculate the KID with CCUB test set with the
Korean, Chinese, and Indian models and see that the CLIP
convolutional layers and DreamSim output provide the best
generalization. For all other experiments, we use the CLIP
convolutional layers as the SCoFT backbone.
We also compare the effect of recording the gradient dur-
ing the ﬁrst, last, and random iterations of denoising during
ﬁne-tuning, as reported in Fig. 7. We see that the best gener-
alization comes when recording the gradient during the ﬁrst
iteration of denoising. This is in contrast to concurrent work
[9] which recorded the last gradient, indicating that culture
is a high-level concept where important information is cre-
ated early in the diffusion process, as opposed to aesthetics
which are low-level and correspond more strongly to later
stages of diffusion.
Effectiveness of the Memorization Loss. We ﬁne-tune
Stable Diffusion using the original LLDMwith and without
Memorization Loss, LM, on the CCUB dataset. To evalu-
ate the consequence of over-ﬁtting and reproducing training
images during few-shot ﬁne-tuning, we randomly select 10
text-image pairs for each culture from the CCUB training
set. For each training text prompt, we generate 20 images.
We evaluate the process using three metrics: CLIP-Image
(CLIP-I), DINO, and DreamSim. All metrics measure the
average pairwise cosine similarity between the embeddings
of generated and real training images that share the same
text prompt. For both metrics, lower values indicate more
effective prevention of overﬁtting and reproduction of train-
ing images. Fig. 6depicts qualitative results of generating
creative images. Quantitative results in Table 2show that
10828
Prompts from Training set Prompts from Test set
Method CLIP-I #DINO #DreamSim #DIV train "DIV test "CLIPScore "
Finetune 0.912 0.836 0.591 0.302 0.317 0.824
Finetune w/ LM 0.897 0.808 0.550 0.356 0.379 0.814
Table 2. Images generated from Stable Diffusion ﬁne-tuned with and without Memorization Loss, LM, are compared in the feature space
from various feature extractors. We see that LMencourages the model to produce images with more diversity (larger feature difference).
Figure 7. Comparison of perceptual models used for feature ex-
traction in SCoFT. KID between held out CCUB images and gen-
erated images is plotted versus training iterations representing gen-
eralization to a validation set, score averaged across three cultures.
Figure 8. Generated images for text prompt: “a photo of a person.”
The proposed approach is able to generate more diverse images
given a generic prompt without a speciﬁc cultural context.
the memorization loss effectively reduces overﬁtting.
To quantify output diversity, we randomly select 10
training text prompts and 10 CCUB testing text prompts.
For each text prompt, we generate 20 images. We intro-
duce the diversity metric (DIV), which calculates the av-
erage pair-wise DreamSim cosine distance between gener-
ated images with the same text prompt. Higher values indi-
cate enhanced diversity in the generated outputs, reﬂecting
a more varied and expressive synthesis of images. We also
report a comparable CLIP Score on the generated image us-
ing CCUB testing text prompts with baseline ﬁne-tuning.
Limitations. To tackle the bias in the data, we aim for
two goals: 1) to generate accurate images given a speciﬁccultural context and 2) to generate diverse images given a
generic text prompt without any speciﬁc cultural context.
Our current approach is focused on achieving the ﬁrst goal.
Our current model can generate promisingly diverse images
for some generic prompts as shown in Figure 8when com-
pared to the baseline model that generates biased images.
Our CCUB dataset was collected by experienced resi-
dents; however more vigorous veriﬁcation will be needed
to improve the quality of the dataset. (See Supp. Sec. 13)
7. Conclusion
The biases of generative AI have already led to substantial
and very public impacts [ 19,34], so it is essential that we
ensure models generate images that more accurately repre-
sent the diversity of the world. We propose Self-Contrastive
Fine-Tuning (SCoFT), which is speciﬁcally designed to
ﬁne-tune the model for high-level concepts using a small
dataset, for instance, to pay attention to subtle cultural el-
ements. SCoFT has potential to generalize to applications
in other domains, such as, reducing the risk of copyright
infringement, better respecting cultural and community-
deﬁned boundaries, and addressing offensiveness across a
broader range of identity characteristics, amongst other cri-
teria. For example, supplement Fig. 19shows initial results
improving images for individuals with disabilities who use
mobility aids. We have also conﬁrmed positive associations
with some automated metrics while demonstrating others
are not a good ﬁt for this task.
Our extensive user survey and metric evaluation quan-
titatively demonstrate improvements in subjective metrics
with respect to image and description alignment, more cul-
turally representative image contents, as well as reductions
in stereotyping and offensiveness.
Acknowledgment. We thank Youeun Shin, Lia Coleman, and
all contributors for their contributions to our dataset. This
work was funded in part by NSF IIS-2112633, the Technol-
ogy Innovation Program (20018295, Meta-human: a virtual
cooperation platform for specialized industrial services) of the
Ministry of Trade, Industry & Energy (MOTIE, Korea), the
Ministry of Science and ICT, Korea, under the Information
Technology Research Center support program (IITP-2024-
2020-0-01789), and the Artiﬁcial Intelligence Convergence
Innovation Human Resources Development (IITP-2024-RS-
2023-00254592) supervised by the Institute for Information &
Communications Technology Planning & Evaluation, and the
NSF under Grant # 2030859 to the Computing Research Associa-
tion CIFellows Project subaward #2021CIF-CarnegieMellon-72.
10829
References
[1]Jason Armitage, Endri Kacupaj, Golsa Tahmasebzadeh,
Swati, Maria Maleshkova, Ralph Ewerth, and Jens Lehmann.
Mlm: a benchmark dataset for multitask learning with mul-
tiple languages and modalities. In Proceedings of the 29th
ACM International Conference on Information & Knowledge
Management , pages 2967–2974, 2020. 2
[2]Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended
latent diffusion. ACM Transactions on Graphics (TOG) , 42
(4):1–11, 2023. 3
[3]Mikołaj Bi ´nkowski, Danica J Sutherland, Michael Arbel, and
Arthur Gretton. Demystifying mmd gans. arXiv preprint
arXiv:1801.01401 , 2018. 5
[4]Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahem-
bwe. Multimodal datasets: misogyny, pornography, and ma-
lignant stereotypes. arXiv preprint arXiv:2110.01963 , 2021.
2,7
[5]Abeba Birhane, Vinay Prabhu, Sang Han, and Vishnu Naresh
Boddeti. On hate scaling laws for data-swamps, 2023. 2,7
[6]Reiner Birkl, Diana Wofk, and Matthias M ¨uller. Midas v3.1
– a model zoo for robust monocular relative depth estimation.
arXiv preprint arXiv:2307.14460 , 2023. 5
[7]Mari Casta ˜neda. The power of (mis) representation: Why
racial and ethnic stereotypes in the media matter. Challeng-
ing inequalities: Readings in race, ethnicity, and immigra-
tion, 2018. 2
[8]Michelle Caswell, Alda Allina Migoni, Noah Geraci, and
Marika Cifor. ‘to be able to imagine otherwise’: commu-
nity archives and the importance of representation. Archives
and Records , 38(1):5–26, 2017. 1
[9]Kevin Clark, Paul Vicol, Kevin Swersky, and David J Fleet.
Directly ﬁne-tuning diffusion models on differentiable re-
wards. arXiv preprint arXiv:2309.17400 , 2023. 4,7
[10] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin John-
son. Redcaps: Web-curated image-text data created by the
people, for the people. arXiv preprint arXiv:2111.11431 ,
2021. 3
[11] Travis L Dixon and Daniel Linz. Overrepresentation and
underrepresentation of african americans and latinos as law-
breakers on television news. Journal of communication , 50
(2):131–154, 2000. 2
[12] Rawan Elbaba. Why on-screen representation matters, ac-
cording to these teens. PBS NewsHour , 14, 2019. 1
[13] Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang,
Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang Liu, Weichong
Yin, Shikun Feng, et al. Ernie-vilg 2.0: Improving text-to-
image diffusion model with knowledge-enhanced mixture-
of-denoising-experts. arXiv preprint arXiv:2210.15257 ,
2022. 3
[14] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy
Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dream-
sim: Learning new dimensions of human visual similar-
ity using synthetic data. arXiv preprint arXiv:2306.09344 ,
2023. 3,7
[15] Rohit Gandikota, Joanna Materzynska, Tingrui Zhou, Anto-
nio Torralba, and David Bau. Concept sliders: Lora adap-tors for precise control in diffusion models. arXiv preprint
arXiv:2311.12092 , 2023. 2
[16] Rohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna
Materzy ´nska, and David Bau. Uniﬁed concept editing in dif-
fusion models. In Proceedings of the IEEE/CVF Winter Con-
ference on Applications of Computer Vision (WACV) , pages
5111–5120, 2024. 2
[17] Noa Garcia, Yusuke Hirota, Yankun Wu, and Yuta
Nakashima. Uncurated image-text datasets: Shedding light
on demographic bias. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
6957–6966, 2023. 3
[18] Ben Halpern. The dynamic elements of culture. Ethics , 65
(4):235–249, 1955. 3
[19] Melissa Heikkil ¨a. The viral ai avatar app lensa undressed me
without my consent, 2022. Accessed on December 16, 2023.
8
[20] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
and Yejin Choi. CLIPScore: a reference-free evaluation met-
ric for image captioning. In EMNLP , 2021. 5
[21] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021. 2,3,4
[22] Andrew Hundt, William Agnew, Vicky Zeng, Severin Ka-
cianka, and Matthew Gombolay. Robots enact malignant
stereotypes. FAccT , page 743–756, 2022. 2,7
[23] Ben Hutchinson, Andrew Smart, Alex Hanna, Emily Den-
ton, Christina Greer, Oddur Kjartansson, Parker Barnes,
and Margaret Mitchell. Towards accountability for machine
learning datasets: Practices from software engineering and
infrastructure. In Proceedings of the 2021 ACM Confer-
ence on Fairness, Accountability, and Transparency , page
560–575, New York, NY , USA, 2021. Association for Com-
puting Machinery. 2
[24] Mary Ritchie Key and Bernard Comrie, editors. IDS. Max
Planck Institute for Evolutionary Anthropology, Leipzig,
2021. 3
[25] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Dif-
fusionclip: Text-guided diffusion models for robust image
manipulation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 2426–
2435, 2022. 3
[26] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli
Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating con-
cepts in text-to-image diffusion models. In ICCV , 2023. 4,
1
[27] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Shechtman, and Jun-Yan Zhu. Multi-concept customization
of text-to-image diffusion. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 1931–1941, 2023. 3
[28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for uni-
ﬁed vision-language understanding and generation. arXiv
preprint arXiv:2201.12086 , 2022. 4,1
[29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
10830
frozen image encoders and large language models. arXiv
preprint arXiv:2301.12597 , 2023. 3,7,1
[30] Fangyu Liu, Emanuele Bugliarello, E. Ponti, Siva
Reddy, Nigel Collier, and Desmond Elliott. Visually
grounded reasoning across languages and cultures. ArXiv ,
abs/2109.13238, 2021. 3
[31] Haoming Lu, Hazarapet Tunanyan, Kai Wang, Shant
Navasardyan, Zhangyang Wang, and Humphrey Shi. Spe-
cialist diffusion: Plug-and-play sample-efﬁcient ﬁne-tuning
of text-to-image diffusion models to learn any unseen style.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 14267–14276, 2023.
3
[32] Sasha Luccioni, Christopher Akiki, Margaret Mitchell, and
Yacine Jernite. Stable bias: Evaluating societal representa-
tions in diffusion models. In Thirty-seventh Conference on
Neural Information Processing Systems Datasets and Bench-
marks Track , 2023. 2,7
[33] Qianqian Ma and Alex Olshevsky. Adversarial crowdsourc-
ing through robust rank-one matrix completion. In Advances
in Neural Information Processing Systems , pages 21841–
21852. Curran Associates, Inc., 2020. 5,6
[34] Emanuel Maiberg. Inside the ai porn marketplace where ev-
erything and everyone is for sale. 2,8
[35] Mohammad S. Majdi and Jeffrey J. Rodriguez. Crowd-
certain: Label aggregation in crowdsourced and ensemble
learning classiﬁcation, 2023. 5
[36] Abhishek Mandal, Susan Leavy, and Suzanne Little. Dataset
diversity: measuring and mitigating geographical bias in im-
age search and retrieval. In Proceedings of the 1st Interna-
tional Workshop on Trustworthy AI for Multimedia Comput-
ing, pages 19–25, 2021. 3
[37] Dana E Mastro and Bradley S Greenberg. The portrayal of
racial minorities on prime time television. Journal of Broad-
casting & Electronic Media , 44(4):690–703, 2000. 2
[38] Marc Miquel-Rib ´e and David Laniado. Wikipedia cultural
diversity dataset: A complete cartography for 300 language
editions. In Proceedings of the International AAAI Confer-
ence on Web and Social Media , pages 620–629, 2019. 3
[39] Eirini Ntoutsi, Pavlos Fafalios, Ujwal Gadiraju, Vasileios
Iosiﬁdis, Wolfgang Nejdl, Maria-Esther Vidal, Salvatore
Ruggieri, Franco Turini, Symeon Papadopoulos, Emmanouil
Krasanakis, et al. Bias in data-driven artiﬁcial intelligence
systems—an introductory survey. Wiley Interdisciplinary
Reviews: Data Mining and Knowledge Discovery , 10(3):
e1356, 2020. 2
[40] Maxime Oquab, Timoth ´ee Darcet, Theo Moutakanni, Huy V .
V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Rus-
sell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-
Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nico-
las Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou,
Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bo-
janowski. Dinov2: Learning robust visual features without
supervision, 2023. 3,7
[41] Justin Pinkney. Text to pokemon generator, 2022. 3
[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 3,7
[43] Deborah Raji, Emily Denton, Emily M. Bender, Alex Hanna,
and Amandalynne Paullada. Ai and the everything in the
whole wide world benchmark. In Proceedings of the Neu-
ral Information Processing Systems Track on Datasets and
Benchmarks . Curran, 2021. 3
[44] Ren´e Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence , 44(3), 2022. 5
[45] William Gaviria Rojas, Sudnya Diamos, Keertan Ranjan
Kini, David Kanter, and Vijay Janapa Reddi. The dollar
street dataset: Images representing the geographic and so-
cioeconomic diversity of the world. 2022. 2
[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models, 2021. 2,6
[47] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kﬁr Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. arXiv preprint arXiv:2208.12242 , 2022. 4
[48] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kﬁr Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 22500–
22510, 2023. 3
[49] Peter Schaldenbrand, James McCann, and Jean Oh.
Frida: A collaborative robot painter with a differentiable,
real2sim2real planning environment. In 2023 IEEE Inter-
national Conference on Robotics and Automation (ICRA) ,
pages 11712–11718. IEEE, 2023. 3
[50] Morgan Klaus Scheuerman, Alex Hanna, and Emily Denton.
Do datasets have politics? disciplinary values in computer
vision dataset development. Proc. ACM Hum.-Comput. In-
teract. , 5(CSCW2), 2021. 2
[51] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:
Open dataset of clip-ﬁltered 400 million image-text pairs.
arXiv preprint arXiv:2111.02114 , 2021. 2
[52] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for
training next generation image-text models. arXiv preprint
arXiv:2210.08402 , 2022. 3
[53] Adrienne Shaw. Identity, identiﬁcation, and media represen-
tation in video game play: An audience reception study . PhD
thesis, University of Pennsylvania, 2010. 1
[54] Makoto Shing and Kei Sawada. Japanese stable diffusion.
2022. 3
[55] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael
Bendersky, and Marc Najork. Wit: Wikipedia-based image
10831
text dataset for multimodal multilingual machine learning.
InProceedings of the 44th International ACM SIGIR Confer-
ence on Research and Development in Information Retrieval ,
pages 2443–2449, 2021. 2
[56] Harini Suresh and John Guttag. A framework for understand-
ing sources of harm throughout the machine learning life cy-
cle. In Proceedings of the 1st ACM Conference on Equity
and Access in Algorithms, Mechanisms, and Optimization ,
New York, NY , USA, 2021. Association for Computing Ma-
chinery. 2
[57] Dmitry Ustalov, Nikita Pavlichenko, and Boris Tseitlin.
Learning from crowds with crowd-kit, 2023. 5
[58] Yael Vinker, Ehsan Pajouheshgar, Jessica Y Bo, Ro-
man Christian Bachmann, Amit Haim Bermano, Daniel
Cohen-Or, Amir Zamir, and Ariel Shamir. Clipasso:
Semantically-aware object sketching. ACM Transactions on
Graphics (TOG) , 41(4):1–11, 2022. 7
[59] Bram Wallace, Akash Gokul, Stefano Ermon, and Nikhil
Naik. End-to-end diffusion latent optimization improves
classiﬁer guidance. arXiv preprint arXiv:2303.13703 , 2023.
3
[60] Angelina Wang, Solon Barocas, Kristen Laird, and Hanna
Wallach. Measuring representational harms in image cap-
tioning. In Proceedings of the 2022 ACM Conference on
Fairness, Accountability, and Transparency , page 324–335,
New York, NY , USA, 2022. Association for Computing Ma-
chinery. 2
[61] Lvmin Zhang and Maneesh Agrawala. Adding conditional
control to text-to-image diffusion models. arXiv preprint
arXiv:2302.05543 , 2023. 5
[62] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 586–595, 2018. 3
10832
