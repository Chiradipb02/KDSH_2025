NoiseCLR: A Contrastive Learning Approach for Unsupervised Discovery of
Interpretable Directions in Diffusion Models
Yusuf Dalva Pinar Yanardag
Virginia Tech
{ydalva, pinary }@vt.edu
Project webpage: https://noiseclr.github.io
Interpretable Directions Learned by NoiseCLR
Inputc) Cat (Breed), Face (Race #2)a) Face (Race #1, Glasses)b) Cat (Eye Color), Face (Mustache)
InputMustacheRace #2Race #1GlassesInputLionEye colorBreedFoxInput
Face
CatArtStyle #1Style #2Style #3Style #4Style #5Style #6Style #7Style #8Face + Cat
Figure 1. NoiseCLR. We propose an unsupervised approach to identify interpretable directions in text-to-image diffusion models, such as
Stable Diffusion [30]. Our method finds semantically meaningful directions across various domains like faces, cats , and art. NoiseCLR
can apply multiple directions either within a single domain (a) or across different domains in the same image (b, c) in a disentangled
manner. Since the directions learned by our model are highly disentangled, there is no need for semantic masks or user-provided guidance
to prevent edits in different domains from influencing each other. Additionally, our method does not require fine-tuning or retraining of the
diffusion model, nor does it need any labeled data to learn directions. Note that our method does not require any text prompts, the direction
names above are provided by us for easy understanding.
Abstract
Generative models have been very popular in the recent
years for their image generation capabilities. GAN-based
models are highly regarded for their disentangled latent
space, which is a key feature contributing to their success
in controlled image editing. On the other hand, diffusion
models have emerged as powerful tools for generating high-
quality images. However, the latent space of diffusion mod-els is not as thoroughly explored or understood. Existing
methods that aim to explore the latent space of diffusion
models usually relies on text prompts to pinpoint specific
semantics. However, this approach may be restrictive in ar-
eas such as art, fashion, or specialized fields like medicine,
where suitable text prompts might not be available or easy
to conceive thus limiting the scope of existing work. In this
paper, we propose an unsupervised method to discover la-
tent semantics in text-to-image diffusion models without re-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
24209
lying on text prompts. Our method takes a small set of unla-
beled images from specific domains, such as faces or cats,
and a pre-trained diffusion model, and discovers diverse se-
mantics in unsupervised fashion using a contrastive learn-
ing objective. Moreover, the learned directions can be ap-
plied simultaneously, either within the same domain (such
as various types of facial edits) or across different domains
(such as applying cat and face edits within the same image)
without interfering with each other. Our extensive experi-
ments show that our method achieves highly disentangled
edits, outperforming existing approaches in both diffusion-
based and GAN-based latent space editing methods.
1. Introduction
Denoising Diffusion Models (DDMs) [14] and Latent Dif-
fusion Models (LDMs) [30] have received considerable at-
tention for their ability in generating high-quality, high-
resolution images across a variety of domains. They have
achieved remarkable outcomes in the field of generative
modeling, particularly with text-to-image models like Sta-
ble Diffusion [30] which inspired researchers to employ
them for image editing tasks through text prompts or var-
ious conditions such as scribble or segmentation maps [44].
A fundamental aspect of image editing in generative
models is the disentangled application of semantics, which
involves making changes that are semantically significant
to specific areas of the image without affecting unintended
regions [23, 41]. Previous research has demonstrated that
generative adversarial networks (GANs) are particularly ef-
fective at disentangled image editing due to their structured
latent space, leading to significant research in both super-
vised and unsupervised exploration of the latent directions
in GANs [11, 32, 43].
However, while identifying directions in the latent space
of GANs is relatively straightforward, such as using prin-
cipal component analysis on sampled latent vectors to dis-
cover semantically meaningful directions [11], uncovering
directions in diffusion models in an unsupervised man-
ner is more challenging. This difficulty arises from the
inherent design of diffusion models, which estimates the
forward noise independently of the input and manage a
significant number of latent variables over several recur-
sive timesteps, unlike the more direct approach in GAN-
based models. Therefore, most of the prior work that pro-
vides fine-grained control over the generation process in
diffusion-based models focus on simple solutions such as
blending latent vectors, model fine-tuning, embedding op-
timization [1, 2, 12]. However, these methods depend on
user-provided text prompts to pinpoint specific semantics,
e.g.‘A photo of a woman with an eyeglass’ . This approach
can be restrictive in areas such as art, fashion where appro-
priate text prompts might not be straightforward to create,
Lipstick
Eyeglass
MustacheNoiseCLRA small set of unlabeled images from a speciﬁc domain (e.g. face)Learned directions that can perform editing to any new image
Pre-trained Diﬀusion Model (Stable Diﬀusion)Figure 2. NoiseCLR in a nutshell . Our method employs a pre-
trained diffusion model such as Stable Diffusion [30], alongside a
small collection of unlabeled images from a specific domain such
asfaces orcats and learns diverse directions in an unsupervised
fashion using a contrastive learning objective. The discovered di-
rections can perform disentangled edits, such as allowing for se-
mantically meaningful edits or adding lipstick oreyeglasses to any
new image.
or in specialized fields such as the medical domain, which
demand extensive domain knowledge to create appropriate
text prompts. This limitation highlights the significance of
discovering directions in the latent space in an unsupervised
manner.
A number of strategies have been introduced to sys-
tematically investigate the latent directions within diffusion
models [19, 26]. However, much of the existing research
acknowledges limitations when working with large mod-
els like Stable Diffusion, often opting for simpler diffusion
models such as DDPM [8, 19, 26]. These methods fail to
fully exploit the capabilities of large-scale models such as
Stable Diffusion and rely on separate DDPM models for
each domain to identify directions. Therefore, despite sig-
nificant advancements, a thorough exploration of the latent
space in large diffusion-based models like Stable Diffusion
remains an ongoing challenge. Discovering directions in la-
tent space of a diffusion-based models is essential not only
in the context of image editing but also for a broad spectrum
of other applications. First, it allows more precise control
over the image generation process, thereby significantly en-
hancing the versatility and applicability of the model across
a variety of creative and specialized domains. Second, this
approach fosters a more transparent and insightful explo-
ration, demystifying what is often seen as a ‘black-box’
model, thus making its latent space more understandable.
Thirdly, these insights enhance trust and reliability in the
model and could be instrumental in identifying and mitigat-
ing potential biases, thus fostering further research in the
ethical domain.
To the best of our knowledge, our approach is the
first unsupervised method that successfully discovers direc-
tions in the latent space of Stable Diffusion in a disentan-
gled manner to the extent of combining multiple directions
within and across various domains (see Fig. 1). Our contri-
24210
butions are as follows:
• We propose NoiseCLR, a contrastive-learning based
framework to discover semantic directions in a pre-
trained text-to-image diffusion model such as Stable Dif-
fusion. Our approach does not need textual prompts, la-
beled data, or user-guidance, relying on a relatively small
number of images (around 100) related to the target do-
main (see Fig. 2).
• Our method demonstrates the ability to discover diverse
and fine-grained directions across diverse categories, such
as face, cars, cats, and artwork.
• Our directions are highly disentangled, can apply mul-
tiple directions either within a single domain or across
different domains. Our experiments demonstrate that
our method can perform edits that are competitive with
both state-of-the-art diffusion-based and GAN-based im-
age editing methods.
2. Related Work
Latent Space Exploration of GANs. Various techniques
have emerged that harness the latent space of GANs for
image manipulation [5, 6, 28]. Supervised methods often
leverage pre-trained attribute classifiers to guide the op-
timization, facilitating the discovery of meaningful direc-
tions within the latent space. Alternatively, they employ la-
beled data to cultivate classifiers aiming directly at learning
desired directions [7, 32]. Conversely, some studies have
demonstrated the potential to identify semantically mean-
ingful directions within the latent space without supervision
[15, 31, 36, 38, 43]. More recent work on GAN-based latent
space explorations are pivoting towards utilizing image-text
alignment methods such as StyleCLIP [27].
Latent Space Exploration of Diffusion Models. As
diffusion-based image generation models are able to syn-
thesize images from various domains, they encode seman-
tically rich content in the form of a latent representations.
In order to benefit from these representations, studies at-
tempted to make use of the semantics encoded in the la-
tent space. As a natural extension of latent space discov-
ery, some works [19, 39] attempted to apply image editing
by modifying the backward diffusion path using represen-
tations learned from the latent variables. While [19] for-
mulate their transformation relying on the features learned
by the bottleneck block of the denoising model, [39] ap-
ply modulation to the latent variables for a target domain,
using stochastic diffusion models. In more recent efforts,
[26] offered a framework to discover latent-specific direc-
tions encoding different semantics, inspired by latent space
discovery literature in GANs. Even though their approach
succeeds in discovering directions in single-domain diffu-
sion models such as DDPMs, their proposed method failsin large-scale diffusion models, such as Stable Diffusion.
Moreover, [21] decomposes images into a set of compos-
able energy functions representing concepts such as light-
ingorcamera position . However, their approach is partic-
ularly limited in terms of the number of concepts that can
be learned due to memory constraints of their method, and
they can only learn conceptualized representations rather
than fine-grained latent directions.
Image Editing with Diffusion Models. The field of im-
age generation has seen a growing interest towards utiliz-
ing diffusion models for editing tasks. One common ap-
proach involves supplying text prompts describing the in-
tended edit. Yet, many implementations result in entangled
edits, where unintended sections of the image are altered
alongside the target area. Exceptions to this trend can be
seen in works like [12, 44] which demonstrate more precise
control over the editing process. For instance, ControlNet
[44] leverages conditional diffusion model to allow users
to manipulate specific image attributes by providing condi-
tions. Likewise, [37] allows to perform content-preserving
edits by overfitting the diffusion model to the input image.
Additionally, [10, 24, 40] offer faithful reconstruction of
the input image, which makes content-preserving edits with
classifier-free guidance possible. Despite being able to pre-
serve the input image while editing, such methods require
per-image optimization, which is a bottleneck against real-
time image editing. In recent efforts, [39] attempts to per-
form the image editing task by modifying the denoising pro-
cess of a stochastic diffusion model for the real-editing task.
Even though such approaches promise realistic image edit-
ing, constructing the ideal prompt for editing is a bottleneck
against achieving realistic edits while staying faithful to the
original image. To address the flexibility problem, [2, 20]
proposed to compose the desired edit into multiple coun-
terparts. However, these methods face difficulties when ap-
plying multiple edits, resulting in entangled results when
several changes are made to the same image.
Contrastive Learning. Contrastive learning has gained
traction recently, achieving state-of-the-art results in vari-
ous unsupervised representation learning tasks. Its princi-
ple lies in learning representations by contrasting positive
pairs against negative ones [9]. This approach has found
applications in numerous computer vision tasks, including
data augmentation [3], diverse scene generation [34], ran-
dom cropping, and flipping [25]. In the context of diffusion
models, approaches structured with a contrastive setup en-
abled tasks such as style transfer [42], and representation
learning [35]. LatentCLR [43] introduces a method of con-
trastive learning to identify latent directions in GAN-based
models by exploring feature divergences within an interme-
diate representation of latent vectors. While sharing similar
24211
Edits using dK
Edits using d1…Denoising Network(SD)
Denoising Network(SD)
RepelAttractAttract
x T timestepsUnlabeled Images
NoiseCLR
Figure 3. NoiseCLR Framework. NoiseCLR employs a contrastive objective to learn latent directions in an unsupervised manner. Our
method utilizes the insight that similar edits in the noise space should attract to each other, whereas edits made by different directions
should be repelled from each other. Given Nunlabeled images from an particular domain such as facial images, we first apply the forward
diffusion process for ttimesteps. Then, by using the noised variables {x1, ..., xN}, we apply the denoising step, conditioning this step
with the learned latent directions. Our method discovers Klatent directions d1, . . . , d Kfor a pretrained denoising network such as Stable
Diffusion, where directions correspond to semantically meaningful edits such as adding a lipstick .
intuitions in terms of utilizing contrastive learning for di-
rection discovery, our approach diverges from LatentCLR.
Unlike LatentCLR, which operates on latent vectors sam-
pled from the GAN model, we focus on noise estimations,
spanning across multiple diffusion steps. Notably, discov-
ering directions in text-to-image diffusion models is con-
siderably more complex than in GANs. This complexity
arises because diffusion models independently estimate for-
ward noise, irrespective of the input, and maintains a sig-
nificant amount of latent variables across several recursive
timesteps.
3. Method
In this section, we describe our proposed method, Noise-
CLR, on discovering interpretable directions. First, we
briefly discuss background on denoising probabilistic dif-
fusion models.
3.1. Denoising Probabilistic Diffusion Models
Diffusion models [14, 30, 33] are generative models that
produce data samples through an iterative denoising pro-
cess, which is often referred as the reverse process. The
reverse process involves a set of noise levels t∈ {1, ..., T},
ϵt=αtϵ, where ϵ∼ N(0,1). The denoising network, ϵθ,
is designed to estimate the noisy component ϵfrom noised
image xtduring the reverse process where xtrefers to the
noised version of the real image x0with a noise level of
ϵt. The objective function for training such a denoising net-
work is formulated as shown as:
LDM=Ex0,ϵt∼N(0,1),th
||ϵt−ϵθ(xt, t)||2
2i
(1)To generate an image using the denoising network ϵθ, the
reverse process is initiated with input xT∼ N (0,1).
Throughout the reverse diffusion process, the variable xt
is iteratively denoised to get x0where t∈ {1, ..., T}. The
iterative denoising process is formulated as Equation 2 for
step size γand timestep t.
xt−1=xt−γϵθ(xt, t) +ξ, ξ∼ N(0, σ2
tI) (2)
Classifier-free guidance [13] offers a way for conditioned
sampling through subtle adjustments in both forward and
backward diffusion processes with a specified condition
c. By training ϵθcompatible with classifier-free guidance,
conditional image generation becomes possible by mod-
ifying the noise prediction ϵθ(xt)with conditional noise
prediction, to get ˜ϵθ(xt, c). For simplicity, we use ϵθ(xt)
instead of ϵθ(xt, t)to represent the predicted noise for
timestep t, astis implicitly denoted with variable xt. The
predicted noise with classifier-free guidance, ˜ϵθ(xt, c), is
defined by Equation 3:
˜ϵθ(xt, c) =ϵθ(xt, ϕ) +λg(ϵθ(xt, c)−ϵθ(xt, ϕ)) (3)
where ϕis null-text and λgis guidance scale.
3.2. Contrastive Learning Objective
The primary objective of NoiseCLR is to learn Ksemanti-
cally meaningful directions, D={d1, . . . , d K}, given a
small set of Nimages, X={x1, . . . , x N}in diffusion
models in an unsupervised manner, where we formulate
each direction as a conditional embedding with the same
dimensionality with text embeddings. The intuition behind
24212
InputChildRace #3Race #1AgeCartoonLipstickEyebrows
MustacheFigure 4. Directions learned by NoiseCLR on face domain. Edits are performed using the directions learned by our method in an
unsupervised manner. We annotate the discovered directions above for the sake of understandability. The edits learned by our method are
both effective in domain examples (e.g. human faces) and out-of-domain images (e.g. paintings).
NoiseCLR is best explained as defining an objective that en-
courages the similarity of the edits done by an arbitrary di-
rection, while discouraging the similarity of edits performed
by different directions. In other words, we want for edits
carried out by the same direction to be attracted towards
each other, while edits conducted by different directions to
repel one another, in line with the core principles of con-
trastive learning. To formulate such an objective, we first
define the feature divergences ∆ϵn
kcaused by an arbitrary
direction dkon an arbitrary data sample xnas follows:
∆ϵn
k=ϵθ(xn
t, dk)−ϵθ(xn
t, ϕ) (4)
We define the target feature divergences obtained from
djand a set of data samples X′⊂Xas our positive
samples, whereas the target feature divergences for sample
xi∈X′and a set of latent directions D′⊂D−djare
selected as the negative samples. We formulate our con-
trastive learning objective in Equation 5:
Lj=−logP|X′|
a=1P|X′|
b=11[a̸=b]exp(sim(∆ϵa
j,∆ϵb
j)/τ)
P|X′|
a=1P|D′|
i=11[i̸=j]exp(sim(∆ϵa
j,∆ϵa
i)/τ)
(5)
To express the semantic similarity between a pair of target
feature differences, we use cosine similarity which is for-
mulated as:
sim(∆ϵa
j,∆ϵb
j) =∆ϵa
j·∆ϵb
j
||∆ϵa
j|| ||∆ϵb
j||(6)Image Editing. Given the set of discovered directions
{d1, . . . , d K}, our editing scheme aims to reflect these se-
mantics to input images in a disentangled manner. To per-
form such edits, we slightly modify Equation 3 with an
editing direction deto obtain ¯ϵθ(xt, c, d e)as formulated in
Equation 7, where cserves as the condition used to generate
the original image. Leveraging the observation that the dif-
ference ϵθ(xt, c)−ϵθ(xt, ϕ)encodes semantic information
encoded by the condition c, we expand the noise prediction
with the difference ϵθ(xt, de)−ϵθ(xt, ϕ)for the timestep
where editing will be performed:
¯ϵθ(xt, c, d e) = ˜ϵθ(xt, c) +λe(ϵθ(xt, de)−ϵθ(xt, ϕ))(7)
where λedenotes the editing scale.
Editing in Multiple Directions. Since we formulate edit-
ing for an arbitrary direction as a summation of predicted
noises for a given timestep t, we are able to perform multi-
ple edits to a given input variable xt. To perform a set Lof
discovered directions {d1, . . . , d L}, we formulate the edit-
ing term as a sum of noise predictions, ˆϵθ(xt, L), which is
formulated as:
ˆϵθ(xt, L) =|L|X
i=1λi(ϵθ(xt, di)−ϵθ(xt, ϕ)) (8)
24213
InputCar shapeCartoonBackgroundTruck
InputBreedLionFoxLeopard Pattern
InputDir #1Dir #2Dir #3Dir #4
Art
CatCarFigure 5. Editing results on various domains. To demonstrate the generalizability of our method across different domains, we provide
editing results on artistic paintings, cats and cars. As demonstrated from in the editing results, our method is able to learn and apply latent
directions from various domains using a single diffusion model. Note that we label the edits only for the sake of understandability.
Using ˆϵθ(xt, L), the overall noise prediction for timestep t
is formulated as ¯ϵθ(xt, c, L) = ˜ϵθ(xt, c) + ˆϵθ(xt, L).
Real Image Editing. In addition to performing edits on
generated images, we expand our editing approach such that
the discovered edits are applicable to real images. Different
than sampling fake images from xT∼ N(0,1), we initially
apply DDIM Inversion [24] to obtain this initial variable xT.
Using this inverted variable, we reformulate ¯ϵθ(xt, c, d e)as
¯ϵθ(xt, de)since the image is conditioned by the initial vari-
ablexTonly. The formulation for real image editing with a
single direction is provided as follows:
¯ϵθ(xt, de) =ϵθ(xt, ϕ) +λe(ϵθ(xt, de)−ϵθ(xt, ϕ))(9)
Note that, our approach in editing with multiple directions
is also applicable for real images.
4. Experiments
To assess the effectiveness of NoiseCLR in identifying se-
mantically meaningful latent directions and demonstrate the
generalizability of our method, we conducted evaluations
across various domains, including human faces, cats, cars,
and paintings.
Experimental Setup We used Stable Diffusion-v1.5 for
all of our experiments. We used several diverse datasets
including FFHQ [16], AFHQ-Cats [4], and Stanford Cars
datasets [18]. For the artistic domain, we perform our ex-
periments on a small subset of paintings to discover latentdirections corresponding to artistic styles. In our default
setting, we train NoiseCLR with N= 100 ,K= 100
andτ= 0.5. To optimize the directions, we use a learn-
ing rate of 10−3and batch size of 6 for AdamW optimizer
[22]. Throughout these experiments, we train our directions
with relatively modest dataset sizes such as 100 for each do-
main. For face and painting domains, we set the size the set
of directions to be learned as |D|= 100 and for the do-
mains of cats and cars, we set the number of directions to
be|D|= 50 . In each experiment, we set the size of the
subset of directions to be used |D′|at every iteration as 20.
To ensure the reproducibility of our experiments, we con-
duct all experiments with a fixed random seed of 0. More-
over, training our method on a single domain requires ap-
proximately 7 hours to learn 100 directions in face domain,
and once trained, performing any edit in a zero-shot manner
takes about 5 seconds using a single NVIDIA L40 GPU.
4.1. Qualitative Results
Unlike existing methods for exploring the latent space of
diffusion models, our approach can identify latent direc-
tions of various domains using a single diffusion model.
Since face images carry significant amount of variance in
terms of facial features and is one of the most popular type
of edits in both GAN and diffusion-based models, we first
investigate the face editing capabilities of the directions dis-
covered by NoiseCLR. Fig. 4 displays a variety of distinct
directions, including broad edits that can change the over-
all structure of the face, like aging orrace, as well as more
detailed directions that modify fine-grained facial features,
24214
Input+ Young+ Young+ Glasses+ Young+ Glasses + LipstickCycle-DiffusionOursSEGA
Input+ Young+ Young+ Glasses+ Young+ Glasses + Lipstick
Composable Diffusion
Concept Discovery
Figure 6. Qualitative Comparisons. We compare our method with Cycle-Diffusion[39], SEGA [2] and Composable Diffusion [20]
in terms of image editing capabilities and Unsupervised Concept Discovery [21] to assess the quality of the representations learned by
NoiseCLR. We present our comparisons for both real-image editing task and conditional image generation with the provided semantics.
As it can be observed from the presented qualitative results, NoiseCLR succeeds over competing methods both in terms of disentangled
image editing and learning fine-grained latent directions.
such as lipstick or amustache . We emphasize that our edits
are highly disentangled, meaning they achieve the intended
modification without affecting any unintended parts. Given
that our method conducts direction discovery in an entirely
unsupervised manner, it has the freedom to explore the se-
mantic space of the diffusion model during training. Con-
sequently, NoiseCLR can discover directions not explicitly
represented in the input dataset but still compatible with the
domain of the training images, such as cartoon direction.
Additionally, as NoiseCLR leverages the semantic under-
standing of the diffusion model, face edits generalize well
to artistic paintings, as demonstrated in the last row of Fig.
4. Besides the facial domain, we also showcase the efficacy
of our approach in the domains of art, cats, and cars. Qual-
itative results are presented in Fig. 5. As evident from the
figure, NoiseCLR is capable of learning diverse semantics
across various domains. This includes identifying multiple
artistic styles, directions for transforming cats into foxes or
lions, and directions for converting cars into trucks.
Qualitative comparisons. We compare our method with
recent approaches, namely, Cycle-Diffusion [39], SEGA[2], Composable Diffusion [20], and Unsupervised Concept
Discovery [21] methods. Over the compared methods, it is
evident that [39] and [2] struggles with the real image edit-
ing task, when multiple semantics are modified even though
the edits are performed in a disentangled manner. On the
other end, the edits performed by [20] manages to preserve
the edit quality, whereas they suffer preserving the image
contents. As can be seen in Fig. 6, NoiseCLR outperforms
the competing approaches both in terms of semantic faith-
fulness and disentanglement capabilities. For comparisons
with diffusion and GAN-based editing methods, please re-
fer to the supplementary material.
4.2. Quantitative Results
In evaluating the efficacy of discovered edits, we conduct
a re-scoring analysis on the directions representing the se-
mantics of “Indian”, “Asian”, “mustache”, “child” and “lip-
stick” which are arbitrarily selected from the directions dis-
covered by NoiseCLR. In our analysis, we assess the change
in classification probability of the CLIP classifier [29] in
the desired attribute and examine if the directions are dis-
24215
Indian Asian Mustache Child Lipstick
Indian 29.8 16.1 8.1 5.6 4.7
Asian -10.5 27.5 0.0 -2.0 1.2
Mustache 3.6 -7.6 48.9 -13.2 1.7
Child -37.7 -14.1 2.3 32.8 11.7
Lipstick -8.9 -3.4 7.3 -0.7 11.0
Table 1. Re-scoring Analysis. The change in classification prob-
ability of the CLIP classifier for various attributes. Bold numbers
indicate that NoiseCLR consistently enhances the target semantics
across all attributes. Additionally, our approach achieves disentan-
gled editing by minimizing its influence on other attribute scores
when modifying a single attribute.
entangled between each other (see Table 1). In the pre-
sented scores, an increase corresponds to increased classi-
fication confidence for the subjected semantic, whereas a
decrease implies the decrease of the presence of the seman-
tic. Ideally, we expect the scores of the unedited semantics
to change minimally, whereas the confidence for the edited
semantic should increase. Relying on the re-scoring anal-
ysis performed, we consider our edits disentangled as the
semantics that are not naturally related do not change sig-
nificantly. However, we also acknowledge that applying the
edit of ”Child” significantly changes the race-based edits
(“Indian”, “Asian”). We relate this due to internal biases
in Stable Diffusion. Moreover, we compared LPIPS [45]
scores to measure how well the similarity to the original im-
age distribution is maintained. Table 3 shows the results for
several edits. The LPIPS metrics clearly demonstrate that
our method consistently achieves lower LPIPS scores com-
pared to other approaches, signifying improved coherence
during the editing process.
User Study. We assess the editing capabilities of our
model through an user study conducted on 50 participants
using Amazon Mturk platform. For each of the methods
that we compare with, we show volunteers several edits
performed with common semantics and asked to determine
if they consider the performed edit successful in terms
of the given semantic, and if the edit is performed in a
disentangled way. For each question, the users are asked
to give a rating between 1-5 to indicate their preference
where 5 means the highest score (see Table 2). Our results
demonstrate that NoiseCLR attained higher scores in both
edit quality and disentanglement evaluations, underscoring
the superior performance of our approach.
5. Limitations
Our method is built upon the pre-trained Stable Diffu-
sion model. Consequently, its manipulation capabilitiesMethod Edit Quality ↑Disentanglement ↑
Composable D. 2.19 2.15
Concept D. 1.20 1.28
SEGA 1.52 1.81
Cycle-Diffusion 1.87 2.73
Ours 2.65 3.05
Table 2. User Study Results. The average response score of the
participants are provided in the table. The scoring is performed
within the scale of 1-to-5.
Method Age Mustache Gender Race
Composable D. 0.19 0.40 0.42 0.40
Cycle-Diffusion 0.10 0.21 0.23 0.26
SEGA 0.11 0.23 0.27 0.27
Ours 0.17 0.17 0.20 0.13
Table 3. LPIPS [45] scores (lower is the better). Our method
is able to achieve lower LPIPS than the other methods, indicating
greater coherence while performing the edits.
are heavily dependent on the datasets Stable Diffusion was
trained on, as well as the language model CLIP utilized by
Stable Diffusion. While the joint representation capabili-
ties of CLIP are impressive, they also have limitations and
can exhibit biases towards certain attributes (e.g. entangle-
ment between background direction and car shape attribute
in Fig. 5). Furthermore, similar to other image synthesis
tools, our framework raises concerns about potential mis-
use for malicious purposes [17].
6. Conclusion
We present an approach to discover to discover latent di-
rections in large text-to-image diffusion models in an un-
supervised way using a novel contrastive learning frame-
work. Our method can combine multiple directions within
and across various domains, such as face, cars, cats and art-
work. Our experiments demonstrate that our method can
perform edits that are competitive with both state-of-the-
art diffusion-based and GAN-based image editing meth-
ods. Our approach not only provides more precise control
over the image generation process, greatly expanding the
model’s versatility and usability in diverse creative and spe-
cialized fields, but it also promotes a more transparent and
insightful exploration. This helps to demystify what is of-
ten perceived as a ‘black-box’ model. Furthermore, these
insights increase trust and reliability in the model and could
play a crucial role in identifying and addressing potential
biases, thereby encouraging further research in ethical con-
siderations.
24216
References
[1] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended
latent diffusion. ACM Transactions on Graphics (TOG) , 42
(4):1–11, 2023. 2
[2] Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas
Struppek, Patrick Schramowski, and Kristian Kersting.
SEGA: Instructing text-to-image models using semantic
guidance. In Thirty-seventh Conference on Neural Informa-
tion Processing Systems , 2023. 2, 3, 7
[3] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In International conference on ma-
chine learning , pages 1597–1607. PMLR, 2020. 3
[4] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha.
Stargan v2: Diverse image synthesis for multiple domains.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , 2020. 6
[5] Yusuf Dalva, Said Fahri Altındis ¸, and Aysegul Dundar. Vec-
gan: Image-to-image translation with interpretable latent di-
rections. In European Conference on Computer Vision , pages
153–169. Springer, 2022. 3
[6] Yusuf Dalva, Hamza Pehlivan, Oyku Irmak Hatipoglu,
Cansu Moran, and Aysegul Dundar. Image-to-image transla-
tion with disentangled latent vectors for face editing. IEEE
Transactions on Pattern Analysis and Machine Intelligence ,
2023. 3
[7] Lore Goetschalckx, Alex Andonian, Aude Oliva, and Phillip
Isola. Ganalyze: Toward visual definitions of cognitive im-
age properties. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 5744–5753,
2019. 3
[8] Ren ´e Haas, Inbar Huberman-Spiegelglas, Rotem Mulayoff,
and Tomer Michaeli. Discovering interpretable directions in
the semantic latent space of diffusion models. arXiv preprint
arXiv:2303.11073 , 2023. 2
[9] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensional-
ity reduction by learning an invariant mapping. In 2006 IEEE
Computer Society Conference on Computer Vision and Pat-
tern Recognition (CVPR’06) , pages 1735–1742. IEEE, 2006.
3
[10] Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng
Song, Mengwei Ren, Ruijiang Gao, Yuxiao Chen, Di Liu,
Qilong Zhangli, et al. Improving negative-prompt inversion
via proximal guidance. arXiv preprint arXiv:2306.05414 ,
2023. 3
[11] Erik H ¨ark¨onen, Aaron Hertzmann, Jaakko Lehtinen, and
Sylvain Paris. Ganspace: Discovering interpretable gan con-
trols. arXiv preprint arXiv:2004.02546 , 2020. 2
[12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
age editing with cross attention control. arXiv preprint
arXiv:2208.01626 , 2022. 2, 3
[13] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. arXiv preprint arXiv:2207.12598 , 2022. 4
[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840–6851, 2020. 2, 4[15] Ali Jahanian, Lucy Chai, and Phillip Isola. On the” steer-
ability” of generative adversarial networks. arXiv preprint
arXiv:1907.07171 , 2019. 3
[16] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 4401–4410, 2019. 6
[17] Pavel Korshunov and S ´ebastien Marcel. Deepfakes: a new
threat to face recognition? assessment and detection. arXiv
preprint arXiv:1812.08685 , 2018. 8
[18] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
3d object representations for fine-grained categorization. In
4th International IEEE Workshop on 3D Representation and
Recognition (3dRR-13) , Sydney, Australia, 2013. 6
[19] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion
models already have a semantic latent space. arXiv preprint
arXiv:2210.10960 , 2022. 2, 3
[20] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and
Joshua B Tenenbaum. Compositional visual generation with
composable diffusion models. In Computer Vision–ECCV
2022: 17th European Conference, Tel Aviv, Israel, Octo-
ber 23–27, 2022, Proceedings, Part XVII , pages 423–439.
Springer, 2022. 3, 7
[21] Nan Liu, Yilun Du, Shuang Li, Joshua B. Tenenbaum, and
Antonio Torralba. Unsupervised compositional concepts dis-
covery with text-to-image generative models. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion (ICCV) , pages 2085–2095, 2023. 3, 7
[22] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 , 2017. 6
[23] Emile Mathieu, Tom Rainforth, Nana Siddharth, and
Yee Whye Teh. Disentangling disentanglement in variational
autoencoders. In International conference on machine learn-
ing, pages 4402–4412. PMLR, 2019. 2
[24] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and
Daniel Cohen-Or. Null-text inversion for editing real im-
ages using guided diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 6038–6047, 2023. 3, 6
[25] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-
sentation learning with contrastive predictive coding. arXiv
preprint arXiv:1807.03748 , 2018. 3
[26] Yong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo Jo,
and Youngjung Uh. Understanding the latent space of diffu-
sion models through the lens of riemannian geometry. arXiv
preprint arXiv:2307.12868 , 2023. 2, 3
[27] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,
and Dani Lischinski. Styleclip: Text-driven manipulation of
stylegan imagery. arXiv preprint arXiv:2103.17249 , 2021. 3
[28] Hamza Pehlivan, Yusuf Dalva, and Aysegul Dundar.
Styleres: Transforming the residuals for real image editing
with stylegan. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 1828–
1837, 2023. 3
[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
24217
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 7
[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684–10695, 2022. 1, 2, 4
[31] Yujun Shen and Bolei Zhou. Closed-form factorization of
latent semantics in gans. arXiv preprint arXiv:2007.06600 ,
2020. 3
[32] Yujun Shen, Ceyuan Yang, Xiaoou Tang, and Bolei Zhou.
Interfacegan: Interpreting the disentangled face representa-
tion learned by gans. IEEE Transactions on Pattern Analysis
and Machine Intelligence , 2020. 2, 3
[33] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 4
[34] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Con-
trastive multiview coding. arXiv preprint arXiv:1906.05849 ,
2019. 3
[35] Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and
Dilip Krishnan. Stablerep: Synthetic images from text-to-
image models make strong visual representation learners. In
NeurIPS , 2023. 3
[36] Paul Upchurch, Jacob Gardner, Geoff Pleiss, Robert Pless,
Noah Snavely, Kavita Bala, and Kilian Weinberger. Deep
feature interpolation for image content changes. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 7064–7073, 2017. 3
[37] Dani Valevski, Matan Kalman, Eyal Molad, Eyal Segalis,
Yossi Matias, and Yaniv Leviathan. Unitune: Text-driven
image editing by fine tuning a diffusion model on a single
image. 42(4), 2023. 3
[38] Andrey V oynov and Artem Babenko. Unsupervised discov-
ery of interpretable directions in the gan latent space. In In-
ternational Conference on Machine Learning , pages 9786–
9796. PMLR, 2020. 3
[39] Chen Henry Wu and Fernando De la Torre. A latent space
of stochastic diffusion models for zero-shot image editing
and guidance. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 7378–7387, 2023. 3,
7
[40] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale,
Trung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu
Chang. Uncovering the disentanglement capability in text-
to-image diffusion models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 1900–1910, 2023. 3
[41] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei
Zhou, and Ming-Hsuan Yang. Gan inversion: A survey.
IEEE Transactions on Pattern Analysis and Machine Intel-
ligence , 45(3):3121–3138, 2022. 2
[42] Serin Yang, Hyunmin Hwang, and Jong Chul Ye. Zero-shot
contrastive loss for text-guided diffusion image style transfer.
InProceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV) , pages 22873–22882, 2023. 3[43] O ˘guz Kaan Y ¨uksel, Enis Simsar, Ezgi G ¨ulperi Er, and Pinar
Yanardag. Latentclr: A contrastive learning approach for un-
supervised discovery of interpretable directions. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision (ICCV) , pages 14263–14272, 2021. 2, 3
[44] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3836–3847, 2023. 2, 3
[45] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 586–595, 2018. 8
24218
