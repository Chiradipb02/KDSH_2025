Data-Efficient Unsupervised Interpolation
Without Any Intermediate Frame for 4D Medical Images
JungEun Kim1*Hangyul Yoon1*Geondo Park1Kyungsu Kim2†Eunho Yang1,3
1Korea Advanced Institute of Science and Technology (KAIST)
2Massachusetts General Hospital and Harvard Medical School3AITRICS
{jungeun122333, hangyulmd, geondopark, eunhoy }@kaist.ac.kr kskim.doc@gmail.com
Abstract
4D medical images, which represent 3D images with
temporal information, are crucial in clinical practice for
capturing dynamic changes and monitoring long-term dis-
ease progression. However, acquiring 4D medical images
poses challenges due to factors such as radiation expo-
sure and imaging duration, necessitating a balance be-
tween achieving high temporal resolution and minimizing
adverse effects. Given these circumstances, not only is data
acquisition challenging, but increasing the frame rate for
each dataset also proves difficult. To address this challenge,
this paper proposes a simple yet effective Unsupervised
Volumetric Interpolation framework, UVI-Net. This frame-
work facilitates temporal interpolation without the need for
any intermediate frames, distinguishing it from the major-
ity of other existing unsupervised methods. Experiments on
benchmark datasets demonstrate significant improvements
across diverse evaluation metrics compared to unsuper-
vised and supervised baselines. Remarkably, our approach
achieves this superior performance even when trained with
a dataset as small as one, highlighting its exceptional ro-
bustness and efficiency in scenarios with sparse supervi-
sion. This positions UVI-Net as a compelling alternative
for 4D medical imaging, particularly in settings where data
availability is limited. The code is available at UVI-Net.
1. Introduction
Video Frame Interpolation (VFI) has been a cornerstone
in the realm of video processing, enriching motion visu-
alization by generating intermediate frames. This method
primarily relies on intermediate frame supervision, where
known frames are used as references to create new inter-
mediate frames. However, applying these VFI methods to
4D medical imaging is not trivial. While the principles of
frame interpolation hold the potential for enhancing med-
*Eqaul Contribution†Correspondence toical diagnostics and treatments [5, 21, 23, 41, 42, 66, 70],
the unique constraints and requirements of medical imaging
present challenges.
One significant challenge lies in obtaining a sufficient
dataset. Unlike general domain videos, 4D medical images
are captured for specific clinical purposes from a relatively
small pool of individuals. Similarly, acquiring intermediate
frames per image is also hampered by limitations and risks
associated with medical imaging modalities.
Computed tomography (CT) exposes patients to ele-
vated radiation levels, potentially increasing the risk of sec-
ondary cancer [67]. Similarly, magnetic resonance imaging
(MRI) faces the obstacle of lengthy scan times, lasting up
to an hour [56], presenting both logistical challenges and is-
sues related to patient comfort. Furthermore, the quality of
ground truth intermediate frames in medical imaging is of-
ten compromised due to factors such as patient movement,
unstable breathing, and the difficulty of maintaining a sta-
ble position during prolonged scans [7, 45], limiting data
variety and accessibility for research.
In light of these challenges, we present the following
question: “Can a VFI model be trained without depending
onanyground truth intermediate frames?”. Unlike other
previous unsupervised approaches in the 2D general do-
main [36, 39, 54] that interpolate frames given the multi-
ple frame sequences, we address the task of freely interpo-
lating between two given frames without any intermediate
frames. To achieve this, we propose a straightforward yet
effective framework to VFI in medical imaging. By inter-
polating the flow between two frames with a two-stage pro-
cess and cycle-consistency constraint, our framework can
effectively operate even with a video composited with two
frames (i.e., only images of the start and end points exist),
entirely in an unsupervised manner. In the initial stage, vir-
tual samples are generated from the two real input images.
Subsequently, the real images are reconstructed based on
these virtual intermediate samples. This reconstruction pro-
cess incorporates the candidate images and warped contex-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
11353
tual information in multiple scales from the input images.
Through this cyclic interpolation approach, we successfully
minimize discrepancies between the generated and the ac-
tual images by using the real images as a form of pseudo-
supervision.
Our proposed method has achieved state-of-the-art re-
sults in unsupervised VFI for 4D medical imaging, out-
performing the existing techniques with a substantial gap.
Our approach also consistently outperforms even for ex-
isting supervised methods. Remarkably, our model shows
competitive or even superior performance when trained
with a minimal training dataset size of just one, contrast-
ing with other baselines that require full datasets, typically
exceeding 60 in size. Additionally, the unsupervised na-
ture of our model allows for further performance enhance-
ments through instance-specific optimization. This process
involves briefly fine-tuning the model using each test sam-
ple during the inference stage, potentially yielding even
more refined results.
In summary, our contributions are three-fold:
• We introduce a simple yet effective unsupervised VFI ap-
proach for 4D medical imaging. Our methodology lever-
ages cycle consistency constraints within the temporal di-
mension, thereby obviating the need for ground truth data
typically required for interpolated images.
• Our approach achieves state-of-the-art performance, sur-
passing other unsupervised and supervised interpolation
methods. This is accomplished without the instance-
specific optimization, which could be employed as a vi-
able option to enhance performance.
• The robustness of our model is particularly evident under
conditions of limited data availability, as demonstrated by
the increasing performance margin relative to other meth-
ods when the dataset size is reduced.
2. Related Works
2.1. Video interpolation
Many studies in the field of video interpolation have been
conducted, with a significant emphasis on frame rate up-
sampling for natural scene videos [43, 50, 53]. These stud-
ies typically rely on ground truth intermediate frames for
training [11, 25, 26, 49, 51, 71, 75, 78]. While some stud-
ies have explored alternative approaches that do not rely
on ground truth intermediate frames, they involve synthe-
sizing frames between a given sequence of intermediate
frames [36, 39, 54] or utilize the information from special-
ized devices, such as event camera [19]. Consequently, ap-
plying these methods in settings like our study presents a
challenge, as there are no intermediate frames available for
synthesis. Furthermore, validation of these methods is re-
stricted to 2D frames, and they encounter challenges when
directly applied to volume sequences. This is primarily at-Data Type Name # of Total Inter
2D NaturalUCF101 [64] 2,374,290
X4K1000FPS [59] 277,704
Adobe240-fps [65] 79,768
Vimeo90K [72] 73,171
ATD-12K [61] 12,000
3D MedicalACDC [6] 2,556
4D-Lung [22] 648
Table 1. Comparison of representative 2D VFI datasets with 3D
medical VFI datasets in our study. The last column indicates the
total number of intermediate frames, representing the sum of in-
termediate frame counts across each dataset.
tributable to the markedly lower availability of intermediate
frames within such datasets, as elucidated in Tab. 1.
Medical 4D image interpolation. To address the above
challenges, frame interpolation methods specifically fo-
cused on 4D medical images are driven. Several recent
works [16, 17] have attempted to interpolate medical 4D im-
ages, but these methods rely on the availability of ground-
truth intermediate images for training. Although Kim and
Ye [30] proposed an interpolation approach without us-
ing the authentic intermediate frames, they do not incorpo-
rate an unsupervised learning technique for the interpolated
samples. Instead, their method involves a post-hoc multi-
plication of the flow calculation model, which is prone to
spatial distortion. This weakness arises since the underly-
ing network does not account for the structural smoothness
between two samples during network training. As a result,
the scaled calculated flow fails to capture the spatial con-
tinuity of intermediate samples beyond the samples pro-
vided by authentic frames. Furthermore, since the method
focus solely on warping without incorporating image syn-
thesis, they encounter specific issues if a voxel is displaced
to a new location without replacement at the original site.
Specifically, it results in the voxel appearing twice in the
backward-warped frame [37, 40], or a hole at the original
location in the forward-warped frame [48]. To overcome the
limitation of nonexistent training for intermediate images
and warping procedure, we propose a novel network incor-
porating pseudo-supervision, including an image synthesis
network to ensure the integrity of intermediate images.
2.2. Learning optical flow
Optical flow learning is crucial in the video and medical
domain. Various learning methods have been extensively
investigated [8, 63, 74] aiming to estimate optical flow.
However, they require a ground truth optical flow for train-
ing, which is limited in availability. To address this limita-
tion, some methods [2–4, 24, 27, 28, 31, 32, 35, 38, 69]
have been developed to compute the similarity between the
warped image and a fixed reference to train networks, al-
lowing training without ground truth optical flow.
11354
3. Background
We first briefly introduce the necessary background on the
flow calculation model in Sec. 3.1 and the existing unsuper-
vised interpolation approaches in Sec. 3.2.
3.1. Flow calculation model
Suppose we are given two input images I0andI1at time
T= 0 andT= 1, respectively. Our main objective is
to predict the intermediate image ˆItat time T=twithin
the range of 0 to 1, given I0andI1, without explicit su-
pervision. An intuitive approach is to train a neural net-
work to directly generate voxel values of ˆItwithout ex-
plicitly computing coordinate transformation. However, the
generation models such as generative adversarial networks
(GAN) [12, 46, 47, 73] typically require a large amount
of training data, making them impractical for the medical
domain where data is limited. In contrast, flow calculation
models [25, 54, 71] can generate 3D images using only two
real input images. Given these advantages, we employ flow-
based methods for this task, as they are capable of generat-
ing 3D images using only two real input images.
Flow-based interpolation approaches employ a flow cal-
culation model Fθwith model parameters θto obtain a co-
ordinate transformation map between two target samples.
Given I0andI1, the flow calculation model Fθtakes I0
andI1as sequential inputs and provides a coordinate trans-
formation map ϕθ
0→1. The objective of the flow calculation
model Fθis to warp I0intoˆI0→1:=I0◦ϕθ
0→1such that it
matches I1, where ◦indicates spatial transformation.
To train flow calculation models, a warping loss
Lwarp(I0, I1)is used, which ensures the quality of com-
puted optical flow. The warping loss is defined based on the
warped images I1◦ϕθ
1→0andI0◦ϕθ
0→1, which corresponds
toI0andI1, respectively. Lwarp can be expressed as
Lθ
warp(I0,I1) =Lsmth(ϕθ
0→1) +Limage (I1, I0◦ϕθ
0→1)
+Lsmth(ϕθ
1→0) +Limage (I0, I1◦ϕθ
1→0), (1)
where Lsmth is a smoothness term that promotes similar
flow values among neighboring voxels, and Limage ensures
alignment between two images. Typically, we utilize the
sum of normalized cross-correlation (NCC) [3] and Char-
bonnier [9] losses as the Limage , since NCC has exten-
sively used is 3D medical flow calculation works [3, 58, 77],
and Charbonnier loss is a common choice in previous VFI
works [26, 33, 51, 58, 68, 77]. The losses are defined as:
Lsmth(ϕ) =∥∇ϕ∥2 (2)
Limage (I,ˆI) =−NCC (I,ˆI) +q
(I−ˆI)2+ϵ2,(3)
where ∇ϕdenotes the flow gradient, and ϵrepresents a
small constant.The fully learned flow calculation model, denoted
asϕθ∗
0→1, is earned by minimizing the warping loss
Lθ
warp(I0, I1)with respect to the model parameter θ. The
calculated flow can be formulated as:
ϕθ∗
0→1s.t.θ∗:= arg min
θX
(I0,I1)∈DLθ
warp(I0, I1),(4)
where Dindicates the training set containing the pairs of I0
andI1.
3.2. Previous unsupervised VFI approaches
Methodology. If the flow from I0to the intermediate tar-
get sample Itcan be ideally acquired as ϕ0→t, the corre-
sponding ˆItcan also be obtained (i.e., ˆIt=I0◦ϕ0→t).
To obtain this flow, current approaches [3, 30] approximate
ϕ0→tas the following linear interpolation of the flow or la-
tent vector:
ϕθ∗
0→t:=t·ϕθ∗
0→1orϕθ∗
0→t:=ϕt·θ∗
0→1, (5)
where t·θindicates the linear multiplication of latent vector.
Therefore, the target Itcan be obtained by approximating it
asˆIt:=I0◦ϕθ∗
0→t.
Limitations. As detailed in the latter part of Sec. 2.1, ex-
isting post-hoc linear interpolation approaches encounter
two major challenges: firstly, they are prone to spatial dis-
tortion since the underlying network Fθ∗thatˆItrelies on
does not account for the structural smoothness between two
samples during network training; and secondly, they often
suffer from artifacts resulting from the warping procedure.
Moreover, the methods heavily rely on post-hoc linear mul-
tiplication, leading to potential overfitting to the linear as-
sumption. In other words, these methods assume that the
structures within a given 4D medical image move only in
a linear direction, and the magnitude of this movement is
linearly proportional to time.
4. Method
We introduce our Unsupervised V olumetric Interpolation
Network, referred to as UVI-Net. The network first gen-
erates intermediate images and then employs cycle consis-
tency constraints to reconstruct authentic images from these
synthesized ones. In Sec. 4.1, we provide an overview and
a detailed presentation of our method. The training and in-
ference procedures are outlined in Sec. 4.2 and Sec. 4.3,
respectively. Additionally, in Sec. 4.4, we introduce an
instance-specific optimization method to further enhance
our model’s performance.
4.1. Methodology overview
To achieve a result exhibiting improved smoothness for the
intermediate sample derived from the network, it is imper-
ative for the network to access the pertinent information to
11355
Figure 1. An overview of time-domain cycle consistency con-
straint. This image illustrates the process of generating ˆIcyc
0. (1)
I0andI1are given two input frames, with I1ommited for sake of
readability. (2) We first generate virtual intermediate frames, and
(3) subsequently generate back the frames with multi-resolution
features (denoted as blue cubics). (4) The resulting reconstructed
images ˆIcyc
0must match the original input frame, I0.
the intermediate sample during the learning process. In light
of this, we propose the cyclic structure model, which first
generates the intermediate images and reconstructs them
back to the two given input images. To ensure consistency
and coherence in the generated images, we impose con-
straints of cycle consistency Lcycbetween the reconstructed
samples, denoted as ˆIcyc
0,ˆIcyc
1, and the corresponding orig-
inal samples I0, I1. The flow of the intermediate frame is
then estimated using our flow calculation model with the
parameter θ∗as follows:
ϕθ∗
0→t:=t·ϕθ∗
0→1s.t. (6)
θ∗:= arg min
θmin
ω,ψX
(I0,I1)∈DLθ
warp(I0, I1)
+L(θ,ω,ψ )
cyc
I0,ˆIcyc
0
+L(θ,ω,ψ )
cyc
I1,ˆIcyc
1
,(7)
where θ,ω, andψindicate the parameters for the flow calcu-
lation, feature extraction, and reconstruction models, which
will be described in the below sections.
Unlike the current approach in Eq. (4), we allow the net-
work to access intermediate samples and update them in its
training, as described in Eq. (7), resulting in improved nat-
ural voxels. We first explain the process of obtaining ˆIcycand provide a detailed explanation of Lcycin the following
sections.
4.2. Training
The overall acquire procedure of ˆIcyc
0andˆIcyc
1is illustrated
in Fig. 1. First, we generate multiple virtual intermediate
samples (see Step 2 in Fig. 1) by randomly sampling values
oft1, t2, andt3as below.
ˆIvir
t1:=I0◦ 
t1·ϕθ
0→1
−0.5≤t1≤0 (8)
ˆIvir
t2:=(
I0◦ 
t2·ϕθ
0→1
0≤t2≤0.5
I1◦ 
(1−t2)·ϕθ
1→0
0.5≤t2≤1(9)
ˆIvir
t3:=I1◦ 
(1−t3)·ϕθ
1→0
1≤t3≤1.5 (10)
Since ˆIvir
t1andˆIvir
t3are generated outside the time range
between the two frames, we limit the maximum time offset
to 0.5 to mitigate the occurrence of artifacts. When gener-
ating the ˆIvir
t2, a synthesized image between the two input
images, we adopt the result created from the image—either
I0orI1—that is closer to the reference point t2, to preserve
the properties of the real image maximally.
Next, we interpolate the generated intermediate samples
(see Step 3 in Fig. 1) to acquire the I0andI1’s candidates
as follows:
ˆIcand
t1→0:=ˆIvir
t1◦−t1
t2−t1·ϕθ
t1→t2
, (11)
ˆIcand
t2→0:=ˆIvir
t2◦t2
t2−t1·ϕθ
t2→t1
, (12)
ˆIcand
t2→1:=ˆIvir
t1◦1−t2
t3−t2·ϕθ
t2→t3
, (13)
ˆIcand
t3→1:=ˆIvir
t3◦t3−1
t3−t2·ϕθ
t3→t2
. (14)
While warping the virtual frames, we simultaneously
warp the feature space of the frames across multiple res-
olutions, obtaining a set of warped feature maps: St1→0,
St2→0,St2→1, andSt3→1. Specifically, following the ar-
chitecture of our feature extractor as shown in Fig. 3, we
extract feature maps resized to 1, 0.5, and 0.25 times their
original size. Then, using the same optical flow as described
in Eq. (11) to (14) (or downscaled as necessary), we obtain
the final warped feature maps. This method enhances the
reconstruction model’s ability to make more accurate pre-
dictions by providing access to both voxel and feature in-
formation. Furthermore, we extract image representations
at various levels, which have proven effective in previous
research on video-related tasks [26, 29, 49].
Using these warped images and features, we obtain the
predictions ˆIcyc
0andˆIcyc
1using the reconstruction model
Rψ(see Step 4 in Fig. 1). The model takes the distance-
based weighted sum images and warped feature map sets,
11356
Figure 2. Schematic overview of our entire inference process. Starting with two input frames, I0andI1, we input the frames into the
flow calculation model to obtain the approximated flow fields ϕ0→tandϕ1→t. We then warp the two frames using the obtained flow
field, and similarly warp the multi-scale voxelwise features. Finally, we refine the distance-inversely weighted added image considering the
information from multi-scale features, resulting in the final interpolated frame ˆIt.
Figure 3. Architecture of the feature extractor module based on 3D
Convolutional Neural Network (CNN). h,w, and dare the input
image’s height, width, and depth, respectively.
and reconstructs the original frames through residual cor-
rections. Each element of the input feature map sets is fed
into individual encoder layers of the reconstruction model
and concatenated channel-wise. The procedure of the recon-
struction model can be written as:
ˆIcyc
0:=Rψ(ˆIcand
t1→0⊕ˆIcand
t2→0,St1→0,St2→0), (15)
ˆIcyc
1:=Rψ(ˆIcand
t2→1⊕ˆIcand
t3→1,St2→1,St3→1), (16)
where ⊕indicates distance-based addition.
With the reconstructed images ˆIcyc
0andˆIcyc
1, we can
introduce the cycle consistency loss. Our cycle-consistent
framework reconstructs real images from the generated in-
termediate images, thereby enhancing the smoothness of the
interpolated images. Without time notation for clarity, con-
sider the reconstructed image ( ˆIcyc) and corresponding real
image ( I). The cycle consistency loss is defined as:
L(θ,ω,ψ )
cyc (I,ˆIcyc) =Limage (I,ˆIcyc) +Lreg(Rψ),(17)where Limage follows Eq. (3), and Lregacts as an L1 reg-
ularization term applied to the predicted residual of the re-
construction model. This term helps control excessive mod-
ification during the reconstruction process.
In essence, even without any intermediate frames, we
utilize the given authentic frames as pseudo supervision for
the intermediate frame, facilitated by the initially generated
virtual intermediate samples ˆIvir. By incorporating a cycle
consistency constraint between the reconstructed and orig-
inal authentic images, our approach enhances spatial con-
tinuity between the two images and generates high-quality
virtual intermediate samples.
4.3. Inference
We illustrate the overall inference procedure of UVI-Net
in Fig. 2. First, we obtain two optical flow ϕθ∗
0→1andϕθ∗
1→0,
where θ∗follows Eq. (7). Next, we attain two It’s candidate
as follows:
ˆIcand
0→t:=I0◦ϕ0→t=I0◦
t·ϕθ∗
0→1
(18)
ˆIcand
1→t:=I1◦ϕ1→t=I1◦
(1−t)·ϕθ∗
1→0
. (19)
Finally, by reconstructing the final image with the two can-
didates considering the temporal distance, we derive ˆItas
ˆIt:=Rψ(ˆIcand
0→t⊕ˆIcand
1→t,S0→t,S1→t), (20)
where S0→tandS1→tare warped feature map sets from
I0andI1, respectively. Remarkably, while baseline ap-
proaches can only use one of ˆI0→tandˆI1→t, we can engage
both information and make to be symmetric even the order
ofI0andI1is switched.
4.4. Instance-Specific Optimization
Instance-specific optimization is a technique used to en-
hance the final performance by fine-tuning models for each
11357
Dataset Supervised Method PSNR ↑ NCC↑ SSIM↑ NMSE ↓ LPIPS ↓
Cardiac✓SVIN [16] 32.51 ±0.254 0.559 ±0.007 0.972 ±0.001 2.930 ±0.155 1.535 ±0.043
MPVF [68] 33.15 ±0.238 0.561 ±0.006 0.971 ±0.001 2.435 ±0.133 1.941 ±0.055
✗VM [3] 31.02 ±0.272 0.555 ±0.006 0.966 ±0.002 4.254 ±0.261 1.772 ±0.064
TM [10] 30.45 ±0.280 0.547 ±0.006 0.958 ±0.002 4.826 ±0.278 2.083 ±0.078
Fourier-Net+ [24] 29.98 ±0.287 0.544 ±0.006 0.957 ±0.002 5.503 ±0.314 2.008 ±0.077
R2Net [27] 28.59 ±0.278 0.509 ±0.007 0.930 ±0.003 7.281 ±0.329 3.482 ±0.138
DDM [30] 29.71 ±0.221 0.541 ±0.006 0.956 ±0.002 5.007 ±0.239 2.136 ±0.066
IDIR* [69] 31.56 ±0.275 0.557 ±0.006 0.968 ±0.001 3.806 ±0.249 1.675 ±0.061
Ours (w/o inst opt.) 33.57 ±0.275 0.565 ±0.007 0.977 ±0.001 2.409 ±0.159 1.134 ±0.044
Ours (w/ inst opt.) 33.59 ±0.268 0.565 ±0.007 0.978 ±0.001 2.384 ±0.157 1.066 ±0.041
Lung✓SVIN [16] 30.99 ±0.309 0.312 ±0.002 0.973 ±0.002 0.852 ±0.063 2.182 ±0.093
MPVF [68] 31.18 ±0.344 0.310 ±0.003 0.972 ±0.002 0.761 ±0.075 2.554 ±0.092
✗VM [3] 32.29 ±0.314 0.316 ±0.002 0.977 ±0.001 0.641 ±0.052 2.063 ±0.108
TM [10] 30.92 ±0.290 0.313 ±0.002 0.973 ±0.001 0.786 ±0.050 2.746 ±0.113
Fourier-Net+ [24] 30.26 ±0.314 0.308 ±0.003 0.971 ±0.002 0.959 ±0.061 2.615 ±0.125
R2Net [27] 29.34 ±0.270 0.294 ±0.003 0.962 ±0.002 1.061 ±0.051 3.277 ±0.122
DDM [30] 30.37 ±0.271 0.308 ±0.003 0.971 ±0.002 0.905 ±0.065 2.283 ±0.106
IDIR* [69] 32.91 ±0.309 0.321 ±0.003 0.980 ±0.002 0.586 ±0.055 2.035 ±0.112
Ours (w/o inst opt.) 33.90 ±0.382 0.319 ±0.003 0.980 ±0.002 0.558 ±0.055 1.512 ±0.112
Ours (w/ inst opt.) 34.00 ±0.387 0.320 ±0.003 0.980 ±0.002 0.552 ±0.055 1.489 ±0.093
Table 2. Quantitative comparison of interpolation results. These metrics were evaluated after repeating each experiment three times and
collecting all frames. The model marked with an ‘*’ is trained exclusively on the test set, as it is designed for training on a single data
pair only. For our model, the results with or without instance-specific optimization are both reported. The table presents both the average
and standard deviation for each metric. NMSE and LPIPS values are presented in units of 10−2. The best and second-best results for each
metric are indicated with bold and underlined , respectively.
test sample. This approach was introduced by Balakrish-
nan et al. [2] within the unsupervised medical image warp-
ing domain. Despite our work being in a different task,
this strategy remains applicable. Utilizing a model weight
pre-trained on the training data, we fine-tune the model
for a relatively small number of epochs on each test data.
Such an adaptive approach is particularly beneficial in med-
ical imaging, allowing for more personalized and accurate
frame interpolation tailored to individual scans.
5. Experiments
This section describes the benchmark datasets for 4D med-
ical imaging used in this study in Sec. 5.1. Next, Sec. 5.2
outlines some settings, including training details and met-
rics for performance evaluation. The results are comprehen-
sively presented in Sec. 5.3, highlighting our method’s ef-
fectiveness and efficiency.
5.1. Datasets
To evaluate the performance of image interpolation, two 4D
image datasets are used, each for the heart and lung. The
ACDC cardiac dataset [6] consists of 100 4D temporal car-
diac MRI images. End-diastolic and end-systolic phase im-
ages are used as the start and end images, respectively. The
initial 90 alphabetically sorted samples form the training
set, with the remaining used for the test set. The 4D-Lung
dataset [22] consists of 82 chest CT scans for radiother-
apy planning from 20 lung cancer patients. In each 4D-CTstudy, the end-inspiratory (0% phase) and end-expiratory
(50% phase) phase scans are set as the initial and final im-
ages, respectively. The first 68 CT scans from 18 patients
in the dataset are included in the training set. For additional
information for dataset, please refer to Appendix A.
5.2. Experimental settings
5.2.1 Baselines
For comparison with our proposed methods, six models are
included as the baselines. V oxelMorph (VM) [2], Trans-
Morph (TM) [10], Fourier-Net+ [24] and R2Net [27] are
first initially trained with the provided dataset to calculate
optical flow. Interpolated images are then obtained by linear
scaling the optical flow, i.e., t·ϕθ∗
0→t. Diffusion Deformable
Model (DDM) [30] also uses dataset training but interpo-
lates by scaling the latent vector, i.e., ϕt·θ∗
0→t. For IDIR [69],
it is crucial to clarify that it requires individual training for
each target registration pair, leading to limited generaliza-
tion, whereas our method is trained using a distinct train-
ing set and subsequently applied for inference on the target
pairs. We also compared the results of our model with two
supervised methods proposed for video interpolation on 4D
medical images: SVIN [16] and MPVF [68]. Detailed infor-
mation about the baseline models is in Appendix B.
5.2.2 Evaluation metrics
To evaluate the similarity between the predicted and ground
truth images, metrics including PSNR (Peak Signal-to-
11358
(a) Performance on the cardiac dataset according to training size.
(b) Performance on the lung dataset according to training size.
Figure 4. Performance trends based on the size of the training datasets. The dashed line represents a supervised setting. As depicted in this
figure, we observe that the performance gap between our model and the baselines increases regardless of whether the setting is supervised or
not, and irrespective of the dataset type. This demonstrates our model’s robustness, particularly in addressing data scarcity issues common
in the medical domain.
Noise Ratio) [14], NCC (Normalized Cross Correlation),
SSIM (Structural Similarity Index Measure) [79], NMSE
(Normalized Mean Squared Error) and LPIPS (Learned Per-
ceptual Image Patch Similarity) [76] are used. Since LPIPS
is available only for 2D, it was averaged across slices along
the x, y, and z axes. Each metric represents the voxel-wise
similarity, correlation, structural similarity, reconstruction
error, and perceptual similarity between the synthesized and
authentic images.
5.2.3 Training details
For the flow calculation model, we employed the network
designed in V oxelMorph [2]. As for the reconstruction
model, we used a small size of 3D-UNet. The detailed con-
figuration of the network and more details are described
in Appendix C. The proposed method was implemented
with PyTorch [52] using an NVIDIA Tesla V100 GPU. The
training process takes approximately 4 hours for the car-
diac dataset and 8 hours for the lung dataset, respectively.
Instance-specific optimization took about 1.12 minutes per
sample for ACDC and 3.13 minutes for 4D-Lung.
5.3. Results
5.3.1 Interpolation Result
The performance of interpolation compared to unsupervised
and supervised methods is shown in Tab. 2. Our method
consistently demonstrates superior performance among allthe models, outperforming others with a significant mar-
gin in every evaluation metric. This trend is observed
across both heart and lung datasets, even in the absence of
instance-specific optimization.
It is important to note that our approach surpasses
IDIR [69], serving as a rigorous comparison baseline for
our method due to IDIR’s test set-specific optimization. The
core methodology behind IDIR undergoes unique adapta-
tion for each test set pair, which involves retraining for every
new instance. While this strategy enables IDIR to tailor its
performance to each dataset, it restricts its practical appli-
cability. Nevertheless, our method demonstrates substantial
superiority over IDIR in terms of performance.
Supervised Models. Notably, our approach also sur-
passes supervised methods. An interesting observation is
the varying performance of these supervised models across
different datasets. As detailed in Tab. 1, the ACDC dataset
contains significantly more frames compared to the 4D-
Lung dataset. This discrepancy implies that the 4D-Lung
dataset experiences limitations in terms of supervision qual-
ity. Therefore, the performance gap is more pronounced in
the lung dataset, underscoring a critical insight: supervised
models tend to underperform with limited supervision from
intermediate frames. This pattern reaffirms the importance
of our method’s ability to achieve high accuracy in scenar-
ios with constrained supervision, highlighting its robustness
and effectiveness in 4D medical VFI tasks.
11359
Figure 5. Visualization examples from 4D cardiac and lung datasets. The model marked with an ‘*’ is trained exclusively on the test
set, while models marked with ‘(SL)’ are trained using supervised learning. Our method generates intermediate frames that are not only
visually appealing but also precise, successfully retaining fine details and maintaining the structural integrity of the original images.
5.3.2 Effect of training dataset size
Fig. 4 illustrates the interpolation performance based on the
number of training samples. With the test sets remaining
fixed, the sizes of the training sets are reduced from their
full down to one. Compared to the other five unsupervised
baselines (VM, TM, Fourier-Net+, R2Net, and DDM), our
method consistently exhibits superior performance across
varying training set sizes, with performance gaps widening
as the dataset size decreases. Remarkably, even with a min-
imal size comprising only one sample , our approach fre-
quently outperforms the baselines that utilize the maximum
training set size. It should be noted that IDIR is not included
in this comparison, as it does not follow a traditional train-
ing process on a training set. For the two supervised base-
line models (SVIN, MPVF), the performance also dimin-
ishes as the number of samples for supervision decreases,
leading to an increasing performance gap between them and
our model. Our consistent performance in scenarios with
small datasets underscores the strengths of our approach in
mitigating the challenges posed by data scarcity in the med-
ical field.
5.3.3 Qualtitative analysis
The comparison of qualitative results between interpolation
methods is shown in Fig. 5. Our method consistently pro-
duces visually appealing and accurate intermediate frames,
capturing fine details and preserving the structural integrity
of the original images.
5.3.4 Downstream task
We also demonstrate that our interpolation method can be
applied to downstream tasks. Specifically, we tested its ef-
fectiveness on segmentation data, which is relatively com-plex, demonstrating our approach’s potential for augment-
ing 3D medical datasets. Details of the experimental setup
and performance can be found in Appendix D.
5.3.5 Additional experiments
Additional experiments, including ablation studies and fur-
ther qualitative results, are detailed in Appendix E. More-
over, we have analyzed the results of extrapolation to en-
sure that generated images during the training process do
not exhibit any unnatural changes or issues.
6. Conclusion
Our framework, UVI-Net, effectively tackles the challenge
of generating intermediate frames for 4D medical images
through unsupervised volumetric interpolation. By lever-
aging pseudo supervision within a cyclic structure, our
method ensures spatial continuity between the generated in-
termediate and real images. Experimental results on bench-
mark datasets validate the efficacy of our approach, reveal-
ing substantial improvements in intermediate frame quality
across various evaluation metrics, surpassing both unsuper-
vised and supervised baselines. Furthermore, our method
has demonstrated robustness not only in situations of frame
scarcity but also in data scarcity contexts. Ultimately, this
study underscores the promise of unsupervised 3D flow-
based interpolation and opens new avenues for research and
development in the field of medical imaging.
Acknowledgement This study was supported by Insti-
tute for Information & communications Technology Promo-
tion (IITP) grant funded by the Korea government (MSIT)
(No.2019-0-00075 Artificial Intelligence Graduate School
Program (KAIST)) and Medical Scientist Training Program
from the Ministry of Science & ICT of Korea.
11360
References
[1] Abolfazl Abdollahi, Biswajeet Pradhan, and Abdullah
Alamri. Vnet: An end-to-end fully convolutional neural net-
work for road extraction from high-resolution remote sens-
ing data. IEEE Access , 8:179424–179436, 2020. 15
[2] Guha Balakrishnan, Amy Zhao, Mert R Sabuncu, John Gut-
tag, and Adrian V Dalca. An unsupervised learning model
for deformable medical image registration. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 9252–9260, 2018. 2, 6, 7
[3] Guha Balakrishnan, Amy Zhao, Mert R. Sabuncu, John Gut-
tag, and Adrian V . Dalca. V oxelmorph: A learning frame-
work for deformable medical image registration. IEEE
Transactions on Medical Imaging , 38(8):1788–1800, 2019.
3, 6, 13, 14, 15
[4] M Faisal Beg, Michael I Miller, Alain Trouv ´e, and Laurent
Younes. Computing large deformation metric mappings via
geodesic flows of diffeomorphisms. International journal of
computer vision , 61:139–157, 2005. 2
[5] J Bellec, F Arab-Ceschia, J Castelli, C Lafond, and E Chajon.
Itv versus mid-ventilation for treatment planning in lung sbrt:
a comparison of target coverage and ptv adequacy by using
in-treatment 4d cone beam ct. Radiation Oncology , 15:1–10,
2020. 1
[6] Olivier Bernard, Alain Lalande, Clement Zotti, Freder-
ick Cervenansky, Xin Yang, Pheng-Ann Heng, Irem Cetin,
Karim Lekadir, Oscar Camara, Miguel Angel Gonzalez
Ballester, et al. Deep learning techniques for automatic mri
cardiac multi-structures segmentation and diagnosis: is the
problem solved? IEEE transactions on medical imaging , 37
(11):2514–2525, 2018. 2, 6
[7] Rhydian Caines, Naomi K Sisson, and Carl G Rowbottom.
4dct and vmat for lung patients with irregular breathing.
Journal of Applied Clinical Medical Physics , 23(1):e13453,
2022. 1
[8] Xiaohuan Cao, Jianhuan Yang, Li Wang, Zhong Xue, Qian
Wang, and Dinggang Shen. Deep learning based inter-
modality image registration supervised by intra-modality
similarity. In Machine Learning in Medical Imaging: 9th
International Workshop, MLMI 2018, Held in Conjunction
with MICCAI 2018, Granada, Spain, September 16, 2018,
Proceedings 9 , pages 55–63. Springer, 2018. 2
[9] Pierre Charbonnier, Laure Blanc-Feraud, Gilles Aubert, and
Michel Barlaud. Two deterministic half-quadratic regular-
ization algorithms for computed imaging. In Proceedings
of 1st international conference on image processing , pages
168–172. IEEE, 1994. 3
[10] Junyu Chen, Eric C Frey, Yufan He, William P Segars, Ye
Li, and Yong Du. Transmorph: Transformer for unsuper-
vised medical image registration. Medical image analysis ,
82:102615, 2022. 6, 13, 15
[11] Zeyuan Chen, Yinbo Chen, Jingwen Liu, Xingqian Xu,
Vidit Goel, Zhangyang Wang, Humphrey Shi, and Xiaolong
Wang. Videoinr: Learning video implicit neural representa-
tion for continuous space-time super-resolution. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 2047–2057, 2022. 2[12] Xianjin Dai, Yang Lei, Yabo Fu, Walter J Curran, Tian Liu,
Hui Mao, and Xiaofeng Yang. Multimodal mri synthesis us-
ing unified generative adversarial networks. Medical physics ,
47(12):6343–6354, 2020. 3
[13] Lee R Dice. Measures of the amount of ecologic association
between species. Ecology , 26(3):297–302, 1945. 15
[14] R. Dosselmann and Xue Dong Yang. Existing and emerging
image quality metrics. In Canadian Conference on Electrical
and Computer Engineering, 2005. , pages 1906–1913, 2005.
7
[15] Dagmar Grob, Luuk Oostveen, Jan R ¨uhaak, Stefan Held-
mann, Brian Mohr, Koen Michielsen, Sabrina Dorn, Mathias
Prokop, Marc Kachelrie β, Monique Brink, et al. Accuracy
of registration algorithms in subtraction ct of the lungs: A
digital phantom study. Medical physics , 46(5):2264–2274,
2019. 13
[16] Yuyu Guo, Lei Bi, Euijoon Ahn, Dagan Feng, Qian Wang,
and Jinman Kim. A spatiotemporal volumetric interpolation
network for 4d dynamic medical image. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 4726–4735, 2020. 2, 6, 13
[17] Yuyu Guo, Lei Bi, Dongming Wei, Liyun Chen, Zhengbin
Zhu, Dagan Feng, Ruiyan Zhang, Qian Wang, and Jinman
Kim. Unsupervised landmark detection-based spatiotempo-
ral motion estimation for 4-d dynamic medical images. IEEE
Transactions on Cybernetics , 2021. 2
[18] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong
Yang, Andriy Myronenko, Bennett Landman, Holger R
Roth, and Daguang Xu. Unetr: Transformers for 3d medi-
cal image segmentation. In Proceedings of the IEEE/CVF
winter conference on applications of computer vision , pages
574–584, 2022. 15
[19] Weihua He, Kaichao You, Zhendong Qiao, Xu Jia, Ziyang
Zhang, Wenhui Wang, Huchuan Lu, Yaoyuan Wang, and
Jianxing Liao. Timereplayer: Unlocking the potential of
event cameras for video interpolation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 17804–17813, 2022. 2
[20] Alessa Hering, Lasse Hansen, Tony CW Mok, Albert CS
Chung, Hanna Siebert, Stephanie H ¨ager, Annkristin Lange,
Sven Kuckertz, Stefan Heldmann, Wei Shao, et al.
Learn2reg: comprehensive multi-task medical image regis-
tration challenge, dataset and evaluation in the era of deep
learning. IEEE Transactions on Medical Imaging , 2022. 15
[21] Kan N Hor, Rolf Baumann, Gianni Pedrizzetti, Gianni Tonti,
William M Gottliebson, Michael Taylor, D Woodrow Ben-
son, and Wojciech Mazur. Magnetic resonance derived my-
ocardial strain assessment using feature tracking. JoVE
(Journal of Visualized Experiments) , (48):e2356, 2011. 1
[22] Geoffrey D. Hugo, Elisabeth Weiss, William C. Slee-
man, Salman Balik, Paul J. Keall, Jun Lu, and Jeffrey F.
Williamson. Data from 4d lung imaging of nsclc patients.
2016. 2, 6
[23] Mi-Young Jeung, Philippe Germain, Pierre Croisille, So-
raya El ghannudi, Catherine Roy, and Afshin Gangi. My-
ocardial tagging with mr imaging: overview of normal and
pathologic findings. Radiographics , 32(5):1381–1398, 2012.
1
11361
[24] Xi Jia, Alexander Thorley, Alberto Gomez, Wenqi Lu, Di-
pak Kotecha, and Jinming Duan. Fourier-net+: Leveraging
band-limited representation for efficient 3d medical image
registration. arXiv preprint arXiv:2307.02997 , 2023. 2, 6,
13, 15
[25] Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan
Yang, Erik Learned-Miller, and Jan Kautz. Super slomo:
High quality estimation of multiple intermediate frames for
video interpolation. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 9000–
9008, 2018. 2, 3
[26] Xin Jin, Longhai Wu, Jie Chen, Youxin Chen, Jayoon Koo,
and Cheul-hee Hahm. A unified pyramid recurrent net-
work for video frame interpolation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 1578–1587, 2023. 2, 3, 4
[27] Ankita Joshi and Yi Hong. R2net: Efficient and flexible
diffeomorphic image registration using lipschitz continuous
residual networks. Medical Image Analysis , 89:102917,
2023. 2, 6, 13, 15
[28] Neerav Karani, Lin Zhang, Christine Tanner, and Ender
Konukoglu. An image interpolation approach for acquisition
time reduction in navigator-based 4d mri. Medical image
analysis , 54:20–29, 2019. 2
[29] Rezaul Karim, He Zhao, Richard P Wildes, and Mennatul-
lah Siam. Med-vt: Multiscale encoder-decoder video trans-
former with application to object segmentation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 6323–6333, 2023. 4
[30] Boah Kim and Jong Chul Ye. Diffusion deformable model
for 4d temporal medical image generation. In Medical Image
Computing and Computer Assisted Intervention–MICCAI
2022: 25th International Conference, Singapore, September
18–22, 2022, Proceedings, Part I , pages 539–548. Springer,
2022. 2, 3, 6, 13, 15
[31] Boah Kim, Dong Hwan Kim, Seong Ho Park, Jieun Kim,
June-Goo Lee, and Jong Chul Ye. Cyclemorph: cycle con-
sistent unsupervised deformable image registration. Medical
image analysis , 71:102036, 2021. 2
[32] Boah Kim, Inhwa Han, and Jong Chul Ye. Diffusemorph:
Unsupervised deformable image registration using diffusion
model. In Computer Vision–ECCV 2022: 17th European
Conference, Tel Aviv, Israel, October 23–27, 2022, Proceed-
ings, Part XXXI , pages 347–364. Springer, 2022. 2
[33] Taewoo Kim, Yujeong Chae, Hyun-Kurl Jang, and Kuk-Jin
Yoon. Event-based video frame interpolation with cross-
modal asymmetric bidirectional motion fields. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 18032–18042, 2023. 3
[34] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 14, 15
[35] Dongyang Kuang. Cycle-consistent training for reducing
negative jacobian determinant in deep registration networks.
InSimulation and Synthesis in Medical Imaging: 4th In-
ternational Workshop, SASHIMI 2019, Held in Conjunction
with MICCAI 2019, Shenzhen, China, October 13, 2019,
Proceedings 4 , pages 120–129. Springer, 2019. 2[36] seungmin Lee, Seongwook Yoon, and Sanghoon Sull. Unsu-
pervised video frame interpolation using online refinement.
InInstitute of Electronics, Information and Communication
Engineers , 2020. 1, 2
[37] Sungho Lee, Narae Choi, and Woong Il Choi. Enhanced cor-
relation matching based video frame interpolation. In Pro-
ceedings of the IEEE/CVF winter conference on applications
of computer vision , pages 2839–2847, 2022. 2
[38] Yang Lei, Yabo Fu, Tonghe Wang, Yingzi Liu, Pretesh Pa-
tel, Walter J Curran, Tian Liu, and Xiaofeng Yang. 4d-
ct deformable image registration using multiscale unsuper-
vised deep learning. Physics in Medicine & Biology , 65(8):
085003, 2020. 2
[39] Yu-Lun Liu, Yi-Tung Liao, Yen-Yu Lin, and Yung-Yu
Chuang. Deep video frame interpolation using cyclic frame
generation. In Proceedings of the AAAI Conference on Arti-
ficial Intelligence , pages 8794–8802, 2019. 1, 2
[40] Yao Lu, Jack Valmadre, Heng Wang, Juho Kannala,
Mehrtash Harandi, and Philip Torr. Devon: Deformable vol-
ume network for learning optical flow. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer
Vision , pages 2705–2713, 2020. 2
[41] Elliot McVeigh and Cengizhan Ozturk. Imaging myocar-
dial strain. IEEE Signal Processing Magazine , 18(6):44–56,
2001. 1
[42] Elliot R McVeigh, Amir Pourmorteza, Michael Guttman,
Veit Sandfort, Francisco Contijoch, Suhas Budhiraja, Zhen-
nong Chen, David A Bluemke, and Marcus Y Chen. Re-
gional myocardial strain measurements from 4dct in patients
with normal lv function. Journal of cardiovascular computed
tomography , 12(5):372–378, 2018. 1
[43] Simone Meyer, Abdelaziz Djelouah, Brian McWilliams,
Alexander Sorkine-Hornung, Markus Gross, and Christo-
pher Schroers. Phasenet for video frame interpolation. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 498–507, 2018. 2
[44] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.
V-net: Fully convolutional neural networks for volumetric
medical image segmentation. In 2016 fourth international
conference on 3D vision (3DV) , pages 565–571. Ieee, 2016.
15
[45] Kotaro Mizuno and Masahiro Muto. Preoperative evaluation
of pleural adhesion in patients with lung tumors using four-
dimensional computed tomography performed during natu-
ral breathing. Medicine , 100(47), 2021. 1
[46] Dong Nie, Roger Trullo, Jun Lian, Caroline Petitjean, Su
Ruan, Qian Wang, and Dinggang Shen. Medical im-
age synthesis with context-aware generative adversarial net-
works. In Medical Image Computing and Computer Assisted
Intervention- MICCAI 2017: 20th International Conference,
Quebec City, QC, Canada, September 11-13, 2017, Proceed-
ings, Part III 20 , pages 417–425. Springer, 2017. 3
[47] Dong Nie, Roger Trullo, Jun Lian, Li Wang, Caroline Pe-
titjean, Su Ruan, Qian Wang, and Dinggang Shen. Medi-
cal image synthesis with deep convolutional adversarial net-
works. IEEE Transactions on Biomedical Engineering , 65
(12):2720–2730, 2018. 3
11362
[48] Simon Niklaus and Feng Liu. Context-aware synthesis for
video frame interpolation. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
1701–1710, 2018. 2
[49] Simon Niklaus and Feng Liu. Softmax splatting for video
frame interpolation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
5437–5446, 2020. 2, 4
[50] Simon Niklaus, Long Mai, and Feng Liu. Video frame in-
terpolation via adaptive separable convolution. In Proceed-
ings of the IEEE international conference on computer vi-
sion, pages 261–270, 2017. 2
[51] Junheum Park, Jintae Kim, and Chang-Su Kim. Biformer:
Learning bilateral motion estimation via bilateral trans-
former for 4k video frame interpolation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 1568–1577, 2023. 2, 3
[52] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,
Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-
son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An
imperative style, high-performance deep learning library.
InAdvances in Neural Information Processing Systems 32 ,
pages 8024–8035. Curran Associates, Inc., 2019. 7
[53] Tomer Peleg, Pablo Szekely, Doron Sabo, and Omry Sendik.
Im-net for high resolution video frame interpolation. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern Recognition , pages 2398–2407, 2019. 2
[54] Fitsum A Reda, Deqing Sun, Aysegul Dundar, Mohammad
Shoeybi, Guilin Liu, Kevin J Shih, Andrew Tao, Jan Kautz,
and Bryan Catanzaro. Unsupervised video interpolation us-
ing cycle consistency. In Proceedings of the IEEE/CVF in-
ternational conference on computer Vision , pages 892–900,
2019. 1, 2, 3
[55] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III
18, pages 234–241. Springer, 2015. 14, 15
[56] Elisabeth Sartoretti, Thomas Sartoretti, Christoph Binkert,
Arash Najafi, ´Arp´ad Schwenk, Martin Hinnen, Luuk van
Smoorenburg, Barbara Eichenberger, and Sabine Sartoretti-
Schefer. Reduction of procedure times in routine clinical
practice with compressed sense magnetic resonance imaging
technique. PLoS One , 14(4):e0214887, 2019. 1
[57] Claude E Shannon. A mathematical theory of communi-
cation. The Bell system technical journal , 27(3):379–423,
1948. 15
[58] Yucheng Shu, Hao Wang, Bin Xiao, Xiuli Bi, and Weisheng
Li. Medical image registration based on uncoupled learning
and accumulative enhancement. In Medical Image Comput-
ing and Computer Assisted Intervention 2021 , pages 3–13,
Cham, 2021. Springer International Publishing. 3
[59] Hyeonjun Sim, Jihyong Oh, and Munchurl Kim. Xvfi: ex-
treme video frame interpolation. In Proceedings of theIEEE/CVF international conference on computer vision ,
pages 14489–14498, 2021. 2
[60] Amber L Simpson, Michela Antonelli, Spyridon Bakas,
Michel Bilello, Keyvan Farahani, Bram Van Ginneken, An-
nette Kopp-Schneider, Bennett A Landman, Geert Litjens,
Bjoern Menze, et al. A large annotated medical image dataset
for the development and evaluation of segmentation algo-
rithms. arXiv preprint arXiv:1902.09063 , 2019. 15
[61] Li Siyao, Shiyu Zhao, Weijiang Yu, Wenxiu Sun, Dimitris
Metaxas, Chen Change Loy, and Ziwei Liu. Deep ani-
mation video interpolation in the wild. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 6587–6595, 2021. 2
[62] Pierre Soille. Erosion and Dilation , pages 63–103. Springer
Berlin Heidelberg, Berlin, Heidelberg, 2004. 13
[63] Hessam Sokooti, Bob De V os, Floris Berendsen,
Boudewijn PF Lelieveldt, Ivana I ˇsgum, and Marius
Staring. Nonrigid image registration using multi-scale
3d convolutional neural networks. In Medical Image
Computing and Computer Assisted Intervention- MICCAI
2017: 20th International Conference, Quebec City, QC,
Canada, September 11-13, 2017, Proceedings, Part I 20 ,
pages 232–239. Springer, 2017. 2
[64] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.
Ucf101: A dataset of 101 human actions classes from videos
in the wild. arXiv preprint arXiv:1212.0402 , 2012. 2
[65] Shuochen Su, Mauricio Delbracio, Jue Wang, Guillermo
Sapiro, Wolfgang Heidrich, and Oliver Wang. Deep video
deblurring for hand-held cameras. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 1279–1288, 2017. 2
[66] Lu Wang, Shelly Hayes, Kamen Paskalev, Lihui Jin, Mark K
Buyyounouski, Charlie C-M Ma, and Steve Feigenberg.
Dosimetric comparison of stereotactic body radiotherapy us-
ing 4d ct and multiphase ct images for treatment planning of
lung cancer: evaluation of the impact on daily dose coverage.
Radiotherapy and Oncology , 91(3):314–324, 2009. 1
[67] Wei-Hao Wang, Chia-Yu Sung, Shih-Chung Wang, and Yu-
Hsuan Joni Shao. Risks of leukemia, intracranial tumours
and lymphomas in childhood and early adulthood after pedi-
atric radiation exposure from computed tomography. CMAJ ,
195(16):E575–E583, 2023. 1
[68] Tzu-Ti Wei, Chin Kuo, Yu-Chee Tseng, and Jen-Jee Chen.
Mpvf: 4d medical image inpainting by multi-pyramid voxel
flows. IEEE Journal of Biomedical and Health Informatics ,
2023. 3, 6, 13
[69] Jelmer M Wolterink, Jesse C Zwienenberg, and Christoph
Brune. Implicit neural representations for deformable image
registration. In International Conference on Medical Imag-
ing with Deep Learning , pages 1349–1359. PMLR, 2022. 2,
6, 7, 13
[70] Mian Xi, Meng-Zhong Liu, Xiao-Wu Deng, Li Zhang, Xiao-
Yan Huang, Hui Liu, Qiao-Qiao Li, Yong-Hong Hu, Ling
Cai, and Nian-Ji Cui. Defining internal target volume (itv)
for hepatocellular carcinoma using four-dimensional ct. Ra-
diotherapy and Oncology , 84(3):272–278, 2007. 1
[71] Xiaoyu Xiang, Yapeng Tian, Yulun Zhang, Yun Fu, Jan P
Allebach, and Chenliang Xu. Zooming slow-mo: Fast and
11363
accurate one-stage space-time video super-resolution. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 3370–3379, 2020. 2, 3
[72] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and
William T Freeman. Video enhancement with task-oriented
flow. International Journal of Computer Vision , 127:1106–
1125, 2019. 2
[73] Heran Yang, Jian Sun, Aaron Carass, Can Zhao, Junghoon
Lee, Zongben Xu, and Jerry Prince. Unpaired brain mr-
to-ct synthesis using a structure-constrained cyclegan. In
Deep Learning in Medical Image Analysis and Multimodal
Learning for Clinical Decision Support: 4th International
Workshop, DLMIA 2018, and 8th International Workshop,
ML-CDS 2018, Held in Conjunction with MICCAI 2018,
Granada, Spain, September 20, 2018, Proceedings 4 , pages
174–182. Springer, 2018. 3
[74] Xiao Yang, Roland Kwitt, Martin Styner, and Marc Nietham-
mer. Quicksilver: Fast predictive image registration–a deep
learning approach. NeuroImage , 158:378–396, 2017. 2
[75] Guozhen Zhang, Yuhan Zhu, Haonan Wang, Youxin Chen,
Gangshan Wu, and Limin Wang. Extracting motion and ap-
pearance via inter-frame attention for efficient video frame
interpolation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 5682–
5692, 2023. 2
[76] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 586–595, 2018. 7
[77] Shengyu Zhao, Yue Dong, Eric I-Chao Chang, and Yan Xu.
Recursive cascaded networks for unsupervised medical im-
age registration. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , 2019. 3
[78] Kun Zhou, Wenbo Li, Xiaoguang Han, and Jiangbo Lu. Ex-
ploring motion ambiguity and alignment for high-quality
video frame interpolation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 22169–22179, 2023. 2
[79] Wang Zhou. Image quality assessment: from error measure-
ment to structural similarity. IEEE transactions on image
processing , 13:600–613, 2004. 7
11364
