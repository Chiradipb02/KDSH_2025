SOK-Bench: A Situated Video Reasoning
Benchmark with Aligned Open-World Knowledge
Andong Wang*1,Bo Wu*2,Sunli Chen3,Zhenfang Chen2,Haotian Guan1
Wei-Ning Lee1,Li Erran Li4,Chuang Gan5,2
1The University of Hong Kong,2MIT-IBM Watson AI Lab,
3Tsinghua University,4AWS AI,5UMass Amherst
Abstract
Learning commonsense reasoning from visual contexts
and scenes in real-world is a crucial step toward advanced
artificial intelligence. However, existing video reasoning
benchmarks are still inadequate since they were mainly de-
signed for factual or situated reasoning and rarely involve
broader knowledge in the real world. Our work aims to
delve deeper into reasoning evaluations, specifically within
dynamic, open-world, and structured context knowledge.
We propose a new benchmark (SOK-Bench), consisting of
44K questions and 10K situations with instance-level an-
notations depicted in the videos. The reasoning process is
required to understand and apply situated knowledge and
general knowledge for problem-solving. To create such
a dataset, we propose an automatic and scalable gener-
ation method to generate question-answer pairs, knowl-
edge graphs, and rationales by instructing the combina-
tions of LLMs and MLLMs. Concretely, we first extract
observable situated entities, relations, and processes from
videos for situated knowledge and then extend to open-
world knowledge beyond the visible content. The task gen-
eration is facilitated through multiple dialogues as itera-
tions and subsequently corrected and refined by our de-
signed self-promptings and demonstrations. With a corpus
of both explicit situated facts and implicit commonsense, we
generate associated question-answer pairs and reasoning
processes, finally followed by manual reviews for quality
assurance. We evaluated recent mainstream large vision-
language models on the benchmark and found several in-
sightful conclusions. For more information, please refer to
our benchmark at www.bobbywu.com/SOKBench .
1. Introduction
The evolutions of Large Language Models (LLMs) [32,
33, 39, 44] and Multimodal Large Language Models
*The authors contributed equally to the work.(MLLMs) [21–23, 26, 42, 58, 62] mark a significant mile-
stone in artificial intelligence. These models are making re-
markable strides across various domains, exhibiting better
capabilities in perceptual, generative, and comprehensive
tasks [3, 60]. As models like ChatGPT [32, 33] and succes-
sors continue to improve in scale, the possibility to capture
commonsense knowledge has also seen obvious advance-
ments. Hence, building models with real-world knowl-
edge is becoming more promising now which was a long-
standing but difficult challenge before [5, 43].
Besides novel models, previous efforts have been di-
rected towards evaluating multimodal commonsense rea-
soning, transitioning from purely language-based models
to those incorporating visual comprehension, such as Vi-
sual Question Answering (VQA) [1, 15, 19, 28] or Visual
Commonsense Reasoning (VCR) [10, 14, 17, 18, 47, 52].
These tasks have an underlying assumption that the given
visual and language inputs (e.g., images, videos, questions,
etc.) contain most of the required information. However,
relying on the given task context does not fully reflect real-
world complexities, narrowing the scope of their reason-
ing evaluation. More recently, the focus has shifted to-
wards developing tasks that require external knowledge for
commonsense reasoning [27, 41, 56] and consider the inte-
gration of factual [46], evidential [20], or external knowl-
edge sources [11, 27, 41, 48, 56] into evaluations. They
primarily provide visual information and external infor-
mation on static images for reasoning. Thus, this way
overlooks several crucial natures such as spatio-temporal,
causal, or dynamic processes of real-world activities and
events, an aspect crucial for truly understanding and inter-
acting with our environment. Also, the knowledge is writ-
ten and concluded by free-form descriptions from crowd-
source ways [11, 27, 41, 48, 56], presenting its own set of
limitations. Consistency in descriptions provided by anno-
tators is difficult to maintain [27, 56], and there are multiple
risks of the method being influenced by distinct descriptive
or empirical biases [10, 14, 17, 47].
Our research aims to delve deeper into the realm of com-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
13384
M a p o  T o f u     + +
  t h e  p e r s o n  d i d  n o t  u s e  t h e  
   i n  ?
A . T h e  d i s h  w o u l d  b e  c o m p l e t e l y  d i f f e r e n t .
B . T h e  d i s h  w o u l d  l a c k  t h i c k e n i n g  a g e n t .   
C . T h e  d i s h  w o u l d  l a c k  s e a s o n i n g  a n d  t a s t e  b l a n d .
D . T h e  d i s h  w o u l d  l o s e  i t s  s i g n a t u r e  f l a v o r .a t t r i b u t e s
w h i t e  
s u b s t a n c er e l a t i o n s h i p
d i s s o l v e d  i nc o u n t e r f a c t u a l
W h a t  w o u l d  h a p p e n i fQ u e s t i o n :  w a t e r g l a s sP r e v i o u s  m e t h o d sR e s e a r c h e r s
H u m a n  a n n o t a t o r sQ A  g e n e r a t i o n  
i n s t r u c t i o n sQ A  d r a f tF i l t e r i n g ,  s e l e c t i o n ,  . . .
C h e c k  Q A 
s t y l e ,  a s p e c t ,  . . .Q A sV i d e o s
O u r sR e s e a r c h e r s
Q A sL L M
V G ,  G K G ,  S K G
Q u e s t i o n  t e m p l a t e s
I n s t r u c t i o n sV i d e o s
V L  m o d e l
T o p - d o w n  g e n e r a t i o nS i t u a t e d  C o m m o n s e n s e  K n o w l e d g e  G r a p hc h e m i c a lr e a c t i o ng e l a t i n i z a t i o nu s a g eC 2 7 H 4 8 O 2 0f o r m u l a
t h i c k e n i n g  a g e n ts o u p sw h i t ed i s s o l v e d  i nw a t e rg l a s si nc o u n t e r f a c t u a ll a c k  t h i c k e n i n g  a g e n tc o r n s t a r c hG e n e r a l  K n o w l e d g e  
G r a p hS i t u a t e d  K n o w l e d g e  
G r a p h1 .  F r o m  [ ] ,  w e  c a n  
s e e  t h a t   i s  .
2 .  F r o m  [ ] ,  w e  
c a n  s e e  t h a t   i s  
  i n  .
3 .  F r o m  [ ] ,  w e  
k n o w  t h a t  c o r n s t a r c h  c a n  b e  u s e d  a s  
 .
4 .  ,  f r o m  3 2  s e c  t o  4 0  
s e c ,  w e  c a n  s e e   i s  p u t  i n  
a  s m a l l  b o w l  a n d  m i x e d  w i t h  w a t e r  t o  
f o r m  a  p a s t e .
5 .  T h i s  p a s t e  i s  t h e n  a d d e d  t o  t h e  
d i s h  ( 9 8  s e c  t o  1 0 5  s e c )  t o  t h i c k e n  
t h e  s a u c e .
6 .  T h e r e f o r e ,  t h e  r i g h t  c h o i c e  i s  T h e  
d i s h  w o u l d  l a c k  t h i c k e n i n g  a g e n t .
7 .  T h e  o t h e r  c h o i c e s  c a n  b e  
e l i m i n a t e d  b e c a u s e  t h e y  a r e  n o t  t h e  
c o r r e c t  c o n s e q u e n c e s  i f  t h e  p e r s o n  
d i d  n o t  u s e  c o r n s t a r c h .o b j e c t  a t t r i b u t e s
w h i t ec o r n s t a r c h
c o r n s t a r c h
w a t e r g l a s s
c o r n s t a r c ho b j e c t - o b j e c t  r e l a t i o n s
d i s s o l v e d  
i n
I n  t h e  v i d e oG e n e r a l  K n o w l e d g e  G r a p ht h i c k e n i n g  a g e n t  t o  t h i c k e ns o u p sR a t i o n a l e
Figure 1. Overview. Instead of using crowd-sourced methods, we design a synthesis pipeline to create the benchmark by leveraging
use LLMs and VLLMs, improving efficiency and ensuring consistency. The method helps to automatically generate high-quality question-
answers (QAs) and focusing the desirable purposes for evaluating the model’s ability. To generate data aligned with open-world knowledge,
we propose to connect situation, general knowledge, and situated commonsense and produced three types of associated knowledge graphs
(refer to the subsection 3.2, 3.3, and 3.4). Specifically, it makes more precise inferences based on situational facts and essential common-
sense knowledge by aligning with the bottom-up or top-down goals, the reasoning process from Q to A is able to demonstrate explicitly.
monsense reasoning, specifically within dynamic, open-
world, and structured contexts. We build Situated Open-
World Commonsense Reasoning (SOK-Bench), a novel
benchmark consists of over 44K questions with answers and
situated commonsense knowledge graphs, covers over 12
types of questions, and sources from about 10K dynamic
situations in real-world activities. The models are required
to produce appropriate inferences by leveraging both facts
within situations and the necessary commonsense or back-
ground knowledge.
Thus we propose the Situated Commonsense Graph to
connect and integrate the required knowledge of the reason-
ing process where situated knowledge and general knowl-
edge are structured, compositional, and aligned. Compared
with other datasets in Figure 1, our reasoning benchmark
has diverse characteristics including instance annotations,
compositional generation, and structural alignment of the
situated open-world knowledge and rationales. Moreover,
we propose a scalable and automatic pipeline for both task
generation and annotation. The typical way to generate
evaluation task instructions is crowd-source human anno-
tation [1, 56] or adversarial filtering [55]. However, collect-
ing and locating high-quality situated commonsense data
are not trivial since considering situation, question, answer
and their inner relations requires experienced annotators,
huge cost, and high consistency. Instead, we generate eval-
uation tasks, questions with answers, and reasoning pro-
cesses, which integrate situated knowledge, general knowl-
edge, and underlying reasoning commonsense goals or log-
ics. We design appropriate instructions and compositions
for prompts and define question types, goals to enhancegeneration quality, logic, and rationality, as shown in Fig-
ure 3 (a). The iterative generation process automatically
utilizes the interactions of LLMs or MLLMs in multi-turns,
as shown in Figure 4. In experiments, we evaluate the main-
stream LLMs and VideoLLMs as representatives for the
proposed reasoning benchmark. Although the models have
trained on web-scale data or adopted pre-trained foundation
models as basis, the experiments indicate significant room
for future improvement. We also provide comprehensive
ablation comparisons and analysis for generation processes.
• We create SOK-Bench, a novel benchmark to evaluate sit-
uated and open-world commonsense reasoning in videos
with compositionality, temporality, and causality.
• We propose a scalable method to generate video question-
answering pairs for reasoning through iterative conver-
sations with LLMs and MLLMs. We designed diverse
prompt compositions and a self-prompting strategy to
construct the video knowledge, commonsense knowl-
edge, situated commonsense knowledge graphs, etc.
• We evaluated representative LLMs and MLLMs on the
proposed benchmark in different settings and found that
existing LLMs and MLLMs still perform inferior on situ-
ated open-knowledge reasoning in videos, which further
proves our new dataset’s research value.
2. Related Works
2.1. Video Question Answering
Video Question Answering. Our work is closely related to
video question answering, which requires a model to watch
13385
Table 1. Comparison between the proposed benchmark with existing datasets.
Generation QA Knowledge Situated Knowledge General Situated Commonsense Knowledge
Approach Real-world Scalability Rationale Representation Grounding Dynamic Knowledge Existence Compositionality
CLEVR [17] Rule % ✔ ✔ program ✔ ✔ single-source ✔ ✔
CLEVRER [52] Rule % ✔ ✔ program ✔ ✔ single-source ✔ ✔
EgoTV [12] Rule % ✔ ✔ program ✔ ✔ single-source % %
STAR [47] Rule ✔ ✔ ✔ graph ✔ ✔ single-source % %
AGQA [10] Rule ✔ ✔ ✔ graph ✔ ✔ single-source % %
VCR [57] Manual ✔ % ✔ description ✔ ✔ multi-source ✔ %
VIOLIN [24] Manual ✔ % % description ✔ ✔ single-source ✔ %
NExT-QA [48] Manual ✔ % % description % ✔ single-source ✔ %
Causal-VidQA [20] Manual ✔ % % description % ✔ single-source ✔ %
V2C [9] Manual ✔ % ✔ graph % ✔ multi-source ✔ %
OK-VQA [27] Manual ✔ % % description % % multi-source ✔ %
A-OKVQA [41] Manual ✔ % ✔ description % % open-world ✔ %
Ours Auto, Rule ✔ ✔ ✔ graph ✔ ✔ open-world ✔ ✔
the video and answer questions related to the video’s con-
tent [7, 10, 19, 20, 47, 48, 52]. Most of these existing bench-
marks ask questions about visual attributes, human actions,
and activities, or physical intuition [7, 36, 52]. However,
none of them has focused on studying events in videos with
situated knowledge, open-domain knowledge, and explicit
multi-step reasoning rationales.
Commonsense Question-Answering. Our research is
aligned with the field of commonsense question answering.
It requires models to make use of commonsense to answer
questions. It was first studied in the field of natural ques-
tion answering [29, 51] without any visual context as in-
put. Later, people evaluated commonsense understanding
in the context of visual question answering [6, 27, 35, 41].
Given a static image, such benchmarks require models to
answer questions based on the visual context of the im-
age and its associated commonsense knowledge. There are
some benchmarks studying domain-specific commonsense
like physics [2] and social events [54]. However, we are the
first benchmark to study reasoning with situated and open-
world knowledge in dynamic situations with dense annota-
tions like knowledge graphs and rationales. Table 1 summa-
rizes the differences compared with previous benchmarks.
LLM for Data Annotation. Our work is situated within
the domain of using large language models for data anno-
tation [8, 37]. Ding et al. [8] examined the effectiveness of
GPT-3 as a data annotator for NLP tasks by comparing it
with traditional annotation methods and analyzing its per-
formance across various tasks. Pei et al. [37] introduced a
self-supervised GPT annotation method using a generating-
recovering paradigm, leveraging one-shot learning for effi-
cient data-to-summary annotation, and evaluated its perfor-
mance using alignment scores and human feedback reward
networks. Both of these two works only focus on the natural
language side. In this work, we propose to prompt LLMs to
cooperate with different vision models [21, 62] to annotate
videos and generate question-answer pairs related to video
content and open knowledge.Vision-Language Models. Recently, there has been great
interest in large pre-trained vision-language models [21, 22,
26, 42, 58, 62] (VLMs). Early models [21, 62] only evalu-
ate their performance on inputs with static images and nat-
ural language. There are also some models [26] evaluat-
ing performance their performance on videos. However,
these VLMs either only showed some qualitative exam-
ples [22, 42, 58] or evaluates on traditional video question
answering benchmarks [49, 50, 53]. However, [49, 50, 53]
are not ideal benchmarks for commonsense reasoning since
they mainly focus on the perception side and require no ex-
ternal knowledge to answer the questions. It remains an
open question as to how to evaluate VLMs’ performance
in understanding situations in videos and reasoning about
open-domain knowledge with commonsense.
3. Situated Commonsense Reasoning
3.1. Benchmark Overview
Statistics. Our benchmark consists of 44K questions and
answers for reasoning with situated open-world knowledge,
accompanied by multiple-choice options, and 10K video
clips. Each question-answer (QA) pair links to a hyper-
graph which is generated from the Situated Knowledge
Graph, General Knowledge Graph, and Situated Common-
sense Knowledge Graph (refer to the below sections and
the algorithms in the Supplementary). The aligned graphs
effectively showcase the relations between situated knowl-
edge and general knowledge. Figure 2 presents four QA
examples involving different types situated commonsense
knowledge. We also present a detailed rationale that eluci-
dates the sequence of reasoning steps that bridge the gap
from each question to its corresponding answer, offering
a clearer understanding of the underlying thought process
(see Figure 1). Figure 3 showed the 12 question types of
our dataset. Each question is accompanied by two types of
answers: a direct answer and a set of four multiple-choice
options. The dual format ensures versatility in response as-
13386
W a s h  D i s h e s     +
 t h e  p e r s o n  d i d  n o t  u s e  t h e   a n d  
 o b j e c t ?
A . T h e r e  w o u l d  b e  n o  w a t e r .
B . T h e  d i s h e s  m a y  n o t  b e  f r e e  f r o m  g r e a s e  a n d  b a c t e r i a .
C . T h e  p e r s o n  w o u l d  n o t  h a v e  a  c o n t a i n e r  t o  h o l d  w a t e r .
D . T h e  p e r s o n  w o u l d  n o t  h a v e  a  t o o l  t o  s c r u b  a n d  c l e a n  t h e  
d i s h e s  e f f e c t i v e l y .a t t r i b u t e s
y e l l o w
g r e e nc o u n t e r f a c t u a l
W h a t  w o u l d  h a p p e n  i f
s p o n g ey e l l o wg r e e nc o u n t e r f a c t u a lt o  s c r u b  a n d  c l e a n  
t h e  d i s h e s  
e f f e c t i v e l yn o t  h a v e  a  t o o lp e r s o n
W o r k  O u t     +
W h a t  i s  t h e   o f  u s i n g  t h e  t h i n g  w h i c h  i s   
a n d  ?
A . T o  w o r k  o u t .  
B . T o  c l e a n  t h e  r o o m .
C . T o  d o  n a i l s .
D . T o  m a k e  t e a .a t t r i b u t e s
m e t a l
g r a yp u r p o s e
p u r p o s e
d u m b b e l lg r a y
m e t a lp u r p o s ea d d  r e s i s t a n c e  
a n d  i n t e n s i t yw o r k  o u t
b u i l d  s t r e n g t h  a n d  m u s c l e
I n d i a n  C h i c k e n  C u r r y     + + +
W h a t  i s  t h e   o f    
 ?
A . T o  i n f u s e  t h e  c u r r y  w i t h  a  w a r m ,  e a r t h y  f l a v o r  a n d  a r o m a .
B . T o  a d d  s w e e t n e s s ,  d e p t h ,  a n d  t e x t u r e  t o  t h e  c u r r y .
C . T o  p r o v i d e  a  m e d i u m  f o r  c o o k i n g  a n d  p r e v e n t  s t i c k i n g .  
D . T o  e n h a n c e  t h e  f l a v o r s  o f  t h e  i n g r e d i e n t s .a c t i o n
h e a t i n g  t h e  i n g r e d i e n t a d d i n g  c u m i n  
s e e d s  a n d  c h o p p e d  o n i o n sr e l a t i o n s h i p
i n  t h e  p a nt e m p o r a l
b e f o r ep u r p o s e
p u r p o s e
 3 6 s  t o  8 6 sh e a t o i li np a na d da d d c u m i n  s e e d sc h o p p e d  o n i o n sp u r p o s m e d i u m  f o r  c o o k i n g p r e v e n t  s t i c k i n g
C a l i f o r n i a  R o l l s     + +     
W h a t  i s  t h e   o f  t h e   a c t i o n  o f  
?
A . T o  m a k e  t h e  r o l l  e a s i e r  t o  e a t  a n d  s e r v e .   
B . T o  c o m p a c t  t h e  i n g r e d i e n t s  a n d  s h a p e  t h e  r o l l .
C . T o  c r e a t e  a  s t i c k y  b a s e  f o r  t h e  i n g r e d i e n t s .
D . T o  e n h a n c e  t h e  k e y  f l a v o r .a c t i o nt e m p o r a l
n e x tp u r p o s e
p u r p o s er o l l i n g  u p  a n d  
s q u e e z i n g  t h e  r o l l
r o l l  u ps q u e e z e r o l l 2 5 1 s  t o  3 0 3 s3 1 8 s  t o  3 4 5 sc u t r o l l  i n t o  p i e c e sp u r p o s e a s i e r  t o  e a t  
a n d  s e r v  r o l lS C K GS C K G
S C K GS C K GFigure 2. SOK-Bench data examples. Each QA pair corresponds to a video clip ( e.g., a video clip showing how to cook California Rolls )
and a type of situated commonsense knowledge ( e.g.,action + temporal + purpose ). For each question, we provide four options, the
correct choice, and the associated situated commonsense graphs.
010002000300040005000
O CT+ST
O CB+ST
O CT+GK
O CB+GK
O CT
O CB
O I
A PU+ST
A CT+ST
A PU
A CT
A PONumber of questionsA B C D
      
              
            
                    
        
        
     
       
        
       
                
       
       
       
    
    
   
       
       
    
    
                  
            
       
         
           (a) (b) Question types
Figure 3. (a)Sankey diagram of the 12 question types. (b)An-
swer distribution among options for each question type. Meaning
of abbreviations: O: Object; A: Action; CT: Counterfactual; CB:
Contribution; PU: Purpose; I: Inference; PO: Possibility; ST: Spa-
tiotemporal; GK: General knowledge. Notably, the “Spatiotempo-
ral” includes “obj attributes”, “obj-obj relations”, “obj attribute +
obj-obj relation”, and “before/after action” (see Section 3.5).
sessment, as depicted in Figure 3.
Benchmark Generation. We present a new method for cre-
ating our benchmark automatically in a structured, control-
lable, and scalable way. Our approach simplifies manual
and cumbersome processes and involves four streamlined
stages: 1) Extracting observable content from the videos; 2)
Compiling relevant commonsense knowledge exhaustively;
3) Aligning the content of the situations with this common-
sense knowledge to reveal underlying logical connections
and implications; 4) Formulating questions and answers by
integrating the gathered information.
We utilize in-context learning, few-shot demonstrations,
and multiple rounds of interactions to prompt LLMs and
MLLMs for the generation. They are guided by a set
of prompt templates to produce the proposed knowledge
graphs. For the final stage, these graphs aid in the creation
of question templates, either through manual construction
or by directing the LLM to generate question-answer pairs
(QAs). This strategy ensures a direct correlation between
the QAs, the graphs, and the rationale that maps the rea-
soning from questions to answers. We’ve taken measures to
ensure an unbiased distribution of answer options and haveemployed both LLM and human annotators to identify and
correct any grammatical errors.
3.2. Situated Knowledge Graph
We derive dynamic situations from videos, which provide
insights into event temporality, causality, and dynamics [4].
Recent research has increasingly represented objects and re-
lationships in real-world dynamic situations [10, 45, 47] and
visual scenes [14, 16, 56] using graphs. Building on this, we
expand upon traditional visual graphs to introduce our Situ-
ated Knowledge Graph. This includes more comprehensive
information derived from dynamic situations, such as peo-
ple and objects, their attributes, relationships, and actions.
Our process begins by generating object and action en-
tities, which are identified based on descriptions provided
with source videos ( e.g., shown in Figure 4). These de-
scriptions segment dynamic situations into distinct activity
moments, associating objects {oi}and actions {at}with
specific timestamps {yt}. Given original descriptions are
human-written and lack a standardized format, we employ
Large Language Models (LLMs) to parse the descriptive
text. The LLMs systematically convert the sentences into
organized lists of objects and actions, along with their cor-
responding timestamps. This approach streamlines the ex-
traction of key elements from the narrative text. For some
objects that are observable in video frames but were not an-
notated, we use BLIP2 [21] to recognize objects in frames
and add them to the initial object list {oi}. Next, we ex-
tract object attributes and spatial relationships between ob-
jects from selected keyframes within the videos. To achieve
this, we utilize MiniGPT4 [32] to articulate these object
attributes and their spatial interrelations. Using a desig-
nated goal prompt, our method efficiently generates object
attributes for visual, physical, and chemical properties and
relationships ( e.g., #Prompt#: Please describe the tofu in
the image in terms of size, shape, color, and texture in one
short sentence within 10 words. #Model#: The tofus are
small, white, sliced squares.). Finally, we align the times-
13387
Video
16s -30s: cut tofu into square pieces
32s -40s: put cornstarch in a small bowl and mix with water 
…
Annotation
Situated Knowledge Graph
 Human
Instruct LLM 
to generate 
knowledge
General 
knowledge 
aspectsGeneral Knowledge Graph
Actions Things related to, general purpose, ways of … 
Objects Physical, chemical, cultural, usage, …
LLM prompt GK examples Generation goal Output format
#System #:You are a helpful, pattern -
following assistant. You know a lot about
cornstarch .
#User#: Please tell me 5 usages of water.
Format: The answer should follow format 
'<the thing applied on>: <how to apply>’. … 
The answer is a list in JSON…
#Assistant# : [{'human': 'hydration'}, 
{'dirt': 'cleaning'}, {'food': 'cooking'}, 
{'crop': 'irrigation'}, {'electricity': 
'power generation’}]
#User#: Please tell me 5 usages of 
cornstarch. Format: The answer should 
follow format … 
#Assistant# : [{sauce: thickening}, …
Situated Commonsense Knowledge Graph
 LLM prompt
SCK examples Generation goal Output format Video content General knowledgeCounterfactual , purpose, …Actions
Objects
#System #: You are a helpful, pattern -following assistant. As a commonsense -driven assistant, your task is to analyze a 
video (length: 119 seconds) that showcases a person introducing 6 cooking steps of cooking mapotofu in kitchen.
The steps are described in a specific format … [video content] …
The background knowledge of the objects appeared in the video is … [general knowledge] …
#User#: What would happen if the person did not use cornstarch? Format: The answer should be short and within 20 words. … 
#Assistant# : The sauce would lack thickness and the dish would have a thinner consistency.
#User#: What would happen if the person did not use ground pork? Format: The answer should be short and within 20 words. … 
#Assistant# : The dish would lack meaty flavor, texture, and richness, resulting in a vegetarian version of mapotofu.
↓Objects and 
actions in 
JSON format
LLM
 VL model
Objects Actions Objects in frames
 +
VL model
Obj attributesAlign 
with 
objectsObj-obj relationship
Figure 4. Generation pipeline of Situated Knowledge Graph (SKG, see Section 3.2), General Knowledge Graph (GKG, see Section 3.3),
and Situated Commonsense Knowledge Graph (SCKG, see Section 3.4).
tamps of frames with those of actions. When an action’s
timestamp coincides with the time at which an object ap-
pears in a frame, we link this action and object by adding
an(action, object) edge to the situated knowledge graph.
Additionally, for each action, we build an additional edge
(timestamp, action) to indicate the action sequence.
3.3. General Knowledge Graph
We construct the general knowledge graph beginning from
the object oior action atas initial nodes. Taking “tofu” as a
node example (a food product prepared by coagulating soy
milk and then pressing the resulting curds into solid white
soft toufu blocks), prompting an LLM with a goal query
(e.g., “Tell me some common knowledge about tofu”) is a
basic approach. While this will generate descriptions with
relevant knowledge, the resulting content lacks specificity
and completeness, and always misses key points (e.g., the
color of tofu). Thus, we propose multiple aspects for knowl-
edge generation ( e.g.,usage ,physical , and culture , etc) and
collaborate with “goal” to make it with concrete descrip-
tions. The direct challenges of LLM outputs are verbose,
unstructured, or repetitive ( e.g., “ Tofu is a popular food
...” or “Tofu is a versatile ingredient that ...”). To addressthis, we use general knowledge examples with the prede-
fined output format to extend the knowledge graph edges
or nodes. Moreover, relying solely on the format prompt
can lead to format inconsistencies in outputs, such as miss-
ing quotation marks ( e.g.,{“tofu”: ... {“physical”: “color”:
“white }}that cause parsing failures. Thus, we utilize a
prompt triplet ⟨general knowledge examples π,generation
goalω,output format for GKGs (Figure 4). By employing
both general knowledge examples andoutput format , we
ensure the generated knowledge is not only of high qual-
ity but also consistently parseable. Here are the generated
outputs for cornstarch usages: [ {“sauce”: “thickening” },
{“batter”: “making crispy” },{“body powder”: “absorbing
moisture” },{“starch”: “making” },{“playdough”: “creat-
ing homemade” }].
3.4. Situated Commonsense Knowledge Graph
We propose Situated Commonsense Knowledge Graph
(SCKG) by integrating information from the previously es-
tablished SKG and GKG for specific dynamic situations. To
illustrate the significance and complexity of generating sit-
uated commonsense knowledge, consider a straightforward
example: thinking about the impact of omitting cornstarch
13388
in the preparation of mapo tofu. Merely knowing that corn-
starch is a powdery substance (from general knowledge) is
insufficient. We need to combine this with its situated use
from the SKG ( e.g., “The person mixes cornstarch with wa-
ter and adds the mixture to the soup.”) and its general prop-
erties from the GKG ( e.g., “Cornstarch can be used as a
thickening agent.”). The result is “The sauce would be less
thick, and the dish would have a thinner consistency.”
We initially thought that a four-element prompt, ⟨video
content gv,general knowledge kg,generation goal ω,out-
put format ϕ⟩, would work for generating situated common-
sense knowledge. Our test indicates that the model, while
generating correct responses, often provides answers that
are too general. For instance, in response to a question
about the impact of not using ground pork in mapo tofu,
the model might simply reply, “The dish would lack flavor
and texture” which, although accurate, lacks detail.
Directly providing examples to instruct the LLM to
achieve a specific level of detail isn’t practical due to the
dependency of knowledge on the varying video content. To
address this issue, we propose a method called Few-Shot
Self-Prompting.
Few-Shot Self-Prompting uses previously generated
situated commonsense knowledge as examples. The prompt
consists of five elements: ⟨video content gv,general knowl-
edge kg,situated commonsense knowledge examples π,
generation goal ω⟩. The details of the algorithm can be
found in the Supplementary Materials.
Figure 4 demonstrates that the situated knowledge ex-
amples are constructed using knowledge about the conse-
quences of not using a specific ingredient ( e.g., not using
cornstarch during cooking). Over time, this process leads
to more concrete and specific situated commonsense knowl-
edge. The final result, for example, would be, “The dish
would lack meaty flavor, texture, and richness, resulting in
a vegetarian version of mapo tofu.”
Our Few-Shot Self-Prompting technique enables us to
explore various situated commonsense knowledge perspec-
tives, such as considering counterfactual scenarios, under-
standing the purpose of an action, and recognizing an ob-
ject’s contribution. Our experiments show that just two it-
erations of knowledge generation can produce high-quality
results, ultimately leading to the creation of the SCKG.
3.5. Question and Answer Generation
Using the three graphs, we create question-answer pairs to
test the model’s ability to make accurate inferences based
on situational facts and essential commonsense knowledge.
We can use two approaches for this: (1)Manually create
question-answer templates in a bottom-up manner, design-
ing question templates and providing answers based on the
graphs. (2)Automatically generate questions using a LLM
in a top-down manner.3.5.1 Bottom-up QA Generation
We manually design question templates in multi hog way
combing both “situation” and “commonsense” (please see
the details of template types in Supplementary Materials).
For one object / action, we find the connected edges in
the three graphs. Next, we design question templates that
jump from edges in Situated Knowledge Graph or Gen-
eral Knowledge Graph to those in Situated Commonsense
Knowledge Graph. Concretely, for example, the template
can be “What would happen if the person did not use the
<obj attribute >and<obj-obj relation >?”(see one con-
crete question example concerning “cornstarch” in Fig-
ure 1). The model needs to first do spatiotemporal reason-
ing to identify the object by <obj attribute >and<obj-obj
relation >(e.g., “white substance” and “dissolved in water
in glass”). Next, the model needs to do situated common-
sense reasoning to understand the consequence if that object
was not used. In such way, we can comprehensively as-
sess model’s situated commonsense reasoning ability. Note
that we also present some easier templates ( e.g., “vanilla obj
counterfactual”) which are single hog.
Apart from the question and the correct answer, consis-
tent with VCR [57], we generate wrong answers that are
relevant but sufficiently dissimilar from the correct answers
to minimize shortcut risk (shown in Figure 2). Incorrect
options are chosen from the same category of contextual
knowledge related to other objects or actions. Also, we en-
sure an even distribution of the four choices (Figure 3(b)).
3.5.2 Top-down QA Generation
Bottom-up QA generation allows for control over question
quality and difficulty but may not create diverse question
types. To address this, we suggest a top-down approach for
QA generation. We design a structured prompt with five
elements: ⟨video content ,integrated graph ,QA examples ,
generation goal ,output format ⟩. The integrated graph com-
bines SKG, GKG, and SCKG. The generation goal directs
the LLM to create multi-hog questions based on multiple
edges from the integrated graph. This prompt design keeps
a strong connection between generated QAs and graphs, re-
ducing the chance of model hallucination compared to bas-
ing QAs only on video content.
However, the top-down approach has drawbacks, and
it’s resource-intensive and slow. With the three graphs in-
volved, it takes a few seconds to generate one question using
the top-down method, while the bottom-up method gener-
ates the entire benchmark in the same time. In this paper,
we mainly use the bottom-up method to create the bench-
mark, but we discuss top-down generation case studies in
the Supplementary.
13389
3.6. Quality Valiation
We invite human helpers to assess the quality by inspecting
a subset of the graphs and QAs. We find 93.08% of QA
pairs are valid. Please see the Supplementary for details.
3.7. Data Source
We created the benchmark based on videos of the two public
video datasets about daily activities.
YouCook2 [61] is a large set of instructional cooking videos
from YouTube. It includes over 2,000 videos across 89
recipe categories, with each video annotated with detailed
step-by-step instructions.
HOMAGE [40] contains 1,752 video clips featuring 75
daily human activities in home environments. The dataset is
thoroughly annotated with action labels, object information,
and spatial-temporal relationships
4. Experiments
To evaluate the quality of our dataset, we ask two questions,
which are answered in the following sections:
• Do current methods effectively tackle our generated prob-
lems? Why do they perform better/worse compared with
other QA datasets?
• Is our QA generation procedure efficient and justifiable?
4.1. Baseline Results
To address the first question, we select a range of recent
video generation models, including Video-LLaMa [58],
PandaGPT [42], Video-ChatGPT [22], AskAnything [22],
and Valley [26]. For deployable models, we specifically
chose their Vicuna-7B-based versions [38]. Detailed base-
line settings are described in the appendix.
Baseline experiment settings. The baseline models are
tested in two settings: “multiple-choice” and “direct-
answer”. In the multiple-choice setting, models receive a
question and four choice candidates; in the direct-answer
setting, models are required to provide direct and succinct
responses to the questions. To calculate the accuracy in the
multiple-choice setting, we need to parse an integer as the
choice taken by the model. If the parsing algorithm fails, we
calculate the BLEU score between model output and each
choice candidate, picking the highest one as the model’s an-
swer. Results are shown in Table 2. We measure the BLEU
score [34] and BERT-F1 score [59] between the model’s
output and the generated answer in the “direct-answer” set-
ting. Results are shown in Table 3 and 4 respectively.
Result Analysis. From the table, we have the following
observations. First, all baseline models perform far from
perfect. GPT4v model has the best performance across dif-
ferent settings, which is consistent with its superior perfor-
mance on standard leaderboards like [25] and [13]. The
best performing AskAnything (where ChatGPT is used as
S i n g a p o r e  C u r r y  L a k s a     + +
?
C o r r e c t  A n s :  P o u r  o i l  i n t o  a  h o t  p a n             G P T 4 v :  I ' m  s o r r y ,  b u t  I  c a n ' t  p r o v i d e  . . .a c t i o n
W h a t  d i d  t h e  p e r s o n  d o d i d  s o m e t h i n gt e m p o r a l
b e f o r ep u r p o s e
t o  e n h a n c e  t h e  
f l a v o r  a n d  a r o m a  o f  t h e  c u r r y  l a k s a  d i s hQ u e s :    t h e  p e r s o n   e n h a n c e  f l a v o rP o u r  o i la d d  c h o p p e d  o n i o n s . . .R e a s o n i n g
b e f o r e
p u r p o s e
Figure 5. GPT4v’s ability to perform complex combined spa-
tiotemporal and situated commonsense reasoning is limited. The
model needs to do two-hog reasoning, i.e., understanding the pur-
pose of “adding chopped onions” is to “enhance the flavor” while
knowing the previous action is “pouring oil”.
the language model) only has a BLEU score of 0.110 in
table 3. It shows the value of the proposed benchmark to
evaluate vision-language models’ capabilities to understand
and reason in videos.
Secondly, we notice a significant performance gap be-
tween API models (the GPT family) and deployable mod-
els (the LLaMa family with Vicuna-7B base model). We
remark that this phenomenon can be attributed to the dif-
ference in both sizes and instruction-following abilities be-
tween GPT LLM family and LLaMa LLM family.
Third, we find that most models perform better on sim-
pler question types ( e.g., OCT and APU) compared to more
complex ones ( e.g., OCB+ST and OCT+ST). For simpler
questions, the choices might reveal the answers ( e.g., Q:
“What did the person use to provide a sticky base to hold
ingredients for California Roll?”, Choices: “Rice”, “Av-
ocado”, “Nori”, and “Cucumber”). GPT4v may use this
to achieve high multi-choice accuracy on OCB. However,
GPT4v might struggle when both spatiotemporal and situ-
ated commonsense reasoning are required (see Figure 5).
4.2. Ablation Studies
We answer the second question by conducting ablation
studies to evaluate the effectiveness of each component
within the generation method for the three graphs.
MLLMs for SKG Generation. To justify the usage of
MLLMs which provide spatiotemporal information, we fo-
cus on the four type QAs, namely “Object Counterfactual”,
“Object Counterfactual + Spatiotemporal”, “Object Contri-
bution”, and “Object Contribution + Spatiotemporal”. In
the human evaluation, we ask whether the reasoning ques-
tions require to observe the video content, with results
showing 91%, 99%, 94%, and 99% respectively. This con-
firms that MLLMs are critical to generating high-quality sit-
uated commonsense questions.
Effectiveness of Prompt Design in GKG Generation. We
use LLM to generate KGs in parsable JSON format. Here,
we test the effectiveness of specifying output format and
providing examples in the prompt structure for GKG gen-
eration. When given only the goal of the task, 0%of the
13390
Table 2. Accuracy of baseline models in multiple-choice setting. The question types are represented by numbers from 1 to 12: 1.
Object Counterfactual + Spatiotemporal (OCT+ST); 2. Object Contribution + Spatiotemporal (OCB+ST); 3. Object Counterfactual +
General Knowledge (OCT+GK); 4. Object Contribution + General Knowledge (OCB+GK); 5. Object Counterfactual (OCT); 6. Object
Contribution (OCB); 7. Object Inference (OI); 8. Action Purpose + Spatiotemporal (APU+ST); 9. Action Counterfactual + Spatiotemporal
(ACT+ST); 10. Action Purpose (APU); 11. Action Counterfactual (ACT); 12. Action Possibility (APO).
ModelsQuestions concerning objects Questions concerning actionsOverall
1 2 3 4 5 6 7 8 9 10 11 12
blind ChatGPT [30] 0.224 0.318 0.441 0.275 0.651 0.488 0.206 0.136 0.135 0.574 0.729 0.189 0.365
GPT4v [31] 0.000 0.600 0.600 0.333 0.800 1.000 0.400 0.300 0.222 0.778 0.885 0.600 0.539
Video-LLaMa [58] 0.251 0.265 0.254 0.258 0.301 0.273 0.319 0.267 0.257 0.255 0.257 0.242 0.264
PandaGPT [42] 0.264 0.281 0.286 0.281 0.373 0.335 0.368 0.236 0.223 0.344 0.445 0.350 0.312
Ask Anything [22] 0.256 0.362 0.441 0.404 0.600 0.543 0.276 0.288 0.180 0.713 0.729 0.65 0.455
Video-ChatGPT [22] 0.309 0.206 0.274 0.210 0.394 0.311 0.274 0.277 0.186 0.358 0.458 0.409 0.312
Valley [26] 0.281 0.265 0.322 0.276 0.288 0.290 0.343 0.244 0.202 0.368 0.487 0.353 0.311
Table 3. BLEU score of baseline models in direct-answer setting.
ModelsQuestions concerning objects Questions concerning actionsOverall
1 2 3 4 5 6 7 8 9 10 11 12
blind ChatGPT [30] 0.032 0.053 0.062 0.038 0.062 0.009 0.010 0.035 0.133 0.046 0.105 0.036 0.052
GPT4v [31] 0.142 0.150 0.078 0.072 0.198 0.015 0.018 0.018 0.124 0.000 0.134 0.092 0.085
Video-LLaMa [58] 0.050 0.051 0.047 0.044 0.051 0.002 0.006 0.026 0.056 0.017 0.058 0.028 0.036
PandaGPT [42] 0.089 0.096 0.085 0.082 0.097 0.008 0.014 0.054 0.103 0.035 0.123 0.052 0.070
Ask Anything [22] 0.141 0.144 0.110 0.111 0.174 0.013 0.030 0.083 0.127 0.194 0.202 0.077 0.110
Video-ChatGPT [22] 0.142 0.125 0.108 0.090 0.152 0.014 0.044 0.079 0.143 0.109 0.164 0.067 0.096
Valley [26] 0.099 0.096 0.095 0.084 0.107 0.010 0.012 0.070 0.112 0.035 0.134 0.048 0.068
Table 4. BERT score of baseline models in direct-answer setting.
ModelsQuestions concerning objects Questions concerning actionsOverall
1 2 3 4 5 6 7 8 9 10 11 12
blind ChatGPT [30] 0.887 0.878 0.892 0.881 0.884 0.896 0.887 0.886 0.883 0.878 0.892 0.886 0.886
GPT4v [31] 0.954 0.953 0.955 0.959 0.956 0.955 0.953 0.963 0.954 0.957 0.952 0.957 0.956
Video-LLaMa [58] 0.961 0.960 0.961 0.961 0.961 0.961 0.961 0.961 0.961 0.961 0.961 0.961 0.961
PandaGPT [42] 0.952 0.953 0.952 0.952 0.953 0.952 0.953 0.952 0.952 0.952 0.952 0.952 0.952
Ask Anything [22] 0.957 0.957 0.958 0.956 0.961 0.960 0.955 0.962 0.959 0.964 0.966 0.959 0.959
Video-ChatGPT [22] 0.955 0.955 0.957 0.956 0.957 0.963 0.952 0.956 0.958 0.958 0.961 0.956 0.957
Valley [26] 0.955 0.955 0.955 0.955 0.954 0.954 0.955 0.955 0.955 0.955 0.954 0.955 0.955
outputs are valid; specifying output format, this figure in-
creases to 11.1%; with both output format and examples,
all outputs can be parsed as JSON files.
Effectiveness of Few-Shot Self-Prompting. We argue the
few-shot examples of SCKG generation are necessary. By
human testing, we evaluate the proportion of concrete and
specific knowledge relations as opposed to un-situated, gen-
eral knowledge. With Few-Shot Self-Prompting, 97% of the
generated knowledge is concrete while the ratio decreases
to61% without the prompt technique.
5. Conclusion
Our research introduces a novel benchmark for Situated
Open-World Commonsense Reasoning, advancing AI’sability to comprehend and reason in dynamic, real-world
contexts. This benchmark includes a wide range of ques-
tions and situational analyses that go beyond traditional rea-
soning paradigms, challenging existing AI systems. Our
novel approach in dataset generation offers scalability and
enhanced logic in QA pair creation. While current models
show promise, our experiment findings highlight the need
for significant improvements, pointing towards exciting av-
enues for future general artificial intelligence.
6. Acknowledgements
We appreciate the contributions of our data quality valida-
tion team, Jiajing Zhang, Bingze Dai, Renxian Wang, Yue
Xu, Yucong Li, Tuo Zhou, Wei Yi Oon, and Zijian Li.
13391
References
[1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C Lawrence Zitnick, and
Devi Parikh. Vqa: Visual question answering. In
ICCV , 2015.
[2] Tayfun Ates, M Samil Atesoglu, Cagatay Yigit, Ilker
Kesen, Mert Kobas, Erkut Erdem, Aykut Erdem, Tilbe
Goksun, and Deniz Yuret. Craft: A benchmark for
causal reasoning about forces and interactions. arXiv ,
2020.
[3] S ´ebastien Bubeck, Varun Chandrasekaran, Ronen El-
dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter
Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.
Sparks of artificial general intelligence: Early exper-
iments with gpt-4. arXiv preprint arXiv:2303.12712 ,
2023.
[4] Shyamal Buch, Crist ´obal Eyzaguirre, Adrien Gaidon,
Jiajun Wu, Li Fei-Fei, and Juan Carlos Niebles. Re-
visiting the” video” in video-language understanding.
InProceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition , pages 2917–
2927, 2022.
[5] Aman Chadha and Vinija Jain. ireason: Multi-
modal commonsense reasoning using videos and nat-
ural language with interpretability. arXiv preprint
arXiv:2107.10300 , 2021.
[6] Yingshan Chang, Mridu Narang, Hisami Suzuki, Gui-
hong Cao, Jianfeng Gao, and Yonatan Bisk. Webqa:
Multihop and multimodal qa. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 16495–16504, 2022.
[7] Zhenfang Chen, Kexin Yi, Antonio Torralba, Josh
Tenenbaum, and Chuang Gan. Comphy: Composi-
tional physical reasoning of objects and events from
videos. In International Conference on Learning Rep-
resentations , 2022.
[8] Bosheng Ding, Chengwei Qin, Linlin Liu, Lidong
Bing, Shafiq Joty, and Boyang Li. Is gpt-3 a good
data annotator? arXiv , 2022.
[9] Zhiyuan Fang, Tejas Gokhale, Pratyay
Banerjee, Chitta Baral, and Yezhou Yang.
Video2Commonsense: Generating commonsense
descriptions to enrich video captioning. In EMNLP ,
2020.
[10] Madeleine Grunde-McLaughlin, Ranjay Krishna, and
Maneesh Agrawala. Agqa: A benchmark for com-
positional spatio-temporal reasoning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 11287–11297, 2021.
[11] Pranay Gupta and Manish Gupta. Newskvqa:
Knowledge-aware news video question answering. In
Pacific-asia conference on knowledge discovery and
data mining , pages 3–15. Springer, 2022.[12] Rishi Hazra, Brian Chen, Akshara Rai, Nitin Kamra,
and Ruta Desai. Egotv: Egocentric task verification
from natural language task descriptions. arXiv , 2023.
[13] Dan Hendrycks, Collin Burns, Steven Basart, Andy
Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-
hardt. Measuring massive multitask language under-
standing. ICLR , 2021.
[14] Drew A Hudson and Christopher D Manning. Gqa: A
new dataset for real-world visual reasoning and com-
positional question answering. In CVPR , 2019.
[15] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin
Kim, and Gunhee Kim. Tgif-qa: Toward spatio-
temporal reasoning in visual question answering. In
CVPR , 2017.
[16] Jingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan Car-
los Niebles. Action genome: Actions as compositions
of spatio-temporal scene graphs. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 10236–10247, 2020.
[17] Justin Johnson, Bharath Hariharan, Laurens Van
Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross
Girshick. Clevr: A diagnostic dataset for composi-
tional language and elementary visual reasoning. In
CVPR , 2017.
[18] Justin Johnson, Bharath Hariharan, Laurens
Van Der Maaten, Judy Hoffman, Li Fei-Fei, C
Lawrence Zitnick, and Ross Girshick. Inferring
and executing programs for visual reasoning. In
Proceedings of the IEEE International Conference on
Computer Vision , pages 2989–2998, 2017.
[19] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L
Berg. Tvqa: Localized, compositional video question
answering. In EMNLP , 2018.
[20] Jiangtong Li, Li Niu, and Liqing Zhang. From
representation to reasoning: Towards both evidence
and commonsense reasoning for video question-
answering. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition
(CVPR) , 2022.
[21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven
Hoi. Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large lan-
guage models. arXiv preprint arXiv:2301.12597 ,
2023.
[22] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wen-
hai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu
Qiao. Videochat: Chat-centric video understanding.
arXiv preprint arXiv:2305.06355 , 2023.
[23] Haotian Liu, Chunyuan Li, Qingyang Wu, and
Yong Jae Lee. Visual instruction tuning. arXiv
preprint arXiv:2304.08485 , 2023.
[24] Jingzhou Liu, Wenhu Chen, Yu Cheng, Zhe Gan,
Licheng Yu, Yiming Yang, and Jingjing Liu. Violin: A
13392
large-scale dataset for video-and-language inference.
InCVPR , 2020.
[25] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-
yuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei
Chang, Michel Galley, and Jianfeng Gao. Mathvista:
Evaluating math reasoning in visual contexts with gpt-
4v, bard, and other large multimodal models. arXiv ,
2023.
[26] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong,
Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu
Wei. Valley: Video assistant with large lan-
guage model enhanced ability. arXiv preprint
arXiv:2306.07207 , 2023.
[27] Kenneth Marino, Mohammad Rastegari, Ali Farhadi,
and Roozbeh Mottaghi. Ok-vqa: A visual question
answering benchmark requiring external knowledge.
InProceedings of the IEEE/cvf conference on com-
puter vision and pattern recognition , pages 3195–
3204, 2019.
[28] Jonghwan Mun, Paul Hongsuck Seo, Ilchae Jung, and
Bohyung Han. Marioqa: Answering questions by
watching gameplay videos. In ICCV , 2017.
[29] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng. Ms
marco: A human-generated machine reading compre-
hension dataset. 2016.
[30] OpenAI. ChatGPT: Optimizing Language Models for
Dialogue. https://openai.com/chatgpt ,
2023. Version 4.0.
[31] OpenAI. Gpt-4 technical report, 2023.
[32] R OpenAI. Gpt-4 technical report. arXiv , pages 2303–
08774, 2023.
[33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
Training language models to follow instructions with
human feedback. Advances in Neural Information
Processing Systems , 35:27730–27744, 2022.
[34] Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. Bleu: a method for automatic evalu-
ation of machine translation. In ACL, 2002.
[35] Jae Sung Park, Chandra Bhagavatula, Roozbeh Mot-
taghi, Ali Farhadi, and Yejin Choi. Visualcomet: Rea-
soning about the dynamic context of a still image. In
Computer Vision–ECCV 2020: 16th European Con-
ference, Glasgow, UK, August 23–28, 2020, Proceed-
ings, Part V 16 , pages 508–524. Springer, 2020.
[36] Maitreya Patel, Tejas Gokhale, Chitta Baral, and
Yezhou Yang. Cripp-vqa: Counterfactual reasoning
about implicit physical properties via video question
answering. arXiv preprint arXiv:2211.03779 , 2022.
[37] Xiaohuan Pei, Yanxi Li, and Chang Xu. Gpt self-
supervision for a better data annotator. arXiv , 2023.[38] Baolin Peng, Chunyuan Li, Pengcheng He, Michel
Galley, and Jianfeng Gao. Instruction tuning with gpt-
4, 2023.
[39] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. Language models
are unsupervised multitask learners. OpenAI blog , 1
(8):9, 2019.
[40] Nishant Rai, Haofeng Chen, Jingwei Ji, Rishi De-
sai, Kazuki Kozuka, Shun Ishizaka, Ehsan Adeli, and
Juan Carlos Niebles. Home action genome: Coopera-
tive compositional action understanding. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 11184–11193, 2021.
[41] Dustin Schwenk, Apoorv Khandelwal, Christopher
Clark, Kenneth Marino, and Roozbeh Mottaghi. A-
okvqa: A benchmark for visual question answering
using world knowledge. In European Conference on
Computer Vision , pages 146–162. Springer, 2022.
[42] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan
Wang, and Deng Cai. Pandagpt: One model
to instruction-follow them all. arXiv preprint
arXiv:2305.16355 , 2023.
[43] Niket Tandon, Aparna S Varde, and Gerard de Melo.
Commonsense knowledge in machine intelligence.
ACM SIGMOD Record , 46(4):49–52, 2018.
[44] Hugo Touvron, Thibaut Lavril, Gautier Izacard,
Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee
Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Ham-
bro, Faisal Azhar, et al. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 , 2023.
[45] Aisha Urooj, Hilde Kuehne, Bo Wu, Kim Chheu,
Walid Bousselham, Chuang Gan, Niels Lobo, and
Mubarak Shah. Learning situation hyper-graphs for
video question answering. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 14879–14889, 2023.
[46] Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick,
and Anton Van Den Hengel. Fvqa: Fact-based vi-
sual question answering. IEEE transactions on pattern
analysis and machine intelligence , 40(10):2413–2427,
2017.
[47] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenen-
baum, and Chuang Gan. Star: A benchmark for situ-
ated reasoning in real-world videos. In NeurIPS , 2021.
[48] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng
Chua. Next-qa: Next phase of question-answering to
explaining temporal actions. In CVPR , 2021.
[49] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang
Zhang, Xiangnan He, and Yueting Zhuang. Video
question answering via gradually refined attention
over appearance and motion. In ACM Multimedia ,
2017.
13393
[50] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A
large video description dataset for bridging video and
language. In CVPR , pages 5288–5296, 2016.
[51] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
gio, William W Cohen, Ruslan Salakhutdinov, and
Christopher D Manning. Hotpotqa: A dataset for
diverse, explainable multi-hop question answering.
arXiv , 2018.
[52] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli,
Jiajun Wu, Antonio Torralba, and Joshua B Tenen-
baum. Clevrer: Collision events for video represen-
tation and reasoning. In International Conference on
Learning Representations , 2020.
[53] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao,
Yueting Zhuang, and Dacheng Tao. Activitynet-qa:
A dataset for understanding complex web videos via
question answering. In AAAI , 2019.
[54] Amir Zadeh, Michael Chan, Paul Pu Liang, Edmund
Tong, and Louis-Philippe Morency. Social-iq: A ques-
tion answering benchmark for artificial social intelli-
gence. In CVPR , 2019.
[55] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and
Yejin Choi. Swag: A large-scale adversarial dataset
for grounded commonsense inference. arXiv preprint
arXiv:1808.05326 , 2018.
[56] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin
Choi. From recognition to cognition: Visual com-
monsense reasoning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recogni-
tion, pages 6720–6731, 2019.
[57] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin
Choi. From recognition to cognition: Visual common-
sense reasoning. In CVPR , 2019.
[58] Hang Zhang, Xin Li, and Lidong Bing. Video-
llama: An instruction-tuned audio-visual language
model for video understanding. arXiv preprint
arXiv:2306.02858 , 2023.
[59] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. Bertscore: Evaluating
text generation with bert, 2020.
[60] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-
a-judge with mt-bench and chatbot arena, 2023.
[61] Luowei Zhou, Chenliang Xu, and Jason Corso. To-
wards automatic learning of procedures from web in-
structional videos. In Proceedings of the AAAI Con-
ference on Artificial Intelligence , 2018.
[62] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and
Mohamed Elhoseiny. Minigpt-4: Enhancing vision-
language understanding with advanced large language
models. arXiv preprint arXiv:2304.10592 , 2023.
13394
