Person-in-WiFi 3D: End-to-End Multi-Person 3D Pose Estimation with Wi-Fi
Kangwei Yan1, Fei Wang1*, Bo Qian1, Han Ding1, Jinsong Han2, Xing Wei1
1Xi’an Jiaotong University, Xi’an 710049, China
2Zhejiang University, Hangzhou 310058, China
{yankangwei,qb90531}@stu.xjtu.edu.cn {feynmanw,dinghan,weixing}@xjtu.edu.cn,hanjinsong@zju.edu.cn
Abstract
Wi-Fi signals, in contrast to cameras, offer privacy pro-
tection and occlusion resilience for some practical scenar-
ios such as smart homes, elderly care, and virtual reality.
Recent years have seen remarkable progress in the esti-
mation of single-person 2D pose, single-person 3D pose,
and multi-person 2D pose. This paper takes a step for-
ward by introducing Person-in-WiFi 3D, a pioneering Wi-Fi
system that accomplishes multi-person 3D pose estimation.
Person-in-WiFi 3D has two main updates. Firstly, it has a
greater number of Wi-Fi devices to enhance the capability
for capturing spatial reflections from multiple individuals.
Secondly, it leverages the Transformer for end-to-end esti-
mation. Compared to its predecessor, Person-in-WiFi 3D is
storage-efficient and fast. We deployed a proof-of-concept
system in 4m ×3.5m areas and collected a dataset of over
97K frames with seven volunteers. Person-in-WiFi 3D at-
tains 3D joint localization errors of 91.7mm (1-person),
108.1mm (2-person), and 125.3mm (3-person), comparable
to cameras and millimeter-wave radars. The project page
is at https://aiotgroup.github.io/Person-in-WiFi-3D.
1. Introduction
Human pose estimation is a critical technology with broad
applications in areas like elderly care, virtual reality, and
smart homes. To achieve precise pose estimation, re-
searchers have explored various methods, including cam-
eras [2, 6,18,26,33], radars [1, 13,15,23,39], and Wi-Fi
signals [10, 21,22,28,29,41]. Among these, camera-based
solutions are most extensively studied, supported by a large
research community and a wealth of labeled and unlabeled
data. This has led to the development of well-known frame-
works like convolutional pose machines [33], OpenPose [2],
AlphaPose [6], Hourglasses network [18], HRNet [26], and
more. However, camera solutions are not always applica-
ble due to their dependence on proper lighting conditions
*Corresponding author.and field of view. They also struggle in severe occlusion
scenarios. Additionally, cameras capturing sensitive infor-
mation such as identity and appearance can lead to privacy
concerns in scenarios where privacy is paramount.
Unlike camera-based solutions, Wi-Fi methods are re-
silient to occlusions and do not capture sensitive personal
details, making them well-suited for indoor scenarios. Cur-
rent Wi-Fi solutions have advanced in estimating single-
person 2D/3D poses. This process involves a regression
problem, mapping Wi-Fi signal variations, caused by an in-
dividual’s movements and presence, to their corresponding
2D/3D pose coordinates. For instance, WiSPPN [28] pre-
dicts 2D keypoint coordinates using pose adjacent matrix
similarity loss. Similarly, MetaFi++ [41] estimates 2D co-
ordinates employing mean squared error loss. In single-
person 3D pose estimation, solutions like WiPose [10],
Winect[21], and GoPose [22] also utilize mean squared er-
ror to learn 3D coordinates. In the case of Wi-Fi-based
multi-person pose estimation, it’s challenging to distinctly
segment individuals from 1-dimensional (1D) Wi-Fi sig-
nals. Addressing this, Person-in-WiFi [29] adopts tech-
niques from OpenPose [2], initially regressing keypoint
heatmaps and part affinity fields, and then associating these
with individuals. One alternative approach is Densepose
from Wi-Fi [7], which transforms 1D Wi-Fi signals to
1280×720×3 image-like tensors under the supervision of
synchronized images. This method may favor overfitting
colors in training scenes, such as the color of subjects’
clothing and surrounding objects, as Wi-Fi signals do not
inherently capture color information.
Up to now, multi-person 3D pose estimation using Wi-
Fi signals remains an unsolved challenge. In our initial at-
tempt to evolve Person-in-WiFi into a 3D version for 14
keypoints, we represented multi-person poses with 3D key-
point heatmaps ∈14×64×64×64and 3D part affinity
fields∈42×64×64×64. We replaced 2D operations, like
convolutions in Person-in-WiFi, with 3D counterparts, and
modified the pose-processing algorithms to produce 3D co-
ordinates from the 3D heatmaps and fields. However, this
deep network failed to converge. We identified six major
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
969
Figure 1. This paper presents Person-in-WiFi 3D, the first multi-person 3D pose estimation system with Wi-Fi signals.
shortcomings in this approach: (1) the 3D network’s exces-
sive size; (2) slow training of the 3D network; (3) sluggish
3D post-processing; (4) coarse spatial resolution, for exam-
ple, e.g., 4000mm/64 = 62.5mm in a 4m×4m×4m space;
(5) the lack of an end-to-end process, leading to error ac-
cumulation during network prediction and post-processing;
(6) inefficient use of storage and memory, as large matrices
for keypoint heatmaps and part affinity fields are primarily
used to store 3D pose coordinates ∈p×14×3forppersons.
In this paper, we introduce Person-in-WiFi 3D. Com-
pared to its 2D predecessor, Person-in-WiFi 3D features
two significant updates. Firstly, it incorporates three Wi-Fi
receivers placed at the corners of the sensing area, captur-
ing more signal reflections from different human body parts.
Secondly, Person-in-WiFi 3D is based on Transformer [27]
and DETR [3], enabling direct estimation of multi-person
3D poses from Wi-Fi signals. Person-in-WiFi 3D consists
of three modules: a Wi-Fi encoder, a pose decoder, and a
refine decoder. The Wi-Fi encoder is designed to extract
global context from tokenized Wi-Fi signals. In the pose de-
coder, multiple queries, initialized randomly, interact with
the globally extracted information to predict a set of poses.
The refine decoder then further refines these predictions.
We approach multi-person pose estimation as a set predic-
tion task and utilize a set-based Hungarian loss, ensuring
each individual’s ground-truth pose is uniquely predicted.
Our system achieves 3D joint localization errors of 91.7mm
in single-person scenarios, 108.1mm in two-person scenar-
ios, and 125.3mm in three-person scenarios, as proven on a
self-collected dataset with over 97,000 Wi-Fi samples. Our
contributions in this work are outlined below:
(1) We introduce Person-in-WiFi 3D, a pioneering sys-
tem designed for estimating multi-person 3D poses using
Wi-Fi signals, marking a first in this area.
(2) We develop the Wi-Fi Pose Transformer, an innova-
tive algorithm that converts Wi-Fi signals into multi-person
3D poses in an end-to-end manner.
(3) We comprehensively evaluate Person-in-WiFi 3D inour self-collected dataset, over 97,000 samples. Our dataset
is ethically cleared for public release. This makes it the
first Wi-Fi pose estimation dataset available for open access,
promising to expedite future research in this domain.
2. Related work
2.1. Human Pose from Images
Multi-person pose estimation from images is a task of de-
tecting joint keypoints of every person in images. It basi-
cally has two main paradigms, i.e., top-down and bottom-
up. In the top-down paradigm, a person detector like
Faster RCNN [8] and Yolo [20] first crops regions of ev-
ery person from an image, then a single-person pose esti-
mator regresses the person keypoints region by region. This
paradigm is more accurate and has many record-breaking
work, such as Hourglass networks [18], AlphaPose [6],
and HR-Net [26]. In contrast, the bottom-up paradigm
firstly regresses all keypoints of all persons in an image,
then performs keypoint grouping methods, e.g., part affin-
ity fields [2], associative embedding [19], and graph clus-
tering [11], to align the regressed keypoints to every per-
son. This paradigm has the advantage of reducing the time
cost of pose estimation as the number of people largely
increases. Recently, following the above two paradigms,
multi-person 3D pose estimation can be achieved with
RGB/D images [34, 43]. Since Wi-Fi signals carry the com-
bined changes caused by all persons in the surroundings,
we cannot crop information belonging to every person from
Wi-Fi signals. Thus, the bottom-up paradigm is more suit-
able for multi-person pose estimation from Wi-Fi signals.
2.2. Human Pose from Wi-Fi
Human pose estimation from WiFi signals is a rela-
tively new and emerging research area that targets privacy-
preserving and occlusion use scenarios. Current Wi-Fi
work have made notable advancements in single-person
2D pose estimation [28, 41], single-person 3D pose es-
970
timation [10, 21,22], and multi-person 2D pose estima-
tion [29]. However, the multi-person 3D pose estimation
is still unsolved. In the hardware aspect, WiSPPN [28] and
MetaFi++ [41] use 1 transmitter and 1 receiver for single-
person 2D pose estimation. Several work achieve single-
person 3D pose estimation with more Wi-Fi transceivers to
capture more reflection information from human body. For
example, WiPose [10] uses 1 transmitter and 3 receivers;
GoPose [22] and Winect [21] both apply 1 transmitter and 4
receivers. Unfortunately, these single-person pose estima-
tion approaches cannot extend to multi-person pose estima-
tion because their algorithms have no person detection strat-
egy or keypoint grouping method. Person-in-WiFi [29] is
the first multi-person 2D pose estimation work, which lever-
ages part affinity fields [2] for keypoint grouping. However,
multi-person 3D pose estimation from Wi-Fi signals is still
an open problem.
3. Wi-Fi Signals
Channel State Information.
Wi-Fi devices communicate via multiple subcarriers in
orthogonal frequency. The propagation process of the Wi-
Fi signals between the transmitter and the receiver can be
formalized as Eq. 1.
Yi=HiXi+ni, i∈[1, n] (1)
where iis the subcarrier index; Xiis the bits sent by the
transmitter via the ithsubcarrier; Yiis the bits received
at the receiver via the ithsubcarrier; nirepresents the
noise. During the propagation, Wi-Fi signals undergo vari-
ous distortions including the influence of propagation dis-
tance, scattering, power fading, etc. Hisymbolizes the
signal distortion summary, technically known as the Chan-
nel State Information (CSI). It is represented as a complex-
value number, from which we can derive both amplitude
and phase. In Wi-Fi systems, CSI phases are often disrupted
by noise and other interferences like time lag, random phase
offsets due to device imperfections, and measurement pro-
cess noise [30, 35]. We employ a phase denoising method
based on linear transformation, as proposed in PhaseFi [30],
to mitigate these interferences in CSI phases. An example
of this denoising result is illustrated in Fig. 2.
Recording CSI over a period captures time-series Wi-Fi
distortions triggered by environmental changes, including
the presence and movements of people nearby. This is the
underlying principle enabling Wi-Fi signals to estimate hu-
man poses.
Hardware Configurations. Our data collection setup
includes four ThinkPad X201 laptops, one as a transmit-
ter and the remaining three as receivers, all equipped with
Intel 5300 network cards. The deployment is arranged as
illustrated in Fig. 3. The transmitter is configured to broad-
cast Wi-Fi in channel 128 (5.64GHz) with 30 subcarriers
Figure 2. CSI Phase denoising, where the left is the original phase,
and the right is the phase after denoising.
Figure 3. One of our experiment areas, a rectangle of 4m×3.5m,
where the camera is in the center of the horizontal axis facing the
experimental area.
and one antenna at a rate of 300 packets per second. The
three receivers are set up to monitor the channel, each uti-
lizing three antennas. Concurrently, an Azure Kinect cam-
era captures RGB-D videos at 15 frames per second. We
manually synchronize Kinect with the three receivers before
the recording begins. Consequently, CSI samples, sized
1×3×3×30×20, are synchronized with one video frame.
The five dimensions of CSI samples respectively represent
the meanings of
(#transmitter, #receiver, #antenna, #subcarrier, #time).
4. Transformer for Person-in-WiFi 3D
•Tokenization. The first issue we address is the effective
tokenization of CSI samples. In language models, tokens
are typically discrete words or characters organized tem-
porally. In Vision Transformers (ViT) [4], tokens are spa-
tially ordered image patches. Our aim is to sequence CSI
tokens, considering CSI samples are inherently time-series
data. Besides, the distinct locations of the transmitter, re-
ceivers, and antennas incorporate spatial information. We,
therefore, flatten these to form a tensor, whose dimensions
are interpreted as
#transmitter× #receiver× #antenna× #time, #subcarrier.
with the first dimension encompassing spatial-temporal el-
ements. For our specific CSI samples, this results in 180 to-
kens (1 ×3×3×20), where each token is a vector ∈1×30,
971
C SI F e at ur e 
E n c o d er ... 
... ... 
... R efi n e D e c o d er 
P o s e D e c o d er Ki n e ct 
S D K 
⨁S T E H u n g ari a n l o s s 
Fi g ure 4. Pers o n-i n- Wi Fi 3 D e m pl o ys a teac her-st u de nt fra me w or k, w here t he Az ure Ki nect ser ves as t he s u per vis or y ele me nt. T he 
pri mar y f u ncti o n of t he Wi- Fi e nc o der a n d dec o der wit hi n t his fra me w or k is t o ge nerate 3 D p oses. S u bse q ue ntl y, t he re ﬁ ne dec o der is 
tas ke d wit h re ﬁ ni n g t hese i nitial o ut p uts. To facilitate trai ni n g t he net w or k i n a n e n d-t o-e n d ma n ner, a set- base d H u n garia n l oss is utilize d. 
re prese nti n g t he s u bcarriers. We t he n c o m bi ne t he a m pli- 
t u des a n d de n oise d p hases al o n g t he s u bcarrier di me nsi o n, 
lea di n g t o eac h t o ke n bei n g ∈1×6 0 . A f ull y-c o n necte d 
la yer is s u bse q ue ntl y use d t o u pscale eac h t o ke n t o a di- 
me nsi o n of ∈1×2 5 6 .
•S p ati al-te m p or al E m be d di n g. C SI sa m ple t o ke ns are 
or ga nize d i n a s patial-te m p oral se q ue nce. To ai d i n dis- 
ti n g uis hi n g t hese t o ke ns, we i ntr o d uce ra n d o ml y i nitialize d 
a n d lear na ble s patial-te m p oral e m be d di n gs ( S T E), eac h of 
di me nsi o n ∈1×2 5 6 . T hese e m be d di n gs are a d de d wit h 
eac h t o ke n. After t hat, we ha ve a n i n p ut of ∈1 8 0 ×2 5 6 ,
w hic h f or ms t he f o u n dati o n f or t he Wi- Fi e nc o der t o effec- 
ti vel y lear n p ose re prese ntati o n fr o m t he C SI sa m ples. 
•C SI E nc o der . O ur arc hitect ure i nc or p orates si x e n- 
c o der la yers t o pr ocess C SI feat ures, eac h c o m prisi n g a 
m ulti- hea d self-atte nti o n m o d ule a n d a fee d-f or war d net- 
w or k, w hic h are f u n da me ntal c o m p o ne nts i n t he Tra ns- 
f or mer arc hitect ure [ 2 7 ]. After t he e nc o der sta ge, we uti- 
lize a M ulti- La yer Perce ptr o n t o pr o d uce a n i nitial set of 
3 D p oses a n d c orres p o n di n g sc ores, de n ote d as Pi nit ∈
1 8 0 ×( 3 K)a n d Si nit ∈1 8 0 ×1, res pecti vel y. I n o ur e x- 
peri me nts, we select Kas 1 4, w hic h re prese nts t he 3 D c o- 
or di nates of 1 4 disti nct ke y p oi nts. 
•P ose Dec o der . F oll o wi n g t he a p pr oac h of D E T R [ 3], 
o ur P ose dec o der, e q ui p pe d wit h 1 0 0 ra n d o ml y i nitialize d 
a n d lear na ble q ueries, re gresses a set of 3 D b o d y c o or di- 
nates P ∈ 1 0 0 ×( 3 K)al o n g wit h c orres p o n di n g c o n ﬁ de nce 
sc ores. T he dec o der la yers are str uct ure d as basic bl oc ks of 
t he D E T R dec o der [ 3], a n d we stac k t hree s uc h la yers. 
A d diti o nall y, dra wi n g i ns pirati o n fr o m Def or ma ble- 
D E T R [ 4 2 ], we are n ot t o pre dict a f ull p ose at t he ﬁ nal 
la yer, b ut rat her t o pre dict a n offset at eac h la yer. T he o ut- 
p ut p oses fr o m eac h la yer are calc ulate d as t he c u m ulati ve s u m of t he c o or di nates fr o m t he pre vi o us la yer a n d t he pre- 
dicte d offsets f or t hat la yer. I n ot her w or ds, gi ve n t he p ose 
Pd−1pre dicte d b y t he (d−1) t h dec o der la yer, t he dt h la yer 
o ut p uts t he p ose as per t he e q uati o n: 
Pd=Pd−1+ ∆ Pd ( 2) 
w here ∆Pdre prese nts t he offsets pre dicte d at t he dt h la yer. 
T he i nitialize d p oses P0∈1 0 0 ×( 3 K)are sa m ple d fr o m 
Pi nit base d o n t heir c o n ﬁ de nce sc ore ra n ki n g. T his met h o d 
of offset pre dicti o n e ns ures t hat eac h la yer of t he dec o der is 
a de q uatel y c o nstrai ne d. 
•Re ﬁ ne Dec o der . T he c o nce pt of re ﬁ ne me nt i n o ur s ys- 
te m is i ns pire d b y P E T R [ 2 4 ]. O ur re ﬁ ne dec o der is s pecif- 
icall y crafte d t o ﬁ ne-t u ne t he ke y p oi nt c o or di nates f or m ore 
precise pre dicti o ns. T he la yers of t he re ﬁ ne dec o der are 
m o dele d after t he basic bl oc ks of t he D E T R dec o der [ 3], 
a n d we e m pl o y t hree s uc h la yers i n t he stac k. Mirr ori n g t he 
p ose dec o der, eac h la yer i n t he re ﬁ ne dec o der pr o d uces a 
relati ve offset ∆J= ( ∆ x, ∆y, ∆z). T he j oi nt c o or di nates 
o ut p ut b y t he dt h la yer a d here t o t he e q uati o n: 
Jd=Jd−1+ ∆ Jd ( 3) 
Differi n g fr o m t he p ose dec o der, t he re ﬁ ne dec o der selects 
o nl y Gb o dies f or re ﬁ ne me nt. I n t he trai ni n g p hase, we 
use set- base d H u n garia n bi partite matc hi n g [ 1 4 ] bet wee n 
t he p ose dec o der o ut p uts P ∈ 1 0 0 ×( 3 K)a n d t he gr o u n d 
tr ut h Pg t ∈G×( 3 K)t o c h o ose Gb o dies f or re ﬁ ne me nt. 
D uri n g i nfere nce, i n t he a bse nce of gr o u n d tr ut h, t his selec- 
ti o n is base d o n c o n ﬁ de nce sc ores. 
•L oss F u ncti o n. I n o ur s yste m, we i m ple me nt a set- 
base d H u n garia n l oss [ 1 4 ] t o e ns ure eac h pers o n’s gr o u n d- 
tr ut h p ose recei ves a u ni q ue pre dicti o n. T he f ocal l oss f u nc- 
ti o n [ 1 6 ] is use d as t he classi ﬁcati o n l oss t o deter mi ne t he 
972
accuracy of predicting a body as a person. Additionally,
we employ Mean Squared Error (MSE) loss for keypoint
regression. The total loss function is expressed as:
L=Lcls+λLkpt (4)
where λis a balancing weight between classification and
regression. The formula for Lkptis:
Lkpt= mean( ||ˆP−Pgt||2). (5)
In this equation, ˆPandPgtdenote the predicted 3D joint
coordinates and the ground truth, respectively.
The formula for Lclsis given by:
Lcls=(
−α(1−ˆy)γlog(ˆy)ify= 1
−αˆyγlog(1−ˆy) otherwise.(6)
Here, αandγare factors adjusting the impact of positive
vs. negative samples and easy vs. hard samples, while y
andˆyrepresent the class labels and confidence scores, re-
spectively.
•Training Details. We optimize Person-in-WiFi 3D us-
ing the Adam optimizer [12], set with a momentum of 0.9
and a weight decay of 10−4. The batch size for training is
set at 32. The training process spans 500 epochs, starting
with an initial learning rate of 2×10−5. This rate is re-
duced by a factor of 0.1at the 450th epoch. In Eq.6, we
set an αvalue of 0.25 and a γvalue of 2. To balance the
classification and keypoint localization losses, the weight λ
is set at 35 in Eq. 4.
5. Experiment
5.1. Dataset Acquisition
Dataset diversity. We recruited 7 volunteers, with heights
of 160-177cm and weights of 55-70kg, to perform 8 daily
actions in 3 locations with different degrees of multipath ef-
fects: an office, a classroom, and a corridor. The actions
were reaching out, raising hands, bending over, stretching,
sitting down, lifting legs, standing, and walking. In each lo-
cation, 1-4 volunteers freely performed these actions for 40
seconds, and we recorded 152 clips of 40 seconds each, in-
cluding 32 single-person, 48 two-person, 48 three-person,
and 24 four-person scenarios. This approach resulted in
a total of 456 CSI and RGB-D clips, ensuring the dataset
captured a wide variety of volunteers, actions, places, and
combinations of these elements across different time.
Dataset statistics. We possess 456 RGB-D clips, each
40 seconds long, with a total of 600 frames per clip (first
550 for training, last 50 for testing) and 270,000 frames
overall. Multi-person 3D keypoint coordinates are gener-
ated from these video clips using the Kinect Body Tracking
SDK, then act as CSI annotations for network training andevaluation. However, the SDK’s performance on our clips
was not entirely satisfactory. We meticulously cleaned the
dataset, frame by frame, discarding samples that the SDK
failed, e.g., the 4-person case. As a result, we have a dataset
with over 97,000 samples, the details of which are outlined
in Table. 1.
1-person 2-person 3-person all
training 28121 36242 25583 89946
test 2586 3184 2054 7824
Table 1. Dataset statistics.
5.2. Evaluation Metrics
(1) Mean Per Joint Dimension Location Error (MPJ-
DLE). Person-in-WiFi 3D estimates multi-person pose in
3D dimension: horizontal, vertical, and depth. We can com-
pute the location error of the horizontal dimension as re-
ported in mmPose-NLP [23] with Eq. 7and Eq. 8.
PJDLE(k ,h) =1
FFX
f=11
PfPfX
p=1|pre(f, p, k, h )−gt(f, p, k, h )|1
(7)
MPJDLE(h) =1
KKX
k=1EPJDLE (k, h) (8)
where pre(f, p, k, h )andgt(f, p, k, h )denote the predicted
and ground truth positions, respectively, for the horizon-
tal dimension of the kthjoint of the pthperson in the fth
frame. The ||1notation is used to calculate the L1 distance.
Therefore, PJDLE(k,h) represents the Per Joint Dimension
Localization Error (PJDLE) for the kthjoint in the horizon-
tal dimension. MPJDLE(h) is the Mean PJDLE calculated
across all joints in the horizontal dimension. Similarly, we
can compute MPJDLE(v) for vertical MPJDLE, and MPJ-
DLE(d) for depth MPJDLE.
(2) Mean Per Joint Position Error (MPJPE). If we
compute L2 distances between 3D predictions and 3D
ground truth as Eq. 9, we have joint position errors.
PJPE(k) =1
FFX
f=11
PfPfX
p=1∥pre(f, p, k )−gt(f, p, k )∥2(9)
MPJPE =1
KKX
k=1EPJPE (k) (10)
PJPE(k) is for the PJPE of the kth joint. MPJPE is the mean
of all PJPE(k), which is widely used to evaluate 3D human
pose estimation [9].
5.3. Results
Table. 2presents PJPE and PJDLEs across horizontal, verti-
cal, and depth dimensions, with units in millimeters (mm).
973
Joint PJPE PJDLE(h) PJDLE(v) PJDLE(d)
neck 81.6 41.0 39.6 47.5
head 88.4 43.3 43.2 52.2
left shoulder 90.5 47.5 41.2 52.6
right shoulder 92.5 50.3 40.9 53.8
left elbow 124.1 65.2 55.3 71.6
left hip 72.9 38.1 30.2 46.3
right elbow 132.9 71.2 58.3 76.1
right hip 74.7 40.5 30.2 46.8
left hand 182.1 91.0 98.2 87.9
left knee 85.1 38.9 33.2 58.3
right hand 202.9 97.8 111.3 97.9
right knee 86.9 40.4 33.0 59.4
left ankle 92.1 41.2 38.4 61.4
right ankle 94.1 43.2 39.2 62.4
Mean 107.2 53.5 49.4 62.4
Table 2. MPJPE and MPJDLEs, measured in millimeters (mm).
Notably, the largest prediction errors occur in the left and
right hands, with 182.1mm and 202.9mm, respectively.
This higher error rate is attributed to the greater diversity
in hand movements compared to overall body movements.
The final MPJPE is calculated at 107.2mm, a level of pre-
cision suitable for various applications like indoor person
localization and tracking, intrusion detection, and coarse-
grained action recognition. Similarly, PJDLEs for the left
and right hands also exhibit the largest errors, for reasons
analogous to those for MPJPE. The mean joint dimen-
sion location errors, MPJDLEs, are 53.5mm, 49.4mm, and
62.4mm in the horizontal, vertical, and depth dimensions,
respectively.
Table. 3displays the estimation errors in scenarios with
one, two, and three persons. We observe that the predic-
tion errors escalate with an increasing number of people.
For instance, considering MPJPE, there is an increase of
16.4mm from 1-person scenarios (91.7mm) to 2-person sce-
narios (108.1mm), and a further increase of 17.2mm from
2-person to 3-person scenarios, closely aligning with the
16.4mm increment. We infer that transitioning from single-
person to multi-person 3D pose estimation is not inherently
problematic for the Wi-Fi system. Rather, the current lim-
itations in achieving multi-person 3D pose estimation are
algorithmic, not due to inherent constraints of the capabili-
ties of Wi-Fi system.
To gain a clearer understanding of prediction error distri-
bution, we analyzed the cumulative distribution of MPJPE,
categorizing it into four segments: upper body, arms, mid-
dle body, and legs. This categorization, shown in Fig. 5,
helps prevent overlapping of the curves. From the figure,
it is evident that 50% of the MPJPEs for pose estimation
in the upper body, middle body, and legs are under 80mm,
while 90% are under 170mm. However, for the arms, partic-Metric 1-person 2-person 3-person
MPJPE 91.7 108.1 125.3
MPJDLE(h) 42.4 51.2 60.6
MPJDLE(v) 43.3 47.7 53.8
MPJDLE(d) 50.0 60.4 69.7
Table 3. As the number of people in the scene increases, the pre-
diction errors increase.
Figure 5. Cumulative distribution of MPJPE in four groups i.e.,
upper body, middle body, legs, and arms.
ularly the left and right hands, the MPJPEs are significantly
higher, with 50% of estimations falling within 140mm and
90% within 380mm. Both Table. 2and Fig. 5underscore
the increased difficulty in accurately estimating arm joints
compared to others. We think this problem could be re-
lieved by enhancing the dataset with more diverse arm po-
sitions in future research.
5.4. Visualization and Failure cases
We visualize multiple instances of multi-person 3D pose es-
timation predictions across three different rooms, as shown
in Fig.1. It is important to note that these images are used
solely for visualization purposes and are not included in the
deep model training. The figure demonstrates that Person-
in-WiFi 3D effectively localizes people, which could be
leveraged for tracking purposes. Additionally, it can esti-
mate a variety of actions, such as sitting, raising hands, and
walking. More crucially, even though the MPJPEs for arm
joints are higher compared to other joints, the predictions
are sufficiently accurate to identify the actions being per-
formed. Therefore, Person-in-WiFi 3D is also capable of
supporting action and interaction recognition applications.
In Fig.6, we present some typical failure cases of Person-
in-WiFi 3D. The leftmost figure illustrates the system is
challenging to accurately estimate the positions of spare
hand gestures. The middle figure depicts a scenario where
Person-in-WiFi 3D incorrectly locates a person about one
974
Figure 6. Failure cases, (1) spare gestures (2) location prediction
error, (3) out of sensing area.
meter away from their actual position. The last figure high-
lights an instance where Person-in-WiFi 3D fails to detect a
person situated out of the system’s coverage area. Address-
ing these shortcomings will likely require collecting more
data, deploying additional devices, etc.
Metric Person-in-WiFi 3D WiPose∗
MPJPE 91.7 101.8
MPJPE(h) 42.4 42.1
MPJPE(v) 43.3 47.8
MPJPE(d) 50.0 58.0
Table 4. Person-in-WiFi 3D is more accurate than WiPose.
(WiPose∗is not officially released yet. The results here originate
from our independent reproduction of WiPose.)
5.5. Comparison with existing work
•Comparison with WiPose. All current Wi-Fi-
based pose estimation research has not been open-sourced
in terms of signal processing, network frameworks, or
datasets. We independently reproduced WiPose [10] and
tested it using our dataset for single-person scenarios. As
shown in Table. 4, Person-in-WiFi 3D demonstrates better
performance in single-person 3D pose estimation compared
to WiPose.
•Comparison with Person-in-WiFi. Person-in-
WiFi [29] focuses on multi-person 2D pose estimation. We
dedicated nearly half a year to adapting it for 3D pose es-
timation, but unfortunately, these efforts were unsuccess-
ful. Person-in-WiFi faces several challenges in the realm
of multi-person 3D pose estimation, challenges which our
Person-in-WiFi 3D effectively overcomes.
(1) inefficient use of storage and memory. Extending
Person-in-WiFi for 3D poses necessitates the use of 3DJoint Heatmaps (JHMs) and 3D Part Affinity Fields (PAFs)
for pose grouping, leading to significant storage and mem-
ory requirements. Specifically. The JHMs ∈14×64×
64×64×64for 14 keypoints, and the PAFs are sized at
42×64×64×64×64for 42 limbs across 3D axes. Stor-
ing JHMs and PAFs for a single frame requires about 7 to
8MB, amounting to roughly 2TB for all frames. In contrast,
our method uses a ground truth tensor sized as the number
of persons p×14×3axes per frame, which totals to ap-
proximately 200MB for all frames, significantly reducing
the storage demand.
(2) slow training. We extend Person-in-WiFi to a two-
stream 3D UNet. Since UNet has to store all middle feature
maps, JHMs, and PAFs, the minibatch size is limited to 4
on an Nvidia 3090 GPU. On our dataset, to train one epoch,
the extended Person-in-WiFi takes ∼6 hours on one Nvidia
3090 GPU. In contrast, our work supports the minibatch
size of 64, only taking 20mins to train one epoch.
(3) sluggish pose grouping. We also extended the pose
grouping from 2D to 3D. It takes about 10s to conduct pose
grouping for 1 frame using 3D JHMs and PAFs. In contrast,
our method can produce 3D human poses at 54fps.
•Comparison to cameras and millimeter-wave
radars. Recent camera-based solutions for multi-person
3D pose estimation have reported MPJPEs in the range of
54-70mm [5, 32,40]. Zhang et al. [37] reported MPJPE
of 29.9mm on self-collected data using millimeter-wave
radars. The MPJPE of Person-in-WiFi 3D stands at
91.7mm in single-person scenarios and 107.2mm in multi-
person scenarios, comparable to cameras and millimeter-
wave radars. We also tried to apply PETR [24] directly
applied to our task, and the MPJPE is 219.0, showcasing
the effectiveness of our enhancements. Notably, Person-in-
WiFi 3D is trained with annotations automatically gener-
ated by the Azure Kinect SDK, whose performance lim-
its the upper bound of Person-in-WiFi 3D. Nevertheless,
Person-in-WiFi 3D is still the first work that achieves multi-
person 3D pose estimation with Wi-Fi signals.
5.6. Ablation Study
•Number of receivers. Our configuration includes a di-
agonally opposed transmitter, two transmitter, and three re-
ceivers, arranged at the corners of a rectangular area to es-
tablish the sensing zone. With three pairs of transmitters
and receivers, Person-in-WiFi 3D is adept at capturing Wi-
Fi signals reflected from the human body at different angles.
While the reduction of receivers will lead to a considerable
drop in system performance, as shown in Table 5.
3 receivers R1+R2 R1+R3 R2+R3 R2
MPJPE(mm) 107.2 148.5 142.8 144.2 173.0
Table 5. Performance on the number of receivers.
975
without Refine decoder with Refine decoder
MPJPE 116.1 107.2
MPJPE(h) 54.7 53.5
MPJPE(v) 46.9 49.4
MPJPE(d) 65.8 62.4
Table 6. Refine decoder improves pose estimation performance.
Metric amplitude amplitude+phase. A. and P.(de.)
MPJPE 192.4 137.7 107.2
MPJPE(h) 105.6 61.6 53.5
MPJPE(v) 103.1 71.1 49.4
MPJPE(d) 71.9 70.0 62.4
Table 7. Incorporating phase information and applying denoising
significantly improves the system outcomes. ‘A. and P.(de.)’ indi-
cates the use of amplitude with denoised phase.
Metric Cross-Person Cross-Environment
MPJPE 131.6 626.4
MPJPE(h) 76.1 414.5
MPJPE(v) 57.4 304.8
MPJPE(d) 62.2 133.0
Table 8. Experimental results of the Cross-Person and Cross-
Environment settings.
•Refine decoder. We conduct an ablation experiment on
the Refine decoder, with results detailed in Table. 6. These
results demonstrate that the Refine decoder improves pose
estimation performance.
•Phase and Phase Denoising. Table. 7presents the ab-
lation study results on phase denoising. We explore three
scenarios: using only amplitude, combining amplitude with
non-denoised phase, and using amplitude with denoised
phase. The experimental findings indicate that incorporat-
ing phase information and applying denoising significantly
improves outcomes of Person-in-WiFi 3D.
•Occlusion. To assess the adaptability of Person-in-
WiFi 3D to occlusion scenarios, we placed a white screen
obstructing the front perspective of the Kinect, while a side-
view camera captured images, as shown in Fig. 7. The fig-
ure demonstrates Person-in-WiFi 3D remains functional
in detecting persons even when their view is obstructed.
Due to the absence of ground truth data from the occluded
Kinect perspective, we are unable to provide quantitative
results for these scenarios.
•Cross-domain ability. Person-in-WiFi 3D under-
goes evaluation in both leave-one-person-out and leave-
one-environment-out scenarios. The results, displayed in
Table.8, show that in cross-person cases, Person-in-WiFi
3D effectively recognizes poses despite individual dif-
ferences in body shape and movement habits. How-
ever, Person-in-WiFi 3D does not implement any cross-
environment adaptation strategies, leading to a large decline
in performance during cross-environment testing.
Figure 7. Person-in-WiFi 3D remains functional in detecting per-
sons when their view is obstructed.
6. Conclusion
We presented Person-in-WiFi 3D, a fully end-to-end ap-
proach that takes the multi-person 3D pose estimation task
as a set-based prediction problem and achieves it elegantly
with a Transformer framework. However, the current sys-
tem still has several limitations.
(1) The upper-bound performance Person-in-WiFi 3D is
severely limited by the Azuere Kinect Body Tracking SDK.
However, the current SDK works badly in cases where there
are more than 3 persons in the view. Future work can intro-
duce VICON system [25], IMUs [36], and optical mark-
ers [17] for more accurate annotations.
(2) Person-in-WiFi 3D has 4 distributed Wi-Fi
transceivers. Accurately adjusting their relative spa-
tial configurations, including positions, elevations, and
antenna orientations, to different places is challenging
to ensure cross-location generalization. Future efforts
could focus on increasing the tolerance of spatial con-
figurations, perhaps by using environment-independent
features like BVP [38] and propagation models such as
Fresnel Zone [31] in Wi-Fi signal preprocessing and deep
networks, or focus on large model pre-training and efficient
cross-environment finetuning.
Acknowledgements – This work was supported by the National Nat-
ural Science Foundation of China under grant 62102307, U21A20462,
62372400, and 62372365, “Pioneer” and “Leading Goose” R&D Program
of Zhejiang under grant No.2024C03287, Key Research and Development
Program of Shaanxi (ProgramNo.2021GXLH-Z-021), China Postdoctoral
Science Foundation under grant 2023T160511 and 2021M692562, and
CAAI-MindSpore Open Fund; and developed on Openl Community.
976
References
[1] Sizhe An and Umit Y Ogras. Fast and scalable human pose
estimation using mmwave point cloud. In Proceedings of
the 59th ACM/IEEE Design Automation Conference, pages
889–894, 2022. 1
[2] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
Realtime multi-person 2d pose estimation using part affinity
fields. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 7291–7299, 2017. 1,
2,3
[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, Au-
gust 23–28, 2020, Proceedings, Part I 16, pages 213–229.
Springer, 2020. 2,4
[4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020. 3
[5] Matteo Fabbri, Fabio Lanzi, Simone Calderara, Stefano Al-
letto, and Rita Cucchiara. Compressed volumetric heatmaps
for multi-person 3d pose estimation. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 7204–7213, 2020. 7
[6] Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu.
RMPE: Regional multi-person pose estimation. In ICCV,
2017. 1,2
[7] Jiaqi Geng, Dong Huang, and Fernando De la Torre. Dense-
pose from wifi. arXiv preprint arXiv:2301.00250, 2022. 1
[8] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-
national conference on computer vision, pages 1440–1448,
2015. 2
[9] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian
Sminchisescu. Human3. 6m: Large scale datasets and pre-
dictive methods for 3d human sensing in natural environ-
ments. IEEE transactions on pattern analysis and machine
intelligence, 36(7):1325–1339, 2013. 5
[10] Wenjun Jiang, Hongfei Xue, Chenglin Miao, Shiyang Wang,
Sen Lin, Chong Tian, Srinivasan Murali, Haochen Hu, Zhi
Sun, and Lu Su. Towards 3d human pose construction using
wifi. In Proceedings of the 26th Annual International Con-
ference on Mobile Computing and Networking, pages 1–14,
2020. 1,3,7
[11] Sheng Jin, Wentao Liu, Enze Xie, Wenhai Wang, Chen Qian,
Wanli Ouyang, and Ping Luo. Differentiable hierarchical
graph grouping for multi-person pose estimation. In Com-
puter Vision–ECCV 2020: 16th European Conference, Glas-
gow, UK, August 23–28, 2020, Proceedings, Part VII 16,
pages 718–734. Springer, 2020. 2
[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980,
2014. 5
[13] Hao Kong, Xiangyu Xu, Jiadi Yu, Qilin Chen, Chenguang
Ma, Yingying Chen, Yi-Chao Chen, and Linghe Kong.m3track: mmwave-based multi-user 3d posture tracking. In
Proceedings of the 20th Annual International Conference on
Mobile Systems, Applications and Services, pages 491–503,
2022. 1
[14] Harold W Kuhn. The hungarian method for the assignment
problem. Naval research logistics quarterly, 2(1-2):83–97,
1955. 4
[15] Guangzheng Li, Ze Zhang, Hanmei Yang, Jin Pan, Dayin
Chen, and Jin Zhang. Capturing human pose using mmwave
radar. In 2020 IEEE International Conference on Pervasive
Computing and Communications Workshops (PerCom Work-
shops), pages 1–6. IEEE, 2020. 1
[16] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Doll ´ar. Focal loss for dense object detection. In Pro-
ceedings of the IEEE international conference on computer
vision, pages 2980–2988, 2017. 4
[17] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Ger-
ard Pons-Moll, and Michael J Black. Amass: Archive
of motion capture as surface shapes. In Proceedings of
the IEEE/CVF international conference on computer vision,
pages 5442–5451, 2019. 8
[18] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-
glass networks for human pose estimation. In Computer
Vision–ECCV 2016: 14th European Conference, Amster-
dam, The Netherlands, October 11-14, 2016, Proceedings,
Part VIII 14, pages 483–499. Springer, 2016. 1,2
[19] Alejandro Newell, Zhiao Huang, and Jia Deng. Associa-
tive embedding: End-to-end learning for joint detection and
grouping. In Advances in Neural Information Processing
Systems. Curran Associates, Inc., 2017. 2
[20] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Unified, real-time object de-
tection. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 779–788, 2016. 2
[21] Yili Ren, Zi Wang, Sheng Tan, Yingying Chen, and Jie Yang.
Winect: 3d human pose tracking for free-form activity using
commodity wifi. Proceedings of the ACM on Interactive,
Mobile, Wearable and Ubiquitous Technologies, 5(4):1–29,
2021. 1,3
[22] Yili Ren, Zi Wang, Yichao Wang, Sheng Tan, Yingying
Chen, and Jie Yang. Gopose: 3d human pose estimation
using wifi. Proceedings of the ACM on Interactive, Mobile,
Wearable and Ubiquitous Technologies, 6(2):1–25, 2022. 1,
3
[23] Arindam Sengupta and Siyang Cao. mmpose-nlp: A natural
language processing approach to precise skeletal pose esti-
mation using mmwave radars. IEEE Transactions on Neural
Networks and Learning Systems, 2022. 1,5
[24] Dahu Shi, Xing Wei, Liangqi Li, Ye Ren, and Wenming Tan.
End-to-end multi-person pose estimation with transformers.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 11069–11078, 2022.
4,7
[25] Leonid Sigal, Alexandru O Balan, and Michael J Black. Hu-
maneva: Synchronized video and motion capture dataset and
baseline algorithm for evaluation of articulated human mo-
tion. International journal of computer vision, 87(1-2):4,
2010. 8
977
[26] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep
high-resolution representation learning for human pose es-
timation. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, pages 5693–5703,
2019. 1,2
[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 2,4
[28] Fei Wang, Stanislav Panev, Ziyi Dai, Jinsong Han, and Dong
Huang. Can wifi estimate person pose? arXiv preprint
arXiv:1904.00277, 2019. 1,2,3
[29] Fei Wang, Sanping Zhou, Stanislav Panev, Jinsong Han, and
Dong Huang. Person-in-wifi: Fine-grained person percep-
tion using wifi. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision, pages 5452–5461,
2019. 1,3,7
[30] Xuyu Wang, Lingjun Gao, and Shiwen Mao. Phasefi: Phase
fingerprinting for indoor localization with a deep learning ap-
proach. In 2015 IEEE Global Communications Conference
(GLOBECOM), pages 1–6. IEEE, 2015. 3
[31] Xuanzhi Wang, Kai Niu, Anlan Yu, Jie Xiong, Zhiyun Yao,
Junzhe Wang, Wenwei Li, and Daqing Zhang. Wimeasure:
Millimeter-level object size measurement with commodity
wifi devices. Proceedings of the ACM on Interactive, Mobile,
Wearable and Ubiquitous Technologies, 7(2):1–26, 2023. 8
[32] Zitian Wang, Xuecheng Nie, Xiaochao Qu, Yunpeng Chen,
and Si Liu. Distribution-aware single-stage models for multi-
person 3d pose estimation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 13096–13105, 2022. 7
[33] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser
Sheikh. Convolutional pose machines. In Proceedings of the
IEEE conference on Computer Vision and Pattern Recogni-
tion, pages 4724–4732, 2016. 1
[34] Qingqiang Wu, Guanghua Xu, Sicong Zhang, Yu Li, and
Fan Wei. Human 3d pose estimation in a lying position
by rgb-d images for medical diagnosis and rehabilitation.
In2020 42nd Annual International Conference of the IEEE
Engineering in Medicine & Biology Society (EMBC), pages
5802–5805. IEEE, 2020. 2
[35] Yaxiong Xie, Zhenjiang Li, and Mo Li. Precise power delay
profiling with commodity wifi. In Proceedings of the 21st
Annual International Conference on Mobile Computing and
Networking, pages 53–64, 2015. 3
[36] Xinyu Yi, Yuxiao Zhou, Marc Habermann, Soshi Shimada,
Vladislav Golyanik, Christian Theobalt, and Feng Xu. Phys-
ical inertial poser (pip): Physics-aware real-time human mo-
tion tracking from sparse inertial sensors. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition, pages 13167–13178, 2022. 8
[37] Jianxiong Zhang, Zhongping Cao, Wen Ding, Rihui Cheng,
Xuemei Guo, and Guoli Wang. Multi-spectrum fusion to-
wards 3d human pose estimation using mmwave radar. In
Proceedings of 2022 Chinese Intelligent Systems Confer-
ence: Volume I, pages 220–232. Springer, 2022. 7
[38] Yi Zhang, Yue Zheng, Kun Qian, Guidong Zhang, Yunhao
Liu, Chenshu Wu, and Zheng Yang. Widar3. 0: Zero-effortcross-domain gesture recognition with wi-fi. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 44(11):
8671–8688, 2021. 8
[39] Mingmin Zhao, Tianhong Li, Mohammad Abu Alsheikh,
Yonglong Tian, Hang Zhao, Antonio Torralba, and Dina
Katabi. Through-wall human pose estimation using radio
signals. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 7356–7365, 2018. 1
[40] Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang,
Hujun Bao, and Xiaowei Zhou. Smap: Single-shot multi-
person absolute 3d pose estimation. In Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, Au-
gust 23–28, 2020, Proceedings, Part XV 16, pages 550–566.
Springer, 2020. 7
[41] Yunjiao Zhou, He Huang, Shenghai Yuan, Han Zou, Lihua
Xie, and Jianfei Yang. Metafi++: Wifi-enabled transformer-
based human pose estimation for metaverse avatar simula-
tion. IEEE Internet of Things Journal, 2023. 1,2,3
[42] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang
Wang, and Jifeng Dai. Deformable detr: Deformable trans-
formers for end-to-end object detection. arXiv preprint
arXiv:2010.04159, 2020. 4
[43] Christian Zimmermann, Tim Welschehold, Christian Dorn-
hege, Wolfram Burgard, and Thomas Brox. 3d human pose
estimation in rgbd images for robotic task learning. In 2018
IEEE International Conference on Robotics and Automation
(ICRA), pages 1986–1992. IEEE, 2018. 2
978
