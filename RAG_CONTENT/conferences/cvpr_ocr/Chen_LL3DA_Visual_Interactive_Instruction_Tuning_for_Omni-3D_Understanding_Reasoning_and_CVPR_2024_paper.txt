LL3DA: Visual Interactive Instruction Tuning for
Omni-3D Understanding, Reasoning, and Planning
Sijin Chen1Xin Chen2,∗Chi Zhang2Mingsheng Li1Gang YU2
Hao Fei3Hongyuan Zhu4Jiayuan Fan1Tao Chen1,†
1Fudan University2Tencent PCG3National University of Singapore
4Institute for Infocomm Research (I2R) & Centre for Frontier AI Research (CFAR), A*STAR, Singapore
https://github.com/Open3DA/LL3DA
∗project lead†corresponding author
CouchArmchair
What is this object? [Click]
W
Describe this scene.
3D Dialogue
Can you please move the picture 
on the wall?
I have arrived at the picture on 
the wall near the couch. What would you like me to do with it?
Can you move it to the other wall, near the table?
Sure! I have moved the picture to the other wall near the  table. Is there 
anything else you need me to do?
Scene Description3D Dense Caption
CouchPlant
Plant
ottomanPicture
I
t
y
C
w
 This is a light brown couch, it is 
situated between two gray ottomans. 
It is under a big painting. The 
painting is gray, y ellow and brown.
3D QA
What is behind the ottoman, that 
is next to the love seat?
Table is located behind ottoman 
that is next to love seat
What is behind the ottoman, that is next to the love seat? [Click]
Trash Can
The couch is located 
at <obj>229, 142, 30, 
38, 63, 53</obj>.
The room contains various couches 
and armchairs , providing comfortable 
seating options. Additionally, there 
are plantsplaced in different areas 
of the room, adding a touch of greenery. Lastly, a picture is hung on one of the walls.
Couc
anythi
t
toman
ott
ch
Armchair
y
Couch
e
The
d
and
at
sea
e 
are
oftArmchair
Figure 1. We propose LL3DA, a Large Language 3D Assistant that demonstrates mighty instruction-following capacities in under-
standing, reasoning, and planning within complex 3D environments. LL3DA takes both the textual instructions and potential visual
interactions into consideration to help remove ambiguities when addressing various tasks in diverse and complex 3D scenes.
Abstract
Recent progress in Large Multimodal Models (LMM)
has opened up great possibilities for various applications inthe ﬁeld of human-machine interactions. However , develop-ing LMMs that can comprehend, reason, and plan in com-
plex and diverse 3D environments remains a challengingtopic, especially considering the demand for understandingpermutation-invariant point cloud representations of the 3D
scene. Existing works seek help from multi-view images byprojecting 2D features to 3D space, which inevitably leadsto huge computational overhead and performance degrada-
tion. In this paper , we present LL3DA, a Large Language
3D A ssistant that takes point cloud as the direct input and
responds to both text instructions and visual interactions.The additional visual interaction enables LMMs to bettercomprehend human interactions with the 3D environment
and further remove the ambiguities within plain texts. Ex-periments show that LL3DA achieves remarkable resultsand surpasses various 3D vision-language models on both3D Dense Captioning and 3D Question Answering.
1. Introduction
The recent surge in Large Language Model (LLM) fam-
ilies [ 14,31,46,52,63] opens up great opportunities for
addressing various machine learning tasks in a generalizedway [ 30,32,40,60]. During this LLM carnival, researchers
are also seeking generalized LLM solutions for various vi-
sion language tasks [ 37,50,62]. Among these, LLM-based
3D scene understanding is a valuable topic that would ben-eﬁt the development of autonomous driving [ 9,23] and em-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
26428
bodied AI agents [ 21,49]. However, it is also challenging
given 1) the diversity and complication of 3D environmentsand 2) the demands for understanding sparse 3D points.
Prior works have made initial success in various 3D vi-
sion and language tasks. The majority of these researchbuild 3D specialists to solve one speciﬁc down-stream task,including 3D Q uestion Answering (3D-QA) [ 2,43],3D
Visual Grounding (3D-VG) [ 7,26,55], and 3D D ense
Captioning (3D-DC) [ 10–12]. There are also several works
[4,13,34,70] study the mutual promotion of different 3D
vision and language tasks with shared structure modellingrelations among objects. Recently, researchers have alsointroduced LLMs for general purpose 3D understanding,where Point-Bind and Point-LLMs [ 24,57] mainly focus
on the understanding of 3D objects. Concurrently, 3D-LLM [ 29] proposes an LLM-driven solution that aggregates
multi-view features for 3D perception, presenting mightycapacities in understanding 3D object and scenes and fol-lowing text instructions produced by human.
Though these methods have achieved remarkable suc-
cess in addressing various challenges in understanding the3D world with natural language, there are certain limita-tions. With limited supervision, 3D specialists could hardlyscale-up for better performance, while the joint pre-trainingstill requires separate heads for speciﬁc tasks. Extractingmulti-view features results in huge computational overheadand ignores the essential geometry and depth information.Additionally, plain texts often lead to ambiguities especiallyin cluttered and complex 3D environments.
To address the above issues, we propose LL3DA, a Large
Language 3D A ssistant that responds to both textual and vi-
sual interactions from human, with the ability to understand,reason, and plan in complex 3D environments (Fig. 1). We
adopt a multi-modal transformer to aggregate information
from textual instructions, visual prompts, and the 3D scene
into a ﬁxed length of learnable querying tokens via the at-tention mechanism. The querying tokens are then projectedand used as the preﬁx for the textual instructions, servingas the input to a pre-trained and frozen LLM. This de-sign not only helps to address the contradiction between
the permutation-invariant 3D scene representations and the
LLM embedding space, but also extracts interaction-aware3D scene representations for efﬁcient instruction following.
We conduct extensive experiments to explore the capac-
ities of LL3DA in understanding, reasoning, and planningwithin complex and diverse 3D environments. Our model
achieves state-of-the-art results on two widely used datasets
for 3D Dense Captioning [ 1,7], and 3D Question Answer-
ing [ 2]. Additionally, by introducing additional visual inter-
actions, our method could further remove the ambiguitieswithin the vague textual instructions.
To summarize, our key contributions lie in:
• We present a LLM-based solution for understanding, rea-soning, and planning in complex 3D environments.
• Our model takes both the textual instructions and visual
interactions as inputs, and extracts interaction-aware fea-tures for effective instruction-following.
• Extensive experiments show that our method surpasses
various state-of-the-art 3D vision language models.
2. Related Work
3D Vision and Language alignment, pre-training, and un-
derstanding [ 5,7,20,70] cover tasks requiring a model to
adopt its understanding towards a complex 3D scene an-swering to, or answering with natural language. Amongthose, 3D D ense Captioning (3D-DC) [ 10,12,54]e x -
pects a model to translate an input 3D scene into a setof instance coordinates and natural language descriptions.Existing methods could be categorized into “detect-then-describe” models [ 4,12,54] and the “set-to-set” prediction
approaches [ 10,11]. The former builds explicit relations on
the instance coordinate estimations, while the latter directlylearns the locations and descriptions for instances from theinput 3D scene. 3D V isual Grounding (3D-VG) [ 1,7,55]
demands a model to respond the natural language querieswith the instance coordinates in the 3D scene. The main-stream of existing methods [ 4,65,70] address 3D-VG via
selecting a candidate from a 3D detector’s prediction. 3D
Question Answering (3D-QA) [ 2,43,59,66] requires a
model to answer the questions with natural language basedon the input 3D scene. The majority of existing meth-ods [ 2,18,48] directly select the desired response from a
given answer set. Researchers have also studied the mutualpromotion of various 3D vision language tasks via train-
ing their shareable architectures simultaneously on multiple
tasks [ 4,13,34,70]. UniT3D [ 13] and 3DJCG [ 4] focus
on the joint promotion between 3D-DC and 3D-VG in therelation modelling, while 3D-VLP [ 34] further includes 3D-
QA. Recently, 3D-LLM [ 29] introduces a family of LLM-
driven 3D generalists that could handle diverse textual in-
structions with reconstructed 3D features from multi-view
images [ 28]. In this paper, we present LL3DA, an LLM so-
lution that directly extracts features from the 3D scene, andhandles both visual prompts and textual instructions to di-versify the possible interactions human could make with thecomplex 3D environments.
Large Multimodal Models (LMM).
Along with the rapid
development of Large Language Models (LLM) [ 15,63],
researchers have made great recent efforts adapting LLMsto visual understanding and reasoning tasks [ 25,38,56,61].
Some methods project or compress global image features as
the preﬁx for text instructions [ 36,40,58,69], while others
extract ROI features as LLM tokens for region-oriented in-struction reasoning [ 6,64]. Meanwhile, InstructBLIP [ 17]
proposes to extract textual instruction-aware visual features,
and has achieved remarkable success in addressing complex
26429
Scene Encoder
ℰଷ஽
Self-Attention
Cross-Attention
Feed Forward
 Feed ForwardTextual Instructions
݌௜௡,݂௜௡
Prompt 
Encoder
Word
Embeddings
3D Scene
Embeddings
……Learnable QueryClick
Identifier
×ܰ
Output Queries ܳ
CLICK
3D Position
Embedding
ROI
Feature
Extractor
Click
Projector
Box
Projector
Trainable Parameters
Frozen Parameters
(a) LL3DA Pipeline (b) Design of the Interactor3D (c) Prompt Encoder
Large Language ModelInteractor3D
Linear
…
…
CLICK
It is a couch located at <obj> 229, 142, 30, 
38, 63, 53</obj>. This is a light 
brown couch, it is situated 
between two gray ottomans. 
BOX
CLICKVisual Prompts
: What is this object?
Tokenizer
Multi-Modal Transformer݂௘௡௖
Textual Instructions
What is this object?Box
Identifier
Figure 2. Overview of the Proposed Approach. (a) The overall pipeline of our proposed LL3DA ﬁrst extracts interaction-aware 3D scene
embeddings, which are later projected to the preﬁx of textual instructions as the input of a frozen LLM. (b) The detailed design of theInteractor3D, which aggregates visual prompts, textual instructions, and 3D scene embeddings into a ﬁxed length querying tokens. (c) Theprompt encoder encodes the user clicks and box coordinates with the positional embeddings and ROI features, respectively.
and unseen instructions. Concurrently, researchers have
also made great attempts solving various 3D tasks usingLLMs. Notably, [ 24,42,57,68] demonstrate remarkable
success in understanding and reasoning about 3D objects.In this paper, we present an LLM-driven solution that couldhandle both interactions in forms of visual prompts and text
instructions. We also propose to extract interaction-aware
3D scene representations for better instruction following.
3. Methodology
To build a general purpose agent that could handle bothvisual and textual interactions with complex 3D environ-ments, we propose LL3DA, an LLM driven auto-regressiveapproach to 3D vision language tasks. In this section, weﬁrst introduce the problem formatting in Sec. 3.1. Next, we
introduce our model designs in details (Sec. 3.2).
3.1. Problem Formatting
Model I/O. As shown in Fig. 2(a), the input of our model
consists of a 3D scene represented by a set of points PC , the
textual instruction It, and potential visual interactions Iv
that serve as supplementary spatial identiﬁers. Here, point
cloudPC =[pin,fin]∈RN×(3+F), wherepin∈RN×3
andfin∈RN×Fare the point coordinates and the addi-
tional point features, including color ,normal , and height .
The output of our model is free-form natural language, part
of whom could be interpreted into 3D coordinates.
Instruction Formatting. Following existing LMMs [ 57],
we begin the textual instructions Itwith the “ ### human: ”
identiﬁer, and ask the model to generate responses after the“### assistant: ” identiﬁer. This endows the model with theability to distinguish information from the context and fur-
ther engage in multi-turn conversations.
Coordinate Representations. To provide LLMs with the
capacity to perceive and respond with 3D coordinates, we
convert the 3D points and 3D bounding boxes to plaintexts. Speciﬁcally, a point is represented by “ <loc>x,y,
z</loc>”, and a bounding box is represented by its center
point and size, i.e. “<obj>c
x,cy,cz,w,h,l</obj>”. Here,
all the numerical data is discretized into unsigned integerswithin a range of [0,255] with respect to the boundary of
the input 3D scene. This design could naturally ﬁt in thevocabulary of existing pre-trained LLMs [ 52,63]. With-
out the introduction of any additional learnable tokens, wecould save the effort of tuning the LLMs.
3.2. Model Design
As shown in Fig. 2(a), our model ﬁrst aggregates a ﬁxed-
length scene embeddings through the Interactor3D, whichtakes the visual prompts, the textual instructions, and the3D scene as the input. Next, the aggregated 3D scene em-beddings are projected to be the preﬁx of text instructions,and serve as the inputs of a frozen LLM. The detailed de-sign of Interactor3D is shown in Fig. 2(b), which consists
of a frozen 3D scene encoder E
3D, a visual prompt encoder,
and a multi-modal transformer.
Scene Encoder. We adopt the masked transformer encoder
pre-trained on ScanNet detection [ 10] as the scene encoder,
E3D. The scene encoder takes PC as its input, and outputs
the 3D scene embeddings:
fenc=E3D(PC)=E3D(pin;fin)∈RM×d. (1)
Here,fenc consists of d-dimensioned features for Mpoints
26430
uniformly down-sampled from the input 3D scene through
theFarthest Point Sampling (FPS) algorithm. In practice,
we choose to keep the scene encoder frozen to save thememory cost during training.
Visual Prompt Encoder. We mainly take two common
types of visual interactions into consideration, user clicksand 3D box annotations [ 35]. Each user click is ﬁrst normal-
ized within a range of [0,1]by the size of the input 3D scene
p
click∈R3. Then, we encode pclick with the 3D Fourier po-
sitional embeddings [ 51] function:
pos(pclick) = [sin (2 πp click·B);c o s( 2πp click·B)].(2)
Here,B∈R3×(d/2)is a learnable matrix. The box annota-
tion is represented by the ROI feature fbox∈Rdextracted
by a pre-trained 3D object detector [ 10]. The two types
of the visual prompts are then projected with separate and
identical FeedForward Networks (FFN).
fclick=FFN click(pos(pclick))
fbox=FFN box(fbox)(3)
In practice, we represent each visual prompt with 8 tokens.
Multi-Modal Transformer (MMT) serves as a role to 1)
address the contradiction between the permutation-invariant3D scene embeddings and position-sensitive causal LLMs,2) bridge the gap between frozen unimodal experts, and 3)ﬁll the needs for interaction-aware feature extraction. In-spired by the Q-Former architecture [ 17,36], MMT aggre-
gates the visual information within a ﬁxed number of 32
learnable querying tokens. In each layer, the queries interactwith the encoded visual prompts [f
click;fbox]and the textual
instructions Itthrough a shared self-attention. Then, we al-
low the learnable querying tokens and the visual prompts to
interact with the task-agnostic 3D scene embeddings fenc
via cross-attention. The output of MMT is 32 queries writ-
ten asQ∈R32×768, which are ﬁnally projected to the word
embedding space of LLMs through a simple linear projec-tor. In practice, we notice that initializing Q-Former withpre-trained BERT [ 19,36] weights will lead to repetitive
outputs, thus we only choose to initialize the pre-trainedword and position embeddings from BERT.
LLM. We consider the decoder-only generative pre-trained
transformers [ 52,63] as our large language model back-
bone. The decoder-only LLMs are sensitive to the inputorders because of the position embeddings and the causal
attention mask. The parameters and the embedding layersof the LLMs are kept frozen to save memory cost. Duringinference, we generate the responses via searching for theoptimal sequence s
∗that satisﬁes:
s∗=a r gm a x
sP(s|PC,It,Iv). (4)
In practice, we use beam search with a beam size of 4.4. Multi-modal Instruction Tuning
A general purpose 3D agent is able to address various tasks
simultaneously in complex 3D scenes. Apart from introduc-ing proper training data, it is important to guide the modelto generate the desired outputs with instructions. Therefore,Sec. 4.1 will ﬁrst introduce how we identify each task. After
that, Sec. 4.2 will present details for the training objective.
4.1. Tasks and Instructions.
As introduced in Sec. 3.1, LL3DA generates text responses
auto-regressively after the “ ### assistant: ” identiﬁer.
3D Dense Captioning requires the localization and descrip-
tion of instances in diverse 3D environments. We adopt ei-
ther user clicks and box annotations as the visual prompt toidentify the object to be described. Additionally, we designtwo types of textual instructions that ask the model to ei-ther “describe” or “describe and localize” the object, whichdiversiﬁes the tasks, and leads to better performance.
3D Question Answering requires the model to generate re-
sponse to the questions based on the global knowledge ofa 3D scene. To help the model better understand the 3Denvironment, we also design two types of textual instruc-tions that ask the model to either “answer” or “answer andlocalize the related objects”. The latter serves as an auxil-iary task widely adopted in various 3D-QA methods [ 2,48].
To diversify the tasks during training, we randomly includeadditional clicks on the objects related to the questions.
Scene Description requires the model to translate its global
knowledge of the 3D scene into natural language descrip-tions, thus we simply ask the “describe” this 3D scene.
Embodied Conversation and Planning could be treated as
multi-turn conversations, where we use “ ### human: ” and
“### assistant: ” as identiﬁers to distinguish the source of
information as introduced in Sec. 3.1.
4.2. Instruction Following Tuning
During training, for tasks requiring additional visual inter-
actions, i.e. 3D-DC and 3D-QA, we randomly choose be-
tween clicks or boxes as means of object identiﬁcation.
Training Objective. Our training objective is to optimize
the trainable parameters θ, so as to maximize the likelihood
of the target response sequence sgiven the input point cloud
PC , and the human interactions IvandIt:
θ∗=a r gm a x
θP(s|PC;Iv;It;θ). (5)
In practice, this is accomplished by adopting the token-wise
cross-entropy loss that trains the model to predict the ith
tokens[i]given the previous (i−1)tokens,s[1,···,i−1].
L(θ)=−|s|/summationdisplay
i=1logP/parenleftbig
s[i]|PC;Iv;It;θ;s[1,···,i−1]/parenrightbig
.(6)
26431
Table 1. Quantitative Comparisons for 3D Dense Captioning on ScanRefer[ 7] and Nr3D[ 1].For fair comparison, we list methods that
are trained under the standard per-word cross-entropy loss without additional 3D scenes. We use the box estimations from V ote2Cap-DETRto simulate the box annotations as the visual prompts. Our proposed LL3DA surpasses previous 3D specialists on both datasets.
MethodScanRefer Nr3D
C@0.25↑ B-4@0.25 ↑ M@0.25↑ R@0.25↑ C@0.5↑ B-4@0.5↑ M@0.5↑ R@0.5↑ C@0.5↑ B-4@0.5↑ M@0.5↑ R@0.5↑
Scan2Cap[ 12] 56.82 34.18 26.29 55.27 39.08 23.32 21.97 44.78 27.47 17.24 21.80 49.06
MORE[ 33] 62.91 36.25 26.75 56.33 40.94 22.93 21.66 44.42 -- - -
SpaCap3D[ 54] - - - - 44.02 25.26 22.33 45.36 33.71 19.92 22.61 50.50
REMAN[ 44] 62.01 36.37 26.76 56.25 45.00 26.31 22.67 46.96 34.81 20.37 23.01 50.99
D3Net[ 8] - - - - 46.07 30.29 24.35 51.67 33.85 20.70 23.13 53.38
Contextual[ 67] - - - - 46.11 25.47 22.64 45.96 35.26 20.42 22.77 50.78
UniT3D[ 13] - - - - 46.69 27.22 21.91 45.98 -- - -
3DJCG[ 4] 64.70 40.17 27.66 59.23 49.48 31.03 24.22 50.80 38.06 22.82 23.77 52.99
3D-VLP[ 34] 70.73 41.03 28.14 59.72 54.94 32.31 24.83 51.51 -- - -
3D-VisTA∗[70] - - - - 61.60 34.10 26.80 55.00 -- - -
V ote2Cap-DETR[ 10] 71.45 39.34 28.25 59.33 61.81 34.46 26.22 54.40 43.84 26.68 25.41 54.43
LL3DA (Ours) 74.17 41.41 27.76 59.53 65.19 36.79 25.97 55.06 51.18 28.75 25.91 56.61
Table 2. Quantitative Comparisons for 3D Question Answering on ScanQA[ 2].We categorize previous works into classiﬁcation based
(“CLS”) and generation based (“GEN”) methods. The results from 3D-LLM∗come from their ﬁne-tuned version. LL3DA out-performs
previous methods on the validation set and two test sets.
MethodAnswer
TypeV alidation Test w/ object Test w/o object
C↑ B-4↑ M↑ R↑ C↑ B-4↑ M↑ R↑ C↑ B-4↑ M↑ R↑
ScanQA[ 2]
CLS64.86 10.08 13.14 33.33 67.29 12.04 13.55 34.34 60.24 10.75 12.59 31.09
Clip-Guided[ 48] ---- 69.53 14.64 13.94 35.15 62.83 11.73 13.28 32.41
Multi-CLIP[ 18] ---- 68.70 12.65 13.97 35.46 63.20 12.87 13.36 32.61
3D-VLP[ 34] 66.97 11.15 13.53 34.51 70.18 11.23 14.16 35.97 63.40 15.84 13.13 31.79
3D-VisTA[ 70] - - - - 68.60 10.50 13.80 35.50 55.70 8.70 11.69 29.60
3D-LLM∗[29]GEN69.40 12.00 14.50 35.70 69.60 11.60 14.90 35.30 ----
LL3DA (Ours) 76.79 13.53 15.88 37.31 78.16 13.97 16.38 38.15 70.29 12.19 14.85 35.17
Here,|s|is the number of tokens in the desired response.
5. Experiments
To test the capacities of LL3DA, we provide numerous eval-
uations. To begin with, we introduce the datasets, metrics,and implementation details (Sec. 5.1). Then, we compare
how our model understands and reasons in complex 3D en-vironments with previous 3D specialists on 3D Dense Cap-
tioning and 3D Question Answering (Sec. 5.2). Addition-
ally, we conduct quantitative ablation studies on the modeldesign and training strategy (Sec. 5.3). Finally, Sec. 5.4
showcases several qualitative results.
5.1. Datasets, Metrics and Implementation Details
Datasets. In this paper, we experiment with 3D data from
ScanNet [ 16], a 3D dataset covering 1,201 and 312 diverse
and complex indoor 3D scenes for training and validation.The language annotations used in this study are sourcedfrom ScanRefer [ 7], Nr3D [ 1], ScanQA [ 2], and the Scan-
Net subset of 3D-LLM [ 29]. This combination covers a
variety of tasks, including instance and scene descriptions,conversations, embodied planning and question answering.Please refer to the supplementary materials for more detailson the statistics of data.
Metrics. Here, we adopt C,B-4 ,M,Ras abbreviations forCiDEr [ 53], BLEU-4 [ 47], METEOR [ 3], and Rouge-L [ 39]
to evaluate the quality of the generated textual responses.
Implementation Details. Following previous works on 3D
vision language tasks [ 10,12], we randomly sample 40k
points from each 3D scene as the 3D input. We adopt thepre-trained OPT-1.3B [ 63] as our causal LLM backbone,
which is frozen and loaded in ﬂoat16 to save memory cost.We adopt the AdamW [ 41] optimizer with a weight decay of
0.1and a learning rate decaying from 10
−4to10−6with a
cosine annealing scheduler for about 100 k iterations. For all
the training tasks, we train with no more than eight NvidiaRTX3090 (24G) GPUs within a day.
5.2. Comparison with SoTA Specialists
We evaluate the model’s capacity to understand and rea-son in 3D environments via 3D-DC and 3D-QA. For each
evaluation task, we ﬁne-tune the trainable parameters in ourmodel on each task for ∼30k iterations.
3D Dense Captioning demands a model to localize and de-
scribe any instance in a 3D scene. We benchmarks state-of-the-art methods on the widely-used ScanRefer [ 7] and
Nr3D [ 1] dataset in Tab. 1under the m@kIoU metric [ 12].
Here,m∈{ C, B-4, M, R }, and the mscore of a caption
is set to 0if the IoU between the predicted box and the
object is less than the given threshold k. Following exist-
ing works [ 10,12], we consider C@0.25 and C@0.5 as the
26432
main metric for ScanRefer, and C@0.5 for Nr3D. Among
the listed methods, UniT3D [ 13], 3DJCG [ 4], and 3D-VLP
[34] are pre-trained on multiple 3D vision and language
tasks annotated on ScanNet scenes. Additionally, UniT3D[13] adopts off-the-shelf image caption models [ 45] and
multi-view images to generate additional instance-captionsfor pre-training. It is worth mentioning that we compare theresults with the 3D-VisTA [ 70] model that is not trained on
additional 3D scenes. To evaluate our model, we adopt thebox predictions produced by V ote2Cap-DETR [ 10] as the
visual prompt. Results show that our method consistently
outperforms existing methods on both datasets. For exam-
ple, our method achieves 65.19% C@0.5 on ScanRefer and51.18% C@0.5 on Nr3D, which is ( +3.38 % and +7.34 %)
higher than the current state-of-the-art 3D vision and lan-guage model, V ote2Cap-DETR.
3D Question Answering requires a model to generate re-
sponses to the natural language queries questioning towardsan 3D scene. We benchmark state-of-the-art methods onthe ScanQA [ 2] validation set as well as two test bench-
marks in Tab. 2, and consider CiDEr as the main metric.
The majority of the listed methods are based on classiﬁca-tion (marked “CLS”), i.e., selecting responses from a pre-
deﬁned answer set. Meanwhile, 3D-LLM [ 29] tries to ad-
dress 3D-QA via auto-regressive text generation (marked“GEN”), and we list their ﬁne-tuned version for compari-son. Results show that our method consistently outperformsexisting methods on all the evaluation sets, and surpassesthe generation based method, 3D-LLM, by a large margin(+7.39 % CiDEr score on the validation set).
5.3. Ablation Studies
In this section, we provide ablation studies on model de-
signs and training strategies. We evaluate on ScanRefer and
ScanQA to quantize the effectiveness.
Large Language Model
Multi-Modal
Transformer
Textual
InstructionsVisual
PromptsTextual
InstructionsLarge Language Model
Textual
InstructionsVisual
PromptsTextual
InstructionsFFN
(a) Early Fusion (ours) (b) Direct InjectionMulti-Modal
Transformer
Figure 3. Two Different Ways to Encode Visual Prompts. Our
proposed method (a) adopts a uniﬁed transformer to aggregate fea-
tures from all kinds of interactions, while (b) directly concatenates
the visual prompts to the scene embeddings. Tab. 3shows that
early fusion leads to a better performance.
Effectiveness of the Q-Former Design. We list two ways
to process the visual prompts in Fig. 3. Here, Fig. 3(a)
is our proposed method that adopts a uniﬁed transformer toaggregate information from both text instructions and visualprompts, while Fig. 3(b) is the “direct injection” version,
which only extract instruction-aware 3D feature with visualprompts concatenated after the scene embeddings. We trainboth models from scratch and evaluate their performance onScanRefer 3D Dense Captioning. The results (Fig. 3) show
that the method we use (Fig. 3(a)) could better capture fea-
ture related to the visual prompts, leading to better instancecaption generation performance ( +3.45% C@0.5).
Table 3. Effectiveness of Q-Former Design on ScanRefer[ 7].
We design two different ways of utilizing visual prompts. The“early fusion” enables direct interaction with the 3D scene, thus itachieves a better performance.
Visual Prompt C@0.5 ↑ B-4@0.5↑ M@0.5↑ R@0.5↑
direct 59.39 33.27 25.19 53.39
ours 62.84 35.81 25.81 54.45
Instructions as Auxiliary Tasks for 3D Dense Caption-
ing. We have introduced two types of task instructions
in Sec. 4.1 for 3D-DC, i.e. the “describe”-only instruc-
tions and “detect and localize” instructions. Additionally,we have introduced two types of visual prompts (Fig. 2&
Sec. 4.2). In this study, we show how they affect the per-
formance when serving as auxiliary tasks for 3D-DC byevaluating on ScanRefer in Tab. 4. All the methods listed
are trained from scratch. In Tab. 4, “Aux.Loc” identiﬁes
whether we train the model with the “detect and localize”instructions, and “Clicks” identiﬁes whether we train themodel with clicks as additional visual prompts. Resultsshow that they are both good auxiliary tasks for 3D-DC.
Table 4. Effectiveness of Instructions as 3D Dense Captioning
Auxiliary Tasks. We train the models from scratch and evaluate
on ScanRefer[ 7]. “Aux.Loc” identiﬁes whether we train with the
“describe and localize” instructions. “Clicks” identiﬁes whetherwe train with clicks as additional visual prompts.
Aux.Loc Clicks C@0.5 ↑ B-4@0.5↑ M@0.5↑ R@0.5↑
- - 60.85 34.09 25.53 53.48
/check - 61.81 34.15 25.49 53.83
- /check 62.20 34.26 25.67 53.87
/check/check 62.84 35.81 25.81 54.45
Instructions as Auxiliary Tasks for 3D Question Answer-
ing. We have made a similar study to analyze how adopt-
ing additional “answer and localize” instructions and visual
prompts improves 3D-QA on ScanQA [ 2] as auxiliary tasks
in Tab. 6. We do not use any visual interactions during infer-
ence. Results show that the additional textual instructionsand visual prompts improve the task diversity and furtherimprove the performance for 3D-QA.
Performance as a Generalist. To test whether LL3DA can
distinguish different tasks given the textual instructions andvisual prompts, we evaluate our model on different tasks
26433
Table 5. Evaluation as a Generalist. The ﬁrst three rows list the performance of models trained from scratch as experts on each dataset.
The results in the following three rows belong to the model ﬁne-tuned from the generalist weights. The last row evaluates the model trainedas a generalist. ScanRefer[ 7] and Nr3D[ 1] are used to evaluate the dense captioning performance, and ScanQA[ 2] is used to evaluate
the question answering performance. Serving as a generalist, our method can differentiate each task, and produce strong results based ontextual instructions and visual prompts.
MethodScanRefer Nr3D ScanQA
C@0.5↑ B-4@0.5↑ M@0.5↑ R@0.5↑ C@0.5↑ B-4@0.5↑ M@0.5↑ R@0.5↑ C↑ B-4↑ M↑ R↑
ScanRefer(scratch) 62.84 35.81 25.81 54.45 - - - - ----
Nr3D(scratch) - - - - 44.95 27.67 25.67 55.79 ----
ScanQA(scratch) - - - - - - - - 74.80 13.68 15.40 36.25
ScanRefer(ﬁne-tuned) 65.19 36.79 25.97 55.06 - - - - ----
Nr3D(ﬁne-tuned) - - - - 51.18 28.75 25.91 56.61 ----
ScanQA(ﬁne-tuned) - - - - - - - - 76.79 13.53 15.88 37.31
w/o ﬁne-tuning 62.98 35.97 25.66 54.65 23.94 13.37 22.31 45.78 75.67 13.33 15.37 37.02
Table 6. Effectiveness of Interactions as 3D Question Answer-
ing Auxiliary Tasks. We train the model from scratch and eval-
uate all the models from scratch on ScanQA[ 2] validation set.
“Aux.Loc” identiﬁes whether we train with the “answer and lo-calize” instructions, and “Visual Prompts” identiﬁes whether wetrain with visual prompts.
Aux.Loc Visual Prompts CiDEr ↑ BLEU-4↑ METEOR ↑ Rouge-L↑
- - 67.85 11.87 13.96 33.87
/check - 72.73 13.27 14.90 35.87
- /check 68.09 12.59 14.20 33.71
/check/check 74.80 13.68 15.40 36.25
without task-speciﬁc ﬁne-tuning in Tab. 5. The ﬁrst three
rows list the performance of LL3DA when trained fromscratch on one speciﬁc task, while the following three rowsrepresent the ﬁne-tuned models. The last row indicates thedirect evaluation of LL3DA. Results show that our modelcould distinguish 3D-DC and 3D-QA given the text instruc-tions and visual prompts, and achieve strong performance(62.98% C@0.5 on ScanRefer, 75.67% CiDEr on ScanQA).However, the generalist model achieves poor performanceon Nr3D [ 1], which is because we did not try to differen-
tiate between Nr3D and ScanRefer during training in theﬁrst place, as ScanRefer and Nr3D are used for the sametask. There is also an interesting observation that thoughwe did not differentiate between these two datasets for 3D-DC, the model still tend to achieve high scores on Scan-Refer (62.98% C@0.5). We are also excited to see thatthe weights of the generalist model can serve as a stronginitialization for ﬁne-tuning. For example, the ﬁne-tunedmodel on ScanRefer could achieve 65.19% C@0.5, which
is+2.35 % higher than the model trained from scratch.
Importance of Textual Instructions. We further conduct
study to see whether the text instructions are necessary for3D-DC in Tab. 7. The ﬁrst row is our baseline method
that directly generates the captions based on visual promptswithout any text instructions, and the second row is ourmethod that is trained with the text instructions introducedin Sec. 4.1. Both methods are trained from scratch for fair
comparison. We notice that since the LLM is frozen, certaintextual instructions are beneﬁcial when generating results in
speciﬁc domains/tasks.
Table 7. Effectiveness of Instructions on 3D Dense Captioning.
We perform experiments on ScanRefer[ 7]. The baseline method
directly generates the captions given the input 3D scene and visualprompts without any textual instructions.
Instructions C@0.5 ↑ B-4@0.5↑ M@0.5↑ R@0.5↑
- 60.20 34.79 25.40 54.03
/check 62.84 35.81 25.81 54.45
Clicks for Better Question Answering. One major chal-
lenge of answering questions in complex 3D environmentsis the vague identiﬁcation of objects with plain texts. There-fore, we try to click on some of the related objects alongwith the textual instructions during evaluation, and see howit could affect the generated answers on the ScanQA vali-dation set in Tab. 8. Results show that this technique would
remove the ambiguities, and further improve the quality ofthe answers ( +6.12 % C). This illustrates the importance of
visual interaction in complex 3D environments.
Table 8. Test Time Visual Interactions for Question Answering
on ScanQA[ 2].The model achieves better performance on the
question answering when we add visual prompts to some of therelated objects along with the text instructions during evaluation.
Visual Prompts CiDEr ↑ BLEU-4↑ METEOR ↑ Rouge-L↑
- 76.79 13.53 15.88 37.31
/check 82.91 11.80 16.74 39.97
5.4. Qualitative Results
We present several visualization results on different tasks in
Fig. 4to show our model’s capacities in understanding, rea-
soning, and planning in different 3D environments. To pre-
vent repetition when generating long sequences, we com-
bine the top- k[22] and top- p[27] sampling strategy with
k=5 0 andp=0.95.
26434
2 chairs
How many chairs are behind 
the long table?
t
3 chairs are behind long 
table
How many chairs are behind 
the long table? [Clicks]
t
CLICK
This is a brown armchair. It is to the right of the couch.
Describe this object in the 3D scene.
3
SpaCap3D : This is a chair with no arms. 
It is next to a desk.
Vote2Cap-DETR :This is a brown armchair. 
It is to the right of the table.
Ground Truth :The armchair is facing the 
desk. The armchair is located to the right 
of the couch, in the middle of the room.3D Dense Captioning
3D Question Answering
+ Visual Prompts3D Embodied Planning
1. pick up any trash off the floor. 2. 
Open the kitchen shelf. 3. Clean the 
kitchen sink with a cleaning solution. 
4. Clean the kitchen sink with a paper towel.
I want to clean the kitchen. What should I do?
s
3D Scene Description
This room is a kitchen with a stove, stovetop, cabinets, refrigerator, 
oven, cooler, recycling bins, microwave, water heater, soap dish, trash can, 
a sink, and a refrigerator. There are multiple walls scattered in the room, 
creating a separate space for dining and cooking. The door leads to the kitchen. Another door and several windows hang around the area, providing a 
large view of the floor. As well as additional doors, walls, windows, 
doorstopper caps, kitchen items, sink and fridge, the food preparation and 
storage areas are visible.
Describe this 3D scene.
D
Clean the refrigerator and cabinets
I want to clean the kitchen. I have done 
these things: 1. Turn on the lamp on the table. 2. Open the kitchen cabinet and take out cleaning supplies. 3. Clean the 
countertops with a cloth. 4. Clean the 
stove and microwave. What should I do next?
I
t
Figure 4. Qualitative Results. We provide several visualization results on various 3D vision and language tasks in diverse 3D environments
(living room, classroom, kitchen, and bedroom). Red highlights the wrong answer.
6. Conclusions
In this paper, we present LL3DA, a large language 3D as-
sistant that could take both textual- and visual- interactionsfrom human for understanding, reasoning, and planning incomplex 3D environments. Our model directly encodes the
3D point cloud and aggregates information from scenes andhuman interactions with the attention mechanism. We show
that the visual interactions could remove the ambiguitiesin cluttered 3D environments, showing mighty instruction-following capacities. Experiments show that our method
could achieve remarkable results on various 3D vision-language benchmarks. We hope our approach could inspire
further designs and training strategies for large 3D languagemodels. In future studies, we believe that the constructionof high-quality and diverse annotations will further enhance
the model’s reasoning and planning capabilities.
7. Acknowledgements
This work is supported by National Natural Science Foun-dation of China (No. 62071127 and 62101137), Na-tional Key Research and Development Program of China(No. 2022ZD0160100), Shanghai Natural Science Foun-dation (No. 23ZR1402900), Shanghai Municipal Scienceand Technology Major Project (No. 2021SHZDZX0103),A*STAR AME Programmatic Funding A18A2b0046,RobotHTPO Seed Fund under Project C211518008, andEDB Space Technology Development Grant under ProjectS22-19016-STDP . The computations in this research wereperformed using the CFFF platform of Fudan University.
26435
References
[1] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed
Elhoseiny, and Leonidas Guibas. Referit3d: Neural listenersfor ﬁne-grained 3d object identiﬁcation in real-world scenes.InEuropean Conference on Computer Vision , pages 422–
440. Springer, 2020. 2,5,7
[2] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki
Kawanabe. Scanqa: 3d question answering for spatial sceneunderstanding. In proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 19129–
19139, 2022. 2,4,5,6,7
[3] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic
metric for mt evaluation with improved correlation with hu-man judgments. In Proceedings of the acl workshop on in-
trinsic and extrinsic evaluation measures for machine trans-lation and/or summarization , pages 65–72, 2005. 5
[4] Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, and Dong
Xu. 3djcg: A uniﬁed framework for joint dense captioningand visual grounding on 3d point clouds. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition , pages 16464–16473, 2022. 2,5,6
[5] Y ang Cao, Yihan Zeng, Hang Xu, and Dan Xu. Coda: Col-
laborative novel box discovery and cross-modal alignmentfor open-vocabulary 3d object detection. In NeurIPS , 2023.
2
[6] Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li,
Maosong Sun, and Y ang Liu. Position-enhanced visualinstruction tuning for multimodal large language models.arXiv preprint arXiv:2308.13437 , 2023. 2
[7] Dave Zhenyu Chen, Angel X Chang, and Matthias Nießner.
Scanrefer: 3d object localization in rgb-d scans using natu-ral language. In European Conference on Computer Vision ,
pages 202–221. Springer, 2020. 2,5,6,7
[8] Dave Zhenyu Chen, Qirui Wu, Matthias Nießner, and An-
gel X Chang. D3net: A speaker-listener architecture forsemi-supervised dense captioning and visual grounding inrgb-d scans. arXiv preprint arXiv:2112.01551 , 2021. 5
[9] Long Chen, Oleg Sinavski, Jan H ¨unermann, Alice Karnsund,
Andrew James Willmott, Danny Birch, Daniel Maund, andJamie Shotton. Driving with llms: Fusing object-level vector
modality for explainable autonomous driving. arXiv preprint
arXiv:2310.01957 , 2023. 1
[10] Sijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Gang
Y u, and Tao Chen. End-to-end 3d dense captioning withvote2cap-detr. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 11124–
11133, 2023. 2,3,4,5,6
[11] Sijin Chen, Hongyuan Zhu, Mingsheng Li, Xin Chen,
Peng Guo, Yinjie Lei, Gang Y u, Taihao Li, and TaoChen. V ote2cap-detr++: Decoupling localization and de-scribing for end-to-end 3d dense captioning. arXiv preprint
arXiv:2309.02999 , 2023. 2
[12] Zhenyu Chen, Ali Gholami, Matthias Nießner, and Angel X
Chang. Scan2cap: Context-aware dense captioning in rgb-dscans. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition
, pages 3193–3203,
2021. 2,5[13] Zhenyu Chen, Ronghang Hu, Xinlei Chen, Matthias
Nießner, and Angel X Chang. Unit3d: A uniﬁed trans-former for 3d dense captioning and visual grounding. InProceedings of the IEEE/CVF International Conference onComputer Vision , pages 18109–18119, 2023. 2,5,6
[14] Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,Mostafa Dehghani, Siddhartha Brahma, et al. Scalinginstruction-ﬁnetuned language models. arXiv preprint
arXiv:2210.11416 , 2022. 1
[15] Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Y unxuan Li, Xuezhi Wang,Mostafa Dehghani, Siddhartha Brahma, et al. Scalinginstruction-ﬁnetuned language models. arXiv preprint
arXiv:2210.11416 , 2022. 2
[16] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nießner. Scannet:Richly-annotated 3d reconstructions of indoor scenes. InProceedings of the IEEE conference on computer vision andpattern recognition , pages 5828–5839, 2017. 5
[17] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, PascaleFung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning,2023. 2,4
[18] Alexandros Delitzas, Maria Parelli, Nikolas Hars, Geor-
gios Vlassis, Sotirios Anagnostidis, Gregor Bachmann, andThomas Hofmann. Multi-clip: Contrastive vision-languagepre-training for question answering tasks in 3d scenes. arXiv
preprint arXiv:2306.02329 , 2023. 2,5
[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectionaltransformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 4
[20] Runyu Ding, Jihan Y ang, Chuhui Xue, Wenqing Zhang,
Song Bai, and Xiaojuan Qi. Pla: Language-driven open-vocabulary 3d scene understanding. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition , pages 7010–7019, 2023. 2
[21] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,Jonathan Tompson, Quan Vuong, Tianhe Y u, et al. Palm-e: An embodied multimodal language model. arXiv preprint
arXiv:2303.03378 , 2023. 2
[22] Angela Fan, Mike Lewis, and Y ann Dauphin. Hierarchical
neural story generation. arXiv preprint arXiv:1805.04833 ,
2018. 7
[23] Daocheng Fu, Xin Li, Licheng Wen, Min Dou, Pinlong Cai,
Botian Shi, and Y u Qiao. Drive like a human: Rethink-ing autonomous driving with large language models. arXiv
preprint arXiv:2307.07162 , 2023. 1
[24] Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xi-
anzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xi-anzhi Li, Hongsheng Li, et al. Point-bind & point-llm:Aligning point cloud with multi-modality for 3d understand-ing, generation, and instruction following. arXiv preprint
arXiv:2309.00615 , 2023. 2,3
26436
[25] Y ucheng Han, Chi Zhang, Xin Chen, Xu Y ang, Zhibin Wang,
Gang Y u, Bin Fu, and Hanwang Zhang. Chartllama: A mul-
timodal llm for chart understanding and generation. arXiv
preprint arXiv:2311.16483 , 2023. 2
[26] Dailan He, Y usheng Zhao, Junyu Luo, Tianrui Hui, Shaofei
Huang, Aixi Zhang, and Si Liu. Transrefer3d: Entity-and-relation aware transformer for ﬁne-grained 3d visual ground-
ing. In Proceedings of the 29th ACM International Confer-
ence on Multimedia , pages 2344–2352, 2021. 2
[27] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Y ejin
Choi. The curious case of neural text degeneration. arXiv
preprint arXiv:1904.09751 , 2019. 7
[28] Yining Hong, Chunru Lin, Yilun Du, Zhenfang Chen,
Joshua B Tenenbaum, and Chuang Gan. 3d concept learn-ing and reasoning from multi-view images. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition , pages 9202–9212, 2023. 2
[29] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng,
Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Inject-ing the 3d world into large language models. arXiv preprint
arXiv:2307.12981 , 2023. 2,5,6
[30] Jiangyong Huang, Silong Y ong, Xiaojian Ma, Xiongkun
Linghu, Puhao Li, Y an Wang, Qing Li, Song-Chun Zhu,Baoxiong Jia, and Siyuan Huang. An embodied generalistagent in 3d world. arXiv preprint arXiv:2311.12871 , 2023.
1
[31] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor
Mihaylov, Daniel Simig, Ping Y u, Kurt Shuster, TianluWang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scalinglanguage model instruction meta learning through the lens ofgeneralization. arXiv preprint arXiv:2212.12017 , 2022. 1
[32] Biao Jiang, Xin Chen, Wen Liu, Jingyi Y u, Gang Y u, and
Tao Chen. Motiongpt: Human motion as a foreign language.arXiv preprint arXiv:2306.14795 , 2023. 1
[33] Y ang Jiao, Shaoxiang Chen, Zequn Jie, Jingjing Chen, Lin
Ma, and Y u-Gang Jiang. More: Multi-order relation min-ing for dense captioning in 3d scenes. arXiv preprint
arXiv:2203.05203 , 2022. 5
[34] Zhao Jin, Munawar Hayat, Y uwei Y ang, Y ulan Guo, and Yin-
jie Lei. Context-aware alignment and mutual masking for 3d-language pre-training. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
10984–10994, 2023. 2,5,6
[35] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C. Berg, Wan-Y en Lo, Piotr Doll ´ar, and
Ross Girshick. Segment anything. arXiv:2304.02643 , 2023.
4
[36] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training withfrozen image encoders and large language models. arXiv
preprint arXiv:2301.12597 , 2023. 2,4
[37] Mingsheng Li, Xin Chen, Chi Zhang, Sijin Chen, Hongyuan
Zhu, Fukun Yin, Gang Y u, and Tao Chen. M3dbench: Let’sinstruct large models with multi-modal 3d prompts. arXiv
preprint arXiv:2312.10763 , 2023. 1[38] Y anda Li, Chi Zhang, Gang Y u, Zhibin Wang, Bin Fu,
Guosheng Lin, Chunhua Shen, Ling Chen, and Y un-
chao Wei. Stablellava: Enhanced visual instruction tun-ing with synthesized image-dialogue data. arXiv preprint
arXiv:2308.10253 , 2023. 2
[39] Chin-Y ew Lin. Rouge: A package for automatic evaluation
of summaries. In Text summarization branches out , pages
74–81, 2004.
5
[40] Haotian Liu, Chunyuan Li, Qingyang Wu, and Y ong Jae Lee.
Visual instruction tuning. arXiv preprint arXiv:2304.08485 ,
2023. 1,2
[41] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 , 2017. 5
[42] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin John-
son. Scalable 3d captioning with pretrained models. arXiv
preprint arXiv:2306.07279 , 2023. 3
[43] Xiaojian Ma, Silong Y ong, Zilong Zheng, Qing Li, Yi-
tao Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d:Situated question answering in 3d scenes. arXiv preprint
arXiv:2210.07474 , 2022. 2
[44] Aihua Mao, Zhi Y ang, Wanxin Chen, Ran Yi, and Y ong-jin
Liu. Complete 3d relationships extraction modality align-ment network for 3d dense captioning. IEEE Transactions
on Visualization and Computer Graphics , 2023. 5
[45] Ron Mokady, Amir Hertz, and Amit H Bermano. Clip-
cap: Clip preﬁx for image captioning. arXiv preprint
arXiv:2111.09734 , 2021. 6
[46] OpenAI. Gpt-4 technical report, 2023. 1
[47] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. Bleu: a method for automatic evaluation of machinetranslation. In Proceedings of the 40th annual meeting of the
Association for Computational Linguistics , pages 311–318,
2002. 5
[48] Maria Parelli, Alexandros Delitzas, Nikolas Hars, Geor-
gios Vlassis, Sotirios Anagnostidis, Gregor Bachmann, andThomas Hofmann. Clip-guided vision-language pre-training
for question answering in 3d scenes. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition , pages 5606–5611, 2023. 2,4,5
[49] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M
Sadler, Wei-Lun Chao, and Y u Su. Llm-planner: Few-shot
grounded planning for embodied agents with large languagemodels. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 2998–3009, 2023. 2
[50] Zhende Song, Chenchen Wang, Jiamu Sheng, Chi Zhang,
Gang Y u, Jiayuan Fan, and Tao Chen. Moviellm: Enhancing
long video understanding with ai-generated movies. arXiv
preprint arXiv:2403.01422 , 2024. 1
[51] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara
Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-mamoorthi, Jonathan Barron, and Ren Ng. Fourier featureslet networks learn high frequency functions in low dimen-
sional domains. Advances in Neural Information Processing
Systems , 33:7537–7547, 2020. 4
[52] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Y asmine Babaei, Nikolay Bashlykov,Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
26437
Llama 2: Open foundation and ﬁne-tuned chat models. arXiv
preprint arXiv:2307.09288 , 2023. 1,3,4
[53] Ramakrishna V edantam, C Lawrence Zitnick, and Devi
Parikh. Cider: Consensus-based image description evalua-tion. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 4566–4575, 2015. 5
[54] Heng Wang, Chaoyi Zhang, Jianhui Y u, and Weidong Cai.
Spatiality-guided transformer for 3d dense captioning onpoint clouds. arXiv preprint arXiv:2204.10688 , 2022. 2,
5
[55] Y anmin Wu, Xinhua Cheng, Renrui Zhang, Zesen Cheng,
and Jian Zhang. Eda: Explicit text-decoupling and dense
alignment for 3d visual grounding. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition , pages 19231–19242, 2023. 2
[56] Peng Xu, Xiatian Zhu, and David A Clifton. Multimodal
learning with transformers: A survey. IEEE Transactions on
Pattern Analysis and Machine Intelligence , 2023. 2
[57] Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiang-
miao Pang, and Dahua Lin. Pointllm: Empowering largelanguage models to understand point clouds. arXiv preprint
arXiv:2308.16911 , 2023. 2,3
[58] Qinghao Y e, Haiyang Xu, Guohai Xu, Jiabo Y e, Ming Y an,
Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,Y aya Shi, et al. mplug-owl: Modularization empowerslarge language models with multimodality. arXiv preprint
arXiv:2304.14178 , 2023. 2
[59] Shuquan Y e, Dongdong Chen, Songfang Han, and Jing Liao.
3d question answering. IEEE Transactions on Visualization
and Computer Graphics , 2022. 2
[60] Fukun Yin, Xin Chen, Chi Zhang, Biao Jiang, Zibo Zhao, Ji-
ayuan Fan, Gang Y u, Taihao Li, and Tao Chen. Shapegpt:
3d shape generation with a uniﬁed multi-modal language
model. arXiv preprint arXiv:2311.17618 , 2023. 1
[61] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun,
Tong Xu, and Enhong Chen. A survey on multimodal largelanguage models. arXiv preprint arXiv:2306.13549 , 2023. 2
[62] Chi Zhang, Zhao Y ang, Jiaxuan Liu, Y ucheng Han, Xin
Chen, Zebiao Huang, Bin Fu, and Gang Y u. Appagent:Multimodal agents as smartphone users. arXiv preprint
arXiv:2312.13771 , 2023. 1
[63] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained trans-former language models. arXiv preprint arXiv:2205.01068 ,
2022. 1,2,3,4,5
[64] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi
Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: In-
struction tuning large language model on region-of-interest.arXiv preprint arXiv:2307.03601 , 2023. 2
[65] Lichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3dvg-
transformer: Relation modeling for visual grounding onpoint clouds. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 2928–2937, 2021. 2
[66] Lichen Zhao, Daigang Cai, Jing Zhang, Lu Sheng, Dong Xu,
Rui Zheng, Yinjie Zhao, Lipeng Wang, and Xibo Fan. To-wards explainable 3d grounded visual question answering:A new benchmark and strong baseline.
IEEE Transactions
on Circuits and Systems for Video Technology , 2022. 2
[67] Y ufeng Zhong, Long Xu, Jiebo Luo, and Lin Ma. Contextual
modeling for 3d dense captioning on point clouds. arXiv
preprint arXiv:2210.03925 , 2022. 5
[68] Qiang Zhou, Chaohui Y u, Shaofeng Zhang, Sitong Wu,
Zhibing Wang, and Fan Wang. Regionblip: A uniﬁed multi-modal pre-training framework for holistic and regional com-prehension, 2023. 3
[69] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. Minigpt-4: Enhancing vision-languageunderstanding with advanced large language models. arXiv
preprint arXiv:2304.10592 , 2023. 2
[70] Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan
Huang, and Qing Li. 3d-vista: Pre-trained transformer for 3dvision and text alignment. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 2911–
2921, 2023. 2,5,6
26438
