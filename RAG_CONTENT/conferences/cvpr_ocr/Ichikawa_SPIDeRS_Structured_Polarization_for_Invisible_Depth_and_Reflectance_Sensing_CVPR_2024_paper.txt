SPIDeRS: Structured Polarization for Invisible Depth and Reflectance Sensing
Tomoki Ichikawa Shohei Nobuhara Ko Nishino
Graduate School of Informatics, Kyoto University
https://vision.ist.i.kyoto-u.ac.jp/
RGB images AoLPs Depth map Normal map Relighting
Figure 1. We propose structured polarization, a novel invisible 3D sensing method using polarized light with per-pixel modulation of the
angle of linear polarization. The method enables completely stealth measurement of 3D shape, surface normals, and reflectance.
Abstract
Can we capture shape and reflectance in stealth? Such
capability would be valuable for many application domains
in vision, xR, robotics, and HCI. We introduce structured
polarization for invisible depth and reflectance sensing
(SPIDeRS), the first depth and reflectance sensing method
using patterns of polarized light. The key idea is to mod-
ulate the angle of linear polarization (AoLP) of projected
light at each pixel. The use of polarization makes it invisi-
ble and lets us recover not only depth but also directly sur-
face normals and even reflectance. We implement SPIDeRS
with a liquid crystal spatial light modulator (SLM) and a
polarimetric camera. We derive a novel method for ro-
bustly extracting the projected structured polarization pat-
tern from the polarimetric object appearance. We evalu-
ate the effectiveness of SPIDeRS by applying it to a number
of real-world objects. The results show that our method
successfully reconstructs object shapes of various materi-
als and is robust to diffuse reflection and ambient light. We
also demonstrate relighting using recovered surface nor-
mals and reflectance. We believe SPIDeRS opens a new
avenue of polarization use in visual sensing.
1. Introduction
Depth sensing has been a central topic of research in com-
puter vision since its inception. Due to its many down-
stream applications, a variety of approaches have been ex-
plored. Structured light, in particular, is a practical methodthat achieves high accuracy, which has enjoyed commercial
success in various forms. It exploits the duality of a projec-
tor with a camera and replaces one or more of a binocular
or multiview stereo with a projector. By throwing known
light patterns to the target, dense and accurate correspon-
dences are established for triangulation. This is particularly
useful for real-world objects which are often featureless and
cannot be handled with camera stereopsis alone. Its ease of
setup in addition to its dense reconstruction has made struc-
tured light a go-to approach in many applications, even for
ground truth acquisition for 3D reconstruction research.
Structured light, however, alters the appearance of the
target as the pattern on the surface needs to be visible to the
camera. This is undesirable for many applications outside
the research lab or an industrial plant. For instance, such
visible patterns would be distracting in xR systems. For
this, many structured light implementations rely on infrared
(IR) light and their extension to time-of-flight (ToF) sensing
[16, 33] as IR light is invisible to the naked eye. IR systems,
however, cannot be used for recovering surface properties
such as texture and reflectance as they are different in IR
from those in the visual spectrum.
Can we make shape and texture capture invisible? Is
there a way to simultaneously recover the reflectance of the
target such that we can even relight the object? Such a sys-
tem will make real-world object capture for downstream
applications completely stealth, creating a large opportu-
nity for effective communication (xR systems), advertise-
ment, robotics, art, and nondestructive inspection, such as
live scene editing and always-on change detection.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
25077
In this paper, we introduce Structured Polarization, a
first-of-its-kind novel depth and reflectance sensing method
using structured light of polarized light, which we refer to as
SPIDeRS. The key idea is to project structured light patterns
of varied angle of linear polarization (AoLP). Polarization
is invisible to the naked eye and a regular camera. By using
structured polarization patterns and an RGB-polarimetric
camera, we can realize invisible depth and texture capture.
Polarization also gives us two advantages that none of the
past visible or IR structured light methods have. The first is
that the surface normals can be directly recovered providing
dense fine details of the target surface. This is in sharp con-
trast to regular depth sensing where the normals can only
be obtained from the triangulated depth, which is inevitably
noisy due to the differentiation. The second is that we can
also estimate the reflectance properties of the target surface
from the polarimetric object appearance. This enables joint
depth and reflectance sensing with a single set of invisible
structured light patterns.
We implement SPIDeRS by assembling a projector-
camera system with an RGB-polarimetric camera and a po-
larization projector whose AoLP can be controlled in the
throw at each pixel. For this, we leverage the fact that liq-
uid crystal acts as a polarizer. We use a liquid crystal spatial
light modulator (SLM) to modulate the polarization orien-
tation of incident polarized light by altering the per-pixel
voltage. This can be understood as an LCD projector with-
out an analyzer that modulates the amplitude such that the
AoLP instead of the intensity is modulated. To accurately
decode the reflected AoLP structured pattern, we derive a
novel method that can extract the pattern from the captured
polarization image while accounting for ambient light and,
most important, specular and diffuse reflection which en-
codes and is independent from the thrown AoLP, respec-
tively. When there is no ambient light, the reflectance as the
bidirectional reflectance distribution function (BRDF) and
per-pixel surface normals can be recovered.
We validate SPIDeRS by evaluating its accuracy on a
number of real-world objects. The results show that our
method successfully reconstructs object shapes of various
materials and is robust to diffuse reflection and ambient
light. We also demonstrate successful reflectance and per-
pixel surface normal recovery which can be used for relight-
ing the object. We believe SPIDeRS opens a new avenue of
research and practical use of polarization and will serve as
a foundation for further studies.
2. Related Work
Various light patterns of intensity or color have been pro-
posed for structured light sensing. Salvi et al. [21] review
these structured light methods based on their coding strate-
gies and requirements. Discrete spatial multiplexing meth-
ods project stripe or two-dimensional array patterns that en-code codewords with neighboring pixels [2, 9]. Continuous
patterns gradually change the intensity or wavelength of the
projected light in space [3, 27]. Discrete time multiplexing
methods separate the space by projecting binary or N-ary
patterns [4, 20]. Phase shifting methods project sinusoidal
stripe patterns with different phases and decode them by ex-
tracting them from captured images [10, 19, 25]. Frequency
multiplexing methods project a sinusoidal or wavelet pat-
tern with spatially varying phase shifts and decode them in
the frequency domain [28]. Mirdehghan et al. [18] generate
sequences of projection patterns that satisfy the specified
conditions by minimizing the expected number of incorrect
corresponding pairs.
Sundar et al. [26] use a Single Photon Avalanche Diode
(SPAD) array to capture binary structured light patterns.
Bajestani and Beltrame [1] use a structured light projec-
tor to capture color and depth with a monochrome event-
based camera. Xu et al. [29] proposed a unified structured
light to acquire both object shape and reflectance with an
LCD mask and an LED array that enables angular sampling.
These methods use visible light which alters the appearance
of the surface. This makes them visible and necessitates
separate image acquisition for object appearance ( i.e., tex-
ture).
Structured light with IR light avoids interference with the
object appearance in the visual spectrum. It has been com-
mercialized as Kinect v1 [24, 33] and has been applied to
real-time applications and dense shape reconstruction from
a single shot [8, 22, 31]. These methods, however, also re-
quire separate visual spectrum image acquisition as IR light
behavior is very different from that in the visual spectrum.
Huang et al. [11] proposed polarization-coded structured
light for target enhancement. It encodes binary structured
light patterns as horizontally polarized light emitted from
the green channel of an LCD projector and vertically po-
larized light emitted from the red and blue channels. An
ordinary camera with a polarizer rotated horizontally cap-
tures the patterns in intensity and decodes them. This
polarization-coded structured light is extended to capture in
ambient light [12] and reconstruction in HDR [34]. These
methods, however, project not only polarization but also
color patterns which inevitably alter the surface appearance,
i.e., they are visible and cannot capture texture. Addition-
ally, they can only project a binary pattern, which limits
encoding strategies both in efficiency and accuracy.
In contrast, our method projects an imperceptible polar-
ization pattern that retains the natural surface appearance
and enables simultaneous texture acquisition with an RGB-
polarimetric camera. In addition to the 3D shape of the
object, we can estimate its reflectance and surface normals
without angular sampling by leveraging the captured sur-
face appearance and polarimetric reflection. Our method
can use a continuous encoding pattern which provides sub-
25078
pixel accuracy by using a liquid crystal SLM.
3. Polarization
Let us start by reviewing the properties of polarization [6,
7, 14].
3.1. Basic Property of Polarization
Light is a transverse electromagnetic wave oscillating per-
pendicularly to its propagation direction. While linearly
polarized light oscillates in a single orientation, circularly
polarized light rotates its oscillating orientation over time.
Completely polarized light is generally represented as a
superposition of coherent linearly and circularly polarized
light, which is called elliptically polarized light. Unpo-
larized light is light whose oscillating orientation changes
completely randomly. Light between unpolarized and po-
larized light is called partially polarized.
A polarizer enables observation of linearly polarized
light by only transmitting light oscillating in a specific ori-
entation. If we observe partially polarized light through a
polarizer whose filter angle ϕcis rotated, the observed in-
tensity changes sinusoidally as a function of it
I(ϕc) =I(1 +ρLcos(2 ϕc−2ϕL)), (1)
where Iis the average intensity over the filter angle, ρL
represents the strength of linear polarization which is re-
ferred to as the Degree of Linear Polarization (DoLP), and
ϕLrepresents the orientation of polarization, referred to as
the Angle of Linear Polarization (AoLP). With a linear po-
larizer, we cannot distinguish circularly polarized light from
unpolarized light.
To recover the linear polarization state, we need to cap-
ture at least three images with different filter angles. A po-
larimetric camera with four on-chip polarizers of different
filter angles ϕc= 0,π
4,π
2, and3
4πfor each pixel enables us
to acquire the linear polarization state in a single shot.
3.2. Mathematical Representation of Polarization
There are two different concise mathematical representa-
tions for polarization: Jones calculus and Mueller calculus.
Jones calculus handles coherent completely polarized light
and we use it to represent a liquid crystal SLM. Mueller
calculus handles incoherent partially polarized light and we
use it to represent reflection on the surface.
In Jones calculus, the polarization state is expressed with
a Jones vector E
E=Ex
Ey
=E0xexp(jϕx)
E0yexp(jϕy)
, (2)
where E0{x,y}andϕ{x,y}are the amplitude and phase of
the electric field for the {x,y}-axis, respectively, and jisthe imaginary unit. Modulation of amplitude and phase is
expressed with a Jones matrix J
Eo=JEi, (3)
where EiandEoare the Jones vectors of the input and out-
put light. A Jones matrix Jconsists of complex numbers.
In Mueller calculus, the polarization state is expressed
with a Stokes vector s
s=
s0
s1
s2
s3
=
I(0) + I(π
2)
I(0)−I(π
2)
I(π
4)−I(3
4π)
s3
=
2I
2IρLcosϕL
2IρLsinϕL
s3
,(4)
where s0represents the intensity of light, s1ands2repre-
sent the linearly polarized component, and s3represents the
circularly polarized component. The DoLP and AoLP can
be extracted from the Stokes vector as
ρL=p
s2
1+s2
2
s0, ϕ L=1
2tan−1s2
s1
, (5)
respectively. We can convert a Jones vector into a Stokes
vector, except for the absolute phase, as
s=
E2
0x+E2
0y
E2
0x−E2
0y
2E0xE0ycos(ϕx−ϕy)
−2E0xE0ysin(ϕx−ϕy)
. (6)
Polarization transformation by reflection is expressed by a
Mueller matrix M
sr=Ms i, (7)
where siandsrare Stokes vectors of incident and reflected
light, respectively.
4. Structured Polarization
We project AoLP patterns with the polarization projector
and decode the patterns reflected by the object surface to
recover the object shape.
4.1. Polarization Projector
We construct a polarization projector that can control the
AoLP of projected light at each pixel. We realize this with
an SLM consisting of Twisted Nematic (TN) liquid crystal.
While such SLM can be often found in a regular intensity
projector as an amplitude modulator combined with an ana-
lyzing polarization filter, our purpose fundamentally differs.
We use it as an AoLP modulator without the analyzer in our
polarization projector implementation. We could simply re-
move the front polarizer in an LCD projector to repurpose it
as a polarization projector, but we found this process to be
25079
error-prone due to the tightly sealed lens system in commer-
cial projectors. We instead build one from the ground-up.
Liquid crystal is a state of matter that simultaneously
shows liquid-like fluidity and solid-like anisotropy. TN liq-
uid crystal has a helical structure of molecules put between
alignment layers of perpendicularly oriented molecules. Its
helical anisotropy rotates the polarization orientation of the
incident polarized light. A voltage applied between the
alignment layers causes the molecules to become parallel
to the electric field and disturbs this helical structure, which
weakens the rotation of the polarization orientation. The
SLM is a two-dimensional array of cells consisting of the
TN liquid crystal and electrodes that can control the voltage
and hence polarization orientation for each pixel.
Light modulation by TN liquid crystal can be expressed
with a Jones matrix.
J=JR(−α)"
cosγ−jβ
γsinγα
γsinγ
−α
γsinγ cosγ+jβ
γsinγ#
,
(8)
where JR(−α)is the rotation matrix of angle α,α=π/2
is the twist angle of the alignment layers, βis the bire-
fringence that depends on the applied voltage, and γ=p
α2+β2. Since we consider only modulation of the po-
larization state, we omit the absolute phase of light [5, 30].
The birefringence βreaches 0as the applied voltage be-
comes large. We can rewrite Eq. (8) as
J=α
γJR(−α+γ) +JR(−α)Ae−jB0
0 ejB
,(9)
where A=1
γq
(γ−α)2cos2γ+β2sin2γandB=
tan−1{βsinγ/((γ−α) cosγ)}. The first term in Eq. (9)
represents the intended voltage-dependent rotation of linear
polarization and the second term represents the elliptical po-
larization.
When the applied voltage is small β≫α, the first term
becomes 0,A= 1, and B=β. In this case, we cannot
control the rotation of polarization. Otherwise the AoLP
rotates as a function of the applied voltage. Figure 2 shows
the DoLP and AoLP rotation of output light as a function
ofβin the range of β≤√
3αfor four AoLPs ϕof inci-
dent light. Although the DoLP decreases in some cases due
to the circular polarization component, we can rotate the
AoLP in the range of 90◦at will.
With an SLM and a polarized light source, we achieve
per-pixel AoLP modulation in our projector throw.
4.2. Polarimetric Reflection of AoLP Patterns
To decode the projected AoLP patterns from its polari-
metric reflection by the target object surface, let us con-
sider how surface reflection modulates the polarization state
DoLP
0.0 0.5 1.0 1.5 2.0 2.50.40.50.60.70.80.91.0
=0.0
=30.0
=45.0
=60.0
AoLP rotation [◦]
0.0 0.5 1.0 1.5 2.0 2.580
60
40
20
0
=0.0
=30.0
=45.0
=60.0
β[rad] β[rad]
Figure 2. DoLP and AoLP rotation through TN liquid crystal. The
angle ϕis the AoLP of the incident light. The TN liquid crystal
enables us to control the AoLP arbitrarily in the range of 90◦.
of incident light. Several polarimetric Bidirectional Re-
flectance Distribution Function (pBRDF) models that rep-
resent both specular and diffuse reflections have been pro-
posed [13, 15, 17, 23].
Polarimetric specular reflection is derived from Fresnel
reflection on mirror microfacets. Baek et al. [23] simplify
the polarimetric specular reflection, assuming a co-axial
projector-camera setup. The simplified polarimetric spec-
ular reflection can be expressed with a Mueller matrix
Ms≈csdiag(1 ,1,−1,−1), (10)
where diag(1 ,1,−1,−1)is a diagonal matrix and csis a
radiometric specular term that represents the shading and
specular BRDF. Since single scattering represents simi-
lar polarimetric behavior [13], we can incorporate it into
Eq. (10) by extending csto the sum of the radiometric terms
of the specular reflection and single scattering.
Specular reflection for a co-axial projector-camera re-
tains the incident polarization state and is desirable for de-
coding polarization patterns. For depth triangulation, how-
ever, the projector-camera system needs parallax, which
causes change in the polarimetric state of specular reflec-
tion. Fortunately, when the discrepancy of the viewing and
lighting directions is less than 20◦, AoLP rotation by specu-
lar reflection is theoretically less than 1.2◦and thus Eq. (10)
holds. We strike this balance by locating the camera and
the projector at a distance from the target surface and with
enough distance from each other for triangulation.
Polarimetric diffuse reflection is derived from Fresnel
transmission on a mirror macrofacet or microfacet. The
transmitted light is depolarized through subsurface scatter-
ing and polarized again by re-transmission. The Mueller
matrix of diffuse reflection becomes
Md=cd
1m12
dm13
d0
m21
dm22
dm23
d0
m31
dm32
dm33
d0
0 0 0 0
, (11)
where cdis the radiometric diffuse term that represents
shading and diffuse BRDF and mdconsists of the Fres-
25080
90◦
−90◦
Figure 3. AoLP pattern extraction from polarimetric reflection on
the object. Diffuse reflection and ambient light alter the AoLP
throw in the reflected polarimetric object appearance and cause
decoding errors (left). Our extraction method robustly extracts the
true projected pattern (right).
nel transmittance of transmission into the surface and re-
transmission into the air. Baek et al. [23] assume that the
elements of linear polarization modulation m22
d,m23
d,m32
d,
andm33
dare close to 0due to the small diffuse DoLP. We
follow this assumption and represent polarimetric diffuse
reflection as
Md≈cd
1m12
dm13
d0
m21
d 0 0 0
m31
d 0 0 0
0 0 0 0
. (12)
The observed Stokes vector when the AoLP pattern is
projected in ambient light becomes
so= (Ms+Md)si+sa, (13)
where siis the Stokes vector of projected light and sais the
Stokes vector of reflected ambient light. From Eqs. (10),
(11) and (13), we can express the observed AoLP as
ϕo=1
2tan−1−css2
i+cdm31
ds0
i+s2
a
css1
i+cdm21
ds0
i+s1a, (14)
where s0
x,s1
x, and s2
xare the elements of the correspond-
ing Stokes vector sx. The ratio of s2
iands1
irepresents
the projected AoLP pattern. Since polarimetric modulation
of specular reflection is represented by a constant diagonal
matrix, it retains the polarization of the projected pattern.
Diffuse reflection and ambient light, however, modulate the
projected pattern and ϕodeviates from the incident AoLP
ϕi=1
2tan−1s2
i
s1
i. In Sec. 4.3, we show that we can robustly
extract the projected AoLP from the polarimetric observa-
tion Eq. (13).
4.3. Encoding and Decoding AoLP Patterns
We can encode the projector pixel with spatially varying
AoLP values instead of intensity. Since we can continu-
ously change the AoLP at each pixel in the polarization
projector throw in the range of 90◦as shown in Sec. 4.1,
we can realize any of the patterns used in regular structured
light, such as time multiplexing and phase shifting.
We extract the AoLP of the projected light from polari-
metric images represented with Eq. (13). In Eq. (14), the
DoLP
0 50 100 150 200 2500.750.800.850.900.95
Filter 1
Filter 2
AoLP rotation [◦]
0 50 100 150 200 25080
60
40
20
0Filter 1
Filter 2
Pixel value Pixel value
Figure 4. Calibration results of DoLP and AoLP when the polar-
izing filter before the SLM is horizontal (Filter 1) and rotated by
45◦(Filter 2). We can continuously rotate the AoLP in the range of
[0◦,90◦]by changing the pixel value. The DoLP is large enough
to be detected for the horizontal filter.
diffuse reflection affects ϕoonly through s0
iwhile the spec-
ular reflection affects it through s1
iands2
i. By subtracting
the effect of s0
ifrom Eq. (13), we can remove the polarized
diffuse component from ϕowhile retaining the polarized
specular component. To implement this diffuse removal,
we use another polarimetric image of projected unpolarized
light with the same intensity as siwhose observed Stokes
vector becomes
sˆo=css0
i+cds0
icdm21
ds0
icdm31
ds0
i0T+sa.(15)
By subtracting Eq. (15) from Eq. (13), we can obtain s−=
so−sˆoas
s−=cd(m12
ds1
i+m13
ds2
i)css1
i−css2
i−css3
iT.(16)
The AoLP of incident pattern ϕiis extracted as
ϕi=−1
2tan−1s2
−
s1
−. (17)
Since the polarization projector cannot project unpolarized
light, we synthesize the polarimetric image represented as
Eq. (15) by averaging two polarimetric images captured
when the projected polarization orientations are perpendic-
ular to each other with the same DoLP.
From Fig. 2, we can capture these polarimetric images
when the rotation of AoLP is 0◦and−90◦, respectively.
Figure 3 shows the effectiveness of our AoLP extraction
method. While the diffuse and ambient light modulates the
observed AoLP, the extracted AoLP shows a clear pattern.
By decoding the extracted patterns, we can obtain dense
correspondences between the camera and the projector and
reconstruct the shape via triangulation.
4.4. Calibration
For accurate encoding, we polarimetrically calibrate our
projector, i.e., establish the relation between the applied
SLM pixel value and the polarization state of its actual
throw. We put the polarimetric camera without a camera
25081
LED telecentric illuminator
Polarizer
SLMConvex lens
Polarization camera
Figure 5. Polarization projector-camera system implementing
SPIDeRS. Our polarization projector consists of an LED telecen-
tric illuminator with a polarizer, SLM, and convex lens.
lens facing the polarization projector instead of a target ob-
ject and directly acquire the polarization state of the pro-
jected light. We input a uniform pattern of each pixel value
into the SLM and obtain a DoLP and an AoLP from the
average Stokes vector of the polarimetric camera. Figure 4
shows the calibration results of DoLP and AoLP of differ-
ent incident polarized light. The results show that we can
continuously rotate AoLP in the range of [0◦,90◦]as shown
in Fig. 2. Since a larger DoLP is desirable for high-fidelity
detection, we project horizontally polarized light onto the
SLM. We can use uniform patterns of 0and255 for the
specular AoLP extraction. Although the DoLP values for 0
and255are slightly different, Fig. 3 shows that this differ-
ence can be ignored.
We also calibrate the extrinsic and intrinsic parameters
of the polarization projector for shape reconstruction, us-
ing known intrinsic camera parameters. We first project the
structured polarization light onto a ChArUco board of dif-
ferent poses and capture them. By detecting the poses of
the board, we obtain 3D points on the board at each cam-
era pixel. We also obtain corresponding points between the
camera and projector by decoding the polarization pattern.
These correspondences provide 3D points on the board at
some projector pixels, which enables us to calibrate the pro-
jector in the same way as camera calibration [32].
5. BRDF and Surface Normal Estimation
If ambient light is not present, we can also estimate the sur-
face normal and reflectance of a dielectric surface by ex-
ploiting their close relation in polarimetric reflection. When
the incident polarization changes, the polarization of spec-
ular reflection changes but that of diffuse reflection remains
the same. This enables the estimation of diffuse polariza-
tion and intensity of each reflection component from which
we can recover the surface normals and BRDF, respectively.
We use FMBRDF [15], which unifies polarimetric and
radiometric reflectance of a dielectric surface, to represent
the surface BRDF. We assume that the diffuse (body reflec-
tion) albedo is spatially varying but the other FMBRDF pa-rameters are uniform over the surface, i.e., the surface con-
sists of a single material but has texture. We use polari-
metric images captured under uniform patterns for specular
extraction to ensure reliable incident polarization states.
Initial Estimates We compute initial surface normals
with Principal Component Analysis (PCA) of 3D points.
To obtain an initial estimate of the spatially varying diffuse
albedo, we separate diffuse and specular reflections by us-
ing polarization. From Eqs. (4) and (16), the specular re-
flection becomes
Is=css0
i=1
ρiq
(s1
−)2+ (s2
−)2, (18)
where ρiis the DoLP of the projected light. Temporarily as-
suming Lambertian diffuse reflection, we obtain initial dif-
fuse albedo from diffuse reflection Id=s0
o−Is. We obtain
initial estimates of the other uniform FMBRDF parameters
by minimizing the error of intensity and DoLP
min
µ,ks,α,β,κNX
iKX
k(LI,k,i+λ1Lρ,k,i), (19)
where LI,k,i =|s0
o,k,i−s0
o,k,i|2is the intensity error,
Lρ,k,i =|ρo,k,i−ρo,k,i|2is the DoLP error, s0
o,k,i and
s0
o,k,i are the first elements of captured and rendered Stokes
vectors at pixel iof image k, respectively, ρo,k,i andρo,k,i
are the captured and rendered DoLP, respectively, Nis the
number of pixels, and Kis the number of images. The
optimized FMBRDF parameters are the refractive index µ,
specular albedo ks, surface roughness α, shape of micro-
facet distribution function β, and concentration parameter
of the microfacet correlation function κ. We set λ1= 0.1.
Joint Estimation of BRDF and Surface Normals After
the initial estimation, we refine the BRDF parameters and
surface normals with joint optimization. Since the AoLP
depends on the surface normal, we add the error of the sec-
ond and third elements of the Stokes vector to Eq. (19)
min
µ,ks,α,β,κ,Kb,NNX
iKX
k(LI,k,i+λ2Lρ,k,i+λ3Ls,k,i),
(20)
where Ls,k,i=|s1o−s1
o|2+|s2o−s2
o|2,Kbrepresents the
spatially varying diffuse albedo and Nrepresents surface
normals. We set λ2= 0.01andλ3= 1. For each pixel,
we obtain six different equations of the Stoke vectors from
two polarimetric images. Since the unknown parameters
are three pixel-wise and five global parameters, we have a
sufficient number of equations to solve for them.
6. Experimental Results
We evaluate the accuracy of SPIDeRS with various real ob-
jects of different materials. We also show that, in the ab-
25082
Captured imageStructured light
using intensity (GT)Captured imagePC structured
light [11]Captured image Ours
1.18/0.82
 0.49/0.31
330 [mm]
280 [mm]
0.83/0.61
 0.36/0.24
300 [mm]
260 [mm]
0.87/0.66
 0.39/0.27
310 [mm]
260 [mm]
Figure 6. Reconstructed depth maps of real objects of various materials and colors. The scene of the first and second rows do not have
ambient light and the third row does. The numbers below each depth map are the mean and median of the depth errors in millimeters. The
reconstruction results of our method are comparable with classic structured light, but its capture is completely invisible.
ObjectStructured light
using intensityPC structured
light [11]Our initial normal Ours
Figure 7. Reconstructed surface normal maps of a real object. Our method fully exploits polarimetric reflection to directly estimate detailed
surface normals instead of obtaining them as byproducts of depth values.
sence of ambient light, we can estimate the BRDF and per-
pixel surface normals of the object. We demonstrate object
relighting with the estimated BRDF and surface normals.
As shown in Fig. 5, we implement SPIDeRS with an
LED telecentric illuminator with a polarizer, a spatial light
modulator with a resolution of 1024×768(HOLOEYE LC
2012), and an f=100mm convex lens. We capture polari-
metric images of the objects onto which our structured
light is projected with a commercial monochrome polari-
metric camera (Lucid TRI050S-PC) or a quad-Bayer RGB-
polarimetric camera (Lucid TRI050S-QC). Since polariza-
tion modulation of the SLM depends on wavelength, we
use a green LED and the monochrome polarimetric camerafor depth and normal reconstruction in Secs. 6.1 and 6.2.
For BRDF estimation in Sec. 6.3, we use a white LED
and the RGB-polarimetric camera, reconstruct the depths
from the green channel, and estimate the surface normal and
the BRDF for each channel. Implementation of an RGB-
polarization projector is left for future work.
For the structured polarization patterns, we use phase-
shifting unwrapped with Gray codes. We post-process the
decoding result by removing camera pixels whose corre-
sponding projector pixels are discontinuous. To alleviate
failures of decoding on the edge (boundary) of Gray codes,
we also project two sequences of shifted patterns and unify
the decoding results of three sequences by averaging the po-
25083
GTStructured light
using intensityOurs
10.9◦/9.6◦
10.9◦/9.3◦
30◦
0◦
Figure 8. Quantitative comparison of reconstructed normal maps
of a real object. The numbers below each normal map are the
mean and median of angular errors in degrees. Our method can
accurately reconstruct detailed geometry.
sitions of corresponding projector pixels.
6.1. Depth Accuracy
We first evaluate recovered depth map accuracy of SPI-
DeRS. We compare our method with classic structured
light using intensity and polarization-coded (PC) structured
light [11]. For the classic structured light, we use the
same encoding strategy as our method and project ampli-
tude modulation patterns by inserting a polarizing filter be-
tween the SLM and the convex lens. Since PC structured
light only uses binary patterns, we emulate it with Gray
code using our polarization projector. As PC structured
light uses an ordinary camera through a polarizing filter, we
use one of the four filter angles of the polarimetric camera
to mimic it. Note that the original work uses an LCD pro-
jector and needs to project the patterns with color and thus
fundamentally alters the object appearance.
Figure 6 shows captured images and reconstructed depth
maps of our method and other methods, with and without
ambient light. Holes in the reconstructed depth maps are
caused by incorrect correspondences. PC structured light
suffers from decoding errors due to the low contrast of the
patterns resulting from DoLP reduction and AoLP alter-
ation by diffuse and ambient light and, by construction, can-
not capture surface texture simultaneously due to the polar-
izing filter in front of the camera. In contrast, our method
successfully reconstructs object shapes of different materi-
als and is robust to diffuse and ambient light while simulta-
neously capturing the surface appearance. The results also
show that the accuracy of our method is comparable with
classic structured light while retaining the surface appear-
ance and being totally invisible to the naked eye. Please see
the supplementary material for results for a metallic surface.
6.2. Surface Normal Accuracy
Figures 7 and 8 show qualitative and quantitative compar-
isons of the estimated surface normal map and those com-
puted from the reconstructed depth maps by PCA, respec-
tively. We obtain the ground truth normal map by photo-Object Relighting1 Relighting2 Object Relighting1 Relighting2
Figure 9. Relighting under a novel illumination with the estimated
BRDF and surface normal. Illuminations are white directional
lights from the left (Relighting1) and red, green, and blue ones
from different directions (Relighting2). Our method can simul-
taneously estimate reflectance and surface normals along with a
depth map by exploiting the captured texture and polarimetric re-
flection. The results can be used to relight the object.
metric stereo. Our method successfully exploits polarimet-
ric reflection to reconstruct detailed surface normals unlike
past methods that can only produce them as differentiated
byproducts of the measured depth maps.
6.3. Relighting
Figure 9 shows relit target objects with estimated BRDFs
under novel illuminations of different directions and colors.
Our method can estimate the BRDF accurately which re-
sults in qualitatively plausible relighting. This is possible
as our method can simultaneously capture the texture and
polarimetric reflection in the visible spectrum. Past struc-
tured light methods cannot estimate the BRDF and surface
normals as they only acquire radiometric images and the es-
timation is ill-posed. Please see the supplementary material
for more results.
7. Conclusion
In this paper, we introduced SPIDeRS, the first structured
polarization depth and reflectance sensing method. SPI-
DeRS acquires object shape, reflectance, and surface nor-
mals from polarimetric images captured by modulating per-
pixel AoLP in the projector throw. A liquid crystal SLM
was used to achieve this structured polarization. We de-
rived a novel method for robustly extracting the projected
AoLP pattern from the observed polarimetric image formed
by polarimetric reflection on the object surface. Our method
enables object shape reconstruction while retaining the sur-
face appearance intact so that surface texture and polarimet-
ric reflection can simultaneously be captured from which
the BRDF and surface normals can also be recovered. Most
important, the whole process is completely invisible to the
naked eye. We believe SPIDeRS provides a new means of
depth and reflectance sensing and will serve as a foundation
for further research in this exciting new area of structured
polarization.
Acknowledgement This work was in part supported by
JSPS 20H05951, 21H04893, 23H03420, and 23KJ1367,
JST JPMJCR20G7 and JPMJAP2305, and RIKEN GRP.
25084
References
[1] Seyed Ehsan Marjani Bajestani and Giovanni Beltrame.
Event-based RGB sensing with structured light. In WACV ,
pages 5458–5467, 2023. 2
[2] Kim L Boyer and Avinash C Kak. Color-Encoded Structured
Light for Rapid Active Ranging. IEEE TPAMI , 9(1):14–28,
1987. 2
[3] Brian Carrihill and Robert Hummel. Experiments with the
Intensity Ratio Depth Sensor. Computer Vision, Graphics,
and Image Processing , 32(3):337–358, 1985. 2
[4] Dalit Caspi, Nahum Kiryati, and Joseph Shamir. Range
Imaging with Adaptive Color Structured Light. IEEE
TPAMI , 20(5):470–480, 1998. 2
[5] Jeffrey A Davis, Ignacio Moreno, and Philbert Tsai. Po-
larization eigenstates for twisted-nematic liquid-crystal dis-
plays. Appl. Opt. , 37(5):937–945, 1998. 4
[6] Eugene Hecht. Optics . Pearson, 5 edition, 2016. 3
[7] Yoshiki Fukao, Ryo Kawahara, Shohei Nobuhara, and Ko
Nishino. Polarimetric Normal Stereo. In CVPR , pages 682–
690, 2021. 3
[8] Ryo Furukawa, Michihiro Mikamo, Ryusuke Sagawa, and
Hiroshi Kawasaki. Single-shot dense active stereo with
pixel-wise phase estimation based on grid-structure using
CNN and correspondence estimation using GCN. In WACV ,
pages 4001–4011, 2022. 2
[9] Paul M Griffin, Lakshmi S Narasimhan, and Soung R Yee.
Generation of uniquely encoded light patterns for range data
acquisition. Pattern Recognition , 25(6):609–616, 1992. 2
[10] Mohit Gupta and Shree K Nayar. Micro Phase Shifting. In
CVPR , pages 813–820. IEEE, 2012. 2
[11] Xiao Huang, Jian Bai, Kaiwei Wang, Qun Liu, Yujie Luo,
Kailun Yang, and Xianjing Zhang. Target enhanced 3D
reconstruction based on polarization-coded structured light.
Opt. Express , 25(2):1173–1184, 2017. 2, 7, 8
[12] Xiao Huang, Yujie Luo, Jian Bai, Ruiqi Cheng, Kejing He,
Kaiwei Wang, Qun Liu, Yupeng Luo, and Juan Du. Polari-
metric target depth sensing in ambient illumination based
on polarization-coded structured light. Appl. Opt. , 56(27):
7741–7748, 2017. 2
[13] Inseung Hwang, Daniel S Jeon, Adolfo Mu ˜noz, Diego
Gutierrez, Xin Tong, and Min H Kim. Sparse Ellipsometry:
Portable Acquisition of Polarimetric SVBRDF and Shape
with Unstructured Flash Photography. ACM TOG , 41(4):1–
14, 2022. 4
[14] Tomoki Ichikawa, Matthew Purri, Ryo Kawahara, Shohei
Nobuhara, Kristin Dana, and Ko Nishino. Shape from Sky:
Polarimetric Normal Recovery Under The Sky. In CVPR ,
pages 14827–14836, 2021. 3
[15] Tomoki Ichikawa, Yoshiki Fukao, Shohei Nobuhara, and Ko
Nishino. Fresnel Microfacet BRDF: Unification of Polari-
Radiometric Surface-Body Reflection. In CVPR , pages
16489–16497, 2023. 4, 6
[16] Leonid Keselman, John Iselin Woodfill, Anders Grunnet
Jepsen, and Achintya Bhowmik. Intel Realsense Stereo-
scopic Depth Cameras. In CVPRW , pages 1–10, 2017. 1
[17] Yuhi Kondo, Taishi Ono, Legong Sun, Yasutaka Hirasawa,
and Jun Murayama. Accurate Polarimetric BRDF for RealPolarization Scene Rendering. In ECCV , pages 220–236.
Springer, 2020. 4
[18] Parsa Mirdehghan, Wenzheng Chen, and Kiriakos N Kutu-
lakos. Optimal Structured Light a la Carte. In CVPR , pages
6248–6257, 2018. 2
[19] Daniel Moreno, Kilho Son, and Gabriel Taubin. Embedded
Phase Shifting: Robust Phase Shifting with Embedded Sig-
nals. In CVPR , pages 2301–2309, 2015. 2
[20] Jeffrey L Posdamer and Martin D Altschuler. Surface Mea-
surement by Space-encoded Projected Beam Systems. Com-
puter Graphics and Image Processing , 18(1):1–17, 1982. 2
[21] Joaquim Salvi, Sergio Fernandez, Tomislav Pribanic, and
Xavier Llado. A state of the art in structured light patterns
for surface profilometry. Pattern Recognition , 43(8):2666–
2680, 2010. 2
[22] Simon Schreiberhuber, Jean-Baptiste Weibel, Timothy Pat-
ten, and Markus Vincze. GigaDepth: Learning Depth
from Structured Light with Branching Neural Networks. In
ECCV , pages 214–229. Springer, 2022. 2
[23] Seung-Hwan Baek, Daniel S. Jeon, Xin Tong, and Min
H. Kim. Simultaneous Acquisition of Polarimetric SVBRDF
and Normals. ACM TOG , 37:1 – 15, 2018. 4, 5
[24] Jan Smisek, Michal Jancosek, and Tomas Pajdla. 3D with
Kinect. In ICCVW , pages 1154–1160, 2011. 2
[25] V Srinivasan, Hsin-Chu Liu, and Maurice Halioua. Auto-
mated phase-measuring profilometry: a phase mapping ap-
proach. Appl. Opt. , 24(2):185–188, 1985. 2
[26] Varun Sundar, Sizhuo Ma, Aswin C Sankaranarayanan, and
Mohit Gupta. Single-Photon Structured Light. In CVPR ,
pages 17865–17875, 2022. 2
[27] Johji Tajima and Masato Iwakawa. 3-D data acquisition by
Rainbow Range Finder. In ICPR , pages 309–313. IEEE,
1990. 2
[28] Mitsuo Takeda and Kazuhiro Mutoh. Fourier transform
profilometry for the automatic measurement of 3-D object
shapes. Appl. Opt. , 22(24):3977–3982, 1983. 2
[29] Xianmin Xu, Yuxin Lin, Haoyang Zhou, Chong Zeng, Yaxin
Yu, Kun Zhou, and Hongzhi Wu. A Unified Spatial-Angular
Structured Light for Single-View Acquisition of Shape and
Reflectance. In CVPR , pages 206–215, 2023. 2
[30] Makoto Yamauchi and Tomoaki Eiju. Optimization of
twisted nematic liquid crystal panels for spatial light phase
modulation. Optics Communications , 115(1-2):19–25, 1995.
4
[31] Yuping Ye, Hongguang Chang, Zhan Song, and Juan Zhao.
Accurate infrared structured light sensing system for dy-
namic 3D acquisition. Appl. Opt. , 59(17):E80–E88, 2020.
2
[32] Zhengyou Zhang. A Flexible New Technique for Camera
Calibration. IEEE TPAMI , 22(11):1330–1334, 2000. 6
[33] Zhengyou Zhang. Microsoft Kinect Sensor and Its Effect.
IEEE MultiMedia , 19(2):4–10, 2012. 1, 2
[34] Zhenmin Zhu, Duoduo You, Fuqiang Zhou, Sheng Wang,
and Yulin Xie. Rapid 3D reconstruction method based on the
polarization-enhanced fringe pattern of an hdr object. Opt.
Express , 29(2):2162–2171, 2021. 2
25085
