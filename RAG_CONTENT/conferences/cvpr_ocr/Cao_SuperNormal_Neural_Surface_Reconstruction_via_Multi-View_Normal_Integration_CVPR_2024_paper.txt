SuperNormal: Neural Surface Reconstruction
via Multi-View Normal Integration
Xu Cao Takafumi Taketomi
CyberAgent
{xucao, taketomi takafumi }@cyberagent.co.jp
Abstract
We present SuperNormal, a fast, high-ﬁdelity approach
to multi-view 3D reconstruction using surface normal maps.
With a few minutes, SuperNormal produces detailed sur-
faces on par with 3D scanners. We harness volume ren-
dering to optimize a neural signed distance function (SDF)
powered by multi-resolution hash encoding. To accelerate
training, we propose directional ﬁnite difference and patch-
based ray marching to approximate the SDF gradients nu-
merically. While not compromising reconstruction quality,
this strategy is nearly twice as efﬁcient as analytical gra-
dients and about three times faster than axis-aligned ﬁnite
difference. Experiments on the benchmark dataset demon-
strate the superiority of SuperNormal in efﬁciency and ac-
curacy compared to existing multi-view photometric stereo
methods. On our captured objects, SuperNormal produces
more ﬁne-grained geometry than recent neural 3D recon-
struction methods. Our code is available at https://
github.com/CyberAgentAILab/SuperNormal .
1. Introduction
Recovering high-quality 3D geometry of real-world ob-
jects from their multi-view images is a long-standing chal-
lenge in computer vision. Recently, neural implicit surface-
based methods have shown remarkable reconstruction re-
sults. Compared to traditional multi-view stereo (MVS)
methods [ 25], neural methods are more robust to view-
dependent observations and textureless surfaces [ 29]. Fur-
thermore, the reconstruction procedure has become highly
efﬁcient [ 30] by introducing multi-resolution hash cod-
ing [ 20]. However, even though this spatial encoding al-
lows ﬁne-grained geometry to be represented, shape recon-
struction from multi-view images tends to smooth out high-
frequency surface details, as shown in Fig. 1bottom right.
Multi-view photometric stereo (MVPS), on the other
hand, aims to recover pixel-level high-frequency surface
detail by introducing additional lighting conditions during
Multi-view posed normal mapsOurs (51 secs)PS-NeRF (8 hrs)Scanner [Rexcan CS+]
Ours (5 mins)NeuS2 (5 mins)~100 images18 normal maps
Scanner [EinScan SE]
Figure 1. (Top) From multi-view surface normal maps,
SuperNormal recovers ﬁne-grained surface details comparable to
3D scanners while being orders of magnitude faster than the exist-
ing MVPS method PS-NeRF [ 31].(Bottom) Using normal maps
produces more faithful high-frequency details than the MVS-
based method NeuS2 [ 30], although both use multi-resolution hash
encoding [ 20].
the image acquisition [ 9]. In a typical workﬂow, a sur-
face normal map is ﬁrst recovered for each view, record-
ing per-pixel surface orientation information. The normal
maps estimated at different viewpoints are then fused into a
3D model, also known as multi-view normal integration [ 5].
However, existing normal fusion methods struggle to reﬂect
the details of the normal maps in the shape of the recov-
ered 3D model. Moreover, the reconstruction process takes
hours even when only a few low-resolution normal maps are
used [ 4,31]. Due to the lack of a fast and accurate multi-
view normal fusion method, current multi-view photometric
stereo results remain unsatisfactory.
This paper presents SuperNormal to unite the best of
both worlds. Normal maps with pixel-level surface details
are utilized to exploit the expressive power and efﬁciency
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
20581
of multi-resolution hash encoding-powered neural implicit
surface. Using normal maps alone avoids per-scene re-
ﬂectance modeling and optimization, thus improving train-
ing efﬁciency compared to using color images. In a few
minutes, our method can produce ﬁne-grained geometry on
par with commercial 3D scanners.
Speciﬁcally, we use volume rendering to optimize a neu-
ral signed distance function (SDF) such that its rendered
normal maps are consistent with input normal maps. To
further improve the training efﬁciency, we propose a direc-
tional ﬁnite difference and patch-based ray marching strat-
egy to approximate the SDF gradients numerically. This
strategy kills two birds with one stone: SDF values eval-
uated at sampled points are allused both for SDF-based
volume rendering and SDF gradient approximation. This
way, we avoid second-order derivative computation during
training. Unlike traditional axis-aligned ﬁnite difference,
directional ﬁnite difference avoids redundant SDF evalua-
tions (Fig. 2). Without compromising reconstruction qual-
ity, our method is nearly twice as fast as using analytical
gradients and about three times faster than ﬁnite difference.
Compared to existing neural MVPS methods, our
method is two orders of magnitude faster and recovers bet-
ter surface details (Fig. 1above). We also capture objects
with complex geometry and show that our method recov-
ers more faithful surface details than the recent neural 3D
reconstruction method [ 30] (Fig. 1bottom). Taking ad-
vantage of recent advances in photometric stereo, we can
estimate high-quality normal maps from images captured
under common lighting conditions with inexpensive equip-
ment [ 10]. Combined with SuperNormal, high-ﬁdelity 3D
objects can be reconstructed using inexpensive equipment
in widely accessible environments.
In summary, this paper’s key contributions are:
•An effective neural 3D reconstruction approach using
multi-view normal maps, enhancing the reconstruction
quality to near 3D scanner levels;
•An efﬁcient method for computing numerical gradients in
SDF-based neural rendering, speeding up the reconstruc-
tion process; and
•A comprehensive evaluation using benchmark objects
with MVPS approaches and an assessment using our cap-
tured objects with MVS approaches.
2. Related work
2.1. Multi-view photometric stereo (MVPS)
MVPS aims at high-ﬁdelity 3D reconstruction using multi-
view observations under varying light conditions [ 9] (usu-
ally one-light-at-time (OLAT) images [ 16,26]).
Existing methods differ in what geometry-related infor-
mation is inferred in each view and how information from
different views is fused into a complete 3D model. Chang etWorld space
SDF samples for    Volume rendering    SDF gradient approximation    Both
Figure 2. (Left) In SDF-based volume rendering, approximating
SDF gradients using axis-aligned ﬁnite difference [ 18,36] requires
additional SDF samples at neighboring positions of volume ren-
dering points, increasing computational complexity and training
time. (Right) Our strategy of patch-based ray marching and direc-
tional ﬁnite difference effectively uses all SDF samples for both
SDF gradient approximation and volume rendering.
al.[5] proposes a level set method [ 22] to fuse multi-view
normal maps into a volumetric representation of the shape.
R-MVPS [ 23] uses the multi-view normal maps to reﬁne a
coarse mesh estimated by multi-view stereo. B-MVPS [ 16]
initialize the shape as a sparse point cloud estimated by
structure from motion (SfM), then propagates the known
point spatially using the normal maps.
Recent MVPS methods have undergone a paradigm shift
since the advent of NeRF [ 19] by using a neural implicit
scene representation, usually parameterized as a multi-layer
perception (MLP). PS-NeRF [ 31] recovers a neural ﬁeld
based on UNISURF [ 21] by regularizing the gradients of
the neural ﬁeld using the normal maps. UA-MVPS [ 12]
fuses the multi-view depth and normal maps into a neural
SDF using neural inverse rendering while considering per-
pixel conﬁdence. MV AS [ 4] shows utilizing the azimuth
components of multi-view normal maps is sufﬁcient to re-
cover the neural implicit surface. While most methods only
use images in the same view for normal map estimation,
MVPSNet [ 35] utilizes neighboring views to infer per-view
depth and normal maps jointly and applies screened Poisson
surface reconstruction (sPSR) [ 13] to obtain the shape.
However, the reconstructed surfaces of these methods
tend to be smoothed and do not reﬂect the high-frequency
details of the input normal maps. Our approach introduces
multi-resolution hash encoding and directional ﬁnite differ-
ence to improve the implicit neural ﬁeld’s expressive power
and training efﬁciency.
2.2. Neural 3D reconstruction
The seminal work NeRF [ 19] and follow-up works [ 2,3,27]
have demonstrated remarkable novel view synthesis qual-
ity by volume rendering a neural implicit ﬁeld. For better
geometry reconstruction quality, IDR [ 32] imposes eikonal
20582
regularization [ 8] such that the neural ﬁeld approximates a
valid SDF. Later, the concurrent works V olSDF [ 33] and
NeuS [ 29] develop SDF-based volume rendering, which
bridges SDF values to volume rendering weights and im-
proves the geometry ﬁdelity. However, these methods re-
quire several hours to recover one object due to optimizing
a deep coordinate-based MLP.
Multi-resolution hash encoding has been proposed to
improve expressive power and accelerate the learning of
neural implicit ﬁeld [ 20]. Several efforts have been made
to incorporate this expressive spatial encoding into neural
SDF learning. Since the ofﬁcial CUDA implementation
does not support the double propagation deduced by the
eikonal regularization, PermutoSDF [ 24] and NeuS2 [ 30]
re-implement the CUDA programming. InstantNSR [ 36]
and Neuralangelo [ 18] use ﬁnite difference instead of auto-
matic differentiation to avoid double propagation.
We propose directional ﬁnite difference, a numerical
way to compute SDF gradients that avoids double propaga-
tion and is even faster than automatic differentiation when
combined with a tailored patch-based sampling strategy.
2.3. Neural 3D reconstruction with normals
Normal maps have been used as auxiliary information to fa-
cilitate neural scene reconstruction in MonoSDF [ 34] and
NeuRIS [ 28]. These works target scene-level reconstruc-
tion, and normal maps help infer the geometry of texture-
less smooth regions ( e.g., walls) where using color images
is likely to break down.
Instead, our method is particularly useful for single ob-
jects with complex surface details. We use photometric
stereo [ 10] to estimate normal maps from images under
varying light conditions. This produces high-quality nor-
mal maps that can be used alone for shape reconstruction.
3. Approach
We aim to recover the surface given a set of normal maps,
object masks, and camera intrinsic and extrinsic parameters.
Since photometric stereo methods estimate the normal maps
in camera space, we ﬁrst apply the camera-to-world rotation
to normal maps to obtain world-space normal maps. Fig-
ure3depicts our pipeline. We represent the geometry as
a parametric SDF and optimize SDF parameters such that
its rendered gradient and opacity values are consistent with
input normal vectors and mask values, respectively.
3.1. Pipeline
SDF-based volume rendering (Forward pass) A signed
distance function (SDF) implicitly captures the geometry
by assigning each spatial point with the signed distance to
its closest surface point. The surface Mcan then be repre-sented as the zero-level-set of the SDF:
M={x|f(x)=0 }. (1)
We parameterize the SDF using multi-resolution hash en-
coding followed by a shallow MLP. In multi-resolution hash
encoding, the 3D space is discretized into a set of multi-
resolution 3D grids. Each grid cell is associated with a
unique hash value mapping the 3D coordinates to a learn-
able feature vector. Speciﬁcally, h(x; )is the feature vec-
tor obtained by concatenating feature vectors of different
levels h(x)=[ x,h1(x),. . . ,h L(x)]. Then, the SDF can be
parameterized as
f(x)=f(h(x; );✓), (2)
where  and✓are learnable hash-encoding feature vec-
tors and MLP parameters. Multi-resolution hash encoding
has proven to speed up training and improve the expressive
power of the neural SDF [ 20,30,36].
During training, we evaluate the SDF values at points
sampled on the rays cast from the camera centers toward
the scene. NeuS [ 29] proposes an unbiased and occlusion-
aware way to transfer the SDF samples to volume rendering
opacities. Given Nordered 3D points {xi}N
i=0on a ray and
their SDF values {f(xi)}N
i=0, the opacity at each point is
designed as
↵i= max✓ s(f(xi))  s(f(xi+1))
 s(f(xi)),0◆
, (3)
where  s(x)=1
1+exp(  sx)is the sigmoid function with a
trainable sharpness s.
With this SDF-based opacity, we can render the normal
and opacity for the pixel pas:
ˆn(p)=NX
i=0Ti↵irf(xi),ˆo(p)=NX
i=0Ti↵i
with Ti=i 1Y
j=0(1 ↵j).(4)
From Eqs. ( 3) and ( 4), the neural SDF must be evaluated
at least once at sampled points for volume rendering. In
Secs. 3.2and3.3, we show that these SDF samples can be
reused for SDF gradient approximation, thus reducing the
computational amount and speeding up the training.
Neural SDF Training (Backward pass) To train the neu-
ral SDF, we supervise the rendered normals and opacities
using input ones n(p)andˆo(p). The loss function consists
20583
Hash EncodingTiny MLPNeuralSDF
(a) Patch-based ray marching(b) Directionalfinite difference(c) SDF-based volume rendering!"(!)%"(!)
&'()Normaland opacity maps
World space
Figure 3. Approach overview. (a) Given input views, we randomly sample patches of pixels, cast rays from those pixels, and march a
plane along the patch of rays (Sec. 3.3). We treat the ray-plane intersections as sampled points and evaluate their SDF values via the neural
SDF. (b)SDF samples neighbor to a point are used to compute the SDF gradients using directional ﬁnite difference (Sec. 3.2).(c)SDF
samples on the same rays are used for SDF-based volume rendering (Sec. 3.1).
of three terms: The normal, the mask, and the eikonal term:
L=normalz }| {X
p(ˆn(p) n(p))2+ 1maskz }| {X
pBCE (ˆ o(p),o(p))
+ 2X
x(krf(x)k2 1)2
| {z }
eikonal,(5)
where pis the sampled pixels in a batch, xis the sampled
points on rays, and BCE is the binary cross entropy func-
tion. The normal term encourages the rendered SDF gradi-
ents to be consistent with the input normal vectors so that
accurate geometry details are recovered. The mask term
aligns the shape’s silhouette with input masks and can be
considered the boundary condition. The eikonal term en-
forces the SDF gradient norm to be close to 1almost every-
where so that the neural SDF is approximately valid [ 22].
3.2. Directional Finite Difference
As seen, both the normal and eikonal terms backpropa-
gate the loss gradient via the SDF gradient to neural SDF
parameters ( i.e., double backpropagation@L
@rf@rf
@✓). The
most common way is to use Pytorch’s automatic differen-
tiation (AD) due to implementation simplicity. However,
the multi-resolution hash encoding package does not ofﬁ-
cially support double propagation. Common workarounds
include CUDA programming [ 24,30] or use ﬁnite differ-
ence (FD) [ 18,36] to bypass double propagation. However,
CUDA programming can be inﬂexible in that it shrinks the
design space ( e.g., NeuS2 [ 30] requires ReLU activation),
and FD will slow down the training. Consider the ordinaryaxis-aligned FD:
rf(x)⇡2
64f(x+✏x) f(x ✏x)
2✏f(x+✏y) f(x ✏y)
2✏f(x+✏z) f(x ✏z)
2✏3
75, (6)
where ✏⇤2{x,y,z }is an offset vector along x,y, orzaxis
with a tiny step size ✏. Computing the gradient at one point
requires evaluating the SDF at its nearby points. These ad-
ditional SDF samples are also included in the computation
graph in the forward pass. Consequently, using FD signiﬁ-
cantly slows the forward rendering and backward optimiza-
tion even if the efﬁcient spatial encoding is used [ 18].
To avoid redundant SDF evaluations and double back-
propagation, we propose directional ﬁnite difference (DFD)
to reuse the volume rendering SDF samples for SDF gradi-
ents approximation. Consider the SDF value at a 3D point
xparameterized by the ray origin oand direction v:
f(x)=f(o+tv), (7)
where tis the distance to the ray origin. From Eq. ( 7), we
have
df
dt=rf(x)>v:=rvf. (8)
Equation ( 8) is exactly the directional derivative of the SDF,
which projects the SDF gradient onto the ray direction.
Suppose we parameterize the same point along three dif-
ferent directions as
x=oi+tivi,i 2{1,2,3}. (9)
Applying Eq. ( 8) then yields three linear equations:
2
4 v>
1 
 v>
2 
 v>
3 3
5
|{z}
Vrf=2
64df
dt1df
dt2df
dt33
75
|{z}
rVf. (10)
20584
From Eq. ( 10), we can compute the SDF gradient by
rf=V 1rVf. (11)
Numerically, we can approximate the directional derivative
by
df
dti⇡f(oi+(ti+ t)vi) f(oi+(ti  t)vi)
2 t
=f(xi+ tvi) f(xi  tvi)
2 t,i 2{1,2,3}
(12)
Equations ( 11) and ( 12) imply that, to numerically approx-
imate the SDF gradient, we can ﬁrst compute ﬁnite differ-
ences along three directions V, then linearly transform the
concatenated ﬁnite difference by V 1. We call this SDF
gradient approximation directional ﬁnite difference. Equa-
tion ( 11) has a unique solution if the three directions are
non-coplanar.
The ordinary ﬁnite difference can be viewed as the spe-
cial case of Eq. ( 11) by choosing the three coordinate axes
as the ray directions. In this case, Vbecomes the identity
matrix. Both DFD and FD require nearby SDF samples to
approximate the SDF gradient. However, by tailoring the
sampling strategy, we can reuse the SDF samples in vol-
ume rendering for DFD without additional SDF samples, as
described in the following.
3.3. Patch-based Ray Marching
DFD requires computing the ﬁnite difference in three direc-
tions. In volume rendering, we naturally have SDF samples
along the viewing ray direction that can be used for ﬁnite
difference. For the other two directions, we propose to sam-
ple patches of pixels instead of single pixels and march a
plane on each patch of rays to locate the sampled points on
parallel planes.
Speciﬁcally, we perform ordinary ray marching for the
center pixel/ray of the patch and treat the sampled points as
the intersections between a marching plane and the center
ray. The points to be sampled on the remaining rays within
the patch can then be found as the intersection of the planes
and the rays:
tj=tiv>
im
v>
jm, (13)
where mis the unit normal direction of the marching plane,
tiis the ray marching distance on the center ray with direc-
tionvi, and tjis the distance from the intersection point to
the ray origin ( i.e., camera center) along the direction vj;
for derivation see Sec. B.2inSupMat.
We choose the marching plane to parallel the image
plane where the patch is located (Fig. 3). With this sampling
strategy, the three directions for DFD computation are the
viewing ray direction and the world-space directions of x-andy-axis of the camera coordinates. The marching plane
normal mbecomes the world-space direction of the z-axis
of the camera coordinates and is the same for all pixels/rays
from the same view. This way, the SDF samples on the
same ray can be used for volume rendering as Eq. ( 4), and
the SDF samples neighbor to a point can be used for SDF
gradient computation as Eq. ( 11). We use the central differ-
ence ( i.e., Eq. ( 12)) for points with neighbors in both direc-
tions and forward/backward difference for boundary points.
This sampling strategy is beneﬁcial in three aspects.
First, it allows us to reuse the SDF samples from neigh-
boring rays to compute DFD. Second, it stabilizes training
by ensuring the existence of V 1, since the viewing ray di-
rection will never co-planer to the image plane. Third, it
reduces the computation overhead during training because
the three ray directions are identical for samples from the
same pixel/ray. The inverse of ray directions V 1can be
pre-computed for each pixel before training. In our experi-
ments, the computation takes about 0.6s e c for two million
pixels. During training, only the 3⇥3matrix-vector multi-
plication Eq. ( 11) is required, which can almost be ignored
compared to AD or FD.
4. Experiments
We evaluate our method quantitatively on the MVPS bench-
mark in Sec. 4.1, investigate different components of our
method in Sec. 4.2and evaluate our method qualitatively
on our captured real-world data in Sec. 4.3.
Implemetation details We apply SDM-UniPS [ 10] to im-
ages in each view to obtain normal maps. In each batch, we
randomly sample 2048 patches of 3⇥3pixels from all input
normal maps. Like INGP [ 20], we skip the empty and oc-
cluded space during ray marching using NerfAcc [ 17]. The
scene is bounded within a unit sphere, and we use a coarse-
to-ﬁne ray marching step decreasing log-linearly from 1e 2
to1e 3. We optimize the shape of DiLiGenT-MV [ 16]
benchmark objects for 5000 batches (roughly equal to 20
epochs), and 30000 batches for our captured data. The
weights of loss terms are set as 1. We use the Adam op-
timizer with an initial learning rate 5e 3. More details can
be found in Sec. BinSupMat.
4.1. Quantitative evaluation
Dataset We evaluate our method using the only MVPS
benchmark dataset DiLiGenT-MV [ 16]. It consists of ﬁve
objects captured from 20views, and 96OLAT images are
captured in each view, yielding a total of 1920 images per
object. Each image is 512⇥612, and the total number of
foreground object pixels is about 2.2million. The camera is
ﬁxed during capture, and the object is placed on a turntable
in a darkroom. The “ground truth” meshes are created using
a 3D scanner.
20585
Baselines We benchmark SuperNormal against several
MVPS methods grouped into three categories. (1) Those
that reﬁne an initial point cloud or mesh and require man-
ual effort, including R-MVPS [ 23] and B-MVPS [ 16]. (2)
Those that use Poisson surface reconstruction (PSR) [ 13]
to fuse multi-view depth and normal maps, including
MVPSNet [ 35]. (3) Those that optimize an implicit
neural surface representation, including UA-MVPS [ 12],
PS-NeRF [ 31], MV AS [ 4], and our method.
Evaluation metrics We use L2 Chamfer distance (CD) and
F-score with threshold ⌧F=0.5m m to evaluate geome-
try accuracy [ 12,15]. For CD and F-score, we only con-
sider visible points from the input views by casting rays for
all pixels and ﬁnding the ﬁrst ray-mesh intersections (More
details in Sec. B.4inSupMat. )
Results As reported in Table 1, our method achieves the
best geometry accuracy among all compared MVPS meth-
ods (See Sec. AinSupMat. for normal accuracy). We at-
tribute this superior performance to two aspects: 1) the pow-
erful expressive capability of multi-resolution hash encod-
ing [20] and 2) the ﬁne-grained normal estimation method
SDM-UniPS [ 10], as discussed in Sec. 4.2. Figure 4visual-
izes the recovered shapes of different methods. Our method
recovers ﬁne-grained surface details on par with the “GT”
meshes produced by a 3D scanner.
Table 1also reports the average runtime measured on
our machine with one RTX 4090Ti graphics card. The
runtime of R-MVPS [ 23] and B-MVPS [ 16] is difﬁcult to
measure since both methods require intensive manual ef-
fort. The code of UA-MVPS [ 12] is not publicly avail-
able, and several hours per object are required, as reported
in MVPSNet [ 35]. Both PS-NeRF [ 31] and MV AS [ 4] re-
quire hours per object since they optimize a dense MLP. Our
method is the fastest among neural implicit representation-
based methods, with less than one minute per object.
On the other hand, MVPSNet [ 35] is highly efﬁcient be-
cause its pipeline mainly consists of feedforward inferenc-
ing a trained neural network and an sPSR [ 13] step, the lat-
ter of which has been optimized over one decade. Although
our method is slower than MVPSNet [ 35], we achieve better
reconstruction quality within a reasonable time.
4.2. Ablation study
Effect of normal maps Since multi-view normal integra-
tion is not bound to speciﬁc normal estimation methods, we
investigate the effect of different normal estimation meth-
ods on normal integration methods. We test three sets of
normal maps, estimated by SDPS [ 6] or SDM-UniPS [ 10],
and the one rendered from the scanned mesh ( i.e., “GT”
normal maps provided by DiLiGenT-MV [ 16]). Table 2
reports the L2-Chamfer distance and F-score averaged on
ﬁve DiLiGenT-MV [ 16] objects. Our method consistentlyachieves better results than PS-NeRF [ 31] and MV AS [ 4]
when using the same set of normal maps. This veri-
ﬁes that our method can better fuse the information from
2.5D normal maps to 3D shapes. Using the SoTA method
SDM-UniPS [ 10] for normal estimation, our method real-
izes the best reconstruction quality.
Effect of spatial encoding Table 2also compares the geom-
etry accuracy between multi-resolution hash encoding (hash
enc.) and positional encoding (pos. enc.). Our results verify
again that multi-resolution hash encoding surpasses posi-
tional encoding in expressiveness, enhancing geometry ac-
curacy. Therefore, the keys to superior MVPS performance
are high-quality normal estimation, an expressive surface
representation, and an effective normal fusion approach.
Effect of SDF gradient computation Table 3compares
the geometry accuracy and runtime using ﬁnite difference
(FD), Pytorch’s automatic differentiation (AD), and direc-
tional ﬁnite difference (DFD) to compute the SDF gradi-
ent. All other parameter settings are kept the same. We do
not observe a signiﬁcant difference regarding reconstruction
quality. However, DFD performs consistently faster than
FD and AD by accelerating both the forward rendering and
backward optimization.
Generalizability of DFD In principle, DFD and patch-
based sampling strategy apply to any SDF-based volume
rendering like NeuS [ 29] or Neuralangelo [ 18]. To verify
this, we incorporate DFD and patch-based sampling into
Neuralangelo [ 18]1. We follow the coarse-to-ﬁne sampling
strategy of Neuralangelo [ 18] but use DFD instead of FD for
SDF gradient approximation. We do not use mask supervi-
sion for Neuralangelo [ 18]. As shown in Figure 5, training
time is reduced by 25% on a scene from the DTU bench-
mark [ 11], even 2.25⇥more pixels are looped in training.
The result suggests that DFD could be a general strategy to
accelerate NeuS-like multi-view reconstruction [ 29].
4.3. Qualitative results on our captured data
To further demonstrate the performance of SuperNormal in
ﬁne-grained surface detail recovery, we also captured ob-
jects with more complex details, as shown in Fig. 6. Thanks
to the generalizability of SDM-UniPS [ 10], we can capture
the objects under casual light conditions ( e.g., within an
apartment) without ensuring a darkroom setup. We man-
ually move a video light during the capture without any
photometric or geometric light calibration. In total, we cap-
ture 18viewpoints ⇥13light conditions for each object
(1under ambient light for SfM). We use Metashape [ 1] to
calibrate camera parameters, use SAM [ 14] to create fore-
ground masks, and apply SDM-UniPS [ 10] to every 12im-
age to infer the per-view normal map.
1Our modiﬁed code of Neuralangelo [ 18] is available at https://
github.com/xucao-42/Neuralangelo_DFD.git .
20586
Table 1. Quantitative evaluation on DiLiGenT-MV [ 16] benchmark. Red and orange cells indicate the best and the second best results
respectively. On average, our approach achieves the best reconstruction quality at the second fastest speed.
L2 Chamfer distance [ mm](#) F-score ( ⌧F=0.5m m )(") Runtime ( #)
Methods Bear Buddha Cow Pot2 Reading Average Bear Buddha Cow Pot2 Reading Average Average
R-MVPS [ 23] 1.070 0.397 0.440 1.504 0.561 0.794 0.262 0.698 0.760 0.198 0.519 0.487 NA
B-MVPS [ 16] 0.212 0.254 0.091 0.201 0.259 0.203 0.958 0.902 0.986 0.946 0.914 0.941 NA
MVPSNet [ 35] 0.317 0.279 0.255 0.310 0.248 0.282 0.813 0.866 0.889 0.838 0.918 0.865 37 secs
UA-MVPS [ 12] 0.414 0.452 0.326 0.414 0.382 0.398 0.707 0.669 0.798 0.731 0.762 0.733 several hrs
PS-NeRF [ 31] 0.260 0.314 0.287 0.254 0.352 0.293 0.898 0.806 0.856 0.919 0.785 0.853 8 hrs
MV AS [ 4] 0.243 0.357 0.216 0.197 0.522 0.307 0.909 0.754 0.907 0.962 0.546 0.816 70 mins
Ours 0.184 0.218 0.193 0.150 0.223 0.194 0.983 0.950 0.977 0.994 0.916 0.962 48 secs
R-MVPS [ 23] B-MVPS [ 16] UA-MVPS [ 12] PS-NeRF [ 31] MV AS [ 4] MVPSNet [ 35] Ours 3D Scanner
 1m m
highlow0
 1m m
highlow0Figure 4. Qualitative comparison of recovered shapes ofBuddhaandPot2and their error maps. Best viewed on screen.Table 2. Quantitative evaluation using normal maps estimated bydifferent methods as inputs. The metrics are averaged over ﬁveDiLiGenT-MV [16] objects. Darker colors indicate better results.L2 Chamfer distance [mm](#)F-score (⌧F=0.5m m)(")SDPS [6] SDM-UniPS [10] GTSDPS [6] SDM-UniPS [10] GTMV AS [4]0.3070.3320.2920.8160.8120.859PS-NeRF [31]0.2930.2960.2240.8530.8440.938Ours (pos. enc.)0.3010.2280.1950.8490.9420.977Ours (hash enc.)0.2810.1940.1140.8600.9620.998
128random patches of  rays per batch   batchesNeuralangelo w/ finite differenceDTU scan#24Neuralangelo w/ DFD
8 hours6 hoursFigure 5. DFD and patch-based sampling accelerate Neuralan-gelo’s training without quality degradation. We train both alterna-tives for 20k batches. For FD, 512 random rays are sampled perbatch; for DFD, 128 random patches of3⇥3rays per batch.
20587
Table 3. Ablation study using different strategies for SDF gradi-
ents computation in volume rendering. We report the metrics aver-
aged over ﬁve DiLiGenT-MV [ 16] objects. FD: Finite difference,
AD: Automatic differentiation, DFD : Directional ﬁnite difference.
Accuracy Runtime (Sec)
CD F-Score Forward Backward Total
FD 0.192 0.963 47.7 70.3 124.6
AD 0.195 0.961 29.4 42.4 78.1
DFD 0.194 0.961 19.6 22.5 48.3
Figure 6. Our real-world data capture setup. The target object
is placed on a turntable before a dark sheet under general indoor
illumination. There is no need to prepare a darkroom. We use
an iPhone to take images and move a video light to illuminate the
object from the camera side. Objects are 6c m to15 cm high.
Figures 1and7display the results from NeuS2 [ 30],
SuperNormal, and a structured-light based commercial
3D scanner EinScan SE. Although NeuS2 [ 30] and our
method use the same multi-resolution hash encoding for
ﬁne-grained scene representation, the surface details recov-
ered from multi-view color images are smoothed out. Our
method, beneﬁtting from the normal map inputs, recovers
more ﬁne-grained surface details.
We ﬁnd that our scanner performs poorly in highly con-
cave regions (Fig. 7above). This is because the scanner
uses a wide baseline stereo camera (about 15 cm apart), and
small concave regions create self-occlusions, thus reducing
the reconstruction quality. In principle, normal estimation
does not suffer from self-occlusion because it is based on
single-view observations. Fusing multi-view normal maps
can then produce better results in concave regions.
5. Discussion
We have demonstrated SuperNormal, a fast, high-ﬁdelity
3D reconstruction approach using multi-view normal maps
estimated by photometric stereo. SuperNormal outperforms
other neural MVPS methods in speed and reconstruction
quality on the benchmark dataset. Compared to multi-view
neural reconstruction methods, using normal maps can ex-
ploit the expressive power of multi-resolution hash encod-
ing, and produce scanner-level surface details. Further, we
have shown the effectiveness of directional ﬁnite differ-
ences in expediting training without sacriﬁcing quality.
SuperNormal provides a promising direction to makeImage NeuS2 [ 30] Ours EinScan SE
Figure 7. Qualitative comparison using our objects. Our approach
recovers better surface detail than multi-view stereo approaches
and achieves competitive results with commercial 3D scanners.
ScannerOurs
OmniData normal
OursFigure 8.Limitations.(Left) Fault artifacts are highlighted in redcircles. (Right) Our method is sensitive to the inaccuracy in nor-mal maps estimated by monocular methods (e.g., OmniData [7]).high-quality 3D scanning more accessible. Unlike costlycommercial 3D scanners, our approach is viable with ev-eryday devices like smartphones and basic equipment suchas a tripod, video light, and a turntable.LimitationsOur method may produce fault-like artifacts,as shown in Fig.8. The artifacts exist no matter our SDFgradient computation approach (i.e., AD, FD, or DFD) andrequire further investigation. Moreover, the performancedepends on mask supervision and high-quality normal maps(e.g., those obtained from photometric stereo). Our methoddoes not work well with the monocular normal estimationmethods like OmniData [7] (Fig.8).
20588
References
[1]Agisoft metashape. https://www.agisoft.com .6
[2]Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter
Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.
Mip-nerf: A multiscale representation for anti-aliasing neu-
ral radiance ﬁelds. ICCV , 2021. 2
[3]Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.
Srinivasan, and Peter Hedman. Zip-NeRF: Anti-aliased grid-
based neural radiance ﬁelds. ICCV , 2023. 2
[4]Xu Cao, Hiroaki Santo, Fumio Okura, and Yasuyuki Mat-
sushita. Multi-view azimuth stereo via tangent space consis-
tency. In Proc. of Computer Vision and Pattern Recognition
(CVPR) , pages 825–834, 2023. 1,2,6,7,11,12
[5]Ju Yong Chang, Kyoung Mu Lee, and Sang Uk Lee. Mul-
tiview normal ﬁeld integration using level set methods. In
Proc. of Computer Vision and Pattern Recognition (CVPR) ,
pages 1–8. IEEE, 2007. 1,2
[6]Guanying Chen, Kai Han, Boxin Shi, Yasuyuki Matsushita,
and Kwan-Yee K. Wong. SDPS-Net: Self-calibrating deep
photometric stereo networks. In CVPR , 2019. 6,7
[7]Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir
Zamir. Omnidata: A scalable pipeline for making multi-
task mid-level vision datasets from 3d scans. In ICCV , pages
10786–10796, 2021. 8
[8]Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and
Yaron Lipman. Implicit geometric regularization for learning
shapes. In Proceedings of Machine Learning and Systems
2020 , pages 3569–3579. 2020. 3,11
[9]Carlos Hernandez, George V ogiatzis, and Roberto Cipolla.
Multiview photometric stereo. IEEE Transactions on Pattern
Analysis and Machine Intelligence (PAMI) , 30(3):548–554,
2008. 1,2
[10] Satoshi Ikehata. Scalable, detailed and mask-free universal
photometric stereo. In Proc. of Computer Vision and Pattern
Recognition (CVPR) , 2023. 2,3,5,6,7
[11] Rasmus Jensen, Anders Dahl, George V ogiatzis, Engil Tola,
and Henrik Aanæs. Large scale multi-view stereopsis evalu-
ation. In CVPR , pages 406–413. IEEE, 2014. 6
[12] Berk Kaya, Suryansh Kumar, Carlos Oliveira, Vittorio Fer-
rari, and Luc Van Gool. Uncertainty-aware deep multi-view
photometric stereo. In Proc. of Computer Vision and Pattern
Recognition (CVPR) , pages 12601–12611, 2022. 2,6,7
[13] Michael Kazhdan and Hugues Hoppe. Screened Poisson sur-
face reconstruction. ACM Transactions on Graphics (ToG) ,
32(3):1–13, 2013. 2,6
[14] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and
Ross Girshick. Segment anything. In Proc. of International
Conference on Computer Vision (ICCV) , pages 4015–4026,
2023. 6
[15] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen
Koltun. Tanks and temples: Benchmarking large-scale scene
reconstruction. ACM Transactions on Graphics (ToG) , 36
(4):1–13, 2017. 6
[16] Min Li, Zhenglong Zhou, Zhe Wu, Boxin Shi, Changyu
Diao, and Ping Tan. Multi-view photometric stereo: A ro-bust solution and benchmark dataset for spatially varying
isotropic materials. IEEE Transactions on Image Process-
ing, 29:4159–4173, 2020. 2,5,6,7,8,11,12
[17] Ruilong Li, Hang Gao, Matthew Tancik, and Angjoo
Kanazawa. NerfAcc: Efﬁcient sampling accelerates nerfs.
InProc. of International Conference on Computer Vision
(ICCV) , pages 18537–18546, 2023. 5,13
[18] Zhaoshuo Li, Thomas M ¨uller, Alex Evans, Russell H. Tay-
lor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin.
Neuralangelo: High-ﬁdelity neural surface reconstruction. In
Proc. of Computer Vision and Pattern Recognition (CVPR) ,
pages 8456–8465, 2023. 2,3,4,6
[19] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:
Representing scenes as neural radiance ﬁelds for view syn-
thesis. In Proc. of European Conference on Computer Vision
(ECCV) , 2020. 2
[20] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a mul-
tiresolution hash encoding. ACM Transactions on Graphics
(ToG) , 41(4):1–15, 2022. 1,3,5,6
[21] Michael Oechsle, Songyou Peng, and Andreas Geiger.
UNISURF: Unifying neural implicit surfaces and radiance
ﬁelds for multi-view reconstruction. In Proc. of International
Conference on Computer Vision (ICCV) , 2021. 2
[22] Stanley Osher, Ronald Fedkiw, and K Piechor. Level set
methods and dynamic implicit surfaces. Appl. Mech. Rev. ,
57(3):B15–B15, 2004. 2,4
[23] Jaesik Park, Sudipta N Sinha, Yasuyuki Matsushita, Yu-
Wing Tai, and In So Kweon. Robust multiview photometric
stereo using planar mesh parameterization. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence (PAMI) ,
39(8):1591–1604, 2016. 2,6,7,11,12
[24] Radu Alexandru Rosu and Sven Behnke. Permutosdf: Fast
multi-view reconstruction with implicit surfaces using per-
mutohedral lattices. In CVPR , 2023. 3,4
[25] Johannes Lutz Sch ¨onberger, Enliang Zheng, Marc Pollefeys,
and Jan-Michael Frahm. Pixelwise view selection for un-
structured multi-view stereo. In Proc. of European Confer-
ence on Computer Vision (ECCV) , 2016. 1
[26] Marco Toschi, Riccardo De Matteo, Riccardo Spezialetti,
Daniele De Gregorio, Luigi Di Stefano, and Samuele Salti.
Relight my NeRF: A dataset for novel view synthesis and
relighting of real world objects. In Proc. of Computer Vi-
sion and Pattern Recognition (CVPR) , pages 20762–20772,
2023. 2
[27] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,
Jonathan T. Barron, and Pratul P. Srinivasan. Ref-NeRF:
Structured view-dependent appearance for neural radiance
ﬁelds. CVPR , 2022. 2
[28] Jiepeng Wang, Peng Wang, Xiaoxiao Long, Christian
Theobalt, Taku Komura, Lingjie Liu, and Wenping Wang.
Neuris: Neural reconstruction of indoor scenes using normal
priors. In ECCV , 2022. 3
[29] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. NeuS: Learning neural im-
plicit surfaces by volume rendering for multi-view recon-
20589
struction. Advances in Neural Information Processing Sys-
tems (NeurIPS) , 2021. 1,3,6
[30] Yiming Wang, Qin Han, Marc Habermann, Kostas Dani-
ilidis, Christian Theobalt, and Lingjie Liu. NeuS2: Fast
learning of neural implicit surfaces for multi-view recon-
struction. In Proc. of International Conference on Computer
Vision (ICCV) , 2023. 1,2,3,4,8,14,15
[31] Wenqi Yang, Guanying Chen, Chaofeng Chen, Zhenfang
Chen, and Kwan-Yee K. Wong. PS-NeRF: Neural inverse
rendering for multi-view photometric stereo. In Proc. of Eu-
ropean Conference on Computer Vision (ECCV) , 2022. 1,2,
6,7,11,12,14
[32] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan
Atzmon, Basri Ronen, and Yaron Lipman. Multiview neu-
ral surface reconstruction by disentangling geometry and ap-
pearance. Advances in Neural Information Processing Sys-
tems (NeurIPS) , 33, 2020. 2
[33] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. V ol-
ume rendering of neural implicit surfaces. Advances in Neu-
ral Information Processing Systems (NeurIPS) , 34:4805–
4815, 2021. 3
[34] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sat-
tler, and Andreas Geiger. MonoSDF: Exploring monocu-
lar geometric cues for neural implicit surface reconstruction.
NIPS , 2022. 3
[35] Dongxu Zhao, Daniel Lichy, Pierre-Nicolas Perrin, Jan-
Michael Frahm, and Soumyadip Sengupta. MVPSNet: Fast
generalizable multi-view photometric stereo. In Proc. of In-
ternational Conference on Computer Vision (ICCV) , pages
12525–12536, 2023. 2,6,7
[36] Fuqiang Zhao, Yuheng Jiang, Kaixin Yao, Jiakai Zhang, Liao
Wang, Haizhao Dai, Yuhui Zhong, Yingliang Zhang, Minye
Wu, Lan Xu, and Jingyi Yu. Human performance model-
ing and rendering via neural animated mesh. ACM Trans.
Graph. , 41(6), 2022. 2,3,4
20590
