NViST: In the Wild New View Synthesis from a Single Image with Transformers
Wonbong Jang Lourdes Agapito
Department of Computer Science
University College London
{ucabwja,l.agapito }@ucl.ac.uk
Figure 1. We introduce NViST, a transformer-based architecture that enables synthesis from novel viewpoints given a single in the wild
input image. We test our model not only on held-out scenes of MVImgNet, a large-scale dataset of casually captured videos of hundreds
of object categories (Right) but also on out-of-distribution challenging phone-captured scenes (Left).
Abstract
We propose NViST, a transformer-based model for effi-
cient and generalizable novel-view synthesis from a single
image for real-world scenes. In contrast to many meth-
ods that are trained on synthetic data, object-centred sce-
narios, or in a category-specific manner, NViST is trained
on MVImgNet, a large-scale dataset of casually-captured
real-world videos of hundreds of object categories with di-
verse backgrounds. NViST transforms image inputs directly
into a radiance field, conditioned on camera parameters via
adaptive layer normalisation. In practice, NViST exploits
fine-tuned masked autoencoder (MAE) features and trans-
lates them to 3D output tokens via cross-attention, while
addressing occlusions with self-attention. To move away
from object-centred datasets and enable full scene synthe-
sis, NViST adopts a 6-DOF camera pose model and only re-
quires relative pose, dropping the need for canonicalizationof the training data, which removes a substantial barrier
to it being used on casually captured datasets. We show
results on unseen objects and categories from MVImgNet
and even generalization to casual phone captures. We con-
duct qualitative and quantitative evaluations on MVImgNet
and ShapeNet to show that our model represents a step
forward towards enabling true in-the-wild generalizable
novel-view synthesis from a single image. Project webpage:
https://wbjang.github.io/nvist_webpage .
1. Introduction
Learning 3D scene representations from RGB images for
novel view synthesis or 3D modelling remains a pivotal
challenge for the computer vision and graphics communi-
ties. Traditional approaches, such as structure from mo-
tion (SfM) and multiview stereo pipelines, endeavor to op-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10181
timize the 3D scene from RGB images directly by lever-
aging geometric and photometric consistency. The advent
of Neural Radiance Fields (NeRF) [51] and its subsequent
developments has marked a significant step forward by en-
coding the entire 3D scene within the weights of a neu-
ral network (or feature grid), from RGB images only. Al-
though NeRF requires more than dozens of images for
training, efforts have been undertaken to generalize NeRF
across multiple scenes by taking a single image as an in-
put [15, 30, 35, 45, 53, 62, 90, 98]. Nonetheless, gener-
alizing NeRF-based models to multiple real-world scenes
remains a challenge due to scale ambiguities, scene mis-
alignments and diverse backgrounds. The huge success of
2D latent diffusion models [65] has sparked interest in 3D
diffusion models. One trend is to make diffusion models
3D-aware by fine-tuning a pre-trained 2D diffusion model.
However, so far these approaches are trained on centered
objects, masked inputs, do not deal with backgrounds, and
assume a simplified camera model (3-DOF) [47, 48, 60, 75].
Other approaches [13, 17, 27, 83] build the diffusion model
on top of volume rendering in 3D space, but they are com-
putationally expensive and slow to sample from.
Many recent breakthroughs in computer vision can be at-
tributed to the emergence of very large datasets paired with
the transformer architecture. For instance, MiDAS [61] ex-
ploits multiple diverse datasets, leading to robust perfor-
mance in zero-shot depth estimation from a single RGB im-
age while SAM [41] demonstrates that an extensive dataset
coupled with a pretrained MAE and prompt engineering can
significantly improve performance in segmentation. How-
ever, in 3D computer vision, scaling up real-world datasets
has not been as straightforward. Synthetic datasets like
ShapeNet [14] or Objaverse [21] have helped to promote
progress, but there is a large domain gap.
The recent release of real-world large-scale multiview
datasets such as Co3D [64] or MVImgNet [99], cou-
pled with the availability of robust SfM tools such as
COLMAP [70, 71] or ORB-SLAM [55] to enable camera
pose estimation, has opened the door to large-scale training
for new view synthesis. However, significant challenges re-
main to train a scene-level new view synthesis model on
such real-world, large scale datasets due to the huge di-
versity of objects, categories, scene scales, backgrounds,
and scene alignment issues. Motivated by this we pro-
pose NViST, a transformer-based architecture trained on a
large-scale dataset to enable in-the-wild new view synthe-
sis (NVS) from a single image. We exploit a subset of
MVImgNet [99] which has one order of magnitude more
categories and ×2 more objects than Co3D [64]. Our con-
tributions can be summed up as follows:
• NViST can model general real-world scenes, including
backgrounds, only requiring relative pose during training.
• Our novel decoder maps MAE features to 3D output to-kens via cross-attention, conditions on camera parameters
via adaptive layer normalisation and addresses occlusions
with self-attention.
• Our qualitative and quantitative evaluations on
MVImgNet test sequences show good performance
on challenging real-world scenes.
• We demonstrate good generalization results on a zero-
shot new-view synthesis task, on phone-captured scenes.
2. Related Work
Transformer, ViT and MAE: The transformer [85], a feed-
forward, scalable, attention-based architecture, has brought
a revolution to the field of natural language understanding.
Inspired by it, the vision transformer (ViT) [23] uses image
patches as tokens to achieve performance levels comparable
to those of CNNs [28, 77] in many computer vision tasks.
While the ViT is trained in a supervised way, masked au-
toencoders (MAE) [29] can be trained in a self-supervised
way by randomly masking and in-painting patches, and be
further fine-tuned on specific tasks.
Neural Implicit Representations: Neural implicit rep-
resentations aim to learn a 3D representation without di-
rect 3D supervision using neural networks. They have
been employed for various tasks, including depth predic-
tion [56] and scene representation through ray marching
and coordinate-based MLPs [78]. Mildenhall et al. [51]
proposed Neural Radiance Fields (NeRF), which integrates
coordinate-based MLPs, positional encoding, and volume
rendering to encode a scene in the weights of neural net-
work. Upon optimisation, novel views can be rendered
with impressively high quality. Beyond novel view syn-
thesis, NeRF has found utility in a diverse range of com-
puter vision tasks such as segmentation [10, 25, 86, 101],
surface reconstruction [7, 52, 57, 88, 89], and camera regis-
tration [18, 20, 36, 44, 46, 49, 91, 95].
Grid-based representations: A limitation of the original
NeRF method is its lengthy training time. As an alterna-
tive to coordinate-based MLPs, grid-based approaches [16,
24, 54, 80, 96, 97] have been proposed to expedite training.
TensoRF [16] proposed the vector-matrix (VM) represen-
tation as an efficient and compact way to represent the 3D
grid. In the context of 3D-aware Generative Adversarial
Networks (GANs), EG3D [12] introduced triplanes by pro-
jecting 3D features into three different planes. Several ap-
proaches use the triplane representation to learn to generate
3d implicit representations from ImageNet [63, 69, 73, 79].
Learning multiple scenes using NeRF: Generalising
NeRFs to multiple scenes remains a challenging prob-
lem. Several methods associate 2D features with the target
views [15, 19, 30, 34, 64, 84, 90, 98], while others condi-
tion the network on latent vectors with a shared MLP across
the dataset [3, 26, 31, 35, 53, 62]. Adding an adversarial
loss to NeRF leads to 3D-aware GANs, which allow consis-
10182
Figure 2. Architecture. NViST is a feed-forward transformer-based model that takes a single in-the-wild image as input, and renders a
novel view. The encoder, a finetuned Masked Autoencoder (MAE), generates feature tokens, which are translated to output tokens via
cross-attention by our novel decoder, conditioned on normalised focal length and camera distance via adaptive layer normalisation. Self-
attention blocks allow reasoning about occlusions. Output tokens are reshaped into a vector-matrix representation that is used for volume
rendering. NViST is trained end-to-end via a balance of losses: photometric L2, perceptual LLPIPS, and a distortion-based regulariser Lreg.
tent rendering from different viewpoints [8, 11, 12, 43, 72].
Most approaches that use a single input image require
aligned datasets like ShapeNet [14, 78] or FFHQ [39], and
fail to deal with scale ambiguities or diverse backgrounds.
Diffusion for NVS: Latent diffusion [65] and its open-
source release Stable Diffusion have transformed the field
of image generation. However, applying diffusion models
to learn 3D implicit representations is not straightforward as
there is no access to 3D ground-truth. 3DiM [93] proposes
a pose-conditioned image-to-image approach. Dreamfu-
sion [59] introduced score distillation sampling (SDS) to
train NeRF with a 2D diffusion model. SDS has been used
in many follow-up works [50, 76, 82, 87, 92]. Fine-tuning
Stable Diffusion on a large-scale synthetic dataset [21] al-
lows diffusion models to be 3D-aware such as Zero-1-to-3
and its follow-ups [47, 48, 60, 75]. Other approaches de-
noise directly in 3D and supervise the model in 2D space
after rendering, [1, 2, 13, 27, 38, 81, 83], use the 2D dif-
fusion model as a prior [22], or optimise NeRF jointly and
regard it as the ground truth [17]. Similar to latent diffu-
sion [5, 40, 73] adopt 2-stage training.
3D Representation with Transformers: Geometry-free
methods employing transformer architectures have been ex-
plored as seen in [42, 66–68]. Others build NeRF rep-
resentations using transformers [37, 45, 69, 74, 94]. The
concurrent work LRM [32] extracts image features through
DINO [9], and refines learnable positional embeddings via
attention mechanisms. Unlike NViST, LRM focuses on
object-centric scenes without background and employs tri-
planes instead of vector-matrix representation.
3. Methodology
NViST is a feed-forward conditional encoder-decoder
model built upon transformers to predict a radiance field
from a single image to enable synthesis from novel view-points. As Figure 2 shows, our architecture is structured
into three key components: two transformer-based modules
(encoder and decoder) and a NeRF renderer, which is com-
posed of a shallow multi-layer perceptron (MLP) and a dif-
ferentiable volume rendering module.
3.1. Encoder
Input images are first split into a set of fixed size non-
overlapping patches before being fed to the encoder, which
predicts feature tokens using a ViT-B transformer architec-
ture. We formulate the encoder EasF, C =E(I)where I
are input images and FandCare the feature and class to-
kens respectively. In practice, we use the encoder of a pre-
trained MAE [29], a self-supervised vision learner trained
to predict masked image patches, which we further finetune
on the training dataset to adapt to the different image reso-
lution from ImageNet, on which MAE [29] is trained. As
illustrated in Figure 3, we find that the self-supervised fea-
tures learnt by the MAE [29] encapsulate a powerful inter-
mediate representation of the scene’s geometric and appear-
ance properties that can be leveraged by the decoder to pre-
dict the radiance field. Note that the weights of the encoder
are continuously updated during end-to-end training, since
we found that this enhances the encoder’s ability to gener-
ate smoother and more segment-focused features, as shown
in Figure 3, and better performance as shown in Table 2.
3.2. Decoder
The goal of the decoder Dis to take the class and feature
tokens, CandF, predicted by the encoder along with the
camera parameters used as conditioning (normalized focal
length fand camera distance z) and predict the radiance
field, which we encode using a vector-matrix (VM) repre-
sentation, parameterized with three matrices Mand three
vectors V. A key feature of our decoder is the use of cross-
10183
Figure 3. Encoder Output Visualisation: (Top) Input images.
(Middle) Features from a fine-tuned MAE, which serve as initial-
isation to our encoder. (Bottom) Features after end-to-end train-
ing. Features shown after reducing to 3dimensions with PCA.
Optimised features appear smoother and more segment-focused,
supporting the fact that updating encoder weights significantly im-
proves the performance (see also ablation in Table 2).
attention and self-attention mechanisms. Unlike previous
models [98] which project spatially aligned ResNet [28]
features onto target rays, our decoder learns this 2D to 3D
correspondence by learning to associate feature tokens with
the relevant output tokens through cross-attention.
The decoder Dhas output tokens O, which are learnable
parameters, initialised following a random normal distribu-
tion. For simplicity, from here on we will use the term out-
put tokens Oto refer to the concatenation of output tokens
and class token C, to which we further add positional em-
beddings [85].
M, V =D(F, C, O, f, z ). (1)
The decoder applies cross-attention between feature tokens
Fand output tokens O, and self-attention amongst output
tokens O. The attention mechanism allows the network to
reason about occlusions, where the class token Cacts as
a global latent vector. For each attention block, we apply
adaptive layer normalisation [33, 58], instead of standard
layer normalisation, to condition on camera parameters.3.2.1 Relative Camera Pose
A key strength of our approach is that it does not require all
objects/scenes in the dataset to be brought into alignment by
expressing their pose with respect to a canonical reference
frame. Instead, we assume the rotation of the camera asso-
ciated with the input image Iito be the identity and we ex-
press the rotation of any other image Ijas a relative rotation
Ri→j. We assume that the camera location of the input im-
age is Ti→i= (0,0, z), where zis the normalized distance
of the input camera from the origin of the world coordinate
frame, located at the centroid of the 3D bounding box con-
taining the point cloud, and Ti→j=RiT(Tj−Ti). Zero-1-
to-3 [48] also adopts a relative-pose based approach, how-
ever it assumes objects are located in the centre and uses a
3-DoF camera pose model (radius, elevation and azimuth),
while NViST uses a full 6-DoF model for the camera pose.
3.2.2 Conditioning on Camera Parameters
There are several ways to apply conditioning such as con-
catenating camera parameters to output tokens Oor feature
tokens F. However, in NViST we apply adaptive layer nor-
malization, as the camera parameters influence the overall
scale of the scene. Conditioning camera parameters im-
proves the model performance as seen in Table 2.
Positional encoding: We apply the positional encoding
from NeRF [51] for fandz, concatenating up to 4-th sine
and cosine embeddings with the original values zandfas
M=⊕4
k=1(sin 2k(f),cos 2k(f),sin 2k(z),cos 2k(z)).
Adaptive Layer Normalisation: We employ an additional
MLPAto regress the shift δ, scale αand gate scale γfrom
the conditioning inputs (z, f, M )for each attention block.
α, δ, γ =A(z, f, M ) (2)
An alternative to adaptive layer normalisation would be
to concatenate Mto output tokens O, but as seen in our ab-
lation (Table 2) this strategy does not lead to better results.
3.2.3 Attention Blocks
NViST uses self-attention blocks between output tokens O
and cross-attention blocks between output tokens Oand
feature tokens F. The embedding dimension for both O
andFise. While standard layer normalization is applied
to feature tokens F, we apply adaptive layer normalization
to output tokens Ousing the shift δand scale αvalues re-
gressed by A, the MLP described in Equation 2.
On=δ+α×Norm (O)
Fn=Layer Norm (F).(3)
Cross-attention can then be expressed as
Attn =Softmax (OnFT
n√e). (4)
10184
Figure 4. Qualitative Results on Test (Unseen) Scenes: We show the capabilities of NViST to synthesize novel views of unknown scenes.
The model correctly synthesizes images from different viewpoints of various categories with diverse backgrounds and scales.
Finally, output tokens are updated via residual connection
O←O+γ×Attn·Fn, where γis the gate scale also
regressed by A. Note that self-attention is obtained in an
equivalent way, just between output tokens.
Reshaping: We use MLPs to reshape the output tokens into
the vector-matrix representation that encodes the radiance
field, adapting their respective dimensionalities. This is fol-
lowed by unpatchifying into the 3matrices and 3vectors
that form the VM representation. Please refer to the supple-
mentary material for more details.
3.3. Rendering
The VM representation of the radiance field predicted by
the decoder is used to query 3D point features which are
then decoded by a multi-layer perception into color cand
density σand finally rendered via volumetric rendering.
Vector-Matrix Representation: For a compact yet ex-
pressive representation of the radiance field, we adopt the
vector-matrix decomposition proposed by TensoRF [16]
which expresses each voxel as the sum of three vectors and
matrices, one pair per axis. Specifically, a 3D feature grid
(T), is decomposed into three vectors ( VX
r1, VY
r2, VZ
r3) and
three matrices ( MY,Z
r1, MZ,X
r2, MX,Y
r3), each pair sharing thesame channel dimensions ( k) such that
T=kX
r1=1VX
r1◦MY,Z
r1+kX
r2=1VY
r2◦MZ,X
r2+kX
r3=1VZ
r3◦MX,Y
r3.
(5)
While the value of density σis obtained by applying ReLU
activation directly to the feature value Txat point x, the
colour cis predicted with a shallow MLP, conditioned on
the viewing direction. The VM representation outperforms
using a triplane as shown in our ablation study (Table 2).
Volume Rendering: For each sampled ray r, we obtain
its final color ˆC(r)∈R3using volumetric rendering,
following the methodology of NeRF [51]. The transmit-
tance Tiis first computed at each point xalong the ray as
Ti= exp( −Pi−1
j=1σjδj), where δiis the distance between
adjacent points δi=ti+1−ti. The pixel color is calcu-
lated by integrating the predicted color at each i-th point ci,
weighted by light absorbance Ti-Ti+1.
ˆC(r) =NX
i=1Ti(1−exp(−σiδi))ci (6)
3.4. Training Losses
We employ a combination of losses to train our architec-
ture in an end-to-end manner including the L2 photometric
10185
rendering loss, a Learned Perceptual Image Patch Similar-
ity (LPIPS) loss [100], and the distortion-based regulariser
proposed by [4]. Given ground-truth pixel colors v, esti-
mates ˆ vand accumulated transmittance values walong the
points xon the ray, our overall loss function is defined as
L=L2(ˆ v,v) +λLLPIPS(ˆ v,v) +βLdist(w,x).(7)
For MVImgNet λ= 0.1andβ= 0.01. We found LPIPS to
be extremely effective on real world datasets (see Table 2).
4. Experimental Evaluation
4.1. MVImgNet
Train/Test Split: MVImgNet contains videos of over 6.5
million real-world scenes across 238categories. Our train-
ing set, contains a subset of 1.14M frames across 38K
scenes of 177categories. For the test set, we hold out every
100thscene from each category to a total of 13,228frames,
from 447scenes and 177categories.
Pre-processing: MVImgNet uses COLMAP to estimate
camera matrices and generate 3D point clouds. Since each
scene has its own scale from SfM, we rescale point clouds
to a unit cube, such that the maximum distance along one
axis equals 1. Then, we center the point clouds in the world
coordinate system. We downsample the original images by
×12while preserving their aspect ratio. Camera intrinsics
are recalibrated accordingly.
Implementation Details: NViST has approximately 216M
parameters: 85M for the encoder, 131M for the decoder,
and7K for the renderer. The encoder takes images of size
160×90, using a patch size of 5, so the total number of fea-
ture tokens is 576. The resolution of the VM representation
is48. The patch size for the decoder is 3, the total number
of output tokens 816, and 16heads. The embedding dimen-
sion is 768for both encoder and decoder, and we sample 48
points along the ray. We train NViST with 2 ×A100-40GB
GPUs for approximately one week, using a batch size of 22
images and rendering 330K pixels, up to one million iter-
ations. The initial learning rates are 6e-5 for encoder, and
4e-4 for decoder and renderer and we decay them following
the half-cycle cosine schedule.
4.1.1 Qualitative Results
Results on Test (Unseen) Scenes: As depicted in Figure
4, our model demonstrates its capability to synthesise new
views of unknown scenes. Figure 4 highlights the model’s
ability to handle objects from diverse categories, such as
flowers, bags, helmets and others. We show that NViST can
deal with a variety of backgrounds, such as tables, wooden
floors, textureless walls or more cluttered environments.
Results on Unseen Category: We take a held-out category
(toy-cars), and test the ability of our model to generalize to
Figure 5. Results on Unseen Category: This figure shows how
the model generalises to a novel category unseen at training. We
validate our model with a held-out category (toy-cars).
Figure 6. Casual Phone Captures: We demonstrate NViST in
OOD scenarios. First row: the model can capture different cate-
gories. Second row: outdoor setting. Third row: many objects.
categories unseen at training. Figure 5 shows that NViST
can synthesize new views of objects from a category not
seen at training time given a single input image.
Casual Phone Captures: Our motivation for this paper was
to train a feed-forward model that we could easily use on ca-
sually captured images, for instance acquired with our own
mobile devices. We test the ability of NViST to deal with
10186
Figure 7. Depth Estimation: Examples of the depth estimates
on test images. Although NViST focuses on novel view synthesis
and is trained with RGB losses only, depth estimation is consistent
and finds good object boundaries. We show a comparison with
the state-of-the-art disparity estimator MiDAS v3.1 with Swin2-
L384 [6]. We provide MiDAS the GT new view images as input,
as it cannot do novel view synthesis. NViST performs well even
though it is not trained with depth supervision, unlike MiDAS.
out-of-distribution scenes/images by performing zero-shot
new-view synthesis on scenes captured by us on a mobile-
phone device. We pre-process the images in the same man-
ner as MVImgNet, using COLMAP to estimate the focal
length and camera distance parameters to be used as con-
ditioning. Figure 6 shows results on out-of-distribution
scenes. The top row highlights the model’s ability to pro-
cess a scene with multiple objects from diverse categories.
The second row reveals its competence on outdoor scenes,
despite their limited presence in the training set. The third
row illustrates NViST’s ability to learn scenes with a large
number of objects. Figure 1 shows two further examples of
results on phone captures.
Depth Estimation: Figure 7 shows qualitative results of
the depth predicted by NViST. Since there is no ground-
truth depth for these images, we qualitatively compare
NViST with the recent MiDASv3.1 with Swin2-L384 [6].
As MiDAS predicts depth from RGB images, we provide
the GT novel view image as input. This shows that depth
estimation with NViST performs well even though it is not
trained with depth supervision, while MiDAS uses direct
depth supervision from multiple datasets.
Figure 8. Qualitative Comparison with PixelNeRF and Vision-
NeRF: NViST displays better performance than PixelNeRF [98]
and VisionNeRF [45], especially when the target view is far away
from the input view.
PSNR↑SSIM↑LPIPS↓
PixelNeRF [98] 17.02 0.41 0.54
VisionNeRF [45] 19.82 0.51 0.47
Ours 20.83 0.57 0.29
Table 1. Quantitative Tests on MVImgNet: We compare NViST
with PixelNeRF and VisionNeRF on new view synthesis from sin-
gle in-the-wild images. NViST outperforms both on all metrics.
4.1.2 Comparisons with baseline models
We compare NViST with PixelNeRF [98] and Vision-
NeRF [45], all trained on MVImgNet using their offi-
cial code releases and applying the same pre-processing
as NViST. Figure 8 and Table 1 show that NViST out-
performs both models by a large margin. Both models
rely on aligned features, but have limitations dealing with
occlusion. We qualitatively compare with the pre-trained
Zero-1-to-3 [48] on a phone capture in Figure 9, using
the approximate camera pose as it assumes a centered ob-
ject. Since Zero-1-to-3 and its follow-up works assume a
3DoF camera and do not model the background, we could
not conduct quantitative comparisons. We could not con-
duct quantitative comparisons with generative models such
as GenVS [13] or with the large-scale model LRM [32] as
their models are not publicly available. Moreover, GenVS
is category-specific and LRM does not model backgrounds.
10187
Figure 9. Qualitative comparison with Zero-1-to-3 [48] on
a phone captured input image: Zero-1-to-3 shows result
w/masked input, Zero1to3∗shows result w/full image input. None
result in good novel-view synthesis, except NViST.
PSNR↑SSIM↑LPIPS↓
Ours 20.83 0.57 0.29
w/o LPIPS 20.72 0.54 0.46
w/o Camera Conditioning 20.24 0.49 0.36
Concat Camera Parameters(1)20.81 0.57 0.30
w/ Self-Attention Decoder(2)20.74 0.57 0.31
w/o VM Representation(3)19.60 0.49 0.44
w/o Updating Encoder 18.54 0.47 0.49
Table 2. Ablation: For (1) we concatenate high dimensional cam-
era feature tokens to output tokens instead of adaptive layer nor-
malisation. For (2) we update all tokens via self-attn (no cross-
attn). For (3), we use triplane instead of VM as the representation.
4.1.3 Ablation Study
We conducted an ablation study to analyse the effect of de-
sign choices on the performance of NViST as summarised
in Table 2. While the perceptual LPIPS loss [100] is not
commonly employed in NeRF-based methods, it appears
to play a crucial role in improving quality. Conditioning
on normalized focal length and camera distance helps the
model deal with scale ambiguities, and adaptive layer nor-
malisation performs better than concatenating camera pa-
rameters to output tokens. Instead of employing cross-
attention in Figure 2, we can concatenate feature tokens
and output tokens and update both of them using self-
attention. However, it increases memory consumption and
does not lead to a better result in Table 2. While project-
ing features onto triplanes has been extensively used be-
fore [12, 32, 43, 69], our experiments show that the use of
a vector-matrix (VM) representation [16] improves perfor-
mance. Note that for the triplane representation, we use a
1-layer MLP to regress occupancy, while occupancy is di-
rectly calculated using Equation 5 in our VM representa-
tion. Updating encoder weights also improves the perfor-
mance as the encoder output is used for cross-attention.Cars Chairs
PSNR↑LPIPS↓PSNR↑LPIPS↓
PixelNeRF [98] 23.17 0.146 23.72 0.128
VisionNeRF [45] 22.88 0.084 24.48 0.077
VD [81] 23.29 0.094 - -
SSDNeRF [17] 23.52 0.078 24.35 0.067
Ours 23.91 0.122 24.50 0.090
Table 3. Quantitative Evaluation on ShapeNet-SRN: NViST
performs similarly to other baselines on ShapeNet Cars/Chairs,
although our method only requires relative pose. For qualitative
comparisons, see supplementary materials.
4.2. ShapeNet-SRN
ShapeNet-SRN [78] has two categories (cars and chairs)
and is a widely used benchmark to compare models that
perform novel view synthesis from a single input image.
Since all objects are aligned and there is no scale ambi-
guity, pre-processing is not needed and we do not use the
LPIPS loss as it is a synthetic dataset. Table 3 shows that
NViST performs similarly to baseline models on ShapeNet-
SRN dataset. Although other models apply absolute pose,
we only employ relative pose, which means we do not fully
exploit the alignment of objects in ShapeNet-SRN.
5. Conclusion
We have introduced NViST, a transformer-based scalable
model for new view synthesis from a single in-the-wild im-
age. Our evaluations demonstrate robust performance on
the MVImgNet test set, novel category synthesis and phone
captures of out-of-distribution scenes. Our design choices
were validated via ablations and a quantitative comparison
was conducted on MVImgNet and ShapeNet-SRN. Inter-
esting future directions include extending NViST to adopt a
probabilistic approach and to multiview inputs.
Limitations: Some loss of sharpness could be due to our
computational constraints, which led us to downsample im-
ages by ×12and train on a fraction of the original dataset.
We pushed for a transformer-based architecture, without
GAN losses or SDS [59], which eased and sped up train-
ing, but may have also contributed to some loss of detail.
Acknowledgements: Our research has been partly sup-
ported by a sponsored research award from Cisco Re-
search and has made use of time on HPC Tier 2 facilities
Baskerville (funded by EPSRC EP/T022221/1 and oper-
ated by ARC at the University of Birmingham) and JADE2
(funded by EPSRC EP/T022205/1). We are grateful to
N. Mitra and D. Stoyanov for fruitful discussions.
10188
References
[1] Titas Anciukevi ˇcius, Zexiang Xu, Matthew Fisher, Paul
Henderson, Hakan Bilen, Niloy J Mitra, and Paul Guerrero.
Renderdiffusion: Image diffusion for 3d reconstruction, in-
painting and generation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 12608–12618, 2023. 3
[2] Titas Anciukevi ˇcius, Fabian Manhardt, Federico Tombari,
and Paul Henderson. Denoising diffusion via image-based
rendering. In International Conference on Learning Repre-
sentations (ICLR) , 2024. 3
[3] ShahRukh Athar, Zexiang Xu, Kalyan Sunkavalli, Eli
Shechtman, and Zhixin Shu. Rignerf: Fully controllable
neural 3d portraits. In Computer Vision and Pattern Recog-
nition (CVPR) , 2022. 2
[4] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P
Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded
anti-aliased neural radiance fields. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5470–5479, 2022. 6
[5] Miguel Angel Bautista, Pengsheng Guo, Samira Abnar,
Walter Talbott, Alexander Toshev, Zhuoyuan Chen, Laurent
Dinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, Afshin
Dehghan, and Josh Susskind. Gaudi: A neural architect for
immersive 3d scene generation. arXiv , 2022. 3
[6] Reiner Birkl, Diana Wofk, and Matthias M ¨uller. Midas v3.1
– a model zoo for robust monocular relative depth estima-
tion. arXiv preprint arXiv:2307.14460 , 2023. 7
[7] Hongrui Cai, Wanquan Feng, Xuetao Feng, Yan Wang,
and Juyong Zhang. Neural surface reconstruction of dy-
namic scenes with monocular rgb-d camera. In Thirty-
sixth Conference on Neural Information Processing Sys-
tems (NeurIPS) , 2022. 2
[8] Shengqu Cai, Anton Obukhov, Dengxin Dai, and Luc
Van Gool. Pix2nerf: Unsupervised conditional p-gan for
single image to neural radiance fields translation. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 3981–3990, 2022. 3
[9] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin.
Emerging properties in self-supervised vision transformers.
InProceedings of the International Conference on Com-
puter Vision (ICCV) , 2021. 3
[10] Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Chen Yang, Wei
Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian. Segment
anything in 3d with nerfs. In NeurIPS , 2023. 2
[11] Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,
and Gordon Wetzstein. pi-gan: Periodic implicit genera-
tive adversarial networks for 3d-aware image synthesis. In
arXiv , 2020. 3
[12] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki
Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,
Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis,
et al. Efficient geometry-aware 3d generative adversarial
networks. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 16123–
16133, 2022. 2, 3, 8[13] Eric R Chan, Koki Nagano, Matthew A Chan, Alexan-
der W Bergman, Jeong Joon Park, Axel Levy, Miika Ait-
tala, Shalini De Mello, Tero Karras, and Gordon Wetzstein.
Generative novel view synthesis with 3d-aware diffusion
models. arXiv preprint arXiv:2304.02602 , 2023. 2, 3, 7
[14] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,
Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,
Manolis Savva, Shuran Song, Hao Su, et al. Shapenet:
An information-rich 3d model repository. arXiv preprint
arXiv:1512.03012 , 2015. 2, 3
[15] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,
Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast
generalizable radiance field reconstruction from multi-view
stereo. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 14124–14133, 2021. 2
[16] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and
Hao Su. Tensorf: Tensorial radiance fields. In Computer
Vision–ECCV 2022: 17th European Conference, Tel Aviv,
Israel, October 23–27, 2022, Proceedings, Part XXXII ,
pages 333–350. Springer, 2022. 2, 5, 8
[17] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian,
Zhuowen Tu, Lingjie Liu, and Hao Su. Single-stage dif-
fusion nerf: A unified approach to 3d generation and re-
construction. arXiv preprint arXiv:2304.06714 , 2023. 2, 3,
8
[18] Yue Chen, Xingyu Chen, Xuan Wang, Qi Zhang, Yu Guo,
Ying Shan, and Fei Wang. Local-to-global registration
for bundle-adjusting neural radiance fields. arXiv preprint
arXiv:2211.11505 , 2022. 2
[19] Yuedong Chen, Haofei Xu, Qianyi Wu, Chuanxia Zheng,
Tat-Jen Cham, and Jianfei Cai. Explicit correspondence
matching for generalizable neural radiance fields. arXiv
preprint arXiv:2304.12294 , 2023. 2
[20] Shin-Fang Chng, Sameera Ramasinghe, Jamie Sherrah, and
Simon Lucey. Gaussian activated neural radiance fields
for high fidelity reconstruction and pose estimation. In
Computer Vision–ECCV 2022: 17th European Conference,
Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part
XXXIII , pages 264–280. Springer, 2022. 2
[21] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,
Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana
Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:
A universe of annotated 3d objects. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 13142–13153, 2023. 2, 3
[22] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan,
Yin Zhou, Leonidas Guibas, Dragomir Anguelov, et al.
Nerdi: Single-view nerf synthesis with language-guided
diffusion as general image priors. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 20637–20647, 2023. 3
[23] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold,
Sylvain Gelly, et al. An image is worth 16x16 words:
Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 2
10189
[24] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk
Warburg, Benjamin Recht, and Angjoo Kanazawa. K-
planes: Explicit radiance fields in space, time, and appear-
ance. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 12479–12488,
2023. 2
[25] Xiao Fu, Shangzhan Zhang, Tianrun Chen, Yichong Lu,
Lanyun Zhu, Xiaowei Zhou, Andreas Geiger, and Yiyi
Liao. Panoptic nerf: 3d-to-2d label transfer for panoptic ur-
ban scene segmentation. arXiv preprint arXiv:2203.15224 ,
2022. 2
[26] Guy Gafni, Justus Thies, Michael Zollh ¨ofer, and Matthias
Nießner. Dynamic neural radiance fields for monocu-
lar 4d facial avatar reconstruction. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 8649–8658, 2021. 2
[27] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M
Susskind, Christian Theobalt, Lingjie Liu, and Ravi Ra-
mamoorthi. Nerfdiff: Single-image view synthesis with
nerf-guided distillation from 3d-aware diffusion. In Inter-
national Conference on Machine Learning , pages 11808–
11826. PMLR, 2023. 2, 3
[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 2, 4
[29] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scal-
able vision learners. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
16000–16009, 2022. 2, 3
[30] Philipp Henzler, Jeremy Reizenstein, Patrick Labatut, Ro-
man Shapovalov, Tobias Ritschel, Andrea Vedaldi, and
David Novotny. Unsupervised learning of 3d object cat-
egories from videos in the wild. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 4700–4709, 2021. 2
[31] Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, and Juyong
Zhang. Headnerf: A real-time nerf-based parametric head
model. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 20374–
20384, 2022. 2
[32] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou,
Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and
Hao Tan. Lrm: Large reconstruction model for single image
to 3d. arXiv preprint arXiv:2311.04400 , 2023. 3, 7, 8
[33] Xun Huang and Serge Belongie. Arbitrary style transfer
in real-time with adaptive instance normalization. In Pro-
ceedings of the IEEE international conference on computer
vision , pages 1501–1510, 2017. 4
[34] Muhammad Zubair Irshad, Sergey Zakharov, Katherine
Liu, Vitor Guizilini, Thomas Kollar, Adrien Gaidon, Zsolt
Kira, and Rares Ambrus. Neo 360: Neural fields for sparse
view synthesis of outdoor scenes. 2023. 2
[35] Wonbong Jang and Lourdes Agapito. Codenerf: Disen-
tangled neural radiance fields for object categories. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 12949–12958, 2021. 2[36] Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Anima
Anandkumar, Minsu Cho, and Jaesik Park. Self-calibrating
neural radiance fields. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 5846–
5854, 2021. 2
[37] Heewoo Jun and Alex Nichol. Shap-e: Generat-
ing conditional 3d implicit functions. arXiv preprint
arXiv:2305.02463 , 2023. 3
[38] Animesh Karnewar, Andrea Vedaldi, David Novotny, and
Niloy J Mitra. Holodiffusion: Training a 3d diffusion model
using 2d images. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages
18423–18433, 2023. 3
[39] Tero Karras, S. Laine, and Timo Aila. A style-based gener-
ator architecture for generative adversarial networks. 2019
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 4396–4405, 2019. 3
[40] Seung Wook Kim, Bradley Brown, Kangxue Yin, Karsten
Kreis, Katja Schwarz, Daiqing Li, Robin Rombach, Anto-
nio Torralba, and Sanja Fidler. Neuralfield-ldm: Scene gen-
eration with hierarchical latent diffusion models. In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR) , 2023. 3
[41] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi
Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer
Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment
anything. arXiv preprint arXiv:2304.02643 , 2023. 2
[42] Jon ´aˇs Kulh ´anek, Erik Derner, Torsten Sattler, and Robert
Babu ˇska. Viewformer: Nerf-free neural rendering from
few images using transformers. In Computer Vision–ECCV
2022: 17th European Conference, Tel Aviv, Israel, Octo-
ber 23–27, 2022, Proceedings, Part XV , pages 198–216.
Springer, 2022. 3
[43] Eric-Tuan Le, Edward Bartrum, and Iasonas Kokkinos.
Stylemorph: Disentangled 3d-aware image synthesis with
a 3d morphable styleGAN. In The Eleventh International
Conference on Learning Representations , 2023. 3, 8
[44] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Si-
mon Lucey. Barf: Bundle-adjusting neural radiance fields.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 5741–5751, 2021. 2
[45] Kai-En Lin, Lin Yen-Chen, Wei-Sheng Lai, Tsung-Yi Lin,
Yi-Chang Shih, and Ravi Ramamoorthi. Vision transformer
for nerf-based view synthesis from a single input image. In
WACV , 2023. 2, 3, 7, 8
[46] Yunzhi Lin, Thomas M ¨uller, Jonathan Tremblay, Bowen
Wen, Stephen Tyree, Alex Evans, Patricio A Vela, and Stan
Birchfield. Parallel inversion of neural radiance fields for
robust pose estimation. arXiv preprint arXiv:2210.10108 ,
2022. 2
[47] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen,
Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45:
Any single image to 3d mesh in 45 seconds without per-
shape optimization, 2023. 2, 3
[48] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-
makov, Sergey Zakharov, and Carl V ondrick. Zero-1-to-3:
Zero-shot one image to 3d object. In Proceedings of the
10190
IEEE/CVF International Conference on Computer Vision ,
pages 9298–9309, 2023. 2, 3, 4, 7, 8
[49] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu
Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Jo-
hannes Kopf, and Jia-Bin Huang. Robust dynamic radiance
fields. arXiv:2301.02239 , 2023. 2
[50] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and
Andrea Vedaldi. Realfusion: 360deg reconstruction of
any object from a single image. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8446–8455, 2023. 3
[51] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. In Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceed-
ings, Part I 16 , pages 405–421. Springer, 2020. 2, 4, 5
[52] Mirgahney Mohamed and Lourdes Agapito. Dynamicsurf:
Dynamic neural rgb-d surface reconstruction with an opti-
mizable feature grid, 2023. 2
[53] Norman M ¨uller, Andrea Simonelli, Lorenzo Porzi,
Samuel Rota Bul `o, Matthias Nießner, and Peter
Kontschieder. Autorf: Learning 3d object radiance
fields from single view observations. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3971–3980, 2022. 2
[54] Thomas M ¨uller, Alex Evans, Christoph Schied, and
Alexander Keller. Instant neural graphics primitives with
a multiresolution hash encoding. ACM Transactions on
Graphics (ToG) , 41(4):1–15, 2022. 2
[55] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D
Tardos. Orb-slam: a versatile and accurate monocular slam
system. IEEE transactions on robotics , 31(5):1147–1163,
2015. 2
[56] Michael Niemeyer, Lars Mescheder, Michael Oechsle,
and Andreas Geiger. Differentiable volumetric rendering:
Learning implicit 3d representations without 3d supervi-
sion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 3504–3515,
2020. 2
[57] Michael Oechsle, Songyou Peng, and Andreas Geiger.
Unisurf: Unifying neural implicit surfaces and radiance
fields for multi-view reconstruction. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 5589–5599, 2021. 2
[58] William Peebles and Saining Xie. Scalable diffusion mod-
els with transformers. arXiv preprint arXiv:2212.09748 ,
2022. 4
[59] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv
preprint arXiv:2209.14988 , 2022. 3, 8
[60] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,
Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-
rokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123:
One image to high-quality 3d object generation us-
ing both 2d and 3d diffusion priors. arXiv preprint
arXiv:2306.17843 , 2023. 2, 3[61] Ren ´e Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards robust monocu-
lar depth estimation: Mixing datasets for zero-shot cross-
dataset transfer. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 44(3), 2022. 2
[62] Daniel Rebain, Mark Matthews, Kwang Moo Yi, Dmitry
Lagun, and Andrea Tagliasacchi. Lolnerf: Learn from one
look. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 1558–1567,
2022. 2
[63] Pradyumna Reddy, Ismail Elezi, and Jiankang Deng. G3dr:
Generative 3d reconstruction in imagenet, 2024. 2
[64] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,
Luca Sbordone, Patrick Labatut, and David Novotny. Com-
mon objects in 3d: Large-scale learning and evaluation of
real-life 3d category reconstruction. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 10901–10911, 2021. 2
[65] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684–10695, 2022. 2, 3
[66] Mehdi SM Sajjadi, Henning Meyer, Etienne Pot, Urs
Bergmann, Klaus Greff, Noha Radwan, Suhani V ora, Mario
Luˇci´c, Daniel Duckworth, Alexey Dosovitskiy, et al. Scene
representation transformer: Geometry-free novel view syn-
thesis through set-latent scene representations. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 6229–6238, 2022. 3
[67] Mehdi S. M. Sajjadi, Daniel Duckworth, Aravindh Mahen-
dran, Sjoerd van Steenkiste, Filip Paveti ´c, Mario Lu ˇci´c,
Leonidas J. Guibas, Klaus Greff, and Thomas Kipf. Ob-
ject Scene Representation Transformer. NeurIPS , 2022.
[68] Mehdi S. M. Sajjadi, Aravindh Mahendran, Thomas Kipf,
Etienne Pot, Daniel Duckworth, Mario Lu ˇci´c, and Klaus
Greff. RUST: Latent Neural Scene Representations from
Unposed Imagery. CVPR , 2023. 3
[69] Kyle Sargent, Jing Yu Koh, Han Zhang, Huiwen Chang,
Charles Herrmann, Pratul Srinivasan, Jiajun Wu, and De-
qing Sun. Vq3d: Learning a 3d-aware generative model on
imagenet. arXiv preprint arXiv:2302.06833 , 2023. 2, 3, 8
[70] Johannes Lutz Sch ¨onberger and Jan-Michael Frahm.
Structure-from-motion revisited. In Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2016. 2
[71] Johannes Lutz Sch ¨onberger, Enliang Zheng, Marc Polle-
feys, and Jan-Michael Frahm. Pixelwise view selection for
unstructured multi-view stereo. In European Conference on
Computer Vision (ECCV) , 2016. 2
[72] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas
Geiger. Graf: Generative radiance fields for 3d-aware im-
age synthesis. Advances in Neural Information Processing
Systems , 33:20154–20166, 2020. 3
[73] Katja Schwarz, Seung Wook Kim, Jun Gao, Sanja Fidler,
Andreas Geiger, and Karsten Kreis. Wildfusion: Learning
3d-aware latent diffusion models in view space. In Inter-
national Conference on Learning Representations (ICLR) ,
2024. 2, 3
10191
[74] Bokui Shen, Xinchen Yan, Charles R Qi, Mahyar Najibi,
Boyang Deng, Leonidas Guibas, Yin Zhou, and Dragomir
Anguelov. Gina-3d: Learning to generate implicit neural
assets in the wild. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
4913–4926, 2023. 3
[75] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua
Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng,
and Hao Su. Zero123++: a single image to consistent multi-
view diffusion base model, 2023. 2, 3
[76] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie
Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d
generation. arXiv preprint arXiv:2308.16512 , 2023. 3
[77] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 , 2014. 2
[78] Vincent Sitzmann, Michael Zollh ¨ofer, and Gordon Wet-
zstein. Scene representation networks: Continuous 3d-
structure-aware neural scene representations. Advances in
Neural Information Processing Systems , 32, 2019. 2, 3, 8
[79] Ivan Skorokhodov, Aliaksandr Siarohin, Yinghao Xu, Jian
Ren, Hsin-Ying Lee, Peter Wonka, and Sergey Tulyakov.
3d generation on imagenet. In International Conference on
Learning Representations , 2023. 2
[80] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct
voxel grid optimization: Super-fast convergence for radi-
ance fields reconstruction. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 5459–5469, 2022. 2
[81] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea
Vedaldi. Viewset diffusion: (0-)image-conditioned 3D gen-
erative models from 2D data. In ICCV , 2023. 3, 8
[82] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and
Gang Zeng. Dreamgaussian: Generative gaussian splat-
ting for efficient 3d content creation. arXiv preprint
arXiv:2309.16653 , 2023. 3
[83] Ayush Tewari, Tianwei Yin, George Cazenavette, Se-
mon Rezchikov, Joshua B. Tenenbaum, Fr ´edo Durand,
William T. Freeman, and Vincent Sitzmann. Diffusion with
forward models: Solving stochastic inverse problems with-
out direct supervision. In arXiv , 2023. 2, 3
[84] Alex Trevithick and Bo Yang. Grf: Learning a general radi-
ance field for 3d representation and rendering. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 15182–15192, 2021. 2
[85] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. Advances
in neural information processing systems , 30, 2017. 2, 4
[86] Suhani V ora, Noha Radwan, Klaus Greff, Henning Meyer,
Kyle Genova, Mehdi SM Sajjadi, Etienne Pot, Andrea
Tagliasacchi, and Daniel Duckworth. Nesf: Neural se-
mantic fields for generalizable semantic segmentation of 3d
scenes. arXiv preprint arXiv:2111.13260 , 2021. 2
[87] Hengyi Wang, Jingwen Wang, and Lourdes Agapito.
Morpheus: Neural dynamic 360 {\deg}surface recon-
struction from monocular rgb-d video. arXiv preprint
arXiv:2312.00778 , 2023. 3[88] Jingwen Wang, Tymoteusz Bleja, and Lourdes Agapito.
Go-surf: Neural feature grid optimization for fast, high-
fidelity rgb-d surface reconstruction. In 2022 International
Conference on 3D Vision (3DV) , pages 433–442. IEEE,
2022. 2
[89] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt,
Taku Komura, and Wenping Wang. Neus: Learning neural
implicit surfaces by volume rendering for multi-view recon-
struction. arXiv preprint arXiv:2106.10689 , 2021. 2
[90] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P
Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo
Martin-Brualla, Noah Snavely, and Thomas Funkhouser.
Ibrnet: Learning multi-view image-based rendering. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 4690–4699, 2021. 2
[91] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen,
and Victor Adrian Prisacariu. Nerf–: Neural radiance
fields without known camera parameters. arXiv preprint
arXiv:2102.07064 , 2021. 2
[92] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongx-
uan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-
fidelity and diverse text-to-3d generation with variational
score distillation. arXiv preprint arXiv:2305.16213 , 2023.
3
[93] Daniel Watson, William Chan, Ricardo Martin-Brualla,
Jonathan Ho, Andrea Tagliasacchi, and Mohammad
Norouzi. Novel view synthesis with diffusion models.
arXiv preprint arXiv:2210.04628 , 2022. 3
[94] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph
Feichtenhofer, and Georgia Gkioxari. Multiview com-
pressive coding for 3D reconstruction. arXiv:2301.08247 ,
2023. 3
[95] Lin Yen-Chen, Pete Florence, Jonathan T Barron, Alberto
Rodriguez, Phillip Isola, and Tsung-Yi Lin. inerf: Invert-
ing neural radiance fields for pose estimation. In 2021
IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS) , pages 1323–1330. IEEE, 2021. 2
[96] Brent Yi, Weijia Zeng, Sam Buchanan, and Yi Ma. Canon-
ical factors for hybrid neural fields. In International Con-
ference on Computer Vision (ICCV) , 2023. 2
[97] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong
Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:
Radiance fields without neural networks. arXiv preprint
arXiv:2112.05131 , 2021. 2
[98] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo
Kanazawa. pixelnerf: Neural radiance fields from one or
few images. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 4578–
4587, 2021. 2, 4, 7, 8
[99] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu,
Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu,
Zhangyang Xiong, Tianyou Liang, et al. Mvimgnet: A
large-scale dataset of multi-view images. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 9150–9161, 2023. 2
[100] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
10192
deep features as a perceptual metric. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 586–595, 2018. 6, 8
[101] Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and
Andrew J Davison. In-place scene labelling and under-
standing with implicit scene representation. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 15838–15847, 2021. 2
10193
