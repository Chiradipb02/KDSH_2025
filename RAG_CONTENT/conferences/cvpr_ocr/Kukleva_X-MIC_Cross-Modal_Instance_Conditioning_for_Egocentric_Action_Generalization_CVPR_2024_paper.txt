X-MIC: Cross-Modal Instance Conditioning
for Egocentric Action Generalization
Anna Kukleva1,2⇤Fadime Sener1Edoardo Remelli1Bugra Tekin1Eric Sauser1
Bernt Schiele2Shugao Ma1
1Meta Reality Labs;2Max Planck Institute for Informatics, Saarland Informatics Campus
{annakukleva, famesener }@meta.com
Abstract
Lately, there has been growing interest in adapting
vision-language models (VLMs) to image and third-person
video classiﬁcation due to their success in zero-shot recog-
nition. However, the adaptation of these models to egocen-
tric videos has been largely unexplored. To address this
gap, we propose a simple yet effective cross-modal adap-
tation framework, which we call X-MIC. Using a video
adapter, our pipeline learns to align frozen text embed-
dings to each egocentric video directly in the shared em-
bedding space. Our novel adapter architecture retains and
improves generalization of the pre-trained VLMs by disen-
tangling learnable temporal modeling and frozen visual en-
coder. This results in an enhanced alignment of text em-
beddings to each egocentric video, leading to a signiﬁcant
improvement in cross-dataset generalization. We evaluate
our approach on the Epic-Kitchens, Ego4D, and EGTEA
datasets for ﬁne-grained cross-dataset action generaliza-
tion, demonstrating the effectiveness of our method.1
1. Introduction
Egocentric action recognition has recently become a pop-
ular research topic due to the rising interest in augmented
reality and robotics. Recently, two large-scale egocen-
tric datasets Epic-Kitchens [ 6] and Ego4D [ 10], capturing
the daily activities of users have been introduced. While
there is a growing interest in studying action recognition
on egocentric datasets, evaluations primarily occur within
the same dataset; lacking cross-dataset evaluations that is
crucial for real-world deployment of recognition models.
Testing models on different datasets presents several chal-
lenges, such as encountering unfamiliar environments, dif-
ferent users, and previously unseen objects and their corre-
sponding actions, all of which can signiﬁcantly impact per-
1https://github.com/annusha/xmic
*work is done during internship at Meta
(ZS CLIP) ‘’Image of a < . . .>’’?           painting
(X-MIC)   ‘’Image of a < . . .>’’?         brush
X-MIC adapterpaintingbrushpaintingbrush
Visual
Encoder
GT label:hold brush +GT label:hold brush 
Text 
Encoder
paintingbrushText 
Encoder+Visual
EncoderpaintingbrushFigure 1. Egocentric video classiﬁcation with VL models.
Top: Standard zero-shot CLIP. As the dominant object in the scene
is painting, the model predicts class “painting” while the object of
interest is “brush”. Bottom: CLIP model with our X-MIC adap-
tation directly in the shared VL embedding. X-MIC vectors adapt
focus of the CLIP model to the hand area, guiding text modality to
capture egocentric domain-speciﬁc information.
formance. Recently, vision-language models [ 13,27,39]
such as CLIP [ 27] have demonstrated remarkable perfor-
mance across diverse third-persons datasets like Kinetics-
600 [ 16] and ImageNet [ 7], showcasing their ability to gen-
eralize effectively and achieving zero-shot performance of
59.8% and 76.2%, respectively. However, their zero-shot
performance drops signiﬁcantly when applied to egocentric
datasets like Epic-Kitchens, with noun and verb recognition
reaching only 8.8% and 5.9%, respectively; highlighting the
domain gap between third-person and egocentric data.
CLIP’s zero-shot generalization to new datasets lever-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
26364
ages learning a shared embedding space for text and visual
modalities. To enhance generalization to new domains, a
prominent research direction [ 44] explores adapting the text
encoder by appending trainable prompt tokens to class to-
kens, modifying the class-text input from “a photo of an ap-
ple” to “ <learnable prompt >apple”. As an alternative ap-
proach, recent work has proposed to train feature adapters
on both the visual and textual domains [ 5,8], drawing in-
sights from the NLP works [ 12,34]. Despite their promis-
ing results, these methods overlook the inherent character-
istics of the egocentric video domain. To overcome this, we
propose a simple yet effective adapter architecture, injecting
egocentric video-speciﬁc knowledge into a frozen VL em-
bedding space, depicted in Fig. 1. Our method transforms
each video through an adapter into a vector for cross -modal
instance c onditioning of text — referred to as X-MIC-
vector. Our cross-modal adaptation performed directly in
the embedding space results in signiﬁcantly improved ef-
ﬁciency during training and testing. Moreover, our new
adapter module disentangles frozen visual encoder from the
visual temporal modeling through cross-modal adaptation.
Each X-MIC-vector is video-speciﬁc, therefore, allowing us
to align any frozen text to each input video individually. Fi-
nally, to align the text embedding to the video, we simply
add the X-MIC-vector to the text embedding vectors.
We extensively evaluate our approach on Epic-
Kitchens [ 6], Ego4D [ 10] and EGTEA [ 20] datasets,
demonstrating superior generalization compared to SOTA
VL-adaptation methods.
Our contributions can thus be summarized as:
•addressing the task of egocentric cross-dataset and zero-
shot action recognition with VLMs that is designed for
real-world applications, e.g. AR, addressing the imprac-
ticality of collecting data from every new environment,
•a simple yet effective framework, referred to as X-MIC,
for cross-modal adaption of VL models directly in the
pre-trained VL embedding space; our module disentan-
gles temporal modeling from the frozen visual encoder,
•a new egocentric spatial-temporal attention module en-
hances information around hands, thereby improving
egocentric action recognition performance,
•thorough comparisons with respect to image and video
state-of-the-art VL adaptation methods which demon-
strate the effectiveness of our approach.
2. Related Work
Egocentric Action Generalization. While egocentric vi-
sion gained attention with datasets like Epic-Kitchens [ 6]
and Ego4D [ 10], current state-of-the-art [ 17,25,29,30,36–
38,40,45] primarily focus on intra-dataset evaluation,
which limits their applicability to real-world scenarios. Sev-
eral methods ﬁne-tuned CLIP on egocentric datasets [ 21,
26,41], yet generalization on ﬁne-grained verbs and nounsrecognition remains underexplored. Our work comprehen-
sively investigates both intra-dataset and inter-dataset gen-
eralization on both verbs and nouns.
Prompt Learning and Adapters. Prompt learning in
NLP [ 9,14,19,32,42] adapts frozen text models by
appending task-speciﬁc information. Extending this to
image recognition, CoOp [ 44] learns appendable vec-
tors in the text token space. CoCoOp [ 43] introduces
image-conditioned prompt learning, boosting performance
but with high computational costs. MaPLe [ 18] lever-
ages shared deep prompts for text and visual encoders,
while PromptSRC [ 28] suggests regularizing constraints
for frozen encoders. Chen et al. [4] found that prompt-
ing boosts model transferability in tasks with fewer num-
ber of visual tokens, like image classiﬁcation, but has lim-
ited impact in tasks with more tokens, such as video under-
standing. Thus, an alternative research direction explores
adapting vision-language models with feature adapters [ 12].
Clip-adapter [ 8] learns new features through an additional
bottleneck layer and blends them with original pre-trained
features in a residual style. Our approach falls under the
adapter category. Unlike previous visual adapters, we in-
troduce cross-modal instance conditioning speciﬁcally de-
signed for egocentric video recognition.
Adapting VLMs to Videos. Recent advancements in
prompt learning extend to third-person videos. A5/A6 [ 15]
introduces a temporal module atop visual encoder, keeping
both encoders frozen. EVL [ 22] discards the text encoder,
relying solely on temporally encoded frame features by vi-
sual encoder. Vita-CLIP [ 1] uses shallow prompts on the
text encoder, similar to [ 44], and introduces deep temporal
prompts for the visual encoder. Recently, OAP [ 3] general-
izes the verbs observed during training to an open vocabu-
lary of objects with a prompt-based object encoder on ego-
centric videos. Our method builds on existing work while
introducing an adapter architecture speciﬁcally tailored to
egocentric domain, resulting in superior performance.
3. X-MIC Adaptation Approach
We begin by introducing the preliminaries such as classi-
ﬁcation with VLMs like CLIP and different types of VL
adaptations in Sec. 3.1. Then, in Sec. 3.2, we give an
overview of our adapter method for text conditioning and
present our egocentric-spatio-temporal attention module.
3.1. Preliminaries and Baselines on VL Adaptation
Vision-language models (VLMs), such as CLIP, demon-
strate effective zero-shot generalization across various
downstream tasks for image recognition and third-person
video recognition. However, certain domains, like egocen-
tric videos, still face challenges due to a signiﬁcant gap be-
tween web-collected and egocentric data.
26365
TVTV
TVTVI.  Early Fusion
II. Uni-Modal
I.  Early Fusion
II. Cross-ModalI.  Late Fusion
II. Uni-ModalCoOpZS CLIPI. No Fusion
CoCoOpClip-Adapter
brushpaintapplebrushpaintapple
brushpaintapplebrushpaintapple
Visual
Encoder IVideoText Labels
Video
ClassiﬁcationText
EncoderVisual
Encoder II
Learnable PromptsLearnable AdaptersVideo FeaturesText FeaturesGradient FlowI.  Late Fusion
II. Cross-Modal
Video
ClassiﬁcationFrozen ParametersX-MIC (ours)brushpaintapple
Baselines
Feature BlendingFigure 2. Overview of our X-MIC method and previous adaptation methods of VLMs. Baselines :No Fusion is a standard zero-shot
video classiﬁcation method. The average of the frame representations is compared to text representations in the shared VL embedding
space. Early Fusion & Uni-Modal is a prompt learning method, where the learnable parameters are concatenated to text tokens and
optimized through the text encoder. Subsequently, the text encoder is adapted to the new domain. Early Fusion & Cross-Modal is an
extension of Early Fusion & Uni-Modal method, where additional learnable parameters are introduced in the form of an adapter. This
adapter maps video representations to embedding space of text tokens, which are then concatenated to learnable prompts and text tokens.
Memory consumption, required for forward-backwards pass through the text encoder, expands with respect to all combinations of all text-
labels and videos in the batch. Late Fusion & Uni-Modal is a method, where adaptation of both encoders is based on the feature blending
of original text and video representations with the adapted corresponding representations. Ours: X-MIC adaptation method falls in Late
Fusion & Cross-Modal category. Adapted video features are blended with the original text features. Simple adaptation of text modality to
each individual video is efﬁcient as it does not require gradient propagation through text or video encoders. Additionally, we propose to
employ Visual Encoder II, offering ﬂexibility in utilizing various types of visual features for conditioning. Note that Visual Encoder I and
II can be represented by a single visual encoder, such as the CLIP visual encoder.
Below, we provide an overview existing prompt learning
and adapter-based methods.
Video Classiﬁcation with VL Dual Encoders. Trained on
hundreds of millions of text and visual data pairs, VL dual
encoders bring the two modalities together in a shared em-
bedding space. When evaluating models pre-trained on ex-
tensive web data, a crucial metric is their ability to transfer
to other downstream tasks without additional ﬁne-tuning, a
process commonly known as zero-shot evaluation. To per-
form zero-shot classiﬁcation, one needs to propagate a set
ofCpredeﬁned classes in the form of text, denoted as t=
“Image of a <class>” through a pre-trained text encoder
T(·). This process extracts individual text embeddings, rep-
resented as et=T(t)2R1⇥Dfor each class. Subse-
quently, these vectors undergo l2normalization, resulting
in¯et=et
||et||(hereafter, the overline symbol ¯eindicates l2
normalization of vector e). Then a matrix ¯ET2RC⇥Dis constructed, representing a simple linear classiﬁer, and is
thus referred to as the text-based classiﬁer. To classify an
input video v, we sample Nframes, denoting the sampled
frames as v0={zi}N, where zirepresents a frame from the
video v. Subsequently, all sampled frames are mapped to
the shared VL embedding space, using the frozen visual en-
coder V(·). Applying average pooling over the embeddings
of the frames yields a single-vector video representation:
¯ev0=avgpool({V(zi)}N)2R1⇥D. The video vector ¯ev0
is then classiﬁed using the text-based classiﬁer ¯ET.
No Fusion. We refer to frozen dual encoders T(·)andV(·)
without additional adaptation as “No Fusion” baseline.
Early Fusion and Uni-Modal Adaptation. A prompt
learning-based method, CoOp [ 44], introduces Plearnable
vectors appended to all Cinput text classes in the token
embeddings of the textual encoder (see Fig. 2). To optimize
these prompts, gradients are propagated through the frozen
26366
text encoder for C⇥P⇥Dadaptable parameters, where
Dis the dimensionality of the tokens. This optimization
remains independent of the batch size of the visual input.
Early Fusion and Cross-Modal Adaptation. A follow-
up work, CoCoOp [ 43], extends learnable text prompts to
cross-modal prompts by introducing an adapter module of
the frozen visual encoder to the token embedding space (see
Fig.2). In this architecture, each of the Cclass-tokens are
appended not only with Plearnable text prompts but also
with individual input-conditioned prompts generated by the
adapter. Optimizing these prompts for a batch of size B
involves propagating B⇥P⇥D⇥Cgradients, making
training inefﬁcient and slow as shown in [ 43].
Late Fusion and Uni-Modal Adaptation. CLIP-
Adapter [ 8] adopts a late fusion approach as an alternative
to early fusion adaptation. The text and visual encoders
are followed by uni-modal adapter modules that generate
adapted uni-modal feature vectors. These adapted features
are then fused with the corresponding original features in
the VL embedding space, subsequently optimized with the
standard classiﬁcation loss. This optimization is efﬁcient
due to the lightweight nature of adapters, eliminating the
need for heavy text-encoder gradient propagation.
3.2. X-MIC Adaptation
Overview. We aim at achieving generalization in egocen-
tric action recognition across domains and to novel action
classes. Our X-MIC-adaptation framework is designed to
improve the alignment between frozen text representations
and the egocentric visual domain directly within the VL em-
bedding space. To adapt the text modality to the egocentric
domain, we introduce a simple cross-modal text condition-
ing operation based on the input videos. Speciﬁcally, each
X-MIC-vector serves as an adapted video representation.
We align any frozen text representation to each individual
input video by a simple addition operation with the X-MIC-
vector. Consequently, text representations are adapted to
individual input videos, and these adapted text embeddings
are further utilized for the classiﬁcation of corresponding
videos into ﬁne-grained noun and verb classes. More-
over, by introducing an egocentric-spatio-temporal atten-
tion module, we aggregate temporal information between
video frames and emphasize areas around hands to enhance
hand-object interactions. X-MIC-vectors offer dual bene-
ﬁts: a simple and efﬁcient cross-modal conditioning ap-
proach, and the decoupling of domain-speciﬁc knowledge
from the frozen VL embedding, resulting in improved gen-
eralization on egocentric videos.
X-MIC Adaptation. Our adaptation method, X-MIC,
aligns frozen text class embeddings directly to the new do-
main in the shared VL embedding space. During training
and inference, our approach resembles zero-shot classiﬁca-tion, as we classify frozen video representations from the
original visual backbone V(·)using an adapted text-based
classiﬁer ¯ETtailored to each input video v. This enables
efﬁcient domain adaptation without the need for ﬁne-tuning
the entire model, categorizing our method as late fusion
with cross-modal adaptation.
Speciﬁcally, for an input video v, we sample Nframes
to form a sparse video sequence v0={zi}N. The video
sequence v0is then decoded using the original visual en-
coder V(·), resulting in a single vector ¯ev. Additionally,
we encode the Cclasses into the text-based classiﬁer ¯ETas
detailed in Sec. 3.1.
To generate the X-MIC-vector, we introduce a second
frozen visual encoder, denoted as VII(·). This secondary
encoder can either be an identical copy of the original en-
coder V(·)or a distinct pre-trained encoder. In Sec. 4.1,
we demonstrate that incorporating a different type of VII(·)
can result in signiﬁcant generalization improvements. For
instance, DINO [ 23], which is uni-modal, captures distinct
characteristics [ 24] of the visual input compared to multi-
modal CLIP like models that focus solely on main objects.
We employ the second encoder VII(·)to produce an
intermediate representation of frames, denoted as xv=
{VII(zi)}N. Before adapting the intermediate representa-
tion, we apply l2-normalization to the vector. See Sec. 4.4
for a detailed analysis of the impact of this normalization.
Our video adapter A(·)incorporates a temporal aggregation
module. By feeding these intermediate representations into
this module, we obtain the ﬁnal X-MIC-vector for adapta-
tion, represented as av=A(¯xv).
Finally, to adapt the frozen text-based classiﬁer ¯ETto
the video v, we simply sum X-MIC-vector with each class
representation in the embedding space: et+av2RD,
and when combined, these updated vectors form an adapted
text-based classiﬁer Eav
T. Subsequently, we classify the
video representation ¯evwith the adapted text-based clas-
siﬁer Eav
T. The process of classiﬁcation with X-MIC-
adaptation can be summarized as follows:
c=argmaxt<et+A(VII(xv)),¯ev>, (1)
where crepresents the class with the highest similarity
between the adapted text-based classiﬁer Eav
Tand the video
v, and <·,·>denotes dot product.
Ego-Spatio-Temporal Attention Module. Our adaptation
module consist of two transformer blocks bS(·)andbT(·)
designed to aggregate different types of information. This
module not only adapts each video to the shared VL em-
bedding space but also captures egocentric video-speciﬁc
spatial and temporal information, through bS(·)andbT(·)
respectively. To capture hand-object interactions better, we
introduce an attention block that focuses on regions around
26367
DINOv2EgocentricattentionTemporal attention
❄ DINOv2
❄ DINOv2
❄ DINOv2
❄ Egocentricattention h      
  +1 h    +1        +1
"
"
"  Visual
Encoder IText
Encoder
Visual
Encoder II
VII
VII
VII
VII
Egocentric 
AttentionTemporal
AttentionEgocentric 
Attentionxtxhandtxt+1xhandt+1xbStxbSt+1avFigure 3. Ego-Spatio-Temporal Attention Module. It takes a
sequence of full frames interleaved with hand crops as input, and
outputs X-MIC vector av, representing video vas a single vector
for text conditioning in the shared VL embedding space.
hands, see Fig. 3. This involves applying self-attention be-
tween hand crops and full frames, guiding the model to em-
phasize information around hands, a crucial region of inter-
est in egocentric videos. To aggregate temporal information
from the video, we employ a temporal self-attention block,
similar to [ 15] that updates the frame representations. Our
X-MIC-vector for adaptation is derived by applying average
pooling over all updated frame representations.
Egocentric videos may include diverse backgrounds and
involve signiﬁcant camera motion. By focusing on the
region around hands through cropping and applying self-
attention to both the full frame and the cropped region, we
guide our model to prioritize attention on the hands. More
speciﬁcally, for each frame, we use the full frame ziand
the cropped hand region zhand
ifrom the same frame. We
get the intermediate representations through the video en-
coder, for the full frame xi=VII(zi)and the hand region
xhand
i=VII(zhand
i). We concatenate the two to obtain
an intra-frame sequence [xi;xhand
i]2R2⇥D. We derive
the intra-frame representation xbS
iby averaging the updated
representations from both the full and cropped frames:
xbS
i=avgpool(bS([xi;xhand
i])). (2)
To capture the temporal relations across frames, we ap-
ply self-attention between all frames of the video. Speciﬁ-
cally, we use the second transformer bT(·)block to update
xbS
iframe representations, which we aggregate with aver-
age pooling into our X-MIC-vector:
av=avgpool(bT([xbS
1,xbS
2,···,xbS
N])). (3)
In this way, our adaptation module effectively incorporates
egocentric video-speciﬁc spatial and temporal information
into the frozen vision-language embedding space, enhanc-
ing generalization to novel classes.4. Experiments
X-MIC is mainly evaluated on the cross-dataset setting be-
tween two large-scale egocentric datasets: Ego4D [ 10] and
Epic-Kitchens [ 6]. We also evaluate generalization perfor-
mance on the small-scale EGTEA [ 20] dataset.
4.1. Datasets
Ego4D [ 10].We a subset of Ego4D [ 10] annotated with
ﬁne-grained noun and verb labels, speciﬁcally from the
FHO benchmark which contains 521 noun and 117 verb
classes. The training set consists of 64K video clips, while
the testing set comprises 33K clips. The average clip dura-
tion is 8 seconds, resulting in a total of approximately 215
hours of videos, excluding irrelevant background clips.
Epic-Kitchens [ 6].We use the Epic-Kitchens100, com-
prising 67K video clips for training and 10K video clips
for testing. The average clip length is 3.5 seconds, total-
ing about 70 hours, excluding irrelevant background clips.
The dataset features annotations for 300 noun classes and
97 verb classes, focusing on kitchen-related topics.
EGTEA [ 20]We use this dataset solely for model testing,
given its training set of 8,000 video clips with clip length of
3.2 seconds. During inference, we combine three test splits
resulting in 6K video clips in total. The dataset is annotated
with ﬁne-grained 20 verb and 54 noun classes.
4.2. Implementation Details
We evaluate the generalization performance based on the
adaptation of the pre-trained CLIP ViT-B/16 model, unless
otherwise speciﬁed. For model training, we use AdamW
with(  1, 2) = (0.9, 0.999) and weight decay of 0.01 for 15
epochs with a ﬁxed learning rate of 1e-6. The Transformer
module b1contains 1 self-attention layer, whereas temporal
attention module b2includes 2 self-attention layers. During
training, we sample 16 random frames, and during evalua-
tion we sample frames uniformly. For detecting the hand re-
gions, we use the 100DOH [ 31] detector, extracting bound-
ing boxes for each frame. Further implementation details
are provided in the supplementary.
Cross-Datasets Evaluation. In our work, we investigate
the generalization performance of ﬁne-grained noun and
verb recognition across egocentric datasets. Our objective is
two-fold: achieving strong performance within the dataset
on the corresponding test set and demonstrating robust gen-
eralization to a shared dataset (Table 1). We compute the
harmonic mean between these two to gauge the balance
of different types of generalization. For instance, we train
our models on Ego4D and subsequently evaluate on both
the Ego4D and Epic-Kitchens test sets. We further anal-
yse zero-shot generalization by identifying disjoint subsets
of shared and novel classes across datasets (Table 2). See
supplementary for corresponding classes.
26368
Trained on Ego4D (E4D) Trained on Epic-Kitchens (EK)
ta Nouns Verbs Nouns Verbs
Evaluation dataset E4D EK hm E4D EK hm EK E4D hm EK E4D hm
ZS CLIP - 5.89 8.74 7.03 2.18 4.25 2.88 8.74 5.89 7.03 4.25 2.18 2.88
CoOp - 28.22 10.87 15.70 22.57 20.42 21.44 21.56 9.37 13.06 30.91 13.35 18.64
Co-CoOp - 30.00 9.51 14.44 21.31 12.99 16.14 24.23 9.27 13.41 34.16 14.17 20.03
CLIP-Adapter - 30.00 8.95 13.78 22.82 19.94 21.28 33.21 5.73 9.77 36.70 16.09 22.37
CLIP-Adapter* X31.26 10.00 15.16 27.32 22.28 24.54 34.40 4.67 8.22 48.69 15.52 23.54
A5 X31.39 7.84 12.55 26.31 22.77 24.41 32.04 3.31 5.99 46.05 17.93 25.81
Vita-CLIP X33.52 10.61 16.11 22.66 25.81 24.13 34.41 9.52 14.91 48.78 13.47 21.11
X-MIC X33.54 15.35 21.06 28.93 26.48 27.65 30.64 12.32 17.57 50.01 18.10 26.58
X-MIC+DINO X35.85 18.96 24.80 28.27 29.49 28.86 44.07 11.45 18.17 53.02 16.01 24.60
Table 1. SOTA comparison on within- and cross-dataset evaluation on Ego4d and Epic Kitchens datasets. Left: Models trained on
Ego4D. Right: Models trained no Epic-Kitchens. Evaluation is on noun and verb classes. ta denotes temporal attention in the corresponding
method, other methods apply simple average of the frames. hm stands for harmonic mean evaluation. X-MIC+DINO denotes our model
with DINO [ 23] as Visual Encoder II.
Trained on E4D, Evaluated on EK Trained on EK, Evaluated on E4D
ta Nouns Verbs Nouns Verbs
Eval. subset shared novel hm shared novel hm shared novel hm shared novel hm
ZS CLIP - 10.38 13.58 11.77 12.32 4.32 6.40 11.38 10.49 10.92 2.73 9.84 4.27
CoOp - 16.86 16.02 16.43 25.03 5.97 9.64 15.77 10.11 12.32 20.22 5.69 8.89
CoCoOp - 16.35 11.51 13.51 24.34 0.00 0.00 17.31 11.46 13.79 15.32 6.46 9.09
CLIP-Adapter - 12.46 5.99 8.09 21.48 3.09 5.40 8.72 7.42 8.02 19.60 3.00 5.20
CLIP-Adapter* X16.24 12.22 13.95 25.29 1.23 2.35 14.67 7.68 10.08 24.17 4.50 7.59
A5 X15.25 5.24 7.80 27.90 3.09 5.56 13.54 5.71 8.03 24.29 0.55 1.07
Vita-CLIP X15.84 6.15 8.86 27.22 4.11 7.14 14.60 9.76 11.69 16.46 6.58 9.40
X-MIC X20.04 21.51 20.75 29.01 7.00 11.27 19.66 12.24 15.09 23.00 7.16 10.92
X-MIC+DINO X25.56 20.52 22.76 31.92 6.38 10.63 18.91 10.67 13.65 20.55 6.48 9.85
Table 2. Zero-shot action generalization . Left: The models are trained on Ego4D (E4D) and subsequently evaluated on Epic-Kitchnes
(EK) using disjoint subsets of classes (shared and novel). Right: The models are trained on Epic-Kitchens (EK) and then evaluated in a
cross-dataset manner on subsets of classes within Ego4D.
4.3. X-MIC Comparison to SOTA
In Tables 1and2, we start with comparing our method
to CLIP. Notably, on both the Ego4D and Epic-Kitchens
datasets, CLIP yields surprisingly low results for both verbs
and nouns, in contrast to its strong performance on third-
person datasets [ 16,33]. Next, we compare our method to
other adaptation methods of VLMs which have shown im-
provements on image and video recognition benchmarks.
Image-based Adaptation Methods. First, we present a
comparison to image-based adaptation models, including
CoOp [ 44], CoCoOp [ 43], and CLIP-Adapter [ 8] which do
not contain a temporal component. Our analysis in Table 1
shows that early fusion-based models like CoOp and Co-
CoOp exhibit limited learning capacity, resulting in poorer
performance compared to other models for both nouns and
verbs when evaluated within-dataset, especially on Epic-
Kitchens. This aligns with earlier ﬁndings shown in [ 4]. A
late fusion-based framework, CLIP-Adapter, improves the
within-dataset scores but demonstrates weaker generaliza-tion on nouns for cross dataset evaluation. However, Table 2
reveals that CoOp [ 44] demonstrates robustness when novel
nouns and verbs are encountered even in the absence of any
temporal attention module. We hypothesize that other mod-
els may be more prone to overﬁtting on the shared classes
due to a larger number of parameters.
Video-based Adaptation Methods. In Table 1, we eval-
uate the performance of recent third-person video adapta-
tion models, speciﬁcally A5 [ 15] and Vita-CLIP [ 1], in an
egocentric scenario. Additionally, we enhance the CLIP-
Adapter model by incorporating temporal attention and
evaluate its effectiveness as a video model. We notice that
the inclusion or exclusion of a temporal component, be-
yond simple averaging, has a relatively minor impact on
noun recognition using CLIP-Adapter. To illustrate, when
trained on the Epic-Kitchens dataset, CLIP-Adapter, with
(denoted as CLIP-Adapter*) and without a temporal at-
tention module, exhibits comparable performance in noun
recognition within the dataset (EK), with scores of 33.21%
and 34.40%, respectively. However, the role of temporal at-
26369
NounsVerbsE4D EGTEA hmE4D EGTEA hmZS CLIP5.89 19.70 9.072.18 18.71 3.51CoOp29.23 23.90 26.2922.57 26.45 24.35Co-CoOp29.85 27.90 28.8421.31 27.74 24.10CLIP-Adapter30.00 21.41 24.9822.82 26.51 24.52CLIP-Adapter *29.18 22.40 25.3427.32 26.57 26.93A533.50 23.70 27.7626.31 28.03 27.14Vita-CLIP33.52 17.24 22.7622.66 27.63 24.89X-MIC33.5429.2131.2128.9331.4130.12Table 3. SOTA comparison on EGTEA. The model is trained on
Ego4D dataset and evaluated in a zero-shot manner on EGTEA.Nouns Verbs
E4D EK hm E4D EK hm
F 31.68 14.20 19.61 27.19 24.02 25.51
H 31.35 14.02 19.37 26.32 26.59 26.46
F+H 33.54 15.35 21.06 28.93 26.48 27.65
Table 4. Inﬂuence of Ego-Spatial-Temporal attention. F
denotes full frames, H denotes hand crops. F+H correspond
to our proposed attention module. All models share the same
architecture of the temporal attention module.
tention becomes crucial in enhancing verb recognition per-
formance, as evidenced by consistent improvements across
both datasets and all models. A5 [ 15], which combines
both early fusion and temporal attention, shows poor cross-
dataset generalization on nouns for both datasets, aligning
with the ﬁndings reported by its authors [ 15] in the con-
text of cross-dataset third-person video generalization. The
recent SOTA model on third-person video generalization,
Vita-CLIP [ 1], demonstrates enhanced noun recognition on
both datasets but exhibits lower verb recognition on Ego4D.
In contrast to other video adaptation models, we decou-
ple temporal attention from the frozen backbone and in-
troduce X-MIC-vector, encapsulating all temporal informa-
tion. Moreover, employing cross-modal adaptation, we in-
troduce video-speciﬁc classiﬁers. For each video, we create
an individual text-based classiﬁer, which is adapted with our
X-MIC-vector. Our approach demonstrates state-of-the-art
generalization performance while maintaining high perfor-
mance on within-dataset evaluation. Moreover, by leverag-
ing DINO pre-trained model [ 23] as visual encoder VII, we
observe signiﬁcant improvements on within-dataset evalu-
ation. In Table 3, we present our evaluation on EGTEA.
Overall, we note consistent trends across all methods.
In Table 2, we observe that video-based models perform
poorly, likely due to overﬁtting on shared classes. Models
like A5 [ 15] and Vita-CLIP [ 1], with a larger number of pa-
rameters, may be more susceptible to this issue. In contrast,
our X-MIC framework decouples the adapter module from
the frozen VL embedding space, enabling enhanced gen-
eralization. Furthermore, we observe that models struggle
more with generalizing on verbs than nouns, likely due to
the object-centric pre-training data of the backbone models,
e.g.CLIP is pre-trained solely on image-text pairs.
4.4. Ablations
In this section, we evaluate the effectiveness of our design
choices. For all ablations, we train models on Ego4D and
evaluate on Ego4D and Epic-Kitchens. As backbone, we
use CLIP ViT-B/16, unless otherwise speciﬁed.
Ego-Spatial-Temporal Attention. In Table 4, we demon-
strate the impact of utilizing full frames, that usuallyincludes scene context, and hand crops on the perfor-
mance of egocentric videos. We observe that concentrat-
ing solely on hand regions enhances verb generalization,
whereas the utilization of full images proves marginally
more advantageous for noun generalization. When em-
ploying our proposed ego-spatial-temporal attention mech-
anism, we achieve a notable improvement in the harmonic
mean. Speciﬁcally, there is a 1.45% increase for nouns and
a 2.14% boost for verbs compared to using full frames.
By guiding the model to consider context in relation to
hand areas, our attention approach not only enhances per-
formance within the dataset but also showcases improved
cross-dataset performance.
Larger backbone. In Table 5, we assess the effectiveness
of our method using a bigger CLIP model, speciﬁcally com-
paring the performance of CLIP ViT-L/14 with ViT-L/16.
While we do not observe performance gains for within the
dataset evaluations, a compelling trend emerges in cross-
dataset generalization, particularly on nouns. Notably, em-
ploying the larger model ViT-L/14 results in a signiﬁcant
improvement of over 7% in noun and 2.45% in verb gener-
alization on Epic. This encouraging outcome underscores
the potential of vision transformers and suggests that fur-
ther exploration and reﬁnement of these models could yield
even more substantial gains in cross-dataset generalization.
Egocentric VL backbone. Table 5presents an evaluation
on X-MIC-model performance using backbones CLIP and
Lavila [ 41], which is pre-trained on text-video pairs from
the Ego4D dataset in a contrastive manner. Note that the
Lavila backbone initializes its model from CLIP pre-trained
models. We ﬁrst compare the zero-shot results from the
original CLIP backbone and Lavila. Lavila demonstrates
a signiﬁcant improvement in noun recognition by 16.59%
on the Ego4D dataset and noun generalization to Epic by
17.18%. While Lavila shows a decrease in verb recognition
accuracy within the dataset by 2.38% , its generalization to
Epic verbs increases by 6.04%. This outcome is surpris-
ing, as we initially expected Lavila to generalize better on
verbs due to its training on an egocentric dataset, indicating
a strong bias toward object-oriented pre-training strategies.
We observe similar trends when our model utilizes CLIP
26370
Nouns Verbs
Evaluation dataset E4D EK hm E4D EK hm
CLIPViT-B/16ZS 5.89 8.74 7.03 2.18 4.25 2.88
X-MIC 33.54 15.35 21.06 28.93 26.48 27.65
ViT-L/14ZS 8.40 13.88 10.46 8.57 9.70 9.10
X-MIC 33.75 22.46 26.97 28.13 28.93 28.52
Lavila ViT-L/14ZS 24.99 31.06 27.69 6.19 15.74 8.88
X-MIC 35.18 34.97 35.08 12.28 24.66 16.37
Table 5. Inﬂuence of different backbones. We compare the performance of CLIP ViT-L/14
with ViT-B/16. Additionally, we provide a comparison of CLIP backbone, pre-trained on text-
image pairs, to Lavila backbone, pre-trained on pairs of egocentric videos and narrations from
full Ego4D. ZS denotes zero-shot evaluation. X-MIC denotes the evaluation of our method
with the corresponding backbones (CLIP or Lavila) without additional DINO backbone.norm Nouns
E4D EK hm
n1 33.54 15.35 21.06
none 32.64 14.34 19.92
n2,n3 32.74 14.59 20.19
n1,n2,n3 31.99 14.49 19.95
n1,n2 15.81 12.3 13.83
n1,n3 12.12 11.34 11.71
Table 6. Inﬂuence of feature normaliza-
tion. [n1] stands for l2-norm of features
after VIIencoder and before the adapter
(our default). [n2] denotes l2-norm of X-
MIC vector before sum, [n3] denotes l2-
norm of text features before sum.
Nouns Verbs
ZS X-MIC ZS X-MIC
prompts E4D EK E4D EK hm E4D EK E4D EK hm
<class> 5.89 8.74 33.54 15.35 21.06 2.18 4.25 28.93 26.48 27.65
Image of a <class> 10.52 6.75 32.31 14.81 20.31 3.28 5.40 28.56 25.98 27.21
Video of a <class> 10.32 6.80 32.62 14.77 20.33 2.93 5.97 28.14 22.70 25.13
Egocentric image a <class> 9.61 7.11 32.09 15.65 21.04 2.98 3.83 28.58 24.02 26.10
Image of a hand holding a <class> 10.09 6.32 32.92 14.28 19.92 3.29 9.87 27.53 19.33 22.71
Egocentric image of a hand holding <class> 9.23 6.86 33.29 15.83 21.45 2.41 6.24 27.66 16.94 21.01
Table 7. Inﬂuence of prompting the frozen text model with additional context. ZS denotes zero-shot CLIP evaluation. Noun recogni-
tion is robust to contextual variations, while verb recognition performs best without additional context.
versus Lavila as a backbone, where noun generalization in-
creases signiﬁcantly, while verb generalization slightly de-
creases.
Prompts for text encoder. In Table 7, we evaluate the per-
formance of zero-shot CLIP and our model by prompting
the frozen text model for classiﬁcation with additional con-
text. Our experiments include speciﬁc details like ”Video
of a” or indications of hands and an egocentric view. We
ﬁnd that zero-shot noun performance is the best with the
standard “Image of a ” context for Ego4D and without con-
text for Epic-Kitchens. However, zero-shot verb recogni-
tion beneﬁts from an additional context, achieving 3.29%
and 9.87% on Ego4d and Epic-Kitchens, respectively. With
our X-MIC adaptation, we observe that noun recognition
remains robust to these changes, while verb recognition is
sensitive and performs best when no additional context is
provided highlighting the complexity of incorporating con-
textual information in egocentric scenarios.
Importance of normalization. We investigate the signif-
icance of feature normalization in the embedding space in
Table 6.n1represents our default choice, involving the nor-
malization of visual features after the VIIencoder and be-
fore the adapter. n2indicates the normalization of X-MIC-
vector, i.e., visual features after our video-adapter module,
prior to summation with frozen text features. Lastly, n3de-
notes the normalization of frozen text features before sum-mation with X-MIC-vector. The ’none’ corresponds to no
normalization. [ n1] demonstrates the optimal balance be-
tween regularization and no regularization. Conﬁgurations
[n1], [n2,n3 ], [n1,n2,n3 ], and ’none’ all yield symmet-
ric feature magnitudes before the summation of frozen text
features and X-MIC-vector and marginally change the har-
monic mean. Conversely, variations such as [ n1,n2 ] and
[n1,n3 ] result in imbalances during the summation of dif-
ferent modalities, leading to suboptimal performance.
5. Conclusions & Limitations
We have introduced X-MIC, a simple yet effective cross-
modal adaptation framework for VLMs, that injects ego-
centric video information into the frozen VL embedding,
achieving signiﬁcant improvements in ﬁne-grained cross-
dataset egocentric recognition of nouns and verbs. More-
over, X-MIC vectors offer decoupling of the domain-
speciﬁc knowledge from the frozen VL embedding. This
allows to explore different visual backbones for text condi-
tioning directly in the embedding space, showing improved
generalization. It is important to note that our method fo-
cuses solely on video classiﬁcation and does not encompass
text-vision tasks like text-to-video retrieval, which would
necessitate using text-conditioned videos instead of our
video-conditioned text representations. We plan to explore
this direction in future work.
26371
References
[1]Vita-clip: Video and text adaptive clip via multimodal
prompting. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 23034–
23044, 2023. 2,6,7
[2]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. Layer normalization. arXiv preprint arXiv:1607.06450 ,
2016. 2
[3]Dibyadip Chatterjee, Fadime Sener, Shugao Ma, and Angela
Yao. Opening the vocabulary of egocentric actions. Ad-
vances in Neural Information Processing Systems , 36, 2024.
2
[4]Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang,
Yibing Song, Jue Wang, and Ping Luo. Adaptformer:
Adapting vision transformers for scalable visual recogni-
tion. Advances in Neural Information Processing Systems ,
35:16664–16678, 2022. 2,6
[5]Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong
Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for
dense predictions. arXiv preprint arXiv:2205.08534 , 2022.
2
[6]Dima Damen, Hazel Doughty, Giovanni Maria Farinella,
Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide
Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and
Michael Wray. Rescaling egocentric vision: Collection,
pipeline and challenges for epic-kitchens-100. International
Journal of Computer Vision (IJCV) , 130:33–55, 2022. 1,2,
5
[7]J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
CVPR09 , 2009. 1
[8]Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao
Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao.
Clip-adapter: Better vision-language models with feature
adapters. International Journal of Computer Vision , pages
1–15, 2023. 2,4,6
[9]Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-
trained language models better few-shot learners. arXiv
preprint arXiv:2012.15723 , 2020. 2
[10] Kristen Grauman, Andrew Westbury, Eugene Byrne,
Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jack-
son Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel
Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Ku-
mar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael
Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao,
Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean
Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph
Feichtenhofer, Adriano Fragomeni, Qichen Fu, Christian
Fuegen, Abrham Gebreselasie, Cristina Gonzalez, James
Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo,
Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Lan-
dini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Man-
galam, Raghava Modhugu, Jonathan Munro, Tullie Mur-
rell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes,
Merey Ramazanova, Leda Sari, Kiran Somasundaram, Au-
drey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo,
Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu, PabloArbelaez, David Crandall, Dima Damen, Giovanni Maria
Farinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V.
Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard
Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg,
Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Tor-
ralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Ma-
lik. Ego4d: Around the World in 3,000 Hours of Egocentric
Video. In IEEE/CVF Computer Vision and Pattern Recogni-
tion (CVPR) , 2022. 1,2,5
[11] Dan Hendrycks and Kevin Gimpel. Gaussian error linear
units (gelus). arXiv preprint arXiv:1606.08415 , 2016. 2
[12] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona
Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer
learning for nlp. In International Conference on Machine
Learning , pages 2790–2799. PMLR, 2019. 2
[13] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representa-
tion learning with noisy text supervision. In International
conference on machine learning , pages 4904–4916. PMLR,
2021. 1
[14] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neu-
big. How can we know what language models know? Trans-
actions of the Association for Computational Linguistics , 8:
423–438, 2020. 2
[15] Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi
Xie. Prompting visual-language models for efﬁcient video
understanding. In European Conference on Computer Vi-
sion, pages 105–124. Springer, 2022. 2,5,6,7
[16] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,
Tim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-
man action video dataset. arXiv preprint arXiv:1705.06950 ,
2017. 1,6
[17] Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and
Dima Damen. Epic-fusion: Audio-visual temporal bind-
ing for egocentric action recognition. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 5492–5501, 2019. 2
[18] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad
Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple:
Multi-modal prompt learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 19113–19122, 2023. 2
[19] Brian Lester, Rami Al-Rfou, and Noah Constant. The power
of scale for parameter-efﬁcient prompt tuning. arXiv preprint
arXiv:2104.08691 , 2021. 2
[20] Yin Li, Miao Liu, and James M Rehg. In the eye of beholder:
Joint learning of gaze and actions in ﬁrst person video. In
Proceedings of the European conference on computer vision
(ECCV) , 2018. 2,5
[21] Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, Michael
Wray, Rui Yan, Eric Z XU, Difei Gao, Rong-Cheng Tu, Wen-
zhe Zhao, Weijie Kong, et al. Egocentric video-language
pretraining. Advances in Neural Information Processing Sys-
tems, 35:7575–7586, 2022. 2
26372
[22] Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard
de Melo, Xiaogang Wang, Jifeng Dai, Yu Qiao, and Hong-
sheng Li. Frozen clip models are efﬁcient video learners. In
European Conference on Computer Vision , pages 388–404.
Springer, 2022. 2
[23] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy
Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.
Dinov2: Learning robust visual features without supervision.
arXiv preprint arXiv:2304.07193 , 2023. 4,6,7
[24] Namuk Park, Wonjae Kim, Byeongho Heo, Taekyung Kim,
and Sangdoo Yun. What do self-supervised vision transform-
ers learn? arXiv preprint arXiv:2305.00729 , 2023. 4
[25] Mandela Patrick, Dylan Campbell, Yuki M. Asano, Is-
han Misra Florian Metze, Christoph Feichtenhofer, Andrea
Vedaldi, and Joao F. Henriques. Keeping your eye on the
ball: Trajectory attention in video transformers. In Advances
in Neural Information Processing Systems (NeurIPS) , 2021.
2
[26] Shraman Pramanick, Yale Song, Sayan Nag,
Kevin Qinghong Lin, Hardik Shah, Mike Zheng Shou,
Rama Chellappa, and Pengchuan Zhang. Egovlpv2:
Egocentric video-language pre-training with fusion in the
backbone. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 5285–5297, 2023. 2
[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 1
[28] Hanoona Rasheed, Muhammad Uzair Khattak, Muhammad
Maaz, Salman Khan, and Fahad Shahbaz Khan. Fine-tuned
clip models are efﬁcient video learners. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 6545–6554, 2023. 2
[29] Fadime Sener, Dibyadip Chatterjee, and Angela Yao. Tech-
nical report: Temporal aggregate representations. arXiv
preprint arXiv:2106.03152 , 2021. 2
[30] Fadime Sener, Dibyadip Chatterjee, Daniel Shelepov, Kun
He, Dipika Singhania, Robert Wang, and Angela Yao. As-
sembly101: A large-scale multi-view video dataset for un-
derstanding procedural activities. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 21096–21106, 2022. 2
[31] Dandan Shan, Jiaqi Geng, Michelle Shu, and David F
Fouhey. Understanding human hands in contact at inter-
net scale. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 9869–9878,
2020. 5
[32] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric
Wallace, and Sameer Singh. Autoprompt: Eliciting knowl-
edge from language models with automatically generated
prompts. arXiv preprint arXiv:2010.15980 , 2020. 2
[33] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.
Ucf101: A dataset of 101 human actions classes from videos
in the wild. arXiv preprint arXiv:1212.0402 , 2012. 6[34] Asa Cooper Stickland and Iain Murray. Bert and pals: Pro-
jected attention layers for efﬁcient adaptation in multi-task
learning. In International Conference on Machine Learning ,
pages 5986–5995. PMLR, 2019. 2
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 2
[36] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi
Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer.
Memvit: Memory-augmented multiscale vision transformer
for efﬁcient long-term video recognition. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 13587–13597, 2022. 2
[37] Fanyi Xiao, Yong Jae Lee, Kristen Grauman, Jitendra Malik,
and Christoph Feichtenhofer. Audiovisual slowfast networks
for video recognition. arXiv preprint arXiv:2001.08740 ,
2020.
[38] Xuehan Xiong, Anurag Arnab, Arsha Nagrani, and Cordelia
Schmid. M&m mix: A multimodal multiview transformer
ensemble. arXiv preprint arXiv:2206.09852 , 2022. 2
[39] Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D
Manning, and Curtis P Langlotz. Contrastive learning of
medical visual representations from paired images and text.
InMachine Learning for Healthcare Conference , pages 2–
25. PMLR, 2022. 1
[40] Yue Zhao and Philipp Kr ¨ahenb ¨uhl. Real-time online video
detection with temporal smoothing transformers. In Eu-
ropean Conference on Computer Vision , pages 485–502.
Springer, 2022. 2
[41] Yue Zhao, Ishan Misra, Philipp Kr ¨ahenb ¨uhl, and Rohit Gird-
har. Learning video representations from large language
models. In CVPR , 2023. 2,7
[42] Zexuan Zhong, Dan Friedman, and Danqi Chen. Factual
probing is [mask]: Learning vs. learning to recall. arXiv
preprint arXiv:2104.05240 , 2021. 2
[43] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
Liu. Conditional prompt learning for vision-language mod-
els. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 16816–16825,
2022. 2,4,6
[44] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
Liu. Learning to prompt for vision-language models. In-
ternational Journal of Computer Vision , 130(9):2337–2348,
2022. 2,3,6
[45] Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk
Cinbis, David Fouhey, Ivan Laptev, and Josef Sivic. Cross-
task weakly supervised learning from instructional videos.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 3537–3545, 2019. 2
26373
