Doubly Abductive Counterfactual Inference for Text-based Image Editing
Xue Song1, Jiequan Cui3, Hanwang Zhang2,3, Jingjing Chen1*, Richang Hong4, Yu-Gang Jiang1
1Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University
2Skywork AI3Nanyang Technological University4Hefei University of Technology
{xsong18, chenjingjing, ygj }@fudan.edu.cn, hanwangzhang@ntu.edu.sg, {jiequancui, hongrc.hfut }@gmail.com
Abstract
We study text-based image editing (TBIE) of a single im-
age by counterfactual inference because it is an elegant for-
mulation to precisely address the requirement: the edited
image should retain the fidelity of the original one. Through
the lens of the formulation, we find that the crux of TBIE
is that existing techniques hardly achieve a good trade-off
between editability and fidelity, mainly due to the overfit-
ting of the single-image fine-tuning. To this end, we pro-
pose a Doubly Abductive Counterfactual inference frame-
work (DAC). We first parameterize an exogenous variable
as a UNet LoRA, whose abduction can encode all the im-
age details. Second, we abduct another exogenous vari-
able parameterized by a text encoder LoRA, which recov-
ers the lost editability caused by the overfitted first ab-
duction. Thanks to the second abduction, which exclu-
sively encodes the visual transition from post-edit to pre-
edit, its inversion—subtracting the LoRA—effectively re-
verts pre-edit back to post-edit, thereby accomplishing the
edit. Through extensive experiments, our DAC achieves a
good trade-off between editability and fidelity. Thus, we can
support a wide spectrum of user editing intents, including
addition, removal, manipulation, replacement, style trans-
fer, and facial change, which are extensively validated in
both qualitative and quantitative evaluations. Codes are in
https://github.com/xuesong39/DAC .
1. Introduction
Text-based image editing (TBIE) modifies a user-uploaded
real image to match a textual prompt while keeping mini-
mal visual changes—the fidelity of the original image. As
shown in Figure 1, the source image Iin (a) is edited with
the prompt “I want the castle covered by snow”. We con-
sider the edited image I′in (b) to be better than that in
(c) because the former keeps a better structure of the cas-
tle, leading to minimal changes to the source image. With-
*Corresponding author
(a) Source image
 (b) Edited image
 (c) Edited image
Figure 1. Illustration of the TBIE task. (a): source image I. (b)
and (c): edited images according to the target prompt “a castle
covered by snow”. TBIE considers (b) to be better than (c).
(a) Abduction
 (b) Action & Prediction
Figure 2. Counterfactual inference framework for TBIE.
out loss of generality1, we denote the prompt into two sub-
prompts PandP′, where Pdescribes the image content of
user’s editing intent and P′describes it after editing. For ex-
ample, Pis “a castle” and P′is “a castle covered by snow”.
TBIE is a challenging task as it is inherently zero-shot:
a source image Iand a prompt ( P,P′) are the only input
and there is no ground-truth image for the target image I′.
Fortunately, thanks to the large-scale text-to-image genera-
tive models, e.g., DALL-E [24], Imagen [27], and Stable
Diffusion [25], language embeddings and visual features
are well-aligned. So, they provide a channel to modify im-
ages via natural language. However, the editing efficacy of
existing methods is still far from satisfactory, for example,
they can only support limited edits like style transfer [15],
add/remove objects [1]; do not support user-uploaded im-
ages [9], or require extra supervision [26] and spatial masks
to localize where to edit [1].
Yet, there is no theory that explains why TBIE is chal-
1Any LLM with proper instruction tuning or in-context learning can
interpret the user intent into PandP′. We have deliberately excluded this
module from our formulation.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
9162
P= “A glass of milk.” →P’= “A glass of milk with a straw.”
P= “A dog.” →P’=  “A jumping dog.”
Iterations
Images Iteration 250 Iteration 500 Iteration 750 Iteration 1000
 Iteration 0Manipulation Addition
 Removal
P= “A man.” →P’= “A bald man.”
P = “A woman.” →P’=  “A woman wearing a short skirt.”Replacement
 Style Transfer
P = “A table.” →P’=  “A table in the style of sketch.”Face ManipulationP= “A woman.” →P’=  “A woman in a big smile.”
Figure 3. The editability of counterfactual I′=G(P′, U)de-
creases when the abductive iteration of arg min U∥G(P, U)−I∥
increases.
lenging, or why existing methods sometimes succeed or
fail. Such an absence will undoubtedly hinder progress in
this field. To this end, as illustrated in Figure 2, we formu-
late TBIE as a counterfactual inference problem [22] based
on text-conditional diffusion models, e.g., we use Stable
Diffusion [25] in this paper.
Why Counterfactual? Counterfactual inference can de-
fine the “minimal visual change” requirement formally. As
prompt Pdescribes the existing contents in source image I,
the generative model Gshould be able to generate Ibased
onP. However, Gis usually probabilistic, i.e., only Pis
not enough to control Gto generate an image exactly the
same as I, thus we need an unknown exogenous variable U
to remove the uncertainty:
Fact:I=G(P, U). (1)
Therefore, the “minimal visual change” in TBIE can be for-
mulated as the following counterfactual:
Counterfactual :I′=G(P′, U), (2)
where Uis abducted from Eq. (1) by arg min U∥G(P, U)−
I∥to ensure that the edited image I′preserves most of the
visual content of Iwhile incorporating the influence of P′.
Why Challenging? The abduction of Uis inevitably ill-
posed, i.e.,Uoverfits to the particular PandI. As a result,
(a) Abduction 1
 (b) Abduction 2
 (c) Action & Prediction
Figure 4. The proposed Doubly Abductive Counterfactual infer-
ence framework (DAC).
G(·, U)may ruin the pre-trained prior distribution and fail
to comprehend P′. As shown in Figure 3, as the number of
iterations of arg min U∥G(P, U)−I∥increases, G(P′, U)
generates I′more similar to I, but at the same time, the ed-
itability of G(P′, U)is decreasing. However, it is elusive to
find a good Uthat balances the trade-off between editabil-
ity and fidelity. Thanks to the counterfactual framework,
we conjecture that the success or failure of existing TBIE
methods is primarily attributed to the trade-off (Section 2).
Our Solution . To this end, we propose Doubly Abductive
Counterfactual inference framework (DAC). As illustrated
in Figure 4, following the three steps of counterfactual in-
ference [22]: abduction, action, and prediction, we have:
•Abduction-1 :U= arg min U∥G(P, U, ∆ = 0) −I∥.
•Abduction-2 :∆ = arg min ∆∥G(P′, U,∆)−I∥, where
∆transforms P′back to P.
•Action : set∆′=−∆.
•Prediction :I′=G(P′, U,∆′).
Our key insight stems from the newly introduced exogenous
variable ∆, which is the semantic change editing an imagi-
native I′back to I. Although the overfitting of Abduction-
2 also disables the natural language editability of G, it still
enables the ∆editability. So, by reversing the change from
∆to∆′=−∆, we can use ∆′to edit Iback to I′. We
detail the implementations of Uand∆in Section 3 and
ablate them in Section 4.3. As shown in Figure 5, com-
pared to existing methods, our DAC achieves a good trade-
off between editability and fidelity, and thus we can support
a wide spectrum of user editing intents including 1) addi-
tion, 2) removal, 3) manipulation, 4) replacement, 5) style
transfer, and 6) face manipulation, which are extensively
validated in both qualitative and quantitative evaluations in
Section 4. We summarize our contributions here:
• We formulate text-based image editing (TBIE) into a
counterfactual inference framework, which not only de-
fines TBIE formally but also identifies its challenge: ed-
itability and fidelity trade-off.
• We propose the Doubly Abductive Counterfactual (DAC)
to address the challenge.
• With extensive ablations and comparisons to previous
methods, we demonstrate that DAC shows a considerable
improvement in versatility and image quality.
9163
Input Image DAC SINE ImagicAddition
“A cat wearing 
a wool cap.”“A banana and 
a green apple.”“A teddy wearing 
sunglasses.”“Two parrots 
looking at each 
other.”DDS
“A black cat 
sitting next to 
a mirror.”“A man wearing a 
black T-shirt and 
giving two thumbs up.”Manipulation
“A brown dog.”
 “A cat not 
wearing a hat.”
“An empty glass.”RemovalPrompt
Style Transfer Replacement Face Manipulation
“A pencil drawing 
of an elephant.”“A color photograph 
of a dog.”
“A cartoon of a flamingo.”“A corgi.” “A woman 
holding a baby.”“Three hamsters 
in a bag.”
 “A woman with a smile.”
 “A man with beard.” “A woman 
with blue hair.”
Input Image DAC SINE Imagic DDS Prompt
Figure 5. Comparison of TBIE qualitative examples across the 6 editing types (only prompt P′shown) between our DAC and three SOTAs
with a similar design philosophy (Table 1). For fairness, examples are chosen based on their best visual quality from various random seeds.
See Section 4.1 for analysis and Appendix for the example selection details.
9164
Table 1. Comparisons with existing methods.
Methods U∆ Method Description Failure Analysis
P2P [9] ✗✓
∆can be realized by adjusting
attention or feature mapsInversion methods are not accurate
for reconstruction w/o UTIME [20] ✗✓
PnP [34] ✗✓
MasaCtrl [4] ✗✓
EDICT [36] ✗✓
AIDI [21] ✗✓
CycleDiffusion [38] ✗✓
NTI [18] ✓ ✗ Modeling Uwith textual inversion, i.e., fitting I
with learnable text embeddingsEditability is not enough
for accurate editing w/o modeling ∆PTI [5] ✓ ✗
SINE [43] ✓ ✗ Modeling Uby textual inversion and fine-tuning SD
DDS [10] ✓ ✓ Uand∆are learned together with the distillation loss Uand∆are entangled, hard to find out the
best trade-off between the editability and fidelity Imagic [14] ✓ ✓ Uand∆are learned by fine-tuning SD and textual inversion separately
DAC ✓ ✓ Section 3 Appendix
Notes. In this paper, our purpose is to advocate that TBIE
(or probably any visual editing) should be a counterfac-
tual reasoning task, where the abduction is a necessary and
crucial step. Unfortunately, we haven’t found a non-fine-
tuning-based abductive learning method, and hence we con-
jecture that the absence of abduction is the key reason for
the existing non-fine-tuning-based visual editing methods
being fast yet not effective (e.g., Emu2 [31] and InfEdit
[40]). Perhaps, only LLM can achieve both editing effi-
ciency and effectiveness because LLM may perform coun-
terfactual [33], but this requires unified vision-language to-
kens, which is in itself a challenging open problem.
2. Related Work
Text-to-Image Generation. The success of Imagen [27]
and DALL ·E [24] with diffusion models [11] opens a new
era of open-domain text-to-image generation, being capa-
ble of generating diverse and high-quality images condi-
tional on arbitrarily complex text descriptions. Thanks to
the stable diffusion model [25], the text-to-image diffusion
process could be conducted in a latent space of reduced di-
mensionality, bringing a significant speedup for training and
inference. It is by far the most popular text-to-image model
for open research, and thus we use a pre-trained one [25] as
our generative model G, although the proposed DAC frame-
work is compatible with other generative models.
Text-based Image Editing. We summarize existing TBIE
works in Table 1 from the perspective of counterfactual in-
ference. They can be categorized into three groups based
on whether Uand∆are considered for both editability and
fidelity. Note that we exclude other image editing meth-
ods like DreamBooth [26], Cones2 [17], and Textual inver-
sion [6] that require multiple images for training, which are
different from the TBIE settings covered in this paper.
Group 1: They directly operate the semantic change on
the intermediate UNet attention maps during the genera-
tion process. The fidelity of the input image is achieved by
DDIM inversion [4, 34] or other advanced inversion meth-ods [21, 36, 38], without explicitly modeling U.
Group 2: PTI [5], NTI [18], and SINE [43] calculate Uby
textual inversion or fine-tuning the stable diffusion model
on the source image. Nevertheless, without ∆, they can-
not realize accurate editing, thus techniques like interpola-
tion [5] are needed.
Group 3: Imagic [14] and DDS [10] learn Uand∆together.
However, the entanglement between Uand∆makes it hard
to find out the best trade-off between fidelity and editability.
Visual Counterfactuals. Counterfactual inference is the
answer to a hindsight question like “When Y=yand
X=x, what would have happened to YhadXbeenx′?”.
The general solution [22] to the counterfactual inference
is to abduct the exogenous variables with the known fact
(Y=y, X =x) and then reset our choice ( X=x′) and
obtain the new prediction ( Y= ?). Counterfactual infer-
ence has a wide application in computer vision such as vi-
sual explanations [8], data augmentations [13], robustness
[2, 28, 32], fairness [16, 41], and VQA [19].
3. Method
Recall in Section 1 that our proposed Doubly Abductive
Counterfactual inference framework (DAC) is to address the
non-editability issue caused by the overfitted abduction of
Uthat was originally introduced for the purpose of keeping
minimal visual change. This issue is elegantly resolved by
introducing another abduction of a semantic change vari-
able∆. In this section, we will detail the implementation of
every step in DAC as illustrated in Figure 4.
3.1. Abduction-1
We introduce the implementation of the abduction loss
∥G(P, U, ∆ = 0) −I∥. This step is identical to the con-
ventional abduction of Uin Figure 2, as we set ∆ = 0 in
Figure 4 (a). In particular, we use Stable Diffusion [25] to
implement Gdue to it being open-source and for a fair com-
parison with other methods. As ∥G(P, U, ∆ = 0) −I∥is
essentially a reconstruction loss, we abduct Uby solving
9165
CrossAttn Blocks
(Downsample)
ResNet Blocks
CrossAttn
Blocks
ResNet Blocks
(Upsample)U
U
U
U
USelf-Attention Blocks
CrossAttn Blocks
(Upsample)MLPFigure 6. Parameterizations of Uand∆by using LoRA (grey)
for UNet (blue) and text encoder (red) in pre-trained Stable Diffu-
sion [25]: Θ(U,∆)(xt, t, c). Except for LoRA, all the other param-
eters are frozen.
the following Gaussian noise regression as in training the
reversed diffusion steps:
arg min
UE(t,ϵ)||ϵ−Θ(U,∆=0)(xt, t, P)||2
2, (3)
where ϵ∈ N(0,I),t∈[0, T]is a sampled time step ( T
is the maximum), Θ(U,∆=0) is the pre-trained noise predic-
tion UNet with trainable Uand all other parameters frozen,
conditionally on language tokens of Pencoded by a frozen
CLIP [23] text encoder2,xt=√αtx0+√1−αtϵis the
noisy input at t, in particular, x0=I, and αtis related to a
fixed variance schedule [11, 29].
We parameterize Uas the UNet LoRA [12] in Θ(U,∆).
As shown in Figure 6, the LoRA structure is built on all of
the attention, convolution, and feed-forward (FFN) layers.
This is because we observe the underfitting issue if we only
apply LoRA on the attention layers, i.e.,Icannot be well-
reconstructed using PandU(See ablation in Appendix).
Without loss of generality, we detail the implementation
of a linear layer with a LoRA structure. Denote z∈Rdas
the intermediate feature, W∈Rd×das the parameter of the
linear layer, then the output z′after LoRA becomes:
z′= (W+UA·UB)·z, (4)
where UA∈Rd×randUB∈Rr×dare low rank matrices
withr < d .
3.2. Abduction-2
We introduce the implementation of the second abduction
loss∥G(P′, U,∆)−I∥with the above abducted U(Figure 4
(b)). Similar to Eq. (3), we minimize:
arg min
∆E(t,ϵ)||ϵ−Θ(U,∆)(xt, t, P′)||2, (5)
where we parameterize ∆as the CLIP text encoder LoRA,
andUcalculated in Abduction-1 is frozen.
2As∆is also a LoRA (Section 3.2), ∆ = 0 corresponds to the original,
unmodified encoder.As shown in Figure 6, the LoRA structure is only built
on the attention layers of the CLIP text encoder. The self-
attention layer language feature y′in the CLIP text encoder
is re-encoded from the original ythrough the LoRA:
y′= (W+ ∆ A·∆B)·y, (6)
where ∆A∈Rd×rand∆B∈Rr×dare low rank matrixes,
r << d . By solving Eq. (5), ∆encodes the visual transition
controlled by P′toP. We highlight that ∆cannot be pa-
rameterized by textual inversion [18], as it does not support
semantic inversion as introduced later in Section 3.3.
IfUis overfitted in Abduction-1, e.g.,Umemorizes ev-
erything about I, the Abduction-2 for ∆might be as trivial
as∆ = 0 . Inspired by the findings in diffusion models
where a larger time step corresponds to better editability
while lower fidelity [37], we design an annealing strategy
onUin solving Eq. (5) at different time steps:
z′= (W+γUA·UB)·z, (7)
γ=1−η
T2(t−T)2+η, (8)
where η∈Ris a small constant value. In general, ηis
a hyper-parameter dependent on both Iand(P, P′); fortu-
nately, it is easy to choose a good one as shown in Figure 11.
3.3. Action & Prediction
We introduce the implementation of action & prediction
I′=G(P′, U,∆′)in Figure 4 (c). First, we take the ac-
tion∆′=−∆to revert the visual transition from Pback to
P′to generate I′. Thus, the text LoRA in Eq. (6) becomes:
y′= (W−∆A·∆B)·y. (9)
Then, with a sampled xT∈ N (0,I), the DDIM sam-
pling [29] is used to generate the edited image I′with the
following iterative update from t=Ttot= 0:
xt−1=√αt−1 
xt−√1−αtΘ(U,∆′)(xt, t, P′)√αt!
+
√1−αt−1Θ(U,∆′)(xt, t, P′), (10)
where we obtain I′=x0. Interestingly, as shown in Fig-
ure 10, we use a weight β∈[−1,1]to tune β∆A·∆Bin
Eq. (9) to manifest the inversion ability of ∆, where β=−1
means reconstruction of the source image as in Eq. (6) and
β >−1means that we start to shift the semantic change
from the source image.
4. Experiment
We followed prior works [4, 5, 10, 14, 21, 34, 36, 38] to
use Stable Diffusion as our generator [25]. For fair com-
parisons, we integrated SD checkpoint V2.1-Base with the
9166
official source codes of the comparing methods: SINE [43],
DDS [10], and Imagic [14] in the Diffusers codebase [35]
and we used the same default hyper-parameters of the
SDV2.1-Base. In particular, during the optimization of U
and∆in Abduction-1 and Abduction-2, we set the rank of
the LoRA to 4 for ∆and 512 for U, the learning rate to 1e-
4. Optimization iterations were 1,000 in both Abduction-1
and Abduction-2. η∈[0.4,0.8]is applied to the annealing
strategy. For the action and prediction steps, we adopted 30
steps for DDIM sampling at the inference time of the stable
diffusion. We used an NVIDIA A100 GPU for editing.
Computation Analysis. In general, it took 120, 0.33, 12,
and 15 minutes to edit a single image by using SINE, DDS,
Imagic, and our DAC. Our method consumes 15 minutes,
including 6 and 9 minutes for the first and second abduc-
tion, and 4-second 30-step DDIM sampling. The time-
saving characteristic of DDS lies in minimal trainable pa-
rameters (latent format of an image in DDS compared with
UNet LoRA or CLIP text encoder LoRA in DAC’s abduc-
tion) and minimal optimization iterations (200 iterations in
DDS compared with 1,000 iterations in DAC).
4.1. Qualitative Evaluation
We demonstrate the advantages of the proposed DAC
method with two kinds of qualitative evaluations: 1) eval-
uation of our method with multiple prompts on the same
source image (results are in Appendix), and 2) evaluation of
our method on the 6-type editing operations. For each edit-
ing, we randomly generated 8 edited images given a source
image and an editing prompt, and chose the one with the
best quality as our final edited image. Note that such a pro-
cess is also adopted for other comparison methods. Follow-
ing previous works [14, 43], we collected most images from
a wide range of domains, i.e., free-to-use high-resolution
images from Unsplash (https://unsplash.com/).
Wide Spectrum of Editing. We demonstrate that our DAC
supports a wide spectrum of editing operations including
1) addition, 2) removal 3) manipulation, 4) replacement,
5) style transfer, and 6) face manipulation. Our results are
summarized in Figure 5 and more results are in Appendix.
For one of the 6 editing types, we provide three image-
prompt examples. Take an example for manipulation, we
make two parrots look at each other, change the white cat
with its mirror to a black one, and let a man give two thumbs
up. After the editing, the images not only resemble the
source image to a high degree but also are coherent with the
text prompt, demonstrating that the DAC method achieves
a great trade-off between fidelity and editability.
Comparisons with Competitive Methods. We com-
pare DAC with leading works on the TBIE task including
Imagic [14], SINE [43], and DDS [10]. And they all be-
long to single-image fine-tuning methods for a fair com-
parison. To have a more comprehensive understanding of
Input Image
P’= “add a green apple. ” 
P’= “turn the cat into a baby.”InstructPix2Pix SEED-LLaMA Emu2
P’= “make two parrots look at each other.”Figure 7. Qualitative examples of large-scale training methods.
the superiority of the DAC method, we compare it with the
three methods in the 6 kinds of editing operations in Fig-
ure 5. Compared with previous methods, the DAC method
enjoys the following merits. First, the generated images
by the DAC method are more consistent with the textual
prompts. With prompts such as “remove the milk in the
glass”, and “let two parrots look at each other”, our method
successfully makes it while it is hard for previous meth-
ods. Second, the DAC method can keep better fidelity to the
source image. With prompts like “replace the squirrel with
a corgi” and “remove the white dog”, the edited images by
the DAC resemble the input images to a much higher degree
than previous methods. All of these samples in Figure 5
indicate that the DAC method does a better trade-off be-
tween fidelity and editability, achieving state-of-the-art per-
formance on the TBIE task.
In addition to single-image fine-tuning methods, there
are works that conduct large-scale training and don’t require
any test-time fine-tuning, e.g., InstructPix2Pix [3], SEED-
LLaMA [7], and Emu2 [31]. We have shown that the “fine-
tuning” is the essential “abduction” for fidelity. However,
these methods only have inference-time editing—only “ac-
tion” and “prediction”, thus they cannot guarantee fidelity
in theory (Figure 7 and more results are in Appendix).
4.2. Quantitative Evaluation
CLIP-score [23] and LPIPS [42]. The experimental set-
tings were set as follows.
• Different editing operations need different trade-offs be-
tween fidelity and editability. For example, style transfer
requires lower image alignment compared to object ma-
nipulation. Thus, the evaluations of six kinds of editing
are conducted individually.
• We applied 9 different prompt-image pairs for each kind
of editing.
• We calculated LPIPS for the image alignment and CLIP-
9167
0.7
 0.6
 0.5
 0.4
 0.3
 0.2
Fidelity(Image Aligment)272829303132Editability(T ext Aligment)DAC (Ours)
DDSImagic
SINE
addition
face
manipulation
removal
replacement
styleFigure 8. Image Alignment: minus LPIPS. Text Alignment: CLIP-
score. Both values are the larger the better.
score for text alignment.
We summarize the results in Figure 8. The proposed
DAC method shows better performance in text alignment
scores for editing like object removal, object manipulation,
object addition, and face manipulation. We achieved simi-
lar results with the DDS [10] in object replacement. For the
style transfer, DAC achieves the best text alignment scores.
The LPIPS score measures the image alignment degree be-
tween the source image and the edited image. However, we
argue that LPIPS fails to reflect the fidelity. For example
in Figure 5, “remove the hat of the cat”. Our DAC suc-
cessfully removes the hat and achieves a better CLIP-score.
DDS and SINE methods cannot remove the hat and thus
have a lower CLIP-score. But DDS and SINE achieve a
much higher LPIPS score because they make no changes at
all to the source image. Therefore, we have to conduct a
user study for a more accurate assessment.
User Study. We quantitatively evaluate our DAC with an
extensive human perceptual evaluation study. First, we col-
lected a diverse set of image-prompt pairs, covering all
the “addition”, “manipulation”, “removal”, “style transfer”,
“replacement”, and “face manipulation” types. It consists
of 54 input images and their corresponding target prompts.
75.34.413.76.6Preference  Rates (%)
DAC DDS Imagic SINE
Figure 9. User study
statistics.110 AMT participants were
given a source image, a target
prompt, and 4 edited images by
DAC, DDS, SINE, and Imagic,
which were randomly shown.
The participants are required to
choose the best-edited image. In
total, we recalled 5,940 answers.
The result is summarized in Fig-
ure 9 and it shows that 75.3%
evaluators preferred our DAC.
The user interface is detailed in
Appendix.
P= “A black dog.” →P’=  “A white dog.”
P= “A car.” →P’= “A jeep.”Input ImageChanging △weightβfrom -1 to 1 
P= “A glass of milk.” →P’= “A glass of milk next to a spoon.”Addition Removal
P = “A brown dog and a white dog.” →P’= “A white dog.”Manipulation Replacement
P= “A photograph of a flamingo.” →P’= “A cartoon of a flamingo.”
Style Transfer Face ManipulationP= “A man.” →P’= “A man closing eyes.”
Figure 10. Ablating the weight βforβ∆A·∆Bin Eq. (9).
4.3. Ablation Analysis
Training Iterations and Editability. We exam-
ined the relationship between training iterations of
arg min U∥G(P, U)−I∥and editability by applying six
editing types. As shown in Figure 3, with the dog image
and the prompt “A dog. →A jumping dog”, we can get a
jumping dog in the edited image using 250 and 500 training
iterations. However, the images are with low fidelity. Train-
ingUin 1000 iterations, the generative model fails to make
the dog jump and the edited image looks the same as the
source one, implying good fidelity but poor editability. This
study indicates that with the increase of training iterations
arg min U∥G(P, U)−I∥, the editability decreases while the
fidelity increases, which means a good Uis needed for the
best trade-off between fidelity and editability.
Ablation on ∆Subtraction. In the action & prediction
I′=G(P′, U,∆′), the ∆is reversed to ∆′=−∆. We
use∆′to edit Iback to I′. Nevertheless, considering
∆′=−β∆, there could be different βvalues. We exam-
ined the effects of βvalues on I′. In Figure 10, with the
black dog image and the prompt “A black dog →A white
dog”, increasing βfrom -1 to 1, the black dog changes to a
gray one and then a white one. From the examples in Fig-
ure 10, the learned ∆can be considered as the direction vec-
tor of our desired semantic change. Different βvalues im-
9168
P= “A banana.” →P’=  “A green banana.”
P= “A photograph of a flamingo.” →P’= “A photograph of a flamingo in a desert.”Input Image
P= “A squirrel.” →P’= “A squirrel eating a pine nut.”Addition Removal
P= “A brown dog and a white dog.” →P’= “A white dog.”Manipulation Replacement
P= “A cat.” →P’= “A cat in the style of cartoon.”Style Transfer Face ManipulationP= “A woman.” →P’= “A woman wearing earrings.”
η= 0.2 
 η= 0.4 
 η= 0.6 
 η= 0.8 
Figure 11. Ablating the annealing hyper-parameter ηin Eq. (8).
ply different strengths to apply the semantic change. How-
ever, for rigid manipulations like Addition and Removal, β
does not show a gradual transition, which is reasonable as it
is hard to quantify the existence level of an object.
Ablation on Annealing Strategy. We ablated the anneal-
ing strategy in the Abduction-2. As shown in Figure 11,
we observe that η∈[0.4,0.8]is a reasonable interval for
successful editing. A larger time step in the stable diffu-
sion model corresponds to better editability while lower fi-
delity. The smaller ηindicates that we leverage more priors
of the pre-trained weights at large time steps, thus increas-
ing the editability while decreasing the fidelity. This is con-
sistent with the phenomenon in Figure 11: as ηincreases
from 0.2 to 0.8, the edited images show better fidelity to
the source images although the editability decreases. With
η∈[0.4,0.8], we achieve a good trade-off.
Ablation on Abduction-1. In the Abduction-1, we abduct
Uto encode the content of I, thus guaranteeing a good
fidelity. However, since images contain various contents,
theUabducted from the same settings (e.g., training it-
erations) may not be able to achieve an overfit encoding
for complex images. Then the remaining information will
be abducted in ∆. When we take the action ∆′=−∆
and implement prediction, such information will be sub-
tracted, leading to information loss in I′(the third column
in Figure 12). To make a complement for such informa-
Input Image 𝑈 & 𝑇 𝑈
P= “A cat.” →P’=  “A cat wearing a hat.”
P= “Two parrots.” →P’=  “Two parrots looking at each other.”P= “An egg in a nest.” →P’=  “A bird in a nest.”
Figure 12. Ablation on Abduction-1.
tion, we could introduce another exogenous variable Tpa-
rameterized as the CLIP text encoder LoRA, which satisfies
arg min T∥G(P, U, T, ∆ = 0) −I∥. Finally, the prediction
becomes I′=G(P′, U, T, ∆′)(the second column in Fig-
ure 12). It could be seen that the incorporation of Tin the
Abduction-1 achieves a better fidelity than the abduction of
Uonly. Moreover, conducting iterative abduction on Uand
Tmore times could further improve fidelity. Considering
that the abduction of Uis enough for most cases and the
computation cost produced by the abduction of T, we only
adopt Uin our experiments.
5. Conclusions
We proposed to formulate the task of TBIE using a theo-
retical framework: counterfactual inference, which clearly
explains why the challenge is the trade-off between editabil-
ity and fidelity: the overfitted abduction of the source im-
age parameterization, which is a single-image reconstruc-
tion fine-tuning. To this end, we propose Doubly Abductive
Counterfactual (DAC). The key idea is that, since we cannot
avoid the overfitting of the above abduction, we use another
overfitted abduction, which encodes the semantic change of
the editing, to reverse the lost editability caused by the first
one. We conducted extensive qualitative and quantitative
evaluations on DAC and other competitive methods. Our
future work is two-fold. First, we will upgrade DAC to sup-
port visual example-based editing [17, 26]. Second, we will
use Fast Diffusion Model [39] and Consistency Models [30]
to speed up the fine-tuning and inference in editing.
Acknowledgements. This work was supported by NSFC
project (No. 62232006), in part by Shanghai Science and
Technology Program (No. 21JC1400600), and by National
Research Foundation, Singapore under its AI Singapore
Programme (AISG Award No: AISG2-RP-2021-022).
9169
References
[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended
diffusion for text-driven editing of natural images. In CVPR ,
pages 18208–18218, 2022. 1
[2] Ananth Balashankar, Xuezhi Wang, Ben Packer, Nithum
Thain, Ed Chi, and Alex Beutel. Can we improve model
robustness through secondary attribute counterfactuals? In
Proceedings of the 2021 Conference on Empirical Methods
in Natural Language Processing , pages 4701–4712, 2021. 4
[3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structpix2pix: Learning to follow image editing instructions.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 18392–18402, 2023.
6, 11
[4] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xi-
aohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mu-
tual self-attention control for consistent image synthesis and
editing. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision (ICCV) , pages 22560–22570,
2023. 4, 5
[5] Wenkai Dong, Song Xue, Xiaoyue Duan, and Shumin Han.
Prompt tuning inversion for text-driven image editing using
diffusion models. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , 2023. 4, 5
[6] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-
nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-
Or. An image is worth one word: Personalizing text-to-
image generation using textual inversion. arXiv preprint
arXiv:2208.01618 , 2022. 4
[7] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li,
Xintao Wang, and Ying Shan. Making llama see and draw
with seed tokenizer. arXiv preprint arXiv:2310.01218 , 2023.
6
[8] Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh,
and Stefan Lee. Counterfactual visual explanations. In
ICML , pages 2376–2384, 2019. 4
[9] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
age editing with cross attention control. arXiv preprint
arXiv:2208.01626 , 2022. 1, 4
[10] Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta de-
noising score. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 2328–2337, 2023. 4,
5, 6, 7, 11, 14
[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. NeurIPS , 33:6840–6851, 2020. 4,
5
[12] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021. 5
[13] Divyansh Kaushik, Eduard Hovy, and Zachary C Lip-
ton. Learning the difference that makes a difference
with counterfactually-augmented data. arXiv preprint
arXiv:1909.12434 , 2019. 4
[14] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen
Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:Text-based real image editing with diffusion models. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 6007–6017, 2023. 4, 5, 6, 11
[15] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Dif-
fusionclip: Text-guided diffusion models for robust image
manipulation. In CVPR , pages 2426–2435, 2022. 1
[16] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo
Silva. Counterfactual fairness. Advances in neural informa-
tion processing systems , 30, 2017. 4
[17] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai
Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang
Cao. Cones 2: Customizable image synthesis with multiple
subjects. arXiv preprint arXiv:2305.19327 , 2023. 4, 8
[18] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and
Daniel Cohen-Or. Null-text inversion for editing real im-
ages using guided diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 6038–6047, 2023. 4, 5
[19] Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-
Sheng Hua, and Ji-Rong Wen. Counterfactual vqa: A cause-
effect look at language bias. In CVPR , pages 12700–12710,
2021. 4
[20] Hadas Orgad, Bahjat Kawar, and Yonatan Belinkov. Editing
implicit assumptions in text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , 2023. 4
[21] Zhihong Pan, Riccardo Gherardi, Xiufeng Xie, and Stephen
Huang. Effective real image editing with accelerated iter-
ative diffusion inversion. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 15912–
15921, 2023. 4, 5
[22] Judea Pearl, Madelyn Glymour, and Nicholas P Jewell.
Causal inference in statistics: A primer . John Wiley & Sons,
2016. 2, 4
[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML , pages 8748–8763, 2021. 5, 6
[24] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022. 1, 4
[25] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , pages 10684–
10695, 2022. 1, 2, 4, 5
[26] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In CVPR , pages 22500–22510, 2023. 1, 4, 8
[27] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. In NeurIPS , pages 36479–36494,
2022. 1, 4
9170
[28] Herbert A Simon. Spurious correlation: A causal interpreta-
tion. Journal of the American statistical Association , pages
467–479, 1954. 4
[29] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 5
[30] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya
Sutskever. Consistency models. 2023. 8
[31] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying
Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing
Liu, Tiejun Huang, et al. Generative multimodal models are
in-context learners. arXiv preprint arXiv:2312.13286 , 2023.
4, 6
[32] Kaihua Tang, Jianqiang Huang, and Hanwang Zhang. Long-
tailed classification by keeping the good and removing the
bad momentum causal effect. NeurIPS , 33:1513–1524,
2020. 4
[33] Zenna Tavares, James Koppel, Xin Zhang, Ria Das, and Ar-
mando Solar-Lezama. A language for counterfactual genera-
tive models. In International Conference on Machine Learn-
ing, pages 10173–10182. PMLR, 2021. 4
[34] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali
Dekel. Plug-and-play diffusion features for text-driven
image-to-image translation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 1921–1930, 2023. 4, 5
[35] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro
Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj,
and Thomas Wolf. Diffusers: State-of-the-art diffusion
models. https://github.com/huggingface/
diffusers , 2022. 6
[36] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact
diffusion inversion via coupled transformations. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 22532–22541, 2023. 4, 5
[37] Luozhou Wang, Shuai Yang, Shu Liu, and Ying-cong Chen.
Not all steps are created equal: Selective diffusion distilla-
tion for image manipulation. In ICCV , pages 7472–7481,
2023. 5
[38] Chen Henry Wu and Fernando De la Torre. A latent space
of stochastic diffusion models for zero-shot image editing
and guidance. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 7378–7387, 2023. 4,
5
[39] Zike Wu, Pan Zhou, Kenji Kawaguchi, and Hanwang Zhang.
Fast diffusion model. arXiv preprint arXiv:2306.06991 ,
2023. 8, 15
[40] Sihan Xu, Yidong Huang, Jiayi Pan, Ziqiao Ma, and Joyce
Chai. Inversion-free image editing with natural language.
arXiv preprint arXiv:2312.04965 , 2023. 4
[41] Junzhe Zhang and Elias Bareinboim. Fairness in decision-
making—the causal explanation formula. In AAAI , 2018. 4
[42] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 586–595, 2018. 6[43] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris N
Metaxas, and Jian Ren. Sine: Single image editing with text-
to-image diffusion models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 6027–6037, 2023. 4, 6, 11
9171
