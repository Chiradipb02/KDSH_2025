VLP: Vision Language Planning for Autonomous Driving
Chenbin Pan1*Burhaneddin Yaman2⊠Tommaso Nesti2Abhirup Mallik2
Alessandro G Allievi2Senem Velipasalar1Liu Ren2
1Syracuse University
2Bosch Research North America & Bosch Center for Artificial Intelligence (BCAI)
{cpan14, svelipas }@syr.edu ,
{burhaneddin.yaman,tommaso.nesti,abhirup.mallik,alessandro.allievi,liu.ren }@us.bosch.com
Abstract
Autonomous driving is a complex and challenging task
that aims at safe motion planning through scene under-
standing and reasoning. While vision-only autonomous
driving methods have recently achieved notable perfor-
mance, through enhanced scene understanding, several
key issues, including lack of reasoning, low generalization
performance and long-tail scenarios, still need to be ad-
dressed. In this paper, we present VLP , a novel Vision-
Language-Planning framework that exploits language mod-
els to bridge the gap between linguistic understanding and
autonomous driving. VLP enhances autonomous driving
systems by strengthening both the source memory founda-
tion and the self-driving car’s contextual understanding.
VLP achieves state-of-the-art end-to-end planning perfor-
mance on the challenging NuScenes dataset by achieving
35.9% and 60.5% reduction in terms of average L2 error
and collision rates, respectively, compared to the previous
best method. Moreover, VLP shows improved performance
in challenging long-tail scenarios and strong generalization
capabilities when faced with new urban environments.
1. Introduction
Autonomous driving is a complex problem requiring scene
understanding and reasoning to ensure safe motion plan-
ning. This sophisticated challenge can be broadly divided
into three main tasks, namely perception, prediction and
planning (also known as P3). Conventional methods adopt
a modular approach by developing and optimizing each task
in a disjoint manner without a holistic view, leading to com-
pounding errors and safety concerns [15, 16, 18]. End-to-
end autonomous driving systems (ADS), unifying all P3
tasks, have garnered attention for their potential to enhance
*Work done while interned at Bosch Research North America.
⊠Corresponding author.
00.20.40.60.81
L2 ErrorCollision RateBoston
VADVLP00.20.40.60.81
L2 ErrorCollision RateSingapore
VADVLPFigure 1. New-city generalization ability of ADS for planning is
evaluated by training on Boston city and testing on Singapore,
and vice versa. Our proposed VLP shows strong generalization
ability by significantly outperforming state-of-the-art vision-only
method, V AD[18], in terms of both L2 error and collision rate.
safe planning. Existing vision-based ADS typically follow
a two-stage process: bird’s eye view (BEV) feature extrac-
tion and downstream tasks [16, 18]. BEV feature extraction
transforms multi-view camera data into a structured top-
view representation embedding spatial information around
the ego-car. BEV features are further utilized as the infor-
mation pool by the downstream P3 tasks. While effective,
these vision-only methods struggle with out-of-domain gen-
eralization, such as maintaining performance in new cities
and long-tail scenarios, hindering real-world deployment.
The way humans process and interpret visual driving
scenes naturally involve a coherent cognitive framework
that maintains a consistent logical flow. This enables hu-
mans effortlessly make correct decisions even when faced
with previously unseen data or scenarios, eliminating gen-
eralization issues in driving tasks. Then, this raises a perti-
nent question: How can ADS achieve human-like driving to
navigate diverse and dynamic road environments? Recent
advancements in language models (LMs) have led to un-
precedented common-sense ability and generalization per-
formance across unseen data and tasks for the natural lan-
guage processing domain [5, 8, 25]. The superior common-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
14760
sense capability of LMs have led to the emergence of multi-
modal LMs for diverse applications ranging from medical
imaging to robotics [4, 9, 33]. However, incorporation of
the reasoning ability of LMs into real-world autonomous
driving tasks, to address generalization and long-tail sce-
narios, is yet to be fully-explored.
To bridge this gap, we propose a Vision Language
Planning (VLP) framework, which integrates the common-
sense capability of LMs into vision-based ADS for safe
self-driving. Our VLP consists of two key components:
Agent-centric Learning Paradigm (ALP) and Self-driving-
car-centric Learning Paradigm (SLP), leveraging LMs to
enhance the ADS from reasoning and decision-making as-
pects, respectively.
The BEV feature map serves as the source memory
pool in ADS for downstream decoding tasks. It summa-
rizes and encodes the driving environment surrounding the
self-driving car, including vehicles, pedestrians, lanes, and
more, into a unified feature map. Hence, capturing compre-
hensive and necessary details in each local position of BEV
is critical for safe and precise self-driving performance. To
enhance the local semantic representation and reasoning ca-
pabilities of BEV , we introduce an innovative Agent-centric
Learning Paradigm (ALP) module. ALP integrates the con-
sistent feature space of a pretrained LM to revamp the agent
features on the BEV , actively shaping semantics, and guid-
ing the BEV reasoning process. Leveraging on the common
sense and logic flow embedded in the LM, our ALP equips
the ADS with robustness and consistent BEV feature space,
enhancing its effectiveness in diverse driving scenarios.
In ADS, the planning module aggregates information
from the preceding perception and prediction phases to
make the final decisions for self-driving. This global per-
spective culminates in the formation of a planning query,
directly influencing the safety and accuracy of the self-
driving navigation. Considering the critical role of the plan-
ning module within ADS, we also present a novel Self-
driving-car-centric Learning Paradigm (SLP) to elevate the
decoding and acquiring information ability of the planning
query. In the SLP, we align the planning query with in-
tended goals and the ego-vehicle driving status by lever-
aging the knowledge encoded in the pretrained LM. The
language model’s comprehension capabilities contribute to
more informed decision-making during the planning phase
as well as enabling a more robust planning query formation
process.
Through VLP, we bridge the gap between human-like
reasoning and autonomous driving, enhancing the model’s
contextual awareness and its ability to generalize effectively
in complex, ever-changing real-world scenarios, as shown
in Fig.1. The main contributions of this work are summa-
rized as follows:
• We propose VLP, a Vision Language Planning model,which incorporates reasoning capability of LMs into
vision-based autonomous driving systems as an enhance-
ment of motion planning and self-driving safety.
• VLP is composed of novel components ALP and SLP,
aiming to improve the ADS from self-driving BEV rea-
soning and self-driving decision-making aspects, respec-
tively.
• Through extensive experiments in real-world driving sce-
narios, we show that VLP significantly and consistently
outperforms the state-of-the-art vision-based approaches
across a spectrum of driving tasks, including open-loop
planning, multi-object tracking, motion forecasting, and
more.
• We conduct the first new-city generalization study on
the nuScenes dataset [6] by training and testing on dis-
tinct cities, demonstrating the remarkable zero-shot gen-
eralization ability of our VLP approach over vision-only
methods.
• To the best of our knowledge, this is the first work in-
troducing LM into multiple stages of ADS to address the
generalization ability in new cities and long-tail cases.
2. Related Work
2.1. End-to-End Autonomous Driving
Early end-to-end approaches have adopted a single neural
network to perform motion planning without explicitly de-
signing intermediate tasks, such as prediction [3]. Thus,
such approaches not only suffer from the sub-optimal per-
formance but also lack interpretability. Modular end-to-end
frameworks introduce intermediate tasks for interpretabil-
ity and provide improved performance [7, 10, 15, 16, 18,
23, 28]. These interpretable end-to-end frameworks explic-
itly model perception, prediction and planning (P3) com-
ponents and train them together with a joint optimization
strategy to enhance planning. UniAD [16] and V AD [18]
are two prominent state-of-the-art end-to-end frameworks.
UniAD leverages rasterized scene representations and ex-
plicitly identifies crucial components within the P3 frame-
work. It coordinates all tasks through a unified query design
for safe planning. On the other hand, V AD uses vectorized
representations for efficient planning and safety improve-
ment. While these approaches generally achieve good per-
formance, their generalization performance have been lim-
ited due to lack of common sense and reasoning process.
2.2. Vision-Language Models
Large language models (LLMs), which are trained on mas-
sive amounts of text data, have shown unprecedented suc-
cess in language-related tasks [5, 8, 29]. The capability of
LLMs for common-sense understanding and versatility in
tackling diverse language tasks has driven the exploration
of multi-modal LLMs [20, 21, 27]. Vision-language mod-
els (VLM) integrate visual and textual information to en-
14761
able common sense understanding of the content by train-
ing on large scale open-world vision and text data [2, 17,
20, 27]. CLIP [27] uses a contrastive learning objective to
learn a joint embedding space for vision and text data, and
has shown immense zero-shot generalization performance,
showcasing the importance of VLMs compared to vision-
only approaches.
2.3. Autonomous Driving with Language Models
In recent years, numerous works have been proposed for ex-
tending LMS to the embodied AI domain to improve zero-
shot generalization performance [4, 9, 30, 35]. These works
incorporate sensory inputs from embodied agents in the
form of language with vision and language data for decision
making process [4, 9]. While these methods have mainly
focused on the robotics domain, few works have been pro-
posed for leveraging embodied LMs for autonomous driv-
ing tasks [11, 24, 31, 32, 34]. In particular, DiLu [32]
and GPT-Driver [24] propose GPT-based driver agents for
closed-loop simulation tasks. In [31], an open loop driving
commentator, which combines vision and low-level driving
actions with language, is proposed to interpret driving ac-
tions and for reasoning. Our work significantly differs from
these approaches by leveraging LMs for end-to-end motion
planning task for real-world driving applications.
3. Methodology
We present a Vision Language Planning (VLP) model,
which equips the ADS with the capacity to emulate human
common sense and engage in contextual reasoning for safe
motion planning. Our proposed VLP model, illustrated in
Fig. 2a, comprises two innovative components that lever-
age LMs in both local and global contexts. The Agent-
centric Learning Paradigm (ALP) concentrates on refining
local details to enhance source memory reasoning, while
the Self-driving-car-centric Learning Paradigm (SLP) fo-
cuses on guiding the planning process for the self-driving-
car (SDC). Below, we first provide an overview of how gen-
eral ADS work in Sec.3.1. Subsequently, we describe our
proposed ALP and SLP in detail in Sec. 3.2 and Sec. 3.3,
respectively.
3.1. Preliminary
In vision-based ADS, a sequence of multi-view camera im-
ages serves as the source input, offering abundant visual
data for downstream tasks. Initially, an image backbone
extracts fundamental features for perception. These fea-
tures are then processed by a BEV encoder to transform
multi-view features into a unified 2D representation from a
top-down view. This BEV feature map encapsulates crucial
perception details like object positions, lane markings, and
road boundaries [22]. Acting as an information hub, this
BEV feature map supports diverse downstream tasks suchas 3D object tracking, mapping, motion prediction, occu-
pancy prediction, and path planning [16, 18]. Specific task-
related information is decoded using a query-based trans-
former decoder designed for each task to efficiently access
the source memory [16]. In the final planning stage, an ego-
query models the ego-vehicle’s status and interacts with en-
vironmental features to determine the optimal path for au-
tonomous driving.
3.2. Agent-centric Learning Paradigm: Enhancing
BEV Source Memory
The BEV feature map serves as the foundational source
memory pool and holds a pivotal role in ADS. Ensuring
that a BEV map provides comprehensive information with
intricate details for driving is essential for enabling down-
stream decoders to make safe, precise, and human-like de-
cisions. However, the BEV in ADS is derived from multi-
view camera images rather than a true bird’s-eye-view im-
age, which can introduce discrepancies between the pro-
duced BEV and the expected BEV representation. There-
fore, to mitigate the discrepancies, we introduce the Agent-
centric Learning Paradigm (ALP) to align the produced
BEV with a true bird’s-eye-view map. Through direct su-
pervision on the BEV map with LM, our ALP enables the
refinement of local details and alignment with the desired
human perspective.
Agent BEV features. In our proposed ALP, three kinds
of BEV agents are considered: ego-vehicle (self-driving-
car), foreground (FG) objects, and lane elements. We first
align the ground-truth area of each agent with the produced
BEV map, and crop the regions of interest. We utilize the
3D bounding box to crop the ego-car and FG object area,
and panoptic scene mask to segment the lane area. Subse-
quently, we perform a pooling operation on the obtained
local BEV region, to generate a single feature represen-
tation for the corresponding agent. After pooling, the lo-
cal agent features in each sample along the batch are con-
catenated to formulate an Agent-BEV tensor denoted as
Abev∈RNB×C, where NBandCdenote total number
of agents in the batch and feature dimension, respectively.
The process can be formulated as:
AEgo
bev=Pool(Crop(BEV, bboxEgo
3D)),∈RC,
AFG
bev=Pool(Crop(BEV, bboxFG
3D)),∈RC,
ALane
bev =Pool(Seg(BEV, SegMaskLane)),∈RC,
Abev=Batch ([AEgo
bev;AFG
bev;ALane
bev]),∈RNbatch×C,(1)
where Batch [; ]denotes the concatenation operation,
A(.)
bevrepresents the single agent feature of the correspond-
ing type, bbox(.)
3DandSegMaskLaneindicate the ground-
truth 3D bounding box and the lane segmentation mask, re-
spectively.
Agent expectation features. To ensure that local BEV fea-
tures express the desired information, we conduct a BEV-
14762
BEV EncoderPerceptionPredictionPlanning
Language ModelALPSample-wiseSLPSelf-drivingBEV-reasoningSelf-drivingDecision-making……
……Training-onlyVision Language Planning“The ego-car is {going straight}. Its future trajectory is {…}”“This object is {a truck}. Its 3d bounding box is {…}. Its …”“This area belongs to a divider. Its bboxis {…}. Its …”Agent-wise……“The ego-car is going straight. Its planned trajectory is {…}”ℒ!"#ℒ$"#
Ego-car prompt: This is the self-driving car. It is {class}, and its 3D bounding box is {[cx, cy, w, l, cz, h, sin, cos, vx, vy]}. It is currently {command}. Its future trajectory will be {[[x1, y1]…[xn, yn]]}.Foreground prompt: This object is {class}. Its 3D bounding box is {[cx, cy, w, l, cz, h, sin, cos, vx, vy]}. Its future trajectory will be {[[x1, y1]…[xt, yt]]}.Lane prompt: This lane belongs to a {lane-label}. The label belongs to {thing/stuff} class. Its 2D bounding box on the BEV map is {[cx, cy, w, h]}.Planning prompt: The self-driving car is driving in an urban area. It is currently {command}. The future trajectory of the car for the next P timestamps will be {[[x1, y1]…[xp, yp]]}.a)b)Figure 2. a) The overview of proposed vision language planning (VLP) framework. VLP enhances ADS from self-driving BEV-reasoning
and self-driving decision-making aspects, through two innovative modules, ALP and SLP, respectively. Leveraging LM and contrastive
learning, ALP conducts agent-wise learning for refining local details on BEV , while SLP engages sample-wise learning for advancing
global context understanding ability of the ADS. VLP is only active during training, ensuring no additional parameters or computations are
introduced during inference. b) Prompt formats used in VLP.
expectation alignment process by leveraging LM and con-
trastive learning. We precisely define the perceptual infor-
mation expected from the corresponding agent, such as
agent labels, bounding boxes, and future trajectories. These
driving-related ground-truth information, which should also
be embedded in the local BEV feature is formulated into a
prompt as illustrated in Fig. 2b. The description is then
passed to the language encoder, LM, to generate the cor-
responding agent expectation feature. We apply an MLP
layerFbevto adapt the expectation feature to the BEV fea-
ture space. Then, the agent expectation features are con-
catenated along the batch to generate an Agent-Expectation
tensor denoted as Aexp∈RNB×C. The procedure can be
formulated as in Eq. (2):
AEgo
exp=Fbev(LM(TEgo[yEgo])),∈RC,
AFG
exp=Fbev(LM(TFG[yFG])),∈RC,
ALane
exp =Fbev(LM(TLane[yLane])),∈RC,
Aexp=Batch ([Aego
exp;AFG
exp;Alane
exp]),(2)
whereA(.)
exp,T(.), andy(.)represent the single agent ex-
pectation feature, the description template, and the ground-
truth in ADS for the respective agent. Note that during train-
ing, we freeze the LM and only train the adaptation layer
Fbevto save memory and retain the pre-trained knowledge
in the LM.
Contrastive Learning Formulation in ALP. Given the
agent BEV feature Abevand the agent expectation feature
Aexp, the alignment between the produced BEV and the
expected BEV is facilitated through a contrastive learning
process [27]. In particular, L2 normalization is applied to
bothAbevandAexpto standardize the feature vectors and
ensure training stability. Then, matrix multiplication is per-
formed between the normalized two-modality tensors witha learnable logit scale αalpto produce a similarity matrix
Spred∈RNB×NBin agent-wise. The ground truth for the
similarity matrix, Sgt, is a unit matrix, signifying that each
agent’s BEV feature should be closest to its corresponding
expected feature in the shared space. To optimize this align-
ment, we apply a cross-entropy loss, LCE, along both the
Agent-BEV mode axis and Agent-Expectation mode axis of
theSpred, and compute an average value as the final ALP
loss,Lalp, for the BEV source reasoning part. The entire
process can be formally expressed as:
Spred=αalp×(Abev
∥Abev∥2⊗Aexp
∥Aexp∥2),
Lalp= (LCE(Spred,Sgt, dim = 0)
+LCE(Spred,Sgt, dim = 1)) /2.(3)
3.3. SDC-centric Learning Paradigm: Enhancing
Ego-vehicle Query
Ego-vehicle query feature. The ego-vehicle dynamic mod-
eling is the core function of the ADS. In previous ADS,
a trainable ego-query Equery is applied to interact with
other agents, AFG
query andALane
query , on BEV map, to gather
the self-driving perception/prediction information. The pro-
duced ego-car query feature Eqfeat is further processed to
predict the future waypoints of the self-driving car yplan
pred.
The process is formulated as in Eq. (4):
Eqfeat =Minter(Equery,AFG
query,Alane
query)∈RB×C,
yplan
pred=Mplan(Eqfeat)∈RB×P×2,(4)
whereMinterandMplandenote the intermediate ego in-
formation gathering module and the planning head in ADS,
respectively. P×2indicate the planned waypoints in the
next P timesteps, and Bis the batch size during training.
14763
Although such mechanism can achieve good planning re-
sults, solely relying on numeric ground-truth can make it
challenging to understand the rationale behind the ADS’ de-
cisions, which can result in inconsistent feature learning for
the planning and lead to limited adaptability to new envi-
ronments. To address these limitations, we propose a SDC-
centric Learning Paradigm (SLP) to equip the ADS with the
capability of making decisions from continuous and robust
feature space.
Ego-vehicle planning feature. We template a language de-
scription of the ego-vehicle status with the planning ground
truth (GT) including high-level driving command and the
future trajectory of the ego-vehicle, as illustrated in Fig. 2b.
The prompt is sent to the LM to obtain the ground truth
planning features of the ego-car embedding driving scenario
information and human driving logic. An MLP layer Fego
is applied to adapt the textual planning features to the ego-
query feature space. The process can be expressed as in
Eq. (5):
Eprompt =Fego(LM(TEgo
slp[yplan]))∈RB×C,(5)
where TEgo
slpindicates the prompt for ego-car description
used in SLP, and yplandenotes the planning ground truth.
Note that the LM is a shared off-the-shelf model for both
ALP and SLP. As in ALP, only the adaptation layer Fego
is trainable during training to save memory and retain the
pre-trained knowledge in LM.
Contrastive Learning Formulation in SLP. Similar to
ALP, we employ a sample-wise contrastive learning ap-
proach for SLP to align the ego-vehicle query feature with
the ego-vehicle textual planning feature as follows:
Sslp
pred=αslp×(Eqfeat
∥Eqfeat∥2⊗Eprompt
∥Eprompt ∥2),
Lslp= (LCE(Sslp
pred,Sslp
gt, dim = 0)
+LCE(Sslp
pred,Sslp
gt, dim = 1)) /2,(6)
where αslpis a learnable logit scale, and Sslp
predandSslp
gt
are the predicted and ground truth similarity matrices, re-
spectively. Aligning the two modes closely in the feature
space, the contrastive process refines the produced ego-
vehicle query using the ground truth-embedded linguistic
feature. The sample-wise contrastive loss rectifies the re-
lationships between individual samples from a human (lin-
guistic) perspective, establishing a connection for the cur-
rent data point with the world-wise common sense embed-
ded in the pretrained language model.
3.4. Training Loss
The overall loss, Lvlp, of our VLP training is composed of
two parts: the BEV encoder reasoning loss Lencand the
decoder decision-making loss Ldecsuch that:Lvlp=ωencLenc+ωdecLdec, (7)
where ωencandωdecrepresent the weights for Lencand
Ldec, respectively.
The encoder reasoning loss, Lenc, is equal to the loss
produced in ALP module, i.e. Lenc=Lalp. The decision-
making loss includes the losses in all downstream tasks
(perception/prediction/planning) in ADS and the SLP loss:
Ldec=Lperc+Lpred+Lplan+Lslp. (8)
Note that the off-the-shelf LM head is discarded during
inference, and thus, our method introduces no additional pa-
rameters and computations to the original ADS.
4. Experiments
4.1. Implementation Details
Dataset. We conduct experiments on the challenging
nuScenes dataset [6], which is the first large-scale pub-
lic dataset to provide data from the entire sensor suite of
an autonomous vehicle (6 cameras, 1 LIDAR, 5 RADAR,
GPS and IMU sensors). The nuScenes contains 1000 driv-
ing scenes from Boston and Singapore, two cities that are
known for their dense traffic and highly challenging driving
conditions. In our experiments, we utilize 6 camera images
as our vision inputs.
Baselines. We adopt two recent SOTA models in au-
tonomous driving, namely UniAD [16] and V AD [18], as
our baselines to evaluate the effectiveness of our approach.
Training. We incorporate our proposed VLP training
strategy into vision-only approaches of UniAD [16] and
V AD [18], and refer to this enriched versions as VLP-
UniAD and VLP-V AD, respectively, for clarity when re-
porting the results. All methods are trained with the same
hyper-parameters reported in the respective baselines [16,
18] for commensurate comparison. For language model,
we use the pretrained LM in CLIP [27]. In VLP study, we
set T=6 (3 seconds) for planning, which is same as baselines
[16] [18]. Experiments are conducted with 8 NVIDIA Tesla
A100 GPUs. More details regarding experimental setup can
be found in supplementary materials.
4.2. Open-loop Planning
Planning constitutes the cornerstone of any ADS, playing a
pivotal role in ensuring safety and devising efficient routes
for the ego-car. In Tab. 1, we present a series of com-
parative experiments that showcase the performance of our
open-loop planning in comparison to the baseline models.
As can be seen in rows 4-6 of the table, the integration of
just SLP leads to noticeable reductions in both the L2 er-
ror and collision rates for all the baseline models. Moving
down the table, rows 7 to 9 demonstrate that the inclusion
14764
of both VLP components (SLP and ALP together) consis-
tently yields further improvements in these planning met-
rics. In particular, VLP-UniAD shows a 28.1% and 48.4%
reduction in terms of average L2 error and collision rate, re-
spectively, compared to baseline UniAD. Similarly, in com-
parison with V AD, VLP-V AD achieves 35.9% and 60.5%
reduction for average L2 error and collision rate, respec-
tively. These significant results underscore the effectiveness
of both SLP and ALP, as well as their adaptability across
various ADS configurations. The reduced L2 error and col-
lision rate achieved through VLP integration contribute to
safer road planning in the realm of self-driving.
4.3. Perception and Prediction
In this section, we showcase the consistent effectiveness of
our proposed VLP across a spectrum of essential driving
tasks. Our proposed approach excels in various perception
and prediction tasks, including multi-object tracking, map-
ping, motion forecasting, occupancy prediction, 3D object
detection, and vectorized scene segmentation. These results
underscore the versatility and reliability of our VLP frame-
work across a wide range of critical driving tasks.
Multi-Object Tracking. The results for multi-object track-
ing (MOT) are presented in Tab. 2. We apply the stan-
dard evaluation protocols of AMOTA (Average Multi-
Object Tracking Accuracy), AMOTP (Average Multi-
Object Tracking Precision), Recall, and IDS (Identity
Switches) to evaluate the MOT performance. As can be
seen, with the incorporation of SLP, AMOTA, Recall, and
IDS values all surpass those of UniAD, while AMOTP ex-
periences a slight degradation. The combined integration of
both SLP and ALP results in improvements across all met-
rics when compared to the UniAD baseline. The enhanced
performance in MOT proves that our VLP can help the sys-
tem better predict and respond to the movements of other
objects on the road, reducing the risk of accidents.
Online Mapping. In Tab. 3, we present the results of online
mapping, encompassing four key mapping elements: lanes,
drivable areas, dividers, and pedestrian crossings. The eval-
uation employs the Intersection-over-Union (IoU) metric
to measure the overlap between the predicted and ground-
truth maps. The results reveal that the inclusion of the SLP
leads to improvements in mapping accuracy for all four el-
ements. Specifically, it enhances the IoU for drivable areas
and crossings by 5.5% and 6.5%, respectively. Furthermore,
the integration of both SLP and ALP yields even further
improvement, with IoU achieving by 6.6% and 10.2% for
drivable areas and crossings, respectively. These enhance-
ments represent substantial gains compared to the SLP-only
configuration. These results underscore the valuable con-
tributions of each paradigm in online mapping. They also
demonstrate the effectiveness of VLP in bridging the gap
between vision-based and language-based information, ul-timately enhancing the system’s comprehension of various
road elements.
Motion Forecasting. We present the motion forecasting
results in Tab. 4, applying the same evaluation metrics as
those in UniAD [16], namely minADE (minimum Average
Displacement Error), minFDE (minimum Final Displace-
ment Error), MR (Miss Rate), and EPA (End-to-end Predic-
tion Accuracy). The results demonstrate that the inclusion
of the SLP contributes to a reduction in minADE, minFDE,
and MR, while simultaneously increasing EPA for the ADS.
This outcome signifies that the supervision and refinement
applied to the final ego-car feature also has a positive in-
fluence on the motion prediction of other vehicles around
the ego-car. The improvements are more obvious with the
EPA metrics. Equipped with full components of VLP, the
performance of the ADS, in motion prediction, experiences
further enhancements. This observation underscores the ca-
pability of ALP to empower the system in the identification
and accurate prediction of the movements of various agents
on the road.
Occupancy Prediction. We present the occupancy predic-
tion results in Tab. 5, which have been obtained using the
IoU and Video Panoptic Quality (VPQ) within two distance
ranges around the ego-car, namely near (”-n.”), covering a
30×30marea, and far (”-f.”), spanning a 50×50marea.
The results highlight the effectiveness of the SLP in consis-
tently improving all four metrics. SLP, primarily focusing
on enhancing the ego-car feature, demonstrates consistent
performance gains in occupancy prediction. The inclusion
of ALP with SLP shows similar improvement over the base-
line as well. Since both configurations of the VLP prioritize
attention to the ego-car during training, more improvements
are observed within the nearby areas.
3D Object Detection. We show the 3D object detec-
tion results in Tab. 6 using nuScenes detection metrics of
mean Average Precision (mAP), Average Translation Er-
ror (ATE), Average Scale Error (ASE), and nuScenes de-
tection score (NDS) as indicated in [6]. As observed in the
table, the incorporation of the VLP consistently leads to im-
provements in NDS. Notably, models incorporating both the
SLP and ALP generally perform better than those with SLP
alone, providing empirical evidence of the ALP’s effective-
ness in enhancing the local BEV representation ability.
Vectorized Scene Segmentation in VAD. The V AD [18]
framework represents the driving scene as a fully vectorized
structure, categorizing map elements into road boundaries,
dividers, and pedestrian crossings. Thus, vectorized scene
segmentation experiments are conducted within the V AD
framework, with IoU serving as the evaluation metric. As
depicted in Tab. 7, the introduction of SLP yields substan-
tial improvements. More specifically, the segmentation for
boundaries, dividers, and crossings sees improvements of
14.9%, 19.2%, and 23.1%, respectively, contributing to an
14765
ID Model SLP ALPL2 (m) ↓ Col. Rate (%) ↓
1s 2s 3s Avg. 1s 2s 3s Avg.
0 NMP [36] - - 2.31 - - - 1.92
1 SA-NMP [36] - - 2.05 - - - 1.59
2 FF [14] 0.55 1.20 2.54 1.43 0.06 0.17 1.07 0.43
3 EO [19] 0.67 1.36 2.78 1.60 0.04 0.09 0.88 0.33
4 ST-P3 [15] 1.33 2.11 2.90 2.11 0.23 0.62 1.27 0.71
5 UniAD [16] 0.48 0.96 1.65 1.03 0.05 0.17 0.71 0.31
6 VLP-UniAD ✓ 0.43 0.86 1.47 0.92 0.03 0.15 0.48 0.22
7 VLP-UniAD ✓ ✓ 0.36 0.68 1.19 0.74 0.03 0.12 0.32 0.16
8 V AD [18] 0.46 0.76 1.12 0.78 0.21 0.35 0.58 0.38
9 VLP-V AD ✓ 0.26 0.47 0.78 0.50 0.12 0.17 0.42 0.23
10 VLP-V AD ✓ ✓ 0.30 0.53 0.84 0.55 0.01 0.07 0.38 0.15
Table 1. Open-loop planning performance. VLP achieves significant end-to-end planning performance improvement over counterpart
vision only UniAD and V AD methods on the nuScenes validation dataset [6]. Based on the planning results, we can conclude that both
SLP and ALP components plays a vital role to ensure safe motion planning.
Model SLP ALP AMOTA ↑AMOTP ↓Recall↑IDS↓
ViP3D [10] 21.7 1.625 36.3 -
QD3DT [13] 24.2 1.518 39.9 -
MUTR3D [37] 29.4 1.498 42.7 3822
UniAD [16] 35.9 1.320 46.7 906
VLP-UniAD ✓ 36.6 1.332 46.8 820
VLP-UniAD ✓ ✓ 36.8 1.315 47.3 678
Table 2. Multi-object tracking. VLP shows improved perfor-
mance over vision-only MOT techniques.
Model SLP ALP Lanes↑Drivable ↑Divider ↑Crossing ↑
UniAD [16] 31.3 69.1 25.7 13.8
VLP-UniAD ✓ 32.0 69.5 27.1 14.7
VLP-UniAD ✓ ✓ 32.3 70.2 27.4 15.2
Table 3. Online mapping. VLP demonstrates improved segmen-
tation IoU ( %) performance over the UniAD.
Model SLP ALP minADE(m) ↓minFDE(m) ↓MR↓EPA↑
PnPNet [23] 1.15 1.95 0.226 0.222
ViP3D [10] 2.05 2.84 0.246 0.226
UniAD [16] 0.71 1.02 0.151 0.456
VLP-UniAD ✓ 0.72 1.04 0.154 0.459
VLP-UniAD ✓ ✓ 0.68 0.98 0.133 0.460
V AD [18] 0.78 1.07 0.121 0.598
VLP-V AD ✓ 0.77 1.03 0.110 0.621
VLP-V AD ✓ ✓ 0.77 1.05 0.115 0.621
Table 4. Motion forecasting. VLP achieves better motion fore-
casting over counterpart vision based methods.
overall increase of 18.6% in the mean IoU (mIoU). VLP
with SLP and ALP also achieves similar improvement over
the V AD baseline. This increased accuracy is crucial for
developing a safe and efficient self-driving system.
4.4. Generalization
Autonomous vehicles are intended to operate in diverse en-
vironments. Generalization allows a system to be deployed
in various urban landscapes, suburban areas, or even ruralModel SLP ALP IoU-n.↑IoU-f.↑VPQ-n. ↑VPQ-f. ↑
FIERY [12] 59.4 36.7 50.2 29.9
StretchBEV [1] 55.5 37.1 46.0 29.0
ST-P3 [15] - 38.9 - 32.1
UniAD [16] 63.4 40.2 54.7 33.5
VLP-UniAD ✓ 64.2 40.7 55.8 34.5
VLP-UniAD ✓ ✓ 64.1 40.2 55.9 34.1
Table 5. Occupancy prediction. VLP shows improvement in both
near ( 30×30m) and far ( 50×50m) ranges, denoted as “n.” and
“f.”, respectively.
Model SLP ALP mAP↑mATE↓mASE↓NDS↑
V AD [18] 0.27 0.70 0.30 0.389
VLP-V AD ✓ 0.27 0.67 0.30 0.394
VLP-V AD ✓ ✓ 0.28 0.67 0.30 0.406
Table 6. 3D object detection. VLP achieves improved object
detection performance over the baseline.
Model SLP ALP Boundary ↑Divider ↑Crossing ↑mIoU↑
V AD [18] 45.6 42.2 31.6 39.8
VLP-V AD ✓ 52.4 50.3 38.9 47.2
VLP-V AD ✓ ✓ 49.4 48.4 39.2 45.7
Table 7. Vectorized scene segmentation. VLP shows improved
segmentation IoU ( %) performance over the V AD.
settings, making the technology applicable and accessible
on a broader scale. Long-tail cases, which are scenarios that
occur infrequently, are often underrepresented in training
data. Generalization helps the system cope with these rare
but critical situations, reducing the risk of biased decision-
making based on inadequate exposure during training. In
this section, we evaluate the VLP in terms of generalization
ability to new cities and long-tail cases.
New-city Generalization. To keep the same number of
multi-view camera inputs, we construct a multi-city dataset
exclusively from nuScenes, by encompassing data from two
14766
Model VLPBoston Singapore
Avg.L2 ↓Avg.Col ↓Avg.L2 ↓Avg.Col ↓
UniAD [16] 1.24 0.32 1.05 0.37
VLP-UniAD ✓ 1.14 0.26 0.87 0.34
V AD [18] 0.86 0.27 0.78 0.39
VLP-V AD ✓ 0.73 0.22 0.63 0.20
Table 8. New-city generalization. To evaluate the generalization
performance, we train the model on Boston and test it on Singa-
pore, and vice versa. VLP shows remarkable zero-shot generaliza-
tion performance improvement over the vision-only methods.
distinct urban environments, Boston and Singapore. To
comprehensively assess generalizability, we conduct two
sets of experiments: (i) training on Boston and testing on
Singapore; and (ii) conversely, training on Singapore and
testing on Boston. The results, presented in Tab. 8, reveal
a noteworthy reduction in planning L2 error and collision
rates for the baselines in both scenarios with the integra-
tion of VLP. In particular, VLP-V AD achieves strong gen-
eralization over counterpart vision-only V AD approach (in
Boston: 15.1% and 18.5%, in Singapore: 19.2% and 48.7%
reduction in terms of average L2 error and collision rates,
respectively). This signifies VLP’s prowess in enhancing
the safety and reliability of the ADS as well as its capac-
ity to transcend training confines and excel in diverse real-
world conditions.
Long-tail Generalization. We assess the performance
of UniAD in Multi-Object Tracking (MOT) and V AD in
3D detection concerning long-tail scenarios, as detailed in
Tab. 9a and Tab. 9b, respectively. Utilizing the long-tail
split methodology from [26], derived from per-class ob-
ject counts, we observe that VLP consistently enhances
the generalization capabilities of both frameworks. Specif-
ically, VLP provides substantial improvements, increasing
AMOTA and Recall by 3.7% and 7.4%, respectively, for
UniAD. In the case of V AD, VLP improves the mean Av-
erage Precision (mAP) by 15.9%. These results highlights
the robust feature space cultivated by our proposed VLP,
emphasizing its efficacy in handling long-tail scenarios.
4.5. Ablation Studies
Effectiveness of SLP and ALP. VLP comprises ALP and
SLP components. Extensive experiments presented in prior
tables show the importance and necessity of both SLP and
ALP for enhanced planning and safety.
Prompt Format. The design of the prompt format is pivotal
for the success of our VLP, serving as the structured input
that guides the learning process. We have performed abla-
tion studies to systematically investigate the impact of dif-
ferent components of the ground truth information included
in the prompt. The driving ground truth encompasses labels,
bounding boxes, trajectories, and high-level commands. To
demonstrate the indispensability of including all aspects ofModel VLP AMOTA ↑AMOTP ↓Recall↑IDS↓
UniAD[16] 29.6 1.446 39.2 68
VLP-UniAD ✓ 30.7 1.435 42.1 66
(a) Multi-object tracking on long-tail cases.
Model VLP mAP↑mATE↓mASE↓mAOE↓
V AD [18] 17.6 0.79 0.33 0.95
VLP-V AD ✓ 20.4 0.75 0.33 0.83
(b) Object detection on long-tail cases.
Table 9. Long-tail generalization. VLP shows consistent im-
provement for long-tail cases accross both a) multi-object tracking
and b) object detection tasks.
label-gt bbox-gt traj-gt commd-gt Avg.L2 ↓Avg.Col ↓
✗ 0.64 0.23
✗ 0.56 0.30
✗ 0.59 0.36
✗ 0.59 0.26
✓ ✓ ✓ ✓ 0.52 0.17
Table 10. Ablation for prompt information. All ground-truth
information components contributes to improved planning perfor-
mance. We provide full task ablation results in the supplementary.
the ground truth in the prompt, we conducted experiments
where each element was selectively removed in individual
settings. The results, presented in Tab. 10, clearly indicate
that the exclusion of any ground truth component leads to
a degradation in planning ability. This underscores the ne-
cessity of incorporating the entire spectrum of ground truth
information in the prompt for optimal performance.
5. Conclusion
We have introduced a novel Vision-Language-Planning
(VLP) approach to enhance the capabilities of Autonomous
Driving Systems (ADS). Our approach leverages both self-
driving-car-centric learning paradigm (SLP) and agent-wise
learning paradigm (ALP) guided by language prompts to
create a comprehensive understanding of the environment.
Through a series of experiments on various driving tasks,
we have demonstrated the effectiveness of our VLP ap-
proach in improving perception, prediction, and planning
aspects of ADS. The generalization experiments showcased
the robustness of our VLP approach, proving its adaptabil-
ity to new cities and long-tail cases. By extending the
capabilities of ADS beyond the training environment, our
VLP approach paves the way for safer and more reliable
autonomous driving in real-world conditions.
Limitations. Our experiments are currently confined to the
nuScenes dataset and camera modality as baseline vision-
based approaches. We will assess VLP on a broader range
of datasets and sensor modalities in our future work.
14767
References
[1] Adil Kaan Akan and Fatma G ¨uney. Stretchbev: Stretch-
ing future instance prediction spatially and temporally. In
European Conference on Computer Vision , pages 444–460.
Springer, 2022. 7
[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katherine Millican, Malcolm Reynolds, et al. Flamingo: a
visual language model for few-shot learning. Advances in
Neural Information Processing Systems , 35:23716–23736,
2022. 3
[3] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski,
Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D
Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al.
End to end learning for self-driving cars. arXiv preprint
arXiv:1604.07316 , 2016. 2
[4] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen
Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,
Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2:
Vision-language-action models transfer web knowledge to
robotic control. arXiv preprint arXiv:2307.15818 , 2023. 2,
3
[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in neural in-
formation processing systems , 33:1877–1901, 2020. 1, 2
[6] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-
ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-
modal dataset for autonomous driving. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 11621–11631, 2020. 2, 5, 6, 7
[7] Sergio Casas, Abbas Sadat, and Raquel Urtasun. Mp3: A
unified model to map, perceive, predict and plan. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 14403–14412, 2021. 2
[8] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al. Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311 , 2022. 1, 2
[9] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-
e: An embodied multimodal language model. arXiv preprint
arXiv:2303.03378 , 2023. 2, 3
[10] Junru Gu, Chenxu Hu, Tianyuan Zhang, Xuanyao Chen,
Yilun Wang, Yue Wang, and Hang Zhao. Vip3d: End-to-end
visual trajectory prediction via 3d agent queries. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 5496–5506, 2023. 2, 7
[11] Md Zahid Hasan, Jiajing Chen, Jiyang Wang, Ameya Joshi,
Senem Velipasalar, Chinmay Hegde, Anuj Sharma, and
Soumik Sarkar. Vision-language models can identify dis-
tracted driver behavior from naturalistic videos. arXiv
preprint arXiv:2306.10159 , 2023. 3[12] Anthony Hu, Zak Murez, Nikhil Mohan, Sof ´ıa Dudas, Jef-
frey Hawke, Vijay Badrinarayanan, Roberto Cipolla, and
Alex Kendall. Fiery: Future instance prediction in bird’s-
eye view from surround monocular cameras. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 15273–15282, 2021. 7
[13] Hou-Ning Hu, Yung-Hsu Yang, Tobias Fischer, Trevor Dar-
rell, Fisher Yu, and Min Sun. Monocular quasi-dense 3d
object tracking. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 45(2):1992–2008, 2022. 7
[14] Peiyun Hu, Aaron Huang, John Dolan, David Held, and
Deva Ramanan. Safe local motion planning with self-
supervised freespace forecasting. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 12732–12741, 2021. 7
[15] Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi
Yan, and Dacheng Tao. St-p3: End-to-end vision-based au-
tonomous driving via spatial-temporal feature learning. In
European Conference on Computer Vision , pages 533–549.
Springer, 2022. 1, 2, 7
[16] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,
Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai
Wang, et al. Planning-oriented autonomous driving. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 17853–17862, 2023. 1, 2, 3,
5, 6, 7, 8
[17] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representa-
tion learning with noisy text supervision. In International
conference on machine learning , pages 4904–4916. PMLR,
2021. 3
[18] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jia-
jie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang
Huang, and Xinggang Wang. Vad: Vectorized scene rep-
resentation for efficient autonomous driving. arXiv preprint
arXiv:2303.12077 , 2023. 1, 2, 3, 5, 6, 7, 8
[19] Tarasha Khurana, Peiyun Hu, Achal Dave, Jason Ziglar,
David Held, and Deva Ramanan. Differentiable raycasting
for self-supervised occupancy forecasting. In European Con-
ference on Computer Vision , pages 353–369. Springer, 2022.
7
[20] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for uni-
fied vision-language understanding and generation. In In-
ternational Conference on Machine Learning , pages 12888–
12900. PMLR, 2022. 2, 3
[21] Liunian Harold Li, Mark Yatskar, D Yin, CJ Hsieh, and
KW Chang. Visualbert: A simple and performant base-
line for vision and language. arxiv 2019. arXiv preprint
arXiv:1908.03557 . 2
[22] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-
hao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer:
Learning bird’s-eye-view representation from multi-camera
images via spatiotemporal transformers. In European con-
ference on computer vision , pages 1–18. Springer, 2022. 3
[23] Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu,
Sergio Casas, and Raquel Urtasun. Pnpnet: End-to-end per-
14768
ception and prediction with tracking in the loop. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 11553–11562, 2020. 2, 7
[24] Jiageng Mao, Yuxi Qian, Hang Zhao, and Yue Wang.
Gpt-driver: Learning to drive with gpt. arXiv preprint
arXiv:2310.01415 , 2023. 3
[25] OpenAI. Introducing chatgpt, 2023. 1
[26] Neehar Peri, Achal Dave, Deva Ramanan, and Shu Kong.
Towards long-tailed 3d detection. In Conference on Robot
Learning , pages 1904–1915. PMLR, 2023. 8
[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 2, 3, 4, 5
[28] Abbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu,
Pranaab Dhawan, and Raquel Urtasun. Perceive, predict,
and plan: Safe motion planning through interpretable seman-
tic representations. In Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part XXIII 16 , pages 414–430. Springer, 2020.
2
[29] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efficient foundation language models.
arXiv preprint arXiv:2302.13971 , 2023. 2
[30] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,
Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandku-
mar. V oyager: An open-ended embodied agent with large
language models. arXiv preprint arXiv:2305.16291 , 2023. 3
[31] Wayve. Lingo-1: Exploring natural language for au-
tonomous driving. https://wayve.ai/thinking/
lingo - natural - language - autonomous -
driving/ , 2023. 3
[32] Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao
Ma, Pinlong Cai, Min Dou, Botian Shi, Liang He, and
Yu Qiao. Dilu: A knowledge-driven approach to au-
tonomous driving with large language models. arXiv
preprint arXiv:2309.16292 , 2023. 3
[33] Shawn Xu, Lin Yang, Christopher Kelly, Marcin Sieniek,
Timo Kohlberger, Martin Ma, Wei-Hung Weng, Attila Ki-
raly, Sahar Kazemzadeh, Zakkai Melamed, et al. Elixr: To-
wards a general purpose x-ray artificial intelligence system
through alignment of large language models and radiology
vision encoders. arXiv preprint arXiv:2308.01317 , 2023. 2
[34] Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo,
Kenneth KY Wong, Zhenguo Li, and Hengshuang Zhao.
Drivegpt4: Interpretable end-to-end autonomous driving via
large language model. arXiv preprint arXiv:2310.01412 ,
2023. 3
[35] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter
Abbeel, and Dale Schuurmans. Foundation models for de-
cision making: Problems, methods, and opportunities. arXiv
preprint arXiv:2303.04129 , 2023. 3[36] Wenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin
Yang, Sergio Casas, and Raquel Urtasun. End-to-end in-
terpretable neural motion planner. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8660–8669, 2019. 7
[37] Tianyuan Zhang, Xuanyao Chen, Yue Wang, Yilun Wang,
and Hang Zhao. Mutr3d: A multi-camera tracking frame-
work via 3d-to-2d queries. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 4537–4546, 2022. 7
14769
