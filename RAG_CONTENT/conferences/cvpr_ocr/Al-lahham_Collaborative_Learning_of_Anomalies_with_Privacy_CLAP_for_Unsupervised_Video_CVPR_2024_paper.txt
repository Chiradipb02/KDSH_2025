Collaborative Learning of Anomalies with Privacy (CLAP) for Unsupervised
Video Anomaly Detection: A New Baseline
Anas Al-lahham Muhammad Zaigham Zaheer Nurbek Tastan Karthik Nandakumar
Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)
Abu Dhabi, UAE
{anas.al-lahham, zaigham.zaheer, nurbek.tastan, karthik.nandakumar }@mbzuai.ac.ae
Abstract
Unsupervised (US) video anomaly detection (VAD) in
surveillance applications is gaining more popularity re-
cently due to its practical real-world applications. As
surveillance videos are privacy sensitive and the availabil-
ity of large-scale video data may enable better US-VAD sys-
tems, collaborative learning can be highly rewarding in this
setting. However, due to the extremely challenging nature
of the US-VAD task, where learning is carried out without
any annotations, privacy-preserving collaborative learning
of US-VAD systems has not been studied yet. In this pa-
per, we propose a new baseline for anomaly detection ca-
pable of localizing anomalous events in complex surveil-
lance videos in a fully unsupervised fashion without any la-
bels on a privacy-preserving participant-based distributed
training configuration. Additionally, we propose three new
evaluation protocols to benchmark anomaly detection ap-
proaches on various scenarios of collaborations and data
availability. Based on these protocols, we modify existing
VAD datasets to extensively evaluate our approach as well
as existing US SOTA methods on two large-scale datasets
including UCF-Crime and XD-Violence. All proposed eval-
uation protocols, dataset splits, and codes are available
here: https://github.com/AnasEmad11/CLAP.
1. Introduction
Recent years have seen a surge in federated learning based
methods, where the goal is to enable collaborative training
of machine learning models without transferring any train-
ing data to a central server. This direction of research in ma-
chine learning is of notable importance as it enables learn-
ing with multiple participants that can contribute data with-
out compromising privacy. Several researchers have studied
federated learning for different applications such as medical
diagnosis [1, 3, 6, 22], network security [10, 19, 21, 30], and
large-scale classification models [4, 14, 42].
Anomaly detection in surveillance videos, being one
...
Central
Parameter SharingServer
 ServerConventional Centralized Training Ours: Collaborative Training
...
......
Figure 1. a) Conventional central training requires all training data
to be on the server to carry out the training. This setting cannot en-
sure privacy, thus hindering collaborations between different enti-
ties holding large-scale surveillance data. b) Our proposed unsu-
pervised video anomaly detection technique does not require the
transfer of training data between the server and participants, thus
ensuring complete privacy.
of the large-scale applications of computer vision, may
greatly benefit from autonomous collaborative training al-
gorithms. V AD in surveillance videos is privacy sensi-
tive and may involve data belonging to several organiza-
tions/establishments. This may result in hectic bureaucratic
processes to obtain data from each establishment for cen-
tralized training. For example, the police department of a
city may not be willing to share street surveillance videos
due to privacy concerns of the general public, or a daycare
facility may have to obtain the consent of all parents to be
allowed to share its CCTV footage. Such restrictions may
hinder the possibility of obtaining large-scale data to train
efficient anomaly detectors making a central training requir-
ing all training data the least preferred option in the real-
world scenarios. Unfortunately, to the best of our knowl-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
12416
edge, there are hardly any notable attempts to leverage fed-
erated learning for video anomaly detection which may be
due to the challenging nature of the anomaly detection task
itself. Anomalies are often unknown and it is not feasible to
collect all possible anomaly examples for a model to learn
from. Furthermore, anomalies are rare in nature, and anno-
tating large amounts of data is laborious.
In this work, we explore video anomaly detection (V AD)
on two fronts: 1) Unsupervised - videos are used without
any labels. 2) Distributed participant based learning - the
server does not get any raw data from participants. Unsu-
pervised V AD is a relatively recent development in the field
of anomaly detection in which no supervision is provided
during training [2, 39]. This class of V AD training is dif-
ferent from the existing one-class classification (OCC) and
weakly supervised (WS) learning. In OCC, only normal
videos are provided for training whereas, in WS both nor-
mal and anomalous videos are provided with binary video-
level labels. Unsupervised V AD is somewhat closer to WS-
V AD, as it also utilizes large sets of videos containing nor-
mal and anomalous events. However, instead of relying on
video-level labels, the network is designed in such a way
that it utilizes several cues, such as the abundance of nor-
mal events/scarcity of anomalies, etc., present in the surveil-
lance videos to drive the overall training [39]. Unsuper-
vised V AD itself is a challenging task and the complexity
increases multifolds when we consider distributed partici-
pants setting for training. However, this is also more re-
warding due to zero annotation labor and a more practical
real-world application enabling collaboration between dif-
ferent large-scale data sources.
To this end, we propose CLAP, an approach for
Collaborative L earning of Anomalies with P rivacy that
takes unlabelled videos at multiple nodes (participants)
as input and collaboratively learns to predict frame-level
anomaly score predictions as output (Figure 1). At an ab-
stract level, our approach can be divided into three distinct
steps: Common knowledge based data segregation for lo-
cal training, knowledge accumulation at server, and local
feedback. As we approach the task as fully unsupervised,
i.e., without any labels, videos at each participant’s end are
segregated to separate normal and anomalous candidates.
To this end, we propose to utilize V on Neumann entropy as
a metric [25], and apply Gaussian Mixture Model (GMM)
to create clusters of normal and anomalous videos. After
certain local epochs, the weights of all local trainings are
accumulated at the server. Based on this, a feedback loop
is formed to refine the initial labels obtained by each partic-
ipant and share the commonly learned knowledge between
each participant.
As video anomaly detection in distributed participant
settings is not well-studied, we explore multiple training
and evaluation scenarios mimicking real-world collabora-tions. These scenarios, listed in the order of complexity, in-
clude all participants having access to similar type of train-
ing data, different participants having access to different
types of anomalies, and different participants having totally
different types and numbers of videos. Overall, the con-
tributions of our manuscript are: 1) We propose, CLAP, a
new baseline for anomaly detection capable of localizing
anomalous events in complex surveillance scenarios in a
fully unsupervised fashion without any labels on a privacy-
preserving participant-based training configuration. To the
best of our knowledge, CLAP is the first rigorous attempt
to tackle video anomaly detection in the federated learning
setting. 2) We propose three new evaluation scenarios to ex-
tensively evaluate CLAP on various scenarios of collabora-
tions and data availability. 3) To carry out these evaluations,
we modify the existing V AD datasets to create new splits.
2. Related Work
2.1. Federated Learning
Federated learning has been studied for various com-
puter vision applications including healthcare [19, 21, 30],
surveillance [5, 7, 18, 24, 28], and autonomous driving
[8, 9, 17, 23, 41]. Video anomaly detection has not been
well-studied in federated learning settings. The closely re-
lated anomaly detection in federated learning setting mostly
includes network security related methods in which differ-
ent network attacks are identified from the normal traffic
of packets [10, 19, 21, 30]. However, given that these are
mostly supervised tasks in the form of one-class classifi-
cation, the problem of distributed learning transforms into
weight-sharing optimization. Recently, Doshi et al. [7] have
proposed a weakly supervised federated learning (FL) video
anomaly detection (V AD) method. The idea is to explore
the FL setting by randomly dividing the data between mul-
tiple clients. In essence, this work is related to our approach
as we also study FL for V AD. However, we primarily ex-
plore the unsupervised setting of V AD. Moreover, without
any training labels for V AD, we additionally propose com-
mon knowledge-based data segregation for local training
and local feedback for improved pseudo-labeling to carry
out the collaborative training. Furthermore, we propose re-
alistic scenarios to evaluate V AD methods in FL setting.
2.2. Unsupervised Anomaly Detection
Introduced by Zaheer et al . [39], fully unsupervised
anomaly detection is a relatively new idea and the methods
that do not require any training labels are still quite sparse
in the literature. This problem is extremely challenging due
to the rarity of anomalies and the complete lack of super-
vision labels. Zaheer et al. [39], by relying on the abun-
dance of normal data, proposed to first train a generative
model to learn the overall normal trends in the dataset. A
12417
classifier model is then trained based on the pseudo labels
obtained from the generator. Both models are then trained
in a cooperative manner to converge as an anomaly detec-
tor. This line of research has been extended by Anas et
al. [2] . They have proposed to utilize hierarchical clus-
tering to obtain fine-grained pseudo-labels. The training of
an anomaly detector is then carried out using these pseudo
labels. Our approach also begins with pseudo-label gen-
eration, followed by training and then a feedback loop to
improve the pseudo-labels. However, we attempt to address
the problem in collaborative learning setting where privacy-
preserving training is carried out by multiple participants.
In addition, we propose a common knowledge aware data
segregation in which all participants share common clusters
knowledge to obtain pseudo-labels for training.
We also acknowledge one-class classification (OCC)
[11, 12, 27, 35] and weakly-supervised (WS) [29, 31, 34,
36, 37, 40] approaches for video anomaly detection. To en-
rich the quality of analysis and to provide several aspects
of our research work, we carry out some additional experi-
ments on weakly-supervised settings by incorporating weak
labels in our training in Section 4. However, the prime focus
of our research work is unsupervised video anomaly detec-
tion, and thus, OCC and WS anomaly detection are not in
the scope of our research work.
3. Methodology
Problem Definition : Given a dataset of training videos
without any labels, the goal of US-V AD is to learn an
anomaly detector Aθ(⋅)that classifies each frame in a given
test video V∗as either normal (0) oranomalous (1). Sup-
pose that there are Kparticipants P1,P2, ...,PKfor collab-
orative training and each participant Pkhas its own local
training dataset Dk={V1,k, V2,k,⋯, VNk,k}containing Nk
videos used to train its own local anomaly detector model
Aθk(⋅),k∈[1, K]. It is assumed that all participants share
a common test set and the performance of the model Aθk(⋅)
on this test set is denoted by Ok. The goal of each par-
ticipant is to collaborate with other participants in order to
obtain a global model θ∗that has better performance O∗on
the test set compared to all Ok, without compromising the
privacy of participant’s local data Dk.
Preprocessing : For simplicity of notation, we drop the par-
ticipant index kunless required. Let the training dataset
ofNvideos at a generic participant be denoted as D=
{V1, V2,⋯, VN}. We split each video Viinto a sequence
ofminon-overlapping segments Sij, where each segment
is in turn composed of rframes. Note that i∈[1, N]refers
to the video index and j∈[1, mi]is the segment index
within a video. Unlike many state-of-the-art AD methods
[26, 29, 32, 33] that compress each video into a fixed num-
ber of segments (i.e., mi=m,∀i∈[1, N]) along the tem-
poral axis, we avoid any compression and make use of all
VPL
2Distributions Aggregation
ServerFedA vg3
21 1
SPLPLR
....
ParticipantFigure 2. Architecture of CLAP, an unsupervised video anomaly
detection model trained by multiple collaborating participants.
available non-overlapping segments1. For each segment
Sij, a feature vector fij∈Rdis obtained using a pre-trained
feature extractor F(⋅).
High-level Overview : Our proposed CLAP model for col-
laborative training consists of three main stages. The aim
is to generate both video-level and segment-level pseudo-
labels to enable the training of the anomaly detector model
Aθ(⋅). In the first Common Knowledge-based Data Seg-
regation (CKDS) stage, the generation of segment-level
pseudo labels is done in a collaborative manner. We gen-
erate a video-level pseudo-label ˆyi∈{0,1},i∈[1, N]
for each video in the training set using hierarchical divi-
sive clustering. Subsequently, segment-level pseudo-labels
˜yij∈{0,1},i∈[1, N],j∈[1, mi]are generated for all
the segments in the training set through collaborative statis-
tical hypothesis testing. Finally, we train a local anomaly
detector Aθ(⋅)∶Rd→[0,1]that assigns an anomaly score
between 0and1(higher values indicate higher confidence
of being an anomaly) to the given video segment based on
its feature representation fij.
During training, we utilize both server knowledge accu-
mulation (SKA) and local feedback stages to improve the
performance of the local model. In the second (SKA) stage,
we use the well-known Federated Averaging (FedAvg) al-
gorithm [20] (we also analyze other FL aggregation meth-
ods in Section 6 of supplementary material) for aggregating
local anomaly detection models. Finally, upon completion
of a pre-determined number of collaboration rounds to up-
date the weights given the initial pseudo-labels, our algo-
rithm initiates the local feedback or pseudo-label refinement
(PLR) stage. During this stage, we use confidence scores
predicted by the network to refine the generated pseudo-
labels from the first stage.
1Note that for simplicity, we use the notation val ijto represent a value
for segment jin video Vi,val irepresents the set of values of all segments
in video Vi, and valsimply represents the collection of all segment-wise
values of all the videos in the dataset D.
12418
3.1. Knowledge-based Data Segregation (CKDS)
At the participant level, since the training does not assume
any labels, we first generate pseudo-labels for the videos in
the training set by clustering them into two groups: normal
and anomalous (see Alg. 1).
Video-Level Pseudo-Labels : Previous works in WS-V AD
have shown that normal video segments have lower tem-
poral feature magnitude compared to anomalous segments
[31]. In addition, we observe the variance of the difference
in the feature magnitude between consecutive segments in
a given anomalous video is higher than in a normal video.
Furthermore, we consider von-Neumann entropy Hof the
covariance matrix computed based on the features as an in-
dicator of the presence of anomalies, i.e., the entropy of seg-
ments is generally expected to be lower for normal videos.
Based on these cues, we represent each video Viusing a
statistical summary xi=[σi, Hi]of its features as follows:
µi=1
(mi−1)(mi−1)
∑
j=1(∣∣fij∣∣2−∣∣fi(j+1)∣∣2), (1)
σi=⌟roo⟪⟪op
⌟roo⟪mo⟨⌟roo⟪mo⟨⌟roo⟪mo⟨⌟roo⟪⟨o⟪1
(mi−2)(mi−1)
∑
j=1((∣∣fij∣∣2−∣∣fi(j+1)∣∣2)−µi)2,(2)
Cov[fi,1, ...,fi,mi]=UiΣiVT
i (3)
Hi=−tr[ΣilogΣi] (4)
where∣∣⋅∣∣2represents the ℓ2norm of a vector. Thus, each
video Viis represented using a 2D vector xi, corresponding
to the variance σiand entropy Hiof the video segments.
Videos in the training set are then divided into two clus-
ters (Cs1andCs2), with∣Cs1∣and∣Cs2∣samples, respectively,
based on the above representation xi.
Intuitively, building on the assumption that normal sam-
ples have a lower entropy, we compute the average entropy
of samples in each cluster and assign the cluster with the
larger average entropy as anomalous ( 0), while the other
cluster is labeled as normal ( 1). At the end of this stage,
all the videos in the training set are assigned a pseudo-label
based on their corresponding cluster label, i.e., ˆyi=s, if
xi∈Cs, where s∈{0,1}.
Segment-Level Pseudo-Labels : All the segments from
videos that are “pseudo-labeled” as normal ( ˆyi=0) by the
previous stage can be considered normal. However, most
of the segments in an anomalous video are also normal due
to the smaller temporal extent of anomalies. To tackle this,
we treat the detection of anomalous segments as a statistical
hypothesis-testing problem. Specifically, the null hypothe-
sis is that a given video segment is normal. By modelingAlgorithm 1 Video-level Pseudo-Label Generation (VPL)
Input: dataset D={V1,⋯, VN}, feature extractor F(⋅)
1:fori=1toNdo
2: Partition Viintomisegments [Si1,⋯, Simi]
3: Extract segment features [fi1,⋯,fimi]usingF(⋅)
4: Compute xi=[σi, Hi]using Eqs. 2 & 4
5:end for
6:Cs1,Cs2←Clustering(GMM), where a=∣Cs1∣, b=∣Cs2∣
7:if1
a∑a
i=1Hi>1
b∑b
i=1Hithen
8:C0=Cs2,C1=Cs1
9:else
10: C0=Cs1,C1=Cs2
11:end if
12:∀i∈[1, N],ˆyi←0ifxi∈C0,elseˆyi←1
return ˆy={ˆy1,⋯,ˆyN}
the distribution of features under the null hypothesis as a
Gaussian distribution, we identify the anomalous segments
by estimating their p-value and rejecting the null hypothesis
if the p-value is less than the significance level α.
To model the distribution of features ( at the participant
level ) under the null hypothesis, we consider only the seg-
ments from videos that are pseudo-labeled as normal by the
VPL stage (see Algo. 1). Let zij∈R˜dbe a low-dimensional
representation of a segment Sij. In this work, we simply set
zij=∣∣fij∣∣2. We assume that zijfollows a Gaussian distri-
bution N(Γ,Θ)under the null hypothesis and estimate the
parameters ΓandΘas follows:
Γ=1
M0N
∑
i=1,ˆyi=0mi
∑
j=1zij, (5)
Θ=1
(M0−1)N
∑
i=1,ˆyi=0mi
∑
j=1(zij−Γ)(zij−Γ)T,(6)
where M0=∑N
i=1,ˆyi=0mi.
Let(Γk,Θk)be the Gaussian parameters at participant
Pkbased on M0,knormal segments. The participants share
these parameters with the server and the server sends back a
Gaussian mixture model G. One simple way to construct G
is to treat (Γk,Θk)of each participant as a mixture compo-
nent and weight these components based on the correspond-
ing number of normal segments. More sophisticated ag-
gregation approaches could also be employed by the server.
Subsequently, for all the segments in videos that are pseudo-
labeled as anomalous, the p-value is computed as:
pij=G(zij),∀ˆyi=1. (7)
Ifpij<α, the segment can be directly assigned a pseudo-
label of 1. However, we identify a potential anomalous re-
gion by sliding a window of size wiacross the video and se-
12419
lecting the region that has the lowest average p-values (i.e.,
minl{1
wi∑(l+wi)
j=(l+1)pij,∀l∈[0, mi−wi]}). Each segment
present in this anomalous region is assigned a pseudo-label
of1, while all the remaining segments are pseudo-labeled as
normal with a value of 0. Thus, a pseudo-label ˜yij∈{0,1}
is assigned to all the segments in the training set. This
window-based labeling may be seen as utilizing the tem-
poral consistency property of the surveillance videos com-
monly utilized in the existing literature [29, 36].
Algorithm 2 Segment-level Pseudo-Label Gen. (SPL)
Input: Mode, 0<β≤1,ˆy(video-level pseudo-labels)
and Gaussian mixture model Gfor Generate mode, ˜y
(current segment-level pseudo-labels) and Q(segment-
level confidence scores) for Update mode
Mode I: Generate
1:fori=1toNdo
2: ifˆyi=1then
3: Compute pijusing Eq. 7, ∀j∈[1, mi]
4: wi←⌈βmi⌉
5: li=arg min l{1
wi∑(l+wi)
j=(l+1)pij,∀l∈[0, mi−wi]}
6: ˜yij←1,∀j∈[li+1, li+w]
7: end if
8:end for
return ˜y
Mode II: Update (PLR)
9:fori=1toNdo
10: Setqijbased on Q∀j∈[1, mi]
11: wi←⌈βmi⌉
12: li=arg max l{1
wi∑(l+wi)
j=(l+1)qij,∀l∈[0, mi−wi]}
13: ˜qij←0,∀j∈[1, mi]
14: ˜qij←1,∀j∈[li+1, li+w]
15: ˆqij←0,∀j∈[1, mi]
16: iflen(˜yi∩˜qi)>0then
17: ˆqij←1,∀j∈[1, mi],if˜qij∈(˜yi∩˜qi)
18: else
19: ˆqij←1,∀j∈[1, mi],if˜qij≠0or˜yi,j≠0
20: end if
21:end for
22:return ˆq
3.2. Server Knowledge Accumulation (SKA) and
Local Feedback
At the beginning of this stage, each participant would have
obtained segment-level pseudo-labels for its own dataset by
sequentially applying Algo. 1 and Algo. 2 as described in
Section 3.1. As earlier, if Dis the unlabeled training dataset
of a generic participant, we can obtain the segment-level
pseudo-labeled training set ˜D={(fij,˜yij)}containing M
samples, where i∈[1, N],j∈[1, mi], and M=∑n
i=1mi.
This labeled training set ˜Dcan be used to train the anomalyAlgorithm 3 CLAP
Require: Local training dataset Dk. Server initializes
parameter θ(0)
∗.
1:foreach participant k=1,2, ..., K do
2: ˆyk←Algorithm 1 (Dk)
3:∀i∈[1, Nk], j∈[1, mi],˜yij←0, Compute zij
4: Compute (Γk,Θk)using Eqs. 5 & 6
5: return(Γk,Θk)to server
6:end for
7:server: G←Mixture of Gaussians ( {(Γk,Θk)}K
k=1)
8:foreach participant k=1,2, ..., K do
9: ˜yk←Algorithm 2 (Generate, ˆyk,G)
10:end for
11:foreach round t=0,1, ..., T do
12: foreach participant k=1,2, ..., K do
13: ˜Dk←(Dk,˜yk)
14: θ(t,0)
k←θ(t)
∗
15: forlocal iteration e=0,1, ..., E do
16: θ(t,e+1)
k←θ(t,e)
k−η⋅∇Ltotal,k(˜Dk, θ(t,e)
k)
17: end for
18: ∆(t)
k←θ(t,E)
k−θ(t)
∗
19: Qk←Aθ(t,E)
k(Dk)
20: ˜yk←Algorithm 2 (Update, Qk,˜yk)
21: return ∆(t)
kto server
22: end for
23: server: θ(t+1)
∗←θ(t)
∗+λ
K∑K
k=1∆(t)
k
24:end for
detector Aθ(⋅)by minimizing the following objective:
min
θLtotal=N
∑
i=1mi
∑
j=1L(Aθ(fij),˜yij), (8)
where Lis an appropriate loss function and θdenotes the
parameters of the anomaly detector ˜A(⋅). Stochastic gradi-
ent descent (SGD) is used for the above optimization. Fol-
lowing recent state-of-the-art methods [32, 36, 39], a basic
neural network architecture is considered for our anomaly
detector (see Supplementary for more details).
Now, we proceed to describe the collaborative training of
the anomaly detector, which is referred to as server knowl-
edge accumulation (SKA). At the beginning of each col-
laboration round t, the server broadcasts the current global
model parameters θ(t)
∗to all the participants. Using these
global parameters as the initialization, each participant will
perform Elocal SGD iterations to get the updated parame-
tersθ(t,E)
k. At the end of the local training, the participant
sends the local gradient ∆(t)
k←θ(t,E)
k−θ(t)
∗back to the
server. The server aggregates these gradients and applies the
update to the global model as θ(t+1)
∗←θ(t)
∗+λ
K∑K
i=1∆(t)
k
as shown in Algo. 3, where λis the learning rate.
12420
In addition to SKA, we also incorporate local feed-
back or pseudo-label refinement process (PLR) as shown
in Algo.2(Update). During this stage, we use confi-
dence scores Qpredicted by the local model to refine the
segment-level pseudo-labels. The aim is to use the high-
confidence segments to update the pseudo-labels ˜ygener-
ated from Algo.2(Generate). First, we determine the max-
imum confidence region by a sliding window wisimilar to
Algo.2(Generate) and assign those segments as ˜qij=1. The
refinement of the old pseudo-labels is based on two rules.
First, if there is an intersection between the maximum con-
fidence region and the generated pseudo-labels (˜yi∩˜qi)>0,
we assign all the segments in ˜qijthat are in the intersec-
tion set a value of 1. On the other hand, if there is no in-
tersection, we assume that the old pseudo-labels missed an
additional anomalous window. Therefore, the whole of the
maximum confidence region will be assigned as anomalous.
3.3. Inference
At the end of all communication rounds, the final global
model Aθ(T)
∗(⋅)is used for inference. A given test video
V∗is partitioned into m∗non-overlapping segments S∗j,
j∈[1, m∗]. Feature vectors f∗jare extracted from each seg-
ment using F(⋅), which are directly passed to the trained de-
tectorAθ(T)
∗(⋅)to obtain segment-level anomaly score pre-
dictions. As the final goal is frame-level anomaly scores,
all frames within a segment of the test video inherit the pre-
dicted anomaly score for that corresponding segment.
4. Experiments and Results
4.1. Datasets and Implementation
We evaluate CLAP and conduct SOTA comparisons on
two publicly available large-scale datasets, UCF-Crime and
XD-Violence. Due to limited space, datasets and imple-
mentation details are provided in Supplementary.
4.2. Training Settings
As the aim of CLAP is to train an anomaly detector with
collaboration between multiple participants, we provide
comparisons of our approach with existing SOTA anomaly
detection methods under the following three different train-
ing settings:
Centralized Training: This is the conventional setting of
training where privacy is not ensured and the participants
have to send all training data to the server for joint training.
Anomaly detection performance is measured on the com-
plete test set.
Local Training: This setting assumes that participants are
not collaborating and each individual participant trains its
anomaly detector locally with its own data. Anomaly de-
tection performance is measured for each participant indi-
vidually on the complete test set.Method UCF-Crime XD-Violence
CentralizedGCL [39] 71.04 73.62
C2FPL [2] 80.65 80.09
CLAP 80.9 81.71
LocalGCL [39] 56.63 58.11
C2FPL [2] 61.33 60.05
CLAP 63.93 62.37
CollaborativeGCL [39] 67.12 68.19
C2FPL [2] 75.20 74.36
CLAP 78.02 77.65
Table 1. AUC performance comparisons of unsupervised SOTA
on UCF-Crime and XD-Violence datasets for five participants.
Collaborative Training: In this setting, all participants col-
laborate to train a joint anomaly detector. Participants do
not need to send their training data to the server to carry out
the training. Anomaly detection performance of the jointly
trained model is measured on the complete test set.
4.3. Comparisons with Unsupervised SOTA
Table 1 summarizes the results of the existing unsupervised
anomaly detection approaches on the three training settings:
centralized, local, and collaborative. We re-implemented
the existing methods [2, 39] on local and collaborative train-
ing settings for fair and detailed comparisons. In essence,
centralized training is the upper bound of the collaborative
training whereas local training is the lower bound. While
better performance compared to the local training setting
may indicate the success of collaborative learning, a bet-
ter model should also demonstrate minimal performance
difference from the centralized training. CLAP demon-
strates AUC performances of 80.91% and 81.71% on UCF-
crime and XD-Violence datasets on the centralized setting
(upper bound). In the local setting (lower bound) with
five participants, CLAP demonstrate AUC performances of
63.93% and 62.37% on the two datasets. In the collab-
orative learning setting, CLAP demonstrates AUC perfor-
mances of 78.02% and 77.65%. Overall, the results of
CLAP are not only better than the existing SOTA unsuper-
vised V AD methods but are also on par with its counterpart
centralized training setting.
4.4. Comparisons with Weakly-supervised SOTA
An unsupervised method can be converted into a super-
vised method upon the availability of labels [39]. We ex-
plore this supervision mode under centralized, local, and
collaborative training settings and report the results in Ta-
ble 2. Compared to the methods developed specifically
for unsupervised video anomaly detection [2, 39], CLAP
demonstrates consistent performance improvements when
video-level labels are present. CLAP also outperforms PRV
[7], a weakly-supervised approach designed specifically for
collaborative learning. Compared to other centralized ap-
proaches that do not facilitate unsupervised training, CLAP
demonstrates better performance than the compared meth-
12421
1
2
3
4
5
6
7
8
9
10
/uni00000033/uni00000044/uni00000055/uni00000057/uni0000004c/uni00000046/uni0000004c/uni00000053/uni00000044/uni00000051/uni00000057/uni00000056/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000039/uni0000004c/uni00000047/uni00000048/uni00000052/uni00000056/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f
/uni00000024/uni00000045/uni00000051/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f(a) Random Distribution
1
2
3
4
5
6
7
8
9
10
11
12
13
/uni00000033/uni00000044/uni00000055/uni00000057/uni0000004c/uni00000046/uni0000004c/uni00000053/uni00000044/uni00000051/uni00000057/uni00000056/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013 /uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f
/uni00000024/uni00000045/uni00000051/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f (b) Event Based Distribution
1
2
3
4
5
6
7
8
9
10
11
/uni00000033/uni00000044/uni00000055/uni00000057/uni0000004c/uni00000046/uni0000004c/uni00000053/uni00000044/uni00000051/uni00000057/uni00000056/uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f
/uni00000024/uni00000045/uni00000051/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f (c) Scene Based Distribution
Figure 3. Distribution of UCF-Crime dataset videos based on the three training data organizations proposed in our paper to evaluate
collaborative learning approaches for video Anomaly Detection. (a) Random distribution of the videos is the baseline in which each
participant has an almost identical number of videos and the classes are balanced. (b) Each participant holds videos containing certain
types of anomalous events such as. shooting, robbery, etc. It is a relatively complex setting with the number of videos and class balance
varying slightly between participants. (c) Each participant holds videos belonging to certain scenes such as shops, offices, etc. This is the
most challenging setting where severe data and class imbalance are present across participants.
Method Unsup. Possible? UCF-Crime XD-ViolenceCentralizedSultani et al. [29] ✗ 75.41 -
RTFM [31] ✗ 84.30 89.34
MSL [15] ✗ 85.30 -
S3R [33] ✗ 85.99 53.52
CLAWS+ [38] ✗ 80.90 -
PRV [7] ✗ 86.30 -
GCL [39] ✓ 79.84 82.18
C2FPL [2] ✓ 83.40 89.34
CLAP ✓ 85.50 90.04LocalGCL [39] ✓ 65.32 59.91
C2FPL [2] ✓ 65.85 63.4
CLAP ✓ 67.47 64.97Collab.PRV [7] ✗ 82.90 -
GCL [39] ✓ 76.82 75.21
C2FPL [2] ✓ 77.60 76.98
CLAP ✓ 83.23 85.67
Table 2. AUC performance comparisons of weakly supervision
SOTA on UCF-Crime and XD-Violence datasets.
ods on XD-Violence dataset and comparable performance
on UCF-Crime dataset. Nevertheless, the goal of this work
is not to surpass performance numbers on certain tasks but
to demonstrate the possibility of unsupervised training un-
der a collaborative learning setting to facilitate a novel re-
search direction in the field of video anomaly detection.
4.5. Collaborative Learning in V AD: A Case Study
of Different Possible Scenarios
In this section, we further explore the collaborative learning
of video anomaly detection by proposing various scenar-
ios of collaboration and consequent re-organization of the
training data. Furthermore, we analyze and discuss the per-
formance of CLAP under these scenarios.
4.5.1 Training Data Splitting
In real-world V AD applications, common sources of
surveillance videos could be different government entities
(e.g., department of transport or police) or private CCTVoperating institutions (e.g., shopping malls or elderly-care
facilities). CLAP is designed to enable collaborative learn-
ing of a joint anomaly detector between such data sources
while eliminating the need to share training data. Consider-
ing this, we propose three different training data splits mim-
icking different kinds of collaborations between the partici-
pants. Each of these is explained below.
Random Split: A baseline setting where each participant
has randomly distributed and equal number of anomalous &
normal videos for training. Figure 3a visualizes the random
distribution of videos between ten participants on the UCF-
crime dataset.
Event Class Based Split: In this setting, each participant
has training videos based on the anomalous events present
within. For example, one participant may have road acci-
dent videos whereas another participant may have robbery
videos. This setting is more challenging than random distri-
bution because each participant may have a different num-
ber of videos. Figure 3b visualizes the distribution of videos
between thirteen participants on the UCF-crime dataset.
Scene Based Split: In this setting, each participant gets the
videos based on the scenes/locations where the videos are
recorded. For example, one participant may have surveil-
lance videos for fuel stations, another participant may have
indoor videos of offices, and so on. This is the most chal-
lenging and representative setting as the dataset is not bal-
anced either among participants or within a participant for
the normal and anomalous classes. Figure 3c visualizes the
distribution of videos between eleven participants on the
UCF-crime dataset.
The intuition behind these splits is that, in real-world
scenarios, several participants for training a joint model
may belong to different entities with different types of video
data available at their disposal. For example, a police de-
partment may have videos related to street crimes, a chil-
dren’s daycare facility may have surveillance available for
abuse or bullying, and a mall may have videos about steal-
12422
Split Participants AUC(%)
Random(IID)5 78.02
10 77.4
25 76.54
50 67.92
Event 13 77.35
Scene 11 73.99Table 3. AUC%
performance of CLAP
on UCF-Crime dataset
using various pro-
posed training splits
under collaborative
learning setting.
/uni00000018 /uni00000014/uni00000013 /uni00000015/uni00000018 /uni00000018/uni00000013
/uni00000031/uni00000052/uni00000011/uni00000003/uni00000052/uni00000049/uni00000003/uni00000033/uni00000044/uni00000055/uni00000057/uni0000004c/uni00000046/uni0000004c/uni00000053/uni00000044/uni00000051/uni00000057/uni00000056/uni00000018/uni00000013/uni00000018/uni00000018/uni00000019/uni00000013/uni00000019/uni00000018/uni0000001a/uni00000013/uni0000001a/uni00000018/uni0000001b/uni00000013/uni0000001b/uni00000018/uni0000001c/uni00000013/uni00000024/uni00000038/uni00000026/uni00000003/uni00000008/uni0000001a/uni0000001b/uni00000011/uni00000013/uni00000015 /uni0000001a/uni0000001a/uni00000011/uni00000017/uni0000001a/uni00000019/uni00000011/uni00000018/uni00000017
/uni00000019/uni0000001a/uni00000011/uni0000001c/uni00000015/uni00000019/uni0000001a/uni00000011/uni00000014/uni00000015/uni00000019/uni0000001b/uni00000011/uni00000015/uni00000014 /uni00000019/uni0000001b/uni00000011/uni0000001a/uni0000001c
/uni00000019/uni00000014/uni00000011/uni00000014/uni00000017/uni00000026/uni0000002f/uni00000024/uni00000033
/uni0000002a/uni00000026/uni0000002f
/uni00000013 /uni00000014/uni00000013 /uni00000014/uni0000001a /uni00000015/uni00000018
/uni00000031/uni00000052/uni00000011/uni00000003/uni00000052/uni00000049/uni00000003/uni00000033/uni00000044/uni00000055/uni00000057/uni0000004c/uni00000046/uni0000004c/uni00000053/uni00000044/uni00000051/uni00000057/uni00000056/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni0000003a/uni00000036/uni00000003/uni0000004f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000056/uni0000001a/uni00000013/uni0000001a/uni00000015/uni0000001a/uni00000018/uni0000001a/uni0000001b/uni0000001b/uni00000013/uni0000001b/uni00000015/uni0000001b/uni00000018/uni0000001b/uni0000001b/uni0000001c/uni00000013
/uni0000001a/uni00000019/uni00000011/uni00000018/uni00000017/uni0000001a/uni0000001c/uni00000011/uni00000016/uni00000015/uni0000001b/uni00000014/uni00000011/uni00000018/uni0000001b/uni00000016/uni00000011/uni00000015/uni00000016/uni00000026/uni0000002f/uni00000024/uni00000033
Figure 4. Left: Comparison between GCL [39] and CLAP with
varying number of participants. Right: Number of participants
with weak video-level supervision available.
ing or shoplifting. More details on the data splitting strate-
gies are provided in the Supplementary.
4.5.2 Experiments Using Data Splits
We conduct experiments on CLAP by splitting the UCF-
crime dataset using the training data splits proposed (Fig-
ure 3) and report the results in Table 3. For the random
splitting, we vary the number of participants between 5and
50to additionally analyze the impact of the number of par-
ticipants on training. Unsurprisingly, random split based
training yields the highest AUC performance of 78.02%
with the participant number set to five. Experiments us-
ing event based splitting results in a closer performance of
77.35%. This demonstrates that CLAP can efficiently han-
dle data variations and partially imbalanced data among par-
ticipants. With the most challenging training setting, scene
based splitting, CLAP achieves an AUC of 73.99%.
4.6. Analysis and Discussions
On Partial Weak-Supervision: The typical protocol for
evaluating unsupervised methods under weakly-supervised
settings is fairly simple. Once pseudo labels are gener-
ated for the whole dataset, for each video labeled as nor-
mal in the weakly supervised ground truth, label correc-
tion on pseudo labels is applied before carrying out the
training [39]. In the collaborative learning setting for real-
world scenarios, ensuring the availability of any form of
training labels means requiring each participant to annotate
their data before participating in the collaborative training.
While CLAP is fully unsupervised, meaning no labels areFedA VG SKA PLR AUC(%)
✓ ✗ ✗ 76.2
✓ ✓ ✗ 77.1
✓ ✓ ✓ 78.02Table 4. Ablation study
of CLAP on UCF-Crime
dataset. SKA: Server
Knowledge Accumula-
tion, PLR: Pseudo Label
Refinement,
required for training, in this section we explore an inter-
esting scenario where some of the participants may have
labels available for training. Figure 4 (right) shows the re-
sults of CLAP on this setting using 25 participants. CLAP
demonstrates consistent performance gains when more par-
ticipants contribute with video-level labels towards the col-
laborative training.
On Varying Number of Participants: To analyze the im-
pact of varying numbers of participants on the unsupervised
V AD training, we conduct a series of experiments using
CLAP and GCL [39] with different numbers of participants
and report the results in Figure 4 (left). Overall, the per-
formance stays comparable when the participant number
is set to 5, 10, and 25. However, it drops notably when
the participant number is set to 50. This may be attributed
to the drop is the number of videos per participant, given
that the dataset size remains the same. With a more large-
scale dataset, our approach may be able to accommodate an
even larger number of participants without dropping perfor-
mance.
Ablation: To evaluate the contribution of the various com-
ponents in CLAP, we conduct an ablation study and report
the results in Table 4. As seen, with each added component
including SKA: Server Knowledge Accumulation stage and
PLR: Psuedo-label refinement over the baseline training,
notable performance gains are observed. This demonstrates
the importance of all components proposed in CLAP to-
wards unsupervised V AD.
5. Conclusion
We proposed a new baseline for anomaly detection
capable of localizing anomalous events in a fully unsuper-
vised fashion on a privacy-preserving collaborative learning
configuration. We also introduced three new evaluation sce-
narios to extensively study anomaly detection approaches
on various scenarios of collaborations and data availability.
Using these scenarios, we evaluate our approach on two
large-scale datasets including UCF-crime and XD-violence.
Alimitation of our approach is that the performance drops
when the number of participants increases. Although
some performance drop is expected in such a situation,
we believe it is partly because of the limited training
data available to each participant in this case. This can
be addressed by curating large-scale anomaly detection
datasets designed specifically for collaborative training.
12423
References
[1] Mohammed Adnan, Shivam Kalra, Jesse C Cresswell, Gra-
ham W Taylor, and Hamid R Tizhoosh. Federated learning
and differential privacy for medical image analysis. Scientific
reports , 12(1):1953, 2022. 1
[2] Anas Al-lahham, Nurbek Tastan, Zaigham Zaheer, and
Karthik Nandakumar. A coarse-to-fine pseudo-labeling
(c2fpl) framework for unsupervised video anomaly detec-
tion. arXiv preprint arXiv:2310.17650 , 2023. 2, 3, 6, 7
[3] Faris Almalik, Naif Alkhunaizi, Ibrahim Almakky, and
Karthik Nandakumar. Fesvibs: Federated split learning
of vision transformer with block sampling. arXiv preprint
arXiv:2306.14638 , 2023. 1
[4] Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp,
Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kid-
don, Jakub Kone ˇcn`y, Stefano Mazzocchi, Brendan McMa-
han, et al. Towards federated learning at scale: System de-
sign. Proceedings of machine learning and systems , 1:374–
388, 2019. 1
[5] Bouziane Brik, Adlen Ksentini, and Maha Bouaziz. Feder-
ated learning for uavs-enabled wireless networks: Use cases,
challenges, and open problems. IEEE Access , 8:53841–
53849, 2020. 2
[6] Erfan Darzidehkalani, Mohammad Ghasemi-Rad, and PMA
van Ooijen. Federated learning in medical imaging: part i:
toward multicentral health care ecosystems. Journal of the
American College of Radiology , 19(8):969–974, 2022. 1
[7] Keval Doshi and Yasin Yilmaz. Privacy-preserving video un-
derstanding via transformer-based federated learning. Data
Science Conference , 2023. 2, 6, 7
[8] Zhaoyang Du, Celimuge Wu, Tsutomu Yoshinaga, Kok-
Lim Alvin Yau, Yusheng Ji, and Jie Li. Federated learning
for vehicular internet of things: Recent advances and open
issues. IEEE Open Journal of the Computer Society , 1:45–
61, 2020. 2
[9] Lidia Fantauzzo, Eros Fan `ı, Debora Caldarola, Antonio
Tavera, Fabio Cermelli, Marco Ciccone, and Barbara Ca-
puto. Feddrive: Generalizing federated learning to seman-
tic segmentation in autonomous driving. In 2022 IEEE/RSJ
International Conference on Intelligent Robots and Systems
(IROS) , pages 11504–11511. IEEE, 2022. 2
[10] Bimal Ghimire and Danda B Rawat. Recent advances on fed-
erated learning for cybersecurity and cybersecurity for feder-
ated learning for internet of things. IEEE Internet of Things
Journal , 9(11):8229–8249, 2022. 1, 2
[11] Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha,
Moussa Reda Mansour, Svetha Venkatesh, and Anton
van den Hengel. Memorizing normality to detect anomaly:
Memory-augmented deep autoencoder for unsupervised
anomaly detection. In Proceedings of the IEEE International
Conference on Computer Vision , pages 1705–1714, 2019. 3
[12] Radu Tudor Ionescu, Fahad Shahbaz Khan, Mariana-Iuliana
Georgescu, and Ling Shao. Object-centric auto-encoders and
dummy anomalies for abnormal event detection in video. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 7842–7851, 2019. 3[13] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri,
Sashank Reddi, Sebastian Stich, and Ananda Theertha
Suresh. Scaffold: Stochastic controlled averaging for feder-
ated learning. In International conference on machine learn-
ing, pages 5132–5143. PMLR, 2020. 1
[14] Fan Lai, Yinwei Dai, Sanjay Singapuram, Jiachen Liu, Xi-
angfeng Zhu, Harsha Madhyastha, and Mosharaf Chowd-
hury. Fedscale: Benchmarking model and system perfor-
mance of federated learning at scale. In International Con-
ference on Machine Learning , pages 11814–11827. PMLR,
2022. 1
[15] Shuo Li, Fang Liu, and Licheng Jiao. Self-training multi-
sequence learning with transformer for weakly supervised
video anomaly detection. In Proceedings of the AAAI Con-
ference on Artificial Intelligence , pages 1395–1403, 2022. 7
[16] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi,
Ameet Talwalkar, and Virginia Smith. Federated optimiza-
tion in heterogeneous networks. Proceedings of Machine
learning and systems , 2:429–450, 2020. 1
[17] Yijing Li, Xiaofeng Tao, Xuefei Zhang, Junjie Liu, and Jin
Xu. Privacy-preserved federated learning for autonomous
driving. IEEE Transactions on Intelligent Transportation
Systems , 23(7):8423–8434, 2021. 2
[18] Yang Liu, Anbu Huang, Yun Luo, He Huang, Youzhi Liu,
Yuanyuan Chen, Lican Feng, Tianjian Chen, Han Yu, and
Qiang Yang. Fedvision: An online visual object detection
platform powered by federated learning. In Proceedings of
the AAAI conference on artificial intelligence , pages 13172–
13179, 2020. 2
[19] Yi Liu, Jialiang Peng, Jiawen Kang, Abdullah M Iliyasu,
Dusit Niyato, and Ahmed A Abd El-Latif. A secure fed-
erated learning framework for 5g networks. IEEE Wireless
Communications , 27(4):24–31, 2020. 1, 2
[20] Brendan McMahan, Eider Moore, Daniel Ramage, Seth
Hampson, and Blaise Aguera y Arcas. Communication-
efficient learning of deep networks from decentralized data.
InArtificial intelligence and statistics , pages 1273–1282.
PMLR, 2017. 3
[21] Viraaji Mothukuri, Reza M Parizi, Seyedamin Pouriyeh, Yan
Huang, Ali Dehghantanha, and Gautam Srivastava. A survey
on security and privacy of federated learning. Future Gener-
ation Computer Systems , 115:619–640, 2021. 1, 2
[22] Dianwen Ng, Xiang Lan, Melissa Min-Szu Yao, Wing P
Chan, and Mengling Feng. Federated learning: a collabo-
rative effort to achieve better medical imaging models for in-
dividual sites that have small labelled datasets. Quantitative
Imaging in Medicine and Surgery , 11(2):852, 2021. 1
[23] Anh Nguyen, Tuong Do, Minh Tran, Binh X Nguyen, Chien
Duong, Tu Phan, Erman Tjiputra, and Quang D Tran. Deep
federated learning for autonomous driving. In 2022 IEEE In-
telligent Vehicles Symposium (IV) , pages 1824–1830. IEEE,
2022. 2
[24] Yiran Pang, Zhen Ni, and Xiangnan Zhong. Federated learn-
ing for crowd counting in smart surveillance systems. IEEE
Internet of Things Journal , 2023. 2
[25] D ´enes Petz. Entropy, von neumann and the von neumann
entropy: Dedicated to the memory of alfred wehrl. In John
12424
von Neumann and the foundations of quantum physics , pages
83–96. Springer, 2001. 2
[26] Didik Purwanto, Yie-Tarng Chen, and Wen-Hsien Fang.
Dance with self-attention: A new look of conditional ran-
dom fields on anomaly detection in videos. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion (ICCV) , pages 173–183, 2021. 3
[27] Mohammad Sabokrou, Mohammad Khalooei, Mahmood
Fathy, and Ehsan Adeli. Adversarially learned one-class
classifier for novelty detection. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition ,
pages 3379–3388, 2018. 3
[28] Abdelkarim Ben Sada, Mohammed Amine Bouras, Jianhua
Ma, Huang Runhe, and Huansheng Ning. A distributed
video analytics architecture based on edge-computing and
federated learning. In 2019 IEEE Intl Conf on Depend-
able, Autonomic and Secure Computing, Intl Conf on Per-
vasive Intelligence and Computing, Intl Conf on Cloud and
Big Data Computing, Intl Conf on Cyber Science and Tech-
nology Congress (DASC/PiCom/CBDCom/CyberSciTech) ,
pages 215–220. IEEE, 2019. 2
[29] Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world
anomaly detection in surveillance videos. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 6479–6488, 2018. 3, 5, 7, 2
[30] Zhongyun Tang, Haiyang Hu, and Chonghuan Xu. A feder-
ated learning method for network intrusion detection. Con-
currency and Computation: Practice and Experience , 34
(10):e6812, 2022. 1, 2
[31] Yu Tian, Guansong Pang, Yuanhong Chen, Rajvinder Singh,
Johan W Verjans, and Gustavo Carneiro. Weakly-supervised
video anomaly detection with robust temporal feature mag-
nitude learning. arXiv preprint arXiv:2101.10030 , 2021. 3,
4, 7
[32] Yu Tian, Guansong Pang, Yuanhong Chen, Rajvinder Singh,
Johan W Verjans, and Gustavo Carneiro. Weakly-supervised
video anomaly detection with robust temporal feature magni-
tude learning. In Proceedings of the IEEE/CVF international
conference on computer vision , pages 4975–4986, 2021. 3,
5
[33] Jhih-Ciang Wu, He-Yen Hsieh, Ding-Jie Chen, Chiou-Shann
Fuh, and Tyng-Luh Liu. Self-supervised sparse represen-
tation for video anomaly detection. In Computer Vision–
ECCV 2022: 17th European Conference, Tel Aviv, Israel,
October 23–27, 2022, Proceedings, Part XIII , pages 729–
745. Springer, 2022. 3, 7
[34] Peng Wu, Jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao,
Zhaoyang Wu, and Zhiwei Yang. Not only look, but also
listen: Learning multimodal violence detection under weak
supervision. In Computer Vision–ECCV 2020: 16th Euro-
pean Conference, Glasgow, UK, August 23–28, 2020, Pro-
ceedings, Part XXX 16 , pages 322–339. Springer, 2020. 3,
2
[35] Muhammad Zaigham Zaheer, Jin-ha Lee, Marcella Astrid,
and Seung-Ik Lee. Old is gold: Redefining the adversarially
learned one-class classifier training paradigm. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 14183–14193, 2020. 3[36] Muhammad Zaigham Zaheer, Arif Mahmood, Marcella
Astrid, and Seung-Ik Lee. Claws: Clustering assisted weakly
supervised learning with normalcy suppression for anoma-
lous event detection. In Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part XXII 16 , pages 358–376. Springer, 2020.
3, 5, 2
[37] Muhammad Zaigham Zaheer, Arif Mahmood, Hochul Shin,
and Seung-Ik Lee. A self-reasoning framework for anomaly
detection using video-level labels. IEEE Signal Processing
Letters , 27:1705–1709, 2020. 3
[38] Muhammad Zaigham Zaheer, Arif Mahmood, Marcella
Astrid, and Seung-Ik Lee. Clustering aided weakly super-
vised training to detect anomalous events in surveillance
videos, 2022. 7, 2
[39] M Zaigham Zaheer, Arif Mahmood, M Haris Khan, Mattia
Segu, Fisher Yu, and Seung-Ik Lee. Generative cooperative
learning for unsupervised video anomaly detection. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 14744–14754, 2022. 2, 5, 6,
7, 8
[40] Chen Zhang, Guorong Li, Yuankai Qi, Shuhui Wang, Laiyun
Qing, Qingming Huang, and Ming-Hsuan Yang. Exploiting
completeness and uncertainty of pseudo labels for weakly
supervised video anomaly detection. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 16271–16280, 2023. 3
[41] Hongyi Zhang, Jan Bosch, and Helena Holmstr ¨om Olsson.
End-to-end federated learning for autonomous driving vehi-
cles. In 2021 International Joint Conference on Neural Net-
works (IJCNN) , pages 1–8. IEEE, 2021. 2
[42] Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li,
Ruiyi Zhang, Guoyin Wang, and Yiran Chen. Towards build-
ing the federated gpt: Federated instruction tuning. arXiv
preprint arXiv:2305.05644 , 2023. 1
12425
