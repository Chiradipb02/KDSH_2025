OmniSeg3D: Omniversal 3D Segmentation via
Hierarchical Contrastive Learning
Haiyang Ying1, Yixuan Yin1, Jinzhi Zhang1, Fan Wang2, Tao Yu1, Ruqi Huang1, Lu Fang1†
1Tsinghua University,2Alibaba Group
Figure 1. We propose OmniSeg3D , a 3D segmentation framework that (a) takes multi-view inconsistent 2D segmentations as input, and
outputs a consistent 3D feature field via a hierarchical contrastive learning method. This method supports (b) hierarchical segmentation,
(c) multi-object selection, and (d) holistic discretization in an interactive manner. See our project at oceanying.github.io/OmniSeg3D.
Abstract
Towards holistic understanding of 3D scenes, a general
3D segmentation method is needed that can segment diverse
objects without restrictions on object quantity or categories,
while also reflecting the inherent hierarchical structure. To
achieve this, we propose OmniSeg3D, an omniversal seg-
mentation method aims for segmenting anything in 3D all
at once. The key insight is to lift multi-view inconsistent 2D
segmentations into a consistent 3D feature field through a
hierarchical contrastive learning framework, which is ac-
complished by two steps. Firstly, we design a novel hier-
archical representation based on category-agnostic 2D seg-
†Corresponding author (fanglu@tsinghua.edu.cn, luvision.net).mentations to model the multi-level relationship among pix-
els. Secondly, image features rendered from the 3D fea-
ture field are clustered at different levels, which can be fur-
ther drawn closer or pushed apart according to the hier-
archical relationship between different levels. In tackling
the challenges posed by inconsistent 2D segmentations, this
framework yields a global consistent 3D feature field, which
further enables hierarchical segmentation, multi-object se-
lection, and global discretization. Extensive experiments
demonstrate the effectiveness of our method on high-quality
3D segmentation and accurate hierarchical structure un-
derstanding. A graphical user interface further facilitates
flexible interaction for omniversal 3D segmentation.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
20612
1. Introduction
3D segmentation forms one of the cornerstones in 3D scene
understanding, which is also the basis of 3D interaction,
editing, and extensive applications in virtual reality, medi-
cal analysis, and robot navigation. To meet the requirement
of complex world sensing, a general/omniversal category-
agnostic 3D scene segmentation method is required, capa-
ble of segmenting any object in 3D without limitations on
object quantity or categories. For instance, to accurately
discretize a pavilion as shown in Fig. 1, the user needs to
accurately segment each roof, column, eaves, and other in-
tricate structures. Existing 3D-based segmentation methods
based on 3D point clouds, meshes, or volumes fall short of
these requirements. They are either restricted to limited cat-
egories due to the scarcity of large-scale 3D datasets, such
as learning-based methods [19, 26, 31], or they could only
identify local geometric similarity or smoothness without
extracting semantic information, as typified by traditional
algorithms [16, 21, 45].
An alternative approach involves lifting 2D image un-
derstanding to 3D space, leveraging the impressive class-
agnostic 2D segmentation performance achieved by recent
methods [8, 27, 29, 32, 47]. Current lifting-based meth-
ods either rely on annotated 2D masks [4, 54, 65], or are
restricted to a limited set of pre-defined classes [3, 46].
Other methods propose distilling semantic-rich image fea-
tures [29, 43] onto point clouds [42, 49] or NeRF [18, 25,
28]. However, due to the absence of boundary information,
directly distilling these semantic feature into 3D space of-
ten leads to noisy segmentations [25, 42]. Further works use
SAM [27] or video segmentation methods [38] to generate
accurate 2D masks of targeted objects, and unproject them
into 3D space [6]. However, these approaches are limited to
single-object segmentation and exhibit unstable results in
cases with severe occlusion because the 2D segmentation is
performed on each image independently.
Therefore, significant challenges still persist. First,
multi-view consistency remains an obstacle due to the sub-
stantial variations in 2D segmentations across different
viewpoints. Second, ambiguity arises when distinguish-
ing in-the-wild objects like eaves and roofs, which inher-
ently possess a hierarchical semantic structure. To this end,
we propose OmniSeg3D, an Omniversal 3D Segmentation
method which enjoys multi-object, category-agnostic, and
hierarchical segmentation in 3D all at once. We demon-
strate that a global 3D feature field (which can be formu-
lated on NeRF [37, 41, 58], point cloud [24], mesh [51, 60],
etc) is inherently well-suited for integrating occlusion-free,
boundary-clear, and hierarchical semantic information from
2D segmentations through hierarchical contrastive learning.
The key lies in hierarchically clustering 2D image features
rendered from the 3D feature field at different levels of
segmentation blocks, where the multi-level segmentations
FeaturepointAFeaturepointB
AB
(2)PushApart
AB
(3)PullCloser
DifferentiableRenderingDifferentiableRendering
(4)AfterOptimization(1)InitializationContrastiveRegularization
Figure 2. Method Overview . We utilize differentiable render-
ing on a 3D feature field to generate 2D feature points, which are
then regularized by multi-view 2D segmentations through a hierar-
chical contrastive learning strategy, resulting in a hierarchical 3D
feature field that supports versatile 3D segmentation tasks.
are specified by a proposed hierarchical 2D representation.
Then the clustered features will be drawn closer or pushed
apart via a hierarchical contrastive loss, which enables the
learning of a feature field that encodes hierarchical infor-
mation into feature distances, effectively eliminating se-
mantic inconsistencies between different images. This uni-
fied framework facilitates multi-object selection, hierarchi-
cal segmentation, global discretization in 3D space.
We evaluate OmniSeg3D on segmentation tasks for sin-
gle object selection and hierarchical inference. Exten-
sive quantitative and qualitative results on real-world and
synthetic datasets demonstrate our method enjoys high-
quality 3D object segmentation and holistic comprehension
of scene structure across various scales. An interactive in-
terface is also provided for flexible 3D segmentation. Our
contributions are summarized as follows:
• We propose a hierarchical 2D representation to reveal
and store the part-level relationship within objects based
on class-agnostic 2D segmentations and a voting strategy.
• We present a hierarchical contrastive learning method
to optimize a globally consistent 3D hierarchical feature
field given 2D observations.
• Extensive experiments demonstrate that our omniversal
3D segmentation framework can segment anything in
3D all at once, which enables hierarchical segmentation,
multi-object selection, and 3D discretization.
2. Related Works
2.1. 2D Segmentation
2D segmentation has experienced a long history. Early
works mainly rely on the clue of pixel similarity and con-
20613
tinuity [1, 13, 17] to segment images. Since the introduc-
tion of FCN [34] and large-scale 2D datasets [15, 53], there
has been a rapid expansion in research of different sub-
fields of 2D segmentation [7, 20, 26, 63]. The involve-
ment of transformer [50] in the segmentation domain has
led to the proposal of several novel segmentation architec-
tures [10, 11, 64]. However, most of these methods are lim-
ited to pre-defined class labels.
Prompt-based segmentation is a special task that enables
segmenting unseen object categories [8, 32, 47]. One recent
breakthrough is the Segment Anything Model (SAM) [27],
aiming to unify the 2D segmentation task through the intro-
duction of a prompt-based segmentation approach , is con-
sidered a promising innovation in the field of vision.
2.2. 3D Segmentation
Closed-set segmentation. The task of 3D segmentation has
been explored with various types of 3D representation such
as RGBD images [52, 55], pointcloud [22, 56, 57], and vox-
els [19, 23, 31, 33]. However, due to the insufficiency of an-
notated 3D datasets for training a unified 3D segmentation
model, they are still limited to closed-set 3D understanding,
which largely restricts the application scenarios.
Given the shortage of annotated 3D datasets [14, 61]
for the development of foundational 3D models, recent
works have proposed to lift 2D information into 3D for
3D segmentation and understanding. Some works rely
on ground truth masks [4, 12, 54, 65] or pre-trained 2D
semantic/instance segmentation models for mask genera-
tion [3, 46]. However, obtaining ground truth annota-
tion is often impractical for general scenarios, and model-
based methods typically provide closed-set object masks
only. ContrastiveLift [3] proposes to segment closed-set
3D objects via contrastive learning. However, it can-
not handle unseen classes and reveal object hierarchy. In
contrast, our method achieves panoptic, category-agnostic,
and hierarchical segmentation based on a hierarchical con-
trastive learning framework, which can be interpreted as
a sound combination of click-based segmentation methods
and holistic 3D modeling.
Open-set segmentation. LERF [25] and subsequent
works [18, 28, 49] propose to distill language feature [43]
into 3D space for open-vocabulary interactive segmenta-
tion. Since the learned feature is trained on entire images
without explicit boundary supervision, these methods prone
to produce noisy segmentation boundaries. Besides, these
methods are unable to distinguish different instances due
to the lack of instance-level supervision. Alternatively, we
take advantage of category-agnostic segmentation methods
and distill the 2D results into 3D to get a consistent feature
field and enable high-quality 3D segmentation.
SPInNeRF [38] utilizes video segmentation to initialize
2D masks and then lift them into 3D space with a NeRF. Afollowed multi-view refinement stage is utilized to achieve
consistent 3D segmentation. SA3D [6] introduces an on-
line interactive segmentation method that propagates one
SAM [27] mask into 3D space and other views iteratively.
However, these methods may heavily rely on a good choice
of reference view and cannot handle complex cases such
as severe occlusion. Instead, our method can segment any-
thing in 3D all at once via a global consistent feature field,
which is more robust to object occlusion.
Hierarchical segmentation. For hierarchical segmenta-
tion, existing methods mainly rely on the paradigm of geo-
metric analysis of single-class objects [9, 39, 40, 57, 59],
which can only be applied to specific categories. In-
stead, we focus on general scenarios and achieve category-
agnostic hierarchical segmentation in 3D.
3. Methods
Given a set of 2D images with poses [37] as input, our
goal is to learn a 3D feature field that supports multi-
object, category-agnostic, and hierarchical segmentation all
at once. We first use a pretrained 2D segmentation model
to segment each image into a set of masks Msegs. The
masks are then organized into smaller units Psegs accom-
panied with a correlation indicator Chi. During training,
a pre-defined 3D feature field can be rendered to features
f∈RDon 2D image plane. With our proposed hierarchical
contrastive clustering strategy, the rendered features will be
forced to establish precise feature distance with right order
that corresponds to the patch relationship depicted in Chi.
In this section, we first introduce our hierarchical 2D repre-
sentation (Sec. 3.1) which models the hierarchical relation-
ship among pixels. Then a hierarchical contrastive learning
framework will be discussed (Sec. 3.2), including both basic
and hierarchical implementations for lifting 2D masks into
3D space. Implementation details are shown in Sec. 3.3. Fi-
nally, we show how to use the optimized 3D field to achieve
various 3D segmentation tasks interactively (Sec. 3.4).
3.1. Hierarchical Representation
2D Label Map Creation. We borrow the idea from [65]
that multi-view 2D label maps can be lifted into a 3D feature
field via differentiable rendering. The key difference is that
for omniversal segmentation, a 2D segmentation method
should be able to handle unseen categories. We seek so-
lution from click-based method like SAM [27], which ex-
hibits a class-agnostic property. Given an input image I, we
sample a grid of points (typically 32×32) as the prompts
and send them into a pretrained SAM [27] to get a set of 2D
binary masks Msegs={mi∈RH×W|i= 1, ...,|Msegs|}
(see Fig. 3(a)). To create a label map as training data for
3D field optimization (as in [65]), one way is to overlap
masks in Msegs one-by-one based on their pixel counts
20614
!!!"!#!$!%!!11111!&12122!'11222!(12243!)12233(c)Modelaspatchesandcorrelationmatrix(b)Modelasoverlappedmasks[22]𝑝!𝑝"𝑝#𝑝$𝑝%One-hotPartition:𝑚!:{𝑝!,𝑝",𝑝#,𝑝$,𝑝%}𝑚":{𝑝",𝑝$,𝑝%}𝑚#:{𝑝#,𝑝$,𝑝%}𝑚$:{𝑝$}𝑚!𝑚"𝑚#𝑚$(a)Masksfrom2DSegmentor
Correlationmatrix𝑝$𝑝%𝑝"𝑝#𝑝!𝑑=1𝑑=𝑑!"#Hierarchytreeof𝑝$OverlapOverlapping-basedindexmap
PatchindexmapPatchesandOne-hotpartitionlist𝑚!"𝑚#"𝑚$"𝑚%"ExtractedMasks
𝑝!𝑝"𝑝#𝑝$𝑝%Figure 3. 2D Hierarchical Representation . (a) For each image, click-based 2D segmentors provide a set of masks {mi}. (b) Directly
overlapping masks implemented by conventional methods [27] lead to the loss of hierarchical information. (c) Patch-based modeling
effectively preserves inclusion information. The hierarchical representation of each image includes a patch index map Ipand a correlation
matrix Chi, where the relevance between piand other patches is evaluated via a voting strategy.
(see Fig. 3(b)). Unfortunately, this method (as done in
SAM [27]), may destroy the hierarchical information em-
bedded inside Msegs, since each pixel in image Imay be-
long to more than one masks in Msegs(consider the fact that
a pixel belonging to the mask of a chair may also belong
to the mask of the chair’s leg). Alternatively, storing each
Msegsdirectly may result in high memory consumption, as
|Msegs|usually exceeds 500, using memory equivalent to
20x input images.
Hierarchical Modeling. To overcome the aforementioned
problem, we design a novel representation that preserves
the hierarchical information within each image and largely
reduces the memory consumption. Specifically, we divide
the entire 2D image into disjoint patches. As shown in
Fig. 3(a), let mi∈Msegs,(i= 1, ...,4)represent masks
inMsegs. For each pixel, we create a one-hot vector to in-
dicate which masks the pixel belongs to. Then we define a
patch set Psegs, where each patch includes pixels that share
the same one-hot vector as shown in Fig. 3(c). Psegsalso re-
sults in a patch index map Ip, on which each pixel contains
an index of the patch.
Next, we proceed to model the hierarchical structure
with patches Psegs (as the unit) and the original masks
Msegs(as the correlation binding). The core idea is that, if
two patches fall into the same mask, then these two patches
has some degree of correlation. To model the degree of the
correlation, we introduce a voting-based strategy. Specifi-
cally, for each pair of patches piandpj, we count the num-
ber of masks that contain both piandpj. By traversing allthe patch pairs, we get a matrix Chi∈RNp×Np:
Chi(pi, pj) =NmX
k=11(pi⊆mk)· 1(pj⊆mk),(1)
where Nm=|Msegs|is the number of masks and Np=
|Psegs|is the number of patches. This process can be in-
terpreted as utilizing masks to vote for the relationship be-
tween patches. To inference the hierarchical relationship
among patches, we select a patch pias the anchor and take
thei-th row of matrix Chi(pi,·) =vi. We then sort the
patches according to the vote counts in vector viand con-
struct a hierarchical tree for anchor patch pi, as illustrated in
Fig. 3(c). Patches located at higher level (smaller d) in the
tree has stronger correlation to pi, which can serve as the
guidance of the hierarchical contrastive learning introduced
in the subsequent section. As a summary, we construct the
hierarchical representation for each image, which consists
of a patch index map Ipand a correlation matrix Chi.
3.2. Hierarchical Contrastive Learning
In this section, we show how to lift the hierarchical relation-
ship of 2D patches into the 3D space through a hierarchical
contrastive learning framework.
3D feature field. Our 3D representation is built upon
NeRFs [37, 41], which uses an MLP FΘto model the den-
sityσiand color ciof each 3D point xi∈R3under view
direction di∈R2. Additionally, we define a segmentation
identity feature fi∈RDto model semantic information of
each 3D point. The formulation is shown below:
(σi,fi) =FΘ(γ1(xi)),ci=FΘ(γ1(xi), γ2(di)),(2)
20615
!!!"!#!$!%
!! 1 1 1 1 1
!& 1 2 1 2 2
!' 1 1 2 2 2
!( 1 2 2 4 3
!) 1 2 2 3 3!!!"!#!$!%
!! 1 1 1 1 1
!& 1 2 1 2 2
!' 1 1 2 2 2
!( 1 2 2 4 3
!) 1 2 2 3 3!!!"!#!$!%
!! 1 1 1 1 1
!& 1 2 1 2 2
!' 1 1 2 2 2
!( 1 2 2 4 3
!) 1 2 2 3 3
(b)Hierarchical Representation (a)Input Images
(d)Hierarchical Contrastive ClusteringHierarchical Tree Inference
RGB
Feature map
(c)Rendering ofRGB andFeature Image
𝑙=1𝑙=2
𝑙=3
Semantic Feature Optimization
𝐷(, )<𝐷(,)
𝐷(,)<𝐷(,)<𝐷(,)1.Basic Contrastive Regularization
2.Hierarchical Order RegularizationPatch index map Correlation matrix
𝐷(&,")Distance Function
Optimization Direction
Feature point sampling
Choose ananchor point
Level 1
Level 2
Level 3Anchor point
Negative samplesPositive
samples
Hierarchical leveling
𝜎
𝐜
𝐟
𝐱,𝐝Differentiable rendering
𝐟−𝐟
MLP #
Figure 4. Hierarchical Contrastive Learning Framework . (a) For each input RGB image, we apply (b) 2D hierarchical modeling to
get a patch index map and a correlation matrix. During training, we utilize (c) NeRF-based rendering pipeline to render features from 3D
space and apply hierarchical contrastive learning (d) to the rendered features to optimize the feature field for segmentation.
where γ1andγ2are positional encoding functions in [41].
Subsequently, by integrating the sampled attributes ci,
σi, and fialong the ray, we can get rendered c(r) =Pn
i=1Tiαiciandf(r) =Pn
i=1Tiαifion 2D image
plane [25, 37], where αi= 1−exp(−σiδi)is the opac-
ity,Ti=Qi−1
j=1(1−αj)is the accumulated opacity, and
δi=ri+1−riis the distance between adjacent samples.
Basic implementation. In this section, we present a basic
implementation that lifts 2D segmentations into 3D space
via differentiable rendering and contrastive learning. No
hierarchical information is considered in this section.
For each image, we randomly sample Npoints on it and
identify the patch id of each point according to the patch
index map Ip. Then we render features fi(i∈[1, N])of
these points via differentiable rendering from the 3D fea-
ture field. For each sampled point, we designate the points
with the same patch id as positive samples, and all the other
sampled points as negative ones. The correlation between
two feature points is modelled as cosine distance fi·fj.
We apply a contrastive clustering method [30] to super-
vised the feature distance between rendered feature points.
Specifically, cluster Fi(i∈[1, Np])is defined as the col-
lection of rendered features that share the same patch id i
andfi
jis the j-th feature in Fi. The center of each cluster
is defined as the mean value ¯fiof features in Fi. Then for
each chosen feature point fi
j, we take ¯fiand¯fkas positive
and negative samples respectively. The contrastive loss is
shown below, which favors high similarity among samples
within the same patch piand low similarity between sam-
ples located in different patches ( piandpk):
LCC=−1
NpNpX
i=1|Fi|X
j=1logexp(fi
j·¯fi/ϕi)
PNp
k=1exp(fi
j·¯fk/ϕk),(3)
where Npis the number of patch ids, ϕiis the temperature
of cluster Fito balance the cluster size and variance: ϕi=Pni
j=1||fi
j−¯fi||2/(nilog(ni+α)), ni=|Fi|,α= 10 is
a smooth term to prevent small clusters from exhibiting an
excessively large ϕi.
Note that ConstrastiveLift [3] uses a slow-fast learning
strategy for stable training. We refer to contrastive cluster-
ing [30] to realize faster training and stable convergence.
Hierarchical implementation. Here we show how to in-
corporate hierarchical information into the pipeline of con-
trastive learning. Still we cluster the sampled feature points
intoNpfeature sets Fi(i∈[1, Np])based on the patch in-
dex map Ip. Then for each anchor patch pi, we find all
related patches according to the correlation matrix Chiand
construct a set {Si
d}where Si
dis the patch index set at level
d∈[1, di
max]of anchor patch pi(e.g., Si=4
d=3={2,3}in
Fig. 3). Note that all the samples in the related patches are
potential positive samples in this formulation.
To achieve hierarchical contrastive clustering in 3D, we
employ the hierarchical regularization proposed in [62].
Firstly, we add a regularization term λd−1to Eq. 3 with
a per-level decay factor λ≤1, which means higher penalty
is applied to the patches with stronger correlation to the an-
chor patch pi. Secondly, a strategy for regularizing the opti-
mization order is implemented to ensure that a patch higher
in the hierarchy tree (smaller d) exhibits a higher feature
similarity with the anchor patch than patches at lower levels
(as shown in Fig. 4(d)). The final loss is shown below:
LH=NpX
i=1di
maxX
d=1λd−1
NL|Fi|X
j=1X
s∈Si
dmax(Li,j(s),Li,j
max(d−1)),
(4)
where Si
dis the patch index set at level dof anchor patch
pi,Li,j(s)is the contrastive loss that favors high similarity
20616
LERF -Waldo 360-Counter
 Blender -Lego LLFF -Flower
 T&T- Truck
 360-Garden
(b)Highlighted Obj. (d)Object/Part Segmentation Results (c)Feature Map (a)Rendered Img.
LERF -Figurines
Figure 5. Qualitative performance visualization. We visualize
both scene-level and object-level feature maps (with UMAP [35])
to reveal the hierarchical structure learned by OmniSeg3D.
between fi
jand feature center of patch ps(s∈Si
d):
Li,j(s) =−logexp(fi
j·¯fs/ϕs)
PNp
k=1exp(fi
j·¯fk/ϕk), (5)
andLi,j
max(d)is the maximum loss at level d:
Li,j
max(d) = max
s∈Si
dLi,j(s). (6)
Since the volumetric rendering may introduce ambiguity
in the calculation of the integration, we apply normaliza-
tion loss to ensure feature vectors distributed on the sphere
surface: Lnorm =1
NPN
i=1(||fi|| −1)2.
3.3. Implementation details
The method is built on top of InstantNGP [41] and we fol-
lows the same parameter settings for positional encoding
and MLP FΘ. We adopt a two-stage training paradigm.
For each scene, we first train our model with Lgeo=
Lc+w1Lregto construct right geometry, where Lc=P
r∥c(r)−cgt(r)∥2
2, andLreg=P
r−o(r) log( o(r)),
o(r) =PN
i=1Tiαi.Lregis used to regularize each ray
to be completely saturated or empty. Then the feature field
will be supervised via Lsem=w2LH+w3Lnorm . The per-
level decay factor is set to λ= 0.5. The hyper-parameters
are set to w1= 1e−3,w2= 5e−4,w3= 5e2for all
the experiments. The ray number of each batch is 8192and both stages are trained for 50k iterations, which takes
30∼40min in total on a single RTX 3090 GPU.
Although we choose InstantNGP [41] as our backbone,
we demonstrate that OmniSeg3D is a lightweight plug-
in which can be easily adapted to 3D representations like
mesh, point cloud, and gaussian splatting [24]. For 2D
backbones, beside of SAM [27], other click-based segmen-
tation methods like [8, 32, 47] can be used as a substitute.
Please refer to our supplementary material for more details.
3.4. Interactive Segmentation
To realize flexible and interactive 3D segmentation, we de-
velop a graphical user interface (GUI). The GUI can serve
as a novel 3D annotation tool, which may largely improve
the efficiency of 3D data annotation and help solve the 3D
data shortage problem. One typical case based on NeRF is
shown in Fig. 1. With a single click on the object of inter-
est, our model generates a score field based on feature sim-
ilarities. By adjusting the binarization threshold, the seg-
mentation can seamlessly traverse the scene hierarchy from
atomic components to entire objects, and holistic portions
of the scene. Besides, users can select and segment multi-
ple objects simultaneously through multiple clicks.
4. Experiments
Our experiments encompass various datasets including in-
door [37, 48] and outdoor [2, 25, 36] scenes. Qualitative re-
sults can be found in Fig. 5. For quantitative performance,
we evaluate OmniSeg3D on both hierarchical (Sec. 4.1) and
instance (Sec. 4.2) 3D segmentation tasks.
4.1. Hierarchical 3D Segmentation
Dataset. To quantitatively evaluate OmniSeg3D, we cre-
ate a scene-scale dataset with hierarchical semantic anno-
tations. We utilize the Replica dataset [48] processed by
Semantic-NeRF [65], which comprises 8 realistic indoor
scenes. We uniformly sample a total of 281 images and
manually annotate each image with a query pixel qand
two corresponding masks, the smaller one ML1properly
included by the larger one ML2⊃ML1.ML1andML2
typically correspond to object parts and complete instances
respectively, as shown in Fig. 6. In case multiple levels
of reasonable segmentations Ma⊂Mb⊂Mcexist, we
choose different pairs as the ground truth (ML1, ML2)in
different images, so that the selected masks exhibit diverse
scales and represent the full range of possible hierarchical
relationships present in the scene.
Benchmark. We benchmark our algorithm as follows.
The model receives as input a 2D query point qin the
given frame I, and is expected to output a dense 2D score
map{score ( p)|p∈I}. Ideally, there exist thresholds
20617
Level 1
Level 1
Level 1
Level 2Level 2
Level 2DINO SAM LSeg Ours, w/o hierarchy OmniSeg3D (Ours) GT Image
TPFP
FNPred
GTFigure 6. Comparison of hierarchical segmentation results on the Replica dataset. Prompts are shown as black dots. Colored pixels denote
TP: True-Positive, FP: False-Positive and FN: False-Negative respectively.
MethodmIoU (%)
Level 1 Level 2 Average
DINO [5] 67.9 64.2 66.1
LSeg [29] 51.7 82.1 66.9
SAM [27] 92.8 80.2 86.5
Ours, w/o hierarchy 93.1 80.4 86.7
OmniSeg3D (ours) 91.3 88.9 90.1
Table 1. Comparison of hierarchical segmentation on Replica [48].
th1> th 2that, when applied to the score map, yields
ML1⊂ML2respectively:
∃this.t. M Li={p∈I|score ( p)> th i}. (7)
For evaluation, we choose the thresholds (th1, th2)that
maximize the IoU between the predicted masks and the
ground truth (ML1, ML2), and define the metrics as:
IoULi= max
thiIoU ({p∈I|score ( p)> th i}, MLi),(8)
and we have IoUAvg= (IoU L1+ IoU L2)/2.
Baseline methods. We first compare OmniSeg3D with
state-of-the-art 2D segmentation models and semantic fea-
ture extractors. SAM [27] predicts three hierarchical masksfrom the point query. We compare each to the ground truth
masks (ML1, ML2)and report the highest IoU. DINO [5]
and LSeg [29] (based on CLIP [43]) predict a feature image,
which is converted to a score map based on cosine similar-
ities and then binarized using Eq. 8 to compute the IoU. In
addition, we compare our full method with the basic imple-
mentation in Sec. 3.2, i.e., 3D contrastive learning without
hierarchical modelling.
Results. The quantitative and qualitative results of hi-
erarchical segmentation on the Replica [48] dataset are
demonstrated in Tab. 1 and Fig. 6 respectively. Our Om-
niSeg3D achieves the highest average mIoU, while substan-
tially leading in level-2 segmentation, which shows the ad-
vantage on high-level semantic understanding.
As shown in Fig. 6, the self-supervised DINO method
struggles to delineate clear object boundaries. LSeg cap-
tures overall semantics better but fails to discriminate be-
tween instances. SAM performs well at fine-grained seg-
mentation, but occasionally fails to group together multiple
objects or large regions, resulting in lower level-2 mIoU.
Our basic implementation without hierarchical modeling
inherits these characteristics of SAM, with slightly better
metrics. Our full method achieves large improvements in
high-level segmentation while maintains comparable per-
formance in level-1 segmentation, which implies that the
hierarchical modelling effectively aggregates fragmented
part-whole correlations from multiple views. Moreover, in
20618
Dataset Method mIoU (%) Acc (%)
NVOSNVOS [44] 70.1 92.0
ISRF [18] 83.8 96.4
SA3D [6] 90.3 98.2
OmniSeg3D (ours) 91.7 98.4
MVSegMVSeg [38] 90.9 98.9
SA3D [6] 92.4 98.9
OmniSeg3D (ours) 94.3 99.3
ReplicaMVSeg [38] 32.4 -
SA3D [6] 83.0 -
OmniSeg3D (ours) 84.4 -
Table 2. Quantitative comparison of instance segmentation.
contrast to the instability in performance that 2D models
may exhibit across different resolutions, our OmniSeg3D
implicitly integrates voting-based correlations from multi-
view inputs, which distills a stable hierarchical semantic or-
der into the 3D representation, thereby enhancing global-
scale semantic clustering.
4.2. 3D Instance Segmentation
While designed for omniversal 3D segmentation, our
method is able to handle 3D instance segmentation as a sub-
task. Different from existing methods [6, 38], we do not
require instance-specific training. The 3D feature field of
OmniSeg3D is trained only once for each scene and reused
for different instances, while still performing competitively
on datasets proposed by previous work.
We follow NVOS [44], SPIn-NeRF [38] and SA3D [6]
to benchmark 3D instance segmentation as prompt propaga-
tion. For each scene, given prompts (scribbles or masks) in
the reference view, the algorithm is supposed to segment the
instance in the target view. The predicted mask is compared
with the ground truth segmentation. As shown in Tab. 2,
OmniSeg3D outperforms the baseline methods in terms of
mIoU and pixel-wise accuracy, while alleviating the need to
retrain different segmentation fields for the same scene.
4.3. Ablation Studies
Hierarchical decay. As illustrated in Eq. 4, we apply a
decay λ∈[0,1]to downweight the contrastive loss for
patches of lower correlation with the anchor. Setting λ= 0
resembles the basic implementation without hierarchical
modeling, while setting λ= 1puts equal emphasis on sam-
ples from all hierarchies, enhancing high-level semantics.
Tab. 3 demonstrates hierarchical segmentation results on the
Replica dataset. With the increase of λ,IoUL1decreases
while IoUL2increases, reaching IoUL1≈IoUL2atλ= 1.
We choose λ= 0.5with the highest average mIoU, imply-
ing a balance between local and global semantic clustering.Hierar. Per-level Hierar. mIoU (%) Instance
model decay λ Lv.1 Lv.2 Avg. mIoU (%)
× - 93.1 80.4 86.7 83.6
✓ 0.1 92.5 84.7 88.6 84.3
✓ 0.2 92.1 86.5 89.4 84.6
✓ 0.5 91.3 88.9 90.1 84.4
✓ 1 89.2 89.2 89.2 83.3
Table 3. Ablation of hierarchical modelling on Replica.
Feat. dim. 4 8 16 32 64 128
Avg. mIoU 89.8 91.8 93.0 93.0 93.1 93.2
Table 4. Ablation of feature dimensions on room-0 of Replica.
Feature dimension. We study how the dimension Dof
semantic features affects the performance of hierarchical
contrastive clustering. The Tab. 4 indicates that the average
mIoU rises with Dand levels off after D= 16 , suggesting
that a Dof16is sufficient for our algorithm.
5. Limitations
Due to the absence of a clear definition for hierarchy lev-
els, there is no assurance that the objects will be segmented
at the same level by simply clustering features with one
threshold. To address this issue, text-aligned hierarchical
segmentation may be a future direction. Besides, since the
contrastive learning is applied on single images, two objects
that have never appeared in the same image may have sim-
ilar semantic feature. This problem can be alleviated by in-
troducing local geometric continuity, but global contrastive
learning across images is also a topic worth exploring.
6. Conclusion
In this paper, we propose OmniSeg3D, an omniversal seg-
mentation method that facilitates holistic understanding of
3D scenes. Leveraging a hierarchical representation and
a hierarchical contrastive learning framework, OmniSeg3D
effectively transforms inconsistent 2D segmentations into
a globally consistent 3D feature field while retaining hi-
erarchical information, which enables correct hierarchi-
cal 3D sensing and high-quality object segmentation per-
formance. Besides, variant interactive functionalities in-
cluding hierarchical inference, multi-object selection, and
global discretization are realized, which may further enable
downstream applications in the field of 3D data annotation,
robotics and virtual reality.
Acknowledgements This work is supported in part by Nat-
ural Science Foundation of China (NSFC) under contract
No. 62125106 and 62088102, in part by Tsinghua-Zhijiang
joint research center.
20619
References
[1] Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien
Lucchi, Pascal Fua, and Sabine S ¨usstrunk. Slic superpix-
els compared to state-of-the-art superpixel methods. IEEE
transactions on pattern analysis and machine intelligence ,
34(11):2274–2282, 2012. 3
[2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.
Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded
anti-aliased neural radiance fields. CVPR , 2022. 6
[3] Yash Bhalgat, Iro Laina, Jo ˜ao F Henriques, Andrew Zisser-
man, and Andrea Vedaldi. Contrastive lift: 3d object instance
segmentation by slow-fast contrastive fusion. arXiv preprint
arXiv:2306.04633 , 2023. 2, 3, 5
[4] WANG Bing, Lu Chen, and Bo Yang. Dm-nerf: 3d scene
geometry decomposition and manipulation from 2d images.
InThe Eleventh International Conference on Learning Rep-
resentations , 2022. 2, 3
[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In Pro-
ceedings of the International Conference on Computer Vi-
sion (ICCV) , 2021. 7
[6] Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi
Xie, Xiaopeng Zhang, and Qi Tian. Segment anything in 3d
with nerfs. arXiv preprint arXiv:2304.12308 , 2023. 2, 3, 8
[7] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. IEEE transactions on pattern
analysis and machine intelligence , 40(4):834–848, 2017. 3
[8] Xi Chen, Zhiyan Zhao, Yilei Zhang, Manni Duan, Donglian
Qi, and Hengshuang Zhao. Focalclick: Towards prac-
tical interactive image segmentation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 1300–1309, 2022. 2, 3, 6
[9] Zhiqin Chen, Andrea Tagliasacchi, and Hao Zhang. Bsp-net:
Generating compact meshes via binary space partitioning. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 45–54, 2020. 3
[10] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-
pixel classification is not all you need for semantic segmen-
tation. Advances in Neural Information Processing Systems ,
34:17864–17875, 2021. 3
[11] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-
der Kirillov, and Rohit Girdhar. Masked-attention mask
transformer for universal image segmentation. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 1290–1299, 2022. 3
[12] Xinhua Cheng, Yanmin Wu, Mengxi Jia, Qian Wang, and
Jian Zhang. Panoptic compositional feature field for ed-
itable scene rendering with network-inferred labels via met-
ric learning. 2023 IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , pages 4947–4957,
2023. 3
[13] Guy Barrett Coleman and Harry C Andrews. Image seg-
mentation by clustering. Proceedings of the IEEE , 67(5):
773–785, 1979. 3[14] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nießner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 5828–5839, 2017. 3
[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248–255. Ieee, 2009. 3
[16] Peter Dorninger and Clemens Nothegger. 3d segmentation
of unstructured point clouds for building modelling. Interna-
tional Archives of the Photogrammetry, Remote Sensing and
Spatial Information Sciences , 35(3/W49A):191–196, 2007.
2
[17] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient
graph-based image segmentation. International journal of
computer vision , 59:167–181, 2004. 3
[18] Rahul Goel, Dhawal Sirikonda, Saurabh Saini, and PJ
Narayanan. Interactive segmentation of radiance fields. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 4201–4211, 2023. 2, 3,
8
[19] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg:
Occupancy-aware 3d instance segmentation. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 2940–2949, 2020. 2, 3
[20] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-
shick. Mask r-cnn. In Proceedings of the IEEE international
conference on computer vision , pages 2961–2969, 2017. 3
[21] Karl Heinz H ¨ohne and William A Hanson. Interactive 3d
segmentation of mri and ct volumes using morphological op-
erations. Journal of computer assisted tomography , 16(2):
285–294, 1992. 2
[22] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan
Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham.
Randla-net: Efficient semantic segmentation of large-scale
point clouds. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 11108–
11117, 2020. 3
[23] Jing Huang and Suya You. Point cloud labeling using 3d con-
volutional neural network. In 2016 23rd International Con-
ference on Pattern Recognition (ICPR) , pages 2670–2675.
IEEE, 2016. 3
[24] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk ¨uhler,
and George Drettakis. 3d gaussian splatting for real-time
radiance field rendering. ACM Transactions on Graphics
(ToG) , 42(4):1–14, 2023. 2, 6
[25] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo
Kanazawa, and Matthew Tancik. Lerf: Language embedded
radiance fields. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 19729–19739,
2023. 2, 3, 5, 6
[26] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten
Rother, and Piotr Doll ´ar. Panoptic segmentation. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 9404–9413, 2019. 2, 3
20620
[27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. arXiv preprint arXiv:2304.02643 , 2023. 2, 3, 4, 6,
7
[28] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitz-
mann. Decomposing nerf for editing via feature field distil-
lation. Advances in Neural Information Processing Systems ,
35:23311–23330, 2022. 2, 3
[29] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen
Koltun, and Rene Ranftl. Language-driven semantic seg-
mentation. In International Conference on Learning Rep-
resentations , 2022. 2, 7
[30] Junnan Li, Pan Zhou, Caiming Xiong, and Steven Hoi. Pro-
totypical contrastive learning of unsupervised representa-
tions. In International Conference on Learning Represen-
tations , 2020. 5
[31] Leyao Liu, Tian Zheng, Yun-Jou Lin, Kai Ni, and Lu Fang.
Ins-conv: Incremental sparse convolution for online 3d seg-
mentation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 18975–
18984, 2022. 2, 3
[32] Qin Liu, Zhenlin Xu, Gedas Bertasius, and Marc Nietham-
mer. Simpleclick: Interactive image segmentation with sim-
ple vision transformers. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 22290–
22300, 2023. 2, 3, 6
[33] Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-
voxel cnn for efficient 3d deep learning. Advances in Neural
Information Processing Systems , 32, 2019. 3
[34] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition , pages 3431–3440, 2015. 3
[35] Leland McInnes, John Healy, and James Melville. Umap:
Uniform manifold approximation and projection for dimen-
sion reduction. arXiv preprint arXiv:1802.03426 , 2018. 6
[36] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon,
Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and
Abhishek Kar. Local light field fusion: Practical view syn-
thesis with prescriptive sampling guidelines. ACM Transac-
tions on Graphics (TOG) , 2019. 6
[37] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. Communications of the ACM , 65(1):99–106, 2021. 2,
3, 4, 5, 6
[38] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Konstanti-
nos G Derpanis, Jonathan Kelly, Marcus A Brubaker, Igor
Gilitschenski, and Alex Levinshtein. Spin-nerf: Multiview
segmentation and perceptual inpainting with neural radiance
fields. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 20669–20679,
2023. 2, 3, 8
[39] Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka,
Niloy Mitra, and Leonidas J Guibas. Structurenet: Hierarchi-
cal graph networks for 3d shape generation. arXiv preprint
arXiv:1908.00575 , 2019. 3[40] Kaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna
Tripathi, Leonidas J Guibas, and Hao Su. Partnet: A large-
scale benchmark for fine-grained and hierarchical part-level
3d object understanding. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition ,
pages 909–918, 2019. 3
[41] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a mul-
tiresolution hash encoding. ACM Transactions on Graphics
(ToG) , 41(4):1–15, 2022. 2, 4, 5, 6
[42] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea
Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al.
Openscene: 3d scene understanding with open vocabularies.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 815–824, 2023. 2
[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 2, 3, 7
[44] Zhongzheng Ren, Aseem Agarwala, Bryan Russell, Alexan-
der G Schwing, and Oliver Wang. Neural volumetric ob-
ject selection. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 6133–
6142, 2022. 8
[45] Ruwen Schnabel, Roland Wahl, and Reinhard Klein. Ef-
ficient ransac for point-cloud shape detection. In Computer
graphics forum , pages 214–226. Wiley Online Library, 2007.
2
[46] Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bul `o, Nor-
man M ¨uller, Matthias Nießner, Angela Dai, and Peter
Kontschieder. Panoptic lifting for 3d scene understanding
with neural fields. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
9043–9052, 2023. 2, 3
[47] Konstantin Sofiiuk, Ilya A Petrov, and Anton Konushin. Re-
viving iterative training with mask guidance for interactive
segmentation. In 2022 IEEE International Conference on
Image Processing (ICIP) , pages 3141–3145. IEEE, 2022. 2,
3, 6
[48] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik
Wijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal,
Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan,
Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang
Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler
Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva,
Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael
Goesele, Steven Lovegrove, and Richard Newcombe. The
Replica dataset: A digital replica of indoor spaces. arXiv
preprint arXiv:1906.05797 , 2019. 6, 7
[49] Ayc ¸a Takmaz, Elisabetta Fedele, Robert W Sumner, Marc
Pollefeys, Federico Tombari, and Francis Engelmann. Open-
mask3d: Open-vocabulary 3d instance segmentation. arXiv
preprint arXiv:2306.13631 , 2023. 2, 3
[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
20621
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 3
[51] Guangyu Wang, Jinzhi Zhang, Kai Zhang, Ruqi Huang, and
Lu Fang. Giganticnvs: Gigapixel large-scale neural render-
ing with implicit meta-deformed manifold. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence , 2023. 2
[52] Weiyue Wang and Ulrich Neumann. Depth-aware cnn for
rgb-d segmentation. In Proceedings of the European con-
ference on computer vision (ECCV) , pages 135–150, 2018.
3
[53] Xueyang Wang, Xiya Zhang, Yinheng Zhu, Yuchen Guo,
Xiaoyun Yuan, Liuyu Xiang, Zerun Wang, Guiguang Ding,
David Brady, Qionghai Dai, et al. Panda: A gigapixel-
level human-centric video dataset. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 3268–3278, 2020. 3
[54] Qianyi Wu, Xian Liu, Yuedong Chen, Kejie Li, Chuanxia
Zheng, Jianfei Cai, and Jianmin Zheng. Object-
compositional neural implicit surfaces. In European Con-
ference on Computer Vision , pages 197–213. Springer, 2022.
2, 3
[55] Yajie Xing, Jingbo Wang, and Gang Zeng. Malleable 2.5 d
convolution: Learning receptive fields along the depth-axis
for rgb-d scene parsing. In European Conference on Com-
puter Vision , pages 555–571. Springer, 2020. 3
[56] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen
Wang, Andrew Markham, and Niki Trigoni. Learning ob-
ject bounding boxes for 3d instance segmentation on point
clouds. Advances in neural information processing systems ,
32, 2019. 3
[57] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J
Guibas. Gspn: Generative shape proposal network for 3d
instance segmentation in point cloud. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3947–3956, 2019. 3
[58] Haiyang Ying, Baowei Jiang, Jinzhi Zhang, Di Xu, Tao Yu,
Qionghai Dai, and Lu Fang. Parf: Primitive-aware radiance
fusion for indoor scene novel view synthesis. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 17706–17716, 2023. 2
[59] Fenggen Yu, Zhiqin Chen, Manyi Li, Aditya Sanghi,
Hooman Shayani, Ali Mahdavi-Amiri, and Hao Zhang.
Capri-net: Learning compact cad shapes with adaptive prim-
itive assembly. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 11768–
11778, 2022. 3
[60] Jinzhi Zhang, Mengqi Ji, Guangyu Wang, Zhiwei Xue,
Shengjin Wang, and Lu Fang. Surrf: Unsupervised multi-
view stereopsis by learning surface radiance field. IEEE
Transactions on Pattern Analysis and Machine Intelligence ,
44(11):7912–7927, 2021. 2
[61] Jianing Zhang, Jinzhi Zhang, Shi Mao, Mengqi Ji, Guangyu
Wang, Zequn Chen, Tian Zhang, Xiaoyun Yuan, Qionghai
Dai, and Lu Fang. Gigamvs: a benchmark for ultra-large-
scale gigapixel-level 3d reconstruction. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 44(11):7534–
7550, 2021. 3[62] Shu Zhang, Ran Xu, Caiming Xiong, and Chetan Rama-
iah. Use all the labels: A hierarchical multi-label contrastive
learning framework. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
16660–16669, 2022. 5
[63] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid scene parsing network. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 2881–2890, 2017. 3
[64] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,
Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao
Xiang, Philip HS Torr, et al. Rethinking semantic segmen-
tation from a sequence-to-sequence perspective with trans-
formers. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 6881–6890,
2021. 3
[65] Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and An-
drew J Davison. In-place scene labelling and understanding
with implicit scene representation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 15838–15847, 2021. 2, 3, 6
20622
