VOODOO 3D: Vo lumetric Po rtrait D isentanglement fo r
One-Shot 3D Head Reenactment
Phong Tran1Egor Zakharov2Long-Nhat Ho1Anh Tuan Tran3Liwen Hu4Hao Li1,4
1MBZUAI2ETH Zurich3VinAI Research4Pinscreen
{the.tran, long.ho}@mbzuai.ac.ae anhtt152@vinai.io ezakharov@ethz.ch
liwen@pinscreen.com hao@hao-li.com
Holographic Display Source Driver Reenactment Geometry Novel Views
View 1
View 2
Figure 1. We introduce VOODOO 3D: a high-fidelity 3D-aware one-shot head reenactment technique. Our method
transfers the expression of a driver to a source and produces view consistent renderings for holographic displays.
Abstract
We present a 3D-aware one-shot head reenactment
method based on a fully volumetric neural disentanglement
framework for source appearance and driver expressions.
Our method is real-time and produces high-fidelity and
view-consistent output, suitable for 3D teleconferencing
systems based on holographic displays. Existing cutting-
edge 3D-aware reenactment methods often use neural ra-
diance fields or 3D meshes to produce view-consistent ap-
pearance encoding, but, at the same time, they rely on lin-
ear face models, such as 3DMM, to achieve its disentan-
glement with facial expressions. As a result, their reenact-
ment results often exhibit identity leakage from the driver
or have unnatural expressions. To address these problems,
we propose a neural self-supervised disentanglement ap-
proach that lifts both the source image and driver video
frame into a shared 3D volumetric representation based on
tri-planes. This representation can then be freely manipu-
lated with expression tri-planes extracted from the driving
images and rendered from an arbitrary view using neural
radiance fields. We achieve this disentanglement via self-supervised learning on a large in-the-wild video dataset.
We further introduce a highly effective fine-tuning approach
to improve the generalizability of the 3D lifting using the
same real-world data. We demonstrate state-of-the-art per-
formance on a wide range of datasets, and also showcase
high-quality 3D-aware head reenactment on highly chal-
lenging and diverse subjects, including non-frontal head
poses and complex expressions for both source and driver.
1. Introduction
Creating 3D head avatars from a single photo is a core ca-
pability in making a wide range of consumer AR/VR and
telepresence applications more accessible, and user expe-
riences more engaging. Graphics engine-based 3D avatar
digitization methods [9, 14,33,37,46,47,49,57] are suit-
able for today’s video games and virtual worlds, and many
commercial solutions exist (AvatarNeo [5], AvatarSDK [1],
ReadyPlayerMe [6], in3D [2], etc.). However, the photo-
realism achieved by modern neural head reenactment tech-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10336
niques is becoming increasingly appealing for advanced ef-
fects in video sharing apps and visual effects. For immer-
sive telepresence systems that use AR/VR headsets, facial
expression capture is typically achieved using tiny video
cameras built into HMDs [29, 50,55,64,69] , while the
identity of the source subject recorded using a separate pro-
cess. However, the teleconferencing solutions based on
holographic 3D displays (LookingGlass [4], LEIA [3], etc.)
use regular webcams [81] or depth sensors [48]. As opposed
to a video-based setting, head reenactment for immersive
applications needs to be 3D-aware, meaning that in addi-
tion to generating the correct poses and expressions from a
photo, multi-view consistency is critical.
While impressive facial reenactments results have been
demonstrated using 2D approaches [26, 27,83,97,102,
103], they typically struggle with preserving the likeness
of the source and exhibit significant identity changes when
varying the camera pose. More recently, 3D-aware one-shot
head reenactment methods [36, 43,53,54,59,99] have
used either 3D meshes or tri-plane neural radiance fields as
a fast and memory efficient volumetric data representations
for neural rendering. However, the expression and identity
disentanglement in these methods is based on variants of
linear face and expression models [15, 52] which lack ex-
pressiveness and high-frequency details. While these meth-
ods can achieve view consistency, facial expressions are of-
ten uncanny, and preserving the likeness of the input source
portrait is challenging, especially for views different than
the source image. Hence, input sources with extreme ex-
pressions and non-frontal poses are often avoided.
In this paper, we introduce the first 3D aware one-shot
head reenactment technique that disentangles source iden-
tities and the target expressions fully volumetrically, and
without the use of explicit linear face models. Our method
is real-time and designed with holographic displays in mind,
where a large number of views (up to 45) can be rendered
in parallel based on their viewing angle. We leverage the
fact that real-time 3D lifting for human heads has recently
been made possible [81] with the help of Vision Trans-
formers (ViT) [25], which avoids the need for inefficient
optimization-based GAN-inversion process [68]. In par-
ticular, 3D lifting allows us to map 2D face images into a
canonical tri-plane representation for both source and target
subjects and treat identity and expression disentanglement
independently from the head pose.
Once the source image and driver frame are lifted into
a pose-normalized tri-plane representation, we extract ap-
pearance features from the source subject and expressions
from the driver. The pose of the driver is estimated sepa-
rately using a 3D face tracker and used as input to a neural
renderer. Tri-plane-based feature extraction ensures view-
consistent rendering, while facial appearance and driver ex-
pression feature use frontalized views from the 3D liftingto enable robust and high-fidelity facial disentanglement.
To handle highly diverse portraits (variations in facial ap-
pearance, hairstyle, head covering, eyewear, etc.), we pro-
pose a new method for fine-tuning Lp3D on real datasets
by introducing a mixed loss function based on real and syn-
thetic datasets. Our volumetric disentanglement and render-
ing framework is trained only using in-the-wild videos from
the CelebV-HQ dataset [112] in a self-supervised fashion.
We not only demonstrate that our volumetric face dis-
entanglement approach produces qualitative superior head
reenactments than existing ones, but also show on a wide
and diverse set of source images how non-frontal poses
and extreme expressions can be handled. We have quan-
titatively assessed our method on multiple benchmarks and
outperform existing 2D and 3D state-of-the-art techniques
in terms of fidelity, expression, and likeness accuracy met-
rics. Our 3D aware head reenactment technique is there-
fore suitable for AR/VR-based immersive applications, and
we also showcase a teleconferencing system using a holo-
graphic display from LookingGlass [4]. We summarize the
main contributions as follows:
• First fully volumetric disentanglement approach for real-
time 3D aware head reenactment from a single photo.
This method combines 3D lifting into a canonical tri-
plane representation and formalized facial appearance
and expression feature extraction.
• A 3D lifting network that is fine-tuned on unconstrained
real-world data instead of only generating synthetic ones.
• We demonstrate superior fidelity, identity preservation,
and robustness w.r.t. current state-of-the-art methods for
facial reenactment on a wide range of public datasets. We
plan to release our code to the public.
2. Related Work
2D Neural Head Reenactment. The problem of generat-
ing animations of photorealistic human heads given images
or video inputs has been thoroughly explored using various
neural rendering techniques in the past few years, outper-
forming traditional 3DMM-based methods [8, 26,31,44,
63,66,78,79,96] which often appear uncanny due to their
compressed linear space. These approaches can be cate-
gorized into one-shot and multi-shot ones. While multi-
shot methods generally achieve high-fidelity results, they
are not suitable for many consumer applications as they typ-
ically require an extensive amount of training data, such as
a monocular video capture [10, 11,18,21,30,34,93,108–
110, 113], and sometimes even a calibrated multi-view
stereo setup [13, 29,55,58,69]. More recently, few-shot
techniques [104] have also been introduced.
To maximize accessibility, a considerable number of
methods [17, 26,27,32,35,39,66,72–74, 76,77,83,86,
87,89,97,101–103, 107] use a single portrait as input by
leveraging advanced generative modeling techniques based
10337
on in-the-wild video training data. While most methods rely
on linear face models to extract facial expressions, the head
reenactment technique from Drobyshev et al. [27] directly
extract expression features from cropped 2D face regions,
allowing them to obtain better face disentanglements, which
results in higher fidelity face synthesis. While similar to our
proposed approach in avoiding the use of low dimensional
linear face models, their method is purely 2D and strug-
gly with ensuring identity and expression consistency when
novel views are synthesized.
3D-Aware One-Shot Head Reenactment. Due to poten-
tial inconsistencies when rendering from different views or
poses, a number of 3D-aware single shot head reenactment
techniques [7, 19,20,24,62,65,70,75,88,92,94,95]
have been introduced. These methods generally use an ef-
ficient 3D representation, such as neural radiance fields or
3D mesh, to geometrically constraint the neural rendering
and improve view consistency. ROME [43] for instance is
a mesh-based method using FLAME blendshapes [51] and
neural textures. While view-consistent results can be pro-
duced for both face and hair regions, the use of low resolu-
tion polygonal meshes hinders the neural renderer to gener-
ate high-fidelity geometric and appearance details.
Implicit representations such as HeadNeRF [36] and
MofaNeRF [38] use a NeRF-based parametric model which
supports direct control of the head pose of the generated
images. While real-time rendering is possible, these meth-
ods require intensive test-time optimization and often fail to
preserve the identity of the source due to the use of com-
pact latent vectors. Most recent methods [53, 54,99] adopt
the highly efficient tri-plane-based neural fields representa-
tion [20] to encode the 3D structure and appearance of the
avatars head. Compared to the previous works on view-
consistent neural avatars [36, 43,53,54,59,99], we refrain
from depending on parametric head models for motion syn-
thesis and, instead, learn the volumetric motion model from
the training data. This methodology enables us to narrow
the identity gap between the source and generated images
and yield a superior fidelity of the generated motion com-
pared to competing approaches, and hence a higher quality
disentanglement for reenactment.
3D GAN Inversion. When training a whole reconstruc-
tion and disentangled reenactment model end-to-end on fa-
cial performance videos, one can introduce substantial over-
fitting and reduce the quality of the results. To address this
problems, we focus our training approach to an inversion of
pre-trained 3D-aware generative models for human heads.
We use tri-plane-based generative network EG3D [20]
as the foundational generator, due to its proficiency in pro-
ducing high-fidelity and view-consistent synthesis of hu-
man heads. For a given image, an effective 3D GAN inver-sion method should leverage these properties for estimat-
ing latent representations, which can be decoded into out-
puts that maintain view consistency and faithfully replicate
the contents of the input. One naive approach is to adapt
GAN inversion methods that were initially designed for 2D
GANs to the EG3D pre-trained network. These methods
either do a time consuming but more precise optimization
[42,68] or train a fast but less accurate encoder network
[67,80] to obtain the corresponding latent vectors. They
often produce incorrect depth prediction, leading to clear
artifacts in novel view synthesis. Hence, some methods
are specifically designed for inverting 3D GANs, which ei-
ther do multi-view optimization [45, 91] or predict residual
features/tri-plane maps for refining the initial inversion re-
sults [12, 81,98,100].
In this work, we rely on the state-of-the-art EG3D inver-
sion method Lp3D [81]. While achieving excellent novel-
view synthesis results, it lacks disentanglement between the
appearance and expression of the provided image and is
unable to impose various driving expressions onto the in-
put. To address this limitation, we propose a new method
that introduces appearance-expression disentanglement in
the latent space of tri-planes using our new self and cross-
reenactment training pipeline while relying on a pre-trained
but fine-tuned Lp3D network for regularization which en-
ables highly consistent view synthesis.
3. 3D-Aware Head Reenactment
As illustrated in Fig. 2, our head reenactment pipeline con-
sists of three stages: 1) 3D Lifting, 2) Volumetric Disen-
tanglement, and 3) Tri-plane Rendering. Given a pair of
source and driver images, we first frontalize them using a
pre-trained but fine-tuned tri-plane-based 3D lifting module
[81]. This driver alignment step is crucial and allows our
model to disentangle the expressions from the head pose,
which prevents overfitting. Then, the frontalized faces are
fed into two separate convolutional encoders to extract the
face features FsandFd. These extracted features are con-
catenated with the ones extracted from the tri-planes of the
source, and all are fed together into several transformer
blocks [90] to produce the expression tri-plane residual,
which is added to the tri-planes of the source image. The fi-
nal target image can be rendered from the new tri-planes us-
ing a pre-trained tri-plane renderer using the driver’s pose.
3.1. Fine-Tuned 3D Lifting
We adopt Lp3d [81] as a 3D face-lifting module, which pre-
dicts the radiance field of any given face image in real-time.
Instead of using an implicit multi-layer perceptron [61] or
sparse voxels [ 28,71] for the radiance field, Lp3D [81] uses
tri-planes [20], which can be computed using a single for-
ward of a deep learning network. Specifically, for a given
source image xs, we first extract the tri-planes Tusing a
10338
3D Lifting
Triplane
Renderer
ViT
 CSuper-
Resolution
Driver aug
3D Lifting
Triplane
Renderer
Pose
EstimatorPdriverReenactment
Volumetric Disentanglement Tri-plane RenderingSource
Driver
...
View 1 View N...Triplane
Renderer
PfrontalFigure 2. Given a pair of source and driver images, our method processes them in three steps: 3D Lifting into tri-plane representations,
Volumetric Disentanglement , which consists of source and driver frontalization and tri-plane residual generation, and Tri-plane Render-
ingvia volumetric ray marching with subsequent super-resolution.
transformer-based appearance encoder Eapp:
Eapp(xs) =T∈R3×H×W×C={Txy, Tyz, Tzx}. (1)
The color cand density σof each point p= (x, y, z )in
the radiance field can be obtained by projecting ponto the
three planes and by summing up the features at the projected
positions:
c, σ =D(Fxy+Fyz+Fzx), (2)
whereDis a shallow MLP decoder for the tri-plane ren-
dering, Fxy, Fyz, andFzxare the feature vectors at the pro-
jected positions on xy, yz , and zxplanes, respectively, cal-
culated using bilinear interpolation. The rendered 128×128
image is then upsampled using a super-resolution module to
produce a high-resolution output. To train the encoder Eapp,
Lp3D [81] uses synthetic data generated from a 3D-aware
face generative model [20]. While these synthetic data have
ground truth camera poses, they are limited to the face dis-
tribution of the generative model. As a result, Lp3D can
fail to generalize to in-the-wild images as shown in Fig. 5.
To prevent this, we fine-tune the pre-trained Lp3D on a
large-scale real-world dataset. We also replace the origi-
nal super-resolution module in Lp3D with a pre-trained GF-
PGAN [85], which is then fine-tuned together with Lp3D
(see Sec. 3.4).
3.2. Disentangling Appearance and Expression
Separating facial expression from the identity appearance
in a 3D radiance field is very challenging especially when
source and driver subjects have misaligned expressions. In
order to simplify the problem, we use our 3D lifting ap-
proach to bring both source and driver heads into a pose-
oriented space where faces are frontalized. Here, we denote
frontalized source and driver images as xf
sandxf
d, respec-
tively. These images are then fed into two separate convo-
lutional source and driver encoders EsandEdto producecoarse feature maps Fs=Es(xf
s)andFd=Ed(xf
d). Since
we already have the source’s tri-plane, which encodes the
3D shape of the source, we use another encoder to encode
this tri-plane and concatenate it together with the coarse
frontalized feature maps of the images to produce expres-
sion feature F:
Ft=Et(T)
F=Fs⊕Fd⊕Ft
Even though face frontalization aligns the source and the
driver, there is still some misalignment between the two
faces, e.g., the positions of the eyes may be different, or one
mouth is open while the other is closed. Therefore, we feed
the concatenation of the feature maps into several trans-
former blocks to produce the final residual tri-plane Ev(F).
This residual is then added back to the source’s tri-planes
to change the source’s expression to the driver’s expression
T′=T+Ev(F). Unlike LPR [54], we do not use a 3D
face model to compute the expression but instead use the
RGB images of the source and the driver directly, allowing
the model to learn high-fidelity and realistic expressions.
3.3. Tri-Plane Rendering
The resulting tri-planes are then volumetrically rendered
into one or multiple output images using pose parame-
ters and viewing angles in the case of a holographic dis-
play. Following EG3D [20], we use a neural radiance fields
(NeRFs)-based volumetric ray marching approach [60].
However, instead of encoding each point in space via po-
sitional encodings [60], the features of the points along rays
are calculated using their projections onto tri-planes. Since
tri-planes are aligned with the frontal face, we can compute
these rays directly using camera extrinsics P driver predicted
by an off-the-shelf 3D head pose estimator [23].
While the renderings are highly view-consistent, the
large number of points evaluated for each ray still limits
10339
the ouput resolution for real-time performance. We there-
fore follow [54] and employ a 2D upsampling network [84]
based on StyleGAN2 [41], which in our experiments pro-
duced higher quality results than the upsampling approach
in EG3D [20]. Finally, for holographic displays, we gen-
erate a number of renderings based on their viewing an-
gles and simply using the head pose parameter. Real-time
performance is achieved using efficient inference libraries
such as TensorRT, half-precision, and batched inference
over multiple GPUs.
3.4. Training Strategy
Fine-Tuning Lp3D. To make Lp3D work with in-the-
wild images, we fine-tune it on a large-scale real-world
video dataset [111]. Unlike the use of synthetic data, real-
world data do not have ground-truth camera parameters and
facial expressions in monocular videos are typically incon-
sistent over time. While the camera parameters can be es-
timated using standard 3D pose estimators, the expression
diferences are difficult to determine. However, we found
that we can ignore this expression difference and fine-tune
Lp3D using real data together with continuous training on
synthetic data. In particular, our experiments indicate that
the fine-tuned model can still faithfully reconstruct 3D faces
from the input without changing expressions and still gen-
eralize successfully on in-the-wild images. Specifically, on
real video data, we sample two frames xr
sandxr
dand es-
timate their camera paramters Pr
sandPr
d. Similar to [20],
we assume a fixed intrinsics for standard portraits for all
images. Then we use Eappfrom Lp3D to calculate the tri-
planes of xr
s, render it using the two poses, and calculate
reconstruction losses on the two rendered images:
Lreal=∥Lp3D( xr
s, Pr
d)−xr
d∥+∥Lp3D( xr
s, xr
s)−xr
s∥,
where Lp3D( x, P )is the face in xre-rendered using camera
posePandLrealis the loss for real images. Simultaneously,
we render two synthetic images employing an identical la-
tent code but through varying camera views and calculate
the synthetic loss Lsyn:
Lsyn=∥Lp3D( xf
s, Ps
d)−xs
d∥+∥Lp3D( xs
s, Ps
s)−xs
s∥
Ltri=∥Eapp(xf
s)−T∥,
where Tis the ground-truth tri-planes returned by EG3D
[20] and Ltriis the tri-plane loss adopted directly from
Lp3D. The final loss Lappfor fine-tuning Lp3D can be for-
mulated as:
Lapp=Lreal+λsynLsyn+λtriLtri
where λsynandλtriare tunable hyperparameters.
Disentangling Appearance and Expressions. In this
stage, we also use real-world videos as training data. For apair of source and driver images xsandxdsampled from the
same video, we apply the reconstruction loss Lreconwhich is
a combination of L1, perceptual [105], and identity losses,
between the reenacted image xs→dand the corresponding
ground-truth xd:
Lrecon =∥xs→d−xd∥1+ϕ(xs→d, xd)
+Lcos(ID(xs→d),ID(xd)),
where ϕis the perceptual loss, ID(· ) is a pretrained face
recognizer [82], and Lcosis the cosine similarity loss:
Lcos((u, v) =< u, v >
max (∥u∥, ϵ )×max (∥v∥, ϵ)
Here ϵis a small constant to void division by 0. Simi-
lar to other works that use RGB images directly to cal-
culate expressions [27], our proposed encoder also suffers
from an “identity leaking” issue. Since there is no cross-
reenactment dataset, the expression module is trained with
self-reenactment video data. Therefore, without proper aug-
mentation and regularization, the expression module can
leak identity information from the driver to the output,
making the model fail to generalize to cross-reenactment
tasks. Hence, we introduce a Cross Identity Regularization.
Specifically, we further sample an additional driver frame
xd′from another video. We incorporate a GAN loss where
real samples are Lp3D( xs, Pd)and fake samples are xs→d′.
This GAN loss is also conditioned on the identity vector of
the source ID( xs). Following [27], we also apply strong
augmentation (random warping and color jittering) and ad-
ditionally mask the border of the driver randomly to fur-
ther reduce potential identity leaks. The loss for expression
training can be summarized as:
Lexp=Lrecon +λCIRLCIR,
where LCIRandλCIRare cross identity regularization and
its hyperparameter, respectively.
Global Fine-Tuning. After training both Lp3D and the
expression module, we iteratively fine-tune the two modules
using the same losses as the previous sections. Specifically,
for every 10000 iterations, we freeze one module and fine-
tune the other and vice versa. In addition, we add a GAN
loss on the super-resolution output of the Lp3D module.
4. Experiments
Implementation Details. We train our model on CelebV-
HQ dataset [112] using 7 NVIDIA RTX A6000 ADA (50Gb
memory each). We use AdamW [56] to optimize the param-
eters with a learning rate of 10−4and batch size of 28. The
Lp3D finetuning takes 5 days for 500K iterations to con-
verge. Training the expression module takes 2 days, and the
10340
Source Driver Ref View Novel View Wrinkles
 GeometryFigure 3. Expression dependent high-fidelity details, incl. eye and forehead wrinkles, as well as nasolabial folds (see zoom-ins)
Source Driver HeadNeRF StyleHEAT MegaPortraits ROME Ours
Figure 4. A qualitative comparison with the baselines on in-the-wild photos. Notice that our method is capable of producing a variety of
facial expressions, and handle highly diverse subjects, with and without accessories, as well as extreme head poses, such as rows 3 and 4.
10341
MethodSelf-reenactment Cross-reenactment
PSNR ↑SSIM↑LPIPS ↓N
AKD↓ECMD ↓FID↓CSIM ↑ECMD ↓FID↓
R
OME [43] 18.46
0.488 0.351 0.030 0.594 138 0.507 0.740 172
StyleHeat
[97] 19.73
0.689 0.278 0.035 0.748 89.8 0.398
0.744 95.5
OTAvatar [59] 19.28
0.749 0.289 0.035 0.651 67.0 0.462
0.901 72.4
MegaPortraits [27] 21.10
0.731 0.291 0.022 0.755 52.0 0.729
0.771 61.7
Ours 22.83
0.768 0.168 0.012 0.426 40.5 0.754 0.754 36.4
T
able 1. Evaluation on HDTF [106] dataset. Our method outperforms the competitors across
almost all of the metrics for both self- and cross-reenactment scenarios.MethodCross-reenactment
CSIM ↑ECMD ↓FID↓
R
OME [43] 0.519
0.91 52.6
HeadNeRF [36] 0.346
0.88 113
StyleHeat [97] 0.467
0.85 50.2
MegaPortraits [27] 0.647
0.77 29.2
Ours 0.608
0.79 23.6
T
able 2. Evaluation on CelebA-HQ [40]
dataset.
Input image w/o finetuning w/ finetuning
Figure 5. Our implementation
of Lp3D [81] before and after
CelebV-HQ [112] fine-tuning.CSIM ↑ECMD ↓
Lp3D 0.548
0.82
Lp3D-FT 0.670
0.76
w/o frontal 0.668
1.01
w/o CIR 0.570
0.97
Ours 0.608
0.79
T
able 3. Ablation studies
conducted on CelebA-HQ [40]
dataset. FT is a fine-tuned
version of Lp3D, and “frontal”
denotes frontalization of the
source and driver.
Source Driver W/o Frontalization  W/o CIR Ours
Figure 6. Ablation study for source and driver frontalization and
cross identity regularization (CIR).
Source Expr driver LPR Ours
Figure 7. Qualitative comparison with LPR [54] method on the
samples from HDTF [106] dataset.
iterative fine-tuning takes another 5 days. More training de-
tails, such as hyperparameter fine-tuning or architecture of
the networks, can be found in the supplementary materials.
Unlike Lp3D, our method reenacts faces without re-
lifting in 3D for every frame. The tri-plane encoding and
frontalization of the source image are only performed once.
For each driver, we perform a single frontalization for the
driver image (0.0115 ms), one expression encoding (0.0034
s), one tri-plane rendering at 128×128resolution (0.0071
s), and one neural upsampling (0.0099 s). Each view runs at
31.9 fps on an Nvidia RTX 4090 GPU including I/O. More
details are provided in the supplemental materials.We compare our method with state-of-the-art 3D-
based [36, 43,59] and 2D-based [27, 97] models. For
MegaPortraits [27], we use our own implementation that
was trained on the CelebV-HQ dataset. Similar to previous
works, we evaluate our method using public benchmarks,
including CelebA-HQ [40] and HDTF [106]. For CelebA-
HQ, we split the data into two equal sets. Each set contains
around 15K images. Then, we use one set as the source and
the rest as driver images. For the HDTF dataset, we perform
cross-reenactment by using the first frame of each video as
source and 200 first frames of other videos as drivers, which
is more than 60K data pairs. Similarly, to evaluate self-
reenactment, we also use the first frames of each video as
sources and the rest of the same video as the driver. Further-
more, we also collected 100 face images on the internet and
around 100 high-quality videos for qualitative comparisons.
Video results are provided in the supplementary materials.
Evaluation. Given a source image xs, a driver image xd,
and reenacted output xs→dwe use EMOCAv2 [22] to ex-
tract the FLAME [52] expression coefficients of the predic-
tion and the driver, as well as the shape coefficients of the
source. We then compute 2 FLAME meshes using the pre-
dicted shape coefficients in world coordinates, one with the
expression coefficients of the driver and one with the ex-
pression coefficients of the reenacted output. We measure
the distance between the 2 meshes and denote this expres-
sion metric as ECMD. Moreover, we also use cosine sim-
ilarity between the embeddings of a face recognition net-
work (CSIM) [101], normalized average keypoint distance
(NAKD) [16], perceptual image similarity (LPIPS) [105],
peak signal-to-noise ratio (PSNR), and structure similarity
index measure (SSIM).
We provide quantitative comparisons on HDTF and
CelebA-HQ datasets in Tab. 1and Tab. 2, respectively,
and show that our method outperforms existing methods
on both datasets. We also note that our FID and CSIM
scores are significantly more reliable than the others, while
expression-based metrics such as NAKD and ECMD are ei-
ther better or very close to the best baseline, w.r.t output
quality, expression accuracy, and identity consistency.
We showcase the qualitative results of cross-identity
reenactment on in-the-wild images in Fig. 4and Fig. 3.
Compared to the baselines [27, 36,43,97], our reenact-
ment faithfully reconstructs intricate and complex elements,
10342
Source Driver Output
 Source Driver OutputFigure 8. Failure cases of our method include side views in
the source, extreme expressions, modeling of cartoonish charac-
ters and paintings, as well as modeling the reflections and semi-
transparency of the eyewear.
such as hairstyle, facial hair, glasses, and facial makeups.
Furthermore, our method effectively generates realistic and
fine-scale dynamic details that mach the driver’s expres-
sions including substantial head pose rotations. We also
conduct a comparative analysis of our results with the cur-
rent state-of-the-art 3D-aware method LPR [54] in Fig. 7.
Compared to LPR, our method achieves superior identity
consistency. We further refer to the supplemental video for
a live demonstration of our holographic telepresence system
and animated head reenactment results and comparisons,
with and without disentangled poses.
Ablation Study. We compare Lp3D with and without
fine-tuning on the CelebA-HQ dataset in Tab. 3and show
several examples in Fig. 5. Without fine-tuning on real data,
our implementation of Lp3D fails to preserve the identity
of the input image, resulting in a considerably lower CSIM
score. We also try without any facial frontalization in the
expression module and instead use the source and driver
images directly to calculate the expression tri-plane resid-
ual. We observe in Fig. 6that without face frontalization,
the model completely ignores the expression of the driver
and keeps the expression of the input source instead. We
show in Tab. 3, that facial frontalization leads to much bet-
ter ECMD score. We then measure the effectiveness of the
GAN-based cross-identity regularization on the CelebA-
HQ dataset, LCIR. Without this loss, identity characteristics
(hairstyle or color) can leak from the driver to the output.
See column 4 in Fig. 6. Tab. 3also shows that cross-identity
regularization can reduce identity leaking and improve the
CSIM score. Lastly, we have also attempted to train our
model end-to-end using the same losses and optimization
process instead of our proposed iterative fine-tuning. Even
with a lower learning rate and the use of pre-trained Lp3D
weights, we were unable to succeed.
Limitations. Limitations of our approach are illustrated
in Figure 8. For source images that are extremely side ways
(i.e., over 90◦), our method can produce a plausible frontal
face, but the likeness cannot be guaranteed due to insuf-
ficient visibility. For very highly stylized portraits, such
as cartoons, our framework often produces photorealistic
facial elements such as teeth which can be inconsistent instyle. Due to the dependence on training data volume and
diversity, accessories such as dental braces or glasses may
disappear or look different during synthesis. We believe that
providing more and better training data can further improve
the performance of our algorithm.
5. Discussion
We have demonstrated that a fully volumetric disentan-
glement of facial appearance and expressions is possible
through a shared canonical tri-plane representation. In par-
ticular, an improved disentanglement also leads to higher
fidelity and more robust head reenactment, when compared
to existing methods that use linear face models for expres-
sions, especially for non-frontal poses. A critical insight
of our approach is that head frontalization via 3D lifting
is particularly effective for extracting features that can en-
code fine details and expressions such as wrinkles and folds.
The resulting reenactment is also highly view-consistent for
large angles, making our solution suitable for holographic
displays. We have also shown that the 3D lifting model can
still be successfully trained with real data despite the fact
that different frames with the same subject have varying fa-
cial expressions. Without a fine-tuned 3D lifting model, our
3D-aware reenactment framework would struggle with pre-
serving the identity of the source, especially for side views.
Our experiments indicate that our results achieve better vi-
sual quality and are more robust to extreme poses, validated
via an extensive evaluation on multiple datasets.
Risks and Potential Misuse. The proposed method is in-
tended to promote avatar-based 3D communication. Never-
theless, our AI-based reenactment solution produces syn-
thetic but highly realistic face videos from only a single
photo, which could be hard to distinguish from a real per-
son. Like deepfakes and other facial manipulation methods,
potential misuse is possible and hence, we refer to the sup-
plemental material for more discussions.
Future Work. We are also interested in expanding our
work to upper and full body reenactment, where hand ges-
tures can be used for more engaging communication. To
this end, we plan to investigate the use of canonical repre-
sentations for human bodies, such as T-poses. As our pri-
mary motivation, we have showcased a solution using holo-
graphic displays for immersive 3D teleconferencing. How-
ever, we believe that our approach can also be extended to
AR/VR HMD-based settings where full 360° head views
are possible. The recent work by An et al. [7] is a promis-
ing avenue for future exploration.
Acknowledgement. This work is supported by the Meta-
verse Center Grant from the MBZUAI Research Office and
by the Swiss SERI Consolidation Grant ”AI-PERCEIVE”.
10343
References
[1] ItSeez3D AvatarSDK, https://avatarsdk.com. 1
[2] in3D, https://in3d.io. 1
[3] Leia, https://www.leiainc.com. 2
[4] Looking Glass Factory, https://lookingglassfactory.com. 2
[5] Pinscreen Avatar Neo, https://www.avatarneo.com. 1
[6] ReadyPlayerMe, https://readyplayer.me. 1
[7] Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Y.
Ogras, and Linjie Luo. Panohead: Geometry-aware 3d full-
head synthesis in 360deg. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), 2023. 3,8
[8] ShahRukh Athar, Zexiang Xu, Kalyan Sunkavalli, Eli
Shechtman, and Zhixin Shu. Rignerf: Fully controllable
neural 3d portraits. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 20364–20373, 2022. 2
[9] Haoran Bai, Di Kang, Haoxian Zhang, Jinshan Pan, and
Linchao Bao. Ffhq-uv: Normalized facial uv-texture
dataset for 3d face reconstruction. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 362–371, 2023. 1
[10] Ziqian Bai, Feitong Tan, Zeng Huang, Kripasindhu Sarkar,
Danhang Tang, Di Qiu, Abhimitra Meka, Ruofei Du, Ming-
song Dou, Sergio Orts-Escolano, Rohit Pandey, Ping Tan,
Thabo Beeler, Sean Fanello, and Yinda Zhang. Learn-
ing personalized high quality volumetric head avatars from
monocular rgb videos. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), 2023. 2
[11] Shrisha Bharadwaj, Yufeng Zheng, Otmar Hilliges,
Michael J. Black, and Victoria Fernandez Abrevaya.
FLARE: Fast learning of animatable and relightable mesh
avatars. ACM Transactions on Graphics, 42:15, 2023. 2
[12] Ananta R Bhattarai, Matthias Nießner, and Artem Sev-
astopolsky. Triplanenet: An encoder for eg3d inversion.
arXiv preprint arXiv:2303.13497, 2023. 3
[13] Sai Bi, Stephen Lombardi, Shunsuke Saito, Tomas Simon,
Shih-En Wei, Kevyn McPhail, Ravi Ramamoorthi, Yaser
Sheikh, and Jason M. Saragih. Deep relightable appear-
ance models for animatable faces. ACM Transactions on
Graphics (TOG), 40, 2021. 2
[14] Volker Blanz and Thomas Vetter. A morphable model for
the synthesis of 3d faces. In Proceedings of the 26th Annual
Conference on Computer Graphics and Interactive Tech-
niques, page 187–194, USA, 1999. ACM Press/Addison-
Wesley Publishing Co. 1
[15] Volker Blanz and Thomas Vetter. A Morphable Model For
The Synthesis Of 3D Faces. Association for Computing
Machinery, New York, NY, USA, 1 edition, 2023. 2
[16] Adrian Bulat and Georgios Tzimiropoulos. How far are we
from solving the 2d & 3d face alignment problem?(and a
dataset of 230,000 3d facial landmarks). In Proceedings
of the IEEE international conference on computer vision,
pages 1021–1030, 2017. 7
[17] Egor Burkov, Igor Pasechnik, Artur Grigorev, and Vic-
tor Lempitsky. Neural head reenactment with latent pose
descriptors. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pages 13786–
13795, 2020. 2[18] Chen Cao, Tomas Simon, Jin Kyu Kim, Gabe Schwartz,
Michael Zollhoefer, Shun-Suke Saito, Stephen Lombardi,
Shih-En Wei, Danielle Belko, Shoou-I Yu, Yaser Sheikh,
and Jason Saragih. Authentic volumetric avatars from a
phone scan. ACM Trans. Graph., 41, 2022. 2
[19] Eric R. Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,
and Gordon Wetzstein. Pi-gan: Periodic implicit generative
adversarial networks for 3d-aware image synthesis. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), pages 5799–5809, 2021.
3
[20] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki
Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,
Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis,
et al. Efficient geometry-aware 3d generative adversarial
networks. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 16123–
16133, 2022. 3,4,5
[21] Chuhan Chen, Matthew O’Toole, Gaurav Bharaj, and Pablo
Garrido. Implicit neural head synthesis via controllable lo-
cal deformation fields. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 416–426, 2023. 2
[22] Radek Dan ˇeˇcek, Michael J Black, and Timo Bolkart.
Emoca: Emotion driven monocular face capture and ani-
mation. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 20311–
20322, 2022. 7
[23] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde
Jia, and Xin Tong. Accurate 3d face reconstruction with
weakly-supervised learning: From single image to image
set. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition workshops, pages 0–0,
2019. 4
[24] Yu Deng, Jiaolong Yang, Jianfeng Xiang, and Xin Tong.
Gram: Generative radiance manifolds for 3d-aware image
generation. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pages
10673–10683, 2022. 3
[25] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold,
Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An im-
age is worth 16x16 words: Transformers for image recog-
nition at scale. ArXiv, abs/2010.11929, 2020. 2
[26] Michail Christos Doukas, Stefanos Zafeiriou, and Viktoriia
Sharmanska. Headgan: One-shot neural head synthesis and
editing. In IEEE/CVF International Conference on Com-
puter Vision (ICCV), 2021. 2
[27] Nikita Drobyshev, Jenya Chelishev, Taras Khakhulin, Alek-
sei Ivakhnenko, Victor Lempitsky, and Egor Zakharov.
Megaportraits: One-shot megapixel neural head avatars.
arXiv preprint arXiv:2207.07621, 2022. 2,3,5,7
[28] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong
Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:
Radiance fields without neural networks. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 5501–5510, 2022. 3
[29] Yonggan Fu, Yuecheng Li, Chenghui Li, Jason Saragih,
Peizhao Zhang, Xiaoliang Dai, and Yingyan (Celine) Lin.
Auto-card: Efficient and robust codec avatar driving for
real-time mobile telepresence. In Proceedings of the
10344
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 21036–21045, 2023. 2
[30] Guy Gafni, Justus Thies, Michael Zollhofer, and Matthias
Nießner. Dynamic neural radiance fields for monocu-
lar 4d facial avatar reconstruction. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 8649–8658, 2021. 2
[31] Guy Gafni, Justus Thies, Michael Zollh ¨ofer, and Matthias
Nießner. Dynamic neural radiance fields for monocular 4d
facial avatar reconstruction. In Proc. Computer Vision and
Pattern Recognition (CVPR), IEEE, 2021. 2
[32] Yue Gao, Yuan Zhou, Jinglu Wang, Xiao Li, Xiang Ming,
and Yan Lu. High-fidelity and freely controllable talking
head video generation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 5609–5619, 2023. 2
[33] Baris Gecer, Stylianos Ploumpis, Irene Kotsia, and Ste-
fanos P Zafeiriou. Fast-ganfit: Generative adversarial
network for high fidelity 3d face reconstruction. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
2021. 1
[34] Philip-William Grassal, Malte Prinzler, Titus Leistner,
Carsten Rother, Matthias Nießner, and Justus Thies. Neural
head avatars from monocular rgb videos. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 18653–18664, 2022. 2
[35] Fa-Ting Hong, Longhao Zhang, Li Shen, and Dan Xu.
Depth-aware generative adversarial network for talking
head video generation. 2022. 2
[36] Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, and Juyong
Zhang. Headnerf: A real-time nerf-based parametric head
model. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 20374–
20384, 2022. 2,3,7
[37] Liwen Hu, Shunsuke Saito, Lingyu Wei, Koki Nagano, Jae-
woo Seo, Jens Fursund, Iman Sadeghi, Carrie Sun, Yen-
Chun Chen, and Hao Li. Avatar digitization from a single
image for real-time rendering. ACM Trans. Graph., 36(6),
2017. 1
[38] Yiyu huang, Hao Zhu, Xusen Sun, and Xun Cao. Mofanerf:
Morphable facial neural radiance field. In ECCV, 2022. 3
[39] Xinya Ji, Hang Zhou, Kaisiyuan Wang, Qianyi Wu, Wayne
Wu, Feng Xu, and Xun Cao. Eamm: One-shot emotional
talking face via audio-based emotion-aware motion model.
InACM SIGGRAPH 2022 Conference Proceedings, 2022.
2
[40] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehti-
nen. Progressive growing of gans for improved quality,
stability, and variation. arXiv preprint arXiv:1710.10196 ,
2017. 7
[41] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), 2019. 5
[42] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improv-
ing the image quality of StyleGAN. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2020. 3
[43] Taras Khakhulin, Vanessa Sklyarova, Victor Lempitsky,
and Egor Zakharov. Realistic one-shot mesh-based head
avatars. In European Conference of Computer vision
(ECCV), 2022. 2,3,7[44] H. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Nießner,
P. P´eerez, C. Richardt, M. Zollh ¨ofer, and C. Theobalt.
Deep video portraits. ACM Transactions on Graphics 2018
(TOG), 2018. 2
[45] Jaehoon Ko, Kyusun Cho, Daewon Choi, Kwangrok Ryoo,
and Seungryong Kim. 3d gan inversion with pose opti-
mization. In Proceedings of the IEEE/CVF Winter Con-
ference on Applications of Computer Vision, pages 2967–
2976, 2023. 3
[46] Alexandros Lattas, Stylianos Moschoglou, Stylianos
Ploumpis, Baris Gecer, Abhijeet Ghosh, and Stefanos P
Zafeiriou. Avatarme++: Facial shape and brdf inference
with photorealistic rendering-aware gans. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 2021.
1
[47] Alexandros Lattas, Stylianos Moschoglou, Stylianos
Ploumpis, Baris Gecer, Jiankang Deng, and Stefanos
Zafeiriou. Fitme: Deep photorealistic 3d morphable model
avatars. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pages
8629–8640, 2023. 1
[48] Jason Lawrence, Dan B Goldman, Supreeth Achar, Gre-
gory Major Blascovich, Joseph G. Desloge, Tommy Fortes,
Eric M. Gomez, Sascha H ¨aberling, Hugues Hoppe, Andy
Huibers, Claude Knaus, Brian Kuschak, Ricardo Martin-
Brualla, Harris Nover, Andrew Ian Russell, Steven M.
Seitz, and Kevin Tong. Project starline: A high-fidelity
telepresence system. ACM Transactions on Graphics (Proc.
SIGGRAPH Asia), 40(6), 2021. 2
[49] Biwen Lei, Jianqiang Ren, Mengyang Feng, Miaomiao Cui,
and Xuansong Xie. A hierarchical representation network
for accurate and detailed face reconstruction from in-the-
wild images. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 394–403, 2023. 1
[50] Hao Li, Laura Trutoiu, Kyle Olszewski, Lingyu Wei, Tris-
tan Trutna, Pei-Lun Hsieh, Aaron Nicholls, and Chongyang
Ma. Facial performance sensing head-mounted display.
ACM Transactions on Graphics (Proceedings SIGGRAPH
2015), 34(4), 2015. 2
[51] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and
Javier Romero. Learning a model of facial shape and ex-
pression from 4D scans. ACM Transactions on Graphics,
(Proc. SIGGRAPH Asia), 36(6):194:1–194:17, 2017. 3
[52] Tianye Li, Timo Bolkart, Michael J Black, Hao Li, and
Javier Romero. Learning a model of facial shape and ex-
pression from 4d scans. ACM Trans. Graph., 36(6):194–1,
2017. 2,7
[53] Weichuang Li, Longhao Zhang, Dong Wang, Bin Zhao,
Zhigang Wang, Mulin Chen, Bang Zhang, Zhongjian
Wang, Liefeng Bo, and Xuelong Li. One-shot high-
fidelity talking-head synthesis with deformable neural radi-
ance field. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pages
17969–17978, 2023. 2,3
[54] Xueting Li, Shalini De Mello, Sifei Liu, Koki Nagano,
Umar Iqbal, and Jan Kautz. Generalizable one-shot neu-
ral head avatar. arXiv preprint arXiv:2306.08768, 2023. 2,
3,4,5,7,8
[55] Stephen Lombardi, Jason Saragih, Tomas Simon, and Yaser
Sheikh. Deep appearance models for face rendering. ACM
Trans. Graph., 37(4), 2018. 2
10345
[56] Ilya Loshchilov and Frank Hutter. Decoupled weight de-
cay regularization. In International Conference on Learn-
ing Representations, 2017. 5
[57] Huiwen Luo, Koki Nagano, Han-Wei Kung, Mclean Gold-
white, Qingguo Xu, Zejian Wang, Lingyu Wei, Liwen Hu,
and Hao Li. Normalized avatar synthesis using stylegan and
perceptual refinement. CoRR, abs/2106.11423, 2021. 1
[58] Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang,
Yuecheng Li, Fernando De la Torre, and Yaser Sheikh.
Pixel codec avatars. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 64–73, 2021. 2
[59] Zhiyuan Ma, Xiangyu Zhu, Guo-Jun Qi, Zhen Lei, and Lei
Zhang. Otavatar: One-shot talking face avatar with control-
lable tri-plane rendering. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 16901–16910, 2023. 2,3,7
[60] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis, 2020. 4
[61] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. Communications of the ACM, 65(1):99–106, 2021.
3
[62] Michael Niemeyer and Andreas Geiger. Giraffe: Repre-
senting scenes as compositional generative neural feature
fields. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pages
11453–11464, 2021. 3
[63] Yuval Nirkin, Yosi Keller, and Tal Hassner. FSGAN: Sub-
ject agnostic face swapping and reenactment. In Proceed-
ings of the IEEE International Conference on Computer Vi-
sion, pages 7184–7193, 2019. 2
[64] Kyle Olszewski, Joseph J. Lim, Shunsuke Saito, and Hao
Li. High-fidelity facial and speech animation for vr hmds.
ACM Transactions on Graphics (TOG), 35:1 – 14, 2016. 2
[65] Roy Or-El, Xuan Luo, Mengyi Shan, Eli Shecht-
man, Jeong Joon Park, and Ira Kemelmacher-Shlizerman.
Stylesdf: High-resolution 3d-consistent image and geome-
try generation. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 13503–13513, 2022. 3
[66] Yurui Ren, Ge Li, Yuanqi Chen, Thomas H. Li, and Shan
Liu. Pirenderer: Controllable portrait image generation
via semantic neural rendering. In Proceedings of the
IEEE/CVF International Conference on Computer Vision
(ICCV), pages 13759–13768, 2021. 2
[67] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam
Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or.
Encoding in style: a stylegan encoder for image-to-image
translation. In IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), 2021. 3
[68] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel
Cohen-Or. Pivotal tuning for latent-based editing of real
images. ACM Trans. Graph., 2021. 2,3
[69] Gabriel Schwartz, Shih-En Wei, Te-Li Wang, Stephen
Lombardi, Tomas Simon, Jason Saragih, and Yaser Sheikh.
The eyes have it: An integrated eye and face model for
photorealistic facial animation. ACM Trans. Graph., 39(4),
2020. 2[70] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and An-
dreas Geiger. Graf: Generative radiance fields for 3d-
aware image synthesis. In Proceedings of the 34th Interna-
tional Conference on Neural Information Processing Sys-
tems, 2020. 3
[71] Katja Schwarz, Axel Sauer, Michael Niemeyer, Yiyi Liao,
and Andreas Geiger. Voxgraf: Fast 3d-aware image synthe-
sis with sparse voxel grids. Advances in Neural Information
Processing Systems, 35:33999–34011, 2022. 3
[72] Aliaksandr Siarohin, St ´ephane Lathuili `ere, Sergey
Tulyakov, Elisa Ricci, and Nicu Sebe. Animating arbitrary
objects via deep motion transfer. In CVPR, 2019. 2
[73] Aliaksandr Siarohin, St ´ephane Lathuili `ere, Sergey
Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion
model for image animation. In Conference on Neural
Information Processing Systems (NeurIPS), 2019.
[74] Aliaksandr Siarohin, Oliver Woodford, Jian Ren, Menglei
Chai, and Sergey Tulyakov. Motion representations for ar-
ticulated animation. In CVPR, 2021. 2
[75] Ivan Skorokhodov, Sergey Tulyakov, Yiqun Wang, and Pe-
ter Wonka. EpiGRAF: Rethinking training of 3d GANs. In
Advances in Neural Information Processing Systems, 2022.
3
[76] Linsen Song, Wayne Wu, Chaoyou Fu, Chen Qian,
Chen Change Loy, and Ran He. Pareidolia face reenact-
ment. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2021. 2
[77] Jiale Tao, Biao Wang, Borun Xu, Tiezheng Ge, Yuning
Jiang, Wen Li, and Lixin Duan. Structure-aware motion
transfer with deformable anchor model. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 3637–3646, 2022. 2
[78] J. Thies, M. Zollh ¨ofer, M. Stamminger, C. Theobalt, and
M. Nießner. Face2face: Real-time face capture and reenact-
ment of rgb videos. In Proc. Computer Vision and Pattern
Recognition (CVPR), IEEE, 2016. 2
[79] J. Thies, M. Zollh ¨ofer, M. Stamminger, C. Theobalt, and M.
Nießner. Headon: Real-time reenactment of human portrait
videos. ACM Transactions on Graphics 2018 (TOG), 2018.
2
[80] Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and
Daniel Cohen-Or. Designing an encoder for stylegan image
manipulation. ACM Transactions on Graphics (TOG), 40
(4):1–14, 2021. 3
[81] Alex Trevithick, Matthew Chan, Michael Stengel, Eric
Chan, Chao Liu, Zhiding Yu, Sameh Khamis, Manmohan
Chandraker, Ravi Ramamoorthi, and Koki Nagano. Real-
time radiance fields for single-image portrait view synthe-
sis. ACM Transactions on Graphics (TOG) , 42(4):1–15,
2023. 2,3,4,7
[82] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong
Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cosface:
Large margin cosine loss for deep face recognition, 2018. 5
[83] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-
shot free-view neural talking-head synthesis for video con-
ferencing. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pages 10039–
10049, 2021. 2
[84] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. To-
wards real-world blind face restoration with generative fa-
cial prior. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2021. 5
10346
[85] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. To-
wards real-world blind face restoration with generative fa-
cial prior. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, pages 9168–9178,
2021. 4
[86] Yaohui Wang, Di Yang, Francois Bremond, and Antitza
Dantcheva. Latent image animator: Learning to animate
images via latent space navigation. In International Con-
ference on Learning Representations, 2022. 2
[87] O. Wiles, A.S. Koepke, and A. Zisserman. X2face: A net-
work for controlling face generation by using images, au-
dio, and pose codes. In European Conference on Computer
Vision, 2018. 2
[88] Jianfeng Xiang, Jiaolong Yang, Yu Deng, and Xin Tong.
Gram-hd: 3d-consistent image generation at high resolution
with generative radiance manifolds. In Proceedings of the
IEEE/CVF International Conference on Computer Vision
(ICCV), pages 2195–2205, 2023. 3
[89] Sitao Xiang, Yuming Gu, Pengda Xiang, Mingming He,
Koki Nagano, Haiwei Chen, and Hao Li. One-shot
identity-preserving portrait reenactment. arXiv preprint
arXiv:2004.12452, 2020. 2
[90] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,
Jose M Alvarez, and Ping Luo. Segformer: Simple and
efficient design for semantic segmentation with transform-
ers. Advances in Neural Information Processing Systems,
34:12077–12090, 2021. 3
[91] Jiaxin Xie, Hao Ouyang, Jingtan Piao, Chenyang Lei, and
Qifeng Chen. High-fidelity 3d gan inversion by pseudo-
multi-view optimization. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 321–331, 2023. 3
[92] Hongyi Xu, Guoxian Song, Zihang Jiang, Jianfeng Zhang,
Yichun Shi, Jing Liu, Wanchun Ma, Jiashi Feng, and Linjie
Luo. Omniavatar: Geometry-guided controllable 3d head
synthesis. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pages
12814–12824, 2023. 3
[93] Yuelang Xu, Hongwen Zhang, Lizhen Wang, Xiaochen
Zhao, Han Huang, Guojun Qi, and Yebin Liu. Latentavatar:
Learning latent expression code for expressive neural head
avatar. In ACM SIGGRAPH 2023 Conference Proceedings.
Association for Computing Machinery, 2023. 2
[94] Zhongcong Xu, Jianfeng Zhang, Junhao Liew, Wenqing
Zhang, Song Bai, Jiashi Feng, and Mike Zheng Shou. Pv3d:
A 3d generative model for portrait video generation. In
The Tenth International Conference on Learning Represen-
tations, 2023. 3
[95] Yang Xue, Yuheng Li, Krishna Kumar Singh, and Yong Jae
Lee. Giraffe hd: A high-resolution 3d-aware generative
model. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pages
18440–18449, 2022. 3
[96] Kewei Yang, Kang Chen, Daoliang Guo, Song-Hai Zhang,
Yuan-Chen Guo, and Weidong Zhang. Face2face ρ: Real-
time high-resolution one-shot face reenactment. 2022. 2
[97] Fei Yin, Yong Zhang, Xiaodong Cun, Mingdeng Cao,
Yanbo Fan, Xuan Wang, Qingyan Bai, Baoyuan Wu, Jue
Wang, and Yujiu Yang. Styleheat: One-shot high-resolution
editable talking face generation via pre-trained stylegan. In
ECCV, 2022. 2,7
[98] Yu Yin, Kamran Ghasedi, HsiangTao Wu, Jiaolong Yang,
Xin Tong, and Yun Fu. Nerfinvertor: High fidelity nerf-ganinversion for single-shot real image animation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), pages 8539–8548, 2023. 3
[99] Wangbo Yu, Yanbo Fan, Yong Zhang, Xuan Wang, Fei
Yin, Yunpeng Bai, Yan-Pei Cao, Ying Shan, Yang Wu,
Zhongqian Sun, and Baoyuan Wu. Nofa: Nerf-based one-
shot facial avatar reconstruction. In ACM SIGGRAPH 2023
Conference Proceedings, 2023. 2,3
[100] Ziyang Yuan, Yiming Zhu, Yu Li, Hongyu Liu, and Chun
Yuan. Make encoder great again in 3d gan inversion
through geometry and occlusion-aware encoding. arXiv
preprint arXiv:2303.12326, 2023. 3
[101] Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and
Victor Lempitsky. Few-shot adversarial learning of real-
istic neural talking head models. In Proceedings of the
IEEE/CVF international conference on computer vision,
pages 9459–9468, 2019. 2,7
[102] Egor Zakharov, Aleksei Ivakhnenko, Aliaksandra
Shysheya, and Victor Lempitsky. Fast bi-layer neural
synthesis of one-shot realistic head avatars. In European
Conference on Computer Vision, pages 524–540. Springer,
2020. 2
[103] Bowen Zhang, Chenyang Qi, Pan Zhang, Bo Zhang,
HsiangTao Wu, Dong Chen, Qifeng Chen, Yong Wang, and
Fang Wen. Metaportrait: Identity-preserving talking head
generation with fast personalized adaptation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), pages 22096–22105, 2023. 2
[104] Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing
Liao. Fdnerf: Few-shot dynamic neural radiance fields for
face reconstruction and expression editing. arXiv preprint
arXiv:2208.05751, 2022. 2
[105] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 586–595, 2018. 5,7
[106] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie
Fan. Flow-guided one-shot talking face generation with
a high-resolution audio-visual dataset. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 3661–3670, 2021. 7
[107] Jian Zhao and Hui Zhang. Thin-plate spline motion model
for image animation. In CVPR, pages 3657–3666, 2022. 2
[108] Xiaochen Zhao, Lizhen Wang, Jingxiang Sun, Hongwen
Zhang, Jinli Suo, and Yebin Liu. Havatar: High-fidelity
head avatar via facial model conditioned neural radiance
field. ACM Trans. Graph., 2023. 2
[109] Yufeng Zheng, Victoria Fern ´andez Abrevaya, Marcel C.
B¨uhler, Xu Chen, Michael J. Black, and Otmar Hilliges. I
m avatar: Implicit morphable head avatars from videos. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR), pages 13545–13555,
2022.
[110] Yufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael J.
Black, and Otmar Hilliges. Pointavatar: Deformable point-
based head avatars from videos. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2023. 2
[111] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei
Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. Celebv-
hq: A large-scale video facial attributes dataset. In Eu-
10347
ropean conference on computer vision, pages 650–667.
Springer, 2022. 5
[112] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei
Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. CelebV-
HQ: A large-scale video facial attributes dataset. In ECCV,
2022. 2,5,7
[113] Wojciech Zielonka, Timo Bolkart, and Justus Thies. Instant
volumetric head avatars. In IEEE/CVF Conf. on Computer
Vision and Pattern Recognition (CVPR), 2023. 2
10348
