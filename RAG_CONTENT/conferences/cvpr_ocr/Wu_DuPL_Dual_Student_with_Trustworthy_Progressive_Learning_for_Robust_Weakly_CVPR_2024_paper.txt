DuPL: Dual Student with Trustworthy Progressive Learning for Robust
Weakly Supervised Semantic Segmentation
Yuanchen Wu1, Xichen Ye1, Kequan Yang1, Jide Li1, Xiaoqiang Li1*
1School of Computer Engineering and Science, Shanghai University, China.
{yuanchenwu,yexichen0930,kqyang,iavtvai,xqli }@shu.edu.cn
Abstract
Recently, One-stage Weakly Supervised Semantic Seg-
mentation (WSSS) with image-level labels has gained in-
creasing interest due to simplification over its cumbersome
multi-stage counterpart. Limited by the inherent ambiguity
of Class Activation Map (CAM), we observe that one-stage
pipelines often encounter confirmation bias caused by in-
correct CAM pseudo-labels, impairing their final segmenta-
tion performance. Although recent works discard many un-
reliable pseudo-labels to implicitly alleviate this issue, they
fail to exploit sufficient supervision for their models. To this
end, we propose a dual student framework with trustwor-
thy progressive learning ( DuPL ). Specifically, we propose
a dual student network with a discrepancy loss to yield di-
verse CAMs for each sub-net. The two sub-nets generate
supervision for each other, mitigating the confirmation bias
caused by learning their own incorrect pseudo-labels. In
this process, we progressively introduce more trustworthy
pseudo-labels to be involved in the supervision through dy-
namic threshold adjustment with an adaptive noise filtering
strategy. Moreover, we believe that every pixel, even dis-
carded from supervision due to its unreliability, is impor-
tant for WSSS. Thus, we develop consistency regularization
on these discarded regions, providing supervision of every
pixel. Experiment results demonstrate the superiority of the
proposed DuPL over the recent state-of-the-art alternatives
on PASCAL VOC 2012 and MS COCO datasets. Code is
available at https://github.com/Wu0409/DuPL .
1. Introduction
Weakly supervised semantic segmentation (WSSS) aims at
using weak supervision, such as image-level labels [13, 42],
scribbles [28, 41], and bounding boxes [22, 32], to allevi-
ate the reliance on pixel-level annotations for segmentation.
Among these annotation forms, using image-level labels is
the most rewarding yet challenging way, as it only provides
*Corresponding author.
/s79/s117/s114/s115Figure 1. CAM pseudo-labels ( train )vs.segmentation perfor-
mance ( val) on PASCAL VOC 2012 . DuPL outperforms state-
of-the-art one-stage competitors and achieves comparable perfor-
mance with multi-stage methods in terms of CAM pseudo-labels
and final segmentation performance. †denotes using ImageNet-
21k pretrained parameters.
the presence of certain classes without offering any localiza-
tion information. In this paper, we also focus on semantic
segmentation using image-level labels.
Prevalent works typically follow a multi-stage pipeline
[18], i.e., pseudo-label generation, refinement, and segmen-
tation training. First, the pixel-level pseudo-labels are de-
rived from Class Activation Map (CAM) through classifi-
cation [46]. Since CAM tends to identify the discrimina-
tive semantic regions and fails to distinguish co-occurring
objects, the pseudo-labels often suffer from the CAM am-
biguity. Thus, they are then refined by training a refinement
network [1, 2]. Finally, the refined pseudo-labels are used
to train a segmentation model in a fully supervised manner.
Recently, to simplify the multi-stage process, many studies
proposed one-stage solutions that simultaneously produce
pseudo-labels and learn a segmentation head [3, 39, 40].
Despite their enhanced training efficiency, the performance
still lags behind their multi-stage counterparts.
One important yet overlooked reason is the confirma-
tion bias of CAM, stemming from the concurrent process
of CAM pseudo-label generation and segmentation super-
vision. For the one-stage pipeline, the segmentation train-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
3534
ImageEarly stageMiddle stageFinal stageSeg. ResultAs training proceedsFigure 2. Confirmation bias of CAM. As training proceeds, the
bias will be consistently reinforced, impairing the final segmenta-
tion performance. Here, we use the ViT-B [12] baseline and intro-
duce more unreliable pseudo-labels to amplify this phenomenon.
ing enforces the backbone features to align with the CAM
pseudo-labels. Since the backbone features are shared for
the segmentation head and the CAM generation, these in-
accurate CAM pseudo-labels not only hinder the learning
process of segmentation but, more critically, reinforce the
CAM’s incorrect judgments. As illustrated in Figure 2, this
issue consistently deteriorates throughout the training phase
and eventually degrades the segmentation performance. Re-
cent one-stage approaches [39, 40, 44] commonly set a fixed
and high threshold to filter unreliable pseudo-labels, which
prioritizes high-quality supervision to implicitly alleviate
this issue. However, this strategy fails to exploit sufficient
supervision for their models. Employing a fixed and high
threshold inevitably discards many pixels that actually have
correct CAM pseudo-labels. Furthermore, these unreliable
regions discarded from supervision often exist in seman-
tically ambiguous regions. Excluding them directly from
supervision makes the model rarely learn the segmentation
in these regions, leading to insufficient training. From this
perspective, we believe that every pixel matters for segmen-
tation and should be properly utilized.
To address the above limitations, this work proposes
adual student framework with trustworthy progressive
learning , dubbed DuPL. Inspired by the co-training [35]
paradigm, we equip two student sub-networks that engage
in mutual learning. They infer diverse CAMs from different
views, and transfer the knowledge learned from one view
to the other. To avoid homogenous students, we impose a
representation-level discrepancy constraint on the two sub-
nets. This architecture effectively mitigates the confirma-
tion bias resulting from their own incorrect pseudo-labels,
thus producing high-fidelity CAMs. Based on our dual stu-
dent framework, we propose trustworthy progressive learn-
ing for sufficient segmentation supervision. We set up a dy-
namic threshold adjustment strategy to involve more pixels
in the segmentation supervision. To overcome the noise in
CAM pseudo-labels, we propose an adaptive noise filteringstrategy based on the Gaussian Mixture Model. Finally, for
the regions where pseudo-labels are excluded from super-
vision due to their unreliability, we employ an additional
strong perturbation branch for each sub-net and develop
consistency regularization on these regions. Overall, our
main contributions are summarized as follows:
• We explore the CAM confirmation bias in one-stage
WSSS. To address this limitation, we propose a dual stu-
dent architecture. Our experiment proves its effectiveness
of reducing the over-activation rate caused by this issue
and promotes the quality of CAM pseudo-labels.
• We propose progressive learning with adaptive noise fil-
tering, which allows more trustworthy pixels to partici-
pate in supervision. For the regions with filtered pseudo-
labels, we develop consistency regularization for suffi-
cient training. This strategy highlights the importance of
fully exploiting pseudo-supervision for WSSS.
• Experiments on the PASCAL VOC and MS COCO
datasets show that DuPL surpasses state-of-the-art one-
stage WSSS competitors and achieves comparable perfor-
mance with multi-stage solutions (Figure 1). Through vi-
sualizing the segmentation results, we observe that DuPL
shows much better segmentation robustness, thanks to our
dual student and trust-worthy progressive learning.
2. Related work
One-stage Weakly Supervised Semantic Segmentation.
Due to the complex process of multi-stage solutions [1, 2],
many recent efforts mainly focused on one-stage solutions
[3, 39, 40, 44]. A common one-stage pipeline is generat-
ing CAM and using online refinement modules to obtain fi-
nal pseudo-labels [3]. These pseudo-labels are then directly
used as the supervision for the segmentation head. Typi-
cally, recent works mainly proposed additional modules or
training objectives to achieve better segmentation. For in-
stance, Zhang et al . [45] introduce a feature-to-prototype
alignment loss with an adaptive affinity field, Ru et al. [39]
leverage pseudo-labels to guide the affinity learning of self-
attention, and Xu et al. [44] utilize feature correspondence
to achieve self-distillation. One common practice of them
is that they all set a high and fixed threshold to filter out un-
reliable pseudo-labels to ensure the quality of supervision.
In contrast, we propose a progressive learning strategy fully
exploit the potential of every pseudo-label.
Confirmation Bias. This phenomenon commonly occurs
in the self-training paradigm of semi-supervised learning
(SSL) [21], where the model overfits the unlabeled im-
ages assigned with incorrect pseudo-labels. In the above
process, this incorrect information is constantly reinforced,
causing the unstable training process [4]. Co-training of-
fers an effective solution to this issue [35]. It uses two di-
verse sub-nets to provide mutual supervision to ensure more
3535
stable and accurate predictions while mitigating the confir-
mation bias [8, 33]. Motivated by this, we propose a dual
student architecture with a representation-level discrepancy
loss to generate diverse CAMs. The two sub-nets learn from
each other through the other’s pseudo-labels, countering the
CAM confirmation bias and achieving better object activa-
tion. To the best of our knowledge, DuPL is the first work
exploring the CAM confirmation bias in one-stage WSSS.
Noise Label Learning in WSSS. In addition to better CAM
pseudo-label generation, several recent works aim at learn-
ing a robust segmentation model using existing pseudo-
labels [10, 27, 31]. URN [27] introduces the uncertainty es-
timation by the pixel-wise variance between different views
to filter noisy labels. Based on the early learning and mem-
orization phenomenon [30], ADELE [31] adaptively cali-
brates noise labels based on prior outputs in the early learn-
ing stage. Different from these works relying on the exist-
ing CAM pseudo-labels by other works, the pseudo-labels
in one-stage methods continuously update in training. To al-
leviate the noise pseudo-labels for our progressive learning,
we design an online adaptive noise filtering strategy based
on the loss feedback from the segmentation head.
3. Method
3.1. Preliminary
We begin with a brief review of how to generate CAM [46]
and its pseudo-labels. Given an image, its feature maps
F∈RD×H×Ware extracted by a backbone network, where
DandH×Wis the channel and spatial dimension, re-
spectively. Then, Fis fed to a global average pooling and a
classification layer to output the final classification score. In
the above process, we can retrieve the classification weight
of every class W∈RC×Dand use it to weight and sum the
feature maps to generate the CAM:
M(c) =ReLU DX
i=1Wc,i·Fi!
, (1)
where cis the c-th class and ReLU is used to eliminate neg-
ative activations. Finally, we apply max-min normaliza-
tion to rescale M∈RC×H×Wto[0,1]. To generate the
CAM pseudo-labels, one-stage WSSS methods commonly
use two background thresholds τlandτhto separate the
background ( M≤τl), uncertain region ( τl<M< τh),
and foreground ( M≥τh) [39, 40]. The uncertain part is
regarded as unreliable regions with noise, and will not be
involved in the supervision of the segmentation head.
3.2. Dual Student Framework
To overcome the confirmation bias of CAM, we propose a
co-training-based dual student network where two sub-nets
(i.e.,ψ1andψ2) have the same network architecture, andtheir parameters are independently updated and non-shared.
As presented in Figure 3, for the i-th sub-net, it comprises
a backbone network ψf
i, a classifier ψc
i, and a segmenta-
tion head ψs
i. To ensure that the two sub-nets activate more
diverse regions in CAMs, we enforce sufficient diversity
to their representations extracted from ψf
i, preventing two
sub-nets from being homogeneous such that one subnet can
learn the knowledge from the other to alleviate the confir-
mation bias of CAM. Therefore, we set a discrepancy con-
straint to minimize the cosine similarity between the feature
maps of two sub-nets. Formally, denoting the input image
asXand the features from the sub-nets as f1=ψf
1(X)and
f2=ψf
2(X), we minimize their similarity by:
D(f1,f2) =−log
1−f1·f2
∥f1∥2× ∥f2∥2
,(2)
where ∥·∥2isl2-normalization. Following [7, 14], we de-
fine a symmetrized discrepancy loss as:
Ldis=D(f1,∆(f2)) +D(f2,∆(f1)), (3)
where ∆is the stop-gradient operation to avoid the model
from collapse. This loss is computed for each image, with
the total loss being the average across all images.
The segmentation supervision of dual student is bidirec-
tional. One is from M1toψ2and the other one is M2to
ψ1, where M1,M2are the CAM from the sub-nets ψ1,
ψ2, respectively. The CAM pseudo-labels Y1fromM1are
used to supervise the prediction maps P2from the other
sub-net’s segmentation head ψs
2, and vice versa. The seg-
mentation loss of our framework is computed as:
Lseg=CE(P1,Y2) +CE(P2,Y1), (4)
where CEis the standard cross-entropy loss function.
3.3. Trustworthy Progressive Learning
Dynamic Threshold Adjustment. As mentioned in Sec-
tion 3.1, one-stage methods [39, 40, 44] set background
thresholds, τlandτh, to generate pseudo-labels, where τhis
usually set to a very high value to ensure that only reliable
foreground pseudo-labels can participate in the supervision.
In contrast, during the training of dual student framework,
the CAMs tend to be more reliable gradually. Based on this
intuition, to fully utilize more foreground pseudo-labels for
sufficient training, we adjust the background threshold τh
with the cosine descent strategy in every iteration:
τh(t) = τh(0)−1
2(τh(0)−τh(T)) (1 −cos(tπ
T)),(5)
where tis the current number of iteration and Tis the total
number of training iterations.
Adaptive Noise Filtering. To further reduce the noise in
the produced pseudo-labels that impacts the segmentation
3536
Input imageBackbone
ViTViT
ℒ!"#
CAMs
CAMs
Seg. headANFDTAProgressive Learning
Seg. headfeat. mapsANFDTAProgressive Learning
ℒ#$%
ℒ#$%LossPixel Nummodeling
modeling
ℒ!"#
preds
predsℒ!"#
Seg. headSeg. head……
Input image
Perturbationpseudo-labels
pseudo-labelsdata flow
stop gradient ANFAdaptive Noise FilteringDynamic Thresholdℒ…Lossfiltered regionℒ'(#Figure 3. The overall framework of DuPL. We use a discrepancy loss Ldisto constrain the two sub-nets to generate diverse CAMs. Their
CAM pseudo-labels are utilized for segmentation cross-supervision Lseg, which mitigates the CAM confirmation bias. In this process, we
set a dynamic threshold to progressively introduce more pixels to segmentation supervision. Adaptive Noise Filtering strategy is equipped
to minimize the noise in pseudo-labels via the segmentation loss distribution. To utilize every pixel, the filtered regions are implemented
consistency regularization Lregwith their perturbed counterparts. The classifier is simplified for the clear illustration.
generalizability and reinforces the CAM confirmation bias,
we develop an adaptive noise filtering strategy to implement
trust-worthy progressive learning. Previous studies suggest
that deep networks tend to fit clean labels faster than noisy
ones [5, 15, 37]. This implies that the samples with smaller
losses are more likely to be considered as the clean ones
before the model overfits the noisy labels. A simple idea is
to use a predefined threshold to divide the clean and noisy
pseudo-labels based on their training losses. However, it
fails to consider that the model’s loss distribution is different
across various samples, even those within the same class.
To this end, we develop an Adaptive Noise Filtering
strategy to distinguish noisy and clean pseudo-labels via
the loss distribution, as depicted in Figure 4. Specifically,
for the input image Xwith its segmentation map Pand
CAM pseudo-label Y, we hypothesize the loss of each pixel
x∈X, defined as lx=CE(P(x),Y(x)), is sampled from
a Gaussian mixture model (GMM) P(x)on all pixels with
two components, i.e., clean cand noisy n:
P(lx) =wcN(lx|µc,(σc)2) +wnN(lx|µn,(σn)2),(6)
where N(µ, σ2)represents one Gaussian distribution,
wc, µc, σcandwn, µn, σncorrespond to the weight, mean,
and variance of two components. Thereinto, the component
with high loss values corresponds to the noise component.
Through Expectation Maximization algorithm [25], we can
infer the noise probability ϱn(lx), which is equivalent to
the posterior probability of P(noise |lx, µn,(σn)2). If
ϱn(lx)> γ, the corresponding pixel will be classified as
noise. Note that not all pseudo-labels Yare composed ofnoise, and thus the loss distributions may not have two clear
Gaussian distributions. Therefore, we additionally measure
the distance between µcandµn. If(µn−µc)≤η, all the
pixel pseudo-labels will be regarded as clean ones. Finally,
the set of noisy pixel pseudo-labels are determined as
Xn={x|ϱn(lx)> γ, µ c−µn> η}, (7)
and they are excluded in the segmentation supervision. In
DuPL, each sub-net’s pseudo-labels are conducted adaptive
noise filtering strategy independently.
Every Pixel Matters. In one-stage WSSS, discarding unre-
liable pseudo-labels that probably contain noises is a com-
mon practice to ensure the quality of the segmentation
or other auxiliary supervision [39, 40, 44]. Although we
gradually introduce more pixels to the segmentation train-
ing, there are still many unreliable pseudo-labels being dis-
carded due to the semantic ambiguity of CAM. Typically,
throughout the training phase, unreliable regions often exist
in non-discriminative regions, boundaries, and background
regions. Such an operation may cause the segmentation
head to lack sufficient supervision in these regions.
To address this limitation, we treat the regions with un-
reliable pseudo-labels as unlabeled samples. Despite no
clear pseudo-labels to supervise the segmentation in these
regions, we can regularize the segmentation head to output
consistent predictions when fed perturbed versions of the
same image. The consistency regularization implicitly im-
poses the model to comply with the smoothness assumption
[6, 20], which provides additional supervision for these re-
gions. Specifically, we first apply strong augmentation ϕ
3537
Density0.250.200.150.100.050.000.05.010.015.020.0LossDensity0.7
Loss0.60.50.40.30.20.10.00.02.04.06.08.010.012.0
Image #1
Image #2Image #2Image #1OriginalAfter AFNGaussian Mixture Model within ANFfiltered
filteredOriginalAfter AFNpersonmotorbikeplanttvfiltered regionsFigure 4. The loss distribution of images with noisy pseudo-
labels. The model produces incorrect pseudo-labels of plant .
Two peaks appear in the loss distribution on the two pseudo-labels,
and the red peak with anomalous losses is mainly caused by noises.
The distribution of normal losses is rescaled for visualization.
to perturb the input image ϕ(X)→eX, and then send it
to the sub-nets to get the segmentation prediction ePifrom
ψs
i. Using the pseudo-label ϕ′(Yi)taking the same affine
transformation in ϕas the supervision, the consistency reg-
ularization of the i-th sub-net is formulated as:
Lregi=1
| Mi|X
x∈XCEh
ePi(ϕ(x)), ϕ′(Yi(x))i
· Mi,
(8)
where Miis the mask indicating the filtered pixels with
unreliable pseudo-labels of the i-th sub-net. The filtered
pixel is masked as 1, and otherwise it is 0. The total reg-
ularization loss of our dual student framework is Lreg=
Lreg 1+Lreg 2. This loss is computed for each image, with
the total loss being the average across all images.
3.4. Training objective of DuPL
As illustrated in Figure 3, DuPL consists four training ob-
jectives, that are, the classification loss Lcls, the discrep-
ancy loss Ldis, the segmentation loss Lseg, and consistency
regularization loss Lreg. Following the common practice
in WSSS, we use the multi-label soft margin loss for clas-
sification. The total optimization objective of DuPL is the
linear combination of the above loss terms:
L=Lcls+λ1Ldis+λ2Lseg+λ3Lreg, (9)
where λiis the weight to rescale the loss terms.
4. Experiments
4.1. Experimental Settings
Datasets. We evaluate the proposed DuPL on the two stan-
dard WSSS datasets, i.e., PASCAL VOC 2012 and MSMethod Backbone train val
Multi-stage WSSS Methods
PPC [13] CVPR’2022 + PSA [1] WR38 73.3 –
ACR [19] CVPR’2023 + IRN [2] WR38 72.3 –
One-stage WSSS Methods
1Stage [3] CVPR’2020 WR38 66.9 65.3
ViT-PCM [38] ECCV’2022 ViT-B†67.7 66.0
AFA [39] CVPR’2022 MiT-B1 68.7 66.5
ToCo [40] CVPR’2023 ViT-B 72.2 70.5
DuPL ViT-B 75.1 73.5
DuPL†ViT-B†76.0 74.1
Table 1. Evaluation of CAM pseudo labels. The results are eval-
uated on the VOC train andval set and reported in mIoU (%).
†denotes using ImageNet-21k pretrained parameters.
COCO 2014 datasets. For the VOC 2012 dataset, it is ex-
tended with the SBD dataset [16] following common prac-
tice. The train, val, and test set are composed of 10582,
1449, and 1456 images, respectively. The test performance
of DuPL is evaluated on the official evolution server. For
the COCO 2014 dataset, its train and val set involve 82k
and 40k images, respectively. The mean Intersection-over-
Union (mIoU) is reported for performance evaluation.
Network Architectures of DuPL. We use the ViT-B [12]
with a lightweight classifier and a segmentation head, and
the patch token contrast loss [40] as our baseline network.
The classifier is a fully connected layer. The segmentation
head consists of two 3×3convolutional layers (with a dila-
tion rate of 5) and one 1×1prediction layer. The patch to-
ken contrast loss is applied to alleviate the over-smoothness
issue of CAM in ViT-like architectures. DuPL is composed
of two subnets with the baseline settings, where the back-
bones are initialized with ImageNet pretrained weights.
Implement Details. We adopt the AdamW optimizer with
an initial learning rate set to 6e−5and a weight decay factor
0.01. The input images are augmented using the strategy in
[40], and cropped to 448×448. For the strong perturbations,
we adopt Random Augmentation strategy [11] on color and
apply additional scaling and horizontal flipping. In the in-
ference stage, following the common practice in WSSS, we
use multi-scale testing and dense CRF processing.
For experiments on the VOC 2012 dataset, the batch
size is set as 4. The total iteration is set as 20k with
2k iterations warmed up for the classifiers and 6k itera-
tions warmed up for the segmentation heads before conduct-
ing Adaptive Noise Filtering. The background thresholds
(τl, τh(0), τh(T)) are set as ( 0.25,0.7,0.55). The thresh-
olds ( γ, η) of Adaptive Noise Filtering are set as ( 0.9,1.0).
The weight factors ( λ1, λ2, λ3) of the loss terms in Section
3.4 are set as ( 0.1,0.2,0.05). For the COCO dataset, the
batch size is set as 8. The network is trained for 80k iter-
3538
Image & GTToCoDuPLImage & GTToCoDuPLFigure 5. Visual comparison of CAMs. We compare the state-of-
the-art one-stage approach, ToCo [40], with our proposed DuPL.
DuPL not only suppresses over-activations but also achieves more
complete object activation coverage.
ations with 5k iterations warmed up for the classifier, and
20k iterations warmed up for the segmentation head. The
other settings are remained the same.
4.2. Experimental Results
CAM and Pseudo-labels. We begin by visualizing the
CAM of DuPL in Figure 5. We can find that, using the same
ViT-B backbone with ImageNet-1k pretrained weights, our
method can generate more complete and accurate CAMs
when compared to current state-of-the-art one-stage work,
i.e., ToCo [40]. Then, we evaluate the CAM pseudo-labels
on the train andval set of the VOC dataset and com-
pare them with recent state-of-the-art WSSS methods. In
one-stage methods, the pseudo-labels are directly generated
using CAMs, while those of multi-stage methods are pro-
duced by the initial seed generation and refinement pro-
cesses. The results are presented in Table 1. As can be
seen, DuPL significantly outperforms the recent one-stage
competitors and even surpasses the multi-stage methods.
Compared with other methods with ViT-B baseline, our
methods can produce higher quality pseudo-labels than the
competitors with both the ImageNet-1k and ImageNet-21k
pretrained weights. Using ViT-B with ImageNet-21k pre-
trained weights, we boost the pseudo-label performance to
76.0% ( +3.8% ) and 74.1% ( +3.6% ) on the train and
val set, respectively.
Final Segmentation Results. Table 2 reports the final seg-
mentation performance of DuPL. To show the superiority
of the proposed method, we compare our performance with
both one-stage and multi-stage prior arts. Notably, the pro-
posed DuPL achieves 73.3% ( +3.5% ), 72.8% ( +2.3% ) and
44.6% ( +3.3% ) mIoU on the VOC val,test and COCO
val set, respectively, which significantly surpasses recent
one-stage methods. The performance of DuPL strongly
supports that fully exploiting the trustworthy pseudo-labels
1http://host.robots.ox.ac.uk:8080/anonymous/
103D8M.html
2http://host.robots.ox.ac.uk:8080/anonymous/
R7RLMS.htmlSup. Net.VOC COCO
val test val
Multi-stage WSSS Methods .
EPS [24] CVPR’2021 I+SDL-V2 71.0 71.8 –
L2G [17] CVPR’2022 I+SDL-V2 72.1 71.7 44.2
PPC [13] CVPR’2022 I+SDL-V2 72.6 73.6 –
Linet al. [29] CVPR’2023 I+TDL-V2 71.1 71.4 45.4
ReCAM [9] CVPR’2022 I DL-V2 68.4 68.2 45.0
W-OoD [23] CVPR’2022 I WR-38 70.7 70.1 –
ESOL [26] NeurIPS’2022 I DL-V2 69.9 69.3 42.6
MCTformer [43] CVPR’2022 I WR-38 71.9 71.6 42.0
OCR [10] CVPR’2023 I WR-38 72.7 72.0 42.5
ACR [19] CVPR’2023 I DL-V2 71.9 71.9 45.3
One-stage WSSS Methods .
RRM [45] AAAI’2020 I WR-38 62.6 62.9 –
1Stage [3] CVPR’2020 I WR-38 62.7 64.3 –
AFA [39] CVPR’2022 I MiT-B1 66.0 66.3 38.9
SLRNet [34] IJCV’2022 I WR-38 67.2 67.6 35.0
TSCD [44] AAAI’2023 I MiT-B1 67.3 67.5 40.1
ToCo [40] CVPR’2023 I ViT-B 69.8 70.5 41.3
DuPL I ViT-B 72.2 71.6143.5
DuPL†I ViT-B†73.3 72.8244.6
Table 2. Semantic Segmentation Results . “Sup.” denotes the su-
pervision type. I: Image-level labels; S: Saliency maps. T: text-
driven supervision from CLIP [36]. “Net.” denotes the backbone
in one-stage methods and the segmentation network in multi-stage
methods. †denotes using ImageNet-21k pretrained weights.
is very important to single-stage methods . Also, DuPL
proves that using the one-stage pipeline is strong enough
to achieve competitive WSSS performance with multi-stage
approaches. Along with the quantitative comparison results,
we visualize and compare the segmentation masks of DuPL,
ToCo [40], and ground-truths in Figure 6. We can see that
DuPL predicts more accurate objects in challenging scenes,
which are close to their ground truths.
Fully-Supervised Counterparts. As presented in Table 3,
the one-stage competitors adopt various backbones, e.g.,
Wide ResNet38 (WR-38), MixFormer-Base1 (MiT-B1),
and ViT-Base (ViT-B). To eliminate the impact of backbone
on segmentation results for fair comparison, we compared
the performance gap between the methods and their fully
supervised counterpart. Notably, when using the ImageNet-
1k pre-trained weight, DuPL achieves 72.2% mIoU and
90.1% of its upper bound performance, significantly ahead
of recent one-stage one-stage methods ( +3.4% ).
4.3. Ablation studies and Analysis
Effectiveness of Components. The proposed DuPL con-
sists of a dual student (DS) architecture and trust-worthy
progressive learning. Within the progressive learning, we
have dynamic threshold adjustment (DTA) and Adaptive
3539
VOCGTToCoDuPL
COCO
Figure 6. Visualization of segmentation results on PSCAL VOC 2012 and MS COCO datasets . We compare the results of DuPL with
those of ToCo [40]. Both of them use ViT-B with ImageNet-1k as the backbone for fair comparison.
Noise Filtering (ANF). In addition to the basic classification
and segmentation loss, DuPL also incorporates two training
losses, i.e., discrepancy loss Ldisand consistency regular-
ization loss Lreg. We now investigate the contributions of
each module and loss in DuPL.
The experiment results are presented in Table 4. We
can observe that employing solely dual student architec-
ture brings a slight improvement of nearly 2% mIoU for
CAM pseudo-labels, resulting in 63.8% mIoU of segmenta-
tion performance. In this setting, the CAM diversity arises
merely from the randomly initialized segmentation heads,
thus the CAMs from the two sub-nets are still highly iden-
tical, leaving a huge space for improvement. When in-
corporating Ldis, the performance of CAM pseudo-label
is improved to 67.3% mIoU, indicating that it can further
benefit the effectiveness of dual student architecture. As
CAM becomes increasingly reliable, DTA progressively
introduces more pixels into the segmentation supervision
and improves the segmentation performance by 2.6%. The
ANF suppresses noise pseudo-labels and improves segmen-
tation performance by 1.5%. It’s noted that high-quality su-
pervision of segmentation benefits the CAM quality, and
DTA with ANF significantly improves the pseudo labels by
4.3%. With the motivation of “every pixel matters”, Ldis
ultimately boosts the segmentation performance to 69.9%
mIoU, leading to the state-of-the-art.
Analysis of Dual Student. DuPL adopts the mutual su-
pervision of two student sub-nets to alleviate the confirma-
tion bias introduced by their own incorrect pseudo-labels.
The confirmation bias issue can be reflected by the over-
activation (OA) rate. A higher OA rate means the model
activates more incorrect pixels for the target classes, caus-
ing a more severe CAM confirmation bias. Here, we count
the number of the false positive (FP) and true positive (TP)
pixel pseudo-labels for each class, and calculate the OA rate
(i.e.,FP/ (TP+FP)). We first compare the baseline and
the ablated variant with Dual Student ( i.e., baseline + DS +
Ldis) under a low background threshold setting ( τh= 0.5).
From Figure 7a, we can see due to the confirmation bias,Method BB. val (F)val (I)ratio (%)
1Stage [3] WR38 80.8 62.7 77.6
SLRNet [34] WR38 80.8 67.2 83.2
AFA [39] MiT-B1 78.7 66.0 83.9
ToCo [40] ViT-B 80.5 69.8 86.7
DuPL ViT-B 80.5 72.2 90.1
DuPL†ViT-B†82.3 73.3 89.1
Table 3. The performance comparison with fully supervised
counterparts on the VOC dataset. The pixel pseudo labels are
used to supervise the segment head. F: fully-supervised supervi-
sion.I: image-level supervision (WSSS). ratio =val (I) /val
(F).†denotes using ImageNet-21k pretrained weights.
Baseline DS Ldis DTA ANF LregM Seg.
✓ 63.2 62.3
✓ ✓ 65.4 63.8
✓ ✓ ✓ 67.3 64.1
✓ ✓ ✓ ✓ 69.2 66.7
✓ ✓ ✓ ✓ ✓ 71.6 68.2
✓ ✓✓ ✓ ✓ ✓ 73.5 69.9
Table 4. Ablation Study. “M” denotes the CAM performance
and “Seg.” denotes the segmentation performance. CRF post-
processing is not conducted in the ablation study.
the baseline over-activates lots of incorrect regions, result-
ing in subpar segmentation outcomes (only 58.9% mIoU).
With dual student, the ablated version significantly reduces
the OA rate by over 15% in many classes, and even re-
duces the OA rate to below 5% for some categories (such as
cow anddog). Further, we evaluate the OA rate of ToCo
[40] and DuPL. From Figure 7b, we can see that ToCo also
suffers from the confirmation bias problem, with OA rate
exceeding 30% in several categories. In contrast, the pro-
posed DuPL significantly overcomes this problem in these
severely over-activated classes, which reflects the effective-
ness of our architecture.
3540
/s32/s66/s97/s115/s101/s108/s105/s110/s101/s42 /s32/s40/s53/s56/s46/s57/s37/s32/s109/s73/s111/s85/s41/s45/s49/s56/s46/s52/s37
/s45/s49/s52/s46/s54/s37/s45/s50/s49/s46/s48/s37
/s45/s49/s52/s46/s54/s37/s45/s49/s51/s46/s55/s37/s45/s49/s54/s46/s55/s37/s45/s56/s46/s56/s37
/s45/s49/s57/s46/s51/s37(a) Comparison of the baseline and the baseline with dual student.
/s32/s84/s111/s67/s111/s45/s55/s46/s51/s37
/s45/s49/s50/s46/s54/s37/s45/s49/s50/s46/s51/s37
/s45/s49/s48/s46/s48/s37/s45/s49/s50/s46/s52/s37
/s45/s54/s46/s50/s37/s45/s49/s51/s46/s52/s37/s45/s53/s46/s51/s37
(b) Comparison of ToCo [40] and the proposed DuPL.
Figure 7. Effectiveness evaluation of our proposed method. The
OA rate (%) are evaluated on the VOC val set. “*” denotes the
baseline is trained under a low background threshold ( τh= 0.5) to
aggregate the CAM conformation bias. The per-class results can
be viewed in Supplementary Material.
Dynamic Threshold Adjustment. In DuPL, τh(t)is a dy-
namic background threshold that progressively decreases
toτh(T)with training, aiming at involving more pseudo-
labels into the segmentation supervision. Table 5a shows
the impact of different τh(T) on the CAM and segmen-
tation performance. We observe that when τh(T) ranges
from 0.65 to 0.55, the model’s performance exhibits steady
improvement. However, when τh(T)is smaller than 0.55,
the excessive introduction of noises becomes challenging to
suppress, thus yielding a negative impact on the model per-
formance. Nevertheless, the model continues to improve in
comparison to the case with a relatively higher τh(T).
Warm-up Stage for The Segmentation Head. Motivated
by the Early-learning nature of deep networks, ANF uses
the feedback from the segmentation head to filter the noise
pseudo-labels. This requires the segmentation head to fit
the CAM pseudo-labels properly. Incorporating ANF too
early may risk filtering out correct pseudo-labels due to
under-fitting, while introducing ANF too late may lead to
the model having already memorized noisy pseudo-labels,
making it challenging to discriminate them. In Table 5b, we
report the impact on the warm-up stage for the segmenta-
tion head. We show that warming up the segmentation head
using 8000 iterations can achieve the best performance.
Discrepancy strategy in Dual Student. We apply the dis-τh(T)M Seg.
0.65 69.4 68.1
0.60 71.8 70.9
0.55 73.5 72.2
0.50 72.3 71.5
(a) Background threshold τh.Iter M Seg.
6000 72.4 70.9
8000 73.5 72.2
10000 72.6 71.7
12000 71.1 69.4
(b) Warm-up stage.
Table 5. Impact of hyper-parameters. The results are evaluated
on the VOC val set. The default settings are marked in color .
None Diff. Aug Ldis Diff. Aug + Ldis
M 69.6 70.7 73.5 70.9
Seg. 68.9 69.8 72.2 69.4
Table 6. Different discrepancy strategies in Dual student. The
results are evaluated on the VOC val set. “Diff. Aug” denotes
that the input images of two-subnets are augmented differently,
and the CAM pseudo-labels will be re-transformed to fit the inputs
for the other sub-net.
crepancy constraint on the representation level to make each
sub-nets generate more diverse CAMs. In Table 6, we com-
pare the impact of different discrepancy strategies. It shows
that only introducing Ldison the representation level is
more beneficial for two sub-nets to transfer the knowledge
learned from one view to the other through CAM pseudo-
labels, thus yielding favorable performance.
5. Conclusion
This work aims to address the problem of CAM confirma-
tion bias and fully utilize the CAM pseudo-labels for bet-
ter WSSS. Specifically, we develop a dual student architec-
ture with two sub-nets that mutually provide the pseudo-
labels for the other, which is empirically proved to counter
the CAM confirmation bias well. With better CAM acti-
vations during the training process, we gradually introduce
more pixels into the supervision for sufficient segmenta-
tion training. We overcome the excessive noisy pseudo-
labels brought by the above operation by proposing an
adaptive noise filter strategy. Such a trustworthy progres-
sive learning paradigm significantly boosts the WSSS per-
formance. Motivated by the idea that “every pixel mat-
ters”, instead of discarding unreliable labels, we fully lever-
age them through consistency regularizations. The exper-
iment results demonstrate that DuPL significantly outper-
forms other one-stage competitors and archives competitive
performance with multi-stage solutions.
Acknowledgements. This work is supported in part by
Shanghai science and technology committee under grant
No. 22511106005. We appreciate the High Performance
Computing Center of Shanghai University, and Shanghai
Engineering Research Center of Intelligent Computing Sys-
tem for the computing resources and technical support.
3541
References
[1] Jiwoon Ahn and Suha Kwak. Learning pixel-level semantic
affinity with image-level supervision for weakly supervised
semantic segmentation. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
4981–4990, 2018. 1, 2, 5
[2] Jiwoon Ahn, Sunghyun Cho, and Suha Kwak. Weakly su-
pervised learning of instance segmentation with inter-pixel
relations. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 2209–2218,
2019. 1, 2, 5
[3] Nikita Araslanov and Stefan Roth. Single-stage seman-
tic segmentation from image labels. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 4253–4262, 2020. 1, 2, 5, 6, 7
[4] Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor,
and Kevin McGuinness. Pseudo-labeling and confirmation
bias in deep semi-supervised learning. In 2020 International
Joint Conference on Neural Networks (IJCNN) , pages 1–8.
IEEE, 2020. 2
[5] Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David
Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan
Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio,
et al. A closer look at memorization in deep networks. In
International conference on machine learning , pages 233–
242. PMLR, 2017. 4
[6] Philip Bachman, Ouais Alsharif, and Doina Precup. Learn-
ing with pseudo-ensembles. Advances in neural information
processing systems , 27, 2014. 4
[7] Xinlei Chen and Kaiming He. Exploring simple siamese rep-
resentation learning. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
15750–15758, 2021. 3
[8] Xiaokang Chen, Yuhui Yuan, Gang Zeng, and Jingdong
Wang. Semi-supervised semantic segmentation with cross
pseudo supervision. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
2613–2622, 2021. 3
[9] Zhaozheng Chen, Tan Wang, Xiongwei Wu, Xian-Sheng
Hua, Hanwang Zhang, and Qianru Sun. Class re-activation
maps for weakly-supervised semantic segmentation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 969–978, 2022. 6
[10] Zesen Cheng, Pengchong Qiao, Kehan Li, Siheng Li, Pengxu
Wei, Xiangyang Ji, Li Yuan, Chang Liu, and Jie Chen. Out-
of-candidate rectification for weakly supervised semantic
segmentation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 23673–
23684, 2023. 3, 6
[11] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V
Le. Randaugment: Practical automated data augmen-
tation with a reduced search space. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition workshops , pages 702–703, 2020. 5
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 2, 5
[13] Ye Du, Zehua Fu, Qingjie Liu, and Yunhong Wang. Weakly
supervised semantic segmentation by pixel-to-prototype
contrast. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 4320–
4329, 2022. 1, 5, 6
[14] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin
Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch,
Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-
laghi Azar, et al. Bootstrap your own latent-a new approach
to self-supervised learning. Advances in neural information
processing systems , 33:21271–21284, 2020. 3
[15] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao
Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-
teaching: Robust training of deep neural networks with ex-
tremely noisy labels. Advances in neural information pro-
cessing systems , 31, 2018. 4
[16] Bharath Hariharan, Pablo Arbel ´aez, Lubomir Bourdev,
Subhransu Maji, and Jitendra Malik. Semantic contours from
inverse detectors. In 2011 international conference on com-
puter vision , pages 991–998. IEEE, 2011. 5
[17] Peng-Tao Jiang, Yuqi Yang, Qibin Hou, and Yunchao Wei.
L2g: A simple local-to-global knowledge transfer frame-
work for weakly supervised semantic segmentation. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 16886–16896, 2022. 6
[18] Alexander Kolesnikov and Christoph H Lampert. Seed, ex-
pand and constrain: Three principles for weakly-supervised
image segmentation. In Computer Vision–ECCV 2016: 14th
European Conference, Amsterdam, The Netherlands, Octo-
ber 11–14, 2016, Proceedings, Part IV 14 , pages 695–711.
Springer, 2016. 1
[19] Hyeokjun Kweon, Sung-Hoon Yoon, and Kuk-Jin Yoon.
Weakly supervised semantic segmentation via adversarial
learning of classifier and reconstructor. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 11329–11339, 2023. 5, 6
[20] Samuli Laine and Timo Aila. Temporal ensembling for semi-
supervised learning. arXiv preprint arXiv:1610.02242 , 2016.
4
[21] Dong-Hyun Lee et al. Pseudo-label: The simple and effi-
cient semi-supervised learning method for deep neural net-
works. In Workshop on challenges in representation learn-
ing, ICML , page 896. Atlanta, 2013. 2
[22] Jungbeom Lee, Jihun Yi, Chaehun Shin, and Sungroh Yoon.
Bbam: Bounding box attribution map for weakly super-
vised semantic and instance segmentation. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 2643–2652, 2021. 1
[23] Jungbeom Lee, Seong Joon Oh, Sangdoo Yun, Junsuk Choe,
Eunji Kim, and Sungroh Yoon. Weakly supervised semantic
segmentation using out-of-distribution data. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 16897–16906, 2022. 6
3542
[24] Seungho Lee, Minhyun Lee, Jongwuk Lee, and Hyunjung
Shim. Railroad is not a train: Saliency as pseudo-pixel su-
pervision for weakly supervised semantic segmentation. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 5495–5505, 2021. 6
[25] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix:
Learning with noisy labels as semi-supervised learning.
arXiv preprint arXiv:2002.07394 , 2020. 4
[26] Jinlong Li, Zequn Jie, Xu Wang, Xiaolin Wei, and
Lin Ma. Expansion and shrinkage of localization for
weakly-supervised semantic segmentation. arXiv preprint
arXiv:2209.07761 , 2022. 6
[27] Yi Li, Yiqun Duan, Zhanghui Kuang, Yimin Chen, Wayne
Zhang, and Xiaomeng Li. Uncertainty estimation via re-
sponse scaling for pseudo-mask noise mitigation in weakly-
supervised semantic segmentation. In Proceedings of the
AAAI Conference on Artificial Intelligence , pages 1447–
1455, 2022. 3
[28] Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun.
Scribblesup: Scribble-supervised convolutional networks for
semantic segmentation. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
3159–3167, 2016. 1
[29] Yuqi Lin, Minghao Chen, Wenxiao Wang, Boxi Wu, Ke
Li, Binbin Lin, Haifeng Liu, and Xiaofei He. Clip is also
an efficient segmenter: A text-driven approach for weakly
supervised semantic segmentation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 15305–15314, 2023. 6
[30] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Car-
los Fernandez-Granda. Early-learning regularization pre-
vents memorization of noisy labels. Advances in neural in-
formation processing systems , 33:20331–20342, 2020. 3
[31] Sheng Liu, Kangning Liu, Weicheng Zhu, Yiqiu Shen, and
Carlos Fernandez-Granda. Adaptive early-learning correc-
tion for segmentation from noisy annotations. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 2606–2616, 2022. 3
[32] Youngmin Oh, Beomjun Kim, and Bumsub Ham.
Background-aware pooling and noise-aware loss for
weakly-supervised semantic segmentation. In Proceedings
of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 6913–6922, 2021. 1
[33] Yassine Ouali, C ´eline Hudelot, and Myriam Tami. Semi-
supervised semantic segmentation with cross-consistency
training. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 12674–
12684, 2020. 3
[34] Junwen Pan, Pengfei Zhu, Kaihua Zhang, Bing Cao, Yu
Wang, Dingwen Zhang, Junwei Han, and Qinghua Hu.
Learning self-supervised low-rank network for single-stage
weakly and semi-supervised semantic segmentation. In-
ternational Journal of Computer Vision , 130(5):1181–1195,
2022. 6, 7
[35] Siyuan Qiao, Wei Shen, Zhishuai Zhang, Bo Wang, and Alan
Yuille. Deep co-training for semi-supervised image recogni-
tion. In Proceedings of the european conference on computer
vision (eccv) , pages 135–152, 2018. 2[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 6
[37] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urta-
sun. Learning to reweight examples for robust deep learn-
ing. In International conference on machine learning , pages
4334–4343. PMLR, 2018. 4
[38] Simone Rossetti, Damiano Zappia, Marta Sanzari, Marco
Schaerf, and Fiora Pirri. Max pooling with vision transform-
ers reconciles class and shape in weakly supervised semantic
segmentation. In European Conference on Computer Vision ,
pages 446–463. Springer, 2022. 5
[39] Lixiang Ru, Yibing Zhan, Baosheng Yu, and Bo Du. Learn-
ing affinity from attention: end-to-end weakly-supervised se-
mantic segmentation with transformers. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 16846–16855, 2022. 1, 2, 3, 4, 5, 6, 7
[40] Lixiang Ru, Heliang Zheng, Yibing Zhan, and Bo Du. To-
ken contrast for weakly-supervised semantic segmentation.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 3093–3102, 2023. 1,
2, 3, 4, 5, 6, 7, 8
[41] Paul Vernaza and Manmohan Chandraker. Learning random-
walk label propagation for weakly-supervised semantic seg-
mentation. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 7158–7166,
2017. 1
[42] Yuanchen Wu, Xiaoqiang Li, Songmin Dai, Jide Li, Tong
Liu, and Shaorong Xie. Hierarchical semantic contrast for
weakly supervised semantic segmentation. In Proceedings
of the Thirty-Second International Joint Conference on Arti-
ficial Intelligence, IJCAI-23 , pages 1542–1550, 2023. 1
[43] Lian Xu, Wanli Ouyang, Mohammed Bennamoun, Farid
Boussaid, and Dan Xu. Multi-class token transformer for
weakly supervised semantic segmentation. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 4310–4319, 2022. 6
[44] Rongtao Xu, Changwei Wang, Jiaxi Sun, Shibiao Xu, Weil-
iang Meng, and Xiaopeng Zhang. Self correspondence dis-
tillation for end-to-end weakly-supervised semantic segmen-
tation. In Proceedings of the AAAI Conference on Artificial
Intelligence , pages 3045–3053, 2023. 2, 3, 4, 6
[45] Bingfeng Zhang, Jimin Xiao, Yunchao Wei, Mingjie Sun,
and Kaizhu Huang. Reliability does matter: An end-to-end
weakly supervised semantic segmentation approach. In Pro-
ceedings of the AAAI Conference on Artificial Intelligence ,
pages 12765–12772, 2020. 2, 6
[46] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,
and Antonio Torralba. Learning deep features for discrimina-
tive localization. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 2921–2929,
2016. 1, 3
3543
