Do Vision and Language Encoders Represent the World Similarly?
Mayug Maniparambil* 1Raiymbek Akshulakov* 2Yasser Abdelaziz Dahou Djilali3 1
Mohamed El Amine Seddik3Sanath Narayan3Karttikeya Mangalam2Noel E. O’Connor1
Abstract
Aligned text-image encoders such as CLIP have become
the de-facto model for vision-language tasks. Further-
more, modality-specific encoders achieve impressive per-
formances in their respective domains. This raises a cen-
tral question: does an alignment exist between uni-modal
vision and language encoders since they fundamentally rep-
resent the same physical world? Analyzing the latent spaces
structure of vision and language models on image-caption
benchmarks using the Centered Kernel Alignment (CKA),
we find that the representation spaces of unaligned and
aligned encoders are semantically similar. In the absence of
statistical similarity in aligned encoders like CLIP , we show
that a possible matching of unaligned encoders exists with-
out any training. We frame this as a seeded graph-matching
problem exploiting the semantic similarity between graphs
and propose two methods - a Fast Quadratic Assignment
Problem optimization, and a novel localized CKA metric-
based matching/retrieval. We demonstrate the effectiveness
of this on several downstream tasks including cross-lingual,
cross-domain caption matching and image classification.
Code available at github.com/mayug/0-shot-llm-vision.
1. Introduction
The recent success of deep learning on vision-language
tasks mainly relies on jointly trained language and im-
age encoders following the success of CLIP and ALIGN
[20, 40]. The standard procedure for training these models
aims at aligning text and image representation using a con-
trastive loss that maximizes the similarity between image-
text pairs while pushing negative captions away [10, 19, 36].
This achieves a statistical similarity across the two latent
spaces, which is key to retrieving the closest cross-modal
representations using cosine similarity. This property is not
valid for unaligned encoders, hence, extra transformations
are needed to bridge the gap. These transformations can be
*joint first authors
1ML Labs, Dublin City University
2University of California Berkeley
3Technological Innovation Institute
Figure 1. For matching, we calculate the kernels for image and
text embeddings and employ QAP-based seeded matching to max-
imize CKA for obtaining the optimal permutation P. For retrieval,
we append query embeddings to base embeddings and retrieve the
best caption that maximizes the local CKA for a query image.
training a mapping network that captures the prior distribu-
tion over the text and image representations [31, 34, 35].
The work of [31] has shown that it is possible to train a
linear mapping from the output embeddings of vision en-
coders to the input embeddings of language models and
exhibit impressive performance on image captioning and
VQA tasks. This indicates that the representations between
the unaligned uni-modal vision and language encoders are
sufficiently high level and differ only by a linear transfor-
mation. However, this linear layer is trained on CC-3M [9]
consisting of three million image-caption pairs.
Is this training step necessary? In an ideal scenario,
we anticipate an alignment between vision and language
encoders as they inherently capture representations of the
same physical world. To this end, we employ Centered
Kernel Alignment (CKA) [12, 22, 42], which is known
for measuring representation similarity both within and be-
tween networks. As shown in Figure 2, we measure the
CKA between a variety of unaligned vision and language
encoders [8, 16, 28, 37, 47], on the image-caption pairs of
the COCO [27] dataset and observe that some have com-
parable scores to that of aligned encoders like CLIP [40],
affirmative of semantic similarities.
We then ask the question: If the unaligned image and text
encoders are semantically similar, is there a way to connect
them in a zero-shot manner? Do they build a similar rep-
resentation graph over the same information coming from
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
14334
the two modalities? We study these questions, revealing
key similarities between unaligned image and text encoders,
and how these similarities can be exploited for downstream
tasks. Furthermore, we devise a caption matching down-
stream task and show using two novel methods that latent
space communication between unaligned encoders could be
achieved by leveraging the semantic similarities between
the cross-modal spaces. Our contributions are:
• We present a matching method that seeks to find the per-
mutation of the captions that maximizes the CKA (see
Fig. 1). Hence, We formulate maximizing CKA as a
quadratic assignment problem and introduce transforma-
tions and normalizations that greatly improve the match-
ing performance.
• We propose a local CKA metric and use it to perform re-
trieval between two unaligned embedding spaces, demon-
strating superior performance with that of relative repre-
sentations [34] on the COCO caption image retrieval.
• The method is benchmarked on COCO, NoCaps [2]
cross-domain caption and image retrieval as well
ImageNet-100 [15] classification tasks despite our
method not being optimized to align the representation in
any manner demonstrating zero-shot communication be-
tween the encoder’s latent spaces.
• Finally, we show a practical application of our method on
cross-lingual image retrieval by making use of sentence
transformers trained in various languages and a CLIP vi-
sion encoder trained only in English.
2. Related Work
Recently, there has been an increasing consensus that good
networks, when trained independently, learn general rep-
resentations across different architectures and tasks. On
the one hand, the works of [6, 22, 26, 33] show that these
networks exhibit representation similarity by learning sim-
ilar latent spaces when trained on similar tasks and data
[3, 5, 11, 24, 32, 44, 46]. Specifically, [22] introduced
centered kernel alignment (CKA) as a similarity metric for
comparing the inner representations across networks. The
CKA measure mitigates the limitation of canonical correla-
tion analysis (CCA) [41] being invariant to an invertible lin-
ear transformation that often leads to difficulty in measur-
ing meaningful similarities between representations. [48]
uses CKA for comparing the representations from differ-
ent layers of different language models and the effect of
downstream task-finetuning on the representation similar-
ities, while [6] utilizes CKA along with Procrustes similar-
ity for understanding the ability of variational autoencoders
(V AEs) [21] in learning disentangled representations. In
general, these approaches study the representation similar-
ity in unimodal models, either vision or language. Clearly,
however, the use of CKA has been limited to visualization
and analysis purposes, whereas we attempt at exploitingCKA as an optimization objective.
Recent works [34, 35] employ relative representations
to match embeddings of unaligned encoders using the co-
sine similarity to a set of anchors. However, these rel-
ative representations are sensitive to the selection of an-
chors and noise in the original embeddings. Similarly, ap-
proaches [4, 14] analyze networks and empirically verify
the “good networks learn similar representations” hypothe-
sis by utilizing model stitching [24], which introduces train-
able stitching layers to enable swapping parts of different
networks. LiMBeR [31] can be seen as stitching the out-
put of an image encoder to the input of a language model in
the form of soft prompts [25]. However, these approaches
involve training of stitching layers for evaluating the repre-
sentation similarity between two models.
In this work, we argue that using an explicit similarity
measure as done in [34, 35] is sensitive to the selection of
anchors and noise in the original embeddings. One design
choice is an implicit measure that captures the similarity of
similarities, hence, inducing more robustness to the align-
ment process. Furthermore, we explore how this similar-
ity can be leveraged for downstream cross-modal tasks in a
training-free manner with the aid of CKA and a set of paral-
lel anchors in the image and text latent embedding spaces.
3. Preliminaries
Centered Kernel Alignment (CKA) has shown its relevance
in understanding and comparing the information encoded
by different layers of a neural network [22]. Formally, CKA
relies on two sets of data X∈Rp×NandY∈Rq×N
through their corresponding kernels K=k(X⊤,X)∈
RN×NandL=ℓ(Y⊤,Y)∈RN×Nwhere k, ℓare some
kernel functions applied on the columns of XandYrespec-
tively (e.g., linear or RBF kernels). Therefore, the CKA is
computed in terms of KandLas:
CKA( K,L) =HSIC( K,L)p
HSIC( K,K) HSIC( L,L),(1)
where HSIC( ·,·)is the Hilbert-Schmidt Independence Cri-
terion [18, 30] defined as:
HSIC( K,L) =1
(N−1)2tr (KCLC ), (2)
withC=I−1
N11⊤the centring matrix. We refer the
reader to [22] for broader properties and studies of the CKA
metric on neural network representations.
4. Proposed Method
Consider a set of Nimage-caption pairs, S=
{(xi,ci)}N
i=1, where xi∈ X andci∈ C represent the i-
th image and its corresponding caption, respectively. In this
14335
Table 1. CKA reduces with shuffling. We measure the CKA
score between DINOv2 [37] and All-Roberta-large-v1 [28] on the
5k COCO [27] image-caption representations pairs of the valset.
The exact ordering yields the best score, whereas randomly shuf-
fling the representations reduces the CKA score.
Shuffling (%) 0 20 40 60 80 100
CKA Score 0.72 0.46 0.27 0.13 0.04 0.01
particular example, we are performing caption-to-image re-
trieval, but it is applicable for the reverse as well. Let
f:X 7→Rd1andg:C 7→Rd2denote some vision and
language encoders respectively. The image-caption pairs
are mapped into their corresponding sets of representations
Z= [z1, . . . , zN]∈Rd1×NandH= [h1, . . . , hN]∈
Rd2×N, where zi=f(xi)andhi=g(ci).
As shown in Table 1, the maximum CKA score is ob-
tained on the ground-truth ordering of the representations
CKA max= CKA( KZ,KH), where KZandKHare the
kernels for the image and text representations, defined re-
spectively as KZ=k(Z⊤,Z)andKH=k(H⊤,H). We
find that the CKA is sensitive to the data ordering. Specifi-
cally, we shuffle x% of data to obtain wrong matches while
keeping the remaining 100-x% aligned, measure the CKA
on each new data set, and observe that it monotonically de-
creases with random shuffling. This motivates our method-
ology for finding an optimal permutation of the image data
that maximizes the CKA.
Formally, let σbe some permutation of the set
{1,···, N}and denote σ(Z) = [ zσ(1),···,zσ(N)]∈
Rd1×Nthe set of permuted image representations by σ. Ifσ
is not identity, it disrupts the original ordering of the image
representations leading to a lower CKA score as shown in
Table 1. Therefore, our goal is to find a permutation σ∗that
maximizes the CKA. Formally:
σ∗= arg max
σCKA( Kσ(Z),KH). (3)
The solution to this problem seeks to realign the permuted
set of images in a way that maximizes the CKA, poten-
tially recovering the ground-truth pairing between images
and their corresponding captions.
To solve the aforementioned optimization problem, we
explore two main approaches (visualized in Fig. 1): the
Quadratic Assignment Problem (QAP) algorithm and Lo-
cal CKA-based retrieval and matching. The QAP algorithm
provides a global matching solution, seeking the optimal
permutation across the query set considered. On the other
hand, Local CKA-based retrieval and matching focuses on
aligning images and captions using a localized metric, fa-
cilitating retrieval on a more granular level. This approach
is more suitable where a single query image is given for a
set of captions or vice versa .4.1. QAP Matching
For some random permutation σ, the optimization problem
in Equation 3 can be reformulated as a quadratic optimiza-
tion problem [45] which reads as:
max
P∈PNtr 
P⊤¯Kσ(Z)P¯KH
, (4)
where PNis the set of all permutation matrices of size N
and¯K= HSIC( K,K)−1
2KC stands for the centered and
re-scaled kernel. In principle, maximizing the above objec-
tive is a relaxation of a graph-matching problem. Moreover,
finding a global maximum of Equation 4 is NP-hard due to
the combinatorial nature of the problem and therefore opti-
mizing it can lead to sub-optimal or approximate solutions.
To overcome the NP-hardness of QAP, in practice,
we suppose that we have access to a base set B=
{(zb
i,hb
i)}M
i=1of image-caption representations pairs and
solve an equivalent objective to Equation 4 only partially
on some unmatched query set Q={zq
i}N
i=1× {hq
i}N
i=1
using a seeded version of the fast QAP algorithm [17]. For-
mally, let Z= [zb
1,···,zb
M,zq
1,···,zq
N]∈Rd1×(M+N)
andH= [hb
1,···,hb
M,hq
1,···,hq
N]∈Rd2×(M+N)be
the matrix concatenating all base and query representa-
tions of images and captions respectively, and denote by
¯KZ,¯KH∈R(M+N)×(M+N)the corresponding centered
and re-scaled kernels. The partial matching for aligning the
query samples is then performed by solving the following:
max
P∈PNtr 
(IM⊕P)⊤¯KZ(IM⊕P)¯KH
, (5)
where IM⊕P∈R(M+N)×(M+N)stands for the block-
diagonal matrix having diagonal blocks IMandP.
4.2. Local CKA based Retrieval and Matching
The concept of a global CKA metric is extended to derive
local similarity measures suitable for retrieval. This pro-
cess begins with a base set B={(zb
i,hb
i)}M
i=1consisting of
aligned pairs of images and captions representations. The
objective is to facilitate caption-image retrieval/matching
within an unaligned query set Q={zq
i}N
i=1× {hq
i}N
i=1.
A local CKA score, denoted as localCKA( zq,hq)for a
couple (zq,hq)∈ Q is calculated by computing a global
CKA score for the image-caption pairs in B, augmented
with the query pair (zq,hq). The local CKA is computed
as follows:
localCKA( zq,hq) = CKA( K[Z,zq],K[H,hq]), (6)
where [M,v]denotes the concatenation of the matrix M
and the vector vcolumn-wise and Z= [zb
1,···,zb
M]∈
Rd1×MandH= [hb
1,···,hb
M]∈Rd2×M. In essence, a
correctly matched image-caption pair in Qwould exhibit a
higher degree of alignment with the base set Bin terms of
14336
the CKA score, resulting in an elevated localCKA score.
This metric can be used to calculate a score between one
source query and Ntarget queries enabling effective re-
trieval. Furthermore, this framework allows for the use of
linear sum assignment [23] for matching tasks.
4.3. Stretching and Clustering
The choice of base samples and the spread of the represen-
tations in each embedding space affect the performance of
the QAP and Local CKA algorithms. To spread the repre-
sentations out in each domain for matching, we introduce
a stretching matrix that normalizes the features of each di-
mension by the variance calculated from the query and base
sets. Given X= [x1,···,xd]⊤∈Rd×N, the stretched
matrix Xsis computed as Xs=SX, where the stretching
matrix S∈Rd×dis a diagonal matrix with inverse empir-
ical standard deviation of the feature dimension as entries,
i.e.,S=diag
1
std(x1),···,1
std(xd)
andxi∈RNis the
ithrow of X. This stretching operation is performed for
both the image and text before calculating the kernels for
both QAP and local CKA matching algorithms. For pick-
ing the most effective base samples, we find that the simple
k-means clustering on the image embeddings works best.
An ablation on how these affect the QAP and local CKA
matching and retrieval accuracies is provided in Sec 7.
5. Experiments
We assess the performance of the proposed method using
various vision and language encoders on a set of down-
stream tasks. We first detail the encoders, datasets, down-
stream tasks, and the baselines used.
5.1. Vision and Language Encoders
The experimental setup covers vision encoders of differ-
ent architectures, such as ViTs [16] and ConvNeXt [29],
trained in various ways: supervised, language-supervised,
and self-supervised, across different training data regimes.
For the language encoder, an encoder capable of produc-
ing a global embedding for a caption is essential. This in-
cludes encoders of multiple architectures varying in size,
languages, and training data sizes. The Huggingface’s
sentence-transformers [43] library is utilized, where each
sentence transformer is first pre-trained on the masked lan-
guage modeling task using a large text corpus, followed by
a finetuning stage on a sentence pairs dataset with a con-
trastive loss. It’s not straightforward to acquire a global
sentence embedding from decoder-only models like GPT
models [7, 39], hence we did not study the semantic align-
ment of these class of models to vision encoders.
The CKA and Matching Score (MS) of the various com-
binations of vision and language encoders are reported in
supplementary. The findings indicate that the All-Roberta-large-v1 [28] demonstrates the best CKA/MS across all vi-
sion models, establishing it as the primary language encoder
for subsequent tasks, unless specified otherwise.
5.2. Baselines
Here, we briefly describe three baselines that we compare
our methods against for caption matching/retrieval, image
classification, and cross-lingual tasks.
Linear Regression: We propose a baseline that learns a lin-
ear transformation from the image embedding space to the
text using Maligned base examples and apply the transfor-
mation to the query image embeddings. Concretely, given
query image embeddings Zq= [zq
1,···,zq
N]∈Rd1×Nand
text embeddings Hq= [hq
1,···,hq
N]∈Rd2×N, and a set
of aligned base samples Zb= [zb
1,···,zb
N]∈Rd1×Mand
Hb= [hb
1,···,hb
N]∈Rd2×M, we first construct a linear
transformation between ZbandHbby minimizing the MSE
loss as W= arg minW∥W⊤Zb−Hb∥2
F. Then we use W
to transform the query image embeddings Zqto the text do-
main as ˆHq=W⊤Zq. Cosine similarity on ˆHqandHq
can be used to perform caption retrieval.
Relative Representations [34]: enable latent space com-
munication between unaligned encoders by representing
each query point relative to an aligned base set. Concretely,
letℓ2-normalized embeddings for image and text queries be
Zq= [zq
1,···,zq
N]∈Rd1×NandH= [hq
1,···,hq
N]∈
Rd2×N, respectively. Utilizing a set of aligned base sample
ℓ2-normalized embeddings Zb= [zb
1,···,zb
M]∈Rd1×M
andHb= [hb
1,···,hb
M]∈Rd2×M, we can construct
relative image and text query representations as Zq
rel=
(Zb)⊤ZqandHq
rel= (Hb)⊤Hq. Relative representations
are a single vector of dimension Mfor each query specify-
ing the cosine similarity of a query sample with all the base
samples. Now we can use the cosine similarity on the rela-
tive representations to perform retrieval. Sec D in appendix
provides a further comparison with our method.
CLIP [40]: We also compare against CLIP which has been
contrastively trained to obtain a joint embedding space- as
an upper limit on performance for both retrieval and match-
ing tasks. We perform retrieval using cosine similarity
For all 3 methods, caption matching can be achieved by
constructing a cost matrix using cosine similarities and us-
ing linear sum assignment to find the permutation matrix.
5.3. Downstream Tasks
Caption Matching: Given Nquery images and their cor-
responding captions, a query set is constructed by shuffling
the captions. The task involves finding the correct permu-
tation over captions for perfect matching. In Retrieval , the
objective is, given one caption, to retrieve the correct image
from the overall set of Nimages. The alignment between
unaligned vision and text encoders is investigated using our
methods on the COCO and NoCaps validation sets.
14337
Table 2. Caption matching and retrieval task performance comparison in cross-domain and in-domain settings. Base samples from
COCO are utilized for matching/retrieval tasks on queries from NoCaps (cross-domain) and COCO (in-domain). CLIP-V denotes the
vision encoder of CLIP [40]. We use the Large version of all vision encoders. Table A.5 shows the reverse setting.
Method Vision ModelNoCaps [2] COCO [27]
Matching accuracy Top-5 retrieval Matching accuracy Top-5 retrieval
Cosine Similarity* CLIP [40] 99.5 99.6 97.1 96.1
Linear regressionCLIP-V [40] 29.3 44.7 42.7 59.1
ConvNeXt [47] 19.0 28.5 31.3 46.1
DINOv2 [37] 38.1 50.3 45.1 65.4
RelativeCLIP-V [40] 61.3 37.6 61.6 41.3
representations [34]ConvNeXt [47] 25.5 17.8 38.6 34.1
DINOv2 [37] 46.0 46.4 47.7 52.3
Ours: QAPCLIP-V [40] 67.3 - 72.3 -
ConvNeXt [47] 46.7 - 66.1 -
DINOv2 [37] 57.7 - 66.0 -
Ours: Local CKACLIP-V [40] 65.1 60.5 71.9 69.9
ConvNeXt [47] 43.7 44.4 64.8 65.5
DINOv2 [37] 58.7 61.8 64.3 70.5
The COCO dataset [27] comprises over 120,000 images
with multiple captions per image. It is used for testing uni-
modal representation quality via a caption-matching task,
utilizing a validation set of 5,000 image-caption pairs. The
NoCaps dataset [2] is designed for testing image captioning
models on unseen objects, with 166,100 captions for 15,100
images from OpenImages. Its validation set includes novel
concepts absent from COCO.
Cross-lingual Caption Matching/Retrieval: The task mir-
rors prior matching and retrieval but uses multilingual cap-
tions, say German. Given Nimages and shuffled German
captions, the objective is to match each image with the cor-
rect caption. In retrieval, the goal is to select the most fitting
German caption for a given query image from the set.
The XTD-10 dataset [1] enhances COCO2014 with
1,000 human-annotated multi-lingual captions in ten lan-
guages for cross-lingual image retrieval and tagging, serv-
ing as a zero-shot model benchmark.
ImageNet-100 Classification. The task setup is similar
to the conventional classification task with small differences
to account for the methods used. Given Nquery images and
their corresponding classes, image representations are ob-
tained by processing them through a vision encoder. In par-
allel, textual representations are generated in a multi-step
process. Initially, several text captions are derived from the
class-associated Wordnet synsets’ lemmas, definitions, and
hypernyms. These captions are then passed through the lan-
guage encoder and averaged to get the text representations.
The classification task is performed by retrieving the closest
text representations to each image representation using our
local CKA metric. We employ the ImageNet-100 dataset.
This dataset is a subset of the larger ImageNet dataset, fea-turing only 100 classes. It includes 130,000 training images,
50,000 validation images, and 100 classes.
5.4. Results
Importance of Good Initialization: For all tasks, we make
use of a set of base samples of size Sthat is kept fixed at
320 samples. The size of the query set is analogously fixed
at 500 samples (see Sec ??for more details). These base
samples are selected after clustering the image embeddings
and choosing one closest sample to each of the Scluster
centers. By aligning the initial samples with the diverse
cluster centers, we ensure sufficient coverage of the sample
space. This enhances the accuracy of the matching process,
as the initial alignment closely mirrors the inherent struc-
ture and variability within the data. In the case of linear
regression, uniform sampling is employed to select the base
samples. For relative representations [34], the same cluster-
ing methodology is applied to select base samples, ensuring
a fair and consistent comparison between all methods.
COCO and NoCaps Caption Matching: We present
the results of cross-domain and in-domain caption match-
ing/retrieval, as detailed in Table 2. We tested each
baseline against three different vision models, while em-
ploying a consistent language model—specifically, the all-
roberta-large-v1. The vision models utilized are OpenAI’s
CLIP ViT-L/14, the ConvNeXT-Base model (trained on the
ImageNet-22k dataset at a resolution of 224x224), and the
ViT-L/14 model trained using the DINOv2 method. It is im-
portant to note that the first row of the results table features
vision and language models both being OpenAI’s CLIP
ViT-L/14. To effectively analyze cross-domain capabili-
ties, our experiment design involved the use of the COCO
14338
validation set as the source of the base set and the No-
Caps validation set for querying. Additionally, in-domain
results are shown, when using COCO validation for both
base and queries. We uniformly sample the query set and
average the results over three different seeds. Although
CLIP’s cosine similarity metric emerges as the most ro-
bust due to the training paradigm inherent in CLIP models,
our methods demonstrate commendable performance with-
out necessitating any training. The DINOv2 model, trained
solely through self-supervision, demonstrates the formation
of semantic concepts independently of language supervi-
sion. This is evident in its remarkable top-5 retrieval scores
of 70.5% and 61.8% on COCO and NoCaps datasets when
coupled with an unaligned language encoder through our
Local Kernel CKA method. However, the best-performing
vision encoder is CLIP’s vision encoder which has been
trained using language supervision.
ImageNet-100 Classification: In Table 3, we detail the per-
formance of our methods on the ImageNet-100 classifica-
tion task. Mirroring our approach in cross-domain matching
and retrieval, we evaluated three different vision models for
each method. Notably, the first row of the table highlights
the performance using CLIP’s embedding cosine similarity.
The results are averaged over three different seeds for sam-
pling the query set. A significant observation from this ta-
ble is the comparatively narrower performance gap between
the CLIP’s cosine similarity and our methods, as well as the
baseline linear regression method, in contrast to the results
observed in cross-domain caption matching/retrieval tasks.
It is interesting that ConvNeXt encoder trained on Im-
ageNet has a classification top1 accuracy improvement of
over 14% compared to CLIP and Dinov2 while on the cap-
tion matching task DinoV2 and CLIP perform much better.
Cross-lingual Caption Retrieval: The results of cross-
lingual caption matching/retrieval are presented in Table 4
for the 10 languages in the XTD-dataset. OpenAI CLIP’s
ViT-L vision encoder, trained on English image-caption
pairs, and a multilingual sentence transformer paraphrase-
multilingual-mpnet-base-v2 were utilized for this task. The
accuracy of CLIP’s cosine retrieval method exhibits a sig-
nificant drop when applied to languages other than English.
E.g., CLIP’s retrieval at 5 experiences a drop of 30 points
when switching from English to other Latin-alphabet lan-
guages (Spanish, French, German, and Italian). For non-
Latin alphabet languages such as Korean, Chinese, Turkish,
etc., CLIP’s performance decreases substantially, collaps-
ing to zero, primarily due to most words resulting in un-
known tokens. In contrast, the QAP and local CKA match-
ing methods demonstrate consistent performance across all
languages, including non-Latin languages, attributing to the
robustness of a multilingual sentence transformer trained
solely on text. On average, QAP surpasses CLIP by 12%
in the caption matching task and also outperforms otherTable 3. ImageNet-100 classification performance comparison.
We observe a narrow performance gap between the CLIP model
and our methods. CLIP-V denotes the vision encoder of CLIP.
Method Vision Model Top 1 Top 5
Cosine Similarity* CLIP 86.1 99.2
Linear RegressionCLIP-V 76.1 93.0
ConvNeXt 84.5 95.4
DINOv2 73.5 92.1
RelativeCLIP-V 8.90 30.3
representations [34]ConvNeXt 7.20 15.7
DINOv2 49.7 75.5
Local CKACLIP-V 68.7 91.2
ConvNeXt 83.3 95.8
DINOv2 67.7 88.3
baselines like relative representations and linear regression
methods. For retrieval at 5, the local CKA-based method
exceeds CLIP’s performance by over 17%.
It is possible to push the performance further by using
language-specific sentence encoders and we report these re-
sults for a few languages in Sec ??of supplementary. This
is a practical application of our method as we can now turn
a well-trained English CLIP model’s vision encoder into a
CLIP model for any low-resource language if a text-only
Sentence Transformer trained on that language is available.
5.5. Matching complexity
In Table 5, we go over the time complexity and runtimes
of QAP matching and local CKA based retrieval in com-
parison to the other baselines for matching when number of
base samples and query samples are 320, 500 respectively.
For all time complexities, we assume number of base sam-
ples m to be of the order of the number of query samples
n. QAP uses the seeded version of the fast QAP algorithm
from the SciPy library, which has a worst time complexity
ofO(n3)[17], while local CKA retrieval requires construct-
ing a graph over all the query image and text pairs, O(n2),
using local CKA, which is also O(n2)resulting in O(n4).
Relative involves the calculation of the relative representa-
tions for every query image and text pair, resulting in a time
complexity of O(n2), but it’s fast due to highly optimized
algorithms for matrix multiplications in PyTorch [38]. Lin-
ear has a time complexity of O(nd), where nis the number
of samples and dis the number of dimensions. It is to be
noted that QAP runs on the CPU, and a CUDA-optimized
version could bring the runtimes further down from 40 sec-
onds. An efficient implementation of Local Kernel CKA
is also possible, where the CKA of base samples is precal-
culated, and the graph is constructed in an additive manner,
which would bring down the time complexity to O(n3). For
both relative and linear matching, we make use of SciPy’s
14339
Table 4. Cross-Lingual caption matching and retrieval performance comparison. Using QAP and local CKA-based methods we are
able to do cross-lingual caption matching/retrieval using CLIP’s ViT-L vision encoder and a multi-lingual sentence transformer paraphrase-
multilingual-mpnet-base-v2. While CLIP performs well on the Latin languages, it degrades on non-Latin languages. In comparison, our
QAP and Local-CKA-based methods perform comparably in Latin languages while outperforming non-Latin languages, highlighting the
efficacy of our training-free transfer approach. See Table A.6 and Table A.7 in appendix for additional results.
LanguageKernel CKA Matching Accuracy Retrieval @ 5
CLIP Ours CLIP Relative[34] Linear Ours (QAP) CLIP Ours (Local)
Latinde 0.472 0.627 41.8 35.0 34.0 39.6 65.1 56.7
en 0.567 0.646 81.5 52.5 40.9 51.6 92.5 69.0
es 0.471 0.634 50.2 37.8 31.7 41.4 68.5 61.6
fr 0.477 0.624 49.4 37.5 30.7 40.2 68.7 57.6
it 0.472 0.638 41.0 37.2 34.9 38.5 61.3 59.7
Non-Latinjp 0.337 0.598 13.2 28.3 23.5 30.5 30.0 49.4
ko 0.154 0.620 0.50 30.4 23.5 30.9 3.30 53.4
pl 0.261 0.642 5.40 36.6 30.2 40.2 18.8 59.5
ru 0.077 0.632 0.80 31.9 30.7 35.1 4.10 53.2
tr 0.301 0.624 4.30 35.8 29.6 38.9 15.2 59.3
zh 0.133 0.641 2.70 36.5 31.1 40.3 8.90 57.8
Avg. – – 26.4 36.3 30.9 38.8 39.6 57.9
Figure 2. Kernel CKA and QAP Matching accuracy are correlated with the training set size and quality of the training set. Here
the language encoder is kept constant to the best BERT-sentence encoder (i.e.All-Roberta-large-v1). There is a clear correlation between
CKA and QAP Matching accuracy across all architectures, training paradigm and data regimes.
Table 5. Run times for different methods
Method QAP Local CKA Relative Linear
Run times 40 seconds 5 mins 1 second 1 second
Complexity O(n3) O(n4) O(n2)O(n×d)
modified Jonker-V olgenant algorithm [13] for linear sum
assignment, which has the worst time complexity of O(n3).6. Analysis
This section focuses on how training paradigms, data
regimes, and encoder size/architecture influence a vision
encoder’s ability to represent the world similarly to a lan-
guage encoder. This is assessed by comparing the seman-
tic alignment of their representation spaces using CKA as
well as QAP matching accuracy. Figure 2 compares the ker-
nel CKA and caption matching accuracy of different vision
14340
encoders with a fixed text-encoder ( i.e., All-Roberta-large-
v1), against the training datasets on which the vision en-
coder was trained for all pairs in the COCO captions vali-
dation set. The findings are summarized below:
Scale and quality of dataset results in encoders with high
semantic alignment with the language space: It is ob-
served that SSL methods like DINOv2 can learn semantic
concepts in a relative manner even without language super-
vision during training. The CKA and QAP matching accu-
racy for DINOv2 embeddings are comparable to CLIP mod-
els, despite lacking language supervision and having sig-
nificantly less data (LVD-142’s 142M vs Open-AI-CLIP’s
400M). A general trend emerges where more training data
leads to semantically richer visual embeddings, evident
when comparing CKA and QAP Accuracies from Ima-
geNet1K to DFN-5B datasets. Notably, training on a cu-
rated dataset proves more effective than on an uncurated
dataset of the same size, especially for smaller models. This
is illustrated by the higher CKA and QAP accuracy of ViT-
Large trained on the curated DFN-2B dataset compared to
ViT-Large/Giant, and ConvNext-xxLarge trained on Laion
2B. Additionally, SSL methods show less semantic con-
sistency when trained on ImageNet1K, as indicated by the
clear difference in QAP accuracies between DINO trained
on ImageNet1K and DINOv2 trained on LVD-142M.
Vision Encoders Trained with Language Supervision
Exhibit Greater Semantic Alignment with Language
Encoders: In line with the findings of Merullo et al.[31], it
is observed in our experiments that vision encoders trained
with more language supervision on datasets of comparable
size exhibit a higher degree of semantic alignment with lan-
guage encoders compared to self-supervised methods. For
example, ViT-Large trained on CLIP-400M with language
supervision demonstrates superior caption-matching capa-
bilities compared to DINOv2’s ViT-Large trained on LVD-
142M. Similarly, we verify that class label supervision, like
that from ImageNet, leads to more semantically aligned
image encoders when compared to self-supervision when
similarly sized models are compared on ImageNet-1k. For
example, all supervised encoders trained on ImageNet-1k
have higher CKA as well as QAP matching accuracy than
all the self-supervised models.
7. Ablations
This section rationalizes our method choices through abla-
tion studies on clustering, stretching, and the global CKA
metric. We demonstrate the impact of these components on
the performance of our methods, primarily through Table 6,
which delineates the effectiveness of the QAP and the lo-
cal CKA metric under various configurations. It shows the
performance metrics in scenarios where each main compo-
nent is either integrated or omitted. Notably, in instances
where the CKA metric is not used, we opt for normalizedTable 6. Impact of clustering and stretching. The matching and
retrieval performance is the best when both clustering and stretch-
ing are employed. Hence, justifying this choice.
Clustering Stretching CKAQAP Local CKA Local CKA
Matching Matching Retrieval @ 5
✗ ✗ ✗ 10.1 16.2 1.0
✗ ✗ ✓ 48.8 48.5 60.2
✗ ✓ ✓ 57.3 56.7 73.0
✓ ✗ ✓ 56.2 55.1 66.4
✓ ✓ ✓ 65.5 63.3 77.2
correlation matrices for each graph. The empirical results
presented are derived from the caption matching/retrieval
task, utilizing both base and query sets extracted from the
COCO validation set of size 320 and 500 respectively.
Choice of the metric: CKA is more beneficial than using
just the scaled correlation matrix to represent the semantic
relationships in an embedding space as matching accuracy
increases from 10.1% to 48.8%. The choice of a robust met-
ric is core to aligning vision and language latent spaces.
Impact of Stretching: It is clear that stretching facilitates
better alignment of embeddings in our methods as stretch-
ing spreads the representations out in each modality without
sacrificing the relative positions of the different embeddings
within each embedding space. This is reflected in the in-
crease of QAP accuracy from 48.8% to 57.3%.
Clustering vs.Uniform Sampling: The choice of the base
set is important in QAP matching and local CKA retrieval,
as it measures any query pair alignment with the base set.
A diverse base set is essential to capture a broad semantic
range, and clustering within one of the embedding spaces
aids in achieving this diversity. The third and fifth rows
of the table demonstrate that clustering enhances the QAP
performance from 57.3% to 65.5%. Consequently, these re-
sults highlight that all the components together significantly
enhance the efficacy of our proposed approach.
8. Conclusion
In this work, we ask the question, ‘Do vision encoders
and language encoders represent the world similarly?’ and
study this using CKA and a caption-matching task. We
find that well-trained vision encoders on sufficiently large
datasets exhibit surprisingly high semantic similarity with
language encoders comparable to aligned encoders, irre-
spective of the training paradigm. Inspired by this, we draw
parallels between CKA and the QAP matching objective
and use seeded graph matching to align vision and language
encoders by maximizing CKA. We also devise a local CKA-
based metric to enable retrieval between unaligned vision
and language encoders demonstrating a better performance
than that of relative representations on cross-domain and
cross-lingual caption matching/retrieval tasks, facilitating
zero-shot latent space communication between unaligned
encoders.
14341
References
[1] Pranav Aggarwal and Ajinkya Kale. Towards zero-
shot cross-lingual image retrieval. arXiv preprint
arXiv:2012.05107 , 2020. 5
[2] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,
Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Ste-
fan Lee, and Peter Anderson. Nocaps: Novel object caption-
ing at scale. In Proceedings of the IEEE/CVF international
conference on computer vision , pages 8948–8957, 2019. 2,
5
[3] Richard Antonello, Javier S Turek, Vy V o, and Alexander
Huth. Low-dimensional structure in the space of language
representations is reflected in brain responses. Advances in
neural information processing systems , 2021. 2
[4] Yamini Bansal, Preetum Nakkiran, and Boaz Barak. Revis-
iting model stitching to compare neural representations. Ad-
vances in neural information processing systems , 2021. 2
[5] Serguei Barannikov, Ilya Trofimov, Nikita Balabin, and
Evgeny Burnaev. Representation topology divergence: A
method for comparing neural network representations. arXiv
preprint arXiv:2201.00058 , 2021. 2
[6] Lisa Bonheme and Marek Grzes. How do variational autoen-
coders learn? insights from representational similarity. arXiv
preprint arXiv:2205.08399 , 2022. 2
[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in neural in-
formation processing systems , 33:1877–1901, 2020. 4
[8] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In Pro-
ceedings of the IEEE/CVF international conference on com-
puter vision , pages 9650–9660, 2021. 1
[9] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
Soricut. Conceptual 12m: Pushing web-scale image-text pre-
training to recognize long-tail visual concepts. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 3558–3568, 2021. 1
[10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In International conference on ma-
chine learning , pages 1597–1607. PMLR, 2020. 1
[11] Alexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato,
Ludovic Denoyer, and Herv ´e J´egou. Word translation with-
out parallel data. arXiv preprint arXiv:1710.04087 , 2017.
2
[12] Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh.
Algorithms for learning kernels based on centered alignment.
The Journal of Machine Learning Research , 13(1):795–828,
2012. 1
[13] David F Crouse. On implementing 2d rectangular assign-
ment algorithms. IEEE Transactions on Aerospace and Elec-
tronic Systems , 52(4):1679–1696, 2016. 7
[14] Adri ´an Csisz ´arik, P ´eter Kor ¨osi-Szab ´o, Akos K Matszan-
gosz, Gergely Papp, and D ´aniel Varga. Similarity andmatching of neural network representations. arXiv preprint
arXiv:2110.14633 , 2021. 2
[15] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei.
Imagenet: A large-scale hierarchical image database. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , 2009. 2
[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 1, 4
[17] Donniell E Fishkind, Sancar Adali, Heather G Patsolic,
Lingyao Meng, Digvijay Singh, Vince Lyzinski, and Carey E
Priebe. Seeded graph matching. Pattern recognition , 87:
203–215, 2019. 3, 6
[18] Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard
Sch¨olkopf. Measuring statistical dependence with hilbert-
schmidt norms. In International conference on algorithmic
learning theory , pages 63–77. Springer, 2005. 2
[19] Michael Gutmann and Aapo Hyv ¨arinen. Noise-contrastive
estimation: A new estimation principle for unnormalized
statistical models. In Proceedings of the Thirteenth Inter-
national Conference on Artificial Intelligence and Statistics ,
pages 297–304. JMLR Workshop and Conference Proceed-
ings, 2010. 1
[20] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representa-
tion learning with noisy text supervision. In International
conference on machine learning , pages 4904–4916. PMLR,
2021. 1
[21] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. ICLR , 2014. 2
[22] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and
Geoffrey Hinton. Similarity of neural network represen-
tations revisited. In International conference on machine
learning , pages 3519–3529. PMLR, 2019. 1, 2
[23] Harold W Kuhn. The hungarian method for the assignment
problem. Naval research logistics quarterly , 2(1-2):83–97,
1955. 4
[24] Karel Lenc and Andrea Vedaldi. Understanding image repre-
sentations by measuring their equivariance and equivalence.
InProceedings of the IEEE conference on computer vision
and pattern recognition , pages 991–999, 2015. 2
[25] Brian Lester, Rami Al-Rfou, and Noah Constant. The power
of scale for parameter-efficient prompt tuning. arXiv preprint
arXiv:2104.08691 , 2021. 2
[26] Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and
John Hopcroft. Convergent learning: Do different neural
networks learn the same representations? arXiv preprint
arXiv:1511.07543 , 2015. 2
[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
ECCV , 2014. 1, 3, 5
14342
[28] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar
Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-
moyer, and Veselin Stoyanov. Roberta: A robustly optimized
bert pretraining approach. arXiv preprint arXiv:1907.11692 ,
2019. 1, 3, 4
[29] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A convnet for the
2020s. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition , pages 11976–11986,
2022. 4
[30] Wan-Duo Kurt Ma, JP Lewis, and W Bastiaan Kleijn. The
hsic bottleneck: Deep learning without back-propagation.
InProceedings of the AAAI conference on artificial intelli-
gence , pages 5085–5092, 2020. 2
[31] Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie
Pavlick. Linearly mapping from image to text space. arXiv
preprint arXiv:2209.15162 , 2022. 1, 2, 8
[32] Tomas Mikolov, Quoc V Le, and Ilya Sutskever. Exploiting
similarities among languages for machine translation (2013).
arXiv preprint arXiv:1309.4168 , 2022. 2
[33] Ari Morcos, Maithra Raghu, and Samy Bengio. Insights on
representational similarity in neural networks with canoni-
cal correlation. Advances in neural information processing
systems , 31, 2018. 2
[34] Luca Moschella, Valentino Maiorca, Marco Fumero, An-
tonio Norelli, Francesco Locatello, and Emanuele Rodol `a.
Relative representations enable zero-shot latent space com-
munication. In The Eleventh International Conference on
Learning Representations , 2022. 1, 2, 4, 5, 6, 7
[35] Antonio Norelli, Marco Fumero, Valentino Maiorca, Luca
Moschella, Emanuele Rodola, and Francesco Locatello.
Asif: Coupled data turns unimodal models to multimodal
without training. arXiv preprint arXiv:2210.01738 , 2022.
1, 2
[36] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-
sentation learning with contrastive predictive coding. arXiv
preprint arXiv:1807.03748 , 2018. 1
[37] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy
V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.
Dinov2: Learning robust visual features without supervision.
arXiv preprint arXiv:2304.07193 , 2023. 1, 3, 5
[38] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-
perative style, high-performance deep learning library. Ad-
vances in neural information processing systems , 32, 2019.
6
[39] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, Ilya Sutskever, et al. Language models are unsu-
pervised multitask learners. OpenAI blog , 1(8):9, 2019. 4
[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 1, 4, 5[41] Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha
Sohl-Dickstein. Svcca: Singular vector canonical correlation
analysis for deep learning dynamics and interpretability. Ad-
vances in neural information processing systems , 30, 2017.
2
[42] Maithra Raghu, Thomas Unterthiner, Simon Kornblith,
Chiyuan Zhang, and Alexey Dosovitskiy. Do vision trans-
formers see like convolutional neural networks? Advances
in Neural Information Processing Systems , 34:12116–12128,
2021. 1
[43] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence
embeddings using siamese bert-networks. In Proceedings of
the 2019 Conference on Empirical Methods in Natural Lan-
guage Processing . Association for Computational Linguis-
tics, 2019. 4
[44] Anton Tsitsulin, Marina Munkhoeva, Davide Mottin, Pana-
giotis Karras, Alex Bronstein, Ivan Oseledets, and Em-
manuel M ¨uller. The shape of data: Intrinsic distance for data
distributions. arXiv preprint arXiv:1905.11141 , 2019. 2
[45] Joshua T V ogelstein, John M Conroy, Vince Lyzinski,
Louis J Podrazik, Steven G Kratzer, Eric T Harley, Don-
niell E Fishkind, R Jacob V ogelstein, and Carey E Priebe.
Fast approximate quadratic programming for graph match-
ing. PLOS one , 10(4):e0121002, 2015. 3
[46] Ivan Vuli ´c, Sebastian Ruder, and Anders Søgaard. Are
all good word vector spaces isomorphic? arXiv preprint
arXiv:2004.04070 , 2020. 2
[47] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei
Chen, Zhuang Liu, In So Kweon, and Saining Xie. Con-
vnext v2: Co-designing and scaling convnets with masked
autoencoders. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 16133–
16142, 2023. 1, 5
[48] John M Wu, Yonatan Belinkov, Hassan Sajjad, Nadir Dur-
rani, Fahim Dalvi, and James Glass. Similarity analysis
of contextual word representation models. arXiv preprint
arXiv:2005.01172 , 2020. 2
14343
