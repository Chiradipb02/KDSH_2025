Weakly-Supervised Audio-Visual Video Parsing with Prototype-based
Pseudo-Labeling
Kranthi Kumar Rachavarapu
IIT Madras
kranthi.rachavarapu@gmail.comKalyan Ramakrishnan
University of Oxford
kalyanr@robots.ox.ac.ukRajagopalan A. N.
IIT Madras
raju@ee.iitm.ac.in
Abstract
In this paper, we address the weakly-supervised Audio-
Visual Video Parsing (AVVP) problem, which aims at la-
beling events in a video as audible, visible, or both, and
temporally localizing and classifying them into known cat-
egories. This is challenging since we only have access to
video-level (weak) event labels when training but need to
predict event labels at the segment (frame) level at test time.
Recent methods employ multiple-instance learning (MIL)
techniques that tend to focus solely on the most discrimina-
tive segments, resulting in frequent misclassifications. Our
idea is to first construct several “prototype” features for
each event class by clustering key segments identified for
the event in the training data. We then assign pseudo labels
to all training segments based on their feature similarities
with these prototypes and re-train the model under weak
and strong supervision. We facilitate this by structuring the
feature space with contrastive learning using pseudo labels.
Experiments show that we outperform existing methods for
weakly-supervised AVVP . We also show that learning with
weak and iteratively re-estimated pseudo labels can be in-
terpreted as an expectation-maximization (EM) algorithm,
providing further insight for our training procedure.
1. Introduction
In this paper, we explore the problem of weakly-supervised
Audio-Visual Video Parsing (A VVP) [48], where the goal
is to classify events in a video and localize them over time
andmodalities, given only video-level ( weak ) event labels
for training. Such a formulation is attractive since it forgoes
the need for expensive and tedious fine-grained labeling.
Moreover, other weakly-supervised video-based temporal
prediction tasks can be posed as special cases of A VVP.
For instance, Audio-Visual Event Localization (A VEL) [47]
considers events that are simultaneously audible and visible,
whereas Temporal Action Localization (TAL) [10] consid-
ers only visible events. A VVP is challenging as an event
GT
MIL
Guitar
Guitar Guitar
Singing
SingingGT
MIL
Visual frames are similar but predicted labels are dif ferent
Visual frames are dif ferent but predicted labels are sameOursGuitar Ours1 Sec 2 Sec 3 Sec 4 Sec 5 Sec
SingingFigure 1. Example predictions of a model trained using MIL under
weak supervision [48] and our method. For clarity, we only show
visual events. The MIL model detects the event “Guitar” correctly
in the beginning, but there are some mistakes (red dotted boxes)
later on. Moreover, although the last three frames are similar, they
are labeled inconsistently by the model. Our method consistently
localizes well. “GT” is the ground truth.
could occur in just the audio stream, visual stream, or both
at the same time. Audio-visual data is ubiquitous, with
highly variable events in terms of appearance and duration.
Recent methods for weakly-supervised A VVP [30, 48]
use multiple-instance learning (MIL) [7] techniques that
optimize for video-level labels during training and make
frame-level predictions during evaluation. Here, video-
level predictions are obtained by pooling (aggregating) the
frame-level predictions. A drawback of MIL approaches is
that a model may learn to focus solely on the most discrim-
inative (“prominent”) instances (here, audio/visual frames
in a video) and still minimize the training loss successfully.
This means such models may not reliably detect the full
temporal extent of events. Fig. 1 shows an example where
an MIL model [48] for A VVP fails to localize events accu-
rately. Moreover, the model labels frames that are visually
similar differently. Another reason for misclassifications in
MIL models is that they typically contain a frame-level clas-
sifier with a single weight vector per event class. However,
this may be insufficient to capture the intra-class variation
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
18952
for the event across time, modalities, and examples while
training with weak labels.
We propose to address these issues with the following
ideas. First, given that the MIL model can find discrim-
inative frames that trigger the video-level labels, we can
identify such frames and propagate their predicted labels
to other frames in the training data based on similarities in
the feature space, thus creating frame-level pseudo labels on
the training data. Second, we can use their features to con-
struct multiple prototypes per event class, ensuring a more
robust representation to propagate pseudo labels with. Once
this is done for the training data, we can re-train the model
with strong supervision from pseudo labels and weak su-
pervision from video-level labels. Fig. 1 shows an example
where our method produces more accurate predictions than
the MIL model. Because our method relies on feature simi-
larities, we further enforce structure in the feature space via
contrastive learning using pseudo labels.
We re-estimate pseudo labels on the training data af-
ter every epoch and train with them over the next epoch.
We show that such a training procedure can be viewed
through an expectation-maximization (EM) lens with the
unknown frame-level labels as latent variables. This inter-
pretation provides convergence guarantees and further in-
sight. Our method achieves state-of-the-art performance
for weakly-supervised A VVP and also works well on the
related weakly-supervised Temporal Action Localization
(TAL) task. We summarize our contributions below:
1. We introduce a prototype-based pseudo-labeling method
for weakly-supervised A VVP that exploits similarities in
feature space.
2. We show how to perform contrastive learning using
pseudo labels to help semantically structure the feature
space.
3. We present an expectation-maximization view of our
general training procedure.
4. We show state-of-the-art results for weakly-supervised
A VVP, and on several metrics for weakly-supervised
TAL.
2. Related Work
Audio-Visual Video Parsing (A VVP). Tian et al. [48] for-
mulated A VVP as a multimodal multiple-instance learning
(MMIL) problem and proposed a hybrid attention network
with a learned pooling function ( HAN) to capture unimodal
and cross-modal contexts. Most subsequent work has used
the HAN architecture with other techniques to improve per-
formance. Lamba et al. [30] utilized cross-modal informa-
tion to learn better representations with adversarial and self-
supervised losses. Wu et al. [52] estimated modality-level
labels by exchanging the audio/visual streams between pairs
of videos, followed by retraining under stronger supervi-
sion. Lin et al. [32] used cross-video and cross-modalitysignals, such as event co-occurrence across modalities, as
additional sources of supervision. Mo et al. [36] leveraged
multi-modal grouping to learn discriminative subspaces.
Gao et al. [11] extract the “presence” and “absence” ev-
idence for events from uni- and cross-modal information.
While Fu et al. [9] focused on learning rate imbalance be-
tween modalities and Zhou et al. [60] utilized CLIP [41]
to get segment-labels, our work leverages in-model feature
similarities for pseudo labeling. PoiBin [40] modeled the
number of positive frames for an event as a latent variable
to improve localization. Different from previous methods,
we estimate segment-level pseudo labels and re-train under
full supervision.
Pseudo-Labeling refers to estimating labels for unlabeled
data using the predictions of a trained model. This has been
used to improve performance on several weakly-supervised
tasks including object detection [1, 46, 59] and image clas-
sification [2, 13, 21]. A few works [34, 39, 58] have em-
ployed pseudo-labeling to improve performance on Tem-
poral Action Localization (TAL), generating labels from
model outputs or attention weights. Luo et al. [34] posed
the TAL task as expectation-maximization, using pseudo-
labels to approximate the E and M steps. Contrary to these,
we propose a pseudo-labeling strategy using feature simi-
larities between segments guided by the model predictions
and weak labels for a video.
Prototype-based Classification. Prototype learning with
a nearest-neighbor (non-parametric) classifier has inspired
various machine learning techniques [5, 12, 15, 43, 44].
Recently, there has been increased interest in integrating
this into deep learning methods, e.g., zero-shot [25], few-
shot [45], unsupervised [54, 55], and supervised [16, 35, 53,
56] learning. Prototype learning has also been widely used
for image segmentation [8, 50, 51, 61]. Recently, prototype-
based methods have also been used for semi-supervised im-
age [37] and text [57] classification. A few works have used
prototype-based methods with multiple-instance learning.
E.g., Rymarczyk et al. [42] processed large medical images
as a bag of patches, and Huang et al. [22] learned prototypes
for the TAL task with graph convolutional networks. These
works adopt an embedding-level MIL approach ([24]) with
one prototype per class, while we adopt instance-level MIL
and construct multiple prototypes per event class.
3. Background
3.1. Problem Definition
In the audio-visual video parsing (A VVP) problem, we aim
to identify audible or visible events in an unconstrained
video and simultaneously localize them in time. Specifi-
cally, given a video xconsisting Tnon-overlapping tem-
poral segments {xt= (xa
t, xv
t)}T
t=1of the audio ( A) and
visual ( V) streams, our objective is to classify each segment
18953
intoCpossible events (e.g., singing, vehicle moving, etc.).
At test time, we need to predict segment-level event labels
yt= (ya
t,yv
t)for each audio and visual segment, where
ya
t,yv
t∈ {0,1}Care segment-level audio and visual event
labels, respectively.
Weak Supervision. In weakly-supervised A VVP, for each
video x, we only have access to the corresponding video-
level event label vector w= (w1, . . . , w C)∈ {0,1}C,
where wc= 1 ifanyof the segments in the video con-
tains the c-th event, and wc= 0 otherwise. Such weak,
video-level labels only indicate whether an event occurs in
the video or not. During evaluation, we still need to pre-
dictsegment-level labels{(ya
t,yv
t)}T
t=1. Note that multiple
events can simultaneously occur in a video, i.e.,P
cwc≥
1, and each segment could have more than one event.
3.2. MIL Framework
We adopt an instance-level MIL approach [24] as done by
prior work [30, 32, 36, 48, 52]: First, each instance (here,
an audio/visual segment) is assigned a classification score
(here, a C-dimensional vector). Then, the scores are aggre-
gated by a pooling operation, and the aggregate is used in
the loss. In our case, this looks like

{fa
t}T
t=1,{fv
t}T
t=1
=fθ(x) (1)
pm
t=gϕ(fm
t)∈[0,1]C,∀t∈[1, T], m∈ {a, v}(2)
ˆ w=σθ,x 
{pa
t}T
t=1,{pv
t}T
t=1
∈[0,1]C, (3)
where fθis a feature extractor (encoder), gϕis a fea-
ture classifier, σθ,xis a MIL-pooling operator. The pre-
dicted video-level probability vector ˆ wis computed from
segment-level classification scores {pm
t}, which are com-
puted from segment features {fm
t}. We adopt the Hybrid
Attention Network (HAN) [48] architecture to implement
fθ,gϕ, and σ. Here, fθconsists of a transformer-based en-
coder with cross-modal attention and gϕis a linear classifier
with the sigmoid activation. Attention-based averaging is
used for σ. We provide implementation details in the Sup-
plementary.
This model is trained end-to-end with supervision at the
video level by minimizing a weighted cross-entropy loss
LMIL=CE(ˆ w,w) (4)
with weights inversely proportional to the event distribution
in the dataset. At test time, we get segment-level predictions
from Eq. (2).
4. Prototype-based Pseudo-Labeling
While instance-level MIL enables finding key segments that
trigger the video-level labels [24, 33], they often miss out
on the full temporal extent of events in a video. However,
if we identify such discriminative segments for each labelin a training video, we could use them to build a set of pro-
totype features that represent each class. A high degree of
similarity of a segment with the prototype features for an
event class means the segment probably contains the event.
We use this idea to generate pseudo labels for all segments
in a training video and then re-train the model under strong
supervision. Fig. 2 shows an overview of our approach. We
provide pseudocode in Algorithm 1.
4.1. Constructing Prototype Features
The segment-level linear classifier gϕ(Eq. (2)) can itself
be considered a prototype-based classifier, where the proto-
type for each class is the corresponding weight vector in ϕ.
However, this requires that an event have similar represen-
tations across modalities and examples in the training data.
We instead construct multiple prototypes for each class in
the following two steps.
Segment Selection. For each training video x, we find pos-
itive segments Px(c)for a target event c(ifwc= 1) as those
the model is highly confident contain the event. I.e.,
Px(c) =(
{xm
t∈x|pm
t(c)≥γp∗(c)}ifwc= 1
ϕ ifwc= 0,(5)
where γ∈(0,1]is a hyperparameter and p∗(c) =
max t,mpm
t(c)is the maximum predicted probability for
the event over time and modalities for a given video.
Clearly, PX(c)will contain at least one audio/visual seg-
ment from video xwhen wc= 1. We assume these seg-
ments contain event cwith high probability (so are likely
true positives). We also find negative segments Nx(c)for a
target event cas those the model is highly confident do not
contain the event when wc= 1(likely true negatives), or is
highly confident do contain the event when wc= 0(definite
false positives):
Nx(c) =(
{xm
t∈x|pm
t(c)≤βp∗(c)} ifwc= 1
{xm
t∈x|pm
t(c) =p∗(c)>= 0.5}ifwc= 0,
(6)
where β∈(0,1)is a hyperparameter set to a small value
to retrieve segments without the event c(if any). Note that
Nx(c)may contain no segments at all even when wc= 1,
i.e., be a null set.
Finally, we aggregate the positive and negative sets over
all videos in training data Dto get prototypical segments as
P(c) =[
x∈DPx(c),N(c) =[
x∈DNx(c). (7)
Prototype Generation. We now generate multiple proto-
types (feature vectors) for each event class by hypothesizing
that having more than one prototype would help better cap-
ture the intra-class variation across modalities and training
examples, and create a more robust representation.
18954
Visual Frames
Audio
EncoderClassifierBCE Loss
EncoderClassifier
Momentum EncoderHybrid Attention Network
Prototype-based Pseudo-LabelingPrototype
Selection
Prototype
GenerationMMIL  Pooling
Estimate
Pseudo-LabelsPseudo-LabelsBCE LossVideo LabelFigure 2. An overview of our method. The top branch is trained using MIL with weak labels andusing full supervision with pseudo labels,
which are generated by the bottom branch. This is done by first selecting key segments for each event from the training data and clustering
them to construct a set of prototype features for the event, used to guide pseudo-label generation.
First, we extract features for all segments in P(c)and
N(c)using Eq. (1), and call the resulting feature sets
FP(c)andFN(c). Here, instead of fθ, we use a mo-
mentum encoder [19] f˜θhaving the same architecture but
whose weights are set (after each update to θ) as an ex-
ponential moving average (EMA) of θover training steps:
˜θ←(1−η)˜θ+ηθ, where η∈(0,1). The momentum en-
coder is more stable and less sensitive to noise from weak
supervision, leading to slightly improved performance.
Next, we perform k-means clustering separately on
FP(c)andFN(c)to get two sets of cluster centroids. We
call the centroids prototypes :
CP(c) ={f+
i}kp
i=1,CN(c) ={f−
j}kn
j=1. (8)
Clustering for finding prototypes imposes a natural bottle-
neck [26], helping discard irrelevant information in the seg-
ments. We repeat this for each event class.
4.2. Estimating Pseudo Labels
We assign pseudo labels to each segment in a training video
by considering its feature similarities with the prototypes
for each class. We assume that if an audio/visual segment
contains event c, it would be on average closer to P(c)in
feature space than to N(c). For this, we explore two ap-
proaches: (i) hard labeling , which assigns the label of the
closest-on-average set among the prototype sets for class c:
ym
t(c) =wc· 1
X
f+∈CP(c)e(f+)Tfm
t
|CP|>X
f−∈CN(c)e(f−)Tfm
t
|CN|
,(9)and (ii) soft labeling , which instead assigns a probability
score:
ym
t(c) =wc·1
|CP|P
f+∈CP(c)e(f+)Tfm
t/τ
P
S∈{CP,CN}1
|S|P
f′∈S(c)e(f′)Tfm
t/τ∈(0,1),
(10)
where τ >0is a temperature hyperparameter.
When training our model, we generate pseudo labels
{ym
t}after every epoch for each training video and use
them to fully supervise the segment-level probabilities
{pm
t}in Eq. (2) using the binary cross-entropy loss
LPL=CE(pm
t,ym
t), (11)
averaged over segments and training examples.
4.3. Contrastive Learning with Pseudo Labels
Given that our method exploits feature similarities between
segments based on the events they may contain, we use con-
trastive learning based on pseudo labels to help semantically
structure the feature space. Since multiple events can occur
simultaneously, for each audio/visual segment, we consider
asimilar segment as one estimated to have some common
event(s) and a dissimilar segment as one estimated to have
no common events. Formally, given a batch of training ex-
amples, let B={fm
t→ym
t}be a dictionary from features
to the assigned pseudo labels for all segments and over all
videos in the batch. For every feature f∈B, the similar
feature-set is then S(f) ={g∈B|B(g)∩B(f)̸=ϕ}and
the dissimilar set is D(f) ={g∈B|B(g)∩B(f) =ϕ}.
To pull segments with a common predicted event closer in
the feature space, we use the contrastive loss [38]
LCL=−logP
g∈S(f)egTf/τc
P
g∈S(f)egTf/τc+P
h∈D(f)ehTf/τc, (12)
18955
MethodAudio Visual Audio-Visual Type@A V Event@A V
Seg. Event Seg. Event Seg. Event Seg. Event Seg. Event
AVE* [47] 47.2 40.4 37.1 34.7 35.4 31.6 39.9 35.5 41.6 36.5
AVSDN * [31] 47.8 34.1 52.0 46.3 37.1 26.5 45.7 35.6 50.8 37.7
HAN [48] 60.1 51.3 52.9 48.9 48.9 43.0 54.0 47.7 55.4 48.0
MA[52] 60.3 53.6 60.0 56.4 55.1 49.0 58.9 53.0 57.9 50.6
CVCM-MA [32] 60.8 53.8 63.5 58.9 57.0 49.5 60.5 54.0 59.5 52.1
MGN-MA [36] 60.2 50.9 61.9 59.7 55.5 49.6 59.2 53.4 58.7 49.9
JoMoLD [4] 61.3 53.9 63.8 59.9 57.2 49.6 60.8 54.5 59.9 52.5
PoiBin [40] 63.1 54.1 63.5 60.3 57.7 51.5 61.4 55.2 60.6 52.3
CMPAE [11] 64.2 56.6 66.4 63.7 59.2 51.8 63.3 57.4 62.8 55.7
Ours 65.9 57.3 66.7 64.3 61.9 54.3 64.8 59.9 63.7 57.9
Table 1. Results ( %F1-scores) on all metrics defined in Tian et al. [48]. ‘*’ indicates reproduced performance. The best and second best
results are in bold and underline , respectively.
Algorithm 1: Pseudocode for our method
1Init: model params ψ={θ, ϕ}, EMA params
ψE={θE, ϕE}
2while ψhas not converged do
3 #estimate segment −level pseudo labels
4 for(x,w)in train set do
5 {ym
t} ←pθ(·|w, x);
6 end
7 #train
8 for(x,w)in train set do
9 {pm
t} ←pθ,ϕ(·|x);
10 ˆ w=σθ,x({pm
t});
11 L=LMIL+λPLLPL+λCLLCL;
12 ψ←ψ−α∇ψL;
13 ψE←(1−η)ψE+ηψ;
14 end
15end
averaged over all f∈B, where τc>0is a temperature
hyperparameter.
4.4. Training and Inference
We train our model using the following loss in an end-to-
end fashion:
L=LMIL+λPLLPL+λCLLCL, (13)
where λPL, λCL>0are hyperparameters. During train-
ing, we generate pseudo labels once every epoch for all
training examples using the best model so far. During in-
ference, we predict the segment-level probabilities using
the non-parametric prototypical pseudo-labeling (Eq. (10))
with video-level predictions from Eq. (3).5. Experiments
5.1. Setup
LLP Dataset. We conduct experiments on the Look, Lis-
ten and Parse (LLP) dataset [48] which consists of 11849
YouTube videos, each of 10s duration, labeled into 25event
categories. These videos are unconstrained with a wide va-
riety of scene content, including daily activities, music per-
formances, and vehicle sounds. We use 10000 videos with
weak (video-level) labels for training. The remaining 1849
fully-annotated videos (with segment-level labels) are used
for validation and testing. The train/val/test split has been
provided in the LLP dataset.
Evaluation Metrics. Following previous work, we use F1-
scores on audio, visual, and audio-visual events as eval-
uation metrics. These are computed both at the segment
and event level. We also include the aggregate metrics
“Type@A V” and “Event@A V”, again computed at the seg-
ment and event level. See Tian et al. [48] for a full explana-
tion of metrics.
Implementation Details. We use ResNet-152 [18] pre-
trained on ImageNet [6] and R(2+1)D-18 [49] pre-trained
on Kinetics-400 [27] as visual feature extractors to generate
a512-d visual feature per segment. We use VGGish [20]
pre-trained on AudioSet [14] to generate a 128-d audio fea-
ture per segment. We use the Adam optimizer [28] with
a batch size of 16and a learning rate α= 3e−4for50
epochs. We take η= 0.999for the momentum encoder,
λPL=λCL= 0.1in Eq. (13), γ= 1in Eq. (5), β= 0.2in
Eq. (6), τ= 0.1in Eq. (10), and τc= 0.2in Eq. (12). We
setkp=kn= 10 in Eq. (8). Hyperparameter values are
based on validation performance.
5.2. Comparisons with Prior Work
For a fair comparison, all methods considered use the same
pre-trained feature extractors and train/val/test split.
Quantitative. We compare methods in Table 1 on all the
18956
JoMoLD
CMPAE
Banjo
Banjo
BanjoSinging Singing
SingingSinging
SingingGT
MA
OursHAN
Singing Singing
GT
MA
CMPAEJoMoLDHAN
Ours
GT
MA
CMPAEJoMoLDHAN
OursBanjo
BanjoBanjoSinging
Singing
Banjo
Banjo BanjoGT
MA
CMPAEJoMoLDHAN
OursSinging SingingSinging Singing
Singing
Singing
GT
MA
CMPAEJoMoLDHAN
Ours
GT
MA
CMPAEJoMoLDHAN
Ours
GT
MA
CMPAEJoMoLDHAN
OursSpeech
SpeechSpeech
Violin
Violin
Violin
Violin
ViolinViolinViolin
Violin        
Violin
ViolinViolin Violin Violin
Speech
SpeechSpeechFigure 3. Audio-Visual Video Parsing results of our methods with HAN [48], MA [52], JoMoLD [4] and CMPAE [11] on two videos.
MethodAudio Visual Audio-Visual Type@A V Event@A V
Seg. Event Seg. Event Seg. Event Seg. Event Seg. Event
Base 59.5 51.1 58.7 53.4 54.5 47.4 57.6 50.6 56.5 48.5
Base+NPL H58.9 52.1 58.5 49.8 55.7 48.2 57.8 50.1 55.7 46.6
Base+NPL S59.3 52.5 59.0 51.1 56.0 48.7 58.2 50.8 56.2 47.6
Base+PPL H62.5 57.4 62.9 59.6 59.6 54.0 61.9 56.3 60.3 55.7
Base+PPL S65.9 57.3 66.7 64.3 61.9 54.3 64.8 59.9 63.7 57.9
Table 2. Performance for different pseudo labeling strategies.
metrics listed earlier. We see that our method outperforms
others on audio, audio-visual, Type@A V , and Event@A V
scores at both the segment and event levels. In particu-
lar, we achieve an average improvement of 2.6points for
audio-visual events. Averaged over all metrics, our method
achieves a 1.86point improvement over the previous best
method CMPAE [11]. Since we do not require additional
learnable parameters to generate pseudo labels, this im-
provement is with a comparable number of parameters to
previous methods.
Qualitative. We qualitatively compare our method with
some previous methods on two examples in Fig. 3. Here,
Fig. 3 (left) contains events “Singing” and “Banjo” in both
modalities. Fig. 3 (Right) shows another scene with the
event “Speech” in only the audio modality, and “Violin” oc-
curring in both modalities. As evident, the common failure
mode of previous methods is that they detect only a portion
of the duration of events, while ours more commonly de-
tects the full extent of events. As discussed in Sec. 1, this
could be because of multiple-instance learning encouraging
the model to pick out only the most discriminative segments
containing an event, as opposed to all segments.
Computational Cost. When training, our approach takes67.4s per epoch ( 17.1s for forward+backward propagation
and50.3s for prototype-based pseudo labeling). Inference
time is 5.9ms on average per video, which is comparable to
the baselines ( 2.9ms).
5.3. Analysis
Pseudo-Labeling Strategies. We report an ablation study
in Table 2 to validate our specific pseudo-labeling method.
Here, Base is the baseline MIL model explained in
Sec. 3.2. Base+NPL refers to the base model trained with
naive pseudo labeling, i.e., by directly using the segment-
level predictions of the model as pseudo labels for re-
training. In this case, HandSrefer to using hard (binarized)
and soft (continuous) pseudo labels, respectively. We ob-
serve no consistent improvement with naive pseudo label-
ing, likely due to the MIL model producing incorrect pre-
dictions for the non-discriminative segments for an event,
leading to less reliable pseudo labels. Base+PPL refers to
the base model trained with prototype -based pseudo labels
as described in Sec. 4.2. In this case, HandSrefer to using
hard and soft pseudo labels, as defined in Eqs. (9) and (10),
respectively. Our method significantly improves perfor-
mance by generating more reliable pseudo labels through-
out a training video. We also see that soft labeling is better
than hard labeling. Since segment-level predictions may be
noisy, we expect mistakes to affect performance more with
hard rather than soft labels.
Reliability of Pseudo Labels. We now assess if the pseudo
labels generated by our method are reliable for re-training.
For this, we take the pre-trained baselines HAN [48] and
MA[52], generate prototypes for each event from the train-
18957
MethodAudio Visual Audio-Visual Type@A V Event@A V
Seg. Event Seg. Event Seg. Event Seg. Event Seg. Event
HAN 60.1 51.3 52.9 48.9 48.9 43.0 54.0 47.7 55.4 48.0
+PPL Test 62.5 55.4 55.3 51.1 52.3 46.9 56.0 50.9 58.3 50.6
MA 60.3 53.6 60.0 56.4 55.1 49.0 58.9 53.0 57.9 50.6
+PPL Test 61.7 55.4 61.8 57.9 57.5 51.6 60.6 55.0 59.4 52.6
Table 3. Evaluating prototype-based pseudo labels on the test set.
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1
Parameter 3035404550556065F1-scoreSegment-level
Event-level
0.050.1 0.2 0.3 0.4 0.5
Parameter 45505560F1-scoreSegment-level
Event-level
Figure 4. Performance for different γandβchoices.
MethodAudio Visual Audio-Visual Type@A V Event@A V
Seg. Event Seg. Event Seg. Event Seg. Event Seg. Event
Base 59.5 51.1 58.7 53.4 54.5 47.4 57.6 50.6 56.5 48.5
No-CL 64.1 56.4 59.6 57.3 57.1 49.6 61.2 56.4 60.8 56.8
With-CL 65.9 57.3 66.7 64.3 61.9 54.3 64.8 59.9 63.7 57.9
Table 4. Effect of contrastive learning (CL) with pseudo labels.
ing data, and simply predict the prototype-based pseudo
labels (Eq. (9)) on the test set for evaluation (i.e., discard
the classifiers at test time). We call the resulting classifiers
HAN+PPL Test andMA+PPL Test and show results in Ta-
ble 3. With no other change to the models other than the
classifiers, we see significant improvements in all metrics,
showing that our pseudo labels are indeed reliable estimates
of the true labels. Note that for the results in Table 1, we
recompute pseudo labels on the training data every epoch,
with a continually improving base model.
Effect of γandβ.The hyperparameters γandβin Eqs. (5)
and (6) roughly control the proportion of segments that will
be selected as positives and negatives for assigning pseudo
labels. We experiment with different values and plot the
average of all segment/event-level metrics in Fig. 4. Perfor-
mance is susceptible to γ, which controls positives, and is
tolerant to a slight variation in β. The best performance is
forγ= 1andβ= 0.2.
Effect of Contrastive Learning. We analyze the impact
of contrastive learning using pseudo labels (Eq. (12)) by
simply setting λCL= 0 in Eq. (13). Results are reported
in Tab. 4. While only training with prototype-based pseudo
labels ( No-CL ) significantly improves the baseline, adding
a contrastive loss based on our pseudo labels ( With-CL )
further yields a large improvement.
Number of Clusters. We take kp=knand try values in
{1,2, ...,100}. We plot the average of all segment/event-
level metrics in Fig. 5. The best performance was for
k= 10 clusters per class. Having fewer clusters helps cap-
1 3 5 8 10 15 20 50 100
Number of Clusters40506070Average F1-scoreSegment-level
Event-levelFigure 5. Performance for different choices of the number of clus-
ters/prototypes per event.
MethodtIoU Avg.
0.1 0.2 0.3 0.4 0.5 0.6 0.7 [0.1:0.5] [0.3:0.7] [0.1:0.7]
EM-MIL [34] 59.1 52.7 45.5 36.8 30.5 22.7 16.4 44.9 30.4 37.7
ASM-Loc [17] 71.2 65.5 57.1 46.8 36.6 25.2 13.4 55.4 35.8 45.1
DELU [3]71.5 66.2 56.5 47.7 40.5 27.2 15.3 56.5 37.4 46.4
Ours 72.7 66.9 57.9 46.9 37.4 26.8 20.1 56.4 37.8 46.9
Table 5. Results ( %mAP) for the TAL task at different tIoU
thresholds.
ture more general features for an event instead of instance-
specific details, but having too few hurts performance by
discarding intra-class variation.
5.4. Results for a Different Task
We conduct preliminary experiments for the weakly-
supervised Temporal Action Localization (TAL) task [10]
on the THUMOS14 dataset [23] to further validate our
method. TAL aims to temporally localize and classify vi-
sual events in videos into known categories. We report
mean average precisions (mAP) at different temporal in-
tersection over union (tIoU) thresholds and the results are
in Table 5. We achieve better or comparable performance
w.r.t. the current state-of-the-art method DELU [3], and out-
perform previous methods. Check the supplementary for a
full comparison.
6. Interpretation as Expectation-Maximization
We now show that iteratively re-estimating soft pseudo
labels and training the model under weak + strong su-
pervision (Eq. (13)) can be interpreted as an expectation-
maximization (EM) algorithm, providing further insight for
our training procedure. For this, we view the segment-level
labels y={(ya
t,yv
t)}T
t=1for each video as latent variables
since they are not observed when training under weak su-
pervision. The observed variables are the video segments
x={(xa
t, xv
t)}T
t=1and weak labels w. For clarity, assume
a single event class, i.e., w,yt
m∈ {0,1}.
Our goal is to maximize the data log-likelihood L(θ) =
lnpθ(w|x)over model parameters θ(assume just one data
point xfor clarity). Instead of maximizing this directly, we
18958
can attempt to maximize the variational lower bound [29]
LVLBfor this likelihood (random variables are capitalized):
LVLB(θ, ϕ) = E
Y∼qϕ(·|w,x)[lnpθ(w|Y, x)]
−DKL[qϕ(·|w, x)||pθ(·|x)],(14)
where qϕ(y|w, x)isanyconditional distribution over labels
y, with parameters ϕ, meant to approximate the true pos-
terior pθ(y|x). The EM algorithm alternates between two
steps, first approximating the posterior by maximizing LVLB
overϕ(E step), and then improving the data likelihood by
maximizing it over θ(M step). Let j≥1index the itera-
tions (note, θandϕhere are not the same as in Sec. 3.2).
E Step. Thej-th update is given by
qϕj(y|w, x) =pθj−1(y|w, x),∀y. (15)
Assuming conditional independence of the segment-level
labels given the video and its weak labels, we can express
the right-hand side as pθ(y|w, x) =Q
t,mpθ(ym
t|w, x),
computed using
pθ(ym
t|w, x) :=(
pθ(ym
t|x)ifw= 1
1−ym
t ifw= 0,(16)
where pθ(ym
t|x)represents the prediction of the model for
modality mand segment tin video x. Thus, pθ(ym
t|w, x)
becomes a softpseudo label for the segment xt
m.
M Step. The j-th update finds θjby maximizing
LVLB(θ, ϕj), which, due to Eq. (15), is equivalent to
min
θ−X
ypθj−1(y|w, x) lnpθ(y|x)
−X
ypθj−1(y|w, x) lnpθ(w|y, x), (17)
where pθ(y|w, x)andpθ(y|x)are computed as in Eq. (16).
The first term in Eq. (17) can be expanded as the to-
tal cross-entropy between the segment-level predictions
pθ(ym
t|x)and the pseudo-label targets pθj−1(ym
t|w, x),
and is therefore equal to LPL(see Eq. (13)). To un-
derstand the second term in Eq. (17), note that in our
setting, pθ(w|y, x) = pθ(w|x)since we make video-
level predictions using MIL-pooling without access to the
segment-level labels ( y). The second term then reduces to
−lnpθ(w|x), which is simply the negative log-likelihood
of the video-level label ( w). This means the second term is
equal to LMIL(see Eq. (13)). I.e., the M step is equivalently
min
θ−X
ypθj−1(y|w, x) lnpθ(y|x)−lnpθ(w|x).(18)If we simply maximized the data log-likelihood L(θ)
directly (no pseudo labels), this could converge to a solu-
tion that picks out just the most discriminative segments in
each video, which may suffice to reduce the video-level loss
LMIL. As we showed in this paper, using pseudo labels can
help mitigate this, but the quality of the estimated pseudo
labels has a crucial role. E.g., hard labeling is inferior to
soft labeling since it does not attempt to compute the E step
(Eq. (15)) exactly. While soft labeling does attempt an ex-
act E step, better inductive biases (such as prototype-based
labeling with contrastive learning) here can help reach a fa-
vorable solution, as reported in Sec. 5.3 and Table 2. We
also compare with an off-the-shelf pseudo-labeling method
[34] in the supplementary material and find that we signifi-
cantly outperform it, showing that it is not pseudo-labeling
per se that works for A VVP - how we estimate the pseudo
labels is paramount.
7. Conclusion
We proposed a prototype-based pseudo-labeling method for
weakly-supervised A VVP that outperforms existing meth-
ods. Some interesting findings are that (i) naive pseudo-
labeling does not produce reliable training targets, (ii) con-
trastive learning using pseudo labels is an effective strategy,
and (iii) training with weak and re-estimated soft pseudo
labels is an EM algorithm.
We end with some limitations and suggestions for fu-
ture work. First, performance is sensitive to hyperparame-
terβas shown in Fig. 4. This could be because identify-
ing negatives for a class from the predictions of an MIL
model is less likely to be accurate than identifying pos-
itives. Moreover, negative segments for an event would
have a much larger variation in content. Second, events
with larger intra-class variation may need more prototypes
to represent, while we assume the same number to be opti-
mal for all events for convenience. Third, while we can in-
terpret our method as an EM algorithm, EM may only con-
verge to a locally optimal solution. As discussed in Sec. 6,
future work could explore inductive biases in the pseudo-
labeling step that favor better solutions. Finally, it is not
clear how many related video-/image-based prediction tasks
our method can generalize to. While we provided results for
TAL in Sec. 5.4, more evidence of adapting our method to
related tasks would be useful in future work.
Acknowledgement
Support from Institute of Eminence (IoE) project No.
SB22231269EEETWO005001 for Research Centre in
Computer Vision is gratefully acknowledged.
18959
References
[1] Hakan Bilen and Andrea Vedaldi. Weakly Supervised Deep
Detection Networks. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 2846–
2854, 2016. 2
[2] Ricardo Cabral, Fernando De la Torre, Joao Paulo Costeira,
and Alexandre Bernardino. Matrix Completion for Weakly-
Supervised Multi-Label Image Classification. IEEE trans-
actions on pattern analysis and machine intelligence , 37(1):
121–135, 2014. 2
[3] Mengyuan Chen, Junyu Gao, Shicai Yang, and Changsheng
Xu. Dual-evidential learning for weakly-supervised tempo-
ral action localization. In Computer Vision–ECCV 2022:
17th European Conference, Tel Aviv, Israel, October 23–27,
2022, Proceedings, Part IV , pages 192–208. Springer, 2022.
7
[4] Haoyue Cheng, Zhaoyang Liu, Hang Zhou, Chen Qian,
Wayne Wu, and Limin Wang. Joint-modal label denoising
for weakly-supervised audio-visual video parsing. In Com-
puter Vision – ECCV 2022 , pages 431–448, Cham, 2022.
Springer Nature Switzerland. 5, 6
[5] Thomas Cover and Peter Hart. Nearest neighbor pattern clas-
sification. IEEE transactions on information theory , 13(1):
21–27, 1967. 2
[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
Li Fei-Fei. ImageNet: A Large-Scale Hierarchical Image
Database. In 2009 IEEE Conference on Computer Vision
and Pattern Recognition , pages 248–255, 2009. 5
[7] Thomas G. Dietterich, Richard H. Lathrop, and Tom ´as
Lozano-P ´erez. Solving the Multiple Instance Problem with
Axis-Parallel Rectangles. Artificial Intelligence , 89(1):31–
71, 1997. 1
[8] Nanqing Dong and Eric P Xing. Few-shot semantic segmen-
tation with prototype learning. In BMVC , 2018. 2
[9] Jie Fu, Junyu Gao, Bing-Kun Bao, and Changsheng
Xu. Multimodal imbalance-aware gradient modulation for
weakly-supervised audio-visual video parsing. IEEE Trans-
actions on Circuits and Systems for Video Technology , 2023.
2
[10] Adrien Gaidon, Zaid Harchaoui, and Cordelia Schmid. Tem-
poral localization of actions with actoms. IEEE transactions
on pattern analysis and machine intelligence , 35(11):2782–
2795, 2013. 1, 7
[11] Junyu Gao, Mengyuan Chen, and Changsheng Xu. Col-
lecting cross-modal presence-absence evidence for weakly-
supervised audio-visual event perception. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 18827–18836, 2023. 2, 5, 6
[12] Salvador Garcia, Joaquin Derrac, Jose Cano, and Francisco
Herrera. Prototype selection for nearest neighbor classifica-
tion: Taxonomy and empirical study. IEEE transactions on
pattern analysis and machine intelligence , 34(3):417–435,
2012. 2
[13] Weifeng Ge, Xiangru Lin, and Yizhou Yu. Weakly Super-
vised Complementary Parts Models for Fine-Grained Im-
age Classification from the Bottom Up. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3034–3043, 2019. 2
[14] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren
Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal,
and Marvin Ritter. Audio set: An ontology and human-
labeled dataset for audio events. In 2017 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Process-
ing (ICASSP) , pages 776–780. IEEE, 2017. 5
[15] Jacob Goldberger, Geoffrey E Hinton, Sam Roweis, and
Russ R Salakhutdinov. Neighbourhood components analy-
sis.Advances in neural information processing systems , 17,
2004. 2
[16] Samantha Guerriero, Barbara Caputo, and Thomas Mensink.
DeepNCM: Deep Nearest Class Mean Classifiers, 2018. 2
[17] Bo He, Xitong Yang, Le Kang, Zhiyu Cheng, Xin Zhou, and
Abhinav Shrivastava. Asm-loc: Action-aware segment mod-
eling for weakly-supervised temporal action localization. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , pages 13925–13935,
2022. 7
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 5
[19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross
Girshick. Momentum contrast for unsupervised visual rep-
resentation learning. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
9729–9738, 2020. 4
[20] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F
Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal,
Devin Platt, Rif A Saurous, Bryan Seybold, et al. Cnn archi-
tectures for large-scale audio classification. In 2017 ieee in-
ternational conference on acoustics, speech and signal pro-
cessing (icassp) , pages 131–135. IEEE, 2017. 5
[21] Mengying Hu, Hu Han, Shiguang Shan, and Xilin Chen.
Weakly Supervised Image Classification through Noise Reg-
ularization. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 11517–
11525, 2019. 2
[22] Linjiang Huang, Yan Huang, Wanli Ouyang, and Liang
Wang. Relational prototypical network for weakly super-
vised temporal action localization. In Proceedings of the
AAAI conference on artificial intelligence , pages 11053–
11060, 2020. 2
[23] Haroon Idrees, Amir R Zamir, Yu-Gang Jiang, Alex Gorban,
Ivan Laptev, Rahul Sukthankar, and Mubarak Shah. The
thumos challenge on action recognition for videos “in the
wild”. Computer Vision and Image Understanding , 155:1–
23, 2017. 7
[24] Maximilian Ilse, Jakub Tomczak, and Max Welling.
Attention-based deep multiple instance learning. In Inter-
national conference on machine learning , pages 2127–2136.
PMLR, 2018. 2, 3
[25] Saumya Jetley, Bernardino Romera-Paredes, Sadeep Jaya-
sumana, and Philip Torr. Prototypical priors: From im-
proving classification to zero-shot learning. arXiv preprint
arXiv:1512.01192 , 2015. 2
18960
[26] Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant in-
formation clustering for unsupervised image classification
and segmentation. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 9865–9874,
2019. 4
[27] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,
Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman,
and Andrew Zisserman. The Kinetics Human Action Video
Dataset, 2017. 5
[28] Diederik P. Kingma and Jimmy Ba. Adam: A Method for
Stochastic Optimization. In ICLR (Poster) , 2015. 5
[29] Diederik P Kingma and Max Welling. Auto-Encoding Vari-
ational Bayes, 2022. 8
[30] Jatin Lamba, Jayaprakash Akula, Rishabh Dabral, Preethi
Jyothi, Ganesh Ramakrishnan, et al. Cross-modal learn-
ing for audio-visual video parsing. arXiv preprint
arXiv:2104.04598 , 2021. 1, 2, 3
[31] Yan-Bo Lin, Yu-Jhe Li, and Yu-Chiang Frank Wang. Dual-
modality seq2seq network for audio-visual event localiza-
tion. In ICASSP 2019-2019 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP) , pages
2002–2006. IEEE, 2019. 5
[32] Yan-Bo Lin, Hung-Yu Tseng, Hsin-Ying Lee, Yen-Yu Lin,
and Ming-Hsuan Yang. Exploring cross-video and cross-
modality signals for weakly-supervised audio-visual video
parsing. Advances in Neural Information Processing Sys-
tems, 34, 2021. 2, 3, 5
[33] Guoqing Liu, Jianxin Wu, and Zhi-Hua Zhou. Key In-
stance Detection in Multi-Instance Learning. In Proceedings
of the Asian Conference on Machine Learning , pages 253–
268, Singapore Management University, Singapore, 2012.
PMLR. 3
[34] Zhekun Luo, Devin Guillory, Baifeng Shi, Wei Ke, Fang
Wan, Trevor Darrell, and Huijuan Xu. Weakly-supervised
action localization with expectation-maximization multi-
instance learning. In Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part XXIX 16 , pages 729–745. Springer, 2020.
2, 7, 8
[35] Pascal Mettes, Elise Van der Pol, and Cees Snoek. Hyper-
spherical prototype networks. Advances in neural informa-
tion processing systems , 32, 2019. 2
[36] Shentong Mo and Yapeng Tian. Multi-modal grouping net-
work for weakly-supervised audio-visual video parsing. In
Advances in Neural Information Processing Systems , 2022.
2, 3, 5
[37] Islam Nassar, Munawar Hayat, Ehsan Abbasnejad, Hamid
Rezatofighi, and Gholamreza Haffari. ProtoCon: Pseudo-
Label Refinement via Online Clustering and Prototypical
Consistency for Efficient Semi-Supervised Learning. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 11641–11650, 2023.
2
[38] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Represen-
tation Learning with Contrastive Predictive Coding, 2018. 4[39] Alejandro Pardo, Humam Alwassel, Fabian Caba Heilbron,
Ali Thabet, and Bernard Ghanem. RefineLoc: Iterative Re-
finement for Weakly-Supervised Action Localization. In
2021 IEEE Winter Conference on Applications of Computer
Vision (WACV) , pages 3318–3327, 2021. 2
[40] Kranthi Kumar Rachavarapu and A N Rajagopalan. Boosting
positive segments for weakly-supervised audio-visual video
parsing. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 10192–10202, 2023. 2,
5
[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 2
[42] Dawid Rymarczyk, Aneta Kaczy ´nska, Jarosław Kraus,
Adam Pardyl, and Bartosz Zieli ´nski. Protomil: Multiple in-
stance learning with prototypical parts for fine-grained inter-
pretability. arXiv e-prints , pages arXiv–2108, 2021. 2
[43] Ruslan Salakhutdinov and Geoff Hinton. Learning a non-
linear embedding by preserving class neighbourhood struc-
ture. In Artificial intelligence and statistics , pages 412–419.
PMLR, 2007. 2
[44] Atsushi Sato and Keiji Yamada. Generalized learning vec-
tor quantization. Advances in neural information processing
systems , 8, 1995. 2
[45] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical
networks for few-shot learning. Advances in neural informa-
tion processing systems , 30, 2017. 2
[46] Peng Tang, Xinggang Wang, Xiang Bai, and Wenyu Liu.
Multiple Instance Detection Network with Online Instance
Classifier Refinement. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
2843–2851, 2017. 2
[47] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chen-
liang Xu. Audio-visual event localization in unconstrained
videos. In ECCV , pages 247–263, 2018. 1, 5
[48] Yapeng Tian, Dingzeyu Li, and Chenliang Xu. Unified mul-
tisensory perception: Weakly-supervised audio-visual video
parsing. In Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceed-
ings, Part III 16 , pages 436–454. Springer, 2020. 1, 2, 3,
5, 6
[49] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann
LeCun, and Manohar Paluri. A closer look at spatiotemporal
convolutions for action recognition. In Proceedings of the
IEEE conference on Computer Vision and Pattern Recogni-
tion, pages 6450–6459, 2018. 5
[50] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan
Wierstra, et al. Matching networks for one shot learning. Ad-
vances in neural information processing systems , 29, 2016.
2
[51] Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou,
and Jiashi Feng. Panet: Few-shot image semantic seg-
mentation with prototype alignment. In proceedings of
the IEEE/CVF international conference on computer vision ,
pages 9197–9206, 2019. 2
18961
[52] Yu Wu and Yi Yang. Exploring heterogeneous clues for
weakly-supervised audio-visual video parsing. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 1326–1335, 2021. 2, 3, 5, 6
[53] Zhirong Wu, Alexei A Efros, and Stella X Yu. Improving
generalization via scalable neighborhood component analy-
sis. In Proceedings of the european conference on computer
vision (ECCV) , pages 685–701, 2018. 2
[54] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.
Unsupervised feature learning via non-parametric instance
discrimination. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 3733–3742,
2018. 2
[55] Wenjia Xu, Yongqin Xian, Jiuniu Wang, Bernt Schiele, and
Zeynep Akata. Attribute prototype network for zero-shot
learning. Advances in Neural Information Processing Sys-
tems, 33:21969–21980, 2020. 2
[56] Hong-Ming Yang, Xu-Yao Zhang, Fei Yin, and Cheng-
Lin Liu. Robust classification with convolutional proto-
type learning. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 3474–3482,
2018. 2
[57] Weiyi Yang, Richong Zhang, Junfan Chen, Lihong Wang,
and Jaein Kim. Prototype-Guided Pseudo Labeling for
Semi-Supervised Text Classification. In Proceedings of the
61st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 16369–16382,
Toronto, Canada, 2023. Association for Computational Lin-
guistics. 2
[58] Yuanhao Zhai, Le Wang, Wei Tang, Qilin Zhang, Junsong
Yuan, and Gang Hua. Two-stream consensus network for
weakly-supervised temporal action localization. In Euro-
pean conference on computer vision , pages 37–54. Springer,
2020. 2
[59] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,
and Antonio Torralba. Learning Deep Features for Discrim-
inative Localization. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 2921–
2929, 2016. 2
[60] Jinxing Zhou, Dan Guo, Yiran Zhong, and Meng Wang. Im-
proving audio-visual video parsing with pseudo visual labels.
arXiv preprint arXiv:2303.02344 , 2023. 2
[61] Tianfei Zhou, Wenguan Wang, Ender Konukoglu, and Luc
Van Gool. Rethinking semantic segmentation: A proto-
type view. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 2582–
2593, 2022. 2
18962
