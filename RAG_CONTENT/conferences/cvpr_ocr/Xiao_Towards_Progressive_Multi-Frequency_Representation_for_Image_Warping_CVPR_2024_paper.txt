Towards Progressive Multi-Frequency Representation for Image Warping
Jun Xiao⇤,1Zihang Lyu⇤,1Cong Zhang1Yakun Ju2Changjian Shui3Kin-Man Lam†,1
1The Hong Kong Polytechnic University2Nanyang Technological University3Vector Institute
{jun.xiao, zihang.lyu, cong-clarence.zhang }@connect.polyu.hk, kin.man.lam@polyu.edu.hk
yakun.ju@ntu.edu.sg, changjian.shui@vectorinstitute.ai
Abstract
Image warping, a classic task in computer vision, aims
to use geometric transformations to change the appearance
of images. Recent methods learn the resampling kernels
for warping through neural networks to estimate missing
values in irregular grids, which, however, fail to capture
local variations in deformed content and produce images
with distortion and less high-frequency details. To address
this issue, this paper proposes an effective method, namely
MFR, to learn Multi-Frequency Representations from in-
put images for image warping. Speciﬁcally, we propose a
progressive ﬁltering network to learn image representations
from different frequency subbands and generate deformable
images in a coarse-to-ﬁne manner. Furthermore, we em-
ploy learnable Gabor wavelet ﬁlters to improve the model’s
capability to learn local spatial-frequency representations.
Comprehensive experiments, including homography trans-
formation, equirectangular to perspective projection, and
asymmetric image super-resolution, demonstrate that the
proposed MFR signiﬁcantly outperforms state-of-the-art
image warping methods. Our method also showcases su-
perior generalization to out-of-distribution domains, where
the generated images are equipped with rich details and less
distortion, thereby high visual quality. The source code is
available at https://github.com/junxiao01/MFR.
1. Introduction
Image warping aims to change the appearance of images
by performing geometric transformations, which involves
changing the positions of image pixels to new positions in
a predeﬁned coordinate systems. As a basic technique in
computer vision, image warping has become an indispens-
able component in numerous vision tasks, such as facial ma-
nipulation [ 11,46], image registration [ 12,20,31], image
⇤These authors contributed equally to this work
†The corresponding author
SRWarp [38]LTEW [21]MFR (Ours)GTWarped ImageFigure 1. Illustration of local image patches generated by SRWarp
[41], LTEW [ 22], and our proposed MFR.
synthesis [ 1,52], etc., substantially affecting their overall
performance.
Traditional image warping methods usually apply an in-
verse transformation function to deform images and depend
on interpolation techniques, such as bicubic interpolation,
to estimate the missing values in irregular grids. How-
ever, previous studies [ 9,30,36,41] have revealed that
these interpolation-based methods often introduce undesir-
able jagging and blurry artifacts, leading to image quality
degradation. Recent works [ 22,41] have formulated im-
age warping as a generalized image super-resolution (SR)
problem with varying scaling factors in the spatial domain.
This is equivalent to stretching local image regions with
different scaling factors in different directions. To gener-
ate deformable image content, Son et al .[41] employed a
pretrained SR model for feature extraction and introduced
adaptive warping layers to model the resampling kernel.
On the other hand, Lee et al.[22] treated image representa-
tion in a continuous space and utilized the coordinate-based
MLP model to synthesize content in irregular grids. Never-
theless, we observe that these methods encounter challenges
in captured local variations in the deformable images, and
their generated images are often distorted and lack high-
frequency details, as demonstrated in Fig. 1. Furthermore,
these methods exhibit poor generalization performance in
out-of-distribution data, i.e., scaling factors and geometric
transformation not included in the training dataset, signiﬁ-
cantly limiting their real-world applications. Consequently,
there is still substantial potential for improvement in image
warping.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
2995
In this paper, we propose an effective approach to learn
multi-frequency representations from an input image for
image warping, namely MFR. Concretely, a progressive
ﬁltering network is devised to learn the image represen-
tations from different frequency subbands in the feature
space. Moreover, the proposed model starts from the input
coarse-scale images and gradually produces ﬁner-scale de-
tails through the learned frequency representations, result-
ing in a coarse-to-ﬁne generation process. As image warp-
ing intrinsically involves signiﬁcant local deformation, we
incorporate learnable Gabor wavelet ﬁlters to improve our
model’s learning capability of spatial-frequency represen-
tations, particularly beneﬁcial for handling high-frequency
information in local regions. Extensive experiments have
shown that MFR can remarkably outperform state-of-the-
art image warping methods in various vision tasks, includ-
ing homography transformation, equirectangular projection
(ERP) to perspective projection, and asymmetric image
super-resolution. Notably, our model demonstrates superior
generalization capability to out-of-distribution data, yield-
ing images with rich details and high visual quality.
2. Related Works
2.1. Image Warping
Image warping serves as a foundational technique in com-
puter vision and has been widely used in various vision
tasks such as image registration [ 12,20,31], image gen-
eration [ 1,52], and image editing [ 11,46]. This technique
employs geometric transformations to map pixel positions
of images to new locations in a distinct coordinate system.
Traditional image warping methods [ 3,5,6,13,21] rely
on interpolation approaches to compute missing values in
irregular grids, but tend to introduce jagged and blurred ar-
tifacts, resulting in suboptimal performance. A recent ap-
proach by Son et al .[41] considers image warping as a
generalized image SR problem with varying scaling fac-
tors in local regions. They introduced an adaptive warp-
ing layer to generate transformed images by leveraging the
features extracted from pretrained SR models. Despite its
merits, this method exhibits poor generalization in generat-
ing large-scale images. To address this issue, Lee et al.[22]
provided an alternative method that treats image warping as
image representation in continuous spaces. They incorpo-
rated Fourier features of the coordinate information with the
SR features and employed a coordinate-based MLP model
to synthesize the warped images. However, this MLP model
has limited model capability in feature representation and
fails to capture local variations in deformable content, re-
sulting in distortion in the generated images. In contrast,
our method focuses on learning local high-frequency repre-
sentations for effectively generating high-frequency details
in deformable regions.2.2. Learning in the Frequency Domain
Image representations in the frequency domain typically
contain distinct patterns and have demonstrated their effec-
tiveness in numerous vision tasks, including image clas-
siﬁcation [ 33–35,43,49], domain generalization [ 8,16,
29,39,50], and image generation [ 4,10,18,19,25,28,
37,42,47,48]. Notably, Huang et al .[16] introduced a
randomization technique in the frequency domain to learn
domain-invariant features for domain generalization. Simi-
larly, Yang et al.[50] swapped the low-frequency spectrum
between the source and target domains to achieve domain
alignment, enhancing the model’s generalization capabili-
ties. For the generation of high-quality images, Tancik et
al.[42] introduced Fourier features to enhance the repre-
sentation ability of the implicit neural representation meth-
ods. Sitzmann et al .[40] proposed the Sinusoidal Repre-
sentation Network (SIREN) in which the ReLU function
is replaced with a sinuous activation function, demonstrat-
ing the potential of frequency representation learning. In-
spired by the success of frequency learning in various ap-
plications, we propose an effective method to learn fre-
quency components from different frequency subbands in
the latent space speciﬁcally for image warping. Addition-
ally, to capture local variations in deformable image con-
tent, we propose using learnable Gabor wavelet ﬁlters to ex-
tract spatial-frequency representations from local regions.
This approach can signiﬁcantly enhances the performance
by generating clear high-frequency details in the locally de-
formable areas.
3. Methodology
Given an input RGB image Iin2RW⇥H⇥3, our objec-
tive is to synthesize high-quality deformable image con-
tent by learning multi-frequency representations from the
input, where WandHrepresent the width and height of
the images, respectively. The overall pipeline of our pro-
posed MFR is shown in Fig. 2. It consists of two stages:
the feature encoding stage and the frequency learning stage,
which are elaborated in the following sections.
3.1. Feature Encoding
In the feature encoding stage, we employ a pretrained SR
model for projecting the input images to the latent feature
space. Subsequently, we leverage the local texture esti-
mator (LTE) [ 22] to integrate both coordinate information
(i.e., relative position) and geometric information (i.e., cur-
vature) into the output of the pretrained SR model. The
overall process of feature encoding is denoted as E . This
incorporation results in latent representations of the input
image, denoted by X2RW⇥H⇥d, where dis the dimen-
sionality of the latent space. However, these representations
alone are insufﬁcient to capture local variations for image
2996
Feature Encoding!"
ℱ
=Frequency Learningℱ
$%(')%(')*)+(')
ConvolutionalLayerFilterLayerEncodedFeaturesElement-wiseAdditionHadamardProductℱFrequencyLearning ModuleGatingMechanismInterpolationXL layers%(*)ℱℱ+(*)+(,)+(-){/0}0234
/5
/5
/5Figure 2. The overall pipeline of the proposed MFR, which consists of two stages: feature encoding and frequency learning.
synthesis.
3.2. Frequency Learning
Given the latent features Xobtained in the feature encod-
ing stage, we vectorized them into a set of latent vectors, ex-
pressed as {xn}N
n=1, where N=H⇥Wis the total number
of latent vectors. Our proposed MFR contains a frequency
learning network which comprises Lfrequency learning
modules stacked sequentially. Each module learns the fre-
quency representations from the latent vectors and gener-
ates the corresponding deformed image content. Speciﬁ-
cally, we assume that xiis the latent vector at the i-th pixel
position. MFR extracts frequency representations zof the
input latent vectors through a sequence of frequency learn-
ing modules F`, as follows:
z(0)
i=G✓0(xi)+xi, (1)
z(`+1)
i=G✓`(xi)⌦⇣
W(`)z(`)
i+b(`)⌘
, (2)
for`=0,···,L 1.z(`)
i2Rdrepresents the frequency
representation generated from the `-th module and the input
latent vector xi.G✓`:Rd!Rddenotes the ﬁlter layer in
the`-th module. W(`)2Rd⇥dandb(`)2Rdrepresent the
weights and bias terms of the `-th layer, respectively. ⌦de-
notes the element-wise multiplication. The proposed MFR
extracts frequency representations from the input using the
ﬁltering layer and embeds them into the frequency repre-
sentation obtained from the previous module, to form new
frequency representations, resulting in a progressive learn-
ing procedure.
When the ﬁlter layer adopts a sinusoidal function like
SIREN [ 40], the associated frequency representations can
be express as z(`)
i=s i n ( !`xi+ `), where !`and `are
the ﬁler frequency and phase in the `-th module. A largervalue of !`means that the corresponding module tends to
learn a higher frequency response from the input vector.
Y¨uceet al .[51] revealed that the network output can be
approximated by a polynomial function using the Taylor ex-
pansion. As a result, the L-th frequency representation z(L)
i
can be equivalently expressed as a linear combination of the
sinusoidal functions, as follows:
z(L)
i=NsineX
j=0ˆ↵jsin(ˆ!jxi+ˆ j), (3)
where the parameters ˆ↵j,ˆ!j, and ˆ jare dependent on the
parameters of the network. Nsinedenotes the total number
of summation terms and is deﬁned as follows:
Nsine=L 1X
i=02i(d)i+1, (4)
Obviously, Nsinegrows exponentially with the number of
frequency learning modules used in our MFR. This means
that we can use a small number of frequency learning mod-
ules to obtain various frequency components of the input.
In each frequency learning module, we compute the out-
puty(`)
ibased on the associated frequency representation
z(`)
iand the input xias follows:
y(`)
i=f `(g`(z(`)
i)+xi), (5)
where g`is the gating mechanism in the `-th module, which
is a 1-D attention mechanism [ 14] and adaptively controls
the information ﬂow from the frequency representation z(`)
i.
f `is the decoding function parametrized by  `in the `-th
module. At the output part of each module, we employ a
short connection to fuse the spatial representations xiwith
the learned frequency representation z(`)
i.
2997
After obtaining the output of each module, our model
computes the ﬁnal output image yas follows:
y=ybic+LX
`=1y(`), (6)
where ybicrepresents the coarse-scale image content ob-
tained by performing bicubic interpolation to the input im-
age. Therefore, our model progressively generates ﬁne-
scale image content and enhances the image quality.
3.3. Gabor Wavelet Filter Layer
The primary challenge in generating high-quality de-
formable images is to effectively capture local intensity
variations within deformable content. To facilitate this, the
ﬁltering layers used in our model play a crucial role. Instead
of using conventional ﬁlters like SIREN [ 40] and Gaussian
ﬁlters, we incorporate 1-D Gabor wavelet ﬁlters into the fre-
quency learning modules which is deﬁned as follows:
FGabor(x)=e (x x0)2
↵2e i!(x x0). (7)
The Gabor wavelet ﬁlter is a Gaussian ﬁlter modulated
by a complex exponential term, which has demonstrated
remarkable capability to capture local variations in both
the spatial and frequency domains in image processing
[17,23,24,26,32,38]. In Eq. ( 7),x0is the predeﬁned cen-
ter point. As the input xdeviates from this center point, the
output response undergoes exponential decay. Similarly, we
can generate different output responses by controlling the
rate of exponential drop-off ↵and the rate of the modulation
!. Our MFR can learn different frequency representations
by learning different values of these two parameters through
backpropagation. It is difﬁcult for conventional ﬁlters, such
as SIREN [ 40] and Gaussian ﬁlters, to capture local varia-
tions from both the spatial and frequency domains simulta-
neously. In contrast, Gabor wavelet ﬁlters are deﬁned over
compact supports in both the spatial and frequency domains
as shown in Fig. 3. As a result, it can effectively capture in-
tensity variations in local regions from both the spatial and
frequency domains for image warping.
4. Experiments and Analysis
4.1. Experiment Settings
Dataset information and implementation settings . The
proposed MFR was trained using DIV2K dataset [ 2], which
is a widely-used dataset in numerous low-level vision tasks
and provides 800 high-resolution images. In the training
stage, we randomly crop local image patches of size 48⇥48
from the input images as the training samples, and set the
batch size to 16. We used the `2loss as the loss function.
Figure 3. Illustration of three 1-D ﬁlters: SIREN [ 40], Gaussian,
and GaborWavelet ﬁlters.
The Adam optimizer was utilized to update the model pa-
rameters with  1=0.9and 2=0.999, and the total num-
ber of training epochs was 600. We initially set the learning
rate to 2⇥10 4and adaptively decay it by utilizing the
cosine annealing strategy.
Evaluation settings . To assess the effectiveness of our
model in image warping, we evaluate the model on three
vision tasks, including homography transformation, ERP
to perspective projection, and asymmetric image super-
resolution. The masked peak signal-to-noise ratio (mP-
SNR) [ 41] is adopted as the performance metric in the ho-
mography transformation task. In asymmetric image super-
resolution, the PSNR is used to evaluate the reconstruction
quality with different scaling factors.
4.2. Experiments on Homography Transformation
We compare our MFR with SRWarp [ 41] and LTEW [ 22]
on benchmark datasets provided in [ 22] in both the in-scale
setting and the out-of-scale settings. In the in-scale setting,
the scaling factors are involved in the training dataset. How-
ever, in the out-of-scale setting, the scaling factors are not
included in the training dataset. To ensure a fair compari-
son, we adopt RRDB [ 45] as the SR backbone for feature
extraction in these three models. For completeness, we in-
clude the results from bicubic interpolation and the original
RRDB model. For the RRDB model, we ﬁrst use the model
to generate the images, which are then resampled by bicu-
bic interpolation. For SRWarp and LTEW, we directly use
their public source codes for implementation.
Table 1shows the average mPSNR scores of different
image warping methods for homograph transformation on
the benchmark datasets with two evaluation settings. It is
obvious that our MFR achieves better performance than
the compared models in the in-scale setting, especially in
the Urban100W dataset, and the performance of MFR is
0.18dB higher than the LTEW method. In the out-of-scale
setting, the proposed MFR signiﬁcantly outperforms the
2998
Table 1. The average mPSNR of different image-warping methods for homograph transformation on benchmark datasets with the in-scale
(is) and out-of-scale (os) settings. The best results are highlighted in bold.
MethodsDIV2KW Set5W Set14W B100W Urban100W
is os is os is os is os is os
Bicubic 27.85 25.03 35.00 28.75 28.79 24.57 28.67 25.02 24.84 21.89
RRDB [ 45] 30.76 26.84 37.40 30.34 31.56 25.95 30.29 26.32 28.83 23.94
SRWarp-RRDB [ 41]31.04 26.75 37.93 29.90 32.11 25.35 30.48 26.10 29.45 24.04
LTEW-RRDB [ 22]31.10 26.92 38.20 31.07 32.15 26.02 30.56 26.41 29.50 24.25
MFR-RRDB (Ours) 31.18 27.12 38.23 31.19 32.26 26.26 30.62 26.53 29.68 24.51
HR ImageBicubic
RRDB
SRWarp [38]
LTEW [21]
MFR (Ours)GT
Figure 4. Illustration of the images generated by different image-warping methods in the in-scale setting.
HR ImageBicubic
RRDB
SRWarp [38]
LTEW [21]
MFR (Ours)GT
Figure 5. Illustration of the images generated by different image-warping methods in the out-of-scale setting.
compared models in all benchmark datasets. In particular, it
outperforms the second-best model (i.e., LTEW) by 0.20dB
and 0.26dB on the DIV2KW and Urban100W datasets, re-
spectively. These results show that MFR has a superior gen-
eralization capability to handle out-of-scale images. Addi-
tionally, for visual comparison, we select two generated im-
ages from each evaluation setting in Fig. 4and Fig. 5. As
observed, SRWarp and LTEW have limited ability to gen-
erate texture and detailed information, resulting in distorted
image content. In contrast, MFR can effectively synthesize
high-frequency information, such as edges, textures, etc.,leading to the generated images with high visual quality.
4.3. Experiments on ERP to Perspective Projection
In addition to evaluating the performance of MFR on im-
ages with out-of-distribution scales, we further explore its
generalization capability on out-of-distribution transforma-
tions, i.e., perspective projection of ERP images. To con-
duct this investigation, we employ MFR, initially trained
for homography transformation, on ERP images from the
Flick360 validation dataset [ 7]. The size of the input ERP
images is 2048 ⇥1024 , and we project the images to the
2999
ERP Image
LTEW [21]
MFR (Ours)
BicubicPerspective ImageFigure 6. Illustration of the results (ERP images !Perspective images) generated by different image-warping methods.
size of 1024 ⇥1024 with a ﬁeld of view (FOV) of 120 .
As this dataset does not provide ground-truth images in the
perspective view, we compare the performance of different
methods through visual results. We choose two generated
images and illustrate them in Fig. 6. For better comparison,
we cropped two local regions marked by green rectangles
and enlarged them.
As observed, the original EPR images suffer from seri-
ous deformation, but our MFR can effectively align them
in the perspective views, while preserving local image con-
tent, compared with other image-warping methods. MFR
produces images with less distortion and delivers clear de-
tails, resulting in superior visual quality.
4.4. Experiments on Asymmetric Image Super-
resolution
We compare the proposed MFR with MetaSR [ 15],
ArbSR [ 44], and LTEW [ 22] for asymmetric image super-
resolution, in both the in-scale and out-of-scale settings. All
methods adopt the RCAN model [ 53] as the backbone for a
fair comparison. Four benchmark datasets, including Set5,
Set14, B100, and Urban100, are used for evaluation. For the
original RCAN model, we ﬁrst upsample the images with a
scaling factor of 4, followed by the bicubic interpolation to
achieve the desired size.
Tables 2and3illustrate the average PSNR scores of dif-
ferent methods for asymmetric image SR on benchmark
datasets in the in-scale and out-of-scale settings, respec-
tively. In the in-scaling setting, MFR can achieve better
performance than the compared methods. Similarly, in the
out-of-scaling setting, our MFR exhibits superior general-
ization capability on different benchmark datasets, in partic-
ular, for the B100 and Urban100 datasets with large scaling
factors. In addition, we show the images generated by dif-
ferent methods under the in-scale and out-of-scale settings
in Fig. 7and Fig. 8, respectively. These results demonstratethat the proposed MFR has a better ability to generate high-
frequency information, including textures and edges, than
the compared methods, resulting in the best visual quality.
4.5. Ablation Study
4.5.1 Experiments on Various Filters
The choice of ﬁlters plays a signiﬁcant role in learning fre-
quency representations in our model. In this experiment,
we investigate the impact of different ﬁlters on the perfor-
mance. To facilitate the evaluation, we compare the learn-
able Gabor wavelet ﬁlter with the SIREN ﬁlter [ 40] and the
conventional Gabor ﬁlter. Instead of adaptively updating the
ﬁlter parameters, we evaluate the performance of our model
using static Gabor ﬁlters. To ensure a fair comparison, all
models employ EDSR [ 27] as the backbone for feature en-
coding. Table 4shows the average mPSNR of our model
using different ﬁlters for homography transformation on the
DIV2KW dataset.
We ﬁnd that the impact of employing different ﬁlters
in our model is marginal in the in-scale setting but it be-
comes substantial in the out-of-scale setting. We demon-
strate the images generated using different ﬁlters in Fig. 9.
From these results, we ﬁnd that employing learnable Ga-
bor wavelet ﬁlters can signiﬁcantly enhance MFR’s capa-
bility to produce high-frequency details while avoiding dis-
tortion, which beneﬁts from the compactness property of
Gabor wavelet ﬁlters in both the spatial and frequency do-
mains. This property enables our proposed MFR to effec-
tively capture local variations in deformable images.
4.5.2 Experiments on Network Structures
Moreover, we study how the structure of the network af-
fects the performance of our MFR. In the generation of
deformable images, we adopt a short connection to fuse
the spatial representations from the input with the fre-
3000
Table 2. The average PSNR(dB) of state-of-the-art methods for asymmetric-scale SR on the benchmark datasets in the in-scale setting.
The best and the second-best results are highlighted in red and blue, respectively.
MethodsSet5 Set14 B100 Urban100
⇥1.5
⇥4.0⇥1.5
⇥3.5⇥1.6
⇥3.05⇥4.0
⇥2.0⇥3.5
⇥2.0⇥3.5
⇥1.75⇥4.0
⇥1.4⇥1.5
⇥3.0⇥3.5
⇥1.75⇥1.6
⇥3.0⇥1.6
⇥3.8⇥3.55
⇥1.55
Bicubic 30.01 30.86 31.40 27.25 27.88 27.27 27.45 28.86 27.94 25.93 24.92 25.19
RCAN [ 53] 34.14 35.05 35.67 30.35 31.02 31.21 29.35 31.30 29.98 30.72 28.81 29.34
MetaSR-RCAN [ 15]34.20 35.17 35.81 30.40 31.05 31.33 29.43 31.26 30.09 30.73 29.03 29.67
ArbSR-RCAN [ 44]34.37 35.40 36.05 30.55 31.27 31.54 29.54 31.40 30.22 31.13 29.36 30.04
LTEW-RCAN [ 22]34.45 35.46 36.12 30.57 31.21 31.55 29.62 31.40 30.24 31.25 29.57 30.21
MFR-RCAN (Ours) 34.48 35.49 36.13 30.66 31.33 31.63 29.65 31.42 30.26 31.33 29.65 30.29
HR ImageBicubic
ArbSR[41]
LTEW [21]
MFR (Ours)
GT
×3.55/×1.55×1.6/×3
Figure 7. Illustration of the visual results generated by different asymmetric-scale SR methods in the in-scale setting.
Table 3. The average PSNR(dB) of state-of-the-art methods for asymmetric-scale SR on the benchmark datasets in the out-of-scale setting.
The best and the second-best results are highlighted in red and blue, respectively.
MethodsSet5 Set14 B100 Urban100
⇥3.0
⇥8.0⇥3.0
⇥7.0⇥3.2
⇥6.1⇥8.0
⇥4.0⇥7.0
⇥4.0⇥7.0
⇥3.5⇥8.0
⇥2.8⇥3.0
⇥6.0⇥7.0
⇥2.9⇥3.2
⇥6.0⇥3.2
⇥7.6⇥7.1
⇥3.1
Bicubic 25.69 26.35 26.84 24.27 24.62 24.79 24.67 25.58 24.98 22.55 21.92 22.15
RCAN [ 53] 29.00 30.01 30.46 26.48 26.94 27.11 26.06 27.19 26.47 25.52 24.50 24.84
MetaSR-RCAN [ 15]28.75 29.74 30.38 26.32 26.85 27.03 26.07 27.15 26.45 25.50 24.47 24.84
ArbSR-RCAN [ 44]28.37 29.35 30.08 26.06 26.63 26.84 25.91 27.14 26.40 25.36 24.12 24.61
LTEW-RCAN [ 22]29.26 30.16 30.64 26.60 27.06 27.25 26.25 27.28 26.62 25.85 24.79 25.18
MFR-RCAN (Ours) 29.27 30.12 30.68 26.61 27.12 27.31 26.29 27.32 26.66 25.88 24.85 25.26
quency representations learned from the networks with a
gate mechanism. To evaluate the effectiveness of these two
mechanisms, we evaluate the model performance with and
without using the short connection and the gate mechanism.
The average mPSNR scores of MFR for homography trans-formation on the DIV2KW and Urban100W datasets are il-
lustrated in Table 5.
Table 5shows that MFR cannot achieve satisfactory per-
formance by only using frequency representations, without
the short connection. Using the gate mechanism, the perfor-
3001
HR ImageBicubic
ArbSR[41]
LTEW [21]
MFR (Ours)
GT
×7/×4×3.2/×7.6Figure 8. Illustration of the visual results generated by different asymmetric-scale SR methods in the out-of-scale setting.
Table 4. The average mPSNR of MFR using different ﬁlters for
homography transformation on the DIV2K dataset. “GW-S” and
“GW-L” denote the static and learnable Gabor wavelet ﬁlter, re-
spectively. The best results are highlighted in bold.
SIREN [ 40] Gabor GW-S GW-L
is 30.87 30.83 30.86 30.88
os 26.81 26.86 26.86 26.90
SIREN [37]
Gabor
Static GaborWavelet
Learnable GaborWavelet (Ours)
GT
Figure 9. Illustration of visual results generated by our models
using different ﬁlters.
mance of the proposed MFR can be further improved. Over-
all, network structure is substantial for the performance of
our MFR. However, it is worth noting that the optimal net-
work structure should be tailored to speciﬁc applications
and implementation scenarios.
5. Conclusion
In this paper, we propose a novel and effective method to
learn the frequency representations of input images for im-
age warping, namely MFR. Concretely, our MFR ﬁrst em-
ploys a pretrained image super-resolution model to project
the input image into the latent space. Then, we propose
a ﬁltering network to progressively learn frequency repre-Table 5. The average mPSNR of MFR using different network
structures for homography transformation on the DIV2KW and
Urban100W datasets. “SC” and “Gate” denote the short connec-
tion and the gate mechanism, respectively. The best results are
highlighted in bold.
SC GateDIV2KW Urban100W
is os is os
30.72 26.80 29.02 24.07
! 30.86 26.88 29.33 24.42
!30.79 26.83 29.12 24.11
!! 30.88 26.90 29.68 24.51
sentations from different frequency subbands of the input
features and generate deformable images in a coarse-to-ﬁne
manner. Furthermore, we incorporate Gabor wavelet ﬁl-
ters into our model to enhance the capability to simulta-
neously capture local variations in deformable regions in
both the spatial and frequency domains. Experiments show
the superior performance of the proposed MFR in vari-
ous tasks, including homography transformation, equirect-
angular to perspective projection, and asymmetry image
super-resolution, signiﬁcantly outperforming state-of-the-
art image-warping methods. In addition, our MFR ex-
hibits better generalization ability when processing out-of-
distribution images with large scaling factors and transfor-
mations. The images generated by our model have rich de-
tailed information and reduced distortion, resulting in the
best visual quality.
Acknowledgments
This work was supported by the Hong Kong Research
Grants Council (RGC) Research Impact Fund (RIF) under
Grant R5001-18.
3002
References
[1]Rameen Abdal, Yipeng Qin, and Peter Wonka. Im-
age2stylegan: How to embed images into the stylegan latent
space? In Proceedings of the IEEE/CVF international con-
ference on computer vision , pages 4432–4441, 2019. 1,2
[2]Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge
on single image super-resolution: Dataset and study. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition workshops , pages 126–135, 2017. 4
[3]Per Rønsholt Andresen and Mads Nielsen. Non-rigid reg-
istration by geometry-constrained diffusion. Medical Image
Analysis , 5(2):81–88, 2001. 2
[4]Reza Azad, Abdur R Fayjie, Claude Kauffmann, Ismail
Ben Ayed, Marco Pedersoli, and Jose Dolz. On the texture
bias for few-shot cnn segmentation. In Proceedings of the
IEEE/CVF winter conference on applications of computer
vision , pages 2674–2683, 2021. 2
[5]M Faisal Beg, Michael I Miller, Alain Trouv ´e, and Laurent
Younes. Computing large deformation metric mappings via
geodesic ﬂows of diffeomorphisms. International journal of
computer vision , 61:139–157, 2005. 2
[6]Thaddeus Beier and Shawn Neely. Feature-based image
metamorphosis. In Seminal Graphics Papers: Pushing the
Boundaries, Volume 2 , pages 529–536. 2023. 2
[7]Mingdeng Cao, Chong Mou, Fanghua Yu, Xintao Wang,
Yinqiang Zheng, Jian Zhang, Chao Dong, Gen Li, Ying
Shan, Radu Timofte, et al. Ntire 2023 challenge on 360deg
omnidirectional image and video super-resolution: Datasets,
methods and results. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
1731–1745, 2023. 5
[8]Hao Cheng, Siyuan Yang, Joey Tianyi Zhou, Lanqing Guo,
and Bihan Wen. Frequency guidance matters in few-shot
learning. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 11814–11824, 2023.
2
[9]Ming-Chao Chiang. Imaging-consistent warping and super-
resolution . Columbia University, 1998. 1
[10] Rizal Fathony, Anit Kumar Sahu, Devin Willmott, and J Zico
Kolter. Multiplicative ﬁlter networks. In International Con-
ference on Learning Representations , 2020. 2
[11] Julia Gong, Yannick Hold-Geoffroy, and Jingwan Lu. Auto-
toon: Automatic geometric warping for face cartoon genera-
tion. In Proceedings of the IEEE/CVF winter conference on
applications of computer vision , pages 360–369, 2020. 1,2
[12] Steven Haker, Lei Zhu, Allen Tannenbaum, and Sigurd An-
genent. Optimal mass transport for registration and warping.
International Journal of computer vision , 60:225–240, 2004.
1,2
[13] Mark Holden. A review of geometric transformations for
nonrigid body registration. IEEE transactions on medical
imaging , 27(1):111–128, 2007. 2
[14] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 7132–7141, 2018. 3
[15] Xuecai Hu, Haoyuan Mu, Xiangyu Zhang, Zilei Wang,
Tieniu Tan, and Jian Sun. Meta-sr: A magniﬁcation-arbitrary network for super-resolution. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 1575–1584, 2019. 6,7
[16] Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu.
Fsdr: Frequency space domain randomization for domain
generalization. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 6891–
6902, 2021. 2
[17] Muwei Jian, Kin-Man Lam, Junyu Dong, and Linlin Shen.
Visual-patch-attention-aware saliency detection. IEEE trans-
actions on cybernetics , 45(8):1575–1586, 2014. 4
[18] Liming Jiang, Bo Dai, Wayne Wu, and Chen Change Loy.
Focal frequency loss for image reconstruction and synthesis.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 13919–13929, 2021. 2
[19] Yakun Ju, Kin-Man Lam, Jun Xiao, Cong Zhang, Cuixin
Yang, and Junyu Dong. Efﬁcient feature fusion for learning-
based photometric stereo. In ICASSP 2023-2023 IEEE Inter-
national Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP) , pages 1–5. IEEE, 2023. 2
[20] Verena Kaynig, Bernd Fischer, and Joachim M Buhmann.
Probabilistic image registration and anomaly detection by
nonlinear warping. In 2008 IEEE Conference on Computer
Vision and Pattern Recognition , pages 1–8. IEEE, 2008. 1,2
[21] Daniel Keysers, Thomas Deselaers, Christian Gollan, and
Hermann Ney. Deformation models for image recognition.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence , 29(8):1422–1435, 2007. 2
[22] Jaewon Lee, Kwang Pyo Choi, and Kyong Hwan Jin. Learn-
ing local implicit fourier representation for image warping.
InEuropean Conference on Computer Vision , pages 182–
200. Springer, 2022. 1,2,4,5,6,7
[23] Tai Sing Lee. Image representation using 2d gabor wavelets.
IEEE Transactions on pattern analysis and machine intelli-
gence , 18(10):959–971, 1996. 4
[24] Dong Li, Huiling Zhou, and Kin-Man Lam. High-resolution
face veriﬁcation using pore-scale facial features. IEEE trans-
actions on image processing , 24(8):2317–2327, 2015. 4
[25] Jiacheng Li, Chang Chen, Wei Huang, Zhiqiang Lang, Feng-
long Song, Youliang Yan, and Zhiwei Xiong. Learning steer-
able function for efﬁcient image resampling. In CVPR , 2023.
2
[26] Zhenxuan Li, Wenzhong Shi, Hua Zhang, and Ming Hao.
Change detection based on gabor wavelet features for very
high resolution remote sensing images. IEEE Geoscience
and Remote Sensing Letters , 14(5):783–787, 2017. 4
[27] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and
Kyoung Mu Lee. Enhanced deep residual networks for single
image super-resolution. In Proceedings of the IEEE confer-
ence on computer vision and pattern recognition workshops ,
pages 136–144, 2017. 6
[28] David B Lindell, Dave Van Veen, Jeong Joon Park, and
Gordon Wetzstein. Bacon: Band-limited coordinate net-
works for multiscale scene representation. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 16252–16262, 2022. 2
[29] Quande Liu, Cheng Chen, Jing Qin, Qi Dou, and Pheng-Ann
Heng. Feddg: Federated domain generalization on medical
3003
image segmentation via episodic learning in continuous fre-
quency space. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 1013–
1023, 2021. 2
[30] Jan Modersitzki. FAIR: ﬂexible algorithms for image regis-
tration . SIAM, 2009. 1
[31] Tony CW Mok and Albert CS Chung. Large deformation
diffeomorphic image registration with laplacian pyramid net-
works. In Medical Image Computing and Computer Assisted
Intervention–MICCAI 2020: 23rd International Conference,
Lima, Peru, October 4–8, 2020, Proceedings, Part III 23 ,
pages 211–221. Springer, 2020. 1,2
[32] Kuong-Hon Pong and Kin-Man Lam. Multi-resolution fea-
ture fusion for face recognition. Pattern Recognition , 47(2):
556–567, 2014. 4
[33] Yuyang Qian, Guojun Yin, Lu Sheng, Zixuan Chen, and Jing
Shao. Thinking in frequency: Face forgery detection by min-
ing frequency-aware clues. In European conference on com-
puter vision , pages 86–103. Springer, 2020. 2
[34] Zequn Qin, Pengyi Zhang, Fei Wu, and Xi Li. Fcanet:
Frequency channel attention networks. In Proceedings of
the IEEE/CVF international conference on computer vision ,
pages 783–792, 2021.
[35] Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, and
Jie Zhou. Global ﬁlter networks for image classiﬁcation.
Advances in neural information processing systems , 34:980–
993, 2021. 2
[36] Enrico Segre. Image warp preserving content intensity.
SIAM Journal on Imaging Sciences , 15(4):1623–1645, 2022.
1
[37] Shayan Shekarforoush, David Lindell, David J Fleet, and
Marcus A Brubaker. Residual multiplicative ﬁlter networks
for multiscale reconstruction. Advances in Neural Informa-
tion Processing Systems , 35:8550–8563, 2022. 2
[38] Linlin Shen and Sen Jia. Three-dimensional gabor wavelets
for pixel-based hyperspectral imagery classiﬁcation. IEEE
Transactions on Geoscience and Remote Sensing , 49(12):
5039–5046, 2011. 4
[39] Changjian Shui, Ruizhi Pu, Gezheng Xu, Jun Wen, Fan
Zhou, Christian Gagn ´e, Charles X Ling, and Boyu Wang.
Towards more general loss and setting in unsupervised do-
main adaptation. IEEE Transactions on Knowledge and Data
Engineering , 2023. 2
[40] Vincent Sitzmann, Julien Martel, Alexander Bergman, David
Lindell, and Gordon Wetzstein. Implicit neural representa-
tions with periodic activation functions. Advances in neural
information processing systems , 33:7462–7473, 2020. 2,3,
4,6,8
[41] Sanghyun Son and Kyoung Mu Lee. Srwarp: Generalized
image super-resolution under arbitrary transformation. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 7782–7791, 2021. 1,2,
4,5
[42] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara
Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-
mamoorthi, Jonathan Barron, and Ren Ng. Fourier features
let networks learn high frequency functions in low dimen-sional domains. Advances in Neural Information Processing
Systems , 33:7537–7547, 2020. 2
[43] Haohan Wang, Xindi Wu, Zeyi Huang, and Eric P Xing.
High-frequency component helps explain the generaliza-
tion of convolutional neural networks. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 8684–8694, 2020. 2
[44] Longguang Wang, Yingqian Wang, Zaiping Lin, Jungang
Yang, Wei An, and Yulan Guo. Learning a single net-
work for scale-arbitrary super-resolution. In Proceedings of
the IEEE/CVF international conference on computer vision ,
pages 4801–4810, 2021. 6,7
[45] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu,
Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: En-
hanced super-resolution generative adversarial networks. In
Proceedings of the European conference on computer vision
(ECCV) workshops , pages 0–0, 2018. 4,5
[46] Olivia Wiles, A Koepke, and Andrew Zisserman. X2face: A
network for controlling face generation using images, audio,
and pose codes. In Proceedings of the European conference
on computer vision (ECCV) , pages 670–686, 2018. 1,2
[47] Jun Xiao, Tianshan Liu, Rui Zhao, and Kin-Man Lam.
Balanced distortion and perception in single-image super-
resolution based on optimal transport in wavelet domain.
Neurocomputing , 464:408–420, 2021. 2
[48] Jun Xiao, Xinyang Jiang, Ningxin Zheng, Huan Yang, Yifan
Yang, Yuqing Yang, Dongsheng Li, and Kin-Man Lam. On-
line video super-resolution with convolutional kernel bypass
grafts. IEEE Transactions on Multimedia , 2023. 2
[49] Kai Xu, Minghai Qin, Fei Sun, Yuhao Wang, Yen-Kuang
Chen, and Fengbo Ren. Learning in the frequency domain.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 1740–1749, 2020. 2
[50] Yanchao Yang and Stefano Soatto. Fda: Fourier domain
adaptation for semantic segmentation. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 4085–4095, 2020. 2
[51] Gizem Y ¨uce, Guillermo Ortiz-Jim ´enez, Beril Besbinar, and
Pascal Frossard. A structured dictionary perspective on
implicit neural representations. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 19228–19238, 2022. 3
[52] Pan Zhang, Bo Zhang, Dong Chen, Lu Yuan, and Fang Wen.
Cross-domain correspondence learning for exemplar-based
image translation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
5143–5153, 2020. 1,2
[53] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng
Zhong, and Yun Fu. Image super-resolution using very
deep residual channel attention networks. In Proceedings of
the European conference on computer vision (ECCV) , pages
286–301, 2018. 6,7
3004
