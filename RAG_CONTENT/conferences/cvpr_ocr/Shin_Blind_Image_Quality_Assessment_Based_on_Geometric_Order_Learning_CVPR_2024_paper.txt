Blind Image Quality Assessment Based on Geometric Order Learning
Nyeong-Ho Shin
Korea University
nhshin@mcl.korea.ac.krSeon-Ho Lee
Korea University
seonholee@mcl.korea.ac.krChang-Su Kim
Korea University
changsukim@korea.ac.kr
Abstract
A novel approach to blind image quality assessment,
called quality comparison network (QCN), is proposed in
this paper , which sorts the feature vectors of input imagesaccording to their quality scores in an embedding space.QCN employs comparison transformers (CTs) and scorepivots, which act as the centroids of feature vectors of
similar-quality images. Each CT updates the score pivotsand the feature vectors of input images based on their or-dered correlation. To this end, we adopt four loss func-
tions. Then, we estimate the quality score of a test imageby searching the nearest score pivot to its feature vector inthe embedding space. Extensive experiments show that theproposed QCN algorithm yields excellent image quality as-sessment performances on various datasets. Furthermore,QCN achieves great performances in cross-dataset evalu-ation, demonstrating its superb generalization capability.The source codes are available at https://github.
com/nhshin-mcl8/QCN .
1. Introduction
Image quality assessment (IQA) aims to estimate the human
perceptual quality of an image. It can be divided into twocategories: full-reference IQA and blind IQA (BIQA). Infull-reference IQA, we estimate the quality of an image by
comparing it with its pristine version, referred to as the ref-erence image. On the other hand, in BIQA, we do not usethe reference image. In real-world problems, generally, ref-
erences are unavailable. Hence, the demand for BIQA hasincreased in various applications, including image restora-
tion [ 17], compression [ 16], and super-resolution [ 21].
Recently, many deep learning techniques have been
developed for BIQA, achieving promising performances.Some of them focus on the structural aspect of a deep net-
work for regressing the quality score of an image [ 11,30,
37], while others explore the data aspect of deep learning
and attempt to pre-train networks using a large amount ofdata specialized for BIQA [ 22,27,39]. These techniques
[11,22,27,30,37,39], however, do not explicitly use scoreScore pivot Feature vector Score
Images0 100
Score pivots
CT
CT
CT
QCN Embedding space53.0985.36
 10.03
69.34
Figure 1. Illustration of the proposed QCN algorithm. The color of
an image boundary represents the quality score of the image. TheCT modules sequentially updates the feature vectors of multipleimages. To guide this update process, we use score pivots, whichact as the centroids of the feature vectors of similar-quality images.As the update goes on in the CT modules, the images and the scorepivots are sorted according to their quality scores in the embeddingspace. We estimate the quality score of a test image by ﬁnding the
nearest score pivot to its feature vector.
relations between images, such as ordering relationship and
score difference. Such relations can provide useful cues forthe score estimation. Thus, Golestaneh et al .[7] and Zhang
et al .[38] exploit the relative rank information between im-
ages to train a network, but they use the score relations inthe training phase only.
A novel approach to BIQA is proposed in this paper,
which assesses image quality reliably by exploiting both or-dering relationships and score differences between images.
To this end, we construct an embedding space, in which
the direction and distance between the embedded vectorsof two images represent the ordering relationship and scoredifference between the two images, respectively. The ba-sic concept of this geometric representation learning, called
geometric order learning (GOL) [ 14], has been proposed
recently for rank estimation tasks, including facial age esti-mation and historical image classiﬁcation. However, GOLmay provide poor results for BIQA since it is designed for
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
12799
discrete rank estimation. In contrast, in BIQA, we should
estimate the continuous quality score of an image.
In this paper, we ﬁrst develop the GOL-based algo-
rithm for BIQA, called quality comparison network (QCN),which arranges input images according to their qualityscores in the embedding space, as illustrated in Figure 1.
The proposed QCN employs multiple comparison trans-
formers (CTs) and score pivots, which act as the centroids
of feature vectors of similar-quality images. In each CT, thescore pivots and the feature vectors of input images are up-dated according to their ordered correlation. To train CTsto achieve this goal, we adopt four loss functions. Then,given a test image, we estimate its quality score by ﬁnding
the nearest score pivot to its feature vector in the embedding
space. Extensive experiments show that the proposed QCNprovides excellent performances on various datasets.
This work has the following major contributions:
• We propose the ﬁrst BIQA algorithm based on geometric
order learning, called QCN.
• We develop the CT networks to construct an effective em-
bedding space, in which the feature vectors of images aresorted according to their quality scores.
• QCN achieves excellent performances on various BIQA
datasets, including BID [ 2], CLIVE [ 5], KonIQ10K [ 9],
SPAQ [ 4], and FLIVE [ 34]. Furthermore, QCN provides
superb performances in cross-dataset evaluation, demon-strating its good generalization capability.
2. Related Work
2.1. Blind Image Quality Assessment
Recently, with the success of deep learning in diverse vi-sion tasks, various deep-learning-based BIQA techniqueshave been proposed. Both network design and network pre-training have been researched for BIQA.
Network design: Several network structures have been de-
veloped for reliable BIQA. Zhang et al .[37] employed two
different encoders to extract image distortion types and im-age contents, respectively. Su et al .[30] designed the local
distortion aware module to identify local distortions in an
image. Ke et al .[11] developed a transformer-based en-
coder to extract the distortion and content features by pre-serving the aspect ratio and composition of an image. How-ever, these algorithms do not explicitly employ the orderingrelations and score differences between images. Hence, forbetter BIQA, Golestaneh et al .[7] and Zhang et al .[38]
employed the ranking loss to train a network, but these al-gorithms exploit the ranking relations in the training only,
not in the test.
For accurate quality score estimation of images based on
the ordering relations and score differences, we propose anovel network architecture for constructing an embeddingspace, in which such relations are well preserved.Network pre-training: On the other hand, pre-training
schemes have been developed for BIQA. Madhusudana et
al.[22] learned to cluster images based on their distortion
degrees via self-supervised learning. Also, to extract thecontent and distortion information, Saha et al .[27] pro-
posed a self-supervised learning algorithm that pre-trainsthe content-aware encoder and the quality-aware encoder.Zhao et al .[39] applied various types of distortions to im-
ages to adopt contrastive learning in the BIQA task.
These pre-training schemes, however, demand signiﬁ-
cant training time and computational power. On the con-trary, without such pre-training, the proposed QCN pro-vides competent BIQA performances.
2.2. Order Learning
Order learning aims to predict the rank of an object by com-paring it with multiple references with known ranks, and itstechniques have been developed mostly for facial age esti-mation [ 12–14,18,28]. Lim et al .[18] ﬁrst proposed the
notion of order learning. For more reliable comparisons,Lee and Kim [ 12] performed the order-identity decomposi-
tion and selected references with similar identity features.Shin et al .[28] developed a regression approach to order
learning. In practice, ordering relationships may be knownfor a limited amount of training data, so Lee et al .[13]
developed a weakly-supervised training scheme for orderlearning. However, these algorithms should conduct mul-
tiple comparisons with many references of different ranks,since they consider only relative priorities between objects.To overcome this issue, Lee et al .[14] proposed GOL,
which exploits metric relations, as well as order relations,among objects. GOL predicts the rank of an object via asimplek-NN search.
In this work, we adopt the concept of GOL but propose a
novel transformer-based network for estimating the contin-uous quality score of an image. Note that the original GOLwas designed for estimating discrete ranks by employing aconvolutional neural network.
3. Proposed Algorithm
3.1. Problem Deﬁnition
Given an input image x, the objective of BIQA is to estimate
its quality score θ(x). Note that an order and score differ-
ences provide useful information for quality score estima-tion; they convey complementary information. Suppose thatthere are three images x,y, andz, and their quality scores
are 5, 10, and 20, respectively; θ(x)=5 ,θ(y)=1 0 , and
θ(z)=2 0 . Sinceθ(x)<θ(y)<θ(z), the order indi-
cates that xandzshould be located at opposite sides with
respect to yin a well-designed embedding space. On the
other hand, since |θ(x)−θ(y)|<|θ(y)−θ(z)|, the score
differences indicate that xshould be closer from ythanzis
12800
Input imagesScore pivots ݔܲ଴,ݔଵ,ڮ,ݔேିଵ
Embedding spaceEncoderComparison
Transformer (CT)Comparison
Transformer (CT)Comparison
Transformer (CT)ܪഥܪ
തܲ
Score
0 100Score pivot Feature vector
Figure 2. An overview of the proposed QCN algorithm: Given Nimages, the encoder extracts their feature vectors (or tokens) and forms
the token matrix H. Then, the three CTs sequentially update the feature vectors, together with learnable score pivots in P, and yield the
updated token matrix ¯Hand the updated pivot matrix ¯P. As a result, the updated feature vectors and score pivots are arranged according
to their quality scores in the embedding space.
fromyin the embedding space.
To exploit such order and score differences for BIQA,
we construct an embedding space, in which the ordering re-lationships and the score differences are reﬂected by the di-rections and the distances between feature vectors, respec-tively. In other words, we attempt to sort the images in theembedding space according to their quality scores.
3.2. QCN
We develop QCN to construct an embedding space, wherethe feature vectors of Nimages,x
0,x1,...,x N−1, from a
training set Xare arranged according to their quality scores.
QCN is composed of an encoder and three CTs, as shown
in Figure 2.
We adopt ResNet50 [ 8] as the encoder backbone. The
encoder transforms the Nimages into feature vectors
hx0,hx1,...,h xN−1∈RC. Then, we form the token ma-
trix
H=[hx0,hx1,...,h xN−1]t∈RN×C. (1)
Next, each CT updates the feature vectors (or tokens)
inHto sort them according to their quality scores. To
guide this update process, we introduce Mscore pivots
p0,p1,...,p M−1, which are learnable parameters function-
ing as the centroids of feature vectors of similar-quality im-ages. Thus, the ﬁrst CT takes the learnable pivot matrix
P=[p
0,p1,...,p M−1]t∈RM×C(2)
as input. For example, if the score range is [0,100] and
M=1 1 , these eleven pivots play the role of the representa-
tive images of scores 0,10,...,100 in the case of uniform
quantization. Through the three CTs, the Nimages and the
Mpivots are arranged in the embedding space, as illustrated
in Figure 2. The ﬁnal CT yields the updated token matrix
¯Hand the updated pivot matrix ¯P.
3.3. CT
As in Figures 3, each CT comprises four modules: fea-
ture self-update (FSU), feature-pivot cross-update (FPCU),pivot self-update (PSU), and pivot-feature cross-update(PFCU) modules.ܪ
Pivot
self-updateFeature 
self-update
Feature-pivot
cross-updatePivot-feature
cross-updateܳ
ܭ
ܸ
ܪܸܭܳܲԢ ܳ
ܲᇱܳ
ܭ
ܸܸܭ
כܪכܲ
Figure 3. A block diagram of a CT.
FSU module: To construct an embedding space in which
the score relations between input images are reﬂected, we
should analyze the correlation among the images. To thisend, we apply the masked self-attention [ 33]t oH.
First, we obtain query Q
H,k e yKH, valueVHby
QH=HUt
q,K H=HUt
k,V H=HUt
v, (3)
using projection matrices Ut
q,Ut
k,Ut
v∈RC×C. Then, we
analyze the correlation via
AM= softmax/parenleftbig
QHKt
H+M/parenrightbig
(4)
whereM∈ RN×Nis a mask whose (i,j)th element is
0i fi/negationslash=j, and−∞ ifi=j. By employing M, each
feature vector is compared with all the others, excludingitself. Notice that Q
HKt
Hin (4) computes the correlation
between images. Then, as in Figure 4(a), the FSU module
updates each feature vector based on the correlation by
H/prime= MaskedAttention( QH,KH,VH,M)
=φ(AMVH+H),(5)
whereφis a feedforward network.
FPCU module: In BIQA, even images with similar scores
may have signiﬁcantly different distortions. To cope withthis problem, we use score pivots to guide the grouping ofimages with similar scores in the embedding space. TheFPCU module updates the pivot matrix Pby considering
H
/primefrom the FSU module.
First, we obtain QPfromPandKH/prime,VH/primefromH/prime.
Then, we perform the cross-attention to yield
P/prime= Attention( QP,KH/prime,VH/prime)=φ(AVH/prime+P)(6)
12801
Score
0 100Score pivot Feature vector
Initial embedding spacePSU PFCU FSU FPCU
(a) (b) (c) (d)
Figure 4. Illustration of the transition of feature vectors and score pivots in each CT. The feature vectors and score pivots are aligned
according to their scores, as they pass through the four modules of FSU, FPCU, PSU, and PFCU.
where
A= softmax/parenleftbig
QPKt
H/prime/parenrightbig
(7)
is the cross-correlation matrix. In ( 6), the score pivots are
updated using the feature vectors, as shown in Figure 4(b).
PSU module: The score pivots should be also ordered.
Thus, we ﬁrst obtain QP/prime,KP/prime, andVP/primefromP/prime. Then,
we perform the self-attention on the pivots in P/primeand yield
the updated pivot matrix
P∗= Attention( QP/prime,KP/prime,VP/prime) (8)
as illustrated in Figure 4(c).
PFCU module: Finally, we update H/primebased on P∗.W e
obtainQH/primefromH/primeandKP∗,VP∗fromP∗. Then, we
apply the cross-attention to the updated token matrix
H∗= Attention( QH/prime,KP∗,VP∗) (9)
as in Figure 4(d).
3.4. Loss Functions
To arrange input images according to their ordering rela-
tionships and score differences, we train QCN with the or-
der loss and the metric loss. Also, for accurate score estima-
tion, we employ the center loss and the mean absolute error(MAE) loss. Note that we compute these four losses on ¯H
and¯P, which are the output of the last CT in Figure 2.
Order loss: We design the order loss to arrange the score
pivots according to their order. Let us deﬁne the directionvectorv(r,s)from point rto pointsin the embedding space
as
v(r,s)=s−r
/bardbls−r/bardbl. (10)
Then, we deﬁne the order loss as
Lorder=/summationtextM−2
m=1v(¯pm,¯pm−1)tv(¯pm,¯pm+1). (11)
To minimize this term, the angle between v(¯pm,¯pm−1)and
v(¯pm,¯pm+1)should be maximized as shown in Figure 5(a),(a)Score
0 100Score pivot Feature vector
Order lossݎ௜ାଶ
Angle maximizationҧ݌௠ҧ݌௠ିଵҧ݌௠ାଵ
ҧ݌௠ିଶҧ݌௠ାଶ
Center lossത݄௫
Attractionҧ݌௠ҧ݌௠ିଵҧ݌௠ାଵ
ҧ݌௠ିଶ෤݌௫ҧ݌௠ାଶ
Interpolation
(b)
Figure 5. Computation of (a) the order loss and (b) the center loss.
which means that the three consecutive pivots ¯pm−1,¯pm,
and¯pm+1should be aligned on a line.
Metric loss: Note that the order loss in ( 11) considers each
triple of consecutive score pivots, thus it attempts to arrangethe score pivots locally. To consider the global relationship
of all pivots as well, we adopt the metric constraint in [ 14]
as the metric loss.
Center loss: In the embedding space, the feature vector ¯h
x
of an image should be near its corresponding score pivot.
However, since we use a ﬁnite number of score pivots torepresent the continuous score range, there may be no scorepivot exactly matching ¯h
x. Hence, we ﬁrst obtain a linearly
interpolated score pivot
˜px=(θ(¯pm+1)−θ(x))¯pm+(θ(x)−θ(¯pm))¯pm+1
θ(¯pm+1)−θ(¯pm)
(12)
whereθ(¯pm)≤θ(x)≤θ(¯pm+1). Note that ˜pxis an inter-
nally dividing point between the two nearest pivots in termsof quality scores, as in Figure 5(b). Then, we attempt to
minimize the distance /bardbl¯h
x−˜px/bardbl. These distances are com-
puted for all feature vectors in ¯H, and the center loss is
deﬁned as
Lcenter=/summationtextN−1
n=0/bardbl¯hxn−˜pxn/bardbl. (13)
12802
Score
0 100Score pivot Feature vector
Feature vector projectionത݄௫
ҧ݌௠ ҧ݌௠ାଵҧ݌௫
Projection
Searching the 2 NN score pivotsҧ݌௠ାଶ
ҧ݌௠ҧ݌௠ିଵҧ݌௠ାଵ
ҧ݌௠ିଶത݄௫
NN score pivots
(a) (b)
Figure 6. To estimate the score of image x, we ﬁrst ﬁnd the two
nearest neighbor (NN) score pivots ¯pmand¯pm+1in (a), and then
project the feature vector ¯hxonto the line from ¯pmto¯pm+1in (b).
MAE loss: We estimate the score of a test instance using
the rule in Section 3.5. To minimize the difference between
the estimate score ˆθ(xn)and the ground-truth score θ(xn)
for eachxn, we adopt the smooth MAE loss Lmae in [6].
Finally, we minimize the overall loss function
L=Lorder+Lmetric+Lcenter+Lmae (14)
to optimize the network parameters in QCN and learn the
score pivots in P.
3.5. Score Estimation
Given an unseen test image x, we estimate its quality score
by applying it together with N−1auxiliary images into
QCN. We select the N−1auxiliary images from the train-
ing setX. More speciﬁcally, we ﬁrst divide the entire score
range uniformly into the N−1intervals. Then, we ran-
domly select an image from each interval. It is shown inthe supplemental document that the score estimation per-formance is not sensitive to this random selection.
Then, QCN yields the feature vector ¯h
xof the test image
and the score pivots in ¯P, which are aligned in the embed-
ding space. Note that the feature vectors of the auxiliaryimages are not employed in the score estimation. Then, we
ﬁnd the adjacent pair of score pivots ¯p
mand¯pm+1mini-
mizing the sum /bardbl¯hx−¯pm/bardbl+/bardbl¯hx−¯pm+1/bardbl, as illustrated
in Figure 6(a). In other words, we search the two nearest
neighbor (NN) pivots of ¯hxin terms of Euclidean distances.
Then, we project ¯hxonto the line from ¯pmto¯pm+1,a si n
Figure 6(b). Hence, the projected point is given by
¯px=¯pm+α(¯pm+1−¯pm), (15)
where
α=(¯hx−¯pm)t(¯pm+1−¯pm)
/bardbl¯pm+1−¯pm/bardbl2. (16)
Then, the score of xis estimated by
ˆθ(x)=θ(¯pm)+α/parenleftbig
θ(¯pm+1)−θ(¯pm)/parenrightbig
. (17)
CLIVE [ 5]MOS# of images
# of images
# of images
MOS MOS
KonIQ10K [ 9] SPAQ [ 4]
Figure 7. MOS histograms of three BIQA datasets.
4. Experimental Results
4.1. Implementation
Training details: We initialize the encoder using ResNet50
pre-trained on ILSVRC2012 [ 3]. We use the AdamW op-
timizer [ 20] with a batch size of 54 and a weight decay of
5×10−4. We set the learning rate to 5×10−5initially and
decrease it using the cosine annealing learning rate sched-uler. By default, the number Mof score pivots and the num-
berNof input images are set to be 101 and 18, respectively.
In IQA, changing the aspect ratio and composition of an im-
age may impact the image quality. Hence, as done in [ 11],
we preserve the aspect ratio of an image during both train-ing and testing. Speciﬁcally, we resize the short side of animage to 384 while maintaining the aspect ratio. For eval-uation, we estimate the quality score of an image and itshorizontally ﬂipped version. Then, we average the predic-
tion scores of the two images. More details are available in
the supplemental document.
Non-uniform score pivot generation: We quantize the en-
tire score range to Mreconstruction levels, which are as-
signed to the Mpivots as the scores. In general, the distri-
bution of quality scores in a BIQA dataset is not uniform, asshown in Figure 7. Hence, to minimize quantization errors,
we adopt the Lloyd-Max algorithm [ 19], instead of uniform
quantization. The impacts of this non-uniform quantization
will be analyzed in Section 4.4.
4.2. Datasets and Evaluation Protocol
We use ﬁve IQA datasets to assess the performance of the
proposed QCN.
• BID [ 2]: It provides 586 images with blur artifacts, e.g.,
due to out-of-focus, complex motion, and simple motion.
• CLIVE [ 5]: It contains 1,162 images in diverse categories
taken from different cameras.
• KonIQ10K [ 9]: It consists of 10,073 images selected
from YFCC-100M [ 31] to cover various types of distor-
tions.
•S P A Q [ 4]: It provides 11,125 photos taken with 66 smart-
phones.
• FLIVE [ 34]: It is one of the largest BIQA datasets, which
contains about 40K images and 120K patches. As donein [22,34,39], we only use the images, not the patches,
for both training and testing.
12803
Table 1. Comparison of BIQA results on the BID, CLIVE, KonIQ10k, SPAQ, and FLIVE datasets. Pre-training algorithms are marked
with the asterisk *. The best results are boldfaced, and the second-best are underlined.
BID CLIVE KonIQ10k SPAQ FLIVE
Algorithm SRCC PCC SRCC PCC SRCC PCC SRCC PCC SRCC PCC
NIQE [ 25] 0.477 0.471 0.454 0.468 0.526 0.475 0.697 0.685 0.105 0.141
ILNIQE [ 36] 0.495 0.454 0.453 0.511 0.503 0.496 0.719 0.654 0.219 0.255
BRISQUE [ 24] 0.574 0.540 0.601 0.621 0.715 0.702 0.802 0.806 0.320 0.356
BMPRI [ 23] 0.515 0.458 0.487 0.523 0.658 0.655 0.750 0.754 0.274 0.315
CNNIQA [ 10] 0.616 0.614 0.627 0.601 0.685 0.684 0.796 0.799 0.306 0.285
WaDIQaM-NR [ 1] 0.653 0.636 0.692 0.730 0.729 0.754 0.840 0.845 0.435 0.430
PQR [ 35] 0.775 0.794 0.857 0.882 0.880 0.884 - - - -
SFA [ 15] 0.820 0.825 0.804 0.821 0.888 0.897 0.906 0.907 0.542 0.626
DB-CNN [ 37] 0.845 0.859 0.844 0.862 0.878 0.887 0.910 0.913 0.554 0.652
HyperIQA [ 30] 0.869 0.878 0.859 0.882 0.906 0.917 0.916 0.919 0.535 0.623
PaQ-2-PiQ [ 34] - - 0.840 0.850 0.870 0.880 - - 0.571 0.623
UNIQUE [ 38] 0.858 0.873 0.854 0.890 0.896 0.901 - - - -
MUSIQ [ 11] - - - - 0.916 0.928 0.917 0.921 0.646 0.739
TReS [ 7] - - 0.846 0.877 0.915 0.928 - - 0.554 0.625
CONRTIQUE* [ 22] - - 0.845 0.857 0.894 0.906 0.914 0.919 0.580 0.641
Re-IQA* [ 27] - - 0.840 0.854 0.914 0.923 0.918 0.925 0.645 0.733
QPT* [ 39] 0.888 0.911 0.895 0.914 0.927 0.941 0.925 0.928 0.610 0.677
Proposed QCN 0.892 0.890 0.875 0.893 0.934 0.945 0.923 0.928 0.644 0.741
Table 2. Cross-dataset evaluation results in SRCC. The ﬁrst and second rows specify the training and test datasets, respectively.
BID CLIVE KonIQ10k
Algorithm CLIVE KonIQ10K BID KonIQ10K BID CLIVE
DBCNN [ 37] 0.725 0.724 0.762 0.754 0.816 0.755
PQR [ 35] 0.680 0.636 0.714 0.757 0.755 0.770
HyperIQA [ 30] 0.770 0.688 0.756 0.772 0.819 0.785
TReS [ 7] - - - 0.733 - 0.786
CONTRIQUE* [ 22] - - - 0.676 - 0.731
Re-IQA* [ 27] - - - 0.769 - 0.791
QPT* [ 39] - - 0.845 0.749 0.825 0.821
Proposed QCN 0.800 0.730 0.886 0.784 0.847 0.840
We adopt the Spearman’s rank correlation coefﬁ-
cient (SRCC) [ 29] and Pearson’s correlation coefﬁcient
(PCC) [ 26] metrics. SRCC and PCC measure how well a
network sorts images according to their ranks and scores,respectively.
For the BID, CLIVE, KonIQ10K, and SPAQ datasets,
we randomly split each dataset into train and test sets witha ratio of 8:2. Then, we repeat the training and testing for
10 different splits and report the median SRCC and PCCscores, as done in [ 11,27,30,39]. For FLIVE, we employ
the same evaluation protocol as in [ 22,34,39] — 30K im-
ages for training and 1.8K images for testing.
4.3. Comparative Assessment
Table 1compares the performances of the proposed QCN
with those of conventional algorithms on the ﬁve IQAdatasets. Note that the pre-training algorithms [ 22,27,39]are listed separately in the middle section of the table.
Comparison with network design techniques: The pro-
posed QCN is one of the network design techniques. Wesee that, in Table 1, QCN outperforms all conventional net-
work design techniques in 9 out of 10 tests.
Compared with MUSIQ [ 11], which is the state-of-the-
art in the network design approach, QCN improves theSRCC and PCC performances by 1.97% and 1.83%, respec-tively, on KonIQ10K. Also, on FLIVE, which is a challeng-ing dataset with various types of distortions, QCN yieldscomparable and better results than MUSIQ. It is worthpointing out that, while we use only 30K training imagesfor FLIVE as in [ 22,34,39], MUSIQ exploits 90K training
patches additionally to boost their performances on FLIVE.
Comparison with network pre-training techniques:
Even without pre-training the network, the proposed QCNprovides competent results to the pre-training techniques.
12804
: Score pivot : Feature vector0 100 0 100
Initial embedding space 1stupdate 2ndupdate Final embedding spaceCT CT CTScores
Figure 8. t-SNE visualization [ 32] of feature vectors and score pivots for the KonIQ10K dataset in each CT. We depict the scores of the
score pivots and the feature vectors in red and blue shades, respectively.
Table 3. Ablation studies for the FSU and PSU modules in QCN
on the KonIQ10K dataset.
Method FPCU / PFCU FSU PSU SRCC PCC
I /check 0.858 0.849
II /check/check 0.916 0.881
III /check/check 0.930 0.939
IV /check/check /check 0.934 0.945
In Table 1, it provides better results than those techniques
in 5 out of 10 tests.
Compared with the state-of-the-art QPT [ 39], QCN im-
proves the results by 0.76% in SRCC and 0.43% in PCCon KonIQ10K. Note that the pre-training is beneﬁcial, es-pecially for small datasets. However, even on BID, whichcontains only about 470 training images, QCN yields com-parable results to QPT.
Cross-dataset evaluation: Table 2compares cross-dataset
evaluation results. Even without pre-training, QCN per-forms the best in all 6 tests. Also, in the challenging combi-nation of training on CLIVE (1,162 images) and testing onKonIQ10K (10,073 images), QCN outperforms the second-
best HyperIQA [ 30] by 1.55%. This indicates that QCN has
better generalization capability than the other algorithms,including the pre-training techniques.
4.4. Analysis
Efﬁcacy of FSU and PSU modules: We conduct ablation
studies to analyze the efﬁcacy of the FSU and PSU modulesin a CT in Figure 3. In Table 3, we compare ablated meth-
ods on the KonIQ10K dataset. Method I uses the FPCUand PFCU modules only. In II and III, FSU and PSU areadditionally used, respectively.
Compared with the full QCN in IV, method I degrades
the results severely. By employing FSU and PSU, II and IIIperform better than method I, but the gaps with IV are stilllarge. Both FSU and PSU modules are essential for reliableTable 4. Ablation studies for the loss functions in ( 14)o nt h e
KonIQ10K dataset.
Method Lmae Lcenter Lmetric Lorder SRCC PCC
I /check 0.855 0.866
II /check/check 0.860 0.877
III /check/check /check 0.929 0.939
IV /check/check /check /check 0.934 0.945
feature vector arrangement.
Loss functions: Table 4compares ablated methods for the
loss terms in ( 14). Method I employs only Lmae. In II, III,
and IV, we additionally use Lcenter ,Lmetric , andLorder in
that order.
From I and II, we see that Lcenter improves the results,
by encouraging the feature vector of an input to be located
near its corresponding score pivots. However, comparedwith the proposed QCN in IV, methods I and II yield infe-rior results, for we cannot sort feature vectors meaningfullywithoutL
order andLmetric . By employing Lmetric , III out-
performs II signiﬁcantly. Also, by comparing IV with III,we see that L
order further improves the results.
Embedding space visualization: Figure 8visualizes how
feature vectors and score pivots for KonIQ10K are alignedthrough the three CTs. The t-SNE method [ 32] is used for
the visualization. Note that they are gradually arranged andseparated according to their scores, as the update goes on.
Performance according to N:Table 5compares the
results according to the number Nof input images on
KonIQ10K. Without auxiliary images ( N=1 ), the per-
formance degrades severely because we cannot exploit the
score relations between images. As Nincreases, the perfor-
mance gets better but saturates around the default N=1 8 .
Performance according to T:Table 6compares the results
according to the number Tof CTs on KonIQ10K. The best
12805
Table 5. Comparison of the performances according to the number
Nof input images on the KonIQ10K dataset.
N 1 6 12 18 24
SRCC 0.909 0.928 0.931 0.935 0.934
PCC 0.924 0.940 0.942 0.945 0.944
Table 6. Comparison of the performances according to the number
Tof CTs on the KonIQ10K dataset.
T 12345
SRCC 0.928 0.930 0.934 0.933 0.933
PCC 0.939 0.941 0.945 0.943 0.943
Table 7. Comparison of the SRCC and PCC scores on KonIQ10K
and SPAQ according to the score pivot generation schemes.
KonIQ10K SPAQ
SRCC PCC SRCC PCC
Uniform 0.929 0.941 0.914 0.919
Non-uniform 0.934 0.945 0.923 0.928
Table 8. Comparison of QCN with GOL [ 14] on the KonIQ10K
and SPAQ datasets.
KonIQ10K SPAQ
SRCC PCC SRCC PCC
GOL [ 14] 0.918 0.909 0.908 0.907
Proposed QCN 0.934 0.945 0.923 0.928
results are achieved at the default T=3.
Non-uniform score pivot generation: We use the Lloyd-
Max algorithm to quantize the scores of pivots non-uniformly. Table 7compares this scheme with the uniform
quantization on the KonIQ10K and SPAQ datasets. We seethat the non-uniform quantization yields better results thanthe uniform quantization on both datasets, so it is used asthe default mode.
Comparison with geometric order learning: Table 8
compares the proposed QCN with the GOL algorithm [ 14]
on the KonIQ10K and SPAQ datasets. Since GOL is de-signed for discrete rank estimation, it may fail to yield ac-curate score predictions. Therefore, QCN performs betterthan GOL in BIQA.
Testing time: To estimate the quality score of a test image,
the proposed QCN exploits auxiliary images, which are se-lected from a training set. For efﬁciency, we extract thefeatures of all auxiliary images in advance. Hence, duringthe test, the feature extraction of the auxiliary images is notrequired. We measure the testing time on KonIQ10K us-ing an RTX 3090 GPU. QCN takes only 0.033s to test animage on average: 0.006s for the feature extraction, 0.001sfor the auxiliary image selection, and 0.026s for the score69.73 (80.29) 69.59 (80.88)
71.29 (58.80) 69.80 (56.46)
Figure 9. Failure cases of the proposed QCN algorithm. For each
image, the predicted score is reported with the ground truth withinthe parentheses.
estimation. Hence, QCN is feasible for many use cases.
Failure cases: Figure 9shows some failure cases of the
proposed QCN. The ﬁrst row shows images underrated by
QCN. In these cases, QCN may estimate the scores by fo-
cusing on the blurred or poorly illuminated background,while the annotators may rate the quality scores by focusingon the standing out composition of the foreground objects.On the other hand, the second row shows images overratedby QCN. In these cases, QCN and the annotators may de-
termine the qualities based on clear background and blurred
foreground, respectively.
5. Conclusions
We proposed a novel BIQA algorithm, called QCN, whicharranges the feature vectors of images based on their quality
scores in the embedding space. First, we designed the CTmodule to update feature vectors to sort them according totheir quality scores. Second, to guide this update process,we introduced score pivots. Third, we employed the fourlosses to arrange the feature vectors meaningfully. Lastly,
we predicted the quality score of a test image by ﬁnding the
nearest score pivot to its feature vector in the embedding
space. Extensive experiments on various BIQA datasetsshowed that QCN provides excellent performance. Further-more, QCN demonstrated its great generalization capabilityin cross-dataset evaluation.
Acknowledgements
This work was supported by the NRF grants funded by theKorea government (MSIT) (No. NRF-2021R1A4A1031864and No. NRF-2022R1A2B5B03002310).
12806
References
[1] Sebastian Bosse, Dominique Maniry, Klaus-Robert M ¨uller,
Thomas Wiegand, and Wojciech Samek. Deep neural net-works for no-reference and full-reference image quality as-sessment. IEEE TIP , 27:206–219, 2017. 6
[2] Alexandre Ciancio, Eduardo AB da Silva, Amir Said, Ramin
Samadani, and Pere Obrador. No-reference blur assessment
of digital pictures based on multifeature classiﬁers. IEEE
TIP, 20:64–75, 2010. 2,5
[3] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. ImageNet: A large-scale hierarchical imagedatabase. In CVPR , 2009. 5
[4] Y uming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, and Zhou
Wang. Perceptual quality assessment of smartphone photog-raphy. In CVPR , 2020. 2,5
[5] Deepti Ghadiyaram and Alan C. Bovik. Massive online
crowdsourced study of subjective and objective picture qual-ity. IEEE TIP , 25:372–7387, 2015. 2,5
[6] Ross Girshick. Fast R-CNN. In ICCV , 2015. 5
[7] S. Alireza Golestaneh, Saba Dadsetan, and Kris M. Kitani.
No-reference image quality assessment via transformers, rel-ative ranking, and self-consistency. In WACV , 2022. 1,2,6
[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
2015. 3
[9] Vlad Hosu, Hanhe Lin, Tamas Sziranyi, and Dietmar Saupe.
KonIQ-10k: An ecologically valid database for deep learn-
ing of blind image quality assessment. IEEE TIP , 29:4041–
4056, 2020. 2,5
[10] Le Kang, Peng Ye, Yi Li, and David Doermann. Convolu-
tional neural networks for no-reference image quality assess-ment. In CVPR , 2014. 6
[11] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and
Feng Yang. MUSIQ: Multi-scale image quality transformer.InICCV , 2021. 1,2,5,6
[12] Seon-Ho Lee and Chang-Su Kim. Deep repulsive clustering
of ordered data based on order-identity decomposition. In
ICLR , 2021. 2
[13] Seon-Ho Lee and Chang-Su Kim. Order learning using par-
tially ordered data via chainization. In ECCV , 2022. 2
[14] Seon-Ho Lee, Nyeong-Ho Shin, and Chang-Su Kim. Geo-
metric order learning for rank estimation. In NeurIPS , 2022.
1,2,4,8
[15] Dingquan Li, Tingting Jiang, Weisi Lin, and Ming Jiang.
Which has better visual quality: The clear blue sky or a
blurry animal? IEEE TMM , 21:1221–1234, 2018. 6
[16] Yang Li, Shiqi Wang, Xinfeng Zhang, Shanshe Wang, Si-
wei Ma, and Y ue Wang. Quality assessment of end-to-endlearned image compression: The benchmark and objectivemeasure. In ACM MM , 2021. 1
[17] Haoyi Liang and Daniel S. Weller. Comparison-based image
quality assessment for selecting image restoration parame-
ters. IEEE TIP , 25:5118–5130, 2016. 1
[18] Kyungsun Lim, Nyeong-Ho Shin, Y oung-Y oon Lee, and
Chang-Su Kim. Order learning and its application to ageestimation. In ICLR , 2020. 2[19] Stuart Lloyd. Least squares quantization in PCM. TIT, 28:
129–137, 1982. 5
[20] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 , 2017. 5
[21] Chao Ma, Chih-Y uan Yang, Xiaokang Yang, and Ming-
Hsuan Yang. Learning a no-reference quality metric forsingle-image super-resolution. CVIU , 158:1–16, 2017. 1
[22] Pavan C. Madhusudana, Neil Birkbeck, Yilin Wang, Balu
Adsumilli, and Alan C. Bovik. Image quality assessmentusing contrastive learning. IEEE TIP , 31:4149–4161, 2022.
1,2,5,6
[23] Xiongkuo Min, Guangtao Zhai, Ke Gu, Y utao Liu, and Xi-
aokang Yang. Blind image quality estimation via distortionaggravation. IEEE TB , 64:508–517, 2018. 6
[24] Anish Mittal, Anush K. Moorthy, and Alan C. Bovik. No-
reference image quality assessment in the spatial domain.IEEE TIP , 21:4695–4708, 2012. 6
[25] Anish Mittal, Rajiv Soundararajan, and Alan C. Bovik. Mak-
ing a “completely blind” image quality analyzer. IEEE Sign.
Process. Letters , 20:209–212, 2012. 6
[26] Karl Pearson. Determination of the coefﬁcient of correlation.
Science , 30:23–25, 1909. 6
[27] Avinab Saha, Sandeep Mishra, and Alan C. Bovik. Re-IQA:
Unsupervised learning for image quality assessment in thewild. In CVPR , 2023. 1,2,6
[28] Nyeong-Ho Shin, Seon-Ho Lee, and Chang-Su Kim. Moving
window regression: A novel approach to ordinal regression.InCVPR , 2022. 2
[29] Charles Spearman. Footrule for measuring correlation.
British Journal of Psychology , 2:89, 1906. 6
[30] Shaolin Su, Qingsen Yan, Y u Zhu, Cheng Zhang, Xin Ge,
Jinqiu Sun, and Yanning Zhang. Blindly assess image qual-ity in the wild guided by a self-adaptive hyper network. InCVPR , 2020. 1,2,6,7
[31] Bart Thomee, David A. Shamma, Gerald Friedland, Ben-
jamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, andLi-Jia Li. YFCC100M: The new data in multimedia research.Communications of the ACM , 59:64–73, 2016. 5
[32] Laurens V an der Maaten and Geoffrey Hinton. Visualizing
data using t-SNE. Journal of machine learning research
,9
(11):2579–2605, 2008. 7
[33] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and IlliaPolosukhin. Attention is all you need. In NeurIPS , 2017. 3
[34] Zhenqiang Ying, Haoran Niu, Praful Gupta, Dhruv Mahajan,
Deepti Ghadiyaram, and Alan Bovik. From patches to pic-tures (PaQ-2-PiQ): Mapping the perceptual space of picturequality. In CVPR , 2020. 2,5,6
[35] Hui Zeng, Lei Zhang, and Alan C Bovik. A probabilistic
quality representation approach to deep blind image qualityprediction. arXiv preprint arXiv:1708.08190 , 2017. 6
[36] Lin Zhang, Lei Zhang, and Alan C. Bovik. A feature-
enriched completely blind image quality evaluator. IEEE
TIP, 8:2579–2591, 2012. 6
[37] Weixia Zhang, Kede Ma, Jia Yan, Dexiang Deng, and Zhou
Wang. Blind image quality assessment using a deep bilin-ear convolutional neural network. IEEE TCSVT , 30:36–47,
2018. 1,2,6
12807
[38] Weixia Zhang, Kede Ma, Guangtao Zhai, and Xiaokang
Yang. Uncertainty-aware blind image quality assessment in
the laboratory and wild. IEEE TIP , 30:3474–3486, 2021. 1,
2,6
[39] Kai Zhao, Kun Y uan, Ming Sun, Mading Li, and Xing Wen.
Quality-aware pre-trained models for blind image quality as-sessment. In CVPR , 2023. 1,2,5,6,7
12808
