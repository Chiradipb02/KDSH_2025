LLM4SGG: Large Language Models for Weakly SupervisedScene Graph GenerationKibum Kim1Kanghoon Yoon1Jaehyeong Jeon1Yeonjun In1Jinyoung Moon2Donghyun Kim3Chanyoung Park1*1KAIST2ETRI3Korea University{kb.kim,ykhoon08,wogud405,yeonjun.in,cy.park}@kaist.ac.kr jymoon@etri.re.kr, d_kim@korea.ac.krAbstractWeakly-Supervised Scene Graph Generation (WSSGG)research has recently emerged as an alternative to the fully-supervised approach that heavily relies on costly annota-tions. In this regard, studies on WSSGG have utilized im-age captions to obtain unlocalized triplets while primarilyfocusing on grounding the unlocalized triplets over imageregions. However, they have overlooked the two issues in-volved in the triplet formation process from the captions:1) Semantic over-simpliﬁcation issue arises when extractingtriplets from captions, where ﬁne-grained predicates in cap-tions are undesirably converted into coarse-grained predi-cates, resulting in a long-tailed predicate distribution, and2) Low-density scene graph issue arises when aligning thetriplets in the caption with entity/predicate classes of inter-est, where many triplets are discarded and not used in train-ing, leading to insufﬁcient supervision. To tackle the twoissues, we propose a new approach, i.e.,LargeLanguageModel for weakly-supervisedSGG(LLM4SGG), where wemitigate the two issues by leveraging the LLM’s in-depthunderstanding of language and reasoning ability during theextraction of triplets from captions and alignment of en-tity/predicate classes with target data. To further engagethe LLM in these processes, we adopt the idea of Chain-of-Thought and the in-context few-shot learning strategy.To validate the effectiveness ofLLM4SGG, we conduct ex-tensive experiments on Visual Genome and GQA datasets,showing signiﬁcant improvements in both Recall@K andmean Recall@K compared to the state-of-the-art WSSGGmethods. A further appeal is thatLLM4SGGis data-efﬁcient, enabling effective model training with a smallamount of training images. Our code is available onhttps://github.com/rlqja1107/torch-LLM4SGG1. IntroductionScene Graph Generation (SGG) is a fundamental task incomputer vision, aiming at extracting structured visual*Corresponding Authorknowledge from images [21,32,36,39,45,56]. Most ex-isting SGG methods are fully-supervised, i.e., they heav-ily rely on the ground-truth annotations that involve theclass information of entities and predicates as well as thebounding box of entities [19,48,50]. However, since cre-ating extensively annotated scene graph datasets is costly,the heavy reliance on these annotations imposes practi-cal limitations on the model training [54]. To mitigatethe high cost associated with manual annotations, weakly-supervised scene graph generation (WSSGG) approacheshave recently emerged, aiming at training an SGG modelwithout any annotated scene graph dataset. Speciﬁcally, themain idea of recent WSSGG methods is to leverage imagecaptions along with associated images, as they can be easilycollected from the Web [20,47,54,57].The training process of WSSGG model using image cap-tions requires four steps as illustrated in Figure1(a).Step1:Preparing an image and its caption.Step 2:Parsingthe image caption,i.e., triplets formed as⟨subject, predi-cate, object⟩are extracted from the image caption throughan off-the-shelf parser [30,43].Step 3:Aligning the tripletsin the caption with entity/predicate classes of interest,i.e.,entity (subject, object) and predicate classes in the extractedtriplets obtained in Step 2 are aligned with the entity andpredicate classes in the target data1, respectively. Thisalignment is based on their synonym/hypernym/hyponymcontained in an external knowledge base (KB), e.g., Word-Net [25].Step 4:Grounding unlocalized entities in the ex-tracted triplets,i.e., unlocalized entities (subjects and ob-jects) are matched with relevant image regions generated bya pre-trained object detector, e.g., Faster R-CNN [29]. Thelocalized entities and predicates in the extracted triplets thenserve as pseudo-labels for training an SGG model.Existing WSSGG approaches mainly focus on Step 4[20,33,47,57]. For example, in Figure1(a), their effortshave been focused on grounding the entitypersonin an un-localized triplet with an image region that captures thesit-tingbehavior. More precisely, LSWS [47] exploits the con-1We use Visual Genome [13] as the target data.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
28306
carryingApersonin a uniform is sitting ona pony
Prepare a caption with its imageStep 1(a) Pipeline of Weakly-Supervised SGG
Adult catlying onlaptop computer
An elephantis carryinga wooden logunder its trunkcatcomputeroncatlaptoponlying on(c) Semantic Oversimplification
(d) Low-Density Scene Graphcatlaptoplying onLLMFrequency 0 in Parser+KB(b) Predicate DistributionFrequencyOurspersonponysitting onAlign classespony → animalKB
Step 3sitting onpersonanimalUnlocalized TripletGroundingStep 4
personanimalsitting onSGG TrainingLocalized Triplet
Conventional ApproachOurs (LLM)
Step 3Alignclasses
LLMStep 2LLMsitting onanimalpersonParaphrased CaptionStep 3OursStep 2Parsethe captionPrepare a caption with its imageStep 1
Prepare a caption with its imageStep 1Step 2Parsethe captionelephantlogelephantNonecarryingelephantbranchcarryingLLMOursStep 3AlignclassesStep 2Parsethe captionFigure 1. (a) The pipeline of weakly-supervised SGG. (b) The predicate distribution of unlocalized triplets (Parser+KB vs. Ours). InParser+KB, the distribution becomes heavily long-tailed, and 12 out of 50 predicates are non-existent. (c) Semantic over-simpliﬁcationcaused by a rule-based parser in Step 2. (d) Low-density scene graph caused by the static structure of KB in Step 3.textual object information to accurately ground the unlocal-ized entities, leveraging the linguistic structure embeddedwithin the triplets. Another line of research [20] employs apre-trained vision-language model [15] to reﬂect the seman-tic interactions among entities within the image caption.However, we argue that existing WSSGG approachesoverlook the importance of the triplet formation processconducted in Step 2 and Step 3. We identify the two majorissues described below, i.e., semantic over-simplication andlow-density scene graph, which incur incomplete unlocal-ized triplets after Step 2 and 3. These incomplete tripletesare mostly uninformative predicates with a limited number,and negatively impact the training of an SGG model evenwhen entities are correctly grounded in Step 4. To demon-strate the impact of incomplete unlocalized triplets, we fol-low the conventional process to extract unlocalized triplets(i.e., Step 1-3), and conduct an examination of triplets ob-tained from COCO caption dataset, which are generatedthrough Scene Parser [43] in Step 2 and WordNet [25] inStep 3. As a result, we identify the following two issues:•Semantic Over-simpliﬁcation:We ﬁnd that the standardscene graph parser [43] operating on heuristic rule-basedprinciples commonly used in Step 2 leads to a seman-tic over-simpliﬁcation of predicates in extracted triplets.In other words, ﬁne-grained predicates are undesirablyconverted into coarse-grained predicates, which we re-fer to as semantic over-simpliﬁcation. For example, inFigure1(c), an informative predicatelying on(i.e., ﬁne-grained predicate) in the image caption is converted intoa less informative predicateon(i.e., coarse-grained pred-icate), because the rule-based parser fails to capture thepredicatelying onat once, and its heuristic rules fall shortof accommodating the diverse range of caption’s struc-ture. As a result, the predicate distribution becomes heav-ily long-tailed, in which coarse-grained predicates (e.g.,with, on, in) greatly outnumber ﬁne-grained predicates(e.g., parked on, covered in) (Figure1(b)). To make thematter worse, numerous ﬁne-grained predicates eventu-ally end up in a frequency of 0, even though they areoriginally present in the captions. Speciﬁcally, 12 out of50 predicates are non-existent, which means that these 12predicates can never be predicted, since the model is nottrained on these predicates at all.•Low-Density Scene Graph:We ﬁnd that the KB-basedtriplet alignment in Step 3 leads to low-density scenegraphs, i.e., the number of remaining triplets after Step3 is small. We attribute the low-density scene graphsprimarily to the utilization of KB in Step 3. Specif-ically, a triplet is discarded if any of the three com-ponents (i.e., subject, predicate, object) or their syn-onym/hypernym/hyponym within the triplet fail to alignwith the entity or predicate classes in the target data.For example, in Figure1(d), the triplet⟨elephant, car-rying, log⟩is discarded becauselogdoes not exist inVisual Genome dataset nor its synonym/hypernym, evenifelephantandcarryingdo exist. In Table1, we re-port the number of triplets and images in Visual Genomedataset, which is a common benchmark dataset used infully-supervised SGG approaches, and COCO captiondataset, which is a common benchmark dataset used inweakly-supervised SGG approaches. We observe thaton average Visual Genome dataset contains 7.1 triplets(i.e., 405K/57K) per image (See Table1(a)), while COCOdataset contains only 2.4 triplets (i.e., 154K/64K) per im-age (See Table1(b)). This indicates that existing WSSGGapproaches suffer from the lack of sufﬁcient supervi-sion per image, leading to poor generalization and per-formance degradation [47,49]. In summary, relying on
28307
DatasetHow to annotate# Triplet # ImageFully-Supervised approach(a) Visual GenomeManual405K 57KWeakly-Supervised approach(b) COCO CaptionParser+KB154K 64K(c) COCO CaptionLLM344K 64KTable 1. Comparison of scene graph density.the static structured knowledge of KB is insufﬁcient tocover the semantic relationships among a wide a rangeof words, which incurs the low-density scene graph afterStep 3.To alleviate the semantic over-simpliﬁcation and thelow-density scene graph issues, we propose a new approach,namelyLargeLanguageModel for weakly-supervisedSGG(LLM4SGG) that adopts a pre-trained Large Lan-guage Model (LLM), which has shown remarkable trans-ferability to various downstream tasks in NLP such as sym-bolic reasoning, arithmetic, and common-sense reasoning[2,5,38]. Inspired by the idea of Chain-of-Thought2(CoT)[41], which arrives at an answer in a stepwise manner,we separate the triplet formation process into two chains,each of which replaces the rule-based parser in Step 2 (i.e.,Chain-1) and the KB in Step 3 (i.e., Chain-2). More pre-cisely, we design a prompt for extracting triplets from a cap-tion, and ask the LLM to extract triplets formed as<subject,predicate, object>(Chain-1). We expect that the predicatesextracted based on a comprehensive understanding of thecaption’s context via LLM are semantically rich, thereby al-leviating the semantic over-simpliﬁcation issue. Besides, toalleviate the low-density scene graph issue, we additionallyincorporate a paraphrased version of the original caption.To this end, we further design a prompt for paraphrasingthe original caption and extracting more triplets from theparaphrased caption. However, entities and predicates inthe triplets obtained after Chain-1 are not yet aligned withthe target data. Hence, we design a another prompt to alignthem with entity/predicate classes of interest, and ask theLLM to align them with semantically relevant lexeme in-cluded in a predeﬁned lexicon, which is the set of vocabu-laries that are present in the target data (Chain-2). To fur-ther engage the LLM in the reasoning process of Chain-1and Chain-2, we employ the in-context few-shot learningthat incorporates a few input-output examples within theprompt, enabling the LLM to perform the task without theneed for ﬁne-tuning.To validate the effectiveness ofLLM4SGG, we apply itto state-of-the-art WSSGG methods [54,57]. Through ex-tensive experiments, we show thatLLM4SGGsigniﬁcantlyenhances the performance of existing WSSGG methods interms of mean Recall@K and Recall@K performance onVisual Genome and GQA datasets by alleviating the se-mantic over-simpliﬁcation and the low-density scene graph2We use the CoT strategy as a means to arrive at an answer in a stepwisemanner, which differs from the Chain-of-Thought prompting.(See Table1(c) where the number of triplets increased to334K). A further appeal ofLLM4SGGis that it is data-efﬁcient, i.e., it outperforms state-of-the-art baselines evenwith a small amount of training images, verifying the effec-tiveness ofLLM4SGG.In summary, we make the following contributions:• We identify two major issues overlooked by existingWSSGG studies, i.e., semantic over-simpliﬁcation andlow-density scene graph.• We leverage an LLM along with the CoT strategy andthe in-context few-shot learning technique to extract in-formative triplets without the need for ﬁne-tuning theLLM. To the best of our knowledge, we are the ﬁrst toleverage an LLM for the SGG task.•LLM4SGGoutperforms the state-of-the-art WSSGGmethods, especially in terms of mR@K, demonstrat-ing its efﬁcacy in addressing the long-tail problem inWSSGG for the ﬁrst time.2. Related WorksWeakly-Supervised Scene Graph Generation(WSSGG).The WSSGG task aims to train an SGGmodel without relying on an annotated scene graph dataset.To achieve this, most WSSGG studies [20,47,54,57]utilize image captions and ground unlocalized tripletswith image regions. Speciﬁcally, VSPNet [49] proposesthe iterative graph alignment algorithm to reﬂect thehigh-order relations between unlocalized triplets and imageregions. SGNLS [57] uses a pre-trained object detector[29] to ground the entities in unlocalized triplets, whichshare the same classes with the output of object detectors.In addition to the information derived from the objectdetector, [20] employs a pre-trained vision-language model[15] to capture the semantic interactions among objects.VS3[54] uses a grounding-based object detector [18],which calculates the similarity between the entity text inunlocalized triplet and image region, thereby groundingthe unlocalized triplets. However, these methods overlookthe triplet formation process that leads to the semanticover-simpliﬁcation (Step 2) and the low-density scenegraph (Step 3). In this regard, existing methods result ina sub-optimal performance even when unlocalized tripletsare correctly grounded in Step 4.Large Language Model (LLM).LLMs have demonstratedremarkable transferability to various downstream tasks suchas symbolic reasoning, arithmetic, and common-sense rea-soning [2,5,38]. Speciﬁcally, GPT-3 (175B) [2] stands asa cornerstone to break the line of numerous language tasks.Inspired by GPT-3, PaLM (540B) [5], LLaMA (65B) [38],OPT (175B) [53], and LaMDA (137B) [37] have been sub-sequently introduced. More recently, advanced GPT mod-els (e.g., GPT-4 [28], ChatGPT [27]) ﬁne-tuned with humanfeedback have gained prominence and widely applied fordiverse applications, e.g., planner of tools [9,24], mobile
28308
task automation [42]. In this work, we employ the powerof LLM (i.e., ChatGPT) to alleviate the two issues, i.e., se-mantic over-simpliﬁcation and low-density scene graph, inthe context of the WSSGG task.In-Context Few-shot Learning.In-context few-shot learn-ing incorporates a few input-output examples related to atarget task, conditioning the LLM on the context of exam-ples. Speciﬁcally, GPT-3 [2] pioneered the concept of in-context learning to facilitate an LLM as a versatile model ondiverse tasks. This breakthrough has proliferated a plethoraof research to leverage the in-context few-shot learning forvarious tasks [24,26,46,55]. More precisely, Chameleon[24] integrates a few examples to enhance its understand-ing of tool planning task. [26]u t i l i z e sp o s i t i v ea n dn e g a t i v eexamples related to questions for question generation tasks.ReAct [46] incorporates examples of reasoning with actionfor solving decision-making tasks. Inspired by recent in-context few-shot learning approaches, we provide a few ex-amples to LLMs to help 1) understand the process of tripletextraction from a caption (i.e., Step 2), and 2) align the en-tity/predicate classes with the target data (i.e., Step 3) in thecontext of the WSSGG task.3. MethodIn this section, we describeLLM4SGGin detail. Wewould like to emphasize thatLLM4SGGmainly focuses onthe triplet formation process conducted in Step 2 (parsing)and Step 3 (aligning), while existing WSSGG approachesmainly focus on Step 4 (grounding). We start by present-ing the problem formulation of WSSGG (Section3.1), fol-lowed by the prompt conﬁguration (Section3.2). Next, weintroduce how LLMs are adopted to address the two issuesof conventional WSSGG approaches when parsing the im-age caption (Section3.3) and aligning the triplets in cap-tions with entity/predicate classes of interest (Section3.4).Finally, we ground the unlocalized triplets by associatingthem with bounding boxes (i.e., image regions) and trainthe SGG model using the localized triplets (Section3.5).The overall pipeline ofLLM4SGGis shown in Figure2.3.1. Problem FormulationIn the fully supervised SGG task, we aim to detect a scenegraphGf={si,pi,oi}Nfi=1that consists of triplets given animageI, whereNfis the number of triplets in the image.siandoidenote theithsubject and the object, respectively,whose bounding boxes aresi,b,oi,b, and entity classes aresi,c,oi,c∈Ce, whereCeis the set of predeﬁned entityclasses in the target data.pidenotes the predicate betweensiandoi, and its class ispi,c∈Cp, whereCpis the set ofpredeﬁned predicate classes in the target data. By using theground truth scene graphs as supervision, fully supervisedSGG approaches train an SGG modelTθ:I→Gf, whichmaps an image to a scene graph.In the weakly supervised SGG task, we aim to gen-erate a scene graph when the ground truth scene graphis not given, i.e., there are no bounding boxes and en-tity/predicate class information. Instead, existing WSSGGapproaches [20,33,47,54,57] use image captions alongwith associated images to produce scene graphs, i.e., local-ized triplets. More precisely, they extract a set of tripletsGw={si,pi,oi}Nwi=1from the image captions, whereNwis the number of triplets extracted from the captions. How-ever, while the extracted triplets contain the class informa-tion (i.e.,si,c,oi,candpi,c), they are unlocalized, sincebounding boxessi,bandoi,bare not included in the cap-tion. Therefore, it is essential to perform the grounding stepto associate the unlocalized triplets with bounding boxes.Once we have obtained the localized triplets, we can applythe conventional SGG training scheme, i.e.,Tθ:I→Gf.In this paper, our focus is to address the semantic over-simpliﬁcation and low-density scene graph issues regardingthe unlocalized tripletsGw, which has been overlooked inexisting WSSGG studies. Speciﬁcally, we aim to producean enhancedGwby reﬁning the process of scene graphdataset construction via an LLM. This reﬁnement includesthe triplet extraction step from the caption (Step 2) and thealignment of entity/predicate classes (Step 3), leveragingthe LLM’s comprehensive understanding of language andreasoning ability.3.2. Prompt ConﬁgurationIn fact, it is not a trivial task for an LLM to immediatelygenerate triplets from a caption whose entities and predi-cates are aligned with entity/predicate classes of interest, assuch a task is a novel task for the LLM. Inspired by the ideaof the Chain-of-thought (CoT) [41], which arrives at an an-swer in a stepwise manner, we separate the triplet formationprocess into the following two chains: Chain-1 – Extract-ing triplets from captions. Chain-2 – Aligning entities andpredicates with the entity/predicate classes of interest. Tocarefully design each chain, we deﬁne the LLM function,i.e.,LLM(·), with the following prompt input:Output=LLM(Task description,In-context examples,Actual question/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipuprightPrompt input),(1)whereLLM(·)is the decoder of the LLM, generating theOutputgiven the prompt input. The prompt input consistsof three components in a sequence: 1)task description,i.e., the delineation of the task that we intend to perform, 2)in-context examples, i.e., sample questions and answersrelated to the task at hand, 3)actual question, i.e., aninquiry from which we intend to derive the answer. Notethatin-context examplesis closely related to the in-contextfew-shot learning [2,40,41], which is shown to enhancethe LLM’s understanding of the task. Note that the aboveconﬁguration of the prompt input is applied to the tripletextraction (Chain-1) (Section3.3) and the alignment of en-tity/predicate classes (Chain-2) (Section3.4).
28309
Align entity/predicate classesStep 3
A surfer on rock holding wooden surfboardMisaligned Tripletstands on
n
surferrockAlign entity/predicate classesQ. Given the lexeme “surfer / stands on,” find the most semantically relevant lexeme in the predefined entity / predicate lexicon.Model Input
A. person / standing onModel OutputLLMQ. Given the sentence “A surfer on rock holding wooden surfboard,” extract meaningful triplets.A. <surfer, on, rock>, <surfer, holding, surfboard>Model OutputModel Input
Q. Given the sentence “A surfer on rock holding wooden surfboard,” paraphrase the sentence and extract meaningful triplets.Model Input
A. Paraphrased sentence:A surfer stands on a rock while holding    wooden surfboardExtracted triplets:<surfer, stands on, rock>,      <surfer, holding, surfboard>Model OutputLLMsurfboardholdingLLMMisaligned entity/predicate
MAligned entity/predicate
Extract triplets from paraphrased captionStep 2-2Extract triplets from original captionStep 2-1
Align entity/predicate classesStep 3Unlocalized Tripletstanding onpersonrocksurfboardholding
From the given sentence, the task is to extract meaningful triplets formed as <subject, predicate, object>.Please follow the following two steps.Step 1: Paraphrase the sentence.Step 2: From the paraphrased sentence, extract meaningful triplets.Q. Given the sentence {Example 1}, extract meaningful triplets.A. Step 1: The sentence can be paraphrased as: {Example Output}Step 2: Meaningful triplets extracted from the paraphrased sentence are: {Example Output]Q. Given the sentence “A surfer on rock holding wooden surfboard,” extract meaningful triplets.…Q. Given the sentence {Example K}, extract meaningful triplets.A. Step 1: The sentence can be paraphrased as: {Example Output}Step 2: Meaningful triplets extracted from the paraphrased sentence are: {Example Output]Task DescriptionFrom the given sentence, the task is to extract meaningful triplets formed as <subject, predicate, object>.Q. Given the sentence {Example 1}, extract meaningful triplets.A. {Example Output]Q. Given the sentence “A surfer on rock holding wooden surfboard,” extract meaningful triplets.…Q. Given the sentence {Example K}, extract meaningful triplets.A. {Example Output]In-context ExamplesActual QuestionTask DescriptionIn-context Examples
Actual QuestionLocalized Tripletsurfboardstanding on
rockholdingperson
< Prompt Input for Step 2-1 (Top) and Step 2-2 (Bottom) > GroundingStep 4
Prompt for paraphrased captionPrompt for original captionperson
surferstands on
n
standing on
Prepare a caption with its imageStep 1
Figure 2. The pipeline ofLLM4SGG. Given an image with its caption, we use an LLM to extract triplets from the original caption (Step2-1) and the paraphrased caption (Step 2-2). Then, we align the entity/predicate classes within the extracted triplets with semanticallysimilar lexeme in the target data via an LLM (Step 3), obtaining the unlocalized triplets. Lastly, we ground the unlocalized triplets overimage regions (Step 4) followed by the training of an SGG model.3.3. Chain-1. Triplet Extraction via LLM (Step 2in Figure2)Based on the LLM’s comprehensive understanding of thecontext of an image caption, we aim to extract triplets fromthe caption. As discussed in Section1, we use not only theoriginal caption but also a paraphrased caption generated bythe LLM to address the low-density scene graph issue.Extracting triplets from aparaphrasedcaption (Step 2-2).To extract triplets from a paraphrased caption, weinform the LLM about the task at hand by presenting thefollowing prompt: FROM THE GIVEN SENTENCE,THETASK IS TO EXTRACT MEANINGFUL TRIPLETS FORMEDAS⟨SUBJECT,PREDICATE,OBJECT⟩(i.e.,Task descrip-tionin Equation1). We then instruct the LLM to follow thetwo steps, i.e., paraphrasing step and triplet extraction step.To help the LLM understand the process of performing thetwo steps, we present few examples to the LLM that de-scribe how to answer the questions (i.e.,In-context exam-plesin Equation13), which involves a manual constructionof questions and corresponding answers to the paraphrasingand the triplet extraction steps. That is, for given a caption,we show the LLM how we expect a paraphrased captionand the extracted triplets would look like (e.g., Given “Fourclocks sitting on a ﬂoor next to a woman’s feet,” we showthe LLM that a paraphrased sentence would be “Four clocksare placed on the ﬂoor beside a woman’s feet,” and extractedtriplets would be⟨clocks, placed on, ﬂoor⟩and⟨clocks, be-side, feet⟩). Lastly, we show the caption of our interest tothe LLM, and let the LLM extract triplets from the caption(i.e.,Actual questionin Equation1). Please refer to Fig-ure2right (bottom) for an example of the prompt input usedin Step 2-2.3We use captions in COCO caption dataset for examples.Extracting triplets from theoriginalcaption (Step 2-1).As extracting triplets from the original caption does not in-volve the caption paraphrasing step, we exclude it from theprompt used in Step 2-2. Please refer to Figure2right (top)for an example of the prompt input used in Step 2-1.In summary, we obtain triplets from both the origi-nal and paraphrased captions after Step 2-1 and Step 2-2, respectively, which in turn alleviates the semantic over-simpliﬁcation issue of predicates and the low-density scenegraph issue.3.4. Chain-2. Alignment of Classes in Triplets viaLLM (Step 3 in Figure2)The entities (i.e., subject and object) and predicates withinthe triplets obtained from Step 2 described in Section3.3arenot yet aligned with the target data. Based on the semanticreasoning ability of the LLM, we aim to align them with thesemantically relevant lexeme in the target data.Aligning entities in the triplets with entity classes of in-terest.We instruct the LLM with the following prompt:GIVEN THE LEXEME{ENTITY},FIND SEMANTICALLYRELEVANT LEXEME IN THE PREDEFINED ENTITY LEXI-CON, where the predeﬁned entity lexicon isCe(i.e.,Taskdescriptionin Equation1). Similar to Section3.3, wepresent a few examples to the LLM that describe how toanswer the questions (i.e.,In-context examplesin Equa-tion1). For example, we provide the LLM with a few ex-amples regarding hierarchical relationships such aspigeonbeing semantically relevant tobird, and singular-plural re-lationships such assurfboardsbeing semantically relevanttosurfboard. Lastly, we show the entity of our interest tothe LLM (i.e.,Actual questionin Equation1), which en-ables the LLM to generate an answer by ﬁnding the most
28310
semantically relevant entity inCe. Please refer to Table7inAppendixA.1for an example of the prompt input.Aligning predicates in the triplets with predicate classesof interest.Likewise, we instruct the LLM with the fol-lowing prompt: GIVEN THE LEXEME{PREDICATE},FINDSEMANTICALLY RELEVANT LEXEME IN THE PREDEFINEDPREDICATE LEXICON, where the predeﬁned predicate lex-icon isCp(i.e.,Task descriptionin Equation1). We alsopresent a few examples to the LLM that describe how toanswer the questions (i.e.,In-context examplesin Equa-tion1). For example, we provide the LLM with a few exam-ples regarding tense relationships such as thelies onbeingsemantically relevant tolying on, and positional relation-ship such asnext tobeing semantically relevant tonear.Lastly, we show the predicate of our interest to the LLM(i.e.,Actual questionin Equation1). Please refer to Ta-ble8in AppendixA.1for an example of the prompt in-put. Furthermore, regarding the alignment of classes withina large predeﬁned lexicon, please refer to AppendixA.2.Note that as our main focus in this work is on developinga framework for utilizing LLMs in weakly-supervised SGGrather than ﬁnding a speciﬁc prompt design that works thebest, we tested with a single prompt design. However, sinceprompt designs are crucial in leveraging LLMs, we plan toexplore various designs in our future work.After performing Chain-1 (Section3.3) and Chain-2(Section3.4), we obtain intermediate unlocalized tripletsˆGw={si,pi,oi}ˆNwi=1, wheresi,c,oi,c∈ {Ce∪None}andpi,c∈ {Cp∪None}. It is worth noting that if there is no se-mantically relevant lexeme, we request the LLM to generateNoneas the answer, due to the fact that the entity/predicateclasses in the target data may not cover a wide range of en-tities/predicates. Similar to the conventional approach, wediscard a triplet if any of its three components (i.e., sub-ject, predicate and object) isNone. Lastly, we obtain theﬁnal unlocalized tripletsGw={si,pi,oi}Nwi=1(Nw≤ˆNw),wheresi,c,oi,c∈Ceandpi,c∈Cp.3.5. Model TrainingGiven the ﬁnal unlocalized tripletsGw, we ground themover relevant image regions to get localized triplets, mean-ing that we obtainsi,bandoi,b. To this end, we employtwo state-of-the-art grounding methods, i.e., SGNLS [57]and VS3[54]. Please refer to AppendixA.3for more de-tail about how each method performs grounding. AftergroundingGw, we obtain localized triplets and use them aspseudo-labels for training a supervised SGG model. Pleaserefer to AppendixA.4for more detail about the model train-ing.4. ExperimentDatasets.Totrainan SGG model without an annotatedscene graph dataset, we use three caption datasets: COCOcaption [3], Conceptual (CC) caption [31], and VisualGenome (VG) caption [44]. For fair comparisons, we usethe same set of images that have been utilized in previousWSSGG studies [20,47,54,57], leading to the utilizationof 64K images on COCO caption dataset, 145K images onCC caption dataset, and 57K images on VG caption dataset.Toevaluatethe trained SGG model, we employ the widelyused Visual Genome (VG) dataset [13] and GQA dataset[11]. The VG dataset contains the ground-truth localizedtriplet information annotated by humans. We follow thestandard split of VG [44], which consist of 150 entityclasses and 50 predicate classes. For the GQA dataset usedin the SGG task, we follow the same pre-processing step ofa previous SGG study [8], which involves selecting top-200frequent entity classes and top-100 frequent predicateclasses. In both datasets, 30% of the total images are usedfor evaluation. Please refer to AppendixC.1for moredetails regarding the datasets. Note that we mainly usethe COCO caption dataset for analysis ofLLM4SGGthroughout this paper, i.e., CC and VG caption datasets areexclusively used in quantitative result on VG (Section4.1).Evaluation metrics.Recent fully-supervised SGG stud-ies [1,48,51] have emphasized improving the accuracy ofpredictions for ﬁne-grained predicates rather than coarse-grained predicates, since the former construct richer scenegraphs. As a result, they commonly use mean Recall@K(mR@K) that computes the average of Recall@K (R@K)across all predicates. In line with the recent emphasis onﬁne-grained predicates, we incorporate both mR@K andR@K in our evaluation, whereas previous WSSGG studies[20,47,54] mainly rely on the R@K metric alone. More-over, we report F@K, which is the harmonic average ofR@K and mR@K to jointly consider R@K and mR@K,following previous SGG studies [12,51]. Regarding theevaluation task, we follow previous WSSGG studies andadopt the Scene Graph Detection (SGDet) task, where boththe ground-truth bounding box and the entity class infor-mation are not provided. Please refer to AppendixC.2formore detail regarding the task.Baselines.Please refer to AppendixC.3for details regard-ing the baselines.Implementation details.Please refer to AppendixC.4re-garding the implementation details.4.1. Quantitative Result on VGTable2shows the performance of baseline models andthose whenLLM4SGGis applied. We have the follow-ing observations based on COCO caption dataset:1)Ap-plyingLLM4SGGto SGNLS and VS3improves the per-formance in terms of R@K and mR@K, which demon-strates the effectiveness of the triplet formation throughLLM4SGG. Notably,LLM4SGGsigniﬁcantly improvesmR@K, implying thatLLM4SGGeffectively alleviates thelong-tailed problem in WSSGG. This can be clearly seenin Figure3,w h i c hs h o w st h ep e r f o r m a n c eg a i no nﬁ n e -
28311
MethodDatasetR@50 R@100mR@50 mR@100F@50 F@100Motif(CVPR’18)- Fully-supervisedVG31.89 36.366.38 7.5710.63 / 12.53 12.53LSWS(CVPR’21)3.29 3.693.27 3.663.28 3.67SGNLS(ICCV’21)3.80 4.462.51 2.783.02 3.43SGNLS(ICCV’21)+LLM4SGG5.09+ 1.295.97+ 1.514.08+ 1.574.49+ 1.714.53+ 1.515.13+ 1.70Li et al(MM’22)6.40 7.331.73 1.982.72 3.12VS3(CVPR’23)6.60 8.012.88 3.254.01 4.62VS3(CVPR’23)+LLM4SGG8.91+ 2.3110.43+ 2.427.11+ 4.238.18+ 4.937.91+ 3.909.17+ 4.55VS3(CVPR’23)+Rwt4.25 5.045.17 5.994.67 5.47VS3(CVPR’23)+Rwt+LLM4SGGCOCOCaption5.10+ 0.856.34+ 1.308.42+ 3.259.90+ 3.916.35+ 1.697.73+ 2.26VS3(CVPR’23)6.69 8.201.73 2.042.75 3.27VS3(CVPR’23)+LLM4SGGCCCaption9.47+ 2.7810.69+ 2.495.40+ 3.676.09+ 4.056.88+ 4.137.76+ 4.49VS3(CVPR’23)14.54 18.482.80 3.794.70 6.29VS3(CVPR’23)+LLM4SGGVGCaption18.40+ 3.8622.28+ 3.806.26+ 3.467.60+ 3.819.34+ 4.6411.33+ 5.04Table 2. Performance comparisons on the SGDet task. The best performance among WSSGG models within each dataset is in bold. Thered numbers indicate the absolute performance improvement after applyingLLM4SGG.Rwtdenotes using the reweighting strategy [51].
FrequencyRecall@100Predicate33Frequency 0 in Parser+KBFigure 3. Per class performance (Bar: number of predicate in-stances, Line: Recall@100).grained predicates.2)VS3+Rwt+LLM4SGGfurther im-proves mR@K of VS3+Rwt. We attribute this to the factthat the conventional approach generates a limited num-ber of ﬁne-grained predicates, which makes the reweightingstrategy less effective within VS3. Especially, non-existentpredicates can never be predicted even when the reweight-ing strategy is applied. On the other hand,LLM4SGGin-creases the number of instances that belong to ﬁne-grainedpredicates, which is advantageous for the reweighting strat-egy. For the per class performance comparison over thereweighting stregty, please refer to AppendixD.1.3)Theperformance gain obtained from applyingLLM4SGGisgreater on VS3(i.e., VS3+LLM4SGG) than on SGNLS(i.e., SGNLS+LLM4SGG). The major reason lies in thedifference in how SGNLS and VS3make use of the poolof 344K unlocalized triplets obtained throughLLM4SGG.Speciﬁcally, in the grounding process of SGNLS, we ob-serve that 100K out of 344K unlocalized triplets (i.e., 29%)fail to be grounded, and thus not used for training. On theother hand, VS3successfully grounds all 344K unlocalizedtriplets and fully utilize them for training, allowing it tofully enjoy the effectiveness ofLLM4SGG. This indicatesthatLLM4SGGmakes synergy when paired with a ground-ing method that is capable of fully utilizing the unlocalizedtriplets. For more details regarding the impact of ground-ing methods, please refer to AppendixB.4)Regarding theperformance comparison with a fully-supervised approach(i.e., Motif), please refer to AppendixD.2.Furthermore, we observe that applyingLLM4SGGtoCC and VG caption datasets also signiﬁcantly improves per-formance, demonstrating the effectiveness ofLLM4SGG.RowPC LP LA# TripletR@50 / 100mR@50 / 100F@50 / 100(a)154K6.60 / 8.012.88 / 3.254.01 / 4.62(b)/check243K9.46 / 11.223.43 / 3.925.03 / 5.81(c)/check/check256K8.42 / 9.855.99 / 6.957.00 / 8.15(d)/check/check327K11.76/13.383.50 / 4.055.39 / 6.22(e)/check/check/check344K8.91 / 10.437.11/8.187.91/9.17Table 3. Ablation studies. (PC: Using Paraphrased Caption inaddition to the original caption / LP: LLM-based Parsing / LA:LLM-based Alignment)4.2. Ablation StudiesIn Table3, we conduct ablation studies on VG datasetto understand the effectiveness of each componentofLLM4SGG, where VS3is used as the grounding method.Note that row (a) is equivalent to vanilla VS3. We havethe following observations.1) Effect of using the para-phrased caption:Including the paraphrased caption in ad-dition to the original caption (row (b)) increases the numberof triplets (154K→243K), resulting in an improved overallperformance. This demonstrates that the paraphrased cap-tion alleviates the low-density scene graph issue.2) Effectof LLM-based parsing:The LLM-based parsing (row (c))for extracting triplets improves mR@K of row (b). This in-dicates that the LLM-based parsing increases the numberof instances that belong to ﬁne-grained predicates, whichin turn alleviates the semantic over-simpliﬁcation issue.3)Effect of LLM-based alignment:The LLM-based align-ment (row (d)) of entities/predicates in the extracted tripletsincreases the number of triplets from 243K to 327K (row(b) vs (d)), which indicates that the low-density scene graphissue is alleviated. Consequently, R@K and mR@K of row(d) are greater than those of row (b).4)The fully-ﬂedgedapproach (row (e)) generally improves R@K and mR@K,showing the best performance in terms of F@K. It is im-portant to highlight that when using the LLM-based pars-ing, the performance of mR@K signiﬁcantly increases witha moderate decrease in R@K. This trade-off is attributedto the fact that R@K generally improves when the coarse-grained predicates are dominant [51]. In contrast, our ap-proach, which addresses the semantic over-simpliﬁcationissue, decreases the number instances that belong to coarse-grained predicates while simultaneously increasing thosethat belong to ﬁne-grained predicates (Figure1(b)), which
28312
3333Figure 4. Performance over various numbers of images used fortraining VS3+LLM4SGG.in turn results in a substantial improvement in mR@K. Wewould like to emphasize that the performance in terms ofmR@K is crucial in the context of SGG research [1,8,48],as ﬁne-grained predicates offer richer information.4.3. Analysis of Data-EfﬁciencyTo assess the effectiveness ofLLM4SGGunder the lackof available training data, we conduct experiments givena limited number of images. Speciﬁcally, among 64Kimages used for training VS3and VS3+LLM4SGGinTable2, we randomly sample 1K (1.5%), 5K (7.8%),and 10K (15.6%) images with replacement, and trainVS3+LLM4SGGfor ﬁve times. Figure4shows the aver-age performance over various numbers of images used fortraining VS3+LLM4SGGalong with the variance (in bluearea). We observe that when using only 1K images, the per-formance is slightly inferior to the baseline that used 64Kfor training (i.e., VS3). However, as we increase the numberof images used for training to 5K images, we observe a sig-niﬁcant improvement in both R@K and mR@K comparedwith the baseline. This demonstrates thatLLM4SGGisdata-efﬁcient, asit outperforms the baseline even with only7.8% of the total images used for training the baseline.Moreover, when further increasing the number of imagesto 10K, we observe further performance improvements,and when it reaches 64K, which is the same as the numberof images used for training the baseline, the performanceis the best. In summary,LLM4SGGenables data-efﬁcientmodel training even with a limited amount of availableimages for training, thanks to alleviating the semanticover-simpliﬁcation and low-density scene graph issues.4.4. Quantitative Result on GQAIn Table 4, we additionally conducted experiments on GQAdataset [11]. Please refer to AppendixE.1for more de-tailed descriptions on the training and evaluation processeson GQA dataset. The GQA dataset contains twice as manypredicates as the Visual Genome dataset and includes com-plicated predicates (e.g., sitting next to, standing in frontof). As a result, when obtaining unlocalized triplets us-ing the conventional WSSGG approach, we observe that 44out of 100 predicates have a frequency of 0, and the pred-icate distribution is extremely long-tailed. Consequently,the baseline (i.e., VS3) exhibits signiﬁcantly lower perfor-mance, especially in terms of mR@K. On the other hand,MethodR@50 / 100mR@50 / 100F@50 / 100Motif (Fully-supervised)28.90 / 33.106.40 / 7.7010.48 / 12.49VS35.90 / 6.971.60 / 1.812.52 / 2.87VS3+LLM4SGG8.88/10.385.33/6.516.66/8.00Table 4. Performance comparison on GQA.our approach shows substantial performance improvementsnot only in R@K but also in mR@K, thanks to the mitiga-tion of semantic over-simpliﬁcation and low-density scenegraph issues. Please refer to AppendixE.2for the predicatedistribution and performance comparison for each class inGQA dataset. Additionally, please refer to AppendixE.3for qualitative analyses on GQA dataset.5. Conclusion & Future workIn this work, we focus on the triplet formation process inWSSGG, whose importance is overlooked by previous stud-ies. To alleviate the semantic over-simpliﬁcation and low-density scene graph issues inherent in the triplet forma-tion process, we propose a new approach, i.e.,LLM4SGG,which leverages a pre-trained LLM during the extraction oftriplets from the captions, and alignment of entity/predicateclasses with those in the target data. It is important tonote that construction of these triplets is not required ev-ery time to train the SGG model; instead, it is a one-timepre-processing step. In this regard, we contribute to gen-erating enhanced triplets compared to the conventional ap-proach. Moreover, we publish an enhanced SGG datasetconstructed by LLM for future studies of SGG. As a re-sult, we outperform baselines in terms of R@K, mR@Kand F@K on Visual Genome and GQA datasets. A poten-tial limitation of our work is the reliance on a proprietaryblackbox LLM, which can also be costly to use. Hence, inAppendixF, we provide discussions on replacing the LLMwith smaller language models.For future work, an LLM can be used to ground the un-localized triplets in Step 4. Recently, vision-language rep-resentation learning has been developed for transformingvisual features into textual features to facilitate the use ofvisual features as input to an LLM [16,58]. In this re-gard, given the visual features of bounding boxes as input,we could ask the LLM to identify relevant bounding boxesbased on the textual information of entities within unlocal-ized triplets using the comprehensive understanding of thecontext of triplets.6. AcknowledgementThis work was supported by Institute of Information &communications Technology Planning & Evaluation (IITP)grant funded by the Korea government(MSIT) (No.2020-0-00004, Development of Previsional Intelligence basedon Long-term Visual Memory Network, No.2022-0-00077,Reasoning, and Inference from Heterogeneous Data, No.2019-0-00079, Artiﬁcial Intelligence Graduate School Pro-gram, Korea University).
28313
References[1] Bashirul Azam Biswas and Qiang Ji. Probabilistic debias-ing of scene graphs. InProceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages10429–10438, 2023.6,8[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-guage models are few-shot learners.Advances in neural in-formation processing systems, 33:1877–1901, 2020.3,4[3] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-tam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick.Microsoft coco captions: Data collection and evaluationserver.arXiv preprint arXiv:1504.00325, 2015.6[4] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:Universal image-text representation learning. InEuropeanconference on computer vision, pages 104–120. Springer,2020.2,3[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, SebastianGehrmann, et al. Palm: Scaling language modeling withpathways.arXiv preprint arXiv:2204.02311, 2022.3[6] Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen,Mengchen Liu, Lu Yuan, and Lei Zhang. Dynamic head:Unifying object detection heads with attentions. InProceed-ings of the IEEE/CVF conference on computer vision andpattern recognition, pages 7373–7382, 2021.2,3,7[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova. Bert: Pre-training of deep bidirectionaltransformers for language understanding.arXiv preprintarXiv:1810.04805, 2018.2,3,7[8] Xingning Dong, Tian Gan, Xuemeng Song, Jianlong Wu,Yuan Cheng, and Liqiang Nie. Stacked hybrid-attention andgroup collaborative learning for unbiased scene graph gener-ation. InProceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 19427–19436,2022.6,8,3[9] Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, JoyaChen, Zihan Fan, and Mike Zheng Shou. Assistgpt: A gen-eral multi-modal assistant that can plan, execute, inspect, andlearn.arXiv preprint arXiv:2306.08640, 2023.3[10] Tao He, Lianli Gao, Jingkuan Song, and Yuan-Fang Li. To-wards open-vocabulary scene graph generation with prompt-based ﬁnetuning. InEuropean Conference on Computer Vi-sion, pages 56–73. Springer, 2022.3[11] Drew A Hudson and Christopher D Manning. Gqa: A newdataset for real-world visual reasoning and compositionalquestion answering. InProceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages6700–6709, 2019.6,8,3,7[12] Siddhesh Khandelwal and Leonid Sigal. Iterative scenegraph generation.Advances in Neural Information Process-ing Systems, 35:24295–24308, 2022.6,3[13] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-tidis, Li-Jia Li, David A Shamma, et al. Visual genome:Connecting language and vision using crowdsourced denseimage annotations.International journal of computer vision,123:32–73, 2017.1,6,2,3,5,7[14] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, StefanPopov, Matteo Malloci, Alexander Kolesnikov, et al. Theopen images dataset v4: Uniﬁed image classiﬁcation, objectdetection, and visual relationship detection at scale.Interna-tional Journal of Computer Vision, 128(7):1956–1981, 2020.2,3,4[15] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,Shaﬁq Joty, Caiming Xiong, and Steven Chu Hong Hoi.Align before fuse: Vision and language representation learn-ing with momentum distillation.Advances in neural infor-mation processing systems, 34:9694–9705, 2021.2,3,4[16] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.Blip-2: Bootstrapping language-image pre-training withfrozen image encoders and large language models.arXivpreprint arXiv:2301.12597, 2023.8[17] Lin Li, Guikun Chen, Jun Xiao, Yi Yang, ChunpingWang, and Long Chen. Compositional feature augmenta-tion for unbiased scene graph generation.arXiv preprintarXiv:2308.06712, 2023.3[18] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, LuYuan, Lei Zhang, Jenq-Neng Hwang, et al. Groundedlanguage-image pre-training. InProceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 10965–10975, 2022.3,2,4[19] Rongjie Li, Songyang Zhang, Bo Wan, and Xuming He.Bipartite graph network with adaptive message passing forunbiased scene graph generation. InProceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 11109–11119, 2021.1,3[20] Xingchen Li, Long Chen, Wenbo Ma, Yi Yang, and JunXiao. Integrating object-aware and interaction-aware knowl-edge for weakly supervised scene graph generation. InPro-ceedings of the 30th ACM International Conference on Mul-timedia, pages 4204–4213, 2022.1,2,3,4,6[21] Yongzhi Li, Duo Zhang, and Yadong Mu. Visual-semanticmatching by exploring high-order attention and distraction.InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 12786–12795, 2020.1[22] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, andPiotr Dollár. Focal loss for dense object detection. InPro-ceedings of the IEEE international conference on computervision, pages 2980–2988, 2017.2[23] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProceedings of the IEEE/CVF international conference oncomputer vision, pages 10012–10022, 2021.4[24] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-WeiChang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao.Chameleon: Plug-and-play compositional reasoning withlarge language models.arXiv preprint arXiv:2304.09842,2023.3,4[25] George A Miller. Wordnet: a lexical database for english.Communications of the ACM, 38(11):39–41, 1995.1,2
28314
[26] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Han-naneh Hajishirzi. Cross-task generalization via natu-ral language crowdsourcing instructions.arXiv preprintarXiv:2104.08773, 2021.4[27] OpenAI. Chatgpt.https://openai.com/blog/chatgpt, 2023.3,4[28] OpenAI. Gpt-4. Technical report, 2023.3[29] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.Faster r-cnn: Towards real-time object detection with regionproposal networks.Advances in neural information process-ing systems, 28, 2015.1,3,2,4[30] Sebastian Schuster, Ranjay Krishna, Angel Chang, Li Fei-Fei, and Christopher D Manning. Generating semanticallyprecise scene graphs from textual descriptions for improvedimage retrieval. InProceedings of the fourth workshop onvision and language, pages 70–80, 2015.1[31] Piyush Sharma, Nan Ding, Sebastian Goodman, and RaduSoricut. Conceptual captions: A cleaned, hypernymed, im-age alt-text dataset for automatic image captioning. InPro-ceedings of the 56th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), pages2556–2565, 2018.6[32] Jiaxin Shi, Hanwang Zhang, and Juanzi Li. Explainable andexplicit visual reasoning over scene graphs. InProceedingsof the IEEE/CVF conference on computer vision and patternrecognition, pages 8376–8384, 2019.1[33] Jing Shi, Yiwu Zhong, Ning Xu, Yin Li, and Chenliang Xu.A simple baseline for weakly-supervised scene graph gener-ation. InProceedings of the IEEE/CVF International Con-ference on Computer Vision, pages 16393–16402, 2021.1,4[34] Mohammed Suhail, Abhay Mittal, Behjat Siddiquie, ChrisBroaddus, Jayan Eledath, Gerard Medioni, and Leonid Si-gal. Energy-based learning for scene graph generation. InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 13936–13945, 2021.3[35] Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, andHanwang Zhang. Unbiased scene graph generation from bi-ased training. InProceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 3716–3725, 2020.5[36] Damien Teney, Lingqiao Liu, and Anton van Den Hengel.Graph-structured representations for visual question answer-ing. InProceedings of the IEEE conference on computervision and pattern recognition, pages 1–9, 2017.1[37] Romal Thoppilan, Daniel De Freitas, Jamie Hall, NoamShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, AliciaJin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda:Language models for dialog applications.arXiv preprintarXiv:2201.08239, 2022.3[38] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.Llama 2: Open foundation and ﬁne-tuned chat models.arXivpreprint arXiv:2307.09288, 2023.3,9[39] Sijin Wang, Ruiping Wang, Ziwei Yao, Shiguang Shan,and Xilin Chen. Cross-modal scene graph matching forrelationship-aware image-text retrieval. InProceedings ofthe IEEE/CVF winter conference on applications of com-puter vision, pages 1508–1517, 2020.1[40] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, BarretZoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma,Denny Zhou, Donald Metzler, et al. Emergent abilities oflarge language models.arXiv preprint arXiv:2206.07682,2022.4[41] Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.Chain-of-thought prompting elicits reasoning in large lan-guage models.Advances in Neural Information ProcessingSystems, 35:24824–24837, 2022.3,4,6[42] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, TaoYu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang,and Yunxin Liu. Empowering llm to use smartphone for in-telligent task automation.arXiv preprint arXiv:2308.15272,2023.4[43] Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, LeiLi, Weiwei Sun, and Wei-Ying Ma. Uniﬁed visual-semanticembeddings: Bridging vision and language with structuredmeaning representations. InProceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 6609–6618, 2019.1,2,6,8[44] Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei.Scene graph generation by iterative message passing. InPro-ceedings of the IEEE conference on computer vision and pat-tern recognition, pages 5410–5419, 2017.6[45] Xu Yang, Kaihua Tang, Hanwang Zhang, and Jianfei Cai.Auto-encoding scene graphs for image captioning. InPro-ceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 10685–10694, 2019.1[46] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,Karthik Narasimhan, and Yuan Cao. React: Synergizingreasoning and acting in language models.arXiv preprintarXiv:2210.03629, 2022.4[47] Keren Ye and Adriana Kovashka. Linguistic structures asweak supervision for visual scene graph generation. InPro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 8289–8299, 2021.1,2,3,4,6[48] Kanghoon Yoon, Kibum Kim, Jinyoung Moon, and Chany-oung Park. Unbiased heterogeneous scene graph generationwith relation-aware message passing neural network. InPro-ceedings of the AAAI Conference on Artiﬁcial Intelligence,pages 3285–3294, 2023.1,6,8,3[49] Alireza Zareian, Svebor Karaman, and Shih-Fu Chang.Weakly supervised visual semantic parsing. InProceedingsof the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 3736–3745, 2020.2,3[50] Rowan Zellers, Mark Yatskar, Sam Thomson, and YejinChoi. Neural motifs: Scene graph parsing with global con-text. InProceedings of the IEEE conference on computervision and pattern recognition, pages 5831–5840, 2018.1,3,4[51] Ao Zhang, Yuan Yao, Qianyu Chen, Wei Ji, Zhiyuan Liu,Maosong Sun, and Tat-Seng Chua. Fine-grained scene graphgeneration with data transfer. InEuropean conference oncomputer vision, pages 409–424. Springer, 2022.6,7,3,5
28315
[52] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang,Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao.Vinvl: Revisiting visual representations in vision-languagemodels. InProceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 5579–5588,2021.1[53] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained trans-former language models.arXiv preprint arXiv:2205.01068,2022.3[54] Yong Zhang, Yingwei Pan, Ting Yao, Rui Huang, Tao Mei,and Chang-Wen Chen. Learning to generate language-supervised and open-vocabulary scene graph using pre-trained visual-semantic space. InProceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 2915–2924, 2023.1,3,4,6,2,7,8[55] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola.Automatic chain of thought prompting in large languagemodels.arXiv preprint arXiv:2210.03493, 2022.4[56] Yiwu Zhong, Liwei Wang, Jianshu Chen, Dong Yu, and YinLi. Comprehensive image captioning via scene graph de-composition. InComputer Vision–ECCV 2020: 16th Euro-pean Conference, Glasgow, UK, August 23–28, 2020, Pro-ceedings, Part XIV 16, pages 211–229. Springer, 2020.1[57] Yiwu Zhong, Jing Shi, Jianwei Yang, Chenliang Xu, and YinLi. Learning to generate scene graph from natural languagesupervision. InProceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 1823–1834, 2021.1,3,4,6,2[58] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-hamed Elhoseiny. Minigpt-4: Enhancing vision-languageunderstanding with advanced large language models.arXivpreprint arXiv:2304.10592, 2023.8
28316
