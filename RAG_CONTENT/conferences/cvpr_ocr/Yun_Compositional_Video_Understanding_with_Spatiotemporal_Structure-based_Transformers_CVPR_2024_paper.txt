Compositional Video Understanding
with Spatiotemporal Structure-based Transformers
Hoyeoung Yun1⇤, Jinwoo Ahn2⇤, Minseo Kim1, Eun-Sol Kim1, 2†
1Department of Computer Science, Hanyang University
2Department of Artiﬁcial Intelligence Application, Hanyang University
{yhy17520, jinwooahn, simon1011, eunsolkim }@hanyang.ac.kr
Abstract
In this paper, we suggest a new novel method to under-
stand complex semantic structures through long video in-
puts. Conventional methods for understanding videos have
been focused on short-term clips, and trained to get visual
representations for the short clips using convolutional neu-
ral networks or transformer architectures. However, most
real-world videos are composed of long videos ranging
from minutes to hours, therefore, it essentially brings lim-
itations to understanding the overall semantic structures
of the long videos by dividing them into small clips and
learning the representations of them. We suggest a new al-
gorithm to learn the multi-granular semantic structures of
videos, by deﬁning spatiotemporal high-order relationships
among object-based representations as semantic units. The
proposed method includes a new transformer architecture
capable of learning spatiotemporal graphs, and a compo-
sitional learning method to learn disentangled features for
each semantic unit. Using the suggested method, we resolve
the challenging video task, which is compositional gener-
alization understanding of unseen videos. In experiments,
we demonstrate new state-of-the-art performances for two
challenging video datasets.
1. Introduction
Recently, as video-content-based services have obviously
proliferated, human-level video understanding using artiﬁ-
cial intelligence has been regarded as one of the fundamen-
tal problems in the computer vision ﬁeld. However previous
methods of video understanding [ 15,31,32] have been fo-
cused on short-term clips ranging from seconds to a few
minutes, and, it is considered one of the challenging prob-
lems to understand the semantic structures in long video
due to the complex and high-order dependencies between
*These authors contributed equally to this work
†Corresponding author
Figure 1. Overall scheme of proposed compositional learning
strategy. We introduce an object-centric spatiotemporal graph as
an alternative representation of the given video and decompose it
to obtain ﬁne-grained semantic units. Subsequently, by composing
these semantic units, we can reproduce higher-level semantics cor-
responding to the complex semantics present in the given video.
scenes or events throughout the spatial and temporal axis.
Due to the complexity, most conventional video under-
standing methods [ 15,31,32] represent each frame (or uni-
formly segmented short-term clips) as a real-valued vector
which makes it hard to consider high-order relationships be-
tween objects within the frame. Moreover, the methods have
limitations in modeling various lengths of actions inherent
within videos and the complex relationships between the ac-
tions as the length of the video increases. In other words, it
is hard to understand multi-granular semantic structures en-
compassing objects, scenes, and video-wide contexts with
conventional video understanding methods. To tackle this
problem, we suggest a novel video understanding method
based on object-oriented representations that can learn the
spatiotemporal semantic structures of the videos in compo-
sitional ways.
The two main goals of this paper are 1) to propose a
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
18751
Figure 2. To represent the semantic structure for a given video,
we construct a symbolic graph with Nnumber of graphs corre-
sponding to Nframes, and each graph, denoted as Gt, encodes
information about nnumber of objects as nodes and the relation-
ship between two nodes as edges at time t.
new transformer-based algorithm to learn object-oriented
semantically disentangled representation of videos and, 2)
to tackle the most challenging video-related task, which
is a compositional generalization task using the proposed
method. Firstly, we introduce a new object-oriented video
understanding algorithm consisting of four modules: spa-
tiotemporal graph construction, spatiotemporal graph trans-
former, object-oriented video encoder, and compositional
learning modules. The proposed algorithm demonstrated
outstanding performance on the CATER[ 10] and MOMA-
LRG[ 23] datasets, which can be considered the most chal-
lenging datasets in terms of the length of the video and the
complexity of the elements constituting the actions in the
videos. Furthermore, we argue that compositional general-
ization ability is a crucial element of video understanding
algorithms to understand long videos and suggest a new
dataset to tackle this problem. Based on the CATER videos,
we present new data splits for comparing the compositional
generalization abilities between models.
Through experiments, we demonstrate that the proposed
algorithm achieves high video understanding performance
in both CATER (synthetic data) and MOMA-LRG (real-
world data) datasets. Speciﬁcally, we show that our pro-
posed method achieves signiﬁcantly higher performance
in compositional generalization tasks compared to existing
comparative methods using object-oriented video represen-
tation.
Our contribution can be summarized as follows.
•A novel object-oriented video understanding method to
learn the multi-granular semantic structure of long videos
is suggested.
•A novel data split for compositional generalization test of
video understanding algorithms is proposed.
•In experiments with both synthetic and real-world videos,
we achieve new state-of-the-art performances.2. Related Work
2.1. Video Representation Learning
Video representation learning is the most fundamental task
for various video-related tasks such as action recognition
[4,15,36], video retrieval [ 8,30], and video captioning
[19,41], which aims to learn meaningful representation
from raw video data. While image representations could
only consider spatial visual features, understanding tempo-
ral dependencies between image frames is crucial for video
representation learning. To tackle this problem, previous
research such as STIP [ 20], HOG3D [ 17], and iDT [ 35]
have tried to design hand-crafted descriptors to capture spa-
tiotemporal patterns from the videos.
Along with the remarkable achievements of convolu-
tional neural networks (CNNs), video representation learn-
ing methods based on CNN have been proposed. Two-
stream CNNs [ 7,28] suggested an additional temporal
stream that uses dense optical ﬂow between consecutive
video frames to exploit the motions of videos. I3D [ 4]
improved existing two-stream CNNs [ 28] by inﬂating 2D
CNNs into 3D [ 31]. Moreover, the non-local block was pro-
posed to help models understand long-range temporal de-
pendencies [ 37].
Recently, transformers [ 33], which are powerful in learn-
ing long-range temporal dependencies, have been applied
because the ConvNet-based models do not have good tem-
poral dependencies on long-term videos [ 1,6]. However,
existing methods based on CNNs or transformers focus on
learning temporal dependencies between image sequences,
so complex relationships between objects in a spatiotempo-
ral manner are hard to contain.
2.2. Compositional Video Learning
Compositional generalization ability, which refers to the ca-
pability for understanding unseen novel data composed of
concepts or components learned during training, can play an
important role in long video understanding. A VT [ 9], which
is a model for action anticipation, forecasts the next ac-
tion based on understanding the previous part of the video.
Video synthesis models like AG2Vid [ 3] predict the follow-
ing circumstance and synthesize the next frame composing
concepts and components shown. In the video captioning
task, DCC [ 13] shows compositionality explaining novel
objects without paired data, which do not exist in caption
corpora. VISA [ 22] suggests a novel task called composi-
tional temporal grounding to assess the compositional gen-
eralizability in temporal grounding task with testing on the
queries of new combinations of seen words during training.
Recently, a number of benchmark datasets such as
CLEVRER [ 40], TVQA [ 21], and AGQA [ 11] have been
suggested for compositional video question answering
tasks. In this paper, the most challenging dataset CATER
18752
[10] in terms of compositional action recognition for long
videos is used for the experiments.
2.3. Transformers for Graphs
As transformers[ 33] have achieved remarkable performance
on a wide range of domains, various application methods for
graphs have emerged[ 14,42].
Earlier transformer methods for graphs applied self-
attention only for locally close neighbors[ 5,34] or with ad-
ditional message-passing modules[ 27]. However, these ap-
proaches have limitations in representing the overall graph
structure and are prone to over-smoothing. Consequently,
self-attention on nodes has been applied to handle edges and
integrate graph structures by incorporating heuristic adap-
tation methods[ 18,24]. Especially, TokenGT[ 16] maintains
the self-attention mechanism of pure transformer[ 33] with-
out modiﬁcations and furnishes both nodes and edges with
speciﬁc token-wise embeddings as input.
Although there has been extensive exploration of trans-
former architectures designed to process graph inputs re-
cently, there is a notable absence of discussions on algo-
rithms speciﬁcally designed for learning temporal graphs.
3. Method
The core idea of the proposed algorithm is to learn represen-
tations of videos at the object level, deﬁning spatiotemporal
high-order relationships among the object-level representa-
tions as semantic units. The suggested method consists of
the following four modules.
3.1. Constructing Spatiotemporal Symbolc Graphs
In this paper, we deﬁne a spatiotemporal symbolic graph as
an input representation of a given video. A spatiotemporal
graph G={Gt}t=1,...,Nconsists of a sequence of snapshot
graphs, and each snapshot graph Gt=(At,Xt,Et)denotes
the attributed graph where At2Rn⇥nrepresents the sym-
metric adjacency matrix with nnodes, Xt2Rn⇥pis the at-
tribute matrix of pattributes per node, Et2Rn⇥n⇥qis the
attribute tensor of qattributes per edge. In that, as described
in Figure 2, a graph for a video with Nframes consists of
Nnumber of snapshot graphs, and each snapshot graph Gt
encodes information about nnumber of objects by consid-
ering the attributes such as color, shape, and material us-
ing the Xt. The information about the relationship, such as
distance and direction between nodes ni,nj, is encoded in
Ei,j. The process of constructing the graph Gfrom a video
can be easily implemented using well-established methods
such as object detection and attribute classiﬁcation, at least
for CATER videos.
3.2. Spatiotemporal Graph Transformer(ST-GT)
As discussed in Section 2, while transformer architectures
that can take graphs as input have been widely proposedrecently, there is a signiﬁcant lack of discussion on algo-
rithms capable of learning temporal graphs. In this paper,
we propose a novel transformer algorithm that takes a tem-
poral graph given as a sequence of static graphs as input,
enabling the learning of spatiotemporal correlations among
nodes.
The suggested method, Spatiotemporal Graph
Transformer(ST-GT), introduces a total adjacency ma-
trixAtotal2R(N⇥n)⇥(N⇥n)which involves temporal
auxiliary edges describing the temporal connection be-
tween two adjacent static graphs. In other words, it involves
additional temporal edges between nodes representing
the same object in adjacent static graphs. This allows the
representation not only of relationships between objects at
a single time point but also the temporal changes of objects
across connected frames. By introducing temporal auxiliary
edges, it is anticipated that the attention mechanism of
the transformer can learn relationships between objects in
adjacent frames.
The total adjacency matrix Atotalwith temporal auxil-
iary edges can be deﬁned as follows.
Atotal
i,j=8
><
>:At,for diagonal blocks
1,for (i, n+i)
1,for (n+j, j)(1)
It can be seen as a static graph consisting of Nsubgraphs,
the temporal relationship between adjacent frames is en-
coded in the off-diagonal blocks.
Inspired by recent work [ 16], all elements in the Gwith
Atotal, which are nodes, spatial edges, and temporal auxil-
iary edges, are tokenized to be fed into the transformer as
inputs. If the edge is deﬁned as an input token, informa-
tion about the connection between tokens might be lost. To
maintain this information, position embeddings represent-
ing spatial and temporal relationships are deﬁned and added
to the input tokens. The elements composing the input to-
kens of the suggested ST-GT can be summarized as follows.
The input Xof ST-GT consists of three types of tokens,
which are node, spatial edge, and temporal auxiliary edge.
Each token contains three kinds of information: feature, po-
sition embeddings, and type identiﬁer. The position embed-
dings represent the structural information of each node or
edge in the total adjacency matrix Atotal. The type identi-
ﬁer is introduced to indicate which of the three components
of the spatiotemporal graph each token represents.
Node-type Token In addition to the attribute feature de-
ﬁned in Section 3.1, information about time and object’s
pose is added. The feature vector of the node-type token
is represented with df-dimensional vectors. To consider
the structural information of each node, the dp-dimensional
eigenvector of graph Laplacian of Atotalis considered as
the positional embedding vector. Finally, dt-dimensional
learnable embedding is added to indicate the type of tokens.
18753
Figure 3. (a) Overall architecture of our model. First, through the graph construction and graph tokenizing process, we obtain ﬁne-grained
semantic units within the raw video. These tokens are passed to ST-GT, which captures spatiotemporal motifs. Next, we rearrange the output
tokens of ST-GT by object and learn higher-order semantics through an object-oriented video encoder. In the ﬁnal stage of the embedding
disentangling process, the complex semantic structure information contained in each object-oriented representation is disentangled to
multiple semantic elements. (b) Details of the process for obtaining semantic elements. (The dotted box in the bottom right corner) The
CATER dataset deﬁnes two types of complex actions: atomic actions (yellow dotted box) and composite actions. (purple dotted box) These
complex action labels are decomposed into multiple semantic elements(e.g. object, action, temporal relation) and used in the training
process.
Spatial/Temporal Edge-type Token As the edge connects
two nodes, the feature vector for the edge-type token is de-
vised by concatenating two feature vectors of nodes. Also,
as the positional embedding, two positional embedding vec-
tors of each node are concatenated. Similar to the node-
type token, two dt-dimensional learnable embeddings are
devised for spatial and temporal edges respectively.
Overall, three types of features as inputs of ST-GT
are constructed: Xnode 2RK⇥(df+dp+dt),Xedge s2
RL⇥(2⇥df+2⇥dp+dt),Xedge t2RM⇥(2⇥df+2⇥dp+dt),
where K,L,M are the number of nodes, spatial edges, and
temporal edges included in total graph (connecting Nnum-
ber of static graphs). To get the equal dimension size of
three types of feature vectors, a single-layer fully connected
network is introduced. Finally, the input of the ST-GT can
be deﬁned as X2R(K+L+M)⇥d.
Furthermore, although the size of the Atotalmatrix itself
is large, it is a highly sparse matrix. Since temporal auxiliary
edges are deﬁned only between adjacent frames, even when
breaking down a long video into multiple segments, it is
sufﬁcient to deﬁne Atotalwithin each segment. This results
in a manageable increase in computational complexity.3.3. Object-oriented Video Encoder
We suggest an object-oriented video encoder, which learns
object-level action representations by grouping each ob-
ject’s representations obtained in Section 3.2. To observe
the movement of objects within the video, output tokens
of spatiotemporal graph transformers are grouped based on
their object indices in the object-wise token rearrangement
module in Figure 3. This allows us to obtain tokens corre-
sponding to each object over time. These tokens are then
fed as input to the object-oriented video encoder, which
consists of causal masked self-attention, feed-forward layer.
Through an object-oriented video encoder, the temporal
movement of each object is encoded, resulting in object-
speciﬁc representations.
3.4. Compositional Learning by Disentangling Em-
beddings
The object-oriented action representation obtained in Sec-
tion3.3contains information on changes in the motion of
each object in the video. The information combines two
types of information, objects and actions. Rather than di-
rectly using the information obtained in Section 3.3for
classiﬁcation, we deﬁne a new learning method that dis-
entangles combined information into subspaces deﬁned by
18754
each semantic element(objects, and actions). This involves
predicting the ﬁnal label(composition of each semantic el-
ement) through a combination of predictions within each
subspace. This approach is essential to achieve the zero-shot
compositional generalization ability targeted in this paper,
as it requires understanding newly encountered data (com-
positional labels) as combinations of previously learned se-
mantic elements. To facilitate this, we introduce a method
where predictions are made for each element based on dis-
entangled features for object and action, rather than using
features where information about object and action is en-
tangled for label prediction.
To resolve this, entangled feature embeddings are pro-
jected onto two independent subspaces(Figure 3). Two
learnable embedding layers are introduced for each object
and action subspace. By concatenating the disentangled two
features, a multi-label classiﬁcation head is applied to pre-
dict labels according to the tasks deﬁned in CATER.
Considering each subspace embedding of objects, ac-
tions, and output of object-oriented video encoder, we com-
pute the overall loss of our model L=Lvid+Lcomp. Where
Lcomp=Lobj+Lact+LTR.Lvidis derived from the bi-
nary cross entropy of presence action classes in the video
using video-level representations. Video-level representa-
tions are obtained by mean pooling object-speciﬁc repre-
sentation of object-oriented video encoder. For strong com-
positional learning, we calculate Lcompby predicting the
object, action, and temporal relation of each composite ac-
tion label with binary cross entropy. Since temporal relation
does not exist in task 1, we applied LTRonly at task 2.
4. Compositional Generalization Test for
Videos
4.1. CATER dataset
For experiments, a most challenging dataset CATER [ 10] is
used to check the compositional video understanding abil-
ity of the suggested method. CATER dataset[ 10] consists of
5.5k videos and each video includes 300 frames with 320 by
240 pixels. Through 300 frames, multiple actions with vari-
ous objects having different durations are included. Speciﬁ-
cally, the number of objects appearing in each video ranges
from 5 to 10 and each object has four types of attributes
such as shape, size, material, and color.
With the CATER dataset, three different tasks are de-
ﬁned. The ﬁrst task is to predict atomic actions within the
trimmed video. There are 14 different atomic actions which
are combinations of object shapes(cube, cone, etc.) and ac-
tion types(rotate, slide, pick-place, and contain). The sec-
ond task is to predict compositional action labels, which
consider a temporal combination of two atomic actions.
Based on Allen’s temporal algebra, three types of tempo-
ral dependency (after, during, and before) are consideredto make compositional action labels. The third task is the
localization of the position in the last frame of the snitch
deﬁned as a special object in CATER. It is a classiﬁcation
problem about which cell in the frame has the snitch, also
challenging because the snitch is ﬁxed at a small size and
easily contained by other objects, making it hard to see it in
a certain range of frames.
4.2. A New Split for Compositional Generalization
Based on the CATER dataset, we constructed a new data
split to check the compositional generalization ability of
the video understanding models. The main idea behind con-
structing the new split for compositional generalization is
that the label comprises combinations of two or more se-
mantic units, where individual semantic units are seen dur-
ing the training phase, but the combinations of labels ob-
served during the test(or validation) phase are novel. In
other words, the aim is to ascertain whether, even though a
(compositional) label is seen for the ﬁrst time, it can be un-
derstood by decomposing the label into semantic units and
combining the units to comprehend the novel label. Since
the CATER dataset provides two-level action labels(tasks 1
and 2), we introduce new data splits to each level for com-
positional generalization tests as follows.
4.2.1 Compositional Generalization Test for Atomic
Action Recognition
The labels provided by the CATER dataset for Task 1 con-
sist of combinations of object and action types. While there
are ﬁve and four types of object and action, there are only
14 labels in total due to certain actions not being applica-
ble to speciﬁc objects (for example cone rotate ). We divide
these 14 labels into three disjoint label sets LA,LB,LC, as
illustrated in the following Figure 4(a), to create splits for
the compositional generalization test.
Based on the label splits LA,LB,LC, video clips cor-
responding to LAandLCare deﬁned as the training set,
while videos of label LBare deﬁned as the test set. With
this training and test split, each object and action compo-
nent consisting of the test labels is included in the training
data, while the speciﬁc label combinations of the test set are
novel.
Formally, the characteristics of the new data split can
be represented as follows. For training and test dataset
D1
train,D1
test,
O=SEEN O(D1
train)=SEEN O(D1
test),
A=SEEN A(D1
train)=SEEN A(D1
test).(2)
where, SEEN O(D)and SEEN A(D)represent the set of ob-
ject and action types included in dataset D, and O,Arepre-
sent the set of total object and action types.
18755
Figure 4. (a) Venn diagram representing the set of 14 labels of the
CATER dataset for Task 1. LA,LB,LCrefers to the three disjoint
sets that we have divided. (b) is an example of a before and after
relationship of new data split for Task 2. As shown in the ﬁgure, if
cylinder slide before cone pick place only appears in the training
set, the corresponding opposite label cylinder slide after cone pick
place only appears in the test set.
4.2.2 Compositional Generalization Test for Compos-
ite Action Recognition
Task 2 of the CATER dataset considers the temporal rela-
tionships between two actions, where each action consists
of object and action types. Therefore, we devise another
compositional generalization split according to temporal re-
lationships. As discussed in 4.1, three types of temporal re-
lationships (i.e., before, during, after) are proposed accord-
ing to Allen’s temporal algebra As the compositional action
labels consist of 14 types, 301 unique temporal action labels
are provided as classiﬁcation labels of Task 2. Similar to the
procedure of the ﬁrst data split, three different disjoint label
setsLA,LB,a n d L Cwere constructed from the 301 tempo-
ral action labels. The detailed procedure to construct three
label sets is as follows.
Before or After relationship After selecting a randomly
selected label X before Y and deﬁning it as an element of
LA, the opposite label Y before X is deﬁned as the element
ofLB. Then, other labels having the same temporal rela-
tionships (e.g., M before N, K before L ) are deﬁned as LC.
The same procedure applied to after relationship.
During relationship IfX during Y is selected as an element
ofLA, then any label with only one action label between X
and Y is deﬁned as an element of LB. In that, labels such
asX during A, A during Y are included in LB. Other labels
having the during relationship are deﬁned as LC.
Same action labels For the temporal labels having the
same action labels, e.g. A before A, B during B ,LA,LB,LC
are constructed in the level of actions. In that, as discussed
in Section 4.2.1 , object and action types are considered to
construct compositional generalization split.
After constructing the LA,LB,LC, video clips corre-
sponding to LAandLCare deﬁned as the training set, while
videos of label LBare deﬁned as the test set. The data splitensures the following characteristics.
For training and test dataset D2
train,D2
test,
Aa=SEEN Aa(D2
train)=SEEN Aa(D2
test),
T=SEEN T(D2
train)=SEEN T(D2
test).(3)
where, SEEN Aa(D)and SEEN T(D)represent the set of
atomic action and temporal relation types included in
dataset D, and Aa,Trepresent the set of total atomic ac-
tion and temporal relation types.
The two types of data split are available at
https://github.com/hy0Y/ST-GT/tree/main/CG data split.
5. Experimental Results
We evaluate the performance of the suggested algorithm
with the original CATER dataset and the newly suggested
split in section 4. Also, to demonstrate our algorithm in
the real-world dataset, we experimented with the MOMA-
LRG[ 23] video dataset.
5.1. Evaluation Metric and Implementation
According to the evaluation protocol of previous studies, the
mean average precision score (mAP) is used as an evalua-
tion metric for Tasks 1 and 2.
Architecture details To implement the ST-GT module, two
transformer layers with 4 heads are applied. For the object-
oriented video encoder, one layer (for task 1, three layers for
task 2) of GPT-2 architecture is adopted. For both modules,
128-dimensional and 256-dimensional embeddings for Task
1 and 2 are used, respectively.
During training, our model takes 2 hours for 50 epochs
with a batch size of 8 on 1-A100 machine. We use the
AdamW optimizer with  1=0.9,  2=0.99, and a weight de-
cay of 0.01, with a learning rate of 1e-4.
5.2. Quantitative Results
Across tasks 1 and 2, the suggested method achieves a
new state-of-the-art performance with meaningful margins.
In particular, our model signiﬁcantly outperformed com-
parative methods for task 2, which requires understanding
the temporal relationship between two actions. Based on
these quantitative results, we can argue that the disentan-
gled object-level representations are crucial for understand-
ing complex patterns deﬁning actions present in the long
videos.
Ablation Study To demonstrate our proposed method, we
conducted two ablation studies. First, to assess the impact
ofLcompmentioned in Section 3.4, we conducted experi-
ments by excluding Lcomp. As shown in Table 3, exclud-
ingLcompand using only Lvidresults in a signiﬁcant per-
formance degradation in both tasks. This demonstrates that
18756
Table 1. mAP score of comparative and our model on Task 1.
Method mAP
FasterRCNN [ 29] 63.85
Single stream SCI3D [ 29] 91.82
SCI3D [ 29] 96.77
R3D [ 4,12] 98.8
R3D + NL [ 37] 98.9
Ours 99.88
Table 2. mAP score of comparative and our model on Task 2.
Method mAP
FasterRCNN [ 29] 25.45
R3D [ 4,12] 44.2
R3D + NL [ 37] 45.9
R3D + LSTM [ 10] 53.4
R3D + NL + LSTM [ 10] 53.1
SCI3D + LSTM [ 29] 66.71
Single stream SCI3D + LSTM [ 29] 69.76
ViViT[ 1] 66.18
FROZEN[ 2] 66.64
Ours 75.40
structurally decomposing and comprehending complex ac-
tions, facilitated by Lcomp, is effective in video understand-
ing.
Table 3. Ablation study of the loss components
Task 1 Task 2
val test val test
Lvid 90.42 90.38 56.16 53.53
Lvid+Lcomp 99.89 99.88 76.07 75.40
Finally, we provide an ablation study about the three dif-
ferent token types of the ST-GT in Table 4. As can be seen
from the experimental results, it is crucial to deﬁne spa-
tial and temporal connectivity information in spatiotempo-
ral graphs as distinct types for learning to understand the
semantic structure of the entire video.
Compositional Generalization Test To check the compo-
sitional generalization ability, the suggested and compara-
tive methods are evaluated with the newly suggested splits.
As a comparative method for the ﬁrst compositional gen-
eralization split, the R3D-based method[ 4,10,12,37] is
used because the code for the SCI3D method[ 38] was not
published. In Table 5, it can be observed that our pro-
posed method demonstrates remarkable performance im-Table 4. Ablation study of 3 token types composing ST-GT’s input
Token Types Dataset
N SE TE CATER MOMA-LRG
X - - 72.49 49.76
XX - 74.49 66.47
X - X 74.48 66.78
XXX 75.40 72.83
Table 5. Compositional generalization result of Task 1
Validation Test
Baseline Ours Baseline Ours
D1
test 25.78 72.95 25.05 71.53
D1
train 93.95 98.9 93.04 99.1
Table 6. Compositional generalization result of Task 2
Split R3D R3D+LSTM Ours
ValidationD2
test 4.31 4.00 69.45
D2
train 60.32 76.49 81.1
TestD2
test 3.47 3.51 69.2
D2
train 58.23 74.84 80.9
provements even for unseen labels ( D1
test) than the com-
parative method. Because the comparative method performs
classiﬁcation based on labels deﬁned by combinations of
action and object types, it is limited to consider the sepa-
rate inﬂuence of each element(action and object types) on
the classiﬁcation decision. However, the suggested method,
by considering information separately for each action and
object type during training, can readily learn even novel
combinations. Furthermore, not only for the compositional
generalization setting but also the suggested method out-
performs the comparative method in a setting similar to the
traditional machine learning problem, which is learning and
evaluating with the same label set( D1
train).
In the data split based on Task 2(considering temporal
relationships between actions), we observed even greater
performance improvements. As a comparative method for
this setting, two R3D-based methods (R3D and R3D with
two LSTM layers) are used because the code for the SCI3D
method was not published. This compositional generaliza-
tion setting, compared to the previous one, involves a large
number of class labels and is more complex, comprising
three elements to form the label(object, action, and tempo-
ral relationship types). As a result, the comparative method
shows very low performance, around 2%. On the other
hand, the suggested method demonstrated the ability to pre-
18757
dict unseen label sets in this problem with an accuracy of
over 80% (Table 6).
5.3. Qualitative Results
To thoroughly understand the characteristics of our pro-
posed model, we conducted two qualitative experiments.
Firstly, in the CATER Task 2 setting, we examined how the
accuracy changes with variations in the time difference be-
tween the occurrences of the two actions that constitute the
label (for example, for a label X before Y , the time differ-
ence between the occurrence of X and Y). As can be seen
in Figure 5(a), we conﬁrmed that the proposed method ex-
hibits excellent performance overall, particularly achieving
remarkably high accuracy in predicting long video clips.
Conventional video understanding methods usually extract
features from uniformly segmented video clips and compre-
hend the overall video by averaging them or adding simple
temporal models. Therefore, this approach has a crucial dis-
advantage; as the video lengthens, the information becomes
aggregated. On the other hand, the suggested method efﬁ-
ciently extracts information corresponding to semantic units
in long videos and can comprehend complex labels through
the relationships between these units, making it robust to
changes in length.
Similarly, we examined how the number of interfering
actions between two actions affects the prediction. As can
be seen in Figure 5(b), similar to the time difference case,
the suggested method demonstrated high performance re-
gardless of the presence of a disrupter between the two ac-
tions.
5.4. Experiments with Real-world Dataset
Finally, additional experiment results with a real-world
video dataset are presented. The MOMA-LRG dataset[ 23]
is proposed to recognize human actions included in videos,
providing annotations in the form of symbolic graphs rep-
resenting relationships between objects and humans in the
video. With the symbolic graph provided in the dataset,
we show the results of the action classiﬁcation tasks (sub-
activity level labels in the MOMA-LRG dataset) in a sim-
ilar setting to the previously conducted with the CATER
dataset. As shown in Table 7, our model achieved remark-
able performance with mAP 72.83, outperforming compar-
ative models, and showed the adaptability and effectiveness
of our method on a real-world dataset.
Table 7. Performance comparison and model details on MOMA-
LRG dataset
Method mAP # params Inference time (ms)
ViViT [1] 65.09 99M 23
FROZEN [2] 69.36 115M 40
Ours 72.83 7.3M 20
Figure 5. Performance with variations in times and disrupters. In
(a), we demonstrated how the mean recall performance varies with
the temporal difference between two actions in videos. The con-
ventional model(R3D+LSTM) that extracts features on a frame-
by-frame basis and averages them exhibits a decrease in perfor-
mance as the temporal gap between actions increases. However,
our model maintains strong performance even in long videos. Also
in (b), the R3D+LSTM model suffers when the number of inter-
fering actions between two actions increases.
6. Conclusion with Future Direction
We have introduced a novel algorithm effective in under-
standing complex semantic structures within long videos.
We anticipate that this research will contribute to achiev-
ing human-level video understanding for long videos such
as movies or TV shows and human-level reasoning perfor-
mance.
7. Acknowledgements
This work is supported by IITP grant funded by MSIT
(Grant No. 2022-0-00264/40%, 2022-0-00612/20%, 2022-
0-00951/20%,) and IITP Artiﬁcial Intelligence Graduate
School Program for Hanyang University funded by MSIT
(Grant No. 2020-0-01373/20%).
18758
References
[1]Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen
Sun, Mario Lu ˇci´c, and Cordelia Schmid. Vivit: A video
vision transformer. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 6836–6846,
2021. 2,7
[2]Max Bain, Arsha Nagrani, G ¨ul Varol, and Andrew Zisser-
man. Frozen in time: A joint video and image encoder for
end-to-end retrieval. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 1728–1738,
2021. 7
[3]Amir Bar, Roei Herzig, Xiaolong Wang, Anna Rohrbach,
Gal Chechik, Trevor Darrell, and Amir Globerson. Com-
positional video synthesis with action graphs. arXiv preprint
arXiv:2006.15327 , 2020. 2
[4]Joao Carreira and Andrew Zisserman. Quo vadis, action
recognition? a new model and the kinetics dataset. In pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 6299–6308, 2017. 2,7
[5]Vijay Prakash Dwivedi and Xavier Bresson. A general-
ization of transformer networks to graphs. arXiv preprint
arXiv:2012.09699 , 2020. 3
[6]Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li,
Zhicheng Yan, Jitendra Malik, and Christoph Feichten-
hofer. Multiscale vision transformers. In Proceedings of
the IEEE/CVF international conference on computer vision ,
pages 6824–6835, 2021. 2
[7]Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman.
Convolutional two-stream network fusion for video action
recognition. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1933–1941,
2016. 2
[8]Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia
Schmid. Multi-modal transformer for video retrieval. In
Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part IV 16 ,
pages 214–229. Springer, 2020. 2
[9]Rohit Girdhar and Kristen Grauman. Anticipative video
transformer. In Proceedings of the IEEE/CVF international
conference on computer vision , pages 13505–13515, 2021. 2
[10] Rohit Girdhar and Deva Ramanan. Cater: A diagnostic
dataset for compositional actions and temporal reasoning.
arXiv preprint arXiv:1910.04744 , 2019. 2,3,5,7
[11] Madeleine Grunde-McLaughlin, Ranjay Krishna, and Ma-
neesh Agrawala. Agqa: A benchmark for compositional
spatio-temporal reasoning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 11287–11297, 2021. 2
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 7
[13] Lisa Anne Hendricks, Subhashini Venugopalan, Marcus
Rohrbach, Raymond Mooney, Kate Saenko, and Trevor Dar-
rell. Deep compositional captioning: Describing novel object
categories without paired training data. In Proceedings ofthe IEEE conference on computer vision and pattern recog-
nition , pages 1–10, 2016. 2
[14] Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun.
Heterogeneous graph transformer. In Proceedings of the web
conference 2020 , pages 2704–2710, 2020. 3
[15] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolu-
tional neural networks for human action recognition. IEEE
transactions on pattern analysis and machine intelligence ,
35(1):221–231, 2012. 1,2
[16] Jinwoo Kim, Dat Nguyen, Seonwoo Min, Sungjun Cho,
Moontae Lee, Honglak Lee, and Seunghoon Hong. Pure
transformers are powerful graph learners. Advances in Neu-
ral Information Processing Systems , 35:14582–14595, 2022.
3
[17] Alexander Klaser, Marcin Marszałek, and Cordelia Schmid.
A spatio-temporal descriptor based on 3d-gradients. In
BMVC 2008-19th British Machine Vision Conference , pages
275–1. British Machine Vision Association, 2008. 2
[18] Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent
L´etourneau, and Prudencio Tossou. Rethinking graph trans-
formers with spectral attention. Advances in Neural Infor-
mation Processing Systems , 34:21618–21629, 2021. 3
[19] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and
Juan Carlos Niebles. Dense-captioning events in videos. In
Proceedings of the IEEE international conference on com-
puter vision , pages 706–715, 2017. 2
[20] Ivan Laptev. On space-time interest points. International
journal of computer vision , 64:107–123, 2005. 2
[21] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg.
Tvqa: Localized, compositional video question answering.
arXiv preprint arXiv:1809.01696 , 2018. 2
[22] Juncheng Li, Junlin Xie, Long Qian, Linchao Zhu, Siliang
Tang, Fei Wu, Yi Yang, Yueting Zhuang, and Xin Eric Wang.
Compositional temporal grounding with structured varia-
tional cross-graph correspondence learning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 3032–3041, 2022. 2
[23] Zelun Luo, Zane Durante, Linden Li, Wanze Xie, Ruochen
Liu, Emily Jin, Zhuoyi Huang, Lun Yu Li, Jiajun Wu,
Juan Carlos Niebles, et al. Moma-lrg: Language-reﬁned
graphs for multi-object multi-actor activity parsing. Ad-
vances in Neural Information Processing Systems , 35:5282–
5298, 2022. 2,6,8
[24] Wonpyo Park, Woonggi Chang, Donggeon Lee, Juntae Kim,
and Seung-won Hwang. Grpe: Relative positional encod-
ing for graph transformer. arXiv preprint arXiv:2201.12787 ,
2022. 3
[25] Jeffrey Pennington, Richard Socher, and Christopher D Man-
ning. Glove: Global vectors for word representation. In
Proceedings of the 2014 conference on empirical methods
in natural language processing (EMNLP) , pages 1532–1543,
2014. 1
[26] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence
embeddings using siamese bert-networks. arXiv preprint
arXiv:1908.10084 , 2019. 2
[27] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei,
Wenbing Huang, and Junzhou Huang. Self-supervised graph
18759
transformer on large-scale molecular data. Advances in Neu-
ral Information Processing Systems , 33:12559–12571, 2020.
3
[28] Karen Simonyan and Andrew Zisserman. Two-stream con-
volutional networks for action recognition in videos. Ad-
vances in neural information processing systems , 27, 2014.
2
[29] Akash Singh, Tom De Schepper, Kevin Mets, Peter
Hellinckx, Jos ´e Oramas, and Steven Latr ´e. Deep set con-
ditioned latent representations for action recognition. arXiv
preprint arXiv:2212.11030 , 2022. 7
[30] Cees GM Snoek, Marcel Worring, et al. Concept-based video
retrieval. Foundations and Trends® in Information Retrieval ,
2(4):215–322, 2009. 2
[31] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,
and Manohar Paluri. Learning spatiotemporal features with
3d convolutional networks. In Proceedings of the IEEE inter-
national conference on computer vision , pages 4489–4497,
2015. 1,2
[32] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann
LeCun, and Manohar Paluri. A closer look at spatiotemporal
convolutions for action recognition. In Proceedings of the
IEEE conference on Computer Vision and Pattern Recogni-
tion, pages 6450–6459, 2018. 1
[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 2,3
[34] Petar Veli ˇckovi ´c, Guillem Cucurull, Arantxa Casanova,
Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph at-
tention networks. arXiv preprint arXiv:1710.10903 , 2017.
3
[35] Heng Wang and Cordelia Schmid. Action recognition with
improved trajectories. In Proceedings of the IEEE inter-
national conference on computer vision , pages 3551–3558,
2013. 2
[36] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua
Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment net-
works: Towards good practices for deep action recognition.
InEuropean conference on computer vision , pages 20–36.
Springer, 2016. 2
[37] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-
ing He. Non-local neural networks. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 7794–7803, 2018. 2,7
[38] Zhuoyuan Wu, Jian Zhang, and Chong Mou. Dense deep un-
folding network with 3d-cnn prior for snapshot compressive
imaging. arXiv preprint arXiv:2109.06548 , 2021. 7
[39] Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar,
and Kannan Achan. Inductive representation learning on
temporal graphs. arXiv preprint arXiv:2002.07962 , 2020.
1
[40] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun
Wu, Antonio Torralba, and Joshua B Tenenbaum. Clevrer:
Collision events for video representation and reasoning.
arXiv preprint arXiv:1910.01442 , 2019. 2
[41] Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, and Wei
Xu. Video paragraph captioning using hierarchical recurrentneural networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 4584–4593,
2016. 2
[42] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang,
and Hyunwoo J Kim. Graph transformer networks. Advances
in neural information processing systems , 32, 2019. 3
18760
