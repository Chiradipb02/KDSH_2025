MarkovGen: Structured Prediction for EfÔ¨Åcient Text-to-Image Generation
Sadeep Jayasumana Daniel Glasner Srikumar Ramalingam Andreas Veit
Ayan Chakrabarti Sanjiv Kumar
Google Research, New York
fsadeep, dglasner, rsrikumar, aveit, ayanchakrab, sanjivk g@google.com
Early Exit Muse: Fewer steps1.5x fasterFull Muse: All stepsMarkovGen: Fewer steps + MRF1.5x faster
Figure 1. MarkovGen improves the speed and quality of token-based image generation models such as Muse, by reducing the number of
sampling steps and replacing them with a light-weight Markov Random Field (MRF) model.
Abstract
Modern text-to-image generation models produce high-
quality images that are both photorealistic and faithful to
the text prompts. However, this quality comes at signiÔ¨Å-
cant computational cost: nearly all of these models are it-
erative and require running sampling multiple times with
large models. This iterative process is needed to ensure
that different regions of the image are not only aligned with
the text prompt, but also compatible with each other. In
this work, we propose a light-weight approach to achiev-
ing this compatibility between different regions of an image,
using a Markov Random Field (MRF) model. We demon-
strate the effectiveness of this method on top of the latent
token-based Muse text-to-image model. The MRF richlyencodes the compatibility among image tokens at different
spatial locations to improve quality and signiÔ¨Åcantly reduce
the required number of Muse sampling steps. Inference
with the MRF is signiÔ¨Åcantly cheaper, and its parameters
can be quickly learned through back-propagation by model-
ing MRF inference as a differentiable neural-network layer.
Our full model, MarkovGen, uses this proposed MRF model
to both speed up Muse by 1:5and produce higher quality
images by decreasing undesirable image artifacts.
1. Introduction
Recent image-to-text models [18, 21, 24, 25, 34] are
remarkably successful at producing high-quality, photo-
realistic images that are faithful to the provided text
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
9316
prompts, and are poised to drive a new generation of tools
for creativity and graphic design. However, the generation
process with these models is iterative and computationally
expensive, requiring multiple sampling steps through large
models. For example, diffusion models [24, 25] require
multiple denoising steps to generate the Ô¨Ånal image, the
Parti model [34] auto-regressively generates image tokens
one at a time. While the recently proposed Muse model [2]
generates multiple tokens at a time, it still requires a large
number of sampling steps to arrive at the Ô¨Ånal image.
This iterative process is needed to ensure that different
regions or patches of the images are not only aligned with
the provided text prompt, but also compatible with each
other . Current text-to-image models achieve this spatial
compatibility by repeatedly applying their full model mul-
tiple times on intermediate image predictions‚Äîa process
that is computationally very expensive. In this paper, we
demonstrate that a signiÔ¨Åcantly lighter-weight approach can
achieve the same compatibility.
To this end, we propose a new structured prediction
approach that applies to image generation models operat-
ing in a discrete token space, such as the VQGAN token
space [2,3,8]. These models generate images by Ô¨Årst select-
ing tokens in a Ô¨Åxed-size token grid and later detokenizing
them into an RGB image. Usual token-based image gen-
eration methods select tokens by independently sampling
from the probability distributions at different patch loca-
tions. In contrast, we model the whole image jointly us-
ing a fully-connected Markov Random Field (MRF) that
encodes compatibility between all pairs of tokens (image
patches). The tokens at different patch locations are then
determined based on this joint distribution. Consequently,
as illustrated in Figure 2, a conÔ¨Ådent token at one location
can inÔ¨Çuence the selected tokens at other locations to en-
hance the overall compatibility of the token arrangement,
and therefore the Ô¨Ådelity of the Ô¨Ånal image. We use mean-
Ô¨Åeld inference [13, 14, 36] to solve this MRF, which also
permits training the compatibility parameters of the model
through back-propagation. During image generation with
a trained model, the MRF inference comes at a negligible
cost compared to the cost of large Transformer models used
to predict the initial token probabilities.
To showcase the beneÔ¨Åts of our MRF model, we intro-
duce a new text-to-image model, MarkovGen, that can work
in conjunction with the Muse model [2]. Muse uses a par-
allel decoding approach where all tokens of an image are
predicted in parallel at each step. Muse has been shown
to be much faster (around 3faster than the closest com-
petitor) than other state-of-the-art image generation models
such as DALL-E, Imagen, Parti, and Stable Diffusion, while
producing similar or better quality images [2]. Although
Muse produces predictions for every patch simultaneously,
single-shot parallel decoding leads to serious quality degra-
Figure 2. BeneÔ¨Åts of encouraging token compatibility with an
MRF model. During MRF inference, a conÔ¨Ådent token, such as
the token representing the giraffe‚Äôs eye, encourages the neighbor-
ing tokens to be compatible to represent other parts of a giraffe
face such as ears and nose. Similarly, tokens representing the
texture of the giraffe body can inÔ¨Çuence nearby tokens to encour-
age consistent patterns. Our formulation also supports long-range
connections, such as the one shown with the dashed yellow line.
dation in the generated images [2]. Muse solves this by
embracing progressive parallel decoding, where a small in-
cremental number of high conÔ¨Ådence tokens are Ô¨Åxed after
each iteration. We show that by learning the compatibility
of the tokens and applying the MRF inference after limited
number of sampling steps with the Muse model, we achieve
signiÔ¨Åcant quality and efÔ¨Åciency gains over the Muse‚Äôs full
iterative approach (see Figure 1).
Reducing the latency and improving the quality of Muse,
one of the fastest text-to-image models, will have important
practical implications for real-world deployments. The suc-
cess of our MRF formulation in modeling spatial and label
relationships of image tokens opens up the future possibility
of reÔ¨Åning predictions of other token-based methods such as
Parti [34] and discrete-diffusion models [11] with MRFs.
In summary, our contributions are:
We propose an MRF model, a type of probabilistic
graphical model, that can predict a globally compati-
ble set of image tokens by explicitly modeling spatial
and token label relationships. To the best of our knowl-
edge, this is the Ô¨Årst work to exploit MRFs to improve
the efÔ¨Åciency and quality of text-to-image models.
Our MarkovGen model, where we replace the last few
steps of Muse with a learned MRF layer, leads to a
1:5speedup as well as improved quality results, as
demonstrated by human evaluation and FID distances.
We show that the MRF model parameters can be
trained in just a few hours, allowing us to quickly com-
bine the MRF model with pre-trained Muse models to
reap efÔ¨Åciency and quality gains.
9317
2. Related Work
Text-to-Image Generation: In recent years, papers such
as [2,3,10,15,21,24,25,27,33‚Äì35] have proposed a diverse
variety of methods to generate high-quality images given a
text prompt as input. We discuss some of the most relevant
approaches below.
Many text-to-image models [9, 18, 19, 21, 24, 25] use de-
noising diffusion probabilistic models (DDPM) [12] to gen-
erate images, where the model is invoked successively to
‚Äúdenoise‚Äù previous intermediate versions and progressively
reÔ¨Åne the image output. While in theory, we need inÔ¨Ånitely
small and many denoising steps, only a few hundreds of
steps are used in practice [28]. Progressive distillation al-
gorithms are being developed to cut down the number of
steps [26]. Most of these models directly operate on and
produce pixel intensities, [24, 35] are variants that operate
on a lower-dimensional latent representation.
In contrast, the Parti [34], DALL-E [22], and Muse [2]
models generate images in a space of discrete token repre-
sentation. They use a VQGAN [8] model, derived from VQ-
V AE [32], to represent non-overlapping image patches with
tokens‚Äîwith values from a discrete vocabulary‚Äîand cast
the image generation task as that of generating image to-
kens. The Parti and DALL-E models approach token gener-
ation with auto-regressive modeling, generating tokens one
at a time in sequence, where each token is generated condi-
tioned on the text input and all previously generated tokens.
The Muse model [2], on the other hand, is trained to
take the text prompt and any already generated image to-
kens as input, and make predictions for all remaining image
tokens simultaneously. In particular, it is trained as a BERT-
style [6] encoder model operating on a masked set of image
tokens (with tokens not already generated being masked),
with cross-attention to an encoding of the text prompt input.
To generate an image, the model is invoked in multiple sam-
pling steps, with all image tokens being masked in the Ô¨Årst
step. At each step, the Muse model makes predictions for
all masked tokens. A subset of these predictions are selected
and added to the set of Ô¨Åxed and non-masked tokens, which
are then used as conditioning input for subsequent invoca-
tions till the all tokens have been Ô¨Åxed. Similar to Muse,
Paella [23] and Cogview2 [7] also exploit progressive par-
allel decoding to achieve speedup. A similar approach to
parallel decoding for text was introduced by [17].
Like many other text-to-image generation models, Muse
Ô¨Årst generates a low-resolution version of the target image,
and then conditions on this low-resolution image to gener-
ate the high-resolution version. It uses a similar architecture
and sampling approach for the high-resolution generation
stage, except in this case, the low-resolution image tokens
are provided as additional conditioning input.
For the selected tokens at each sampling stage of Muse,
the token values are determined independently for each to-ken from the predicted per-token distributions. Our struc-
tured prediction approach, in contrast, considers compat-
ibility between the values of different tokens, and by do-
ing so, is able to improve the quality and reduce the num-
ber of sampling steps required‚Äîin both the low- and high-
resolution stages.
Many of the text-to-image algorithms are also being ex-
tended to develop algorithms to handle other conditional in-
puts [35], and text-to-video generation [10, 27, 33].
Structured Prediction: Markov and Conditional random
Ô¨Åelds (CRF)s have a long history of being used in com-
puter vision for diverse applications such as stereo, seg-
mentation, and image reconstruction [31]. These MRF and
CRF models have typically been used to enforce smooth-
ness constraints, i.e., that semantic labels, pixel intensities,
stereo depths, etc. at nearby locations are similar. In neural
network-based methods too, they have been a useful post-
processing step [4] to yield smooth consistent results.
While early MRF and CRF models considered edges
only among immediate pixel neighbors on the image plane,
[14] introduced ‚Äúfully-connected‚Äù CRF models that had far
longer range connections, and showed that the energy for
these models could effectively be minimized using mean-
Ô¨Åeld inference. Using this fully-connected formulation,
[36] proposed back-propagating through the mean-Ô¨Åeld in-
ference steps to jointly train a CRF model with a CNN net-
work to achieve better semantic image segmentation.
In this work, we use an MRF formulation to achieve
consistency in predicted image tokens in the context of
text-to-image generation, and like [36], we also use a fully
connected MRF model and learn its parameters by back-
propagation. However, in our case, the MRF is deÔ¨Åned over
tokenized patches, the label space corresponds to the vocab-
ulary of a VQGAN [8] and the MRF enforces consistency
between different token values rather than smoothness.
It is worth mentioning here that CRFs have recently also
been proposed to improve text generation [29, 30]. Like
our case, these methods also use a Transformer model to
generate ‚Äúunaries‚Äù that are then provided as input to a CRF
model. However, these methods consider edges only be-
tween neighboring tokens, and since text sequences are one-
dimensional, are able to use chain decoding techniques (like
beam search) for inference. In contrast, our method reasons
with a two-dimensional MRF model with edges between all
pairs of patches in the image.
3. Structured Token Prediction
In this section, we introduce our MRF formulation for
structured token prediction. In token-based image genera-
tion, a neural network (often a Transformer model) makes
predictions to generate a Ô¨Åxed size ( 1616, for example) to-
ken image containing token labels. This token image is then
sent through a detokenizer to generate an RGB image [8].
9318
402123517434082211902499960042080016421121374952001121900
1819240211902Spatial compatibility40223517434089219022046004208001642178974952001121800
Label compatibility
Imperfect token ImageFixed token imageMRF InferenceDetokenizationDetokenizationFigure 3. Given individual token probabilities from an underly-
ing Transformer-based image generation backbone, the MRF im-
proves image quality by utilizing learned spatial and label com-
patibility relations in the latent token space.
Consider a common vocabulary size of V= 8192 . For a
full sized 1616image, there are 8192256= 6:7101002
different arrangements of tokens, many of which will rep-
resent some kind of ‚Äúgarbage‚Äù images that lie outside the
manifold of photorealistic images. Intuitively, a structured
prediction mechanism that accounts for the compatibility of
token arrangements could signiÔ¨Åcantly reduce this massive
search space of token arrangements and make the token pre-
diction models more efÔ¨Åcient.
We propose a probabilistic graphical model for this
structured prediction task. SpeciÔ¨Åcally, we formulate Ô¨Ånd-
ing the token arrangement as maximum a posteriori (MAP)
inference of an MRF model, as described in the following.
The high-level idea is illustrated in Figure 3.
Leti2 f1;2; : : : ; ngdenote the location indices of
the token image, arranged in row-major order. Let L=
fl1; l2; : : : ; l Vgbe the token labels, which are used to in-
dex each element in the codebook of V tokens. For a
1616token image with vocabulary size 8192 , we have
n= 256 andV= 8192 . DeÔ¨Åne a random variable Xi2L
for each i= 1;2; : : : ; n to hold the token assignment for
theithlocation. The collection of these random variables
X= [X1; X2; : : : ; X n]then forms a random Ô¨Åeld, where
the value of one variable depends on that of the others. We
can then model the probability of an assignment to this ran-
dom Ô¨Åeld (and therefore a token arrangement on the grid)
with the Gibbs measure:
P(X=x) =1
Zexp( E(x)); (1)
where x2 Lnis a given token arrangement and Z=P
xexp( E(x))is the partition function. The ‚Äúenergy‚Äù
E(x)of an assignment x=fx1; x2; : : : ; x ngis modeled
with two components: the unary component ui(:)and the
pairwise component pij(:; :):Algorithm 1 The MRF Inference Algorithm
Qi(k) softmax( fi(k)),8(i; k)
fornumiterations do
Qi(k) Pn
j=1WsijQj(k),8(i; k)
Qi(k) PV
k0=1Wckk0Qi(k0),8(i; k)
Qi(k) Qi(k) +fi(k),8(i; k)
Qi(k) softmax( Qi)(k),8(i; k)
end for
return Q
E(x) =nX
i=1ui(xi) +nX
i=1nX
j=1pij(xi; xj): (2)
The unary component captures the conÔ¨Ådence of the
neural network prediction model, such as a Transformer
model, for a given token and a location. Therefore, given
a condition y, such as a text prompt or pre-Ô¨Åxed tokens, if
the neural network‚Äôs predicted logit value for location iand
labelxiisfi(xi; y), we set:
ui(xi) = fi(xi; y): (3)
Note that we use negative logits because the energy function
is in the log domain and a high energy corresponds to a low
probability. We drop conditioning on yhereafter to keep the
notation uncluttered. Also note that our MRF formulation
is not conditioned on y.
The pairwise component, pij(xi; xj), captures the com-
patibility of the label xiassigned to the location iand the
label xjassigned to the location j. It encodes the notion
that while some pairs of tokens are highly compatible with
each other and can appear in the same image, other pairs
are highly incompatible. For example, a token representing
an eye of a giraffe is more likely to appear next to a token
representing a different part of a giraffe face, than a token
representing something completely different like a part of
car wheel. We factorize this pairwise compatibility into two
parts: the spatial similarity s(i; j)between the locations i
andj(for example, if iandjare close to each other in the
2D token image, they will be strongly related) and the label
compatibility c(xi; xj)between the tokens xiandxj(for
example, highly compatible tokens are able to coexist with
each other). We therefore have:
pij(xi; xj) = c(xi; xj)s(i; j): (4)
In classic MRFs, the pairwise interactions exist only be-
tween neighboring pixels. In contrast, for increased Ô¨Çexi-
bility, we allow interactions between all pairs of locations,
9319
similar to the fully-connected CRFs in the image segmenta-
tion setting [14,36]. However, there are a number of impor-
tant differences in our formulation compared to the fully-
connected CRFs in image segmentation: in the latter, spatial
similarity s(i; j)is derived conditioned on the input image
(hence the name conditional random Ô¨Åelds), using Gaus-
sian potentials in spatial and bilateral domains. This Gaus-
sian assumption is crucial for the tractability of their models
since the image segmentation CRFs work in a large image
grid: in practical implementations pixels that are far away
by more than a few standard deviations of the Gaussian ker-
nel are considered not connected [1]. In contrast, we make
our graphical model truly fully-connected and learn s(i; j)
with backpropagation without Ô¨Åxing them to be Gaussian.
Furthermore, the CRFs in image segmentation can assume
a Potts model for label compatibility because assigning the
same label to nearby pixels generally improves the smooth-
ness of the segmentation. In our application, on the other
hand, it is not straightforward to assign semantic meanings
to tokens and Potts model does not intuitively makes sense
since the same token at similar locations does not increase
the meaningfulness of a token assignment. We therefore
learn the pairwise connections pij(:; :), completely from
data without using any priors or heuristics. Thus, our MRF
formulation has two learnable weight matrices: Ws, with
Wsij:=s(i; j)andWc, with Wckk0:=c(k; k0).
Given our probabilistic graphical model, Ô¨Ånding the Ô¨Å-
nal token arrangement amounts to Ô¨Ånding the assignment x
that maximizes P(X=x). This can be done efÔ¨Åciently
via mean-Ô¨Åeld inference, where we approximate P(X)
Q(X) :=Q
iQi(Xi), with Qi(:)being the marginal dis-
tribution for Xi. The distribution Q(X)is then iteratively
reÔ¨Åned to minimize the KL divergence between PandQ.
We refer the reader to [13] and [14] for more details on the
derivations. The resulting inference algorithm is summa-
rized in Algorithm 1. Note that all operations of this algo-
rithm can be implemented via simple matrix multiplication
and other common operations such as softmax( :), which
are readily available in any deep learning library. Impor-
tantly, the cost our MRF inference is negligible compared
to prediction with a large Transformer model.
4. MarkovGen
We now demonstrate the beneÔ¨Åts of the proposed MRF
model by using it to speed up the state-of-the-art Muse im-
age generation model [2]. We achieve this speed-up by re-
placing the last few sampling steps of Muse with MRF in-
ference. SpeciÔ¨Åcally, we let Muse execute the Ô¨Årst few steps
and then use our extremely lightweight MRF inference
to fast-forward the remaining steps. This model, dubbed
MarkovGen, improves the speed of image generation by
1:5while simultaneously also improving quality.
The Muse model works in the discrete VQGAN tokenModel Time (ms)
Muse base (single step) 10.40
Muse super-resolution (single step) 24.00
MRF inference on base 0.29
MRF inference on super-resolution 0.29
T5-XXL inference 0.30
Detokenizer 0.15
Muse 442.05
MarkovGen (ours) 281.03
Table 1. Average inference times for different components of the
MarkovGen models on a TPUv4 device. The MRF inference is al-
most free compared to the costs of the Muse Transformer models.
Furthermore, MRF inference is independent of the image resolu-
tions (rows 3 and 4). We make Muse inference 1:5faster by
introducing the MRF model.
space [8], which is gradually emerging as a centerpiece of
many text-to-image generation algorithms. Muse generates
images by Ô¨Årst performing a series of inference steps with
the base model to predict a small grid of 1616image
tokens, conditioned on text embeddings generated by a T5-
XXL [20] text encoder. This is followed by a few steps
of the super-resolution (SR) model to predict a larger grid
of3232image tokens by conditioning on both the text
embeddings and the tokens generated by the base model.
Due to the larger set of tokens, a single iteration of the
SR transformer model is substantially more computation-
ally expensive than that of the base model. We exploit this
multi-scale approach to speed up inference efÔ¨Åciency by us-
ing more steps in the base model, followed by much fewer
steps with the SR model. Once SR tokens are generated,
the VQGAN [8] detokenizer is used to render the image in
pixel space.
The goal of MarkovGen is to fast-forward the later part
of the Muse model and replacing it with the MRF outlined
in the previous section. To achieve this, we train the MRF
to match the Ô¨Ånal predictions of the Muse model, given the
output at an intermediate step. By fast-forwarding after the
stepk, out of a total of n, we instantly save (n k)=n
100% of the Muse model‚Äôs inference time. This is because
the inference time of the MRF is negligible compared to that
of the Muse steps as shown in Table 1. The same strategy is
used for both the base model and the SR model, to achieve
an overall boost of 1:5in inference speed.
Muse determines the token values independently at each
sampling stage, and our structured prediction, enforces the
learned compatibility relations jointly on the different token
values, and thereby leading to improved quality as demon-
strated in the experiments.
9320
Step 1Step 2Step 3Step 4Direct sampling
With MRFStep 1Step 2Step 3Step 4Early Exit MuseMarkovGenFigure 4. The Ô¨Årst four steps of the Muse super-resolution model without (top) and with (bottom) the application of the MarkovGen MRF
model. Note that the MRF Ô¨Åxes complex object structures such as the dog‚Äôs face as well as texture-inconsistencies in areas such as the
brick wall. MarkovGen generates good looking high quality images starting from the Ô¨Årst step.
5. Experiments
In this section, we show that MarkovGen achieves both
faster inference and improved image quality compared to
Muse, which it uses as its backbone. Muse by itself has
already been shown to be much faster than other state-
of-the-art text-to-image models such as Dall-E, Dall-E 2,
Parti, Imagen, and Stable Diffusion [2], outperforming the
second-fastest method by a factor of approximately 3(Ta-
ble 3 of [2]). Furthermore, as evidenced by Table 1 & 2
of [2], Muse achieves better quality results compared to
these methods, as measured with the FID scores. Human
evaluation results for image quality in [2] showed that hu-
mans preferred Muse outputs for 70:6%prompts while Sta-
ble Diffusion was preferred for only 25:4%. Since Muse is
already shown to outperform other state-of-the-art methods
in terms of both speed and quality, we focus on comparing
our results to that of Muse.
Model and Dataset: We use a Muse model with approxi-
mately 1.7B parameters, trained on the WebLI dataset [5].
This model was generously made available to us by the au-
thors of the Muse paper. We refer the reader to [2] for more
details on the architecture and the training setup of Muse.
The same WebLI dataset was used to train the MRF model.
MRF Training: We train two MRF models, one for fast-forwarding the base and one for SR model respectively.
Each model contains two weight matrices for spatial and
label compatibilities: 256256Ws
base and81928192
Wc
base for thr base, and 10241024 Ws
SRand81928192
Wc
SRfor the SR model. All MRF weights are trained with
back-propagation and gradient descent, with the ADAM op-
timizer. We use a two-stage approach for MRF training.
First, we pre-train the MRF model using a self-supervised
masked-token prediction loss [2, 6]. SpeciÔ¨Åcally, we ob-
tain VQGAN tokens for an image, randomly mask 20% of
them and train the MRF model to predict the masked to-
kens using the categorical cross-entropy loss. Second, we
Ô¨Åne-tune the MRF model to imitate the last n ksteps of
the Muse model: Given the output of the Muse model after
thekth iteration, the spatial and label compatibility matrices
are learned such that the MRF inference matches the Ô¨Ånal
predictions of the Muse model after niterations using the
KL divergence loss. Both base and SR MRF models are
trained in the same manner. Both MRF models complete
training in just a few hours on TPUv4 chips.
Experimental Setup: The base model operates on a 1616
token grid with 24 sampling steps to produce 256256im-
ages. The SR model works on a 3232token grid and
produces 512512images in 8 additional steps. Markov-
Gen uses both the base and SR MRF models to trade with
the base and SR sampling steps of the Muse model, respec-
9321
Figure 5. Percentage of prompts for which human raters prefer images by a given model in a side-by-side comparison. We observe that
human raters strongly prefer the images generated by MarkovGen over those of both early exit Muse (left) and even the more expensive
and slower full Muse model (center).
Model FID
Early Exit Muse base (18 iters) 14.37
Full Muse base (24 iters) 13.13
MarkovGen (18 iters) 12.28
Table 2. Quantitative evaluation of FID scores on the MS-
COCO [16] dataset for 256256image resolution. MarkovGen
outperforms both the Early Exit as well as the full Muse model.
tively. We apply the base MRF after step 20 of the base
Muse model, and the SR MRF after 3 steps of the SR Muse
model, cutting down 4 and 5 steps respectively. This results
in a speedup of 1:5for MarkovGen compared to Muse.
In our experiments we compare 3 different models: (A)
full Muse, (B) MarkovGen, and (C) an early exit Muse
model that also stops Muse iterations early to have com-
parable speed to MarkovGen, but without the application of
the MarkovGen MRF model.
Qualitative Evaluation:
In Figure 4 we study the progression of the generated
images during the invocation of the Muse SR model. At
each step, we show the output of early exit Muse (top) and
the improved result after the application of the MarkovGen
MRF (bottom). The results show that while the Muse model
slowly improves result quality, MarkovGen provides high
quality results already after the Ô¨Årst step. We observe that
MarkovGen with just 3 SR steps already consistently pro-
duces images comparable or better than the full Muse re-
sults after the total of 8 SR steps. Figure 6 demonstrates
this using a series of Parti Prompts [34], where we compare
the results of the full Muse model (left), early exit Muse
(middle) and, our MarkovGen model with a 1:5speed-up
(right). We observe that the model is able to produce a wide
variety of images ranging from artistic to natural images.
Quantitative Evaluation: Figure 5 summarizes the results
of our human evaluation study. Using the 1633 promptsfrom the Parti promts dataset, we generated three images
with full Muse, early exit Muse, and our proposed Markov-
Gen model respectively. To allow raters to focus on im-
age quality, we use the same random seed across models to
ensure that image content and degree of alignment to the
prompt are the same across the generated images. Asked to
evaluate which image is of higher quality, we present hu-
man raters with two generated images side-by-side. Raters
are given the option of choosing either image or that they are
indifferent. All image pairs are evaluated by 3 independent
raters that were hired through a high-quality crowd comput-
ing platform. The raters and the authors of this paper were
anonymous to each other. For each pairwise comparison,
we consider an image to be of higher quality if it is selected
by at least 2 raters.
From the results we observe that human raters strongly
prefer the images generated by MarkovGen over those of
both early exit Muse (left) and even the more expensive
and slower full Muse model (center). We also compared
early exit Muse to full Muse (right) and veriÔ¨Åed that human
raters can clearly identify the quality improvement achieved
by the last stages of the Muse model. This result demon-
strates that MarkovGen not only achieves a drastic speed-
up of 1.5x over Muse, but also signiÔ¨Åcantly improves image
quality.
In addition to human evaluation, Table 2 shows single-
shot FID scores on the MS-COCO dataset [16]. We use
the base Muse model for this evaluation. Again, in line
with the human evaluation, we observe that MarkovGen
achieves better results than full Muse, which in turn out-
performs early exit Muse.
6. Conclusion
The proposed MarkovGen model showed a signiÔ¨Åcant
inference speed-up of 1:5and a clear quality gain over
Muse by fast-forwarding the last few steps of Muse model
with MRF inference. The MRF model achieves this by
9322
A squirrel in a fieldA black and white landscape photograph of a black treeA robot cooking in the kitchen
A blue sports car on the roadA wooden deck overlooking a mountain valleyFull MuseEarly Exit MuseText promptMarkovGen
Three wine bottles
A boat
The finale of a fireworks display
A blue sports car on the road
A chairFull MuseEarly Exit MuseText promptMarkovGen
A robot cooking in the kitchen
The finale of a fireworks display
A corgi wearing a red bowtie and a purple party hat
A robot painted as graffiti on a brick wall. a sidewalk is in front of the wall, and grass is growing out of cracks in the concrete.A pickup truck
A cowEarly Exit Muse: Fewer steps1.5x fasterFull Muse: All stepsMarkovGen: Fewer steps + MRF1.5x fasterEarly Exit Muse: Fewer steps1.5x fasterFull Muse: All stepsMarkovGen: Fewer steps + MRF1.5x faster
A chair
Three wine bottles
A set of 2x2 emoji icons with happy, angry, surprised and sobbing faces. The emoji icons look like pandas. All of the pandas are wearing colorful sunglasses.An oil painting of two rabbits in the style of American Gothic, wearing the same clothes as in the original.
Figure 6. Within each set of three, MarkovGen (right) speeds up Muse (left) by 1:5and improves image quality. A similar speed up by
only reducing the step count with early exit Muse (middle) results in a signiÔ¨Åcant loss of quality.
learning the spatial and token label compatibility relation-
ships in the discrete VQGAN token space. Our MRF model
can be trained in just a few hours, allowing us to use it in
conjunction with pre-trained Muse models, to observe al-
most immediate improvements.
While providing clear beneÔ¨Åts over independent per-
patch token selection, our current MRF model does not yet
utilize the provided text prompt, with text guidance coming
solely through the unaries. An interesting direction of future
work would be to make the spatial and token compatibility
weights be dependent on the text prompt, allowing the MRF(or in this case, the CRF) to adapt to text input.
Another direction of future work lies in training the
Muse model itself jointly with the MRF layers, so as to en-
sure that the unaries produced by Muse are optimal for use
with MRF-based decoding.
Acknowledgment
We would like to thank Apurv Suman, Dilip Krishnan,
Jarred Barber, Huiwen Chang, Jason Baldridge, and the
anonymous reviewers for their valuable feedback.
9323
References
[1] Andrew Adams, Jongmin Baek, and Myers Abraham Davis.
Fast high-dimensional Ô¨Åltering using the permutohedral lat-
tice. In Eurographics , 2010. 5
[2] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,
Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy,
William T. Freeman, Michael Rubinstein, Yuanzhen Li, and
Dilip Krishnan. Muse: Text-to-image generation via masked
generative transformers. ICML , 2023. 2, 3, 5, 6
[3] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and
William T. Freeman. Maskgit: Masked generative image
transformer. In CVPR , 2022. 2, 3
[4] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L. Yuille. Semantic image segmen-
tation with deep convolutional nets and fully connected crfs.
InICLR , 2015. 3
[5] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,
Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam
Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov,
Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari,
Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Brad-
bury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia,
Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner,
Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu
Soricut. Pali: A jointly-scaled multilingual language-image
model, 2022. 6
[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 3, 6
[7] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.
Cogview2: Faster and better text-to-image generation via hi-
erarchical transformers. In NeurIPS , 2022. 3
[8] Patrick Esser, Robin Rombach, and Bj ¬®orn Ommer. Taming
transformers for high-resolution image synthesis. In CVPR ,
2021. 2, 3, 5
[9] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,
Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-
based text-to-image generation with human priors. In ECCV ,
2022. 3
[10] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan
Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh.
Long video generation with time-agnostic vqgan and time-
sensitive transformer, 2022. 3
[11] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo
Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-
tor quantized diffusion model for text-to-image synthesis. In
CVPR , 2021. 2
[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In NeurIPS , 2020. 3
[13] D. Koller and N. Friedman. Probabilistic Graphical Models:
Principles and Techniques . MIT Press, 2009. 2, 5
[14] Philipp Krahenbuhl and Vladlen Koltun. EfÔ¨Åcient inference
in fully connected crfs with gaussian edge potentials. In
NeurIPS , 2011. 2, 3, 5[15] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Shechtman, and Jun-Yan Zhu. Multi-concept customization
of text-to-image diffusion, 2023. 3
[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ¬¥ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision‚ÄìECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13 , pages 740‚Äì755. Springer, 2014. 7
[17] Y . Liu M. Ghazvininejad, O. Levy and L. Zettlemoyer.
Mask-predict: Parallel decoding of conditional masked lan-
guage models. In EMNLP , 2019. 3
[18] Midjourney, 2022. https:://www.midjourney.com. 1, 3
[19] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. GLIDE: towards photorealistic image genera-
tion and editing with text-guided diffusion models. In ICML ,
2022. 3
[20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J. Liu. Exploring the limits of transfer learning with a
uniÔ¨Åed text-to-text transformer. J. Mach. Learn. Res. , 2020.
5
[21] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents. preprint , 2022. [arxiv:2204.06125].
1, 3
[22] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In International Confer-
ence on Machine Learning , pages 8821‚Äì8831. PMLR, 2021.
3
[23] Dominic Rampas, Pablo Pernias, Elea Zhong, and
Marc Aubreville. Fast text-conditional discrete denois-
ing on vector-quantized latent spaces. preprint , 2022.
[arXiv:2211.07292]. 3
[24] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¬®orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , 2022. 1, 2, 3
[25] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,
Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J
Fleet, and Mohammad Norouzi. Photorealistic text-to-
image diffusion models with deep language understanding.
preprint , 2022. [arXiv:2205.11487]. 1, 2, 3
[26] Tim Salimans and Jonathan Ho. Progressive distillation for
fast sampling of diffusion models. In ICLR , 2022. 3
[27] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman.
Make-a-video: Text-to-video generation without text-video
data, 2022. 3
[28] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In Proceedings of the 32nd
International Conference on Machine Learning , 2015. 3
9324
[29] Yixuan Su, Deng Cai, Yan Wang, David Vandyke, Simon
Baker, Piji Li, and Nigel Collier. Non-autoregressive text
generation with pre-trained language models. In EACL ,
2021. 3
[30] Zhiqing Sun, Zhuohan Li, Haoqing Wang, Zi Lin, Di He,
and Zhi-Hong Deng. Fast structured decoding for sequence
models. In NeurIPS , 2019. 3
[31] Richard Szeliski, Ramin Zabih, Daniel Scharstein, Olga
Veksler, Vladimir Kolmogorov, Aseem Agarwala, Marshall
Tappen, and Carsten Rother. A comparative study of en-
ergy minimization methods for markov random Ô¨Åelds with
smoothness-based priors. IEEE Transactions on Pattern
Analysis and Machine Intelligence , 30(6):1068‚Äì1080, 2008.
3
[32] A ¬®aron van den Oord, Oriol Vinyals, and Koray
Kavukcuoglu. Neural discrete representation learning.
preprint , 2017. [arXiv:1711.00937]. 3
[33] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-
dermans, Hernan Moraldo, Han Zhang, Mohammad Taghi
Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.
Phenaki: Variable length video generation from open domain
textual description, 2022. 3
[34] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-
jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-
fei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han,
Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and
Yonghui Wu. Scaling autoregressive models for content-rich
text-to-image generation. In ICML , 2022. 1, 2, 3, 7
[35] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models, 2023.
3
[36] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-
Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang
Huang, and Philip H. S. Torr. Conditional random Ô¨Åelds as
recurrent neural networks. In ICCV , 2015. 2, 3, 5
9325
