JDEC: JPEG Decoding via Enhanced Continuous Cosine Coefficients
Woo Kyoung Han1,2Sunghoon Im2Jaedeok Kim3* Kyong Hwan Jin1*
1Korea University2DGIST3NVIDIA
{wookyoung0727, kyong jin}@korea.ac.kr, sunghoonim@dgist.ac.kr, jaedeokk@nvidia.com
Abstract
We propose a practical approach to JPEG image de-
coding, utilizing a local implicit neural representation with
continuous cosine formulation. The JPEG algorithm sig-
nificantly quantizes discrete cosine transform (DCT) spec-
tra to achieve a high compression rate, inevitably result-
ing in quality degradation while encoding an image. We
have designed a continuous cosine spectrum estimator to
address the quality degradation issue that restores the dis-
torted spectrum. By leveraging local DCT formulations,
our network has the privilege to exploit dequantization and
upsampling simultaneously. Our proposed model enables
decoding compressed images directly across different qual-
ity factors using a single pre-trained model without rely-
ing on a conventional JPEG decoder. As a result, our
proposed network achieves state-of-the-art performance in
flexible color image JPEG artifact removal tasks. Our
source code is available at https://github.com/
WooKyoungHan/JDEC .
1. Introduction
Within the dynamic evolution of high-efficiency image
compression, it is notable that JPEG [33] maintains a piv-
otal position. JPEG, renowned for its compatibility and
standardization, is the most famous image coder-decoder
(CODEC) among conventional lossy compression methods.
Therefore, a high-quality JPEG decoder applies to all exist-
ing compressed JPEG files. JPEG reduces file size through
downsampling color components and quantizing the dis-
crete cosine transform (DCT) spectra, which leads to a com-
plicated loss of image information and distortion. Conse-
quently, the design of a high-quality JPEG decoder presents
a dual challenge: 1) the restoration of complex losses from
the JPEG encoder and 2) the modeling of a network that
employs a spectrum as an input and its image as an output.
Many deep neural networks (DNNs) have been pro-
posed as promising solutions for the JPEG artifact removal
*Corresponding author.
Figure 1. Overall concept of proposed JPEG decoding Instead
of using a conventional JPEG decoder to refine the high-quality
(HQ) image from the low-quality (LQ) image, our JDEC directly
decodes the LQ spectrum by learning a continuous spectrum.
[11, 14, 18, 24, 35–37]. Most existing methods, such as
[11, 24], are dedicated to specific quality factors, provid-
ing multiple models to cover JPEG compression. In recent
studies [14, 18], the quality-dedicated problem has been ad-
dressed through the utilization of quantization maps [14]
or the estimation of quality factors [18]. The existing arti-
fact removal networks commonly take the decoded image
as input, even though the encoded spectrum contains more
information than the decoded image, according to the data
processing inequality [10]. The property is explained in the
supplement material.
Due to the characteristics of the JPEG algorithm, it is
non-trivial to design a neural network that takes spectra
as inputs [15, 34]. Park et al . [28] proposed a method
of processing spectra to transformers. In the context of
spectral processing, our approach extends beyond proposed
classification networks [15, 28, 34] by leveraging the ca-
pabilities of the embedding strategy, paving the way for
more effective decoding. The spectrum conversion aligns
with recent advancements in implicit neural representa-
tion (INR), where methods adopting sinusoidal functions
[5, 16, 20, 21, 27, 32] have demonstrated significant ad-
vancement across various tasks.
In this paper, we propose an advanced model, the
JPEG Decoder with Enhanced Continuous cosine coef-
ficients (JDEC), for retrieving high-quality images from
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
2784
JPEG (q= 100)
 FBCNN [18]
 JDEC ( ours)
45.31 (dB) / 0.146 43.87 (dB) / 0.175 46.80 (dB) / 0.139
Figure 2. Visual Demonstration at q= 100 (PSNR (dB) ↑/
Bit-Error-Rate (BER) ↓) of decoding compressed image: JPEG
(quality factor = 100), image enhancement approach [18] pre-
dicted from JPEG image ( q= 100 ), and JDEC ( ours) predicted
directly from a JPEG bit-stream. We highlight the occurrence of
bit errors overlaid with green dots.
compressed spectra. As an artifact removal network, our
JDEC does not require a conventional JPEG decoder com-
pared to existing methods shown in Fig. 1. JDEC captures
the dominant frequency and its amplitude, thereby repre-
senting the high-quality spectrum through continuous co-
sine formulation (CCF). The CCF module estimates a con-
tinuous form of a given discrete cosine spectrum. The pro-
posed model represents a considerable improvement in de-
coding JPEG bitstream. As shown in Fig. 2, our JDEC
decodes high-quality images with fewer bit errors than the
original JPEG decoder.
In summary, our main contributions are as follows:
• We propose a local implicit neural representation that de-
codes JPEG files across various quality factors (QF) with
continuous cosine spectra.
• We show that the suggested continuous cosine formula-
tion module lets the network predict spectra highly corre-
lated with the ground truth’s spectrum.
• We demonstrate that our proposed method operates as a
practical decoder, delivering superior image quality, in-
cluding the generally used quality factor.
2. Related Work
JPEG Background According to Shannon’s source coding
theorem [30], a loss of image information is unavoidable
to achieve high-efficiency compression. The JPEG initiates
the encoding process by decomposing an input RGB image
to luminance and chroma components [33]. The chroma
components are downsampled using the nearest neighbor
method by a factor of ×2. The JPEG subtracts the mid-
point of the pixel value (=128) to images and divides it
into8×8crops. Then, each crop is transformed into 2D-
DCT [2] spectra. Following this, the encoder quantizes the
spectrum of each block using a predefined quantization ma-
trix depending on a quality factor q, and then the quantized
spectrum is coded using Huffman coding. We illustrate the
process of the JPEG encoder in Fig. 3.
Due to the nature of DCT, the energy of spectra is
concentrated in low-frequency components. Since the
quantization matrix treats high-frequency components more
severely than low-frequency components, most distortions
Figure 3. Overall process of the JPEG encoder. Luminance
and chroma components are separated from an RGB image. Both
components are converted to DCT spectra and quantized with a
pre-defined quantization matrix (Q-map). All losses occur in the
orange area.
occur in the high-frequency components. In the JPEG de-
coder, the quantization matrix is directly applied to the
quantized spectra, transforming them into images. Con-
sequently, all incurred losses, especially those in high-
frequency components, are directly conveyed to the result-
ing image.
JPEG Artifact Removal To address the aforementioned
problem, learning-based methods have enhanced the qual-
ity of a decoded image. Dong et al. [11] introduced a neu-
ral network that utilizes a super-resolution network [12] for
JPEG artifact removal. Most of the proposed neural net-
works are dedicated to a specific quality factor [7, 8, 11, 23].
To tackle the quality-dedicated issue, Jiang et al. [18] pro-
posed a method to estimate a quality factor, solving flexible
JPEG artifact removal and handling a double JPEG artifact.
However, the existing artifact removal methods take images
as input, incorporating the conventional JPEG decoder be-
fore using their network. Recently, Bahat et al. [4] proposed
a novel method for JPEG decoding, which takes spectra as
input. However, the proposed method does not consider
color components with trainable decoding and does not re-
cover high-quality factors.
Learning in the Frequency domain In an image classifica-
tion task, skipping a conventional JPEG decoding [15, 34]
has been proposed, especially optimizing CNNs. Embed-
ding techniques [15] tackle the size mismatch issue between
luma and chroma components, such as upsampling chroma
components before forwarding to a network and upsam-
pling chroma features after forwarding to a shallow net-
work. The proposed methods boost computation time with-
out dropping the original performance. Recently, the ap-
proach adopting vision transformers [13] instead of CNNs
has promising performance [28]. We adopt the proposed
embedding method from [28] and modified [25] SwinV2
transformer suitable for image decoding.
3. Method
Problem Formulation LetIGT∈RH×W×3be a ground-
truth RGB image. The JPEG encoder separates IGTto lu-
minance component ( IY∈RH×W×1) and chroma compo-
nents ( IC∈RH×W×2) and downsamples chroma compo-
nents by a factor of 2, i.e. ( I↓
C∈RH
2×W
2×2). The super-
2785
Figure 4. Decoding a JPEG bitstream with the proposed JDEC. JDEC consists of an encoder ( Eφ) with group spectra embedding ( gϕ),
a decoder ( fθ), and continuous cosine formulation ( Tψ). Inputs of JDEC are as follows: compressed spectra ( ˜XY,˜XC), quantization map
Q. Note that our JDEC does not take ˜Ias an input. JDEC formulates latent features into a trainable continuous cosine coefficient as a
function of block grid δand forward to INR ( fθ). Therefore, each B×Bblock shares the estimated continuous cosine spectrum.
script↓indicates a ×2downsampling. Then, each compo-
nent is divided into 8×8blocks ( I∈R8×8⊂IY,I↓
C).
2D-DCT [2] into spectra X∈R8×8is defined as below:
DCT (I) :=X=DID⊤. (1)
The orthonormal basis matrix D(=D8)is defined as:
DN:= [α]⊙cos([π[Fk|N]N−1
k=0⊗[k]N−1⊤
k=0]) (2)
=r
2
N
1√
21√
2···1√
2
cos(1π
2N) cos(3π
2N)··· cos((2N−1)π
2N)
............
cos((N−1)π
2N)cos(3(N−1)π
2N)···cos((2N−1)(N−1)π
2N )
.
where Fk|N:= (2 k+1)/2Nis a fixed frequency of a coor-
dinate kwith a given size Nand[α]is the scaling matrix for
orthonormality. The operations ⊗and⊙are a Kronecker
product and element-wise multiplication, respectively.
Quantization is conducted with a predefined quantization
matrix Q= [QY;QC]∈N8×8×2s.t.
˜C:=
X⊙1
Q
,˜X=˜C⊙Q, (3)
where ⌊·⌉is a round operation that maps to the near-
est integer. 1/Qdenotes an element-wise division. The
JPEG encoder compresses the header Qand the body ofa code ˜Cseparately. In the decoder part, the JPEG re-
stores the image from the compressed header and body by
˜I=DCT−1(˜C⊙Q).
To summarize this, the corrupted JPEG image ˜Iis ob-
tained by
˜IY
˜IC
="
DCT−1(⌊DCT (IY)⊙1
QY⌉ ⊙QY)
DCT−1(⌊DCT (I↓
C)⊙1
QC⌉ ⊙QC)↑#
,(4)
where the superscript ↑indicates ×2upsampling in the spa-
tial domain. We here observe from Eq. (4) that most of
the loss of image information is induced by the quanti-
zation step. The shape of the chroma component ˜XC∈
RH
2×W
2×2is different from the luminance component
˜XY∈RH×W×1. We will consider observations in the de-
sign of our proposed network.
We propose a JPEG decoder network, JDEC JΘ, defined
by
JΘ: (˜XY,˜XC;Q)7→bI. (5)
The network directly accepts quantized spectrum ˜XYand
˜XCwith a quantization matrix as the network inputs, en-
abling to decode JPEG directly from encoded JPEG data.
Our proposed JDEC comprises mainly three parts: encoder
Eφwith group embedding, continuous cosine formulation
Tψ, and decoder fθwith an implicit neural representation.
2786
Figure 5. Graphical summary of fθ(Tψ(δ,z;Q)).Each 1×1-
sized feature zmaps into a B×Bpixel area. Tψembeds the local
coordinates of B×Barea and forwards to fθ.
Encoder ( Eφ)The encoder is a function
Eφ: (˜XY,˜XC)7→z∈RH
B×W
B×C. We model the
encoder Eφby using SwinV2 [25]. To allow the different
shape of spectrum ˜XYand˜XC, we apply the group spectra
embedding layer gϕproposed by [28]. The embedding
layer ( gϕ) transforms luminance and chroma spectra
through two steps. We convert 8×8spectra into B×Bfor
luma and B/2×B/2for chroma via sub-block conversion
[17] in the (a) part of gϕin Fig. 4. We implement the block
sizeB= 41.
X′
Y=D∗
B(D⊤XYD)D∗⊤
B, (6)
X′
C=D∗
B/2(D⊤XCD)D∗⊤
B/2, (7)
where D∗
Nindicates a block diagonal matrix with size 8×8.
In part (b) of Fig. 4, spectra are reshaped and concatenated
toRH
B×W
B×3B2
2which is the sum of converted size. Then
initialized latent vector z′∈RH
B×W
B×Care conducted in
(b) part of gϕ. Following the prior work, [22], we adopt the
deep feature extractor, replacing the Swin attention module
with the SwinV2 attention module.
Continuous Cosine Formulation ( Tψ)Each JPEG block
shares a distorted DCT spectrum. Modifying the entire
spectrum is required to restore the distortion of a block.
To address the spectrum distortion issue derived from the
JPEG encoder, we introduce Continuous Cosine Formula-
tion (CCF) module, which enhances the cosine spectrum.
The CCF constructs a continuous spectrum corresponding
toB×Bembedded block by estimating dominant frequen-
cies and amplitudes of a cosine transform. Illustrated in Fig-
ure 5, each block has identical amplitudes and frequencies
within the embedded block coordinate δ:= [(i, j)]B−1
i,j=0.
Our CCF takes a latent vector zfrom encoder Eφand a
quantization matrix Q. The CCF Tψconsists of three el-
ements: frequency estimator hf:RC7→R2K, coefficient
estimator hc:RC7→RK, and quantization matrix encoder
hq:R1287→RK. Each frequency and coefficient estima-
tor comprises sequential convolution and non-linear acti-
vation layers. As a method for quantization recovery, we
implement an amplitude recovery method as described be-
low, drawing inspiration from the existing dequantization
1Bshould be divisor or multiple of 16network [16],
bX=C⊙Q′∼Eq.(3), (8)
where Q′=hq(Q)andC=hc(z).
We hypothesize that estimating the frequency compo-
nents effectively mitigates aliasing (i.e., quantization and
downsampling) derived from JPEG. It has been demon-
strated that trainable frequencies and phasors effectively
mitigate upsampling and dequantization [16, 21].
We thus formulate the CCF module approximates B×B
spectral features from the fiber of z:
Tψ(z, δh,w;Q) =bX⊗(cos(πFh⊗δh)⊙cos(πFw⊗δw)),
(9)
where [Fh;Fw] =hf(z).δh,wdenotes vertical and hor-
izontal coordinates of δ. Note that bX,Fh,Fw∈RKare
amplitude and frequencies for the spatial coordinate δ, re-
spectively. i.e. the CCF maps embedded features and block
coordinates by Tψ: (R1×1×C,RB×B×2)7→RB×B×K.
Decoder ( fθ)Our decoder fθ:RK7→R3is a local implicit
neural representation function of {z,Q}, δ. i.e. :
bI=fθ(bX⊗(cos(πFh⊗δh)⊙cos(πFw⊗δw))).(10)
Therefore, in the B×Bblock of Eq. (10), the estimated
basis of ˆXand its reconstruction follows:
I=D⊤XD≃f′
θ(ΛhbX′Λw), (11)
where Λh,w= cos([ πFh,w⊗δh,w])andf′
θsatisfy fθ=
f′
θ◦Wfor a trainable fully-connected layer W. With a
linear layer W, the Eq. (10) complete the quadratic form
Λ1bX′Λ2=W(Tψ(z,Q;δ))by including summation of
features. We optimize a set of trainable parameters Θ :=
{φ;ψ;θ}with the equation below:
bΘ = arg min
Θ||IGT−bI(˜XY,˜XC,Q; Θ)||1. (12)
We will demonstrate the estimated frequencies ( Fh,Fw)
and amplitudes bXof networks follows Xin the following
section.
4. Experiments
4.1. Network Details
Encoder ( Eφ) and Decoder ( fθ)The linear layer in group
spectra embedding module gϕhas an embedding size C
of 256. We modified the deep feature extract part of
SwinIR [22]. The window attention module is replaced with
SwinV2 [25], with a window size of 7. [22] and [28] re-
ported that a window size of 8 significantly drops the per-
formance of the network. Each residual Swin transformer
block includes 6 Swin transformer layers. The decoder fθis
2787
Figure 6. RD curve results on LIVE-1 [31] (top left), ICB [29] (top right), BSDS500 [3] (bottom left). We highlight the high-quality factor
parts q∈[90,100] in the bottom right part. We show PSNR as a measure of distortion (higher is better). We observe that our JDEC
decodes high-quality images better than other methods.
Test LIVE-1 [31] BSDS500 [3] ICB [29]
Method q= 10 q= 20 q= 30 q= 40 q= 10 q= 20 q= 30 q= 40 q= 10 q= 20 q= 30 q= 40
JPEG25.69|24.20 28.06 |26.49 29.37 |27.84 30.28 |28.84 25.84|24.13 28.21 |26.37 29.57 |27.72 30.52 |28.69 29.44|28.53 32.01 |31.11 33.20 |32.35 33.95 |33.14
0.759 0.841 0.875 0.894 0.759 0.844 0.880 0.900 0.753 0.807 0.833 0.844
DMCNN [36]27.18|27.03 29.45 |29.08 - - 27.16|26.95 29.35 |28.84 - - 30.85|- 32.77 |- - -
0.810 0.874 - - 0.799 0.866 - - 0.796 0.830 - -
IDCN [37]27.62|27.32 30.01 |29.49 - - 27.61|27.22 28.01 |25.57 - - 31.71|- 33.99 |- - -
0.816 0.881 - - 0.805 0.873 - - 0.809 0.838 - -
Swin2SR* [9]27.98|- - - 32.53 |- - - - - 32.46|- - - 36.25 |-
- - - - - - - - - - - -
DnCNN [35]26.68|26.47 29.12 |28.77 30.43 |30.04 31.34 |30.94 26.82|26.53 29.26 |28.74 30.63 |30.02 31.59 |30.92 29.78|29.71 31.99 |31.90 32.98 |32.89 33.52 |33.42
0.794 0.866 0.895 0.911 0.793 0.867 0.898 0.915 0.726 0.765 0.786 0.978
QGAC [14]27.65|27.43 29.88 |29.56 31.17 |30.77 32.08 |31.64 27.75|27.48 30.04 |29.55 31.36 |30.73 32.29 |31.53 32.12|32.09 34.22 |34.18 35.18 |35.13 35.71 |35.65
0.819 0.882 0.908 0.922 0.819 0.884 0.911 0.926 0.814 0.844 0.859 0.865
FBCNN [18]27.77|27.51 30.11 |29.70 31.43 |30.92 32.34 |31.80 27.85|27.53 30.14 |29.58 31.45 |30.74 32.36 |31.54 32.18|32.15 34.38 |34.34 35.41 |35.35 36.02 |35.95
0.816 0.881 0.908 0.923 0.814 0.881 0.909 0.924 0.813 0.844 0.859 0.869
JDEC (ours)27.95|27.71 30.26 |29.87 31.59 |31.12 32.50 |31.98 28.00|27.67 30.31 |29.71 31.65 |30.88 32.53 |31.68 32.55|32.51 34.73 |34.68 35.75 |35.68 36.37 |36.28
0.821 0.885 0.911 0.925 0.819 0.885 0.912 0.927 0.818 0.847 0.862 0.871
Table 1. Quantitative comparisons (PSNR (dB) |PSNR-B (dB) (top), SSIM (bottom)) with the color JPEG artifact removal networks. Red
and blue colors indicate the best and the second-best performance, respectively. (-) indicates not reported. (*) indicates using additional
datasets. Note that only JPEG [33] and our JDEC get spectra as input.
an MLP composed of 5 linear layers with 512 hidden chan-
nelsKand ReLU activations.
CCF The CCF includes a frequency estimator hf, an am-
plitude estimator hc, and a quantization matrix encoder hq.
[16, 20, 21] show that learning frequency, phase, and ampli-
tude components enhance the performance of the INR. The
quantization matrix encoder hqis a single fully connected
layer, having 512( =K) channels. The amplitude and fre-
quency estimator ( hc, hf) is designed with two 3×3convo-
lutional layers with a ReLU activation. The frequency esti-
mator has 2K(= 1024) output channels for handwaxis,while the amplitude estimator has K(= 512) channels.
4.2. Training
Dataset Following the previous work [14, 18], we use
DIV2K and Flickr2K [1]. Each dataset contains 800 and
2650 images, respectively. For generating synthetic JPEG
compression, we use the OpenCV standard [6]. We com-
press images using randomly sampled quality factors with
steps of 10 in the range [10,100]. We directly extract quan-
tization maps Qand coefficients of spectra ˜Cfrom JPEG
files and construct spectra ˜X, following the Eq. (3). Since
2788
BSDS500 [3]
 LIVE1 [31]
DnCNN [35]
 QGAC [14]
 FBCNN [18]
 JDEC ( ours)
 GT
Figure 7. Qualitative comparison in color JPEG artifact removal ( q= 10 ).
RGB Image
JPEG
 DnCNN [35]
 QGAC [14]
FBCNN [18]
 JDEC ( ours)
GT (IC)
RGB Image
JPEG
 DnCNN [35]
 QGAC [14]
FBCNN [18]
 JDEC ( ours)
GT (IC)
Figure 8. Qualitative comparison in chroma components Icof
images ( q= 10 ).
the dynamic range of spectra depends on frequency, we
should normalize spectra in a range of [−1,1]. The quan-
tization maps are normalized with the same normalization
function. The ground truth (GT) images are prepared with
a range of [−0.5,0.5]because the JPEG encoder subtracts
the midpoint of the image range (=128).
Implementation Detail We use 112×112patches as in-
puts to our network. This size is chosen because it is the
least common multiple of the minimum unit size of color
JPEG ( 16×16) and the window size of our Swin architec-
ture [25] ( 7×7). The network is trained for 1000 epochs
with batch size 16. We optimize our network by Adam [19].
The learning rate is initialized as 1e-4 and decayed by factor
0.5 at [200,400,600,800].
4.3. Evaluation
Quantitative Result For evaluation, we use LIVE-1 [31],
testset of BSDS500 [3] and ICB [29] dataset. In the as-
pect of the JPEG decoder, we present the rate-distortionLuma (IY)|Chroma (IC)
Method q= 10 q= 20 q= 30 q= 40
JPEG 34.39 |35.77 37.32 |38.90 39.85 |40.72 39.82 |39.59
DnCNN [35] 35.30 |35.85 37.60 |38.01 38.78 |38.96 39.45 |39.51
QGAC [14] 37.28 |38.18 39.75 |39.94 41.00 |40.69 41.73 |41.09
FBCNN [18] 37.12 |38.36 39.71 |40.21 40.97 |41.04 41.81 |41.50
JDEC (ours)37.32 |38.90 39.85 |40.72 41.11 |41.52 41.92 |41.96
Table 2. Quantitative comparisons of each components in ICB [29]
datasets. (PSNR(dB))
curve to illustrate the trade-off between bits-per-pixel (bpp)
and peak signal-to-noise ratio (PSNR) where quality fac-
tors in a range of [10,100]. We observed that BSDS500 [3]
is saved as JPEG with a quality factor of 95. Therefore,
the reported BSDS500 data is within a quality factor of 90.
We compare our JDEC against existing compression artifact
removal models: DnCNN [35], QGAC [14], and FBCNN
[18] in Fig. 6. The selected models cover a relatively wide
range of quality factors with a single network. We evalu-
ate DnCNN [35] following the suggested method in QGAC
[14], with channels being processed independently. Despite
QGAC [14] having a training range of [10,100], it experi-
ences a drop in performance in the range of (90,100] across
all datasets. FBCNN [18] also exhibits a performance drop
in the range of [95,100] when evaluated on the LIVE-1 [31]
dataset. In comparison, JDEC outperforms all other meth-
ods, regardless of the quality factor or dataset.
Regarding JPEG artifact removal, we report PSNR,
structural similarity index (SSIM), and PSNR-B for esti-
mating de-blocking in Tab. 1. We include DMCNN [36],
IDCN [37], and transformer-based Swin2SR [9] as addi-
tional comparative groups since they cover a range of qual-
ity factors. Note that the Swin2SR has trained on a limited
range of quality factors in a range of [10,40]with additional
datasets, including the train and test dataset of BSDS500
[3] and Waterloo [26]. We partitioned the data presented
in Tab. 1 to distinguish between networks operating within
limited and expansive ranges. Our JDEC shows remarkable
performance compared to other methods. The maximum
PSNR interval is 0.37dB on ICB for q= 10 .
We demonstrate Tab. 2 to observe the restoration effects
2789
Test LIVE-1 [31]
Method q= 80 q= 90 q= 95∗q= 100
JPEG34.23|33.45 36.86|36.45 39.33|38.90 43.07|42.37
0.948 0.967 0.979 0.993
DNCNN [35]35.01|34.69 37.29|36.97 39.20|38.79 41.15|40.59
0.954 0.970 0.980 0.987
QGAC [14]35.75|35.19 37.75|37.20 37.50|37.01 38.97|38.56
0.960 0.973 0.974 0.979
FBCNN [18]36.02|35.41 38.25|37.68 40.23|39.65 42.23|41.52
0.961 0.974 0.983 0.990
JDEC (ours)36.31|35.73 38.72|38.17 40.41|39.90 45.14|44.20
0.963 0.976 0.983 0.995
Test ICB [29]
Method q= 80 q= 90 q= 95∗q= 100
JPEG36.34|35.82 37.72|37.40 39.17|39.01 41.31|41.28
0.891 0.912 0.934 0.955
DNCNN [35]35.57|35.44 36.75|36.64 37.99|37.92 39.73|39.69
0.844 0.868 0.891 0.915
QGAC [14]37.58|37.47 38.34|38.21 36.84|36.68 37.55|37.48
0.902 0.919 0.912 0.926
FBCNN [18]38.03|37.91 39.17|39.03 40.36|40.22 41.61|41.52
0.902 0.920 0.938 0.951
JDEC (ours)38.43|38.29 39.58|39.41 40.77|40.63 43.61|43.52
0.906 0.924 0.943 0.968
Table 3. Quantitative comparisons of high-quality images in
LIVE-1 [31] and ICB [29] datasets (PSNR |PSNR-B(dB)) (top),
SSIM (bottom). *: Quality factor 95 is a generally used default
quality factor in the JPEG encoder.
IDMethod Quality Factor q
gϕ-(a) Tψ 10 20 30 40
0∗27.95 |27.71 30.26 |29.87 31.59 |31.12 32.50 |31.98
1 ✗ 27.76 |27.51 30.04 |29.62 31.35 |30.84 32.25 |31.68
2 ✗ 27.69 |27.43 29.95 |29.53 31.25 |30.71 32.14 |31.54
3 ✗ ✗ 26.90 |26.61 28.37 |28.06 28.73 |28.36 28.96 |28.57
4 Eq. (13) 27.88 |27.64 30.21 |29.83 31.54 |31.07 32.45 |31.93
Table 4. Quantitative ablation study of JDEC on LIVE-1 [31]
(PSNR |PSNR-B (dB)). ∗: ID-0 is the proposed method JDEC. The
definition of each ID number is shown in Sec. 4.4.
of two components of different sizes IY∈RH×W×1,IC∈
RH
2×W
2×2. According to Tab. 2, the performance difference
in the chroma component ICis greater than the difference
of the luma component IYindicating an empirical upsam-
pling effect.
In Tab. 3, we show the comparison of the high-quality
image decoding. The∗mark indicates the commonly used
default quality factor of the JPEG, including OpenCV [6].
As a practical decoder for JPEG, only our JDEC decodes
the best images among other baselines, including the con-
ventional JPEG decoder.
Qualitative Result We show color JPEG artifact removal
task in Fig. 7. There are two main distortions of JPEG
compression: 1) lack of High-frequency components and 2)
color differences. We demonstrate the effect of our JDEC
in addressing the distortions in high frequencies in the first
row of Fig. 7. While other methods suffer from aliasing,
our JDEC successfully recovers the details of the butterfly’s
antennae. In the second row of Fig. 7, our JDEC relieves
color distortion derived from JPEG compression.
In Fig. 8, we sort out the chroma components from each
image to demonstrate the effect of our JDEC in relieving
color distortions. When observing the chroma components
Figure 9. Quantitative ablation study of JDEC on LIVE-1
[31] (RD-curve), against ablation models. Our proposed JDEC
achieves higher PSNR than any other models in most of the qval-
ues.
using other methods, it is noticeable that the chroma compo-
nents remain significantly distorted. However, our JDEC re-
stores them closer to the original. It demonstrates that JDEC
robustly restores color components subjected to quantiza-
tion and downsampling, effectively mitigating distortion.
4.4. Ablation Study
Network Components We conducted ablation studies for
the main components of our proposed JDEC. The proposed
method, CCF Tψcontains a frequency estimator which
makes JDEC learn enhanced spectra. To support this, we
train JDEC without a frequency estimator, directly forward-
ing concatenated coordinates (ID-1). We use additional
3×3 convolutional layers to have a comparable number of
parameters. The sub-block conversion is the main element
of encoder Eφ. The spatial area gains a degree of freedom
by using the sub-block conversion of the DCT matrix. We
conduct the ablation study of sub-block conversion by em-
bedding inputs directly (ID-2 of Tab. 4). The drop in perfor-
mance is severe when both components are missing (ID-3
of Tab. 4). We also observed that ID-3, training without
both the group embedding and CCF, leads to a significant
performance drop as shown in Fig. 9.
Fourier Features Comparing to the existing sinusoidal rep-
resentation, the formulation of [20] will be compatible for
our CCF. The modified Fourier feature is as follows :
C⊙cos(π(F·δ+hq(Q))
sin(π(F·δ+hq(Q))
. (13)
We label the model using Eq. (13) instead of Tψas ID-
4. The rate-distortion curve of all ablation models is illus-
trated in Fig. 9. As shown in Fig. 9, the maximum gain of
our CCF is 0.58dB against ID-4, where the quality factor is
100. Eq. (13) is considered as using additional terms than
ID-0 (JDEC) by trigonometric sum. However, it has led to
performance degradation as shown in Tab. 4.
4.5. Continuous Cosine Spectrum
In this section, we demonstrate that our CCF extracts dom-
inant frequencies and amplitudes from highly compressed
2790
JPEG (Input) GT JDEC (ours) bX(Fh,Fw)in Fig. 5
(a) High-frequency
JPEG (Input) GT JDEC (ours) bX(Fh,Fw)in Fig. 5
(b) Low-frequency
Figure 10. Comparison of the estimated spectra of the Contin-
uous Cosine Formulation (CCF). The quality factor of input is
10. The estimated CCF spectrum follows the spectrum of ground-
truth images despite severe distortion.
JPEG spectra. The ranges of input images (8 ×8) are high-
lighted with red boxes in each image. For visualization, we
observe components of CCF, including estimated frequen-
ciesFh,Fwand amplitudes bX. We scatter frequencies in
2D space and assign a color to each amplitude. We quantize
the frequencies to [0, 50] with steps of 1 and interpolate to
continuous values. In Fig. 10a, most of the high-frequency
components have been removed. The estimated spectrum
with CCF is centered on high-frequency components de-
spite such circumstances. In the case of Fig. 10b, the dom-
inant components of the spectrum are focused on relatively
low-frequency. Even in this case, the extracted spectrum of
CCF is concentrated in the low-frequency elements as in the
ground truth.
5. Discussion
Implicit Neural Representation As discussed in Sec. 2 and
Eq. (4), a JPEG encoder downsamples chroma components.
Therefore, the JPEG decoder should map: RH
2×W
2×27→
RH×W×2for chroma. Our JDEC addresses this issue
through CCF ( Tψ) by embedding δinto1×1-sized features,
making the proposed JDEC a function of δ. Our model is
able to decode high-resolution images when provided with
dense coordinates that were not observed during training.
We show the advanced additional upsampling results in the
supplement material.
Extreme Reconstruction We primarily propose a decoding
network to generate high-quality images due to its practical
applicability. Consequently, we pursued the network with-
out explicitly considering the scenario of high compressionJPEG (bpp : 0.052)
 FBCNN[18]
 JDEC+
22.62 (dB) 23.96 (dB) 26.70 (dB)
Figure 11. Reconstruction of the extremely compressed image
(q= 0)in LIVE-1 [31] dataset.
#Params. Mem. Time FLOPs PSNR|PSNR-B (dB)
Method (M) (GB) (ms) (G) q= 10 q= 40
FBCNN [18] 70.1 0.61 71.95 709.97 32.18|32.15 36.02|35.95
Swin2SR [9] 11.5≤ 2.79 2203.59 3301.5 32.46|-36.25|-
JDEC 38.9 1.76 224.79 1006.72 32.55|32.51 36.37|36.28
JDEC-CNN†26.2 0.81 56.59 476.33 32.31|32.27 36.19|36.09
Table 5. Computational resources & performance comparison for
a560×560pixels in ICB [29]. †:We replace the deep feature
extractor of Fig. 4 with a CNN structure for comparison with the
CNN-based model [18].
(q= 0). However, by incorporating all image quality fac-
tors within the range [0,100] with a step size of 10 during
the learning process, we successfully developed a decoding
method tailored for highly compressed images. We label
the additional network as JDEC+. As shown in Fig. 11, our
JDEC+ recovers the highly compressed images better than
image restoration models.
Computation Time and Memory In Tab. 5, we re-
port computational resources including the number of pa-
rameters, memory consumption, floating-point operations
(FLOPs), and computational time in GPU (NVIDIA RTX
3090 24GB). The input size is 560×560for ours, while
other methods have the size of 512×512.
6. Conclusion
We proposed a local implicit neural representation ap-
proach for decoding compressed color JPEG files. Our
JPEG Decoder with Enhanced Continuous cosine coeffi-
cients (JDEC) contains a novel continuous cosine formu-
lation (CCF) to extract a high-quality spectrum of images.
JDEC takes a distorted spectrum as an input of the network
and decodes it to a high-quality image regardless of the
given quality factor. The suggested CCF extracts the dom-
inant components of the ground truth spectrum, effectively.
The results of benchmark datasets demonstrate that our net-
work outperforms existing models as a practical JPEG de-
coder.
Acknowledgement This work was partly supported by Smart
HealthCare Program (www.kipot.or.kr) funded by the Korean Na-
tional Police Agency (KNPA) (No. 230222M01) and Institute
of Information & communications Technology Planning & Eval-
uation (IITP) grant funded by the Korea government (MSIT)
(No.2021-0-02068, Artificial Intelligence Innovation Hub).
2791
References
[1] Eirikur Agustsson and Radu Timofte. NTIRE 2017 Chal-
lenge on Single Image Super-Resolution: Dataset and Study.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) Workshops , 2017. 5
[2] Nasir Ahmed, T. Natarajan, and Kamisetty R Rao. Discrete
cosine transform. IEEE transactions on Computers , 100(1):
90–93, 1974. 2, 3
[3] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji-
tendra Malik. Contour detection and hierarchical image seg-
mentation. IEEE transactions on pattern analysis and ma-
chine intelligence , 33(5):898–916, 2010. 5, 6
[4] Yuval Bahat and Tomer Michaeli. What’s in the image? ex-
plorable decoding of compressed images. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 2908–2917, 2021. 2
[5] Nuri Benbarka, Timon H ¨ofer, Hamd ul-Moqeet Riaz, and
Andreas Zell. Seeing Implicit Neural Representations As
Fourier Series. In Proceedings of the IEEE/CVF Winter Con-
ference on Applications of Computer Vision (WACV) , pages
2041–2050, 2022. 1
[6] G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of
Software Tools , 2000. 5, 7
[7] Lukas Cavigelli, Pascal Hager, and Luca Benini. Cas-cnn:
A deep convolutional neural network for image compression
artifact suppression. In 2017 International Joint Conference
on Neural Networks (IJCNN) , pages 752–759. IEEE, 2017.
2
[8] Yunjin Chen and Thomas Pock. Trainable nonlinear reaction
diffusion: A flexible framework for fast and effective image
restoration. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence , 39(6):1256–1272, 2017. 2
[9] Marcos V Conde, Ui-Jin Choi, Maxime Burchi, and Radu
Timofte. Swin2sr: Swinv2 transformer for compressed im-
age super-resolution and restoration. In European Confer-
ence on Computer Vision , pages 669–687. Springer, 2022. 5,
6, 8
[10] Thomas M Cover. Elements of information theory . John
Wiley & Sons, 1999. 1
[11] Chao Dong, Yubin Deng, Chen Change Loy, and Xiaoou
Tang. Compression artifacts reduction by a deep convolu-
tional network. In Proceedings of the IEEE international
conference on computer vision , pages 576–584, 2015. 1, 2
[12] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Image super-resolution using deep convolutional net-
works. IEEE transactions on pattern analysis and machine
intelligence , 38(2):295–307, 2015. 2
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In International Con-
ference on Learning Representations , 2020. 2
[14] Max Ehrlich, Larry Davis, Ser-Nam Lim, and Abhinav Shri-
vastava. Quantization guided jpeg artifact correction. In
Computer Vision–ECCV 2020: 16th European Conference,Glasgow, UK, August 23–28, 2020, Proceedings, Part VIII
16, pages 293–309. Springer, 2020. 1, 5, 6, 7
[15] Lionel Gueguen, Alex Sergeev, Ben Kadlec, Rosanne Liu,
and Jason Yosinski. Faster neural networks straight from
jpeg. Advances in Neural Information Processing Systems ,
31, 2018. 1, 2
[16] W. Han, B. Lee, S. Park, and K. Jin. ABCD : Arbitrary
bitwise coefficient for de-quantization. In 2023 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 5876–5885, 2023. 1, 4, 5
[17] Jianmin Jiang and Guocan Feng. The spatial relationship of
dct coefficients between a block and its sub-blocks. IEEE
Transactions on Signal Processing , 50(5):1160–1169, 2002.
4
[18] Jiaxi Jiang, Kai Zhang, and Radu Timofte. Towards flex-
ible blind jpeg artifacts removal. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 4997–5006, 2021. 1, 2, 5, 6, 7, 8
[19] Diederik P. Kingma and Jimmy Ba. Adam: A Method for
Stochastic Optimization. In 3rd International Conference on
Learning Representations, ICLR 2015, San Diego, CA, USA,
May 7-9, 2015, Conference Track Proceedings , 2015. 6
[20] Jaewon Lee and Kyong Hwan Jin. Local texture estima-
tor for implicit representation function. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 1929–1938, 2022. 1, 5, 7
[21] Jaewon Lee, Kwang Pyo Choi, and Kyong Hwan Jin. Learn-
ing local implicit fourier representation for image warping.
InEuropean Conference on Computer Vision (ECCV) , pages
182–200. Springer, 2022. 1, 4, 5
[22] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc
Van Gool, and Radu Timofte. SwinIR: Image Restoration
Using Swin Transformer. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) Work-
shops , pages 1833–1844, 2021. 4
[23] Pengju Liu, Hongzhi Zhang, Kai Zhang, Liang Lin, and
Wangmeng Zuo. Multi-level wavelet-cnn for image restora-
tion. In Proceedings of the IEEE conference on computer
vision and pattern recognition workshops , pages 773–782,
2018. 2
[24] Pengju Liu, Hongzhi Zhang, Kai Zhang, Liang Lin, and
Wangmeng Zuo. Multi-level wavelet-cnn for image restora-
tion. In Proceedings of the IEEE conference on computer
vision and pattern recognition workshops , pages 773–782,
2018. 1
[25] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,
Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu
Wei, and Baining Guo. Swin transformer v2: Scaling up
capacity and resolution. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 12009–12019, 2022. 2, 4, 6
[26] Kede Ma, Zhengfang Duanmu, Qingbo Wu, Zhou Wang,
Hongwei Yong, Hongliang Li, and Lei Zhang. Waterloo Ex-
ploration Database: New challenges for image quality as-
sessment models. IEEE Transactions on Image Processing ,
26(2):1004–1016, 2017. 6
[27] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:
2792
Representing Scenes as Neural Radiance Fields for View
Synthesis. In Proceedings of the European Conference on
Computer Vision (ECCV) , 2020. 1
[28] Jeongsoo Park and Justin Johnson. Rgb no more: Minimally-
decoded jpeg vision transformers. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 22334–22346, 2023. 1, 2, 4
[29] Rawzor. Image compression benchmark. . url:
http://imagecompression.info/. 5, 6, 7, 8
[30] C. E. Shannon. A mathematical theory of communication.
The Bell System Technical Journal , 27(3):379–423, 1948. 2
[31] Hamid R Sheikh, Muhammad F Sabir, and Alan C Bovik.
A statistical evaluation of recent full reference image quality
assessment algorithms. IEEE Transactions on image pro-
cessing , 15(11):3440–3451, 2006. 5, 6, 7, 8
[32] Vincent Sitzmann, Julien Martel, Alexander Bergman, David
Lindell, and Gordon Wetzstein. Implicit Neural Represen-
tations with Periodic Activation Functions. In Advances in
Neural Information Processing Systems , pages 7462–7473.
Curran Associates, Inc., 2020. 1
[33] G.K. Wallace. The jpeg still picture compression standard.
IEEE Transactions on Consumer Electronics , 38(1):xviii–
xxxiv, 1992. 1, 2, 5
[34] Kai Xu, Minghai Qin, Fei Sun, Yuhao Wang, Yen-Kuang
Chen, and Fengbo Ren. Learning in the frequency domain.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 1740–1749, 2020. 1, 2
[35] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and
Lei Zhang. Beyond a gaussian denoiser: Residual learning of
deep cnn for image denoising. IEEE transactions on image
processing , 26(7):3142–3155, 2017. 1, 5, 6, 7
[36] Xiaoshuai Zhang, Wenhan Yang, Yueyu Hu, and Jiaying Liu.
Dmcnn: Dual-domain multi-scale convolutional neural net-
work for compression artifacts removal. In 2018 25th IEEE
international conference on image processing (icip) , pages
390–394. IEEE, 2018. 5, 6
[37] Bolun Zheng, Yaowu Chen, Xiang Tian, Fan Zhou, and
Xuesong Liu. Implicit dual-domain convolutional network
for robust color image compression artifact reduction. IEEE
Transactions on Circuits and Systems for Video Technology ,
30(11):3982–3994, 2019. 1, 5, 6
2793
