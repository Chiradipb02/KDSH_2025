BEHA VIOR Vision Suite: Customizable Dataset Generation via Simulation
Yunhao Ge1,2∗, Yihe Tang1∗, Jiashu Xu3∗, Cem Gokmen1∗, Chengshu Li1, Wensi Ai1,
Benjamin Jose Martinez1, Arman Aydin1, Mona Anvari1, Ayush K Chakravarthy1, Hong-Xing Yu1,
Josiah Wong1, Sanjana Srivastava1, Sharon Lee1, Shengxin Zha4, Laurent Itti2, Yunzhu Li1,7,
Roberto Mart ´ın-Mart ´ın6, Miao Liu4, Pengchuan Zhang5, Ruohan Zhang1, Li Fei-Fei1, Jiajun Wu1
1Stanford University2University of Southern California3Harvard University4GenAI, Meta
5FAIR, Meta6The University of Texas at Austin7The University of Illinois Urbana-Champaign
Abstract
The systematic evaluation and understanding of com-
puter vision models under varying conditions require large
amounts of data with comprehensive and customized labels,
which real-world vision datasets rarely satisfy. While cur-
rent synthetic data generators offer a promising alternative,
particularly for embodied AI tasks, they often fall short for
computer vision tasks due to low asset and rendering qual-
ity, limited diversity, and unrealistic physical properties. We
introduce the BEHAVIOR Vision Suite (BVS), a set of tools
and assets to generate fully customized synthetic data for
systematic evaluation of computer vision models, based on
the newly developed embodied AI benchmark, BEHAVIOR-
1K. BVS supports a large number of adjustable parame-
ters at the scene level (e.g., lighting, object placement),
the object level (e.g., joint configuration, attributes such as
“filled” and “folded”), and the camera level (e.g., field of
view, focal length). Researchers can arbitrarily vary these
parameters during data generation to perform controlled
experiments. We showcase three example application sce-
narios: systematically evaluating the robustness of mod-
els across different continuous axes of domain shift, eval-
uating scene understanding models on the same set of im-
ages, and training and evaluating simulation-to-real trans-
fer for a novel vision task: unary and binary state predic-
tion. Project website: https://behavior-vision-
suite.github.io/
1. Introduction
Large-scale datasets and benchmarks have fueled computer
vision research in the past decade [2, 9, 11, 19–21, 23, 32,
∗equal contribution
†correspondence to yunhaoge@cs.stanford.edu ,
{yihetang, zharu }@stanford.edu40, 44, 53, 63]. Driven by these datasets and benchmarks,
thousands of models and algorithms tackling different per-
ception challenges are being proposed every year, on the
topics of object detection [71], segmentation [30], action
recognition [58], video understanding [38] and beyond. De-
spite their success, real-world datasets face inherent limi-
tations. First, the ground-truth object/pixel-level labels are
either prohibitively expensive to acquire (e.g., segmentation
masks) [39] or suffering from inaccuracies (e.g., depth sens-
ing) [46]. Consequently, each real dataset often only offers
limited labels, hindering the development and evaluation of
computer vision models that perform a wide range of per-
ception tasks on the same input. Even when annotations are
affordable and accurate, real-world datasets are limited by
the availability of source images. For example, images of
rare events, such as traffic accidents or low-light conditions,
might be difficult to acquire from the Internet or real-world
sensors. Finally, once collected, these real-world datasets
have a fixed data distribution and cannot be easily changed.
This makes it challenging for researchers to conduct cus-
tomized experiments, often leading to models that overfit
the datasets and eventually rendering the entire benchmarks
obsolete [25, 26, 34, 52].
To avoid this limitation, researchers and practitioners
have devised various methods to generate synthetic datasets
that complement the real ones [17]. In the realm of indoor
scene understanding, 3D reconstruction datasets [4, 49, 62]
provide a promising avenue to generate source images
from arbitrary viewpoints and free (geometric) annotations.
However, due to the imperfect nature of 3D reconstruc-
tion techniques, the rendered images are not very realis-
tic. Since each entire scene is a static mesh, these datasets
offer very limited customizability beyond camera trajecto-
ries. Recent synthetic indoor datasets (often designed by
3D artists) [12, 36, 37, 51] not only offer free geometric and
semantic annotations, but also support object layout recon-
figuration as objects are usually independent CAD models.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
22401
BEHA VIOR Vision SuiteApplications8000+ 3D ObjectsCustomizable Data Generatorw/ Controlled generation
1000 Scene InstancesLightingObject propertiesObject statesSpatial relationshipsCamera pose
Object States / Relations Prediction
Scene Understanding
Controlled Evaluation of Vision Algorithms
Figure 1. Overview of BEHA VIOR Vision Suite (BVS), our proposed toolkit for computer vision research. BVS builds upon the extended
object assets and scene instances from BEHA VIOR-1K [34], and provides a customizable data generator that allows users to generate
photorealistic, physically plausible labeled data in a controlled manner. We demonstrate BVS with three representative applications.
However, these datasets do not guarantee physical plausi-
bility, as object penetration and levitation occur frequently,
and offer no customization capability beyond changing ob-
ject poses. 3D simulators [7, 13, 31, 33, 52, 56], on the
other hand, guarantee physical plausibility with their under-
lying physics engines. They allow users to customize the
joint configuration of articulated objects and even more ad-
vanced object states such as “cooked” or “sliced” [31, 33].
Yet these 3D simulators generally cater to embodied AI and
robotics researchers, and as a result, they lack photorealism
compared to the synthetic datasets mentioned before (often
due to speed constraints), and do not offer ready-made tools
to generate customized image/video datasets for computer
vision researchers.
To overcome the aforementioned challenges, we pro-
pose BEHA VIOR Vision Suite (BVS), a customizable data
generation tool that enables systematic evaluation and un-
derstanding of computer vision models (see Fig. 1 for an
overview). To do so, we expand the 3D asset library in
BEHA VIOR-1K [34], focusing on enhancing both object
diversity and scene variety, as well as adding features to in-
crease the value of the assets for vision tasks. We also intro-
duce a customizable dataset generator, which uses the sim-
ulator from the BEHA VIOR-1K benchmark [34, 55] to gen-
erate custom vision datasets. We build a versatile and cus-
tomizable toolbox to generate high-quality synthetic data
for systematic model evaluation and understanding.
In summary, BEHA VIOR Vision Suite possesses the fol-
lowing unique combination of desirable features:
• BVS offers image/object/pixel-level labels (scene graph,
point cloud, depth, segmentation, etc.);
• BVS covers a wide variety of indoor scenes and objects
(8K+ objects, 1K scene instances, fluid, soft bodies);• BVS provides physical plausibility and photorealism;
• BVS supports customization in terms of object models,
poses, joint configurations, semantic states, lighting, tex-
ture, material, camera setting, etc.;
• BVS includes easy-to-use tooling to generate customized
data for new use cases.
To demonstrate the usefulness of BVS, we show three
example applications: 1) parametrically evaluating model
robustness across different conditions such as lighting and
occlusion, 2) evaluating different types of representative
computer vision models on the same set of images, and 3)
training and evaluating sim2real transfer for object state and
relation prediction. We hope that BVS can unlock more
possibilities for the computer vision community.
2. Related works
In this section, we compare BEHA VIOR Vision Suite with
other real RGB-D datasets, 3D reconstruction datasets, syn-
thetic datasets, and 3D simulators in terms of customizabil-
ity and visual quality (see Tab. 1).
Real Indoor Scene RGB-D Datasets. RGB-D image
datasets of real indoor scenes [1, 5, 46, 54, 66] have driven
advances in 3D perception and holistic scene understand-
ing, with recent additions like ARKitScenes [1] and Scan-
Net++ [66] offering dense semantic and 3D annotations.
Despite having minimum domain gaps with respect to real-
world applications, these real datasets are expensive to an-
notate and inherently static, limiting users’ ability to gener-
ate images from novel camera views, acquire new types of
annotations, or alter scenes. Our work complements these
limitations by offering a fully customizable generator for
photorealistic synthetic data.
22402
Camera Obj. Pose Obj. State Toolkit
Real RGB-D Datasets ✗ ✗ ✗ N/A Real
3D Reconstruction Datasets ✓ ✗ ✗ N/A Medium
Synthetic Datasets ✓ ✓ ✗ ∼ High
3D Simulators ✓ ✓ ✓ ✗ Low
BEHA VIOR Vision Suite (ours) ✓ ✓ ✓ ✓ HighDataset CategoryCustomizability Visual
Quality
Table 1. Comparison of real and different types of synthetic
datasets with BEHA VIOR Vision Suite. ‘Camera’ denotes the
ability to render images from any viewing angle. ‘Obj. Pose’ refers
to the modifiability of the object layout. ‘Obj. State’ indicates
whether an object’s physical states (e.g., open/close, folded) and
semantic states (e.g., cooked, soaked) can be modified. ‘Toolkit’
indicates the availability of utility functions for sampling object
layout and camera poses under specified conditions (e.g., view-
ing half-open kitchen cabinets filled with grocery items). ‘Visual
Quality’ evaluates the photorealism of rendered images.
3D Reconstruction Datasets. 3D reconstruction datasets
such as Gibson, Matterport, and HM3DSem [4, 49, 62, 65]
allow the rendering of novel views. While these datasets
have tremendously benefited the embodied navigation com-
munity, their utility for broader computer vision applica-
tions remains limited. Each scene, being a single 3D mesh,
restricts further customization, such as modifying the ob-
ject layout. Moreover, the visual quality of rendered novel
views depends on the reconstruction’s fidelity, often result-
ing in artifacts. While Taskonomy [67] and Omnidata [10]
have extended mid-level visual cues such as surface normal
for these datasets, semantic label acquisition remains ex-
pensive. In contrast, our work offers the flexibility to gener-
ate images with customized object layouts with consistent
visual quality, while also providing comprehensive labels at
no additional cost.
Synthetic Datasets. Synthetic datasets offer an alterna-
tive approach that eliminates the need for manual seman-
tic labeling by rendering realistic images from interior
scenes composed of independent object models [6, 8]. Ob-
ject layouts are usually created by artists [12, 36, 51] or
parsed from real scans [37], offering semantic realism of
the scenes [18, 60]. Methods like OpenRooms [37] and
Unity Synthetic Homes [57] also allow users to configure
rendering options, such as lighting. However, despite their
photorealism, the rendered images often lack physical plau-
sibility, with common issues like object penetration or slight
levitation. In addition, the object models are mostly fully
rigid and support very limited semantic states. Our gen-
erator not only ensures the physical plausibility of images
created, but also supports broader relationship customiza-
tion (e.g., “cooked” or “filled”) and more granular control
over the sampled state, such as openness level through joint
limit annotations.
3D Simulators. A large number of 3D simulators withphysical realism have been developed recently. Kubric [22]
focuses on generating physically plausible object clusters
without full-scene simulation. iGibson [33, 52] and Habi-
tat 2.0 [56] offer reconfigurable indoor scenes with artic-
ulated assets, the former notably supporting extended ob-
ject states such as wetness level. ThreeDWorld [13] empha-
sizes physical prediction, especially with non-rigid objects.
ProcTHOR [7] automates the large-scale generation of se-
mantically plausible virtual environments. Since these 3D
simulators cater to the embodied AI and robotics commu-
nity, their visual quality is often not prioritized. In con-
trast, we use OmniGibson, a new simulator that surpasses
the photorealism of the aforementioned ones, according to
a user study [34], positioning our work as more suitable for
computer vision research. Moreover, we provide a range
of utility functions in this work, allowing for easy creation
of diverse images tailored to specific needs—a feature most
existing 3D simulators lack.
3. BEHA VIOR Vision Suite
BEHA VIOR Vision Suite contains two main components
(Fig. 1): the extended BEHA VIOR-1K assets and the cus-
tomizable dataset generator. The assets serve as the foun-
dation, while the generator leverages these assets to create
vision datasets tailored to downstream tasks of interest.
3.1. Extended BEHA VIOR-1K Assets
The extended BEHA VIOR-1K assets comprises a diverse
collection of 8,841 object models and 1,000 scene instances,
derived from 51 artist-designed raw scenes. Of these ob-
jects, 2,156 are structural elements like walls, floors, and
ceilings, while the remaining 6,685 nonstructural items
span 1,937 categories, including food, tools, electronics,
clothing, and office supplies, among others. This catego-
rization is detailed in Fig. 2. Predominantly indoor, the 51
raw scenes also incorporate outdoor elements such as gar-
dens and encompass a wide variety of environments: houses
(23), offices (5), restaurants (6), grocery stores (4), hotels
(3), schools (5), and generic halls (4), as well as a simulated
twin of a mock apartment in our research lab. This collec-
tion of assets is the result of a year-long effort to extend the
BEHA VIOR-1K [34] assets to enhance their applicability in
computer vision.
We expanded the object collection from 5,215 to 8,841
by adding more everyday objects, segmenting building
structures into individual objects for more precise 3D
bounding box labels, and procedurally generating sliced
food. In addition, we have developed functionality that en-
ables the generation of diverse scene variations by altering
furniture object models and incorporating additional every-
day objects. We will release 1000 scene instances aug-
mented from the 51 raw scenes.
22403
Raw scenes Augmented 
scene instances 
F olded: 
F alse Open: 
100% F illed: 
Milk, 
30% 
(a) Examples of 3D objects and the semantic 
properties they support (b) Distribution of scenes, room types and 
objects (c) Examples of raw scenes and augmented 
scene instances Figure 2. Overview of extended BEHA VIOR-1K assets: Covering a wide range of object categories and scene types, our 3D assets have
high visual and physical fidelity and rich annotations of semantic properties, allowing us to generate 1,000+ realistic scene configurations.
To improve physical realism, we refined collision
meshes using V-HACD [42] and CoACD [61], manually
selecting the best parameters to ensure a balance between
physical accuracy, affordance preservation, and simula-
tion efficiency. For more than 2,000 objects, where this
method was insufficient, we manually designed their col-
lision meshes.
We enhanced lighting realism by annotating actual light
source objects, such as lamps and ceiling lights, to mimic
real-world illumination. For more detailed semantic prop-
erties, we annotated appropriate container fillable volumes
(e.g., cups, pots) and fluid source/sink locations (e.g.,
faucets, drains, sprayers), enabling us to spawn fluids in
the scene realistically. Scene objects were annotated if they
cannot be freely moved, e.g., when they physically support
other objects. Cluttered objects were distinctly annotated,
allowing them to be replaced with alternative clutters.
Altogether, we designed the assets to form a strong ba-
sis for custom data generation (discussed in §3.2), with a
functional organization that allows accurate object random-
ization, and the annotations to provide a large number of
modifiable parameters at both the object and scene levels.
3.2. Customizable Dataset Generator
The customizable dataset generator, the software compo-
nent of the BEHA VIOR Vision Suite, is designed to gen-erate synthetic datasets tailored to particular specifications.
Built on OmniGibson [34], it leverages NVIDIA Omni-
verse’s photorealistic, real-time renderer and OmniGibson’s
procedural sampling functions for object states to gener-
ate custom images and videos that satisfy arbitrary require-
ments. The produced datasets include rich, comprehensive
annotations–segmentation masks, 2D/3D bounding boxes,
depth, surface normals, flows, and point clouds–at no ad-
ditional cost. Crucially, it empowers users with extensive
control over the dataset generation process, allowing them
to specify requirements on scene layouts, object states, cam-
era angles, and lighting conditions, all while ensuring phys-
ical plausibility through the physics engine.
Capabilities. The generator has the following capabilities:
•Scene Object Randomization: It can swap scene ob-
jects with alternative models from the same category,
which are grouped based on visual and functional simi-
larities. This randomization significantly varies scene ap-
pearances while maintaining layouts’ semantic integrity.
•Physically Realistic Pose Generation: The generator can
procedurally change the physical states of objects to sat-
isfy certain predicates. This includes 1) placing objects
with respect to other objects in the scene in a certain way
(e.g., inside, on top of, or under), 2) opening or closing
articulated objects, 3) filling containers with fluids, and
22404
4) folding or unfolding pieces of cloth. The generator can
generate multiple valid configurations for the same pred-
icate and ensures physical plausibility.
•Predicate-Based Rich Labeling: Beyond usual labels (se-
mantic & instance segmentation, bounding boxes, surface
normals, depth, etc.), the generator also provides annota-
tions including unary states of an object (e.g., whether
an articulated object is open, or an appliance is toggled
on), binary predicates between two objects (e.g., if one is
touching, on top of, next to another) or between an object
and a substance (e.g., if an object is filled/covered/soaked
with a substance), and continuous labels (e.g., joint open-
ness for articulated objects, filled fraction for containers).
•Camera Pose and Trajectory Sampling: Finding proper
camera pose in a 3D scene is a challenging but crucial
step in the rendering pipeline: the camera shall not be
occluded and points at the subject of interest. The gener-
ator uses occupancy grids and hand-crafted heuristics to
generate both static camera poses and plausible traversal
trajectories that satisfy these constraints to curate image
or scene traversal video datasets.
•Configurable Rendering: Through a user-friendly API,
the generator allows for the customization of rendering
parameters, including lighting and camera specifics such
as aperture and field of view.
Dataset Generation. Images in the BVS dataset can be
generated as follows. First, we select one of the 51 raw
scenes from the user-configured scene category (say, an of-
fice). Scene objects are randomized with instances from the
same category. Depending on the user configuration, we
determine additional objects to add to the scene. We place
the objects using the pose generation capabilities based on
user-specified requirements. This might include cluttering
certain areas (e.g., filling a fridge with perishables) or indi-
vidually manipulating object states (e.g., making a cabinet
open or a table covered with water) for predicate prediction.
We then generate a camera pose (or a sequence of poses
as a camera trajectory), as well as randomize the scene’s
lighting parameters and the camera’s intrinsics based on
the user’s specifications. Finally, we render an image (or
a sequence of images) and record it alongside all relevant
labels requested by the user, including additional modali-
ties (depth/segmentation/etc.), bounding boxes, and predi-
cate and object state values.
4. Applications and Experiments
We present three applications and corresponding experi-
ments to demonstrate the utility of BVS: first, systemati-
cally evaluating model robustness against various continu-
ous domain shifts, such as the lighting condition (§4.1); sec-
ond, assessing various scene understanding models using
a consistent set of images with comprehensive annotationsAxis # scenes # video clips
Articulation 17 237
Lighting 16 441
Visibility 14 211
Zoom 9 215
Pitch 16 268
Table 2. We generate up to 200–500 short video clips with diverse
scene configurations for parametric evaluation (§4.1). Each video
clip varies along one axis of distribution shift with a single target
object. On average, each video has 300 frames.
(§4.2); and third, training a model for a new vision task, ob-
ject states and relations prediction, on synthesized data and
evaluating its simulation-to-real transfer capability. (§4.3).
4.1. Parametric Model Evaluation
Parametric model evaluation is essential for developing and
understanding perception models, enabling a systematic as-
sessment of performance robustness against various domain
shifts. Previous efforts, such as 3DCC [29], have explored
image corruption generation using 3D information, yet their
scope is constrained by the static nature of input meshes,
limiting the type and extent of possible variations. Lever-
aging the flexibility of the simulator, our generator extends
parametric evaluation to more diverse axes, including scene,
camera, and object state changes.
Task Design and Dataset Generation. We focus on five
key parameters difficult to rigorously control in real-world
datasets yet significantly influence model performance: ob-
ject articulation, lighting, object visibility, camera zoom,
and camera pitch. Each parameter varies along a continu-
ous axis for evaluating baseline models. For instance, object
visibility varies from fully occluded to fully visible.
We generate 200 to 500 videos for each axis (Tab. 2), us-
ing our collection of more than 8,000 3D assets. Each video
includes a target object with changes focused on a single pa-
rameter under examination. Fig. 3 shows examples of target
objects with variations along each axis. We maintained con-
sistency in other aspects of the environment, systematically
synthesizing images to isolate the main parameter’s impact.
To validate our findings in real-world conditions and fur-
ther assess the sim2real transfer capability of parametric
evaluation, we collected a smaller-scale real dataset for each
of the 5 axes and replicated the evaluation. For setup details
and additional results, please refer to the appendix.
Baselines and Metrics. We explore two vision tasks: open-
vocabulary detection and open-vocabulary segmentation ,
hypothesizing that models for these tasks may be sensitive
to object-centric domain shifts. For baselines, we select
the current state-of-the-art (SOTA) models on real datasets:
GLIP [35], RAM [69], and Grounding DINO [41] for de-
tection, and ODISE [68], OpenSeeD [64], and Grounding
SAM [27] for segmentation.
22405
Figure 3. Parametric evaluation of object detection models on five
example video clips. Selected frames from these clips are shown
on the left, with the target object highlighted in magenta. Average
Precisions (APs) for our baseline models in §4.2 are plotted on the
right. Since BVS allows for full customization of scene layout and
camera viewpoints, we can systematically evaluate model robust-
ness against variations in object articulation, lighting conditions,
visibility, zoom (object proximity), and pitch (object pose). As il-
lustrated, current SOTA models exhibit limited robustness to these
axes of variation.
Results and Analysis. In Fig. 3 and Fig. 4, we present
example images when varying each parameter as well as
respective detection Average Precision (AP) performance.
AP, calculated exclusively for the target object (highlighted
in magenta), assesses the model’s recognition accuracy. De-
tailed analyses reveal the following:
•Articulation varies the joint angles of the articulated
target object, from fully closed to fully open, including pro-
cesses such as opening/closing drawers or doors, and fold-
ing/unfolding laptops. A notable negative correlation be-
tween the degree of articulation and model performance
suggests that models, typically trained or evaluated on exist-
ing benchmarks featuring mostly closed articulated objects
(e.g., closed washing machines and microwaves), struggle
with recognizing objects in open states.
•Lighting adjusts global illumination of the environ-
Figure 4. Mean performance of open-vocab object detection and
segmentation models across five axes. The larger a model’s col-
ored envelope, the more robust it is. Through BVS, new vision
models can be systematically tested for their robustness along
these five dimensions and beyond: our users can easily add new
axes of domain shift with just a few lines of code.
ment from dark to bright. We observed improving model
performance up to a midpoint brightness level of 0.5, be-
yond which it plateaus. This suggests that, while cur-
rent models suffer from low-light conditions, their perfor-
mance saturates once the brightness level surpasses a cer-
tain threshold.
•Visibility shifts the visibility of the target object from
fully occluded to fully visible, which is computed as the ra-
tio of visible to total pixels of the target object. We observe
a steep decline in model performance as visibility drops be-
low 0.5, which highlights a significant opportunity to en-
hance model robustness to partial occlusions.
•Zoom controls camera zoom from zoomed-in to
zoomed-out. Results show that extremely close views,
where a partial view of the target object occupies the en-
tire image, hinder performance due to a lack of contextual
information. In contrast, too-zoomed-out views make the
object too small for models to detect it effectively. Optimal
performance is achieved at moderate zoom levels.
•Pitch varies camera pitch from looking up to look-
ing down. We find that models perform inconsistently
with seemingly benign changes in camera viewpoint, gener-
ally showing improved performance when the camera looks
down at the target object. One potential explanation is that
objects in large-scale real datasets are often captured from
above, making this perspective more familiar to the models.
To summarize, we observe significant performance dis-
crepancies across three models on all five axes, with our
parallel experiments in real settings (see the appendix) con-
firming that these trends observed in synthetic data mirror
those in real-world scenarios. This underscores the lack of
robustness of the current SOTA models in extreme or out-
of-distribution test environments. By generating large-scale
synthetic datasets with controlled variability, BVS provides
a unique and powerful test bed to evaluate model perfor-
mance. Furthermore, consistent with §4.2, the relative per-
formance remains steady across the five axes, highlighting
22406
T raversal 
V ideos 
Annotations 
Figure 5. Holistic Scene Understanding Dataset. We generated extensive traversal videos across representative scenes, each with 10+
camera trajectories. For each image, BVS generates various labels (e.g., scene graphs, segmentation masks, depth) as shown on the right.
Open-vocab Det. AP↑AP small↑AP medium↑AP large↑COCO (AP)
GLIP [35] 41.4 7.0 27.5 61.8 60.8
RAM [69] 41.3 6.4 27.8 63.9 61.4
Grounding DINO [41] 44.7 11.9 31.2 66.3 63.0Open-vocab Seg. AP↑AP small↑AP medium↑AP large↑ADE (AP)
ODISE [64] 57.1 41.0 53.2 65.0 13.9
OpenSeeD [68] 57.3 42.0 54.1 64.8 15.0
Grounding SAM [27] 59.2 42.9 54.4 65.1 14.8
Depth Est. RMS↓AbsRel ↓Log10↓δ1↑δ2↑δ3↑NYUv2 ( δ1)
DPT [50] 0.66 0.14 0.05 0.09 0.15 0.20 0.90
NVDS [59] 0.58 0.13 0.04 0.10 0.15 0.21 0.93
iDisc [47] 0.49 0.13 0.04 0.12 0.19 0.22 0.94Point Cloud Recon. Comp. Ratio ↑Comp. ↑Acc.↓Replica (C.R.)
GradSLAM [28] 50.0 14.8 29.8 67.9
NICE-SLAM [70] 66.3 12.0 23.5 89.3
Table 3. A comprehensive evaluation of SOTA models on four vision tasks. Our synthetic dataset can be a faithful proxy for real datasets
as the relative performance between different models closely correlates to that of the real datasets .
the predictive value of the datasets generated by BVS.
4.2. Holistic Scene Understanding
One of the major advantages of synthetic datasets, including
BVS, is that they offer various types of labels (segmentation
masks, depth maps, and bounding boxes) for the same sets
of input images. We believe that this feature can fuel the de-
velopment of versatile vision models that can perform mul-
tiple perception tasks at the same time in the future. Since
such models are not currently available, we instead evaluate
the current SOTA methods on a subset of the tasks that BVS
supports (see below). This will also serve as a validation of
the photorealism of our datasets, i.e., models trained on real
datasets should perform reasonably without fine-tuning.
Task Design and Dataset Generation. Equipped with
BVS’s powerful generator (see §3.2), we generated 100+
full scene traversal videos with a total of 266240 frames
with per-frame ground truth annotations in multiple modal-
ities. Fig. 5 shows an overview of the generated dataset.
Baselines and Metrics. In Tab. 3, we assess 11 models in
four tasks. Specifically, we consider Detection andSegmen-
tation tasks, both in the challenging open vocabulary set-
ting [41, 68], as well as Depth Estimation andPoint Cloud
Reconstruction , with standard metrics used.Results and Analysis. We summarize all our evaluation re-
sults in Tab. 3. We observe that the relative performance
of these models on our synthetic dataset has a high corre-
lation with that on real datasets such as MS COCO [39]
or NYUv2 [46], indicating that our generated synthetic
datasets can be a faithful proxy for real datasets.
In summary, we provide a comprehensive benchmark to
score and understand a wide range of existing models for
each of the four tasks on exactly the same images. Al-
though most current vision models focus on a single output
modality, we hope BEHA VIOR Vision Suite could moti-
vate researchers and practitioners to develop versatile mod-
els that concurrently predict multiple modalities in the fu-
ture, where our benchmarking results for single-task SOTA
methods in this section could serve as a useful reference.
4.3. Object States and Relations Prediction
BVS’s capabilities extend beyond model evaluation shown
in §4.1 and §4.2. Users can also leverage BVS to generate
training data with specific object configurations that are dif-
ficult to accumulate or annotate in the real world. This sec-
tion illustrates BVS’s practical application in synthesizing
a dataset that facilitates the training of a vision model ca-
pable of zero-shot transfer to real-world images on the task
of object relationship prediction [3, 14–17, 24]. Additional
22407
SyntheticRealOnTopUnderInsideOpen /Close
Figure 6. Sample images of each class from our generated syn-
thetic and collected real datasets.
experiments focusing on unary object states, filled and
folded , are detailed in the appendix.
Task Design and Dataset Generation. Predicting object
relationships, such as open andinside , is a crucial yet
challenging perception task due to the difficulties in col-
lecting such data in the real world, let alone the costly
annotations[32, 43, 45]. We use our generator to synthe-
size 12.5k images with five labels ( open ,close ,ontop ,
inside ,under ), depicting relationships between target
objects. We also collected and labeled 910 real images with
unseen object instances and scenes to test sim2real perfor-
mance. Examples are shown in Fig. 6.
Baselines and Metrics. Adapting from [45], our model
takes an image and target objects’ bounding boxes as input,
and outputs a five-way classification over the five labels.
We specifically define open/close as a binary relation-
ship between the movable link and the unmovable base of
an articulated object, enabling fine-grained articulation state
assessment. For example, the model can be queried for the
open or closed status of individual drawers of a cabinet. De-
tailed model architecture is available in the appendix.
We compare our model with zero-shot CLIP, which is
not trained on our synthetic dataset, in terms of precision,
recall, and F1, on the synthetic evaluation set and the real
test set. Specifically, by harnessing CLIP’s zero-shot ca-
pabilities [48], this baseline outputs a five-way classifica-
tion prediction by comparing the image embeddings with
the five verbalized prompts’ text embeddings.
Results and Analysis. Tab. 4 presents the quantitative re-
sults on the held-out synthetic dataset and the real dataset
for our method. Although there is some performance gap,
our model trained on only synthetic data can transfer to
real images with promising overall accuracy. Additionally,
unary state prediction experiments, detailed in the appendix,
also reveal high accuracy in both domains. These resultsOpen Close Ontop Inside Under Avg.
Precision 0.962 0.897 0.947 0.989 0.874 0.932
Recall 0.822 0.978 0.913 0.995 0.949 0.929
Precision 0.943 0.958 0.545 0.906 0.948 0.863
Recall 0.757 0.915 0.913 0.776 0.703 0.817Test on
Synthetic
Real
Table 4. Classification results on held-out synthetic eval set and
real test set for our method adapted from [45].
Method Precision Recall F1
Zero-shot CLIP 0.293 0.282 0.271
Ours 0.863 0.817 0.839
Table 5. Classification results on the real test set. Task-specific
training on synthetic data boosts performance on real images.
underscore that BVS offers a promising way to obtain re-
alistic synthetic data that researchers can use not only for
evaluation (as shown in §4.1 and §4.2), but also for train-
ing models that can then be transferred to the real world.
In fact, from Tab. 5, we observe that task-specific training
on synthetic data is a very effective method to obtain good
performance on real images.
5. Conclusion
We have introduced the BEHA VIOR Vision Suite (BVS),
a novel toolkit designed for the systematic evaluation and
comprehensive understanding of computer vision models.
BVS enables researchers to control a wide range of parame-
ters across scene, object, and camera levels, facilitating the
creation of highly customized datasets. Our experiments
highlight BVS’s versatility and efficacy through three key
applications. First, we show its ability to evaluate model
robustness against various domain shifts, underscoring its
value in systematically assessing model performance under
challenging conditions. Second, we present comprehensive
benchmarking of scene understanding models on a unified
dataset, illustrating the potential for developing multi-task
models using a single BVS dataset. Lastly, we investigate
BVS’s role in facilitating sim2real transfer for novel vision
tasks, including object states and relations prediction. BVS
highlights synthetic data’s promise in advancing the field,
offering researchers the means to generate high-quality, di-
verse, and realistic datasets tailored to specific needs.
Acknowledgments. The work is in part supported by
the Stanford Institute for Human-Centered AI (HAI), NSF
CCRI #2120095, RI #2338203, ONR MURI N00014-22-1-
2740, N00014-21-1-2801, Amazon, Amazon ML Fellow-
ship, and Nvidia.
22408
References
[1] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry,
Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe,
Daniel Kurz, Arik Schwartz, and Elad Shulman. ARK-
itscenes - a diverse real-world dataset for 3d indoor scene
understanding using mobile RGB-d data. In Thirty-fifth Con-
ference on Neural Information Processing Systems Datasets
and Benchmarks Track (Round 1) , 2021. 2
[2] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,
and Juan Carlos Niebles. Activitynet: A large-scale video
benchmark for human activity understanding. In Proceed-
ings of the ieee conference on computer vision and pattern
recognition , pages 961–970, 2015. 1
[3] Paola Cascante-Bonilla, Khaled Shehada, James Seale
Smith, Sivan Doveh, Donghyun Kim, Rameswar Panda, Gul
Varol, Aude Oliva, Vicente Ordonez, Rogerio Feris, et al.
Going beyond nouns with vision & language models using
synthetic data. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 20155–20165,
2023. 7
[4] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-
ber, Matthias Niessner, Manolis Savva, Shuran Song, Andy
Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-
d data in indoor environments. International Conference on
3D Vision (3DV) , 2017. 1, 3
[5] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nießner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes. In
Proc. Computer Vision and Pattern Recognition (CVPR),
IEEE , 2017. 2
[6] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,
Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana
Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Obja-
verse: A universe of annotated 3d objects. arXiv preprint
arXiv:2212.08051 , 2022. 3
[7] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs,
Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve,
Aniruddha Kembhavi, and Roozbeh Mottaghi. Procthor:
Large-scale embodied ai using procedural generation. Ad-
vances in Neural Information Processing Systems , 35:5982–
5994, 2022. 2, 3
[8] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong
Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-
tian Laforte, Vikram V oleti, Samir Yitzhak Gadre, Eli
VanderBilt, Aniruddha Kembhavi, Carl V ondrick, Georgia
Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi.
Objaverse-xl: A universe of 10m+ 3d objects. arXiv preprint
arXiv:2307.05663 , 2023. 3
[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248–255. Ieee, 2009. 1
[10] Ainaz Eftekhar, Alexander Sax, Roman Bachmann, Jitendra
Malik, and Amir Zamir. Omnidata: A scalable pipeline for
making multi-task mid-level vision datasets from 3d scans,
2021. 3[11] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge. International journal of computer
vision , 88:303–338, 2010. 1
[12] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang
Zhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d fur-
niture shape with texture. International Journal of Computer
Vision , 129:3313–3337, 2021. 1, 3
[13] Chuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca,
Martin Schrimpf, James Traer, Julian De Freitas, Jonas Ku-
bilius, Abhishek Bhandwaldar, Nick Haber, et al. Threed-
world: A platform for interactive multi-modal physical sim-
ulation. arXiv preprint arXiv:2007.04954 , 2020. 2, 3
[14] Yunhao Ge, Harkirat Behl, Jiashu Xu, Suriya Gunasekar,
Neel Joshi, Yale Song, Xin Wang, Laurent Itti, and Vibhav
Vineet. Neural-sim: Learning to generate training data with
nerf. In European Conference on Computer Vision , pages
477–493. Springer, 2022. 7
[15] Yunhao Ge, Jiashu Xu, Brian Nlong Zhao, Laurent Itti, and
Vibhav Vineet. Em-paste: Em-guided cut-paste with dall-
e augmentation for image-level weakly supervised instance
segmentation, 2022.
[16] Yunhao Ge, Jiashu Xu, Brian Nlong Zhao, Neel Joshi, Lau-
rent Itti, and Vibhav Vineet. Dall-e for detection: Language-
driven compositional image synthesis for object detection.
arXiv preprint arXiv:2206.09592 , 2022.
[17] Yunhao Ge, Jiashu Xu, Brian Nlong Zhao, Neel Joshi, Lau-
rent Itti, and Vibhav Vineet. Beyond generation: Harnessing
text to image models for object detection and segmentation.
arXiv preprint arXiv:2309.05956 , 2023. 1, 7
[18] Yunhao Ge, Hong-Xing Yu, Cheng Zhao, Yuliang Guo,
Xinyu Huang, Liu Ren, Laurent Itti, and Jiajun Wu. 3d copy-
paste: Physically plausible object insertion for monocular 3d
detection. Advances in Neural Information Processing Sys-
tems, 36, 2024. 3
[19] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In 2012 IEEE conference on computer vision and pat-
tern recognition , pages 3354–3361. IEEE, 2012. 1
[20] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-
ski, Joanna Materzynska, Susanne Westphal, Heuna Kim,
Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz
Mueller-Freitag, et al. The” something something” video
database for learning and evaluating visual common sense.
InProceedings of the IEEE international conference on com-
puter vision , pages 5842–5850, 2017.
[21] Kristen Grauman, Andrew Westbury, Eugene Byrne,
Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson
Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d:
Around the world in 3,000 hours of egocentric video. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 18995–19012, 2022. 1
[22] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch,
Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapra-
gasam, Florian Golemo, Charles Herrmann, Thomas Kipf,
Abhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh-
Ti (Derek) Liu, Henning Meyer, Yishu Miao, Derek
22409
Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Rad-
wan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi,
Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun,
Suhani V ora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi,
Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: a scal-
able dataset generator. 2022. 3
[23] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi
Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.
Vizwiz grand challenge: Answering visual questions from
blind people. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 3608–3617,
2018. 1
[24] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing
Zhang, Philip Torr, Song Bai, and XIAOJUAN QI. Is syn-
thetic data from generative models ready for image recogni-
tion? In The Eleventh International Conference on Learning
Representations , 2022. 7
[25] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-
vath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,
Samyak Parajuli, Mike Guo, et al. The many faces of robust-
ness: A critical analysis of out-of-distribution generalization.
InProceedings of the IEEE/CVF international conference on
computer vision , pages 8340–8349, 2021. 1
[26] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-
hardt, and Dawn Song. Natural adversarial examples. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 15262–15271, 2021. 1
[27] IDEA-Research. Grounding sam. https :
/ / github . com / IDEA - Research / Grounded -
Segment-Anything , 2023. 5, 7
[28] Krishna Murthy Jatavallabhula, Soroush Saryazdi, Ganesh
Iyer, and Liam Paull. gradslam: Automagically differen-
tiable slam. arXiv preprint arXiv:1910.10672 , 2019. 7
[29] O ˘guzhan Fatih Kar, Teresa Yeo, Andrei Atanov, and Amir
Zamir. 3d common corruptions and data augmentation,
2022. 5
[30] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. arXiv preprint arXiv:2304.02643 , 2023. 1
[31] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt,
Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani,
Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d
environment for visual ai. arXiv preprint arXiv:1712.05474 ,
2017. 2
[32] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
tidis, Li-Jia Li, David A Shamma, et al. Visual genome:
Connecting language and vision using crowdsourced dense
image annotations. Int J Comput Vis , 123:32–73, 2017. 1, 8
[33] Chengshu Li, Fei Xia, Roberto Mart ´ın-Mart ´ın, Michael Lin-
gelbach, Sanjana Srivastava, Bokui Shen, Kent Vainio, Cem
Gokmen, Gokul Dharan, Tanish Jain, et al. igibson 2.0:
Object-centric simulation for robot learning of everyday
household tasks. arXiv preprint arXiv:2108.03272 , 2021. 2,
3
[34] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen,
Sanjana Srivastava, Roberto Mart ´ın-Mart ´ın, Chen Wang,Gabrael Levine, Michael Lingelbach, Jiankai Sun, Mona
Anvari, Minjune Hwang, Manasi Sharma, Arman Aydin,
Dhruva Bansal, Samuel Hunter, Kyu-Young Kim, Alan Lou,
Caleb R Matthews, Ivan Villa-Renteria, Jerry Huayang Tang,
Claire Tang, Fei Xia, Silvio Savarese, Hyowon Gweon,
Karen Liu, Jiajun Wu, and Li Fei-Fei. Behavior-1k: A
benchmark for embodied ai with 1,000 everyday activities
and realistic simulation. In Proceedings of The 6th Confer-
ence on Robot Learning , pages 80–93. PMLR, 2023. 1, 2, 3,
4
[35] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-
wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu
Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded
language-image pre-training. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10965–10975, 2022. 5, 7
[36] Wenbin Li, Sajad Saeedi, John McCormac, Ronald Clark,
Dimos Tzoumanikas, Qing Ye, Yuzhong Huang, Rui Tang,
and Stefan Leutenegger. Interiornet: Mega-scale multi-
sensor photo-realistic indoor scenes dataset. arXiv preprint
arXiv:1809.00716 , 2018. 1, 3
[37] Zhengqin Li, Ting-Wei Yu, Shen Sang, Sarah Wang, Meng
Song, Yuhan Liu, Yu-Ying Yeh, Rui Zhu, Nitesh Gun-
davarapu, Jia Shi, et al. Openrooms: An end-to-end open
framework for photorealistic indoor scene datasets. arXiv
preprint arXiv:2007.12868 , 2020. 1, 3
[38] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift
module for efficient video understanding. In Proceedings of
the IEEE/CVF international conference on computer vision ,
pages 7083–7093, 2019. 1
[39] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D.
Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva
Ramanan, Piotr Doll’a r, and C. Lawrence Zitnick. Microsoft
COCO: common objects in context. CoRR , abs/1405.0312,
2014. 1, 7
[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13 , pages 740–755. Springer, 2014. 1
[41] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun
Zhu, et al. Grounding dino: Marrying dino with grounded
pre-training for open-set object detection. arXiv preprint
arXiv:2303.05499 , 2023. 5, 7
[42] Khaled Mamou. V olumetric approximate convex decompo-
sition. In Game Engine Gems 3 , chapter 12, pages 141–158.
A K Peters / CRC Press, 2016. 4
[43] Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B
Tenenbaum, and Jiajun Wu. The neuro-symbolic concept
learner: Interpreting scenes, words, and sentences from nat-
ural supervision. In International Conference on Learning
Representations , 2018. 8
[44] Roberto Martin-Martin, Mihir Patel, Hamid Rezatofighi,
Abhijeet Shenoi, JunYoung Gwak, Eric Frankel, Amir
Sadeghian, and Silvio Savarese. Jrdb: A dataset and bench-
mark of egocentric robot visual perception of humans in built
22410
environments. IEEE transactions on pattern analysis and
machine intelligence , 2021. 1
[45] Toki Migimatsu and Jeannette Bohg. Grounding predi-
cates through actions. In 2022 International Conference on
Robotics and Automation (ICRA) , pages 3498–3504, 2022. 8
[46] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob
Fergus. Indoor segmentation and support inference from
rgbd images. In ECCV , 2012. 1, 2, 7
[47] Luigi Piccinelli, Christos Sakaridis, and Fisher Yu. idisc: In-
ternal discretization for monocular depth estimation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 21477–21487, 2023. 7
[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 8
[49] Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans,
Oleksandr Maksymets, Alex Clegg, John Turner, Eric Un-
dersander, Wojciech Galuba, Andrew Westbury, Angel X
Chang, et al. Habitat-matterport 3d dataset (hm3d): 1000
large-scale 3d environments for embodied ai. arXiv preprint
arXiv:2109.08238 , 2021. 1, 3
[50] Ren ´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-
sion transformers for dense prediction. ArXiv preprint , 2021.
7
[51] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit
Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb,
and Joshua M Susskind. Hypersim: A photorealistic syn-
thetic dataset for holistic indoor scene understanding. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 10912–10922, 2021. 1, 3
[52] Bokui Shen, Fei Xia, Chengshu Li, Roberto Mart ´ın-Mart ´ın,
Linxi Fan, Guanzhi Wang, Claudia P ´erez-D’Arpino, Shya-
mal Buch, Sanjana Srivastava, Lyne Tchapmi, et al. igibson
1.0: A simulation environment for interactive tasks in large
realistic scenes. In 2021 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS) , pages 7520–7527.
IEEE, 2021. 1, 2, 3
[53] Gunnar A. Sigurdsson, Abhinav Gupta, Cordelia Schmid,
Ali Farhadi, and Karteek Alahari. Actor and observer: Joint
modeling of first and third-person videos. In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR) , 2018. 1
[54] Shuran Song, Samuel P. Lichtenberg, and Jianxiong Xiao.
Sun rgb-d: A rgb-d scene understanding benchmark suite.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) , 2015. 2
[55] Sanjana Srivastava, Chengshu Li, Michael Lingelbach,
Roberto Mart ´ın-Mart ´ın, Fei Xia, Kent Elliott Vainio, Zheng
Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behav-
ior: Benchmark for everyday household activities in virtual,
interactive, and ecological environments. In Conference on
Robot Learning , pages 477–490. PMLR, 2022. 2
[56] Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wi-
jmans, Yili Zhao, John Turner, Noah Maestre, MustafaMukadam, Devendra Singh Chaplot, Oleksandr Maksymets,
et al. Habitat 2.0: Training home assistants to rearrange their
habitat. Advances in Neural Information Processing Systems ,
34:251–266, 2021. 2, 3
[57] Unity Technologies. Unity synthhomes: A synthetic home
interior dataset generator. https://github.com/
Unity-Technologies/SynthHomes , 2022. 3
[58] Heng Wang and Cordelia Schmid. Action recognition with
improved trajectories. In Proceedings of the IEEE inter-
national conference on computer vision , pages 3551–3558,
2013. 1
[59] Yiran Wang, Min Shi, Jiaqi Li, Zihao Huang, Zhiguo Cao,
Jianming Zhang, Ke Xian, and Guosheng Lin. Neural video
depth stabilizer. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 9466–9476,
2023. 7
[60] Zian Wang, Wenzheng Chen, David Acuna, Jan Kautz, and
Sanja Fidler. Neural light field estimation for street scenes
with differentiable virtual object insertion. In European Con-
ference on Computer Vision , pages 380–397. Springer, 2022.
3
[61] Xinyue Wei, Minghua Liu, Zhan Ling, and Hao Su. Approx-
imate convex decomposition for 3d meshes with collision-
aware concavity and tree search. ACM Transactions on
Graphics (TOG) , 41(4):1–18, 2022. 4
[62] Fei Xia, Amir R. Zamir, Zhi-Yang He, Alexander Sax, Jiten-
dra Malik, and Silvio Savarese. Gibson Env: real-world per-
ception for embodied agents. In Computer Vision and Pat-
tern Recognition (CVPR), 2018 IEEE Conference on . IEEE,
2018. 1, 3
[63] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and
Dieter Fox. Posecnn: A convolutional neural network for
6d object pose estimation in cluttered scenes. arXiv preprint
arXiv:1711.00199 , 2017. 1
[64] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-
long Wang, and Shalini De Mello. Open-vocabulary panop-
tic segmentation with text-to-image diffusion models. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 2955–2966, 2023. 5,
7
[65] Karmesh Yadav, Ram Ramrakhya, Santhosh Kumar Ramakr-
ishnan, Theo Gervet, John Turner, Aaron Gokaslan, Noah
Maestre, Angel Xuan Chang, Dhruv Batra, Manolis Savva,
et al. Habitat-matterport 3d semantics dataset. arXiv preprint
arXiv:2210.05633 , 2022. 3
[66] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner,
and Angela Dai. Scannet++: A high-fidelity dataset of 3d
indoor scenes. In Proceedings of the International Confer-
ence on Computer Vision (ICCV) , 2023. 2
[67] Amir R. Zamir, Alexander Sax, William B. Shen, Leonidas J.
Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy:
Disentangling task transfer learning. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) . IEEE,
2018. 3
[68] Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan
Li, Jianwei Yang, and Lei Zhang. A simple framework for
open-vocabulary segmentation and detection. In Proceed-
22411
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 1020–1031, 2023. 5, 7
[69] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li,
Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo,
Yaqian Li, Shilong Liu, et al. Recognize anything: A strong
image tagging model. arXiv preprint arXiv:2306.03514 ,
2023. 5, 7
[70] Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hu-
jun Bao, Zhaopeng Cui, Martin R Oswald, and Marc Polle-
feys. Nice-slam: Neural implicit scalable encoding for slam.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 12786–12796, 2022.
7
[71] Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, and
Jieping Ye. Object detection in 20 years: A survey. Proceed-
ings of the IEEE , 2023. 1
22412
