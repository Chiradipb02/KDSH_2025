EscherNet: A Generative Model for Scalable View Synthesis
Xin Kong1*Shikun Liu1*Xiaoyang Lyu2Marwan Taher1
Xiaojuan Qi2Andrew J. Davison1
1Dyson Robotics Lab, Imperial College London2The University of Hong Kong
∗Corresponding Authors: {x.kong21,shikun.liu17 }@imperial.ac.uk
Figure 1. We introduce EscherNet, a diffusion model that can generate a ﬂexible number of consistent target views (highlighted in blue)
with arbitrary camera poses, based on a ﬂexible number of reference views (highlighted in purple ). EscherNet demonstrates remarkable
precision in camera control and robust generalisation across synthetic and real-world images featuring multiple objects and rich textures.
Abstract
We introduce EscherNet, a multi-view conditioned dif-
fusion model for view synthesis. EscherNet learns im-
plicit and generative 3D representations coupled with a
specialised camera positional encoding, allowing precise
and continuous relative control of the camera transforma-
tion between an arbitrary number of reference and target
views. EscherNet offers exceptional generality, ﬂexibility,
and scalability in view synthesis — it can generate more
than 100 consistent target views simultaneously on a sin-
gle consumer-grade GPU, despite being trained with a ﬁxed
number of 3 reference views to 3 target views. As a result,
EscherNet not only addresses zero-shot novel view synthe-
sis, but also naturally uniﬁes single- and multi-image 3Dreconstruction, combining these diverse tasks into a single,
cohesive framework. Our extensive experiments demon-
strate that EscherNet achieves state-of-the-art performance
in multiple benchmarks, even when compared to methods
speciﬁcally tailored for each individual problem. This re-
markable versatility opens up new directions for designing
scalable neural architectures for 3D vision. Project page:
https://kxhit.github.io/EscherNet .
1. Introduction
View synthesis stands as a fundamental task in computer vi-
sion and computer graphics. By allowing the re-rendering
of a scene from arbitrary viewpoints based on a set of ref-
erence viewpoints, this mimics the adaptability observed in
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
9503
human vision. This ability is not only crucial for practi-
cal everyday tasks like object manipulation and navigation,
but also plays a pivotal role in fostering human creativity,
enabling us to envision and craft objects with depth, per-
spective, and a sense of immersion.
In this paper, we revisit the problem of view synthesis
and ask: How can we learn a general 3D representation to
facilitate scalable view synthesis? We attempt to investigate
this question from the following two observations:
i)Up until now, recent advances in view synthesis have
predominantly focused on training speed and/or rendering
efﬁciency [ 12,18,31,48]. Notably, these advancements
all share a common reliance on volumetric rendering for
scene optimisation. Thus, all these view synthesis methods
are inherently scene-speciﬁc , coupled with global 3D spatial
coordinates. In contrast, we advocate for a paradigm shift
where a 3D representation relies solely on scene colours
and geometries, learning implicit representations without
the need for ground-truth 3D geometry, while also main-
taining independence from any speciﬁc coordinate system.
This distinction is crucial for achieving scalability to over-
come the constraints imposed by scene-speciﬁc encoding.
ii)View synthesis, by nature, is more suitable to be cast
as a conditional generative modelling problem , similar to
generative image in-painting [ 25,60]. When given only a
sparse set of reference views, a desired model should pro-
vide multiple plausible predictions, leveraging the inherent
stochasticity within the generative formulation and drawing
insights from natural image statistics and semantic priors
learned from other images and objects. As the available
information increases, the generated scene becomes more
constrained, gradually converging closer to the ground-truth
representation. Notably, existing 3D generative models cur-
rently only support a single reference view [ 20–23,44]. We
argue that a more desirable generative formulation should
ﬂexibly accommodate varying levels of input information.
Building upon these insights, we introduce EscherNet,
an image-to-image conditional diffusion model for view
synthesis. EscherNet leverages a transformer architec-
ture [ 51], employing dot-product self-attention to capture
the intricate relation between both reference-to-target and
target-to-target views consistencies. A key innovation
within EscherNet is the design of camera positional encod-
ing (CaPE), dedicated to representing both 4 DoF (object-
centric) and 6 DoF camera poses. This encoding incorpo-
rates spatial structures into the tokens, enabling the model to
compute self-attention between query and key solely based
on their relative camera transformation. In summary, Esch-
erNet exhibits these remarkable characteristics:
•Consistency : EscherNet inherently integrates view con-
sistency thanks to the design of camera positional encod-
ing, encouraging both reference-to-target and target-to-
target view consistencies .•Scalability : Unlike many existing neural rendering meth-
ods that are constrained by scene-speciﬁc optimisation,
EscherNet decouples itself from any speciﬁc coordinate
system and the need for ground-truth 3D geometry, with-
out any expensive 3D operations ( e.g. 3D convolutions
or volumetric rendering), making it easier to scale with
everyday posed 2D image data .
•Generalisation : Despite being trained on only a ﬁxed
number of 3 reference to 3 target views, EscherNet ex-
hibits the capability to generate any number of target
views, with any camera poses, based on any number of
reference views . Notably, EscherNet exhibits improved
generation quality with an increased number of reference
views, aligning seamlessly with our original design goal.
We conduct a comprehensive evaluation across both
novel view synthesis and single/multi-image 3D reconstruc-
tion benchmarks. Our ﬁndings demonstrate that EscherNet
not only outperforms all 3D diffusion models in terms of
generation quality but also can generate plausible view syn-
thesis given very limited views. This stands in contrast to
these scene-speciﬁc neural rendering methods such as In-
stantNGP [ 31] and Gaussian Splatting [ 18], which often
struggle to generate meaningful content under such con-
straints. This underscores the effectiveness of our method’s
simple yet scalable design, offering a promising avenue for
advancing view synthesis and 3D vision as a whole.
2. Related Work
Neural 3D Representations Early works in neural 3D
representation learning focused on directly optimising on
3D data, using representations such as voxels [ 26] and point
clouds [ 40,41], for explicit 3D representation learning. Al-
ternatively, another line of works focused on training neural
networks to map 3D spatial coordinates to signed distance
functions [ 35] or occupancies [ 28,37], for implicit 3D rep-
resentation learning. However, all these methods heavily
rely on ground-truth 3D geometry, limiting their applicabil-
ity to small-scale synthetic 3D data [ 2,55].
To accommodate a broader range of data sources, dif-
ferentiable rendering functions [ 33,46] have been intro-
duced to optimise neural implicit shape representations with
multi-view posed images. More recently, NeRF [ 29] paved
the way to a signiﬁcant enhancement in rendering qual-
ity compared to these methods by optimising MLPs to en-
code 5D radiance ﬁelds. In contrast to tightly coupling 3D
scenes with spatial coordinates, we introduce EscherNet as
an alternative for 3D representation learning by optimising a
neural network to learn the interaction between multi-view
posed images, independent of any coordinate system.
Novel View Synthesis The success of NeRF has sparked
a wave of follow-up methods that address faster training
and/or rendering efﬁciency, by incorporating different vari-
ants of space discretisation [ 3,12,14], codebooks [ 49], and
9504
Points
 V oxels
<latexit sha1_base64="ydC2pm3VlnYPfSrK8+OGgiflbaY=">AAADEnicfVLLThsxFHWmL5i+AizZILKpKhTNVH3sKlQQ6gY1iAaQmCi6du4Mbjy2ZXtK0Ch/0T3b8gvsqm77A/2C/gaeNF04kXpXx+c+fO6DasGtS5Lfreje/QcPH62sxo+fPH32vL22fmJVZRj2mRLKnFGwKLjEvuNO4Jk2CCUVeErHe43/9Csay5X87K40DkooJM85A+epYXsjm9WoLy+4w+nx/sH7JB62O0k3mdnWMkjnoEPm1huutf5kI8WqEqVjAqw9TxPtBjUYx5nAaZxVFjWwMRR4XoK7EPnOlhV8hHZQH3LplfSMCsJq6zxAWfi+QocDWgkwk5AtYYwMhQhZyRnmBljI6iLXQjkb6sKJVqYRPfpSWUfVwge2ogx0M7SlYspIaFoPHZcGdM6LkKRUiVFI5ULBQurEOsPlQqrfk1juMNfhG51SYiY+jrN99NsweOgn/kmjAafMyzoDU5QwmfrtFNlOg/4XyOW/QI/i5jTSxUNYBievuunb7puj153dD/MjWSGbZJu8ICl5R3bJR9IjfcLIFbkm38lN9C26jX5EP/+GRq15zgYJLPp1BwrFBFM=</latexit>SDF>￿
<latexit sha1_base64="P/hWhUEb5ApMHet2+hcJif4LVbY=">AAADEXicfVLLbhMxFHWGVzu80nbZTUU2CFXRDCqwYVFBhdhUTQVpK3Wi6Nq5MzXx2JbtoYNG+Qr23cIvsENs+wV8Ab9RTwgLJxJ3dXzuw+c+qBbcuiT53Ylu3b5z997aenz/wcNHj7sbmydWVYbhkCmhzBkFi4JLHDruBJ5pg1BSgad0+rb1n35GY7mSH90XjaMSCslzzsB5atzdzOY1mssL7nD24eDd62Tc7SX9ZG47qyBdgB5Z2GC80fmTTRSrSpSOCbD2PE20GzVgHGcCZ3FWWdTAplDgeQnuQuS7O1bwCdpRc8ilFzIwKghrrPMAZeHbCh0OaCXA1CFbwhQZChGykjPMDbCQ1UWuhXI21IW1VqYVPflUWUfV0ge2ogx0O7OVYspIaFsPHZcGdM6LkKRUiUlI5ULBUmptneFyKdWvSax2mOvwjU4pMRcfx9kB+m0YPPQTP9JowCnzrMnAFCXUM7+dIttt0f8CufwX6FEc+9NIlw9hFZw876cv+y+O93r7bxZHska2yRPylKTkFdkn78mADAkjNbki38j36Gv0I/oZ/fobGnUWOVsksOj6Br5iBD0=</latexit>SDF<￿
SDF
<latexit sha1_base64="Wy5n8WXswBg1Z5+UQHJt2/8OBpg=">AAADI3icfVJNbxMxEHWWr7J8peXIJSIcClpFuxUfJ6SqRYJLRUCkrZRdRbPO7MbEa1u2t2wb9Q/wP7hzhb/ADXHhwJm/gbeEgxOJOT2/eWO/8UyuODM2jn92gkuXr1y9tnE9vHHz1u073c2tQyNrTXFEJZf6OAeDnAkcWWY5HiuNUOUcj/L5fps/OkFtmBTv7KnCrIJSsIJRsI6adB8U2+MmOo3OotTO0EKUqhnLHvae98ZvX+6lhpUVZJNuPx7EF9FbB8kS9MkyhpPNzu90KmldobCUgzHjJFY2W4C2jHI8D9PaoAI6hxLHFdgZL6Ke4WyKJlscMOGsDbX0ZAtjHUBRukb9hIW85qAbn61gjhQ591nBKBYaqM+qslBcWuP7wkZJ3Zqevq+NzeXKA6bOKaj2F9cuk1pA27qf+KBBFaz0yTyXfOpTBZewUtoYq5lYKXXD5+sdFso/o5WSX5gPw/QFumloPHA//lqhBiv1o0UK2g25OXfTKdOoRf8TMvFP6FAYutVIVhdhHRzuDJKngydvHvd395ZLskHukftkmyTkGdklr8iQjAglH8ln8oV8DT4F34LvwY+/0qCzrLlLvAh+/QGZlgoq</latexit>
f([x,y,z,θ,￿])=[RGBσ]
NeRF
<latexit sha1_base64="O8KnzplzFCLA50Z8gcYtTnjXBHU=">AAADJnicfVI9jxMxEHWWjzuWrxyUNBEpOFAU7SIOKqQTUNCcCOhyFykborEzu2fitS3bC0Gr/AT+Bz0t/AU6hOjo+Bt4Q65wIjHV85s39ozfUC24dUnyqxVduHjp8s7ulfjqtes3brb3bp1YVRmGQ6aEMiMKFgWXOHTcCRxpg1BSgad0/rzJn75HY7mSx+6jxkkJheQ5Z+A8NW3fy/fHdUbzzmj59k2vk71A4WBFDJaT+0/PU8fTdjfpJ6vobIN0DbpkHYPpXutPNlOsKlE6JsDacZpoN6nBOM4ELuOssqiBzaHAcQnuTOS9jhV8hnZSH3HpuxsYFchq6zxAWfhZw4QDWgkwi5AtYY4MhQhZyRnmBljI6iLXQjkb9oULrUzT9OxdZR1VGw/YijLQzUduXaaMhGb0MPHBgM55EZKUKjELqVwo2ChdWGe43Cj1/ovtCXMdntEpJVbNx7E32Lth8Mj/+CuNBpwyD+oMTFHCYundKbJeg/4n5PJc6FEc+9VINxdhG5w87KeP+wevH3UPn62XZJfcIXfJPknJE3JIXpIBGRJGPpEv5Cv5Fn2Ovkc/op//pFFrXXObBBH9/gtxmwuG</latexit>
f([XR,∆P])=XT
Zero-1-to-3
<latexit sha1_base64="DY4NUgjfkSeEggampKQe6xXr4S4=">AAADRXicfZJLj9MwEMfd8FrCqwtHLhW9LKiqkhUPCQlpBQhxKRS03a3UdKuxOwmmjm3ZDhRF+UZ8D+7cENw5cUNcISlFwi1iTn//ZsaeGQ/VglsXRZ9awanTZ86e2zkfXrh46fKV9u7VI6sKw3DElFBmTMGi4BJHjjuBY20QcirwmC4eNf7jN2gsV/LQvdM4zSGTPOUMXI1m7Sfp3qRMaNoZV7Myvv+sOnnZ66zA8N9gUJ0cTm8++CunIbN2N+pHK+tsi3gtumRtw9lu62syV6zIUTomwNpJHGk3LcE4zgRWYVJY1MAWkOEkB/dKpL2OFXyOdloOuKyLHxrlhZXW1QJlVo/CdzighQCz9GkOC2QohE8lZ5gaYD7VWaqFctavC5damabo+evCOqo2HrAFZaCbOW9dpoyEpnXf8daATnnmQ0qVmPsoFQo2UpfWGS43Uuv1ENsdpto/o1NKrIoPw+Qx1r9hcFBP/LlGA06ZW2UCJsthWdW/kyW9Rv0vkMs/gbUKw3o14s1F2BZH+/34bv/Oi9vdg4frJdkh18kNskdico8ckKdkSEaEkffkI/lMvgQfgm/B9+DH79Cgtc65RjwLfv4C0Y8W7g==</latexit>
f([XR
￿∶N,PR
￿∶N,PT
￿∶M])=XT
￿∶M
EscherNet
Figure 2. 3D representations overview. EscherNet generates a
set ofMtarget views XT
1:Mbased on their camera poses PT
1:M,
leveraging information gained from a set of Nreference views
XR
1:Nand their camera poses PR
1:N. EscherNet presents a new way
of learning implicit 3D representations by only considering the
relative camera transformation between the camera poses of PR
andPT, making it easier to scale with multi-view posed images,
independent of any speciﬁc coordinate systems.
encodings using hash tables [ 31] or Gaussians [ 18].
To enhance NeRF’s generalisation ability across diverse
scenes and in a few-shot setting, PixelNeRF [ 59] attempts
to learn a scene prior by jointly optimising multiple scenes,
but it is constrained by the high computational demands re-
quired by volumetric rendering. Various other approaches
have addressed this issue by introducing regularisation tech-
niques, such as incorporating low-level priors from local
patches [ 34], ensuring semantic consistency [ 16], consid-
ering adjacent ray frequency [ 57], and incorporating depth
signals [ 9]. In contrast, EscherNet encodes scenes directly
through the image space, enabling the learning of more gen-
eralised scene priors through large-scale datasets.
3D Diffusion Models The emergence of 2D generative
diffusion models has shown impressive capabilities in gen-
erating realistic objects and scenes [ 15,43]. This progress
has inspired the early design of text-to-3D diffusion mod-
els, such as DreamFusion [ 39] and Magic3D [ 19], by op-
timising a radiance ﬁeld guided by score distillation sam-
pling (SDS) from these pre-trained 2D diffusion models.
However, SDS necessitates computationally intensive itera-
tive optimisation, often requiring up to an hour for conver-
gence. Additionally, these methods, including recently pro-
posed image-to-3D generation approaches [ 8,27,56], fre-
quently yield unrealistic 3D generation results due to their
limited 3D understanding, giving rise to challenges such as
the multi-face Janus problem.
To integrate 3D priors more efﬁciently, an alterna-
tive approach involves training 3D generative models di-
rectly on 3D datasets, employing representations like point
clouds [ 32] or neural ﬁelds [ 4,11,17]. However, this de-sign depends on 3D operations, such as 3D convolution and
volumetric rendering, which are computationally expensive
and challenging to scale.
To address this issue, diffusion models trained on multi-
view posed data have emerged as a promising direction,
designed with no 3D operations. Zero-1-to-3 [ 21] stands
out as a pioneering work, learning view synthesis from
paired 2D posed images rendered from large-scale 3D ob-
ject datasets [ 6,7]. However, its capability is limited to
generating a single target view conditioned on a single ref-
erence view. Recent advancements in multi-view diffusion
models [ 20,22,23,44,45,58] focused on 3D generation
and can only generate a ﬁxed number of target views with
ﬁxed camera poses. In contrast, EscherNet can generate an
unrestricted number of target views with arbitrary camera
poses, offering superior ﬂexibility in view synthesis.
3. EscherNet
Problem Formulation and Notation In EscherNet, we
recast the view synthesis as a conditional generative mod-
elling problem, formulated as:
XT∼p(XT|XR,PR,PT). (1)
Here,XT={XT
1:M}andPT={PT
1:M}represent a set of
Mtarget views XT
1:Mwith their global camera poses PT
1:M.
Similarly, XR={XR
1:N}andPR={PR
1:N}represent
a set ofNreference views XR
1:Nwith their global camera
posesPR
1:N. BothNandMcan take on arbitrary values
during both model training and inference.
We propose a neural architecture design, such that the
generation of each target view XT
i∈ XTsolely depends
on its relative camera transformation to the reference views
(PR
j)−1PT
i,∀PR
j∈ PR, introduced next.
3.1. Architecture Design
We design EscherNet following two key principles: i) It
builds upon an existing 2D diffusion model, inheriting its
strong web-scale prior through large-scale training, and ii)
It encodes camera poses for each view/image, similar to
how language models encode token positions for each to-
ken. So our model can naturally handle an arbitrary number
of views for any-to-any view synthesis .
Multi-View Generation EscherNet can be seamlessly in-
tegrated with any 2D diffusion model with a transformer
architecture, with no additional learnable parameters . In
this work, we design EscherNet by adopting a latent
diffusion architecture, speciﬁcally StableDiffusion
v1.5 [43]. This choice enables straightforward compar-
isons with numerous 3D diffusion models that also leverage
the same backbone (more details in the experiment section).
To tailor the Stable Diffusion model, originally designed
for text-to-image generation, to multi-view generation as
9505
Lightweight
Vision EncoderProjection
Reshape
 [(B, N), H, W, C] -> [B, (N, HW), C]
Target View Reference View 
N views
B ObjectsN views N views M views
B ObjectsM views M viewsCross Attention 
Block with CaPEFeed-Forward  
Block
Residual 
Block × 2Self-Attention
Block with CaPEReference-to-Target
Consistency
Target-to-Target
Consistency
Reshape
[(B, M), H, W, C] -> [B, (M, HW), C]Dot-Product
Attention with CaPE
Linear Linear LinearLinear
Query Key ValueReshape
[B, (M, HW), C -> (B, M), H, W, C]
Key Query ValueMatMulScaleSoftMaxMatMul
CaPE for
Ref. PosesCaPE for
Tar. PosesStable Diffusion
U-Net LayerConvNeXt-V2
Tiny
Figure 3. EscherNet architecture details. EscherNet adopts the Stable Diffusion architectural design with minimal but important mod-
iﬁcations. The lightweight vision encoder captures both high-level and low-level signals from Nreference views. In U-Net, we apply
self-attention within Mtarget views to encourage target-to-target consistency, and cross-attention within Mtarget and Nreference views
(encoded by the image encoder) to encourage reference-to-target consistency. In each attention block, CaPE is employed for the key and
query, allowing the attention map to learn with relative camera poses, independent of speciﬁc coordinate systems.
applied in EscherNet, several key modiﬁcations are imple-
mented. In the original Stable Diffusion’s denoiser U-Net,
the self-attention block was employed to learn interactions
within different patches within the same image. In Esch-
erNet, we re-purpose this self-attention block to facilitate
learning interactions within distinct patches across Mdif-
ferent target views, thereby ensuring target-to-target consis-
tency. Likewise, the cross-attention block, originally used
to integrate textual information into image patches, is repur-
posed in EscherNet to learn interactions within Nreference
toMtarget views, ensuring reference-to-target consistency.
Conditioning Reference Views In view synthesis, it is
crucial that the conditioning signals accurately capture
both the high-level semantics and low-level texture details
present in the reference views. Previous works in 3D dif-
fusion models [ 21,22] have employed the strategy of en-
coding high-level signals through a frozen CLIP pre-trained
ViT [ 42] and encoding low-level signals by concatenating
the reference image into the input of the U-Net of Sta-
ble Diffusion. However, this design choice inherently con-
strains the model to handle only one single view.
In EscherNet, we choose to incorporate both high-level
and low-level signals in the conditioning image encoder,
representing reference views as sets of tokens. This de-
sign choice allows our model to maintain ﬂexibility in han-
dling a variable number of reference views. Early ex-
periments have conﬁrmed that using a frozen CLIP-ViT
alone may fail to capture low-level textures, preventing the
model from accurately reproducing the original reference
views given the same reference view poses as target poses.
While ﬁne-tuning the CLIP-ViT could address this issue,it poses challenges in terms of training efﬁciency. Instead,
we opt to ﬁne-tune a lightweight vision encoder, speciﬁ-
callyConvNeXtv2-Tiny [54], which is a highly efﬁcient
CNN architecture. This architecture is employed to com-
press our reference views to smaller resolution image fea-
tures. We treat these image features as conditioning tokens,
effectively representing each reference view. This conﬁgu-
ration has proven to be sufﬁcient in our experiments, deliv-
ering superior results in generation quality while simultane-
ously maintaining high training efﬁciency.
3.2. Camera Positional Encoding (CaPE)
To encode camera poses efﬁciently and accurately into ref-
erence and target view tokens within a transformer archi-
tecture, we introduce Camera Positional Encoding (CaPE),
drawing inspiration from recent advancements in the lan-
guage domain. We ﬁrst brieﬂy examine the distinctions be-
tween these two domains.
– In language, token positions (associated with each
word) follow a linear and discrete structure, and their length
can be inﬁnite . Language models are typically trained with
ﬁxed maximum token counts (known as context length),
and it remains an ongoing research challenge to construct
a positional encoding that enables the model to behave rea-
sonably beyond this ﬁxed context length [ 13,36].
– In 3D vision, token positions (associated with each
camera) follow a cyclic, continuous, and bounded struc-
ture for rotations and a linear, continuous, and unbounded
structure for translations. Importantly, unlike the language
domain where the token position always starts from zero,
there are no standardised absolute global camera poses in
9506
a 3D space. The relationship between two views depends
solely on their relative camera transformation.
We now present two distinct designs for spatial posi-
tion encoding, representing camera poses using 4 DoF for
object-centric rendering and 6 DoF for the generic case, re-
spectively. Our design strategy involves directly applying
a transformation on global camera poses embedded in the
token feature, which allows the dot-product attention to di-
rectly encode the relative camera transformation, indepen-
dent of any coordinate system.
4 DoF CaPE In the case of 4 DoF camera poses, we adopt
a spherical coordinate system, similar to [ 21,22], denoted
asP={α,β,γ,r}including azimuth, elevation, camera
orientation along the look-at direction, and camera distance
(radius), each position component is disentangled .
Mathematically, the position encoding function π(v,P),
characterised by its d-dimensional token feature v∈Rd
and pose P, should satisfy the following conditions:
⟨π(v1,θ1),π(v2,θ2)⟩=⟨π(v1,θ1−θ2),π(v2,0)⟩,(2)
⟨π(v1,r1),π(v2,r2)⟩=⟨π(v1,r1/r2),π(v2,1)⟩.(3)
Here⟨·,·⟩represents the dot product operation, θ1,2∈
{α,β,γ}, withinα,γ∈[0,2π),β∈[0,π), andr1,2>
0. Essentially, the relative 4 DoF camera transformation is
decomposed to the relative angle difference in rotation and
the relative scale difference in view radius.
Notably, Eq. 2aligns with the formula of rotary posi-
tion encoding (RoPE) [ 47] derived in the language domain.
Given that log(r1)−log(r2) = log(s·r1)−log(s·r2)(for
any scalar s >0), we may elegantly combine both Eq. 2
and Eq. 3in a uniﬁed formulation using the design strat-
egy in RoPE by transforming feature vector vwith a block
diagonal rotation matrix φ(P)encoding P.
–4 DoF CaPE :π(v,P) =φ(P)v,
φ(P) =
Ψ0···0
0Ψ0...
...0...0
0···0Ψ
,Ψ=
Ψα0···0
0Ψβ0...
...0Ψγ0
0···0Ψr
.(4)
Rotation: Ψθ=/bracketleftbiggcosθ−sinθ
sinθcosθ/bracketrightbigg
, (5)
View Radius: Ψr=/bracketleftbiggcos(f(r))−sin(f(r))
sin(f(r)) cos( f(r))/bracketrightbigg
, (6)
wheref(r) =πlogr−logrmin
logrmax−logrmin∈[0,π]. (7)
Here,dim(v) =dshould be divisible by 2|P|= 8.
Note, it’s crucial to apply Eq. 7to constrain logrwithin
the range of rotation [0,π], so we ensure the dot product
monotonically corresponds to its scale difference.
6 DoF CaPE In the case of 6 DoF camera poses, denoted
asP= [R t
01]∈SE(3), each position component is en-
tangled , implying that we are not able to reformulate as a
multi-dimensional position as in 4 DoF camera poses.Mathematically, the position encoding function π(v,P)
should now satisfy the following condition:
⟨π(v1,P1),π(v2,P2)⟩=/angbracketleftBig
π(v1,P−1
2P1),π(v2,I)/angbracketrightBig
. (8)
Let’s apply a similar strategy as used in 4 DoF CaPE,
which increases the dimensionality of P∈R4×4toφ(P)∈
Rd×dby reconstructing it as a block diagonal matrix, with
each diagonal element being P. Sinceφ(P)also forms a
real Lie group, we may construct π(·,·)for a key and query
using the following equivalence:
(φ(P−1
2P1)v1)⊺(φ(I)v2) = (v⊺
1φ(P⊺
1P−⊺
2))v2 (9)
= (v⊺
1φ(P⊺
1))(φ(P−⊺
2)v2) = (φ(P1)v1)⊺(φ(P−⊺
2)v2) (10)
=⟨π(v1,φ(P1)),π(v2,φ(P−⊺
2))⟩. (11)
–6 DoF CaPE :π(v,P) =φ(P)v,
φ(P) =
Ψ0···0
0Ψ0...
...0...0
0···0Ψ
,Ψ=/braceleftBigg
P if key
P−⊺if query. (12)
Here,dim(v) =dshould be divisible by dim(P) = 4 .
Similarly, we need to re-scale the translation tfor each
scene within a unit range for efﬁcient model training. It’s
worth noting that 6 DoF CaPE is concurrently explored in
[30], with a focus on scene-level representations.
In both 4 and 6 DoF CaPE implementation, we can efﬁ-
ciently perform matrix multiplication by simply reshaping
the vector vto match the dimensions of Ψ(8 for 4 DoF,
4 for 6 DoF), ensuring faster computation. The PyTorch
implementation is attached in Appendix A.
4. Experiments
Training Datasets In this work, we focus on object-
centric view synthesis, training our model on Objaverse-1.0
which consists of 800K objects [ 7]. This setting allows us to
fairly compare with all other 3D diffusion model baselines
trained on the same dataset. We adopt the same training data
used in Zero-1-to-3 [ 21], which contains 12 randomly ren-
dered views per object with randomised environment light-
ing. To ensure the data quality, we ﬁlter out empty rendered
images, which make up roughly 1% of the training data.
We trained and reported results using EscherNet with
both 4 DoF and 6 DoF CaPE. Our observations revealed
that 6 DoF CaPE exhibits a slightly improved performance,
which we attribute to its more compressed representation
space. However, empirically, we found that 4 DoF CaPE
yields visually more consistent results when applied to real-
world images. Considering that the training data is conﬁned
within a 4 DoF object-centric setting, we present EscherNet
with 4 DoF CaPE in the main paper. The results obtained
with 6 DoF CaPE are provided in Appendix C.
In all experiments, we re-evaluate the baseline models by
using their ofﬁcially open-sourced checkpoints on the same
set of reference views for a fair comparison. Our experi-
ment settings are provided in Appendix B.
9507
4.1. Results on Novel View Synthesis
We evaluate EscherNet in novel view synthesis on the
Google Scanned Objects dataset (GSO) [ 10] and the RTMV
dataset [ 50], comparing with 3D diffusion models for view
synthesis, such as Zero-1-to-3 [ 21] and RealFusion [ 27]
(primarily for generation quality with minimal reference
views). Additionally, we also evaluate on NeRF Synthetic
Dataset [ 29], comparing with state-of-the-art scene-speciﬁc
neural rendering methods, such as InstantNGP [ 31] and 3D
Gaussian Splatting [ 18] (primarily for rendering accuracy
with multiple reference views).
Notably, many other 3D diffusion models [ 20,22,23,
44,58] prioritise 3D generation rather than view synthesis.
This limitation conﬁnes them to predicting target views with
ﬁxed target poses , making them not directly comparable.
Compared to 3D Diffusion Models In Tab. 1and Fig. 5,
we show that EscherNet signiﬁcantly outperforms 3D dif-
fusion baselines, by a large margin, both quantitatively and
qualitatively. Particularly, we outperform Zero-1-to-3-XL
despite it being trained on ×10more training data, and Re-
alFusion despite it requiring expensive score distillation for
iterative scene optimisation [ 39]. It’s worth highlighting
that Zero-1-to-3 by design is inherently limited to generat-
ing a single target view and cannot ensure self-consistency
across multiple target views, while EscherNet can generate
multiple consistent target views jointly and provides more
precise camera control.
Training
Data# Ref.
ViewsGSO-30 RTMV
PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓
RealFusion - 1 12.76 0.758 0.382 - - -
Zero123 800K 1 18.51 0.856 0.127 10.16 0.505 0.418
Zero123-XL 10M 1 18.93 0.856 0.124 10.59 0.520 0.401
EscherNet 800k 1 20.24 0.884 0.095 10.56 0.518 0.410
EscherNet 800k 2 22.91 0.908 0.064 12.66 0.585 0.301
EscherNet 800k 3 24.09 0.918 0.052 13.59 0.611 0.258
EscherNet 800k 5 25.09 0.927 0.043 14.52 0.633 0.222
EscherNet 800k 10 25.90 0.935 0.036 15.55 0.657 0.185
Table 1. Novel view synthesis performance on GSO and RTMV
datasets. EscherNet outperforms Zero-1-to-3-XL with signiﬁ-
cantly less training data and RealFusion without extra SDS opti-
misation. Additionally, EscherNet’s performance exhibits further
improvement with the inclusion of more reference views.
Compared to Neural Rendering Methods In Tab. 2and
Fig.4, we show that EscherNet again offers plausible view
synthesis in a zero-shot manner, without scene-speciﬁc op-
timisation required by both InstantNGP and 3D Gaussian
Splatting. Notably, EscherNet leverages a generalised un-
derstanding of objects acquired through large-scale train-
ing, allowing it to interpret given views both semantically
and spatially, even when conditioned on a limited number
of reference views. However, with an increase in the num-
ber of reference views, both InstantNGP and 3D Gaussian# Reference Views (Less →More)
1 2 3 5 10 20 50 100
InstantNGP (Scene Speciﬁc Training)
PSNR↑10.92 12.42 14.27 18.17 22.96 24.99 26.86 27.30
SSIM↑0.449 0.521 0.618 0.761 0.881 0.917 0.946 0.953
LPIPS↓0.627 0.499 0.391 0.228 0.091 0.058 0.034 0.031
GaussianSplatting (Scene Speciﬁc Training)
PSNR↑9.44 10.78 12.87 17.09 23.04 25.34 26.98 27.11
SSIM↑0.391 0.432 0.546 0.732 0.876 0.919 0.942 0.944
LPIPS↓0.610 0.541 0.441 0.243 0.085 0.054 0.041 0.041
EscherNet (Zero Shot Inference)
PSNR↑13.36 14.95 16.19 17.16 17.74 17.91 18.05 18.15
SSIM↑0.659 0.700 0.729 0.748 0.761 0.765 0.769 0.771
LPIPS↓0.291 0.208 0.161 0.127 0.114 0.106 0.099 0.097
Table 2. Novel view synthesis performance on NeRF Synthetic
dataset. EscherNet outperforms both InstantNGP and Gaussian
Splatting when provided with fewer than ﬁve reference views
while requiring no scene-speciﬁc optimisation. However, as the
number of reference views increases, both methods show a more
signiﬁcant improvement in rendering quality.
Splatting exhibit a signiﬁcant improvement in the render-
ing quality. To achieve a photo-realistic neural rendering
while retaining the advantages of a generative formulation
remains an important research challenge.
# Reference Views (Less →More)
1 2 3 5 10 20
InstantNGP (Scene Speciﬁc Training)
PSNR 10.37
 PSNR 11.72
 PSNR 12.82
 PSNR 15.58
 PSNR 19.71
 PSNR 21.28
3D Gaussian Splatting (Scene Speciﬁc Training)
PSNR 9.14
 PSNR 10.63
 PSNR 11.43
 PSNR 14.81
 PSNR 20.15
 PSNR 22.88
EscherNet (Zero Shot Inference)
PSNR 10.10
 PSNR 13.25
 PSNR 13.43
 PSNR 14.33
 PSNR 14.97
 PSNR 15.65
Figure 4. Generated views visualisation on the NeRF Synthetic
drum scene. EscherNet generates plausible view synthesis even
when provided with very limited reference views, while neural
rendering methods fail to generate any meaningful content. How-
ever, when we have more than 10 reference views, scene-speciﬁc
methods exhibit a substantial improvement in rendering quality.
We report the mean PSNR averaged across all test views from the
drum scene. Results for other scenes and/or with more reference
views are shown in Appendix D.
9508
Reference
Views
1 View 2 Views 5 Views
 1 View 2 Views 5 Views
 1 View 2 Views 5 Views
 1 View 2 Views 5 Views
Zero-1-to-3-XL
[1 View]
EscherNet
[1 View]
EscherNet
[2 Views]
EscherNet
[5 Views]
Ground
Truth
Figure 5. Novel view synthesis visualisation on GSO and RTMV datasets. EscherNet outperforms Zero-1-to-3-XL, delivering superior
generation quality and ﬁner camera control. Notably, when conditioned with additional views, EscherNet exhibits an enhanced resemblance
of the generated views to ground-truth textures, revealing more reﬁned texture details such as in the backpack straps and turtle shell.
4.2. Results on 3D Generation
In this section, we perform single/few-image 3D generation
on the GSO dataset. We compare with SoTA 3D gener-
ation baselines: Point-E [ 32] for direct point cloud genera-
tion, Shape-E [ 17] for direct NeRF generation, DreamGaus-
sian [ 17] for optimising 3D Gaussian [ 18] with SDS guid-
ance, One-2-3-45 [ 20] for decoding an SDF using multiple
views predicted from Zero-1-to-3, and SyncDreamer [ 22]
for ﬁtting an SDF using NeuS [ 52] from 16 consistent ﬁxed
generated views. We additionally include NeuS trained on
reference views for few-image 3D reconstruction baselines.
Given any reference views, EscherNet can generate mul-
tiple 3D consistent views, allowing for the straightforward
adoption with NeuS [ 52] for 3D reconstruction. We gen-
erate 36 ﬁxed views, varying the azimuth from 0◦to 360◦
with a rendering every 30◦at a set of elevations (-30◦, 0◦,
30◦), which serve as inputs for our NeuS reconstruction.
Results In Tab. 3and Fig. 6, we show that EscherNet
stands out by achieving signiﬁcantly superior 3D recon-
struction quality compared to other image-to-3D generative
models. Speciﬁcally, EscherNet demonstrates an approx-
imate 25% improvement in Chamfer distance over Sync-
Dreamer, considered as the current best model, when con-
ditioned on a single reference view, and a 60% improvement
when conditioned on 10 reference views. This impressive
performance is attributed to EscherNet’s ability to ﬂexibly# Ref. Views Chamfer Dist. ↓ V olume IoU ↑
Point-E 1 0.0447 0.2503
Shape-E 1 0.0448 0.3762
One2345 1 0.0632 0.4209
One2345-XL 1 0.0667 0.4016
DreamGaussian 1 0.0605 0.3757
DreamGaussian-XL 1 0.0459 0.4531
SyncDreamer 1 0.0400 0.5220
NeuS 3 0.0366 0.5352
NeuS 5 0.0245 0.6742
NeuS 10 0.0195 0.7264
EscherNet 1 0.0314 0.5974
EscherNet 2 0.0215 0.6868
EscherNet 3 0.0190 0.7189
EscherNet 5 0.0175 0.7423
EscherNet 10 0.0167 0.7478
Table 3. 3D reconstruction performance on GSO. EscherNet
outperforms all other image-to-3D baselines in generating more
visually appealing with accurate 3D geometry, particularly when
conditioned on multiple reference views.
handle any number of reference and target views, providing
comprehensive and accurate constraints for 3D geometry.
In contrast, SyncDreamer faces challenges due to sensitivity
to elevation angles and constraints imposed by a ﬁxed 30◦
elevation angle by design, thus hindering learning a holistic
representation of complex objects. This limitation results in
degraded reconstruction, particularly evident in the lower
regions of the generated geometry.
4.3. Results on Text­to­3D Generation
EscherNet’s ﬂexibility in accommodating any number of
reference views enables a straightforward approach to the
9509
Reference One-2-3-45-XL DreamGaussian-XL SyncDreamer EscherNet Ground-Truth
Figure 6. Single view 3D reconstruction visualisation on GSO. EscherNet’s ability to generate dense and consistent novel views signif-
icantly improves the reconstruction of complete and well-constrained 3D geometry. In contrast, One-2-3-45-XL and DreamGaussian-XL,
despite leveraging a signiﬁcantly larger pre-trained model, tend to produce over-smoothed and noisy reconstructions; SyncDreamer, con-
strained by sparse ﬁxed-view synthesis, struggles to tightly constrain geometry, particularly in areas in sofa and the bottom part of the bell.
A bald eagle carved
out of wood. ⇒
A robot made of vegetables, 4K. ⇒
Figure 7. Text-to-3D visualisation with MVDream (up) and
SDXL (bottom). EscherNet offers compelling and realistic view
synthesis for synthetic images generated with user-provided text
prompts. Additional results are shown in Appendix E.text-to-3D generation problem by breaking it down into two
stages: text-to-image, relying on any off-the-shelf text-to-
image generative model, and then image-to-3D, relying on
EscherNet. In Fig. 7, we present visual results of dense
novel view generation using a text-to-4view model with
MVDream [ 45] and a text-to-image model with SDXL [ 38].
Remarkably, even when dealing with out-of-distribution
and counterfactual content, EscherNet generates consistent
3D novel views with appealing textures.
5. Conclusions
In this paper, we have introduced EscherNet, a multi-view
conditioned diffusion model designed for scalable view
synthesis. Leveraging Stable Diffusion’s 2D architecture
empowered by the innovative Camera Positional Embed-
ding (CaPE), EscherNet adeptly learns implicit 3D repre-
sentations from varying number of reference views, achiev-
ing consistent 3D novel view synthesis. We provide detailed
discussions and additional ablative analysis in Appendix F.
Limitations and Discussions EscherNet’s ﬂexibility in
handling any number of reference views allows for au-
toregressive generation, similar to autoregressive language
models [ 1,5]. While this approach signiﬁcantly reduces in-
ference time, it leads to a degraded generation quality. Ad-
ditionally, EscherNet’s current capability operates within a
3 DoF setting constrained by its training dataset, which may
not align with real-world scenarios, where views typically
span inSE(3)space. Future work will explore scaling Es-
cherNet with 6 DoF training data with real-world scenes,
striving for a more general 3D representation.
9510
References
[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in Neural In-
formation Processing Systems (NeurIPS) , 2020. 8
[2] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,
Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,
Manolis Savva, Shuran Song, Hao Su, et al. Shapenet:
An information-rich 3d model repository. arXiv preprint
arXiv:1512.03012 , 2015. 2
[3] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and
Hao Su. Tensorf: Tensorial radiance ﬁelds. In Proceedings
of the European Conference on Computer Vision (ECCV) ,
2022. 2
[4] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen
Tu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf: A
uniﬁed approach to 3d generation and reconstruction. arXiv
preprint arXiv:2304.06714 , 2023. 3
[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al. Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311 , 2022. 8
[6] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong
Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-
tian Laforte, Vikram V oleti, Samir Yitzhak Gadre, et al.
Objaverse-xl: A universe of 10m+ 3d objects. arXiv preprint
arXiv:2307.05663 , 2023. 3,17
[7] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,
Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana
Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:
A universe of annotated 3d objects. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , 2023. 3,5,17
[8] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan,
Yin Zhou, Leonidas Guibas, Dragomir Anguelov, et al.
Nerdi: Single-view nerf synthesis with language-guided dif-
fusion as general image priors. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR) , 2023. 3
[9] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra-
manan. Depth-supervised nerf: Fewer views and faster train-
ing for free. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2022. 3
[10] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kin-
man, Ryan Hickman, Krista Reymann, Thomas B McHugh,
and Vincent Vanhoucke. Google scanned objects: A high-
quality dataset of 3d scanned household items. In Proceed-
ings of the IEEE International Conference on Robotics and
Automation (ICRA) . IEEE, 2022. 6,13
[11] Ziya Erkoc ¸, Fangchang Ma, Qi Shan, Matthias Nießner,
and Angela Dai. Hyperdiffusion: Generating implicit
neural ﬁelds with weight-space diffusion. arXiv preprint
arXiv:2303.17015 , 2023. 3
[12] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie
Shotton, and Julien Valentin. Fastnerf: High-ﬁdelity neu-ral rendering at 200fps. In Proceedings of the International
Conference on Computer Vision (ICCV) , 2021. 2
[13] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji,
and Sinong Wang. Lm-inﬁnite: Simple on-the-ﬂy length
generalization for large language models. arXiv preprint
arXiv:2308.16137 , 2023. 4
[14] Peter Hedman, Pratul P Srinivasan, Ben Mildenhall,
Jonathan T Barron, and Paul Debevec. Baking neural ra-
diance ﬁelds for real-time view synthesis. In Proceedings
of the International Conference on Computer Vision (ICCV) ,
2021. 2
[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems (NeurIPS) , 2020. 3
[16] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf
on a diet: Semantically consistent few-shot view synthesis.
InProceedings of the International Conference on Computer
Vision (ICCV) , 2021. 3
[17] Heewoo Jun and Alex Nichol. Shap-e: Generat-
ing conditional 3d implicit functions. arXiv preprint
arXiv:2305.02463 , 2023. 3,7,13
[18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk ¨uhler,
and George Drettakis. 3d gaussian splatting for real-time
radiance ﬁeld rendering. ACM Transactions on Graphics
(TOG) , 2023. 2,3,6,7,13
[19] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,
Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution
text-to-3d content creation. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR) , 2023. 3
[20] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang
Xu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh
in 45 seconds without per-shape optimization. arXiv preprint
arXiv:2306.16928 , 2023. 2,3,6,7
[21] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-
makov, Sergey Zakharov, and Carl V ondrick. Zero-1-to-3:
Zero-shot one image to 3d object. In Proceedings of the In-
ternational Conference on Computer Vision (ICCV) , 2023.
3,4,5,6,13
[22] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie
Liu, Taku Komura, and Wenping Wang. Syncdreamer: Gen-
erating multiview-consistent images from a single-view im-
age. arXiv preprint arXiv:2309.03453 , 2023. 3,4,5,6,7,
13,17
[23] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,
Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,
Marc Habermann, Christian Theobalt, et al. Wonder3d: Sin-
gle image to 3d using cross-domain diffusion. arXiv preprint
arXiv:2310.15008 , 2023. 2,3,6,17
[24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. In Proceedings of the International Confer-
ence on Learning Representations (ICLR) , 2019. 13
[25] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher
Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting
using denoising diffusion probabilistic models. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , 2022. 2
9511
[26] Daniel Maturana and Sebastian Scherer. V oxnet: A 3d con-
volutional neural network for real-time object recognition.
InProceedings of the IEEE/RSJ Conference on Intelligent
Robots and Systems (IROS) , 2015. 2
[27] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and
Andrea Vedaldi. Realfusion: 360deg reconstruction of any
object from a single image. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR) , 2023. 3,6
[28] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
bastian Nowozin, and Andreas Geiger. Occupancy networks:
Learning 3d reconstruction in function space. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , 2019. 2
[29] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance ﬁelds for view syn-
thesis. In Proceedings of the European Conference on Com-
puter Vision (ECCV) , 2020. 2,6,13
[30] Takeru Miyato, Bernhard Jaeger, Max Welling, and Andreas
Geiger. Gta: A geometry-aware attention mechanism for
multi-view transformers. In International Conference on
Learning Representations (ICLR) , 2024. 5
[31] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a mul-
tiresolution hash encoding. ACM Transactions on Graphics
(ToG) , 2022. 2,3,6,13
[32] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela
Mishkin, and Mark Chen. Point-e: A system for generat-
ing 3d point clouds from complex prompts. arXiv preprint
arXiv:2212.08751 , 2022. 3,7,13
[33] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and
Andreas Geiger. Differentiable volumetric rendering: Learn-
ing implicit 3d representations without 3d supervision. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) , 2020. 2
[34] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall,
Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Reg-
nerf: Regularizing neural radiance ﬁelds for view synthesis
from sparse inputs. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) , 2022.
3
[35] Jeong Joon Park, Peter Florence, Julian Straub, Richard
Newcombe, and Steven Lovegrove. DeepSDF: Learning
continuous signed distance functions for shape representa-
tion. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , 2019. 2
[36] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico
Shippole. Yarn: Efﬁcient context window extension of large
language models. arXiv preprint arXiv:2309.00071 , 2023. 4
[37] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc
Pollefeys, and Andreas Geiger. Convolutional occupancy
networks. In Proceedings of the European Conference on
Computer Vision (ECCV) , 2020. 2
[38] Dustin Podell, Zion English, Kyle Lacey, Andreas
Blattmann, Tim Dockhorn, Jonas M ¨uller, Joe Penna, and
Robin Rombach. Sdxl: Improving latent diffusion mod-els for high-resolution image synthesis. arXiv preprint
arXiv:2307.01952 , 2023. 8
[39] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv
preprint arXiv:2209.14988 , 2022. 3,6
[40] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classiﬁcation
and segmentation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) , 2017.
2
[41] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. Advances in Neural Information
Processing Systems (NeurIPS) , 2017. 2
[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In Proceedings of the International Conference on
Machine Learning (ICML) , 2021. 4
[43] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , 2022. 3
[44] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu,
Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao
Su. Zero123++: a single image to consistent multi-view dif-
fusion base model. arXiv preprint arXiv:2310.15110 , 2023.
2,3,6
[45] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,
and Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-
eration. arXiv preprint arXiv:2308.16512 , 2023. 3,8
[46] Vincent Sitzmann, Michael Zollh ¨ofer, and Gordon Wet-
zstein. Scene representation networks: Continuous 3d-
structure-aware neural scene representations. Advances in
Neural Information Processing Systems (NeurIPS) , 2019. 2
[47] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,
Bo Wen, and Yunfeng Liu. Roformer: Enhanced trans-
former with rotary position embedding. arXiv preprint
arXiv:2104.09864 , 2021. 5
[48] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel
grid optimization: Super-fast convergence for radiance ﬁelds
reconstruction. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) , 2022. 2
[49] Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas
M¨uller, Morgan McGuire, Alec Jacobson, and Sanja Fidler.
Variable bitrate neural ﬁelds. In Proceedings of SIGGRAPH ,
2022. 2
[50] Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan
Kautz, Alexander Keller, Sameh Khamis, Thomas M ¨uller,
Charles Loop, Nathan Morrical, Koki Nagano, et al. Rtmv:
A ray-traced multi-view synthetic dataset for novel view syn-
thesis. arXiv preprint arXiv:2205.07058 , 2022. 6,13
[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in Neural
Information Processing Systems (NeurIPS) , 2017. 2
9512
[52] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. Neus: Learning neural implicit
surfaces by volume rendering for multi-view reconstruction.
arXiv preprint arXiv:2106.10689 , 2021. 7
[53] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE Transactions on Image Process-
ing, 2004. 13
[54] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei
Chen, Zhuang Liu, In So Kweon, and Saining Xie. Con-
vnext v2: Co-designing and scaling convnets with masked
autoencoders. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) , 2023. 4
[55] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-
guang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d
shapenets: A deep representation for volumetric shapes. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) , 2015. 2
[56] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,
and Zhangyang Wang. Neurallift-360: Lifting an in-the-wild
2d photo to a 3d object with 360deg views. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , 2023. 3
[57] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Im-
proving few-shot neural rendering with free frequency regu-
larization. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2023. 3
[58] Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, and Heng
Wang. Consistent-1-to-3: Consistent image to 3d view syn-
thesis via geometry-aware diffusion models. arXiv preprint
arXiv:2310.03020 , 2023. 3,6
[59] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.
pixelnerf: Neural radiance ﬁelds from one or few images.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) , 2021. 3
[60] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and
Thomas S Huang. Generative image inpainting with con-
textual attention. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 5505–5514,
2018. 2
[61] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , 2018. 13
9513
