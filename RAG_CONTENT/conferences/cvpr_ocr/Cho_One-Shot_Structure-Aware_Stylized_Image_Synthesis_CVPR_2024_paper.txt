One-Shot Structure-Aware Stylized Image Synthesis
Hansam Cho1,2∗, Jonghyun Lee1,2∗, Seunggyu Chang1, Yonghyun Jeong1†
1NA VER Cloud,2School of Industrial and Management Engineering, Korea University
{chosam95, tomtom1103}@korea.ac.kr,
{seunggyu.chang, yonghyun.jeong}@navercorp.com
Input OSASIS (Ours) MTG JoJoGAN Style DiffuseIT InST
(b) One -shot image stylization with OOD reference image
(a) One -shot image stylization
Figure 1. OSASIS is able to (a) stylize an input image with a single reference image while robustly preserving the structure and content of
the input image. It is also able to (b) incorporate out-of-domain (OOD) data as the reference image while other baseline methods fail
Abstract
While GAN-based models have been successful in image
stylization tasks, they often struggle with structure preser-
vation while stylizing a wide range of input images. Re-
cently, diffusion models have been adopted for image styl-
ization but still lack the capability to maintain the original
quality of input images. Building on this, we propose OS-
ASIS: a novel one-shot stylization method that is robust in
structure preservation. We show that OSASIS is able to ef-
fectively disentangle the semantics from the structure of an
image, allowing it to control the level of content and style
implemented to a given input. We apply OSASIS to var-
ious experimental settings, including stylization with out-
of-domain reference images and stylization with text-driven
manipulation. Results show that OSASIS outperforms other
stylization methods, especially for input images that were
rarely encountered during training, providing a promising
solution to stylization via diffusion models. The source code
can be found at https://github.com/hansam95/OSASIS.
*Work done during an internship at NA VER Cloud.
†Corresponding author.1. Introduction
In the literature of generative models, image stylization
refers to training a model in order to transfer the style of
a reference image to various input images during infer-
ence [21, 22,28]. However, collecting a sufficient num-
ber of images that share a particular style for training
can be difficult. Consequently, one-shot stylization has
emerged as a viable and practical solution, with genera-
tive adversarial networks (GANs) showing promising re-
sults [2, 16,36,38,39].
Despite significant advancements in GAN-based styliza-
tion techniques, the accurate preservation of an input im-
age’s structure continues to pose a significant challenge.
This difficulty is particularly pronounced for input images
that contain elements infrequently encountered during train-
ing, often characterized by complex structural nuances that
diverge from those observed in more commonly presented
images. Figure 1(a) illustrates this challenge, where entities
such as hands and microphones, when processed through
GAN-based stylization, diverge considerably from their
original structural integrity. In addition, GAN-based styl-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
8302
ization methods often fail to accurately separate the struc-
ture and style of the reference image during inference. As
shown in Figure 1(b), the lack of disentanglement results in
structural artifacts from reference images bleeding into the
stylized image.
Recently, diffusion models (DMs) have shown remark-
able performance in various image-related tasks, includ-
ing high-fidelity image generation [25, 26], super resolution
[27], and text-driven image manipulation [13, 15,17]. For
image stylization, several studies, including DiffuseIT [15]
and InST [37], have been proposed. However, they primar-
ily focus on developing a diffusion model framework tai-
lored to the stylization task. In contrast, our work priori-
tizes preserving the structure of the input image over solely
introducing an appropriate diffusion model for stylization.
As illustrated in Figure 1(a), it can be seen that the capabil-
ity to preserve structure doesn’t stem from diffusion models
itself, but from our methodology.
In this study, we propose One-shot Structure-Aware
Stylized Image Synthesis (OSASIS), which effectively dis-
entangles the structure and transferable semantics of a style
image within the structure of a diffusion model. OSASIS
selects an appropriate encoding timestep of a structural la-
tent code to control the strength of structure preservation
and enhances its preservation ability through a structure-
preserving network. To acquire a semantically meaningful
latent, we utilize the semantic encoder proposed in diffu-
sion autoencoders (DiffAE) [23]. Following the approach
of mind the gap (MTG) [39], we bridge the domain gap by
finetuning a pretrained DM using a combination of direc-
tional CLIP losses. Once trained, we find that by properly
conditioning the semantic latent code, our method achieves
structure-aware image stylization.
We conduct qualitative and quantitative experiments on
a wide range of input and style images. By quantitatively
extracting data with rare structural elements from the train-
ing set (i.e. low-density images), we show that OSASIS is
robust in structure preservation, outperforming other meth-
ods. In addition, we directly optimize the semantic latent
code for text-driven manipulation. Combining the opti-
mized latent with the finetuned DM, OSASIS is able to gen-
erate stylized images with manipulated attributes.
2. Background
2.1. Diffusion Models
Diffusion models are latent variable models that are trained
to reverse a forward process[8]. The forward process, which
is defined as a Markov chain with a Gaussian transition de-
fined in Eq.1, involves iteratively mapping an image to a
predefined prior N(0,I)overTsteps. DDPM[8] proposes
to parameterize the reverse process defined in Eq. 2with a
noise prediction network ϵθ(xt, t), which is trained with theloss function Lsimple in Eq.3.
q(xt|xt−1) =N(xt;p
1−βtxt−1, βtI) (1)
pθ(xt−1|xt) =N(xt−1;µθ(xt, t), σtI) (2)
where µθ(xt, t) =1√1−βt(xt−βt√1−αtϵθ(xt, t))
Lsimple =Et,x0,ϵh
∥ϵθ(xt, t)−ϵ)∥2i
, ϵ∼ N(0,I) (3)
In contrast to DDPM, DDIM [29] defines the forward
process as non-Markovian and derives the corresponding
reverse process as Eq. 5, in which fθ(xt, t)is the model’s
prediction of x0. DDIM also introduces an image encoding
method by deriving ordinary differential equations (ODEs)
corresponding with the reverse process. By reversing the
ODE, DDIM introduces an image encoding process, de-
fined as Eq. 4.
xt+1=√αt+1fθ(xt, t) +p
1−αt+1ϵθ(xt, t) (4)
xt−1=√αt−1fθ(xt, t) +p
1−αt−1ϵθ(xt, t) (5)
where fθ(xt, t) =xt−√1−αtϵθ(xt, t)√αt
For our work, we adopt specific terminologies to refer
to the forward and reverse process of DDPM, in which we
call forward DDPM and reverse DDPM, respectively. Sim-
ilarly, the denoising reverse process of DDIM is referred to
as reverse DDIM, while the deterministic image encoding
process of DDIM is referred to as forward DDIM.
2.2. Diffusion Autoencoders
DiffAE [23] proposes a semantic encoder Encϕthat en-
codes a given image x0to a semantically rich latent variable
zsem, represented as:
zsem= Enc ϕ(x0). (6)
This latent variable has been shown to be linear, decod-
able, and semantically meaningful, hence being an attrac-
tive property that our model seeks to leverage. Similar to
the aforementioned forward DDIM, DiffAE is also able to
encode an image given zsemto a fully reconstructable latent
xTby Eq. 7, in which we denote forward DiffAE. Corre-
spondingly, the forward DiffAE encoded latent xTcan be
decoded by conditioning itself and zsemto the reverse pro-
cess defined as Eq. 8, referred to as reverse DiffAE.
xt+1=√αt+1fθ(xt, t,zsem) +p
1−αt+1ϵθ(xt, t,zsem)
(7)
xt−1=√αt−1fθ(xt, t,zsem) +p
1−αt−1ϵθ(xt, t,zsem)
(8)
where fθ(xt, t,zsem) =xt−√1−αtϵθ(xt, t,zsem)√αt
8303
𝐼𝐴𝑠𝑡 𝑦𝑙 𝑒 
Se ma ntic 
E nc o der 
𝑧𝑠 𝑒 𝑚 𝑠𝑡 𝑦𝑙 𝑒 
O S A SI S 𝐿𝑐 𝑟 𝑜 𝑠 𝑠 
𝐿𝑖 𝑛 C LI P 
S P N 
D DI M 
(𝜖𝜃𝐵)D DI M 
(𝜖𝜃𝐴)
Freeze M o del O S A SI S D DI M 
(𝜖𝜃𝐴)
D o mai n A ( p h ot orealistic) 
D o mai n B (st ylize d) O S A SI S 𝐼𝐴𝑖 𝑛 
Se ma ntic 
E nc o der 
𝑧𝑠 𝑒 𝑚 𝑖 𝑛 
𝑥𝑡0𝑠𝑡 𝑦𝑙 𝑒 𝑥𝑡0𝑖 𝑛 
𝐿𝑟 𝑒 𝑐 𝑜 𝑛 𝐼𝐵𝑠𝑡 𝑦𝑙 𝑒 መ𝐼𝐵𝑠𝑡 𝑦𝑙 𝑒 𝐼𝐵𝑖 𝑛 𝐼𝐴𝑖 𝑛 𝑥𝑡
+
𝑧𝑠 𝑒 𝑚 𝑖 𝑛 
𝑥𝑡−1
Fi g ure 2. O ver vie w of O S A SI S. D uri n g ﬁ net u ni n g, cr oss- d o mai n l oss c o m pares t he p h ot orealistic i ma ge ( b o u n de d yell o w) t o its st ylize d 
c o u nter parts ( b o u n de d gree n). C o nc urre ntl y, t he i n- d o mai n l oss ga u ges t he ali g n me nt of directi o nal s hifts wit hi n t he sa me d o mai n, w hic h 
are deli neate d b y yell o w a n d gree n. Rec o nstr ucti o n l oss c o m pares t he ori gi nal st yle i ma ge wit h a rec o nstr ucte d c o u nter part. I nt uiti vel y, 
t he c o m bi nati o n of t he directi o nal l osses g uara ntees t hat f or eac h iterati o n t he ge nerate d Ii n 
Bis p ositi o ne d f or pr ojecti o n vect ors fr o m Is t yl e 
B
a n d Ii n 
At o be c olli near t o its cr oss- d o mai n a n d i n- d o mai n c o u nter parts i n t he C LI P s pace. 
3. Met h o ds 
O ur a p pr oac h ai ms t o ac hie ve effecti ve st ylizati o n b y i ni- 
tiall y dise nta n gli n g t he str uct ural a n d se ma ntic i nf or mati o n 
of i ma ges. We de ﬁ ne str uct ural i nf or mati o n as t he o verall 
o utli ne of a n i ma ge, w hereas we f urt her dec o nstr uct t he se- 
ma ntics of a n i ma ge i nt o a c o m bi nati o n of c o nte nt a n d st yle. 
To i nitiall y dise nta n gle t he se ma ntics fr o m t he str uct ure, we 
e m pl o y t w o disti nct late nt c o des: t he str uct ural late nt c o de 
xt0a n d t he se ma ntic late nt c o de zs e m . We ﬁ net u ne a pre- 
trai ne d D DI M ϵA
θc o n diti o ne d o n t he se ma ntic late nt c o de 
zs e m via C LI P directi o nal l osses i n or der t o bri d ge t he d o- 
mai n ga p bet wee n t he i n p ut a n d st yle i ma ges. O nce ﬁ ne- 
t u ne d, we c o ntr ol t he a m o u nt of l o w-le vel vis ual feat ures 
(e. g . te xt ure a n d c ol or), referre d t o as st yle, a n d hi g h-le vel 
vis ual feat ures ( e. g . o bject a n d i de ntit y), referre d t o as c o n- 
te nt d uri n g i nfere nce. Pr o per c o n diti o ni n g of zs e m t o t he 
ﬁ net u ne d D DI M all o ws us t o ac hie ve t his c o ntr ol, effec- 
ti vel y perf or mi n g st ylizati o n. F urt her m ore, we directl y o p- 
ti mize t he se ma ntic late nt c o de zs e m f or te xt- dri ve n ma ni p- 
ulati o n. B y c o m bi ni n g t he o pti mize d late nt wit h t he ﬁ ne- 
t u ne d D DI M, O S A SI S is a ble t o pr o d uce st ylize d i ma ges 
wit h ma ni p ulate d attri b utes. Fi g ure 2pr o vi des a n o ver vie w 
of o ur met h o d. 
3. 1. Tr ai ni n g 
To e ns ure t hat t he c ha n ges i n t he C LI P e m be d di n g s pace 
occ ur i n t he desire d directi o n, we pre pare a si n gle i ma ge 
Ist yl e 
Bfr o m a st ylize d d o mai n ( de n ote d d o mai n B) a n d ai m 
t o c o n vert it t o a p h ot orealistic d o mai n ( de n ote d d o mai n A). Rece nt st u dies ha ve s h o w n t hat pretrai ne d D Ms ca n 
ge nerate d o mai n-s peci ﬁc i ma ges base d o n u nsee n d o mai n 
i ma ges [ 1 3 ,2 0 ]. B uil di n g o n t his, we utilize a pretrai ne d 
D D P M ϵθt o ge nerate a si n gle i ma ge Ist yl e 
Afr o m d o mai n A 
t hat is se ma nticall y ali g ne d wit h Ist yl e 
B.Ist yl e 
Bis e nc o de d t o 
a s peci ﬁc ti meste p t0b y utilizi n g t he f or war d D D P M: 
xt0=√αt0x0+1−αt0z,z∼ N (0,I),( 9) 
a n d s u bse q ue ntl y fr o m xt0,Ist yl e 
Ais ge nerate d b y f oll o wi n g 
t he re verse D D P M: 
xt−1=1√1−βt(xt−βt√1−αtϵθ(xt, t )) + σtz,( 1 0) 
w here z∼ N (0,I). After i nitializi n g Ist yl e 
Aa n d Ist yl e 
B, we 
pr ocee d t o freeze a pretrai ne d D DI M ϵA
θa n d t he se ma ntic 
e nc o der E nc ϕpr o p ose d b y Diff A E, w hic h is utilize d d uri n g 
t he i ma ge e nc o di n g pr ocess. A d diti o nall y, we create a c o p y 
of ϵA
θcalle d ϵB
θ, w hic h is ﬁ net u ne d t hr o u g h a c o m bi nati o n 
of C LI P directi o nal l osses a n d a rec o nstr ucti o n l oss. D uri n g 
trai ni n g, we n ote t hat Ii n 
Ais ge nerate d fr o m t he pretrai ne d 
Diff A E. C o nse q ue ntl y, o ur met h o d e na bles trai ni n g wit h o ut 
t he necessit y f or a dataset. 
Str uct ur al L ate nt C o de ϵB
θo pti mizes t o war ds ge nerat- 
i n g Ii n 
Bt hat re ﬂects t he se ma ntics of t he st yle i ma ge. H o w- 
e ver, si nce Ist yl e 
Aa n d Ist yl e 
Bsta ys ﬁ xe d w hile Ii n 
Ais c o n- 
sta ntl y ge nerate d, it is cr ucial t o caref ull y c h o ose a n e nc o d- 
i n g pr ocess t hat ge nerates Ii n 
Bt hat preser ves t he str uct ural 
8304
integrity of Iin
A. In order to achieve this, we utilize forward
DiffAE to encode Iin
Aby computing Eq. 7. First, Iin
Ais en-
coded into a semantic latent code zin
sem=Encϕ(Iin
A). By
following the forward process, zin
semis conditioned to the
frozen DDIM ϵA
θ,
xt+1=√αt+1fθ(xt, t,zin
sem) +p
1−αt+1ϵA
θ(xt, t,zin
sem)
(11)
resulting in Iin
Aencoded to a structural latent code xin
t0. The
input image is encoded to a specific timestep t0which can
be adjusted to control the level of structure preservation.
Structure-Preserving Network Although the structural
latent code xin
t0succeeds in preserving the overall structure
of the generated images Iin
A, the encoding process defined in
Eq.11inherently adds noise, which inevitably results in the
loss of structural information. To address this, we introduce
a structure-preserving network (SPN), which utilizes a 1x1
convolution that effectively preserves the spatial informa-
tion and structural integrity of Iin
A. To generate the output
of the next timestep xt−1, we use reverse DiffAE with SPN:
xSPN
t =SPN (Iin
A) (12)
x′
t=xt+λSPN∗xSPN
t (13)
xt−1=√αt−1fθ(x′
t, t,zin
sem) +p
1−αt−1ϵB
θ(x′
t, t,zin
sem)
(14)
We add the output of the SPN to xt,e.g. the previous
timestep output of ϵB
θ, and feed it into our training target
ϵB
θ. We regulate the degree of spatial information reflected
in the model by multiplying the output of the SPN xSPN
t by
λSPN . After fully reversing the timesteps, Iin
Bis generated.
Loss Function Inspired by MTG [39], we train ϵB
θby
optimizing our total loss, which is comprised of the cross-
domain loss, in-domain loss, and reconstruction loss. The
cross-domain loss aims to align the direction of changes
from domain A to domain B, ensuring that the change from
Iin
AtoIin
Bis kept consistent with the change from Istyle
A to
Istyle
B. Although the cross-domain loss provides the changes
in semantic information for the model to optimize upon,
it often leads to unintended changes when implemented
alone. Hence the in-domain loss is introduced to provide
additional information, measuring the similarity of changes
within both domains A and B.
The reconstruction loss provides additional guidance in
capturing the cross-domain change from Istyle
A toIstyle
B.
Similar to our encoding process of Eq. 11,Istyle
A is encoded
to a structural latent code xstyle
t0conditioned on semantic la-
tent code zstyle
sem. Following Eq. 12-14,i.e. the process of
generating the output of the next timestep conditioned on
zstyle
sem,ˆIstyle
B is generated. The reconstruction loss is calcu-
lated by comparing ˆIstyle
B withIstyle
B, comprised of the L1loss, perceptual similarity loss [35], and the L1CLIP em-
bedding loss. Detailed information regarding the loss func-
tion and experimental setup is available in the supplemen-
tary material.
3.2. Sampling
Mixing Content and Style Once trained, the model ϵB
θ
is capable of stylizing images from domain A to B. Styliz-
ing an image involves mixing two images in its latent space.
While this process is straightforward with StyleGAN [10],
it is still an ongoing research area for diffusion models. Un-
like the original DiffAE which conditions a single semantic
latent code zsemto the feature maps of DDIM, we discover
that properly conditioning zsemto the feature maps of ϵB
θ
achieves content and style mixing. This is done by condi-
tioning zstyle
sem to its low-level feature maps to transfer the
style of a style image, and conditioning zin
semto its high-
level feature maps to transfer the content of an input image.
The change point of conditioning is set as fch. Since ϵB
θis
a UNet-based model, conditioning the two latents is sym-
metrical. To preserve the structural information of the input
image, we use xin
t0as the structural latent code. The detailed
process of sampling is described in the supplementary ma-
terial.
Text-driven Manipulation Instead of optimizing a
model, we directly optimize the semantic latent code of in-
put image zin
semto achieve text-driven manipulation. Simi-
lar to previous works [13], we use CLIP directional loss for
optimization. After optimization, the optimized zin
semcan
be passed into the ϵB
θto incorporate the style into the image.
Comprehensive details about the experimental approach for
text-driven manipulation are available in the supplementary
material.
4. Experiments
For evaluating our approach, we focus on images from
low-density regions that include rarely encountered at-
tributes during training. This is ideal for demonstrating the
structure-preserving ability of our method as low-density
region images contain diverse objects and occlusions that
obscure the subject. To select these images, we leverage the
property that the encoded stochastic subcode xTtends to
show residuals of the original input image rather than being
normally distributed, as shown by DiffAE [23]. We ran-
domly select 20,000 images from the FFHQ dataset [10],
which is the dataset ϵA
θwas trained on. We reconstruct each
image using its semantic subcode zsemand stochastic sub-
codexT∼ N (0,I)(i.e. stochastic reconstruction). We
compare the reconstructed image with the original image
using perceptual similarity loss [35] to determine the qual-
ity of the reconstruction. We hypothesize that high-density
region images that contain frequently encountered attributes
would be reconstructed accurately, whereas those from low-
density regions would not. Figure 3shows that our hypothe-
8305
Original Full Recon. Stochastic Recon.High density Low densityFigure 3. High and Low-density images. Full Recon. refers to
reconstruction via conditioning its encoded zsemandxT, whereas
Stochastic Recon. refers to reconstruction via conditioning its en-
coded zsemandxT∼ N (0,I).
sis is well supported. Finally, we select the images from the
top 100 highest LPIPS score group (i.e. low-density) and
lowest LPIPS score group (i.e. high-density).
4.1. Qualitative Comparison
In Figure 4, we present a comparative analysis of the perfor-
mance of OSASIS against other stylization methods. Our
results demonstrate that OSASIS outperforms other meth-
ods in terms of preserving the overall structure while styl-
izing. MTG [39] and JoJoGAN [2] use outdated inversion
methods that struggle to preserve the diverse structure of
input images. Nonetheless, recent advancements in GAN-
based inversion techniques have demonstrated significant
improvements in handling out-of-distribution input images
for editing purposes [24, 31,33]. To ensure a fair compar-
ison, we employ HFGI [31] for MTG and JoJoGAN. De-
spite these adjustments, OSASIS remains distinguished in
its structural preservation capabilities. Furthermore, GAN-
based methods produce unintended modifications, such as
changes in facial expressions. Since DiffuseIT [15] stylizes
images without training, they struggle to overcome the do-
main gap between the input and style images. InST [37]
utilizes textual inversion to deduce the style image’s con-
cept and subsequently conditions its generation procedure
on this concept. However, the guidance strategy outlined
in [7] tends to produce style-concentrated images, leading
to unintended variations such as changes in facial expres-
sions and identities. Moreover, as noted in InST, there are
difficulties in faithfully transferring the color of the style
images. More qualitative comparison results are provided
in the supplementary material.
4.2. Quantitative Comparison
We conduct a quantitative comparison with other methods.
By using ArtFID [32] as a metric for effective stylization
and the identity similarity measure with ArcFace [3] to as-MethodsArtFID↓
AAHQ MetFaces Prev
MTG+HFGI 36.39 38.02 37.27
JoJoGAN+HFGI 40.41 44.74 41.09
DiffuseIT 44.93 53.35 48.18
InST 38.16 50.33 35.86
OSASIS(Ours) 34.89 43.20 33.20
MethodsID Similarity↑
AAHQ MetFaces Prev
MTG+HFGI 0.3730 0.4656 0.4063
JoJoGAN+HFGI 0.5145 0.5207 0.4743
DiffuseIT 0.6992 0.7158 0.6994
InST 0.2253 0.2188 0.2238
OSASIS(Ours) 0.6825 0.7323 0.7029
MethodsStructure Distance↓
AAHQ MetFaces Prev
MTG+HFGI 0.0386 0.0350 0.0360
JoJoGAN+HFGI 0.0411 0.0454 0.0430
DiffuseIT 0.0309 0.0300 0.0310
InST 0.0492 0.0443 0.0488
OSASIS(Ours) 0.0361 0.0295 0.0391
Table 1. Quantitative comparison. ArtFID evaluates the pertinence
of stylization, whereas ID similarity and structure distance mea-
sure whether the stylized image stays true to its original input.
sess content preservation. For measuring structure preser-
vation, we employ the structure distance metric [30]. For
our source of style images, we select five style images from
each of three datasets: i) AAHQ [18], ii) MetFaces [11],
iii) style images used in previous researches. In Table 1, we
evaluate OSASIS against MTG [39], JoJoGAN [2], Diffu-
seIT [15], and InST [37]. For our initial pretrained ϵA
θand
all baseline models, we use publicly available pretrained
models that were trained on FFHQ. As previously men-
tioned, HFGI [31] is employed to invert input images for
MTG and JoJoGAN. Note that Table 1presents outcomes
only for low-density images, while a comprehensive com-
parison is presented in the supplementary material. Our re-
sults indicate that while MTG occasionally achieves a better
ArtFID score than our model, we outperform significantly
in terms of identity similarity and structure preservation.
Additionally, while DiffuseIT excels at preserving the iden-
tity and the structure of the image, it exhibits inferior styl-
ization results compared to GAN-based methods due to a
domain gap between the input and style images.
4.3. OOD Reference Image
OSASIS is able to stylize images with out-of-domain
(OOD) reference images, a feature that is not commonly
seen in GAN-based methods. OSASIS can effectively dis-
entangle semantics from the structure, resulting in only the
style factor of the style image being transferred to the in-
put image. In contrast, GAN-based methods have entan-
gled style codes in terms of structure and semantics, which
8306
Input
Style
OSASIS (Ours) MTG JoJoGAN DiffuseIT
 InST
Figure 4. Comparison with other stylization methods. Note that our method successfully preserves the low-density attributes while other
baseline methods fail to do so.
Input
Style
OSASIS (Ours) MTG JoJoGAN DiffuseIT InST
Figure 5. Stylization with OOD reference images. Due to the limited capabilities of GAN-based inversion methods, the baseline methods
fail in disentangling the structure and semantics of the style image. This results in structural artifacts being transferred into the output
image, whereas OSASIS successfully extracts only the semantics.
makes it difficult to transfer only the style factor from the
reference image. As shown in Figure 5, OSASIS is able to
stylize images with OOD reference images while preserv-
ing its content, whereas other baseline methods suffer from
severe artifacts. Although DiffuseIT and InST manage to
avoid severe artifacts, they still struggle with addressing do-
main gap and handling strong concept conditioning.
4.4. Stylization Results on Other Datasets
To evaluate the generalization capabilities of our method
across various datasets, we executed stylization on several
distinct datasets: AFHQ-dog [1], LSUN-church [34], and
DeepFashion [19]. Figure 6displays the efficacy and versa-
tility of our approach across these diverse datasets. Notably,
our method exhibits proficiency beyond facial stylization,
adeptly adapting to various image domains.ArtFID↓ ID Sim↑ SD↓
w/o SPN 36.41 0.6595 0.0371
w SPN (λ SPN =0.1) 34.89 0.6825 0.0361
w SPN (λ SPN =0.5) 36.62 0.7177 0.0348
w SPN (λ SPN =1.0) 43.15 0.7321 0.0355
Table 2. Quantitative ablation study of SPN
4.5. Stylization with Text-driven Manipulation
For text-driven manipulation, we directly optimize
zin
semusing CLIP directional loss. Once zin
semis optimized,
it can be used to stylize the input image with the aforemen-
tioned mixing process. In Figure 7, we show our qualitative
results of stylization with text-driven manipulation, where
our model successfully incorporates the style of a reference
image with manipulated attributes while being robust in
content preservation.
8307
Input
Style
LSUN -church
Input
Style
Input
Style
AFHQ -Dog DeepFashionFigure 6. Stylization result of OSASIS on LSUN-church, AFHQ-dog, and DeepFashion.
Input Style Smiling Angry Beards
Figure 7. Stylization with text-driven manipulation. The opti-
mized semantic code doesn’t overwrite or harm the structure and
style of the image, thus preserving the overall structure while ma-
nipulating attributes.
Figure 8. Ablation study of latent codes. The result shows that
OSASIS is capable of effectively disentangling the structure from
the semantics. By conditioning the semantic codes appropriately,
we are able to control the content and style factors in the generated
image.
4.6. Ablation Study
Latent Code Furthermore, we conduct ablation studies to
shed light on the nature of mixing content and style into the
feature maps of the UNet. In the first row of Figure 8, we
perform normal stylization on an input image, i.e. by en-
coding its structural latent code xin
t0, conditioning zstyle
sem tolow-level feature maps, and conditioning zin
semto high-level
feature maps. The second row shows the results of the se-
mantic codes being conditioned oppositely. The resulting
generated image 1) holds the structural integrity encoded
from the structural latent code xin
t0, 2) preserves the con-
tent from the given content image (i.e. identity, facial ex-
pressions), and 3) retains the stylized attributes of the given
style image (i.e. skin complexion). We show that by condi-
tioning the semantic codes to their respective feature maps,
OSASIS achieves control over mixing content and style.
Structure-Preserving Network Our SPN is imple-
mented to aid the structural latent code xin
t0in preserving
the overall structure of the stylized image, due to the encod-
ing process Eq.11 being formulated to add noise. Figure 9
shows the effects of our SPN, where the third column is a
stylized sample without, and the fourth column is a stylized
sample with SPN. It can be seen that the stochastic latent
codexin
t0faithfully preserves the overall structure of the im-
age, but without SPN, objects along the edges of content
images (e.g. hands, poles, fingers) are distorted. To inves-
tigate the effect of SPN further, we conduct a quantitative
ablation study. Table 2validates the efficacy of SPN, indi-
cating substantial improvements in ID similarity and struc-
ture distance, thereby cementing its importance for main-
taining content and structural integrity. It’s important to
note, however, that a λSPN value above 0.1 might overem-
phasize structural aspects, which can compromise the styl-
ization quality, suggesting the necessity for careful calibra-
tion of λSPN to achieve an optimal balance in stylization.
5. Related Work
5.1. One-shot Image Stylization
Stylizing input images with only one reference image orig-
inates from neural style transfer, first introduced by Gatys
et al. [6]. However, this method requires the stylized im-
age to be optimized every generation, which was addressed
8308
Without SPN Input Style
 With SPN
Figure 9. Ablation study of SPN. The results demonstrate that SPN
is a crucial to ensure the preservation of the underlying structure
while applying the stylization process.
by Johnson et al. [9] by introducing an image transform net-
work for fast stylization. However, traditional NST methods
are limited in their ability to capture the semantic informa-
tion of input and style images.
For GAN-based models, one-shot image stylization is
achieved by using one-shot adaptation methods, which aims
to transfer a generator to a new domain using a single refer-
ence image. One-shot adaptation methods typically involve
finetuning a generator using only a single reference image.
Once the generator is finetuned, these methods can uncondi-
tionally generate stylized images, and by using GAN inver-
sion techniques, they also achieve input image stylization.
StyleGAN-NADA [5], while applicable to one-shot image
stylization tasks, exhibits limited capability, primarily due
to its initial development for text-driven style transfer pur-
poses. The first successful GAN-based one-shot adapta-
tion method is MTG [39], which uses CLIP directional loss
to finetune the generator and mitigate overfitting problems.
Other works have since focused on improving generation
quality [16], content preservation [36], and entity trans-
fer [38]. In contrast to one-shot adaptation approaches, our
method aims to stylize real images with a single reference
image instead of generating stylized synthesized images.
Therefore, we refer to our method as one-shot image styl-
ization. From our perspective, the work that is most com-
parable to our own is JoJoGAN [2]. JoJoGAN generates
a training dataset by random style mixing and finetunes a
generator to create a style mapper.
5.2. Image Manipulation with Diffusion Models
Image manipulation has advanced significantly in recent
years, with methods presented by StyleGAN2 [12] being
widely explored. However, the potential of diffusion mod-
els for high-quality image manipulation has been elucidated
in recent research. DiffAE [23] introduces a semantic en-coder that generates semantically meaningful latent vectors
for diffusion models, which can be manipulated for attribute
editing. DiffusionCLIP [13] demonstrates the effectiveness
of diffusion-based text-guided image manipulation by fine-
tuning a DDIM with CLIP directional loss. Asyrp [17] un-
covers a semantic latent space in the architecture of dif-
fusion models.The authors train the h-space manipulation
module with CLIP directional loss, achieving consistent im-
age editing results. While DiffusionCLIP and Asyrp em-
ploy CLIP directional loss, their focus is on text-guided ma-
nipulation whereas our work targets image-guided manipu-
lation. DiffuseIT [15] aims to perform image translation
guided by either text or image using a CLIP and a pretrained
ViT [4]. Their approach leverages the reverse process of
DDPM and incorporates CLIP and ViT to guide the image
generation process. InST [37] employs textual inversion to
extract the concept from a style image. By conditioning the
generation process on this extracted concept, InST is able
to stylize input images.
6. Conclusion
We have introduced OSASIS, a novel one-shot image styl-
ization method based on diffusion models. In contrast to
GAN-based and other diffusion-based stylization methods,
OSASIS shows robust structure awareness in stylization, ef-
fectively disentangling the structure and semantics from an
image. While OSASIS demonstrates significant advance-
ments in structure-aware stylization, several limitations ex-
ist. A notable constraint of OSASIS is its training time,
which is longer than comparison methods. This extended
training duration is a trade-off for the method’s enhanced
ability to maintain structural integrity and adapt to vari-
ous styles. Additionally, OSASIS requires training for each
style image. This requirement can be seen as a limitation
in scenarios where rapid deployment across multiple styles
is needed. Despite these challenges, the robustness of OS-
ASIS in preserving the structural integrity of the input im-
ages, its effectiveness in out-of-domain reference styliza-
tion, and its adaptability in text-driven manipulation make it
a promising approach in the field of stylized image synthe-
sis. Future work will address these limitations, particularly
in optimizing training efficiency and reducing the necessity
for individual style image training, to enhance the practical-
ity and applicability of OSASIS in diverse real-world sce-
narios.
Acknowledgment We thank the ImageVision team of
NA VER Cloud for their thoughtful advice and discussions.
Training and experiments were done on the Naver Smart
Machine Learning (NSML) platform [14]. This study was
supported by BK21 FOUR.
8309
References
[1] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha.
Stargan v2: Diverse image synthesis for multiple domains.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition, pages 8188–8197, 2020. 6
[2] Min Jin Chong and David Forsyth. Jojogan: One shot face
stylization. In European Conference on Computer Vision,
pages 128–152. Springer, 2022. 1,5,8
[3] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos
Zafeiriou. Arcface: Additive angular margin loss for deep
face recognition. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition, pages
4690–4699, 2019. 5
[4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020. 8
[5] Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano,
Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip-
guided domain adaptation of image generators. ACM Trans-
actions on Graphics (TOG), 41(4):1–13, 2022. 8
[6] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Im-
age style transfer using convolutional neural networks. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 2414–2423, 2016. 7
[7] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. In NeurIPS 2021 Workshop on Deep Generative
Models and Downstream Applications, 2021. 5
[8] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems, 33:6840–6851, 2020. 2
[9] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Percep-
tual losses for real-time style transfer and super-resolution.
InComputer Vision–ECCV 2016: 14th European Confer-
ence, Amsterdam, The Netherlands, October 11-14, 2016,
Proceedings, Part II 14, pages 694–711. Springer, 2016. 8
[10] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition, pages 4401–4410, 2019. 4
[11] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine,
Jaakko Lehtinen, and Timo Aila. Training generative adver-
sarial networks with limited data. Advances in neural infor-
mation processing systems, 33:12104–12114, 2020. 5
[12] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improv-
ing the image quality of stylegan. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 8110–8119, 2020. 8
[13] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Dif-
fusionclip: Text-guided diffusion models for robust image
manipulation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 2426–
2435, 2022. 2,3,4,8[14] Hanjoo Kim, Minkyu Kim, Dongjoo Seo, Jinwoong Kim,
Heungseok Park, Soeun Park, Hyunwoo Jo, KyungHyun
Kim, Youngil Yang, Youngkwan Kim, et al. Nsml: Meet the
mlaas platform with a real-world case study. arXiv preprint
arXiv:1810.09957, 2018. 8
[15] Gihyun Kwon and Jong Chul Ye. Diffusion-based image
translation using disentangled style and content representa-
tion. arXiv preprint arXiv:2209.15264, 2022. 2,5,8
[16] Gihyun Kwon and Jong Chul Ye. One-shot adaptation of gan
in just one clip. arXiv preprint arXiv:2203.09301, 2022. 1,
8
[17] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion
models already have a semantic latent space. arXiv preprint
arXiv:2210.10960, 2022. 2,8
[18] Mingcong Liu, Qiang Li, Zekui Qin, Guoxin Zhang, Pengfei
Wan, and Wen Zheng. Blendgan: Implicitly gan blending
for arbitrary stylized face generation. Advances in Neural
Information Processing Systems, 34:29710–29722, 2021. 5
[19] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xi-
aoou Tang. Deepfashion: Powering robust clothes recog-
nition and retrieval with rich annotations. In Proceedings of
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2016. 6
[20] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided
image synthesis and editing with stochastic differential equa-
tions. In International Conference on Learning Representa-
tions, 2022. 3
[21] Utkarsh Ojha, Yijun Li, Jingwan Lu, Alexei A Efros,
Yong Jae Lee, Eli Shechtman, and Richard Zhang. Few-shot
image generation via cross-domain correspondence. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 10743–10752, 2021. 1
[22] Justin NM Pinkney and Doron Adler. Resolution dependent
gan interpolation for controllable image synthesis between
domains. arXiv preprint arXiv:2010.05334, 2020. 1
[23] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizad-
wongsa, and Supasorn Suwajanakorn. Diffusion autoen-
coders: Toward a meaningful and decodable representation.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 10619–10629, 2022.
2,4,8
[24] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel
Cohen-Or. Pivotal tuning for latent-based editing of real im-
ages. ACM Transactions on graphics (TOG), 42(1):1–13,
2022. 5
[25] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 10684–10695, 2022. 2
[26] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi,
Rapha Gontijo Lopes, et al. Photorealistic text-to-image
diffusion models with deep language understanding. arXiv
preprint arXiv:2205.11487, 2022. 2
8310
[27] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-
mans, David J Fleet, and Mohammad Norouzi. Image super-
resolution via iterative refinement. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 2022. 2
[28] Guoxian Song, Linjie Luo, Jing Liu, Wan-Chun Ma, Chun-
pong Lai, Chuanxia Zheng, and Tat-Jen Cham. Agilegan:
stylizing portraits by inversion-consistent transfer learning.
ACM Transactions on Graphics (TOG), 40(4):1–13, 2021. 1
[29] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In International Conference
on Learning Representations, 2020. 2
[30] Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali
Dekel. Splicing vit features for semantic appearance transfer.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 10748–10757, 2022.
5
[31] Tengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, and
Qifeng Chen. High-fidelity gan inversion for image attribute
editing. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2022. 5
[32] Matthias Wright and Bj ¨orn Ommer. Artfid: Quantitative
evaluation of neural style transfer. In Pattern Recognition:
44th DAGM German Conference, DAGM GCPR 2022, Kon-
stanz, Germany, September 27–30, 2022, Proceedings, pages
560–576. Springer, 2022. 5
[33] Yiran Xu, Zhixin Shu, Cameron Smith, Seoung Wug Oh,
and Jia-Bin Huang. In-n-out: Faithful 3d gan inversion with
volumetric decomposition for face editing. arXiv preprint
arXiv:2302.04871, 2023. 5
[34] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas
Funkhouser, and Jianxiong Xiao. Lsun: Construction of a
large-scale image dataset using deep learning with humans
in the loop. arXiv preprint arXiv:1506.03365, 2015. 6
[35] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 586–595, 2018. 4
[36] Yabo Zhang, Yuxiang Wei, Zhilong Ji, Jinfeng Bai, Wang-
meng Zuo, et al. Towards diverse and faithful one-shot adap-
tion of generative adversarial networks. In Advances in Neu-
ral Information Processing Systems, 2022. 1,8
[37] Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang,
Chongyang Ma, Weiming Dong, and Changsheng Xu.
Inversion-based style transfer with diffusion models. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 10146–10156, 2023. 2,5,8
[38] Zicheng Zhang, Yinglu Liu, Congying Han, Tiande Guo,
Ting Yao, and Tao Mei. Generalized one-shot domain
adaption of generative adversarial networks. arXiv preprint
arXiv:2209.03665, 2022. 1,8
[39] Peihao Zhu, Rameen Abdal, John Femiani, and Peter Wonka.
Mind the gap: Domain gap control for single shot domain
adaptation for generative adversarial networks. In Interna-
tional Conference on Learning Representations, 2021. 1,2,
4,5,8
8311
