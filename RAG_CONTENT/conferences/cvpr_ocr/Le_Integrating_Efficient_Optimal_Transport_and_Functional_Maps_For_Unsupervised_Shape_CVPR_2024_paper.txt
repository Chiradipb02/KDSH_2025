Integrating Efﬁcient Optimal Transport and Functional Maps For Unsupervised
Shape Correspondence Learning
Tung Le1Khai Nguyen2Shanlin Sun1Nhat Ho2Xiaohui Xie1
1University of California, Irvine
2The University of Texas at Austin
1{thanhtul, shanlins, xhx }@uci.edu2{khainb, minhnhat }@utexas.edu
Abstract
In the realm of computer vision and graphics, accu-
rately establishing correspondences between geometric 3D
shapes is pivotal for applications like object tracking, reg-
istration, texture transfer, and statistical shape analysis.
Moving beyond traditional hand-crafted and data-driven
feature learning methods, we incorporate spectral methods
with deep learning, focusing on functional maps (FMs) and
optimal transport (OT). Traditional OT-based approaches,
often reliant on entropy regularization OT in learning-based
framework, face computational challenges due to their
quadratic cost. Our key contribution is to employ the sliced
Wasserstein distance (SWD) for OT, which is a valid fast op-
timal transport metric in an unsupervised shape matching
framework. This unsupervised framework integrates func-
tional map regularizers with a novel OT-based loss derived
from SWD, enhancing feature alignment between shapes
treated as discrete probability measures. We also introduce
an adaptive reﬁnement process utilizing entropy regularized
OT, further reﬁning feature alignments for accurate point-
to-point correspondences. Our method demonstrates supe-
rior performance in non-rigid shape matching, including
near-isometric and non-isometric scenarios, and excels in
downstream tasks like segmentation transfer. The empirical
results on diverse datasets highlight our framework’s effec-
tiveness and generalization capabilities, setting new stan-
dards in non-rigid shape matching with efﬁcient OT metrics
and an adaptive reﬁnement module. Code is available at1.
1. Introduction
Establishing precise correspondences between geometric
3D shapes is a core challenge in various domains of com-
puter vision and graphics, including but not limited to, ob-
ject tracking, registration, reconstruction, deformation, tex-
ture transfer, and statistical shape analysis [ 7,14,22,53,
1https : / / github . com / Tungthanhlee / EOT -
Correspondence56,63]. To facilitate the mapping between non-rigid shapes,
early approaches [ 6,9,49] concentrated on the development
of hand-crafted features, leveraging geometric invariance as
a key principle. In the latter approaches [ 4,10,16,28],
there has been a shift towards the utilization of data-driven
methods for feature learning, which has resulted in marked
enhancements in terms of accuracy, efﬁciency, and robust-
ness.
Recently, an increasing body of work has exploited the
use of spectral methods [ 5,18,21,33,47], especially the
functional map (FM) representation [ 40]. Speciﬁcally, the
FM methods succinctly encode correspondences through
compact matrices, utilizing a truncated spectral basis. With
recent developments in deep learning, deep FM (DFM) is
quickly employed in numerous settings [ 11,12,28,55]
by incorporating feature learning as geometric descriptors
for FM frameworks. Most DFM works focus on learn-
ing features that optimize FM priors to express desirable
map priors, e.g. area preservation, isometry, and bijectiv-
ity, which achieves remarkable results even without super-
vision [ 10,12,20,21,47]. On the other hand, less attention
is paid to the problem of explicitly aligning features out-
putted from the feature extractor network, due to the lack of
smoothness and consistency of linear assignment problems.
In this work, we focus on jointly learning features via
the functional map, and explicit features, i.e. directly from
the feature extractor to establish correct correspondence.
Nonetheless, learning to map explicit features is not easy
since the geometric objects might potentially undergo arbi-
trary deformations. Therefore, we propose to employ opti-
mal transport (OT), which is a well-known approach for lin-
ear assignment problems, to cast the feature alignment from
3D shapes as a probability measures matching problem.
The Wasserstein distance [ 42,61] is widely acknowl-
edged as an effective OT metric for comparing two prob-
ability measures, particularly when their supports are dis-
joint. However, it comes with the drawback of high com-
putational complexity. Speciﬁcally, for discrete proba-
bility measures with at most msupports, the time and
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
23188
memory complexities are O(m3logm)and O(m2), re-
spectively. This computational burden is exacerbated in
3D shape applications where each shape, represented as
mesh, is treated as a distinct probability measure. To ame-
liorate the computational demands, entropic regularization
coupled with the Sinkhorn algorithm [ 13] can yield an ✏-
approximation of the Wasserstein distance with a time com-
plexity of O(m2/✏2)[2,30–32]. Nonetheless, this method
does not alleviate the O(m2)memory complexity due to
the necessity of storing the cost matrix. Additionally, the
entropic regularization fails to produce a valid metric be-
tween probability measures as it does not satisfy the tri-
angle inequality. An alternative, more efﬁcient method is
the sliced Wasserstein distance (SWD) [ 8], which calculates
the expectation of the Wasserstein distance between ran-
dom one-dimensional push-forward measures derived from
the original measures. SWD offers a time complexity of
O(mlogm)and a linear memory complexity of O(m).
Motivated by the above discussion, we introduce a novel
differentiable unsupervised OT-based loss derived from ef-
ﬁcient sliced Wasserstein distance, which accounts for asso-
ciating two extracted extrinsic features to align two meshes
combined with functional map regularizers. Our proposed
approach leverages a valid efﬁcient OT metric to obtain
highly discriminative local feature matching. Addition-
ally, the integration of functional map regularizers promotes
smoothness in the mapping process, allowing our method to
achieve both precise and smooth correspondence.
Furthermore, we introduce an adaptive reﬁnement pro-
cess tailored for each pair of shapes, utilizing entropy regu-
larized OT to enhance matching performance. The differen-
tiable nature of entropic regularization in OT enables our re-
ﬁnement strategy to leverage the Sinkhorn algorithm. This
approach yields a soft point-wise map, which is instrumen-
tal in calculating FM regularizers. These regularizers are
then used to iteratively update features, thereby facilitating
the retrieval of precise point-to-point correspondences.
Finally, we demonstrate our proposed approach on a di-
verse and extensive selection of datasets. Our contributions
are as follows:
•We propose an unsupervised learning framework that em-
ploys efﬁcient optimal transport to jointly learn with func-
tional map in shape matching paradigm. Subsequently,
we derive two novel unsupervised loss functions based
on sliced Wasserstein distance, which is a valid fast op-
timal transport metric, to effectively align mesh features
by interpreting them as probability measures, potentially
offering a promising avenue for advancements in shape
matching through efﬁcient optimal transport.
•To enhance the quality of point mapping, we propose
an adaptive reﬁnement module that iteratively reﬁnes the
optimal transport similarity matrix estimated via entropy
regularization optimal transport.•We outperform previous state-of-the-art works in vari-
ous settings of non-rigid shape matching including near-
isometric and non-isometric shape matching. Addition-
ally, when applied to a downstream task such as seg-
mentation transfer, our approach continues to outperform
contemporary state-of-the-art methods in non-rigid shape
matching. This success not only demonstrates the efﬁ-
cacy of our method in speciﬁc applications but also under-
lines its strong generalization capabilities across various
use cases in shape matching.
2. Related work
Shape matching has been extensively explored for decades.
For a comprehensive examination of this topic, we encour-
age readers to consult the detailed analyses presented in sur-
veys [ 48,58]. In this section, we focus speciﬁcally on the
literature subset that directly relates to our research objec-
tives.
2.1. Deep functional maps for shape correspon-
dence.
Our methodology is founded on the functional map repre-
sentation, initially introduced in [ 40] and substantially de-
veloped through subsequent research, e.g. [ 41]. The cen-
tral concept of functional maps revolves around expressing
shape correspondences as transformations between their re-
spective spectral embeddings. This is efﬁciently achieved
by utilizing compact matrices formulated from reduced
eigenbases. The functional maps approach has seen con-
siderable enhancements in terms of accuracy, efﬁciency,
and robustness, as evidenced by a variety of recent contri-
butions [ 23,26,46]. In contrast to axiomatic approaches
that rely on manually engineered features [ 54], deep func-
tional map methods aim to autonomously learn features
from training data. The pioneering work in this domain was
FMNet [ 33], which introduced a method to learn non-linear
transformations of SHOT descriptors [ 49]. Subsequent de-
velopments [ 21,47] facilitated the unsupervised training of
FMNet by incorporating isometry losses in both spatial and
spectral domains. This unsupervised approach has been
further enhanced with the advent of robust mesh feature
extractors [ 50], leading to the development of new frame-
works [ 10,12,16,28] that learn directly from geometric
data, achieving top-tier performance.
2.2. Optimal transport for shape correspondence
Optimal transport has emerged as a powerful tool in the ﬁeld
of shape correspondence, offering innovative approaches
to match and analyze complex shapes in computer graph-
ics and computer vision. Starting with the axiomatic
shape matching approach, [ 52] proposed an algorithm for
probabilistic correspondence that optimizes an entropy-
regularized Gromov-Wasserstein (GW) objective [ 37] to
23189
ﬁnd the correspondence between two given shapes. The
proposed framework is inefﬁcient since solving entropy-
regularized GW objective is relatively expensive and it does
not perform well on non-isometric shape matching. To ad-
dress the computational overhead of solving OT cost, [ 51]
brought robust OT to the forefront, signiﬁcantly enhancing
the accuracy and efﬁciency of point cloud registration, but
the framework is designed for point cloud that avoids the
connectivity of the shape mesh. Perhaps the most relevant
work to ours is Deep Shells [ 18], which is an improve-
ment of [ 17]. Deep Shells demonstrated how OT can be
seamlessly integrated into deep neural networks, offering a
new perspective in shape matching with improved adapt-
ability and precision. However, computing OT cost via
Sinkhorn algorithm in Deel Shells [ 18] can be expensive
since it has to store the cost matrix with quadratic memory
cost and quadratic time complexity. In light of this, we pro-
pose to employ an efﬁcient OT in learning shape correspon-
dence. To be speciﬁc, we employ sliced Wasserstein dis-
tance, which calculates the expectation the Wasserstein dis-
tance between two random one-dimensional push-forward
measures derived from original measures. Recently, sliced
Wasserstein distance has been successfully applied in point
cloud [ 39] and shape [ 27] deformation. However, to the best
of our knowledge, we are the ﬁrst to employ sliced Wasser-
stein distance on shape correspondence framework.
3. Background
In this section, we brieﬂy recap functional map representa-
tion [ 40]. After that, we review the deﬁnition of Wasserstein
distance and its closed-formed solution sliced Wasserstein
distance.
3.1. (Deep) Functional Maps
Given a pair of smooth shapes XandY, which are dis-
cretized as triangular meshes with nxandnyvertices, re-
spectively. The functional map method aims to obtain a
dense correspondence between the two shapes by com-
pactly representing the correspondence matrix as a smaller
matrix. Speciﬁcally, the leading keigenfunctions of the
Laplace-Beltrami operator are computed on both shapes X,
Yand are presented as  x2Rnx⇥kand y2Rny⇥k, re-
spectively. The geometric features of the shape are either
precomputed [ 49] or extracted from a neural network [ 50],
represented as Fx2Rnx⇥dandFy2Rny⇥d, where dis
the feature dimension. The extracted features are then pro-
jected into the eigenbasis to get the corresponding coefﬁ-
cients A= †
xFx2Rk⇥dandB= †
yFy2Rk⇥d, where
†denotes the Moore-Penrose pseudo-inverse. After that, the
bidirectional optimal functional map C⇤
xy,C⇤
yx2Rk⇥kis
obtained by solving the linear system:
C⇤
xy= arg min
CEdata(C)+Ereg(C), (1)where Edata(C)= kCA Bk2promotes the descriptor
preservation, whereas the Eregis a regularization term im-
posing structural properties of C[40]. Finally, the dense
correspondence can be reconstructed from estimated C⇤by
conducting nearest neighbor search between the rows of
 xCyxand that of  y, with possible post-processing [ 19,
36,43].
3.2. Efﬁcient Optimal Transport
Wasserstein distance. Forp 1, given two probability
measures µ2P p(Rd)and⌫2P p(Rd), the Wasserstein
distance [ 59] between µand⌫is :
Wp
p(µ, ⌫)= i n f
⇡2⇧(µ,⌫)Z
Rd⇥Rdkx ykp
pd⇡(x, y),(2)
where ⇧(µ, ⌫)are the set of all couplings between µand
⌫i.e., joint probability measures that have marginals as µ
and⌫respectively. The Wasserstein distance is the optimal
transportation cost between µand⌫since it is computed
with the optimal coupling. As mentioned in the introduction
section, the downside of Wasserstein distance is a high com-
putational complexity in the discrete case i.e., O(m3logm)
in time and O(m2)in space for mis the number of supports.
To reduce the time complexity, entropic regularized optimal
transport [ 13] is introduced.
Sinkhorn divergence. Forp 1, given two probability
measures µ2P p(Rd)and⌫2P p(Rd), the Sinkhorn-p
divergence [ 13] between µand⌫is :
Sp
✏,p(µ, ⌫)= i n f
⇡2⇧✏(µ,⌫)Z
Rd⇥Rdcd⇡(x, y)+✏H(⇡),(3)
where ⇧✏(µ, ⌫)= {⇡2⇧(µ, ⌫)|KL(⇡,µ⌦⌫)✏}
with KL denotes the Kullback Leibler divergence. The cost
c:Rd⇥Rd7!Ris deﬁned as cp(x, y)= kx ykp
pon
Rd⇥Rd. The entropy term H(⇡)allows us to solve for
the correspondence ⇡via Sinkhorn-Knopp algorithm with
O(m2)in time complexity.
Sliced Wasserstein distance. The sliced Wasserstein (SW)
distance [ 8] between two probability measures µ2P p(Rd)
and⌫2P p(Rd)is given by:
SWp
p(µ, ⌫)=E✓⇠U(Sd 1)[Wp
p(✓]µ, ✓]⌫ )], (4)
where ✓]⌫denotes the push-forward measure of µvia func-
tionf(x)=✓>x, and the one-dimensional Wasserstein dis-
tance appears in a closed form which is Wp
p(✓]µ, ✓]⌫ )=R1
0|F 1
✓]µ(z) F 1
✓]⌫(z)|pdz. Here, F✓]µandF✓]⌫are the
cumulative distribution function (CDF) of ✓]µand✓]⌫re-
spectively. When µand⌫have at most nsupports, the com-
putation of the SW is only O(nlogn)in time and O(n)in
23190
Figure 1. Overview of unsupervised shape matching via efﬁcient OT. Our framework takes as input a pair of shapes XandYand
outputs point-to-point correspondence. Firstly, the features extractor tasks the pair input and extracts vertex-wise features FxandFy.
Subsequently, the differentiable functional map solver is used to compute functional map given pre-computed eigenfunctions and the
extracted features. In parallel, our framework estimates a soft feature similarity matrix, derived from the same extracted features. After
that, an OT cost is computed given soft feature similarity and extracted feature FxandFy. Finally, a proper loss is optimized together with
regularized functional map loss and OT loss.
space. The SW often is computed by using LMonte Carlo
samples ✓1,...,✓ Lfrom the unit sphere:
dSWp
p(µ, ⌫;L)=1
LLX
l=1Wp
p(✓l]µ, ✓ l]⌫). (5)
Energy-based Sliced Wasserstein distance. Energy-based
sliced Wasserstein (EBSW) is a more discriminative variant
of the SW proposed in [ 38]. The deﬁnition of the EBSW is
given as:
EBSWp
p(µ, ⌫;f)=E✓⇠ µ,⌫(✓;f,p)⇥
Wp
p(✓]µ, ✓]⌫ )⇤
,(6)
where fis the energy function e.g., f(x)= ex, and
 µ,⌫(✓;f,p)/f(Wp
p(✓]µ, ✓]⌫ ))2P(Sd 1)is the energy-
based slicing distribution. The EBSW can be computed
based on importance sampling with Lsamples from pro-
posal distribution  0(✓), e.g., U(Sd 1). For ✓1,...,✓ Li.i.d⇠
 0(✓), we have:
\IS-EBSWp
p(µ, ⌫;f,L)
=LX
l=1Wp
p(✓l]µ, ✓ l]⌫)ˆwµ,⌫,  0,f,p(✓l),(7)
for wµ,⌫,  0,f,p(✓)=f(Wp
p(✓]µ,✓]⌫ ))
 0(✓)is the impor-
tance weighted function and ˆwµ,⌫,  0,f,p(✓l)=
wµ,⌫,  0,f,p(✓l)PL
l0=1wµ,⌫,  0,f,p(✓l0)is the normalized importance weights.4. Learning Shape Correspondence with Efﬁ-
cient Optimal Transport
In this section, we provide in-depth details of our proposed
non-rigid shape matching framework. The whole frame-
work is described in Fig. 11. Our pipeline starts by ex-
tracting features from the feature extractor as described in
Sec. 4.1. Then we describe functional map in Sec. 4.2.
Thirdly, we illustrate how efﬁcient OT in Sec. 4.3is applied
to our framework and propose two novel loss functions for
learning precise shape mapping. Thirdly, we summarize
our unsupervised losses in Sec. 4.4. Finally, we propose
an adaptive reﬁnement process in Sec. 4.5.
4.1. Feature extractor
Our architecture is designed in the form of a Siamese net-
work. Speciﬁcally, we utilize the same feature extractor
with shared learning parameters to extract features from
a pair of input shapes. We employ DiffusionNet [ 50] as
our feature extractor since DiffusionNet is agnostic to dis-
cretization and resolution of the meshes, thereby ensur-
ing robust shape correspondence. Consequently, from the
pair of inputs, we extract two sets of features, denoted by
Fx2Rnx⇥dandFy2Rny⇥dvia DiffusionNet.
4.2. Functional map module
As discussed in 3.1, we aim to employ deep functional map
as a proxy to learn an intrinsic feature shape matching.
Speciﬁcally, we employ regularized functional map [ 44],
23191
to compute optimal functional map C⇤as mentioned in
Sec. 3.1. During training, the network aims to minimize
the structural regularization of functional map:
Lfmap=↵1Lbij+↵2Lothor, (8)
where Lbij=kCxyCyx Ik2+kCyxCxy Ik2pro-
motes identity mapping and Lothor =kCT
xyCyx Ik2+
kCT
yxCxy Ik2imposes locally area-preserving [ 44].
4.3. Feature extrinsic alignment via efﬁcient opti-
mal transport
We aim to integrate efﬁcient OT into deep functional map to
promote precise mesh feature alignment. Thanks to the fast
computation and the closed-form solution of sliced Wasser-
stein (SW) distance, we derive a novel loss function based
on SW distance.
Soft feature similarity. Firstly, from a pair of features
Fx,Fyextracted from shapes X,Y, respectively, we esti-
mate a soft feature similarity matrix ˆ⇧xy2Rnx⇥nysuch
that:
ˆ⇧i,j
xy=exp((Fi
x·Fj
y)/⌧)Pny
k=1exp((Fix·Fky)/⌧)), (9)
where ⌧is scaling factor, and Fi
x,Fj
y2Rdrepresent d-
dimensional features of point ithin shape Xandjthin
shape Y, respectively. Similarly, the ˆ⇧yxis constructed in
the same fashion as in Eq. 9.
Feature alignment via OT. Finding precise point-to-point
mapping based on feature similarity requires solving lin-
ear assignment problem in Rd, which is expensive to inte-
grate into a learning-based framework. Therefore, in this
work, we relax the constraints to cast the feature-matching
problem as a probability distribution matching problem. In
other words, we represent the extracted features Fx,Fyas
probability distributions deﬁned over Rd. After that, we
attempt to learn mappings that minimize the “distance” be-
tween the two distributions, i.e. probability measures. The
OT cost [ 60] is a naturally ﬁtted discrepancy between proba-
bility measures, thereby being employed in our framework.
SW distance as an efﬁcient OT. Thanks to the fast com-
putation and its closed-form solution of SW distance, we
derive a novel loss function that jointly learns the mapping
and minimizes the discrepancy between two feature proba-
bility measures as follows:
LbiSW=(E✓⇠U(Sd 1)[Wp
p(✓]Fx,✓ ]ˆFy)
+Wp
p(✓]Fy,✓ ]ˆFx)])1
p,(10)
where ˆFx=ˆ⇧yxFxand ˆFy=ˆ⇧xyFy. The loss LbiSW
minimizes the discrepancy between the feature probabil-
ity measures in one shape and the softly permuted featuresets of its counterpart in a bidirectional manner. The loss
converges toward zero when the soft feature similarity ˆ⇧
approaches a (partial) permutation matrix, indicating that
the point-wise corresponding features are closely aligned.
Moreover, the loss encourages the cycle consistency of the
mapping. It is worth noting that our loss diverges from con-
trastive losses explored in prior works [ 11,28,62]. Where
the contrastive loss only considers whether individual point
correspondences are correct or not, our proposed loss intro-
duces a more general and ﬂexible matching by conceptu-
alizing the point features as probability measures and em-
ploying OT cost as a metric of evaluation.
Bidirectional EBSW. It is worth noting that the proposed
loss LbiSW in Eq. 10employs the projecting directions
sampled from uniform distribution over unit-hypersphere
as the shared slicing distributions. Despite being easy to
sample, the uniform distribution is not able to differen-
tiate between informative and non-informative projecting
features. Therefore, inspired by [ 38], we propose a bidi-
rectional energy-based SW loss deﬁned in the importance
sampling form as:
LbiEBSW =✓E✓⇠ 0(✓)[(W✓,X+W✓,Y)w(✓)]
E✓⇠ 0(✓)[w(✓)]◆1
p
,(11)
where we denote W ✓,X:=Wp
p(✓]Fx,✓ ]ˆFy),W✓,Y:=
Wp
p(✓]Fy,✓ ]ˆFx), and w(✓):=exp(W✓,X+W✓,Y)
 0(✓). The loss
LbiEBSW shares the same properties for shape correspon-
dence as the vanilla SW loss in Eq. 10. However, it imposes
a more expressive mechanism for selecting projection di-
rections in the computation of the SW distance. Moreover,
the vanilla SW loss can be seen as a summation of two SW
distances since the slicing distribution is ﬁxed as uniform.
In contrast, the bidirectional EBSW loss has the slicing
distribution shared and affected by both one-dimensional
Wasserstein distances. Hence, the bidirectional EBSW is
considerably different from the original EBSW in [ 38].
We provide detailed computation and discussion of
LbiSWandLbiEBSW at Sup. 10.
4.4. Loss functions
Proper functional maps. We employ the notion of proper
functional map introduced by [ 45]:The functional map Cxy
is deemed “proper” if there exists a (partial) permutation
matrix ⇧yxso that Cxy= †
y⇧yx x.Drawing on this
concept, we introduce a loss term that not only promotes the
“properness” of the functional map but also concurrently
regularizes the (OT) cost, namely:
Lproper =kCxy  †
yˆ⇧yx xk2(12)
It is worth noting that while our Lproper might bear re-
semblance to the coupling loss in [ 12], the proposed loss
23192
diverges by using soft feature similarity ˆ⇧yxjointly opti-
mized with the feature extrinsic alignment through OT as
discussed in Sec. 4.3. Therefore, it serves as a strong regu-
larization for imposing structural smoothness of functional
map and promoting precise mapping via OT.
Total loss. Our framework is trained end-to-end without an-
notation by minimizing the following unsupervised losses:
Ltotal= 1Lfmap+ 2LOT+ 3Lproper , (13)
where  iis the weight for each loss, and LOTcould be
either LbiSWorLbiEBSW .
4.5. Adaptive reﬁnement via entropic OT
Adaptive reﬁnement. To provide a more precise corre-
spondence, we propose an adaptive reﬁnement module de-
signed to incrementally improve the ﬁnal match for each
individual shape pairing. Speciﬁcally, we estimate the
pseudo soft correspondence ˜⇧via entropic regularized op-
timal transport [ 13] as mentioned in Eq. 3is deﬁned as:
˜⇧xy=QX(QY···(QX(p✏))), (14)
where Q(·)is the projection operator of a given probabil-
ity density p:Rd⇥Rd!Rdeﬁned as: p✏(x, y)/
exp(  1
✏c2(x, y)). Thanks to the differentiable property of
the Sinkhorn algorithm, we can reﬁne each individual pair
by minimizing the Ltotalto update the features accordingly.
In contrast to the axiomatic method [ 36] that often requires
alternately updating the functional map and pointwise map,
our method offers a differentiable process that facilitates si-
multaneous updates. Furthermore, it is noteworthy that our
approach is orthogonal to [ 18] since we only employ en-
tropic OT for reﬁnement once during the inference, thereby
reducing the computation and memory cost of the Sinkhorn
algorithm. We provide detailed algorithms of adaptive re-
ﬁnement at Sup. 10.
Inference. During inference, our ﬁnal mapping is obtained
by nearest neighbor search on features extracted from the
feature extractor module.
5. Experimental results
Datasets. We conduct a series of experiments across di-
verse shape-matching datasets and their application on a
downstream task. Speciﬁcally, we perform experiment on
human shape matching with near-isometric dataset such as
FAUST [ 7] and SCAPE [ 3] as well as non-isometric dataset
SHREC’19 [ 35]. Furthermore, our study extends to two
non-isometric animal datasets: SMAL [ 64] and the more
recent DeformingThings4D [ 29,34]. Finally, we conclude
our experiments by performing segmentation transfer on 3D
semantic segmentation dataset introduced in [ 1].Baselines. We conduct extensive comparisons with a
wide range of non-rigid shape matching methods: (1) Ax-
iomatic methods including ZoomOut [ 36], BCICP [ 43],
Smooth Shells [ 17]; (2) Supervised methods including
FMNet [ 33], GeomFMaps [ 15], TransMatch [ 57]; (3)
Unsupervised methods including SURFMNet [ 47], Deep
Shells [ 18], AFMap [ 28], SSLMSM [ 11], UDMSM [ 10],
ULRSSM [ 12]. While there are numerous non-rigid shape-
matching methods in the literature, we decided to choose
the most recent and relevant to our works for comparison.
Metrics. Regarding shape matching metric, similar to all of
our competing methods, we employ mean geodesic errors
(⇥100)[25]. For segmentation transfer, we use semantic
segmentation mIOU as in [ 24].
5.1. Near-isometric Shape Matching
Datasets. We employ a more challenging remeshed version
of FAUST [ 7] and SCAPE [ 3], as proposed in [ 15,43]. The
remeshed FAUST dataset includes 100 shapes, represent-
ing10individuals in 10different poses, with the evaluation
focusing on the ﬁnal 20shapes. Similarly, the remeshed
SCAPE dataset comprises 71poses of a single individual,
where again, the last 20shapes are used for evaluation pur-
poses. Additionally, the SHREC’ 19dataset presents a more
complex challenge due to its signiﬁcant variations in mesh
connectivity, encompassing 44shapes and 430 pairs for
evaluation.
Results. We conduct experiments on FAUST, SCAPE, and
the combination of both datasets. Quantitative results in
Tab. 1show that supervised methods tend to overﬁt the
trained dataset. On the other hand, unsupervised meth-
ods typically can achieve a better generalization on new
datasets. Compared to Deep Shells, an OT-based method,
we outperform in most settings as shown in Tab. 1and
Fig.2. Compared to state-of-the-art ULRSSM, our method
indicates a slightly better mapping demonstrated in Fig. 2.
5.2. Non-isometric Shape Matching
Datasets. We consider SMAL [ 64] and DeformingTh-
ings4D [ 29,34] for evaluating non-isometric shape match-
ing. For the SMAL dataset, we adopt the data split in [ 16]
that uses ﬁve species for training and three unseen species
for testing, resulting in a 29/20split of the dataset. Regard-
ing DeformingThings4D, denoted as DT4D-H, we follow
the split also presented in [ 16] comprising 198 samples for
training and 95for testing.
Results. To measure the performance on non-isometric
datasets, i.e. SMAL and DT4D-H, we compare our method
with previous state-of-the-art baselines as shown in Tab. 2.
Regarding the DT4D-H dataset, we only perform compar-
isons on the challenging intra-class scenario. Our proposed
23193
Table 1. Quantitative results on near-isometric shape matching. The color denotes the best and second -best result. Our method
outperforms various methods including axiomatic, supervised and unsupervised methods in most settings.
Method FAUST SCAPE FAUST + SCAPE
FAUST SCAPE SHREC’19 FAUST SCAPE SHREC’19 FAUST SCAPE SHREC’19
Axiomatic
ZoomOut [ 36] 6.1 \\\ 7.5 \\ \\
BCICP [ 43] 6.1 \\\ 11.0 \\ \\
Smooth Shells [ 17] 2.5 \\\ 4.7 \\ \\
Supervised
FMNet [ 33] 11.0 30.0 \ 33.0 17.0 \\ \\
GeomFMaps [ 15] 2.6 3.3 9.9 3.0 3.0 12.2 2.6 3.0 7.9
TransMatch [ 57] 1.8 32.8 19.0 18.5 16.0 39.5 1.7 13.5 12.9
Unsupervised
SURFMNet [ 47] 15.0 32.0 \ 32.0 12.0 \ 33.0 29.0 \
Deep Shells [ 18] 1.7 5.4 27.4 2.7 2.5 23.4 1.6 2.4 21.1
AFMap [ 28] 1.9 2.6 6.4 2.2 2.2 9.9 1.9 2.3 5.8
SSLMSM [ 11] 2.0 7.0 9.1 2.7 3.1 8.4 1.9 4.3 6.2
UDMSM [ 10] 1.5 7.5 20.1 3.2 2.0 28.3 1.7 7.6 28.7
ULRSSM [ 12] 1.6 3.6 7.2 1.9 1.9 7.6 1.7 3.2 4.6
Ours 1.5 3.4 5.5 1.6 1.8 7.0 1.6 2.2 4.7
Figure 2. Qualitative results of different methods evaluated on
SHREC’19 datasets. Correspondence is visualized by texture
transfer. The red arrow indicates poor mappings.
method outperforms previous methods in both dataset as
shown in Tab. 2. Visualization in Fig. 3shows that AFMap
often fails to retrieve a non-isometric mapping. In addition,
ULRSSM demonstrates better mapping despite some ambi-
guity. On the other hand, our method obtains a precise and
smooth mapping, thus visually better than the two state-of-
the-art methods.Table 2. Quantitative results for non-isometric matching on
SMAL and DT4D-H. Our method surpass state-of-the-art meth-
ods on challenging non-isometric dataset such as SMAL and
DT4D-H.
Method SMAL DT4D-H
Deep Shells [ 18] 29.3 31.1
GeoFMaps [ 15] 7.6 22.6
AFMap [ 28] 5.4 11.6
ULRSSM [ 12] 4.2 4.5
Ours 4.0 4.2
5.3. Segmentation transfer
Table 3. Quantitative results for 3D shape segmentation trans-
fer. Our method is effectively applied to semantic segmentation
transfer on 3D shapes, establishing a new benchmark for state-of-
the-art performance in this domain.
Method Coarse Fine-grained
AFMaps [ 28] 81.3 43.2
UDMSM [ 10] 85.3 45.2
ULRSSM [ 12] 84.2 58.2
Ours 87.8 60.5
Datasets. We illustrate the performance of our proposed
method on the task of segmentation transfer on 3D seman-
23194
Figure 3. Qualitative results of various methods on challenging
non-isometric SMAL dataset. Our method demonstrates superior
point mapping capabilities compared to previous works. More vi-
sualization is provided in Sup. 12.
Figure 4. Qualitative results of segmentation transfer. Our
method exhibits a high-quality segmentation map via computed
correspondence. More visualization is provided in Sup. 12.
tic segmentation dataset proposed in [ 1]. Speciﬁcally, the
dataset is derived from FAUST [ 7], which is manually an-
notated into two types of label: coarse annotations include
4classes and ﬁne-grained annotations comprise 17cate-
gories. After excluding non-connected meshes, we test our
method on 79meshes by computing correspondence among
the collection and then transferring annotation from one sin-
gle mesh to the others.
Results. To further demonstrate the robustness, we applyour methods on co-segmentation, also known as segmen-
tation transfer task. We train all methods on the remeshed
FAUST r mentioned in Sec. 5.1. It is worth noting while
the FAUST r is remeshed to around 10K faces, the segmen-
tation dataset in [ 1] is remeshed to 20K triangular faces.
Therefore, it showcases the generalization of our method
that does not depend on the discretization and resolution of
mesh. Tab. 3indicates that our method sets a new state-
of-the-art on the segmentation-transfer task on FAUST [ 1]
dataset in both coarse and ﬁne-grained annotation. Fig. 4
shows that our method is very closed to ground truth with-
out the need for training semantic segmentation models.
6. Ablation study
Table 4. Ablation study on SHREC’19. In the ﬁrst setting, we
replace LOTwith LMSEin Eq. 13. In the second row, we substi-
tuteLOTwith LuniSW. The third row indicates the LOTbeing
LbiSWas in Eq. 10. The fourth row indicates not using adaptive
reﬁnement at the end of the training process.
Ablation Setting SHREC’19
w.LMSE 34.3
w.LuniSW 4.9
w.LbiSW 4.8
w.o. adaptive reﬁnement 7.2
Ours 4.7
Settings. We conduct an ablation study to validate our con-
tribution. We train our model on FAUST+SCAPE dataset
and evaluate it on SHREC’19 dataset. Firstly, we evaluate
the effectiveness of different losses in the feature alignment
component. Furthermore, we also investigate the impor-
tance of the adaptive reﬁnement module.
Results. Our results are summarized in Tab. 4. First, by
comparing the ﬁrst row with the last row, we conclude that
LMSEcan not learn to align features for retrieving p2p cor-
respondence. Secondly, we observe that by using biSW, we
can gain a slightly better performance. Finally, the last row
indicates that by employing importance sampling energy-
based SW, we can even gain better performance.
7. Conclusion
In conclusion, we introduce an innovative framework
that integrates functional maps with an efﬁcient optimal
transport method, notably the sliced Wasserstein dis-
tance, to address computational challenges and enhance
feature alignment. Our approach signiﬁcantly outper-
forms existing methods in non-rigid shape matching
across various scenarios, including both near-isometric
and non-isometric forms. This advancement, con-
ﬁrmed through successful applications in tasks like
segmentation transfer, highlights our method’s efﬁ-
cacy and strong generalization potential in shape matching.
23195
References
[1]Ahmed Abdelreheem, Ivan Skorokhodov, Maks Ovsjanikov,
and Peter Wonka. Satr: Zero-shot semantic segmentation of
3d shapes. arXiv preprint arXiv:2304.04909 , 2023. 6,8
[2]Jason Altschuler, Jonathan Niles-Weed, and Philippe Rigol-
let. Near-linear time approximation algorithms for optimal
transport via Sinkhorn iteration. In Advances in Neural In-
formation Processing Systems , pages 1964–1974, 2017. 2
[3]Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Se-
bastian Thrun, Jim Rodgers, and James Davis. Scape: shape
completion and animation of people. In ACM SIGGRAPH .
2005. 6
[4]Souhaib Attaiki and Maks Ovsjanikov. Ncp: Neural corre-
spondence prior for effective unsupervised shape matching.
InNeurIPS , 2022. 1
[5]Souhaib Attaiki, Gautam Pai, and Maks Ovsjanikov. Dpfm:
Deep partial functional maps. In International Conference
on 3D Vision (3DV) , 2021. 1
[6]Mathieu Aubry, Ulrich Schlickewei, and Daniel Cremers.
The wave kernel signature: A quantum mechanical approach
to shape analysis. In ICCV , 2011. 1,3
[7]Federica Bogo, Javier Romero, Matthew Loper, and
Michael J Black. Faust: Dataset and evaluation for 3d mesh
registration. In CVPR , 2014. 1,6,8
[8]Nicolas Bonneel, Julien Rabin, Gabriel Peyr ´e, and Hanspeter
Pﬁster. Sliced and radon wasserstein barycenters of mea-
sures. Journal of Mathematical Imaging and Vision , 51:22–
45, 2015. 2,3
[9]Michael M Bronstein and Iasonas Kokkinos. Scale-invariant
heat kernel signatures for non-rigid shape recognition. In
CVPR , 2010. 1
[10] Dongliang Cao and Florian Bernard. Unsupervised deep
multi-shape matching. In ECCV , 2022. 1,2,6,7
[11] Dongliang Cao and Florian Bernard. Self-supervised learn-
ing for multimodal non-rigid 3d shape matching. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 17735–17744, 2023. 1,5,6,7
[12] Dongliang Cao, Paul Roetzer, and Florian Bernard. Unsu-
pervised learning of robust spectral shape matching. arXiv
preprint arXiv:2304.14419 , 2023. 1,2,5,6,7
[13] Marco Cuturi. Sinkhorn distances: Lightspeed computation
of optimal transport. Advances in neural information pro-
cessing systems , 26, 2013. 2,3,6
[14] Huong Quynh Dinh, Anthony Yezzi, and Greg Turk. Texture
transfer during shape transformation. ACM Transactions on
Graphics (ToG) , 24(2):289–310, 2005. 1
[15] Nicolas Donati, Abhishek Sharma, and Maks Ovsjanikov.
Deep geometric functional maps: Robust feature learning for
shape correspondence. In CVPR , 2020. 6,7
[16] Nicolas Donati, Etienne Corman, and Maks Ovsjanikov.
Deep orientation-aware functional maps: Tackling symme-
try issues in shape matching. In CVPR , 2022. 1,2,6
[17] Marvin Eisenberger, Zorah Lahner, and Daniel Cremers.
Smooth shells: Multi-scale shape registration with functional
maps. In CVPR , 2020. 3,6,7[18] Marvin Eisenberger, Aysim Toker, Laura Leal-Taix ´e, and
Daniel Cremers. Deep shells: Unsupervised shape corre-
spondence with optimal transport. NIPS , 2020. 1,3,6,7
[19] Danielle Ezuz and Mirela Ben-Chen. Deblurring and denois-
ing of maps between shapes. In Computer Graphics Forum .
Wiley Online Library, 2017. 3
[20] Dvir Ginzburg and Dan Raviv. Cyclic functional mapping:
Self-supervised correspondence between non-isometric de-
formable shapes. In Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part V 16 , pages 36–52. Springer, 2020. 1
[21] Oshri Halimi, Or Litany, Emanuele Rodola, Alex M Bron-
stein, and Ron Kimmel. Unsupervised learning of dense
shape correspondence. In CVPR , 2019. 1,2
[22] Kun Han, Shanlin Sun, Thanh-Tung Le, Xiangyi Yan, Haoyu
Ma, Chenyu You, and Xiaohui Xie. Hybrid neural diffeo-
morphic ﬂow for shape representation and generation via
triplane. In Proceedings of the IEEE/CVF Winter Confer-
ence on Applications of Computer Vision , pages 7707–7717,
2024. 1
[23] Qixing Huang, Fan Wang, and Leonidas Guibas. Functional
map networks for analyzing and exploring large shape col-
lections. ACM Transactions on Graphics (ToG) , 33(4):1–11,
2014. 2
[24] Sangpil Kim, Hyung-gun Chi, Xiao Hu, Qixing Huang, and
Karthik Ramani. A large-scale annotated mechanical com-
ponents benchmark for classiﬁcation and retrieval tasks with
deep neural networks. In Computer Vision–ECCV 2020:
16th European Conference, Glasgow, UK, August 23–28,
2020, Proceedings, Part XVIII 16 , pages 175–191. Springer,
2020. 6
[25] Vladimir G Kim, Yaron Lipman, and Thomas Funkhouser.
Blended intrinsic maps. ACM Transactions on Graphics
(ToG) , 30(4):1–12, 2011. 6
[26] Artiom Kovnatsky, Michael M Bronstein, Alexander M
Bronstein, Klaus Glashoff, and Ron Kimmel. Coupled quasi-
harmonic bases. In Computer Graphics Forum . Wiley Online
Library, 2013. 2
[27] Thanh Tung Le, Khai Nguyen, Kun Han, Nhat Ho, Xiao-
hui Xie, et al. Diffeomorphic mesh deformation via efﬁcient
optimal transport for cortical surface reconstruction. In The
Twelfth International Conference on Learning Representa-
tions , 2023. 3
[28] Lei Li, Nicolas Donati, and Maks Ovsjanikov. Learning
multi-resolution functional maps with spectral attention for
robust shape matching. NIPS , 2022. 1,2,5,6,7
[29] Yang Li, Hikari Takehara, Takafumi Taketomi, Bo Zheng,
and Matthias Nießner. 4dcomplete: Non-rigid motion esti-
mation beyond the observable surface. In ICCV , 2021. 6
[30] Tianyi Lin, Nhat Ho, and Michael Jordan. On efﬁcient opti-
mal transport: An analysis of greedy and accelerated mirror
descent algorithms. In International Conference on Machine
Learning , pages 3982–3991, 2019. 2
[31] Tianyi Lin, Nhat Ho, Xi Chen, Marco Cuturi, and Michael I.
Jordan. Fixed-support Wasserstein barycenters: Computa-
tional hardness and fast algorithm. In NeurIPS , pages 5368–
5380, 2020.
23196
[32] Tianyi Lin, Nhat Ho, and Michael I. Jordan. On the efﬁ-
ciency of entropic regularized algorithms for optimal trans-
port. Journal of Machine Learning Research (JMLR) , 23:
1–42, 2022. 2
[33] Or Litany, Tal Remez, Emanuele Rodola, Alex Bronstein,
and Michael Bronstein. Deep functional maps: Structured
prediction for dense shape correspondence. In ICCV , 2017.
1,2,6,7
[34] Robin Magnet, Jing Ren, Olga Sorkine-Hornung, and Maks
Ovsjanikov. Smooth non-rigid shape matching via effective
dirichlet energy optimization. In International Conference
on 3D Vision (3DV) , 2022. 6
[35] Simone Melzi, Riccardo Marin, Emanuele Rodol `a, Umberto
Castellani, Jing Ren, Adrien Poulenard, Peter Wonka, and
Maks Ovsjanikov. Shrec 2019: Matching humans with dif-
ferent connectivity. In Eurographics Workshop on 3D Object
Retrieval , 2019. 6
[36] Simone Melzi, Jing Ren, Emanuele Rodol `a, Abhishek
Sharma, Peter Wonka, and Maks Ovsjanikov. Zoomout:
spectral upsampling for efﬁcient shape correspondence.
ACM Transactions on Graphics (ToG) , 38(6):1–14, 2019. 3,
6,7
[37] Facundo M ´emoli. Gromov–wasserstein distances and the
metric approach to object matching. Foundations of com-
putational mathematics , 11:417–487, 2011. 2
[38] Khai Nguyen and Nhat Ho. Energy-based sliced wasserstein
distance. Advances in Neural Information Processing Sys-
tems, 2023. 4,5,2
[39] Trung Nguyen, Quang-Hieu Pham, Tam Le, Tung Pham,
Nhat Ho, and Binh-Son Hua. Point-set distances for learn-
ing representations of 3d point clouds. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 10478–10487, 2021. 3
[40] Maks Ovsjanikov, Mirela Ben-Chen, Justin Solomon, Adrian
Butscher, and Leonidas Guibas. Functional maps: a ﬂexible
representation of maps between shapes. ACM Transactions
on Graphics (ToG) , 31(4):1–11, 2012. 1,2,3
[41] Maks Ovsjanikov, Etienne Corman, Michael Bronstein,
Emanuele Rodol `a, Mirela Ben-Chen, Leonidas Guibas,
Frederic Chazal, and Alex Bronstein. Computing and pro-
cessing correspondences with functional maps. In SIG-
GRAPH ASIA 2016 Courses , pages 1–60. 2016. 2
[42] Gabriel Peyr ´e, Marco Cuturi, et al. Computational optimal
transport: With applications to data science. Foundations
and Trends® in Machine Learning , 11(5-6):355–607, 2019.
1
[43] Jing Ren, Adrien Poulenard, Peter Wonka, and Maks Ovs-
janikov. Continuous and orientation-preserving correspon-
dences via functional maps. ACM Transactions on Graphics
(ToG) , 37:1–16, 2018. 3,6,7
[44] Jing Ren, Mikhail Panine, Peter Wonka, and Maks Ovs-
janikov. Structured regularization of functional map compu-
tations. In Computer Graphics Forum . Wiley Online Library,
2019. 4,5
[45] Jing Ren, Simone Melzi, Peter Wonka, and Maks Ovs-
janikov. Discrete optimization for shape matching. In Com-
puter Graphics Forum . Wiley Online Library, 2021. 5[46] Emanuele Rodol `a, Luca Cosmo, Michael M Bronstein, An-
drea Torsello, and Daniel Cremers. Partial functional cor-
respondence. In Computer Graphics Forum . Wiley Online
Library, 2017. 2
[47] Jean-Michel Roufosse, Abhishek Sharma, and Maks Ovs-
janikov. Unsupervised deep learning for structured shape
matching. In ICCV , 2019. 1,2,6,7
[48] Yusuf Sahillio ˘glu. Recent advances in shape correspon-
dence. The Visual Computer , 36(8):1705–1721, 2020. 2
[49] Samuele Salti, Federico Tombari, and Luigi Di Stefano.
Shot: Unique signatures of histograms for surface and tex-
ture description. Computer Vision and Image Understand-
ing, 125:251–264, 2014. 1,2,3
[50] Nicholas Sharp, Souhaib Attaiki, Keenan Crane, and Maks
Ovsjanikov. Diffusionnet: Discretization agnostic learning
on surfaces. arXiv preprint arXiv:2012.00888 , 2020. 2,3,4
[51] Zhengyang Shen, Jean Feydy, Peirong Liu, Ariel H Curiale,
Ruben San Jose Estepar, Raul San Jose Estepar, and Marc
Niethammer. Accurate point cloud registration with robust
optimal transport. Advances in Neural Information Process-
ing Systems , 34:5373–5389, 2021. 3
[52] Justin Solomon, Gabriel Peyr ´e, Vladimir G Kim, and Suvrit
Sra. Entropic metric alignment for correspondence prob-
lems. ACM Transactions on Graphics (ToG) , 35(4):1–13,
2016. 2
[53] Robert W Sumner and Jovan Popovi ´c. Deformation transfer
for triangle meshes. ACM Transactions on Graphics (ToG) ,
23(3):399–405, 2004. 1
[54] Jian Sun, Maks Ovsjanikov, and Leonidas Guibas. A concise
and provably informative multi-scale signature based on heat
diffusion. In Computer graphics forum , pages 1383–1392.
Wiley Online Library, 2009. 2
[55] Mingze Sun, Shiwei Mao, Puhua Jiang, Maks Ovsjanikov,
and Ruqi Huang. Spatially and spectrally consistent deep
functional maps. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 14497–14507,
2023. 1
[56] Shanlin Sun, Thanh-Tung Le, Chenyu You, Hao Tang, Kun
Han, Haoyu Ma, Deying Kong, Xiangyi Yan, and Xiaohui
Xie. Hybrid-csr: Coupling explicit and implicit shape repre-
sentation for cortical surface reconstruction. arXiv preprint
arXiv:2307.12299 , 2023. 1
[57] Giovanni Trappolini, Luca Cosmo, Luca Moschella, Ric-
cardo Marin, Simone Melzi, and Emanuele Rodol `a. Shape
registration in the time of transformers. NIPS , 2021. 6,7
[58] Oliver Van Kaick, Hao Zhang, Ghassan Hamarneh, and
Daniel Cohen-Or. A survey on shape correspondence. In
Computer graphics forum , pages 1681–1707. Wiley Online
Library, 2011. 2
[59] C´edric Villani. Topics in optimal transportation . Number 58.
American Mathematical Soc., 2003. 3
[60] C´edric Villani. Topics in optimal transportation . American
Mathematical Soc., 2021. 5
[61] C´edric Villani et al. Optimal transport: old and new .
Springer, 2009. 1
[62] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas
Guibas, and Or Litany. Pointcontrast: Unsupervised pre-
training for 3d point cloud understanding. In Computer
23197
Vision–ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part III 16 , pages
574–591. Springer, 2020. 5
[63] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Fast global
registration. In Computer Vision–ECCV 2016: 14th Euro-
pean Conference, Amsterdam, The Netherlands, October 11-
14, 2016, Proceedings, Part II 14 , pages 766–782. Springer,
2016. 1
[64] Silvia Zufﬁ, Angjoo Kanazawa, David W Jacobs, and
Michael J Black. 3d menagerie: Modeling the 3d shape and
pose of animals. In CVPR , 2017. 6
23198
