MM-Narrator: Narrating Long-form Videos with
Multimodal In-Context Learning
Chaoyi Zhangπ*Kevin Lin∫Zhengyuan Yang∫Jianfeng Wang∫Linjie Li∫
Chung-Ching Lin∫Zicheng Liu∂Lijuan Wang∫
πUniversity of Sydney∫Microsoft∂Advanced Micro Devices
https://MM-Narrator.github.io
Figure 1. We present MM-Narrator, a training-free framework towards automatic audio description (AD) generation for long-form videos
via iterations: for each scene, it perceives multimodal inputs (i.e., seeing visual frames and hearing character dialogues ), recalls the
context AD depicting past scenes, and infers AD prediction for the current scene. Zoom in for details.
Abstract
We present MM-Narrator, a novel system leveraging
GPT-4 with multimodal in-context learning for the gener-
ation of audio descriptions (AD). Unlike previous methods
that primarily focused on downstream ﬁne-tuning with short
video clips, MM-Narrator excels in generating precise au-
dio descriptions for videos of extensive lengths, even be-
yond hours, in an autoregressive manner. This capability
is made possible by the proposed memory-augmented gen-
eration process, which effectively utilizes both the short-
*Work done during internship at Microsoft.term textual context and long-term visual memory through
an efﬁcient register-and-recall mechanism. These contex-
tual memories compile pertinent past information, includ-
ing storylines and character identities, ensuring an accu-
rate tracking and depicting of story-coherent and character-
centric audio descriptions. Maintaining the training-free
design of MM-Narrator, we further propose a complexity-
based demonstration selection strategy to largely enhance
its multi-step reasoning capability via few-shot multimodal
in-context learning (MM-ICL). Experimental results on
MAD-eval dataset demonstrate that MM-Narrator consis-
tently outperforms both the existing ﬁne-tuning-based ap-
proaches and LLM-based approaches in most scenarios,
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
13647
as measured by standard evaluation metrics. Additionally,
we introduce the ﬁrst segment-based evaluator for recur-
rent text generation. Empowered by GPT-4, this evaluator
comprehensively reasons and marks AD generation perfor-
mance in various extendable dimensions.
1. Introduction
Audio Description (AD) is an essential task that trans-
forms visual content into spoken narratives [ 1], primarily
assisting visual impairments in accessing video content.
Given its evident importance, the notable expectations for
AD to fulﬁll include complementing the existing audio dia-
logue, enhancing viewer understanding, and avoiding over-
lap with the original audio. This process involves identify-
ing not just who is present in the scene and what actions are
taking place, but also precisely how and when the actions
occur. Additionally, AD should capture subtle nuances and
visual cues across different scenes, adding layers of com-
plexity to its generation.
In addition to aiding visually impaired audiences, AD
also enhances media comprehension for autistic individuals,
supports eyes-free activities, facilitates child language de-
velopment, and mitigates inattentional blindness for sighted
users [ 25,47]. However, traditional human-annotated AD,
while detailed, incurs signiﬁcant costs and often suffers
from inconsistencies due to low inter-annotator agree-
ment [ 21], highlighting the need for automatic AD gen-
eration systems. Furthermore, AD serves as an emerging
testbed for benchmarking the capabilities of LLM/LMM
systems in long-form multimodal reasoning [ 21,22,30], to-
wards next-level advanced video understanding.
In this paper, we present MM-Narrator, a multimodal
AD narrator, to effectively leverage multimodal clues, in-
cluding visual, textual, and auditory elements, to enable
comprehensive perception and reasoning. In particular,
MM-Narrator distinguishes itself by naturally identifying
characters through their dialogues, in contrast to existing
methods that may underutilize subtitles [ 21,22].
Apart from an intricate multimodal understanding of the
video content, generating story-coherent AD for long-form
videos also relies on an accurate tracking and depicting
of character-centric evolving storylines over extended du-
rations, even spanning hours. This differs AD generation
from conventional dense video captioning [ 24,28,61,65]:
Unlike mere frame-by-frame scene description, AD should
weave a coherent narrative, utilizing characters as pivotal
elements to maintain an uninterrupted storytelling ﬂow [ 1].
To achieve contextual understanding, we propose to lever-
age both short-term and long-term memories to assist MM-
Narrator in its recurrent AD generation process. Speciﬁ-
cally, short-term textual memory sets the stage for generat-
ing coherent narrations, whereas long-term visual memoryaids in character re-identiﬁcation during long-form videos,
especially for scenes lacking dialogue.
As a GPT-4 empowered multimodal agent, MM-Narrator
could further beneﬁt from multimodal in-context learning
(MM-ICL) via our proposed complexity-based multimodal
demonstration selection. With complexity deﬁned with
the chain-of-thought (CoT) technique [ 63], MM-Narrator
could efﬁciently form and learn from a smaller candidate
pool of multimodal demonstrations, effectively improving
its multimodal reasoning capability in a few-shot approach.
This proposed complexity-based selection surpasses both
random sampling and similarity-based retrieval, which are
classic ICL solutions in choosing few-shot examples.
In summary, our contributions are four-folds: (1)We
present MM-Narrator, an automatic AD narrator for long-
form videos that can perceive multimodal inputs, recall past
memories, and prompt GPT-4 to produce story-coherent
and character-centric AD. (2)We propose a complexity-
based multimodal in-context learning (MM-ICL) to further
boost its AD generation performance with few-shot exam-
ples, offering new insights into the question “what makes
good ICL examples?” under complex text generation sce-
narios with multimodal reasoning needed. (3)Our training-
free MM-Narrator outperforms both ﬁne-tuning-based SO-
TAs and LLM/LMM baselines, including GPT-4V , in most
classic captioning metrics. (4)Furthermore, we introduce
the ﬁrst GPT-4 based evaluator for recurrent text genera-
tion, measuring more comprehensive AD generation qual-
ities at both text-level and sequence-level. Results suggest
that MM-Narrator generates AD comparable to human an-
notations across several considered aspects.
2. Related Work
Audio Description (AD) offers verbal narration of key
visual elements in videos [ 1], enriching the viewing ex-
perience for individuals who are blind or have low vision.
AD differs from video captioning [ 7,24,28,31,61,65],
which solely describes the visual content of a given video
clip. Instead, AD generation considers multiple modalities,
aiming to generate coherent narratives of storylines, char-
acters, and actions in a way that complements the regular
audio track. Initial studies [ 50,51,53,57] concentrated on
developing audio segmentation and transcription system
to collect high-quality video datasets with temporally
aligned ADs. These foundational efforts pave the way
for more advanced explorations in LSMDC [ 51]. Recent
research [ 22] has ventured into training transformer models
equipped with a frozen LLM. Researchers also incorporate
an external character bank [ 21] to enhance the accuracy
of AD generation. Different from prior works [ 21,22]
that rely on downstream ﬁne-tuning, our proposed MM-
Narrator generates accurate ADs in a training-free manner.
13648
Figure 2. MM-Narrator generates AD sequence for long-form videos via iterations.
LLM for Video Understanding. The remarkable success
of Large Language Models (LLMs) [ 8,14,15,17,44,58] has
sparked increasing interest in their application to video un-
derstanding. Recent works [ 6,12,27,30,37,54] generally
fall into two main categories: ( i) visual instruction tuning,
and ( ii) prompting LLMs. The ﬁrst approach [ 27,32,37,38]
typically ﬁne-tunes an LLM-based model. This involves in-
tegrating the pre-trained LLMs and additional trainable net-
works. The second category [ 4] involves prompting LLMs
to invoke specialized expert tools, transforming the input
video into a textual document, which then serves as input
to the LLMs for reasoning [ 6,12,30]. However, this strat-
egy may not be effective for processing lengthy or speech-
dense videos, as the LLMs often face challenges with ex-
cessive token lengths. Different from prior work, we pro-
pose to leverage short-term textual memory and long-term
visual memory with a register-and-recall mechanism, to ef-
fectively generate ADs for long-form videos.
In-Context Learning (ICL) [ 13,34,36,39,40], as a new
paradigm, allows LLMs to learn from a few examples with-
out needing parameter updates via downstream ﬁne-tuning.
This learning-from-analogy strategy [ 16] augments original
query question with a context formed by natural language
demonstrations. Existing studies highlight that the success
of ICL largely depends on the selection of effective demon-
strations. One common solution [ 33,52] is to form the
ICL prompt with closest neighbors, which are retrieved with
highest similarity to the query embedding. Other query-
based metrics are also explored in ﬁnding supportive ICL
examples on the basis of query content, such as mutual
information [ 55,56] and perplexity [ 20]. Although prior
works have demonstrated their superiority in text classiﬁ-
cation tasks or open-domain QA [ 16], they have not ex-
plored ICL on complex text generation tasks under multi-
modal scenarios. In this work, we propose to quantify the
demonstration complexity as the number of reasoning steps
in chain-of-thoughts (CoTs) [ 63,68], and select the mostintuitive examples to improve AD generation with few-shot
MM-ICL.
3. Method
Given a long-form video V, consisting of multiple video
clips {vt}, MM-Narrator generates an AD sequence {Tt}
in an autoregressive manner, as shown in Figure 2.W e
ﬁrst present MM-Narrator, a multimodal narrator that con-
ducts recurrent AD generation via prompting GPT-4 (§ 3.1).
Building upon MM-Narrator, we propose the complexity-
based MM-ICL to further enhance its multimodal reason-
ing capabilities through intuitive few-shot demonstrations
(§3.2). Notably, the entire MM-Narrator framework oper-
ates in a training-free manner.
3.1. Recurrent AD Narrator
At each iteration of scanning through a speciﬁc long-
form video, MM-Narrator utilizes multimodal experts for
perception, recalls past memories in both short-term and
long-term contexts, and prompts LLM to generate an audio
description. We describe each step as below.
Multimodal perceptions. We employ specialized vision
and audio expert models to extract multimodal informa-
tion from the input video clip. These off-the-shelf mul-
timodal models are employed as integral tools within our
MM-Narrator framework. We denote a video clip consist-
ing of Nframes with timestamp tasvt={I1,I2,. . . ,IN}.
We deploy vision experts [ 48,60,64] to gather visual per-
ceptions, which involves obtaining per-frame visual fea-
tures and text-formed outputs. Speciﬁcally, for each frame
Ii, we collect CLIP-ViT features xCLIP
i, image captions
xcap
i, and people detections xdet
i. Alongside these crucial
visuals, we observe the spoken dialogues play a profound
role, which is underutilized in existing approaches [ 21,22].
The spoken dialogues not only offer information comple-
mentary to the visuals, but also primarily serve as the
13649
only access to identify characters with their names when
no external video metadata is given. To be speciﬁc, we
concatenate the subtitles within a certain time window
Tsubasxsub
t"Tsub. These subtitles can be sourced from
the Internet or generated through automated speech recog-
nition (ASR) as an audio expert [ 10]. To summarize,
for a given video clip vt, the multimodal experts pro-
duce a comprehensive tuple of perception clues Xt=
({xCLIP
i},{xcap
i},{xdet
i},xsub
t"Tsub), where {x 
i}denotes
the per-frame outputs. Among these, {xcap
i}andxsub
t"Tsub
are directly used in constructing LLM prompts, while the
others facilitate the proposed register-and-recall mechanism
for long-term character re-identiﬁcation.
Short-term memory queue. To equip MM-Narrator
with contextual understanding for coherent AD genera-
tion, we maintain a short-term memory queue Mshort =
{Tt K,. . . ,Tt 1}to contain the Kmost recently predicted
ADs with timestamps. The short-term memory queue will
be updated over time during inference. This lightweight
textual queue is instrumental in creating story-coherent AD
narrations, enabling visually impaired audiences to follow
the storytelling more intuitively.
Long-term visual memory. To endow MM-Narrator with
the ability to recall characters identiﬁed in previous video
clips, we construct a frame-level character re-identiﬁcation
visual bank. This visual bank, designed for long-term
use, is operated by a register-and-recall mechanism as fol-
lows: (1) we register xCLIP
j as the visual signature for
each globally-indexed frame Ijin all previous video clips
Ij"{v1,v2,. . . ,v t 1}, and (2) for each current frame Ii,
we ﬁrst ﬁlter-out the invalid matches resulting in nonposi-
tive cosine similarity Sim cos(xCLIP
i,xCLIP
j), and then re-
trieve the past predicted AD which owns the highest simi-
larity to the current visual signature xCLIP
i. For simplicity,
this mechanism is activated only when a single individual is
detected in a frame (i.e., ∂xdet
 ∂=1), typically in close-up
shots of the character, making frame-level CLIP-ViT fea-
tures [ 48] compatible for character re-identiﬁcation. Given
any AD that covers multiple frames, this frame-level visual
retriever supports the MM-Narrator in re-identifying mul-
tiple characters appearing in the video clip. Additionally,
the retrieval candidate pool is reﬁned to include only past
predicted ADs where person named entities are recognized
through a Named Entity Recognition (NER) tool [ 18]. This
strategy focuses MM-Narrator on the main characters who
contribute to the past storyline.
Prompting LLM for AD generation. Gathering all
aforementioned text-formed outputs, MM-Narrator builds
prompts to query GPT-4 for recurrent AD generation.
Speciﬁcally, the input prompt contains the following el-
ements: task introduction, visual captions ( xcap
i) with
successfully re-identiﬁed characters, recent context ADs(Mshort ) and character dialogues ( xsub
t"Tsub). Noticeably,
we also found that adding task-speciﬁc hints into the prompt
could empirically beneﬁt overall AD generation, which we
attribute as an explicit attention guidance via prompt engi-
neering. A breakdown of our AD generation prompt con-
structed by MM-Narrator, is provided in the supplementary
(Figure 7).
3.2. Multimodal In-Context Learning
In this section, we further extend MM-Narrator with
multimodal in-context learning (MM-ICL) on few-shot ex-
amples. Our exploration begins by examining two primary
methods of demonstration selection: random and similarity-
based approaches. We then critically evaluate the question,
“What makes for effective ICL examples? ” and propose a
complexity-based MM-ICL approach to improve the mul-
timodal reasoning capability with the most intuitive multi-
modal demonstrations.
Random MM-ICL. Firstly, we build an in-context learning
(ICL) demonstration pool, denoted as P, from the training
dataset. Each demonstration within the pool is composed of
a pair (Q,A), where Qrepresents the text-formed question
created using multimodal experts, and Ais the correspond-
ing ground-truth AD, serving as the answer . Then, for each
test query q, we randomly sample Cdemonstrations from
Pto facilitate the ICL process.
Furthermore, we are further interested in two essential
questions: “What makes good examples for AD genera-
tion?” and“How to ﬁnd and use them for ICL?”
Similarity-based MM-ICL. A common approach, as sug-
gested in existing literature [ 33], is to identify “good ex-
amples” based on similarity, employing a k-NN algorithm
to select examples that exhibit the highest ksimilarity be-
tween the embeddings of Qiand the test query q. This solu-
tion expects to ﬁnd supportive examples to beneﬁt few-shot
performance via a “soft-copy” mechanism [ 23,43], which
is often used in text classiﬁcation tasks, such as sentiment
analysis, or relatively-simple text generation task such as
open-domain QA [ 16].
However, we empirically ﬁnd that this similarity-based
approach does not manage to enhance the ICL capability
for AD generation, regardless of whether the retrieved ex-
amples are presented in descending order [ 33] or ascending
order [ 16]. We hypothesize that for complex text generation
tasks such as AD generation, which requires multimodal
perception and reasoning, similarity or relevance may not
be the most suitable criteria for identifying effective ICL
examples for improving overall performance.
Complexity-based MM-ICL. Our empirical analysis re-
veals that not all questions are equally challenging, in terms
of the complexity of multimodal fusion. Take Figure 1as
an example: when comparing Titanic (1997) toSpider Man
(2018) , the latter presents a more complex case. It requires
13650
Figure 3. Our proposed SegEval evaluator to measure recurrent text generation quality with GPT-4 under customized marking criteria.
Noticeably, GPT-4 is agnostic to the source of each assistant output (i.e., which Segis GT or PD), and it would measure Segquality taking
oracle contexts into consideration. Take the response shown above as example, its corresponding re-scaled ris 2.25. Zoom in for details.
the inference that “Peter and Spider Man are the same char-
acter”, a deduction drawn from context AD and subtitles,
alongside describing his actions from visual frames, en-
riched by contextual understanding from the context AD.
This observation led us to hypothesize that complexity
could be a more suitable metric for identifying effective
ICL examples for tasks involving intricate multimodal fu-
sion. To this end, we propose to query LLM to articulate
the chain-of-thoughts (CoTs) as reasoning steps, denoted as
R, that assist in deriving the answer Afrom the question Q.
This process evolves our demonstration format from simple
(Q,A)pairs to more comprehensive (Q,R,A)tuples.
Instead of the conventional random sampling from the
entire pool P, we propose selecting the most straight-
forward examples, quantiﬁed by the shortest number of
reasoning steps. These are compiled into a simpler subset
pool Psimple , from which we conduct our demonstration
sampling. This method ensures the inclusion of more
intuitive and concise examples in our MM-ICL process.
We present detailed ablation study in § 5.4, validating
that complexity serves as a robust measure for selecting
effective ICL examples for improving AD generation.
4. Segment-based GPT-4 Evaluator
The lack of standard AD annotation guidelines, varying
cultural background and preferences of human annotators
imply that AD is an inherently subjective recurrent text gen-
eration process, leading to notable inter-annotator disagree-
ments [ 21] and challenges in evaluation using traditional
reference-based captioning metrics. To this end, inspired
by [32,35], we propose a segment-based GPT-4 evaluator
SegEval to measure the recurrent AD generation, in terms
of multi-domain qualities.
Suppose LADs form one segment Seg. For each Seg,
the evaluator takes into consideration an oracle context win-
dow Ctxof length W, to measure its multi-aspect scores.
Speciﬁcally, we gather W- 1 adjacent segments to formCtx, which consists ofW 1
2past andW 1
2future segments
surrounding the targeted Seg. Given a pair of predicted
(PD) and ground-truth (GT) AD segments, SegEval would
treat them as outputs of two separate AD generation sys-
tems, and query GPT-4 to reason and mark their raw marks
independently. The ﬁnal score is calculated as the ratio rof
these raw marks between predicted and human-annotated
AD, via post-processing. If the re-scaled ris higher than
1.0, it indicates that GPT-4 might favour the predicted AD
over human annotations under the speciﬁc aspect. Besides,
this rescaling operation makes it comparable among differ-
ent approaches, sharing human annotations as the marking
standard. Noticeably, although GPT-4 is unaware of the seg-
ment source that which Segis the GT or PD, we always
form Ctxfrom GT annotations to set the oracle for investi-
gating contextual inﬂuences.
Overall, as shown in Figure 3,SegEval can measure
context-irrelevant, short-context and long-context scores by
ﬂexibly changing the value of W. For example, it could
measure text-level qualities such as originality and consis-
tency (when W=1), while it could also mark sequence-
level qualities such as coherence, diversity and speciﬁcity
(when W>1). The details of each marking criteria are
provided in supplementary (§ D).
5. Experiments
5.1. Evaluation Setup
Datasets. We conduct experiments on the AD genera-
tion benchmark established in AutoAD [ 22], where MAD-
v2-Named and MAD-eval- Named are released as training
and testing splits, respectively. MAD-v2- Named consists
of 334,296 ADs and 628,613 subtitles from 488 movies,
while MAD-eval- Named is compromised of 6,520 ADs
and 10,602 subtitles from 10 movies.
Metrics. Following AutoAD [ 22], we report three tradi-
tional captioning metrics to measure the quality of ADs gen-
13651
erated versus human-annotated ones, including ROUGE-
L[29](R-L), CIDEr [ 59](C) and SPICE [ 9](S). Be-
sides, we follow AutoAD-II to benchmark the text sequence
generation over their recall-based metric ‘Recall@k within
Neighbours’ ( R@k/N ), where the text similarity is mea-
sured by BertScore [ 67]. We also report Bleu-1 [ 46] and
METEOR [ 11] for ablation studies. To reduce experimen-
tal variability, each experiment of MM-Narrator is repeated
three times in Tables 1to6, as well as Figure 5, with mean
(and std) reported.
5.2. Comparison with State-of-the-Art Approaches
Fine-tuning-based SOTAs. We ﬁrst compare our training-
free framework against the ﬁne-tuning-based SOTAs, in-
cluding ClipCap [ 41], ClipDec [ 42] and AutoAD-I [ 22]. As
shown in Table 1, our training-free approach outperforms
its ﬁne-tuning-based counterparts [ 22,41,42], in terms of
ROUGE-L, SPICE and R@k/N, especially the AutoAD-
I[22] (R-L 12.1 vs 11.9; S 4.5 vs 4.4; R@k/N 48.0 vs
42.1) which is proposed to conduct partial data pretraining
over an extra large-scale text-only A V-AD dataset [ 2,22]
(consisting of 3.3M ADs from over 7k movies) to address
the lack of paired training data for AD generation. Unlike
[21,22] who report to struggle with beneﬁting from charac-
ter dialogues, our MM-Narrator could better integrate mul-
timodal information and effectively identify characters from
appropriate subtitle usage (shown as model D in § 5.3).
Training-free LLM/LMM Baselines. We next com-
pare our MM-Narrator with LLM and LMM baselines: (a)
VLog [ 6] and (b) VideoChat-Text [ 27] are two LLM-based
methods for multimodal video understanding. They convert
multimodal perceptions into natural languages via several
pretrained models [ 26,49,62,64], and then utilizes a LLM
to generate texts based on task-speciﬁc prompts. To make a
fair comparison, we make them query GPT-4 with the same
AD generation prompt as we use in MM-Narrator. (c) MM-
Vid [ 30] is a LMM system which generates AD through
incorporating external knowledge with clip-level video de-
scription generated by GPT-4V [ 45,66].
As shown in Table 2, our MM-Narrator ( w/o MM-ICL )
would outperform VLog and VideoChat, which is mainly
attributed to the proposed short-term memory queue and
long-term visual memory to effectively leverage relevant
contextual information recalled from past ADs. In addi-
tion, while MM-Narrator is based on GPT-4 (text-only),
it also surpasses the GPT-4V(ision) based MM-Vid sys-
tem in terms of R-L and SPICE. The results suggest that
a memory-augmented LLM can be comparably valuable to
the perception-enhanced ones. Furthermore, with our pro-
posed MM-ICL, MM-Narrator outperforms these training-
free LLM/LMM counterparts by a large margin. Finally,
the bottom two rows of Table 2further validate the effec-
tiveness of the proposed MM-ICL design.Method Training-Free R-L (  )C (  )S (  ) R@5/16 (  )
ClipCap [ 41] 7 8.5 4.4 1.1 36.5
ClipDec [ 42] 7 8.2 6.7 1.4 -
AutoAD-I [ 22] 7 11.9 14.3 4.4 42.1
MM-Narrator ≥ 12.1 11.6 4.5 48.0
Table 1. Comparisons with ﬁne-tuning-based state-of-the-art
methods on MAD-eval- Named benchmark. Note: the random
guess will result in a R@5/16 of 31.3%.
Method LLM/LMM R-L (  )C (  )S (  ) R@5/16 (  )
VLog [ 6] GPT-4 7.5 1.3 2.1 42.3
VideoChat [ 27] GPT-4 7.9 2.4 1.8 42.5
MM-Vid [ 30] GPT-4V 9.8 6.1 3.8 46.1
MM-Narrator
w/oMM-ICL GPT-4 10.3 4.9 3.8 47.1
w/MM-ICL GPT-4 12.1 11.6 4.5 48.0
Table 2. Comparisons with training-free LLM/LMM baselines on
MAD-eval- Named benchmark.
Method Training-Free R-L (  )C (  )S (  ) R@5/16 (  )
AutoAD-II † [ 21] 7 13.4 19.5 - 50.8
MM-Narrator † ≥ 13.4 13.9 5.2 49.0
Table 3. Evaluation on MAD-eval- Named benchmark, with an
external character bank annotated and utilized for improved char-
acter recognition (denoted as †).
Utilizing External Character Bank. Previously, all dis-
cussed methods share the same and only knowledge source
to assist in character recognition. More speciﬁcally, they,
like us humans, mostly identify characters and infer their
names through hearing (i.e., auditory cues) alone when
watching movies. Given this single source of gaining char-
acter information, our MM-Narrator would convey contex-
tual information via retrieving visual and temporal memo-
ries. However, these methods suffer from an unavoidable
limitation: The character identities would unfortunately re-
main mystery until their names are being ﬁrst-time called in
dialogues.
To alleviate that, following AutoAD-II [ 21] we also in-
vestigate how our method could beneﬁt from incorporating
an external character bank. To construct this character bank,
[21] exploits actor portrait images (from an external movie
database) to retrieve a few most similar frames for each
main character in each movie. Unlike [ 21] who trains an
auxiliary character recognizer from these retrieved frames,
we maintain our training-free designs by simply concate-
nating these frames into short video clips to introduce each
character (with ADs as their names). Next, we prepend
these video clips to the long-form videos, such that they
could work compatibly with our register-and-recall mecha-
nism. As shown in Table 3, our MM-Narrator ( w/ ExtChar-
Bank ) could further boost its performance and generate out-
comes comparable to the ﬁne-tuning-based AutoAD-II.
Qualitative Results. Qualitative comparisons over MAD-
13652
Figure 4. Qualitative comparisons between ClipCap, MM-Vid, AutoAD-II, and our MM-Narrator, where the latter two approaches are
equipped with the external character bank. The movies are from (a) Signs (2002) , (b) Ides of March (2011) , (c)Charlie St. Cloud (2010) ,
and (d) Les Mis ´erables (2012) . Zoom in for details.
eval dataset are shown as Figure 4, while the qualita-
tive demonstrations of applying our MM-Narrator on other
long-form videos (external to the MAD-eval dataset) are
shown in Figure 1. Additional qualitative results are in-
cluded in supplementary (Figure 10and11).
5.3. Building MM-Narrator From Image Captioner
As shown in Figure 5, we quantitatively demonstrate
how our training-free MM-Narrator are developed step by
step. Starting from (A) an image captioner, we elaborate
how multimodal perception beneﬁts MM-Narrator to form
an intricate multimodal understanding over video content.
Speciﬁcally, it includes adding (B) multiple frames, (C)
subtitles, and (D) a task-speciﬁc hint1. Noticeably, simply
adding the dialogues (C) might not result in an immediate
performance gain. However, with prompt engineering in
(D), MM-Narrator pays more attention to effectively lever-
age multimodal clues for character-centric AD generation.
Next, we illustrate how we transform MM-Narrator into
recurrent AD narrator to produce story-coherent AD, with
incorporation of past memories and complexity-based MM-
ICL. Speciﬁcally, MM-Narrator maintains (E) a short-term
memory queue, learns from (F) multimodal demonstra-
tions via MM-ICL, and retrieves (G) long-term visual mem-
ory for character re-identiﬁcation, which could be further
boosted with (H) an external character bank.
5.4. Ablations on Multimodal In-Context Learning
We investigated three groups of MM-ICL proposed to
augment the baseline. Speciﬁcally, we built random R1
and similarity-based S1, by adapting classic ICL tech-
niques [ 13,33] from conventional NLP tasks into multi-
1“Hint: try to infer character names from subtitles for AD generation. ”
Figure 5. Ablations on each component for MM-Narrator.
Model Pool SizeDemo.CoT R-L (  )C (  ) B-1 (  )Format
Baseline w/o MM-ICL
B1 - - 711.8 ±0.18.6±0.19.7±0.2
Random MM-ICL
R1 100% (Q,A) 713.2 ±0.112.9 ±0.212.2 ±0.1
R2 100% (Q,R,A)≥13.4 ±0.113.4 ±0.212.7 ±0.1
R3 10% random (Q,A) 713.3 ±0.113.0 ±0.112.3 ±0.0
R4 10% random (Q,R,A)≥13.3 ±0.113.4 ±0.112.6 ±0.0
Similarity-based MM-ICL
S1 100% (Q,A) 713.5 ±0.013.1 ±0.012.6 ±0.1
Complexity-based MM-ICL
C1 10% shortest (Q,A) 713.2 ±0.113.3 ±0.312.3 ±0.1
C2 10% shortest (Q,R,A)≥13.4 ±0.013.9 ±0.112.8 ±0.0
C3 10% longest (Q,R,A)≥13.3 ±0.112.7 ±0.212.4 ±0.1
Table 4. Our different MM-ICL designs for MM-Narrator †. The
baseline (B1) and
each representative MM-ICL implementation (R1, S1 and C2) are
highlighted.
modal AD generation. Next, we presented our complexity-
based design as C2.
13653
MethodLLM/LMMText-level QualitySequence-level QualityContext-irrelevant ScoresShort-context Scores Long-context ScoresOrig.Cons.Cohe.Dive.Spec.Cohe.Dive.Spec.±0.02±0.02±0.01±0.06±0.04±0.01±0.01±0.03GT-1.001.001.001.001.001.001.001.00ClipCap [41]GPT-20.43 0.420.26 0.35 0.350.26 0.42 0.33VLog [6]GPT-41.03 0.880.34 0.55 0.520.32 0.57 0.43MM-Vid [30]GPT-4V0.85 0.780.51 0.81 0.660.53 0.84 0.62MM-NarratorGPT-41.05±0.101.03±0.050.52±0.060.70±0.060.66±0.040.57±0.050.70±0.020.61±0.05MM-NarratorGPT-4V1.49±0.101.45±0.050.94±0.071.01±0.041.13±0.080.87±0.041.05±0.041.14±0.05MM-Narrator †GPT-40.95±0.021.06±0.010.62±0.040.75±0.010.76±0.010.62±0.040.80±0.030.71±0.03MM-Narrator †GPT-4V1.45±0.141.46±0.040.98±0.031.06±0.041.24±0.090.94±0.021.09±0.051.12±0.03Table 5. Evaluating AD generation with SegEval on MAD-eval- Named benchmark, with segment size Lset to 5. The context window
sizes Ware set as 1 / 3 / 11 to compute context-irrelevant / short-context / long-context scores, respectively. Orig .,Cons .,Cohe .,Dive .,
andSpec .stand for originality ,consistency ,coherence ,diversity , and speciﬁcity , respectively. The scoring variances of these GPT-4
evaluators are denoted below for references, which are estimated by three repeated evaluations over the same inference outputs. These re-
scaled scores measure the corresponding AD prediction (PD) qualities of each speciﬁc method, compared to the shared marking standards
set by ground-truth (GT) ADs. For example, given a pair of PD and GT segments, without revealing to the evaluator which segment is GT
or PD, if it reasons and marks the raw qualities ( R.Q.) as 8 and 5 for PD and GT segments, respectively, we derive the re-scaled score ras
R.Q.PD
R.Q.GT=8
5=1.6. † indicates our incorporation with ExtCharBank.
Method R-L (  )C (  )M (  ) B-1 (  )
MM-Narrator
+ GPT-4 12.1±0.411.6±0.45.7±0.211.8±0.3
+ GPT-4V 11.8±0.17.0±0.26.5±0.19.3±0.1
MM-Narrator †
+ GPT-4 13.4±0.013.9±0.16.7±0.012.8±0.0
+ GPT-4V 12.8±0.09.8±0.27.1±0.010.9±0.0
Table 6. Comparisons over classic reference-based captioning
scores, when incorporating our MM-Narrator with GPT-4V .
The results as shown in Table 4, verify our hypothesis
that complexity serves as an appropriate measure for select-
ing effective ICL demonstrations for improving AD gener-
ation. It also indicates that our proposed complexity-based
design (C2) is more preferable than classic ones (R1, S1)
for AD generation, especially the CIDEr score. In sup-
plementary (§ C), we further discuss three sub-questions to
elaborate an in-depth analysis, including 1) Does CoT help?
2)Are more intuitive examples helpful for AD Generation?
and 3) Does complexity-based MM-ICL work effectively?
5.5. Evaluating AD Generation with GPT-4
In Table 6, we observe a few performance drop on clas-
sic reference-based captioning scores when incorporating
MM-Narrator with GPT-4V [ 45]. As shown in Figure 4,
the decrease in performance can be primarily attributed to
the more detailed and much richer ADs generated by our
method, which diverge from the typically shorter human-
annotated ADs in MAD-eval- Named . This suggests that
taking human annotated AD as oracles to measure AD-level
captioning scores might be unsuitable for advanced LMM
approaches, which further motivates our proposal of evalu-
ating recurrent text generation with GPT-4.
Adjusting W, our proposed SegEval could ﬂexibly
measure both text-level andsequence-level qualities. Asshown in Table 5, the performance ranking order observed
inSegEval aligns with our other experimental results, val-
idating the reliability of SegEval as an evaluation tool, ex-
cept for GPT-4V based MM-Vid where ours falls short on
diversity . Furthermore, when employing GPT-4V as our vi-
sion expert, MM-Narrator not only outperforms others by a
large margin, but also closely mirrors the quality of human
annotated ADs in multiple aspects, gaining more favor from
the source-agnostic GPT-4 evaluator.
Compared to classic reference-based captioning scores,
SegEval could better reﬂect the recurrent text generation
qualities with GPT-4. One human validation on SegEval
is shown in Figure 3, and more examples can be found in
supplementary (Figure 9). Moreover, SegEval could be
easily extended to support more comprehensive evaluation
perspectives by querying it with extra customized marking
criteria.
6. Conclusion
MM-Narrator represents a signiﬁcant leap in automatic
audio description (AD) generation for long-form videos,
leveraging the power of GPT-4 and innovative multimodal
in-context learning (MM-ICL). This recurrent AD narra-
tor excels in generating story-coherent and character-centric
AD by combining immediate textual context with long-term
visual memory. Its training-free design, coupled with our
proposed complexity-based MM-ICL demonstration selec-
tion strategy, outperforms both existing ﬁne-tuning-based
and LLM-based approaches in most scenarios, as measured
by traditional captioning metrics. Furthermore, we intro-
duce a GPT-4 empowered evaluator for a more comprehen-
sive measurement of recurrent text generation qualities. Its
results suggest that MM-Narrator generates AD comparable
to human annotations across several considered aspects.
13654
References
[1]American council of the blind. https://adp.acb.
org/ .2
[2]Audiovault. https://audiovault.net/ .6
[3]Google text-to-speech: a python library and cli tool to in-
terface with google translate’s text-to-speech api. https:
//gtts.readthedocs.io/en/latest/ .13
[4]Langchain. https://langchain.readthedocs.
io/.3
[5]Pyscenedetect: Video scene cut detection and analysis tool.
https://www.scenedetect.com/ .13
[6]VLog: Video as a Long document. https://https:
//github.com/showlab/VLog , 2023. GitHub reposi-
tory. 3,6,8
[7]Nayyer Aafaq, Ajmal Mian, Wei Liu, Syed Zulqarnain Gi-
lani, and Mubarak Shah. Video description: A survey of
methods, datasets, and evaluation metrics. ACM Computing
Surveys , 52(6):1–37, 2019. 2
[8]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katherine Millican, Malcolm Reynolds, et al. Flamingo: a
visual language model for few-shot learning. Advances in
Neural Information Processing Systems , 35:23716–23736,
2022. 3
[9]Peter Anderson, Basura Fernando, Mark Johnson, and
Stephen Gould. SPICE: Semantic propositional image cap-
tion evaluation. In ECCV , 2016. 6
[10] Max Bain, Jaesung Huh, Tengda Han, and Andrew Zisser-
man. WhisperX: Time-accurate speech transcription of long-
form audio. INTERSPEECH 2023 , 2023. 4,13
[11] Satanjeev Banerjee and Alon Lavie. METEOR: An auto-
matic metric for mt evaluation with improved correlation
with human judgments. In ACL workshop , 2005. 6
[12] Aanisha Bhattacharya, Yaman K Singla, Balaji Krishna-
murthy, Rajiv Ratn Shah, and Changyou Chen. A video
is worth 4096 tokens: Verbalize story videos to understand
them in zero shot. EMNLP , 2023. 3
[13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS , 33:1877–
1901, 2020. 3,7,13
[14] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-
hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-
hao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.
Xing. Vicuna: An open-source chatbot impressing GPT-4
with 90% ChatGPT quality, March 2023. 3
[15] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al. PaLM: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311 , 2022. 3
[16] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong
Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang
Sui. A survey for in-context learning. arXiv preprint
arXiv:2301.00234 , 2022. 3,4[17] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-
E: An embodied multimodal language model. arXiv preprint
arXiv:2303.03378 , 2023. 3
[18] Jenny Rose Finkel, Trond Grenager, and Christopher D Man-
ning. Incorporating non-local information into information
extraction systems by gibbs sampling. In ACL, pages 363–
370, 2005. 4
[19] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and
Tushar Khot. Complexity-based prompting for multi-step
reasoning. arXiv preprint arXiv:2210.00720 , 2022. 12
[20] Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith,
and Luke Zettlemoyer. Demystifying prompts in lan-
guage models via perplexity estimation. arXiv preprint
arXiv:2212.04037 , 2022. 3
[21] Tengda Han, Max Bain, Arsha Nagrani, G ¨ul Varol, Weidi
Xie, and Andrew Zisserman. AutoAD II: The Sequel - who,
when, and what in movie audio description. In ICCV , 2023.
2,3,5,6
[22] Tengda Han, Max Bain, Arsha Nagrani, G ¨ul Varol, Weidi
Xie, and Andrew Zisserman. AutoAD: Movie description in
context. In CVPR , 2023. 2,3,5,6,13
[23] Xiaochuang Han, Daniel Simig, Todor Mihaylov, Yulia
Tsvetkov, Asli Celikyilmaz, and Tianlu Wang. Understand-
ing in-context learning via supportive pretraining data. arXiv
preprint arXiv:2306.15091 , 2023. 4
[24] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and
Juan Carlos Niebles. Dense-captioning events in videos. In
ICCV , pages 706–715, 2017. 2
[25] Elisa Lewis. Deep dive: How audio description beneﬁts ev-
eryone, 2021. Accessed on: 2023-11-13. 2
[26] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2: bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML ,
2023. 6
[27] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wen-
hai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu
Qiao. Videochat: Chat-Centric video understanding. arXiv
preprint arXiv:2305.06355 , 2023. 3,6
[28] Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen,
Rohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang,
William Yang Wang, et al. V ALUE: A multi-task bench-
mark for video-and-language understanding evaluation. In
NeurIPS Datasets and Benchmarks Track , 2021. 2
[29] Chin-Yew Lin. ROUGE: A package for automatic evaluation
of summaries. ACL, 2004. 6
[30] Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin,
Ehsan Azarnasab, Zhengyuan Yang, Jianfeng Wang, Lin
Liang, Zicheng Liu, Yumao Lu, et al. MM-Vid: Advanc-
ing video understanding with GPT-4V (ision). arXiv preprint
arXiv:2310.19773 , 2023. 2,3,6,8
[31] Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe
Gan, Zicheng Liu, Yumao Lu, and Lijuan Wang. SwinBert:
End-to-end transformers with sparse attention for video cap-
tioning. In CVPR , 2022. 2
[32] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. NeurIPS , 2023. 3,5
13655
[33] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,
Lawrence Carin, and Weizhu Chen. What makes
good in-context examples for GPT-3? arXiv preprint
arXiv:2101.06804 , 2021. 3,4,7,13
[34] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hi-
roaki Hayashi, and Graham Neubig. Pre-train, prompt, and
predict: A systematic survey of prompting methods in natu-
ral language processing. ACM Computing Surveys , 55(9):1–
35, 2023. 3
[35] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen
Xu, and Chenguang Zhu. GPTEval: NLG evaluation us-
ing GPT-4 with better human alignment. arXiv preprint
arXiv:2303.16634 , 2023. 5
[36] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei
Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and
Ashwin Kalyan. Learn to explain: Multimodal reasoning
via thought chains for science question answering. NeurIPS ,
35:2507–2521, 2022. 3
[37] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui
Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. Valley:
Video assistant with large language model enhanced ability.
arXiv preprint arXiv:2306.07207 , 2023. 3
[38] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fa-
had Shahbaz Khan. Video-ChatGPT: Towards detailed video
understanding via large vision and language models. arXiv
preprint arXiv:2306.05424 , 2023. 3
[39] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh
Hajishirzi. MetaICL: Learning to learn in context. arXiv
preprint arXiv:2110.15943 , 2021. 3
[40] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike
Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Re-
thinking the role of demonstrations: What makes in-context
learning work? arXiv preprint arXiv:2202.12837 , 2022. 3
[41] Ron Mokady, Amir Hertz, and Amit H Bermano. Clip-
Cap: Clip preﬁx for image captioning. arXiv preprint
arXiv:2111.09734 , 2021. 6,8,13
[42] David Nukrai, Ron Mokady, and Amir Globerson. Text-
only training for image captioning using noise-injected clip.
EMNLP Findings , 2022. 6
[43] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas
Joseph, Nova DasSarma, Tom Henighan, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, et al. In-
context learning and induction heads. arXiv preprint
arXiv:2209.11895 , 2022. 4
[44] OpenAI. GPT-4 technical report. 2023. 3,14
[45] OpenAI. GPT-4V(ision) system card. 2023. 6,8,14
[46] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. Bleu: a method for automatic evaluation of machine
translation. In ACL, 2002. 6
[47] Elisa Perego. Gains and losses of watching audio described
ﬁlms for sighted viewers. Target , 28(3):424–444, 2016. 2
[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML , pages 8748–8763. PMLR, 2021. 3,4,13[49] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman,
Christine McLeavey, and Ilya Sutskever. Robust speech
recognition via large-scale weak supervision. In ICML ,
2023. 6
[50] Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt
Schiele. A dataset for movie description. In CVPR , pages
3202–3212, 2015. 2
[51] Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket
Tandon, Christopher Pal, Hugo Larochelle, Aaron Courville,
and Bernt Schiele. Movie description. Int. J. Comput. Vis. ,
123:94–120, 2017. 2
[52] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learn-
ing to retrieve prompts for in-context learning. arXiv preprint
arXiv:2112.08633 , 2021. 3
[53] Mattia Soldan, Alejandro Pardo, Juan Le ´on Alc ´azar, Fabian
Caba, Chen Zhao, Silvio Giancola, and Bernard Ghanem.
MAD: A scalable dataset for language grounding in videos
from movie audio descriptions. In CVPR , pages 5026–5035,
2022. 2
[54] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng
Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan
Lu, Jenq-Neng Hwang, et al. MovieChat: From dense to-
ken to sparse memory for long video understanding. arXiv
preprint arXiv:2307.16449 , 2023. 3
[55] Taylor Sorensen, Joshua Robinson, Christopher Michael
Rytting, Alexander Glenn Shaw, Kyle Jeffrey Rogers,
Alexia Pauline Delorey, Mahmoud Khalil, Nancy Fulda,
and David Wingate. An information-theoretic approach
to prompt engineering without ground truth labels. arXiv
preprint arXiv:2203.11364 , 2022. 3
[56] Eshaan Tanwar, Manish Borthakur, Subhabrata Dutta, and
Tanmoy Chakraborty. Multilingual llms are better cross-
lingual in-context learners with alignment. arXiv preprint
arXiv:2305.05940 , 2023. 3
[57] Atousa Torabi, Christopher Pal, Hugo Larochelle, and Aaron
Courville. Using descriptive video services to create a large
data source for video annotation research. arXiv preprint
arXiv:1503.01070 , 2015. 2
[58] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
LLaMA 2: Open foundation and ﬁne-tuned chat models.
arXiv preprint arXiv:2307.09288 , 2023. 3
[59] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi
Parikh. CIDEr: Consensus-based image description evalu-
ation. In CVPR , 2015. 6
[60] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-
Yuan Mark Liao. YOLOv7: Trainable bag-of-freebies sets
new state-of-the-art for real-time object detectors. In CVPR ,
pages 7464–7475, 2023. 3
[61] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang
Wang, and William Yang Wang. VaTeX: A large-scale, high-
quality multilingual dataset for video-and-language research.
InICCV , October 2019. 2
[62] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun
Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun
Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali
13656
Wang, Limin Wang, and Yu Qiao. Internvideo: General
video foundation models via generative and discriminative
learning. 2022. 6
[63] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.
Chain-of-thought prompting elicits reasoning in large lan-
guage models. NeurIPS , 35:24824–24837, 2022. 2,3
[64] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan,
Zicheng Liu, Junsong Yuan, and Lijuan Wang. GRiT: A gen-
erative region-to-text transformer for object understanding.
arXiv preprint arXiv:2212.00280 , 2022. 3,6
[65] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: A
large video description dataset for bridging video and lan-
guage. In CVPR , pages 5288–5296, 2016. 2
[66] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,
Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The Dawn
of LMMs: Preliminary explorations with GPT-4V (ision).
arXiv preprint arXiv:2309.17421 , 2023. 6
[67] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-
berger, and Yoav Artzi. Bertscore: Evaluating text genera-
tion with bert. arXiv preprint arXiv:1904.09675 , 2019. 6
[68] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola.
Automatic chain of thought prompting in large language
models. arXiv preprint arXiv:2210.03493 , 2022. 3
13657
