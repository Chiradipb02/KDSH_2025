ElasticDiffusion: Training-free Arbitrary Size Image Generation
through Global-Local Content Separation
Moayed Haji-Ali Guha Balakrishnan
Rice University
{mh155, guha, vicenteor }@rice.eduVicente Ordonez
ùüìùüèùüê	√ó	ùüìùüèùüê
Figure 1. ElasticDiffusion generates high quality images at arbitrary sizes using a pretrained diffusion model trained on a single image
size, with equivalent memory footprint and no further training. These results are based on Stable Diffusion 1.4, which was trained to
generate 512√ó512images. The examples shown in this collage are presented without any image cropping, stretching, or post-processing.
Abstract
Diffusion models have revolutionized image generation in
recent years, yet they are still limited to a few sizes and as-
pect ratios. We propose ElasticDiffusion, a novel training-
freedecoding method that enables pretrained text-to-image
diffusion models to generate images with various sizes.
ElasticDiffusion attempts to decouple the generation tra-
jectory of a pretrained model into local and global sig-
nals. The local signal controls low-level pixel informa-
tion and can be estimated on local patches, while the
global signal is used to maintain overall structural con-
sistency and is estimated with a reference image. We test
our method on CelebA-HQ (faces) and LAION-COCO (ob-
jects/indoor/outdoor scenes). Our experiments and qualita-tive results show superior image coherence quality across
aspect ratios compared to MultiDiffusion and the standard
decoding strategy of Stable Diffusion. Project Webpage:
https://elasticdiffusion.github.io/
1. Introduction
Diffusion models are a powerful family of algorithms that
achieve remarkable quality and obtain the current state-of-
the-art performance on various image synthesis tasks. As is
the case with most deep neural networks, diffusion models
are typically trained on one or a few image resolutions. For
instance, Stable Diffusion (SD) [39], one of the most widely
adopted diffusion models, is trained on a square images of
size 512 x 512, yet fails to maintain its performance at dif-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
6603
ferent aspect ratios during inference time. In practice, many
applications require a wide aspect ratio or portrait mode,
such as digital billboards, wearable devices, automotive dis-
plays, and any application relying on a computer screen.
Recent studies address the issue of variable image size
in different ways. SDXL [35] and Any-Size-Diffusion [62]
explicitly finetune models on images with a range of aspect
ratios, which requires extensive computation, a quadratic
memory footprint, and larger training data. In addition,
this strategy requires a set of resolutions to be specified up-
front during training time, while the models still struggle to
generalize to new resolutions during inference, often result-
ing in artifacts. Recent works also show remarkable results
in generating panoramic images using pretrained diffusion
models by overlapping generated patches into a larger im-
age [3, 63]. These methods work well for landscape im-
ages with repetitive patterns. However, their lack of global
guidance limits their abilities to generate images of single
objects or faces where global structure is important. Recent
work [24, 50, 51] aimed at extending pre-trained diffusion
models capabilities to others domains, often in a training-
free way [4, 7, 13, 28, 30, 34, 47, 54, 61]. Few concurrent
work [9, 10, 12, 31, 60] focus on adapting them to higher
resolutions, yet they are constrained to square images.
In this work, we propose ElasticDiffusion, a novel de-
coding strategy that takes a pretrained diffusion model and
generate images at arbitrary sizes during inference using a
constant memory footprint. To achieve this, we revisit the
guidance mechanism of conditional diffusion models to de-
couple global and local content generation. Global content
controls the high-level aspects of the image, whereas local
content adds finer, more granular details. This separation fa-
cilitates generating the local content in patches for images
of varying sizes, all while being guided with global content
that we derive from a reference image at the diffusion model
pretraining resolution. This enables the synthesis of images
at diverse resolutions and aspect ratios while adhering the
diffusion forward calls to the model‚Äôs initial training size.
To aid in this task, we introduce several techniques includ-
ing an efficient patch fusion method for smooth boundaries,
a novel guidance strategy to reduce image artifacts, and a
global content resampling technique that amplifies the res-
olution of diffusion models up to 4X the training size.
Figure 1 shows a diverse array of images generated with
ElasticDiffusion. Several of theses images include a single
object or were generated with extreme aspect ratios, show-
casing our method‚Äôs ability to produce coherent images un-
der various sizes. Quantitatively, ElasticDiffusion outper-
forms baselines across most aspect ratios. More impor-
tantly, despite relying on Stable Diffusion 1.4, ElasticDiffu-
sion obtains comparable FID ( 228.87vs230.21) and CLIP
scores ( 26.07vs28.06) as SDXL when generating images
at1024√ó1024 , which is the native resolution of SDXL.2. Related Work
Diffusion Models have been widely adopted for their high-
quality outputs in generative tasks [8, 11, 15, 20, 22, 35,
37, 39, 43, 55, 58]. These models involve iterative decod-
ing with many steps leading to high compute and memory
requirements. Recent work addressed these issues by de-
vising faster sampling strategies [26, 48], hierarchical mod-
els [19, 37, 43], progressive training at increasing resolu-
tions [33, 35, 39], and two-stage models [35, 39, 42, 53].
Stable Diffusion (SD) [39] trains a variational auto-encoder
to compress images into a low-dimensional 64√ó64latent
space and trains a diffusion model on this latent space. To
train for higher resolutions, models are initially trained at a
256√ó256resolution, before fine-tuning them at 512√ó512
and1024√ó1024 in the case of SDXL [35]. SD is one of
the few large-scale diffusion models that released trained
parameters, making it the building block for many subse-
quent work [5, 16, 40, 44, 52, 53]. However, these models,
including SD, are confined to specific resolutions and do
not generalize well to aspect ratios unseen during training.
Interestingly, despite being presented with a 256√ó256reso-
lution during their early training stages, both SD and SDXL
fail to generate realistic images at this resolution after be-
ing fine-tuned for larger outputs. ElasticDiffusion enables
high quality generation at unseen resolutions including re-
enabling consistent high quality outputs for SD at 256√ó256.
Mixture of diffusers [63] and MultiDiffusion [3] generate
panoramic images using a pre-trained diffusion model by
generating overlapping crops and combining the generation
signal of the overalpping regions. Blending multiple gener-
ation signals spatially has been of interest in many previous
work [1, 63]. SDXL [35] and Any-Size-Diffusion [62] fine-
tune a pre-trained SD on a fixed set of resolutions. A con-
current work [14] uses dilated convolution kernels to reduce
SD artifacts when generating non-square images. ElasticD-
iffusion extends a pre-trained SD to generate images of var-
ious sizes at a constant memory requirement and without
additional training, all while ensuring global coherence.
Guided diffusion models devise strategies to condition
image generation based on text and other modalities [2, 2,
6, 18, 23, 25, 32, 39, 57, 59]. Classifier guidance [32] uses
a pre-trained classifier on noisy images to guide the gener-
ation process. Classifier-free guidance [18] eliminates the
need for a pretrained classifier but requires training a condi-
tional diffusion model, limiting its applications. Universal
Guidance [2] applies a pre-trained classifier on the noise-
free images produced by DDIM [48], bypassing training
on noisy images. StableSR [53] uses LoRA [21] to condi-
tion the generation on low-resolution inputs to achieve su-
perb image super-resolution. Inspired by this, we propose
Reduce-Resolution Guidance (Sec. 4) to constrain the gen-
eration of high-res images using a lower resolution version,
substantially reduceing artifacts without extra training.
6604
3. Background: Diffusion Models
A conditional diffusion model œµŒ∏:X √ó C ‚Üí X predicts a
less noisy version of the input image x‚ààRH√óW√ó3, condi-
tioned on variable c‚ààRD(e.g. a text embedding). Starting
withxT‚àº N(0,I), the reverse diffusion process progres-
sively denoise xToverTsteps to generate a realistic image
x0that conforms to the input condition cthrough:
xt‚àí1=œµŒ∏(xt, c) fort=T, T‚àí1, . . . , 1,
where œµŒ∏is the denoising network. Standard diffusion mod-
els operate in the pixel space, but others like Latent Dif-
fusion Models [39] instead operate on a latent image en-
coding space to reduce memory footprints. A U-Net archi-
tecture is commonly used to implement the denoising net-
work and is typically trained on images at a fixed resolution
H√óW[8, 39, 43]. The convolutional architecture of a
U-Net allows for inputs and outputs of arbitrary spatial di-
mensions . In order to generate an image of a different size
¬ØH√ó¬ØWat inference time, one can sample an initial noise
variable ¬ØxT‚ààR¬ØH√ó¬ØW√ó3and follow the same diffusion pro-
cess. However, we find that this works poorly in practice,
resulting in a significant degradation in output quality.
Denoising Diffusion Implicit Models (DDIMs) intro-
duce a faster non-Markovian sampling strategy, bypassing
denoising steps. The reverse diffusion step in DDIM is:
xt‚àí1=‚àö¬ØŒ±t‚àí1
xt‚àíq
1‚àí¬ØŒ±tœµ(t)
Œ∏(xt)
‚àö¬ØŒ±t| {z }
predicted ÀÜxt
0+p
1‚àí¬ØŒ±t‚àí1|{z}
direction
pointing
toxt,(1)
where ¬ØŒ±t=Qt
i=11‚àíŒ≤iis a cumulative product of the
noise levels using a predetermined variance schedule Œ≤.ÀÜxt
0
is a noise-free estimation of xtobtained by subtracting the
predicted noise scaled for step t. We assume a deterministic
sampling process in the DDIM formula [48] for simplicity.
Classifier-Free Guidance updates the reverse diffusion
process to condition it on a given condition cas:
ÀÜœµ(t)
Œ∏(xt) =œµ(t)
Œ∏(xt)|{z}
unconditional
score+(1+ w)¬∑(œµ(t)
Œ∏(xt, c)‚àíœµ(t)
Œ∏(xt))| {z }
‚àÜC(x,c): class direction score.(2)
œµŒ∏(x, c)is a pretrained conditional diffusion model, and w
is a scaling factor. The difference between the conditional
œµŒ∏(x, c)and unconditional œµŒ∏(x)scores, denoted as class
direction score , gives the guidance direction towards c.
4. ElasticDiffusion
The aim of this work is to develop a method capable of syn-
thesizing images at arbitrary size ¬ØH√ó¬ØW, and conforming
Figure 2. PCA of diffusion scores: class-direction score (top)
dictates global content by clustering on semantic parts, while the
unconditional score (bottom) lacks pixel correlations.
to a global condition cusing a pre-trained diffusion model
that is limited at inference to its training resolution H√óW.
To achieve this, we observe a main insight with respect to
the diffusion scores in Eq. (2) that we visualize in Fig. 2.
The class direction score ( ‚àÜc) primarily dictates the im-
age‚Äôs overall composition by showing clustering along se-
mantic regions ( e.g. hair) and edges. Thus, upscaling this
score maintains global content in higher resolutions. In con-
trast, the unconditional score lacks inter-pixel correlation,
suggesting a localized influence in enhancing detail at the
pixel level. Thus, computing it requires a pixel-level pre-
cision. We study the global and local behaviour of these
scores in Supp. Sec. 2.1. Our intuition behind this observa-
tion is that latent pixels, specifically in early diffusion steps
where noise is high, display weak and short-range correla-
tions. This leads the unconditional score to focus on local
patterns due to these limited pixel interactions. Meanwhile,
as‚àÜcrepresents a latent direction towards a specified global
condition, it exerts a global influence by guiding clusters
of pixels with similar directions towards semantic regions,
overriding their initial noisy state. Leveraging this insight,
we propose a method to decouple the generation of local
and global content during the diffusion process. Specifi-
cally, we compute the unconditional score on local patches
of size H√óWwhile simultaneously resizing a class di-
rection score, originally derived for a reference latent of the
dimensions H√óWas well. This dual strategy facilitates the
generation of images at varied sizes using a pretrained dif-
fusion model at its native resolution, all while maintaining
the same memory requirement and without further training.
We first present our approach for computing the uncondi-
tional score (Sec. 4.1). We then detail our method to esti-
mate (Sec. 4.2) and upscale the resolution (Sec. 4.3) of the
class direction score. Finally, we combine the two estimated
scores with a novel guidance strategy to generate images at
arbitrary sizes (Sec. 4.4). The generation process of Elas-
ticDiffusion is illustrated in Fig. 3.
4.1. Estimating the Unconditional Score
Building upon previous work [3, 53, 63], we estimate the
unconditional score for a large latent signal ¬Øxt‚ààR¬ØH√ó¬ØW√ó3
by concatenating scores derived from local patches. Specif-
6605
Figure 3. Illustration of ElasticDiffusion: We generate images at various sizes by generating local and global content separately. For
local content, we partition the latent ¬Øxtintonon-overlapping patches pk, each concatenated with context ckto estimate their unconditional
score. For global content, we downsample ¬Øxttoxt, pad to a square size ( ÀÜxt), compute class-direction score ( ‚àÜc), and upscale to match ¬Øxt.
ically, we divide image ¬ØxttoKpatches Pk‚ààRH√óW√ó3
and compute the score as œµŒ∏(¬Øxt) ={œµŒ∏(Pk);‚àÄk‚â§K}. A
common challenge encountered with this implementation is
discontinuities at boundaries, as illustrated in Fig. 4 (A). To
address this, earlier research calculated the diffusion model
score on explicitly overlapping patches and averaged their
scores in the intersecting regions [3, 53, 63]. While this
strategy mitigates discontinuities, it requires large overlap
between patches, thereby substantially increasing inference
time and blurring details. To speed up the process, we intro-
duce a more effective method that enhances boundary tran-
sitions in local patches by incorporating contextual informa-
tion from adjacent patches, thus negating the need for sig-
nal averaging in overlapping areas. Specifically, we select
patches smaller than the full size pk‚ààRh√ów√ó3withh < H
andw < W , and concatenate them with context pixels from
adjacent patches, denoted as ck‚ààR(H‚àíh)√ó(W‚àíw)√ó3, to
compute the diffusion model unconditional score Suas:
ÀúœµŒ∏(xt) ={œµŒ∏(pk|ck)|¬Øxt={pk;‚àÄk‚â§K}}, (3)
where piis a local patch and ciare the context pixels sur-
rounding it. This substantially increases the efficiency of
the process. For instance, to generate an image of size
1024√ó1024 , previous methods [3, 53, 63] used 87.5%over-
lap between adjacent patches, necessitating 81forward dif-
fusion calls per decoding step. In comparison, ElasticDiffu-
sion achieves comparable results with only 9forward calls,as demonstrated in Fig. 4 (D). Employing the same num-
ber of calls, previous techniques result in obvious boundary
discontinuities, as depicted in Fig. 4 (B).
4.2. Estimating the Class Direction Score
A simple approach to estimate a class direction score of an
intermediate latent signal ¬Øxt‚ààR¬ØH√ó¬ØW√ó3is to upsample
the score from a reference latent xt‚ààRN√óM√ó3to¬ØH√ó¬ØW.
This is possible due to our observation that the class direc-
tion score represents a latent direction that can be shared
between nearby pixels. We validate this observation empir-
ically in Supplementary. We choose N < H andM < W
such that N√óMis as close as possible to H√óWand
¬ØH
¬ØW=N
M. This is important to maintain the aspect ratio and
prevent stretching the global content. Formally, we com-
pute the class direction score Sdas:
xt‚ÜêDownsample(¬Ø xt, N√óM),
‚àÜC(¬Øxt, c) = Upsample(‚àÜ C(xt, c),¬ØH√ó¬ØW),(4)
where ‚àÜC(.)is the class direction score from Eq. (2),
Downsample andUpsample are downsampling and up-
sampling operations. We use a nearest-neighbors approach
to prevent altering the statistics of the latent signal. In or-
der to maintain the input to the diffusion models at the
sizeH√óW, we dynamically pad the downsampled la-
tentxtusing a random background with a constant color.
6606
Figure 4. Comparing strategies for calculating diffusion model
score on a local patch. No overlap between adjacent patches (A)
leads to discontinuities at the boundaries. Strategies (B) and (C),
explicitly overlap nearby patches, necessitating substantial over-
lap to be effective. Our implicit overlapping method (D) achieves
superior results with computational demand similar to (B).
This encourages the model to concentrate content gener-
ation within the center area. We then crop the extended
parts from the predicted noise to the target image resolu-
tionN√óM. Formally we modify the forward call for the
reverse diffusion step as:
ÀÜxt‚ÜêPad 
xt,At‚àö¬ØŒ±t+‚àö
1‚àí¬ØŒ±t¬∑ Zt
,
œµŒ∏(xt, c) = Crop ( œµŒ∏(ÀÜxt, c), N√óM),(5)
where Zt‚àº N(0, I)represents the injected Gaussian noise
at each step, and Atrepresents a background image of size
(H‚àíN)√ó(W‚àíM)with a constant color value Yrandomly
sampled from a uniform distribution. This simple padding
operation guarantees that the input to the diffusion model
is kept at H√óW, while encouraging the diffusion model
to keep the generated content within the cropped N√óM
center that we are interested in.
4.3. Refined Class Direction Score
Sharing the class direction score among nearby pixels can
result in over-smoothed images. To mitigate this, we pro-
pose an iterative resampling technique that increases the
resolution of the estimated class direction score by extrap-
olating missing image components from their surrounding
context, following [29]. Our technique involves a grad-
ual enhancement of the class direction score‚Äôs resolution
by estimating and integrating it for newly sampled pixels.
Specifically, in each iteration, we replace n%of the pixels
Figure 5. The Effect of Reduced-Resolution Guidance (RRG).
Higher RRG weights effectively eliminates emerging artifacts al-
beit at the cost of slightly blurrier outputs. Œ¥= 200 strikes a good
balance. Improvements are more noticeable when zooming in.
Figure 6. Effect of Resampling. Applying more resampling steps
noticeably enhances detail in the generated images.
inxtwith newly sampled ones from xtto get an updated
version Àúxt. Following each update, the direction score is re-
calculated and blended with the previously calculated score.
Formally, we consider S0
d=Sdand define the iterative re-
sampling as:
Sr+1
d=Sr
d‚äôm+ÀúSd‚äô(1‚àím), (6)
where m‚àà {0,1}¬ØH√ó¬ØWis a mask with a value of 1 at the
positions of the newly sampled pixels and 0 elsewhere. ÀúSd
represents the recalculated class direction score on Àúxtas
per Eq. (4). This method estimates the class direction score
ofn%new pixels while retaining the information of the
previously estimated score, thereby increasing the score‚Äôs
overall resolution. In our experiments, we set nto20% and
repeat the process Rtimes, depending on the target genera-
tion resolution. Fig. 6 demonstrates the effectiveness of our
method in enhancing the details of the generated images.
4.4. Reduced-Resolution Guidance (RRG)
We effectively estimate the unconditional score signal and
concurrently steer the image generation using the class di-
rection score. However, inaccuracies in unconditional score
estimation or fluctuations in local content, especially from
distant patches, can lead to artifacts. To enhance image co-
herence and diminish artifacts, we consider a downsampled
version of the latent at each decoding step as a reference and
aim to align the decoded latent with it through our reduced-
resolution latent update strategy. Specifically, we utilize the
noise-free sample ÀÜxt
0of the latent xtfrom Eq. (1) and gen-
erate a corresponding noise-free sample ÀÜxt
0from its down-
sampled counterpart xtin Eq. (4) as:
ÀÜxt
0=1‚àö¬ØŒ±t(xt‚àí‚àö
1‚àí¬ØŒ±t¬∑(œµŒ∏(xt) + (1 + w)¬∑‚àÜC(xt, c))),
Here, both ÀÜxt
0andÀÜxt
0corresponds to the same latent up-
date at different resolutions. Due to its smaller dimension,
ÀÜxt
0has a broader context when computing the unconditional
6607
Figure 7. Qualitative comparison at various resolutions on CelebAHQ faces. ElasticDiffusion consistently generates coherent images
at all resolutions. StableDiffusion , and MultiDiffusion produce repeating body parts at higher resolutions, while StableDiffusion-XL fails to
maintain its quality at lower resolutions, resulting in noisy outputs at 256√ó256. We exclude MultiDiffusion results at 256√ó256as it is
not designed to produce images at lower resolutions.
signal. To guide xtwith its downsampled reference, we re-
fine the latent xtwith the direction that minimizes L2dif-
ference between ÀÜxt
0andÀÜxt
0. Formally,
¬Øxt‚àí1‚Üê¬Øxt‚àí1‚àíŒ¥t‚àáxt||Upsample( ÀÜxt
0,¬ØH√ó¬ØW)‚àíÀÜxt
0||,(7)
where Œ¥trepresent the weight of the guidance. Since the
overall image structure is determined in the early diffusion
steps, we start with Œ¥t= 400 and follow a cosine sched-
uler to decrease this weight for later diffusion steps. This
scheduling strategy mitigates potential quality degradation
from matching the generated image with a lower-resolution
version while allowing the model to fill-in higher-resolution
details in the later decoding stages. Fig. 5 illustrates how
RRG eliminates emerging artifacts.
5. Experiments
ElasticDiffusion supports generating images across differ-
ent resolutions and aspect ratios. Our experiments focus
on: (1) square images at multiple resolutions, and (2) im-
ages with varied aspect ratios and resolutions.
Datasets. We evaluate the generation of square images on
the Multi-Modal CelebAHQ dataset [56], which includes
high-resolution square face images accompanied with text
descriptions. We evaluate different aspect ratio generation
using the LAION-COCO dataset [46], derived from the
web-crawled LAION-5B dataset [45], which includes a va-
riety of image aspect ratios and contains landscapes, people,objects, and everyday scenes, each paired with a synthetic
caption generated using BLIP [27]. We consider four com-
mon aspect ratios: 9:16, 16:9, 3:4 and 4:3.
Evaluation Metrics. Following prior text-to-image syn-
thesis works [3, 33, 35, 38, 43], we use automatic evalu-
ation metrics Frechet Inception Distance (FID) andCLIP-
score .FID [17] measures both the realism and diversity of
the generated images by calculating the difference between
features of the real and generated images computed using
Inception-V3 [49] pretrained on ImageNet [41]. CLIP-
score uses a pretrained text-image CLIP model [36] to mea-
sure alignment between the generated images and input
prompts. We use 10,000 images to compute these scores.
Baselines. We compare our approach against prior diffu-
sion model generation methods, specifically focusing on
Stable Diffusion (SD) andMultiDiffusion (MD) .SDfollows
the standard reverse diffusion process on an image latent
space. MDuses a pretrained SDfor panoramic image gen-
eration, by creating smaller, overlapping patches and av-
eraging the Diffusion Model scores in intersecting areas.
For baseline comparisons, we fix the pre-trained diffusion
model to SD1.4, which is trained to generate images at a
resolution of 512√ó512. Additionally, we compare our
method with SDXL , an enhanced SD model that is three
times larger than the standard one. SDXL is trained at a
higher resolution of 1024p, and fine-tuned on a specific set
of aspect ratios with pixel sums close to 10242. We exclude
6608
Figure 8. Qualitative comparison on various aspect ratios.
ElasticDiffusion effectively handles a variety of aspect ratios. In
comparison, SDandMultiDiffusion generate unrealistic images,
while SDXL outputs exhibit a decline in the perceptual quality.
the latent refinement module in SDXL and focus our anal-
ysis on the base model. Throughout our experiments, we
employ a DDIM sampling strategy with 50 steps and use
7.0for the classifier-free guidance scaling factor. We also
ensure consistency in seeds and captions for all baselines.
5.1. Qualitative Results
We show square image generation samples in Fig. 7 gen-
erated from the CelebAHQ dataset at various resolutions.
Both MultiDiffusion andStable Diffusion have a tendency
to replicate textures and body parts at higher resolutions, re-
sulting in images that lack coherence. At resolutions lower
than its training resolution 512√ó512,StableDiffusion aligns
poorly with the provided captions and produces unappeal-
ing images. SDXL , trained for 1024√ó1024 resolution,
has a similar trend of reduced perceptual quality at lower
sizes, eventually producing complete noise at a resolution
of256√ó256. In contrast, ElasticDiffusion maintains im-
age coherence across all tested resolutions, and yields re-
sults comparable to SDXL at1024√ó1024 , despite using a
less powerful base model and lower memory. We excludedTable 1. Quantitative comparison of on CelebA-HQ and LAION-
COCO at different resolution. We indicate the best performances
inbold , and underline the second best ones. #Calls represent the
number of diffusion model calls required at each decoding steps.
Res. MethodCelebA-HQ LAION-COCO#Calls Mem.
FID‚ÜìCLIP‚ÜëFID‚ÜìCLIP‚Üë
256
√ó
256SD1.4258.43 20.14 54.06 21.43 2 7.2GB
SDXL 368.06 14.40 175.87 14.60 2 18.5GB
Ours 1.4235.23 23.88 23.77 26.30 2 8.6GB
512
√ó
512SD1.4233.40 24.00 20.50 27.33 2 8.6GB
SDXL 240.20 21.57 42.58 25.34 2 21.6GB
768
√ó
768SD1.4238.87 23.45 29.89 27.01 2 11.1GB
MD1.4240.56 22.82 29.98 27.31 50 8.6GB
SDXL 225.48 24.23 23.31 27.88 2 24.5GB
Ours 1.4225.86 26.66 25.78 25.93 17 8.6GB
1024
√ó
1024SD1.4266.01 21.90 47.01 25.70 1 14.7GB
MD1.4264.57 21.55 37.70 26.96 162 8.6GB
SDXL 230.21 24.62 25.58 28.06 1 27.5GB
Ours 1.4228.87 23.74 27.76 26.07 33 8.6GB
results at 512√ó512because both our method and Multi-
Diffusion generate same outputs as the base Stable Diffu-
sion model. We also omitted the results of MultiDiffusion at
256√ó256since it is not designed to generate images at sizes
smaller than the base model. Fig. 8 presents generated sam-
ples at various aspect ratios. We observe a similar trend of
pattern repetition and reduced perceptual quality for images
generated by the baselines, in contrast to those generated
by ElasticDiffusion. Finally, we apply our method using
SDXL as the base model to enable Full-HD image genera-
tion (1920√ó1080 ) and provide examples in Supp. Sec. 3.3.
5.2. Quantitative Results
Table 1 shows quantitative evaluations. StableDiffusion
shows increasing image quality degradation, as indicated
by the FID metric when processing images of sizes differ-
ent from its training resolution 512√ó512.MultiDiffusion
slightly improves FIDat the expense of a substantially more
forward calls. SDXL demonstrates similar declines in qual-
ity at resolutions far from its fine-tuning size of 1024√ó1024 .
ElasticDiffusion, however, improves FIDwhile maintaining
a comparable CLIP-score to the base model. ElasticDiffu-
sion also significantly improves the performance for lower
resolutions 256√ó256, while achieving comparable results
toSDXL at its training resolution 1024√ó1024 , with only
‚àº31% of the memory requirement for SDXL .
6609
Table 2. Quantitative comparison of on LAION-COCO
datasets at various aspect ratios (A) and resolutions (R). best
performances are in bold , andthe second best are underlined . Ver-
tical means the resolution is transposed from H:W to W:H.
A R MethodHorizontal Vertical
FID‚ÜìCLIP‚ÜëFID‚ÜìCLIP‚Üë
3:4384
√ó
512SD1.4 38.86 24.63 17.66 26.54
MD1.4 ‚Äì ‚Äì ‚Äì ‚Äì
SDXL 104.84 21.33 68.84 22.40
Ours 1.435.10 24.91 15.50 27.33
512
√ó
680SD1.4 45.54 24.96 16.81 27.33
MD1.4 43.44 24.56 19.13 26.80
SDXL 62.80 24.20 28.23 26.17
Ours 1.441.06 24.40 18.90 26.83
768
√ó
1024SD1.4 71.00 24.09 28.83 26.30
MD1.4 54.89 25.02 26.35 26.95
SDXL 47.05 25.79 19.50 27.41
Ours 1.447.03 24.91 22.52 25.80
9:16288
√ó
512SD1.4 23.50 24.69 24.01 24.89
MD1.4 ‚Äì ‚Äì ‚Äì ‚Äì
SDXL 121.83 17.65 112.41 18.54
Ours 1.423.23 25.26 22.86 26.30
512
√ó
904SD1.4 29.86 25.34 27.45 26.01
MD1.4 26.35 25.70 26.70 25.28
SDXL 29.60 25.40 27.27 26.08
Ours 1.422.85 25.01 26.68 26.12
Table 2 provides an evaluation on a variety of aspect
ratios and resolutions for both horizontal and vertical im-
age resolutions. StableDiffusion obtains worse FID score
with increasing resolutions, while surprisingly maintaining
or improving its CLIP-score . We posit that since CLIP-
score quantifies the agreement between the input prompt
and the generated image, StableDiffusion benefits from gen-
erating repetitive textures and artifacts that align closely
with the prompt. For example, a photo of repeated lipsticks
might align more with the caption ‚Äùlipstick‚Äù despite its
poor composition. MultiDiffusion generally enhances im-
age quality but does not achieve satisfactory performance.
MultiDiffusion struggles with objects that span the entire
image ( e.g. Fig. 8). Our method consistently improves FID
over the baseline on horizontal and most vertical resolutions
while preserving fidelity to the input prompts, thereby at-
taining comparable or superior CLIP-scores . Remarkably,
even with a larger model size and explicit fine-tuning at a
similar resolution of 768√ó1280 ,SDXL only marginally
surpasses our method at the 768√ó1024 resolution.Table 3. Ablation analysis of ElasticDiffusion on 500 images.
Model Details FID‚ÜìCLIP‚Üë
ElasticDiffusion 133.67 25.82
w/oResampling 150.01 23.82
w/oRRG 150.64 24.34
w/oImp. overlap , w/ exp. overlap 141.42 25.82
5.3. Ablation study
Tab. 3 presents results of our method when excluding key
components, demonstrating that each element improves
performance. Fig. 4 illustrates the effectiveness of our pro-
posed implicit overlap method in resolving boundary dis-
continuities at a reduced computational cost. Fig. 5 high-
lights the effectiveness of Reduced-Resolution Guidance in
removing emerging artifacts. Fig. 6 shows the effectiveness
of our iterative resampling technique in enhancing details.
6. Discussion and Conclusion
Experimental results highlight the adaptability and effec-
tiveness of ElasticDiffusion at steering diffusion models to
produce an array of resolutions and aspect ratios. ElasticD-
iffusion requires no fine-tuning, consumes a constant mem-
ory footprint, enables both increased and reduced resolu-
tions, and can generate a variety of aspect ratios.
ElasticDiffusion, however, does have several practical
limitations. First, inaccuracies in estimating the global and
local signals may result in artifacts. Although we attempt
to mitigate artifacts with our Reduced-resolution guidance ,
it can still generate blurrier outputs. Second, since the
global content guidance is initially estimated at the original
training resolution of the underlying diffusion model, our
method is less effective in generating images at significantly
extended sizes beyond the training resolution. In particular,
at extreme resolutions beyond 4X, our method produces ar-
tifacts and images of a lower perceptual quality. We provide
examples of these failure cases in Supp. Sec. 10.4.
The main insight underpinning our method is a novel
reinterpretation of classifier-free guidance in somewhat dis-
entangling both global and local content. Our comprehen-
sive evaluations demonstrate the feasibility of disentangling
these signals, yet the full extent of their separation offers an
avenue for further exploration in this direction. We hope
that our findings inspire future work in investigating the
separation of global and local content guidance signals for
image synthesis. This separation holds potential for vari-
ous applications such as selectively manipulating local and
global content or enhancing style transfer. Additionally, the
rich semantic representation in the class-direction score has
the potential for improving discriminative models.
Acknowledgments. This work was partially funded by
NSF Award #2201710.
6610
References
[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended
diffusion for text-driven editing of natural images. In 2022
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) . IEEE, 2022. 2
[2] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild,
Soumyadip Sengupta, Micah Goldblum, Jonas Geip-
ing, and Tom Goldstein. Universal guidance for diffusion
models, 2023. 2
[3] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.
Multidiffusion: Fusing diffusion paths for controlled image
generation. In ICML , 2023. 2, 3, 4, 6
[4] Angela Castillo, Jonas Kohler, Juan C. P ¬¥erez, Juan Pablo
P¬¥erez, Albert Pumarola, Bernard Ghanem, Pablo Arbel ¬¥aez,
and Ali Thabet. Adaptive guidance: Training-free accelera-
tion of conditional diffusion models, 2023. 2
[5] Duygu Ceylan, Chun-Hao Paul Huang, and Niloy J. Mitra.
Pix2video: Video editing using image diffusion. In ICCV ,
2023. 2
[6] Hyungjin Chung, Jeongsol Kim, Michael T. Mccann,
Marc L. Klasky, and Jong Chul Ye. Diffusion posterior sam-
pling for general noisy inverse problems, 2023. 2
[7] Jiwoo Chung, Sangeek Hyun, and Jae-Pil Heo. Style in-
jection in diffusion: A training-free approach for adapting
large-scale diffusion models for style transfer, 2024. 2
[8] Prafulla Dhariwal and Alex Nichol. Diffusion models beat
gans on image synthesis. In NeurIPS , 2021. 2, 3
[9] Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe
Song, and Zhanyu Ma. Demofusion: Democratising high-
resolution image generation with no $$$. In CVPR , 2024.
2
[10] Alexandros Graikos, Srikar Yellapragada, Minh-Quan Le,
Saarthak Kapse, Prateek Prasanna, Joel Saltz, and Dimitris
Samaras. Learned representation-guided diffusion models
for large-image generation, 2024. 2
[11] Jiatao Gu, Qingzhe Gao, Shuangfei Zhai, Baoquan Chen,
Lingjie Liu, and Josh Susskind. Control3diff: Learning con-
trollable 3d diffusion models from single-view images. In
3DV24 , 2023. 2
[12] Lanqing Guo, Yingqing He, Haoxin Chen, Menghan Xia,
Xiaodong Cun, Yufei Wang, Siyu Huang, Yong Zhang, Xin-
tao Wang, Qifeng Chen, Ying Shan, and Bihan Wen. Make
a cheap scaling: A self-cascade diffusion model for higher-
resolution adaptation, 2024. 2
[13] Feihong He, Gang Li, Lingyu Si, Leilei Yan, Shimeng Hou,
Hongwei Dong, and Fanzhang Li. Cartoondiff: Training-free
cartoon image generation with diffusion transformer models,
2023. 2
[14] Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun,
Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng
Chen, and Ying Shan. Scalecrafter: Tuning-free higher-
resolution visual generation with diffusion models, 2023. 2
[15] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and
Qifeng Chen. Latent video diffusion models for high-fidelity
long video generation, 2023. 2[16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image
editing with cross attention control. In ICLR , 2022. 2
[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In NIPS , 2017. 6
[18] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance, 2022. 2
[19] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet,
Mohammad Norouzi, and Tim Salimans. Cascaded diffu-
sion models for high fidelity image generation. Journal of
Machine Learning Research , 23(47):1‚Äì33, 2022. 2
[20] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William
Chan, Mohammad Norouzi, and David J. Fleet. Video diffu-
sion models. In NeurIPS , 2022. 2
[21] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. In
ICLR , 2022. 2
[22] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren,
Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin,
and Zhou Zhao. Make-an-audio: Text-to-audio generation
with prompt-enhanced diffusion models. In ICML , 2023. 2
[23] Yujin Jeong, Wonjeong Ryoo, Seunghyun Lee, Dabin Seo,
Wonmin Byeon, Sangpil Kim, and Jinkyu Kim. The power
of sound (tpos): Audio reactive video generation with stable
diffusion. In ICCV , 2023. 2
[24] Zhiyu Jin, Xuli Shen, Bin Li, and Xiangyang Xue. Training-
free diffusion model adaptation for variable-sized text-to-
image synthesis, 2023. 2
[25] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Dif-
fusionclip: Text-guided diffusion models for robust image
manipulation. In CVPR , 2022. 2
[26] Zhifeng Kong and Wei Ping. On fast sampling of diffusion
probabilistic models. In ICML , 2021. 2
[27] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation, 2022. 6
[28] Jin Liu, Huaibo Huang, Chao Jin, and Ran He. Portrait dif-
fusion: Training-free face stylization with chain-of-painting,
2023. 2
[29] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher
Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting
using denoising diffusion probabilistic models. In CVPR ,
2022. 5
[30] Sicheng Mo, Fangzhou Mu, Kuan Heng Lin, Yanli Liu,
Bochen Guan, Yin Li, and Bolei Zhou. Freecontrol:
Training-free spatial control of any text-to-image diffusion
model with any condition, 2023. 2
[31] Brian B. Moser, Stanislav Frolov, Federico Raue, Sebastian
Palacio, and Andreas Dengel. Dynamic attention-guided dif-
fusion for image super-resolution, 2024. 2
[32] Alex Nichol and Prafulla Dhariwal. Improved denoising dif-
fusion probabilistic models. In ICML , 2021. 2
[33] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
6611
Mark Chen. Glide: Towards photorealistic image genera-
tion and editing with text-guided diffusion models. In ICML ,
2022. 2, 6
[34] Bo Peng, Xinyuan Chen, Yaohui Wang, Chaochao Lu, and
Yu Qiao. Conditionvideo: Training-free condition-guided
text-to-video generation, 2023. 2
[35] Dustin Podell, Zion English, Kyle Lacey, Andreas
Blattmann, Tim Dockhorn, Jonas M ¬®uller, Joe Penna, and
Robin Rombach. Sdxl: Improving latent diffusion models
for high-resolution image synthesis, 2023. 2, 6
[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In ICML , 2021.
6
[37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents, 2022. 2
[38] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents, 2022. 6
[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¬®orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , 2022. 1, 2, 3
[40] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In CVPR , 2023. 2
[41] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Li Fei-Fei. Imagenet large scale visual recognition challenge,
2015. 6
[42] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-
mans, David J. Fleet, and Mohammad Norouzi. Image super-
resolution via iterative refinement, 2021. 2
[43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,
Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J
Fleet, and Mohammad Norouzi. Photorealistic text-to-image
diffusion models with deep language understanding. In
NeurIPS , 2022. 2, 3, 6
[44] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas
Blattmann, Patrick Esser, and Robin Rombach. Fast high-
resolution image synthesis with latent adversarial diffusion
distillation, 2024. 2
[45] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, Patrick Schramowski, Srivatsa Kundurthy, Katherine
Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia
Jitsev. Laion-5b: An open large-scale dataset for training
next generation image-text models, 2022. 6
[46] Christoph Schuhmann, Andreas A. K ¬®opf, Richard Vencu,
Theo Coombes, and Ross Beaumont. Laion coco: 600m syn-
thetic captions from laion2b-en, 2023. 6[47] Fengyuan Shi, Jiaxi Gu, Hang Xu, Songcen Xu, Wei Zhang,
and Limin Wang. Bivdiff: A training-free framework for
general-purpose video synthesis via bridging image and
video diffusion models, 2023. 2
[48] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In ICLR , 2021. 2, 3
[49] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jonathon Shlens, and Zbigniew Wojna. Rethinking the in-
ception architecture for computer vision. In CVPR , 2015.
6
[50] Andrey V oynov, Amir Hertz, Moab Arar, Shlomi Fruchter,
and Daniel Cohen-Or. Anylens: A generative diffusion
model with any rendering lens, 2023. 2
[51] Bingyuan Wang, Hengyu Meng, Zeyu Cai, Lanjiong Li, Yue
Ma, Qifeng Chen, and Zeyu Wang. Magicscroll: Nontypi-
cal aspect-ratio image generation for visual storytelling via
multi-layered semantic-aware denoising, 2023. 2
[52] Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian,
Guanglu Song, Yu Liu, and Hongsheng Li. Animatelcm:
Accelerating the animation of personalized diffusion models
and adapters with decoupled consistency learning, 2024. 2
[53] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin C. K.
Chan, and Chen Change Loy. Exploiting diffusion prior for
real-world image super-resolution, 2023. 2, 3, 4
[54] Lezhong Wang, Jeppe Revall Frisvad, Mark Bo Jensen, and
Siavash Arjomand Bigdeli. Stereodiffusion: Training-free
stereo image generation using latent diffusion models, 2024.
2
[55] Daniel Watson, William Chan, Ricardo Martin-Brualla,
Jonathan Ho, Andrea Tagliasacchi, and Mohammad
Norouzi. Novel view synthesis with diffusion models. In
ICLR , 2023. 2
[56] Weihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu.
Tedigan: Text-guided diverse face image generation and ma-
nipulation, 2021. 6
[57] Guy Yariv, Itai Gat, Lior Wolf, Yossi Adi, and Idan Schwartz.
Audiotoken: Adaptation of text-conditioned diffusion mod-
els for audio-to-image generation, 2023. 2
[58] Ahmet Burak Yildirim, Vedat Baday, Erkut Erdem, Aykut
Erdem, and Aysegul Dundar. Inst-inpaint: Instructing to re-
move objects with diffusion models, 2023. 2
[59] Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and
Jian Zhang. Freedom: Training-free energy-guided condi-
tional diffusion model, 2023. 2
[60] Shen Zhang, Zhaowei Chen, Zhenyu Zhao, Zhenyuan Chen,
Yao Tang, Yuhao Chen, Wengang Cao, and Jiajun Liang.
Hidiffusion: Unlocking high-resolution creativity and effi-
ciency in low-resolution trained diffusion models, 2023. 2
[61] Peiang Zhao, Han Li, Ruiyang Jin, and S. Kevin Zhou. Loco:
Locally constrained training-free layout-to-image synthesis,
2024. 2
[62] Qingping Zheng, Yuanfan Guo, Jiankang Deng, Jianhua
Han, Ying Li, Songcen Xu, and Hang Xu. Any-size-
diffusion: Toward efficient text-driven synthesis for any-size
hd images, 2023. 2
[63] ¬¥Alvaro Barbero Jim ¬¥enez. Mixture of diffusers for scene com-
position and high resolution image generation, 2023. 2, 3, 4
6612
