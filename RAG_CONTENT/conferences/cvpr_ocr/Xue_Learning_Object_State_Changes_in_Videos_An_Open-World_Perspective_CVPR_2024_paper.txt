Learning Object State Changes in Videos: An Open-World Perspective
Zihui Xue1,2Kumar Ashutosh1,2Kristen Grauman1,2
1The University of Texas at Austin2FAIR, Meta
Abstract
Object State Changes (OSCs) are pivotal for video un-
derstanding. While humans can effortlessly generalize OSC
understanding from familiar to unknown objects, current
approaches are conﬁned to a closed vocabulary. Address-
ing this gap, we introduce a novel open-world formulation
for the video OSC problem. The goal is to temporally local-
ize the three stages of an OSC—the object’s initial state,
its transitioning state, and its end state—whether or not
the object has been observed during training. Towards this
end, we develop V IDOSC, a holistic learning approach that:
(1) leverages text and vision-language models for super-
visory signals to obviate manually labeling OSC training
data, and (2) abstracts ﬁne-grained shared state representa-
tions from objects to enhance generalization. Furthermore,
we present HowToChange, the ﬁrst open-world benchmark
for video OSC localization, which offers an order of mag-
nitude increase in the label space and annotation volume
compared to the best existing benchmark. Experimental re-
sults demonstrate the efﬁcacy of our approach, in both tra-
ditional closed-world and open-world scenarios.1
1. Introduction
In video understanding, the study of objects primarily re-
volves around tasks like recognition [ 16], detection [ 24],
and tracking [ 8,62], with the assumption that objects main-
tain a consistent visual appearance throughout the video.
Yet, objects in video are often dynamic. They can undergo
transformations that change their appearance, shape, and
even topology. For example, a pineapple goes from whole
to peeled to sliced, or wood is carved into a new shape.
Object State Changes (OSCs) [ 2,15,17,22,47,50–
52,60,64] add a critical dimension to video understand-
ing. On the one hand, they provide insights into human
actions—observing a piece of metal being shaped into a
hook, for instance, implies the action of bending; observ-
ing an egg shell go from whole to broken implies the ac-
tion of cracking. On the other hand, OSCs are essential
1Project webpage: https : / / vision . cs . utexas . edu /
projects/VidOSC/ .
Figure 1. Top: The video OSC objective is to temporally localize
an object’s three states (i.e., initial, transitioning, end). Bottom:
OSCs naturally exhibit a long tail. Certain OSCs, such as melting
butter or marshmallow, are frequently showcased in instructional
videos while others like melting jaggery might be rarely seen. We
introduce an innovative open-world formulation that requires ex-
trapolating to novel objects never encountered during training.
for assessing goal completion, in a way that is invariant to
the speciﬁc procedure used. For instance, the readiness of
cake batter signiﬁes the completion of a mixing task, re-
gardless of whether it was stirred by hand or a mechanical
mixer. These core functionalities are instrumental for vari-
ous real-world applications [ 5,6,12,13,20,21,66], rang-
ing from AR/VR assistants [ 42] that guide users through
complex tasks by monitoring object states, to robotic ma-
nipulation [ 9,55] where understanding the state of objects
is critical for task planning and failure recovery.
However, the development of video OSC is still at a
primitive stage. Existing approaches [ 1,2,15,46,50,51,
58] assume a closed vocabulary , where each state transfor-
mation is associated with a limited set of objects—often just
1 or 2. For example, the concept of “melting” might be lim-
ited to familiar items like butter and marshmallow. Conse-
quently, the learned models are only capable of identifying
state changes for objects observed during training and stum-
ble when presented with novel objects. In contrast, in real-
world situations a single state transition like “melting” can
be linked with a plethora of objects—some ubiquitous and
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
18493
others rare. Importantly, there is an intrinsic connection that
threads these OSC processes together. As humans, even if
we have never encountered an unusual substance (e.g., jag-
gery) before, we can still understand that it is experiencing
a “melting” process based on its visual transformation cues
(see Fig. 1, bottom).
In light of these limitations, we propose a a novel open-
world formulation of the video OSC problem. We for-
mally characterize OSCs in videos in terms of three states
that must be temporally localized: initial, transitioning, and
end (see Fig. 1, top). In our open-world setting , there
areknown objects and novel objects. The model is only
presented with known objects during training (e.g., fry-
ing chicken, frying onions). Then, it is evaluated on both
known and novel objects (e.g., frying cauliﬂower). In ad-
dition to this fundamental open-world generalization, we
also tackle a more comprehensive notion of the transitioning
state. Speciﬁcally, our transitioning states encapsulate not
only action-induced modiﬁcations to the object (e.g., peel-
ing), but also passive transformations the object undergoes
without human interference (e.g., melting) and edited-out
transformations (e.g., the cake goes from raw to baked even
if baked off camera). Though largely overlooked in existing
approaches [ 1,15,46,58], these cases are both common and
technically interesting, since they force a model to reason
about the effects of the core state change rather than search
for evidence of a human actor carrying out the action.
Armed with this new problem formulation, we propose
a holistic video OSC learning approach, anchored by two
innovative ideas. First, we explore text and vision-language
models (VLMs) for supervisory signals during training. We
pioneer the use of textual state descriptions to generate a
long tail of object state pseudo-labels, by leveraging the re-
markable capabilities of VLMs [ 23,44,63] and large lan-
guage models (LLMs) [ 40,53,54]. This strategy eliminates
the need for exhaustive label collection for training data, fa-
cilitating large-scale model training.2Second, to confront
the open-world challenge, we propose object-agnostic state
prediction, consisting of three key techniques: a shared state
vocabulary to unify the label space across objects sharing
the same state transition (e.g., melting butter and melting
marshmallow), temporal modeling to grasp the progression
of state changes over time, and object-centric features that
better represent the objects during an OSC. These designs
equip our model with the ability to generalize state under-
standing from known objects to novel ones. We term our
approach V IDOSC.
Complementing this, we present a large-scale real-world
dataset HowToChange. Sourced from the HowTo100M col-
lection [ 36], it sets a new benchmark in the ﬁeld of unprece-
dented scale and an authentic long-tail distribution, setting
2Note that at test time, our model requires only the video, with no need
for additional text, ensuring utmost ﬂexibility and applicability.it apart from the constrained scope and closed-world setting
of earlier benchmarks (see Table 1). Finally, experimental
results demonstrate the efﬁcacy of V IDOSC, surpassing the
state-of-the-art in both traditional closed-world and novel
open-world scenarios by a great margin.
2. Related Work
Object State Changes Image-based methods explore the
compositional nature of objects and their attributes, includ-
ing zero-shot recognition of unseen combinations [ 22,29,
33,37–39,41,43], but do not consider the temporal pro-
gression of OSCs in video, which brings new challenges.
Video-based methods develop human-centric models that
leverage state change cues to facilitate action recognition
[1,15,46,58], or explore joint discovery of object states
and actions [ 2,50,51]. Notably, all the existing meth-
ods assume a closed world, recognizing only the OSC cat-
egories (“known” objects) seen during training. Our ap-
proach distinguishes itself in three crucial ways: (1) we in-
troduce a novel open-world3formulation, where the objects
encountered during evaluation can be entirely unseen dur-
ing training; (2) we adopt an object-centric perspective, al-
lowing for scenarios where OSCs occur with no observable
human actions in the video; and (3) we propose to utilize
the text modality and VLMs as supervisory signals, greatly
scaling up model training and boosting performance.
Early datasets for video OSC capture a limited array of
OSCs in fewer than 1,000 videos [ 2,31]. The more re-
cent ChangeIt dataset [ 50] marks an advance in dataset scale
(34K videos), yet is still restricted to a small OSC vocabu-
lary of 44 total object-state transitions. It lacks the scope
to adequately test unseen objects, since most state changes
coincide with only 1 or 2 objects across all the videos (see
Table 1). Other datasets explore different aspects of video
OSCs, including object state recognition [ 47], point-of-no-
return (PNR) frame localization [ 17], and object segmen-
tation [ 52,64], but they lack any temporal annotations of
ﬁne-grained OSC states needed for our task’s evaluation.
Our HowToChange dataset increases the OSC vocabulary
space by an order of magnitude and allows for the ﬁrst time
rigorous study of the open-world temporal OSC challenge.
Vision and Language LLMs [ 40,53,54] have revolution-
ized various research ﬁelds with their exceptional perfor-
mance across diverse applications. Building on this mo-
mentum, the use of web-scale image-text data has emerged
as a powerful paradigm in computer vision, with pow-
erful VLMs now advancing an array of image tasks, in-
cluding zero-shot classiﬁcation [ 44], detection [ 18], seg-
mentation [ 32] and visual question answering [ 27,28].
Similarly, joint video-text representation learning [ 3,30,
36,67] has been advanced by multimodal datasets like
3also called “unseen compositions” in object-attribute learning [ 34,37].
18494
HowTo100M [ 36] and Ego4D [ 17], which offer large-scale
collections of videos paired with their corresponding nar-
rations. These datasets have also facilitated tasks like step
discovery [ 11], localization in procedural activities [ 35] and
long egocentric videos [ 45]. The vision-language multi-
modal cycle consistency loss proposed in [ 14] helps dis-
cover long-term temporal dynamics in video. In line with
these developments, we propose to leverage the text ac-
companying instructional videos as well as existing high-
performing VLMs to provide supervisory signals that guide
the training of our video OSC model and allow learning for
the open world.
Learning in an Open World The open world setting
has received increasing attention, predominantly within
the image domain for object recognition [ 4,48], detec-
tion [ 10,18,25,65] and object-attribute compositional-
ity [29,33,37–39,41,43]. In the video domain, compo-
sitionality is advanced by the Something-Else dataset [ 34],
where the training combinations of verbs and nouns do not
overlap with the test set, sparking work on the dynamics of
subject-object interactions [ 7,34,47]. More recent efforts
leverage VLMs for zero-shot action recognition [ 7,26,57].
Concurrent work explores video object segmentation with
state-changing objects [ 64] and recognition of novel object-
state compositions for food chopped in different styles [ 47].
Despite these advances, temporal video OSC understanding
in the open world remains unexplored. Our open-world for-
mulation requires generalizing the temporal localization of
ﬁne-grained object states from known to novel objects.
3. Approach
We present the V IDOSC framework for learning video
OSCs, detailing the open-world formulation (Sec. 3.1),
model design for object-agnostic state prediction (Sec. 3.2),
and text-guided training scheme (Sec. 3.3).
3.1. Video OSC in the Open World
We begin by formally deﬁning an object state change (OSC)
in videos as a visually detectable transformation, where an
object experiences a change that is not easily reversible,
in line with [ 17]. We characterize an OSC category as
an object combined with a speciﬁc state transition, such
as “chicken + shredding”, and delineate an OSC process
through three distinct states: initial (precondition state),
transitioning, and end (postcondition state).4It is essential
to highlight that we take an object-centric perspective: the
“transitioning state” accounts for instances where the video
depicts an active action applied to the object, as well as sce-
narios where the object undergoes a passive transformation
absent of human intervention (e.g., melting, drying).
4Due to real-world data variation, some videos may lack one of the
OSC states, and there may be multiple segments in a video corresponding
to the same state. Our formulation accounts for all these scenarios.Given a video in which an object may be changing state,
the objective is to temporally localize each of the three OSC
states. Consistent with prior work [ 50,51], we formulate
the task as a frame-wise classiﬁcation problem. Formally,
a video sequence is represented by a temporal sequence of
Tfeature vectors X={x1,x2,···,xT}, where Tdenotes
the video duration. The goal is to predict the OSC state la-
bel for each timepoint, denoted by Y={y1,y2,···,yT},
yt2{1,···,K+1}, where Kis the total number of OSC
states, and there is one additional category representing the
background class not associated with any OSC state.5
Next, we propose an open-world problem formulation,
capturing the intrinsic long-tail distribution observed in
real-world scenarios. Consider Nstate transitions, each
paired with a set of associated objects, denoted by On=
{on
1,on
2,···,on
m(n)}, for n={1,2,···,N}, where m(n)
denotes the number of objects linked with the n-th state
transition. Within a speciﬁc state transition (like frying),
certain objects in the set (such as chicken or onions) are fre-
quently observed, while the combination of the same state
transition with other objects (such as cauliﬂower) appear
less often. Motivated by this inherent long-tail, we propose
to split Oninto two distinct subsets: On
known covering the
common combinations observed during training, and On
novel
comprising the infrequent combinations, which are unseen
during training due to their rarity. During inference, the
model is evaluated on the entire object set Onfor a com-
prehensive reﬂection of the open-world setting.
3.2. Object-Agnostic State Prediction
To address the open-world challenges and ensure robust
generalization to novel objects, V IDOSC integrates three
key techniques: (1) a shared state vocabulary that consoli-
dates the state label space; (2) temporal modeling within the
video OSC model design; and (3) object-centric features.
Shared State Vocabulary The inherent link among dif-
ferent OSC categories is overlooked in prior work. Com-
mon practice involves developing separate models for each
OSC category [ 2,50] (e.g., one model exclusively for melt-
ing butter and another for melting chocolate), or using one
single model that still treats every OSC as a distinct en-
tity in the label space [ 51] (e.g., considering melted but-
ter and melted chocolate as two separate end states). Such
a formulation results in an extensive label set with K=
3⇥PN
n=1m(n), where 3 denotes the number of OSC states
(i.e., initial, transitioning and end) andPN
n=1m(n)is the
total number of OSC categories. This compartmentalized
view can inadvertently hinder the model’s generalization
ability. Yet, on closer inspection, states across varied ob-
jects are intrinsically related. For instance, the “melting”
principle remains consistent, even when applied to visually
5Not all training videos may feature an OSC due to data collection
noise, yet all evaluation videos are manually veriﬁed to include one.
18495
[ASR transcription]you're going to use somerotisserie chicken so justget your rotisserie chickenand shred it up
(c) Model Training
Video EncoderDecoder............(d) Model Testing(b) Pseudo Label Generation(a) Mining for OSC examples
LLAMA2This video may contain theOSC of chicken + shredding(object + state transition)Known OSCsshredding chicken...shredding cabbageNovel OSCsshredding onionshredding coconut
State Description [whole chicken, shredding chicken, shredded chicken] ...
State = Initial State = Transitioning State = Background State = End CLIP
object features...
Figure 2. Our proposed V IDOSC framework: (a) Mining for OSC examples (Sec. 3.3): We leverage ASR transcriptions paired with
videos and the capabilities of LLM to automatically mine OSC examples. (b) Pseudo Label Generation (Sec. 3.3): We utilize textual
state descriptions and a VLM for supervisory signals during training; (c) Model Training (Sec. 3.2): We develop a video model for object-
agnostic state prediction. (d) Model Testing (Sec. 3.1): We propose an open-world formulation, evaluating on both known and novel OSCs.
Notably, while we employ the text modality to guide model training, our model is purely video-based and requires no text input at the test
phase, ensuring maximum ﬂexibility and applicability. Ground truth for the test set is manually annotated.
distinct objects like butter and chocolate. This motivates us
to group OSC labels that share the same state transition as
one, and train a single model per state transition.6By adopt-
ing object-agnostic OSC labels, we encourage the model to
learn state representations shared by visually different ob-
jects, and thus facilitate the transfer from known to novel
objects.
Temporal Modeling Recognizing that state changes un-
fold over time, it is important to capture the temporal dy-
namics in videos. An object’s state at any frame, be it
initial, transitioning, or end, is often best understood in
the context of preceding and succeeding frames. Con-
trary to prior works [ 2,50,51] that rely on isolated frame-
wise modeling without considering the temporal progres-
sion of video OSCs, we address this gap by proposing a
temporally-aware model design. As illustrated in Fig. 2(c),
we adopt an encoder-decoder architecture. For the encod-
ing phase, input video features X={x1,x2,···,xT}
are ﬁrst projected using an MLP fproject , and augmented
with sinusoidal positional embeddings, Zpos. Subsequently,
a transformer encoder [ 56]ftransformer is adopted to cap-
ture the temporal dynamics among these features, yield-
6See Supp. for the multi-task model variant, where we develop one
uniﬁed model for all state transitions.ingZ=ftransformer (fproject (X)+Zpos). For the decoding
phase, a MLP decoder g, maps these temporally-aware hid-
den representations to OSC state predictions: ˜Y=g(Z).
See Sec. 5for architecture and training details. This design
ensures that when predicting the state for a frame, the model
has assimilated temporal context from the entire sequence.
Essentially, our model exploits the fact that the dynamics of
the state change has greater invariance to the object cate-
gory than how the object looks, emphasizing temporal ob-
ject transformations over objects’ static appearances, and
thereby enhancing generalization to novel OSCs.
Object-Centric Features Finally, we discuss how to better
represent “object” features in the problem. In many scenar-
ios, due to camera placement and framing, the object going
through a state transition might only occupy a small portion
of the frame, surrounded by other visual elements such as
background, people, or bystander objects. Recognizing this
challenge, we introduce an enhancement to our model’s in-
put features Xto emphasize the object of interest. To be
speciﬁc, we leverage an off-the-shelf detector [ 49] to iden-
tify the active object (i.e., the object being manipulated) re-
gion at each timepoint t, yielding feature xobj
tthat centers
on the object (see bounding boxes in Fig. 2(c)). The input
feature is then constructed as a concatenation of the orig-
inal global feature and the localized object-centric feature,
18496
i.e.,X={[xt,xobj
t]}T
t=1. By emphasizing the object in this
manner, our model is better positioned to discern the intri-
cate state changes and provide more informed predictions.
3.3. Text and VLMs as Supervision
To scale up training and ensure broad generalization, we
propose a novel training pipeline that leverages LLMs and
VLMs for OSC mining and pseudo-labeling.
Mining for OSC examples Utilizing a vast collection of
“how-to” instructional videos as the training source, we
develop an automated mining process to capture the rich,
real-world variability of OSCs. The motivation is that in-
structional videos usually have accompanying Automatic
Speech Recognition (ASR) transcriptions that offer valu-
able OSC cues. For example, a speaker mentioning, “so just
get your rotisserie chicken and shred it up” suggests that the
chicken may undergo a state transition of shredding in the
video. Leveraging this fact, we employ LLAMA2 [ 54] to
analyze ASR transcriptions for identifying candidate videos
and their associated OSC categories. This text mining stage
allows us to corral a long tail of OSCs, discovering likely
state change terms—even if rare—from the free-form nat-
ural language narrations given by how-to instructors in the
training videos. See Fig. 2(a).
Pseudo Label Generation Next, to negate the need of
manually labeling large-scale training data, we propose a
novel pseudo-labeling pipeline facilitated by VLMs. From
the identiﬁed OSC category (e.g., shredding chicken), we
form three textual state descriptions for its initial, tran-
sitioning, and end states (e.g., whole chicken, shredding
chicken, and shredded chicken). We then adopt both the
vision and language encoder from a well-trained VLM (we
experiment with CLIP [ 44] and VideoClip [ 61]) to compute
the cross-modal similarity between every frame in training
video and the three state descriptions, producing a score ma-
trixS2RT⇥3. The pseudo label ˆytat timepoint tis then
assigned based on this score matrix:
ˆyt=8
>>>>><
>>>>>:Background ifP(S[t,:])<⌧
Initial elif S[t,0] S[t,1]>  andS[t,0] S[t,2]> 
Transitioning elif S[t,1] S[t,0]>  andS[t,1] S[t,2]> 
End elif S[t,2] S[t,0]>  andS[t,2] S[t,1]> 
Ambiguous otherwise
where  is the threshold that separates states and ⌧is the
threshold that differentiates between states and background.
Essentially if the VLM scores some state more strongly than
the other two—and the cumulative conﬁdence score of all
states is high—then it is adopted as the pseudo label. Oth-
erwise it is omitted as ambiguous. See Fig. 2(b).
To further reﬁne these labels, we enforce a causal order-
ing constraint. Given the inherent progression of OSCs, the
anticipated order is initial states followed by transitioning
states, and ﬁnally, the end states. Any frame whose pseudoFigure 3. Ground truth annotation distribution across 20 state tran-
sitions (top) and 134 objects (bottom) in HowToChange (Evalua-
tion). In line with our open-world formulation, annotations cover a
diverse range of object-state transition combinations, categorized
into known and novel OSCs.
label does not respect this natural progression is re-assigned
to the ambiguous category. Our training employs a cross-
entropy loss between pseudo label ˆytand the corresponding
model prediction ˜yt, with ambiguous frames excluded to
maintain clarity and distinction among the state labels. See
Section 4.2in Supp. for detailed pseudo label analysis.
4. The HowToChange Dataset
Existing OSC datasets fall short in capturing the open-
world’s diversity and long-tail distribution of OSCs. To ad-
dress this gap, we introduce HowToChange. It encompasses
varied state transitions coupled with a diverse range of ob-
jects, providing an authentic reﬂection of their real-world
frequency—from the commonplace to the rare.
Data Collection The HowTo100M collection [ 36] contains
a wealth of instructional videos that often feature OSCs and
is particularly suitable for this task. We speciﬁcally focus
on the HowTo100M Food & Entertaining category because
(1) it constitutes a third of the entire HowTo100M videos,
(2) cooking tasks offer a wealth of objects, tools, and state
changes, providing an excellent test bed for open-world
OSC, and (3) in cooking activities a single state transition
can often be associated with a varied range of objects, open-
ing the door to learning compositionality. (Note, we also ex-
periment with non-cooking domains below.) We process a
total of 498,475 videos and 11,390,287 ASR transcriptions
with LLAMA2. From the responses, we identify the most
frequently seen state transitions and objects associated with
them to establish an OSC vocabulary, resulting in 134 ob-
jects, 20 state transitions, and 409 unique OSCs. The num-
ber of objects associated with each state transition ( m(n))
18497
Datasets # Obj # ST # OSC ObjPer # VideosGT
Label?
Alayrac et al. [ 2] 5 6 7 1.2 630 3
Task-Fluent [ 31] 25 14 32 2.3 809 3
ChangeIt (Training) [ 50] 42 27 44 1.6 34,428 7
ChangeIt (Evaluation) [ 50] 42 27 44 1.6 667 3
HowToChange (Training) 122 20 318 15.9 36,075 7
HowToChange (Evaluation) 134 20 409 20.5 5,424 3
Table 1. Comparison with existing video datasets focusing on ob-
ject states. ‘Obj’ and ‘ST’ represent objects and state transitions,
respectively. ‘ObjPer’ denotes the average number of objects asso-
ciated with each state transition; higher values indicate more need
to generalization across objects. We present the ﬁrst open-world
benchmark for temporal video OSC, with an order of magnitude
increase in OSC vocabulary and annotation volume.
spans from 6 for “zesting” to 55 for “chopping”. The state
transitions applied to each object vary from 1 to 15, with
onions being the most versatile. In total, we identify 36,075
videos for the training set of HowToChange, with an aver-
age duration of 41.2 seconds.
Data Splits In line with our open-world formulation, we di-
vide the 409 identiﬁed OSCs into two disjoint subsets based
on their frequency of occurrence. Within each state transi-
tion, we categorize the top 75% frequent objects as known
and the bottom 25% as novel. This yields a total of 318
known OSCs that are seen during training and testing, span-
ning 20 state transitions associated with 122 objects, and 91
novel OSCs that are only seen during testing, encompassing
the same transitions across 58 objects.
Ground Truth Label Collection (Evaluation Set) To fa-
cilitate thorough evaluation, we obtain manual annotations
for a subset of 5,424 videos from the collected dataset.7The
annotation workﬂow is as follows: each annotator is pre-
sented with a video segment along with an OSC category
that was previously identiﬁed by LLAMA2. The annota-
tor has the option to reject the video segment if it does not
contain the speciﬁed OSC. Otherwise, they label the time
ranges corresponding to the initial, transitioning, and end
states of the OSC. Adhering to our object-centric empha-
sis, annotators are instructed to label based on the visual
changes of the object, rather than human-centric actions,
and exclude time ranges where the object of interest is not
visible, ensuring clean and focused temporal labels. Fig. 3
provides the distribution of annotated videos. On average,
we collect 271 annotations per state transition, with a video
duration of 41.5 seconds, and 12.9% of videos belong to
the novel category. The entire annotation process required
around 1,507 hours by 30 professional annotators.
Dataset Comparison Table 1compares HowToChange
with existing video datasets on temporal OSC understand-
7Our training is purely guided by a VLM and requires no ground truth
labels. The annotations are reserved exclusively for evaluation.ing. HowToChange offers an unprecedented scale—with
9.3x more OSC categories and 8.1x more annotated video
instances compared to the previous largest collection [ 50].
Furthermore, notably, in prior datasets, each state transition
is typically coupled with 1 or 2 objects, preventing sub-
sequent models from generalizing to new objects, as we
will see in results. In contrast, HowToChange pioneers the
open-world formulation, presenting a much broader range
of objects associated with each state transition—from 6 to
55 objects per state transition, and averaging 20. This fa-
cilitates the development of models with generalized OSC
understanding. Please see Supp. for full data collection and
annotation details.
5. Experiments
Datasets In addition to our new HowToChange dataset,
we also evaluate on ChangeIt [ 50] due to its expansive
data scale (34K videos spanning many activities) and high
relevance to our task. Beyond the conventional split of
ChangeIt, we propose a new split tailored to our open-world
formulation. Speciﬁcally, from the 44 available OSC cat-
egories in ChangeIt, we concentrate on the 25 categories
where each state transition is paired with more than one
object. With those, we form the “ChangeIt (open-world)”
subset that comprises 8 state transitions and 25 objects.
Within each state transition, objects are randomly divided
into known and novel categories, yielding 13 known OSC
categories for training and 12 novel OSC categories exclu-
sively reserved for evaluation. A detailed breakdown of this
split can be found in Supp. To sum up, our evaluation en-
compasses: ChangeIt; ChangeIt (open-world); and How-
ToChange, offering a comprehensive setup in both closed-
world and open-world scenarios.
Evaluation For ChangeIt and ChangeIt (open-world), we
adhere to the original dataset’s evaluation protocol [ 50], re-
porting action and state precision@1 as the evaluation met-
rics. For our dataset, besides precision@1, which evaluates
a single frame for each state within a video, we advocate for
the use of F1 score and precision over all frames to ensure
a more holistic evaluation.
Baselines We compare our approach with four base-
lines across two categories: (a) self-supervised approaches
on identifying object states enforced by the causal order-
ing constraint: LookForTheChange [ 50] trains a dedicated
model for each OSC category, while MultiTaskChange [ 51]
evolves this into a multi-task approach8, catering to sev-
eral OSCs concurrently; (b) zero-shot VLMs: image-
based CLIP [ 44], video-based VideoCLIP [ 61] and Intern-
Video [ 59]. All baselines in (a) use the same training data
as our model, whereas the zero-shot models (b) are directly
8To ensure a thorough evaluation, we train both single-task and multi-
task variants of our approach. See Supp. for a detailed discussion.
18498
MethodChangeIt ChangeIt (open-world) HowToChange
State
Prec.@1Action
Prec.@1State Prec.@1 Action Prec.@1 F1 (%) Prec (%) Prec.@1 (%)
known novel known novel known novel known novel known novel
CLIP [ 44] 0.30 0.63 0.29 0.29 0.71 0.70 26.9 25.4 27.3 26.6 47.5 47.5
VideoCLIP [ 61] 0.33 0.59 0.25 0.24 0.62 0.55 36.6 34.3 39.7 38.5 48.3 44.8
InternVideo [ 59] 0.27 0.57 0.29 0.25 0.60 0.61 29.9 29.5 31.4 30.8 46.9 46.3
LookForTheChange [ 50] 0.35 0.68 0.36 0.25 0.77 0.68 30.3 28.7 32.5 30.0 37.2 36.1
MultiTaskChange [ 51] 0.49 0.80 0.41 0.22 0.72 0.62 33.9 29.9 38.5 34.1 43.1 38.8
VIDOSC (ours) 0.57 0.84 0.56 0.48 0.89 0.82 46.4 43.1 46.6 43.7 60.7 58.2
Table 2. Results on ChangeIt, ChangeIt (open-world), and HowToChange. V IDOSC outperforms all approaches in both closed-world and
open-world scenarios, across known and novel OSCs.
peeling avocadotying ropeTransitioning StateInitial StateEnd Statetying tiepeeling dragon fruitgrating orangegrating cauliflowerKnown OSCsNovel OSCs
chopping shallotchopping capsicumTransitioning StateInitial StateEnd State
Figure 4. Top-1 frame predictions given by V IDOSC for the initial, transitioning, and end states, on ChangeIt (open-world) (ﬁrst 2 rows)
and HowToChange (last 2 rows). V IDOSC not only accurately localizes the three ﬁne-grained states for known OSCs, but also generalizes
this understanding to novel objects, such as cauliﬂower and capsicum, which are not observed during training.
evaluated on the test set with no training.
Implementation Videos are sampled at one frame per sec-
ond. Each one-second video segment gets assigned to an
OSC state label, and is encoded by InternVideo [ 59], a
general video foundation model (which is kept frozen for
training efﬁciency). Our video model consists of a 3-layer
transformer with a hidden dimension of 512 and 4 atten-
tion heads as the encoder and a 1-layer MLP as the decoder.
Consistent with prior work [ 50,51], our model predicts
the OSC state label (i.e., initial, transitioning, end state, or
background) for a video, assuming the video OSC category
is known (say from a recognition model’s output or a user’s
speciﬁc query). While the standard output does not include
the state transition name, our multi-task version detailed in
Supp. is capable of this.
Main Results Table 2presents results on all three datasets.
VIDOSC outperforms all approaches in both traditional
closed-world and our newly introduced open-world set-
tings. The large performance gains—as much as 9.6%jumps in precision vs. the next best method—underscore
the effectiveness of two pivotal components in V IDOSC.
First, its use of text and VLMs for supervisory signals dur-
ing training: particularly on novel OSCs , V IDOSC extracts
meaningful cues from the video modality to reﬁne pseudo
labels from VLMs, and ultimately surpasses the VLM base-
lines. Second, our model for object-agnostic state predic-
tion, designed for the open-world context, effectively nar-
rows the gap between known and novel OSCs. For instance,
on ChangeIt (open-world), MultiTaskChange [ 51] experi-
ences a 19% decline in state precision@1 while V IDOSC
has only a 8% drop. Note that we observe a more pro-
nounced performance drop of all approaches on ChangeIt
(open-world) than on HowToChange due to its limited num-
ber of objects available per state transition. These results
also point to the potential for future development in bridg-
ing the known and novel OSC performance gap.
Qualitative Results Fig.4presents V IDOSC’s top-1 frame
predictions on ChangeIt (open-world) and HowToChange,
18499
CLIPVideoCLIPLookForTheChangeMultiTaskChangeVIDOSC (ours)Ground TruthSlicing Shallot
BackgroundInitial StateTransitioning StateEnd StateInternVideo
Figure 5. Comparison of model predictions across a test video
depicting the OSC of “slicing shallot” on HowToChange. The
x-axis represents temporal progression through the video. V I-
DOSC gives temporally smooth and coherent predictions that best
align with the ground truth, signiﬁcantly outperforming baselines
in capturing the video’s global temporal context.
Shared Temporal Object Prec.@1 (%)
State Modeling Centric known novel  
7 3 3 58.5 53.3 5.2
3 7 3 52.9 48.2 4.7
3 3 7 59.8 56.7 3.1
3 3 3 60.7 58.2 2.5
Table 3. Ablation Study.  denotes the performance gap between
known OSCs and novel OSCs.
across both known and novel OSCs. Notably, despite never
seeing objects such as cauliﬂower and capsicum during
training, V IDOSC effectively leverages visual state change
cues and correctly localizes the three states of these objects
going through OSCs, demonstrating its strong generaliza-
tion capability. For a more holistic view, Fig. 5compares
VIDOSC’s frame-by-frame predictions with all baselines
for a given test video. V IDOSC provides temporally co-
herent predictions, smoothly progressing through the OSC
states in the natural order (i.e., from initial to transitioning
then end). In contrast, baseline approaches often yield frag-
mented and inconsistent predictions, indicating a lack of
understanding of the video’s global temporal context, pri-
marily due to their reliance on frame-wise modeling. See
Supp. and Supp. video for more qualitative examples and
VIDOSC’s interpretability on object relations.
Ablation To further dissect the performance gains brought
by our three model design techniques (i.e., shared state vo-
cabulary, temporal modeling, and object-centric features),
we conduct an ablation study, removing one component at
a time. Table 3conﬁrms the essential role of each ele-
ment. A shared state vocabulary is particularly crucial in
the open-world context, as its absence increases the gap be-
tween known and novel OSCs from 2.5% to 5.2%. Fur-
thermore, temporal modeling provides a substantial perfor-
mance boost, and object-centric features offer further gains.
See Supp. for an additional analysis of V IDOSC’s perfor-
mance with different pseudo labels.
Query (novel) Retrieved Nearest (known)
cauliflower (pre-mashing)potato (post-mashing) strawberry (post-mashing)Retrieved Furthest (known)
pepper (pre-grilling)tomato (post-grilling)zucchini (post-grilling)peach (pre-peeling)squash (post-peeling)garlic (post-peeling)
Figure 6. Frame retrieval on the HowToChange test set. Given a
query frame showcasing a novel OSC at its initial state, V IDOSC
retrieves the nearest and furthest frame among all known OSCs at
their endstates. The closest post-OSC frame follows the transition
trajectory of the query while the furthest post-OSC frame depicts a
substantially different object, demonstrating V IDOSC’s capability
to generalize the OSC progression for novel objects.
Frame Retrieval VIDOSC learns features that accurately
characterize the evolution of an OSC process. To illus-
trate, we consider a novel frame retrieval setting. Within
the HowToChange test set, when presented with a query
video featuring a novel OSC at its initial state, V IDOSC
seeks the most similar and most contrasting video from a
pool of candidates at their end states. The frame triplets
with the smallest and largest feature distance are shown in
Fig.6. Remarkably, the retrieved nearest post-OSC frames
correspond to the query’s anticipated state transition trajec-
tory, despite the object and state gap. Conversely, the fur-
thest frames exhibit end states of markedly different objects.
These results further lend support to V IDOSC’s capability
to understand the evolution of an OSC process, even for
novel objects it has never encountered during training.
6. Conclusion
This work aims at a comprehensive exploration of video
OSCs, with a novel open-world formulation. To address the
challenges, we leverage text and VLMs to assist the train-
ing of a video OSC model at scale and design three mod-
eling techniques to achieve object-agnostic state prediction
for better generalization to novel OSCs. Furthermore, we
present the most expansive video OSC dataset collection
HowToChange, which echoes the natural long-tail of state
transitions coupled with varied objects, fostering a realistic
representation of real-world scenarios. As for future work,
we will consider extending V IDOSC to video sequences
featuring concurrent OSC processes, and integrating spatial
understanding of OSC within our open-world framework.
Acknowledgements: UT Austin is supported in part by the IFML
NSF AI Institute. KG is paid as a research scientist by Meta.
18500
References
[1]Nachwa Aboubakr, James L Crowley, and R ´emi Ron-
fard. Recognizing manipulation actions from state-
transformations. arXiv preprint arXiv:1906.05147 , 2019. 1,
2
[2]Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, and Simon
Lacoste-Julien. Joint discovery of object states and manip-
ulation actions. In Proceedings of the IEEE International
Conference on Computer Vision , pages 2127–2136, 2017. 1,
2,3,4,6
[3]Kumar Ashutosh, Rohit Girdhar, Lorenzo Torresani, and
Kristen Grauman. Hiervl: Learning hierarchical video-
language embeddings. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 23066–23078, 2023. 2
[4]Abhijit Bendale and Terrance Boult. Towards open world
recognition. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1893–1902,
2015. 3
[5]Jing Bi, Jiebo Luo, and Chenliang Xu. Procedure planning
in instructional videos via contextual modeling and model-
based policy learning. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 15611–
15620, 2021. 1
[6]Chien-Yi Chang, De-An Huang, Danfei Xu, Ehsan Adeli,
Li Fei-Fei, and Juan Carlos Niebles. Procedure planning in
instructional videos. In European Conference on Computer
Vision , pages 334–350. Springer, 2020. 1
[7]Dibyadip Chatterjee, Fadime Sener, Shugao Ma, and Angela
Yao. Opening the vocabulary of egocentric actions. arXiv
preprint arXiv:2308.11488 , 2023. 3
[8]Gioele Ciaparrone, Francisco Luque S ´anchez, Siham Tabik,
Luigi Troiano, Roberto Tagliaferri, and Francisco Herrera.
Deep learning in video multi-object tracking: A survey. Neu-
rocomputing , 381:61–88, 2020. 1
[9]Xinke Deng, Yu Xiang, Arsalan Mousavian, Clemens Epp-
ner, Timothy Bretl, and Dieter Fox. Self-supervised 6d object
pose estimation for robot manipulation. In 2020 IEEE In-
ternational Conference on Robotics and Automation (ICRA) ,
pages 3665–3671. IEEE, 2020. 1
[10] Akshay Dhamija, Manuel Gunther, Jonathan Ventura, and
Terrance Boult. The overlooked elephant of object detection:
Open set. In Proceedings of the IEEE/CVF Winter Confer-
ence on Applications of Computer Vision , pages 1021–1030,
2020. 3
[11] Nikita Dvornik, Isma Hadji, Ran Zhang, Konstantinos G
Derpanis, Richard P Wildes, and Allan D Jepson. Step-
former: Self-supervised step discovery and localization in
instructional videos. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
18952–18961, 2023. 3
[12] Dave Epstein and Carl Vondrick. Learning goals from fail-
ure. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 11194–11204,
2021. 1
[13] Dave Epstein, Boyuan Chen, and Carl Vondrick. Oops!
predicting unintentional action in video. In Proceedings ofthe IEEE/CVF conference on computer vision and pattern
recognition , pages 919–929, 2020. 1
[14] Dave Epstein, Jiajun Wu, Cordelia Schmid, and Chen Sun.
Learning temporal dynamics from cycles in narrated video.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 1480–1489, 2021. 3
[15] Alireza Fathi and James M Rehg. Modeling actions through
state changes. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 2579–
2586, 2013. 1,2
[16] Tao Gong, Kai Chen, Xinjiang Wang, Qi Chu, Feng Zhu,
Dahua Lin, Nenghai Yu, and Huamin Feng. Temporal roi
align for video object recognition. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence , pages 1442–
1450, 2021. 1
[17] Kristen Grauman, Andrew Westbury, Eugene Byrne,
Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson
Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d:
Around the world in 3,000 hours of egocentric video. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 18995–19012, 2022. 1,2,3
[18] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
Open-vocabulary object detection via vision and language
knowledge distillation. arXiv preprint arXiv:2104.13921 ,
2021. 2,3
[19] Tengda Han, Weidi Xie, and Andrew Zisserman. Temporal
alignment networks for long-term video. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 2906–2916, 2022. 1
[20] Farnoosh Heidarivincheh, Majid Mirmehdi, and Dima
Damen. Detecting the moment of completion: Temporal
models for localising action completion. arXiv preprint
arXiv:1710.02310 , 2017. 1
[21] Farnoosh Heidarivincheh, Majid Mirmehdi, and Dima
Damen. Action completion: A temporal model for moment
detection. arXiv preprint arXiv:1805.06749 , 2018. 1
[22] Phillip Isola, Joseph J. Lim, and Edward H. Adelson. Dis-
covering states and transformations in image collections. In
CVPR , 2015. 1,2
[23] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representa-
tion learning with noisy text supervision. In International
conference on machine learning , pages 4904–4916. PMLR,
2021. 2
[24] Licheng Jiao, Ruohan Zhang, Fang Liu, Shuyuan Yang, Biao
Hou, Lingling Li, and Xu Tang. New generation deep learn-
ing for video object detection: A survey. IEEE Trans-
actions on Neural Networks and Learning Systems , 33(8):
3195–3215, 2021. 1
[25] KJ Joseph, Salman Khan, Fahad Shahbaz Khan, and Vi-
neeth N Balasubramanian. Towards open world object de-
tection. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 5830–5840,
2021. 3
[26] Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi
Xie. Prompting visual-language models for efﬁcient video
18501
understanding. In European Conference on Computer Vi-
sion, pages 105–124. Springer, 2022. 3
[27] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for uni-
ﬁed vision-language understanding and generation. In In-
ternational Conference on Machine Learning , pages 12888–
12900. PMLR, 2022. 2
[28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. arXiv
preprint arXiv:2301.12597 , 2023. 2
[29] Xiangyu Li, Xu Yang, Kun Wei, Cheng Deng, and Muli
Yang. Siamese contrastive embedding network for composi-
tional zero-shot learning. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition ,
pages 9326–9335, 2022. 2,3
[30] Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, Michael
Wray, Rui Yan, Eric Z XU, Difei Gao, Rong-Cheng Tu, Wen-
zhe Zhao, Weijie Kong, et al. Egocentric video-language
pretraining. Advances in Neural Information Processing Sys-
tems, 35:7575–7586, 2022. 2
[31] Yang Liu, Ping Wei, and Song-Chun Zhu. Jointly recogniz-
ing object ﬂuents and tasks in egocentric videos. In Pro-
ceedings of the IEEE International Conference on Computer
Vision , pages 2924–2932, 2017. 2,6,1
[32] Timo L ¨uddecke and Alexander Ecker. Image segmenta-
tion using text and image prompts. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 7086–7096, 2022. 2
[33] Massimiliano Mancini, Muhammad Ferjad Naeem, Yongqin
Xian, and Zeynep Akata. Open world compositional zero-
shot learning. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 5222–
5230, 2021. 2,3
[34] Joanna Materzynska, Tete Xiao, Roei Herzig, Huijuan Xu,
Xiaolong Wang, and Trevor Darrell. Something-else: Com-
positional action recognition with spatial-temporal interac-
tion networks. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 1049–
1059, 2020. 2,3
[35] Effrosyni Mavroudi, Triantafyllos Afouras, and Lorenzo
Torresani. Learning to ground instructional articles in videos
through narrations. arXiv preprint arXiv:2306.03802 , 2023.
3
[36] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,
Makarand Tapaswi, Ivan Laptev, and Josef Sivic.
Howto100m: Learning a text-video embedding by watching
hundred million narrated video clips. In Proceedings of the
IEEE/CVF international conference on computer vision ,
pages 2630–2640, 2019. 2,3,5,1,6
[37] Ishan Misra, Abhinav Gupta, and Martial Hebert. From red
wine to red tomato: Composition with context. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 1792–1801, 2017. 2,3
[38] Muhammad Ferjad Naeem, Yongqin Xian, Federico
Tombari, and Zeynep Akata. Learning graph embeddings
for compositional zero-shot learning. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 953–962, 2021.
[39] Tushar Nagarajan and Kristen Grauman. Attributes as op-
erators: factorizing unseen attribute-object compositions. In
Proceedings of the European Conference on Computer Vi-
sion (ECCV) , pages 169–185, 2018. 2,3
[40] OpenAI. Gpt-4 technical report. Technical report, OpenAI,
2023. Submitted on 15 Mar 2023, last revised 27 Mar 2023.
2,6,9
[41] Khoi Pham, Kushal Kaﬂe, Zhe Lin, Zhihong Ding, Scott
Cohen, Quan Tran, and Abhinav Shrivastava. Learning
to predict visual attributes in the wild. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 13018–13028, 2021. 2,3
[42] Chiara Plizzari, Gabriele Goletto, Antonino Furnari, Sid-
dhant Bansal, Francesco Ragusa, Giovanni Maria Farinella,
Dima Damen, and Tatiana Tommasi. An outlook into the fu-
ture of egocentric vision. arXiv preprint arXiv:2308.07123 ,
2023. 1
[43] Senthil Purushwalkam, Maximilian Nickel, Abhinav Gupta,
and Marc’Aurelio Ranzato. Task-driven modular networks
for zero-shot compositional learning. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 3593–3602, 2019. 2,3
[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 2,5,6,7,9
[45] Santhosh Kumar Ramakrishnan, Ziad Al-Halah, and Kris-
ten Grauman. Naq: Leveraging narrations as queries to su-
pervise episodic memory. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 6694–6703, 2023. 3
[46] Nirat Saini, Bo He, Gaurav Shrivastava, Sai Saketh Ramb-
hatla, and Abhinav Shrivastava. Recognizing actions using
object states. In ICLR2022 Workshop on the Elements of
Reasoning: Objects, Structure and Causality , 2022. 1,2
[47] Nirat Saini, Hanyu Wang, Archana Swaminathan, Vinoj
Jayasundara, Bo He, Kamal Gupta, and Abhinav Shrivas-
tava. Chop & learn: Recognizing and generating object-state
compositions. ICCV , 2023. 1,2,3
[48] Walter J Scheirer, Anderson de Rezende Rocha, Archana
Sapkota, and Terrance E Boult. Toward open set recogni-
tion. IEEE transactions on pattern analysis and machine
intelligence , 35(7):1757–1772, 2012. 3
[49] Dandan Shan, Jiaqi Geng, Michelle Shu, and David F
Fouhey. Understanding human hands in contact at inter-
net scale. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 9869–9878,
2020. 4
[50] Tom´aˇs Sou ˇcek, Jean-Baptiste Alayrac, Antoine Miech, Ivan
Laptev, and Josef Sivic. Look for the change: Learning
object states and state-modifying actions from untrimmed
web videos. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 13956–
13966, 2022. 1,2,3,4,6,7,5
18502
[51] Tom´aˇs Sou ˇcek, Jean-Baptiste Alayrac, Antoine Miech,
Ivan Laptev, and Josef Sivic. Multi-task learning of ob-
ject state changes from uncurated videos. arXiv preprint
arXiv:2211.13500 , 2022. 1,2,3,4,6,7,5,8
[52] Pavel Tokmakov, Jie Li, and Adrien Gaidon. Breaking the”
object” in video object segmentation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 22836–22845, 2023. 1,2
[53] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efﬁcient foundation language models.
arXiv preprint arXiv:2302.13971 , 2023. 2
[54] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and ﬁne-tuned chat models. arXiv
preprint arXiv:2307.09288 , 2023. 2,5,1
[55] Jonathan Tremblay, Thang To, Balakumar Sundaralingam,
Yu Xiang, Dieter Fox, and Stan Birchﬁeld. Deep object pose
estimation for semantic robotic grasping of household ob-
jects. arXiv preprint arXiv:1809.10790 , 2018. 1
[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 4
[57] Mengmeng Wang, Jiazheng Xing, and Yong Liu. Actionclip:
A new paradigm for video action recognition. arXiv preprint
arXiv:2109.08472 , 2021. 3
[58] Xiaolong Wang, Ali Farhadi, and Abhinav Gupta. Actions˜
transformations. In Proceedings of the IEEE conference
on Computer Vision and Pattern Recognition , pages 2658–
2667, 2016. 1,2
[59] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun
Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun
Wang, et al. Internvideo: General video foundation models
via generative and discriminative learning. arXiv preprint
arXiv:2212.03191 , 2022. 6,7,9
[60] Te-Lin Wu, Yu Zhou, and Nanyun Peng. Localizing active
objects from egocentric vision with symbolic world knowl-
edge. arXiv preprint arXiv:2310.15066 , 2023. 1
[61] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko,
Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and
Christoph Feichtenhofer. Videoclip: Contrastive pre-training
for zero-shot video-text understanding. arXiv preprint
arXiv:2109.14084 , 2021. 5,6,7,9
[62] Rui Yao, Guosheng Lin, Shixiong Xia, Jiaqi Zhao, and Yong
Zhou. Video object segmentation and tracking: A survey.
ACM Transactions on Intelligent Systems and Technology
(TIST) , 11(4):1–47, 2020. 1
[63] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-
jtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive
captioners are image-text foundation models. arXiv preprint
arXiv:2205.01917 , 2022. 2
[64] Jiangwei Yu, Xiang Li, Xinran Zhao, Hongming Zhang, and
Yu-Xiong Wang. Video state-changing object segmentation.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 20439–20448, 2023. 1,2,3[65] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-
Fu Chang. Open-vocabulary object detection using captions.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 14393–14402, 2021.
3
[66] He Zhao, Isma Hadji, Nikita Dvornik, Konstantinos G Der-
panis, Richard P Wildes, and Allan D Jepson. P3iv: Prob-
abilistic procedure planning from instructional videos with
weak supervision. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
2938–2948, 2022. 1
[67] Yue Zhao, Ishan Misra, Philipp Kr ¨ahenb ¨uhl, and Rohit
Girdhar. Learning video representations from large lan-
guage models. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 6586–
6597, 2023. 2
18503
