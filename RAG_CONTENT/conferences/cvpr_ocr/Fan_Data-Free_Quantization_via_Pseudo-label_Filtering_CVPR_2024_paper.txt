Data-Free Quantization via Pseudo-label Filtering
Chunxiao Fan1,2, Ziqi Wang1, Dan Guo1,2*, Meng Wang1,2
1School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, 230009, Anhui, China
2Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, Hefei, 230088, Anhui, China
fanchunxiao@hfut.edu.cn, zackiewang29@gmail.com, guodan@hfut.edu.cn, eric.mengwang@gmail.com
Abstract
Quantization for model compression can efficiently re-
duce the network complexity and storage requirement, but
the original training data is necessary to remedy the perfor-
mance loss caused by quantization. The Data-Free Quan-
tization (DFQ) methods have been proposed to handle the
absence of original training data with synthetic data. How-
ever, there are differences between the synthetic and orig-
inal training data, which affects the performance of the
quantized network, but none of the existing methods con-
siders the differences. In this paper, we propose an efficient
data-free quantization via pseudo-label filtering, which is
the first to evaluate the synthetic data before quantization.
We design a new metric for evaluating synthetic data us-
ing self-entropy, which indicates the reliability of synthetic
data. The synthetic data can be categorized with the met-
ric into high- and low-reliable datasets for the following
training process. Besides, the multiple pseudo-labels are
designed to label the synthetic data with different reliabil-
ity, which can provide valuable supervision information and
avoid misleading training by low-reliable samples. Exten-
sive experiments are implemented on several datasets, in-
cluding CIFAR-10, CIFAR-100, and ImageNet with various
models. The experimental results show that our method can
perform excellently and outperform existing methods in ac-
curacy.
1. Introduction
Deep Neural Networks (DNNs) [26] have shown tremen-
dous potential in a number of fields, but their high com-
puting costs and storage requirements make the implemen-
tation complex, especially on embedded systems and edge
devices [9, 36]. In recent years, many model compression
methods [7] have been proposed to decrease the computa-
tional and memory requirements while keeping the perfor-
mance. The existing methods can be divided into network
*Corresponding author.
0.25 0.50 0.750.51.01.52.0original
synthetic
ùìóùê¨ùêûùê•ùêü 0.000.0densityFigure 1. The self-entropy of the prediction results on the origi-
nal and synthetic data using pre-trained network. The prediction
results on the synthetic dataset always have a higher self-entropy
than that on the original dataset, owing to different reliability.
pruning [41, 45], model quantization [2, 21], knowledge
distillation [18, 31], and neural architecture search (NAS)
[33, 39]. Among these techniques, quantization uses finite
approximations to represent the full-precision values in the
pre-trained network, which needs to be quantized. It can
efficiently reduce the network complexity for acceleration
and storage, but the approximate operation inevitably af-
fects the network performance, resulting in accuracy drops
after quantization.
To reduce the performance loss caused by quantiza-
tion, many methods propose to optimize the quantizer
[1, 2, 10, 11, 35] or retrain the pre-trained network with
quantization constraint [4, 15, 27]. In these methods, the
original training data is very helpful for maintaining model
performance, but it may not be feasible due to data privacy
concerns in specific scenarios. The Data-Free Quantization
(DFQ) methods [3, 5, 28, 30, 42, 47, 48] have been pro-
posed to deal with the absence of original training data. In
principle, the batch normalization (BN) [20] layers in the
pre-trained model contain the statistical information of the
original training data, i.e. mean and variance, which can
be regarded as prior information for data synthesis [46].
Thus, the synthetic data can be generated from random ini-
tial data by guiding its distribution closer to the original data
with the full-precision pre-trained model. Generally, exsit-
ing DFQ methods can be divided into Generator-Based Ap-
proach (GBA) [5, 22, 37, 42] and Distill-Based Approach
(DBA) [3, 28, 30, 47].
The GBA trains a specific generative network ( e.g., GAN
[12] or V AE [24]) to generate the synthetic data, which re-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
5589
Existing Methods
Synthetic data
ùìõùêäùêãRetrainOurs
Synthetic data
Pre-trained Quantized
Self-Entropy Filtering
high -reliable data low-reliable data
major pseudo
 -
label
 auxiliary pseudo
 -
label
ùìõùêäùêã+ùìõPRetrain
Pre-trained Quantized
Figure 2. The difference between existing data-free methods and
our method. The existing methods optimize the quantizer or re-
train the pre-trained network with quantization constraint directly
without evaluating the synthetic data. We propose to evaluate the
synthetic data with self-entropy and divide the synthetic data into
high- and low-reliable datasets before training the quantized net-
work for better performance.
quires a complex training process for the generative net-
work. The DBA regards the synthetic data as trainable,
and iterates them to fit the original data distribution us-
ing the back-propagation of the pre-trained model, avoid-
ing the training process for the generative network. How-
ever, although many exquisite technologies are designed in
these existing methods, a noticeable difference still exists
between the synthetic and original training data, which af-
fects the performance of the quantized network. Thus, it
is necessary to evaluate the synthetic training data before
quantization.
In this paper, we propose to evaluate the synthetic data
before quantization as shown in Figure 2. To complete this
goal, we aim to deal with three problems: 1) How to eval-
uate the synthetic data? In existing frameworks, the syn-
thetic training data is generated with the information in the
pre-trained model, and it lacks clear evaluation indicators
for the synthetic data. 2) How to label the evaluated syn-
thetic data? After evaluating the synthetic data, it is nec-
essary to assign suitable labels to the synthetic data accord-
ing to different evaluation results, which aims to further im-
prove the reliability of synthetic data and avoid mislead-
ing caused by inappropriate labels. 3) How to design the
training process with evaluated synthetic data? The train-
ing process needs to be able to learn supervision informa-
tion from the synthetic data with different labels, and more
importantly, avoid misleading caused by the data with low
evaluation results.
To overcome the problems above, we propose an effi-
cient Data-Free Quantization via Pseudo-label Filtering as
follows: 1)The self-entropy [23] is used as a metric for
synthetic data to evaluate the reliability of the pre-trained
network on it. We notice that the pre-trained network has a
relatively higher self-entropy on the synthetic dataset thanthe original dataset, as shown in Figure.1, owing to different
reliability performance, so the self-entropy can be used as
a metric to evaluate the reliability of synthetic data before
quantization. 2)The synthetic data is labeled using multiple
pseudo-labels, which include major and auxiliary pseudo-
labels, to reflect its reliability. Major pseudo-labels are as-
signed to high-reliable samples, providing valuable super-
vision information. In contrast, low-reliable samples are
labeled with auxiliary pseudo-labels to give a soften super-
vised learning for enhancing the robustness of the quantized
network and avoiding misled by low-reliable samples. 3)
The pseudo-label training is designed to integrate the super-
vision information provided by major and auxiliary pseudo-
labels. The knowledge distillation is selected as the basic
framework to train the quantized model, which has simi-
lar performance and intermediate features as the pre-trained
network using the proposed synthetic data evaluation.
The main contributions of this work can be summarized
as follows:
‚Ä¢ We propose an efficient data-free quantization via
pseudo-label filtering, which is the first to evaluate the
synthetic data before quantization, so that the samples
with different reliability can provide different information
and improve the performance.
‚Ä¢ The self-entropy is used as the metric for the synthetic
data evaluation, which represents the reliability of syn-
thetic data under a specific label. With the metric, the syn-
thetic data can be categorized into high- and low-reliable
samples for the following training process.
‚Ä¢ The multiple pseudo-labels are designed in our method
to label the synthetic data with different reliability. It
incorporates major and auxiliary pseudo-labels for the
high- and low-reliable samples, providing valuable super-
vision information and avoiding misleading quantization
by low-reliable samples.
‚Ä¢ Extensive experiments are conducted on the CIFAR-
10, CIFAR-100, and ImageNet in various models. Our
method can achieve superior performances compared
with existing DFQ methods.
2. Related Work
Quantization. Quantization is widely used in the model
compression for neural networks to represent the full-
precision model with low-bit approximations. To allevi-
ate the performance loss caused by approximate operations,
many methods have been proposed, which can be grouped
into post-training quantization (PTQ) [1, 13, 34, 35] and
quantization-aware training (QAT) [2, 4, 10, 11]. The PTQ
methods take calibration data to optimize the parameters in
the quantizer, and the QAT methods retrain the pre-trained
network with a quantization constraint. However, the origi-
nal training data is necessary for these methods, it may not
be feasible in specific scenarios. To deal with the challenge
5590
Figure 3. The framework of the proposed method. In the proposed method, synthetic data is evaluated with self-entropy of the pre-trained
network on the synthetic data, which is divided into high- and low-reliable data. Then, the multiple pseudo-labels are used to label the
evaluated synthetic data according to its reliability. The high-reliable samples are assigned with major pseudo-labels, and the low-reliable
samples use auxiliary pseudo-labels. With these multiple pseudo-labels, the pseudo-labels cross-entropy loss can be obtained and combined
with the MSE loss function to guide the quantized network has similar prediction ability with pre-trained network.
without original training data, the Data-Free Quantization
(DFQ) is proposed.
Data-free Quantization. Many works [14, 34, 43] at-
tempt to optimize the quantized network solely relying on
the information inherent in the pre-trained network itself
without the demand for training data. D-FQ [34] proposes
the method of weight equalization and bias correction to
make the network more suitable to quantize. SQuant [14]
adopts a new rounding metric based on a diagonal Hessian
approximation to improve the performance of the quantized
network. However, owing to the absence of training data
to adjust the pre-trained network, these methods can hardly
achieve a significant performance improvement.
Many methods propose to utilize generated data, and
can be divided into Generator-Based Approach (GBA) [5,
32, 37, 38, 42, 44, 48] and Distill-Based Approach (DBA)
[3, 30, 30, 47]. (1) GBA proposes to use a generator for syn-
thesizing training data. GDFQ [42] constructs informative
data from the pre-trained model and generates data approx-
imating the original dataset. Qimera [5] proposes to use su-
perposed latent embeddings to generate higher-quality sam-
ples. These methods always demand much time and re-
sources to generate high-quality data. (2) DBA utilizes the
pre-trained model to directly optimize the generated data,
thereby eliminating the need for the generator. ZeroQ [3]
matches the statistics of batch normalization to optimize for
a distilled dataset. IntraQ [47] attempts to synthesize im-
ages with intra-class heterogeneity. HAST [28] generates
more hard samples to enhance model training effectiveness.
However, even with many ingenious designs, there is
a difference between the generated synthetic data and the
original training data, but none of the existing methods takes
this into account, and the synthetic data is directly used
for the training of the quantized network. In the proposed
method, we propose to evaluate the generated synthetic data
and label different samples according to the evaluation re-
sults to enhance the performance of the quantized network.3. Proposed Method
The framework of our proposed method is illustrated in
Figure 3, including Reliability Filtering, Multiple Pseudo-
Labels, and Pseudo-Label Training.
3.1. Reliability Filtering
In principle, the mean and variance in BN layers of the
pre-trained model are affected by the original training data.
Thus, the synthetic data can be generated from random ini-
tial samples by adapting the output distribution of BN layers
the same as that stored in the pre-trained model with:
LBN=LX
l=1 
||¬µp
l‚àí¬µs
l||2
2+||œÉp
l‚àíœÉs
l||2
2
, (1)
where Ldenotes the layer number of the model. ¬µp
landœÉp
l
are the mean and variance stored in l-th BN layer of the pre-
trained model, and ¬µs
landœÉs
lare the mean and variance
of synthetic data batch in l-th layer. The synthetic data is
generated by minimize LDATA as,
LDATA =LBN+Œ≥¬∑ LIL, (2)
where LIL=PN
i=1CE (P(ÀÜxi),ÀÜyi)is the loss to improve
the predicted probability of pre-trained network for the as-
signed label [16]. Ndenotes the number of samples for
the synthetic data, and CE(¬∑)represents the cross-entropy
loss. ÀÜxidenotes the generated synthetic sample and P(ÀÜxi)
represents its predicted probability after the softmax layer.
ÀÜyidenotes the assigned label. Œ≥is a hyper-parameters to
balance two losses of LBNandLIL.
However, the synthetic data can hardly have precisely
the same features as the original data. As discussed above,
we notice that the synthetic dataset always has a higher self-
entropy compared with the original dataset as shown in Fig-
ure 1. The reason is that the pre-trained network is trained
on the original dataset, which can have high reliability on
most samples. Owing to the difference between the original
5591
and synthetic data, it is impossible for the pre-trained net-
work to have high reliability on the whole synthetic data,
and the low-reliable data may lead to incorrect guidance
during training and seriously influence the quantization per-
formance.
Thus, we apply self-entropy as a metric to evaluate the
reliability of the pre-trained network on the synthetic data
as in Eq. 3:
Hself(ÀÜxi) =‚àí1
logNcNcX
c=1(P(ÀÜxi, c)¬∑log (P(ÀÜxi, c))),(3)
where Ncrefers to the number of prediction classes, and
P(ÀÜxi, c)denotes the predicted probability of the class cob-
tained by the pre-trained network.
In principle, Hselfindicates the uncertainty, whereas a
low self-entropy for the predicted results can refer to a reli-
able prediction with high confidence [23]. In other words,
lowHselfmeans the pre-trained network can have a high
possibility for the specific label on the synthetic data, which
means it has high reliability for the samples, so it can
provide adequate supervision information for training the
quantized network to fit the performance of the pre-trained
network.
Based on the reliability evaluation, the samples in the
synthetic dataset ÀÜXcan be divided into high-reliable dataset
ÀÜXhand low-reliable dataset ÀÜXlwith a reliable threshold t
as,
ÀÜXh=n
ÀÜxhÀÜxh‚ààÀÜX,Hself ÀÜxh
‚©Ωto
,
ÀÜXl=n
ÀÜxlÀÜxl‚ààÀÜX,Hself ÀÜxl
> to
,(4)
where tis a dynamic threshold parameter for fast conver-
gence and learning more supervision information. To con-
verge fast, the requirement for high-reliable samples can be
relaxed at the beginning of training to provide more samples
and quickly improve network performance; As the training
progresses, the network performance gradually improves,
which raises the standard for high-reliable samples. It is
necessary to use high-reliable samples to provide better su-
pervision information and avoid the impact of low-reliable
samples on network performance. At the same time, using
more low-reliable samples can also improve the robustness
of the network. Thus, tcontinuously decrease as the train-
ing epoch increases:
t=Tu‚àíft(epoch )(Tu‚àíTl), (5)
where TlandTuare the lower and upper boundaries, and
t‚àà[Tl, Tu], which continuously decrease as the training
epoch increase. ft(epoch ) =epoch
E,epoch andEdenote
the current and whole epoch for training, respectively.
3.2. Multiple Pseudo-Labels
With the reliability filtering, the samples in ÀÜXhhave
high reliability and can provide more supervision informa-tion for the quantized model. In the proposed method, ma-
jor pseudo-labels are designed for these samples, and the
pseudo-label ÀÜym
ifor the high-reliable sample ÀÜxh
iis assigned
as,
ÀÜym
i= arg max
c‚ààCP(ÀÜxh
i, c), (6)
where C={1,2, ..., N c}denotes the set of all the possible
prediction classes. We define the loss of Lh
CEfor supervised
learning with high-reliable dataset:
Lh
CE=1
NhNhX
i=1CE 
Pq(ÀÜxh
i),ÀÜym
i
, (7)
where Nhrefers to the number of high-reliable samples,
andPq ÀÜxh
i
denotes the predicted probability for all classes
with the quantized network on high-reliable samples ÀÜxh
i.
Generally, the high-reliable samples account for a small
portion of the total samples [23]. In quantization, we aim to
train the quantized network, which can fit the performance
of the pre-trained network. The low-reliable synthetic data
can also reflect the features of the pre-trained network and
provide some information for robustness. But owing to the
low predicted probability for the labels with low reliability,
they may mislead the training if applied directly in training.
To deal with it, auxiliary pseudo-labels are designed to
give a soften supervised learning with the low-reliable sam-
ples, which consists of the primary label ÀÜyp
iand secondary
label ÀÜys
i. The primary label ÀÜyp
irepresents the label with
the highest predicted probability, and the secondary label
ÀÜys
irepresents that with the second highest probability. In
the prediction with high self-entropy, the labels excluding
ÀÜyp
ican also have a decent probability, especially the sec-
ondary label ÀÜys
i. Thus, the pseudo-labels from ÀÜyp
iandÀÜys
i
can be leveraged to enhance the training and mitigate the
risk of misleading for these samples [29]. The primary la-
belÀÜyp
iand secondary label ÀÜys
ican be obtained as,
ÀÜyp
i= arg max
c‚ààCP(ÀÜxl
i, c),
ÀÜys
i= arg max
c‚ààÀÜCP(ÀÜxl
i, c),(8)
where ÀÜCdenotes the set of classes removing the class with
the highest predicted probability.
The auxiliary cross-entropy Ll
CEloss are defined with
the primary and secondary labels ÀÜyp
i,ÀÜys
ias,
Ll
CE=1
NlNlX
i=1 
Œªp
i¬∑CE 
Pq(ÀÜxl
i),ÀÜyp
i
+Œªs
i¬∑CE 
Pq(ÀÜxl
i),ÀÜys
i
,(9)
where Nlrefers to the number of low-reliable samples, Œªp
i
andŒªs
irepresent the weights of primary and secondary la-
bels to balance their importance, and can be obtained as,
Œªp
i=P(ÀÜxl
i, p)
P(ÀÜxl
i, p) +P(ÀÜxl
i, s),
Œªs
i=P(ÀÜxl
i, s)
P(ÀÜxl
i, p) +P(ÀÜxl
i, s),(10)
5592
where P(ÀÜxl
i, p)andP(ÀÜxl
i, s)denote the predicted probabili-
ties of the primary and secondary labels with the pre-trained
model, respectively.
With the designed major and auxiliary pseudo-labels, the
high- and low-reliable samples can be simultaneously used
to provide the supervision information for model training:
Ltotal
CE=Lh
CE+Œ≤¬∑ Ll
CE, (11)
where Œ≤is the hyper-parameter to balance the importance
of samples with different reliability, which is less than 1.
3.3. Pseudo-Label Training
To train a high-performance quantized model, we design
a pseudo-label training based on the knowledge distillation
framework [18], whereby the pre-trained network is em-
ployed as the teacher for the quantized network. In order to
learn the supervision information from the designed major
and auxiliary pseudo-labels, the designed cross-entropy loss
Ltotal
CE is used in training and combined with LMSE to form
the prediction similarity loss LP, which aims to guide the
quantized network has similar prediction ability with pre-
trained network as,
LP=Ltotal
CE+¬µ¬∑ LMSE, (12)
where ¬µis the hyperparameter to balance the two losses.
LMSE denotes the Mean Squared Error (MSE) between the
intermediate feature layers of the two networks. Owing to
the reduction of bits number for weights and activations in
the quantized network, the features extracted by interme-
diate layers are significantly affected, resulting in perfor-
mance degradation. LMSE is utilized to minimize the dif-
ference between the intermediate feature layers of the two
networks:
LMSE=LX
k=1 
1
NNX
i=1(fk(ÀÜxi)‚àífq
k(ÀÜxi))2!
, (13)
where fk(ÀÜxi)andfq
k(ÀÜxi)represent the outputs in the k-th
layer of the full-precision model and the quantized model,
respectively.
In common, the Kullback-Leibler (KL) divergence LKL
[3] between the outputs of the pre-trained and quantized net-
works can be used to minimize the discrepancy between the
outputs of the two networks, thereby ensuring the perfor-
mance of the quantized network.
LKL=1
NNX
i=1
P(ÀÜxi)¬∑logP(ÀÜxi)
Pq(ÀÜxi)
, (14)
where P(ÀÜxi)andPq(ÀÜxi)denotes predicted probability us-
ing the pre-trained and quantized networks, respectively.
Thus, the total loss function for the entire training pro-
cess can be obtained as,
Ltotal=LKL+œÑLP, (15)where hyperparameter œÑis used to balance the weights of
the two losses.
4. Experiment
4.1. Experiment Setup
Datasets and Networks. Data-Free Quantization is
typically evaluated on CIFAR-10/100 [25] and ImageNet
(ILSVRC2012) [8] datasets. The proposed method is im-
plemented and examined with ResNet [17] and MobileNet
[19], i.e. ResNet-20 on CIFAR-10/100; ResNet-18/50, and
MobileNet-V1 on ImageNet (ILSVRC2012).
Implementation Details. For a fair comparison, we syn-
thesize 5,120 images as the synthetic data, which is the
same as the settings of IntarQ [47] and HAST [28], and
these synthetic samples are optimized with 1000 iterations.
The hyper-parameters Œ≥are set as 10 for CIFAR-10/100
and 0.1 for ImageNet. We adopt the SGD optimizer with
a weight decay of 10‚àí4and momentum of 0.9 for model
training. The initial learning rate is set as 0.001 for CIFAR-
10/100 and 10‚àí5for ImageNet. The hyper-parameters Tl,
Tu,Œ≤,¬µandœÑare respectively set to 0.2, 0.5, 0.3, 100 and
1 for CIFAR-10/100 and 0.1, 0.4, 0.5, 4000 and 1 for Im-
ageNet. All full-precision pre-trained models are provided
by pytorchcv [42]. All layers are quantized, including the
first and last layers of the model. Our implementation is
conducted with PyTorch on a GPU Nvidia GTX 3090 Ti
workstation, CUDA 11.4, and Ubuntu 18.04.
4.2. Comparisons with State-of-the-Art Methods
The proposed method is compared with state-of-the-art
data-free quantization methods on CIFAR-10/100 and Im-
ageNet as listed in Tables 1 ‚àº4. W-bit/A-bit represent the
bits number of weights and activations after quantization,
respectively. Among these methods, GDFQ [42], DSG [44],
ZAQ [32], Qimera [5], ARC [6], ARC+AIT [48], AdaSG
[38] and AdaDFQ [37] belong to GBA. ZeroQ [3], IntraQ
[47] and HAST [28] belong to DBA.
CIFAR-10/100. For ResNet-20 on CIFAR-10/100, the
model is quantized into 4-bit and 3-bit. As shown in Ta-
ble 1, the proposed method achieves the superior perfor-
mances at 92.47% (4-bit), 88.04% (3-bit) on CIFAR-10
and 66.94% (4-bit), 57.03% (3-bit) on CIFAR-100. So our
method can get the higher performance compared with most
of exsiting methods, i.e. +0.98% over IntraQ, +0.37% over
AdaSG, +0.16% over AdaDFQ, and +0.11% over HAST
on CIFAR-10 (4-bit). The performance improvement is
higher on CIFAR-100 than CIFAR-10, i.e. +1.96% (4-bit)
and +8.78% (3-bit) over IntraQ, +0.52% (4-bit) and +4.27%
(3-bit) over AdaSG, +0.13% (4-bit) and +4.29% (3-bit)
over AdaDFQ, and +0.26% (4-bit) and +1.36% (3-bit) over
HAST. Generally, the proposed method can get the best per-
formance except in 3-bit on CIFAR-10 dataset (0.30% lower
5593
Table 1. Top-1 accuracy (%) comparison with the state-of-the-
art methods on CIFAR-10, CIFAR-100 for 3/4-bit ResNet-20 . *
represents the results reimplemented in paper [37].
Dataset Method Venue W4A4 W3A3
CIFAR-10
For
ResNet-20
(FP:94.03)ZeroQ[3] CVPR‚Äô20 84.68 29.32
GDFQ[42] ECCV‚Äô20 90.25 71.10
DSG[44] CVPR‚Äô21 88.74 32.90
Qimera[5] NeurIPS‚Äô21 91.26 74.43*
ARC[6] IJCAI‚Äô21 88.55 -
ARC+AIT[48] CVPR‚Äô22 90.49 -
IntraQ[47] CVPR‚Äô22 91.49 77.07
AdaSG[38] AAAI‚Äô23 92.10 84.14
AdaDFQ[37] CVPR‚Äô23 92.31 84.89
HAST[28] ICCV‚Äô23 92.36 88.34
Ours - 92.47 88.04
CIFAR-100
For
ResNet-20
(FP:70.33)ZeroQ[3] CVPR‚Äô20 58.42 15.38
GDFQ[42] ECCV‚Äô20 63.58 43.87
DSG[44] CVPR‚Äô21 62.36 25.48
Qimera[5] NeurIPS‚Äô21 65.10 46.13*
ARC[6] IJCAI‚Äô21 62.76 40.15*
ARC+AIT[48] CVPR‚Äô22 61.05 41.34*
IntraQ[47] CVPR‚Äô22 64.98 48.25
AdaSG[38] AAAI‚Äô23 66.42 52.76
AdaDFQ[37] CVPR‚Äô23 66.81 52.74
HAST[28] ICCV‚Äô23 66.68 55.67
Ours - 66.94 57.03
Table 2. Top-1 accuracy (%) comparison with the state-of-the-art
methods on ImageNet for 4/5-bit ResNet-18 .
Dataset Method Venue W5A5 W4A4
ImageNet
For
ResNet-18
(FP:71.47 )ZeroQ[3] CVPR‚Äô20 69.65 60.68
GDFQ[42] ECCV‚Äô20 66.82 60.60
DSG[44] CVPR‚Äô21 69.53 60.12
ZAQ[32] CVPR‚Äô21 64.54 52.64
Qimera[5] NeurIPS‚Äô21 69.29 63.84
ARC[6] IJCAI‚Äô21 68.88 61.32
ARC+AIT[48] CVPR‚Äô22 70.28 65.73
IntraQ[47] CVPR‚Äô22 69.94 66.47
AdaSG[38] AAAI‚Äô23 70.29 66.50
AdaDFQ[37] CVPR‚Äô23 70.29 66.53
HAST[28] ICCV‚Äô23 - 66.91
Ours - 70.35 67.02
than HAST). By analysis, HSAT aims to improve the qual-
ity of synthetic data by increasing the proportion of hard
samples, resulting in more high quality synthetic data, but
our method emphasises on the synthetic data evaluation for
better utilization of synthetic data.
ImageNet. To further verify the effectiveness of our
method on the large-scale dataset, we compare the perfor-
mance on ImageNet using different networks with state-
of-the-art methods in Tables 2 ‚àº4.(1)The ResNet-18
is implemented with the proposed method, and the re-
sults are listed in Table 2. It can be observed that the
proposed method achieves the best performance among
these methods, which are 70.35% (5-bit) and 67.02% (4-Table 3. Top-1 accuracy (%) comparison with the state-of-the-
art methods on ImageNet for 4/5-bit MobileNet-V1 . ‚ÄôIL‚Äô denotes
using the inception loss [16].
Dataset Method Venue W5A5 W4A4
ImageNet
For
MobileNet-V1
(FP:73.39)ZeroQ[3]+IL[16] CVPR‚Äô20 67.11 25.43
GDFQ[42] ECCV‚Äô20 59.76 28.64
DSG[44]+IL[16] CVPR‚Äô21 66.61 42.19
SQuant[13] ICLR‚Äô22 64.20 10.32
IntraQ[47] CVPR‚Äô22 68.17 51.36
HAST[28] ICCV‚Äô23 68.52 57.70
Ours - 69.44 59.51
Table 4. Top-1 accuracy (%) comparison with the state-of-the-art
methods on ImageNet for 4/5-bit ResNet-50 .
Dataset Method Venue W5A5 W4A4
ImageNet
For
ResNet-50
(FP:77.73)GDFQ[42] ECCV‚Äô20 71.63 54.16
ZAQ[32] CVPR‚Äô21 73.38 53.02
Qimera[5] NeurIPS‚Äô21 75.32 66.25
ARC[6] IJCAI‚Äô21 74.13 64.37
ARC+AIT[48] CVPR‚Äô22 76.00 68.27
AdaSG[38] AAAI‚Äô23 76.03 68.58
AdaDFQ[37] CVPR‚Äô23 76.03 68.38
Ours - 76.08 68.97
bit). Especially in the case of 4-bit, compared with
GDFQ (60.60%), Qimera (63.84%), AdaDFQ (66.53%),
and HAST (66.91%), our method can get much better per-
formance. (2)TheMobileNet-V1 is selected for the evalua-
tion of the proposed method on the light-weighted network
as shown in Table 3. Generally, light-weighted networks
always have a heavy accuracy drop after quantization, but
the proposed method can also outperform existing methods,
which get 0.92% and 1.81% higher accuracy than the ad-
vanced method HAST for 5-bit and 4-bit, respectively. (3)
TheResNet-50 is used to evaluate the performance of the
proposed method on the network with complex and deeper
structure as Table 4. The proposed method achieves supe-
rior performances at 76.08% (5-bit) and 68.97% (4-bit), es-
pecially compared with the latest methods AdaSG (68.58%)
and AdaDFQ (68.38%) at 4-bit setup. Thus, the proposed
method can efficiently improve performance on various net-
work structures and datasets, proving the effectiveness of
our proposed evaluation for synthetic data.
4.3. Ablation Study
Effect of Different Components in LP.To evaluate the
effect of each component in the proposed method, we test
each item in the designed prediction similarity loss LP. Ta-
ble 5 lists different networks ( N1‚àº N 7) trained with differ-
ent loss combinations (the common LKLis used in all the
networks to provide the basic performance of quantized net-
work). Symbol ‚úìmeans the component is used for quanti-
zation training, and symbol √óindicates that the component
is removed for training. Lh
CEandLl
CEdenote the cross-
5594
ùìùùüí ùìùùüì ùìùùüî Ours
original synthetic high -reliable low-reliable
(e) (f) (g) (h)(a) (b) (c) (d)Figure 4. Feature visualization using t-SNE. Figure 4 (a ‚àºd) show the feature distributions of the network N4(w/oLh
CEandLl
CE),N5
(w/oLl
CE),N6(w/oLh
CE) andN7(Ours) on the original test dataset. Figure 4 (e ‚àºh) show the feature distributions of the pre-trained
network on the original dataset, the synthetic dataset, the evaluated high-reliable dataset ÀÜXhand low-reliable dataset ÀÜXl, respectively.
0.00.51.0
0.00.51.0Predicted probabilityhighest probability
0.00.51.0
0.00.51.0
(a) high -reliable data in CIFAR -10(b) low -reliable data in CIFAR -10secondary highest probability
(c) high -reliable data in CIFAR -100 (d) low -reliable data in CIFAR -100
Figure 5. The highest and secondary highest predicted probabilities of pre-trained network on the high- and low-reliable datasets. The
highest predicted probabilities on the high-reliable dataset are far higher than the secondary highest predicted probabilities, since the pre-
trained network has high performance on these data. While on the low-reliable dataset, the pre-trained network cannot perform well, which
can be reflected by the lower highest predicted probabilities and higher secondary highest predicted probabilities.
Table 5. Ablation studies of losses with 3/4-bit ResNet-20 on the
CIFAR-100.
-Lh
CELl
CELMSE W4/A4 W3/A3
N1√ó √ó √ó 65.81 50.87
N2‚úì √ó √ó 66.30 53.22
N3‚úì ‚úì √ó 66.52 54.14
N4√ó √ó ‚úì 66.32 54.59
N5‚úì √ó ‚úì 66.68 56.43
N6√ó ‚úì ‚úì 66.57 55.86
N7‚úì ‚úì ‚úì 66.94 57.03
entropy loss functions using high-reliable and low-reliable
samples, respectively. LMSE represents the usage of the
MSE loss function. There are two observations. (1) Mul-
tiple Pseudo-labels can provide supervision information for
model training, and improve the performance of the quan-
tized network. In the comparison for Lh
CEandLl
CE(i.e.
comparing N1withN3, and comparing N4withN7), it
is obvious that the designed Lh
CEandLl
CEcan improve
the quantized network performance, which can prove the
efficiency of proposed reliability filtering. By comparing
the network with different combinations of Lh
CEandLl
CETable 6. Top-1 accuracy (%) of the combination of GDFQ [42],
IntraQ [47] and our training method on CIFAR-100 for 4-bit
ResNet-20 and ImageNet for 4-bit ResNet-18 . The combination
with our multiple pseudo-labels is denoted as + Ltotal
CE.
Dataset Method Acc Acc Up
Cifar100
(FP:70.33)GDFQ[42] 63.58 -
GDFQ[42]+ Ltotal
CE 64.01 0.43 ‚Üë
IntraQ[47] 64.98 -
IntraQ[47]+ Ltotal
CE 65.49 0.51 ‚Üë
Ours 66.94 -
ImageNet
(FP:73.09)GDFQ[42] 60.60 -
GDFQ[42]+ Ltotal
CE 61.36 0.76 ‚Üë
IntraQ[47] 66.47 -
IntraQ[47]+ Ltotal
CE 66.79 0.32 ‚Üë
Ours 67.02 -
(i.e. comparing N2withN3, comparing N5withN7, and
comparing N6withN7), the network trained using multiple
pseudo-labels can have better performance than using only
one of them, which shows the designed multiple pseudo-
labels works and can provide more supervision information
5595
for improving the performance of the quantized network.
Especially, the performance of N5is higher than that of
N6, which can show better training efficiency using high-
reliable data than low-reliable data. (2) Mean Squared Er-
ror (MSE) can make significant performance gain for quan-
tized network. In the comparison with and w/o LMSE (i.e.
comparing N1withN4, comparing N2withN5, and com-
paring N3withN7), the performance can have a stable im-
provement with LMSE, which can prove the efficiency for
minimizing the difference between the intermediate feature
layers of the two networks.
To give visualizations for the improvement of network
performance with the designed loss function, the feature
distributions of the network N4(w/oLh
CEandLl
CE),N5
(w/oLl
CE),N6(w/oLh
CE) andN7(Ours) on the original test
dataset are shown in Figure 4 (a ‚àºd) with t-SNE [40], which
can display the distribution for classification by transferring
data from high dimension into the two-dimensional space.
It is evident that the designed Lh
CEandLl
CEcan improve
the classification performance of the trained quantized net-
work, which can be observed from the aggregation effect of
different classes (comparing N4withN5in Figure 4 (a) and
(b), and comparing N4withN6in Figure 4 (a) and (c)), so
the designed reliability filtering can improve the training of
quantized network. In addition, N7in Figure 4 (d) has the
best aggregation effect among these four networks, which
can prove the efficiency of the designed multiple pseudo-
labels in the proposed method.
Effect of Reliability Filtering. To show the efficiency
of proposed self-entropy metric for evaluating the reliabil-
ity of samples in the synthetic data, we present the feature
distributions of the pre-trained network on the original train-
ing dataset, the synthetic dataset, the evaluated high-reliable
dataset ÀÜXhand low-reliable dataset ÀÜXlas shown in Figure 4
(e‚àºh). The feature distributions of the pre-trained net-
work on the original training dataset are shown in Figure 4
(e), which means the pre-trained network can have a good
classification performance on the original training dataset.
However, the feature distributions in Figure 4 (f) show the
pre-trained network can hardly efficiently classify the syn-
thetic data. From the results in Figure 4 (g) and (h), it is
evident that the pre-trained network can have an excellent
classification performance on ÀÜXh. This means the applica-
tion of designed reliability filtering can efficiently filter the
high-reliable samples, which is suitable to provide supervi-
sion information and train quantized network.
Effect of Multiple Pseudo-Labels. The highest and sec-
ondary highest predicted probabilities of pre-trained net-
work on the high- and low-reliable datasets are plotted in
Figure 5. It can be observed that the highest predicted
probabilities on the high-reliable dataset are far higher than
the secondary highest predicted probabilities, since the pre-
trained network has high reliability on these data. While onthe low-reliable dataset, the pre-trained network cannot per-
form well, which can be reflected by the lower highest pre-
dicted probabilities and higher secondary highest predicted
probabilities. Therefore, the designed secondary label ÀÜys
i
can also provide information for fitting the performance of
the pre-trained network, which can enhance the training and
mitigate the risk of misleading for the high-reliable samples.
Effect of Pseudo-Label Training. Our designed multi-
ple pseudo-labels Ltotal
CE can also combined with other train-
ing framework. To evaluate the proposed training frame-
work, two classic DFQ methods (GDFQ and IntraQ) are
selected as baselines, and we combine the designed Ltotal
CE
with these two baselines. We examine the performance
on CIFAR-100 for 4-bit ResNet-20 and ImageNet for 4-
bit ResNet-18 as listed in Table 6. From the experimental
results, the combination with our designed Ltotal
CE can effi-
ciently improve the performance of the quantized network
for the existing methods, which can prove the effectiveness
of our designed multiple pseudo-labels. In addition, it is
also observed that the proposed training framework can still
have the best accuracy among these methods, which can
prove that the proposed pseudo-label training can guide the
quantized network to have a similar prediction ability with
the pre-trained network and learn supervision information
from the synthetic data with different labels.
5. Conclusion
This work proposes an efficient data-free quantization
method via pseudo-label filtering, which is the first to eval-
uate the synthetic data before training the quantized net-
work. In the proposed method, self-entropy is selected as
an evaluation metric to divide the synthetic data into high-
reliable and low-reliable data. The multiple pseudo-labels
are designed to label the evaluated samples, which can fur-
ther improve the reliability and avoid misleading caused by
low-reliable data. The pseudo-label training is designed
to integrate the supervision information provided by multi-
ple pseudo-labels. Extensive experiments are implemented
and evaluated on CIFAR-10/100 and ImageNet datasets,
demonstrating that the proposed framework performs bet-
ter than existing methods.
Acknowledgement. This work is supported in part
by the National Key R&D Program of China (No.
2022ZD0118201), the National Natural Science Foun-
dation of China (No.61802105, 62272144, 72188101,
62020106007, and U20A20183), the University Synergy
Innovation Program of Anhui Province (No. GXXT-2021-
005 and GXXT-2022‚Äì033), the Fundamental Research
Funds for the Central Universities (No. JZ2022HGTB0250
and PA2023IISL0096), and the Major Project of Anhui
Province (202203a05020011).
5596
References
[1] Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry.
Aciq: Analytical clipping for integer quantization of neural
networks. International Conference on Learning Represen-
tations , 2019. 1, 2
[2] Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen
Blankevoort, and Nojun Kwak. Lsq+: Improving low-bit
quantization through learnable offsets and better initializa-
tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition Workshops , pages 696‚Äì
697, 2020. 1, 2
[3] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami,
Michael W Mahoney, and Kurt Keutzer. Zeroq: A novel
zero shot quantization framework. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 13169‚Äì13178, 2020. 1, 3, 5, 6
[4] Ting-An Chen, De-Nian Yang, and Ming-Syan Chen.
Alignq: Alignment quantization with admm-based correla-
tion preservation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
12538‚Äì12547, 2022. 1, 2
[5] Kanghyun Choi, Deokki Hong, Noseong Park, Youngsok
Kim, and Jinho Lee. Qimera: Data-free quantization with
synthetic boundary supporting samples. Advances in Neural
Information Processing Systems , 34:14835‚Äì14847, 2021. 1,
3, 5, 6
[6] Kanghyun Choi, Hye Yoon Lee, Deokki Hong, Joonsang Yu,
Noseong Park, Youngsok Kim, and Jinho Lee. It‚Äôs all in the
teacher: Zero-shot quantization brought closer to the teacher.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 8311‚Äì8321, 2022. 5,
6
[7] Tejalal Choudhary, Vipul Mishra, Anurag Goswami, and Ja-
gannathan Sarangapani. A comprehensive survey on model
compression and acceleration. Artificial Intelligence Review ,
53:5113‚Äì5155, 2020. 1
[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE Conference on Computer Vision and
Pattern Recognition , pages 248‚Äì255. Ieee, 2009. 5
[9] Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie.
Model compression and hardware acceleration for neural
networks: A comprehensive survey. Proceedings of the
IEEE , 108(4):485‚Äì532, 2020. 1
[10] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani,
Rathinakumar Appuswamy, and Dharmendra S Modha.
Learned step size quantization. In International Conference
on Learning Representations , 2020. 1, 2
[11] Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li,
Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differ-
entiable soft quantization: Bridging full-precision and low-
bit neural networks. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 4852‚Äì4861,
2019. 1, 2
[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, andYoshua Bengio. Generative adversarial networks. Commu-
nications of the ACM , 63(11):139‚Äì144, 2020. 1
[13] Cong Guo, Yuxian Qiu, Jingwen Leng, Xiaotian Gao, Chen
Zhang, Yunxin Liu, Fan Yang, Yuhao Zhu, and Minyi Guo.
Squant: On-the-fly data-free quantization via diagonal hes-
sian approximation. In International Conference on Learn-
ing Representations , 2021. 2, 6
[14] Cong Guo, Yuxian Qiu, Jingwen Leng, Xiaotian Gao, Chen
Zhang, Yunxin Liu, Fan Yang, Yuhao Zhu, and Minyi Guo.
Squant: On-the-fly data-free quantization via diagonal hes-
sian approximation. arXiv preprint arXiv:2202.07471 , 2022.
3
[15] Tiantian Han, Dong Li, Ji Liu, Lu Tian, and Yi Shan. Im-
proving low-precision network quantization via bin regular-
ization. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 5261‚Äì5270, 2021. 1
[16] Matan Haroush, Itay Hubara, Elad Hoffer, and Daniel
Soudry. The knowledge within: Methods for data-free model
compression. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 8494‚Äì
8502, 2020. 3, 6
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 770‚Äì778, 2016. 5
[18] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015. 1, 5
[19] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efficient convolu-
tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861 , 2017. 5
[20] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. In International Conference on Machine Learn-
ing, pages 448‚Äì456. pmlr, 2015. 1
[21] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry
Kalenichenko. Quantization and training of neural networks
for efficient integer-arithmetic-only inference. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 2704‚Äì2713, 2018. 1
[22] Yongkweon Jeon, Chungman Lee, and Ho-young Kim. Ge-
nie: Show me the data for quantization. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 12064‚Äì12073, 2023. 1
[23] Youngeun Kim, Donghyeon Cho, Kyeongtak Han,
Priyadarshini Panda, and Sungeun Hong. Domain
adaptation without source data. IEEE Transactions on
Artificial Intelligence , 2(6):508‚Äì518, 2021. 2, 4
[24] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114 , 2013. 1
[25] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009. 5
[26] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep
learning. nature , 521(7553):436‚Äì444, 2015. 1
5597
[27] Jung Hyun Lee, Jihun Yun, Sung Ju Hwang, and Eunho
Yang. Cluster-promoting quantization with bit-drop for min-
imizing network quantization loss. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 5370‚Äì5379, 2021. 1
[28] Huantong Li, Xiangmiao Wu, Fanbing Lv, Daihai Liao,
Thomas H Li, Yonggang Zhang, Bo Han, and Mingkui Tan.
Hard sample matters a lot in zero-shot quantization. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 24417‚Äì24426, 2023. 1, 3, 5,
6
[29] Xinhao Li, Jingjing Li, Lei Zhu, Guoqing Wang, and Zi
Huang. Imbalanced source-free domain adaptation. In Pro-
ceedings of the 29th ACM International Conference on Mul-
timedia , pages 3330‚Äì3339, 2021. 4
[30] Yuhang Li, Feng Zhu, Ruihao Gong, Mingzhu Shen, Xin
Dong, Fengwei Yu, Shaoqing Lu, and Shi Gu. Mixmix: All
you need for data-free compression are feature and data mix-
ing. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision , pages 4410‚Äì4419, 2021. 1, 3
[31] Zheng Li, Xiang Li, Lingfeng Yang, Borui Zhao, Renjie
Song, Lei Luo, Jun Li, and Jian Yang. Curriculum tem-
perature for knowledge distillation. In Proceedings of the
AAAI Conference on Artificial Intelligence , pages 1504‚Äì
1512, 2023. 1
[32] Yuang Liu, Wei Zhang, and Jun Wang. Zero-shot adversarial
quantization. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 1512‚Äì
1521, 2021. 3, 5, 6
[33] Vasco Lopes, Fabio Maria Carlucci, Pedro M Esperanc ¬∏a,
Marco Singh, Antoine Yang, Victor Gabillon, Hang Xu,
Zewei Chen, and Jun Wang. Manas: Multi-agent neural ar-
chitecture search. Machine Learning , pages 1‚Äì24, 2023. 1
[34] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and
Max Welling. Data-free quantization through weight equal-
ization and bias correction. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
1325‚Äì1334, 2019. 2, 3
[35] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Chris-
tos Louizos, and Tijmen Blankevoort. Up or down? adap-
tive rounding for post-training quantization. In International
Conference on Machine Learning , pages 7197‚Äì7206, 2020.
1, 2
[36] Kalin Ovtcharov, Olatunji Ruwase, Joo-Young Kim, Jeremy
Fowers, Karin Strauss, and Eric S Chung. Accelerating deep
convolutional neural networks using specialized hardware.
Microsoft Research Whitepaper , 2(11):1‚Äì4, 2015. 1
[37] Biao Qian, Yang Wang, Richang Hong, and Meng Wang.
Adaptive data-free quantization. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 7960‚Äì7968, 2023. 1, 3, 5, 6
[38] Biao Qian, Yang Wang, Richang Hong, and Meng Wang. Re-
thinking data-free quantization as a zero-sum game. In Pro-
ceedings of the AAAI Conference on Artificial Intelligence ,
2023. 3, 5, 6
[39] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang,
Zhihui Li, Xiaojiang Chen, and Xin Wang. A comprehensivesurvey of neural architecture search: Challenges and solu-
tions. ACM Computing Surveys (CSUR) , 54(4):1‚Äì34, 2021.
1
[40] Laurens Van der Maaten and Geoffrey Hinton. Visualizing
data using t-sne. Journal of machine learning research , 9
(11), 2008. 8
[41] Wenxiao Wang, Minghao Chen, Shuai Zhao, Long Chen,
Jinming Hu, Haifeng Liu, Deng Cai, Xiaofei He, and Wei
Liu. Accelerate cnns from three dimensions: A comprehen-
sive pruning framework. In International Conference on Ma-
chine Learning , pages 10717‚Äì10726. PMLR, 2021. 1
[42] Shoukai Xu, Haokun Li, Bohan Zhuang, Jing Liu, Jiezhang
Cao, Chuangrun Liang, and Mingkui Tan. Generative low-
bitwidth data free quantization. In Computer Vision‚ÄìECCV
2020: 16th European Conference, Glasgow, UK, August 23‚Äì
28, 2020, Proceedings, Part XII 16 , pages 1‚Äì17. Springer,
2020. 1, 3, 5, 6, 7
[43] Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, and
Kevin Bailly. Spiq: Data-free per-channel static input quan-
tization. In Proceedings of the IEEE/CVF Winter Confer-
ence on Applications of Computer Vision , pages 3869‚Äì3878,
2023. 3
[44] Xiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao Gong,
Qinghua Yan, Renshuai Tao, Yuhang Li, Fengwei Yu, and
Xianglong Liu. Diversifying sample generation for accu-
rate data-free quantization. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 15658‚Äì15667, 2021. 3, 5, 6
[45] Yuyao Zhang and Nikolaos M Freris. Adaptive filter prun-
ing via sensitivity feedback. IEEE Transactions on Neural
Networks and Learning Systems , 2023. 1
[46] Yunshan Zhong, Mingbao Lin, Mengzhao Chen, Ke Li, Yun-
hang Shen, Fei Chao, Yongjian Wu, and Rongrong Ji. Fine-
grained data distribution alignment for post-training quanti-
zation. In European Conference on Computer Vision , pages
70‚Äì86. Springer, 2022. 1
[47] Yunshan Zhong, Mingbao Lin, Gongrui Nan, Jianzhuang
Liu, Baochang Zhang, Yonghong Tian, and Rongrong Ji. In-
traq: Learning synthetic images with intra-class heterogene-
ity for zero-shot network quantization. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 12339‚Äì12348, 2022. 1, 3, 5, 6, 7
[48] Baozhou Zhu, Peter Hofstee, Johan Peltenburg, Jinho Lee,
and Zaid Alars. Autorecon: Neural architecture search-
based reconstruction for data-free compression. In 30th In-
ternational Joint Conference on Artificial Intelligence, IJCAI
2021 , pages 3470‚Äì3476. International Joint Conferences on
Artificial Intelligence, 2021. 1, 3, 5, 6
5598
