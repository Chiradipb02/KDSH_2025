Localization Is All You Evaluate:
Data Leakage in Online Mapping Datasets and How to Fix It
Adam Lilja1,2Junsheng Fu2Erik Stenborg2Lars Hammarstrand1
1Chalmers University of Technology2Zenseact
{firstname.lastname }@{chalmers.se, zenseact.com }
Abstract
The task of online mapping is to predict a local map us-
ing current sensor observations, e.g. from lidar and cam-
era, without relying on a pre-built map. State-of-the-art
methods are based on supervised learning and are trained
predominantly using two datasets: nuScenes and Argoverse
2. However, these datasets revisit the same geographic lo-
cations across training, validation, and test sets. Speciﬁ-
cally, over 80% of nuScenes and 40% of Argoverse 2 val-
idation and test samples are less than 5m from a train-
ing sample. At test time, the methods are thus evaluated
more on how well they localize within a memorized im-
plicit map built from the training data than on extrapo-
lating to unseen locations. Naturally, this data leakage
causes inﬂated performance numbers and we propose ge-
ographically disjoint data splits to reveal the true per-
formance in unseen environments. Experimental results
show that methods perform considerably worse, some drop-
ping more than 45mAP , when trained and evaluated on
proper data splits. Additionally, a reassessment of prior
design choices reveals diverging conclusions from those
based on the original split. Notably, the impact of lift-
ing methods and the support from auxiliary tasks (e.g.,
depth supervision) on performance appears less substantial
or follows a different trajectory than previously perceived.
https://github.com/LiljaAdam/geographical-splits
1. Introduction
A core capability for an autonomous vehicle is to estimate
the road in its vicinity. There are two complementary ap-
proaches for this task: retrieving the information from a
pre-built map using localization [ 5], and directly predict-
ing the online map using onboard sensors like camera and
lidar [ 19]. The former, Online Map Retrieval (OMR), as-
sumes there exists a map over the deployment area, while
the latter, Online Map Estimation (OME) assumes no such
map exists. A pre-built map provides detailed information
but also requires robust localization and continuous map up-
dates to be useful. OME sidesteps this and instead relies
TrainValidationTest
Figure 1. Example of substantial geographical overlap between
train, val, and test sets for in nuScenes’. Green circle, Orange
cross, and Red plus are training, validation, and test samples.
solely on onboard sensors and algorithms. It is thus inde-
pendent of variations in current surroundings compared to
mapped data. The challenge with OME instead lies in gen-
eralizing to new locations, beyond the places captured in the
training data.
The current state-of-the-art methods for online mapping
are based on supervised learning. While there exist large
public datasets [ 1,2,6,12,33,36,38] that support training
of perception and planning models for many of the crucial
tasks of an autonomous vehicle, only a few of these pro-
vide the HD maps needed to train online mapping models,
see Tab. 1. Moreover, as these datasets are mainly con-
structed to support object detection, object tracking, and
motion forecasting tasks, we argue that they, in their origi-
nal form, are not ideal for training online mapping models
for two reasons: (1) The training, validation, and test sets
are constructed by splitting the data temporally. This is an
easy way to ensure that, e.g., the same vehicle is not present
in the same position in the training and test sets yielding fair
evaluation of object detection methods. However, since the
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
22150
Figure 2. Example from nuScenes where input images, predictions, and ground truth for a test sample are displayed together with the
ground truth of the closest train sample. Lane dividers, road boundaries, and pedestrian crossings are visualized in orange, green, and blue.
areas where these datasets are collected are relatively small,
the same areas are revisited multiple times. Failing to ac-
count for this when dividing the data, results in signiﬁcant
geographical inter-set overlap between the training, valida-
tion and test sets as Fig. 1exempliﬁes. Fig. 2further vi-
sualizes the input images, prediction, ground truth, and the
closest training sample of a test sample for another example.
The test sample is very close to the nearest training sam-
ple, enabling a method to score well on that test sample by
memorizing the image-to-map connection from the training
sample, and recalling it at test time. This connection can be
created using any features in the images such as buildings
and trafﬁc signs. This would rather resemble localizing in
a pre-built map and presenting it at test time (OMR), than
the intended task of online mapping (OME). (2) As each
data sample is collected from a data sequence under nor-
mal driving, there is substantial intra-set overlap/correlation
between the data samples within the sets. This correlation
is especially evident near intersections where the vehicle is
standing still or driving slowly. Essentially, the ground truth
HD maps for close training samples are only slight transfor-
mations of the same information. Both these aspects violate
the independent and identically distributed data assumption
fundamental in training machine learning models.
In this paper, we show that there is a signiﬁcant ge-
ographical overlap in two of the most commonly used
datasets for online mapping, nuScenes [ 2] and Argoverse 2
[36]. Furthermore, we show that this overlap causes severe
inﬂation in the reported performance of the state-of-the-art
methods and, more critically, results in poorer generaliz-
ability than initially perceived.
To support future research in OME we provide geo-
graphically disjoint splits for both nuScenes and Argov-
erse 2. We provide splits under two slightly different prob-
lem settings, Near Extrapolation where we assume we have
training data from the same neighbourhood/city and Far Ex-
trapolation where training and test data are from separate
cities. The former is an easier setting and can be viewed as
a proper substitution for the original splits while the latterenables the exploration of the more relevant question; how
well do these methods generalize to new environments with
larger distribution shifts?
We re-evaluate state-of-the-art methods trained on these
splits to give a more representative view of their perfor-
mance. Additionally, we perform more detailed experi-
ments to investigate how the large overlap has affected con-
clusions regarding important algorithmic choices.
2. Related Work
This section introduces the online mapping setting (OME),
highlights key methodologies and datasets used for training
and evaluation. While the OME ﬁeld is relatively recent,
the utilization of machine learning in processing geospatial
data (GeoML) has a longer history. Hence, we also pro-
vide a brief overview of GeoML, noting parallels between
its challenges and those encountered in OME.
2.1. Online Mapping
The current online mapping methods are either
segmentation-based [ 7,9,14,16,17,21,23,24,27,37,40]
or vector-based [ 16,18–20,30]. The main difference lies in
how the online map is represented. In segmentation-based
maps, the aim is to predict a rasterized grid where each
cell is classiﬁed as, e.g., empty, lane marking or road edge.
For vector-based methods, the predicted map is described
by a set of objects with a given class and the geometry is
described by a vector of point coordinates, i.e., a polyline.
Both categories universally adopt a core technique
known as lifting . This entails converting image features
from perspective view (PV) to Bird’s Eye View (BEV) fea-
tures, from which the map is predicted. The primary chal-
lenge for these methods lies in accurately mapping features
from the perspective view to their corresponding locations
in BEV due to the absence of depth information in images.
Various lifting methods have been proposed, broadly cate-
gorized as either pulling the features to BEV from PV , or
pushing the features from PV to BEV .
In essence, pulling methods retrieve features from PV
22151
based on dense queries in BEV [ 7,17,23,41]. A straight-
forward approach is the Inverse Perspective Mapping (IPM)
[22] and involves projecting predeﬁned points in BEV
into PV using camera parameters and interpolating features
from these projected positions. Alternatively, methods like
Geometry-Guided Kernel Transformer (GKT) [ 7] and BEV-
Former [ 17] use a combination of geometry and attention
mechanisms to pull features to BEV-space efﬁciently. In
contrast, Cross-View Transformer (CVT) [ 41] pulls features
without an intermediate BEV representation using cross-
attention with a canonical form of all camera views.
Thepushing methods used in, e.g.[9,16,21,24,27,37],
specialize in learning how to map PV features to BEV .
Among them, depth-based approaches aim to learn the
depth distribution for each image pixel to project PV fea-
tures accurately. For instance, LSS [ 24] tries to learn a cate-
gorical depth distribution for each pixel and use it to weigh
how much the corresponding PV feature should inﬂuence
the corresponding BEV-cell. Pyramid Occupancy Network
(PON) [ 27] uses a multi-scale dense transformer for low-
resolution BEV projection, employing deconvolutions for
upsampling predictions, whereas HDMapNet [ 16] learns
BEV projection through a Multilayer Perceptron (MLP).
These lifting methods have been successfully adapted to
a segmentation head for online mapping. Also the vector-
based approach, introduced in HDMapNet [ 16] and further
developed in works such as [ 18–20,30], have shown great
promise by utilizing network heads inspired by the object-
detection community. While HDMapNet uses a handcrafted
post-processing step, subsequent methods are instead end-
to-end trainable. For example, VectorMapNet [ 20] uses
IPM [ 22] for lifting image features to BEV from which a
transformer decoder predicts coarse object representations.
These are then reﬁned in a joint Autoregressive Transformer
(ART) that attends the coarse prediction and all BEV fea-
tures. MapTR [ 18] utilizes GKT [ 7] for lifting and a DETR-
like [ 4] transformer decoder for predicting the objects. They
use deformable attention to attend BEV features with hier-
archical queries to predict a collection of objects deﬁned by
a set of points. MapTRv2 [ 19] builds on its predecessor,
but uses LSS[ 24] for lifting and adds PV depth estimation
and segmentation in both PV and BEV as auxiliary supervi-
sion. Lastly, StreamMapNet [ 39] uses BEVFormer-lifting,
multi-point attention, and temporal information fusion.
All these methods, except [ 27,39], are primarily evalu-
ated on the original nuScenes and Argoverse 2 splits with
considerable inter-set overlap. The validity of conclusions
drawn regarding their performance on online mapping tasks
is thus severely limited. To give a fairer view of their per-
formance on the intended problem setting, we re-evaluate
them on our proposed splits and analyze the results.Dataset Split SourceMain Map
Purpose#Samples Geo.
SplitTrain Val Test
nuScenes [ 2] Original nuSc OD/MF 28k6k6k 7
Argoverse 1 [ 6] Original argo1 OD/MF 39k15k13k3
Argoverse 2 [ 36] Original argo2 OD/MF 110k24k24k7
Waymo [ 33] Original way OD/MF 122k30k40k7
nuScenes [ 2] Near nuSc OM 28k6k6k 3
Argoverse 2 [ 36]Near argo2 OM 110k24k24k3
nuScenes [ 2] Far-A nuSc OM 30k9k- 3
nuScenes [ 2] Far-B nuSc OM 31k9k- 3
Argoverse 2 [ 36]Far-A argo2 OM 110k46k- 3
Argoverse 2 [ 36]Far-B argo2 OM 101k55k- 3
Argoverse 2 [ 36]Far-C argo2 OM 101k55k- 3
Table 1. Datasets used for online mapping. The proposed splits are
shown in bold. OD = object detection, MF = motion forecasting,
OM = online mapping.
SplitnuScenes Argoverse 2
Val Test Val Test
Orig. 79.4%85.5%45.0%41.9%
Near 0.9%1.1%0.0%0.0%
Table 2. Ratios of validation and test samples within 5m of train-
ing samples. The Near Extrapolation split has negligible overlap
compared to the Original (Orig.) split for both datasets.
2.2. Online Mapping Datasets
A summary of datasets used for online mapping is provided
in Tab. 1. Three original datasets provide the HD-maps
required to enable the training of online mapping models,
Argoverse 1 and 2 [ 6,36], nuScenes [ 2] and Waymo [ 33].
All these primary datasets are mainly intended for object
detection and motion forecasting tasks, and, in addition to
supplying HD maps, these datasets provide rich annotations
for dynamic objects. The predeﬁned data splits provide fair
and consistent evaluations across studies, but were origi-
nally designed for dynamic object perception rather than
online mapping. They are temporally divided to prevent
sample overlap across sets within a sequence, but do not en-
sure geographic separation. Despite this, nuScenes [ 2] and
Argoverse 2 [ 36] are widely used for training online map-
ping models and have become the de facto standard. For
example, online mapping methods using nuScenes include
[3,7,9,14–17,21,23–25,27,29,37,40,41] and Argoverse
2 is used in [ 18–20,30].
The nuScenes dataset contains 1 000 driving sequences
collected in two cities (Boston and Singapore) with an area
coverage of about 5km2[33] and captures different types of
city roads as well as containing diverse weather and illumi-
nation conditions. In total, the sequences consist of 40,000
key-frame samples at a rate of 2Hz, accompanied by ob-
ject annotations. Additional sensor data is present between
these key-frame samples, albeit without any object annota-
tion. Online mapping methods typically adhere to the con-
vention established by object detection methods, utilizing
only key-frame samples for training. Furthermore, these
22152
10020030040050000.050.1
10020030040050000.050.1Number of samplesRatio of cellsRatio of cellsnuScenesArgoverse 2Figure 3. Number of samples within a cell with length 60m. Ar-
goverse 2 has higher intra-set sample density.
samples are closely together across the different sets as
the same geographical position is re-visited multiple times.
Fig.1illustrates a single example, where the proximity of
validation and test samples to training samples is evident.
Further analysis, as Tab. 2depicts, reveals that approxi-
mately 80%and85%of validation and test samples, respec-
tively, are within 5meters of a sample used during training.
The other prominently used dataset, Argoverse 2, is an
extension of the 2019 version Argoverse [ 6]. In contrast to
its predecessor, Argoverse 2 is not geographically split, but
is much larger and collected from 6 U.S. cities with an area
coverage of 17km2[1]. It comprises 1 000 annotated driv-
ing sequences, which are on average 15s long and anno-
tated at 10Hz. Each sample offers observations from sim-
ilar sensors as nuScenes, albeit in a slightly different con-
ﬁguration, and the provided HD maps are in 3D, but with a
focus on drivable area, road boundaries, lane dividers, and
pedestrian crossings. By inspecting Argoverse 2, one can
see that it also suffers from a considerable level of inter-set
geographical overlap. Tab. 2shows that approximately 45%
and42% of the validation and test samples are within a 5m
range of the closest train sample.
Comparing nuScenes and Argoverse 2, we note that the
inter-set sample overlap is larger for nuScenes, and that Ar-
goverse 2 boasts a larger number of samples and a more ex-
tensive coverage area. Another difference arises from Argo-
verse 2 being more densely sampled, resulting in less spatial
variation and a higher intra-set sample density. The discrep-
ancy is highlighted in Fig. 3, where samples are discretized
into cells with a side length of 60m, the typical evalua-
tion range for most online mapping methods, e.g.[16,18–
20,30]. The distribution of non-empty cells over the num-
ber of samples they contain has more probability mass with
the higher counts for Argoverse 2. Despite Argoverse 2 hav-
ing nearly four times the number of samples than nuScenes,
the number of non-empty cells is only 1.7times higher.2.3. Machine Learning on Geospatial Data
Utilizing machine learning for geospatial applications has
been a longstanding challenge. The choice of geographic
regions for training and evaluation signiﬁcantly impacts the
evaluation outcomes and, as highlighted in [ 28], it is cru-
cial to select regions that mirror the desired goals for an
accurate assessment. Neglecting geographic considerations
in evaluating these models can thus yield inadequate and
inaccurate assessments of their performance. This issue,
characterized by geographic overlap between training and
evaluation data, has been observed in various domains, in-
cluding satellite image modeling [ 11], ecological mapping
[10], medical imaging [ 34], and 3D object detection [ 31].
In [26], the authors discuss how deviating from the as-
sumption of independent training and evaluation data can
lead to favouring complex models. They propose a solution
known as block cross-validation, where data is strategically
split geographically to mitigate these concerns. Similarly,
[32] advocates for spatially partitioning the data instead
of random allocation to bolster the independence between
training and validation datasets used for cross-validation.
In the online mapping community, [ 27] acknowledged
the need for a geographically divided train and test split
when evaluating their method (PON) on nuScenes. They
coarsely partition the sets along large, visually similar,
neighbourhoods, limiting the diversity within each set. Fur-
thermore, their proposed geographical split does not take
the validation set into account and discards samples from
the original test set reducing the total size of the dataset.
Another approach is to divide training and evaluation ac-
cording to the different cities as explored for nuScenes in
[25]. Although being a valid split set, the distribution shift is
large, and signiﬁcantly increases the difﬁculty of the prob-
lem. Since only one data split is proposed, cross-validation
cannot be performed. For Argoverse 2, [ 39] proposes a split
that divides the training and validation samples according to
geographical positions, but does not allocate a geograph-
ically separate testing set. A separate test set represent-
ing unseen data is crucial for unbiased evaluation; hyper-
parameters should not be optimized speciﬁcally for this set.
Hence, the need for robust data partitioning persists.
3. Geographically Disjoint Splits
In the online mapping setting, the difﬁculty level varies de-
pending on assumptions about the geographic distribution
shift between training and target data, ranging from near to
far extrapolation. Near extrapolation assesses performance
within the same cities as the training data, while far extrap-
olation evaluates generalization to cities beyond those in the
training set. We believe that addressing the latter scenario,
which poses a greater difﬁculty, should be the primary long-
term target of the OME ﬁeld.
22153
3.1. Near Extrapolation
We created balanced geographically disjoint sets for train-
ing, validation, and testing in nuScenes and Argoverse 2 by
partitioning data based on sample locations, reducing inter-
set overlap. We employ the original splits’ testing data as
the map is provided for those samples. Split proportions are
70%,15%, and 15% for training, validation, and test sets.
To preserve diversity in zone classes (in the urban planning
sense, e.g., residential, commercial, and industrial) while
maintaining the frequency of road object classes, weather
conditions, and time of day, ﬁne-grained and thorough par-
titioning is performed. Our splits ensure proportional repre-
sentation of samples from various criteria in each set, mir-
roring the full dataset’s distribution and ensuring represen-
tation from all cities. Regions were deﬁned manually based
on map attributes, and splits are visualized in supplemen-
tary materialand available on the project webpage.
While partitioning the data, we did not account for the
intra-set overlap (Sec. 2.2). We do, however, acknowledge
its potential importance and believe it warrants considera-
tion in the utilization of these datasets. The speciﬁcs of how
to address this concern and the associated implications are
left for future research. We realize that the use of original
test data has implications for multi-task networks (e.g. also
performing object detection) and have ensured that balance
remains when removing the original test data. The number
of samples in each set can be seen in Tab. 1.
nuScenes Tab. 2displays the ratio of validation and test
samples located closer than 5m of a training sample for the
Near Extrapolation split. Our suggested splits show only
minimal overlap. The remaining overlap is due to the sam-
ples close to cut-off regions between sets. To see the ef-
fects of these samples we conduct, in supplementary mate-
rial, experiments where these samples have been ﬁltered out
and note that their impact is negligible. Further, we show
that the weather conditions and time of day are equally dis-
tributed through the sets in our Near Extrapolation splits.
Argoverse 2 As Tab. 2presents, no validation or test sam-
ples lie within 5m of a training sample for the proposed
split. Attributes concerning the conditions associated with
the different sequences are not available for Argoverse 2
making it hard to do a quantitative analysis. Our main fo-
cus is thus to give a balanced geographically separated split,
partitioning areas with similar zone classes equally in the
different sets. The distribution of the number of samples in
each city as well as highlights the diversity of geographical
distribution can be found in supplementary material.
3.2. Far Extrapolation
Far Extrapolation through city-wise data splitting intro-
duces a greater distribution shift between training and eval-nuScenes Argoverse 2
Set A B A B C
TrainBoston, Boston, Miami, Miami, Pittsburgh,
Onenorth Queenstown, Pittsburgh Rest Rest
Holland Village
ValQueenstown, Onenorth Rest Pittsburgh Miami
Holland Village
Table 3. Far Extrapolation splits for nuScenes and Argoverse 2
where the folds are approximately similarly sized.
uation. A subset of cities from each dataset is designated
for training, while the remainder is allocated for evalua-
tion. Notably, there is an uneven city-wise sample distri-
bution in both datasets, with, for instance, Boston contain-
ing55% of samples in nuScenes, and Miami and Pittsburgh
each constituting 35% of the Argoverse 2 data. To miti-
gate this imbalance, cities are grouped to achieve approx-
imately equal-sized folds, with the training set comprising
70% of the data. Refer to Tab. 3for the proposed city-wise
folds in both nuScenes and Argoverse 2. Given the varied
attributes of each city, the method’s performance is sensi-
tive to the composition of training and validation sets. As
such, these city-wise folds ought to be utilized for cross-
validation, where the average performance across different
folds serves as the performance measure.
4. Experiments
To demonstrate the geographical data leakage problem with
the original splits of nuScenes and Argoverse 2, we evaluate
the performance of state-of-the-art online mapping methods
on both the original and proposed geographically disjoint
data splits. Additionally, we re-validate studies performed
in previous works.
Unless speciﬁed, no modiﬁcations have been made to
the conﬁgurations of the evaluated methods, and we di-
rect readers to the respective papers for speciﬁc training de-
tails. Additionally, the performance is measured using stan-
dard practice in the respective ﬁeld, i.e., Intersection over
Union (IoU) for segmentation-based methods and mean av-
erage precision (mAP) [ 16] for vector-based methods. For
the latter, the average precision AP ⌧is calculated through
thresholding the Chamfer distance between matched pre-
diction/ground truth-pairs for the thresholds ⌧2T,T =
{0.5,1.0,1.5}to get
mAP =1
|T|X
⌧2TAP⌧. (1)
We report mAP for individual object classes and their mean.
4.1. Data Leakage Effects
To investigate the effects of data leakage across data parti-
tions we train several vector- and segmentation-based meth-
ods on both the Original and the geographically disjoint
22154
nuScenesModel Sensor Backbone Lifting Decoder SplitDivider Boundary Crossing MeanVal Test Val Test Val Test Val TestVectorMapNet [20] Camera Resnet50 IPM ARTOrig48.9 47.9 40.9 63.8 39.8 52.8 43.2 54.8Near13.5 17.3 14.9 21.6 13.7 15.7 14.0 18.2MapTR [18] Camera Resnet18 GKT DETROrig38.0 52.2 37.2 46.0 24.2 28.9 33.1 42.4Near10.6 12.1 13.7 20.15.11.09.8 11.1MapTR [18] Camera Resnet50 GKT DETROrig51.0 65.8 52.6 60.5 43.0 53.3 48.8 59.9Near16.0 19.9 26.7 33.3 14.45.9 19.0 19.7MapTRv2 [19] Camera Resnet50 LSS DETROrig61.8 77.3 63.7 70.9 59.1 69.8 61.5 72.7Near20.9 23.4 32.6 40.5 26.5 14.8 26.7 26.2MapTRv2 [19]CameraResnet50SECONDLSS DETROrig54.9 70.5 55.1 63.9 51.9 63.1 54.0 65.8Lidar Near15.1 18.0 27.4 35.2 17.47.0 20.0 20.1StreamMapNet [39] Camera Resnet50 BEVFormer DETROrig64.5 79.3 62.3 69.5 61.0 74.5 62.6 74.4Near23.0 22.6 29.5 35.2 25.8 26.0 26.1 27.9Argoverse 2VectorMapNet [20]2DCamera Resnet50 IPM ARTOrig51.9 46.8 42.1 40.7 38.0 38.7 44.0 42.0Near39.8 35.0 31.5 32.4 26.8 31.3 32.7 32.9MapTR [18]2DCamera Resnet50 GKT DETROrig64.0 62.8 63.2 61.0 63.7 62.4 63.6 62.1Near50.0 45.2 47.5 48.3 46.6 50.9 48.0 48.2MapTRv2 [19]2DCamera Resnet50 LSS DETROrig71.7 68.9 67.0 63.8 64.5 63.1 67.7 65.3Near58.4 56.6 51.3 53.5 49.7 55.6 53.1 55.2MapTRv2 [19]3DCamera Resnet50 LSS DETROrig68.7 66.0 64.3 61.7 59.6 58.7 64.2 62.1Near56.2 55.0 47.8 51.0 46.2 51.8 50.1 52.6StreamMapNet [39]2DCamera Resnet50 BEVFormer DETROrig58.3 56.6 63.9 62.9 62.7 63.1 61.7 60.8Near52.7 47.9 50.0 54.8 49.4 55.2 50.7 52.6Table 4. mAP comparison for methods trained on Original (Orig) and Near Extrapolation (Near) splits. All methods show a signiﬁcant
performance drop when trained and evaluated on Near. Autoregressive Transformer [ART], Object Detection with Transformer [DETR].
Near Extrapolation splits. The results for vector-based
methods are reported in Tab. 4. All evaluated methods see
a signiﬁcant performance drop when using geographically
disjoint splits compared to the Original splits. The aver-
age performance decrement is more than 35mAP and 12
mAP for nuScenes and Argoverse 2, respectively. More-
over, the effect is consistent over all lifting methods, sensor
modalities, and decoders, but the ranking among the evalu-
ated methods remains. The performance drop also remains
consistent when adding lidar or considering the 3D geome-
try of the online map.
The best-performing method (MapTRv2) on nuScenes
using images as input drops from an mAP of 72.7to just
26.2, showcasing a difference of 46.5mAP, when trained
and evaluated appropriately. The drop is less pronounced,
although still signiﬁcant, on Argoverse 2, decreasing from
65.3to55.2mAP. In general, the impact of the split is
particularly distinct for methods trained on nuScenes. In
light of these ﬁndings, we conclude that the smaller size
of nuScenes, although convenient, makes it inadequate for
training current online mapping methods. Moreover, al-
though Argoverse 2 has more samples, it is somewhat sur-
prising that algorithms trained on it still exhibit substan-
tially improved generalization ability considering that the
intra-set overlap is also larger. We hypothesize that, de-
spite the intra-set overlap being greater, this overlap does
not hinder training; instead, it possibly functions as natu-
ral and beneﬁcial data augmentation similarly as synthetic
augmentations have shown to be highly useful for image
classiﬁcation and object detection tasks [ 8].To illustrate the signiﬁcance of these numbers, a qual-
itative example is provided in Fig. 4, comparing MapTR
[18] predictions on a validation sample when trained on
the Original and the Near Extrapolation nuScenes splits.
Studying the ﬁgure carefully, we can see that the model
trained on the original split accurately predicts the road edge
(green line) even for areas completely occluded in the im-
ages, highlighted by the teal box. The method also pre-
dicts the lane divider (yellow line) on the opposing road
through the trucks and barriers, as highlighted in the pink
box. Given their single-shot nature and lack of consider-
ation for previous sensor data, it is unreasonable that they
can accurately predict road structures that are not visible or
clearly indicated by other structures in the current view. The
method appears to learn to localize validation and test sam-
ples within the provided training map. This is not unique to
this particular method or example, but rather a consequence
of the overlap between train and evaluation sets. More qual-
itative examples are provided in supplementary material.
For segmentation-based methods, Tab. 5shows reduced
performance when evaluated on geographically disjoint
data. This suggests the impact extends beyond vectorized
methods. Here, HDMapNet is kept the same as from the
original paper and the other lifting techniques, namely In-
verse Perspective Mapping (IPM), Cross-View Transformer
(CVT) and Geometry-Guided Kernel Transformer (GKT)
have been altered to predict the classes we are interested in.
Architectural details such as the image feature extraction
backbone, Efﬁcientnet-b4 [ 35], and segmentation decoder
from SimpleBEV [ 13] are kept the same for all lifters for
22155
Original SplitNear Extrapolation SplitGround Truth
Figure 4. Predictions from MapTR [ 18] trained on Original and Geographical splits along with the ground truth. Yellow lines denote (lane)
dividers, green (road) boundaries, and blue pedestrian crossings. Note that, when trained on the Original split, the branch to the parallel
road on the left (teal box) is not visible in any image, yet appears in the predicted map. Also, the divider on the opposing road to the right
(pink box) is predicted very well. When training on geographically split data (here Near Extrapolation), this method fails to predict these.
Model SplitDivider Boundary Crossing Mean
Val Test Val Test Val Test Val TestnuScenesGKTOrig. 25.82 5 .42 5 .62 2 .76.25 .11 9 .21 7 .7
Near. 12.51 7 .91 2 .61 6 .91.41 .98 .81 2 .3
CVTOrig. 30.93 0 .13 0 .52 5 .51 1 .77.62 4 .42 1 .1
Near. 16.91 1 .61 7 .01 0 .34.51 .11 2 .87.7
IPMOrig. 46.85 2 .45 0 .05 2 .82 6 .82 7 .44 1 .24 4 .2
Near. 29.62 9 .43 6 .23 3 .61 6 .29.72 7 .42 4 .2
HDMapNetOrig 38.34 7 .53 5 .54 1 .22 0 .12 7 .63 1 .33 8 .8
Near 8.62 4 .02 2 .12 5 .41 0 .61 4 .11 7 .12 1 .2Argoverse 2GKTOrig. 37.32 8 .23 1 .42 8 .31 0 .35.72 6 .32 0 .7
Near. 32.72 9 .02 6 .22 3 .28.71 .52 2 .51 7 .9
CVTOrig. 40.12 9 .43 2 .02 9 .31 2 .17.12 8 .42 1 .9
Near. 35.13 1 .72 6 .52 8 .51 0 .91.42 4 .22 0 .5
IPMOrig. 58.54 5 .75 0 .65 0 .13 3 .43 2 .44 7 .54 2 .7
Near. 50.53 9 .14 4 .14 5 .22 9 .72 8 .84 1 .53 7 .7
Table 5. Map segmentation on the datasets, evaluating perfor-
mance with IoU. The performance drops for all methods using
the Near. versus Orig.. As for the vector-based OME, the drop
is larger on nuScenes than Argoverse 2.
fair comparison. As for the vector-based online mapping
methods, the drop is larger on nuScenes than Argoverse 2
and the ranking among methods remains largely unchanged.
Overall, the Near Extrapolation split yields a more con-
sistent performance between the validation and test sets
compared to the Original split. Suggesting a balanced dis-
tribution across sets and facilitates drawing reliable conclu-
sions about hyperparameter choices. Ensuring that insights
gained from the validation set generalize well to the test set.
4.2. Far Extrapolation Cross-validation
We perform cross-validation using multiple folds of city-
wise data partitioning to evaluate the performance under in-
creased distribution shifts. Tab. 6shows the vector-based
methods’ performance for both nuScenes and Argoverse 2
using the Far Extrapolation splits. The performance drops
even further using this split, emphasizing the difﬁculty
current methods experience with extrapolating outside the
training distribution.Model Split Divider Boundary Crossing Mean CVnuScenesVectorMapNetA 7.68 .45 .97 .38.7B 11.9 12 .26 .1 10 .1
MapTRA 12.9 21 .1 11 .1 15 .014.9B 14.1 24 .65 .9 14 .9
MapTRv2A 18.6 27 .9 18 .9 21 .821.4B 22.4 27 .0 13 .3 20 .9
StreamMapNetA 16.4 22 .7 18 .7 19 .321.3B 21.7 30 .6 17 .4 23 .2Argoverse 2VectorMapNetA 16.5 19 .0 16 .5 21 .1
24.0 B 29.4 26 .0 20 .5 25 .3
C 28.0 27 .1 22 .1 25 .7
MapTRA 41.7 37 .3 34 .7 37 .9
41.8 B 47.5 45 .9 40 .2 44 .5
C 41.2 44 .6 43 .3 43 .0
MapTRv2A 42.2 41 .9 37 .4 40 .5
45.5 B 53.4 50 .9 42 .0 48 .8
C 50.3 48 .6 42 .6 47 .2
StreamMapNetA 43.4 43 .3 40 .2 42 .3
46.6 B 51.0 52 .5 46 .7 50 .1
C 42.7 50 .1 49 .4 47 .4
Table 6. Vector-based methods’ mAP on the Far Extrapolation
folds and their corresponding cross-validation mean (CV).
4.3. Sample Density
We investigate the effect of the training set’s sample density
for both datasets. As discussed in Sec. 2.2only the key-
frame samples are typically used when training methods on
nuScenes. We leverage the fact that allsamples have the
vehicle poses required to extract the ground truth from the
HD map and are able to use 4times as many samples for
training. It is also forthright to downsample Argoverse 2 to
the desired sample density by selecting every fourth sample,
effectively simulating the sample density of nuScenes.
The training schedule is adjusted such that the total num-
ber of optimizer steps is similar between the sparsely and
densely sampled data. Tab. 7reports the result for Map-
TRv2. For nuScenes there is a distinguished increase in per-
formance on the original split, achieving up to a 24.9 mAP
improvement on the validation set by utilizing the densely
sampled data. However, the method’s performance using
the Near Extrapolation split sees only marginal improve-
ment (max increase of 0.7 mAP). Argoverse 2 sees smaller
differences between the sample densities.
22156
SplitTrainSamplingDivider Boundary Crossing MeanVal Test Val Test Val Test Val TestnuScenesOrig.Sparse61.8 77.3 63.7 70.9 59.1 69.8 61.5 72.7Dense90.3 89.7 84.4 84.5 86.4 88.3 86.4 87.5NearSparse20.9 23.4 32.6 40.5 26.5 14.8 26.7 26.2Dense21.7 24.1 34.1 40.9 25.7 15.9 27.2 26.9Argo 2Orig.Sparse67.7 64.9 63.6 59.8 58.8 57.1 63.4 60.6Dense71.7 68.9 67.0 63.8 64.5 63.1 67.7 65.3NearSparse55.4 54.0 48.2 52.5 44.8 51.5 49.5 52.6Dense58.4 56.6 51.3 53.5 49.7 55.6 53.1 55.2Table 7. MapTRv2 mAP trained on varying dataset density. For
nuScenes, dense sampling boosts performance by up to 24.9on
the Orig., while Near only sees minor improvements.
4.4. Re-validation
As original works validate design choices and hyperparam-
eters on poorly separated data, it is highly relevant to re-
visit these results to check applicability to proposed split.
Though not covering all prior tests, we highlight some
interesting observations on the Near Extrapolation split.
Hyperparameter-search shows minor differences, see sup-
plementary material, while new conclusions emerge regard-
ing lifting method and auxiliary tasks.
Lifting methods In [18], an ablation study investigates
the impact of various lifters for MapTR, with GKT yield-
ing the best results. However, upon re-running this test (see
Tab.8), we observe a contradiction to the previous ﬁndings.
The BEVFormer lifter slightly outperforms GKT. Nonethe-
less, the differences between the lifters are marginal, mak-
ing it challenging to determine the superiority of any spe-
ciﬁc lifter.
Auxiliary tasks For MapTRv2 [ 19], we re-run the abla-
tion studies on the proposed auxiliary tasks in Tab. 9. For
nuScenes, we note that, in contrast to conclusions based on
the original split, the addition of depth supervision does not
yield a signiﬁcant performance boost. Additionally, one
can infer that it is only when all auxiliary tasks are com-
bined that the improvement becomes apparent. However,
the effects of the additional tasks are smaller than initially
concluded when training on the original split. Consider-
ing Argoverse 2, there are bigger differences between the
performance among the auxiliary tasks. Similarly to the re-
validation on nuScenes, the effectiveness of, e.g., depth su-
pervision is not as striking as previously advertised.
5. Conclusion
We propose and employ geographically disjoint splits of the
most used datasets, revealing that the performance of state-
of-the-art online mapping methods is signiﬁcantly lower
than previously reported. We argue that these splits offer a
more accurate measure of how well online mapping meth-
ods generalize to new geographic areas. While the Near Ex-
trapolation split acts as a drop-in replacement to the originalLifting Split Divider Boundary Crossing MeannuScenesGKTOrig 51.0 52 .6 43 .0 48 .8
Near 16.0 26 .7 14 .4 19 .0
BEVFormerOrig 49.7 53 .5 40 .2 47 .8
Near 16.2 28 .2 17 .5 20 .6
LSSOrig 52.1 52 .4 45 .4 50 .0
Near 17.3 27 .7 18 .0 21 .0Argoverse 2GKTOrig 64.0 63 .2 63 .7 63 .6
Near 50.0 47 .5 46 .6 48 .0
BEVFormerOrig 63.7 63 .8 63 .0 63 .5
Near 49.5 47 .2 46 .3 47 .7
LSSOrig 64.8 65 .2 61 .9 64 .0
Near 49.7 47 .3 45 .1 47 .3
Table 8. Validation mAP for lifting methods in MapTR. Marginal
differences between the lifters make it challenging to establish the
superiority of any particular method.
Depth SegPVSegBEV nuScenes Argoverse 2
Orig. Geo. Geo.
56.6 25 .3 48 .8
3 59.8 25 .9 48 .8
33 60.5 26 .1 50 .6
33 61.0 25 .9 50 .9
33 59.2 25 .5 51 .5
33 3 61.5 26 .7 53 .1
Table 9. Validation mAP for auxiliary tasks in MapTRv2.
nuScenes Orig. numbers are from [ 19]. Nearyields a smaller per-
formance boost of auxiliary tasks. For Argoverse 2, the differences
are greater, but inconsistent with the result on Orig..
splits, we urge the community to target the Far Extrapola-
tion setting moving forward.
Even though performance numbers have decreased for
all methods with these splits, the ranking between meth-
ods remains largely the same. The performance disparity
is more pronounced on nuScenes than Argoverse 2. How-
ever, our follow-up re-validation experiments have revealed
new insights, diverging from conclusions based on the orig-
inal split. Notably, the impact of the lifting method and
the support from auxiliary tasks, e.g. depth supervision, on
performance appears less substantial or follows a different
trajectory than initially perceived.
In summary, online mapping remains a formidable chal-
lenge, and to make substantial progress, we must anchor our
conclusions in fair evaluations based on clean data splits.
We look forward to what innovations will come from the
improved evaluation ability with the release of our geo-
graphically disjoint data splits.
Acknowledgements: This work was partially supported
by the Wallenberg AI, Autonomous Systems and Soft-
ware Program (WASP) funded by the Knut and Alice
Wallenberg Foundation. Computational resources were
provided by the National Academic Infrastructure for
Supercomputing in Sweden (NAISS) at NSC Berzelius
and C3SE Alvis partially funded by the Swedish Re-
search Council through grant agreement no. 2022-06725.
22157
References
[1]Mina Alibeigi, William Ljungbergh, Adam Tonderski, Georg
Hess, Adam Lilja, Carl Lindstrom, Daria Motorniuk, Jun-
sheng Fu, Jenny Widahl, and Christoffer Petersson. Zenseact
open dataset: A large-scale and diverse multimodal dataset
for autonomous driving. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , pages
20178–20188, 2023. 1,4
[2]Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-
ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-
modal dataset for autonomous driving. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
11621–11631, 2020. 1,2,3
[3]Yigit Baran Can, Alexander Liniger, Danda Pani Paudel, and
Luc Van Gool. Structured bird’s-eye-view trafﬁc scene un-
derstanding from onboard images. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 15661–15670, 2021. 3
[4]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In European confer-
ence on computer vision , pages 213–229. Springer, 2020. 3
[5]Athanasios Chalvatzaras, Ioannis Pratikakis, and Angelos A
Amanatiadis. A survey on map-based localization techniques
for autonomous vehicles. IEEE Transactions on Intelligent
Vehicles , 8(2):1574–1596, 2022. 1
[6]Ming-Fang Chang, John W Lambert, Patsorn Sangkloy, Jag-
jeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter
Carr, Simon Lucey, Deva Ramanan, and James Hays. Argov-
erse: 3d tracking and forecasting with rich maps. In Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
2019. 1,3,4
[7]Shaoyu Chen, Tianheng Cheng, Xinggang Wang, Wenming
Meng, Qian Zhang, and Wenyu Liu. Efﬁcient and robust
2d-to-bev representation learning via geometry-guided ker-
nel transformer, 2022. 2,3
[8]Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V .
Le. Randaugment: Practical automated data augmenta-
tion with a reduced search space. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) Workshops , 2020. 6
[9]Hao Dong, Xianjing Zhang, Jintao Xu, Rui Ai, Weihao Gu,
Huimin Lu, Juho Kannala, and Xieyuanli Chen. Superfu-
sion: Multilevel lidar-camera fusion for long-range hd map
generation. In Proceedings of the IEEE International Con-
ference on Robotics and Automation (ICRA) , 2023. 2,3
[10] P. Ploton et al. Spatial validation reveals poor predictive per-
formance of large-scale ecological mapping models. In Na-
ture Communications, vol. 11, no. 1, p. 4540 , 2020. 4
[11] Hao Feng, Yongcheng Wang, Zheng Li, Ning Zhang, Yuxi
Zhang, and Yunxiao Gao. Information leakage in deep
learning-based hyperspectral image classiﬁcation: A survey.
Remote Sensing , 15(15), 2023. 4
[12] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel
Urtasun. Vision meets robotics: The kitti dataset. Interna-
tional Journal of Robotics Research (IJRR) , 2013. 1[13] Adam W Harley, Zhaoyuan Fang, Jie Li, Rares Ambrus,
and Katerina Fragkiadaki. Simple-bev: What really mat-
ters for multi-sensor bev perception? In 2023 IEEE Inter-
national Conference on Robotics and Automation (ICRA) ,
pages 2759–2765. IEEE, 2023. 6
[14] Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi
Yan, and Dacheng Tao. St-p3: End-to-end vision-based au-
tonomous driving via spatial-temporal feature learning. In
European Conference on Computer Vision , pages 533–549.
Springer, 2022. 2,3
[15] Yanqin Jiang, Li Zhang, Zhenwei Miao, Xiatian Zhu, Jin
Gao, Weiming Hu, and Yu-Gang Jiang. Polarformer: Multi-
camera 3d object detection with polar transformer. In Pro-
ceedings of the AAAI conference on Artiﬁcial Intelligence ,
pages 1042–1050, 2023.
[16] Qi Li, Yue Wang, Yilun Wang, and Hang Zhao. Hdmapnet:
An online hd map construction and evaluation framework. In
2022 International Conference on Robotics and Automation
(ICRA) , pages 4628–4634. IEEE, 2022. 2,3,4,5
[17] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-
hao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer:
Learning bird’s-eye-view representation from multi-camera
images via spatiotemporal transformers. In European con-
ference on computer vision , pages 1–18. Springer, 2022. 2,
3
[18] Bencheng Liao, Shaoyu Chen, Xinggang Wang, Tianheng
Cheng, Qian Zhang, Wenyu Liu, and Chang Huang. MapTR:
Structured modeling and learning for online vectorized HD
map construction. In The Eleventh International Conference
on Learning Representations , 2023. 2,3,4,6,7,8
[19] Bencheng Liao, Shaoyu Chen, Yunchi Zhang, Bo Jiang, Qian
Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang.
Maptrv2: An end-to-end framework for online vectorized hd
map construction, 2023. 1,3,6,8
[20] Yicheng Liu, Tianyuan Yuan, Yue Wang, Yilun Wang, and
Hang Zhao. Vectormapnet: End-to-end vectorized hd map
learning. In International Conference on Machine Learning ,
pages 22352–22369. PMLR, 2023. 2,3,4,6
[21] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang,
Huizi Mao, Daniela L Rus, and Song Han. Bevfusion: Multi-
task multi-sensor fusion with uniﬁed bird’s-eye view repre-
sentation. In 2023 IEEE international conference on robotics
and automation (ICRA) , pages 2774–2781. IEEE, 2023. 2,3
[22] Hanspeter A Mallot, Heinrich H B ¨ulthoff, JJ Little, and Ste-
fan Bohrer. Inverse perspective mapping simpliﬁes optical
ﬂow computation and obstacle detection. Biological cyber-
netics , 64(3):177–185, 1991. 3
[23] Lang Peng, Zhirong Chen, Zhangjie Fu, Pengpeng Liang,
and Erkang Cheng. Bevsegformer: Bird’s eye view semantic
segmentation from arbitrary camera rigs. In Proceedings of
the IEEE/CVF Winter Conference on Applications of Com-
puter Vision (WACV) , pages 5935–5943, 2023. 2,3
[24] Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encod-
ing images from arbitrary camera rigs by implicitly unpro-
jecting to 3d. In Computer Vision–ECCV 2020: 16th Euro-
pean Conference, Glasgow, UK, August 23–28, 2020, Pro-
ceedings, Part XIV 16 , pages 194–210. Springer, 2020. 2,3,
1
22158
[25] Zequn Qin, Jingyu Chen, Chao Chen, Xiaozhi Chen, and
Xi Li. Unifusion: Uniﬁed multi-view fusion transformer
for spatial-temporal representation in bird’s-eye-view. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) , pages 8690–8699, 2023. 3,4
[26] David R. Roberts, V olker Bahn, Simone Ciuti, Mark S.
Boyce, Jane Elith, Gurutzeta Guillera-Arroita, Severin
Hauenstein, Jos ´e J. Lahoz-Monfort, Boris Schr ¨oder, Wil-
fried Thuiller, David I. Warton, Brendan A. Wintle, Florian
Hartig, and Carsten F. Dormann. Cross-validation strategies
for data with temporal, spatial, hierarchical, or phylogenetic
structure. Ecography , 40(8):913–929, 2017. 4
[27] Thomas Roddick and Roberto Cipolla. Predicting seman-
tic map representations from images using pyramid occu-
pancy networks. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages
11138–11147, 2020. 2,3,4
[28] E. Rolf. Evaluation challenges for geospatial ml. In arXiv
preprint arXiv:2303.18087v1 , 2023. 4
[29] Avishkar Saha, Oscar Mendez, Chris Russell, and Richard
Bowden. Translating images into maps. In 2022 Interna-
tional conference on robotics and automation (ICRA) , pages
9200–9206. IEEE, 2022. 3
[30] Juyeb Shin, Francois Rameau, Hyeonjun Jeong, and Dong-
suk Kum. Instagram: Instance-level graph modeling for vec-
torized hd map learning. arXiv preprint arXiv:2301.04470 ,
2023. 2,3,4
[31] Andrea Simonelli, Samuel Rota Bulo, Lorenzo Porzi, Peter
Kontschieder, and Elisa Ricci. Are we missing conﬁdence in
pseudo-lidar methods for monocular 3d object detection? In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3225–3233, 2021. 4
[32] Kai Sun, Yingjie Hu, Gaurish Lakhanpal, and Ryan Zhenqi
Zhou. Spatial cross-validation for GeoAI . Handbook of
Geospatial Artiﬁcial Intelligence, CRC Press 201-214, 2021.
4
[33] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien
Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,
Yuning Chai, Benjamin Caine, et al. Scalability in perception
for autonomous driving: Waymo open dataset. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 2446–2454, 2020. 1,3
[34] Iulian Emil Tampu, Anders Eklund, and Neda Haj-Hosseini.
Inﬂation of test accuracy due to data leakage in deep
learning-based classiﬁcation of oct images. Scientiﬁc Data ,
9(1), 2022. 4
[35] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model
scaling for convolutional neural networks. In International
conference on machine learning , pages 6105–6114. PMLR,
2019. 6
[36] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lam-
bert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Rat-
nesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes,
Deva Ramanan, Peter Carr, and James Hays. Argoverse 2:
Next generation datasets for self-driving perception and fore-
casting. In Proceedings of the Neural Information Process-
ing Systems Track on Datasets and Benchmarks (NeurIPS
Datasets and Benchmarks) , 2021. 1,2,3[37] Enze Xie, Zhiding Yu, Daquan Zhou, Jonah Philion, Anima
Anandkumar, Sanja Fidler, Ping Luo, and Jose M Alvarez.
Mˆ 2bev: Multi-camera joint 3d detection and segmentation
with uniﬁed birds-eye view representation. arXiv preprint
arXiv:2204.05088 , 2022. 2,3
[38] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying
Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Dar-
rell. Bdd100k: A diverse driving dataset for heterogeneous
multitask learning. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
2636–2645, 2020. 1
[39] Tianyuan Yuan, Yicheng Liu, Yue Wang, Yilun Wang, and
Hang Zhao. Streammapnet: Streaming mapping network for
vectorized online hd map construction. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer
Vision , pages 7356–7365, 2024. 3,4,6
[40] Yunpeng Zhang, Zheng Zhu, Wenzhao Zheng, Junjie Huang,
Guan Huang, Jie Zhou, and Jiwen Lu. Beverse: Uniﬁed per-
ception and prediction in birds-eye-view for vision-centric
autonomous driving. arXiv preprint arXiv:2205.09743 ,
2022. 2,3
[41] Brady Zhou and Philipp Kr ¨ahenb ¨uhl. Cross-view transform-
ers for real-time map-view semantic segmentation. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 13760–13769, 2022. 3
22159
