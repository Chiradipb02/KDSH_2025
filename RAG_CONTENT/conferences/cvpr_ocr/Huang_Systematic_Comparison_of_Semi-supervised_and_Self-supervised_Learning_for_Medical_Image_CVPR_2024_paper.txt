Systematic comparison of semi-supervised and self-supervised learning
for medical image classification
Zhe Huang*Ruijie Jiang*Shuchin Aeron Michael C. Hughes
Tufts University, School of Engineering
{Zhe.Huang, Ruijie.Jiang, Shuchin.Aeron, Michael.Hughes}@tufts.edu
Abstract
In typical medical image classification problems, labeled
data is scarce while unlabeled data is more available. Semi-
supervised learning and self-supervised learning are two
different research directions that can improve accuracy by
learning from extra unlabeled data. Recent methods from
both directions have reported significant gains on tradi-
tional benchmarks. Yet past benchmarks do not focus on
medical tasks and rarely compare self- and semi- methods
together on an equal footing. Furthermore, past bench-
marks often handle hyperparameter tuning suboptimally.
First, they may not tune hyperparameters at all, leading
to underfitting. Second, when tuning does occur, it often
unrealistically uses a labeled validation set that is much
larger than the training set. Therefore currently published
rankings might not always corroborate with their practi-
cal utility This study contributes a systematic evaluation of
self- and semi- methods with a unified experimental pro-
tocol intended to guide a practitioner with scarce overall
labeled data and a limited compute budget. We answer
two key questions: Can hyperparameter tuning be effective
with realistic-sized validation sets? If so, when all meth-
ods are tuned well, which self- or semi-supervised meth-
ods achieve the best accuracy ? Our study compares 13
representative semi- and self-supervised methods to strong
labeled-set-only baselines on 4 medical datasets. From
20000+ GPU hours of computation, we provide valuable
best practices to resource-constrained practitioners: hy-
perparameter tuning is effective, and the semi-supervised
method known as MixMatch delivers the most reliable gains
across 4 datasets.
1. INTRODUCTION
Deep neural networks can deliver exceptional performance
on classification tasks when trained with vast labeled
*Authors Zhe Huang and Ruijie Jiang contributed equally to this work.
Code: github.com/tufts-ml/SSL-vs-SSL-benchmark [MIT license].datasets. However, in medical imaging applications as-
sembling a large dataset with appropriate label can be pro-
hibitively costly due to manual effort required by a human
expert. In contrast, images alone, without labels, are of-
ten readily available in health records databases. In recent
years, significant research has focused on developing meth-
ods that leverage both the large unlabeled set and a small
labeled set to enhance image classifier training, aiming to
surpass models trained solely on labeled data.
Two ways of leveraging unlabeled data are particu-
larly popular: semi-supervised and self-supervised learn-
ing, both often abbreviated as SSL. Recent efforts in semi-
supervised learning [74, 88] usually train deep classifiers
jointly [5, 69] using an objective with two loss terms,
one favoring labeled-set accuracy and the other favoring
label-consistency or label-smoothness. Alternatively, self-
supervised methods [65] take a two-stage approach, first
training deep representations on the unlabeled set, then fine-
tuning a classifier on the labeled set. Exemplars are numer-
ous [10, 17, 19, 20, 37]. Despite the remarkable progress
reported in each direction, these two paradigms have been
largely developed independently [21]. A direct compar-
ison of the two paradigms has been notably absent. A
practitioner building a medical image classifier from lim-
ited labeled data may ask, “ Which recent semi- or self-
supervised methods are likely to be most effective? ”
Performance can be quite sensitive to hyperparameters
for both semi-SL [69, 71] and self-SL [75]. We argue that
any careful comparison must consider tuning hyperparame-
ters of all methods in a fair fashion. However, recent work
has not handled this well. First, as reviewed in Table 1, re-
cent benchmarks for both semi- and self- paradigms often
omit anytuning of hyperparameters (see the Fungi semi-
SL experiments of Su et al. [71] or Ericsson et al. [27]’s
self-SL benchmark). This practice of using off-the-shelf de-
faults likely leads to under-performing given a new task, and
may impact some methods more than others. An even big-
ger issue with prevailing semi-SL practice was originally
raised by Oliver et al. [62]: many published papers per-
form hyperparameter tuning on a labeled validation set that
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
22282
is larger (sometimes much larger) than the limited labeled
train set. Such experimental settings are unrealistic . SSL
is intended for practitioners without abundant available la-
beled data. Practitioners that need to make the most of 1000
available labeled images will not elect to put more images in
validation than training, and thus are not helped by bench-
marks that do. Unfortunately, five years later after [62] we
find this issue is still widespread (see Tab. 1). For example,
Wang et al. [76]’s semi-SL results on TissueMNIST tune on
a validation set over 50x larger than the labeled train set.
Oliver et al. [62] further cast some doubt on whether ef-
fective tuning is possible with small labeled sets, saying
“Extensive hyperparameter tuning may be somewhat fu-
tile due to an excessively small collection of held-out data
to measure performance on”. There thus remains a press-
ing question for resource-constrained practitioners: “ Given
limited available labeled data and limited compute, is
hyperparameter tuning worthwhile? ”
This study makes progress toward answering these ques-
tions by delivering a comprehensive comparison of semi-
and self-supervised learning algorithms under a resource-
constrained scenario that matches how SSL might be used
on real-world medical tasks. Our goal is to enable practi-
tioners to tackle new image classification challenges where
only modest-sized labeled datasets exist. We target settings
with roughly 2-10 class labels of interest, where each class
has 30-1000 available labeled images for all model devel-
opment (including training and validation). We select rep-
resentative tasks across 4 datasets that span low-resolution
(28x28) to moderate-resolution (112x112 and 384x384).
On each task, we run careful experiments with represen-
tative recent methods from both semi- and self-supervised
learning to clarify what gains are possible with unlabeled
data and how to achieve them.
We emphasize realism throughout, in 4 distinct ways: (1)
using validation set sizes that are never bigger than the train
set, avoiding the unrealistically-large validation sets com-
mon in past work (Tab. 1); (2) performing hyperparameter
search using the same protocol ,same compute budget (fixed
number of hours)* and same hardware (one NVIDIA A100
GPU) for all algorithms for fair comparison; (3) respect-
ing natural class imbalance ; (4) profiling performance over
time, to inform labs with smaller runtime budgets.
In summary, the contributions of this study are:
1. We provide a systematic comparison of semi- and self-
supervised methods on equal footing to connect two re-
search directions that have been heretofore separate.
2. We adopt a realistic experimental protocol designed
to consider the same constraints on available labels and
runtime that SSL practitioners face, avoiding the unre-
*Oliver et al. [62] give each method 1000 trials of a cloud hyperpa-
rameter tuning service. However, training speed can differ substantially
across SSL methods. We argue a fixed wallclock time budget is more fair.alistic aspects of past benchmarks, especially the use of
far too large validation sets.
3. We show that hyperparameter tuning is viable with a
realistic-sized validation set , and in many cases neces-
sary to do well at a new classification task with new data.
Ultimately, we hope this study guides practitioners with
limited data toward successful deployment of semi- and
self-supervised methods on real problems.
2. BACKGROUND AND METHODS
Unified Problem Formulation. Following the recent sur-
vey by Chen et al. [21], we adopt a unified perspective
for supervised, semi-supervised and self-supervised image
classification with limited available labeled data. For model
development (including training and hyperparameter selec-
tion), we assume there are two available datasets. First, a
small labeled dataset Lof feature-label pairs (x, y), where
each image is represented by a D-dimensional feature vec-
torx∈RDand its corresponding class label takes one of
Cpossible values: y∈ {1,2, . . . C}. Second, an unlabeled
setUcontaining only feature vectors. Typically, we assume
the unlabeled set is much larger: |U| ≫ |L| .
Given labeled set Land unlabeled set U, we wish to train
a neural network that can map each input xto a probabil-
ity vector in the C-dimensional simplex ∆Crepresenting a
distribution over Cclass labels. Let fv(·) :RD→RFde-
note a backbone neural network with parameters vproduc-
ing an F-dimensional embedding given any input image.
Letgw(·) :RF→∆Cdenote a final linear-softmax clas-
sification layer with parameters w. The following unified
objective can capture all three learning paradigms:
v∗, w∗←arg min
v,wP
x,y∈LλLℓL(y, gw(fv(x))) (1)
+P
x∈UλUℓU(x, fv, gw).
Here, ℓLrepresents a labeled-set loss (e.g. multi-class
cross-entropy), and ℓUrepresents a unlabeled-set loss.
λL, λU≥0are weights for the corresponding loss terms.
The design of ℓUis usually what differs substantially across
methods. For instance, setting ℓUto be cross-entropy com-
puted with pseudo-labels generated from classifier gre-
covers PseudoLabel [58]; a temperature-scaled instance-
similarity contrastive loss recovers SimCLR [17].
All three learning paradigms that we study optimize
Eq. (1), yet differ in the number of phases and in how to
set the scalar weights λL, λUon each loss term. Super-
vised learning ignores the unlabeled term throughout train-
ing (λU= 0), thus learning parameters using only the la-
beled set. Semi-supervised methods include both terms in
one end-to-end training, keeping both λU>0andλL>0.
Self-supervised learning has two phases. In phase 1
(“pretraining”), the labeled term is omitted ( λL= 0, λU=
1) and the focus of learning is an effective representation
22283
Benchmark Methods Unrealistic Experiments Labeled train size Labeled val. size Acc vs. Time?
Realistic eval. SSL [a]Semi CIFAR-10 (Tab. 1-2, Fig. 2) 4000 5000 no
SVHN (Tab. 1-2, Fig. 3-4) 1000 7325
Fine-grained SSL [b] Semi & 1 Self∗Semi-Aves 5959 8000 no
Semi-Fungi 4141 None
USB [c] Semi TissueMNIST 80/400 23640†no
Semi-Aves 5959 None
Self benchmark [d] Self ImageNet 1.28 mil. None no
SSL-vs-SSL (ours) Semi & Self no 400-1660 no bigger than train yes
Table 1. Comparison of related benchmarks of semi-supervised and self-supervised learning. Past works either do no hyperparameter
tuning at all (val. size = None), or use an unrealistically large validation set (defined as larger than the labeled train set ) in some or
all experiments. Further, each work almost exclusively looks at methods from one paradigm, either semi- or self- (*: [b] includes one
self-supervised method, MOCO). In contrast, in this paper we benchmark 6 semi- and 7 self-supervised algorithms with hyperparameter
tuning on realistic validation sets. Acc vs. Time? indicates whether the work analyzes performance over training time. Number marked †
confirmed via GitHub comment by USB authors. Citations: a: Oliver et al. [62], b: Su et al. [71] c: Wang et al. [76] d: Ericsson et al. [27].
layer fv(classifier gwis not included in this phase). In
phase 2 (“fine-tuning”), we focus on the labeled term and
omit the unlabeled term ( λL= 1, λU= 0). We fix the
representation parameter vand fine-tune the classifier w.
We now identify the 16 methods we will evaluate:
Supervised methods. The goal of leveraging unlabeled
dataUis to obtain better performance than what we could
obtain using only the labeled set L. Therefore, we naturally
compare to 3 high-quality supervised baselines that use only
the labeled set. First, “Sup” denotes a classifier trained with
supervised loss ℓLset to multi-class cross-entropy. Sec-
ond, “MixUp” trains with cross entropy with the addition
of mixup data augmentation [85]. Finally, “SupCon” pur-
sues a supervised contrastive learning loss for ℓL[49].
Semi-supervised methods. We compare 6 semi-
supervised methods that train deep classifiers on both la-
beled and unlabeled data simultaneously as in Eq. (1). To
represent the state-of-the-art of semi-supervised image clas-
sification, we select Pseudo Label (“PseudoL”) [58], Mean
Teacher (“MeanTch”) [73], MixMatch [5], FixMatch [69],
FlexMatch [84] and CoMatch [59]. These choices cover
a reasonably wide spectrum of unlabeled loss design strat-
egy, year of publication, and computation cost. CoMatch
represents a recent trend of combining semi- and self-
supervision. See App D.2 for a wider literature review.
Self-supervised methods. We compare 7 self-
supervised algorithms: SimCLR [17], MOCO (v2) [20, 37],
SwA V [10], BYOL [34], SimSiam [19], DINO [11] and
Barlow Twins (“BarlowTw”) [82]. These algorithms epito-
mize the field of self-supervised learning as of this writing.
SimCLR, MOCO (v2), and SwA V are based on contrastive
learning, which learns effective representations when pro-
vided with similar and dissimilar samples. BYOL was de-
signed to circumvent the need for dissimilar samples. Sim-
Siam is a simple yet effective Siamese representation learn-
ing method. DINO uses a teacher-student network architec-ture for representation distillation. Barlow Twins preserves
information and reduces redundancy by favoring a close-to-
identity cross-correlation matrix between augmented pairs
of data. See App D.3 for further review.
3. RELATED WORK
Table 1 summarizes key attributes of existing major bench-
marks for semi-supervised and self-supervised methods.
We now discuss how our study situates in this context.
Comparison to Oliver et al. Oliver et al. [62] provide
an influential benchmark of deep semi-supervised methods.
Like our work, they highlight the pressing issue of using un-
realistically large validation sets for hyperparameter tuning.
However, most of their experiments (e.g. all their tables
and their figures 2-4) still use this unrealistic setting to be
comparable to other works; only one subsection (Sec. 4.6)
examines validation sets no larger than the train set. Instead,
we exclusively use validation set sizes no larger than train-
ing set , mimicking what practitioners would face in real-
applications. Our definition of a “realistic” size for a vali-
dation set – no larger than the labeled train set – is broader
than Oliver et al. ’s definition of 10% of the train set. We fa-
vor our definition because Oliver et al. find specifically that
“for validation sets of the same size (100%) as the training
set, some differentiation between the approaches is possi-
ble.” In contrast, they found reliable differentiation was not
always possible with much smaller validation sets.
Furthermore, the benchmarking results presented by
Oliver et al. require each algorithm to complete “1000 tri-
als of Gaussian Process-based black-box optimization using
Google Cloud ML Engine”. This resource-intensive pro-
cess seems impractical for researchers outside well-funded
industrial labs. We use more modest compute budgets and
specifically allow each method the same total wallclock run-
time for tuning, again obeying practical constraints.
Other semi-supervised benchmarks. TorchSSL [84]
22284
benchmarked eight popular semi-supervised learning algo-
rithm using a unified codebase. The same group later ex-
tended that effort to natural language and audio process-
ing in their USB benchmark [76]. Another recent effort
for fine-grained classification by Su et al. [71] evaluates
semi-supervised learning on datasets that exhibit class im-
balance and contains images from novel classes in the unla-
beled set, studying the effect of different initializations and
the contents of unlabeled data on the performance of semi-
supervised methods. While making significant contribu-
tions, these works focused on semi-supervision almost ex-
clusively and did not include multiple self-supervised learn-
ing methods, thus leaving open the questions “ How do the
two paradigms compare? Which methods are best overall? ”
Prior self-supervised benchmarks. Several works have
proposed different benchmarks for self-supervised meth-
ods. Goyal et al. [32] introduced a benchmark that covers
9 different tasks, such as object detection and visual nav-
igation. Ericsson et al. [27] compared thirteen top self-
supervised models on 40 downstream tasks. Da Costa
et al. [23] presented a library of self-supervised methods
that can be easily plugged into different downstream tasks
and datasets. However, these works mainly consider other
self-SL methods without direct comparison to the semi-SL
paradigm. Moreover, they mostly do not incorporate the use
of a validation set for hyperparameter tuning at all . This
can lead to suboptimal accuracy at test time and complicate
comparisons to methods that do tune.
Self-supervision for medical images. Our work is
complementary to recent efforts that assess self-supervised
pipelines for medical image classification [2, 3]. They fo-
cus primarily on how to design multi-stage transfer learning
pipelines and do not comprehensively compare many differ-
ent self-supervised methods or anysemi-supervised meth-
ods. Further, many datasets studied by [3] come from pro-
prietary projects conducted at Google. In contrast, all our
data and experiments are open and reproducible by others.
Combining semi- and self-supervision. Recent re-
search has explored the fusion of semi-supervised learning
and self-supervised learning ideas [50, 59, 83, 87]. Some
recent surveys [21, 65] compare semi- and self-supervised
learning, but they focus on literature review while we offer
realistic and comprehensive benchmarking experiments.
4. DATASETS AND TASKS
We study four open-access medical image classification
datasets, all with 2D images that are fully deidentified. Ta-
ble 2 reports statistics for all train/test splits. The exact
splits used in our study can be found in our open-source
codebase, documented in App. A.
Two datasets – PathMNIST and TissueMNIST – are
selected from the MedMNIST collection [80] (criteria in
App. B.2). Prior experiments by Yang et al. [80] suggestTissueMNIST PathMNIST
Labeled Unlab.
Train Val Test Train
total 400 400 47280 165066
GE 15 15 1677 5851
DCT 19 19 2233 7795
POD 19 19 2202 7686
LEU 28 28 3369 11761
IE 37 37 4402 15369
TAL 59 59 7031 24549
PT 95 95 11201 39108
CD/CT 128 128 15165 52947Labeled Unlab.
Train Val Test Train
total 450 450 7180 89546
NORM 39 39 741 7847
MUC 40 40 1035 7966
ADI 47 47 1338 9319
STR 47 47 421 9354
BACK 48 48 847 9461
DEB 52 52 339 10308
LYM 52 52 643 10349
MUS 61 61 592 12121
TUM 64 64 1233 12821
TMED-2 AIROGS
Labeled Unlab.
Train Val Test Train
total 1660 235 2019 353500
PSAX 223 50 342 -
A2C 325 28 319 -
A4C 462 39 423 -
PLAX 650 118 935 -Labeled Unlab.
Train Val Test Train
total 600 600 6000 94242
Glaucoma 60 60 600 -
No Glauc. 540 540 5400 -
Table 2. Summary statistics of train/validation/test splits for all
datasets in our study. Each table’s rows are arranged in ascending
order based on the number of per-class labeled train images. Full
description of class names can be found in App B.3.
their 28x28 resolution is a reasonable choice for rapid proto-
typing; using larger 224x224 resolution does not yield much
more accurate classifiers for these two datasets.
Two other datasets represent more moderate resolutions
closely tied to contemporary clinical research: the 112x112
Tufts Medical Echocardiogram Dataset (TMED-2) and the
384x384 AIROGS dataset. Further details for each dataset
are provided in a dedicated paragraph below.
For each dataset, our data splitting strategy closely mir-
rors the conditions of real SSL applications. First, we let la-
beled training and validation sets contain a natural distribu-
tion of classes even if that may be imbalanced. This reflects
how data would likely be affordably collected (by random
sampling) and avoids artificially balanced training sets that
will not match the population an algorithm would encounter
in a deployment. Second, to be realistic we ensure vali-
dation sets are never larger than the available labeled train
sets. This is in contrast to previous benchmarks: Wang et al.
[76]’s TissueMNIST hyperparameter search used a valida-
tion set of 23,640 images even though the labeled training
set contained only 80 or 400 images. In practice, such a
large validation set would almost certainly be re-allocated
to improve training, as noted in Oliver et al. [62].
TissueMNIST is a lightweight dataset of 28x28 images
of human kidney cortex cells, organized into 8 categories.
The original dataset is fully-labeled and distributed with
predefined train/val/test split of 165,466/23,640/47,280 im-
22285
ages with some class imbalance. We assume a total label-
ing budget of 800 images, evenly split between training and
validation (to ensure hyperparameter tuning can differenti-
ate methods). We form a labeled training set of 400 images
from the predefined training set, sampling each class by its
frequency in the original training set. We form a labeled
validation set of 400 images sampled from the predefined
validation set. For unlabeled set, we keep all remaining im-
ages in the original training split, discarding known labels.
PathMNIST is another lightweight dataset of 28x28 im-
ages of patches from colorectal cancer histology slides that
comprise of 9 tissue types. The original dataset is fully-
labeled and distributed with predefined train/val/test split of
89,996/10,004/7,180 images. Class imbalance is less severe
than TissueMNIST. We assume a total labeled data budget
of 900 images, again evenly split between training and val-
idation. Labeled train, labeled valid, and unlabeled sets are
sampled via the same procedures as in TissueMNIST. For
both MedMNIST datasets, we use the predefined test set.
TMED-2 [39, 40] is an open-access dataset of 112x112
2D grayscale images captured from routine echocardiogram
scans. Each scan produces dozens of ultrasound images of
the heart, captured from multiple acquisition angles (i.e.,
different anatomic views). In this study, we adopt Huang
et al.’s view-classification task: the goal is to classify each
image into one of 4 canonical view types (see App. B.3).
View classification is clinically important: measurements or
diagnoses of heart disease can only be made when looking
at the right anatomical view [60, 77]. We used the provided
train/validation/test split (id #1), which is naturally imbal-
anced and matches our criteria for realistic sizing. We fur-
ther use the provided large unlabeled set of 353,500 images.
TMED-2’s unlabeled set is both authentic (no true labels
are available at all, unlike other benchmarks that “forget”
known labels) and uncurated [41] (contains images of view
types beyond the 4 classes in the labeled task).
AIROGS [24] is a public dataset released for a recent
competition where the binary classification task is to decide
whether the patient should be referred for glaucoma or not,
given a color fundus image of the retina (eye). We selected
this dataset because it represents an active research chal-
lenge even with all available labels. Furthermore, because
labels for this dataset were acquired at great expense (mul-
tiple human annotators graded each of over 100k images),
we hope our work helps assess how self-/semi-supervision
could reduce the annotation costs of future challenges. We
selected the 384x384 resolution favored by several high-
performing competition entries. We chose a labeling budget
of 1200 total images, evenly split between train and valida-
tion; the rare positive class (9 to 1 imbalance) is representa-
tive of many screening tasks. We took the rest of available
images as the unlabeled train set.5. EXPERIMENTAL DESIGN
Performance metric. We use balanced accuracy [33, 35]
as our primary performance metric. For a task with C≥
2classes, let y1:Ndenote true labels for Nexamples in a
test set, and ˆy1:Ndenote a classifier’s predicted labels. Let
TPccount true positives for class c(number of correctly
classified examples whose true label is c), and let Nccount
the total number of examples with true label c. Then we
compute balanced accuracy (BA) as a percentage:
BA(y1:N,ˆy1:N) =1
CCX
c=1TPc(y1:N,ˆy1:N)
Nc(y1:N)·100% (2)
Balanced accuracy is more suitable than standard accuracy
for imbalanced problems when each class matters equally.
The expected BA of a uniform random guess is100
C%.
On AIROGS, we also track metrics recommended by its
creators [24] for clinical utility: AUROC, partial AUROC
(>90% specificity) and sensitivity-at-95%-specificity.
Architectures. CNN backbones are popular in medical
imaging [6, 28–31, 42, 45, 54–56, 77, 78]. We use ResNet-
18 [36] on Tissue and Path, and WideResNet-28-2 [81] on
TMED-2. We experiment with both ResNet-18 and 50 [36]
on AIROGS to assess architectural differences.
Training with early stopping. For each training phase,
we perform minibatch gradient descent on Eq. (1) with
Adam [51] optimizer and a cosine learning rate sched-
ule [69]. Each training phase proceeds for up to 200 epochs,
where one epoch represents enough minibatch updates to
process the equivalent of the entire combined training set
L ∪ U . After every epoch, we record balanced accuracy on
the validation set. If this value plateaus for 20 consecutive
epochs, we stop the current training phase early.
Hyperparameters. Semi- and self-supervised learning
can both be sensitive to hyperparameters [71, 75]. Our
evaluations tune both shared variables (e.g. learning rates,
weight decay, unlabeled loss weight λU) as well as hyperpa-
rameters unique to each algorithm. See App. F.1 for details
on all hyperparameters for all methods.
Unified procedure for training and hyperparameter
tuning. We formulate a unified procedure mindful of real-
istic hardware and runtime constraints for a non-industrial-
scale lab working on a new medical dataset. We assume
each algorithm has access to one NVIDIA A100 GPU for a
fixed number of hours. Within the alotted compute budget,
for each algorithm we execute a serial random search over
hyperparameters, sequentially sampling each new configu-
ration and then training until either an early stopping criteria
is satisfied or the maximum epoch is reached. We track the
best-so-far classifier in terms of validation-set performance
every epoch. Algorithm D.1 provides pseudocode for the
procedure. Each time a new hyperparameter configuration
is needed, we sample each hyperparameter value indepen-
22286
dently from a distribution designed to cover its common set-
tings in prior literature (App. F.1). Our choice of random
search on a budget is thought to yield better performance
than grid search [4], fairly expends the same effort to train
all methods regardless of their cost-per-epoch or hyperpa-
rameter complexity, and does not assume industrial-scale
access to 1000 cloud-computing trials as in Oliver et al.
[62]. We find that performance saturates after about 25
hours per 80000 unlabeled examples. We thus allocate for
the total time budget 25 hours for PathMNIST, 50 hours for
TissueMNIST, and 100 hours for TMED-2. We allow 100
hours for AIROGS due to its larger resolution. Due to lim-
ited resources, we select only a few competitive methods to
run on AIROGS based on the results on other datasets.
Self-supervised classification phase. Self-supervised
methods by definition do not utilize label information when
training representation layer weights v. To enable a proper
comparison, for all self-SL methods our Alg. D.1 introduces
an additional classification layer, training it anew after each
epoch, each time using a 10 trial random search for an L2-
penalty regularization hyperparameter. We retain the best
performing weights won the validation set. We emphasize
thatwdoes not impact overall self-supervised training of v.
Data augmentation. Random flip and crop are used
for all semi-supervised and supervised methods. MixMatch
also utilizes MixUp [85], while RandAugment [22] is used
by FixMatch and FlexMatch. For each self-supervised
method as well as SupCon, we apply the same SimCLR
augmentation: random flip, crop, color jitter, and grayscale.
Multiple trials. We repeat Alg. D.1 for each method
across 5 separate trials (distinct random seeds). We record
themean balanced accuracy on valid and test sets of these
trials every 60 minutes (every 30 min. for Tissue and Path).
6. RESULTS & ANALYSIS
In each panel of Fig. 1, we focus on one dataset and archi-
tecture, plotting for each algorithm one line showing test set
balanced accuracy (averaged across 5 trials) as a function of
time spent running Alg. D.1. Variance across trials is visu-
alized in Fig C.4. Extra results on AIROGS showing other
performance metrics over time (AUROC, partial AUROC,
and sensitivity at 95% specificity) are found in Fig. C.3.
We emphasize that following best practice, each checkpoint
represents the best hyperparameters as selected on the val-
idation set; we then report that checkpoint’s test set perfor-
mance. These results help us answer two key questions:
Is hyperparameter tuning worthwhile for SSL meth-
ods when validation set sizes cannot be larger than the
train set? Across Fig. 1 and C.3, all 16 algorithms show
roughly monotonic improvements in test performance over
time, despite using a realistic-sized validation set. The final
performance of every algorithm and dataset shows improve-
ment due to our unified training and tuning procedure (fur-Gain in Bal. Acc. over best labeled-set-only
name Path Tissue TMED AIROGS median worst
MixMatch 12.4 3.7 -0.2 5.7 4.7 -0.2
MOCO 7.9 4.4 -18.8 n/a 4.4 -18.8
SwAV 8.4 4.3 -10.1 n/a 4.3 -10.1
SimCLR 11.0 5.9 -7.8 1.6 3.8 -7.8
BYOL 12.1 6.2 -6.0 0.9 3.5 -6.0
BarlowTw 11.2 2.2 -1.9 n/a 2.2 -1.9
SimSiam 1.9 2.1 -11.6 n/a 1.9 -11.6
FlexMtch 11.1 4.5 -1.1 -4.2 1.7 -4.2
DINO 12.6 6.5 -3.4 -3.8 1.6 -3.8
FixMatch 9.3 2.9 0.2 -4.5 1.6 -4.5
CoMatch 14.1 0.2 -3.0 n/a 0.2 -3.0
MeanTch -1.6 -1.9 -1.2 n/a -1.6 -1.9
PseudoL 0.6 -1.4 -4.4 -3.2 -2.3 -4.4
ref. val. 69.8 33.9 94.9 71.5
Table 3. Gains from SSL methods over only using labeled set ,
across 4 datasets. Self (italicized) , Semi (normal). We report gains
in percentage balanced accuracy (higher is better) in the mean final
test set performance (averaged over 5 runs of Alg. D.1) over a
reference value that represents the best labeled-set-only run among
Sup (minimize cross entropy), SupCon, and MixUp. MixMatch
has the highest median gain, and is the only method to never score
notably worse ( >1 point) than the best labeled set only run.
ther discussion in App. E.1). Therefore, we answer: Yes,
realistically-sized validation sets for hyperparameter tun-
ing and checkpoint selection can be effective.
What are the best SSL methods? . Among the many
methods we study in Fig. 1, no method clearly outperforms
others across all datasets. On Path, CoMatch, MixMatch,
DINO, and BYOL perform best. On Tissue, self-supervised
methods like DINO, BYOL, and SimCLR score well. On
TMED-2, ultimately only FixMatch and MixMatch are
competitive with the MixUp baseline. On AIROGS, only
MixMatch, SimCLR and BYOL beat the Sup baseline.
For each semi- and self- SSL method, Tab. 3 reports its
relative gain in balanced accuracy over the best labeled-set-
only supervised baseline on each dataset. We conclude that
MixMatch represents the best overall choice as it consis-
tently performs at or near the top across all tasks. It is the
only method to never deliver results worse by more than
1 percentage point than the best labeled-set-only baseline
(see “worst” column of Tab. 3). This is in stark contrast
to USB [76], where FixMatch and FlexMatch are ranked
notably higher that MixMatch. Such differences stress the
importance of choosing proper evaluation protocol tailored
to specific needs. We do caution that hyperparameter tun-
ing is strongly recommended for MixMatch to succeed on a
new dataset (see AIROGS result in Tab. 4).
Pretraining vs. from scratch. Plots in App. C show
accuracy-over-time profiles using initial weights pretrained
22287
01 2 4 8 16
hours606570758085bal. acc. on testPathMNIST
ResNet18
SupConMixUp
SupFixMatchFlexMtchMixMatchCoMatch
MeanTchPseudoLSimCLRBYOLDINO
BarlowTw
SwAV
SimSiamMOCOsemi self
01 2 4 816 32
hours25303540bal. acc. on testTissueMNIST
ResNet18
SupConMixUp
SupFixMatchFlexMtchMixMatch
CoMatch
MeanTchPseudoLSimCLRBYOLDINO
BarlowTwSwAV
SimSiamMOCOsemi self
01248163264
hours7580859095bal. acc. on testTMED-2
WideRes28
SupConMixUp
SupFixMatch
FlexMtchMixMatch
CoMatchMeanT ch
PseudoL
SimCLRBYOLDINOBarlowT w
SwAV
SimSiam
MOCOsemiself
01 2 4 816 32 64
hours60657075bal. acc. on testAIROGS
ResNet18
Sup
FixMatchFlexMtchMixMatch
PseudoLSimCLRBYOL
DINOsemiself
01 2 4 816 32 64
hours60657075bal. acc. on testAIROGS
ResNet50
Sup
FixMatchFlexMtchMixMatch
PseudoLSimCLRBYOL
DINOsemiself
SupConlabeled-set-only
MixUp
SupFixMatchsemi
FlexMtch
MixMatch
CoMatch
MeanTch
PseudoLSimCLRself
BYOL
DINO
BarlowTw
SwAV
SimSiam
MOCOFigure 1. Balanced accuracy over time profiles of semi- and self-supervised methods across all 4 datasets. At each time, we report the
test set bal. acc. of each method (mean over 5 trials of Alg. D.1). Best viewed electronically. Top Row: On these 3 datasets, we compare all
6 semi- and 7 self- methods (see legend in lower right, citations in Sec. 2), to 3 labeled-set-only baselines. Bottom Row: On larger AIROGS
dataset, we compare selected methods representative of the best in the top row. Thin lines show final performance to ease comparison.
From these charts, we suggest that our unified training and tuning (Alg. D.1) is effective, as all methods show gains over time.
on ImageNet. Compared to training from scratch, we see
only slight gains (less than 3 points of BA) in ultimate
test-set accuracy when top methods use pretraining, which
aligns with observations in [66] but counters the “large mar-
gin” gains (sometimes over 10 points) reported by Su et al.
[71]. Pretraining’s benefits still include improved conver-
gence speed and reduced variance across trials.
ResNet-18 vs. ResNet-50. The bottom row of
Fig 1 compares ResNet-18 and ResNet-50 architectures on
AIROGS. We broadly find that the method rankings are
similar. More notably, ResNet-50 is not substantially bet-
ter than ResNet-18 in this task when compared using the
same runtime budget. Profiles of other performance metrics
in App C.3 suggest the same conclusions.
Tuning vs. Transferring from another dataset. To
make the most of limited labeled data, one strategy is to
use the entire labeled set for training, reserving no vali-
dation set at all. This approach relies on pre-established
“best” hyperparameters sourced from other datasets and
transferred to the target task, obviating the need for data-
specific tuning. Su et al. [71] employed this strategy in
their benchmark, avoiding tuning due to concerns that small
validation sets would yield unreliable selection. However,
this claim in their work was not supported by experiments.
Here, we directly compare our hyperparameter tuning strat-
egy (Alg. D.1) with Su et al.’s transfer strategy. We train
the latter on the combined train + validation sets, using
“best” hyperparameters from CIFAR-10 and Tissue (detailsin App. F.2). Tab. 4 reports the final test-set balanced accu-
racy of each strategy on Path, TMED-2 and AIROGS.
Our hyperparameter tuning strategy is competitive
across all datasets: the “Tune” columns of Tab. 4 have
the highest fraction of green cells. The alternative trans-
fer strategy is faster and occasionally yield reasonable re-
sults. However, there is no guarantee of good performance
from hyperparameter transfer, as evidenced by the inferior
cases in Tab. 4 highlighted in yellow and red. There is also
no consistent distinction between transferring from Tissue
(an arguably more related medical dataset) and transferring
from CIFAR-10, underscoring the challenge of identifying
a “closely related” dataset for hyperparameter transfer.
Overall, we recommend researchers pursuing SSL for a
new medical classification task should adopt our hyperpa-
rameter tuning strategy. Transferring off-the-shelf hyper-
parameters has highly variable performance and can even
result in divergence (see MixMatch on AIROGS in Tab. 4).
Even with reasonable hyperparameters, having no valida-
tion set makes checkpoint selection challenging. Adjacent
epochs that exhibit similar low training losses may differ
by over 5% in test accuracy in our experiments. Improving
transfer from existing hyperparameters to a new task is an
interesting further direction of research.
22288
PathMNIST TMED-2 AIROGS
Hyperparam. strategy: Tune Best from Best from Tune Best from Best from Tune Best from Best from
Tissue CIFAR-10 Tissue CIFAR-10 Tissue CIFAR-10
semi MixMatch 82.24 83.87 73.51 94.71 92.67 74.73 77.14 61.75 Diverged
CoMatch 83.95 80.51 86.42 91.87 92.51 92.67
FixMatch 79.11 82.64 79.73 95.06 95.12 95.73 66.92 66.31 65.50
FlexMatch 80.91 78.70 80.23 93.77 93.97 93.70 67.23 72.82 74.52
Pseudo-Label 70.42 66.12 64.71 90.52 88.78 82.13 68.21 64.43 66.93
Mean-Teacher 68.21 55.90 56.13 93.70 91.21 58.26
self SimCLR 81.05 79.86 80.05 87.04 84.41 87.24 73.01 67.15 67.95
BYOL 81.79 80.35 80.16 88.90 88.41 85.15 72.40 70.13 70.63
SwA V 77.90 78.24 81.30 84.76 82.17 82.20
MoCo 77.73 79.95 79.12 76.14 78.56 79.11
SimSiam 71.78 68.62 70.44 83.24 80.93 80.20
BT 80.85 72.79 80.25 92.96 91.63 90.81
DINO 82.52 77.50 80.67 91.45 84.80 90.03 67.61 66.46 67.13
Table 4. Hyperparameter strategy evaluation. Table reports ultimate test-set balanced accuracy as percentage (mean over 5 trials, higher
is better) on three target datasets: PathMNIST, TMED-2, and AIROGS. 3 different hyperparameter strategies are compared for each SSL
method. Due to computation constraints, we only select a few algorithms to run on AIROGS, explained in Sec. 5. Tune strategy: tuning
for best hyperparameters via our proposed Alg. D.1 for 100 hours (TMED-2 and AIROGS) and 25 hours (PathMNIST), using provided
train/validation split. Best from Tissue/CIFAR-10 strategy: just one training phase on the target dataset using the “best” hyperparameters
selected from the named source data. These runs combine train and validation data for fitting following Su et al. [71], and typically take
1-4/5-30/10-40 hours on Path/TMED-2/AIROGS depending on the algorithm and whether early stopping is triggered. To highlight which
strategies are most effective, we bold the best strategy within each dataset-specific row, and color cells relative to this bold value as green
(within 2.5 of best), yellow (within 5 of best), and red (worse by 5 or more points).
7. DISCUSSION & CONCLUSION
We have contributed a benchmark that helps practioners
quantify what gains in a classification task are possible
from the addition of unlabeled data and which methods help
achieve them. We offer a unified approach to training and
hyperparameter selection of semi-supervised methods, self-
supervised methods, and supervised baselines that is both
affordable and realistic for research labs tackling new chal-
lenges without industrial-scale resources.
Limitations. We deliberately focused on a modest num-
ber of labeled examples (30 - 1000) per class, motivated
by projects where substantive effort has already gone into
labeled data collection. Applications in more scarce-label
regimes (e.g. zero-shot or few-shot) may need to also con-
sult other benchmarks, as should those looking at hundreds
of fine-grained classes. We also did not specifically study
situations where the distribution of labeled and unlabeled
data significantly differs [41, 44, 62, 67].
To enable thorough experiments, we focused on moder-
ate ResNet architectures that have proven effective in clin-
ical applications [42, 78]. As ViT-based methods become
popular in medical imaging [1, 43], it would be valuable
to understand if similar findings hold with vision trans-
formers [26]. We leave this to future work. Ideally, our
results would be verified across multiple train-validation
splits; however the resources needed remain prohibitive.Our analysis here is limited the specific metrics of bal-
anced accuracy and sensitivity/specificity (for AIROGS).
Other clinically useful metrics, such as AUPRC, calibra-
tion, or net benefit, may be needed to decide if a classifier is
appropriate for deployment [70]. For medical applications,
it is also key to understand fairness across subpopulations
[13] to avoid propagating structural disadvantages.
Broader impact. All data analyzed here represent fully-
deidentified open-access images approved for widespread
use by their creators. We think the benefit of promoting
these medical tasks to advance ML research outweighs the
slight risk of patient reidentification by a bad actor.
Outlook. Our experiments show that real benefits from
the addition of unlabeled data are sometimes possible: our
recommended methods see gains of +5 points of balanced
accuracy on TissueMNIST, +10 points on PathMNIST, and
+5 points on AIROGS against strong labeled-set-only base-
lines. In contrast, most tested methods do not add signif-
icant gains on TMED-2, perhaps due to that benchmark’s
uncurated nature [41]. We hope our work enables the re-
search community to convert decades of effort on SSL into
improved patient outcomes and better scientific understand-
ing of disease and possible treatments. We further hope that
our benchmark inspires those that pursue improved method-
ological contributions to favor realistic evaluation protocols
and clinically-relevant datasets.
22289
References
[1] N Ahmadi, MY Tsang, AN Gu, TSM Tsang, and P Abolmae-
sumi. Transformer-based spatio-temporal analysis for clas-
sification of aortic stenosis severity from echocardiography
cine series. IEEE Transactions on Medical Imaging , 2023. 8
[2] Shekoofeh Azizi, Basil Mustafa, Fiona Ryan, Zachary
Beaver, Jan Freyberg, Jonathan Deaton, Aaron Loh, Alan
Karthikesalingam, Simon Kornblith, Ting Chen, Vivek
Natarajan, and Mohammad Norouzi. Big Self-Supervised
Models Advance Medical Image Classification. In Interna-
tional Conference on Computer Vision (ICCV) . arXiv, 2021.
4
[3] Shekoofeh Azizi, Laura Culp, Jan Freyberg, Basil Mustafa,
Sebastien Baur, Simon Kornblith, Ting Chen, Nenad Toma-
sev, Jovana Mitrovi ´c, Patricia Strachan, S. Sara Mahdavi,
Ellery Wulczyn, Boris Babenko, Megan Walker, Aaron
Loh, Po-Hsuan Cameron Chen, Yuan Liu, Pinal Bav-
ishi, Scott Mayer McKinney, Jim Winkens, Abhijit Guha
Roy, Zach Beaver, Fiona Ryan, Justin Krogue, Mozziyar
Etemadi, Umesh Telang, Yun Liu, Lily Peng, Greg S. Cor-
rado, Dale R. Webster, David Fleet, Geoffrey Hinton, Neil
Houlsby, Alan Karthikesalingam, Mohammad Norouzi, and
Vivek Natarajan. Robust and data-efficient generalization
of self-supervised machine learning for diagnostic imaging.
Nature Biomedical Engineering , 7(6):756–779, 2023. 4, 23
[4] James Bergstra and Yoshua Bengio. Random search for
hyper-parameter optimization. Journal of machine learning
research , 13(2), 2012. 6
[5] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas
Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A
holistic approach to semi-supervised learning. Advances in
neural information processing systems , 32, 2019. 1, 3, 21,
23
[6] Benjamin Billot, Colin Magdamo, Steven E Arnold,
Sudeshna Das, and Juan Eugenio Iglesias. Robust segmen-
tation of brain mri in the wild with hierarchical cnns and no
retraining. In International Conference on Medical Image
Computing and Computer-Assisted Intervention , pages 538–
548. Springer, 2022. 5
[7] Avrim Blum and Tom Mitchell. Combining labeled and un-
labeled data with co-training. In Proceedings of the eleventh
annual conference on Computational learning theory , pages
92–100, 1998. 21
[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in neural in-
formation processing systems , 33:1877–1901, 2020. 21
[9] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and
Matthijs Douze. Deep clustering for unsupervised learning
of visual features. In Proceedings of the European confer-
ence on computer vision (ECCV) , pages 132–149, 2018. 21
[10] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-
otr Bojanowski, and Armand Joulin. Unsupervised learning
of visual features by contrasting cluster assignments. Ad-
vances in neural information processing systems , 33:9912–
9924, 2020. 1, 3, 21[11] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In Pro-
ceedings of the IEEE/CVF international conference on com-
puter vision , pages 9650–9660, 2021. 3
[12] Paola Cascante-Bonilla, Fuwen Tan, Yanjun Qi, and Vicente
Ordonez. Curriculum labeling: Revisiting pseudo-labeling
for semi-supervised learning. In Proceedings of the AAAI
Conference on Artificial Intelligence , 2021. 21
[13] Leo Anthony Celi, Jacqueline Cellini, Marie-Laure
Charpignon, Edward Christopher Dee, Franck Dernoncourt,
Rene Eber, William Greig Mitchell, Lama Moukheiber, Ju-
lian Schirmer, et al. Sources of bias in artificial intelligence
that perpetuate healthcare disparities—A global review.
PLOS Digital Health , 1(3), 2022. 8
[14] Olivier Chapelle, Bernhard Scholkopf, and Alexander
Zien. Semi-supervised learning (chapelle, o. et al., eds.;
2006)[book reviews]. IEEE Transactions on Neural Net-
works , 20(3):542–542, 2009. 21
[15] Chen Chen, Chen Qin, Huaqi Qiu, Cheng Ouyang, Shuo
Wang, Liang Chen, Giacomo Tarroni, Wenjia Bai, and
Daniel Rueckert. Realistic adversarial data augmentation for
mr image segmentation. In Medical Image Computing and
Computer Assisted Intervention MICCAI , 2020. 23
[16] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-
woo Jun, David Luan, and Ilya Sutskever. Generative pre-
training from pixels. In International conference on machine
learning , pages 1691–1703. PMLR, 2020. 21
[17] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In International conference on ma-
chine learning , pages 1597–1607. PMLR, 2020. 1, 2, 3, 21
[18] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad
Norouzi, and Geoffrey E Hinton. Big self-supervised mod-
els are strong semi-supervised learners. Advances in neural
information processing systems , 33:22243–22255, 2020. 21
[19] Xinlei Chen and Kaiming He. Exploring simple siamese rep-
resentation learning. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
15750–15758, 2021. 1, 3
[20] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
Improved baselines with momentum contrastive learning.
arXiv preprint arXiv:2003.04297 , 2020. 1, 3
[21] Yanbei Chen, Massimiliano Mancini, Xiatian Zhu, and
Zeynep Akata. Semi-supervised and unsupervised deep vi-
sual learning: A survey. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence , 2022. 1, 2, 4
[22] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V
Le. Randaugment: Practical automated data augmen-
tation with a reduced search space. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition workshops , pages 702–703, 2020. 6, 21
[23] Victor Guilherme Turrisi Da Costa, Enrico Fini, Moin Nabi,
Nicu Sebe, and Elisa Ricci. solo-learn: A library of self-
supervised methods for visual representation learning. J.
Mach. Learn. Res. , 23(56):1–6, 2022. 4
22290
[24] Coen de Vente, Koenraad A. Vermeer, Nicolas Jaccard, He
Wang, Hongyi Sun, Firas Khader, Daniel Truhn, Temir-
gali Aimyshev, Yerkebulan Zhanibekuly, Tien-Dung Le,
Adrian Galdran, Miguel Ángel González Ballester, Gustavo
Carneiro, Devika R. G, Hrishikesh P. S, Densen Puthussery,
Hong Liu, Zekang Yang, Satoshi Kondo, Satoshi Ka-
sai, Edward Wang, Ashritha Durvasula, Jónathan Heras,
Miguel Ángel Zapata, Teresa Araújo, Guilherme Aresta,
Hrvoje Bogunovi ´c, Mustafa Arikan, Yeong Chan Lee,
Hyun Bin Cho, Yoon Ho Choi, Abdul Qayyum, Imran Raz-
zak, Bram van Ginneken, Hans G. Lemij, and Clara I.
Sánchez. AIROGS: Artificial Intelligence for RObust Glau-
coma Screening Challenge, 2023. 5, 15
[25] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 21
[26] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 8
[27] Linus Ericsson, Henry Gouk, and Timothy M Hospedales.
How well do self-supervised models transfer? In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 5414–5423, 2021. 1, 3, 4
[28] Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko,
Susan M Swetter, Helen M Blau, and Sebastian Thrun.
Dermatologist-level classification of skin cancer with deep
neural networks. nature , 542(7639):115–118, 2017. 5
[29] Loveleen Gaur, Ujwal Bhatia, NZ Jhanjhi, Ghulam Muham-
mad, and Mehedi Masud. Medical image-based detection of
covid-19 using deep convolution neural networks. Multime-
dia systems , 29(3):1729–1738, 2023.
[30] Hemant Ghayvat, Muhammad Awais, AK Bashir, Sharnil
Pandya, Mohd Zuhair, Mamoon Rashid, and Jamel Nebhen.
Ai-enabled radiologist in the loop: novel ai-based frame-
work to augment radiologist performance for covid-19 chest
ct medical image annotation and classification from pneu-
monia. Neural Computing and Applications , 35(20):14591–
14609, 2023.
[31] Amirata Ghorbani, David Ouyang, Abubakar Abid, Bryan
He, Jonathan H Chen, Robert A Harrington, David H Liang,
Euan A Ashley, and James Y Zou. Deep learning interpre-
tation of echocardiograms. NPJ digital medicine , 3(1):10,
2020. 5
[32] Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan
Misra. Scaling and benchmarking self-supervised visual rep-
resentation learning. In Proceedings of the ieee/cvf Inter-
national Conference on computer vision , pages 6391–6400,
2019. 4
[33] Margherita Grandini, Enrico Bagli, and Giorgio Visani.
Metrics for multi-class classification: an overview. arXiv
preprint arXiv:2008.05756 , 2020. 5
[34] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin
Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch,Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-
laghi Azar, et al. Bootstrap your own latent-a new approach
to self-supervised learning. Advances in neural information
processing systems , 33:21271–21284, 2020. 3
[35] Isabelle Guyon, Kristin Bennett, Gavin Cawley, Hugo Jair
Escalante, Sergio Escalera, Tin Kam Ho, Núria Macià,
Bisakha Ray, Mehreen Saeed, Alexander Statnikov, et al.
Design of the 2015 ChaLearn AutoML challenge. In
2015 International Joint Conference on Neural Networks
(IJCNN) , pages 1–8. IEEE, 2015. 5
[36] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 5
[37] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross
Girshick. Momentum contrast for unsupervised visual rep-
resentation learning. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
9729–9738, 2020. 1, 3, 21
[38] Wassily Hoeffding. Probability inequalities for sums of
bounded random variables. The collected works of Wassily
Hoeffding , pages 409–426, 1994. 23
[39] Zhe Huang, Gary Long, Benjamin Wessler, and Michael C
Hughes. A new semi-supervised learning benchmark
for classifying view and diagnosing aortic stenosis from
echocardiograms. In Proceedings of the Machine Learning
for Healthcare Conference . PMLR, 2021. 5, 15
[40] Zhe Huang, Gary Long, Benjamin S Wessler, and Michael C
Hughes. TMED 2: A dataset for semi-supervised classifica-
tion of echocardiograms. In DataPerf: Benchmarking Data
for Data-Centric AI Workshop , 2022. 5, 15
[41] Zhe Huang, Mary-Joy Sidhom, Benjamin S Wessler, and
Michael C Hughes. Fix-a-step: Semi-supervised learn-
ing from uncurated unlabeled data. In Proceedings of The
26th International Conference on Artificial Intelligence and
Statistics (AISTATS) , 2023. 5, 8
[42] Zhe Huang, Benjamin S Wessler, and Michael C Hughes.
Detecting heart disease from multi-view ultrasound images
via supervised attention multiple instance learning. In Ma-
chine Learning for Healthcare Conference , pages 285–307.
PMLR, 2023. 5, 8
[43] Zhe Huang, Xiaowei Yu, Benjamin S Wessler, and
Michael C Hughes. Semi-supervised multimodal multi-
instance learning for aortic stenosis diagnosis. arXiv preprint
arXiv:2403.06024 , 2024. 8
[44] Zhe Huang, Xiaowei Yu, Dajiang Zhu, and Michael C
Hughes. Interlude: Interactions between labeled and un-
labeled data to enhance semi-supervised learning. arXiv
preprint arXiv:2403.10658 , 2024. 8
[45] Tongtong Huo, Yi Xie, Ying Fang, Ziyi Wang, Pengran
Liu, Yuyu Duan, Jiayao Zhang, Honglin Wang, Mingdi Xue,
Songxiang Liu, et al. Deep learning-based algorithm im-
proves radiologists’ performance in lung cancer bone metas-
tases detection on computed tomography. Frontiers in On-
cology , 13:1125637, 2023. 5
[46] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej
Chum. Label propagation for deep semi-supervised learning.
22291
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 5070–5079, 2019. 21
[47] Longlong Jing and Yingli Tian. Self-supervised visual fea-
ture learning with deep neural networks: A survey. IEEE
transactions on pattern analysis and machine intelligence ,
43(11):4037–4058, 2020. 21
[48] Jakob Nikolas Kather, Johannes Krisam, Pornpimol
Charoentong, Tom Luedde, Esther Herpel, Cleo-Aron Weis,
Timo Gaiser, Alexander Marx, Nektarios A Valous, Dyke
Ferber, et al. Predicting survival from colorectal cancer his-
tology slides using deep learning: A retrospective multicen-
ter study. PLoS medicine , 16(1):e1002730, 2019. 15
[49] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,
Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and
Dilip Krishnan. Supervised contrastive learning. Advances
in neural information processing systems , 33:18661–18673,
2020. 3
[50] Byoungjip Kim, Jinho Choo, Yeong-Dae Kwon, Seongho
Joe, Seungjai Min, and Youngjune Gwon. Selfmatch: Com-
bining contrastive self-supervision and consistency for semi-
supervised learning. arXiv preprint arXiv:2101.06480 , 2021.
4
[51] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 5
[52] Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende,
and Max Welling. Semi-supervised learning with deep gen-
erative models. Advances in neural information processing
systems , 27, 2014. 21
[53] Abhishek Kumar, Prasanna Sattigeri, and Tom Fletcher.
Semi-supervised learning with gans: Manifold invariance
with improved inference. Advances in neural information
processing systems , 30, 2017. 21
[54] Devidas T Kushnure, Shweta Tyagi, and Sanjay N Talbar.
Lim-net: Lightweight multi-level multiscale network with
deep residual learning for automatic liver segmentation in
ct images. Biomedical Signal Processing and Control , 80:
104305, 2023. 5
[55] Zhengfeng Lai, Chao Wang, Luca Cerny Oliveira, Brittany N
Dugger, Sen-Ching Cheung, and Chen-Nee Chuah. Joint
semi-supervised and active learning for segmentation of gi-
gapixel pathology images with cost-effective labeling. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 591–600, 2021.
[56] Zhengfeng Lai, Chao Wang, Henrry Gunawan, Sen-Ching S
Cheung, and Chen-Nee Chuah. Smoothed adaptive weight-
ing for imbalanced semi-supervised learning: Improve re-
liability against unknown distribution data. In Interna-
tional Conference on Machine Learning , pages 11828–
11843. PMLR, 2022. 5
[57] Samuli Laine and Timo Aila. Temporal ensembling for semi-
supervised learning. arXiv preprint arXiv:1610.02242 , 2016.
21
[58] Dong-Hyun Lee. Pseudo-label: The simple and efficient
semi-supervised learning method for deep neural networks.
InWorkshop on challenges in representation learning at
ICML , 2013. 2, 3, 21[59] Junnan Li, Caiming Xiong, and Steven CH Hoi. Comatch:
Semi-supervised learning with contrastive graph regulariza-
tion. In Proceedings of the IEEE/CVF international confer-
ence on computer vision , pages 9475–9484, 2021. 3, 4
[60] Ali Madani, Ramy Arnaout, Mohammad Mofrad, and Rima
Arnaout. Fast and accurate view classification of echocar-
diograms using deep learning. NPJ digital medicine , 1(1):6,
2018. 5
[61] Shaobo Min, Xuejin Chen, Hongtao Xie, Zheng-Jun Zha,
and Yongdong Zhang. A mutually attentive co-training
framework for semi-supervised recognition. IEEE Transac-
tions on Multimedia , 23:899–910, 2020. 21
[62] Avital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus
Cubuk, and Ian Goodfellow. Realistic evaluation of deep
semi-supervised learning algorithms. Advances in neural in-
formation processing systems , 31, 2018. 1, 2, 3, 4, 6, 8, 13,
22, 23
[63] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-
sentation learning with contrastive predictive coding. arXiv
preprint arXiv:1807.03748 , 2018. 21
[64] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort,
Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu
Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg,
et al. Scikit-learn: Machine learning in python. the Journal
of machine Learning research , 12:2825–2830, 2011. 20
[65] Guo-Jun Qi and Jiebo Luo. Small data challenges in big data
era: A survey of recent progress on unsupervised and semi-
supervised methods. IEEE Transactions on Pattern Analysis
and Machine Intelligence , 44(4):2168–2187, 2020. 1, 4
[66] Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy
Bengio. Transfusion: Understanding transfer learning for
medical imaging. Advances in neural information processing
systems , 32, 2019. 7
[67] Kuniaki Saito, Donghyun Kim, and Kate Saenko.
Openmatch: Open-set consistency regularization for
semi-supervised learning with outliers. arXiv preprint
arXiv:2105.14148 , 2021. 8
[68] Azizi Shekoofeh, Mustafa Basil, Ryan Fiona, Beaver
Zachary, Freyberg Jan, Deaton Jonathan, Loh Aaron,
Karthikesalingam Alan, Kornblith Simon, Chen Ting, et al.
Big self-supervised models advance medical image classifi-
cation. arXiv preprint arXiv:2101.05224 , 2021. 23
[69] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao
Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk,
Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying
semi-supervised learning with consistency and confidence.
Advances in neural information processing systems , 33:596–
608, 2020. 1, 3, 5, 21
[70] Ewout W Steyerberg and Yvonne Vergouwe. Towards bet-
ter clinical prediction models: seven steps for development
and an abcd for validation. European Heart Journal , 35(29),
2014. 8
[71] Jong-Chyi Su, Zezhou Cheng, and Subhransu Maji. A real-
istic evaluation of semi-supervised learning for fine-grained
classification. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 12966–
12975, 2021. 1, 3, 4, 5, 7, 8, 13, 16, 23, 25
22292
[72] Teppei Suzuki. Consistency regularization for semi-
supervised learning with pytorch. https://github.
com / perrying / pytorch - consistency -
regularization , 2020. 13
[73] Antti Tarvainen and Harri Valpola. Mean teachers are better
role models: Weight-averaged consistency targets improve
semi-supervised deep learning results. Advances in neural
information processing systems , 30, 2017. 3, 21
[74] Jesper E Van Engelen and Holger H Hoos. A survey on
semi-supervised learning. Machine learning , 109(2):373–
440, 2020. 1, 21
[75] Diane Wagner, Fabio Ferreira, Danny Stoll, Robin Tibor
Schirrmeister, Samuel Müller, and Frank Hutter. On the im-
portance of hyperparameters and data augmentation for self-
supervised learning. arXiv preprint arXiv:2207.07875 , 2022.
1, 5
[76] Yidong Wang, Hao Chen, Yue Fan, Wang Sun, Ran Tao,
Wenxin Hou, Renjie Wang, Linyi Yang, Zhi Zhou, Lan-Zhe
Guo, et al. Usb: A unified semi-supervised learning bench-
mark for classification. Advances in Neural Information Pro-
cessing Systems , 35:3938–3961, 2022. 2, 3, 4, 6
[77] Benjamin S Wessler, Zhe Huang, Gary M Long Jr, Stefano
Pacifici, Nishant Prashar, Samuel Karmiy, Roman A Sandler,
Joseph Z Sokol, Daniel B Sokol, Monica M Dehn, et al. Au-
tomated detection of aortic stenosis using machine learning.
Journal of the American Society of Echocardiography , 36(4):
411–420, 2023. 5
[78] Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe
Huang, Masha Zorin, Stanisław Jastrz˛ ebski, Thibault Févry,
Joe Katsnelson, Eric Kim, et al. Deep neural networks im-
prove radiologists’ performance in breast cancer screening.
IEEE transactions on medical imaging , 39(4):1184–1194,
2019. 5, 8
[79] Jiancheng Yang, Rui Shi, and Bingbing Ni. Medmnist clas-
sification decathlon: A lightweight automl benchmark for
medical image analysis. In 2021 IEEE 18th International
Symposium on Biomedical Imaging (ISBI) , pages 191–195.
IEEE, 2021. 14
[80] Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin
Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni. Medm-
nist v2-a large-scale lightweight benchmark for 2d and 3d
biomedical image classification. Scientific Data , 10(1):41,
2023. 4, 14, 15
[81] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-
works. In Proceedings of the British Machine Vision Confer-
ence (BMVC) , 2016. 5
[82] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and
Stéphane Deny. Barlow twins: Self-supervised learning via
redundancy reduction. In International Conference on Ma-
chine Learning , pages 12310–12320. PMLR, 2021. 3
[83] Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lu-
cas Beyer. S4l: Self-supervised semi-supervised learning. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 1476–1485, 2019. 4
[84] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jin-
dong Wang, Manabu Okumura, and Takahiro Shinozaki.
Flexmatch: Boosting semi-supervised learning with curricu-lum pseudo labeling. Advances in Neural Information Pro-
cessing Systems , 34:18408–18419, 2021. 3
[85] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and
David Lopez-Paz. mixup: Beyond empirical risk minimiza-
tion. arXiv preprint arXiv:1710.09412 , 2017. 3, 6, 21
[86] Wenqiao Zhang, Lei Zhu, James Hallinan, Shengyu Zhang,
Andrew Makmur, Qingpeng Cai, and Beng Chin Ooi. Boost-
MIS: Boosting medical image semi-supervised learning with
adaptive pseudo labeling and informative active annotation.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , 2022. 23
[87] Mingkai Zheng, Shan You, Lang Huang, Fei Wang, Chen
Qian, and Chang Xu. Simmatch: Semi-supervised learning
with similarity matching. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 14471–14481, 2022. 4
[88] Xiaojin Zhu. Semi-Supervised Learning Literature Survey.
Technical Report 1530, Department of Computer Science,
University of Wisconsin Madison., 2005. 1, 21
[89] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty.
Semi-supervised learning using gaussian fields and harmonic
functions. In Proceedings of the 20th International confer-
ence on Machine learning (ICML-03) , pages 912–919, 2003.
21
[90] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi,
Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A
comprehensive survey on transfer learning. Proceedings of
the IEEE , 109(1):43–76, 2020. 21
22293
