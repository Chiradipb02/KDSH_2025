HHMR: Holistic Hand Mesh Recovery by
Enhancing the Multimodal Controllability of Graph Diffusion Models
Mengcheng Li1, Hongwen Zhang2, Yuxiang Zhang1, Ruizhi Shao1, Tao Yu3, Yebin Liu1*.
1Department of Automation, Tsinghua University
2School of Artificial Intelligence, Beijing Normal University.
3Beijing National Research Center for Information Science and Technology, Tsinghua University
Figure 1. We introduce HHMR , a graph diffusion-based generation framework that are compatible with various human hand mesh recovery
tasks.
Abstract
Recent years have witnessed a trend of the deep integra-
tion of the generation and reconstruction paradigms. In this
paper, we extend the ability of controllable generative mod-
els for a more comprehensive hand mesh recovery task: di-
rect hand mesh generation, inpainting, reconstruction, and
fitting in a single framework, which we name as Holistic
HandMeshRecovery (HHMR). Our key observation is that
different kinds of hand mesh recovery tasks can be achieved
by a single generative model with strong multimodal con-
trollability, and in such a framework, realizing different
tasks only requires giving different signals as conditions. To
achieve this goal, we propose an all-in-one diffusion frame-
work based on graph convolution and attention mechanisms
for holistic hand mesh recovery. In order to achieve strong
control generation capability while ensuring the decoupling
of multimodal control signals, we map different modalities
to a shared feature space and apply cross-scale random
*Corresponding Author.masking in both modality and feature levels. In this way,
the correlation between different modalities can be fully ex-
ploited during the learning of hand priors. Furthermore, we
propose Condition-aligned Gradient Guidance to enhance
the alignment of the generated model with the control sig-
nals, which significantly improves the accuracy of the hand
mesh reconstruction and fitting. Experiments show that our
novel framework can realize multiple hand mesh recovery
tasks simultaneously and outperform the existing methods
in different tasks, which provides more possibilities for sub-
sequent downstream applications including gesture recog-
nition, pose generation, mesh editing, and so on.
1. Introduction
Hand mesh recovery aims to recover hand pose and mesh
from images, which has achieved a wide range of applica-
tions in ARVR, human-computer interaction, robotics, and
etc,. In the past decades, with the development of deep
learning techniques, the paradigm of hand mesh recovery
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
645
has undergone a shift from keypoints-based mesh&model
fitting [13, 24, 46, 50, 60] to learning-based regression.
Regression-based hand mesh recovery methods are based
on the encoder-decoder architecture which first encodes
various kinds of inputs, usually images or keypoints, and
then decodes to either 3D poses [44], pose and shape pa-
rameters [1, 5, 56, 61] of a parametric hand model [38]
or the vertex coordinates of a 3D hand mesh directly
[3, 4, 14, 26, 30, 31, 45]. However, the one-way regres-
sion strategy does not establish or fully utilize the underly-
ing distributions of natural hand pose and mesh, which sig-
nificantly restricts the achievement of more advanced hand
mesh recovery tasks, such as mesh&pose inpainting or gen-
eration.
Despite the classical hand mesh recovery task, another
spectrum of research focuses on formulating the priors
(or distributions) of natural hand pose or mesh by low-
dimensional manifolds [48]. Acquiring such a prior can
not only facilitate the hand mesh recovery but also enable
the generation of hands that ideally follow the biomechan-
ical constraints. Classical hand priors either rely on hand-
crafted 3D hand models with restricted joint angles [44] and
degrees of freedom (DoFs) [53]. Romero et al. [38] intro-
duced an explicit parametric hand model by principal com-
ponent analysis (PCA) on a dataset with natural hand pose
and shape parameters. However, the limited representative
power of explicit constraints or PCA significantly limits the
ability to represent complex postures not to say generate di-
verse hand meshes.
Benefiting from the rapid development of generative
learning techniques, generation-based implicit hand mod-
els become a hot topic. Some works [27, 64] have utilized
the Variational Autoencoder (V AE) to map the plausible
hand parameters to a Gaussian space. For the first time,
it becomes possible to generate hand meshes through sam-
pling in feasible distributions. Implicit hand models can
also be used for hand mesh recovery. By using two V AEs
to encode the image domain and hand pose domain inde-
pendently, CrossV AE or Generative Adversarial Networks
(GAN) are incorporated to align the two domains and en-
able the generation of hand parameters that align with the
input image ([43, 49, 51, 52]). However, existing implicit
hand models are mostly built on the parameter space or 3D
skeleton space, which makes it hard to generate meshes or
utilize geometric constraints directly. Moreover, additional
inversion or fitting processes are necessary for hand mesh
recovery based on existing implicit hand models, which is
time-consuming and sophisticated.
An inevitable trend for related research is the unifica-
tion of hand mesh recovery and the generative hand mod-
els. This will enhance the mutual benefits between existing
reconstruction and generation paradigms and finally enable
direct hand mesh generation, inpainting, reconstruction, andfitting simultaneously in an End2End manner. We call such
a comprehensive hand mesh recovery task as Holistic Hand
MeshRecovery, named HHMR, and propose an All-In-One
diffusion framework based on GCN and attention for holis-
tic hand mesh recovery without any additional finetuning
or inversion steps. Our key insight is that conditional gen-
eration based on diffusion models, through carefully mul-
timodal designed and conditioned training, is a powerful
methodology for achieving holistic hand mesh recovery.
Given different modalities of conditions as input, the dif-
fusion model can produce plausible hand mesh and pose
results thus naturally supports: i) hand mesh reconstruction
when given image as condition, ii) hand mesh inpainting
when given incomplete mesh or 2D&3D skeleton as condi-
tion, iii) hand mesh generation when given random noise as
input (with no condition), and iv) hand mesh fitting when
given a 2D skeleton as the condition. To fulfill the accu-
rate multimodal controllability of the diffusion models, we
map different conditions to a shared feature space and ap-
ply a random mask strategy at both modality and feature
levels to enhance the correlation learning between different
modalities. Moreover, we propose a condition-aligned gra-
dient guidance strategy during diffusion learning to further
improve the alignment with the conditions.
We evaluate the proposed holistic hand mesh recov-
ery framework on various downstream tasks. Empiri-
cal results demonstrate that i) on hand mesh generation
task, our approach generates more diverse poses. ii)
On hand mesh inpainting task, our method can recover
multiple plausible hand meshes from incomplete inputs.
iii) Our method achieved comparable results with SOTAs
on single-hypothesis reconstruction and outperforms SO-
TAs on multi-hypothesis reconstruction tasks. iv) With
condition-aligned gradient guidance, our method achieves
performance with higher precision in 2D fitting tasks.
The contributions can be summarized as:
• We propose an All-In-One framework based on a graph
diffusion model for holistic hand mesh recovery. The
hand prior learned in the graph diffusion model can be
readily applied to different downstream tasks without any
additional finetuning or inversion steps.
• We map different modality conditions to a shared feature
space and apply a random mask strategy at both modal-
ity and feature levels to enhance the correlation learning
between different modalities.
• We propose a condition-aligned gradient guidance strat-
egy during inference to further improve the alignment
with the conditions.
• Extensive experiments and comparisons validate the ef-
fectiveness of our framework and demonstrate various
downstream applications.
646
2. Related Work
2.1. Hand Mesh Recovery
In the past few decades, significant progress has been made
in human hand recovery. Early research [13, 24, 46, 50, 58,
60] utilized optimization methods to fit the hand pose from
2D skeletons detection.
With the development of neural networks, some
learning-based hand mesh reconstruction methods have
been proposed. One approach is utilizing the popular
parameter-based MANO [38] model and regressing the
pose and shape parameters of it. Due to the differentiabil-
ity of the MANO model, it becomes feasible to estimate
model parameters end-to-end from a single-view image in-
put ([1, 5, 54, 56, 59, 61]). However, parameter estima-
tion is a highly nonlinear task that lacks the correlation with
the 3D space. Another approach is directly regressing hand
mesh, which is typically achieved by building a mapping
network between 2D image input and 3D hand mesh out-
put. Graph convolution network (GCN) is one of the widely
employed mapping networks ([3, 4, 14, 26, 28, 45]). Addi-
tionally, there are methods that utilize one-dimensional lixel
heatmaps [33] to represent the 3D coordinates of vertices,
or employ UV map [2] to establish connections between 2D
and 3D. Lin et al. [30, 31] used transformer-based network
to regress vertices.
However, these hand reconstruction methods only pro-
vide a single plausible estimation, whereas in reality, the oc-
cluded parts could assume various poses. Thus, we propose
a probability diffusion-based model that generates multiple
plausible hand meshes from one input RGB image.
2.2. Hand Prior and Generation
The hand prior model is aimed at learning a distribution
of plausible hand poses. Previous prior models can be
broadly categorized into two types: one type learns the un-
conditional distribution pdata(x)of the human hand, and
the other is the task-specific prior that learning the condi-
tional distribution pdata(x|c)under given conditions (such
as RGB image or 2D skeleton).
For the unconditional prior distribution, one straight-
forward approach is to manually constrain the reasonable
range of hand poses based on the biological structure of the
human hand. Yang et al . [53] have constructed the hand
pose prior by manually defining the degrees of freedom for
each joint and the range of rotation angles. Spurr et al .
[44] proposed a set of losses that constrain hand pose to
lie within the range of biomechanically feasible 3D hand
configurations. The other line of works is learning-based
which involves learning feasible distribution from a large
dataset of poses. Javier et al. [38] applied principal com-
ponent analysis (PCA) on the pose and shape parameters of
a hand dataset. In both human hand domain [27, 64] andbody domain [35], researches have been conducted to map
the pose distribution to a standard Gaussian distribution by
an V AE network. Tiwari et al. [48] described the prior of
the human body by learning the neural distance from the
sampled pose to the low-dimensional manifold of reason-
able pose distributions. These unconditional priors often
require optimization when applied in downstream applica-
tions, which can be time-consuming.
The conditional prior model often learns the potential
pose distribution under specific task constraints, such as
predicting plausible pose from RGB images or 2D skele-
tal. A common approach is to construct V AEs in different
domains ( e.g. depth image, RGB image or 3D skeleton) and
then build hand priors through latent space alignment([43,
49, 51, 52]). Recently, Ci et al. [6] predicted the gradient of
log-likelihood of the human body pose distribution through
a score matching based model. However, these implicit con-
ditional prior models are mostly built on the 3D skeletons
and rely on specific conditions, which restricts their appli-
cation.
2.3. Diffusion based generative model
The diffusion model [41] is a generative model based on
stochastic diffusion processes, which uses a Markov chain
to gradually convert one distribution into another as mod-
eled in Thermodynamics. In practice, it pre-defines a for-
ward process that gradually adds noise to a dataset dis-
tribution until it converges to a standard Gaussian noise
distribution. Subsequently, a neural network is trained to
learn the reverse process and gradually denoise a random
noise into a reasonable data sample. Recently, some im-
age generation algorithms [22, 37, 42] that utilize diffusion
models have achieved significant breakthroughs. Diffusion
models have also demonstrated great success in the fields
of 3D object generation [29, 36, 39], human pose estima-
tion [12, 15, 23], and human motion generation [40, 47].
We build a diffusion-based hand generation model with a
classifier-free strategy, which can run with or without condi-
tional. Hence, it is capable of handling various downstream
tasks.
3. Preliminary: Diffusion Model
The denoising diffusion model is a probabilistic generative
model that consists of a forward process and a reverse pro-
cess. We denote the middle distribution for each step in the
process as p(xt), t= 0,1, ..., T , where the initial p(x0)is
the dataset distribution we want to generate, and the final
p(xT)∼ N(0,I)is a standard Gaussian noise distribution.
The forward process converts p(x0)top(xT)by grad-
ually adding small Gaussian noises with predefined means
and variances in a Markov Chain:
p(xt|xt−1) =N(p
1−βtxt−1, βtI) (1)
647
where {βt}is a predefined set of small constants.
The reverse process is a generation process that converts
the noise sampled from p(xT)to a reasonable data sample
inp(x0). The reverse process is also a Gaussian distribution
[11]. Moreover, the reverse diffusion process can be written
as:
pθ(xt−1|xt,c) =N(µθ
t(xt, t,c),ΣtI) (2)
where crepresents the conditions for generation and µθ
t,
Σtare means and variances of the reverse Gaussian process.
Usually, Σtis determined by {βt}, while µtis untrackable.
Thus, a neural network µθ
t(xt, t,c)is applied to predict the
means µt.
Furthermore, for a more effective learning of µθ
t, it is
typically reparameterized to learn i) the noise ϵadded from
x0toxtor ii) x0instead. Image generation tasks [9, 21,
22, 42] usually employ the first setting, whereas we follow
the human motion generation methods [40, 47] to predict
x0directly:
¯x0=fθ(xt,c, t), µθ
t(xt, t,c) =gt(¯x0, xt, t), (3)
where fis a neural network, θrepresents the training
parameters, gtis a determined reparameterized function.
More details can be referred to [22].
In summary, the training algorithm for the diffusion
model involves four parts: i) sample data x0from dataset
p(x0), ii) run the forward process to add noise on x0and
yield xt, iii) run the neural network fθto give a prediction
of¯x0, iv) back propagate the prediction loss ∥¯x0−x0∥.
4. Method
4.1. Problem Statement
Our goal is to develop a diffusion model that learns a prior
knowledge of the human hand from the given task-specific
conditions fθ(xt,c, t). Considering the highly non-linear
mapping from the parameter space of the hand template to
3D spatial space, we directly process on the 3D meshes in-
stead. We drew inspiration from the network architecture
of image generation method [37], but employed 3D graph
convolutions network (GCN) [7] instead of 2D image con-
volutions to encode the local information of the human hand
mesh. The details of the network architecture will be pre-
sented in the Sec.4.2.
In order to accommodate different downstream tasks, our
approach supports a variety of conditions cas input. Specif-
ically, we set cto a RGB image cimage to handle monocular
hand mesh reconstruction task; set cto 2D skeleton cskel 2d
for 2D hand mesh fitting task; set cto incomplete 3D skele-
toncskel 3dfor hand mesh inpainting task. We can also set
c=∅to achieve unconditional generation, and apply it to
the task of hand postures generation and hand mesh com-
pletion.Due to the different input conditions, our network can
be trained using different types of datasets. For the dataset
containing paired human hand images and annotated hand
meshes, we can train our network under all the afore-
mentioned conditions: c=cimage|cskel 2d|cskel 3d. For
the dataset containing only human hand poses, we employ
skeletal information c=cskel 2d|cskel 3das the condition to
train our generation model.
4.2. Graph Diffusion Network
Network architecture. As shown in Fig. 2, the main
structure of our diffusion model is a U-shaped network, con-
sisting of a downsampling encoder and an upsampling de-
coder with skip connections. Each block of both encoder
and decoder are formed by 4 layers: i.e., GCN layer, self-
attention layer, cross-attention layer, and optional upsam-
pling or downsampling layer.
Similar to the convolution in 2D image networks, we use
GCN [7] to aggregate information from each vertex’s neigh-
bors to encode local information of hand mesh, and expand
the receptive field through stacking multiple layers of GCN.
Then a self-attention layer is utilized to facilitate long-range
global information propagation on the 3D hand mesh, en-
abling our prior network to construct global relationships
across the surface vertices. We also apply a cross-attention
layer to inject conditional information cinto our generative
model.
Condition Mapping. In order to encode different condi-
tions into the same space that are compatible with the input
of the cross-attention layer, we designed distinct networks
to map different conditions to a shared feature space:
• For 3D skeleton and 2D skeleton condition, we simply
use MLP to encode cskel 2dandcskel 3d.
• For RGB image condition cimage , we first use a CNN-
based network to encode the latent image feature map and
then crop it into several image patches. After that, a Vi-
sion Transformer (ViT) [10] based model is applied to
further process image information. We also add a global
token cglobal while running ViT to encode the global in-
formation as the original ViT does. The global token
is further stacked with the output image patch features
cpatch of ViT to form the image condition cimage ∈
R(P+1)×D, where Pis the number of image patches.
Conditions with Random Mask. In our method, we
stack all three types of condition c=cimage⊕cskel 2d⊕
cskel 3d∈ R(P+1+1+1) ×Das the input of cross-attention
layers. To enhance the network’s generalization and ver-
satility, we design a two-level masking strategy. On the
multimodal level, in order to decouple the relationship
between conditions, we independently set each condition
cimage|cskel 2d|cskel 3dto an empty set ∅with probability
648
Figure 2. The pipeline of our graph diffusion model. With task-specific conditions, our model progressively removes noise from randomly
Gaussian noise and directly reconstructs the complete hand meshes. Additionally, we introduce a gradient-based guidance to improve the
alignment between the generated results and observations.
pmduring training. Furthermore, we also globally mask all
conditions with probability pallto train our model’s uncon-
ditional generation capability. On the feature level, We de-
sign different masking strategies for each condition individ-
ually. For image condition cimage , we encode it into several
image patches and randomly mask each patch with proba-
bility pimage , which enables our method to handle complex
occlusion input. For skeleton condition cskel 2dandcskel 3d,
we add Gaussian noise on each joint and randomly mask
each finger with probability pskel, which facilitates the gen-
eration from incomplete hand skeletons.
Note that when applying our model to a specific task, the
irrelevant conditions can be directly set to ∅. For example,
in the monocular reconstruction task, the condition is c=
cimage⊕ ∅ ⊕ ∅ , which we write as c=cimage for short.
4.3. Condition-aligned Gradient Guidance
In the 2D image generation task, a classifier guidance [9]
approach is proposed for the conditional diffusion model. It
trains an additional classifier network to predict the proba-
bility of input condition from xt. Then, by modifying the
means of the reverse process with a gradient of the log-
likelihood predicted from the classifier network, the diffu-
sion model can generate images that are more consistent
with the input conditions.Inspired by previous work [9], we propose a Condition-
aligned Gradient Guidance to encourage that the generated
hand is consistent with the input condition. Specifically, in
our implementation, we add a gradient guidance bias to the
means of each Gaussian distribution of the reverse process:
¯µt=µt−sΣt▽xt∥Pfθ(xt,c, t)−Px0∥ (4)
where sis a scale factor and Pis a task-specific operator.
Noted that fθ(xt,c, t)is the prediction of the GT mesh x0,
this method can be seen as a form of neural fitting. The
operator Pdetermines the specific fitting target. For exam-
ple, by setting Pas a joint regression matrix J, the gradi-
ent guidance works as a supervise term to constrain the 3D
joints of generated hand meshes to be consistent with the
3D skeleton condition.
In addition to serving as a supervisory, this gradient-
based guidance approach can also be viewed as a geometric
control during the generation process. For example, in the
hand mesh inpainting task, given an incomplete hand mesh,
we can apply the gradient guidance on the given part and
leave the missing part uncontrolled. In this situation, the
operator Pis a per-vertex binary mask MVthat indicates
the missing part of hand meshes. We can also use partial
skeletons to control the generation of the human hand by
setting PasMJJ, where MJis a per-joint binary mask.
649
Note that in contrast to the original classifier guidance
method, our gradient guidance approach does not need any
extra classifier network. Hence, our approach requires no
additional training, which is less time-consuming and plug-
to-use.
4.4. Loss Functions
For training our graph diffusion model, we utilize (1) dif-
fusion data loss, (2) vertex loss & joint loss and (3) mesh
smooth loss.
Data Loss. We use L1 loss to supervise the output of our
diffusion model as described in Sec.3:
Ldata=∥x0−¯x0∥ (5)
Vertex Loss & Joint Loss. We pretrained a joint regression
matrix Jto regress joints from output hand mesh and apply
L1 loss to supervise 3D coordinates of vertices and joints:
LV=X
∥V−VGT∥1
LJ=X
∥JV− JVGT∥1(6)
Mesh Smooth Loss. To ensure the geometric smoothness
of the predicted vertices, two different smooth losses are ap-
plied. First, we regularize the normal consistency between
the predicted and the ground truth mesh:
Ln=X
∥e·nGT∥1, (7)
where eis the edges of the generated hand mesh and nGT
is the faces normal vector calculated from the ground truth
mesh. Also, we minimize the L1 distance of each edge
length between the generated mesh and the ground truth
mesh:
Le=X
∥e−eGT∥1. (8)
5. Experiments
5.1. Experimental Setting
Implementation Details. Our network is implemented by
PyTorch. For RGB image condition, we use ResNet50 [18]
with initial weights pretrained on ImageNet [8] as the back-
bone for the mapping network and then evenly crop the im-
age feature into 8×8patches. For 2D/3D skeleton con-
ditions, we use a three-layer MLP with dropout [20] and
GELU [19] activation. For the diffusion model, we set the
denoising steps to 1000 during training, and utilized the
DDIM [42] algorithm to speed up inference.
Training Datasets. We simultaneously leveraged hand
pose datasets with only hand pose annotations and recon-
struction datasets with both RGB images and hand pose an-
notations. For pose datasets, we use Two-hand 500K [64]
and InterHand2.6M [34] datasets. Two-hand 500K [64] is aAPD( mm)↑SI(%)↓
PCA Generation 15.00 0.39
Ours Generation 17.30 0.05
PCA Inpainting 12.28 0.06
Ours Inpainting 13.59 0.01
Table 1. Quantitatively comparison of hand mesh generation and
inpainting.
two-hand pose dataset that utlized hand instances sampled
from single-hand datasets [34, 55, 62, 63]. InterHand2.6M
[34] contains both single-hand and two hands motion se-
quence data in 30FPS. we split the pose data of two hands
into two individual single-hand data. For reconstruction
datasets, we use FreiHand [63], HO3D V3 [16, 17], Com-
pHand [3, 4], both of them contain hand images and hand
mesh annotations.
Training Details. We train our model with a mixture of
the above datasets. We train our model using the AdamW
optimizer [32] on a single NVIDIA RTX 4090 GPU with
a mini-batch size 64. We trained for 1M mini-batches in
total. The learning rate is setting to 5e−4and decrease to
1e−5with cosine annealing.
5.2. Hand Mesh Generation
Our model can work without any condition by setting c=
∅. In this setting, our approach involves sampling a set
of noisy point clouds from a Gaussian distribution and de-
noising them into a complete human hand mesh as shown
in Fig.3. To quantitatively evaluate our generative model,
we follow Tiwari et al. [48] to use Average Pairwise Dis-
tance (APD) to quantify the diversity of generated meshes
and calculate the percentage of self-intersecting faces (SI)
to evaluate the realism. We randomly generate n= 500
hand meshes with our model and randomly sample n= 500
meshes within the PCA space of hand parameters from
MANO [38]. As shown in Tab.1, our results exhibit higher
APD and lower SI compared to the PCA sampling, indicat-
ing that our method is capable of producing more diverse
and realistic hand meshes.
5.3. Hand Mesh Inpainting
Given an incomplete hand mesh or hand 3D skeleton, our
model can inpaint the whole plausible hand mesh accord-
ing to the prior knowledge learned from a large hand pose
dataset. The key idea of mesh inpainting is using the gradi-
ent guidance to keep the given part of the hand unchanged
while allowing the diffusion model to give various genera-
tions for the missing part. As illustrated in the Fig.3, where
the thumb, index, and middle fingers are all missing from
the hand mesh, our model can provide various potential
completion results. Also, given a 3D hand skeleton that
missing the middle, ring, and little fingers, our model can
generate diverse whole-hand meshes. Please refer to the
650
Figure 3. Qualitative results of our method for different downstream tasks. From left to right are i) hand mesh generation results from
random Gaussian noise, ii) hand mesh inpainting from incomplete hand mesh or skeleton, iii) hand mesh reconstruction from monocular
RGB image, and iv) hand mesh fitting from 2D skeletons.
Figure 4. Qualitative results comparison with MobRecon [4] and
MeshGraphormer [30].
Supplementary for the details of the mesh inpainting al-
gorithm. We also conducted quantitative evaluation. As
shown in Tab.1, our method achieved a higher APD and a
lower SI compared to randomly sampling by PCA.
5.3.1 Hand Mesh Reconstruction
Single-Hypothesis. Given an RGB image as condition
cimage , our model can work as a monocular hand mesh re-
construction network. We first quantitatively evaluate our
method on Freihand [63] dataset. We use DDIM [42] algo-
rithm to run T= 10 denoising steps and only sample one
hypothesis. Note that we follow MeshGraphormer [30] to
use the test time augmentation during evaluation. We report
the Mean Per Joint Position Error (MPJPE) and the Mean
Per Vertex Position Error (MPVPE) after rigid alignment in
Tab.3, it can be seen that our method can achieve compara-
ble results with state-of-the-art (SOTA) methods for single-
hypothesis reconstruction (n=1). We then demonstrate qual-MPJPE MPVPE
Ours ( wGCN) 5.99 5.97
Only SA ( w/oGCN) 6.39 6.35
Table 2. Ablation study of using GCN module.
itative comparison results on the Freihand dataset in Fig.4,
our method performs better on side viewpoint situations and
occlusions situations. We believe that this is due to the fact
that our model is a prior model, which can better recover
hand meshes from incomplete observations. We also con-
ducted ablation study on the usage of GCN module. We
modify our model by replacing all GCNs with self-attention
layers (named as “Only SA”) and report results in Tab.2,
where our method is clearly better.
Multi-Hypothesis. By denoising different noises sampled
from Gaussian distribution, our method is capable of pro-
viding multiple mesh reconstruction hypotheses from a sin-
gle image. We first qualitatively demonstrate the multi-
hypothesis results in Fig.5, showing that our method can
maintain alignment with the input image in visible areas
while offering multiple potential guesses for the occluded
regions. We then quantitatively analyze the effect of the hy-
pothesis count in Tab.4. We also evaluate the effect of infer-
ence steps. Following human body multi-hypothesis meth-
ods [6, 25], we report the minimum MPJPE and MPVPE.
As shown in Tab.4 and Tab.3, when only 8 samples are
drawn, our method already outperforms the SOTAs [4, 30].
It is also indicated in Tab.4 that satisfactory results can be
achieved with just T= 10 steps of denoising.
651
MPJPE MPVPE
FreiHAND [63] 11.0 10.9
YotubeHand [26] 8.4 8.6
I2L-MeshNet [33] 7.4 7.6
HIU-DMTL [57] 7.1 7.3
CMR [3] 16.9 7.0
I2UV-HandNet [2] 6.7 6.9
METRO [31] 6.7 6.8
Tang et al. [45] 6.7 6.7
MobRecon [4] 6.1 6.2
MeshGraphormer [30] 5.9 6.0
Ours (n=1) 6.0 6.0
Ours (n=8) 5.9 5.9
Ours (n=16) 5.8 5.8
Table 3. Quantitative results on the Freihand dataset. Our method
applied T= 10 steps denoising by DDIM [42] and nrepresents
the number of samples.
Figure 5. Results for multi-hypothesis reconstruction from monoc-
ular image.
MPJPE MPVPE
T=10 n=8 5.91 5.90
n=16 5.76 5.76
n=32 5.63 5.64
T=25 n=8 5.79 5.79
n=16 5.58 5.59
n=32 5.41 5.43
T=50 n=8 5.73 5.73
n=16 5.51 5.52
n=32 5.33 5.35
Table 4. Effect of denoising steps and sampling quantity. Trep-
resents the number of denoising steps by DDIM. nrepresents the
number of samples.
5.3.2 Hand Mesh Fitting
Our method can also take a 2D skeleton as a condition
cskel 2d, and can also utilize the gradient guidance approach
to further enhance alignment. We set the operator Pin
Equ.4 as P=MΠJ, where Jis a pre-trained skeleton
regression matrix, Πis the projection operator, Mis a per-MPVPE MPJPE
Fitting 2D skeleton 7.00 7.27
cskel 2D+▽skel 2D 6.28 6.26
cimage 6.54 6.57
cimage +cskel 2D 5.32 5.29
cimage +cskel 2D+▽skel 2D 5.05 4.94
Table 5. Quantitative results for hand mesh fitting, cimage rep-
resents input RGB image, cskel2Drepresents input 2D skeleton,
▽skel2Drepresents applying the gradient guidance on 2D skeleton
joint mask that masks out 2D joints with low detection con-
fidence. We employ the ground truth 2D skeleton as super-
vision for quantitative evaluation under various inputs.
We firstly compare with the traditional 2D skeleton fit-
ting algorithm by utilizing the hand PCA prior [38]. We
utilize 2D skeleton cskel 2Das input conditional and also
apply the gradient guidance ▽skel 2D. As demonstrated by
the second and third rows in Tab.5, our method achieved
higher fitting accuracy. This is because our method is a 3D
prior model that is capable of learning the relationship from
2D skeleton to 3D hand mesh.
We also introduce the 2D skeleton information cskel 2D
into the hand mesh reconstruction task with image input
cimage to further improve the accuracy by 2D fitting. Quan-
titative results in Tab.5 demonstrate that our fitting method
can further improve the alignment with the input image
compared to the reconstruction results. please refer to the
Supplementary Materials for more qualitative results.
6. Discussion
Conclusion. We introduce a graph diffusion based gener-
ative network that is compatible with various downstream
hand mesh recovery tasks, including hand mesh generation,
hand mesh inpainting, hand mesh reconstruction and hand
mesh fitting. Our network can take different task-specific
conditions as input, and directly denoise a 3D hand mesh
from randomly sampled noisy point clouds. We also de-
signed a conditional mask training strategy to address miss-
ing and noisy conditional inputs, and further employed a
gradient guidance method for enhancing the consistency be-
tween the generation output and the input conditions.
Limitation. Although our method uses a mask training
strategy to work with noisy and incomplete conditional in-
puts, it may still fail for extremely missing or excessively
noisy input conditions. Additionally, while our method only
requires T= 10 denoising steps to achieve decent results,
increasing the denoising steps for more precise generation
is time-consuming.
Acknowledgement : This paper is supported by Na-
tional Key R&D Program of China (2022YFF0902200),
the NSFC project No.62125107, the Beijing Mu-
nicipal Science & Technology Z231100005923030.
652
References
[1] Adnane Boukhayma, Rodrigo de Bem, and Philip HS Torr.
3d hand shape and pose from images in the wild. In CVPR ,
2019. 2, 3
[2] Ping Chen, Yujin Chen, Dong Yang, Fangyin Wu, Qin Li,
Qingpei Xia, and Yong Tan. I2uv-handnet: Image-to-uv pre-
diction network for accurate and high-fidelity 3d hand mesh
modeling. In ICCV , 2021. 3, 8
[3] Xingyu Chen, Yufeng Liu, Chongyang Ma, Jianlong Chang,
Huayan Wang, Tian Chen, Xiaoyan Guo, Pengfei Wan, and
Wen Zheng. Camera-space hand mesh recovery via semantic
aggregation and adaptive 2d-1d registration. In CVPR , 2021.
2, 3, 6, 8
[4] Xingyu Chen, Yufeng Liu, Yajiao Dong, Xiong Zhang,
Chongyang Ma, Yanmin Xiong, Yuan Zhang, and Xiaoyan
Guo. Mobrecon: Mobile-friendly hand mesh reconstruction
from monocular image. In CVPR , 2022. 2, 3, 6, 7, 8
[5] Yujin Chen, Zhigang Tu, Di Kang, Linchao Bao, Ying
Zhang, Xuefei Zhe, Ruizhi Chen, and Junsong Yuan. Model-
based 3d hand reconstruction via self-supervised learning. In
CVPR , 2021. 2, 3
[6] Hai Ci, Mingdong Wu, Wentao Zhu, Xiaoxuan Ma, Hao
Dong, Fangwei Zhong, and Yizhou Wang. Gfpose: Learning
3d human pose prior with gradient fields. In CVPR , 2023. 3,
7
[7] Micha ¨el Defferrard, Xavier Bresson, and Pierre Van-
dergheynst. Convolutional neural networks on graphs with
fast localized spectral filtering. In NeurIPS , 2016. 4
[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR , 2009. 6
[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. In NeurIPS , 2021. 4, 5
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In ICLR , 2021. 4
[11] William Feller. Retracted chapter: On the theory of stochas-
tic processes, with particular reference to applications. In
Selected Papers I , pages 769–798. Springer, 2015. 4
[12] Runyang Feng, Yixing Gao, Tze Ho Elden Tse, Xueqing Ma,
and Hyung Jin Chang. Diffpose: Spatiotemporal diffusion
model for video-based human pose estimation. In ICCV ,
2023. 3
[13] Chengying Gao, Yujia Yang, and Wensheng Li. 3d interact-
ing hand pose and shape estimation from a single rgb image.
Neurocomputing , 2022. 2, 3
[14] Liuhao Ge, Zhou Ren, Yuncheng Li, Zehao Xue, Yingying
Wang, Jianfei Cai, and Junsong Yuan. 3D hand shape and
pose estimation from a single RGB image. In CVPR , 2019.
2, 3[15] Jia Gong, Lin Geng Foo, Zhipeng Fan, Qiuhong Ke, Hossein
Rahmani, and Jun Liu. Diffpose: Toward more reliable 3d
pose estimation. In CVPR , 2023. 3
[16] Shreyas Hampali, Mahdi Rad, Markus Oberweger, and Vin-
cent Lepetit. Honnotate: A method for 3d annotation of hand
and object poses. In CVPR , 2020. 6
[17] Shreyas Hampali, Sayan Deb Sarkar, Mahdi Rad, and Vin-
cent Lepetit. Keypoint transformer: Solving joint identifica-
tion in challenging hands and object interactions for accurate
3d pose estimation. In CVPR , 2022. 6
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
2016. 6
[19] Dan Hendrycks and Kevin Gimpel. Gaussian error linear
units (gelus). arXiv preprint arXiv:1606.08415 , 2016. 6
[20] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya
Sutskever, and Ruslan R Salakhutdinov. Improving neural
networks by preventing co-adaptation of feature detectors.
arXiv preprint arXiv:1207.0580 , 2012. 6
[21] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. In NeurIPS 2021 Workshop , 2021. 4
[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In NeurIPS , 2020. 3, 4
[23] Karl Holmquist and Bastian Wandt. Diffpose: Multi-
hypothesis human pose estimation using diffusion models.
InICCV , 2023. 3
[24] Ludovic Hoyet, Kenneth Ryall, Rachel McDonnell, and
Carol O’Sullivan. Sleight of hand: perception of finger mo-
tion from reduced marker sets. In SIGGRAPH , 2012. 2, 3
[25] Nikos Kolotouros, Georgios Pavlakos, Dinesh Jayaraman,
and Kostas Daniilidis. Probabilistic modeling for human
mesh recovery. In ICCV , 2021. 7
[26] Dominik Kulon, Riza Alp Guler, Iasonas Kokkinos,
Michael M Bronstein, and Stefanos Zafeiriou. Weakly-
supervised mesh-convolutional hand reconstruction in the
wild. In CVPR , 2020. 2, 3, 8
[27] Jiaman Li, Ruben Villegas, Duygu Ceylan, Jimei Yang,
Zhengfei Kuang, Hao Li, and Yajie Zhao. Task-generic hi-
erarchical human motion prior using vaes. In 3DV, 2021. 2,
3
[28] Mengcheng Li, Liang An, Hongwen Zhang, Lianpeng Wu,
Feng Chen, Tao Yu, and Yebin Liu. Interacting attention
graph for single image two-hand reconstruction. In CVPR ,
2022. 3
[29] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,
Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution
text-to-3d content creation. In CVPR , 2023. 3
[30] Kevin Lin, Lijuan Wang, and Zicheng Liu. Mesh
graphormer. In ICCV , 2021. 2, 3, 7, 8
[31] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end hu-
man pose and mesh reconstruction with transformers. In
CVPR , 2021. 2, 3, 8
[32] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 , 2017. 6
[33] Gyeongsik Moon and Kyoung Mu Lee. I2l-meshnet: Image-
to-lixel prediction network for accurate 3d human pose and
653
mesh estimation from a single rgb image. In ECCV , 2020. 3,
8
[34] Gyeongsik Moon, Shoou-I Yu, He Wen, Takaaki Shiratori,
and Kyoung Mu Lee. Interhand2.6m: A dataset and baseline
for 3D interacting hand pose estimation from a single rgb
image. In ECCV , 2020. 6
[35] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,
Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and
Michael J Black. Expressive body capture: 3d hands, face,
and body from a single image. In CVPR , 2019. 3
[36] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR ,
2022. 3
[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , 2022. 3, 4
[38] Javier Romero, Dimitrios Tzionas, and Michael J. Black.
Embodied hands: Modeling and capturing hands and bod-
ies together. In SIGGRAPH Asia , 2017. 2, 3, 6, 8
[39] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In CVPR , 2023. 3
[40] Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit H
Bermano. Human motion diffusion as a generative prior.
arXiv preprint arXiv:2303.01418 , 2023. 3, 4
[41] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In ICML . PMLR, 2015.
3
[42] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In ICLR , 2020. 3, 4, 6, 7,
8
[43] Adrian Spurr, Jie Song, Seonwook Park, and Otmar Hilliges.
Cross-modal deep variational hand pose estimation. In
CVPR , 2018. 2, 3
[44] Adrian Spurr, Umar Iqbal, Pavlo Molchanov, Otmar Hilliges,
and Jan Kautz. Weakly supervised 3d hand pose estimation
via biomechanical constraints. In ECCV , 2020. 2, 3
[45] Xiao Tang, Tianyu Wang, and Chi-Wing Fu. Towards accu-
rate alignment in real-time 3D hand-mesh reconstruction. In
ICCV , 2021. 2, 3, 8
[46] Jonathan Taylor, Vladimir Tankovich, Danhang Tang, Cem
Keskin, David Kim, Philip Davidson, Adarsh Kowdle, and
Shahram Izadi. Articulated distance fields for ultra-fast
tracking of hands interacting. TOG , 2017. 2, 3
[47] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel
Cohen-or, and Amit Haim Bermano. Human motion diffu-
sion model. In ICLR , 2023. 3, 4
[48] Garvita Tiwari, Dimitrije Anti ´c, Jan Eric Lenssen, Nikolaos
Sarafianos, Tony Tung, and Gerard Pons-Moll. Pose-ndf:
Modeling human pose manifolds with neural distance fields.
InECCV , 2022. 2, 3, 6
[49] Chengde Wan, Thomas Probst, Luc Van Gool, and Angela
Yao. Crossing nets: Combining gans and vaes with a shared
latent space for hand pose estimation. In CVPR , 2017. 2, 3[50] Jiayi Wang, Franziska Mueller, Florian Bernard, Suzanne
Sorli, Oleksandr Sotnychenko, Neng Qian, Miguel A
Otaduy, Dan Casas, and Christian Theobalt. Rgb2hands:
real-time tracking of 3d hand interactions from monocular
rgb video. TOG , 2020. 2, 3
[51] Linlin Yang and Angela Yao. Disentangling latent hands for
image synthesis and pose estimation. In CVPR , 2019. 2, 3
[52] Linlin Yang, Shile Li, Dongheui Lee, and Angela Yao.
Aligning latent spaces for 3d hand pose estimation. In ICCV ,
2019. 2, 3
[53] Lixin Yang, Xinyu Zhan, Kailin Li, Wenqiang Xu, Jiefeng
Li, and Cewu Lu. Cpf: Learning a contact potential field to
model the hand-object interaction. In ICCV , 2021. 2, 3
[54] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng
Li, Liang An, Zhenan Sun, and Yebin Liu. Pymaf-x: To-
wards well-aligned full-body model regression from monoc-
ular images. TPAMI , 2023. 3
[55] Jiawei Zhang, Jianbo Jiao, Mingliang Chen, Liangqiong Qu,
Xiaobin Xu, and Qingxiong Yang. A hand pose tracking
benchmark from stereo matching. In ICIP . IEEE, 2017. 6
[56] Xiong Zhang, Qiang Li, Hong Mo, Wenbo Zhang, and Wen
Zheng. End-to-end hand mesh recovery from a monocular
rgb image. In ICCV , 2019. 2, 3
[57] Xiong Zhang, Hongsheng Huang, Jianchao Tan, Hongmin
Xu, Cheng Yang, Guozhu Peng, Lei Wang, and Ji Liu. Hand
image understanding via deep multi-task learning. In ICCV ,
2021. 8
[58] Yuxiang Zhang, Zhe Li, Liang An, Mengcheng Li, Tao Yu,
and Yebin Liu. Light-weight multi-person total capture using
sparse multi-view cameras. In ICCV , 2021. 3
[59] Yuxiang Zhang, Hongwen Zhang, Liangxiao Hu, Jiajun
Zhang, Hongwei Yi, Shengping Zhang, and Yebin Liu. Prox-
ycap: Real-time monocular full-body capture in world space
via human-centric proxy-to-motion learning, 2023. 3
[60] Wenping Zhao, Jinxiang Chai, and Ying-Qing Xu. Com-
bining marker-based mocap and rgb-d camera for acquiring
high-fidelity hand motion data. In SIGGRAPH , 2012. 2, 3
[61] Yuxiao Zhou, Marc Habermann, Weipeng Xu, Ikhsanul
Habibie, Christian Theobalt, and Feng Xu. Monocular real-
time hand shape and motion capture using multi-modal data.
InCVPR , 2020. 2, 3
[62] Christian Zimmermann and Thomas Brox. Learning to esti-
mate 3d hand pose from single rgb images. In ICCV , 2017.
6
[63] Christian Zimmermann, Duygu Ceylan, Jimei Yang, Bryan
Russell, Max J. Argus, and Thomas Brox. Freihand: A
dataset for markerless capture of hand pose and shape from
single rgb images. In ICCV , 2019. 6, 7, 8
[64] Binghui Zuo, Zimeng Zhao, Wenqian Sun, Wei Xie, Zhou
Xue, and Yangang Wang. Reconstructing interacting hands
with interaction prior from monocular images. In ICCV ,
2023. 2, 3, 6
654
