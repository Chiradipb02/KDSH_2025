D4M: Dataset Distillation via Disentangled Diffusion Model
Duo Su1,5,6,‚Ä†Junjie Hou2,5,6,‚Ä†Weizhi Gao3Yingjie Tian4,5,6,7,‚àóBowen Tang8
1School of Computer Science and Technology, UCAS2Sino-Danish College, UCAS
3Department of Computer Science, NCSU4School of Economics and Management, UCAS
5Research Center on Fictitious Economy and Data Science, CAS
6Key Laboratory of Big Data Mining and Knowledge Management, CAS
7MOE Social Science Laboratory of Digital Economic Forecasts and Policy Simulation, UCAS
8Institute of Computing Technology, CAS
https://junjie31.github.io/D4M/
Abstract
Dataset distillation offers a lightweight synthetic dataset
for fast network training with promising test accuracy. To
imitate the performance of the original dataset, most ap-
proaches employ bi-level optimization and the distillation
space relies on the matching architecture. Nevertheless,
these approaches either suffer significant computational
costs on large-scale datasets or experience performance
decline on cross-architectures. We advocate for design-
ing an economical dataset distillation framework that is
independent of the matching architectures. With empiri-
cal observations, we argue that constraining the consis-
tency of the real and synthetic image spaces will enhance
the cross-architecture generalization. Motivated by this,
we introduce Dataset Distillation via Disentangled Diffu-
sion Model (D4M), an efficient framework for dataset dis-
tillation. Compared to architecture-dependent methods,
D4M employs latent diffusion model to guarantee consis-
tency and incorporates label information into category pro-
totypes. The distilled datasets are versatile, eliminating the
need for repeated generation of distinct datasets for various
architectures. Through comprehensive experiments, D4M
demonstrates superior performance and robust generaliza-
tion, surpassing the SOTA methods across most aspects.
1. Introduction
The rapid growth in machine learning, resulting in large
models and vast datasets, poses a challenge to researchers
due to the escalating computational and storage demands.
Can the ‚ÄôDivide-and-Conquer‚Äô algorithm [1] mitigate this
‚Ä†Equal contribution. ‚àóCorresponding author.
(a) Synthesis-Time Matching
PredictionSoft LabelD!M(b) Dual-Time Matching (c) Training-Time MatchingModelModel
Real Image
GeneralSynthetic Image
Architecture-Specific Synthetic Image 
Synthesis-Time
PredictionSoft LabelModelModelPredictionPredictionModelModel
Training-TimeSynthesis-TimeMatching Process
Model
Model
Label Text&Figure 1. Comparison of various matching strategies in dataset
distillation. (a) The bi-level optimization implements data match-
ing at synthesis time. (b) Dual-Time Matching strategy decouples
the bi-level optimization process into synthesis time and training
time to save computational overhead. (c) D4M utilizes multi-
modal features (image and texts) to synthesize high-quality im-
ages. D4M does not require matching process at Synthesis-Time.
challenge? From the perspective of dataset, recent research
extends the coreset selection [3, 7, 39] to distillation tech-
niques aimed at reducing dataset scales. Dataset Distilla-
tion (DD) aims to synthesize a small dataset Sfrom the
original large-scale dataset T, where |S| ‚â™ |T | . The infor-
mation in Tis condensed into a small dataset through DD.
Initially, the DD framework uses the bi-level optimization
to generate datasets where the inner loop updates the net-
work used for testing the classification performance and the
outer loop synthesizes images according to matching strate-
gies, such as gradient [24, 51, 53], distribution [40, 52] or
trajectory [4, 8].
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
5809
Unfortunately, the existing solutions of DD mainly fo-
cus on small and simple datasets, such as CIFAR [21] and
MNIST [23, 44]. When it comes to large-scale and high-
resolution datasets such as ImageNet [9], there exists un-
affordable computational requirements and reduced perfor-
mance. Another challenge in DD is the cross-architecture
generalization. Previous methods conduct data matching
within a fixed discriminative architecture, which makes the
output space biased from the original image space. As
demonstrated in Fig. 2, this kind of dataset may be insight-
ful for the networks but suffers from the lack of semantic
information for humankind. Furthermore, the dataset has
to be distilled from scratch again and again to adapt to the
emerging network architectures. Obviously, these limita-
tions constrain the scientific value and practical utility of
the current solutions. In this paper, we argue that an ideal
DD method should meet the following properties.
Guess What These Are?
Figure 2. Visualizations of previous DD methods. Synthesis-Time
Matching sacrifices part of the visual semantic expression in order
to imitate the performance of the original dataset.
1) The synthesis process should not depend on a spe-
cific network architecture. Typically, a fixed architecture
is required for data matching, which leads to low cross-
architecture generalization performance because the output
space is constrained by the architecture. This problem arises
once the matching process occurs in the synthesis time as
shown in Fig. 1(a) and (b). Some work leverages a model
pool instead of an individual matching model to alleviate
this issue but makes the network hard to optimize [41, 54].
When the distillation process is architecture-free, there is
no need to distill datasets for different architectures repeat-
edly. In addition, constraining the consistency of input and
output spaces will make the distilled images more realistic.
GlaD [5] seems to be a solution where the images are syn-
thesized via Generative Adversarial Networks. However,
the synthetic images are still matched by the inner loop.
2) The method is capable of distilling datasets of various
sizes and resolutions with limited computational resources.
As illustrated in Fig. 1(a), most DD solutions use bi-level
optimization during synthesis time. While the large-scale
datasets are unable to perform a number of unrolled it-
erations on such a nested loop system. Some works at-
tempt to distill the ImageNet-1K but yield low testing accu-
racy [4, 8]. A more effective method is depicted in Fig. 1(b):
the bi-level optimization is decoupled into synthesis time
and training time [48]. However, the Dual-Time Matching(DTM) strategy leads to information loss at each stage, pos-
ing challenges for distillation on small datasets instead.
Inspired by these insights, we propose the Dataset
Distillation via Disentangled Diffusion Model ( D4M), an
efficient approach designed for DD across varying sizes and
resolutions as depicted in Fig. 1(c). In D4M, the Synthesis-
Time Matching (STM) is superseded by Training-Time
Matching (TTM) which facilitates the fast distillation of
large-scale datasets with constrained computational re-
sources. Furthermore, D4M alleviates the architectural de-
pendency and improves the cross-architecture generaliza-
tion performance of the distilled dataset. As the generative
model, Diffusion Models ensure the consistency between
input and output spaces, and its synthesis process does not
rely on any specific matching architecture. To mitigate
the information loss due to insufficient data matching, the
conditioning mechanism in Latent Diffusion Model (LDM)
consistently infuses the semantic information of labels into
the synthetic data during the denoising process. The syn-
thesis process of D4M solely depends on the prototypes ex-
tracted from the original data, with synthesis speed scaling
linearly with the size of datasets. Moreover, the synthetic
images exhibit realism at a high resolution of 512√ó512.
Our pivotal contributions are summarized as follows:
‚Ä¢ To the best of our knowledge, this is the first work that
overcomes the pronounced dependency on specific archi-
tectures inherent in traditional DD frameworks. We in-
troduce the TTM strategy, which paves the way for the
generation of a curated and versatile distilled dataset.
‚Ä¢ We propose D4M that integrates the diffusion model into
DD task for the first time. By leveraging label texts and
the learned prototypes, we construct a multi-modal DD
model that simultaneously enhances distillation efficiency
and model performance.
‚Ä¢ The method realizes the attainment of resolutions up to
512√ó512 that exhibit high-fidelity and robust adaptability
in the realm of DD. This improvement is evidenced across
a spectrum of datasets, extending from the ImageNet-1K
to CIFAR-10/100.
‚Ä¢ We conduct extensive experiments and ablation studies.
The results outperform the SOTA in most cases, sub-
stantiating the superior performance, computational effi-
ciency, and robustness of our method.
2. Related Work
2.1. Dataset Distillation
The existing DD approaches are taxonomized into meta-
learning matching and data matching frameworks [13, 25,
34, 49]. The meta-learning matching aims to optimize the
meta-test loss on real dataset for the model meta-trained by
the distilled dataset. The gradients are back-propagated to
supervise the DD directly [10, 27, 31, 32, 42, 54].
5810
Label
TextsTench,
Goldfish,
‚Ä¶
Tissue.
Diffusion 
ProcessDenoising 
Process
‚Ä¶‚Ä¶‚Ä¶
Number of  
Prototypes‚Ä¶
Number 
of  
Classes‚Ä¶
‚Ä¶ùìîùìî
ùúèùúèùúΩùúΩùììùìì
Synthetic Images‚Ä¶‚Ä¶‚Ä¶
‚Ä¶
‚Ä¶
ùìîùìîEncoder ùììùììDecoder ùúèùúèùúΩùúΩText EncoderImage 
FeaturePrototypePrototype with Noise
Clustering
Real ImagesNumber 
of  
ClassesFigure 3. Pipeline of Dataset Distillation via Disentangled Diffusion Model (D4M). Rather than using the embedded features directly,
D4M disentangles feature extraction from image generation in diffusion models through prototype learning.
Unlike optimizing the performance on the DD explicitly,
data matching encourages the consistency between the same
network architecture trained by distilled and real dataset.
Matching the gradients generated by the networks is a reli-
able surrogate task [18, 24, 51, 53]. Matching Training Tra-
jectory (MTT) [4, 11] is then proposed to solve the issue that
errors are accumulated during validation in gradient match-
ing. TESLA [8] reduced the complexity of gradients calcu-
lating with constant memory, allowing DD to be achieved in
ImageNet for the first time. Besides, distribution matching
optimizes the distance between the two distributions, such
as MMD [52] and CAFE [40].
The aforementioned methods only implement various
matching strategies at synthesis time. SRe2L [48] argues
that decoupling the bi-level optimization into Squeeze, Re-
cover, and Relabel leads to a good performance on large-
scale datasets. Inspired by this, we summarize previous
works into STM and DTM. D4M implements the TTM with
the help of soft labels, which is considered a feature distri-
bution matching approach.
2.2. Diffusion Models
The Diffusion Model has demonstrated remarkable capa-
bilities within the generative models. Given samples x
observed from a target distribution, the goal of generative
models is approximating the true distribution P(x), en-
abling the generation of novel samples from it. Denois-
ing Diffusion Probabilistic Models (DDPM) [16] aims to
learn a reverse process of a fixed Markov Chain for gen-
erating images. However, DDPM is expensive to optimize
and evaluate in the original pixel space.
Latent Diffusion Model (LDM) [33], a recent state-of-
the-art diffusion model, addresses this by abstracting high-
frequency, imperceptible details into a compact latent space,thereby streamlining both training and inference. LDM
has been applied in image editing [38, 43], video process-
ing [2, 12], audio generation [17, 36] and 3D model recon-
struction [6, 19, 20, 29]. Notably, the proficiency of LDM
in abstracting and generating images within the latent space
exactly resonates with the foundational tenets of DD.
3. Method
3.1. Preliminaries on Diffusion Models
A pivotal step in DD is the generation of the distilled im-
ages. Distinct from the data-matching approaches, our
method harnesses the prior knowledge embedded in the pre-
trained generative models, offering a high-quality initializa-
tion for TTM. Recently, diffusion models have emerged as
SOTA in generative models [28, 46]. As aforementioned,
the synthesis process of the diffusion model does not rely
on any specific matching architecture, ensuring the consis-
tency between input and output spaces. For a sequence of
denoising autoencoders œµŒ∏, the training objective of Denois-
ing Diffusion Probability Model (DDPM) [16] is defined as
LDM=Ex,œµ‚àºN(0,1),th
‚à•œµ‚àíœµŒ∏(xt, t)‚à•2
2i
, (1)
with the timestamp tuniformly sampled from {1, . . . , T }.
Although the DDPM does not cater to our goal of synthe-
sizing images within the condensed features, we turn our
attention to the LDM [33].
LDM effectively compresses the working space from the
original pixel space xto a more compact latent space z.
Such a transition is close to our intent of encapsulating im-
ages into condensed features. LDM constructs an optimized
low-dimensional latent space by training a perceptual com-
pression model composed of the encoder ( E) and decoder
(D). This latent space effectively abstracts high-frequency
5811
imperceptible details than pixel space [33]. In this case, the
objective function with text encoder œÑŒ∏is redefined as
LLDM =EE(x),y,œµ‚àºN(0,1),th
‚à•œµ‚àíœµŒ∏(zt, t, œÑŒ∏(y))‚à•2
2i
.
(2)
3.2. Disentangled Diffusion Model
The existing diffusion methods are capable of generating
high-quality images directly from the given images and
prompts. However, it is imperative for the DD model to
aggregate the given images into a few condensed features
before synthesis. The images in the original dataset en-
capsulate a spectrum of information from low-level texture
patterns to high-level semantic information, along with po-
tential redundancies. Since the diffusion models do not
have the capability of aggregating this information among
images, it is necessary to extract the salient feature repre-
sentative of each category before employing the generative
model. Consequently, it is essential to disentangle the dif-
fusion models.
Employing prototypes in standard classification tasks
offers the benefit of addressing the open-world recogni-
tion challenge, thereby enhancing the robustness of mod-
els [26, 45, 50]. Therefore, initializing the input of the dif-
fusion model with prototypes not only reduces data redun-
dancy but also elevates the quality of the distilled dataset.
As illustrated in Fig. 3, we leverage the pre-trained autoen-
coderEinherent in the LDM to extract feature represen-
tations from original images. Subsequently, we perform a
clustering algorithm to calculate the cluster centers as pro-
totypes for each category. Given the considerable size of
the original dataset, we adopt the Mini-Batch k-Means [35]
to mitigate the memory overhead of large-scale clustering.
This approach iteratively optimizes a mini-batch of samples
in each step, accelerating the clustering process with a min-
imal compromise in accuracy.
Specifically, the clustering algorithm consists of two pri-
mary steps: assignment z
zc‚Üêz (3)
s.t.arg min
c‚à•z‚àízc‚à•2, c= 1, . . . , C (4)
and update zc
zc‚Üê(1‚àíŒ∑)zc+Œ∑z. (5)
Here zis the latent variable generated by E, and zcrepre-
sents the cluster centers (prototypes), Cis the number of
cluster centers. The learning rate Œ∑is often calculated by
1
|zc|. Ultimately, we employ the prototypes ¬ØZ={zc
l|c=
1, . . . , C, l = 1, . . . , L }from all categories as input to the
diffusion process for image synthesis.Algorithm 1 D ataset Distillation via Disentangled
Diffusion Model ( D4M)
Input: (T,L): Real images and their label texts.
Input: E: Pre-trained encoder.
Input: D: Pre-trained decoder.
Input: œÑŒ∏: Pre-trained text encoder.
Input: Ut: Pre-trained time-conditional U-Net.
Input: C: Number of prototypes.
1:Z=E(T)‚àºPz ‚ñ∑Compressed latent space
2:for each L‚àà Ldo
3: formini-batch z‚ààLdo
4: zc‚àºPz, c= 1, . . . , C
‚ñ∑Initialize cluster centers
5: zc‚Üêz,s.t.arg min
c‚à•z‚àízc‚à•2‚ñ∑Assignment
6: Œ∑=1
|zc|‚ñ∑Update learning rate
7: zc‚Üê(1‚àíŒ∑)zc+Œ∑z ‚ñ∑Update
8: end for
9: y=œÑŒ∏(L) ‚ñ∑Label text embedding
10: for each zcdo
11: zc
t‚àºq(zc
t|zc) ‚ñ∑Diffusion process
12: Àúzc=Ut(Concat (zc
t, y))‚ñ∑Denoising process
13: end for
14:end for
15:S=D(ÀúZc) ‚ñ∑Generate image
Output: S: Distilled images.
Moreover, LDM is capable of modeling the conditional
distribution, enabling DD tasks to incorporate the label in-
formation into synthetic images. In Eq. (2), LDM intro-
duces a domain-specific encoder œÑŒ∏to map the textual labels
(prompts) into the feature space. This mapping is seam-
lessly integrated into the U-Net architecture ( Ut) through a
cross-attention layer, facilitating the fusion of multi-modal
features. For each prototype zcand its corresponding label
L, the synthesis process is formulated as
output =D(Ut(Concat (zc
t, œÑŒ∏(L))) (6)
where zc
trepresents the c-th prototype with noise. The dis-
tillation process is summarized in Algorithm 1.
3.3. Training-Time Matching
Since eliminates the necessity of matching with a specific
architecture, separating data matching from the synthesis
process reduces the computational overhead on large-scale
datasets and addresses the cross-architecture issue inher-
ent in the STM strategy. However, based on previous re-
search [4, 8, 48] and preliminary experiments, we find that
training large-scale distilled datasets with hard labels is
prone to low testing accuracy. To address this, we intro-
duce the TTM strategy, which is considered a distribution
matching approach.
5812
ImageNet-1K
Tiny-ImageNet
Goldfinch Turtle Carrier Pinwheel Wok Hourglass Trolleybus Pizza Mushroom BeaconCIFAR-10
CIFAR-100
Airplane Bird Cat Deer Truck Otter Peppers Tiger Porcupine Bicycle
Figure 4. Visualization results. The top row of each dataset comes from D4M and the bottom comes from SRe2L [48] (ImageNet-1K and
Tiny-ImageNet) and MTT [4] (CIFAR-10/100). The images generated by D4M have better resolution and are more lifelike.
Figure 5. Visualization results within one category. D4M (top)
provides richer semantic information than SRe2L.
TTM refers to training on distilled datasets with soft
labels. Label softening is widely adapted in distillation
tasks [15, 30, 47]. Since D4M infuses the label features
into the synthetic data, it is natural to use the soft label dur-
ing TTM. We employ soft label to align the distribution of
student prediction SŒ∏(x)with teacher network T:
Œ∏student = arg min
Œ∏‚ààŒòLKL(T(x), SŒ∏(x)) (7)
where T(x)/SŒ∏(x)is the teacher/student prediction for the
distilled image xandLKLrepresents the KL divergence.
The output of the teacher network, also known as soft pre-
diction or soft label, encapsulates richer semantic informa-
tion compared to hard labels. Matching with the soft labels
during training will enhance the robustness and generaliza-
tion capability of the trained model [15]. For a fair com-
parison, we use the soft label storage method similar to the
FKD [37] method, which generates soft labels and conducts
matching at each training epoch:
Œ∏t+1
student = arg min
Œ∏‚ààŒòLKL(Tt(x), St
Œ∏(x)). (8)4. Experiments
4.1. Setting and Evaluation
We evaluate the performance of D4M across various
datasets and networks. All models employed for ImageNet-
1K and Tiny-ImageNet are sourced from the PyTorch of-
ficial model repository, while the ConvNet utilized for
CIFAR-10/100 is based on the architecture proposed by Gi-
daris et al . [14]. Performance validation was carried out
using PyTorch on NVIDIA V100 GPUs. Detailed training
and validation hyperparameters are available in the supple-
mentary material.
4.2. Dataset Distillation Results
In our comparative analysis, we evaluate the D4M against
a range of techniques, encompassing both meta-learning
and data-matching strategies. For small datasets, our com-
parison included two meta-learning methods: KIP [32]
and FRePO [54], alongside four data-matching techniques:
DSA [51], CAFE [40], TESLA [8], and SRe2L [48]. In
the context of large-scale datasets, our focus shifted to a de-
tailed comparison between TESLA and SRe2L.
CIFAR-10 and CIFAR-100 For small dataset distilla-
tion, the STM strategy outperforms when the number of
categories and IPC (Image Per Class) are limited. How-
ever, as the category increases, the TTM strategy becomes
more effective. This shift is attributed to the fact that the op-
timal solution derived from STM fails to ensure the conver-
gence of the network training with large category numbers,
thereby capping the testing performance. As evidenced in
5813
Dataset IPCMeta-Learning Data-MatchingFull DatasetKIP FRePO DSA CAFE TESLA SRe2L‚Ä†D4M
CIFAR-1010 62.7 ¬±0.3 65.5 ¬±0.6 52.1 ¬±0.5 50.9 ¬±0.5 66.4¬±0.8(60.2)56.2¬±0.4
50 68.6 ¬±0.2 71.7 ¬±0.2 60.6 ¬±0.5 62.3 ¬±0.4 72.6 ¬±0.7 72.8¬±0.584.8¬±0.1
CIFAR-10010 28.3 ¬±0.1 42.5 ¬±0.2 32.3 ¬±0.3 31.5 ¬±0.2 41.7 ¬±0.3 - 45.0¬±0.1
50 - 44.3 ¬±0.2 42.8 ¬±0.4 42.9 ¬±0.2 47.9 ¬±0.3 - 48.8¬±0.356.2¬±0.3
Table 1. Top-1 Accuracy ‚Üëon small datasets . We train the ConvNet-W128 [14] from scratch 5 times on the distilled dataset and evaluate
them on the original test dataset to get the ¬Øx¬±std.‚Ä†: SRe2L [48] achieves 60.2% Top-1 Accuracy on CIFAR-10 with IPC-1K.
Dataset IPC Method R18 R50 R101
ImageNet-1KFull Dataset‚Ä†69.8 80.9 81.9
10TESLA 7.7 - -
SRe2L 21.3 28.4 30.9
D4M 27.9 33.5 34.2
50SRe2L 46.8 55.6 60.8
D4M 55.2 62.4 63.4
100SRe2L 52.8 61.0 65.8
D4M 59.3 65.4 66.5
200SRe2L 57.0 64.6 65.9
D4M 62.6 67.8 68.1
Tiny-ImageNetFull Dataset‚Ä°61.9 62.0 62.3
50SRe2L 44.0 47.7 49.1
D4M 46.2 51.8 51.0
D4M-G 46.8 51.9 53.2
100SRe2L 50.8 53.5 54.2
D4M 51.4 54.8 55.3
D4M-G 53.3 54.9 54.5
Table 2. Top-1 Accuracy ‚Üëon large-scale datasets . SRe2L [48]
and our D4M employ ResNet18 as the teacher model to generate
the soft label while TESLA [8] uses the ConvNetD4. All standard
deviations in this table are <1.‚Ä†: The results of ImageNet-1K
come from the official PyTorch websites. ‚Ä°: The results of Tiny-
ImageNet come from the model trained from scratch with the of-
ficial PyTorch code.
Tab. 1, when applied to CIFAR-100, D4M attains a Top-1
accuracy of 45.0% with merely IPC-10. This performance
surpasses that of FRepo and TESLA by 2.5% and 3.3%.
ImageNet-1K and Tiny-ImageNet The TTM strategy
demonstrates remarkable efficacy in large-scale DD tasks as
presented in Tab. 2. The effectiveness stems from its abil-
ity to improve the quality of the synthetic data rather than
imitate the performance of the original data. Consequently,
it facilitates the processing of large-scale datasets with re-
duced computational complexity and memory demands. In
terms of accuracy, the proposed D4M sets new benchmarks,
achieving 66.5% and 51.0% with IPC-100 on ImageNet-1K
and Tiny-ImageNet. Notably, it replicates the full datasetAblation R18 R50 R101
Teacher: R18
w/ STM 23.6 29.7 32.3
w/o STM 27.9(+4.3) 33.5(+3.8) 34.2(+1.9)
Teacher: R50
w/ STM 15.8 20.6 22.3
w/o STM 20.7(+4.9) 24.7(+4.1) 26.7(+4.4)
Teacher: R101
w/ STM 12.5 16.0 17.6
w/o STM 19.4(+6.9) 23.0(+7.0) 24.2(+6.6)
Table 3. Comparison of Top-1 Accuracy ‚Üëon different match-
ing strategy . We use the R18 as the distribution matching archi-
tecture. All methods are evaluated with IPC-10.
performance with 81.2% and 81.9%, respectively. More-
over, our approach significantly surpasses the leading data-
matching method, SRe2L, across both datasets. This supe-
riority is attributed to the integration of multi-modal fusion
embedding in D4M.
Benefit to the architecture-free synthesis process, the
datasets distilled by D4M exhibit versatility. To substan-
tiate this characteristic, we extract 200 categories from the
distilled ImageNet as the distilled Tiny-ImageNet in accor-
dance with the predefined mapping [22]. The experimental
outcomes of D4M-G in Tab. 2 demonstrate that our method
not only manifests a pronounced distillation effect but also
retains the applicability inherent to the original dataset.
4.3. Matching Strategy Analysis
As mentioned in Sec. 2, the DD task often uses the STM
strategy to generate images. In order to validate the supe-
riority of TTM strategy, we conduct the comparative exper-
iments listed in Tab. 3. We execute the synthesis process
through BN distribution matching on images distilled via
D4M, resulting in distribution-matched synthetic images.
It is evident that the test performance with STM failed
regardless of the chosen teacher network. The images dis-
5814
10 50 100 200
IPC203040506070T op-1 Acc. (%)
T eacher: ResNet-18
ResNet-18
ResNet-50
ResNet-101
10 50 100 200
IPC203040506070T op-1 Acc. (%)
T eacher: ResNet-50
ResNet-18
ResNet-50
ResNet-101
10 50 100 200
IPC203040506070T op-1 Acc. (%)
T eacher: ResNet-101
ResNet-18
ResNet-50
ResNet-101Figure 6. Top-1 Accuracy ‚Üëof ImageNet-1K on various teacher-student pairs . The result of each pair increases consistently with larger
IPC
Ablation R18 R50 R101
Dataset: ImageNet-1K
w/o PT 15.6 20.7 20.6
w/ PT 27.9(+12.3) 33.5(+12.8) 34.2(+13.6)
Dataset: Tiny-ImageNet
w/o PT 30.5 35.6 37.3
w/ PT 46.2(+15.7) 51.8(+16.2) 51.0(+13.7)
Table 4. Comparison of Top-1 Accuracy ‚Üëon different initial-
ization of diffusion process . PT is the abbreviation of Prototype.
All methods are evaluated with IPC-10.
tilled via D4M encapsulate not only the salient features of
the original prototypes but also the text information of cat-
egory labels. Therefore, the network solely trained with the
original images proves inadequate for effectively managing
such fused multi-modal features. Should the fused features
be aligned with these networks, it would result in the dis-
ruption of the fused information, thereby diminishing the
overall accuracy. It is worth noting that D4M potentially
offers high-quality initialization for STM, as it synthesizes
images with higher testing accuracy compared to those de-
rived from random white noise initialization.
4.4. Prototype Analysis
To ascertain the critical role of prototypes in D4M, we con-
duct an ablation study on the diffusion process with random
initialization and prototype initialization. The results listed
in Tab. 4 demonstrate that the incorporation of a learned
prototype markedly enhances the effectiveness of D4M.
To showcase the merits of the prototype intuitively, we
employ ResNet-18 for feature extraction from the distilled
dataset, followed by t-SNE for dimensionality reduction.
The visualization results (Fig. 7) reveal that the data synthe-
sized via D4M demonstrates enhanced inter-class discrimi-
nation and intra-class consistency.
20
 0 2020
020MTT
40
 20
 0 20 4020
020SRe2L
Alligator Chain Seashore
20
 0 2040
20
020D4MFigure 7. T-SNE visualizations on Tiny-ImageNet . The fea-
ture embedding distribution of D4M displays more compact within
classes and discriminative among classes.
4.5. Teacher-Student Network Analysis
We studied the performance of different teacher-student
models with D4M and the experimental results are shown
in Fig. 6. Under the same teacher network, the accuracy
of ResNet-18, ResNet-50, and ResNet-101 increases grad-
ually. When IPC is small (such as 10 and 50), the student
network trained with an enhanced teacher is prone to over-
fitting, resulting in reduced testing accuracy. As IPC in-
creases, the large network shows stronger learning ability
and the Top-1 accuracy improves. We further compare the
performance of the distilled ImageNet on different teacher-
student pairs, including CNNs and ViTs (Tab. 5). As a
student network, the ViT-based networks assimilate the in-
ductive bias inherent in CNN-based teachers, leveraging its
global attention mechanism to attain the best Top-1 accu-
racy. Conversely, as a teacher network, ViT does not have
such an inductive bias characteristic, yielding suboptimal
results on their student networks. Nevertheless, ViT-based
students consistently achieve superior Top-1 accuracy.
4.6. Qualitative Analysis
A pivotal advantage of D4M lies in its utilization of the
outputs from the image decoder Das the distilled dataset,
avoiding the need for STM. This implies that the pixel space
of the generated image remains unaltered by any matching
optimization, thereby preserving the reality of the distilled
5815
Teacher NetworkStudent Network
ResNet-18 MobileNet-V2 EfficientNet-B0 Swin-T ViT-B
ResNet-18 55.2 47.9 55.4 58.1 45.5
MobileNet-V2 47.6 42.9 49.8 58.9 50.4
Swin-T 27.5 21.9 26.4 38.1 34.2
Table 5. Top-1 Accuracy ‚Üëon ImageNet-1K with various teacher-student architectures . ViT-based students show powerful learning
ability with IPC-50.
image. Figures 4 and 5 exemplify the superior image qual-
ity achieved by D4M in comparison to its counterparts. It is
evident that the D4M method not only guarantees the high
resolution of the distilled image and preserves the integrity
of semantic information but also ensures the richness of fea-
tures within the same category. More visualizations and
analysis can be found in supplementary material.
Method Resolution Time(s) ‚ÜìGPU(GB) ‚Üì
Dataset: ImageNet-1K
MTT‚Ä†128√ó128 45.0 79.9
TESLA‚Ä†64√ó64 46.0 13.9
SRe2L 224 √ó224 5.2 34.8
D4M 512√ó512 2.7 6.1
Dataset: Tiny-ImageNet
MTT 64 √ó64 5.4 48.9
SRe2L 64 √ó64 11.0 33.8
D4M 512√ó512 2.7 6.1
Table 6. Synthesis time ‚Üìand GPU memory ‚Üìcost on large-
scale datasets .‚Ä†: The runtime of MTT [4] and TESLA [8] on
ImageNet-1K are measured for 10 iterations (500 matching steps).
4.7. Distillation Cost Analysis
We conduct the analysis of GPU memory consumption
across various DD methods, with the corresponding results
presented in Tab. 6. Notably, the architecture-free nature
of D4M during synthesis ensures the fixed time and GPU
memory costs. When considering STM and DTM, we ob-
serve an increase in both time and GPU memory usage
with the enlargement of the matching architecture. For in-
stance, the peak GPU memory utilization for SRe2L in the
recovery of a 64 √ó64 image on ConvNet is 4.2 GB, whereas
on ResNet-50, it reaches a substantial 33.8 GB. Similarly,
when synthesizing a 64 √ó64 image on ConvNet, MTT de-
mands a peak GPU memory of 48.9 GB. Furthermore, the
number of iteration steps impacts the generation time for a
single image in data matching. With the increased iteration
steps, the time cost for SRe2L to recover a 224 √ó224 image
on ResNet-50 gradually rises from 1.31s to 10.48s. Notably,D4M demonstrates a remarkable reduction in time cost by
a factor of 3.82 when compared to SRe2L. Figure 8 reveals
that D4M attains best accuracy at a constant time cost.
0.0 2.5 5.0 7.5 10.0
Time Per Images (s)2030405060T op-1 Acc. (%)
ImageNet-1K
SRe2L-Re18
SRe2L-Re50
D4M-R18
D4M-R50
0.0 2.5 5.0 7.5 10.0
Time Per Images (s)35404550T op-1 Acc. (%)
Tiny-ImageNet
SRe2L-Re18
SRe2L-Re50
D4M-R18
D4M-R50
Figure 8. Top-1 Accuracy ‚Üëand synthesis time ‚Üìon large-scale
datasets . D4M is architecture-free at synthesis time, thereby a
constant runtime cost. Re is the abbreviation of Recover.
5. Conclusion
We introduce D4M, a novel and efficient dataset distillation
framework leveraging the TTM strategy. For the first time,
D4M addresses the cross-architecture generalization issue
by integrating the principles of diffusion models with proto-
type learning. The distilled dataset not only boasts realistic
and high-resolution images with limited resources but also
exhibits a versatility comparable to that of the full dataset.
D4M demonstrates outstanding performance compared to
other dataset distillation methods, particularly when applied
to large-scale datasets such as ImageNet-1K. Last but not
least, rethinking the relationship between generative mod-
els and dataset distillation offers fresh perspectives, paving
the way for the community to develop more efficient dataset
distillation methods in future endeavors.
Limitation and future works. In the situation of ex-
treme distillation (IPC-1/10), we observe a significant per-
formance degradation. Our future work will concentrate on
refining the distillation process for this challenging scenario
and try to distill more real-world multi-modal datasets.
Acknowledgement. This work is supported by the National
Natural Science Foundation of China (No. 12071458).
5816
References
[1] Richard E Blahut. Fast algorithms for signal processing .
Cambridge University Press, 2010. 1
[2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-
horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.
Align your latents: High-resolution video synthesis with la-
tent diffusion models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
22563‚Äì22575, 2023. 3
[3] Zal ¬¥an Borsos, Mojmir Mutny, and Andreas Krause. Coresets
via bilevel optimization for continual learning and stream-
ing.Advances in neural information processing systems , 33:
14879‚Äì14890, 2020. 1
[4] George Cazenavette, Tongzhou Wang, Antonio Torralba,
Alexei A Efros, and Jun-Yan Zhu. Dataset distillation
by matching training trajectories. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 4750‚Äì4759, 2022. 1, 2, 3, 4, 5, 8
[5] George Cazenavette, Tongzhou Wang, Antonio Torralba,
Alexei A Efros, and Jun-Yan Zhu. Generalizing dataset
distillation via deep generative prior. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3739‚Äì3748, 2023. 2
[6] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao
Chen, and Gang Yu. Executing your commands via motion
diffusion in latent space. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 18000‚Äì18010, 2023. 3
[7] Yutian Chen, Max Welling, and Alex Smola. Super-samples
from kernel herding. arXiv preprint arXiv:1203.3472 , 2012.
1
[8] Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Scaling
up dataset distillation to imagenet-1k with constant memory.
InInternational Conference on Machine Learning , pages
6565‚Äì6590. PMLR, 2023. 1, 2, 3, 4, 5, 6, 8
[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248‚Äì255. Ieee, 2009. 2
[10] Zhiwei Deng and Olga Russakovsky. Remember the past:
Distilling datasets into addressable memories for neural net-
works. Advances in Neural Information Processing Systems ,
35:34391‚Äì34404, 2022. 2
[11] Jiawei Du, Yidi Jiang, Vincent YF Tan, Joey Tianyi Zhou,
and Haizhou Li. Minimizing the accumulated trajectory
error to improve dataset distillation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3749‚Äì3758, 2023. 3
[12] Patrick Esser, Johnathan Chiu, Parmida Atighehchian,
Jonathan Granskog, and Anastasis Germanidis. Structure
and content-guided video synthesis with diffusion models.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 7346‚Äì7356, 2023. 3
[13] Jiahui Geng, Zongxiong Chen, Yuandou Wang, Herbert
Woisetschlaeger, Sonja Schimmler, Ruben Mayer, Zhiming
Zhao, and Chunming Rong. A survey on dataset distilla-tion: Approaches, applications and future directions. arXiv
preprint arXiv:2305.01975 , 2023. 2
[14] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot
visual learning without forgetting. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 4367‚Äì4375, 2018. 5, 6
[15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015. 5
[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840‚Äì6851, 2020. 3
[17] Yujin Jeong, Wonjeong Ryoo, Seunghyun Lee, Dabin Seo,
Wonmin Byeon, Sangpil Kim, and Jinkyu Kim. The power
of sound (tpos): Audio reactive video generation with sta-
ble diffusion. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 7822‚Äì7832, 2023. 3
[18] Zixuan Jiang, Jiaqi Gu, Mingjie Liu, and David Z Pan. Delv-
ing into effective gradient matching for dataset condensation.
In2023 IEEE International Conference on Omni-layer Intel-
ligent Systems (COINS) , pages 1‚Äì6. IEEE, 2023. 3
[19] Seung Wook Kim, Bradley Brown, Kangxue Yin, Karsten
Kreis, Katja Schwarz, Daiqing Li, Robin Rombach, Antonio
Torralba, and Sanja Fidler. Neuralfield-ldm: Scene genera-
tion with hierarchical latent diffusion models. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 8496‚Äì8506, 2023. 3
[20] Juil Koo, Seungwoo Yoo, Minh Hieu Nguyen, and Minhyuk
Sung. Salad: Part-level latent diffusion for 3d shape gen-
eration and manipulation. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 14441‚Äì
14451, 2023. 3
[21] Alex Krizhevsky and Geoffrey Hinton. Learning multiple
layers of features from tiny images. Technical report, Uni-
versity of Toronto, Toronto, Ontario, 2009. 2
[22] Ya Le and Xuan Yang. Tiny imagenet visual recognition
challenge. CS 231N , 7(7):3, 2015. 6
[23] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist hand-
written digit database. ATT Labs [Online]. Available:
http://yann.lecun.com/exdb/mnist , 2, 2010. 2
[24] Saehyung Lee, Sanghyuk Chun, Sangwon Jung, Sangdoo
Yun, and Sungroh Yoon. Dataset condensation with con-
trastive signals. In International Conference on Machine
Learning , pages 12352‚Äì12364. PMLR, 2022. 1, 3
[25] Shiye Lei and Dacheng Tao. A comprehensive survey to
dataset distillation. arXiv preprint arXiv:2301.05603 , 2023.
2
[26] Gen Li, Varun Jampani, Laura Sevilla-Lara, Deqing Sun,
Jonghyun Kim, and Joongkyu Kim. Adaptive prototype
learning and allocation for few-shot segmentation. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 8334‚Äì8343, 2021. 4
[27] Noel Loo, Ramin Hasani, Alexander Amini, and Daniela
Rus. Efficient dataset distillation using random feature ap-
proximation. Advances in Neural Information Processing
Systems , 35:13877‚Äì13891, 2022. 2
[28] Calvin Luo. Understanding diffusion models: A unified per-
spective. arXiv preprint arXiv:2208.11970 , 2022. 3
5817
[29] Zhaoyang Lyu, Jinyi Wang, Yuwei An, Ya Zhang, Dahua
Lin, and Bo Dai. Controllable mesh generation through
sparse latent point diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 271‚Äì280, 2023. 3
[30] Rafael M ¬®uller, Simon Kornblith, and Geoffrey E Hinton.
When does label smoothing help? Advances in neural in-
formation processing systems , 32, 2019. 5
[31] Timothy Nguyen, Zhourong Chen, and Jaehoon Lee. Dataset
meta-learning from kernel ridge-regression. arXiv preprint
arXiv:2011.00050 , 2020. 2
[32] Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon
Lee. Dataset distillation with infinitely wide convolutional
networks. Advances in Neural Information Processing Sys-
tems, 34:5186‚Äì5198, 2021. 2, 5
[33] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¬®orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684‚Äì10695, 2022. 3, 4
[34] Noveen Sachdeva and Julian McAuley. Data distillation: A
survey. arXiv preprint arXiv:2301.04272 , 2023. 2
[35] David Sculley. Web-scale k-means clustering. In Proceed-
ings of the 19th international conference on World wide web ,
pages 1177‚Äì1178, 2010. 4
[36] Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li, Zheng
Zhu, Jie Zhou, and Jiwen Lu. Difftalk: Crafting diffusion
models for generalized audio-driven portraits animation. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 1982‚Äì1991, 2023. 3
[37] Zhiqiang Shen and Eric Xing. A fast knowledge distillation
framework for visual recognition. In European Conference
on Computer Vision , pages 673‚Äì690. Springer, 2022. 5
[38] Yu Takagi and Shinji Nishimoto. High-resolution image re-
construction with latent diffusion models from human brain
activity. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 14453‚Äì
14463, 2023. 3
[39] Mariya Toneva, Alessandro Sordoni, Remi Tachet des
Combes, Adam Trischler, Yoshua Bengio, and Geof-
frey J Gordon. An empirical study of example forget-
ting during deep neural network learning. arXiv preprint
arXiv:1812.05159 , 2018. 1
[40] Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang,
Shuo Wang, Guan Huang, Hakan Bilen, Xinchao Wang, and
Yang You. Cafe: Learning to condense dataset by align-
ing features. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 12196‚Äì
12205, 2022. 1, 3, 5
[41] Kai Wang, Jianyang Gu, Daquan Zhou, Zheng Zhu, Wei
Jiang, and Yang You. Dim: Distilling dataset into genera-
tive model. arXiv preprint arXiv:2303.04707 , 2023. 2
[42] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and
Alexei A Efros. Dataset distillation. arXiv preprint
arXiv:1811.10959 , 2018. 2
[43] Chen Henry Wu and Fernando De la Torre. A latent space
of stochastic diffusion models for zero-shot image editingand guidance. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 7378‚Äì7387, 2023. 3
[44] Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-
mnist: a novel image dataset for benchmarking machine
learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
2
[45] Hong-Ming Yang, Xu-Yao Zhang, Fei Yin, and Cheng-
Lin Liu. Robust classification with convolutional proto-
type learning. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 3474‚Äì3482,
2018. 4
[46] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Run-
sheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-
Hsuan Yang. Diffusion models: A comprehensive survey of
methods and applications. ACM Computing Surveys , 2022.
3
[47] Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A
gift from knowledge distillation: Fast optimization, network
minimization and transfer learning. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 4133‚Äì4141, 2017. 5
[48] Zeyuan Yin, Eric Xing, and Zhiqiang Shen. Squeeze, recover
and relabel: Dataset condensation at imagenet scale from a
new perspective. arXiv preprint arXiv:2306.13092 , 2023. 2,
3, 4, 5, 6
[49] Ruonan Yu, Songhua Liu, and Xinchao Wang. Dataset
distillation: A comprehensive review. arXiv preprint
arXiv:2301.07014 , 2023. 2
[50] Baoquan Zhang, Xutao Li, Yunming Ye, Zhichao Huang,
and Lisai Zhang. Prototype completion with primitive
knowledge for few-shot learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3754‚Äì3762, 2021. 4
[51] Bo Zhao and Hakan Bilen. Dataset condensation with differ-
entiable siamese augmentation. In International Conference
on Machine Learning , pages 12674‚Äì12685. PMLR, 2021. 1,
3, 5
[52] Bo Zhao and Hakan Bilen. Dataset condensation with dis-
tribution matching. In Proceedings of the IEEE/CVF Win-
ter Conference on Applications of Computer Vision , pages
6514‚Äì6523, 2023. 1, 3
[53] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset
condensation with gradient matching. arXiv preprint
arXiv:2006.05929 , 2020. 1, 3
[54] Yongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba. Dataset
distillation using neural feature regression. Advances in Neu-
ral Information Processing Systems , 35:9813‚Äì9827, 2022. 2,
5
5818
