Efﬁcient Hyperparameter Optimization with Adaptive Fidelity Identiﬁcation
Jiantong Jiang1, Zeyi Wen2,3*, Atif Mansoor1, Ajmal Mian1
1The University of Western Australia,2HKUST (Guangzhou),3HKUST
jiantong.jiang@research.uwa.edu.au, wenzeyi@ust.hk, {atif.mansoor, ajmal.mian }@uwa.edu.au
Abstract
Hyperparameter Optimization and Neural Architecture
Search are powerful in attaining state-of-the-art machine
learning models, with Bayesian Optimization (BO) stand-
ing out as a mainstream method. Extending BO into the
multi-ﬁdelity setting has been an emerging research topic
in this ﬁeld, but faces the challenge of determining an ap-
propriate ﬁdelity for each hyperparameter conﬁguration to
ﬁt the surrogate model. To tackle the challenge, we propose
a multi-ﬁdelity BO method named FastBO, which excels in
adaptively deciding the ﬁdelity for each conﬁguration and
providing strong performance while ensuring efﬁcient re-
source usage. These advantages are achieved through our
proposed techniques based on the concepts of efﬁcient point
and saturation point for each conﬁguration, which can be
obtained from the empirical learning curve of the conﬁgu-
ration, estimated from early observations. Extensive experi-
ments demonstrate FastBO’s superior anytime performance
and efﬁciency in identifying high-quality conﬁgurations and
architectures. We also show that our method provides a way
to extend any single-ﬁdelity method to the multi-ﬁdelity set-
ting, highlighting the wide applicability of our approach.
1. Introduction
Hyperparameters are crucial in machine learning pipelines.
Hyperparameter optimization (HPO) [ 11] and Neural Ar-
chitecture Search (NAS) [ 9] aims to ﬁnd the hyperparame-
ters or architectures that can yield good performance with-
out human experts. Among different HPO and NAS meth-
ods, Bayesian Optimization (BO) [ 2,14,40] is an effec-
tive model-based method that has shown remarkable suc-
cess [ 8,39]. BO maintains a surrogate model of the tar-
get performance metric based on past evaluations of hyper-
parameter conﬁgurations, which guides the choice of more
promising conﬁgurations to evaluate.
Despite its sample efﬁciency, standard BO requires a
full evaluation of each conﬁguration, involving full-scale
*Zeyi Wen is the corresponding author.training and testing of models, which can be highly time-
consuming, particularly with the recent trend to larger mod-
els. To avoid expensive full evaluations, multi-ﬁdelity
methods [ 4,16,25,26] have been proposed, where the ﬁ-
delities refer to the levels of performance metrics obtained
under different resource levels. These methods follow the
principle of successive halving (SHA) [ 16]: initially, they
evaluate a set of randomly selected conﬁgurations using a
small number of resources; then, based on the low-ﬁdelity
performances, the poorly-performing ones are successively
eliminated, while the well-performing ones continue to
be evaluated with increasing resources. Follow-up stud-
ies [10,22,27,37,47] propose model-based multi-ﬁdelity
methods, replacing the random conﬁguration selection with
a more informed model to improve sample efﬁciency.
Nevertheless, current model-based multi-ﬁdelity meth-
ods face a major limitation: they are built upon the SHA
framework, which operates under the assumption that learn-
ing curves of different conﬁgurations rarely intersect. This
assumption does not hold in practice [ 46], i.e., early perfor-
mance observations cannot always indicate the ﬁnal ﬁdelity
performance at the full resource level. This leads to a fun-
damental challenge when extending model-based methods
to the multi-ﬁdelity setting: What is the appropriate ﬁdelity
for each conﬁguration to ﬁt the surrogate model? In other
words, which ﬁdelity can provide performance observations
that reliably indicate the ﬁnal ﬁdelity performance? Exist-
ing methods struggle to address this fundamental challenge.
In particular, BOHB [ 10] and Hyper-Tune [ 27] ﬁt separate
surrogate models for different ﬁdelities, failing to capture
inter-ﬁdelity correlations. FTBO [ 44] and A-BOHB [ 22]
ﬁt a joint model but require strong assumptions to remain
tractable. Another work by Salinas et al. [ 37] suggests us-
ing the last observed ﬁdelity performance to ﬁt the surro-
gate model. However, it widens the gap between poorly-
and well-performing conﬁgurations at the early stage, po-
tentially leading to an inaccurate surrogate model.
To this end, we propose a multi-ﬁdelity extension of BO,
namely FastBO, which tackles the challenge of deciding the
appropriate ﬁdelity for each conﬁguration to ﬁt the surro-
gate model. FastBO identiﬁes a so-called efﬁcient point for
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
26181
each conﬁguration to be the ﬁdelity. The point balances
computational cost and performance quality while captur-
ing valuable learning curve trends. In essence, FastBO se-
lects the ﬁdelity for each conﬁguration instead of evaluating
all the conﬁgurations at the same ﬁdelity. Additionally, a
saturation point for each conﬁguration is identiﬁed to be an
approximation of the ﬁnal ﬁdelity, leading to high-quality
performance while reducing resource wastage. The two cru-
cial points are adaptively derived from the estimated learn-
ing curve of each conﬁguration. Furthermore, the warm-up
and post-processing stages are carefully designed to enable
judicious early-termination detection and efﬁcient satura-
tion level evaluation. Empirical evaluation against the state-
of-the-art methods shows that FastBO has strong anytime
performance and can considerably save up to 87% of the
time required to identify a good conﬁguration or architec-
ture, lowering the barriers for engaging in HPO and NAS.
In summary, we make the following major contributions.
1.We propose a multi-ﬁdelity model-based HPO method
that adaptively decides the ﬁdelities for conﬁgurations
and efﬁciently offers strong performance, thanks to the
introduced concepts of efﬁcient and saturation points.
2.We develop a learning curve modeling module to enable
adaptive derivation of the key points, a warm-up stage to
allow early-termination detection, and a post-processing
stage to ensure efﬁcient saturation-level evaluation.
3.We show that our strategy can be used to extend exist-
ing single-ﬁdelity methods to the multi-ﬁdelity setting,
demonstrating the effectiveness and generality of our
method and highlighting promising future opportunities.
2. Related Work
HPO and NAS incur signiﬁcant costs, especially consider-
ing the escalating model evaluation overhead. Despite ef-
forts to directly accelerate computation [ 17–19,52], smarter
HPO strategies are still required for the widespread adop-
tion of automation. Two crucial directions include model-
based and multi-ﬁdelity methods, which can also be com-
bined. Here, we review the methods in these categories.
Model-based methods. BO is the representative of model-
based methods. Based on the surrogate model constructed
by historical evaluation results, BO selects the conﬁgura-
tions to evaluate via an acquisition function that balances
exploration and exploitation. Commonly used surrogate
models are Gaussian processes [ 40], random forests [ 14],
tree-structured Parzen estimator [ 2], and deep networks [ 41,
42]. Popular acquisition functions include Expected Im-
provement [ 28], Knowledge Gradient [ 12], Upper Conﬁ-
dence Bound [ 43], and Predictive Entropy Search [ 13]. Re-
cent studies on BO have explored the utilization of expert
priors [ 15,24,33,38] and derivative information [ 1,34,49].
There also has been a recent focus on enhancing the inter-
pretability [ 5,50,51] of the HPO process [ 3,31,32].Multi-ﬁdelity methods. Multi-ﬁdelity methods exploit low
and high ﬁdelities for conﬁgurations to save the evaluation
time. Successive halving (SHA) [ 16] runs a set of conﬁg-
urations using a small number of resources and promotes
only the best-performing half of conﬁgurations to continue
for twice as many resources. Hyperband [ 25] calls SHA as
a sub-routine with varying maximum resources and intro-
duces a reduction factor to control the fraction of promo-
tion. ASHA [ 26] extends SHA to the asynchronous setting
by aggressive early-stopping. Later, PASHA [ 4] further ex-
tends ASHA through more aggressive early-stopping based
on the ranking of conﬁgurations during the tuning process.
Combination of model-based and multi-ﬁdelity meth-
ods. BOHB [ 10] and a parallel work [ 47] ﬁrst propose
to combine model-based and multi-ﬁdelity methods by re-
placing the random sampling in Hyperband with BO. A-
BOHB [ 22] employs a joint GP surrogate over ﬁdelities and
supports asynchronous scheduling. Hyper-Tune [ 27] im-
proves its Hyperband by a delayed strategy to decrease in-
accurate promotions. Salinas et al. [ 37] proposed to extend
methods to multi-ﬁdelity settings by using the performance
of the last ﬁdelity in an ASHA running. DyHPO [ 48] and
DPL [ 20] introduce new surrogates for multi-ﬁdelity BO
considering the learning curves; the former uses deep GP
kernels while the latter integrates deep power law functions.
3. Problem Formulation
Given an algorithm having hyperparameters  1,. . . ,  mwith
domains ⇤1,. . . ,⇤m, we deﬁne its hyperparameter space as
⇤=⇤1⇥...⇥⇤m. Here, we deﬁne the problem and out-
line the key challenge related to hyperparameter optimiza-
tion (HPO). Notations are in Supp. 7for reference.
Single-ﬁdelity setting. For each hyperparameter conﬁg-
uration  , we denote f( )as the performance achieved
using  . For consistency, the metric in this paper refers
to descending metrics like validation loss, with ascending
metrics being treated similarly. In the single-ﬁdelity HPO
setting, we aim to ﬁnd  ⇤minimizing function f( ), i.e.,
 ⇤= arg min 2⇤f( ). BO is one of the most popu-
lar single-ﬁdelity HPO methods. The vanilla BO has two
key components: a surrogate model Mto approximate the
objective function f( ), and an acquisition function ato
identify a promising conﬁguration from search space. With
these ingredients, BO iterates three steps: (i) select a con-
ﬁguration  iby maximizing the acquisition function; (ii)
evaluate  ito get yiand add the data ( i,yi)into the cur-
rent observation set Di 1={( 1,y1),. . . ,( i 1,yi 1)};
(iii) update the surrogate model and the acquisition function
based on the augmented Di. In this work, Mis a Gaussian
Process and ais Expected Improvement.
Multi-ﬁdelity setting. Multi-ﬁdelity methods consider re-
source information, such as training epochs or training sub-
set ratios. Evaluations at various resource levels results in
26182
different performance levels, known as the ﬁdelities . Differ-
ent ﬁdelities provide a way to balance computational cost
and performance quality. In multi-ﬁdelity HPO problems,
the target is extended to  ⇤= arg min 2⇤f( ,r), where
f( ,r)is the objective function obtained for  atr. We use
rto denote the resource level, which can also be interpreted
as the ﬁdelity, and r2{rmin,. . . ,r max}.
Extending single-ﬁdelity methods to the multi-ﬁdelity
setting. The inefﬁciency of single-ﬁdelity methods stems
from their reliance on the ﬁnal ﬁdelity evaluation of
f( ,rmax)to be the evaluation of its objective f( ,r).
Fitting surrogate models by such ﬁnal ﬁdelity evaluations
incurs high cost due to the full evaluation of the conﬁgu-
rations. Notably, low-ﬁdelity evaluations at r<r maxpro-
vide informative insights into the objective but are computa-
tionally cheaper, which is valuable to the optimization pro-
cess. Therefore, we seek an effective way to extend single-
ﬁdelity methods like BO to the multi-ﬁdelity setting. More
speciﬁcally, recalling the earlier steps of BO, when eval-
uating the conﬁguration  iin the second step, we instead
acquire its low-ﬁdelity performance yri
iatri, where ride-
notes the ﬁdelity used for  ito ﬁt the surrogate model. The
observations Dithen becomes {( 1,yr1
1),. . . ,( i,yri
i)}.T o
conclude, in order to extend single-ﬁdelity methods to the
multi-ﬁdelity setting, the key challenge to be addressed is
to determine rifor each  i.
4. Methodology
In this section, we propose a novel multi-ﬁdelity model-
based algorithm FastBO. We ﬁrst propose the key concepts
of efﬁcient point and saturation point, which are crucial in
deciding the ﬁdelity level to ﬁt the surrogate model and to
approximate the ﬁnal ﬁdelity respectively. Secondly, we
elaborate on the details of learning curve modeling, where
the two crucial points can be extracted. Then, we present
the techniques associated with the auxiliary warm-up and
post-processing stages. Finally, we summarize FastBO and
discuss its wide applicability to any single-ﬁdelity methods.
4.1. Estimation of Efﬁcient and Saturation Points
In our method, we adaptively identify efﬁcient and satura-
tion points for each conﬁguration. The two points are cru-
cial in the optimization process. We ﬁrst formally deﬁne the
efﬁcient point as follows.
Deﬁnition 1 (Efﬁcient point) .For a given learning curve
Ci(r)of hyperparameter conﬁguration  i, where rrepre-
sents the resource level (also referred to as ﬁdelity), the ef-
ﬁcient point eiof iis deﬁned as: ei=m i n {r|Ci(r) 
Ci(2r)<  1}, where  1is a predeﬁned small threshold.
The semantic of Deﬁnition 1is that starting from the ef-
ﬁcient point onwards, when the resources are doubled (i.e.,from rto2r), the performance improvement falls below a
small threshold  1. Consequently, this point characterizes
the ﬁdelity at which a conﬁguration demonstrates strong
performance while still efﬁciently utilizing resources. In
simpler terms, it signiﬁes an appropriate ﬁdelity of perfor-
mance that can be achieved with comparably efﬁcient re-
source usage. Therefore, we make the following remark.
Remark 1. The efﬁcient points of the hyperparameter con-
ﬁgurations can serve as their appropriate ﬁdelities used for
ﬁtting the surrogate model. This is due to their (i) opti-
mal resource-to-performance balance, (ii) ability to capture
valuable learning curve trends, and (iii) customization for
different hyperparameter conﬁgurations.
We elaborate on the reasons in Remark 1as follows.
Firstly, efﬁcient points balance the trade-off between com-
putational cost and result quality. Beyond the efﬁcient point
of a given conﬁguration, allocating additional resources to
that conﬁguration becomes less efﬁcient. Secondly, ef-
ﬁcient points capture valuable trends within the learning
curves. For example, the learning rate inﬂuences the shape
of learning curves; the identiﬁcation of efﬁcient points for
conﬁgurations with smaller learning rates often occurs at
later stages. The insights into learning curve behaviors en-
able more informed decision-making. Thirdly, the ability
to customize the ﬁdelity for each speciﬁc conﬁguration is a
signiﬁcant advantage. This adaptive approach is more rea-
sonable than previous studies that use a ﬁxed ﬁdelity for all
conﬁgurations, as it better accounts for the unique charac-
teristics of individual learning curves.
This insight leads us to use the efﬁcient point eiiden-
tiﬁed for each conﬁguration  ias its ﬁdelity used to ﬁt the
surrogate model. Speciﬁcally, we evaluate  iuntil reaching
eiand obtain the observed performance yei
i. The resulting
data point ( i,yei
i)is then added into the current observa-
tion set Di 1to reﬁt the surrogate model. We proof the
superiority of FastBO over SHA-based methods in Supp. 8.
Besides efﬁcient points, we identify saturation points for
all conﬁgurations from their learning curves as well. We
provide the deﬁnition of the saturation point as follows.
Deﬁnition 2 (Saturation point) .For a given learning curve
Ci(r)of conﬁguration  i, where rrepresents the resource
level (also referred to as ﬁdelity), the saturation point siof
 iis deﬁned as: si=m i n {r|8r0>r , |Ci(r0) Ci(r)|<
 2}, where  2is a predeﬁned small threshold.
The semantic of Deﬁnition 2is that beyond the satura-
tion point, the observed performance no longer exhibits no-
table variations with more resources. Thus, this point char-
acterizes the ﬁdelity at which the performance of a conﬁg-
uration stabilizes. The concept of saturation point is well-
recognized within the machine learning community. Build-
ing on the above deﬁnition, we make the following remark.
26183
Remark 2. The saturation points of the hyperparameter
conﬁgurations can serve as their approximate ﬁnal ﬁdeli-
ties, as they provide performance results that meet prede-
ﬁned quality thresholds while reducing resource wastage.
This insight leads us to use the saturation point siidenti-
ﬁed for each conﬁguration  ias its ﬁnal ﬁdelity approxima-
tion. The point is used in the post-processing stage for pro-
moting some well-performing conﬁgurations to get higher-
ﬁdelity performances. In essence, when aiming for a full
evaluation of the conﬁgurations, we suggest that terminat-
ing the evaluation at the saturation point is sufﬁcient. A
more intuitive illustration of the concepts of efﬁcient and
saturation points is provided in Supp. 9.
4.2. Learning Curve Modeling
From Deﬁnitions 1and 2, we can extract the efﬁcient
and saturation points of conﬁgurations from their learn-
ing curves. The curve Ci(r)corresponds to conﬁguration
 iand describes the predictive performance with  ias a
function of the ﬁdelity r. Here, rcan be either the num-
ber of training instances or the number of training epochs.
In the context of learning curves, the former is referred
to as observation learning curves, while the latter is iter-
ation learning curves [ 29]. Both types are applicable to
FastBO, so we use the term learning curve to encompass
both. Given the observation set Ow
i={(r, yr
i)}r=rmin,...,w
for i, which comprises pairs of data points representing
ﬁdelities r2{rmin,. . . ,w }and the corresponding evalua-
tions yr
i, where wis a pre-deﬁned warm-up point to stop
collecting data, FastBO can estimate a learning curve for
 ibased on Ow
iby ﬁrst constructing a parametric learning
curve model, then estimating the parameters.
Constructing a parametric learning curve model. Empir-
ical learning curves can be modeled with function classes
relying on some parameters. Viering and Loog [ 46] com-
prehensively summarized the parametric models studied in
machine learning. In practice, different problems have dif-
ferent learning curves; even under the same problem, differ-
ent hyperparameter conﬁgurations ( e.g., learning rate, regu-
larization, etc.) may lead to signiﬁcantly different learning
curves. Since one single parametric model is not enough
to characterize all the learning curves by itself, we consider
combining different parametric models into a single model.
Speciﬁcally, we consider three parametric models POW3,
EXP3 and LOG2, as listed in Tab. 1, which have shown
Table 1. Parametric learning curve models used.
Model Formula Family
POW3 y=d+ax ↵Power law
EXP3 y=d+e ax+bExponential
LOG2 y=d+alog(x)Logarithmicgood ﬁtting and predicting performance in previous empiri-
cal studies [ 29,46]. We provide detailed discussions on the
choice of parametric models in Supp. 10.
Here, we denote each parametric model as cj(r|✓j)with
parameters ✓j, where the independent variable rrepresents
the ﬁdelity. We combine three models into one model via a
weighted linear combination:
C(r| )=X
j2{1,2,3}!jcj(r|✓j), (1)
where  ={!1,!2,!3,✓1,✓2,✓3}is the parameter of the
combined model, which consists of parameters {✓1,✓2,✓3}
and weight {!1,!2,!3}of every single model. Therefore,
each pair of observations (r, yr
i)inOw
ican be modeled by
the combined model as yr
i=C(r| )+✏, where yr
iis the
observed dependent variable and ✏represents the error term.
Estimating parameters in the parametric learning curve
model. We employ maximum likelihood estimation to esti-
mate the parameters  in the parametric model C(r| ). As-
suming that ✏⇠N(0, 2), the probability of an observed
performance yr
iunder parameters is given by p(yr
i| , 2)=
N(yr
i;C(r| ), 2). Given the observations Ow
iof ithat
contains a set of observed data points (r, yr
i), the likelihood
function can be expressed as:
L( , 2;r,yr
i)=Y
p(yk
i| , 2)
=wY
k=rmin1
 p
2⇡exp✓
 (yr
i C(r=k| ))2
2 2◆
.(2)
We estimate  by maximizing log-likelihood function,
which is easily calculated given Eq. 2.
An existing model-free method [ 7] also considers us-
ing learning curves for the HPO problem. However, it tar-
gets predicting the high-ﬁdelity performance from the low-
ﬁdelity observations and thus stopping conﬁgurations that
are unlikely to beat the current best values, which is differ-
ent from our main target of identifying appropriate ﬁdelity
levels for the conﬁgurations to ﬁt the surrogate model from
their estimated learning curves.
4.3. Warm-up And Post-processing Stages
In addition to its core components, FastBO incorporates
two auxiliary stages: the warm-up and post-processing
stages. For the completeness of our method, we provide an
overview of these stages, outlining their targets and present-
ing the key techniques of early-termination detection and
saturation-level evaluation that are applied within.
Warm-up stage. The warm-up stage prepares the early
observation set Ow
ifor each conﬁguration  ithat is used
to estimate its learning curve, as discussed in § 4.2. Here
w2(rmin,rmax)is a pre-determined ﬁdelity, denoted as
warm-up point. Speciﬁcally, we initiate the evaluation of
26184
each newly selected  i, proceeding until reaching w. Dur-
ing this process, we record each ﬁdelity rand its evaluation
result yr
i, forming pairs (r, yr
i). Upon reaching w, we pause
the evaluation for  iand obtain its early observation set
Ow
i={(r, yr
i)}r=rmin,...,w, and start modeling the learning
curve. During the warm-up stage, we monitor the perfor-
mance changes across every two continuous ﬁdelities. If we
detect that the performance of  ihas consecutively dropped
twice by more than a ratio ↵, i.e., ( yr 1
i yr 2
i)>↵ yr 2
i
and(yr
i yr 1
i)>↵ yr 1
i, we promptly terminate the eval-
uation for  iat its current ﬁdelity r, because such con-
secutive performance deterioration indicates  iis unlikely
to achieve satisfactory performance. Once terminated, we
directly incorporate the current performance yr
iof iinto
Di 1that is used for updating the surrogate model. Thus,
further operations like learning curve modeling are dis-
continued for  i. Moreover, if we observe a single case
of performance drop without subsequent occurrences, i.e,
yr 1
i yr 2
i>↵ yr 2
iandyr
i yr 1
i↵yr 1
i, we opt not
to include data from ﬁdelity r 1inOw
i. This is to manu-
ally ﬁlter out potential noise in the data that may adversely
affect the ﬁtting of the learning curve.
Post-processing stage. The post-processing stage aims
at two tasks: promoting the well-performing conﬁgura-
tions for saturation-level evaluations and identifying the
best conﬁguration and its performance. Firstly, FastBO pro-
motes the top- kwell-performing conﬁgurations and evalu-
ates them to their saturation points to ensure high-quality
performance while maintaining efﬁcient resource utiliza-
tion. We set kto be always less than or equal to the number
of parallel workers available, ensuring a manageable over-
head of saturation-level evaluations. It is worth noting that
the additional time required is factored into the overall time.
Secondly, FastBO ﬁnds the best conﬁguration along with its
performance achieved so far, which is a standard ﬁnal step
in most HPO methods. However, an increase in ﬁdelities
does not always result in performance improvement, possi-
bly due to overﬁtting, resource saturation, or problem com-
plexity. Therefore, we treat the evaluation at each ﬁdelity
as an individual task and record all these intermediate eval-
uation results, which is also a common practice in recent
implementations. In this way, FastBO ﬁnds the best per-
formance by considering all the results, rather than relying
solely on the highest-ﬁdelity performances of the conﬁgu-
rations. In the parallel setting, treating each ﬁdelity evalua-
tion as an individual task offers an added beneﬁt due to its
ﬁner granularity. More speciﬁcally, when a worker is idle,
it takes on a new task of evaluating a conﬁguration at a spe-
ciﬁc ﬁdelity, rather than evaluating an entire conﬁguration.
4.4. FastBO and Generalization
Algorithm 1summarizes our proposed FastBO. It takes sur-
rogate model M, acquisition function a, warm-up point w,Algorithm 1: FastBO algorithm
input : M,a,w,↵,k, 1, 2.
output:  ⇤,y⇤
1i 0,D ;
2while not meet the stop criterion do
3 ﬁnd i arg max 2⇤a( ,Mi 1)
4 Ow
i,t warm-up given w,↵//cf.§4.3
5 ifOw
iis not empty then
6 ﬁtCi(r)toOw
i//cf.§4.2
7 ﬁndei,sigiven Ci(r), 1, 2//cf.§4.1
8 yei
i continue evaluating  itoei
9 else
10 ei t,si rmax
11 Di D i 1[( i,yei
i)
12 reﬁtMitoDi
13 i i+1
14 ⇤,y⇤ post-process given s={si},k//cf.§4.3
performance decrease ratio ↵, promotion number k, and
thresholds  1, 2as inputs, and output the best-founded
conﬁguration  ⇤and its performance y⇤. FastBO follows
a similar iterative process of model-based methods but re-
places the expensive full evaluations with a more intelligent
alternative ( cf. Lines 4-10). Speciﬁcally, each conﬁguration
 iﬁrst enters a warm-up stage to collect its early observa-
tion set Ow
iand to be detected and terminated if it exhibits
consecutive performance deterioration ( cf. Line 4). If  i
is not terminated, FastBO then estimates a learning curve
Ci(r)for ibased on Ow
i(cf. Line 6), and thus the efﬁcient
point and saturation point of  ican be obtained ( cf. Line
7). After that,  icontinues to be evaluated until reaching
ei(cf. Line 8); the result is added to the observation set D
(cf. Line 11) that is used for updating M(cf. Line 12). On
the other hand, the poorly-performing conﬁguration will be
terminated early at ﬁdelity twith its result being added di-
rectly to D(cf. Lines 10, 11). Finally, the post-processing
stage promotes the most promising conﬁgurations to their
saturation points and ﬁnds the best-founded conﬁguration
 ⇤and its performance y⇤(cf. Line 14).
Generalizing FastBO to single-ﬁdelity methods. The core
of FastBO is to tackle the key challenge of deciding an ap-
propriate ﬁdelity for each conﬁguration to ﬁt the surrogate
model by adaptively identifying its efﬁcient point. This
strategy of using the efﬁcient point performances for sur-
rogate model ﬁtting also provides a simple but effective
way to bridge the gap between single- and multi-ﬁdelity
methods. While it is primarily described in the context of
model-based methods, the strategy can be generalized to
various single-ﬁdelity methods. For example, when evalu-
ating conﬁgurations within the population for an evolution-
26185
Fashion-MNIST
AirlinesAlbertCovertypeChristineFigure 1. Performance of average validation accuracy on the LCBench benchmark.
Slice
CIFAR-10 CIFAR-100ProteinImageNet16-120
(a) NAS-Bench-201 benchmark(b) FCNet benchmark
Figure 2. Performance of (a)average validation error on NAS-Bench-201 and (b)average validation loss on FCNet.
ary algorithm-based HPO method, we can similarly evalu-
ate the efﬁcient point performances instead of the ﬁnal per-
formances of these conﬁgurations and integrate the perfor-
mances in the subsequent processes, such as selection and
variation. Relying on the efﬁcient point rather than the ﬁnal
ﬁdelity or all ﬁdelities available simpliﬁes the extension of
the single-ﬁdelity methods to the multi-ﬁdelity setting. The
rationale behind this adaptive ﬁdelity identiﬁcation strategy
is discussed in Remark 1. We also demonstrate in our ex-
periments the efﬁcacy of this strategy in extending a range
of single-ﬁdelity methods to the multi-ﬁdelity setting.
5. Experiments
We empirically evaluate the performance of FastBO and
compare it with the random search baseline (RS) and 9
competitive baselines from 3 related categories, includ-
ing (i) model-based methods: standard Gaussian Process-
based BO [ 40]; (ii) multi-ﬁdelity methods: ASHA [ 26],
Hyperband [ 25], PASHA [ 4]; and (iii) model-based multi-
ﬁdelity methods: A-BOHB [ 22], A-CQR [ 37], BOHB [ 10],
DyHPO [ 48], Hyper-Tune [ 27]. RS and BO are single-
ﬁdelity baselines, while the others are multi-ﬁdelity ones.
Our experiments are conducted on 10 datasets from 3
popular benchmarks LCBench [ 53], NAS-Bench-201 [ 8]
and FCNet [ 21]. Detailed information on the benchmarks
is provided in Supp. 13.1. All the experiments are evalu-
ated with four parallel workers and 10 random seeds. Weallocate 20% total budget for warm-up, i.e., w=rmin+
0.2·(rmax rmin). Ratio ↵is set to 0.1; thresholds  1and
 2are set to 0.001 and 0.00051. We set kbased on the num-
ber of workers #wand the number of started conﬁgurations
#c:k= max {d#c/10e,#w}. We provide more experi-
ments and discussions on the hyperparameters in Supp. 12.
We use implementations of the baselines in Syne Tune [ 36].
Details of the baseline settings are in Supp. 13.2.
5.1. Anytime Performance
To evaluate the anytime performance, we compare FastBO
against the baselines on wall-clock time. For fair compar-
isons, all the baselines, even single-ﬁdelity BO and RS, are
extended to consider intermediate results at all the ﬁdelities
when identifying the conﬁguration, akin to FastBO as dis-
cussed in § 4.3. Consequently, all the baselines are able to
achieve their best possible anytime performance.
The results on LCBench, NAS-Bench-201, and FCNet
are shown in Figs. 1and2. We report the validation ac-
curacy, error, and loss over wall-clock time for the three
benchmarks, as provided by the benchmarks. We provide
the results on NAS-Bench-301 [ 39] in Supp. 11.1. Overall,
FastBO can handle various performance metrics and shows
strong anytime performance. We can observe that FastBO
gains an advantage earlier than other methods, rapidly con-
1Parameters  1and 2given here are derived after standardizing met-
rics to a uniform scale from 0 to 1.
26186
Table 2. Comparison of relative efﬁciency on conﬁguration identiﬁcation. FastBO is set as the baseline with a relative efﬁciency of 1.00.
Wall-clock time (abbr. WC time) reports the elapsed time spent for each method on ﬁnding conﬁgurations with similar performance
metrics, i.e., validation error ( ⇥10 2) for Covertype and ImageNet16-120 and validation loss ( ⇥10 5) for Slice.
DatasetMetric Method
FastBO BO PASHA A-BOHB A-CQR BOHB DyHPO Hyper-Tune
CovertypeVal. error 22.9 ±0.223.0 ±0.3 25.1 ±2.5 23.5 ±1.1 31.6 ±1.9 32.5 ±0.8 23.0 ±0.3 23.0 ±0.2
WC time (h) 0.7±0.3 2.9±0.7 3.9±1.0 2.0±1.0 3.9±0.2 2.5±1.0 1.7±0.6 1.8±0.7
Rel. efﬁciency 1.00 0.25 0.18 0.37 0.19 0.29 0.41 0.40
ImageNet
16-120Val. error 55.3 ±0.257.4 ±1.2 55.7 ±0.3 55.8 ±1.6 55.5 ±0.9 55.5 ±1.1 55.5 ±1.0 55.3 ±2.0
WC time (h) 2.2±0.7 6.6±0.9 2.5±1.2 5.9±1.1 6.0±1.3 3.2±0.7 4.3±1.0 3.4±1.1
Rel. efﬁciency 1.00 0.34 0.90 0.38 0.37 0.68 0.51 0.67
SliceVal. loss 26.3 ±2.626.4 ±4.4 26.8 ±9.5 26.3 ±6.3 27.1 ±4.2 26.8 ±5.6 27.4 ±2.3 28.7 ±1.3
WC time (h) 0.4±0.1 3.1±0.7 1.2±0.9 2.1±0.7 2.5±0.7 2.2±0.9 2.5±0.5 1.8±0.6
Rel. efﬁciency 1.00 0.13 0.35 0.20 0.17 0.19 0.17 0.24
verging to the global optimum after the initial phase.
The superiority can be attributed to two main factors.
Firstly, FastBO maintains, and in some cases even sur-
passes, the sample efﬁciency of vanilla BO, thanks to
our techniques that enable quick and precise identiﬁcation
of the ﬁdelities for conﬁgurations to update the surrogate
model. We provide more explanations and experiments on
sample efﬁciency in Supp. 11.2. Secondly, the multi-ﬁdelity
extension speeds up the evaluations, contributing to its over-
all efﬁciency. In contrast, the single-ﬁdelity baselines tend
to waste more time on the full evaluations. While the multi-
ﬁdelity baselines efﬁciently explore numerous conﬁgura-
tions, they limit their evaluations to only constrained ﬁdeli-
ties for some time, thus struggling to provide relatively high
performance in a short time. This issue in multi-ﬁdelity
methods is particularly pronounced in PASHA when ap-
plied to NAS-Bench-201 and FCNet, as shown in Fig. 2.
In Supp. 11.3, we further provide the ranks of all methods
and statistically show FastBO’s superiorty on an early stage.
It is worth noting that all the additional overhead introduced
by FastBO is taken into account in the wall-clock time.
Regarding the ﬁnal performance, most methods are able
to converge to satisfactory solutions, with negligible differ-
ences among them in most cases. Although our goal is not
to offer the best ﬁnal performance as we limit the evalua-
tions to at most the saturation point even for those we con-
sider most promising, FastBO still achieves top-2 ﬁnal per-
formance on 8 out of 10 datasets. In contrast, model-free
methods sometimes cannot obtain a satisfactory ﬁnal per-
formance because they randomly select the conﬁgurations.
For example, on the “Covertype” dataset, only 3 out of 2000
conﬁgurations yield a validation accuracy exceeding 75%.
As a result, all the model-free methods face challenges in
converging to a satisfactory ﬁnal performance.5.2. Efﬁciency on Conﬁguration Identiﬁcation
One explanation for PASHA’s suboptimal anytime perfor-
mance ( cf. Fig. 2) lies in its primary goal [ 4]: the goal of
PASHA is not high accuracy but to identify the best conﬁg-
uration more quickly. To ensure equitable comparisons, we
report the time spent for each method on identifying a sat-
isfactory conﬁguration, consistent with the experiments de-
scribed in PASHA [ 4]. Results on three expensive datasets
“Covertype”2, “ImageNet16-120”, and “Slice” of the three
benchmarks are shown in Tab. 2. Similar results on addi-
tional datasets can be found in Supp. 11.4. Besides PASHA,
results of other model-free multi-ﬁdelity methods are not in-
cluded, as PASHA demonstrates its superiority over them.
Tab. 2shows that FastBO saves 10% to 87% wall-clock
time over other methods when achieving up to 9.6% better
performance values. It can be observed from the “rel. efﬁ-
ciency” rows, where we set FastBO as the baseline with a
relative efﬁciency of 1.00 and report the efﬁciency of other
methods relative to ours. When compared with vanilla BO,
FastBO signiﬁcantly shortens the time in identifying a good
conﬁguration by a factor of 3 to 8, because FastBO pauses
a conﬁguration earlier at an appropriate ﬁdelity and ﬁts the
surrogate model to guide the next conﬁguration search. This
advantage creates opportunities to efﬁciently explore more
conﬁgurations. Another observation is that PASHA always
gets a relatively high variance in wall-clock time. This is
due to the fact that different random seeds can have a larger
impact on such model-free methods.
5.3. Effectiveness of Adaptive Fidelity Identiﬁcation
As discussed in § 4.1, FastBO is able to adaptively identify
the efﬁcient point eifor each conﬁguration  iand serves
2We convert the accuracy of “Covertype” into error for readability.
26187
SliceImageNet16-120Covertype
SliceImageNet16-120Covertype(a) Impact of adaptive fidelity identification(b) Generality of FastBOFigure 3. Performance comparison: (a)Performance of FastBO that adaptively sets ri=eiwith the schemes that use ﬁxed rifor all
conﬁgurations. (b)Performance of single-ﬁdelity methods CQR, BORE, REA and their multi-ﬁdelity variants using our extension method.
eias its ﬁdelity rifor surrogate model ﬁtting. To investi-
gate the effectiveness of the adaptive ﬁdelity identiﬁcation
strategy, we conduct an ablation study to compare the per-
formance achieved with and without applying this strategy.
Speciﬁcally, we compare FastBO, where riis adaptively set
toei, with the partial evaluation schemes that employ ﬁxed
predeﬁned values as the ﬁdelity for all the conﬁgurations
to ﬁt the surrogate model. We consider three representative
ﬁxed ﬁdelities, including 25%, 50%, and 75% of the total
resource budget. In addition, we include a comparison with
vanilla BO that can be viewed as using 100% resource bud-
get as the ﬁxed ﬁdelity for all conﬁgurations.
We provide the results on three representative datasets
in Fig. 3(a), with more results available in Supp. 11.5.W e
have three main observations. Firstly, FastBO always out-
performs the partial evaluation baselines that use a ﬁxed ﬁ-
delity, indicating the effectiveness of the adaptive strategy.
Secondly, FastBO shows stronger performance than vanilla
BO. The limitation of vanilla BO lies in the additional time
required for full evaluations. Secondly, compared to the
vanilla BO, partial evaluation schemes with ﬁxed ricon-
verge faster in the initial stage due to their ability to evaluate
more conﬁgurations promptly, but this advantage is gradu-
ally offset over time because they fail to ﬁnd appropriate
ﬁdelities to create an accurate surrogate model. This causes
a suboptimal ﬁnal performance compared to vanilla BO, as
shown in the ﬁrst two ﬁgures in Figs. 3(a). In the case of the
last one, we can observe a noticeable upward trend exhib-
ited by the vanilla BO towards the end of the evaluation, in-
dicating its potential to improve the ﬁnal performance given
abundant time. The comparison between the partial evalua-
tion baselines and vanilla BO also demonstrates the impor-
tance of our adaptive strategy, which ensures that the ﬁdeli-
ties align optimally with each conﬁguration.
5.4. Generality of The Proposed Extension Method
The adaptive ﬁdelity identiﬁcation strategy provides a sim-
ple way to extend single-ﬁdelity methods to the multi-
ﬁdelity setting, as discussed in § 4.4. To examine the abil-
ity of our extension method, we conduct experiments using
three popular single-ﬁdelity methods CQR [ 37], BORE [ 45]and REA [ 35], extending them to the multi-ﬁdelity variants
with our extension method, referred to as FastCQR, Fast-
BORE, and FastREA respectively. Similar to FastBO, all
the multi-ﬁdelity extensions evaluate the conﬁgurations to
the adaptively identiﬁed efﬁcient point and use the corre-
sponding performances for the subsequent operations. The
results on three datasets are illustrated in Fig. 3(b) and
similar results on other datasets are in Supp. 11.6.W e
can clearly observe that the multi-ﬁdelity variants with our
extension method always outperform their single-ﬁdelity
counterparts. It is worth noting that REA is an evolu-
tionary algorithm-based HPO method and is also signiﬁ-
cantly improved by our extension. The observation high-
lights the ability of the proposed adaptive strategy to extend
any single-ﬁdelity method to the multi-ﬁdelity setting. It
also suggests future opportunities to extend other advanced
single-ﬁdelity techniques into the multi-ﬁdelity setting.
6. Conclusion
In this paper, we propose a model-based multi-ﬁdelity HPO
method FastBO, which adaptively identiﬁes the appropriate
ﬁdelity for each conﬁguration to ﬁt the surrogate model and
offers high-quality performance while ensuring efﬁcient re-
source utilization. The advantages are achieved through
our concepts of efﬁcient and saturation point, the proposed
techniques of learning curve modeling, and well-designed
warm-up and post-processing stages with judicious early-
termination detection and efﬁcient saturation-level evalua-
tion. Moreover, the proposed adaptive ﬁdelity identiﬁcation
strategy provides a simple way to extend any single-ﬁdelity
method to the multi-ﬁdelity setting. Experiments demon-
strate the effectiveness and wide generality of our pro-
posed techniques. FastBO source code is freely available
athttps://github.com/jjiantong/FastBO .
Acknowledgment
This research was funded by ARC Grant number
DP190102443. Ajmal Mian is the recipient of an Australian
Research Council Future Fellowship Award (project num-
ber FT210100268) funded by the Australian Government.
26188
References
[1]Sebastian E Ament and Carla P Gomes. Scalable ﬁrst-order
Bayesian Optimization via structured automatic differenti-
ation. In International Conference on Machine Learning ,
pages 500–516. PMLR, 2022. 2
[2]James Bergstra, R ´emi Bardenet, Yoshua Bengio, and Bal ´azs
K´egl. Algorithms for hyper-parameter optimization. Ad-
vances in Neural Information Processing Systems , 24, 2011.
1,2
[3]Bernd Bischl, Martin Binder, Michel Lang, Tobias Pielok,
Jakob Richter, Stefan Coors, Janek Thomas, Theresa Ull-
mann, Marc Becker, Anne-Laure Boulesteix, et al. Hyper-
parameter optimization: Foundations, algorithms, best prac-
tices, and open challenges. Wiley Interdisciplinary Reviews:
Data Mining and Knowledge Discovery , 13(2):e1484, 2023.
2
[4]Ondrej Bohdal, Lukas Balles, Martin Wistuba, Beyza Ermis,
C´edric Archambeau, and Giovanni Zappella. PASHA: ef-
ﬁcient HPO and NAS with progressive resource allocation.
InInternational Conference on Learning Representations .
OpenReview.net, 2023. 1,2,6,7,4,9
[5]Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia
Rudin, and Jonathan K Su. This looks like that: deep learn-
ing for interpretable image recognition. Advances in Neural
Information Processing Systems , 32, 2019. 2
[6]Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree
boosting system. In Proceedings of the 22nd acm sigkdd
international conference on knowledge discovery and data
mining , pages 785–794, 2016. 9
[7]Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter.
Speeding up automatic hyperparameter optimization of deep
neural networks by extrapolation of learning curves. In In-
ternational Joint Conference on Artiﬁcial Intelligence , 2015.
4
[8]Xuanyi Dong and Yi Yang. NAS-Bench-201: Extending the
scope of reproducible neural architecture search. In Interna-
tional Conference on Learning Representations , 2020. 1,6,
8
[9]Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter.
Neural architecture search: A survey. The Journal of Ma-
chine Learning Research , 20(1):1997–2017, 2019. 1
[10] Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB:
Robust and efﬁcient hyperparameter optimization at scale.
InInternational Conference on Machine Learning , pages
1437–1446. PMLR, 2018. 1,2,6,9
[11] Matthias Feurer and Frank Hutter. Hyperparameter opti-
mization. Automated Machine Learning: Methods, Systems,
Challenges , pages 3–33, 2019. 1
[12] Peter I Frazier, Warren B Powell, and Savas Dayanik. A
knowledge-gradient policy for sequential information col-
lection. SIAM Journal on Control and Optimization , 47(5):
2410–2439, 2008. 2
[13] Jos´e Miguel Hern ´andez-Lobato, Matthew W Hoffman, and
Zoubin Ghahramani. Predictive entropy search for efﬁcient
global optimization of black-box functions. Advances in
Neural Information Processing systems , 27, 2014. 2[14] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown.
Sequential model-based optimization for general algorithm
conﬁguration. In Learning and Intelligent Optimization ,
pages 507–523. Springer, 2011. 1,2
[15] Carl Hvarfner, Danny Stoll, Artur L. F. Souza, Marius Lin-
dauer, Frank Hutter, and Luigi Nardi. $ \pi$BO: Augmenting
acquisition functions with user beliefs for bayesian optimiza-
tion. In International Conference on Learning Representa-
tions . OpenReview.net, 2022. 2
[16] Kevin Jamieson and Ameet Talwalkar. Non-stochastic best
arm identiﬁcation and hyperparameter optimization. In Ar-
tiﬁcial Intelligence and Statistics , pages 240–248. PMLR,
2016. 1,2,9
[17] Kasra Jamshidi, Harry Xu, and Keval V ora. Accelerating
graph mining systems with subgraph morphing. In European
Conference on Computer Systems , pages 162–181, 2023. 2
[18] Jiantong Jiang, Zeyi Wen, Zeke Wang, Bingsheng He, and
Jian Chen. Parallel and distributed structured svm training.
IEEE Transactions on Parallel and Distributed Systems , 33
(5):1084–1096, 2021.
[19] Jiantong Jiang, Zeyi Wen, and Ajmal Mian. Fast parallel
bayesian network structure learning. In IEEE International
Parallel and Distributed Processing Symposium , pages 617–
627. IEEE, 2022. 2
[20] Arlind Kadra, Maciej Janowski, Martin Wistuba, and Josif
Grabocka. Scaling laws for hyperparameter optimization. In
Advances in Neural Information Processing Systems , 2023.
2
[21] Aaron Klein and Frank Hutter. Tabular benchmarks for
joint architecture and hyperparameter optimization. arXiv
preprint arXiv:1905.04970 , 2019. 6,8
[22] Aaron Klein, Louis C Tiao, Thibaut Lienart, Cedric Archam-
beau, and Matthias Seeger. Model-based asynchronous hy-
perparameter and neural architecture search. arXiv preprint
arXiv:2003.10865 , 2020. 1,2,6,9
[23] Prasanth Kolachina, Nicola Cancedda, Marc Dymetman, and
Sriram Venkatapathy. Prediction of learning curves in ma-
chine translation. In Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , pages
22–30, 2012. 2
[24] Cheng Li, Santu Rana, Sunil Gupta, Vu Nguyen, Svetha
Venkatesh, Alessandra Sutti, David Rubin de Celis Leal, Teo
Slezak, Murray Height, Mazher Mohammed, and Ian Gib-
son. Accelerating experimental design by incorporating ex-
perimenter hunches. In International Conference on Data
Mining , pages 257–266. IEEE Computer Society, 2018. 2
[25] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Ros-
tamizadeh, and Ameet Talwalkar. Hyperband: A novel
bandit-based approach to hyperparameter optimization.
Journal of Machine Learning Research , 18(1):6765–6816,
2017. 1,2,6,9
[26] Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina
Gonina, Jonathan Ben-Tzur, Moritz Hardt, Benjamin Recht,
and Ameet Talwalkar. A system for massively parallel hy-
perparameter tuning. Proceedings of Machine Learning and
Systems , 2:230–246, 2020. 1,2,6,9
[27] Yang Li, Yu Shen, Huaijun Jiang, Wentao Zhang, Jixiang
Li, Ji Liu, Ce Zhang, and Bin Cui. Hyper-tune: Towards
26189
efﬁcient hyper-parameter tuning at scale. Proceedings of the
VLDB Endowment , 15(6):1256–1265, 2022. 1,2,6,9
[28] Jonas Mockus. The application of Bayesian methods for
seeking the extremum. Towards global optimization , 2:117,
1998. 2
[29] Felix Mohr and Jan N van Rijn. Learning curves for deci-
sion making in supervised machine learning–a survey. arXiv
preprint arXiv:2201.12150 , 2022. 4,2
[30] Felix Mohr, Tom J Viering, Marco Loog, and Jan N van Rijn.
Lcdb 1.0: An extensive learning curves database for classi-
ﬁcation tasks. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases , pages 3–
19. Springer, 2022. 2
[31] Julia Moosbauer, Julia Herbinger, Giuseppe Casalicchio,
Marius Lindauer, and Bernd Bischl. Explaining hyperpa-
rameter optimization via partial dependence plots. Advances
in Neural Information Processing Systems , 34:2280–2291,
2021. 2
[32] Julia Moosbauer, Giuseppe Casalicchio, Marius Lindauer,
and Bernd Bischl. Improving accuracy of interpretability
measures in hyperparameter optimization via Bayesian al-
gorithm execution. arXiv preprint arXiv:2206.05447 , 2022.
2
[33] ChangYong Oh, Efstratios Gavves, and Max Welling.
BOCK: Bayesian optimization with cylindrical kernels.
InInternational Conference on Machine Learning , pages
3868–3877. PMLR, 2018. 2
[34] Misha Padidar, Xinran Zhu, Leo Huang, Jacob Gardner, and
David Bindel. Scaling gaussian processes with derivative
information using variational inference. Advances in Neural
Information Processing Systems , 34:6442–6453, 2021. 2
[35] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V
Le. Regularized evolution for image classiﬁer architecture
search. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence , pages 4780–4789, 2019. 8,5,9
[36] David Salinas, Matthias Seeger, Aaron Klein, Valerio Per-
rone, Martin Wistuba, and Cedric Archambeau. Syne tune:
A library for large scale hyperparameter tuning and repro-
ducible research. In International Conference on Automated
Machine Learning , pages 16–1. PMLR, 2022. 6,9
[37] David Salinas, Jacek Golebiowski, Aaron Klein, Matthias W.
Seeger, and C ´edric Archambeau. Optimizing hyperpa-
rameters with conformal quantile regression. In Inter-
national Conference on Machine Learning , pages 29876–
29893. PMLR, 2023. 1,2,6,8,5,9
[38] Bobak Shahriari, Alexandre Bouchard-C ˆot´e, and Nando Fre-
itas. Unbounded Bayesian Optimization via regularization.
InArtiﬁcial intelligence and statistics , pages 1168–1176.
PMLR, 2016. 2
[39] Julien Siems, Lucas Zimmer, Arber Zela, Jovita Lukasik,
Margret Keuper, and Frank Hutter. Nas-bench-301 and the
case for surrogate benchmarks for neural architecture search.
CoRR , abs/2008.09777, 2020. 1,6,3
[40] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practi-
cal Bayesian optimization of machine learning algorithms.
Advances in Neural Information Processing Systems , 25,
2012. 1,2,6,9[41] Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Na-
dathur Satish, Narayanan Sundaram, Mostofa Patwary, Mr
Prabhat, and Ryan Adams. Scalable Bayesian optimization
using deep neural networks. In International Conference on
Machine Learning , pages 2171–2180. PMLR, 2015. 2
[42] Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and
Frank Hutter. Bayesian optimization with robust bayesian
neural networks. Advances in Neural Information Process-
ing Systems , 29, 2016. 2
[43] Niranjan Srinivas, Andreas Krause, Sham M Kakade, and
Matthias Seeger. Gaussian process optimization in the bandit
setting: No regret and experimental design. arXiv preprint
arXiv:0912.3995 , 2009. 2
[44] Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams.
Freeze-thaw Bayesian optimization. arXiv preprint
arXiv:1406.3896 , 2014. 1
[45] Louis C Tiao, Aaron Klein, Matthias W Seeger, Edwin V
Bonilla, Cedric Archambeau, and Fabio Ramos. BORE:
Bayesian optimization by density-ratio estimation. In In-
ternational Conference on Machine Learning , pages 10289–
10300. PMLR, 2021. 8,5,9
[46] Tom Viering and Marco Loog. The shape of learning curves:
a review. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence , 2022. 1,4,2
[47] Jiazhuo Wang, Jason Xu, and Xuejun Wang. Combina-
tion of hyperband and bayesian optimization for hyper-
parameter optimization in deep learning. arXiv preprint
arXiv:1801.01596 , 2018. 1,2
[48] Martin Wistuba, Arlind Kadra, and Josif Grabocka. Super-
vising the multi-ﬁdelity race of hyperparameter conﬁgura-
tions. Advances in Neural Information Processing Systems ,
35:13470–13484, 2022. 2,6,9
[49] Jian Wu, Matthias Poloczek, Andrew G Wilson, and Peter
Frazier. Bayesian optimization with gradients. Advances in
neural information processing systems , 30, 2017. 2
[50] Peiyu Yang, Naveed Akhtar, Zeyi Wen, Mubarak Shah, and
Ajmal Mian. Re-calibrating feature attributions for model in-
terpretation. In International Conference on Learning Rep-
resentations , 2022. 2
[51] Peiyu Yang, Naveed Akhtar, Zeyi Wen, and Ajmal Mian.
Local path integration for attribution. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence , pages 3173–
3180, 2023. 2
[52] Xueying Zhu, Jie Sun, Zhenhao He, Jiantong Jiang, and
Zeke Wang. Staleness-reduction mini-batch k-means. IEEE
Transactions on Neural Networks and Learning Systems ,
2023. 2
[53] Lucas Zimmer, Marius Thomas Lindauer, and Frank Hutter.
Auto-Pytorch: Multi-ﬁdelity metalearning for efﬁcient and
robust AutoDL. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 43:3079–3090, 2021. 6,8
26190
