Zero-Painter: Training-Free Layout Control for Text-to-Image Synthesis
Marianna Ohanyan1∗Hayk Manukyan1∗Zhangyang Wang1,2
Shant Navasardyan1Humphrey Shi1,3
1Picsart AI Research (PAIR)2UT Austin3Georgia Tech
https://github.com/Picsart-AI-Research/Zero-Painter
Figure 1. Embark on a visual journey with Zero-Painter: a novel training-free framework for layout-conditional text-to-image generation.
This new pipeline brings images to life using object masks and individual descriptions, seamlessly fused with a powerful global text prompt.
Abstract
We present Zero-Painter, a novel training-free frame-
work for layout-conditional text-to-image synthesis that fa-
cilitates the creation of detailed and controlled imagery
from textual prompts. Our method utilizes object masks
and individual descriptions, coupled with a global text
prompt, to generate images with high fidelity. Zero-Painter
employs a two-stage process involving our novel Prompt-
Adjusted Cross-Attention (PACA) and Region-Grouped
*Equal contribution.Cross-Attention (ReGCA) blocks, ensuring precise align-
ment of generated objects with textual prompts and mask
shapes. Our extensive experiments demonstrate that Zero-
Painter surpasses current state-of-the-art methods in pre-
serving textual details and adhering to mask shapes.
1. Introduction
Recent innovations in generative AI have revolutionized the
creative landscape, allowing the generation of strikingly re-
alistic images [35, 38, 42] or videos [4, 5, 14, 18] from text.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
8764
However, crafting detailed prompts to guide every aspect
of an image can be cumbersome and time-intensive. Fur-
thermore, traditional text-to-image models often falter when
faced with intricate prompts that describe multiple objects
and their respective attributes. To address these challenges,
layout-conditional text-to-image models have been devel-
oped that leverage additional inputs such as segmentation
masks [1–3, 22] or bounding boxes [22, 28, 57] together
with text. This approach facilitates the creation of images
with precise attributes, granting artists and designers gran-
ular control over the visual components.
Early iterations of layout-conditional text-to-image
methods [9, 50], employing GANs [11] and diffusion mod-
els [15, 44], achieved remarkable results using a closed
vocabulary. However, their reliance on fixed class la-
bels restricted their ability to prompt free-form attributes
for the object in the layout. The introduction of GLI-
GEN [22] marked a significant advancement with its open-
vocabulary and a multitude of new control mechanisms,
including bounding box and text pairs, keypoints, edges,
depth, and class-based segmentation maps. Later eDiff-
I [2] introduced the Paint-With-Words approach, allowing
open-vocabulary prompting with free-form mask control.
Subsequently, MultiDiffusion [3] was introduced, capable
of processing local prompts independently from the global
prompt, adding flexibility. While these methods can gen-
erate visually convincing and prompt-aligned results, they
are not always capable of keeping the shapes of the objects
inside the mask. Despite this innovation, the absence of
explicit mask conditioning often resulted in discrepancies
between the shapes of generated objects and the provided
masks.
To overcome these challenges, we present Zero-Painter,
an innovative training-free method for layout-conditional
text-to-image synthesis. It generates images from object
masks and individual descriptions, alongside a global text
prompt, as showcased in Fig. 1. Our process is bifur-
cated into two stages: initially, we generate individual
objects, each endowed with unique attributes, using our
Prompt-Adjusted Cross-Attention (PACA) module. These
objects are then seamlessly integrated into a single scene
through our Region-Grouped Cross-Attention (ReGCA)
block. This ensures the generated objects not only align
with the prompts but also conform to the shapes of the
provided masks. Through rigorous testing, we have found
that Zero-Painter surpasses state-of-the-art methods, partic-
ularly in maintaining the textual integrity of individual ob-
jects and adhering to the shapes of the given masks.
In summary, our contributions are as follows:
• We introduce Zero-Painter, a novel training-free frame-
work for layout-conditional text-to-image synthesis, en-
abling the generation of objects with specified shapes and
distinct attributes.• We unveil the Prompt-Adjusted Cross-Attention (PACA)
and Region-Grouped Cross-Attention (ReGCA) blocks,
which significantly improve the shape fidelity and char-
acteristic preservation of the generated objects.
• Comprehensive experiments validate Zero-Painter’s su-
periority over existing state-of-the-art methods, as evi-
denced through both quantitative and qualitative compar-
isons.
2. Related Work
2.1. Text-to-Image Generation
Recently, significant advancements have occurred in the
field of text-to-image generation. Approaches based on
Generative Adversarial Networks (GANs) [12, 30, 53, 61]
have yielded promising results in constrained domains.
With the rise of transformer [49] models, zero-shot open-
domain models were introduced. Notably, both Dall-E [34]
and VQ-GAN [8] propose a two-stage approach. Initially,
they employ a discrete Variational Autoencoder (V AE)
[19, 37] to discover a comprehensive semantic space.
Later, Parti [60] illustrates the practicality of expanding
autoregressive models in terms of scalability.
With the introduction of Diffusion-based models [38],
the quality of text-to-image generation has significantly im-
proved. DALL-E 2 [36] utilizes CLIP [32] for the text-to-
image mapping process through diffusion mechanisms and
trains a CLIP decoder. Furthermore, Imagen [42] lever-
ages large pre-trained language models like T5 on textual
data [33], achieving superior alignment between images and
text, as well as enhanced sample fidelity. Lastly, eDiff-I
[2] employs an expert-based approach, with different expert
models handling generation at various timestep ranges.
2.2. Layout-to-Image Generation
Past research focused on image generation from structured
layouts with fixed classes for content control [10, 13, 17, 25,
29, 46, 54]. CLIP [31] marked a paradigm shift by introduc-
ing zero-shot learning and enabling a transition from fixed
to free-form text control. Recent advancements, including
No-token-left-behind [28] and Gligen [22], proposed meth-
ods incorporating free-form text and bounding boxes, while
Make-A-Scene [9] presented an innovative approach with a
fixed set of labels but free-form mask-based control. Later
approaches like Spa-text [1], eDiff-I [2], and MultiDiffu-
sion [3] combine elements of free-form text and masks to
broaden the scope of image generation.
2.3. Text-Guided Image Inpainting
The problem of image inpainting is known in the commu-
nity and has been tackled in numerous works: [12, 21, 24,
26, 27, 27, 45, 55, 58, 59, 59, 62, 63]. With the recent
8765
rise of text-guided generative models, the text-guided ver-
sion of image-inpainting has become relevant. Research
in this field is progressing rapidly, and works like Smart-
Brush [52], Imagen Editor [51], and Uni-paint [56] present
significant advancements. Most notably, Stable Inpainting
[38] is a modification of the Stable Diffusion model, that
is fine-tuned for text-guided inpainting. The base model is
enhanced by concatenating the input image and inpainting
mask as additional conditioning to the UNet model’s latent
input. The weights of the additional channels have been
initialized with zeros, and fine tuned on the LAION [43]
dataset using randomly generated inpainting masks.
3. Method
In this section we introduce Zero-Painter, a training-
free layout-conditional text-to-image generation frame-
work. First, we provide a brief background on diffusion
models, focusing on Stable Diffusion (SD) [38]. Then,
we present an overview of Zero-Painter, our two-stage
pipeline that encompasses Single Object Generation (SOG)
and Comprehensive Composition (CC). We delve into each
stage, highlighting the design of Prompt-Adjusted Cross-
Attention (PACA) and Region-Grouped Cross-Attention
(ReGCA) layers for improved shape alignment and char-
acteristic preservation.
3.1. Stable Diffusion
Stable Diffusion [38] is an LDM that works in the latent
space of VQ-V AE [48] (or VQ-GAN [8] for the original
LDM). During the diffusion process, Gaussian noise is iter-
atively added to the input latent tensor x0∈Rh×w×c, such
that the conditional distribution q(xt|xt−1)is:
q(xt|xt−1) =N(xt;p
1−βtxt−1, βtI), t= 1, .., T (1)
where {βt}T
t=1are hyperparameters, and Tis large enough
thatxTbecomes very close to N(0, I). By unraveling the
process and denoting αt=Qt
i=1(1−βi)one can get a
direct formula for xt:
xt=√αtx0+√
1−αtϵ, ϵ∼N(0,1). (2)
The objective of SD is to learn a backward process
pθ(xt−1|xt) =N(xt−1;µθ(xt, t),Σθ(xt, t)),(3)
where µθ(xt, t),Σθ(xt, t)are parametric learnable func-
tions (for simplicity usually Σθ(xt, t) =diag(σ2)for a hy-
perparameter σ). Later, sampling xT∼ N (0, I)and per-
forming the backwards process for t=T, . . . , 1allows the
generation of valid images, where final latent x0has to be
decoded using the decoder D(x0)of the V AE.An alternative deterministic sampling approach was pro-
posed in [44] called DDIM:
xt−1=√αt−1xt−√1−αtϵt
θ(xt)√αt
+
p
1−αt−1ϵt
θ(xt), t=T, . . . , 1,(4)
where
ϵt
θ(xt) =√1−αt
βtxt+(1−βt)(1−αt)
βtµθ(xt, t).(5)
For Stable Diffusion the function ϵt
θ(xt, τ)is directly
predicted using a UNet. For text-to-image generation the
UNet is altered by adding Cross-Attention layers, and an
additional input textual prompt τis provided:
xt−1=√αt−1xt−√1−αtϵt
θ(xt, τ)√αt
+
p
1−αt−1ϵt
θ(xt, τ), t=T, . . . , 1.(6)
3.2. Zero-Painter
The problem of layout-conditional text-to-image generation
can be formulated as follows: Given a layout as a set of bi-
nary masks Mi∈ {0,1}H×W, i= 1, . . . , n , indicating the
shapes and positions of individual objects in a desired im-
age, with corresponding textual prompts τidescribing each
object separately, as well as a global prompt τglobal describ-
ing the image in its entirety, the goal is to generate an out-
put image I∈RH×Wmatching τglobal while containing
the objects following the shapes and positions of the layout
{Mi}n
i=1, and the prompts {τi}n
i=1.
To this end, Zero-Painter introduces an optimization-free
two-stage pipeline, enabling the independent generation of
objects followed by their seamless composition into a sin-
gle image. This two-stage approach gives an advantage of
utilizing the whole capacity of a diffusion model on single
object generation resulting in better shape and characteris-
tics alignment than current one-stage methods (see Fig. 9).
During the first stage, we leverage PACA layer to indi-
vidually generate images Ii, i= 1, . . . , n , each contain-
ing a single object on a flat background that follows the
shape/position of the binary mask Miand matches the de-
scription τi(see Fig. 2). The use of flat backgrounds aids
in the easy identification and segmentation of the generated
objects, especially if they differ slightly from the original
mask.
The second stage takes the generated images I1, . . . , I n,
and combines them according to the global prompt τglobal
and the individual mask-prompt pairs (Mi, τi). To coher-
ently combine the generated objects, we first separate them
from their backgrounds by utilizing the Segment Anything
Model (SAM) [20] and obtaining new object masks M′
i∈
8766
"tabby cat "
"red apple ""Brown tabby cat
on white stairs "
"blue vase "
CombineSAMGlobal
prompt:
Layout:
Self-Attn ReGCA FFN Convolution
Transformer BlockInpainting Model
Self-Attn PACA FFN Convolution
Transformer BlockT2I Model
Concatenate
Figure 2. Optimization-Free Two-Stage Pipeline for Zero-Shot Image Composition: (a) In the first stage, we focus on single object
generation, leveraging the innovative Prompt-Adjusted Cross-Attention (PACA) layer. (b) Moving to the comprehensive composition
stage, we introduce the Region-Grouped Cross-Attention (ReGCA) block, facilitating seamless and dynamic composition of generated
objects.
{0,1}H×W. Note that using the initial mask Mifor separat-
ing the foreground object in Iimay cause undesirable object
cuts if the generated object shape has a slight mismatch with
the given layout mask Mi(see Fig. 7). Then we leverage
Stable Inpainting [38] modified with our Region-Grouped
Cross-Attention (ReGCA) module to seamlessly combine
the generated objects and fill the background region indi-
cated by the mask M′
bg= (1−M′
1)⊙. . .⊙(1−M′
n). The
inpainting is done with the textual guidance of τglobal .
3.3. Single Object Generation (SOG)
The goal of the SOG stage is to produce an image from
a binary mask Miand textual description τi, ensuring the
generated object’s shape matches Miand its description
matches τi. The rest of the image is filled with a flat back-
ground for easier separation of the generated objects later.
We use a pre-trained Stable Diffusion (SD) [38] model to
generate an image with a specified pair ( Mi,τi). [2] shows
that the high timesteps in the diffusion process are mostly
responsible for creating the object silhouette, while later
steps create details and refine the object. Since in our case
the target shape is known and described by Mi, we start the
diffusion backward process from an intermediate timestep
T′< T, and provide the shape information through a start-
ing latent xT′obtained using the mask Mi. To construct
xT′we consider two factors: (i)the background of the final
image should be of a flat color; (ii)the foreground object
should be constrained to Mi. We first obtain the latent code
of the flat background corresponding to timestep T′by ap-
plying noise on a latent encoding of a constant black image:
xflat
T′=√αT′E(Iflat) +√
1−αT′ϵ, (7)
where Iflatis the constant black image, E()is the V AE en-
coder and ϵ∼ N(0,1)is sampled from the standard gaus-
sian distribution.
We initialize the foreground region MiofxT′from a
sampled Gaussian noise ϵ∼N(0,1). While this devi-
ates from the value expected according to Eq. (2), we findthat providing any specific value for x0may encourage the
model to generate an image similar to x0, introducing un-
necessary characteristics. Furthermore, for sufficiently high
values of T′the noise to signal ratio is so high, that ϵbe-
comes a good enough approximation, and the deviation re-
mains within the tolerance of the model. To further miti-
gate potential quality loss, we add a refinement sub-stage to
object composition stage of Zero-Painter, aiming to correct
any remaining errors in the generated image.
In summary, the starting latent xT′for the single object
generation stage is defined as
xT′= (1−Mi)⊙xflat
T′+Mi⊙ϵ, (8)
where Miis the mask, and ϵ∼ N(0,1)is randomly sam-
pled from the standard gaussian distribution.
After we get the initial latent xT′we apply the DDIM
backward process for t=T′, . . . , 1by leveraging SD,
enhanced with our Prompt-Aware Cross-Attention (PACA)
layers designed for the individual object shape alignment
withMiand coherence with the prompt τi:
xpred
0(t) =xt−√1−αtϵt
θ(xt)√αt,
xt−1=√αt−1xpred
0(t) +p
1−αt−1ϵt
θ(xt),(9)
where ϵt
θ()is SD augmented with PACA layers.
To further ensure that the object does not extend outside
of the masked area, we blend the noised latent of the flat
background xflat
tand the predicted xtat every timestep t=
T′, . . . , 0:
xt=Mi⊙xt+ (1−Mi)⊙xflat
t, (10)
3.3.1 Prompt-Aware Cross-Attention (PACA)
The PACA layer (Fig. 5) plays a crucial role in the SOG
process. It ensures accurate object generation based on tex-
tual descriptions and masks, while preventing generation
8767
Figure 3. Effect of the SOT token. The similarity with the SOT
token has been increased during text-to-image generation (at ev-
ery step) in the non-white areas of the masks (left side). Prompt:
”photo of a red apple, centered”.
(a) Output
 (b) SOT Token
 (c) Other Tokens
Figure 4. Similarity of the SOT token vs all other tokens com-
bined.
Figure 5. Overview of Prompt-Aware Cross-Attention(PACA)
during the Invdividual Object Generation stage.
outside the masked area. For our PACA layer we employ
a mechanism similar to eDiff-I [2], i.e. by modifying the
cross-attention similarity matrix S. To encourage the gen-
eration of the object inside the masked area, we increase
similarity values of queries qj, j∈Micorresponding to
the masked area Mi, with the keys of all prompt tokens
excluding the SOT. We exclude the SOT since we noticed
that during vanilla text-to-image generation increasing the
similarity of a pixel with the SOT token results in the out-
put pixel becoming a generic background pixel (see Figs. 3
and 4). For the same reason, we increase the similarity val-
ues of non-masked pixel qj, j /∈Miwith the SOT token.
Therefore, the similarity matrix Sof selected cross atten-
tion layers is modified as follows:
S′
j=(
Sj+wtPN
k=11kifj∈Mi
Sj+wt 10 otherwise(11)
Figure 6. Left to right: the cobined mask used for inpainting, the
combined input image, the resulting comprehensive composition
and an overlay with original input layout.
Figure 7. Example to illustrate importance of SAM. Left to right:
original mask, output of Single Object Generation, output cropped
using the original mask, output cropped using the mask adjusted
by SAM.
where Sjis the column of the similarity matrix corre-
sponding to pixel with index j, 1k= [0...1...0]is an indi-
cator vector and Nis the index of the EOT token, accord-
ingly, index 0 is the SOT token. Inspired by [2] we choose
wt=w′log(1 + σt) max ( QKT), where σtis the noise-to-
signal ratio and w′is a hyperparameter.
3.4. Comprehensive Composition (CC)
The CC phase aims to combine all previously generated
objects into a single image that fits the description of the
global prompt τglobal. In 3.4.1 we describe how to perform
object segmentation for each individual object. In 3.4.2
- the process of inpainting the background region and in
3.4.3 - details concerning Region-Grouped Cross-Attention
(ReGCA).
3.4.1 Object Segmentation
Objects from the SOG process may not precisely follow the
input mask Mi, especially with hand-drawn masks. Extract-
ing objects using the original mask may introduce back-
ground segments in the CC stage (see Fig. 7). To address
this, we perform object segmentation on the output images,
completely separating the generated object from the image.
Using a pre-trained Segment-Anything-Model [20] with the
bounding box of the original mask as input, we find the in-
tersection of the output and original masks.
ˆMi=SAM (x0|bbox(Mi))∗Mi (12)
where bbox(Mi), is the minimal bounding box spanning the
mask Mi(in[x, y, w, h ]format).
8768
Figure 8. Region-Grouped Cross-Attention (ReGCA) architec-
ture.
3.4.2 Inpainting
We combine all the components using a pre-trained Sta-
ble Inpainting model [38]. Rather than creating a new ob-
ject in the masked region, we utilize the model to syn-
thesize a background around the existing ones. We con-
struct the input image for the inpainting model by combin-
ingxi
0generated during SOG using predicted masks ˆMi:
xknown =P
ixi
0ˆMi. Similarly we define the combined
mask: M= 1−P
iˆMi.
Similar to Sec. 3.3, we choose a starting step T′′< T ,
discouraging the model from generating new objects, and
prompting it to focus on generating a background. Addi-
tionally, to better maintain the structural coherence of pre-
existing objects, we initialize the initial latent noise xT′′
within the known region as the noised latent xknown
T′′of the
input image xknown.
xknown
T′′=√αt−1xknown+p
1−αt−1ϵ1 (13)
xT′′=Mxknown
T′′+ (1−M)ϵ2 (14)
where ϵ1andϵ2are random noise vectors sampled from
N(0,1)
We perform DDIM iterations from timestep T′′down to
a minimum timestep tminusing the mask M. For the last
tmin timesteps, we increase the mask to cover the entire
image, allowing the model to fine-tune the known region
together with the newly generated regions, to obtain a more
homogeneous final image.
3.4.3 Region-Grouped Cross-Attention (ReGCA)
Similar to PACA, the ReGCA (Fig. 8) layer is crucial for
obtaining a coherent image after inpainting. We modify
cross-attention layers for two purposes: first, we use neg-
ative prompts to prohibit the model from generating exist-
ing objects outside the known region. Second, we ensure
the model receives sufficient information about existing ob-
jects through cross-attention values, even when the corre-sponding object prompts are missing from the global cap-
tion. To achieve this, we divide the pixels of the latent
vector into groups, based on which object, or background
they belong to (see Fig. 8). For the object with index o,
mask ˆMoand prompt Towe select the subset of queries
qo
i={qi|i∈ˆMo}. For each group we compute its own
set of key-value pairs ko
j,vo
j, as well as their unconditional
counterparts ˜ko
j,˜vo
j.ko
j,vo
jare computed from the tokens
of the object prompt To, while ˆKkandˆVk- using an empty
string.
We add an additional group for the pixels belonging to
the background qbg
i={qi|i /∈ˆMo∀o}. For this group,
we use the global prompt τglobal for computing kbg
jand
vbg
j, while for ˜kbg
j,˜vbg
jwe construct a new prompt from
the comma separated concatenation of all object prompts
Tuc
bg=∪To. Using a non-empty prompt with the uncon-
ditional model serves as a negative prompt, preventing the
generation of existing objects in the background area.
After individually computing the cross attention outputs
for each group, they are combined into a single output by
putting pixels from each output into their corresponding po-
sitions in the original input.
4. Experiments
4.1. Implementation details
Our implementation is based on the ”Stability-AI” GitHub
repository [41]. We use pre-trained weights of Stable Dif-
fusion 1.4 [39] and Stable-Inpainting 1.5 [40] from the hug-
gingface repository. For starting timesteps we use T′, T′′=
800andtmin= 100 . We use 40 DDIM steps (we skip 10
steps due to T′= 800 ).
4.2. Quantitative Results
To assess our model and compare it with other state-of-the-
art models, we created a validation set consisting of 3000
MSCOCO [23] segmentation layouts. We filtered out masks
that have an area <5%of the image, and resize all layouts
to512x512.
We compared our model with existing layout2image ap-
proaches, that have publicly available repositories: eDiff-I
using Stable Diffusion 1.4 [2, 6], Multidiffusion [3, 47], as
well as bounding-box based approaches: Gligen [7, 22] and
NTLB [16, 28]. For the latter, we used the bounding boxes
of the corresponding masks as input. We use Local CLIP-
Score to compute text-alignment: we crop the image using
bounding boxes of each mask and compute the CLIP Score
with the corresponding object prompt.
SCLIP(I, C) =
1
nnX
i=1max 
100·cos 
Eimg 
Icrop
i
, Etxt(τi)
,0
(15)
8769
"Room"
"brown wooden
stools"
"red stools""Cars on road
during daytime"
"red Porsche 911"
"blue Porsche
911""Living room"
"Orange flowers"
"blue ceramic
vase""tropical and
subtropical
regions of
Africa"
"an elephant"
"yellow minivan""Dining room"
"table"
"red chair"
"blue chair"
"yellow flower
photo""Brown tabby cat
on white stairs
photo"
"Brown tabby cat"
"blue vase"
"red apple"T ext
Figure 9. Qualitative comparison between the Zero-Painter pipeline and state-of-the-art models.
where τiis the object prompt, Icrop
iis the cropped region
of image Iusing the bounding box of MiandEimg,Etxt
correspond to extracting CLIP embedding. We measure
shape alignment using the average of Local Intersection
over Union (IoU). Utilizing a pre-trained SAM model [20]
on the cropped region, we determine the shape of the fi-
nal generated object and compute its IoU with the original
mask.
SIoU=1
nnX
i=1IoU(SAM (I|box(Mi)), Mi) (16)
where IoU =Area of Overlap
Area of Union,Iis generated image.As evident from Tab. 1 Zero-Painter outperforms other
state-of-the-art approaches.
4.3. Qualitative Results
To present a more detailed and visual comparison of our
model, we hand-crafted a smaller test-set using images from
Unsplash as the base of our layouts. For a more exten-
sive comparison with a larger image set, see the Appendix.
Meanwhile, Fig. 9 displays a subset of examples.
As mentioned above, the most common issue for com-
petitor models is the leakage of properties between differ-
ent objects, e.g. the colors of the chair in column 1, colors
8770
Figure 10. Ablation study.
of the cars in column 2. In addition, some objects can be
completely neglected, like the vase in column 6 and 3 for
Multidiff. We also note, that this particular implementation
of EDIFF-I might generate visual artifacts like in column
2 and 4. Similarly, Multidiff’s outputs can sometimes look
”mutated”, like the chairs in column 1.
In comparison, our model is less likely to struggle from
all aforementioned issues. Since the objects are gener-
ated individually using PACA module and later refined with
ReGCA, the properties of each object are much better pre-
served in the final image.
5. Ablation Study
In this section, we assess the significance of two presented
modules PACA and ReGCA. Results without PACA and
ReGCA modules are presented in Fig (column 2). PACA
ensures that the generated object tries to fill as much of the
mask as possible Fig (column 3). Moreover by increasing
the similarity with all tokens in the object prompt, PACA
ensures that the object properties are kept as closely as pos-
sible. For instance, consider the red apple in the 1st row, 3rd
column, which is generated using PACA. Notably, in the 1st
row, 2nd column, where ”red apple” is intended, the model
generated something similar to a ”red vase”. The impact of
PACA becomes more evident in the 3rd row, where, in its
absence, the model generates a gray blob instead of the in-
tended ”candies.” This deviation highlights PACA’s crucial
role in maintaining coherence between generated content
and textual description.
The CC stage faces challenges even when objects are ac-
curately generated in the PACA stage. When using a basic
inpainting pipeline, two issues arise. First, some objects
extend beyond their intended boundaries, disrupting visual
coherence (column 4, see the dog’s ears in row 4 and theModel EDIFF-I GLIGEN NTLB MDF Ours
CLIP (local) 25.3 25.52 25.71 26.10 26.68
IoU (local) 0.62 N/A 0.50 0.58 0.75
Table 1. Quantitative comparison.
Figure 11. Zero-Painter’s limitation in handling overlapping
masks.
vase in the 1st row). Second, inpainting with τglobal fails
when it lacks information about all objects, as seen in the
first row where the absence of the ”apple” prompt results
in a failed inpainting. ReGCA in column 5 prevents shape
continuation, addressing limitations due to missing object
prompts in τglobal.
6. Limitations
While Zero-Painter excels in generating detailed and con-
trolled images, there are still limitations. One such limi-
tation arises when dealing with overlapping masks. In in-
stances where masks intersect or overlap, the resulting im-
ages may exhibit unnatural or less visually coherent out-
comes (see Fig. 11). This challenge occurs during the
CC stage: since we are using ReGCA for inpainting only
the background region, objects inside masks remain un-
changed. Although Zero-Painter excels in many scenarios,
improving this limitation is an area for future enhancement.
7. Conclusion
To this end, we propose Zero-Painter that is a training-free
framework for layout-conditional text-to-image synthesis.
The method utilizes object masks, individual descriptions,
and a global text prompt, employing a two-stage process
with novel Prompt-Adjusted Cross-Attention (PACA) and
Region-Grouped Cross-Attention (ReGCA) blocks. These
advancements ensure precise alignment of generated ob-
jects with textual prompts and mask shapes. Extensive
experiments demonstrate that Zero-Painter’s ability to pre-
serve textual details and keeping shapes are superior to all
existing methods. The paper introduces innovative contri-
butions, including the novel framework, PACA and ReGCA
blocks, and comprehensive experimental validation.
8771
References
[1] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta,
Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried,
and Xi Yin. Spatext: Spatio-textual representation for con-
trollable image generation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 18370–18380, 2023. 2
[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,
Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Ait-
tala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Kar-
ras, and Ming-Yu Liu. ediff-i: Text-to-image diffusion
models with ensemble of expert denoisers. arXiv preprint
arXiv:2211.01324 , 2022. 2, 4, 5, 6
[3] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.
Multidiffusion: Fusing diffusion paths for controlled image
generation. arXiv preprint arXiv:2302.08113 , 2023. 2, 6
[4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel
Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,
Zion English, Vikram V oleti, Adam Letts, and et al. Sta-
ble video diffusion: Scaling latent video diffusion models to
large datasets. arXiv.org , 2023. 1
[5] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue,
Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luh-
man, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya
Ramesh. Video generation models as world simulators.
2024. 1
[6] cloneofsimo. Paint with words sd, 2023. 6
[7] GLIGEN Contributors. GLIGEN: Open-Set Grounded
Text-to-Image Generation. https://github.com/
gligen/GLIGEN , 2023. 6
[8] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
transformers for high-resolution image synthesis. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 12873–12883, 2021. 2, 3
[9] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,
Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-
based text-to-image generation with human priors, 2022. 2
[10] Vidit Goel, Elia Peruzzo, Yifan Jiang, Dejia Xu, Nicu
Sebe, Trevor Darrell, Zhangyang Wang, and Humphrey Shi.
Pair-diffusion: Object-level image editing with structure-
and-appearance paired diffusion models. arXiv preprint
arXiv:2303.17546 , 2023. 2
[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. Advances in
neural information processing systems , 27, 2014. 2
[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial networks. Commu-
nications of the ACM , 63(11):139–144, 2020. 2
[13] S. He, W. Liao, M. Yang, Y . Yang, Y . Song, B. Rosen-
hahn, and T. Xiang. Context-aware layout to image gen-
eration with enhanced object appearance. arXiv preprint
arXiv:2103.11897 , 2021. 2
[14] Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan,
Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang,Shant Navasardyan, and Humphrey Shi. Streamingt2v: Con-
sistent, dynamic, and extendable long video generation from
text. arXiv preprint arXiv:2403.14773 , 2024. 1
[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems , 33:6840–6851, 2020. 2
[16] Apple Inc. ml-no-token-left-behind. https://github.
com/apple/ml-no-token-left-behind , 2023. 6
[17] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adver-
sarial networks. CVPR , 2017. 2
[18] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-
vosyan, Roberto Henschel, Zhangyang Wang, Shant
Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-
image diffusion models are zero-shot video generators. arXiv
preprint arXiv:2303.13439 , 2023. 1
[19] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional Bayes. arXiv preprint arXiv:1312.6114 , 2013. 2
[20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C. Berg, Wan-Yen Lo, Piotr Doll ´ar, and
Ross Girshick. Segment anything. arXiv:2304.02643 , 2023.
3, 5, 7
[21] Jingyuan Li, Ning Wang, Lefei Zhang, Bo Du, and Dacheng
Tao. Recurrent feature reasoning for image inpainting. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 7760–7768, 2020. 2
[22] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-
wei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.
Gligen: Open-set grounded text-to-image generation. 2023.
2, 6
[23] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D.
Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva
Ramanan, Piotr Doll’a r, and C. Lawrence Zitnick. Microsoft
COCO: common objects in context. CoRR , abs/1405.0312,
2014. 6
[24] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang,
Andrew Tao, and Bryan Catanzaro. Image inpainting for ir-
regular holes using partial convolutions. In Proceedings of
the European conference on computer vision (ECCV) , pages
85–100, 2018. 2
[25] K. Ma and B. Zhao. Attribute-guided image generation from
layout. arXiv preprint arXiv:2008.11932 , 2020. 2
[26] Hayk Manukyan, Andranik Sargsyan, Barsegh Atanyan,
Zhangyang Wang, Shant Navasardyan, and Humphrey Shi.
Hd-painter: High-resolution and prompt-faithful text-guided
image inpainting with diffusion models. arXiv preprint
arXiv:2312.14091 , 2023. 2
[27] Shant Navasardyan and Marianna Ohanyan. The family of
onion convolutions for image inpainting. International Jour-
nal of Computer Vision , pages 1–30, 2022. 2
[28] Roni Paiss, Hila Chefer, and Lior Wolf. No token left be-
hind: Explainability-aided image classification and genera-
tion, 2022. 2, 6
[29] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan
Zhu. Semantic image synthesis with spatially-adaptive nor-
malization. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition , 2019. 2
8772
[30] Tingting Qiao, Jing Zhang, Duanqing Xu, and Dacheng Tao.
Mirrorgan: Learning text-to-image generation by redescrip-
tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 1505–1514,
2019. 2
[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In Proceedings
of the 38th International Conference on Machine Learning ,
pages 8748–8763. PMLR, 2021. 2
[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 2
[33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Pe-
ter J. Liu, Minh-Thang Luong, and other authors. Exploring
the limits of transfer learning with a unified text-to-text trans-
former. arXiv preprint arXiv:1910.10683 , 2019. 2
[34] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In International Confer-
ence on Machine Learning , pages 8821–8831. PMLR, 2021.
2
[35] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey
Chu, and Mark Chen. Hierarchical text-conditional im-
age generation with clip latents, 2022. URL https://arxiv.
org/abs/2204.06125 , 7, 2022. 1
[36] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022. 2
[37] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Gen-
erating diverse high-fidelity images with VQ-V AE-2. In
Advances in Neural Information Processing Systems , pages
14825–14836, 2019. 2
[38] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10684–10695, 2022. 1, 2, 3, 4, 6
[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. Compvis stable diffu-
sion v1.4. https://huggingface.co/CompVis/
stable-diffusion-v1-4 , 2023. November 2023. 6
[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. Runwayml stable diffu-
sion v1.5. https://huggingface.co/runwayml/
stable-diffusion-v1-5 , 2023. November 2023. 6
[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. Stable diffusion ver-
sion 2. https://github.com/Stability-AI/
stablediffusion , Year Accessed. November 2023. 6[42] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems , 35:36479–36494, 2022. 1, 2
[43] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, Patrick Schramowski, Srivatsa R Kundurthy, Katherine
Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia
Jitsev. LAION-5b: An open large-scale dataset for train-
ing next generation image-text models. In Thirty-sixth Con-
ference on Neural Information Processing Systems Datasets
and Benchmarks Track , 2022. 3
[44] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 2, 3
[45] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,
Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,
Naejin Kong, Harshith Goka, Kiwoong Park, and Victor
Lempitsky. Resolution-robust large mask inpainting with
fourier convolutions. In Proceedings of the IEEE/CVF Win-
ter Conference on Applications of Computer Vision , pages
2149–2159, 2022. 2
[46] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R. De-
von Hjelm, and Shikhar Sharma. Object-centric image gen-
eration from layouts. In AAAI Conference on Artificial Intel-
ligence , 2020. 2
[47] Omer Bar Tal. MultiDiffusion: Fusing Diffusion Paths for
Controlled Image Generation. https://github.com/
omerbt/MultiDiffusion , 2023. 6
[48] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete
representation learning. Advances in neural information pro-
cessing systems , 30, 2017. 3
[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 2
[50] Bo Wang, Tao Wu, Minfeng Zhu, and Peng Du. Interac-
tive image synthesis with panoptic layout generation. In
2022 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , pages 7773–7782, 2022. 2
[51] S. Wang et al. Imagen editor and editbench: Advancing
and evaluating text-guided image inpainting. arXiv preprint
arXiv:2212.06909 , 2022. 3
[52] S. Xie et al. Smartbrush: Text and shape guided ob-
ject inpainting with diffusion model. arXiv preprint
arXiv:2212.05034 , 2022. 3
[53] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,
Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-
grained text to image generation with attentional generative
adversarial networks. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 1316–
1324, 2018. 2
[54] Xingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang, Ir-
fan Essa, and Humphrey Shi. Prompt-free diffusion: Taking”
8773
text” out of text-to-image diffusion models. arXiv preprint
arXiv:2305.16223 , 2023. 2
[55] Xingqian Xu, Shant Navasardyan, Vahram Tadevosyan, An-
dranik Sargsyan, Yadong Mu, and Humphrey Shi. Image
completion with heterogeneously filtered spectral hints. In
Proceedings of the IEEE/CVF Winter Conference on Appli-
cations of Computer Vision , 2023. 2
[56] Shiyuan Yang, Xiaodong Chen, and Jing Liao. Uni-paint:
A unified framework for multimodal image inpainting with
pretrained diffusion model. In Proceedings of the 31st ACM
International Conference on Multimedia , page 3190–3199,
New York, NY , USA, 2023. Association for Computing Ma-
chinery. 3
[57] Z. Yang, D. Liu, C. Wang, and J. Yang. Model-
ing image composition for complex scene generation.
arXiv:2206.00923 , 2022. 2
[58] Zili Yi, Qiang Tang, Shekoofeh Azizi, Daesik Jang, and Zhan
Xu. Contextual residual aggregation for ultra high-resolution
image inpainting. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
7508–7517, 2020. 2
[59] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and
Thomas S Huang. Free-form image inpainting with gated
convolution. In Proceedings of the IEEE/CVF international
conference on computer vision , pages 4471–4480, 2019. 2
[60] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-
jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-
fei Yang, Burcu Karagol Ayan, et al. Scaling autoregres-
sive models for content-rich text-to-image generation. arXiv
preprint arXiv:2206.10789 , 2022. 2
[61] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-
gang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-
gan: Text to photo-realistic image synthesis with stacked
generative adversarial networks. In Proceedings of the IEEE
international conference on computer vision , pages 5907–
5915, 2017. 2
[62] Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao
Liang, Eric I Chang, and Yan Xu. Large scale image comple-
tion via co-modulated generative adversarial networks. In In-
ternational Conference on Learning Representations (ICLR) ,
2021. 2
[63] Haitian Zheng, Zhe Lin, Jingwan Lu, Scott Cohen, Eli
Shechtman, Connelly Barnes, Jianming Zhang, Ning Xu,
Sohrab Amirghodsi, and Jiebo Luo. Cm-gan: Image inpaint-
ing with cascaded modulation gan and object-aware training.
arXiv preprint arXiv:2203.11947 , 2022. 2
8774
