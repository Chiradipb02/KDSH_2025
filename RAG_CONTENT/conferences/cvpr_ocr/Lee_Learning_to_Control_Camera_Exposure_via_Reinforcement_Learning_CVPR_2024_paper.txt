Learning to Control Camera Exposure via Reinforcement Learning
Kyunghyun Lee*
LG AI Research
kyunghyun.lee@lgresearch.aiUkcheol Shin*
CMU
ushin@andrew.cmu.eduByeong-Uk Lee
KRAFTON
byeonguk.lee@krafton.com
Deep Reinforcement Learning based Automatic Exposure Control (ours)
Conventional Built -in Automatic Exposure Control
Illumination Change
(a) Automatic exposure control for sudden lighting changes
# Features : 1504
 # Features : 609
(1) Well-exposed image acquisition (2) Object detection (3) Feature extraction
(b) Effectiveness on various vision applications (left: ours, right: built-in AE)
Figure 1. Automatic camera exposure control via deep reinforcement learning. Our proposed method, named DRL-AE, trains an agent
to control camera exposure parameters (i.e., exposure time and gain) to acquire well-exposed images with rapid convergence and real-time
processing (1ms on a CPU device). The trained agent instantly converges within five frames under dramatic lighting change scenario (a)
and affects the performance of various vision applications (b), compared to the camera built-in AE controller [14, 18].
Abstract
Adjusting camera exposure in arbitrary lighting condi-
tions is the first step to ensure the functionality of com-
puter vision applications. Poorly adjusted camera expo-
sure often leads to critical failure and performance degra-
dation. Traditional camera exposure control methods re-
quire multiple convergence steps and time-consuming pro-
cesses, making them unsuitable for dynamic lighting con-
ditions. In this paper, we propose a new camera exposure
control framework that rapidly controls camera exposure
while performing real-time processing by exploiting deep
reinforcement learning. The proposed framework consists
*Both authors contributed equally to this work.of four contributions: 1) a simplified training ground to sim-
ulate real-world’s diverse and dynamic lighting changes, 2)
flickering and image attribute-aware reward design, along
with lightweight state design for real-time processing, 3) a
static-to-dynamic lighting curriculum to gradually improve
the agent’s exposure-adjusting capability, and 4) domain
randomization techniques to alleviate the limitation of the
training ground and achieve seamless generalization in the
wild. As a result, our proposed method rapidly reaches a
desired exposure level within five steps with real-time pro-
cessing (1ms). Also, the acquired images are well-exposed
and show superiority in various computer vision tasks, such
as feature extraction and object detection.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
2975
1. Introduction
Camera exposure control is the task of adjusting expo-
sure level by controlling exposure time, gain, and aperture
to achieve a desired level of brightness and image quality for
a given scene. Poorly adjusted exposure parameters result
in over-exposed, under-exposed, blurry, or noisy images,
which can cause performance degradation in image-based
applications and, in the worst cases, even life-threatening
accidents. Therefore, finding proper camera exposure is the
first primary step to ensure the functionality of computer vi-
sion applications, such as object detection [5, 16], semantic
segmentation [9, 17], depth estimation [10, 26], and visual
odometry [1, 13].
There are several essential requirements in camera ex-
posure control. The rapid convergence must be guaranteed
to maintain an appropriate exposure level under dynamic
light-changing scenarios. Also, the exposure control loop
is one of the lowest loops in the camera system. There-
fore, lightweight algorithm design must be considered for
on-board level operation. Finally, the quality of a converged
image should not be sacrificed to meet the requirements.
Further, the number of simultaneously controlled param-
eters is also important because it affects the converge time
and final quality of the converged image. One-by-one con-
trol methods [14, 18,20] control exposure parameters in
a one-by-one manner to achieve a desired exposure level,
rather than joint controlling exposure parameters. However,
the converged parameters are often not optimal, such as
[long exposure time, low gain] and [short exposure time,
high gain] pairs. As a result, the values result in undesirable
image artifacts, such as motion blur due to long exposure
time or severe noise due to high gain.
Joint exposure parameter control [7, 8,21,23,24] often
needs multiple searching steps in a wide range of searching
space to find an optimal combination. As a result, they cause
a flickering effect and slow convergence speed. Also, the re-
cent methods require high-level computational complexity
due to its optimization algorithm [7, 8], image assessment
metric [7, 8,20,21], and GPU inference [23].
In this paper, we propose a new joint exposure param-
eter control method that exploits reinforcement learning to
achieve instant convergence and real-time processing. The
proposed framework consists of four contributions:
• A simplified training ground to simulate real-world’s di-
verse and dynamic lighting changes.
• Flickering and image attribute-aware reward design,
along with lightweight and intuitive state design for real-
time processing.
• A static-to-dynamic lighting curriculum learning to grad-
ually improve agent’s exposure adjusting capability.
• Domain randomization techniques to alleviate the limita-
tion of the training ground and achieve seamless general-
ization in the wild without additional training.The proposed method is thoroughly validated in three
different environments: light-controlled darkroom, expo-
sure control dataset [21], and real-world environments. We
demonstrate that our proposed method rapidly adjusts cam-
era exposure within five steps with real-time processing of
1ms. Also, the images acquired from our method are well-
exposed and show superiority in numerous computer vision
tasks, such as feature extraction and object detection.
2. Related Work
2.1. Optimization-based Exposure Control
One branch to control camera exposure parameters is ex-
ploiting white-box and black-box optimizations to find opti-
mal parameters for the desired exposure level. Camera built-
in Auto-Exposure (AE) control methods [14, 18] adjust ex-
posure parameters (i.e., exposure time and gain) based on
differentiable optimization by using the equation between
Exposure Value (EV) and exposure parameters [15]. They
control exposure parameters one-by-one to achieve pre-
defined image brightness. These built-in AE methods pro-
vide real-time processing ability but result in non-optimum
solutions (e.g., long exposure time and low gain) and lim-
ited scalability. The former limitation causes motion blur
and severe image noise due to long exposure time and high
gain. The latter limitation indicates the methods cannot be
extendable to maximize other image attributes, such as im-
age gradient or entropy.
Recent AE algorithms are designed to maximize de-
sirable image attributes for computer vision applications,
such as image gradient [20, 22,25], entropy [7, 8], noise
level [21], and optical flow [4]. However, these algorithms
mainly focused on image metrics for better quality, not
the control method. Therefore, they adopt heuristic con-
trol algorithm [20] or black-box optimization methods,
such as Bayesian optimization [7, 8] and Nelder-Mead
optimization [21]. These black-box optimizations and at-
tribute assessment metrics often require multiple explo-
rations that cause a flickering effect, multiple steps to con-
verge, or heavy computation time. Differing from the pre-
vious method, the proposed method provides rapid conver-
gence, real-time processing, and potential scalability by ex-
ploiting Deep Reinforcement Learning (DRL).
2.2. Data-driven Exposure Control
Another emerging branch of AE is utilizing a neural net-
work to predict appropriate exposure parameters. Tomasi
et al. [23] proposed an exposure parameter estimation net-
work that predicts optimal exposure time and gain for each
given image. The neural network, consisting of a few convo-
lution and linear layers, is trained with Ground Truth (GT)
exposure parameters in a supervised manner. However, the
GT label generation needs a time-consuming and compli-
2976
Exposure Time 
& GainΔ𝐸
Δ𝐺
Spatial Randomization
(Sec. 3.4)Crop
H-Flip
V-Flip
Resize
Rotate
Consecutive Images
{𝑰𝒕−𝑵,…,𝑰𝒕}
⋱
Consecutive RoI patches
{𝑰𝒕−𝑵𝑹𝒐𝑰,…,𝑰𝒕𝑹𝒐𝑰}⋱
DRL Agent
(Fully -connected layers)Vectorized intensity 
history (Sec.3.2)……
⋯
𝒔𝒕={𝒗𝒕−𝑵,…,𝒗𝒕} 
Captured 
ImageExposure Time
& Gain
Machine Vision 
Camera
Random ObjectsLED 
LightLight -controlled 
Dark room (Sec.3.1)[Easy] Moderate 
Lighting
[Normal] Bright or 
Dark Lighting
[Hard] Dynamic 
Lighting Change
Assign Lighting 
Condition
(Sec. 3.3)
LED (backside)
Object
CameraLight controllerFigure 2. Training framework overview. Our DRL agent is trained with the SAC algorithm in the light-controlled dark room environment.
For each episode, a lighting condition is assigned by the current curriculum level. The lighting condition can be fixed at random brightness
or dynamically changed within each episode, depending on the level. Given the lighting condition, the agent takes a vectorized intensity
history for a randomly selected RoI patch as a state. Afterward, the agent estimates exposure time and gain differences that maximize a
reward function. With this framework, the trained agent successfully generalized into a real environment without additional training.
cated process that collects multiple images with varying ex-
posure parameters for every scene. Also, a heavy compu-
tation caused by the use of convolution layers is another
drawback. Differing from the method, our method trains a
neural network by maximizing a reward function without
relying on specific GT data or generation processes.
3. DRL for Automatic Exposure Control
Applying DRL to Automatic Exposure (AE) control task
while achieving rapid convergence speed and real-time pro-
cessing presents several challenges. Our proposed method
provides effective solutions for the following questions.
1. Environment: what is the most effective form of training
environment to learn camera exposure control?
2. Reward: what aspects does the agent need to maximize?
3. State: where is the bottleneck for real-time processing?
4. Generalization: how to achieve seamless generalization
in the wild lighting condition?
3.1. Training Environment
DRL requires a large number of samples and interaction
with the environment to train the agent. Also, the environ-
ment needs to provide a diverse and wide range of prob-
lems for the agent to solve. The optimal form of exposure
control is to instantly adjust exposure parameters for a va-
riety of lighting conditions, from static lighting conditions
to dramatic lighting changes. To this end, the training envi-
ronment for camera exposure control must provide diverse
lighting change scenarios to the agent.Numerous options exist, such as a simulation, a real-
world environment with natural sunlight, and a controlled
real-world environment. The simulation can provide vari-
ous images and lighting conditions but has the limitations of
an imperfect exposure parameter implementation and a sim-
to-real domain gap. On the other hand, the real-world envi-
ronment with natural sunlight has no domain gap, but the
lighting conditions change very slowly. Therefore, we con-
struct a controlled real-world environment in a darkroom
with controllable LEDs to adjust lighting conditions.
The constructed light-controlled darkroom is shown
in Fig. 2. The environment has one machine vision cam-
era, a random target object, a light controller, and an LED
bar. The environment provides a random lighting scenario
from dark to bright light conditions, and the agent adjusts
exposure parameters to capture a high-quality image of the
target object in the suggested scenario. The detailed sensor
specification can be found in the supplementary material.
3.2. State, Action, and Reward Design
3.2.1 State Design: Vectorized Intensity History
The widely adopted state design in related fields is uti-
lizing a feature map from a pre-trained network [22, 23].
However, we found the CNN feature is not effective in the
exposure control task due to its disadvantages: 1) unclear re-
lation between CNN feature and exposure level, 2) general-
ization problem of CNN feature, and 3) heavy computation
for on-board devices due to multiple convolution layers.
2977
Therefore, instead of using the CNN feature or other
complicated features as a state, we utilize vectorized image
intensity as a straightforward and lightweight state repre-
sentation for the exposure control task. As shown in Fig. 2,
we first vectorize image intensity map from a Region-of-
Interest (RoI) patch of gray-scale image RHRoI×WRoIto the
intensity vector RS. The RoI patch can be an entire image
area or specific regions decided by a domain randomization
process. After that, we stack consecutive frame’s intensity
vectors to embed previous state history, as follows:
vt=f(IRoI
t), v t∈RS,
st=concat(v t−n, ..., v t−1, vt),
where Itis a normalized image at time step t,f(·)is an
averaging process through the x-axis of a gray-scale image,
defined as1
HRoIP
xgray(IRoI
t),vtrepresents the vector-
ized intensity that has a dimension of S, and stis a state
vector. To ensure a fixed and reasonable state length, we re-
size the given RoI patch to have S= 128 and stack previous
states with n= 3. The proposed state design is effective,
straightforward, and computationally efficient compared to
the CNN feature, as described in Sec. 4.1.
3.2.2 Action Design: Relative and Continuous Action
Obviously, we have two controllable parameters, expo-
sure time and gain, to adjust camera exposure. However,
there are numerous options for its action design, such as 1)
discrete vs. continuous action space, and 2) absolute vs. rel-
ative action range. A discrete action space discretizes the
action range into a few action values. It has the advantage
that the training process is simplified, but there is an ap-
proximation gap to the optimal values. The absolute and
relative action range is about the change of camera parame-
ters. In the absolute action range, an action value is directly
matched to the specific value of camera parameters. On the
other hand, in the relative action range, an action value in-
dicates the amount of change in camera parameters.
Among the options, we select continuous-relative action
space. This is because our goal is rapid convergence with
minimum exploration step, but discrete action space needs
multiple steps and often does not converge, depending on
its quantization level. Also, we empirically found that the
absolute action range often induces a flickering effect and
unstable convergence, as described in Sec. 4.1.
3.2.3 Reward Design: Flickering and Image Attribute
The desirable behavior of exposure control is maximizing
image attributes, such as sharp edge, moderate brightness,
and low-level noise, and maintaining image attributes dur-
ing exposure parameter transition. Therefore, we designed
the reward function from three perspectives: 1) a moderatebrightness level to provide clear visibility and edge infor-
mation, 2) a smoothed exposure transition to ensure stable
convergence and prevent flickering, and 3) a low-level noise
to provide clear image and avoid too-high gain value. The
designed reward functions are as follows:
Rmean =1
PP
xy|IRoI
t−M|pm,
Rflk=1
PP
xy||IRoI
t−IRoI
t−1||,
Rnoise =1
PP
xysobel(IRoI
t),
Rtotal=wmRmean +wfRflk+wnRnoise,
where Pis the number of pixels, M= 0.5 indicates mid-
tone brightness, pm= 0.5 is a parameter for non-linearity
andsobel is a gradient operator. Also, we set wm= 1.5,
wf=−1.0, wn=−0.1 in practice. The proposed reward
design might be a primitive and basic form for camera ex-
posure control, however, it can be easily extendable by in-
corporating modern image assessment metrics [4, 8,21].
3.3. Static-to-dynamic Lighting Curriculum
In the wild, the agent must be able to control exposure
parameters for a variety of lighting change scenarios. How-
ever, training every scenario simultaneously results in an
unstable training process and poor generalization. There-
fore, we propose static-to-dynamic curriculum strategy that
starts with a simple control task and gradually experiences
dynamic and dramatic lighting change scenarios. In the end,
the trained models possess a comprehensive exposure con-
trol capability for diverse lighting conditions.
We divide the difficulty of lighting conditions into three
levels: easy, normal, and hard. The easy level has static
lighting conditions with moderate brightness. The normal
level also has a fixed brightness but with a darker or brighter
than easy level. Lastly, in the hard level, the LED bright-
ness dynamically changes from dark to bright or the op-
posite way during each scenario. The probability of each
level is gradually updated according to the proceeded train-
ing episode te. The probability set [pe, pn, ph]starts from
[1,0,0], through [0,1,0], and ends with [pf
e, pf
n, pf
h] =
[0.1,0.4,0.5]. In summary, the probability for each diffi-
culty level is updated as follows:
pe=

1, t e< Te
(te−Te)
(Tn−Te), T e≤te< Tn
pf
e, T n≤te
pn=

0, te< Te
1−(te−Te)
(Tn−Te), T e≤te< Tn
pf
n, T n≤te
ph=(
0, t e< Tn
pf
h, T n≤te
We use Te= 25, 000, T n= 45, 000in practice.
2978
3.4. Spatial Domain Randomization
In the wild, the agent encounters various surrounding en-
vironments and object contexts, such as office, road, tunnel,
and mountain. Although the light-controlled darkroom can
provide various lighting scenarios, it is difficult to contain
diverse environments and contexts because it only has a few
target objects with a fixed background. Therefore, without
proper randomization techniques, the agent may overfit to
perform exposure control for only a few target objects, re-
sulting in generalization failure in the wild.
The main idea is to provide as much diverse image struc-
ture and context information as possible by augmenting
the image from the darkroom environment. Specifically, we
spatially augment the images with random flipping, crop-
ping, rotating, and resizing but do not change color and
brightness information. Each augmentation and its param-
eter is randomly selected at the beginning of each training
episode and fixed during the episode. With the proposed
domain randomization technique, the trained agent can be
generalized in the real world without any fine-tuning.
3.5. Policy Optimization
As our action space is continuous, we use the SAC [3]
algorithm. We excluded on-policy algorithms like PPO [19]
because they are widely known to be less sample efficient
than the off-policy algorithms like SAC, TD3, and DDPG.
We tested TD3 [2] and DDPG [11] as well, but SAC showed
the best result.
SAC algorithm is a kind of actor-critic algorithm, which
has critic Q(θ)and actor π(ϕ). The objective functions to
update the critic are as follows:
JQ(θ) =E(st,at)∼D1
2
Qθ(st, at)−ˆQ(st, at)2
,
ˆQ(st, at) =r(st, at) +γEst+1∼p[V¯θ(st+1)],
whereDis a replay buffer, r(st, at)is a reward function and
γis a discount factor. The objective functions for updating
the actor is as follows:
Jπ(ϕ) = Est∼D[Eat∼πϕ[αlog(π ϕ(at|st))−Qθ(st, at)]],
withαdefined as a temperature parameter.
4. Experiments
In this section, we validate our proposed method in three
different environments: light-controlled darkroom, expo-
sure control dataset [21], and real-world environments.
Throughout the experiments, we provide the validation re-
sult of DRL design components, ablation study of reward
and training strategy, convergent step comparison, compar-
ison with built-in AE for object detection and feature ex-
traction, and computational time analysis.Table 1. Self-evaluation of DRL-AE framework in the light-
controlled darkroom. DR and CL indicate domain randomization
and curriculum learning. ”-” indicates the agent doesn’t converge.
The best performance in each block is highlighted in bold.
FrameworkMethodsReward Frames to
Component per Frame Converge
RL
AlgorithmDDPG [11] 1.11 -
TD3 [2] 1.03 -
SAC [3] 1.61 5
StateCNN 0.85 -
Vector 1.61 5
ActionAbsolute 0.65 -
Relative 1.61 5
RewardRflkRnoise
1.41 -
✓ 1.44 16
✓ 1.35 9
✓ ✓ 1.61 5
Training
StrategyDR CL
✓ - -
✓ 1.64 15
✓ ✓ 1.61 5
4.1. Self-evaluation in Light-controlled Darkroom
We first validate our DRL design components and their
variants in the light-controlled darkroom. We utilize re-
ward per frame and the averaged number of frames to con-
verge as evaluation metrics to measure image quality and
convergence speed, respectively. Here, when the difference
between current and previous images is less than a cer-
tain threshold, we regard it as the convergence. The test-
ing scenarios include various lighting conditions, such as
fixed lighting, progressive light changes, and dynamic light
changes. The results are shown in Tab. 1.
We found SAC method [3] shows the best result among
the off-policy RL methods. Other algorithms can reach up
to 1.0 reward per frame, but they usually do not converge
well by showing oscillation. For the state design, the CNN
feature is not desirable for the exposure control task due to
its intensity-agnostic property. Also, absolute action space
seems to make the overall learning process difficult because
it needs to estimate the optimum values directly.
The reward function and the training strategies play an
important role in the stable and rapid convergence pro-
cess and image attribute preservation. Rnoise suppress high
noise level and regularize gain parameter control, leading to
better convergence. Also, Rflkmakes the agent preserve the
image attribute during the exposure transition. CL makes
the agent encompass the comprehensive exposure control
capability for the test set’s various lighting conditions. Ad-
ditionally, DR allows the agent to quickly converge for ar-
bitrary context by increasing generalization ability.
2979
Shin et al. [21]
DRL-AE (Ours)
(a) Acquired image comparison for each convergent step
(b) Convergence trajectory of exposure parameter optimization
Figure 3. Convergent step comparison in exposure control
dataset [21]. Within three frames, our method already reaches
a well-exposed image (a) with minimum exploration (b). On the
other hand, Shin et al. [21] search local areas with multiple steps
(about 30 frames) to converge.
4.2. Convergent Step Comparison
Exposure Control Dataset [21]. The dataset provides
multiple images with many different pairs of exposure and
gain values, which are captured from the real world. The
dataset consists of several locations, including indoor and
outdoor places, with a wide range of exposure and gain
values. Outdoor images can have the exposure time from
100µs to7450µs, at intervals of 150µs, and the gain from
0dB to20dB with2dB interval. Similarly, indoor images
have exposure time from 4ms to67ms with3ms intervals
and gain from 0dB to24dB with2dB intervals.
We evaluate our method with Shin et al. [21]. The final
converged points are slightly different because of the differ-
ence between the proposed reward function and the assess-
ment metric of [21]. Both algorithms converge to a compa-
rable point, as shown in Fig. 3. It only takes three frames
to converge with our method. However, the Nelder-Mead
optimization method in [21] takes at least 30 timesteps to
converge completely. Therefore, it is hard to use it in real
scenarios, although they may find a more optimal point.Indoor Environment
DRL-AE (Ours)
Built-in AE
Outdoor Environment
DRL-AE (Ours)
Built-in AE
Figure 4. Real-world generalization. We compare our method
with the camera’s built-in exposure control algorithm in real-world
scenarios. Camera lenses are occluded at the initial and suddenly
removed in the first frame. Our agent converges to a well-exposed
image within 3-5 frames. Yet, the built-in AE algorithm is still in
the middle of adjusting the exposure parameters and is far from
the well-exposed image, especially in the indoor case. Note that
our agent is only trained in the light-controlled darkroom, and this
is the zero-shot inference result in the wild.
Real-world Indoor and Outdoor Environment. We
evaluate our method with the camera’s built-in AE control
algorithm in real-world scenarios. The purposes of this ex-
periment are twofold: 1) comparing convergence speed with
the built-in AE algorithm, and 2) testing zero-shot general-
ization performance in the wild.
Before starting, we cover the camera lens with enough
time to converge in the dark, then quickly remove it to test
the convergence speed for a sudden lighting change. Fig. 4
shows the captured initial five images during each optimiza-
tion. Our method converges to a well-exposed image within
3-5 frames in both indoor and outdoor scenes. However, the
built-in AE algorithm takes much longer to converge: 30
frames for indoors and 10 frames for outdoors.
Also, we found that our agent shows satisfactory zero-
shot generalization performance, even though it is only
trained in the light-controlled darkroom with limited object
context. We believe our state design (i.e., vectorized inten-
sity history) and spatial domain randomization bring this re-
sult by removing the potential domain gap issue of the CNN
feature and augmenting object context as much as possible.
2980
# Features: 876
# Features: 1377(a) Feature extraction from DRL-AE (Ours)
# Features: 332
# Features: 239
(b) Feature extraction from built-in AE
Figure 5. SIFT [12] feature extraction result. Captured images
from the proposed algorithm and built-in AE are processed to de-
tect SIFT features. The images were simultaneously captured in
real-time from two separate cameras equipped on a driving ve-
hicle. Our method can provide plenty of SIFT features over the
image plane. On average, our method detects 38% more features
across a total of 5355 images.
4.3. Real-time Driving Env: Feature Extraction
In this experiment, two cameras are attached to the top of
a moving car, and images are captured simultaneously. One
camera is used for our algorithm, and the other camera is
for the built-in AE algorithm. We tested the algorithms on
real-world driving scenarios, including campus and urban
roads. Our algorithm runs on a laptop equipped with an i7-
7700HQ@2.80GHz CPU unit. Given an image, the agent
predicts exposure time and gain commands in real-time.
The estimated actions are transmitted to the attached cam-
era. After driving sequences acquisition, we extract SIFT
[12] features from each captured image. Please note that the
images of DRL-AE and built-in AE have slightly different
views due to the difference in the installed position.
Fig.5shows the comparison of feature extraction results.
From the total of 5355 image pairs, our method produces
1,306 SIFT features on average with a 1,157 median value.
On the other hand, the built-in AE method only results in
946 features on average, with a 711 median value. There-
fore, 38% more features are detected on average, and the
difference is up to 62% for the median value. The number
of detected features and feature repeatability during expo-
sure transition are critical for Visual Odometry (VO) and
SLAM tasks. So, we believe our method can be valuable
for VO, SLAM, and visual tracking tasks as well.
(a) Object detection from DRL-AE (Ours)
(b) Object detection from built-in AE
Figure 6. Object detection result. Captured images from the pro-
posed algorithm and built-in AE are processed to detect target ob-
jects. The experiment used the same image sequence as the SIFT
experiment. We utilize Yolo-v5 [6] for car and pedestrian detec-
tion. On average, our method detects 5% more objects compared
to the built-in AE algorithm.
4.4. Real-time Driving Env: Object Detection
Similar to the feature extraction experiment, the captured
images are processed with YOLO-v5 [6]. The images are
taken from campus and urban road scenes, so we only take
into account cars and pedestrians. Fig. 6shows the compar-
ison of object detection results from DRL-AE and built-in
AE methods. Recent detection models, including YOLO-
v5, adopt modern augmentation methods to make the model
robust the image brightness changes. Therefore, YOLO-v5
tends to detect objects well in even poorly exposed images.
However, our algorithm detects 5% more objects in terms
of the total number of detected objects. Furthermore, the ob-
jects in our image are detected much earlier than the built-in
AE method. This is highly critical for autonomous vehicles
that are driving at high speed. Earlier detected objects can
prevent human injury and potential accidents.
4.5. Real-world Env: RoI-aware Exposure Control
Our DRL-AE framework can also take arbitrary input
sizes because the framework resizes the image before the
vectorized intensity processing. Also, our domain random-
ization strategy produces a random size of the Region of
Interest (RoI) patch by using crop, flip, resize, and rotation
in the training stage. Therefore, our agent is able to control
camera exposure for a specific RoI or entire image. Fig. 7
shows the RoI-aware exposure control results.
2981
Figure 7. RoI-aware camera exposure control. Our agent is able
to control camera exposure for a specific RoI or entire image.
Given a RoI box, the agent adjusts the camera parameters to maxi-
mize the image attribute for a specific RoI area. It allows the cam-
era to capture the detailed context for the regions of interest.
The agent adjusts the camera parameters to maximize the
image attribute for a specific RoI area. It allows the camera
to capture the detailed context for the regions of interest.
We expect that DRL-AE can be combined with object de-
tection, object tracking, and human gaze and attention. As a
result, the combination can lead to adaptive exposure con-
trol schemes, such as attention-aware or detection-aware ex-
posure control.
4.6. Computation Time Analysis
Our agent has a simple Multi-Layer Perceptron (MLP)
architecture with two hidden layers of 256 units. Also, our
method does not require complex matrix computation like
convolutions. Therefore, our agent can be run on a CPU
device in real-time. We measure the inference time of the
agent on the Ryzen 5950x CPU. Tab. 2shows the compu-
tation time results, compared with shin et al. [21]. The net-
work’s inference time takes 1 msregardless of image reso-
lution because the input image is resized to a fixed resolu-
tion. Also, even including other operations, such as image
resizing and RoI cropping, it takes a maximum 6 ms, which
is still in the real-time range. Therefore, it can run at 170-
1000 Hz on a CPU device.
5. Conclusion & Future Work
Conclusion. In this paper, we proposed a novel joint ex-
posure parameter control framework that exploits Deep Re-
inforcement Learning (DRL) to achieve instant exposure
convergence and real-time processing. The proposed frame-
work, named DRL-AE, effectively solves the challenges
when applying DRL to the exposure control task, such asTable 2. Processing time analysis. Our algorithm does not use any
complex metric or computation; the agent consists of two MLP
layers with 256 hidden units, and the vectorized intensity history
does not need complex operations. Therefore, our agent can be run
on a CPU device in real-time. Here, we measure the processing
time on the Ryzen 5950x CPU.
Method Image Size Processing Time (ms)
Shin et al. [21]1600 x 1200 108.7
800 x 600 18.2
Ours1600 x 1200 1.0
800 x 600 1.0
1) training environment to provide diverse lighting change
scenarios, 2) flickering and image attribute-aware reward
design, 3) lightweight state design by using vectorized in-
tensity history, and 4) domain generalization via spatial do-
main randomization strategy.
The proposed method is thoroughly validated in three
different environments: light-controlled dark room, expo-
sure control dataset [21], and real-world environments. We
demonstrate that our proposed method instantly adjusts
camera exposure within five steps with real-time processing
of 1 mson a CPU device. Also, our method shows satis-
factory generalization performance in the wild. The images
acquired from our method are well-exposed and show supe-
riority in numerous computer vision tasks, such as feature
extraction and object detection1. To the best of our knowl-
edge, our approach is the first solution that applies DRL
to control camera exposure. We hope our paper encourages
active research of advanced camera exposure control algo-
rithms to achieve robust visual perception ability.
Future Work. This paper shows that DRL can be used
in the field of camera exposure control. There are lots of
open research topics, such as motion-aware AE control, ad-
vanced reward function, aperture control, hardware gener-
alization over various cameras, and further domain general-
ization in the real world. In the future, we plan to extend the
current darkroom environment to generate object or camera
motion, allowing the agent to consider motion blur for ex-
posure parameter control. Controlling camera aperture by
using a mechanical aperture control module is another re-
search direction.
Acknowledgment
This research was supported by a grant (P0026022) from
R&D Program funded by Ministry of Trade, Industry and
Energy of Korean government.
1Further visualization and video demos are available at https://
sites.google.com/view/drl-ae .
2982
References
[1] Christian Forster, Matia Pizzoli, and Davide Scaramuzza.
Svo: Fast semi-direct monocular visual odometry. In 2014
IEEE international conference on robotics and automation
(ICRA), pages 15–22. IEEE, 2014. 2
[2] Scott Fujimoto, Herke Hoof, and David Meger. Address-
ing function approximation error in actor-critic methods. In
International conference on machine learning, pages 1587–
1596. PMLR, 2018. 5
[3] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey
Levine. Soft actor-critic: Off-policy maximum entropy deep
reinforcement learning with a stochastic actor. In Interna-
tional conference on machine learning, pages 1861–1870.
PMLR, 2018. 5
[4] Bin Han, Yicheng Lin, Yan Dong, Hao Wang, Tao Zhang,
and Chengyuan Liang. Camera attributes control for visual
odometry with motion blur awareness. IEEE/ASME Trans-
actions on Mechatronics, 2023. 2,4
[5] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-
shick. Mask r-cnn. In CVPR, pages 2980–2988. IEEE, 2017.
2
[6] Glenn Jocher, Ayush Chaurasia, Alex Stoken, Jirka Borovec,
NanoCode012, Yonghye Kwon, Kalen Michael, TaoXie, Jia-
cong Fang, imyhxy, Lorna, 曾逸夫(Zeng Yifu), Colin Wong,
Abhiram V , Diego Montes, Zhiqiang Wang, Cristi Fati, Je-
bastin Nadar, Laughing, UnglvKitDe, Victor Sonck, tkianai,
yxNONG, Piotr Skalski, Adam Hogan, Dhruv Nair, Max
Strobel, and Mrinal Jain. ultralytics/yolov5: v7.0 - YOLOv5
SOTA Realtime Instance Segmentation, 2022. 7
[7] Joowan Kim, Younggun Cho, and Ayoung Kim. Expo-
sure control using bayesian optimization based on entropy
weighted image gradient. In 2018 IEEE International con-
ference on robotics and automation (ICRA), pages 857–864.
IEEE, 2018. 2
[8] Joowan Kim, Younggun Cho, and Ayoung Kim. Proac-
tive camera attribute control using bayesian optimization for
illumination-resilient visual navigation. IEEE Transactions
on Robotics, 36(4):1256–1271, 2020. 2,4
[9] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten
Rother, and Piotr Doll ´ar. Panoptic segmentation. arXiv
preprint arXiv:1801.00868, 2018. 2
[10] Byeong-Uk Lee, Kyunghyun Lee, and In So Kweon. Depth
completion using plane-residual representation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 13916–13925, 2021. 2
[11] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel,
Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and
Daan Wierstra. Continuous control with deep reinforcement
learning. arXiv preprint arXiv:1509.02971, 2015. 5
[12] David G Lowe. Object recognition from local scale-invariant
features. In Proceedings of the seventh IEEE interna-
tional conference on computer vision, pages 1150–1157.
Ieee, 1999. 7
[13] Raul Mur-Artal and Juan D Tard ´os. Orb-slam2: An open-
source slam system for monocular, stereo, and rgb-d cam-
eras. 33(5):1255–1262, 2017. 2[14] Masaru Muramatsu. Photometry device for a camera, 1997.
US Patent 5,592,256. 1,2
[15] Sidney F Ray, Wally Axford, and Geoffrey G Attridge. The
Manual of Photography: Photographic and Digital Imaging.
Elsevier Science & Technology, 2000. 2
[16] Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster,
stronger. In CVPR, pages 6517–6525. IEEE, 2017. 2
[17] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III
18, pages 234–241. Springer, 2015. 2
[18] Nitin Sampat, Shyam Venkataraman, Thomas Yeh, and
Robert L Kremens. System implications of implementing
auto-exposure on consumer digital cameras. In Sensors,
Cameras, and Applications for Digital Photography, pages
100–108. International Society for Optics and Photonics,
1999. 1,2
[19] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-
ford, and Oleg Klimov. Proximal policy optimization algo-
rithms. arXiv preprint arXiv:1707.06347, 2017. 5
[20] Inwook Shim, Tae-Hyun Oh, Joon-Young Lee, Jinwook
Choi, Dong-Geol Choi, and In So Kweon. Gradient-based
camera exposure control for outdoor mobile platforms. IEEE
Transactions on Circuits and Systems for Video Technology,
29(6):1569–1583, 2018. 2
[21] Ukcheol Shin, Jinsun Park, Gyumin Shim, Francois Rameau,
and In So Kweon. Camera exposure control for robust robot
vision with noise-aware image quality assessment. In 2019
IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), pages 1165–1172. IEEE, 2019. 2,4,
5,6,8
[22] Ukcheol Shin, Kyunghyun Lee, and In So Kweon. Drl-isp:
Multi-objective camera isp with deep reinforcement learn-
ing. In 2022 IEEE/RSJ International Conference on Intel-
ligent Robots and Systems (IROS), pages 7044–7051. IEEE,
2022. 2,3
[23] Justin Tomasi, Brandon Wagstaff, Steven L Waslander, and
Jonathan Kelly. Learned camera gain and exposure control
for improved visual feature detection and matching. IEEE
Robotics and Automation Letters, 6(2):2028–2035, 2021. 2,
3
[24] Yu Wang, Haoyao Chen, Shiwu Zhang, and Wencan Lu. Au-
tomated camera-exposure control for robust localization in
varying illumination environments. Autonomous Robots, 46
(4):515–534, 2022. 2
[25] Zichao Zhang, Christian Forster, and Davide Scaramuzza.
Active exposure control for robust visual odometry in hdr
environments. In 2017 IEEE international conference on
robotics and automation (ICRA), pages 3894–3901. IEEE,
2017. 2
[26] Chaoqiang Zhao, Qiyu Sun, Chongzhen Zhang, Yang Tang,
and Feng Qian. Monocular depth estimation based on deep
learning: An overview. Science China Technological Sci-
ences, 63(9):1612–1627, 2020. 2
2983
