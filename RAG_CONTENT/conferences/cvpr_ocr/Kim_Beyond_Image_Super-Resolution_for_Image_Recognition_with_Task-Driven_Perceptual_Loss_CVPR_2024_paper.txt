Beyond Image Super-Resolution for Image Recognition
with Task-Driven Perceptual Loss
Jaeha Kim1Junghun Oh1Kyoung Mu Lee1,2
1Dept. of ECE&ASRI,2IPAI, Seoul National University, Korea
jhkim97s2@gmail.com, {dh6dh, kyoungmu}@snu.ac.kr
Abstract
In real-world scenarios, image recognition tasks, such
as semantic segmentation and object detection, often pose
greater challenges due to the lack of information available
within low-resolution (LR) content. Image super-resolution
(SR) is one of the promising solutions for addressing the
challenges. However, due to the ill-posed property of SR,
it is challenging for typical SR methods to restore task-
relevant high-frequency contents, which may dilute the ad-
vantage of utilizing the SR method. Therefore, in this pa-
per, we propose Super- Resolution forImage Recognition
(SR4IR) that effectively guides the generation of SR images
beneficial to achieving satisfactory image recognition per-
formance when processing LR images. The critical compo-
nent of our SR4IR is the task-driven perceptual (TDP) loss
that enables the SR network to acquire task-specific knowl-
edge from a network tailored for a specific task. Moreover,
we propose a cross-quality patch mix and an alternate train-
ing framework that significantly enhances the efficacy of the
TDP loss by addressing potential problems when employing
the TDP loss. Through extensive experiments, we demon-
strate that our SR4IR achieves outstanding task perfor-
mance by generating SR images useful for a specific image
recognition task, including semantic segmentation, object
detection, and image classification. The implementation
code is available at https://github.com/JaehaKim97/SR4IR.
1. Introduction
After the pioneering AlexNet [31], numerous deep neu-
ral network-based methods have achieved superior perfor-
mance in various image recognition tasks, such as image
classification [13, 20, 23, 24, 40, 53, 56], object detec-
tion [37, 39, 47, 48], and semantic segmentation [6, 42,
64, 72]. In real-world applications, such as medical im-
age recognition and surveillance systems, a network for a
task, which we refer to as a task network , may encounter in-
put images containing low-resolution (LR) contents due to
(a) Ground-truth
 (b) Bilinear-up
(c) SwinIR [33]
 (d) SwinIR[33] + SR4IR(Ours)
Figure 1. Visualizations and comparisons of the results on se-
mantic segmentation task with PASCAL VOC dataset [14].
(a) The ground-truth image and the class label map. (b) The
bilinear-upsampled image and the predicted result when trained on
bilinear-upsampled images. (c) the SR result of the SwinIR [33]
and the predicted result when trained on the SR images. (d) the
SR result of the SwinIR and the predicted result when trained by
the proposed SR4IR framework. We use DeepLabV3 [7] as a task
network and the downsampling scale factor is x4.
various factors, including limitations of cameras or storage
capabilities and the presence of tiny objects. In this circum-
stance, achieving satisfactory task performance is limited
by the lack of high-frequency components crucial for a task.
In this paper, we address these challenges by employ-
ing single image super-resolution (SR) methods, as SR can
serve as a prominent pre-processing tool to restore miss-
ing high-frequency details in LR images. Conventionally,
SR aims to restore high-resolution (HR) images mainly
via pixel-wise loss [18, 29, 33, 35, 70, 74] or perceptual
loss [5, 15, 27]. However, due to the ill-posed nature of SR,
these approaches may not ensure the reconstruction of fea-
tures crucial for achieving better performance for a specific
task, thereby leading to marginal performance improvement
from SR. Thus, in this paper, our primary goal is to develop
a comprehensive framework, named Super- Resolution for
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
2651
Image Recognition (SR4IR), wherein the SR and task net-
work interplay harmoniously to generate SR images, es-
pecially focusing on incorporating task-relevant features,
which are exploited to enhance task performance.
To attain our goal, we focus on leveraging the concept
of perceptual loss, as it is devised to encourage SR images
to mimic the HR counterparts within a feature space. Typ-
ically, the perceptual loss is computed using VGGNet [53]
pre-trained on ImageNet dataset [49]. However, we note
that employing perceptual loss in the conventional man-
ner results in modest improvements in task performance.
Instead, we propose a Task-Driven Perceptual (TDP) loss
that enables the emulation of HR images within the feature
space of a task network. By doing so, our TDP loss facil-
itates the restoration of high-frequency details, especially
beneficial for enhancing task performance.
The effectiveness of our TDP loss is significantly influ-
enced by the knowledge that the task network acquires. For
high-level vision tasks, it is widely acknowledged that a net-
work can fall into learning non-intrinsic features, leading to
biased feature representations [16]. A network possessing
biased knowledge on a task can diminish the efficacy of the
TDP loss, as it may fail to deliver meaningful signals to the
SR network for the restoration of other valuable features. To
tackle the problem, we introduce a novel data augmentation
strategy for training the task network, called Cross-Quality
patch Mix (CQMix), which blends SR and HR images at
the patch level to randomly eliminate high-frequency com-
ponents. Consequently, CQMix can prevent a task network
from learning shortcut features, thereby allowing us to fully
harness the benefits of TDP loss.
In addition to calculating perceptual loss using a task
network, we introduce further variations to the perceptual
loss by applying it to a task network undergoing training
instead of a pre-trained one. This strategy substantially im-
proves the effectiveness of our TDP loss, since a pre-trained
(then fixed) task network cannot learn to produce meaning-
ful features from SR images. To this end, we constitute the
SR4IR as an alternate training framework that trains the SR
and task network alternately throughout the training proce-
dure. In the first phase, we train the SR network using the
TDP loss with the temporarily frozen task network. Sub-
sequently, we train only the task network with task-specific
loss using data augmented by the CQMix.
To evaluate the proposed methods, we have examined
the integration of SR with three representative image recog-
nition tasks: semantic segmentation, object detection, and
image classification. For each task, we evaluate our method
on two representative SR architectures, EDSR [35] and
SwinIR [33], with scale factors of ×4 and×8. Our exten-
sive experiments clearly demonstrate that our SR4IR signif-
icantly improves task performance while achieving percep-
tually appealing SR results compared to baseline methods,as shown in Figure 1. These results indicate that our frame-
work can be generally adopted in various applications. We
summarize our contributions as follows:
• To our knowledge, we are the first to introduce a compre-
hensive SR framework that addresses challenges posed by
LR contents across various image recognition tasks.
• We propose the task-driven perceptual (TDP) loss that fa-
cilitates learning to restore task-related features acquired
by a task network, enhancing task performance.
• We propose the cross-quality patch mix and the alternate
training framework to address the potential problems of
TDP loss, further enhancing the efficacy of TDP loss.
• The extensive experiments demonstrate that our SR4IR is
generally applicable across a wide range of image recog-
nition tasks to enhance task performance while generating
perceptually satisfying SR images.
2. Related Work
Single Image Super-Resolution. In the era of deep learn-
ing, deep neural network-based methods have risen to
prominence in the field of SR. After the introduction of
the simple deep-learning-based SR network SRCNN [12],
VDSR [29] brought about innovation in network architec-
ture design by stacking deeper convolutional layers with
the inclusion of residual connections. EDSR [35] and
SRResNet [32] brought the Residual Block concept into
SR networks, and subsequent methods such as RDN [74],
DBPN [18] further evolved the Dense Residual Block, re-
sulting in high-performance network structures. SAN [11],
HAN [44], and RCAN [73] enhance the SR performance
by incorporating channel attention mechanisms, enabling
the exploitation of useful channel-wise features. Re-
cently, transformer-based methods such as SwinIR [33],
Restormer [67], and Uformer [62] have demonstrated re-
markable SR performance by harnessing the powerful ca-
pabilities of Vision Transformers [13].
On the other hand, perceptual super-resolution (SR)
methods constitute an alternative approach dedicated to
restoring photorealistic SR results. Johnson et al. [27] intro-
duced perceptual loss, which minimizes the distance in the
feature space and has shown its effectiveness in reconstruct-
ing high-frequency components in SR. Zhang et al. [71] il-
lustrated that the perceptual quality of images is not directly
correlated with the peak signal-to-noise ratio (PSNR), then
introduced the perceptual metric known as LPIPS. Subse-
quent to the development of adversarial loss [17], GAN-
based SR methods [32, 34, 45, 51, 54, 55, 59, 60] have been
successful in achieving realistic SR results. More recently,
diffusion-based SR methods [8, 28, 43, 50, 61, 63, 65] have
further pushed the boundaries by leveraging a diffusion-
based model [21] to produce even more realistic SR results.
2652
Figure 2. Overview of the proposed SR4IR framework. Our SR4IR framework consists of two training phases, where SR and task
networks are alternately trained. During the first phase, SR4IR updates the SR network using the TDP loss, which is introduced in
Section 3.1, while the task network is temporarily frozen. In the second phase, SR4IR updates the task network using the proposed data
augmentation strategy called CQMix, which is introduced in Section 3.2, while the SR network is temporarily frozen.
Image restoration for high-level vision tasks. There ex-
ist pioneering recent works [9, 10, 38, 41, 52, 75] focused
on designing image restoration techniques that offer advan-
tages in high-level computer vision tasks. Zhou et al. [76]
introduced a weight map that uses pixel loss primarily in
high-frequency regions, showing its effectiveness in image
classification. TDSR [19] presented an object detection-
driven SR method that updates the SR network using a com-
bination of reconstruction and detection losses and provides
a comprehensive study on balancing the joint loss. Wang et
al. [58] proposed adding an SR branch to the segmentation
branch, enabling the effective utilization of HR representa-
tion. Bai et al. [2, 3] have demonstrated that incorporating
adversarial loss through a multitask generative adversarial
network can further enhance the performance of low-quality
object detection. In contrast to previous approaches, our
method successfully restores task-specific high-frequency
components by leveraging the concept of perceptual loss.
Moreover, we are the first to introduce a generalizable task-
combined SR framework that achieves remarkable perfor-
mance across a range of image recognition tasks.
3. Proposed method
Overview. Figure 2 shows the overview of the proposed
Super- Resolution forImage Recognition (SR4IR) frame-
work. The SR4IR comprises two distinct training phases
that are alternately executed to train either the super-
resolution (SR) or the task network. In the first phase,
we train the SR network using the Task-Driven Percep-
tual (TDP) loss, which is designed to guide the SR network
to restore high-frequency details relevant to the task (Sec-
tion 3.1). In the second phase, we train the task network us-
ing the novel Cross-Quality patch Mix (CQMix) augmenta-
tion method, which prevents the task network from learning
biased features, further enhancing the effectiveness of the
TDP loss (Section 3.2). Lastly, we present the effect of al-
ternate training with respect to TDP loss and summarize our
final training objectives in each training phase (Section 3.3).Notation. LetIHR∈IHRandILR∈ILRdenote the high-
resolution (HR) and low-resolution (LR) images, paired
with their corresponding task labels y∈Y(e.g., class la-
bels in image classification task). SθSR:ILR→ISRde-
notes the SR network parameterized by θSRwhere ISRis
the super-resolved image set. Given the SR images ISR,
we predict the corresponding labels using the task network:
Hθhead◦Fθfeat:ISR→Y, where FθfeatandHθheadindicate the
feature extractor and the task-specific head module ( e.g., the
last fully-connected layer for image classification tasks) pa-
rameterized by θfeatandθhead, respectively.
Problem Definition. The goal of SR4IR is to produce
photo-realistic SR images from ILRthat can benefit the sub-
sequent image recognition task, which is formally given by:
min
θSR,θfeat,θheadLSR(SθSR(ILR),IHR)
+Ltask(Hθhead◦Fθfeat◦SθSR(ILR),y),(1)
where the first and second terms are used to evaluate the
quality of the super-resolved results ( e.g., LPIPS [71]) and
the performance for the target high-level vision task ( e.g.,
classification accuracy), respectively.
3.1. Task-Driven Perceptual Loss
High-resolution (HR) images contain high-frequency infor-
mation that is crucial to solving high-level vision tasks.
However, due to the ill-posed nature of the SR, such task-
relevant features are hard to restore using traditional pixel-
wise reconstruction loss. Even with the use of perceptual
loss [5, 15, 27], designed to produce realistic SR results,
there is no guarantee of successfully reconstructing useful
features for a specific task. To address this challenge, we
propose task-driven perceptual (TDP) loss that effectively
guides the SR network to produce images containing high-
frequency details relevant to the task. Specifically, the TDP
loss aims to maximize the similarity between HR and SR
2653
images within the latent space of the feature extractor of the
task network Fθfeat, whose outputs are substantially relevant
to the task. We define the TDP loss as follows:
LTDP=∥Fθfeat(ISR)−Fθfeat(IHR)∥1. (2)
3.2. Cross-Quality Patch Mix
The effectiveness of TDP loss is inherently related to the
training procedure of the task network. For high-level vi-
sion tasks, it is widely known that a network is susceptible
to shortcut learning [16], which means that a network can
rely on biased and non-intrinsic features in training data,
leading to poor generalization ability. In particular, learning
biased features can pose challenges in employing the TDP
loss, as biased features learned by the task network can hin-
der the SR network from acquiring comprehensive features
present in HR images through the TDP loss. For instance,
if a task network exclusively learns ‘ beak ’ features to clas-
sify bird species, the TDP loss may struggle to effectively
guide SR images to restore other key high-frequency con-
tents, such as ‘ wings ’ or ‘ feet’. This limitation could hinder
the overall task performance boosting that could be attained
through the SR method.
To address this issue, we introduce a novel data augmen-
tation strategy called Cross- Quality patch Mix (CQMix)
when training the task network in the second phase. CQMix
is designed to enhance the effectiveness of the TDP loss by
encouraging the task network to use diverse high-frequency
content as clues. Specifically, CQMix augments an input
image by randomly choosing either HR or SR patches for
each grid region, as shown in Figure 3, where the num-
ber of patches is hyper-parameter. By doing so, CQMix
creates an image in which high-frequency components are
randomly erased at a patch level, effectively preventing the
task network from learning specific high-frequency features
as shortcuts. Moreover, it is noteworthy that our CQMix
is generally applicable to various vision tasks, unlike other
data augmentation strategies, such as Mix-Up [69] or Cut-
Mix [66], as long as HR and LR pairs are available.
3.3. Alternate Training Framework
Conventional perceptual loss calculates the similarity be-
tween SR and HR images within the feature spaces of a fixed
network, such as fixed VGG [53] pre-trained on ImageNet
dataset [49]. However, this approach can be problematic
for TDP loss, which aims to restore task-relevant features
from HR images. To effectively utilize the TDP loss to en-
hance task performance, the task network must be capable
of extracting task-relevant features from SR images. How-
ever, since the fixed network is not originally trained on ISR,
which keeps changing during training, it cannot effectively
Figure 3. Concept of CQMix. The black and white region in the
figure represents 0and1region in the binary mask M.
extract task-relevant features from ISR, thus reducing the
effectiveness of the TDP loss. To resolve this domain gap
issue, we propose an alternative training framework, as il-
lustrated in Figure 2, which enables us to employ the feature
space of the on-training feature extractor of the task net-
workFθfeat, for calculating the TDP loss. By doing so, we
can further boost the efficacy of our TDP loss by provid-
ing a more meaningful guide to the SR network to acquire
task-related knowledge.
Specifically, in the first phase, we train SR network SθSR
with pixel-wise reconstruction loss Lpixeland the proposed
TDP loss, while keeping the feature extractor Fθfeatfrozen.
In the second phase, we train the task network Hθhead◦Fθfeat
with a task-specific loss Ltask,e.g., cross-entropy loss for
image classification, while freezing the SR network. We use
three types of images in this phase: HR, SR, and augmented
images by our CQMix. In summary, the objectives of our
SR4IR in each phase are defined as follows:
min
θSRLpixel(SθSR(ILR),IHR) +LTDP,inphase 1,
min
θfeat,θheadLtask(Hθhead◦Fθfeat(Icat),y), inphase 2,(3)
where Icatis the concatenation of ISR,IHR, andIaug, where
theIaugare the images augmented by the CQMix.
4. Experiments
Datasets. We select representative datasets and corre-
sponding evaluation metrics for various image recogni-
tion tasks, including semantic segmentation, object detec-
tion, and image classification. For semantic segmenta-
tion and object detection, we use the PASCAL VOC2012
dataset [14]. For the image classification task, we use the
Stanford Cars [30] and CUB-200-2011 [57] datasets. Fol-
lowing the conventional SR datasets [1, 4, 68], we construct
the corresponding LR datasets applying bicubic downsam-
pling with scale factors x4 and x8.
Evaluation. We denote the task network Hθhead◦Fθfeatand
the SR network SθSRasTandS, respectively, for conve-
2654
nience. Then, the available training scenarios when training
bothTandSconcurrently are as follows:
•IHR→T: Train the TwithLtaskusing (IHR,y). This
can be regarded as an oracle in our problem.
•ILR→T: Train the TwithLtaskusing (B(ILR),y),
whereBis bilinear upsampling operator.
•S→T: Train the SwithLpixelusing (ILR,IHR)first.
Then train TwithLtaskusing (S(ILR),y), while freez-
ingS, as in previous work [76].
•T→S: Train TwithLtaskusing (IHR,y)first. Then,
trainSwithLpixel+Ltaskusing (ILR,IHR,y), while freez-
ingT, as in previous work [19, 38, 41].
•S+T: Jointly train both SandTwithLpixel+Ltask
using (ILR,IHR,y).
We evaluate the proposed method, named Super- Resolution
forImage Recognition (SR4IR), by comparing it with the
above methods. For evaluation metrics, we use mean In-
tersection over Union (mIoU) for semantic segmentation,
mean Average Precision (mAP) for object detection, and
Top-1 accuracy for image classification. The mAP score is
calculated via the COCO [36] mAP evaluator, which uses
10 IoU thresholds from 0.5 to 0.95 with a step size of 0.05.
Furthermore, we use LPIPS [71] and PSNR to measure the
quality of the SR images.
Network architecture. For the SR network, we employ
two architectures: EDSR-baseline [35] and SwinIR [33].
We initialize the SR network from network weights pre-
trained on DIV2K [1], widely used for SR training. Re-
garding the task network, we select representative network
architectures tailored to each specific task. Specifically, we
adopt DeepLabV3 [7] for semantic segmentation, Faster-
RCNN [48] for object detection, and ResNet18 [20] for
image classification. The DeepLabV3 and Faster-RCNN
are based on MobileNetV3 [22] backbone. The backbone
networks are initialized using ImageNet [49] pre-trained
weights provided by PyTorch [46] for easier optimization.
Implementations. We utilize AdamW [26] and SGD op-
timizer for the SR and task network. The learning rates are
set to 10−4for the SR network, 2×10−2for DeepLabV3
and Faster-RCNN, and 3×10−2for ResNet18, as these
values have been shown to produce the best results for their
respective tasks. The batch sizes are set to 16 for both se-
mantic segmentation and object detection and 128 for image
classification. We apply cosine annealing [25] scheduling to
all optimizers, and all experiments are conducted under Py-
Torch [46] with eight NVIDIA RTX A6000 GPUs. For the
CQMix, the number of patches is set to 64 for both semantic
segmentation and object detection and 16 for image classi-
fication, regarding the image resolution for each dataset.4.1. Quantitative results
Semantic segmentation. Table 1 shows the quantitative
results on the semantic segmentation task. On the x4 SR
scale, our SR4IR significantly boosts the mIoU scores com-
pared to the case trained on the bilinear upsampled im-
ages (ILR→T), by +2.1 and +3.6, for EDSR-baseline
and SwinIR, respectively. Remarkably, our method with
SwinIR can achieve a mIoU score of 62.2, which is com-
parable to the 63.3mIoU score achieved by the oracle
(IHR→T), demonstrating the effectiveness of combining
the SR method when dealing with LR images. Furthermore,
SR4IR surpasses the mIoU scores of all SR combined base-
lines,S→T,T→S, andS+T, with a gain of +0.5
and +2.7 compared to the second-best baselines, in the re-
sults of the EDSR-baseline with SR scales of x4 and x8,
respectively. This validates that our method successfully re-
stores the high-frequency details that are beneficial for the
subsequent task. In particular, compared to ILR→T, the
performance improvement of the proposed SR4IR becomes
even more significant as the SR scale increases from x4 to
x8, the performance gain increases from +2.1 to +5.7, for
the EDSR-baseline case. This indicates that our method is
even more effective when the LR images lose a substantial
amount of essential high-frequency details relevant to the
task, demonstrating the effectiveness of SR4IR.
Furthermore, the proposed SR4IR achieves the best
LPIPS score in all cases, with a substantial margin of +12%
gain over the second-best baseline for SwinIR on a scale
of x4. These results demonstrate that our SR results are
not only beneficial from a task perspective but are also ca-
pable of producing visually appealing results. Noticeably,
separate and sequential training of SR and task networks
(S→T)achieves the best PSNR values but a much lower
mIoU score than our SR4IR. The results validate that restor-
ing high-frequency information relevant to the task is much
more critical to improving task performance than minimiz-
ing the pixel-wise error from ground-truth images.
Moreover, it is noteworthy that training jointly both SR
and the task network (S+T)performs even worse than
sequential training of the SR and task networks (S→T).
This indicates that simultaneously updating both the SR and
task networks diminishes the restoring power of the SR due
to joint optimization, highlighting the advantages of our al-
ternate training framework.
Object detection. Table 2 presents the numerical results
of the object detection. The proposed SR4IR consistently
achieves the best mAP score compared to all SR combined
baselines. Specifically, for the case of x8 SR scale with
the SwinIR model, our SR4IR outperforms the task network
trained on bilinear upsampled images (ILR→T)and the
second-best SR combined baselines (S→T)by substantial
margins of +8.9 and +4.1, respectively.
2655
Methods SR Methodx4 x8
mIoU ↑ LPIPS ↓ PSNR ↑ mIoU ↑ LPIPS ↓ PSNR ↑
IHR→T Oracle ( IHR) 63.3 0.000 + inf 63.3 0.000 + inf
ILR→T Bilinear up 58.6 0.300 26.16 49.3 0.476 22.54
S→T 60.2 0.241 28.94 52.3 0.407 24.41
T→S EDSR- 56.6 0.352 26.83 49.2 0.461 23.14
S+T baseline [35] 58.0 0.318 27.64 50.0 0.480 23.58
SR4IR (Ours) 60.7 0.220 28.21 55.0 0.380 23.91
S→T
SwinIR [33]61.4 0.221 29.57 53.9 0.387 24.75
T→S 53.2 0.376 26.04 48.7 0.466 23.06
S+T 57.5 0.341 27.55 50.0 0.480 23.63
SR4IR (Ours) 62.2 0.194 28.83 56.5 0.355 24.23
Table 1. Performance on the semantic segmentation. We adopt DeepLabV3 [7] with the MobileNetV3 [22] backbone as the task network
Tand PASCAL VOC2012 [14] as an evaluation dataset. The bold and underlined mean the best and second best, respectively.
Methods SR Methodx4 x8
mAP ↑ LPIPS ↓ PSNR ↑ mAP ↑ LPIPS ↓ PSNR ↑
IHR→T Oracle ( IHR) 36.7 0.000 + inf 36.7 0.000 + inf
ILR→T Bilinear up 27.5 0.411 22.77 18.9 0.559 20.29
S→T 30.3 0.336 24.62 21.9 0.494 21.62
T→S EDSR- 27.2 0.317 23.37 15.5 0.476 20.48
S+T baseline [35] 28.3 0.354 23.71 20.3 0.506 20.98
SR4IR (Ours) 32.4 0.275 23.94 25.5 0.416 20.86
S→T
SwinIR [33]31.5 0.309 25.13 23.7 0.474 21.85
T→S 29.7 0.284 23.59 18.9 0.449 20.59
S+T 31.4 0.331 23.94 21.9 0.489 21.11
SR4IR (Ours) 33.9 0.239 24.38 27.8 0.382 21.07
Table 2. Performance on the object detection. We adopt Faster-RCNN [48] with the MobileNetV3 backbone [22] as the task network T
and PASCAL VOC2012 [14] as an evaluation dataset. The bold and underlined mean the best and second best, respectively.
Methods SR MethodStanfordCars [30] CUB-200-2011 [57]
x4 x8 x4 x8
Top-1 Acc. ↑(%) LPIPS ↓Top-1 Acc. ↑(%) LPIPS ↓Top-1 Acc. ↑(%) LPIPS ↓Top-1 Acc. ↑(%) LPIPS ↓
IHR→T Oracle ( IHR) 86.4 0.000 86.4 0.000 78.3 0.000 78.3 0.000
ILR→T Bilinear up 78.7 0.304 61.0 0.478 70.8 0.298 58.2 0.465
S→T 82.7 0.176 68.3 0.328 71.1 0.216 59.8 0.365
T→S EDSR- 80.1 0.190 56.9 0.448 70.5 0.208 56.5 0.401
S+T baseline [35] 80.8 0.196 65.2 0.378 70.8 0.226 58.6 0.380
SR4IR (Ours) 83.3 0.158 71.4 0.326 72.8 0.179 62.6 0.326
S→T
SwinIR [33]84.0 0.159 71.4 0.308 72.8 0.201 61.2 0.349
T→S 82.0 0.162 61.0 0.397 72.1 0.190 58.7 0.377
S+T 83.4 0.179 66.6 0.355 72.9 0.212 61.2 0.364
SR4IR (Ours) 85.1 0.136 73.9 0.292 74.7 0.157 65.3 0.301
Table 3. Performance on the image classification. We adopt ResNet18 [20] with a fully-connected layer as the task network T. The bold
and underlined mean the best and second best, respectively.
Image classification. Table 3 shows the quantitative re-
sults of the image classification task. Our SR4IR signifi-
cantly improves image classification performance, remark-
ably achieving +12.9% improvement on the x8 scale Stan-
fordCars dataset and a +7.1% enhancement on the x8 scale
CUB-200-2011 dataset when combined with the SwinIR
model, compared to the task network trained on bilinear up-
sampled images (ILR→T). Furthermore, we note that our
method consistently improves performance across various
datasets, demonstrating its strong generalization capability.
4.2. Qualitative results
In this section, we provide visual results of the SR with their
corresponding task performance. Specifically, we compareour SR4IR with two baselines, ILR→TandS→T,
where the latter achieved the second-best quantitative re-
sults. For a more extensive visual comparison with all base-
lines, please refer to our supplementary materials.
In the context of semantic segmentation, as shown in
Figure 4, the proposed SR4IR successfully generates a seg-
mentation map that closely resembles the ground truth. In
contrast, the baselines ILR→TandS→Tproduce in-
correct estimations on ‘ husky ’. Regarding object detection,
illustrated in Figure 5, SR4IR enables the object detection
network to accurately identify two people who are closely
situated, while ILR→TandS→Tfail to distinguish and
predict them as a single person. In image classification, as
shown in Figure 6, the proposed SR4IR empowers the sub-
2656
(a)ILR→T
 (b)S→T
 (c)SR4IR (Ours)
 (d) Ground-truth
Figure 4. Visualization of images and semantic segmentation results on PASCAL VOC dataset. We present the restored images and
the corresponding predicted segmentation maps, respectively. For (b) and (c), we use the SwinIR model with an SR scale factor of x4.
(a)ILR→T
 (b)S→T
 (c)SR4IR (Ours)
 (d) Ground-truth
Figure 5. Visualization of object detection results on PASCAL VOC dataset. The red box with orange annotation means the predicted
object bounding box with the corresponding prediction. For (b) and (c), we use the SwinIR model with an SR scale factor of x4.
(a)ILR→T
 (b)S→T
 (c)SR4IR (Ours)
 (d) Ground-truth
Figure 6. Visualization of images and image classification results on CUB-200-2011 dataset. We present the restored images and
the corresponding caption. The caption below the image represents the predicted image classification results, which is indicated by a
checkmark if the prediction is correct. For (b) and (c), we use the SwinIR model with an SR scale factor of x4.
sequent task network to make the correct label predictions,
while the other methods fail to do so. Furthermore, it should
be noted that the visualized images produced by our SR4IR
achieve the most visually pleasing results.
4.3. Ablation studies
In the ablation studies, we analyze the effectiveness of each
component of our method and further report the comparison
with previous related works. We report the results of seman-
tic segmentation on the x8 scale PASCAL VOC dataset and
image classification on the x8 scale StanfordCars dataset,
using the EDSR-baseline network. Please refer to our sup-
plementary materials for more diverse ablation studies.
Effectiveness of the TDP loss. Table 4 shows the task
performance of our SR4IR, comparing the cases with and
without the proposed TDP loss. Without the TDP loss, the
performance of SR4IR decreases significantly, with a de-
crease of -1.7 mIoU score in semantic segmentation and a
decrease in precision of -1.5% in image classification. TheMethodsSegmentation Classification
mIoU ↑ LPIPS ↓ Top-1 Acc. ↑(%) LPIPS ↓
SR4IR without TDP loss 53.3 0.407 69.9 0.327
SR4IR (Ours) 55.0 0.380 71.4 0.326
Table 4. Effectiveness of the proposed TDP loss. We evaluate
the impact of TDP loss by including or excluding LTDPin phase 1
of equation (3) on the performance of SR4IR.
results demonstrate that the TDP loss is crucial for a sig-
nificant boost in task performance in our SR4IR, showing
that restoring the task-relevant high-frequency details is es-
sential in enhancing the performance of the subsequent task
network. Furthermore, the absence of TDP loss results in
a significant deterioration in the LPIPS score with a -7%
decrease in segmentation. These results validate that our
TDP loss not only enhances task performance but also con-
tributes to generating perceptually realistic SR results.
Effectiveness of the CQMix. To verify the effectiveness
of the proposed CQMix, we conduct experiments on train-
ing the task network with various types of images, as shown
in Table 5. When we train the task network with only ISR,
2657
the performance is poor, indicating that the effectiveness of
the TDP loss decreases when the task network is not trained
to properly utilize the high-frequency information present
in HR images. However, training the task network solely
onIHRalso yielded subpar results, as the task network is
not trained to handle ISR. Combining both ISRandIHR
performs better compared to individual methods. Finally,
adding augmented images through our CQMix further en-
hances the performance, with gains of +0.3 in segmentation
and +1.1% in classification, respectively, highlighting the
substantial benefits of preventing shortcut learning.
Effectiveness of the alternate training framework. To
validate the effectiveness of the proposed alternate training
framework, we compare our methods with three perceptual-
loss-related approaches, denoted as (A),(B)and (C). In
(A), we substitute the proposed TDP loss to the conven-
tional perceptual loss, which uses a fixed VGG network
pre-trained on ImageNet. In (B), rather than using the on-
training network proposed in our SR4IR, we employ TDP
loss using the fixed task network pre-trained on the corre-
sponding target task, i.e.,IHR→T. In (C), rather than
adopting alternate training, we apply the TDP loss with a
trainable on-training task network within a joint training,
where both the SR and task networks are trained jointly.
Table 6 presents performance results between ( A), (B),
(C), and our method. Compared to the performance of
SR4IR without TDP loss, which achieves a 53.3 mIoU score
and 69.9% accuracy as presented in Table 4, the method
(A)achieves similar or even worse performance, demon-
strating that adopting conventional perceptual loss does not
help to boost the task performance. The method (B)also
results in marginal performance gains, as the fixed and pre-
trained task network is not trained to handle the SR results.
Notably, the method (C)fails to learn because optimizing
the TDP loss leads to undesirable local optima when the
task network is trainable, which results in forcing the task
network to produce a constant output regardless of varied
inputs. In contrast, our SR4IR achieves significant per-
formance improvement compared to the above approaches,
highlighting the importance of adopting an alternate train-
ing framework to fully leverage the power of the TDP loss.
Comparison with previous methods. To highlight the
advantages of our SR4IR, we include a quantitative com-
parison with two related previous methods, TDSR [19] and
SOD-MTGAN [3], which are task-driven restoration meth-
ods. The results are presented in Table 7. Due to the absence
of a publicly available training code, we report the results
of our re-implemented version, which is elaborated in the
supplementary material. In comparison to TDSR and SOD-
MTGAN, our SR4IR achieves significantly superior perfor-
mance, underlining the effectiveness of leveraging percep-
tual loss when compared to the case of training without per-
ceptual loss (TDSR) and adversarial loss (SOD-MTGAN).Training image typesSegmentation Classification
(mIoU ↑) (Top-1 Acc.% ↑)
ISR 51.7 66.7
IHR 53.3 59.3
ISR+IHR 54.7 70.3
ISR+IHR+Iaug(Ours) 55.0 71.4
Table 5. Effectiveness of the proposed CQMix. We compare the
performance of SR4IR by varying the training images Icatin phase
2 of equation (3).
Methods Feature extractor for perceptual lossSegmentation Classification
(mIoU ↑) (Top-1 Acc.% ↑)
(A) Pre-trained on ImageNet 53.6 69.4
(B) Pre-trained on corresponding task 53.6 70.3
(C) On-training (joint) 3.5 3.7
Ours On-training (alternate) 55.0 71.4
Table 6. Effectiveness of the proposed alternate training frame-
work. We compare our SR4IR with three perceptual-loss-related
approaches, by varying the feature extractor Fθfeatused for the
TDP loss in equation (2).
MethodsSegmentation Classification
(mIoU ↑) (Top-1 Acc. % ↑)
TDSR†[19] 49.8 56.4
SOD-MTGAN †[3] 51.6 68.1
SR4IR (Ours) 55.0 71.4
Table 7. Comparison with related previous methods. The sym-
bol†denotes the results of our re-implementation version.
5. Conclusion
In this paper, we tackle the practical scenarios for image
recognition tasks where low-resolution (LR) content pre-
vails when processing the images. We tackle the chal-
lenge by employing image super-resolution (SR) tech-
niques. Considering the ill-posed nature of SR, we fo-
cus on developing methods that encourage an SR network
to reconstruct task-related features beneficial for a specific
task. In particular, we propose task-driven perceptual (TDP)
loss that effectively guides the restoration of task-related
high-frequency contents. Furthermore, our cross-quality
patch mix (CQMix) and alternate training framework help
us fully harness the advantages of our TDP loss by pre-
venting a task network from learning biased features and
eliminating the domain gap problem. Extensive experimen-
tal results demonstrate that our Super- Resolution forImage
Recognition (SR4IR) can be generally and effectively appli-
cable to various image recognition tasks, such as semantic
segmentation, object detection, and image classification.
Acknowledgments. This work was supported in part by
the IITP grants [No. 2021-0-01343, Artificial Intelligence
Graduate School Program (Seoul National University),
No.2021-0-02068, and No.2023-0-00156], the NRF grant
[No.2021M3A9E4080782] funded by the Korean govern-
ment (MSIT).
2658
References
[1] Eirikur Agustsson and Radu Timofte. NTIRE 2017 chal-
lenge on single image super-resolution: Dataset and study.
InCVPR Workshops , 2017. 4, 5
[2] Yancheng Bai, Yongqiang Zhang, Mingli Ding, and Bernard
Ghanem. Finding tiny faces in the wild with generative ad-
versarial network. In CVPR , 2018. 3
[3] Yancheng Bai, Yongqiang Zhang, Mingli Ding, and Bernard
Ghanem. Sod-mtgan: Small object detection via multi-task
generative adversarial network. In ECCV , 2018. 3, 8
[4] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and
Marie line Alberi Morel. Low-complexity single-image
super-resolution based on nonnegative neighbor embedding.
InBMVC , 2012. 4
[5] Joan Bruna, Pablo Sprechmann, and Yann LeCun. Super-
resolution with deep convolutional sufficient statistics. In
ICLR , 2016. 1, 3
[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L. Yuille. Deeplab: Semantic im-
age segmentation with deep convolutional nets, atrous con-
volution, and fully connected crfs. IEEE TPAMI , 2016. 1
[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and
Hartwig Adam. Rethinking atrous convolution for semantic
image segmentation. arXiv , 2017. 1, 5, 6
[8] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune
Gwon, and Sungroh Yoon. Ilvr: Conditioning method for
denoising diffusion probabilistic models. In ICCV , 2021. 2
[9] Ziteng Cui, Yingying Zhu, Lin Gu, Guo-Jun Qi, Xiaoxiao
Li, Renrui Zhang, Zenghui Zhang, and Tatsuya Harada. Ex-
ploring resolution and degradation clues as self-supervised
signal for low quality object detection. In ECCV , 2022. 3
[10] Dengxin Dai, Yujian Wang, Yuhua Chen, and Luc Van Gool.
Is image super-resolution helpful for other vision tasks? In
WACV , 2016. 3
[11] Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and
Lei Zhang. Second-order attention network for single image
super-resolution. In CVPR , 2019. 2
[12] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Image super-resolution using deep convolutional net-
works. IEEE TPAMI , 2016. 2
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In ICLR , 2020. 1,
2
[14] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
and A. Zisserman. The PASCAL Visual Object Classes
Challenge 2012 (VOC2012) Results. http://www.pascal-
network.org/challenges/VOC/voc2012/workshop/index.html.
1, 4, 6
[15] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge.
Texture synthesis using convolutional neural networks. In
NIPS , 2015. 1, 3
[16] Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis,
Richard Zemel, Wieland Brendel, Matthias Bethge, and Fe-lix A Wichmann. Shortcut learning in deep neural networks.
Nature Machine Intelligence , 2020. 2, 4
[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In NIPS , 2014.
2
[18] Muhammad Haris, Gregory Shakhnarovich, and Norimichi
Ukita. Deep back-projection networks for super-resolution.
InCVPR , 2018. 1, 2
[19] Muhammad Haris, Shakhnarovich Greg, and Ukita Norim-
ichi. Task-driven super resolution: Object detection in low-
resolution images. In ICONIP , 2021. 3, 5, 8
[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
2016. 1, 5, 6
[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. NIPS , 2020. 2
[22] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh
Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,
Ruoming Pang, Vijay Vasudevan, Quoc V . Le, and Hartwig
Adam. Efficientnet: Rethinking model scaling for convolu-
tional neural networks. ICCV , 2019. 5, 6
[23] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. MobileNets: Efficient convolu-
tional neural networks for mobile vision applications. arXiv ,
2017. 1
[24] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kil-
ian Q. Weinberger. Densely connected convolutional net-
works. In CVPR , 2017. 1
[25] Loshchilov I. and Hutter F. Sgdr: Stochastic gradient descent
with warm restarts. In ICLR , 2017. 5
[26] Loshchilov Ilya and Hutter Frank. Decoupled weight decay
regularization. In ICLR , 2019. 5
[27] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
losses for real-time style transfer and super-resolution. In
ECCV , 2016. 1, 2, 3
[28] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming
Song. Denoising diffusion restoration models. NIPS , 2022.
2
[29] Jiwon Kim, Jung Kown Lee, and Kyoung Mu Lee. Accurate
image super-resolution using very deep convolutional net-
works. In CVPR , 2016. 1, 2
[30] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
3d object representations for fine-grained categorization. In
ICCV workshops , 2013. 4, 6
[31] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classification with deep convolutional neural net-
works. In NIPS , 2012. 1
[32] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe
Shi. Photo-realistic single image super-resolution using a
generative adversarial network. In CVPR , 2017. 2
[33] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc
Van Gool, and Radu Timofte. SwinIR: Image restoration
using swin transformer. In ICCV , 2021. 1, 2, 5, 6
2659
[34] Jie Liang, Hui Zeng, and Lei Zhang. Details or artifacts:
A locally discriminative learning approach to realistic image
super-resolution. In CVPR , 2022. 2
[35] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and
Kyoung Mu Lee. Enhanced deep residual networks for single
image super-resolution. In CVPR Workshops , 2017. 1, 2, 5,
6
[36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence
Zitnick. Microsoft COCO: common objects in context. In
ECCV , 2014. 5
[37] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Dollár. Focal loss for dense object detection. In ICCV ,
2017. 1
[38] Ding Liu, Bihan Wen, Xianming Liu, Zhangyang Wang, and
Thomas S Huang. When image denoising meets high-level
vision tasks: A deep learning approach. In IJCAI , 2018. 3, 5
[39] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C.
Berg. Ssd: Single shot multibox detector. In ECCV , 2016. 1
[40] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A convnet for the
2020s. CVPR , 2022. 1
[41] Zhuang Liu, Hung-Ju Wang, Tinghui Zhou, Zhiqiang Shen,
Bingyi Kang, Evan Shelhamer, and Trevor Darrell. Explor-
ing simple and transferable recognition-aware image pro-
cessing. IEEE TPAMI , 2022. 3, 5
[42] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In
CVPR , 2015. 1
[43] Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens Sjölund,
and Thomas B Schön. Image restoration with mean-reverting
stochastic differential equations. In ICML , 2023. 2
[44] Ben Niu, Weilei Wen, Wenqi Ren, Xiangde Zhang, Lianping
Yang, Shuzhen Wang, Kaihao Zhang, Xiaochun Cao, and
Haifeng Shen. Single image super-resolution via a holistic
attention network. In ECCV , 2020. 2
[45] JoonKyu Park, Sanghyun Son, and Kyoung Mu Lee.
Content-aware local gan for photo-realistic super-resolution.
InICCV , 2023. 2
[46] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. In NIPS Workshops , 2017. 5
[47] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Unified, real-time object de-
tection. In CVPR , 2016. 1
[48] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In NIPS , 2015. 1, 5, 6
[49] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Li Fei-Fei. ImageNet large scale visual recognition chal-
lenge. IJCV , 2015. 2, 4, 5
[50] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-
mans, David J Fleet, and Mohammad Norouzi. Image super-
resolution via iterative refinement. IEEE TPAMI , 2022. 2[51] Mehdi SM Sajjadi, Bernhard Scholkopf, and Michael
Hirsch. Enhancenet: Single image super-resolution through
automated texture synthesis. In ICCV , 2017. 2
[52] Jacob Shermeyer and Adam Van Etten. The effects of super-
resolution on object detection performance in satellite im-
agery. In CVPR Workshops , 2019. 3
[53] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In ICLR ,
2015. 1, 2, 4
[54] Jae Woong Soh, Gu Yong Park, Junho Jo, and Nam Ik Cho.
Natural and realistic single image super-resolution with ex-
plicit natural manifold discrimination. In CVPR , 2019. 2
[55] Sanghyun Son, Jaeha Kim, Wei-Sheng Lai, Ming-Hsuan
Yang, and Kyoung Mu Lee. Toward real-world super-
resolution via adaptive downsampling models. IEEE TPAMI ,
2021. 2
[56] Mingxing Tan and Quoc V . Le. Efficientnet: Rethinking
model scaling for convolutional neural networks. ICML ,
2019. 1
[57] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Be-
longie. The caltech-ucsd birds-200-2011 dataset. Technical
Report CNS-TR-2011-001, California Institute of Technol-
ogy, 2011. 4, 6
[58] Li Wang, Dong Li, Yousong Zhu, Lu Tian, and Yi Shan.
Dual super-resolution learning for semantic segmentation. In
CVPR , 2020. 3
[59] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu,
Chao Dong, Yu Qiao, and Chen Change Loy. ESRGAN:
enhanced super-resolution generative adversarial networks.
InECCV Workshops , 2018. 2
[60] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.
Real-esrgan: Training real-world blind super-resolution with
pure synthetic data. In ICCV workshop , 2021. 2
[61] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image
restoration using denoising diffusion null-space model. In
ICLR , 2023. 2
[62] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang
Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: A gen-
eral u-shaped transformer for image restoration. In CVPR ,
2022. 2
[63] Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xing-
long Wu, Yapeng Tian, Wenming Yang, and Luc Van Gool.
Diffir: Efficient diffusion model for image restoration. In
ICCV , 2023. 2
[64] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,
Jose M Alvarez, and Ping Luo. Segformer: Simple and ef-
ficient design for semantic segmentation with transformers.
2021. 1
[65] Fanghua Yu, Jinjin Gu, Zheyuan Li, Jinfan Hu, Xiangtao
Kong, Xintao Wang, Jingwen He, Yu Qiao, and Chao Dong.
Scaling up to excellence: Practicing model scaling for photo-
realistic image restoration in the wild. In CVPR , 2024. 2
[66] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk
Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-
larization strategy to train strong classifiers with localizable
features. In ICCV , 2019. 4
2660
[67] Syed Waqas Zamir, Aditya Arora, Salman Khan, Mu-
nawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang.
Restormer: Efficient transformer for high-resolution image
restoration. In CVPR , 2022. 2
[68] Roman Zeyde, Michael Elad, and Matan Protter. On sin-
gle image scale-up using sparse-representations. In Interna-
tional Conference on Curves and Surfaces , 2010. 4
[69] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and
David Lopez-Paz. Mixup: Beyond empirical risk minimiza-
tion. In ICLR , 2017. 4
[70] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang.
Learning deep cnn denoiser prior for image restoration. In
CVPR , 2017. 1
[71] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR , 2018. 2, 3, 5
[72] Wenqiang Zhang, Zilong Huang, Guozhong Luo, Tao Chen,
Xinggang Wang, Wenyu Liu, Gang Yu, and Chunhua Shen.
Topformer: Token pyramid transformer for mobile semantic
segmentation. In CVPR , 2022. 1
[73] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng
Zhong, and Yun Fu. Image super-resolution using very deep
residual channel attention networks. In ECCV , 2018. 2
[74] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and
Yun Fu. Residual dense network for image super-resolution.
InCVPR , 2018. 1, 2
[75] Xiaotong Zhao, Wei Li, Yifan Zhang, and Zhiyong Feng.
Residual super-resolution single shot network for low-
resolution object detection. IEEE Access , 6:47780–47793,
2018. 3
[76] Liguo Zhou, Guang Chen, Mingyue Feng, and Alois Knoll.
Improving low-resolution image classification by super-
resolution with enhancing high-frequency content. In ICPR ,
2021. 3, 5
2661
