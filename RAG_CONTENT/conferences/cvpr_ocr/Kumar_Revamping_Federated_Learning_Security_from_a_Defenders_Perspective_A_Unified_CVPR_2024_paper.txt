Revamping Federated Learning Security from a Defender’s Perspective: A
Unified Defense with Homomorphic Encrypted Data Space
K Naveen Kumar
IIT Hyderabad, India
cs19m20p000001@iith.ac.inReshmi Mitra
SEMO, USA
rmitra@semo.eduC Krishna Mohan
IIT Hyderabad, India
ckm@cse.iith.ac.in
Abstract
Federated Learning (FL) facilitates clients to collabo-
rate on training a shared machine learning model without
exposing individual private data. Nonetheless, FL remains
susceptible to utility and privacy attacks, notably evasion
data poisoning and model inversion attacks, compromising
the system’s efficiency and data privacy. Existing FL de-
fenses are often specialized to a particular single attack,
lacking generality and a comprehensive defender’s perspec-
tive. To address these challenges, we introduce Federated
Cryptography Defense (FCD), a unified single framework
aligning with the defender’s perspective. FCD employs
row-wise transposition cipher based data encryption with
a secret key to counter both evasion black-box data poison-
ing and model inversion attacks. The crux of FCD lies in
transferring the entire learning process into an encrypted
data space and using a novel distillation loss guided by the
Kullback-Leibler (KL) divergence. This measure compares
the probability distributions of the local pretrained teacher
model’s predictions on normal data and the local student
model’s predictions on the same data in FCD’s encrypted
form. By working within this encrypted space, FCD elimi-
nates the need for decryption at the server, resulting in re-
duced computational complexity. We demonstrate the prac-
tical feasibility of FCD and apply it to defend against eva-
sion utility attack on benchmark datasets (GTSRB, KBTS,
CIFAR10, and EMNIST). We further extend FCD for de-
fending against model inversion attack in split FL on the CI-
FAR100 dataset. Our experiments across the diverse attack
and FL settings demonstrate practical feasibility and ro-
bustness against utility evasion (impact >30) and privacy
attacks (MSE >73) compared to the second best method.
1. Introduction
In recent years, the demand for collaborative learning al-
gorithms that meet a comprehensive set of criteria has in-
creased [24]. The criteria encompass key aspects such asdata confidentiality, privacy, enhanced utility, robustness,
and fairness [45]. Federated Learning (FL) [10, 12, 55]
is a pioneering approach to collaborative machine learning
(ML) that fulfils these vital requirements. The applicability
of FL is evident in its wide-ranging applications, including
mobile user personalization (Gboard) [1], healthcare [25],
finance sector [17], among others.
Threats to FL. The decentralized nature of FL makes it
highly susceptible to adversarial threats, primarily catego-
rized as utility-centric and privacy-centric [48]. Utility-
centric threats involve malicious attempts to poison data
or models, thereby compromising accuracy [11, 40, 59].
Privacy-centric threats such as model inversion attacks
(MIA) pertain to participant data reconstruction, risking the
leakage of sensitive information and eroding trust in the
FL system [20, 37, 51]. These threats are further classi-
fied based on their origin, distinguishing between causative
(during training) and evasion (during testing) attacks [48].
Notably, evasion attacks manipulate the model’s predictions
by modifying the input test data during inference on de-
ployed models, posing a severe threat to FL’s utility com-
pared to causative attacks [24, 48]. Effectively countering
these threats is pivotal for enhancing FL’s security and pre-
serving the previously mentioned factors. Therefore, this
paper focuses on mitigating the untargeted black-box eva-
sion data poisoning attacks that are commonly encountered
in real-world deployments [48]. Our overall aim is to im-
plement a unified defense framework that can safeguard FL
against both evasion utility and MIA privacy attacks during
inference. This choice is motivated by a research gap to de-
fend evasion attacks [24] [21, 50] and the limited work that
tackles both attacks simultaneously.
Limitations of existing FL defense strategies. Table 1 il-
lustrates the gaps within current FL defense approaches and
underscores the vast scope of our work (please refer to the
Supplementary material for a comprehensive related work
discussion). Notably, existing research in FL adversarial de-
fenses has predominantly concentrated on a particular sin-
gle type, such as utility or privacy-centred attacks. They of-
ten neglect the defender’s perspective, especially address-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
24387
Table 1. Comparison of existing defenses and their applicability
for utility-centric attacks (U-CA) and privacy-centric attacks (P-
CA) in FL. EA: evasion attack, DIm: defense impact, DB: defense
budget, DV: defense visibility, src: source of the defense, S: server,
and C: client. denotes strongly yes, denotes strongly no.
FL
defense
category (src)FL
defense
methodsCan defend U-P
tradeoff
analysisDefender’s
perspective
analysis
U-CA |EA P-CA DIm DB DV
Adversarial
training ( C)FAT [61], RS [8],
FedDynAT [39],
GALP [13]|
Byzantine
robust
aggregation
techniques (S)Krum [3],
ShieldFL [30],
FLTrust [5]|
Data/ update
analysis
(S and/or C)DeepSight [38],
FL-Defender [18],
SparseFed [36]|
Secure
multi-party
computation (C)AMPC [58],
Byrd et al. [4]|
Trusted
execution
environments (C)Flatee[35],
Chen et al. [6],
PPFL [34]|
Differential
privacy (C)NbAFL [52],
Huet al. [16],
2DP-FL [54]|
Homomorphic
encryption (C)DCAE [62],
PEFL [29],
Batchcrypt[57]|
FCD (ours)
src: S and C|
ing both utility and privacy threats in a unified manner.
This perspective is essential as it offers insights into the de-
fense’s robustness, adaptability, budget, multipurpose effi-
ciency, visibility, and overall impact on the FL system. Fur-
ther, within the domain of FL, adversaries can exhibit multi-
faceted intentions, spanning both utility and privacy threats.
Deploying distinct defense mechanisms for each individual
attack escalates the defender’s budget, increases the over-
all computational complexity, and reduces the adaptability
to practical FL trustworthy systems. Also, defenses that
are easily accessible and transparent to attackers are sus-
ceptible to circumvention. While differential privacy (DP)
[16, 52, 54], secure multi-party computation (MPC) [4, 58],
and trusted execution environments (TEE) [6, 34, 35] have
been proposed as potential solutions to strengthen privacy in
FL, none of the existing solutions offers a satisfactory unifi-
cation of privacy and utility defense under different attacks,
coupled with reasonable efficiency [45].
In this work, we propose an approach called Federated
Cryptography Defense (FCD) to counter both utility and
privacy evasion attacks through the strategic implementa-
tion of fully homomorphic encryption (FHE) in the data
space via row-based cryptography transformation. Unlike
existing defense methods (as shown in Table 1), which typ-
ically focus on defending against either utility or privacy
threats, our defense framework aligns with the defender’s
perspective by working in the data space rather than gra-
dient space. It stands out as a practical, cost-effective, and
multipurpose defense mechanism. One key advantage is thepreservation of data confidentiality, achieved through trans-
ferring the entire process on encrypted data using a shared
secret key. Further, our approach enables server-side test-
ing on encrypted data without the need for decryption, thus
significantly reducing computational complexity. On the
other hand, clients engage in training on encrypted data,
where we introduce a novel distillation loss guided by the
Kullback-Leibler (KL) divergence. This novel loss func-
tion minimizes the distillation loss between the pretrained
teacher model and the local student model responses. In
addition, the KL divergence loss, combined with the con-
ventional cross-entropy loss, serves as a total loss for up-
dating the local model. Furthermore, we have developed
two distinct threat models, TM1 and TM2, addressing util-
ity and privacy vulnerabilities in FL and expanding on the
attacker’s potential impact on the FL system. We present
the first proposal for this type of unified defense in FL to
the best of our knowledge. Our contributions are:
1.Novel defense: We introduce a unified defense frame-
work called FCD, which uses row-based transposition
cipher to defend against both evasion adversarial and
MIA attacks at the server aligning to the defender’s per-
spective.
2.Loss function: To enable effective learning, we pro-
pose a novel distillation loss guided by KL divergence
between teacher and student model predictions on the
client’s side.
3.Threat models: We introduce two realistic threat mod-
els (TM1 and TM2) concerning to utility and privacy at-
tacks.
4.Theoretical analysis: We present a possible theoretical
analysis of our method w.r.t. convergence and resilience
to attacks.
5.Effective tradeoff analysis: We perform extensive
experiments with five different datasets and mod-
els, namely, GTSRB (CNN), KBTS (CNN), CI-
FAR10 (ResNet18), CIFAR100 (VGG11), and EMNIST
(LeNet), alternating between threat models. We also in-
vestigate the performance under homogeneous and het-
erogeneous (non-IID) data shard settings.
2. Preliminaries
Definition 2.1 (FL data setup. ) We consider an FL sys-
tem with a central server and nclients. Each client Ck,
where k∈[1, n], possesses its private local dataset Dk,
referred to as a shard. In this context, we define Dk=
{Xi,k,Yi,k}Nk
i=1⊆Rd×R.Without loss of generality, we
assume that ∥Xi,k∥2= 1 holds for all k∈[1, n], where
i∈[1,Nk]and the final component of each point is fixed at
1/2, denoted as Xd= 1/2(l2norm data normalization).
The server uses synchronous federated weighted averaging
24388
(FedAvg) [32] for aggregation, represented as:
θt+1
g=θt
g+nX
k=1λk∇θt
k (1)
where λk=NkPNk, andP
kλk= 1.
We investigate two FL data shard settings: (i) Homoge-
neous , where each client’s dataset size is identical, i.e.,
|D1|=|D2|=..|Dn|=|D|
n, and (ii) Heterogeneous , in-
volving non-independent and non-identically (non-IID) dis-
tributed data achieved by partitioning the dataset using a
Dirichlet distribution [33] with parameter β= 1 among
clients. Additional information on standard FL, evasion at-
tacks in FL, homomorphic encryption, and transposition ci-
pher is provided in the supplementary material.
Threat model. We introduce two distinct threat mod-
els, TM1 and TM2, formulated to reflect real-world FL
production deployment settings, as illustrated in Figure 1.
Our threat models address an honest-but-curious (HbC) ad-
versary at the central server, as inspired by related work
[41, 48]. In TM1 (evasion utility attack) , the adversary
conducts an indiscriminate evasion attack by manipulating
test data at the central server during inference, aiming to
misclassify a substantial portion of the inputs [24, 48]. In
TM2 (privacy attack) , the adversary’s goal shifts to per-
forming a model inversion attack (MIA) [15] with the aim
of reconstructing private data. We provide our practical as-
sumptions and more details about the threat models in the
Supplementary material.
Benign
clients  
Central  
server
Aggregated global
deployment model
Malicious input
Stop (97.5%)Gradient
noiseGlobal
model 
Model
inversion
attack
Reconstructed
private dataTM1: utility-centr ed
data poisoning attackTM2: privacy-centr ed
model inversion attack
Global
model 
MisclassificationBenign
updates
Figure 1. Overview of two different threat models (TM) with po-
tential vulnerabilities and attacks during inference.
3. Proposed Framework
Problem statement. In each communication round t, sup-
pose that the server receives model updates from mclients,
and there exists a test dataset Dtest={Xi,Yi}Nte
i=1with a
size of Nte. The adversary introduces a µρ-bounded poi-
soned test dataset in TM1, denoted as ˜Dtest={˜Xi,Yi}ˆNte
i=1,
with the dataset size ˆNtevarying based on the attack vol-
umeρas:Definition 3.1 (µρ-bounded adversary. ) Let Fbe the
model’s function class. An adversarial perturbation is char-
acterized by the mapping A:=F × X × R→˜X.
Forµ > 0, we define the l2norm ball as B2(X, µ) :=
{˜X ∈ Rd:∥˜X − X∥ ≤ µ}TX. We classify the
adversary as µρ-bounded if it adheres to the condition
A(F,Xi,Yi)ρ
i=1∈ B(Xi, µ)ρ
i=1. Furthermore, for a given
µ > 0, ρ > 0, we denote the worst-case adversarial per-
turbation as A+:= arg max ˜X∈B (X,µ)L(fθ(˜Xi,Yi)ρ
i=1),
where Lrepresents the loss function of the global model.
Next, we formulate the objective of the adversary in MIA
attack (TM2) using the below definition.
Definition 3.2 (ϵ-distorted .) LetX∗represent the data re-
constructed by the adversary using the MIA attack. The ob-
jective of the adversary is to minimize the reconstruction er-
rorPin terms of as the mean squared error (MSE) between
the reconstructed data X∗and the original input data Xas
P:=∥X∗−X∥ 2≡1
NteNteX
i=1(X∗
i−Xi)2≤ϵ,for some ϵ≥0,
where ϵis the distortion of the reconstructed image.
Futher, the global model, Gθgattains a global test accu-
racy of Agbefore the attack and A∗
gafter the attack (Defi-
nition 3.1). Now, our FCD integrated clients have two pri-
mary objectives. First, they aim to minimize the impact of
the attack, which is quantified by U= (Ag− A∗
g), in order
to preserve the global test accuracy. Second, they seek to
enhance the privacy of local data Dk={Xi,k,Yi,k}Nk
i=1by
maximizing the reconstruction error P. Here, UandPde-
fine the utility and privacy gains in the context of FL, where
lower values of Uand higher values of Pare considered
desirable.
FCD framework description. The FCD framework intro-
duces modifications to standard FL, which can be catego-
rized into server-side and client-side changes. Algorithm 1
and Figure 2 present our FCD integrated FL system.
Server side. To initiate the FL process, the central server ini-
tializes a key denoted as K. This key consists of unique ran-
dom integers in the range from 0 to h−1, where hrepresents
the height of the input image data. The key Kis defined as
K= [κ0, κ1, . . . , κ i, κj, . . . , κ h−1], and it follows a prop-
erty that if i, j∈0, . . . , h −1withi̸=j, then κi̸=κj.
This property guarantees that no row in the input image can
be missed, ensuring the completeness of the data. During
each communication round t, the central server follows the
standard FL procedure by sending the current version of the
global model parameters θt
gto all clients. Synchronous fed-
erated weighted averaging-based aggregation is employed
at the server, as outlined in [32]. Further, the central server
performs the FCD transformation on the evasion-attacked
24389
test data to obtain E(ˆDtest). Subsequently, the server tests
the current aggregated global model Gt
θon the FCD trans-
formed test data, i.e., E(ˆDtest), as opposed to standard FL
systems that test on the original poisoned test data ˆDtest.
Client side. LetE(Xk)represent the FCD-encrypted form
of input samples from client Ckbased on the shared se-
cret key K. We denote Qkas the prediction probabilities
when the local model fθ,kis applied to the transformed data
E(Xk), which can be expressed as Qk=σ(fθ,k(E(Xk))),
where σ(.)denotes the softmax function. Consequently,
the cross-entropy loss LCEkof client Ck, trained on FCD-
encrypted data ( in contrast to general FL systems that cal-
culates LCEkon normal data Xk), can be calculated as fol-
lows:
LCEk=NkX
i=1LCE(Qi,k,Yi,k) =−NkX
i=1Yi,klogQi,k.(2)
To enhance the learning process and facilitate knowl-
edge transfer, we employ the knowledge distillation (KD)
approach [60], guided by the Kullback-Leibler (KL) diver-
gence. In this approach, the student model fθ,kis trained
to mimic the predictions of the teacher model. In our FCD
framework, there is no distinct teacher model, instead, it is
the same local model fθ,kinitially trained on normal data
at the beginning of the FL process. We introduce Pkto
denote the prediction probabilities when the local teacher
model fθ,kis applied to normal data Xk, represented as
Pk=σ(fθ,k(Xk)). Subsequently, we calculate the dis-
tillation loss guided by KL divergence ( LKLD k) of client k
between the prediction probabilities of the teacher and the
student local model, which has been trained on normal and
FCD-encrypted data, respectively, as follows:
LKLD k(Qk∥Pk) =NkX
i=1RX
r=1Qr
i,klogQr
i,k
Pr
i,k, (3)
where Ris the number of classes. Hence, the complete
loss for client Ck, unlike traditional FL systems that solely
consider LCEk, is expressed as:
Lk=LCEk+αkLKLD k, (4)
where αkserves as the weighting factor, regulating the
balance between the cross-entropy loss and the distillation
loss for client Ck. Finally, each client minimizes the to-
tal loss Lkover its FCD-transformed dataE(Xk)across E
local iterations. The local model fθ,kundergoes parame-
ter updates through a backward pass, adjusting as θnew
k=
θold
k−ηk∇θold
kLk,where ηksignifies the learning rate of
clientCk. Following each training phase, the client(s) trans-
mit their local updates, denoted as ∇θt
k=θt
k−θt
g, back to
the server.Algorithm 1 Standard FL with our FCD framework
Input: Global model Gθ,t, local data Dk= (Xk,Yk)
Output: Global test accuracy Ag
1:Client execution (θt
g,K):
2:foreach client k= 1tondo
3: Train teacher model fθ,kon normal data Dk
4:{Pi,k}i=(1,Nk)← {σ(fθ,k(Xi,k))}i=(1,Nk)
5:E(Xk)←FCD (Xk,K)
6:foreach client k= 1tondo
7: Initialize the local model θt
k←θt
g
8: forb= 1tobatches ∈ E(Xk)do
9: {Qb,k} ← { σ(fθ,0(E(Xk[b])))}
10: LCEk(Qb,k,Yb,k)←Eq. 2, Cross-entropy loss
11: LKLD k(Qb,k∥Pb,k)←Eq. 3, Distillation loss
12: Lk← L CEk+αkLKLD k▷Total loss (Eq. 4)
13: θt
k←θt
k−η∇θt
kLk
14: ∇θt
k←θt
k−θt
g
15: return ∇θt
k
16:Server execution (∇θt
k):
17:Share θt
g,Kto all the clients
18:Receive model updates from selected clients ← ∇θt
k
19:Perform model aggregation using FedAvg (Eq. 1)
20:Update the global model parameter: θt+1
g
21:ˆXtest←Poisoned test data ▷TM1, TM2
22:E(ˆXtest)←FCD (ˆXtest,K)
23:Compute Ag←Test (Gθt+1
g,E(ˆXtest))
24:return Ag
FCD cryptographic encryption. The missing element in
the framework described above is the FCD encryption (a de-
tailed algorithm and a visual representation are in the Sup-
plementary material). We introduce a homomorphic en-
cryption technique for FL utilizing a symmetric transpo-
sition cipher [43], with a secret key Kshared among all
clients by the server. To ensure data privacy and minimize
the potential for brute-force attacks, we define the key’s di-
mension as K ∈Rh, where hcorresponds to the image
height. The FCD encryption process comprises two steps:
(1) transposing the input image xto obtain x′=xT, where
rows are transformed into columns, and ( 2) reordering rows
based on the sequence specified in the secret key K. Es-
sentially, the positions of pixel values in all colour channels
(R, G, B ) are altered according to K, i.e.,RK(x′) =x′[:
,K,:],where RK(.)denotes the row-wise shuffling of in-
put data based on the secret key. Further, we introduce the
concept of ζ-separability for the FCD encrypted data space.
This concept relates to the distance between FCD-encrypted
data space and adversarial perturbations. It specifies that the
FCD data space maintains a separation of at least ζfrom
attacked data, ensuring robustness against µρ-bounded ad-
versarial perturbation (as defined in Definition 3.1). This
24390
 
Local model FCD encrypted input data space    
Back propagationAggregated
global model  Secret key      shared by the server
Speed limit 80 (93%)
Model inversion attack
TM2
 
 
 
 Original input
data space  Client-side training
TM1
 
 
Gradient
noiseMalicious infer ence
Local teacher model 
prediction pr obabilitiesReconstructed private dataServer -side executionFigure 2. Overview of the FCD-integrated FL system, with a focus on one client’s training and server-side execution. We compute the
cross-entropy loss ( LCE) for the local model on encrypted data and calculate the distillation loss ( LKL) using KL divergence between
the pretrained teacher model and the local student model for normal and encrypted data, respectively. These losses collectively update the
local model fθ. Similarly, test data is transformed into the encrypted space to counter potential attacks.
property is crucial to ensure that the FCD-encrypted data
space effectively mitigates utility evasion attacks during in-
ference (TM1).
Definition 3.3 (ζ-separability. )
LetE(D) :=[
k∈[1,n],i∈[1,Nk]{(E(Xk,i),Yk,i)} ⊆Rd×R
represent the FCD encrypted local training data space.
Similarly, E(Dtest)denotes the FCD encrypted test data
space such that for both training and test splits, E(X) =
RK(XT). We say that our FCD encrypted data space
isζ-separable w.r.t. µρ-bounded adversarial data pertur-
bation ˜X(as given in Definition 3.1) for some ζ > 0
iff∥E(Xi)−˜Xj∥ ≥ ζholds for any E(Xi)∈ E(D),
E(Xi)∈ E(Dtest), and ˜Xj∈ Aµρ(Xtest).
Rationale for FCD Design. The transposition of data be-
fore row-wise transformations enhances information secu-
rity robustness by obscuring critical transformation details,
providing defense against black-box evasion attacks. The
use of a row-based transposition cipher, compatible with
row-major programming languages (Python, C, and C++)
[46], ensures efficiency in memory access, parallel process-
ing, and cryptographic operations, aligning with both secu-
rity and computational considerations in our FCD integrated
FL system. Please refer to the Supplementary material for a
more detailed rationale.
FCD dual property benefits aligned with defender’s per-
spective. (i) Low visibility: The FCD transformation, in-
corporating both transposition and a secret key, ensures low
visibility by creating unique gradients for each key, offering
robust protection against various attacks. (ii) Low budget:
FCD’s vectorized operations and constrained key dimen-
sion facilitate efficient implementation, making it suitable
for large-scale FL systems with minimal resource require-
ments and addressing concerns related to processing timeand aligning w.r.t. defender’s perspective. Please refer to
the Supplementary material for more details about the FCD
dual property.
Lemma 3.4 The expected time complexity of our FCD en-
cryption function E(X)is linear, specifically O(nh), where
nrepresents the number of samples, and hdenotes the im-
age height. This is a significant enhancement compared to
theO(nwN2logN)time complexity required for encryp-
tion and decryption of model parameters, where nWrep-
resents the number of model parameters, and Nis the bit
length of the key [19]. (Please refer Supplementary mate-
rial for the proof.)
3.1. Convergence and Feasibility Analysis of FCD
We start by establishing the convergence of our FCD in-
tegrated FL global model. Subsequently, we demonstrate
the robustness of our FCD against gradient noise-based µρ-
bounded data poisoning attacks (as defined in Definition
3.1). Finally, we establish the resilience of our FCD against
MIAs with ϵ-distorted characteristics (as described in Def-
inition 3.2) at the server. We provide the respective proofs
in the Supplementary material.
Theorem 3.5 FCD convergence. Under the regular-
ity conditions of L-smoothness, τ-strong convexity, and
a decaying learning rate, our FCD integrated FL with
clients trained on encrypted data E(X), obtained using
FCD(X,K), the global model converges to
E[Gθg]−G∗≤2L
τ(γ+T)B+C
τ+ 2L∥θ0
g−θ∗
g∥2
+D.
Here, the variables have the following meanings: B=
Γ + ( E−1)2, where Γrepresents the measure of non-IID
data distribution. Csignifies the client selection for ag-
gregation, with C= 0 when all nclient updates are con-
sidered. Tdenotes the number of global communication
24391
rounds, and G∗represents the optimal global model [28].
The positive constant D≤ψquantifies how distillation
enhances the convergence rate. It accelerates convergence
by transferring learnable knowledge from the local teacher
model trained on Xto a student model trained on E(X).
The constant ψquantifies how distillation accelerates con-
vergence in our specific setup.
Corollary 3.5.1 (Robustness to µρ-bounded attacks.) Let
A:=Gθg× X test×R→˜Xtestbe the induced adver-
sarial perturbations on the test data using the black-box
global model Gθg. This perturbation adheres to the con-
straint ∥˜Xtest−X test∥ ≤µsuch that A(Gθg,Xtest,i,Yi)ρ
i=1∈
B(Xtest,i, µ)ρ
i=1(according to Definition 3.1). Furthermore,
Gθgis robust to ˜Xtestthrough the mechanism of our FCD
integrated FL system. It allows Gθgto test on encrypted
data based on K, i.e.,E(˜Xtest)instead of ˜Xtest. This ro-
bustness is achieved because the adversary’s objective is to
generate perturbations in the original test data space that
maximize the global loss (Definition 3.1), as denoted by
A+:= arg max ˜Xtest∈B(Xtest,µ)L(Gθg(˜Xtest,i,Yi)ρ
i=1).How-
ever, this adversarial strategy is effective when training and
testing occur in the normal data space Xat the client
side, matching the distribution of Xtest. In our FCD in-
tegrated FL system, training and testing occur in the en-
crypted data space. Notably, L(Gθg(˜Xtest,i,Yi)ρ
i=1)̸=
L(Gθg(E(˜Xtest,i),Yi)ρ
i=1)asE(˜Xtest)≜RK(˜XT
test)̸=˜Xtest
and∥E(Xi)−˜Xj∥ ≥ ζholds for any E(Xi)∈ E(D),
E(Xi)∈ E(Dtest), and ˜Xj∈ A µρ(Xtest)(Definition 3.3).
Hence, the learning process occurs in different data spaces
unknown to the adversary. Consequently, the gradients of
the loss function trained on normal data are notably dis-
tinct from those trained on FCD encrypted data, effectively
mitigating µρ-bounded adversarial data.
Theorem 3.6 (Resilience to ϵ-distorted MIA attacks.) Let
X∗represent the data reconstructed by the adversary us-
ing MIA attack as per the conditions specified in Definition
3.2. We demonstrate that training on FCD-encrypted data
space, denoted as E(X), imparts resilience to ϵ-distorted
MIA attacks. Specifically, our result establishes that:
∥X∗∥ − ∥E (X)∥≤ϵ+δ,for some ϵ≥0andδ≥0.
4. Experiments
Datasets, implementation details, and metrics. Table 2
presents comprehensive details about the datasets, FL setup,
attack percentage, and metrics. For TM1, we employed
the black-box and active data poisoning technique called
MSimBA [23]. Each experiment was conducted thrice, andresults were averaged with standard deviations presented.
Please refer to the Supplementary material for more details.
Baselines. For TM1, we have chosen FAT [61] and Ran-
domized Smoothing (RS) [8] based on their relevance and
applicability in evasion attacks within FL, as outlined in
Table 1. Additionally, we explore the effectiveness un-
der the following configurations: (a) With/without attack
and defense and (b) two FCD setups under different client
counts and attack percentages ( Ap). These setups include
FCD-homoFL (homogeneous FL) and FCD-hetFL (hetero-
geneous FL). In TM2, we follow the baselines as per [27].
4.1. FCD Result Discussion
Benign setting. Table 3 showcases the performance of
our FCD defense, along with other baselines and config-
urations, across four datasets under a no attack scenario .
Notably, FCD demonstrates comparable or higher accuracy
than vanilla FL, particularly excelling in datasets like GT-
SRB and KBTS. This improvement is attributed to FCD’s
ability to capture diverse transformation patterns with a se-
cret key, facilitating reduced training loss and improved
model convergence. Furthermore, with its symmetric cryp-
tographic transformation, FCD outperforms FAT and RS for
GTSRB and KBTS. However, in CIFAR10 and EMNIST,
FCD’s performance, while slightly lower than base FL, sur-
passes FAT and RS. This trade-off between utility and pri-
vacy is expected, as FCD provides both aspects with a mod-
est decrease in base accuracy.
Attack setting. We analyse FCD effectiveness under two
threat models separately.
(i) TM1: Table 4 illustrates the impact on utility ( U) for
FCD compared to other baselines under M-SimBA evasion
attack on GTSRB and KBTS datasets. The results indicate
a significant utility degradation with higher attack percent-
ages and more clients, emphasizing the impact of substan-
tial test data modification by M-SimBA. FCD consistently
demonstrates superior performance and robustness across
all attack percentages. In homogeneous FL settings, FCD
shows substantial improvement, maintaining a consistently
lower attack impact on utility compared to other defenses.
Particularly at Ap= 30% , the attack impact on utility is
2-6 units lower for FCD as the number of clients increases,
compared to other baselines. Similar trends are observed for
Ap= 50% ,100% . In heterogeneous FL settings, FCD out-
performs other defenses, showcasing robustness even when
clients are trained on varying amounts of non-IID data. For
the KBTS dataset, FCD exhibits substantial improvement
and lower attack impact across all attack percentages and
FL settings compared to other defenses. The limited train-
ing data in the KBTS dataset results in the RS technique
showing limited robustness, especially for Ap= 30% in
homogeneous and heterogeneous FL, followed by FAT. Ta-
24392
Table 2. Comprehensive experimental details: datasets, models, FL setup, attack configuration, and metrics.
Threat
modelDatasetTotal
clients ,nClient updates
used per round ,mAttack percentage (%),
Ap=ρ
Nte×100Model Global epochs Local epochs α Metrics
TM1GTSRB [44] 3, 5,
10, 15, 253, 5,
10, 15, 2530, 50, and 100Custom
CNN200 50.1, 0.2,
0.5 (default),
1,
1.5,
2U=Ag− A∗
gKBTS [31]
CIFAR10 [22] 100 40 and 70 ResNet18 [14]500 10EMNIST [7] 10000 100 and 500 LeNet5 [26]
TM2 CIFAR100 [22] Exact setup used in [27], n = m = 2 - VGG11 [42] 200 1 MSE
Table 3. Comparison of evasion defenses for homogeneous (Hom)
and heterogeneous (Het) FL settings on four datasets, in terms of
best global test accuracy ( Ag%)↑under no attack . All values are
percentages. NDdenotes no defense FL system.
GTSRB [44] KBTS [31] CIFAR10 [22] EMNIST [7]
Method Hom Het Hom Het Hom Het Hom Het
ND 96.35 ±0.02 94.98 ±0.06 98.30 ±0.80 97.10 ±0.62 82.14 ±1.15 81.54 ±1.64 89.26 ±2.12 88.34 ±1.34
FAT [61] 97.38 ±0.51 96.22 ±0.84 97.37 ±0.46 96.75 ±0.10 78.12 ±1.90 75.43 ±2.81 84.12 ±1.55 83.28 ±1.48
RS [8] 97.63 ±0.34 96.39 ±0.59 97.09 ±0.37 96.26 ±0.26 76.45 ±0.63 75.18 ±1.57 84.65 ±1.27 82.95 ±2.18
FCD (ours) 97.72 ±0.43 96.68 ±0.18 97.43 ±0.12 96.80 ±0.46 77.28 ±1.77 76.16 ±0.19 85.56 ±2.12 83.16 ±2.08
ble 5 presents results for CIFAR10 and EMNIST datasets,
highlighting FCD’s consistent robustness against evasion
attacks. However, a slightly higher attack impact on util-
ity is attributed to the server’s random selection of fewer
client updates for aggregation, leading to a less accurate
global model. Despite this randomness favoring the at-
tacker, FCD maintains superior performance compared to
other defenses. Additionally, Figure 3 provides qualitative
results of the FCD method under M-SimBA attack, comple-
menting the quantitative analysis.
(ii) TM2: Table 6 presents the MSE for FCD compared to
other baselines under MIA attacks in split FL [27]. FCD
exhibits a very high reconstruction error for MIA due to its
ability to operate under encrypted data space. This is be-
cause the FCD integrated FL system trains the local model
on encrypted data space, resulting in the reconstructed im-
ages by the adversary also being in encrypted form, leading
to a high MSE, as shown in Figure 4. In summary, FCD, as
a unified defense, demonstrates robustness against evasion
utility attacks and resilience to MIA privacy attacks under
TM1 and TM2, respectively.
5. Limitations and Potential Solutions
(i)Adaptive key attack : An adversary can introduce noise
to transform images and attempt inverse transformations
with an assumed key to generate adversarial samples. To
counter such attacks, employing multi-key encryption stan-
dards, such as E(. . .E(E(X,K1),K2), . . .Kl), proves ef-
fective. The adaptive nature of the attack necessitates the
adversary to guess keys randomly or heuristically, given the
lack of direct access to the key(s). Successfully misleading
the model becomes challenging unless the estimated key is
sufficiently close to the correct key. Due to the high di-
mensionality of K ∈Rh, finding a key proximate to Kis
Original data
M-SimBA
attack data
(human eye
imper ceivable
perturbations)
FCD encrypted
data
TransposeRow-wise secret-
key based
transformationSpeed limit
80 (95.2%)Social
distance
(93.8%)Pedestrian
crossing
(94.9%)No entry
(95.3%)Yield 
(94.8%)
Stop
 (90.7%)Overtaking 
(89.4%)Bicycle crossing
(92.9%)Speed
limit 100
(91.6%)Deer
crossing
(89.8%)
Speed limit
80 (93.6%)Social
distance
(91.4%)Pedestrian
crossing
(93.7%)No entry
(93.9%)Yield 
(92.8%)Figure 3. Visualization of FCD transformed M-SimBA adversarial
samples on the GTSRB dataset in TM1 with original class labels,
adversarial attack samples and labels, and FCD-transformed im-
ages and labels, with corresponding confidence scores provided
beneath each label.
Original data
MIA  on SFL
MIA  on ResSFL
MIA  on FCD
Figure 4. Visualization of FCD’s successful defense against MIA
on the CIFAR100 dataset in TM2. As FL is trained on FCD-
transformed images, the regenerated images from MIA closely re-
semble the encrypted data space, preserving the original data form.
a non-trivial task. Moreover, conducting a computational
search for all keys in the multi-key encryption becomes a
computationally demanding problem.
(iii)Adaptive gradient attack : Estimation over transfor-
mations proves beneficial for calculating gradients in ad-
versarial defenses [2]. Instead of taking a single step in
the direction of gradients ∇xf(x), the adversary can ag-
gregate over multiple steps, given byPs
i=1∇xf(x), where
sis the number of keys used by the attacker to gener-
24393
Table 4. Comparison of evasion defenses in terms of impact on utility ( U)↓under M-SimBA attack across different FL configurations
for two datasets. NDdenotes an FL system without defense. Bold indicate best results.
Dataset SettingAp→ 30% 50% 100%
n=m ND FAT [61] RS [8] FCD (ours) ND FAT [61] RS [8] FCD (ours) ND FAT [61] RS [8] FCD (ours)
GTSRB [44]Hom3 49.55 ±1.60 0.99±0.57 0.94±0.71 0.52±0.73 50.05 ±1.34 1.35±1.11 0.43±1.90 0.27±1.12 53.25 ±1.10 1.92±1.16 1.74±1.23 0.85±0.99
5 58.15 ±1.69 1.17±0.56 1.09±0.29 0.71±1.32 58.25 ±1.41 1.43±1.47 1.83±1.50 1.01±0.87 64.25 ±1.83 3.01±0.34 2.32±1.61 1.27±1.59
10 71.85 ±0.90 2.47±0.62 1.86±0.51 0.91±1.04 73.45 ±0.41 4.64±0.43 4.3±1.85 3.12±0.84 79.35 ±1.80 7.88±1.13 5.93±1.31 4.41±1.72
15 70.35 ±1.23 8.84±1.88 3.77±1.12 2.84±0.70 72.65 ±1.84 9.95±1.18 5.94±1.87 4.13±1.38 75.25 ±1.17 11.81 ±1.25 7.83±1.03 5.2±1.25
25 79.55 ±1.13 13.03 ±1.51 8.93±1.53 6.62±0.68 79.95 ±1.05 13.47 ±0.97 14.48 ±1.62 7.92±1.39 81.55 ±0.67 17.12 ±0.94 11.3±0.27 9.02±0.61
Het3 56.18 ±1.93 0.34±1.57 0.31±0.11 0.23±0.19 56.58 ±1.15 0.86±0.66 0.15±1.05 0.10±0.77 59.48 ±1.53 1.03±1.36 0.96±1.70 0.22±0.75
5 62.68 ±1.09 2.41±1.81 3.59±1.75 1.78±1.09 64.28 ±1.81 4.39±1.42 2.38±0.58 1.41±0.26 64.68 ±0.59 3.38±1.64 4.59±1.86 2.9±0.97
10 64.28 ±1.46 7.74±1.27 6.79±0.29 5.82±0.93 69.28 ±1.54 3.76±1.65 7.29±1.52 1.81±1.43 73.78 ±1.62 9.72±1.15 7.39±0.98 5.52±0.36
15 73.78 ±1.12 7.68±0.95 7.44±0.69 0.96±1.53 74.58 ±0.76 8.62±0.19 10.21 ±0.93 6.33±0.73 75.38 ±1.18 11.92 ±1.57 13.15 ±1.72 9.28±1.01
25 74.28 ±0.45 14.33 ±0.40 17.79 ±1.73 8.08±0.11 76.98 ±0.35 15.11 ±0.92 23.61 ±0.15 9.28±1.79 77.08 ±1.32 18.86 ±1.93 24.86 ±1.30 11.58 ±0.17
KBTS [31]Hom3 15.5 ±0.29 2.74±0.22 6.42±1.64 0.13±1.45 17.3±0.71 4.73±1.37 7.75±1.49 0.13±1.06 17.7±0.36 5.64±1.40 8.41±1.51 1.73±0.39
5 20 ±1.39 4.03±0.72 8.63±1.91 0.63±0.52 20.5±1.29 8.96±0.70 10.37 ±0.66 0.73±1.66 24.1±1.96 12.24 ±1.87 16.35 ±1.16 4.23±0.61
10 20.4 ±0.88 16.65 ±1.95 36.85 ±0.20 0.63±1.10 25.5±1.55 17.43 ±1.72 38.7±1.25 1.33±1.27 27.7±0.64 17.57 ±0.13 46.6±1.68 6.93±0.28
15 31.7 ±0.81 21.41 ±0.95 51.56 ±0.63 1.03±1.21 32.5±0.42 30.21 ±1.22 66.5±0.43 1.67±1.04 41.4±0.91 31.94 ±1.41 77.45 ±1.26 8.53±1.01
25 46.9 ±1.04 34.17 ±1.07 85.94 ±1.60 1.33±1.65 40.7±1.88 34.41 ±1.23 86.11 ±2.01 2.03±1.42 48.4±1.94 35.38 ±1.35 87.29 ±1.27 11.83 ±0.15
Het3 15.2 ±1.59 1.56±0.51 9.47±1.23 0.35±1.18 18.1±1.89 2.05±1.58 11.3±1.02 0.5±0.62 20.8±1.86 6.48±1.08 17.38 ±1.81 1.7±0.89
5 22.2 ±0.39 5.49±1.10 13.13 ±1.74 0.53±1.02 23.4±0.13 7.02±1.73 19.64 ±1.09 2.4±1.24 26±1.45 10.87 ±1.71 22.77 ±1.26 5.24±0.51
10 27.3 ±1.68 14.59 ±0.39 39.77 ±0.20 1.32±1.48 29.6±0.58 14.79 ±1.99 40.29 ±0.32 5.12±1.95 32.2±1.14 18.15 ±0.32 41.76 ±1.23 6.7±0.30
15 32.7 ±0.38 17.09 ±0.66 57.1±1.56 1.7±0.21 36.6±0.98 21.19 ±1.93 58.34 ±0.36 6.6±1.53 42.6±0.23 23.59 ±0.57 60.82 ±1.37 9.5±0.12
25 43.6 ±0.49 31.7±1.39 73.31 ±1.40 1.1±1.12 45.1±1.32 32.66 ±1.63 76.1±1.53 8.2±1.20 46.2±0.51 34.11 ±1.20 78.19 ±0.51 12.2±0.70
Table 5. Comparison of evasion defenses in terms of impact on utility ( U)↓under M-SimBA attack across different FL configurations,
n,mfor two datasets. NDdenotes an FL system without defense. Bold indicate best results.
Dataset SettingAp→ 30% 50% 100%
n m ND FAT [61] RS [8] FCD (ours) ND FAT [61] RS [8] FCD (ours) ND FAT [61] RS [8] FCD (ours)
CIFAR10 [22]Hom 10040 27.37 ±1.34 16.05 ±0.85 18.23 ±0.27 10.81 ±0.49 32.82 ±0.42 20.2±1.24 42.89 ±0.37 11.95 ±0.69 58.53 ±0.99 46.18 ±0.92 65.26 ±1.13 13.04 ±0.44
70 30.54 ±0.46 18.93 ±1.50 22.37 ±1.42 12.94 ±0.60 45.85 ±0.24 24.67 ±1.06 45.41 ±0.77 13.04 ±1.42 66.07 ±0.94 48.1±0.98 65.44 ±0.40 14.02 ±1.16
Het 10040 25.39 ±1.46 10.47 ±1.44 20.22 ±0.82 9.82±1.11 35.95 ±0.39 21.11 ±0.38 39.87 ±1.32 10.8±0.95 57.1±0.71 37.49 ±0.17 61.45 ±0.35 13.84 ±0.11
70 32.97 ±0.54 11.12 ±0.31 24.14 ±1.09 11.83 ±1.26 41.96 ±0.50 22.63 ±1.01 41.88 ±1.18 12.92 ±0.97 61.68 ±0.79 40.12 ±0.91 63.92 ±1.30 14.04 ±0.86
EMNIST [7]Hom 10000100 22.1 ±0.42 9.47±0.19 27.73 ±0.18 10.44 ±0.23 39.76 ±0.64 19.7±0.18 38.42 ±1.19 12.47 ±0.76 58.17 ±1.07 36.65 ±1.41 56.86 ±0.58 12.96 ±0.51
500 34.91 ±0.45 17.4±0.46 36.2±0.96 12.34 ±0.51 41.8±0.68 27.77 ±0.20 47.43 ±1.15 15.28 ±0.73 73.71 ±1.13 47.49 ±0.80 72.99 ±0.57 15.7±1.39
Het 10000100 20.32 ±1.03 10.04 ±0.47 25.98 ±0.54 9.49±0.84 40.32 ±0.75 17.8±1.10 38.67 ±0.22 11.10 ±0.84 61.85 ±0.47 40.02 ±0.52 61.56 ±0.49 12.06 ±1.45
500 34.63 ±0.73 14.19 ±1.34 34.37 ±0.46 11.88 ±0.28 47.12 ±0.46 30.04 ±0.28 49.55 ±1.29 13.71 ±0.70 70.14 ±1.48 45.5±0.58 71.23 ±0.85 15.93 ±0.36
Table 6. Comparison of MSE ( ↑) of FCD with other methods un-
der TM2 MI attack for the CIFAR100 dataset.
Defense method MSE
Laplacian [47] 0.011
Dropout [15] 0.009
TopkPrune [56] 0.005
AdvNoise [53] 0.018
DistCorr [49] 0.019
Bottleneck Layers [9] 0.02
ResSFL [27] 0.05
FCD (ours) 73.23
ate adversarial samples. The introduced multi-key encryp-
tion method E(. . .E(E(X,K1),K2), . . .Kl), with a suffi-
ciently large l >> s and each key having dimension Rh,
makes it computationally challenging to search for nearly
identical keys. Additionally, the order of Eencryption in
multi-key encryption is unique, i.e., E(E(X,Ki),Kj)̸=
E(E(X,Kj),Ki)for any given keys Ki,Kj. Thus, the pro-
posed solutions offer simplicity and robustness from a two-
level perspective (multi-key, order of encryption). We plan
to explore such attacks and solutions in our future work.6. Conclusion
In this paper, we introduced FCD, a unified defense method
in FL that simultaneously protects against utility evasion at-
tacks and privacy model inversion attacks (MIA). The crux
of FCD is that it respects the defender’s perspective, care-
fully managing the trade-off between utility and privacy by
transforming the entire learning process into a homomor-
phic encrypted data space. We formulated two threat mod-
els, TM1 & TM2, to address utility and privacy attacks and
provided theoretical analysis on convergence along with ro-
bustness evaluations for both utility and privacy. Our ex-
tensive evaluations across various attack scenarios demon-
strate that FCD maintains a lower attack impact on utility
and achieves higher Mean Squared Error (MSE) on recon-
structed data for privacy attacks, aligning closely with the
ideal defender’s perspective under different attack settings.
However, our approach shows promising results but can be
further improved to minimize attack impact under random
client selection. Future efforts will also involve investigat-
ing multi-key multi-level encryption standards to enhance
FCD’s resilience against adaptive attackers.
24394
References
[1] Federated learning: Collaborative machine learning without
centralized training data. 2017. 1
[2] Anish Athalye, Nicholas Carlini, and David Wagner. Obfus-
cated gradients give a false sense of security: Circumventing
defenses to adversarial examples. In International Confer-
ence on Machine Learning , pages 274–283. ICML, 2018. 7
[3] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui,
and Julien Stainer. Machine learning with adversaries:
Byzantine tolerant gradient descent. Advances in Neural In-
formation Processing Systems , 30, 2017. 2
[4] David Byrd and Antigoni Polychroniadou. Differentially pri-
vate secure multi-party computation for federated learning in
financial applications. In Proceedings of the First ACM In-
ternational Conference on AI in Finance , pages 1–9, 2020.
2
[5] Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang
Gong. Fltrust: Byzantine-robust federated learning via trust
bootstrapping. arXiv preprint arXiv:2012.13995 , 2020. 2
[6] Yu Chen, Fang Luo, Tong Li, Tao Xiang, Zheli Liu, and Jin
Li. A training-integrity privacy-preserving federated learn-
ing scheme with trusted execution environment. Information
Sciences , 522:69–79, 2020. 2
[7] Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre
Van Schaik. Emnist: Extending mnist to handwritten letters.
In2017 international joint conference on neural networks
(IJCNN) , pages 2921–2926. IEEE, 2017. 7, 8
[8] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified
adversarial robustness via randomized smoothing. In inter-
national conference on machine learning , pages 1310–1320.
PMLR, 2019. 2, 6, 7, 8
[9] Amir Erfan Eshratifar, Amirhossein Esmaili, and Massoud
Pedram. Bottlenet: A deep learning architecture for intelli-
gent mobile cloud computing services. In 2019 IEEE/ACM
International Symposium on Low Power Electronics and De-
sign (ISLPED) , pages 1–6. IEEE, 2019. 8
[10] Chen Fang, Yuanbo Guo, Yongjin Hu, Bowen Ma, Li Feng,
and Anqi Yin. Privacy-preserving and communication-
efficient federated learning in internet of things. Computers
& Security , 103:102199, 2021. 1
[11] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Zhen-
qiang Gong. Local model poisoning attacks to byzantine-
robust federated learning. In Proceedings of the 29th
USENIX Conference on Security Symposium , pages 1623–
1640, 2020. 1
[12] Xiaojie Guo, Zheli Liu, Jin Li, Jiqiang Gao, Boyu Hou,
Changyu Dong, and Thar Baker. V eri fl: Communication-
efficient and fast verifiable aggregation for federated learn-
ing. IEEE Transactions on Information Forensics and Secu-
rity, 16:1736–1751, 2020. 1
[13] Ehsan Hallaji, Roozbeh Razavi-Far, Mehrdad Saif, and En-
rique Herrera-Viedma. Label noise analysis meets adversar-
ial training: A defense against label poisoning in federated
learning. Knowledge-Based Systems , page 110384, 2023. 2
[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. CoRR ,
abs/1512.03385, 2015. 7[15] Zecheng He, Tianwei Zhang, and Ruby B Lee. Attacking and
protecting data privacy in edge–cloud collaborative inference
systems. IEEE Internet of Things Journal , 8(12):9706–9716,
2020. 3, 8
[16] Rui Hu, Yuanxiong Guo, Hongning Li, Qingqi Pei, and Yan-
min Gong. Personalized federated learning with differential
privacy. IEEE Internet of Things Journal , 7(10):9530–9539,
2020. 2
[17] Ahmed Imteaj and M Hadi Amini. Leveraging asynchronous
federated learning to predict customers financial distress. In-
telligent Systems with Applications , 14:200064, 2022. 1
[18] Najeeb Moharram Jebreel and Josep Domingo-Ferrer. Fl-
defender: Combating targeted attacks in federated learning.
Knowledge-Based Systems , 260:110178, 2023. 2
[19] Zoe L Jiang, Hui Guo, Yijian Pan, Yang Liu, Xuan Wang,
and Jun Zhang. Secure neural network in federated learn-
ing with model aggregation under multiple keys. In 2021 8th
IEEE International Conference on Cyber Security and Cloud
Computing (CSCloud)/2021 7th IEEE International Confer-
ence on Edge Computing and Scalable Cloud (EdgeCom) ,
pages 47–52. IEEE, 2021. 5
[20] Xiao Jin, Pin-Yu Chen, Chia-Yi Hsu, Chia-Mu Yu, and
Tianyi Chen. Cafe: Catastrophic data leakage in vertical fed-
erated learning. Advances in Neural Information Processing
Systems , 34:994–1006, 2021. 1
[21] Taejin Kim, Shubhranshu Singh, Nikhil Madaan, and Carlee
Joe-Wong. pfeddef: Characterizing evasion attack transfer-
ability in federated learning. Software Impacts , page 100469,
2023. 1
[22] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009. 7, 8
[23] K Naveen Kumar, C Vishnu, Reshmi Mitra, and C Krishna
Mohan. Black-box adversarial attacks in autonomous vehicle
technology. In 2020 IEEE Applied Imagery Pattern Recog-
nition Workshop (AIPR) , pages 1–7. IEEE, 2020. 6
[24] K Naveen Kumar, C Krishna Mohan, and Linga Reddy
Cenkeramaddi. The impact of adversarial attacks on fed-
erated learning: A survey. IEEE Transactions on Pattern
Analysis and Machine Intelligence , 2023. 1, 3
[25] Yogesh Kumar and Ruchi Singla. Federated learning systems
for healthcare: perspective and recent progress. Federated
Learning Systems: Towards Next-Generation AI , pages 141–
156, 2021. 1
[26] Yann LeCun, L ´eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
7
[27] Jingtao Li, Adnan Siraj Rakin, Xing Chen, Zhezhi He,
Deliang Fan, and Chaitali Chakrabarti. Ressfl: A resistance
transfer framework for defending model inversion attack in
split federated learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 10194–10202, 2022. 6, 7, 8
[28] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and
Zhihua Zhang. On the convergence of fedavg on non-iid
data. In 8th International Conference on Learning Repre-
sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020 , 2020. 6
24395
[29] Xiaoyuan Liu, Hongwei Li, Guowen Xu, Zongqi Chen, Xi-
aoming Huang, and Rongxing Lu. Privacy-enhanced feder-
ated learning against poisoning adversaries. IEEE Transac-
tions on Information Forensics and Security , 16:4574–4588,
2021. 2
[30] Zhuoran Ma, Jianfeng Ma, Yinbin Miao, Yingjiu Li, and
Robert H Deng. Shieldfl: Mitigating model poisoning at-
tacks in privacy-preserving federated learning. IEEE Trans-
actions on Information Forensics and Security , 17:1639–
1654, 2022. 2
[31] Markus Mathias, Radu Timofte, Rodrigo Benenson, and Luc
Van Gool. Traffic sign recognition — how far are we from
the solution? In The 2013 International Joint Conference on
Neural Networks (IJCNN) , pages 1–8, 2013. 7, 8
[32] Brendan McMahan, Eider Moore, Daniel Ramage, Seth
Hampson, and Blaise Aguera y Arcas. Communication-
efficient learning of deep networks from decentralized data.
InArtificial Intelligence and Statistics , pages 1273–1282.
PMLR, 2017. 3
[33] Thomas Minka. Estimating a dirichlet distribution, 2000. 3
[34] Fan Mo, Hamed Haddadi, Kleomenis Katevas, Eduard
Marin, Diego Perino, and Nicolas Kourtellis. Ppfl: privacy-
preserving federated learning with trusted execution environ-
ments. In Proceedings of the 19th annual international con-
ference on mobile systems, applications, and services , pages
94–108, 2021. 2
[35] Arup Mondal, Yash More, Ruthu Hulikal Rooparaghunath,
and Debayan Gupta. Poster: Flatee: Federated learning
across trusted execution environments. In 2021 IEEE Euro-
pean Symposium on Security and Privacy (EuroS&P) , pages
707–709. IEEE, 2021. 2
[36] Ashwinee Panda, Saeed Mahloujifar, Arjun Nitin Bhagoji,
Supriyo Chakraborty, and Prateek Mittal. Sparsefed: Mit-
igating model poisoning attacks in federated learning with
sparsification. In International Conference on Artificial In-
telligence and Statistics , pages 7587–7624. PMLR, 2022. 2
[37] Hanchi Ren, Jingjing Deng, and Xianghua Xie. Grnn: gen-
erative regression neural network—a data leakage attack for
federated learning. ACM Transactions on Intelligent Systems
and Technology (TIST) , 13(4):1–24, 2022. 1
[38] Phillip Rieger, Thien Duc Nguyen, Markus Miettinen, and
Ahmad-Reza Sadeghi. Deepsight: Mitigating backdoor at-
tacks in federated learning through deep model inspection.
arXiv preprint arXiv:2201.00763 , 2022. 2
[39] Devansh Shah, Parijat Dube, Supriyo Chakraborty, and
Ashish Verma. Adversarial training in communica-
tion constrained federated learning. arXiv preprint
arXiv:2103.01319 , 2021. 2
[40] Virat Shejwalkar, Amir Houmansadr, Peter Kairouz, and
Daniel Ramage. Back to the drawing board: A critical eval-
uation of poisoning attacks on production federated learn-
ing. In 2022 IEEE Symposium on Security and Privacy (SP) ,
2022. 1
[41] Young Ah Shin, Geontae Noh, Ik Rae Jeong, and Ji Young
Chun. Securing a local training dataset size in federated
learning. IEEE Access , 10:104135–104143, 2022. 3[42] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In In-
ternational Conference on Learning Representations , 2015.
7
[43] Massoud Sokouti, Babak Sokouti, and Saeid Pashazadeh. An
approach in improving transposition cipher system. Indian
Journal of Science and Technology , 2(8):9–15, 2009. 4
[44] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and
Christian Igel. The German Traffic Sign Recognition Bench-
mark: A multi-class classification competition. In IEEE
International Joint Conference on Neural Networks , pages
1453–1460, 2011. 7, 8
[45] Nurbek Tastan and Karthik Nandakumar. Capride learn-
ing: Confidential and private decentralized learning based
on encryption-friendly distillation loss. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8084–8092, 2023. 1, 2
[46] Jeyarajan Thiyagalingam, Olav Beckmann, and Paul HJ
Kelly. An exhaustive evaluation of row-major, column-major
and morton layouts for large two-dimensional arrays. In Per-
formance Engineering: 19th Annual UK Performance Engi-
neering Workshop , pages 340–351. University of Warwick
Coventry, UK, 2003. 5
[47] Tom Titcombe, Adam J Hall, Pavlos Papadopoulos, and
Daniele Romanini. Practical defences against model in-
version attacks for split neural networks. arXiv preprint
arXiv:2104.05743 , 2021. 8
[48] Dmitrii Usynin, Alexander Ziller, Marcus Makowski, Rick-
mer Braren, Daniel Rueckert, Ben Glocker, Georgios
Kaissis, and Jonathan Passerat-Palmbach. Adversarial in-
terference and its mitigations in privacy-preserving collabo-
rative machine learning. Nature Machine Intelligence , 3(9):
749–758, 2021. 1, 3
[49] Praneeth Vepakomma, Abhishek Singh, Otkrist Gupta, and
Ramesh Raskar. Nopeek: Information leakage reduction to
share activations in distributed deep learning. In 2020 Inter-
national Conference on Data Mining Workshops (ICDMW) ,
pages 933–942. IEEE, 2020. 8
[50] Su Wang, Rajeev Sahay, and Christopher G Brinton. How
potent are evasion attacks for poisoning federated learning-
based signal classifiers? arXiv preprint arXiv:2301.08866 ,
2023. 1
[51] Zhibo Wang, Mengkai Song, Zhifei Zhang, Yang Song, Qian
Wang, and Hairong Qi. Beyond inferring class representa-
tives: User-level privacy leakage from federated learning.
InIEEE INFOCOM 2019-IEEE Conference on Computer
Communications , pages 2512–2520. IEEE, 2019. 1
[52] Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H Yang,
Farhad Farokhi, Shi Jin, Tony QS Quek, and H Vincent Poor.
Federated learning with differential privacy: Algorithms and
performance analysis. IEEE Transactions on Information
Forensics and Security , 15:3454–3469, 2020. 2
[53] Jing Wen, Siu-Ming Yiu, and Lucas CK Hui. Defending
against model inversion attack by adversarial examples. In
2021 IEEE International Conference on Cyber Security and
Resilience (CSR) , pages 551–556. IEEE, 2021. 8
[54] Zuobin Xiong, Zhipeng Cai, Daniel Takabi, and Wei Li. Pri-
vacy threat and defense for federated learning with non-iid
24396
data in aiot. IEEE Transactions on Industrial Informatics ,
18(2):1310–1321, 2021. 2
[55] Guowen Xu, Hongwei Li, Sen Liu, Kan Yang, and Xiaodong
Lin. Verifynet: Secure and verifiable federated learning.
IEEE Transactions on Information Forensics and Security ,
15:911–926, 2019. 1
[56] Ziqi Yang, Jiyi Zhang, Ee-Chien Chang, and Zhenkai Liang.
Neural network inversion in adversarial setting via back-
ground knowledge alignment. In Proceedings of the 2019
ACM SIGSAC Conference on Computer and Communica-
tions Security , pages 225–240, 2019. 8
[57] Chengliang Zhang, Suyi Li, Junzhe Xia, Wei Wang, Feng
Yan, and Yang Liu. Batchcrypt: Efficient homomorphic en-
cryption for cross-silo federated learning. In Proceedings
of the 2020 USENIX Annual Technical Conference (USENIX
ATC 2020) , 2020. 2
[58] Chi Zhang, Sotthiwat Ekanut, Liangli Zhen, and Zengxi-
ang Li. Augmented multi-party computation against gradi-
ent leakage in federated learning. IEEE Transactions on Big
Data , 2022. 2
[59] Jiale Zhang, Bing Chen, Xiang Cheng, Huynh Thi Thanh
Binh, and Shui Yu. Poisongan: Generative poisoning attacks
against federated learning in edge computing systems. IEEE
Internet of Things Journal , 8(5):3310–3322, 2020. 1
[60] Linfeng Zhang, Chenglong Bao, and Kaisheng Ma. Self-
distillation: Towards efficient and compact neural networks.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence , 44(8):4388–4403, 2021. 4
[61] Giulio Zizzo, Ambrish Rawat, Mathieu Sinn, and Beat
Buesser. Fat: Federated adversarial training. arXiv preprint
arXiv:2012.01791 , 2020. 2, 6, 7, 8
[62] Tianyuan Zou, Yang Liu, Yan Kang, Wenhan Liu, Yuanqin
He, Zhihao Yi, Qiang Yang, and Ya-Qin Zhang. Defending
batch-level label inference and replacement attacks in ver-
tical federated learning. IEEE Transactions on Big Data ,
2022. 2
24397
