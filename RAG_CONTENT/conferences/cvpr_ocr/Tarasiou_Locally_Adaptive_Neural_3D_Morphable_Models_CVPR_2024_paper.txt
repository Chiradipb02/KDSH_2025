Locally Adaptive Neural 3D Morphable Models
Michail Tarasiou Rolandos Alexandros Potamias Eimear O’Sullivan
Stylianos Ploumpis Stefanos Zafeiriou
Imperial College London
{michail.tarasiou10,r.potamias,e.o-sullivan,s.ploumpis,s.zafeiriou }@imperial.ac.uk
Abstract
We present the Locally Adaptive Morphable Model
(LAMM), a highly flexible Auto-Encoder (AE) framework
for learning to generate and manipulate 3D meshes. We
train our architecture following a simple self-supervised
training scheme in which input displacements over a set of
sparse control vertices are used to overwrite the encoded
geometry in order to transform one training sample into
another. During inference, our model produces a dense
output that adheres locally to the specified sparse geom-
etry while maintaining the overall appearance of the en-
coded object. This approach results in state-of-the-art per-
formance in both disentangling manipulated geometry and
3D mesh reconstruction. To the best of our knowledge
LAMM is the first end-to-end framework that enables di-
rect local control of 3D vertex geometry in a single forward
pass. A very efficient computational graph allows our net-
work to train with only a fraction of the memory required by
previous methods and run faster during inference, generat-
ing 12k vertex meshes at >60fps on a single CPU thread.
We further leverage local geometry control as a primitive
for higher level editing operations and present a set of
derivative capabilities such as swapping and sampling ob-
ject parts. Code and pretrained models can be found at
https://github.com/michaeltrs/LAMM .
1. Introduction
The capacity to generate and manipulate digital 3D objects
lies at the core of a multitude of applications related to the
entertainment and media industries [9, 44], virtual and aug-
mented reality [12, 25, 29, 37, 42] and healthcare [10, 24].
In particular, the ability to manually shape virtual humans
can result in highly realistic avatars, while granting ultimate
creative control to individual users.
However, achieving fine control in mesh manipulation
necessitates the learning of a disentangled representation of
3D shapes which is still an open research problem [16–18].
Recently proposed methods based on Graph Convolutional
Figure 1. Overview of the Locally Adaptive Morphable Model
(LAMM) use during inference. (top) Our trained decoder fdecre-
ceives as inputs a latent code zand displacements δVCover a set of
sparse control points (red vertices). Here displacements for con-
trol points in the nose region are shown with green arrows. The
decoder generates the shape of the object encoded in z, overwrit-
ing local geometry to respect δVC. (bottom) We can sample latent
codes to generate new instances of human heads in the (a) identity,
(b) expression space. Similarly, we can either randomly sample or
provide vertex displacements manually at control points to manip-
ulate (c) local identity features (ears, nose, mouth shown here) and
(d) add expressions while retaining identity features unchanged.
Network (GCN)-based Auto-Encoders (AEs) [21, 27, 36]
have demonstrated impressive performance in dimension-
ality reduction but typically learn a highly entangled latent
space making them unsuitable for detailed shape manipu-
lation. Additionally, despite having a low parameter count,
these methods struggle to handle high-resolution meshes,
limiting their applicability. Few works [17, 18] have dealt
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
1867
with the disentanglement of local identity attributes, how-
ever these methods still rely on GCNs and opt for con-
trolling manipulations through the state of the latent code
which is partitioned and assigned to predefined object re-
gions. Using the latent code to drive shape manipulation
requires the use of explicit optimization objectives to learn
a disentangled latent space. Moreover, partitioning its state
is critically suboptimal for learning compressed representa-
tions of 3D objects. We propose a different paradigm which
does not involve partitioning the latent code or relying on
its state to drive changes in shape, resulting in state-of-the-
art (SOTA) disentanglement and reconstruction capabilities
in a unified architecture. Instead, we use a global latent
code for 3D object unconditional generation and utilise ad-
ditional inputs to jointly train our generative model to lo-
cally overwrite the latent encoded geometry. Our main con-
tributions are as follows:
• We present the Locally Adaptive Morphable Model
(LAMM), a general framework for manipulating the ge-
ometry of registered meshes. To the best of our knowl-
edge, this is the first method that allows direct shape con-
trol with a single forward pass. Applied on human 3D
heads, LAMM exhibits SOTA disentanglement proper-
ties and allows for very fine geometric control over both
facial identities and expressions.
• Our models, trained for manipulation, concurrently ex-
hibit SOTA performance in mesh dimensionality reduc-
tion compared against methods trained exclusively on
this task. As a result, a single model can be used to gen-
erate entirely new shapes and apply both localized and
global modifications to their geometry.
• We show how our framework can leverage direct control
as a primitive to achieve higher level editing operations
such as region swapping and sampling.
• By deviating from GCN-based AE design, LAMM can
scale to much larger meshes, needs only a fraction of
GPU memory to train and can be significantly faster dur-
ing inference compared to competing methods. For ex-
ample, trained on 72k vertex meshes with batch size 32,
our model requires 7.5GB of GPU memory and runs at
0.045son a single CPU thread. This model outperforms
SpiralNet++ [21] which equivalently requires >40GB of
memory and runs ×13slower at 0.58s.
• We release training and evaluation codes to support fu-
ture research. Additionally we will release pre-trained
models and a Blender add-on [11] to facilitate intuitive
mesh manipulation and fine-grained control.
2. Related Work
3D Morphable Models . Blanz and Vetter pioneered an in-
novative approach for synthesis and reconstruction of 3D
human faces through their seminal 3D Morphable Model
(3DMM) [30]. First, they establish a fixed mesh topol-ogy, ensuring that each face vertex is semantically signif-
icant and could be uniquely identified, e.g. the ”tip of the
nose”. Subsequently, Principal Component Analysis (PCA)
is employed to map 3D shapes and textures into a lower di-
mensional latent code. By adjusting these codes the PCA-
based 3DMM can seamlessly generate new facial instances.
Such 3DMMs are simple and very robust in terms of perfor-
mance. They are well established as exceptionally reliable
frameworks for 3D face/body analysis, finding applications
in numerous human centric models [5, 16, 19, 25, 26, 30–
33] which remain relevant to this date.
However, the linear formulation of PCA limits its expres-
sivity and capacity to capture high frequency details. In an
effort to capture nonlinear variations in the shape of human
faces performing expressions [36] were the first to utilise
Graph Convolutional Networks (GCN) for 3D dimension-
ality reduction. Their Convolutional Mesh AutoEncoder
(COMA) makes use of fast and efficient spectral convolu-
tions applied on a mesh surface [13] to compress and gen-
erate 3D meshes. An inductive bias more suitable for regis-
tered meshes is proposed in [7] who incorporate the spiral
operator [27] in a GCN-based AE. This operator uses a spi-
ral sequence to define an explicit local ordering for aggre-
gating node features, thus giving the convolution kernels a
sense of orientation across the mesh surface and greater ex-
pressive power. Following a similar principle, [21] further
improve the performance of this architecture by enabling
context aggregation across larger areas through dilated spi-
ral operators. The deep GCN-based family of models dis-
cussed above have been consistently shown to outperform
PCA at high levels of compression. However, when increas-
ing the size of the latent space to values that are practical for
applications, e.g. monocular 3D face reconstruction [6, 19],
PCA is shown to match and quickly surpass GCN archi-
tectures. We argue that the translation equivariant property
of GCNs is not optimal for modelling registered meshes as
it does not account for the distinct semantics of each ver-
tex. In contrast, by employing fully connected layers to en-
code input shapes into tokens and a global receptive field
for downstream processing, our framework respects this at-
tribute of the data, having the capacity to treat similar fea-
tures differently depending on their location.
Face Manipulation. All statistical models of 3D
meshes discussed above can be used to manipulate mesh
geometry, e.g. the optimal fit for a given shape can be dis-
cerned through latent code optimization. The main issue
with this approach is that latent spaces learnt for genera-
tive modelling are typically highly entangled [3, 16, 17, 35].
This attribute discards the possibility for finely-tuned con-
trol of local geometry as varying any one dimension of
the latent code can concurrently impact multiple proper-
ties of the reconstructed data. Numerous studies have
been proposed to specifically address the challenge of en-
1868
Figure 2. Architecture overview. A source meshVsis encoded into latent code zand decoded using additional displacements at control
points δVC. (top) Tokenization and encoder modules. A 3D input mesh Vsis split into regions Vs
i=Vs[Ri], each region is tokenized via
region-specific linear weights Win
iand compressed by the encoder module into latent code z. (bottom) Displacement control, decoder and
inverse tokenization modules. User displacements at control points are split into corresponding regions Ci, processed by region specific
control networks fδiand used by the decoder to overwrite the geometry encoded in z. The inverse tokenization module translates decoder
outputs into region geometries, via region-specific linear weights Wout
i, which are merged together, using Ri, into the target estimate ˆVt.
tangled representations of 3D face data for shape manip-
ulation [16]. They predominantly utilize face regions to
disentangle a latent representation and achieve local con-
trol through optimization, either by employing linear mod-
els [20] or GCN [2, 17, 18, 43]. Notably, works by [17, 18]
adeptly deform 3D face geometry to manipulate an indi-
vidual’s identity. Both works harness the backbone archi-
tecture from [21], along with various face regions, each
governed by non-overlapping parts of the latent code. In
[17] their AE learns a disentangled latent space by creat-
ing a composite batch through swapping arbitrary features
across different samples. This enables the definition of a
contrastive loss that leverages known differences and simi-
larities in the latent representations. In a related approach,
[18] also segment their latent codes to region specific parts
and use a loss component forcing segments to adhere to the
local eigenprojections of identity attributes. Similar to these
works, we also employ predefined regions to segment our
inputs. Importantly, we do not partition our latent codes into
region-specific segments, which is beneficial for retaining
strong performance in mesh reconstruction. Furthermore,
we do not enforce spatial disentanglement through explic-
itly defined loss components (we train only with L1loss)
but rather reach SOTA disentanglement performance solely
as an emerging property of our architecture.3. Method
In this section we present the LAMM framework for the dis-
entangled direct manipulation of 3D shapes. First, we de-
scribe the architecture, followed by a self-supervised train-
ing scheme for learning direct local control over mesh ge-
ometry. In the following, we assume a 3D mesh is repre-
sented as M={V,F}, where V ∈RN×3is a set of N
vertices represented as points in 3D space, and Fis a set of
faces defining a shared topology.
3.1. Architecture
We leverage the AE framework for mesh representation
learning, with targeted modifications in the decoder archi-
tecture to facilitate direct manipulation of mesh geome-
try. First, a source meshVsis encoded into a latent space
z=fenc(Vs)∈RDand subsequently decoded into an
estimated target geometry ˆVt=fdec(z, δVC)using con-
trol vertex displacements δVCas additional input to the de-
coder. Both encoder and decoder modules utilise patch-
based backbones, namely the Transformer [41] and MLP-
Mixer [39], that operate on sets of tokens and exhibit a ho-
mogeneous feature space, i.e. feature dimensions stay con-
stant throughout the network. An overview of the proposed
architecture is presented schematically in Fig.2.
1869
Tokenization. For deriving the inputs to the encoder
we first split each source object into Knon-overlapping
dense regions, Ri⊆ {1,2, . . . , N }(Fig.3). We further
flatten each region into a vector of size vs
i∈3Niand
tokenize it as a linear projection Win
ivs
i, where Win
i∈
RD×3Ni. The proposed tokenization scheme differs sig-
nificantly from the one used in [15, 39] that operate on
fixed-size square patches. First, no parameter sharing takes
place during this step. Instead each tokenization weight is
uniquely assigned to and solely applied to a corresponding
input vertex coordinate, thus, respecting the semantic mean-
ing of each vertex. This allows us to define regions with
varying number of vertices that are designed to include dis-
tinct object parts based on the template mesh, e.g. a ”whole
eye” in human face meshes. Finally, our regions are typ-
ically much larger than commonly employed in computer
vision [15, 38, 39], which results in a small number of to-
kens. For example, while the original implementation of
ViT [15] used 256 tokens for image classification, we use
as few as 11 tokens for 3D face modelling. Since the Trans-
former space and time complexities are both quadratic to
the number of tokens, our models are very efficient in terms
of memory consumption, can scale to very large meshes and
can run significantly faster than GCNs especially on a CPU.
Encoder. Our fixed size tokenized representation for re-
gioniisx0
i=Win
ivs
i. To these features we prepend a
learnable identity token x0
0∈RD[14, 40] which is inde-
pendent of the input. After processing by the L-layer homo-
geneous encoder, we retain only the state of the first index
token xL
0∈RDthat forms the encoded representation of
thesource geometry. To obtain our latent code we linearly
project this vector into a space of desired dimensionality
z=WdownxL
0∈Rd. The tokenization and encoder mod-
ules are illustrated in the top part of Fig.2.
Decoder. In the decoder module, we first linearly project
the latent code back into a Ddimensional vector y0
0=
Wupz. Analogously to the encoder, we build our decoder
inputs by appending Kadditional learned tokens y0
i∈RD
to the projected latent code y0
0[8, 38]. We then select
a set of non-overlapping, sparse vertex indices Ci⊆ R i
that are used as control points (Fig.3). We gather all de-
sired displacements at control points per region δVCi=
(Vt− Vs)[Ci]∈R3|Ci|and process each with a fully con-
nected network fδithe output of which is added to respec-
tive learned tokens y0
i, i≥1. We design fδiwith GELU
activations [22] and no bias terms. In this manner, for the
special case of δVCi= 0 we get fδi(0) = 0 and LAMM
seamlessly reproduces the behaviour of an AE.
Tokenization−1. Following the inverse process of re-
gion splitting and tokenization steps we now project each
token independently to a vector ˆvt
i=Wout
iyL
i∈R3Ni, re-
shape its dimensions to Ni×3and use Rito merge regions
into a unified 3D template mesh, ˆVt. The decoder and in-
Figure 3. Templates for UHM12k and Handy data. Colors indicate
dense regions Ri. Control points Ciare shown in red dots.
verse tokenization modules are presented schematically in
the bottom part of Fig.2.
3.2. Training
Self-supervised objective. At each step we sample B
source andtarget elements Vs,Vt∈RB×N×3from the
training set and construct our source andtarget batches re-
spectively as {Vs,Vs}and{Vs,Vt}. In this manner, the
first half of our batch is used for AE training and the sec-
ond half for learning source-to-target transformations. For
identity manipulation we use a random permutation of Vs
asVt. For expression manipulation, VsandVtinclude the
same identities with neutral and non-neutral expressions.
Training with direct supervision from real target data
has the benefit of providing realistic examples for the tar-
getgeometry space. However, there are a few issues with
this approach. In practice, interactive vertex control is an
iterative process. As a result typical user input displace-
ments are expected to be smaller and significantly sparser
than the ones that globally transform one sample into an-
other. To reduce the effect of displacement size discrep-
ancy we train with a linear combination of source andtar-
getsamples αVs+ (1−α)Vt, α∼ U(αmin,1)as the new
target values. This effectively reduces the absolute values
of displacements encountered during training in addition to
augmenting the target space. The sparsity discrepancy is
a trickier problem as it is not trivial to find realistic train-
ing pairs with only localized differences. We find that this
issue is solved to a satisfactory degree implicitly as the pro-
posed architecture naturally propagates deformations only
to non-zero-displacement regions. We believe this attribute
to be an intrinsic property of our architecture. In support of
this claim, we show in the supplementary material that only
through AE training, i.e. δVCi= 0, corrupting the values
of learnable decoder tokens y0
i, i > 0influences only their
corresponding region geometries.
Multilayer Loss. To adequately train our architecture,
a distance-based loss between the target and output geome-
tries is applied which suits our fundamental learning objec-
tive. In all experiments we utilize the L1distance metric,
1870
calculated as ||ˆVt−Vt||1. We introduce an extension to this
loss, applied at every level of the model, which increases
the encoder’s capacity to encode input-related information
and guides the decoder transformation from mean totarget
shape. Both employed backbones [39, 41] produce homo-
geneous feature spaces of size Dacross all encoder and de-
coder layers. At every layer lwe decode region features
xl
i,yl
i, i≥1using Wout
iand merge regions together to ob-
tain a multilayer output geometry.
The objective of the encoder is to obtain a compressed
representation of the input in the form of the latent code z.
At input, we want our tokens to retain as much information
about the input shape to begin with, so we add a loss com-
ponent ||Wout
ix0
i−vs
i||1that encourages the input tokens to
decode close to the source geometry. Furthermore, we pro-
vide some guidance to our encoder to progressively discard
input related information from the region features through
a loss component ||Wout
ixL
i−¯vi||1that encourages the fi-
nal encoder region tokens to match the mean shape for each
region ¯viover the training data. Similarly, we expand this
pattern for all intermediate layers in a linear fashion.
Lenc=LX
l=0λl
KKX
i=1||Wout
ixl
i−1
L((L−l)vs
i+l¯vi)||1(1)
In the decoder, we first center initialised tokens y0
i
through the term ||Wout
iy0
i−¯vi||1which encourages them
to decode into the mean shape. The output of the final layer
will be our model’s estimate of the object geometry, thus,
we supervise through the term ||Wout
iyL
i−vt
i||1. Similarly,
we apply a multilayer loss to intermediate layers to encour-
age a progressive transformation of the region tokens from
mean totarget geometry.
Ldec=LX
l=0λl
KKX
i=1||Wout
iyl
i−1
L(lvt
i+(L−l)¯vi)||1(2)
In both eqs.1, 2 λlis a weight for the loss component at
layer l. In practice we opt for a simple solution of keep-
ing all λl= 1 which leads to stable training and good
generalization performance. We note that in both equa-
tions the inverse tokenization weights Wout
iare shared in
every loss term, forcing a common geometric representa-
tion throughout the network which progressively transforms
from source tomean totarget . Additionally, our data are
typically mean centered prior to any processing, thus, ¯vire-
duces to the zero vector. In this case, our multilayer loss can
be viewed as a form of regularization forcing the values of
Wout
ito be either small in values or orthogonal to the learnt
tokens y0
i.4. Experiments
4.1. Implementation details
Datasets. For effectively learning disentangled human head
identity manipulations we need a large scale dataset of 3D
heads in neutral expression. For this reason, we register
6k heads from the original data used in the UHM model
[32] into a new quads-based template which consists of 12k
vertices, is more friendly towards rigging and is animation
ready. We will refer to this data as UHM12k. To create
our synthetic expression data we applied a set of 28 blend-
shapes [1] to the UHM12k data. To experiment with high
resolution meshes, we utilise the linear models provided
by the UHM [32] and sample 10k identities consisting of
72k vertices. To evaluate our method on hand data, we uti-
lized Handy [34], a large-scale hand model, trained from
more than 1k distinct identities. In particular, to generate
our hand dataset, we randomly sampled 10k hands from the
Handy latent space. All datasets were split into training and
evaluation sets at a 9-1 ratio.
Architecture. We optimize our architecture through an
ablation study on autoencoding using the UHM12k data,
presented in the supplementary material. Overall, our ar-
chitecture consists of five encoder and three decoder layers
with 512 feature dimension. For manipulation experiments
we split the UHM12k and the Handy templates into 11 and
9 regions and utilise 73 (101 for expression modelling) and
72 control points respectively, all of which are illustrated in
Figs.3, 5, 6. For identity control with the UHM data we use
the same face regions as [17, 18] and 91 control points.
Training. Our proposed architecture can train for sig-
nificantly longer than GCN baselines without observing a
drop in evaluation set performance. We train all networks
(including baselines) in mesh reconstruction and manipu-
lation for 1,500 epochs on a single Nvidia Titan X GPU,
with batch size 32 (adjusted for GCNs according to mem-
ory). We use the AdamW optimizer [23] starting with a 10
epoch learning rate warm up to a 10−4which is decayed
via a 1-cycle cosine decay [28] to 10−6. For local control
experiments, we find it is crucial to initialize our models
from checkpoints pre-trained exclusively for autoencoding.
Further details are provided in the supplementary material.
4.2. 3D shape reconstruction
In Table 1 we compare the reconstruction performance of
our proposed framework for autoencoding against PCA [4]
and state of the art GCNs [7, 21, 36]. Overall, our method
is shown to significantly outperform others applied in head
and hand data with improvements being more pronounced
on UHM12k. Comparing the two employed backbones,
the MLPMixer is a consistently strong performer, surpass-
ing baselines in all cases tested. Transformer-based back-
bones are found to be more suitable for the expression en-
1871
Figure 4. Qualitative assessment of trained AE in UHM12k. (top)
New shape generation through sampling of the latent space z, (bot-
tom) Interpolation between latent codes for two samples from the
evaluation data shows a smooth transition between identities.
Table 1. Quantitative evaluation of 3D shape reconstruction for
models trained exclusively in autoencoding with latent size 256.
Presented values are mean Euclidean distances ( ×10−2mm).
UHM12k +expr. UHM Handy
PCA 10.42 11.49 11.73 25.90
COMA [36] 13.11 14.20 15.40 27.20
Neural3DMM [7] 11.82 12.97 13.88 26.75
SpilarNet++ [21] 11.53 12.58 13.55 26.72
LAMM-Transformer 9.08 9.09 13.70 26.31
LAMM-MLPMixer 7.97 9.51 11.48 24.60
hanced data, but weaker than the MLPMixer for the UHM
and Handy data. In Fig.4 (top) we present generated sam-
ples drawn from our head model. Only the decoder part
of our architecture is used here. Similar to previous works
[4, 7, 21, 36] sampling new instances of global shape can
be achieved by fitting a Gaussian probability distribution to
zvalues collected over the training data. In the bottom part
of Fig.4 we choose two highly diverse identities from the
evaluation set, encode both and present geometries gener-
ated by linear interpolation of their latent codes, noting a
smooth transition between generated shapes.
4.3. Local control
Global manipulation. We present a quantitative evalu-
ation of models trained in shape manipulation in Table 2.
First, we note that AE performance is very similar to the
values presented in Table 1 where models are trained exclu-
sively in mesh reconstruction. In fact, AE performance is
found to improve in many cases suggesting the complemen-
tarity of the two tasks. We also find that expression training
can increase AE performance significantly. If we train for
expression manipulation but select our model based on its
AE performance our Transformer based model achieves
a distance error of 7.79×10−2mm, which is 14% less
compared to the best value we obtained from AE training
in Table 1 ( 9.08×10−2mm). Figs.5, 6 illustrate per-vertex
mean prediction errors plotted over respective templatesTable 2. Quantitative evaluation of models trained in 3D
shape manipulation. We present mean Euclidean distance error
(×10−2mm) for the AE case ( Vt=Vs), control ( ∈C) and re-
maining ( /∈C) vertices. We note AE performance can surpass
that of models trained exclusively for mesh reconstruction. †neu-
tral expression only.
UHM12k UHM12k+expr.
Backbone AE ∈C / ∈C AE† ∈ C / ∈C
Transformer 8.79 18.47 26.03 8.61 15.86 9.67
MLPMixer 8.36 19.87 28.08 8.98 15.92 9.73
UHM Handy
Backbone AE ∈C / ∈C AE ∈C / ∈C
Transformer 15.86 26.23 38.80 26.78 28.32 55.69
MLPMixer 11.63 17.97 32.03 24.59 25.57 51.66
for identity and expression manipulations. In Fig.5 these
are shown to be clearly smaller close to control points
verifying the metrics reported in Table 2. For human heads,
errors are mostly concentrated in the frontal region of the
face. For hands, errors are significantly larger and more
evenly spread. However, there are clear concentrations near
the wrist which can be attributed to the absence of control
points in these regions. We additionally show examples
ofsource totarget generations from the evaluation data,
observing how predicted geometries match the appearance
oftarget for both datasets. For expression manipulation, we
can generally observe from Fig.6 and Table 2 that errors are
smaller compared to identity manipulation. As expected,
we observe that errors are also concentrated in the face
region where most of the deformation takes place and
particularly small in head regions that do not participate
in facial expressions such as the ears and skull. We also
note that, in contrast to identity manipulation, errors are
significantly lower here at non-control points which can be
attributed to the fact that /∈ Ciincludes many vertices with
very small deformations, e.g. skull.
Region swap. For swaping regions among meshes it is
critical to find a set of control point displacements that best
transform the source region Vs
ito a desired target shapeVt
i.
However, calculating displacements directly results in non
realistic geometries as the two regions may be offset by a
significant amount. To resolve this issue, we first rigidly
align regions by their common outer edge indices.Due to
the simplicity of applied transformations, rigid alignment
is found not to disturb the appearance of the target shape,
keeping it realistic. The outcome of this operation is re-
duced offsets at the boundaries, however, there are still dis-
continuities due to the restricted nature of rigid alignment.
It is critical to note, however, that the region’s control ver-
tices, sparsely located in its interior, represent a valid sparse
geometry since there exists a valid dense shape that fits
these vertices while respecting the region’s boundary con-
ditions. After finding the optimal rigid transformation, it
1872
Figure 5. Identity manipulation performance under direct vertex control for UHM12k (left) and Handy (right) data. We observe that errors
are typically reduced close to control points. For both datasets our method learns to produce highly realistic output geometries.
Figure 6. Expression manipulation performance under direct vertex control for UHM12k data. (left) We observe that per-vertex errors are
very small and that ˆVtclosely resembles the appearance of Vt. (right) Linear interpolation in expression intensities for the two samples. We
observe a smooth transition between facial expressions and the ability to perform highly localized edits by manipulating eyes independently.
Figure 7. Head region swapping for UHM data.
can be applied to the control points of the target region to
obtain the updated target locations of the control vertices,
which in turn are used to calculate δVCi. We employ this
methodology for region swapping in human heads. Fig.7
demonstrates swapping the eyes, ears, nose and mouth re-
gions between two diverse meshes from the UHM data.
Region sampling and disentanglement. We show it
is possible to generate new dense region shapes by sam-
pling valid displacements for the control vertices. Using
the same approach described for region swapping, we ran-
domly select data pairs from the training data and collect a
set of valid control vertex displacements for every region.
To these, we fit a Gaussian distribution from which we can
sample from to produce new valid control point displace-
ments. We present samples of generated face regions inthe top part of Fig.8 using the UHM data. In the bottom
part, we utilise UHM12k to illustrate face part interpolation
for the ears and nose regions. We evaluate the capacity of
our model to have only localized effects in manipulated ge-
ometry in Fig.9 compared to SOTA methods SD-V AE [17]
and LED-V AE [18]. Consistently, we find that our method
leads to more localized effects for all face regions and im-
proved disentanglement over generated geometries. Impor-
tantly, we note that in contrast to [17], our framework is
trained without any loss component aimed towards disen-
tanglement which indicates this to be a intrinsic property
of our method. We provide additional evidence to support
this claim in the supplementary material. Furthermore, we
find that SD-V AE has low reconstruction performance at
18.90×10−2mm, more than double the reconstruction error
of our method from Table 2 and more than SpiralNet++ [21]
which is the backbone employed by SD-V AE. As we dis-
cuss in the supplementary material, this is mainly attributed
to the choice of partitioning the latent code made by SD-
V AE to control region geometries.
5. Conclusion
We have introduced LAMM, a comprehensive framework
for learning 3D mesh representations and achieving spa-
tially disentangled shape manipulation. To our knowledge,
LAMM is the first end-to-end trainable method that is capa-
ble of directly manipulating mesh shape, by using desired
displacements as inputs, in a single forward pass. Our ex-
1873
Figure 8. Random generation and interpolation of head regions.(top) Random head region generation on UHM (bottom) Region interpola-
tion for ears and nose using UHM12k. We observe a smooth transition between generated regions while remaining areas are unaffected.
Figure 9. Evaluation of disentanglement performance compared to SD-V AE [17], LED-V AE [18]. For all data in the evaluation set we
randomly sample 20 shapes for each head region and show per-vertex mean Euclidean distances between input and output shapes.
tensive experiments with 3D human head and hand mesh
data demonstrate that LAMM simultaneously attains state-
of-the-art performance in both disentanglement and recon-
struction within a unified architecture. Notably, it scales
more effectively to high-resolution data, requires less mem-
ory for training, and offers faster inference speeds compared
to existing methods. When implemented within the identity
space of human heads, our method exhibits significant po-
tential for applications in face sculpting. Additionally, whentrained in the expression space of human heads, LAMM
demonstrates considerable promise for manually enhancing
digital characters with expressive details and for its appli-
cation in the field of animation. Fast CPU inference makes
LAMM an energy efficient and economically viable option,
democratizing access to advanced 3D modeling.
Acknowledgements RP and SZ were supported by
GNOMON (EP/X011364). SZ was supported by EPSRC
Project DEFORM (EP/S010203/1).
1874
References
[1] 3d scan store. https://www.3dscanstore.com/ .
Accessed: 2023-11-13. 5
[2] Mohammadamin Aliari, Tiberiu Beauchamp, AndrePopa,
and Eric Paquette. Face editing using part-based optimiza-
tion of the latent space. Computer Graphics Forum , 2023.
3
[3] Tristan Aumentado-Armstrong, Stavros Tsogkas, Allan Jep-
son, and Sven Dickinson. Geometric disentanglement for
generative latent shape models. In 2019 IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV) , pages 8180–
8189, 2019. 2
[4] V olker Blanz and Thomas Vetter. A morphable model for
the synthesis of 3d faces. In Proceedings of the 26th Annual
Conference on Computer Graphics and Interactive Tech-
niques , page 187–194, USA, 1999. ACM Press/Addison-
Wesley Publishing Co. 5, 6
[5] James Booth, Anastasios Roussos, Stefanos Zafeiriou, Allan
Ponniah, and David Dunaway. A 3d morphable model learnt
from 10,000 faces. In 2016 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 5543–5552,
2016. 2
[6] James Booth, Epameinondas Antonakos, Stylianos
Ploumpis, George Trigeorgis, Yannis Panagakis, and
Stefanos Zafeiriou. 3d face morphable models” in-the-
wild”. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 48–57, 2017. 2
[7] Giorgos Bouritsas, Sergiy Bokhnyak, Stylianos Ploumpis,
Michael Bronstein, and Stefanos Zafeiriou. Neural 3d mor-
phable models: Spiral convolutional networks for 3d shape
representation learning and generation. In The IEEE Inter-
national Conference on Computer Vision (ICCV) , 2019. 2,
5, 6
[8] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In Computer Vision
– ECCV 2020 , pages 213–229, Cham, 2020. Springer Inter-
national Publishing. 4
[9] Lucy Chai, Richard Tucker, Zhengqi Li, Phillip Isola, and
Noah Snavely. Persistent nature: A generative model of
unbounded 3d worlds. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 20863–20874, 2023. 1
[10] Richard J Chen, Ming Y Lu, Tiffany Y Chen, Drew F K
Williamson, and Faisal Mahmood. Synthetic data in machine
learning for medicine and healthcare. Nat. Biomed. Eng. , 5
(6):493–497, 2021. 1
[11] Blender Online Community. Blender - a 3D modelling and
rendering package . Blender Foundation, Stichting Blender
Foundation, Amsterdam, 2018. 2
[12] Enric Corona, Mihai Zanfir, Thiemo Alldieck, Eduard
Gabriel Bazavan, Andrei Zanfir, and Cristian Sminchisescu.
Structured 3d features for reconstructing relightable and an-
imatable avatars. In CVPR , 2023. 1
[13] Micha ¨el Defferrard, Xavier Bresson, and Pierre Van-
dergheynst. Convolutional neural networks on graphs withfast localized spectral filtering. In Advances in Neural Infor-
mation Processing Systems . Curran Associates, Inc., 2016.
2
[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: Pre-training of deep bidirectional trans-
formers for language understanding. In Proceedings of the
2019 Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota, 2019. Association for
Computational Linguistics. 4
[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In International Conference on Learning Representa-
tions , 2021. 4
[16] Bernhard Egger, William A. P. Smith, Ayush Tewari, Ste-
fanie Wuhrer, Michael Zollhoefer, Thabo Beeler, Florian
Bernard, Timo Bolkart, Adam Kortylewski, Sami Romdhani,
Christian Theobalt, V olker Blanz, and Thomas Vetter. 3D
Morphable Face Models – Past, Present and Future. arXiv
e-prints , 2019. 1, 2, 3
[17] Simone Foti, Bongjin Koo, Danail Stoyanov, and Matthew J.
Clarkson. 3d shape variational autoencoder latent disen-
tanglement via mini-batch feature swapping for bodies and
faces. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 18730–
18739, 2022. 1, 2, 3, 5, 7, 8
[18] Simone Foti, Bongjin Koo, Danail Stoyanov, and Matthew J.
Clarkson. 3d generative model latent disentanglement via
local eigenprojection. Computer Graphics Forum , 42(6):
e14793, 2023. 1, 3, 5, 7, 8
[19] Baris Gecer, Stylianos Ploumpis, Irene Kotsia, and Stefanos
Zafeiriou. Ganfit: Generative adversarial network fitting
for high fidelity 3d face reconstruction. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 1155–1164, 2019. 2
[20] Donya Ghafourzadeh, Sahel Fallahdoust, Cyrus Rahgoshay,
Andre Beauchamp, Adeline Aubame, Tiberiu Popa, and Eric
Paquette. Local control editing paradigms for part-based 3D
face morphable models. Computer Animation and Virtual
Worlds , 32(6):e2028. 3
[21] Shunwang Gong, Lei Chen, Michael Bronstein, and Stefanos
Zafeiriou. Spiralnet++: A fast and highly efficient mesh
convolution operator. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision Workshops , pages 0–
0, 2019. 1, 2, 3, 5, 6, 7
[22] Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities
and stochastic regularizers with gaussian error linear units.
CoRR , abs/1606.08415, 2016. 4
[23] Loshchilov Ilya, Hutter Frank, et al. Decoupled weight decay
regularization. Proceedings of ICLR , 2019. 5
[24] Firas Khader, Gustav M ¨uller-Franzes, Soroosh Tayebi
Arasteh, Tianyu Han, Christoph Haarburger, Maximilian
Schulze-Hagen, Philipp Schad, Sandy Engelhardt, Bettina
1875
Baeßler, Sebastian Foersch, Johannes Stegmaier, Christiane
Kuhl, Sven Nebelung, Jakob Nikolas Kather, and Daniel
Truhn. Denoising diffusion probabilistic models for 3D med-
ical image generation. Scientific Reports , 13:7303, 2023. 1
[25] Alexandros Lattas, Stylianos Moschoglou, Stylianos
Ploumpis, Baris Gecer, Jiankang Deng, and Stefanos
Zafeiriou. Fitme: Deep photorealistic 3d morphable model
avatars. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pages
8629–8640, 2023. 1, 2
[26] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and
Javier Romero. Learning a model of facial shape and ex-
pression from 4D scans. ACM Transactions on Graphics,
(Proc. SIGGRAPH Asia) , 36(6):194:1–194:17, 2017. 2
[27] Isaak Lim, Alexander Dielen, Marcel Campen, and Leif
Kobbelt. A simple approach to intrinsic correspondence
learning on unstructured 3d meshes. In Computer Vision –
ECCV 2018 Workshops: Munich, Germany, September 8-14,
2018, Proceedings, Part III , page 349–362, Berlin, Heidel-
berg, 2019. Springer-Verlag. 1, 2
[28] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradi-
ent descent with warm restarts. In International Conference
on Learning Representations , 2017. 5
[29] Foivos Paraperas Papantoniou, Alexandros Lattas, Stylianos
Moschoglou, and Stefanos Zafeiriou. Relightify: Re-
lightable 3d faces from a single image via diffusion models.
InProceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV) , 2023. 1
[30] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami
Romdhani, and Thomas Vetter. A 3d face model for pose
and illumination invariant face recognition. In 2009 Sixth
IEEE International Conference on Advanced Video and Sig-
nal Based Surveillance , pages 296–301, 2009. 2
[31] Stylianos Ploumpis, Haoyang Wang, Nick Pears,
William AP Smith, and Stefanos Zafeiriou. Combining 3d
morphable models: A large scale face-and-head model. In
Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 10934–10943, 2019.
[32] Stylianos Ploumpis, Evangelos Ververas, Eimear O’Sullivan,
Stylianos Moschoglou, Haoyang Wang, Nick Pears,
William AP Smith, Baris Gecer, and Stefanos Zafeiriou. To-
wards a complete 3d morphable model of the human head.
IEEE transactions on pattern analysis and machine intelli-
gence , 43(11):4142–4160, 2020. 5
[33] Stylianos Ploumpis, Stylianos Moschoglou, Vasileios Tri-
antafyllou, and Stefanos Zafeiriou. 3d human tongue recon-
struction from single” in-the-wild” images. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 2771–2780, 2022. 2
[34] Rolandos Alexandros Potamias, Stylianos Ploumpis,
Stylianos Moschoglou, Vasileios Triantafyllou, and Stefanos
Zafeiriou. Handy: Towards a high fidelity 3d hand shape
and appearance model. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 4670–4680, 2023. 5
[35] Dafei Qin, Jun Saito, Noam Aigerman, Groueix Thibault,
and Taku Komura. Neural face rigging for animating andretargeting facial meshes in the wild. In SIGGRAPH 2023
Conference Papers , 2023. 2
[36] Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, and
Michael J. Black. Generating 3D faces using convolutional
mesh autoencoders. In European Conference on Computer
Vision (ECCV) , pages 725–741, 2018. 1, 2, 5, 6
[37] Michael Stengel, Koki Nagano, Chao Liu, Matthew Chan,
Alex Trevithick, Shalini De Mello, Jonghyun Kim, and
David Luebke. Ai-mediated 3d video conferencing. In ACM
SIGGRAPH Emerging Technologies , 2023. 1
[38] Michail Tarasiou, Erik Chavez, and Stefanos Zafeiriou. Vits
for sits: Vision transformers for satellite image time se-
ries. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 10418–
10428, 2023. 4
[39] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lu-
cas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung,
Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario
Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp ar-
chitecture for vision. In Advances in Neural Information
Processing Systems , pages 24261–24272. Curran Associates,
Inc., 2021. 3, 4, 5
[40] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herve Jegou. Training
data-efficient image transformers & distillation through at-
tention. In International Conference on Machine Learning ,
pages 10347–10357, 2021. 4
[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in Neural
Information Processing Systems 30 , pages 5998–6008. Cur-
ran Associates, Inc., 2017. 3, 5
[42] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin
Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang
Wen, Qifeng Chen, and Baining Guo. Rodin: A generative
model for sculpting 3d digital avatars using diffusion. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 4563–4573, 2023. 1
[43] Peizhi Yan, James Gregson, Qiang Tang, Rabab Ward, Zhan
Xu, and Shan Du. Neo-3df: Novel editing-oriented 3d face
creation and reconstruction. In Proceedings of the Asian
Conference on Computer Vision , pages 486–502, 2022. 3
[44] Haotian Zhang, Ye Yuan, Viktor Makoviychuk, Yunrong
Guo, Sanja Fidler, Xue Bin Peng, and Kayvon Fatahalian.
Learning physically simulated tennis skills from broadcast
videos. ACM Trans. Graph. 1
1876
