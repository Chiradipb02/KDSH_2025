Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving?
Zhiqi Li1,2*, Zhiding Yu2†, Shiyi Lan2, Jiahan Li1, Jan Kautz2, Tong Lu1, Jose M. Alvarez2
1National Key Lab for Novel Software Technology, Nanjing University2NVIDIA
Abstract
End-to-end autonomous driving recently emerged as a
promising research direction to target autonomy from a
full-stack perspective. Along this line, many of the latest
works follow an open-loop evaluation setting on nuScenes
to study the planning behavior. In this paper, we delve
deeper into the problem by conducting thorough analyses
and demystifying more devils in the details. We initially
observed that the nuScenes dataset, characterized by rela-
tively simple driving scenarios, leads to an under-utilization
of perception information in end-to-end models incorporat-
ing ego status, such as the ego vehicle’s velocity. These
models tend to rely predominantly on the ego vehicle’s sta-
tus for future path planning. Beyond the limitations of the
dataset, we also note that current metrics do not compre-
hensively assess the planning quality, leading to potentially
biased conclusions drawn from existing benchmarks. To
address this issue, we introduce a new metric to evalu-
ate whether the predicted trajectories adhere to the road.
We further propose a simple baseline able to achieve com-
petitive results without relying on perception annotations.
Given the current limitations on the benchmark and met-
rics, we suggest the community reassess relevant prevail-
ing research and be cautious about whether the contin-
ued pursuit of state-of-the-art would yield convincing and
universal conclusions. Code and models are available at
https://github.com/NVlabs/BEV-Planner .
1. Introduction
End-to-end autonomous driving aims to jointly consider
perception and planning in a full-stack manner [1, 5, 32,
35]. An underlying motivation is to evaluate autonomous
vehicle (A V) perception as a means to an end (planning),
instead of overfitting to certain perception metrics.
Unlike perception, the planning is generally much more
open-ended and hard to quantify [6, 7]. This open-ended
nature of planning would ideally favor a closed-loop eval-
*Work done during an internship at NVIDIA.
†Corresponding author: zhidingy@nvidia.com.
BEVNetPerceptionPlannerPrediction
EgoStatusEgoStatusSensorsTrajectories(a.1)PipelineofAD-MLP
(b)CommonlyUsedPipelineofEnd-to-EndAutonomousDrivingModel
EgoStatusTrajectoriesMLP
PastTrajectories
BEVNetSensors
TrajectoriesBEVFeaturesAttentionEgoQuery(c)PipelineofOurBEV-Planner
EgoStatusMLP
EgoStatus
EgoStatusTrajectoriesMLP(a.2)PipelineofEgo-MLPFigure 1. (a) AD-MLP uses both ego status and past trajectory GTs
as input. Our reproduced version (Ego-MLP) drops the past tra-
jectories. (b) The existing end-to-end autonomous driving pipeline
consists of perception, prediction, and planning modules. Ego sta-
tus can be integrated into the bird’s-eye view (BEV) generation
module or within the planning module. (c) We design a simple
baseline for comparison with existing methods. The simple base-
line does not leverage the perception or prediction module and di-
rectly predicts the final trajectories based on BEV features.
uation setting where other agents could react to the behav-
ior of the ego vehicle, and the raw sensor data could also
change accordingly. However, both agent behavior model-
ing and real-world data simulation within closed-loop sim-
ulators [8, 19] remain challenging open problems to date.
As such, closed-loop evaluation inevitably introduces con-
siderable domain gaps to the real world.
Open-loop evaluation, on the other hand, aims to treat
human driving as the ground truth and formulate planning
as imitation learning [13]. Such formulation allows the
readily usage of real-world datasets via simple log-replay,
avoiding the domain gaps from simulation. It also offers
other advantages, such as the capacity to train and vali-
date models in complex and diverse traffic scenarios, which
are often difficult to generate with high fidelity in simula-
tions [5]. For these benefits, a well-established body of re-
1
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
14864
search focuses on open-loop end-to-end autonomous driv-
ing with real-world dataset [2, 12, 13, 16, 43].
Current prevailing end-to-end autonomous driving meth-
ods [12, 13, 16, 43] commonly use nuScenes [2] for open-
loop evaluation of their planning behavior. For instance,
UniAD [13] studies the influence of different perception
task modules to the final planning behavior. However, AD-
MLP [45] recently points out that a simple MLP network
can also achieve state-of-the-art planning results, relying
solely on the ego status information. This motivates us to
ask an important question:
Is ego status all you need for open-loop end-to-end au-
tonomous driving?
Our answer is yes and no , considering both the pros and
cons of using ego status in current benchmarks:
Yes. Information such as velocity, acceleration and yaw
angle in the ego status should apparently benefit the plan-
ning task. To verify this, we fix an open issue1of AD-MLP
and remove the use of history trajectory ground truths (GTs)
to prevent potential label leakage. Our reproduced model,
Ego-MLP (Fig. 1 a.2), relies solely on the ego status and
is on par with state-of-the-art methods in terms of existing
L2 distance and collision rate metrics. Another observa-
tion is that only existing methods [13, 16, 43], which incor-
porate ego status information within the planner module,
can obtain results on par with Ego-MLP. Although these
methods employ additional perception information (track-
ing, HD map, etc.), they don’t demonstrate superiority com-
pared to Ego-MLP. These observations verify the dominat-
ing role of ego status in the open-loop evaluation of end-to-
end autonomous driving.
And No. It is also evident that autonomous driving as a
safety-critical application should not depend solely on ego
status for decision-making. So why does this phenomenon
occur where using only ego status can achieve state-of-the-
art planning results? To address the question, we present a
comprehensive set of analyses covering existing open-loop
end-to-end autonomous driving methods. We identify ma-
jor shortcomings within existing research, including aspects
related to datasets, evaluation metrics, and specific model
implementations. We itemize and detail these shortcomings
in the rest of the section:
Imblanced dataset. NuScenes is a commonly used bench-
mark for open-loop evaluation tasks[11–13, 16, 17, 43].
However, our analysis shows that 73.9% of the nuScenes
data involve scenarios of driving straightforwardly, as re-
flected by the distribution of the trajectory in Fig. 2. For
these straight-driving scenarios, maintaining the current ve-
locity, direction, or turning rate can be sufficient most of the
1https://github.com/E2E-AD/AD-MLP/issues/4 .
(a) Trajectory Heatmap(b) Typical Scene of nuScenes
Figure 2. (a) The ego car trajectory heatmap on nuScenes dataset.
(b) The majority of the scenes within the nuScenes dataset consist
of straightforward driving situations.
time. Hence, ego status information can be easily leveraged
as a shortcut to fit the planning task, leading to the strong
performance of Ego-MLP on nuScenes.
Existing metrics are not comprehensive. The remain-
ing 26.1% of nuScenes data involve more challenging driv-
ing scenarios for potentially better benchmarks of planning
behaviors. However, we argue that the widely used cur-
rent metrics, such as the L2 distance between prediction
and planning GT and the collision rates between ego ve-
hicle and surrounding obstacles, fail to accurately measure
the model’s planning behavior quality. Through visualiz-
ing numerous predicted trajectories generated from various
methods, we note that some highly risky trajectories, such
as running off the road may not get severely penalized in
existing metrics. In response to this issue, we introduce a
new metric to calculate the interaction rate between the pre-
dicted trajectories and the road boundaries. While focusing
on intersection rates with road boundaries, the benchmark
will experience a substantial transformation. In terms of
this new metric, Ego-MLP tends to predict trajectories that
deviate from the road more frequently than UniAD.
Ego status bias against driving logic. With ego status be-
ing a potential source causing overfitting, we further ob-
serve an interesting phenomenon. Our experiment results
suggest that, in some cases, completely removing visual in-
put from an existing end-to-end autonomous driving frame-
work does not significantly degrade the planning behav-
ior. This contradicts the basic driving logic in the sense
that perception is expected to provide useful information
for planning. For instance, blanking all the camera input
in V AD [16] leads to complete failure of the perception
module but minor degrade in planning, when ego status is
present. However, altering the input ego velocity can sig-
nificantly influence the final predicted trajectory.
In conclusion, we conjecture that recent efforts in end-to-
end autonomous driving and their state-of-the-art scores on
nuScenes are likely to be caused by the over-reliance on ego
status, coupled with the dominance of simple driving sce-
narios. Furthermore, current metrics fall short in compre-
hensively assessing the quality of model-predicted trajecto-
2
14865
ries. These open issues and shortcomings may have under-
presented the potential complexity of the planning task and
created a misleading impression that ego status is all you
need for open-loop end-to-end autonomous driving.
The potential interference of ego status in current open-
loop end-to-end autonomous driving research raises another
question: Is it possible to negate this influence by removing
ego status from the whole model? However, it’s important
to note that even excluding the impact of ego status, the
reliability of open-loop autonomous driving research based
on the nuScenes dataset remains in question.
2. Related Work
2.1. BEV perception
In recent years, BEV-based autonomous driving perception
methods have made great progress. Lift-Splat-Shoot [31]
firstly propose to use latent depth distribution to per-
form view transformation. BEVFormer [22] introduces
temporal clues into BEV perception and greatly boosts
the 3D detection performance. A series of subsequent
works [14, 15, 21, 23, 24, 26–28, 30, 41, 42] obtain more
accurate 3D perception results by obtaining more accurate
depth information or making better use of temporal infor-
mation. The incorporation of temporal information typi-
cally necessitates the alignment of features across different
timesteps [14, 18, 22, 39]. In the alignment process, the
ego status is either implicitly encoded within the input fea-
ture [39] or is explicitly used to translate BEV features [14].
Methods [4, 20, 25, 29, 38, 44] explored map perception
based on BEV features.
2.2. End-to-end autonomous driving
Modern autonomous driving systems are usually divided
into three main tasks: perception, prediction, and plan-
ning. End-to-end autonomous driving that directs learn-
ing from raw sensor data to planning trajectories or driv-
ing commands eliminates the need for manual feature ex-
traction, leading to efficient data utilization and adaptabil-
ity to diverse driving scenarios. There exists a body of re-
search [34, 37, 40] focused on closed-loop end-to-end driv-
ing within simulators [8, 19]. However, a domain gap per-
sists between the simulator environment and the real world,
particularly concerning sensor data and the motion status of
agents. Recently, open-loop end-to-end autonomous driv-
ing has attracted more attention. End-to-end autonomous
driving methods [3, 9, 13, 16, 33, 43] that involve learning
intermediate tasks claim their effectiveness in improving fi-
nal planning performance. AD-MLP [45] pointed out the
issue of imbalanced data distribution in nuScenes and at-
tempted to use only ego status as the model input to achieve
arts performance. However, AD-MLP benefits from utiliz-
ing the historical trajectory of the ego car as input. Giventhat none of the existing methods use the historical trajec-
tory information of the ego car, we argue that using the
historical trajectory in open-loop autonomous driving is a
subject of debate, as the model itself does not generate this
historical trajectory but rather by an actual human driver.
3. Proposed BEV-Planner
In fact, ST-P3 [12], a previous method that often serves as
a baseline, uses partially incorrect GT data during training
and evaluation2. Consequently, when conducting compar-
isons between other methods and ST-P3, the validity of the
conclusions drawn must be carefully evaluated. Therefore,
in this paper, it is necessary for us to redesign a baseline
method to compare with existing methods. At the same
time, to better explore the impact of ego status, we also need
a relatively clear baseline method. Based on these consid-
erations, we have designed a very simple baseline in this
paper, named BEV-Planner , as shown in the Fig. 1(c). For
our pipeline, we first generate the BEV feature and concate-
nate it with history BEV features, mainly following the pre-
vious method [12, 14, 21]. Please note while concatenating
BEV features from different timesteps, we didn’t perform
feature alignment. After obtaining the BEV features, we di-
rectly perform a cross-attention [36] between the BEV fea-
tures and the ego query, which is a learnable embedding.
The final trajectories are predicted based on the refined ego
query through MLPs. The process can be formulated as fol-
lows:
τ=MLP (attn (q=Q, k =B, v =B)), (1)
where Qis the ego query, Bis the BEV features after tem-
poral fusion. τis the final predicted trajectories.
To align with existing methods, we also designed base-
line approaches that incorporate ego status into the BEV or
planner modules. The strategy of incorporating ego status
into the BEV aligns with previous approaches [13, 16, 22].
The strategy of incorporating ego status in the planner is di-
rectly concatenating the ego query with a vector containing
ego status.
Compared to existing methods, this simple method
didn’t require any human-labeled data, including bounding
boxes, tracking IDs, HD maps, etc. For this proposed base-
lines, we only use one L1 loss for trajectory supervision. We
wish to underscore that our proposed baseline method is not
intended for real-world deployment, owing to its deficien-
cies in providing adequate constraints and interoperability.
4. Experiments
4.1. Implementation Details
Our baseline uses an R50 backbone [10]. The input reso-
lution is 256×704, smaller than existing methods [13, 16].
2https://github.com/OpenDriveLab/ST-P3/issues/24
3
14866
The BEV resolution is 128×128with a perception range of
around 50 meters. For the baseline that uses history BEV
features, we directly concatenate the BEV features from the
past 4 timesteps to current BEV features along the chan-
nel dimension without alignment. A BEV encoder from
method [14] is further used to squeeze the channel dimen-
sion to 256. We train our model for 12 epochs on 8 V100
GPUs, with a batch size of 32 and a learning rate of 1e-4.
4.2. Metrics
In the Appendix, we will introduce the shortcomings of the
currently commonly used collision rate and another metric
used to evaluate the smoothness of predicted trajectory.
Curb Collision Rate (CCR). In this study, to more com-
prehensively assess the quality of predicted trajectories, we
employed a new metric that calculates the collision rate
between the predicted trajectories and curbs (road bound-
aries). Staying on the road is vital for the safety of au-
tonomous driving systems, yet existing evaluation metrics
overlook the integration of map priors. Intuitively, safe tra-
jectories should avoid collision with curbs. Our Curb Colli-
sion Rate (CCR) typically indicate the possibility of leaving
the drivable area, which can pose safety hazards. We rec-
ognize that certain annotated road boundaries on nuScenes
are indeed traversable, and ground truth trajectories may
intersect with these boundaries under specific conditions.
However, from a statistical viewpoint, this metric can effec-
tively represent the overall rationality of the model’s pre-
dicted trajectories. The implementation of the CCR metric
is informed by the collision rate. To facilitate this, we ras-
terize the road boundary using a resolution of 0.1 meters.
More details are in the Appendix.
Union Implementation. Given the lack of a standardized
approach to assessment metrics, methodologies might dif-
fer in the nuances of their metric executions. In this study,
we utilized the official open-source repositories from vari-
ous methodologies to produce predicted trajectories. Sub-
sequently, we employed a consistent metric implementation
across all methods for evaluation, guaranteeing equity. Ad-
dressing concerns raised by AD-MLP [45] regarding the po-
tential for false collisions due to a coarse-grained grid size
(0.5m), we adopt a finer default grid size of 0.1m in our
work to mitigate this issue.
4.3. Discussion
Ego status plays a key role. While focusing solely on
previous metrics L2 distance andcollision rate , it is ob-
served that the simple strategy (ID-7), which simply con-
tinues straight at the current velocity, achieves surprisingly
good results. The Ego-MLP model, which didn’t leverage
perception clues, is actually on par with UniAD and V AD,which use more complex pipelines. From another perspec-
tive, it is observed that existing methods can only match the
performance of Ego-MLP when the ego vehicle’s status is
incorporated into the planner. In contrast, reliance solely on
camera inputs leads to results significantly inferior to those
achieved by Ego-MLP. Considering these observations, we
may tentatively infer an intriguing conclusion: utilizing a
combination of sensory information and ego status appears
to yield results comparable to those achieved by employ-
ing ego status alone. Therefore, in models that integrate
both ego vehicle status and perception information, a perti-
nent question arises: What specific role does the perception
information, acquired from camera inputs, play within the
final planning module?
Ego Status vs. Perceptual Information Undoubtedly,
perceptual information constitutes the indispensable foun-
dation of all autonomous driving systems, with ego status
additionally offering crucial data such as the vehicle’s ve-
locity and acceleration to aid the system’s decision-making
process. Incorporating both perceptual information and ego
status for the ultimate planning should indeed be a judicious
strategy within an end-to-end autonomous driving system.
However, as shown in Tab. 1, relying solely on ego status
can yield planning results that are on par with or even supe-
rior to those methods that utilize both ego status and percep-
tion modules on previous L2 or collation rate metrics. To
ascertain the roles that perceptual information and ego sta-
tus play in the final planning process, we introduced varying
degrees of perturbation to the images and the ego status, as
shown in the Tab. 2. We use the official V AD model (which
leverages ego status in the planner module) as the base
model. It is observable that when disturbances are added to
the images, the results of planning marginally decrease and
may even exhibit improvement, while perceptual perfor-
mance significantly deteriorates. Surprisingly, even when
blank images are used as input, leading to the complete
breakdown of the perception module, the model’s planning
capabilities remain largely unaffected. The corresponding
visualization results are as illustrated in the Fig. 3. In con-
trast to the model’s remarkable robustness to variations in
image inputs, it exhibits considerable sensitivity to ego sta-
tus. Upon altering the velocity of the ego car, we can ob-
serve that the planning results of the model are significantly
impacted, as shown in Fig. 4. Setting the ego car’s speed to
100 m/s results in the model generating wildly impractical
planning trajectories. We posit that an autonomous driving
system displaying such heightened sensitivity to ego status
information harbors considerable safety risks. Furthermore,
with planning results being predominantly dictated by ego
status, the functions of other modules in the model can-
not be reflected. For example, while comparing V AD(ID-
6) and BEV-Planner++ (ID-12), we can observe that they
4
14867
ID MethodEgo Status L2 (m) ↓ Collision (%) ↓ CCR (%) ↓ckpt. sourcein BEV in Planer 1s 2s 3s Avg. 1s 2s 3s Avg. 1s 2s 3s Avg.
0 ST-P3 ✗ ✗ 1.59†2.64†3.73†2.65†0.69†3.62†8.39†4.23†2.53†8.17†14.4†8.37†Official
1 UniAD ✗ ✗ 0.59 1.01 1.48 1.03 0.16 0.51 1.64 0.77 0.35 1.46 3.99 1.93 Reproduce
2 UniAD ✓ ✗ 0.35 0.63 0.99 0.66 0.16 0.43 1.27 0.62 0.21 1.32 3.63 1.72 Official
3 UniAD ✓ ✓ 0.20 0.42 0.75 0.46 0.02 0.25 0.84 0.37 0.20 1.33 3.24 1.59 Reproduce
4 V AD-Base ✗ ✗ 0.69 1.22 1.83 1.25 0.06 0.68 2.52 1.09 1.02 3.44 7.00 3.82 Reproduce
5 V AD-Base ✓ ✗ 0.41 0.70 1.06 0.72 0.04 0.43 1.15 0.54 0.60 2.38 5.18 2.72 Official
6 V AD-Base ✓ ✓ 0.17 0.34 0.60 0.37 0.04 0.27 0.67 0.33 0.21 2.13 5.06 2.47 Official
7 GoStright - ✓ 0.38 0.79 1.33 0.83 0.15 0.60 2.50 1.08 2.07 8.09 15.7 8.62 -
8 Ego-MLP - ✓ 0.15 0.32 0.59 0.35 0.00 0.27 0.85 0.37 0.27 2.52 6.60 2.93
9 BEV-Planner* ✗ ✗ 0.27 0.54 0.90 0.57 0.04 0.35 1.80 0.73 0.63 3.38 7.93 3.98 -
10 BEV-Planner ✗ ✗ 0.30 0.52 0.83 0.55 0.10 0.37 1.30 0.59 0.78 3.79 8.22 4.26 -
11 BEV-Planner+ ✓ ✗ 0.28 0.42 0.68 0.46 0.04 0.37 1.07 0.49 0.70 3.77 8.15 4.21 -
12 BEV-Planner++ ✓ ✓ 0.16 0.32 0.57 0.35 0.00 0.29 0.73 0.34 0.35 2.62 6.51 3.16 -
Table 1. Open-loop planning performance. †: The official implementation of ST-P3 (ID-0) utilized partial erroneous ground truth
trajectories, with details provided in the appendix. The official UniAD (ID-2) utilized ego status in its BEV module. It is of particular note
that the performance of the officially open-sourced model exceeds the results reported in the original paper [13]. We implemented minor
modifications to the official codebases of UniAD and V AD to investigate the variations in results arising from different applications of ego
status (ID-1, 3 & 4). A naive strategy (ID-7) of proceeding at the current speed also yields satisfactory results. Without the perception
module, Ego-MLP (ID-8), utilizing solely ego velocity, acceleration, yaw angle, and driving command, achieves performance on par with
current state-of-the-art models on previous L2 distance and collision rate metrics. *: Our simple baseline (ID-9) didn’t utilize historical
temporal information. The baseline (ID-10) utilizes the temporal clues from the past 4 frames. To ensure the comprehensiveness of our
experiment, we also conduct investigations into the influence of ego status on our baseline model (ID-11& 12).
MethodImg Ego Status L2 (m) ↓ Collision (%) ↓ CCR (%) ↓ Det. Map
Corruption Noise 1s 2s 3s Avg. 1s 2s 3s Avg. 1s 2s 3s Avg. (NDS) (mAP)
V AD-Base* - - 0.41 0.70 1.06 0.72 0.04 0.43 1.15 0.54 0.60 2.38 5.18 2.72 46.0 47.5
V AD-Base - - 0.17 0.34 0.60 0.37 0.04 0.27 0.67 0.33 0.21 2.13 5.06 2.47 45.5 47.0
V AD-Base Snow - 0.19 0.41 0.76 0.45 0.00 0.20 0.76 0.32 0.21 2.27 5.98 2.82 36.1 29.4
V AD-Base Fog - 0.19 0.40 0.75 0.45 0.02 0.20 0.68 0.30 0.23 2.21 5.90 2.78 34.3 29.4
V AD-Base Glare - 0.19 0.40 0.74 0.44 0.02 0.18 0.59 0.26 0.21 2.11 5.57 2.63 41.7 38.3
V AD-Base Rain - 0.19 0.41 0.75 0.45 0.02 0.18 0.66 0.29 0.31 2.38 5.98 2.89 29.1 13.0
V AD-Base Blank - 0.19 0.41 0.77 0.46 0.00 0.40 1.21 0.54 0.35 3.05 7.73 3.71 0.0 0.0
V AD-Base - v×0.0 3.81 6.19 8.48 6.16 1.00 6.76 16.18 7.98 0.19 0.41 3.10 1.23 45.5 47.0
V AD-Base - v×0.5 1.95 3.20 4.41 3.19 0.02 1.00 4.10 1.71 0.37 2.56 5.57 2,83 45.5 47.0
V AD-Base - v×1.5 1.94 3.20 4.47 3.20 0.14 2.89 6.21 3.08 1.54 6.29 13.2 7.01 45.5 47.0
V AD-Base - v=100 m/s 113 206 306 208 8.00†9.66†10.49†9.38†24.8†27.5†28.8†27.0†45.5 47.0
Table 2. The V AD-base model’s robustness to images and ego status. To ascertain the impact of perceptual information and ego status on
the ultimate planning performance, we systematically introduced noise into each component separately. We utilize the official V AD-Base
checkpoint that uses ego status in its planner module. *: the results of V AD-Base without ego status in its planner. We can observe that
introducing corruption to images markedly affects the perception results, especially in the case of using blank images; nonetheless, this does
not markedly disrupt the ultimate planning results. In contrast to the minor impact of image corruption on planning, modifications to the ego
vehicle’s velocity have a significant effect on the planning results. Experimental results reveal that in an end-to-end model incorporating
both ego status and perceptual information, decision-making is disproportionately influenced by ego status, thereby substantially increasing
the model’s safety risks. †: The collision rate is not precise as the ego car may have departed from the local BEV area. When the input
velocity is zero, the model produces almost stationary trajectories, resulting in excellent performance in the CCR. This can be seen as a
limitation of the CCR metric.
MethodEgo Status L2 (m) ↓ Collision (%) ↓ CCR (%) ↓
in BEV in Planer 1s 2s 3s Avg. 1s 2s 3s Avg. 1s 2s 3s Avg.
BEV-Planner ✗ ✗ 0.30 0.52 0.83 0.55 0.10 0.37 1.30 0.59 0.78 3.79 8.22 4.26
BEV-Planner (init*) ✗ ✗ 0.26 0.49 0.81 0.52 0.03 0.20 1.00 0.42 0.59 3.18 7.36 3.71
BEV-Planner+Map ✗ ✗ 0.53 0.94 1.40 0.96 0.12 0.37 2.19 0.89 0.68 2.38 4.73 2.60
Table 3. While adding map perception task into the BEV-Planner method, we can observe that the model obtains worse L2 distance and
collision rate performance, but achieves better CCR. We use a pretrained map perception checkpoint as the initialization of the BEV-Planner
(init*) and BEV-Planner+Map model.
5
14868
(a) VAD-Base (Original)(b) VAD-Base (Blank)(c) VAD-Base (Snow)
(d) VAD-Base (Fog)(e) VAD-Base (Glare)(f) VAD-Base (Rain)   
0s20s
10sFigure 3. We exhibit the predicted trajectories of the V AD model (incorporating ego status in its planner) under various image corruptions.
All trajectories within a given scene (spanning 20 seconds) are presented in the global coordinate system. Each triangular marker signifies
a ground truth trajectory point of the ego vehicle, with different colors representing distinct timesteps. Notably, the model’s predicted
trajectory maintains plausibility, even when blank images serve as input. The trajectories within the red boxes, however, are suboptimal, as
further elucidated in ??. While corruptions were applied to all surround-view images, for the sake of visualization, only the corresponding
front-view images at the initial timestep are displayed.
MethodEgo Status L2 (m) ↓ L2-ST (m) ↓ L2-LR (m) ↓
in BEV in Planer 1s 2s 3s Avg. 1s 2s 3s Avg. 1s 2s 3s Avg.
BEV-Planner ✗ ✗ 0.30 0.52 0.83 0.55 0.27 0.47 0.78 0.48 0.43 0.78 1.23 0.81
BEV-Planner+Map ✗ ✗ 0.53 0.94 1.40 0.96 0.54 0.95 1.42 0.97 0.52 0.87 1.28 0.89
Table 4. L2-ST is the L2 distance with going straight driving commands. L2-LR is the L2 distance with turning left/right commands.
MethodEgo Status Collision (m) ↓ Collision-ST (m) ↓ Collision-LR (m) ↓
in BEV in Planer 1s 2s 3s Avg. 1s 2s 3s Avg. 1s 2s 3s Avg.
BEV-Planner ✗ ✗ 0.10 0.37 1.30 0.59 0.07 0.20 0.92 0.40 0.15 1.76 4.84 2.25
BEV-Planner+Map ✗ ✗ 0.12 0.37 2.19 0.89 0.14 0.38 2.20 0.91 0.00 0.29 2.05 0.78
Table 5. Collision-ST is the collision rate with going straight driving commands. Collision-LR is the collision rate with turning left/right
commands.
obtain basically similar results in terms of L2 and collision
rate. Is it justifiable to assert that our BEV-Planner++ de-
sign, characterized by its simplicity and effectiveness, can
attain comparable outcomes to other more intricate method-
ologies, even in the absence of utilizing perception data? In
fact, as the performance of the final planning module is pre-
dominantly influenced by the ego vehicle status, the design
of other components does not significantly affect the plan-
ning results. Consequently, we argue that methods utiliz-
ing ego status are not directly comparable and conclusions
should not be drawn from such comparisons.
How about not using ego status? Given that the ego ve-
hicle status exerts a dominant influence on the planning re-
sults, it prompts an important inquiry: Is it feasible and
beneficial to exclude ego status in open-loop end-to-end re-
search?
Neglected Ego Status in Perception Stage. In fact, exist-
ing methods [16, 43] ignore the impact of using ego statuson planning in BEV Encoder. More details are in the Ap-
pendix.
Without Ego Status, the Simpler, the Better? People
might wonder why our BEV-Planner, without using addi-
tional perception tasks (including Depth, HD map, Track-
ing,etc.) and ego status, achieves better results in L2 dis-
tance and collision rate than other methods (ID-1 and 4).
Since our BEV-Planner performs poorly in terms of CCR,
what would happen if we added map perception tasks to
our baseline? To address these questions, we designed a
“BEV-Planner+Map” model by introducing a map percep-
tion task into our pipeline, mainly following the designs of
UniAD. As shown in Tab. 3, when map perception is in-
troduced, the model exhibits poorer results in terms of L2
distance and collision rate metrics. The only aspect that
aligns with our expectations is that the introduction of map
perception significantly reduces the CCR. Through a com-
parison of BEV-Planner with BEV-Planner (init*), we ob-
serve that the use of map-pretrained weights can enhance
performance. This finding implies that the decrease in L2
6
14869
Original velocity
Velocity = 0m/s
0.5x Original Velocity 
1.5x Original Velocity 
Velocity = 100m/s Out of BEVFigure 4. For the V AD-based model that incorporates ego status
in its planner, we introduce the noise to the ego velocity with the
visual inputs remaining constant. Notably, when the velocity data
of the ego vehicle is perturbed, the resulting trajectories exhibit
substantial alterations. Setting the vehicle’s speed to zero results
in a stationary prediction, while a speed of 100 m/s leads to the
projection of an implausible trajectory. This indicates a dispropor-
tionate dependence of the model’s decision-making process on the
ego status, even though the perception module continues to pro-
vide accurate surrounding information.
879 1758 2637 3516 4395 5274 6153 7032 7911 8790 9669 10548
Iterations0.51.01.52.02.5Avg. L2 LossL2 Loss
Colition Rate
BEV-Planer
BEV-Planner+
BEV-Planner++
0.00.20.40.60.81.01.2
Avg. Collision Rate (%)
Figure 5. Introducing ego status in the BEV-Planner++ enables the
model to converge very rapidly.
and Collision rate observed with the integration of Map-
Former in “BEV-Planner+Map” is not due to the pretrained
weights. We posit that in most straight-driving scenarios,
the addition of lane information may not yield markedly ef-
fective information and could indeed introduce some degree
of interference. To verify our hypothesis, we evaluated the
performance of these methods under varying driving com-
mands. As shown in Tab. 4 and Tab. 5, adding map in-
SampleASampleCSampleDSampleB
BEV-Planner++BEV-PlannerScene
Figure 6. Comparing the BEV features of our baselines with the
corresponding scenes.
Method P.P. Avg. L2(m) Avg. Colli.(%) Avg. CCR(%)
UniAD ✓ 0.77 0.51 7.83
UniAD ✗ 0.66 0.62 1.72
Table 6. P.P. indicates the post-processing optimization module of
UniAD. We use the officially released weights of UniAD for this
ablation study. By default, UniAD incorporates a post-processing
step to refine the predicted trajectory from the end-to-end model,
aiming to mitigate collisions with other agents. Nonetheless, this
approach is limited by this singular optimization objective, which
overlooks additional safety-critical factors in autonomous driving,
such as lane adherence. Our new metrics, the CCR reveal that
UniAD’s post-processing substantially increases the risk of the ego
vehicle running off the road.
formation significantly increases the L2 distance error and
collision rate with going straight commands. In contrast,
for turning scenarios, the incorporation of map information
effectively reduces the collision rate. Based on the above
observations, we can tentatively draw the following conclu-
sions:
• In simple straightforward driving scenarios, the addition
of perceptual information does not appear to enhance the
model’s performance with respect to L2 distance and col-
lision rate. Conversely, the implementation of more intri-
cate multi-task learning paradigms may, in fact, lead to a
decrease in the model’s overall efficacy.
• In more complex scenarios, such as turns, incorporat-
ing perceptual information can be beneficial for planning
purposes. However, given the relatively small propor-
tion (13%) of turning scenes in the existing evaluation
datasets, the introduction of perceptual information tends
to adversely affect the average performance metrics (L2
distance and collision rate) in the final analysis.
• It is imperative to develop a more robust and represen-
tative evaluation dataset. The metrics derived from the
current evaluation dataset are not entirely persuasive and
fail to accurately reflect the true capabilities of the model.
7
14870
New metrics will bring new conclusions. The preced-
ing methodology primarily centered around the L2 distance
and collision rate metrics. Our discussion thus far has been
largely concentrated on these two metrics. What we want
to emphasize is that these two metrics, L2 distance and col-
lision rate, only reflect a partial aspect of a model’s plan-
ning capabilities. It is not advisable to assess the quality
of a model based exclusively on these two metrics. In this
paper, we introduce a new metric to evaluate the model’s
comprehension and adherence to the map: Curb Collision
Rate (CCR). As shown in Tab. 1, we can observe that the
GoStright strategy frequently intersects with road bound-
aries, which is in line with our expectations. In terms of
this new metric, Ego-MLP performs worse than UniAD and
V AD. Our method BEV-Planner, performs the worst on this
metric because it does not use any map information. This
suggests that relying on past metrics to judge the superiority
of different open-loop methods is biased.
Based on our proposed new metrics, we also found that
the existing collision rate metric can be manipulated with
post-processing. More specifically, within UniAD [13], a
non-linear optimization module is employed to refine the
trajectory predicted by the end-to-end model, ensuring that
the anticipated path steers clear of the occupancy grid,
thereby aiming to prevent collisions. However, this op-
timization, while significantly reducing the collision rates
with other agents, inadvertently introduces additional safety
risks. The absence of adequate constraints in its optimiza-
tion process, such as the integration of map priors, markedly
increases the risk of the optimized trajectory encroaching
upon road boundaries, as shown in Tab. 6. In this paper, we
report the results of UniAD without its post-processing by
default.
What we wish to underscore is that the primary inten-
tion behind proposing CCR is to illuminate the inadequa-
cies within the existing evaluation systems. However, even
with the incorporation of CCR, open-loop evaluation sys-
tems still encounter numerous challenges. We assert that the
evaluation of open-loop autonomous driving systems neces-
sitates a more diverse and stringent evaluation framework.
This would enable a more accurate reflection of these sys-
tems’ capabilities and limitations.
What the baseline learned in its BEV? As shown
in Fig. 5, with the influence of ego status, the models con-
verge rapidly. Considering the challenge of generating valu-
able BEV features from visual images and comparing the
convergence curve of BEV-Planer that does not use ego sta-
tus, this further demonstrates that ego status information
dominates the learning process. Since our baselines are
solely supervised by ego trajectory, we are wondering what
the model is learning from the images. As shown in Fig. 6,
we observed a distinct phenomenon: in BEV-Planner++, theactivation range of the feature map predominantly encom-
passes the immediate vicinity around the ego vehicle, fre-
quently manifesting behind the vehicle itself. This pattern
marks a significant deviation from the BEV-Planner’s BEV
features, which typically concentrate on the area ahead of
the vehicle. We speculate that this is due to the introduc-
tion of ego status information, which negates the model’s
need to extract information from BEV features. Hence, the
BEV-Planner++ method has almost not learned any effec-
tive information.
5. Conclusion
In this paper, we present an in-depth analysis of the short-
comings inherent in current open-loop, end-to-end au-
tonomous driving methods. Our objective is to contribute
findings that will foster the progressive development of end-
to-end autonomous driving.
Our conclusions are summarized as follows:
• The planning performance of existing open-loop au-
tonomous driving models based on nuScenes is highly af-
fected by ego status (velocity, acceleration, yaw angle).
With ego status involved, the model’s final predicted tra-
jectories are basically dominated by it, resulting in a di-
minished use of sensory information.
• Existing planning metrics fall short of fully capturing the
true performance of models. The evaluation results of the
model may vary significantly across different metrics. We
advocate for the adoption of more diverse and compre-
hensive metrics to prevent models from achieving local
optimality on specific metrics, which may lead to the ne-
glect of other safety hazards.
• Compared to pushing the state-of-the-art performance on
the existing nuScenes dataset, we assert that the develop-
ment of more appropriate datasets and metrics represents
a more critical and urgent challenge to tackle.
Limitation There are trade-offs between different plan-
ning metrics. Designing an integrated evaluation system for
open-loop evaluation presents a significant challenge. Al-
though our baseline method excels in terms of L2 distance
and collision rate, its performance is not exceptional in the
CCR, primarily because our approach does not utilize any
perception annotations, such as HD maps.
Acknowledgments
Thanks to Bencheng Liao and Shaoyu Chen for providing
helpful information and model weights of V AD. Tong Lu
and Zhiqi Li are supported by the National Natural Science
Foundation of China (Grant No. 62372223) and China Mo-
bile Zijin Innovation Insititute (No. NR2310J7M). Zhiqi Li
is also supported by the NVIDIA Graduate Fellowship Pro-
gram.
8
14871
References
[1] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski,
Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D
Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al.
End to end learning for self-driving cars. arXiv preprint
arXiv:1604.07316 , 2016. 1
[2] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-
ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-
modal dataset for autonomous driving. In CVPR , 2020. 2
[3] Sergio Casas, Abbas Sadat, and Raquel Urtasun. Mp3: A
unified model to map, perceive, predict and plan. In CVPR ,
2021. 3
[4] Li Chen, Chonghao Sima, Yang Li, Zehan Zheng, Jiajie Xu,
Xiangwei Geng, Hongyang Li, Conghui He, Jianping Shi,
Yu Qiao, et al. Persformer: 3d lane detection via perspective
transformer and the openlane benchmark. In European Con-
ference on Computer Vision , pages 550–567. Springer, 2022.
3
[5] Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger,
Andreas Geiger, and Hongyang Li. End-to-end au-
tonomous driving: Challenges and frontiers. arXiv preprint
arXiv:2306.16927 , 2023. 1
[6] Laurene Claussmann, Marc Revilloud, Dominique Gruyer,
and S ´ebastien Glaser. A review of motion planning for high-
way autonomous driving. IEEE Transactions on Intelligent
Transportation Systems , 21(5):1826–1848, 2019. 1
[7] Dmitri Dolgov, Sebastian Thrun, Michael Montemerlo, and
James Diebel. Practical search techniques in path planning
for autonomous driving. Ann Arbor , 1001(48105):18–80,
2008. 1
[8] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio
Lopez, and Vladlen Koltun. Carla: An open urban driving
simulator. 2017. 1, 3
[9] Junru Gu, Chenxu Hu, Tianyuan Zhang, Xuanyao Chen,
Yilun Wang, Yue Wang, and Hang Zhao. Vip3d: End-to-end
visual trajectory prediction via 3d agent queries. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 5496–5506, 2023. 3
[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
2016. 3
[11] Peiyun Hu, Aaron Huang, John Dolan, David Held, and
Deva Ramanan. Safe local motion planning with self-
supervised freespace forecasting. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 12732–12741, 2021. 2
[12] Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi
Yan, and Dacheng Tao. St-p3: End-to-end vision-based au-
tonomous driving via spatial-temporal feature learning. In
European Conference on Computer Vision , pages 533–549.
Springer, 2022. 2, 3
[13] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,
Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai
Wang, et al. Planning-oriented autonomous driving. In Pro-
ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition , pages 17853–17862, 2023. 1, 2, 3,
5, 8
[14] Junjie Huang and Guan Huang. BEVDet4D: Exploit tempo-
ral cues in multi-camera 3d object detection. arXiv preprint
arXiv:2203.17054 , 2022. 3, 4
[15] Linyan Huang, Zhiqi Li, Chonghao Sima, Wenhai Wang,
Jingdong Wang, Yu Qiao, and Hongyang Li. Leveraging
vision-centric multi-modal expertise for 3d object detection.
arXiv preprint arXiv:2310.15670 , 2023. 3
[16] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jia-
jie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang
Huang, and Xinggang Wang. Vad: Vectorized scene rep-
resentation for efficient autonomous driving. arXiv preprint
arXiv:2303.12077 , 2023. 2, 3, 6
[17] Tarasha Khurana, Peiyun Hu, Achal Dave, Jason Ziglar,
David Held, and Deva Ramanan. Differentiable raycasting
for self-supervised occupancy forecasting. In European Con-
ference on Computer Vision , pages 353–369. Springer, 2022.
2
[18] Hongyang Li, Chonghao Sima, Jifeng Dai, Wenhai Wang,
Lewei Lu, Huijie Wang, Enze Xie, Zhiqi Li, Hanming Deng,
Hao Tian, et al. Delving into the devils of bird’s-eye-view
perception: A review, evaluation and recipe. arXiv preprint
arXiv:2209.05324 , 2022. 3
[19] Quanyi Li, Zhenghao Peng, Lan Feng, Qihang Zhang,
Zhenghai Xue, and Bolei Zhou. Metadrive: Composing
diverse driving scenarios for generalizable reinforcement
learning. IEEE transactions on pattern analysis and machine
intelligence , 45(3):3461–3475, 2022. 1, 3
[20] Qi Li, Yue Wang, Yilun Wang, and Hang Zhao. Hdmapnet:
An online hd map construction and evaluation framework.
2022. 3
[21] Yinhao Li, Zheng Ge, Guanyi Yu, Jinrong Yang, Zengran
Wang, Yukang Shi, Jianjian Sun, and Zeming Li. BEVDepth:
Acquisition of reliable depth for multi-view 3d object detec-
tion. arXiv preprint arXiv:2206.10092 , 2022. 3
[22] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-
hao Sima, Tong Lu, Qiao Yu, and Jifeng Dai. BEVFormer:
Learning bird’s-eye-view representation from multi-camera
images via spatiotemporal transformers. arXiv preprint
arXiv:2203.17270 , 2022. 3
[23] Zhiqi Li, Zhiding Yu, David Austin, Mingsheng Fang, Shiyi
Lan, Jan Kautz, and Jose M Alvarez. Fb-occ: 3d occupancy
prediction based on forward-backward view transformation.
arXiv preprint arXiv:2307.01492 , 2023. 3
[24] Zhiqi Li, Zhiding Yu, Wenhai Wang, Anima Anandkumar,
Tong Lu, and Jose M Alvarez. Fb-bev: Bev representation
from forward-backward view transformations. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 6919–6928, 2023. 3
[25] Bencheng Liao, Shaoyu Chen, Xinggang Wang, Tianheng
Cheng, Qian Zhang, Wenyu Liu, and Chang Huang. Maptr:
Structured modeling and learning for online vectorized hd
map construction. arXiv preprint arXiv:2208.14437 , 2022.
3
[26] Xuewu Lin, Tianwei Lin, Zixiang Pei, Lichao Huang, and
Zhizhong Su. Sparse4d v2: Recurrent temporal fusion with
sparse model. arXiv preprint arXiv:2305.14018 , 2023. 3
9
14872
[27] Haisong Liu, Yao Teng, Tao Lu, Haiguang Wang, and Limin
Wang. Sparsebev: High-performance sparse 3d object de-
tection from multi-camera videos. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 18580–18590, 2023.
[28] Yingfei Liu, Junjie Yan, Fan Jia, Shuailin Li, Qi Gao, Tian-
cai Wang, Xiangyu Zhang, and Jian Sun. PETRv2: A uni-
fied framework for 3d perception from multi-camera images.
arXiv preprint arXiv:2206.01256 , 2022. 3
[29] Yicheng Liu, Tianyuan Yuan, Yue Wang, Yilun Wang, and
Hang Zhao. Vectormapnet: End-to-end vectorized hd map
learning. In International Conference on Machine Learning ,
pages 22352–22369. PMLR, 2023. 3
[30] Jinhyung Park, Chenfeng Xu, Shijia Yang, Kurt Keutzer,
Kris Kitani, Masayoshi Tomizuka, and Wei Zhan. Time will
tell: New outlooks and a baseline for temporal multi-view 3d
object detection. arXiv preprint arXiv:2210.02443 , 2022. 3
[31] Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encoding
images from arbitrary camera rigs by implicitly unprojecting
to 3d. In ECCV , 2020. 3
[32] Aditya Prakash, Kashyap Chitta, and Andreas Geiger. Multi-
modal fusion transformer for end-to-end autonomous driv-
ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 7077–7087,
2021. 1
[33] Abbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu,
Pranaab Dhawan, and Raquel Urtasun. Perceive, predict,
and plan: Safe motion planning through interpretable seman-
tic representations. In Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part XXIII 16 , pages 414–430. Springer, 2020.
3
[34] Hao Shao, Letian Wang, Ruobing Chen, Hongsheng Li, and
Yu Liu. Safety-enhanced autonomous driving using inter-
pretable sensor fusion transformer. In Conference on Robot
Learning , pages 726–737. PMLR, 2023. 3
[35] Ardi Tampuu, Tambet Matiisen, Maksym Semikin, Dmytro
Fishman, and Naveed Muhammad. A survey of end-to-end
driving: Architectures and training methods. IEEE Trans-
actions on Neural Networks and Learning Systems , 33(4):
1364–1384, 2020. 1
[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS , 2017. 3
[37] Li Wang, Xinyu Zhang, Baowei Xv, Jinzhao Zhang, Rong
Fu, Xiaoyu Wang, Lei Zhu, Haibing Ren, Pingping Lu, Jun
Li, et al. Interfusion: Interaction-based 4d radar and lidar fu-
sion for 3d object detection. In 2022 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS) , pages
12247–12253. IEEE, 2022. 3
[38] Ruihao Wang, Jian Qin, Kaiying Li, Yaochen Li, Dong Cao,
and Jintao Xu. Bev-lanedet: An efficient 3d lane detection
based on virtual camera via key-points. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 1002–1011, 2023. 3
[39] Shihao Wang, Yingfei Liu, Tiancai Wang, Ying Li, and Xi-
angyu Zhang. Exploring object-centric temporal modelingfor efficient multi-view 3d object detection. arXiv preprint
arXiv:2303.11926 , 2023. 3
[40] Penghao Wu, Xiaosong Jia, Li Chen, Junchi Yan, Hongyang
Li, and Yu Qiao. Trajectory-guided control prediction for
end-to-end autonomous driving: A simple yet strong base-
line. Advances in Neural Information Processing Systems ,
35:6119–6132, 2022. 3
[41] Enze Xie, Zhiding Yu, Daquan Zhou, Jonah Philion, Anima
Anandkumar, Sanja Fidler, Ping Luo, and Jose M Alvarez.
M2BEV: Multi-camera joint 3d detection and segmentation
with unified birds-eye view representation. arXiv preprint
arXiv:2204.05088 , 2022. 3
[42] Chenyu Yang, Yuntao Chen, Hao Tian, Chenxin Tao, Xizhou
Zhu, Zhaoxiang Zhang, Gao Huang, Hongyang Li, Yu Qiao,
Lewei Lu, et al. Bevformer v2: Adapting modern image
backbones to bird’s-eye-view recognition via perspective su-
pervision. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 17830–
17839, 2023. 3
[43] Tengju Ye, Wei Jing, Chunyong Hu, Shikun Huang, Ling-
ping Gao, Fangzhen Li, Jingke Wang, Ke Guo, Wencong
Xiao, Weibo Mao, et al. Fusionad: Multi-modality fusion for
prediction and planning tasks of autonomous driving. arXiv
preprint arXiv:2308.01006 , 2023. 2, 3, 6
[44] Tianyuan Yuan, Yicheng Liu, Yue Wang, Yilun Wang, and
Hang Zhao. Streammapnet: Streaming mapping network
for vectorized online hd map construction. arXiv preprint
arXiv:2308.12570 , 2023. 3
[45] Jiang-Tian Zhai, Ze Feng, Jinhao Du, Yongqiang Mao,
Jiang-Jiang Liu, Zichang Tan, Yifu Zhang, Xiaoqing Ye,
and Jingdong Wang. Rethinking the open-loop evaluation of
end-to-end autonomous driving in nuscenes. arXiv preprint
arXiv:2305.10430 , 2023. 2, 3, 4
10
14873
