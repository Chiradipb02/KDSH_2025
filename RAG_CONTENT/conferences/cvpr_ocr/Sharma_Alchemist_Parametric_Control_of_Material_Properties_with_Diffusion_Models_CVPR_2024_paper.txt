Alchemist: Parametric Control of Material Properties with Diffusion Models
Prafull Sharma*,1,2Varun Jampani†,2Yuanzhen Li2Xuhui Jia2
Dmitry Lagun2Fredo Durand2Bill Freeman1,2Mark Matthews2
1MIT CSAIL2Google Research
www.prafullsharma.net/alchemist
Albedo Roughness -1  +1  +1 0TransparencyInput Output Input Output Input Output
RoughnessInput Output
MetallicInput Output
AlbedoInput Output
Transparency Metallic -1  +1  +1 0
Figure 1. Overview. Our method, Alchemist, edits material properties of objects in input images by relative attribute strength s.Top:
We set the strength s= 1, resulting in a beetle without specular hightlights, a dark metallic dinosaur, and boot with gray albedo. Our
model generates plausible transparency including the light tint, caustics, and hallucinated plausible details behind the object. Bottom: We
demonstrate smooth edits for linearly chosen strength values.
Abstract
We propose a method to control material attributes of ob-
jects like roughness, metallic, albedo, and transparency in
real images. Our method capitalizes on the generative prior
of text-to-image models known for photorealism, employ-
ing a scalar value and instructions to alter low-level mate-
rial properties. Addressing the lack of datasets with con-
trolled material attributes, we generated an object-centric
synthetic dataset with physically-based materials. Fine-
tuning a modified pre-trained text-to-image model on this
*This research was performed while Prafull Sharma was at Google.
†Varun Jampani is now at Stability AI.synthetic dataset enables us to edit material properties in
real-world images while preserving all other attributes. We
show the potential application of our model to material
edited NeRFs.
1. Introduction
Achieving fine-grained control over material properties
of objects in images is a complex task with wide commer-
cial applications beyond computer graphics. This ability is
particularly relevant in image editing, advertising, and im-
age forensics. We propose a method for precise editing of
material properties in images, harnessing the photorealistic
generative prior of text-to-image models. We specifically
1
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
24130
target four key material properties: roughness, metallic,
albedo, and transparency. Our results illustrate that genera-
tive text-to-image models contain a strong understanding of
light transport which can be leveraged for precise control of
these material properties. The physics of light transport af-
fects the appearance of the object. How we view the objects
is an interplay of physical factors such as surface geome-
try, illumination sources, camera intrinsics, color science,
sensor linearity, and tone-mapping. However, the most sig-
nificant of these factors is material properties.
In computer graphics, Bidirectional Reflectance Distri-
bution Functions (BRDFs) [11,25,26] define material prop-
erties which led to the development of principled and phys-
ically based BRDF models [4]. Prior methods typically em-
ployed an inverse rendering approach to disentangle and es-
timate complex scene attributes like geometry and illumina-
tion for material modification [36]. Recent work by Subias
et al. proposed a GAN-based method trained on synthetic
data for perceptual material edits, focusing on metallic and
roughness parameters, necessitating the masking of the tar-
geted real-world object [81]. Our approach uses the gen-
erative prior of text-to-image models. We directly modify
real-world images in pixel space, eliminating the need for
auxiliary information such as explicit 3D geometry or depth
maps, environment maps, and material annotations, thereby
bypassing the process of accurately estimating object and
scene-level properties.
Manipulating material properties in pixel space using a
pre-trained text-to-image model presents two main chal-
lenges. First, the scarcity of real-world datasets with pre-
cisely labeled material properties makes generalizing from
supervised training difficult. Second, text-to-image models
are trained with textual descriptions like ”gold,” ”wood,”
or ”plastic,” which often lack fine-grained details about the
material. This issue is compounded by the inherent discon-
nect between the discrete nature of words and the continu-
ous nature of material parameters.
To overcome the first challenge, we render a synthetic
dataset featuring physically-based materials and environ-
ment maps, thus addressing the need for fine-grained an-
notations of material properties. For the second challenge,
we employ extra input channels to an off-the-shelf diffusion
model, refining this model with an instruction-based pro-
cess inspired by InstructPix2Pix [3]. Despite being trained
on only 500 synthetic scenes comprising 100 unique 3D ob-
jects, our model effectively generalizes the control of mate-
rial properties to real input images, offering a solution to the
issue of continuous control.
To summarize, we present a method that utilizes a pre-
trained text-to-image model to manipulate fine-grained ma-
terial properties in images. Our approach offers an alterna-
tive to traditional rendering pipelines, eliminating the need
for detailed auxiliary information. The key contributions ofour method are as follows:
1. We introduce an image-to-image diffusion model for
parametric control of low-level material properties,
demonstrating smooth edits of roughness, metallic,
albedo and transparency.
2. We render a synthetic dataset of fine-grained material
edits using 100 3D objects and randomized environ-
ment maps, cameras, and base materials.
3. Our proposed model generalizes to real images despite
being trained on synthetic data.
2. Related Work
Diffusion models for image generation. Denoising Dif-
fusion Probabalistic Models (DDPMs) have been an active
focus of the research community [12, 14,28–30, 33,34,79]
for their excellent photorealistic image generation capabili-
ties from text prompts [56, 65,67,71]. Image-to-image tasks
are possible by modifying the denoising network to ac-
cept image inputs, allowing style-transfer [78], inpainting,
uncropping, super-resolution, and JPEG compression [70].
Furthermore, the generative priors of 2D diffusion models
have been utilized towards novel-view synthesis, 3D gener-
ation, and stylistic 3D editing [6, 8,19,23,31,43,45,62,64,
72,73,76,83,87,91,93,97]. Our image-to-image method
leverages and further controls this learned prior of DDPMs.
Control in generative models. Controlling generative
model output remains an active area of study with many
works proposing text-based methods [1, 3,5,10,21,27,
35,37,44,53,61,82,84,86]. Other works proposed al-
ternative control inputs such as depth maps, sketches [85,
92], paint strokes [51], identity [48, 90], or photo collec-
tions [41, 68,69,75]. Prompt-to-Prompt [27], P+[86], and
Null-text inversion [53] present editing techniques based
on reweighting of cross-attention maps. ControlNet [94]
and T2I-Adapter [55] demonstrate control through spatial
inputs defining mid-level information. Generated images
from diffusion models can also incorporate new subjects
from an image collection using a small number of exem-
plars [7, 20,41,68,69,75,89]. While these works control
high and mid-level information about objects, control of
low-level properties such as materials remains a challenge
for them, leading us to our present line of study.
Material understanding and editing. Editing materials
in images is a significant challenge, requiring a strong un-
derstanding of image formation. Human vision research has
extensively explored how attributes like albedo, roughness,
illumination, and geometry affect object perception [16, 17,
17,18,50,54,57–59, 80].
Image based material editing was introduced by Khan et
al. presenting simple material operations using depth esti-
mates [36]. Subsequent works demonstrated disentangle-
24131
Figure 2. Method. We generate a synthetic dataset by taking each of 100 objects, applying randomized materials and illumination maps,
and modifying the shading network according to randomly sampled attribute strength s. Each object is rendered from 15 randomized
cameras (see Section 3 for details). During training we provide the s= 0image as context and randomly choose a target image of known
attribute strength. At test time we provide the user-input context image and edit strength.
Metallic
 Roughness
 Transparency
 Albedo
Figure 3. Synthetic dataset. Samples from our synthetic dataset
illustrating appearance change for a linear attribute change.
ment of material and lighting with a statistical prior [46],
editing gloss appearance [2,49], intrinsic image decomposi-
tion [42], and 2D editing of material exemplars [98]. Renzo
et al. have proposed linear operations for editing material
parameters [15]. We forego these “decompositional” ap-
proaches and instead leverage the largely self-supervised
prior of DDPMs for direct editing in pixel-space. Similar
use of priors from pre-trained networks has been leveraged
for material segmentation [74].
Generative models, particularly Generative Adversarial
Networks (GANs) [22], have been investigated for their
ability to alter material perception, focusing on gloss and
metallic properties [13, 81]. The application of semantic
and material editing in NeRFs has also been explored using
text-prompts and semantic masks [24, 96].
3. Method
There is no existing object-centric dataset that precisely
varies only single material attributes. Curating such a realworld dataset would be infeasible due to the difficulty of
creating physical objects in this way with known parame-
ters. Therefore, we opt to render a synthetic dataset, giv-
ing us full control of material attributes. Using this data,
we propose a method to perform material attribute control
given a context image, instruction, and a scalar value defin-
ing the desired relative attribute change. The method is
based on latent diffusion model for text-to-image generation
with modification that allows us to condition the network on
the relative attribute strength.
3.1. Datasets
We render our dataset with the Cycles renderer from
Blender [9], using publicly available 3D assets, physically-
based materials, and environment maps. Each scene begins
with one of 100 unique object meshes from polyhaven.com.
Each of these is paired with five randomly chosen mate-
rials of the 1200 available from ambientcg.com, and illu-
minated with one of 400 environment maps. The mate-
rial is a Principled BRDF shader node, the base shader in
Blender. The base configuration of the material is kept as
a control defined as 0 strength change for each of the at-
tributes. This control serves as the context input image to
the method against which relative changes in roughness,
metallic, albedo, and transparency are applied, sampling
10 random relative values for each attribute, the details of
which are described below. Finally, we render 15 images of
each setup using different camera viewpoints and intrinsics.
This creates a wide combination of scenes with diversity
in material, lighting, and background conditions. Samples
from the rendered dataset are presented in Figure 3.
Roughness and Metallic. For both roughness and metal-
lic properties, we operate in an additive framework. In the
case when the material has the associated map for rough-
ness or metallic, we use an additive node yielding a para-
metric control between [-1, 1]. For materials where either
of these spatial maps are missing, we control the attribute
24132
control directly as a constant map, assuming the base 0.5 as
the control state of the attribute. Note that these values are
clamped between [0, 1] so in some cases, further increasing
or decreasing the roughness does not result in any change in
the rendered image. We account for this by under-sampling
such images where the gradient of change is constant.
Reducing the roughness value results in a surface that re-
flects light more uniformly and sharply, giving it a glossy or
shiny appearance. On the other hand, increasing the rough-
ness value leads to a more diffused light reflection, making
the surface appear matte or dull. Low metallic value results
in appearance predominantly determined by the base color,
as in the case of plastic and wood. Increasing the metallic
leads to the surface absorbing more of the incoming light,
resulting in a darker appearance of the object.
Albedo. We implement a color mixing between the original
albedo map of the object and a spatially constant gray (RGB
= 0.5) albedo map. The parametric controller operates be-
tween 0 and 1, where 0 corresponds to the original albedo,
and 1 corresponds to completely gray albedo. This can be
considered as detexturing the albedo and can be interesting
when combined with roughness and metallic parameters to
achieve a mirror-like or a shading-only image.
Transparency. We introduce the ability to control trans-
parency by controlling the transmission value in the BSDF
shader node. The attribute value is chosen to be in range
[0, 1]. For a chosen transmission value t, we choose to re-
duce the roughness and metallic component in an additive
manner by t, and also add a white overlay to the albedo to
increase the intensity of the appeared color. For the value of
0, we expect the same opaque object and at 1, we would get
a transparent version of the object, making it appear as if it
was made of glass. Note that we made the choice to retain
the effect of the albedo resulting in a fine tint on the object.
3.2. Parametric Control in Diffusion Models
The rendered synthetic data is used to finetune an image-
to-image diffusion model conditioned on relative attribute
strength and a generic text instruction providing parametric
control over material properties. We operate in latent space
using Stable Diffusion 1.5, a widely adopted text-to-image
latent diffusion model.
Diffusion models perform sequential denoising on noisy
input samples, directing them towards the dataset distribu-
tion by maximizing a score function [77]. A noising pro-
cess is defined over timesteps t∈T, resulting in a normal
distribution at T. We operate in latent space by using a pre-
trained variational encoder Eand decoder D[39], a potent
aid to conditional image generation [67]. Training draws
an image sample Ifrom the dataset, encodes it into a latent
z=E(I), then noises it at taszt. A denoising network ϵθ
predicts the added noise given the latent zt, diffusion time
t, and conditioning variables.Our image-to-image model is conditioned on an input
image to be edited, provided as E(Ic)concatenated to the
latent being denoised zt. Text conditioning is provided via
cross-attention layers using a generic prompt, p=“Change
the<attribute name> of the <object class> .”
Since textual CLIP embeddings [63] do not encode fine-
grained information well [60], prompt-only conditioning of
sexpressed textually (i.e. “ Change the roughness of the
apple by 0.57. ”) yields inconsistent output. To facilitate
relative attribute strength conditioning, we also concatenate
a constant scalar grid of edit strength s.
We initialize the weights of our denoising network with
the pre-trained checkpoint of InstructPix2Pix [3], provid-
ing an image editing prior and understanding of instructive
prompts. During training (Fig. 2), we minimizes the loss:
L=EE(I),E(Ic),s,p,ϵ∼N(0,1),t
||ϵ−ϵΘ(zt, t,E(Ic),s, p)||2
2
(1)
We always provide the s= 0 image as context Ic, and
draw an edited image Ieat random for noising. Since we
always render an s= 0sample, and other sare chosen with
stratified sampling, our distribution has a slight bias towards
zero. Since many edit strengths may have little effect (i.e.
we cannot lower the roughness of an object with 0 rough-
ness), we find that providing too many of these examples
biases the network towards inaction. We therefore down-
weight such null examples, defined as ||Ic−Ie||2< τ, by
wnull via rejection sampling. In practice we set wnull=
0.80,τ= 0.05. We train with fp16 precision for 10k
steps using Adam [38] and learning rate of 5e-5 . We use
the text encoders and noise schedule from Stable Diffusion.
We trained the model on a machine with 2 x A100 for 10k
steps.
At test time we provide a held out image as context Ic,
edit strength s, and prompt pfor the object class of the input
image. We denoise for 20steps using the DPM-solver ++
based noise scheduler [47].
Multi-attribute editing. We edit multiple attributes in a
single diffusion pass by concatenating more than one edit
strength, drawn from {sa, srsm}giving us [zt|E(Ic)|s]as
the final UNet input, where |is concatenation.
Classifier-free guidance. Ho et al. [30] proposed classifier-
free guidance (CGF) to improve visual quality and faithful-
ness of images generated by diffusion models. We retain
the same CFG setup as InstructPix2Pix for both image and
prompt conditioning. We do not however impose CFG with
respect to the relative attribute strengths s. We want the
network to be faithful to edit strength and forced to reason
about incoming material attributes. As scan be 0 by defini-
tion of the problem itself, and downweighted as described
above, we did not find CFG on snecessary.
We will release the dataset generation pipeline, image
renderings with metadata, and the training code.
24133
Roughness
 Metallic
 Albedo
 Transparency
Input
Input
InputInput
Input
InputInput
Input
InputInput
Input
Inputbag, +1
pumpkin, -1
statue, -1car, +1
pumpkins, +1
cat statue , -1statue, +1
fish, +1
toy, +1binocular, +1
chair, +1
bananas, +1Figure 4. Single-attribute editing results. Outputs from our model trained on individual attributes. Left of columns are held-out input and
right are model output (object class, s). Increased “Roughness” replaces specular highlights on the bag with base albedo. “Metallic” varies
contributions from albedo and shine in regions of the pumpkins and cat. Surfaces are changed to a flat grey “Albedo” revealing object
illumination. “Transparency” preserves object tint while inpainting background and hallucinating plausible hidden structures and caustics.
4. Results
We present qualitative analysis demonstrating the gen-
eralization capability of our model to real images despite
being trained on synthetic data. Comparisons to baselines
show the effectiveness of our model for fine-grained mate-
rial editing, further supported by a user study. We extend
the use of our model to NeRF material editing on the DTU
dataset [32, 52].
4.1. Results on Real Images
We demonstrate the effectiveness of our technique
through editing material attributes, one at a time, for real un-
seen images, in Figure 4. For each of the material attributes
we use a separate model trained only on that attribute. We
observe that the model outputs preserve geometry and take
the global illumination into account.
Roughness. As the roughness is increased, the output
shows removal of the specular highlights replaced by esti-
mate of the base albedo. The highlights are amplified when
the roughness is reduced as shown in the case of the pump-
kin and statue.
Metallic. The increase in the metallic component of the car
and pumpkin results in dampening of the base albedo and
increase in the shine on the surface. The effect is reverse
for the cat statue when the metallic strength was reduced.
Our method shows similar behavior to the Principled BRDF
shaders, which present perceptually subtle effects when tun-
ing the metallic value.
Albedo. As the relative strength for the albedo is turnedto 1, we observe the albedo of the Buddha statue, fish, and
toy go to gray. This is not a trivial in-image desaturation
operation as the network maintains the highlights, shadows,
and the light tint from the plausible environment map.
Transparency. The transparent renditions of the binocu-
lar and the chair demonstrate the prior over 3D geometry
of the objects, using which it generates the appropriately
tinted glass-like appearance and in-paints background ob-
jects. With the edit of the banana, we can see the caustics
underneath and the preservation of the specular highlights.
4.2. Baseline Comparisons
We compare our method, Alchemist, to the GAN-based
in-image material editing of Subias et al. [81], Prompt-to-
Prompt [27] with Null-text Inversion (NTI) [53], and In-
structPix2Pix [3] in Figure 5. Furthermore, we fine-tuned
the InstructPix2Pix prompt-based approach with our syn-
thetic dataset. Subias et al.’s method results in exagger-
ated material changes as their objective is perceptual, not
physically-based material edits. Null-text inversion and In-
structPix2Pix change the global image information instead
of only the object of interest: lighting changes for rough-
ness and albedo edits, or a geometry change for metallicity
edit. When InstructPix2Pix is trained on our dataset with a
prompt-only approach, we observe the model exaggerating
the intended effect, yielding artifacts on the panda for metal-
lic and the water for the transparency case. The model also
changes the albedo of the sand when only asked to make the
change to the crab. Our method faithfully edits only the ob-
ject of interest, introducing the specular hightlights on the
24134
iPix2Pix
 iPix2Pix (our data)
 Ours
 Subias et al.
 Null-text inversion
Does not support 
albedo changeInput
Roughness
cat statue , -1
Metallic
pandas, +1
Albedo
crabs, +1
Transparency
dophin, +1
Does not support 
transparency changeFigure 5. Qualitative comparison. Comparison of Alchemist with baseline methods. We increase each of the attributes shown on the left.
InstructPix2Pix w/
our data Our Method
PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓
Roughness 30.9
0.89 0.13 31.5 0.90 0.09
Metallic 31.0 0.89 0.10 31.1 0.89 0.09
Albedo 26.9 0.88 0.14 27.2 0.88 0.10
Transparency 26.9 0.85 0.13 27.1 0.85 0.13
Table 1. Quantitative analysis. Metrics for the prompt-only In-
structPix2Pix trained on our data and our proposed method com-
puting the PSNR, SSIM [88], and LPIPS [95] on a held-out unseen
synthetic rendered dataset of 10 scenes with 15 cameras.
leg of the cat statue, dampening the albedo for the metal-
lic panda, changing the albedo of the crab to gray while
retaining the geometry and illumination effects, and turn-
ing the dolphin transparent with plausible refractive effects.
Specific configurations for each baseline is presented in the
supplement.
Quantitative Comparisons. Due to the lack of an existing
dataset for quantitative analysis, we rendered 10 held-out
scenes with unseen 3D objects, materials, and environment
maps. Each scene was rendered from 15 different view-
points with 10 random scalar values for each attribute. We
present the average PSNR, SSIM [88], and LPIPS [95] of
edits against GT for prompt-only InstructPix2Pix and Al-
chemist in Table 1. While the PSNR and SSIM scores are
quite close, our model does better in terms of the LPIPS
score. We also note that prompt-only InstructPix2Pix fine-tuned on our data does not yield smooth transitions as the
relative strength is linearly changed, visible in video results
presented in the supplement. Note that the image recon-
struction metrics are not commonly used for evaluation of
probabilistic generative models. Samples of the test data
and model outputs are presented in the supplement.
User study. To further the comparison between the baseline
and our method, we conducted a user study presenting ran-
domly selected N=14 users with pairs of edited images. For
each image pair, the users were asked to choose between
the two based on: (1) Photorealism, and (2) “Which edit do
you prefer?”. For both questions, the users were presented
with the instruction, i.e. for transparency, the instruction
was stated as ”the method should be able to output the same
object as transparent retaining the tint of the object”. Each
user was presented with a total of 12 image pairs (3 image
results for each of the 4 attributes).
Our method was chosen as the one with more photo-
realistic edits (69.6% vs. 30.4%) and was strongly preferred
overall (70.2% vs. 29.8%). This is likely due to the ap-
parent exaggeration exhibited by InstructPix2Pix trained on
our data with prompt-only approach, leading to saturated
effects making it less photo-realistic.
Smoothness in Parametric Control.
We demonstrate that our model achieves fine grained
24135
Albedo Roughness -1  +1  +1 0
Transparency Metallic -1  +1  +1 0
Figure 6. Slider results. Alchemist produces edits smoothly with attribute strength. We note that the outputs for linear change in the input
relative strength in InstructPix2Pix prompt-only trained on our data results in non-smooth transitions. Refer to the supplement videos.
“Change the material 
of the cat.”
albedo: 2
roughness:  -1
metallic:  1
“Change the transparency 
of the cup.”
transparency: 1Input Image
Segmentation
 Output
 Output
 Segmentation
Input Image
Figure 7. Spatial Localization. Edit results (bottom) when the
scalar strength input is masked by the shown segmentation (top).
The image is only altered in the segmented region, becoming ei-
ther shiny (cat ), or semi-transparent (cup).
control of material parameters by linearly varying the
strength of a single attribute, as shown in Figure 6. Ob-
serve that the model generates plausible specular highlights
on the headphone instead of naively interpolating pixel val-
ues to the extrema and introduces more dampening of the
albedo on the cat to give it a metallic look. For transparency,
the model preserves the geometry while refracting the light
through the object to produce a transparent look. The in-structPix2Pix model trained on our data did not yield such
smooth results as the relative strength of the attributes were
changed in text format. Please refer to the supplementary
for video results.
4.3. Specializations
Spatial localization. Attending to a specific instance of a
class when multiple objects of the same class are present
in the image is a difficult task using just language instruc-
tion. We explore the possibility of changing the material
attribute of a specific instance of the class by only attribut-
ing the scalar value in the segmented region, assuming a
known segmentation map from an instance segmentation
method such as Segment Anything [40]. Though the net-
work was not trained for this specific task, we find that the
network does respect the localization of the relative attribute
strength, though requires over-driving to values beyond 1.
We observe that mask-based editing works in such cases,
i.e. changing the material properties of specific instance of
cat and cup, as shown in Figure 7.
Multi-attribute changes. To enable control over multiple
attributes in a single diffusion pass, we train our network on
two versions of the dataset to vary albedo (s a), roughness
(sr), and metallic (s m). In the axis-only sampled version,
we keep the context image at the baseline, and vary a single
attribute at a time, such that only one of {sa, sr, sm}is non-
zero for any given training target image. In the volume sam-
pled version, {sa, sr, sm}are all allowed to be non-zero,
effectively sampling the 3D volume of material attributes.
In both cases, we keep the same number of input images.
We present the qualitative analysis of the joint control
24136
Input Axis-only sampled V olume sampled
Figure 8. Multi-Attribute Editing. Comparison between an
“axis-only sampled” model trained on images where only one
of{sa, sr, sm}is̸= 0, vs. a “volume sampled” one where all
{sa, sr, sm}may be ̸= 0. We show edits with (sa, sr, sm) =
(1,−1, 1). The former tends to edit only a single attribute, while
the latter successfully achieves the desired “silver” appearance.
Original held-out views
 NeRF renderings w/ edited images
Figure 9. NeRF Results. Left: Original test views from DTU.
Right: We edit training views of each scene, train a NeRF, then
render held-out test views. The respective edits (sa, sr, sm)from
top to bottom are: scan30: (0,−0.5, 0.5),scan118: (1,−1, 1)
andscan69: (1,1,0).
in Figure 8. We find that the axis-only sampled training
fails to compose the three attributes, generally showing bias
towards one of the attributes. The model trained on the
volume of these attributes successfully generalizes, demon-
strating ability to edit multiple attributes at once. We find
this essential to producing a strong metallic appearance on
objects, as the Principled BSDF shader requires a white,
non-rough, and highly metallic surface to produce this look.
Material editing of NeRFs. We test the efficacy of per-
frame editing using our method for two-step material con-
trol in neural radiance field (NeRF) reconstruction. We use
a selection of scenes from the DTU MVS [32] dataset and
edit them to have reduced albedo or higher specular reflec-
tions. We train a NeRF with the vanilla configuration based
on [66] (complete details in the supplement).
In the results presented in Figure 9, we observe highly
Input Roughness, +1 Input Transparency, +1Figure 10. Limitations. Alchemist sometimes fails to achieve the
desired result. Left: A shiny surface remains on the teapot after a
roughness edit. Right: The stem of a candy-cane is omitted.
plausible renderings from held-out views showing 3D struc-
ture with the intended albedo, roughness, and metallic
change. Please refer to the supplement for rendered video
of NeRFs trained on edited data.
5. Discussion
Our model generalizes to editing fine-grained material
properties in real images, despite being trained solely on
synthetic data. We believe that our method could extend
to a wide range of material alterations achievable with a
shader. However, our approach does have limitations, such
as producing minimal perceptual changes for roughness and
metallic attributes, and occasionally yielding physically un-
realistic transparency, as illustrated in Figure 10. The model
lacks a complete 3D world model and is unable inpaint to
maintain physical consistency as seen in the candy-cane ex-
ample. As is typical with generative models, our method
generates plausible interpretations that are true to the given
instructions, but it does not necessarily replicate the exact
outcomes of a traditional graphics renderer.
6. Conclusion
We present a method that allows precise in-image con-
trol of material properties, utilizing the advanced generative
capabilities of text-to-image models. Our approach shows
that even though the model is trained on synthetic data, it
effectively edits real images, achieving seamless transitions
as the relative strength of the desired attribute is varied.
Beyond image editing, we demonstrate the applicability to
NeRF allowing for editable materials in NeRFs. We believe
that our work can further impact downstream applications
and allow for improved control over low-level properties of
objects.
7. Acknowledgements
This work was supported by NSF 2105819 and gifts
from Google and Amazon. We would like to thank For-
rester Cole, Charles Herrmann, Junhwa Hur, and Nataniel
Ruiz for helpful discussions. Thanks to Shriya Kumar and
Parimarjan Negi for proofreading the submission.
24137
References
[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended
diffusion for text-driven editing of natural images. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 18208–18218, 2022. 2
[2] Ivaylo Boyadzhiev, Kavita Bala, Sylvain Paris, and Edward
Adelson. Band-sifting decomposition for image-based mate-
rial editing. ACM Transactions on Graphics (TOG), 34(5):1–
16, 2015. 3
[3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structpix2pix: Learning to follow image editing instructions.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 18392–18402, 2023.
2,4,5
[4] Brent Burley. Physically based shading at disney. In ACM
SIGGRAPH 2012 Courses. ACM, 2012. 2
[5] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xi-
aohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mu-
tual self-attention control for consistent image synthesis and
editing. arXiv preprint arXiv:2304.08465, 2023. 2
[6] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen
Tu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf:
A unified approach to 3d generation and reconstruction. In
ICCV, 2023. 2
[7] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui
Jia, Ming-Wei Chang, and William W Cohen. Subject-driven
text-to-image generation via apprenticeship learning. arXiv
preprint arXiv:2304.00186, 2023. 2
[8] Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai,
Gang Yu, Lei Yang, and Guosheng Lin. It3d: Improved text-
to-3d generation with explicit view synthesis. arXiv preprint
arXiv:2308.11473, 2023. 2
[9] Blender Online Community. Blender - a 3D modelling and
rendering package. Blender Foundation, Stichting Blender
Foundation, Amsterdam, 2018. 3
[10] Yuren Cong, Martin Renqiang Min, Li Erran Li, Bodo
Rosenhahn, and Michael Ying Yang. Attribute-centric
compositional text-to-image generation. arXiv preprint
arXiv:2301.01413, 2023. 2
[11] Robert L Cook and Kenneth E. Torrance. A reflectance
model for computer graphics. ACM Transactions on Graph-
ics (ToG), 1(1):7–24, 1982. 2
[12] Katherine Crowson, Stella Biderman, Daniel Kornis,
Dashiell Stander, Eric Hallahan, Louis Castricato, and Ed-
ward Raff. Vqgan-clip: Open domain image generation and
editing with natural language guidance. In European Con-
ference on Computer Vision, pages 88–105. Springer, 2022.
2
[13] Johanna Delanoy, Manuel Lagunas, J Condor, Diego Gutier-
rez, and Bel ´en Masia. A generative framework for image-
based editing of material appearance using perceptual at-
tributes. In Computer Graphics Forum, volume 41, pages
453–464. Wiley Online Library, 2022. 3
[14] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems, 34:8780–8794, 2021. 2[15] Francesco Di Renzo, Claudio Calabrese, and Fabio Pellacini.
Appim: Linear spaces for image-based appearance editing.
ACM Transactions on Graphics (TOG), 33(6):1–9, 2014. 3
[16] Katja Doerschner, Huseyin Boyaci, and Laurence T Mal-
oney. Estimating the glossiness transfer function induced
by illumination change and testing its transitivity. Journal
of Vision, 10(4):8–8, 2010. 2
[17] Roland W Fleming. Visual perception of materials and their
properties. Vision research, 94:62–75, 2014. 2
[18] Roland W Fleming, Ron O Dror, and Edward H Adelson.
Real-world illumination and the perception of surface re-
flectance properties. Journal of vision, 3(5):3–3, 2003. 2
[19] Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel.
Scenescape: Text-driven consistent scene generation. ArXiv,
abs/2302.01133, 2023. 2
[20] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-
nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-
Or. An image is worth one word: Personalizing text-to-
image generation using textual inversion. arXiv preprint
arXiv:2208.01618, 2022. 2
[21] Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-Bin
Huang. Expressive text-to-image generation with rich text.
InProceedings of the IEEE/CVF International Conference
on Computer Vision, pages 7545–7556, 2023. 2
[22] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial networks. Commu-
nications of the ACM, 63(11):139–144, 2020. 3
[23] Ayaan Haque, Matthew Tancik, Alexei A Efros, Alek-
sander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf:
Editing 3d scenes with instructions. arXiv preprint
arXiv:2303.12789, 2023. 2
[24] Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander
Holynski, and Angjoo Kanazawa. Instruct-NeRF2NeRF:
Editing 3D Scenes with Instructions. In Proceedings of
the IEEE/CVF International Conference on Computer Vision
(ICCV), 2023. 3
[25] Xiao D He, Patrick O Heynen, Richard L Phillips, Kenneth E
Torrance, David H Salesin, and Donald P Greenberg. A fast
and accurate light reflection model. ACM SIGGRAPH Com-
puter Graphics, 26(2):253–254, 1992. 2
[26] Xiao D He, Kenneth E Torrance, Francois X Sillion, and
Donald P Greenberg. A comprehensive physical model
for light reflection. ACM SIGGRAPH computer graphics,
25(4):175–186, 1991. 2
[27] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
age editing with cross attention control. arXiv preprint
arXiv:2208.01626, 2022. 2,5
[28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems, 33:6840–6851, 2020. 2
[29] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,
Mohammad Norouzi, and Tim Salimans. Cascaded diffusion
models for high fidelity image generation. The Journal of
Machine Learning Research, 23(1):2249–2281, 2022. 2
[30] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. arXiv preprint arXiv:2207.12598, 2022. 2,4
24138
[31] Lukas H ¨ollein, Ang Cao, Andrew Owens, Justin John-
son, and Matthias Nießner. Text2room: Extracting tex-
tured 3d meshes from 2d text-to-image models. ArXiv,
abs/2303.11989, 2023. 2
[32] Rasmus Jensen, Anders Dahl, George V ogiatzis, Engin Tola,
and Henrik Aanæs. Large scale multi-view stereopsis eval-
uation. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 406–413, 2014. 5,8
[33] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,
Eli Shechtman, Sylvain Paris, and Taesung Park. Scal-
ing up gans for text-to-image synthesis. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 10124–10134, 2023. 2
[34] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
Elucidating the design space of diffusion-based generative
models. Advances in Neural Information Processing Sys-
tems, 35:26565–26577, 2022. 2
[35] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen
Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:
Text-based real image editing with diffusion models. arXiv
preprint arXiv:2210.09276, 2022. 2
[36] Erum Arif Khan, Erik Reinhard, Roland W Fleming, and
Heinrich H B ¨ulthoff. Image-based material editing. ACM
Transactions on Graphics (TOG), 25(3):654–663, 2006. 2
[37] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Dif-
fusionclip: Text-guided diffusion models for robust image
manipulation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 2426–
2435, 2022. 2
[38] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980,
2014. 4
[39] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114, 2013. 4
[40] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. arXiv preprint arXiv:2304.02643, 2023. 7
[41] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Shechtman, and Jun-Yan Zhu. Multi-concept customization
of text-to-image diffusion. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 1931–1941, 2023. 2
[42] Guilin Liu, Duygu Ceylan, Ersin Yumer, Jimei Yang, and
Jyh-Ming Lien. Material editing using a physically based
rendering network. In Proceedings of the IEEE International
Conference on Computer Vision, pages 2261–2269, 2017. 3
[43] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-
makov, Sergey Zakharov, and Carl V ondrick. Zero-1-to-3:
Zero-shot one image to 3d object. ArXiv, abs/2303.11328,
2023. 2
[44] Xihui Liu, Zhe Lin, Jianming Zhang, Handong Zhao, Quan
Tran, Xiaogang Wang, and Hongsheng Li. Open-edit: Open-
domain image manipulation with open-vocabulary instruc-
tions. In European Conference on Computer Vision, pages
89–106. Springer, 2020. 2
[45] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie
Liu, Taku Komura, and Wenping Wang. Syncdreamer:Learning to generate multiview-consistent images from a
single-view image. arXiv preprint arXiv:2309.03453, 2023.
2
[46] Stephen Lombardi and Ko Nishino. Reflectance and natural
illumination from a single image. In Computer Vision–ECCV
2012: 12th European Conference on Computer Vision, Flo-
rence, Italy, October 7-13, 2012, Proceedings, Part VI 12,
pages 582–595. Springer, 2012. 3
[47] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongx-
uan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided
sampling of diffusion probabilistic models. arXiv preprint
arXiv:2211.01095, 2022. 4
[48] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu.
Subject-diffusion: Open domain personalized text-to-image
generation without test-time fine-tuning. arXiv preprint
arXiv:2307.11410, 2023. 2
[49] Yusuke Manabe, Midori Tanaka, and Takahiko Hori-
uchi. Glossy appearance editing for heterogeneous mate-
rial objects. Journal of Imaging Science and Technology,
65(6):60406–1, 2021. 3
[50] Phillip J Marlow, Juno Kim, and Barton L Anderson. The
perception and misperception of specular surface reflectance.
Current Biology, 22(20):1909–1913, 2012. 2
[51] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided
image synthesis and editing with stochastic differential equa-
tions. arXiv preprint arXiv:2108.01073, 2021. 2
[52] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. In ECCV, 2020. 5
[53] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and
Daniel Cohen-Or. Null-text inversion for editing real im-
ages using guided diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 6038–6047, 2023. 2,5
[54] Isamu Motoyoshi and Hiroaki Matoba. Variability in con-
stancy of the perceived surface reflectance across different
illumination statistics. Vision Research, 53(1):30–39, 2012.
2
[55] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-
gang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning
adapters to dig out more controllable ability for text-to-image
diffusion models. arXiv preprint arXiv:2302.08453, 2023. 2
[56] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741, 2021. 2
[57] Shin’ya Nishida and Mikio Shinya. Use of image-based
information in judgments of surface-reflectance properties.
JOSA A, 15(12):2951–2965, 1998. 2
[58] Ga ¨e Obein, Kenneth Knoblauch, and Franc ¸ise Vi ´eot. Dif-
ference scaling of gloss: Nonlinearity, binocularity, and con-
stancy. Journal of vision, 4(9):4–4, 2004. 2
[59] Maria Olkkonen and David H Brainard. Perceived glossi-
ness and lightness under real-world illumination. Journal of
vision, 10(9):5–5, 2010. 2
24139
[60] Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar
Mosseri, Michal Irani, and Tali Dekel. Teaching clip to count
to ten. arXiv preprint arXiv:2302.12066, 2023. 4
[61] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,
and Dani Lischinski. Styleclip: Text-driven manipulation
of stylegan imagery. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision (ICCV), pages
2085–2094, October 2021. 2
[62] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv
preprint arXiv:2209.14988, 2022. 2
[63] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning, pages
8748–8763. PMLR, 2021. 4
[64] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,
Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aber-
man, Michael Rubinstein, Jonathan Barron, et al. Dream-
booth3d: Subject-driven text-to-3d generation. arXiv
preprint arXiv:2303.13508, 2023. 2
[65] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125,
1(2):3, 2022. 2
[66] Daniel Rebain, Mark Matthews, Kwang Moo Yi, Dmitry La-
gun, and Andrea Tagliasacchi. Lolnerf: Learn from one look.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 1558–1567, 2022. 8
[67] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. 2022 ieee. In CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 10674–10685, 2021. 2,4
[68] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. arXiv preprint arXiv:2208.12242, 2022. 2
[69] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei,
Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein,
and Kfir Aberman. Hyperdreambooth: Hypernetworks for
fast personalization of text-to-image models. arXiv preprint
arXiv:2307.06949, 2023. 2
[70] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,
Jonathan Ho, Tim Salimans, David Fleet, and Mohammad
Norouzi. Palette: Image-to-image diffusion models. In
ACM SIGGRAPH 2022 Conference Proceedings, pages 1–
10, 2022. 2
[71] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems, 35:36479–36494, 2022. 2
[72] Etai Sella, Gal Fiebelman, Peter Hedman, and Hadar
Averbuch-Elor. V ox-e: Text-guided voxel editing of 3d ob-
jects, 2023. 2[73] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon
Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee,
and Seungryong Kim. Let 2d diffusion model know 3d-
consistency for robust text-to-3d generation. arXiv preprint
arXiv:2303.07937, 2023. 2
[74] Prafull Sharma, Julien Philip, Micha ¨el Gharbi, Bill Freeman,
Fredo Durand, and Valentin Deschaintre. Materialistic: Se-
lecting similar materials in images. ACM Transactions on
Graphics (TOG), 42(4):1–14, 2023. 3
[75] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-
booth: Personalized text-to-image generation without test-
time finetuning. arXiv preprint arXiv:2304.03411, 2023. 2
[76] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,
and Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-
eration, 2023. 2
[77] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International confer-
ence on machine learning, pages 2256–2265. PMLR, 2015.
4
[78] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro
Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang,
Glenn Entis, Yuanzhen Li, Yuan Hao, Irfan Essa, Michael
Rubinstein, and Dilip Krishnan. Styledrop: Text-to-image
generation in any style. arXiv preprint arXiv:2306.00983,
2023. 2
[79] Yang Song and Stefano Ermon. Generative modeling by esti-
mating gradients of the data distribution. Advances in neural
information processing systems, 32, 2019. 2
[80] Katherine R Storrs, Barton L Anderson, and Roland W
Fleming. Unsupervised learning predicts human percep-
tion and misperception of gloss. Nature Human Behaviour,
5(10):1402–1417, 2021. 2
[81] J Daniel Subias and Manuel Lagunas. In-the-wild material
appearance editing using perceptual attributes. In Computer
Graphics Forum, volume 42, pages 333–345. Wiley Online
Library, 2023. 2,3,5
[82] Ming Tao, Bing-Kun Bao, Hao Tang, Fei Wu, Longhui Wei,
and Qi Tian. De-net: Dynamic text-guided image editing ad-
versarial networks. arXiv preprint arXiv:2206.01160, 2022.
2
[83] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,
Michael Niemeyer, and Federico Tombari. Textmesh: Gen-
eration of realistic 3d meshes from text prompts. ArXiv,
abs/2304.12439, 2023. 2
[84] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali
Dekel. Plug-and-play diffusion features for text-driven
image-to-image translation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 1921–1930, 2023. 2
[85] Andrey V oynov, Kfir Aberman, and Daniel Cohen-Or.
Sketch-guided text-to-image diffusion models. In ACM SIG-
GRAPH 2023 Conference Proceedings, pages 1–11, 2023.
2
[86] Andrey V oynov, Qinghao Chu, Daniel Cohen-Or, and Kfir
Aberman. p+: Extended textual conditioning in text-to-
image generation. arXiv preprint arXiv:2303.09522, 2023.
2
24140
[87] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh,
and Greg Shakhnarovich. Score jacobian chaining: Lifting
pretrained 2d diffusion models for 3d generation, 2022. 2
[88] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE transactions on image processing,
13(4):600–612, 2004. 6
[89] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei
Zhang, and Wangmeng Zuo. Elite: Encoding visual con-
cepts into textual embeddings for customized text-to-image
generation. arXiv preprint arXiv:2302.13848, 2023. 2
[90] Guangxuan Xiao, Tianwei Yin, William T Freeman, Fr ´edo
Durand, and Song Han. Fastcomposer: Tuning-free multi-
subject image generation with localized attention. arXiv
preprint arXiv:2305.10431, 2023. 2
[91] Jiale Xu, Xintao Wang, Yan-Pei Cao, Weihao Cheng, Ying
Shan, and Shenghua Gao. Instructp2p: Learning to edit 3d
point clouds with text instructions. arXiv e-prints, 2023. 2
[92] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-
adapter: Text compatible image prompt adapter for text-to-
image diffusion models. arXiv preprint arXiv:2308.06721,
2023. 2
[93] Lu Yu, Wei Xiang, and Kang Han. Edit-diffnerf: Editing
3d neural radiance fields using 2d diffusion model. arXiv
e-prints, 2023. 2
[94] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 3836–3847, 2023. 2
[95] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 586–595, 2018. 6
[96] Xingchen Zhou, Ying He, F. Richard Yu, Jianqiang Li, and
You Li. Repaint-nerf: Nerf editting via semantic masks and
diffusion models. arXiv e-prints, 2023. 3
[97] Jingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, and
Guanbin Li. Dreameditor: Text-driven 3d scene editing with
neural fields. SIGGRAPH Asia, 2023. 2
[98] K ´aroly Zsolnai-Feh ´er, Peter Wonka, and Michael Wimmer.
Photorealistic material editing through direct image manip-
ulation. In Computer Graphics Forum, volume 39, pages
107–120. Wiley Online Library, 2020. 3
24141
