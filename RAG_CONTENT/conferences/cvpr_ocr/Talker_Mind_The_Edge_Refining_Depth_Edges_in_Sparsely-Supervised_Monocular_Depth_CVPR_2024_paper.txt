Mind The Edge: ReÔ¨Åning Depth Edges in Sparsely-Supervised Monocular Depth
Estimation
Lior Talker1Aviad Cohen1Erez Yosef1;2Alexandra Dana1Michael Dinerstein1
1Samsung Israel R&D Center, Tel Aviv, Israel2Tel Aviv University, Israel
flior.talker,aviad.cohen,alex.dana,m.dinerstein g@samsung.com erez.yo@gmail.com
(a) RGB Baseline Ours RGB Baseline Ours
(b) Baseline Ours Baseline Ours
Figure 1. ReÔ¨Åning depth edges with our method when using Packnet-SAN [18] as an MDE baseline. (a) Depth estimation (DDAD
dataset). Zoom-in on the crops (on the right) to see the improvement in the 2D localization of the depth edges between the baseline and
our method. (b) Augmented reality. An example of virtual objects planted in a scene from the KITTI dataset for AR applications. Zoom-in
and inspect the boundaries for the best impression of the depth edges accuracy.
Abstract
Monocular Depth Estimation (MDE) is a fundamental
problem in computer vision with numerous applications.
Recently, LIDAR-supervised methods have achieved re-
markable per-pixel depth accuracy in outdoor scenes. How-
ever, signiÔ¨Åcant errors are typically found in the proximity
of depth discontinuities, i.e., depth edges, which often hin-
der the performance of depth-dependent applications that
are sensitive to such inaccuracies, e.g., novel view synthe-
sis and augmented reality. Since direct supervision for the
location of depth edges is typically unavailable in sparse
LIDAR-based scenes, encouraging the MDE model to pro-
duce correct depth edges is not straightforward. To the best
of our knowledge this paper is the Ô¨Årst attempt to address
the depth edges issue for LIDAR-supervised scenes. In this
work we propose to learn to detect the location of depth
edges from densely-supervised synthetic data, and use it to
generate supervision for the depth edges in the MDE train-
ing. To quantitatively evaluate our approach, and due to
the lack of depth edges GT in LIDAR-based scenes, we
manually annotated subsets of the KITTI and the DDAD
datasets with depth edges ground truth. We demonstrate
signiÔ¨Åcant gains in the accuracy of the depth edges with
comparable per-pixel depth accuracy on several challeng-ing datasets. Code and datasets are available at https:
//github.com/liortalker/MindTheEdge .
1. Introduction
Monocular Depth Estimation (MDE) aims to recover the
depth of each pixel in a single RGB image. It is used
in many important applications, such as robotic navigation
[10], novel view synthesis [6] and Augmented Reality (AR)
[27]. Nonetheless, MDE is an ill-posed problem; that is,
a single RGB image may be generated from many possi-
ble scenes. However, in recent years, many MDE meth-
ods, based on Convolutional Neural Networks (CNNs) have
shown remarkable results. CNN-based supervised methods
are trained with depth Ground Truth (GT) which is usually
dense for indoor scenes, e.g., acquired using a RGBD cam-
era [41], and sparse for outdoor scenes, e.g., acquired using
a LIDAR sensor [15].
It was observed in several papers [29, 49, 55] that the
2D locations of depth discontinuities, which we refer to as
depth edges in this paper, are often poorly estimated, result-
ing in thick smooth depth gradients or incorrectly-localized
edges (see the baseline in Fig. 1). Some applications that
use predicted depth maps are highly sensitive to errors in
depth edges, which are often part of the silhouette of ob-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10606
RGB
LIDAR
Ledge
Depth edges GTTrain DEE on synthetic data
EDB RGB
 RGB
Depth GTTrain MDE with EL
RGB
LIDAR
Inference with DEE on real data
Ledge
Ldepth
DEE ModelDepth edges prob.
 Depth edges prob.
DEE Model
MDE ModelDepth map
MDE ModelDepth map
MDE Train Step 2
MDE Train Step 3
MDE Train Step 1
 A
 B
C
 D
 MDE Inference
Figure 2. Overview of our proposed MDE training method. (A) Training the DEE model on synthetic data. (B) Inferring depth edges
on the training set of the real data using the trained DEE model. (C) Training the MDE model on real data with the Edge Loss (EL) using
the supervision from the previous step. (D) Inference using the MDE model on the real data. Solid and broken lines represent the dataÔ¨Çow
and the GT used in loss functions, respectively.
jects. One example of such an application is Novel View
Synthesis (NVS) ‚Äì generating a new view of a scene cap-
tured from one or more views. In NVS methods that use
depth explicitly [6], these type of localization errors in the
2D location of depth edges, may result in wrongly local-
ized object parts in another newly generated view. Another
important application that often uses predicted depth is vir-
tual object rendering for AR [27], which computes for each
pixel the closest occluding object from the point of view
of the user. When relying on inaccurate depth edges in the
computation of the occlusion, signiÔ¨Åcant artifacts and unre-
alistic appearance may occur (see Fig. 1).
Roughly speaking, the MDE network solves two sub-
tasks: (i) depth edge estimation, and (ii) continuous surface
depth estimation. Our method is based on the observation
that MDE networks tend to focus on the latter much more
than the former. The underlying reasons for this behaviour
are probably twofold. The Ô¨Årst is the small impact of the
depth edges on the network‚Äôs loss since they occupy only
a tiny portion of the image. The second, which is partially
discussed in [9, 51], is due to alignment errors between the
RGB image and the LIDAR signal. SpeciÔ¨Åcally, LIDAR
measurements are often wrongly associated with objects al-
though belonging to the background, or absent in occluded
areas that are revealed in the time gap between the RGB and
the LIDAR data acquisition (Fig. 3b). This phenomenon is
also depicted in Fig. 3a for the LIDAR of the KITTI dataset
[15], where regions close to depth edges suffer from a lower
density of LIDAR measurements.
In this paper we propose to improve the accuracy of the
depth edges in MDE methods by directly encouraging the
depth edges of the predicted depth map to be well-localized.
Assuming that depth edges GT is available, a dedicated
depth edges loss that encourages the network to generatedepth discontinuities in the correct locations may be used.
However, due to the sparsity of the depth GT in typical out-
door scenes (e.g., KITTI), which are considered in this pa-
per as our target domain, obtaining accurate depth edges
GT is a challenging task. To this end, we are the Ô¨Årst to pro-
pose a method to tackle this problem in sparsely-supervised
MDE methods. We note that another aspect of depth edges
accuracy, namely edge sharpness [7], is not considered in
this paper, and is complementary to our work.
To this end, we propose to train a Depth Edge Estima-
tion (DEE) network (Sec. 3.5), which predicts the probabil-
ity that a pixel is located on a depth edge, using an accu-
rate dense depth GT of a synthetic dataset [22] (Fig. 2A).
Using the DEE network we infer a probabilistic map of
depth edges on the training set of the target real domain
(Fig. 2B). These maps are then used to guide the proposed
edge loss in the training of the MDE network (Fig. 2C). Al-
though training on synthetic data and inferring on real data
is known to lead to a performance decrease due to a ‚Äôdomain
gap‚Äô [32, 38], our experiments (Sec. 4) demonstrate that the
DEE network performs very well in practice. In particular,
the DEE network‚Äôs obtained depth edges are signiÔ¨Åcantly
more accurate than the depth edges ‚Äônaturally‚Äô obtained in
the standard MDE training (Fig. 5). This is probably, as
discussed above, due to the low attention paid to estimating
the correct locations of the depth edges in the MDE train-
ing. Importantly, the MDE, which is trained only on the
target dataset, beneÔ¨Åts from the depth edges predictions of
the DEE network despite their imperfection.
Due to the lack of depth edges GT for evaluation in
LIDAR-supervised real-world datasets, we manually anno-
tated depth edges in two evaluation sets of 102 and 50 im-
ages from the KITTI and DDAD datasets, respectively (see
an example in the top of Fig. 6). We show on these newly
10607
(a) (b)
Figure 3. The density of the LIDAR near edges in a partial
set of the KITTI dataset (our proposed KITTI-DE dataset).
Denote the set of all pixels with a distance dto the closest edge as
Pd, and the set of pixels, p, inp2Pdwith LIDAR measurement
asPL
d. (a) The ratio of LIDAR measurements, jPL
dj=jPdj, out of
all pixels in Pd, as a function of d. (b) An example from the KITTI
dataset of a gap in the LIDAR measurments (left of the pole) and
an inÔ¨Åltration of LIDAR measurements from the background to
the pole (right of the pole). For visualization purposes the LIDAR
measurements are dilated.
annotated datasets that our approach generates depth with
signiÔ¨Åcantly more accurate depth edges than the MDE base-
line, while maintaining a similar depth accuracy.
Our main contributions are twofold:
A novel method to improve the localization of depth
edges in MDE methods while preserving their per-pixel
depth accuracy.
A benchmark of human-annotated depth edges to evalu-
ate their quality in sparsely-supervised MDE algorithms.
2. Previous Work
Depth estimators can be trained using full supervision with
LiDAR or other absolute depth measurements [1, 4, 18,
26, 30, 31, 42], by self-supervision using two or more
RGB images [16, 17, 19, 46, 54] or using semi-supervision
[3, 21, 25, 47]. The main objective of these methods is to
reduce the overall mean absolute relative error (ARE), re-
cently achieving state-of-the art (SOTA) result with ARE
close to 5% on the KITTI dataset [1, 18, 31]. However, spe-
ciÔ¨Åc applications such as AR require that the edges of the
predicted depth are also accurate.
To achieve this, various depth estimation models were
trained in a fully-supervised manner using dense depth GT
in datasets such as NYU-2 [41], IBims-1 [24] and Middle-
bury [39]. Dense outdoor depth maps include the small
scale ETH3D [40] dataset and large-scale DIODE [44].
SharpNet [34] improved depth prediction on object edges
in indoor scenes by predicting occluding contours. Their
solution was Ô¨Årst pretrained to predict occluding contours
on synthetic data and then Ô¨Åne-tuned on the indoor NYUv2
dataset to constrain normal, depth and occluding contours.In their subsequent work [33] the authors predicted dis-
placement Ô¨Åelds of pixels with poorly predicted depth val-
ues of dense depth maps. DCTNet [53] boosted the resolu-
tion of depth maps from low-resolution ones using an edge
attention and high-resolution RGB images, where they as-
sume co-occurrence between the texture edges of RGB im-
ages and depth edges. These methods require training on
dense GT collected using expensive short-range laser scan-
ners which are mostly ineffective for outdoor scenes.
Other works utilized the partial consistency between the
edges of semantic classes and depth edges to improve the
depth predictions on these regions. A dedicated loss was
developed [55] which slightly improved the ARE near the
edges of the segmentation masks. However, the edges be-
tween semantic classes do not necesserily match the depth
edges; for example, depth edges between the instances of
the same class (e.g., buildings). [37] and [49] used stereo
images in addition to panoptic segmentation maps to re-
Ô¨Åne depth edges. Recently, [8] attempted to solve a phe-
nomenon of ‚Äôedge-fattening‚Äô by redesigning the triplet loss
for depth estimation. However, the solution is designed for
self-supervised approaches, and their per-pixel accuracy is
low. In contrast, our work improves depth edges without
using additional segmentation maps from the target domain,
while achieving comparable ARE to SOTA methods.
In an attempt to overcome the sparsity limitation of
ground-truth depth maps for outdoor scenes, disparity maps
were generated from the 3D Movies Dataset [36]. However,
stereo matching is a non-trivial task, which is especially in-
accurate around depth edges [36]. Moreover, without know-
ing the exact baseline and other camera characteristics, the
estimated depth maps units cannot be metric-reconstructed
as needed for applications that require absolute depth. Mi-
DAS [36] and DPT [35] were also trained on this dataset,
achieving visually striking depth maps. However, the lack
of absolute scale limits their usage.
In recent works, [29] and [11] introduced methods to
merge two depth maps at different resolutions, trained on
dense depth datasets. The accuracy of [29] was evalu-
ated using order ranking around depth edges, but did not
measure the absolute depth metrics on sparsely-supervised
datasets, while [11] did measure absolute depth metrics but
not the accuracy around depth edges. To the best of our
knowledge we are the Ô¨Årst to consider the problem of depth
edges in sparse LIDAR-based scenes; therefore, no ‚Äônatural‚Äô
candidates for comparison exist. Nevertheless, due to their
impressive depth edges and their similar goal, we compare
our method to [29] and [11] (BoostingDepth and Gradient-
Fusion, respectively) and show their limitations (Sec. 4.4).
3. Method
The Ô¨Çow of our MDE training method can be described in
three steps as illustrated in Fig. 2A-C. In the Ô¨Årst step, the
10608
DEE network is trained on the source synthetic dataset with
the depth edges GT, to predict a probabilistic map of depth
edges for a given RGB image and the corresponding LIDAR
measurement (Fig. 2A and Sec. 3.5). In the second step,
inference is applied on the training set of the target real data
using the trained DEE network, resulting in (approximate)
depth edge labels, ~EGT(Fig. 2B). In the last step we train
the MDE network with an Edge Detection Block (EDB) to
improve the localization of the depth edges (Fig. 2C and
Sec. 3.2). The training is carried out using a straightforward
supervised depth loss with LIDAR as GT, and the proposed
edge loss with ~EGTas approximate GT. In the following
sections we describe our method and the used steps in detail.
3.1. Monocular Depth Estimation
MDE models are commonly trained to regress per-pixel
depth,D(I), from an RGB image, I, given a depth GT,
DGT, inSscales, using a loss function:
Lms
depth =1
SX
sLdepth(Ds(I);Ds
GT); (1)
whereDs(I)andDs
GTindicate the predicted depth and GT
depth in scale s, respectively. For brevity we omit the scale
indexes for the rest of the paper, although, unless explicitly
stated, all losses are multi-scale.
3.2. Edge Detection Block (EDB)
To encourage the model to produce depth edges at the cor-
rect locations, we use a differentiable layer that computes
the per-pixel probability of depth edges, ~E(D(I)), from the
predicted depth map, D(I), whereIis the input RGB im-
age. GivenD(I), the EDB computes the magnitude of the
spatial image gradient, jrD(I)j, and then transforms it into
an ‚Äôedge-ness‚Äô probability score:
~E(D(I)) =sigmoid (jrD(I)j tgrad) (2)
by shifting it with the paramater tgrad and passing it through
a sigmoid function.
In practice, when using the standard image gradient
jrD(I)j=sdD(I)
dx2
+dD(I)
dy2
;
some cyclic gradient patterns may emerge since both
dD(I)=dxanddD(I)=dyare unconstrained (see depiction
in the supplementary material). Therefore, we compute the
gradient as a derivative in the direction perpendicular to
the edge. To this end, the normal direction to the edge is
Ô¨Årst computed from the depth edge GT (estimated using the
DEE network - Sec. 3.5), ~EGT, by:
=atan2 
d~EGT
dy;d~EGT
dx!The derivative in the direction perpendicular to the edge for
every pixel (x;y)is therefore given by:
rDI(x;y) =DI(x+ cos;y+ sin) 
DI(x cos;y sin);
whereDI=D(I)and the coordinates xcosand
ysinare rounded in practice. The predicted edge prob-
ability, ~E(D(I)), is then used by the depth edges loss as
described in the following section.
3.3. Depth Edges Loss
Given the depth edges GT, ~EGT, and the output of the EDB,
~E(D(I)), we use the following loss to encourage depth dis-
continuity at ~EGT:
Ledge(~E(D(I));~EGT) =BBCE
~E(D(I));~EGT
(3)
where theBBCE [50] is the Balanced Binary Cross En-
tropy loss where the positives (edge pixels) and the nega-
tives (non-edge pixels) are reweighted in a standard BCE
loss so they have equal contribution.
3.4. Total Loss
Our MDE model is trained with a linear combination of
the edge loss,Ledge, and the standard depth loss, Lms
depth
(Eq. 1). The total loss is given by
L=Lms
depth(D(I);DGT) +Ledge(~E(D(I));~EGT);
(4)
whereis a parameter to balance between the two losses,
andDGTis the depth GT.
3.5. Depth Edges Estimation (DEE)
Since the actual depth edges GT is unavailable for the target
real data we use the DEE network, which predicts depth
edges, ~E(I;D0), from RGB, I, and LIDAR, D0. We note
that the LIDAR measurements have signiÔ¨Åcant impact on
the performance of the DEE network when given as input
in addition to the RGB (ablation study is presented in the
supplementary material). In order to train the DEE network
we use a synthetic dataset with dense depth and LIDAR that
are available for each RGB image. To extract depth edges
GT,EGT, we use the Canny edge detector [5] on the dense
depth GT. The DEE network is trained with the depth edges
loss as presented in Eq. 3; that is, Ledge(~E(I;D0);EGT).
The predicted depth edges, ~E(I;D0), obtained from
the DEE network are dense with Gaussian-like distribu-
tions. To produce one-pixel wide thin depth edges, we use
two standard edge-detection post-processing operations [5]:
(a) Non-Maximum Suppression (NMS), and (b) hysteresis
(with 0:85and0:9as low and high parameters).
10609
4. Experiments
In the following we describe experiments that demonstrate
the effectiveness of our suggested method and compare it to
the SOTA and other relevant methods.
4.1. Datasets
To train the DEE network we use the GTA-PreSIL dataset
[22], which is rendered from the Grand Theft Auto (GTA)
video game. It consists of 44K images for training and
7K images for validation and testing, where each image
has a corresponding dense depth GT. To generate the GT
for the depth edges, we use Canny edge detector [5] with
low and high thresholds of 4and5meters, respectively. We
note that the DEE network could have been trained with
Virtual KITTI [13], probably achieving better performance
on KITTI (details below) due to a smaller domain gap be-
tween the datasets. However, since we aim to demonstrate a
general-purpose DEE network, GTA-PreSIL was the better
choice since its structures and styles are signiÔ¨Åcantly differ-
ent from the target real domains.
We demonstrate our method on three different datasets:
Synscapes [48], KITTI [43] and DDAD [20]. The data split
for the highly realistic Synscapes dataset, where each image
has a corresponding dense depth, is similar to [45]. We have
chosen this dataset since (1) it provides perfect depth edges
GT to evaluate our method, and (2) it has dense depth that
allows us to fully analyze the impact of our method on the
per-pixel depth accuracy. For KITTI we use the Eigen split
[12], and for the DDAD dataset we use the ofÔ¨Åcial split.
KITTI & DDAD Depth Edges Evaluation Sets: To en-
able direct evaluation of the depth edges of MDE networks
on the KITTI and DDAD datasets, we introduce the KITTI
Depth Edges (KITTI-DE) and the DDAD Depth Edges
(DDAD-DE) evaluation sets which consist of depth edges
annotations of 102 images from the KITTI dataset and 50
images from DDAD dataset, respectively. To ease the man-
ual annotation process, we started from the corresponding
images of the Semantic and Instance Segmentation eval-
uation benchmark of KITTI [2] and the validation set of
DDAD. To get an initial approximation of the depth edges
we derive the edges of the instance segmentation masks for
the relevant classes, and the semantic segmentation maps
for the other classes. Then, false depth edges were manually
removed (e.g. car wheels point of contact on the road), and
missing depth edges were added (e.g., edges between build-
ing). See the supplementary material for additional details
of the annotation process. We note that the objective of the
proposed depth edges evaluation sets is to complement the
standard depth evaluation sets, not to replace it, thus allow-
ing to inspect another important aspect of MDE.
RGB Baseline Ours RGB Baseline Ours
Figure 4. Examples of depth predictions in the KITTI-DE
dataset. The depth predictions for the baseline and our method
correspond to Packnet-SAN and Packnet-SAN+EL, respectively.
4.2. Metrics
To evaluate the performance of the MDE networks, we use
some of the common per-pixel MDE depth metrics, as well
as metrics to evaluate the quality of the edges of the pre-
dicted depth. Both depth and depth edges are evaluated in
the bottom 60% of the image where LiDAR is commonly
available, as deÔ¨Åned in Garg et al. [14], for all datasets.
Depth metrics: We use the common Absolute Relative
Error (ARE) to measure the per-pixel depth error, given by
ARE (~d;d) =j~d dj=d, where ~danddare the predicted
depth and GT depth, respectively. Note that the ARE is
computed only over the sparse LIDAR measurements in the
KITTI and the DDAD datasets, which are very sparse near
edges (Fig. 3), making the ARE a poor metric to measure
the quality of the 2D localization of depth edges.
Depth edges metrics: To evaluate the edge quality of the
predicted depth, one option is the ORD metric [49], which
measures the percentage of order disagreements between
pairs of depth points in the predicted depth against the GT
depth. We argue that this metric suffers from a similar prob-
lem as the ARE - since LIDAR measurements are sparse,
many possible 2D edges can obtain the same ORD value.
Therefore, in addition to the ORD metric, we use the
most common metric from the edge detection literature,
BSDS [28]. It consists of Ô¨Ånding a bijective matching,
10610
(~E;E), between the edge pixels of the predicted depth,
~E, and the edge pixels of the GT, E. Each match (~e;e)2
(~E;E)between edge pixel, ~e2~E, and edge pixel GT,
e2E, is obtained such that the locations of the pix-
els are similar; that is, jj~e;ejj2tewhereteis a small
threshold (we use te= 2). The precision, PR(~E;E), and
recall,RE(~E;E), are then given by j(~E;E)j=j~Ej, and
j(~E;E)j=jEj, respectively.
In practice, ~Eis obtained by extracting edges from the
predicted depth dusing Canny edge detector with low and
high thresholds of thlowandthhigh, respectively. Since
there is an inherent tradeoff between the precision and re-
call, we invoke multiple runs of a Canny edge detector with
different parameters to obtain a precision-recall curve. Fi-
nally, to quantify the depth edges quality in a single number,
we compute the Area Under the Curve (AUC). Since a large
part of the graph is uncovered by all MDE algorithms, we
report both the AUC for a partial representative range and
for all range ([0,1]). Note that the edge metrics used in the
I-BIMS1 dataset [24], acc
DBE andcomp
DBE , deÔ¨Åne a matching
between the predicted edges and the GT edges, similarly
to our AUC metric. However, they suffer from a signiÔ¨Å-
cant limitation in comparison to our AUC metric since their
matching is not bijective and allows multiple-to-multiple
matching. For example, broken and non-consecutive pre-
dicted edge can be perfectly matched to a consecutive GT
edge, and yield the best score under these metrics.
4.3. Results
We use our method with three SOTA sparsely-supervised
MDE methods as baselines: Packnet-SAN [18], AdaBins
[4] and PixelFormer [1]. For all methods we use the pub-
licly available code and weights and resume training for ten
more epochs with our proposed loss (Sec. 3.3), and with-
out it as a baseline for the comparison. In the following
sections, we refer to the original methods by Packnet-SAN,
AdaBins and PixelFormer, and refer to these methods with
the addition of our edge loss (Sec. 3) as fmethod nameg
+ EL (ours), where EL is abbreviation for ‚ÄôEdge Loss‚Äô.
We further show on the supplementary material that train-
ing simoultaneously (or sequentially) on both source (GTA-
PreSIL) and target (KITTI or DDAD) datasets is inferior to
our method. We note that, since Packnet-SAN exhibits bet-
ter depth edges than both other methods, we use it as our
main baseline in all experiments.
4.3.1 The KITTI dataset
In Fig. 4 we present an image from the KITTI-DE dataset
alongside the depth predictions of the baselines and our
method. It can be seen, for example on the crops in the bot-
tom row, that our method has considerably more accurate
depth edges than the baseline in many parts of the scene.
The precision-recall curves for the KITTI-DE dataset,which are presented in the leftside of Fig. 5, are computed
for the baselines and competing methods (solid curves) and
for our method with the edge loss (non-red dashed curves).
The precision-recall curves of our method in comparison
to the corresponding baselines, when compared using the
same precision (same xcoordinate), have a signiÔ¨Åcantly
higher recall. In Tab. 1, the AUC metric for the edge qual-
ity is presented, where our method achieves an AUC of
61:87%,53:47% and46:23% in comparison to the Packnet-
SAN, AdaBins and PixelFormer baselines that achieve an
AUC of 47:56%,41:23% and32:79%, respectively, indi-
cating a relative improvement between 30% and40%.
The quality of the per-pixel depth is also evaluated on the
KITTI-DE dataset, and the ARE is presented in Tab. 1. The
Packnet-SAN, AdaBins and PixelFormer baselines achieve
an ARE of 3:45%,3:14% and3:00%, respectively, in com-
parison to our Packnet-SAN + EL, AdaBins + EL and Pix-
elFormer + EL that achieve an ARE of 3:61%,3:11% and
2:94%, respectively, which indicates a comparable perfor-
mance. We also evaluate the methods on the standard KITTI
test set (Tab. 1), where only the per-pixel depth quality can
be estimated. The Packnet-SAN, AdaBins and PixelFormer
baselines achieve an ARE of 6:17%,6:28% and5:46%, re-
spectively, in comparison to our Packnet-SAN + EL, Ad-
aBins + EL and PixelFormer + EL that achieve an ARE
of6:50%,6:21% and5:59%, respectively, which also in-
dicates a comparable performance. We note that, interest-
ingly, the quality of edges is not highly correlative to the
per-pixel depth quality, which demonstrates the need for the
depth edges annotated datasets we propose.
4.3.2 The DDAD dataset
In Fig. 6 we present an image from the DDAD-DE dataset,
alongside the depth predictions of Packnet-SAN and our
method with Packnet-SAN as baseline. Furthermore, the
depth edges (extracted similarly to KITTI), which are laid
on top of the RGB images, are presented in the top row.
It can be seen that our method has considerably more ac-
curate edges that Ô¨Åt more tightly on objects‚Äô silhouette in
comparison to the baseline. Moreover, some missing edges
are added and some clear false positives are discarded in our
method in comparison to the baseline.
The precision-recall curves for the DDAD-DE dataset
are presented in the right side of Fig. 5, where our method,
when compared with the same precision (same xcoordi-
nate), has signiÔ¨Åcantly higher recall than the Packnet-SAN
baseline. In Tab. 2, the AUC metric for the edge quality is
presented, where our method achieves an AUC of 48:32%
in comparison to Packnet-SAN that achieves an AUC of
31:52%, indicating around 50% of relative improvement.
The quality of the per-pixel depth is also evaluated on the
DDAD-DE dataset, and the ARE is presented in Tab. 2. The
Packnet-SAN baseline and our methods achieve an ARE of
10611
KITTI-DE DDAD-DE
Figure 5. Precision and recall of the depth edges on the KITTI-DE and DDAD-DE evaluation sets . Each point on the graphs of the
MDE methods is generated with different parameters of the Canny edge detector. Each of the points on the graphs that correspond to the
DEE method is generated by thresholding the depth edge probability in the range (0;1).
Method KITTI-DE KITTI test
AUC (edges)" ORD#ARE#<1:25"ORD#ARE#<1:25"
Packnet-SAN 47.56% (39.40%) 7.68% 3.45% 98.66% 12.40% 6.17% 95.39%
Packnet-SAN + BoostingDepth (O) 46.04% (37.07%) 10.35% 9.32% 88.90% 12.63% 11.10% 86.41%
Packnet-SAN + BoostingDepth (K) 36.19% (31.27%) 9.47% 7.24% 93.62% 11.45% 8.33% 91.99%
Packnet-SAN + GradientFusion 44.51% (34.10%) 9.18% 5.93% 95.66% 11.15% 7.18% 94.17%
Packnet-SAN + EL (ours) 61.87% (49.02%) 7.75% 3.61% 98.53% 12.48% 6.50% 95.06%
AdaBins 41.23% (34.11%) 7.69% 3.14% 98.78% 10.14% 6.28% 95.85%
AdaBins + EL (ours) 53.47% (44.00%) 7.64% 3.11% 98.79% 10.13% 6.21% 95.87%
PixelFormer 32.79% (26.44%) 7.47% 3.00% 98.79% 7.56% 5.45% 96.98%
PixelFormer + EL (ours) 46.23% (35.33%) 7.53% 2.94% 98.80% 7.58% 5.59% 96.72%
Table 1. Results on the KITTI dataset. The AUC is given for the range where at least one MDE method has valid measurement:
[0.12,0.65]. In parentheses we also report the AUC of the full [0,1] range. In BoostingDepth, O is for the original training (dense data) by
the authors, and K is for our training (KITTI data).
8:89% and8:99%, respectively, which indicates a compa-
rable performance. On the standard DDAD evaluation set
(Tab. 2), the ARE of the Packnet-SAN and our method are
9:49% and10:0%, respectively, indicating, a 5%decrease
in per-pixel depth accuracy of our method.
4.3.3 The Synscapes dataset
We present the results of an additional experiment on the
Synscapes dataset in Tab. 2 and in the supp. material, where
we show that an increase in the depth edges accuracy yields
an increase in the per-pixel depth accuracy (e.g., ARE) for
dense depth GT. It further suggests that the slight decrease
in the per-pixel depth accuracy, which is sometimes ob-
served in KITTI and DDAD, might be due to the sparseness
of their depth GT, especially near depth edges.
4.4. Comparison to Merging Methods [11, 29]
To the best of our knowledge there are no other methods to
improve the depth edges of sparsely-supervised MDE meth-
ods; however, BoostingDepth [29] and GradientFusion [11]share a similar aim to ours, even though these methods were
designed for indoor scenes with dense GT. To this end, we
use the predictions of Packnet-SAN with BoostingDepth‚Äôs
original depth merger for comparison with our method. Ad-
ditionally, we train the depth merger on the KITTI dataset
for a fair comparison (see details in the supplementary ma-
terial). The fusion-net in GradientFusion was trained solely
on the high-res HR-WSI dataset [49]. Their method is based
on a preprocessing of HR-WSI‚Äôs images with a guided Ô¨Ål-
ter to produce high and low resolutions that allow the self-
supervised training. We therefore use their original fusion-
net that was used for all datasets and MDE methods. Two
experiments are presented: one with LeRes [52], which was
the main backbone for GradientFusion (see supp. material),
and one with Packnet-SAN.
For both BoostingDepth (with the original weights) and
GradientFusion, the AUC metric presented in Tab. 1 and
Fig. 5 shows a slightly worse performance than the Packnet-
SAN baseline, and a signiÔ¨Åcantly worse performance than
Packnet-SAN with our method. One possible reason for the
10612
RGB Baseline Ours
Figure 6. Examples of depth predictions of Packnet-SAN and Packnet-SAN + EL (ours) on images from the DDAD-DE dataset.
The top row, from left to right: RGB, depth edges GT (manually annotated), depth edges extracted (using Canny) from the baseline and
our depth, respectively. The bottom row, from left to right: the predicted depth of the baseline and our method, respectively, followed by
two zoom-in crops with the RGB, baseline and our predicted depth, respectively.
Method DDAD-DE DDAD test Synscapes
AUC (edges)" ORD#ARE#1" ORD ARE1 AUC ORD ARE1
Packnet-SAN 31.52% (23.32%) 8.03% 8.89% 91.6% 8.95% 9.49% 90.7% 61.17% 30.8% 5.43% 96.2%
Packnet-SAN 48.32% (32.29%) 8.38% 8.99% 91.4% 9.43% 10.0% 89.5% 65.38% 21.1% 4.85% 96.5%
+ EL (ours)
Table 2. Results on the DDAD and Synscapes datasets. The AUC (for DDAD) is given for the range where at least one MDE method
has valid measurement: [0.14,0.37]. In parentheses we also report the AUC of the full [0,1] range.
performance is probably due to the domain gap between the
datasets used to train the depth mergers and KITTI. Addi-
tional reasons for BoostingDepth might be: (i) The depth
merger can process only square images, which deviates sig-
niÔ¨Åcantly from KITTI‚Äôs aspect ratio, and (ii) It is highly sen-
sitive to the network‚Äôs Receptive Field (RF), where Packnet-
SAN has a large RF of 1028, close to the size of the im-
age. The ARE performance is signiÔ¨Åcantly worse than both
other methods, probably due to the usage of the GAN-based
pix2pix [23] to train the depth merger which is known to
create results that ‚Äôlook‚Äô realistic but not necessarily accu-
rate. The performance of GradientFusion for KITTI is low
since self-supervision using the guided Ô¨Ålter strongly de-
pends on image resolution (as stated in their paper also),
which is low in datasets like KITTI.
BoostingDepth with the depth merger that was trained
on KITTI presents worse edges (AUC) than the original
weights, but better ARE. We hypothesize that the basic
assumption of BoostingDepth, in which an MDE network
produces more Ô¨Åne details in high resolution (but worse
overall shape) breaks for LIDAR-based scenes due to the
lack of depth GT near depth edges. See supp. material for a
more detailed discussion, visual results and training details.4.5. Application: AR occlusions
We present an experiment in rendering virtual objects for
AR applications. We use the predicted depth of the Packnet-
SAN baseline and our method to compute the occluded
parts of the virtual objects from the point of view of the
camera. In Fig. 1b three animated characters are planted
in the scene. To observe the difference in the depth edges
accuracy, zoom-in on the sides of the vehicle and on the ‚Äôno-
entry‚Äô post. The occlusions of the character in our method
seems much more realistic than the baseline.
5. Conclusion
We have presented a method to improve the localization
of the depth edges in sparsely-supervised MDE methods,
while preserving the per-pixel depth accuracy. The method
is based on the observation that detecting the location of the
depth edges can be learned effectively from synthetic data.
To evaluate our method on real data, we introduced two
depth edges evaluation sets for the KITTI and the DDAD
datasets by manual annotation. The proposed method can
be further improved by considering the images (and LI-
DAR) of the target dataset (e.g., KITTI) in the training pro-
cedure, possibly using methods from the unsupervised do-
main adaptation literature.
10613
References
[1] Ashutosh Agarwal and Chetan Arora. Attention attention ev-
erywhere: Monocular depth prediction with skip attention.
InProceedings of the IEEE/CVF Winter Conference on Ap-
plications of Computer Vision , pages 5861‚Äì5870, 2023. 3,
6
[2] Hassan Alhaija, Siva Mustikovela, Lars Mescheder, Andreas
Geiger, and Carsten Rother. Augmented reality meets com-
puter vision: EfÔ¨Åcient data generation for urban driving
scenes. International Journal of Computer Vision (IJCV) ,
2018. 5
[3] Ali Jahani Amiri, Shing Yan Loo, and Hong Zhang. Semi-
supervised monocular depth estimation with left-right con-
sistency using deep neural network. In 2019 IEEE Inter-
national Conference on Robotics and Biomimetics (ROBIO) ,
pages 602‚Äì607. IEEE, 2019. 3
[4] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.
Adabins: Depth estimation using adaptive bins. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 4009‚Äì4018, 2021. 3, 6
[5] John Canny. A computational approach to edge detection.
IEEE Transactions on pattern analysis and machine intelli-
gence , (6):679‚Äì698, 1986. 4, 5
[6] Ang Cao, Chris Rockwell, and Justin Johnson. Fwd: Real-
time novel view synthesis with forward warping and depth.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 15713‚Äì15724, 2022.
1, 2
[7] Chuangrong Chen, Xiaozhi Chen, and Hui Cheng. On the
over-smoothing problem of cnn based disparity estimation.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 8997‚Äì9005, 2019. 2
[8] Xingyu Chen, Ruonan Zhang, Ji Jiang, Yan Wang, Ge Li,
and Thomas H Li. Self-supervised monocular depth estima-
tion: Solving the edge-fattening problem. In Proceedings of
the IEEE/CVF Winter Conference on Applications of Com-
puter Vision , pages 5776‚Äì5786, 2023. 3
[9] Xuelian Cheng, Yiran Zhong, Yuchao Dai, Pan Ji, and Hong-
dong Li. Noise-aware unsupervised deep lidar-stereo fusion.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 6339‚Äì6348, 2019. 2
[10] Diogo Santos Ortiz Correa, Diego Fernando Sciotti, Mar-
cos Gomes Prado, Daniel Oliva Sales, Denis Fernando Wolf,
and Fernando Santos Os ¬¥orio. Mobile robots navigation in
indoor environments using kinect sensor. In 2012 Second
Brazilian Conference on Critical Embedded Systems , pages
36‚Äì41. IEEE, 2012. 1
[11] Yaqiao Dai, Renjiao Yi, Chenyang Zhu, Hongjun He, and
Kai Xu. Multi-resolution monocular depth map fusion by
self-supervised gradient-based composition. In Proceedings
of the AAAI Conference on ArtiÔ¨Åcial Intelligence , volume 37,
pages 488‚Äì496, 2023. 3, 7
[12] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map
prediction from a single image using a multi-scale deep net-
work. Advances in neural information processing systems ,
27, 2014. 5
[13] Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora
Vig. Virtual worlds as proxy for multi-object tracking anal-ysis. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 4340‚Äì4349, 2016. 5
[14] Ravi Garg, Vijay Kumar Bg, Gustavo Carneiro, and Ian Reid.
Unsupervised cnn for single view depth estimation: Geom-
etry to the rescue. In European conference on computer vi-
sion, pages 740‚Äì756. Springer, 2016. 5
[15] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In 2012 IEEE conference on computer vision and pat-
tern recognition , pages 3354‚Äì3361. IEEE, 2012. 1, 2
[16] Cl ¬¥ement Godard, Oisin Mac Aodha, Michael Firman, and
Gabriel J Brostow. Digging into self-supervised monocular
depth estimation. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 3828‚Äì3838,
2019. 3
[17] Ariel Gordon, Hanhan Li, Rico Jonschkowski, and Anelia
Angelova. Depth from videos in the wild: Unsupervised
monocular depth learning from unknown cameras. In Pro-
ceedings of the IEEE/CVF International Conference on
Computer Vision , pages 8977‚Äì8986, 2019. 3
[18] Vitor Guizilini, Rares Ambrus, Wolfram Burgard, and
Adrien Gaidon. Sparse auxiliary networks for uniÔ¨Åed
monocular depth prediction and completion. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 11078‚Äì11088, 2021. 1, 3, 6
[19] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raven-
tos, and Adrien Gaidon. 3d packing for self-supervised
monocular depth estimation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 2485‚Äì2494, 2020. 3
[20] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raven-
tos, and Adrien Gaidon. 3d packing for self-supervised
monocular depth estimation. In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2020. 5
[21] Akhil Gurram, Ahmet Faruk Tuna, Fengyi Shen, Onay Ur-
falioglu, and Antonio M L ¬¥opez. Monocular depth estimation
through virtual-world supervision and real-world sfm self-
supervision. IEEE Transactions on Intelligent Transporta-
tion Systems , 2021. 3
[22] Braden Hurl, Krzysztof Czarnecki, and Steven Waslander.
Precise synthetic image and lidar (presil) dataset for au-
tonomous vehicle perception. In 2019 IEEE Intelligent Ve-
hicles Symposium (IV) , pages 2522‚Äì2529. IEEE, 2019. 2,
5
[23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adver-
sarial networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1125‚Äì1134,
2017. 8
[24] Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, and
Marco Korner. Evaluation of cnn-based single-image depth
estimation methods. In Proceedings of the European Con-
ference on Computer Vision (ECCV) Workshops , pages 0‚Äì0,
2018. 3, 6
[25] Yevhen Kuznietsov, Jorg Stuckler, and Bastian Leibe. Semi-
supervised deep learning for monocular depth map predic-
tion. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 6647‚Äì6655, 2017. 3
[26] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and
10614
Il Hong Suh. From big to small: Multi-scale local planar
guidance for monocular depth estimation. arXiv preprint
arXiv:1907.10326 , 2019. 3
[27] Marcio C de F Macedo and Antonio L Apolinario. Occlu-
sion handling in augmented reality: Past, present and future.
IEEE Transactions on Visualization and Computer Graph-
ics, 2021. 1, 2
[28] David R Martin, Charless C Fowlkes, and Jitendra Ma-
lik. Learning to detect natural image boundaries using lo-
cal brightness, color, and texture cues. IEEE transactions on
pattern analysis and machine intelligence , 26(5):530‚Äì549,
2004. 5
[29] S Mahdi H Miangoleh, Sebastian Dille, Long Mai, Sylvain
Paris, and Yagiz Aksoy. Boosting monocular depth estima-
tion models to high-resolution via content-adaptive multi-
resolution merging. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
9685‚Äì9694, 2021. 1, 3, 7
[30] Chao Ning and Hongping Gan. Trap attention: Monocu-
lar depth estimation with manual traps. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5033‚Äì5043, 2023. 3
[31] Luigi Piccinelli, Christos Sakaridis, and Fisher Yu. idisc: In-
ternal discretization for monocular depth estimation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 21477‚Äì21487, 2023. 3
[32] Koutilya PNVR, Hao Zhou, and David Jacobs. Sharingan:
Combining synthetic and real data for unsupervised geome-
try estimation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 13974‚Äì
13983, 2020. 2
[33] Michael Ramamonjisoa, Yuming Du, and Vincent Lep-
etit. Predicting sharp and accurate occlusion boundaries in
monocular depth estimation using displacement Ô¨Åelds. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 14648‚Äì14657, 2020. 3
[34] Michael Ramamonjisoa and Vincent Lepetit. Sharpnet: Fast
and accurate recovery of occluding contours in monocular
depth estimation. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision Workshops , pages
0‚Äì0, 2019. 3
[35] Ren ¬¥e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-
sion transformers for dense prediction. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 12179‚Äì12188, 2021. 3
[36] Ren ¬¥e Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. IEEE transactions on pattern analysis and machine
intelligence , 2020. 3
[37] Faraz Saeedan and Stefan Roth. Boosting monocular depth
with panoptic segmentation maps. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer
Vision , pages 3853‚Äì3862, 2021. 3
[38] Swami Sankaranarayanan, Yogesh Balaji, Arpit Jain,
Ser Nam Lim, and Rama Chellappa. Learning from synthetic
data: Addressing domain shift for semantic segmentation. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 3752‚Äì3761, 2018. 2[39] Daniel Scharstein, Heiko Hirschm ¬®uller, York Kitajima,
Greg Krathwohl, Nera Ne Àási¬¥c, Xi Wang, and Porter West-
ling. High-resolution stereo datasets with subpixel-accurate
ground truth. In German conference on pattern recognition ,
pages 31‚Äì42. Springer, 2014. 3
[40] Thomas Schops, Johannes L Schonberger, Silvano Galliani,
Torsten Sattler, Konrad Schindler, Marc Pollefeys, and An-
dreas Geiger. A multi-view stereo benchmark with high-
resolution images and multi-camera videos. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 3260‚Äì3269, 2017. 3
[41] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
Fergus. Indoor segmentation and support inference from
rgbd images. In European conference on computer vision ,
pages 746‚Äì760. Springer, 2012. 1, 3
[42] Minsoo Song, Seokjae Lim, and Wonjun Kim. Monocular
depth estimation using laplacian pyramid-based depth resid-
uals. IEEE transactions on circuits and systems for video
technology , 31(11):4381‚Äì4393, 2021. 3
[43] Jonas Uhrig, Nick Schneider, Lukas Schneider, Uwe Franke,
Thomas Brox, and Andreas Geiger. Sparsity invariant cnns.
InInternational Conference on 3D Vision (3DV) , 2017. 5
[44] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo,
Haochen Wang, Falcon Z Dai, Andrea F Daniele, Moham-
madreza Mostajabi, Steven Basart, Matthew R Walter, et al.
Diode: A dense indoor and outdoor depth dataset. arXiv
preprint arXiv:1908.00463 , 2019. 3
[45] Yufeng Wang, Yi-Hsuan Tsai, Wei-Chih Hung, Wenrui Ding,
Shuo Liu, and Ming-Hsuan Yang. Semi-supervised multi-
task learning for semantics and depth. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer
Vision , pages 2505‚Äì2514, 2022. 5
[46] Jamie Watson, Oisin Mac Aodha, Victor Prisacariu, Gabriel
Brostow, and Michael Firman. The temporal opportunist:
Self-supervised multi-frame monocular depth. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 1164‚Äì1174, 2021. 3
[47] Felix Wimbauer, Nan Yang, Lukas V on Stumberg, Niclas
Zeller, and Daniel Cremers. Monorec: Semi-supervised
dense reconstruction in dynamic environments from a sin-
gle moving camera. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
6112‚Äì6122, 2021. 3
[48] Magnus Wrenninge and Jonas Unger. Synscapes: A pho-
torealistic synthetic dataset for street scene parsing. arXiv
preprint arXiv:1810.08705 , 2018. 5
[49] Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe Lin,
and Zhiguo Cao. Structure-guided ranking loss for single im-
age depth prediction. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
611‚Äì620, 2020. 1, 3, 5, 7
[50] Saining Xie and Zhuowen Tu. Holistically-nested edge de-
tection. In Proceedings of the IEEE international conference
on computer vision , pages 1395‚Äì1403, 2015. 4
[51] Yan Xu, Xinge Zhu, Jianping Shi, Guofeng Zhang, Hujun
Bao, and Hongsheng Li. Depth completion from sparse li-
dar data with depth-normal constraints. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 2811‚Äì2820, 2019. 2
10615
[52] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus,
Long Mai, Simon Chen, and Chunhua Shen. Learning to
recover 3d scene shape from a single image. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 204‚Äì213, 2021. 7
[53] Zixiang Zhao, Jiangshe Zhang, Shuang Xu, Zudi Lin, and
Hanspeter PÔ¨Åster. Discrete cosine transform network for
guided depth map super-resolution. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5697‚Äì5707, 2022. 3
[54] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G
Lowe. Unsupervised learning of depth and ego-motion from
video. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 1851‚Äì1858, 2017. 3
[55] Shengjie Zhu, Garrick Brazil, and Xiaoming Liu. The edge
of depth: Explicit constraints between segmentation and
depth. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 13116‚Äì13125,
2020. 1, 3
10616
