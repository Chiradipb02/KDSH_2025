COSAL PURE: Learning Concept from Group Images for
Robust Co-Saliency Detection
Jiayi Zhu1, Qing Guo3†, Felix Juefei-Xu2, Yihao Huang4, Yang Liu4, Geguang Pu1,5†
1East China Normal University, China2New York University, USA
3IHPC & CFAR, Agency for Science, Technology and Research, Singapore
4Nanyang Technological University, Singapore
5Shanghai Industrial Control Safety Innovation Tech. Co., Ltd, China
Concept Learning
Concept-guided Purification
Co-salient Object Detectors
c
 T2I DiffusionGroup Images Containing some Adv. Examples Purified Group ImagesDetection Results Detection Results
CosalPure Validation of c Learned concept
Figure 1. Examples of our method C OSAL PURE and comparative results before and after purification. C OSAL PURE comprises two modules: group-image
concept learning and concept-guided purification. Firstly, the concept learning module inputs a group of images that contain some adversarial cases and
obtain their shared co-salient semantic information ( i.e., the learned concept), denoted as c. We can validate the effectiveness of the learned cthrough
the visualization via a text-to-image (T2I) diffusion model. Secondly, steered by the previously learned concept, we employ certain diffusion generation
techniques to purify the entire group of images. Before our purification, the co-salient object detection results are poor, but after purification, the detection
results are satisfactory. Please enlarge to see more details.
Abstract
Co-salient object detection (CoSOD) aims to identify
the common and salient (usually in the foreground) regions
across a given group of images. Although achieving sig-
nificant progress, state-of-the-art CoSODs could be easily
affected by some adversarial perturbations, leading to sub-
stantial accuracy reduction. The adversarial perturbations
can mislead CoSODs but do not change the high-level se-
mantic information ( e.g., concept) of the co-salient objects.
In this paper, we propose a novel robustness enhancement
framework by first learning the concept of the co-salient ob-
jects based on the input group images and then leverag-
ing this concept to purify adversarial perturbations, which
are subsequently fed to CoSODs for robustness enhance-
†Geguang Pu (ggpu@sei.ecnu.edu.cn) and Qing Guo (ts-
ingqguo@ieee.org) are corresponding authors.ment. Specifically, we propose COSAL PURE containing two
modules, i.e., group-image concept learning and concept-
guided diffusion purification. For the first module, we adopt
a pre-trained text-to-image diffusion model to learn the con-
cept of co-salient objects within group images where the
learned concept is robust to adversarial examples. For the
second module, we map the adversarial image to the latent
space and then perform diffusion generation by embedding
the learned concept into the noise prediction function as an
extra condition. Our method can effectively alleviate the in-
fluence of the SOTA adversarial attack containing different
adversarial patterns, including exposure and noise. The ex-
tensive results demonstrate that our method could enhance
the robustness of CoSODs significantly. The project is avail-
able at https://v1len.github.io/CosalPure/.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
3669
1. Introduction
Co-salient object detection (CoSOD) plays a pivotal role
in visual information analysis, aiming to identify and ac-
centuate common and salient objects across a set of images
[33]. This study area, crucial for applications like image
segmentation and object recognition, has witnessed con-
siderable advancement with the advent of neural network-
based methodologies. These methods excel in discerning
shared saliency cues among images, offering a significant
leap over traditional saliency detections [20]. However,
their robustness is severely tested under adverse conditions,
such as adversarial attacks and various image common cor-
ruption, including but not limited to motion blur [13].
The susceptibility of CoSOD methods to adversarial per-
turbations, such as those introduced by Jadena [12], poses
a significant challenge. These perturbations, while not al-
tering the high-level semantic information of images, can
drastically reduce the accuracy of co-salient object detec-
tion. The disparity between the corrupted image’s saliency
map and the ground truth, as a result of these attacks, high-
lights a critical vulnerability in current CoSOD approaches.
Currently, there are indeed methods aimed at defending
against adversarial attacks, such as DiffPure, which em-
ploys a noise addition and denoising strategy to eliminate
perturbations. However, when restoring the image, Diff-
Pure does not take into account the identity of the object
within the image ( i.e., without object-specific information).
As a result, the restored images produced by DiffPure may
contain artifacts that are artificially generated. These arti-
facts will affect the detection results of the CoSOD method.
To fill this gap, this work introduces a novel robustness
enhancement framework, C OSAL PURE. The intuitive idea
is to first learn a concept from the group images and then
use it to guide the data purification based on text-to-image
(T2I) diffusion. The ‘concept’ means the high-level seman-
tic information of co-salient objects in the group images and
falls within the text’s latent space. Specifically, this innova-
tive approach comprises two meticulously designed mod-
ules: group-image concept learning and concept-guided dif-
fusion purification. The first module focuses on learning the
concept of co-salient objects from group images, demon-
strating robustness and resilience to adversarial examples.
The second module strategically maps adversarial images
into a latent space, following which diffusion generation
techniques, steered by the previously learned concept, are
employed to purify these images effectively.
As shown in the left panel of Fig. 1, when a group of im-
ages containing some adversarial examples is passed into
co-salient object detectors, the detection results are poor.
We first apply the concept learning module to obtain the
shared co-salient semantic information c(i.e., the learned
concept). The bottom right corner of Fig. 1 shows the vi-
sualization results of cvia a T2I diffusion model. It is evi-dent that the semantic information in the visualized images
aligns with the original group images, demonstrating the ef-
fectiveness of the concept learning module. Secondly, we
utilize the just-proven effective learned concept cto guide
the purification. From the right panel of Fig. 1, we can ob-
serve that the purified group images via C OSAL PURE ex-
hibit satisfactory performance in the CoSOD task.
Extensive experimental results substantiate the effective-
ness of C OSAL PURE. C OSAL PURE stands as a robust,
concept-driven solution, paving the way for more reliable
and accurate co-salient object detection in an era where im-
age manipulation and corruption are increasingly prevalent.
2. Related Work
Co-salient object detection. Different from single-image
saliency detection [1, 6, 18, 20, 27, 28], the goal of co-
saliency detection is to detect common salient objects in a
group of images [9, 16, 17, 29, 34, 35], evolving from early
feature-based approaches to sophisticated deep learning and
semantic-driven methods. Deciphering correspondences
among co-salient objects across multiple images is pivotal
for co-saliency detection. This challenge can be effec-
tively tackled through optimization-based methods [3, 19],
machine learning-based models [5, 31], and deep neural
networks [29, 34, 35]. GICD [35] employs a gradient-
induced mechanism that pays more attention to discrimi-
native convolutional kernels which helps to locate the co-
salient regions. GCAGC [34] presents an adaptive graph
convolutional network with attention graph clustering for
co-saliency detection.
Adversarial attack for co-salient object detection.
Jadena [12] is an adversarial attack that jointly tunes the
exposure and additive perturbations, which can drastically
reduce the accuracy of co-salient object detection.
Text-to-image diffusion generation model. The popular-
ity of Text-to-Image (T2I) generation [30] is propelled by
diffusion models [7, 15, 23], necessitating training on ex-
tensive text and image paired datasets like LAION-5B [25].
The adeptly trained model demonstrates proficiency in gen-
erating diverse and lifelike images based on user-specific
input text prompts, realizing T2I generation. T2I personal-
ization [10, 24] is geared towards steering a diffusion-based
T2I model to generate innovative concepts.
Diffusion-based image purification methods. DiffPure
[22] is a notable approach in the field of image processing,
specifically designed to enhance the robustness of images
against adversarial attacks. It employs a strategy of intro-
ducing controlled noise via the forward stochastic differen-
tial equation (SDE) [21] and subsequently denoising the im-
age via the reverse SDE to counteract adversarial perturba-
tions. While effective in reducing these perturbations, it is
noteworthy that DiffPure does not explicitly consider object
semantics during the image restoration process. Diffusion-
3670
Driven Adaptation (DDA) [11] is a test-time adaptation
method that improves model accuracy on shifted target data
by updating inputs through a diffusion model [15], effec-
tively avoiding domain-wise re-training.
3. Preliminaries and Motivation
3.1. Co-salient Object Detection (CoSOD)
We have a group of images I={Ii∈RH×W×3}N
i=1
that contain Nimages, and these images have common
salient objects. We denote a CoSOD method as C OSOD(·),
taking Ias input and predicting Nsalient maps,
S={Si}N
i=1=COSOD(I), (1)
where Si∈RH×Wis a binary map ( i.e., the saliency map)
indicating the salient region of the i-th image Ii. We show
an example of co-saliency detection results in Fig. 1.
3.2. Robust Issues of CoSOD
However, at times, part of the acquired images may be
of low quality ( i.e., corrupted by some degradation), which
will affect the robustness of CoSOD methods. In partic-
ular, [12] proposes the joint adversarial noise and exposure
attack that can reduce the detection accuracy of state-of-the-
art CoSODs significantly. To be specific, within the entire
group, there are Mimages that have been added adversarial
perturbations, denoted as {I′
j}M
j=1, while the remaining are
considered as clean images {Ik}N−M
k=1. In this scenario, we
can reformulate Eq. (1) as
S′={S′
i}N
i=1=CoSOD ({I′
j}M
j=1∪ {Ik}N−M
k=1).(2)
The difference between Sfrom Eq. (1) and S′indicates the
robustness of the CoSOD method. Previous research find-
ings indicate that existing CoSOD methods are susceptible
to the influence of anomalous data ( e.g., adversarial noise
and exposure) [12]. Note that the degraded images ( e.g.,
{I′
j}M
j=1) may not only affect the saliency maps of them-
selves but also impact the saliency maps of clean images.
Therefore, to enhance the robustness of CoSOD meth-
ods, defense methods should be developed. However, few
works are focusing on this direction. A typical defense
method is to purify the input images to remove the effects of
degradations. In the following, we study the SOTA purifica-
tion method, i.e., DiffPure [22], to enhance the robustness
and show that enhancing the robustness of CoSOD is a non-
trivial task and new technologies should be developed.
3.3. DiffPure and Challenges
A highly intuitive approach is to perform image recon-
struction on input images, hoping to remove degradations.
As an existing method for image reconstruction, DiffPure
DiffPureGroundtruth GICD GCAGC Input
Co-salient
Object
DetectionPurified ImageFigure 2. CoSOD results for DiffPure.The input images are under the at-
tack method [12]. Processed by DiffPure [22], the purified images perform
inferior in the CoSOD task together with their respective group images.
[22] can remove adversarial perturbations by applying for-
ward diffusion followed by a reverse generative process.
However, DiffPure is limited in its ability to address adver-
sarial additive perturbations. It presents limited capabilities
for handling other degradations like adversarial exposure.
Fig. 2 illustrates two cases, with the upper case depicting
a koala and the lower case representing a train. The in-
put images for both cases are under the attack method [12].
The images processed through DiffPure visually eliminate
perturbations. However, when the purified images under-
went co-salient object detection together with the images
within their respective groups, the detection results are in-
ferior. Fig. 2 illustrates that the DiffPure cannot enhance the
robustness of CoSOD under the attack method [12].
We tend to design a more effective purification method.
DiffPure is specifically designed against adversarial attacks
for image classification and neglects the specific properties
of the CoSOD task: ❶Only partial images within the group
are attacked, and the clean images contain rich complemen-
tary information, which could help enhance the robustness.
❷Although the adversarial patterns may affect the semantic
features of images, the fact group images contain co-salient
objects has not changed. How to utilize such a property
should be carefully studied.
4. Methodology: C OSAL PURE
4.1. Overview
Beyond DiffPure [22], we propose to learn the concept
of co-salient objects from the group images and leverage it
to guide the purification. Specifically, given group images
I′={I′
j}M
j=1∪ {Ik}N−M
k=1that contains Mdegraded im-
ages and N−Mclean images, we first learn the concept
fromI′via the recent developed textual inversion method.
The learned concept is a token and lies in the latent space
of texts. We name it as ‘concept’ since we can use it to
generate new images containing the ‘concept’. Note that
the number of the degraded images ( i.e.,M) is unknown
during application. We denote the concept of learning as
c=ConceptLearn (I′), (3)
and we detail the whole process in Sec. 4.2.
3671
Image 
Encoder
Image Embedding z
T ext Encoder
T ext: “A photo of S*”
Concept Embedding c 
T ext EmbeddingGroup ImagesSample
Conditional
Diffusion
Model
Pred Noise
GT NoiseText-to-Image Diffusion Model
Image 
Encoder
Image Embedding z
Continuous 
Representation
Conditional
Diffusion
Model
Concept Embedding c 
Pred NoiseLoop from 
t = T to 1
Image 
Decoder
updateImage-to-Image Diffusion Model
Resize
(a) Group Image Concept Learning (b) Concept-guided Diffusion PurificationFigure 3. Overview of C OSAL PURE. The details of (a) are in Sec. 4.2, while the details of (b) are in Sec. 4.3.
Learned ConceptConcept Learning
Text-to-image Model
(a) (b)Learned ConceptConcept Learning
Text-to-image ModelClean Images Clean Images Adversarial Images
Figure 4. Demonstration of the effectiveness of concept learning. (a) Five
clean images are utilized for concept learning, and the learned concept can
be reconstructed into an image through a pre-trained text-to-image model.
(b) The first two images are attacked by Jadena [12] while the subsequent
three images are clean, and the learned concept can also be reconstructed
into a high-quality image. (a) and (b) use the same random seed.
After obtaining the concept, we aim to leverage it for
purification by
ˆI=ConceptPure (c,I),I∈ I′, (4)
where the image ˆIis the purified image of Ithat may be a
clean image or a perturbed image. We detail the concept-
guided diffusion purification in Sec. 4.3. For each image
inI′, we can handle it via Eq. (4) and get a novel group
denoted as ˆI. Then, we feed ˆIto CoSOD methods to see
whether their robustness is enhanced or not.
The core idea is valid based on a critical assumption: the
perturbed images in I′do not affect the concept learning.
We detail this in the Sec. 4.2.
4.2. Group-Image Concept Learning
In this section, we introduce the detail of group-
image concept learning ( i.e., Eq (3)), which tends to
utilize a group of input images for learning the text-
aligned embedding of common objects they have, as
shown in Fig. 3 (a). We denote this process as c=
ConceptLearn
{I′
j}M
j=1∪ {Ik}N−M
k=1
where crepresentsa token aligned with the texts’ latent space and represents
the semantic information of common objects.
To this end, we formulate group-image concept learning
as the personalizing text-to-image problem [10, 24] to en-
able text-to-image (T2I) diffusion models to rapidly swift
new concept acquisition.
Text-to-Image Diffusion Model. We introduce the ar-
chitecture and procedure of a classical T2I diffusion model
[23]. It consists of three core modules: (1) image autoen-
coder, (2) text encoder, (3) and conditional diffusion model.
The image autoencoder module has two submodules: an
encoder Eand a decoder D. It serves a dual purpose, where
the encoder maps an input image Xto a low-dimensional
latent space with z=E(X), while the decoder transforms
the latent representation back into the image space with
D(E(X))≈X. The text encoder Γfirstly processes a text y
by tokenizing it and secondly translates it into a latent space
text embedding Γ(y). The conditional diffusion model ϵθ
takes the time step t, the noisy latent ztatt-th time step and
the text embedding Γ(y)as input to predict the noise added
onzt, denoted as ϵθ(zt, t,Γ(y)).
Given a pre-trained T2I diffusion model and group im-
agesI′={I′
j}M
j=1∪ {Ik}N−M
k=1used for CoSOD task, we
aim to learn a concept of the common object within I′by
c= arg min
c∗EX∈I′,z∈E(X),y,ϵ∈N(0,1),t(
∥ϵθ(zt, t,Υ(Γ(y),c∗))−ϵ∥2
2), (5)
where yis a fixed text ( i.e., ‘a photo of S∗’) and the function
Υ(Γ(y),c∗)is to replace the token of ‘ S∗’ within Γ(y)with
c∗. Intuitively, Eq. (5) forces the concept c∗to represent
the co-salient objects within group images and also lies in
the text latent space corresponding to the text ‘ S∗’. After
obtaining c, we can embed ‘ S∗’ into other texts to generate
new images via the T2I diffusion model. For example, in
Fig. 4 (b), we learn a concept of the co-salient objects ( i.e.,
piano), which corresponds to the text ‘ S∗’. Then, we feed a
text ( e.g., ‘a photo of S∗’) to the T2I model that generates an
image containing the object, which means that the learned
concept represents the salient objects in I′very well.
3672
Robustness of concept learning. It is obvious that the
above method naturally aligns with our objective since we
can exploit it to obtain the common semantic content among
the group images for the CoSOD task. The key problem is
whether the concept learning would be affected by degra-
dations like adversarial perturbation in the group images.
We conduct an empirical study to validate this. Specifi-
cally, given a group of clean images ( i.e.,I), we use it to
learn a concept via Eq. (5). Meanwhile, we conduct the ad-
versarial CoSOD attack [12] on two images within Iand
form a new group I′. With I′, we learn another concept via
Eq. (5). Then, we can leverage the two learned concepts to
generate images based on the same text prompt. As shown
in Fig. 4, we find that the two generated images based on
two concepts are similar, demonstrating that the adversarial
examples have limited influence on concept learning. This
inspires us to leverage the learned concept to purify the ad-
versarial examples.
4.3. Concept-guided Diffusion Purification
We propose reconstructing the group images based on
the learned concept cto eliminate the potential adversarial
patterns as shown in Fig. 3 (b). Considering the advantage
of the continuous representation [4, 14] in the smooth image
reconstruction and its ability to remove perturbations, we
employ a continuous representation module for the initial
processing of the input image X, denoted as ˜X=CR(X).
This module not only somewhat denoises Xbut also ad-
dresses the issue of pre-trained CoSOD models designed
for specific resolutions that do not match the input resolu-
tion of our employed diffusion model. Subsequently, the en-
coder of the image autoencoder module maps ˜Xto the latent
space z0=E(˜X). The following procedure is based on the
diffusion pipeline and needs two procedures: forward pro-
cess and reverse process. The forward diffusion process is
a fixed Markov chain that iteratively adds a Gaussian noise
to the latent z0overTtimesteps, obtaining a sequence of
noised images z1,z2,···,zT. In each step of the forward
progress, the latent at time step t∈[1, T]is updated by
zt=atzt−1+btϵt, ϵt∼ N(0,I), (6)
where atandbtare coefficients and N(0,I)represents
the standard Gaussian distribution. By superimposing time
steps from t= 1toT, Eq. (6) can be simplified to
zt=√αtz0+√
1−αtϵt, ϵt∼ N(0,I), (7)
where we have a2
t+b2
t= 1, andαt=a2
t,αt=Qt
τ=1ατ.
As we set the time step as T, the complete forward process
can be expressed as
zT∼q(z1:T|z0) =TY
t=1q(zt|zt−1). (8)
my_aeroplane my_axe my_camel my_dolphin
Processed
Image
Attention
Map
Learned
ConceptFigure 5. Attention maps for learned concepts on processed images.
For the reverse process, it iteratively removes the noise to
generate an image in Ttimesteps. Unlike doing the reverse
process directly, our method incorporates the obtained se-
mantic embedding cas additional object information into
the pipeline. Then, we can start from zT(alternatively
called ˆzT) and progressively predict
ϵt−1=ϵθ(ˆzt, t,c), (9)
and obtain the latent at time step t−1via
ˆzt−1=√¯αt−1(1−αt)
1−¯αt˜z0
+√αt(1−¯αt−1)
1−¯αtˆzt+σtξ, (10)
with
˜z0=ˆzt−√1−¯αtϵt−1√¯αt, (11)
where σ2
t=(1−αt)(1−αt−1)
1−αtandξ∼ N (0,I)according
to the sample process of DDPM [15]. We can directly ob-
tain the reconstructed image ˆxby using the decoder with
formula ˆx=D(ˆz0).
To confirm that the concept learned by C OSAL PURE is
applied accurately in the image reconstruction, we employ
DAAM [26] to establish attention maps for the learned con-
cepts on processed images as shown in Fig. 5. In each
case, the attention map of the semantic embedding c(i.e.,
the learned concept) aligns well with the object itself in the
image, indicating the effectiveness of C OSAL PURE.
5. Experiment
5.1. Experimental Setup
Datasets. We conduct experiments on Cosal2015 [32],
iCoseg [2], CoSOD3k [8], and CoCA [36]. These four
datasets contain 2,015, 643, 3,316, and 1,295 images of 50,
38, 160, and 80 groups respectively. We apply the SOTA
adversarial attack for CoSOD ( i.e., Jadena [12]) to the first
50% of images in each group, while the remaining 50% of
3673
Table 1. Co-saliency detection performance. “Source-Only” means the group of images before processing, including 50% adversarial images and 50%
clean images. We highlight the top results of each CoSOD method and each dataset in red.
GICD GCAGC PoolNet
SR↑ AP↑ Fβ↑ MAE↓ SR↑ AP↑ Fβ↑ MAE↓ SR↑ AP↑ Fβ↑ MAE↓Cosal2015Source-Only 0.3493 0.7306 0.4038 0.1676 0.5285 0.7853 0.6302 0.1570 0.5677 0.7425 0.6095 0.1276
DiffPure 0.4595 0.7478 0.5118 0.1444 0.4987 0.6998 0.5901 0.2162 0.6327 0.7779 0.6714 0.1181
DDA 0.4565 0.7579 0.5158 0.1469 0.5955 0.7928 0.6774 0.1542 0.6233 0.7863 0.6691 0.1181
COSAL PURE 0.5602 0.7898 0.6177 0.1296 0.5975 0.7449 0.6521 0.2063 0.6908 0.8268 0.7258 0.1086iCosegSource-Only 0.4012 0.7269 0.5063 0.1420 0.6469 0.8237 0.7173 0.1146 0.5847 0.8116 0.6472 0.1057
DiffPure 0.4447 0.7291 0.5503 0.1269 0.6609 0.8043 0.7051 0.1257 0.6796 0.8328 0.7144 0.0905
DDA 0.4665 0.7519 0.5948 0.1280 0.6982 0.8257 0.7390 0.1235 0.6578 0.8483 0.7179 0.0940
COSAL PURE 0.5396 0.7611 0.6329 0.1208 0.7060 0.8052 0.7265 0.1413 0.7278 0.8730 0.7577 0.0850CoSOD3kSource-Only 0.3281 0.6988 0.4003 0.1439 0.4445 0.7325 0.5702 0.1376 0.4466 0.6606 0.5255 0.1386
DiffPure 0.3887 0.6976 0.4683 0.1342 0.4996 0.7364 0.6279 0.1272 0.5247 0.7021 0.6064 0.1340
DDA 0.3838 0.7083 0.4776 0.1344 0.5337 0.7655 0.6544 0.1251 0.5105 0.7078 0.5875 0.1311
COSAL PURE 0.4659 0.7327 0.5487 0.1221 0.5946 0.7999 0.6881 0.1144 0.5859 0.7432 0.6605 0.1215CoCASource-Only 0.1837 0.5490 0.3402 0.1168 0.2339 0.5177 0.4698 0.1227 0.2239 0.4296 0.4082 0.1500
DiffPure 0.1706 0.5362 0.3492 0.1213 0.2231 0.5051 0.4995 0.1190 0.2185 0.4426 0.4286 0.1649
DDA 0.2054 0.5543 0.3668 0.1156 0.2671 0.5476 0.5165 0.1129 0.2416 0.4503 0.4470 0.1548
COSAL PURE 0.2409 0.5753 0.3976 0.1119 0.3057 0.5884 0.5512 0.1040 0.2633 0.4681 0.4745 0.1604
Table 2. Co-saliency detection success rates (SR) of entire group of images, only adversarial images and only clean images. This table is to intuitively
illustrate the impact of different methods on the adversarial and clean portions of group images.
GICD GCAGC PoolNet
avg↑ adv↑ clean↑ avg↑ adv↑ clean↑ avg↑ adv↑ clean↑Cosal2015Source-Only 0.3493 0.1053 0.5884 0.5285 0.3741 0.6797 0.5677 0.3671 0.7642
DiffPure 0.4595 0.3560 0.5609 0.4987 0.4533 0.5432 0.6327 0.5636 0.7003
DDA 0.4565 0.3079 0.6021 0.5955 0.4924 0.6964 0.6233 0.5185 0.7259
COSAL PURE 0.5602 0.5416 0.5785 0.5975 0.5977 0.5972 0.6908 0.6569 0.7239iCosegSource-Only 0.4012 0.1516 0.6336 0.6469 0.6161 0.6756 0.5847 0.3451 0.8078
DiffPure 0.4447 0.3161 0.5645 0.6609 0.6258 0.6936 0.6796 0.5741 0.7777
DDA 0.4665 0.3483 0.5765 0.6982 0.6903 0.7057 0.6578 0.5419 0.7657
COSAL PURE 0.5396 0.5129 0.5645 0.7060 0.7064 0.7057 0.7278 0.6645 0.7867CoSOD3kSource-Only 0.3281 0.1118 0.5364 0.4445 0.2901 0.5932 0.4466 0.2354 0.6500
DiffPure 0.3887 0.2987 0.4754 0.4996 0.4333 0.5636 0.5247 0.4462 0.6003
DDA 0.3838 0.2606 0.5026 0.5337 0.4394 0.6246 0.5105 0.3945 0.6222
COSAL PURE 0.4659 0.4597 0.4718 0.5946 0.5955 0.5938 0.5859 0.5703 0.6009CoCASource-Only 0.1837 0.0877 0.2739 0.2339 0.1818 0.2829 0.2239 0.1371 0.3053
DiffPure 0.1706 0.1212 0.2170 0.2231 0.1802 0.2634 0.2185 0.1754 0.2589
DDA 0.2054 0.1499 0.2574 0.2671 0.2264 0.3053 0.2416 0.1897 0.2904
COSAL PURE 0.2409 0.2360 0.2455 0.3057 0.2998 0.3113 0.2633 0.2264 0.2979
images are kept in the clean state. We select the “augment”
version of Jadena and follow the settings[12].
Evaluation settings. We choose GICD [35] and GCAGC
[34] to evaluate our method as they are commonly used
state-of-the-art CoSOD methods. Additionally, we take
PoolNet [20] into consideration, assessing the performance
in salient object detection.
Baseline methods. Indeed, there is currently no spe-
cific image processing method designed for CoSOD at-
tacks. Hence, we employ two alternative approaches as
baselines. DiffPure [22] is a method that utilizes a dif-
fusion model for purifying perturbation-based adversarial
images. Diffusion-Driven Adaptation (DDA) [11] builds
upon a diffusion-based model by introducing a novel self-
ensembling scheme, enhancing the adaptation process by
dynamically determining the degree of adaptation. DiffPureand DDA employ the same sampling noise scale as our pro-
posed C OSAL PURE.
Metrics. We employ four metrics to evaluate the co-
salient object detection result, including detection success
rate (SR), average precision (AP) [33], F-measure score Fβ
withβ2= 0.3[1] and mean absolute error (MAE) [33]. For
the detection success rate, we calculate the intersection over
union (IOU) between each co-salient object detection result
of the reconstructed image and the corresponding ground-
truth map. We divide the number of successful results (IOU
> 0.5) by the total number of results to calculate SR. In addi-
tion, to intuitively illustrate the impact of different methods
on CoSOD results, we not only compute SR for the entire
group of images but also separately calculate SR for only
adversarial images and only clean images.
Implementation details. In the group-image concept learn-
3674
Image Groundtruth GICD GCAGC PoolNetSource-Only DiffPure DDA CosalPureImage Groundtruth GICD GCAGC PoolNetSource-Only DiffPure DDA CosalPure
Image Groundtruth GICD GCAGC PoolNetSource-Only DiffPure DDA CosalPureImage Groundtruth GICD GCAGC PoolNetSource-Only DiffPure DDA CosalPure
Figure 6. Visualization of co-salient object detection results. We show four visualized cases in this figure, with the source-only/purified image, the ground-
truth of the co-saliency map, and the results of GICD, GCAGC, and PoolNet in the columns. Our method, C OSAL PURE, is highlighted in green.
ing procedure, the sampled images are simply resized from
224×224resolution to 768×768resolution before being
passed into the image encoder. For the continuous represen-
tation [4, 14] module employed in the concept-guided diffu-
sion purification procedure, we constructed a dataset to train
it. We select 50,000 samples with a resolution of 224×224
from ImageNet (1,000 categories, each with 50 samples)
and apply noise with the intensity of 16/255 via the PGD
attack to these samples to construct the inputs of the contin-
uous representation module. For the ground truth images,
we apply clean images with a resolution of 768×768cor-
responding to the input images. We follow the experimental
setup of [14] and trained for 10 epochs to obtain the required
module. The group-image concept learning procedure and
the concept-guided purification procedure utilize the same
pre-trained image encoder and conditional diffusion model.
For the concept-guided purification procedure, we set the
number of timesteps Tto 250, and the same configuration
is applied to baseline methods.
5.2. Comparison on Adversarial Attacks
We denote the images before reconstruction (contain-
ing 50% adversarial images and 50% clean images) as
"Source-Only". The comparison between our proposed
COSAL PURE and baselines are shown in Table 1. We con-
sider an image to be successfully detected in the co-salient
object detection (CoSOD) task if the IOU of its CoSOD
result and the ground-truth map exceed 0.5. Compared
to DiffPure [22] and DDA [11], C OSAL PURE outperforms
them in terms of co-salient object detection success rates
(SR) across all four datasets. For the other three metrics
(i.e., AP [33], Fβ[1] and MAE [33]), C OSAL PURE remainsthe best at most of the time. We show four visualized cases
in Fig. 6. For each case, we present the generated images of
COSAL PURE and two baselines, DiffPure and DDA. Obvi-
ously C OSAL PURE generates higher-quality images, as it
leverages the intrinsic commonality of objects across the
group of images. Additionally, we showcase the compar-
ison of the detection results on GICD, GCAGC, and Pool-
Net. The results from C OSAL PURE closely approximate
the ground-truth map, while the baseline methods struggle
to display the correct results.
To intuitively illustrate the impact of different methods
on the adversarial and clean portions of group images, we
measure the co-salient object detection success rate from
three perspectives. In Table 2, "avg" represents the evalu-
ation across the entire group of images, "adv" and "clean"
correspond to evaluations on only the 50% images that are
under the SOTA attack [12] and on only the 50% images
that remain clean. C OSAL PURE at some times have a lower
“clean” SR compared to DDA or source-only. However,
DiffPure and DDA are unsatisfactory in “adv” SR, while
COSAL PURE exhibits a significant lead in “adv” SR, result-
ing in it consistently performing the best in “avg” SR.
5.3. Ablation Study
To validate the effect of the learned concepts on CoSOD
results, we conduct ablation studies on Cosal2015 [32] and
CoSOD3k [8]. In Table 3, “w/o concept inversion” repre-
sents only utilizing the continuous representation module
and not applying the subsequent purification process. “w/
None concept” denotes passing a meaningless “None” as
the concept during the purification. “w/ learned concept”
denotes the complete pipeline, firstly learning the concept
3675
Table 3. Ablation study. “w/o concept inversion” represents only utilize the continuous representation module and not apply the subsequent purification
process. “w/ None concept” denotes passing a meaningless “None” as the concept during the purification procedure. “w/ learned concept” denotes the
complete pipeline, firstly learning the concept from the entire group of images and secondly passing in the learned concept during the purification procedure.
GICD GCAGC PoolNet
SR↑ AP↑ Fβ↑ MAE↓ SR↑ AP↑ Fβ↑ MAE↓ SR↑ AP↑ Fβ↑ MAE↓Cosal2015Source-Only 0.3493 0.7306 0.4038 0.1676 0.5285 0.7853 0.6302 0.1570 0.5677 0.7425 0.6095 0.1276
COSAL PURE w/o concept inversion 0.5186 0.7791 0.5809 0.1350 0.5528 0.7322 0.6430 0.2145 0.6843 0.8241 0.7225 0.1098
COSAL PURE w/ None concept 0.5225 0.7784 0.5886 0.1354 0.5334 0.7091 0.6132 0.2263 0.6774 0.8172 0.7160 0.1110
COSAL PURE w/ learned concept 0.5602 0.7898 0.6177 0.1296 0.5975 0.7449 0.6521 0.2063 0.6908 0.8268 0.7258 0.1086CoSOD3kSource-Only 0.3281 0.6988 0.4003 0.1439 0.4445 0.7325 0.5702 0.1376 0.4466 0.6606 0.5255 0.1386
COSAL PURE w/o concept inversion 0.4424 0.7314 0.5317 0.1243 0.5753 0.7923 0.6804 0.1170 0.5747 0.7424 0.6508 0.1223
COSAL PURE w/ None concept 0.4297 0.7216 0.5158 0.1273 0.5488 0.7715 0.6584 0.1288 0.5711 0.7373 0.6453 0.1231
COSAL PURE w/ learned concept 0.4659 0.7327 0.5487 0.1221 0.5946 0.7999 0.6881 0.1144 0.5859 0.7432 0.6605 0.1215
Image Groundtruth GICD GCAGC PoolNetSource-Onlyw/o concept
inversion
w/  None 
conceptw/  learned
concept
Figure 7. Visualization for ablation study.
Table 4. Extension to motion blur.
SR↑ AP↑ Fβ↑ MAE↓
Source-Only 0.3915 0.7408 0.4373 0.1590
DiffPure 0.3146 0.6774 0.3763 0.1738
DDA 0.3900 0.7381 0.4425 0.1579
COSAL PURE 0.4575 0.7419 0.5241 0.1505
from the entire group of images and secondly passing the
learned concept during the purification procedure to accom-
plish image reconstruction. From Table 3, it is evident that
the learned concept contributes to significant improvements
in various metrics. As shown in Fig. 7, when we do not
apply the concept-guided purification or when we pass in a
meaningless concept, the generated image performs poorly
in the CoSOD task. This improves when we pass in the
learned effective concept which includes object semantics.
The example proves that the learned concept contributes to
the reconstruction of images used for the CoSOD task.
5.4. Extention to Common Corruption
In addition to adversarial attacks on CoSOD, we also
broaden our experiments to include a common corruption
type: motion blur. We select the Cosal2015 dataset [32]
and, similar to the adversarial experiments, apply motion
blur [13] to the first 50% of images in each group while
keeping the remaining 50% of images clean. Here weset the number of timesteps Tto 500 and do not employ
the continuous representation module. As shown in Ta-
ble 4, C OSAL PURE performs better than other diffusion-
based image processing methods at all four metrics.
6. Conclusions
This paper presented C OSAL PURE, an innovative frame-
work enhancing the robustness of co-salient object detec-
tion (CoSOD) against adversarial attacks and common im-
age corruptions. Central to our approach are two key
innovations: group-image concept learning and concept-
guided diffusion purification. Our framework effectively
captures and utilizes the high-level semantic concept of co-
salient objects from group images, demonstrating notable
resilience even in the presence of adversarial examples.
Empirical evaluations across datasets like Cosal2015,
iCoseg, CoSOD3k, and CoCA showed that C OSAL PURE
significantly outperforms existing methods such as DiffPure
and DDA in CoSOD tasks. Not only did it achieve higher
success rates, but it also excelled in performance metrics
like AP, F-measure, and MAE. Additionally, its effective-
ness against common image corruptions, like motion blur,
underscores its versatility.
Our C OSAL PURE represents a substantial advancement
in CoSOD, offering robust, concept-driven image purifica-
tion. It opens avenues for more resilient co-salient object
detection, vital in today’s landscape of sophisticated image
manipulation and corruption. Future work might extend this
framework to broader image analysis applications and ex-
plore its adaptability to real-world scenarios.
7. Acknowledgments
Geguang Pu is supported by National Key Research and
Development Program (2020AAA0107800), and Shanghai
Collaborative Innovation Center of Trusted Industry Inter-
net Software. This work is also supported by the National
Research Foundation, Singapore, and DSO National Labo-
ratories under the AI Singapore Programme (AISG Award
No: AISG2-GC-2023-008), and Career Development Fund
(CDF) of the Agency for Science, Technology and Research
(A*STAR) (No.: C233312028).
3676
References
[1] Radhakrishna Achanta, Sheila Hemami, Francisco Estrada,
and Sabine Susstrunk. Frequency-tuned salient region de-
tection. In 2009 IEEE conference on computer vision and
pattern recognition , pages 1597–1604. IEEE, 2009.
[2] Dhruv Batra, Adarsh Kowdle, Devi Parikh, Jiebo Luo, and
Tsuhan Chen. icoseg: Interactive co-segmentation with in-
telligent scribble guidance. In 2010 IEEE computer soci-
ety conference on computer vision and pattern recognition ,
pages 3169–3176. IEEE, 2010.
[3] Xiaochun Cao, Zhiqiang Tao, Bao Zhang, Huazhu Fu, and
Wei Feng. Self-adaptively weighted co-saliency detection
via rank constraint. IEEE Transactions on Image Processing ,
23(9):4175–4186, 2014.
[4] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning
continuous image representation with local implicit image
function. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 8628–8638,
2021.
[5] Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, and Shi-
Min Hu. Salientshape: group saliency in image collections.
The visual computer , 30:443–453, 2014.
[6] Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, Philip HS
Torr, and Shi-Min Hu. Global contrast based salient region
detection. IEEE transactions on pattern analysis and ma-
chine intelligence , 37(3):569–582, 2014.
[7] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu,
and Mubarak Shah. Diffusion models in vision: A survey.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence , 2023.
[8] Deng-Ping Fan, Zheng Lin, Ge-Peng Ji, Dingwen Zhang,
Huazhu Fu, and Ming-Ming Cheng. Taking a deeper look at
co-salient object detection. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition ,
pages 2919–2929, 2020.
[9] Deng-Ping Fan, Tengpeng Li, Zheng Lin, Ge-Peng Ji, Ding-
wen Zhang, Ming-Ming Cheng, Huazhu Fu, and Jianbing
Shen. Re-thinking co-salient object detection. IEEE trans-
actions on pattern analysis and machine intelligence , 44(8):
4339–4354, 2021.
[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-
nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-
Or. An image is worth one word: Personalizing text-to-
image generation using textual inversion. arXiv preprint
arXiv:2208.01618 , 2022.
[11] Jin Gao, Jialing Zhang, Xihui Liu, Trevor Darrell, Evan
Shelhamer, and Dequan Wang. Back to the source:
Diffusion-driven test-time adaptation. arXiv preprint
arXiv:2207.03442 , 2022.
[12] Ruijun Gao, Qing Guo, Felix Juefei-Xu, Hongkai Yu,
Huazhu Fu, Wei Feng, Yang Liu, and Song Wang. Can
you spot the chameleon? adversarially camouflaging im-
ages from co-salient object detection. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 2150–2159, 2022.
[13] Dan Hendrycks and Thomas Dietterich. Benchmarking neu-
ral network robustness to common corruptions and perturba-tions. Proceedings of the International Conference on Learn-
ing Representations , 2019.
[14] Chih-Hui Ho and Nuno Vasconcelos. Disco: Adversarial
defense with local implicit functions. Advances in Neural
Information Processing Systems , 35:23818–23837, 2022.
[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840–6851, 2020.
[16] Bo Jiang, Xingyue Jiang, Ajian Zhou, Jin Tang, and Bin Luo.
A unified multiple graph learning and convolutional network
model for co-saliency estimation. In Proceedings of the 27th
ACM international conference on multimedia , pages 1375–
1382, 2019.
[17] Bo Li, Zhengxing Sun, Lv Tang, Yunhan Sun, and Jinlong
Shi. Detecting robust co-saliency with recurrent co-attention
neural network. In IJCAI , page 6, 2019.
[18] Guanbin Li and Yizhou Yu. Deep contrast learning for salient
object detection. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 478–487,
2016.
[19] Hongliang Li, Fanman Meng, Bing Luo, and Shuyuan Zhu.
Repairing bad co-segmentation using its quality evaluation
and segment propagation. IEEE Transactions on Image Pro-
cessing , 23(8):3545–3559, 2014.
[20] Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi
Feng, and Jianmin Jiang. A simple pooling-based design
for real-time salient object detection. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 3917–3926, 2019.
[21] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-
Yan Zhu, and Stefano Ermon. Sdedit: Image synthesis and
editing with stochastic differential equations. arXiv preprint
arXiv:2108.01073 , 2021.
[22] Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash
Vahdat, and Anima Anandkumar. Diffusion models for
adversarial purification. arXiv preprint arXiv:2205.07460 ,
2022.
[23] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10684–10695, 2022.
[24] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 22500–
22510, 2023.
[25] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for
training next generation image-text models. arXiv preprint
arXiv:2210.08402 , 2022.
[26] Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang,
Gefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin,
and Ferhan Ture. What the daam: Interpreting stable diffu-
3677
sion using cross attention. arXiv preprint arXiv:2210.04885 ,
2022.
[27] Linzhao Wang, Lijun Wang, Huchuan Lu, Pingping Zhang,
and Xiang Ruan. Saliency detection with recurrent fully con-
volutional networks. In Computer Vision–ECCV 2016: 14th
European Conference, Amsterdam, The Netherlands, Octo-
ber 11–14, 2016, Proceedings, Part IV 14 , pages 825–841.
Springer, 2016.
[28] Wenguan Wang, Qiuxia Lai, Huazhu Fu, Jianbing Shen,
Haibin Ling, and Ruigang Yang. Salient object detection
in the deep learning era: An in-depth survey. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence , 44(6):
3239–3259, 2021.
[29] Zheng-Jun Zha, Chong Wang, Dong Liu, Hongtao Xie, and
Yongdong Zhang. Robust deep co-saliency detection with
group semantic and pyramid attention. IEEE transactions
on neural networks and learning systems , 31(7):2398–2408,
2020.
[30] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang,
and In So Kweon. Text-to-image diffusion model in genera-
tive ai: A survey. arXiv preprint arXiv:2303.07909 , 2023.
[31] Dingwen Zhang, Deyu Meng, Chao Li, Lu Jiang, Qian Zhao,
and Junwei Han. A self-paced multiple-instance learning
framework for co-saliency detection. In Proceedings of the
IEEE international conference on computer vision , pages
594–602, 2015.
[32] Dingwen Zhang, Junwei Han, Chao Li, Jingdong Wang, and
Xuelong Li. Detection of co-salient objects by looking deep
and wide. International Journal of Computer Vision , 120:
215–232, 2016.
[33] Dingwen Zhang, Huazhu Fu, Junwei Han, Ali Borji, and
Xuelong Li. A review of co-saliency detection algorithms:
Fundamentals, applications, and challenges. ACM Transac-
tions on Intelligent Systems and Technology (TIST) , 9(4):1–
31, 2018.
[34] Kaihua Zhang, Tengpeng Li, Shiwen Shen, Bo Liu, Jin Chen,
and Qingshan Liu. Adaptive graph convolutional network
with attention graph clustering for co-saliency detection. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 9050–9059, 2020.
[35] Zhao Zhang, Wenda Jin, Jun Xu, and Ming-Ming Cheng.
Gradient-induced co-saliency detection. In Computer
Vision–ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part XII 16 , pages
455–472. Springer, 2020.
[36] Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao,
Jufeng Yang, and Ming-Ming Cheng. Egnet: Edge guid-
ance network for salient object detection. In Proceedings of
the IEEE/CVF international conference on computer vision ,
pages 8779–8788, 2019.
3678
