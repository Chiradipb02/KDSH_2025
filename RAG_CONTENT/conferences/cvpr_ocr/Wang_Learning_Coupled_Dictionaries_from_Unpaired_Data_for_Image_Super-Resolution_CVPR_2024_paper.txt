Learning Coupled Dictionaries from Unpaired Data for Image Super-Resolution
Longguang Wang1, Juncheng Li2, Yingqian Wang3, Qingyong Hu4, Yulan Guo3
1Aviation University of Air Force2Shanghai University
3National University of Defense Technology4Independent Researcher
Abstract
The difÔ¨Åculty of acquiring high-resolution (HR) and low-
resolution (LR) image pairs in real scenarios limits the per-
formance of existing learning-based image super-resolution
(SR) methods in the real world. To conduct training on real-
world unpaired data, current methods focus on synthesizing
pseudo LR images to associate unpaired images. However,
the realness and diversity of pseudo LR images are vulner-
able due to the large image space. In this paper, we cir-
cumvent the difÔ¨Åculty of image generation and propose an
alternative to build the connection between unpaired im-
ages in a compact proxy space. SpeciÔ¨Åcally, we Ô¨Årst con-
struct coupled HR and LR dictionaries, and then encode
HR and LR images into a common latent code space using
these dictionaries. In addition, we develop an autoencoder-
based framework to couple these dictionaries during opti-
mization by reconstructing input HR and LR images. The
coupled dictionaries enable our method to employ a shal-
low network architecture with only 18 layers to achieve
efÔ¨Åcient image SR. Extensive experiments show that our
method ( DictSR ) can effectively model the LR-to-HR map-
ping in coupled dictionaries and produces state-of-the-art
performance on benchmark datasets.
1. Introduction
Image super-resolution (SR) aims at reconstructing a high-
resolution (HR) image from low-resolution (LR) observa-
tions. To this end, most existing learning-based methods
[1‚Äì4] rely on paired training data to build an LR-to-HR
mapping. Since paired data is difÔ¨Åcult to acquire in the real
world, manually designed degradations are commonly em-
ployed to synthesize paired data for training [5, 6]. Never-
theless, as pre-deÔ¨Åned degradations are empirical, synthetic
images cannot meet the diversity of real-world images and
limit the performance of previous methods in real scenarios
[7, 8].
To remedy this problem, numerous efforts have been
made to directly conduct training on unpaired data [7‚Äì9]. To
enable the learning of LR-to-HR mapping, existing meth-
HR images
HR imagespseudo LR images LR images
latent code(a) Previous Approaches
(b) Our Approachminimize
gap
G
ELR
LR imagesEHRFigure 1. Comparison between previous approaches and our ap-
proach. Instead of synthesizing pseudo LR images and minimizing
the gap in the image space (a), we encode HR and LR images into a
common latent code space using coupled dictionaries to associate
unpaired HR and LR images (b).
ods focus on synthesizing pseudo paired data for training.
Given HR images xpx(target domain) and LR images
ypy(source domain), pseudo LR images are generated by
learning an HR-to-LR mapping. Then, HR images and the
resultant pseudo LR images are adopted to learn the LR-to-
HR mapping. Early methods [7, 9] commonly use genera-
tive adversarial networks (GANs) with cycle-consistency or
adversarial loss to synthesize pseudo LR images. However,
these GAN-based methods usually suffer mode collapse and
require elaborate Ô¨Åne-tuning. Recently, more powerful dif-
fusion model [10] is introduced for pseudo LR image syn-
thesis. The large latent space in the diffusion model im-
proves the diversity of synthesized images at the cost of
lower content Ô¨Ådelity, which hinders further performance
improvements.
Due to the low rank characteristic of natural images,
an image can be represented as a spare code on an over-
complete dictionary [11, 12]. On top of this perspective,
we seek to associate unpaired LR and HR images in a com-
pact space without relying on synthesizing pseudo LR im-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
25712
ages high in diversity and realness (Fig. 1). By representing
images using spatial codes over a dictionary, the descrip-
tion length is signiÔ¨Åcantly reduced, which allows us to ef-
Ô¨Åciently minimize the gap between HR and LR image do-
mains. To this end, we use autoencoders to learn LR and
HR dictionaries rich of context information from LR and
HR images. To Ô¨Åt the large space of real-world LR patches,
our LR dictionary explicitly models the content and degra-
dation variations in a hierarchical manner (Fig. 2). During
training, by coupling HR and LR dictionaries, an LR-to-HR
mapping can be learned in the dictionaries. During infer-
ence, an LR image is Ô¨Årst decomposed over the LR dictio-
nary and then using the HR dictionary for reconstruction to
produce the SR result. The contributions of this paper can
be summarized as follows:
We associate unpaired images using coupled HR and LR
dictionaries instead of synthesizing pseudo LR images.
By decomposing unpaired images over the dictionary to
obtain compact proxy codes, an LR-to-HR mapping can
be efÔ¨Åciently learned in the code space.
We formulate the LR dictionary as a combination of an
LR content dictionary and a degradation dictionary to
model LR images in a hierarchical manner.
We develop an autoencoder-based framework to optimize
coupled dictionaries together with the network parame-
ters in an end-to-end manner.
Extensive experiments show that our coupled dictionar-
ies can effectively learn the LR-to-HR mapping and our
method ( DictSR ) achieves state-of-the-art performance
on benchmark datasets.
2. Related Works
In this section, we Ô¨Årst review recent advances of paired and
unpaired image SR methods. Then, we discuss dictionary
learning approach that is related to our work.
2.1. Paired Image SR
Most previous learning-based image SR methods rely on
paired data with aligned contents for training. Early meth-
ods [1‚Äì3, 13‚Äì15] commonly employ bicubic downsampling
to synthesize HR-LR image pairs as the training data. De-
spite promising results on synthetic data, these methods
suffer severe performance drop on real-world images since
their degradations differ from the bicubic one. Later, sev-
eral methods adopt more complicated degradations includ-
ing blur, compression, and noise to generate HR-LR image
pairs [5, 16‚Äì19]. Although the handcrafted degradations
span a large space, the gap between real-world degradations
still remains and limits the accuracy in real scenarios. Re-
cently, a number of real-world paired datasets are developed
to capture real degradations [20, 21]. However, the high la-
bor cost limits the scales of these datasets, thereby hindering
further performance improvement.2.2. Unpaired Image SR
To circumvents the difÔ¨Åculty of acquiring real-world paired
data, a number of methods directly conduct training on un-
paired images. Early approaches assume real-world LR im-
ages follow a speciÔ¨Åc distribution and employ GANs to syn-
thesize pseudo LR image in this distribution. SpeciÔ¨Åcally,
Bulat et al. [22] and Lugmayr et al. [23] Ô¨Årst trained a
degradation network to synthesize pseudo LR images from
HR images and then use these paired images to learn the
LR-to-HR mapping. Yuan et al. [9] and Maeda et al.
[7] developed uniÔ¨Åed frameworks to simultaneously learn
a degradation network and an SR network, which produce
superior accuracy. Liu et al. [24] and Yang et al. [25] intro-
duced physical properties to LR image synthesis as regular-
izions for higher realness. Despite improved performance
against paired approaches, these GAN-based methods usu-
ally suffer mode collapse and cannot generate LR images as
diverse as real ones.
To address the aforementioned problem of GAN-based
methods, Wolf et al. [26] developed a Ô¨Çow-based model
to synthesize LR images. Although the diversity of syn-
thesized LR images is improved, this method has a high
computational cost due to the limited expressive power of
Ô¨Çow-based models [27, 28]. Recently, Yang et al. [10] in-
troduced the powerful diffusion model for pseudo LR image
synthesis. The large latent space in the diffusion model im-
proves the diversity but decreases the content Ô¨Ådelity, which
also results in limited performance.
2.3. Dictionary Learning
Dictionary learning aims at Ô¨Ånding a sparse representation
of the input data in the form of a linear combination of ba-
sic elements as well as those basic elements themselves.
The idea of dictionary learning can be dated back to sev-
eral decades ago [29, 30]. Before the era of deep learning,
dictionary learning is widely used to model the LR-to-HR
mapping for image SR. SpeciÔ¨Åcally, Yang et al. [12] pro-
posed coupled dictionaries to associate LR and HR image
patches via sparse coding. Zhang et al. [11] developed
a multi-scale dictionary to simultaneously model local and
non-local priors in the images. Lu et al. [31] took the ge-
ometrical structure of the dictionary into consideration and
proposed a geometry constrained sparse coding method for
image SR.
Since the huge success of neural networks, learning-
based methods have dominated a wide range of tasks. Re-
cently, the idea of dictionary learning has been introduced
to learning-based frameworks and produces superior per-
formance. SpeciÔ¨Åcally, Van et al. [32] proposed a vector
quantized variational autoencoder (VQ-V AE) to learn dis-
crete representations of an image over a codebook. Then,
a CNN is trained to model their distribution for image syn-
thesis. Then, Razavi et al. [33] extended this approach to a
25713
EHR
ELR
ELRDHR
DLR
DLR
LR imageHR image
dowscaled 
image  
embedding spacereconstructed 
results
‚Ä¶0
1
N-1ùêÉùëêùëúùëõùêªùëÖ
‚Ä¶0
1
N-1ùêÉùëêùëúùëõùêøùëÖ
‚Ä¶12
K-1
‚Ä¶‚Ä¶0
00
1
N-1K-1ùêÉùëêùëúùëõùêøùëÖ+ùêÉùëëùëíùëîshared sharedencoding discretization decoding
ùêÖùêªùëÖ
ùêÖùêªùëÖ‚Üì
ùêÖùêøùëÖùêÖùëûùêªùëÖ
ùêÖùëûùêªùëÖ‚Üì
ùêÖùëûùêøùëÖ
ELR
LR image SR resultsùêÖùêøùëÖ ùêÖùëûùêªùëÖ
indices
DHR
‚Ä¶12
K-1
‚Ä¶‚Ä¶0
00
1
N-1K-1ùêÉùëêùëúùëõùêøùëÖ+ùêÉùëëùëíùëî‚Ä¶0
1
N-1ùêÉùëêùëúùëõùêªùëÖ(a) Training Phase (b) Test PhaseFigure 2. An illustration of our framework during training phase (a) and test phase (b). Entries with different colors correspond to various
contents while entries with different shades refer to the same content but diverse degradations.
hierarchical one to learn discrete representation at different
scale, which achieves superior performance. Later, Esser
et al. [34] further developed VQ-GAN by leveraging the
powerful transformer to model the composition of images.
Recently, Maeda et al. [35] introduced dictionary learning
for image SR by plugging a learnable dictionary into a CNN
framework. However, this method relies on paired data to
learn the LR-to-HR mapping and cannot be extended to un-
paired SR.
3. Methodology
Ideally, a pair of LR and HR images can be decomposed
to a common latent code over coupled LR-HR dictionaries.
From this point of view, we seek to learn coupled dictionar-
ies from unpaired data for image SR. To this end, we formu-
late the coupled dictionaries as a combination of an HR con-
tent dictionary DHR, an LR content dictionary DLR, and adegradation dictionary Ddeg. These three dictionaries are
tightly coupled and jointly optimized in three branches, as
illustrated in Fig. 2. During the training phase, the pipeline
for each branch consists of three stages, including encoding,
descretization, and decoding. SpeciÔ¨Åcally, input images are
Ô¨Årst encoded into embeddings. Then, these embeddings are
discretized in the embedding space to the entries in corre-
sponding dictionaries. Finally, the discretized embeddings
are fed to the decoders to reconstruct the input image. Dur-
ing the test phase, the input LR image is Ô¨Årst encoded to an
indicex map using DLRandDdeg. Then, the corresponding
entries in DHRis retrieved to reconstruct the SR result.
3.1. Encoding Stage
Given an HR image IHR2RHW3and a unpaired LR
image ILR2Rhw3,IHRis Ô¨Årst downscaled to obtain
IHR#2RH
sW
s3. Here,sis the scale factor and set to
4 as we focus on4SR in this paper. Then, IHRis fed
25714
to the HR image encoder to produce FHR2RH
8W
8512,
with each embedding corresponding to a 88patch in the
input HR image. Meanwhile, ILRandIHR#are passed to
the LR image encoder to obtain FLR2Rh
2w
2128and
FHR#2RH
8W
8128, respectively. Each embedding in the
resultant feature maps corresponds to a 22patch in the
input LR image.
The HR image encoder employs a fully convolutional
structure and consists of four residual blocks at four reso-
lution levels ( i.e.,HW,H
2W
2,H
4W
4, andH
8W
8).
Meanwhile, the LR image encoder consists of two residual
blocks athwandh
2w
2resolution levels.
3.2. Discretization Stage
After the encoding stage, the resultant embedding at each
location of the feature map is discretized to its closest dic-
tionary entry in the embedding space, resulting in FHR
q,
FHR#
q, and FLR
q. Motivated by [36], we introduce a lin-
ear projection to map the output feature maps to a low-
dimensional latent space (4-dim in our experiments) for
code index lookup. Then, the discretized embedding is pro-
jected back to the embedding space through another projec-
tion layer. During backpropagation, straight-through esti-
mator (STE) is employed for end-to-end optimization.
To associate HR and LR images, HR and LR dictionaries
should be coupled at this stage. However, there exists two
major challenges: (1) Misaligned Content. Under paired
settings, corresponding patches in HR and LR images share
the same content, which can be employed as a cue to couple
HR and LR dictionaries for joint optimization. However,
this cue does not hold for unpaired images, which imposes
great challenges to learning coupled dictionaries. (2) Di-
verse Degradations. Since an HR image corresponds to
inÔ¨Ånite LR images under various degradations, modeling
such a one-to-many correlation using coupled dictionaries
is quite difÔ¨Åcult. To address the above challenges, we for-
mulate the coupled dictionaries as a combination of an HR
content dictionary, an LR content dictionary, and a degra-
dation dictionary. The LR content dictionary couples with
the HR content dictionary to build connections between LR
and HR image contents. Meanwhile, the degradation dictio-
nary captures the degradation variations for different image
contents.
3.2.1 HR and LR Content Dictionary
First, we construct an HR content dictionary DHR2RN4
and an LR content dictionary DLR2RN4to model the
content variation in the embedding space, where Nis the
number of entries. To handle the Ô¨Årst challenge, we in-
troduce downscaled version of HR images as a bridge to
associate HR and LR content dictionaries to enable joint
training. For embeddings fHR2FHR(branch 1) andfHR#2FHR#(branch 2), the discretized embeddings fHR
q
andfHR#
q are obtained as:
i= arg min
ijjfHR# DLR(i)jj; (1)
(
fHR#
q =DLR(i)
fHR
q=DHR(i); (2)
whereiis the index of the closest entry in DLRto the em-
beddingfHR#. By employing the same spatial codes ( i.e.,
i) to represent both fHRandfHR#, HR and LR content
dictionaries are coupled. Inspired by [34], the discretiza-
tion losses for branches 1 and 2 are deÔ¨Åned as:
(
L1
dis=jjFHR FHR
qjj2
2
L2
dis=jjFHR# FHR#
qjj2
2: (3)
3.2.2 Degradation Dictionary
Second, we construct a degradation dictionary Ddeg2
RNK4on top of DLRto model the degradation varia-
tions. SpeciÔ¨Åcally, each group of entries ( e.g.,Ddeg(i)2
RK4) is used to capture the degradation variation around
the corresponding entry in the content dictionary ( i.e.,
DLR(i)). During discretization, a hierarchical approach is
employed to obtain the discretized embedding fLR
q:
8
><
>:j= arg min
jjjfLR DLR(j)jj
k= arg min
kjjfLR (DLR(j) +Ddeg(j; k))jj:(4)
For each embedding fLRinFLR(branch 3), the closest
entry in DLRis Ô¨Årst selected. DLR(j)depicts the clean
image content in fLRbut ignores the degradation. Then,
the degradation is taken into consideration by retrieving the
closest entry in DLR(j) +Ddeg(j), resulting in fLR
q. Simi-
larly, a discretization loss is deÔ¨Åned as:
L3
dis=jjFLR FLR
qjj2
2: (5)
3.3. Decoding Stage
After the discretization stage, the feature map produced by
the encoders are represented using the dictionaries. Then,
FHR
qis fed to the HR image decoder to reconstruct the HR
image ^IHR. Meanwhile, FHR#
q andFLR
qare passed to the
LR image decoder to produce ^IHR#and^ILR, respectively.
The reconstruction loss is deÔ¨Åned as the L1 loss between
the reconstructed results and the input images:
Lrec=jj^IHR IHRjj+jj(^IHR) (IHR)jj+
jj^IHR# IHR#jj+jj^ILR ILRjj;(6)
25715
where(IHR)refers to the feature maps extracted by a pre-
trained VGG model [37].
Similar with the encoders, the HR image decoder is com-
posed of four residual blocks at different resolution levels
and a tail convolutional layer to regress the reconstructed
images. Meanwhile, the LR image decoder comprises of
two residual blocks and a tail convolutional layer.
3.4. Loss Function
The overall loss function used for training is deÔ¨Åned as fol-
lows:
L=Lrec+1(L1
dis+L2
dis+L3
dis) (7)
where1is empirically set to 10 in our experiments.
4. Experiments
In this section, we Ô¨Årst introduce the experimental setup.
Then, we compare our method with previous paired and un-
paired image SR methods on benchmark datasets. Finally,
we conduct experiments to investigate the effectiveness of
our major designs.
4.1. Experimental Setup
4.1.1 Datasets and Metrics
We conduct experiments on two widely applied unpaired
SR datasets, including AIM-RWSR [38] and NTIRE-
RWSR [39]. Note that, we focus on 4SR in this paper
and our method can be extended to SR tasks with different
scale factors.
AIM-RWSR. The AIM-RWSR dataset is a synthetic
dataset and uses handcrafted degradations to synthesize
2650 noisy and compressed LR images. Meanwhile, 800
images in the DIV2K dataset are employed as HR images.
NTIRE-RWSR. The NTIRE-RWSR dataset follows the
same setting as the AIM-RWSR dataset to produce unpaired
HR and LR images. Different from AIM-RWSR, NTIRE-
RWSR features a more complicated degradation type that
consists of highly related high-frequency noises.
For evaluation on the AIM-RWSR and NTIRE-RWSR
datasets, we use peak signal-to-noise ratio (PSNR), struc-
tural similarity (SSIM) and learned perceptual image patch
similarity (LPIPS) as metrics.
4.1.2 Training Details
During the training phase, 24 LR patches of size 6464and
24 HR patches of size 256256were randomly cropped
from LR and HR images, respectively. Random rotation
and random Ô¨Çipping were employed for data augmentation.
The Adam method [40] with 1= 0:9and2= 0:999was
used for optimization. The learning rate was initialized as
210 4and halved after every 25 epochs. The training wasTable 1. Results achieved on AIM-RWSR for 4SR. Best and
second best results are highlighted and underlined .
Method #Layers MACs LPIPS PSNR SSIMPairedBicubic - - 0.537 22.35 0.617
RCAN [3] 400 260G 0.472 22.32 0.604
ZSSR [41] 8 - 0.639 22.21 0.603
IKC [5] 188 80G 0.479 22.25 0.600
DAN [42] 171 315G 0.471 22.41 0.609
BSRGAN [17] 350 291G 0.299 22.47 0.623
Real-ESRGAN [16] 350 291G 0.238 22.08 0.622UnpairedCycleGAN [38] 350 291G 0.476 21.19 0.530
CinCGAN [9] 37 823G 0.461 21.60 0.613
Lugmayr et al. [23] 350 291G 0.472 21.59 0.550
FSSR [43] 350 291G 0.390 20.82 0.510
DASR [8] 350 291G 0.336 21.60 0.564
DeFlow [26] 350 291G 0.349 22.25 0.620
PCR-ESRGAN [46] 350 291G 0.321 21.59 0.610
DictSR (Ours) 18 129G 0.259 22.46 0.629
stopped after 100 epochs. In our experiments, the numbers
of entries in the dictionaries ( i.e.,NandK) were set to
4096 and 32, respectively.
4.2. Performance Evaluation
For performance evaluation, six representative paired SR
methods (RCAN [3], ZSSR [41], IKC [5], DAN [42], BSR-
GAN [17] and Real-ESRGAN [16]) and eight state-of-the-
art unpaired image SR methods (CinCGAN [9], Lugmayr
et al. [23], FSSR [43], Impressionism [44], DASR [8], De-
Flow [26], DAP [45], and PCR-ESRGAN [46]) are included
for comparison. Quantitative results are presented in Ta-
bles 1 and 2 while visual results are provided in Fig. 3. Note
that, Lugmayr et al. , FSSR, DASR, DeFlow, Impression-
ism, and DAP employ ESRGAN as the SR model. For our
DictSR, the LR image encoder and the HR image decoder
are included to calculate the numbers of layers since only
these modules are employed during inference. For other
methods, the numbers of layers in the SR networks are pre-
sented. MACs (multiply-accumulate operations) is calcu-
lated based on 128128input LR images. The MACs of
ZSSR is not reported since this method conducts training
during the inference time and requires considerable compu-
tational cost.
Quantitative Results. For the AIM-RWSR dataset, it can
be observed from Table 1 that our DictSR achieves the best
performance in terms of all metrics among all unpaired SR
methods. Moreover, our DictSR also produces competi-
tive results as compared to paired SR methods. Previous
unpaired SR methods synthesize pseudo LR images to as-
sociate unpaired LR and HR images. However, the large
image space poses great challenges to these methods to
balance the realness and diversity. In contrast, by build-
ing the connection between unpaired images in a compact
25716
LRFSSR DASR
DeFlow DictSR (Ours)Bicubic
GTLR
FSSR DASR
DeFlow DictSR (Ours)BicubicGT
Figure 3. Visualization results produced by different methods.
proxy space using dictionaries, our DictSR produces supe-
rior quantitative results. In addition, our DictSR employs
a shallow and efÔ¨Åcient network structure with much lower
computational complexity. SpeciÔ¨Åcally, our method grad-
ually reduces the spatial resolution of feature maps to1
4
size while previous methods maintain full-resolution fea-
ture maps. As compared to previous methods, our 18-
layer network produces superior performance with only half
MACs, which further demonstrates the effectiveness of our
method.
For the NTIRE-RWSR dataset, we can observe from Ta-
ble 2 that our DictSR also produces competitive results. As
compared to DeFlow, although the improvements in terms
of PSNR and SSIM are marginal, our method produces no-table gains on LPIPS (0.204 vs. 0.218). This demonstrates
the superior perceptual quality of our results. In addition,
our DictSR also surpasses most previous paired methods
with much a shallower network. This further shows the
great potential of our learned coupled dictionaries.
Qualitative Results. Figure 3 compares the visual results
produced by different unpaired image SR methods. It can be
observed that our method produces higher perceptual qual-
ity with fewer artifacts. For example, in the second scene,
FSSR and DeFlow produces results with unpleasant ringing
artifacts while DASR suffer notable blurring artifacts. In
contrast, the results of our DisctSR can better recover the
text details. This further demonstrates the superiority of our
method.
25717
Table 2. Results achieved on NTIRE-RWSR for 4SR. Best and
second best results are highlighted and underlined .
Method #Layers MACs LPIPS PSNR SSIMPairedBicubic - - 0.632 25.52 0.671
RCAN [3] 400 260G 0.576 25.31 0.640
ZSSR [41] 8 - 0.620 24.93 0.642
IKC [5] 188 80G 0.384 26.50 0.748
DAN [42] 171 315G 0.554 25.15 0.671
BSRGAN [17] 350 291G 0.265 24.56 0.669
Real-ESRGAN [16] 350 291G 0.251 24.68 0.687UnpairedCycleGAN [38] 350 291G 0.417 24.75 0.700
CinCGAN [9] 37 823G - 24.19 0.683
Impressionism [44] 350 291G 0.232 24.67 0.683
FSSR [43] 350 291G 0.332 23.04 0.590
DeFlow [26] 350 291G 0.218 25.87 0.710
DAP [45] 350 291G 0.252 25.40 0.707
PCR-ESRGAN [46] 350 291G 0.223 24.97 0.682
DictSR (Ours) 18 129G 0.204 25.90 0.711
Figure 4. Visualization of the embedding space. Blue circles rep-
resent the embeddings of LR patches while other 100 stars corre-
spond to 100 entries in the LR content dictionary DLR.
4.3. Model Analyses
In this subsection, we conduct experiments to study our
method from three aspects. First, we study our dictionar-
ies. Then, we demonstrate the effectiveness of our learn-
ing framework. Finally, we investigate the effect of differ-
ent encoder and decoder architectures. All experiments are
conducted on the AIM-RWSR dataset.
4.3.1 Dictionaries
(1) Hierarchical Structure
To cover diverse degradations in LR images while asso-
ciating HR and LR image contents, a combination of an LR
content dictionary DLRand a degradation dictionary Ddeg
is employed. To demonstrate the effectiveness of such a hi-
erarchical structure, we remove Ddegand directly use DLRto discretize feature maps extracted from LR images ( i.e.,
A1 in Table 3). It can be observed that model A1 suffers
an accuracy drop as compared to the full model (Ours in
Table 3). Without the hierarchical structure, the LR con-
tent dictionary DLRcannot well cover the large degradation
space, thereby producing limited performance. In contrast,
the hierarchical structure facilitates our method to achieve
notable gains.
(2) Dictionary Size
Intuitively, larger dictionaries contribute to larger model
capacity and ultimately result in higher performance. Con-
sequently, we conduct experiments to study the effect of
dictionary size from the following three aspects.
Entry. We Ô¨Årst conduct experiments to investigate dic-
tionaries with different numbers of entries. SpeciÔ¨Åcally, we
develop two network variants (B1/B2 in Table 3) by em-
ploying dictionaries with more/fewer entries. Using dictio-
naries with fewer entries (2048 entries), the performance
of model B1 is slightly lower than our full model. How-
ever, further including more entries (8192 entries) cannot
introduce consistent gains on all metrics. Consequently, we
employ 4096 entries as the default setting.
Dimension. During the discretization stage, a linear
projection is employed to map the feature maps to a low-
dimensional latent space for code index lookup. Here,
we conduct experiments to study the dimension of the
latent space. As the dimension of the latent codes is
reduced from 64 to 4, the number of activated entries
is increased from 105 to 4050. This indicates that the
dictionary can better cover the embeddings in the low-
dimensional latent space to activate more entries. As a
results, the LPIPS/PSNR/SSIM scores are improved from
0.268/21.99/0.616 to 0.259/22.46/0.629. In our experi-
ments, the dimension of 4 is adopted as the default setting.
(3) Visualization of Dictionaries
To further study the embedding space of LR feature
maps, we visualize the entries of the LR content dictionary
DLRand the embeddings of FLRin Fig. 4. As we can see,
the entries in DLRspan the space of embeddings extracted
from LR patches. This validates that DLRcan well cover
the content variation in the LR embedding space.
4.3.2 Learning Framework
(1) Encoder
Each entry in our LR/HR dictionaries corresponds a
22=88patch in the LR/HR image. We further conduct ex-
periments to investigate the effect of the patch size. Specif-
ically, we deepen the LR/HR encoders to encode larger
patches into embeddings for discretization. Small patch size
cannot well distinguish the degradations and image con-
tents in the LR image. As a result, the degradations will
also be reconstructed in the SR results with inferior perfor-
25718
Table 3. Results achieved by our method with different settings.
Dictionary Framework Activated
Entries ( DLR)LPIPS (#) PSNR (") SSIM (")
Hierarchical Entries Dim Patch (LR/HR) Coupling
A1 7 4096 4 22=88 3 3560 0.281 22.40 0.619
B1 3 2048 4 22=88 3 2027 0.278 22.41 0.621
B2 3 8192 4 22=88 3 8050 0.269 22.44 0.626
C1 3 4096 16 22=88 3 568 0.262 22.25 0.625
C2 3 4096 64 22=88 3 105 0.268 21.99 0.616
D1 3 4096 4 11=44 3 1522 0.271 22.41 0.623
D2 3 4096 4 44=1616 3 3810 0.313 21.43 0.588
E1 3 4096 4 22=88 7 4096 0.533 8.28 0.050
Ours 3 4096 4 22=88 3 4050 0.259 22.46 0.629
Table 4. Results achieved by our method with different network architectures. EHR,DHR,ELRandDLRrepresent HR image encoder,
HR image decoder, LR image encoder, and LR image decoder, respectively.
EHR DHR ELR DLR LPIPS (#) PSNR (") SSIM (")
F1 20-layer 12-layer 6-layer 6-layer 0.271 22.46 0.625
F2 12-layer 20-layer 6-layer 6-layer 0.256 22.49 0.631
F3 12-layer 12-layer 14-layer 6-layer 0.263 22.48 0.629
F4 12-layer 12-layer 6-layer 14-layer 0.261 22.48 0.630
Ours 12-layer 12-layer 6-layer 6-layer 0.259 22.46 0.629
mance. Although large patch size can identify structured
image contents from degradations, the reconstructed SR re-
sults are usually of high quality but low Ô¨Ådelity [47]. In
summary, a reasonable patch size is critical to the perfor-
mance. Compared to model D1 and D2, our model produces
superior accuracy in terms of all metrics. Consequently,
patch sizes of 22and88are adopted as the default
setting.
(2) Coupling Operation
By employing the same spatial codes over HR content
dictionary DHRand LR content dictionary DLRto repre-
sent paired HR and LR images, these dictionaries are tightly
coupled. To investigate its effectiveness, we introduce a net-
work variant (E1 in Table 3) by removing this coupling op-
eration. SpeciÔ¨Åcally, HR and LR feature maps are repre-
sented using separate spatial codes during the discretization
stage. From Table 3 we can see that model E1 produces ex-
tremely low quantitative scores. Without the coupling op-
eration, the HR and LR content dictionaries are not associ-
ated such that the reconstructed SR result does not share the
same content with the LR input. In contrast, the coupling
operation enables our model to produce SR results faithful
to the LR image.
4.3.3 Network Architecture
Our network consists of four modules, including HR im-
age encoder, HR image decoder, LR image encoder, and
LR image decoder. We conduct experiments to study the
effect of the model capacity for these four modules. Specif-ically, we Ô¨Årst develop four network variants (F1-F4 in Ta-
ble 4) by deepening these four modules. As we can see,
our method does not obtain consistent performance gains
as the HR encoder, LR encoder and LR decoder are deep-
ened. Meanwhile, with larger HR decoder, model F2 pro-
duces slight improvements on all metrics. This indicates
that our method relies more on the design of dictionaries
(Table 3) rather than the designs of the network architec-
ture. Despite the marginal improvements brought by larger
HR decoder, a 12-layer decoder is employed to achieve the
accuracy-efÔ¨Åciency balance.
5. Conclusion
In this paper, we propose to associate unpaired LR and HR
images using coupled dictionaries rather than synthesizing
pseudo LR images. SpeciÔ¨Åcally, we construct an HR con-
tent dictionary, an LR content dictionary, and a degradation
dictionary to model unpaired HR and LR images. To couple
these dictionaries, we develop an autoencoder-based frame-
work for optimization. By representing images using com-
pact proxy codes, an LR-to-HR mapping can be effectively
learned by our dictionaries. Experiments on several bench-
marks show that our method produces superior performance
against previous state-of-the-art approaches.
Acknowledgments: This work was partially supported by the Na-
tional Natural Science Foundation of China (No. 62301601, 62301306,
and 62306331), the Shenzhen Science and Technology Program (No.
RCYX20200714114641140), the Guangdong Basic and Applied Basic
Research Foundation (2022B1515020103), and the Young Elite Scientists
Sponsorship Program by CAST (No. 2023QNRC001).
25719
References
[1] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Learning a deep convolutional network for image
super-resolution. In ECCV , pages 184‚Äì199, 2014. 1, 2
[2] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and
Kyoung Mu Lee. Enhanced deep residual networks for single
image super-resolution. In CVPRW , pages 136‚Äì144, 2017.
[3] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng
Zhong, and Yun Fu. Image super-resolution using very deep
residual channel attention networks. In ECCV , pages 1646‚Äì
1654, 2018. 2, 5, 7
[4] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc
Van Gool, and Radu Timofte. SwinIR: Image restoration
using swin transformer. In ICCVW , pages 1833‚Äì1844, 2021.
1
[5] Jinjin Gu, Hannan Lu, Wangmeng Zuo, and Chao Dong.
Blind super-resolution with iterative kernel correction. In
CVPR , pages 1604‚Äì1613, 2019. 1, 2, 5, 7
[6] Zongsheng Yue, Qian Zhao, Jianwen Xie, Lei Zhang, Deyu
Meng, and Kwan-Yee K Wong. Blind image super-resolution
with elaborate degradation modeling on noise and kernel. In
CVPR , pages 2128‚Äì2138, 2022. 1
[7] Shunta Maeda. Unpaired image super-resolution using
pseudo-supervision. In CVPR , pages 291‚Äì300, 2020. 1, 2
[8] Yunxuan Wei, Shuhang Gu, Yawei Li, Radu Timofte, Long-
cun Jin, and Hengjie Song. Unsupervised real-world im-
age super resolution via domain-distance aware training. In
CVPR , pages 13385‚Äì13394, 2021. 1, 5
[9] Yuan Yuan, Siyuan Liu, Jiawei Zhang, Yongbing Zhang,
Chao Dong, and Liang Lin. Unsupervised image super-
resolution using cycle-in-cycle generative adversarial net-
works. In CVPRW , pages 701‚Äì710, 2018. 1, 2, 5, 7
[10] Tao Yang, Peiran Ren, Lei Zhang, et al. Synthesizing real-
istic image restoration training pairs: A diffusion approach.
arXiv , 2023. 1, 2
[11] Kaibing Zhang, Xinbo Gao, Dacheng Tao, and Xuelong Li.
Multi-scale dictionary for single image super-resolution. In
CVPR , pages 1114‚Äì1121, 2012. 1, 2
[12] Jianchao Yang, Zhaowen Wang, Zhe Lin, S. Cohen, and
T. Huang. Coupled dictionary training for image super-
resolution. IEEE Transactions on Image Processing ,
21(8):3467‚Äì3478, aug 2012. 1, 2
[13] Xiangyu Chen, Xintao Wang, Jiantao Zhou, and Chao
Dong. Activating more pixels in image super-resolution
transformer. In CVPR , 2023. 2
[14] Ziwei Luo, Haibin Huang, Lei Yu, Youwei Li, Haoqiang
Fan, and Shuaicheng Liu. Deep constrained least squares for
blind image super-resolution. In CVPR , pages 17642‚Äì17652,
2022.
[15] Zheng Chen, Yulun Zhang, Jinjin Gu, Linghe Kong, Xi-
aokang Yang, and Fisher Yu. Dual aggregation transformer
for image super-resolution. In ICCV , pages 12312‚Äì12321,
2023. 2
[16] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.
Real-ESRGAN: Training real-world blind super-resolution
with pure synthetic data. In ICCVW , pages 1905‚Äì1914, 2021.
2, 5, 7[17] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timo-
fte. Designing a practical degradation model for deep blind
image super-resolution. In ICCV , pages 4791‚Äì4800, 2021.
5, 7
[18] Xuhai Chen, Jiangning Zhang, Chao Xu, Yabiao Wang,
Chengjie Wang, and Yong Liu. Better‚Äù cmos‚Äù produces
clearer images: Learning space-variant blur estimation for
blind image super-resolution. In CVPR , pages 1651‚Äì1661,
2023.
[19] Zeshuai Deng, Zhuokun Chen, Shuaicheng Niu, Thomas Li,
Bohan Zhuang, and Mingkui Tan. EfÔ¨Åcient test-time adapta-
tion for super-resolution with second-order degradation and
reconstruction. In NeurIPS , volume 36, 2024. 2
[20] Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei
Zhang. Toward real-world single image super-resolution: A
new benchmark and a new model. In ICCV , pages 3086‚Äì
3095, 2019. 2
[21] Chang Chen, Zhiwei Xiong, Xinmei Tian, Zheng-Jun Zha,
and Feng Wu. Camera lens super-resolution. In CVPR , pages
1652‚Äì1660, 2019. 2
[22] Adrian Bulat, Jing Yang, and Georgios Tzimiropoulos. To
learn image super-resolution, use a GAN to learn how to do
image degradation Ô¨Årst. In ECCV , pages 185‚Äì200, 2018. 2
[23] Andreas Lugmayr, Martin Danelljan, and Radu Timofte. Un-
supervised learning for real-world super-resolution. In IC-
CVW , pages 3408‚Äì3416, 2019. 2, 5
[24] Yang Liu, Ziyu Yue, Jinshan Pan, and Zhixun Su. Unpaired
learning for deep image deraining with rain direction regu-
larizer. In ICCV , pages 4753‚Äì4761, 2021. 2
[25] Yang Yang, Chaoyue Wang, Risheng Liu, Lin Zhang, Xiao-
jie Guo, and Dacheng Tao. Self-augmented unpaired image
dehazing via density and depth decomposition. In CVPR ,
pages 2037‚Äì2046, 2022. 2
[26] Valentin Wolf, Andreas Lugmayr, Martin Danelljan, Luc
Van Gool, and Radu Timofte. DeFlow: Learning com-
plex image degradations from unpaired data with conditional
Ô¨Çows. In CVPR , pages 94‚Äì103, 2021. 2, 5, 7
[27] Ivan Kobyzev, Simon JD Prince, and Marcus A Brubaker.
Normalizing Ô¨Çows: An introduction and review of current
methods. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence , 43(11):3964‚Äì3979, 2020. 2
[28] George Papamakarios, Eric Nalisnick, Danilo Jimenez
Rezende, Shakir Mohamed, and Balaji Lakshminarayanan.
Normalizing Ô¨Çows for probabilistic modeling and inference.
Journal of Machine Learning Research , 22(57):1‚Äì64, 2021.
2
[29] Julien Mairal, Francis Bach, Jean Ponce, and Guillermo
Sapiro. Online dictionary learning for sparse coding. In
ICML , pages 689‚Äì696, 2009. 2
[30] Ivana To Àási¬¥c and Pascal Frossard. Dictionary learning. IEEE
Signal Processing Magazine , 28(2):27‚Äì38, 2011. 2
[31] Xiaoqiang Lu, Haoliang Yuan, Pingkun Yan, Yuan Yuan, and
Xuelong Li. Geometry constrained sparse coding for single
image super-resolution. In CVPR , pages 1648‚Äì1655, 2012.
2
[32] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete
representation learning. NeurIPS , 30, 2017. 2
25720
[33] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gen-
erating diverse high-Ô¨Ådelity images with vq-vae-2. NeurIPS ,
32, 2019. 2
[34] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
transformers for high-resolution image synthesis. In CVPR ,
pages 12873‚Äì12883, 2021. 3, 4
[35] Shunta Maeda. Image super-resolution with deep dictionary.
InECCV , pages 464‚Äì480, 2022. 3
[36] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,
James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,
and Yonghui Wu. Vector-quantized image modeling with
improved vqgan. In ICLR , 2021. 4
[37] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR , pages 586‚Äì595,
2018. 5
[38] Andreas Lugmayr, Martin Danelljan, Radu Timofte, Manuel
Fritsche, Shuhang Gu, Kuldeep Purohit, Praveen Kandula,
Maitreya Suin, AN Rajagoapalan, Nam Hyung Joon, et al.
AIM 2019 challenge on real-world image super-resolution:
Methods and results. In ICCVW , pages 3575‚Äì3583, 2019. 5,
7
[39] Andreas Lugmayr, Martin Danelljan, and Radu Timo-
fte. NTIRE 2020 challenge on real-world image super-
resolution: Methods and results. In CVPRW , pages 494‚Äì495,
2020. 5
[40] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In ICLR , 2015. 5
[41] Assaf Shocher, Nadav Cohen, and Michal Irani. ‚ÄùZero-shot‚Äù
super-resolution using deep internal learning. In CVPR ,
pages 3118‚Äì3126, 2018. 5, 7
[42] Zhengxiong Luo, Yang Huang, Shang Li, Liang Wang, and
Tieniu Tan. Unfolding the alternating optimization for blind
super resolution. In NeurIPS , pages 5632‚Äì5643, 2020. 5, 7
[43] Manuel Fritsche, Shuhang Gu, and Radu Timofte. Frequency
separation for real-world super-resolution. In ICCVW , pages
3599‚Äì3608, 2019. 5, 7
[44] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li,
and Feiyue Huang. Real-world super-resolution via kernel
estimation and noise injection. In CVPRW , pages 466‚Äì467,
2020. 5, 7
[45] Wei Wang, Haochen Zhang, Zehuan Yuan, and Changhu
Wang. Unsupervised real-world super-resolution: A domain
adaptation perspective. In ICCV , pages 4318‚Äì4327, 2021. 5,
7
[46] Andr ¬¥es Romero, Luc Van Gool, and Radu Timofte. Unpaired
real-world super-resolution with pseudo controllable restora-
tion. In CVPRW , pages 798‚Äì807, 2022. 5, 7
[47] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen
Li, Ying Shan, and Ming-Ming Cheng. Vqfr: Blind face
restoration with vector-quantized dictionary and parallel de-
coder. In ECCV , pages 126‚Äì143. Springer, 2022. 8
25721
