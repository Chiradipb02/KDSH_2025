Robust Image Denoising through Adversarial Frequency Mixup
Donghun Ryou2Inju Ha1Hyewon Yoo2Dongwan Kim1Bohyung Han1,2
Computer Vision Laboratory,1ECE &2IPAI, Seoul National University
{dhryou, hij1112, yoohyewony, dongwan123, bhhan }@snu.ac.kr
Abstract
Image denoising approaches based on deep neural net-
works often struggle with overfitting to specific noise dis-
tributions present in training data. This challenge per-
sists in existing real-world denoising networks, which are
trained using a limited spectrum of real noise distributions,
and thus, show poor robustness to out-of-distribution real
noise types. To alleviate this issue, we develop a novel
training framework called Adversarial Frequency Mixup
(AFM). AFM leverages mixup in the frequency domain
to generate noisy images with distinctive and challenging
noise characteristics, all the while preserving the properties
of authentic real-world noise. Subsequently, incorporat-
ing these noisy images into the training pipeline enhances
the denoising network’s robustness to variations in noise
distributions. Extensive experiments and analyses, con-
ducted on a wide range of real noise benchmarks demon-
strate that denoising networks trained with our proposed
framework exhibit significant improvements in robustness
to unseen noise distributions. The code is available at
https://github.com/dhryougit/AFM.
1. Introduction
Image denoising based on deep neural networks [5, 6, 22,
26–28, 30, 31] has witnessed unprecedented success ben-
efiting from the simplicity of the problem formulation and
the construction of new datasets. Traditionally, denoising
networks have been trained using synthetic noise models,
such as Gaussian or Poisson noise, which are artificially
added to images for training and evaluation purposes. How-
ever, real-world noise, influenced by various factors within
the Image Signal Processing (ISP) pipeline such as demo-
saicing and gamma correction, exhibits a distinct signal de-
pendency and often follows distributions that differ from
synthetic counterparts. This divergence between synthetic
and real-world noise distributions raises significant general-
ization issues when applying denoising models to real noisy
images.
While efforts have been devoted to creating datasets with
SIDD Noisy
 Restormer 39.26 dB
 Ours 38.16 dB
CC Noisy
 Restormer 31.95 dB
 Ours 33.79 dB
Poly Noisy
 Restormer 38.37 dB
 Ours 39.13 dB
Figure 1. The difference in real-world denoising results for in-
distribution and out-of-distribution samples. While the state-of-
the-art Restormer [28] performs well on in-distribution samples
from SIDD [1], its performance is degraded on out-of-distribution
examples from CC [16] and Poly [23]. In contrast, a DnCNN [30]
model trained with our AFM exhibits robustness for both samples.
clean and noisy image pairs in the real-world, collecting
these datasets poses a significant challenge. Thus, vari-
ous self-supervised approaches [10, 12, 18] have emerged
as promising solutions for image denoising, aiming to re-
duce the dependence on paired noisy-clean image datasets.
Nevertheless, these methods often fall short in performance
when compared to supervised learning approaches. As a
result, supervision on real noise datasets still remains favor-
able in practical scenarios.
Image denoisers trained with supervision on real noise
datasets, however, come with their own set of flaws. Most
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
2723
Noisy freq.
 GT freq.
 Output freq.
Figure 2. Visualizing the frequency magnitudes of denoising
results for an in-distribution (SIDD [1], top row) and out-of-
distribution (Poly [23], bottom row) sample, using a model trained
without AFM. For the in-distribution sample, the model yields a
cleaner frequency map after denoising than the ground truth, but
for the out-of-distribution sample, the model struggles to denoise
certain regions in the frequency map.
notably, we observe that even state-of-the-art denoisers en-
counter difficulty generalizing to variations in noise distri-
butions, which arise from factors such as different camera
sensor types, shooting environments, and ISP processes.
We illustrate this phenomenon in Figure 1, and provide a
frequency analysis in Figure 2. While this lack of real noise
generalization has a detrimental effect on the widespread
use of denoising models, it is a relatively unexplored issue
in the field of image denoising research.
In this work, we propose Adversarial Frequency Mixup
(AFM), a model-agnostic training framework that improves
the generalizability of denoising networks to variations in
real noise distributions. To this end, AFM constructs im-
ages with noise that is unique and difficult to denoise, while
maintaining the properties of real-world noise. Then, by
integrating these noisy images into the training pipeline,
the denoising network becomes robust to variations in real
noise distributions. More specifically, AFM works by mix-
ing up a noisy and denoised image according to a mixup
mask in the frequency domain. This mixup mask is gener-
ated by a separate lightweight neural network, which yields
two variants of our approach: AFM-E and AFM-B. AFM-E
generates the mixup mask in an element-wise manner, while
AFM-B generates the mask by assigning a mixup value for
each frequency band. Moreover, AFM is trained using an
adversarial loss and can generate adversarial mixup masks.
Overall, our contributions are summarized as follows:
• We propose AFM, a model-agnostic training framework
that improves the denoising network’s generalization and
robustness to real-world out-of-distribution noise images
by generating new noisy images through an adversarialmixup in the frequency domain.
• We design two adversarial mask generation networks,
AFM-E and AFM-B. AFM-E generates mixup masks in
an element-wise fashion while AFM-B generates mixup
masks for individual bands.
• We demonstrate the effectiveness of AFM on multi-
ple real-world out-of-distribution image denoising bench-
marks. Our method consistently outperforms ordinary
training on various denoising architectures and even state-
of-the-art denoising models by significant margins.
2. Related Works
This section reviews existing image denoising models based
on deep neural networks with and without supervision and
discusses recent efforts for robust image denoising.
2.1. Supervised Image Denoising
In recent years, there have been significant advancements
in the area of supervised image denoising, where paired
noisy and clean images are available for training. The
initial breakthroughs were largely driven by CNN-based
models, with Denoising Convolutional Neural Network
(DnCNN) [30] leading the stage for further innovations in
this domain. Building upon this foundation, the U-Net-
based architectures [4, 5, 26, 27] have emerged as promi-
nent models, leveraging skip connections to effectively
combine local and global contextual information. Addition-
ally, the introduction of transformer-based models [22, 28]
marked a paradigm shift in denoising strategies. Equipped
with attention mechanisms, transformer-based models ex-
cel at identifying complex dependencies, effectively dimin-
ishing noise distortions in the process. These models have
shown exceptional skill in reducing noise, substantially mit-
igating the impact of specific noise distributions they have
been trained on using paired datasets. Yet, they encounter
challenges in effectively managing noise distributions that
diverge from those they have been trained on, which poses
hurdles for their application in real-world scenarios.
2.2. Self-supervised Image Denoising
Collecting clean and noisy pair data from the real-world
is cost-intensive. To mitigate these issues, there has been
an active research on exploring unsupervised and self-
supervised learning techniques like N2N [12], N2S [2],
N2V [10], and R2R [18]. These approaches demonstrate
the feasibility of training denoising networks using datasets
comprising solely noisy pairs or individual noisy images,
without the need for a clean and noisy pair dataset. More-
over, DIP [21], Self2Self [19] and Neighbor2Neighbor [7]
have introduced novel approaches to train denoising net-
works even in the absence of training data, by using a single
noisy image to produce a clean counterpart. However, these
2724
Denoising NetworkDenoising NetworkAFM Module<latexit sha1_base64="NB3UBhddkD0vX6Dq0T3JyQ7ymN4=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi94SMA9IljA76SRjZmeXmVkhLPkCLx4U8eonefNvnCR70MSChqKqm+6uIBZcG9f9dnIrq2vrG/nNwtb2zu5ecf+goaNEMayzSESqFVCNgkusG24EtmKFNAwENoPR7dRvPqHSPJIPZhyjH9KB5H3OqLFS7b5bLLlldwayTLyMlCBDtVv86vQiloQoDRNU67bnxsZPqTKcCZwUOonGmLIRHWDbUklD1H46O3RCTqzSI/1I2ZKGzNTfEykNtR6Hge0MqRnqRW8q/ue1E9O/9lMu48SgZPNF/UQQE5Hp16THFTIjxpZQpri9lbAhVZQZm03BhuAtvrxMGmdl77J8UTsvVW6yOPJwBMdwCh5cQQXuoAp1YIDwDK/w5jw6L8678zFvzTnZzCH8gfP5A6I9jNc=</latexit>I<latexit sha1_base64="FuhhAouaiUhaxnlmSrc7u85g/00=">AAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi94imAckS5idzCZDZmeXmV4hLPkILx4U8er3ePNvnCR70MSChqKqm+6uIJHCoOt+O4WV1bX1jeJmaWt7Z3evvH/QNHGqGW+wWMa6HVDDpVC8gQIlbyea0yiQvBWMbqd+64lrI2L1iOOE+xEdKBEKRtFKre6QYnY/6ZUrbtWdgSwTLycVyFHvlb+6/ZilEVfIJDWm47kJ+hnVKJjkk1I3NTyhbEQHvGOpohE3fjY7d0JOrNInYaxtKSQz9fdERiNjxlFgOyOKQ7PoTcX/vE6K4bWfCZWkyBWbLwpTSTAm099JX2jOUI4toUwLeythQ6opQ5tQyYbgLb68TJpnVe+yevFwXqnd5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8AfO5w9sEY+k</latexit>ˆI
<latexit sha1_base64="y8mtd1893tYkN04kyTDRpkShYWc=">AAAB+HicbVDLSsNAFJ3UV62PRl26GSyCq5KIr2XRje4q2Ae0IUwm03boZBJmbsQa8iVuXCji1k9x5984bbPQ6oELh3Pu5d57gkRwDY7zZZWWlldW18rrlY3Nre2qvbPb1nGqKGvRWMSqGxDNBJesBRwE6yaKkSgQrBOMr6Z+554pzWN5B5OEeREZSj7glICRfLua3eR+H9gDZCOiwty3a07dmQH/JW5BaqhA07c/+2FM04hJoIJo3XOdBLyMKOBUsLzSTzVLCB2TIesZKknEtJfNDs/xoVFCPIiVKQl4pv6cyEik9SQKTGdEYKQXvan4n9dLYXDhZVwmKTBJ54sGqcAQ42kKOOSKURATQwhV3NyKqfmfUDBZVUwI7uLLf0n7uO6e1U9vT2qNyyKOMtpHB+gIuegcNdA1aqIWoihFT+gFvVqP1rP1Zr3PW0tWMbOHfsH6+AZ6NpOj</latexit>Ihard<latexit sha1_base64="OD+SpEzhoYDg0cIRyrvH92OQH9E=">AAAB/HicbVDLSsNAFJ3UV62vaJduBovgqiTia1l0o7sK9gFNCJPJpB06eTBzI5YQf8WNC0Xc+iHu/BunbRZaPXDhcM693HuPnwquwLK+jMrS8srqWnW9trG5tb1j7u51VZJJyjo0EYns+0QxwWPWAQ6C9VPJSOQL1vPHV1O/d8+k4kl8B5OUuREZxjzklICWPLPujAjkN4XnAHuAfERkUHhmw2paM+C/xC5JA5Voe+anEyQ0i1gMVBClBraVgpsTCZwKVtScTLGU0DEZsoGmMYmYcvPZ8QU+1EqAw0TqigHP1J8TOYmUmkS+7owIjNSiNxX/8wYZhBduzuM0AxbT+aIwExgSPE0CB1wyCmKiCaGS61sx1f8TCjqvmg7BXnz5L+keN+2z5untSaN1WcZRRfvoAB0hG52jFrpGbdRBFE3QE3pBr8aj8Wy8Ge/z1opRztTRLxgf35ULlWQ=</latexit>ˆIhard<latexit sha1_base64="+JKePTREmwZfw+bDQr94T62eiAA=">AAAB8nicbVDLSgNBEJyNrxhfUY9eFoPgKeyKr2PQi94imAdsljA76SRDZmeWmV4xLPkMLx4U8erXePNvnCR70GhBQ1HVTXdXlAhu0PO+nMLS8srqWnG9tLG5tb1T3t1rGpVqBg2mhNLtiBoQXEIDOQpoJxpoHAloRaPrqd96AG24kvc4TiCM6UDyPmcUrRTcdjsIj5gNcNItV7yqN4P7l/g5qZAc9W75s9NTLI1BIhPUmMD3EgwzqpEzAZNSJzWQUDaiAwgslTQGE2azkyfukVV6bl9pWxLdmfpzIqOxMeM4sp0xxaFZ9Kbif16QYv8yzLhMUgTJ5ov6qXBRudP/3R7XwFCMLaFMc3ury4ZUU4Y2pZINwV98+S9pnlT98+rZ3WmldpXHUSQH5JAcE59ckBq5IXXSIIwo8kReyKuDzrPz5rzPWwtOPrNPfsH5+Aa3OZGO</latexit>Igt
FFTFFTMask Generation NetworkAFM-E or   AFM-BMask
IFFTIFFT
<latexit sha1_base64="xfB69AZWhj0EFXEkugFPQcrwPSg=">AAACDXicbZDLSsNAFIZP6q3WW9Slm2AV6qYk4m1ZFER3FawttKFMppN26GQSZiZCiX0BN76KGxeKuHXvzrdx0gaprQcGPv7/HOac34sYlcq2v43c3PzC4lJ+ubCyura+YW5u3ckwFpjUcMhC0fCQJIxyUlNUMdKIBEGBx0jd61+kfv2eCElDfqsGEXED1OXUpxgpLbXNvVaAVA8jljwMf/FyWLo+mDDaZtEu26OyZsHJoAhZVdvmV6sT4jggXGGGpGw6dqTcBAlFMSPDQiuWJEK4j7qkqZGjgEg3GV0ztPa10rH8UOjHlTVSJycSFEg5CDzdma4op71U/M9rxso/cxPKo1gRjscf+TGzVGil0VgdKghWbKABYUH1rhbuIYGw0gEWdAjO9MmzcHdYdk7KxzdHxcp5FkcedmAXSuDAKVTgCqpQAwyP8Ayv8GY8GS/Gu/Exbs0Z2cw2/Cnj8weIeJx/</latexit>|F(I)|
<latexit sha1_base64="KXY/vqhaVNCeTgMRIFbXZAco+xY=">AAAB/3icbVDLSsNAFJ34rPUVFdy4GSxC3ZREfC2Lguiugn1AE8pkOm2HTh7M3AglzcJfceNCEbf+hjv/xkmbhbYeGDiccy/3zPEiwRVY1rexsLi0vLJaWCuub2xubZs7uw0VxpKyOg1FKFseUUzwgNWBg2CtSDLie4I1veF15jcfmVQ8DB5gFDHXJ/2A9zgloKWOuT92fAIDSkRyk5adAYHkLj0ed8ySVbEmwPPEzkkJ5ah1zC+nG9LYZwFQQZRq21YEbkIkcCpYWnRixSJCh6TP2poGxGfKTSb5U3yklS7uhVK/APBE/b2REF+pke/pySysmvUy8T+vHUPv0k14EMXAAjo91IsFhhBnZeAul4yCGGlCqOQ6K6YDIgkFXVlRl2DPfnmeNE4q9nnl7P60VL3K6yigA3SIyshGF6iKblEN1RFFY/SMXtGb8WS8GO/Gx3R0wch39tAfGJ8/FqaWKA==</latexit>|F(ˆI)|<latexit sha1_base64="FuhhAouaiUhaxnlmSrc7u85g/00=">AAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi94imAckS5idzCZDZmeXmV4hLPkILx4U8er3ePNvnCR70MSChqKqm+6uIJHCoOt+O4WV1bX1jeJmaWt7Z3evvH/QNHGqGW+wWMa6HVDDpVC8gQIlbyea0yiQvBWMbqd+64lrI2L1iOOE+xEdKBEKRtFKre6QYnY/6ZUrbtWdgSwTLycVyFHvlb+6/ZilEVfIJDWm47kJ+hnVKJjkk1I3NTyhbEQHvGOpohE3fjY7d0JOrNInYaxtKSQz9fdERiNjxlFgOyOKQ7PoTcX/vE6K4bWfCZWkyBWbLwpTSTAm099JX2jOUI4toUwLeythQ6opQ5tQyYbgLb68TJpnVe+yevFwXqnd5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8AfO5w9sEY+k</latexit>ˆI<latexit sha1_base64="NB3UBhddkD0vX6Dq0T3JyQ7ymN4=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi94SMA9IljA76SRjZmeXmVkhLPkCLx4U8eonefNvnCR70MSChqKqm+6uIBZcG9f9dnIrq2vrG/nNwtb2zu5ecf+goaNEMayzSESqFVCNgkusG24EtmKFNAwENoPR7dRvPqHSPJIPZhyjH9KB5H3OqLFS7b5bLLlldwayTLyMlCBDtVv86vQiloQoDRNU67bnxsZPqTKcCZwUOonGmLIRHWDbUklD1H46O3RCTqzSI/1I2ZKGzNTfEykNtR6Hge0MqRnqRW8q/ue1E9O/9lMu48SgZPNF/UQQE5Hp16THFTIjxpZQpri9lbAhVZQZm03BhuAtvrxMGmdl77J8UTsvVW6yOPJwBMdwCh5cQQXuoAp1YIDwDK/w5jw6L8678zFvzTnZzCH8gfP5A6I9jNc=</latexit>I<latexit sha1_base64="y8mtd1893tYkN04kyTDRpkShYWc=">AAAB+HicbVDLSsNAFJ3UV62PRl26GSyCq5KIr2XRje4q2Ae0IUwm03boZBJmbsQa8iVuXCji1k9x5984bbPQ6oELh3Pu5d57gkRwDY7zZZWWlldW18rrlY3Nre2qvbPb1nGqKGvRWMSqGxDNBJesBRwE6yaKkSgQrBOMr6Z+554pzWN5B5OEeREZSj7glICRfLua3eR+H9gDZCOiwty3a07dmQH/JW5BaqhA07c/+2FM04hJoIJo3XOdBLyMKOBUsLzSTzVLCB2TIesZKknEtJfNDs/xoVFCPIiVKQl4pv6cyEik9SQKTGdEYKQXvan4n9dLYXDhZVwmKTBJ54sGqcAQ42kKOOSKURATQwhV3NyKqfmfUDBZVUwI7uLLf0n7uO6e1U9vT2qNyyKOMtpHB+gIuegcNdA1aqIWoihFT+gFvVqP1rP1Zr3PW0tWMbOHfsH6+AZ6NpOj</latexit>Ihard
<latexit sha1_base64="ZOf4+yt265wE1vU+fF45KMpRvBE=">AAAB+HicbVDLSsNAFJ3UV62PRl26CRbBVUnE17LoRncV7APaECbTm3bo5MHMjVhDvsSNC0Xc+inu/BunbRZaPXDhcM693HuPnwiu0La/jNLS8srqWnm9srG5tV01d3bbKk4lgxaLRSy7PlUgeAQt5Cigm0igoS+g44+vpn7nHqTicXSHkwTckA4jHnBGUUueWc1ucq+P8IAZUDXJPbNm1+0ZrL/EKUiNFGh65md/ELM0hAiZoEr1HDtBN6MSOROQV/qpgoSyMR1CT9OIhqDcbHZ4bh1qZWAFsdQVoTVTf05kNFRqEvq6M6Q4UoveVPzP66UYXLgZj5IUIWLzRUEqLIytaQrWgEtgKCaaUCa5vtViIyopQ51VRYfgLL78l7SP685Z/fT2pNa4LOIok31yQI6IQ85Jg1yTJmkRRlLyRF7Iq/FoPBtvxvu8tWQUM3vkF4yPb5cNk7Y=</latexit>Ieasy
<latexit sha1_base64="/mhgF6MynT1PJWOKoz5djjTmICk=">AAACQ3icbVBNS8MwGE79dn5NPXoJDmFDHK34dRQF0dsEp8N1jDRNt7C0KclbYZT+Ny/+AW/+AS8eFPEqmG5DdPpCyMPzQd48Xiy4Btt+siYmp6ZnZufmCwuLS8srxdW1ay0TRVmdSiFVwyOaCR6xOnAQrBErRkJPsBuvd5rrN3dMaS6jK+jHrBWSTsQDTgkYql28dUMCXUpEepaV04us4kpfgutJ4et+aK40zPA2/ulyuwS+neWB4gWpk+38TlXaxZJdtQeD/wJnBEpoNLV28dH1JU1CFgEVROumY8fQSokCTgXLCm6iWUxoj3RY08CIhEy30kEHGd4yjI8DqcyJAA/Yn4mUhDrfzTjzjfW4lpP/ac0EgqNWyqM4ARbR4UNBIjBInBeKfa4YBdE3gFDFza6YdokiFEztBVOCM/7lv+B6t+ocVPcv90rHJ6M65tAG2kRl5KBDdIzOUQ3VEUX36Bm9ojfrwXqx3q2PoXXCGmXW0a+xPr8AHAyy0g==</latexit>F(I) m+F(ˆI) (1 m)
<latexit sha1_base64="nNjKynRLOsgfyr6AiUWodQJVHxY=">AAACQ3icbVDLSgMxFM34tr6qLt0Ei9AilhnxtRQF0V0Fq8VOKZlMpg3NTIbkjlCG+Tc3/oA7f8CNC0XcCmbaIlq9EHI45x5ycrxYcA22/WRNTE5Nz8zOzRcWFpeWV4qra9daJoqyOpVCqoZHNBM8YnXgIFgjVoyEnmA3Xu8012/umNJcRlfQj1krJJ2IB5wSMFS7eOuGBLqUiPQsK6cXWcWVvoTygPWC1Ml2XE8KX/dDc6VhVsHb+KfF7RL4tv1ebRdLdtUeDP4LnBEoodHU2sVH15c0CVkEVBCtm44dQyslCjgVLCu4iWYxoT3SYU0DIxIy3UoHHWR4yzA+DqQyJwI8YH86UhLqPJrZzOPrcS0n/9OaCQRHrZRHcQIsosOHgkRgkDgvFPtcMQqibwChipusmHaJIhRM7QVTgjP+5b/gerfqHFT3L/dKxyejOubQBtpEZeSgQ3SMzlEN1RFF9+gZvaI368F6sd6tj+HqhDXyrKNfY31+AQY6stI=</latexit>F(I) (1 m)+F(ˆI) m<latexit sha1_base64="4VqPzJLZ6zARCAOwZQGwRQOXeZE=">AAAB9XicbVC7TsMwFL3hWcqrwMhiUSExVQniNVawMBaJPqQ2VI7jtFYdO7IdUBX1P1gYQIiVf2Hjb3DaDNByJMtH59wrH58g4Uwb1/12lpZXVtfWSxvlza3tnd3K3n5Ly1QR2iSSS9UJsKacCdo0zHDaSRTFccBpOxjd5H77kSrNpLg344T6MR4IFjGCjZUeeoHkoR7H9sriSb9SdWvuFGiReAWpQoFGv/LVCyVJYyoM4Vjrrucmxs+wMoxwOin3Uk0TTEZ4QLuWChxT7WfT1BN0bJUQRVLZIwyaqr83MhzrPJqdjLEZ6nkvF//zuqmJrvyMiSQ1VJDZQ1HKkZEorwCFTFFi+NgSTBSzWREZYoWJsUWVbQne/JcXSeu05l3Uzu/OqvXroo4SHMIRnIAHl1CHW2hAEwgoeIZXeHOenBfn3fmYjS45xc4B/IHz+QNF1ZMI</latexit>mChannel Concatenation
(a) Overall Procedure(b) AFMModuleFigure 3. An overview of our Adversarial Frequency Mixup (AFM) framework. (a) Overall procedure. The input image Iis fed to the
denoising network to produce a denoised image, ˆI. Then, IandˆIare passed to the AFM module, which generates a new image Ihard.
BothIandIhardare used to train the denoising network. (b) Details of the AFM module, Two images, IandˆI, are mixed in the frequency
domain according to the mask mmmgenerated by the mask generator. Note that, while the AFM module generates both IhardandIeasy, only
Ihardis used to train the denoising network and Ieasyis used to update the mask generator. (The process of training the mask generator is
not illustrated in this figure.)
methods are plagued by inefficiency of significant time con-
sumption, as the network needs to be retrained for each dis-
tinct image. Importantly, their performance often falls short
when compared to supervised methods.
2.3. Generalization for Denoising
Existing denoising models have a generalization issue,
overfitting to training noise distribution. Only a few works
have been addressed this challenge [3, 15]. Mohan et
al. [15] observe that the noise overfitting is caused by bias
terms and remove all the biases from the network. However,
this method is targeted to enhance robustness within various
levels of noise, but not noise types. Chen et al. [3] introduce
the input pixel and attention feature masks during training
to focus on reconstructing a clean image itself rather than
denoising, but its performance on a real noise still remains
insufficient.
3. Methods
Our goal is to train a denoising network that demonstrates
strong generalization performance across various examples
with unseen real noise. To accomplish this, we propose Ad-
versarial Frequency Mixup (AFM), a novel training frame-
work based on augmented images in the frequency domain
with realistic noise distribution.
3.1. Background
Denoising networks aim to generate clean images, regard-
less of noise patterns imposed on input images. Such a pro-
cedure is formulated as
Dθ(xxx+nnn) =xxx, (1)
where Dθdenotes the denoising network parametrized by θ
and(xxx,nnn)indicates a pair of a clean image and its noise.For supervised learning, a prevalent approach is to train a
denoising model using a real noise dataset such as SIDD [1],
which consists of clean ( xxx) and noisy image ( xxx+nnn) pairs.
The objective of the problem is to optimize the model pa-
rameter θby minimizing the following loss:
L=∥Dθ(xxx+nnn)−xxx∥. (2)
However, conventional methods are prone to learning fixed
mappings between clean and noisy images, which often
leads to poor generalization. In practical scenarios, there is
a wide range of noise variations due to the diverse charac-
teristics of camera sensors and the Image Signal Processing
(ISP) pipelines. Therefore, it is imperative for real noise
denoising models to generalize to such variations in noise
distributions.
3.2. Adversarial Frequency Mixup
Figure 3(a) illustrates an overview of the proposed AFM
approach, designed to improve generalization to unseen real
noise images. Given a noisy input image I∈RC×H×W, we
first predict a denoised image ˆIusing a denoising network
Dθ, which is given by
ˆI=Dθ(I). (3)
Then, we mixup the original input Iand the prediction ˆIin
the frequency domain as follows:
Ihard=F−1
F(I)⊙mmm+F(ˆI)⊙(1−mmm)
,(4)
where Fis the Fast Fourier Transform (FFT), ⊙is an ele-
mentwise multiplication operator, mmm∈[0,1]1×H×Wis an
arbitrary mask in the frequency domain, and Ihardis the re-
sulting mixed image1. Since the mask mmmis bounded by
1The resulting image is termed Ihardfor the reasons that will be dis-
cussed in Section 3.2 and 3.3
2725
[0,1], the right-hand side of Eq. (4) represents an element-
wise interpolation between IandˆIin the frequency domain.
Given IandˆI, the objective here is to generate a new im-
age with some noise that is characterized by a distribution
distinct from the original noisy image, while also resem-
bling noise encountered in real-world scenarios. Our de-
sign choices in Eq. (4)—specifically, the use of frequency
mixup—are directly inspired by this objective.
The Fourier Transform on IandˆImaps each image to
the frequency domain, where it is relatively easier to manip-
ulate the noise distribution of an image compared to the spa-
tial domain. An intuitive visualization supporting this claim
is shown in Figure 2, where the regions of noise and clean
signal are clearly distinguishable in the frequency domain.
Furthermore, changes in the frequency component act glob-
ally on the image, which minimizes the risk of unwanted
manipulation to the underlying image content. Essentially,
the resulting Ihardstays in the manifold of real noise im-
ages because it is an interpolation of IandˆI. Therefore,
frequency mixup prevents the construction of synthetic im-
ages with arbitrary and artificial noise.
By deriving a simplified expression of Eq. (4), we de-
velop a more intuitive understanding of the mixup opera-
tion:
Ihard=F−1
F(ˆI) +mmm⊙(F(I)− F(ˆI))
=ˆI+F−1
mmm⊙ F(I−ˆI)
=ˆI+F−1(mmm)∗ F−1(F(ˆnnn))
=ˆI+F−1(mmm)∗ˆnnn
=ˆI+hhh∗ˆnnn, (5)
where hhhdenotes a filter, ∗denotes a convolution operator,
andˆnnn=I−ˆIdenotes the predicted noise. Essentially, fre-
quency mixup does not randomly alter the image, but rather
transforms the noise component ˆnnnaccording to hhh, which is
defined by the inverse Fourier transform of mask mmm.
What remains is the choice of mmm, or more specifically,
how to create an appropriate mask for the frequency mixup.
As will be discussed with much more detail in Section 3.3,
we opt for an adversarial design of mmm, which results in a
real noise image that the denoising network considers as
hard (hence the term Ihard). Then, we can incorporate Ihard
into the training pipeline to improve the real-world general-
ization of the denoising network.
3.3. Mask Design
The mask mmmis adversarially generated such that the differ-
ence between the denoised output of Ihard,Dθ(Ihard), and
the ground truth, Igt, is maximized. As illustrated in Fig-
ure 3(b), this is done with the mask generation network Gϕ,
which can be implemented by either AFM-E or AFM-B.
ConvBatchNormReLU
1x1ConvSigmoid: Downsampling: Upsampling1×𝐻×𝑊: Skip Connections12×𝐻×𝑊: Conv+BN+ReLU(a) Element-wise mask generation network (AFM-E)
(𝑣!,𝑣",𝑣#…𝑣$)ConvReLUPoolFC layerFC layerSoftmaxMixupValues1×𝐻×𝑊12×𝐻×𝑊: Conv+ReLU+Pool𝑣!𝑣"𝑣#𝑣$. . .𝑣"𝑣!
(b) Band-wise mask generation network (AFM-B)
Figure 4. Architecture details of the two mask generation net-
works.
Regardless of implementation choices, mmmis expressed as:
mmm=Gϕh
I,ˆI,|F(I)|,|F(ˆI)|i
, (6)
where |F(I)|and|F(ˆI)|are the frequency magnitudes of
IandˆI, respectively, and [·,·]is a concatenation operator
along the channel dimension. Given the mask mmm, we con-
struct Ihardfollowing Eq. (4). Furthermore, to help stabilize
the training of Gϕ, we also construct Ieasyusing the opposite
mask, 1−mmm:
Ieasy=F−1
F(I)⊙(1−mmm) +F(ˆI)⊙mmm
.(7)
We formulate the following loss function to make Ihard
adversarial and train the mask generation network, Gϕ:
LAFM=PSNR (ˆIhard, Igt)−γPSNR (ˆIeasy, Igt),(8)
where ˆIhard=Dθ(Ihard),ˆIeasy=Dθ(Ieasy), and γis a hy-
perparameter that balances the two terms. The first term
satisfies our main purpose: generating the adversarial case
Ihardby minimizing the Peak Signal-to-Noise Ratio (PSNR)
between ˆIhardandIgt. The second term aims to maximize
the PSNR between ˆIeasyandIgt, since Ieasy, which is a com-
plement term of Ihard, should be generated as a relatively
easy input for the denoising network. Although Ieasyis not
strictly necessary in Eq. (8) to train an adversarial Gϕ, we
empirically find that it helps prevent the generation of trivial
masks.
3.3.1 AFM-E: Element-wise Mask Generation
AFM-E employs pixel segmentation to construct a mask
with the same size as the input image. Thus, each element
2726
Algorithm 1 AFM Training Procedure
Require: I: Input noisy image
Require: Dθ(·): Denoising network
Require: Gϕ: Mask generation network
Require: AFM(· |Gϕ): AFM module
1:fort= 1toTdo
2: ˆI← D θ(I)
3: Ihard←AFM(I,ˆI|Gϕ)
4: ˆIhard← D θ(Ihard)
5: Update DθbyLD ▷Eq. (9)
6: ˆI← D θ(I)
7: Ihard, Ieasy←AFM(I,ˆI|Gϕ)
8: ˆIhard← D θ(Ihard)
9: ˆIeasy← D θ(Ieasy)
10: Update GϕbyLAFM ▷Eq. (8)
11:end for
inF(I)andF(ˆI), may be mixed with a different mixup
ratio. As depicted in Figure 4(a), the combined input goes
through an encoder-decoder architecture with skip connec-
tions based on UNet [20]. More details regarding the archi-
tecture can be found in the Appendix.
3.3.2 AFM-B: Band-wise Mask Generation
While AFM-E is an effective method to generate a mixup
mask, we observe that masks generated by AFM-E natu-
rally exhibit some level of rotational invariance, i.e. mask
elements that correspond to the same frequency share simi-
lar mixup values. Thus, we introduce a simplified but pow-
erful alternative: band-wise mask generation. As depicted
in Figure 4(b), the network is constructed using few convo-
lutional and fully-connected layers, which yields a set of N
mixup values. The resulting mask is formed with Ncircu-
lar bands that are equally distributed along the polar axis.
All elements within the area of a single band correspond to
a mixup value produced by the network.
3.4. Training Procedure
In our training procedure, detailed in Algorithm 1, we con-
currently train the denoising network and the AFM network.
At each iteration, we employ a two-step approach. In the
first step, we keep the mask generation network fixed and
update the denoising network with the following loss:
LD=−PSNR (ˆI, I gt)−λPSNR (ˆIhard, Igt)
=Lrec+λLhard, (9)
where λis a hyperparameter. In the second step, we keep
the denoising network fixed and update the AFM module
with the adversarial loss function defined in Eq. (8).It is worth noting that our AFM framework only affects
the training process. At inference, we simply omit the AFM
module and make predictions with the denoising network
without any additional memory or computational costs.
4. Experiments
We apply our framework, AFM, to various image denoising
architectures and evaluate on multiple out-of-distribution
(OOD) benchmarks. Our results demonstrate that AFM
significantly improves the OOD generalization of image
denoising networks regardless of architecture (Table 1),
and even outperforms state-of-the-art denoising networks in
terms of OOD generalization (Table 2). Finally, we present
some qualitative results.
4.1. Experimental Settings
Datasets Across all experiments, we train the denoising
network exclusively on the Smartphone Image Denoising
Dataset (SIDD) Medium [1] dataset. We measure the in-
distribution (ID) performance on the SIDD validation set,
and evaluate the out-of-distribution (OOD) performance
across five real noise benchmarks to ensure a robust as-
sessment across various data domains. The five OOD
benchmarks include Poly [23], CC [16], HighISO [24],
iPhone [9], and Huawei [9]. The image size of Poly,CC
and HighISO is 512 ×512 and the image size of iPhone and
Huawei is 1024 ×1024.
Training details We train all models for 200K iterations
with a batch size of 32 and a training patch size of 256 ×256.
For the denoising network and the mask generation net-
work, we employ the AdamW [13] optimizer with an initial
learning rate of 10−3, which decreases to 10−6following
a cosine annealing schedule. Finally, we set λ= 0.8and
γ= 0.3.
4.2. Results
In Table 1, we compare the ID and OOD performance of
existing denoising architectures with and without using the
proposed framework. We test on four different architec-
tures, including DnCNN [30], CBDNet [6], NAFNet [5],
and MPRNet [27]2. The results show that incorporating
AFM consistently yields performance improvements across
nearly all OOD benchmarks and architectures, while re-
taining competitiveness for ID performance. For example,
integrating AFM leads to average OOD PSNR improve-
ments of +0.75 dB, +0.33 dB, +0.54 dB, and +1.00 dB
on DnCNN, CBDNet, MPRNet, and NAFNet, respectively.
This demonstrates that AFM can be applied to various de-
noising networks to improve the network’s generalization
and robustness to unseen noise distributions.
2For large-scale architectures such as MPRNet and NAFNet, we reduce
the number of blocks and channels to reduce training costs.
2727
Table 1. Quantitative comparisons of denoising networks with and without our proposed AFM-B, on the SIDD [1] validation set (in-
distribution) and other real noise benchmarks (out-of-distribution). We present performance in terms of PSNR ↑(dB) and SSIM (Structural
Similarity Index Measure) ↑. Dagger (†)denotes architectures that have been reimplemented with fewer model parameters.
In-distribution Out-of-distribution
Architecture Metric SIDD [1] Poly [23] CC [16] HighISO [24] iPhone [9] Huawei [9] OOD Avg.
DnCNN [30]PSNR 38.62 37.36 35.69 37.85 39.87 38.26 37.81
SSIM 0.9501 0.9740 0.9755 0.9703 0.9688 0.9654 0.9708
DnCNN-OursPSNR 38.35 37.75 36.84 39.17 40.65 38.39 38.56
SSIM 0.9478 0.9804 0.9830 0.9801 0.9777 0.9683 0.9779
CBDNet [6]PSNR 38.35 37.83 36.25 38.18 40.63 38.35 38.25
SSIM 0.9476 0.9809 0.9780 0.9722 0.9759 0.9656 0.9745
CBDNet-OursPSNR 39.27 37.86 36.75 39.00 40.51 38.76 38.58
SSIM 0.9551 0.9812 0.9843 0.9789 0.9757 0.9691 0.9778
MPRNet†[27]PSNR 39.55 37.54 35.96 38.01 40.41 38.17 38.02
SSIM 0.9572 0.9795 0.9792 0.9753 0.9771 0.9683 0.9759
MPRNet†-OursPSNR 39.41 37.92 36.56 39.11 40.62 38.60 38.56
SSIM 0.9568 0.9807 0.9806 0.9788 0.9765 0.9689 0.9771
NAFNet†[5]PSNR 39.84 37.10 35.66 38.10 37.75 37.65 37.27
SSIM 0.9592 0.9788 0.9807 0.9774 0.9111 0.9679 0.9631
NAFNet†-OursPSNR 39.81 37.70 36.56 38.34 40.10 38.64 38.27
SSIM 0.9591 0.9792 0.9823 0.9760 0.9723 0.9684 0.9756
Table 2. Quantitative comparisons between our AFM-B and state-of-the-art supervised and self-supervised image denoising networks on
the SIDD [1] validation set (in-distribution) and other real-noise benchmarks (out-of-distribution). We present performance in terms of
PSNR↑(dB) and SSIM ↑, and also report the number of parameters and MACs at inference. The values of MACs are estimated by an input
with the spatial size of 256 ×256. Networks marked with asterisk (*) are evaluated using official out-of-the-box models.
In-distribution Out-of-distribution
Architecture Metric SIDD [1] Poly [23] CC [16] HighISO [24] iPhone [9] Huawei [9] OOD Avg. Params (M) MACs (G)
SupervisedMIRNet-v2* [29]PSNR 39.84 37.43 35.96 38.19 40.50 38.11 38.045.9 140.3SSIM 0.9593 0.9802 0.9798 0.9777 0.9789 0.9685 0.9770
Uformer* [22]PSNR 39.89 37.48 36.02 38.14 40.31 38.37 38.0650.9 89.5SSIM 0.9594 0.9794 0.9794 0.9763 0.9751 0.9684 0.9757
Restormer* [28]PSNR 40.02 37.66 36.33 38.29 40.13 38.42 38.1726.1 141.0SSIM 0.9603 0.9793 0.9807 0.9756 0.9734 0.9675 0.9753
Self-supervisedR2R* [18]PSNR 35.09 36.84 35.28 37.37 39.25 38.35 37.420.7 44.0SSIM 0.9154 0.9726 0.9758 0.9716 0.9614 0.9667 0.9696
AP-BSN* [11]PSNR 35.49 36.28 33.51 37.48 39.76 36.26 36.663.1 216.2SSIM 0.9085 0.9735 0.9729 0.9740 0.9728 0.9499 0.9688
CVF-SID* [17]PSNR 34.20 33.06 29.11 33.31 36.90 33.11 33.101.2 77.9SSIM 0.913 0.9531 0.9372 0.9521 0.9540 0.9270 0.9447
Supervised DnCNN-OursPSNR 38.35 37.75 36.84 39.17 40.65 38.39 38.560.7 43.8SSIM 0.9478 0.9804 0.9830 0.9801 0.9777 0.9683 0.9779
We also conduct comparisons with state-of-the-art
(SOTA) supervised denoising networks, such as MIRNet-
v2 [26], Uformer [22], and Restormer [28], and present
the results in Table 2. We evaluate all SOTA networks us-
ing the officially published weights, and compare these net-
works with a DnCNN architechture trained using the AFM-
B framework. Remarkably, despite requiring less memory
and computation at inference, DnCNN with AFM-B outper-forms all SOTA models by at least +0.39 dB (Restormer) in
terms of PSNR, and +0.009 (MIRNet-v2) in terms of SSIM.
We also compare the denoising performance with other
SOTA dataset-based self-supervised methods. As shown in
Table 2, models trained with a dataset-based self-supervised
approach using the SIDD dataset show poor performance in
both in-distribution and out-of-distribution scenarios, while
our method significantly outperforms in both.
2728
CC Noisy
Ground truth
 DnCNN 37.40 dB
 NAFNet 36.55 dB
 MPRNet 37.62 dB
MIRNet-v2 38.08 dB
 Uformer 37.77 dB
 Restormer 37.86 dB
 Ours 39.48 dB
HighISO Noisy
Ground truth
 DnCNN 35.58 dB
 NAFNet 35.77 dB
 MPRNet 35.69 dB
MIRNet-v2 36.08 dB
 Uformer 35.93 dB
 Restormer 35.69 dB
 Ours 36.86 dB
Figure 5. Comparison between the denoised outputs of various denoising networks including ours (DnCNN trained with AFM-B), on the
out-of-distribution (OOD) datasets. DnCNN with AFM-B displays cleaner outputs compared to other networks that are trained without
AFM.
Qualitative results Figure 5 visualizes the denoised out-
puts of DnCNN trained with AFM-B on samples of two
OOD benchmarks: CC and HighISO. For comparison, we
also visualize outputs of other denoising networks, trained
without AFM. These qualitative comparisons clearly show
that the models trained with AFM produce cleaner outputs
compared to those trained without AFM.
5. Analysis and Discussions
We analyse the effectiveness of AFM when compared with
other generalization methods and provide some discussions.
5.1. Comparison with Generalization Techniques
We compare our AFM framework with a few other tech-
niques that aim to improve generalization. We briefly sum-
marize each technique:
•Dropout [8] randomly drops features along the channel
dimension before the last convolution layer of the net-
work.
•Input mask [3] randomly masks the features after the
first convolutional layer across the spatial dimension.
•CutMix [25] is a common data augmentation scheme to
create novel data samples by cutting and pasting patches
from a different image within the training dataset.•Adversarial training first generates adversarial pertur-
bations in the input image using the PGD attack [14],
then trains the network to denoise the adversarial pertur-
bations.
•Random frequency mixup is identical to AFM-B, ex-
cept that the mixup value for each frequency band is sam-
pled from a uniform distribution.
•ASM-E (Adversarial Spatial Mixup) is identical to
AFM-E, except that the mixup is conducted in the spa-
tial domain instead of the frequency domain.
We train a DnCNN network with each of the techniques
mentioned above, and present the ID and average OOD re-
sults in Table 3. We first observe that Dropout and In-
put mask do not improve generalization performance, ev-
idenced by a drop of average OOD performance. In addi-
tion, we notice that CutMix shows minor gains and is out-
performed by both AFM-E and AFM-B, which exhibit sig-
nificant improvements in OOD performance.
An intriguing observation is made in the results of adver-
sarial training. Despite sharing some similarities with our
AFM framework, we find that a naive adversarial training
using the PGD attack fails to improve robustness to OOD
images (-0.63 dB). We hypothesize that the difference lies
in how the noise is created; while the PGD attack gener-
2729
Table 3. Comparing the performance of AFM-E and AFM-B with
other conventional generalization methods on the DnCNN archi-
tecture. We present performance in terms of PSNR ↑(dB) and
SSIM↑. The full table with results on individual OOD benchmarks
are available in the Appendix.
Algorithm Metric SIDD [1] OOD Avg.
Normal TrainingPSNR 38.62 37.81
SSIM 0.9501 0.9708
Dropout [8]PSNR 37.27 37.57
SSIM 0.9294 0.9697
Input mask [3]PSNR 37.83 37.28
SSIM 0.9441 0.9731
CutMix [25]PSNR 38.59 37.96
SSIM 0.9500 0.9719
Adversarial TrainingPSNR 38.43 37.18
SSIM 0.9483 0.9668
Random Freq. MixupPSNR 38.53 38.17
SSIM 0.9493 0.9714
ASM-EPSNR 38.46 37.92
SSIM 0.9490 0.9717
AFM-E (Ours)PSNR 38.41 38.51
SSIM 0.9485 0.9774
AFM-B (Ours)PSNR 38.35 38.56
SSIM 0.9478 0.9779
ates a synthetic adversarial noise, our AFM is designed to
generate realistic adversarial noise. This implies that noise
augmentation to maintain realistic properties plays a vital
role in improving generalization performance. Addition-
ally, random frequency mixup leads to a modest improve-
ment in OOD performance (+0.36 dB), but not as much as
AFM-E (+0.70 dB) or AFM-B (+0.75 dB), which highlights
the importance of the adversarial mask generation network
in our AFM module. Last, ASM-E fails to improve the gen-
eralization performance, which reinforces the rationale for
employing mixup in the frequency domain.
5.2. Discussions
Frequency analysis Figure 6 visualizes an example of the
adversarial mask mmm, as well as the frequency magnitudes
of the corresponding images, Ihard,ˆIhard, and ˆI, from the
training dataset. Since the input is an in-distribution sam-
ple, the denoised image ˆIdisplays clean frequencies. How-
ever, the denoising network struggles to denoise Ihard; in
fact,|F(ˆIhard)|in Figure 6 bears strong resemblance to the
output frequency in Figure 2, which is sampled from an
OOD dataset. This implies that our AFM module constructs
training images that share many characteristics with OOD
images.
Generalization in image denoising The results in Table 2
sheds light on the lack of generalization to unseen real-
noise distributions, even in the best performing denoising
mmm
 F(Ihard)
 F(ˆIhard)
 F(ˆI)
Figure 6. Visualizing the mixup mask mmm, as well as the frequency
magnitudes of Ihard,ˆIhard, and ˆI. The top row presents the results
obtained by AFM-E, while the bottom row shows those by AFM-
B. In both outcomes, it is observable that ˆIhardretains more noise
in the frequency domain compared to ˆI.
networks, e.g. Restormer. This raises a significant concern;
generalization to varying real-noise distributions is often
overlooked, despite the fact that generalization is a neces-
sity for the widespread use of image denoising networks.
Thus, we hope our work will inspire future image denoising
research to explore this direction as well, instead of focus-
ing exclusively on improving in-distribution performance.
6. Conclusion
We proposed Adversarial Frequency Mixup (AFM), a novel
training framework for image denoising networks that facil-
itates better generalization and robustness to diverse real-
world noise distributions. By leveraging an adversarial
mixup in the frequency domain, the AFM module generates
new noisy images that retain the properties of noise encoun-
tered in the real-world. Then, these images are integrated
into the training dataset to learn improved image denoising
networks with the out-of-distribution robustness. Notably,
image denoisers trained with our AFM framework exhibited
significantly superior generalization capabilities over those
trained with conventional supervised training methods. Fi-
nally, we identified that the lack of robustness in image de-
noising networks is an essential yet often overlooked area
of research, and hope that future work will pursue this di-
rection actively.
Acknowledgements This work was partly supported by
Samsung Electronics Co., Ltd and Samsung Advanced
Institute of Technology. It was also partly supported
by the Bio & Medical Technology Development Pro-
gram of the National Research Foundation (NRF) [No.
2021M3A9E4080782] and Institute of Information & com-
munications Technology Planning & Evaluation (IITP)
grant [No. 2021-0-01343, No. 2021-0-02068], funded by
the Korea government (MSIT).
2730
References
[1] Abdelrahman Abdelhamed, Stephen Lin, and Michael S
Brown. A high-quality denoising dataset for smartphone
cameras. In CVPR , pages 1692–1700, 2018. 1, 2, 3, 5, 6,
8
[2] Joshua Batson and Loic Royer. Noise2self: Blind denoising
by self-supervision. In ICML , pages 524–533. PMLR, 2019.
2
[3] Haoyu Chen, Jinjin Gu, Yihao Liu, Salma Abdel Magid,
Chao Dong, Qiong Wang, Hanspeter Pfister, and Lei Zhu.
Masked image training for generalizable deep image denois-
ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 1692–1703,
2023. 3, 7, 8
[4] Liangyu Chen, Xin Lu, Jie Zhang, Xiaojie Chu, and Cheng-
peng Chen. Hinet: Half instance normalization network for
image restoration. In CVPR , pages 182–192, 2021. 2
[5] Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun.
Simple baselines for image restoration. In ECCV , pages 17–
33, 2022. 1, 2, 5, 6
[6] Shi Guo, Zifei Yan, Kai Zhang, Wangmeng Zuo, and Lei
Zhang. Toward convolutional blind denoising of real pho-
tographs. In CVPR , pages 1712–1722, 2019. 1, 5, 6
[7] Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu, and
Jianzhuang Liu. Neighbor2neighbor: Self-supervised de-
noising from single noisy images. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 14781–14790, 2021. 2
[8] Xiangtao Kong, Xina Liu, Jinjin Gu, Yu Qiao, and Chao
Dong. Reflash dropout in image super-resolution. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 6002–6012, 2022. 7, 8
[9] Zhaoming Kong, Fangxi Deng, Haomin Zhuang, Jun Yu, Li-
fang He, and Xiaowei Yang. A comparison of image denois-
ing methods, 2023. 5, 6
[10] Alexander Krull, Tim-Oliver Buchholz, and Florian Jug.
Noise2void-learning denoising from single noisy images. In
CVPR , pages 2129–2137, 2019. 1, 2
[11] Wooseok Lee, Sanghyun Son, and Kyoung Mu Lee. Ap-
bsn: Self-supervised denoising for real-world images via
asymmetric pd and blind-spot network. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 17725–17734, 2022. 6
[12] Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli
Laine, Tero Karras, Miika Aittala, and Timo Aila.
Noise2noise: Learning image restoration without clean data.
arXiv preprint arXiv:1803.04189 , 2018. 1, 2
[13] Ilya Loshchilov and Frank Hutter. Decoupled weight de-
cay regularization. In International Conference on Learning
Representations , 2019. 5
[14] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,
Dimitris Tsipras, and Adrian Vladu. Towards deep learn-
ing models resistant to adversarial attacks. arXiv preprint
arXiv:1706.06083 , 2017. 7
[15] Sreyas Mohan, Zahra Kadkhodaie, Eero P. Simoncelli, and
Carlos Fernandez-Granda. Robust and interpretable blindimage denoising via bias-free convolutional neural networks.
InICLR , 2020. 3
[16] Seonghyeon Nam, Youngbae Hwang, Yasuyuki Matsushita,
and Seon Joo Kim. A holistic approach to cross-channel im-
age noise modeling and its application to image denoising.
InProceedings of the IEEE conference on computer vision
and pattern recognition , pages 1683–1691, 2016. 1, 5, 6
[17] Reyhaneh Neshatavar, Mohsen Yavartanoo, Sanghyun Son,
and Kyoung Mu Lee. Cvf-sid: Cyclic multi-variate function
for self-supervised image denoising by disentangling noise
from image. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 17583–
17591, 2022. 6
[18] Tongyao Pang, Huan Zheng, Yuhui Quan, and Hui Ji.
Recorrupted-to-recorrupted: unsupervised deep learning for
image denoising. In CVPR , pages 2043–2052, 2021. 1, 2, 6
[19] Yuhui Quan, Mingqin Chen, Tongyao Pang, and Hui Ji.
Self2self with dropout: Learning self-supervised denoising
from single image. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
1890–1898, 2020. 2
[20] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
InMICCAI , pages 234–241. Springer, 2015. 5
[21] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
Deep image prior. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 9446–9454,
2018. 2
[22] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang
Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: A gen-
eral u-shaped transformer for image restoration. In CVPR ,
pages 17683–17693, 2022. 1, 2, 6
[23] Jun Xu, Hui Li, Zhetong Liang, David Zhang, and Lei
Zhang. Real-world noisy image denoising: A new bench-
mark. arXiv preprint arXiv:1804.02603 , 2018. 1, 2, 5, 6
[24] Huanjing Yue, Jianjun Liu, Jingyu Yang, Truong Q Nguyen,
and Feng Wu. High iso jpeg image denoising by deep fusion
of collaborative and convolutional filtering. IEEE Transac-
tions on Image Processing , 28(9):4339–4353, 2019. 5, 6
[25] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk
Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-
larization strategy to train strong classifiers with localizable
features. In Proceedings of the IEEE/CVF international con-
ference on computer vision , pages 6023–6032, 2019. 7, 8
[26] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar
Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling
Shao. Learning enriched features for real image restoration
and enhancement. In ECCV , pages 492–511. Springer, 2020.
1, 2, 6
[27] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar
Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling
Shao. Multi-stage progressive image restoration. In CVPR ,
pages 14821–14831, 2021. 2, 5, 6
[28] Syed Waqas Zamir, Aditya Arora, Salman Khan, Mu-
nawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang.
Restormer: Efficient transformer for high-resolution image
restoration. In CVPR , pages 5728–5739, 2022. 1, 2, 6
2731
[29] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar
Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling
Shao. Learning enriched features for fast image restoration
and enhancement. IEEE transactions on pattern analysis and
machine intelligence , 45(2):1934–1948, 2022. 6
[30] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and
Lei Zhang. Beyond a gaussian denoiser: Residual learning of
deep cnn for image denoising. IEEE transactions on image
processing , 26(7):3142–3155, 2017. 1, 2, 5, 6
[31] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward
a fast and flexible solution for cnn-based image denoising.
IEEE Transactions on Image Processing , 27(9):4608–4622,
2018. 1
2732
