CycleINR: Cycle Implicit Neural Representation for Arbitrary-Scale Volumetric
Super-Resolution of Medical Data
Wei Fang1,2Y uxing Tang1Heng Guo1,2Mingze Y uan1,4Tony C. W . Mok1,2
Ke Yan1,2Jiawen Y ao1,2Xin Chen3Zaiyi Liu3Le Lu1Ling Zhang1Minfeng Xu1,2
1DAMO Academy, Alibaba Group
2Hupan Lab, 310023, Hangzhou, China
3Guangdong Provincial People’s Hospital
4Peking University
lucas.fw@alibaba-inc.com
Abstract
In the realm of medical 3D data, such as CT and MRI
images, prevalent anisotropic resolution is characterizedby high intra-slice but diminished inter-slice resolution.The lowered resolution between adjacent slices poses chal-lenges, hindering optimal viewing experiences and imped-ing the development of robust downstream analysis algo-
rithms. V arious volumetric super-resolution algorithms aim
to surmount these challenges, enhancing inter-slice reso-lution and overall 3D medical imaging quality. However ,existing approaches confront inherent challenges: 1) of-ten tailored to speciﬁc upsampling factors, lacking ﬂexibil-
ity for diverse clinical scenarios; 2) newly generated slices
frequently suffer from over-smoothing, degrading ﬁne de-tails, and leading to inter-slice inconsistency. In response,this study presents CycleINR, a novel enhanced ImplicitNeural Representation model for 3D medical data volu-metric super-resolution. Leveraging the continuity of the
learned implicit function, the CycleINR model can achieve
results with arbitrary up-sampling rates, eliminating theneed for separate training. Additionally, we enhance thegrid sampling in CycleINR with a local attention mech-anism and mitigate over-smoothing by integrating cycle-consistent loss. We introduce a new metric, Slice-wise Noise
Level Inconsistency (SNLI), to quantitatively assess inter-
slice noise level inconsistency. The effectiveness of our ap-proach is demonstrated through image quality evaluationson an in-house dataset and a downstream task analysis onthe Medical Segmentation Decathlon liver tumor dataset.
1. Introduction
V olumetric medical imaging, a cornerstone of diagnostic ra-diology involving techniques such as computed tomography
Signal ܆
INR Fitting 
Sampling (orange points) Sampled Signal ܇ INR FittingSampling (blue points)Sampled Signal ෡܆
Cycle-Consistent Loss
Figure 1. The core concept of CycleINR involves the initial use of
signal Xto ﬁt a continuous Implicit Neural Representation (INR)
function (shown in dotted blue curves). Subsequently, new points
(depicted as hollow orange dots) are sampled from this functionto create a new INR function (illustrated by dotted orange curves).The signal ˆXis then sampled from the new function at the same
positions as X. The construction of a cycle-consistent loss is
achieved by assessing the similarity between ˆXand X.
(CT) and magnetic resonance imaging (MRI), preserves the
3D three-dimensional characteristics of the body’s inter-nal structures through the compilation of multiple cross-sectional images [ 47]. While high-resolution volumetric
medical imaging offers detailed anatomical and functionalinformation enhancing diagnosis, its widespread clinical
adoption is hindered by prolonged acquisition time and el-evated storage costs. As a common alternative, anisotropicvolumes, characterized by high resolution within each slice
and lower resolution between slices, have become preva-lent [ 41]. However, this reduced inter-slice resolution in-
troduces challenges: limiting detailed sagittal or coronalviews, complicating lesion observation, and posing difﬁcul-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
11631
ties for robust 3D medical image interpretation algorithms.
Addressing this, there is a critical need for an accurate 3Dsuper-resolution method, essentially framing it as a volu-metric super-resolution task [ 33,52]. It is noteworthy that
video frame interpolation [ 35], which involves synthesizing
intermediate images between a pair of input frames, exhibitsnotable similarities with volumetric interpolation.
Implementing volumetric super-resolution algorithms
poses challenges on various fronts. Firstly, most super-resolution methods are constrained to speciﬁc super-resolution ratios, necessitating the training of multiple mod-els for different super-resolution levels. Secondly, the newlygenerated slices often exhibit over-smoothing, creating anoticeable slice-wise inconsistency issue in volumetric sce-narios. This becomes evident when scrolling through theslices and observing the contrast between the original andgenerated slices. In contrast, in 2D scenarios, newly gener-ated pixels are usually interpolated between existing pixels,making them less perceptible.
To tackle these challenges, we propose CycleINR (cycle-
consistent loss enhanced implicit neural representation)as a solution for volumetric super-resolution. CycleINRframework employs a uniﬁed implicit neural voxel functionto represent both low-resolution (LR) and high-resolution(HR) images, each at different sampling rates. This enables
treating volumetric super-resolution as an implicit neural
function approximation problem. Leveraging the continuityof the implicit neural function, a single implicit voxel func-tion achieves volumetric super-resolution with arbitrary up-sampling scales. To mitigate over-smoothing, we integratecycle-consistent loss (CCL) into our method. This entails
the utilization of pixel values at LR grid points to ﬁt the im-plicit neural function, generating pixel values for initially
empty points (forward path). Furthermore, we employ thenewly generated HR images to reconstruct the LR images(backward path). The backward path ensures the generated
image maintains a consistent noise level with the original
images. These paths establish a cyclical process, ensur-ing both accurate and consistent HR image recovery. Ad-ditionally, in our sampling process from the implicit neuralrepresentation function, we design a local attention mech-anism (LAM) to consider both spatial proximity and nu-
merical similarity. This ensures accurate and reliable sam-
pling by incorporating both local spatial relationships andpixel value similarities. Finally, we introduce a novel met-ric, slice-wise noise level inconsistency (SNLI), to quantita-tively measure inter-slice inconsistency, tackling limitationsin existing metrics such as PSNR (peak signal-to-noise ra-
tio), SSIM (structural similarity index), and LPIPS (learnedperceptual image patch similarity) [ 54]. These metrics are
more favorable towards assessing the smoothness or single-image similarity, making SNLI crucial for evaluating inter-slice variations in volumetric data and showcasing the ef-fectiveness of CCL.
Our contributions are outlined as follows:
1. We introduce CycleINR, a novel framework for volu-
metric super-resolution in 3D medical imaging. Lever-aging its coordinate-based nature, our method offers
ﬂexibility for arbitrary upsampling scales and the abil-
ity to generate multiple super-resolution results withoutthe need for additional training.
2. We propose a cycle-consistent loss within the implicit
neural representation framework to address the slice-wise inconsistency problem, ensuring consistency be-
tween newly generated and existing images. Addition-ally, a local attention mechanism captures both spatialproximity and numerical similarity, enhancing the depic-
tion of relationships between pixels.
3. We propose a new metric SNLI (slice-wise noise level
inconsistency) to quantitatively measure inter-slice noiselevel inconsistency in 3D data.
4. We conduct extensive experiments, evaluating both im-
age quality and downstream tasks, demonstrating the ef-fectiveness of our proposed method.
2. Related Work
Single Image Super-Resolution. Single image super-
resolution (SISR) has garnered signiﬁcant attention incomputer vision, evolving over decades [ 51]. Common
interpolation-based methods like cubic and linear interpo-lation are widely employed for their simplicity [ 3]. Im-
age prior like total variation can also enhance image super-resolution by formulating the task as a convex optimizationproblem [ 36]. In the era of deep learning, Dong et al. [ 8]
introduced super-resolution convolutional neural networks(CNN) to learn the mapping from LR images to HR images.Subsequent SISR research has delved into reﬁning deeplearning models, exploring strategies like deeper networkarchitectures [ 18][13], recursive-supervision [ 19], and deep
learning as priors [ 53]. Advanced upsampling methods in
deep learning include deconvolution for direct HR image
generation [ 9], sub-pixel convolution, and pixel shufﬂing
for efﬁcient upsampling at the output stage [ 37]. Residual
learning [ 24] and the integration of generative adversarial
networks [ 22,44,45] have further enriched the SISR land-
scape.
Implicit Neural Representation . Digitalized visual sig-
nals transition from their natural continuous form to dis-crete representations, such as 2D images or volumetric med-ical data. Implicit neural representation (INR) emergesas a coordinate-based strategy to address this transfor-mation, employing a trainable continuous function oftenimplemented with a straightforward fully connected net-work [ 40]. INR was ﬁrst applied in 3D shape model-
ing [ 6,27,31]. Subsequently, it has been extensively
employed across various 3D tasks, including object shape
11632
Latent Code 
Trilinear Interpolation
W W W
Q K V*
σ
灤
…
∑
(c) Attention- enhanced 
Latent Code Grid 
Sampling (ALCGS)…(a) CycleINR meta architecture (b) LAM-enhanced INR ModelCombineSoftmaxLinearLAM-enhanced 
INR ModelOriginal LR
Generated LRGenerated HR Ground Truth HR INR Fitting 
LossCycle-Consistent Loss
O
GGGGGGGGGGGGGGGGGGGGGGG
濄濅濋濆ܵ௣௥௘ௗEncoder Latent code array
Single pixel embedding DecoderOriginal LR
Generated HRCoordinateLatent code
C
Single pixel predictionALCGS
Figure 2. Schematic plot of our CycleINR framework for volumetric super-resolution. (a) Meta architecture of the proposed CycleINR. (b)
Local Attention Mechanism (LAM) enhanced INR model. (c) Attention-enhanced Latent Code Grid Sampling (ALCGS) process.
modeling [ 2,14], scene reconstruction [ 17,39], and struc-
ture rendering[ 25,29]. Notably, INR has made signiﬁcant
contributions to view synthesis, exempliﬁed by the neuralradiance ﬁelds (NeRF) [ 28]. INR has also been applied
to 2D image super-resolution [ 4,23,26,49,50]. Chen et
al. [ 5] used INR to learn a continuous function for 2D im-
ages. The function, taking an image coordinate and sur-rounding 2D deep features as inputs, predicts the RGB
value at the given coordinate, enabling image generation atarbitrary resolutions. Sitzmann et al. [ 40] enhanced image
representation by replacing the rectiﬁed linear units (ReLU)with periodic activations in multi-layer perceptron, achiev-ing higher image quality.
Volumetric Super-Resolution. V olumetric super-
resolution in 3D volumetric medical images, which aimsto enhance spatial resolution between slices, has witnessedsigniﬁcant progress. DA-VSR [ 33] addressed domain adap-
tation challenges, SAINT [ 32] employed a two-stage frame-
work with separate 2D CNNs for sagittal and coronal up-scaling, ArSSR [ 48] offered an INR-based arbitrary scale
SISR model for 3D MR images, and TVSRN [ 52] intro-
duced a transformer-based approach for CT scans. Addi-tionally, Fang et al. [ 12] proposed a medical slice synthesis
method using mutual distillation.3. CycleINR
V olumetric super-resolution can be viewed as a z-axisupsampling process applied to 3D volume data. LetI
lr(x,y,z)∈RX×Y×Zdenote the original low-resolution
(LR) CT scan, where X,Y, andZrepresent the number
of slices along the x, y, and z axes, respectively. The de-
sired output of volumetric super-resolution is denoted as
Ivsr(x,y,z)∈RX×Y×[Z·r], wherer>1represents the
upsampling factor along the z-axis. This implies that super-resolution is applied solely in the z-direction, maintainingthe original resolution in the x and y directions, allowingmore detailed information between slices in the volumet-
ric data. For instance, setting the upsampling factor rto 5
facilitates the conversion from 5mm slice-thickness data to1mm, or 4 for the conversion from 5mm to 1.25mm slice-thickness. In the context of super-resolution methods em-ploying the coordinate-based INR (implicit neural represen-tation) model, rcan also be a decimal, given the availability
of coordinates. The volumetric super-resolution transform
can be represented as:
F:I
lr∈RX×Y×Z→Ivsr∈RX×Y×[Z·r]. (1)
11633
3.1. Local Attention Mechanism-Enhanced INR
In the original INR model, 3D volumetric medical data
can be represented using a coordinate-based implicit voxelfunctionI=f
θ(x), whereθrepresents the trainable pa-
rameters of the INR model, x=(x,y,z)denotes any 3D
voxel spatial coordinate, and Idenotes the voxel intensity
at the coordinate xin the image. The spatial coordinates are
normalized to the range [-1, 1] along the x, y, and z dimen-sions, differing from physical coordinates. Both HR and LRimages can be regarded as the explicit representation of theimplicit function f
θ(x)but at different sampling rates. Once
the voxel function fθ(x)is well approximated, the desired
super-resolution results at an arbitrary scale can be achievedby simply adjusting the sampling rate (Figure 1).
However, in I=f
θ(x), the image information is de-
coded into the parameters of the function, restricting thefunction’s speciﬁcity to the represented image. As traininga unique model for each image is impractical, a more ﬂexi-ble function that can represent multiple images is essential.Inspired by [ 5], we reformulate the voxel function by incor-
porating a latent code using I=f
θ(x,z), wherezis a vec-
tor that represents the latent code capturing pixel-speciﬁcspatial features, which is generated for each pixel througha convolutional encoder, capturing local semantic informa-tion around the pixels. Hence, it can be considered as a
latent representation for the pixel in the latent space. The
architecture of the INR model with local attention mecha-nism enhanced latent code grid sampling process, is illus-trated in Figure 2(b). The following paragraphs will delve
into a detailed explanation of these designs.
Encoder Network. We ﬁrst employ an encoder E
θto
extract semantic features from the LR volume. The encodertakes the LR image volume I
lr∈Rh×w×das input and pro-
duces a feature map Zlr∈Rh×w×d×|z|, referred to as the
latent code. This implies that each voxel in the LR volumeis transformed into a latent code with a length of |z|. The
encoding process can be represented as:
Z
lr=Eθ(Ilr), (2)
whereθis the trainable parameters of the encoder. We
utilize a residual convolutional neural network encoder asdescribed in [ 11]. Our strategy to extract voxel-wise la-
tent space is crucial for facilitating the decoder network inseamlessly incorporating local image intensity information.This integration proves instrumental in effectively recover-ing ﬁne details within the high-resolution image, especiallywhen dealing with large upsampling scales.
Attention-enhanced Latent Code Grid Sampling.
Following the encoding process, the obtained latent codecorresponds to the voxels in the LR volume. To acquire thelatent code for the voxels at coordinate xin the HR volume
Z
hr, an additional operation known as grid sampling is re-
quired. Prior works [ 5,48] performed grid sampling usingtechniques such as trilinear interpolation or cell decoding,
which can be expressed as:
ˆZhr
x=L/summationdisplay
k=1(Sk
S·Zlr
k), (3)
whereZlr
k(k=1,...,L)denotes the latent code of the
L=8 surrounding coordinates in the feature map Zlr.Sk
represents the volume of the cuboid formed using the coor-
dinatexand the coordinate diagonal to the kthcoordinate,
andSis the sum of all Sk.
To better depict the relationships between voxels, we
propose an attention-enhanced latent code grid sampling(ALCGS) mechanism to enhance the generated voxel latentcode at coordinate x, which is represented as follows:
˜Z
hr
x=N/summationdisplay
i=1σ(WQ(ˆZhr
x)·WK(Zlr
i))WV(Zlr
i), (4)
whereNis the number of neighboring latent codes used for
ensembling, and σdenotes the Softmax function. Speciﬁ-
cally, the initial latent code ˆZhr
xof the voxel at coordinate x
is ﬁrstly computed by using trilinear interpolation. Subse-quently,Nnearest neighboring latent codes Z
lr
iare selected
aroundˆZhr
xfor aggregation. The initial latent code ˆZhr
xis
then mapped to the query space, while the Nnearby latent
codesZlr
iare mapped to the key and value space, respec-
tively, through linear layers WQ,WK, andWV. Following
this, the similarities between the query vector and each keyvector are computed, which are then converted to weightcoefﬁcients through the Softmax function. Finally, the im-proved latent code ˜Z
hr
xis obtained by weighted summation.
The local attention mechanism establishes pixel relation-
ships not solely dependent on physical distance but also onvisual information and latent code similarity. This aggre-gation strategy enhances trilinear interpolation and enablesthe generated latent codes to preserve richer details, similarto the functionality of bilateral ﬁltering.
Decoder Network. The decoder network D
θtakes the
query voxel coordinate xand its latent code ˜Zhr
xas input
and outputs the estimated voxel value Ihr(x), expressed as:
Ihr(x)=Dθ(x,˜Zhr
x). (5)
Following the design of [ 48], the decoder network com-
prises eight standard fully connected (FC) layers, with a
ReLU activation following each FC layer. A residual con-
nection is employed between the input of the decoder net-work and the intermediate fourth FC layer.
3.2. Cycle-Consistent Loss for INR Model
Inspired by CycleGAN [ 55], we integrate a cycle-consistent
loss into the INR model. This loss acts as a constraint to en-sure consistency between the generated image and the orig-inal image in terms of image features, such as noise level.
11634
Despite both being generative models, generative adversar-
ial networks (GANs) and INR diverge in their objectives.GANs transform one probability distribution function intoanother, whereas the INR model learns a continuous func-tion from discrete data. Arbitrary scaled super-resolutionimages can be sampled from this continuous function. Thenewly sampled image can also be used to ﬁt a new con-tinuous function. We assert that the newly generated func-tion should remain consistent with the original discrete data,leading to the incorporation of cycle-consistent loss in theINR model.
The synergy between cycle-consistent loss and the INR
model is highly effective, highlighting the INR model’ssuperior suitability for incorporating cycle-consistent losscompared to GANs. GANs are designed to transform be-tween distinct input and output distributions, while INRmodels aim to model the same function for both the inputLR image and the output HR image. It is expected that thenewly generated continuous function should produce valuesmatching the original discrete data when sampled.
Let’s represent the local attention mechanism (LAM)-
enhanced INR model described in Figure 2(b) asG
θ. In this
context, the cycle-consistent loss is articulated as:
ˆIhr
j=Gθ(Ilr
j,Chr
j), (6)
LCycle=M/summationdisplay
j||Gθ(ˆIhr
j,Clr
j)−Ilr
j||1, (7)
wherejandM are the index and the number of training
samples, respectively. The INR model Gθtakes the LR im-
ageIlr
jand the coordinates of the voxels to be generated
Chr
jas input, producing the sampled HR image ˆIhr
j.I n
Equation 7, the newly generated image ˆIhr
j, along with the
coordinates of the original LR image Clr
j, is once again fed
as input into the INR model to generate an estimation for theLR image. The estimated LR image is subsequently com-pared for similarities with the original LR image I
lr
jusing
the L1 loss, constituting the cycle-consistent loss.
It is worth noting that HR pixel coordinates may not
cover all LR pixel coordinates, given the random samplingof scaling factors, which may involve decimals during train-ing. Even with integers, especially when a scaling factor isan even number and the interpolation mode align-corner isset to False, there may be no overlap between the pixel po-sitions in the upscaled image and those of the original LRimage. This complexity makes the downsampling processin Equation 7non-trivial, and obtaining it can not be accom-
plished by simply selecting pixels at regular intervals.
The forward INR ﬁtting loss can be represented as:
L
INR=||ˆIhr
j−Ihr
j||1. (8)
The overall loss of training is represented as follows:
LCycleINR =LINR+λ·LCycle, (9)whereλserves as a regularization factor that helps to bal-
ance the INR ﬁtting loss and cycle-consistent loss.
3.3. SNLI: Measure Slice-wise Noise Inconsistency
To quantitatively measure the inter-slice inconsistencycaused by the over-smoothing in the newly generated slices,we propose a novel metric named slice-wise noise level in-consistency (SNLI). This inconsistency often manifests asa perceptible ‘slice discontinuity’ phenomenon during ax-ial slice scrolling. Notably, there is currently no existingmetric that adequately captures this phenomenon. The pro-posed SNLI metric is deﬁned as follows:
SNLI=Ψ (ζ(I
t)),t=0,1,...,T, (10)
where It∈RX×Yis thetthaxial slice of the image vol-
ume.ζis a robust wavelet-based estimator of the Gaussian
noise standard deviation, described in [ 10].Ψrepresents
the process of calculating the standard deviation. Thus, theSNLI metric helps measure the slice-wise noise level incon-sistency between axial slices, which was ignored by manydeep learning-based volumetric super-resolution methods.
4. Experiments
4.1. Experimental Setup
Data. We employed two datasets in our experiment. The
ﬁrst, a privately collected chest CT data (referred to as thechest dataset hereafter) consists of 204 CT volumes with1mm slice thickness. We randomly split it into 124 for train-ing, 40 for validation, and 40 for testing. Following estab-lished practices in volumetric super-resolution [ 32,48], we
derived the LR volumes by down-sampling from the corre-sponding HR volume, creating LR-HR training pairs. The
CycleINR model and other comparing learning-based meth-ods were trained on these pairs. Testing involved down-sampling to 2mm, 3mm, and 5mm thickness using inter-val sampling, aiming to recover 1mm data using super-
resolution approaches. Image quality was evaluated bycomparing results with ground truth 1mm data. The seconddataset is from the public Medical Segmentation Decathlon
[1] Task03
Liver training set (referred to as the MSD liver
dataset). It serves to assess whether the HR images gen-erated by the CycleINR model achieve performance com-parable to the original HR images in downstream segmen-
tation tasks. The MSD liver dataset consists of 131 vol-umes with liver and tumor masks. For evaluation, we se-
lected 33 volumes with 1mm slice thickness as the test setand utilized the remaining 98 image-mask pairs to train asegmentation model using nnUNet [ 16]. The test data was
downsampled and then restored to 1mm as on the chest CTdataset. We compared the segmentation results of different
methods, setting the segmentation from the original 1mmdata as references.
11635
/g8/g20/g14/g12/g15/g11/g7/g14 /g8/g20/g14/g12/g15/g11/g7/g14/g3/g5 /g8/g20/g14/g12/g15/g11/g7/g14/g3/g6 /g8/g20/g14/g12/g15/g11/g7/g1/g14/g3/g4/g2 /g9/g17/g18/g17/g16/g12/g15 /g10/g12/g13/g13/g14/g19/g12/g15
GT Cubic Trilinear TVSRN[ 52] ArSSR[ 48] SAINT[ 32] CycleINR
Figure 3. Visualization results of different super-resolution (x5) methods from axial, sagittal, and coronal views. The second and third rows
display newly generated images at new positions, while the ﬁrst and fourth rows show generated images at their original positions. In thebottom row, red arrows in TVSRN, ArSSR, and SAINT highlight horizontal lines in the coronal view, indicating slice-wise inconsistency.
Implementation Details. The encoder consists of three
blocks, where each block includes six 3D convolutional lay-ers followed by ReLU. The latent code vector length |z|is
set to 128. The feature channels of the intermediate layers
are set to 64. The number of neighboring latent codes N
used for ensembling is set to 27 to include all the 3×3×3
local regions around a speciﬁc voxel. The decoder is conﬁg-ured with 8 fully connected layers, where the intermediatelayer has a dimension of 256. During CycleINR training,LR-HR pairs are randomly sampled on the ﬂy from the orig-inal entire volume with an upsample scale ranging from 2.0to 6.0, with one decimal point precision. The balancing fac-torλfor the INR ﬁtting loss and cycle-consistent loss is setto 1. The Adam optimizer [ 20] minimizes the total loss in
Equation 9, with an initial learning rate of 10
−4, and decays
as half every 200 epochs. The training runs for a total of3,000 epochs. During testing, Gaussian window-weighted
sliding window inference is employed to mitigate stitching
artifacts. Note that these hyper-parameters are set followingempirical observations or prior works.
Compared Methods. For comparison, we include cubic
interpolation, trilinear interpolation, and three additionaldeep learning-based volumetric super-resolution methods:TVSRN [ 52], ArSSR [ 48], and SAINT [ 32]. These meth-
ods, featuring distinct architectural designs, demonstratethe capability of achieving arbitrary upscaling ratios, ex-
11636
Scale Method PSNR( ↑) SSIM( ↑) LPIPS alex(↓) LPIPS vgg(↓) LPIPS squeeze(↓) SNLI( ↓)
x2Cubic 39.5039 0.9705 0.0148 0.0639 0.0161 0.5318
Trilinear 40.6750 0.9757 0.0201 0.0618 0.0189 0.8450
TVSRN [ 52] 43.6167 0.9820 0.0311 0.0824 0.0310 1.4307
ArSSR [ 48] 42.6713 0.9799 0.0370 0.0913 0.0357 1.5738
SAINT [ 32] 44.3977 0.9833 0.0361 0.0867 0.0354 1.6724
CycleINR (Ours) 43.0137 0.9805 0.0201 0.0625 0.0206 0.3527
x3Cubic 35.0140 0.9393 0.0306 0.1084 0.0292 0.5322
Trilinear 36.5867 0.9515 0.0328 0.0973 0.0293 0.7422
TVSRN [ 52] 40.4857 0.9696 0.0607 0.1419 0.0583 1.4389
ArSSR [ 48] 39.3398 0.9659 0.0443 0.1135 0.0413 1.3026
SAINT [ 32] 40.8705 0.9711 0.0625 0.1404 0.0599 1.7535
CycleINR (Ours) 39.2748 0.9644 0.0293 0.0902 0.0280 0.6674
x5Cubic 31.0470 0.8896 0.0562 0.1627 0.0480 0.5336
Trilinear 32.6606 0.9106 0.0525 0.1413 0.0433 0.6682
TVSRN [ 52] 36.8459 0.9503 0.0927 0.1989 0.0862 1.3593
ArSSR [ 48] 35.1960 0.9394 0.0611 0.1485 0.0528 1.1714
SAINT [ 32] 36.9940 0.9519 0.0996 0.2044 0.0951 1.6996
CycleINR (Ours) 35.0022 0.9354 0.0464 0.1289 0.0399 0.6050
Table 1. Image quality evaluation of different methods. bald signiﬁes the best, and underline represents the suboptimal. Note that PSNR
and SSIM are not the sole determinants, and SNLI proves particularly valuable for comparing deep learning-based methods.
cept for TVSN, which requires training separate models for
super-resolution factors of x2, x3, and x5, respectively.
Evaluation Metrics. For the image quality evaluation,
we ﬁrst use peak signal-to-noise ratio (PSNR) and struc-
tural similarity (SSIM) [ 46] as fundamental metrics for dif-
ferent super-resolution methods. However, acknowledgingthe bias of PSNR and SSIM towards favoring blurry im-ages, where an over-smoothed super-resolution result couldyield high scores under these metrics despite signiﬁcant loss
of image details, we also include learned perceptual im-age patch similarity (LPIPS) [ 54] in the evaluation. LPIPS
scores are calculated using three backbones (AlexNet [ 21],
VGGNet [ 38], and SqueezeNet [ 15]). In addition, we calcu-
late the SNLI metric to measure the inconsistency betweenslices, often ignored by most volumetric super-resolution
methods. For downstream segmentation task evaluation, we
use the Dice similarity coefﬁcient (DSC) [ 7] and normalized
surface Dice (NSD) [ 30] with a tolerance of 1mm.
4.2. Super-resolution Results Evaluation
Image Quality Evaluation. Figure 3visualizes the
super-resolution results produced by different methods, andTable 1presents the corresponding image quality metrics.
In summary, traditional cubic and trilinear interpolationmethods exhibit limitations in recovering HR images, par-ticularly at large upscaling scales such as x5, where bonestructures are notably challenging to recover, resulting inlow PSNR and SSIM scores. While the three deep learning-based methods demonstrate relatively favorable results, a
shared challenge is their oversight of over-smoothness inthe generated slices, as evident in both the visualization re-sults and the SNLI metric. In Figure 3, the newly generated
axial slices are over-smoothed, leading to the loss of ﬁnevessel details in the liver. Moreover, in the sagittal and coro-nal views, horizontal lines emerge due to noise level incon-sistencies between slices. The SNLI metric for these threedeep learning-based methods is markedly elevated com-pared to other methods. However, with the enhancement ofCCL, CycleINR demonstrates signiﬁcant improvements, ef-
fectively recovering ﬁne details without over-smoothing in
the newly generated slices, and revealing the absence of vis-ible horizontal lines in sagittal and coronal views. Despitea slight decrease in PSNR and SSIM scores, it is impor-tant to note, as mentioned in [ 54], that this decline does not
necessarily signify a negative outcome. Furthermore, the
SNLI metric demonstrates the superiority of the proposedCycleINR in preserving consistency between axial slicescompared to other deep learning-based methods. This em-phasizes the substantial effectiveness of employing CCL inmitigating over-smoothing issues in newly generated slices.
Downstream Task Evaluation. To make a compre-
hensive comparison of the super-resolution results gener-ated by various deep learning models, we adopt a segmen-tation evaluation strategy, aligning with approaches from
prior works [ 34,43]. This strategy allows for a thorough
assessment of the SR results, moving beyond visual appear-
11637
INR LAM CCL PSNR( ↑) SSIM( ↑) LPIPS alex(↓) LPIPS vgg(↓) LPIPS squeeze(↓) SNLI( ↓)
/check 35.1960 0.9394 0.0611 0.1485 0.0528 1.1714
/check/check 35.1395 0.9391 0.0603 0.1478 0.0522 1.1395
/check/check 34.9610 0.9350 0.0470 0.1298 0.0405 0.6134
/check/check/check 35.0022 0.9354 0.0464 0.1289 0.0400 0.6050
Table 2. Ablation study on the Local Attention Mechanism (LAM) and Cycle-Consistent Loss (CCL).
Scale Method DSC L↑NSD L↑DSC T↑NSD T↑
x2Cubic 0.9868 0.9568 0.9069 0.8632
Trilinear 0.9892 0.9622 0.9069 0.8575
TVSRN [ 52] 0.9886 0.9514 0.8552 0.8065
ArSSR [ 48] 0.9856 0.9459 0.8516 0.7952
SAINT [ 32] 0.9884 0.9540 0.8476 0.7904
CycleINR 0.9911 0.9648 0.8730 0.8226
x3Cubic 0.9625 0.7862 0.7621 0.6891
Trilinear 0.9783 0.9037 0.7858 0.7083
TVSRN [ 52] 0.9783 0.9105 0.7672 0.6892
ArSSR [ 48] 0.9777 0.9056 0.7811 0.6988
SAINT [ 32] 0.9820 0.9250 0.8042 0.7338
CycleINR 0.9832 0.9294 0.8087 0.7381
x5Cubic 0.8678 0.4475 0.5644 0.4386
Trilinear 0.9487 0.6873 0.6755 0.5385
TVSRN [ 52] 0.9481 0.8228 0.6440 0.5431
ArSSR [ 48] 0.9589 0.7907 0.6582 0.5297
SAINT [ 32] 0.9619 0.8561 0.6006 0.5050
CycleINR 0.9620 0.8566 0.6881 0.5640
Table 3. Downstream task evaluation on the MSD liver tu-
mor dataset. Comparison of segmentation across different super-resolution methods with regard to the segmentation on the original1mm data. ’
L ’ and ’ T’ represent the liver and tumor, respectively.
Results are compared to segmentation on the original 1mm image,not the ground truth mask, to evaluate the ﬁdelity of the upscaledimage to the original 1mm characteristics.
ance to provide a more clinically relevant measure of per-
formance. As depicted in Table 3, the segmentation results
of CycleINR align better with the reference compared to
other methods, particularly superior at higher upsamplingscales. While traditional interpolation methods might re-
tain advantages in speciﬁc scenarios with smaller upscalingratios (e.g., x2), it is crucial to highlight that our proposedCycleINR consistently exhibits superior performance com-pared to other deep learning methods across various upscal-
ing ratios. This underscores the clinical relevance of ourmethod and establishes its efﬁcacy in achieving better re-
sults for downstream tasks.
4.3. Ablation Studies
We perform ablation studies to assess the impact of two keycomponents in our CycleINR framework: the local atten-tion mechanism (LAM) during latent code grid sampling,
and the cycle-consistent loss (CCL) aimed at minimizinginter-slice consistency. The contributions of these compo-nents to the overall performance are shown in Table 2.
Local Attention Mechanism (LAM). The local at-
tention mechanism in our process effectively combinesphysical distance and latent code similarities to aggregateweights for latent code representation. Drawing inspirationfrom bilateral ﬁltering, our LAM employs attention mecha-nisms for improved expressive power. As shown in Table 2,
integrating LAM into the latent code grid sampling processenhances the LPIPS metric [ 54] by preserving edges and
mitigating excessive smoothing, similar to the principles oftraditional bilateral ﬁltering [ 42].
Cycle-Consistent Loss (CCL). The purpose of CCL is
to impose a constraint ensuring consistency between thegenerated and original images in terms of image charac-teristics, such as noise level. As shown in Table 2, the
integration of CCL leads to a substantial improvement inboth the LPIPS and SNLI metrics. Visual results in Fig-ure 3further demonstrate that CCL enhances CycleINR by
effectively mitigating over-smoothing, as evidenced by sig-niﬁcant reductions in LPIPS and SNLI scores.
5. Conclusion
We present CycleINR, a novel 3D medical image super-
resolution approach. Unlike traditional approaches, Cy-
cleINR reframes the task as function approximation, lever-aging the continuity of the implicit neural representationfunction. This enables efﬁcient high-resolution image gen-eration without speciﬁc training for upsampling ratios. Theintroduced cycle-consistent loss mitigates over-smoothing,
a local attention mechanism further enhances performance,
and a novel metric, slice-wise noise level inconsistency, as-sesses the quality of 3D medical images. Overall, CycleINRshows promise for volumetric super-resolution, offering ef-ﬁciency and ﬂexibility for enhanced visual quality, and ro-bust downstream analysis. For future work, extending Cy-
cleINR to diverse medical imaging modalities and evaluat-ing its performance in larger clinical scenarios would of-
fer insights into generalizability. Exploring applications be-yond the medical ﬁeld, like video interpolation, presents acompelling direction for adapting CycleINR to handle thechallenges in temporal sequences.
11638
References
[1] Michela Antonelli, Annika Reinke, Spyridon Bakas, Key-
van Farahani, Annette Kopp-Schneider, Bennett A Landman,Geert Litjens, Bjoern Menze, Olaf Ronneberger, Ronald MSummers, et al. The medical segmentation decathlon. Nature
communications , 13(1):4128, 2022. 5
[2] Matan Atzmon and Y aron Lipman. Sal: Sign agnos-
tic learning of shapes from raw data. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition , pages 2565–2574, 2020. 3
[3] Thierry Blu, Philippe Th ´evenaz, and Michael Unser. Linear
interpolation revitalized. IEEE Transactions on Image Pro-
cessing , 13(5):710–719, 2004. 2
[4] Hao-Wei Chen, Y u-Syuan Xu, Min-Fong Hong, Yi-Min
Tsai, Hsien-Kai Kuo, and Chun-Yi Lee. Cascaded localimplicit transformer for arbitrary-scale super-resolution. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition , pages 18257–18267, 2023. 3
[5] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning
continuous image representation with local implicit imagefunction. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 8628–8638,
2021. 3,4
[6] Zhiqin Chen and Hao Zhang. Learning implicit ﬁelds for
generative shape modeling. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 5939–5948, 2019. 2
[7] Lee R Dice. Measures of the amount of ecologic association
between species. Ecology , 26(3):297–302, 1945. 7
[8] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Image super-resolution using deep convolutional net-works. IEEE transactions on pattern analysis and machine
intelligence , 38(2):295–307, 2015. 2
[9] Chao Dong, Chen Change Loy, and Xiaoou Tang. Acceler-
ating the super-resolution convolutional neural network. InComputer Vision–ECCV 2016: 14th European Conference,Amsterdam, The Netherlands, October 11-14, 2016, Pro-ceedings, Part II 14 , pages 391–407. Springer, 2016. 2
[10] David L Donoho and Iain M Johnstone. Ideal spatial adapta-
tion by wavelet shrinkage. biometrika , 81(3):425–455, 1994.
5
[11] Jinglong Du, Zhongshi He, Lulu Wang, Ali Gholipour,
Zexun Zhou, Dingding Chen, and Y uanyuan Jia. Super-resolution reconstruction of single anisotropic 3d mr imagesusing residual convolutional neural network. Neurocomput-
ing, 392:209–220, 2020. 4
[12] Chaowei Fang, Liang Wang, Dingwen Zhang, Jun Xu, Yix-
uan Y uan, and Junwei Han. Incremental cross-view mutualdistillation for self-supervised medical ct synthesis. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 20677–20686, 2022. 3
[13] Chaowei Fang, Dingwen Zhang, Liang Wang, Y ulun Zhang,
Lechao Cheng, and Junwei Han. Cross-modality high-
frequency transformer for mr image super-resolution. In Pro-
ceedings of the 30th ACM International Conference on Mul-timedia , pages 1584–1592, 2022. 2[14] Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna,
and Thomas Funkhouser. Local deep implicit functions for3d shape. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 4857–
4866, 2020. 3
[15] Forrest N Iandola, Song Han, Matthew W Moskewicz,
Khalid Ashraf, William J Dally, and Kurt Keutzer.Squeezenet: Alexnet-level accuracy with 50x fewer pa-rameters and¡ 0.5 mb model size. arXiv preprint
arXiv:1602.07360 , 2016. 7
[16] Fabian Isensee, Paul F Jaeger, Simon AA Kohl, Jens Pe-
tersen, and Klaus H Maier-Hein. nnu-net: a self-conﬁguringmethod for deep learning-based biomedical image segmen-
tation. Nature methods , 18(2):203–211, 2021. 5
[17] Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei
Huang, Matthias Nießner, Thomas Funkhouser, et al. Local
implicit grid representations for 3d scenes. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition , pages 6001–6010, 2020. 3
[18] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate
image super-resolution using very deep convolutional net-works. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 1646–1654, 2016. 2
[19] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Deeply-
recursive convolutional network for image super-resolution.InProceedings of the IEEE conference on computer vision
and pattern recognition , pages 1637–1645, 2016. 2
[20] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 6
[21] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-works. Advances in neural information processing systems ,
25, 2012. 7
[22] Christian Ledig, Lucas Theis, Ferenc Husz ´ar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew Aitken,Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a generative ad-versarial network. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 4681–4690,
2017. 2
[23] Jaewon Lee and Kyong Hwan Jin. Local texture estima-
tor for implicit representation function. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 1929–1938, 2022. 3
[24] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and
Kyoung Mu Lee. Enhanced deep residual networks for singleimage super-resolution. In Proceedings of the IEEE confer-
ence on computer vision and pattern recognition workshops ,
pages 136–144, 2017. 2
[25] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and
Christian Theobalt. Neural sparse voxel ﬁelds. Advances
in Neural Information Processing Systems , 33:15651–15663,
2020. 3
[26] Ying-Tian Liu, Y uan-Chen Guo, and Song-Hai Zhang.
Enhancing multi-scale implicit learning in image super-resolution with integrated positional encoding. arXiv
preprint arXiv:2112.05756 , 2021. 3
11639
[27] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
bastian Nowozin, and Andreas Geiger. Occupancy networks:
Learning 3d reconstruction in function space. In Proceedings
of the IEEE/CVF conference on computer vision and patternrecognition , pages 4460–4470, 2019. 2
[28] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance ﬁelds for view syn-thesis. Communications of the ACM , 65(1):99–106, 2021.
3
[29] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and
Andreas Geiger. Differentiable volumetric rendering: Learn-
ing implicit 3d representations without 3d supervision. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition , pages 3504–3515, 2020. 3
[30] Stanislav Nikolov, Sam Blackwell, Alexei Zverovitch,
Ruheena Mendes, Michelle Livne, Jeffrey De Fauw, Y ojanPatel, Clemens Meyer, Harry Askham, Bernardino Romera-Paredes, et al. Deep learning to achieve clinically applica-ble segmentation of head and neck anatomy for radiotherapy.arXiv preprint arXiv:1809.04430 , 2018. 7
[31] Jeong Joon Park, Peter Florence, Julian Straub, Richard
Newcombe, and Steven L ovegrove. Deepsdf: Learning con-
tinuous signed distance functions for shape representation.InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 165–174, 2019. 2
[32] Cheng Peng, Wei-An Lin, Haofu Liao, Rama Chellappa, and
S Kevin Zhou. Saint: spatially aware interpolation networkfor medical slice synthesis. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 7750–7759, 2020. 3,5,6,7,8
[33] Cheng Peng, S Kevin Zhou, and Rama Chellappa. Da-vsr:
domain adaptable volumetric super-resolution for medicalimages. In Medical Image Computing and Computer As-
sisted Intervention–MICCAI 2021: 24th International Con-ference, Strasbourg, France, September 27–October 1, 2021,Proceedings, Part VI 24 , pages 75–85. Springer, 2021. 2,3
[34] Cheng Peng, Wei-An Lin, Haofu Liao, Rama Chellappa, and
Shaohua Kevin Zhou. Deep slice interpolation via marginalsuper-resolution, fusion, and reﬁnement. In State of the Art
in Neural Networks and Their Applications , pages 133–145.
Elsevier, 2023. 7
[35] Fitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun,
Caroline Pantofaru, and Brian Curless. Film: Frame inter-polation for large motion. In European Conference on Com-
puter Vision , pages 250–266. Springer, 2022. 2
[36] Feng Shi, Jian Cheng, Li Wang, Pew-Thian Y ap, and Ding-
gang Shen. Lrtv: Mr image super-resolution with low-rankand total variation regularizations. IEEE transactions on
medical imaging , 34(12):2459–2466, 2015. 2
[37] Wenzhe Shi, Jose Caballero, Ferenc Husz ´ar, Johannes Totz,
Andrew P Aitken, Rob Bishop, Daniel Rueckert, and ZehanWang. Real-time single image and video super-resolutionusing an efﬁcient sub-pixel convolutional neural network. InProceedings of the IEEE conference on computer vision andpattern recognition , pages 1874–1883, 2016. 2
[38] Karen Simonyan and Andrew Zisserman. V ery deep convo-lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 , 2014. 7
[39] Vincent Sitzmann, Michael Zollh ¨ofer, and Gordon Wet-
zstein. Scene representation networks: Continuous 3d-structure-aware neural scene representations. Advances in
Neural Information Processing Systems , 32, 2019. 3
[40] Vincent Sitzmann, Julien Martel, Alexander Bergman, David
Lindell, and Gordon Wetzstein. Implicit neural representa-tions with periodic activation functions. Advances in neural
information processing systems , 33:7462–7473, 2020. 2,3
[41] Philippe Th
´evenaz, Thierry Blu, and Michael Unser. Chapter
28 - image interpolation and resampling. In Handbook of
Medical Image Processing and Analysis (Second Edition) ,
pages 465–493. Academic Press, Burlington, second editionedition, 2009. 1
[42] Carlo Tomasi and Roberto Manduchi. Bilateral ﬁltering for
gray and color images. In Sixth international conference on
computer vision (IEEE Cat. No. 98CH36271) , pages 839–
846. IEEE, 1998. 8
[43] Jiancong Wang, Y uhua Chen, Yifan Wu, Jianbo Shi, and
James Gee. Enhanced generative adversarial network for 3dbrain mri super-resolution. In Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision ,
pages 3627–3636, 2020. 7
[44] Xintao Wang, Ke Y u, Shixiang Wu, Jinjin Gu, Yihao Liu,
Chao Dong, Y u Qiao, and Chen Change Loy. Esrgan: En-
hanced super-resolution generative adversarial networks. In
Proceedings of the European conference on computer vision(ECCV) workshops , pages 0–0, 2018. 2
[45] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.
Real-esrgan: Training real-world blind super-resolution withpure synthetic data. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 1905–1914,
2021. 2
[46] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE transactions on image processing ,
13(4):600–612, 2004. 7
[47] Lauren H Williams and Trafton Drew. What do we know
about volumetric medical image interpretation?: A review ofthe basic science and medical image perception literatures.Cognitive Research: Principles and Implications , 4:1–24,
2019. 1
[48] Qing Wu, Y uwei Li, Y awen Sun, Y an Zhou, Hongjiang Wei,
Jingyi Y u, and Y uyao Zhang. An arbitrary scale super-
resolution approach for 3d mr images via implicit neural rep-
resentation. IEEE Journal of Biomedical and Health Infor-
matics , 27(2):1004–1015, 2022. 3,4,5,6,7,8
[49] Xingqian Xu, Zhangyang Wang, and Humphrey Shi. Ul-
trasr: Spatial encoding is a missing key for implicit im-age function-based arbitrary-scale super-resolution. arXiv
preprint arXiv:2103.12716 , 2021. 3
[50] Jingyu Y ang, Sheng Shen, Huanjing Y ue, and Kun Li. Im-
plicit transformer network for screen content image continu-ous super-resolution. Advances in Neural Information Pro-
cessing Systems , 34:13304–13315, 2021. 3
[51] Wenming Y ang, Xuechen Zhang, Y apeng Tian, Wei Wang,
Jing-Hao Xue, and Qingmin Liao. Deep learning for single
11640
image super-resolution: A brief review. IEEE Transactions
on Multimedia , 21(12):3106–3121, 2019. 2
[52] Pengxin Y u, Haoyue Zhang, Han Kang, Wen Tang, Corey W
Arnold, and Rongguo Zhang. Rplhr-ct dataset and trans-former baseline for volumetric super-resolution from ct
scans. In International Conference on Medical Image Com-
puting and Computer-Assisted Intervention , pages 344–353.
Springer, 2022. 2,3,6,7,8
[53] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang.
Learning deep cnn denoiser prior for image restoration. InProceedings of the IEEE conference on computer vision andpattern recognition , pages 3929–3938, 2017. 2
[54] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness ofdeep features as a perceptual metric. In Proceedings of the
IEEE conference on computer vision and pattern recogni-tion , pages 586–595, 2018. 2,7,8
[55] Jun-Y an Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE
international conference on computer vision , pages 2223–
2232, 2017. 4
11641
