FutureHuman3D: Forecasting Complex Long-Term
3D Human Behavior from Video Observations
Christian Diller
Technical University of Munich
christian.diller@tum.deThomas Funkhouser
Google
tfunkhouser@google.comAngela Dai
Technical University of Munich
angela.dai@tum.de
TimeInput Image Sequence
with Action Labels&Long-Term Future Prediction as Sequence of 3D Poses with Action LabelsProjected 2D<screw= <pour= <screw= <add= <add= <take=
<take=
<cut=<cut=
<change temperature=
Predicted 3D
Figure 1. We propose a novel generative approach to model long-term future human behavior by jointly forecasting a sequence of coarse
action labels and their concrete realizations as 3D body poses. For broad applicability, our autoregressive method only requires weak
supervision and past observations in the form of 2D RGB video data, together with a database of uncorrelated 3D human poses.
Abstract
We present a generative approach to forecast long-term
future human behavior in 3D, requiring only weak super-
vision from readily available 2D human action data. This
is a fundamental task enabling many downstream applica-
tions. The required ground-truth data is hard to capture in
3D (mocap suits, expensive setups) but easy to acquire in
2D (simple RGB cameras). Thus, we design our method
to only require 2D RGB data at inference time while being
able to generate 3D human motion sequences. We use a
differentiable 2D projection scheme in an autoregressive
manner for weak supervision, and an adversarial loss for
3D regularization. Our method predicts long and complex
human behavior sequences (e.g., cooking, assembly) consist-
ing of multiple sub-actions. We tackle this in a semantically
hierarchical manner, jointly predicting high-level coarse
action labels together with their low-level fine-grained real-
izations as characteristic 3D human poses. We observe that
these two action representations are coupled in nature, and
joint prediction benefits both action and pose forecasting.
Our experiments demonstrate the complementary nature of
joint action and 3D pose prediction: our joint approach
outperforms each task treated individually, enables robust
longer-term sequence prediction, and improves over alter-
native approaches to forecast actions and characteristic 3D
poses.1. Introduction
Predicting future human behavior is fundamental to ma-
chine intelligence, with many applications in content cre-
ation, robotics, mixed reality, and more. For instance, a
monitoring system might issue early warnings of potentially
dangerous behaviour, and a robotic assistant can use fore-
casting to place tools at the right place and time they will
be needed in the future. Consider the specific scenario of
an assembly line monitoring system deployed to issue early
warnings of behavior that could be harmful in the near future:
The system needs to have a long-term understanding of the
future, enabling it to forecast multiple action steps ahead so
that it can act in time before a harmful action occurs. How-
ever, simply understanding the next action steps on a high
level is not sufficient: it must also reason about where the
action will occur. Actions such as “grab a tool” are likely
harmless when performed in a toolbox; they become danger-
ous when done next to an active table saw or moving robot
arm. The monitoring system thus also needs to be able to
reason about spatial relations in 3D – for both the location
and body pose of involved humans.
To support these types of applications, we must address
two tasks: 1) forecasting long-term action sequences, and 2)
predicting future 3D human poses. Prior work has focused on
each of these tasks separately: activity forecasting predicts
future action labels without considering the 3D poses [ 32,
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
19902
34,35,46,48,67], while 3D pose forecasting focuses on
fixed frame rate sequence prediction limited to single actions
in short-term time frames without considering longer-term
action sequences [30, 58, 59, 88, 91].
We propose that these two tasks are coupled in nature:
predicting action labels with realized 3D poses helps to
encourage richer feature learning and can materialize sub-
category level differences in actions for predicting future
activities, and grounding 3D poses with actions provides
global structure for longer-term forecasting.
Leveraging this insight, we design a method that takes
in a sequence of recent RGB image observations and their
action labels, and jointly predicts a sequence of future 3D
characteristic poses and action labels (Fig. 1). In our design,
we had to address two significant research challenges: 1)
forecasting 3D poses from 2D images without any paired 3D
training data, and 2) forecasting long sequences of actions
comprising several discrete action steps.
The first challenge arises from limited training data. It
would be ideal to have a dataset with ground truth 3D pose
and action annotations for complex sequences of actions.
Unfortunately, no such dataset exists. There are RGB video
datasets with tracked 3D poses for limited types of actions
(e.g., walking or waving); and there are video datasets with
action labels for complex sequences of actions (e.g., cook-
ing or assembly). However, there is no single dataset that
has both types of annotations, and capturing one would be
difficult due to the challenges of setting up 3D pose track-
ers in settings where people typically perform complex se-
quences of actions (e.g., cooking in a kitchen). Instead, we
have to learn to use 2D video observations for 3D pose and
action label forecasting without paired data. We achieve
this by weakly supervising our pose forecasting in 2D us-
ing readily available 2D action datasets [ 8,68] and formu-
late an adversarial loss encouraging likely 3D characteristic
poses with respect to a distribution learned from 3D pose
datasets [ 40,57,77]. Crucially, this does not require any
correspondence between the 2D video and 3D pose data.
The second challenge arises from the difficulties of pre-
dicting long sequences of discrete events. One option would
be to train a model to output a multi-step sequence of ac-
tions and poses all at once – however, this is impossible
given the exponential growth of multi-step sequences and
the limited amount of available training data. Another op-
tion would be to train a model that predicts the next future
poses and actions at fixed time points in the future (e.g., 1s
in advance) and then recurrently make long-term predictions
– however, this time-based forecasting approach produces
sequences that tend to “drift” over the long-term, since the in-
termediate poses at fixed time steps are usually “in between”
semantically meaningful actions and thus do not provide a
distinctive input representation for the next prediction. To
address this issue, we train our autoregressive approach toiteratively generate the next discrete action label along with
the3D characteristic pose for that action. A 3D characteris-
tic pose [ 22] is the set of 3D joint positions corresponding
to the most distinctive moment a semantic action is being
performed (e.g., when a hand grasps an object, when two ob-
jects are first brought together, etc.). By training our method
to produce these poses as intermediate outputs (and inputs
to the next step), we are able to generate more semantically
plausible forecasts over longer action sequences.
Our experiments with two RGB video datasets demon-
strate that our approach for joint prediction of action be-
haviors and 3D poses outperforms state-of-the-art methods
applied separately to each task. Additionally, we find that
predicting actions and their 3D characteristic poses enables
more robust autoregressive prediction for longer-term fore-
casting. Overall, our contributions are:
•The first method to learn forecasting of future 3D poses
from datasets with only 2D RGB video and action label
data (i.e., without any paired 3D data).
•The first method to forecast future 3D poses jointly with
action labels from commonly available video input.
•The first method to forecast future characteristic 3D poses
and action labels for long-term and complex behaviors.
2. Related Work
3D Human Pose Forecasting. Forecasting 3D human
poses has been studied in many previous works and is com-
monly formulated as a 3D sequential motion prediction task,
taking an input 3D sequence of poses and generating an
output 3D sequence of poses. For short-term future pre-
diction (up to ≈1second), RNN-based approaches have
achieved impressive performance [ 1,16,29,36,37,42,
61,65,85]. As RNNs tend to struggle to capture longer-
term dependencies with a fixed-size history, graph-based
[17,18,20,50,52,53,58,73,88,93] and attention-based
[2,11,59,62,78] approaches have been proposed to encode
temporal history. Some methods also explored the appli-
cability of temporal convolutions [ 51,63] and MLP-only
architectures [ 10,38] to the task of human motion forecast-
ing. Additionally, various approaches have been proposed to
model future human motion stochastically to produce diverse
future sequence predictions, either with adversarial GAN
formulations [ 7,49], conditional variational autoencoders
(V AEs) [ 3,9,12,56,60,70,82,88,90], or diverse sam-
pling [ 21,91]. More recently, diffusion methods [ 74,75]
have been used for human motion generation and forecasting
[6,19,44,79,80,89,92,94]. These methods require 3D
ground truth sequences for training, limiting applicability to
scenarios where 3D inputs and ground-truth are not available.
Ours requires only 2D training data for the action sequences,
which is far more plentiful and easier to obtain. We generate
valid 3D poses by leveraging an adversarial loss formulation,
operating on a database of uncorrelated 3D poses.
19903
2D Human Pose 
ObservationsTimePose 
History 
Encoder
Action 
Encoder
Object 
EncoderActions
Objects2D 
Proj.
drawer, frying 
pan, hand knife, onion hand, stove<take= <cut=<change 
temperature=&
&
Input 2D Sequence
Action: <add= Action CE Loss
2D Joint Loss 3D Adversarial LossMLP Decoder
Characteristic 
3D Pose
Figure 2. Our approach takes as input a sequence of RGB images, from which 2D poses are extracted, as well as their corresponding action
label and initial set of objects. Each input is encoded into a joint latent space to jointly predict the next action label and characteristic 3D
pose. While action labels are directly supervised, the 3D pose decoder is trained to match the next 2D action pose using differentiable
projection, and an adversarial 3D loss encourages valid 3D pose prediction.
Human Action Forecasting. Action forecasting has been
studied by many approaches to predict future actions from
a sequence of observed actions [ 24,25,28,45] or directly
from an input video sequence [ 27,31,33,34,64,71,72,72].
Various methods have been developed to learn effective
representations, including Hidden Markov Models [ 48],
RNNs [ 24–27,31,41,72,86], transformer-based networks
[34,35,69], and self-supervised feature learning [ 39,81].
There are approaches that focus on the short-term future
[28,31,33,34,64,69,71,72] or on longer-term actions
[24,25,27,28,31,33–35,45,64,71,72,72]. Such method
focus on characterizing anticipation with action labels only,
while we aim to predict a richer characterization of the an-
ticipated future by leveraging characteristic 3D poses, repre-
sentative of future action goals in a sequence of action-pose
predictions. Forecasting actions alongside human poses in
2D only has been studied in a few works, for 2D hand place-
ment [ 54] or full-body 2D human poses at most 1 second into
the future [ 95]. Our approach addresses the benefits of 3D
reasoning in human motion forecasting, without requiring
full 3D sequences for supervision.
Goal-Driven Future Prediction. Goal-driven forecasting
has previously been explored beyond action label forecast-
ing, and has been leveraged to predict goal locations for
future human walking trajectories [ 14] and for future video
sequences by predicting keyframes [ 5,43,66,76]. Diller
et al. [ 22] introduced the task of forecasting characteris-
tic 3D poses , salient keyframe poses representing the next
action. These goal-based poses are more semantically mean-
ingful and consistent across different action sequences than
time-based ones, and thus are better suited for long-term
forecasting. We build upon these ideas by introducing a new
goal-driven method for joint action anticipation and charac-
teristic 3D pose forecasting in an auto-regressive system that
can predict complex, long-term behavior sequences.3. Method Overview
Our method aims to learn to jointly model future human
actions along with the characteristic 3D poses representative
of those actions. From a sequence of RGB image obser-
vations of a person performing a series of actions and the
corresponding action labels, we predict a sequence of future
action labels and 3D poses characteristic of these actions.
This enables joint reasoning of not only global semantic
behavior but also the physical manifestation thereof.
In the absence of 3D pose data of complex human ac-
tions, we weakly supervise forecasted 3D poses to align to
future poses in 2D, and constrain the poses to be valid in
3D using an adversarial loss with a database of 3D poses.
This does not require any correspondence between 3D pose
data and 2D video, enabling action sequence supervision
on commonly available 2D human action data together with
carefully captured but unrelated human poses in 3D.
An overview of this approach is shown in Fig. 2. For
an input sequence S={(Ii, ai, oi)}ofNRGB images
{Ii}with corresponding actions {ai}and initially involved
objects {oi}, we aim to predict the future Mactions {ˆak}
that will be taken along with their characteristic poses in 3D
{ˆYk}. We define the human pose as a collection of Jbody
joints at salient locations, so each output pose ˆYkis predicted
as a set of J3D coordinates. We first extract information
about the observed 2D pose movement by detecting 2D poses
{Xi}, each with J2D joints, with a state-of-the-art 2D pose
estimator that seamlessly integrates into our pipeline in a
pre-trained and frozen form.
Next, we encode this information along with previously
observed action and object labels to predict the next future
action label ˆakand characteristic 3D pose ˆYk. We can then
forecast a future sequence by autoregressively predicting
a series, considering the 2D projections of the previously
predicted 3D poses along with previously predicted actions
as input to a new prediction.
19904
4. Joint Forecasting of Actions and Character-
istic 3D Poses
Our network takes as input the previous 2D observations
{Xi}extracted from the {Ii}images, as well as action and
object labels {ai}and{oi}as one-hot vectors. Since we only
predict action labels, object labels are given from the objects
seen at the beginning of the sequence, and subsequently re-
used for the entire sequence. Each of these are encoded in
parallel with three separate encoders; the actions and objects
with MLPs while the poses are projected into latent space
with a single linear layer and then processed with a stack of
three residual blocks. These encoded features are then all
concatenated together in latent space, and processed jointly
with an MLP to produce a common latent code z. Finally, we
decode both poses and actions in parallel based on zusing
an MLP decoder each, yielding the next action label class as
a vector ˆak∈RNaand 3D characteristic pose ˆYk∈RJ×3,
withNathe number of action classes. For a more detailed
architecture specification, we refer to the appendix.
We jointly learn future action labels and characteristic 3D
poses by supervising ˆakandˆYkto match the observed future
2D video, and constrain ˆYkto form a valid 3D pose by an
adversarial loss, optimizing for the overall loss:
L=λactionLaction +λpose2dLpose2d+λadv3dLadv3d(1)
where Laction denotes the action loss, as described in
Sec. 4.1, Lpose2dandLadv3dconstraining the predicted pose,
as described in Sec. 4.2, and the λweighting each loss.
4.1. Action Forecasting
Predicted future actions are decoded from the latent code
zby an MLP decoder to predict the action class ˆak, super-
vised by cross entropy with the ground truth future action:
Laction = CE(ˆ ak, agt
k).
4.2. Characteristic Pose Forecasting
Our goal is to forecast complex action behavior not only in
terms of action labels, but also manifested as a sequence
of characteristic poses in 3D. Since we only have 2D pose
annotations available, we first constrain these poses to rep-
resent future actions in 2D and make use of an adversarial
regularization in 3D. This does not require any correspon-
dence between 2D and 3D data, only a collection of valid
3D poses, which are readily available.
Differentiable 2D Projection Our generator network pre-
dicts the next characteristic action pose ˆYkas a set of 3D
joints. To constrain ˆYkbased on the target future 2D pose
Xgtextracted from the ground truth future image, we differ-
entiably project ˆYkinto the 2D image with intrinsic parame-
tersKand extrinsic rotation and translation R, t:
ˆX=K(RˆYk+t) (2)Since we learn from third-person video with a fixed cam-
era, we can use the same camera parameters for all sequences
used for training. We can then define the 2D pose loss as the
mean squared error between the projected pose prediction
and the ground truth:
Lpose2d=||Xgt−Xk||2
2 (3)
Note that we only predict the Jjoints that have been
observed in the video data (excluding any joints that remain
occluded in the observed video data), so this loss can be
applied to all predicted joints.
Adversarial 3D Pose Regularization. While the action and
pose prediction losses provide effective predictions when
considered in the 2D projections, the {ˆYk}remain undercon-
strained in 3D and thus tend to exhibit large distortions and
implausible bone lengths and angles, when trained with only
2D supervision. We thus constrain the predicted poses to
form valid 3D poses by formulating an adversarial 3D loss
from a critic network which is simultaneously trained to dis-
tinguish predicted poses from a database of real 3D skeleton
samples. Note that there is no correspondence between these
skeletons and the 2D poses extracted from the action video
sequences – any database of 3D skeletons can be used. We
can thus train our approach with an entirely uncorrelated 3D
pose dataset without requiring 3D action pose correlations.
We then formulate Ladv3das a Wasserstein loss [ 4], train-
ing the critic network in an alternating fashion with the
generator. This enables effective forecasting of future 3D
characteristic poses for predicted future action labels, with-
out requiring any 3D observations as input.
In order to enable the critic network to learn effectively
about likely intrinsic pose constraints (e.g., lengths, kine-
matic chains, or valid joint angles), the critic takes as input
not only the 3D joint locations of ˆYkbut also their kinematic
statistics as a matrix Ψ, following [83, 84].
Ψencodes joint angles and bone lengths as Ψ =BTB,
where B= (b1, b2, . . . , b b)is a matrix with columns bi=
jk−jlrepresenting the vectors between each joint jkandjl.
Ψthen contains bone lengths l2
ion its diagonal, and angular
representations on the off-diagonal entries.
4.3. Sequence Prediction
In order to forecast longer-term future behavior, our 3D
pose predictions enable a natural autoregressive sequence
prediction by taking the predictions ˆXt,ˆatat time step tas
part of the input for time step t+ 1. We can thus predict a
sequence of Mfuture action labels {ˆak}and characteristic
3D poses {ˆYk}; we use M= 10 for MPII Cooking II [ 68]
andM= 5for IKEA-ASM [8], respectively.
4.4. Training Details
We train our approach for the J= 9joints commonly seen
across the input observed video data, characterizing the up-
19905
MPII Cooking II IKEA ASM
2d 3d Action Accuracy 2d 3d Action Accuracy
Approach MPJPE [px] ↓Quality ↑ top-1↑top-3↑ MPJPE [px] ↓Quality ↑ top-1↑top-3↑
Zero Velocity 118 – – – 74 – – –
Train Average 166 – – – 91 – – –
A VT [34] RGB – – 19% 42% – – 22% 49%
A VT [34] RGB+Skeleton – – 20% 40% – – 23% 47%
FUTR [35] RGB – – 27% 48% – – 19% 45%
FUTR [35] RGB+Skeleton – – 27% 49% – – 20% 46%
RepNet [83] + DLow (min-10) [91] 72 0.72 – – 45 0.31 – –
RepNet [83] + GSPS (min-10) [60] 59 0.66 – – 51 0.15 – –
RepNet [83] + STARS (det.) [88] 70 0.62 – – 54 0.27 – –
RepNet [83] + EqMotion [87] 68 0.66 – – 55 0.23 – –
Joint 2D Pose & Action [95] 55 - 27% 43% 44 - 22% 46%
Ours 50 0.55 29% 51% 40 0.31 29% 50%
Table 1. Quantitative comparison with state-of-the-art action label and 3D pose forecasting. Our joint approach enables more accurate future
action and pose predictions, compared to approaching both tasks separately, and outperforms joint action and 2D pose forecasting.
per body in MPII Cooking II [68] and IKEA-ASM [8].
Additionally, we use loss weights λaction = 1e6,λpose=
1, and λadv3d= 1, empirically chosen to numerically bal-
ance each individual loss with the others.
We train our approach on a single NVIDIA GeForce RTX
2080TI for ≈12hours until convergence. We use ADAM
with batch size 4096, weight decay 0.001, and a constant
learning rate of 0.0001 for both generator and discriminator.
4.5. Datasets
We train and evaluate our approach on two datasets: MPII
Cooking II [ 68] and IKEA-ASM [ 8]. Both datasets contain
sequences of human actors performing complex, unscripted
actions, and provide annotations of fine-grained sub-action
steps. MPII Cooking II [ 68] is an action recognition dataset
with 272 complex cooking sequences and an average se-
quence time of 182s (35 annotated sub-actions, each 5.2s
on average). IKEA-ASM contains 370 sequences of actors
assembling IKEA furniture, with an average of 74s per se-
quence (15 annotated sub-actions, each 4.9s on average).
In both datasets, each action sequence has been filmed
from a fixed camera setup; the third-person point of view
enables extraction of 2D poses with an off-the-shelf 2D pose
estimator. We use OpenPose [ 13] in our experiments and
note that our approach is agnostic to the concrete method
of 2D pose detection. We provide more in-depth discussion
and additional experiments in the supplemental material.
We consider the 9 upper-body joints of the OpenPose
skeletons, as the other joints are almost always occluded
in the video observations, and remove global translation by
centering each 2D pose at the neck joint.
Characteristic poses, in contrast to an arbitrary pose
within a labeled action range, are the most representative
pose of that action, and are annotated for all sub-actions in
each sequence as the most articulated pose of that sub-action,
following the annotation protocol of [ 22]. Annotation can be
done efficiently and was performed by the authors within just
32 hours, yielding a total of ≈18,000 characteristic poses(≈12,000 for MPII Cooking II and ≈6,000 for IKEA-ASM).
These poses are indicative of the action they represent as
demonstrated in Tab. 2: Using such poses significantly im-
proves performance, validating our annotation protocol.
For the 3D adversarial loss, we use ≈800,000 human
poses from popular 3D pose datasets: Human3.6m [ 40],
AMASS [ 57], and GRAB [ 77]. Note that none of these 3D
poses have any correspondence with the 2D posed actions
from the MPII Cooking II dataset, instead depicting various
human skeletons in natural and diverse poses.
5. Results
We evaluate sequence forecasting of action labels and char-
acteristic 3D poses on the MPI Cooking II [ 68] and IKEA-
ASM [ 8] datasets, and 3D pose quality by comparing to our
database of high-fidelity 3D poses.
5.1. Evaluation Metrics
2D Pose Error. Since we only have 2D ground-truth data
available for complex action sequences, we first project pre-
dicted 3D poses back into 2D, and evaluate the 2D mean
per-joint position error (MPJPE) [ 40], in comparison with
2D poses extracted from ground-truth future frames using
[13]: E MPJPE =1
MPM
j=1||ˆX−Xgt||2.
3D Pose Quality. In the absence of annotated ground truth
3D poses for the action video sequences, we measure the
quality of predicted 3D poses as how distinguishable they
are in comparison to a set of real 3D poses. We follow
[3] and evaluate quality by training a binary classifier on
50,000 human poses generated at different training steps
(representing examples of unrealistic 3D poses) and 50,000
real 3D pose samples. For classification accuracy aof this
classifier, quality is measured as 1−a, with a quality of 1
indicating full indistinguishability from real poses. We refer
to the supplemental for more details on this quality metric.
Action Accuracy. We report the action accuracy of the pre-
dicted sequences, as the mean over all sequences in the test
19906
152025303540
135710Action Accuracy [%]# steps in autoregressive sequenceOursAction OnlyFigure 3. Action accuracy over time. Our joint action-characteristic
pose forecasting enables more robust autoregressive action fore-
casting than action prediction without considering pose.
set. We evaluate the top- naccuracy based on whether the
ground truth action is among the nhighest scoring predic-
tions, for n= 1andn= 3.
5.2. Comparison to Human Pose Forecasting
Tab. 1 compares our method to state-of-the-art 3D pose fore-
casting methods DLow [ 91], GSPS [ 60], STARS [ 88], and
EqMotion [ 87]. These methods expect sequences of ob-
served 3D human poses as input; we thus first apply a state-
of-the-art weakly supervised 3D pose estimator [ 83] on our
2D input poses, producing inputs and supervision in 3D. This
method estimates 3D poses using an adversarial formulation,
requiring a database of 3D poses not correlated with the 2D
pose inputs. To ensure a fair comparison, this database is
exactly the same as the one our method uses.
We chose the 3D pose estimator of [ 83] since its weakly
supervised formulation is most comparable to our approach.
An additional comparison to a fully supervised approach for
3D pose lifting (SPIN [ 47]) is provided in the supplemental.
We then train the 3D pose prediction methods from
scratch on this generated data, using their original parameter
settings. Stochastic methods DLow and GSPS are set to pre-
dict 10 possible future sequences; we report the minimum
error across these. We use STARS in the method’s deter-
ministic mode. Each method takes as input a pose history
ofMposes and outputs a sequence of Mposes, analogous
to our setup where each pose is a characteristic pose corre-
sponding to an action step ( M= 10 for MPII Cooking II
andM= 5 for IKEA-ASM). Our approach to lift 2D to
future 3D poses and actions in an end-to-end fashion enables
more effective pose forecasting than these state-of-the-art
3D pose forecasting approaches on both datasets.
In addition, we compare to the joint 2D action and pose
forecasting approach of Zhu et al. [ 95]. Our approach of
forecasting long-term sequences of 3D poses alongside ac-
tions is able to outperform their 2D MPJPE pose prediction
and action accuracy performance, due to improved spatial
reasoning when forecasting 3D poses.Statistical 2D Baselines. We additionally compare with
two statistical baselines in 2D, following [ 22]: the average
target train pose, and a zero-velocity baseline which was
introduced by Martinez et al. [ 61] as competitive with state
of the art. We outperform both baselines, indicating that our
method learns a strong action pose representation.
5.3. Comparison to Action Label Forecasting
We compare the action accuracy of our joint action-pose
forecasting to A VT [ 34] and FUTR [ 35], two state-of-the-
art action anticipation methods, in Tab. 1. We train and
evaluate both A VT and FUTR on input RGB frames and
their action and object labels, equal to our training setup,
and use their original training settings initialized with a pre-
trained vision transformer [ 23] for A VT and extracted I3D
features [ 15] from our datasets for FUTR. Additionally, as
we consider extracted 2D poses from the input RGB images,
we also evaluate a variant that is trained and evaluated on
RGB images overlaid with 2D poses (“+Skeleton”). Our
approach outperforms these baselines in both scenarios, by
jointly predicting future actions and characteristic 3D poses.
5.4. Ablation Studies
What is the effect of pose forecasting on long-term action
understanding? Tab. 3 shows that there is a notable im-
provement in action accuracy between training only with an
action loss vs. training action and 2D pose loss jointly. This
becomes more apparent when training action only vs action
and full pose prediction (2D and 3D losses). In addition,
Fig. 3 shows the correspondence between autoregressive pre-
diction length and action accuracy: jointly forecasting poses
and actions enables more robust autoregressive forecasting
over time. We conclude that pose forecasting is beneficial
for long-term action understanding.
How does action forecasting affect pose prediction per-
formance? Tab. 3 demonstrates that pose forecasting trained
jointly with action prediction is complementary and enables
more accurate pose prediction.
What is the effect of characteristic pose forecasting?
Since state-of-the-art pose forecasting focuses on fixed frame
rate predictions independent of actions, we compare with
Poses 2D 3D Action Accuracy
Train Test MPJPE [px] ↓ Quality ↑top-1↑ top-3↑
Uncoupled Uncoupled 75 0.29 28% 48%
Middle Middle 58 0.45 26% 43%
Random Random 67 0.37 22% 42%
Characteristic Characteristic 50 0.55 29% 51%
Table 2. Ablation on pose forecasting on MPII Cooking II [ 68]. Our
characteristic pose representation maximizes MPJPE and action
performance: We consider pose prediction following state-of-the-
art pose forecasting as decoupled from actions (uncoupled), as well
as poses coupled to actions but in the middle of an action range,
or at a random time therein, and our characteristic pose prediction.
The same pose type is used for both train and evaluation.
19907
MPII Cooking II IKEA ASM
Losses During Training 2D 3D Action Accuracy 2D 3D Action Accuracy
Action 2D Proj. 3D Adv. MPJPE [px] ↓Quality ↑ top-1↑top-3↑ MPJPE [px] ↓Quality ↑ top-1↑top-3↑
✓ × × – – 21% 41% – – 24% 45%
✓ ✓ × 62 0.10 26% 49% 46 0.05 27% 49%
× ✓ × 54 0.21 – – 44 0.09 – –
× ✓ ✓ 58 0.53 – – 43 0.29 – –
✓ ✓ ✓ 50 0.55 29% 51% 40 0.31 29% 50%
Table 3. Ablation on the effect of the action, 2D projection, and 3D adversarial losses. Combining all together for joint forecasting enables
complementary learning to produce the best performance.
such joint forecasting of action and pose where predicted
poses are sampled at equally spaced points in time in Tab. 2
(uncoupled). Additionally, we consider alternative poses to
forecast for each action rather than a characteristic 3D pose
(middle of the annotated action range, and randomly selected
within the action range). We keep the same pose representa-
tion for training and testing (i.e., evaluate on middle poses
when trained on them, etc.), for a fair comparison. We ob-
serve the best performance when forecasting characteristic
3D poses along with action labels, showing their usefulness
for forecasting long sequences of 3D poses and actions.
5.5. Qualitative Results
Qualitative evaluations for the predicted poses are shown in
Fig. 5 on data from MPII Cooking II [ 68] and in Fig. 4 on
data from IKEA-ASM [ 8]. We compare our approach with
state-of-the-art 3D pose forecasting of DLow [ 91], GSPS
[60], and STARS [ 88]. For each method, we show a 3D body
mesh in addition to the predicted 3D pose joints, to more
comprehensively show the 3D structure of the forecasting
results; we obtain body meshes by fitting SMPL [ 55] to eachmethods’ predicted 3D body joints.
As there is no 3D ground truth available, we show the
camera perspective with background for context as well
as without background for a 3D pose only version. The
two views demonstrate the fit to the ground truth 2D along
with the quality of the 3D pose, respectively. Our approach
leads to poses that better follow the ground-truth action
poses in 2D compared to both previous methods while still
maintaining a valid pose structure in 3D. Notably, this is
true for both datasets, as our approach effectively forecasts
the different data characteristics of both cooking as well as
furniture assembly. In particular, our joint action-3D pose
forecasting enables more accurate forecasting with diverse
and accurate 3D pose structures.
5.6. Limitations
While we have demonstrated the potential of joint action
and 3D pose forecasting, several limitations remain. For
instance, our method leverages a separate 2D pose extraction
as input to training, while an end-to-end formulation could
potentially better leverage other useful signal in the input
Input
<rotate= <pick up= <align= <spin= <pick up=Target
<align= <spin= <pick up= <align= <spin=
DLow GSPS
TimeOurs
<align= <spin= <align= <spin= <spin=
STARS
Figure 4. Qualitative comparison between DLow [ 91], GSPS [ 60], STARS [ 88], and our method on IKEA-ASM [ 8] data. For each method,
we show the 3D predicted pose projected into the 2D target view, without background (small) and with background for context (full size).
Our joint reasoning captures the individual characteristic action poses more faithfully while producing spatially plausible 3D poses.
19908
frames. Additionally, a more holistic body representation
than pose joints would be important for finer-grained inter-
actions that involve reasoning over small limbs (e.g., hands)
and body surface contact.
6. Conclusion
In this paper, we proposed to forecast future human behavior
by jointly predicting future actions alongside characteris-
tic 3D poses. We do not require any 3D annotated action
sequences, or 3D input data; instead, we learn complex ac-
tion sequences from 2D action video data, and regularizepredicted poses with an adversarial formulation against un-
correlated 3D pose data. Experiments demonstrate that our
joint forecasting enables complementary feature learning,
outperforming each individual task considered separately.
Acknowledgements
This project is funded by the Bavarian State Ministry of
Science and the Arts and coordinated by the Bavarian Re-
search Institute for Digital Transformation (bidt), and the
German Research Foundation (DFG) Grant “Learning How
to Interact with Scenes through Part-Based Understanding”.
Input Ours
<wash= <wash= <move lid= <shake= <add=Target
<wash= <wash= <shake= <take= <add=<open= <wash= <dry= <throw in garbage= <take= <spice= <screw= <take= <take= <screw=
<spice= <screw= <take= <pour= <screw=<spice= <screw= <take= <add= <stir=
Time Time
STARS
 DLow
 GSPS
Figure 5. Qualitative comparison between DLow [ 91], GSPS [ 60], STARS [ 88], and our method on two sequences (left and right) from
MPII Cooking II [ 68]. For each method, we show the 3D predicted pose projected into 2D, without background (small) and with background
for context (full size). By considering both 3D pose and action forecasting together, we more effectively forecast the longer-term behavior.
19909
References
[1]Emre Aksan, Manuel Kaufmann, and Otmar Hilliges. Struc-
tured prediction helps 3d human motion modelling. In 2019
IEEE/CVF International Conference on Computer Vision,
ICCV 2019, Seoul, Korea (South), October 27 - November 2,
2019 , pages 7143–7152. IEEE, 2019. 2
[2]Emre Aksan, Manuel Kaufmann, Peng Cao, and Otmar
Hilliges. A spatio-temporal transformer for 3d human motion
prediction. In International Conference on 3D Vision, 3DV
2021, London, United Kingdom, December 1-3, 2021 , pages
565–574. IEEE, 2021. 2
[3]Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salzmann,
Lars Petersson, and Stephen Gould. A stochastic conditioning
scheme for diverse human motion prediction. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5223–5232, 2020. 2, 5
[4]Mart ´ın Arjovsky, Soumith Chintala, and L ´eon Bottou. Wasser-
stein generative adversarial networks. In Proceedings of the
34th International Conference on Machine Learning, ICML
2017, Sydney, NSW, Australia, 6-11 August 2017 , pages 214–
223. PMLR, 2017. 4
[5]Amir Bar, Roei Herzig, Xiaolong Wang, Anna Rohrbach, Gal
Chechik, Trevor Darrell, and Amir Globerson. Compositional
video synthesis with action graphs. In Proceedings of the
38th International Conference on Machine Learning, ICML
2021, 18-24 July 2021, Virtual Event , pages 662–673. PMLR,
2021. 3
[6]Germ ´an Barquero, Sergio Escalera, and Cristina Palmero.
Belfusion: Latent diffusion for behavior-driven human mo-
tion prediction. In IEEE/CVF International Conference on
Computer Vision, ICCV 2023, Paris, France, October 1-6,
2023 , pages 2317–2327. IEEE, 2023. 2
[7]Emad Barsoum, John Kender, and Zicheng Liu. Hp-gan:
Probabilistic 3d human motion prediction via gan. In Pro-
ceedings of the IEEE conference on computer vision and
pattern recognition workshops , pages 1418–1427, 2018. 2
[8]Yizhak Ben-Shabat, Xin Yu, Fatemeh Saleh, Dylan Campbell,
Cristian Rodriguez-Opazo, Hongdong Li, and Stephen Gould.
The ikea asm dataset: Understanding people assembling furni-
ture through actions, objects and pose. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer
Vision , pages 847–859, 2021. 2, 4, 5, 7
[9]Andreas Blattmann, Timo Milbich, Michael Dorkenwald, and
Bj¨orn Ommer. Behavior-driven synthesis of human dynam-
ics. In IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2021, virtual, June 19-25, 2021 , pages
12236–12246. Computer Vision Foundation / IEEE, 2021. 2
[10] Arij Bouazizi, Adrian Holzbock, Ulrich Kressel, Klaus Diet-
mayer, and Vasileios Belagiannis. Motionmixer: Mlp-based
3d human body pose forecasting. In Proceedings of the Thirty-
First International Joint Conference on Artificial Intelligence,
IJCAI 2022, Vienna, Austria, 23-29 July 2022 , pages 791–798.
ijcai.org, 2022. 2
[11] Yujun Cai, Lin Huang, Yiwei Wang, Tat-Jen Cham, Jianfei
Cai, Junsong Yuan, Jun Liu, Xu Yang, Yiheng Zhu, Xiaohui
Shen, Ding Liu, Jing Liu, and Nadia Magnenat-Thalmann.
Learning progressive joint propagation for human motionprediction. In Computer Vision - ECCV 2020 - 16th European
Conference, Glasgow, UK, August 23-28, 2020, Proceedings,
Part VII , pages 226–242. Springer, 2020. 2
[12] Yujun Cai, Yiwei Wang, Yiheng Zhu, Tat-Jen Cham, Jianfei
Cai, Junsong Yuan, Jun Liu, Chuanxia Zheng, Sijie Yan,
Henghui Ding, Xiaohui Shen, Ding Liu, and Nadia Magnenat-
Thalmann. A unified 3d human motion synthesis model via
conditional variational auto-encoder∗. In 2021 IEEE/CVF
International Conference on Computer Vision, ICCV 2021,
Montreal, QC, Canada, October 10-17, 2021 , pages 11625–
11635. IEEE, 2021. 2
[13] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y . A.
Sheikh. Openpose: Realtime multi-person 2d pose estima-
tion using part affinity fields. IEEE Transactions on Pattern
Analysis and Machine Intelligence , 2019. 5
[14] Zhe Cao, Hang Gao, Karttikeya Mangalam, Qi-Zhi Cai, Minh
V o, and Jitendra Malik. Long-term human motion prediction
with scene context. In European Conference on Computer
Vision , pages 387–404. Springer, 2020. 3
[15] Jo˜ao Carreira and Andrew Zisserman. Quo vadis, action
recognition? A new model and the kinetics dataset. In 2017
IEEE Conference on Computer Vision and Pattern Recog-
nition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017 ,
pages 4724–4733. IEEE Computer Society, 2017. 6
[16] Hsu-Kuang Chiu, Ehsan Adeli, Borui Wang, De-An Huang,
and Juan Carlos Niebles. Action-agnostic human pose fore-
casting. In IEEE Winter Conference on Applications of Com-
puter Vision, WACV 2019, Waikoloa Village, HI, USA, Jan-
uary 7-11, 2019 , pages 1423–1432. IEEE, 2019. 2
[17] Qiongjie Cui and Huaijiang Sun. Towards accurate 3d human
motion prediction from incomplete observations. In IEEE
Conference on Computer Vision and Pattern Recognition,
CVPR 2021, virtual, June 19-25, 2021 , pages 4801–4810.
Computer Vision Foundation / IEEE, 2021. 2
[18] Qiongjie Cui, Huaijiang Sun, and Fei Yang. Learning dy-
namic relationships for 3d human motion prediction. In
2020 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19,
2020 , pages 6518–6526. Computer Vision Foundation / IEEE,
2020. 2
[19] Rishabh Dabral, Muhammad Hamza Mughal, Vladislav
Golyanik, and Christian Theobalt. Mofusion: A framework
for denoising-diffusion-based motion synthesis. In IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023 , pages
9760–9770. IEEE, 2023. 2
[20] Lingwei Dang, Yongwei Nie, Chengjiang Long, Qing Zhang,
and Guiqing Li. MSR-GCN: multi-scale residual graph con-
volution networks for human motion prediction. In 2021
IEEE/CVF International Conference on Computer Vision,
ICCV 2021, Montreal, QC, Canada, October 10-17, 2021 ,
pages 11447–11456. IEEE, 2021. 2
[21] Lingwei Dang, Yongwei Nie, Chengjiang Long, Qing Zhang,
and Guiqing Li. Diverse human motion prediction via gumbel-
softmax sampling from an auxiliary space. In MM ’22: The
30th ACM International Conference on Multimedia, Lisboa,
Portugal, October 10 - 14, 2022 , pages 5162–5171. ACM,
2022. 2
19910
[22] Christian Diller, Thomas Funkhouser, and Angela Dai. Fore-
casting characteristic 3d poses of human actions. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 15914–15923, 2022. 2, 3, 5, 6
[23] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In 9th International Conference on Learning Represen-
tations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 .
OpenReview.net, 2021. 6
[24] Yazan Abu Farha and Juergen Gall. Uncertainty-aware antici-
pation of activities. In 2019 IEEE/CVF International Confer-
ence on Computer Vision Workshops, ICCV Workshops 2019,
Seoul, Korea (South), October 27-28, 2019 , pages 1197–1204.
IEEE, 2019. 3
[25] Yazan Abu Farha, Alexander Richard, and Juergen Gall.
When will you do what? - anticipating temporal occurrences
of activities. In 2018 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2018, Salt Lake City, UT,
USA, June 18-22, 2018 , pages 5343–5352. Computer Vision
Foundation / IEEE Computer Society, 2018. 3
[26] Yazan Abu Farha, Alexander Richard, and Juergen Gall.
When will you do what? - anticipating temporal occurrences
of activities. In 2018 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2018, Salt Lake City, UT,
USA, June 18-22, 2018 , pages 5343–5352. Computer Vision
Foundation / IEEE Computer Society, 2018.
[27] Yazan Abu Farha, Qiuhong Ke, Bernt Schiele, and Juergen
Gall. Long-term anticipation of activities with cycle consis-
tency. In Pattern Recognition - 42nd DAGM German Confer-
ence, DAGM GCPR 2020, T ¨ubingen, Germany, September
28 - October 1, 2020, Proceedings , pages 159–173. Springer,
2020. 3
[28] Basura Fernando and Samitha Herath. Anticipating human
actions by correlating past with the future with jaccard simi-
larity measures. In IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021 ,
pages 13224–13233. Computer Vision Foundation / IEEE,
2021. 3
[29] Katerina Fragkiadaki, Sergey Levine, Panna Felsen, and Jiten-
dra Malik. Recurrent network models for human dynamics.
In2015 IEEE International Conference on Computer Vision,
ICCV 2015, Santiago, Chile, December 7-13, 2015 , pages
4346–4354. IEEE Computer Society, 2015. 2
[30] Katerina Fragkiadaki, Sergey Levine, Panna Felsen, and Jiten-
dra Malik. Recurrent network models for human dynamics.
InProceedings of the IEEE international conference on com-
puter vision , pages 4346–4354, 2015. 2
[31] Antonino Furnari and Giovanni Maria Farinella. What would
you expect? anticipating egocentric actions with rolling-
unrolling lstms and modality attention. In 2019 IEEE/CVF
International Conference on Computer Vision, ICCV 2019,
Seoul, Korea (South), October 27 - November 2, 2019 , pages
6251–6260. IEEE, 2019. 3
[32] Antonino Furnari and Giovanni Maria Farinella. Rolling-
unrolling lstms for action anticipation from first-person video.IEEE transactions on pattern analysis and machine intelli-
gence , 43(11):4021–4036, 2020. 1
[33] Harshala Gammulle, Simon Denman, Sridha Sridharan, and
Clinton Fookes. Predicting the future: A jointly learnt model
for action anticipation. In 2019 IEEE/CVF International
Conference on Computer Vision, ICCV 2019, Seoul, Korea
(South), October 27 - November 2, 2019 , pages 5561–5570.
IEEE, 2019. 3
[34] Rohit Girdhar and Kristen Grauman. Anticipative video trans-
former. In 2021 IEEE/CVF International Conference on
Computer Vision, ICCV 2021, Montreal, QC, Canada, Octo-
ber 10-17, 2021 , pages 13485–13495. IEEE, 2021. 2, 3, 5,
6
[35] Dayoung Gong, Joonseok Lee, Manjin Kim, Seong Jong Ha,
and Minsu Cho. Future transformer for long-term action
anticipation. In IEEE/CVF Conference on Computer Vision
and Pattern Recognition, CVPR 2022, New Orleans, LA, USA,
June 18-24, 2022 , pages 3042–3051. IEEE, 2022. 2, 3, 5, 6
[36] Anand Gopalakrishnan, Ankur Mali, Dan Kifer, C. Lee Giles,
and Alexander G. Ororbia II. A neural temporal model for
human motion prediction. In IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2019, Long Beach,
CA, USA, June 16-20, 2019 , pages 12116–12125. Computer
Vision Foundation / IEEE, 2019. 2
[37] Liang-Yan Gui, Yu-Xiong Wang, Xiaodan Liang, and Jos ´e
M. F. Moura. Adversarial geometry-aware human motion
prediction. In Computer Vision - ECCV 2018 - 15th Euro-
pean Conference, Munich, Germany, September 8-14, 2018,
Proceedings, Part IV , pages 823–842. Springer, 2018. 2
[38] Wen Guo, Yuming Du, Xi Shen, Vincent Lepetit, Xavier
Alameda-Pineda, and Francesc Moreno-Noguer. Back to
MLP: A simple baseline for human motion prediction. In
IEEE/CVF Winter Conference on Applications of Computer
Vision, WACV 2023, Waikoloa, HI, USA, January 2-7, 2023 ,
pages 4798–4808. IEEE, 2023. 2
[39] Tengda Han, Weidi Xie, and Andrew Zisserman. Memory-
augmented dense predictive coding for video representation
learning. In Computer Vision - ECCV 2020 - 16th European
Conference, Glasgow, UK, August 23-28, 2020, Proceedings,
Part III , pages 312–329. Springer, 2020. 3
[40] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian
Sminchisescu. Human3.6m: Large scale datasets and predic-
tive methods for 3d human sensing in natural environments.
IEEE Trans. Pattern Anal. Mach. Intell. , 36(7):1325–1339,
2014. 2, 5
[41] Ashesh Jain, Avi Singh, Hema S Koppula, Shane Soh, and
Ashutosh Saxena. Recurrent neural networks for driver ac-
tivity anticipation via sensory-fusion architecture. In 2016
IEEE International Conference on Robotics and Automation
(ICRA) , pages 3118–3125. IEEE, 2016. 3
[42] Ashesh Jain, Amir Roshan Zamir, Silvio Savarese, and
Ashutosh Saxena. Structural-rnn: Deep learning on spatio-
temporal graphs. In 2016 IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV ,
USA, June 27-30, 2016 , pages 5308–5317. IEEE Computer
Society, 2016. 2
[43] Dinesh Jayaraman, Frederik Ebert, Alexei A. Efros, and
Sergey Levine. Time-agnostic prediction: Predicting pre-
19911
dictable video frames. In 7th International Conference on
Learning Representations, ICLR 2019, New Orleans, LA,
USA, May 6-9, 2019 . OpenReview.net, 2019. 3
[44] Chiyu ”Max” Jiang, Andre Cornman, Cheolho Park, Ben-
jamin Sapp, Yin Zhou, and Dragomir Anguelov. Motion-
diffuser: Controllable multi-agent motion prediction using
diffusion. In IEEE/CVF Conference on Computer Vision and
Pattern Recognition, CVPR 2023, Vancouver, BC, Canada,
June 17-24, 2023 , pages 9644–9653. IEEE, 2023. 2
[45] Qiuhong Ke, Mario Fritz, and Bernt Schiele. Time-
conditioned action anticipation in one shot. In IEEE Con-
ference on Computer Vision and Pattern Recognition, CVPR
2019, Long Beach, CA, USA, June 16-20, 2019 , pages 9925–
9934. Computer Vision Foundation / IEEE, 2019. 3
[46] Kris M Kitani, Brian D Ziebart, James Andrew Bagnell, and
Martial Hebert. Activity forecasting. In European conference
on computer vision , pages 201–214. Springer, 2012. 2
[47] Nikos Kolotouros, Georgios Pavlakos, Michael J. Black, and
Kostas Daniilidis. Learning to reconstruct 3d human pose
and shape via model-fitting in the loop. In 2019 IEEE/CVF
International Conference on Computer Vision, ICCV 2019,
Seoul, Korea (South), October 27 - November 2, 2019 , pages
2252–2261. IEEE, 2019. 6
[48] Hilde Kuehne, Ali Arslan, and Thomas Serre. The language of
actions: Recovering the syntax and semantics of goal-directed
human activities. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 780–787,
2014. 2, 3
[49] Jogendra Nath Kundu, Maharshi Gor, and R. Venkatesh Babu.
Bihmp-gan: Bidirectional 3d human motion prediction GAN.
InThe Thirty-Third AAAI Conference on Artificial Intelli-
gence, AAAI 2019, The Thirty-First Innovative Applications
of Artificial Intelligence Conference, IAAI 2019, The Ninth
AAAI Symposium on Educational Advances in Artificial In-
telligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 -
February 1, 2019 , pages 8553–8560. AAAI Press, 2019. 2
[50] Bin Li, Jian Tian, Zhongfei Zhang, Hailin Feng, and Xi Li.
Multitask non-autoregressive model for human motion pre-
diction. IEEE Trans. Image Process. , 30:2562–2574, 2021.
2
[51] Chen Li, Zhen Zhang, Wee Sun Lee, and Gim Hee Lee. Con-
volutional sequence to sequence model for human dynamics.
In2018 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-
22, 2018 , pages 5226–5234. Computer Vision Foundation /
IEEE Computer Society, 2018. 2
[52] Maosen Li, Siheng Chen, Yangheng Zhao, Ya Zhang, Yan-
feng Wang, and Qi Tian. Dynamic multiscale graph neural
networks for 3d skeleton based human motion prediction. In
2020 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19,
2020 , pages 211–220. Computer Vision Foundation / IEEE,
2020. 2
[53] Maosen Li, Siheng Chen, Zihui Liu, Zijing Zhang, Lingxi Xie,
Qi Tian, and Ya Zhang. Skeleton graph scattering networks
for 3d skeleton-based human motion prediction. In IEEE/CVF
International Conference on Computer Vision Workshops,ICCVW 2021, Montreal, BC, Canada, October 11-17, 2021 ,
pages 854–864. IEEE, 2021. 2
[54] Miao Liu, Siyu Tang, Yin Li, and James M Rehg. Forecasting
human-object interaction: joint prediction of motor attention
and actions in first person video. In European Conference on
Computer Vision , pages 704–721. Springer, 2020. 3
[55] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-
ard Pons-Moll, and Michael J. Black. SMPL: A skinned
multi-person linear model. ACM Trans. Graphics (Proc. SIG-
GRAPH Asia) , 34(6):248:1–248:16, 2015. 7
[56] Thomas Lucas, Fabien Baradel, Philippe Weinzaepfel, and
Gr´egory Rogez. Posegpt: Quantization-based 3d human
motion generation and forecasting. In Computer Vision -
ECCV 2022 - 17th European Conference, Tel Aviv, Israel,
October 23-27, 2022, Proceedings, Part VI , pages 417–435.
Springer, 2022. 2
[57] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger-
ard Pons-Moll, and Michael J. Black. AMASS: Archive of
motion capture as surface shapes. In International Conference
on Computer Vision , pages 5442–5451, 2019. 2, 5
[58] Wei Mao, Miaomiao Liu, Mathieu Salzmann, and Hongdong
Li. Learning trajectory dependencies for human motion pre-
diction. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 9489–9497, 2019. 2
[59] Wei Mao, Miaomiao Liu, and Mathieu Salzmann. History
repeats itself: Human motion prediction via motion attention.
InEuropean Conference on Computer Vision , pages 474–489.
Springer, 2020. 2
[60] Wei Mao, Miaomiao Liu, and Mathieu Salzmann. Generating
smooth pose sequences for diverse human motion prediction.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 13309–13318, 2021. 2, 5, 6, 7, 8
[61] Julieta Martinez, Michael J. Black, and Javier Romero. On
human motion prediction using recurrent neural networks.
In2017 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26,
2017 , pages 4674–4683. IEEE Computer Society, 2017. 2, 6
[62] ´Angel Mart ´ınez-Gonz ´alez, Michael Villamizar, and Jean-
Marc Odobez. Pose transformers (POTR): human motion pre-
diction with non-autoregressive transformers. In IEEE/CVF
International Conference on Computer Vision Workshops, IC-
CVW 2021, Montreal, BC, Canada, October 11-17, 2021 ,
pages 2276–2284. IEEE, 2021. 2
[63] Omar Medjaouri and Kevin Desai. HR-STAN: high-
resolution spatio-temporal attention network for 3d human
motion prediction. In IEEE/CVF Conference on Computer
Vision and Pattern Recognition Workshops, CVPR Workshops
2022, New Orleans, LA, USA, June 19-20, 2022 , pages 2539–
2548. IEEE, 2022. 2
[64] Antoine Miech, Ivan Laptev, Josef Sivic, Heng Wang,
Lorenzo Torresani, and Du Tran. Leveraging the present
to anticipate the future in videos. In IEEE Conference on
Computer Vision and Pattern Recognition Workshops, CVPR
Workshops 2019, Long Beach, CA, USA, June 16-20, 2019 ,
pages 2915–2922. Computer Vision Foundation / IEEE, 2019.
3
[65] Dario Pavllo, David Grangier, and Michael Auli. Quaternet:
A quaternion-based recurrent model for human motion. In
19912
British Machine Vision Conference 2018, BMVC 2018, New-
castle, UK, September 3-6, 2018 , page 299. BMV A Press,
2018. 2
[66] Karl Pertsch, Oleh Rybkin, Jingyun Yang, Shenghao Zhou,
Konstantinos Derpanis, Kostas Daniilidis, Joseph Lim, and
Andrew Jaegle. Keyframing the future: Keyframe discovery
for visual prediction and planning. In Learning for Dynamics
and Control , pages 969–979. PMLR, 2020. 3
[67] Nicholas Rhinehart and Kris M Kitani. First-person activity
forecasting with online inverse reinforcement learning. In Pro-
ceedings of the IEEE International Conference on Computer
Vision , pages 3696–3705, 2017. 2
[68] Marcus Rohrbach, Anna Rohrbach, Michaela Regneri, Sikan-
dar Amin, Mykhaylo Andriluka, Manfred Pinkal, and Bernt
Schiele. Recognizing fine-grained and composite activities
using hand-centric features and script data. International
Journal of Computer Vision , pages 1–28, 2015. 2, 4, 5, 6, 7, 8
[69] Debaditya Roy and Basura Fernando. Action anticipation
using pairwise human-object interactions and transformers.
IEEE Trans. Image Process. , 30:8116–8129, 2021. 3
[70] Tim Salzmann, Marco Pavone, and Markus Ryll. Motron:
Multimodal probabilistic human motion forecasting. In
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24,
2022 , pages 6447–6456. IEEE, 2022. 2
[71] Fadime Sener and Angela Yao. Zero-shot anticipation for
instructional activities. In 2019 IEEE/CVF International
Conference on Computer Vision, ICCV 2019, Seoul, Korea
(South), October 27 - November 2, 2019 , pages 862–871.
IEEE, 2019. 3
[72] Fadime Sener, Dipika Singhania, and Angela Yao. Temporal
aggregate representations for long-range video understanding.
InComputer Vision - ECCV 2020 - 16th European Conference,
Glasgow, UK, August 23-28, 2020, Proceedings, Part XVI ,
pages 154–171. Springer, 2020. 3
[73] Theodoros Sofianos, Alessio Sampieri, Luca Franco, and
Fabio Galasso. Space-time-separable graph convolutional net-
work for pose forecasting. In 2021 IEEE/CVF International
Conference on Computer Vision, ICCV 2021, Montreal, QC,
Canada, October 10-17, 2021 , pages 11189–11198. IEEE,
2021. 2
[74] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In Proceedings of the 32nd
International Conference on Machine Learning, ICML 2015,
Lille, France, 6-11 July 2015 , pages 2256–2265. JMLR.org,
2015. 2
[75] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising
diffusion implicit models. In 9th International Conference on
Learning Representations, ICLR 2021, Virtual Event, Austria,
May 3-7, 2021 . OpenReview.net, 2021. 2
[76] Didac Suris, Ruoshi Liu, and Carl V ondrick. Learning the
predictability of the future. In IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2021, virtual, June 19-
25, 2021 , pages 12607–12617. Computer Vision Foundation /
IEEE, 2021. 3
[77] Omid Taheri, Nima Ghorbani, Michael J. Black, and Dim-
itrios Tzionas. Grab: A dataset of whole-body human grasp-ing of objects. In Computer Vision – ECCV 2020 , pages
581–600, Cham, 2020. Springer International Publishing. 2,
5
[78] Yongyi Tang, Lin Ma, Wei Liu, and Wei-Shi Zheng. Long-
term human motion prediction by modeling motion context
and enhancing motion dynamics. In Proceedings of the
Twenty-Seventh International Joint Conference on Artificial
Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Swe-
den, pages 935–941. ijcai.org, 2018. 2
[79] Julian Tanke, Linguang Zhang, Amy Zhao, Chengcheng Tang,
Yujun Cai, Lezi Wang, Po-Chen Wu, Juergen Gall, and Cem
Keskin. Social diffusion: Long-term multiple human motion
anticipation. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 9601–9611, 2023. 2
[80] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel
Cohen-Or, and Amit Haim Bermano. Human motion diffu-
sion model. In The Eleventh International Conference on
Learning Representations, ICLR 2023, Kigali, Rwanda, May
1-5, 2023 . OpenReview.net, 2023. 2
[81] Carl V ondrick, Hamed Pirsiavash, and Antonio Torralba. An-
ticipating visual representations from unlabeled video. In
2016 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2016, Las Vegas, NV , USA, June 27-30,
2016 , pages 98–106. IEEE Computer Society, 2016. 3
[82] Jacob Walker, Kenneth Marino, Abhinav Gupta, and Martial
Hebert. The pose knows: Video forecasting by generating
pose futures. In IEEE International Conference on Computer
Vision, ICCV 2017, Venice, Italy, October 22-29, 2017 , pages
3352–3361. IEEE Computer Society, 2017. 2
[83] Bastian Wandt and Bodo Rosenhahn. Repnet: Weakly super-
vised training of an adversarial reprojection network for 3d
human pose estimation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
7782–7791, 2019. 4, 5, 6
[84] Bastian Wandt, Hanno Ackermann, and Bodo Rosenhahn. 3d
reconstruction of human motion from monocular image se-
quences. IEEE transactions on pattern analysis and machine
intelligence , 38(8):1505–1516, 2016. 4
[85] Borui Wang, Ehsan Adeli, Hsu-Kuang Chiu, De-An Huang,
and Juan Carlos Niebles. Imitation learning for human pose
prediction. In 2019 IEEE/CVF International Conference on
Computer Vision, ICCV 2019, Seoul, Korea (South), October
27 - November 2, 2019 , pages 7123–7132. IEEE, 2019. 2
[86] Yu Wu, Linchao Zhu, Xiaohan Wang, Yi Yang, and Fei
Wu. Learning to anticipate egocentric actions by imagina-
tion. IEEE Transactions on Image Processing , 30:1143–1152,
2020. 3
[87] Chenxin Xu, Robby T Tan, Yuhong Tan, Siheng Chen,
Yu Guang Wang, Xinchao Wang, and Yanfeng Wang. Eqmo-
tion: Equivariant multi-agent motion prediction with invariant
interaction reasoning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
1410–1420, 2023. 5, 6
[88] Sirui Xu, Yu-Xiong Wang, and Liang-Yan Gui. Diverse hu-
man motion prediction guided by multi-level spatial-temporal
anchors. In Computer Vision - ECCV 2022 - 17th European
Conference, Tel Aviv, Israel, October 23-27, 2022, Proceed-
19913
ings, Part XXII , pages 251–269. Springer, 2022. 2, 5, 6, 7,
8
[89] Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, and Liang-Yan
Gui. Interdiff: Generating 3d human-object interactions with
physics-informed diffusion. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 14928–
14940, 2023. 2
[90] Xinchen Yan, Akash Rastogi, Ruben Villegas, Kalyan
Sunkavalli, Eli Shechtman, Sunil Hadap, Ersin Yumer, and
Honglak Lee. Mt-vae: Learning motion transformations to
generate multimodal human dynamics. In Proceedings of the
European Conference on Computer Vision (ECCV) , pages
265–281, 2018. 2
[91] Ye Yuan and Kris Kitani. Dlow: Diversifying latent flows for
diverse human motion prediction. In European Conference
on Computer Vision , pages 346–364. Springer, 2020. 2, 5, 6,
7, 8
[92] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong,
Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-
driven human motion generation with diffusion model. CoRR ,
abs/2208.15001, 2022. 2
[93] Chongyang Zhong, Lei Hu, Zihao Zhang, Yongjing Ye, and
Shihong Xia. Spatio-temporal gating-adjacency GCN for
human motion prediction. In IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, CVPR 2022, New Or-
leans, LA, USA, June 18-24, 2022 , pages 6437–6446. IEEE,
2022. 2
[94] Zixiang Zhou and Baoyuan Wang. UDE: A unified driving en-
gine for human motion generation. In IEEE/CVF Conference
on Computer Vision and Pattern Recognition, CVPR 2023,
Vancouver, BC, Canada, June 17-24, 2023 , pages 5632–5641.
IEEE, 2023. 2
[95] Yanjun Zhu, David Doermann, Yanxia Zhang, Qiong Liu, and
Andreas Girgensohn. What and how? jointly forecasting hu-
man action and pose. In 2020 25th International Conference
on Pattern Recognition (ICPR) , pages 771–778. IEEE, 2021.
3, 5, 6
19914
