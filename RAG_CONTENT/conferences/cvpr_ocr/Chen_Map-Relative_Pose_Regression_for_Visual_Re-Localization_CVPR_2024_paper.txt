Map-Relative Pose Regression for Visual Re-Localization
Shuai Chen1,2Tommaso Cavallari1Victor Adrian Prisacariu1,2Eric Brachmann1
1Niantic2University of Oxford
Abstract
Pose regression networks predict the camera pose of a
query image relative to a known environment. Within this
family of methods, absolute pose regression (APR) has re-
cently shown promising accuracy in the range of a few cen-
timeters in position error. APR networks encode the scene
geometry implicitly in their weights. To achieve high ac-
curacy, they require vast amounts of training data that, re-
alistically, can only be created using novel view synthesis
in a days-long process. This process has to be repeated
for each new scene again and again. We present a new
approach to pose regression, map-relative pose regression
(marepo ), that satisfies the data hunger of the pose re-
gression network in a scene-agnostic fashion. We condi-
tion the pose regressor on a scene-specific map representa-
tion such that its pose predictions are relative to the scene
map. This allows us to train the pose regressor across
hundreds of scenes to learn the generic relation between
a scene-specific map representation and the camera pose.
Our map-relative pose regressor can be applied to new
map representations immediately or after mere minutes of
fine-tuning for the highest accuracy. Our approach out-
performs previous pose regression methods by far on two
public datasets, indoor and outdoor. Code is available:
https://nianticlabs.github.io/marepo .
1. Introduction
Today, neural networks have conquered virtually all sectors
of computer vision, but there is still at least one task that
they struggle with: visual relocalization. What is visual
relocalization? Given a set of mapping images and their
poses, expressed in a common coordinate system, build a
scene representation. Later, given a query image, estimate
its pose, i.e. position and orientation, relative to the scene.
Successful approaches to visual relocalization rely
on predicting image-to-scene correspondences, either via
matching [8, 21, 38–40, 42, 57] or direct regression [4–
6, 14, 56], then solving for the pose using traditional and
robust algorithms like PnP [18] and RANSAC [17].
Adopting a different perspective, approaches based on
20xFigure 1. Camera pose estimation performance vs. mapping
time. The figure shows the median translation error of several
pose regression relocalization methods on the 7-Scenes dataset and
the time required (proportional to the bubble size) to train each
relocalizer on the target scenes. Our proposed approach, marepo ,
achieves superior performance – by far – on both metrics, thanks
to its integration of scene-specific geometric map priors within an
accurate, map-relative, pose regression framework.
pose regression [12, 25, 32, 46] attempt to perform visual
relocalization without resorting to traditional pose solving,
by using a single feed-forward neural network to infer poses
from single images. The mapping data is treated as a
training set where the camera extrinsics serve as supervi-
sion. Generally, pose regression approaches come in two
flavors, but they both struggle with accuracy compared to
correspondence-based methods.
Absolute pose regression (APR) methods [7, 24, 25] in-
volve training a dedicated pose regressor for each individual
scene, enabling the prediction of camera poses to that par-
ticular scene. Though the scene coordinate space can be im-
plicitly encoded in the weights of the neural networks, ab-
solute pose regressors exhibit low pose estimation accuracy,
primarily due to the often limited training data available for
each scene, and struggle to generalize to unseen views [43].
Relative pose regression is a second flavor of pose re-
gression methods [10, 16, 26, 51, 54]. The regressor is
trained to predict the relative pose between two images. In a
typical inference scenario, the regressor is applied to a pair
formed by an unseen query and an image from the mapping
set (typically selected via a nearest neighbor-type match-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
20665
ing); then, the predicted relative pose can be combined with
the known pose of the mapping image to yield the absolute
query pose. These methods can be trained on a lot of scene-
agnostic data, but their accuracy is still limited: a metric
pose between two images can only be predicted approxi-
mately [2].
Motivated by those limitations, we propose a new flavor
of absolute pose regression: map-relative pose regression
(marepo ). We couple a scene-specific representation – en-
coding the scale-metric reference space of each target scene
– with a general, scene-agnostic, absolute pose regression
network. In particular, we utilize a fast-training scene co-
ordinate regression model as our scene representation and
train, once and ahead of time, a pose regression network
that learns the relationship between a scene coordinate pre-
diction and the corresponding camera pose. This generic
relationship allows us to train the pose regressor on hun-
dreds of different scenes, effectively solving the issue of
the limited availability of training data afflicting absolute
pose regression models. On the other hand, since at local-
ization time our pose regressor is conditioned on a scene-
specific map representation, it is able to predict accurate
scale-metric poses, unlike relative pose regressors.
Our experiments show that marepo is a pose regression
network with an accuracy on par with structure-based re-
localization methods (e.g. [6]), exceeding the accuracy of
all other single-frame absolute pose regression methods by
far (see Fig. 1). Our scene-agnostic pose regressor can be
applied to each new scene representation right away, or (op-
tionally) fine-tuned in just a few minutes for best accuracy.
We summarize our main contributions as follows:
1. We propose marepo , a novel Absolute Pose Regression
approach that combines a generic and scene-agnostic
Map-Re lative Po se regression method with a scene-
specific metric representation. We show that the network
can perform end-to-end inference on previously unseen
images and, thanks to the strong and explicit 3D geomet-
ric knowledge encoded by the scene-specific component,
it can directly estimate accurate, absolute, metric poses.
2. We introduce a transformer-based network architecture
that can process a dense set of correspondences between
2D locations in a query image and their corresponding
3D coordinates within the reference system of a previ-
ously mapped scene, and estimate the pose of the cam-
era that captured the query image. We further show how
a dynamic position encoding applied to the 2D locations
in the query image can significantly improve the perfor-
mance of the method by encoding the intrinsic camera
parameters within the transformer input.
2. Related Works
Over the years many efforts in the literature have tackled
the problem of visual relocalization, and we have seen arough demarcation of the types of approach into two main
fields: the more traditional approaches, relying on geomet-
ric concepts and the estimation of correspondences between
images and maps; and the more recent “direct” approaches,
relying on neural networks to predict the absolute position
and orientation of the image without an intermediate, ex-
plicit, matching step linking the 2D image realm with a 3D
map of the scene. In the remainder of this section, we briefly
explore the main approaches in each of these categories.
2.1. Geometry-based Visual Relocalization
Geometry-based approaches rely on estimating correspon-
dences between pixels in the query images and points in the
scene’s map. These correspondences effectively establish a
2D-to-3D matching that can be exploited by pose-solving
methods such as PnP/RANSAC [17, 18] to compute the
pose of the camera at the moment it captures the image.
There are several ways to estimate those correspon-
dences: from classic computer vision approaches using
off-the-shelf feature detectors and descriptors to compute
matches between image pixels and a database of previ-
ously observed 3D points [8, 21, 41, 42]; to more advanced,
neural-based approaches that rely on learned descriptors,
improved matchers, and different map representations in
order to estimate better correspondences from more chal-
lenging images or viewpoints [29, 35, 38, 40]. These ap-
proaches leverage the underlying geometric principles gov-
erning image formation and capture, yielding accurate pose
estimations with low errors, often in the order of a few cen-
timeters. However, they are not without a drawback: they
generally require the creation of a map (e.g., in the form of
a 3D point cloud created via Structure-from-Motion) of the
scene ahead of time in order to associate the descriptors to
3D coordinates, and that is typically time-consuming.
In recent years, a new approach to geometry-based re-
localization started to become prominent: scene coordinate
regression (SCR). In this scenario, the map of the scene is
directly encoded in a fixed-size set of weights of a neural
network. At localization time, the query image is passed
through the network, yielding per-pixel scene coordinates
that can be directly used by a pose solver to estimate the
camera pose [3–6, 14, 27, 56]. While effective, these meth-
ods have typically required training a new network for ev-
ery new target scene, potentially taking several hours [4],
thus hindering their large scale application. Recently, an
approach to scene coordinate regression that can take mere
minutes to be trained for every scene was presented in [6],
making practical deployment of SCR networks a possibility.
As the correspondence-based methods mentioned above,
coordinate regression approaches are also very accurate by
relying on geometric information on the structure of the
scene. Nevertheless, scene coordinate regression methods
still require an explicit stage where a pose solver has to
20666
process each correspondence generated by the method to
estimate the camera pose. Conversely, Absolute Pose Re-
gression methods do not have this requirement since the re-
gressor network can go directly from image to pose in an
end-to-end fashion.
2.2. Absolute Pose Regression
Absolute Pose Regression (APR) approaches have also gar-
nered notable attention recently, primarily due to their sim-
plicity and efficiency. These methods directly predict cam-
era poses via end-to-end neural networks. Kendall et al.
introduced the first APR approach, named PoseNet [23–
25], where a feed-forward neural network directly regresses
a 7-dimensional pose vector for every query image. Suc-
cessive works explore diverse architectural designs such as
hourglass networks [30], bifurcated translation and rota-
tion regression [34, 55], attention layers [45–47, 53], and
LSTM layers [52]. Other research efforts attempt to im-
prove APR performance with different supervisions, such
as a geometric loss [24], relative pose constraints [7], uncer-
tainty awareness [24, 33], or a sequential formulation like
temporal filtering [13] and multitasking [37]. Despite these
advancements, the accuracy of single-frame-based pose re-
gression remains limited when compared to alternative ap-
proaches, such as those based on geometric principles.
Among recent advancements within APR, a promising
direction is incorporating novel view synthesis techniques,
either by synthesizing large amounts of training data to
solve overfitting issues [12, 32, 36] or by integrating them
into a fine-tuning process before test time [7, 11, 12]. One
drawback of the former is that generating high-quality syn-
thetic data can be a time-intensive process; as for the latter,
in addition to time requirements, those approaches typically
require extra data from the scene of interest. These limita-
tions pose significant constraints in environments subject to
rapid changes, such as those with frequent alterations in fur-
nishings or appearances.
This paper introduces a new category of approach to
the pose regression domain, one that tops the need for
extensive mapping time, reducing it to mere minutes per
scene. It demonstrates enhanced accuracy over previous
single-frame APR methods and exhibits rapid scalability to
new environments, making it flexible to deploy in a fast-
changing world as we live in today.
3. Method
Prevalent pose regression methods are built on top of end-
to-end neural network-based approaches. They can be for-
mulated as ˆP=F(I): the camera poses ˆPare directly pre-
dicted by providing an input image Ito the network F. A
benefit of this type of approach is its conceptual simplicity
and highly efficient inference speed. However, the forward
process of typical APR networks – which rely on 2D oper-ations over images and features – does not exploit any 3D
geometric reasoning, resulting in insufficient performance
compared to state-of-the-art geometry-based methods. In
this paper, we propose a first map-relative pose regression
approach empowered with explicit 3D geometric reasoning
within its formulation, allowing us to regress accurate cam-
era poses while maintaining real-time efficiency and end-
to-end simplicity like any other pose regression method.
In the remainder of this section we first give an overview
of the transformer-based network architecture we deploy
to perform pose regression (Sec. 3.1); then we describe
the main components and ideas behind the proposed ap-
proach (Sec. 3.2); the loss function optimized during train-
ing (Sec. 3.3); and, finally, we show how the scene-agnostic
pose-regression transformer can be optionally fine-tuned for
a specific testing scene in a matter of minutes, thus improv-
ing the performance of the method even further compared
to a non-fine-tuned regressor (Sec. 3.4).
3.1. Architecture Overview
The main architecture of our method is formed of two com-
ponents: (1) a CNN-based scene geometry prediction net-
workGthat maps pixels from the input image to 3D scene
coordinates; and (2) a transformer-based map-relative pose
regressor Mthat, given the scene coordinates, estimates the
camera poses. Ideally, the network Gis designed to asso-
ciate each input image to scene-specific 3D information,
thus requiring some training process for every new scene
processed by the method. Conversely, the map-relative pose
regressor Mis a scene-agnostic module trained with large
amounts of data and can generalize to unseen maps.
We illustrate our proposed network architecture in Fig. 2.
Given an image Ifrom scene S, we pass it to our model
which outputs a pose ˆP. The process is formulated as:
ˆP=M(ˆH, K ) =M(GS(I), K), (1)
where ˆH=GS(I)indicates the image-to-scene coordi-
nates predicted by G(which was trained ad-hoc for scene
S), and K∈R3×3is the camera intrinsic matrix associ-
ated with the input image. This formulation makes the ap-
proach similar to both standard Absolute Pose Regression,
in that it generates poses via a feed-forward pass through
a neural network, as well as Scene Coordinate Regression
since the scene geometry prediction network regresses 3D
coordinates directly from each input image. Unlike stan-
dard APR, our method has full geometric reasoning on the
link between the image and the scene, and, unlike SCR ap-
proaches, it does not require a traditional, non-deterministic
RANSAC stage to infer the pose. Theoretically, any algo-
rithm capable of predicting 3D scene coordinates from an
input image could be a viable candidate as G, since the fol-
lowing transformer we deploy to perform pose regression
(M) does not depend on the prior component.
20667
CNN Transformer....
Scene-Specific
Scene Geometry Prediction Network
Scene- Agnostic
Map-relative Pose Regressor Figure 2. Illustration of the marepo network. A scene-specific geometry prediction module GSprocesses a query image to predict a scene
coordinate map ˆH. Then, a scene-agnostic map-relative pose regressor Mis used to directly regress the camera pose. Our network’s
training and inference rely solely on RGB images Iand camera intrinsics Kwithout requiring depth information or pre-built point clouds.
Dynamic P .E.
(B,H,W ,128)
4x SA
Transformer
4x SA
Transformer
Regression Head
4x SA
Transformer
Re-Attention
Sec. 3.2.2Sec. 3.2.1
Regression Head
Regression HeadScene Coord.
(B,H,W ,3) (B,3,3)
only used in trainingtrain + inference
Sec. 3.3
Figure 3. The map-relative pose regressor Mtakes as input a
tensor of predicted scene coordinate maps and the corresponding
camera intrinsics, embeds the information with dynamic positional
encoding into higher dimensional features, and finally estimates
the camera poses ˆP. During training, we also predict ˆP0andˆP1
for intermediate supervision.
In the next section, we will focus on detailing the map-
relative pose regression network M.
3.2. Map-Relative Pose Regression Architecture
To achieve a robust and scene-agnostic map-relative pose
regression, we carefully design the simple yet effective ar-
chitecture depicted in Fig. 3. The main components of this
module are: (a) a novel dynamic positional encoding used
to increase the dimensionality of the input scene coordi-
nates – as well as embed their spatial location within the
input image – taking into account the intrinsic properties of
the camera that captured the frame; (b) several multi-layer
self-attention transformer blocks; and finally (c), an MLP-
based pose regression head. Given scene coordinate maps
ˆH(predicted by GS) and the corresponding camera intrinsicmatrices K, the network is able to directly estimate 6-DoF
(six degrees of freedom) metric camera poses. We detail the
designs of each component in the following sub-sections.
3.2.1 Dynamic Positional Encoding
Unlike many vision transformers (ViTs) for high-level
tasks [9, 15, 50], where the transformer is conditioned
to operate directly upon input RGB images (or higher-
dimensional features), our transformer is designed to inter-
pret accurate 3D geometric information strongly connected
to real-world physics. The content a camera captures in
its frame is strictly associated with its intrinsic parameters;
thus, we propose to use a positional encoding that is con-
ditioned on each individual sensor, allowing us to train the
main transformer blocks in a fashion that is generic, i.e.,
independent of the camera calibration parameters.
Our positional encoding scheme entails the fusion of two
different components: (1) a camera-aware 2D positional
embedding, associating each predicted scene coordinate to
its corresponding pixel location; and (2) a 3D positional em-
bedding that embeds the actual 3D scene coordinate values
into a high-frequency domain.
Camera-Aware 2D Positional Embedding We draw in-
spiration from LoFTR’s [50] positional embedding, but in-
tegrate information from the camera’s intrinsics to generate
the high-frequency components that are fed to the network.
Specifically, for each pixel coordinate (u, v)in the in-
put image, we first compute the (x, y)components of
the 3D ray originating in the camera center and passing
through the pixel (ignoring the zcomponent); then apply
the positional embedding from [50] on the (now camera-
invariant) ray’s directional components. This generates
a high-frequency/high-dimensionality embedding, allowing
the transformer to correlate the input 3D coordinates (pre-
dicted by GSand defined in a scene-specific coordinate sys-
tem) with 3D rays originating from the current camera posi-
tion, helping with the task of regressing the current camera
20668
pose w.r.t. the origin of the scene coordinate system. For-
mally, we define the Camera-Aware 2D Positional Embed-
ding as follows:
PEi
2D(u,v):=

sin (ωk·Xray(u)), i= 4k
cos (ωk·Xray(u)), i= 4k+ 1
sin (ωk·Yray(v)), i = 4k+ 2
cos (ωk·Yray(v)), i = 4k+ 3,(2)
where ωk=1
100002k/dis the frequency band defined for
d-dimensional features in which positional encoding is ap-
plied on, iis the current feature index, and XrayandYrayare
theXandYcomponents of the rays passing through (u, v):
Xray(u) =λu−cx−ε
fx,
Yray(v) =λv−cy−ε
fy,(3)
withfx/yandcx/ycorresponding to the intrinsics of the in-
put frame, ε= 0.5to achieve zero-mean in the center of the
image, and λ= 400 chosen as a heuristic constant to keep
a reasonable numerical magnitude for the final embedding.
3D Positional Embedding We use a 3D positional em-
bedding to map the scene coordinates p∈R3predicted by
GSto high frequency/dimensionality, inspired by [31]:
PE3D(p) = Convd
3(2m+1)[p,sin 
20πp
,cos 
20πp
,
. . . ,sin 
2m−1πp
,cos 
2m−1πp
].
(4)
Here, in addition to the sinusoidal embedding mapping
the 3D coordinates to a 3(2m+ 1) -dimensional space, we
also apply a further 1×1convolution Convd
6m+3to ensure
bothPE2DandPE3Dhave the same number of channels.
Fused Positional Embedding Finally, we fuse the 2D
and 3D embeddings before passing them to the transformer:
PEf=PE3D+PE2D. (5)
3.2.2 Re-Attention for Deep Transformer
As illustrated in the left part of Fig. 3, the core of our map-
relative pose regression architecture is formed by twelve
self-attention transformers arranged over three blocks of
four transformers each. In our implementation, we use Lin-
ear Transformers [22] as they reduce the computation com-
plexity of each layer from quadratic to linear in the length of
the input (i.e., the resolution of the scene coordinate map).
Since the Dynamic Positional Encoding is fed to the net-
work only at the beginning, we found that the informa-
tion flow became weaker as the depth of the network in-
creases. To solve this problem, we add what we call a“Re-Attention” mechanism, introducing residual connec-
tions every four blocks. Experimentally, we find that this
practice is quite effective, allowing the network to converge
more quickly and leading to a better generalization.
3.2.3 Pose Regression Head
The last component of the marepo architecture is a pose re-
gression head. Its structure is simple: first, a residual block
formed of three 1×1convolution layers followed by global
average pooling generates a single embedding that repre-
sents the whole input scene-coordinate map. Such embed-
ding is then passed to a small MLP (3 layers) that directly
outputs the camera pose as a 10-dimensional representation.
The pose representation can then be unpacked into transla-
tion and rotation: the translation is represented by four ho-
mogeneous coordinates (inspired by [6]); the rotation is en-
coded as a 6D vector representing two un-normalized axes
of the coordinate system that are later used to form a full ro-
tation matrix by normalization and cross-product, as in [58].
3.3. Loss Function
The map-relative pose regressor architecture described
above is able to directly output a metric pose ˆP(formed
of a3×3rotation matrix ˆR, and a translation vector ˆt) for
each image. In order to train such system we use a standard
L1 pose regression loss proposed in [2], defined as follows:
LˆP=∥ˆR−R∥1+∥ˆt−t∥1. (6)
Experimentally, we found that adding supervision at inter-
mediate layers of the regressor is beneficial to the overall
performance. Therefore, at training time, we additionally
apply the pose regression head after each block of four self-
attention transformers (see Fig. 3, right) and compute aux-
iliary losses LP0andLP1as described above. Thus, the
overall loss we optimize during training is as follows:
L=LˆP0+LˆP1+LˆP, (7)
During inference we only use the last output pose, ˆP.
3.4. (Optionally) Fine-Tuning the Pose Regressor
As described earlier, the proposed map-relative pose regres-
sor is formed by two main components: an initial scene-
specific network GSthat is able to predict metric scene co-
ordinates for each pixel (in our implementation, we use an
off-the-shelf scene coordinate regression architecture that
can be trained in a few minutes for each scene [6]); and a
scene-agnostic regressor Mthat exploits the geometric in-
formation encoded by the scene coordinates to predict the
camera pose. The latter is trained once – ahead of time –
over a large corpus of data, but it is reusable as-is for every
new target scene. We find that this hybrid approach works
20669
exceptionally well compared to other APR methods that are
trained with the traditional end-to-end image-to-pose proto-
col, over the course of hours or days.
Still, we also explore whether applying a scene-specific
adaptation stage to the transformer-based regressor can be
beneficial to the performance of the method. In this scheme,
for each new scene being evaluated, after training the scene-
specific coordinate regressor GS, we fine-tune the pose-
regressor Mon the same mapping images, using the same
loss as in Sec. 3.3. Fine-tuning the transformer is very effi-
cient in terms of resources required: in the next section we
show that, with only two passes over the training dataset for
each new scene (taking typically between 1-10 minutes, de-
pending on the number of frames), our method can further
improve its performance from what was already state-of-
the-art compared to pose regression-based methods.
Notably, the final fine-tuning step is completely optional
in our approach: the pre-trained Mis already capable of
predicting accurate camera poses, given 3D scene coordi-
nates predicted by the geometric network module GS.
4. Experiments
4.1. Implementation Details
In this section, we provide details of the process used to
train the marepo network. We first detail the generation of
training data, then describe the architectural configuration.
Training Data Generation In our implementation, we
employ the Accelerated Coordinate Encoding architecture
and training protocol proposed in [6] as Gto train the scene-
specific geometry prediction network GSfor each scene
Sin the training dataset, since that allows the training of
scene-specific coordinate-regression networks in a speedy
fashion ( ∼5 minutes for each new scene). To train M, we
use 450 scenes (indexing from 0 to 459, excluding 200-209)
from the Map-Free Dataset [2]. Each image in the dataset
has an associated ground truth camera pose computed by
the authors of the dataset via SfM [44]. The data includes
around 500K frames, with each scene containing images
scanning a small outdoor location. Frames from each scan
have been split into mapping and query, with ≈500map-
ping frames and ≈500queries. In practice, we train 900
scene-specific coordinate regressors GSusing frames from
each scene and its two splits. Given the efficient scaling ca-
pability of the method, we are able to generate a vast amount
of 2D-3D correspondences between image pixels and scene
coordinates by applying each GSto the frames of the unseen
split of its corresponding scan. We use data augmentation
during the generation of the correspondences, processing 16
variants of each frame. Specifically, we apply random im-
age rotations of up to 15°; rescale each frame to 0.67∼1.5
times its original resolution; and, finally, extract randomcrops. We save the scene coordinate maps output of the
preprocessing, together with their corresponding masks in-
dicating which pixels are valid after rotation, the augmented
intrinsics, and camera poses. This forms the fixed dataset
we use to train the map-relative pose regressor M.
Additionally, we perform online data augmentation by
randomly jittering the scene coordinates by ±1mand ro-
tating them by up to 180° to further increase data diversity.
Note that the random jittering is applied image-wise, i.e.,
all scene coordinates of an input frame are perturbed by the
same transform. We do this to avoid overfitting, ensuring
that the network Mdoes not learn an absolute pose for each
frame but rather a pose relative to the scene coordinates.
Network Configuration The scene-specific networks GS
process images having the shortest side 480 pixels long and
output dense scene coordinate maps with 8x smaller reso-
lution. The map-relative pose regressor Mis built upon
a cascade of linear-attention transformer blocks [50] with
dmodel = 256 andh= 8 parallel attention layers. For the
3D position embedding, we prudently choose m= 5 fre-
quency bands due to presence of potentially noisy input.
Training and Hardware Details We train Musing 8
NVIDIA V100 GPUs with a batch size of 64. We use the
AdamW [28] optimizer with a learning rate between 3e−4
to2e−3with a 1-cycle scheduler [49]. The model is trained
for≈10days, iterating through the dataset for 150 epochs.
During inference our entire model, including the scene-
specific network GSand the map-relative pose regressor M,
requires only one GPU, and can estimate camera poses with
a real-time throughput, as later shown in Tab. 2.
4.2. Quantitative Evaluation
In the following paragraphs we show the performance
ofmarepo on two public datasets: one depicting indoor
scenes, and one outdoor. We show that the proposed map-
relative pose regressor module Mcan generalize its pre-
dictions to previously unseen scenes, thanks to the scene-
specific geometry prediction network GSproviding it with
2D-3D correspondences in each scene’s metric space.
7-Scenes Dataset We first evaluate our method on the Mi-
crosoft 7-Scenes dataset [19, 48], an indoor relocalization
dataset that provides up to 7000 mapping images per scene.
Each scene covers a limited area (between 1m3and18m3);
despite that, previous APR methods require tens of hours or
even several days [32] to train a model to relocalize in them.
This is nonideal in a practical scenario as the appearance
of the scene might have changed within that time frame,
thus rendering the trained APR out of date. Conversely,
marepo requires only minutes of training time ( ≈5) for
each new scene to generate a geometry-prediction network
20670
Table 1. Re-localization results on the indoor 7-Scenes dataset. Pose errors are shown as median translation (cm) and rotation (°)
errors. Numbers in bold represent the best performance among the APR-based approaches. marepo denotes our model with generic
transformer-based pose regressor M.marepoSreports the performance of the model after Mhas been fine-tuned for each scene.
Methods Chess Fire Heads Office Pumpkin Kitchen Stairs Average Mapping Time
SCRDSAC* [4] 1.9/1.11 1.9/1.24 1.1/1.82 2.6/1.18 4.2/1.41 3.0/1.70 4.2/1.42 2.7/1.41 Hours
ACE [6] 1.9/0.7 1.9/0.9 0.9/0.6 2.7/0.8 4.2/1.1 4.2/1.3 3.9/1.1 2.8/ 0.93 5 Minutes
APRPoseNet(PN)[25] 32/8.12 47/14.4 29/12.0 48/7.68 47/8.42 59/8.64 47/13.8 44/10.4 Hours
PN Learn σ2[24] 14/4.50 27/11.8 18/12.1 20/5.77 25/4.82 24/5.52 37/10.6 24/7.87 Hours
geo. PN[24] 13/4.48 27/11.3 17/13.0 19/5.55 26/4.75 23/5.35 35/12.4 23/8.12 Hours
LSTM PN[52] 24/5.77 34/11.9 21/13.7 30/8.08 33/7.00 37/8.83 40/13.7 31/9.85 Hours
Hourglass PN[30] 15/6.17 27/10.8 19/11.6 21/8.48 25/7.01 27/10.2 29/12.5 23/9.53 Hours
BranchNet[55] 18/5.17 34/8.99 20/14.2 30/7.05 27/5.10 33/7.40 38/10.3 29/8.30 Hours
MapNet[7] 8/3.25 27/11.7 18/13.3 17/5.15 22/4.02 23/4.93 30/12.1 21/7.77 Hours
Direct-PN[11] 10/3.52 27/8.66 17/13.1 16/5.96 19/3.85 22/5.13 32/10.6 20/7.26 Days
MS-Transformer[47] 11/4.66 24/9.60 14/12.2 17/5.66 18/4.44 17/5.94 17/5.94 18/7.28 Hours
DFNet (VGG) [12] 5/1.88 17/6.45 6/3.63 8/2.48 10/2.78 22/5.45 16/3.29 12/3.71 Days
DFNet (EB0) [12] 3/1.15 9/3.71 8/6.08 7/2.14 10/2.76 9/2.87 11/5.58 8/3.47 Days
LENS [32] 3/1.3 10/3.7 7/5.8 7/1.9 8/2.2 9/2.2 14/3.6 8/3.00 Days
marepo (Ours) 2.6/1.35 2.5/1.42 2.3/2.21 3.6/1.44 4.2/1.55 5.1/1.99 6.7/1.83 3.9/1.68 5 Minutes
marepoS(Ours) 2.1/1.24 2.3/1.39 1.8/2.03 2.8/1.26 3.5/1.48 4.2/1.71 5.6/1.67 3.2/1.54 ≤15 Minutes
Table 2. Pose accuracy comparison on the outdoor Wayspots dataset. Results are reported as the percentage of frames below 10cm/5°
and0.5m/5° pose error. The map-relative pose regressors Mof our marepoSexperiment are fine-tuned in ≈1minute for each scene.
Scene DSAC* ACE PN MST marepo marepoS
[4] [6] [23–25] [47] Ours Ours
Throughput (fps) 17.9 17.9 166.7 28.4 55.6 55.6
Bears 82.6%/91.6% 80.7%/92.6% 12.9%/35.7% 0.5%/12.8% 80.7% /99.3% 80.7% /99.5%
Cubes 83.8%/98.1% 97.0%/98.1% 0.0%/0.4% 0.00%/9.9% 72.4% /96.9% 71.8%/ 96.9%
Inscription 54.1%/69.7% 49.0%/69.6% 1.1%/6.3% 1.3%/9.7% 37.8% /74.2% 37.1%/74.1%
Lawn 34.7%/38.0% 35.8%/38.5% 0.0%/0.2% 0.0%/0.0% 32.6%/ 41.6% 34.2% /41.1%
Map 56.7%/87.1% 56.5%/84.7% 14.9%/49.1% 5.6%/25.7% 53.9%/87.7% 55.1% /87.9%
Square Bench 69.5%/97.9% 66.7%/97.8% 0.0%/3.0% 0.0%/0.0% 68.6%/ 100% 70.7% /100%
Statue 0.0%/0.0% 0.0%/0.0% 0.0%/0.0% 0.0%/0.0% 0.0%/0.0% 0.0%/0.0%
Tendrils 25.1%/26.5% 34.9%/36.8% 0.0%/0.0% 0.9%/23.6% 27.9%/33.4% 29.3% /34.8%
The Rock 100%/100% 100%/100% 24.2%/77.5% 10.7%/52.6% 98.1%/ 100% 99.8% /100%
Winter Sign 0.2%/5.7% 1.0%/7.6% 0.0%/0.0% 0.0%/0.0% 0.0%/ 0.7% 0.0%/0.3%
Average 50.7%/61.5% 52.2%/62.6% 5.3%/17.2% 1.9%/13.4% 47.2%/63.4% 47.9% /63.5%
GSspecifically tuned for the target environment. We com-
pare our method with prior Pose Regression approaches
in Tab. 1, showing that marepo is not only a partly scene-
agnostic approach that enjoys the fastest mapping time of
all APR-based methods, but also obtains ≈50% better av-
erage performance (in terms of median error). We also
show the performance of a fine-tuned variant of our method,
marepoS, where, in addition to training the scene-specific
GSscene coordinate predictor, we also use the mapping
frames to run two epochs of fine-tuning on the Mregressor
(see Sec. 3.4). The fine-tuned model marepoSachieves fur-
ther improvements in average performance, requiring only
between 1.5∼10extra minutes of training time, resulting
in the only single-frame pose regression-based method able
to achieve a similar level of accuracy as one of the current
best 3D geometry-based methods, while being more effi-
cient in terms of computational resources required.Wayspots Dataset We further evaluate our method on the
Wayspots dataset [2, 6], which depicts challenging outdoor
scenes that even current geometry-based methods struggle
with. The dataset contains scans of 10 different areas with
associated ground truth poses provided by a visual-inertial
odometry system [1, 20]. In Tab. 2 we show a comparison
of the performance of the proposed marepo (as well as the
marepoSmodels, fine-tuned on the mapping frames of each
scene) with two APR-based approaches we reproduced; we
also include a comparison with two scene coordinate re-
gression approaches: DSAC* [4] and the current state-of-
the-art on Wayspots, ACE [6]. marepo significantly outper-
forms previous APR-based methods – such as PoseNet [25]
and MS-Transformers [47] – that require on average sev-
eral hours of training time, and compares favorably with
geometry-based methods. We show, for the first time, that
an end-to-end image-to-pose regression method relying on
20671
ModelAccuracy
5cm/5°10cm/5°
Full Architecture ( marepo ) 16.6% 39.6%
- Re-Attention 10.9% 28.3%
- Dynamic P.E. 3.9% 18.6%
Table 3. We gradually remove Re-Attention andDynamic Posi-
tional Encoding and report the %of frames relocalized within
5cm/5° and 10cm/5°.
# T Blocks dmodelAccuracy
5cm/5°10cm/5°
4 128 5.8% 22.2%
8 128 14.7% 39.1%
12 128 16.6% 39.6%
12 256 19.0% 43.5%
Table 4. Effect on performance of different dimensionality choices
in the pose regressor’s model. # T Blocks denotes the number of
transformer blocks used in the model. dmodel denotes the width of
the transformer layers.
geometric priors can achieve a similar level of performance
as methods that require the deployment of a (slower) ro-
bust solver to estimate the camera pose from a set of po-
tentially noisy 2D-3D correspondences. More specifically,
marepo requires only five minutes to train a network en-
coding the location of interest within the weights of the GS
scene-specific coordinate regressor and ( optionally ) approx-
imately one minute to fine-tune the map-relative regressor
M(as the Wayspot scans have significantly less frames then
the 7-Scenes scenes above). At inference time marepo (or
its fine-tuned variant) can perform inference at ≈56frames
per second, making it not only accurate, but also extremely
efficient in comparison to other methods.
4.3. Ablation Experiments
In the following, we provide additional insights into the de-
sign choices adopted whilst designing our method.
Architecture Ablation We run several controlled experi-
ments to justify our architectural design. Note that, for the
experiments in this subsection, we trained the transformer-
based pose regressor for 50 epochs instead of 150 as in the
main experiments. This allows us to complete each experi-
ment in approximately two days without affecting the rela-
tive ranking of results in the ablations. For the first exper-
iment, see Tab. 3, we train a smaller transformer M(with
dmodel = 128 ) and gradually remove the Re-Attention and
Dynamic Positional Encoding components to evaluate their
impact on the performance of the Wayspots dataset. The
pose accuracy is shown as the percentage of frames relocal-
ized within 5cm/5° and 10cm/5° error. The table shows
consistent degradation without the proposed components.
Next, we show the impact of diverse model configura-
tions by deploying different numbers of transformer blocks
anddmodel dimensions in Tab. 4. We experimented withModelAccuracy
10cm/5°50cm/5°
Per-scene marepo 0.7% 6.0%
Per-scene M 2.9% 18.7%
marepo (Ours) 47.2% 63.4%
Table 5. Effect of different training strategies. Per-scene marepo
trains the entire network from scratch for every new mapping
scene. In per-scene M, we train only the regressor from scratch,
on top of a pre-trained GS. Finally, marepo is trained over the en-
tire training dataset and can generalize well to unseen scenes.
training even larger models with dmodel = 512 and 16/20
transformer blocks, but found they necessitated substan-
tially more GPU resources and time. We therefore cannot
recommend them given the performance-time trade-offs.
Per-Scene Training The proposed pose regressor com-
ponent Mis designed to be scene-agnostic and has been
trained on a large corpus of data. Still, we are inter-
ested in evaluating its performance when trained ad-hoc
for individual scenes, similar to existing APR-based meth-
ods. We conduct two experiments where, instead of using
the full training set, we train scene-specific models using
mapping sequences from the Wayspots dataset, as shown
in Tab. 5. First, the models are trained from scratch, i.e.,
both the scene geometry regressor GSand the map-relative
pose regressor Mare trained as a single entity, similar
to PoseNet [25]. The results show extremely poor perfor-
mance, likely due to the model’s inability to learn explicit
3D geometry relations of the scene. For the second variant,
we assume a pre-trained GSis provided, then train Mfrom
scratch for each scene. We see results in a similar order
of magnitude as other APR methods, such as PoseNet [25]
or MST [47] (cf. Tab. 2); still, this training approach per-
formed quite poorly compared to the full marepo model,
where Mis trained on a large-scale dataset to predict truly
generic and scene-independent map-relative poses.
5. Conclusion
In conclusion, our paper introduces marepo , a novel ap-
proach in Pose Regression that combines the strengths of
a scene-agnostic pose regression network with a strong ge-
ometric prior provided by a fast-training scene-specific met-
ric representation. The method addresses the limitations
of previous APR techniques, offering both scalability and
precision in predicting accurate scale-metric poses across
diverse scenes. We demonstrate marepo ’s superior accu-
racy and its capability for rapid adaptation to new scenes
compared to existing APR methods on two datasets. Ad-
ditionally, we show how integrating the transformer-based
network architecture with dynamic positional encoding en-
sures robustness to varying camera parameters, establishing
marepo as a versatile and efficient solution for regression-
based visual relocalization.
20672
References
[1] Apple. ARKit. Accessed: 26 March 2024. 7
[2] Eduardo Arnold, Jamie Wynn, Sara Vicente, Guillermo
Garcia-Hernando, ´Aron Monszpart, Victor Adrian
Prisacariu, Daniyar Turmukhambetov, and Eric Brach-
mann. Map-free visual relocalization: Metric pose relative
to a single image. In ECCV , 2022. 2, 5, 6, 7
[3] Eric Brachmann and Carsten Rother. Neural- Guided
RANSAC: Learning where to sample model hypotheses. In
ICCV , 2019. 2
[4] Eric Brachmann and Carsten Rother. Visual camera re-
localization from RGB and RGB-D images using DSAC.
IEEE TPAMI , 2021. 1, 2, 7
[5] Eric Brachmann, Alexander Krull, Sebastian Nowozin,
Jamie Shotton, Frank Michel, Stefan Gumhold, and Carsten
Rother. DSAC - Differentiable RANSAC for Camera Local-
ization. In CVPR , 2017.
[6] Eric Brachmann, Tommaso Cavallari, and Victor Adrian
Prisacariu. Accelerated coordinate encoding: Learning to
relocalize in minutes using rgb and poses. In CVPR , 2023.
1, 2, 5, 6, 7
[7] S. Brahmbhatt, J. Gu, K. Kim, J. Hays, and J. Kautz.
Geometry-Aware Learning of Maps for Camera Localiza-
tion. In CVPR , 2018. 1, 3, 7
[8] Federico Camposeco, Andrea Cohen, Marc Pollefeys, and
Torsten Sattler. Hybrid scene compression for visual local-
ization. In CVPR , 2019. 1, 2
[9] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In ECCV , 2020. 4
[10] Kefan Chen, Noah Snavely, and Ameesh Makadia. Wide-
baseline relative camera pose estimation with directional
learning. In CVPR , 2021. 1
[11] Shuai Chen, Zirui Wang, and Victor Prisacariu. Direct-
PoseNet: Absolute pose regression with photometric consis-
tency. In 3DV, 2021. 3, 7
[12] Shuai Chen, Xinghui Li, Zirui Wang, and Victor Prisacariu.
DFNet: Enhance absolute pose regression with direct feature
matching. In ECCV , 2022. 1, 3, 7
[13] Ronald Clark, Sen Wang, Andrew Markham, Niki Trigoni,
and Hongkai Wen. Vidloc: A deep spatio-temporal model
for 6-dof video-clip relocalization. In CVPR , 2017. 3
[14] Siyan Dong, Shuzhe Wang, Yixin Zhuang, Juho Kannala,
Marc Pollefeys, and Baoquan Chen. Visual localization via
few-shot scene region classification. In 3DV, 2022. 1, 2
[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 4
[16] Sovann En, Alexis Lechervy, and Fr ´ed´eric Jurie. Rpnet: An
end-to-end network for relative camera pose estimation. In
ECCVW , 2018. 1
[17] Martin A. Fischler and Robert C. Bolles. Random sample
consensus: a paradigm for model fitting with applications toimage analysis and automated cartography. In CACM , 1981.
1, 2
[18] Xiao-Shan Gao, Xiao-Rong Hou, Jianliang Tang, and
Hang-Fei Cheng. Complete solution classification for the
perspective-three-point problem. IEEE TPAMI , 2003. 1, 2
[19] Ben Glocker, Shahram Izadi, Jamie Shotton, and Antonio
Criminisi. Real-time rgb-d camera relocalization. In ISMAR ,
2013. 6
[20] Google. ARCore. Accessed: 26 March 2024. 7
[21] Martin Humenberger, Yohann Cabon, Nicolas Guerin, Julien
Morat, J ´erˆome Revaud, Philippe Rerole, No ´e Pion, Cesar de
Souza, Vincent Leroy, and Gabriela Csurka. Robust image
retrieval-based visual localization using Kapture, 2020. 1, 2
[22] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and
Franc ¸ois Fleuret. Transformers are rnns: Fast autoregressive
transformers with linear attention. In ICML , 2020. 5
[23] A. Kendall and R. Cipolla. Modelling uncertainty in deep
learning for camera relocalization. In ICRA , 2016. 3, 7
[24] A. Kendall and R. Cipolla. Geometric loss functions for cam-
era pose regression with deep learning. In CVPR , 2017. 1, 3,
7
[25] A. Kendall, M. Grimes, and R. Cipolla. Posenet: A convolu-
tional network for real-time 6-dof camera relocalization. In
ICCV , 2015. 1, 3, 7, 8
[26] Jake Levinson, Carlos Esteves, Kefan Chen, Noah Snavely,
Angjoo Kanazawa, Afshin Rostamizadeh, and Ameesh
Makadia. An analysis of svd for deep rotation estimation.
NeurIPS , 2020. 1
[27] Xiaotian Li, Shuzhe Wang, Yi Zhao, Jakob Verbeek, and
Juho Kannala. Hierarchical scene coordinate classification
and regression for visual localization. In CVPR , pages
11983–11992, 2020. 2
[28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. ICLR , 2017. 6
[29] Simon Lynen, Bernhard Zeisl, Dror Aiger, Michael Bosse,
Joel Hesch, Marc Pollefeys, Roland Siegwart, and Torsten
Sattler. Large-scale, real-time visual-inertial localization re-
visited. International Journal of Robotics Research , 39(9),
2019. 2
[30] I. Melekhov, J. Ylioinas, J. Kannala, and E. Rahtu. Image-
based localization using hourglass networks. In ICCVW ,
2017. 3, 7
[31] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF
Representing scenes as neural radiance fields for view syn-
thesis. In ECCV , 2020. 5
[32] Arthur Moreau, Nathan Piasco, Dzmitry Tsishkou, Bogdan
Stanciulescu, and Arnaud de La Fortelle. LENS: Localiza-
tion enhanced by nerf synthesis. In CoRL , 2021. 1, 3, 6,
7
[33] Arthur Moreau, Nathan Piasco, Dzmitry Tsishkou, Bog-
dan Stanciulescu, and Arnaud de La Fortelle. Coordinet:
uncertainty-aware pose regressor for reliable vehicle local-
ization. In WACV , 2022. 3
[34] T. Naseer and W. Burgard. Deep regression for monocular
camera-based 6-dof global localization in outdoor environ-
ments. In IROS , 2017. 3
20673
[35] V ojtech Panek, Zuzana Kukelova, and Torsten Sattler.
Meshloc: Mesh-based visual localization. In ECCV , pages
589–609. Springer, 2022. 2
[36] Pulak Purkait, Cheng Zhao, and Christopher Zach. Synthetic
view generation for absolute pose regression and image syn-
thesis. In BMVC , 2018. 3
[37] N. Radwan, A. Valada, and W. Burgard. Vlocnet++:
Deep multitask learning for semantic visual localization and
odometry. In IEEE Robotics and Automation Letters , 2018.
3
[38] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and
Marcin Dymczyk. From coarse to fine: Robust hierarchical
localization at large scale. In CVPR , 2019. 1, 2
[39] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,
and Andrew Rabinovich. Superglue: Learning feature
matching with graph neural networks. In CVPR , 2020.
[40] Paul-Edouard Sarlin, Ajaykumar Unagar, M ˚ans Larsson,
Hugo Germain, Carl Toft, Viktor Larsson, Marc Pollefeys,
Vincent Lepetit, Lars Hammarstrand, Fredrik Kahl, and
Torsten Sattler. Back to the Feature: Learning robust camera
localization from pixels to pose. In CVPR , 2021. 1, 2
[41] T. Sattler, B. Leibe, and L. Kobbelt. Improving image-based
localization by active correspondence search. In ECCV ,
2012. 2
[42] T. Sattler, B. Leibe, and L. Kobbelt. Efficient & Effective
Prioritized Matching for Large-Scale Image-Based Localiza-
tion. In IEEE TPAMI , 2017. 1, 2
[43] T. Sattler, Q. Zhou, M. Pollefeys, and L. Leal-Taixe. Under-
standing the limitations of cnn-based absolute camera pose
regression. In CVPR , 2019. 1
[44] Johannes Lutz Sch ¨onberger and Jan-Michael Frahm.
Structure-from-motion revisited. In CVPR , 2016. 6
[45] Yoli Shavit and Yosi Keller. Camera pose auto-encoders for
improving pose regression. In ECCV , 2022. 3
[46] Yoli Shavit, Ron Ferens, and Yosi Keller. Paying attention to
activation maps in camera pose regression. In arXiv preprint
arXiv:2103.11477 , 2021. 1
[47] Yoli Shavit, Ron Ferens, and Yosi Keller. Learning multi-
scene absolute pose regression with transformers. In ICCV ,
2021. 3, 7, 8
[48] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram
Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene co-
ordinate regression forests for camera relocalization in rgb-d
images. In CVPR , 2013. 6
[49] Leslie N Smith and Nicholay Topin. Super-convergence:
Very fast training of neural networks using large learning
rates. In Artificial intelligence and machine learning for
multi-domain operations applications , 2019. 6
[50] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and
Xiaowei Zhou. LoFTR: Detector-free local feature matching
with transformers. CVPR , 2021. 4, 6
[51] Mehmet ¨Ozg¨ur T ¨urko ˘glu, Eric Brachmann, Konrad
Schindler, Gabriel Brostow, and ´Aron Monszpart. Visual
Camera Re-Localization Using Graph Neural Networks and
Relative Pose Supervision. In 3DV, 2021. 1
[52] F. Walch, C. Hazirbas, L. Leal-Taixe, T. Sattler, S. Hilsen-
beck, and D. Cremers. Image-based localization using lstms
for structured feature correlation. In ICCV , 2017. 3, 7[53] Bing Wang, Changhao Chen, Chris Xiaoxuan Lu, Peijun
Zhao, Niki Trigoni, and Andrew Markham. Atloc: Atten-
tion guided camera localization. In AAAI , 2020. 3
[54] Dominik Winkelbauer, Maximilian Denninger, and Rudolph
Triebel. Learning to localize in new environments from syn-
thetic training data. In ICRA , 2021. 1
[55] J. Wu, L. Ma, and X. Hu. Delving Deeper into Convolutional
Neural Networks for Camera Relocalization. In ICRA , 2017.
3, 7
[56] Luwei Yang, Ziqian Bai, Chengzhou Tang, Honghua Li, Ya-
sutaka Furukawa, and Ping Tan. SANet: Scene agnostic net-
work for camera localization. In ICCV , 2019. 1, 2
[57] Qunjie Zhou, S ´ergio Agostinho, Aljo ˇsa O ˇsep, and Laura
Leal-Taix ´e. Is geometry enough for matching in visual lo-
calization? In ECCV , 2022. 1
[58] Yi Zhou, Connelly Barnes, Lu Jingwan, Yang Jimei, and Li
Hao. On the continuity of rotation representations in neural
networks. In CVPR , 2019. 5
20674
