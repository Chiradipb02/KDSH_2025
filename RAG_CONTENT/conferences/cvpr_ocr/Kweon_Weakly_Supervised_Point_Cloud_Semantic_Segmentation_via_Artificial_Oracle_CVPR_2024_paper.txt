Weakly Supervised Point Cloud Semantic Segmentation via Artiﬁcial Oracle
Hyeokjun Kweon∗
KAIST
0327june@kaist.ac.krJihun Kim∗
KAIST
jihun1998@kaist.ac.krKuk-Jin Yoon
KAIST
kjyoon@kaist.ac.kr
Abstract
Manual annotation of every point in a point cloud is a
costly and labor-intensive process. While weakly super-
vised point cloud semantic segmentation (WSPCSS) with
sparse annotation shows promise, the limited information
from initial sparse labels can place an upper bound on per-
formance. As a new research direction for WSPCSS, we
propose a novel Region Exploration via Artiﬁcial Label-
ing (REAL) framework. It leverages a foundational im-
age model as an artiﬁcial oracle within the active learn-
ing context, eliminating the need for manual annotation
by a human oracle. To integrate the 2D model into the
3D domain, we ﬁrst introduce a Projection-based Point-to-
Segment (PP2S) module, designed to enable prompt seg-
mentation of 3D data without additional training. The
REAL framework samples query points based on model
predictions and requests annotations from PP2S, dynami-
cally reﬁning labels and improving model training. Fur-
thermore, to overcome several challenges of employing an
artiﬁcial model as an oracle, we formulate effective query
sampling and label updating strategies. Our comprehen-
sive experiments and comparisons demonstrate that the
REAL framework signiﬁcantly outperforms existing meth-
ods across various benchmarks. The code is available at
https://github.com/jihun1998/AO .
1. Introduction
Point cloud semantic segmentation has been extensively
researched for robotics or autonomous driving applica-
tions. While fully supervised approaches [ 7,10,14,28,29,
35] utilizing deep learning have demonstrated remarkable
progress, the densely annotated point-wise labels required
for these methods pose a signiﬁcant challenge due to the
labor-intensive and expensive annotation process. This ob-
stacle hinders the practical application of existing methods.
In response to this challenge, Weakly Supervised Point
Cloud Semantic Segmentation (WSPCSS) has gained atten-
tion, leveraging more affordable weak labels [ 15,23,24,26,
40,42,47,48]. Among various types of weak labels, sparse
annotations have become a widely adopted setting, where
only a tiny subset ( e.g., 0.02%) of the entire point cloud is
Figure 1. Visualization of the proposed REAL framework. We
tackle the challenge of WSPCSS, leveraging a foundational im-
agery model as an artiﬁcial oracle within the context of active
learning. The below examples illustrate the evolving labels as
training proceeds.
labeled. Existing WSPCSS works have proposed remedies
focusing on two key aspects: 1) fully utilizing the limited
information from sparse labels and 2) learning the features
beneﬁcial for segmentation from the data. The methods
based on self-labeling use the regions conﬁdently predicted
by the model as pseudo-labels for training the model itself.
Meanwhile, various studies have proposed self-supervised
learning methods, such as consistency against data augmen-
tation, contrastive learning, or masked modeling.
While these approaches have shown substantial results,
the overall amount of information provided is constrained
by the initial sparse annotation, potentially placing an up-
per limit on performance. In this paper, instead of solely
focusing on how to exploit the given limited information,
we embark on a new and more challenging research direc-
tion. Speciﬁcally, we propose a novel WSPCSS method
using an additional publicly available source of informa-
tion—leveraging advancements in the 2D domain with the
Segment Anything Model (SAM) [ 19].
We begin by utilizing the promptable segmentation ca-
pability of SAM to enhance initially provided sparse anno-
tations. To bridge the gap between the 2D model and 3D
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
3721
point cloud data, we introduce the Projection-based Point-
to-Segment (PP2S) module. This module comprises the
projection of weakly annotated points onto images, acquir-
ing 2D masks using SAM, and back-projecting these masks
into 3D, facilitating segmentation in the point cloud. The
proposed PP2S-based preprocessing effectively improves
label quality, resulting in signiﬁcantly enhanced semantic
segmentation performance.
However, the labels preprocessed by PP2S remain in-
complete due to potential noises from calibration errors or
SAM ambiguity. To further enhance label quality during
training, we propose the Region Exploration via Artiﬁcial
Labeling (REAL) framework, utilizing PP2S as an artiﬁ-
cial oracle within the context of active learning, as depicted
in Fig. 1. The REAL framework samples query points ac-
cording to model predictions and subsequently requests an-
notations from PP2S to dynamically enhance training la-
bels. Here, unlike general active learning methods relying
on a human oracle, our artiﬁcial oracle cannot directly as-
sign the classes of the requested query. Therefore, we de-
sign specialized strategies for query sampling and label re-
ﬁnement to unlock the segmentation capability of the PP2S.
Note that the main novelty of our approach lies in how
we effectively integrate the beneﬁts of SAM into the realm
of WSPCSS. We analyze the REAL framework through
extensive experiments, including quantitative and qualita-
tive ablation studies. Furthermore, our method achieves
new state-of-the-art (SoTA) results on S3DIS [ 3] and Scan-
NetV2 [ 8] under all the tested settings, surpassing the exist-
ing WSPCSS methods. Remarkably, even with the 0.004%
setting, our method outperforms conventional methods us-
ing the 0.02% setting, underscoring the superiority of the
proposed approach.
2. Related Work
2.1. Weakly Supervised Approaches
Originating from weakly supervised learning in imagery
domain [ 2,21,22,31,39,45], weakly supervised ap-
proach has emerged as a cost-effective approach to miti-
gate the challenges associated with acquiring fully labeled
data [ 15,18,23,24,26,40,42,47,48]. Xu and Lee [ 40]
ﬁrst propose weakly supervised conﬁgurations involving
annotations of 10% of the data and a single point label for
each category. PSD [ 48] introduces a perturbed branch and
constraint self-distillation loss between the perturbed and
original branches. OTOC [ 26] utilizes pseudo-label meth-
ods and iteratively updates pseudo-labels through a self-
training strategy. MIL [ 42] proposes the transformer model
derived from multiple instance learning and integrates adap-
tive global weighted pooling to their model. HybridCR [ 23]
introduces an architecture that leverages pseudo-label meth-
ods and consistency regularization between the original andaugmented branches. CPCM [ 24] introduces a region-wise
masking strategy and contextual masked training method to
integrate the beneﬁt of the masked autoencoder. Despite the
signiﬁcant progress in WSPCSS, in this paper, we would
like to explore a new and more challenging research direc-
tion. In this light, we suggest a novel WSPCSS method,
using a publicly available additional source of information.
2.2. Point Cloud Segmentation Using 2D Models
There have been extensive efforts to achieve point cloud
segmentation in an image-based manner as in [ 12,16,20,
30,43]. With the rapid advancement in the 2D domain,
multi-modal self-supervised approaches [ 1,6,13] and em-
ploying language-based model [ 9,27,34] have also been
particularly prevalent. Recently, there has been a surge
in attempts to address various point cloud tasks using the
Segment Anything Model (SAM) [ 19] The zero-shot ap-
proach [ 44,46], which directly applies SAM to the image
corresponding to the point cloud, has demonstrated promis-
ing segmentation results. However, SAM’s lack of explicit
semantics limits its applicability to high-level perception
tasks. Adaptation-based approaches [ 11] can make SAM
directly learn 3D tasks; however, they necessitate dense
ground truth data during the learning process, rendering
them unsuitable for WSPCSS. To tackle the weakly super-
vised setting, this paper presents a novel WSPCSS method
that effectively reﬁnes weak labels using the foundational
2D model as an artiﬁcial oracle.
2.3. Active Learning for Point Cloud Segmentation
Active Learning (AL) aims to identify a subset of instances
that an oracle manually annotates during training. As in
[17,32,36,41], AL has also been extensively explored for
WSPCSS. Given that the REAL framework actively em-
ploys SAM as an artiﬁcial oracle, it may share a conceptual
scheme with conventional AL-based methods to some ex-
tent. Nevertheless, note that our weakly-supervised setting
signiﬁcantly differs from them. Our framework solely relies
on initial sparse labels and does not involve any additional
manual annotation within the training loop.
3. Method
3.1. Preliminaries
Problem setting. We denote an input point cloud as a set of
Npoints:X={x1,...,x N}. In typical fully-supervised
point cloud semantic segmentation, the labels for all points
are provided as Y={y1,...,y N}whereyi∈{1,...,C}
andCis the number of classes. On the other hand, in
weakly-supervised point cloud semantic segmentation, only
a tiny subset of the points is labeled, and the other points
are unlabelled. We formulate this setting as (X,Y) =
(XL,YL)∪(XU,YU). Here,XL={xL
1,...,xL
n}is the
3722
points labeled with sparse annotations YL={yL
1,...,yL
n}.
Conversely, XU={xU
1,...,xU
N−n}is not labeled, and we
represent it as YU={yU
1,...,yU
N−n}, whereyU
i= 0.
Segment Anything Model. SAM is a foundational model
for promptable segmentation, comprising 1) an image en-
coder, 2) a prompt encoder, and 3) a decoder predicting the
mask using the embeddings from encoders. We denote the
inference process of SAM as
M2D=SAM(I;Z), (1)
whereIandZare an input image and prompt, respectively.
M2Ddenotes a set of the pixels of the predicted mask. In
this paper, we use a point prompt ( i.e., a pixel coordinate).
3.2. Projection­based Point­to­Segment
We begin by enhancing the initially given weak annotations
using SAM. However, SAM is a vision foundation model
designed and trained for 2D imagery. Therefore, to directly
process 3D point cloud data using SAM, it is necessary to
conduct additional adaptation or ﬁne-tuning.
Instead, we propose to utilize image data that captures
the same scene as the provided point cloud data. We be-
lieve that incorporating image data is not overly restrictive
in practice. In fact, point cloud data is usually created from
RGBD frames using Structure from Motion (SfM) [ 3,8] or
captured by LiDAR paired with RGBD cameras [ 4,5,33].
Hence, most point cloud datasets naturally contain RGB im-
ages, depth maps, and associated camera parameters. Under
this philosophy, we introduce a Projection-based Point-to-
Segment (PP2S) module, enabling promptable segmenta-
tion of 3D point cloud data using SAM via projection. It is
noteworthy that our method utilizes image data during the
training phase only and can perform semantic segmentation
using input point cloud data alone during testing.
Figure 2illustrates the visualization of PP2S. Formally,
for each point cloud X, we assume that the model can ac-
cess data from Jcameras: images, depth maps, and projec-
tion matrices. Initially, we project the set of weakly anno-
tated points XL={xL
i}onto the image Ijas:
pi,j=projjxL
iwherei∈{1,...,n},j∈{1,...,J}
(2)
where projjdenotes the projection matrix, and pi,jrepre-
sents the 2D coordinate of the ith weakly annotated points
on thejth image. After the projection, we discard points
that fall outside the image boundaries. We also ﬁlter out
occluded points using the depth map Dj, though alternative
visibility testing algorithms can be employed.
Then, SAM performs segmentation using the coordi-
nates of projected points as individual input prompts as
M2D
i,j=SAM(Ij;pi,j), (3)
whereM2D
i,jis a set of the pixels of predicted 2D mask on
Ij, given input point prompt pi,j. Sincepi,jcorresponds to
Figure 2. Visualization of the proposed Projection-based Point-
to-Segment (PP2S) module. The process begins by projecting the
weak labels onto images through projection matrices. Using these
projected points as point prompts, SAM predicts 2D masks, which
are subsequently back-projected onto the point cloud.
xL
iand is labeled as yL
i, the pixels in M2D
i,jare also assigned
toyL
i. As multiple cameras exist per a single point cloud,
we conduct the above process for all images in parallel.
Subsequently, we back-project the acquired 2D masks to
3D coordinates. Initially, similar to Equ. 2, we project the
point cloud onto the jth image as pk,j=projjxk, where
xkis thekth point in the point cloud and pk,jdenotes the
projection of xk. Ifpk,jis an element of M2D
i,j, it means
that the projection result of xkbelongs to the mask of the
weakly annotated point xL
i, within the image Ij. Therefore,
we deﬁne the 3D mask M3D
i,jas the set of xksuch that its
projection pk,jis inM2D
i,j.
The above process can be formulated as
M3D
i,j={xk|pk,j∈M2D
i,j}, (4)
whereM3D
i,jdenotes the set of points predicted by SAM to
be the same segment with xL
ifrom the perspective of jth
image. Accordingly, y3D
i,j, the class of M3D
i,j, isyL
i.
However, the 3D masks are not mutually exclusive.
Given that a single 3D point could be observed from multi-
ple images, it may be shared by more than one 3D mask. To
address this, we establish a consensus among the 3D masks
through a mask-wise voting system. In our system, each
3D mask M3D
i,jcasts a vote for its points being set to y3D
i,j.
Therefore, from the perspective of each point, its class is
determined by the voting of the 3D masks that include the
point. The above process can be formulated as
vk={y3D
i,j} ∀i,jsuch that xk∈M3D
i,j, (5)
wherevkis a multiset of the voted classes. Finally, we as-
sign the class with the highest number of votes among vkas
3723
Figure 3. Possible sources of noise in PP2S. (A): The 3D door
point is projected onto the chair in the image, resulting in an incor-
rect 2D mask. (B): Erroneous back-projection causes wall points
in 3D to be mislabeled as chair .(C) Above: The 2D mask of the
ceiling covers only a small portion of the entire object. (C) Below:
The 2D mask of the wall encroaches into the regions of column .
the artiﬁcial label akforkth pointxk:
ak=/braceleftBigg
0 ifvk=∅
Mode(vk)otherwise,(6)
where the Mode operator returns the most frequent element.
The proposed PP2S module effectively leverages the
segmentation capability of SAM to expand the sparsely an-
notated initial labels. These resulting artiﬁcial labels, de-
noted asA={ak}, can directly serve as a supervised sig-
nal for training. Notably, we have observed that training
withAyields signiﬁcant performance improvements com-
pared to using initial Y. The detailed experimental results
are demonstrated in Section 4.
3.3. Region Exploration via Artiﬁcial Labeling
As mentioned, the proposed PP2S effectively enhances the
initial sparse annotations, resulting in a substantial im-
provement in segmentation performance. However, we also
have recognized several limitations associated with this pre-
processing approach. The PP2S-enhanced labels can serve
as better guidance than the initial sparse labels. Neverthe-
less, they are still ﬁxed during the entire training phase and
thereby impose an upper bound on performance. Notably,
we cannot guide the regions not covered by the 2D masks
obtained within PP2S. Furthermore, even if we assume that
the initially provided sparse annotations are perfectly cor-
rect, errors stemming from factors such as noises of camera
projection matrices (Fig. 3A, B) or inherent ambiguity of
SAM (Fig. 3C) are inevitable. While the voting system
can mitigate some of these, it cannot completely eliminate
them.Algorithm 1 Region Exploring via Artiﬁcial Labeling
Require: Segmentation model fθ, Training dataset D=
{(X,Y)}, Learning rate η, Number of epochs T
Ensure: Optimized fθ
1:for(X,Y)inDdo
2:Y←PP2S(X;XL)
3:end for
4:Initialize the model parameter θ.
5:fort←1,2,...,T do
6: for(X,Y)inDdo
7:Yt−1←Y
8:S←softmax(fθ(X))
9:ˆY←argmax(S)
10:Q←QuerySampling (X,S,Yt−1)
11:A←PP2S(X;Q)
12:Yt←LabelUpdate (Yt−1,A,ˆY)
13:Y←Yt
14:θ←θ−η∇θL(S,Y)
15: end for
16:end for
Then, how can we further improve label quality, espe-
cially during training? We draw inspiration from the con-
cept of active learning. In active labeling, the model iden-
tiﬁes a set of instances, known as a query, during train-
ing and requests human annotators ( i.e., oracle) to label
these points. Typically, the query consists of instances that
confuse the model at a given timestep, making annotating
the query highly effective for the model to learn decision
boundaries. However, active learning requires manual an-
notation within the training loop, potentially making it even
more expensive than WSPCSS.
To address this challenge, we present the Region Ex-
ploration via Artiﬁcial Labeling (REAL) framework—a
novel WSPCSS method utilizing the PP2S module as an
artiﬁcial oracle , as depicted in Fig. 4. Speciﬁcally, the pro-
posed PP2S module serves as an oracle in the REAL frame-
work, responding to requests to annotate the query. Given
that the artiﬁcial oracle is, in fact, a pretrained network, the
REAL framework is completely relieved of the necessity for
manual annotation, except for the initial sparse labels.
However, unlike a human oracle in conventional meth-
ods, our artiﬁcial oracle lacks explicit semantic knowledge.
Therefore, we cannot expect the PP2S module to directly
label the classes for the confusing points. Instead, we focus
on leveraging what PP2S can provide to us. PP2S cannot
directly annotate the given points; however, it can precisely
predict which points should be grouped into a segment. To
fully utilize this attribute, we deﬁne the query Q={qi}to
consist of the conﬁdently identiﬁed points, instead of am-
biguous ones from the perspective of the model. Trusting
the prediction of the model on these points, we can self-
label the query with minimal risk. We represent this pro-
3724
Figure 4. Visualization of the proposed Region Exploration via Artiﬁcial Labeling (REAL) framework. The objective is to dynamically
enhance the quality of labels for weakly supervised point cloud semantic segmentation. In the REAL framework, the model fθtakes the
point cloud Xas input and predicts semantic segmentation logits S. We then sample the most conﬁdent points from the unlabeled regions
of the previous label Yt−1based on the predictions ˆYand reliability scores R. The sampled points then serve as prompts for the proposed
PP2S module, producing artiﬁcial labels A. Finally, by combining the model prediction and A, we reﬁne Yt−1into the updated labels Yt,
which serve as pseudo-ground truth for training the logits.
cess as QuerySampling in Algorithm 1, which will be thor-
oughly discussed in Section 3.4.
Subsequently, we request the PP2S module to act as an
artiﬁcial oracle, annotating the unknown regions using the
given query. For this, the self-labeled query serves as an
input prompt for the PP2S module as follows:
A=PP2S(X;Q), (7)
whereArepresents the artiﬁcial label obtained by PP2S
from the given query Q. This approach is akin to using a
set of sparsely annotated points ( XL) for PP2S in the pre-
processing phase, as described in Section 3.2. The primary
distinction in the REAL framework is that the input prompt
is self-labeled, not manually annotated.
With the obtained artiﬁcial label A, we update the label
of the previous step, Yt−1, intoYt. However, Amay not
be entirely reliable, as it could be more prone to noise com-
pared to labels provided by a human oracle. This potential
for noise could arise from inaccuracies in the self-labeled
input query or the issues highlighted in Fig. 3.
To mitigate the risk of incorporating errors during the
label-updating process, we devise a cautious approach us-
ing the prediction of the model. Speciﬁcally, if a discrep-
ancy exists between the class assigned by the artiﬁcial labelAand the model’s prediction ˆYfor a speciﬁc point, we ex-
clude that point from the updating process. This conserva-
tive update process can be formulated as:
yt
i=/braceleftBigg
ai ifai= ˆyi
yt−1
i otherwise.(8)
Finally, the model fθis optimized by minimizing the cross-
entropy loss between ˆYandYt.
3.4. Query Sampling Strategy for REAL
In the REAL framework, query sampling is a critical com-
ponent, as the quality of the artiﬁcial label depends on the
quality of the input query. We posit that an effective query
must meet two key criteria: 1) it should exhibit a high level
of conﬁdence, and 2) it should aid the model in acquiring
new knowledge. To address the ﬁrst criterion, we design
a margin-based reliability metric for evaluating the conﬁ-
dence of the model’s prediction for each point as follows:
ri=Max(si)−Max2(si), (9)
wheresi∈RCis the predicted logit of the ith pointxi,
andridenotes its reliability. Max and Max2 are opera-
tors that return the maximum and the second maximum el-
3725
Figure 5. Veriﬁcation of the reliability metric. The points present-
ing higher reliability show higher semantic segmentation accuracy.
ements of the input vector, respectively. We verify a ro-
bust correlation between the devised reliability metric and
the accuracy of model predictions, as illustrated in Fig. 5.
For instance, among points with reliability scores exceed-
ing 0.5, the model correctly predicts the class for 84.5% of
them. We set the threshold for query sampling at 0.95, cor-
responding to around 94% accuracy.
On the contrary, directly quantifying the utility of a
query for the second criterion is inherently elusive. In
this context, we propose an intuitive hypothesis: if the
model has successfully leveraged the labels from the pre-
vious timestep, Yt−1, the regions previously annotated in
Yt−1are expected to contribute less novel information dur-
ing the current timestep, t. Therefore, the focus of new
queries should be on effectively exploring the regions not
previously or incorrectly labeled in Yt−1.
To this end, we narrow the sampling range of the query
to the points that are predicted by the model differently from
the previous label. We formulate the above constraint as
Zc={xi} ∀isuch that ˆyi=candyt−1
i̸=c. (10)
Here,Zcis the mother set for sampling qc, the query point
of the class c. Considering the ﬁrst criterion, we sample qc
as the most reliable point among Zc, andQis the set of qc.
We formulate the above processes as
Q=QuerySampling (X,S,Yt−1). (11)
4. Experiments
4.1. Experimental settings
Datasets. We conduct experiments on S3DIS [ 3] and Scan-
NetV2 [ 8]. S3DIS comprises 6 areas with 272 rooms and
13 categories. We use the area 5 for evaluation following
previous studies. ScanNetV2 includes 1613 3D scenes with
20 categories. We follow the ofﬁcial split (1201 training,
312 validation, and 100 test scenes). In addition to the
point cloud data, the proposed REAL framework requires
the data from cameras ( i.e., images, depth maps, and cam-
era projection matrices) to leverage SAM. The datasets in-
herently include such data since they are reconstructed from
the sequence of RGBD frames. During training, we utilize
the data from 48 and 17 cameras per scene on average for
S3DIS and ScanNetV2, respectively. Note that our frame-
work does not require 2D data at inference time.Table 1. A quantitative comparison between the initially sparse
annotations ( Initial ) and the artiﬁcial labels from PP2S ( PP2S ).
Pre, Rec, and # denote precision, recall, and the proportion of an-
notated points relative to the total number of points. The perfor-
mance of the point cloud semantic segmentation model trained by
each model is represented as Model mIoU . Every metric is in %.
Labels mIoU Pre Rec # Model mIoU
Initial 0.004 100 0.004 0.004 39.7
PP2S 30.4 83.1 32.4 38.4 53.0
Figure 6. Validation of our query sampling strategy. Setting I
samples the queries following our strategy while setting IIsamples
from the entire point set. We present the improvement obtained by
artiﬁcial labels in terms of precision ( Prec ) and recall ( Rec).
Evaluation protocols. Previous studies [ 23,24,42,48]
have conducted experiments with various levels of sparsity.
We believe that the most cost-efﬁcient yet practical setting
involves labeling only one point per object. Accordingly,
our experiments mainly target the 1pt weak setting, which
corresponds to 0.004% of points in S3DIS. For a fair com-
parison, we also conduct experiments under the 0.02% set-
ting, which is the most widely used. For ScanNetV2, we
follow the conventional works, using 20pt per scene. The
overall performance is assessed on all points within the test
set. Following the standard practice, we employ the mean
Intersection over Union (mIoU) as our quantitative metric.
Implementation details. We choose PTv2 [ 37] network as
our main backbone. To verify the effect of the backbone,
we additionally test Closer [ 25] backbone for the 0.004%
setting on S3DIS. Training details are the same as standard
settings of the backbones. We employ a pretrained ViT-H
model for SAM. Further details are in the Supp.
4.2. Analysis on REAL
PP2S-based preprocessing. We ﬁrst validate the prepro-
cessing strategy built upon the proposed PP2S. This strategy
aims to enhance the initial sparse annotations into artiﬁcial
labels by harnessing the promptable segmentation capabil-
ity of SAM via projection. Table 1provides various met-
3726
Figure 7. Progressive visualization of the behavior of the proposed REAL framework throughout training. Ais the artiﬁcial label, and Y
denotes the subsequent label updated at timestep t. The highlighted boxes indicate newly explored regions by artiﬁcial oracle, which were
unknown (red, blue) or wrongly labeled (green) in earlier timesteps.
rics of the initial labels and those of the labels enhanced
by PP2S. Furthermore, we compare the performance of the
point cloud semantic segmentation model when trained with
the original sparse annotations and artiﬁcial labels enhanced
by PP2S. The results demonstrate that our PP2S signiﬁ-
cantly improves the quality of the label, consequently boost-
ing the model’s performance. This analysis supports the
validity of PP2S as a bridge for transferring the segmenta-
tion capability of SAM into the realm of weakly supervised
point cloud semantic segmentation.
Query sampling strategy. We conduct ablation studies re-
garding the proposed query sampling strategy introduced in
Section 3.4. In this section, we compare two settings: (I)
using the proposed strategy sampling from the narrowed
mother set of Equ. 10and (II) sampling the most reliable
point among every point without any constraint. Figure 6
provides both qualitative and quantitative comparisons be-
tween (I) and (II), displaying the improvement achieved by
the artiﬁcial labels of each setting. (I) outperforms (II) in
terms of recall while maintaining precision. This distinction
primarily arises from the fact that most query points of (II)
are already labeled in Yt−1, limiting their potential for ex-
ploring into unknown regions. On the other hand, the query
of (I) is explicitly restricted to represent genuinely novel in-
formation compared with previous labels Yt−1, even with-
out compromising precision. This implies that the proposed
query sampling strategy yields better labels, facilitating the
model to learn segmentation effectively. Accordingly, the
performance with our query sampling strategy ( 62.7% ) is
signiﬁcantly higher than without using it ( 59.8% ). Addi-
tional results are in Supp .Active label enhancement. To demonstrate the behavior
of the proposed REAL framework intuitively, Fig. 7depicts
progressive visualization throughout training. The labels
are signiﬁcantly improved as training proceeds, thanks to
the artiﬁcial labels from our artiﬁcial oracle. Notably, the
highlighted boxes indicate newly explored regions, which
were unknown (red, blue) or wrongly labeled (green) by
the labels in earlier timesteps. The REAL framework effec-
tively integrates these regions into the current label.
4.3. Comparison with State­of­the­arts
We compare our method with SoTA methods on S3DIS, as
shown in Table 2. The proposed REAL signiﬁcantly en-
hances performance across all settings. Speciﬁcally, REAL
with PTv2 [ 37] outperforms CPCM by 2.9% in mIoU under
the 0.02% setting, which is even comparable to the SoTA
results of the 1% setting. Furthermore, REAL with PTv2
surpasses the other SoTA methods under the 0.02% setting,
using only 1pt for each object (0.004%). We also observe a
signiﬁcant performance improvement when employing the
REAL in Closer [ 25]. These results imply that the gain of
REAL is not restricted by the backbone.
Besides, Fig. 8provides a qualitative comparison. No-
tably, the semantic confusion in the baseline is remark-
ably improved in ours. Furthermore, REAL demonstrates
a meaningful enhancement in its ability to group the points
of each object precisely. Consequently, REAL achieves
remarkable semantic segmentation results even in compli-
cated scenes, using only 0.004% of annotations.
Furthermore, we evaluate the performance of our method
against SoTA methods on ScanNetV2 [ 8], as detailed in Ta-
3727
Table 2. Comparisons of the proposed methods with SoTA methods on S3DIS test set. Every metric is in mIoU (%). Bold number
represents the best result.†denotes the results from our reimplementation.
Settings Methods mIoU ceil. ﬂoor wall beam col. wind. door chair table book. sofa board clutter
FullyRandLA [ 14] 62.4 91.2 95.7 80.1 0.0 25.2 62.3 47.4 75.8 83.2 60.8 70.8 65.2 54.0
KPConv [ 35] 67.1 92.8 97.3 82.4 0.0 23.9 58.0 69.0 91.0 81.5 75.3 75.4 66.7 58.9
RFCR [ 10] 68.7 94.2 98.3 84.3 0.0 28.5 62.4 71.2 92.0 82.6 76.1 71.1 71.6 61.3
Closer†[25] 66.0 93.2 98.2 81.2 0.0 24.8 49.9 69.3 90.4 80.4 72.0 74.6 68.0 55.8
PTv2†[38] 70.9 93.8 98.5 86.1 0.0 29.5 60.5 78.0 93.2 81.8 76.6 79.2 83.0 61.7
1%SPT [ 47] 61.8 91.5 96.9 80.6 0.0 18.2 58.1 47.2 75.8 85.7 65.3 68.9 65.0 50.2
PSD [ 48] 63.5 92.3 97.7 80.7 0.0 27.8 56.2 62.5 78.7 84.1 63.1 70.4 58.9 53.2
HybridCR [ 23] 65.3 92.5 93.9 82.6 0.0 24.2 64.4 63.2 78.3 81.7 69.0 74.4 68.2 56.5
0.03%PSD [ 48] 48.2 87.9 96.0 62.1 0.0 20.6 49.3 40.9 55.1 61.9 43.9 50.7 27.3 31.1
HybridCR [ 23] 51.5 85.4 91.9 65.9 0.0 18.0 51.4 34.2 63.8 78.3 52.4 59.6 29.9 39.0
0.02%MIL [ 42] 51.4 86.6 93.2 75.0 0.0 29.3 45.3 46.7 60.5 62.3 56.5 47.5 33.7 32.2
CPCM [ 24] 62.3 92.6 95.6 79.4 0.0 17.8 49.3 59.4 85.7 75.6 69.1 60.7 68.2 55.8
PTv2†[38] 55.1 80.5 94.7 73.9 0.0 21.7 42.9 38.9 81.4 59.2 62.5 52.9 66.1 42.1
+PP2S 60.0 79.0 93.1 77.5 0.1 31.5 49.7 61.4 70.0 73.7 69.9 56.9 71.0 45.6
+REAL 65.2 86.9 95.8 80.2 0.0 27.0 60.3 77.8 72.3 79.9 70.8 64.7 77.4 54.1
0.004%Closer†[25] 38.7 82.5 92.4 69.4 0.1 15.0 23.3 35.6 50.9 47.8 0.3 19.1 37.3 29.1
+PP2S 44.9 62.9 88.8 67.9 0.0 11.7 29.2 44.7 46.4 63.2 53.5 25.7 50.7 38.6
+REAL 57.8 79.0 97.7 70.8 0.1 24.4 50.4 60.1 84.5 64.6 64.8 66.8 39.9 48.7
PTv2†[38] 39.7 76.6 87.2 65.4 0.0 9.4 30.6 22.3 61.7 50.3 41.7 14.6 22.4 34.1
+PP2S 53.0 65.9 93.1 68.9 0.0 27.3 42.7 55.8 68.4 63.7 58.2 52.5 50.5 41.8
+REAL 62.7 84.0 95.7 80.3 0.0 31.1 57.4 63.7 75.5 76.9 68.7 70.0 65.4 46.8
Figure 8. Qualitative comparisons between the semantic segmen-
tation results of the baseline and ours on S3DIS under 0.004% set-
ting. The displayed boxes indicate the regions where signiﬁcant
improvements are achieved by the proposed method.
ble3. Our method demonstrates superior performance on
both the validation and test splits. Notably, REAL outper-
forms CPCM by 3.6% in the validation set and 2.7% in the
test set. These results imply that our method effectively uti-
lizes the potential of weak labels. The qualitative results for
ScanNetV2 can be found in Supp .
5. Conclusion
Expensive and labor-intensive point-wise annotation hin-
ders the practical application of point cloud semantic seg-
mentation. While conventional WSPCSS methods have ex-
hibited promising results, the ﬁxed input sparse label po-
tentially constrains the achievable performance. In pur-
suit of a novel research direction, we propose the utiliza-
tion of an additional source of information, introducing the
Region Exploration via Artiﬁcial Labeling (REAL) frame-
work. This framework aims to harness the power of the im-
age foundational model, SAM, within the context of active
learning. To establish the connection between the 2D modelTable 3. Comparisons of the proposed methods with SoTA meth-
ods on ScanNetV2 val/test set. The results are in mIoU.
Settings Methods Val. Test
FullyKPConv [ 35] - 68.4
RFCR [ 10] - 70.2
1%SPT [ 47] - 51.1
PSD [ 48] - 54.7
HybridCR [ 23]56.9 56.8
20 ptsMIL [ 42]57.8 54.4
CPCM [ 24]62.7 62.8
Ours (REAL) 66.3 65.5
and 3D data, we designed the Projection-based Point-to-
Segment (PP2S) module. The PP2S is employed as an artiﬁ-
cial oracle in the REAL framework, dynamically enhancing
labels during training. Although using an artiﬁcial oracle
eliminates the burden of manual annotation, it introduces
some drawbacks due to the model’s imperfections. As a
remedy, we devised several strategies for query sampling
and label updating. Through extensive experiments, we
demonstrated the working logic of our framework in detail.
Finally, the proposed REAL framework achieved new SoTA
on various datasets, surpassing existing works. We believe
that our approach pioneers the effective integration of the
foundational image model into the realm of WSPCSS.
Acknowledgements This research was supported
by National Research Foundation of Korea (NRF)
grant funded by the Korea government (MSIT)
(NRF2022R1A2B5B03002636) and the Challengeable
Future Defense Technology Research and Development
Program through the Agency For Defense Development
(ADD) funded by the Defense Acquisition Program
Administration (DAPA) in 2024 (No.912768601).
3728
References
[1] Mohamed Afham, Isuru Dissanayake, Dinithi Dissanayake,
Amaya Dharmasiri, Kanchana Thilakarathna, and Ranga Ro-
drigo. Crosspoint: Self-supervised cross-modal contrastive
learning for 3d point cloud understanding. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 9902–9912, 2022. 2
[2] Jiwoon Ahn and Suha Kwak. Learning pixel-level semantic
afﬁnity with image-level supervision for weakly supervised
semantic segmentation. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
4981–4990, 2018. 2
[3] Iro Armeni, Sasha Sax, Amir R Zamir, and Silvio Savarese.
Joint 2d-3d-semantic data for indoor scene understanding.
arXiv preprint arXiv:1702.01105 , 2017. 2,3,6
[4] Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-
zel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Se-
mantickitti: A dataset for semantic scene understanding of
lidar sequences. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 9297–9307,
2019. 3
[5] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-
ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-
modal dataset for autonomous driving. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 11621–11631, 2020. 3
[6] Anthony Chen, Kevin Zhang, Renrui Zhang, Zihan Wang,
Yuheng Lu, Yandong Guo, and Shanghang Zhang. Pimae:
Point cloud and image interactive masked autoencoders for
3d object detection. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
5291–5301, 2023. 2
[7] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d
spatio-temporal convnets: Minkowski convolutional neural
networks. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 3075–3084,
2019. 1
[8] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nießner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 5828–5839, 2017. 2,3,6,7
[9] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang,
Song Bai, and Xiaojuan Qi. Pla: Language-driven open-
vocabulary 3d scene understanding. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 7010–7019, 2023. 2
[10] Jingyu Gong, Jiachen Xu, Xin Tan, Haichuan Song, Yanyun
Qu, Yuan Xie, and Lizhuang Ma. Omni-supervised point
cloud segmentation via gradual receptive ﬁeld component
reasoning. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 11673–
11682, 2021. 1,8
[11] Shizhan Gong, Yuan Zhong, Wenao Ma, Jinpeng Li, Zhao
Wang, Jingyang Zhang, Pheng-Ann Heng, and Qi Dou.
3dsam-adapter: Holistic adaptation of sam from 2d to 3dfor promptable medical image segmentation. arXiv preprint
arXiv:2306.13465 , 2023. 2
[12] Ji Hou, Saining Xie, Benjamin Graham, Angela Dai, and
Matthias Nießner. Pri3d: Can 3d priors help 2d represen-
tation learning? In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 5693–5702,
2021. 2
[13] Ji Hou, Xiaoliang Dai, Zijian He, Angela Dai, and Matthias
Nießner. Mask3d: Pre-training 2d vision transformers by
learning masked 3d priors. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 13510–13519, 2023. 2
[14] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan
Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham.
Randla-net: Efﬁcient semantic segmentation of large-scale
point clouds. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 11108–
11117, 2020. 1,8
[15] Qingyong Hu, Bo Yang, Guangchi Fang, Yulan Guo, Ale ˇs
Leonardis, Niki Trigoni, and Andrew Markham. Sqn:
Weakly-supervised semantic segmentation of large-scale 3d
point clouds. In European Conference on Computer Vision ,
pages 600–619. Springer, 2022. 1,2
[16] Wenbo Hu, Hengshuang Zhao, Li Jiang, Jiaya Jia, and
Tien-Tsin Wong. Bidirectional projection network for cross
dimension scene understanding. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 14373–14382, 2021. 2
[17] Zeyu Hu, Xuyang Bai, Runze Zhang, Xin Wang, Guangyuan
Sun, Hongbo Fu, and Chiew-Lan Tai. Lidal: Inter-frame un-
certainty based active learning for 3d lidar semantic segmen-
tation. In European Conference on Computer Vision , pages
248–265. Springer, 2022. 2
[18] Jihun Kim, Hyeokjun Kwon, Yunseo Yang, and Kuk-Jin
Yoon. Learning point cloud completion without complete
point clouds: A pose-aware approach. In 2023 IEEE/CVF
International Conference on Computer Vision (ICCV) , pages
14157–14167. IEEE, 2023. 2
[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. arXiv preprint arXiv:2304.02643 , 2023. 1,2
[20] Hyeokjun Kweon and Kuk-Jin Yoon. Joint learning of 2d-3d
weakly supervised semantic segmentation. Advances in Neu-
ral Information Processing Systems , 35:30499–30511, 2022.
2
[21] Hyeokjun Kweon, Sung-Hoon Yoon, Hyeonseong Kim,
Daehee Park, and Kuk-Jin Yoon. Unlocking the poten-
tial of ordinary classiﬁer: Class-speciﬁc adversarial erasing
framework for weakly supervised semantic segmentation. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 6994–7003, 2021. 2
[22] Hyeokjun Kweon, Sung-Hoon Yoon, and Kuk-Jin Yoon.
Weakly supervised semantic segmentation via adversarial
learning of classiﬁer and reconstructor. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 11329–11339, 2023. 2
3729
[23] Mengtian Li, Yuan Xie, Yunhang Shen, Bo Ke, Ruizhi
Qiao, Bo Ren, Shaohui Lin, and Lizhuang Ma. Hybridcr:
Weakly-supervised 3d point cloud semantic segmentation
via hybrid contrastive regularization. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 14930–14939, 2022. 1,2,6,8
[24] Lizhao Liu, Zhuangwei Zhuang, Shangxin Huang, Xun-
long Xiao, Tianhang Xiang, Cen Chen, Jingdong Wang, and
Mingkui Tan. Cpcm: Contextual point cloud modeling for
weakly-supervised point cloud semantic segmentation. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 18413–18422, 2023. 1,2,6,8
[25] Ze Liu, Han Hu, Yue Cao, Zheng Zhang, and Xin Tong.
A closer look at local aggregation operators in point cloud
analysis. In Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceed-
ings, Part XXIII 16 , pages 326–342. Springer, 2020. 6,7,
8
[26] Zhengzhe Liu, Xiaojuan Qi, and Chi-Wing Fu. One thing
one click: A self-training approach for weakly supervised
3d semantic segmentation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 1726–1736, 2021. 1,2
[27] Yuheng Lu, Chenfeng Xu, Xiaobao Wei, Xiaodong Xie,
Masayoshi Tomizuka, Kurt Keutzer, and Shanghang Zhang.
Open-vocabulary point-cloud object detection without 3d
annotation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 1190–
1199, 2023. 2
[28] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classiﬁcation
and segmentation. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 652–660,
2017. 1
[29] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. Advances in neural information
processing systems , 30, 2017. 1
[30] Damien Robert, Bruno Vallet, and Loic Landrieu. Learn-
ing multi-view aggregation in the wild for large-scale 3d se-
mantic segmentation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
5575–5584, 2022. 2
[31] Shenghai Rong, Bohai Tu, Zilei Wang, and Junjie Li.
Boundary-enhanced co-training for weakly supervised se-
mantic segmentation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
19574–19584, 2023. 2
[32] Feifei Shao, Yawei Luo, Ping Liu, Jie Chen, Yi Yang, Yulei
Lu, and Jun Xiao. Active learning for point cloud semantic
segmentation via spatial-structural diversity reasoning. In
Proceedings of the 30th ACM International Conference on
Multimedia , pages 2575–2585, 2022. 2
[33] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien
Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,
Yuning Chai, Benjamin Caine, et al. Scalability in perception
for autonomous driving: Waymo open dataset. In Proceed-ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 2446–2454, 2020. 3
[34] Ayc ¸a Takmaz, Elisabetta Fedele, Robert W Sumner, Marc
Pollefeys, Federico Tombari, and Francis Engelmann. Open-
mask3d: Open-vocabulary 3d instance segmentation. arXiv
preprint arXiv:2306.13631 , 2023. 2
[35] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud,
Beatriz Marcotegui, Franc ¸ois Goulette, and Leonidas J
Guibas. Kpconv: Flexible and deformable convolution for
point clouds. In Proceedings of the IEEE/CVF international
conference on computer vision , pages 6411–6420, 2019. 1,
8
[36] Tsung-Han Wu, Yueh-Cheng Liu, Yu-Kai Huang, Hsin-Ying
Lee, Hung-Ting Su, Ping-Chia Huang, and Winston H Hsu.
Redal: Region-based and diversity-aware active learning
for point cloud semantic segmentation. In Proceedings of
the IEEE/CVF international conference on computer vision ,
pages 15510–15519, 2021. 2
[37] Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Heng-
shuang Zhao. Point transformer v2: Grouped vector atten-
tion and partition-based pooling. Advances in Neural Infor-
mation Processing Systems , 35:33330–33342, 2022. 6,7
[38] Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Heng-
shuang Zhao. Point transformer v2: Grouped vector atten-
tion and partition-based pooling. In NeurIPS , 2022. 8
[39] Lian Xu, Wanli Ouyang, Mohammed Bennamoun, Farid
Boussaid, and Dan Xu. Multi-class token transformer for
weakly supervised semantic segmentation. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 4310–4319, 2022. 2
[40] Xun Xu and Gim Hee Lee. Weakly supervised semantic
point cloud segmentation: Towards 10x fewer labels. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 13706–13715, 2020. 1,2
[41] Zongyi Xu, Bo Yuan, Shanshan Zhao, Qianni Zhang, and
Xinbo Gao. Hierarchical point-based active learning for
semi-supervised point cloud semantic segmentation. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 18098–18108, 2023. 2
[42] Cheng-Kun Yang, Ji-Jia Wu, Kai-Syun Chen, Yung-Yu
Chuang, and Yen-Yu Lin. An mil-derived transformer for
weakly supervised point cloud segmentation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 11830–11839, 2022. 1,2,6,8
[43] Cheng-Kun Yang, Min-Hung Chen, Yung-Yu Chuang, and
Yen-Yu Lin. 2d-3d interlaced transformer for point cloud
segmentation with scene-level supervision. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 977–987, 2023. 2
[44] Yunhan Yang, Xiaoyang Wu, Tong He, Hengshuang Zhao,
and Xihui Liu. Sam3d: Segment anything in 3d scenes. arXiv
preprint arXiv:2306.03908 , 2023. 2
[45] Sung-Hoon Yoon, Hyeokjun Kweon, Jegyeong Cho, Shin-
jeong Kim, and Kuk-Jin Yoon. Adversarial erasing frame-
work via triplet with gated pyramid pooling layer for weakly
supervised semantic segmentation. In Computer Vision–
ECCV 2022: 17th European Conference, Tel Aviv, Israel,
3730
October 23–27, 2022, Proceedings, Part XXIX , pages 326–
344. Springer Nature Switzerland Cham, 2022. 2
[46] Dingyuan Zhang, Dingkang Liang, Hongcheng Yang,
Zhikang Zou, Xiaoqing Ye, Zhe Liu, and Xiang Bai. Sam3d:
Zero-shot 3d object detection via segment anything model.
arXiv preprint arXiv:2306.02245 , 2023. 2
[47] Yachao Zhang, Zonghao Li, Yuan Xie, Yanyun Qu, Cuihua
Li, and Tao Mei. Weakly supervised semantic segmentation
for large-scale point cloud. In Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence , pages 3421–3429, 2021. 1,
2,8
[48] Yachao Zhang, Yanyun Qu, Yuan Xie, Zonghao Li, Shanshan
Zheng, and Cuihua Li. Perturbed self-distillation: Weakly
supervised large-scale point cloud semantic segmentation. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 15520–15528, 2021. 1,2,6,8
3731
