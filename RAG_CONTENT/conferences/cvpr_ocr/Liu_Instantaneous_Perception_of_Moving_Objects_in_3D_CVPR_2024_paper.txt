Instantaneous Perception of Moving Objects in 3D
Di Liu1Bingbing Zhuang3Dimitris N. Metaxas1Manmohan Chandraker2,3
1Rutgers University2University of California, San Diego3NEC Labs America
(a)AFast-Moving Vehicle
(b)AParkingVehicle Startsto Move
(c)AForwardMovingVehicle Stops andReverses
Figure 1. Illustration of Instantaneous Motion Perception. We visualize motion of three objects from Waymo dataset [46], each with
three consecutive frames. Objects in fast and subtle motions are marked as red and blue, respectively, with arrow length indicating motion
magnitude. While standard motion detection handles general large motions such as (a), we focus on instantaneous perception of subtle
motions that may indicate changes in driving behavior, for example (b) parking car starts to move, and (c) forward moving car stops and
reverses. The visualized subtle motions (b)(c) are output from our framework. We also provide the video visualization in supplementary.
Abstract
The perception of 3D motion of surrounding traffic par-
ticipants is crucial for driving safety. While existing works
primarily focus on general large motions, we contend that
the instantaneous detection and quantification of subtle mo-
tions is equally important as they indicate the nuances in
driving behavior that may be safety critical, such as behav-
iors near a stop sign of parking positions. We delve into
this under-explored task, examining its unique challenges
and developing our solution, accompanied by a carefully
designed benchmark. Specifically, due to the lack of cor-
respondences between consecutive frames of sparse Lidar
point clouds, static objects might appear to be moving – theso-called swimming effect. This intertwines with the true
object motion, thereby posing ambiguity in accurate esti-
mation, especially for subtle motions. To address this, we
propose to leverage local occupancy completion of object
point clouds to densify the shape cue, and mitigate the im-
pact of swimming artifacts. The occupancy completion is
learned in an end-to-end fashion together with the detec-
tion of moving objects and the estimation of their motion,
instantaneously as soon as objects start to move. Extensive
experiments demonstrate superior performance compared
to standard 3D motion estimation approaches, particularly
highlighting our method’s specialized treatment of subtle
motions.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
19573
1. Introduction
Human drivers pay special attention to surrounding moving
objects to understand and predict their driving behavior, and
react accordingly to avoid collisions. Similarly, intelligent
autonomous systems must also navigate safely through traf-
fic scenes, where preventing collisions with moving objects
is considerably more complex than with static background
scenes. This gives rise to several lines of computer vision
research centered around motion, ranging from low-level
tasks like 3D scene flow [24], to middle-level motion seg-
mentation or detection [49], and high-level perception on
3D object tracking [54].
However, these methods are designed to handle general
3D motion without considering the extent and context of the
motion. In this paper, we would like to focus on an impor-
tant subset of motion – small subtle motions. Such motions
are of significance as they often indicate changes in driving
intention or behavior; for instance, as illustrated in Fig. 1,
parking vehicles start to move and cut into the driving lane,
or vehicles in the driving lane start to reverse back for re-
verse parking. As a more general note, the instant capture
of all nuanced changes happening in the scene is essential
for situation awareness, especially in corner-scenario cases.
This however remains under-explored in computer vision,
thus motivates our research in this paper, which aims to de-
tect the presence of subtle motions as well as estimate their
motion flow instantaneously.
While prominent motions from fast-moving objects are
more feasible to detect and quantify due to a strong signal-
to-noise ratio, recovering subtle motions with high accuracy
presents its challenges. Specifically, the Lidar sensor cap-
tures only a sparse point set of the surrounding scene el-
ements, and the pattern of points varies depending on the
relative position between the Lidar and the scene. Conse-
quently, there are typically no point correspondences across
frames with a moving Lidar sensor, even for static scene el-
ements. This further implies that static objects may appear
to be moving, known as the swimming artifact [8, 21]. It
intertwines with and hence obfuscates the true object mo-
tion, especially under small motions; as such, we empiri-
cally observe that a model trained for general motions does
not perform as well with subtle motions.
To address this, our framework proposes to learn shape
completion before performing motion detection and estima-
tion. Taking sequential frames of Lidar point clouds within
a short period as input, our method voxelizes the point
clouds as occupancy grids, and leverages the accumulated
Lidar points from nearby frames to generate a denser occu-
pancy grid, which is then applied as supervision for occu-
pancy completion. This effectively densifies and enhances
the surface cue to mitigate the impact of the swimming ar-
tifact. To prevent the noises brought about by imperfect
completion from affecting the downstream motion estima-tion task, we refrain from performing full object shape com-
pletion but rather do so locally only for the visible surface
part, where the major motion signals lie. Our framework
takes the point clouds of each object individually as input
to our network, which is trained exclusively on the regime
of small motion. We name our method S’More, indicating
subtle motion regressor. Since there is no standard train-
ing dataset and evaluation benchmark specific to subtle mo-
tions. We contribute one by extracting small motions from
the large-scale Waymo dataset [46], leveraging its existing
annotations. We demonstrate the efficacy of the proposed
method with the newly proposed benchmark.
In summary, our contributions include:
• Introduce the task of detecting and estimating subtle mo-
tions for vehicles, with insights on their practical signifi-
cance and challenges.
• Design a framework with occupancy completion to miti-
gate the swimming artifacts on small motion estimation.
• Translate our insights into favorable performance in a new
evaluation benchmark tailored for small motions.
2. Related Work
3D Scene Flow. 3D scene flow aims to estimate the motion
field of each observed 3D point. It is an important tool for
analyzing scene dynamics and has been extensively studied
in computer vision [10, 17, 23, 25, 33, 41, 52, 57]. While
the scene flow of background points as the dominant rigid
motion may be reliably estimated [8], accurately estimating
the motion flow for dynamic foreground objects remains a
challenge. This leads to object-aware scene flow works [3,
22, 23, 48] that leverage rigidity prior of objects. However,
the nearest neighbor-based approach for motion estimation
like scene flow suffers from inherent ambiguity brought by
the “swimming” effect [21] of Lidar point clouds, which is
more severe with smaller motion magnitude. In this work,
we develop insights towards addressing this issue.
Moving Object Detection. Moving object detection is an
essential capability for autonomous vehicles and other ar-
eas, which results in many prior works, such as [7, 13, 15,
20, 29, 38, 39, 45, 49]. However, they detect motions at a
coarse level for general large motions. SemanticKITTI [4],
a commonly used dataset in this field, labels moving ob-
ject in a coarse sequence level instead of in an instanta-
neous manner. In contrast, our approach offers special
treatment for detecting small motions instantaneously. An-
other possible way for moving object detection is through
3D detection [14, 18, 27, 28, 34, 37, 40, 58] and track-
ing [19, 54]. However, we found empirically that such
methods stumble in identifying small motions due to im-
perfect object localization. Lastly, we also note a concur-
rent work, M-detector [53], that instantly detects point-level
moving events based on occlusion principles.
19574
…Frame i-1FrameiFrame i+1    …
Sequential LiDAR InputV oxelizationEncoderDecoderMotionDetector
OccupancyPredictionObjectofInterest…MotionFlowPredictorOccupancyGT
CompletedOccupancy
OccupancyCompletion
Line of SightEmptyGridUnknownGridOccupied Grid
Training-time+Inference-timeTraining-timeonly
Figure 2. Overview of S’More. Given a sequential LiDAR point cloud, we first identify objects of interest by filtering out background
and objects with large motion. We then voxelize the point cloud for each object, extract features through an encoder-decoder network, and
perform occupancy completion. The output is passed to a motion detector and a motion flow predictor for final detection and estimation.
Occupancy Prediction. Occupancy is an effective 3D
scene representation that has wide applications in au-
tonomous driving. Argo et al. [1] and Reza et al. [36] apply
the occupancy flow field for perception and motion forecast-
ing. [21] performs 4D occupancy forecasting supervised
by point cloud forecasting. ALSO [6] utilizes occupancy
completion as a tool for self-supervised feature learning
for Lidar point clouds. Another line of work [47, 50, 51]
learns occupancy prediction from monocular cameras. In
this work, we present the first known attempt to use occu-
pancy completion to facilitate the estimation of small mo-
tions from LiDAR point clouds.
Subtle Motions in General. The field of computer vi-
sion has shown a long history of interest in small mo-
tions in a broader context. Differential structure-from-
motion [35, 60] aims to recover instantaneous camera mo-
tion from optical flow. Several works [9, 16, 30–32, 55]
utilize accidental camera motion to perform 3D reconstruc-
tion. Another line of research [12, 26] targets magnifying
invisible subtle motions in videos. Our work focuses on
perceiving the subtle motions of surrounding vehicles, a ca-
pability critical to the safety of autonomous systems.
3. Method
3.1. Problem Definition and Challenges
Our goal is to identify moving objects and estimate their
motion using sequential point clouds; we focus in this pa-
3DtrackingLargemotionSmallmotionorstaticS’MoreSmallmotionStaticFigure 3. Integration of S’More with 3D tracking systems.
per on vehicles while leaving the human category for future
work. Unlike existing works, we concentrate on small mo-
tion for instantaneous detection as objects begin to move.
As a preprocessing step, our framework filters out fast-
moving objects, thereby targeting static and slow-moving
ones. Fig. 3 illustrates a practical use case of such setting in
a 3D tracking system. Further, we make a practical assump-
tion that the ego-vehicle’s motion can be reliably estimated
by ICP, possibly aided by GPS/INS, as validated by recent
studies [8]. This allows for the exclusion of ego-motion
from the observed object motion, resulting in a noisy obser-
vation of the true object motion w.r.t. the world coordinate.
Our method is object-centric, processing point clouds from
five consecutive frames ( Ft, t=1, ..., T ) to classify objects
as static or moving. For moving objects, we estimate the
motion flow from F1toFTfor each point in F1, setting
T=5as per [20].
Swimming Effect. The detection and estimation of small
motions present its challenges, primarily arising from the
sparse nature of Lidar point clouds. Remarkably, the spa-
tial distribution of captured points closely depends on the
19575
(a)(b)(c)Figure 4. Illustration of Swimming Effect on ground (a) and
static object (b), and (c) our ground truth occupancy completion
for (b). Bule and red points indicate points from two frames.
relative position between the Lidar sensor and surrounding
scene elements. Hence, as the Lidar sensor moves along
with the ego-vehicle, there are typically no exact point cor-
respondences across frames, and even static scene elements
may appear to be moving. This effect manifests itself on
both background scene and foreground objects, as illus-
trated in Fig. 4 (a)(b). In particular, the ground points vi-
sually appear to be swimming across frames, hence termed
asswimming effect [21]. This effect poses challenges, es-
pecially to characterizing subtle motions as one would need
to distinguish the true object motion from this effect. We
note that the sparse nature of Lidar points distinguishes the
problem from optical flow [11], where dense correspon-
dences exist and small motions simplify the flow estimation
through brightness consistency assumption.
3.2. Our Framework
Fig. 2 illustrates the overview of S’More. Given five con-
secutive Lidar frames, we filter out fast-moving objects
and for each remaining ones, our network estimates the
small motion or the lack thereof. We start by voxelizing
the input point clouds, followed by feature extraction using
an encoder-decoder network, and then perform occupancy
completion, the output of which is passed to the motion seg-
mentation head and the instantaneous flow estimation head.
3.2.1 Occupancy Completion
Input Voxelization. First, following state-of-the-art 3D de-
tection frameworks [34, 54], we voxelize the point cloud
Xt∈RN×3at each frame as a binary grid of size
[Wx, Wy, Wz], with the voxels containing Lidar points
filled with 1, and 0 elsewhere. This grid may be viewed
as an incomplete occupancy grid, in that it indicates part
of the visible object surface captured by Lidar at a single
timestamp. We stack consecutive frames to form a spatial-
temporal grid of size [T, W x, Wy, Wz].
Local Occupancy Completion. Recall that the sparse na-
ture of Lidar point clouds poses challenges to accurate small
motion estimation, due to the actual subtle motion inter-
twined with the swimming effect. To mitigate this issue,
our framework first learns occupancy completion that effec-tively densifies the object surface, to offer stronger cues for
subsequent networks to reason correspondence, and hence
the motion between frames.
Before proceeding, one should be mindful of the poten-
tial trade-off brought by this step – the estimated occupancy
completion may well be imperfect, introducing extra noise
to the system. This may be harmful to the final motion
segmentation and estimation if the noises reach a certain
level, hence defeating the purpose of occupancy comple-
tion. Therefore, while the standard shape completion prob-
lem ( e.g. [42]) is tasked to recover the entire object shape
from a single-frame input, it is an overly complicated and
unnecessary task in our case, besides the infeasibility of get-
ting the ground truth in real driving scenes. Instead, since
theTLidar frames collectively observe only a local part of
the object within an instantaneous timeframe, we target lo-
cal occupancy completion at these observed regions, while
refraining from hallucinating areas invisible to all TLidar
frames. This way, we enhance the critical signal essen-
tial for motion characterization while minimizing the extra
noises from imperfect completions.
Supervision for Occupancy. We densify the local occu-
pancy grid by leveraging nearby frames, as shown in Fig. 2.
Specifically, for each frame Ft, t∈[1, ..., T ]in the input
window, we warp all Lidar points from the rest T−1frames
toFt, using the ground truth object motion (recall that ego-
motion has been factored out), and then mark the corre-
sponding voxel as occupied, i.e. 1. In addition, we mark the
points along the line-of-sight as empty i.e. 0. All other vox-
els are deemed as unknown. We apply a fast voxel traversal
algorithm [2] to implement this step, similarly as in [21],
with an example illustrated in Fig. 4(b)(c). Note we only
use the Lidar frames inside the input window to generate
the target occupancy grid, to simplify the task. By learning
occupancy, the network is explicitly enforced to learn the
notion of dense shapes in an end-to-end manner, thereby
facilitating the task of motion detection and estimation.
3.2.2 Network Architecture and Losses
Network Architecture. We apply an encoder-decoder
for occupancy grid prediction, which is passed to another
encoder-decoder for motion detector and motion flow pre-
dictor. The motion detector classifies input objects as static
or moving, while the flow estimator regresses a motion vec-
tor for each occupied voxel in the grid. We then extract
the motion flow for each raw input point as the predicted
flow in the voxel that point resides in. Note that we do not
enforce rigidity constraints on the flow field, maintaining
the method’s generality, though we do evaluate the setting
with rigidity prior as well. We utilize the encoder-decoder
structure as in [56], consisting of simple convolutional lay-
ers with skip connection; we follow [21] to treat the height
19576
and temporal dimension as the channel dimension, which
allows to use 2D convolutional layers for efficiency; see
supplementary for details. Our network processes each ob-
ject separately, but remains efficient and runs at 27 fps for a
scene consisting of 30 objects of interest.
Overall Losses. The overall loss function of our model is a
weighted combination of five terms:
L=λoccLocc+λmotLmot+λepeLepe+λrelLrel+λangLang.
(1)
Specifically, we apply a binary cross-entropy (BCE) loss
Loccfor the occupancy grid prediction, a BCE loss Lmot
on static/moving object classification, a L1lossLepeand
a scale-aware Lrelloss on motion flow prediction for mov-
ing objects. Additionally, since the motion direction carries
important information about driving intention such as re-
versing or left/right turning, we add an angular loss Langfor
the motion flow. We denote the set of occupied and empty
voxels as ϕoandϕe, respectively.
Occupancy Loss is written as
Locc=Ev∈{ϕo,ϕe}h
ˆOvlog(Ov) + (1 −ˆOv) log(1 −Ov)i
,
(2)
where OvandˆOvindicate the predicted and ground truth
occupancy at voxel v.
Flow Prediction Losses. For each voxel v, we define the
ground truth flow (denoted as ˆfv) as the average of the
ground truth flow associated with the points falling into that
voxel. The relative flow loss Lrelis written as
Lrel=Ev∈ϕo∥ˆfv−fv∥2
∥ˆfv∥2+ε, (3)
where ˆfvindicates predicted flow at voxel vandεis a small
constant. Note this loss is inverse scaled by the flow mag-
nitude to emphasize learning on small motions. And the
augular loss is written as
Lang=Ev∈ϕoacos 
⟨fv,ˆfv⟩
∥fv∥2· ∥ˆfv∥2+ε!
(4)
where ⟨·,·⟩denotes the dot product between the vectors.
4. Experiments
4.1. Evaluation of S’More
Evaluation Benchmark. In the absence of existing bench-
mark dedicated to subtle motions, we curate one such
dataset from the Waymo open dataset [46], where each se-
quence provides Lidar frames at 10Hz for about 20s. We
collect the point clouds for each object from every five con-
secutive frames (0.5s) denoted as Fi, i=[1, ...,5]. To gener-
ate ground truth motion status, we follow [20] to derive theTable 1. Quantitative Comparisons with ICP, Point-to-Plane
ICP, Generalized ICP, CenterPoint and FastNSF.
EPE (↓) Angle Error ( ↓) F1 Score ( ↑)
FastNSF [24] 0.1189 0.5592 0.6180
ICP [5] 0.0554 0.4499 0.7456
Point-to-Plane ICP [43] 0.2263 0.4379 0.7856
Generalized ICP [44] 0.1117 0.4170 0.7693
CenterPoint [54] 0.0927 0.5622 0.7270
S’More 0.0437 0.3189 0.8323
spatial transformation from the 3D boxes annotations, and
hence compute the scene flow fifrom F1toF5for every
point xiinF1. To concentrate on small motions, we deem
the data sample valid only when the minimal flow magni-
tudefmin= min xi∈F1||fi||is less than 0.2m. Further, we
label the object as static if fmin<fthre. We set fthre=0.05m
but also evaluate under other settings shortly. This way, we
collect about 140k and 9k samples for training and test sets,
respectively. More details are in the supplementary.
Evaluation Metrics. We apply the standard F1 score to
measure the accuracy of static/moving object classification.
We apply end-point error (EPE) and the angular error to
measure the object motion flow error.
Baselines. In the absence of existing detection methods
dedicated to small motions, we mainly compare with: (i)
the classical Iterative Closest Point (ICP) [5], which re-
mains competitive [8] for motion flow task; (ii) the point-
to-plane ICP [43] and the generalized ICP [44] imple-
mented in Open3D [59]; (iii) the leading scene flow method
FastNSF [24]; (iv) the detection and tracking based method
CenterPoint [54], where we use ground-truth tracking by
associating detected objects with the ground truth, and the
motion flows are derived from boxes transformation. For all
methods we use their output motion flows to detect moving
objects, according to the aforementioned criterion.
Comparison. Tab. 1 shows quantitative evaluation re-
sults, indicating the significantly superior performance of
our model compared to the baselines. We note that the
object-tracking method CenterPoint gives decent accuracy
but lags behind S’More, likely because their imperfect ob-
ject localization causes ambiguity in distinguishing small
motions from static ones. In Fig. 5, we provide a visual-
ization comparison with ICP and FastNSF. The input com-
prises two sets of point clouds: the first frame (in red) and
the last frame (in green). We visualize the flow accuracy
through alignment – we shift the red points with the flow
and the resultant points (marked as blue) should ideally
align well with the green points if the flows are correct.
Our model demonstrates superior alignment accuracy, es-
pecially at the subtle level of local registration, attributable
to its advanced motion estimation capabilities. More exam-
ples are provided in the supplementary material.
19577
(a)Input(b)GroundTruth(c)S’More(ours)(d)FastNSF(e)ICP
Figure 5. Qualitative Comparison. We exhibit point cloud registration results for two point cloud sets: the first frame (in red) and the last
frame (in green). The results are shown using (b) ground truth motion, and estimated motions by (c) S’More(ours), (d) FastNSF, and (e)
ICP. The blue points indicate resultant positions after adding flow to the red points, which should ideally align with the green points.
Inputw/oOccw/Occ as auxiliary taskS‘MoreGroundTruth
(f)(d)(e)(a)w/oOcc(b) w/Occ as auxiliary task(c) S‘MoreMotFlow
MotionDetectorFlowPredictorOccPredictorMotFlowOcc
MotFlowOcc
w/oOccw/Occ as auxS‘Morew/oOccw/Occ as auxS‘More
Figure 6. Ablation study on Occupancy Completion. (a)(b)(c) illustrate structure of having the occupancy module removed or as an
auxiliary task. (d)(e)(f) show the qualitative and quantitative comparisons; the visualization in (d) follows the same protocol in Fig. 5.
(a)TwoFramesofInput(b)GTOccCompletion(c)PredictedOccCompletion
Figure 7. Example Results of Occupancy Completion. The blue
and red points represent different frames in the input.
4.2. Ablation study on Occupancy Completion
To investigate the impact of occupancy completion, we re-
move this module from S’More, with structure shown in
Fig. 6(a) in relation to S’More in Fig. 6(c). Further, we also
evaluate the setting with the occupancy completion as justan auxiliary trained in parallel with the motion detector and
flow predictor, as shown in Fig. 6(b). We report the accu-
racy in Fig. 6(e)(f), which indicates the significant impact
of occupancy completion towards good performance. We
attribute this to its role in effectively densifying object sur-
faces. In Fig. 6(d) we provide visualization of point cloud
registration to evaluate the estimated motion, further sup-
porting the efficacy of occupancy completion. The qualita-
tive results of the occupancy completion itself are demon-
strated in Fig. 7.
False Positive/Negative. This analysis explores how in-
corporating occupancy impacts the reduction of false posi-
tives and negatives. In Fig. 8(a), we display three sequential
frames that highlight how the model, without occupancy, er-
roneously detects a stationary vehicle as moving, marked by
a red box. Conversely, when employing dense occupancy
estimation, the model correctly identifies the stationary na-
ture of the vehicle, reducing the false positives. Similarly, in
Fig. 8(b), we show that without occupancy, the model fails
19578
(a)
w/oOccw/ Occw/ Occw/oOcc(b)Frame iFrame i+1 Frame i+2 (false positive)
(false negative)Figure 8. Visualization of false positive/negative samples in the absence of occupancy completion. Each row shows three consecutive
frames, one per column. The motion of objects is marked with blue for detected subtle motions and red for GT large motions, with the arrow
length representing the motion’s magnitude. In (a), a false positive occurs where a stationary vehicle is mistakenly marked as moving. In
(b), a moving vehicle is incorrectly identified as static, a false negative. Both of them are rectified with the occupancy completion; note the
ground truth is same as the correct detection here hence is not visualized.
Table 2. Ablation Study of Losses. This study observes a correlation between decreasing flow thresholds fthreand the degradation of
model performance. Notably, our model achieves the best average performance over five thresholds in terms of all metrics.
End-Point-Error (EPE) ( ↓) Angle Error ( ↓) F1 Score ( ↑)
fthre 0.05 0.04 0.03 0.02 0.01 Avg 0.05 0.04 0.03 0.02 0.01 Avg 0.05 0.04 0.03 0.02 0.01 Avg
w/oLocc 0.0492 0.0443 0.0440 0.0489 0.0439 0.0461 0.3489 0.3455 0.3828 0.4219 0.4405 0.3879 0.8256 0.8202 0.8025 0.7575 0.7603 0.7932
w/oLangle 0.0458 0.0441 0.0452 0.0444 0.0454 0.0450 0.3469 0.3654 0.4052 0.4295 0.4860 0.4066 0.8344 0.8338 0.8111 0.7704 0.7841 0.8068
w/oLrel 0.0446 0.0435 0.0439 0.0437 0.0430 0.0437 0.3179 0.3395 0.3780 0.4156 0.4291 0.3760 0.8378 0.8307 0.7937 0.7907 0.7847 0.8075
S’More 0.0437 0.0425 0.0436 0.0428 0.0421 0.0429 0.3189 0.3392 0.3834 0.4053 0.4324 0.3758 0.8323 0.8320 0.8003 0.7997 0.7857 0.8100
to detect a moving vehicle as static. This is rectified using
the model with occupancy. This comparison underscores
the effectiveness of using occupancy data in enhancing mo-
tion detection accuracy in our model. An interesting obser-
vation we found is that the occurrence frequency of false
positives is much higher than false negatives. We attribute
this to the swimming effect, where static objects appear to
be moving, especially under subtle motion conditions.
4.3. Analysis on Large Motion
Performance on large motion. While our focus is on small
motions, it would be more complete to study its perfor-
mance on large motions as well. It is of particular interest
to compare against 3D object tracking based method given
the practical system illustrated in Fig. 3. To this end, we
(a)(b)Figure 9. Performance Analysis of (a) occupancy with different
grid sizes and (b) point-/object-level flow estimation. We evaluate
under different value of fthrefor comprehensiveness.
train another model of S’More including large-motion data
and evaluate only on the large motion ( fmin>0.2m) regime.
As shown in Tab. 3, both S’More and CenterPoint achieve
nearly perfect detection accuracy (F1 →1.0), as expected
19579
Table 3. Quantitative evaluation on large motions .
EPE (↓) Angle Error ( ↓) F1 Score ( ↑)
CenterPoint [54] 0.1155 0.0247 0.9998
S’More 0.2790 0.0486 0.9847
Table 4. Benefits of having small-motion-specific training data .
Training dataset EPE ( ↓) Angle Error ( ↓) F1 Score ( ↑)
Small + large motion 0.0979 0.5550 0.1414
Small motion only 0.0437 0.3189 0.8323
due to the large signal-to-noise ratio. This perfectness sup-
ports our focus on the small motion regime for enhancing
a practical system. We also note that being a detection-
tracking method, CenterPoint yields more precise flow esti-
mation, as its accuracy largely depends on 3D box localiza-
tion instead of motion.
Benefit of using small-motion-specific dataset. Recall
that we have filtered out the large-motion data during the
dataset curation, i.e. they are not in training data. Our ex-
perience is that excluding large motions in training data im-
proves the model performance on the small motion regime.
We report the accuracy in Tab. 4 to quantify this effect. The
benefit may be explained by the unique swimming artifact,
which mandates a small-motion-specific dataset.
4.4. Evaluation in terms of Latency
With a focus on instantaneous detection, a time-sensitive
task, it is helpful to also evaluate with a time-related met-
ric. Our original task is to detect objects moving more than
fthre=0.05m in a 0.5s latency. Here, we increase fthreto tar-
get at larger motion, which effectively allows proportionally
increased latency if assuming constant velocity, hence de-
creasing the requirement on the latency. We report in Tab. 5
the detection accuracy (F1) across different latencies, indi-
cating the consistently superior performance from S’More.
4.5. Important Design Choices
Grid Size. We study the impact of occupancy grid size
and find it important in our design. We compare the per-
formance of two grid sizes: a balanced 100×100×100
grid and an alternative 500×500×4grid, where the latter
significantly reduces the resolution along z-axis. The re-
sults in Fig. 9(a), reveal a notable performance degradation
(see the two dashed lines are consistently lower than the
solid lines) when the z-axis resolution is reduced, despite
the increased axial resolution to 500×500. This may also
result in detection ambiguity which stems from the model’s
reduced capacity to discern subtle vertical variations, and
lead to less reliable vehicle localization and motion detec-
tion.Table 5. Evaluation of detection (F1 score) in terms of latency.
Latency 0.5s 0.7s 0.9s 1.1s 1.3s
ICP [5] 0.7758 0.8129 0.8229 0.8308 0.8346
CenterPoint [54] 0.7270 0.7658 0.7800 0.7899 0.7935
S’More 0.8323 0.9003 0.9224 0.9568 0.9582
Point-/Object-level Instance Flow Estimation. We study
the behavior of the point-level and object-level training
strategies for motion flow estimation. The point-level ap-
proach predicts a separate flow for each point, whereas
the object-level strategy regresses a single rigid transfor-
mation for the entire object to calculate motion flow. The
results detailed in Fig. 9(b) in terms of EPE and Angle Er-
ror across various flow thresholds, reveal that point-level
instance flow estimation (indicated by two solid lines) con-
sistently outperforms the object-level approach (indicated
by two dashed lines). Notably, point-level estimation main-
tains stable performance even at very low thresholds. In
contrast, object-level flow prediction exhibits significant
fluctuations in performance across different thresholds.
Loss Components. We ablate each loss component and
report the results under various flow thresholds in Tab. 2.
Notably, we observe that as the flow threshold decreases,
there is a corresponding degradation in model performance.
This trend aligns with our expectation, as lower thresholds
are designed to detect subtler motions. Subtler motions of-
ten come with more severe swimming effects and thus lead
to less accurate predictions. We also observe that each loss
component plays a critical role in tuning the model for these
subtleties, making them essential for maintaining perfor-
mance across varying motion dynamics.
5. Conclusion
This paper defines the problem of perceiving subtle motion
for vehicles, presenting practical significance. To mitigate
swimming artifacts causing ambiguity in subtle motion per-
ception, we leverage occupancy completion as an effective
strategy to facilitate motion learning. Despite the overall
good performance, our method faces challenges under ex-
tremely sparse or high-occluded objects. Also, we currently
only handle vehicles but not pedestrians or cyclists. We
hope our work and its limitations can inspire more research
into this important yet under-explored problem.
Acknowledgement
This research project has been partially funded by research
grants to Dimitris N. Metaxas through NSF: 2310966,
2235405, 2212301, 2003874, and FA9550-23-1-0417.
19580
References
[1] Ben Agro, Quinlan Sykora, Sergio Casas, and Raquel Urta-
sun. Implicit occupancy flow fields for perception and pre-
diction in self-driving. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
1379–1388, 2023. 3
[2] John Amanatides, Andrew Woo, et al. A fast voxel traver-
sal algorithm for ray tracing. In Eurographics , pages 3–10.
Citeseer, 1987. 4
[3] Aseem Behl, Despoina Paschalidou, Simon Donn ´e, and An-
dreas Geiger. Pointflownet: Learning representations for
rigid motion estimation from point clouds. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , 2019. 2
[4] Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-
zel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Se-
mantickitti: A dataset for semantic scene understanding of
lidar sequences. In ICCV , 2019. 2
[5] Paul J Besl and Neil D McKay. Method for registration of
3-d shapes. In Sensor fusion IV: control paradigms and data
structures , pages 586–606. Spie, 1992. 5, 8
[6] Alexandre Boulch, Corentin Sautier, Bj ¨orn Michele, Gilles
Puy, and Renaud Marlet. Also: Automotive lidar self-
supervision by occupancy estimation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2023. 3
[7] Qi Chang, Zhennan Yan, Mu Zhou, Di Liu, Khalid Sawalha,
Meng Ye, Qilong Zhangli, Mikael Kanski, Subhi Al’Aref,
Leon Axel, et al. Deeprecon: Joint 2d cardiac segmentation
and 3d volume reconstruction via a structure-specific gener-
ative method. In International Conference on Medical Image
Computing and Computer-Assisted Intervention , pages 567–
577. Springer, 2022. 2
[8] Nathaniel Chodosh, Deva Ramanan, and Simon Lucey. Re-
evaluating lidar scene flow for autonomous driving. arXiv
preprint arXiv:2304.02150 , 2023. 2, 3, 5
[9] Ilya Chugunov, Yuxuan Zhang, Zhihao Xia, Xuaner Zhang,
Jiawen Chen, and Felix Heide. The implicit values of a good
hand shake: Handheld multi-frame neural depth refinement.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 2852–2862, 2022. 3
[10] Fangqiang Ding, Andras Palffy, Dariu M Gavrila, and
Chris Xiaoxuan Lu. Hidden gems: 4d radar scene flow
learning using cross-modal supervision. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2023. 2
[11] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip
Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van
Der Smagt, Daniel Cremers, and Thomas Brox. Flownet:
Learning optical flow with convolutional networks. In Pro-
ceedings of the IEEE international conference on computer
vision , 2015. 4
[12] Brandon Y Feng, Hadi Alzayer, Michael Rubinstein,
William T Freeman, and Jia-Bin Huang. 3d motion mag-
nification: Visualizing subtle motions from time-varying ra-
diance fields. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 9837–9846, 2023. 3[13] Artem Filatov, Andrey Rykov, and Viacheslav Murashkin.
Any motion detector: Learning class-agnostic scene dynam-
ics from a sequence of lidar point clouds. In 2020 IEEE in-
ternational conference on robotics and automation (ICRA) ,
pages 9498–9504. IEEE, 2020. 2
[14] Yunhe Gao, Mu Zhou, Di Liu, Zhennan Yan, Shaoting
Zhang, and Dimitris N Metaxas. A data-scalable transformer
for medical image segmentation: architecture, model effi-
ciency, and benchmark. arXiv preprint arXiv:2203.00131 ,
2022. 2
[15] Yunhe Gao, Zhuowei Li, Di Liu, Mu Zhou, Shaoting Zhang,
and Dimitris N Meta. Training like a medical resident: uni-
versal medical image segmentation via context prior learn-
ing. arXiv preprint arXiv:2306.02416 , 2023. 2
[16] Hyowon Ha, Sunghoon Im, Jaesik Park, Hae-Gon Jeon, and
In So Kweon. High-quality depth from uncalibrated small
motion clip. In Proceedings of the IEEE conference on
computer vision and pattern Recognition , pages 5413–5421,
2016. 3
[17] Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng
Song, Mengwei Ren, Ruijiang Gao, Anastasis Stathopou-
los, Xiaoxiao He, Yuxiao Chen, et al. Proxedit: Improving
tuning-free real image editing with proximal guidance. In
Proceedings of the IEEE/CVF Winter Conference on Appli-
cations of Computer Vision , pages 4291–4301, 2024. 2
[18] Xiaoxiao He, Chaowei Tan, Bo Liu, Liping Si, Weiwu Yao,
Liang Zhao, Di Liu, Qilong Zhangli, Qi Chang, Kang Li,
et al. Dealing with heterogeneous 3d mr knee images: A fed-
erated few-shot learning method with dual knowledge dis-
tillation. In 2023 IEEE 20th International Symposium on
Biomedical Imaging (ISBI) , pages 1–5. IEEE, 2023. 2
[19] Kuan-Chih Huang, Ming-Hsuan Yang, and Yi-Hsuan Tsai.
Delving into motion-aware matching for monocular 3d ob-
ject tracking. In ICCV , 2023. 2
[20] Shengyu Huang, Zan Gojcic, Jiahui Huang, Andreas Wieser,
and Konrad Schindler. Dynamic 3d scene analysis by point
cloud accumulation. In European Conference on Computer
Vision , 2022. 2, 3, 5
[21] Tarasha Khurana, Peiyun Hu, David Held, and Deva Ra-
manan. Point cloud forecasting as a proxy for 4d occupancy
forecasting. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2023. 2, 3, 4
[22] Kuan-Hui Lee, Matthew Kliemann, Adrien Gaidon, Jie Li,
Chao Fang, Sudeep Pillai, and Wolfram Burgard. Pillarflow:
End-to-end birds-eye-view flow estimation for autonomous
driving. In 2020 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS) , pages 2007–2013.
IEEE, 2020. 2
[23] Xueqian Li, Jhony Kaesemodel Pontes, and Simon Lucey.
Neural scene flow prior. Advances in Neural Information
Processing Systems , 34:7838–7851, 2021. 2
[24] Xueqian Li, Jianqiao Zheng, Francesco Ferroni, Jhony Kae-
semodel Pontes, and Simon Lucey. Fast neural scene flow.
InProceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV) , 2023. 2, 5
[25] Zhuowei Li, Long Zhao, Zizhao Zhang, Han Zhang, Di Liu,
Ting Liu, and Dimitris N Metaxas. Steering prototypes with
19581
prompt-tuning for rehearsal-free continual learning. In Pro-
ceedings of the IEEE/CVF Winter Conference on Applica-
tions of Computer Vision , pages 2523–2533, 2024. 2
[26] Ce Liu, Antonio Torralba, William T Freeman, Fr ´edo Du-
rand, and Edward H Adelson. Motion magnification. ACM
transactions on graphics (TOG) , 24(3):519–526, 2005. 3
[27] Di Liu, Jiang Liu, Yihao Liu, Ran Tao, Jerry L Prince, and
Aaron Carass. Label super resolution for 3d magnetic reso-
nance images using deformable u-net. In Medical Imaging
2021: Image Processing , pages 606–611. SPIE, 2021. 2
[28] Di Liu, Zhennan Yan, Qi Chang, Leon Axel, and Dimitris N
Metaxas. Refined deep layer aggregation for multi-disease,
multi-view & multi-center cardiac mr segmentation. In Inter-
national Workshop on Statistical Atlases and Computational
Models of the Heart , pages 315–322. Springer, 2021. 2
[29] Di Liu, Yunhe Gao, Qilong Zhangli, Ligong Han, Xiaox-
iao He, Zhaoyang Xia, Song Wen, Qi Chang, Zhennan Yan,
Mu Zhou, et al. Transfusion: multi-view divergent fusion
for medical image segmentation with transformers. In In-
ternational Conference on Medical Image Computing and
Computer-Assisted Intervention , pages 485–495. Springer,
2022. 2
[30] Di Liu, Xiang Yu, Meng Ye, Qilong Zhangli, Zhuowei Li,
Zhixing Zhang, and Dimitris N Metaxas. Deformer: Inte-
grating transformers with deformable models for 3d shape
abstraction from a single image. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 14236–14246, 2023. 3
[31] Di Liu, Long Zhao, Qilong Zhangli, Yunhe Gao, Ting Liu,
and Dimitris N Metaxas. Deep deformable models: Learning
3d shape abstractions with part consistency. arXiv preprint
arXiv:2309.01035 , 2023.
[32] Di Liu, Qilong Zhangli, Yunhe Gao, and Dimitris Metaxas.
Lepard: Learning explicit part discovery for 3d articulated
shape reconstruction. Advances in Neural Information Pro-
cessing Systems , 36, 2024. 3
[33] Xingyu Liu, Charles R Qi, and Leonidas J Guibas.
Flownet3d: Learning scene flow in 3d point clouds. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 529–537, 2019. 2
[34] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang,
Huizi Mao, Daniela L Rus, and Song Han. Bevfusion:
Multi-task multi-sensor fusion with unified bird’s-eye view
representation. In 2023 IEEE International Conference on
Robotics and Automation (ICRA) , 2023. 2, 4
[35] Yi Ma, Jana Ko ˇseck´a, and Shankar Sastry. Linear differen-
tial algorithm for motion recovery: A geometric approach.
International Journal of Computer Vision , 36:71–89, 2000.
3
[36] Reza Mahjourian, Jinkyu Kim, Yuning Chai, Mingxing
Tan, Ben Sapp, and Dragomir Anguelov. Occupancy flow
fields for motion forecasting in autonomous driving. IEEE
Robotics and Automation Letters , 7(2):5639–5646, 2022. 3
[37] Carlos Mart ´ın-Isla, V ´ıctor M Campello, Cristian Izquierdo,
Kaisar Kushibar, Carla Sendra-Balcells, Polyxeni Gkontra,
Alireza Sojoudi, Mitchell J Fulton, Tewodros Weldebirhan
Arega, Kumaradevan Punithakumar, et al. Deep learn-
ing segmentation of the right ventricle in cardiac mri: Them&ms challenge. IEEE Journal of Biomedical and Health
Informatics , 2023. 2
[38] Benedikt Mersch, Xieyuanli Chen, Ignacio Vizzo, Lucas
Nunes, Jens Behley, and Cyrill Stachniss. Receding moving
object segmentation in 3d lidar data using sparse 4d convo-
lutions. IEEE Robotics and Automation Letters , 7(3):7503–
7510, 2022. 2
[39] Benedikt Mersch, Tiziano Guadagnino, Xieyuanli Chen, Ig-
nacio Vizzo, Jens Behley, and Cyrill Stachniss. Building vol-
umetric beliefs for dynamic environments exploiting map-
based moving object segmentation. IEEE Robotics and Au-
tomation Letters , 2023. 2
[40] Zhixiang Min, Bingbing Zhuang, Samuel Schulter, Buyu
Liu, Enrique Dunn, and Manmohan Chandraker. Neurocs:
Neural nocs supervision for monocular 3d object localiza-
tion. In CVPR , 2023. 2
[41] Himangi Mittal, Brian Okorn, and David Held. Just go with
the flow: Self-supervised scene flow estimation. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 11177–11185, 2020. 2
[42] Jeong Joon Park, Peter Florence, Julian Straub, Richard
Newcombe, and Steven Lovegrove. Deepsdf: Learning con-
tinuous signed distance functions for shape representation.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 165–174, 2019. 4
[43] Szymon Rusinkiewicz and Marc Levoy. Efficient variants of
the icp algorithm. In Proceedings third international confer-
ence on 3-D digital imaging and modeling , 2001. 5
[44] Aleksandr Segal, Dirk Haehnel, and Sebastian Thrun.
Generalized-icp. In Robotics: science and systems , 2009.
5
[45] Jiadai Sun, Yuchao Dai, Xianjing Zhang, Jintao Xu, Rui Ai,
Weihao Gu, and Xieyuanli Chen. Efficient spatial-temporal
information fusion for lidar-based 3d moving object segmen-
tation. In 2022 IEEE/RSJ International Conference on In-
telligent Robots and Systems (IROS) , pages 11456–11463.
IEEE, 2022. 2
[46] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien
Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,
Yuning Chai, Benjamin Caine, et al. Scalability in perception
for autonomous driving: Waymo open dataset. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , 2020. 1, 2, 5
[47] Wenwen Tong, Chonghao Sima, Tai Wang, Li Chen, Silei
Wu, Hanming Deng, Yi Gu, Lewei Lu, Ping Luo, Dahua Lin,
et al. Scene as occupancy. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 8406–
8415, 2023. 3
[48] Kavisha Vidanapathirana, Shin-Fang Chng, Xueqian Li, and
Simon Lucey. Multi-body neural scene flow. 2023. 2
[49] Jun Wang, Xiaolong Li, Alan Sullivan, Lynn Abbott, and Si-
heng Chen. Pointmotionnet: Point-wise motion learning for
large-scale lidar point clouds sequences. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) Workshops , pages 4419–4428, 2022. 2
[50] Xiaofeng Wang, Zheng Zhu, Wenbo Xu, Yunpeng Zhang,
Yi Wei, Xu Chi, Yun Ye, Dalong Du, Jiwen Lu, and Xin-
gang Wang. Openoccupancy: A large scale benchmark for
19582
surrounding semantic occupancy perception. arXiv preprint
arXiv:2303.03991 , 2023. 3
[51] Yi Wei, Linqing Zhao, Wenzhao Zheng, Zheng Zhu, Jie
Zhou, and Jiwen Lu. Surroundocc: Multi-camera 3d occu-
pancy prediction for autonomous driving. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 21729–21740, 2023. 3
[52] Song Wen, Hao Wang, Di Liu, Qilong Zhangli, and Dimitris
Metaxas. Second-order graph odes for multi-agent trajectory
forecasting. In Proceedings of the IEEE/CVF Winter Confer-
ence on Applications of Computer Vision , pages 5101–5110,
2024. 2
[53] Huajie Wu, Yihang Li, Wei Xu, Fanze Kong, and Fu Zhang.
Moving event detection from lidar point streams. Nature
Communications , 2024. 2
[54] Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-
based 3d object detection and tracking. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , 2021. 2, 4, 5, 8
[55] Fisher Yu and David Gallup. 3d reconstruction from ac-
cidental motion. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 3986–
3993, 2014. 3
[56] Wenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin
Yang, Sergio Casas, and Raquel Urtasun. End-to-end in-
terpretable neural motion planner. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8660–8669, 2019. 4
[57] Gang Zhang, Xiaoyan Li, and Zhenhua Wang. Sequential
multi-view fusion network for fast lidar point motion esti-
mation. In European Conference on Computer Vision , 2022.
2
[58] Qilong Zhangli, Jingru Yi, Di Liu, Xiaoxiao He, Zhaoyang
Xia, Qi Chang, Ligong Han, Yunhe Gao, Song Wen, Haim-
ing Tang, et al. Region proposal rectification towards robust
instance segmentation of biological images. In International
Conference on Medical Image Computing and Computer-
Assisted Intervention , pages 129–139. Springer, 2022. 2
[59] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3D: A
modern library for 3D data processing. arXiv:1801.09847 ,
2018. 5
[60] Bingbing Zhuang, Loong-Fah Cheong, and Gim Hee Lee.
Rolling-shutter-aware differential sfm and image rectifica-
tion. In ICCV , 2017. 3
19583
