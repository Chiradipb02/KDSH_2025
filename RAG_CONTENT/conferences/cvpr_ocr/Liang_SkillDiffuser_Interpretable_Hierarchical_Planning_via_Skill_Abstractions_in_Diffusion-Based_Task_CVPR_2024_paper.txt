SkillDiffuser: Interpretable Hierarchical Planning via Skill Abstractions in
Diffusion-Based Task Execution
Zhixuan Liang1,3Yao Mu1,3Hengbo Ma2
Masayoshi Tomizuka2Mingyu Ding2†Ping Luo1,3†
1The University of Hong Kong2University of California, Berkeley3Shanghai AI Laboratory
{zxliang, ymu, pluo }@cs.hku.hk {hengbo ma, tomizuka, myding }@berkeley.edu
https://skilldiffuser.github.io/
Abstract
Diffusion models have demonstrated strong potential for
robotic trajectory planning. However, generating coher-
ent trajectories from high-level instructions remains chal-
lenging, especially for long-range composition tasks requir-
ing multiple sequential skills. We propose SkillDiffuser, an
end-to-end hierarchical planning framework integrating in-
terpretable skill learning with conditional diffusion plan-
ning to address this problem. At the higher level, the skill
abstraction module learns discrete, human-understandable
skill representations from visual observations and language
instructions. These learned skill embeddings are then used
to condition the diffusion model to generate customized la-
tent trajectories aligned with the skills. This allows gener-
ating diverse state trajectories that adhere to the learnable
skills. By integrating skill learning with conditional trajec-
tory generation, SkillDiffuser produces coherent behavior
following abstract instructions across diverse tasks. Ex-
periments on multi-task robotic manipulation benchmarks
like Meta-World and LOReL demonstrate state-of-the-art
performance and human-interpretable skill representations
from SkillDiffuser. More visualization results and informa-
tion could be found on our website.
1. Introduction
Recent research [6, 7, 18, 19] has demonstrated diffusion
models’ superior generative capabilities compared to previ-
ous models that help enhance reinforcement learning across
various dimensions, including the generation of action tra-
jectories [2, 19], policy representation [5, 50], and data syn-
thesis [14, 23]. However, their ability to generate coherent
trajectories for intricate tasks still poses challenges in terms
of performance and generalizability, as these tasks often re-
†Corresponding authors.
Visual Inputpick objplace objpush objmove top…
Instruction InputDenoise U-NetLearnable Skill Set!!!"Denoise U-Net
“pick the apple and place it in the bowl”Instruction Input
!!!"
(a)Language-conditioned Diffuser(b) Our SkillDiffuserwidelygeneralizableTransferable and InterpretableNon-transferable
“pick the apple and place it in the bowl”easily adaptable
High Level AbstractionFigure 1. Comparison of SkillDiffuser and previous language
conditioned diffusers. SkillDiffuser utilizes high-level abstrac-
tion to translate visual observations and language instructions into
human understandable skills with language grounding. It then en-
ables the low-level diffusion model condition on these skills, not
only improving the execution performance of multi-step composi-
tion tasks but greatly enhancing the generalization and adaptability
of the framework.
quire the fulfillment of abstract instructions that consist of
numerous coordination-intensive sequential steps.
Previous approaches [2, 14], such as Decision Diffuser,
aim to tackle this challenge by decomposing complex tasks
into simpler sub-skills, organized within a predefined skill
library. These methods rely on conditioning the diffusion
model with one-hot skill representations to generate trajec-
tories for each of these sub-skills. Nevertheless, these tech-
niques encounter difficulties in attempting to autonomously
learn end-to-end from a wide range of datasets, which hin-
ders their scalability and ability to achieve end-to-end learn-
ing [1]. In addition, without explicitly learning reusable
skills, models cannot capture intricate inter-step dependen-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
16467
cies and constraints, yielding fragmented and illogical tra-
jectories. Decomposing ambiguous instructions into learn-
able sub-goals and skills can better enable models to fol-
low logical step sequences, respect task structures, and
transfer common procedures ( e.g. reusable skill abstrac-
tion and adaptable skill-based diffusion) between different
tasks. This skill-centric paradigm paves the way for dif-
fusion models that can interpret and execute elaborate, ab-
stract instructions necessitating numerous sequential steps.
Inspired by the above observations, we propose SkillD-
iffuser, a hierarchical planning framework unifying high-
level skill learning with low-level conditional diffusion-
based execution. As shown in Fig. 1, compared with previ-
ous language-conditioned diffusion policies, SkillDiffuser
is able to interpret and execute complex instructions end-to-
end with higher transferability. SkillDiffuser induces inter-
pretable sub-latent goals by learning reusable skills tailored
to the task instructions. The framework conditions a diffu-
sion model on these learned skills to generate customized,
coherent trajectories aligned with the overall objectives.
By integrating hierarchical skill decomposition with con-
ditional trajectory generation, SkillDiffuser achieves con-
sistent, skill-oriented behavior without relying on a prede-
fined skill library. Moreover, SkillDiffuser is designed to
operate solely on visual observations, eliminating the need
for robot proprioception ( i.e. fully observed states). This
end-to-end methodology, featuring learnable skills, enables
SkillDiffuser to execute abstract instructions across a vari-
ety of tasks efficiently.
SkillDiffuser works as follows. 1)A skill predictor with
vector quantization [45] is used for high-level skill learn-
ing to distill tasks into discrete and interpretable skills.
Rather than forecasting skill durations, we adopt a fixed
prediction horizon – predicting skills at regular intervals.
This horizon-based discretization process seamlessly inte-
grates visual and linguistic inputs into a cohesive skill set
guiding the low-level diffusion model. 2)For skill-based
trajectory generation, we utilize a classifier-free diffusion
model as policy, with skills directly embedded as guidance.
This setting allows for generating multi-modal state trajec-
tories aligned with skill specifications while avoiding over-
fitting to a closed dataset. 3)To enable action inference
from predicted states, we train an inverse dynamics network
to decode motions between two consecutive frames gener-
ated. By separating state prediction from motion decoding,
SkillDiffuser yields a fully adaptive framework for directing
diverse embodiments via transferable state-space plans.
We evaluate the model’s performance in both skill learn-
ing and multi-task planning on the LOReL [29] Sawyer and
Meta-World [51] datasets, with crucial experimental set-
tings by considering robotic agents in real-world scenarios
must operate with limited state information, chiefly visual
observations. Furthermore, we present success rates on un-seen compositional tasks, the reusability and visualizations
of learned skills to illustrate the model does have the ability
to abstract high-level skills that are not only effective but
also understandable to humans, bringing us closer to intel-
ligent agents that acquire skills in a direct manner.
Our contributions are three-fold: 1)We propose an
end-to-end hierarchical planning framework via skill learn-
ing for sub-goal abstraction; 2)We adopt a classifier-free
diffusion model conditioned on learned skills to generate
skill-oriented transferable state trajectories; 3)We demon-
strate state-of-the-art performance on complex bench-
marks and provide interpretable visualizations of human-
understandable skill representations.
2. Related Works
2.1. Imitation Learning and Multi-task Learning
Imitation learning (IL) has evolved from foundational be-
havioral cloning to sophisticated multi-task learning frame-
works. With traditional approaches relying on supervised
learning from expert demonstrations [33, 37, 38], recent ad-
vancements have shifted towards learning the reward [16]
or Q-function [12] from expert data, enhancing the abil-
ity to mimic complex behaviors. A new challenge lies in
multi-task IL [41], where imitators are trained across varied
tasks, aiming for generalization to new scenarios with task
specifications ranging from vector states [28] to vision and
language descriptions [8, 11, 13, 48].
Multi-task learning approaches often leverage shared
representations to learn a spectrum of tasks simultaneously,
enhancing the flexibility and efficiency of the learning pro-
cess. The Meta-World benchmark [51] assesses multi-task
and meta reinforcement learning, highlighting the need for
algorithms capable of rapid adaptation. Building on this,
the Prompting Decision Transformer [49] showcases few-
shot policy generalization using task-specific prompts. And
diffusion policy has also been explored in multi-task set-
tings [14], which shows proficiency in generating diverse
behaviors across tasks. However, unlike methods that lever-
age state inputs [14, 49] or access robot proprioception [30],
SkillDiffuser uses raw visual inputs only.
2.2. Skill Discovery and Hierarchical Learning
Skill learning, the process by which robots acquire new
abilities or refine existing ones, is gaining increasing atten-
tion due to its pivotal role in enabling autonomous systems
to adapt to new tasks, improve the performance over time,
and interact naturally with humans and complex environ-
ments. Traditionally, this domain was influenced by hand-
crafted features and expert demonstrations [32].
With the development of deep learning, Eysenbach et
al. [9] and A. Sharma et al . [40] investigated skill dis-
covery in learning methods, achieving policies conditioned
16468
on learned latent variables and maintaining consistent skill
codes throughout trajectories. In the domain of skill learn-
ing through language instructions, LISA [13] stands out by
sampling multiple skills per trajectory, integrating language
conditioning in a unique manner.
Our SkillDiffuser follows this way to extract sub-skills of
each instruction at the higher-level. But differently, SkillD-
iffuser employs an adaptable diffusion policy at the lower-
level to condition on different sub-skills to generate differ-
ent actions, which formulates a creative hierarchical plan-
ning framework also advancing research on hierarchical
techniques of reinforcement learning [22, 27, 52].
2.3. Planning with Diffusion Model
Diffusion models [18] make great breakthroughs in image
synthesis [18] recent years and has shown promising results
in various generative applications. A seminal work that
performs planning with diffusion directly is Diffuser [19],
which laid the groundwork for using diffusion models in
behavior synthesis. Then, a branch of this kind planning
methods achieved state-of-the-art performance in a variety
of decision tasks [3, 5, 7, 23, 31]. Among them, the work
done by Chi et al. in [5] introduces the concept of learning
the gradient of the action-distribution score function and it-
eratively optimizing it, demonstrating its significant poten-
tial in visuomotor policy learning. These works have further
extended this direction and strengthened the versatility and
generalization of diffusion-based planners.
Our approach is inspired by classifier-free diffusion
guidance [17], which offers a significant advantage over
classifier-guided models [6]. By adopting the classifier-free
approach, we can circumvent the challenges associated with
training a reward model and Q-function, which are partic-
ularly cumbersome in many real-world planning scenarios
of which the complexity is very high. Recent studies have
also extended this direction, which use conditional diffu-
sion models to generate customized trajectories. Decision
Diffuser [2] is an example which is designed to generate
trajectories for a predefined skill library. However, unlike
our method, it can’t autonomously learn skill abstractions
in an end-to-end fashion, which makes it difficult to scale to
more tasks. This highlights the necessity for diffusion mod-
els with dynamic, learnable skill abstractions, facilitating
complex instruction execution.
3. Preliminary
3.1. Planning with Diffusion over States
As introduced in previous works [2, 19], diffusion model
is a promising tool to address the problem of planning in
reinforcement learning which is cast as a Markov Deci-
sion Process (MDP) [34]. Within the MDP framework
M= (S,A,T,R, γ), the planning policy aims to iden-tify an optimal action sequence a∗
0:Tthat maximizes the
expected cumulative rewards over a finite time horizon T,
governed by the state transition dynamics Tand reward
function R.Sis the state space and Ais the action space.
By treating the state trajectory as sequence data τ, with
sequence modeling, diffusion probabilistic models concep-
tualize planning as an iterative denoising process. The
model progressively refines trajectories by reversing a for-
ward diffusion process that is modeled as a Gaussian pro-
cess, whereby noise is incrementally added to the data, de-
noted as pθ(τi−1|τi). Training involves minimizing the
ELBO of the data’s negative log-likelihood, similar to vari-
ational Bayesian inference, with the optimization objective:
θ∗= arg min
θ−Eτ0
logpθ 
τ0
, (1)
where p 
τN
is a standard normal distribution and τ0de-
notes noiseless sequence data.
For practical implementation, a simplified surrogate loss
function is proposed in [18], focusing on predicting the
Gaussian mean of the reverse diffusion step:
Ldenoise (θ) =Ei,τ0∼q,ϵ∼N[||ϵ−ϵθ(τi, i)||2]. (2)
3.2. Classifier-free Diffusion Guidance
On the basis of unconditioned diffusion-based method, in
the realm of offline reinforcement learning, a critical en-
deavor is to generate trajectories with the highest reward-
to-go. With the flourishing development of conditional dif-
fusion models [6], classifier-guided approaches embark on
this by infusing specific trajectory information (encoded in
y(τ)), such as the return J(τ0)or designated constraints,
into the diffusion process:
q(τi|τi−1), p θ(τi−1|τi,y(τ)). (3)
With assumptions specified in [10], we have
τi−1∼ N(µθ+αΣ∇τlogp 
y(τi)
,Σ), (4)
where αis a hyperparameter that adjusts the guidance
strength, Σis the specified covariance of the noise and µθis
the learned mean value of noise in unconditional diffusion.
However, the classifier-guided diffusion model requires
an accurate estimation of guidance gradient based on the
trajectory classifier y(τ), which may not be feasible and
need to introduce a separate dynamic programming proce-
dure to estimate a Q-function in the training process.
Thus, classifier-free guidance offer an alternative, which
augments the trajectory generation process with a guidance
signal that amplifies the features of high-reward or optimal
characteristics, i.e.y(τ), that are implicitly present in the
data. Mathematically, the noise to add during the reverse
denoising process is:
ˆϵ=ϵθ(τi,∅, i) +ω(ϵθ(τi,y, i)−ϵθ(τi,∅, i)),(5)
where ωis the guidance scale, and ∅represents the ab-
sence of guidance. Setting ω= 0removes the classifier-free
16469
guidance towards an unconditional diffusion model, while a
large value of ωstrengthens the influence of the conditional
information during trajectory generation.
Also, the loss function to minimize can be rewritten as,
Ldiff(θ) =Ei,τ,ϵhϵ−ϵθ 
τi,(1−β)y(τi) +β∅, i2i
,
(6)
where βis a hyperparameter that controls the probability of
dropping specific condition y(τ), enhancing sample diver-
sity while maintaining context relevance.
After training the noise prediction model ϵθwith the
above L(θ), the trajectory is sampled starting from Gaus-
sian noise and progressively denoised with modified ˆϵ
through Eq. 5, employing re-parameterization technique.
In summary, through classifier-free guidance, we can
modulate the trajectory sampling process to require the gen-
erated trajectories more aligned with the desired features
represented by y(τ). This process iteratively applies the
conditioned noise model to refine the target trajectories that
contain future states satisfying the constraints.
4. Methodology
4.1. Overview
Building upon the motivations discussed above, we present
SkillDiffuser, an advanced methodological framework for
robust multi-task learning across various robots. This dy-
namic approach leverages the cooperation of skill learn-
ing at the higher level and a conditional diffusion model
at the lower level. An overall framework is illustrated in
Fig. 2. Notably, we leverage language-grounded represen-
tations to abstract skills, thereby rendering the execution
of tasks through our diffusion policy both interpretable and
comprehensible to humans.
4.2. High Level Interpretable Skill Abstractions
In our SkillDiffuser framework, the high level interpretable
skill abstraction module plays a crucial role in understand-
ing and executing complex tasks. However, given the multi-
task environments we consider, each task with only a single
language instruction may be broken down into a sequence
of sub-tasks or skills, which are not explicitly delineated
within the instruction itself. Furthermore, suppose the of-
fline training dataset is denoted as D, it consists of trajecto-
ries derived from a sub-optimal policy for various tasks. A
trajectory τ= (l,{it,at}T
t=1)includes language descrip-
tionland a sequence of image observations and actions
(it,at)over time steps T, with no reward labels attached.
But the trajectories do not indicate the boundaries between
sub-tasks as well, which thus requires the proposed meth-
ods capable of segmenting and interpreting the tasks into
sub-goals in an unsupervised manner.
To address this challenge, we build a skill abstraction
component upon a horizon-based skill predictor adapted
Skill1Visual ObservationLanguage EncoderVisual Encoder
State DiffusionSkill PredictorVector Quantization
Inverse DynamicsLayerNorm
InstructionVisual EmbeddingLanguage Embedding
Skill Set0123K…“𝐶𝑙𝑜𝑠𝑒	𝑑𝑟𝑎𝑤𝑒𝑟”
…State PredictionActions	𝑎!,𝑎",	…,	𝑎#
🔥……High Level Abstraction
Low Level ExecutionFigure 2. Overall framework of SkillDiffuser. It’s a hierarchical
planning model that leverages the cooperation of interpretable skill
abstractions at the higher level and a skill conditioned diffusion
model at the lower level for task execution in a multi-task learning
environment. The high-level skill abstraction is achieved through
a skill predictor and a vector quantization operation, generating
sub-goals (skill set) that the diffusion model employs to determine
the appropriate future states. Future states are converted to actions
using an inverse dynamics model. This unique fusion enables a
consistent underlying planner across different tasks, with the vari-
ation only in the inverse dynamics model.
from GPT-2 [35]. It is designed to parse and decompose
tasks by fusing visual input and natural language instruc-
tions, alongside a Vector Quantization (VQ) sub-module
that discretizes the learned skills into a skill set. The
specifics of this component are illustrated below.
Firstly, as we utilize only images as robot state informa-
tion, we transform the images into latent space features with
a fixed image encoder ( e.g. R3M [30]). For convenience,
we denote the image encoder as Φim:I → RI, where
Irepresents the space of input images and RI, the resul-
tant feature space, serves to condense the visual informa-
tion into a form that is conducive for high-level semantics.
Simultaneously, we use a language encoder to pre-process
the natural language instructions, which is formalized as
Φlang:L→ RL, with Lbeing the space of language in-
structions and RLthe language feature space. The outputs
of both encoders are then fed into the skill predictor, which
operates to integrate these two modalities.
The skill predicting process is as follows: An image
it∈ I at time step tis encoded into a visual embedding
st= Φ im(it). This embedding stis then input to the
skill predictor, along with the language instruction l∈L,
through the language encoder’s output lt= Φ lang(l). The
16470
Skill MLPLearned SkillDiffusion step 𝒊Time MLPSkill Set0123K…z
Lang EncoderVisual EncoderImage
High Level AbstractionLang
Noise𝜏!"#𝜏!
𝜏!𝜏"
In-context ConcatenateTime EmbedSkill EmbedFusion MLPConv BlockConv BlockActivationConv BlockConv BlockActivationN ×BlockofU-Net
Inverse DynamicsLow-level ActionsCurrent ObservationEnvsInput𝜏Figure 3. SkillDiffuser’s low level skill-conditioned diffusion planning model. Notably, while the schematic here employs images to
represent visual features for illustrative purposes, in actual implementation, both the input to and the sampling output of the diffusion model
are state embeddings. The current observation is also the feature embedding of current visual observation.
skill predictor, represented as f:RI× RL→ C, generates
a skill code ˜zby˜z=f(st,lt)that encapsulates task’s re-
quirements interpreted from the visual and language inputs.
After that, Vector Quantization [45] operation q(·)
is taken to transform these predicted skills into a low-
dimensional discrete set C. The discrete skill set contains K
skill embeddings
z1,z2, . . . ,zK	
which represent differ-
ent potential skills. VQ operation is achieved by mapping
the latent ˜zto its closest entry of the skill set with skill vec-
tors updated to be the moving average of the embeddings z
closest to them, same as [45]. This process is as follows,
˜z=f(Φim(it),Φlang(l)), (7)
z=q(˜z) = arg min
zk∈C∥˜z−zk∥2. (8)
VQ enforces each learnt skill zto lie in C, which is equal
to learning Kprototypes for the language embeddings uti-
lizing k-means [24] algorithm. This acts as a bottleneck,
restricting the flow of language information and aiding in
the learning of discrete skill codes. The back-propagation
through the non-differentiable quantization is achieved by
a straight-through gradient estimator, which simply copies
the gradients and enables model to be trained end-to-end.
In our approach, we apply a consistent skill code z
across a defined number of time steps, termed the horizon.
This consistent application across multiple horizons adeptly
addresses the challenge of varying sub-task durations with-
out altering the horizon itself. Consequently, this strategy
not only preserves the flexibility required for diverse task
execution but also maintains the model’s architectural sta-
bility by avoiding horizon-induced structural changes.Importantly, the discrete nature of the learned skill codes
enhances the interpretability and controllability of the sys-
tem’s behavior, as each skill code is associated with some
human-understandable language phrases. An example is
depicted in Fig 6. Through this method, SkillDiffuser can
not only learn to perform tasks based on language instruc-
tions but also achieve them in a human-interpretable way,
allowing for a deeper understanding and control of the
decision-making process of specific embodied agents.
4.3. Low Level Skill-conditioned Diffusion Planning
As highlighted in Section 1, while existing approaches like
Decision Diffuser [2] have introduced conditional diffu-
sion models constrained by skills, their capability is limited
to generating trajectories that meet only predefined skills.
Consequently, these models fall short of realizing a diffu-
sion model capable of conditioning on an arbitrary learned
skill. To overcome these constraints and enable diffusion
models to plan over a learned continuous spectrum of skills,
we propose an approach that leverages the classifier-free
guided diffusion model with the skills embedded.
SkillDiffuser begins by employing a diffusion model to
operate over the continuous skill space Zlearned during
high-level skill abstraction. We employ a U-Net to serve as
the noise prediction model ϵθ(·)and guide it by in-context
conditioning. More specifically, we firstly utilize a skill
MLP (similar to point-wise feed-forward network [46]), de-
noted as Λ, to align skill features with denoising model.
After that, we fuse the skill embeddings Λ(z)into the state
features throughout the residual blocks of U-Net. Details
16471
are depicted in Fig. 3. In this way, we make the diffusion
model contextually modulate the influence of skill embed-
dings at each step. And following Eq. 5, we can synthesize
future states attending to these given skills. This is a signif-
icant shift from previous static conditioning framework to a
more dynamic and adaptive trajectory generation process.
Moreover, following previous works [2, 7, 23], we adopt
a state-only diffusion model, which eschews the direct gen-
eration of actions in diffusion model. And instead, we uti-
lize another MLP, denoted as Ψ(·), to perform inverse dy-
namics after state generation to infer feasible actions that
can achieve transitions between two continuous states. Ad-
ditionally, we integrate observations from the current frame
to provide more detailed information for motion prediction
and achieve closed-loop control. Mathematically, it is:
at= Ψ([ ˜st,˜st+1],it)fort= 0, . . . , T −1, (9)
where ˜stand˜st+1are consecutive observation embeddings
within ˜τgenerated by diffusion model, itis the current ob-
servation and atis the inferred action.
As the resulting action sequence {a0,a1, . . . ,aT−1}de-
rived from the generated states, it encapsulates the skills to
execute the tasks, which allows for remarkable adaptabil-
ity across multiple tasks. And when faced with a new task,
we are only required to change the inverse dynamics model
Ψ(·)specific to the new task’s kinematics, with the archi-
tecture and parameters of diffusion model unchanged. Such
modularity ensures the generative capabilities of SkillDif-
fuser are not task-specific but can be leveraged across a di-
verse range of tasks with varying dynamics. A schematic
diagram of the low level module is shown in Fig. 3.
4.4. Training the SkillDiffuser
We engineered a threefold loss function for SkillDiffuser.
Firstly, for the inverse dynamics model which is task-
specific, we employ a behavior cloning loss [44] to train our
inverse MLP emulating expert actions from observed state
transitions. This loss, denoted as Linv, is formulated as:
Linv=Eτ∼Dha−Ψ(˜s,˜s′,i)2
2i
, (10)
where the notations are similar to Eq. 9.
Correspondingly, the other parts including both skill ab-
straction and low level execution are task-agnostic. For the
high-level skill abstraction module, we apply a vector quan-
tization (VQ) loss to refine the skill predictor. This VQ loss,
LVQ, ensures the embeddings produced by the skill predic-
tor closely match the skill set vectors, thereby improving the
interpretability and consistency of the skill representations.
Inspired by VQ-V AE [45], we formulate it as:
LVQ=Eτ
∥q(˜z)−˜z∥2
2
, (11)
where ˜zfollows Eq. 7.
Lastly, for the low-level state-only skill-conditioned dif-
fusion execution, we incorporate a diffusion loss Ldiffasper Eq. 6, ensuring our model’s state predictions are in line
with both the skill guidance and temporal dynamics ob-
served in expert demonstrations. Here, we take y(τ) =
Λ(z)withzderived from Eq. 7 and Eq. 8.
To be noted, we train our SkillDiffuser with two opti-
mizers, one for inverse dynamics model with Linvand the
other for overall parameters of high-level skill abstraction
and low-level planning with LVQ+λLdiff, where λis a loss
weight. This carefully constructed loss architecture enables
SkillDiffuser to excel in a multi-task environment, general-
izing across tasks by simply substituting the inverse dynam-
ics model Ψ(·)specific to each new task’s requirements.
Additionally, we utilize a pre-trained distilBERT [39] as
our language encoder, adopting the configuration consistent
with LOReL [29] and LISA [13], while freezing its param-
eters to guarantee stability in language understanding. And
we employ diverse settings to serve as the visual encoder to
ensure fair comparison in different datasets. We elaborate
the details in corresponding parts of Sec. 5. More training
details are shown in Appendix G and we also provide some
pseudo-code of our algorithm in Appendix B.
5. Experiments
We first present a comprehensive evaluation on the LOReL
Sawyer dataset, and then perform the ablation study com-
pared on Meta-World benchmark to illustrate the efficiency
of our method. Finally, we visualize the learned skills of
our method both on LOReL and Meta-World MT10.
5.1. Datasets
LOReL Sawyer Dataset [29] which is abbreviated from
Language-conditioned Offline Reward learning, is com-
posed of pseudo-expert trajectories or play data gathered
from an arbitrary reinforcement learning policy, annotated
with post-hoc crowd-sourced language directives. The
LOReL Sawyer dataset encompasses 50k trajectories, each
with 20 steps, in a simulated Sawyer robot environment. We
assess our approach using the same six tasks as the original
paper [29] with paraphrased instructions under five differ-
ent situations ( i.e. seen, unseen verb, unseen none, unseen
verb + noun and human provided). This comes to a total of
77 instructions for all 6 tasks combined. More details about
the dataset can be found in Appendix D.1.
Meta-World Dataset [51]. It is a comprehensive bench-
mark designed for evaluating multi-task and meta reinforce-
ment learning algorithms. It introduces a comprehensive
suite of 50 distinct robotic manipulation tasks, all located
within a unified tabletop environment controlled by a simu-
lated Sawyer arm. The Multi-Task 10 (MT10) within Meta-
World is a subset comprising ten carefully selected tasks,
balanced in terms of diversity and complexity. Details of
the ten tasks can be found in Appendix D.3.
16472
Task Instruction Random LCBC [42] LCRL [20] Lang DT [13] LISA [13] SkillDiffuser
close drawer 52% 50% 58% 10% 100% 95 ±3.2%
open drawer 14% 0% 8% 60% 20% 55±13.3%
turn faucet left 24% 12% 13% 0% 0% 55±9.3%
turn faucet right 15% 31% 0% 0% 30% 25 ±4.4%
move black mug right 12% 73% 0% 20% 60% 18 ±3.9%
move white mug down 5% 6% 0% 0% 30% 10±1.7%
Average over tasks 20% 29% 13% 15% 40% 43±1.1%
Table 1. Task-wise success rates on LOReL Sawyer Dataset. We show our success rates compared to random policy, language-
conditioned imitation learning (LCBC), language-conditioned Q-learning (LCRL), a flat non-hierarchical Decision Transformer (Lang-
DT), and LISA. The results on each dataset are calculated over 3 seeds. SkillDiffuser outperforms all other methods in terms of average
performance over all tasks. Best methods and those within 6% of the best are highlighted in bold .
Rephrasal Type Lang DT LISA [13] SkillDiffuser
seen 15 40 43.65±4.7
unseen noun 13.33 33.33 36.01±6.3
unseen verb 28.33 30 36.70±9.5
unseen noun+verb 6.7 20 42.02±3.8
human provided 26.98 27.35 40.16±2.1
Average 18.07 30.14 39.71
Table 2. Rephrasal-wise success rates (in %) on LOReL
Sawyer. Results of Lang DT, LISA and our SkillDiffuser are
shown here. The standard error is calculated over 3 random seeds.
5.2. Evaluation Results on LOReL Sawyer Dataset
Baselines. We employ random policy, language condi-
tioned imitation learning (LCBC) [42], language condi-
tioned Q-learning (LCRL) [20], Lang-DT (also known as
Flat Baseline in [13]) and state-of-the-art skill-learning
method LISA [13] as our baselines. We follow the same
settings as LOReL [29] for the first three algorithms, and
follow LISA [13] for the last two. The random policy serves
as a baseline. And LCBC mimics offline dataset behav-
iors based on instructions, aligning with previous works
focusing on imitation learning to achieve language condi-
tioned behavior. In contrast, LCRL employs reinforcement
learning, labeling each episode’s final state with language-
instructed rewards. Lang-DT plays as a non-hierarchical
benchmark with a Causal Transformer [25], while LISA in-
corporates a dual-transformer structure, with one for skill
prediction and the other for action planning. We do not
compare with LOReL planner [29] as it uses MPC on
a learned reward function relying on human annotations,
while LISA and ours learn with trajectory data only.
Results. To ensure fair comparison, our method, SkillD-
iffuser, is designed to maintain a parameter count similar
to baseline models. It employs the same visual and lan-
guage encoder architecture as used in LISA [13] with metic-
ulously matching of embedding dimensions and the number
of heads across the layers of SkillDiffuser.
The results are present in Tables 1 and 2, showing task-
wise and rephrasal-wise success rates for LOReL, averagedMethod Lang DT LOReL [29] LISA [13] SkillDiffuser
Success Rate 13.33±1.3 18.18±1.8 20.89±0.6 25.21±2.7
Table 3. Performance on LOReL multi-step composition tasks.
over 10 runs with a 20-step time horizon. SkillDiffuser,
our approach, achieves the highest average performance in
six different tasks, indicating its superior cross-task adapt-
ability particularly when compared to similar approaches
which yet are based on Decision Transformer [4], such as
LISA [13]. Moreover, SkillDiffuser consistently excels in
all rephrased types for LOReL test tasks, outperforming
LISA by 9.6% on average. This demonstrates the model’s
robustness in handling varied skill representations, marking
a notable advancement in skill-conditioned diffusion model.
5.3. Performance on LOReL Compositional Tasks
Settings. We conduct experiments following the same set-
tings of unseen composition tasks of LISA [13] with 12
composition instructions in Tab. 3. Detailed instructions
are listed in Appendix D.2 with such an example that “open
drawer and move black mug right”. We extend the max
number of episode steps from customary 20 to 40, as LISA.
Results. We observe SkillDiffuser achieves 2x the perfor-
mance of non-hierarchical baseline (i.e. w/o skill abstrac-
tion) and also improves about 25% over LISA, highlighting
its effectiveness. MPC-based LOReL planners are unable
to perform as well in open scenarios like composition tasks.
5.4. Ablation Study on Meta-World Dataset
Settings. We conduct experiments on Meta-World MT10
benchmark with finely annotated instructions. We also use
visual observations only. Details are in Appendix D.3.
Baselines. We evaluate our method against three baselines,
all modified from existing models. The first, Flat R3M, is
adapted from R3M [30] paper’s planner. As the original one
utilizes the first four terms of robot proprioception, we elim-
inate them and make the planner focus exclusively on visual
observations. The second baseline is a variant of our SkillD-
iffuser, lacking the high-level skill abstraction module but
retaining the low-level conditional diffusion-based planner,
16473
Method Lang Skill Set Diffuser Performance
Flat R3M [30] ✗ ✗ ✗ 13.3%
LISA [13] ✓ ✓ ✗ 13.8%
Lang Diffuser ✓ ✗ ✓ 16.7%
SkillDiffuser ✓ ✓ ✓ 23.3%
Table 4. Ablation study of language skill conditioning on Meta-
World. Results of Flat R3M, Language-condition diffuser, LISA
and our SkillDiffuser are shown here. All are averaged over 3 runs.
to assess the impact of skill abstraction. This version in-
tegrates a two-layer MLP to predict options from visual
and language inputs, functioning as a language-conditioned
diffusion planner. The last baseline is our re-implemented
LISA [13] for Meta-World Dataset to validate the efficiency
of diffusion model. To ensure fairness, we use R3M as the
visual encoder for all of these methods on Meta-World.
Ablation on Language Skill Conditioning. Table 4 indi-
cates our Meta-World MT10 task is quite different from and
much more challenging than previous tasks that use states
as observations [49] or take into robot proprioception [30].
We only utilize single-frame visual input and instructions.
The Flat R3M method, lacking language conditioning and
skill sets can only succeed on tasks like drawer-close and
reach through behavior cloning. Lang-conditioned Diffuser
and modified LISA both outperform Flat R3M, suggesting
the value of each corresponding module. Our SkillDiffuser,
discretizing skills into a skill set, achieves a 6% higher per-
formance than language-conditioned diffuser and a 9.5%
higher than LISA, demonstrating the effectiveness of this
combinational architecture.
5.5. Ablation Study on Reusability of Learned Skills
To evaluate the reusability of our learned skills, we calcu-
late the average number of different skills used for a sin-
gle instruction and the total number of instructions using
each skill of LOReL Sawyer Dataset in Table 5. (With max
episode step being 20, we experiment with skill horizon
10.) We observe each single instruction uses 1.55 sub-skills
on average and each skill is called multiple times than the
number of instructions (75 with 5 eval episodes), verifying
the transferability of learnt skills. As suggested in Tab.10
of [13], except a very small horizon will hurt the perfor-
mance, learning sub-skills to get refined semantics helps
perform different actions at different stages. Besides, we
also visualize resulting images from applying discrete skills
in Appendix E.1 to further validate skills’ interpretability.
5.6. Visualization Results of Learned Skill Set
We show the visualization of skill set on LOReL compo-
sitional tasks here in Fig. 4 and results on original LOReL
dataset in Fig. 5 and Meta-World dataset in Fig. 6 in Ap-
pendix C. The visual analysis of our SkillDiffuser’s learned
skills on LOReL compositional tasks reveals that out of the
20-size skill set, our method learned 11 skills ( e.g.pull han-# of learnt skills # of inst # of success Use 1 skill Use 2 skills Average
17 375 144 64 80 1.55
Freq of 17 skills 30, 8, 14, 10, 5, 19, 4, 7, 20, 2, 9, 20, 25, 6, 10, 3, 18
Table 5. Average number of different skills used for a single in-
struction and total number of instructions used for each skill.
Figure 4. Visualization of skill heat map on LOReL Sawyer
compositional tasks. We display the word frequency associated
with a skill set of size 20 in LOReL, normalized by column. The
data’s sparsity and distinct highlights indicate certain language to-
kens are uniquely linked to specific skills. There are eleven skills
learned by our method. (zoom in for best view)
dle[skill 0], open counter [skill 14], etc.) notably distin-
guished by their unique word highlights. These bright spots
across eleven columns (changing from only one column at
initial which corresponds to default BC) in the heatmap un-
derscore the model’s ability to identify and isolate distinct
skills from visual inputs, without an explicitly defined skill
library. This indicates not only a significant interpretative
advancement over previous diffusion-based planning but a
successful abstraction of high-level skill representations.
6. Conclusion
This paper presents SkillDiffuser, an integrated framework
that enables robots to perform tasks from natural language
instructions by enabling interpretable skill learning and
conditional diffusion planning. It employs vector quanti-
zation to learn discrete and comprehensible skill represen-
tations directly from visual and linguistic demonstrations.
Subsequently, these skills condition a diffusion model to
generate state trajectories adhering to the learned skills.
Through integrating hierarchical skill decomposition with
conditional trajectory generation, SkillDiffuser can compre-
hend and execute abstract instructions for various manipu-
lation tasks. Extensive experiments on manipulation bench-
marks demonstrate state-of-the-art performance, highlight-
ing its effectiveness for multi-step composition tasks and
ability to automatically learn interpretable skills.
Acknowledgements
This paper is partially supported by the National Key R&D
Program of China No.2022ZD0161000 and the General Re-
search Fund of Hong Kong No.17200622.
16474
References
[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Cheb-
otar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu,
Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i
can, not as i say: Grounding language in robotic affordances.
2022. 1
[2] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B Tenenbaum,
Tommi S Jaakkola, and Pulkit Agrawal. Is conditional gen-
erative modeling all you need for decision making? In The
Eleventh International Conference on Learning Representa-
tions , 2022. 1, 3, 5, 6
[3] Joao Carvalho, An T Le, Mark Baierl, Dorothea Koert, and
Jan Peters. Motion planning diffusion: Learning and plan-
ning of robot motions with diffusion models. arXiv preprint
arXiv:2308.01557 , 2023. 3
[4] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee,
Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srini-
vas, and Igor Mordatch. Decision transformer: Reinforce-
ment learning via sequence modeling. Advances in neural
information processing systems , 34:15084–15097, 2021. 7
[5] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric
Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion
policy: Visuomotor policy learning via action diffusion. In
Proceedings of Robotics: Science and Systems (RSS) , 2023.
1, 3
[6] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems , 34:8780–8794, 2021. 1, 3
[7] Yilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir
Nachum, Joshua B Tenenbaum, Dale Schuurmans, and
Pieter Abbeel. Learning universal policies via text-guided
video generation. Advances in Neural Information Process-
ing Systems , 2023. 1, 3, 6
[8] Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI
Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel,
and Wojciech Zaremba. One-shot imitation learning. Ad-
vances in neural information processing systems , 30, 2017.
2
[9] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and
Sergey Levine. Diversity is all you need: Learning skills
without a reward function. arXiv preprint arXiv:1802.06070 ,
2018. 2
[10] William Feller. On the theory of stochastic processes, with
particular reference to applications. In Selected Papers I ,
pages 769–798. Springer, 2015. 3
[11] Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and
Sergey Levine. One-shot visual imitation learning via meta-
learning. In Conference on robot learning , pages 357–368.
PMLR, 2017. 2
[12] Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming
Song, and Stefano Ermon. Iq-learn: Inverse soft-q learning
for imitation. Advances in Neural Information Processing
Systems , 34:4028–4039, 2021. 2
[13] Divyansh Garg, Skanda Vaidyanath, Kuno Kim, Jiaming
Song, and Stefano Ermon. Lisa: Learning interpretable skill
abstractions from language. Advances in Neural Information
Processing Systems , 35:21711–21724, 2022. 2, 3, 6, 7, 8, 5[14] Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan
Zhang, Dong Wang, Bin Zhao, and Xuelong Li. Diffusion
model is an effective planner and data synthesizer for multi-
task reinforcement learning. Advances in neural information
processing systems , 2023. 1, 2
[15] Bernd Heidergott, editor. Taylor Series Expansions , pages
179–263. Springer US, Boston, MA, 2007. 1
[16] Jonathan Ho and Stefano Ermon. Generative adversarial im-
itation learning. Advances in neural information processing
systems , 29, 2016. 2
[17] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. In NeurIPS 2021 Workshop on Deep Generative
Models and Downstream Applications , 2021. 3
[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840–6851, 2020. 1, 3, 2
[19] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey
Levine. Planning with diffusion for flexible behavior syn-
thesis. In International Conference on Machine Learning ,
pages 9902–9915. PMLR, 2022. 1, 3
[20] Yiding Jiang, Shixiang Shane Gu, Kevin P Murphy, and
Chelsea Finn. Language as an abstraction for hierarchical
deep reinforcement learning. Advances in Neural Informa-
tion Processing Systems , 32, 2019. 7
[21] Diederik P Kingma and Jimmy Ba. Adam: A method
for stochastic optimization. In International Conference on
Learning Representations , 2015. 7
[22] Alexander C Li, Carlos Florensa, Ignasi Clavera, and Pieter
Abbeel. Sub-policy adaptation for hierarchical reinforce-
ment learning. arXiv preprint arXiv:1906.05862 , 2019. 3
[23] Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi
Tomizuka, and Ping Luo. Adaptdiffuser: Diffusion models
as adaptive self-evolving planners. In International Confer-
ence on Machine Learning , 2023. 1, 3, 6
[24] James MacQueen et al. Some methods for classification
and analysis of multivariate observations. In Proceedings of
the fifth Berkeley symposium on mathematical statistics and
probability , pages 281–297. Oakland, CA, USA, 1967. 5
[25] Valentyn Melnychuk, Dennis Frauen, and Stefan Feuer-
riegel. Causal transformer for estimating counterfactual out-
comes. In International Conference on Machine Learning ,
pages 15293–15329. PMLR, 2022. 7
[26] Diganta Misra. Mish: A self regularized non-monotonic ac-
tivation function. In 31st British Machine Vision Confer-
ence 2020, BMVC 2020, Virtual Event, UK, September 7-10,
2020 , 2020. 6
[27] Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey
Levine. Data-efficient hierarchical reinforcement learning.
Advances in neural information processing systems , 31,
2018. 3
[28] Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl,
Steven Lin, and Sergey Levine. Visual reinforcement learn-
ing with imagined goals. Advances in neural information
processing systems , 31, 2018. 2
[29] Suraj Nair, Eric Mitchell, Kevin Chen, Silvio Savarese,
Chelsea Finn, et al. Learning language-conditioned robot
behavior from offline data and crowd-sourced annotation. In
16475
Conference on Robot Learning , pages 1303–1315. PMLR,
2022. 2, 6, 7, 4
[30] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea
Finn, and Abhinav Gupta. R3m: A universal visual repre-
sentation for robot manipulation. In Conference on Robot
Learning , pages 892–909. PMLR, 2023. 2, 4, 7, 8, 6
[31] Fei Ni, Jianye Hao, Yao Mu, Yifu Yuan, Yan Zheng, Bin
Wang, and Zhixuan Liang. Metadiffuser: Diffusion model
as conditional planner for offline meta-rl. In International
Conference on Machine Learning , 2023. 3
[32] Scott Niekum, Sarah Osentoski, George Konidaris, Sachin
Chitta, Bhaskara Marthi, and Andrew G Barto. Learn-
ing grounded finite-state representations from unstructured
demonstrations. The International Journal of Robotics Re-
search , 34(2):131–157, 2015. 2
[33] Dean A Pomerleau. Efficient training of artificial neural net-
works for autonomous navigation. Neural computation , 3
(1):88–97, 1991. 2
[34] Martin L Puterman. Markov decision processes: discrete
stochastic dynamic programming . John Wiley & Sons, 1994.
3
[35] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, Ilya Sutskever, et al. Language models are unsu-
pervised multitask learners. OpenAI blog , 1(8):9, 2019. 4
[36] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III
18, pages 234–241. Springer, 2015. 6
[37] St ´ephane Ross and Drew Bagnell. Efficient reductions for
imitation learning. In Proceedings of the thirteenth inter-
national conference on artificial intelligence and statistics ,
pages 661–668. JMLR Workshop and Conference Proceed-
ings, 2010. 2
[38] St ´ephane Ross, Geoffrey Gordon, and Drew Bagnell. A re-
duction of imitation learning and structured prediction to no-
regret online learning. In Proceedings of the fourteenth inter-
national conference on artificial intelligence and statistics ,
pages 627–635. JMLR Workshop and Conference Proceed-
ings, 2011. 2
[39] V Sanh. Distilbert, a distilled version of bert: smaller, faster,
cheaper and lighter. In Proceedings of Thirty-third Confer-
ence on Neural Information Processing Systems , 2019. 6
[40] Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar,
and Karol Hausman. Dynamics-aware unsupervised discov-
ery of skills. arXiv preprint arXiv:1907.01657 , 2019. 2
[41] Avi Singh, Eric Jang, Alexander Irpan, Daniel Kappler, Mur-
taza Dalal, Sergey Levinev, Mohi Khansari, and Chelsea
Finn. Scalable multi-task imitation learning with au-
tonomous improvement. In 2020 IEEE International Confer-
ence on Robotics and Automation (ICRA) , pages 2167–2173.
IEEE, 2020. 2
[42] Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Ste-
fan Lee, Chitta Baral, and Heni Ben Amor. Language-
conditioned imitation learning for robot manipulation tasks.
Advances in Neural Information Processing Systems , 33:
13139–13150, 2020. 7[43] Alan Stuart and J Keith Ord. Kendall’s advanced theory of
statistics. vol. 1: Distribution theory. Kendall’s advanced
theory of statistics. Vol. 1: Distribution theory , 1994. 2
[44] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral
cloning from observation. In Proceedings of the 27th In-
ternational Joint Conference on Artificial Intelligence , pages
4950–4957, 2018. 6
[45] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete
representation learning. Advances in neural information pro-
cessing systems , 30, 2017. 2, 5, 6
[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 5
[47] Yuxin Wu and Kaiming He. Group normalization. In Pro-
ceedings of the European conference on computer vision
(ECCV) , pages 3–19, 2018. 6
[48] Danfei Xu, Suraj Nair, Yuke Zhu, Julian Gao, Animesh Garg,
Li Fei-Fei, and Silvio Savarese. Neural task programming:
Learning to generalize across hierarchical tasks. In 2018
IEEE International Conference on Robotics and Automation
(ICRA) , pages 3795–3802. IEEE, 2018. 2
[49] Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding
Zhao, Joshua Tenenbaum, and Chuang Gan. Prompting de-
cision transformer for few-shot policy generalization. In in-
ternational conference on machine learning , pages 24631–
24645. PMLR, 2022. 2, 8
[50] Long Yang, Zhixiong Huang, Fenghao Lei, Yucun Zhong,
Yiming Yang, Cong Fang, Shiting Wen, Binbin Zhou, and
Zhouchen Lin. Policy representation via diffusion prob-
ability model for reinforcement learning. arXiv preprint
arXiv:2305.13122 , 2023. 1
[51] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian,
Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-
world: A benchmark and evaluation for multi-task and meta
reinforcement learning. In Conference on robot learning ,
pages 1094–1100. PMLR, 2020. 2, 6
[52] Jesse Zhang, Haonan Yu, and Wei Xu. Hierarchical rein-
forcement learning by discovering intrinsic options. arXiv
preprint arXiv:2101.06521 , 2021. 3
16476
