3DSFLabelling: Boosting 3D Scene Flow Estimation by Pseudo Auto-labelling
Chaokang Jiang1, Guangming Wang2, Jiuming Liu3, Hesheng Wang3, Zhuang Ma1,
Zhenqiang Liu1, Zhujin Liang1, Yi Shan1, Dalong Du1†
1PhiGent Robotics,2University of Cambridge,3Shanghai Jiaotong University
ts20060079a31@cumt.edu.cn ,gw462@cam.ac.uk ,{liujiuming, wanghesheng }@sjtu.edu.cn ,
mazhuang097@outlook.com ,{zhenqiang.liu, zhujin.liang, yi.shan, dalong.du }@phigent.ai
jiangchaokang.github.io/3DSFLabelling-Page
Abstract
Learning 3D scene flow from LiDAR point clouds
presents significant difficulties, including poor generaliza-
tion from synthetic datasets to real scenes, scarcity of real-
world 3D labels, and poor performance on real sparse Li-
DAR point clouds. We present a novel approach from the
perspective of auto-labelling, aiming to generate a large
number of 3D scene flow pseudo labels for real-world Li-
DAR point clouds. Specifically, we employ the assumption
of rigid body motion to simulate potential object-level rigid
movements in autonomous driving scenarios. By updating
different motion attributes for multiple anchor boxes, the
rigid motion decomposition is obtained for the whole scene.
Furthermore, we developed a novel 3D scene flow data aug-
mentation method for global and local motion. By perfectly
synthesizing target point clouds based on augmented mo-
tion parameters, we easily obtain lots of 3D scene flow la-
bels in point clouds highly consistent with real scenarios.
On multiple real-world datasets including LiDAR KITTI,
nuScenes, and Argoverse, our method outperforms all pre-
vious supervised and unsupervised methods without requir-
ing manual labelling. Impressively, our method achieves
a tenfold reduction in EPE3D metric on the LiDAR KITTI
dataset, reducing it from 0.190mto a mere 0.008merror.
1. Introduction
3D scene flow estimation through deducing per-point
motion filed from consecutive frames of point clouds,
serves a critical role across various applications, encom-
passing motion prediction [33, 48], anomaly motion de-
tection [15], 3D object detection [8, 16, 50], and dy-
namic point cloud accumulation [14]. With the advanc-
ing of deep learning on point clouds [37, 38], many works
[4, 9, 18, 27, 36, 39, 51] have developed the learning-based
1†Corresponding author.
Figure 1. The proposed 3D scene flow pseudo-auto-labelling
framework. Given point clouds and initial bounding boxes, both
global and local motion parameters are iteratively optimized. Di-
verse motion patterns are augmented by randomly adjusting these
motion parameters, thereby creating a diverse and realistic set of
motion labels for the training of 3D scene flow estimation models.
methods to estimate per-point motion from 3D point clouds.
Some state-of-the-art methods [4, 39, 51] have reduced the
average 3D EndPoint Error (EPE3D) to a few centimetres
on the KITTI Scene Flow dataset (stereoKITTI) [30, 31].
However, due to the scarcity of scene flow labels, these
methods rely heavily on synthetic datasets such as Fly-
ingThings3D (FT3D) [29] for network training.
When evaluated on the stereoKITTI dataset [30, 31],
PV-RAFT [47] demonstrates an average EPE3D of just
0.056m. However, when evaluated on the Argoverse
dataset [3], the EPE3D metric astonishingly exceeds 10m
[24]. Therefore, learning 3D scene flow on synthetic
dataset [29] has a large gap with real-world application.
Jin et al. [18] recently introduce a new synthetic dataset,
GTA-SF, simulating LiDAR scans for autonomous driv-
ing. They propose a teacher-student domain adaptation
framework to reduce the gap between synthetic and real
datasets and improve some performance of 3D scene flow
estimation. However, their performance is still poor in
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
15173
(EPE3D) Unit: cm3DSFLabellingOriginal SF ModelSF Model(EPE3D) Unit: cmFigure 2. The accuracy improvement after integrating our pro-
posed pseudo-auto-labelling method. Models trained on syn-
thetic data performance poorly in 3D scene flow estimation for
LiDAR-based autonomous driving. Our proposed 3D pseudo-
auto-labelling method improves accuracy, reaching an EPE3D be-
low2cmacross datasets [2, 3, 31].
real-world LiDAR data because of ideal sensor models and
lack of scene variety. Ideally, models should learn from
real sensor data in the autonomous driving field. How-
ever, labelling each point’s 3D motion vector for the 3D
scene flow task is extremely costly. This has driven many
works [6, 22, 28, 32, 39, 44] towards unsupervised or self-
supervised learning of 3D scene flow. Although these meth-
ods have achieved reasonable accuracy, they still fall be-
hind supervised methods, highlighting the importance of
real sensor data and corresponding 3D scene flow labels.
In this work, we address three key challenges in the field
of autonomous driving: the reliance on synthetic datasets
that still have a poor generalization with real-world sce-
narios, the scarcity of scene flow labels in actual driving
scenes, and the poor performance of existing 3D scene flow
estimation networks on real LiDAR data. Inspired by the
rigid motion assumptions in RigidFlow [22] and RSF [5],
we propose a novel scene flow auto-labelling approach that
leverages the characteristics of rigid motion prevalent in au-
tonomous driving scenarios (Fig. 1). Specifically, we utilize
3D anchor boxes to segment 3D objects in point clouds. The
attributes of each object-level box are not only position and
size but also rotation, translation, motion status, and normal
vector attributes. By leveraging the constrained loss func-
tions for the box parameters and inter-frame association, we
optimize the attributes of the boxes, subsequently combin-
ing these parameters with the source point cloud to produce
a realistic target point cloud. Importantly, the generated tar-
get point cloud maintains a one-to-one correspondence with
the source point cloud, enabling the efficient generation of
pseudo 3D scene flow labels.
To capture a more diverse range of motion patterns, we
introduce a novel data augmentation strategy for 3D scene
flow auto-labelling. Utilizing the attributes of each box,
we simulate the rotations, translations, and motion statusof both the ego vehicle and surrounding environment by
adding Gaussian noise to these attributes. Consequently,
we obtain numerous 3D scene flow labels with diverse mo-
tions that closely resemble real-world scenarios, furnish-
ing the neural network with rich real training data and
significantly improving the generalization capabilities of
learning-based methods. Experimental results validate that
our pseudo-label generation strategy consistently achieves
state-of-the-art scene flow estimation results across various
models [4, 36, 51] and datasets [2, 3, 30] (Fig. 2).
In summary, our contributions are as follows:
• We propose a new framework for the automatic labelling
of 3D scene flow pseudo-labels, significantly enhancing
the accuracy of current scene flow estimation models, and
effectively addressing the scarcity of 3D flow labels in
autonomous driving.
• We propose a universal 3D box optimization method with
multiple motion attributes. Building upon this, we fur-
ther introduce a plug-and-play 3D scene flow augmenta-
tion module with global-local motions and motion status.
This allows for flexible motion adjustment of ego-motion
and dynamic environments, setting a new benchmark for
scene flow data augmentation.
• Our method achieves state-of-the-art performance on
KITTI, nuScenes, and Argoverse LiDAR datasets. Im-
pressively, our approach surpasses all supervised and un-
supervised methods without requiring any synthesising
data and manual scene flow labels.
2. Related Work
2.1. Supervised 3D Scene Flow Learning
In recent years, the performance of methods [28, 34, 42]
for 3D scene flow based on point cloud deep learning has
surpassed traditional methods. FlowNet3D [28] pioneers an
end-to-end approach to learning 3D scene flow from point
clouds. Some works, such as HALFlow [13], 3DFlow [43],
PointPWC [49], and WSAFlowNet [45], utilize PWC struc-
tures to learn 3D scene flow in a coarse-to-fine manner.
Other methods address the disorderliness of points by vox-
elizing point clouds and using sparse convolution or voxel
correlation fields to learn 3D scene flow, such as PV-RAFT
[47], DPV-RAFT [46], and SCTN [21]. Additional work
refines the estimated scene flow through iterative proce-
dures. MSBRN [4] proposes bidirectional gated recurrent
units for iteratively estimating scene flow. GMSF [51] and
PT-FlowNet [9] introduce point cloud transformers into 3D
scene flow estimation networks. These supervised learning
methods for 3D scene flow heavily rely on ground truth and
are all trained on the FT3D dataset [29] and evaluated on
stereoKITTI [30, 31] for network generalization test.
15174
2.2. Unsupervised 3D Scene Flow Learning
JGwF [32] and PointPWC [49] initially propose sev-
eral self-supervised learning losses such as cycle consis-
tency loss and chamfer loss. EgoFlow [40] distinguishes 3D
scene flow into ego-motion flow and remaining non-rigid
flow, achieving self-supervised learning based on tempo-
ral consistency. SFGAN [44] introduces generative adver-
sarial concepts into self-supervised learning for 3D scene
flow. Recently, works like R3DSF [12], RigidFlow [22],
and LiDARSceneFlow [7] greatly improve the accuracy of
3D scene flow estimation by introducing local or object-
level rigidity constraints. RigidFlow [22] explicitly en-
forces rigid alignment within super-voxel regions by de-
composing the source point cloud into multiple super-
voxels. R3DSF [12] separately considers background and
foreground object-level 3D scene flow, relying on segmen-
tation and odometry tasks [25, 26].
2.3. 3D Scene Flow Optimization
3D scene flow optimization techniques have demon-
strated remarkable generalization capabilities, attracting a
significant amount of academic research recently. Graph
prior [35] optimizes scene flow to be as smooth as possi-
ble by using the Laplacian of point clouds. Some tech-
niques introduce neural networks to optimize 3D scene
flow. NSFP [23] introduces a novel implicit regularizer, the
Neural Scene Flow Prior, which primarily depends on run-
time optimization and robust regularization. RSF [5] com-
bines global ego-motion with object-specific rigid move-
ments to optimize 3D bounding box parameters and com-
pute scene flow. FastNSF [24] also adopts neural scene flow
prior, and it shows more advantages in dealing with dense
LiDAR points compared to learning methods. SCOOP [20],
in the runtime phase, directly optimizes the flow refine-
ment module using self-supervised objectives. Although
optimization-based approaches for 3D scene flow estima-
tion have demonstrated impressive accuracy, they typically
involve high computational costs.
3. 3DSFLabelling
3D scene flow estimation infers the 3D flow, SFpred∈
R3×N1from the source point cloud PCS∈R3×N1and
the target point cloud PCT∈R3×N2for each point in
the source point. Previous self-supervised learning meth-
ods [32, 49] typically use the estimated 3D motion vector
SFpred to warp the source point cloud PCSto the target
point cloud PCSw. By comparing the difference between
PCSwandPCT, a supervisory signal is generated.
In contrast with previous self-supervised learning meth-
ods, we propose bounding box element optimization to ob-
tain the boxes and the box motion parameters from raw un-
labelled point cloud data. Then, we use object-box-levelmotion parameters and global motion parameters to warp
each box’s points and the whole point cloud to the tar-
get point cloud, generating corresponding pseudo 3D scene
flow labels. During the warping process of each object box,
we propose augmenting the motion attributes of each object
and the whole scene. This diversity assists the network in
capturing a broader range of motion behaviours.
3.1. Prerequisites
Apart from the two input point clouds, we do not require
any extra labels, such as object-level tracking and semantic
information, or vehicle ego-motion labels. To reinforce the
geometric constraints in the pseudo label generation mod-
ule, we employ Open3d [52] to generate coarse per-point
normals. Despite these normals not being perfectly ac-
curate, they are readily obtainable and can provide useful
geometric constraints. Finally, we establish initial 3D an-
chor boxes with specific centers ( x, y, z ), width w, length l,
height h, and rotation angle θ, in accordance with the range
of input points. As depicted in Fig. 3, the inputs of our
model consist of the initial anchor box set, PCS,PCT, and
point cloud normals NS.
3.2. Motion Parameter Optimization Module
As shown in Fig. 3, we present the process of sim-
ulating the motion of point clouds in actual autonomous
driving by updating four sets of parameters: differentiable
bounding boxes Φ = [ c, s, θ ], global motion parame-
tersΘ = [ Rego, tego], motion parameters for each box
[Rperbox , tperbox ], and motion probability PMfor each box.
The variables c,s, and θrepresent the center coordinates,
size, and orientation of the 3D box, respectively.
Inspired by RSF [5], we use the motion of object-level
bounding boxes to present the point-wise 3D motion and
make the step-like boxes differentiable through sigmoid ap-
proximation. By transforming the individual points to the
bounding boxes, we introduce an object-level perception of
the scene, enabling a more natural capture of rigid motion.
This method proves advantageous in autonomous driving
scenarios, where most objects predominantly exhibit rigid
behaviour [12]. Additionally, in the context of autonomous
driving, most scene motion is typically caused by the ego
motion of the vehicle. Hence, setting global motion param-
eters is necessary to simulate the global consistent rigid mo-
tion of the whole scene. To discern whether the motion of
each box is caused by ego-motion, we also set up a motion
probability for each bounding box.
With the initial set of four motion parameters, the source
point cloud is warped to the target frame, as follows:
PCΘ
T, PCΦ
T= Ω 1(Θ, PC S),Ω2(Υ(Φ , PC S)),(1)
where Θrepresents global motion parameters. Φrepresents
motion parameters of each bounding box, and Ω1andΩ2
15175
Figure 3. The proposed learning framework of pseudo 3D scene flow automatic labelling. The input comprises 3D anchor boxes, a pair
of point clouds, and their corresponding coarse normal vectors. The optimization of motion parameters primarily updates the bounding
box parameters, global motion parameters, local motion parameters, and the motion probability of the boxes. The attribute parameters for
boxes are updated through backward optimization from six objective functions. Once optimized, the motion parameters simulate various
types of motion using a global-local data augmentation module. A single source frame point cloud, along with the augmented motion
parameters, produces diverse 3D scene flow labels. These labels serve to guide the supervised neural network to learn point-wise motion.
are background and foreground warping functions, respec-
tively, generating the warped point clouds PCΘ
TandPCΦ
T.
Υsignifies the removal of boxes with too few points.
Based on the real target frame of point cloud and the
generated target point clouds PCΘ
TandPCΦ
T, we define
loss functions to update and optimize the box attributes. We
separately calculate the background and foreground losses:
LBG=κ(NΘ
T⊕PCΘ
T, NT⊕PCT)+δ(PCΘ
T, PC T),(2)
LFG=1
KboxX
PM×(κ(NΦ
T⊕PCΦ
T, NT⊕PCT)
+δ(PCΦ
T, PC T)),
(3)
where κis a function calculating nearest neighbour matches
between the transformed point cloud and the target point
cloud. δis a pairwise distance function with location en-
coding. Kboxis the number of boxes, PMis the motion
probability of each box, and the term NT⊕PCTrepresents
the concatenation of the target point cloud’s normal and po-
sitions. As for the motion probability PMof each box:
PM=σ(α×(Ω3(Φ, γi)+βi))−α×(Ω3(Φ, γi)−βi)),(4)
where σ(x)represents the sigmoid function, αis a hyper-
parameter ‘slope’ in the sigmoid, βrepresents the half size
of the vector of 3D dimensions w,l, and hof the bound-
ing box. Coordinate values γin the source point cloud arewarped to the target point cloud via motion box parame-
tersΦ. For each dynamic box, each point’s relative position
to the box’s centre is calculated. Higher motion probability
PMis assigned to the points closer to the centre. A fixed hy-
perparameter α, controlling motion probability, may not ef-
fectively respond to diverse and complex autonomous driv-
ing scenarios. Therefore, we adopt an adaptive computation
ofαbased on the variance of the point nearest-neighbour
consistency loss from the previous generation. The vari-
ance in the nearest-neighbour consistency loss for differ-
ent points in the background implies the distribution of dy-
namic objects in the scene. With fewer moving objects indi-
cated by a lower variance, αshould be adaptively reduced,
tending to produce lower motion probability PMfor points.
In addition to LBGandLFG, we introduce box dimen-
sion regularization, heading term, and angle term to con-
strain the dimensions, heading, and rotation angles of the
bounding boxes within a reasonable range [5]. We also in-
troduce a mass term to ensure that there are as many points
as possible within the box, making the estimated motion pa-
rameters of the box more robust [5].
3.3. Data Augmentation for 3D Flow Auto-labelling
Existing data augmentation practices [49] often add con-
sistent random rotations and noise offsets to the input
points, which indeed yields certain benefits. However, in
15176
Figure 4. The proposed pseudo label generation module. With
the augmented motion probability P∗
M, bounding boxes are cat-
egorized into dynamic and static types. Using global and local
motion parameters, the PCSis warped to the target point cloud
PC∗
T. Finally, pseudo 3D scene flow labels SFare derived from
the correspondence between PC∗
TandPCS.Kboxrepresents the
number of boxes.
autonomous driving scenarios, there are frequently various
complex motion patterns for multiple objects. To make
models learn complex scene motion rules, we propose a
novel data augmentation method for scene flow labelling in
both global and object-level motions. Our method simulates
a broad spectrum of 3D scene flow data variations, orig-
inating from ego-motion and dynamic object movement,
thereby providing a promising solution to the challenge of
securing abundant 3D scene flow labels.
As in Fig. 3, random noise is applied to either global or
local motion parameters respectively. We generate a ran-
dom rotation angle αand a random unit vector ufor the
rotation direction using random noise. They are used to
create the Lie algebra ξ. Subsequently, the Lie algebra ξ
is converted into a rotation matrix Musing the Rodrigues’
rotation formula and applied to the original rotation matrix
Rto obtain a new rotation matrix R∗, as follows:
M=I+ sin(|ξ|)ξ
|ξ|×+ (1−cos(|ξ|))ξ
|ξ|×2
,(5)
ξ=αu,R∗=RM. (6)
The Lie algebra element ξ, the product of scalar αand unit
vector u, signifies rotation magnitude and direction, with
αandurepresenting rotation angle and axis, respectively.
Iis identity matrix, and ξ×is the antisymmetric matrix of
ξ. Lie algebra intuitively and conveniently represents minor
SO(3)group variations. Rodrigues’ rotation formula, map-
ping from the Lie algebra to the Lie group, facilitates the
transformation of angle-based noise into a form directly ap-
plicable to the rotation matrix. This transformation brings
mathematical convenience, making the update of the rota-
tion matrix concise and efficient.
Importantly, our data augmentation targets dynamically
moving objects, because persistently adding varied motionnoise to bounding boxes perceived as static objects may dis-
rupt original data distribution. Moreover, the translation and
motion probability are also augmented. As depicted in Fig.
3, we generate noise within an appropriate range and di-
rectly add it to the translation matrix or motion probability,
resulting in augmented translation and motion probability.
3.4. Pseudo Label Generation for 3D Scene Flow
The motion parameters are fed into the pseudo label
generation module to obtain point-wise 3D scene flow la-
bels. The specific process of the label generation module is
shown in Fig. 4. We determine the motion state of the 3D
bounding box through the motion probability PM:
PC∗
T=(
PCS×R∗
ego+t∗
ego ifP∗
M<J,
PCego
S×R∗
perbox +t∗
perbox ifP∗
M≥J.(7)
PCego
Sis the points in the dynamic box from the source
point cloud, transformed through global rotation and trans-
lation. When PMis less than threshold J, the current bound-
ing box is deemed static. Conversely, if PMexceeds a pre-
defined threshold J, the current bounding box is considered
dynamic. For static boxes, based on the existing global mo-
tion, we apply a uniform noise to all static boxes to simu-
late various ego-motion patterns. By adding minute noise to
the motion probability PMfor each box, we can construct
various motion states and show a greater variety of scene
motions. Before transforming the dynamic boxes, a prior
global transformation of all points is required. For dynamic
bounding boxes, we add various noises to their existing mo-
tion, generating new rotations and translations, thereby cre-
ating various motion patterns. We warp the source point
cloud within each box to the target frame using the box’s
motion parameters, obtaining the pseudo target point cloud
PC∗
T.
The generated pseudo target point cloud PC∗
Tand the
real source frame point cloud PCShave a perfect corre-
spondence. Therefore, the 3D scene flow labels can be eas-
ily obtained by directly subtracting PCSfrom PC∗
T:
SF=PC∗
T−PCS. (8)
The generated scene flow labels capture various motion pat-
terns from real autonomous driving scenes. They help the
model understand and adjust to complex driving conditions.
This improves the model’s ability to generalize in unfamil-
iar real-world scenarios.
4. Experiments
4.1. Datasets
Test Datasets: Graph prior [35] introduces two au-
tonomous driving datasets, Argoverse scene flow [3] and
nuScenes scene flow [2] datasets. Scene flow labels in the
15177
Table 1. Comparison of our method with the best-performing methods on multiple datasets [2, 3, 10] and metrics. ’None’, ’Weak’, ’Self’,
and ’Full’ represent non-learning, weakly supervised, self-supervised, and supervised methods, respectively. “ ↑” means higher is better,
and “ ↓” means lower is better. Our method uses GMSF [51] as a baseline and combines it with our proposed pseudo-auto-labelling
framework, 3DSFlabelling. Despite the use of a supervised learning structure, no ground truth is utilized in training.
Method Sup.LiDAR KITTI Scene Flow [10] Argoverse Scene Flow [3] nuScenes Scene Flow [2]
EPE3D ↓Acc3DS ↑Acc3DR ↑Outliers ↓EPE3D ↓Acc3DS ↑Acc3DR ↑Outliers ↓EPE3D ↓Acc3DS ↑Acc3DR ↑Outliers ↓
Graph prior [35] None – – – – 0.2570 0.2524 0.4760 – 0.2890 0.2012 0.4354 –
RSF [5] None 0.0850 0.8830 0.9290 0.2390 – – – – 0.1070 0.7170 0.8620 0.3210
NSFP [23] None 0.1420 0.6880 0.8260 0.3850 0.1590 0.3843 0.6308 – 0.1751 0.3518 0.6345 0.5270
R3DSF [12] Weak 0.0940 0.7840 0.8850 0.3140 0.4160 0.3452 0.4310 0.5580 – – – –
FlowNet3D [28] Full 0.7220 0.0300 0.1220 0.9650 0.4550 0.0134 0.0612 0.7360 0.5050 0.2120 0.1081 0.6200
PointPWC [49] Full 0.3900 0.3870 0.5500 0.6530 0.4288 0.0462 0.2164 0.9199 0.7883 0.0287 0.1333 0.9410
DCA-SRSFE [18] Full 0.5900 0.1505 0.3331 0.8485 0.7957 0.0712 0.1468 0.9799 0.7042 0.0538 0.1183 0.9766
FLOT [36] Full 0.6532 0.1554 0.3130 0.8371 0.2491 0.0946 0.3126 0.8657 0.4858 0.0821 0.2669 0.8547
MSBRN [4] Full 0.0139 0.9752 0.9847 0.1433 0.8691 0.2432 0.2854 0.7597 0.6137 0.2354 0.2924 0.7638
GMSF [51] Full 0.1900 0.2962 0.5502 0.6171 7.2776 0.0036 0.0144 0.9930 9.4231 0.0034 0.0086 0.9943
Mittal et al. [32] Self 0.9773 0.0096 0.0524 0.9936 0.6520 0.0319 0.1159 0.9621 0.8422 0.0289 0.1041 0.9615
Jiang et al. [17] Self 0.4908 0.2052 0.4238 0.7286 0.2517 0.1236 0.3666 0.8114 0.4709 0.1034 0.3175 0.8191
Ours Self 0.0078 0.9924 0.9947 0.1328 0.0093 0.9780 0.9880 0.1302 0.0185 0.9534 0.9713 0.1670
Figure 5. Registration visualization results of our method (GMSF [51]+3DSFlabelling) and baselines on the LiDAR KITTI and Argoverse
datasets [3, 10]. The estimated target point cloud PCswis derived from warping the source point cloud PCSto the target point cloud via
3D scene flow. The larger the overlap between PCsw(blue) and the target point cloud PCT(green), the higher the predicted accuracy of
the scene flow. Local areas are zoomed in for better visibility. Our 3D scene flow estimation notably improves performance.
datasets are derived from LiDAR point clouds, object tra-
jectories, map data, and vehicle pose. The datasets con-
tain 212 and 310 test samples, respectively. R3DSF [12]
introduces the lidarKITTI [10], which shares 142 scenes
with stereoKITTI, collected via Velodyne’s 64-beam Li-
DAR. Unlike FT3D [29] and stereoKITTI [30, 31], the point
clouds from lidarKITTI are sparsely distributed. Note that
LiDAR scene flow ground truths contain errors. We miti-
gate this by fusing the ground truth with the first point cloud
to create a corrected second frame for network input, thusavoiding evaluation errors.
Training Datasets used in previous methods: FT3D
[29] and stereoKITTI [30, 31] are the frequently used
datasets for training previous 3D scene flow models [4, 28,
36, 49, 51]. FT3D consists of 19,640 training pairs, while
stereoKITTI [30, 31] contains 142 dense point clouds, with
the first 100 frames used for model fine-tuning in some
works [23, 32]. Some works [23, 28, 32, 35, 49] train their
models on 2,691 pairs of Argoverse [3] data and 1,513 pairs
of nuScenes [2] data, with 3D scene flow annotations fol-
15178
Table 2. The comparative results between our method and base-
line. “ ↑” signifies accuracy enhancement. In real-world LiDAR
scenarios, our method markedly improves the 3D flow estimation
accuracy across three datasets [2, 3, 30] on the three baselines.
This demonstrates that the proposed pseudo-auto-labelling frame-
work can substantially boost the accuracy of existing methods,
even without the need for ground truth.
Dataset Method EPE3D ↓ Acc3DS ↑Acc3DR ↑
FLOT [36] 0.6532 0.1554 0.3130
FLOT+3DSFlabelling 0.0189 ↑97.1% 0.9666 0.9792
MSBRN [4] 0.0139 0.9752 0.9847
MSBRN+3DSFlabelling 0.0123 ↑11.5% 0.9797 0.9868
GMSF [51] 0.1900 0.2962 0.5502LiDAR
KITTI
GMSF+3DSFlabelling 0.0078 ↑95.8% 0.9924 0.9947
FLOT [36] 0.2491 0.0946 0.3126
FLOT+3DSFlabelling 0.0107 ↑95.7% 0.9711 0.9862
Argoverse MSBRN [4] 0.8691 0.2432 0.2854
MSBRN+3DSFlabelling 0.0150 ↑98.3% 0.9482 0.9601
GMSF [51] 7.2776 0.0036 0.0144
GMSF+3DSFlabelling 0.0093 ↑99.9% 0.9780 0.9880
FLOT [36] 0.4858 0.0821 0.2669
FLOT+3DSFlabelling 0.0554 ↑88.6% 0.7601 0.8909
nuScenes MSBRN [4] 0.6137 0.2354 0.2924
MSBRN+3DSFlabelling 0.0235 ↑96.2% 0.9413 0.9604
GMSF [51] 9.4231 0.0034 0.0086
GMSF+3DSFlabelling 0.0185 ↑99.8% 0.9534 0.9713
lowing the settings of the Graph prior [35]. The R3DSF
[12] training set utilizes FT3D and semanticKITTI datasets
[1], relying on ego-motion labels and semantic segmenta-
tion labels from semanticKITTI.
Training Datasets used in our methods: Because we
do not need any labels for training data, we use raw Li-
DAR point clouds sampled from raw data. For testing on
the lidarKITTI [31], we use LiDAR point clouds from se-
quences 00 to 09 of the KITTI Odometry dataset [11] for
auto-labelling and training. For testing on the nuScenes
scene flow dataset [2], we randomly sample 50,000 pairs of
LiDAR point clouds from the 350,000 LiDAR point clouds
in the nuScenes sweeps dataset [2]. For testing on the Ar-
goverse scene flow Dataset [3], we use the LiDAR point
clouds from sequences 01 to 05 of the Argoverse 2 Sensor
Dataset [3] for auto-labelling and training. In the selection
of training data, we exclude the test scenes.
4.2. Implementation Details
The effectiveness of the proposed auto-labelling frame-
work is demonstrated using three prominent deep learn-
ing models: FLOT [36], MSBRN [4], and GMSF [51].
These models use optimal transport, coarse-to-fine strate-
gies, and transformer architectures respectively. Hyperpa-
rameters consistent with the original networks are employed
during the training process. The input point clouds, from
which ground points have been filtered, are randomly sam-
pled to incorporate 8192 points. The LiDAR point cloud
data from KITTI [10] is confined to the front view perspec-
tive, maintaining consistency with previous studies [12].Table 3. Model comparison on the Argoverse dataset [3]. ’M’
represents millions of parameters, and time is in milliseconds.
Method Sup. EPE3D ↓Acc3DS ↑Acc3DR ↑Time↓Params. ↓
PointPWC [49] Full 0.4288 0.0462 0.2164 147 ms 7.7 M
PV-RAFT [47] Full 10.745 0.0200 0.0100 169 ms –
R3DSF [12] Weak 0.4160 0.3452 0.4310 113 ms 8.0 M
FlowStep3D [19] Self 0.8450 0.0100 0.0800 729 ms –
NSFP [23] None 0.1590 0.3843 0.6308 2864 ms –
Fast-NSF [24] None 0.1180 0.6993 0.8355 124 ms –
MBNSF [41] None 0.0510 0.7936 0.9237 5000+ ms –
MSBRN+3DSFlabelling Self 0.0150 0.9482 0.9601 341 ms 3.5 M
GMSF+3DSFlabelling Self 0.0093 0.9780 0.9880 251 ms 6.0 M
FLOT+3DSFlabelling Self 0.0107 0.9711 0.9862 78 ms 0.1 M
Furthermore, we utilize four scene flow evaluation met-
rics [28, 36, 49, 51]: Average Endpoint Error (EPE3D),
ACC3DS, ACC3DR, and Outliers.
4.3. Quantitative Results
The experimental results are presented in Table 1. We
list the best-performing optimized [5, 12, 23, 35], self-
supervised [17, 32], and supervised [18, 28, 49] models
in the table. Our method achieves excellent performance
on all datasets [2, 3, 10] and metrics. Particularly, com-
pared to the baselines [51], there is an order of magnitude
reduction in EPE3D on most datasets. The proposed auto-
labelling method generates effective scene flow labels, per-
fectly simulating the rigid motion of various objects in the
real world. The designed global-local data augmentation
further expands the 3D scene flow labels. As a result, our
method significantly outperforms other methods. We have
also applied this plug-and-play auto-labelling framework
for 3D scene flow (3DSFlabelling) to three existing models,
as demonstrated in Table 2. The proposed method signifi-
cantly enhances the accuracy of 3D scene flow estimation
in these models [4, 36, 51].
Moreover, many existing works utilize a large number
of model parameters [12, 47, 49] or adopt optimization
methods [23, 24, 41] during testing for a more accurate
estimation of 3D scene flow. These methods are highly
time-consuming, and cannot ensure accuracy when reduc-
ing model parameters. Our proposed 3DSFlabelling effec-
tively addresses this challenge. In Table 3, by using the
small-parameter model FLOT (iter=1) [36] combined with
our auto-labelling framework, we surpass all current su-
pervised, unsupervised, weakly supervised, and optimized
methods. This strongly validates the effectiveness of gener-
ating real-world labels in solving the challenges.
4.4. Visualization
Fig. 5 visualizes the precision of our method and others
on two datasets [3, 31]. FLOT [36], with its mathemati-
cally optimal transport approach to matching point clouds,
exhibits superior generalization. MSBRN [4], leveraging
a multi-scale bidirectional recurrent network, robustly esti-
15179
Figure 6. Error visualizing of our method (GMSF+3DSFlabelling)
and baselines on the nuScenes dataset [2]. Using 3D EndPoint Er-
ror (EPE3D) as the metric, we categorize the error into six lev-
els. Combining GMSF [51] with our proposed 3DSFlabelling, we
manage to keep the EPE3D for most points within 0.02 meters,
clearly outperforming other methods largely.
Table 4. Generalization comparison experiment. “A”, “N”, and
“K” represent the Argoverse [3], nuScenes [2], and KITTI [10]
datasets. ‘ ⇝’ representing a model trained on the dataset on the
left and directly evaluated on another new dataset on the right.
Method Sup.A⇝N N ⇝A A ⇝K N ⇝K
EPE3DAcc3DS EPE3DAcc3DS EPE3DAcc3DS EPE3DAcc3DS
PointPWC [49] Self 0.5911 0.0844 0.7043 0.0281 0.8632 0.0119 0.9307 0.0027
RigidFlow [12] Self 0.1135 0.3445 0.3991 0.0152 0.3645 0.2118 0.5042 0.0141
MSBRN [4] Full 0.5309 0.0055 0.3761 0.0098 0.6036 0.0056 0.4926 0.0081
GMSF [51] Full 0.0334 0.9037 0.3078 0.1278 0.0442 0.8764 0.0574 0.8135
Ours Self 0.0115 0.9693 0.0264 0.9192 0.0414 0.9020 0.0208 0.9595
mates 3D scene flow on KITTI. GMSF [51] utilizes a trans-
former architecture for powerful fitting learning, but it lacks
cross-domain generalization. The proposed method consis-
tently shows better alignment between predicted and target
point clouds across all scenes. Additionally, a visualization
of the scene flow error on the nuScenes dataset is presented
in Fig. 6. In two randomly selected test scenes, our method
keeps the scene flow EPE3D mostly within 0.02 m, clearly
outperforming other baselines. More visual comparisons
will be presented in the supplementary material.
Table 4 provides quantitative results, demonstrating theTable 5. Ablation study of 3D scene flow data augmentation. “No
Aug” and “Trad. Aug” represents no data augmentation and tra-
ditional data augmentation [49], respectively. Our data augmenta-
tion method has a very positive impact on the model.
ModelData Augmentation Methods KITTI Argoverse nuScenes
No AugTrad. Aug Our Aug EPE3DACC3DS EPE3DACC3DS EPE3DACC3DS
Ours
(FLOT)✓ – – 0.0601 0.7291 0.0492 0.8015 0.7364 0.6642
– ✓ – 0.0540 0.7622 0.0430 0.8679 0.0610 0.7417
– – ✓ 0.0189 0.9666 0.0107 0.9711 0.0554 0.7601
Ours
(MSBRN)✓ – – 0.0131 0.9781 0.0180 0.9411 0.0797 0.8510
– ✓ – 0.0129 0.9790 0.0177 0.9427 0.0793 0.8547
– – ✓ 0.0123 0.9797 0.0150 0.9482 0.0235 0.9413
Ours
(GMSF)✓ – – 0.0103 0.9901 0.0139 0.9637 0.0213 0.9468
– ✓ – 0.0081 0.9918 0.0137 0.9663 0.0212 0.9473
– – ✓ 0.0078 0.9924 0.0093 0.9780 0.0185 0.9534
generalization of our 3DSFlabelling combined with the ex-
isting method (GMSF [51]) on new datasets. For instance,
we train a model on the Argoverse dataset and directly eval-
uate it on the nuScenes dataset. These two datasets belong
to different domains, posing a domain generalization prob-
lem. The results in Table 4 indicate that our framework per-
forms exceptionally well on the new dataset, consistently
achieving an EPE3D of less than 5 cm, and even reaching
an average endpoint error of less than 2 cm.
4.5. Ablation Study
This section explores the advantages of global-local data
augmentation. In Table 5, we compare existing 3D scene
flow data augmentation [49] with our proposed global-
local data augmentation method. Our augmentation strat-
egy shows significant enhancement in all evaluation met-
rics. This is attributed to the effective simulation of various
motion patterns in autonomous driving by global-local data
augmentation. The introduction of various motion transfor-
mations excellently utilizes the limited training data to ex-
tend a variety of 3D scene flow styles. More ablation studies
are referring to the supplement material.
5. Conclusion
We package 3D point clouds into boxes with different
motion attributes. By optimizing the motion parameters for
each box and warping the source point cloud into the tar-
get point cloud, we create pseudo 3D scene flow labels. We
also design a global-local data augmentation method, in-
troducing various scene motion patterns and significantly
increasing the diversity and quantity of 3D scene flow la-
bels. Tests on multiple real-world datasets show that our
3D scene flow auto-labelling significantly enhances the per-
formance of existing models. Importantly, this approach
eliminates the need for 3D scene flow estimation models to
depend on manually annotated 3D scene flow labels.
6. Acknowledgements
This work was supported by PhiGent Robotics.
15180
References
[1] Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-
zel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Se-
mantickitti: A dataset for semantic scene understanding of
lidar sequences. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 9297–9307,
2019. 7
[2] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-
ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-
modal dataset for autonomous driving. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 11621–11631, 2020. 2, 5, 6, 7, 8
[3] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jag-
jeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter
Carr, Simon Lucey, Deva Ramanan, et al. Argoverse: 3d
tracking and forecasting with rich maps. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8748–8757, 2019. 1, 2, 5, 6, 7, 8
[4] Wencan Cheng and Jong Hwan Ko. Multi-scale bidirec-
tional recurrent network with hybrid correlation for point
cloud based scene flow estimation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 10041–10050, 2023. 1, 2, 6, 7, 8
[5] David Deng and Avideh Zakhor. Rsf: Optimizing rigid scene
flow from 3d point clouds without labels. In Proceedings of
the IEEE/CVF Winter Conference on Applications of Com-
puter Vision , pages 1277–1286, 2023. 2, 3, 4, 6, 7
[6] Fangqiang Ding, Zhijun Pan, Yimin Deng, Jianning Deng,
and Chris Xiaoxuan Lu. Self-supervised scene flow estima-
tion with 4-d automotive radar. IEEE Robotics and Automa-
tion Letters , 7(3):8233–8240, 2022. 2
[7] Guanting Dong, Yueyi Zhang, Hanlin Li, Xiaoyan Sun, and
Zhiwei Xiong. Exploiting rigidity constraints for lidar scene
flow estimation. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages
12776–12785, 2022. 3
[8] Emec ¸ Erc ¸elik, Ekim Yurtsever, Mingyu Liu, Zhijie
Yang, Hanzhen Zhang, Pınar Topc ¸am, Maximilian Listl,
Yılmaz Kaan Caylı, and Alois Knoll. 3d object detec-
tion with a self-supervised lidar scene flow backbone. In
European Conference on Computer Vision , pages 247–265.
Springer, 2022. 1
[9] Jingyun Fu, Zhiyu Xiang, Chengyu Qiao, and Tingming Bai.
Pt-flownet: Scene flow estimation on point clouds with point
transformer. IEEE Robotics and Automation Letters , 8(5):
2566–2573, 2023. 1, 2
[10] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In 2012 IEEE conference on computer vision and pat-
tern recognition , pages 3354–3361. IEEE, 2012. 6, 7, 8
[11] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel
Urtasun. Vision meets robotics: The kitti dataset. The Inter-
national Journal of Robotics Research , 32(11):1231–1237,
2013. 7
[12] Zan Gojcic, Or Litany, Andreas Wieser, Leonidas J Guibas,
and Tolga Birdal. Weakly supervised learning of rigid 3dscene flow. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 5692–5703,
2021. 3, 6, 7, 8
[13] Xiuye Gu, Yijie Wang, Chongruo Wu, Yong Jae Lee, and
Panqu Wang. Hplflownet: Hierarchical permutohedral lattice
flownet for scene flow estimation on large-scale point clouds.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 3254–3263, 2019. 2
[14] Shengyu Huang, Zan Gojcic, Jiahui Huang, Andreas Wieser,
and Konrad Schindler. Dynamic 3d scene analysis by point
cloud accumulation. In European Conference on Computer
Vision , pages 674–690. Springer, 2022. 1
[15] Hafsa Iqbal, Abdulla Al-Kaff, Pablo Marin, Lucio Marce-
naro, David Martin Gomez, and Carlo Regazzoni. Detec-
tion of abnormal motion by estimating scene flows of point
clouds for autonomous driving. In 2021 IEEE International
Intelligent Transportation Systems Conference (ITSC) , pages
2788–2793. IEEE, 2021. 1
[16] Chaokang Jiang, Guangming Wang, Jinxing Wu, Yanzi
Miao, and Hesheng Wang. Ffpa-net: Efficient feature fu-
sion with projection awareness for 3d object detection. arXiv
preprint arXiv:2209.07419 , 2022. 1
[17] Chaokang Jiang, Guangming Wang, Yanzi Miao, and Hesh-
eng Wang. 3-d scene flow estimation on pseudo-lidar: Bridg-
ing the gap on estimating point motion. IEEE Transactions
on Industrial Informatics , 19(6):7346–7354, 2023. 6, 7
[18] Zhao Jin, Yinjie Lei, Naveed Akhtar, Haifeng Li, and Mu-
nawar Hayat. Deformation and correspondence aware un-
supervised synthetic-to-real scene flow estimation for point
clouds. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 7233–
7243, 2022. 1, 6, 7
[19] Yair Kittenplon, Yonina C Eldar, and Dan Raviv. Flow-
step3d: Model unrolling for self-supervised scene flow es-
timation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 4114–
4123, 2021. 7
[20] Itai Lang, Dror Aiger, Forrester Cole, Shai Avidan, and
Michael Rubinstein. Scoop: Self-supervised correspon-
dence and optimization-based scene flow. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5281–5290, 2023. 3
[21] Bing Li, Cheng Zheng, Silvio Giancola, and Bernard
Ghanem. Sctn: Sparse convolution-transformer network for
scene flow estimation. In Proceedings of the AAAI Confer-
ence on Artificial Intelligence , pages 1254–1262, 2022. 2
[22] Ruibo Li, Chi Zhang, Guosheng Lin, Zhe Wang, and Chun-
hua Shen. Rigidflow: Self-supervised scene flow learning
on point clouds by local rigidity prior. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 16959–16968, 2022. 2, 3
[23] Xueqian Li, Jhony Kaesemodel Pontes, and Simon Lucey.
Neural scene flow prior. Advances in Neural Information
Processing Systems , 34:7838–7851, 2021. 3, 6, 7
[24] Xueqian Li, Jianqiao Zheng, Francesco Ferroni, Jhony Kae-
semodel Pontes, and Simon Lucey. Fast neural scene flow.
InProceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV) , pages 9878–9890, 2023. 1, 3, 7
15181
[25] Jiuming Liu, Guangming Wang, Chaokang Jiang, Zhe Liu,
and Hesheng Wang. Translo: A window-based masked point
transformer framework for large-scale lidar odometry. In
Proceedings of the AAAI Conference on Artificial Intelli-
gence , pages 1683–1691, 2023. 3
[26] Jiuming Liu, Guangming Wang, Zhe Liu, Chaokang Jiang,
Marc Pollefeys, and Hesheng Wang. Regformer: an efficient
projection-aware transformer network for large-scale point
cloud registration. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 8451–8460,
2023. 3
[27] Jiuming Liu, Guangming Wang, Weicai Ye, Chaokang Jiang,
Jinru Han, Zhe Liu, Guofeng Zhang, Dalong Du, and Hes-
heng Wang. Difflow3d: Toward robust uncertainty-aware
scene flow estimation with diffusion model. arXiv preprint
arXiv:2311.17456 , 2023. 1
[28] Xingyu Liu, Charles R Qi, and Leonidas J Guibas.
Flownet3d: Learning scene flow in 3d point clouds. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 529–537, 2019. 2, 6, 7
[29] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,
Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A
large dataset to train convolutional networks for disparity,
optical flow, and scene flow estimation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 4040–4048, 2016. 1, 2, 6
[30] Moritz Menze, Christian Heipke, and Andreas Geiger. Joint
3d estimation of vehicles and scene flow. ISPRS annals of
the photogrammetry, remote sensing and spatial information
sciences , 2:427–434, 2015. 1, 2, 6, 7
[31] Moritz Menze, Christian Heipke, and Andreas Geiger. Ob-
ject scene flow. ISPRS Journal of Photogrammetry and Re-
mote Sensing , 140:60–76, 2018. 1, 2, 6, 7
[32] Himangi Mittal, Brian Okorn, and David Held. Just go with
the flow: Self-supervised scene flow estimation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 11177–11185, 2020. 2, 3, 6, 7
[33] Mahyar Najibi, Jingwei Ji, Yin Zhou, Charles R Qi,
Xinchen Yan, Scott Ettinger, and Dragomir Anguelov. Mo-
tion inspired unsupervised perception and prediction in au-
tonomous driving. In European Conference on Computer
Vision , pages 424–443. Springer, 2022. 1
[34] Chensheng Peng, Guangming Wang, Xian Wan Lo, Xinrui
Wu, Chenfeng Xu, Masayoshi Tomizuka, Wei Zhan, and
Hesheng Wang. Delflow: Dense efficient learning of scene
flow for large-scale point clouds. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 16901–16910, 2023. 2
[35] Jhony Kaesemodel Pontes, James Hays, and Simon Lucey.
Scene flow from point clouds with or without learning. In
2020 International Conference on 3D Vision (3DV) , pages
261–270, 2020. 3, 5, 6, 7
[36] Gilles Puy, Alexandre Boulch, and Renaud Marlet. Flot:
Scene flow on point clouds guided by optimal transport. In
ECCV 2020: 16th European Conference, Glasgow, UK, Au-
gust 23–28, 2020, Proceedings, Part XXVIII 16 , pages 527–
544, 2020. 1, 2, 6, 7[37] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classification
and segmentation. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 652–660,
2017. 1
[38] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. Advances in neural information
processing systems , 30, 2017. 1
[39] Yaqi Shen, Le Hui, Jin Xie, and Jian Yang. Self-supervised
3d scene flow estimation guided by superpoints. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 5271–5280, 2023. 1, 2
[40] Ivan Tishchenko, Sandro Lombardi, Martin R Oswald, and
Marc Pollefeys. Self-supervised learning of non-rigid resid-
ual flow and ego-motion. In 2020 international conference
on 3D vision (3DV) , pages 150–159. IEEE, 2020. 3
[41] Kavisha Vidanapathirana, Shin-Fang Chng, Xueqian Li, and
Simon Lucey. Multi-body neural scene flow. arXiv preprint
arXiv:2310.10301 , 2023. 7
[42] Guangming Wang, Xinrui Wu, Zhe Liu, and Hesheng Wang.
Hierarchical attention learning of scene flow in 3d point
clouds. IEEE Transactions on Image Processing , 30:5168–
5181, 2021. 2
[43] Guangming Wang, Yunzhe Hu, Zhe Liu, Yiyang Zhou,
Masayoshi Tomizuka, Wei Zhan, and Hesheng Wang. What
matters for 3d scene flow network. In European Conference
on Computer Vision , pages 38–55. Springer, 2022. 2
[44] Guangming Wang, Chaokang Jiang, Zehang Shen, Yanzi
Miao, and Hesheng Wang. Sfgan: Unsupervised generative
adversarial learning of 3d scene flow from the 3d scene self.
Advanced Intelligent Systems , 4(4):2100197, 2022. 2, 3
[45] Yun Wang, Cheng Chi, and Xin Yang. Exploiting im-
plicit rigidity constraints via weight-sharing aggregation for
scene flow estimation from point clouds. arXiv preprint
arXiv:2303.02454 , 2023. 2
[46] Ziyi Wang, Yi Wei, Yongming Rao, Jie Zhou, and Jiwen Lu.
3d point-voxel correlation fields for scene flow estimation.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence , 2023. 2
[47] Yi Wei, Ziyi Wang, Yongming Rao, Jiwen Lu, and Jie Zhou.
Pv-raft: Point-voxel correlation fields for scene flow esti-
mation of point clouds. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 6954–6963, 2021. 1, 2, 7
[48] Pengxiang Wu, Siheng Chen, and Dimitris N Metaxas. Mo-
tionnet: Joint perception and motion prediction for au-
tonomous driving based on bird’s eye view maps. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 11385–11395, 2020. 1
[49] Wenxuan Wu, Zhi Yuan Wang, Zhuwen Li, Wei Liu, and Li
Fuxin. Pointpwc-net: Cost volume on point clouds for (self-)
supervised scene flow estimation. In European Conference
on Computer Vision , pages 88–107, 2020. 2, 3, 4, 6, 7, 8
[50] Yi Zhang, Yuwen Ye, Zhiyu Xiang, and Jiaqi Gu. Sdp-net:
Scene flow based real-time object detection and prediction
from sequential 3d point clouds. In Proceedings of the Asian
Conference on Computer Vision , 2020. 1
15182
[51] Yushan Zhang, Johan Edstedt, Bastian Wandt, Per-
Erik Forss ´en, Maria Magnusson, and Michael Felsberg.
Gmsf: Global matching scene flow. arXiv preprint
arXiv:2305.17432 , 2023. 1, 2, 6, 7, 8
[52] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3d:
A modern library for 3d data processing. arXiv preprint
arXiv:1801.09847 , 2018. 3
15183
