Label Propagation for Zero-shot Classiﬁcation with Vision-Language Models
Vladan Stojni ´c1Yannis Kalantidis2Giorgos Tolias1
1VRG, FEE, Czech Technical University in Prague2NA VER LABS Europe
Abstract
Vision-Language Models (VLMs) have demonstrated im-
pressive performance on zero-shot classiﬁcation, i.e. classi-
ﬁcation when provided merely with a list of class names.
In this paper, we tackle the case of zero-shot classiﬁca-
tion in the presence of unlabeled data. We leverage the
graph structure of the unlabeled data and introduce ZLaP ,
a method based on label propagation (LP) that utilizes
geodesic distances for classiﬁcation. We tailor LP to graphs
containing both text and image features and further pro-
pose an efﬁcient method for performing inductive infer-
ence based on a dual solution and a sparsiﬁcation step.
We perform extensive experiments to evaluate the effec-
tiveness of our method on 14 common datasets and show
that ZLaP outperforms the latest related works. Code:
https://github.com/vladan-stojnic/ZLaP
1. Introduction
Vision-Language Models (VLMs) have demonstrated im-
pressive performance on a variety of computer vision tasks.
They are usually trained on large datasets of image-text
pairs and contain visual and textual encoders that map to
a common feature space. Visual encoders from such mod-
els have been shown to produce strong visual representa-
tions for perception tasks [ 31]. Given labeled data from a
downstream dataset, one can ﬁne-tune the model or learn
classiﬁers and achieve really high classiﬁcation accuracy.
Besides using the visual encoder in isolation, the joint
text and visual encoder feature space of VLMs enables us
to deﬁne text-based “classiﬁers”, e.g. using the class names
as textual prompts. This means that we only need a list of
class names to perform zero-shot classiﬁcation for a target
dataset, i.e. without access to any labeled images. Although
utilizing priors or devising better textual prompts can im-
prove zero-shot performance [ 8,24,29,46], here, we are
interested in the case where we further have access to unla-
beled data. Our goal is to ﬁnd the best way of utilizing such
data for zero-shot classiﬁcation.Transductive Inductive525456586062
CLIP CLIP+ ZLaP + ZLaP InMaP InMaP + ZLaP + ZLaPaccuracy
Figure 1. Zero-shot classiﬁcation performance over 14 datasets
using the proposed ZLaP classiﬁer over CLIP [ 31], as well as over
the (concurrent) InMaP [ 30] approach. Our method offers per-
formance gains for both transductive (left) and inductive (right)
inference. Average accuracy over 14 common datasets is reported.
In this paper, we leverage the inherent structure of the
unlabeled data represented by a proximity graph and apply
label propagation (LP) between the text-based classiﬁers
and unlabeled images to derive geodesic distances we then
use for classiﬁcation. We tailor LP to VLMs and graphs
containing both text and image features, and show that with-
out proper handling of the bimodality, vanilla application of
LP fails dramatically. We introduce ZLaP, a novel classiﬁ-
cation method based on label propagation that can perform
both transductive and inductive inference. We perform the
former with the standard (primal) solution of LP for clas-
siﬁcation and devise a more efﬁcient dual solution for the
latter. Our method is not only highly effective but also ef-
ﬁcient, making LP a more attractive inductive classiﬁer in
terms of complexity.
We implement our methods using publicly available
VLMs as feature encoders, primarily the ResNet and ViT
CLIP [ 31] models, and perform extensive experiments to
evaluate the effectiveness of our method on 14 common
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
23209
datasets. We show that we are able to achieve top perfor-
mance on two zero-shot inference setups, i.e. inductive and
transductive inference. Figure 1summarizes our gains over
14 datasets on both setups when applying our LP-powered
classiﬁers on top of CLIP, as well as after incorporating
the class proxies from the recent InMaP [ 30] zero-shot ap-
proach.
It is worth highlighting that ZLaP is a non-parametric
method, i.e. it does not involve a learning step. In fact, our
approach does not even require access to the VLM model
weights and can therefore be used to improve the zero-shot
performance of a black-box model even, e.g. provided only
via an API. In summary, our contributions are as follows.
• We tailor label propagation to VLMs and zero-shot clas-
siﬁcation over bi-modal graphs, proposing per modality
neighbor search and balancing of contributions.
• We propose an efﬁcient way for performing inductive in-
ference with label propagation via a dual solution and
through sparsiﬁcation. This not only improves the test-
time efﬁciency of our method but also performance.
• We complement our method with the class proxies pre-
sented in concurrent work [ 30] and achieve state of the
art results for zero-shot on 14 common datasets.
2. Related work
In this section, we discuss related works that improve the
already impressive zero-shot classiﬁcation performance of
vision-language models [ 4,16,31] even further. This is
achieved by devising better distance metrics, utilizing ex-
ternal knowledge to learn more expressive textual prompts,
or by leveraging synthetic and unlabeled data.
Improved distance metrics. Zero-shot classiﬁcation can
be improved by devising a better distance metric between
image and text representations [ 8,46]. CALIP [ 8] uses a
parameter-free attention mechanism and a local patch repre-
sentation, i.e. instead of global representations, to improve
the estimation of class-to-image similarity. CLIP-DN [ 46]
improves the test-time similarity estimation by alignment
with the similarity used during contrastive pre-training of
VLMs. To achieve this, the method assumes access to unla-
beled data from the target distribution. TPT [ 35] optimizes
textual prompts via a consistency objective across test im-
age augmentations. Our method can be considered a part of
this line of work, as label propagation is a similarity mea-
sure in a geodesic space instead of the Euclidean space.
Improved textual prompts using language models.
CLIP [ 31] uses hand-crafted prompts that are specialized
for each domain. Instead of using hand-crafted prompts,
generating them with large language models (LLMs) is
shown to be promising [ 24,29]. VisDesc [ 24] andCuPL [ 29] query LLMs to generate diverse descriptions of
all classes, while WafﬂeCLIP [ 32] operates on top of Vis-
Desc to systematically analyze which parts of the gener-
ated prompts are the most important. Instead of generating
class descriptions, CHiLS [ 26] targets diversifying the set of
classes, by generating sub-classes per class, either through
an existing class hierarchy or by querying an LLM. It then
performs zero-shot classiﬁcation using sub-classes and link-
ing them to the parent class. We show that methods improv-
ing the textual prompts are complementary to our approach.
Synthetic data. Recent methods [ 9,38,43] demonstrate
that the use of synthetic data is beneﬁcial for zero-shot
classiﬁcation. CLIP+SYN [ 9] uses a stable-diffusion-based
model to generate synthetic images using class names and
uses them to train a linear classiﬁer, initialized by the
VLM class representations. SuS-X [ 38] considers a sim-
ilar approach, but relies on a non-parametric classiﬁer.
CaFO [ 43] follows the same path, but additionally includes
text prompts generated by LLMs.
External datasets. Besides the use of synthetic data,
SuS-X proposes a variant that operates on an extensive un-
labeled image dataset (LAION-5B [ 34]). This dataset en-
compasses a distribution that is a super set of the target
one. The method generates pseudo-labels using the zero-
shot approach which are then incorporated within the non-
parametric classiﬁer. NeuralPriming [ 40] additionally as-
sumes that images have captions, which are used to improve
the pseudo-labeling.
Unlabeled images from the target distribution. An-
other line of research [ 11,12,27,30] propose operating
on unlabeled datasets from the target distribution. The
main ingredient of all these methods is the prediction of
pseudo-labels for unlabeled examples, that are later used
for further processing. UPL [ 12] optimizes learnable text
prompts based on the pseudo-labels. SVL-Adapter [ 27]
ﬁrst trains a self-supervised model on unlabeled data, and
then an adapter module to align its outputs to the pseudo-
labels. ReCLIP [ 11] performs transductive label propaga-
tion to obtain the pseudo-labels and uses them to ﬁne-tune
the VLM visual and textual encoders. In contrast to that,
we do not ﬁne-tune the model, which we may not have
access to, and efﬁciently use label propagation for induc-
tive inference too. InMaP [ 30] is a concurrent work that
uses pseudo-labels to update the class representations such
that they are now closer to image representations. We show
in the experiments that this approach is complementary to
ours. In contrast to all those methods, we do not explicitly
require pseudo-label prediction, but rather capture interac-
tions between all unlabeled examples through a proximity
graph and label propagation.
23210
3. Method
We ﬁrst deﬁne the task of zero-shot classiﬁcation with ac-
cess to unlabeled examples, present label propagation with
our contributions and then present the proposed approach
for zero-shot classiﬁcation using unlabeled examples.
3.1. Problem formulation
Vision-language models consist of an image encoder f:
I →Rdand a text encoder g:T →Rd, whereIand
Trepresent the space of images and text, respectively. We
consider the outputs of these encoders to be ℓ2-normalized.
LetCdenote a set of known classes with associated class
names{l1,...,lC}andP={p1,...,pP}a set of prompt
templates. Each prompt is combined with a class name to
produce a textual description of the class, i.e.pi(lc)for the
i-th template used for class c. Class representations wc=
1/P/summationtextP
i=1g(pi(lc))are obtained using the VLM. Then, for
a test image u, we extract representation u=f(u), and
perform zero-shot classiﬁcation by argmaxcuTwc.
We further assume access to a set UofMunlabeled im-
ages. Let {u1,...,uM}denote the representations of the
unlabeled images.
In this work, we assume no direct access to the VML
model weights, i.e. VLM training is neither possible nor
desired. This allows to consider the underlying VLM as
a black box, possibly only available through an API that
generates features. We consider two inference setups:
Inductive inference: We consider an inductive setup,
where we need to construct a classiﬁer that can operate on
new examples. This classiﬁer should take advantage of the
unlabeled examples in U.
Transductive inference: We consider Uto be the test set,
i.e. all test examples are jointly provided in advance. Pre-
diction for test example uimay therefore depend on the
representations and predictions of all other test examples.
In this transductive setup the models are not required to pro-
vide predictions for any example that is not in U.
3.2. Label propagation (LP)
Let{x1,...,xN}, withxi∈Rd, be a set of features for N
examples. Each feature represents a graph node. We con-
struct an adjacency matrix S∈RN×Nwith zero diagonal,
andsijequal tox⊤
ixjifxjis in thek-nearest neighbors
ofxi(denoted by xj∈kNN(xi)), and 0 otherwise. We
obtain a symmetric adjacency matrix by ¯S=S+S⊤, and
its symmetrically normalized version by ˆS=D−1
2¯SD−1
2,
whereD= diag(¯S1N)is the degree matrix, and 1Nis the
all-onesN-dimensional vector. We assume the ﬁrst Cex-
amples, and the corresponding nodes, to be labeled among
Cclasses; each class is assigned to a single node1.
1The theoretical part described in this section holds for the case of
more labeled examples per class too. We consider this speciﬁc case forTransductive inference. Label propagation [ 44] is origi-
nally proposed for the transductive inference setup; we need
to predict labels for the unlabeled nodes of the graph. Given
the normalized adjacency matrix ˆS, label propagation is an
iterative process given by
ˆy(t+1)
c=αˆSˆy(t)
c+(1−α)yc∀c∈ {1,...,C} (1)
until convergence. Where α∈(0,1)is a propagation
hyper-parameter, yc=ec∈ {0,1}Nis a one-hot vec-
tor with the non-zero element at index c, andtis the cur-
rent iteration. Prediction of the label for an unlabeled node
j∈ {C+1,...,N}is then given by
ˆyj= argmaxcˆyc(j), (2)
whereˆyc(j) =eT
jˆycis thej-th element of the vector ˆyc.
One can show [ 44] that this iterative solution is equivalent
to solving Clinear systems
Lˆyc=yc∀c∈ {1,...,C}, (3)
whereL=I−αˆSis the graph Laplacian. These linear
systems have a closed-form solution
ˆyc=L−1yc=Linvyc. (4)
However, this closed-form solution is not practical for large
datasets as the inverse graph Laplacian Linvis a non-sparse
RN×Nmatrix. For this reason it is usual [ 3,7,13,15] to
solve ( 3) using the conjugate-gradient (CG) method, which
is known to be faster than running the iterative solution [ 13].
Using CG is possible because Lis positive-deﬁnite.
Observe that ( 4) simply picks one of the columns of Linv.
Matrix element Linv(j,c)is the conﬁdence of example jbe-
longing to class c. Its values are similarities, after label
propagation, between each node pair. It is a type of geodesic
similarity that captures the geometry of the feature space as
this is indicated by the graph structure. Focusing on a clas-
siﬁcation task, we are only interested in similarities between
an unlabeled example and a class node.
Dual solution. Herein, we show that solving Clinear sys-
tems of the form in ( 4) to obtain predictions for all unlabeled
nodes using ( 2) is equivalent to solving N−Clinear sys-
tems of form
ˆzj=L−1ej∀j∈ {C+1,...,N}, (5)
and obtaining the unlabeled node prediction using
ˆyj= argmaxcˆzj(c). (6)
simplicity of the presentation and because it corresponds to the task of
zero-shot classiﬁcation with unlabeled examples.
23211
This comes from the fact that
ˆzj(c) =eT
cL−1ej=eT
jL−1ec=ˆyc(j). (7)
Although we present the dual solution using the closed-
form ( 4), the same holds with the CG solution of ( 3). Using
the dual solution ( 5) is not practical for transductive learn-
ing as usually the unlabeled nodes are many more than the
labeled ones. However, we show that this dual solution is
efﬁciently used for inductive inference.
As discussed, we can view Linvas a pairwise similarity
matrix. The conﬁdence of example jbelonging to class c,
due to symmetry of Linv, is equivalently obtained either by
Linv(j,c)orLinv(c,j). This constitutes, an additional inter-
pretation of the duality in the solution.
Inductive inference. Test examples now come individu-
ally and are not known during graph construction. A pos-
sible way to perform inductive inference is by adding the
new node to the graph, which is expensive for a test-time
operation as ˆSwould have to be updated for each new test
example. Instead, inspired by [ 13] that uses LP for retrieval,
we construct indicator vector yx∈RNfor test example x
such that
yx(j) =/braceleftBigg
xTxj,ifxj∈kNN (x)
0, otherwise .(8)
Then, we solve linear system
ˆzx=L−1yx, (9)
as in the dual formulation ( 5) and get a prediction with
ˆyx= argmaxcˆzx(c). With the usual formulation of label
propagation in ( 4),Clinear systems need to be solved to get
prediction for a single test example. The dual formulation
allows us to do it by solving only a single linear system.
Fast inductive inference with sparsiﬁcation. We further
introduce an additional off-line step, where we solve ( 4), get
ˆycfor allc∈ {1,...,C}, and store them in a matrix ˆY=
[ˆy1;...;ˆyc]∈RN×C. Then, the solution for a test example
is equivalent to a weighted sum of rows of ˆY[1], which is a
byproduct of using the indicator vector ( 8) for representing
a test example. Its prediction is given by ˆzx=yT
xˆY, and
is equivalent to that obtained via ( 9). However, storing the
wholeˆYcan be expensive for very large values of Nand
C. We propose to sparsify ˆYby keeping only the largest
values in each row, column, or over the whole matrix.
Note that [ 14] proposes a low-rank decomposition of the
inverse graph Laplacian for the task of retrieval. Our so-
lution is tailored to zero-shot classiﬁcation, and we choose
to obtain and sparsify the ﬁrst Crows ofLinvinstead of
approximating the whole matrix. Additionally, our solution
requires one (sparse) vector to matrix multiplications at test-
time instead of two.3.3. LP for zero­shot VLM classiﬁcation
We are given a set of classes Cwith extracted VLM
representations {w1,...,wC}and a set of unlabeled images
Uwith extracted VLM representations {u1,...,uM}. We
use them as nodes {w1,...,wC,u1,...,uM}of the graph
for label propagation. Nodes of class representations (text
nodes) are labeled and image nodes unlabeled.
To construct the adjacency matrix S, we need to perform
thek-nearest neighbor search between nodes. However, it is
known that there exists a large modality gap between image
and text representations coming from VLMs [ 21,38,47].
The respective similarity distributions for CLIP are shown
in Figure 3a. This modality gap makes standard kNN search
between nodes not useful for label propagation; image
nodes mostly get connected to image nodes, and text nodes
mostly get connected to text nodes. As a consequence, few
edges exist between labeled and unlabeled nodes.
To alleviate this problem, we perform the kNN search
separately for connecting image nodes to image nodes and
for connecting image nodes and text nodes. We do not per-
form the search using text nodes as queries, i.e. text nodes
get linked only if they appear in the kNN list of an image.
This way we also avoid linking text nodes with each other,
which is beneﬁcial as each of them is labeled to a different
class. Formally, the values of the adjacency matrix are
sij=

uT
iuj,ifuj∈kNNu(ui)
uT
iwj,ifwj∈kNNw(ui)
0, otherwise ,(10)
where kNN uand kNN wdenote that the search is per-
formed within the image or class features only, respectively.
Moreover, during inductive inference for image u, we
perform the kNN search in a similar way to construct indi-
cator vector yuwhose elements are given by
yu(i) =

uTuj,ifuj∈kNNu(u)
uTwj,ifwj∈kNNw(u)
0, otherwise .(11)
Due to the two types of edges, i.e. image-to-image and
image-to-text, we use power function h(v) =vγto trans-
form the image-to-text (cross-modal) similarities. This way
we effectively balance their contribution in the graph and
the indicator vector. To that end, we use h(uT
iwj)and
h(uTwj)in (10) and ( 11), respectively, instead of uT
iwj
anduTwj.
We refer to the proposed method described above as
Zero-shot classiﬁcation with LabelPropagation ( ZLaP ).
We further denote the variant of our method after sparsi-
fying the ˆYmatrix for inductive inference as ZLaP∗.
23212
Figure 2. t-SNE visualization for the original CLIP features (left) and our geodesic similarity (right). The former is estimated with the
features as input, while the latter with the Linvused as a pairwise similarity matrix. ⋆: class representation, •: image representation. Figure
generated for ﬁve random classes from the CUB dataset.
−0.20 0.2 0.4 0.6 0.8 1
similarityimage-to-image
text-to-text
image-to-text
(a) Using text prompts−0.20 0.2 0.4 0.6 0.8 1
similarityimage-to-image
proxy-to-proxy
image-to-proxy
(b) Using proxies from InMaP [ 30]
Figure 3. Similarity distributions among features of the same
or different modality, using 7 textual templates [ 38] (left) or the
InMaP proxies (right) as class representations.
t-SNE visualization of the bi-modal space. In Figure 2
we visualize the bi-modal feature space for CLIP features
usingt-SNE [ 39] in two cases, i.e. the Euclidean case and
using geodesic similarities obtained by Linv,i.e. after la-
bel propagation. When using Euclidean afﬁnities (left), we
see that due to the large differences in the similarity dis-
tributions (text-text, image-to-image, and text-to-image, as
shown in Figure 3a) all class representations (stars) are
clustered together far from the image nodes. However, us-
ing the geodesic afﬁnities from Linv(right) we see that class
representations are more spread.
4. Experiments
In this section, we ﬁrst present the datasets we use, our ex-
perimental setup and competing methods. We then present
component analysis for ZLaP and results for transductive
and inductive zero-shot classiﬁcation on 14 datasets.
4.1. Datasets
We evaluate the proposed method on 14 diverse im-
age classiﬁcation datasets: ImageNet ILSVRC2012 [ 33],Describable Textures Dataset (DTD) [ 5], EuroSAT [ 10],
FGVC-Aircraft [ 23], Oxford Flowers 102 [ 25], Food-
101 [ 2], Oxford-IIIT Pet [ 28], SUN397 [ 42], Stanford
Cars [ 17], Caltech101 [ 6], UCF101 [ 36], CIFAR10 [ 18],
CIFAR100 [ 18], CUB-200-2011 [ 41]. For the ﬁrst 11
datasets we borrow the train and test splits from CoOp[ 45].
We use the ofﬁcial training and test splits for CIFAR10, CI-
FAR100 and CUB-200-2011.
4.2. Experimental setup
In the transductive (inductive) inference setup, unlabeled
nodes in the graph are the test (train) images. We always
measure classiﬁcation accuracy over the test images.
VLMs and textual prompts. We report results using the
publicly available ResNet50 and ViT-B/16 CLIP [ 31] mod-
els. We adopt the 7 templates from SuS-X [ 38] as class
prompts for all results apart from Table 3where we utilize
the LLM generated prompts from [ 29].
Compared methods. Our baseline is zero-shot recogni-
tion with CLIP [31] using text encoder features as class rep-
resentations. TPT [35] is based on test-time prompt tuning
such that different image augmentations produce consistent
predictions. The aforementioned methods do not exploit
unlabeled data; their performance is therefore unchanged
in both inference setups. CLIP-DN [46] normalizes fea-
ture distributions during test-time and assumes access to the
mean feature vector of the target distribution. In the trans-
ductive (inductive) setup the mean vector is estimated on
the test (training) set. InMaP [30] is a concurrent work that
extracts updated class representations using pseudo-labels
on the unlabeled set. In the transductive (inductive) setup
the learning is performed on the test (training) images.
23213
Implementation details. We reproduce results for CLIP2,
CLIP-DN3, and InMaP4using their public implementa-
tions. For TPT [ 35] we report the numbers provided in [ 30].
We run InMaP using a single set of hyper-parameters for all
14 datasets, i.e. the default values reported in the ofﬁcial
implementation4. We also ﬁx the values of k,γ, andαfor
ZLaP across all datasets to 5, 5.0, and 0.3, respectively, for
CLIP, and 10, 3.0, and 0.3, respectively, for InMaP.
ZLaP variants. We refer to ZLaP using text class repre-
sentations as CLIP + ZLaP . Since InMaP is complemen-
tary to our work, we further evaluate the performance of
ZLaP when {w1,...,wC}are the InMaP proxies. We re-
fer to this as InMaP + ZLaP in the results. We refer to
ZLaP with a sparse ˆYfor inductive inference as ZLaP∗.
4.3. Components of ZLaP
Bi-modal graph adjustments. In Table 1we show the
importance of two design choices to adapt LP to bi-modal
graphs, i.e. separating the nearest neighbor search across
modalities using ( 10) and ( 11), and transforming cross-
modal similarities using a power function h(·). We see that
separate search is crucial; without it LP is not effective at
all. The power function gives an extra boost in both setups,
especially in the case of transductive inference. In Table 2
we report the percentage of images that are connected to
their groundtruth class nodes within a path of length n, with
and without our adjustments. We see that for any such paths
to exist for the case without adjustments, kneeds to be ex-
tremely high. With adjustments, k= 5is enough for 71.4%
of the nodes to be connected to the correct class nodes.
Sparsifying matrix ˆYfor inductive inference. We ex-
plore three ways of approximating ˆYby sparsiﬁcation, i.e.
wither keeping only the largest ξcolumns per row, the
largestξrows per column, or the largest ξelements of the
whole matrix. In all cases, the rest of the elements are
set to zero. In Figure 4we show the inﬂuence that these
three variants have on performance. Not only these vari-
ants speed-up inference, but we can also see improvements
in performance when sparsiﬁcation percentage is high, i.e.
low percentage of non-zero elements. We attribute this to
the fact that less conﬁdent predictions in ˆY, many of them
erroneous, are now set to zero. Although the best variant to
choose seems to vary per dataset, we found that keeping the
top element per row performs well across different datasets.
We therefore use the ξ= 1top element per row for our ex-
periments. This amounts to different percentages of sparsity
per dataset; we are keeping approximately 2.3%on average
2https://github.com/OpenAI/CLIP
3https://github.com/fengyuli-dev/distribution-
normalization
4https://github.com/idstcv/InMaPEq.(10)h(·) ImageNet DTD CUB
✗ ✗ 0.1 2.1 0.5
✗ ✓ 0.1 2.1 0.5
✓ ✗ 50.2 32.3 41.6
✓ ✓ 61.8 41.9 52.1
(a) Transductive inference
Eq.(10)-(11)h(·) ImageNet DTD CUB
✗ ✗ 0.1 2.1 0.5
✗ ✓ 0.1 2.1 0.5
✓ ✗ 60.8 42.4 49.6
✓ ✓ 62.2 42.8 49.7
(b) Inductive inference
Table 1. Adjusting LP to bi-modal graphs. Impact of using
separate kNN search for constructing the graph (Eq.( 10)) or the
indicator vector (Eq.( 11)), as well as power function h(·)for bal-
ancing the contributions of two types of edges in the graph.
Joint kNN search Separate kNN search
k n =1n=2n=3 n=1n=2n=3
5 0.0 0.0 0.0 71.4 85.1 100.0
10 0.0 0.0 0.0 82.9 95.4 100.0
100 40.1 100.0 100.0 100.0 100.0 100.0
Table 2. Impact of the separate kNN search on the shortest paths
between image nodes and the text node of their class. We report
the percentage of images whose shortest path to the text node of
their ground-truth class has length equal to or less than n.Left: the
vanilla approach. Right: our separate kNN search using Eq. ( 10).
Analysis on DTD for the transductive setup.
across all datasets. Regarding the inference speed-up, the
primal solution takes ∼2.6sec per image, the dual takes
∼4.4ms, while the sparsiﬁed approach takes ∼0.6ms,
measured on ImageNet dataset.
Using the class proxies from InMaP. We observe that
many class-to-image similarities ( e.g.u⊤
iwjin (10)) be-
come negative on some datasets when using ZLaP with In-
MaP proxies (see Figure 3b). We therefore perform min-
max normalization in range [0,1]after constructing adja-
cency matrix Sor the indicator vector, for the transductive
and inductive inference setups.
4.4. Results
Transductive inference. We present results for transduc-
tive zero-shot classiﬁcation in Figure 5. ZLaP improves
the zero-shot performance of CLIP signiﬁcantly on all
datasets. It also outperforms the recent TPT and CLIP-DN
approaches on the vast majority of cases, with large gains
23214
10−110010110262.262.462.662.8
non-zero elements %accuracyImageNet
row
column
matrix
10010110249.55050.55151.5
non-zero elements %accuracyCUB
10110242.54343.54444.545
non-zero elements %accuracyDTD
Figure 4. Sparcifying matrix ˆYfor inductive CLIP+ZLaP: effect of maintaining only the top elements per row/column/matrix.
CLIP-DN CLIP+ZLaPInMaP+ZLaP5658606264
57.356.660.060.461.3accuracy
(a) Transductive - ResNet50CLIP-DN CLIP+ZLaPInMaP+ZLaP5560657075
67.366.868.970.771.7accuracy
(b) Transductive - ViT-B/16
CLIP-DNCLIP +ZLaP+ZLaP* InMaP +ZLaP+ZLaP*5658606264
57.356.658.759.560.461.060.9accuracy
(c) Inductive - ResNet50CLIP-DNCLIP +ZLaP+ZLaP* InMaP +ZLaP+ZLaP*5560657075
67.366.869.169.470.771.771.6accuracy
(d) Inductive - ViT-B/16
Figure 5. Zero-shot classiﬁcation accuracy averaged over 14 datasets for the transductive (top) and inductive (bottom) setups. Results
per dataset are reported in the supplementary material.
in average accuracy. Compared to InMaP, ZLaP offers
lower accuracy on average. However, by incorporating
InMaP’s class representations to our graph, we can improve
our results even further and outperform all other methods,
for an improvement of approximately +5% over CLIP with
both backbones.
Inductive inference. We report results for the inductive
inference setup in Figure 5. ZLaP achieves a noticeable im-provements over the CLIP baseline in this setup as well.
Gains are more prominent for the case of ZLaP using the In-
MaP proxies, where gains over CLIP are +4.4% and +4.9%
for the two backbones. We also observe that, although
InMaP slightly outperforms ZLaP when used over CLIP,
the combination of the two achieves state-of-the-art perfor-
mance in this case as well. We further see that ZLaP∗re-
tains the state-of-the-art performance of our method, while
sparsifying ˆYoffers signiﬁcant speed-up at inference time.
23215
Transductive Inductive
Results with RN50
CLIP 63.0 63.0
+ZLaP 64.6 64.2
InMaP 64.8 64.6
+ZLaP 65.8 65.0
Results with ViT-B-16
CLIP 71.9 71.9
+ZLaP 72.6 73.3
InMaP 73.9 74.0
+ZLaP 74.8 74.2
Table 3. Zero-shot classiﬁcation using prompts generated by
LLMs [ 29].We report average accuracy on 12 datasets using
prompts from CuPL [ 29] together with our 7 standard prompts.
Results per dataset are reported in the supplementary material.
Leveraging LLM generated prompts. In Table 3we
report average zero-shot classiﬁcation accuracy for ZLaP
using the prompts recently proposed in CuPL [ 29]. These
are prompts generated by LMMs that are available on
the CuPL Github page5for 12 of the datasets we use
(all datasets besides CUB and Eurosat). ZLaP improves
zero-shot performance in this case as well, for both the
transductive and inductive setups. This veriﬁes that our
method is complementary to improved prompt engineering.
Multi-label classiﬁcation. We apply ZLaP for multi-
label classiﬁcation on the MS-COCO [ 22] dataset. ZLaP
improves the zero-shot performance of CLIP by +6.0%
mAP (56.8% vs. 50.8%) for inductive inference without
any modiﬁcation of the approach or its hyper-parameters.
Web-crawled unlabeled images. All previous exper-
iments use unlabeled images that come from the target
distribution, i.e. they are known to depict one of the classes
of interest but their labels are discarded. To see the impact
of ZLaP in a more realistic setup using web-crawled images
we rely on LAION-400M [ 34] composed by image-caption
pairs. We construct the set of unlabeled images with 10,000
images per class that are chosen either randomly, or based
on proximity of their image or text features to the class
representation. Random selection fails, but the other two
options provide some improvement compared to CLIP,
with the caption-based neighbors being a bit better. The
complete set of results is presented in the supplementary
material.
5https://github.com/sarahpratt/CuPLTransductive Inductive
BLIP [ 20] 54.6 54.6
+ZLaP 59.6 57.9
ALBEF [ 19] 36.0 36.0
+ZLaP 41.2 46.8
EV A-CLIP-8B [ 37] 83.6 83.6
+ZLaP 84.6 84.5
EV A-CLIP-18B [ 37] 83.9 83.9
+ZLaP 84.8 84.7
Table 4. Accuracy on ImageNet using different VLMs.
Different VLMs We use CLIP as the VLM of choice
throughout our experiments. In Table 4, we present re-
sults when ZLaP is applied on top of four recent VLMs,
namely BLIP [ 20], ALBEF [ 19], and two versions of EV A-
CLIP [ 37]. We use the implementations of BLIP and AL-
BEF that are available in the LA VIS library6, while for
EV A-CLIP we use implementation from the ofﬁcial Github
repository7. ZLaP improves the results of all four different
VLMs in both transductive and inductive setups.
5. Conclusions
Label propagation is an intuitive way of encoding the global
structure of unlabeled data into geodesic distances over a
locally Euclidean space. In this paper, we show that this
method can be successfully tailored to both transductive
and inductive zero-shot classiﬁcation with vision-language
models, and achieve state-of-the-art performance on both
setups. To that end, we show that it is highly important to
take proper care of the peculiarities of the bi-modal nature
of the task during graph construction. We further carefully
design an efﬁcient variant of label propagation for the in-
ductive inference case, that may enable label propagation to
be applied to other tasks beyond zero-shot classiﬁcation.
Vision-language models trained on billion-scale datasets
are redeﬁning computer vision research. The proposed
ZLaP is a training-free approach able to improve the gen-
eralization performance of black-box VLMs using only un-
labeled data, for an annotation-free, text-based and open-
world classiﬁcation paradigm that will inevitably be ubiq-
uitous in the near future.
Acknowledgements. This work was supported by the Ju-
nior Star GACR GM 21- 28830M and the Czech Technical
University in Prague grant No. SGS23/173/OHK3/3T/13.
We thank Ahmet Iscen for many helpful comments.
6https://github.com/salesforce/LAVIS
7https : / / github . com / baaivision / EVA / tree /
master/EVA-CLIP-18B
23216
References
[1] Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux. La-
bel propagation and quadratic criterion. In Semi-Supervised
Learning , pages 192–216. The MIT Press, 2006. 4
[2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.
Food-101 – mining discriminative components with random
forests. In ECCV , 2014. 5
[3] Siddhartha Chandra and Iasonas Kokkinos. Fast, exact and
multi-scale inference for semantic image segmentation with
deep Gaussian CRFs. In ECCV , 2016. 3
[4] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell
Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-
mann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scal-
ing laws for contrastive language-image learning. In CVPR ,
2023. 2
[5] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A.
Vedaldi. Describing textures in the wild. In CVPR , 2014.
5
[6] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener-
ative visual models from few training examples: An incre-
mental bayesian approach tested on 101 object categories. In
CVPRW , 2004. 5
[7] Leo Grady. Random walks for image segmentation. pami ,
28(11):1768–1783, 2006. 3
[8] Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzheng Ma, Xu-
peng Miao, Xuming He, and Bin Cui. CALIP: zero-shot en-
hancement of CLIP with parameter-free attention. In AAAI ,
2023. 1,2
[9] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing
Zhang, Philip H. S. Torr, Song Bai, and Xiaojuan Qi. Is syn-
thetic data from generative models ready for image recogni-
tion? In ICLR , 2023. 2
[10] Patrick Helber, Benjamin Bischke, Andreas Dengel, and
Damian Borth. Eurosat: A novel dataset and deep learning
benchmark for land use and land cover classiﬁcation. IEEE
Journal of Selected Topics in Applied Earth Observations
and Remote Sensing , 2019. 5
[11] Xuefeng Hu, Ke Zhang, Lu Xia, Albert Chen, Jiajia Luo,
Yuyin Sun, Ken Wang, Nan Qiao, Xiao Zeng, Min Sun,
Cheng-Hao Kuo, and Ram Nevatia. Reclip: Reﬁne con-
trastive language image pre-training with source free domain
adaptation. In WACV , 2024. 2
[12] Tony Huang, Jack Chu, and Fangyun Wei. Unsupervised
prompt learning for vision-language models. arXiv preprint
arXiv:2204.03649 , 2022. 2
[13] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon,
and Ond ˇrej Chum. Efﬁcient diffusion on region manifolds:
Recovering small objects with compact CNN representa-
tions. In CVPR , 2017. 3,4
[14] Ahmet Iscen, Yannis Avrithis, Giorgos Tolias, Teddy Furon,
and Ondrej Chum. Fast spectral ranking for similarity search.
InCVPR , 2018. 4
[15] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej
Chum. Label propagation for deep semi-supervised learning.
InCVPR , 2019. 3
[16] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc V . Le, Yun-Hsuan Sung, Zhen Li, and TomDuerig. Scaling up visual and vision-language representation
learning with noisy text supervision. In ICML , 2021. 2
[17] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
3d object representations for ﬁne-grained categorization. In
ICCVW , 2013. 5
[18] Alex Krizhevsky. Learning multiple layers of features from
tiny images. Technical report, 2009. 5
[19] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Gotmare,
Shaﬁq R. Joty, Caiming Xiong, and Steven Chu-Hong Hoi.
Align before fuse: Vision and language representation learn-
ing with momentum distillation. In NeurIPS , 2021. 8
[20] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.
Hoi. BLIP: bootstrapping language-image pre-training for
uniﬁed vision-language understanding and generation. In
ICML , 2022. 8
[21] Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Ye-
ung, and James Y . Zou. Mind the gap: Understanding
the modality gap in multi-modal contrastive representation
learning. In NeurIPS , 2022. 4
[22] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and
C. Lawrence Zitnick. Microsoft COCO: common objects in
context. In ECCV , 2014. 8
[23] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew B.
Blaschko, and Andrea Vedaldi. Fine-grained visual classi-
ﬁcation of aircraft. arXiv preprint arXiv:1306.5151 , 2013.
5
[24] Sachit Menon and Carl V ondrick. Visual classiﬁcation via
description from large language models. In ICLR , 2023. 1,2
[25] Maria-Elena Nilsback and Andrew Zisserman. Automated
ﬂower classiﬁcation over a large number of classes. In In-
dian Conference on Computer Vision, Graphics and Image
Processing , 2008. 5
[26] Zachary Novack, Julian J. McAuley, Zachary Chase Lipton,
and Saurabh Garg. Chils: Zero-shot image classiﬁcation
with hierarchical label sets. In ICML , 2023. 2
[27] Omiros Pantazis, Gabriel J. Brostow, Kate E. Jones, and
Oisin Mac Aodha. Svl-adapter: Self-supervised adapter for
vision-language pretrained models. In BMVC , 2022. 2
[28] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and
C. V . Jawahar. Cats and dogs. In CVPR , 2012. 5
[29] Sarah M. Pratt, Rosanne Liu, and Ali Farhadi. What does a
platypus look like? generating customized prompts for zero-
shot image classiﬁcation. In ICCV , 2023. 1,2,5,8
[30] Qi Qian, Yuanhong Xu, and Juhua Hu. Intra-modal proxy
learning for zero-shot visual categorization with clip. In
NeurIPS , 2023. 1,2,5,6
[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In ICML , 2021.
1,2,5
[32] Karsten Roth, Jae Myung Kim, A. Sophia Koepke, Oriol
Vinyals, Cordelia Schmid, and Zeynep Akata. Wafﬂing
around for performance: Visual classiﬁcation with random
words and broad concepts. In ICCV , 2023. 2
23217
[33] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large
scale visual recognition challenge. IJCV , 2015. 5
[34] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, Patrick Schramowski, Srivatsa Kundurthy, Katherine
Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia
Jitsev. LAION-5B: an open large-scale dataset for training
next generation image-text models. In NeurIPS , 2022. 2,8
[35] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom
Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-
time prompt tuning for zero-shot generalization in vision-
language models. In NeurIPS , 2022. 2,5,6
[36] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.
UCF101: A dataset of 101 human actions classes from
videos in the wild. arXiv preprint arXiv:1212.0402 , 2012.
5
[37] Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan
Zhang, Xiaosong Zhang, and Xinlong Wang. EV A-CLIP-
18B: scaling CLIP to 18 billion parameters. arXiv preprint
arXiv:2402.04252 , 2024. 8
[38] Vishaal Udandarao, Ankush Gupta, and Samuel Albanie.
Sus-x: Training-free name-only transfer of vision-language
models. In ICCV , 2023. 2,4,5
[39] Laurens Van der Maaten and Geoffrey Hinton. Visualizing
data using t-sne. JMLR , 9(11), 2008. 5
[40] Matthew Wallingford, Vivek Ramanujan, Alex Fang, Aditya
Kusupati, Roozbeh Mottaghi, Aniruddha Kembhavi, Lud-
wig Schmidt, and Ali Farhadi. Neural priming for sample-
efﬁcient adaptation. In NeurIPS , 2023. 2
[41] Peter Welinder, Steve Branson, Takeshi Mita, Catherine
Wah, Florian Schroff, Serge Belongie, and Pietro Perona.
Caltech-ucsd birds 200. 2010. 5
[42] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva,
and Antonio Torralba. SUN database: Large-scale scene
recognition from abbey to zoo. In CVPR , 2010. 5
[43] Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Han-
qiu Deng, Yu Qiao, Peng Gao, and Hongsheng Li. Prompt,
generate, then cache: Cascade of foundation models makes
strong few-shot learners. In CVPR , 2023. 2
[44] Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Ja-
son Weston, and Bernhard Sch ¨olkopf. Learning with local
and global consistency. In NIPS , 2003. 3
[45] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
Liu. Learning to prompt for vision-language models. IJCV ,
2022. 5
[46] Yifei Zhou, Juntao Ren, Fengyu Li, Ramin Zabih, and Ser-
Nam Lim. Test-time distribution normalization for con-
trastively learned vision-language models. In NeurIPS , 2023.
1,2,5
[47] Xiangyang Zhu, Renrui Zhang, Bowei He, Aojun Zhou,
Dong Wang, Bin Zhao, and Peng Gao. Not all features mat-
ter: Enhancing few-shot clip with adaptive prior reﬁnement.
InICCV , 2023. 4
23218
