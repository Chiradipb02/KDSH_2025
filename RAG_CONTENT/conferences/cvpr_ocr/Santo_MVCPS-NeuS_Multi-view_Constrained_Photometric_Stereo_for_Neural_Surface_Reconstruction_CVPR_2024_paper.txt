MVCPS-NeuS: Multi-view Constrained Photometric Stereo
for Neural Surface Reconstruction
Hiroaki Santo Fumio Okura Yasuyuki Matsushita
Graduate School of Information Science and Technology, Osaka University
{santo.hiroaki, okura, yasumat}@ist.osaka-u.ac.jp
Abstract
Multi-view photometric stereo (MVPS) recovers a high-
fidelity 3D shape of a scene by benefiting from both multi-
view stereo and photometric stereo. While photometric stereo
boosts detailed shape reconstruction, it necessitates record-
ing images under various light conditions for each viewpoint.
In particular, calibrating the light directions for each view
significantly increases the cost of acquiring images. To make
MVPS more accessible, we introduce a practical and easy-to-
implement setup, multi-view constrained photometric stereo
(MVCPS), where the light directions are unknown but con-
strained to move together with the camera. Unlike con-
ventional multi-view uncalibrated photometric stereo, our
constrained setting reduces the ambiguities of surface nor-
mal estimates from per-view linear ambiguities to a single
and global linear one, thereby simplifying the disambigua-
tion process. The proposed method integrates the ambiguous
surface normal into neural surface reconstruction (NeuS) to
simultaneously resolve the global ambiguity and estimate
the detailed 3D shape. Experiments demonstrate that our
method estimates accurate shapes under sparse viewpoints
using only a few multi-view constrained light sources.
1. Introduction
Multi-view photometric stereo (MVPS) is a 3D reconstruc-
tion approach that combines photometric stereo (PS) and
multi-view stereo (MVS). In a conventional setting, a scene
is recorded from multiple viewpoints, and at each view-
point, multiple images are captured under varying light di-
rections [ 14,19,29,40]. With the advancement of neural
surface reconstruction [ 37,41], recent works [ 14,40] have
achieved high-fidelity 3D reconstruction by incorporating
the surface normal derived from PS via inverse rendering.
To eliminate the cost of calibrating the cameras and light
directions in MVPS, it is generally favored to work in an
uncalibrated setting, where the camera can freely move and
light directions are treated unknown. For determining the
rotation stagecamera
strobe
⋯Figure 1. The minimal setting for multi-view constrained photomet-
ric stereo, equipping a camera, three strobes, and a manual rotation
stage with markers. The camera and light sources move together
with respect to the target object.
camera parameters including its postures, we can safely rely
on mature Structure-from-Motion (SfM) methods; however,
uncalibrated light directions pose a problem in determin-
ing surface details. Namely, when the light directions are
unknown, the problem becomes uncalibrated photometric
stereo (UPS), and it is known that surface normal can only
be estimated up to a linear ambiguity. As a result, in uncali-
brated MVPS, it yields an ambiguous surface normal map
for each view, where each of them has a different liner ambi-
guity. Due to the per-view linear ambiguity in the estimated
surface normal maps, it remains challenging to fully take
advantage of PS’s capability in uncalibrated MVPS.
In this paper, we consider a setting of MVPS, where light
directions are unknown but constrained to move together
with a camera, which we call multi-view constrained photo-
metric stereo (MVCPS). Such a setup can be easily achieved
by employing a rig that secures a camera and light sources,
or by using a rotation stage to move a target object in front
of a camera and light sources, as shown in Fig. 1. In fact, the
setting is hardly new, and most existing single-view or multi-
view PS works employ such setups [ 19,22,25,27,31,33–
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
20475
35, 45] because of the ease of implementation.
We show that, in MVCPS with unknown light directions,
the ambiguity of surface normal can be reduced from per-
view ones to a single and global one. By jointly factorizing
the multi-view & multi-light observations, our method de-
rives multi-view surface normal maps that have a unique and
common linear ambiguity. Leveraging the surface normal
maps with reduced ambiguity, we develop a neural surface
reconstruction method that jointly resolves the ambiguity,
thereby achieving high-fidelity 3D shape recovery. The pro-
posed method also introduces a confidence estimation for
surface normal based on the reduced ambiguity, enhancing
robustness against outliers like shadows.
Our experimental results demonstrate that the proposed
method yields more accurate estimations under minimal
light directions and sparse views, e.g., three light sources
and four viewpoints, compared to state-of-the-art multi-view
uncalibrated photometric stereo methods. To sum up, this
paper provides the following three contributions:
•We show that our MVCPS reduces the per-view ambigui-
ties into a single and global ambiguity, which allows for
better disambiguation and shape recovery.
•We develop an outlier detection method based on the
global ambiguity for robust estimation.
• The proposed method achieves detailed shape reconstruc-
tion by integrating the ambiguous surface normal into
neural surface reconstruction.
2. Related Work
In this section, we first review previous works of single-
and multi-view PS. We then describe recent works of neural
surface representations.
Single-view Photometric Stereo Conventional PS [ 36,
38] started with the Lambertian assumption [ 17] and known
light directions. The assumption of known light directions
has been later relaxed to deal with uncalibrated settings,
where light directions are treated unknown.
UPS simultaneously estimates scene shape and light direc-
tions and eliminates the necessity of light source calibration,
while there exists a linear ambiguity. Early works [ 1,9]
employ the Lambertian assumption and factorize the input
observations into surface normals and light directions. To
resolve the ambiguity, the uses of known surface albedo [ 9],
shadows [16], and specularity [4, 5] have been explored.
Recent works [ 2] use a learning-based approach to re-
solve the ambiguity by data prior. As discussed in [ 3], the
learning-based method also implicitly uses shadows and
specularity to disambiguate the estimation. More recently,
inverse rendering-based approaches [ 11,18,20] have been
proposed. While they achieve accuracy comparable to that
of calibrated settings in scenes with dense light sources, they
still face challenges when the light sources are sparse.Multi-view Photometric Stereo (MVPS) Early works of
MVPS start with estimating a base shape from multi-view
observations, such as a mesh [ 29], SDF volume [ 22], and
sparse point cloud [ 19], and then refine the shape using PS
to achieve detailed shape recovery. Kaya et al. [13] propose
an integration of recent deep learning-based MVS and PS.
Subsequently, their method has been extended [ 12,14] to
account for the uncertainties in the depth and normal estima-
tions. While these methods achieve good 3D reconstruction,
they assume dense observations, i.e., the number of view-
points and light sources is large enough to obtain accurate
estimations from both MVS and PS.
Along with the recent advances in neural surface recon-
struction, Yang et al. [40] propose PS-NeRF, which incor-
porates normal maps estimated by PS into neural surface
reconstruction. They estimate per-view normal maps in ad-
vance and use them to optimize a neural surface. They use
the state-of-the-art learning-based UPS method [ 2] for nor-
mal map estimation for each view. However, when only a
limited number of light sources are available, UPS fails to
accurately estimate the normal maps, resulting in a collapsed
shape estimation.
More recently, MVPSNet [ 44] proposes a feature extrac-
tor to leverage shading information observed under varying
lights, thereby facilitating improved stereo matching. They
also use the UPS method [ 2] to estimate the light directions.
The proposed method shares the spirit of PS-NeRF. How-
ever, to overcome the challenges posed by limited viewpoints
and light sources, we propose to simultaneously optimize
shapes and disambiguate surface normal in neural surface
reconstruction instead of involving per-view disambiguation
of the surface normal.
Neural Surface Reconstruction Neural implicit surfaces
have achieved remarkable advancements in novel view syn-
thesis [ 26] and shape recovery [ 28,37,41,42]. While these
methods achieve high-quality shape reconstruction, they re-
quire a large number of images to accurately optimize the
neural surface.Scenes with sparse viewpoints have been chal-
lenging, and several works tackle this problem. Long et
al. [23] propose a pre-trained encoder that estimates a coarse
shape from sparse inputs. In the direction of using prior
from a pre-trained model, Wu et al. [39] introduce the use of
multi-view consistency, which has been used in conventional
MVS [ 7]. Yu et al. [ 43] propose the more explicit use of a
pre-trained prior, i.e., a monocular depth and normal estima-
tion method. They estimate the per-view depth and normal
maps from a single RGB image by [6] for supervision.
Using prior knowledge from a large amount of data is
effective in cases where the training data covers test scenes.
In contrast, the proposed method uses additional observa-
tions under a few different light sources and demonstrates
superior performance in scenes with sparse viewpoints.
20476
3. Proposed Method
Our method observes a target object from vview-
points under llight directions and obtains observa-
tions{M1,···,Mv} ⊂Rp×l, where prepresents the num-
ber of pixels. We assume that cameras’ intrinsics and extrin-
sics are known by SfM or markers placed in the scene. We
further assume that foreground masks {s1,···,sv} ⊂Rp
are available. The proposed method jointly decomposes the
observations {Mi}into per-view surface normal maps and
shared light directions w.r.t. the camera. Subsequently, a
neural surface is optimized with the supervision of color ob-
servations, foreground masks, and the decomposed surface
normals. In this section, we describe these details.
3.1. Multi-view Constrained Photometric Stereo
(MVCPS)
We begin with a naive extension of factorization-based UPS
for multi-view observations. We then show that, by assuming
the camera and lights move together, we can jointly factor-
ize the multi-view observations to reduce the ambiguity of
surface normal maps.
Singular Value Decomposition-based multi-view UPS
Hayakawa [ 9] propose a UPS method that decomposes sin-
gle view observations under varying lights using singular
value decomposition (SVD). It can be simply extended to
the multi-view context by solving UPS for each view. The
observation at i-th view, Mi∈Rp×l, can be decomposed
into left- and right-singular vectors Ui∈Rp×p,Vi∈Rl×l
and singular values Σi∈Rp×las:
Mi=UiΣiV⊤
i.
U⊤
iUi=Ip,V⊤
iVi=Il, the diagonal elements of Σiare
singular values, and Inis ann×nidentity matrix. With the
Lambertian assumption, it becomes rankMi= 3forl≥3
under different and non-coplanar light directions, the obser-
vations Mcan be written as a product of low-dimensional
matrices as
Mi=U′
iΣ′
i|{z}
ˆNiV′⊤
i|{z}
ˆL⊤
i, (1)
where U′
i∈Rp×3andV′
i∈Rl×3are first three singular
vectors of UiandVi, respectively, and Σ′
iis a3×3di-
agonal matrix containing singular values. The decomposed
ˆNi∈Rp×3andˆLi∈Rl×3denote the estimates of the sur-
face normals and light directions for i-th view, respectively.
The solution is known to contain a linear ambigu-
ityXi∈R3×3(where det(Xi)̸= 0) for each view, which
means any invertible 3×3matrix Xican be inserted as
Mi=
ˆNiXi
X−1
iˆL⊤
i
=ˆN′
iˆL′
i (2)
𝑴ଶ
𝑴ଵ
⋯⋯
⋯#light#view
normal
𝑵෡ଵ
𝑵෡ଶlight
𝑳෠
𝑿𝑿ିଵambiguity
ൈ ൈ
view-independent
⋯Figure 2. Decomposition of multi-view observations by HO-GSVD
to obtain different combinations of surface normals and light
directions. As such, a straightforward application of UPS
to the multi-view setting suffers from per-view ambigui-
ties{Xi}.
Our MVCPS via Higher-Order Generalized SVD Dif-
ferent from the general uncalibrated case described above,
our MVCPS setting assumes that the camera and lights move
together. In this setting, we can assume unknown but consis-
tent light directions ˆLiacross all views with respect to the
camera as
∀i,ˆLi=ˆL. (3)
With the knowledge of consistent light directions ˆL,
we cast the problem of multi-view UPS to matrix fac-
torization based on higher-order generalized SVD (HO-
GSVD) [ 15,30]. With the HO-GSVD, we can decompose
an arbitrary number of observations {Mi}into{Ui},{Σi},
and common V∈Rl×las
Mi=UiΣiV⊤. (4)
Following Eq. (1), we obtain estimates of surface normal
{ˆNi}and a consistent light direction ˆL:
Mi=ˆNiXX−1ˆL⊤, (5)
where ˆNi=U′
iΣ′
iandˆL=V′∈Rl×3is the first three
singular vectors of V. The matrix X∈R3×3(det(X)̸= 0)
is the global linear ambiguity. Compared to the per-view
linear ambiguities {Xi}that appeared in Eq. (2), with our
method, we can reduce the ambiguity to a global one X
shared by all the views (Fig. 2). While the decomposition of
observations {Mi}into{ˆNi}andˆLcan also be achieved
by applying SVD to the concatenated observation matri-
ces
M⊤
1,M⊤
2,···⊤, HO-GSVD offers a more accurate
low-rank approximation (Eq. (5)) when more than three
light sources are available. As we will see in the next sec-
tion, the proposed method resolves the ambiguity Xthrough
neural surface reconstruction using the supervision by the
decomposed per-view surface normals {ˆNi}.
20477
𝑿
ൈ
volume rendering decomposed 
normaldisambiguated
normal
loss
SDF𝑵෡௜ 𝑵෡௜𝑿Figure 3. Optimization of SDF by decomposed surface normal
3.2. Neural Surface Reconstruction for MVCPS
Once we have per-view surface normals {ˆNi}with ambi-
guityX, we fuse them to a neural surface reconstruction
method by extending NeuS [ 37] to recover the detailed 3D
shape and resolve the ambiguity Xsimultaneously (Fig. 3).
Following the work of NeuS [ 37], we represent a scene
surface Sby a zero-level set of a neural signed distance
function (SDF) fθ:R3→Ras
S={x∈R3|fθ(x) = 0},
where xis a 3D point, and θis a learnable parameter. Our
MVCPS provides the surface normal maps {ˆNi}with an
unknown global ambiguity represented as X. We will call
{ˆNiX}as disambiguated surface normal maps hereafter.
Normal consistency loss The gradient of the SDF,
∇fθ(x)at a point x, represents the surface normal of the
point. By applying volume rendering along a camera ray,
p(t) =o+tv(t >0), where o∈R3andv∈S2represent
a camera position and viewing direction, respectively, we
can compute the surface normal n∗from the SDF as
n∗(o,v) =Z∞
0w(t)∇fθ(p(t))dt.
The weight w(t)for a point on the ray is computed from the
volume density σ(t)as
w(t) =T(t)σ(t),
where T(t) = exp
−Rt
oσ(u)du
is an accumulated trans-
mittance. We employ the normalized S-density [ 37] as the
volume density σ(t)computed from the SDF fθ. We also
follow the uniform and near-surface sampling strategy in
[37] for computing the integration.
To optimize the ambiguity matrix X∈R3×3through
backpropagation, we introduce a normal consistency loss
between the SDF surface normal n∗and the disambiguated
surface normal {ˆNiX}:
Lnormal =X
(o,v)∈χ||τc(n∗(o,v))−ˆn(o,v)||1,(6)
where τcdenotes the transformation of a surface normal from
the world coordinates to camera coordinates, and ˆn(o,v)isthe surface normal derived from the corresponding pixel of
disambiguated surface normals {ˆNiX}.
Although the normal loss Lnormal does not resolve the
ambiguities by itself, the SDF surface normals n∗(o,v)are
also constrained by photo and mask consistency losses in
the case of NeuS, which constrains the ambiguity matrix X.
Concurrently, the SDF is refined using the disambiguated
surface normals, which contain detailed shape information
derived from MVCPS.
While photo-consistency loss is only applicable when the
same scene point is observed from multiple viewpoints, the
normal loss can constrain the surface orientation even when
the scene point is observed only from a single view, which
enables the recovery of shape from sparse observations.
Other loss functions We follow the original NeuS imple-
mentation for the remaining losses, considering color, mask,
and regularization. We here briefly recap them.
To optimize SDF fθby color observations, we follow the
volume rendering of a radiance field cϕ:R3×S2→R3
+
proposed by NeRF [ 26] and render the color C∗∈R3
+as
C∗(o,v) =Z∞
0w(t)cϕ(p(t),v)dt.
The color loss is computed between rendered and ob-
served intensities as
Lcolor=X
(o,v)∈χ||C∗(o,v)−C(o,v)||1,
where χis the set of sampled rays and C(o,v)is the
observed intensity of the corresponding pixel in the in-
put images. We employ the L1 loss for robust estimation.
For the observed intensities C, we use the median image
of input images captured under different light directions,
as following [ 19], or images captured under natural illu-
mination when available. We also employ the mask loss
and Eikonal loss [ 8] as described in [ 37]. The mask loss
Lmask = BCE ( s∗(o,v), s(o,v))is defined as the binary
cross entropy between the rendered mask s∗(o,v)and input
mask s(o,v). The Eikonal loss regularizes the norm of the
gradients of the SDF fθas
Lreg=X
x∈ψ(||∇fθ(x)||2−1)2, (7)
where ψis the set of sampled points on the rays.
Training The SDF fθand radiance field cϕare represented
by multilayer perceptrons (MLPs). We optimize the param-
eters of MLPs, θandϕ, and the ambiguity matrix Xusing
the following overall loss function:
L=Lcolor+λmaskLmask+λnormalLnormal +λregLreg,
(8)
20478
where λmask,λnormal , and λregare weights for the losses. To
achieve better convergence in the simultaneous optimization
of the SDF parameters and ambiguity matrix X, we use a
coarse-to-fine optimization strategy with positional encod-
ing [21], which gradually enables high-frequency encoding
as the training progresses.
3.3. Confidence Estimation
The proposed factorization method in MVCPS assumes a
Lambertian surface without shadows. However, in the real
world, outliers like shadows and specularity cannot be ne-
glected, as they lead to estimation errors. Particularly with
a limited number of light sources, e.g., only three, accurate
estimation becomes difficult even using robust optimization
algorithms or recent learning-based methods. In the proposed
method, inspired by the fact that the ambiguity matrix Xis
shared across all views, we introduce a confidence estima-
tion during training to migrate adverse effects arising from
inaccurate estimations.
Let us represent the loss function Las a function of the
ambiguity matrix Xand the view index iwhere rays are
sampled, denoted as L(X, i). We consider two different
views, uandv, and assess the changes in the loss at u-th
view by applying the gradient of the loss at v-th view. The
changes ∆Lis written as
∆Lu,v=L(X, u)− L
X−α∂L(X, v)
∂X, u
,(9)
where αis the step size of the updates. When assuming that
the gradient at the v-th view,∂L(X,v)
∂X, improves the ambigu-
ity matrix X, it is expected that the loss at u-th view should
improve, given that the ambiguity matrix Xis shared across
all views. Nonetheless, if the decomposed surface normal is
incorrect at the u-th view, for example, due to shadows, the
ambiguity matrix Xbecomes irrelevant for such pixels. As
a result, the losses at these pixels may increase or decrease,
regardless of the improvements in X.
Building on this premise, we propose a variance-based
confidence estimation method. We compute the changes of
the loss ∆Lu,vfor each pixel over different training steps
and construct the variance maps {q1,···,qv} ⊂Rp
+. Al-
though it is not guaranteed that the gradient at the v-th view
is always accurate, from a statistical standpoint, the am-
biguity matrix Xis expected to improve as training pro-
gresses. Therefore, by statistically analyzing the changes in
loss across different training steps, we expect the surface
normals of pixels exhibiting lower variance to have higher
confidence. From the variance maps {qi}, we compute the
confidence {¯qi} ∈[0,1]pby applying exponential mapping,
¯qi= exp
−κqi
max(qi)
, (10)BLOBBY BUNNY
Figure 4. Our synthetic dataset
rotation stageobjectcameraLEDsFigure 5. Setup of our real-world
experiment
BEAR BUDDHA RABBIT CUBE FROG
Figure 6. Scenes for our real-world experiments
where κis a hyperparameter to define the confidence for
a pixel with maximum variance, i.e.,qi
max(qi)= 1. In our
implementation, the parameter κis set to 5. The normal loss
Lnormal is weighted by the confidence of each pixel.
4. Experiments
We evaluate the proposed method on our synthetic dataset,
a public MVPS real-world dataset, DiLiGenT-MV , and our
real-world dataset. In the following sections, we detail the
experimental settings and present the evaluation results.
Comparison methods We compare the proposed method
with NeuS [ 37], PS-NeRF [ 40], and MonoSDF [ 43]. NeuS
and MonoSDF are MVS methods based on neural surface re-
construction, taking RGB images as inputs. As RGB images,
we use median images of images captured with varying light
directions. PS-NeRF is a state-of-the-art multi-view UPS
that optimizes the neural surface using RGB images and per-
view normal maps, which are estimated from single-view
images captured under multiple unknown lighting directions.
For a fair comparison, we adopt the same neural surface
representation as proposed by NeuS across all methods but
use the respective loss functions. More specifically, when
training the comparison methods, we replace the normal loss
of Eq. (6) by:
Lnormal =X
(o,v)∈χ||τc(n∗(o,v))−ˆn′(o,v)||1,(11)
where surface normal ˆn′is computed in the individual com-
parison methods. For PS-NeRF, the input surface normal ˆn′
is computed using a learning-based UPS [ 3], which is the
extended version of [ 2] used in the original PS-NeRF. For
MonoSDF, we use a monocular normal and depth estimation
20479
method [ 6] to obtain per-view normal and depth maps from a
single image. We input the estimated normal maps as ˆn′and
also incorporate a scale- and shift-invariant loss, proposed
in MonoSDF, between the estimated depth and the rendered
depth of the SDF. The weight of the depth loss is set to be
the same as that of the normal loss.
Implementation details We implement the proposed
method based on the official implementation of NeuS1. We
train the model for 100K iterations with batch size 1024 .
The training takes about 10hours on a NVIDIA A6000 GPU.
We use Adam optimizer with default parameters. We use
different learning rates for updating parameters of MLPs,
θandϕand the ambiguity matrix X, set to 5×10−4and
1×10−3, respectively.
We set the weights of losses as λmask = 0.1 and
λreg= 0.1. The weight of the normal loss λnormal is initially
set to 0.03 and linearly increased to 0.1over the first 50K
iterations. The coarse-to-fine optimization via positional en-
coding is also adapted from 0to50K iterations. To compute
the confidence map, we store the change of loss, Eq. (9),
every 500iterations with downscaled resolution. The update
stepαin Eq. (9)is set to the learning rate for the ambigu-
ity matrix. For the first 20K iterations, we do not use the
confidence weighting to ensure stable computation of the
variance. To avoid collapsed estimation, we fix the weight of
the normal loss λnormal for PS-NeRF and MonoSDF to 0.03.
Evaluation metrics For evaluation, we extract a mesh
from the optimized SDF using the Marching cubes algo-
rithm [ 24]. Invisible surfaces from any cameras are excluded
using a rasterizer-based renderer [ 32], and we compute the
Chamfer distance between the point clouds extracted from
the estimated and the ground truth meshes. We additionally
evaluate the angular error between the rendered normal maps
of the optimized SDF and the ground truth.
4.1. Synthetic Experiments
We render two scenes for our evaluation, BLOBBY [ 10] and
BUNNY2, using a rendering software, Blender3. We use 24
viewpoints, rotating the camera at equal intervals around
the object, and for each viewpoint, render 3images under
different directional lights, which move together with the
camera. As shown in Fig. 4, we use a textureless surface for
BLOBBY and a wood texture4for BUNNY .
1NeuS implementation, https://github.com/Totoro97/NeuS/ ,
last accessed on March 25, 2024.
2Stanford Bunny, https://graphics.stanford.edu/data/
3Dscanrep/, last accessed on March 25, 2024.
3Blender 3.3, https://www.blender.org/ , last accessed on March
25, 2024.
4Poly Haven, https://polyhaven.com/ , last accessed on March 25,
2024.As discussed in [ 3], specularity is useful for accurate
estimation of UPS; hence, we include specularities in the
input images for PS-NeRF and MonoSDF. Conversely, the
proposed method and NeuS assume the Lambertian surface,
and thus, we render them as diffuse surfaces.
We use four views located at 90-degree intervals and three
light directions. Figure 7and Table 1present the estimated
normal maps and estimation errors, respectively. Here, we
visualize only one view per scene, and more complete results
can be found in our supplementary material. Due to sparse
viewpoints, NeuS fails to reconstruct correct surfaces. In
contrast, MonoSDF, which uses the same inputs as NeuS, can
accurately recover the rough shapes. However, MonoSDF
tends to produce over-smoothed surfaces, as observed in the
BUNNY scene. PS-NeRF provides a reasonable estimation,
but due to the lower accuracy of surface normal estimation
by UPS, the accuracy of its optimized shapes is not as high
as that of the proposed method. In terms of both the Chamfer
distances and mean angular errors, we observe the accurate
estimation achieved by the proposed method.
4.2. Real-world Experiments
We use two datasets for real-world evaluation, DiLiGenT-
MV [ 19] and our own dataset. We first describe the datasets
and then introduce the experimental results.
DiLiGenT-MV: DiLiGenT-MV captures five objects from
20viewpoints and uses fixed 96light directions. Since the
proposed method assumes the Lambertian reflectance, we
here show the results for two scenes with relatively diffuse
materials, BEAR and BUDDHA shown in Fig. 6. The re-
maining scenes are shown in our supplementary material.
Our dataset: For recording real-world data, we use a po-
larimetric camera (FLIR BFS-U3-51S5PC-C) for obtaining
the diffuse-only and diffuse+specular observations for com-
parison methods. For just running our method, in practice,
we can simply attach an off-the-shelf polarization filter to an
ordinary camera, as shown in Fig. 1.
We capture three objects: RABBIT, CUBE, and FROG,
shown in Fig. 6. RABBIT and FROG are made of pottery,
while CUBE is made of plastic. We use our capturing setup,
shown in Fig. 5, to capture images from 60viewpoints with
3light directions, rotating the target object. The ground truth
is obtained by a laser scanner and aligned with the estimated
meshes by NeuS, using all of the 60views.
Results: Figures 7and8visualize the estimated normal
maps and meshes, respectively, and Table 1represents the es-
timation errors. Similar to our synthetic experiments, NeuS
presents larger errors for most of the scenes, and PS-NeRF
faces challenges in recovering the normal map from lim-
ited observations. Although our dataset (RABBIT, CUBE,
and FROG) is captured using cross-polarization to reduce
specular reflections, BEAR and BUDDHA contain specu-
20480
Table 1. Comparison results with four views. PS-NeRF and ours use three light sources. “CD” denotes the Chamfer distance ( ↓) between the
mesh extracted from the estimated SDF and the ground truth. “MAE” denotes mean angular errors ( ↓) in degrees across all available views.
We show the MAE of rendered normal maps of the estimated SDF and estimated normal maps fed to the optimization. The estimated normal
maps by the proposed method are disambiguated by the estimated ambiguity matrix X. Bold font and underline are used to denote the best
and second-best results, respectively.
NeuS MonoSDF PS-NeRF Ours
CD
MAE (SDF) CD
MAE (SDF) MAE (Est.) CD
MAE (SDF) MAE (Est.) CD
MAE (SDF) MAE (Est.)
BLOBBY 24.3 14.0 27.3
8.40 30.9 25.3
7.05 16.0 8.83
2.28 7.01
BUNNY 16.3
21.9 7.61
11.5 20.6 5.38 8.53 17.1 1.79
5.38 12.4
BEAR 10.2
10.4 1.7 5.81 13.0 6.32
7.02 18.1 1.30
5.11 12.9
BUDDHA 26.6
30.1 12.2 23.1
26.2 13.4
22.5 29.8 2.18
14.5 19.3
RABBIT 2.17 13.1 2.20
14.2 26.2 3.81
13.4 29.8 1.65
9.67 15.2
CUBE 95.2
34.6 0.90 9.64 14.1 9.50
14.7 32.6 0.85
8.34 12.6
FROG 40.1
33.7 20.2
23.6 26.2 13.5 19.3 25.6 2.01
14.8 18.2
NeuS MonoSDF PS-NeRF Ours
Ground truth Normal (SDF) Normal (SDF) Normal (Est.) Normal (SDF) Normal (Est.) Normal (SDF) Normal (Est.)BLOBBY
11.5
9.7 29.4 5.1 11.0 1.9 5.5B
UNNY
24.5
9.7 10.6 8.1 15.7 6.8 14.2BEAR
13.4
5.5 11.4 10.1 15.6 5.0 12.1B
UDDHA
33.8
25.1 26.1 18.9 35.7 13.9 15.3RABBIT
14.2
15.3 23.9 16.0 28.8 11.5 17.6CUBE
38.7
9.3 13.7 8.6 27.1 7.8 13.3FR
OG
high
low45◦
0
35.1
23.6 27.1 18.8 21.5 14.9 19.1
Figure 7. Evaluation of the normal maps. For each scene and method, we present the rendered normal map of the SDF, the estimated normal
map fed to the optimization, and corresponding error maps side-by-side. The numbers under the error maps represent mean angular errors in
degrees. The estimated normal maps by the proposed method are disambiguated by the estimated ambiguity matrix X.
lar reflections. Nevertheless, the proposed method robustly
estimates accurate shapes.
In scenes such as BEAR and CUBE, the monocular nor-
mal estimation in MonoSDF performs well, leading to ac-
curate estimations. However, for example, in the CUBEscene, the detailed shape is lacking. In contrast, the proposed
method can recover detailed shapes in the CUBE scene.
BUDDHA and FROG present challenges for all methods;
however, the proposed method achieves globally accurate
shape recovery.
20481
Table 2. Comparison of the proposed method with and without HO-GSVD. “MAE (UPS)” and “MAE (SDF)” denote the mean angular
errors ( ↓) in degrees for the disambiguated normal maps and rendered normal maps of SDF, respectively. “Proj. mat.” represents the error
between the estimated ambiguity matrix and the ground truth measured by the Frobenius norm (↓). Bold font indicates better results.
4
views 8
views 20
views
MAE
(UPS) Proj. mat. MAE (SDF) MAE
(UPS) Proj. mat. MAE (SDF) MAE
(UPS) Proj. mat. MAE (SDF)
Ours 0.13
0.0023 1.78 0.085
0.0010 1.32 0.054
0.00055 1.29
w/o HO-GSVD 0.62
0.0050 3.00 0.59
0.0037 2.01 0.421
0.0026 1.77
GT NeuS MonoSDF PS-NeRF OursBEAR
 B
UDDHA
 RABBIT
 CUBE
 FR
OG
Figure
8. Estimated meshes for the real-world scenes.
GT Est. Err. map Conf. map
0°45°
01
Ours w/o Conf.
Figure 9. Results of confidence estimation. The left-hand side shows
the estimated meshes for the BUNNY scene, both ours and ours
without confidence estimation (“w/o Conf.”). On the right-hand
side, for each row, the first two columns present the normal maps of
the ground truth and the one estimated, projected by the estimated
ambiguity matrix. The last two columns show the error maps (“Err.
map”) of the estimated normal and confidence maps (“Conf. map”).
5. Discussion
This paper presents a practical and easy-to-implement 3D
reconstruction method, MVCPS. Our constrained setting al-
lows us to decompose the observations into per-view surfacenormal maps and shared light directions w.r.t. the camera by
HO-GSVD, reducing the per-view ambiguities in UPS to a
single and global linear ambiguity. We demonstrate that, by
integrating the decomposed normal maps into neural surface
reconstruction, the proposed method can jointly estimate
accurate 3D shapes and the ambiguity matrix. We compare
the proposed method with the state-of-the-art methods and
show the proposed method’s effectiveness in the challenging
setting of sparse views and lights.
One of the limitations of our method is the Lambertian
assumption, which is rarely met in the real world. Dealing
with non-Lambertian objects under sparse viewpoints and
light sources is one of our future venues.
Ablation study on HO-GSVD To assess the impact of HO-
GSVD compared to SVD-based UPS, we evaluate the accu-
racy of disambiguation in the proposed optimization stage.
We assume that the factorization perfectly works and use
the normal maps of the ground truth as input. We use the
BUNNY scene with 4,8, and 20views. Table 2compares
the disambiguated accuracy of the surface normal using a
shared linear ambiguity across all views (HO-GSVD), with
those using per-view linear ambiguities (SVD). Since we can
only disambiguate the surface normals of the views used in
the optimization when using view-independent ambiguities,
we compare the mean angular errors for those specific views
in this experiment. This result demonstrates the consistent
advantage of HO-GSVD for more accurate ambiguity resolu-
tion. We further investigate the effectiveness of HO-GSVD
in improving factorization accuracy, with details provided in
our supplementary material.
Ablation study on confidence estimation Figure 9shows
the results of the proposed method with and without confi-
dence estimation. We also visualize the disambiguated nor-
mal maps, the corresponding error maps, and the estimated
confidence maps. As seen in the error maps and the confi-
dence maps, pixels with higher errors tend to have lower
confidence, as expected. In the BEAR scene, both shad-
owed and specular pixels exhibit lower confidence, which
contributes to robust estimation. Observing the estimated
mesh without confidence estimation reveals that the shape is
heavily affected by the shadow on the ear.
Acknowledgement
This work was supported by JSPS KAKENHI Grant Num-
bers JP22K17910 and JP23H05491.
20482
References
[1]P.N. Belhumeur, D.J. Kriegman, and A.L. Yuille. The bas-
relief ambiguity. In Computer Vision and Pattern Recognition
(CVPR), 1997. 2
[2]Guanying Chen, Kai Han, Boxin Shi, Yasuyuki Matsushita,
and Kwan-Yee K. Wong. SDPS-Net: Self-calibrating deep
photometric stereo networks. In Computer Vision and Pattern
Recognition (CVPR), 2019. 2,5
[3]Guanying Chen, Michael Waechter, Boxin Shi, Kwan-Yee K.
Wong, and Yasuyuki Matsushita. What is learned in deep
uncalibrated photometric stereo? In European Conference on
Computer Vision (ECCV), 2020. 2,5,6
[4]Ondrej Drbohlav and Mike Chaniler. Can two specular pixels
calibrate photometric stereo? In International Conference on
Computer Vision (ICCV), 2005. 2
[5]Ondˇrej Drbohlav and Radim ˇS´ara. Specularities reduce ambi-
guity of uncalibrated photometric stereo. In European Con-
ference on Computer Vision (ECCV), 2002. 2
[6]Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir
Zamir. Omnidata: A scalable pipeline for making multi-task
mid-level vision datasets from 3D scans. In International
Conference on Computer Vision (ICCV), 2021. 2,6
[7]Yasutaka Furukawa and Jean Ponce. Accurate, dense, and
robust multiview stereopsis. Transactions on Pattern Analysis
and Machine Intelligence (PAMI), 32(8):1362–1376, 2009. 2
[8]Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and
Yaron Lipman. Implicit geometric regularization for learning
shapes. In International Conference on Machine Learning
(ICML), 2020. 4
[9]Hideki Hayakawa. Photometric stereo under a light source
with arbitrary motion. Journal of the Optical Society of Amer-
ica (JOSA), 11(11):3079–3089, 1994. 2,3
[10] Micah K. Johnson and Edward H. Adelson. Shape estima-
tion in natural illumination. In Computer Vision and Pattern
Recognition (CVPR), 2011. 6
[11] Berk Kaya, Suryansh Kumar, Carlos Oliveira, Vittorio Ferrari,
and Luc Van Gool. Uncalibrated neural inverse rendering for
photometric stereo of general surfaces. In Computer Vision
and Pattern Recognition (CVPR), 2021. 2
[12] Berk Kaya, Suryansh Kumar, Carlos Oliveira, Vittorio Ferrari,
and Luc Van Gool. Uncertainty-aware deep multi-view pho-
tometric stereo. In Computer Vision and Pattern Recognition
(CVPR), 2022. 2
[13] Berk Kaya, Suryansh Kumar, Francesco Sarno, Vittorio Fer-
rari, and Luc Van Gool. Neural radiance fields approach to
deep multi-view photometric stereo. In IEEE Winter Con-
ference on Applications of Computer Vision (WACV), 2022.
2
[14] Berk Kaya, Suryansh Kumar, Carlos Oliveira, Vittorio Ferrari,
and Luc Van Gool. Multi-view photometric stereo revisited.
InIEEE Winter Conference on Applications of Computer
Vision (WACV), 2023. 1,2
[15] Idris Kempf, Paul J. Goulart, and Stephen R. Duncan. A
higher-order generalized singular value decomposition for
rank-deficient matrices. SIAM Journal on Matrix Analysis
and Applications, 44(3):1047–1072, 2023. 3[16] David J. Kriegman and Peter N. Belhumeur. What shadows
reveal about object structure. Journal of the Optical Society
of America (JOSA), 18(8):1804–1813, 2001. 2
[17] JH Lambert. Photometria. Augustae Vindelicorum, 1760. 2
[18] Junxuan Li and Hongdong Li. Self-calibrating photometric
stereo by neural inverse rendering. In European Conference
on Computer Vision (ECCV), 2022. 2
[19] Min Li, Zhenglong Zhou, Zhe Wu, Boxin Shi, Changyu Diao,
and Ping Tan. Multi-view photometric stereo: A robust so-
lution and benchmark dataset for spatially varying isotropic
materials. IEEE Transactions on Image Processing, 29:4159–
4173, 2020. 1,2,4,6
[20] Zongrui Li, Qian Zheng, Boxin Shi, Gang Pan, and Xudong
Jiang. Dani-net: Uncalibrated photometric stereo by differ-
entiable shadow handling, anisotropic reflectance modeling,
and neural inverse rendering. In Computer Vision and Pattern
Recognition (CVPR), 2023. 2
[21] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon
Lucey. BARF: Bundle-adjusting neural radiance fields. In
International Conference on Computer Vision (ICCV), 2021.
5
[22] F. Logothetis, R. Mecca, and R. Cipolla. A differential volu-
metric approach to multi-view photometric stereo. In Inter-
national Conference on Computer Vision (ICCV), 2019. 1,
2
[23] Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and
Wenping Wang. SparseNeuS: Fast generalizable neural sur-
face reconstruction from sparse views. In European Confer-
ence on Computer Vision (ECCV), 2022. 2
[24] William E. Lorensen and Harvey E. Cline. Marching cubes:
A high resolution 3D surface construction algorithm. In
SIGGRAPH, New York, NY , USA, 1987. Association for
Computing Machinery. 6
[25] Lilika Makabe, Heng Guo, Hiroaki Santo, Fumio Okura, and
Yasuyuki Matsushita. Near-light photometric stereo with
symmetric lights. In IEEE International Conference on Com-
putational Photography (ICCP), 2023. 1
[26] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:
Representing scenes as neural radiance fields for view syn-
thesis. In European Conference on Computer Vision (ECCV),
2020. 2,4
[27] Kazuma Minami, Hiroaki Santo, Fumio Okura, and Ya-
suyuki Matsushita. Symmetric-light photometric stereo. In
IEEE Winter Conference on Applications of Computer Vision
(WACV), 2022. 1
[28] Michael Oechsle, Songyou Peng, and Andreas Geiger.
UNISURF: Unifying neural implicit surfaces and radiance
fields for multi-view reconstruction. In International Confer-
ence on Computer Vision (ICCV), 2021. 2
[29] Jaesik Park, Sudipta N Sinha, Yasuyuki Matsushita, Yu-Wing
Tai, and In So Kweon. Multiview photometric stereo using
planar mesh parameterization. In International Conference
on Computer Vision (ICCV), 2013. 1,2
[30] Sri Ponnapalli, Michael Saunders, Charles Loan, and Orly
Alter. A higher-order generalized singular value decomposi-
tion for comparison of global mrna expression from multiple
organisms. PloS one, 6:e28072, 2011. 3
20483
[31] Yvain Qu ´eau, Bastien Durix, Tao Wu, Daniel Cremers,
Franc ¸ois Lauze, and Jean-Denis Durou. LED-based pho-
tometric stereo: Modeling, calibration and numerical solution.
Journal of Mathematical Imaging and Vision, 60(3):313–340,
2018. 1
[32] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Tay-
lor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia
Gkioxari. Accelerating 3D deep learning with PyTorch3D.
arXiv:2007.08501, 2020. 6
[33] Hiroaki Santo, Michael Waechter, and Yasuyuki Matsushita.
Deep near-light photometric stereo for spatially varying re-
flectances. In European Conference on Computer Vision
(ECCV), 2020. 1
[34] Hiroaki Santo, Masaki Samejima, Yusuke Sugano, Boxin Shi,
and Yasuyuki Matsushita. Deep photometric stereo networks
for determining surface normal and reflectances. Transactions
on Pattern Analysis and Machine Intelligence (PAMI), 44(1):
114–128, 2022.
[35] Boxin Shi, Zhipeng Mo, Zhe Wu, Dinglong Duan, Sai-Kit
Yeung, and Ping Tan. A benchmark dataset and evaluation for
non-Lambertian and uncalibrated photometric stereo. Trans-
actions on Pattern Analysis and Machine Intelligence (PAMI),
41(2):271–284, 2019. 2
[36] William M. Silver. Determining shape and reflectance using
multiple images. Master’s thesis, Massachusetts Institute of
Technology, 1980. 2
[37] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. NeuS: Learning neural implicit
surfaces by volume rendering for multi-view reconstruction.
InNeural Information Processing Systems (NeurIPS), 2021.
1,2,4,5
[38] Robert J. Woodham. Photometric method for determining sur-
face orientation from multiple images. Optical engineering,
19(1):139–144, 1980. 2
[39] Haoyu Wu, Alexandros Graikos, and Dimitris Samaras. S-
V olSDF: Sparse multi-view stereo regularization of neural
implicit surfaces. In International Conference on Computer
Vision (ICCV), 2023. 2
[40] Wenqi Yang, Guanying Chen, Chaofeng Chen, Zhenfang
Chen, and Kwan-Yee K. Wong. PS-NeRF: Neural inverse
rendering for multi-view photometric stereo. In European
Conference on Computer Vision (ECCV), 2022. 1,2,5
[41] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan
Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural
surface reconstruction by disentangling geometry and appear-
ance. In Neural Information Processing Systems (NeurIPS),
2020. 1,2
[42] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. V ol-
ume rendering of neural implicit surfaces. In Neural Informa-
tion Processing Systems (NeurIPS), 2021. 2
[43] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sat-
tler, and Andreas Geiger. MonoSDF: Exploring monocular
geometric cues for neural implicit surface reconstruction. In
Neural Information Processing Systems (NeurIPS), 2022. 2,
5
[44] Dongxu Zhao, Daniel Lichy, Pierre-Nicolas Perrin, Jan-
Michael Frahm, and Soumyadip Sengupta. MVPSNet: Fastgeneralizable multi-view photometric stereo. In International
Conference on Computer Vision (ICCV), 2023. 2
[45] Zhenglong Zhou and Ping Tan. Ring-light photometric stereo.
InEuropean Conference on Computer Vision (ECCV), 2010.
2
20484
