Single Mesh Diffusion Models with Field Latents for Texture Generation
Thomas W. Mitchel1,2†Carlos Esteves1Ameesh Makadia1
1Google Research2PlayStation
Input Generated textures Generative transfer to new geometry
Figure 1. Our latent diffusion models operate directly on the surfaces of 3D shapes, synthesizing new high-quality textures (center) after
training on a single example (left). Both our novel latent representation and diffusion models are isometry-equivariant, facilitating a notion
of generative texture transfer by sampling pre-trained models on new geometries (right). Above our models are conditioned on coarse
semantic labels re ﬂecting a subjective distribution of content, which delineate the sole and interior of the shoes, the eyes and mouths of the
skulls, and the decals on the bottles.
Abstract
We introduce a framework for intrinsic latent diffusion
models operating directly on the surfaces of 3D shapes,
with the goal of synthesizing high-quality textures. Our ap-
proach is underpinned by two contributions: Field Latents,
a latent representation encoding textures as discrete vec-
torﬁelds on the mesh vertices, and Field Latent Diffusion
Models, which learn to denoise a diffusion process in the
learned latent space on the surface. We consider a single-
textured-mesh paradigm, where our models are trained to
generate variations of a given texture on a mesh. We show
the synthesized textures are of superior ﬁdelity compared
those from existing single-textured-mesh generative models.
Our models can also be adapted for user-controlled editing
tasks such as inpainting and label-guided generation. The
efﬁcacy of our approach is due in part to the equivariance
†Work done while at Google Research.of our proposed framework under isometries, allowing our
models to seamlessly reproduce details across locally sim-
ilar regions and opening the door to a notion of genera-
tive texture transfer. Code and visualizations are available
athttps://single-mesh-diffusion.github.
io/.
1. Introduction
The emergence of latent diffusion models ( LDMs ) [44] as
powerful tools for 2D content creation has motivated efforts
to replicate their success across different modalities. A par-
ticularly attractive direction is the synthesis of textured 3D
assets, due to the high cost of building photorealistic ob-
jects which can require intricately con ﬁgured scanning so-
lutions [ 13] or expertise in 3D modeling.
The majority of 3D synthesis methods employing LDMs
operate in an image-based setting where the 3D repre-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
7953
sentation is optimized through differentiably rendered im-
ages, often in conjunction with score distillation sampling
(SDS) [3,4,32,41–43,52,53]. This design allows the
models to leverage the full capabilities of pre-trained 2D
LDMs. A second, alternative method is to rasterize both
geometry and texture onto a 3D grid which are compressed
into triplane latent features over which the DM operates
[58]. However, in applications where texture synthesis takes
precedence over 3D geometry, both approaches sacri ﬁceﬁ-
delity in favor of a workable representation — models re-
lying on planar renderings suffer from view inconsistencies
when mapping back to the 3D shape while rasterization in-
herently aliases ﬁne textural details.
Here, we observe that the discretization of a surface as
a triangle mesh is itself a tractable and effective represen-
tational space. To this end, we present an original frame-
work for intrinsic LDMs operating directly on the surfaces
of shapes, with the goal of generating high-quality textures.
Our method consists of two distinct novel components,
serving as the main contributions of our work. The ﬁrst
is a latent representation called Field Latents (FLs). Tex-
tures are mapped to tangent vector features at the vertices
which are responsible for characterizing the local texture.
The choice of tangent vector features over scalars allows
for the capture of directional information related to the lo-
cal texture which we show enables superior quality recon-
structions. More generally, FLs offer an effective form of
perceptual compression in which a high-resolution texture,
analogous to a continuous signal de ﬁned over a surface, is
mapped to a collection of discrete vector ﬁelds taking values
at the vertices of a lower-resolution mesh. The second com-
ponent is a Field Latent Diffusion Model (FLDM ), which
learns a denoising diffusion process in ﬁeld latent space.
Our FLDMs are built upon ﬁeld convolutions [ 35], surface
convolution operators designed speci ﬁcally to process tan-
gent vector features.
Although we outline a general framework for construct-
ing diffusion models for surfaces, inspired by the success of
single-image generative models [ 28,38,46,54], we tailor
and deploy our FLDM architecture in the single-textured-
mesh setting. We address the problem of generating varia-
tions of a given texture on a mesh (Figure 1, left and center),
as well as tasks supporting user control such as label-guided
generation and inpainting. The single-textured-mesh set-
ting also lets us circumvent the data scarcity issue. Large-
scale 3D datasets with high-quality textures have only re-
cently become available [ 8,9], with only a fraction of sam-
ples across disparate categories possessing complex non-
uniform textures. Furthermore, many textured 3D assets
that are handcrafted or derived from real-world scans are ge-
ometrically or stylistically unique, upon which large-scale
models can be challenging to condition.
Experiments reveal our models enable ﬂexible synthe-sis of high-resolution textures of qualitatively superior ﬁ-
delity compared to existing single-textured-asset diffusion
models, while simultaneously sidestepping the challenges
of using 2D-to-3D approaches. Furthermore, both FLs and
FLDMs are isometry-equivariant — each commute with
distance-preserving shape deformations. As a result, we
ﬁnd that our approach can be used for generative texture
transfer , wherein a FLDM trained on a single textured mesh
can be sampled on a second similar mesh to texture it in the
style of the ﬁrst (Figure 1, right).
2. Related Work
Existing generative models that synthesize textured 3D as-
sets typically map content to an internal 2D representa-
tion which is computationally preferable to working directly
over 3D Euclidean space. Perhaps the most popular ap-
proach involves iteratively rendering objects from differ-
ent viewpoints, enabling the use of pre-trained 2D LDMs
(e.g.Stable Diffusion [ 44]) either indirectly as optimization
priors [ 30,32,41,42] or to directly apply a full denois-
ing process [ 3,4,43,53]. The former case, pioneered by
DreamFusion [ 41], uses score distillation sampling (SDS),
in which a neural radiance ﬁeld is optimized such that the
rendered images appear to be reasonable samples from a
pre-trained LDM; in the latter, LDMs with depth condi-
tioning are used to iteratively denoise the rendered images
which are aggregatively projected to the texture map. How-
ever, SDS-based methods are computationally feasible only
with low-resolution LDMs and small NeRFs, and are unable
to synthesize ﬁne details. Additionally, approaches that di-
rectly denoise and project must contend with artifacts aris-
ing from view inconsistencies and synthesized textures may
contain unnatural tones or lighting effects as residuals from
their image-trained LDM backbone.
Alternative representational spaces to drive textured 3D
synthesis have also been explored. Texture Fields [ 17,39]
encode textures as 3D neural ﬁelds and triplane feature
maps have emerged recently as an ef ﬁcient domain for 3D
geometry [ 56,58,63]. In particular, Sin3DM [ 58] encodes
geometry and texture into an implicit triplane latent repre-
sentation, then trains and samples a 2D DM on the resulting
feature maps. However, geometry and texture must ﬁrst be
rasterized to a 3D grid before triplane convolutions can be
employed, which can lead to aliasing of high- ﬁdelity tex-
tures and geometric details due to memory constraints on
the maximal grid resolution. To disentangle geometry from
texture, several methods map textures to domains such as
the image plane [ 6,40,51,60] or sphere [ 7]. Point-UV
diffusion [ 61] trains and samples 2D DMs directly in the
UV-atlas, though this approach suffers on meshes with dis-
connected or fragmented UV mappings.
Similar to our approach, a third class of models encode
textures as features at the simplices of a mesh. GAN-based
7954
methods Texturify [ 48] and Mesh2Tex [ 2] compress tex-
tures to attributes at the mesh facets, and differentiable ren-
dering enables adversarial supervision in the image space.
Alternatively, Intrinsic Neural Fields ( INFs ) [27] intro-
duces a latent representation wherein textures are encoded
as eigenfunctions of the Laplacian taking values at the mesh
vertices. However, features at points on faces are recov-
ered through barycentric interpolation, which we will show
leads to a breakdown in reconstruction ﬁdelity at higher
compression ratios. Recently, Manifold Diffusion Fields
[14,64] introduce the ﬁrst fully intrinsic DMs de ﬁned over
2D Riemannian manifolds, and are based on a global atten-
tion mechanism which aggregates features at the vertices,
limiting their applicability to coarse, low-resolution meshes
due to complexity constraints. Here, our proposed FLDMs
are also fully intrinsic but are built upon locally-supported
convolutions de ﬁned over the manifold [ 35], allowing them
to scale to higher-resolution meshes.
Our single-textured-mesh FLDMs are inspired by recent
work in which DMs are trained to capture internal patch
distributions from a single image with the goal of generating
diverse samples with similar visual content [ 28,38,54]; In
turn, these models can trace their lineage to the in ﬂuential
SinGAN architecture [ 46] and its derivatives [ 19,21]. Here,
we mirror the approach proposed by SinDiffusion [ 54] and
denoise with a shallow convolutional UNet to control the
receptive ﬁeld and prevent the DM from over ﬁtting to the
texture. Similar to our setting, Sin3DM [ 58] was the ﬁrst to
train LDMs in a single-textured-mesh paradigm, however
they take an extrinsic approach to generating geometry and
texture, whereas our approach focuses only on synthesizing
textures which we show qualitatively are of higher- ﬁdelity
than those produced by Sin3DM.
3. Method Overview
We introduce a framework for intrinsic LDMs operating di-
rectly on surfaces. The ﬁrst component is a latent repre-
sentation of mesh textures comprised of tangent vector fea-
tures. At each vertex, the tangent features characterize the
texture of a local neighborhood on the surface, and the col-
lection of these features constitute a stack of vector ﬁelds
we call Field Latents (FLs). The FL space is learned with
a locally-supported variational autoencoder [ 25] (FL-V AE )
which is described in Section 4.
The second key component is the Field Latent Diffusion
Model (FLDM ), which learns to denoise a diffusion pro-
cess in the tangent space of a surface. Denoising networks
are constructed with ﬁeld convolutions ( FCs), surface con-
volution operators acting on tangent vector ﬁelds [ 35]. We
extend FCs to interleave scalar embeddings with the tangent
vector features, enabling the injection of diffusion time-
steps and optional conditioning, such as user-speci ﬁed la-
bels, into the denoising model.In practice, FL-V AEs and FLDMs are applied to synthe-
size new textures from a single textured triangle mesh. We
pre-train a single FL-V AE by superimposing planar meshes
over a large high-quality image dataset, obtaining a general-
purpose latent space which can be used to train FLDMs on
arbitrary textured meshes. To do so, a texture is ﬁrst mapped
to distributions in the tangent space at the vertices with the
FL-V AE encoder, and an FLDM is trained to iteratively de-
noise tangent vector latent features. Afterwards, samples
from the FLDM can be decoded with the FL-V AE decoder
to synthesize new textures.
Importance of Isometry-Equivariance Our FL-V AEs
and FLDMs are designed to be equivariant under isome-
tries, i.e. they commute with distance-preserving shape
deformations. Unlike images, points on a surface have
no canonical orientation. The choice of local coordinates
is ambiguous up to an arbitrary rotation, making it im-
possible to simply adapt standard image-based V AEs and
diffusion models to surfaces. However, isometries mani-
fest locally as rotations. Thus, designing our framework
to be isometry-equivariant inherently solves the orientation
ambiguity problem, enabling consistent, repeatable results.
Isometry-equivariance also results in tangible bene ﬁts —
our models are able to seamlessly reproduce textural details
across locally similar areas of meshes. Furthermore, while
existing models transfer textures with pointwise maps [ 11]
or generatively via conditioning on a learned token [ 5,43],
we can simply sample our pre-trained FLDMs on new, sim-
ilar meshes to texture them in the learned style.
4. Field Latents
As in Knoppel et al. [26] we associate tangent vectors with
complex numbers. For a surface Mand point p∈M, we
assign to the tangent space an arbitrary orthonormal basis
such that for any v∈TpMwe havev≡reiθ,withr=|v|
andθthe direction of vexpressed in the frame. We make
this convention explicit by denoting Cp≡TpM.
To describe the FL-V AE, we require a notion of multi-
dimensional Gaussian noise in the tangent bundle of a sur-
faceM. Speci ﬁcally, we denote the Gaussian distribution
overdcopies of the tangent bundle TMdbyT NM(0,Id).
Samplesϵ∼T NM(0,Id)are vector ﬁelds inTMdsuch
that at each point p∈M, the coef ﬁcients of ϵ(p)∈Cd
p
expressed in the orthonormal basis are themselves samples
from thed−dimensional standard normal distribution
ϵ(p) =ϵ1+iϵ2,ϵ1,ϵ2∼N(0,Id). (1)
In this work we are concerned with isometries γ:M→
N, which enjoy the property that their push-forwards dγ:
TM→TN, which de ﬁne a smooth map between tan-
gent spaces, manifest pointwise as rotations. From Equa-
7955
tion ( 1) it is easy to see that T NM(0,Id)is symmetric un-
der such transformations. Thus, it follows that sampling
fromT NN(0,Id)is equivalent to pushing forward sam-
ples from T NM(0,Id),
ϵ′=γϵ forϵ′∼T NN(0,Id)
ϵ∼T NM(0,Id)(2)
withγϵ= [dγ·ϵ]◦γ−1denoting the standard action of
diffeomorphisms on vector ﬁelds via left-shifts.
FL-V AE Encoder The encoder Eis a network that, for
any point p∈M, takesn−dimensional scalar functions
ψ∈L2(M,Rn)(e.g. texture RGB values with n= 3)
restricted to the surrounding geodesic neighborhood Bp⊂
Mto the parameters of dindependent normal distributions
in the tangent space at p:
Ep:L2(Bp,Rn)→Cd
p×Rd
≥0
ψ7→(µψ
p,σψ
p)(3)
Here, the means of the distributions µψ
pare themselves tan-
gent vectors while the standard deviations σψ
parescalars .
Latent codes characterizing the local restriction of ψare
collections of tangent vectors zψ
p∈Cd
pdrawn from the
multivariate normal distribution parameterized by (µψ
p,σψ
p).
Using the reparameterization trick [ 25] latent codes can be
generated as zψ
p=µψ
p+σψ
p⊙ϵ(p)withϵ∼T NM(0,Id).
FL-V AE Decoder The principal advantage in a tangent
vector latent representation is that it naturally admits a de-
scriptive coordinate function which can be queried to asso-
ciate features with neighboring points. The decoder Dis
designed as a neural ﬁeld operating over the local parame-
terization of the surface induced by the logarithm map about
a pointp,logp:Bp→Cp, withlogpqencoding the posi-
tion ofq∈Bpin the tangent space of p. Formally,
Dp: logp(Bp)×Cd
p→Rn
�
logpq,zψ
p
7→bψp(q),(4)
withbψpdenoting the prediction of ψmade from the per-
spective of p. Decoding follows a similar process as pro-
posed in [ 18]. Speci ﬁcally, given a tuple�
logpq,zψ
p
, the
decoder constructs two features. First, an invariant scalar
feature is obtained by vectorizing the upper-triangular part
of the Hermitian outer product of the latent code with it-
self, vec j≥i
zψ
p
zψ
p∗
∈Cd(d+1)
2. Second, a positionally-
aware scalar feature cψ
pq∈Cdis formed by the natural co-
ordinate function
cψ
pq≡logpq·zψ
p, (5)which corresponds to storing the inner product and determi-
nant of each tangent vector with the position of qrelative to
pgiven by logpq. The neural ﬁeld receives the concatena-
tion of these two features and returns the prediction bψp(q).
Equivariance Recall we are concerned with constructing
a latent representation equivariant under isometries — that
for anyψ∈L2(M,Rn)and isometry γ:M→Nwe have
d[γψ]γ(p)=γbψp, (6)
whereγψ=ψ◦γ−1denotes the standard action of diffeo-
morphisms by left-shifts. Thus, it follows from Equation ( 2)
that a suf ﬁcient condition for Equation ( 6) to hold is if for
allψ∈L2(M,Rn), isometries γ:M→N, andp∈M,
the encoded mean µψ
pand standard deviation σψ
psatisfy
dγ|p·µψ
p=µγψ
γ(p)andσψ
p=σγψ
γ(p), (7)
withdγ|pdenoting the push-forward of γin the tangent
space atp, which rotates the local coordinate system. A
detailed proof appears in the supplement, sec. 8.1.
Architecture In practice, we discretize a surface Mby
a triangle mesh with vertices Vand faces F. Textures are
scalar functions ψdeﬁned continuously over the faces and
vertices, taking values in R3. For each p∈V, we con-
sider its neighborhood to be the surrounding one-ring, the
collection of faces in Fthat share pas a vertex. The en-
coder architecture is chosen so as to satisfy the conditions in
Equation ( 7). A ﬁxed number of points {qi}are uniformly
sampled in the one-ring neighborhood at which the texture
function is evaluated. The resulting scalar features {ψ(qi)}
are interleaved with the logarithms {logpqi}to construct
tangent vector features which are concatenated with a to-
ken following [ 12]. The features are passed to eight succes-
sive VN-Transformer layers [ 1], modi ﬁed to process tan-
gent vector features. Afterwards, the token is extracted and
used to predict the mean and standard deviation.
The neural ﬁeld comprising the decoder consists of a
ﬁve-layer real-valued MLP, to which the real and imagi-
nary parts of the complex feature vector are passed. For any
point in a triangle, the latent codes at the adjacent vertices
provide three distinct predictions which are linearly blended
with barycentric interpolation only at inference. This is in
contrast to INF [ 27], wherein linearly blended features are
passed to the neural ﬁeld to make predictions. A compre-
hensive discussion of the FL-V AE architecture and training
objective is provided in section 9.1in the supplement.
5. Field Latent Diffusion Models
FLDMs de ﬁne a noising process on surface vector
ﬁelds, and learn a denoising diffusion probabilistic model
7956
Method PSNR ↑ DSSIM↓ LPIPS↓
FL-V AE 22.38 (20.59) 0.51 (0.83) 1.02 (1.81)
FL-V AE (Bary.) 21.33 (19.61) 0.66 (1.01) 1.31 (2.03)
INFs [ 27] 18.86 (16.45) 1.16 (1.64) 2.15 (2.73)
Table 1. Texture reconstruction quality on meshes from the Google
Scanned Objects dataset [ 13]. Reconstructions are evaluated on
high-res ( 30K vertices) and low-res ( 5K vertices, in parenthesis)
meshings of the objects. Metrics are computed in the texture at-
lases, with DSSIM and LPIPS scaled by a factor of 10.
(DDPM ) [22] using an equivariant surface network. In prin-
ciple they can be applied to any vector data on surfaces, and
here we use them to learn feature distributions in FL space.
Latent vector ﬁeldsZ∈TMdcan be produced by com-
pressing a texture ψ∈L2(M,R3)to a collection of tan-
gent vector features at each point with the FL-V AE encoder
as in Equation ( 3) and sampling from the distributions.
The forward process progressively adds noise in the tan-
gent space, dictated by a monotonically decreasing sched-
ule{αt}0≤t≤Twithα0= 1 andαT= 0. The noised
vector ﬁelds can be expressed at an arbitrary time tin the
discretized forward process with
Zt=√αtZ+√
1−αtϵ∈TMd, (8)
whereϵ∼T NM(0,Id)as in Equation ( 1).
The denoising function εis a surface network trained
to reverse the forward process. Speci ﬁcally, it is a learn-
able map on the space of latent vector ﬁelds, condi-
tioned on timesteps tand optional scalar embeddings ρ∈
L2(M,Rm), with the goal of predicting the added noise,
ε:TMd×R≥0×L2(M,Rm)→TMd, (9)
trained subject to the loss
LFLDM=Eϵ∼T NM(0,Id),0≤t≤T∥ϵ−ε(Zt,t,ρ)∥2
2.(10)
At inference, new latent vector ﬁelds are generated fol-
lowing the iterative DDPM denoising process. Beginning
with representing the fully-noised vector ﬁelds at step Tby
a sample eZ∼T NM(0,Id),eZT=eZ, the trained denois-
ing function εis used to estimate the previous step in the
forward process via the relation
eZt−1=C1(αt,αt−1)eZt+C2(αt,αt−1)ε(eZt,t,ρ)
+C3(αt,αt−1)ϵ,(11)
withϵ= 0fort= 1andϵ∼T NM(0,Id)otherwise, and
Ci(αt,αt−1)∈Ras de ﬁned in 9.1of the supplement.
Equivariance From the equivalence of the distributions
T NM(0,Id)andT NN(0,Id)under isometries as in
Equation ( 2), it follows that for any isometry γ:M→N,
a suf ﬁcient condition for both the forward and reverse pro-
cesses in Equations ( 8) and ( 11) to satisfy
γZt= [γZ]tandγeZt=
γeZ
t, (12)
Ground truth FL-V AE FL-V AE (Bary.) INFs [ 27]
Figure 2. Visual comparison of textures compressed and recon-
structed with the FL-V AE and INF [ 27] on30K vertex meshes.
Compared to barycentric coordinates, our proposed logarithmic
coordinate function more richly extends latent features across the
mesh, enabling the reconstruction of ﬁner details. Zoom in to view.
for0≤t≤Tis if the denoising network εin Equation ( 9)
has the property that for any vector ﬁeldZ∈TMd,
γε(Z,t,ρ) =ε(γZ,t,γρ). (13)
A detailed proof is provided in sec. 8.2of the supplement.
Denoising with Field Convolutions To satisfy the condi-
tion in Equation ( 12), our denoising network is constructed
with ﬁeld convolutions (FC) [ 35], which belong to a class of
models that can operate on tangent vector ﬁelds [ 45,57].
FCs convolve vector ﬁeldsZ∈TMCwithC′×Cﬁl-
ter banks fc′c∈L2(C,C), returning vector ﬁeldsZ∗f∈
TMC′. In conventional diffusion models, features inside
the denoising network are conditioned on timesteps tvia an
additive embedding applied inside the convolution blocks.
Unfortunately, no direct analogue exists for vector ﬁeld fea-
tures on surfaces — additive embeddings break equivari-
ance and we ﬁnd that multiplicative embeddings destabilize
training. Thus, to enable conditioning of our denoising net-
workεon both timesteps tand optional user-input features
ρ∈L2(M,Rm), we inject embeddings directly into con-
volutions by extending FCs to convolve vector ﬁelds with
ﬁltersfc′c∈L2(C×Re,C), withethe embedding di-
mension. Critically, this formulation preserves equivariance
without sacri ﬁcing stability.
Following [ 54], the denoising network εtakes the form
of a shallow two-level U-Net but with ﬁeld convolutions;
the relatively small size of the network’s receptive ﬁeld al-
lows distributions of latent features to be learned without
overﬁtting to the single textural example [ 28,54,58].
7957
Input FLDM Sin3DM [ 58] Input FLDM Sin3DM [ 58]
Figure 3. Visual comparison of unconditionally (label-free) generated textures from FLDMs and Sin3DM [ 58]. Our FLDM’s isometry-
equivariant construction allows for replication of textural details across locally similar regions. In contrast, Sin3DM’s extrinsic approach
associates textural features with 3D space; Synthesized details appear as repetitions or extrusions of previous patterns along the major axes
of the mesh, and we observe that novel textures cannot be created without modifying geometry. Zoom in to compare.
Method SIFID ↓LPIPS↑
FLDM 3.27 1.15
Sample (left) 0.71 0.94
Sin3DM [ 58] 6.58 2.20
Sample (right) 2.42 2.91
FLDM Sin3DM
Table 2. Fidelity and diversity of unconditionally generated tex-
tures trained on selected Objaverse [ 8] and Scanned Objects [ 13]
meshes. SIFID and LPIPS are scaled by factors of 105and10,
respectively. Unlike our FLDMs, Sin3DM synthesizes new geom-
etry which can in ﬂate diversity scores. As an example, the indi-
vidual metrics are shown for the FLDM and Sin3DM samples on
the right. The latter attains a high LPIPS score, but exhibits poor
textural diversity and quality.
6. Experiments
Pre-training the FL-V AE It is important to note that the
FL-V AE proposed in Section 4only interfaces with the
mesh geometry through its local ﬂattening in the image of
the logarithm map. Thus, the FL-V AE operates indepen-
dently of the local curvature, and a model pre-trained’ on
ﬂat textured meshes can be used to encode and decode tex-
tures on arbitrary 3D shapes.
In all of our experiments we use the same FL-V AE with
ad= 8dimensional latent space, pre-trained on the Open-
ImagesV4 dataset [ 29]. Speci ﬁcally, we generate 1K dif-
ferent planar meshes containing 10K vertices each. Each
sample512×512 image is overlaid by a randomly cho-
sen triangulation, with the image serving as the “texture”
deﬁned over the mesh. The FL-V AE is then trained with
the sum of reconstruction loss and KL divergence using the
Adam [ 24] optimizer. We deliberately train the FL-V AE
on coarser triangulations than we expect during inference
on textured meshes. This forces the model to reconstruct
high frequencies during training, which we ﬁnd increases
robustness to sampling density and triangulation quality, in
addition to improving the ﬁdelity of reconstructed FLDM
samples.Training single-textured-mesh FLDMs All diffusion
experiments share the same training regime. During pre-
processing, we create 500 copies of a given mesh each with
30K vertices and different triangulations to prevent the de-
noising network from learning the connectivity. Each copy
shares the same UV-atlas as the original and the texture is
mapped to vector ﬁeld latent features at the vertices us-
ing the pre-trained FL-V AE encoder. FLDMs have a max
timestep of T= 1000 and noise is added on a linear sched-
ule. The denoising network is trained subject to the loss in
Equation ( 10) using the Adam [ 24] optimizer.
6.1. Texture Compression and Reconstruction
First we quantitatively evaluate the capacity of our proposed
FL-V AE for compressing and reconstructing textures. We
compare FL-V AEs against two other approaches: the state-
of-the-art INF [ 27] and a modi ﬁed version of FL-V AEs
serving as an ablation. INFs train a neural ﬁeld per tex-
ture and predicts the texture values using the values of the
Laplacian eigenfunctions extended over triangles through
barycentric interpolation. Similarly, the modi ﬁed FL-V AE
(denoted FL-V AE Barycentric) omits our proposed loga-
rithmic coordinate function in Equation ( 5) in favor of lin-
early interpolating the values of the invariant outer product
features across triangles. It is otherwise identical to our pro-
posed approach, including pre-training methodology. Thus,
this also serves to compare the ef ﬁcacy of our proposed
coordinate function against barycentric interpolation in ex-
tending vertex features continuously over the mesh.
To evaluate, we select 16objects exhibiting a range
of complex textures with high-frequency details from the
Google Scanned Objects dataset [ 13]. Two versions of the
dataset are used, with objects remeshed to have 30K (high-
res) and5K (low-res) vertices, respectively. The single pre-
trained FL-V AE is used to map the textures to distributions
over the tangent space, and the mean is used to sample the
reconstructed texture at the pixel indices in the texture atlas.
7958
Input Labels (ρ) FLDM samples
Figure 4. Label-guided generation. FLDMs can be conditioned
on a subjective user-input labeling which generated textures will
reﬂect. See Figure 1for more examples. Zoom in to view .
A separate INF neural ﬁeld is trained for each mesh, using
the speci ﬁed regime for high-res reconstruction [ 27].
Following INFs, we evaluate the quality of the recon-
structed textures using the average PSNR, DSSIM [ 55], and
LPIPS [ 62] metrics computed in texture atlases. The re-
sults are reported in Table 1, with several examples of re-
constructions shown in Figure 2. Our proposed FL-V AE
achieves the best results across all metrics at both resolu-
tions, followed by the barycentric FL-V AE and INFs. The
signi ﬁcant gap in performance between the barycentric FL-
V AE and INFs is likely due to the reliance of the latter the
on the Laplacian eigenfunctions as input features. Areas
ofﬁne textural detail may not necessarily overlap with re-
gions where the eigenfunctions exhibit high variability, po-
tentially contributing to the “raggedness” observed in INF
reconstructions. Furthermore, the superior performance of
our proposed FL-V AE relative to its barycentric counterpart
suggests that our logarithmic coordinate function in Equa-
tion ( 5) provides a richer interpolant over the mesh faces.
This directly increases the representational capacity of the
latent features, enabling the reconstruction of ﬁner textural
details with the same number of features.
6.2. Unconditional (Label-Free) Generation
Label-free generation can be performed by training and
sampling FLDMs conditioned only on the diffusion
timescale ( i.e.ρ= 0). We compare against Sin3DM [ 58]
which to our knowledge is the only prior generative model
designed to operate in a single-textured-mesh paradigm.
We follow their proposed evaluation protocol, and train
FLDMs and Sin3DM on 10assets from the Objaverse [ 8]
and Scanned Objects [ 13] datasets selected for texture com-
plexity and diversity. For each asset, 64textures are sam-
pled from the trained models, which are rendered on the
mesh from 24different views. Fidelity is measured with the
mean SIFID (Single Image Fr ´echet Inception Distance) [ 46]
pairwise between the renderings and those of the input mesh
from the same view; the mean LPIPS [ 62] between renders
of samples from the same viewpoint measures diversity. We
note that unlike FLDMs, which only synthesize new tex-
tures, Sin3DM synthesizes both textures and geometries,
with the latter potentially in ﬂating LPIPS scores.
The results are shown in Table 2, Figure 3, and in the
supplement (Figure 8). Compared to Sin3DM, our FLDMs
Input Mask FLDM samples
Figure 5. Inpainting with FLDMs. The input texture is preserved
in the masked regions and new content is synthesized elsewhere
with agreement on the boundaries. Zoom in to view .
are able to synthesize higher- ﬁdelity textures which is likely
due in part to the former’s rasterization of textured geome-
try to a 3D grid before encoding to the latent space, poten-
tially aliasing ﬁne details. More generally, we observe that
by operating over an extrinsic representation of textured ge-
ometry, Sin3DM intertwines texture with a mesh’s 3D em-
bedding. Thus, new textural details appear as repetitions
or extrusions of earlier patterns along the major axes of the
models, as seen in the vase, snake, and sculpture samples in
Figure 3. Furthermore, we observe that texture and position
are so strongly linked that Sin3DM is unable to produce sig-
niﬁcant textural alterations without correspondingly modi-
fying the geometry, as seen in the vase, snake, and lantern
samples. Here, the advantage of our isometry-equivariant
formulation becomes clear as our FLDMs can effectively
replicate textural details across areas of the mesh that are
locally similar, regardless of the relative con ﬁguration of
these regions in the 3D embedding.
6.3. Label-Guided Generation
Textured objects often exhibit distinct content speci ﬁc to
certain regions of the mesh which may not be geometri-
cally unique; for example, in Figure 1regions on the sole,
body, and inside of the shoe lack distinguishing geomet-
ric features, as is the case with the eyes and mouth of the
skull and the decal on the bottle. In such cases, a user may
ﬁnd it desirable for synthesized textures to re ﬂect a sub-
jective distribution of content on the input mesh, which we
facilitate by extending our denoising model to incorporate
optional conditioning from user-designated mesh features
ρ∈L2(M,Rm). In the simplest case, the user can paint
a coarse, semantic labeling to subjectively delineate salient
regions, such as the ﬂippers, head, and rock base of the seal
sculpture in Figure 4. After training an FLDM conditioned
on the labels, generated textures re ﬂect the speci ﬁed seg-
mentation. Further examples are shown in Figure 1. La-
beled areas contain different textural features, and condi-
tioning ensures the FLDM samples respect these distribu-
tions, e.g.that teeth are synthesized only on the mouth.
6.4. Inpainting
In certain cases a user may wish to preserve the input
texture in certain regions while synthesizing new con-
tent elsewhere. Texture inpainting is a well studied topic
7959
InputFLDM samples across geometries
Figure 6. Generative texture transfer. Since our FL-V AE and
FLDMs commute with isometries, we can encode a texture and
train an FLDM on an input mesh (left), then sample the pre-trained
FLDM on a new, similar mesh and decode the latents to texture it
(center to right). Conditioning labels segmenting the shell aper-
tures are not visible. Zoom in to view.
[15,20,31], and here we propose a straight-forward ap-
proach with FLDMs. Given a pre-trained FLDM condi-
tioned on a user-input binary mask m:M→{0,1}speci-
fying the regions to preserve (Figure 5), generative inpaint-
ing can be performed by modifying the iterative sampling
process. Speci ﬁcally, after estimating previous the latent
featureseZt−1as in Equation ( 11), the features in the mask
region are replaced with the appropriately noised input la-
tents from the forward process Zt−1such that
eZt−17→m·Zt−1+(1−m)·eZt−1. (14)
This approach encourages the inpainted texture to agree
with the original at the mask boundaries due to convolu-
tional structure of the denoising network, with this effect
observed on the hind leg of sampled textures in Figure 5.
6.5. Generative Texture Transfer
Perhaps the most dramatic consequence of isometry-
equivariance is that it naturally facilitates a notion of gen-
erative texture transfer. Since the FL-V AE and FLDMs
commute with isometries γ:M→Nas in Equa-
tions ( 6) and ( 12), our models see TMdandTNdas func-
tionally the same space. In practice, we ﬁnd that long as
surfacesMandNareapproximately isometric, we can en-
code a texture and train an FLDM on M, then sample the
model on Nand decode the synthesized latents to texture it.
FLDM samples on increasingly coarse remeshings-
Figure 7. Controlling the scale of synthesized textures. Here, the
FLDM trained on the textured skull in the center-left of Figure 1
is sampled on progressively coarser remeshings of a new skull ge-
ometry, dilating the size of the tesserae. Zoom in to view details.
Several examples are shown in Figure 6. An FLDM is
pre-trained with label conditioning on each of the textured
shells in the ﬁrst column and sampled with corresponding
labels on the others to texture them in the same style. Ad-
ditional examples are shown on the right side of Figure 1.
Notably, our model successfully transfers between different
topologies, as the skull meshes are genus zero and three.
More generally, we are able to transfer textures between
highly dissimilar shapes (see supplement for examples).
6.6. Controlling Textural Scale
An additional feature of our FLDMs is that they offer
user-control over the scale of sampled textures. Internally,
FLDMs normalize the ﬁlter support by dividing the dis-
tances between adjacent vertices by the radius of the mean
vertex area element er=p
A/(π|V|), whereAis the sur-
face area of the mesh and |V|is the number of vertices.
Thus, the scale of the synthesized textural details can be
controlled by increasing or decreasing the resolution of the
mesh upon which the pre-trained FLDM is sampled. An ex-
ample shown in Figure 7, where the FLDM trained on the
textured skull on the center left of Figure 1is sampled on
progressively coarser remeshings of a new skull geometry.
As the number of vertices decreases, the average one-ring
occupies an increasingly larger fraction of the surface area,
dilating the size of the synthesized textural details.
7. Conclusion and Limitations
We present an original framework for latent diffusion mod-
els operating directly on the surfaces to generate high-
quality textures. Our approach consists of two main con-
tributions: a novel latent representation mapping textures
to vector ﬁelds called Field Latents andField Latent Diffu-
sion Models which learn to denoise a diffusion process in
the latent space on the surface. We apply our method in a
single-textured-mesh paradigm , and generate variations of
textures with state-of-the-art ﬁdelity. Limitations of our ap-
proach include the relatively long training time of FLDMs
and an inability to re ﬂect directional information in synthe-
sized textures. The latter could potentially be addressed by
conditioning FLDMs on user-speci ﬁed vector ﬁelds, with
embeddings formed via dot products with network features.
7960
References
[1] Serge Assaad, Carlton Downey, Rami Al-Rfou, Nigamaa
Nayakanti, and Ben Sapp. Vn-transformer: Rotation-
equivariant attention for vector neurons. arXiv preprint
arXiv:2206.04176 , 2022. 4,3
[2] Alexey Bokhovkin, Shubham Tulsiani, and Angela Dai.
Mesh2tex: Generating mesh textures from image queries.
arXiv preprint arXiv:2304.05868 , 2023. 3
[3] Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp,
and Kangxue Yin. Texfusion: Synthesizing 3d textures with
text-guided image diffusion models. In ICCV , pages 4169–
4181, 2023. 2
[4] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey
Tulyakov, and Matthias Nießner. Text2tex: Text-driven
texture synthesis via diffusion models. arXiv preprint
arXiv:2303.11396 , 2023. 2
[5] Qimin Chen, Zhiqin Chen, Hang Zhou, and Hao Zhang.
Shaddr: Real-time example-based geometry and texture gen-
eration via 3d shape detailization and differentiable render-
ing. arXiv preprint arXiv:2306.04889 , 2023. 3
[6] Zhiqin Chen, Kangxue Yin, and Sanja Fidler. Auv-net:
Learning aligned uv maps for texture transfer and synthesis.
InCVPR , pages 1465–1474, 2022. 2
[7] An-Chieh Cheng, Xueting Li, Sifei Liu, and Xiaolong Wang.
Tuvf: Learning generalizable texture uv radiance ﬁelds.
arXiv preprint arXiv:2305.03040 , 2023. 2
[8] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,
Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana
Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:
A universe of annotated 3d objects, 2022. 2,6,7
[9] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo,
Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte,
Vikram V oleti, Samir Yitzhak Gadre, Eli VanderBilt, Anirud-
dha Kembhavi, Carl V ondrick, Georgia Gkioxari, Kiana
Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-xl: A
universe of 10m+ 3d objects, 2023. 2
[10] Congyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard,
Andrea Tagliasacchi, and Leonidas J Guibas. Vector neu-
rons: A general framework for so (3)-equivariant networks.
InICCV , pages 12200–12209, 2021. 2
[11] Nicolas Donati, Etienne Corman, Simone Melzi, and Maks
Ovsjanikov. Complex functional maps: A conformal link be-
tween tangent bundles. In Computer Graphics Forum , pages
317–334. Wiley Online Library, 2022. 3,7
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 4
[13] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kin-
man, Ryan Hickman, Krista Reymann, Thomas B. McHugh,
and Vincent Vanhoucke. Google scanned objects: A high-
quality dataset of 3d scanned household items, 2022. 1,5,6,
7
[14] Ahmed A Elhag, Joshua M Susskind, and Miguel An-gel Bautista. Manifold diffusion ﬁelds. arXiv preprint
arXiv:2305.15586 , 2023. 3
[15] Julien Fayer, Bastien Durix, Simone Gasparini, and
G´eraldine Morin. Texturing and inpainting a complete tubu-
lar 3d object reconstructed from partial views. Computers &
Graphics , 74:126–136, 2018. 8
[16] Marc Finzi, Samuel Stanton, Pavel Izmailov, and An-
drew Gordon Wilson. Generalizing convolutional neural net-
works for equivariance to lie groups on arbitrary continuous
data. In ICML , pages 3165–3176. PMLR, 2020. 4
[17] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,
Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja
Fidler. Get3d: A generative model of high quality 3d tex-
tured shapes learned from images. NeurIPS , 35:31841–
31854, 2022. 2
[18] James Gardner, Bernhard Egger, and William Smith.
Rotation-equivariant conditional spherical neural ﬁelds for
learning a natural illumination prior. NeurIPS , 35:26309–
26323, 2022. 4
[19] Niv Granot, Ben Feinstein, Assaf Shocher, Shai Bagon, and
Michal Irani. Drop the gan: In defense of patches near-
est neighbors as single image generative models. In CVPR ,
pages 13460–13469, 2022. 3
[20] Artur Grigorev, Artem Sevastopolsky, Alexander Vakhitov,
and Victor Lempitsky. Coordinate-based texture inpainting
for pose-guided human image generation. In CVPR , pages
12135–12144, 2019. 8
[21] Tobias Hinz, Matthew Fisher, Oliver Wang, and Stefan
Wermter. Improved techniques for training single-image
gans. In Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision , pages 1300–1309, 2021. 3
[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. NeurIPS , 33:6840–6851, 2020. 5,
3
[23] Shi-Min Hu, Zheng-Ning Liu, Meng-Hao Guo, Jun-Xiong
Cai, Jiahui Huang, Tai-Jiang Mu, and Ralph R Martin.
Subdivision-based mesh convolution networks. ACM Trans-
actions on Graphics (TOG) , 41(3):1–16, 2022. 4
[24] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization, 2017. 6,5
[25] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114 , 2013. 3,4
[26] Felix Kn ¨oppel, Keenan Crane, Ulrich Pinkall, and Peter
Schr ¨oder. Globally optimal direction ﬁelds. ACM TOG , 32
(4):1–10, 2013. 3
[27] Lukas Koestler, Daniel Grittner, Michael Moeller, Daniel
Cremers, and Zorah L ¨ahner. Intrinsic neural ﬁelds: Learning
functions on manifolds. In ECCV , pages 622–639. Springer,
2022. 3,4,5,6,7
[28] Vladimir Kulikov, Shahar Yadin, Matan Kleiner, and Tomer
Michaeli. Sinddm: A single image denoising diffusion
model. In ICML , pages 17920–17930. PMLR, 2023. 2,3,5,
4
[29] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-
jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan
Popov, Matteo Malloci, Alexander Kolesnikov, et al. The
open images dataset v4: Uni ﬁed image classi ﬁcation, object
7961
detection, and visual relationship detection at scale. IJCV ,
128(7):1956–1981, 2020. 6,3
[30] Zhen Liu, Yao Feng, Michael J Black, Derek
Nowrouzezahrai, Liam Paull, and Weiyang Liu. Meshdif-
fusion: Score-based generative 3d mesh modeling. arXiv
preprint arXiv:2303.08133 , 2023. 2
[31] A Maggiordomo, P Cignoni, and M Tarini. Texture inpaint-
ing for photogrammetric models. In Computer Graphics Fo-
rum. Wiley Online Library, 2023. 8
[32] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and
Daniel Cohen-Or. Latent-nerf for shape-guided generation of
3d shapes and textures. In CVPR , pages 12663–12673, 2023.
2
[33] Thomas W. Mitchel. Extending Convolution Through Spa-
tially Adaptive Alignment . PhD thesis, Johns Hopkins Uni-
versity, 2022. 4
[34] Thomas W. Mitchel, Benedict Brown, David Koller, Tim
Weyrich, Szymon Rusinkiewicz, and Michael Kazhdan. Ef-
ﬁcient spatially adaptive convolution and correlation, 2020.
4
[35] Thomas W Mitchel, Vladimir G Kim, and Michael Kazhdan.
Field convolutions for surface cnns. In ICCV , pages 10001–
10011, 2021. 2,3,5,4
[36] Thomas W Mitchel, Szymon Rusinkiewicz, Gregory S
Chirikjian, and Michael Kazhdan. Echo: Extended convo-
lution histogram of orientations for local surface description.
InComputer Graphics Forum , pages 180–194. Wiley Online
Library, 2021. 4
[37] Thomas W Mitchel, Noam Aigerman, Vladimir G Kim, and
Michael Kazhdan. M ¨obius convolutions for spherical cnns.
InACM SIGGRAPH 2022 Conference Proceedings , pages
1–9, 2022. 4
[38] Yaniv Nikankin, Niv Haim, and Michal Irani. Sinfusion:
Training diffusion models on a single image or video. arXiv
preprint arXiv:2211.11743 , 2022. 2,3
[39] Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo
Strauss, and Andreas Geiger. Texture ﬁelds: Learning tex-
ture representations in function space. In ICCV , pages 4531–
4540, 2019. 2
[40] Dario Pavllo, Jonas Kohler, Thomas Hofmann, and Aure-
lien Lucchi. Learning generative models of textured 3d
meshes from real-world images. In ICCV , pages 13879–
13889, 2021. 2
[41] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv
preprint arXiv:2209.14988 , 2022. 2
[42] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,
Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-
rokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123:
One image to high-quality 3d object generation using both
2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843 ,
2023. 2
[43] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes,
and Daniel Cohen-Or. Texture: Text-guided texturing of 3d
shapes. arXiv preprint arXiv:2302.01721 , 2023. 2,3
[44] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-thesis with latent diffusion models. In CVPR , pages 10684–
10695, 2022. 1,2
[45] Klaus Hildebrandt Ruben Wiersma, Elmar Eisemann. Cnns
on surfaces using rotation-equivariant features. ACM TOG ,
39(4), 2020. 5,4
[46] Tamar Rott Shaham, Tali Dekel, and Tomer Michaeli. Sin-
gan: Learning a generative model from a single natural im-
age. In ICCV , pages 4570–4580, 2019. 2,3,7
[47] Nicholas Sharp, Souhaib Attaiki, Keenan Crane, and Maks
Ovsjanikov. Diffusionnet: Discretization agnostic learning
on surfaces. ACM TOG , XX(X), 2022. 3,4
[48] Yawar Siddiqui, Justus Thies, Fangchang Ma, Qi Shan,
Matthias Nießner, and Angela Dai. Texturify: Generat-
ing textures on 3d shape surfaces. In ECCV , pages 72–88.
Springer, 2022. 3
[49] Saurabh Singh and Shankar Krishnan. Filter response nor-
malization layer: Eliminating batch dependence in the train-
ing of deep neural networks. In CVPR , pages 11237–11246,
2020. 2
[50] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 3
[51] David Svitov, Dmitrii Gudkov, Renat Bashirov, and Victor
Lempitsky. Dinar: Diffusion inpainting of neural textures for
one-shot human avatars. In ICCV , pages 7062–7072, 2023.
2
[52] Zhibin Tang and Tiantong He. Text-guided high-de ﬁnition
consistency texture model, 2023. 2
[53] Tianfu Wang, Menelaos Kanakis, Konrad Schindler, Luc Van
Gool, and Anton Obukhov. Breathing new life into 3d assets
with generative repainting, 2023. 2
[54] Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong
Chen, Dong Chen, Lu Yuan, and Houqiang Li. Sindiffu-
sion: Learning a diffusion model from a single natural im-
age. arXiv preprint arXiv:2211.12445 , 2022. 2,3,5,4
[55] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE transactions on image processing ,
13(4):600–612, 2004. 7
[56] Jiacheng Wei, Hao Wang, Jiashi Feng, Guosheng Lin, and
Kim-Hui Yap. Taps3d: Text-guided 3d textured shape gen-
eration from pseudo supervision. In CVPR , pages 16805–
16815, 2023. 2
[57] Ruben Wiersma, Ahmad Nasikun, Elmar Eisemann, and
Klaus Hildebrandt. Deltaconv: anisotropic operators for ge-
ometric deep learning on point clouds. ACM Transactions on
Graphics (TOG) , 41(4):1–10, 2022. 5,4
[58] Rundi Wu, Ruoshi Liu, Carl V ondrick, and Changxi Zheng.
Sin3dm: Learning a diffusion model from a single 3d tex-
tured shape. arXiv preprint arXiv:2305.15399 , 2023. 2,3,5,
6,7
[59] Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep
convolutional networks on 3d point clouds. In CVPR , pages
9621–9630, 2019. 4
[60] Rui Yu, Yue Dong, Pieter Peers, and Xin Tong. Learning tex-
ture generators for 3d shape collections from internet photo
sets. In BMVC , 2021. 2
7962
[61] Xin Yu, Peng Dai, Wenbo Li, Lan Ma, Zhengzhe Liu, and
Xiaojuan Qi. Texture generation on 3d meshes with point-uv
diffusion. In ICCV , pages 4206–4216, 2023. 2
[62] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR , pages 586–595,
2018. 7
[63] Xuanmeng Zhang, Jianfeng Zhang, Rohan Chacko, Hongyi
Xu, Guoxian Song, Yi Yang, and Jiashi Feng. Getavatar:
Generative textured meshes for animatable human avatars.
InICCV , pages 2273–2282, 2023. 2
[64] Peiye Zhuang, Samira Abnar, Jiatao Gu, Alex Schwing,
Joshua M Susskind, and Miguel ´Angel Bautista. Diffusion
probabilistic ﬁelds. In ICLR , 2022. 3
7963
