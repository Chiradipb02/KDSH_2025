DIFFUSE MIX: Label-Preserving Data Augmentation with Diffusion Models
Khawar Islam1Muhammad Zaigham Zaheer2Arif Mahmood3Karthik Nandakumar2
1FloppyDisk.AI2Mohamed bin Zayed University of Artificial Intelligence3Information Technology University, Punjab
1khawarr.islam@gmail.com2{zaigham.zaheer, karthik.nandakumar }@mbzuai.ac.ae3arif.mahmood@itu.edu.pk
Source (S)
Target (T)
CutMix (S+T)
 Mixup (S+T)
 GridMix (S+T) ResizeMix (S+T)
DiffuseMix (S)
 AugMix (T)
 PixMix (T)
 IPMix (T)
PuzzleMix (S+T)
DiffuseMix (S)SmoothMix (S+T)
DiffuseMix (T)AdaAutoMix (S+T)
DiffuseMix (T)
Figure 1. Top row: existing mixup methods interpolate two different training images [22, 48]. Bottom row: label-preserving methods.
For each input image, D IFFUSE MIXemploys conditional prompts to obtain generated images. The input image is then concatenated with
a generated image to obtain a hybrid image. Each hybrid image is blended with a random fractal to obtain the final training image.
Abstract
Recently, a number of image-mixing-based augmenta-
tion techniques have been introduced to improve the gen-
eralization of deep neural networks. In these techniques,
two or more randomly selected natural images are mixed
together to generate an augmented image. Such meth-
ods may not only omit important portions of the input im-
ages but also introduce label ambiguities by mixing images
across labels resulting in misleading supervisory signals.
To address these limitations, we propose DIFFUSE MIX, a
novel data augmentation technique that leverages a diffu-
sion model to reshape training images, supervised by our
bespoke conditional prompts. First, concatenation of a
partial natural image and its generated counterpart is ob-
tained which helps in avoiding the generation of unrealistic
images or label ambiguities. Then, to enhance resilience
against adversarial attacks and improves safety measures,
a randomly selected structural pattern from a set of frac-
tal images is blended into the concatenated image to form
the final augmented image for training. Our empirical re-
sults on seven different datasets reveal that DIFFUSE MIX
achieves superior performance compared to existing state-
of-the-art methods on tasks including general classification,
fine-grained classification, fine-tuning, data scarcity, and
adversarial robustness. Augmented datasets and codes are
available here: https://diffusemix.github.io/1. Introduction
In the era of deep learning, image-mixing-based data aug-
mentation techniques stand out for their simplicity and
effectiveness in addressing the generalization of learning
models toward testing scenarios [3, 6, 19, 21–23, 23, 34,
40, 46, 50]. These techniques ingeniously mix randomly
selected natural images and their respective labels from the
training dataset using a number of mixing combinations to
synthesize new augmented images and labels. Such a pro-
cess often implies linear interpolation of data, resulting in
the generation of novel training images. These approaches
have been proven to be effective in improving the perfor-
mance of deep models [13, 19, 36, 45, 48],.
However, these techniques may face a number of chal-
lenges such as the omission of salient image regions (Fig-
ure 1) and label ambiguities due to random placements of
images [23]. A few researchers have attempted to alleviate
these issues by introducing saliency-based mixup strategies
in which important regions of one image are pasted onto
the less important portions (mainly context) of another im-
age [21, 23, 40]. These methods not only suffer from the
costs but also the shortcomings of saliency detection meth-
ods. Moreover, as these methods still rely on mixing of two
or more images belonging to different classes, the underly-
ing issue of omitting the important context still persists.
Recently, Diffusion Models [11, 12, 31, 37, 38] have
emerged as transformative approaches offering image-to-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
27621
Table 1. Comparison of different image mixing techniques: most methods utilize natural images as source and target except [41] using
hidden state. D IFFUSE MIXuses a generated image produced by a diffusion model leveraging conditional prompts and a fractal image for
augmentation.
InputMixup
[48]ManifoldMixup
[41]CutMix
[45]SaliencyMix
[40]StyleMix
[18]PuzzleMix
[23]CoMixup
[22]PixMix
[17]GuidMixup
[21]DIFFUSE MIXComponentsSource image ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
Target image ✓ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✗
Fractal image ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✓
Textual Prompts ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓
Interpolation ✓ ✓ ✗ ✗ ✓ ✗ ✗ ✓ ✓ ✓
Concatenation ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✗ ✓ ✓TasksAdversarial Robustness ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✗ ✓
General Classification ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
Fine Grained ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ ✓
Transfer Learning ✗ ✗ ✓ ✓ ✗ ✗ ✗ ✗ ✗ ✓
Data Scarcity ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ ✓
image generation and editing processes. Although the idea
of using images generated by diffusion models directly as
augmented images for training a classifier has been studied
by some researchers [1, 39], this way of data augmentation
does not result in significant performance gains. In fact, as
reported in [1], the model trained using generated images
directly as augmentation may even result in lower perfor-
mance than the baseline trained without any augmentation.
The underlying problem can be attributed to the limited con-
trol that these diffusion models offer over generated images.
Owing to the sensitivity of diffusion models to conditional
prompts, generation of desired complex scenes ,layouts , and
shapes in an image is a cumbersome task [49]. Thus, poorly
constructed prompts pose the risk of producing images that
may not be suitable for data augmentation as the generated
images may deviate drastically from the actual data distri-
bution. Training on such images may result in overfitting of
the learning model on wrong data distribution, consequently
resulting in performance degradation. Therefore, careful se-
lection of prompts for data augmentation needs further in-
vestigation. Moreover, to mitigate the risk of poorly gener-
ated images affecting the overall training, a more efficient
way of utilizing the generated images is necessary.
To this end, we propose a novel data augmentation
method, D IFFUSE MIX, that leverages the capabilities of a
Stable Diffusion model to generate diverse samples based
on our tailored conditional prompts. In contrast to Trabucco
et al. [39], rather than solely relying on Stable Diffusion for
augmentation, we propose an effective approach which uti-
lizes both original and generated images to create hybrid
images. This way, visual diversity obtained by the diffusion
models is infused with the original images while retaining
the key semantics. In addition, to increase overall structural
diversity, we blend self-similarity-fractals with the hybrid
images to create the final training images. This blending
has previously been found useful for ML safety measures
[17, 20] while in our approach, this added diversity helps inavoiding overfitting on the generated contents resulting in
performance improvements. Our experimental results show
that D IFFUSE MIXbenchmarks better generalization as well
as increased adversarial robustness compared to the existing
state-of-the-art (SOTA) augmentation methods. Moreover,
it offers compatibility with a broad spectrum of datasets and
can be incorporated into the training of various existing ar-
chitectures. Some notable aspects of this research work are
as follows:
• We introduce a new data augmentation method driven by
a diffusion model, which generates diverse images via our
bespoke conditional prompts.
• We propose to concatenate a portion of the natural image
with its generative counterpart to obtain hybrid images.
The combination brings richer visual appearances while
preserving key semantics.
• We collect a fractal image dataset and blend into the hy-
brid images. This improves the overall structural com-
plexity of the augmented images and helps to avoid over-
fitting on generated images, thus resulting in better gen-
eralization.
• Extensive experiments on seven datasets for various tasks
including general classification, fine-grained classifica-
tion, adversarial robustness, transfer learning, and data
scarcity demonstrate the superior performance of our pro-
posed method compared to the existing SOTA image aug-
mentation techniques.
2. Related Work
Data augmentation has become indispensable in enhanc-
ing the diversity of training datasets, thereby mitigating the
risks of overfitting. Traditional approaches employed strate-
gies—such as horizontal and vertical translations, affine
transformations, scaling, and squeezing—when training a
model. This not only improves the performance but also
improves the generalization of the model on test datasets.
Diffusion Models for Augmentation: Recently, several re-
27622
1-M u
Mask M u
=  { { , , . . . ,
Input Image I
 =  { { , , ,Figure 2. Architecture of the proposed D IFFUSE MIXapproach . An input image and a randomly selected prompt are input to a diffusion
model to obtain a generated image. Input and generated images are concatenated using a binary mask to obtain a hybrid image. A random
fractal image is finally blended with this hybrid image to obtain the augmented image.
searchers have explored the possibility of data augmenta-
tion with diffusion models. Azizi et al. [1] proposed the uti-
lization of fine-tuned text-to-image diffusion models on Im-
ageNet classification, revealing that augmenting the training
set with these synthetic samples may boost classification
performance. Similarly, Trabucco et al. [39] investigated
diffusion models to create more diverse and semantically
varied datasets, aiming to improve outcomes in tasks such
as image classification. Li et al. [28] further explored diffu-
sion models-based augmentation for knowledge distillation
without real images.
Image Mixing Augmentation: Image mixing is a promi-
nent class of augmentation methods for training robust
CNN models [4, 29, 33, 44]. Some of these methods in-
clude Mixup, CutMix, and AugMix. Mixup [47] generates
synthetic images by linearly interpolating pixel values from
two randomly selected images. In contrast, CutMix [46] in-
volves pasting a random patch from one image onto another.
AugMix [15] employs a stochastic combination of data aug-
mentation operations on an input image. SaliencyMix [40]
utilizes saliency maps to concentrate the augmentation on
the image’s most vital regions, ensuring overall image in-
tegrity. Manifold Mixup [41] enhances representation by
interpolating network hidden states during training. This
entails blending two hidden states with a random weight to
produce an interpolated manifold-based hidden state. Puz-
zleMix [23], an improvement over the traditional mixup,
factors in image saliency, and local statistics during image
blending. This method segments an image into patches, al-
locates weights based on saliency and local statistics, and
merges patches from different images in accordance with
their weights. PixMix [17] have studied mixing of input
images with fractal and feature visualization images to im-
prove ML safety measures. A detailed summary of severalimage-mixing-based methods along with their components
and application tasks is provided in Table 1.
Automated Augmentation: AutoAugment [7], for in-
stance, employed reinforcement learning to pinpoint opti-
mal data augmentation policies, while RandAugment [9] in-
tegrates a suite of random data augmentation operations to
improve model generalization. AdaAug[5] is proposed to
efficiently learn adaptive augmentation policies in a class-
dependent and potentially instance-dependent manner.
In contrast to previous methods, our approach empha-
sizes the concatenation of original and generated images,
using a pre-defined library of conditional prompts. The ob-
tained hybrid images are blended with fractal images to fur-
ther improve the overall performance.
3. D IFFUSE MIX
3.1. Background and Overview
Existing image-mixing-based methods may induce label
ambiguity by placing one image on top of the other and
consequently overlapping either some portions of the ob-
ject or its context [21, 45]. In contrast, the core idea of D IF-
FUSE MIXis to concatenate a portion of the original image
with its counterpart generated image in such a way that ba-
sic image semantics are preserved while providing diverse
object details and contexts for better augmentation.
The proposed method as illustrated in Figure 2 com-
prises of three pivotal steps: generation ,concatenation ,
and fractal blending . Firstly, conditional prompts are used
with a diffusion model to obtain a generative counterpart
of the input image. Then, a portion of the original image
is concatenated with the rest of the portion taken from the
generated image forming a hybrid image. This step is to
ensure that the training network always has access to the
27623
Sunset
 Input image
Sketch with Crayon
Aurora
Autumn
 Snowy
Rainbow
 Watercolor
Mosaic
 Ukiyo-eFigure 3. A set of bespoke conditional prompts are used to obtain
generated images preserving important features and adding rich
visual appearance to the input images.
original data along with the generated one. Subsequently,
a random fractal image is blended into the hybrid image
to obtain the final training image with a diverse structure.
Blending fractal images has proven to be effective towards
ML safety [16, 17]. In our work, we study the effective-
ness of blending fractal images mainly towards improved
performance.
3.2. Method
The proposed D IFFUSE MIXis an effective data augmen-
tation technique which can be used to enhance the robust-
ness and generalization of the deep learning models. For-
mally, Ii∈Rh×w×cis an image from the training dataset,
Dmix(·) :Rh×w×c→Rh×w×cdenotes our data aug-
mentation method. To obtain the final augmented image
Aijuv, input image Iigoes through proposed generation us-
ing prompt pj, concatenation using mask Mu, and blend-
ing using fractal image Fv. The overall augmentation pro-
cess, as also seen in Algorithm 1, can be represented as
Aijuv=Dmix(Ii, pj, Mu, Fv, λ).
Generation: Our generation step G(.)consists of a pre-
trained diffusion model that takes a prompt pjfrom a pre-
defined set of kprompts, P={p1, p2, . . . , p k}where
j∈[1, k]along with the input image Iiand produces an
augmented counterpart image ˆIij. Image editing process
in conventional diffusion models is often open-ended and
guided by text prompts to obtain diverse image-to-image
or text-to-image translations. In our case, as the goal is to
achieve a slightly modified but not too different version of
Ii,filter-like prompts are curated in Pwhich do not alter the
image drastically. Examples of the prompts used in D IF-
FUSE MIXare shown in Figure 3. The overall generation
step can be represented as: ˆIij=G(Ii, pj), where pjis a
randomly selected prompt.
Concatenation: We concatenate a portion of the original
input image Iiwith its counterpart generated image ˆIijus-
ing a randomly selected mask Mufrom the set of masks to
create a hybrid image Hiju:
Hiju= (ˆIij⊙Mu) + (Ii⊙(1−Mu)). (1)Algorithm 1 DIFFUSE MIX
Require: Ii∈ D training images dataset, m: number of
augmented images, pj∈ P set of prompts, Mu∈ M
set of masks, Fv∈ F library of fractal images, λ: blend
ratio
Ensure: D′:mAugmented images
1:D′← ∅
2:foreach image IiinDdo
3: forain{1 :m}do
4: Randomly select prompt pjfromP
5: Generate image: ˆIij← G(Ii, pj)
6: Randomly select mask MufromM
7: Hybrid image: Hiju←Mu⊙Ii+(1−Mu)⊙ˆIij
8: Randomly select FvfromF
9: Blended image: Aijuv←(1−λ)Hiju+λFv
10: AddAijuvtoD′
11: end for
12:end for
13:return D′
The mask Muconsists of zeros and ones only and ⊙is a
pixel-wise multiplication operator. The set of masks con-
tains four kinds of masks including horizontal, vertical and
flipped versions. Such masking ensures the availability of
the semantics of the input image to the learning network
while reaping the benefits of the generated images.
Fractal Blending: A fractal image dataset *Fis collected
and used for inducing structural variations in the hybrid im-
ages. A randomly selected fractal image Fv∈ F is blended
to the hybrid image Hijuwith a blending factor λas:
Aijuv=λFv+ (1−λ)Hiju, (2)
where λis the blending factor. This results in the final
augmented image Aijuv used to train or fine-tune a deep
learning model. The overall augmentation process of D IF-
FUSE MIXcan be represented as:
Aijuv= (1−λ)(Ii⊙Mu+ˆIij⊙(1−Mu)) +λFv,(3)
4. Experiments and Results
In this section, we present the experimental details, datasets
used to evaluate our approach, and analyses of the results.
Datasets. To provide comparisons with existing studies on
image augmentation [5, 8, 9, 15, 19, 21–23, 30, 40, 41,
45, 48], we evaluate our approach on several general image
classification andfine-grained image classification datasets.
In the general image classification category, we employ
three datasets including ImageNet [10], CIFAR100 [25] and
*Examples of fractal images are provided in Appendix 8.
27624
Tiny-ImageNet-200 [26]. In fine-grained image classifica-
tion category, we employ four datasets including Oxford-
102 Flower [35], Stanford Cars [24], Aircraft [32], and
Caltech-UCSD Birds-200-2011 (CUB) [42]. These datasets
offer a diverse array of scenarios where images contain a
wide range of objects such as plants and animals in vari-
ous scenes, textures, transportation modes, human actions,
satellite imagery, and general objects.
Implementation Details. We utilize InstructPix2Pix [2]
diffusion model to generate images with the help of our in-
troduced textual library. For the generation of Mask Min
Eq. 1, a template image is divided into two equal parts, ei-
ther horizontally or vertically. Randomly, one half is turned
on and the second half is turned off. In all experiments†,
λ= 0.20is used for blending the fractal image in Eq. (2)
& (3). Analysis on λvalues is provided in Appendix 1.
Textual Prompt Selection. In order to ensure that only
appropriate prompts are applied, a bespoke textual library
offilter-like global visual effects is predefined: ‘autumn’,
‘snowy’, ‘sunset’, ‘watercolor art’, ‘rainbow’, ‘aurora’,
‘mosaic’,‘ukiyo-e’, and ‘a sketch with crayon’ . These
prompts are selected because of their generic nature and ap-
plicability to a wide variety of images. Secondly, these do
not alter the image structure significantly while producing a
global visual effect in the image. Each prompt in the textual
library is appended with a template ‘A transformed version
of image into prompt ’ to form a particular input to the dif-
fusion model. Examples of images generated through these
prompts are shown in Figure 3. More visual examples and
discussions on appropriate prompt selection are provided in
Appendix 3.
Visualizing Intermediate Steps. Figure 4 depicts images
obtained in each step of D IFFUSE MIX. It can be observed
that D IFFUSE MIXyields a broader spectrum of augmented
images derived from the training set. These images contain
full object with no portions omitted and provide suitable
variations for training.
5. Performance Evaluation
5.1. General Classification
General Classification (GC) can be a notable way to quan-
tify the effectiveness of an image augmentation method.
A higher GC accuracy would mean that an augmentation
method successfully provided plausible data variations to
improve the learning process. We evaluate our approach
for the GC task on three challenging datasets including
Tiny-ImageNet-200, CIFAR-100, and ImageNet. Similar
to the existing SOTA methods, PreActResNet18 network is
employed for the experiments on Tiny-ImageNet-200 and
†Following AugMix [15], we employ JS-divergence during training.
Figure 4. Example images from different stages of D IFFUSE MIX:
input image ( Ii), generated image ( ˆIij), mask ( Mu), hybrid image
(Hiju), fractal image ( Fv), and final augmented image ( Aijuv).
CIFAR-100 datasets [19, 21, 22, 40], while ResNet 50is em-
ployed for ImageNet dataset [15, 23, 45].
In Table 2 Top-1 and Top-5 accuracy on Tiny-ImageNet
and CIFAR- 100 datasets is compared with the existing
SOTA methods. The proposed D IFFUSE MIXtechnique
demonstrates better performance gains compared to the ex-
isting image augmentation approaches. On Tiny-ImageNet
dataset, compared to Vanilla model, our D IFFUSE MIXre-
sults in notable Top-1 and Top-5 accuracy gains of 8.54%
and10.01%. Moreover, compared to the second best per-
former, Guided-AP [21], Top-1 and Top-5 accuracy gains of
our approach are 1.14% and1.17%. Similar trends are ob-
served on CIFAR 100dataset, where D IFFUSE MIXdemon-
strates Top-1 and Top-5 accuracy gains of 6.17% and4.39%
over vanilla and 1.3%and0.53% over Guided-AP.
We also evaluate D IFFUSE MIXon large-scale ImageNet
dataset offering more challenging scenarios where existing
SOTA methods [15, 23, 45] only report Top-1 accuracy.
Compared to the second best performer PuzzleMix [23], our
approach demonstrates a performance gain of 1.13% (Ta-
ble 3). Compared to Vanilla implementation, our approach
demonstrates a performance gain of 2.95%. The GC results
on these challenging and diverse benchmark datasets high-
light the effectiveness of D IFFUSE MIXin enabling better
learning. It also suggests its capability to combat overfit-
ting and achieve better generalization.
5.2. Adversarial Robustness
Following existing SOTA methods [15, 18, 21–23, 40, 41,
45], we evaluate the robustness of our approach against ad-
versarial attacks and input perturbations. In these exper-
iments, fast adversarial training [43] is adapted to create
adversarially perturbed input images. The goal of these
experiments is to evaluate whether an augmentation ap-
proach can demonstrate better resilience against adversarial
attacks. FGSM [43] error rates are computed to evaluate the
performance against adversarial attacks.
27625
Table 2. Top-1 and Top-5 accuracy on general classification task of
PreactResNet-18 trained from scratch for 300 epochs following the results of
Kang and Kim [21]. Extended table can be seen in Appendix 7 Table 12.
Tiny-ImageNet-200 CIFAR-100MethodTop-1 (%) Top-5 (%) Top-1 (%) Top-5 (%)
Vanilla (CVPR’16) [14] 57.23 73.65 76.33 91.02
SaliencyMix (ICLR’21) [40] 56.54 76.14 79.75 94.71
Guided-SR (AAAI’23) [21] 55.97 74.68 80.60 94.00
PuzzleMix (ICML’20) [23] 63.48 75.52 80.38 94.15
Co-Mixup (ICLR’21) [22] 64.15 - 80.15 -
Guided-AP (AAAI’23) [21] 64.63 82.49 81.20 94.88
DIFFUSE MIX 65.77 83.66 82.50 95.41Table 3. Top-1 / Top-5 performance on ImageNet-1K
dataset benchmark when trained on ResNet-50 for 100
epochs for general classification task . An extended ver-
sion of this table is provided in Appendix 7 Table 13.
Method Top-1 (%) Top-5 (%)
Vanilla (CVPR’16) [14] 75.97 92.66
PixMix (CVPR’22) [17] 77.40 -
PuzzleMix (ICML’20) [23] 77.51 93.76
GuidedMixup (AAAI’23) [21] 77.53 93.86
Co-Mixup (ICLR’21) [22] 77.63 93.84
YOCO (ICML’22) [13] 77.88 -
DIFFUSE MIX 78.64 95.32
Table 4. FGSM error rates on CIFAR-100 and Tiny-ImageNet-200
datasets for PreactResNet-18, following [23].
FGSM Error Rates (%)MethodCIFAR-100 Tiny-ImageNet-200
Vanilla (CVPR’16) [14] 23.67 42.77
Mixup (ICLR’18) [48] 23.16 43.41
Manifold (ICML’19) [41] 20.98 41.99
CutMix (ICCV’19) [45] 23.20 43.33
AugMix (ICLR’20) [15] 43.33 -
PuzzleMix (ICML’20) [23] 19.62 36.52
DIFFUSE MIX 17.38 34.53
As shown in Table 4, D IFFUSE MIXdemonstrates an er-
ror rate of 17.38% on CIFAR-100, which is lower than all
compared methods. PuzzleMix is the second best performer
obtaining 19.62% error rate. Similarly on Tiny ImageNet-
200, D IFFUSE MIXoutperforms SOTA by a notable margin
obtaining 34.53% error rate while PuzzleMix remained the
second best performer with 36.52% error rate. These results
demonstrate that even under adversarial perturbations, D IF-
FUSE MIXremains resilient surpassing the performance of
existing SOTA approaches.
5.3. Fine-Grained Visual Classification
Compared to the general classification, the task of Fine-
Grained Visual Classification (FGVC) is notably challeng-
ing since it is difficult for a learning model to identify sub-
tle differences between two different objects belonging to
the same general class. A robust image augmentation tech-
nique should preserve these subtle albeit critical details for
the learning model to successfully train on the fine-grained
classification task. To evaluate D IFFUSE MIXon this task,
we conduct experiments on three datasets including CUB
[35], Stanford Cars [24], and Aircraft [32] utilizing ResNet-
50 [14] network.
As shown in Table 5, D IFFUSE MIXsignificantly en-
hances the generalization capability of ResNet-50, yielding
superior performances on all benchmark datasets. Specif-
ically, D IFFUSE MIXoutperforms widely-acknowledgedTable 5. Top-1 (%) performance comparison on fine-grained task
of ResNet-50. Extended comparisons are provided in Appendix 7
Table 14.
Method Birds Aircraft Cars
Vanilla (CVPR’16) [14] 65.50 80.29 85.52
RA (NIPS’20) [9] - 82.30 87.79
AdaAug (ICLR’22) [5] - 82.50 88.49
Mixup (ICLR’18) [48] 71.33 82.38 88.14
CutMix (ICCV’19) [45] 72.58 82.45 89.22
SnapMix (AAAI’21) [19] 75.53 82.96 90.10
PuzzleMix (ICML’20) [23] 74.85 82.66 89.68
Co-Mixup (ICLR’21) [22] 72.83 83.57 89.53
Guided-AP (AAAI’23) [21] 77.08 84.32 90.27
DIFFUSE MIX 79.37 85.76 91.26
methods such as Mixup and CutMix, with notable mar-
gins. On CUB dataset, D IFFUSE MIXachieved an accuracy
of79.37%, which is a significant leap from the 65.50%
Vanilla accuracy. It also outperformed recent methods in-
cluding Guided-AP 77.08%, and PuzzleMix 72.83%. Sim-
ilarly, on the Aircraft dataset, D IFFUSE MIXobtained an ac-
curacy of 85.76% surpassing the second best, GuidedMixup
84.32%. The Stanford Cars dataset further validates the re-
markable performance of D IFFUSE MIXregistering 91.26%
accuracy. On the same dataset, other methods such as Snap-
Mix and PuzzleMix achieved 90.10% and89.68% respec-
tively. The consistent performance gains on various chal-
lenging datasets iterate the effectiveness of D IFFUSE MIX
in retaining critical salient information necessary to perform
fine-grained classification.
5.4. Transfer Learning
Transfer learning or fine-tuning is a widely used way of
customizing large architectures with limited computational
resources as well as for quick experiments. Most image-
mixing-based augmentation methods [15, 15, 19, 21, 21–
23, 40, 41, 45, 48] have not reported performance in this
important scenario. Nevertheless, we evaluate the perfor-
27626
Table 6. Top-1 (%) accuracy on data scarcity task of ResNet-18
on Flower102 dataset where only 10 random images per class are
used. Extended comparisons are provided in Appendix 7 Table 15.
Method Valid Test
Vanilla (CVPR’16) [14] 64.48 59.14
SnapMix (AAAI’21) [19] 65.71 59.79
PuzzleMix (ICML’20) [23] 71.56 66.71
Co-Mixup (ICLR’20) [22] 68.17 63.20
GuidedMixup (AAAI’23) [21] 74.74 70.44
DIFFUSE MIX 77.14 74.12
mance of our approach on fine-tuning the baseline model
using three different datasets including Flower102, Aircraft,
and Stanford Cars on ImageNet-pretrained ResNet-50 pro-
vided by PyTorch and report results in Table 8.
For the Flower102 dataset, D IFFUSE MIXachieved an
accuracy of 98.02%, which is higher than the second best
method, AdaAug 97.19%. A similar trend is observable in
the Aircraft 85.65% and Cars 93.17% datasets. We also ob-
serve that the accuracy obtained by D IFFUSE MIXwith fine-
tuning (Table 8) is comparable to the performance when
training is done from scratch (Table 5). Since fine-tuning
consumes significantly less computational resources com-
pared to training from scratch, this experiment elaborates
the practical significance of D IFFUSE MIX.
5.5. Data Scarcity
Data scarcity is one of the major issues when training deep
neural networks. If the training examples per class are lim-
ited, deep networks may not learn meaningful patterns by
leading to overfitting and loss of generalization. Augmen-
tation methods are often used to overcome these challenges
by generating more data for training. Under this setting,
we compare the accuracy of ResNet-18 [14] trained using
only10images per class of the original Flower102 dataset.
As shown in Table 6, our method consistently outperforms
other mixup methods in situations where data is limited
yielding accuracies of 77.14% Top-1 and 74.12% Top-5.
Furthermore, our approach is designed to enhance the diver-
sification of the training dataset. By leveraging our bespoke
conditional prompts, D IFFUSE MIXartificially expands and
enriches the overall data landscape, enabling a more robust
learning of neural networks.
5.6. Analysis and Discussions
In this section, we provide a detailed ablation study and fur-
ther analysis of various design choices in D IFFUSE MIX.
Ablation Studies: To evaluate the importance of each
component we conduct an ablation study by removing
each component and report the observed performance on
ResNet-50 using Stanford Cars and Flowers-102 datasets in
Table 7.When all of the components of D IFFUSE MIXare re-
moved, the baseline (vanilla) using only original images Ii
obtains Top-1 and Top-5 accuracies of 85.52% &90.34%
on Cars dataset and 78.83% &94.38% on Flowers dataset.
Next, we add fractal blending ( Fv) to the input images re-
sulting in slight performance gains on both datasets. Fur-
ther, we remove both concatenation ( Hiju) and fractal
blending ( Fv) by conducting experiments using generated
images ( ˆIij) directly as augmented images to train the net-
work. This setting brings the method closer to the ap-
proaches proposed in [1, 39] which utilize the images gener-
ated using diffusion models directly as augmented images.
In this experiment, the accuracies obtained on Cars dataset
are87.63% Top-1and90.23% Top-5. Similarly, we ob-
serve the accuracies of 77.38% and93.15% on Flowers 102
dataset. The results are consistent with the findings in [1]
that direct use of generated images may not yield signifi-
cant performance gains over the vanilla method. Further,
we carry out experiments by using generated images with
fractal blending to train the model. On the Cars dataset,
we observe slight performance gains over the vanilla model
with accuracies of 89.42% Top-1 and 91.57% Top-5. How-
ever, in Flowers dataset, we observe a slight performance
degradation. This demonstrates that to unleash the maxi-
mum benefits of fractal blending, it should be accompanied
by more diversity in the training dataset. Next, we just re-
move the fractal blending from D IFFUSE MIXwhile incor-
porating concatenation between generated ( ˆIij) and origi-
nal (Ii) images to create hybrid images ( Hiju) used as aug-
mented training examples. This setting increases the accu-
racies to 90.59% Top-1 & 96.73% Top-5 on Cars dataset
and79.22% Top-1 & 94.38% Top-5 on Flowers dataset.
These performance improvements are due to the availability
of both generated and original image contents in each aug-
mented image, highlighting the importance of the concate-
nation step in D IFFUSE MIX. Finally, when all components
including fractal blending are used, the best accuracies of
91.26% Top-1 & 99.96% Top-5 on Cars dataset and 80.20%
Top-1 & 95.40% Top-5 on Flowers dataset are achieved.
Table 7. Ablation study using Stanford Cars (cars) and Flowers102
(Flow) datasets. Top-1 and Top-5 accuracies are reported with dif-
ferent combinations ofIi: Input image, ˆIij: Generated images
using prompts pj,Hiju: Hybrid images using random mask Mu,
andFv: fractal images used to obtain final blended image Aijuv.
Ii ✓ ✓ - - - -
ˆIij - - ✓ ✓ - -
Hiju - - - - ✓ ✓
Fv - ✓ - ✓ - ✓CarsTop-1 85.52 86.73 87.63 89.42 90.59 91.26
Top-5 90.34 92.38 90.23 91.57 96.73 99.96FlowTop-1 78.73 78.34 77.38 77.81 79.22 80.20
Top-5 94.38 94.91 93.15 93.24 94.38 95.40
27627
(a) CUB-200-2011
 (b) FGVC Aircraft
 (c) Stanford CarsFigure 5. Effect of the
number of prompts on over-
all performance. A de-
tailed ablation study show-
cases the gains in Top-1
(%) and Top-5 (%) accu-
racy across CUB Birds-200,
Aircraft, and Stanford Cars
datasets with an increase in
the number of prompts in
DIFFUSE MIX.
Table 8. Top-1 (%) accuracy of D IFFUSE MIXonfine-tuning ex-
periments using ImageNet pretrained ResNet-50.
Method Flower102 Aircraft Cars
Vanilla (CVPR’16) [14] 94.98 81.60 88.08
AA (CVPR’19) [8] 93.88 83.39 90.82
RA (NIPS’20) [9] 95.23 82.98 89.28
Fast AA (NIPS’19) [30] 96.08 82.56 89.71
AdaAug (ICLR’22) [5] 97.19 83.97 91.18
DIFFUSE MIX 98.02 85.65 93.17
Overall, consistent gains achieved with each added compo-
nent signifies the design choices in D IFFUSE MIXtowards
an effective image augmentation technique.
Number of Prompts Vs. Performance: The variety of
augmented images is determined by the number of ran-
dom prompts which is an important factor in D IFFUSE MIX.
A higher number of prompts corresponds to more diverse
training samples. Figure 5 shows the effect of increasing
the number of prompts on the performance using three dif-
ferent datasets including Birds, Aircraft and Cars. Increas-
ing the number of prompts consistently yields performance
gains on all three datasets using both Top-1 and Top-5 ac-
curacy. However, the peak performances are achieved when
all ten prompts proposed in our approach are used for train-
ing. In almost all datasets, the increasing accuracy trends
can be seen even at 10prompts which shows that the addi-
tion of more prompts may further improve the performance.
However, it will also incur more computational costs.
Increasing Masks Vs Performance: We conduct exper-
iments using Oxford Flower 102 dataset to study the im-
pact of various masks on the overall performance of D IF-
FUSE MIXand report the results in Table 9. Using even
only one kind of mask, vertical in this example, our ap-
proach achieves significantly higher accuracies than the
vanilla baseline. When both vertical and horizontal masks
are used, the accuracies improve further. However, the best
accuracies are achieved when both horizontal and vertical
masks are used along with random flipping between the po-
sitions of input and generated images adding more diversity
to the training data. It is also possible to use the masking
techniques from previous approaches, such as [27, 45], inTable 9. Ablation on the effects of masking in D IFFUSE MIXon
Flower102 dataset. All variants yield notably superior results com-
pared to vanilla on ResNet-50. However, best results are achieved
when all four vertical and horizontal masks are used.
Mask Top-1 (%) Top-5 (%)
Vanilla (CVPR’16) [14] 89.74 94.38
Ver Mask ( ) 94.02 98.42
Hor + Ver Masks ( ,) 94.27 99.03
Hor + Ver + Flipping ( ,,,) 95.37 99.39
DIFFUS MIX. We provide additional analysis on this in Ap-
pendix 4.
6. Conclusion
In this paper, we introduced D IFFUSE MIX, a data augmen-
tation technique based on diffusion models to increase di-
versity in the data while preserving the original seman-
tics of the input image. It involves generation, concate-
nation, and fractal blending steps to create the final aug-
mented image. On multiple tasks such as general clas-
sification, fine-grained classification, data scarcity, fine-
tuning, and adversarial robustness involving several bench-
mark datasets including ImageNet-1k, Tiny-ImageNet-200,
CIFAR-100, Oxford Flower102, Caltech Birds, Stanford-
Cars, and FGVC Aircraft , DIFFUSE MIXdemonstrates con-
sistent performance gains and outperforms existing SOTA
image augmentation methods.
Limitations: DIFFUSE MIXhas two notable limitations: (1)
Image generation relies heavily on text prompts and a
wrong textual input may lead to unrealistic results. We
address this issue by proposing a set of filter-like prompts
generally applicable to a wide range of natural images. (2)
DIFFUSE MIXrequires additional overheads for generating
images (more on this in Appendix 2). This is a small price to
pay for a guaranteed better convergence of large-scale clas-
sification models and can be mitigated by generating and
storing augmented images once.
Acknowledgement: We are thankful to Hamza Saleem for
the fruitful insights. Arif Mahmood is funded by the Infor-
mation Technology University of Punjab, Pakistan.
27628
References
[1] Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mo-
hammad Norouzi, and David J Fleet. Synthetic data from
diffusion models improves imagenet classification. arXiv
preprint arXiv:2304.08466 , 2023.
[2] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structpix2pix: Learning to follow image editing instructions.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 18392–18402, 2023.
[3] Jie-Neng Chen, Shuyang Sun, Ju He, Philip HS Torr, Alan
Yuille, and Song Bai. Transmix: Attend to mix for vision
transformers. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 12135–
12144, 2022.
[4] Yuan-Chih Chen and Chun-Shien Lu. Rankmix: Data
augmentation for weakly supervised learning of classifying
whole slide images with diverse sizes and imbalanced cat-
egories. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 23936–
23945, 2023.
[5] Tsz-Him Cheung and Dit-Yan Yeung. Adaaug: Learn-
ing class-and instance-adaptive data augmentation policies.
InInternational Conference on Learning Representations ,
2021.
[6] Hyeong Kyu Choi, Joonmyung Choi, and Hyunwoo J Kim.
Tokenmixup: Efficient attention-guided token-level data
augmentation for transformers. Advances in Neural Infor-
mation Processing Systems , 35:14224–14235, 2022.
[7] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-
van, and Quoc V Le. Autoaugment: Learning augmentation
policies from data. arXiv preprint arXiv:1805.09501 , 2018.
[8] Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Va-
sudevan, and Quoc V . Le. Autoaugment: Learning augmen-
tation strategies from data. In CVPR , 2019.
[9] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V
Le. Randaugment: Practical automated data augmen-
tation with a reduced search space. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition workshops , pages 702–703, 2020.
[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Fei-Fei Li. Imagenet: A large-scale hierarchical image
database. In CVPR , 2009.
[11] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems , 34:8780–8794, 2021.
[12] Chengbin Du, Yanxi Li, Zhongwei Qiu, and Chang Xu. Sta-
ble diffusion is untable. arXiv preprint , 2023.
[13] Junlin Han, Pengfei Fang, Weihao Li, Jie Hong, Moham-
mad Ali Armin, , Ian Reid, Lars Petersson, and Hongdong
Li. You only cut once: Boosting data augmentation with a
single cut. In International Conference on Machine Learning
(ICML) , 2022.
[14] K. He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
residual learning for image recognition. In CVPR , 2016.
[15] Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret
Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Aug-
mix: A simple data processing method to improve robustnessand uncertainty. In International Conference on Learning
Representations .
[16] Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob
Steinhardt. Unsolved problems in ml safety. arXiv preprint
arXiv:2109.13916 , 2021.
[17] Dan Hendrycks, Andy Zou, Mantas Mazeika, Leonard Tang,
Bo Li, Dawn Song, and Jacob Steinhardt. Pixmix: Dream-
like pictures comprehensively improve safety measures. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 16783–16792, 2022.
[18] Minui Hong, Jinwoo Choi, and Gunhee Kim. Stylemix: Sep-
arating content and style for enhanced data augmentation. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 14862–14870, 2021.
[19] Shaoli Huang, Xinchao Wang, and Dacheng Tao. Snap-
mix: Semantically proportional mixing for augmenting fine-
grained data. In Proceedings of the AAAI Conference on Ar-
tificial Intelligence , pages 1628–1636, 2021.
[20] Zhenglin Huang, Xiaoan Bao, Na Zhang, Qingqi Zhang,
Xiao Tu, Biao Wu, and Xi Yang. Ipmix: Label-preserving
data augmentation method for training robust classifiers. Ad-
vances in Neural Information Processing Systems , 36, 2024.
[21] Minsoo Kang and Suhyun Kim. Guidedmixup: an efficient
mixup strategy guided by saliency maps. In AAAI , pages
1096–1104, 2023.
[22] JangHyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh
Song. Co-mixup: Saliency guided joint mixup with super-
modular diversity. In International Conference on Learning
Representations , 2020.
[23] Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puz-
zle mix: Exploiting saliency and local statistics for optimal
mixup. In International Conference on Machine Learning ,
pages 5275–5285. PMLR, 2020.
[24] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
3d object representations for fine-grained categorization. In
CVPRw , pages 554–561, 2013.
[25] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009.
[26] Ya Le and Xuan Yang. Tiny imagenet visual recognition
challenge. CS 231N , 7(7):3, 2015.
[27] Jin-Ha Lee, Muhammad Zaigham Zaheer, Marcella Astrid,
and Seung-Ik Lee. Smoothmix: a simple yet effective data
augmentation to train robust classifiers. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition workshops , pages 756–757, 2020.
[28] Zheng Li, Yuxuan Li, Penghai Zhao, Renjie Song, Xi-
ang Li, and Jian Yang. Is synthetic data from diffusion
models ready for knowledge distillation? arXiv preprint
arXiv:2305.12954 , 2023.
[29] Wen Liang, Youzhi Liang, and Jianguo Jia. Miamix:
Enhancing image classification through a multi-stage aug-
mented mixied sample data augmentation method. arXiv
preprint arXiv:2308.02804 , 2023.
[30] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and
Sungwoong Kim. Fast autoaugment. Advances in Neural
Information Processing Systems , 32, 2019.
27629
[31] Xue-Jing Luo, Shuo Wang, Zongwei Wu, Christos Sakaridis,
Yun Cheng, Deng-Ping Fan, and Luc Van Gool. Camdiff:
Camouflage image augmentation via diffusion model. arXiv
preprint arXiv:2304.05469 , 2023.
[32] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew
Blaschko, and Andrea Vedaldi. Fine-grained visual classi-
fication of aircraft. arXiv preprint arXiv:1306.5151 , 2013.
[33] Thomas Mensink and Pascal Mettes. Infinite class mixup.
arXiv preprint arXiv:2305.10293 , 2023.
[34] Ning Miao, Tom Rainforth, Emile Mathieu, Yann Dubois,
Yee Whye Teh, Adam Foster, and Hyunjik Kim. Instance-
specific augmentation: Capturing local invariances. 2022.
[35] Maria-Elena Nilsback and Andrew Zisserman. Automated
flower classification over a large number of classes. In 2008
Sixth Indian conference on computer vision, graphics & im-
age processing , pages 722–729. IEEE, 2008.
[36] Huafeng Qin, Xin Jin, Yun Jiang, Mounim A El-Yacoubi,
and Xinbo Gao. Adversarial automixup. arXiv preprint
arXiv:2312.11954 , 2023.
[37] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,
Jonathan Ho, Tim Salimans, David Fleet, and Mohammad
Norouzi. Palette: Image-to-image diffusion models. In
ACM SIGGRAPH 2022 Conference Proceedings , pages 1–
10, 2022.
[38] Yu Takagi and Shinji Nishimoto. High-resolution image re-
construction with latent diffusion models from human brain
activity. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 14453–
14463, 2023.
[39] Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan
Salakhutdinov. Effective data augmentation with diffusion
models. In ICLR 2023 Workshop on Mathematical and Em-
pirical Understanding of Foundation Models , 2023.
[40] AFM Uddin, Mst Monira, Wheemyung Shin, TaeChoong
Chung, Sung-Ho Bae, et al. Saliencymix: A saliency guided
data augmentation strategy for better regularization. arXiv
preprint arXiv:2006.01791 , 2020.
[41] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Na-
jafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Ben-
gio. Manifold mixup: Better representations by interpolating
hidden states. In International conference on machine learn-
ing, pages 6438–6447. PMLR, 2019.
[42] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-
ona, and Serge Belongie. The caltech-ucsd birds-200-2011
dataset. 2011.
[43] Eric Wong, Leslie Rice, and J Zico Kolter. Fast is better than
free: Revisiting adversarial training. ICLR , 2020.
[44] Lingyu Yan, Yu Ye, Chunzhi Wang, and Yun Sun. Locmix:
local saliency-based data augmentation for image classifica-
tion. Signal, Image and Video Processing , pages 1–10, 2023.
[45] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk
Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-
larization strategy to train strong classifiers with localizable
features. CoRR , abs/1905.04899, 2019.
[46] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk
Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-
larization strategy to train strong classifiers with localizablefeatures. In Proceedings of the IEEE/CVF international con-
ference on computer vision , pages 6023–6032, 2019.
[47] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and
David Lopez-Paz. mixup: Beyond empirical risk minimiza-
tion. In International Conference on Learning Representa-
tions .
[48] Hongyi Zhang, Moustapha Ciss ´e, Yann N. Dauphin, and
David Lopez-Paz. mixup: Beyond empirical risk minimiza-
tion. In ICLR , 2018.
[49] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3836–3847, 2023.
[50] Qihao Zhao, Yangyu Huang, Wei Hu, Fan Zhang, and Jun
Liu. Mixpro: Data augmentation with maskmix and pro-
gressive attention labeling for vision transformer. In The
Eleventh International Conference on Learning Representa-
tions , 2022.
27630
