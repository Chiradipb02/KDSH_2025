Text Prompt with Normality Guidance for Weakly Supervised Video Anomaly
Detection
Zhiwei Yang1, Jing Liu1*, Peng Wu2
1Guangzhou Institute of Technology, Xidian University, Guangzhou, China
2School of Computer Science, Northwestern Polytechnical University, Xi’an, China
{zwyang97, neouma }@163.com, xdwupeng@gmail.com
Abstract
Weakly supervised video anomaly detection (WSVAD) is
a challenging task. Generating ﬁne-grained pseudo-labelsbased on weak-label and then self-training a classiﬁer is
currently a promising solution. However , since the exist-
ing methods use only RGB visual modality and the utiliza-tion of category text information is neglected, thus limitingthe generation of more accurate pseudo-labels and affect-ing the performance of self-training. Inspired by the manual
labeling process based on the event description, in this pa-
per , we propose a novel pseudo-label generation and self-training framework based on Text Prompt with Normality
Guidance (TPWNG) for WSVAD. Our idea is to trans-
fer the rich language-visual knowledge of the contrastivelanguage-image pre-training (CLIP) model for aligning thevideo event description text and corresponding video framesto generate pseudo-labels. Speciﬁcally, We ﬁrst ﬁne-tunethe CLIP for domain adaptation by designing two rank-ing losses and a distributional inconsistency loss. Fur-ther , we propose a learnable text prompt mechanism withthe assist of a normality visual prompt to further improve
the matching accuracy of video event description text and
video frames. Then, we design a pseudo-label generationmodule based on the normality guidance to infer reliableframe-level pseudo-labels. Finally, we introduce a temporalcontext self-adaptive learning module to learn the tempo-ral dependencies of different video events more ﬂexibly andaccurately. Extensive experiments show that our methodachieves state-of-the-art performance on two benchmarkdatasets, UCF-Crime and XD-Violence, demonstrating theeffectiveness of our proposed method.
1. Introduction
Anomaly detection has been widely researched and appliedin various ﬁelds, such as computer vision [ 23,35,40,43,
*Corresponding authors.
Two or more people fighting 
together, with violent actions 
such as punching and kicking.
“Fighting ”
Figure 1. Illustration of the manual video frame labeling process.
49], natural language processing [ 1], and intelligent opti-
mization [ 29]. One of the most important research issues is
the video anomaly detection (V AD). The main purpose of
V AD is to automatically identify events or behaviors in the
video that are inconsistent with our expectations.
Due to the rarity of anomalous events and the difﬁ-
culty of frame-level labeling, current V AD methods fo-cus on semi-supervised [ 14,16,18] and weakly supervised
[11,26,52] paradigms. Semi-supervised V AD methods aim
to learn normality patterns from normal data, and deviationsfrom this pattern are considered as anomalies. However,
due to the lack of discriminative anomaly information in
the training phase, these models are often prone to over-ﬁtting, leading to poor performance in complex scenarios.Subsequently, weakly supervised video anomaly detection(WSV AD) methods came into prominence. WSV AD in-volves both normal and abnormal videos with video-level
labels in the training phase, but the exact location of abnor-
mal frames is unknown. Current WSV AD methods mainlyinclude one-stage methods based on multi-instance learning(MIL) [ 17,26,27] and two-stage methods based on pseudo-
label self-training [ 6,11,51,53]. While the one-stage meth-
ods based on MIL show promising results, this paradigmtends to focus on video snippets with prominent anomalousfeatures and suboptimal attention to minor anomalies, thuslimiting its further performance improvement.
In contrast to the one-stage methods mentioned above,
two-stage methods based on pseudo-label self-training gen-erally use an off-the-shelf classiﬁer or MIL to obtain initial
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
18899
pseudo-labels, and then train the classiﬁer with further re-
ﬁned pseudo-labels. Because these methods train the classi-ﬁer directly with the generated ﬁne-grained pseudo-labels,they show great potential in performance. However, thesemethods still have two aspects that have not been consid-ered: ﬁrst, the generation of pseudo-labels is based only onvisual modality and lacks the utilization of textual modality,which limits the accuracy and completeness of the gener-ated pseudo-labels. Second, the mining of temporal depen-dencies among video frames is insufﬁcient.
To further exploit the potential of pseudo-label-based
self-training on WSV AD, we dedicate to investigating thetwo problems mentioned above in this paper. Our motiva-
tion for the ﬁrst question is that we explore how the tex-
tual modal information can be effectively utilized to assist ingenerating pseudo-labels. Recalling our manual process ofvideo frame labeling, we mainly based on textual deﬁnitionsof anomalous events, i.e., prior knowledge about anoma-lous events, to accurately locate the video frames. As illus-trated in Fig. 1, assuming that we need to annotate the ab-
normal video frames that contain “ﬁghting” event, we will
ﬁrst associate the textual deﬁnition of “ﬁghting” and then
look for matching video frames, which is actually a processof text-image matching based on prior knowledge. Inspiredby this process, we associate a highly popular and powerfulcontrastive language-image pre-training (CLIP) [ 19] model
to assist us in achieving this goal. On the one hand, theCLIP learns a large number of image-text pairs on the web,and thus has a highly rich prior knowledge; on the otherhand, the CLIP is trained by comparative learning, whichempowers it with excellent image-text alignment capabili-ties. For the second motivation , because different video
events have diverse durations, this leads to different ranges
of temporal dependencies. Existing methods either do notconsider temporal dependencies or only consider dependen-cies within a ﬁxed temporal range, leading to inadequatemodeling of temporal dependencies. Therefore, in order toachieve more ﬂexible and adequate modeling of temporal
dependencies, we should investigate methods that can adap-tively learn temporal dependencies of different lengths.
Based on the above two motivations, we propose a novel
pseudo-label generation and self-training framework based
onText Prompt withNormality Guidance (TPWNG) for
WSV AD. Our main idea is to utilize the CLIP model tomatch the textual descriptions of video events with the cor-responding video frames, and then infer the pseudo-labelsfrom match similarities. However, since the CLIP modelis trained at the image-text level, it may suffer from do-main bias and lacks the ability to learn temporal dependen-cies in videos. In order to better transfer the prior knowl-edge of CLIP to the WSV AD task, we ﬁrst construct a con-trastive learning framework by designing two ranking lossesand a distributional inconsistency loss to ﬁne-tune the CLIPmodel for domain adaptation under the weakly-supervised
setting. To further improve the accuracy of aligning the de-scriptive text of video events with video frames, we employlearnable textual prompts to facilitate the text encoder ofCLIP to generate more generalized textual embedding fea-tures. On this basis, we propose a normality visual prompt(NVP) mechanism to aid this process. In addition, be-cause abnormal videos contain normal video frames as well,we design a pseudo-label generation (PLG) module basedon normality guidance, which can reduce the interferencecaused by individual normal video frames to the alignmentof abnormal video frames, thus facilitating the obtaining ofmore accurate frame-level labels.
Furthermore, to compensate for the lack of temporal re-
lationship modeling in CLIP as well as to more ﬂexible and
adequately mine the temporal dependencies between video
frames, we introduce a temporal context self-adaptive learn-ing (TCSAL) module for temporal dependency modeling,inspired by the work [ 25]. TCSAL allows the attention
module in the Transformer to adaptively adjust the atten-tion span according to the inputs by designing a temporal
span adaptive learning mechanism. This can facilitate themodel to capture the temporal dependencies of video events
of different durations more accurately and ﬂexibly.
Overall, our main contributions are summarized below:
• We propose a novel framework, i.e., TPWNG, to per-
form pseudo-label generation and self-training for WS-
V AD. TPWNG ﬁne-tunes CLIP with the designed rank-ing loss and distributional inconsistency loss to trans-fer its strong text-image alignment capability to assistpseudo-label generation by means of the PLG module.
• We design a learnable text prompt and normality visual
prompt mechanism to further improve the alignment ac-
curacy of video events description text and video frames.
• We introduce a TCSAL module to learn the temporal de-
pendencies of different video events more ﬂexibly and ac-curately. To the best of our knowledge, we are the ﬁrst tointroduce the idea of self-adaptive learning of temporalcontext dependencies for V AD.
• Extensive experiments have been conducted on two
benchmark datasets, UCF-Crime and XD-Violence,where the excellent performance demonstrates the effec-tiveness of our method.
2. Related Work
2.1. Video Anomaly Detection
The V AD task has been widely focused and researched, andmany methods have been proposed to solve this problem.According to different supervision modes, these methodscan be mainly categorized into semi-supervised-based andweakly supervised-based V AD.
Semi-supervised V AD. Early researchers mainly used
18900
semi-supervised approaches to solve the V AD problem
[2,7,8,10,14,15,20,24,31,33,41–44,46,50]. In the
semi-supervised setting, only normal data can be acquiredin the training phase, which aims to build a model that cancharacterize normal behavioral patterns by learning normal
data. During the testing phase, data that contradict withthe normal patterns are considered anomalies. Common
semi-supervised V AD methods mainly include one-classclassiﬁer-based [ 21,33,37] and reconstruction [ 8,38]o r
prediction errors-based methods [ 14,42]. For example, Xu
et al. [ 38] used multiple one-classiﬁers to predict anomaly
scores based on appearance and motion features. Hasan etal. [8] built a fully convolutional auto-encoder to learn reg-
ular patterns in the video. Liu et al. in [ 14] proposed a novel
video anomaly detection method that utilizes the U-Net ar-chitecture to predict future frames, where frames with largeprediction errors are considered as anomalous.
Weakly Supervised V AD. Compared to semi-
supervised V AD methods, WSV AD can utilize bothnormal and anomalous data with video-level labelsin the training phase, but the exact frame location
where the abnormal event occurred is unknown. In
such a setting, the one-stage approaches based on MIL[3–5,13,17,22,26,27,32,34,45,54] and the two-
stage approaches based on pseudo-labels self-training[6,11,51,53] are the two prevailing approaches. For ex-
ample, Sultani et al. [ 26] ﬁrst proposed a deep MIL ranking
framework for V AD, where they considered anomalous andnormal videos as positive and negative bags, respectively,
and the snippets in the videos are considered as instances.Then a ranking loss is used to constrain the snippets withthe highest anomaly scores in the positive and negative
bags to stay away from each other. Later, many variants of
the method were proposed on this basis. For example, Tian
et al. [ 27] proposed a top-k MIL based V AD method with
robust temporal feature magnitude learning.
However, these one-stage methods generally use a MIL
framework, which leads to models that tend to focus onlyon the most signiﬁcant anomalous snippets while ignor-ing nontrivial anomalous snippets. A two-stage approachbased on pseudo-label self-training provides a relativelymore promising solution. The two-stage approach ﬁrst gen-
erates initial pseudo-labels using MIL or an off-the-shelf
classiﬁer and then reﬁnes the labels before using them forsupervised training of the classiﬁer. For example, Zhonget al. in [ 53] reformulated the WSV AD problem as a su-
pervised learning task under noisy labels obtained by anoff-the-shelf video classiﬁer. Feng et al. in [ 6] introduced
a multiple instance pseudo label generator that producesmore reliable pseudo labels for ﬁne-tuning a task-speciﬁcfeature encoder with self-training mechanism. Zhang et al.
in [51] exploited completeness and uncertainty properties
to enhance pseudo labels for effective self-training. How-ever, all these existing methods only generate pseudo-labels
based on visual unimodal information and lack the utiliza-tion of textual modal. Therefore, in this paper, we endeavorto combine both visual and textual modal information in or-der to generate more accurate and complete pseudo-labelsfor self-training of the classiﬁer.
2.2. Large Vision-Language Models
Recently, there has been an emergence of large vision-language models that learn the interconnections betweenvisual and textual modalities by pre-training on large-scaledatasets. Among these methods, the CLIP demonstrates un-precedented performance in many visual-language down-stream tasks, e.g. image classiﬁcation [ 55], object detec-
tion [ 56], semantic segmentation [ 12] and so on. The CLIP
model has recently been successfully extended to the videodomain as well. VideoCLIP [ 39] is proposed to align video
and textual representations by contrasting temporally over-
lapping video-text pairs with mined hard negatives. Action-CLIP [ 30] formulated the action recognition task as a mul-
timodal learning problem rather than a traditional unimodalclassiﬁcation task. However, there are fewer attempts to
utilize CLIP models to solve V AD tasks. Joo et al. in [ 9]
simply utilizes CLIP’s image encoder for extracting morediscriminative visual features and does not use textual in-formation. Wu et al. [ 36], Zanella et al. [ 48] mainly use
textual features from CLIP to enhance the expressiveness ofthe overall features, followed by MIL-based anomaly clas-siﬁer learning. The major difference with the above worksis that our method is the ﬁrst to utilize the textual featuresencoded by the CLIP text encoder in conjunction with thevisual features to generate pseudo-labels, and then employa supervised approach to train an anomaly classiﬁer.
3. Method
In this section, we ﬁrst present the deﬁnition of the WSV ADtask, then introduce the overall architecture of our proposedmethod, and subsequently elaborate on the details of eachmodule and the execution process.
3.1. Overall Architecture
Formally, we ﬁrst deﬁne sets Da={(va
i,yi)}M
i=1and
Dn={(vn
i,yi)}M
i=1containing Mabnormal and normal
videos with ground-truth labels, respectively. For each va
i,
it is labeled yi=1, indicating that this video contains at
least one anomalous video frame, but the exact location ofthe anomalous frame is unknown. For each v
n
i, it is labeled
yi=0, indicating that this video consists entirely of normal
frames. With this setting, WSV AD task is to utilize coarse-grained video-level labels to enable a classiﬁer to learn topredict ﬁne-grained frame-level anomaly scores.
Fig. 2illustrates the overall pipeline of our approach.
Normal and abnormal video along with learnable category
18901
CLIP
Image
Encoder
Normal video
abuse
arrest
normalassault......
 CLIP
Text
EncoderLearnable
Prompt
Text Prompt
CLIP
Image
Encoder
Abnormal videoTCSAL ClassifierD
A
DFFNPLG
PLGvideo features
video featurestext prompt 
featuresalignment map
alignment map
...... ...... ......
NVP n
rankL
a
rankL
dilLclL
clL
aggregation
product
Figure 2. The overall architecture of our proposed TPWNG.
prompt text are encoded as feature embedding by the image
encoder and text encoder of CLIP, respectively. Then, thetext encoder of CLIP is encouraged by ﬁne-tuning it to pro-duce textual feature embedding of video event categoriesthat accurately match anomalous or normal video frames,and the NVP assists in this process. Meanwhile, the imagefeatures feed the TCSAL module to perform self-adaptivelearning of temporal dependencies. Finally, a video frameclassiﬁer is trained to predict anomaly scores under the su-pervision of pseudo-labels obtained by the PLG module.
3.2. Text and Normality Visual Prompt
Learnable Text Prompt. Constructing textual prompts
that can accurately describe various video event categoriesis a prerequisite for realizing the alignment of text andcorresponding video frames. However, it is impracticalto manually deﬁne description texts that can completelycharacterize anomalous events in all different scenarios.
Therefore, inspired by CoOp [ 55], we employ a learnable
text prompt mechanism to adaptively learn representativevideo event text prompts to align the corresponding videoframes. Speciﬁcally, we construct a learnable prompt tem-plate, which adds llearnable prompt vectors in front of the
tokenized category name, as follows:
p
label=(∂1, ..., ∂ l, Tokenizer (label)), (1)
where∂ldenotes the l−thprompt vector. Tokenizer is
converting original category labels, i.e., “ﬁghting”, “acci-dent”, ..., “normal”, etc., into class tokens by means ofCLIP tokenizer. Then, we add the corresponding location
information posto the learnable prompts and then feed it
to the CLIP text encoder ζ
textto get the feature embedding
Tlabel∈RDof the video event description text as follows:
Tlabel=ζtext(plabel⊕pos), (2)
Finally, we compute all video event categories according to
Eqs. ( 1) and ( 2) to obtain the video event description text
embedding set E={Ta
1,Ta
2, ..., Ta
k−1,Tn
k}, where
{Ta
i}k−1
i=1denotes the description text embedding of preced-
ingk−1abnormal events and Tn
kdenotes the description
text embedding of normal events.
Normality Visual Prompt. For an anomalous video,
which contains both anomalous and normal frames, our
core task is to infer pseudo-labels from the match similar-
ities between the description text of the anomalous eventsand the video frames. However, this process is susceptibleto interference from normal frames in the anomalous videobecause they have a similar background to the anomalous
frames. To minimize this interference, we propose a NVP
mechanism. NVP is used to assist the normal event descrip-tion text to more accurately align normal frames in the ab-normal video, and thus indirectly assist the description textof abnormal event to align abnormal video frames in theabnormal video by means of the distribution inconsistencyloss that will be introduced in Sec. 3.5. Speciﬁcally, we
ﬁrst compute the match similarities S
nn
i, k∈RFbetween the
description text embedding of normal event and the videoframe features in the normal video. Then, the match simi-larities after softmax operation are used as weights to aggre-
18902
gate normal video frame features to obtain NVP Qi∈RD.
The formulas are represented as follows:
Snn
i, k=Xn
i(Tn
k)/latticetop,Qi=softmax ((Snn
i, k)/latticetop)Xn
i,(3)
whereXn
i∈RF×Ddenotes the visual features of the nor-
mal video vn
iobtained by the CLIP image encoder, where
FandDdenote the number of video frames and feature di-
mensions, respectively. Then, we concatenate QiandTn
kin
the feature dimension and feed an FFN layer with skip con-nections to obtain the enhanced description text embedding
˙T
n
kof normal events. The formula is represented as follows:
˙Tn
k=FFN((Tn
k∪Qi))+Tn
k. (4)
3.3. Pseudo Label Generation Module
In this subsection, we detail how to generate frame-level
pseudo labels. For a normal video, we can directly getthe frame-level pseudo-labels, i.e., for a v
n
i={Ij}F
j=1
containing Fnormal frames, it corresponds to a label set
{γn
i, j=0}F
j=1. Our main goal is to infer the pseudo-labels
for anomalous videos that contain both anomalous and nor-
mal frames. To this end, we propose a PLG module for in-ferring accurate pseudo-labels based on the normality guid-
ance. PLG module infers frame-level pseudo-labels by in-corporating the match similarities between the descriptiontext of the normal event and the abnormal video as a guideinto the match similarities between the description text of
the corresponding abnormal event and the abnormal video.
Speciﬁcally, we ﬁrst compute the match similarities
S
an
i, k=Xa
i(˙Tn
k)/latticetopbetween normal event description text
embedding enhanced with NVP and anomalous video fea-tures, where X
a
i∈RF×Ddenotes the visual features of
the anomalous video va
iobtained by the CLIP image en-
coder. Similarly, we compute the match similarities Saa
i, τ=
Xa
i(Ta
τ)/latticetopbetween the description text embedding Ta
τof
the corresponding τ-th(1/lessorequalslantτ/lessorequalslantk−1)real anomaly cate-
gory and the anomaly video features Xa
i.
Theoretically, for Saa
i, τ, it should have high match sim-
ilarities corresponding to abnormal frames and low matchsimilarities for normal frames. But it may be interfered by
normal frames from the same video having the same back-
ground. To reduce the interference of normal frames, weinfer pseudo-labels by incorporating the matching similar-ity corresponding to the description text of normal eventswith certain weights as a guide into the matching similar-ity of the description text of corresponding real abnormal
events. Speciﬁcally, we ﬁrst perform a normalization and
fusion operation on S
aa
i, τandSan
i, kas follows:
ψi=α˜San
i, k+(1−α)(1−˜Saa
i, τ), (5)
where˜∗denotes the normalization operation and αdenotes
the guidance weight. After obtaining ψi, we similarly per-
form a normalization operation on it to obtain ˜ψi. Then,we set a threshold θon˜ψito obtain the frame-level pseudo-
labels in the anomalous video as follows:
γa
i, j=/braceleftbigg
1,˜ψi,j≥θ;
0,˜ψi,j<θ ,i=1,2,...,M;j=1,2,...,F
(6)
whereγa
i, jdenotes the pseudo-label of the j-th frame in
thei-th anomaly video. Finally, we combine the frame-
level pseudo-labels γn
i, jandγa
i, jof normal and anomalous
videos to get the total pseudo-label set {γi, j}F
j=1.
3.4. Temporal Context Self-adaptive Learning
To adaptively adjust the learning range of temporal relation-
ship based on the input video data, inspired by the work[25], we introduce a TCSAL module. The backbone of
TCSAL is the transformer-encoder, but unlike the originaltransformer, the spanning range of attention is controlled bya soft mask function χ
zfor each self-attention head at each
layer.χzis a piecewise function mapping a distance to a
value between [0, 1] as follows:
χz(h)=m i n/bracketleftbigg
max/bracketleftbigg1
R(R+z−h),0/bracketrightbigg
,1/bracketrightbigg
,(7)
wherehrepresents the distance between the current t-th
frame in a video and the r−th(r∈[1,t−1]) frame in the
past temporal range. Ris a hyperparameter used to control
the softness. zis a learnable parameter that is adaptively
tuned with the input as follows:
z=Fσ(C/latticetopX+b), (8)
hereσrepresents the sigmoid operation, Candbare learn-
able parameters during model training. With the soft maskfunction χ
z, the corresponding attention weights ωt, ris
computed within this mask, i.e.,
ωt, r=χz(t−r)exp(βt,r)/summationtextt−1
q=1χz(t−q)exp(βt,q), (9)
hereβt,rdenotes the dot product output of the Query cor-
responding to the t-th frame in a video with the Keycorre-
sponding to the r−thframe in the past. Under the control of
χz, the self-attention heads will be able to adaptively adjust
the self-attention span range according to the input.
Finally, the video features after temporal context adap-
tive learning are fed into a classiﬁer to predict the frame-level abnormality scores {η
i, j}F
j=1.
3.5. Objective Function
First, we ﬁne-tune the CLIP text encoder. For a normal
video, we further compute the match similarities set ϕna
i=
{Sna
i, τ=Xn
i(Ta
τ)/latticetop|1/lessorequalslantτ/lessorequalslantk−1}between the descrip-
tion texts of the other k−1anomalous events and the normal
frames. We expect that the maximum in the similarity set
18903
ϕna
ishould be as small as possible while the maximum in
Snn
i, kshould be as large as possible. Thus, we design the
following ranking loss for constraints:
Ln
rank=m a x ( 0 ,1−max(Snn
i, k)+max(max( ϕna
i)).(10)
For an anomalous video, we ﬁrst calculate the similarities
San
i, k=Xa
i(˙Tn
k)/latticetopbetween the description text embedding
of normal event and the anomalous video features, the sim-
ilaritySaa
i, τ=Xa
i(Ta
τ)/latticetopbetween the description text em-
bedding of the τ-th(1/lessorequalslantτ/lessorequalslantk−1)real anomalous event
category and the anomalous video features, and the similar-ity setϕ
aa
i={Saa
i, g=Xa
i(Ta
g)/latticetop|1/lessorequalslantg/lessorequalslantk−1,g/negationslash=τ}
between the description text embedding of other k−2
anomalous event categories and the anomalous video fea-tures, respectively. We expect that the maximum value inS
an
i, kshould be greater than the maximum value in ϕaa
i.
Similarly, the maximum value in Saa
i, τshould be greater
than the maximum value in ϕaa
i. In short, it means that we
expect that the description texts of real abnormal and nor-
mal events should match the abnormal and normal framesin the abnormal video with the highest possible similarity,respectively. Thus, the ranking loss for anomalous videos isdesigned as follows:
L
a
rank=m a x ( 0 ,1−max(San
i, k)+max(max( ϕaa
i)))+
max(0,1−max(Saa
i, τ)+max(max( ϕaa
i))).
(11)
In addition, to further ensure that the description texts ofreal abnormal events and normal events can accurately alignthe abnormal and normal video frames in the abnormal
video, respectively, we design a distribution inconsistencyloss (DIL). DIL is used to constrain the similarities betweenthe description text of the real abnormal event and the video
frames to be inconsistent with the similarity distribution be-tween the description text of the normal event and the videoframes. We use cosine similarity to perform this loss:
L
dil=1
MFM/summationdisplay
i=1F/summationdisplay
j=1˜Saa
i, j, τ·˜San
i, j, k/vextenddouble/vextenddouble/vextenddouble˜Saa
i, j, τ/vextenddouble/vextenddouble/vextenddouble
2·/vextenddouble/vextenddouble/vextenddouble˜San
i, j, k/vextenddouble/vextenddouble/vextenddouble
2. (12)
Then, following the work [ 26], in order to make the gen-
erated pseudo-labels satisfy sparsity and smoothing in tem-poral order, we impose sparsity and smoothing constraints,
L
sp=/summationtextF
j=1(˜Saa
i, j, τ−˜Saa
i, j+1,τ)2,Lsm=/summationtextF
j=1˜Saa
i, j, τ ,
on the similarity vectors ˜Saa
i, τ.
Then, we calculate the binary cross-entropy between
the anomaly score ηi,jpredicted by the classiﬁer and the
pseudo-label γi, jas the classiﬁcation loss:
Lcl=−1
MFM/summationdisplay
i=1F/summationdisplay
j=1[ηi,jlog(γi,j)
+(1−ηi,j)log(1−γi,j)].(13)The ﬁnal overall objective function balanced by λ1and
λ2is designed as follows:
Lall=Ln
rank+La
rank+Ldil+Lcl+λ1Lsp+λ2Lsm.(14)
4. Experiments
4.1. Datasets and Evaluation Metrics
Datasets. We conduct extensive experiments on two bench-
mark datasets, UCF-Crime [ 26] and XD-Violence [ 34].
UCF-Crime is a large-scale real scene dataset for WSV AD.
UCF-Crime duration is 128 hours in total and contains 1900
surveillance videos covering 13 anomaly event categories,
of which 1610 videos with video-level labels are used fortraining and 290 videos with frame-level labels are usedfor testing. XD-Violence is a large-scale violence detection
dataset collected from movies, online videos, surveillancevideos, CCTVS, etc. XD-Violence lasts 217 hours and con-tains 4754 videos covering 6 anomaly event categories, ofwhich 3954 training videos with video-level labels and 800test videos with frame-level labels.
Evaluation Metrics. Following the previous methods
[6,26], for the UCF-Crime dataset, we measure the perfor-
mance of our method using the area under the curve (AUC)of the frame-level receiver operating characteristics (ROC).
Similarly, for the XD-Violence dataset, we follow the eval-uation criterion of average precision (AP) suggested by thework [ 34] to measure the effectiveness of our method.
4.2. Implementation Details
The image and text encoders in our method use a pre-trained
CLIP (VIT-B/16), in which both the image and text en-coders are kept frozen, except for the text encoder wherethe ﬁnal projection layer is unfrozen for ﬁne-tuning. Thefeature dimension Dis 512. FFN is a standard block
from Transformer. The length lof the learnable sequence
in the text prompt is set to 8. The normality guidanceweightαis set to 0.2 for both the UCF-Crime and XD-
Violence datasets. The pseudo-labels generation threshold θ
is set to 0.55 and 0.35 for the UCF-Crime and XD-Violencedatasets, respectively. The parameter Rused to control the
softness of the soft mask function is set to 256. The sparseloss and smoothing loss weights are set to λ
1=0.1and
λ2=0.01. Please refer to the supplementary materials for
more details on implementation.
4.3. Comparison with State-of-the-art Methods
We compare the performance on the UCF-Crime and XD-Violence datasets with the current state-of-the-art (SOTA)methods in Tab. 1. As can be observed from the table,
our method achieves a new SOTA on both the UCF-Crimeand XD-Violence datasets. Speciﬁcally, for the UCF-Crimedataset, our method outperforms the current SOTA method
18904
Methods UCF (AUC) XD (AP)
WeaklySultani et al.[ 26] 77.92% 73.20%
GCN [ 53] 82.12% -
HL-Net [ 34] 82.44% 73.67%
CLAWS [ 45] 82.30% -
MIST [ 6] 82.30% -
RTFM [ 27] 84.30% 77.81%
CRFD [ 32] 84.89% 75.90%
GCL [ 47] 79.84% -
MSL [ 11] 85.62% 78.58%
MGFN [ 3] 86.67% 80.11%
Zhang et al.[ 51] 86.22% 78.74%
UR-DMU [ 54] 86.97% 81.66%
CLIP-TSA [ 9] 87.58% 82.17%
Ours 87.79% 83.68%
Table 1. AUC and AP on UCF-Crime and XD-Violence dataset.
CLIP-TSA [ 9] by 0.21%, which is not a trivial improve-
ment for the challenging WSV AD task. Most importantly,compared to methods MIST [ 6] and Zhang et al. [ 51] sim-
ilar to ours that also use pseudo-label-based self-training,our method signiﬁcantly outperforms them by 5.49% and
1.57%, respectively. This fully demonstrates that our pro-posed pseudo-label generation and self-training framework
is vastly superior to the above two approaches. This alsoindicates that transferring visual language multimodal as-sociations through CLIP is conducive to generating moreaccurate pseudo-labels compared to merely utilizing uni-
modal visual information. For the XD-Violence dataset, our
method also surpasses the current optimal method CLIP-TSA [ 9] by 1.52%. Compared to a similar pseudo-label-
based self-training method Zhang et al. [ 51], our method
also outperforms it by 4.94%. The consistent superior per-formance on two large-scale real datasets strongly demon-strates the effectiveness of our method. This also showsthe extraordinary potential of the pseudo-label based self-training scheme, if accurate pseudo-labels can be generatedutilizing multiple modality information.
4.4. Ablation Studies
We conduct ablation experiments in this subsection to ana-lyze the effectiveness of each component of our framework.
Effectiveness of Normal Visual Prompt. To verify the
validity of NVP, we execute three comparison experiments:without NVP, with NVP based on frame averaging (NVP-FA), and with NVP based on match similarities aggregation(NVP-AS). As can be seen from the results in Tab. 2,i n
the absence of NVP, the performance of our method on theUCF-Crime and XD-Violence datasets decreases by 2.54%and 2.10% compared to with an NVP-AS, respectively.NVP-AS boosts the performance of the method by 0.47%and 0.55% more compared to NVP-FA on UCF-Crime andUCF-Crime (AUC) XD-Violence (AP)
w/o NVP 85.25% 81.58%
w NVP-FA 87.32% 83.13%
w NVP-AS 87.79% 83.68%
Table 2. The AUC and AP of our method on the UCF and XD
datasets without NVP, with NVP-FA, and with NVP-AS.
UCF-Crime (AUC) XD-Violence (AP)
w/o NG 85.83% 81.32%
wN G 87.79% 83.68%
Table 3. The AUC and AP of our method on the UCF and XDdatasets with NG and without NG.
XD-Violence datasets, respectively. This reveals two facts:
ﬁrst, NVP can help the text embedding to better match nor-
mal frames in anomalous videos, which indirectly aids ingenerating more accurate pseudo-labels in cooperation withthe DIL and the normality guidance mechanism. Second,
the NVP-AS can effectively reduce the interference of somenoise snippets (e.g., prologue, perspective switching, etc.)in normal videos compared to the NVP-FA approach, thusobtaining a purer NVP.
Effectiveness of the Normality Guidance. In the
pseudo-label generation module, instead of inferringpseudo-labels directly based on the similarity between the
corresponding abnormal event description text and the ab-
normal video, we incorporate guidance from the match sim-ilarities of the normal event description text counterparts,aiming to reduce the interference of partially noisy videoframes and generate more accurate pseudo-labels. To verifythe contribution of the normality guidance, we compare theimpact of the pseudo-label generation module on the per-formance of our method with and without normal guidance(NG), respectively. As can be observed from Tab. 3, when
our method is equipped with normal guidance, the perfor-mance rises by 1.96% and 2.36% on the UCF-Crime andXD-Violence datasets, respectively. This validates the ef-fectiveness of the normality guidance.
Effectiveness of TCSAL. To analyze the effectiveness
of TCSAL module, we conduct comparative experimentswith the Transformer-encoder (TF-encoder) module in [ 28],
MTN module in [ 27], and GL-MHSA module in [ 54]b y
replacing the temporal learning module in our frameworkwith each of these three modules. From Tab. 4, it can be
observed that the TF-encoder module has the lowest per-formance, which is understandable since the global self-attention computation way makes it neglect to pay atten-
tion to the local temporal information. Both MTN and GL-MHSA outperform TF-encoder with comparable perfor-
mance. Our introduced TCSAL module achieved the bestperformance on both datasets. This indicates that adopting
18905
(a) (b) (c) (d)
(e) (f) (g) (h)Explosion
ShootingFighting Riot Normal
Accident Explosion Normal
Figure 3. Anomaly score curves of several test samples on the UCF-Crime and XD-Violence dataset.
UCF (AUC) XD (AP)
w TF-encoder 85.12% 80.02%
w MTN 86.22% 81.02%
w GL-MHSA 86.43% 81.23%
w TCSAL 87.79% 83.68%
Table 4. The AUC and AP of our method on the UCF and XD
datasets with different temporal modules.
Loss term Dataset
bs Ln
rankLarankLdil UCF (AUC) XD (AP)
/check 77.12% 73.32%
/check/check 81.34% 78.67%
/check/check 84.45% 81.56%
/check/check 82.47% 79.96%
/check/check /check /check 87.79% 83.68%
Table 5. Comparison of the AUC and AP of our method with
different loss terms on the UCF-Crime and XD-Violence datasets.”bs” indicates that L
cl,Lsp,Lsmthree loss functions are used.
the mechanism of self-attention span range adaptive learn-
ing enables the temporal learning module to self-adapt tothe inputs of videos with different event lengths, achievingmore accurate modeling of temporal dependencies while
weakening the interference of other non-relevant temporalinformation in the non-event span range.
4.5. Qualitative Results
We show the anomalous scores of our method on severaltest videos in Fig. 3. It can be obviously noticed that there
is a steep rise in the anomaly scores when various anoma-lous events occur, and as the anomalous events end, theanomaly scores fall back to the lower range rapidly. For nor-mal events, our method gives a lower abnormal score. Thisintuitively demonstrates that our method has good sensitiv-
ity to abnormal events and can accurately and timely detect
the occurrence of abnormal events while maintaining a lowabnormal score prediction for normal events.4.6. Analysis of Losses
To analyze the impact of the three loss functions Ln
rank,
La
rank, andLdil, we perform ablation experiments on the
UCF-Crime and XD-Violence datasets. As shown in Tab. 5,
when all three loss functions are absent, the performance ofour method is unsatisfactory. This reveals that the originalCLIP suffers from domain bias and is not directly applica-ble to the V AD domain. When three loss functions are avail-able individually, the performance of our method is clearlyimproved, where the L
a
rank gives the biggest boost to the
performance. When all three losses are combined and co-operate with each other, our method achieves the best per-formance. This demonstrates the effectiveness of the threeloss functions we have designed, and they can effectivelyassist CLIP in domain adaptation for WSV AD.
5. Conclusions
In this paper, we propose a novel framework, TPWNG, toperform pseudo-label generation and self-training for WS-V AD. TPWNG ﬁnetunes CLIP with the designed ranking
loss and distributional inconsistency loss to transfer its text-
image alignment capability to assist pseudo-label genera-tion with the PLG module. Further, we design a learnabletext prompt and normality visual prompt mechanisms tofurther improve the alignment accuracy of video events de-scription text and video frames. Finally, we introduce a TC-
SAL module to learn the temporal dependencies of differ-
ent video events more ﬂexibly and accurately. We performextensive experiments on the UCF-Crime and XD-Violencedatasets, and the superior performance compared to existingmethods demonstrates the effectiveness of our method.
6. Acknowledgments
This work was supported by the Guangzhou Key Researchand Development Program (No. 202206030003), the Fun-damental Research Funds for the Central Universities, theInnovation Fund of Xidian University (No. YJSJ24006),and the Guangdong High-level Innovation Research Insti-tution Project (No. 2021B0909050008).
18906
References
[1] Christophe Bertero, Matthieu Roy, Carla Sauvanaud, and
Gilles Tr ´edan. Experience report: Log mining using natural
language processing and application to anomaly detection.InISSRE , pages 351–360, 2017. 1
[2] Ruichu Cai, Hao Zhang, Wen Liu, Shenghua Gao, and
Zhifeng Hao. Appearance-motion memory consistency net-work for video anomaly detection. In AAAI , pages 938–946,
2021. 3
[3] Yingxian Chen, Zhengzhe Liu, Baoheng Zhang, Wilton
Fok, Xiaojuan Qi, and Yik-Chung Wu. Mgfn: Magnitude-contrastive glance-and-focus network for weakly-supervisedvideo anomaly detection. In AAAI , pages 387–395, 2023. 3,
7
[4] MyeongAh Cho, Minjung Kim, Sangwon Hwang, Chae-
won Park, Kyungjae Lee, and Sangyoun Lee. Look aroundfor anomalies: Weakly-supervised anomaly detection viacontext-motion relational learning. In CVPR , pages 12137–
12146, 2023.
[5] MyeongAh Cho, Minjung Kim, Sangwon Hwang, Chae-
won Park, Kyungjae Lee, and Sangyoun Lee. Look aroundfor anomalies: Weakly-supervised anomaly detection viacontext-motion relational learning. In CVPR , pages 12137–
12146, 2023. 3
[6] Jia-Chang Feng, Fa-Ting Hong, and Wei-Shi Zheng. Mist:
Multiple instance self-training framework for video anomalydetection. In CVPR , pages 14009–14018, 2021. 1,3,6,7
[7] Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha,
Moussa Reda Mansour, Svetha Venkatesh, and Antonvan den Hengel. Memorizing normality to detect anomaly:Memory-augmented deep autoencoder for unsupervisedanomaly detection. In CVPR , pages 1705–1714, 2019. 3
[8] Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit K
Roy-Chowdhury, and Larry S Davis. Learning temporal reg-ularity in video sequences. In CVPR , pages 733–742, 2016.
3
[9] Hyekang Kevin Joo, Khoa V o, Kashu Yamazaki, and Ngan
Le. Clip-tsa: Clip-assisted temporal self-attention forweakly-supervised video anomaly detection. In ICIP , pages
3230–3234, 2023. 3,7
[10] Sangmin Lee, Hak Gu Kim, and Yong Man Ro. Bman:
Bidirectional multi-scale aggregation networks for abnormalevent detection. IEEE TIP , 29:2395–2408, 2019. 3
[11] Shuo Li, Fang Liu, and Licheng Jiao. Self-training multi-
sequence learning with transformer for weakly supervisedvideo anomaly detection. In AAAI , pages 1395–1403, 2022.
1,3,7
[12] Yuqi Lin, Minghao Chen, Wenxiao Wang, Boxi Wu, Ke Li,
Binbin Lin, Haifeng Liu, and Xiaofei He. Clip is also an ef-ﬁcient segmenter: A text-driven approach for weakly super-vised semantic segmentation. In CVPR , pages 15305–15314,
2023. 3
[13] Tianshan Liu, Kin-Man Lam, and Jun Kong. Distilling privi-
leged knowledge for anomalous event detection from weaklylabeled videos. IEEE TNNLS , pages 1–15, 2023. 3
[14] Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. Fu-ture frame prediction for anomaly detection–a new baseline.
InCVPR , pages 6536–6545, 2018. 1,3
[15] Weixin Luo, Wen Liu, and Shenghua Gao. A revisit of sparse
coding based anomaly detection in stacked rnn framework.InICCV , pages 341–349, 2017. 3
[16] Hui Lv, Chen Chen, Zhen Cui, Chunyan Xu, Yong Li, and
Jian Yang. Learning normal dynamics in videos with meta
prototype network. In CVPR , pages 15425–15434, 2021. 1
[17] Hui Lv, Chuanwei Zhou, Zhen Cui, Chunyan Xu, Yong Li,
and Jian Yang. Localizing anomalies from weakly-labeledvideos. IEEE TIP , 30:4505–4515, 2021. 1,3
[18] Hyunjong Park, Jongyoun Noh, and Bumsub Ham. Learning
memory-guided normality for anomaly detection. In CVPR ,
pages 14372–14381, 2020. 1
[19] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML , pages 8748–8763, 2021. 2
[20] Mohammad Sabokrou, Mahmood Fathy, Mojtaba Hoseini,
and Reinhard Klette. Real-time anomaly detection and lo-calization in crowded scenes. In CVPR , pages 56–62, 2015.
3
[21] Mohammad Sabokrou, Mohammad Khalooei, Mahmood
Fathy, and Ehsan Adeli. Adversarially learned one-classclassiﬁer for novelty detection. In CVPR , pages 3379–3388,
2018. 3
[22] Hitesh Sapkota and Qi Yu. Bayesian nonparametric submod-
ular video partition for robust anomaly detection. In CVPR ,
pages 3212–3221, 2022. 3
[23] Fangtao Shao, Jing Liu, Peng Wu, Zhiwei Yang, and
Zhaoyang Wu. Exploiting foreground and background sepa-ration for prohibited item detection in overlapping x-ray im-ages. PR, 122:108261, 2022. 1
[24] Giulia Slavic, Abrham Shiferaw Alemaw, Lucio Marcenaro,
David Martin Gomez, and Carlo Regazzoni. A kalman vari-
ational autoencoder model assisted by odometric clustering
for video frame prediction and anomaly detection. IEEE TIP ,
32:415–429, 2022. 3
[25] Sainbayar Sukhbaatar, ´Edouard Grave, Piotr Bojanowski,
and Armand Joulin. Adaptive attention span in transform-ers. In ACL, pages 331–335, 2019. 2,5
[26] Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world
anomaly detection in surveillance videos. In CVPR , pages
6479–6488, 2018. 1,3,6,7
[27] Yu Tian, Guansong Pang, Yuanhong Chen, Rajvinder Singh,
Johan W Verjans, and Gustavo Carneiro. Weakly-supervisedvideo anomaly detection with robust temporal feature mag-nitude learning. In ICCV , pages 4975–4986, 2021. 1,3,7
[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and IlliaPolosukhin. Attention is all you need. NeurIPS , 30, 2017. 7
[29] Chao Wang, Jing Liu, Kai Wu, and Zhaoyang Wu. Solving
multitask optimization problems with adaptive knowledgetransfer via anomaly detection. IEEE TEC , 26(2):304–318,
2021. 1
18907
[30] Mengmeng Wang, Jiazheng Xing, and Yong Liu. Actionclip:
A new paradigm for video action recognition. arXiv preprint
arXiv:2109.08472 , 2021. 3
[31] Xuanzhao Wang, Zhengping Che, Bo Jiang, Ning Xiao, Ke
Yang, Jian Tang, Jieping Ye, Jingyu Wang, and Qi Qi. Robust
unsupervised video anomaly detection by multipath frameprediction. IEEE TNNLS , 33(6):2301–2312, 2021. 3
[32] Peng Wu and Jing Liu. Learning causal temporal relation
and feature discrimination for anomaly detection. IEEE TIP ,
30:3513–3527, 2021. 3,7
[33] Peng Wu, Jing Liu, and Fang Shen. A deep one-class neural
network for anomalous event detection in complex scenes.IEEE TNNLS , 31(7):2609–2622, 2019. 3
[34] Peng Wu, Jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao,
Zhaoyang Wu, and Zhiwei Yang. Not only look, but alsolisten: Learning multimodal violence detection under weaksupervision. In ECCV , pages 322–339, 2020. 3,6,7
[35] Peng Wu, Jing Liu, Xiangteng He, Yuxin Peng, Peng Wang,
and Yanning Zhang. Towards video anomaly retrieval fromvideo anomaly detection: New benchmarks and model.arXiv preprint arXiv:2307.12545 , 2023. 1
[36] Peng Wu, Xuerong Zhou, Guansong Pang, Lingru Zhou,
Qingsen Yan, Peng Wang, and Yanning Zhang. Vadclip:
Adapting vision-language models for weakly supervised
video anomaly detection. In AAAI , pages 6074–6082, 2024.
3
[37] Dan Xu, Elisa Ricci, Yan Yan, Jingkuan Song, and Nicu
Sebe. Learning deep representations of appearance andmotion for anomalous event detection. arXiv preprint
arXiv:1510.01553 , 2015. 3
[38] Dan Xu, Yan Yan, Elisa Ricci, and Nicu Sebe. Detecting
anomalous events in videos by learning deep representationsof appearance and motion. CVIU , 156:117–127, 2017. 3
[39] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko,
Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, andChristoph Feichtenhofer. Videoclip: Contrastive pre-trainingfor zero-shot video-text understanding. arXiv preprint
arXiv:2109.14084 , 2021. 3
[40] Minghui Yang, Jing Liu, Zhiwei Yang, and Zhaoyang Wu.
Slsg: Industrial image anomaly detection by learning bet-ter feature embeddings and one-class classiﬁcation. arXiv
preprint arXiv:2305.00398 , 2023. 1
[41] Zhiwei Yang, Jing Liu, and Peng Wu. Bidirectional retro-
spective generation adversarial network for anomaly detec-
tion in videos. IEEE Access , 9:107842–107857, 2021. 3
[42] Zhiwei Yang, Peng Wu, Jing Liu, and Xiaotao Liu. Dy-
namic local aggregation network with adaptive clusterer foranomaly detection. In ECCV , pages 404–421, 2022. 3
[43] Zhiwei Yang, Jing Liu, Zhaoyang Wu, Peng Wu, and Xiaotao
Liu. Video event restoration based on keyframes for videoanomaly detection. In CVPR , pages 14592–14601, 2023. 1
[44] Muchao Ye, Xiaojiang Peng, Weihao Gan, Wei Wu, and Yu
Qiao. Anopcn: Video anomaly detection via deep predictivecoding network. In ACM MM , pages 1805–1813, 2019. 3
[45] Muhammad Zaigham Zaheer, Arif Mahmood, Marcella
Astrid, and Seung-Ik Lee. Claws: Clustering assisted weaklysupervised learning with normalcy suppression for anoma-
lous event detection. In ECCV , pages 358–376, 2020. 3,
7
[46] Muhammad Zaigham Zaheer, Jin-Ha Lee, Arif Mahmood,
Marcella Astrid, and Seung-Ik Lee. Stabilizing adversarially
learned one-class novelty detection using pseudo anomalies.IEEE TIP , 31:5963–5975, 2022.
3
[47] Muhammad Zaigham Zaheer, Arif Mahmood, Muham-
mad Haris Khan, Mattia Segu, Fisher Yu, and Seung-IkLee. Generative cooperative learning for unsupervised videoanomaly detection. In CVPR , pages 14744–14754, 2022. 7
[48] Luca Zanella, Benedetta Liberatori, Willi Menapace, Fabio
Poiesi, Yiming Wang, and Elisa Ricci. Delving into cliplatent space for video anomaly recognition. arXiv preprint
arXiv:2310.02835 , 2023. 3
[49] Vitjan Zavrtanik, Matej Kristan, and Danijel Sko ˇcaj. Draem-
a discriminatively trained reconstruction embedding for sur-face anomaly detection. In ICCV , pages 8330–8339, 2021.
1
[50] Shuangfei Zhai, Yu Cheng, Weining Lu, and Zhongfei
Zhang. Deep structured energy based models for anomalydetection. In ICML , pages 1100–1109, 2016. 3
[51] Chen Zhang, Guorong Li, Yuankai Qi, Shuhui Wang, Laiyun
Qing, Qingming Huang, and Ming-Hsuan Yang. Exploitingcompleteness and uncertainty of pseudo labels for weaklysupervised video anomaly detection. In CVPR , pages 16271–
16280, 2023. 1,3,7
[52] Jiangong Zhang, Laiyun Qing, and Jun Miao. Temporal con-
volutional network with complementary inner bag loss forweakly supervised anomaly detection. In ICIP , pages 4030–
4034, 2019. 1
[53] Jia-Xing Zhong, Nannan Li, Weijie Kong, Shan Liu,
Thomas H Li, and Ge Li. Graph convolutional label noisecleaner: Train a plug-and-play action classiﬁer for anomaly
detection. In CVPR , pages 1237–1246, 2019. 1,3,7
[54] Hang Zhou, Junqing Yu, and Wei Yang. Dual memory units
with uncertainty regulation for weakly supervised video
anomaly detection. In AAAI , pages 3769–3777, 2023. 3,
7
[55] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
Liu. Learning to prompt for vision-language models. IJCV ,
130(9):2337–2348, 2022. 3,4
[56] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp
Kr¨ahenb ¨uhl, and Ishan Misra. Detecting twenty-thousand
classes using image-level supervision. In ECCV , pages 350–
368, 2022. 3
18908
