LoS: Local Structure-guided Stereo Matching
Kunhong Li1,2Longguang Wang4Ye Zhang1,2Kaiwen Xue5Shunbo Zhou5Yulan Guo3∗
1Sun Yat-Sen University2The Shenzhen Campus of Sun Yat-Sen University3National University of Defense Technology
4Aviation University of Air Force5Huawei Cloud Computing Technologies Co., Ltd.
Figure 1. Example results of LoS on Middlebury, ETH3D, KITTI2012 and Holopix 50k. In each example, we show the left image,
estimated disparity map and slant plane. Note that, these are all unseen samples for LoS.
Abstract
Estimating disparities in challenging areas is difficult
and limits the performance of stereo matching models. In
this paper, we exploit local structure information (LSI) to
better handle these areas. Specifically, our LSI comprises
a series of key elements, including the slant plane (param-
eterised by disparity gradients), disparity offset details and
neighbouring relations. This LSI empowers our method
to effectively handle intricate structures, including object
boundaries and curved surfaces. We bootstrap the LSI
from monocular depth and subsequently refine it to bet-
ter capture the underlying scene geometry constraints in
an iterative manner. Building upon the LSI, we introduce
the Local Structure-Guided Propagation (LSGP), which en-
hances the disparity initialization, optimization, and refine-
ment processes. By combining LSGP with a Gated Re-
current Unit (GRU), we present our novel stereo matching
method, referred to as LocalStructure-guided stereo match-
ing (LoS). Remarkably, LoS achieves top-ranking results on
four widely recognized public benchmark datasets (ETH3D,
Middlebury, KITTI 15 & 12) and robust vision challenge,
demonstrating the superior capabilities of our model.
1. Introduction
The primary objective of stereo matching is to identify ac-
curate correspondences, referred to as disparities, between
pairs of input images. Existing learning-based methods
∗Corresponding author: Yulan Guo (yulan.guo@nudt.edu.cn).commonly regress the disparity map based on the raw fea-
ture correlations/costs. Specifically, most previous meth-
ods adopt filtering-based techniques to refine the cost vol-
ume and then regress accurate disparities. These methods
first construct a 3D/4D cost volume and then filter this vol-
ume using 2D/3D convolutional neural networks (CNNs).
However, these methods require a pre-defined disparity
range to produce satisfactory results. To remedy this lim-
itation, another group of methods [18, 20, 46, 51] lever-
age optimization-based technique to optimize disparities di-
rectly using a 2D convolutional gated recurrent unit (Con-
vGRU) without relying on pre-defined disparity ranges.
Nonetheless, these methods usually require a substantial
number of iterations to achieve convergence.
Challenging areas in stereo pairs are the primary reason
why optimization-based methods necessitate numerous it-
erations to yield satisfactory results. As shown in Fig. 2,
these challenging areas in the left image of a stereo pair
encompass: Class 1, regions on the left side that are out-
side the visible range of the right view. Class 2, occluded
areas on the left side of foreground objects. Class 3, tex-
tureless areas. Class 4, edge areas (due to blurry edges
and image downsampling [4, 51]). For these challenging
areas, accurate disparities cannot be obtained solely from
the appearance information in these areas due to the am-
biguity of pixel correspondence. Instead, geometry and
depth cues in a wider neighboring areas should be adopted
to reason accurate disparities. To this end, several efforts
have been made to formulate local geometry as a slant
plane[4, 14, 35, 36, 42], which is represented using either
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
19746
Class 1 Class 2 Class 3
 22 0
 22 0
 22 0
(a) The left image of a stereo pair (b) The number of iterations 
required for convergence
Class 1 Class 2 Class 3
 22 0
 22 0
 22 0
(a) The left image of a stereo pair (b) The number of iterations 
required for convergenceClass 4Figure 2. An illustration of four categories of challenging areas,
including left-side areas that are out of right view’s visible range
(class 1), occluded areas (class 2), textureless areas (class 3), and
edge areas (class 4). The number of iterations to achieve conver-
gence that required for RAFT-Stereo [20] is shown in (b).
the plane’s normal vector [4, 14, 35, 42] or the disparity
gradient [36]. However, the slant plane is limited to simple
planar structures and cannot well model non-planar geome-
tries, such as object boundaries and curved surfaces.
In this paper, we propose a novel representation of local
structural information (LSI) by combining the slant plane
with disparity offset details and neighbor relations. Our pro-
posed LSI explicitly characterizes local structures and ex-
hibits improved performance in modeling non-planar struc-
tures. On top of this LSI representation, we introduce a
local structure-guided propagation (LSGP) method to op-
timize the disparity map by propagating low-uncertainty
disparities while updating high-uncertainty ones. Integrat-
ing our approach into an optimization-based framework, we
present a stereo matching method called LoS, which stands
forLocalStructure-guided stereo matching.
Our primary contributions can be summarized as:
• We propose LoS, a local structure-guided stereo matching
method that integrates structure information to improve
stereo matching performance in challenging areas.
• We introduce local structure-guided propagation (LSGP)
to explicitly leverage structure information for updating
disparities in challenging areas.
• Extensive experiments on four popular public bench-
marks demonstrate the effectiveness of LoS.
2. Related Work
In this section, we first review stereo matching methods us-
ing filtering-based and optimization-based techniques for
disparity regression. Then, we discuss recent methods that
exploits local structure information for disparity estimation.
2.1. Filtering-based Methods
Filtering-based methods commonly employ 2D/3D CNNs
to process 3D/4D cost volumes for disparity regression.
Following the traditional pipeline, Zbontar and LeCun [49]
replace the hand-crafted cost with learned matching score,
and regularize it with semi-global matching (SGM) [15].DispNet [24] is the first end-to-end stereo matching method
that introduces the explicit correlation computation into
disparity estimation. To boost the performance of stereo
matching, 4D cost volumes are constructed to represent the
scene geometry. The cost volume can be built with con-
catenated features [8, 17] or group-wise correlation [12]
on a fixed scale [8, 12, 17, 19, 26] or multiple lev-
els [22, 31, 32]. Since these methods achieve cost aggre-
gation/regularization with 3D CNNs, they usually relies on
a preset disparity range to reduce memory cost. To further
reduce computational and memory costs, commonly used
the techniques include compressing the channel dimension
of cost volume and aggregating the cost with 2D convolu-
tion [47], and enforcing limit on the size of 4D cost vol-
ume [10, 13, 44]. However, these methods usually suffer
from performance drop. Additionally, filtering-based meth-
ods are usually poor in generalization [32, 50].
2.2. Optimizing-based Methods
Optimization-based methods iteratively update the dispar-
ity in an optimization-based framework. Specifically, GRU-
based optimization methods [37, 40, 48] show great power
in dense correspondence matching tasks. Inspired by [37],
Lipson et al. propose RAFT-Stereo [20], which iteratively
optimizes the disparity map with multi-level ConvGRU and
multi-level cost volumes. Li et al. propose CREStereo [18]
by introducing a cascade optimization architecture and an
adaptive sampling strategy to enhance model performance
in practical applications. Zhao et al. propose DLNR [51]
to hold the detail information in feature maps using a
decoupled Long Short-Term Memory (LSTM). Although
these optimization methods achieve remarkable perfor-
mance, they need dozens of iterations to achieve conver-
gence. To alleviate this problem, Xu et al . [46] propose
IGEV to process the cost volume using a light-weighted 3D
CNN as the structure information for GRU updating. How-
ever, due to the heavy memory cost and poor generalization
ability of 3D CNN, the structure information provided by
the cost volume is limited by the disparity range and some-
times noisy (containing artifacts).
2.3. Stereo Matching with Local Structure
Slant plane is widely applied in stereo matching [4, 6, 23,
35]. The plane parameters, either single-scale [4, 33] or
multi-scale [6], are usually random initialized [4, 14, 35]
or initialized from sparse matching [33], and are usually
iteratively optimized [4, 6, 14, 33, 35]. Bleyer et al . [4]
propose PatchMatchStereo, a method that employs random
initialization of normal vectors and refines them through
patchmatch propagation. Chakrabarti et al . [7] propose
CoRStereo based on multi-level slant plane represented
by normal vectors and optimized by a consensus frame-
work. Recently, the slant plane is introduced into deep-
19747
(a) The pipeline of LoStDisparity
Updating
Ir
Il
 DhrUpsampling
&
RefinementInitialization Step Optimization Step
Monocular 
Branch
Binocular
Branch
(b) Monocular branch (c) Binocular branchPAM Extractor
D0
LSGP
(N1 )    pLSGP
(N1 )    p
Ir
IlMonocular Branch
Draw
δraw
 δ0MidasMidas AlignUpCCBinocular Branch
FctxFstat0Fstat0
Il
DM
 DmonoLSI InitContext Network
Monocular Depth NetworkDrawδraw Drawδraw S0Drawδraw Drawδraw S0
(d) Disparity updating (e) Upsampling and refinementCC Concatenation
UpBilinear 
Upsample
Align Scale Alignment
MFEMotion Feature 
Encoder
δ0
S0
D0
SNu
DNu
δNu
ClrCrlFlFr
D0S0
δ0
Fstat0Fstat0S1
δ1
Uδ1
UD1
UD1
U
ΔSΔDS1
D1
δ1++
++……
……
……
……Slr
Dlr
δlr
FstatlrFstatlr
Convex
Upsample
++
++ DhrDNuDNu
δNuδNu
FstatNuFstatNuSNuSNu
LSGP
(N2 )    pLSGP
(N2 )    pLSGP
(N2 )    pLSGP
(N2 )    p
ΔSΔDLocal Structure 
Information
Disparity Map
Uncertainty Map
Hidden StateMFE MFE
Head
Fstat1Fstat1Head
Multi -level ConvGRUDhr
U
δhr
UShr
(a) The pipeline of LoS
Binocular Branch
DmonoAlign
DMCCC
IlContext Network
Monodepth 
NetworkMidasMidasFctx
Fstat0Fstat0Fctx
Fstat0Drawδraw Drawδraw S0
LSI Init
D0
δ0
Draw
δraw
 Ir
Il
Ir
IlMonocular Branch
FeaturesClrCrlFlFrClrCrlFlFr
LSGP
(N1 )    pLSGP
(N1 )    pS0
(d) Disparity updating (e) Upsampling and refinementD0S0
δ0
Fstat0Fstat0S1
δ1
Uδ1
UD1
UD1
U
ΔSΔDS1
D1
δ1++
++……
……
……
……Slr
Dlr
δlr
FstatlrFstatlr
Convex
Upsample
++
++ DhrDNuDNu
δNuδNu
FstatNuFstatNuSNuSNu
LSGP
(N2 )    pLSGP
(N2 )    pLSGP
(N2 )    pLSGP
(N2 )    p
ΔSΔDLocal Structure 
Information
Disparity Map
Uncertainty Map
Hidden StateMFE MFE
Head
Fstat1Fstat1Head
Multi -level ConvGRUDhr
U
δhr
UShrCC
ConcatenationC
Concatenation
Align
Scale AlignmentAlign
Scale Alignment
LSI Init
Local Structure Information 
InitializationLSI Init
Local Structure Information 
Initialization
Features
Parallax Attention Module 
Feature ExtractorFeatures
Parallax Attention Module 
Feature Extractor
LSGP ( N )
Local Structure Guided 
Propagation with N iterationsLSGP ( N )
Local Structure Guided 
Propagation with N iterations
MFE
Motion Feature EncoderMFE
Motion Feature EncoderDisparity
Updating
Ir
Il
 DhrUpsampling
&
Refinement
D0
D0
S0
S0
δ0
δ0
DNuDNu
DNu
SNuSNu
SNu
δNuδNu
δNuMonocular 
Branch
Binocular
BranchMonocular 
Branch
Binocular
Branch
ShrDisparity
Updating
Ir
Il
 DhrUpsampling
&
Refinement
D0
S0
δ0
DNu
SNu
δNuMonocular 
Branch
Binocular
Branch
Shr
(b) Monocular branch (c) Binocular branch
(a) The pipeline of LoS
Binocular Branch
DmonoAlign
DMCCC
IlContext Network
Monodepth 
NetworkMidasMidasFctx
Fstat0Fstat0Fctx
Fstat0Drawδraw Drawδraw S0
LSI Init
D0
δ0
Draw
δraw
 Ir
Il
Ir
IlMonocular Branch
FeaturesClrCrlFlFrClrCrlFlFr
LSGP
(N1 )
(d) Disparity updating (e) Upsampling and refinementD0S0
δ0
Fstat0Fstat0S1
δ1
Uδ1
UD1
UD1
U
ΔSΔDS1
D1
δ1++
++……
……
……
……Slr
Dlr
δlr
FstatlrFstatlr
Convex
Upsample
++
++ DhrDNuDNu
δNuδNu
FstatNuFstatNuSNuSNu
LSGP
(N2 )LSGP
(N2 )
ΔSΔDLocal Structure 
Information
Disparity Map
Uncertainty Map
Hidden StateMotion 
Feature 
Encoder
Head
Fstat1Fstat1Head
Multi -level ConvGRUDhr
U
δhr
UShrCC
ConcatenationC
Concatenation
Align
Scale AlignmentAlign
Scale Alignment
LSI Init
Local Structure Information 
InitializationLSI Init
Local Structure Information 
Initialization
Features
Parallax Attention Module 
Feature ExtractorFeatures
Parallax Attention Module 
Feature ExtractorDisparity
Updating
Ir
Il
 DhrUpsampling
&
Refinement
D0
D0
S0
S0
δ0
δ0
DNuDNu
DNu
SNuSNu
SNu
δNuδNu
δNuMonocular 
Branch
Binocular
BranchMonocular 
Branch
Binocular
Branch
ShrDisparity
Updating
Ir
Il
 DhrUpsampling
&
Refinement
D0
S0
δ0
DNu
SNu
δNuMonocular 
Branch
Binocular
Branch
Shr
(b) Monocular branch (c) Binocular branch
Motion 
Feature 
Encoder
S0Initialization Step Optimization Step
Initialization Step Optimization Step
LSGP ( N )
Local Structure Guided 
Propagation with N iterationsLSGP ( N )
Local Structure Guided 
Propagation with N iterationsFigure 3. The architecture of LoS. We show the overall pipeline in (a), and illustrate the details of monocular branch (b), binocular branch
(c), disparity updating (d) and upsampling and refinement (e). Note that in (d) and (e), we omit the variables Fl,FrandFctx, which are
used but not updated by GRU and LSGP, and we simplify the Siupdating procedure, which is detailed in steps 5 and 6 of Alg.1.
learning based methods. Wang et al . [42] introduce the
slant plane into correlation sampling and disparity refine-
ment with learned plane parameters. Tankovich et al. [36]
use the tile representation, which contains several pixels and
a slant plane represented by disparity gradient, to efficiently
propagate information and achieve accurate disparity and
local structure estimation.
3. Methodology
Our LoS consists of an initialization step and an optimiza-
tion step, with the core module being the LSGP, as illus-
trated in Fig. 3(a). Given a stereo pair {Il,Ir}with dimen-
sions sH×sW, where s= 4is the spatial scale factor. The
initialization step initializes the disparity map D0and lo-
cal structure information S0for the optimization step. The
optimization step iteratively updates D0andS0forNiter-
ations, resulting in DNandSN, which are then upsampled
and refined to obtain the final disparity map Dhrwith a res-
olution of sH×sW.
3.1. Local Structure-guided Propagation
The objective of our LSGP module is to propagate low-
uncertainty disparities to update high-uncertainty dispari-
ties under the guidance of local structure information (LSI).
Compared to the previous GRU-based propatation tech-
nique, our approach eliminates time-consuming operations
(such as feature warping for correlation calculation, correla-
tion sampling from volumes), making our propagation sig-
nificantly more efficient, see Fig. 5(b).Local Structure Information (LSI). For a pixel at loca-
tionp= (h, w), its neighboring window is defined as a 3×3
square centered at p:N(p) ={pi|pi= (hi, wi)}, where
piis constrained with |hi−h| ≤1,|wi−w| ≤1. Then,
we use the LSI S(p)to describe the local structure of N(p).
The LSI Sis based on the slant plane, which is represented
by disparity gradients G. Since Gis only limited to planar
structures, we further introduce disparity offset details O
and local relations Rto model non-planar structures such
as edges and curve surfaces. Therefore, the LSI is defined
asS={G,O,R}. Here, Gwith dimensions Hp×Wp×2
describes the horizontal and vertical gradients of disparities,
whileOandRare with dimensions Hp×Wp×9. The spa-
tial dimension (Hp, Wp)∈ {(H, W ),(sH, sW )}.
p
pi{
o( p,p i )O( p,p i )G( p)·Δpi 
Disparity
PositionG( p)·Δpi 
O( p,p i )o( p,p i )p
pi
Disparity
PositionG( p)·Δpi 
O( p,p i )o( p,p i )
p piD( p)
D( pi)
Disparity
PositionG( p)·Δpi 
O( p,pi )o( p,pi )
p piD( p)
D( pi)SurfaceSlant Plane
Figure 4. 1d illustration for Eq. 2.Propagation Process.
Given a disparity map D,
an uncertainty map δand
LSIS={G,O,R}, we
propagate neighboring dis-
parities D(N(p))and uncertainties δ(N(p))to update
D(p)andδ(p)according to S(p):
Dk(p) =X
pi∈N(p)wk(p,pi)(Dk−1(pi) +o(p,pi)),
δk(p) =X
pi∈N(p)wk(p,pi)δk−1(pi),(1)
the subscript kdenotes the k-th iteration, and the lowercase
ois the disparity offsets derived from GandO:
o(p,pi) =G(p)·∆pi+O(p,pi), (2)
where ∆pi=p−piis the pixel offset vector. ois com-
puted only once for each propagation process. Besides, wk
19748
（251，248，116）（ 197，059，087）（ 181，048，098）
（255，222，074）（ 251，248，116）（ 245，106，040）
（255，188，026）（ 255，222，074）（ 251，248，116）
Dk-1(N (p))
Dk-1 Dknegative 
offsetpositive 
offsetnegative 
offsetpositive 
offset
small 
weightlarge 
weightsmall 
weightlarge 
weighto(p,N (p)) o(p,N (p))
wk
SoftmaxDk-1Dk-1(N (p))o(p,N (p)) o(p,N (p)) negative 
offsetpositive 
offsetnegative 
offsetpositive 
offset
o(p,N (p))
Dk-1(N (p))
Dk-1
 Dk
wk(p,N (p)) small 
weightlarge 
weightsmall 
weightlarge 
weightSum
Softmaxo(p,N (p))
Dk-1(N (p))
Dk-1 Dk
wk(p,N (p)) small 
weightlarge 
weightsmall 
weightlarge 
weightSum
Dk(p)negative 
offsetpositive 
offsetnegative 
offsetpositive 
offset
o(p,N (p))
Dk-1(N (p))
Softmax wk(p,N (p)) small 
weightlarge 
weightsmall 
weightlarge 
weightSum
Dk(p)Dk(p)
negative 
offsetpositive 
offsetnegative 
offsetpositive 
offsetnegative 
offsetpositive 
offsetnegative 
offsetpositive 
offset
Dk-1 Dk
Dk-1 Dko(p,N (p))
Dk-1(N (p))SumDk(p)
Soft
maxwk(p,N (p)) r(p,N (p))
negative 
offsetpositive 
offsetnegative 
offsetpositive 
offset
small 
weightlarge 
weightsmall 
weightlarge 
weight
δk-1 δkδk(p)+0.1
δk-1(p,N (p)) Sum
Dk-1 Dko(p,N (p))
Dk-1(N (p)) SumDk(p)
Soft
maxwk(p,N (p)) r(p,N (p))
 small 
weightlarge 
weightsmall 
weightlarge 
weight
δk-1 δkδk(p)+0.1
δk-1(p,N (p)) Sum
Dk-1 Dko(p,N (p))
Dk-1(N (p)) SumDk(p)
Eq. (3) wk(p,N (p)) r(p,N (p))
δk-1 δkδk(p)+0.1
δk-1(N (p)) Sumsmall large 0small large 0
(a) T he k-th propagation proces
GRU (CREStereo)
GRU (Raft -Stereo)
LSGP 0.400s 0.325s 0.075s 0.325s 0.075s 0.400s 0.325s 0.075s
0.195s 0.102s 0.195s 0.102s 0.297s 0.195s 0.102s 0.297s
0.010s0.010s
Feature Warping/ Correlation SamplingCorrelation Generation Updating/Propagation
(b) Time consumption of 50 iterations  
Disparity map  Dk-1 Dko(p,N (p))
Dk-1(N (p)) SumDk(p)
Eq. (3) wk(p,N (p)) r(p,N (p))
Uncertainty map  δk-1 δkδk(p)+0.1
δk-1(N (p)) Sum
(a) T he k-th propagation proce ss
GRU (CREStereo)
GRU (RAFT -Stereo)
LSGP 0.400s 0.325s 0.075s 0.325s 0.075s 0.400s 0.325s 0.075s
0.195s 0.102s 0.195s 0.102s 0.297s 0.195s 0.102s 0.297s
0.010s0.010s
Correlation Operation Updating/Propagation
(b) Time consumption of 50 updating iterations  Constrained 
local relationDisparity offsets
Local weights
large 0Figure 5. Local Structure-Guided Propagation. We illustrate the k-
th propagation process in (a), and compare the time consumption
of GRU updating and LSGP on images with 960×640resolution
for 50 iterations in (b).
is the weights andP
pi∈N(p)wk(p,pi) = 1 . To suppress
the spreading of high-uncertainty disparities, we split N(p)
intoN+(p)andN−(p)according to δk−1.N+(p)con-
tains all the low-uncertainty neighbours piofp, where low
uncertainty is defined as δk−1(pi)≤δk−1(p) +δwith
the margin δ= 0.1. Note that during training, we set
N+(p) =N(p)andN−(p) =∅to enhance model ro-
bustness. wkis computed according to this split:
wk(p,pi) =

exp(ˆδk−1(pi)r(p,pi))P
pj∈N+exp(ˆδk−1(pj)r(p,pj)),ifpi∈ N+
0 ,ifpi∈ N−,
(3)
where ˆδk−1andrare derived from uncertainty map δk−1
and local relations R. To achieve numerical stability and
ensure that δalways takes effect, all elements in rare con-
strained to be smaller than -1, i.e.r(p,pi) =R(p,pi)−
max(R(p))−1. Since the elements of rare always smaller
than -1, we use uncertainty instead of confidence as the scal-
ing factor. To ensure that Ralways takes effect, we use
ˆδk−1=δk−1+ 0.1.
Uncertainty vs. Local Relations. Uncertainties and lo-
cal relations play different roles in Eq. 3. The uncertainty
δ(N(p))is the state of a pixel to indicate how reliable the
current disparity value is, so it should keep the same updat-
ing tracks as the disparity value. Therefore, δ(N(p))is up-
dated with the same weight as D(N(p))in Eq. 1. While the
local relation R(p,pi)is the inherent correlation between
the pixel pairs, and should remain unchanged in LSGP.
Model Architecture. There is no learnable parametersin LSGP, all variables are updated by GRU. To adapt LSGP,
we slightly modify the multi-level GRU used in RAFT-
Stereo [20]: 1) We expand the output channel of the dis-
parity head to 13, allocating 1 channel for disparity redisual
∆D, 1 for uncertainty δi
U, 2 for disparity gradient residual
∆Gand 9 for disparity offset details residual ∆O(refer to
step 3 in Alg. 1). 2) We introduce a two-layer head Φto
initialize and update local relations R.
3.2. Initialization Step
The initialization step consists of two branches. The binoc-
ular branch initially passes raw disparity map Drawand un-
certainty map δrawto the monocular branch, and then the
monocular branch initialize the LSI S0and feeds it back to
the binocular branch.
3.2.1 Monocular Branch
In the monocular branch, Ilis encoded into a context feature
mapFctxand hidden state F0
statusing a context network.
Besides, we also introduce the depth prior by estimating the
monocular depth of Il. The monocular depth network is an
off-the-shelf model, MiDaS [3, 28], with a fixed I/O size of
384×384. The monocular depth generated by MiDaS is
then aligned to the true scale and upsampled to create the
disparity map Dmono with a spatial dimension of H×W.
Scale Alignment. MiDaS outputs an up-to-scale depth
mapDM, where each pixel’s depth is represented as inverse
depth, akin to virtual disparity. To align monocular depth
DMwith raw disparity Draw, we calculate scale and shift
factors by solving a weighted least square problem:
sd, td= arg min
sd,td384∗384X
i=1O(i),
O(i) = (1 −δraw(i))(sdDM(i) +td−Draw(i))2.(4)
BothDrawandδraware resampled to 384×384to match
the size of DM.
Finally, Dmono is computed and upsampled to the size
ofH×W:Dmono =Bilinear Upsample (sdDM+td).
Although Dmono is aligned to the estimated true scale, we
do not use it as the initial disparity map directly due to the
lacks of object details. Instead, we use Dmono as a prior to
initialize S0and then optimize Drawwith LSGP.
LSI Initialization. We first initialize the local relations
R0= Φ(cat( Draw,Dmono,Fctx,F0
stat)), where cat() rep-
resents the concatenation operator. Subsequently, we have
o0(p,pi) =Dmono(p)−Dmono(pi), and initialize dis-
parity gradients G0and disparity offset details O0:
G0(p) = arg min
G(p)X
pi∈N(p)w(p,pi)(G(p)·∆pi−o0(p,pi))2,
O0(p,pi) =o0(p,pi)−G0(p)·∆pi,
(5)
where w(p,pi)is computed from Eq. 3 based on δraw,R0
andN+(p) =N(p). And we have S0={G0,O0,R0}.
19749
3.2.2 Binocular Branch
In the binocular branch, IlandIrare encoded into Fl,Fr,
Clr,Crl,Draw, andδrawusing a Parallax Attention Mech-
anism (PAM)-based feature extractor [41]. Subsequently,
Drawandδraware optimized as D0andδ0with LSGP.
PAM Extractor. In the PAM extractor, the input im-
ages are first progressively downsampled to 1/32 resolution
and then gradually upsampled to 1/4 resolution using a Res-
UNet. The feature pyramids from the Res-UNet are then
fed into four parallax attention modules to produce the cor-
respondence feature maps FlandFrand two correlation
matrices: Clrwith dimensions H×Wl×WrandCrlwith
dimensions H×Wr×Wl. Note that both ClrandCrlare
softmaxed along the last dimension. Finally, Drawis re-
gressed from Clrandδrawis estimated from the left-right
consistency between ClrandCrl:
Draw(h, wl) =wlX
wr=0(wl−wr)Clr(h, wl, wr),
δraw(h, wl) =wlX
wr=0Clr(h, wl, wr)Crl(h, wr, wl).(6)
LSGP. LSGP is applied on resolution (Hp, Wp) =
(H, W )withN1iterations, as shown in Fig 3(c). Since the
disparity map Draware extremely noisy, N1is relatively
large to ensure sufficient propagation.
3.3. Optimization Step
3.3.1 Disparity Updating
Disparity updating contains Niterations. For the i-th iter-
ation, previous disparity map Di−1, uncertainty map δi−1
and LSI Si−1are first updated by a multi-level ConvGRU to
obtain Di
U,δi
UandSi. Subsequently, Di
Uandδi
Uare fur-
ther updated by LSGP based on Si. The process of a single
iteration is illustrated in Fig. 3(d) and Alg. 1.
Algorithm 1 A Single Iteration for Disparity Updating
1:Fi−1
mot←MFE(Fl,Fr,Di−1,δi−1,Si−1)
2:Fi
stat←ConvGRU (Fi−1
stat,Fi−1
mot,Fctx)
3:(∆D,∆G,∆O,δi
U)←Head(Fi
stat)
4:Di
U←Di−1+∆D
5:Ri←Φ(cat( Di
U,Dmono,Fctx,Fi
stat))
6:Si← {Gi−1+∆G,Oi−1+∆O,Ri}
7:Di,δi←LSGP (Di
U,δi
U,Si, N2)
Motion Feature. The motion feature Fi−1
motis generated
by a motion feature encoder (MFE). In MFE, we encode the
concatenated features cat(Di−1,δi−1,Si−1)and a small
dynamic cost volume Cwith two separate two-layer CNNs,
then fuse them together with a single layer CNN, and con-
catenate the fused feature maps with cat(Di−1,δi−1,Si−1)to obtain Fi−1
mot. The small dynamic cost volume Cwith di-
mensions G×D×H×Wis constructed by warping right
image feature Frand then computing group-wise correla-
tions:
C(g, d, h, w ) =1
CgCgX
c=1Fl(c, h, w )Fr(c, h′, w′),(7)
where g∈[0, G−1]is the group index, d∈[0, D−1]is
the depth index, and h′=h+fh(d),w′=w+fw(d)−
Di−1(h, w). We adopt a 2D-1D alternate local search
strategy [18] where, in 1D search mode, fh(d) = 0 and
fw(d)∈[−4,4], and in 2D search mode, fh(d)∈[−1,1]
andfw(d)∈[−1,1]. This setting results in Chaving a
depth D= 9, and we empirically set G= 8.
Multi-level ConvGRU and LSGP. The multi-level Con-
vGRU operates on four levels simultaneously, namely 1/4,
1/8, 1/16 and 1/32 resolutions. The GRUs are based on sep-
arable convolutions [18, 37] and are cross-connected [20],
implying that the hidden states of adjacent levels are also
the inputs to the current level. As shown in Fig 3(d), LSGP
works on resolution (Hp, Wp) = ( H, W )withN2itera-
tions.
3.3.2 Upsampling and Refinement
We employ convex upsampling [37] to reconstruct the dis-
parity map, uncertainty map and LSI to the original resolu-
tion, and then refine the disparity map with LSGP.
Upsampling. Convex upsampling treats the original res-
olution disparity values as a weighted sum of their coarse-
resolution neighbors in a 3×3grid. We apply an additional
GRU updating to align hidden states FN
stat and disparity
mapDN, resulting in Slr,δlrandDlr. Then, we upsam-
pleSlr,δlrandDlrto obtain Shr,δhr
UandDhr
U.
Refinement with LSGP. we apply LSGP with N2it-
erations to obtain Dhraccording to Dhr
U,Shrandδhr
U,
as shown in Fig 3(e). Here, LSGP works on resolution
(Hp, Wp) = (sH, sW ).
3.4. Supervision
Loss Functions. During training, we collect the interme-
diate outputs in disparity updating as a sequence, compute
thel1distance between the predicted values and the ground
truth values. The total loss composes of three parts:
L =Ld+Lo+Lg,
Ld=P2N
i=1γ2N−i∥Dgt−Di∥1,
Lo=PN
i=1γN−i∥ogt−oi∥1,
Lg=PN
i=1γN−i∥Ggt−Gi∥1(8)
where Di∈ {CUp(D1),CUp(D1
U),···,Dhr
U,Dhr}and
oiis computed with Eq. 2 based on {Oi,Gi} ⊆Si∈
{CUp(S1),···,Shr}. We set γ= 0.9andCUp() stands
for convex upsampling.
19750
RANSAC(unmasked)
RANSAC(masked) Least Square (ours)
RANSAC(unmasked)
RANSAC(masked)
Least Square (ours)Figure 6. An illustration of the disparity gradient labels. RANSAC
based method depends on densification pre-processing, and may
introduce artifacts on edges (see black boxes). Our weighted least
square solution directly works with semi-dense ground truth and
there are less noises. Best zoomed in.
Labels Generation. The label ogtis simply derived
fromDgtwith unfolding operator, while Ggtis generated
by solving a weighted least square problem. For a pixel p,
we compute the disparity gradient within a 9×9window
N9(p)centered at p:
Ggt(p) = arg min
G(p)X
pj∈N9(p)w(p,pj)(G(p)·∆pj−ogt(p,pj))2,
(9)
where wis the weight:
w(p,pj) =M(p,pj)exp(−d(p,pj)), (10)
where d(p,pj) =∥p−pj∥2
2+ (Dgt(p)−Dgt(pj))2.
Mis a binary mask, which is set to 1 if and only if
bothDgt(p)andDgt(pi)are valid. Compared to the
RANSAC-based implementation used in HITNet [36], our
disparity gradient label generation method offers two ad-
vantages: 1) It is efficient since the least square problem has
a closed-form solution, while the RANSAC-based method
requires several iterations. 2) It can work with semi-dense
ground truth directly, without the need for any densification
pre-processing. The disparity gradient label examples are
shown in Fig. 6.
4. Experiments
In this section, We mainly evaluate our LoS on different
benchmarks and analyse the LSGP. The ablation studies are
contained in the supplementary.
4.1. Benchmark Evaluations
We evaluate our LoS on four popular public benchmarks,
the quantitative results are shown in Table 1 and visual com-
parison results are illustrated in Fig. 7.
Datasets. We evaluate the proposed LoS on four pop-
ular public benchmarks, including ETH3D [30], Middle-
bury [29], KITTI 2012 [11] and KITTI 2015 [25]. For
model training, we collect data from various public datasets
to compose the basic training set (BTS), including Scene-
Flow [24], CRE [18], MPI-Sintel [5], FallingThings [39]
and Instereo2K [1]. There are 300,028samples in the BTS.Implementation Details. We implement our model us-
ing Pytorch [27] and train the model with the AdamW opti-
mizer [21]. The training process consists of 300ksteps, and
the input data is 640×480with a batch size of 16. We set the
max learning rate to 4e−4, the learning rate linearly warms
up from 5%to100% in the first 6ksteps ( 2%). Then, af-
ter180ksteps ( 60%), the learning rate is linearly decreased
from 100% to5%. We set N= 5,N1= 32 ,N2= 4during
training and N= 10 ,N1= 64 ,N2= 4for test.
ETH3D . Following CREStereo [18], we train the model
from scratch without fine-tuning. The full training set is
composed of the BTS and ETH3D training set, with ETH3D
being augmented to 2% of the full training set. We achieve
the best performance among all of the published methods
in terms of Bad1.0 metrics and achieve the state-of-the-art
performance on AvgErr metrics. For ETH3D, most of the
challenging areas belong to classes 3 and 4, which are co-
visible for the stereo pairs. Thus, our LSGP achieves con-
sistent improvement when being tested with all pixels and
with only non-occluded pixels.
Middlebury . The training strategy on Middlebury is the
same as ETH3D and the full training set is the combina-
tion of the BTS and Middlebury training set. LoS outper-
forms all existing methods in terms of AvgErr-all metric.
Since our LoS updates the disparities in challenging areas
for dozens of times with LSGP, it achieves the lowest av-
erage error. Compared to DLNR [51], our LoS achieves
comparable performance but is more efficient. Specifically,
LoS runs at 0.93 s/frame on RTX4090 while DLNR runs at
1.68 s/frame on Tesla A100.
KITTI 12 & KITTI 15 . We first train the models with
the BTS and then finetune the models for 50ksteps with the
combination of KITTI 12, KITTI 15 and BTS. The KITTIs
account for 80% in the finetuning dataset and the rest are
randomly sampled from BTS. During finetuning, the max
learning rate is 1e−4. We rank the first when testing within
reflective regions on KITTI 2012, because most of the re-
flective regions are from cars, which can be handled well by
LSGP. Additionally, we also achieves state-of-the-art per-
formance on other metrics of KITTI 12 & 15, which shows
the superiority of our LoS.
Robust Vision Challenge . Following [16], we first train
the model with ETH3D, Middlebury and BTS and then in-
troduce KITTIs to finetune the model for 50ksteps. During
finetuning, we augment the KITTIs to 50% and set the max
learning rate to 1e−4. We achieve the best overall perfor-
mance within the robust vision challenge settings, which
demonstrates that local structure guidance benefits the ro-
bustness of stereo matching.
4.2. Analyse LSGP
Efficiency Evaluation. We conduct an efficiency compar-
ison among RAFT-Stereo, CREStereo, IGEV , and LoS un-
der their default inference settings, as summarized in Ta-
19751
MethodETH3D Middlebury KITTI 2015 KITTI 2012
Bad1.0
allAvgErr
allBad1.0
nocAvgErr
nocBad2.0
allAvgErr
allBad2.0
nocAvgErr
nocD1-bg
allD1-fg
allD1-all
allD1-all
nocout-3
allout-3
nocout-3
all(R)out-3
noc(R)
PSMNet [8] - - - - - - - - 1.86 4.62 2.32 2.14 1.89 1.49 10.18 8.36
GwcNet [12] - - - - - - - - 1.74 3.93 2.11 1.92 1.70 1.32 9.28 7.80
LEAStereo [9] - - - - 12.1 2.89 7.15 1.43 1.40 2.91 1.65 1.51 1.45 1.13 6.50 5.35
AdaStereo [34] 3.34 0.25 3.09 0.24 19.8 3.39 13.7 2.22 2.59 5.55 3.08 2.83 - - - -
HITNet [36] 3.11 0.22 2.79 0.20 12.8 3.29 6.46 1.71 1.74 3.20 1.98 1.74 1.89 1.41 7.54 5.91
CFNet [31] 3.70 0.26 3.31 0.24 - - - - 1.54 3.56 1.88 1.73 1.58 1.23 7.29 5.96
RAFT-Stereo [20] 2.60 0.19 2.44 0.18 9.37 2.71 4.74 1.27 1.58 3.05 1.82 1.69 1.66 1.30 6.48 5.40
PCWNet [32] - - - - - - - - 1.37 3.16 1.67 1.53 1.37 1.04 6.20 4.99
ACVNet [45] 2.86 0.24 2.58 0.23 19.5 12.1 13.7 2.22 1.37 3.07 1.65 1.52 1.47 1.13 8.67 7.03
CREStereo [18] 1.09 0.14 0.98 0.13 8.13 2.10 3.71 1.15 1.45 2.86 1.69 1.54 1.46 1.14 7.27 6.27
DLNR [51] - - - - 6.98 1.91 3.20 1.06 1.60 2.59 1.76 1.61 - - - -
IGEV [46] 1.51 0.20 1.12 0.14 8.16 3.64 4.83 2.89 1.38 2.67 1.59 1.49 1.44 1.12 5.00 4.35
CroCo-Stereo [43] 1.14 0.15 0.99 0.14 11.1 2.36 7.29 1.76 1.38 2.65 1.59 1.51 - - - -
LoS(Ours) 1.03 0.15 0.91 0.14 8.03 1.75 4.20 1.12 1.42 2.81 1.65 1.52 1.38 1.10 4.45 3.47
iResNet ROB [19] 4.67 0.27 4.23 0.25 31.7 6.56 24.8 4.51 2.27 4.89 2.71 2.40 - - - -
CFNet RVC [31] 3.70 0.26 3.31 0.24 16.1 5.07 10.1 3.49 1.65 3.53 1.96 1.76 - - - -
CREStereo++ RVC [16] 1.70 0.16 1.59 0.15 9.46 2.20 4.68 1.28 1.55 3.53 1.88 1.75 - - - -
LoS RVC(Ours) 1.47 0.14 1.26 0.13 9.30 2.36 5.14 1.57 1.58 3.08 1.83 1.71 - - - -
Table 1. Results on four popular benchmarks. Top: Comparison with fine-tuned models. Bottom : Comparison with winners of the Robust
Vision Challenges 2018, 2020 and 2022. The second and third rows show the metrics and testing masks. All metrics are presented in
percentages except for AvgErr presented by pixels. For testing masks, “all” denotes being tested with all pixels while “noc” denotes being
tested with a non-occlusion mask. “(R)” denotes being tested within reflective regions. The best and second best are marked with colors.
Left image RAFT -Stereo (3DV21) CREStereo (CVPR22) IGEV -Stereo (CVPR23) LoS (Ours)
Middlebury ETH3DKITTI
2015KITTI
2012
Figure 7. Qualitative results on the test set of Middlebury, ETH3D, KITTI2015, and KITTI2012. Since ETH3D benchmark do not provide
the error map, we only show the zoomed highlight parts. Best zoomed in.
RAFT-Stereo [20] CREStereo [18] IGEV [46] LoS (Ours)
960×640 0.43s 0.41s 0.47s 0.31s
1920×1080 1.21s 0.99s 1.09s 0.73s
Table 2. Time consumption comparison between typical optimiza-
tion based methods. All the models are tested with the officially
released codes and default inference settings on a RTX4090 GPU.
ble 2. Our LoS is more efficient than the counterparts while
achieves better cross-dataset overall performance. This effi-
ciency improvement stems from the introduction of LSGP,
which are more efficient (as depicted in Fig 5(b)), to reduce
GRU iterations.Challenging Areas Evaluation. We evaluate the RAFT-
Stereo, IGEV and LoS on UnrealStereo4K [38] dataset with
four category masks. The process of mask generation is de-
tailed in supplementary. We use the parameters trained with
Middlebury, and since CREStereo [18] do not release their
Middlebury weights, we exclude CREStereo in this eval-
uation. As shown in Table 3, our LoS significantly outper-
forms the counterparts on AvgErr metric, and achieves com-
parable performance in terms of Bad2.0, which is consistent
with the results in Table 1. Additionally, LoS significantly
surpass the LoS model without LSGP in all metrics. The
results demonstrate that LSGP markedly improves the dis-
19752
(a)  left image 
(b)  left disparity (c)  class1 mask 
(d)  class2 mask (e)  class3 mask 
(f)  class4 mask Figure 8. Test sample from UnrealStereo4K dataset.
time
(s)overallclass1
(4.62%)class2
(3.72%)class3
(27.9%)class4
(5.74%)
RAFT-Stereo 3.34 15.7 / 14.11 36.3 / 32.15 47.5 / 16.54 23.5 / 19.96 48.2 / 20.81
IGEV 3.90 25.8 / 22.23 43.0 / 41.53 48.5 / 20.58 38.2 / 26.73 52.3 / 25.93
LoS 1.85 15.0 /6.28 29.1 /9.05 48.8 / 13.81 26.6 /7.08 51.3 /13.36
(w/o LSGP) 1.44 19.9 / 18.81 55.0 / 18.90 67.5 / 49.59 31.6 / 26.51 55.7 / 27.19
Table 3. Evaluation with different category masks on Unreal-
Stereo4K. We report Bad2.0(%)/AvgErr(pixel) metrics for each
method and class. We also report the average ratio for each class.
(a) Stereo pair (b) Disparity map
(c) Disparity offset o0 derived 
from Dmono
(d) Softmaxed local relations R0 
(e) Disparity offset olr derived 
from Glr, Olr
(f) Softmaxed local relations Rlr 
Figure 9. An illustration of LSI. We visualize oandRof two
stages. Since oandRare the representations for pixel-pairs
{(p,pi)|pi∈ N(p)}, we arrange the images into a 3×3grid
according to the relative position between pandpi. For clarity,
we limit |o| ≤0.1andsoftmax( R)≤0.5. Best zoomed in.
parity estimation accuracy in challenging areas, especailly
the class 1 region.
Understanding LSI. We visualize the LSI in Fig. 9. The
local relation constrains pixels to derive disparities from
neighboring pixels belonging to the same object. For ex-
ample, the third column of (d) and (f) are brighter on theleft side, which means the pixels on the left side of the
image (class1 regions) tend to obtain information from the
right neighbors. On the contrary, the pixels on the left side
of foreground objects (class2 regions) tend to obtain infor-
mation from their left neighbors, resulting in the brighter
regions in the first column of (d) and (f). It is also evi-
dent that the disparity offsets oeffectively manage bound-
aries and curved surfaces, indicated by the green boxes in
Fig. 9 (e). Comparing Fig. 9 (c) and (e), ooptimized by
GRU and is more accurate, which benefits the LSGP. The
ablation study also highlights the substantial enhancement
in LSI quality achieved through the iterative updating by
GRU, consequently resulting in a notable improvement in
performance. Please refer to the supplementary material for
further details.
Limitations. First, the LSI struggles with extremely
complex structures, such as these structures involving inter-
occlusions, as indicated by the yellow boxes in Fig. 9 (e).
These complex structures are also prevalent in KITTI, in-
cluding dense vegetation and wooded areas, which limit the
performance of LoS. Second, there are no constraints im-
posed on disparity offset residuals Ocurrently, leading to
residuals in self-disparity offset o(p,p)after several GRU
updating, as demonstrated in the central subplot of Fig. 9
(e). These residuals cause LSGP to partially replicate the
role of GRU updating (step 4 of Alg. 1), which constrains
our ability to adjust N2freely during inference. Third, de-
spite our efforts to mitigate the impact of high-uncertainty
pixels by splitting the neighbors (Eq. 3), these pixels may
still affect pixels with low uncertainty, marginally diminish-
ing our method’s performance in terms of Badx.0 metric.
5. Conclusion
In this paper, we propose LoS, an optimization-based stereo
matching method enhanced by local structure guidance. We
first present the local structure of a scene with an extended
LSI to capture more details and handle non-planar struc-
tures such as curve faces and object boundaries. Then we
propose LSGP on the basis of LSI to update the estimated
disparity map with local structure guidance. Despite the
limitations, our LSI introduces informative local structure
guidance to stereo matching, and LSGP significantly and ef-
ficiently improves the disparity accuracy under the structure
guidance. Extensive experiments on four popular bench-
marks and robust vision challenge demonstrate the effec-
tiveness, robustness and efficiency of the proposed LoS.
Acknowledgement. This work was partially sup-
ported by the Shenzhen Science and Technology Program
(No. RCYX20200714114641140), National Natu-
ral Science Foundation of China (No. U20A20185,
62372491, 62301601), and the Guangdong Basic and
Applied Basic Research Foundation (2022B1515020103).
19753
References
[1] Wei Bao, Wei Wang, Yuhua Xu, Yulan Guo, Siyu Hong, and
Xiaohu Zhang. Instereo2k: a large real dataset for stereo
matching in indoor scenes. Science China Information Sci-
ences , 63:1–11, 2020. 6
[2] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter
Wonka, and Matthias M ¨uller. Zoedepth: Zero-shot trans-
fer by combining relative and metric depth. arXiv preprint
arXiv:2302.12288 , 2023. 2
[3] Reiner Birkl, Diana Wofk, and Matthias M ¨uller. Midas v3.1
– a model zoo for robust monocular relative depth estimation.
arXiv preprint arXiv:2307.14460 , 2023. 4
[4] Michael Bleyer, Christoph Rhemann, and Carsten Rother.
Patchmatch stereo-stereo matching with slanted support win-
dows. In Proceedings of the British Machine Vision Confer-
ence (BMVC) , pages 1–11, 2011. 1, 2
[5] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A nat-
uralistic open source movie for optical flow evaluation. In
Proceedings of the European Conference on Computer Vi-
sion (ECCV) , pages 611–625. Springer-Verlag, 2012. 6
[6] Ayan Chakrabarti, Ying Xiong, Steven J. Gortler, and Todd
Zickler. Low-level vision by consensus in a spatial hierarchy
of regions. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , 2015. 2
[7] Ayan Chakrabarti, Ying Xiong, Steven J. Gortler, and Todd
Zickler. Low-level vision by consensus in a spatial hierarchy
of regions. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , 2015. 2
[8] Jia-Ren Chang and Yong-Sheng Chen. Pyramid stereo
matching network. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 5410–5418, 2018. 2, 7, 3
[9] Xuelian Cheng, Yiran Zhong, Mehrtash Harandi, Yuchao
Dai, Xiaojun Chang, Hongdong Li, Tom Drummond, and
Zongyuan Ge. Hierarchical neural architecture search for
deep stereo matching. In Advances in Neural Information
Processing Systems (NeurIPS) , pages 22158–22169, 2020. 7
[10] Shivam Duggal, Shenlong Wang, Wei-Chiu Ma, Rui Hu,
and Raquel Urtasun. Deeppruner: Learning efficient stereo
matching via differentiable patchmatch. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 4384–4393, 2019. 2
[11] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2012. 6, 3
[12] Xiaoyang Guo, Kai Yang, Wukui Yang, Xiaogang Wang,
and Hongsheng Li. Group-wise correlation stereo network.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 3273–3282,
2019. 2, 7
[13] Yulan Guo, Yun Wang, Longguang Wang, Zi Wang, and
Chen Cheng. Cvcnet: Learning cost volume compression for
efficient stereo matching. IEEE Transactions on Multimedia
(TMM) , 2022. 2
[14] Philipp Heise, Sebastian Klose, Brian Jensen, and Alois
Knoll. Pm-huber: Patchmatch with huber regularization forstereo matching. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV) , pages 2360–
2367, 2013. 1, 2
[15] Heiko Hirschmuller. Stereo processing by semiglobal match-
ing and mutual information. IEEE Transactions on Pattern
Analysis and Machine Intelligence (TPAMI) , 30(2):328–341,
2007. 2
[16] Junpeng Jing, Jiankun Li, Pengfei Xiong, Jiangyu Liu,
Shuaicheng Liu, Yichen Guo, Xin Deng, Mai Xu, Lai Jiang,
and Leonid Sigal. Uncertainty guided adaptive warping
for robust and efficient stereo matching. In Proceedings of
the IEEE/CVF International Conference on Computer Vision
(ICCV) , 2023. 6, 7
[17] Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter
Henry, Ryan Kennedy, Abraham Bachrach, and Adam Bry.
End-to-end learning of geometry and context for deep stereo
regression. In Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV) , pages 66–75, 2017.
2
[18] Jiankun Li, Peisen Wang, Pengfei Xiong, Tao Cai, Ziwei
Yan, Lei Yang, Jiangyu Liu, Haoqiang Fan, and Shuaicheng
Liu. Practical stereo matching via cascaded recurrent net-
work with adaptive correlation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 16263–16272, 2022. 1, 2, 5, 6,
7, 3
[19] Zhengfa Liang, Yulan Guo, Yiliu Feng, Wei Chen, Linbo
Qiao, Li Zhou, Jianfeng Zhang, and Hengzhu Liu. Stereo
matching using multi-level cost volume and multi-scale fea-
ture constancy. IEEE Transactions on Pattern Analysis and
Machine Intelligence (TPAMI) , 43(1):300–315, 2019. 2, 7
[20] Lahav Lipson, Zachary Teed, and Jia Deng. Raft-stereo:
Multilevel recurrent field transforms for stereo matching. In
Proceedings of the IEEE International Conference on 3D Vi-
sion (3DV) , pages 218–227. IEEE, 2021. 1, 2, 4, 5, 7, 3
[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 , 2017. 6
[22] Yamin Mao, Zhihua Liu, Weiming Li, Yuchao Dai, Qiang
Wang, Yun-Tae Kim, and Hong-Seok Lee. Uasnet: Uncer-
tainty adaptive sampling network for deep stereo matching.
InProceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV) , pages 6311–6319, 2021. 2
[23] David Marr and Tomaso Poggio. Cooperative computation
of stereo disparity: A cooperative algorithm is derived for ex-
tracting disparity information from stereo image pairs. Sci-
ence, 194(4262):283–287, 1976. 2
[24] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,
Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A
large dataset to train convolutional networks for disparity,
optical flow, and scene flow estimation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 4040–4048, 2016. 2, 6, 3
[25] Moritz Menze and Andreas Geiger. Object scene flow for au-
tonomous vehicles. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
2015. 6, 3
[26] Guang-Yu Nie, Ming-Ming Cheng, Yun Liu, Zhengfa Liang,
Deng-Ping Fan, Yue Liu, and Yongtian Wang. Multi-level
19754
context ultra-aggregation for stereo matching. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 3283–3291, 2019. 2
[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-
perative style, high-performance deep learning library. Ad-
vances in Neural Information Processing Systems (NeurIPS) ,
32, 2019. 6
[28] Ren ´e Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence (TPAMI) , 44(3), 2022. 4
[29] Daniel Scharstein, Heiko Hirschm ¨uller, York Kitajima,
Greg Krathwohl, Nera Ne ˇsi´c, Xi Wang, and Porter West-
ling. High-resolution stereo datasets with subpixel-accurate
ground truth. In Proceedings of the German Conference on
Pattern Recognition (GCPR) , pages 31–42. Springer, 2014.
6, 3
[30] Thomas Sch ¨ops, Johannes L. Sch ¨onberger, Silvano Galliani,
Torsten Sattler, Konrad Schindler, Marc Pollefeys, and An-
dreas Geiger. A multi-view stereo benchmark with high-
resolution images and multi-camera videos. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , 2017. 6, 3
[31] Zhelun Shen, Yuchao Dai, and Zhibo Rao. Cfnet: Cascade
and fused cost volume for robust stereo matching. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 13906–13915, 2021.
2, 7, 3
[32] Zhelun Shen, Yuchao Dai, Xibin Song, Zhibo Rao, Dingfu
Zhou, and Liangjun Zhang. Pcw-net: Pyramid combina-
tion and warping cost volume for stereo matching. In Pro-
ceedings of the European Conference on Computer Vision
(ECCV) , pages 280–297. Springer, 2022. 2, 7
[33] Sudipta N Sinha, Daniel Scharstein, and Richard Szeliski.
Efficient high-resolution stereo matching using local plane
sweeps. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pages
1582–1589, 2014. 2
[34] Xiao Song, Guorun Yang, Xinge Zhu, Hui Zhou, Zhe Wang,
and Jianping Shi. Adastereo: A simple and efficient ap-
proach for adaptive stereo matching. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 10328–10337, 2021. 7
[35] Tatsunori Taniai, Yasuyuki Matsushita, Yoichi Sato, and
Takeshi Naemura. Continuous 3d label stereo matching us-
ing local expansion moves. IEEE Transactions on Pattern
Analysis and Machine Intelligence (TPAMI) , 40(11):2725–
2739, 2017. 1, 2
[36] Vladimir Tankovich, Christian Hane, Yinda Zhang, Adarsh
Kowdle, Sean Fanello, and Sofien Bouaziz. Hitnet: Hierar-
chical iterative tile refinement network for real-time stereo
matching. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pages
14362–14372, 2021. 1, 2, 3, 6, 7[37] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field
transforms for optical flow. In Proceedings of the European
Conference on Computer Vision (ECCV) , pages 402–419.
Springer, 2020. 2, 5
[38] Fabio Tosi, Yiyi Liao, Carolin Schmitt, and Andreas Geiger.
Smd-nets: Stereo mixture density networks. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , pages 8942–8952, 2021. 7, 1
[39] Jonathan Tremblay, Thang To, and Stan Birchfield. Falling
things: A synthetic dataset for 3d object detection and pose
estimation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition Workshops
(CVPRW) , pages 2038–2041, 2018. 6
[40] Fangjinhua Wang, Silvano Galliani, Christoph V ogel, and
Marc Pollefeys. Itermvs: Iterative probability estimation for
efficient multi-view stereo. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 8606–8615, 2022. 2
[41] Longguang Wang, Yulan Guo, Yingqian Wang, Zhengfa
Liang, Zaiping Lin, Jungang Yang, and Wei An. Parallax
attention for unsupervised stereo correspondence learning.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence (TPAMI) , 44(4):2108–2125, 2020. 5
[42] Yun Wang, Longguang Wang, Hanyun Wang, and Yulan
Guo. Spnet: Learning stereo matching with slanted plane
aggregation. IEEE Robotics and Automation Letters (RAL) ,
7(3):6258–6265, 2022. 1, 2, 3
[43] Philippe Weinzaepfel, Vaibhav Arora, Yohann Cabon,
Thomas Lucas, Romain Br ´egier, Vincent Leroy, Gabriela
Csurka, Leonid Antsfeld, Boris Chidlovskii, and J ´erˆome
Revaud. Improved cross-view completion pre-training for
stereo matching. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV) , 2023. 7
[44] Bin Xu, Yuhua Xu, Xiaoli Yang, Wei Jia, and Yulan Guo.
Bilateral grid learning for stereo matching networks. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 12497–12506, 2021.
2
[45] Gangwei Xu, Junda Cheng, Peng Guo, and Xin Yang. Atten-
tion concatenation volume for accurate and efficient stereo
matching. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pages
12981–12990, 2022. 7
[46] Gangwei Xu, Xianqi Wang, Xiaohuan Ding, and Xin Yang.
Iterative geometry encoding volume for stereo matching. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , pages 21919–21928,
2023. 1, 2, 7
[47] Haofei Xu and Juyong Zhang. Aanet: Adaptive aggrega-
tion network for efficient stereo matching. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 1959–1968, 2020. 2
[48] Haofei Xu, Jiaolong Yang, Jianfei Cai, Juyong Zhang, and
Xin Tong. High-resolution optical flow from 1d atten-
tion and correlation. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision (ICCV) , pages
10498–10507, 2021. 2
19755
[49] Jure Zbontar and Yann LeCun. Computing the stereo match-
ing cost with a convolutional neural network. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , pages 1592–1599, 2015. 2
[50] Feihu Zhang, Xiaojuan Qi, Ruigang Yang, Victor Prisacariu,
Benjamin Wah, and Philip Torr. Domain-invariant stereo
matching networks. In Proceedings of the European Confer-
ence on Computer Vision (ECCV) , pages 420–439. Springer,
2020. 2, 3
[51] Haoliang Zhao, Huizhou Zhou, Yongjun Zhang, Jie Chen,
Yitong Yang, and Yong Zhao. High-frequency stereo match-
ing network. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
1327–1336, 2023. 1, 2, 6, 7
19756
