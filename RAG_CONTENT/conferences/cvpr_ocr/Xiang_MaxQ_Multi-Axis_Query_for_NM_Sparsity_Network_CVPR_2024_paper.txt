MaxQ: Multi-Axis Query for N:M Sparsity Network
Jingyang Xiang1Siqi Li1Junhao Chen1Zhuangzhi Chen2Tianxin Huang1
Linpeng Peng1Yong Liu1*
1APRIL Lab, Zhejiang University, Hangzhou, China
2IVSN, Zhejiang University of Technology, Hangzhou, China
Abstract
N:M sparsity has received increasing attention due to
its remarkable performance and latency trade-off com-
pared with structured and unstructured sparsity. How-
ever, existing N:M sparsity methods do not differentiate
the relative importance of weights among blocks and leave
important weights underappreciated. Besides, they di-
rectly apply N:M sparsity to the whole network, which will
cause severe information loss. Thus, they are still sub-
optimal. In this paper, we propose an efﬁcient and effec-
tive Multi-Axis Query methodology, dubbed as MaxQ, to
rectify these problems. During the training, MaxQ em-
ploys a dynamic approach to generate soft N:M masks,
considering the weight importance across multiple axes.
This method enhances the weights with more importance
and ensures more effective updates. Meanwhile, a spar-
sity strategy that gradually increases the percentage of
N:M weight blocks is applied, which allows the network
to heal from the pruning-induced damage progressively.
During the runtime, the N:M soft masks can be precom-
puted as constants and folded into weights without caus-
ing any distortion to the sparse pattern and incurring ad-
ditional computational overhead. Comprehensive experi-
ments demonstrate that MaxQ achieves consistent improve-
ments across diverse CNN architectures in various com-
puter vision tasks, including image classiﬁcation, object
detection and instance segmentation. For ResNet50 with
1:16 sparse pattern, MaxQ can achieve 74.6% top-1 ac-
curacy on ImageNet and improve by over 2.8% over the
state-of-the-art. Codes and checkpoints are available at
https://github.com/JingyangXiang/MaxQ .
1. Introduction
Deep convolutional neural networks (CNNs) have achieved
great success in various computer vision tasks, including
image classiﬁcation [ 17,39], object detection [ 12,18], se-
*Corresponding authorFigure 1. Comparison of the accuracy-sparse pattern Pareto curve
of the ResNet50 on ImageNet. MaxQ shows the top-performing
Pareto frontier compared with previous N:M sparsity methods [ 1,
25,44,45].
mantic segmentation [ 7,13]. However, the expensive mem-
ory and computational overhead have presented challenges
for deploying them on mobile or edge devices. Therefore, it
is vital to study the network compression to reduce its run-
time overhead while maximally retaining its performance.
Among the many compression methods [ 15,22,24,33,
37], network sparsity stands out as a highly effective tool
for achieving practical memory and FLOPs saving. Most
researchers have paid their attention to structured spar-
sity [ 20,21,30] and unstructured sparsity [ 16,28]. How-
ever, both of them have certain shortcomings in terms of
performance and acceleration, limiting their application.
How to realize a better trade-off remains an open problem,
as it hinges on factors like the granularity of sparsity and
hardware design.
Recently, N:M sparsity [ 35], a ﬁne-grained sparsity pat-
tern, is considered a highly promising solution. It can
achieve a better trade-off between performance and latency
compared to structured and unstructured sparsity thanks to
its hardware and software co-design [ 29,35]. Several stud-
ies have enhanced the performance of N:M sparsity. The
pioneer work ASP [ 35] proposes to remove the two weights
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
15845
N:M SpartityQuery1-st2-th3-rd4-nd2-nd1-st3-rd4-th5-th
6-th8-th7-th
QueryBlock3Block4Block1Block2Weight ImportanceFigure 2. The framework of our MaxQ method, which queries the weights importance among the blocks and generates soft masks by
querying the weight across multiple axes. For simplify, we only show single axis query.
with the smallest magnitude from every four consecutive
weights and use the pretrain-prune-ﬁnetune (PPF) pipeline
to get the sparse network. This approach effectively pre-
serves the performance of the dense network while it still
suffers from expensive computation. To address this, Zhou
et al.[1] proposes to learn N:M sparsity from scratch by ex-
tending a regularization term to Straight-Through Estima-
tor [3], which improves the efﬁciency of sparse architecture
update. Zhang et al.[44] treats the N:M sparsity problem as
a combinatorial problem and solves it in an efﬁcient divide-
and-conquer manner. Although these methods improve the
training efﬁciency or performance of N:M sparsity network,
they are typically constructed upon the premise of weight
block independence and do not differentiate the weight im-
portance among blocks. At the same time, these methods
apply N:M sparsity to the whole network N:M from the be-
ginning of training, leading to critical information loss. As
a result, they are still sub-optimal.
In this paper, we propose a simple yet effective Multi-
Axis Query methodology, dubbed as MaxQ , to identify the
weight importance and build high-performance N:M spar-
sity network. In contrast to the previous approaches, which
considered the weights in N:M blocks individually, MaxQ
queries the weight importance along multiple axes to iden-
tify more critical connections and ensures more effective
updates for them. It is worth saying MaxQ achieves this
according to the threshold from weight magnitude via a dy-
namic and parameter-free approach, which is different from
the previous methods achieving this by learning a thresh-
old parameter. Therefore, it requires no additional learnable
parameters and simpliﬁes the training process. Addition-
ally, MaxQ follows an incremental pruning schedule, which
gradually increases the percentage of N:M sparse block
based on the epoch. It progressively allows the network
to heal from the pruning-induced damage and notably en-
hances performance. This strategy makes the training pro-
cess more stable and weights to be trained more sufﬁciently,
thus improving convergence and performance. With only
one training pass from scratch, our obtained sub-modelsperform better than the previous method across diverse
CNN architectures in various computer vision tasks. More-
over, our MaxQ can achieve good results in Post-Training-
Quantization (PTQ) even though it is a self-structured re-
parameterized network, contrary to the previous structured
re-parameterized network.
The contributions of our work are highlighted as follows:
•We propose a multi-axis query method MaxQ with two
novel features: (1) a multi-axis query approach to identify
important connections among the N:M sparse blocks; (2)
a dynamic approach to generate soft pruning masks in a
parameter-free manner.
•Our method follows an incremental pruning schedule by
increasing the percentage of N:M blocks according to
their `1-norm. It enhances the performance of the N:M
sparsity network in one training pass.
•Experimentally, MaxQ achieves consistent improvement
across different N:M sparse patterns compared to previ-
ous methods (as shown in Fig. 1) and is applicable to
downstream tasks for computer vision. For image classi-
ﬁcation on ImageNet, MaxQ achieves 74.6% top-1 accu-
racy for a 1:16 sparse ResNet50, improving the previous
best [ 44] by 2.8%. For object detection and instance seg-
mentation on the COCO [ 32] dataset, MaxQ can achieve
comparable results with a dense baseline model under the
1:4 structured sparsity.
•MaxQ is friendly to quantization. The ResNet50 with 2:4
sparsity achieves 0.5% drop (77.6% Ñ77.1%) in the top-
1 accuracy when quantized to INT8 using PTQ, which
beneﬁts its deployment.
2. Related Work
2.1. Sparsity Granularity in Network Compression
Sparsity is an essential tool for network compression and
has attracted extensive attention from both industry and
academia. According to the granularity of sparse weights,
network sparsity can be categorized into structured spar-
sity [ 19,30,41], unstructured sparsity [ 10,11,14,16,27,
15846
Granularity Performance Latency Hardware Library
Filter 3 333 General General
Weight 333 3 Speciﬁc Speciﬁc
1ˆN 33 33 General Speciﬁc
N:M 33 333 Ampere GPUs General
Table 1. An overview of mainstream sparsity granularity.
28] and semi-structured sparsity [ 5,9,31,35]. Tab. 1
overviews these sparsity and their characteristics. Struc-
tured sparsity removes the entire channel of a convolutional
or fully connected layer, which enables signiﬁcant speedup
on general-purpose hardware. Unstructured sparsity re-
moves single weight and achieves negligible performance
loss even under a large sparse ratio [ 10,27]. However, the
former suffers huge performance degradation at high com-
pression ratios, and the latter gains rare speedup on general-
purpose hardware for its expensive memory access and low
computational density resulting from irregular sparse ten-
sors. In recent years, semi-structured sparsity has received
extensive attention from researchers. By limiting the distri-
bution of weights to speciﬁc sparse types, semi-structured
sparsity can achieve a better trade-off between performance
and latency through a synergistic software and hardware
design. Among the semi-structured sparsity, 1 ˆN[9,31]
and N:M [ 35] are now widely supported by software and
hardware. 1 ˆN makes N consecutive output channels keep
the same sparse pattern, enabling signiﬁcant speedup on
general-purpose hardware. N:M sparsity forces at most N
of the M consecutive weights along the input channel di-
mension to be non-zero, achieving good results while main-
taining high sparsity and high-performance acceleration on
NVIDIA Ampere GPUs.
The core purpose of network sparsity is to remove unim-
portant weights from the network and preserve the net-
work’s performance. Therefore, learning the appropriate
mask is indeed necessary. Mask is usually obtained from
the weight values according to speciﬁc criteria, and the pro-
cess must follow a schedule. We empirically categorize ex-
isting studies into two groups below based on criteria and
schedule.
Mask Criteria. Extensive studies have investigated how
to identify the critical weights in a network. Among them,
the most widely used is the magnitude-based criterion that
selects the pruning targets by their absolute value or `1-
norm. Apart from this, He et al.[21] proposed to prune
ﬁlters via geometric median to compress CNN models with
redundancy rather than those with ”relatively less” impor-
tance. Molchanov et al.[36] proposed to estimate the con-
tribution of a neuron (ﬁlter) to the ﬁnal loss via the ﬁrst
and second-order Taylor expansions. Lee et al.[28] intro-
duced a saliency criterion based on connection sensitivity
to identify structurally important connections in the net-
work. Some other criteria, such as BN-based [ 34], have
also proven effective.Mask Schedule. Learning schedule for masks is also vital
for a sparse network. Most of the early research [ 16,30]
follows the PPF pipeline iteratively, which is complicated
to use and time-consuming. In order to simplify this pro-
cess, recent studies use soft and incremental pruning to train
sparse networks from scratch without dependence on pre-
trained weights. For example, Humble et al.[26] proposed
soft masking for channel pruning, which allows pruned
channels to adaptively return to the network while simul-
taneously pruning towards a target cost constraint. Hou
et al .[23] proposed a channel exploration methodology
to prune and regrow the channels with a sampling-based
strategy repeatedly. Evci [ 10] introduced a sparse-to-sparse
training procedure with a ﬁxed parameter count and a ﬁxed
computational cost, sacriﬁcing no accuracy relative to ex-
isting dense-to-sparse training methods. Compared to hard
and one-shot pruning, these soft and incremental strategies
can improve the model’s performance, especially at a high
sparse ratio.
3. Methodology
3.1. Preliminaries
Here, we deﬁne the N:M sparsity problem. Without loss
of generality, we suppose an L-layer CNN with parame-
tersW“tw1,. . . ,wLu, with wlPRCl
outˆCl
inˆKl
hˆKl
w.
Cl
out,Cl
in,Kl
handKl
wrepresent the number of output chan-
nels, input channels, kernel height and kernel width for l-
th layer respectively. N:M sparsity groups every M con-
secutive weights along the input channel in each layer and
requires each group to have at most N non-zero elements.
With this pattern, only N non-zero values in each group
need to be stored and metadata is adopted to encode the
position of each non-zero value. This process can com-
press the origin matrix and be accelerated by designated
processing units ( e.g. NVIDIA Ampere Tensor Cores). To
this end, we ﬁrst rearrange wltomlPRGlˆM, where
Gl“Cl
outˆKl
hˆKl
wˆCl
in
Mand denote M“tm1,. . . ,mLu.
Meanwhile, N:M binary masks blPt0,1uGlˆMare intro-
duced to achieve such sparsity Then, the optimization ob-
jective can be formulated as:
min
M,BLpM¨B;Dqs.t.}bl
gl,:}0§N (1)
where gl“1,2,. . . ,Gl. The Lp¨qrepresents the loss func-
tion and Ddenotes the observed data.
3.2. Multi-Axis Query
Although extensive studies have promoted the performance
of N:M sparsity, they only focus on picking the N most
important weights from M elements and applying binary
masks to them, which makes a portion of the weights with
15847
0.250.080.380.990.34300.08710.65700.9988Figure 3. The process of computing soft masks. The weights of the
model are sorted in descending order based on their magnitudes for
clear understanding. We assume pN, p, ⌧ q“p 4,0.5,0.1q.
more importance underappreciated. Therefore, it is vital
to exploit the importance of weights further to improve the
performance of N:M sparsity networks.
To represent the importance of the weights in the N:M
sparse pattern, we ﬁrst relax the constrain of the binary ma-
trixbltoslPtx|x•0uGlˆM. Then, the optimization
objective can be rewritten as:
min
M,SLpM¨S;Dqs.t.}sl
gl,:}0§N (2)
It can be seen as replacing the previous hard mask bl
with a soft mask sl.slsatisﬁes the N:M sparse pattern and
can be folded into the network as constants, which will not
cause any distortion to the sparse pattern and introduce any
extra cost during the runtime.
To get the value of sl, we propose MaxQ to measure the
importance of weights. As shown in Fig. 2, MaxQ can be
divided into two parts in what follows.
Part 1: Apply N:M Sparsity.
First, for l-th layer, we initialize all elements in blto 1.
Fort-th epoch, we sort each element in ml
gaccording to it
absolute value and sort the mlaccording to it’s `1-norm to
identify the set of blocks to apply N:M sparsity:
Ml
g“ArgTopKM-N`
´ˇˇml
g,:ˇˇ˘
Tl
t“ArgTopKrGl ts´!››ml
g››
1)¯ (3)
where MlPRGlˆpM´Nq,Tl
kPRrGl tsand trepre-
sents the percentage of weight blocks to apply N:M spar-
sity, which gives the indices of weights with the pM´Nq
smallest absolute value in each block and blocks with the
topP
Gl tT
value in `1-norm respectively. Then we prune
the weight by zeroizing 
bl
i,j|iPTl
t,jPMl
i(
.
Part 2: Measure Importance. We show our function S“
GpV,p,⌧ qto measure weight importance. Give a vector
VPRNand a sparse rate p, we get the threshold  by
$
’&
’% h“minptopK pabspVq,p1´pq¨Nqq
 l“maxptopK p´abspVq,p¨Nqq
 “pabsp hq`absp lqq{2(4)maskHard N:M MaskSoft N:M Mask02.501.1001.32.9002.21.91.52.600mask0101001100111100Multi-Axis QueryN:M Query
Prune the valueKeep the valueWeightElement-wise multiplicationFigure 4. MaxQ to generate soft masks.
Then, we measure weight importance with sigmoid:
si“sigmoid pp|vi|´ q{⌧q (5)
where i“1,2,. . . ,N and⌧is the global temperature pa-
rameter to control the level of softness in the masks. Fig. 3
shows the process when pN,p,⌧ q“p 4,0.5,0.1q.
To measure the weight importance among the blocks, we
query the weight along ﬁlter axis and kernel axis to generate
corresponding soft masks:
slpfq
i,:,:,:“`
G`
wl
i,:,:,:,pM´Nq{M,⌧˘˘
slpkq
:,:,k1,k2“`
G`
wl
:,:,k1,k2,pM´Nq{M,⌧˘˘ (6)
where i“1,2,. . . ,Cl
out,k1“1,. . . ,Kl
handk2“1,. . . ,Kl
w.
We rearrange (RA) the slpfqandslpkqto match the shape
ofbland obtain the soft mask as shown in Fig. 4:
sl“bl`bldRApslpfqq`bldRApslpkqq
“bld´
1`RApslpfqq`RApslpkqq¯ (7)
It is worth noting slcan be precomputed. Meanwhile, since
blsatisﬁes the N:M sparse pattern, slwill also satisfy this
pattern. Therefore, MaxQ will not bring any additional
overhead during the runtime compared to the conventional
N:M sparsity network.
3.3. Incremental Sparsity
Previous methods ﬁxed  tto 1 throughout the training pro-
cess, which means the whole network is in N:M sparse pat-
tern from the beginning of training. It will cause severe in-
formation loss and is detrimental to network convergence.
To this end, incremental sparsity, which gradually in-
creases the sparse ratio based on the current epoch/step, has
been shown to be an effective technique to heal sparse net-
works from pruning and improve their performance. For un-
structured sparsity, incremental sparsity can be achieved by
gradually removing a single weight; for structured sparsity,
it can be achieved by removing the channels across layers.
Since networks often have a large number of parameters and
15848
Algorithm 1: Overview of the MaxQ method.
1Input : An L-layer CNN model with weights
W“tw1,. . . , wLu; target sparse pattern N:M; total
training epochs Ttotal; initial and ﬁnal epoch for
incremental pruning ti,tf; training set D;
2Output : A sub-model satisfying the target sparse pattern
N, M and its optimal weight values W˚;
3Randomly initialize the model weights W;
4Rearrange the model weights WtoM;
5foreach training epoch tPr1,. . . ,T totalsdo
6 Compute the percentage of N:M blocks  tvia Eq. ( 8);
7 foreach mini-batch PDdo
8 forlPr1,. . . ,L sdo
9 Reset 
bl
i,j|@i,@j(
to 1 ;
10 Get the indices MlandTl
tvia Eq. ( 3);
11 Set 
bl
i,j|iPTl
t,jPMl
i(
to 0 ;
12 Get the slpfqandslpkqvia Eq. ( 4)„Eq. (6);
13 Get the soft mask slvia Eq. ( 7)
14 Forward via Eq. ( 2);
15 Backward and update via the SGD optimizer ;
16Compute M˚“tml¨sl|@lu;
17Rearrange M˚back to W˚;
channels, the sparsity process can be viewed as continuous.
Different from them, N:M sparse is characterized by several
M consecutive weight groups. If we increase the sparsity by
simply reducing››bl
g,:››
0from MtoN, the sparse networks
will suffer critical performance degradation, especially at
high sparsity.
In this paper, we propose gradually increasing the per-
centage of N:M sparse blocks to achieve a smoother spar-
sity process. We apply the same sparse schedule for each
layer The percentage of N:M sparse blocks at the t-th train-
ing epoch is computed as:
 t“minp1,maxp0,1´r1´pt´tiq{ptf´tiqs3qq(8)
where tiandtfdenote the beginning and ending epochs in
the incremental sparsity process. Speciﬁcally, if tis smaller
than ti, the network is trained in a dense state, and when
tis larger than tf, the network is trained in a N:M sparse
pattern. Notably, we ﬁrstly apply the N:M sparse pattern for
blocks with larger `1-norm because we ﬁnd it can reduce the
performance degradation when the sparsity increases and
keep the convergence process stable. We conduct ablation
studies in Sec. 4.5to show its advantage. As an algorithm
guideline, the pseudo-code of MaxQ is provided in Algo-
rithm 1.
4. Experiment
4.1. Experiment Settings
To validate the effectiveness of MaxQ, we conducted im-
age classiﬁcation on ImageNet with heaveweight CNNsModel Method N:M Top-1 Epochs FLOPs Params
ResNet34Baseline - 74.6% 120 3.67G 21.8M
ASP 1:4 70.9% 200 1.01G 5.85M
SR-STE 1:4 73.8% 120 1.01G 5.85M
LBC 1:4 73.7% 120 1.01G 5.85M
MaxQ 1:4 74.2% 120 1.01G 5.85M
ASP 2:4 73.9% 200 1.90G 11.2M
SR-STE 2:4 74.3% 120 1.90G 11.2M
LBC 2:4 74.1% 120 1.90G 11.2M
MaxQ 2:4 74.5% 120 1.90G 11.2M
ResNet50Baseline - 77.3% 120 4.11G 25.6M
ASP 2:4 77.4% 200 2.12G 13.8M
SR-STE 2:4 77.0% 120 2.12G 13.8M
LBC 2:4 77.2% 120 2.12G 13.8M
MaxQ 2:4 77.6% 120 2.12G 13.8M
ASP 1:4 76.5% 200 1.11G 7.93M
SR-STE 1:4 75.3% 120 1.11G 7.93M
LBC 1:4 75.9% 120 1.11G 7.93M
MaxQ 1:4 77.3% 120 1.11G 7.93M
ASP 2:8 76.6% 200 1.11G 7.93M
SR-STE 2:8 76.2% 120 1.11G 7.93M
LBC 2:8 76.5% 120 1.11G 7.93M
MaxQ 2:8 77.2% 120 1.11G 7.93M
ASP 1:16 71.5% 200 0.44G 3.52M
SR-STE 1:16 71.5% 120 0.44G 3.52M
LBC 1:16 71.8% 120 0.44G 3.52M
MaxQ 1:16 74.6% 120 0.44G 3.52M
Table 2. Results of the different N:M sparsity training methods for
ResNet34 and ResNet50 on ImageNet.
Model Method N:M Top-1 Epochs FLOPs Params
MobileNetV1Baseline - 71.9% 120 578M 4.23M
ASP 2:4 70.4% 200 302M 2.66M
SR-STE 2:4 71.5% 120 302M 2.66M
MaxQ 2:4 72.1% 120 302M 2.66M
ASP 1:4 65.4% 200 164M 1.88M
SR-STE 1:4 67.8% 120 164M 1.88M
MaxQ 1:4 68.5% 120 164M 1.88M
Table 3. Results of the different N:M sparsity training methods for
lightweight model MobileNetV1 on ImageNet.
ResNet34 [ 17], ResNet50 [ 17] and lightweight Mo-
bileNetV1 [ 24]. We compare MaxQ with N:M sparsity and
unstructured sparsity in Sec. 4.2and Sec. 4.3, respectively.
We also conducted object detection and semantic segmenta-
tion on the COCO [ 32] benchmark with Faster-RCNN [ 38]
and Mask-RCNN [ 18] in Sec. 4.4. All experiments are im-
plemented on PyTorch with NVIDIA RTX 3090 and trained
with the same conﬁgurations as previous work [ 1,44] to
make a fair comparison. tiandtfare set to 0 and 3/4 of the
total training epochs respectively. Ablation studies about
components, tiandtfand strategy for incremental spar-
sity are demonstrated in Sec. 4.5. Performance analysis is
shown in Sec. 4.6.
4.2. Comparison with N:M sparsity
We ﬁrst apply our MaxQ to ResNet34 and ResNet50 to val-
idate its effectiveness. As shown in Tab. 2, MaxQ leads all
N:M sparse patterns and networks. For ResNet34, MaxQ
15849
Method Top-1 Sparsity FLOPs Params S U
Baseline 77.3% 0.0 4.10G 25.6M - -
RigL [ 10] 74.6% 80 0.92G 5.12M 73
GMP [ 46] 75.6% 80 0.82G 5.12M 73
MAP [ 2] 75.9% 80 - 5.12M 77
STR [ 27] 76.2% 81 0.82G 5.12M 73
SR-STE 75.3% 1:4 1.13G 7.97M 33
LBC 75.9% 1:4 1.13G 7.97M 33
MaxQ 77.3% 1:4 1.13G 7.97M 33
SR-STE 76.2% 2:8 1.13G 7.97M 33
LBC 76.5% 2:8 1.13G 7.97M 33
MaxQ 77.2% 2:8 1.13G 7.97M 33
DNW [ 42] 68.3% 95 0.20G 1.28M 77
RigL [ 10] 70.0% 95 0.49G 1.28M 73
GMP [ 46] 70.6% 95 0.20G 1.28M 73
STR [ 27] 70.4% 95 0.16G 1.24M 77
OptG [ 43] 72.5% 95 0.22G 1.28M 77
SR-STE 71.5% 1:16 0.44G 3.52M 33
LBC 71.8% 1:16 0.44G 3.52M 33
MaxQ 74.6% 1:16 0.44G 3.52M 33
Table 4. Results of the N:M and unstructured sparsity methods for
ResNet50 on ImageNet. S: Structured. U: Uniform.
outperforms SR-STE [ 1] and LBC [ 44] at 1:4 sparse pat-
tern at the top-1 accuracy by 0.4% and 0.5% respectively.
For ResNet50, MaxQ achieves similar improvement at 1:4
and 2:8 sparse patterns. Meanwhile, we also conduct exper-
iments on lightweight CNN MobileNetV1, as compressing
this lightweight network is more beneﬁcial for further ac-
celeration on mobile devices. Similar to ResNet, we apply
N:M sparsity to all except the ﬁrst, last layers and depthwise
convolutional layers. The results in Tab. 3also demonstrate
the superiority against the others.
The results in Tab. 2and Tab. 3also show two interest-
ing properties of our MaxQ method. ResNet34, ResNet50
and MobileNetV1 achieve 74.5%, 77.6% and 72.1% top-1
accuracy at 2:4 sparse pattern, while their dense counter-
part achieves 74.6%, 77.3% and 71.9% respectively, which
means our MaxQ can achieve almost lossless or better re-
sults on various networks at 2:4 sparse pattern. On the other
hand, our MaxQ achieves a more signiﬁcant improvement
at a high sparse ratio. For example, our 1:16 ResNet50
achieves 74.6% top-1 accuracy, which outperforms the pre-
vious state-of-the-art by 2.8%.
4.3. Comparison with Unstructured Sparsity
We also compare the performance of MaxQ with state-
of-the-art unstructured sparsity methods using ResNet50.
The results in Tab. 4demonstrate that our MaxQ consis-
tently achieves outstanding accuracy under various spar-
sity constraints. For example, ResNet50 with 1:4 and 2:8
sparse patterns achieve 77.3% and 77.2% top-1 accuracy re-
spectively. Meanwhile, ResNet50 with 1:16 sparse pattern
achieves 74.6% accuracy, surpassing STR and OptG over
4.2% and 2.1%. In contrast to unstructured sparsity, N:M
organizes weights in a structured format, avoiding inefﬁ-
cient memory access and low computational density due toModel Method N:M mAP
F-RCNNBaseline - 37.4
SR-STE 2:4 38.2
LBC 2:4 38.5
MaxQ 2:4 38.7
SR-STE 1:4 37.2
LBC 1:4 37.3
MaxQ 1:4 37.7
Table 5. Results for object detection on COCO benchmark.
Model Method N:M Box mAP Mask mAP
M-RCNNBaseline - 38.2 34.7
SR-STE 2:4 39.0 35.3
LBC 2:4 39.3 35.4
MaxQ 2:4 39.2 35.5
SR-STE 1:4 37.6 33.9
LBC 1:4 37.8 34.0
MaxQ 1:4 38.3 34.4
Table 6. Results for instance segmentation on COCO benchmark.
irregular distribution of weights. Additionally, the uniform
distribution of N:M sparse weights ensures that computa-
tional loads are balanced when performing parallel compu-
tation and avoiding speed degradation due to load imbal-
ance across threads. These exhibit the effectiveness and ad-
vantages of exploring N:M sparsity.
4.4. Object Detection and Instance Segmentation
In addition to the image classiﬁcation, we conducted exper-
iments on the challenging dataset COCO based on MMDe-
tection [ 4], to exploit the generalization ability of MaxQ.
For object detection, we employ classical models Faster R-
CNN, and for instance segmentation, we use Mask R-CNN.
Tab. 5demonstrates that MaxQ consistently outperforms
previous methods in object detection. For example, MaxQ
yields a robust performance of 38.7 mAP at 2:4 sparse pat-
tern, which exceeds the previous state-of-the-art LBC by 0.2
mAP and improves it’s dense counterpart by 1.3 mAP. Sim-
ilar trends are observed in instance segmentation, as shown
in Tab. 6. Furthermore, MaxQ delivers results comparable
to dense baseline models at a 1:4 structure sparsity. These
results suggest that N:M sparse networks exhibit similar or
better feature transfer capabilities than dense networks and
can be effectively applied in downstream tasks.
4.5. Ablation Study
Method Top-1
Baseline (SR-STE) 71.5%
+ Multi-Axis Query 74.2%
+ Incremental Pruning ( default ) 74.6%
+ Train 200 Epochs 75.2%
Table 7. Ablation study of different components in MaxQ.
We investigate the effectiveness of different components
in the MaxQ through ablation studies. All the following
15850
results are based on the ResNet50 model with 1:16 sparse
pattern on the ImageNet dataset.
Components. In Tab. 7, we investigate the effectiveness
of different components in MaxQ, namely multi-axis query
and incremental pruning. The baseline is derived from SR-
STE where the model is trained solely with the sparse-
reﬁned straight-through estimator. There is no weight im-
portance identifying among blocks and incremental prun-
ing stage. The multi-axis query identiﬁes and enhances the
weights with more importance using soft masks, prevent-
ing crucial weights from being overlooked. It ensures more
effective updates for important weights during training, re-
sulting in 2.7% accuracy improvement. When the percent-
age of N:M sparse block in each layer increases accord-
ing to a speciﬁc schedule, instead of being ﬁxed to 100%
early in training, we can obtain a more optimal N:M sparse
network. This dynamic approach further improves accu-
racy by 0.4%. Additionally, to achieve a fair comparison
with ASP, we conduct an experiment to study the effect of
training epochs. We train MaxQ for 200 epochs, the same
ASP setup. The experiment result shows that longer train-
ing epochs lead to considerable performance gains, improv-
ing accuracy by 0.6% compared to the default settings. It’s
worth noting that for N:M sparse networks, the PPF train-
ing pipeline is unnecessary as it does not signiﬁcantly im-
prove its performance. On the contrary, applying soft and
incremental strategies to train an N:M sparse network from
scratch yields better results. Moreover, these strategies are
more straightforward when compared to the PPF pipeline.
Model N:M titf tf´ti Top-1 FLOPs(Train)
ResNet501:16 0 30 30 74.51% 0.74 ˆ(3.2e18)
1:16 0 60 60 74.41% 0.78 ˆ
1:16 0 90 90 74.55% 0.81 ˆ
1:16 30 60 30 74.56% 0.81 ˆ
1:16 30 90 60 74.52% 0.85 ˆ
1:16 60 90 30 74.47% 0.89 ˆ
Table 8. Ablation study of different tiandtfin MaxQ.
Choice of tiandtf.We conduct ablation studies involving
different tiandtfin MaxQ. To explain, larger tiandtf´ti
indicate more weights can be sufﬁciently trained before ap-
plying N:M sparsity and a smoother sparsity process. And
the total train FLOPs are positively correlated with tiand
tf. As illustrated in Tab. 8, our method can achieve excel-
lent performance across various tiandtf. Moreover, more
training FLOPs does not bring a notable improvement to the
ﬁnal result. It suggests that the weight distribution charac-
teristics of the trained dense network are not essential and
explains why the PPF strategy for the ASP does not yield
favorable results in the N:M sparsity networks. Superior
N:M sparsity networks can be trained in one training pass
by soft, dynamic and incremental strategies.
Sparsity strategy. We introduce two other strategies to in-
vestigate the efﬁciency of our strategy for incremental spar-Figure 5. Convergence visualization for different strategies. In-
verse means we ﬁrstly apply the N:M sparsity for blocks with
smaller `1-norm. N-M means reducing››bl
g,:››
0from M to N.
sity. tiandtfare set to 0 and 90 respectively. As shown in
Fig.5, both of inverse and N-M suffer severe performance
degradation when the sparse ratio increases (when epoch
increases to 90). Applying N:M sparsity ﬁrst to weight
blocks with larger `1-norm will bring more stable and ef-
ﬁcient convergence than others. At the same time, our strat-
egy achieves 74.55% top-1 accuracy on ImageNet, better
than the inverse with 74.13% and the N-M with 74.18%.
4.6. Performance Analysis
Model N:M MethodTrain speedTop-1FLOPs
(Train)BS=128 BS=256
ResNet50- Dense 798 884 77.3% 1 ˆ(3.2e18)
2:4SR-STE 642 854 77.0% 0.83 ˆ
LBC 373 487 77.2% 0.72ˆ
MaxQ 507 732 77.6% 0.91ˆ
2:8SR-STE 625 862 76.2% 0.74 ˆ
LBC 382 512 76.5% 0.53ˆ
MaxQ 514 743 77.2% 0.86ˆ
1:16SR-STE 628 852 71.5% 0.69 ˆ
LBC 364 538 71.8% 0.38ˆ
MaxQ 502 725 74.6% 0.81ˆ
Table 9. Train speed (NVIDIA RTX 3090, measured in sam-
ples/second/GPU), ImageNet top-1 accuracy and FLOPs (train)
with different N:M sparse pattern and methods.
Model N:M Method FP32 INT8 Acc Drop
ResNet50 2:4MaxQ 77.6% 77.1% 0.5%
SR-STE 77.1% 76.6% 0.5%
Table 10. Post-Training Quantization (PTQ) results on SR-STE
and MaxQ for ResNet50 with 2:4 sparse pattern.
Inference. As mentioned earlier, MaxQ can be considered
a self-structured re-parameterization process during infer-
ence. Thus, the soft masks and weights can be folded as
constants and do not introduce any additional overhead dur-
ing the inference. Furthermore, as the soft masks also ad-
here to the N:M sparse pattern, MaxQ does not disrupt the
sparse pattern and can still leverage its beneﬁts, resulting in
favorable latency on Ampere GPUs.
15851
Figure 6. Parameter distribution from SR-STE at inference.
Figure 7. Parameter distribution from MaxQ at inference.Training.The training efﬁciency of the algorithm is also animportant aspect to consider. While many algorithms havesmaller FLOPs theoretically, they may not be highly par-allelized or have a large amount of memory access. Thus,there will be an increase but not a reduction in GPU days.To investigate the efﬁciency of various algorithms for N:Msparsity, we present the train speed, top-1 accuracy, andtraining FLOPs for SR-STE, LBC, and MaxQ in Tab.9. Fora fair comparison, their train speeds are tested with the sametraining script on the same machine with an NVIDIA RTX3090 and automatic mixed precision. SR-STE is the fastest.MaxQ, although about 15% slower than SR-STE, achievesthe highest top-1 accuracy among the three methods. Thedecrease in train speed primarily results from the multi-axisquery, which incurs high memory access but involves min-imal computational workload. Furthermore, although LBChas the theoretically least training FLOPs, it is the slowestamong the three methods. Compared to the train speed ofdense network, LBC is almost 50% slower for a batch sizeof 128 and 30% slower for a batch size of 256. In summary,MaxQ stands out as the most efﬁcient algorithm, strikingthe best balance between training speed and accuracy.4.7. QuantizationA structured re-parameterized network with simple PTQmay suffer from signiﬁcant degradation in accuracy [6,8],which is completely unusable. To gain insights into thecharacteristics of MaxQ quantization, we ﬁrst visualizethe weight distribution of SR-STE and MaxQ in Fig.6and Fig.7respectively. The weight distribution of SR-STE and MaxQ exhibits notable differences for the samemodel. Speciﬁcally, in shallower layers, MaxQ has asharper weight distribution. While in deeper layers, it issmoother and has a broader dynamic range. We conductedquantization experiments on SR-STE and MaxQ for com-parison. The results are presented in Tab.10. To our sur-prise, our MaxQ demonstrates quantization-friendliness de-spite being perceived as a self-structured re-parameterizedprocess. For ResNet50, MaxQ with a 2:4 sparse pattern canachieve 0.5% drop in the top-1 accuracy (77.6%Ñ77.1%),when it is quantized to INT8 using a simple uniform PTQmethod provided by the Ascend Tensor Compiler (ATC).Similar results are observed for SR-STE. These experimentsindicate that the weight distribution obtained by MaxQ arealso applicable to quantize. Furthermore, the performanceof the INT8 model can still be improved by the more ad-vanced quantization methods, such as non-uniform PTQand Quantization-Aware Traning (QAT).5. ConclusionN:M sparsity is a crucial method to reduce inference over-head and enable fast inference on NVIDIA Ampere GPUs.In this paper, we propose a novel Multi-Axis Query method,MaxQ, to identify the critical weights and build a high-performance N:M sparsity network. During the training,MaxQ employs a dynamic approach to generate soft N:Mmasks, which enhances the weights with more importanceand ensures more effective updates for them. During theruntime, soft N:M masks can be folded into the network asconstants, which will not cause any distortion to the sparsepattern or bring additional computational costs. Further,MaxQ follows a gradual sparse schedule by increasing thepercentage of N:M weight blocks. It progressively allowsthe network to heal from sparsity, avoiding severe infor-mation loss and achieving stable and efﬁcient convergence.Experiments on various vision tasks demonstrate that MaxQcan achieve notable performance gains over the state-of-the-art methods on multiple N:M sparse patterns and CNNs.AcknowledgementsThis work is funded in part by the KeyResearch and Development Project of Zhejiang Provinceunder Grant 2021C01035.
15852
References
[1]Zhou Aojun, Ma Yukun, Zhu Junnan, Liu Jianbo, Zhang
Zhijie, Yuan Kun, Sun Wenxiu, and Li Hongsheng. Learn-
ing n:m ﬁne-grained structured sparse neural networks from
scratch. In International Conference on Learning Represen-
tations , 2021. 1,2,5,6
[2]Jihye Back, Namhyuk Ahn, and Jangho Kim. Mag-
nitude attention-based dynamic pruning. arXiv preprint
arXiv:2306.05056 , 2023. 6
[3]Yoshua Bengio, Nicholas L ´eonard, and Aaron Courville.
Estimating or propagating gradients through stochastic
neurons for conditional computation. arXiv preprint
arXiv:1308.3432 , 2013. 2
[4]Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu
Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,
Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tian-
heng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu,
Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,
Chen Change Loy, and Dahua Lin. MMDetection: Open
mmlab detection toolbox and benchmark. arXiv preprint
arXiv:1906.07155 , 2019. 6
[5]Zhuangzhi Chen, Jingyang Xiang, Yao Lu, Qi Xuan, Zhen
Wang, Guanrong Chen, and Xiaoniu Yang. Rgp: Neural net-
work pruning through regular graph with edges swapping.
IEEE Transactions on Neural Networks and Learning Sys-
tems, pages 1–13, 2023. 3
[6]Xiangxiang Chu, Liang Li, and Bo Zhang. Make repvgg
greater again: A quantization-aware approach. arXiv
preprint arXiv:2212.01593 , 2022. 8
[7]Ioana Croitoru, Simion-Vlad Bogolin, and Marius
Leordeanu. Unsupervised learning of foreground ob-
ject segmentation. International Journal of Computer Vision
(IJCV) , 127(9):1279–1302, 2019. 1
[8]Xiaohan Ding, Honghao Chen, Xiangyu Zhang, Kaiqi
Huang, Jungong Han, and Guiguang Ding. Re-
parameterizing your optimizers rather than architectures. In
The Eleventh International Conference on Learning Repre-
sentations , 2022. 8
[9]Erich Elsen, Marat Dukhan, Trevor Gale, and Karen Si-
monyan. Fast sparse convnets. In Proceedings of the
IEEE/CVF conference on computer vision and pattern
recognition , pages 14629–14638, 2020. 3
[10] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Cas-
tro, and Erich Elsen. Rigging the lottery: Making all tickets
winners. In International Conference on Machine Learning
(ICML) , pages 2943–2952, 2020. 2,3,6
[11] Jonathan Frankle and Michael Carbin. The lottery ticket hy-
pothesis: Finding sparse, trainable neural networks. In In-
ternational Conference on Learning Representations , 2019.
2
[12] Ross Girshick. Fast r-cnn. In IEEE International Conference
on Computer Vision (ICCV) , pages 1440–1448, 2015. 1
[13] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In IEEE International Confer-
ence on Computer Vision (ICCV) , pages 580–587, 2014. 1[14] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic net-
work surgery for efﬁcient dnns. Advances in neural informa-
tion processing systems , 29, 2016. 2
[15] Song Han, Huizi Mao, and William J Dally. Deep com-
pression: Compressing deep neural networks with pruning,
trained quantization and huffman coding. arXiv preprint
arXiv:1510.00149 , 2015. 1
[16] Song Han, Jeff Pool, John Tran, and William Dally. Learning
both weights and connections for efﬁcient neural network. In
Advances in Neural Information Processing Systems . Curran
Associates, Inc., 2015. 1,2,3
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 770–778, 2016. 1,5
[18] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-
shick. Mask r-cnn. In IEEE International Conference on
Computer Vision (ICCV) , 2017. 1,5
[19] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning
for accelerating very deep neural networks. In Proceedings
of the IEEE international conference on computer vision ,
pages 1389–1397, 2017. 2
[20] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi
Yang. Soft ﬁlter pruning for accelerating deep convolutional
neural networks. In International Joint Conference on Arti-
ﬁcial Intelligence (IJCAI) , pages 2234–2240, 2018. 1
[21] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang.
Filter pruning via geometric median for deep convolutional
neural networks acceleration. In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 4340–
4349, 2019. 1,3
[22] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015. 1
[23] Zejiang Hou, Minghai Qin, Fei Sun, Xiaolong Ma, Kun
Yuan, Yi Xu, Yen-Kuang Chen, Rong Jin, Yuan Xie, and
Sun-Yuan Kung. Chex: Channel exploration for cnn model
compression. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 12287–
12298, 2022. 3
[24] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-
tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861 , 2017. 1,5
[25] Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner,
Joseph Naor, and Daniel Soudry. Accelerated sparse neural
training: A provable and efﬁcient method to ﬁnd n: m trans-
posable masks. Advances in neural information processing
systems , 34:21099–21111, 2021. 1
[26] Ryan Humble, Maying Shen, Jorge Albericio Latorre, Eric
Darve, and Jose Alvarez. Soft masking for cost-constrained
channel pruning. In European Conference on Computer Vi-
sion, pages 641–657. Springer, 2022. 3
[27] Aditya Kusupati, Vivek Ramanujan, Raghav Somani,
Mitchell Wortsman, Prateek Jain, Sham Kakade, and Ali
Farhadi. Soft threshold weight reparameterization for learn-
15853
able sparsity. In International Conference on Machine
Learning , pages 5544–5555. PMLR, 2020. 2,3,6
[28] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr.
SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON
CONNECTION SENSITIVITY. In International Confer-
ence on Learning Representations , 2019. 1,3
[29] Bin Lin, Ningxin Zheng, Lei Wang, Shijie Cao, Lingxiao
Ma, Quanlu Zhang, Yi Zhu, Ting Cao, Jilong Xue, Yuqing
Yang, et al. Efﬁcient gpu kernels for n: M-sparse weights in
deep learning. Proceedings of Machine Learning and Sys-
tems, 5, 2023. 1
[30] Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang,
Baochang Zhang, Yonghong Tian, and Ling Shao. Hrank:
Filter pruning using high-rank feature map. In IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 1529–1538, 2020. 1,2,3
[31] Mingbao Lin, Yuxin Zhang, Yuchao Li, Bohong Chen, Fei
Chao, Mengdi Wang, Shen Li, Yonghong Tian, and Ron-
grong Ji. 1xn pattern for pruning convolutional neural net-
works. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 2022. 3
[32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In Eu-
ropean Conference on Computer Vision (ECCV) , pages 740–
755, 2014. 2,5
[33] Hanxiao Liu, Karen Simonyan, and Yiming Yang.
Darts: Differentiable architecture search. arXiv preprint
arXiv:1806.09055 , 2018. 1
[34] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang,
Shoumeng Yan, and Changshui Zhang. Learning efﬁcient
convolutional networks through network slimming. In Pro-
ceedings of the IEEE international conference on computer
vision , pages 2736–2744, 2017. 3
[35] Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko
Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and
Paulius Micikevicius. Accelerating sparse deep neural net-
works. arXiv preprint arXiv:2104.08378 , 2021. 1,3
[36] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio,
and Jan Kautz. Importance estimation for neural network
pruning. Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 11264–11272, 2019.
3
[37] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yely-
sei Bondarenko, Mart Van Baalen, and Tijmen Blankevoort.
A white paper on neural network quantization. arXiv
preprint arXiv:2106.08295 , 2021. 1
[38] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In Advances in neural information pro-
cessing systems , pages 91–99, 2015. 5
[39] Karen Simonyan and Andrew Zisserman. Very deep con-
volutional networks for large-scale image recognition. In In-
ternational Conference on Learning Representations (ICLR) ,
2015. 1
[40] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herv ´eJ´egou. Trainingdata-efﬁcient image transformers & distillation through at-
tention. In International Conference on Machine Learning
(ICML) , pages 10347–10357, 2021. 1
[41] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and
Hai Li. Learning structured sparsity in deep neural net-
works. Advances in neural information processing systems ,
29, 2016. 2
[42] Mitchell Wortsman, Ali Farhadi, and Mohammad Rastegari.
Discovering neural wirings. Advances in Neural Information
Processing Systems , 32, 2019. 6
[43] Yuxin Zhang, Mingbao Lin, Mengzhao Chen, Fei Chao, and
Rongrong Ji. Optg: Optimizing gradient-driven criteria in
network sparsity. arXiv preprint arXiv:2201.12826 , 2022. 6
[44] Yuxin Zhang, Mingbao Lin, ZhiHang Lin, Yiting Luo, Ke
Li, Fei Chao, YONGJIAN WU, and Rongrong Ji. Learning
best combination for efﬁcient n:m sparsity. In Advances in
Neural Information Processing Systems , 2022. 1,2,5,6
[45] Yuxin Zhang, Yiting Luo, Mingbao Lin, Yunshan Zhong,
Jingjing Xie, Fei Chao, and Rongrong Ji. Bi-directional
masks for efﬁcient N: M sparse training. In International
Conference on Machine Learning , pages 41488–41497.
PMLR, 2023. 1
[46] Michael Zhu and Suyog Gupta. To prune, or not to prune: ex-
ploring the efﬁcacy of pruning for model compression. In In-
ternational Conference on Learning Representations Work-
shop (ICLRW) , 2017. 6
15854
