Video-P2P: Video Editing with Cross-attention Control
Shaoteng Liu1Yuechen Zhang1Wenbo Li1Zhe Lin3Jiaya Jia1,2
1The Chinese University of Hong Kong2SmartMore3Adobe
Figure 1. Video-P2P generates new characters while optimally maintaining the pose and environment in videos.
Abstract
Video-P2P is the ﬁrst framework for real-world video
editing with cross-attention control. While attention con-
trol has proven effective for image editing with pre-trained
image generation models, there are currently no large-scale
video generation models publicly available. Video-P2P ad-
dresses this limitation by adapting an image generation
diffusion model to complete various video editing tasks.
Speciﬁcally, we propose to ﬁrst tune a Text-to-Set (T2S)
model to complete an approximate inversion and then opti-
mize a shared unconditional embedding to achieve accurate
video inversion with a small memory cost. We further prove
that it is crucial for consistent video editing. For attention
control, we introduce a novel decoupled-guidance strategy,
which uses different guidance strategies for the source and
target prompts. The optimized unconditional embedding forthe source prompt improves reconstruction ability, while an
initialized unconditional embedding for the target prompt
enhances editability. Incorporating the attention maps of
these two branches enables detailed editing. These techni-
cal designs enable various text-driven editing applications,
including word swap, prompt reﬁnement, and attention re-
weighting. Video-P2P works well on real-world videos for
generating new characters while optimally preserving their
original poses and scenes. It signiﬁcantly outperforms pre-
vious approaches.
1. Introduction
Video creation and editing are key tasks [ 16,17,25,37,42].
Text-driven editing becomes one promising pipeline. Sev-
eral methods have demonstrated the ability to edit gener-
ated or real-world images with text prompts [ 13,19,24].
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
8599
Figure 2. Video-P2P vs Image-P2P. Editing a video frame-by-
frame (Image-P2P) cannot guarantee semantic consistency across
frames. Video-P2P enables changing the penguin into the same
robotic type in every frame.
Till now, it is still challenging to edit only local objects
in a video, such as changing a running “dog” into a “cat”
without inﬂuencing the environment. This paper proposes
a pipeline that can edit a video both locally and globally, as
shown in Figs. 1and5.
Text-driven image editing requires a model capable of
generating target content, such as changing the category or
property of an object. Diffusion models have demonstrated
outstanding generation capabilities in this area [ 3,13,19,
40]. Among these methods, attention control emerges as the
most effective pipeline for detailed image editing [ 13,24].
In order to edit a real image, this pipeline includes two nec-
essary steps: (1) inverting the image into latent features with
a pre-trained diffusion model, and (2) controlling attention
maps in the denoising process to edit the corresponding
parts of the image. For example, by swapping their attention
maps, we can replace a “child” with a “panda”.
In this paper, we aim to build an attention control-based
pipeline for video editing. Since no large-scale pre-trained
video generation models are publicly available, we propose
a novel framework to show that a pre-trained image diffu-
sion model can be adapted for detailed video editing.
While a pre-trained image diffusion model can be uti-
lized for video editing by processing frames individually
(Image-P2P), it lacks semantic consistency across frames
(the 2nd row of Fig. 2). To maintain semantic consistency,
we propose using a structure on inversion and attention con-
trol for all frames, by transforming the Text-to-image diffu-
sion model (T2I) into a Text-to-set model (T2S). This ap-
proach is effective, as illustrated in the 3rd row, where the
robotic penguin maintains its consistency across frames.We adopt the method proposed in [ 47] to convert a T2I
model into a T2S model by altering the convolution ker-
nels and replacing the self-attentions with frame-attentions.
This conversion yields a model that generates a set of se-
mantically consistent images. The generation quality will
be degraded with the inﬂation step but it can be recovered
after tuning on the original video. Although the tuned T2S
model is not an ideal video generation model, it sufﬁces
to create an approximate inversion for a video as shown in
Fig. 3(c). It is just an approximation because errors are
accumulated in the denoising pass, consistent with conclu-
sions in [ 24,43].
To improve the inversion quality, we propose to optimize
a shared unconditional embedding for all frames to align the
denoising latent features with the diffusion latent features.
Our experiments show that shared embedding is efﬁcient for
video inversion and crucial for video editing. Comparisons
are shown in Fig. 3and Fig. 9.
As discussed in [ 13], successful attention control re-
quires a model to have both reconstruction ability and ed-
itability. While image inversion has been argued to possess
both abilities in [ 24], we ﬁnd that video editing presents
different challenges. The T2S model, as an inﬂation model
not trained on any videos, is not robust to the perturbations
caused by various unconditional embeddings. Although our
optimized embedding can achieve reconstruction, chang-
ing prompts can destabilize the model and result in a low-
quality generation. On the other hand, we ﬁnd that the ap-
proximate inversion with an initialized unconditional em-
bedding (the text embedding of an empty prompt) is ed-
itable but cannot reconstruct well. To address this issue, we
propose a decoupled-guidance strategy in attention control,
utilizing different guidance strategies for the source and tar-
get prompts. Speciﬁcally, we use the optimized uncondi-
tional embedding for the source prompt and the initialized
unconditional embedding for the target prompt. We incor-
porate the attention maps from these two branches to gener-
ate the target video. These two simple designs prove effec-
tive and successfully complete video editing. Our contribu-
tions can be summarized as:
•We propose the ﬁrst framework for video editing with
attention control. A decoupled-guidance strategy is de-
signed to further improve performance.
•We introduce an effective and efﬁcient video inversion
method with shared unconditional embedding optimiza-
tion to improve video editing substantially.
•We conduct extensive ablation studies and comparisons
to show the effectiveness of our video editing framework.
8600
Figure 3. Inversion Comparison. (b) The inﬂated model cannot generate high-quality results. (c) Tuning the model can create an approxi-
mate video inversion. (d) Optimizing a shared unconditional embedding can accurately reconstruct the input video.
2. Related Work
2.1. Text Driven Generation
DALL-E [ 32] ﬁrst considers the text-to-image (T2I) gen-
eration task as a sequence-to-sequence translation prob-
lem, with subsequent research improving generation qual-
ity [6,10,50]. Denoising Diffusion Probabilistic Mod-
els (DDPMs)[ 15] have recently gained popularity for T2I.
GLIDE[ 26] utilizes classiﬁer-free guidance to improve text
conditioning. DALLE-2 [ 33] leverages CLIP [ 31] for
better text-image alignment. Latent Diffusion Models
(LDMs) [ 34] propose processing in the latent space to en-
hance training efﬁciency. In our work, we employ a pre-
trained image diffusion model based on LDMs.
Text-to-video (T2V) generation is a nascent research
area. GODIV A [ 46] ﬁrst introduces VQ-V AE [ 41] to T2V .
CogVideo [ 18] combines T2V with CogView-2 [ 6], uti-
lizing pre-trained text-to-image models. Video Diffusion
Models (VDM) [ 17] propose a space-time U-Net for per-
forming diffusion on pixels. Imagen Video [ 16] success-
fully generates high-quality videos with cascaded diffusion
models and v-prediction parameterization. Phenaki [ 42]
generates videos with time-variable prompts. Make-A-
Video [ 37] combines the appearance generation of T2I
models with movement information from video data. While
these approaches generate reasonable short videos, they still
contain artifacts and do not support real-world video edit-
ing. Additionally, most of these approaches are not pub-
licly available at this time. Tune-A-Video [ 47] inﬂates an
image diffusion model into a video model and tunes it to
reconstruct the input video. It allows for changes in seman-
tic content but with limited temporal consistency. We ﬁnd
that using DDIM inversion results can improve its tempo-
ral consistency. However, it cannot avoid altering unrelated
regions. We adapt some designs of TA V to do our model
initialization.
2.2. Text Driven Editing
Generative models have demonstrated impressive perfor-
mance in image editing, with approaches ranging from
GANs [ 12,27,28,44] to diffusion models [ 1,19].
SDEdit [ 22] adds noise to an input image and uses the
diffusion process to recover an edited version. Prompt-to-Prompt [ 13] uses attention control to minimize changes to
unrelated parts, while Null-Text Inversion [ 24] improves
real image editing. Plug-and-Play [ 40] uses feature and
self-attention injection, which are unable to avoid edit-
ing the whole image when changing a local part. In-
structPix2Pix [ 3] enables ﬂexible text-driven editing with
user-provided instructions. Textual Inversion [ 11], Dream-
Booth [ 35], and Custom-Diffusion [ 20] learn special to-
kens for personalized concepts and generate related images.
Some improved inversion methods [ 7,23] are proposed af-
ter our work.
Video editing with generative models has seen sev-
eral advances recently. Text2Live [ 2] employs CLIP to
edit textures in videos but struggles with signiﬁcant se-
mantic changes. Dreamix [ 25] uses a pre-trained Imagen
Video [ 16] backbone to perform image-to-video and video-
to-video editing, with the ability to change motion as well.
Gen-1 [ 8] trains models jointly on images and videos for
tasks such as stylization and customization. While these
methods enable modifying video content, they operate like
guided generation and tend to modify all regions together
when editing an object. Our proposed method allows for
local editing with a diffusion model pre-trained on images.
Besides, there have been some concurrent works and fol-
lowers after our work, such as [ 5,9,21,30,45,49,52].
Video-P2P focuses more on introducing the image diffu-
sion models to detailed video editing, instead of conditional
video generation.
3. Method
LetVbe a real video containing nframes. We adopt the
Prompt-to-Prompt setting by introducing a source prompt P
and an edited prompt P⇤which together generate an edited
video V⇤containing nframes. The prompts are provided
by the user. Similar to TA V [ 47], we assume that the object
of interest is present in the ﬁrst frame.
To achieve cross-attention control in video editing, we
propose Video-P2P, a framework with two key technical
designs: (1) optimizing a shared unconditional embedding
for video inversion, and (2) using different guidance for the
source and edited prompts, and incorporating their attention
maps. The framework is illustrated in Fig. 4.
8601
3.1. Preliminary
Latent Diffusion Models (LDMs). LDMs generate an im-
age latent z0using a random noise vector ztand a textual
condition Pas inputs. As variants of DDPMs, these models
aim to predict artiﬁcial noise by minimizing the following
objective:
min
✓Ez0,"⇠N(0,I),t⇠Uniform (1,T)k" "✓(zt,t ,C)k2
2,(1)
where C= (P)is the embedding of the text prompt, and
noise "is added to z0according to step tto obtain zt. Dur-
ing inference, the model predicts noise "✓(·)forTsteps to
generate an image from zT.
DDIM sampling and inversion. Deterministic DDIM
sampling can be used to generate an image from latent fea-
tures in a small number of denoising steps:
zt 1=r↵t 1
↵tzt+ r
1
↵t 1 1 r
1
↵t 1!
·"✓(zt,t ,C).
(2)
We use an encoder to encode the real image before the dif-
fusion process and a decoder to decode after the denoising
process. DDIM sampling can be reversed in a few steps
through the equation:
zt+1=r↵t+1
↵tzt+ r
1
↵t+1 1 r
1
↵t 1!
·"✓(zt,t ,C),
(3)
known as DDIM inversion [ 38]. This can be used to obtain
the corresponding latent features of a real image.
Null-text inversion. To mitigate the ampliﬁcation effect
of text conditioning during image generation, classiﬁer-free
guidance is proposed, which performs unconditional pre-
diction [ 14]:
˜"✓(zt,t ,C,?)=w·"✓(zt,t ,C)+( 1  w)·"✓(zt,t ,?),
(4)
where ?= (””) is the embedding of a null text and wis
the guidance weight. However, the classiﬁer-free guidance
increases errors accumulated in the denoising process, lead-
ing to imperfect image reconstruction using the DDIM in-
version. [ 24] proposes to align the diffusion latent trajectory
z⇤
T,...,z⇤
0with the denoising latent trajectory zT,...,z 0
by optimizing a step-wise unconditional embedding ?t:
min
?t  z⇤
t 1 zt 1  2
2. (5)
3.2. Video Inversion
We begin by constructing a T2S model that is capable of
performing an approximate inversion. Following the VDM
baselines [ 16,17] and TA V [ 47], we employ 1⇥3⇥3pattern
convolution kernels and temporal attention. Moreover, we
Figure 4. Framework. We optimize one shared unconditional em-
bedding for the reconstruct branch (orange). The initialized un-
conditional embedding is utilized for the editable branch (green).
Their attention maps are incorporated to create the target video.
replace the self-attentions with frame-attentions, which take
the ﬁrst frames v0and the current frame vias inputs and
update features for the frame vi. The formulation of the
frame-attention is as follows:
Q=WQvi,K=WKv0,V=WVv0, (6)
where Ware the projection matrices in attention. Then,
attention maps are calculated as:
M= Softmax✓QKT
p
d◆
, (7)
where dis the latent projection dimension. The model pro-
cesses a video pair-by-pair and computes ntimes to obtain
the prediction for every frame. We ﬁnd that the simple de-
sign sufﬁces for video inversion since the reversed latent
features can capture temporal information. Besides, frame-
attention conserves memory and speeds up the process.
While model inﬂation can aid in preserving semantic
consistency across frames, it adversely impacts the gener-
ation quality of the T2I model. This is because the self-
attention parameters are utilized to compute frame correla-
tions, which have not been pre-trained. Consequently, the
T2S model, generated through inﬂation, is insufﬁcient for
the approximate inversion, as demonstrated in Fig. 2. To ad-
dress this, we ﬁne-tune the query projection matrices WQ
of the frame- and cross-attentions, as well as additional tem-
poral attention, to perform noise prediction based on the in-
put video following [ 47]. After this initialization, the T2S
model is capable of generating semantically consistent im-
age sets while maintaining the quality of each frame, result-
ing in successful approximate inversion.
Using the ﬁne-tuned T2S model, we perform video in-
version by optimizing a shared unconditional embedding.
During inversion, each latent feature ztcontains a channel
for the frames with dimension n, where zt,idenotes the la-
tent feature for the i-th frame. We employ DDIM inversion
8602
to generate latent features z⇤
0,...,z⇤
T. The unconditional
embedding is deﬁned as follows:
min
?tnX
i=1  z⇤
t 1,i zt 1,i(¯zt,i,¯zt,0,?t,C)  2
2,where (8)
¯zt 1,i=zt 1,i(¯zt,i,¯zt,0,?t,C) (9)
is updated at each step. The T2S model’s frame-attentions
use two latent features to calculate the corresponding fea-
ture for the next step. Notice ?tis shared by all frames ( i=
1,...,n ) which minimizes the memory usage. Besides, us-
ing the same unconditional embedding for all frames avoids
destabilizing the semantic consistency in attention control.
3.3. Decoupled-guidance Attention Control
To perform attention control on real images, existing
works [ 13,24] require an inference pipeline with both re-
construction ability and editability. However, achieving
such a pipeline for a T2S model is challenging. Video in-
version allows us to establish an inference pipeline to recon-
struct the original video well. However, the T2S model is
not as robust as T2I models due to a lack of pre-training with
videos. As a result, its editability is compromised with the
optimized unconditional embedding, leading to degraded
generation quality when changing prompts. In contrast, we
ﬁnd that using an initialized unconditional embedding ?
makes the model more editable while it cannot reconstruct
perfectly. This inspires us to combine the abilities of two
inference pipelines. For the source prompt, we use the opti-
mized unconditional embedding in the classiﬁer-free guid-
ance. For the target prompt, we choose the initialized un-
conditional embedding. We then incorporate attention maps
from these two branches to obtain the edited video, where
the unchanged parts are inﬂuenced by the source branch and
the edited parts are inﬂuenced by the target branch.
The pseudo algorithm is shown in Alg. 1. We adopt the
attention control methods from Image-P2P to Video-P2P.
For example, to perform word swap, the Editfunction can
be represented as:
Edit(Mt,M⇤
t,t): =(
M⇤
tift<⌧
Mtotherwise,(10)
MtandM⇤
tare the cross-attention maps for every frame at
every step, and DMis the tuned T2S model. Changing the
frame-attentions maps has a small inﬂuence on the ﬁnal re-
sults. Attention maps are swapped only for the ﬁrst ⌧steps
because attentions are formed in the early period. Mt,wis
the average attention map of the word wcalculated at step
t. It is averaged over steps T,...,t independently for every
frame. For the j-th frame, we calculate:
Mt,w,j=1
T tTX
i=tMi,w,j j=1,...,n . (11)Algorithm 1: Prompt-to-Prompt video editing
1Input: A source prompt P, a target prompt P⇤,
source video Vsrc.
2Output: Edited video Vdst.
3Latent features from DDIM inversion: zT;
4z⇤
T zT;
5Initialized unconditional embedding ?⇤and
optimized unconditional embedding ?;
6fort=T,T 1,...,1do
7 zt 1,Mt DM(zt,P,t ,?);
8 M⇤
t DM(z⇤
t,P⇤,t ,?⇤);
9 cMt Edit(Mt,M⇤
t,t);
10 z⇤
t 1 DM(z⇤
t,P⇤,t ,?⇤){M⇤
t cMt};
11 ↵ B 
Mt,w 
[B 
M⇤
t,w⇤ 
;
12 z⇤
t 1 (1 ↵) zt 1+↵ z⇤
t 1;
13end
14Return (z0,z⇤
0)
B 
Mt,w 
represents the binary mask obtained from the at-
tention map. A value is set to 1 when larger than a threshold.
4. Experiments
4.1. Implementation Details
We develop our method based on CompVis Stable Diffusion
(v1-5). Similar to TA V [ 47], we ﬁx the image autoencoder
and sample 8 or 24 frames at the resolution of 512 ⇥512
from a video. To initialize the model, we ﬁne-tune the T2S
model for 500 steps to reconstruct the original video. Dur-
ing attention control, we set the cross-attention replacing
ratio to 0.4 and the attention threshold to 0.3. For prompt
reﬁnement, we set the reﬁnement ratio to 0.4. Parameters
can be adjusted to control the editing ﬁdelity for different
examples. All 8-frame experiments are conducted on a sin-
gle V100 GPU, with 5 minutes for initialization (tuning), 6
minutes for inversion, and 1 minute for inference. It is dif-
ﬁcult to avoid the inversion cost when preserving the back-
ground faithfully according to [ 24]. To speed up to less than
one minute, one can replace the null-text inversion with re-
cent inversion methods [ 7,23]. In the appendix, we show
that a shared unconditional embedding works on 24 frames.
Our inversion process extends to over 100 frames. However,
editing longer videos (1min) remains challenging. Video-
P2P and most existing works concentrate on shorter videos.
4.2. Applications
Video-P2P can be utilized for a range of editing applica-
tions, including prompt reﬁnement, attention re-weighting,
and word swapping. Video-P2P is able to maintain semantic
consistency across different frames and preserve the tempo-
ral coherence of the original video during the editing pro-
cess. More examples can be found in the appendix.
8603
Figure 5. Videos edited by Video-P2P with text prompts. Video-P2P can do both word swaps and prompt reﬁnement.
Word swap. Video-P2P enables the replacement of entities
based on word swapping while maintaining the coherence
of unrelated regions. As illustrated in Fig. 5, Video-P2P
seamlessly replaces the man on the motorbike with Spider-
Man while minimizing the changes to the motorbike’s ap-
pearance (the 4th row). The generated Spider-Man exhibits
a consistent appearance across frames, and the background
remains unchanged. Furthermore, we can replace a dog
with a cat while preserving its gesture and the surrounding
grass (the 5th row).
Prompt reﬁnement. Video-P2P is able to do prompt reﬁne-
ment, such as modifying object properties. For example,
we can transform the running dog into a robotic one (the6th row in Fig. 5), and convert a motorbike into a Lego toy
with the same motion (the 3rd row). Notice the grass and
sky are almost not inﬂuenced. Additionally, Video-P2P can
perform global editing like changing the weather to sunset
or ﬂooding the road with water (2nd row). Style transfer
can also be accomplished by Video-P2P, as exempliﬁed by
transforming the video into a watercolor painting.
Attention re–weighting. Similar to Image-P2P, Video-P2P
also enables attention re-weighting. By adjusting the cross-
attention of speciﬁc words, we can manipulate the extent of
the corresponding generation. For instance, we can regulate
how ﬂuffy a dog is in the video (the 6th row of Fig. 5).
8604
Figure 6. Video-P2P v.s. Tune-A-Video (TA V). Video-P2P offers the ability to edit content locally, while TA V+DDIM cannot avoid
inﬂuencing unrelated regions.
Figure 7. Video-P2P v.s. Dreamix. Both methods can change the
dogs to cats. Video-P2P can preserve background’s details.
4.3. Comparison
Comparison with Tune-A-Video. Both TA V+DDIM [ 47]
and our Video-P2P allow for video editing with text
prompts. However, TA V+DDIM cannot avoid altering the
entire video content when editing speciﬁc objects, while
Video-P2P can edit a local area and minimize the inﬂuence
on other regions. Fig. 6(Left) demonstrates that Video-P2P
preserves the complex shape of the cloud when replacing a
lion with King Kong, whereas TA V+DDIM can only main-
tain the color tone of the sky in this case.
Although our model initialization is similar to TA V ,
Video-P2P can still generate temporal-consistent results
where TA V+DDIM fails. As demonstrated in Fig. 6(Right),
TA V struggles to generate a temporally consistent sequence
in the second row, even when the inputs are features from
DDIM inversion. In contrast, our method can produce bet-
ter structure-preserved results, as shown in the third row.Comparison with Dreamix. In contrast to Dreamix [ 25],
which uses a pre-trained video diffusion model that is not
publicly available, our method yields superior results for
subject replacement. Although our method cannot perform
video motion editing due to the lack of temporal priors, we
outperform Dreamix in preserving details and motion con-
sistency. As it is not open-sourced, we conducted our eval-
uation on its released demo. As demonstrated in Fig. 7,
both methods can transform two dogs into two cats, but
our method preserves the details of the drawer in the back-
ground (the 3rd row). Furthermore, Dreamix may affect the
time sequence to some extent, as the generated cat moves
more slowly than the original dog in the video. Our method
completely preserves the motion of the original video.
Quantitative results. We create a test set of 45 samples
based on the DA VIS [ 29] dataset and report ﬁve metrics for
quantitative analysis. The CLIP Score measures the tex-
tual similarity between the text prompt and video, while
Masked PSNR and LPIPS [ 51] evaluate the quality of struc-
ture preservation. We also proposed a novel metric, Ob-
ject Semantic Variance (OSV), to measure semantic consis-
tency across frames. In addition, we report an average CLIP
similarity (Temp) between consecutive frames (as in Gen-
1[8] and FateZero [ 30]) to evaluate the temporal coherence.
For detailed explanations of these metrics, please refer to
the appendix. Our results, as shown in Table 1, demon-
strate that Video-P2P performs well on all metrics. Com-
pared to TA V+DDIM, Video-P2P achieves higher Masked
PSNR and lower LPIPS, indicating better preservation of
unchanged regions. Compared to the other two methods,
Video-P2P has much lower OSV and Temp, indicating its
superior ability to maintain semantic and temporal consis-
tency across frames. When comparing to Image-P2P, we
8605
Figure 8. Model initialization improves the generation quality.
SharedMultipleFigure 9. Ablation on shared unconditional embeddings.A Lego child is driving a bike on the road.
w/ Decoupledw/o DecoupledFigure 10. Ablation on decoupled-guidance attention control.
consider it as a reference point rather than a direct competi-
tor because it does not account for the consistency between
different frames. Image-P2P represents the upper bound of
PSNR and LPIPS, which serve as metrics to evaluate per-
frame quality. Among all video editing methods, Video-
P2P is the best one. Moreover, in Tab. 3, we report the user
study results, where Video-P2P ranks ﬁrst on average and
has a high preference rate compared to other methods.
4.4. Ablation Study
Model initialization. While the inﬂated image diffusion
model can generate semantically consistent images, the T2S
model’s generation ability is compromised during inﬂation.
As seen in Fig. 8(the 3rd column), directly using the in-
ﬂated T2S model produces unrealistic results with an inac-
curate background. To mitigate this, we initialize the T2S
model by ﬁne-tuning the given video. This is evident in
Fig.8(4th column), where the cat’s appearance improves,
and the grass reconstruction becomes more accurate.
Shared unconditional embedding. The use of shared un-
conditional embeddings is crucial to ensure stable and con-
sistent generations. On our test set, we found that 10% of
the videos exhibit instability when multiple unconditional
embeddings are employed. A visual example is included
in Fig. 9(Right). As shown in Tab. 1(row 5), seman-
tic and temporal consistency are diminished without shared
unconditional embedding, reminiscent of the content code
in [39]. We observe that optimizing a shared unconditional
embedding can signiﬁcantly improve the PSNR compared
to TA V+DDIM in Tab. 2. However, using multiple un-
conditional embeddings for each frame only increases the
PSNR by 0.2 but results in higher parameter usage ( ntimes)CLIP "M.PSNR "LPIPS #OSV #Temp "
TA V+DDIM 0.3343 17.37 0.4651 55.67 0.9697
Image-P2P 0.3272 22.92 0.3082 76.92 0.9021
Ours (w/o MI) 0.3213 20.62 0.3255 50.27 0.9460
Ours (w/o DG) 0.3226 18.97 0.3866 68.75 0.9451
Ours (w/o SU) 0.3351 20.64 0.3221 48.78 0.9665
Ours 0.3367 20.65 0.3213 47.38 0.9725
Table 1. Quantitative evaluation. We evaluate textual similar-
ity (CLIP), region preservation (Masked PSNR, LPIPS), semantic
consistency (OSV), and temporal consistency (Temp). DG and SU
mean Decoupled-Guidance and shared unconditional embeddings.
VQV AETAV
+DDIMMulti-
uncondShared-
uncond
PSNR(dB) " 24.73 15.43 22.97 22.75
#Param. # / 0.13M 22.68M 2.94M
Table 2. Video reconstruction quality. A shared unconditional
embedding can reconstruct a high-quality video with a small size.
Image-P2P TA V TA V+DDIM Video-P2P
Structure 2.67 3.33 2.61 1.39
Preserving 13.59% 6.52% 10.87% 69.02%
Text 3.40 2.78 2.28 1.54
Alignment 3.80% 14.13% 19.57% 62.50%
Realism & 3.38 2.98 2.21 1.43
Quality 4.35% 7.61% 19.02% 69.02%
Table 3. User study of average ranking #and preference rate ".
and leads to a lower Masked PSNR after attention control.
Thus, we conclude that shared unconditional embedding is
the most effective and efﬁcient method for video inversion.
Decoupled-guidance attention control. To obtain the la-
tent features of the input video, we optimize an uncondi-
tional embedding using the source prompt. It is important
to note that this embedding is only suitable for the source
prompt during the prompt-to-prompt process. Using the
optimized embedding for the target prompt may negatively
impact the quality of the generated results, as shown in
Fig.10(Left). Instead, we utilize the initialized uncondi-
tional embedding for the target prompt and incorporate at-
tention maps from two branches. The decoupled-guidance
attention control approach signiﬁcantly improves the edit-
ing quality, as shown in Fig. 10(Right). Quantitative abla-
tions can be found in Tab. 1(the 4th row and 6th row).
5. Conclusion
Video-P2P provides the ﬁrst framework for video editing
with cross-attention control. We optimize a shared uncondi-
tional embedding based on a well-initialized T2S model for
video inversion. We also propose the decoupled-guidance
strategy for attention control. These techniques enable
Video-P2P to perform various applications, such as word
swap, prompt reﬁnement, and attention re-weighting.
Acknowlegdement Supported in part by the Research
Grants Council under the Areas of Excellence scheme grant
AoE/E-601/22-R and the Shenzhen Science and Technol-
ogy Program under No. KQTD20210811090149095.
8606
References
[1]Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended
latent diffusion. arXiv preprint arXiv:2206.02779 , 2022. 3
[2]Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-
ten, and Tali Dekel. Text2live: Text-driven layered image
and video editing. In Computer Vision–ECCV 2022: 17th
European Conference, Tel Aviv, Israel, October 23–27, 2022,
Proceedings, Part XV , pages 707–723, 2022. 3,2
[3]Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structpix2pix: Learning to follow image editing instructions.
arXiv preprint arXiv:2211.09800 , 2022. 2,3
[4]Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´eJ´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In Pro-
ceedings of the IEEE/CVF international conference on com-
puter vision , pages 9650–9660, 2021. 1
[5]Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra.
Pix2video: Video editing using image diffusion. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 23206–23217, 2023. 3
[6]Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.
Cogview2: Faster and better text-to-image generation via hi-
erarchical transformers. arXiv preprint arXiv:2204.14217 ,
2022. 3
[7]Wenkai Dong, Song Xue, Xiaoyue Duan, and Shumin Han.
Prompt tuning inversion for text-driven image editing using
diffusion models. arXiv preprint arXiv:2305.04441 , 2023. 3,
5
[8]Patrick Esser, Johnathan Chiu, Parmida Atighehchian,
Jonathan Granskog, and Anastasis Germanidis. Structure
and content-guided video synthesis with diffusion models.
arXiv preprint arXiv:2302.03011 , 2023. 3,7
[9]Ruoyu Feng, Wenming Weng, Yanhui Wang, Yuhui Yuan,
Jianmin Bao, Chong Luo, Zhibo Chen, and Baining Guo.
Ccedit: Creative and controllable video editing via diffusion
models. arXiv preprint arXiv:2309.16496 , 2023. 3
[10] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,
Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-
based text-to-image generation with human priors. In Com-
puter Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part XV ,
pages 89–106, 2022. 3
[11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-
nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-
Or. An image is worth one word: Personalizing text-to-
image generation using textual inversion. arXiv preprint
arXiv:2208.01618 , 2022. 3
[12] Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano,
Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip-
guided domain adaptation of image generators. ACM Trans-
actions on Graphics (TOG) , pages 1–13, 2022. 3
[13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kﬁr Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
age editing with cross attention control. arXiv preprint
arXiv:2208.01626 , 2022. 1,2,3,5
[14] Jonathan Ho and Tim Salimans. Classiﬁer-free diffusion
guidance. arXiv preprint arXiv:2207.12598 , 2022. 4[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems , pages 6840–6851, 2020. 3
[16] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben
Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
video: High deﬁnition video generation with diffusion mod-
els.arXiv preprint arXiv:2210.02303 , 2022. 1,3,4
[17] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William
Chan, Mohammad Norouzi, and David J Fleet. Video dif-
fusion models. arXiv preprint arXiv:2204.03458 , 2022. 1,3,
4
[18] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,
and Jie Tang. Cogvideo: Large-scale pretraining for
text-to-video generation via transformers. arXiv preprint
arXiv:2205.15868 , 2022. 3
[19] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen
Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:
Text-based real image editing with diffusion models. arXiv
preprint arXiv:2210.09276 , 2022. 1,2,3
[20] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Shechtman, and Jun-Yan Zhu. Multi-concept customization
of text-to-image diffusion. arXiv preprint arXiv:2212.04488 ,
2022. 3
[21] Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu,
and Jiashi Feng. Magicedit: High-ﬁdelity and temporally
coherent video editing. arXiv preprint arXiv:2308.14749 ,
2023. 3
[22] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-
Yan Zhu, and Stefano Ermon. Sdedit: Image synthesis and
editing with stochastic differential equations. arXiv preprint
arXiv:2108.01073 , 2021. 3
[23] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki
Tanaka. Negative-prompt inversion: Fast image inversion
for editing with text-guided diffusion models. arXiv preprint
arXiv:2305.16807 , 2023. 3,5
[24] Ron Mokady, Amir Hertz, Kﬁr Aberman, Yael Pritch,
and Daniel Cohen-Or. Null-text inversion for editing real
images using guided diffusion models. arXiv preprint
arXiv:2211.09794 , 2022. 1,2,3,4,5
[25] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav
Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid
Hoshen. Dreamix: Video diffusion models are general video
editors. arXiv preprint arXiv:2302.01329 , 2023. 1,3,7
[26] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741 , 2021. 3
[27] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan
Zhu. Semantic image synthesis with spatially-adaptive nor-
malization. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 2337–2346,
2019. 3
[28] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,
and Dani Lischinski. Styleclip: Text-driven manipulation of
stylegan imagery. In Proceedings of the IEEE/CVF Inter-
8607
national Conference on Computer Vision , pages 2085–2094,
2021. 3
[29] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-
bel´aez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017
davis challenge on video object segmentation. arXiv preprint
arXiv:1704.00675 , 2017. 7
[30] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,
Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-
ing attentions for zero-shot text-based video editing. arXiv
preprint arXiv:2303.09535 , 2023. 3,7
[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763, 2021. 3
[32] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In International Confer-
ence on Machine Learning , pages 8821–8831, 2021. 3
[33] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022. 3
[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10684–10695, 2022. 3
[35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kﬁr Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. arXiv preprint arXiv:2208.12242 , 2022. 3
[36] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 , 2014. 1
[37] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
Oran Gafni, et al. Make-a-video: Text-to-video generation
without text-video data. arXiv preprint arXiv:2209.14792 ,
2022. 1,3
[38] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 4
[39] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan
Kautz. Mocogan: Decomposing motion and content for
video generation. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1526–1535,
2018. 8
[40] Narek Tumanyan, Michal Geyer, Shai Bagon, and
Tali Dekel. Plug-and-play diffusion features for text-
driven image-to-image translation. arXiv preprint
arXiv:2211.12572 , 2022. 2,3
[41] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete
representation learning. Advances in neural information pro-
cessing systems , 2017. 3
[42] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-
dermans, Hernan Moraldo, Han Zhang, Mohammad TaghiSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.
Phenaki: Variable length video generation from open domain
textual description. arXiv preprint arXiv:2210.02399 , 2022.
1,3
[43] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Ex-
act diffusion inversion via coupled transformations. arXiv
preprint arXiv:2211.12446 , 2022. 2
[44] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,
Jan Kautz, and Bryan Catanzaro. High-resolution image syn-
thesis and semantic manipulation with conditional gans. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 8798–8807, 2018. 3
[45] Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao,
Xinlong Wang, and Chunhua Shen. Zero-shot video editing
using off-the-shelf image diffusion models. arXiv preprint
arXiv:2303.17599 , 2023. 3
[46] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji,
Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Gen-
erating open-domain videos from natural descriptions. arXiv
preprint arXiv:2104.14806 , 2021. 3
[47] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,
Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and
Mike Zheng Shou. Tune-a-video: One-shot tuning of image
diffusion models for text-to-video generation. arXiv preprint
arXiv:2212.11565 , 2022. 2,3,4,5,7
[48] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey
Shi, and Zhangyang Wang. Sinnerf: Training neural radiance
ﬁelds on complex scenes from a single image. In ECCV ,
2022. 1
[49] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change
Loy. Rerender a video: Zero-shot text-guided video-to-video
translation. arXiv preprint arXiv:2306.07954 , 2023. 3
[50] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-
jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-
fei Yang, Burcu Karagol Ayan, et al. Scaling autoregres-
sive models for content-rich text-to-image generation. arXiv
preprint arXiv:2206.10789 , 2022. 3
[51] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR , 2018. 7,1
[52] Yuyang Zhao, Enze Xie, Lanqing Hong, Zhenguo Li,
and Gim Hee Lee. Make-a-protagonist: Generic video
editing with an ensemble of experts. arXiv preprint
arXiv:2305.08850 , 2023. 3
8608
