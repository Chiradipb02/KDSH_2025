DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis
Yuming Gu1,2, Hongyi Xu2, You Xie2, Guoxian Song2, Yichun Shi2,
Di Chang1,2,Jing Yang1, Linjie Luo2
1University of Southern California,2ByteDance Inc.
https://freedomgu.github.io/DiffPortrait3D
{yuminggu,dichang,jyang010 }@usc.edu
{hongyixu,you.xie,guoxian.song,yichun.shi,linjie.luo }@bytedance.com
Abstract
We present DiffPortrait3D, a conditional diffusion model
that is capable of synthesizing 3D-consistent photo-realistic
novel views from as few as a single in-the-wild portrait.
Specifically, given a single RGB input, we aim to synthe-
size plausible but consistent facial details rendered from
novel camera views with retained both identity and facial
expression. In lieu of time-consuming optimization and fine-
tuning, our zero-shot method generalizes well to arbitrary
face portraits with unposed camera views, extreme facial
expressions, and diverse artistic depictions. At its core,
we leverage the generative prior of 2D diffusion models
pre-trained on large-scale image datasets as our render-
ing backbone, while the denoising is guided with disentan-
gled attentive control of appearance and camera pose. To
achieve this, we first inject the appearance context from the
reference image into the self-attention layers of the frozen
UNets. The rendering view is then manipulated with a novel
conditional control module that interprets the camera pose
by watching a condition image of a crossed subject from
the same view. Furthermore, we insert a trainable cross-
view attention module to enhance view consistency, which
is further strengthened with a novel 3D-aware noise gen-
eration process during inference. We demonstrate state-of-
the-art results both qualitatively and quantitatively on our
challenging in-the-wild and multi-view benchmarks.
1. Introduction
Faithfully reconstructing the 3d appearance of human faces
from a single 2D unconstrained portrait is a long-standing
goal for computer vision, with a wide range of downstream
applications in visual effects, digital avatars, 3D animation,
and many others. In this work, we challenge ourselves to
synthesize high-fidelity consistent novel views from as few
as a single portrait, with high resemblance to the inputs
novel view synthesis referenceFigure 1. Given a single portrait as reference (left), DiffPortrait3D
is adept at producing high-fidelity and 3d-consistent novel view
synthesis (right). Notably, without any finetuning, DiffPortrait3D
is universally effective across a diverse range of facial portraits,
encompassing, but not limited to, faces with exaggerated expres-
sions, wide camera views, and artistic depictions.
in both individual appearance, expression and background
content. Notably to the best of our knowledge, we are the
first zero-shot novel portrait synthesis work that supports
versatile facial appearances and backgrounds, exaggerated
expressions, wide views, and a plethora of artist styles.
Long-range portrait view synthesis from sparse inputs
requires a generative prior to hallucinating plausible scene
features that are unobserved in the inputs. Recently, 3D
aware generative adversarial network (GAN) [2, 5, 6, 11,
16, 35, 42, 53, 54] demonstrated striking quality and multi-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10456
view-consistent image synthesis, by integrating 3D neu-
ral representations [34, 52] with style-based image genera-
tion [15, 24, 25]. Thereafter a line of work [3, 29, 39, 45, 55]
has explored either optimization-based or encoder-based
approaches to carefully invert the image into the latent or
feature embedding of 3D GANs, and then synthesize novel
views with 3D-aware generative priors. Nevertheless, al-
most all existing 3D-aware GANs are trained on limited im-
age datasets. Hence when it comes to much more wild and
nuanced portraits with large domain gap with the training
distributions, GANs tend to struggle in faithfully depicting
the 3D faces, resulting in loss of resemblance, corrupted ge-
ometry, or blurry extrapolation (see Figure 3, 4).
With the recent advent of text-to-image diffusion mod-
els [20, 40, 43, 44], we have witnessed unprecedented di-
versity and stability in image synthesis exhibited by large
diffusion models pre-trained on billions of images, such as
Imagen [41] and Stable Diffusion (SD) [1]. We therefore
aim to capitalize on the generative power of production-
ready diffusion models (SD in our work), for the task of
portrait view synthesis. However, unlike previous 3D GAN-
inversion works, simply inverting the reference image into
a generative noise or a textual description does not natu-
rally lift the image into a 3D scene, and it struggles to re-
tain consistent appearances when deviating from the refer-
ence view. The introduction of ControlNet [57] enhances
the controllability of Stable Diffusion by injecting local-
ized spatial conditions. However, it remains unclear how
to achieve appearance-disentangled view control such as in
the paradigm of ControlNet. Moreover, without inherent
3D representation, the direct application of existing 2D im-
age diffusion models to long-range animated view synthesis
results in severe flickering artifacts.
In this work, we propose DiffPortrait3D , a novel zero-
shot approach that lifts 2D diffusion model for synthesizing
3D consistent novel views from as few as a single portrait.
Our key insight is to decompose the task into explicitly dis-
entangled control of appearance and camera view. Specifi-
cally, we first utilize a trainable copy of the SD UNets to de-
rive semantic appearance context from the reference image
and then provide layer-by-layer contextual guidance to the
self-attention modules of a locked SD network. This allows
us to preserve the capability of the large diffusion models
while generating images with retained reference character-
istics regardless of the rendering views. On top of that, we
further achieve view control by adding camera pose atten-
tion to the locked UNet decoder as done in ControlNet [57].
By design, the camera pose attention is intelligently ex-
tracted from an RGB portrait image of a proxy subject cap-
tured at the same view, to minimize appearance leakage
from the condition image (e.g., shape and expression from
landmarks). Additionally, to alleviate flickering artifacts
when animating the views, we adopt a cross-view attentionmodule as used in many video diffusion models [17, 21].
This ensures the unobservable region is completed in a con-
sistent fashion. View consistency is further enhanced during
inference with a novel 3D-aware noise generation process.
With the locked parameters of Stable Diffusion, we fine-
tuned our control modules in stages with multi-view syn-
thetic dataset by PanoHead [2] and real-image Nersem-
ble dataset [27]. Our method demonstrates native general-
ization capability to in-the-wild portraits without run-time
fine-tuning. We extensively evaluate our framework on a
few challenging benchmarks. DiffPortrait3D outperforms
prior methods both quantitatively and qualitatively in terms
of visual quality, resemblance, and view consistency. The
contributions of our work can be summarized as:
‚Ä¢ A novel zero-shot view synthesis method that extends 2D
Stable Diffusion for generating 3d consistent novel views
given as little as a single portrait.
‚Ä¢ We demonstrate compelling fine-tuning-free novel view
synthesis results given a single unconstrained portrait, re-
gardless of its appearance, expression, pose, and style.
‚Ä¢ Explicitly disentangled control of appearance and camera
view, enabling effective camera control with preserved
identity and expression.
‚Ä¢ Long-range 3D view consistency with a cross-view atten-
tion module and 3D-aware noise generation.
Our code and model will be available for research purposes.
2. Related Works
Our study focuses on the application of 2D diffusion models
for zero-shot portrait novel view synthesis (NVS). Within
this context, we undertake an extensive survey of progress
in techniques related to novel view synthesis, categorized
into regression-based and generative approaches.
Regression based NVS. Facial NVS is attainable through
the use of explicit parametric geometry priors, as demon-
strated by 3D Morphable Models (3DMM) [13, 36, 47, 50,
59]. However, the limited parametric space of 3DMM poses
challenges in faithfully depicting diverse facial expressions.
Recent strides in Neural Radiance Fields (NeRF) [16, 22,
34, 56] have yielded high-fidelity results in novel view syn-
thesis. Notably in the realm of portrait NVS, FDNeRF [56]
constructed a NeRF model that integrates aligned features
from inputs to generate novel view portraits. Nevertheless,
achieving photo-realistic 3D-aware novel views with such
models typically necessitates the availability of dense cali-
brated images.
Generative NVS with GAN GANs [15] employ adver-
sarial learning to synthesize images that faithfully capture
the distribution of the training dataset. Previous studies
have demonstrated the effectiveness of 2D GANs in portrait
manipulation, employing techniques such as latent space
exploration [8] and exemplar image utilization [23, 51].
10457
ùíü++‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ñù"√ó$,&!,'!,(!‚Ñù"√ó'!√ó(!,$,&!Project In Multi-view AttentionProject Out‚Ñù"√ó$,&!,'!,(!View-ConsistencyModule+
(c) View-Consistency Module: SD U-Net module: zero-conv layer: additive control: attention concatenation(a) Pipeline Overview
ùêº)*+ùìè,
View ControlNetAppearance Ref: view-consistency module‚Ä¶‚Ñ∞
+(b) Cross-Referenced Self-AttentionùëÑ,ùêæ‚®Å,ùëâ‚®Å‚®Åùê¥ùë°ùë°ùëõ(()
‚®Å: concatenationTransBlockResBlock
ùëÖùëß-
ùìïùíìùíÜùíá
ùêº./0
ùêº1ùìïùíÑùíÇùíéùìïùëÑ,ùêæ,ùëâùê¥ùë°ùë°ùëõ(()ùúë()*(()Figure 2. (a) Overview of our DiffPortrait3D framework. Given a single reference image Iref,we aim to synthesize its novel views as IT
at camera perspectives aligned with condition images Icam.We leverage a pre-trained LDM Fas our image synthesis backbone (middle),
where its self-attention layers cross query the appearance context from Irefvia our appearance reference module Fref(right). Our view
control module (left) Fcamderives additive view condition from Icamand exerts on F. Additionally, we plug in view consistency modules
(dotted rectangles, middle) to Fto enhance multi-view coherence. During training, the images Icam are rendered using an off-the-shelf
3D GAN renderer R, where its camera perspectives are aligned with IT. (b) The intermediate spatial features œÜ(¬∑)sourced from Irefare
concatenated into the corresponding self-attention blocks in F. (c) An attention mechanism is employed across the multi-view dimensions
by our view-consistency module.
Nevertheless, the absence of inherent 3D representations in
these 2D GANs presents a challenge in maintaining 3D con-
sistency for the task of NVS.
Recent advancements on 3D aware GANs [2, 5, 6,
11, 16, 35, 42, 53, 54], built upon foundations of 2D
GANs, have demonstrated striking quality and multi-view-
consistent image synthesis. These methodologies typi-
cally leverage StyleGAN2 [26] as a fundamental compo-
nent, incorporating it with differential rendering and di-
verse 3D representations, such as signed distance functions
as in StyleSDF [35] and tri-plane representations used by
EG3D [6]. Thereafter a line of work [3, 29, 39, 45, 55]
has explored either optimization-based or encoder-based
approaches to carefully invert the image into the latent or
feature embedding of 3D GANs, and then synthesize novel
views with 3D-aware generative priors. It is noteworthy,
however, that these methods heavily depend on a pre-trained
3D GAN generator and exhibit limitations in their capacity
to generate unposed portraits with in-the-wild expressions,
styles, and camera views.
Diffusion Model based NVS In lieu of directly con-
fronting the intricacies of learning a 3D diffusion model, re-
cent research endeavors have embraced an alternative strat-egy, harnessing powerful 2D diffusion models to improve
the processes of 3D modeling and novel view synthesis.
DreamFusion [38] pioneered this strategy by distilling a
2D text-to-image generation model for fine-tuning a NeRF
model. GENVS [7] introduced a diffusion-based model ex-
plicitly tailored for 3D-aware generative novel view syn-
thesis from a single input image. Their methodology in-
volves modeling samples from the potential rendering dis-
tribution, effectively mitigating ambiguity and generating
plausible novel views through the utilization of diffusion
processes. Recent noteworthy study, Zero-1-to-3 [31, 32]
utilizes a stable diffusion model to capture geometric priors
derived from an extensive synthetic dataset, yielding high-
quality predictions. Moreover, Consistent123 [30], a case-
aware approach, utilizes Zero-1-to-3 as 3D prior for the ini-
tial structural representation before generating high texture
fidelity. However, it is crucial to note that these approaches
primarily concentrate on general objects, resulting in a di-
minished quality when applied to portrait synthesis.
3. Methods
Given as few as a single RGB portrait image, denoted as
Iref, captured from any camera perspective, we aim to syn-
10458
reference ours GOAE Triplanenet EG3D-PTI Zero-1-to-3
Figure 3. Qualitative comparison of novel view synthesis on in-the-wild images. Compared to the baselines, our method shows superior
generalization capability to novel view synthesis of wild portraits with unseen appearances, expressions and styles, even without any
reliance on fine-tuning.
thesize a new image ITat a novel query view as indicated by
a condition image Icam. The synthesized image ITshould
retain the expression and appearance of the foreground in-
dividual as well as the background context as in Iref,while
follows the rendering view of Icam.Note that IcamandIref
could be of a completely different identity.
Our proposed approach, DiffPortrait3D, leverages a la-
tent diffusion model (LDM) as the backbone of our render-
ing framework, as depicted in Figure 2 (a) (Section 3.1). We
then introduce an auxiliary appearance control branch (Sec-
tion 3.2) to exert layer-by-layer guidance with local struc-
tures and textures from reference images Iref.To enable ef-
fective camera control with Icam,our view control module,
designed in a fashion of ControlNet [57], implicitly derives
camera pose from Icamand inject to the diffusion process as
an additive condition (Section 3.3). Lastly we discuss about
enhancing view consistency with our integrated multi-view
attentions, and noise generation with 3D awareness at infer-
ence (Section 3.4).
3.1. Preliminaries
Latent Diffusion Models. Diffusion models [20, 43, 44]
are generative models designed to synthesize desired data
samples from Gaussian noise via removing noises itera-
tively. Latent diffusion models [40] are a class of diffu-
sion models that operates in the encoded latent space of an
autoencoder D(E(¬∑)),where EandDdenotes the encoder
and decoder respectively. Specifically, given an image I
and the text condition ctext, the encoded image latent z0
=E(I)is diffused Ttime steps into a Gaussian-distributed
zT‚àº N (0,1). The model is then trained to learn the re-
verse denoising process with the objective,
Lldm=Ez0,ctext,t,œµ‚àºN (0,1)œµ‚àíœµŒ∏ 
zt, ctext, t2
2
,(1)TheœµŒ∏is formulated as a trainable U-Net architecture
with layers of intervened convolutions (ResBlock) and self-
/cross-attentions (TransBlock). In this paper, we build our
network as a plug-and-play module to the recent state-of-
the-art text-to-image latent diffusion model, Stable Diffu-
sion [1].
ControlNet. As introduced by [57], ControlNet effec-
tively enhances latent diffusion models with spatially local-
ized, task-specific image conditions. As its core, it repli-
cates the original Stable Diffusion as a trainable side path,
and adds additional ‚Äúzero convolution‚Äù layers. The extra
conditions outputted from the ‚Äúzero convolution‚Äù layers are
then added to the skipped connections of the SD-UNets. Let
cpbe the extra condition, the noise prediction of U-Net with
ControlNet then becomes œµŒ∏ 
zt, ctext, cp, t
.
3.2. Appearance Reference Module
In order to synthesize a novel view of Irefwith LDM, one
could try to condition the denoising with an ‚Äúinverted‚Äù text
condition ctext[28]. However, providing a precise textual
description of Ireffor LDM to comprehensively recover all
its components is often a challenging undertaking. Alter-
natively, one could also condition œµŒ∏onIrefdirectly as a
ControlNet. Such a design, however, tend to generate im-
ages predominantly influenced by the camera pose in Iref.
Inspired by [4, 49], we opt for integrating appearance at-
tributes of the reference image Irefinto the UNet backbone
as cross-referenced self-attentions. Note that to eliminate
the harmful influence of inaccurate text description, we set
ctextempty and use the reference image Irefas the only
source of appearance.
To illustrate our appearance reference module, let us de-
note the pretrained LDM as F, where its self-attention is
10459
reference ground truth ours GOAE Triplanenet EG3D-PTI
 Zero-1-to-3Figure 4. Qualitative comparison of novel view synthesis on NeRSemble [27]. Our method achieves effective view control for novel
synthesis with the best perceptual quality and retained identity and expression, even for portraits with exaggerated expressions and under
substantial change of camera view for synthesis.
calculated as
Attn (¬∑) =softmax (QKT
‚àö
d)¬∑V (2)
Q=WQ¬∑œÜ(zt), K=WK¬∑œÜ(zt), V=WV¬∑œÜ(zt),
(3)
where Q, K, V are the query, key, and value features pro-
jected from the spatial features œÜ(zt)with corresponding
projection matrices respectively.
To guide the denoising process with Iref,we adapt the
self-attention mechanism within Fsuch that it is able to
cross query the correlated local contents and textures from
E(Iref), in addition to its own spatial features. Specifi-
cally we replicate Finto a trainable counterpart Frefwith
œÜref(¬∑)serving as intermediate representations within the
UNet architecture. As depicted in Figure 2 (b), we then
modify the vanilla self-attention in Fin a way that the spa-
tial context œÜref(E(Iref))in the appearance branch Frefis
cross-queried layer by layer as,
K‚äï=WK¬∑(œÜ(zt)‚äïœÜref(E(Iref))),
V‚äï=WV¬∑(œÜ(zt)‚äïœÜref(E(Iref))),(4)
where ‚äïdenotes concatenation. Note that we do not apply
noise to Iref,ensuring meticulous transfer of referenced
structure and appearance attributes into the novel portrait
synthesis. We lock the parameters of SD-UNet F, and train
our appearance reference module Frefwith paired multi-
view images.
Notable, when more reference images are available, e.g.,
in some multi-view capture settings, our appearance refer-ence module can be easily extended by concatenating mul-
tiple appearance contexts as
œÜ(zt)‚äïœÜref(E(I1
ref))‚äï...‚äïœÜref(E(In
ref)).(5)
Our trained module is capable of seamlessly integrating the
multi-view appearance clues into 3D-consistent appearance
context (Figure 8).
3.3. View Control Module
In this stage, we aim to attain control over the synthesis
viewpoint without influencing either the derived appear-
ance attributes by Frefor the synthesis capability of a pre-
trained LDM F. This naturally leads to the paradigm of
ControlNet [57] where the additional view control is con-
nected via ‚Äúzero convolution‚Äù layers of a trainable LDM
copy, with both FrefandFlocked. Here we denote our
view control module as Fcam,to be trained with multi-view
images. One straightforward design of Fcam would be to
employ the spatial feature maps extracted from the ground-
truth target images as image conditions, such as landmarks,
segmentation, or edges. We note that such ‚Äúground-truth‚Äù
condition images are not available during inference and
therefore the view is typically manipulated with images of
a different identity. However, we argue that such condition
images contain entangled semantic appearance information,
such as shape and expression, which is likely to be passed
along with the camera pose to F. Herein, appearance leak-
age from the view condition image will be reflected on the
novel view synthesis during inference. This artifact is more
pronounced when IrefandIcamexhibit distinct appearance
features.
10460
w/o View-Consistency oursreference
novel view synthesis
w/o 3D aware noiseFigure 5. Ablation on view consistency. Excessive background
variation and slight shading change across multiple novel views
are observable without our view-consistency module. Our 3D-
aware noise, compared to random Gaussian noise, helps maintain
structural coherence during view animation.
Instead, we utilize a portrait image from a distinct ran-
dom identity as the view condition, and generate novel-view
images that mirror the head pose as in the condition portrait
Icam. Our design unifies the view manipulation setting in
training and inference, and facilitates the natural disentan-
glement of view and appearance control. However, training
ControlNet for cross-identity view control requires paired
images at a identical view, and obtaining such data pairs
is typically unfeasible in real-world capture settings. To ad-
dress this hurdle, we leverage off-the-shelf 3D GAN renders
R(v, zv), as exemplified in prior works [2, 6], to generate
synthetic pose images Icam. Here the vdenotes the cam-
era parameters calibrated from the target image and zvis a
random Gaussian noise input to the 3D GAN. Since Icam
andIrefpossess substantial difference in expression and
appearance, our view control module is therefore instructed
to derive camera pose from Icam only. Moreover, by de-
sign, the camera pose is directly interpreted by our view
control module, allowing us to mimic the rendering view
simply with an RGB image. This largely eases the cumber-
someness in feature processing of Icam,e.g., landmarks de-
tection or semantic parsing, which could be unreliable with
heavy occlusion or under wide views.
3.4. View Consistency Module
To this end, we have facilitated the generation of a novel-
view portrait via the seamless combination of an appear-
ance reference module, a view ControlNet and a pre-
trained LDM. Nevertheless, achieving consistency in fea-
tures across various views poses a significant challenge as
many explanations exist for the unobservable region. In-
spired by AnimateDiff [17], we introduce a view consis-
reference ours GOAE Triplanenet EG3D-PTI
 Zero-1-to-3Figure 6. Reconstruction . DiffPortrait3D demonstrates meticu-
lous reconstruction of referenced appearance, even with side views
and 3D cartoon styles, substantially outperforming the baseline
methods.
tency module that incorporates cross-view attention within
a batch of views. Such a module employ an attention mech-
anism along the dimension of views to establish feature cor-
relation among the multiple novel view synthesis. Similar
to AnimateDiff, we integrate these view consistency mod-
ules into the up- and down-sampling blocks of the LDM
F, as depicted in Figure 2 (c). However, we note that such
frame-wise modules were originally proposed for temporal
coherence and as motion prior, trained with sequential video
frames. In contrast, the animated view motion is purely de-
fined by the sequence of Icam.Therefore, we trained our
view-consistency modules with batches of randomly shuf-
fled views, permitting the modules to focus on cross-view
attentions in lieu of motion distribution.
As illustrated in Figure 2 (c), we train our view-
consistency modules in groups of multi-view condition im-
ages{Icam}with a shape of (B, V, C, H, W ), where B
andVare the batch size and the number of views, while
C, H, W denote the number of image channels, image
height and width respectively. We note that the appearance
within each batch of generation is referenced from the same
image Iref. Inside F, we reshape the input to ResBlocks
and TransBlocks as (B√óV, C‚Ä≤, H‚Ä≤, W‚Ä≤), where C‚Ä≤, H‚Ä≤, W‚Ä≤
represent the latent feature channel, height and width re-
spectively. Following the operations of self- and cross-
attention, we then transform the layer input into a shape
of(B√óH‚Ä≤√óW‚Ä≤, V, C‚Ä≤), performing view-wise attention
within the view consistency modules.
3D-aware inference. It has been empirically observed
that the image layout is formed in the early denoising steps.
Therefore instead of denoising from multiple random Gaus-
sian noises, structural and textural consistency is likely to
be enhanced when synthesizing multiple novel views by
initiating the denoising process from ‚Äú3D-consistent‚Äù noise
samples. We propose an efficient two-stage process to gen-
erate noise samples with 3D awareness. On our multi-view
image dataset, we first trained a 3D-convolution based NVS
model with inclusion of 3D feature field and neural feature
rendering (please refer to the supplementary paper for de-
tails). We employ this NVS model to provide a proxy syn-
10461
Ours Eg3D-PTI GOAE Triplanenet
POSE‚Üì -/0.0068/- -/ 0.0021 /- -/0.0022/- -/0.0134/-
LPIPS ‚Üì0.02/0.23/0.02 0.21/0.26/0.36 0.12/0.28/0.21 0.10/0.39/0.17
SSIM‚Üë0.92/0.62/0.93 0.66/0.59/0.46 0.72/0.57/0.54 0.76/0.50/0.63
DIST‚Üì0.06/0.21/0.04 0.21/0.24/0.27 0.15/0.25/0.20 0.15/0.29/0.19
ID‚Üë0.95/0.70/0.92 0.15/0.28/0.12 0.55/0.39/0.54 0.70/0.45/0.70
FID‚Üì7.65/27.4/11.7 33.13/56.2/95.0 54.10/84.8/92.0 63.54/112.1/88.0
(a)w/oFref Iref Iref
finetuning unaligned aligned
LPIPS ‚Üì 0.55 0.28 0.27
SSIM‚Üë 0.47 0.68 0.68
DIST‚Üì 0.43 0.19 0.18
ID‚Üë 0.21 0.70 0.70
FID‚Üì 99.09 25.77 25.37
(b)
Table 1. (a) Quantitative comparison of our method and GAN-based baselines, showing numerical results of reconstruction/novel view
synthesis of NeRSemble [27], and reconstruction of in-the-wild test images( from left to right). For a fair comparison to our baselines, the
evaluation is performed at the resolution of 256√ó256. (b) Ablation study of our method without finetuning appearance reference module,
with unaligned reference images, and with aligned reference images, evaluated on NeRSemble at the resolution of 512√ó512.
thesis ÀúIat the target novel view, which is typically blurry
but 3D consistent. We then diffuse the latent feature E(ÀúI)
with 1000 time steps into a Gaussian noise as the input to
the LDM. In essence, the two-step generated noise still con-
tains some image layout semantics in a very coarse grain
and in practice, enhanced view consistency is observed in
our task as demonstrated in Figure 5.
4. Experiments
Dataset and Training. Our model was trained in three
stages on our multi-view image dataset as an image recon-
struction task. That being said, both the appearance refer-
ence image Irefand the target image ITare sourced from
the same identity but with different views, whereas Icam
is synthesized with EG3D [6] using a random latent Gaus-
sian noise and the calibrated camera parameters of IT. We
lock the parameters of the SD-UNet Fduring the whole
training stage. In the first stage, we train all the parame-
ters of our appearance reference module Frefwithout any
camera guidance. Next we freeze the weights of Fref,
and train our view control module Fcamwith paired Icam.
Lastly the view consistency module, performing cross-view
attentions among 8 views at once, is trained with the rest
modules frozen. All training was conducted on 6 Nvidia
A100 GPUs at a learning rate of 10‚àí5, with 16 images pro-
cessed in each step. During inference, we empirically set
100 steps for DDIM denoising [43] and unconditional guid-
ance scale [19] as 3 for a good balance of quality and speed.
We trained our modules on a hybrid dataset comprised
of photo-realistic multi-view images NeRSemble [27] and
synthetic ones by PanoHead [2]. NeRSemble dataset con-
sists of high-resolution videos of 220 subjects performing a
wide range of dynamic expressions, captured from 16 cal-
ibrated synchronized cameras. We sampled 2000 pairs of
multi-view frames from NeRSemble for training, where 1
randomly-selected view is used for appearance reference
and 8 other views as targets. Given the scarcity of availablecamera views and the background variation, we augmented
our training dataset with another 2000 pairs of multi-view
images synthesized via PanoHead [2]. For evaluation, we
used another unseen 500 multi-view pairs from NeRSem-
ble, and 360 single-view internet-collected in-the-wild por-
traits [12, 33, 37], containing a wide variation in appear-
ance, expression, camera perspective, and style. We note
that for training, all the images are cropped and aligned
as in EG3D [6] whereas we do not perform image align-
ment during inference (unless explicitly stated for compar-
ison to GAN-based methods). For testing on both datasets,
the novel camera views are all manipulated with EG3D ren-
derings.
4.1. Qualitative Evaluations
Given a single reference portrait, our method demonstrates
high-fidelity and 3D-consistent novel view synthesis at a
resolution of 512√ó512,as illustrated in Figure 1. While
only being trained on aligned real portrait images, our
method shows superior generalization capability to novel
identities, styles, expressions and views. This is largely
credited to the preservation of the generative prior of pre-
trained LDM by our design. As evidenced in Figure 4, our
view control module is also able to effectively control the
synthesis view. Compared to the ground truth (second col-
umn, Figure 4), our novel portraits are highly plausible but
with some noticeable identity differences. This is due to the
limited visual appearance clue in the single reference image,
and the problem can be largely alleviated with additional
references (please refer to Figure 8 and the supplementary
paper for visual results).
We extensively compare to a few state-of-the-art novel
portrait synthesis works on both image reconstruction
(Figure 6) and novel view synthesis (Figure 3, 4):
GOAE [55],TriPlaneNet [3], Pivot Tuning (EG3D-PTI) [6]
and Zero-1-to-3 [32]. GOAE [55] and TriPlaneNet [3] de-
signed an effective image encoder for EG3D [6], whereas
10462
EG3D-PTI runs latent code optimization and finetunes the
weights of EG3D per image. We did not compare to Live3D
Portrait [46] given unavailable implementation and model.
Zero-1-to-3 [32] leverages Stable Diffusion but was trained
on 3D object dataset Objaverse [9]. While not required
by our method, we cropped and aligned the test images as
in EG3D. Nevertheless, our method outperforms substan-
tially over the prior work in terms of both perceptual qual-
ity, and preservation of identity and expression. Notably
all 3D GAN-based baselines fail to reconstruct side views
(Figure 6), exaggerated expressions (Figure 4), or out-of-
domain styles (Figure 3), whereas Zero-1-to-3 synthesizes
novel portraits with very limited perceptual quality.
4.2. Quantitative Evaluations
We evaluate methods for single-view novel portrait synthe-
sis on 4 main aspects. We use LPIPS ‚Üì[58], DISTS ‚Üì[14],
SSIM‚Üë[48] for evaluation of 2D image reconstruction,
ID‚Üë[10] for identity consistency, FID ‚Üì[18] for perceptual
quality, and POSE ‚Üìfor camera view control accuracy. No-
tably, to evaluate reconstruction fairly, we estimate camera
parameters from the ground-truth target image and uses the
EG3D renderings as condition Icam.The error in camera
estimation could result in some image misalignment and
therefore we mainly rely on perceptual metrics LPIPS and
DISTS for reconstruction evaluation. The identity similar-
ity is calculated between the synthesized and reference im-
age by calculating the cosine similarity of the face embed-
dings with a pretrained face recognition module [10].
Table 1a shows the numerical comparison on reconstruc-
tion of NeRSemble and in-the-wild test images, and novel
view synthesis of NeRSemble respectively. On all image
metrics, our method shows our method is superior than all
prior work by a large margin, demonstrating the most com-
pelling image quality. Our pose reconstruction is slightly
worse than the baseline. However, we argue that this is
largely due to the camera misalignment between the ground
truth and the condition EG3D rendering.
4.3. Ablations
We ablate the efficacy of the individual component with ex-
tensive ablation experiments for noval view synthesis on
NeRSemble test set. As illustrated in Figure 5, we demon-
strate the necessity of our view consistency module and
3D-aware noise in maintaining appearance coherence cross
multiple views. Without them, substantial variations are ob-
served, especially on the unobserved region of the reference
image, when altering the camera views. The weights of our
appearance reference module is initiated from a copy of SD-
UNet which should be already able to derive local appear-
ance context from the reference image. However, as evi-
denced by Table 1b and Figure 7, significant improvements
are achieved by our finetuning on multi-view images. We
reference ground truth w/o /f_ine-tuning
appearance referenceoursFigure 7. Fine-tuning appearance reference module helps better
retain the spatial features from the reference image.
ground truthreference novel viewview 1 view 2 view 3
use view 1 use view 1,2 use view 1,2,3
Figure 8. Our method seamlessly supports multiple reference im-
ages as input, and the novel view synthesis quality is progressively
enhanced with more references.
reason that the necessity of finetuning is due to the removal
of cross attention from text. Lastly unlike many GAN-based
methods that requires the reference image to be aligned, our
model supports free-form portraits as inputs without quality
degeneration even though the model was trained on camera-
aligned multi-view images. This is numerically shown in
Table 1b where aligning the reference image (as in EG3D)
only leads to neglectable differences.
5. Discussion
Conclusion. We presented DiffPortrait3D , a novel con-
ditional diffusion model that is capable of generating con-
sistent novel portraits from sparse input views. By design,
our framework seamlessly cross-references the key charac-
teristics from the input images and effectively adds cam-
era pose control into the latent diffusion process, modu-
lated with enhanced consistency across views. Trained only
with a few thousand of synthetic and real multi-view im-
ages, our model successfully showcases compelling novel
portrait synthesis results, regardless of appearances, expres-
sions, camera perspectives, and styles. This is largely cred-
ited to our explicitly disentangled control of appearance and
view within both model design and training, without harm-
ing the generalization capability of large pretrained diffu-
sion models. We believe that our framework opens up pos-
sibilities for accessible 3D reconstruction and visualization
from a single picture.
10463
References
[1] Stability AI. Stable diffusion v1.5 model card.
https://huggingface.co/runwayml/stable-diffusion-v1-5 ,
2022. 2, 4
[2] Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Y .
Ogras, and Linjie Luo. Panohead: Geometry-aware 3d full-
head synthesis in 360deg. In CVPR , pages 20950‚Äì20959,
2023. 1, 2, 3, 6, 7
[3] Ananta R Bhattarai, Matthias Nie√üner, and Artem Sev-
astopolsky. Triplanenet: An encoder for eg3d inversion.
arXiv preprint arXiv:2303.13497 , 2023. 2, 3, 7
[4] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xi-
aohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mu-
tual self-attention control for consistent image synthesis and
editing. arXiv preprint arXiv:2304.08465 , 2023. 4
[5] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,
and Gordon Wetzstein. pi-gan: Periodic implicit generative
adversarial networks for 3d-aware image synthesis. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 5799‚Äì5809, 2021. 1, 3
[6] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki
Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,
Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero
Karras, and Gordon Wetzstein. Efficient geometry-aware 3D
generative adversarial networks. In CVPR , 2022. 1, 3, 6, 7
[7] Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexan-
der W. Bergman, Jeong Joon Park, Axel Levy, Miika Ait-
tala, Shalini De Mello, Tero Karras, and Gordon Wetzstein.
GeNVS: Generative novel view synthesis with 3D-aware dif-
fusion models. In arXiv , 2023. 3
[8] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Infogan: Interpretable rep-
resentation learning by information maximizing generative
adversarial nets, 2016. 2
[9] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,
Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana
Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:
A universe of annotated 3d objects. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 13142‚Äì13153, 2023. 8
[10] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos
Zafeiriou. Arcface: Additive angular margin loss for deep
face recognition. In CVPR , pages 4690‚Äì4699, 2019. 8
[11] Yu Deng, Jiaolong Yang, Jianfeng Xiang, and Xin Tong.
Gram: Generative radiance manifolds for 3d-aware image
generation. CVPR , 2022. 1, 3
[12] DeviantArt. deviantart. https://www.deviantart.com , 2024. 7
[13] Abdallah Dib, Cedric Thebault, Junghyun Ahn, Philippe-
Henri Gosselin, Christian Theobalt, and Louis Chevallier.
Towards high fidelity monocular face reconstruction with
rich reflectance using self-supervised learning and ray trac-
ing, 2021. 2
[14] Keyan Ding, Kede Ma, Shiqi Wang, and Eero P. Simoncelli.
Image quality assessment: Unifying structure and texture
similarity. CoRR , abs/2004.07728, 2020. 8
[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, andYoshua Bengio. Generative adversarial nets. In NeurIPS ,
2014. 2
[16] Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt.
Stylenerf: A style-based 3d-aware generator for high-
resolution image synthesis. CVPR , 2022. 1, 2, 3
[17] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu
Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your
personalized text-to-image diffusion models without specific
tuning. arXiv preprint arXiv:2307.04725 , 2023. 2, 6
[18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In Advances in Neural Information Processing Sys-
tems. Curran Associates, Inc., 2017. 8
[19] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. arXiv preprint arXiv:2207.12598 , 2022. 7
[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. NeurIPS , 33:6840‚Äì6851, 2020. 2,
4
[21] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William
Chan, Mohammad Norouzi, and David J Fleet. Video dif-
fusion models. arXiv:2204.03458 , 2022. 2
[22] Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, and Juyong
Zhang. Headnerf: A real-time nerf-based parametric head
model. In CVPR , pages 20374‚Äì20384, 2022. 2
[23] Omer Kafri, Or Patashnik, Yuval Alaluf, and Daniel Cohen-
Or. Stylefusion: A generative model for disentangling spatial
segments, 2021. 2
[24] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks. In
CVPR , 2019. 2
[25] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improving
the image quality of StyleGAN. In CVPR , 2020. 2
[26] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improving
the image quality of stylegan. In CVPR , 2020. 3
[27] Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim
Walter, and Matthias Nie√üner. Nersemble: Multi-view radi-
ance field reconstruction of human heads. ACM Transactions
on Graphics , 2023. 2, 5, 7
[28] Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-
diffusion: Pre-trained subject representation for control-
lable text-to-image generation and editing. arXiv preprint
arXiv:2305.14720 , 2023. 4
[29] Connor Z Lin, David B Lindell, Eric R Chan, and Gordon
Wetzstein. 3d gan inversion for controllable portrait image
animation. arXiv preprint arXiv:2203.13441 , 2022. 2, 3
[30] Yukang Lin, Haonan Han, Chaoqun Gong, Zunnan Xu,
Yachao Zhang, and Xiu Li. Consistent123: One image to
highly consistent 3d asset using case-aware diffusion priors,
2023. 3
[31] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen,
Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45:
Any single image to 3d mesh in 45 seconds without per-
shape optimization, 2023. 3
10464
[32] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-
makov, Sergey Zakharov, and Carl V ondrick. Zero-1-to-3:
Zero-shot one image to 3d object, 2023. 3, 7, 8
[33] Midjourney. midjourney. https://www.midjourney.com ,
2024. 7
[34] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. In ECCV , 2020. 2
[35] Roy Or-El, Xuan Luo, Mengyi Shan, Eli Shecht-
man, Jeong Joon Park, and Ira Kemelmacher-Shlizerman.
Stylesdf: High-resolution 3d-consistent image and geometry
generation. CVPR , 2022. 1, 3
[36] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami
Romdhani, and Thomas Vetter. A 3d face model for pose
and illumination invariant face recognition. In 2009 sixth
IEEE international conference on advanced video and sig-
nal based surveillance , pages 296‚Äì301. Ieee, 2009. 2
[37] Pexels. pexels. https://www.pexels.com/ , 2024. 7
[38] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv ,
2022. 3
[39] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel
Cohen-Or. Pivotal tuning for latent-based editing of real im-
ages. ACM Transactions on Graphics , 42(1):1‚Äì13, 2022. 2,
3
[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¬®orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , pages 10684‚Äì
10695, 2022. 2, 4
[41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. NeurIPS , 35:36479‚Äì36494, 2022. 2
[42] Ivan Skorokhodov, Sergey Tulyakov, Yiqun Wang, and Pe-
ter Wonka. Epigraf: Rethinking training of 3d gans. arXiv
preprint arXiv:2206.10535 , 2022. 1, 3
[43] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 2, 4, 7
[44] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. arXiv preprint arXiv:2011.13456 , 2020. 2, 4
[45] Alex Trevithick, Matthew Chan, Michael Stengel, Eric R.
Chan, Chao Liu, Zhiding Yu, Sameh Khamis, Manmohan
Chandraker, Ravi Ramamoorthi, and Koki Nagano. Real-
time radiance fields for single-image portrait view synthesis.
InSIGGRAPH , 2023. 2, 3
[46] Alex Trevithick, Matthew Chan, Michael Stengel, Eric R.
Chan, Chao Liu, Zhiding Yu, Sameh Khamis, Manmohan
Chandraker, Ravi Ramamoorthi, and Koki Nagano. Real-
time radiance fields for single-image portrait view synthesis,
2023. 8
[47] Satoshi Tsutsui, Weijia Mao, Sijing Lin, Yunyi Zhu, Murong
Ma, and Mike Zheng Shou. Novel view synthesis for high-fidelity headshot scenes. arXiv preprint arXiv:2205.15595 ,
2022. 2
[48] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: From error visibility to
structural similarity. IEEE Transactions on Image Process-
ing, 13(4):600‚Äì612, 2004. 8
[49] Haohan Weng, Tianyu Yang, Jianan Wang, Yu Li, Tong
Zhang, CL Chen, and Lei Zhang. Consistent123: Improve
consistency for one image to 3d object synthesis. arXiv
preprint arXiv:2310.08092 , 2023. 4
[50] Fanzi Wu, Linchao Bao, Yajing Chen, Yonggen Ling, Yibing
Song, Songnan Li, King Ngi Ngan, and Wei Liu. Mvf-net:
Multi-view 3d face morphable model regression. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 959‚Äì968, 2019. 2
[51] Sitao Xiang, Yuming Gu, Pengda Xiang, Mingming He,
Koki Nagano, Haiwei Chen, and Hao Li. One-shot
identity-preserving portrait reenactment. arXiv preprint
arXiv:2004.12452 , 2020. 2
[52] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany,
Shiqin Yan, Numair Khan, Federico Tombari, James Tomp-
kin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in
visual computing and beyond. In Computer Graphics Forum ,
pages 641‚Äì676. Wiley Online Library, 2022. 2
[53] Xudong Xu, Xingang Pan, Dahua Lin, and Bo Dai. Gener-
ative occupancy fields for 3d surface-aware image synthesis.
NeurIPS , 34:20683‚Äì20695, 2021. 1, 3
[54] Yang Xue, Yuheng Li, Krishna Kumar Singh, and Yong Jae
Lee. Giraffe hd: A high-resolution 3d-aware generative
model. In CVPR , 2022. 1, 3
[55] Ziyang Yuan, Yiming Zhu, Yu Li, Hongyu Liu, and Chun
Yuan. Make encoder great again in 3d gan inversion through
geometry and occlusion-aware encoding. arXiv preprint
arXiv:2303.12326 , 2023. 2, 3, 7
[56] Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing
Liao. Fdnerf: Few-shot dynamic neural radiance fields for
face reconstruction and expression editing, 2022. 2
[57] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
ICCV , pages 3836‚Äì3847, 2023. 2, 4, 5
[58] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 586‚Äì595, 2018. 8
[59] Yiyu Zhuang, Hao Zhu, Xusen Sun, and Xun Cao. Mofanerf:
Morphable facial neural radiance field. In European Confer-
ence on Computer Vision , pages 268‚Äì285. Springer, 2022.
2
10465
