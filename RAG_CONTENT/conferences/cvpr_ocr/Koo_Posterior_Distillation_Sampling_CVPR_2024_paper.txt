Posterior Distillation Sampling
Juil Koo Chanho Park Minhyuk Sung
KAIST
{63days,charlieppark,mhsung }@kaist.ac.kr
Input Scene
“a photo of a man”“...Captain America” “...Spider Man” “...Emma Stone”
“...Deadpool” “...Batman” “...Leonardo DiCaprio”Edited Results
“...roses” “...purple tulips” “...sunflowers” “a photo of a plant”
NeRF Editing
SVG Editing
“Eiffel Tower” → “Space Needle”
Figure 1. Parametric image editing results obtained by Posterior Distillation Sampling (PDS). PDS is an optimization tailored for
editing across diverse parameter spaces. It preserves the original details of the source content while aligning them with the input texts.
Abstract
We introduce Posterior Distillation Sampling (PDS), a
novel optimization method for parametric image editing
based on diffusion models. Existing optimization-based
methods, which leverage the powerful 2D prior of diffusion
models to handle various parametric images, have mainly
focused on generation. Unlike generation, editing requires
a balance between conforming to the target attribute and
preserving the identity of the source content. Recent 2D im-
age editing methods have achieved this balance by leverag-
ing the stochastic latent encoded in the generative process
of diffusion models. To extend the editing capabilities of dif-
fusion models shown in pixel space to parameter space, wereformulate the 2D image editing method into an optimiza-
tion form named PDS. PDS matches the stochastic latents of
the source and the target, enabling the sampling of targets
in diverse parameter spaces that align with a desired at-
tribute while maintaining the source’s identity. We demon-
strate that this optimization resembles running a generative
process with the target attribute, but aligning this process
with the trajectory of the source’s generative process. Ex-
tensive editing results in Neural Radiance Fields and Scal-
able Vector Graphics representations demonstrate that PDS
is capable of sampling targets to fulfill the aforementioned
balance across various parameter spaces. Our project page
is at https://posterior-distillation-sampling.github.io.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
13352
Input Scene Instruct-NeRF2NeRF Inversion2NeRF DDS PDS (Ours)
“...reading a book”
“...drinking a cup of coffee”
“...with hot air balloons”“...with windmills”
“deer unicorn doll”Figure 2. A comparison of 3D scene editing between PDS and other baselines. Given input 3D scenes on the left, PDS, marked by
green boxes on the rightmost side, successfully performs complex editing, such as geometric changes and adding objects, according to the
input texts. On the other hand, the baselines either fail to change the input 3D scenes or produce results that greatly deviate from the input
scenes, losing their identity.
13353
1. Introduction
Diffusion models [13, 45–48] have recently led to rapid de-
velopment in text-conditioned generation and editing across
diverse domains, including 2D images [11, 15, 20, 49, 52],
3D objects [18, 19, 21, 32], and audio [7, 14, 55]. Among
them, in particular, 2D image diffusion models [5, 26, 37,
39, 41] have demonstrated their powerful generative prior
aided by Internet-scale image and text datasets [3, 42, 43].
Nonetheless, this rich 2D generative prior has been confined
to pixel space, limiting their broader applicability. A pio-
neer work overcoming this limitation, DreamFusion [34],
has introduced Score Distillation Sampling (SDS). It lever-
ages the generative prior of text-to-image diffusion mod-
els to synthesize 3D scenes represented by Neural Radiance
Fields (NeRFs) [28] from texts. Beyond NeRF represen-
tations [4, 23, 36, 44, 50, 51, 57], SDS has been widely
applied to various parameter spaces, where images are not
represented by pixels but specific parameterizations, such as
texture [1, 25], material [54] and Scalable Vector Graphics
(SVGs) [16, 17, 53].
While SDS [34] has achieved great advances in generat-
ing parametric images, editing is also an essential element
for full freedom in handling visual content. Editing differs
from generation in that it requires considerations of both the
target text and the original source content, thereby empha-
sizing two key aspects: (1) alignment with the target text
prompt and (2) preservation of the source content’s iden-
tity. To extend SDS, which lacks the latter aspect, Hertz et
al.[10] propose Delta Denoising Score (DDS). DDS re-
duces the noisy gradients inherent in SDS, leading to better-
maintaining background details and sharper editing outputs.
However, the optimization function of DDS still lacks an
explicit term for identity preservation.
To address the absence of preserving the source’s iden-
tity in SDS [34] and DDS [10], we turn our attention to a
recent 2D image editing method [15, 52] based on diffusion
models, known as stochastic diffusion inversion. Their pri-
mary objective is to compute the stochastic latent of an input
image within the generative process of diffusion models.
Once the stochastic latent of a source image is computed,
the source image can be edited by running a generative pro-
cess with new conditions, such as new target text prompts,
while feeding the source’s stochastic latent into the process.
Feeding the source’s stochastic latent into the target image’s
generative process ensures that the target image maintains
the structural details of the source while moving towards
the direction of the target text. Thus, this editing process
reflects the aforementioned two key aspects of editing.
To extend the editing capabilities of the stochastic dif-
fusion inversion method from pixel space to parameter
space, we reformulate this method into an optimization
form named Posterior Distillation Sampling (PDS). Unlike
SDS [34] and DDS [10], which match two noise variables,PDS aims to match the stochastic latents of the source and
the optimized target. We demonstrate that our optimiza-
tion process resembles aligning forward process posteriors
of the source and the target, ensuring that the target’s gener-
ative process trajectory does not significantly deviate from
that of the source.
When parametric images come from NeRF [28],
Haque et al. [9] have recently introduced a promising text-
driven NeRF editing method called Iterative Dataset Up-
date (Iterative DU). To edit 3D scenes, it performs an edit-
ing process in 2D space bypassing direct edit in 3D space.
Thus, when a text prompt induces large variations in 2D
space across different views, it has difficulty producing the
right edit in 3D space. On the other hand, our method di-
rectly updates NeRF in 3D space, thus gradually transform-
ing a 3D scene into its edited version in a view-consistent
manner even in the case where text prompts induce large
variations, such as large geometric changes or the addition
of objects to unspecified regions.
Our extensive editing experiment results, including
NeRF editing (Section 6.1) and SVG editing (Section 6.2),
demonstrate the versatility of our method for parametric im-
age editing. In NeRF editing, we are the first to produce
large geometric changes or to add objects to arbitrary re-
gions without specifying local regions to be edited. Figure 2
shows these examples. Qualitative and quantitative com-
parisons of SVG editing with other optimization methods,
namely SDS [34] and DDS [10], have demonstrated that
PDS produces only the necessary changes to source SVGs,
effectively aligning them with the target prompts.
2. Related Work
2.1. Score Distillation Sampling
Following the remarkable success of diffusion models in
text-to-image generation, there have been attempts to lever-
age the 2D prior of diffusion models for various other
types of generative tasks. In these tasks, images are rep-
resented through rendering processes with specific param-
eters, including Neural Radiance Fields [17, 34, 50], tex-
ture [1, 25], material [54] and Scalable Vector Graphics
(SVGs) [16, 17, 53]. The primary method employed in
these tasks is Score Distillation Sampling (SDS). SDS is an
optimization approach that updates the rendering parame-
ter towards the image distribution of diffusion models by
enforcing the noise prediction on noisy rendered images
to match sampled noise. Concurrently, Wang et al. [50]
also have introduced Score Jacobian Chaining which con-
verges toward a similar algorithm as SDS but from a dif-
ferent mathematical derivation. Wang et al. [51] have pro-
posed Variational Score Distillation (VSD) to address over-
saturation, over-smoothing, and low-diversity problems in
SDS [34]. Instead of updating a single data point, VSD
13354
updates multiple data points to align an optimized distri-
bution with the diffusion model’s image distribution. Zhu
and Zhuang [57] use more accurate predictions of diffusion
models via iterative denoising at every SDS update step.
When it comes to editing, Hertz et al. [10] propose Delta
Denoising Score (DDS), an adaptation of SDS for editing
tasks. It reduces the noisy gradient directions in SDS to
better maintain the input image details. Nonetheless, its op-
timization function lacks an explicit term to preserve the
identity of the input image, thus often producing outputs
that significantly deviate from the input images. To allevi-
ate this issue, we propose Posterior Distillation Sampling, a
novel optimization approach that incorporates a term dedi-
cated to preserving the identity of the source in its optimiza-
tion function.
2.2. Text-Driven NeRF Editing
Haque et al. [9] have proposed a text-driven NeRF edit-
ing method, known as Iterative Dataset Update (Itera-
tive DU). It iteratively replaces reference images, initially
used for NeRF [28] reconstruction, with edited images
using Instruct-Pix2Pix [2]. By applying a reconstruc-
tion loss with these iteratively updated images to an in-
put NeRF [28] scene, the scene is gradually transformed
to its edited counterpart. Mirzae et al. [29] improve
Instruct-NeRF2NeRF [9] by computing local regions to be
edited. However, this iterative image replacement method
suffers from edits that involve large variations across dif-
ferent views, such as complex geometric changes or adding
objects to unspecified regions. Thus, they have mainly fo-
cused on appearance changes.
Instead of the Iterative DU method, several recent
works [22, 33, 58] directly apply SDS [34] or DDS [10]
to NeRF editing. However, these optimizations do not fully
consider the preservation of the source’s identity and are
thus prone to producing outputs that substantially diverge
from the input scenes. In contrast, our novel optimization
inherently guarantees the preservation of the source’s iden-
tity, facilitating involved NeRF editing while maintaining
the identity of the original scene.
2.3. Diffusion Inversion
Diffusion inversion computes the latent representation of an
input image encoded in diffusion models. This allows for
real image editing by finding the corresponding latent that
can fairly reconstruct the given image. The computed la-
tent is then decoded into a new image through a generative
process. Using the deterministic generative process of De-
noising Diffusion Implicit Models (DDIM) [46], one can
approximately run the ODE of the generative process in re-
verse [6, 46], referred to as DDIM inversion. Several recent
works have improved DDIM inversion by adjusting text
features [8, 30, 31], introducing new cross-attention mapsduring a generative process [11] or alternatively coupling
intermediate latents from two inversion trajectories [49].
Meanwhile, an alternative approach, known as DDPM in-
version [15, 52], employs the stochastic generative process
of Denoising Diffusion Probabilistic Models (DDPM) [13].
They focus on capturing the structural details of an input
image encoded in its stochastic latent. We extend the edit-
ing capabilities of this DDPM inversion method to parame-
ter space by reformulating the method into an optimization
form.
3. Preliminaries
We first discuss existing optimization-based approaches to
handle parametric images, then introduce our novel para-
metric image editing method in Section 4.
3.1. Score Distillation Sampling (SDS) [34]
Score Distillation Sampling (SDS) [34] is proposed to gen-
erate parametric images by leveraging the 2D prior of pre-
trained text-to-image diffusion models. Given an input data
x0and a text prompt y, the training objective function of
diffusion models is to predict injected noise ϵusing a noise
predictor ϵϕ:
L(x0) =Et∼U(0,1),ϵt
w(t)∥ϵϕ(xt, y, t)−ϵt∥2
2
,(1)
where w(t)is a weighting function and xtresults from the
forward process of diffusion models:
xt:=√¯αtx0+√
1−¯αtϵt,ϵt∼ N(0,I) (2)
with variance schedule variables ¯αt:=Qt
s=1αs. When the
input data x0is generated by a differentiable image genera-
torx0=g(θ), parameterized by θ, SDS updates θby back-
propagating the gradient of Equation 1 while omitting the
U-Net jacobian term∂ϵϕ
∂xtfor computation efficiency:
∇θLSDS(x0=g(θ)) =Et,ϵt
w(t)(ϵϕ(xt, y, t)−ϵt)∂x0
∂θ
,
(3)
where we denote a noise prediction of diffusion models with
classifier-free guidance [12] by ϵϕfor simplicity. Through
this optimization process, SDS is capable of generating a
parametric image which conforms to the input text prompt
y.
3.2. Delta Denoising Score (DDS) [10]
Even though SDS has been widely used for various para-
metric images, its optimization is designed for generation,
thus it does not reflect one of the key aspects of editing:
preserving the source identity.
To extend SDS to editing, Hertz et al. [10] have pro-
posed Delta Denoising Score (DDS). Given source data xsrc
13355
and its corresponding text prompt ysrc, the goal of DDS is
to synthesize new target data xtgtthat is aligned with a tar-
get text prompt ytgt. In the SDS formula 3, DDS replaces
randomly sampled noise ϵwith a noise prediction given a
source data-text pair ϵϕ(xsrc
t, ysrc, t):
∇θLDDS=
Et,ϵt
w(t) 
ϵϕ(xtgt
t, ytgt, t)−ϵϕ(xsrc
t, ysrc, t)∂xtgt
0
∂θ
,
(4)
where the same noise ϵtis shared for xsrc
tandxtgt
t:
ϵt∼ N(0,I),
xsrc
t=√¯αtxsrc
0+√
1−¯αtϵt,
xtgt
t=√¯αtxtgt
0+√
1−¯αtϵt. (5)
While DDS extends SDS for editing tasks, it lacks an
explicit term in its optimization to preserve the identity of
the source. As a result, DDS is still prone to produce editing
results that significantly deviate from the source.
3.3. Stochastic Latent in Generative Process
To achieve both conformity to the text and preservation of
the source’s identity, we turn our attention to the rich in-
formation encoded in the stochastic generative process of
DDPM [13]. When βt:= 1−αtare small, it is well-
known that the posterior of the forward process also follows
a Gaussian distribution according to a property of Gaus-
sians. The forward process posteriors are represented as:
q(xt−1|xt,x0) =N(µ(xt,x0), σtI), (6)
where σt:=1−¯αt−1
1−¯αtβtand the posterior mean µis a linear
combination of x0andxt:µ(xt,x0) :=γtx0+δtxtwith
γt:=√¯αt−1(1−αt)
1−¯αtandδt:=√αt(1−¯αt−1)
1−¯αt.
Since x0is unknown during a generative process, we ap-
proximate x0with a one-step denoised estimate as follows:
˜x0(xt, y;ϵϕ) :=1√¯αt(xt−√
1−¯αtϵϕ(xt, y, t)).(7)
Consequently, one step of the generative process is repre-
sented as follows:
xt−1=µϕ(xt, y;ϵϕ) +σtzt,zt∼ N(0,I),(8)
where µϕ(xt, y;ϵϕ) =γt˜x0(xt, y;ϵϕ) +δtxt.
Using Equation 8, one can compute stochastic latent ˜zt
that captures the structural details of x0. This involves com-
puting xtandxt−1via the forward process and then rear-
ranging Equation 8 as follows:
˜zt(x0, y;ϵϕ) =xt−1−µϕ(xt, y;ϵϕ)
σt. (9)Several recent works [15, 52], known as DDPM inver-
sion, have utilized the stochastic latent for image editing
tasks. To edit an image using ˜zt, they first pre-compute ˜ztof
the source image across all tin the generative process. They
then run a new generative process with a new target prompt
while incorporating the pre-computed ˜ztof the source into
the process instead of randomly sampled noise zt.
Although these works [15, 52] have utilized the rich in-
formation encoded in ˜ztfor an editing purpose, their ap-
plications have been limited within 2D-pixel space due to
reliance on the generative process. In our work, we broaden
the application of the stochastic latent to parameter space by
reformulating the method as an optimization form, enabling
parametric image editing .
4. Posterior Distillation Sampling
Here, we introduce Posterior Distillation Sampling (PDS), a
novel optimization function designed for parametric image
editing.
Our objective is to synthesize xtgt
0that is aligned with
ytgtwhile it retains the identity of xsrc
0. To achieve this, we
employ the stochastic latent ˜ztin our optimization. For sim-
plicity, we denote the stochastic latents of the source and the
target as follows:
˜zsrc
t:=˜zt(xsrc
0, ysrc;ϵϕ) (10)
˜ztgt
t:=˜zt(xtgt
0, ytgt;ϵϕ). (11)
Using the stochastic latents, we define a novel objective
function as follows:
L˜zt(xtgt
0=g(θ)) :=Et,ϵt−1,ϵt
∥˜ztgt
t−˜zsrc
t∥2
2
,(12)
where, similar to Equation 5, ˜zsrc
tand˜ztgt
tshare the same
noises, denoted by ϵt−1andϵt, when computing their re-
spective xt−1andxt.
Rather than matching noise variables as in SDS [34] and
DDS [10], we match the stochastic latents of the source and
the target via the optimization. By taking the gradient of L˜zt
with respect to θand ignoring the U-Net jacobian term as
previous works [10, 34, 50], one can obtain PDS as follows:
∇θLPDS:=Et,ϵt,ϵt−1
w(t)(˜ztgt
t−˜zsrc
t)∂xtgt
0
∂θ
.(13)
Expanding Equation 13, the following detailed formulation
is derived:
∇θLPDS:=
Et,ϵt,ϵt−1
(ψ(t)(xtgt
0−xsrc
0) +χ(t)(ˆϵtgt
t−ˆϵsrc
t))∂xtgt
0
∂θ
,
(14)
13356
PDS (Ours)DDSSDS
Figure 3. A visual comparison of the editing process through
SDS [34], DDS [10] and PDS. The figure illustrates the trajec-
tories of samples drawn from p(x0|y= 1) as they are shifted
towards p(x0|y= 2) . PDS notably moves the samples near the
boundary of the two marginals—the optimal endpoint in that it
balances the necessary change with the original identity.
where ˆϵsrc
t:=ϵϕ(xsrc
t, ysrc, t)andˆϵtgt
t:=ϵϕ(xtgt
t, ytgt, t).
We leave a more detailed derivation to the supplementary
material .
Matching ztgt
twithzsrc
tensures that the posteriors of xtgt
0
andxsrc
0do not significantly diverge, despite being steered
by different prompts, ytgtandysrc. This approach is akin to
running a generative process with ytgtwhile remaining near
the trajectory made by the posteriors of xsrc
0. Consequently,
PDS enables the sampling of xtgt
0that aligns with ytgt, while
also retaining the identity of xsrc
0. This is achieved through
the distillation of the posteriors of xsrc
0into the target sam-
pling process.
4.1. Comparison with SDS [34] and DDS [10]
In Figure 3, we visually illustrate the difference among the
three optimization methods: SDS [34], DDS [10] and PDS.
Here, we model a 2D distribution x0∼p(x0)∈R2that is
separated by two marginals, p(x0|y= 1) andp(x0|y= 2)
which are colored by red and blue, respectively. Then, we
train a diffusion model conditioned on the class labels y.
Using the pre-trained conditional diffusion model, we aim
to transition xtgt
0starting from xsrc
0∼p(x0|y= 1) to-
wards the other marginal p(x0|y= 2) . The trajectories
of three optimization methods are plotted in Figure 3 with
their endpoints denoted by stars. As illustrated, SDS and
DDS significantly displace the data from the initial position,
whereas our method is terminated near the boundary of the
two marginals. This is the optimal endpoint for an editing
purpose as it indicates proximity to both the starting points
andp(x0|y= 2) , thereby achieving a balance between the
necessary change and the original identity.
4.2. Comparison with Iterative DU
When a parameterization of images is given as NeRF [28],
recent works [9, 29] have shown promising NeRF editing
results based on a method known as Iterative Dataset Up-
date (Iterative DU). This method bypasses 3D editing by
performing the editing process within 2D space. Given an
Input 2D Editing Results NeRF Outputs
IP2PDDPM
InversionIN2N Inv2N Ours
“...raising his arms”Figure 4. An example of editing inducing large variations
across different views. The figure shows NeRF editing results
of ours and Iterative DU methods, IN2N [9] and Inv2N, with their
corresponding 2D editing results obtained by IP2P [2] and DDPM
Inversion [15], respectively. When 2D editing leads to large vari-
ations, the Iterative DU methods fail to produce accurate edits in
3D space.
image dataset {Isrc
v}N
v=1used for NeRF [28] reconstruction
with viewpoints v, they randomly replace Isrc
vwith its 2D
edited version using Instruct-Pix2Pix (IP2P) [2]. By itera-
tively updating the input images, they progressively trans-
form the input NeRF scene into an edited version of it.
In contrast to Iterative DU which performs editing in
2D space, our approach directly edits NeRFs [28] in 3D
space. To visually demonstrate this difference, Figure 4
presents a qualitative comparison of ours and various meth-
ods based on Iterative DU. Specifically, we compare ours
with Instruct-NeRF2NeRF (IN2N) [9] which uses IP2P [2]
for 2D editing. Additionally, we include another Iterative-
DU-based method, Inversion2NeRF (Inv2N), which em-
ploys DDPM inversion [15] for its 2D editing process.
Given the prompt “raising his arms” , the figure illustrates
significant variations in 2D edited images across different
views: the man raises either only one arm or both arms, as
marked by the red circle. Furthermore, the red arrow high-
lights the inconsistency in the poses of raising arms across
different views. Such notable discrepancies in 2D editing
hinder the Iterative DU methods from transferring these ed-
its into 3D space. Particularly noteworthy is the compari-
son of our method with Inv2N, both of which leverage the
stochastic latent for editing. However, while Inv2N confines
its editing within 2D space, ours directly updates NeRF pa-
rameters in 3D space by reformulating the 2D image edit-
ing method [15] into an optimization form. Consequently,
as shown in Figure 4 and Figure 2, ours is the only one
to facilitate complex geometric changes and the addition of
objects in 3D scenes. It demonstrates the strength of our
method lies in the novel optimization design, which allows
13357
for direct 3D editing, not just relying on the editing capabil-
ities of DDPM inversion [15].
5. NeRF Editing with PDS
As one of the applications of PDS, we present a detailed
pipeline for NeRF [28] editing. NeRF can be seen as a pa-
rameterized rendering function. The rendering process is
expressed as Iv=g(v;θ), where the function takes a spe-
cific viewpoint vto render the image Ivat that viewpoint
with the rendering parameter θ. Using the publicly avail-
able Stable Diffusion [39] as our diffusion prior model, we
encode the current rendering at viewpoint vto obtain the tar-
get latent xtgt
0,v:xtgt
0,v:=E(g(v;θ)), where Eis a pre-trained
encoder. Similarly, given the original source images {Isrc
v}
used for NeRF [28] reconstruction, the source latent xsrc
0,vis
also computed by encoding the source image at viewpoint
v:xsrc
0,v:=E(Isrc
v).
For real scenes, there are no given source prompts. Thus,
we manually create descriptions for the real scenes, such
as“a photo of a man” in Figure 1. For target prompts
ytgt, we adjust ysrcby appending a description of a desired
attribute—e.g., “...raising his arms” in Figure 4—or by sub-
stituting an existing word in ysrcwith a new one, such as
changing “deer doll” to“unicorn doll” in the last row of
Figure 2. Given a pre-fixed set of viewpoints {v}, we ran-
domly select a viewpoint vto compute xsrc
0,vandxtgt
0,v. The
pairs of (xsrc
0,v, ysrc)and(xtgt
0,v, ytgt)are fed into the PDS op-
timization to update θin a direction dictated by the target
prompt. After the optimization, the updated NeRF param-
eter˜θrenders an edited 3D scene that is aligned with the
target prompt: ˜Iv:=g(v;˜θ).
To further improve the final output, we take a refinement
stage inspired by DreamBooth3D [36]. During iterations of
the refinement stage, we randomly select an edited render-
ing˜Ivand refine it into a more realistic-looking image using
SDEdit [24]. The edited NeRF scenes through PDS opti-
mization are then further refined by a reconstruction loss
with these repeatedly updated images.
In some cases of source prompts we create, we observe
some gap between the ideal text prompt, which would ide-
ally reconstruct the input image through the generative pro-
cess, and the actual prompt we provide. To alleviate this
discrepancy issue, we have found it effective to finetune
the Stable Diffusion [39] with {Isrc
v}andysrcfollowing the
DreamBooth [40] setup.
6. Experiment Results
In this section, we conduct editing experiments across two
types of parameterized images. Section 6.1 presents NeRF
editing results, comparing our NeRF editing capabilities to
the state-of-the-art NeRF editing methods. Furthermore,
Section 6.2 shows SVG editing results to compare PDSagainst other optimization methods, namely SDS [34] and
DDS [10].
6.1. NeRF Editing
Datasets. We use real scenes we capture as well as the
scenes from IN2N [9] and LLFF [27]. The total number of
scenes is 13, and the final number of pairs of source scenes
and target text prompts is 37with multiple target prompts
for each scene.
Baselines. For extensive comparisons, we evaluate our
method against three baselines: Instruct-NeRF2NeRF
(IN2N) [9], DDS [10] and Inversion2NeRF (Inv2N). First,
we compare ours with IN2N [9], which is a state-of-the-
art NeRF editing method with its code publicly available.
Additionally, as introduced in Section 4.2, we conduct a
comparison with Inv2N, another method based on Iterative
DU, which performs editing within 2D space rather directly
in 3D space, but employs DDPM inversion [15] instead of
IP2P [2] for 2D editing.
Results. Figure 2 presents the qualitative comparisons of
NeRF editing. Notably, as depicted in rows 1 and 2, our
method is the only one that makes large geometric changes
in 3D scenes from the input text, folding the man’s arms
to create natural poses of him reading a book or drink-
ing coffee. In contrast, Iterative-DU-based methods like
IN2N [9] and Inv2N fail to produce the right edits in 3D
space. DDS [10] produces the outputs that completely lose
the identity of the input scenes, focusing solely on conform-
ing to the input texts. Rows 3 and 4 of Figure 2 show
the editing scenarios of adding objects in outdoor scenes
without specifying local regions, which also leads to large
variations. Here, our method successfully adds objects like
windmills and hot air balloons in the input scenes, main-
taining their background details. On the other hand, the
baselines either fail to add the objects in 3D space or pro-
duce outputs that significantly deviate from the original
scenes. When it comes to appearance change, which in-
duces relatively little variations across different views, both
our method and IN2N [9] effectively produce the desired
appearance change in 3D scenes, as shown in the last row of
Figure 2. However, ours most preserves the original identity
of the input scene, such as the object’s color, while making
appropriate changes. Additional qualitative results are pre-
sented through videos on our project page1.
To further assess the perceptual quality of the editing
results, we conduct a user study compared to the base-
lines. Following Ritchie [38], participants were shown in-
put NeRF scene videos, editing prompts, and edited NeRF
scene videos produced by ours and the baselines. They were
1https://posterior-distillation-sampling.github.io
13358
Table 1. A quantitative comparison of NeRF editing between
ours and other baselines. Ours outperforms the baselines quanti-
tatively. Bold indicates the best result for each column.
Methods CLIP [35] Score ↑User Preference
Rate (%) ↑
IN2N [9] 0.2280 27.71
DDS [10] 0.2210 13.71
Inv2N 0.2232 9.24
PDS (Ours) 0.2477 49.33
Input SDS DDS PDS (Ours)
“A pumpkin” → “A banana”
“A cat as 3D rendered”→ “A dog as 3D rendered”
“A drawing of a cat”→ “A drawing of a dog”
Figure 5. A qualitative comparison of SVG editing using three
different optimization methods: SDS [34], DDS [10] and PDS.
PDS makes changes according to input text while most preserving
the structural semantics of the input SVGs.
then asked to choose the most appropriate edited NeRF
scene video. As illustrated in Table 1, our editing results
are most preferred over the baselines in human evaluation
by a large margin: 49.33% (Ours) vs. 27.71% (IN2N [9],
the second best). See the supplementary material for a
more detailed user study setup.
For a quantitative evaluation, we measure CLIP [35]
Score that measures the similarity between edited 2D ren-
derings and target text prompts in CLIP [35] space. As
shown in Table 1, ours outperforms the baselines quanti-
tatively. This is corroborated by the qualitative results il-
lustrated in Figure 2, especially in scenarios of geometric
changes or object addition, where the other baselines have
difficulty in making the right edits.
6.2. SVG Editing
Experimental Setup. We use pairs of SVGs and their
corresponding text prompts used in VectorFusion [17] as
input. By manually creating target text prompts, we con-
duct experiments with a total of 48pairs of input SVGsTable 2. A quantitative comparison of SVG editing between
SDS [34], DDS [10] and PDS. Ours outperforms the others in
LPIPS [56] while achieving a CLIP [35] score that is on par with
the others. Bold indicates the best result for each column.
Methods CLIP [35] Score ↑ LPIPS [56] ↓User Preference
Rate (%) ↑
SDS [34] 0.2606 0.4855 30.83
DDS [10] 0.2460 0.5982 20.24
PDS (Ours) 0.2504 0.3121 48.94
and target text prompts. For comparison, we evaluate our
method against other optimization methods, SDS [34] and
DDS [10]. To perform editing with SDS, we start with a
source SVG as an initial updated SVG and then update it us-
ing a target prompt according to the SDS [34] optimization.
Following DDS, we use CLIP [35] score and LPIPS [56] as
quantitative metrics.
Results. Qualitative results of SVG editing are shown in
Figure 5. It demonstrates that while all the methods ef-
fectively change input SVGs according to the target text
prompts, ours best preserves the structural semantics of the
input SVGs. This is particularly evident in row 2 of Fig-
ure 5, where ours maintains the overall color pattern of the
input SVG.
The trends from the qualitative results are mirrored in
our quantitative results. As seen in Table 2, ours signif-
icantly surpasses the others in LPIPS [56] by a large mar-
gin, which measures the fidelity to the input SVG, while our
CLIP score is on par with the others. This demonstrates that
our method introduces only minimal necessary changes to
meet the described attributes in the target text prompts.
We further provide a user study result of SVG editing in
Table 2. We use the same user study setup used in NeRF
editing (Section 6.1). Consistent with the qualitative and
quantitative results, ours are most preferred in human eval-
uation.
7. Conclusion
We propose Posterior Distillation Sampling (PDS), an op-
timization method for parametric image editing. PDS
matches the stochastic latents of the source and the target
to fulfill both conformity to the target text and preservation
of the source identity in parameter space. We demonstrate
the versatility of PDS in parametric image editing through
a comparative analysis between ours and other optimization
methods and extensive experiments across various parame-
ter spaces.
Acknowledgements This work was supported by NRF
grant (RS-2023-00209723) and IITP grants (2022-0-
00594, RS-2023-00227592) funded by the Korean gov-
ernment (MSIT), Seoul R&BD Program (CY230112), and
grants from the DRB-KAIST SketchTheFuture Research
Center, Hyundai NGV , KT, NCSOFT, and Samsung Elec-
tronics.
13359
References
[1] Anonymous. Learning pseudo 3D guidance for view-
consistent 3D texturing with 2D diffusion. In Submitted to
The Twelfth International Conference on Learning Represen-
tations , 2023. under review. 3
[2] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-
structPix2Pix: Learning to follow image editing instructions.
InCVPR , 2023. 4, 6, 7
[3] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun
Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m:
Image-text pair dataset. https://github.com/
kakaobrain/coyo-dataset , 2022. 3
[4] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-
tasia3D: Disentangling geometry and appearance for high-
quality text-to-3D content creation. In ICCV , 2023. 3
[5] DeepFloyd. Deepfloyd if. https://www.deepfloyd.
ai/deepfloyd-if/ . 3
[6] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. NeurIPS , 2021. 4
[7] Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish,
and Soujanya Poria. Text-to-audio generation using instruc-
tion tuned llm and latent diffusion model. arXiv preprint
arXiv:2304.13731 , 2023. 3
[8] Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng
Song, Mengwei Ren, Ruijiang Gao, Yuxiao Chen, Di Liu,
Qilong Zhangli, et al. Improving negative-prompt inversion
via proximal guidance. arXiv preprint arXiv:2306.05414 ,
2023. 4
[9] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander
Holynski, and Angjoo Kanazawa. Instruct-NeRF2NeRF:
Editing 3D scenes with instructions. In ICCV , 2023. 3, 4,
6, 7, 8
[10] Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta de-
noising score. In ICCV , 2023. 3, 4, 5, 6, 7, 8
[11] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image
editing with cross-attention control. In ICLR , 2023. 3, 4
[12] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. In NeurIPS 2021 Workshop on Deep Generative
Models and Downstream Applications , 2021. 4
[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. NeurIPS , 2020. 3, 4, 5
[14] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren,
Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang
Yin, and Zhou Zhao. Make-an-audio: Text-to-audio genera-
tion with prompt-enhanced diffusion models. arXiv preprint
arXiv:2301.12661 , 2023. 3
[15] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer
Michaeli. An edit friendly DDPM noise space: Inversion and
manipulations. arXiv preprint arXiv:2304.06140 , 2023. 3,
4, 5, 6, 7
[16] Shir Iluz, Yael Vinker, Amir Hertz, Daniel Berio, Daniel
Cohen-Or, and Ariel Shamir. Word-as-image for semantic
typography. ACM TOG , 2023. 3
[17] Ajay Jain, Amber Xie, and Pieter Abbeel. Vectorfusion:
Text-to-svg by abstracting pixel-based diffusion models. In
CVPR , 2023. 3, 8[18] Heewoo Jun and Alex Nichol. Shap-e: Generat-
ing conditional 3d implicit functions. arXiv preprint
arXiv:2305.02463 , 2023. 3
[19] Juil Koo, Seungwoo Yoo, Minh Hieu Nguyen, and Minhyuk
Sung. SALAD: Part-level latent diffusion for 3d shape gen-
eration and manipulation. In ICCV , 2023. 3
[20] Yuseung Lee, Kunho Kim, Hyunjin Kim, and Minhyuk
Sung. Syncdiffusion: Coherent montage via synchronized
joint diffusions. In NeurIPS , 2023. 3
[21] Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. Diffusion-
sdf: Text-to-shape via voxelized diffusion. In CVPR , 2023.
3
[22] Yuhan Li, Yishun Dou, Yue Shi, Yu Lei, Xuanhong Chen, Yi
Zhang, Peng Zhou, and Bingbing Ni. Focaldreamer: Text-
driven 3D editing via focal-fusion assembly. arXiv preprint
arXiv:2308.10608 , 2023. 4
[23] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,
Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: High-resolution
text-to-3D content creation. In CVPR , 2023. 3
[24] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided
image synthesis and editing with stochastic differential equa-
tions. In ICLR , 2022. 7
[25] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and
Daniel Cohen-Or. Latent-nerf for shape-guided generation of
3D shapes and textures. In CVPR , 2023. 3
[26] Midjourney. Midjourney. https://www.midjourney.
com/ . 3
[27] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,
Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and
Abhishek Kar. Local light field fusion: Practical view syn-
thesis with prescriptive sampling guidelines. ACM TOG ,
2019. 7
[28] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:
Representing scenes as neural radiance fields for view syn-
thesis. In ECCV , 2020. 3, 4, 6, 7
[29] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus A
Brubaker, Jonathan Kelly, Alex Levinshtein, Konstantinos G
Derpanis, and Igor Gilitschenski. Watch your steps: Local
image and scene editing by text instructions. arXiv preprint
arXiv:2308.08947 , 2023. 4, 6
[30] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki
Tanaka. Negative-prompt inversion: Fast image inversion
for editing with text-guided diffusion models. arXiv preprint
arXiv:2305.16807 , 2023. 4
[31] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and
Daniel Cohen-Or. Null-text inversion for editing real images
using guided diffusion models. In CVPR , 2023. 4
[32] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela
Mishkin, and Mark Chen. Point-e: A system for generat-
ing 3d point clouds from complex prompts. arXiv preprint
arXiv:2212.08751 , 2022. 3
[33] Jangho Park, Gihyun Kwon, and Jong Chul Ye. ED-NeRF:
Efficient text-guided editing of 3D scene using latent space
NeRF. arXiv preprint arXiv:2310.02712 , 2023. 4
13360
[34] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3D using 2D diffusion. In ICLR ,
2023. 3, 4, 5, 6, 7, 8
[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML , 2021. 8
[36] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,
Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman,
Michael Rubinstein, Jonathan Barron, et al. Dreambooth3D:
Subject-driven text-to-3D generation. In ICCV , 2023. 3, 7
[37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022. 3
[38] Daniel Ritchie. Rudimentary framework for running two-
alternative forced choice (2afc) perceptual studies on me-
chanical turk. 7
[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , 2022. 3, 7
[40] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In CVPR , 2023. 7
[41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. NeurIPS , 2022. 3
[42] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:
Open dataset of clip-filtered 400 million image-text pairs.
arXiv preprint arXiv:2111.02114 , 2021. 3
[43] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for training
next generation image-text models. NeurIPS , 2022. 3
[44] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,
and Xiao Yang. MVDream: Multi-view diffusion for 3D
generation. arXiv preprint arXiv:2308.16512 , 2023. 3
[45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In ICML , 2015. 3
[46] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In ICLR , 2021. 4
[47] Yang Song and Stefano Ermon. Generative modeling by esti-
mating gradients of the data distribution. In NeurIPS , 2019.
[48] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. In ICLR , 2021. 3
[49] Bram Wallace, Akash Gokul, and Nikhil Naik. EDICT:
Exact diffusion inversion via coupled transformations. In
CVPR , 2023. 3, 4[50] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,
and Greg Shakhnarovich. Score jacobian chaining: Lifting
pretrained 2D diffusion models for 3D generation. In CVPR ,
2023. 3, 5
[51] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan
Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and
diverse text-to-3D generation with variational score distilla-
tion. In NeurIPS , 2023. 3
[52] Chen Henry Wu and Fernando De la Torre. A latent space of
stochastic diffusion models for zero-shot image editing and
guidance. In ICCV , 2023. 3, 4, 5
[53] Ximing Xing, Chuang Wang, Haitao Zhou, Jing Zhang, Qian
Yu, and Dong Xu. Diffsketcher: Text guided vector sketch
synthesis through latent diffusion models. In NeurIPS , 2023.
3
[54] Xudong Xu, Zhaoyang Lyu, Xingang Pan, and Bo Dai.
Matlaber: Material-aware text-to-3D via latent BRDF auto-
encoder. arXiv preprint arXiv:2308.09278 , 2023. 3
[55] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao
Weng, Yuexian Zou, and Dong Yu. Diffsound: Discrete
diffusion model for text-to-sound generation. IEEE/ACM
Transactions on Audio, Speech, and Language Processing ,
2023. 3
[56] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR , 2018. 8
[57] Joseph Zhu and Peiye Zhuang. HiFA: High-fidelity text-
to-3D with advanced diffusion guidance. arXiv preprint
arXiv:2305.18766 , 2023. 3, 4
[58] Jingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, and
Guanbin Li. Dreameditor: Text-driven 3D scene editing with
neural fields. arXiv preprint arXiv:2306.13455 , 2023. 4
13361
