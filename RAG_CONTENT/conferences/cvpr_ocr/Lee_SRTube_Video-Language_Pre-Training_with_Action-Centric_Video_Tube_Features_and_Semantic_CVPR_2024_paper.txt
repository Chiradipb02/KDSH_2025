SRTube: Video-Language Pre-Training with Action-Centric Video Tube
Features and Semantic Role Labeling
Ju-Hee Lee and Je-Won Kang*
Dept. of Electronic and Electrical Engineering and Graduate Program in Smart Factory,
Ewha W. University, Seoul, South Korea
juhee69@ewhain.net,jewonk@ewha.ac.kr
Abstract
In recent years, large-scale video-language pre-training
(VidLP) has received considerable attention for its effec-
tiveness in relevant tasks. In this paper, we propose a
novel action-centric VidLP framework that employs video
tube features for temporal modeling and language features
based on semantic role labeling (SRL). Our video encoder
generates multiple tube features along object trajectories,
identifying action-related regions within videos, to over-
come the limitations of existing temporal attention mech-
anisms. Additionally, our text encoder incorporates high-
level, action-related language knowledge, previously under-
utilized in current VidLP models. The SRL captures action-
verbs and related semantics among objects in sentences and
enhances the ability to perform instance-level text match-
ing, thus enriching the cross-modal (CM) alignment pro-
cess. We also introduce two novel pre-training objectives
and a self-supervision strategy to produce a more faithful
CM representation. Experimental results demonstrate that
our method outperforms existing VidLP frameworks in vari-
ous downstream tasks and datasets, establishing our model
a baseline in the modern VidLP framework.
1. Introduction
Video-language pre-training (VidLP) models [7, 9, 16, 35,
43, 45] have been actively used for multi-modal learning.
Their effectiveness is acknowledged across various down-
stream tasks, including video question answering [21, 25],
video retrieval [17, 38], and video captioning [32, 47].
High-performance VidLP frameworks include key modules
such as temporal modeling and cross-modal (CM) align-
ment [12, 15, 50, 54]. In recent studies, video transformers
[34, 53] are used to establish baselines due to their ability
to generate rich and joint representations of video and lan-
guage [2, 7, 9, 16, 43]. In [12], a baseline VidLP architec-
*Je-Won Kang is a corresponding author.
Figure 1. Our motivation is to bridge semantic gaps between video
and text using more organized action-related features and their se-
mantic alignments in CM.
ture was built through the dissection and reassembly of the
crucial components, resulting in superior performance. This
approach seems to effectively construct the VidLP frame-
work by empirically optimizing specific modules. How-
ever, when further expanding to a temporal direction, cur-
rent video transformers [7, 9, 34] would deteriorate com-
putational efficiency and overlook essential spatio-temporal
contexts within videos due to the conventional patch-wise
cross-attention. In [8, 27], visual features extracted from
single or just few frames often served as substitutes for tem-
poral features, since the straightforward cross-attention ex-
hibited limited abilities to capture temporal contexts.
To address this problem, recent VidLP studies have intro-
duced object-centric video transformers [3, 20, 28, 48, 57]
to manage more relevant objects in adjacent frames. Exist-
ing object-centric image-language pre-training approaches
have used off-the-shelf object detection models to align
objects in images with words [28, 31]. However, these
schemes are infeasible to be applied to video, because they
require pre-extracted features of all the objects. To mi-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
13689
grate this object-centric approach from image to video do-
main, in [44], an anchor frame was selected to extract vi-
sual features using an object-aware video transformer. De-
spite the reduced computational complexity, since an an-
chor frame is uniformly sampled, temporal contexts are not
sufficiently reflected for CM alignments. Because videos
exhibit dynamic changes of objects over time, accurate tem-
poral modeling has been emphasized in the majority of stud-
ies [2, 3, 7, 34]. HiTeA [50] proposed a method to cap-
ture temporal contexts, by extracting features from long and
short clips at various time steps. Although various features
obtained from diverse video frames are reflected to the CM
alignment, the plane features from the standard video trans-
former cannot encode the temporal semantics. The previous
studies using visual tokens [3, 54], linguistic tokens [45],
and region tokens [2] have relied on the query attention to
capture objects but ignored semantic relationships in tem-
poral, which remains further challenges in VidLP.
In this paper, we propose an action-centric VidLP frame-
work using structured and action-related video and lan-
guage features to enhance the CM alignment. Specifically,
as shown in Fig. 1, we enrich our framework with a video
tube feature for temporal modeling and a language feature
grounded in semantic role labeling (SRL) to identify seman-
tically relevant regions of videos, thereby facilitating a more
robust CM alignment. Temporal tube along object trajecto-
ries offers action information as an inherent advantage for
video representation. While the previous video transform-
ers incorporating temporal modeling [3] calculated the at-
tention only to correlated regions in video, our method tries
to build the semantic gaps, by generating multiple tube fea-
tures with precise coordinates of each object and consider-
ing their relations, which is a challenge with query-driven
attention. Moreover, The proposed method also exploits
high-level action-related knowledge from language, which
was underutilized in existing VidLP models. The SRL aids
in instance-level text matching, by exploring action-verbs
and the related semantics among objects within sentences
[42]. These enhanced features are combined with a CM fu-
sion. Our study is the pioneering effort that integrates struc-
tural insights from video and linguistic features to synergize
both, whereas few recent studies used temporal information
from either video or language individually [44, 50].
Further, a novel pre-training objective and a self-
supervision strategy are proposed to produce a faithful CM
representation through the proposed features. It has been
pointed out that the pre-training schemes combined with
temporal modeling and CM fusion were crucial to improve
the performance in recent VidLP studies [12, 17, 28, 44, 50].
However, the existing methods are hardly applied to the pro-
posed video and language features for rich semantic align-
ments. Therefore, we introduce new action-centric proxy
tasks, i.e., masked action modeling (MAM) and action num-bering modeling (ANM). Through a new proxy task, we en-
able the effective integration of features with distinct tem-
poral semantics.
Our work has several primary contributions as follows:
• We propose an action-centric VidLP framework (dubbed
SRTube), explicitly combining structured video and lin-
guistic features with CM semantic alignments. This pio-
neering approach utilizes video tube features and seman-
tic phrase features for improved video representation and
is the first to holistically exploit action-related knowledge
from both video and language.
• We introduce a novel pre-training objective and a self-
supervision strategy to craft a reliable CM representation
using our proposed features. Our new proxy task facili-
tates an effective integration of the multi-modal features.
• We present experimental results on various datasets and
downstream tasks to demonstrate the efficacy of our
method. The superior performance justifies the effective-
ness of our method, which can be considered as a new
baseline for modern VidLP frameworks.
2. Related work
2.1. Video-language Pre-training
Owing to large-scale video text benchmarks [5, 37], pre-
trained models have demonstrated impressive performance
on diverse multi-modal downstream tasks [5, 16, 45, 49].
Current VidLP methods commonly used multi-modal fu-
sion and temporal modeling to achieve highly improved per-
formance [12, 15, 50, 54]. First, they tried to produce an
effective CM representation with contextualized video and
language features [10, 12, 17, 28, 50]. Existing CM align-
ment and fusion methods in VidLP can be broadly catego-
rized into two groups. One strategy used a unified multi-
modal fusion module to combine visual and language fea-
tures, thus creating a CM representation [16, 43]. The other
approach used separate visual and language encoders and
leveraged a cross-attention mechanism of a transformer for
inter-modal interactions [12, 17, 28].
Temporal modeling has been emphasized due to its im-
pact [2, 4, 9, 10, 26, 50], as the current self-attention in
transformers tended to exacerbate computational complex-
ity and neglect essential temporal contexts within videos
[8]. Video transformer architectures have adopted tem-
poral attention mechanism [7, 9, 34]. TimeSformer [7]
decoupled space and time attention to alleviate computa-
tional complexity. Also, X-Vit[9] proposed to integrate
two lightweight global temporal attention mechanisms.
Mformer-L [3] proposed a trajectory attention that tracked
relevant objects between adjacent frames. [57] used pre-
extracted object trajectory features, and [48] proposed a
Patch-2-Word attention applied to related objects in video.
However, those studies struggle with the lack of inductive
13690
Figure 2. (a): An overall framework of SRTube is presented. The video feature extractor produces global visual features Vand tube
features U. Simultaneously, the text extractor generates global text features Tand semantic phrase features S.UandSare combined to
produce action-centric features via the cross fusion transformer (CFT) using two types of attention. (b): we show the detailed structure of
the CFT, where CM alignments are performed using the global features and action-centric features through the proposed proxy tasks.
biases for videos, which is a challenge with query-driven
attention. Our method tries to overcome the drawbacks, by
focusing on more relevant spatio-temporal regions identi-
fied with tube features. Our tube builder is pre-trained for
action recognition, thus providing well-characterized fea-
tures for this purpose.
2.2. Linguistic Features in Multi-modal Analysis
While language features were often used as supplemen-
tary, they were actively applied to recent VidLP tasks
[13, 31, 51, 54]. Certain words or phrases in a sentence
hold rich semantic information. OSCAR [31] extracted text
tags from images and used these tags as anchor points to
align with words. In [39], adverbs were employed to en-
hance video understanding by predicting changes in actions
through measuring textual relationships. [17] distinguished
verbs and nouns using a part-of-speech (PoS) tag and con-
structed related questions. This approach enabled the model
to synchronize local objects.
Scene graphs have been employed to extract structural
knowledge from sentences [13, 51]. ERNIE-ViL [51] pro-
posed a method to build semantic connections across vi-
sion and language using scene graphs. ROSITA [13] ac-
quired object region features from images in scene graphs
from text. However, while scene graphs concentrated on all
words, they were hardly applied to motion representation
and temporal modeling. Compared to scene graphs, SRL
[42] in NLP was used to discern relationships between verbsand adjacent words in sentences, clarifying the role of each
element. This enhanced sentence structure comprehension
[40, 42] by designating a semantic role to each word. In
our study, the texture features aligned with verbs are used
to perform CM alignments with tube features in videos to
create action-oriented features.
3. Proposed Method
3.1. Overall Architecture
Fig. 2 presents an overall network architecture of SRTube.
Our model integrates action-related video and language fea-
tures and their explicit semantic cues into the framework.
The video and text pairs are fed into a transformer-based
video feature extractor and a text feature extractor. Our ar-
chitecture employs the same video encoder and text encoder
as VindLU [12], which are respectively expressed with light
purple and green colors in Fig. 2, as a baseline. In the pro-
posed architecture, a tube builder and an SRL builder as
expressed as deep purple and green are incorporated into
the feature extractors. On top of the baseline cross-fusion
model [12], using visual to text attention (VTA), we further
develop a tube to SRL attention (TSA) for the fusion. Those
feature builders and TSA module are newly introduced in
our architecture.
13691
3.2. SRTube Feature Extraction
3.2.1 Video feature extractor
Visual features. Following VindLU [12], we employ
BEiT [6] vision transformer and adapt it by adding a
2-layer temporal attention layer [7] before self-attention
layer as our video encoder. Fig. 2 presents the generation
of a sequence of visual context embedding, denoted as
V={vi}L
i=0, vi∈RC.LandCare a sequence length and
an embedding dimension, which is set to 196and768, re-
spectively. v0is assigned to vclsas [CLS] token embedding.
Tube features. We use a tube feature, representing the tra-
jectory of individual objects over time, to effectively capture
the temporal dynamics of objects within the video. Fig. 2
(a) presents the proposed tube builder incorporated into the
video feature extractor. The tube builder produces a set of
tube features U∈RN×K×D={uk
n}, in which Nis the
number of tube features in each frame, which also repre-
sents the maximum number of objects to be captured in the
frame. Dis a feature dimension, which is set to 256, andK
is the number of frames. Uis obtained with Vand learned
queries, Q∈RN×K×C={qk
n}.qk
nis the tube query cor-
responding to uk
nin the k-th frame. qk
nis learnable for a
transformer to predict the class or location of objects by at-
tention operations as in [11]. When {uk
n}is obtained from
all the frames, the tube features go through a temporal pool-
ing to generate a set of features {un}N
n=1, which represent
Nobject trajectories in a video sequence.
We explain a detailed architecture as shown in Fig.
3. Motivated by [11, 56], the tube builder comprises
self-attention layers, cross-attention layers, and classifi-
cation heads with feed-forward neural network (FFN).
In our implementation, we develop a spatial-temporal
self-attention (STSA), using the query. Specifically, we
apply a spatial self-attention, in which qk
nwithin the same
frame goes through self-attention, and then calculate a
temporal self-attention over time. We produce an output
embedding zk
nusing cross-attention between Vand the
output of the STSA. zk
nis used to predict a coordinate of a
bounding box bk
n.zk
nandbk
nare concatenated to produce
uk
n. We use a multi-layer perceptron (MLP) head, when
projecting uk
ninto a cross-fusion transformer.
Tube builder training. We pretrain our tube builder us-
ing an action-recognition task that predicts both bounding
boxes and their corresponding action labels as in [56] with
A V A v2 datasets [18]. While the previous method [56] fo-
cused on only the recognition of foreground objects, our
method is designed to include background labels to reflect
overall temporal dynamics. These strategies lead to richer
semantic expressions of both the static and dynamic video
scenes. For this, we add several bounding boxes for back-
Figure 3. The architecture of the tube builder that produces a tube
feature representing the trajectories of several objects in video.
grounds that are not overlapped with action regions and
their binary results indicating if the regions are foreground
or background to the training dataset. The number of labels
is determined by the difference between Nand the number
of GT bounding boxes.
3.2.2 Text feature extractor
Text features. As in the previous studies [5, 12, 16], an in-
put sentence is tokenized and fed into a pre-trained encoder
to obtain a text embedding sequence T={ti}L
i=0, ti∈RC,
where t0is assigned to tclsas the [CLS] token embedding.
LandCare the same as in the visual encoder. We adopt
BERT [14] as the text encoder.
Semantic phrase features. S={si}L
i=0, si∈RCdenotes
a set of semantic phrase features generated from an SRL
builder. s0is assigned to a [CLS] token. These features
are used to facilitate the alignments with tube features. The
SRL builder identifies verbs and verb-related phrases in a
sentence and tags their semantic role labels as VERB . Then,
the other noun phrases related to the verbs are labeled as
ARG.ARG is assigned numbers such as ARG0 andARG1
in the order of their importance to provide specific seman-
tic roles. Specifically, ARG0 andARG1 represent the agent
of the verb and the corresponding object, respectively. Be-
cause VERB presents an action in a sentence, ARG expresses
the corresponding actor in video. In this manner, semantic
phrase are highly correlated with tube feature in video.
We explain our implementation in detail. We use a
BERT-based model to predict semantic labels from train-
ing sentences [42]. Then, we remove noisy elements, e.g.
ARGM label which including time and place, and aggre-
gate relevant ARG andVERB elements to construct seman-
13692
tic phrases based on predicted labels in the pre-processing.
For instance, with the sentence “A girl passionately singing
song at the stage”, we extract “a girl singing song” as a se-
mantic phrase consisting of ARG0 ,VERB , andARG1 . Addi-
tionally, to address instances involving non- VERB sentence,
such as basic descriptions (e.g., “4k panorama video”),
we adopted the word ’background’ in place of a semantic
phrase. More detailed examples of semantic phrase pro-
cessing are described in the supplementary material.
3.3. Cross Fusion Transformer
Fig. 2 presents a cross fusion transformer (CFT) designed
to achieve alignments between the proposed video and lan-
guage features. The fusion model is composed of a series of
self-attention, cross-attention, and FFNs. We use a visual-
to-text attention (VTA) and a tube-to-SRL attention (TSA).
Visual-to-Text attention (VTA). As shown in Fig. 2 (b),
we use a pair of VandTto generate a fused embedding
sequence MGthrough cross-attention. MGis used to ex-
press global contexts, because Vis pooled over an entire
video sequence and Tcontains all the words in a sentence.
The key K, query Q, and value Vare determined as follows:
MG=MHA (Q=V,V=T,V=T), (1)
where MHA is multi-head attention.
Tube-to-SRL attention (TSA). We propose TSA attention
in CFT to facilitate action-oriented alignments. MLis gen-
erated as a fused embedding sequence using a pair of Uand
Sthrough TSA, as shown in Fig. 2. MLrepresents specific
semantic embedding as compared with MG, sinceUandS
are the action-related features. It is generated by the multi-
head attention via a key, query, and value, as follows:
ML=MHA (Q=U,V=S,V=S). (2)
3.4. Pre-training objective
We use five proxy tasks to train SRTube, including two
novel proxy tasks that are introduced to incorporate action-
oriented elements in the training. We also use three gen-
eral proxy tasks of a masked language modeling (MLM),
a video-text matching (VTM), and video-text contrastive
(VTC). Despite improved performance by MLM, VTM, and
VTC, they have posed challenges with dynamic temporal
events in video. To address this issue, the proposed proxy
tasks use the tube features and semantic phrases.
Masked action modeling (MAM). MAM uses a VERB to-
ken to enhance a semantic alignment between a set of tube
features and semantic phrase features, since the token re-
flects a temporal attribution in a sentence. Specifically, we
generate a masked semantic embedding Smby masking
some words, which tagged as VERB from semantic phrases.
As following BERT [14], the masking is done by replacing
the word with a special [MASK] token, as shown in Fig. 4.
Figure 4. Illustrations of our proxy tasks MAM and ANM. In a
sentence “Man putting cardboard into recycling bin on suburban
street,” MAM masks “putting” tagged as VERB , and ANM counts
the verb semantic phrase to 1.
Our objective is to predict the masked text tokens (e.g.
“putting” in Fig. 4), motivated by the MLM. UandSm
are provided to TSA in the CFT. The output embedding is
fed into the prediction head, and, as a result, the predicted
probability distribution of a masked token pmis obtained.
We apply cross-entropy loss to predict masked text tokens
and the objective function is defined as,
LMAM =E(U,Sm)∼DH(yv|pm(U, Sm)), (3)
where His cross-entropy loss and yvis the masked verb. D
is the distribution.
Action numbering modeling (ANM). We propose an
ANM task as a self-supervised learning scheme to enhance
an ability to discriminate actions from video. It is in-
spired by our observation that video-text pairs often exhibit
more dynamic attributions, when the corresponding sen-
tences contain more verbs. Our model counts the number
of verbs and predicts the action-related attribution, as shown
in Fig. 4, whereas previous methods relied on only simple
linguistic analysis such as pos-tagging [13, 17].
We use the SRL builder to count the number, served as
a pseudo label ynfor the task. Our model generates an em-
bedding with the output [CLS] token of ML. We obtain a
predicted probability distribution paby feeding MLinto a
prediction network including fully connected and softmax
layers. We predict yn, ranging from 0 to 10, and utilize a
mean squared error (MSE) loss function, as follows:
LANM =MSE (yn, pa), (4)
where MSE denotes a mean squared error.
The ANM enhances the ability to discriminate tempo-
ral attributions of video, including dynamic motions and
static scenes (when yn=0). ANM often acts as a regula-
tor to avoid a motion bias. When relying on only the fea-
tures pre-trained with action recognition datasets, the static
13693
scenes would not be appropriately distinguished.
3.5. Total Loss Function in Training
The total loss Lallis defined as follow :
Lall=LMLM +LV TM +LV TC +LMAM +LANM,(5)
where LMLM ,LV TM andLV TC are the general losses in
VidLP methods [12].
4. Experiment
4.1. Experimental Setting
Pre-training Datasets and Details. We use A V A v2 dataset
[18] to train a tube builder. As in the recent works [5, 12,
28, 50], our model is trained on the large-scaled video-text
pair datasets such as WebVid2M [5] and CC3m [41].
We train a tube builder and SRTube for 20 epochs on
24 NVIDIA V100 GPUs. We use AdamW [22] optimizer
with an initial learning rate of 5×10−5. We split a video
sequence into uniform clips and sample 16 frames from ran-
domly selected clips. We resize frame to 224×224.
Fine-tuning. For TR tasks, we fine-tune only the CFT with
a VTC loss. For VQA task, we add an MLP to take an
input [CLS] embedding for classification and optimize the
model with cross-entropy loss. For VC task, to generate
a sentence, the model predicts [MASK] until [END] token
appears. All fine-tuning experiments are performed on the
same GPU setting with pre-training.
Downstream Datasets. Our method has been extensively
evaluated with various downstream tasks and datasets, in-
cluding text-to-video retrieval (TR), video question and an-
swering (VQA), video captioning (VC), and zero shot re-
trieval (ZR). We summarize the datasets as follows:
•TR: MSRVTT [47], DideMo [19], LSMDC [1], SSv2-
Lable, SSv2-Template [27], ActivityNet Caption[23]
•VQA: MSRVTT-QA [47], TVQA [25], MSVD-QA [46].
•VC: MSRVTT [47], MSVD [46]
•ZR: MSRVTT [47], DideMo [19], LSMDC [1]
4.2. Experimental Results
We evaluate the performance of the proposed method in
comparison to the state-of-the-art VidLP studies [12, 17, 44,
45, 50] in the following tasks.
Text-to-Video Retrieval. Table 1 summarizes the results
in TR on MSR-VTT [47], LSMDC [1], and DiDeMo [19]
benchmarks, including the results of the fine-tuning and
zero-shot setting. We use R@1, R@5, and R@10 as the
evaluation metrics. For clear comparisons, we present the
number of pre-training data in the table.
On MSRVTT and LSMDC datasets, our method
achieves the highest scores among the tested methods with
all the metrics. In the table, we remark several methods[28, 44, 49] with ∗and†, sharing similar motivations to
ours. [28, 44] and [49] use pre-trained object detectors
and trajectories, respectively. Our method improves the
R@1 score by approximately 10.5%and7.9%in compar-
ison to [44] and [49]. Moreover, our model surpasses the
performance of HiTeA [50] which utilizes features from di-
verse temporal duration, demonstrating the superiority of
the proposed temporal modeling. Our results on DiDeMo
dataset are second-ranked with R@5 and R@10 metrics,
slightly worse than VindLU [12]. DiDeMo includes video
sequences that exceed one minute, contain diverse scene
changes. Our tube features assume continuous scenes to
hold reliable actions, when sampling short-clips. Future re-
search will consider adaptive frame selection to avoid noisy
tube features and irrelevant clips.
We compare the results on the Something-to-Something
dataset (SSv2) and ActivityNet caption dataset, including
more motion dynamics, in Table 2 and 3. Our model im-
proves 1.2%accuracy with R@1 over X-CLIP [36], which
utilized a large pre-training dataset, and exhibits superior
performance compared to the other recent models.
Zero-shot Retrieval. We show zero-shot retrieval results
without fine-tuning to present the reliable performance of
the proposed model. Table 1 presents the zero-shot per-
formance of tested methods. Our model achieves the best
scores in all the datasets and metrics. In comparison to a re-
cent method [17], our approach provides an improved score
approximately by 3.2%,3.2%, and 5.2%on the MSR-VTT,
LSMDC, and DiDeMo datasets, respectively.
Video Question and Answering. Table 4 presents the
results on MSR-VTT, MSVD, and TVQA. Our method
shows improved scores on MSR-VTT and MSVD. The
TVQA dataset, consisting of drama content, differs from the
MSVD dataset, by having more continuous scenes, which
enhances the prediction of tube trajectories [55]. Due to
the characteristic, our model surpasses [12] by a margin of
0.6. Our action-oriented approach leads to significant dif-
ferences. Furthermore, as shown in Table 7, our method
significantly improves the accuracy of “What type” ques-
tions, which typically involve action-related answer.
Video Captioning. We conduct fine-tuning and evalua-
tion tests on VC tasks on MSR-VTT and MSVD datasets.
The CIDEr metric is employed to perform an evaluation
of captioning performance. Table 5 presents that SRTube
significantly outperforms the state-of-the-art [28], which is
instant-level matching method, by a 1.6%margin on MSR-
VTT dataset. In MSVD dataset, our method is slightly
worse than [30], using a larger size of PT pairs and second-
ranked.
4.3. Ablation Study
Tube feature and SRL phrase. We demonstrate the effec-
tiveness of our proposed visual and textual semantic feature
13694
MSR-VTT LSMDC DiDeMo
Method Pre-training dataset #PT pairs R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10
UniVL [35] H100M 136M - 49.6 63.1 - - - - - -
ClipBERT [26] COCO, VG 0.2M 22.0 46.8 59.9 - - - 20.4 48.0 60.8
Frozen [5] W2M, CC3M 5.5M 31.0 59.5 70.5 15.0 30.8 39.8 31.0 59.8 72.4
VIOLET [16] YT, H100M 180M 34.5 63.0 73.4 16.1 36.6 41.2 32.6 62.8 74.7
OA-Trans* [44] W2M, CC3M 5.5M 35.8 63.4 76.5 18.2 34.3 43.7 34.8 64.4 75.1
All-in-one [45] H100M 138M 37.9 68.1 77.1 - - - 37.9 68.1 77.1
BridgeFormer [17] W2M, CC3M 5.5M 37.6 64.8 751 17.9 35.4 44.5 37.0 62.2 73.9
TW-BERT†[49] W2M, CC3M 5.5M 38.4 65.1 76.6 21.0 38.8 49.2 41.8 71.1 81.2
VindLU [12] W2M, CC3M 5.5M 43.8 70.3 75.5 - - - 54.6 82.3 88.2
SRTube(ours) W2M, CC3M 5.5M 46.3 71.1 81.9 27.7 46.9 55.7 54.9 82.1 87.2
Zero shot text to video retrieval
Frozen [5] W2M, CC3M 5.5M 18.7 39.6 51.6 9.3 22.0 30.1 21.1 46.0 56.2
VIOLET [16] W2M, CC3M 5.5M 25.9 49.5 59.7 - - - 23.5 49.8 59.8
ALPRO* [28] W2M, CC3M 5.5M 24.1 44.7 55.4 - - - 23.8 47.3 57.9
OA-Trans* [44] W2M, CC3M 5.5M 23.4 47.5 55.6 - - - - - -
BridgeFormer [17] W2M, CC3M 5.5M 26.0 46.4 56.4 12.2 25.9 32.2 25.6 50.6 61.1
SRTube(ours) W2M, CC3M 5.5M 29.1 49.4 60.0 16.4 25.8 37.7 34.4 55.1 63.4
Table 1. Performance comparison of our model and SOTA models in TR on MSR-VTT, LSMDC, and DiDeMo datasets under fine-tuning
settings (top) and ZR (bottom). #PT pairs is the number of video-text pairs for pre-training. H100M : HowTo100M, CC3M : Conceptual
Caption, YT: YT-Temporal [52], W2M : WebVid-2M, COCO : microsoft coco [33], VG: Visual Genome [24]
SSv2-Lable SSv2-Template
Method R@1 R@5 R@10 R@1 R@5 R@10
Frozen [5] - - - 52.9 94.8 99.4
Singularity [27] 44.1 73.5 82.2 77.0 98.9 99.4
VindLU [12] 51.2 78.8 - 82.2 98.9 -
SRTube(ours) 52.9 79.5 88.8 83.5 100.0 100.0
Table 2. Performance comparison of TR on SSv2 dataset. For
a fair comparison, we compare the methods pre-trained on W2M
and CC3M. SSv2-Label task involves using ground truth sentences
for video retrieval. SSv2-Template task employs object-masked
sentences to retrieve videos [27].
ActivityNet Caption
Method #PT Pairs R@1 R@5
All-in-one [45] 138M 22.4 53.7
Singularity [27] 5.5M 43.0 70.6
TW-BERT [49] 5.5M 31.7 62.3
X-CLIP [36] 400M 44.3 74.1
HiTeA [50] 5.5M 45.1 73.5
SRTube(ours) 5.5M 46.5 74.1
Table 3. Performance comparison in TR on ActivityNet Caption.
in Table 6. For fair evaluations, we limit the training ob-
jectives to only MLM, VTC, and VTM. We assess the im-
pact of each semantic feature in ZR task on the MSR-VTT
dataset. Table 6 presents that UandSlead to performance
increase of 1.6%and1.4%with R@1 metric, respectively.Methods #PT pairs MSR-VTT MSVD TVQA
HERO [29] 136M - 45.9 74.2
MERLOT [52] 180M 43.1 - 78.7
ALPRO [28] 5.5M 42.1 45.9 -
Singularity [27] 17M 43.5 - -
VindLU [12] 5.5M 43.6 - 79.0
SRTube(ours) 5.5M 44.1 46.0 79.6
Table 4. Performance comparison with existing methods in VQA.
Method #PT pairs MSRVTT MSVD
UniVL [35] 180M 49.9 -
LA VENDER [30] 5.5M 58.0 142.9
STOA-VLP [57] 2.5M 60.2 131.8
SRTube(ours) 5.5M 61.8 142.5
Table 5. Performance comparison in VC with state-of-arts method
on MSRVTT and MSVD with a CIDEr score.
With R@10, there is a slight loss with S, which could occur
when the SRL over-recognizes verbs. However, this can
be compensated for by the tube features and the proposed
objectives. When combining all the features, the features
produce 3.8%,4.3%, and 2.4%improvements on the R@1,
R@5, and R@10 metric at final.
Pre-training objectives. We conduct an ablation study of
pre-training objectives in Table 7 in VQA task. All the pre-
training objectives improve the accuracy on top of LBase .
Our analysis includes a detailed breakdown of accuracy in
13695
MSR-VTT
Method R@1 R@5 R@10
Baseline( V,T) 24.7 42.1 52.1
Baseline + Tube ( V,U,T) 26.3 45.0 53.4
Baseline + SRL ( V,T,S) 26.1 44.2 51.8
Baseline + Tube + SRL ( V,U,T,S) 28.5 46.4 54.5
Table 6. Ablation tests of the tube and semantic phrase features.
MethodALL
(13,157)What
(8,149)Who
(4,552)Others
(456)
LBase 44.7 37.6 54.8 73.2
LBase +LMAM 44.9 37.8 54.9 73.2
LBase +LANM 45.4 38.5 55.5 73.9
LBase +LMAM +LANM 46.0 39.2 55.7 74.3
Table 7. Ablation tests on the proposed objectives in VQA on
MSVD dataset. LBase is sum of the MLM, VTM, and VTC losses.
# Frame mAP.
4 24.1
8 25.4
16 26.1
32 24.9# Tube queries mAP.
4 23.0
8 26.2
12 24.4
16 24.5
Table 8. Ablation study on frame counts and tube query numbers
with the mAP values and A V A v2 dataset.
different question categories, “what”, “who”, and “other”.
To predict answers to “what” questions, the model has to
focus on the action of objects, while “who” questions are
related to appearance information. Our experimental re-
sults show that the integration of MAM and ANM lead to
observable increase in performance: 1.6%in “what” ques-
tions. This result indicates that our model can catch the
appearance of objects both action information in temporal
domain.
Parameters in tube feature. We conduct an ablation study
to examine the effect of the number of frames and queries
on the performance of the tube builder. We fix either the
number of frames or the number of queries during the abla-
tion tests. Table 8 presents the impact on performance when
the number of frames is varied with a constant query count
as8. The right section examines the effects of the number
of tube queries with a fixed frame count as 16.
4.4. Qualitative Visualization
Fig. 5 presents qualitative examples of STRTube. We show
visualization of a cross-attention map, using a sample from
the MSR-VTT test set. In Fig. 5, we show examples of
semantic phrases, the predicted bounding boxes, and the at-
tention maps for words corresponding to VERB , highlighted
in bold in the figure. For instance, in (a), the attention map
Figure 5. Visualization of the attention map generated by the CFT,
using samples from MSR-VTT dataset. Our model attends to the
patches associated with moving objects by tracking the action tra-
jectory. SRL label is expressed with blue, pink for ARG, and green
forVERB .
for the VERB “shot” focuses on objects related arguments.
This indicates that our method understands the motion of
moving objects, while generating video tube features. Fur-
thermore, in (c) and (d), where only the subject of the ac-
tion is present, our method more distinctly focuses on the
features necessary to differentiate the actions.
5. Conclusion
We proposed a new VidLP framework that used video tube
features for temporal analysis and SRL for language pro-
cessing, aiming to identify action-related regions in videos
more effectively. The video encoder generated multiple
tube features along object trajectories to identify action-
related regions within videos, addressing limitations of ex-
isting attention mechanisms. Moreover, the text encoder
incorporated high-level, action-related language knowledge
using SRL, enhancing instance-level text matching and the
CM alignment. This approach, enhanced by two novel pre-
training objectives and a self-supervision strategy, outper-
formed existing VidLP models in various tasks and datasets,
establishing a new baseline in the VidLP domain.
Acknowledgment
This work was partly supported by Institute of Informa-
tion &communications Technology Planning &Evalua-
tion (IITP) grant funded by the Korea government(MSIT)
(No.2021-0-02068, Artificial Intelligence Innovation Hub)
and was partly supported by the NRF grant funded by MSIT
(No.NRF-2022R1A2C4002052).
13696
References
[1] Rohrbach Anna, Rohrbach Marcus, Tandon Niket, and
Schiele Bernt. A dataset for movie description. In IEEE
Conf. Comput. Vis. Pattern Recog. , pages 3202–3212, 2015.
[2] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen
Sun, Mario Lu ˇci´c, and Cordelia Schmid. Vivit: A video
vision transformer. In Int. Conf. Comput. Vis. , pages 6836–
6846, 2021.
[3] Y . Asano, D. Campbell, C. Feichtenhofer, J. Henriques, F.
Metze, I. Misra, M. Patrick, A. Vedaldi, et al. Keeping your
eye on the ball: Trajectory attention in video transformers.
Adv. Neural Inform. Process. Syst. , 34:12493–12506, 2021.
[4] Piyush Bagad, Makarand Tapaswi, and Snoek Cees GM. Test
of time: Instilling video-language models with a sense of
time. In IEEE Conf. Comput. Vis. Pattern Recog. , pages
2503–2516, 2023.
[5] Max Bain, Arsha Nagrani, G ¨ul Varol, and Andrew Zisser-
man. Frozen in time: A joint video and image encoder for
end-to-end retrieval. In Int. Conf. Comput. Vis. , pages 1728–
1738, 2021.
[6] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:
Bert pre-training of image transformers. arXiv preprint
arXiv:2106.08254 , 2021.
[7] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is
space-time attention all you need for video understanding?
InICML , page 4, 2021.
[8] Shyamal Buch, Crist ´obal Eyzaguirre, Adrien Gaidon, Jia-
jun Wu, Li Fei-Fei, and Niebles Juan Carlos. Revisiting
the” video” in video-language understanding. In IEEE Conf.
Comput. Vis. Pattern Recog. , pages 2917–2927, 2022.
[9] Adrian Bulat, Perez Rua Juan Manuel, Swathikiran Sud-
hakaran, Brais Martinez, and Georgios Tzimiropoulos.
Space-time mixing attention for video transformer. Adv. Neu-
ral Inform. Process. Syst. , 34:19594–19607, 2021.
[10] Meng Cao, Tianyu Yang, Junwu Weng, Can Zhang, Jue
Wang, and Yuexian Zou. Locvtp: Video-text pre-training
for temporal localization. In Eur. Conf. Comput. Vis. , pages
38–56. Springer, 2022.
[11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In Eur. Conf. Com-
put. Vis. , pages 213–229. Springer, 2020.
[12] Feng Cheng, Xizi Wang, Jie Lei, David Crandall, Mohit
Bansal, and Gedas Bertasius. Vindlu: A recipe for effec-
tive video-and-language pretraining. In IEEE Conf. Comput.
Vis. Pattern Recog. , pages 10739–10750, 2023.
[13] Yuhao Cui, Zhou Yu, Chunqi Wang, Zhongzhou Zhao, Ji
Zhang, Meng Wang, and Jun Yu. Rosita: Enhancing vision-
and-language semantic alignments via cross-and intra-modal
knowledge integration. In Proceedings of the 29th ACM
International Conference on Multimedia , pages 797–806,
2021.
[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018.[15] Z. Dou, Y . Xu, Z. Gan, J. Wang, S. Wang, L. Wang, C. Zhu,
P. Zhang, L. Yuan, and N. Peng. An empirical study of train-
ing end-to-end vision-and-language transformers. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 18166–18176, 2022.
[16] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, Wang William
Yang, Lijuan Wang, and Zicheng Liu. Violet: End-to-end
video-language transformers with masked visual-token mod-
eling. arXiv preprint arXiv:2111.12681 , 2021.
[17] Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xi-
aohu Qie, and Ping Luo. Bridging video-text retrieval with
multiple choice questions. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 16167–16176, 2022.
[18] Chunhui Gu, Chen Sun, David A Ross, Carl V ondrick, Car-
oline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan,
George Toderici, Susanna Ricco, Rahul Sukthankar, et al.
Ava: A video dataset of spatio-temporally localized atomic
visual actions. In IEEE Conf. Comput. Vis. Pattern Recog. ,
pages 6047–6056, 2018.
[19] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef
Sivic, Trevor Darrell, and Bryan Russell. Localizing mo-
ments in video with natural language. In Int. Conf. Comput.
Vis., pages 5803–5812, 2017.
[20] Roei Herzig, Elad Ben-Avraham, Karttikeya Mangalam,
Amir Bar, Gal Chechik, Anna Rohrbach, Trevor Darrell, and
Amir Globerson. Object-region video transformers. In IEEE
Conf. Comput. Vis. Pattern Recog. , pages 3148–3159, 2022.
[21] Nayoung Kim, Seong Jong Ha, and Je-Won Kang.
Video question answering using language-guided deep
compressed-domain video feature. In IEEE Conf. Comput.
Vis. Pattern Recog. , pages 1708–1717, 2021.
[22] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014.
[23] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and
Niebles Carlos Juan. Dense-captioning events in videos. In
Int. Conf. Comput. Vis. , pages 706–715, 2017.
[24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
tidis, Li-Jia Li, David A Shamma, et al. Visual genome:
Connecting language and vision using crowdsourced dense
image annotations. International journal of computer vision ,
123:32–73, 2017.
[25] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg.
Tvqa: Localized, compositional video question answering.
arXiv preprint arXiv:1809.01696 , 2018.
[26] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Berg Tamara L,
Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for
video-and-language learning via sparse sampling. In IEEE
Conf. Comput. Vis. Pattern Recog. , pages 7331–7341, 2021.
[27] Jie Lei, Tamara L Berg, and Mohit Bansal. Revealing single
frame bias for video-and-language learning. arXiv preprint
arXiv:2206.03428 , 2022.
[28] Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles,
and Steven CH Hoi. Align and prompt: Video-and-language
pre-training with entity prompts. In IEEE Conf. Comput. Vis.
Pattern Recog. , pages 4953–4963, 2022.
13697
[29] Linjieb Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng
Yu, and Jingjing Liu. Hero: Hierarchical encoder for video+
language omni-representation pre-training. arXiv preprint
arXiv:2005.00200 , 2020.
[30] Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng
Liu, Ce Liu, and Lijuan Wang. Lavender: Unifying video-
language understanding as masked language modeling. In
IEEE Conf. Comput. Vis. Pattern Recog. , pages 23119–
23129, 2023.
[31] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei
Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu
Wei, et al. Oscar: Object-semantics aligned pre-training for
vision-language tasks. In Eur. Conf. Comput. Vis. , pages
121–137. Springer, 2020.
[32] Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe
Gan, Zicheng Liu, Yumao Lu, and Lijuan Wang. Swin-
bert: End-to-end transformers with sparse attention for video
captioning. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 17949–
17958, 2022.
[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13 , pages 740–755. Springer, 2014.
[34] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,
Stephen Lin, and Han Hu. Video swin transformer. In IEEE
Conf. Comput. Vis. Pattern Recog. , pages 3202–3211, 2022.
[35] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan
Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou.
Univl: A unified video and language pre-training model for
multimodal understanding and generation. arXiv preprint
arXiv:2002.06353 , 2020.
[36] Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang,
and Rongrong Ji. X-clip: End-to-end multi-grained con-
trastive learning for video-text retrieval. In Proceedings
of the 30th ACM International Conference on Multimedia ,
pages 638–647, 2022.
[37] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,
Makarand Tapaswi, Ivan Laptev, and Josef Sivic.
Howto100m: Learning a text-video embedding by watching
hundred million narrated video clips. In Int. Conf. Comput.
Vis., pages 2630–2640, 2019.
[38] Niluthpol Chowdhury Mithun, Juncheng Li, Florian Metze,
and Amit K Roy-Chowdhury. Learning joint embedding
with multimodal cues for cross-modal video-text retrieval. In
Proceedings of the 2018 ACM on International Conference
on Multimedia Retrieval , pages 19–27, 2018.
[39] Davide Moltisanti, Frank Keller, Hakan Bilen, and Laura
Sevilla-Lara. Learning action changes by measuring verb-
adverb textual relationships. In IEEE Conf. Comput. Vis.
Pattern Recog. , pages 23110–23118, 2023.
[40] Arka Sadhu, Kan Chen, and Ram Nevatia. Video object
grounding using semantic roles in language description. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 10417–10427, 2020.[41] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. Conceptual captions: A cleaned, hypernymed, im-
age alt-text dataset for automatic image captioning. In Pro-
ceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , pages
2556–2565, 2018.
[42] Peng Shi and Jimmy Lin. Simple bert models for rela-
tion extraction and semantic role labeling. arXiv preprint
arXiv:1904.05255 , 2019.
[43] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and
Cordelia Schmid. Videobert: A joint model for video and
language representation learning. In Int. Conf. Comput. Vis. ,
pages 7464–7473, 2019.
[44] Jinpeng Wang, Yixiao Ge, Guanyu Cai, Rui Yan, Xudong
Lin, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Object-
aware video-language pre-training for retrieval. In IEEE
Conf. Comput. Vis. Pattern Recog. , pages 3313–3322, 2022.
[45] Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Lin Kevin
Qinghong, Satoshi Tsutsui, Xudong Lin, Guanyu Cai, Jian-
ping Wu, Ying Shan, et al. All in one: Exploring unified
video-language pre-training. In IEEE Conf. Comput. Vis.
Pattern Recog. , pages 6598–6608, 2023.
[46] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,
Xiangnan He, and Yueting Zhuang. Video question answer-
ing via gradually refined attention over appearance and mo-
tion. In Proceedings of the 25th ACM international confer-
ence on Multimedia , pages 1645–1653, 2017.
[47] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large
video description dataset for bridging video and language. In
IEEE Conf. Comput. Vis. Pattern Recog. , pages 5288–5296,
2016.
[48] Xu Yang, Zhangzikang Li, Haiyang Xu, Hanwang Zhang,
Qinghao Ye, Chenliang Li, Ming Yan, Yu Zhang, Fei Huang,
and Songfang Huang. Learning trajectory-word alignments
for video-language tasks. arXiv preprint arXiv:2301.01953 ,
2023.
[49] Xu Yang, Zhangzikang Li, Haiyang Xu, Hanwang Zhang,
Qinghao Ye, Chenliang Li, Ming Yan, Yu Zhang, Fei Huang,
and Songfang Huang. Learning trajectory-word alignments
for video-language tasks. arXiv preprint arXiv:2301.01953 ,
2023.
[50] Qinghao Ye, Guohai Xu, Ming Yan, Haiyang Xu, Qi
Qian, Ji Zhang, and Fei Huang. Hitea: Hierarchical
temporal-aware video-language pre-training. arXiv preprint
arXiv:2212.14546 , 2022.
[51] Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu,
and Haifeng Wang. Ernie-vil: Knowledge enhanced vision-
language representations through scene graphs. In AAAI ,
pages 3208–3216, 2021.
[52] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu,
Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Mer-
lot: Multimodal neural script knowledge models. Adv. Neu-
ral Inform. Process. Syst. , 34:23634–23651, 2021.
[53] H. Zhang, J. Duan, M. Xue, J. Song, L. Sun, and M. Song.
Bootstrapping vits: Towards liberating vision transformers
from pre-training. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 8944–
8953, 2022.
13698
[54] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang,
Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao.
Vinvl: Revisiting visual representations in vision-language
models. In IEEE Conf. Comput. Vis. Pattern Recog. , pages
5579–5588, 2021.
[55] Shun Zhang, Jia-Bin Huang, Jongwoo Lim, Yihong Gong,
Jinjun Wang, Narendra Ahuja, and Ming-Hsuan Yang.
Tracking persons-of-interest via unsupervised representation
adaptation. International Journal of Computer Vision , 128:
96–120, 2020.
[56] Jiaojiao Zhao, Yanyi Zhang, Xinyu Li, Hao Chen, Bing
Shuai, Mingze Xu, Chunhui Liu, Kaustav Kundu, Yuanjun
Xiong, Davide Modolo, et al. Tuber: Tubelet transformer
for video action detection. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 13598–13607, 2022.
[57] Weihong Zhong, Mao Zheng, Duyu Tang, Xuan Luo, Heng
Gong, Xiaocheng Feng, and Bing Qin. Stoa-vlp: Spatial-
temporal modeling of object and action for video-language
pre-training. arXiv preprint arXiv:2302.09736 , 2023.
13699
