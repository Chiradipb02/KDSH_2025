BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis
via Bridging Image and Video Diffusion Models
Fengyuan Shi1Jiaxi Gu2Hang Xu2Songcen Xu2Wei Zhang2Limin Wang1,3*
1State Key Laboratory for Novel Software Technology, Nanjing University
2Huawei Noah‚Äôs Ark Lab3Shanghai AI Laboratory
https://bivdiff.github.io
‚ÄúMake him on the road‚Äù 
Video editingControllable video generationVideo inpaintingVideo diffusion foundation modelsPretrained T2Ve.g. VidRD, ZeroScopeImage synthesis diffusion modelsPretrained T2IControllable image generatione.g., ControlNet, T2I AdapterPretrained T2IImage editinge.g., Instruct Pix2Pix, Prompt2PromptPretrained T2IImage inpaintinge.g., Stable Diffusion InpaintingPretrained T2IOther tasks‚Ä¶‚Ä¶
‚ÄúA white swan moving on the lake‚Äù ‚Äú‚Äù 
Figure 1. Given an image diffusion model (IDM) for a specific image synthesis task, and a text-to-video diffusion foundation model
(VDM), our model can perform training-free video synthesis, by bridging IDM and VDM.
Abstract
Diffusion models have made tremendous progress in text-
driven image and video generation. Now text-to-image
foundation models are widely applied to various down-
stream image synthesis tasks, such as controllable image
generation and image editing, while downstream video syn-
thesis tasks are less explored for several reasons. First, it
requires huge memory and computation overhead to train a
video generation foundation model. Even with video foun-
dation models, additional costly training is still required
for downstream video synthesis tasks. Second, although
some works extend image diffusion models into videos in a
training-free manner, temporal consistency cannot be well
preserved. Finally, these adaption methods are specifi-
cally designed for one task and fail to generalize to differ-
ent tasks. To mitigate these issues, we propose a training-
free general-purpose video synthesis framework, coined as
BIVDiff , via bridging specific image diffusion models and
general text-to-video foundation diffusion models. Specif-
ically, we first use a specific image diffusion model (e.g.,
*Corresponding author (lmwang@nju.edu.cn).ControlNet and Instruct Pix2Pix) for frame-wise video gen-
eration, then perform Mixed Inversion on the generated
video, and finally input the inverted latents into the video
diffusion models (e.g., VidRD and ZeroScope) for temporal
smoothing. This decoupled framework enables flexible im-
age model selection for different purposes with strong task
generalization and high efficiency. To validate the effective-
ness and general use of BIVDiff, we perform a wide range
of video synthesis tasks, including controllable video gener-
ation, video editing, video inpainting, and outpainting.
1. Introduction
Diffusion models [11, 29, 31] have shown impressive ca-
pabilities in generating diverse and photorealistic images.
By scaling up dataset and model size, large-scale text-to-
image diffusion models [4, 10, 20, 24, 25, 27] gain strong
generalization ability and make tremendous breakthroughs
in text-to-image generation. By fine-tuning these powerful
image generation foundation models on high-quality data
in specific areas, various downstream image synthesis tasks
also come a long way, such as controllable image genera-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
7393
tion [19, 37], image editing [2, 9, 17], personalized image
generation [6, 26], and image inpainting [25]. However,
video diffusion models are less explored for different video
synthesis tasks due to several critical issues.
First, training video generation foundation models re-
quires substantial training on a massive amount of labeled
video data, heavily depending on a large scale of comput-
ing resources [7, 8, 12, 13, 28]. Even with video foundation
models available, additional training on high-quality data in
specific areas is still required for downstream video synthe-
sis tasks such as controllable video generation [5, 32] and
video editing [15, 18]. To improve training efficiency, Tune-
A-Video [33] fine-tunes a pre-trained text-to-image model
on the input video. Although Tune-A-Video can learn tem-
poral consistency, this kind of per-input fine-tuning is still
time-consuming. And it may overfit the small number of in-
put videos and its generalization ability is limited (e.g., poor
motion editability). Second, while some works extend im-
age diffusion models into videos in a training-free manner,
their temporal consistency cannot be well kept and flickers
can still be observed (e.g., Fig. 6), due to the weak tempo-
ral modeling. Finally, previous works are usually proposed
for one specific task and it requires different methods to ex-
tend from images to videos for different downstream video
synthesis tasks with limited cross-task generality.
Image generation models can exhibit strong generaliza-
tion and diversity, and yield many powerful downstream
image synthesis models through fine-tuning. But frame-
wise video generation with image models would lead to
temporal inconsistency. Video generation foundation mod-
els can generate temporally coherent videos but require
additional costly training for downstream video synthesis
tasks. A question arises naturally: Is it possible to build a
training-free framework for general-purpose video synthe-
sis by jointly leveraging the strengths of both pre-trained
image and video diffusion models? The key challenge is
how to design a simple and general interface to bridge these
two types of diffusion models to efficiently achieve tempo-
ral consistency in video synthesis.
To this end, we propose a general training-free video
synthesis framework (BIVDiff), via bridging the specific
image diffusion models and a general text-to-video diffu-
sion model. Specifically, we first use a task-specific image
diffusion model (like ControlNet [37], Instruct Pix2Pix [2])
to generate the target video in a frame-by-frame manner,
then perform DDIM Inversion [30] on the generated video,
and finally input the inverted latents into the video diffusion
model (VDM) for temporal smoothing. Decoupling im-
age and video models enables flexible model selection for
different synthesis purposes, which endows the framework
with strong task generalization and high efficiency (Fig. 1).
Despite using inverted latents by image DDIM Inversion,
VDM tends to generate contents inconsistent with IDM insome cases, due to the distribution shifts. Moreover, for
the case with a large gap between the latent distributions of
image and video diffusion models, VDMs will fail to gen-
erate videos. For example, in the case of inputting source
videos, the initial noisy frame latents obtained by frame-
wise DDIM Inversion of image diffusion models are highly
correlated, making some VDMs (e.g., VidRD [8]) with i.i.d.
random latent requirement collapse to meaningless noises
(Fig. 10). Accordingly, we introduce an improved version
called Mixed Inversion. Specifically, we perform DDIM In-
version with both image and video diffusion models. Both
latents by Image and Video DDIM Inversion encode the
content of videos. The former could be further tempo-
rally smoothed by VDM but its distribution may be different
from the one required by VDM. The latter cannot be further
temporally smoothed by VDM but the distribution is consis-
tent with VDM. We use a weighted sum of these two latents
to adjust the distribution of initial latents fed into VDM.
With this Mixed Inversion, we can flexibly adjust the latent
distribution to make VDMs produce more consistent and
better results, and trade off between temporal smoothing
and open generation capability of VDMs. To validate the
effectiveness of BIVDiff, we perform experiments on vari-
ous representative video synthesis tasks, including 1) Con-
trollable Video Generation; 2) Video Editing; and 3) Video
Inpainting and Outpainting. Our contributions are summa-
rized as follows:
‚Ä¢ We propose a general training-free video synthesis frame-
work, via bridging downstream task-specific image dif-
fusion models and text-to-video diffusion models. Our
BIVDiff is simple, efficient, and generalizable for differ-
ent video synthesis tasks.
‚Ä¢ We introduce Mixed Inversion, i.e., mixing the DDIM in-
verted latents of image and video diffusion models, to ad-
just the latent distribution to make VDMs produce more
consistent and better results, and trade off between tempo-
ral smoothing and open generation capability of VDMs.
‚Ä¢ We perform extensive experiments on various video
synthesis tasks, including controllable video generation
video editing, video inpainting, and outpainting, demon-
strating the effectiveness and general use of BIVDiff.
2. Related Work
2.1. Diffusion Models for Image Synthesis
The emergence of diffusion models [11, 29, 31] has signif-
icantly advanced the progress of text-to-image generation.
ADM [4] proposes classifier guidance for text-driven im-
age generation. GLIDE [20] introduces classifier-free guid-
ance [10] to improve image quality further. DALLE-2 [24]
trains a prior model on CLIP text latents for better text-
image alignment. Imagen [27] shows that text encoding
with large language models (e.g., T5 [23]) is effective at
7394
VDM
(iii) Video Inpainting/Outpainting(ii) Video Editing(i) Controllable Video Generation
(a) Frame-wise Video GenerationImage DiffusionVideo Diffusion√ó1‚àíùõº√óùõº%ùíµ!"#$%ùíµ%!&'($
(b) Mixed InversionConvSelf/cross-attentionTemporal attentionMixed Inversion
‚Ä¶
A sports car is moving on the roade.g., VidRD
(c) Video Temporal Smoothing√óT √óT ‚Ä¶e.g., Instruct Pix2PixIDM‚Ä¶e.g., ControlNetIDM‚Ä¶e.g., Stable Diffusion InoaintingIDMVideo EditingStyle TransferReplace BackgroundScene Change Attributeediting
ReplaceObjectVideo InpaintingVideo OutpaintingEraseObjectReplaceObjectControllable Video GenerationPose-to-VideoDepth-to-VideoEdge-to-VideoOtherTasksTraining-free Video Synthesis%ùíµ)
%ùíµ$ùíµ)Figure 2. BIVDiff pipeline. Our framework consists of three components, including Frame-wise Video Generation, Mixed Inversion, and
Video Temporal Smoothing. We first use the image diffusion model to do frame-wise video generation, then perform Mixed Inversion on
the generated video, and finally input the inverted latents into the video diffusion model for video temporal smoothing.
image synthesis. Latent diffusion models (LDM) [25] per-
form diffusion and denoising processes in latent space, to
increase training efficiency.
With the powerful pre-trained text-to-image diffusion
foundation models, various downstream image synthesis
tasks have also made great progress, such as controllable
image generation [19, 37], image editing [2, 9, 17], person-
alized image generation [6, 26], image inpainting [25], etc.
ControlNet [37] trains an auxiliary U-Net on image-control
pairs to make models generate images conditioned on spe-
cific controls, such as depth, edge and human pose. Instruct
Pix2Pix [2] is trained on generated training data to edit im-
ages from instructions. Textual Inversion [6] and Dream-
Booth [26] optimize a single word embedding using a few
images of a user-provided concept for personalized image
generation. Although effective, additional fine-tuning or
optimization on input images is still required to transfer
text-to-image foundation models into specific downstream
image synthesis tasks, which is costly.
2.2. Diffusion Models for Video Synthesis
Inspired by text-to-image diffusion models [7, 8, 12, 13,
28], some works propose text-to-video diffusion models by
adding extra temporal modules and train models on a large
scale of video data. In addition to text-to-video generation,
video diffusion models are also applied in various down-
stream video synthesis tasks, such as controllable video
generation [3, 5, 32, 35] and video editing [15, 18].
Training these video models is memory-hungry and
computationally expensive. Some works attempt to adapt
pre-trained image diffusion models to videos for effi-
cient video synthesis. Tune-A-Video [33] adopts one-
shot tuning on each input video for text-driven video edit-
ing. VideoP2P[16] is built on Tune-A-Video [33] and
Prompt2Prompt [9], and introduce Null-Text Inversion [17]
to improve the editing quality further. And there are alsosome training-free video synthesis methods, such as Con-
trolVideo [38] and FateZero[21]. ControlVideo [38] pro-
poses full-frame attention, i.e., concatenating all frames into
a ‚Äùbig image‚Äù and performing self-attention on it, while
Fate-Zero [21] fuses self-attention with a blending mask to
ensure frame consistency. Although One-shot tuning and
optimization make models generate high-fidelity videos,
they suffer from poor generalization ability (e.g., cannot
edit complex motion). Training-free adapting image diffu-
sion models to videos provides an efficient solution to video
synthesis, but they are less effective in maintaining cross-
frame consistency at the level of texture and details [36],
thus flickering artifacts are still severe.
Unlike previous works of adapting image models to
videos by adding some modules or complex attention op-
erations for a specific task, we present a simple method
to bridge image and video models, and combine both ad-
vantages for training-free video synthesis. With a specific
downstream image model (e.g., ControlNet [37]) and a gen-
eral diffusion-based text-to-video foundation model (e.g.,
VidRD [8]), we can efficiently adapt to different video
synthesis tasks (e.g., controllable video generation) in a
training-free manner.
3. Method
Given a video synthesis task, we choose an image diffu-
sion model (IDM) of its image task version and a text-to-
video diffusion foundation model (VDM) . Let random la-
tentsZT={zT
i}m
i=1or video V={vi}m
i=1be the inputs,
where Tis the number of diffusion step, and mis frame
number. Let P‚àóbe the target prompt, and Cbe the condi-
tions (e.g., depth maps and masks) according to target task.
Our goal is to generate a temporally coherent video V‚àó.
Our framework consists of three components, including
Frame-wise Video Generation, Mixed Inversion, and Video
7395
(i) Depth: A person on a motorcycle does a burnout on a frozen lake.
(ii) Canny: A silver jeep car is moving on the winding forest road.
(iii) Pose: An astronaut moonwalks on the moon.Figure 3. Qualitative results of our proposed BIVDiff on con-
trollable video generation task, conditioned on depth maps, canny
edges and human pose sequence. We choose ControlNet [37] as
our image diffusion model.
Temporal Smoothing. As shown in Fig. 2, we first use the
image diffusion model to perform frame-wise video genera-
tion, then perform Mixed Inversion on the generated video,
and finally input the inverted latents into the video diffusion
model for video temporal smoothing.
3.1. Frame-wise Video Generation
The first step of our proposed framework is to perform
frame-wise video generation with image diffusion models.
For the given video synthesis task, we can choose an im-
age counterpart. For instance, if we want to do control-
lable video generation, then we can use ControlNet [37] to
generate the frames under the conditioning controls (e.g.,
edges, depth, etc.) independently. As for video editing,
we can select one image editing model, such as Instruct
Pix2Pix [2], to edit each frame in the video according to the
target prompt independently. The generation process can be
formulated as:
ÀÜZ0={ÀÜz0
i=IDM(fi,C)}m
i=1, (1)
where fiis the i-th random latent or frame in the given
video, and Care conditions (e.g., text prompt, depth maps,
and masks). Due to this decoupled design, our framework
gains great flexibility and strong generalization ability. Thatis to say that we can choose arbitrary downstream image
diffusion models for general-purpose video synthesis.
3.2. Mixed Inversion
The key of bridging image and video diffusion models is
DDIM Inversion. After IDM denoising, we need to con-
duct DDIM Inversion to convert denoised latents to initial
noisy latents as the input to the subsequent VDM. By DDIM
Inversion, we can preserve the information that IDM gener-
ates, and make VDM synthesized videos consistent with the
results of IDM but temporally coherent, instead of free gen-
eration. The frame-wise DDIM Inversion process can be
formulated as:
ÀÜZT={ÀÜzT
i=DDIMimg
inv(ÀÜz0
i)}m
i=1,
where DDIMimg
invmeans DDIM Inversion with image diffu-
sion models. It is worth noting that we choose an image
diffusion foundation model (e.g., Stable Diffusion [25]) for
DDIM Inversion instead of the same model for frame-wise
video generation, and the prompt is œïfor DDIM Inversion.
Despite using inverted latents by image DDIM Inversion,
VDM tends to generate content inconsistent with IDM in
some cases, due to the distribution shifts. Moreover, when
the gap between the latent distributions of image and video
diffusion models is big, VDMs will fail to generate correct
videos. For example, in the cases of inputting source videos,
the initial noised latents of frames obtained by frame-wise
DDIM Inversion with image diffusion models are highly
correlated, making some VDMs (e.g., VidRD [8]) requiring
i.i.d. random latents as inputs collapse and generate mean-
ingless noises (Fig. 10).
To solve these problems, we introduce Mixed Inversion.
As shown in Fig. 2, we perform DDIM Inversion with both
image and video diffusion models. Both latents by Image
and Video DDIM Inversion keep the contents of videos.
The former can be temporally smoothed by VDM but the
distribution may be different from the distributions required
by VDM. The latter cannot be further temporally smoothed
by VDM distribution but the distribution is consistent with
VDM. We can weighted-sum these two latents to adjust the
distribution of initial latents fed into VDM. The latents mix-
ing process is as follows:
ÀÜZT
img={ÀÜzT
i=DDIMimg
inv(ÀÜz0
i)}m
i=1, (2)
ÀÜZT
video =DDIMvideo
inv(ÀÜZ0), (3)
ÀÜZT=Œ±¬∑ÀÜZT
img+ (1‚àíŒ±)¬∑ÀÜZT
video, (4)
where DDIMvideo
inv means DDIM Inversion with our video
diffusion model [8] and Œ±is the mixing ratio used to ad-
just the ratio of the image and video latent components.
With Mixed Inversion, we can adjust the latent distribution
7396
Original prompt: A man moonwalks
Style transfer: Make it Minecraft style
Replace object: Replace the man with Spider Man
Replace Background: Change the background to stadiumOriginal prompt: A car is moving on the road
Attention Refine: A white car is moving on the road
Attention Refine: A car is moving on the road  at sunsetAttention Replace: A bicycle  is moving on the road
(a) With Instruct Pix2Pix (b) With Prompt2PromptFigure 4. Qualitative results of our proposed BIVDiff on video editing task. We select two popular image editing methods, Instruct
Pix2Pix [2] and Prompt2Prompt [9] as image models, and test a wide range of editing types.
to make VDMs produce correct results, and trade eoff be-
tween temporal smoothing and open generation capability
of VDMs.
3.3. Video Temporal Smoothing
Although we can resort to image diffusion models for video
synthesis tasks by frame-wise generation, temporal consis-
tency is ignored, leading to visible flickers (e.g., Fig. 6).
Video generation foundation models learn temporal consis-
tency and can generate temporally coherent videos. There-
fore, we do temporal smoothing on the video generated by
IDM, by feeding the inverted latents into VDM. VDM can
effectively capture the information stored in the inverted la-
tents, and make the input videos consistent in temporal di-
mension, without destroying the contents created by IDM.
The temporal smoothing process is formulated as:
Z0=VDM (ÀÜZT,P‚àó). (5)
After temporal smoothing, we use vae decoder to decode
the denoised latents to the target video.
4. Experiment
4.1. Implementation Details
To validate the effectiveness of our framework, we perform
experiments on four representative video synthesis tasks,
including 1) controllable video generation with Control-
Net [37], 2) video editing with Instruct Pix2Pix [2] andPrompt2Prompt [9], 3) video inpainting with Stable Diffu-
sion Inpainting [25] and 4) video outpainting with Stable
Diffusion Inpainting [25]. For the video diffusion founda-
tion model, we choose VidRD [8]. In the case of models
for DDIM Inversion, we use Stable Diffusion 1.5 for frame-
wise inversion, and VidRD for video-level inversion.
In our experiments, we generate 8 frames with 512 √ó
512 resolution for each video. The classifier-free guid-
ance scale is 7.5 and the total timestep is 50. For the
mixing ratio Œ±in Mixed Inversion, we set 1.0, 1.0, 0.25,
and 0.1 for BIVDiff with ControlNet, Instruct Pix2Pix,
Prompt2Prompt and Stable Diffusion Inpainting as the de-
fault settings, respectively. And there is no per-video opti-
mization (e.g. Null-text Inversion [17]) in our experiments.
4.2. Qualitative Results
Controllable Video Generation. By bridging pre-trained
controllable image generation model ControlNet [37] and
text-to-video foundation model VidRD [8], our framework
BIVDiff supports zero-shot controllable video generation.
Fig. 3 shows the generated videos conditioned on depth
maps, canny edge maps, and human pose sequences. As
shown in Fig. 3, the generated videos are well-matched with
the conditions and keep significant temporal consistency,
such as backgrounds, and both the appearance and structure
of foreground objects.
Video Editing. Video editing is another important applica-
tion in video synthesis. We choose two representative image
editing models Instruct Pix2Pix [2] and Prompt2Prompt [9]
7397
(a) Video Inpainting (b) Video OutpaintingSource
Video
Mask
SD
BIVDiff
(ours)Figure 5. Qualitative results of our proposed BIVDiff on video inpainting and outpainting task. We adopt Stable Diffusion Inpainting [25]
as our image model. Our method can erase objects and complete the masked regions well.
for zero-shot video editing. For video editing with Instruct
Pix2Pix, we test various editing types, including style trans-
fer, object replacement, and background replacement, as
shown in Fig. 4 (a). As for Prompt2Prompt, we follow the
paper to do attention replacement and attention refinement.
As shown in Fig. 4 (b), our framework can replace the ob-
ject, edit attribute, and do global editing, which are inherited
from Prompt2Prompt in a zero-shot manner.
Video Inpainting and Outpainting. Additionally, we in-
troduce an image inpainting model Stable Diffusion In-
painting [25] for video inpainting. For video outpainting,
we can transfer the inpainting model to outpainting easily,
by making masked regions of outpainting be the erased re-
gions of inpainting. As shown in Fig. 5, independently pro-
cessing each frame makes imperfect shadows that have not
been completely erased and inconsistent areas to be filled in.
We can eliminate these temporal inconsistencies by com-
bining image and video diffusion models.
Additional Models. To further validate the effectiveness
and general use of BIVDiff, we introduce more diffu-
sion models, including another video diffusion model Ze-
roScope [1] and image diffusion model T2I-Adapter [19].
The qualitative results are in Supplementary Material.
4.3. Comparison with Baselines
We quantitatively and qualitatively compare our method
with some baselines on controllable video genera-
tion (Text2Video-Zero [14], FateZero [21] and Tune-A-
Video[38]) and video editing (Text2Video-Zero [14] and
ControlVideo [14]). For quantitative comparison, we use
DA VIS dataset in LOVEU-TGVE Benchmark [34], which
consists of 16 videos and with 4 prompts per video, for au-tomatic metrics and user study evaluation. Following Tune-
A-Video [33], we adopt CLIP [22] to calculate frame con-
sistency and textural alignment score. For user study, we
follow Dreamix [18] to invite 25 human raters working on
AI, arts and other areas, to rate videos by quality, fidelity,
and alignment score on a scale of 1‚àí5. We also test the
practical running time to compare inference speed.
Quantitative Comparison. Table 1 shows the quantita-
tive results. For automatic metrics, our method has the
best frame consistency due to the strong temporal model-
ing of VDM and comparable textual alignment. And our
method is most favored by participants in the user study
experiment since we can generate temporally coherent and
realistic high-quality videos. Moreover, BIVDiff achieves a
comparable inference speed in practice. Without modifying
structures and inference pipelines inside IDM and VDM,
we avoid time-consuming attention operations [21] or train-
ing [33] and benefit from parallel GPU computing.
Qualitative Comparison. We present visual comparisons
in Fig. 6. Fig. 6(a) shows that ControlNet generates high-
quality frames matched with controls (e.g., depth maps),
but has severe frame inconsistency (e.g., the background
is inconsistent across frames). Text2Video-Zero and Con-
trolVideo generate temporally smooth videos, but there are
still some slight flickers due to weak temporal modeling,
and they struggle to accurately match the given controls
(e.g., the lane lines disappear). In contrast, our method can
generate temporally coherent videos well-matched with the
conditions. Similar results can be found in video editing
(Fig. 6(b)). Our method can keep more details in the input
video (e.g., floor and shadows) and the generated videos are
more realistic (e.g., the body of Spider Man).
7398
Source 
video
ControlNet
Text2Video -
Zero
ControlVideo
BIVDiff
(Ours)
Source 
video
Instruct
Pix2Pix
Text2Video -
Zero
Tune -A-Video
BIVDiff
(Ours)
(a) Controllable Video Generation (b) Video EditingAn army green jeep car is moving 
on the winding forest roadReplace the man with Spider Man
Spider Man moowalks  (For Tune -A-Video)Figure 6. Qualitative comparison with baselines on controllable video generation and video editing task. Our BIVDiff generates high-
quality and temporally coherent videos, and shows better (a) control and temporal consistency and (b) fidelity and realness.
MethodAutomatic Metrics User Study Inference Time
Frame Consistency Textual Alignment Quality Alignment Fidelity Avg. (per video)
Text2Video-Zero 91.69 26.85 2.74 3.16 2.98 2.96 25s
ControlVideo 92.63 26.12 2.61 3.12 2.54 2.76 57s
BIVDiff (Ours) 92.67 26.25 3.38 3.24 2.72 3.11 61s
Text2Video-Zero 91.57 25.37 2.26 2.23 2.46 2.32 56s
FateZero 90.75 26.42 2.38 1.7 3.05 2.38 221s
Tune-A-Video 90.46 28.33 2.30 2.23 2.35 2.29 11min + 26s
BIVDiff (Ours) 93.50 26.16 2.98 2.30 2.68 2.65 64s
Table 1. Quantitative comparison with baselines. The upper part is the result of controllable video generation with depth control. The
bottom part is the result of video editing. Tune-A-Video adopts null-text inversion and one-shot tuning, while Text2Video-Zero and our
BIVDiff are based on InstructPix2Pix and training-free. Our method has the best temporal consistency and is most favored by humans.
IDM VDM BIVDiff
(Alternate)BIVDiff
(Fuse)BIVDiff
(Sequential)
Bridging Strategies012345Quality
Figure 7. User study for bridging strategies ablation study.
4.4. Ablation Study
In this section, we study several key designs of our method,
including the strategies of bridging image and video diffu-
sion models, and the mixing ratio Œ±in Mixed Inversion.
Ablation on bridging strategies. To validate the effective-
ness of our bridging framework, we realize different strate-
gies for comparisons, including 1) IDM . We use Control-
0.0 0.2 0.4 0.6 0.8 1.0
Mixing ratio012345Quality
ControlNet
Instruct Pix2Pix
Prompt2Prompt
StableDiffusion InpaintingFigure 8. User study for mixing ratio ablation study.
Net [37] to do frame-wise video generation under the guid-
ance of depth control. 2) VDM . We adopt VidRD [8] for
text-to-video generation without depth control. 3) IDM and
VDM Alternate . We use IDM and VDM for alternate de-
noising, i.e. one IDM denoising step by one VDM denois-
ing step. 4) IDM and VDM Fuse . We perform IDM and
VDM denoising simultaneously, and average these two la-
tents. 5) IDM and VDM Sequential , i.e. our proposed
7399
Source 
Video
IDM
VDM
IDM and VDM
Alternate
IDM and VDM
Fuse
IDM and VDM
Sequential
(BIVDiff )
A striking mallard floats effortlessly 
on the sparkling pondFigure 9. Ablation on strategies of bridging image and video dif-
fusion models. Our sequential strategy can temporally smooth the
videos (e.g., consistent appearance, structure, and background),
and limit the open generation ability of VDM (e.g., the generated
mallard of VDM is not in the final result of our method.)
BIVDiff. The user study in Fig. 7 shows our sequential
strategy works best. As shown in Fig. 9, videos generated
by ControlNet are temporally inconsistent and VDM pro-
duces temporally consistent videos but unmatched with the
given depth control. Bridging IDM and VDM during the
denoising process (‚ÄúAlternate‚Äù and ‚ÄúFuse‚Äù) tries to combine
the results of IDM and VDM (e.g., there are two mallards
in the videos). In contrast, our proposed BIVDiff bridges
IDM and VDM in a sequential way, and generates tempo-
rally coherent videos consistent with depth control.
Ablation on mixing ratios. We also conduct an ablation
study on video editing with Prompt2Prompt [9] to ana-
lyze the effects of mixing ratio. As shown in Fig. 10, the
larger Œ±is, the more temporally consistent the generated
videos are. For example, there is a car and multiple bicy-
cles that should not have appeared in the edited videos of
Prompt2Prompt [9]. In contrast, videos generated by our
method are more consistent with the input video and text
prompt, and temporally coherent when Œ±is 0.25. How-
ever, with Œ±increasing, the quality of synthesized videos
degrades and there are a lot of noises and artifacts in the
videos. This is because the frames in the edited videos are
similar (e.g., similar large areas of background) and latents
by frame-wise DDIM Inversion with image diffusion mod-
els are highly correlated. When the video diffusion mod-
BIVDiff
(Œ±=1.0)BIVDiff
(Œ±=0.5)BIVDiff
(Œ±=0.25)BIVDiff
(Œ±=0.1)BIVDiff
(Œ±=0)P2PSource 
Video
A car bicycle  is moving on the roadFigure 10. Ablation on the mixing ratio Œ±in Mixed Inversion.
Larger Œ±leads to more temporally consistent videos, and smaller
Œ±makes the distribution of latents fed into VDM closer to VDM‚Äôs
and generates higher quality videos.
els, such as VidRD [8], require i.i.d. random latents as in-
put, models will corrupt and produce noised videos. Fig. 8
shows video quality under different mixing ratios for each
IDM and VDM pair. In practice, we can use small Œ±to
bridge latent distribution gaps and generate correct videos.
5. Conclusion
In this paper, we present a training-free framework for
general-purpose video synthesis, coined as BIVDiff, via
bridging downstream image diffusion models and text-to-
video foundation diffusion models. We first use an im-
age diffusion model (e.g., ControlNet [37]) for frame-wise
video generation, then perform Mixed Inversion on the
generated video, and finally input the inverted latents into
the video diffusion model (e.g., ViDRD [8]) for temporal
smoothing. We introduce Mixed Inversion to adjust the
latent distribution to make VDMs produce correct results,
and balance between temporal smoothing and open genera-
tion capability of VDMs. Extensive experiments on a wide
range of video synthesis tasks demonstrate the effectiveness
and generalization power of our method.
Acknowledgements. This work is supported by National
Key R &D Program of China (No. 2022ZD0160900), Na-
tional Natural Science Foundation of China (No. 62076119,
No. 61921006)), and Collaborative Innovation Center of
Novel Software Technology and Industrialization.
7400
References
[1] Zeroscope. https : / / huggingface . co /
cerspense/zeroscope_v2_576w , 2023. 6
[2] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structpix2pix: Learning to follow image editing instructions.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 18392‚Äì18402, 2023.
2, 3, 4, 5
[3] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li,
Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video:
Controllable text-to-video generation with diffusion models.
arXiv preprint arXiv:2305.13840 , 2023. 3
[4] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems , 34:8780‚Äì8794, 2021. 1, 2
[5] Patrick Esser, Johnathan Chiu, Parmida Atighehchian,
Jonathan Granskog, and Anastasis Germanidis. Structure
and content-guided video synthesis with diffusion models.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 7346‚Äì7356, 2023. 2, 3
[6] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,
Amit Haim Bermano, Gal Chechik, and Daniel Cohen-Or.
An image is worth one word: Personalizing text-to-image
generation using textual inversion. In The Eleventh Interna-
tional Conference on Learning Representations, ICLR 2023,
Kigali, Rwanda, May 1-5, 2023 , 2023. 2, 3
[7] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew
Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-
Yu Liu, and Yogesh Balaji. Preserve your own correlation:
A noise prior for video diffusion models. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 22930‚Äì22941, 2023. 2, 3
[8] Jiaxi Gu, Shicong Wang, Haoyu Zhao, Tianyi Lu, Xing
Zhang, Zuxuan Wu, Songcen Xu, Wei Zhang, Yu-Gang
Jiang, and Hang Xu. Reuse and diffuse: Iterative
denoising for text-to-video generation. arXiv preprint
arXiv:2309.03549 , 2023. 2, 3, 4, 5, 7, 8
[9] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image
editing with cross-attention control. In The Eleventh Interna-
tional Conference on Learning Representations, ICLR 2023,
Kigali, Rwanda, May 1-5, 2023 , 2023. 2, 3, 5, 8
[10] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. arXiv preprint arXiv:2207.12598 , 2022. 1, 2
[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840‚Äì6851, 2020. 1, 2
[12] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben
Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
video: High definition video generation with diffusion mod-
els.arXiv preprint arXiv:2210.02303 , 2022. 2, 3
[13] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William
Chan, Mohammad Norouzi, and David J. Fleet. Video dif-
fusion models. arXiv preprint arXiv:2204.03458 , 2022. 2,
3[14] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-
vosyan, Roberto Henschel, Zhangyang Wang, Shant
Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-
image diffusion models are zero-shot video generators. arXiv
preprint arXiv:2303.13439 , 2023. 6
[15] Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu,
and Jiashi Feng. Magicedit: High-fidelity and temporally
coherent video editing. arXiv preprint arXiv:2308.14749 ,
2023. 2, 3
[16] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya
Jia. Video-p2p: Video editing with cross-attention control.
arXiv preprint arXiv:2303.04761 , 2023. 3
[17] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and
Daniel Cohen-Or. Null-text inversion for editing real im-
ages using guided diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 6038‚Äì6047, 2023. 2, 3, 5
[18] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav
Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid
Hoshen. Dreamix: Video diffusion models are general video
editors. arXiv preprint arXiv:2302.01329 , 2023. 2, 3, 6
[19] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-
gang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning
adapters to dig out more controllable ability for text-to-image
diffusion models. arXiv preprint arXiv:2302.08453 , 2023. 2,
3, 6
[20] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741 , 2021. 1, 2
[21] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,
Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-
ing attentions for zero-shot text-based video editing. arXiv
preprint arXiv:2303.09535 , 2023. 3, 6
[22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748‚Äì8763. PMLR, 2021. 6
[23] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research , 21(1):5485‚Äì5551, 2020. 2
[24] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents. arXiv preprint arXiv:2204.06125 , 1
(2):3, 2022. 1, 2
[25] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¬®orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684‚Äì10695, 2022. 1, 2, 3, 4, 5, 6
[26] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
7401
tuning text-to-image diffusion models for subject-driven
generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 22500‚Äì
22510, 2023. 2, 3
[27] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems , 35:36479‚Äì36494, 2022. 1, 2
[28] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman.
Make-a-video: Text-to-video generation without text-video
data. In The Eleventh International Conference on Learn-
ing Representations, ICLR 2023, Kigali, Rwanda, May 1-5,
2023 , 2023. 2, 3
[29] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International confer-
ence on machine learning , pages 2256‚Äì2265. PMLR, 2015.
1, 2
[30] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In 9th International Con-
ference on Learning Representations, ICLR 2021, Virtual
Event, Austria, May 3-7, 2021 . OpenReview.net, 2021. 2
[31] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. arXiv preprint arXiv:2011.13456 , 2020. 1, 2
[32] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,
Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,
and Jingren Zhou. Videocomposer: Compositional video
synthesis with motion controllability. arXiv preprint
arXiv:2306.02018 , 2023. 2, 3
[33] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian
Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu
Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning
of image diffusion models for text-to-video generation. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 7623‚Äì7633, 2023. 2, 3, 6
[34] Jay Zhangjie Wu, Xiuyu Li, Difei Gao, Zhen Dong, Jin-
bin Bai, Aishani Singh, Xiaoyu Xiang, Youzeng Li, Zuwei
Huang, Yuanxi Sun, et al. Cvpr 2023 text guided video edit-
ing competition. arXiv preprint arXiv:2310.16003 , 2023. 6
[35] Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong
Zhang, Yingqing He, Hanyuan Liu, Haoxin Chen, Xiaodong
Cun, Xintao Wang, et al. Make-your-video: Customized
video generation using textual and structural guidance. arXiv
preprint arXiv:2306.00943 , 2023. 3
[36] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change
Loy. Rerender a video: Zero-shot text-guided video-to-video
translation. arXiv preprint arXiv:2306.07954 , 2023. 3
[37] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3836‚Äì3847, 2023. 2, 3, 4, 5, 7, 8[38] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng
Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo:
Training-free controllable text-to-video generation. arXiv
preprint arXiv:2305.13077 , 2023. 3, 6
7402
