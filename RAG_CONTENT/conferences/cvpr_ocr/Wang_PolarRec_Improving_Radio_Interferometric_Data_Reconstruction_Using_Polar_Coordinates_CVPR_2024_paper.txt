PolarRec: Improving Radio Interferometric Data Reconstruction
Using Polar Coordinates
Ruoqi Wang1, Zhuoyang Chen1, Jiayi Zhu1, Qiong Luo*,1,2, Feng Wang3
1The Hong Kong University of Science and Technology (Guangzhou),
2The Hong Kong University of Science and Technology,3Guangzhou University
{rwang280, zchen190, jzhu718 }@connect.hkust-gz.edu.cn, luo@ust.hk, fengwang@gzhu.edu.cn
Abstract
In radio astronomy, visibility data, which are measure-
ments of wave signals from radio telescopes, are trans-
formed into images for observation of distant celestial ob-
jects. However, these resultant images usually contain both
real sources and artifacts, due to signal sparsity and other
factors. One way to obtain cleaner images is to reconstruct
samples into dense forms before imaging. Unfortunately,
existing reconstruction methods often miss some compo-
nents of visibility in frequency domain, so blurred object
edges and persistent artifacts remain in the images. Fur-
thermore, the computation overhead is high on irregular
visibility samples due to the data skew. To address these
problems, we propose PolarRec, a transformer-encoder-
conditioned reconstruction pipeline with visibility samples
converted into the polar coordinate system. This coordinate
system matches the way in which radio telescopes observe
a celestial area as the Earth rotates. As a result, visibil-
ity samples distribute in the polar system more uniformly
than in the Cartesian space. Therefore, we propose to use
radial distance in the loss function, to help reconstruct com-
plete visibility effectively. Also, we group visibility samples
by their polar angles and propose a group-based encoding
scheme to improve the efficiency. Our experiments demon-
strate that PolarRec markedly improves imaging results by
faithfully reconstructing all frequency components in the
visibility domain while significantly reducing the computa-
tion cost in visibility data encoding. The code is available
at https://github.com/RapidsAtHKUST/PolarRec.
1. Introduction
In radio astronomy, visibility refers to radio signal data from
celestial objects, obtained by radio telescopes. These data
are represented as complex values in the uv-plane , a geo-
metric plane defined for interferometric observations. Vis-
*Corresponding Author.
SparseVisibility
Dirty ImageClean ImageImagingReconstructionReal PartImaginary Part(a) Traditional method: imaging followed by reconstruction
Sparse Visibility
Dense InpaintingClean ImageReconstructionImaging
RealPartImaginary Part
(b) Recent method: reconstruction followed by imaging.
Figure 1. Two visibility data processing flows.
ibility data are subsequently converted into images through
imaging for further analysis. However, these images,
known as dirty images , are often dominated by artifacts
[23]. This phenomenon is due to limitations in telescope
configurations, under which not the entire uv-plane is sam-
pled. Therefore, visibility data normally have to be recon-
structed before being utilized in scientific analysis. In this
paper, we propose a visibility reconstruction method, aim-
ing to reconstruct the real sky by recovering all visibility
components in the uv-plane effectively and efficiently.
Traditional methods first transfer the sparse visibility
data into dirty images through the imaging process and then
reconstruct the dirty images to clean images [1, 3, 10, 14,
26, 30]. The process is shown in Figure 1 (a). In contrast,
some recent deep-learning-based studies [23, 31] have pro-
posed to first do inpainting on the visibility data to recon-
struct the sparse samples to dense coverage and then per-
form imaging to obtain the clean image, as shown in Fig-
ure 1 (b), showing better performance. In order to produce
a high-fidelity sharp image, it is crucial to densely sample
the full visibility domain [31]. In this paper, we adopt the
reconstruction-and-imaging processing flow.
Existing methods for reconstructing visibility data face
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
12841
0.4
 0.2
 0.0 0.2 0.40.4
0.2
0.00.20.4
��(a) Cartesian Coordinates.
0°45°90°
135°
180°
225°
270°315°0.10.20.30.40.5 (b) Polar Coordinates.
Figure 2. Visibility data samples with different coordinate sys-
tems.
challenges in both effectiveness and efficiency. Specifically,
the first challenge is to effectively capture visibility com-
ponents within the uv-plane. For example, Radionets [23]
and U-Nets [22] use a convolutional neural network that is
based on pixels and grids, resulting in discontinuities in re-
constructed visibility maps. In comparison, Wu et al. [31]
use neural fields to address the continuity problem, but their
method can only restore the low-frequency part of visibil-
ity (located near the center of the uv-plane), largely miss-
ing the high-frequency portion (found far from the uv-plane
center). Such discontinuity and incompleteness in the visi-
bility domain cause blurred edges of observed objects, dis-
appearance of faint astronomical sources, and persistence of
artifacts, in resultant images.
The second major issue is the inefficiency of current
strategies. For instance, Radionets [23] encode visibility
data together with an excessive amount of zero-inpainting,
causing a large amount of unnecessary computation. Wu
et al. [31] embed each visibility sample point as a token
in their Transformer encoder [29] to attend to each other,
which is of quadratic computation cost in the number of
sample points [12], too high for real applications. These
limitations force a compromise between computational cost
and quality of reconstruction in current methods, severely
limiting the application of deep learning techniques in radio
astronomy imaging and reconstruction.
To address these challenges, we explore the observa-
tional nature of radio telescopes and then propose PolarRec,
which leverages the polar coordinates of sample points to
enhance the reconstruction quality and reduce the cost. Our
key observation is that visibility samples are obtained by
telescopes as the Earth rotates, and that the high- and low-
frequency signals are distributed on the uv-plane in accor-
dance with their distances from the center. As a result, the
visibility points distribute more evenly in the polar coordi-
nate system than in Cartesian coordinates [19] (see Figure
2). Moreover, the polar-based system has shown promising
properties in vision tasks including instance segmentation
and object detection [7, 19, 32, 33]. Therefore, we propose
to adopt the polar coordinate system, using the radial co-
ordinate to associate with the frequency information in theloss function and the angular coordinate to group sample
points.
Specifically, we first design a loss function for visibility
data with a weighting scheme based on the radial coordi-
nates. Our weighted loss function, Radial Visibility Loss,
associates visibility data with radial coordinates in the fre-
quency domain, enabling effective reconstruction of visibil-
ity data, on both high- and low-frequency components. In
comparison, existing work recovers mainly low-frequency
components. Consequently, our method produces sharper,
more detailed imaging results. Furthermore, by introducing
group-level encoding of visibility points according to angu-
lar coordinates, our method is more efficient than visibility-
point-level encoding. This group encoding improves com-
putation efficiency, making the use of Transformer encoders
for visibility encoding more practical and scalable.
In summary, our main contributions are as follows:
• We propose PolarRec to seamlessly integrate visibility
data with a Transformer-encoded reconstruction pipeline
by using polar coordinates, addressing two fundamental
challenges in visibility reconstruction.
• We introduce the Radial Visibility Loss (RVL), which
incorporates the radial coordinates of visibility data,
adeptly capturing both low- and high-frequency visibility
components.
• As the first to study the granularity of visibility encod-
ing, we innovatively utilize angular coordinates to group
sampled points for Transformer encoding, markedly en-
hancing computational efficiency.
We have experimentally evaluated our proposed Polar-
Rec on public datasets of four galaxy morphologies, includ-
ing an overall comparison with other state-of-the-art meth-
ods, ablation studies, and tests on group sizes and grouping
techniques. The experimental results demonstrate the effec-
tiveness and efficiency of our method.
2. Background and Related Work
2.1. Very Long Baseline Interferometry (VLBI)
In radio astronomy, using radio interference signals to im-
age distant astronomical sources requires telescopes of very
large aperture [4], because the angular resolution of a tele-
scope is inversely proportional to the diameter. A major ob-
servation technique is the Very Long Baseline Interferom-
etry (VLBI), which uses multiple radio telescopes spread-
ing over the globe to form a virtual Earth-sized telescope.
The radio waves from astronomical sources are recorded
separately at individual telescopes. Then, these signals are
cross-correlated for all pairs of antennas at a central loca-
tion, generating visibility data. A VLBI observation is typi-
cally performed for hours to measure as many points in the
uv-plane as possible. However, the measurement results re-
main sparse due to the limited number of antennas [4, 28].
12842
Real-skyLow-pass-1Low-pass-2High-pass-1High-pass-2Band-stopImaging resultsImaginaryRealImage domainVisibility domain
Figure 3. Effects of band limiting.
Consequently, sparse-to-dense reconstruction on visibility
data is necessary to improve the imaging quality for the fol-
lowing analysis.
2.2. Interferometric Imaging
Visibility data, represented as complex values, is the re-
sult of a Fourier transform of the sky’s brightness distri-
bution [18]. Imaging converts visibility data into images,
which can be analyzed to provide insights about the ob-
served celestial bodies [28]. In the imaging process, an in-
verse Fourier transform maps the (u, v)coordinates from
the Fourier domain to (l, m)coordinates in the image do-
main [31]. The transformation can be described as:
I(l, m) =Z
uZ
ve2πi(ul+vm)V(u, v)dudv. (1)
In this equation, V(u, v)is the visibility data in Fourier
space, and I(l, m)represents the intensity distribution in
the image domain.
2.3. Radio Interferometric Data Reconstruction
Radio interferometric data reconstruction is vital for con-
verting sparse visibility data into clear, artifact-free images.
However, previous methods in this domain [1, 3, 10, 14,
23, 26, 30, 31] have been limited by either their inability
to capture continuous and complete frequency components
or computational inefficiencies. To the best of our knowl-
edge, no previous studies have managed to reconstruct all
components in visibility data or increase the granularity of
visibility encoding for efficiency.
3. Our Method
In this section, we first investigate the relation between
visibility components and imaging results. After that, we
present PolarRec, which adopts the polar coordinate sys-
tem in visibility reconstruction. In PolarRec, we design
Radial Visibility Loss to incorporate a weighting scheme
based on the radial coordinate within the uv-plane. More-
over, we group the visibility points by their angular coordi-
nates and then extract grouping tokens for the subsequentTransformer encoding. An overview of our method is pre-
sented in Figure 4.
3.1. Imaging Results of Visibility
In radio interferometry, visibility data plays a vital role in
generating high-fidelity images of celestial objects. The
imaging process is the inverse Fourier transformation of
visibility data to construct the brightness distribution of the
observed sky. Consequently, missing or incomplete visibil-
ity components can significantly degrade the imaging result,
leading to artifacts and loss of crucial information about the
object’s structure. Therefore, we first investigate the impact
of missing visibility components on the final imaging out-
put.
We explore the impact of visibility components in var-
ious frequency regions by applying standard band-limiting
operations [17] and analyzing their effects on the imaging
results. As shown in Figure 3, the loss of high frequency
data due to a low-pass filter results in blur and artifacts and
causes the disappearance of weak sources. Meanwhile, a
high-pass filter eliminates low-frequency data and reduces
the overall quality, but it maintains clear object edges as
well as dim or small sources. Additionally, band-and-stop
filters lead to blurred images and artifacts.
In summary, different frequency ranges cause distinct
imaging effects. This observation suggests that recovering
all missing visibility components could enhance the quality
of imaging results. More specifically, if a resultant image
has blurred edges or misses dim light sources, it is probably
due to poor reconstruction of the high-frequency compo-
nents of the visibility data.
3.2. Polar Coordinates
Our utilization of the polar coordinates is to represent the
visibility more uniformly. It is because visibility sampling
is based on Earth’s rotation, and high- and low-frequency
visibility components are distributed on the uv-plane ac-
cording to their distances from the origin of the plane.
In the uv-plane, we convert (u, v)coordinates to polar
coordinates, denoted as (r(u, v), θ(u, v)):
r(u, v) =p
u2+v2 (2)
θ(u, v) =arctan2 (v, u) (3)
Where r(u, v)represents the radial distance from the ori-
gin, and θ(u, v)represents the angle of the vector from the
positive u-axis.
3.3. Radial Visibility Loss
We propose Radial Visibility Loss (RVL), incorporating the
radial coordinates of visibility data to capture both low- and
high-frequency visibility.
12843
SparseVisibilityPoints{𝑢!,𝑣!, V(𝑢!,𝑣!)}MLPDenseQuery(𝑢",𝑣")ReconstructedVisibility
Ground TruthVisibilityRVLIntra-Group EncodingInter-Group EncodingFiLMConditioning
……
𝑣"!…Transformer Block𝑣""𝑣"#𝑣"$………………𝑉(𝑢%,𝑣%)(𝑢%,𝑣%)PositionalEmbeddingLinearEmbeddingPE(𝑢!,𝑣!)
MLP ProjectionAdaptiveAvgPooling
Transformer BlockTransformer Block𝑉%Intra-Group EncodingInter-Group Encoding
Telescope ArraysPolar Groups
tm…t3t2t1𝑉′
Output 𝑇
ImagingResultImagingFigure 4. An overview of our method. Sparse visibility data V(us, vs)are grouped by the angular coordinate and passed through two
encoding layers: intra-group encoding to generate group tokens and inter-group encoding by a Transformer encoder. The encoded output
Tthen conditions the predicted visibility generation in the neural field. The final output is compared with the ground truth to compute the
Radial Visibility Loss.
To compute RVL, we first compute the weight matrix
w1(u, v)to down-weight easy visibility components (com-
ponents whose predicted values are close to the ground
truth) based on Focal Frequency Loss (FFL) [17]:
w1(u, v) =|Vr(u, v)−Vp(u, v)|α(4)
where Vr(u, v) =Ar+iBrandVp(u, v) =Ap+iBpare
the ground truth and predicted visibilities in complex form
respectively, and αis a scaling factor.
We then introduce an additional weight w2(u, v), com-
puted from r(u, v), to make our model pay more attention
to high-frequency components of the visibility during re-
construction. The weight term w2(u, v)is calculated as fol-
lows:
w2(u, v) =r(u, v)
max( r(u, v))+ 1β
(5)
Wherer(u,v)
max( r(u,v))normalizes the radial coordinate, ensur-
ing the weight is more significant for points farther from
the center, corresponding to higher frequencies in the visi-
bility data. Adding 1 prevents any weights from becoming
zero, maintaining the influence of all visibility components
during the learning process. βis a scaling factor.
The final weight w(u, v)is computed as:
w(u, v) =r(u, v)
max( r(u, v))+ 1β
|Vr(u, v)−Vp(u, v)|α
(6)
A thorough visibility distance measure must consider
both amplitude and phase as they hold distinct aspects of
imaging information [17]. The complex visibility value can
be represented as a vector within a two-dimensional plane.
Specifically, we map ground truth visibility Vr(u, v) =Ar+iBrand the corresponding predicted value Vp(u, v) =
Ap+iBpto vectors ⃗ prand⃗ pp(as shown in Figure 5). Ac-
cording to the definition of 2D discrete Fourier transform,
the vector magnitude relates to the amplitudes |⃗ pr|and|⃗ pp|,
and the angles θrandθpreflect the phase. Consequently,
following FFL [17], we define the single-point visibility dis-
tance considering both magnitude and direction as:
d(⃗ pr, ⃗ pp) =∥⃗ pr−⃗ pp∥2
2=|Vr(u, v)−Vp(u, v)|2.(7)
The final visibility distance is computed as the average of
these individual distances over all visibility points:
d(Vr, Vp) =1
MNM−1X
u=0N−1X
v=0|Vr(u, v)−Vp(u, v)|2.(8)
The final expression for the Radial Visibility Loss is
derived by executing the Hadamard product between the
weight matrix w(u, v) =w1(u, v)w2(u, v)and the visibil-
ity distance matrix d(Vr, Vp), as follows:
RVL =1
MNM−1X
u=0N−1X
v=0w(u, v)|Vr(u, v)−Vp(u, v)|2(9)
Real PartImagnary Partr
p
Ar ApBr
Bpd(pr,pp)
pr
pp
0
Figure 5. Example of visibility distance.
12844
3.4. Encoding by Angular-Coordinate Groups
Using Transformers for encoding visibility samples causes
quadratic computation cost in the number of sample points
[12]. Such high costs constrain practical applications, espe-
cially as the number of visibility points in each image ex-
pands with more telescope arrays or extended observation
duration. To solve the problem, we propose a group-based
encoding scheme to improve the efficiency.
As shown on the left in Figure 4, the sparsely sampled
visibility data is in the form of {us, vs, V(us, vs)}, where
(us, vs)are the coordinates at which a measurement is sam-
pled and V(us, vs)is the complex value of the sample
point. We embed the real and imaginary components in
V(us, vs)separately.
First, we divide all sample points into mgroups ac-
cording to their angular coordinate θ(us, vs). To inte-
grate each visibility value and its corresponding position,
we encode each sample individually using positional em-
bedding (PE (us, vs)in Figure 4). Specifically, we encode
the positional information of a sample point using Random
Fourier Embedding [27]. After that, the positional em-
bedding PE (us, vs)and the embedding of complex value
of visibility V(us, vs)are concatenated to form visibility
tokens V′. Denote the sparse visibility tokens as V′=
[v′
1;v′
2;v′
3;. . . v′
n], where V′∈Rn×d,nis the number of
sample measurement points and dis the number of dimen-
sions of visibility tokens.
Then, we apply a Multi-Layer Perceptron (MLP) map-
ping layer and averaging aggregating for intra-group encod-
ing. The encoding result is ˆV= [ˆv1; ˆv2; ˆv3;. . .ˆvm], where
ˆV∈Rm×d. For each group i, we compute the group token
ˆvias follows:
ˆvi=Avg(MLP(vj)), i= 1tom, v j∈group i.(10)
In the implementation of intra-group encoding, we sort
the tokens V′of sample points according to their angular
coordinates. After mapping these tokens through MLP, we
use adaptive average pooling to compute the group tokens
ˆV. For data collected with the same telescope configuration,
the process of sorting based on angular coordinates needs to
be performed only once, as the locations of sample points
remain unchanged. The intra-group encoding process is as
shown in Algorithm 1.
Finally, the group tokens ˆVgo through inter-group en-
coding by a Transformer encoder. We base our encoder
design on Transformer structures similar to prior work
[12, 29, 31]. The input group tokens are then transformed
into latent tokens by multi-headed self-attention layers.
3.5. Neural Field Conditioning
Our method follows the conditional neural field pipeline
proposed by Wu et al. [31]. Given the sparsely sampledAlgorithm 1 Intra-Group Encoding
1:foreachusi, vsi, V(usi, vsi)in set of sampled points do
2: pei←PE(usi, vsi)
3: lei←LinearEmbedding (V(usi, vsi))
4: v′
i←concat (pei,lei)
5: θi←arctan2 (usi, vsi)
6:end for
7:V′←[v′
1;v′
2;. . .;v′
n],Θ←[θ1;θ2;. . .;θn]
8:ifsorted indices not precomputed then
9: sorted indices ←argsort (V′,Θ)
10:end if
11:V′←V′[sorted indices ]
12:VMLP←MLP(V′′)
13:ˆV←AdaptiveAvgPooling (VMLP, m)
14:return ˆV
visibility V(us, vs), our objective is to determine a neural
fieldΦ, fulfilling a constraint set by the function F:
F(Φ(us, vs), V(us, vs)) = 0 . (11)
We approximate this implicit function Φ(u, v)with an
MLP of llayers parameterized by weights Θm.
We use the output tokens of inter-group encoding T=
[t1;t2;t3;. . . tl]to extend the neural field with a learning-
based prior, with each token corresponding to an MLP layer.
Using the FiLM conditioning [21], the output tokens mod-
ulate the ith layer’s activation xiby:
FiLM( xi) =γ(ti)⊙xi+β(ti), i∈1tol, (12)
where γandβare simple affine layers with non-linearities
and⊙signifies a Hadamard product [16].
The MLP parameters Θmand the encoder parameters Θe
are jointly optimized during training:
min
Θm,ΘeRVL(Φ (ud, vd;{T}; Θm), Vgt(ud, vd)),
with{T}= Ψ ({us, vs, V(us, vs)}; Θe),(13)
where (ud, vd)are the dense coordinates in visibility plane
andVgt(ud, vd)is the ground truth of visibility inpainting.
4. Experiments
In this section, we conduct a comprehensive evaluation
of our method in comparison with several classic and re-
cent state-of-the-art methods to demonstrate the overall im-
provement achieved by our approach. We also design exper-
iments to explore the effects of different grouping methods
and group sizes on the reconstruction results. In addition,
we conduct ablation experiments to study the effects of in-
dividual weighting techniques in RVL.
4.1. Experimental Setup
Platform . We conduct all experiments on a server with two
AMD EPYC 7302 CPUs, 128GB main memory, and eight
12845
Nvidia RTX 3090 GPUs each with 24GB device memory.
The server is equipped with an NVME 2TB SSD and four
1TB SATA hard disks. The operating system is Ubuntu
20.04. Our model is implemented in PyTorch 1.8.1 [20].
Datasets . In the experiments, we evaluate our method on
different public datasets of distinct galaxy morphologies [8,
13]: Merging Galaxies (MG), In-between Round Smooth
Galaxies (IRSG), Unbarred Tight Spiral Galaxies (UTSG),
and Edge-on Galaxies with Bulge (EGB). These data are de-
rived from sources under the DESI Legacy Imaging Surveys
[11], integrating contributions from the Beijing-Arizona
Sky Survey (BASS) [35], the DECam Legacy Survey (DE-
CaLS) [2], and the Mayall z-band Legacy Survey [24]. The
visibility data are generated from these images using the
eht-imaging toolkit [5, 6], denoted by {us, vs, V(us, vs)}.
The parameters for observation are adjusted to mirror an 8-
telescope Event Horizon Telescope (EHT) setup [31], with
the EHT being one of the most prominent arrays leveraging
VLBI techniques. All visibility data generated from our im-
age datasets are in 128X128 grids. We sample the Fourier
transform results of our image datasets to use as the ground
truth. In each image dataset, we randomly select 80% of the
images for training, and the other 20% for testing.
Methods under Comparison . We compare our method
with three other methods for radio interferometry recon-
struction, including the classic method CLEAN [14], which
is for dirty image reconstruction, and two latest deep
learning-based approaches for visibility data reconstruction
– Radionets [23] and Neural Interferometry [31]. We use
the original code of these methods and follow the param-
eter setting in the original code for the best performance.
All these methods are implemented on PyTorch. In ad-
dition, we test the magnetic resonance imaging (MRI) re-
construction U-Net [22, 34] to reconstruct visibility data
as supplementary baselines. Compared with CLEAN [14]
which assumes point sources, we use learning-based meth-
ods to learn priors from data for more accurate reconstruc-
tion. Different from leaning-based methods [22, 31, 34],
PolarRec utilizes polar coordinates for visibility data. We
use the polar groups as encoding granularity to improve the
efficiency whereas others use pixel grids [22, 34] or points
[31]. We use radial coordinate based loss, RVL, which can
capture both low- and high-frequency visibility. In contrast,
others use simple MSE [22, 31, 34].
Evaluation Metrics . To measure differences in fre-
quency data, we use the Log Frequency Distance (LFD)
[17], which is defined as follows:
LFD= log"
1
MN M−1X
u=0N−1X
v=0|Vr(u, v)−Vp(u, v)|2!
+ 1#
(14)
where Vr(u, v)andVp(u, v)represent the real and pre-
dicted visibility respectively. A lower LFD is better.
To evaluate the quality of images produced from the
(a) Dirty Image     (b) U-Net      (c) Radionets(d) Neu-Int     (e) PolarRec(f) Real SkyVis      ImageVis    ImageVis       ImageVis       ImageVis       ImageVis     Image
EGBUTSGIRSGMGFigure 6. Visual comparison of visibility and image. The recon-
structed images, visibility data and their corresponding error maps
are provided. The small panels to the left of each image are the
corresponding visibility data, with the real part on top and the
imaginary part at the bottom.
reconstructed visibility, we employ two common metrics:
Peak Signal-to-Noise Ratio (PSNR) and Structural Similar-
ity Index Measure (SSIM). PSNR quantifies the overall im-
age quality of the resultant images, and SSIM quantifies the
perceptual similarity to the ground truth images. We com-
pute these two metrics using the scikit-image package [25],
which follows the formulas presented by Hore et al. [15].
A higher PSNR and SSIM is better.
To evaluate the efficiency, we use inference time and
Floating Point Operations (FLOPs). The inference time in
the experiments is tested on a single Nvidia RTX 3090.
We compare time performance between our method and
Neural Interferometry only, because only these two are
transformer-encoder based.
4.2. Overall Comparison
We calculate the LFD, PSNR, and SSIM values for all test
images reconstructed using our method and other methods,
presenting both mean values and standard deviations. These
results are listed in Table 1. The results show that Polar-
Rec consistently outperforms the other methods in all three
measures and four datasets, underscoring the effectiveness
of our reconstruction method.
We also illustrate some representative reconstructed vis-
ibility and the corresponding images on the four datasets in
Figure 6, including all deep-learning-based methods under
comparison as well as the dirty images and the ground truth
images of the real sky. Comparing the dirty images in Fig-
ure 6 (a) and the ground truth in Figure 6 (f), we find there
12846
Table 1. Overall performance comparison (mean and standard deviation) on different datasets. All comparison methods have significant
difference with our method ( p <0.01).
Models Img VisMG IRSG UTSG EGB
LFD↓PSNR ↑SSIM ↑LFD↓PSNR ↑SSIM ↑LFD↓PSNR ↑SSIM ↑LFD↓PSNR ↑SSIM ↑
Dirty N/A 10.453 (1.256) 0.680 (0.055) N/A 10.875 (1.293) 0.711 (0.050) N/A 10.655 (1.092) 0.690 (0.047) N/A 10.158 (1.216) 0.674 (0.055)
CLEAN [14]✓ N/A 18.708 (2.336) 0.767 (0.031) N/A 20.974 (2.192) 0.795 (0.025) N/A 17.200 (2.465) 0.750 (0.035) N/A 20.175 (2.411) 0.779 (0.033)
U-Net [34] ✓1.171 (0.198) 18.126 (1.906) 0.818 (0.021) 1.235 (0.230) 19.770 (1.925) 0.828 (0.024) 1.256 (0.227) 14.877 (1.701) 0.786 (0.032) 1.219 (0.234) 19.232 (2.078) 0.822 (0.024)
Radionets [23] ✓1.580 (0.202) 19.687 (1.897) 0.836 (0.022) 1.499 (0.206) 21.369 (1.887) 0.854 (0.022) 1.419 (0.231) 20.828 (2.003) 0.844 (0.022) 1.545 (0.220) 20.322 (1.809) 0.836 (0.023)
Neu Int[31] ✓1.229 (0.282) 20.560 (2.287) 0.875 (0.028) 0.916 (0.302) 24.099 (2.760) 0.898 (0.027) 1.038 (0.313) 21.323 (2.303) 0.880 (0.027) 1.146 (0.343) 21.276 (2.675) 0.879 (0.031)
PolarRec (Ours) ✓0.924 (0.284) 24.206 (2.547) 0.889 (0.029) 0.666 (0.238) 26.540 (2.653) 0.911 (0.025) 0.707 (0.241) 25.430 (2.565) 0.905 (0.025) 0.707 (0.241) 25.880 (2.529) 0.906 (0.025)
are many artifacts and distortion of object structure in dirty
images because of the sparsity of the visibility.
As shown in Figure 6 (b), reconstructions of the vis-
ibility by MRI reconstruction U-Net [34] are dominated
by the artifacts in resultant images. In Figure 6 (c), Ra-
dionets [23] are able to reconstruct more visibility content
than the U-Net. However, the reconstruction is discontin-
uous. Although Radionets reduce artifacts in the imaging
results, it cannot distinguish between separate sources that
are in close proximity. Furthermore, many faint astronomi-
cal sources are missing in the reconstructed images. In con-
trast, as shown in Figure 6 (d), Neural Interferometry [31]
(denoted Neu-Int) can continuously and realistically recon-
struct the low-frequency components of the visibility, but
misses much information from the high-frequency compo-
nents, leading to a loss of details in the reconstruction.
The results of PolarRec (Figure 6 (e)) show that our
method can effectively reconstruct more complete and con-
tinuous visibility data than others. The resultant imaging
results not only eliminate artifacts but also restore the true
structure of astronomical sources while preserving details
and small faint sources.
4.3. Computational Cost Comparison
To demonstrate the effectiveness of the group-granularity
encoding in PolarRec, we conduct a comparative analysis
against the Neural Interferometry [31] method with point-
granularity encoding due to both methods are built upon
the Transformer architecture. We fix the number of visi-
bility points as 1660 and the size of the reconstructed visi-
bility map as 128 ×128, and compare the computational
efficiency of the encoders with batch size from 4 to 32.
The comparison focuses on two main metrics: the number
of Floating Point Operations (GFLOPs) and inference time
(Latency (ms)).
Table 2 shows the results of this comparison. The la-
tency of group granularity encoding in PolarRec is signifi-
cantly lower than point granularity encoding in Neural In-
terferometry [31]. Moreover, as the batch size increases,
the rise in latency for PolarRec is much smaller compared
to Neural Interferometry [31]. These results indicate thatTable 2. Comparative Analysis of Computational Efficiency
Method BatchSize Latency (ms) GFLOPs
4 29.53 (3.33) 44.02
Neural 8 59.02 (3.19) 88.04
Interferometry 16 115.62 (7.30) 176.08
32 232.04 (5.64) 352.16
4 3.72 (1.84) 3.05
PolarRec 8 3.89 (3.23) 6.10
Group Size = 32 16 4.20 (2.00) 12.20
32 5.37 (3.05) 24.40
4 3.47 (0.10) 2.39
PolarRec 8 3.90 (2.94) 4.78
Group Size = 64 16 4.02 (2.11) 9.57
32 5.11 (1.91) 19.14
PolarRec has greater advantages in handling large batches
of data, which has important implications for the processing
and analysis of massive astronomical observational data.
4.4. Effect of Group Size
In this experiment, we vary the group size in number of visi-
bility points and investigate its impact on the reconstruction
results. We also record the inference latency of the encod-
ing process, and use Floating Point Operations (FLOPs) to
measure the computation cost. LFD, PSNR and SSIM are
used to assess the reconstruction quality.
As illustrated in Table 3, there is a sharp drop in both
FLOPs and inference latency as the group size increases
from 1 to 16. In contrast, the image quality in SSIM and
PSNR is almost constant under most settings. Only the
PSNR value of MG dataset decreases slightly. Between a
group size of 32 to 128, there is a slight decrease in PSNR
and SSIM, implying that increasing group size beyond 32
Table 3. Effects of group size. The experiment compares PSNR
and SSIM metrics against GFLOPs and latency across different
group sizes for four datasets.
Group PSNR SSIMGFLOPs TimeSize MG IRSG UTSG EGB MG IRSG UTSG EGB (ms)
1 24.29 26.75 25.38 25.97 0.890 0.910 0.903 0.904 366.14 221.53
4 24.21 26.54 25.43 25.88 0.889 0.911 0.905 0.906 98.93 24.43
8 23.39 26.52 25.50 25.79 0.881 0.911 0.905 0.904 56.14 11.71
16 23.37 26.50 25.28 25.67 0.881 0.910 0.904 0.904 34.95 6.75
32 23.51 26.80 25.63 26.07 0.881 0.912 0.905 0.906 24.40 4.83
64 23.36 26.20 24.94 25.62 0.882 0.911 0.902 0.905 19.14 4.46
128 22.06 26.02 24.48 25.30 0.866 0.906 0.897 0.904 16.51 4.41
12847
might compromise the output quality. Moreover, when the
group size is set to 1, it is the same as encoding at the gran-
ularity of individual points. The results indicate that en-
coding with group granularity as input to the transformer
encoder is significantly more efficient than encoding at the
point granularity even when the group size is small.
4.5. Ablation Experiment
This ablation experiment aims to examine the significance
of two weight matrices w1, w2in the Radial Visibility Loss
by omitting them one at a time. The results in Table 4 and
Figure 7 show that the full Radial Visibility Loss is the best.
Removing either component w1, orw2results in reduced
performance across all metrics.
Table 4. Ablation study performance comparison (mean and stan-
dard deviation) on different datasets.
MG IRSG
LFD↓PSNR ↑SSIM ↑LFD↓PSNR ↑SSIM ↑
w/ow21.088 (0.309) 22.498 (2.578) 0.884 (0.030) 0.876 (0.305) 24.435 (2.577) 0.904 (0.026)
w/ow11.075 (0.317) 22.706 (2.797) 0.882 (0.032) 0.818 (0.283) 25.213 (2.641) 0.907 (0.027)
RVL 0.924 (0.284) 24.206 (2.547) 0.889 (0.029) 0.666 (0.238) 26.540 (2.653) 0.911 (0.025)
UTSG EGB
LFD↓PSNR ↑SSIM ↑LFD↓PSNR ↑SSIM ↑
w/ow20.908 (0.299) 23.477 (2.574) 0.897 (0.027) 0.933 (0.310) 23.793 (2.507) 0.897 (0.026)
w/ow10.854 (0.281) 24.393 (2.627) 0.901 (0.028) 0.864 (0.287) 24.569 (2.500) 0.901 (0.026)
RVL 0.707 (0.241) 25.430 (2.565) 0.905 (0.025) 0.707 (0.241) 25.880 (2.529) 0.906 (0.025)
Image        Imag         RealImage Domain       Visibility Domain
W/O 𝑤!W/O 𝑤"RVL         Real Sky  W/O 𝑤!W/O 𝑤"RVL         Real Sky  
Figure 7. Visual examples of ablation experiments.
4.6. Effect of Grouping Method
We also vary the grouping method and measure its perfor-
mance impact on PolarRec. We implemented three group-
ing strategies: (1) grouping by clustering visibility points
based on their positions, (2) grouping by the radial coordi-
nate, and (3) grouping by the angular coordinate. Each of
these strategies was tested under various group sizes on dif-
ferent datasets. The LFD and PSNR results are presented in
Figure 8. Regardless of the group size, grouping the visi-
bility points by angular coordinates always has the best per-
formance.
4.7. Test on Real-World Observations
We test our model trained on the Galaxy10 synthetic dataset
[13] with real observational data [9]. An example is shown
16 32 64
Group Size0.00.20.40.60.81.01.21.4LFDMG
Cluster
Radial
Angular
16 32 64
Group Size0.00.20.40.60.81.01.21.4LFDIRSG
Cluster
Radial
Angular
16 32 64
Group Size0.00.20.40.60.81.01.21.4LFDUTSG
Cluster
Radial
Angular
16 32 64
Group Size0.00.20.40.60.81.01.21.4LFDEGB
Cluster
Radial
Angular
16 32 64
Group Size202224262830PSNRMG
Cluster
Radial
Angular
16 32 64
Group Size202224262830PSNRIRSG
Cluster
Radial
Angular
16 32 64
Group Size202224262830PSNRUTSG
Cluster
Radial
Angular
16 32 64
Group Size202224262830PSNREGB
Cluster
Radial
AngularFigure 8. Comparison of grouping method over various group
sizes and different datasets.
in Figure 9. Our reconstructed image is of much higher
quality than the dirty image from the original sparse visibil-
ity data even though some artifacts remain. Our future work
will focus on improving the generalizability of our visibility
reconstruction methods.
Sparse VisibilityDirty ImageReconstructed VisibilityReconstructed Image
Figure 9. An Example of Reconstruction of Real Data
5. Conclusion and Future Work
We have presented PolarRec, reconstructing interferomet-
ric visibility with sample points represented in the polar co-
ordinate system. By adopting angular and radial coordi-
nates of visibility points, our method addresses two critical
challenges in visibility reconstruction: reconstruction qual-
ity and computational efficiency. Our results show that Po-
larRec markedly improves imaging outcomes while signifi-
cantly reducing the computation cost, demonstrating a new,
feasible, and extendable approach for cross-disciplinary
progress in computer vision and radio astronomy.
While PolarRec is designed for radio astronomical data
reconstruction, its underlying principles may find utility in
other domains requiring sophisticated frequency component
analysis and reconstruction, such as magnetic resonance
imaging or seismic imaging. Our proposed structural sparse
point encoding method may also be used in point cloud data
analysis. Future work could also explore these potential ap-
plications and generalize the work to other research fields.
Acknowledgements
Our work has been funded by NSFC General Research
Fund No. 62372393 and No. 12373097.
12848
References
[1] JG Ables. Maximum entropy spectral analysis. Astronomy
and Astrophysics Supplement, Vol. 15, p. 383 , 15:383, 1974.
1, 3
[2] Robert D Blum, Kaylan Burleigh, Arjun Dey, David J
Schlegel, Aaron M Meisner, Michael Levi, Adam D Myers,
Dustin Lang, John Moustakas, Anna Patej, et al. The decam
legacy survey. In American Astronomical Society Meeting
Abstracts# 228 , pages 317–01, 2016. 6
[3] Katherine L Bouman, Michael D Johnson, Daniel Zoran,
Vincent L Fish, Sheperd S Doeleman, and William T
Freeman. Computational imaging for vlbi image
reconstruction. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , pages 913–922,
2016. 1, 3
[4] Katherine L Bouman, Michael D Johnson, Adrian V Dalca,
Andrew A Chael, Freek Roelofs, Sheperd S Doeleman, and
William T Freeman. Reconstructing video of time-varying
sources from radio interferometric measurements. IEEE
Transactions on Computational Imaging , 4(4):512–527,
2018. 2
[5] Andrew A Chael, Michael D Johnson, Katherine L
Bouman, Lindy L Blackburn, Kazunori Akiyama, and
Ramesh Narayan. Interferometric imaging directly with
closure phases and closure amplitudes. The Astrophysical
Journal , 857(1):23, 2018. 6
[6] Andrew A Chael, Katherine L Bouman, Michael D Johnson,
Ramesh Narayan, Sheperd S Doeleman, John FC Wardle,
Lindy L Blackburn, Kazunori Akiyama, Maciek Wielgus,
Chi-kwan Chan, et al. ehtim: Imaging, analysis, and
simulation software for radio interferometry. Astrophysics
Source Code Library , pages ascl–1904, 2019. 6
[7] Shaoyu Chen, Xinggang Wang, Tianheng Cheng, Qian
Zhang, Chang Huang, and Wenyu Liu. Polar
parametrization for vision-based surround-view 3d
detection. arXiv preprint arXiv:2206.10965 , 2022. 2
[8] Aleksandra ´Ciprijanovi ´c, Ashia Lewis, Kevin Pedro,
Sandeep Madireddy, Brian Nord, Gabriel N Perdue, and
Stefan Wild. Semi-supervised domain adaptation for
cross-survey galaxy morphology classification and anomaly
detection. Machine Learning and the Physical Sciences
workshop, Advances in Neural Information Processing
Systems , 2022. 6
[9] The Event Horizon Telescope Collaboration. First
sagittarius a* eht results: Calibrated data, 2022. We release
a data set to accompany the First Sagittarius A* Event
Horizon Telescope Results paper series (EHT Collaboration
et al. 2022a,b,c,d,e,f). The data set is derived from the Rev7
Correlation of the Event Horizon Telescope (EHT)’s April
2017 observation campaign (EHT Collaboration et al.
2019c), with further processing and science validation as
described in EHT Collaboration et al. 2022b. It is made
public simultaneously with four imaging pipelines
(2022-D02-02, EHT Collaboration et al. 2022c). This data
set contains Sagittarius A* (Sgr A*) data for both low and
high bands for two observed days (April 6th and 7th, 2017).
Data from the 2017 observations were processed throughthree independent reduction pipelines (Blackburn et al.
2019, Janssen et al. 2019, M87 Paper III, and Sgr A* Paper
II, Paper III). This release includes the fringe fitted, a-priori
calibrated, and network calibrated data from both the
EHT-HOPS and rPICARD (CASA) pipelines, which are
used in the First Sgr A* EHT results. Independent flux
calibration is performed based on estimated station
sensitvities during the campaign (Issaoun et al. 2017,
Janssen et al. 2019, Wielgus et al. 2022). A description of
the data properties, their validation, and estimated
systematic errors is given in M87 Paper III and Sgr A*
Paper II and Paper III with additional details in Wielgus et
al. (2019) and Wielgus et al. (2022). The data are time
averaged to 10 seconds and frequency averaged over all 32
intermediate frequencies (IFs). All polarization information
is explicitly removed. To make the resulting uvfits files
compatible with popular very-long-baseline interferometry
(VLBI) software packages, the circularly polarized
cross-hand visibilities RL and LR are set to zero along with
their errors, while parallel-hands RR and LL are both set to
an estimated Stokes I value. Measurement errors for RR and
LL are each set to sqrt(2) times the statistical errors for
Stokes I. 8
[10] Liam Connor, Katherine L Bouman, Vikram Ravi, and
Gregg Hallinan. Deep radio-interferometric imaging with
polish: Dsa-2000 and weak lensing. Monthly Notices of the
Royal Astronomical Society , 514(2):2614–2626, 2022. 1, 3
[11] Arjun Dey, David J Schlegel, Dustin Lang, Robert Blum,
Kaylan Burleigh, Xiaohui Fan, Joseph R Findlay, Doug
Finkbeiner, David Herrera, St ´ephanie Juneau, et al.
Overview of the desi legacy imaging surveys. The
Astronomical Journal , 157(5):168, 2019. 6
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold,
Sylvain Gelly, et al. An image is worth 16x16 words:
Transformers for image recognition at scale. In Proceeding
of the International Conference on Learning
Representations , 2021. 2, 5
[13] Leung Henry. Galaxy10 decals dataset.
https://github.com/henrysky/Galaxy10, 2021. 6, 8
[14] JA H ¨ogbom. Aperture synthesis with a non-regular
distribution of interferometer baselines. Astronomy and
Astrophysics Supplement Series , 15:417, 1974. 1, 3, 6, 7
[15] Alain Hore and Djemel Ziou. Image quality metrics: Psnr
vs. ssim. In 2010 20th international conference on pattern
recognition , pages 2366–2369. IEEE, 2010. 6
[16] Roger A Horn. The hadamard product. In Proc. Symp.
Appl. Math , pages 87–169, 1990. 5
[17] Liming Jiang, Bo Dai, Wayne Wu, and Chen Change Loy.
Focal frequency loss for image reconstruction and synthesis.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 13919–13929, 2021. 3, 4, 6
[18] Honghao Liu, Qiong Luo, and Feng Wang. Efficient radio
interferometric imaging on the gpu. In 2022 IEEE 18th
International Conference on e-Science (e-Science) , pages
95–104. IEEE, 2022. 3
12849
[19] Ming Nie, Yujing Xue, Chunwei Wang, Chaoqiang Ye,
Hang Xu, Xinge Zhu, Qingqiu Huang, Michael Bi Mi,
Xinchao Wang, and Li Zhang. Partner: Level up the polar
representation for lidar 3d object detection. In Proceedings
of the IEEE/CVF International Conference on Computer
Vision , pages 3801–3813, 2023. 2
[20] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An
imperative style, high-performance deep learning library.
Advances in neural information processing systems , 32,
2019. 6
[21] Ethan Perez, Florian Strub, Harm De Vries, Vincent
Dumoulin, and Aaron Courville. Film: Visual reasoning
with a general conditioning layer. In Proceedings of the
AAAI Conference on Artificial Intelligence , 2018. 5
[22] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
U-net: Convolutional networks for biomedical image
segmentation. In Medical Image Computing and
Computer-Assisted Intervention–MICCAI 2015: 18th
International Conference, Munich, Germany, October 5-9,
2015, Proceedings, Part III 18 , pages 234–241. Springer,
2015. 2, 6
[23] Kevin Schmidt, Felix Geyer, Stefan Fr ¨ose, Paul-Simon
Blomenkamp, Marcus Br ¨uggen, Francesco de Gasperin,
Dominik Els ¨asser, and Wolfgang Rhode. Deep
learning-based imaging in radio interferometry. arXiv
preprint arXiv:2203.11757 , 2022. 1, 2, 3, 6, 7
[24] David R Silva, Robert D Blum, Lori Allen, Arjun Dey,
David J Schlegel, Dustin Lang, John Moustakas, Aaron M
Meisner, Francisco Valdes, Anna Patej, et al. The mayall
z-band legacy survey. In American Astronomical Society
Meeting Abstracts# 228 , pages 317–02, 2016. 6
[25] Himanshu Singh and Himanshu Singh. Basics of python
and scikit image. Practical Machine Learning and Image
Processing: For Facial Recognition, Object Detection, and
Pattern Recognition Using Python , pages 29–61, 2019. 6
[26] He Sun and Katherine L Bouman. Deep probabilistic
imaging: Uncertainty quantification and multi-modal
solution characterization for computational imaging. In
Proceedings of the AAAI Conference on Artificial
Intelligence , pages 2628–2637, 2021. 1, 3
[27] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara
Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi
Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier
features let networks learn high frequency functions in low
dimensional domains. Advances in Neural Information
Processing Systems , 33:7537–7547, 2020. 5
[28] A Richard Thompson, James M Moran, and George W
Swenson. Interferometry and synthesis in radio astronomy .
Springer Nature, 2017. 2, 3
[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. Advances in
neural information processing systems , 30, 2017. 2, 5
[30] Ruoqi Wang, Zhuoyang Chen, Qiong Luo, and Feng Wang.
A conditional denoising diffusion probabilistic model forradio interferometric image reconstruction. In 26th
European Conference on Artificial Intelligence , pages
2499–2506, 2023. 1, 3
[31] Benjamin Wu, Chao Liu, Benjamin Eckart, and Jan Kautz.
Neural interferometry: Image reconstruction from
astronomical interferometers using transformer-conditioned
neural fields. In Proceedings of the AAAI Conference on
Artificial Intelligence , 2022. 1, 2, 3, 5, 6, 7
[32] Enze Xie, Peize Sun, Xiaoge Song, Wenhai Wang, Xuebo
Liu, Ding Liang, Chunhua Shen, and Ping Luo. Polarmask:
Single shot instance segmentation with polar representation.
InProceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pages 12193–12202, 2020. 2
[33] Enze Xie, Wenhai Wang, Mingyu Ding, Ruimao Zhang, and
Ping Luo. Polarmask++: Enhanced polar representation for
single-shot instance segmentation and beyond. IEEE
Transactions on Pattern Analysis and Machine Intelligence ,
44(9):5385–5400, 2021. 2
[34] Yutong Xie and Quanzheng Li. Measurement-conditioned
denoising diffusion probabilistic model for under-sampled
medical image reconstruction. In Medical Image
Computing and Computer Assisted Intervention–MICCAI
2022: 25th International Conference, Singapore, September
18–22, 2022, Proceedings, Part VI , pages 655–664.
Springer, 2022. 6, 7
[35] Hu Zou, Xu Zhou, Xiaohui Fan, Tianmeng Zhang, Zhimin
Zhou, Jundan Nie, Xiyan Peng, Ian McGreer, Linhua Jiang,
Arjun Dey, et al. Project overview of the beijing–arizona
sky survey. Publications of the Astronomical Society of the
Pacific , 129(976):064101, 2017. 6
12850
