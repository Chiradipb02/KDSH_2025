Task-Customized Mixture of Adapters for General Image Fusion
Pengfei Zhu, Yang Sun, Bing Cao*, Qinghua Hu
Tianjin Key Lab of Machine Learning, College of Intelligence and Computing, Tianjin University, China
{zhupengfei,yangsun,caobing,huqinghua }@tju.edu.cn
Abstract
General image fusion aims at integrating important in-
formation from multi-source images. However, due to the
significant cross-task gap, the respective fusion mechanism
varies considerably in practice, resulting in limited perfor-
mance across subtasks. To handle this problem, we pro-
pose a novel task-customized mixture of adapters (TC-MoA)
for general image fusion, adaptively prompting various fu-
sion tasks in a unified model. We borrow the insight from
the mixture of experts (MoE), taking the experts as effi-
cient tuning adapters to prompt a pre-trained foundation
model. These adapters are shared across different tasks and
constrained by mutual information regularization, ensur-
ing compatibility with different tasks while complementarity
for multi-source images. The task-specific routing networks
customize these adapters to extract task-specific informa-
tion from different sources with dynamic dominant inten-
sity, performing adaptive visual feature prompt fusion. No-
tably, our TC-MoA controls the dominant intensity bias for
different fusion tasks, successfully unifying multiple fusion
tasks in a single model. Extensive experiments show that
TC-MoA outperforms the competing approaches in learn-
ing commonalities while retaining compatibility for gen-
eral image fusion (multi-modal, multi-exposure, and multi-
focus), and also demonstrating striking controllability on
more generalization experiments. The code is available at
https://github.com/YangSun22/TC-MoA .
1. Introduction
Image fusion aims to integrate complementary informa-
tion from multi-source images captured by different sen-
sors in the same scene onto a single image. It is usu-
ally used to enhance important information and visual qual-
ity [14]. Currently, general image fusion mainly includes
multi-modal, multi-exposure, and multi-focus image fusion,
etc. Fusion tasks exhibit diverse fusion mechanisms. The
Multi-Exposure image Fusion (MEF) focuses on integrat-
*Corresponding author.
Value of ùëÉùëüùëúùëöùëùùë°
Source image X Source image Y Fusion Image ùëÉùëüùëúùëöùëùùë° ùë¶Mulit -Exposure Visible -infrared Mulit -Focus
1.0 0.0ùëÉùëüùëúùëöùëùùë° ùë•
MFF MFF MEF MEF VIF VIFFigure 1. Prompt can adaptively select the complementary infor-
mation from multi-source features. The dominant intensity bias
vary according to the task, which is reflected by the different
shades of colors.
ing images with multiple exposure levels into a high-quality
full exposure image [40]. Each source image contributes
its own illumination and structural information to the fused
image [28]. The Visible-Infrared image Fusion (VIF) is a
subfield of the Multi-Modal Fusion (MMF) [8, 16, 48] that
aims at fusing complementary information of infrared and
visible modalities to produce robust and informative fused
images [25]. Infrared images provide more intensity infor-
mation, while visible images offer more texture and gra-
dient information. The Multi-Focus image Fusion (MFF)
aims at generating an all-in-focus image from a series of
partially focused images [23]. A patch of multi-focus fused
image typically corresponds to only one source image. Con-
sequently, it can be observed that the MEF and VIF tasks in-
volve relatively balanced fusion, while MFF is an extremely
unbalanced task that tends to exhibit polarized choices.
With the rapid development of deep learning tech-
niques, image fusion has achieved great progress in recent
years [31, 41], while most existing methods [28, 54, 55]
focus on single image fusion scenario alone. Task-specific
methods [28, 36] often employ task-specific strategies such
as designing complex task-biased networks or utilizing
task-specific fusion loss functions, resulting in the weak
generalization to other tasks [41]. Considering that different
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
7099
fusion tasks have similar objectives, i.e., integrating com-
plementary information from multiple source images, some
recently proposed approaches [20, 41, 51] attempted to con-
duct various fusion tasks using a unified model, forming
general image fusion. These methods, however, are either
enmeshed in dominant-task bias [51] or multi-task com-
monality [20, 41] at the expense of individuality, leading
to unsatisfactory performance. This motivates us to explore
a more compatible fusion paradigm that is adaptively and
dynamically compatible with different fusion scenarios.
To handle this challenge, inspired by the impressive rep-
resentation capability of pre-trained foundation models, we
introduce the foundation model as a fixed encoder to extract
complementary features for multi-source images. Different
from most existing methods, we borrow insight from the
mixture of experts (MoE) [24, 33], taking each expert as
an efficient tuning adapter to perform adaptive visual fea-
ture prompt fusion based on the foundation model. The
task-specific routing networks customize these adapters to
generate task-specific fusion prompt for different sources,
forming a novel task-customized mixture of adapters (TC-
MoA) architecture. An mutual information regularization is
further developed to constrain the prompt, which guarantees
complementarity for diverse sources. It is worth noting that
the prompt have significant task bias and dominant intensity
gaps. As shown in Fig. 1, the prompt of MFF has a greater
color difference than VIF and MEF, meaning that feature
selection has more bipolarity on dominant intensity bias.
Our model effectively perceives the fusion intensity bias
among varied fusion tasks in a single model and is there-
fore compatible with a wider range of fusion tasks. Exten-
sive experiments verify our superiority over the competing
methods in general image fusion, including multi-modal,
multi-exposure, and multi-focus fusion. More importantly,
our TC-MoA even shows unprecedented controllability and
generalizability for unseen fusion tasks, fully demonstrat-
ing our potential in broader scenarios. We summarize our
main contributions as follows:
‚Ä¢ We propose a unified general image fusion model, pro-
viding a new task-customized mixture of adapters (TC-
MoA) for adaptive multi-source image fusion (benefiting
from the dynamically aggregating effective information
from the respective modalities).
‚Ä¢ We propose a mutual information regularization for
adapters, which allows our model to more accurately
identify the dominant intensity of different source images.
‚Ä¢ To the best of our knowledge, we for the first time pro-
pose an MoE-based flexible adapter for the foundation
model in general image fusion. By only adding 2.8%
of learnable parameters, our model copes with numerous
fusion tasks. Extensive experiments demonstrate our su-
periority against the competing methods, while showing
significant controllability and generalizability.2. Related Work
Image Fusion. Image fusion focuses on generating a fused
image containing complementary information from differ-
ent source images. Some early methods [16, 22, 31] tackled
their respective task by leveraging CNN. Then, GAN-based
[43, 46] and Transformer-based methods [13, 30, 36] have
been proposed to improve the fusion quality. Furthermore,
high-level tasks [3, 21, 34] are also introduced to guide the
fusion of images. Feature decomposition [52, 53] for high-
low frequency has also gained significant attention as a re-
search direction. Different from these methods, some most
recent methods focus on general image fusion that aims to
address multiple fusion tasks in a single model. Zhang et al.
[51] performed a supervised training framework for MFF,
which was generalized to other tasks by adjusting fusion
conditions. Xu et al. [41, 42] appraised the quantity and
quality of information from source images or features with
the image quality assessment (IQA) to decide the fusion
paradigm. PMGI [45] and SwinFusion [26] incorporated
a unified fusion framework and loss function, and separate
model training for individual tasks. Liang et al. [20] per-
formed a self-supervised fusion framework to learn fusion
commonality while ignoring task-specific individuality. In
this work, we accommodate diverse fusion tasks by dynam-
ically customizing the mixture of adapters, rather than suf-
fering individuality for cross-task commonality.
Parameter-Efficient Fine-Tuning. To efficiently adapt
pre-trained models to the respective downstream applica-
tion tasks, some Parameter-Efficient Fine-Tuning (PEFT)
studies have been proposed. PEFT can be mainly divided
into the Adapter [10, 19, 32] and the Prefix Tuning [11, 18].
Jia et al. [11] introduced prompt tuning to the ViT structure.
Chen et al. [4] proposed a new adapter by scaling the origi-
nal features instead of summing. Lian et al. [19] bridged the
gap between the pre-trained features and the downstream
task features by linearly varying the original features. In
this paper, different from these methods that focus on tuning
high-level tasks, we for the first time generalize the power-
ful foundational model to general image fusion.
Mixture of Experts. Shazeer et al. [33] first proposed the
mixture of experts (MoE) to increase model capacity with-
out increasing computational complexity. Based on this,
Lepikhin et al. [15] and Fedus et al. [7] integrated MoE
with the transformer structure, further pushing the upper
limit of network capacity. In addition, MoE has been vali-
dated as effective in other challenges. Ma et al. [24] tack-
led the multi-task problem by designing multi-gates MoE.
Mustafa et al. [29] exploited MoE to train the contrastive
learning-based vision-language foundation model. Inspired
by this, we take each expert as an adapter, forming a task-
customized mixture of adapters for tuning our general im-
age fusion framework.
7100
TC-MoA
Transformer
Block
Block
Patch EmbeddingBlockBlock
ùëø
ùíÄ. .
TC-MoA ùúèViT EncoderBlockBlock
. . . .
Transformer
BlockBlock
Patch EmbeddingBlockBlock
. .BlockBlock
. . . .
Fusion Layer  (‚Ñ±)  ùê¥1
ùê¥2
ùê¥3
ùê¥4Adapter
Bank
ùê∫ùëâùê∫ùê∏ùê∫ùêπRouter Bank
0.23
0.28
0.30
0.190.17
0.32
0.27
0.24
0.32
0.13
0.19
0.36
0.25TC-MoA
TC-MoA
ùëùùëüùëúùëöùëùùë°
ùëÜ
VIF tokens MEF tokens MFF tokens Source -specific ùëÜ
Block
BlockBlockViT Decoder
Block
Figure 2. An overview of our proposed TC-MoA method. Our approach gradually modulates the fusion results by inserting TC-MoA into
the frozen ViT backbone. TC-MoA generates task-specific prompt through a task-specific router bank and an shared adapter bank. The
fusion layer utilizes prompt as scale and source-specific embeddings as biases to obtain fusion images.
3. Method
3.1. Overview
In this paper, we propose a task-customized mixture
of adapters for general image fusion, which is a novel
parameter-efficient fine-tuning method to jointly and adap-
tively prompt various fusion tasks. Given a pair of source
images
(X, Y)|X, Y‚ààRH√óW√ó3	
, the network inte-
grates complementary information from different sources
to obtain the fused image IFusion ‚ààRH√óW√ó3. As shown
in Fig. 2, we feed the source images into the ViT network
and obtain tokens of the source images through the patch
embedding layer. ViT consists of an encoder for feature
extraction and a decoder for image reconstruction, both of
which are composed of transformer blocks. A TC-MoA is
inserted into each œÑ(œÑ= 4) transformer blocks in both
the encoder and decoder. Each TC-MoA consists of a task-
specific router bank, a task-shared adapter bank, and a fu-
sion layer. Our network gradually modulates the fusion re-
sults through these TC-MoA.
3.2. Task-Customized Mixture of Adapters
As shown in Fig. 2 , each TC-MoA consists of a task-
specific router bank
GV, GE, GF	
, a task-shared adapter
bank{A1, . . . , A N}and a prompting fusion layer F. The
TC-MoA includes two main stages: prompt generation and
prompt-driven fusion. For ease of expression, we take the
VIF as an example, assuming that the input comes from the
VIF dataset and uses Gto denote GV.
Prompt Generation. First, we obtain multi-source fea-
tures for subsequent processing. The network structure be-
fore the j-th TC-MoA is defined as Ej(¬∑), and the extracted
features are defined as fxandfy, where f‚ààRpH√ópW√óC.We concatenate fxandfyas a feature representation of
pairs of multi-source tokens. This allows tokens from
different sources to exchange information in the subse-
quent network. However, directly computing on the high-
dimensional concatenated features will bring amounts of
unnecessary parameters. Therefore, we use L(¬∑)to per-
form feature dimension reduction and obtain the processed
multiple-source features Œ¶as follows,
fx=Ej(X), fy=Ej(Y) (1)
Œ¶ =L(Cat(fx, fy)) (2)
where Cat(¬∑)represents the concatenation of features, L(¬∑)
consists of a linear layer and normalization layers. Next,
depending on the task to which Œ¶belongs, we select a task-
specific router from the router bank to customize the routing
schemes, i.e., which adapters in the adapter bank should be
input for each token pair.
G(x) =Softmax (TopK (x¬∑Wg+
N(0,1)¬∑Softplus (x¬∑Wnoise))) (3)
where TopK (¬∑)keeps only the top K(K= 2) values, set-
ting the rest to ‚àí‚àû (after Softmax (¬∑), the value becomes
0 ).WgandWnoise are learnable parameters. The cus-
tomization routing schemes vary for tasks, as evidenced by
the proportion of the number of times that different adapters
have been routed in Fig. 2 (b). After that , we weight sum
the output from the adapters to obtain the prompt. Each
router masters a task-biased appetite for customizing a suit-
able mixture of adapters, and each adapter generates the
prompt. The task-customized multi-source prompt is then
7101
calculated as follows,
prompt =GAP (Sigmoid (NX
i=1G(Œ¶)i¬∑Ai(Œ¶)) (4)
where GAP (¬∑)represents the global average pooling, iis
the index of adapters and G(¬∑)iis the routing value for
thei-th adapter. The task-customized prompt, denoted
asprompt ‚ààRpH√ópW√ó2, is composed of prompt xand
prompt y‚ààRpH√ópW√ó1and has a value range of (0,1).
Prompt-Driven Fusion. The task-customized prompt is
constrained by mutual information regularization (MIR),
which guarantees complementarity for diverse sources.
Thus the prompt can be taken as an evaluation of the pro-
portion of important information in each source. By dot-
multiplication of multi-source features and prompt, we re-
tain complementary information while removing redundant
information. Afterward, considering that the feature repre-
sentation should contain a source-correlated bias (e.g. vis-
ible or infrared images), we introduce input-independent
learnable parameters for each source, i.e., source embed-
dingS. After feature prompting and shifting, we obtain the
refined source features (hx, hy)as follows,
hx=prompt x¬∑fx+Sx
hy=prompt y¬∑fy+Sy
fTC‚àíMoA =F(hx+hy)(5)
where SxandSyrepresent the source embeddings of vis-
ible and infrared images in VIF respectively. F(¬∑)fuses
the refined multi-source features using an additive opera-
tion and then passes them through a set of convolutional
layers. These layers introduce local receptive fields to re-
duce the checkerboard artifacts and align the solution space
of the subsequent transformer blocks. Ultimately, we ob-
tain a fusion feature through the task-customized prompt.
To encourage the model to extract important information
gradually, the features input into the next transformer block
are processed as follows,
fx‚Ä≤=Œªffx+ (1‚àíŒªf)fTC‚àíMoA
fy‚Ä≤=Œªffy+ (1‚àíŒªf)fTC‚àíMoA(6)
where Œªfis a learnable parameter initialized to 0.5.
Mutual Information Regularization. In order to ensure
that the model dynamically retains complementary infor-
mation while discarding redundant information of multi-
source features, we impose regularization constraints on the
prompt. Assuming that the information representation of
features varies linearly, we define the MIR as follows,
min|prompt x+prompt y‚àí1| (7)
The MIR allows the model to accurately identify the dom-
inant intensity of different sources, which is positively cor-
related with the information content of the sources.3.3. Task-Customized Loss Function
In addition to accommodating the individuality of differ-
ent tasks in our network structure, we also customize unsu-
pervised loss functions for each fusion task. We add Laux
to the loss function of each task to ensure the training of
TC-MoA and Lauxis the auxiliary loss in [33] to avoid
adapters learning unbalanced. On the other hand, in order to
generate high-quality fusion images, we impose constraints
on the structural information ( Lssim), intensity information
(LPixel ) for VIF loss function, and gradient information
(LGrad ) of the fusion images for different fusion tasks.
For the VIF task, our objective is to retain the most
pronounced high-frequency and low-frequency information
from the source images in the fused image. Thus, we design
theLMaxPixel andLMaxGrad . To avoid confusing gradi-
ents, we retain the sign of the gradient values in all loss
functions related to gradient information. For the MEF task,
we consider that the luminance of the fused images should
be at an average level with all the gradient information.
Thus we design the loss functions for MEF with LAvgPixel
andLMaxGrad . Additionally, we adopt Lmefssim [28]
which is specially designed for the MEF task, instead of
the SSIM loss function. For the MFF task, we believe that
each patch of the fused image should only depend on a sin-
gle source image with the maximum gradient. This is to
prevent the objects‚Äô edges in defocused images from being
preserved, thereby affecting the quality of the fused image.
For this purpose, we select only one of the source images
to compute the loss function for each patch in the image,
i.e.LMaskPixel andLMaskGrad . Please refer to the supple-
mentary material for details of the loss functions.
4. Experiments
4.1. Experimental Setting
Datasets. We conduct experiments in three image fusion
scenarios: visible-infrared, multi-focus, and multi-exposure
fusion. For VIF, we evaluate our model on the LLVIP [12]
dataset. The training set contains 12025 image pairs, and
we randomly select 70 samples out of the test set for evalu-
ation. For MEF, we employ the SCIE [2] dataset (589 pairs)
for training and MEFB [49] dataset (100 pairs) for test-
ing, we utilize the most underexposed and overexposed im-
ages from sequences in SCIE as inputs. For MFF, we train
our model on the RealMFF [47] and the MFI-WHU [46]
datasets and follow the test setting in MFIF [50].
Implementation Details. Our experiments are performed
on a server with 8 √óNVIDIA RTX A6000 GPUs. Al-
though our model supports inputs of arbitrary size, we en-
sure that each fusion task receives an equal amount of data
per iteration by randomly cropping all samples to a size of
448√ó448. We employ the pre-trained MAE-large model [9]
with GAN loss as our backbone. We train 20 epochs jointly
7102
TC-MoA
CDDFuse
 DDFM
DeFusion
Visible
 Infrared
IFCNN
 SDNet
U2Fusion
DenseFuse
 TarDAL
TC‚àíMoAùëåùëàùëâFigure 3. Qualitative comparisons of various methods in VIF task.
with all the fusion tasks, and the batch size for each fusion
task is set to 3. We adopt the AdamW optimizer with the
initial learning rate of 1.5√ó10‚àí4. To ensure training sta-
bility, we apply the Exponential Moving Average (EMA) to
routers and adapters optimization. Each TC-MoA consists
ofN(N= 4) adapters embedded in different layers, but
only the top K(K= 2) adapters are activated.
Evaluation Metrics. Since the commonly used evaluation
metrics for fusion tasks are not exactly the same, we cus-
tomize a set of metrics for each task. Existing metrics can
be mainly divided into the following four categories [40]. ‚ãÑ
The information theory-based metrics: entropy (EN), peak
signal-to-noise ratio (PSNR), mutual information (MI), nor-
malized MI (NMI), Nonlinear correlation information en-
tropy ( Qncie), feature mutual information (FMI). ‚ãÑThe im-
age feature-based metrics: phase congruency-based metric
Qp, standard deviation (SD), gradient-based metric Qabf,
andQg.‚ãÑThe structural similarity-based metrics: Qc[5],
Qw,Qs[38], structural similarity (SSIM), multiscale struc-
tural similarity (MS-SSIM [37]) and MEF-SSIM [27]. ‚ãÑ
The human perception inspired metrics: visual information
fidelity ( V IF ),QcbandQcv. Detailed information on met-
rics can be found in [25] and [23].
4.2. Evaluation on Multi-Modal Fusion
In this section, we compare our TC-MoA with ten task-
specific image fusion methods and four general image fu-
sion methods on the LLVIP dataset.
Quantitative Comparisons . We evaluate the fusion per-
formance on 8 quantitative metrics, as shown in Table 1.
Our method outperforms all the general image fusion meth-
ods, demonstrating our superior compatibility across multi-Table 1. Quantitative results of the VIF task in LLVIP dataset.
Boldface in red andboldface in blue show the best and second-
best values, respectively. The underline represents the best value
in the general image fusion methods.
Method V IF Q c EN SD Qcv‚ÜìMS-SSIM FMI Qw
Densefuse [31] 0.545 0.533 6.830 9.381 817.213 0.878 0.876 0.622
AUIF [53] 0.402 0.370 6.137 7.769 1087.569 0.784 0.869 0.530
DIDFuse [52] 0.366 0.348 5.991 7.765 897.007 0.767 0.863 0.479
TarDAL [21] 0.550 0.562 7.347 9.609 549.177 0.864 0.860 0.628
YDTR [36] 0.486 0.524 6.638 8.810 883.333 0.835 0.876 0.614
RFN-Nest [17] 0.497 0.456 7.052 9.609 857.157 0.862 0.871 0.385
SwinFuse [39] 0.399 0.321 5.878 7.457 1306.652 0.734 0.870 0.425
UMF-CMGR [6] 0.414 0.479 6.607 8.520 810.670 0.801 0.876 0.561
DDFM [55] 0.588 0.561 7.069 9.696 760.006 0.908 0.878 0.663
CDDFuse [54] 0.694 0.645 7.342 9.733 495.473 0.933 0.883 0.830
SDNet [44] 0.527 0.575 6.897 9.318 936.389 0.878 0.872 0.749
IFCNN [51] 0.679 0.634 7.223 9.662 521.741 0.946 0.882 0.856
U2Fusion [41] 0.503 0.492 6.647 8.789 857.455 0.878 0.870 0.695
DeFusion [20] 0.606 0.606 7.216 9.701 532.092 0.890 0.880 0.676
TC-MoA 0.726 0.637 7.428 9.805 423.773 0.949 0.886 0.858
Table 2. Quantitative results of the MEF task in MEFB [49] .
Method Psnr QcQpQgQsMEF-SSIM FMI Qw
Deepfuse [31] 57.104 0.426 0.352 0.325 0.641 0.897 0.873 0.548
MEF-GAN [43] 56.947 0.309 0.124 0.246 0.487 0.772 0.846 0.300
MEFNet [28] 56.594 0.656 0.595 0.565 0.838 0.914 0.890 0.866
IFCNN [51] 57.195 0.553 0.562 0.478 0.720 0.943 0.882 0.834
FusionDN [42] 56.977 0.500 0.504 0.434 0.672 0.924 0.877 0.776
PMGI [45] 57.117 0.489 0.525 0.442 0.666 0.936 0.885 0.804
U2Fusion [41] 57.055 0.457 0.505 0.415 0.585 0.930 0.882 0.787
DeFusion [20] 57.131 0.539 0.378 0.376 0.751 0.902 0.877 0.733
TC-MoA 57.213 0.578 0.598 0.528 0.767 0.964 0.888 0.845
ple fusion tasks. For most task-specific image fusion meth-
ods, TC-MoA also performs remarkable improvements, al-
though these methods deploy complex task-specific de-
signs. Specifically, our method achieves a significant ad-
vantage in VIF, Qcv, and EN metrics correlated to human
perception or information theory. This indicates that our
fused image conforms more to human visual perception and
contains more information from the source images. The re-
sults on MS-SSIM, SD, and Qwmetrics also explain that
our fusion results possess a sufficient amount of structural
information and gradient information.
Qualitative Comparisons. As shown in Fig. 3, our model
outperforms the competing methods in terms of visual qual-
ity. For instance, the contours of the tree in dark areas are
well depicted and the background texture is clearer. It is
worth noting that our model has the capability to directly
generate color images, or we can also follow other ap-
proaches to conduct on gray images and colorize the results
to obtain TC-MoAY UV. Compared with the competing
methods, our model exhibits higher contrast between fore-
ground and background, and more saturated colors. This
comparison demonstrates our model is effective in generat-
ing fused images with superior perceptual quality.
4.3. Evaluation on Multi-Exposure Fusion
In this section, we compare TC-MoA with three task-
specific MEF methods i.e. Deepfuse[31], MEF-GAN[43]
and MEF-Net [28], and five general image fusion meth-
7103
TC-MoAMEFNet IFCNN
DeFusionover-exposure under -exposure
U2Fusion FusionDN
PMGIDeepFuse MEF -GAN
TC-MoA  Light
Figure 4. Qualitative comparisons in MEF task.
odsi.e. IFCNN [51], FusionDN [42], PMGI [45], U2Fusion
[41], and DeFusion [20].
Quantitative Comparisons. We employ 8 quantitative
metrics to evaluate our model and the competing methods
on MEFB, as presented in Table 2. Our model achieves the
SoTA performance in the general image fusion methods and
achieves competitive results in task-specific methods. For
example, our model significantly improves the MEF-SSIM
scores due to the compatibility of TC-MoA with diverse
tasks, enabling task-specific optimization while reducing
task conflicts. The MEF-SSIM focuses on the structure and
contrast distortion of images, and Qpmeasures the phase
congruency of the source and fused images. The highest
MEF-SSIM, PSNR, and Qpvalues indicate our effective-
ness in the preservation of structural information, detailed
features, and image quality in the fused images.
Qualitative Comparisons. As shown in Fig. 4, our method
preserves the most texture details and color in both high and
low illumination regions. Specifically, in the high illumina-
tion region, our model effectively retains more structural
information of the clouds around the sun. In the low illumi-
nation region, the colors of the fonts on the hot air balloon
are completely confused in the PMGI and U2Fusion meth-
ods. As a comparison, our method maintains detailed infor-
mation while keeping more accurate color information. In
fact, there is no objective standard for the brightness of the
fused image for multi-exposure images. To solve this prob-
lem, our method makes the fusion of images controllable
by modulating the prompt. The image of ‚ÄúTC-MoA Light‚Äù
shows the result of this modulation. Detailed information
about the modulation can be found in Sec. 4.6.
4.4. Evaluation on Multi-Focus Fusion
We compare TC-MoA with two task-specific MFF methods
i.e. IFCNN [51] (supervised on MFF) and MFF-GAN [46],
and four general image fusion methods i.e. FusionDN [42],
DeFusion U2FusionSource 1 Source 2
PMGIIFCNN MFF -GAN
TC-MoA
Figure 5. Qualitative comparisons in MFF task.
Table 3. Quantitative results of the MFF task.
Method NMI Qncie MI QcbQcv‚ÜìMS-SSIM FMI Qw
IFCNN [51] 0.847 0.825 6.495 0.691 44.373 0.991 0.881 0.912
MFF-GAN [46] 0.728 0.820 5.689 0.609 72.460 0.977 0.876 0.877
FusionDN [42] 0.675 0.818 5.449 0.519 178.491 0.942 0.864 0.794
PMGI [45] 0.702 0.819 5.522 0.542 140.957 0.923 0.866 0.575
U2Fusion [41] 0.670 0.818 5.329 0.530 142.325 0.948 0.867 0.830
DeFusion [20] 0.768 0.821 5.838 0.625 83.195 0.960 0.870 0.685
TC-MoA 0.875 0.827 6.695 0.718 36.512 0.990 0.881 0.891
PMGI [45], U2Fusion [41], and DeFusion [20].
Quantitative Comparisons. We employ 8 quantitative
metrics on the dataset provided in the [50], as illustrated in
Table 3. Our method shows competitive performance com-
pared to most image fusion methods. For general image
fusion, our model achieves superior performance across all
metrics. Additionally, TC-MoA directly generates the fused
image without predicting the decision map. Nevertheless,
we still significantly outperform IFCNN in NMI, MI, Qcb,
and Qcv metrics. This indicates that our fusion results, com-
pared to supervised method, retain more source image de-
tails and demonstrate remarkable human visual perception.
Qualitative Comparisons. As depicted in Fig. 5, our fused
image exhibits superior consistency in terms of texture and
color, surpassing that of the other methods. U2Fusion ex-
hibits color deviations in the far-focus region, while IFCNN
distorts the near-focus image. Furthermore, these methods
are unable to effectively remove the distortion around the
flowers, while we introduce an MFF-specific loss, which
achieves the most visually appealing results for the flower.
4.5. Ablation Study
We conduct ablation studies to verify the effect of our TC-
MoA and network architecture.
TC-MoA. To verify the effectiveness of TC-MoA, we re-
move it from our framework. As shown in the first and
second rows of Table 4, the models trained by multi-task
training outperform those trained for a single task, based
on 5 commonly used metrics. Interestingly though, this
7104
Table 4. Ablation studies on TC-MoA. S and M represent single
task training and multi-task training, while SA and MA denote
single adapter and multiple adapters respectively.
Task S M SA MA Qabf Qp FMI Qc SSIM
‚úì ‚úì 0.5984 0.4073 0.8857 0.6335 0.4540
‚úì ‚úì 0.5997 0.4116 0.8862 0.6357 0.4544 VIF
‚úì ‚úì 0.6007 0.4119 0.8862 0.6365 0.4550
‚úì ‚úì 0.6385 0.5916 0.8875 0.5754 0.4115
‚úì ‚úì 0.6362 0.5899 0.8875 0.5765 0.4106 MEF
‚úì ‚úì 0.6449 0.5980 0.8883 0.5776 0.4116
‚úì ‚úì 0.6517 0.6733 0.8797 0.7702 0.6774
‚úì ‚úì 0.6562 0.6799 0.8806 0.7742 0.6792 MFF
‚úì ‚úì 0.6568 0.6811 0.8808 0.7755 0.6794
rule reverses in the case of MEF, indicating that inter-task
competition occurs for models trained directly with multi-
ple tasks. Comparatively, the model incorporating the mul-
tiple adapters improves the performance across all metrics,
suggesting that TC-MoA is dynamically compatible with
different tasks.
Adapter-based Fine-tuning. We performed ablation ex-
periments on the backbone and adapter methods, as shown
in Table 5. The fine-tuning method of ‚ÄúFrozenBackbone‚Äù
represents freezing the entire backbone and only unfreezing
the final linear layer. Compared to the approach of ‚ÄúFrozen-
Backbone‚Äù, our TC-MoA achieves significant fusion per-
formance improvements by introducing a small number of
learnable parameters. More importantly, our method out-
performs the existing adapter-based visual fine-tuning ap-
proach AdaptFormer [4] on both base and large versions
of pre-trained models, demonstrating the superiority of the
TC-MoA structure in fusion tasks. In addition to this, Ta-
ble 9 shows our base version achieves competitive perfor-
mance on the VIF task, which can be further enhanced by
the large version.
4.6. Analysis and Discussion
Efficiency . We have observed the speed issues brought
about by the massive parameters of the pre-trained models.
To address this, we have optimized the ViT architecture by
the shifted windows, which will be detailed in Sec. 8. After
optimization, for multi-task with arbitrary-size inputs, Ta-
ble 6 shows the base and large versions of the model have
accelerated by 178% and 167%, resulting in acceptable FPS
(Frames Per Second) compared with other methods. The
‚ÄúFrozen‚Äù refers to the vanilla ViT architecture.
prompt‚Ä≤
x=¬µ+Œ±(prompt x‚àí¬µ) +Œ≤
prompt‚Ä≤
y=¬µ+Œ±(prompt y‚àí¬µ)‚àíŒ≤(8)
Prompt Controllability . Employing the formula noted in
Eq. (8), we manipulate both the scale and shift of the prompt
provided to the trained model, where ¬µ= 0.5represents
the mean, while Œ±andŒ≤denote the scaling and shifting
factors, respectively. We conduct the manipulation on some
samples and present an example in Fig. 6 (a).Table 5. Ablation studies on fine-tuning. BandLrepresent the
use of ViT-base and ViT-large structures respectively.
VIF MEF MFFArchitectureQabf QpSSIM Qabf QpSSIM Qabf QpSSIM
FrozenBackbone B0.175 0.089 0.275 0.250 0.139 0.227 0.295 0.189 0.466
AdaptFormer B 0.531 0.336 0.427 0.574 0.504 0.377 0.579 0.561 0.651
TC-MoA B 0.576 0.396 0.450 0.604 0.540 0.394 0.612 0.619 0.660
FrozenBackbone L0.330 0.238 0.363 0.452 0.408 0.333 0.450 0.428 0.596
AdaptFormer L 0.576 0.392 0.446 0.616 0.563 0.396 0.640 0.650 0.671
TC-MoA L 0.601 0.412 0.455 0.645 0.598 0.412 0.657 0.681 0.679
Table 6. Efficiency and scalability comparisons of approaches.
U2F DDFM CDD Frozen BTC-MoA BFrozen LTC-MoA L
Total Params (M) 1.32 - 1.78 111.65 115.40 339.12 348.70
Trainable Params (M) 1.32 - 1.78 - 3.87 - 9.58
FPS (VIF 640 √ó512) 4.72 0.01 2.52 1.17 3.33 0.60 1.60
FPS (MEF arbitrary-size) 2.21 - - 1.37 3.58 0.71 1.86
FPS (MFF arbitrary-size) 1.86 - - 1.39 3.98 0.71 1.95
Obviously, as the value of Œ≤increases, the brightness and
texture of the fused image increasingly resemble the source
image X, and inversely, the source image Y. Thus, Œ≤can
be considered as a shifting of the dominant intensity bias.
Moreover, a higher value of Œ±tends to favor one source
image over the other in the image patches, which can be
considered as a scaling of the dominant intensity deviation.
For example, outputs with Œ≤= -0.3 are more similar to the
under-exposure image globally. As Œ±increases from 0 to
3, the building areas of the fused image gradually become
brighter, but the brightness of the clouds remains basically
unchanged. This suggests that changing the scale factor of
the prompt motivates the model to select different regions.
Overall, this experiment demonstrates that the prompt is
controllable and can be linearly transformed to obtain fuse
images in different degrees.
Generalizability. Our method is highly generalizable
to tasks with similar fusion rules by zero-shot fusion.
Specifically, we take samples from medical image fu-
sion dataset [1] and pan-sharpening dataset generated by
the Quickbird satellite, which are fused based on VIF‚Äôs
paradigm. We obtain a base fused image with Œ±= 1 and
Œ≤= 0, as shown in Fig. 6 (b) and (c). However, with-
out any prior knowledge of these unknown tasks, the output
fused image is an under-performing image, missing many
high and low-frequency information. Interestingly, by ma-
nipulating the prompt, we can find a suitable fusion rule for
these new tasks, obtaining reasonable fusion results.
Routers Controllability. As shown in Fig. 7, the images
generated by the MFF router are darker than VIF in areas
without gradient guidance. The MFF selects the reference
sources based on gradients, it tends to average the informa-
tion from sources without gradient guidance. In contrast,
the VIF router tends to preserve the maximum intensity in-
formation. Moreover, at the bottom of the image, the MFF
router tends to preserve the gradient information depending
on a single source, while the VIF router preserves the gra-
dient information from both sources. Additionally, by ma-
nipulating the combination of different routers, our model
7105
ùõº=0 ùõº=1 ùõº=3ùõΩ=‚àí0.3 ùõΩ=0.3 ùõΩ=0 ùõΩ=‚àí0.3 ùõΩ=0.3 ùõΩ=0 ùõΩ=‚àí0.3 ùõΩ=0.3 ùõΩ=0
(a) (b) (c)Figure 6. Visualisation of controllability and generalization of our method.
Router of MFF Routers of MEF and VIF Router of VIF
Fused Image ùëÉùëüùëúùëöùëùùë°
Figure 7. Visualisation of different routers on medical images.
copes with controlling the dynamic changing fusion degree,
obtaining more controllable results.
Hyperparameters. We conducted hyperparameters analy-
sis on two aspects: the number of experts ( N) and the in-
terval between two TC-MoAs ( œÑ). We utilize the ViT-large
as our backbone, which consists of 24 transformer blocks in
the encoder and 8 in the decoder. If œÑ= 4, our model con-
tains 8 instances of TC-MoA. We employ two widely-used
metrics Qabfand SSIM to inform the selection of hyperpa-
rameters. As presented in Fig. 8, as Nincreases, the perfor-
mance initially increases but eventually declines, with the
peak performance detected at N= 4. This suggests that
increasing the number of experts does not necessarily en-
hance the model‚Äôs performance. Similarly, when œÑ= 4,
the model demonstrates the best performance across multi-
task. Hence, our experiments are conducted under N= 4
andœÑ= 4.
5. Conclusion
In this paper, we propose a task-customized mixture of
adapters for general image fusion. With the help of TC-
MoA, different fusion tasks customize their respective mix-
ture of adapters to obtain prompt guidance for multiple-
source image fusion in a unified model. To ensure compat-
0.600 0.600 0.601 0.566 0.448 0.601 0.459 N1œÑ4N2œÑ4N4œÑ4N8œÑ4N4œÑ2N4œÑ4N4œÑ8
0.454 0.455 0.455 0.423 0.422 0.455 0.323 N1œÑ4N2œÑ4N4œÑ4N8œÑ4N4œÑ2N4œÑ4N4œÑ8VIF0.636 0.645 0.645 0.611 0.480 0.645 0.445 N1œÑ4N2œÑ4N4œÑ4N8œÑ4N4œÑ2N4œÑ4N4œÑ8
0.962 0.964 0.964 0.875 0.852 0.964 0.871 N1œÑ4N2œÑ4N4œÑ4N8œÑ4N4œÑ2N4œÑ4N4œÑ8MEF0.679 0.680 0.679 0.656 0.538 0.679 0.481 N1œÑ4N2œÑ4N4œÑ4N8œÑ4N4œÑ2N4œÑ4N4œÑ8MFFùë∏ùíÇùíÉùíáSSIM0.656 0.656 0.657 0.632 0.396 0.657 0.474 N1œÑ4N2œÑ4N4œÑ4N8œÑ4N4œÑ2N4œÑ4N4œÑ8Figure 8. Experiments on the hyperparameters.
ibility with different tasks while maintaining complemen-
tarity for multi-source images, we further propose mutual
information regularization to constrain these adapters. Ex-
perimental results have shown that TC-MoA achieves ad-
vanced performance in general image fusion against the
competing methods. In addition, TC-MoA demonstrates
strong prompt controllability and router controllability to
perform flexible manipulation on the fused images.
6. Acknowledgement
This work was sponsored in part by the National Key R&D
Program of China 2022ZD0116500, in part by the Na-
tional Natural Science Foundation of China (62222608,
62106171, U23B2049, 61925602), in part by the Haihe
Lab of ITAI under Grant 22HHXCJC00002, in part by the
Tianjin Natural Science Foundation under Grant 21JCY-
BJC00580, in part by Tianjin Natural Science Funds for
Distinguished Young Scholar under Grant 23JCJQJC00270,
and in part by the Key Laboratory of Big Data Intelligent
Computing, Chongqing University of Posts and Telecom-
munications under Grant BDIC-2023-A-008. This work
was also sponsored by CAAI-CANN Open Fund, developed
on OpenI Community. The authors appreciate the sugges-
tions from CVPR anonymous peer reviewers.
7106
References
[1] Aanlib. Ihttp://www.med.harvard.edu/AANLIB/home.html ,
(Accessed 5 January 2020). 7
[2] Jianrui Cai, Shuhang Gu, and Lei Zhang. Learning a deep
single image contrast enhancer from multi-exposure images.
IEEE Transactions on Image Processing , 27(4):2049‚Äì2062,
2018. 4
[3] Bing Cao, Yiming Sun, Pengfei Zhu, and Qinghua Hu.
Multi-modal gated mixture of local-to-global experts for dy-
namic image fusion. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision (ICCV) , pages
23555‚Äì23564, 2023. 2, 5
[4] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang,
Yibing Song, Jue Wang, and Ping Luo. Adaptformer:
Adapting vision transformers for scalable visual recogni-
tion. Advances in Neural Information Processing Systems ,
35:16664‚Äì16678, 2022. 2, 7
[5] Nedeljko Cvejic, Artur Loza, David Bull, and Nishan Cana-
garajah. A similarity metric for assessment of image fusion
algorithms. International journal of signal processing , 2(3):
178‚Äì182, 2005. 5
[6] Wang Di, Liu Jinyuan, Fan Xin, and Risheng Liu. Unsuper-
vised misaligned infrared and visible image fusion via cross-
modality image generation and registration. In International
Joint Conference on Artificial Intelligence (IJCAI) , 2022. 5
[7] William Fedus, Barret Zoph, and Noam Shazeer. Switch
transformers: Scaling to trillion parameter models with sim-
ple and efficient sparsity. The Journal of Machine Learning
Research , 23(1):5232‚Äì5270, 2022. 2
[8] Zongbo Han, Changqing Zhang, Huazhu Fu, and Joey Tianyi
Zhou. Trusted multi-view classification with dynamic evi-
dential fusion. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 45(2):2551‚Äì2566, 2023. 1
[9] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll¬¥ar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 16000‚Äì
16009, 2022. 4
[10] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021. 2
[11] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,
Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-
sual prompt tuning. In European Conference on Computer
Vision , pages 709‚Äì727. Springer, 2022. 2
[12] Xinyu Jia, Chuang Zhu, Minzhen Li, Wenqi Tang, and Wenli
Zhou. Llvip: A visible-infrared paired dataset for low-light
vision. In Proceedings of the IEEE/CVF international con-
ference on computer vision , pages 3496‚Äì3504, 2021. 4
[13] Xin Jin, Xiuliang Xi, Ding Zhou, Xiaoxuan Ren, Jie Yang,
and Qian Jiang. An unsupervised multi-focus image fusion
method based on transformer and u-net. IET Image Process-
ing, 17(3):733‚Äì746, 2023. 2
[14] Harpreet Kaur, Deepika Koundal, and Virender Kadyan. Im-
age fusion techniques: a survey. Archives of computational
methods in Engineering , 28:4425‚Äì4447, 2021. 1[15] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao
Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam
Shazeer, and Zhifeng Chen. Gshard: Scaling giant mod-
els with conditional computation and automatic sharding.
CoRR , abs/2006.16668, 2020. 2
[16] Hui Li and Xiao-Jun Wu. Densefuse: A fusion approach to
infrared and visible images. IEEE Transactions on Image
Processing , 28(5):2614‚Äì2623, 2018. 1, 2
[17] Hui Li, Xiao-Jun Wu, and Josef Kittler. Rfn-nest: An end-to-
end residual fusion network for infrared and visible images.
Information Fusion , 73:72‚Äì86, 2021. 5
[18] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimiz-
ing continuous prompts for generation. arXiv preprint
arXiv:2101.00190 , 2021. 2
[19] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao
Wang. Scaling & shifting your features: A new baseline
for efficient model tuning. Advances in Neural Information
Processing Systems , 35:109‚Äì123, 2022. 2
[20] Pengwei Liang, Junjun Jiang, Xianming Liu, and Jiayi Ma.
Fusion from decomposition: A self-supervised decomposi-
tion approach for image fusion. In European Conference on
Computer Vision , pages 719‚Äì735. Springer, 2022. 2, 5, 6
[21] Jinyuan Liu, Xin Fan, Zhanbo Huang, Guanyao Wu, Risheng
Liu, Wei Zhong, and Zhongxuan Luo. Target-aware dual
adversarial learning and a multi-scenario multi-modality
benchmark to fuse infrared and visible for object detection.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 5802‚Äì5811, 2022. 2,
5
[22] Yu Liu, Xun Chen, Hu Peng, and Zengfu Wang. Multi-focus
image fusion with a deep convolutional neural network. In-
formation Fusion , 36:191‚Äì207, 2017. 2
[23] Yu Liu, Lei Wang, Juan Cheng, Chang Li, and Xun Chen.
Multi-focus image fusion: A survey of the state of the art.
Information Fusion , 64:71‚Äì91, 2020. 1, 5
[24] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong,
and Ed H Chi. Modeling task relationships in multi-task
learning with multi-gate mixture-of-experts. In Proceed-
ings of the 24th ACM SIGKDD international conference
on knowledge discovery & data mining , pages 1930‚Äì1939,
2018. 2
[25] Jiayi Ma, Yong Ma, and Chang Li. Infrared and visible im-
age fusion methods and applications: A survey. Information
fusion , 45:153‚Äì178, 2019. 1, 5
[26] Jiayi Ma, Linfeng Tang, Fan Fan, Jun Huang, Xiaoguang
Mei, and Yong Ma. Swinfusion: Cross-domain long-range
learning for general image fusion via swin transformer.
IEEE/CAA Journal of Automatica Sinica , 9(7):1200‚Äì1217,
2022. 2
[27] Kede Ma, Kai Zeng, and Zhou Wang. Perceptual quality
assessment for multi-exposure image fusion. IEEE Transac-
tions on Image Processing , 24(11):3345‚Äì3356, 2015. 5
[28] Kede Ma, Zhengfang Duanmu, Hanwei Zhu, Yuming Fang,
and Zhou Wang. Deep guided learning for fast multi-
exposure image fusion. IEEE Transactions on Image Pro-
cessing , 29:2808‚Äì2819, 2019. 1, 4, 5
7107
[29] Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe
Jenatton, and Neil Houlsby. Multimodal contrastive learn-
ing with limoe: the language-image mixture of experts. Ad-
vances in Neural Information Processing Systems , 35:9564‚Äì
9576, 2022. 2
[30] Linhao Qu, Shaolei Liu, Manning Wang, and Zhijian Song.
Transmef: A transformer-based multi-exposure image fusion
framework using self-supervised multi-task learning. In Pro-
ceedings of the AAAI conference on artificial intelligence ,
pages 2126‚Äì2134, 2022. 2
[31] K Ram Prabhakar, V Sai Srikar, and R Venkatesh Babu.
Deepfuse: A deep unsupervised approach for exposure fu-
sion with extreme exposure image pairs. In Proceedings of
the IEEE international conference on computer vision , pages
4714‚Äì4722, 2017. 1, 2, 5
[32] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
Learning multiple visual domains with residual adapters. Ad-
vances in neural information processing systems , 30, 2017.
2
[33] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy
Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outra-
geously large neural networks: The sparsely-gated mixture-
of-experts layer. arXiv preprint arXiv:1701.06538 , 2017. 2,
4
[34] Yiming Sun, Bing Cao, Pengfei Zhu, and Qinghua Hu. Det-
fusion: A detection-driven infrared and visible image fusion
network. In Proceedings of the 30th ACM International Con-
ference on Multimedia , pages 4003‚Äì4011, 2022. 2
[35] Linfeng Tang, Jiteng Yuan, and Jiayi Ma. Image fusion in
the loop of high-level vision tasks: A semantic-aware real-
time infrared and visible image fusion network. Information
Fusion , 82:28‚Äì42, 2022. 1
[36] Wei Tang, Fazhi He, and Yu Liu. Ydtr: Infrared and visible
image fusion via y-shape dynamic transformer. IEEE Trans-
actions on Multimedia , 2022. 1, 2, 5
[37] Z. Wang, E.P. Simoncelli, and A.C. Bovik. Multiscale struc-
tural similarity for image quality assessment. In The Thrity-
Seventh Asilomar Conference on Signals, Systems & Com-
puters, 2003 , pages 1398‚Äì1402 V ol.2, 2003. 5
[38] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli.
Image quality assessment: from error visibility to structural
similarity. IEEE Transactions on Image Processing , 13(4):
600‚Äì612, 2004. 5
[39] Zhishe Wang, Yanlin Chen, Wenyu Shao, Hui Li, and Lei
Zhang. Swinfuse: A residual swin transformer fusion net-
work for infrared and visible images. IEEE Transactions on
Instrumentation and Measurement , 71:1‚Äì12, 2022. 5
[40] Fang Xu, Jinghong Liu, Yueming Song, Hui Sun, and Xuan
Wang. Multi-exposure image fusion techniques: A compre-
hensive review. Remote Sensing , 14(3):771, 2022. 1, 5
[41] Han Xu, Jiayi Ma, Junjun Jiang, Xiaojie Guo, and Haibin
Ling. U2fusion: A unified unsupervised image fusion net-
work. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 44(1):502‚Äì518, 2020. 1, 2, 5, 6
[42] Han Xu, Jiayi Ma, Zhuliang Le, Junjun Jiang, and Xiaojie
Guo. Fusiondn: A unified densely connected network for
image fusion. In Proceedings of the AAAI conference on
artificial intelligence , pages 12484‚Äì12491, 2020. 2, 5, 6[43] Han Xu, Jiayi Ma, and Xiao-Ping Zhang. Mef-gan: Multi-
exposure image fusion via generative adversarial networks.
IEEE Transactions on Image Processing , 29:7203‚Äì7216,
2020. 2, 5
[44] Hao Zhang and Jiayi Ma. Sdnet: A versatile squeeze-and-
decomposition network for real-time image fusion. Interna-
tional Journal of Computer Vision , 129:2761‚Äì2785, 2021. 5
[45] Hao Zhang, Han Xu, Yang Xiao, Xiaojie Guo, and Jiayi Ma.
Rethinking the image fusion: A fast unified image fusion
network based on proportional maintenance of gradient and
intensity. In Proceedings of the AAAI conference on artificial
intelligence , pages 12797‚Äì12804, 2020. 2, 5, 6
[46] Hao Zhang, Zhuliang Le, Zhenfeng Shao, Han Xu, and Jiayi
Ma. Mff-gan: An unsupervised generative adversarial net-
work with adaptive and gradient joint constraints for multi-
focus image fusion. Information Fusion , 66:40‚Äì53, 2021. 2,
4, 6
[47] Juncheng Zhang, Qingmin Liao, Shaojun Liu, Haoyu Ma,
Wenming Yang, and Jing-Hao Xue. Real-mff: A large re-
alistic multi-focus image dataset with ground truth. Pattern
Recognition Letters , 138:370‚Äì377, 2020. 4
[48] Qingyang Zhang, Haitao Wu, Changqing Zhang, Qinghua
Hu, Huazhu Fu, Joey Tianyi Zhou, and Xi Peng. Provable
dynamic fusion for low-quality multimodal data. In Interna-
tional Conference on Machine Learning , 2023. 1
[49] Xingchen Zhang. Benchmarking and comparing multi-
exposure image fusion algorithms. Information Fusion , 74:
111‚Äì131, 2021. 4, 5
[50] Xingchen Zhang. Deep learning-based multi-focus image fu-
sion: A survey and a comparative study. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 2021. 4, 6
[51] Yu Zhang, Yu Liu, Peng Sun, Han Yan, Xiaolin Zhao, and
Li Zhang. Ifcnn: A general image fusion framework based
on convolutional neural network. Information Fusion , 54:
99‚Äì118, 2020. 2, 5, 6
[52] Zixiang Zhao, Shuang Xu, Chunxia Zhang, Junmin Liu,
Pengfei Li, and Jiangshe Zhang. Didfuse: Deep image de-
composition for infrared and visible image fusion. arXiv
preprint arXiv:2003.09210 , 2020. 2, 5
[53] Zixiang Zhao, Shuang Xu, Jiangshe Zhang, Chengyang
Liang, Chunxia Zhang, and Junmin Liu. Efficient and model-
based infrared and visible image fusion via algorithm un-
rolling. IEEE Transactions on Circuits and Systems for Video
Technology , 32(3):1186‚Äì1196, 2021. 2, 5
[54] Zixiang Zhao, Haowen Bai, Jiangshe Zhang, Yulun Zhang,
Shuang Xu, Zudi Lin, Radu Timofte, and Luc Van Gool.
Cddfuse: Correlation-driven dual-branch feature decompo-
sition for multi-modality image fusion. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5906‚Äì5916, 2023. 1, 5
[55] Zixiang Zhao, Haowen Bai, Yuanzhi Zhu, Jiangshe Zhang,
Shuang Xu, Yulun Zhang, Kai Zhang, Deyu Meng, Radu
Timofte, and Luc Van Gool. Ddfm: denoising diffusion
model for multi-modality image fusion. arXiv preprint
arXiv:2303.06840 , 2023. 1, 5
7108
