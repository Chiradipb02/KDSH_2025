Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation with
Deterministic Sampling Prior
Zike Wu1,4Pan Zhou⇤2,4Xuanyu Yi1,4Xiaoding Yuan3Hanwang Zhang1,5
1Nanyang Technological University2Singapore Management University
3Johns Hopkins University4Sea AI Lab5Skywork AI
zike001@e.ntu.edu.sg ,panzhou@smu.edu.sg ,xuanyu001@e.ntu.edu.sg ,xyuan19@jhu.edu ,hanwangzhang@ntu.edu.sg
Anastronaut is riding a horseA ceramic lion
A tiger wearing a tuxedoA baby dragonhatching out of a stone eggA panda rowing a boatA skiing penguin wearing a puffy jacket
The Himeji CastleThe Great Wall
Figure 1. Examples generated by Consistent3D. Our methods can generate detailed, diverse 3D objects and large-scale scenes from a wide
range of textual prompts.
Abstract
Score distillation sampling (SDS) and its variants have
greatly boosted the development of text-to-3D generation,
but are vulnerable to geometry collapse and poor textures
yet. To solve this issue, we ﬁrst deeply analyze the SDS
and ﬁnd that its distillation sampling process indeed corre-
sponds to the trajectory sampling of a stochastic differen-
tial equation (SDE): SDS samples along an SDE trajectory
to yield a less noisy sample which then serves as a guid-
ance to optimize a 3D model. However, the randomness in
SDE sampling often leads to a diverse and unpredictable
⇤Corresponding author.sample which is not always less noisy, and thus is not a
consistently correct guidance, explaining the vulnerability
of SDS. Since for any SDE, there always exists an ordinary
differential equation (ODE) whose trajectory sampling can
deterministically and consistently converge to the desired
target point as the SDE, we propose a novel and effective
“Consistent3D” method that explores the ODE determinis-
tic sampling prior for text-to-3D generation. Speciﬁcally,
at each training iteration, given a rendered image by a 3D
model, we ﬁrst estimate its desired 3D score function by a
pre-trained 2D diffusion model, and build an ODE for tra-
jectory sampling. Next, we design a consistency distilla-
tion sampling loss which samples along the ODE trajectory
to generate two adjacent samples and uses the less noisy
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
9892
sample to guide another more noisy one for distilling the
deterministic prior into the 3D model. Experimental re-
sults show the efﬁcacy of our Consistent3D in generating
high-ﬁdelity and diverse 3D objects and large-scale scenes,
as shown in Fig. 1. The codes are available at https:
//github.com/sail-sg/Consistent3D .
1. Introduction
Diffusion models (DMs) have recently garnered signiﬁcant
attention in the realm of image synthesis, as evidenced by
their remarkable capabilities [ 34,48]. This notable progress
can be largely attributed to the integration of large-scale
image-text pair datasets and the evolution of scalable gen-
erative model architectures [ 30,35]. This recent success
has seamlessly transcended into the domain of text-to-3D
generation by leveraging the pre-trained 2D diffusion mod-
els [33,34] to guide the 3D generation process, regardless
of the absence of large-scale 3D generative models [ 3,45].
The pivotal breakthrough in this ﬁeld stems from the
ﬁnding that one can use the score function predicted by pre-
trained 2D diffusion models, such as Stable Diffusion [ 34],
to estimate the 3D score function [ 7,14,45]. Since this
score function indicates the direction of the higher data den-
sity [ 10,40], one can ﬁrst use it to build a stochastic dif-
ferential equation (SDE) [ 41], and then sample along the
SDE solution trajectory ( i.e., SDE reverse process) to iter-
atively improve a learnable 3D model ( e.g., NeRF [ 26] or
Mesh [ 37]). This is also the underlying mechanism behind
the prevalent and leading text-to-3D approach, Score Dis-
tillation Sampling (SDS) [ 31]. In each training iteration,
SDS follows the forward SDE to inject noise into a ren-
dered image by a learnable 3D model, and then samples a
more realistic pseudo-image along the SDE solution trajec-
tory, where the 3D score function of the SDE is estimated
by a pre-trained diffusion model [ 45]. Next, SDS pulls its
rendered image closer to the pseudo-image via optimizing
the learnable 3D model.
However, as illustrated in Fig. 2, the high randomness
inherent in the SDE solution distribution [ 11,41] leads to a
highly diverse and unpredictable next point in the solution
trajectory, e.g., the pseudo-image [ 38,55] in SDS. Although
this trajectory may eventually converge to a speciﬁc target,
e.g., the desired realistic image in SDS, the sampled next
point does not always provide the correct guidance in each
iteration. This lack of reliability also applies to SDS, sig-
niﬁcantly increasing the optimization difﬁculty of the 3D
model. It also helps to explain why SDS is so vulnera-
ble and often suffers from geometry collapse and poor ﬁne-
grained texture in practice [ 38,43,47].
To address this critical issue, in this paper, we propose a
novel and effective method, dubbed“Consistent3D”, which
guides text-to-3D generation using deterministic samplingDataNoise
Figure 2. Comparison between the (reverse) trajectory samplings
in the stochastic differential equation (SDE) and ordinary differ-
ential equation (ODE).
prior. As illustrated in Fig. 2, for these unpredictable and
uncontrollable trajectories sampled from SDE solution dis-
tribution, there theoretically always exists a correspond-
ing ordinary differential equation (ODE) whose trajectory
shares the same marginal distributions with the SDE solu-
tion [ 41]. Importantly, this ODE trajectory is deterministic
and consistently converges to the same target point as the
SDE. Sampling along the ODE trajectory guarantees a pre-
dictable and deterministic next point, which always directs
towards the desired target and thus providing a reliable and
consistent guidance. This motivates us to explore the text-
to-3D generation from the ODE deterministic sampling per-
spective.
Speciﬁcally, during each training iteration, we begin by
estimating the desired 3D score function from the rendered
images produced by the learnable 3D model using pre-
trained 2D diffusion models. Subsequently, we build a cor-
responding ODE for solution trajectory sampling. To effec-
tively optimize the underlying 3D representations, we then
introduce a Consistency Distillation Sampling loss (CDS),
which leverages deterministic sampling prior along the
ODE ﬂow. In detail, for each rendered image, we ﬁrst inject
aﬁxed noise to the rendered image so that the corresponding
noisy sample lies in the ODE solution distribution and thus
can be well denoised. Following this, we sample two adja-
cent points from the ODE trajectory given the noisy sample.
The less-noisy sample is then used to guide its more-noisy
counterpart, thereby distilling the deterministic prior of the
ODE trajectory into the 3D model. Here we use ﬁxed noise
to ensure that the samplings from the ODE trajectory for
all rendered images converge to the same targeted realistic
image, thereby offering more consistent guidance and en-
hancing the optimization of the 3D model.
Extensive experimental results showcase the efﬁcacy of
Consistent3D in generating high-ﬁdelity and diverse 3D ob-
jects, along with large-scale scenes, as shown in Fig. 1and
Fig.4. Comparative evaluations against existing methods,
including DreamFusion [ 31], Magic3D [ 20] and Proliﬁc-
Dreamer [ 47], demonstrate the superiority of Consistent3D
9893
in both qualitative and quantitative terms. The proposed ap-
proach effectively addresses the challenges associated with
randomness in the SDE solution distribution, offering a
more reliable and consistent framework to guide the text-
to-3D generation process.
2. Related Works
Diffusion Models [11,41,50] are powerful tools for com-
plex data modeling and generation. Their robust and stable
capabilities for complex data modeling have also led to their
successful application in various domains, such as image [ 4,
8,48], video [ 12,13,18], and 3D [ 31,45],etc. Regarding
improving the sampling efﬁciency, there are two main ap-
proaches: learning-free sampling and learning-based sam-
pling. Learning-free sampling typically involves discretiz-
ing reverse-time SDE [ 5,41] or ODE [ 16,21,23,39,52],
while learning-based sampling is mainly based on knowl-
edge distillation [ 25,36,42]. This paper is driven by recent
progress in learning-based sampling, particularly in distill-
ing knowledge from ODE sampling [ 36,42].
Text-to-3D Generation stands for generating 3D contents
from a given text description. Current 3D generative mod-
els [15,29], usually work in a single object category and
suffer from limited diversity due to the lack of large-scale
3D datasets. To achieve open-vocabulary 3D generation,
pioneered by DreamFusion [ 31], several approaches pro-
pose to lift text-image diffusion models [ 34] for 3D genera-
tion [ 47,54,55]. The key mechanism of such approaches is
the score distillation sampling (SDS), where diffusion pri-
ors are used to supervise the optimization of a 3D represen-
tation. The following works continue to further improve the
stability and ﬁdelity of generation of various aspects, e.g.,
advanced 3D representation [ 3,43,46,51], coarse-to-ﬁne
training strategy [ 20,47,55] and 3D-aware diffusion pri-
ors [19,22,38,44,54].
3. Preliminaries
Diffusion Models (DMs). They consist of a forward dif-
fusion process and a reverse sampling process. During the
forward process, DMs gradually add Gaussian noise to the
vanilla sample x0⇠pdata(x)and generate a series of noisy
samples xtaccording to the distribution:
pt(xt|x0)=N(xt;x0, 2
tI), (1)
where  tvaries along time-step t. Accordingly, one can
easily sample a noisy sample at any time step tbyxt=
x0+ t✏t, where ✏t⇠N(0,I).
The reverse process from a Gaussian noise xTto a real-
istic sample x0is also called trajectory sampling, and can
be formally formulated into a reverse SDE [ 16]:
dx= ˙ t trlogpt(x)dt+p
˙ t tdw, (2)where wis the standard Wiener process, ˙ tis the time
derivative of  t, and rlogpt(x)is the score function
which indicates the direction of the higher data density [ 41].
Meanwhile, there exists a corresponding reverse ordinary
deterministic equation (ODE) which is deﬁned as follows:
dx= ˙ t trlogpt(x)dt. (3)
For this ODE, its trajectory shares the same marginal prob-
ability density as the SDE which ensures the same conver-
gence point of ODE and SDE.
Given any noise xT⇠N(0, 2
TI), one can either solve
the reverse SDE in Eq. ( 2) or the reverse ODE in Eq. ( 3) via
any numerical solver [ 1,23,53] to generate a real sample
ˆx0⇠pdata(x). In practice, the pre-trained diffusion mod-
els [16,34] are used to estimate the score function, thereby
guiding the sampling process.
Text-to-3D Generation via Score Distillation Sampling
(SDS). Given a camera pose ⇡, SDS distills 2D priors of
a pre-trained diffusion model D (·)into a 3D model ( e.g.,
NeRF, Mesh) parameterized by ✓. Formally, SDS applies
a denoising training objective to the rendered image x⇡=
g(✓,⇡)where g(·)is a differentiable renderer and ⇡is a
camera pose, and computes the gradient as
r✓LSDS(✓)=Et,✏
 (t)(x⇡ D (xt,t ,y))@x⇡
@✓ 
,(4)
where  (t)denotes the loss weight, xt=x⇡+ t✏tdenotes
the noisy sample and yis the text condition.
4. Method
Here we elaborate on our proposed Consistent3D for effec-
tive text-to-3D generation. In Sec. 4.1, we reveal the un-
derlying mechanism of Score Distillation Sampling (SDS)
which aims to approximate the SDE sampling process and
motivates our proposed methods. Then in Sec. 4.2, we in-
troduce Consistency Distillation Sampling (CDS), a loss de-
signed to efﬁciently distill deterministic sampling priors for
text-to-3D generation, accompanied with a theoretical justi-
ﬁcation regarding the error bound. Finally, we present how
to use our CDS to build our text-to-3D generation frame-
work, dubbed “Consistent3D”, in Sec. 4.3.
4.1. Revisit Score Distillation Sampling
Before introducing our proposed method, we ﬁrst connect
SDE in Eq. ( 2) with the leading text-to-3D generation ap-
proach SDS in Eq. ( 4), since this connection directly moti-
vates us to use ODE in Eq. ( 3) for text-to-3D generation.
First, we discretize the reverse SDE in Eq. ( 2) and per-
form the stochastic sampling process following Ho et al.
[11], which results in the SDE solution trajectory deﬁned as
xti=ˆxi 1+ ti✏tiwith ✏ti⇠N(0,I),
ˆxi=D (xti,ti),(5)
9894
Algorithm 1: Text-to-3D Generation with CDS
Input : initial 3D model parameter ✓, pre-trained
diffusion model D , text prompt y,
training iteration N, time-step range
[tmin,tmax], learning rate ⌘
Output: ✓
Sample ✏⇤⇠N(0,I)//Fixed noise
foreach i2{0,...,N }do
t2 tmax (tmax tmin)p
i/N
Sample camera pose ⇡
Sample t12U[t2+ , t2+  ]
x⇡ g(✓,⇡)
xt1 x⇡+ t1✏⇤
di (xt1 D (xt1,t1,y))/ t1
ˆxt2 xt1+( t2  t1)di
ˆx0 x⇡+s g[  t1(✏⇤ di)]
LCDS(✓;⇡)  (t2)kˆx0 sg [D (ˆxt2,t2,y)]k2
2
✓ ✓ ⌘r✓LCDS(✓;⇡)
end
where ˆx0=0is to ensure ˆxT=ˆx0+ T✏T⇠N(0, 2
TI),
and time step schedule {ti}satisﬁes T=t1>t 2>···>
tN=0. To build a connection between SDE and SDS, in
Eq. ( 5), we follow SDS to approximate the score function
rlogpt(x)in vanilla SDE with a score network D (xt,t).
Then by iteratively running Eq. ( 5) from t1totN, one can
eventually compute the desired SDE solution in expecta-
tion, e.g., a real sample ˆxN⇠pdata(x)if the network D (·)
is a well-trained diffusion model like Stable Diffusion [ 34].
On the other hand, by ﬁxing the camera pose ⇡, by ﬁxing
the camera pose ⇡, for a rendered image x⇡by a learnable
3D model ✓, the optimization process of SDS introduced in
Sec. 3can be formulated as:
xti=xi 1
⇡+ ti✏tiwith ✏ti⇠N(0,I),
xi
⇡=g(✓i,⇡)with ✓i=arg min
✓kg(✓,⇡) D (xti,ti)k,(6)
where x0
⇡=g(✓0,⇡)in which ✓0denotes the randomly
initialized 3D model [ 20,31], and g(·)is a differentiable
renderer [ 6,26]. Compared the stochastic sampling process
in Eq. ( 5) with SDS process in Eq. ( 6), one can observe that
if for each iteration i, one can ideally optimize 3D model ✓i
so that kg(✓i,⇡) D (xti,ti)k=0, then one can have
xi
⇡=g(✓i,⇡)=D (xti,ti). (7)
In this case, the SDS optimization process becomes exactly
the same as the stochastic sampling process with xi
⇡re-
placed by ˆxi.
However, as illustrated in Fig. 2, sampling along the SDE
solution trajectory according to Eq. ( 5) results in an unpre-
dictable and highly variable next point ˆxi, which does notguarantee the correct direction. This issue also extends to
the SDS optimization process, which is equivalent to the
SDE trajectory in Eq. ( 6) when the 3D model is ideally
trained in each iteration ( i.e., Eq. ( 7) holds). Consequently,
such inherent randomness in SDS leads to less accurate and
reliable guidance throughout all training iterations. This
could also help explain why SDS is so vulnerable and of-
ten suffers from geometry collapse and poor ﬁne-grained
texture as observed in many works [ 43,47,55].
4.2. Consistency Distillation Sampling
3D Deterministic Sampling. Given the stochastic and un-
predictable nature of SDS, we are motivated to explore the
potential of the ODE deterministic process which can pro-
vide consistent and more accurate guidance than SDE for
3D generation as shown by Fig. 2. We start by focusing on
the ODE sampling process for a 3D model ✓:
d✓= ˙ t trlogpt(✓)dt, (8)
where ✓is randomly initialized according to a certain dis-
tribution. Following Poole et al. [ 31] and Wang et al. [ 45],
one can derive the 3D score function r✓logpt(✓)from the
2D score function using the chain rule:
r✓logpt(✓)=E⇡
rx⇡logpt(x⇡)@x⇡
@✓ 
, (9)
where the 2D score function rxlogpt(x)can be estimated
asrxlogpt(x)=( D (x,t) x)/ 2
tby a pre-trained dif-
fusion model D (x,t). Therefore, the key to generating a
satisfactory 3D model is to accurately perform the 3D ODE
sampling in Eq. ( 8) using the pre-trained diffusion model.
Unfortunately, unlike the forward SDE process in which
a noisy sample can be easily sampled from the perturbation
kernel by xt⇠pt(xt|x⇡)in Eq. ( 1), the forward ODE re-
quires iterative simulation of the ODE ﬂow, such as DDIM
inversion [ 27] which is complex and time-consuming. This
makes the approximation of the ODE ﬂow with conven-
tional SDS less efﬁcient and is often impractical. Thus,
directly applying SDS loss to the ODE ﬂow is practically
prohibited.
Inspired by recent advances in diffusion model distilla-
tion techniques that facilitate approximation of this deter-
ministic ﬂow without extensive simulation [ 42], we develop
a simple yet effective Consistency Distillation Sampling
loss (CDS) tailored for general text-to-3D generation tasks.
Further detailed discussions can be found in Appendix A.
Optimization objective. We aim to enforce the optimiza-
tion process of the 3D model ✓to match the deterministic
ﬂow between two adjacent ODE sampling steps. Speciﬁ-
cally, we always use a ﬁxed Gaussian noise ✏⇤to perturb
the sample, analogous to setting a ﬁxed starting point at the
ﬁnal diffusion time step. This approach ensures a consis-
tent perturbation in all iterations, similar to the technique
9895
"the Imperial State Crown of England"
NeRF/Mesh
   
Figure 3. Overview of CDS. In each training iteration, the ren-
dered image is perturbed by a ﬁxed noise and then served as a start
point of the deterministic ﬂow for computing the CDS loss.
used in Consistency Training [ 42]. Next, we optimize ✓
by minimizing the following Consistency Distillation Sam-
pling (CDS) loss:
E⇡⇥
 (t2)kD (xt1,t1,y) sg(D (ˆxt2,t2,y))k2
2⇤
,(10)
where sg(·)is a stop-gradient operator, t1>t 2are two
adjacent diffusion time steps, xt1=x⇡+ t1✏⇤, and ˆxt2is
a less noisy sample derived from deterministic sampling by
running one discretization step of a numerical ODE solver
from xt1. Particularly, we adopt the Euler solver to compute
ˆxt2by:
ˆxt2=xt1+ t2  t1
 t1(xt1 D (xt1,t1,y)). (11)
In practice, we follow Poole et al. [ 31] and reparameterize
the ﬁrst component in Eq. ( 10) to skip the CDS gradient
directly to x⇡and✓without computing the U-Net Jacobian.
Time step schedule. As our target is to match the probabil-
ity ﬂow ODE of the reverse sampling process, we follow the
conventional DMs [ 16,41] and set the time steps to decrease
monotonically along with the training iteration of the 3D
models. This approach redeﬁnes our 3D generation process
more as a deterministic sampling rather than a mere training
process as previous SDS-based approaches [ 31,45], thus al-
lowing us to take full advantage of deterministic sampling
prior.
Speciﬁcally, we deﬁne the time step schedule of t2ac-
cording to the current training iteration:
t2:=tmax (tmax tmin)p
i/N, (12)
where iandNdenotes the current iteration and total iter-
ation, respectively. For the initial time step t1which indi-
cates the perturbation level to the rendered image, we em-
pirically uniformly sample it within [t2+ , t2+ ] , which
is different from the predetermined time step schedule in
Consistency Distillation [ 42]. This is because we empir-
ically ﬁnd that the random sampled time step t1within a
small interval collaborated with the deterministic anchor t2exhibits self-calibration behaviors, which can actively cor-
rect the cumulative error made in earlier steps and alleviate
issues such as ﬂoaters and Janus faces [ 14,38]. We delve
deeper into this phenomenon in Sec. 5.4. For more clarity,
we summarize our entire text-to-3D generation procedure
with the proposed CDS in Algorithm 1.
Justiﬁcation. In the following, we offer a theoretical justiﬁ-
cation to demonstrate that, upon achieving convergence, our
Consistency Distillation Sampling is capable of generating
a high-ﬁdelity 3D model.
Theorem 1. Assume that the diffusion model D (·)satisﬁes
the Lipschitz condition. Deﬁne  : =s u p |t1 t2|. For any
given camera pose ⇡, if convergence is achieved according
to Eq. (10), then there exists a corresponding real image
x⇤⇠pdata(x)such that
kx⇡ x⇤k2=O( ), (13)
where x⇡=g(✓,⇡)denotes the rendered image for pose ⇡.
Proof. The proof is based on the truncation error of the Eu-
ler solver. We provide the full proof in Appendix C.
For a 3D model optimized using the CDS, Theorem 1
guarantees that images rendered from any viewpoint of this
model are realistic and closely align with the corresponding
real-world scenes.
4.3. Consistent3D
Now we are ready to introduce our proposed Consistent3D.
As illustrated in Fig. 3, we present a clear design space for
our Consistent3D generation framework using our proposed
Consistency Distillation Sampling (CDS).
Following previous work [ 20,47], Consistent3D is a
coarse-to-ﬁne approach consisting of two stages. Specif-
ically, in the coarse stage, we optimize a low-resolution
Neural Radiance Field (NeRF) [ 2,28]. For the reﬁne-
ment stage, we further optimize a high-resolution textured
3D mesh [ 37] from the neural ﬁeld initialization converting
from the coarse stage. For these two stages, we always use
our proposed CDS.
NeRF Optimization Stage. We adopt multi-resolution
hash grids, Instant NGP [ 28] to parameterize the scene
by density and color with MLPs, which improves training
and rendering efﬁciency. We follow Magic3D on density
bias initialization, camera and light augmentation. In addi-
tion, we use orientation loss [ 31] and 2D normal smooth
loss [ 24]. At this stage, we render 64⇥64images and
use our proposed CDS as guidance. We set tmax=0 .7,
tmin=0.1, =0.1, and  =0 .2.
Mesh Reﬁnement Stage. We convert the neural ﬁeld
into Signed Distance Field (SDF) by subtracting it with
a ﬁxed threshold and then optimizing a high-resolution
9896
The Summer PalaceThe Lotus Temple
The Osaka Castle
The Forbidden City
A photo of a tigerdressed as a doctor
A DSLR photo of a human skeletonrelaxing in a lounge chair
A DSLR photo of a pandathrowing wads of cash into the air
A DSLR photo of a beardressed in medieval armorMichelangelo style statue ofdog reading news on a cellphone 
A beagle in a detective’s outfitA blue motorcycle
A DSLR photo of a tigermade out ofyarn
Figure 4. Consistent3D can generate diverse and high-ﬁdelity objects or large-scale scenes highly correlated with the given text prompts.
DMTet [ 37]. We also initialize the volume texture ﬁeld di-
rectly with the color ﬁeld from the coarse stage. In addition,
we use normal consistency loss and Laplacian smoothness
loss. In the reﬁnement stage, we render 512 ⇥512 images
and set tmax=0.5,tmin=0.02, =0.1, and  =0 .1.
5. Experiment
5.1. Implementation Details
Consistent3D is implemented in PyTorch with a single
NVIDIA A100 GPU based on threestudio [9] with Stable
Diffusion v2.1 [ 34]. We use the Adan [ 49] optimizer with
a learning rate of 0.05for grid encoder and 0.005 for other
parameters, and a weight decay of 2⇥10 8. Further imple-
mentation details are provided in Appendix B.1.5.2. Text-guided 3D Generation
As illustrated in Fig. 4, our Consistent3D demonstrates ver-
satility in generating high-ﬁdelity 3D objects. Its generated
images are not only realistic but also maintain consistency
from various viewpoints. Furthermore, it is capable of gen-
erating large-scale scenes in 360 with remarkable detail.
See more qualitative results in Appendix B.2.
5.3. Comparison with the State-of-The-Art
In this section, we present comprehensive qualitative and
quantitative experiments to evaluate the efﬁcacy of our Con-
sistent3D framework in text-to-3D generation. We com-
pare our generation performance with DreamFusion [31],
Magic3D [20], and ProliﬁcDreamer [47]. For a fair com-
parison, we use the implementations of all the baseline
methods from the open-source repository threestudio [9].
9897
DreamFusionMagic3DProlificDreamerConsistent3D (Ours)
A squirrel dressed like Henry VIII king of England
A DSLR Photo of the Ironman
A DSLR photo of the Imperial State Crown of England
A rabbit, animated movie character, high detail 3d model
A DSLR photo of a bald eagle
A panda wearing a necktie and sitting in an office chair
Figure 5. Qualitative Comparisons of Text-to-3D Generation. Our approach yields results with enhanced ﬁdelity and more robust
geometry.
9898
A DSLR photo of a shibainuplaying golf wearing tartan golf clothes and hat
A DSLR photo of a squirrel in samurai armorwielding a katana
(c) w/o fixed noise(d) Full (Ours)(b) w/o timestep interval(a) w/o timestep scheduleFigure 6. Ablation study of component-wise contribution of Consistent3D: (a) random time step schedule; (b) predetermined time step
schedule; (c) random noise in each iteration; (d) our proposed conﬁguration.
Method Loss CLIP-R
DreamFusion [ 31] SDS 0.310
Magic3D [ 20] SDS 0.311
ProliﬁcDreamer [ 47] VSD 0.336
Consistent3D (Ours) CDS 0.348
Table 1. Quantitative Comparisons of CLIP R-Precision. Scores
were averaged from 40prompts in the DreamFusion gallery.
Qualitative Results. In Fig. 5, we provide qualitative
comparisons with the baseline methods. Our approach ex-
hibits more photorealistic details and geometry than both
the SDS-based approaches like DreamFusion and Magic3D
and the VSD-based approach ProliﬁcDreamer. This im-
provement mainly stems from our Consistency Distillation
Sampling (CDS) which effectively leverages the full poten-
tial of large-scale diffusion models by accurately distilling
deterministic sampling priors into the 3D model.
Quantitative Results. In Tab. 1, we report the results
of CLIP R-Precision [ 32] for 3D objects generated using
40randomly selected text prompts from the DreamFusion
gallery. See more details in Appendix B.1. Each 3D object
is rendered from 120viewpoints with a uniform azimuth an-
gle. The CLIP R-Precision score is computed by averaging
the similarity scores between each rendered view and the
corresponding text prompt. Additionally, we also conduct a
head-to-head user study in Tab. 3. Our quantitative analysis
shows the superior performance of our method.
5.4. Ablation Study
We present an ablation study to evaluate the effects of var-
ious components in our approach in Fig. 6and Tab. 2.W e
conduct experiments with the following conﬁgurations: (a)
a random time step schedule in DreamFusion [ 31]; (b) a
predetermined time step schedule from Consistency Distil-
lation [ 42]; (c) varied random noise in each iteration; and(d) our proposed method incorporating all components. The
results in Fig. 6(a) reveal that a random time step sched-
ule detrimentally affects both geometry and texture model-
ing, since it disrupts established rules of sampling process.
Fig. 6(b) suggests that a predetermined time-step sched-
ule is suboptimal for optimization-based methods, since
gradient descent does not ensure monotonic optimization
progress. This implies that minor randomness helps to ac-
commodate these variations. Fig. 6(c) shows that ﬁxed
noise aids in better convergence by providing a consistent
perturbation in each iteration.
6. Conclusion
In this work, we ﬁrst connect Score Distillation Sam-
pling (SDS), a leading text-to-3D generation approach, with
the solution trajectory sampling of a stochastic differential
equation (SDE). This connection helps us to understand the
vulnerability in SDS, since the randomness in SDE sam-
pling often provides a highly diverse sample, which is not
always less noisy, and could guide the 3D model in the
wrong direction. Then motivated by the fact that an or-
dinary differential equation (ODE) of an SDE can provide
a deterministic and consistent sampling trajectory, we pro-
pose a novel and effective “Consistent3D” by designing a
consistency distillation sampling loss to distill the determin-
istic sampling prior into a 3D model for text-to-3D gener-
ation. Extensive experimental results show that our Con-
sistent3D surpasses state-of-the-art methods in generating
high-ﬁdelity and diverse 3D objects and large-scale scenes.
Acknowledgement
Pan Zhou was supported by the Singapore Ministry of Ed-
ucation (MOE) Academic Research Fund (AcRF) Tier 1
grant.
9899
References
[1]Kendall Atkinson. An Introduction to Numerical Analysis .
John Wiley & Sons, 1991. 3
[2]Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter
Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.
Mip-nerf: A multiscale representation for anti-aliasing neu-
ral radiance ﬁelds. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 5855–5864,
2021. 5
[3]Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.
Fantasia3d: Disentangling geometry and appearance for
high-quality text-to-3d content creation. arXiv preprint
arXiv:2303.13873 , 2023. 2,3
[4]Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in Neural Informa-
tion Processing Systems , 34:8780–8794, 2021. 3
[5]Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Score-
based generative modeling with critically-damped langevin
diffusion. In International Conference on Learning Repre-
sentations , 2021. 3
[6]Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie
Shotton, and Julien Valentin. Fastnerf: High-ﬁdelity neural
rendering at 200fps. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 14346–
14355, 2021. 4
[7]Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and
Dimitris Samaras. Diffusion models as plug-and-play pri-
ors. In Advances in Neural Information Processing Systems ,
2022. 2
[8]Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo
Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-
tor quantized diffusion model for text-to-image synthesis. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 10696–10706, 2022. 3
[9]Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian
Laforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zi-
Xin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang.
threestudio: A uniﬁed framework for 3d content generation.
https://github.com/threestudio-project/
threestudio , 2023. 6
[10] Jonathan Ho and Tim Salimans. Classiﬁer-free diffusion
guidance. arXiv preprint arXiv:2207.12598 , 2022. 2
[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. In Advances in Neural Infor-
mation Processing Systems , pages 6840–6851. Curran Asso-
ciates, Inc., 2020. 2,3
[12] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben
Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
video: High deﬁnition video generation with diffusion mod-
els.arXiv preprint arXiv:2210.02303 , 2022. 3
[13] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William
Chan, Mohammad Norouzi, and David J Fleet. Video dif-
fusion models. arXiv preprint arXiv:2204.03458 , 2022. 3
[14] Susung Hong, Donghoon Ahn, and Seungryong Kim. Debi-
asing scores and prompts of 2d diffusion for robust text-to-3d
generation. arXiv preprint arXiv:2303.15413 , 2023. 2,5[15] Heewoo Jun and Alex Nichol. Shap-e: Generat-
ing conditional 3d implicit functions. arXiv preprint
arXiv:2305.02463 , 2023. 3
[16] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
Elucidating the design space of diffusion-based generative
models. In Advances in Neural Information Processing Sys-
tems, 2022. 3,5,12
[17] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk ¨uhler,
and George Drettakis. 3d gaussian splatting for real-time
radiance ﬁeld rendering. ACM Transactions on Graphics
(ToG) , 42(4):1–14, 2023. 15
[18] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-
vosyan, Roberto Henschel, Zhangyang Wang, Shant
Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-
image diffusion models are zero-shot video generators. arXiv
preprint arXiv:2303.13439 , 2023. 3
[19] Weiyu Li, Rui Chen, Xuelin Chen, and Ping Tan. Sweet-
dreamer: Aligning geometric priors in 2d diffusion for con-
sistent text-to-3d. arXiv preprint arXiv:2310.02596 , 2023.
3
[20] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,
Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution
text-to-3d content creation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 300–309, 2023. 2,3,4,5,6,8,12
[21] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo
numerical methods for diffusion models on manifolds. In In-
ternational Conference on Learning Representations , 2021.
3
[22] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,
Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,
Marc Habermann, Christian Theobalt, et al. Wonder3d: Sin-
gle image to 3d using cross-domain diffusion. arXiv preprint
arXiv:2310.15008 , 2023. 3
[23] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan
Li, and Jun Zhu. Dpm-solver: A fast ode solver for dif-
fusion probabilistic model sampling in around 10 steps. In
Advances in Neural Information Processing Systems , 2022.
3
[24] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and
Andrea Vedaldi. Realfusion: 360deg reconstruction of any
object from a single image. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 8446–8455, 2023. 5
[25] Chenlin Meng, Ruiqi Gao, Diederik P Kingma, Stefano Er-
mon, Jonathan Ho, and Tim Salimans. On distillation of
guided diffusion models. arXiv preprint arXiv:2210.03142 ,
2022. 3
[26] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance ﬁelds for view syn-
thesis. Communications of the ACM , 65(1):99–106, 2021. 2,
4
[27] Ron Mokady, Amir Hertz, Kﬁr Aberman, Yael Pritch, and
Daniel Cohen-Or. Null-text inversion for editing real im-
ages using guided diffusion models. In Proceedings of
9900
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 6038–6047, 2023. 4
[28] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a mul-
tiresolution hash encoding. ACM Transactions on Graphics
(ToG) , 41(4):1–15, 2022. 5
[29] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela
Mishkin, and Mark Chen. Point-e: A system for generat-
ing 3d point clouds from complex prompts. arXiv preprint
arXiv:2212.08751 , 2022. 3
[30] William Peebles and Saining Xie. Scalable diffusion models
with transformers. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 4195–4205,
2023. 2
[31] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv
preprint arXiv:2209.14988 , 2022. 2,3,4,5,6,8,12
[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 8,12
[33] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022. 2
[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684–10695, 2022. 2,3,4,6
[35] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III
18, pages 234–241. Springer, 2015. 2
[36] Tim Salimans and Jonathan Ho. Progressive distillation
for fast sampling of diffusion models. arXiv preprint
arXiv:2202.00512 , 2022. 3
[37] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and
Sanja Fidler. Deep marching tetrahedra: a hybrid repre-
sentation for high-resolution 3d shape synthesis. Advances
in Neural Information Processing Systems , 34:6087–6101,
2021. 2,5,6
[38] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,
and Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-
eration. arXiv preprint arXiv:2308.16512 , 2023. 2,3,5
[39] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In International Conference
on Learning Representations , 2021. 3,15
[40] Yang Song and Stefano Ermon. Improved techniques for
training score-based generative models. Advances in neural
information processing systems , 33:12438–12448, 2020. 2
[41] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-basedgenerative modeling through stochastic differential equa-
tions. In International Conference on Learning Represen-
tations , 2021. 2,3,5,12
[42] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya
Sutskever. Consistency models. arXiv preprint
arXiv:2303.01469 , 2023. 3,4,5,8
[43] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang
Zeng. Dreamgaussian: Generative gaussian splatting for efﬁ-
cient 3d content creation. arXiv preprint arXiv:2309.16653 ,
2023. 2,3,4,15
[44] Ayush Tewari, Tianwei Yin, George Cazenavette, Semon
Rezchikov, Joshua B Tenenbaum, Fr ´edo Durand, William T
Freeman, and Vincent Sitzmann. Diffusion with forward
models: Solving stochastic inverse problems without direct
supervision. arXiv preprint arXiv:2306.11719 , 2023. 3
[45] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,
and Greg Shakhnarovich. Score jacobian chaining: Lifting
pretrained 2d diffusion models for 3d generation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 12619–12629, 2023. 2,3,4,
5
[46] Yiming Wang, Qin Han, Marc Habermann, Kostas Dani-
ilidis, Christian Theobalt, and Lingjie Liu. Neus2: Fast
learning of neural implicit surfaces for multi-view recon-
struction. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 3295–3306, 2023. 3
[47] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan
Li, Hang Su, and Jun Zhu. Proliﬁcdreamer: High-ﬁdelity and
diverse text-to-3d generation with variational score distilla-
tion. arXiv preprint arXiv:2305.16213 , 2023. 2,3,4,5,6,8,
12
[48] Zike Wu, Pan Zhou, Kenji Kawaguchi, and Hanwang Zhang.
Fast diffusion model. arXiv preprint arXiv:2306.06991 ,
2023. 2,3
[49] Xingyu Xie, Pan Zhou, Huan Li, Zhouchen Lin, and
Shuicheng Yan. Adan: Adaptive nesterov momentum al-
gorithm for faster optimizing deep models. arXiv preprint
arXiv:2208.06677 , 2022. 6
[50] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Run-
sheng Xu, Yue Zhao, Yingxia Shao, Wentao Zhang, Bin
Cui, and Ming-Hsuan Yang. Diffusion models: A compre-
hensive survey of methods and applications. arXiv preprint
arXiv:2209.00796 , 2022. 3
[51] Taoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng
Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussian-
dreamer: Fast generation from text to 3d gaussian splatting
with point cloud priors. arXiv preprint arXiv:2310.08529 ,
2023. 3
[52] Qinsheng Zhang, Molei Tao, and Yongxin Chen. gDDIM:
Generalized denoising diffusion implicit models. arXiv
preprint arXiv:2206.05564 , 2022. 3
[53] Qinsheng Zhang, Jiaming Song, and Yongxin Chen. Im-
proved order analysis and design of exponential inte-
grator for diffusion models sampling. arXiv preprint
arXiv:2308.02157 , 2023. 3
[54] Minda Zhao, Chaoyi Zhao, Xinyue Liang, Lincheng
Li, Zeng Zhao, Zhipeng Hu, Changjie Fan, and Xin
9901
Yu. Efﬁcientdreamer: High-ﬁdelity and robust 3d cre-
ation via orthogonal-view diffusion prior. arXiv preprint
arXiv:2308.13223 , 2023. 3
[55] Joseph Zhu and Peiye Zhuang. Hifa: High-ﬁdelity text-
to-3d with advanced diffusion guidance. arXiv preprint
arXiv:2305.18766 , 2023. 2,3,4
9902
