DiffSal: Joint Audio and Video Learning for Diffusion Saliency Prediction
Junwen Xiong1,2, Peng Zhang1,2∗, Tao You1*, Chuanyue Li1, Wei Huang3, Yufei Zha1,2
1Northwestern Polytechnical University
2Ningbo Institute of Northwestern Polytechnical University
3Nanchang University
Abstract
Audio-visual saliency prediction can draw support from
diverse modality complements, but further performance en-
hancement is still challenged by customized architectures
as well as task-speciﬁc loss functions. In recent studies,
denoising diffusion models have shown more promising in
unifying task frameworks owing to their inherent ability of
generalization. Following this motivation, a novel Diffusion
architecture for generalized audio-visual Saliency predic-
tion (DiffSal) is proposed in this work, which formulates
the prediction problem as a conditional generative task of
the saliency map by utilizing input audio and video as the
conditions. Based on the spatio-temporal audio-visual fea-
tures, an extra network Saliency-UNet is designed to per-
form multi-modal attention modulation for progressive re-
ﬁnement of the ground-truth saliency map from the noisy
map. Extensive experiments demonstrate that the proposed
DiffSal can achieve excellent performance across six chal-
lenging audio-visual benchmarks, with an average rela-
tive improvement of 6.3% over the previous state-of-the-
art results by six metrics. The project url is https:
//junwenxiong.github.io/DiffSal .
1. Introduction
With the functionality of visual and auditory sensory sys-
tems, human beings can quickly focus on the most interest-
ing areas during their daily activities. Such a comprehensive
capability of visual attention in multi-modal scenarios has
been explored by numerous researchers and referred to as
anaudio-visual saliency prediction (A VSP) task. Based on
the related techniques, many valuable practical applications
have come into utility ranging from video summarization
[29] and compression [ 65] to virtual reality [ 15] and aug-
mented reality [ 46].
Signiﬁcant efforts have been dedicated to advancing
studies by concentrating on elevating the quality of multi-
modal interaction and reﬁning the generalizability of model
structures in this domain. Among the prevalent A VSP
*Corresponding author.
Audio 
Encoder
Video 
EncoderFusion Video 
Decoder
Multi-Scale Short ConnectionsAudio 
Encoder
Video 
EncoderLocalization 
and Fusion𝑲𝑳  𝑫𝒊𝒗
𝑪𝑬 + 𝑪𝑪 + 𝑵𝑺𝑺
(a) Localization-based Audio-Visual Saliency Prediction
(b) 3D Convolution-based Audio-Visual Saliency Prediction
(c) Diffusion-based Audio-Visual Saliency Prediction⋯
Estimation
𝑲𝑳  𝑫𝒊𝒗𝑲𝑳  𝑫𝒊𝒗 + 𝑪𝑪 + 𝑺𝑰𝑴
𝑲𝑳  𝑫𝒊𝒗 + 𝑪𝑪 + 𝑵𝑺𝑺or
oror
𝑴𝑺𝑬
Audio 
Encoder
Video 
Encoder
Conditional diffusion
 model
Noisy 
mapsFigure 1. Comparison of conventional audio-visual saliency pre-
diction paradigms and our proposed diffusion-based approach.
Both the localization-based and 3D convolution-based methods
use tailored network structures and sophisticated loss functions to
predict saliency areas. Differently, our diffusion-based approach
is a generalized audio-visual saliency prediction framework using
simple MSE objective function.
approaches, as depicted in Figure 1(a), localization-based
methods [ 38,39,50] have gained a lot of attention. These
methods typically consider the sounding objects as saliency
targets in the scene and transform the saliency prediction
task into a spatial sound source localization problem. Even
though the semantic interactions between audio and visual
modalities have been considered in these methods, their fo-
cus on a generalized network structure is still limited and
inevitably results in constrained performance.
In contrast, recent 3D convolution-based methods [ 10,
30,58,61] exhibit superior performance in predicting
audio-visual saliency maps, as illustrated in Figure 1(b).
However, these methods require customized architectures
with built-in inductive biases tailored for saliency predic-
tion tasks. For instance, Jain et al. [30] and Xiong et al.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
27273
[58] both embrace a 3D encoder-decoder structure akin to
UNet, but integrate their empirical designs into the decoder.
Moreover, both localization-based and 3D convolution-
based methods employ sophisticated loss functions, con-
tributing to a more intricate audio-visual saliency modeling
paradigm.
Effective audio-visual interaction and the generalized
saliency network are two essential factors for the seamless
application of A VSP technology in the real world. Unfor-
tunately, an in-depth exploitation of both challenges in the
existing works is far from sufﬁcient. Inspired by the strong
generalization capabilities, diffusion models [ 25,26,43]
can be employed as a uniﬁed framework for generative tasks
with class labels [ 17], text prompts [ 20], images [ 11], and
even sounds [ 45] as the conditions for modeling. However,
it remains an open question how to design a diffusion model
that satisﬁes the effective audio-visual interaction and the
generalized saliency network.
In this work, we present a conditional Diffusion model
for generalized audio-visual Saliency prediction (DiffSal),
which aims to tackle these two challenges simultaneously,
as illustrated in Figure 1(c). Our DiffSal utilizes the in-
put video and audio as the conditions to reformulate the
prediction problem as a conditional generative task of the
saliency map. During the training phase, the model is fed
the video and audio cues as well as a degraded saliency
map, which has been obtained from the ground-truth with
varying degrees of injected noise. By constructing a two-
stream encoder to explore audio and video feature pairs
with spatio-temporal coherence, the obtained similar pixel-
wise multi-modal features can be utilized to guide the dif-
fusion model generation process. In addition, a novel net-
work Saliency-UNet is employed to recover the original
saliency maps from noisy inputs, which utilizes information
from spatio-temporal audio and video features as the condi-
tions. To explore the latent semantic associations between
audio and video features, an effective multi-modal interac-
tion mechanism is proposed. The entire DiffSal framework
employs a simple mean square error loss to predict ground-
truth saliency maps from random noise. During the infer-
ence phase, following the reversed diffusion process, Diff-
Sal performs multi-step denoising to generate predictions
based on randomly generated noisy saliency maps.
Beneﬁting from such a diffusion-based framework, we
demonstrate two distinct properties that appeal to the A VSP
task. ( i) In contrast to existing methods with spatio-
temporal visual branching [ 30,50,58], DiffSal enables
spatio-temporal modeling of audio and video, and can be
generalized to audio-only, video-only, as well as audio-
visual scenarios. ( ii) Thanks to the iterative denoising prop-
erty of the diffusion model, DiffSal can iteratively reuse
Saliency-UNet to improve performance without retraining.
To summarize, our main contributions are: (1) We for-mulate the saliency prediction task as a conditional gen-
erative problem and propose a novel conditional diffusion
saliency model, which is beneﬁcial from the generalized
network structure and effective audio-visual interaction. (2)
We demonstrate two properties of DiffSal that are effective
on saliency prediction: the ability to be applied to either
uni-modal or multi-modal scenarios, and to perform ﬂex-
ible iterative reﬁnement without retraining. (3) Extensive
experiments have been conducted on six challenging audio-
visual benchmarks and the results demonstrate that DiffSal
achieves excellent performance, exhibiting an average rela-
tive improvement of 6.3% over the previous state-of-the-art
results across four metrics.
2. Related Work
2.1. Audio­Visual Saliency Prediction
For audio-visual saliency prediction, different strategies
for multi-modal correlation modeling have been proposed
to estimate the saliency maps over consecutive frames.
Early solutions [ 38,39] attempted to localize the moving-
sounding target by canonical correlation analysis(CCA)
to establish the cross-modal connections between the two
modalities. With the advent of deep learning, Tsiami et
al. [50] continued the localization-based approach by ex-
tracting audio representation using SoundNet [ 4], and fur-
ther performed spatial sound source localization through bi-
linear operations. Unfortunately, these methods exhibited
sub-optimal performance only and thus led to the emer-
gence of more effective 3D convolution-based approaches
[30,48,58] based on the encoder-decoder network frame-
works. Jain et al. [30] and Xiong et al. [58] both embrace
the UNet-style encoder-decoder structure by incorporating
their empirical designs into the decoder. Moreover, Chang
et al. [10] employs a complex hierarchical feature pyramid
network to aggregate deep semantic features. Considering
that both the localization-based and 3D convolution-based
methods use tailored network structures and sophisticated
loss functions to predict saliency areas. In this study, by
formulating the task as a conditional generation problem, a
novel conditional diffusion model is proposed for general-
ized audio-visual saliency prediction.
2.2. Diffusion Model
Recently, diffusion models have gained signiﬁcant traction
in the ﬁeld of deep learning. During diffusion modeling,
the Markov process is employed to introduce noise into the
training data followed by a training of deep neural networks
to reverse it. Thanks to the high-quality generative re-
sults and strong generalization capabilities, diffusion mod-
els have achieved an impressive performance in generative
tasks, such as image generation [ 3,6,7,12,16], image-
to-image translation [ 32,44,52,56,63], video generation
[23,27,60], text-to-image synthesis [ 20,42,62], and etc.
27274
Video Clips( 𝐼)
𝑇𝑣× 𝐻 𝑣× 𝑊 𝑣× 3Video 
Encoder{𝐟𝑣1, 𝐟𝑣2, 𝐟𝑣3, 𝐟𝑣4}
Audio 
Encoder𝐟𝑎
Audio Signals( 𝐴) 
Convert to log mel-
spectrogram frames
𝑆𝑇 ……𝑆0𝓛
መ𝑆0
Diffusion
DenoisingSaliency-UNet
መ𝑆0= 𝑔 𝜓(𝑆𝑡,𝑡,𝐟𝑣,𝐟𝑎)
𝐟𝑣=𝑉𝑖𝑑𝑒𝑜𝐸𝑛𝑐𝑜𝑑𝑒𝑟 𝐼
𝐟𝑎=𝐴𝑢𝑑𝑖𝑜𝐸𝑛𝑐𝑜𝑑𝑒𝑟 (𝐴)
Conditions: 𝑡,𝐟𝑣,𝐟𝑎
𝓝(𝟎, 𝑰)
𝑇𝑎× 𝐻 𝑎× 𝑊 𝑎× 1Conv MLP𝑆𝑡 𝑡
⋯⋯
⋮
Spatial DownResNet Block
ℎ𝑣𝑖× 𝑤 𝑣𝑖× 𝐶𝑣𝑖
⋯⋯Multi-modal
Attention Block
Spatial UpTemporal Conv(𝑇𝑣𝑖+ 1) × ℎ 𝑣𝑖× 𝑤 𝑣𝑖× 𝐶𝑣𝑖
⋯
⋯
⋯⋮⋯
⋯
⋯
𝐟𝑎𝐟𝑣2𝐟𝑎𝐟𝑣4
𝐟𝑎𝐟𝑣1መ𝑆0
Prediction Head
Conv&Up𝐟𝑎
∗ 𝐟𝑣𝑖 Pool 𝜎∗
𝐟𝑠𝑖Cat
Multi-modal Interaction Module
Cat Concatenation ∗ Element-wise product 𝜎 Softmax 𝑆0 Data Sample 𝑡 Sampling Step Pool Average Pooling𝐟𝑠4
𝐟𝑠2
𝐟𝑠1Saliency-UNet Video and Audio Encoders
𝐟𝑎𝑣𝑠𝑖𝐟𝑣𝑖
𝐟𝑠𝑖
𝐟𝑎
Efficient  Spatio-Temporal Cross-AttentionCat
MIMEfficient
Cross-Attention
𝐾𝑄
𝑉MLP
Updated 
Features
MIM Multi-modal Interaction Module 𝐟𝑎𝑣𝑠𝑖
Temporal 
Enhancement
Multi-modal 
Attention 
ModulationResNet 
StageFigure 2. An overview of the proposed DiffSal framework. DiffSal ﬁrst encodes spatio-temporal video features fvand audio features fa
by the Video and Audio Encoders, respectively. Then the Saliency-UNet takes audio features faand video features fvas the conditions to
guide the network in generating the saliency map ˆS0from the noisy map St.
Beyond generative tasks, diffusion models have proven to
be highly effective in various computer vision tasks. For in-
stance, DiffSeg [ 1] proposes a diffusion model conditioned
on an input image for image segmentation. Chen et al. [11]
propose a model named DiffusionDet, which formulates ob-
ject detection as a generative denoising process from noisy
boxes to object boxes. Subsequently, the pipeline of this
model is extended by Gu et al. [21] by introducing noise ﬁl-
ters during diffusion, as well as incorporating a mask branch
for global mask reconstruction, which makes making Diffu-
sionDet more applicable to instance segmentation tasks. To
the best of our knowledge, there have been no previous suc-
cessful attempts to apply diffusion models to saliency pre-
diction, which inspires the proposed DiffSal in this work to
explore the potential of diffusion models in the domain of
audio-visual saliency prediction.
3. Preliminaries
Diffusion models [ 26] are likelihood-based models for
points sampling from a given distribution by gradually de-
noising random Gaussian noise in Tsteps. In the forward
diffusion process, the increased noises are added to a sam-
ple point x0iteratively as x0→ ··· → xT−1→xT, to
obtain a completely noisy image xT. Formally, the forward
diffusion process is a Markovian noising process deﬁned by
a list of noise scales {¯at}T
t=1as:
q(xt|x0) :=N(xt|√¯αtx0,(1−¯αt)I),
xt=√¯αtx0+√
1−¯αtϵ,ϵ∈ N(0,I),(1)where¯αt:=/producttextt
s=1αt=/producttextt
s=1(1−βs)andβsdenote
the noise variance schedule [ 26],ϵis the noise, Ndenotes
normal distribution, x0is the original image, and xtis noisy
image after tsteps of the diffusion process.
The reverse diffusion process aims to learn the posterior
distribution p(xt−1|x0,xt)forxt−1estimation given xt.
Typically, this can be done using a step-dependent neural
network in multiple parameterized ways. Instead of directly
predicting the noise ϵ, we choose to parameterize the neural
networkfθ(xt,t)to predict x0as [11]. For model opti-
mization, a mean squared error loss is employed to match
fθ(xt,t)andx0:
L=∥fθ(xt,t)−x0∥2,t∈R{1,2,...,T}, (2)
where the step tis randomly selected at each training itera-
tion. From starting with a pure noise xt∈ N(0,I)during
inference stage, the model can gradually reduce the noise
according to the update rule [ 47] using the trained fθas be-
low:
xt−1=√¯αt−1fθ(xt,t)+
/radicalBig
1−¯αt−1−σ2
txt−√¯αtfθ(xt,t)√1−¯αt+σtϵ.(3)
Iteratively applying Eq. 3, a new sample x0can be gen-
erated from fθvia a trajectory xT→xT−1→ ··· → x0.
Specially, some improved sampling strategies skip such an
operation in the trajectory to achieve better efﬁciency [ 47].
27275
To control the generation process, the conditional infor-
mation can be modeled and incorporated in the diffusion
model as an extra input fθ(xt,t,C). The class labels [ 17],
text prompts [ 20], and audio guidance [ 2] are the prevalent
forms of conditional information documented in the litera-
ture.
4. Method
To tackle the challenges of effective audio-visual interaction
and saliency network generalization, we formulate audio-
visual saliency prediction as a conditional generation mod-
eling of the saliency map, which treats the input video and
audio as the conditions. Figure 2illustrates the overview
of the proposed DiffSal, which contains parts of Video and
Audio Encoders as well as Saliency-UNet. The former is
used to extract multi-scale spatio-temporal video features
and audio features from image sequences and correspond-
ing audio signals. By conditioning on these semantic video
and audio features, the latter performs multi-modal atten-
tion modulation to progressively reﬁne the ground-truth
saliency map from the noisy map. Each part of DiffSal is
elaborated on below.
4.1. Video and Audio Encoders
Video Encoder . LetI= [I1,···,Ij,···,ITv],Ij∈
RHv×Wv×3denotes an RGB video clip of length Tv. This
serves as the input to a video backbone network, which pro-
duces spatio-temporal feature maps. The backbone consists
of 4 encoder stages and outputs 4 hierarchical video fea-
ture maps, illustrated in Figure 2. The generated feature
maps are denoted as {fi
v}N
i=1∈RTi
v×hi
v×wi
v×Ci
v, where
(hi
v,wi
v) = (Hv,Wv)/2i+1,N= 4. In practical imple-
mentation, we employ the off-the-shelf MViTv2 [ 33] as
a video encoder to encode the spatial and temporal infor-
mation of image sequences. More generally, MViTv2 can
also be replaced with other general-purpose encoders, e.g.,
S3D[ 57], Video Swin Transformer[ 34].
Audio Encoder . To temporally synchronize the audio fea-
tures with the video features in a better way, initially, we
transform the raw audio into a log-mel spectrogram through
Short-Time Fourier Transform (STFT). Then, the spec-
trogram is partitioned into Taslices of dimension Ha×
Wa×1with a hop-window size of 11 ms. To extract
per-frame audio feature ¯fa,iwherei∈ {1,···,Ta}, a pre-
trained 2D fully convolutional VGGish network [ 24] is per-
formed on AudioSet [ 19], resulting in a feature map of size
Rha×wa×Ca. To improve the inter-frame consistency, we
further introduce a temporal enhancement module consist-
ing of a patch embedding layer as well as a transformer
layer. Then, the audio features are rearranged in the spatio-
temporal dimension and fed into the patch embedding layer
to obtain¯fa∈RTa×ha×wa×Ca. For the retaining of tempo-
ral position information, a learnable positional embeddingeposis incorporated along the temporal dimension:
¯fa= [¯fa,0+epos
0,···,¯fa,Ta+epos
Ta], (4)
where[·,·]represents concatenation operation. The pro-
cessed feature is ﬁnally fed into the Multi-head Self At-
tention (MSA), the layer normalization (LN) [ 5] and the
MLP layer to produce the spatio-temporal audio features
fa∈RTa×ha×wa×Ca:
¯fa=MSA(LN(¯fa))+¯fa,
fa=MLP(LN(¯fa))+¯fa.(5)
4.2. Saliency­UNet
To learn the underlying distribution of saliency maps, we
design a novel conditional denoising network gψwith
multi-modal attention modulation, named Saliency-UNet.
This network is designed to leverage both audio features fa
and video features {fi
v}N
i=1as the conditions, guiding the
network in generating the saliency map ˆS0from the noisy
mapSt:
ˆS0=gψ(St,t,fa,fv), (6)
whereSt=√¯αtS0+√1−¯αtϵ, noiseϵis from a Gaussian
distribution, and t∈R{1,2,...,T}is a random diffusion
step.
Our Saliency-UNet can be functionally divided into two
parts: feature encoding and feature decoding, as shown in
Figure 2. The ﬁrst part encodes multi-scale noise feature
maps{fi
s}N
i=1∈Rhi
v×wi
v×Ci
vfrom the noisy map Stus-
ing multiple ResNet stages. The second part utilizes our
designed multi-modal attention modulation (MAM) across
multiple scales for the interaction of noise features, audio
features, and video features. The MAM stage comprises
an upsampling layer, a multi-modal attention block, and a
3D temporal convolution. This stage not only computes
the global spatio-temporal correlation between multi-modal
features but also progressively enhances the spatial resolu-
tion of the feature maps. At last, a prediction head is em-
ployed to produce the predicted saliency map ˆS0. The entire
network incorporates 4 layers of the ResNet stage for fea-
ture encoding and 4 layers of the MAM stage for feature
decoding.
For more robust multi-modal feature generation, two
techniques in MAM are introduced: efﬁcient spatio-
temporal cross-attention and multi-modal interaction mod-
ule.
Efﬁcient Spatio-Temporal Cross-Attention . Given video
features fi
v∈RTi
v×hi
v×wi
v×Ci
v, audio features fa∈
RTa×ha×wa×Caand noise features fi
s∈Rhi
v×wi
v×Ci
v,
the video features and noise features are concate-
nated as a spatio-temporal noise feature map [fi
v,fi
s]∈
R(Ti
v+1)×hi
v×wi
v×Ci
v. The processed feature map is then
fed into the multi-modal interaction module along with the
27276
audio features to obtain favs∈R(Ti
v+1)×hi
v×wi
v×Ci
v. To
introduce audio information in the saliency prediction, we
design an efﬁcient spatio-temporal cross-attention (ECA),
which not only effectively reduces the computational over-
head of the standard cross-attention [ 51], but also possesses
spatio-temporal interactions between features.
For all processed features, they are ﬁrst converted to
2D feature sequences in the spatio-temporal domain and
obtained Q=V= [fi
v,fi
s]∈R(Ti
v+1)hi
vwi
v×Ci
v,K=
fi
avs∈R(Ti
v+1)hi
vwi
v×Ci
v. We ﬁnd that directly following
the standard cross-attention would take a prohibitively large
amount of memory due to the high spatio-temporal resolu-
tion of the feature map. Therefore, a spatio-temporal com-
pression technique is employed that can effectively reduce
the computational overhead without compromising perfor-
mance, as:
[fi
v,fi
s] =ECA(QWQ,STC(K)WK,STC(V)WV), (7)
whereWQ,WK,WV∈RCi×Ciare parameters of lin-
ear projections, STC(·)is the spatio-temporal compression,
which is deﬁned as:
STC(x) =LN(Conv3d(x)), (8)
Here, the dimension of features is reduced by controlling
the kernel size as well as the stride size of 3D convolution.
Multi-modal Interaction Module . To take full advantage
of different modal features, we model the interaction be-
tween audio features fa, video features fi
vand noise fea-
tures fi
sat each scale ito obtain a robust multi-modal fea-
ture representation. A straightforward approach would be to
directly concatenate and aggregate fa,fi
vandfi
s, but this fu-
sion method does not acquire global correlations between
various modalities. Therefore, an effective multi-modal
interaction strategy is proposed to capture crucial audio-
visual activity changes in the spatio-temporal domain. In
speciﬁc, this process starts with convolution and upsam-
pling on the audio features, which is to construct spatially
size-matched feature triples (/tildewidefa,fi
v,fi
s). Subsequently, the
video features and noise features are concatenated to obtain
spatio-temporal noise features [fi
v,fi
s]. This processed fea-
ture undergoes an element-wise product operation with the
audio features, resulting in ¯fi
avs. Following this, we perform
average activation along the temporal dimension over ¯fi
avs
to pool global temporal information into a temporal descrip-
tor. For the indication of critical motion regions, a softmax
function is applied to obtain a mask by highlighting the seg-
ments of the corresponding spatio-temporal audio features,
which exhibit key audio-visual activity changes:
/tildewidefa=Conv(UpSample (fa)),
fi
avs=softmax(Pool([fi
v,fi
s]∗/tildewidefa))∗/tildewidefa.(9)whereConv(·),∗,softmax andPool denote the operations
of convolution, element-wise product, softmax and average
pooling, respectively.
4.3. Overall Training and Inference Algorithms
Training . A diffusion process is initiated to create noisy
maps by introducing corruption to ground-truth saliency
maps. To reverse this process, the Saliency-UNet is trained
for saliency map denoising. The overall training procedure
of DiffSal is outlined in Algorithm 1in the Appendix. In
detail, Gaussian noises are sampled following αtin Eq. 1
and added to the ground-truth saliency maps, resulting in
noisy samples. At each sampling step t, the parameter αtis
pre-deﬁned by a monotonically decreasing cosine scheme,
as employed in [ 26]. The standard mean square error serves
as the optimization function to supervise the model training:
L=∥S0−gψ(St,t,fa,fv)∥2. (10)
whereS0andgψ(St,t,fa,fv)denote the ground-truth and
predicted saliency maps, respectively.
Inference . The proposed DiffSal engages in denoising
noisy saliency maps sampled from a Gaussian distribu-
tion, and progressively reﬁnes the corresponding predic-
tions across multiple sampling steps. In each sampling step,
the Saliency-UNet processes random noisy saliency maps
or the predicted saliency maps from the previous sampling
step as input and generates the estimated saliency maps for
the current step. For the next step, DDIM [ 47] is applied to
update the saliency maps. The detailed inference procedure
is outlined in Algorithm 2.
5. Experiments
Experiments are conducted on six audio-visual datasets.
The following subsections introduce the implementation
details and evaluation metrics. The experimental results are
represented with analysis through ablation studies and com-
parison with state-of-the-art works.
5.1. Setup
Audio-Visual Datasets : Six audio-visual datasets in
saliency prediction have been employed for the evaluation,
which are: A V AD [ 38], Coutrot1 [ 13], Coutrot2 [ 14], DIEM
[40], ETMD [ 31], and SumMe [ 22]. The signiﬁcant char-
acteristics of these datasets are elaborated below. (i) The
A V AD dataset comprises 45 video clips with durations rang-
ing from 5 to 10 seconds. These clips cover various audio-
visual activities, such as playing the piano, playing bas-
ketball, conducting interviews, etc. This dataset contains
eye-tracking data from 16 participants. (ii) The Coutrot1
and Coutrot2 datasets are derived from the Coutrot dataset.
Coutrot1 consists of 60 video clips covering four visual cat-
egories: one moving object, several moving objects, land-
scapes, and faces. The corresponding eye-tracking data are
27277
Table 1. Ablation of different components in DiffSal.
MethodComponents A V AD ETMD
ECA MIM CC ↑SIM↑CC↑SIM↑
baseline 0.701 0.547 0.632 0.498
✓ 0.716 0.556 0.644 0.503
✓ 0.714 0.551 0.638 0.502
✓ ✓ 0.738 0.571 0.652 0.506
obtained from 72 participants. Coutrot2 includes 15 video
clips recording four individuals having a meeting, with eye-
tracking data from 40 participants. (iii) The DIEM dataset
contains 84 video clips, including game trailers, music
videos, advertisements, etc., captured from 42 participants.
Notably, the audio and visual tracks in these videos do not
naturally correspond. (iv) The ETMD dataset comprises 12
video clips extracted from various Hollywood movies, with
eye-tracking data annotated by 10 different persons. (v) The
SumMe dataset consists of 25 video clips covering diverse
topics, such as playing ball, cooking, travelling, etc.
Implementation Details : To facilitate implementation, a
pre-trained MViTv2 [ 33] model on Kinetics [ 9] and a pre-
trained VGGish [ 24] on AudioSet [ 19] are adopted. The in-
put samples of the network consist of 16-frame video clips
of size224×384×3with the corresponding audio, which
is transformed into 9slices of 112×192 log-Mel spec-
trograms. For the spatio-temporal compression in efﬁcient
spatio-temporal cross-attention, the kernel size and stride
size of the 3D convolution in the ith MAM stage are set
to2iand2i, respectively. For a fair comparison, the video
branch of DiffSal is pre-trained using the DHF1k dataset
[53] following [ 58], and the entire model is ﬁne-tuned on
these audio-visual datasets using this pre-trained weight.
The training process chooses Adam as the optimizer with
the started learning rate of 1e−4. The computation platform
is conﬁgured by four NVIDIA GeForce RTX 4090 GPUs
in a distributed fashion, using Pytorch. The total sampling
stepTis deﬁned as 1000 and the entire training is termi-
nated within 5 epochs. The batch size is set to 20 across all
experiments. During inference, the iterative denoising step
is set to 4.
Evaluation Metrics : Following previous works, four
widely-used evaluation metrics are adopted [ 8]: CC, NSS,
AUC-Judd (AUC-J), and SIM. The same evaluation codes
are used as in previous works [ 50,58].
5.2. Ablation Studies
Extensive ablation studies are performed to validate the de-
sign choices in the method. The A V AD and ETMD datasets
are selected for ablation studies, following the approach in
[58].
Effect of Components of DiffSal . To validate the effec-
tiveness of each module in the proposed framework, a base-Table 2. Ablation of video and audio modalities.
ModelA V AD ETMD
CC↑SIM↑CC↑SIM↑
Audio-Only 0.343 0.283 0.365 0.295
Video-Only 0.716 0.556 0.644 0.503
Ours 0.738 0.571 0.652 0.506
GT
Audio-OnlyVideo
Audio
Video-Only
Ours
Figure 3. Visualizing the saliency results when different modal-
ities are used. The audio-only approach can localize the sound
source coming from the performer’s guitar, while the video-only
approach focuses on both the performer’s face as well as the gui-
tar.
line model is initially deﬁned as the video-only version
of DiffSal and replaces the multi-modal attention modula-
tion in Saliency-UNet with a pure convolution operation.
As shown in Table 1, the baseline model demonstrates a
good performance that indicates a potential capability of the
diffusion-based framework in the A VSP task. With the in-
corporation of the designed efﬁcient spatio-temporal cross-
attention (ECA), and the multi-modal interaction module
(MIM) components, the overall performance of the model
has been enhanced continually. As the core module, ECA
presents a signiﬁcant improvement in the CC metric by
0.015 on the A V AD dataset, and 0.012 on the ETMD dataset
for the whole DiffSal framework. With the addition of the
audio features and the MIM, the model also has another im-
provement of 0.022 in the CC metric on the A V AD dataset.
All of these have demonstrated the effectiveness of ECA
and MIM in the proposed DiffSal.
Effect of Video and Audio Modalities . Table 2shows
the contribution of spatio-temporal information from each
modality in the Video and Audio Encoders to the over-
all performance. The experimental observations reveal that
27278
Table 3. Ablation of different multi-modal interaction methods.
MethodA V AD ETMD
CC↑SIM↑CC↑SIM↑
DiffSal(w/ MIM) 0.738 0.571 0.652 0.506
w/ Bilinear 0.716 0.556 0.644 0.503
w/ Addition 0.706 0.543 0.606 0.464
w/ Concatenation 0.704 0.528 0.610 0.432
Table 4. Ablation of different cross-attention strategies. The
computational cost is evaluated based on input audio of size
1×9×112×192 and video of size 3×16×224×384. #
Params and #Mem denote the number of parameters and memory
footprint of the model, respectively.
Attention #Params #MemA V AD ETMD
CC↑SIM↑CC↑SIM↑
SCA 76.43M 5.32G 0.713 0.531 0.628 0.476
ECA 76.57M 1.20G 0.738 0.571 0.652 0.506
the video-only model exhibits signiﬁcantly greater strength
than the spatio-temporal audio-only version, which veriﬁes
the essential role of the video modality. Figure 3also vi-
sualizes the model predictions utilizing the two modal en-
coders separately. It is clear that either the audio-only or
video-only approach can predict saliency areas in the scene,
and the combination of the two modalities leads to more ac-
curate predictions. This demonstrates the generalization of
the DiffSal framework to audio-only, video-only, and audio-
visual scenarios as well.
Effect of Different Cross-Attention Strategies . The de-
sign of efﬁcient spatio-temporal cross-attention mechanism
is further evaluated. As shown in Table 4, using efﬁcient
spatio-temporal cross-attention (ECA) not only leads to bet-
ter performance but also greatly reduces the memory foot-
print of the model compared to using the standard cross-
attention (SCA) strategy. This shows that the designed ECA
can compress the effective spatio-temporal cues in the fea-
tures and reduce the interference of irrelevant noises.
Effect of Different Multi-modal Interaction Methods .
The effects of using different multi-modal interaction meth-
ods, such as Bilinear [ 49], Addition, and Concatenation, are
compared in Table 3. These multi-modal interaction meth-
ods can be found in recent state-of-the-art works [ 30,50],
and the video features and noise features are ﬁrstly con-
catenated before their interaction with audio features. Ex-
perimental results show that the proposed MIM can out-
perform all the other three interaction methods, and obtain
more robust multi-modal features. In contrast, the perfor-
mance degradation of the other three methods suffers from
the noise information embedded in the features.
Effect of Different Training Losses . Table 5compares
the impact on DiffSal using different loss functions fromTable 5. Ablation of different training losses.
ModelLosses A V AD ETMD
LCELKLLMSE CC↑SIM↑CC↑SIM↑
Ours✓ 0.690 0.490 0.617 0.422
✓ 0.720 0.552 0.644 0.496
✓ 0.738 0.571 0.652 0.506
/uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013 /uni00000014/uni00000015
/uni00000051/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000004c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b/uni00000031/uni0000000c/uni00000013/uni00000011/uni00000019/uni00000013/uni00000013/uni00000013/uni00000011/uni00000019/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000013/uni00000026/uni00000026
/uni00000024/uni00000039/uni00000024/uni00000027
/uni00000027/uni0000002c/uni00000028/uni00000030
/uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013 /uni00000014/uni00000015
/uni00000051/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000004c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b/uni00000031/uni0000000c/uni00000013/uni00000011/uni00000018/uni00000014/uni00000013/uni00000013/uni00000011/uni00000018/uni00000017/uni00000013/uni00000013/uni00000011/uni00000018/uni0000001a/uni00000013/uni00000013/uni00000011/uni00000019/uni00000013/uni00000013/uni00000036/uni0000002c/uni00000030
/uni00000024/uni00000039/uni00000024/uni00000027
/uni00000027/uni0000002c/uni00000028/uni00000030
Figure 4. Performance analysis of denoising steps on A V AD and
DIEM datasets.
previous state-of-the-art approaches [ 50,58], such as the
cross entropy (CE) loss LCEand the Kullback-Leibler di-
vergence (KL) loss LKL. During the training process, it is
observed that the model with the LCEconverges slowly and
yields sub-optimal performance. Compared to the LMSE ,
employing the LKLcan achieve acceptable results, but
there is still a gap in the performance of training with the
LMSE . This suggests that simple MSE loss can be used in
the A VSP task as an alternative to these task-tailored loss
functions.
Effect of Denoising Steps . The impact of the number of
iterative denoising steps on the ﬁnal performance is studied
in Figure 4, which shows that more iteration steps result in
better performance. With diminishing marginal beneﬁts as
the step number increases, a steady increase in performance
is observed. For a linearly increasing of the computational
cost with the step number, N= 4is used to maintain a good
balance between performance and computational cost.
5.3. Method Comparison
Comparisons with State-of-the-art Methods . As shown
in Table 6, the experimental results of our DiffSal are com-
pared with recent state-of-the-art works on six audio-visual
saliency datasets. The table highlights the superiority of
DiffSal, as it outperforms the other comparable works on all
datasets by deﬁned metrics. Notably, DiffSal signiﬁcantly
surpasses the previous top-performing methods, such as
CASP-Net [ 58] and ViNet [ 30], and becomes the new state-
of-the-art on these six benchmarks. The performance boost
is very encouraging: DiffSal can achieve an average rela-
tive performance improvement of up to 6.3% compared to
the second-place performer. Such substantial improvements
validate the effectiveness of the diffusion-based approach as
an effective audio-visual saliency prediction framework.
Qualitative Results . The ability of the model to handle
challenging scenarios, such as fast movement on the ten-
27279
Table 6. Comparison with state-of-the-art methods on six audio-visual saliency datasets. Bold text in the table indicates the best result, and
underlined text indicates the second best result. Our DiffSal signiﬁcantly outperforms the previous state-of-the-arts by a large margin.
Method #Params #FLOPsDIEM Coutrot1 Coutrot2
CC↑NSS↑AUC-J↑SIM↑CC↑NSS↑AUC-J↑SIM↑ CC↑ NSS↑AUC-J↑SIM↑
ACLNet TPAMI′2019 [54] - - 0.522 2.02 0.869 0.427 0.425 1.92 0.850 0.361 0.448 3.16 0.926 0.322
TASED-Net ICCV′2019 [37] 21.26M 91.80G 0.557 2.16 0.881 0.461 0.479 2.18 0.867 0.388 0.437 3.17 0.921 0.314
STA ViS CVPR′2020 [50] 20.76M 15.31G 0.579 2.26 0.883 0.482 0.472 2.11 0.868 0.393 0.734 5.28 0.958 0.511
ViNetIROS′2021 [30] 33.97M 115.31G 0.632 2.53 0.899 0.498 0.56 2.73 0.889 0.425 0.754 5.95 0.951 0.493
TSFP-Net arXiv′2021 [10] - - 0.651 2.62 0.906 0.527 0.571 2.73 0.895 0.447 0.743 5.31 0.959 0.528
CASP-Net CVPR′2023 [58] 51.62M 283.35G 0.655 2.61 0.906 0.543 0.561 2.65 0.889 0.456 0.788 6.34 0.963 0.585
Ours (DiffSal) 76.57M 187.31G 0.660 2.65 0.907 0.543 0.638 3.20 0.901 0.515 0.835 6.61 0.964 0.625
Method #Params #FLOPsA V AD ETMD SumMe
CC↑NSS↑AUC-J↑SIM↑CC↑NSS↑AUC-J↑SIM↑ CC↑ NSS↑AUC-J↑SIM↑
ACLNet TPAMI′2019 [54] - - 0.580 3.17 0.905 0.446 0.477 2.36 0.915 0.329 0.379 1.79 0.868 0.296
TASED-Net ICCV′2019 [37] 21.26M 91.80G 0.601 3.16 0.914 0.439 0.509 2.63 0.916 0.366 0.428 2.1 0.884 0.333
STA ViS CVPR′2020 [50] 20.76M 15.31G 0.608 3.18 0.919 0.457 0.569 2.94 0.931 0.425 0.422 2.04 0.888 0.337
ViNetIROS′2020 [30] 33.97M 115.31G 0.674 3.77 0.927 0.491 0.571 3.08 0.928 0.406 0.463 2.41 0.897 0.343
TSFP-Net arXiv′2021 [10] - - 0.704 3.77 0.932 0.521 0.576 3.07 0.932 0.428 0.464 2.30 0.894 0.360
CASP-Net CVPR′2023 [58] 51.62M 283.35G 0.691 3.81 0.933 0.528 0.620 3.34 0.940 0.478 0.499 2.60 0.907 0.387
Ours (DiffSal) 76.57M 187.31G 0.738 4.22 0.935 0.571 0.652 3.66 0.943 0.506 0.572 3.14 0.921 0.447
GT
Ours
CASP-Net
ViNet
STAViS
Figure 5. Qualitative results of our method compared with other state-of-the-art works. Challenging scenarios involving fast movement on
the tennis court and multiple speakers indoors.
nis court and multiple speakers indoors, is further exam-
ined. Figure 5compares the DiffSal against other state-of-
the-art approaches, such as CASP-Net [ 58], ViNet [ 30] and
STA ViS [ 50]. It is observed that DiffSal produces saliency
maps much closer to the ground-truth for various challeng-
ing scenes. In contrast, CASP-Net focuses mainly on audio-
visual consistency and lacks adopting an advanced network
structure, leading to sub-optimal results. STA ViS is only
able to generate unsurprisingly saliency maps by employ-
ing sound source localization. More visualization results
can be found in the supplementary.
Efﬁciency Analysis . Table 6compares the number of pa-
rameters and computational costs of the DiffSal with pre-
vious state-of-the-art works. Compared to CASP-Net, the
computational complexity of DiffSal is at a moderate level,
even though incorporating Saliency-UNet in DiffSal leads
to an increase in the number of model parameters. From
a performance perspective, the DiffSal model achieves the
best performance with the second-highest computationalcomplexity.
6. Conclusion
In this work, we introduce a novel Diffusion architecture for
generalized audio-visual Saliency prediction (DiffSal), for-
mulating the prediction problem as a conditional generative
task of the saliency map by utilizing input video and audio
as conditions. The framework involves extracting spatio-
temporal video and audio features from image sequences
and corresponding audio signals. A Saliency-UNet is de-
signed to perform multi-modal attention modulation, pro-
gressively reﬁning the ground-truth saliency map from the
noisy map. Extensive experiments have proven that DiffSal
achieves superior performance compared to previous state-
of-the-art methods in six challenging audio-visual bench-
marks.
Acknowledgements. This work was supported by NSFC
under Grants 62271239, 61862043, 62171382 and Jiangxi
Double Thousand Plan under Grant JXSQ2023201022.
27280
References
[1] Tomer Amit, Tal Shaharbany, Eliya Nachmani, and Lior
Wolf. Segdiff: Image segmentation with diffusion proba-
bilistic models. arXiv preprint arXiv:2112.00390 , 2021. 3
[2] Tenglong Ao, Zeyi Zhang, and Libin Liu. Gesturediffuclip:
Gesture diffusion model with clip latents. arXiv preprint
arXiv:2303.14613 , 2023. 4
[3] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tar-
low, and Rianne Van Den Berg. Structured denoising dif-
fusion models in discrete state-spaces. Advances in Neural
Information Processing Systems , 34:17981–17993, 2021. 2
[4] Yusuf Aytar, Carl V ondrick, and Antonio Torralba. Sound-
net: Learning sound representations from unlabeled video.
Advances in neural information processing systems , 29,
2016. 2
[5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. Layer normalization. arXiv preprint arXiv:1607.06450 ,
2016. 4
[6] Yaniv Benny and Lior Wolf. Dynamic dual-output diffu-
sion models. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 11482–
11491, 2022. 2
[7] Sam Bond-Taylor, Peter Hessey, Hiroshi Sasaki, Toby P
Breckon, and Chris G Willcocks. Unleashing transform-
ers: Parallel token prediction with discrete absorbing diffu-
sion for fast high-resolution image generation from vector-
quantized codes. In European Conference on Computer Vi-
sion, pages 170–188. Springer, 2022. 2
[8] Zoya Bylinskii, Tilke Judd, Aude Oliva, Antonio Torralba,
and Fr ´edo Durand. What do different evaluation metrics tell
us about saliency models? IEEE transactions on pattern
analysis and machine intelligence , 41(3):740–757, 2018. 6
[9] Joao Carreira and Andrew Zisserman. Quo vadis, action
recognition? a new model and the kinetics dataset. In pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 6299–6308, 2017. 6
[10] Qinyao Chang and Shiping Zhu. Temporal-spatial fea-
ture pyramid for video saliency detection. arXiv preprint
arXiv:2105.04213 , 2021. 1,2,8
[11] Shoufa Chen, Peize Sun, Yibing Song, and Ping Luo. Dif-
fusiondet: Diffusion model for object detection. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 19830–19843, 2023. 2,3
[12] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon
Kim, Hyunwoo Kim, and Sungroh Yoon. Perception pri-
oritized training of diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 11472–11481, 2022. 2
[13] Antoine Coutrot and Nathalie Guyader. How saliency, faces,
and sound inﬂuence gaze in dynamic social scenes. Journal
of vision , 14(8):5–5, 2014. 5
[14] Antoine Coutrot and Nathalie Guyader. Multimodal saliency
models for videos. In From Human Attention to Computa-
tional Attention , pages 291–304. Springer, 2016. 5
[15] Ana De Abreu, Cagri Ozcinar, and Aljosa Smolic. Look
around you: Saliency maps for omnidirectional images in
vr applications. In 2017 Ninth International Conferenceon Quality of Multimedia Experience (QoMEX) , pages 1–6,
2017. 1
[16] Valentin De Bortoli, James Thornton, Jeremy Heng, and Ar-
naud Doucet. Diffusion schr ¨odinger bridge with applications
to score-based generative modeling. Advances in Neural In-
formation Processing Systems , 34:17695–17709, 2021. 2
[17] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems , 34:8780–8794, 2021. 2,4
[18] Richard Droste, Jianbo Jiao, and J Alison Noble. Uniﬁed im-
age and video saliency modeling. In Computer Vision–ECCV
2020: 16th European Conference, Glasgow, UK, August 23–
28, 2020, Proceedings, Part V 16 , pages 419–435. Springer,
2020. 3
[19] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren
Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal,
and Marvin Ritter. Audio set: An ontology and human-
labeled dataset for audio events. In 2017 IEEE interna-
tional conference on acoustics, speech and signal processing
(ICASSP) , pages 776–780. IEEE, 2017. 4,6
[20] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo
Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-
tor quantized diffusion model for text-to-image synthesis. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 10696–10706, 2022. 2,
4
[21] Zhangxuan Gu, Haoxing Chen, Zhuoer Xu, Jun Lan,
Changhua Meng, and Weiqiang Wang. Diffusioninst: Dif-
fusion model for instance segmentation. arXiv preprint
arXiv:2212.02773 , 2022. 3
[22] Michael Gygli, Helmut Grabner, Hayko Riemenschneider,
and Luc Van Gool. Creating summaries from user videos.
InEuropean conference on computer vision , pages 505–520.
Springer, 2014. 5
[23] William Harvey, Saeid Naderiparizi, Vaden Masrani, Chris-
tian Weilbach, and Frank Wood. Flexible diffusion modeling
of long videos. Advances in Neural Information Processing
Systems , 35:27953–27965, 2022. 2
[24] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F
Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal,
Devin Platt, Rif A Saurous, Bryan Seybold, et al. Cnn archi-
tectures for large-scale audio classiﬁcation. In 2017 ieee in-
ternational conference on acoustics, speech and signal pro-
cessing (icassp) , pages 131–135. IEEE, 2017. 4,6
[25] Jonathan Ho and Tim Salimans. Classiﬁer-free diffusion
guidance. arXiv preprint arXiv:2207.12598 , 2022. 2
[26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840–6851, 2020. 2,3,5
[27] Tobias H ¨oppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen,
and Andrea Dittadi. Diffusion models for video prediction
and inﬁlling. arXiv preprint arXiv:2206.07696 , 2022. 2
[28] Feiyan Hu, Simone Palazzo, Federica Proietto Salanitri, Gio-
vanni Bellitto, Morteza Moradi, Concetto Spampinato, and
Kevin McGuinness. Tinyhd: Efﬁcient video saliency pre-
diction with heterogeneous decoders using hierarchical maps
distillation. In Proceedings of the IEEE/CVF Winter Confer-
27281
ence on Applications of Computer Vision , pages 2051–2060,
2023. 1,2,3
[29] Hou-Ning Hu, Yen-Chen Lin, Ming-Yu Liu, Hsien-Tzu
Cheng, Yung-Ju Chang, and Min Sun. Deep 360 pilot:
Learning a deep agent for piloting through 360deg sports
videos. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 3451–3460, 2017. 1
[30] Samyak Jain, Pradeep Yarlagadda, Shreyank Jyoti, Shyam-
gopal Karthik, Ramanathan Subramanian, and Vineet
Gandhi. Vinet: Pushing the limits of visual modality for
audio-visual saliency prediction. In 2021 IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems (IROS) ,
pages 3520–3527. IEEE, 2021. 1,2,7,8,3
[31] Petros Koutras and Petros Maragos. A perceptually based
spatio-temporal computational framework for visual saliency
estimation. Signal Processing: Image Communication , 38:
15–31, 2015. 5
[32] Bo Li, Kaitao Xue, Bin Liu, and Yu-Kun Lai. Vqbb: Image-
to-image translation with vector quantized brownian bridge.
arXiv preprint arXiv:2205.07680 , 2022. 2
[33] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Man-
galam, Bo Xiong, Jitendra Malik, and Christoph Feichten-
hofer. Mvitv2: Improved multiscale vision transformers for
classiﬁcation and detection. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 4804–4814, 2022. 4,6
[34] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,
Stephen Lin, and Han Hu. Video swin transformer. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 3202–3211, 2022. 4
[35] Cheng Ma, Haowen Sun, Yongming Rao, Jie Zhou, and
Jiwen Lu. Video saliency forecasting transformer. IEEE
Transactions on Circuits and Systems for Video Technology ,
32(10):6850–6862, 2022. 1,3
[36] Marcin Marszałek, Ivan Laptev, and Cordelia Schmid. Ac-
tions in context. In IEEE Conference on Computer Vision &
Pattern Recognition , 2009. 1
[37] Kyle Min and Jason J Corso. Tased-net: Temporally-
aggregating spatial encoder-decoder network for video
saliency detection. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 2394–2403,
2019. 8,3
[38] Xiongkuo Min, Guangtao Zhai, Ke Gu, and Xiaokang Yang.
Fixation prediction through multimodal analysis. ACM
Transactions on Multimedia Computing, Communications,
and Applications (TOMM) , 13(1):1–23, 2016. 1,2,5
[39] Xiongkuo Min, Guangtao Zhai, Jiantao Zhou, Xiao-Ping
Zhang, Xiaokang Yang, and Xinping Guan. A multimodal
saliency model for videos with high audio-visual correspon-
dence. IEEE Transactions on Image Processing , 29:3805–
3819, 2020. 1,2
[40] Parag K Mital, Tim J Smith, Robin L Hill, and John M Hen-
derson. Clustering of gaze during dynamic scene viewing
is predicted by motion. Cognitive computation , 3(1):5–24,
2011. 5
[41] Yuxin Peng, Yunzhen Zhao, and Junchao Zhang. Two-
stream collaborative learning with spatial-temporal attentionfor video classiﬁcation. IEEE Transactions on Circuits and
Systems for Video Technology , 29(3):773–786, 2018. 1
[42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents. arXiv preprint arXiv:2204.06125 , 1
(2):3, 2022. 2
[43] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684–10695, 2022. 2
[44] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,
Jonathan Ho, Tim Salimans, David Fleet, and Mohammad
Norouzi. Palette: Image-to-image diffusion models. In
ACM SIGGRAPH 2022 Conference Proceedings , pages 1–
10, 2022. 2
[45] Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li, Zheng
Zhu, Jie Zhou, and Jiwen Lu. Difftalk: Crafting diffusion
models for generalized audio-driven portraits animation. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 1982–1991, 2023. 2
[46] Vincent Sitzmann, Ana Serrano, Amy Pavel, Maneesh
Agrawala, Diego Gutierrez, Belen Masia, and Gordon Wet-
zstein. Saliency in vr: How do people explore virtual envi-
ronments? IEEE transactions on visualization and computer
graphics , 24(4):1633–1642, 2018. 1
[47] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 3,5
[48] Hamed R Tavakoli, Ali Borji, Esa Rahtu, and Juho Kannala.
Dave: A deep audio-visual embedding for dynamic saliency
prediction. arXiv preprint arXiv:1905.10693 , 2019. 2
[49] Joshua B Tenenbaum and William T Freeman. Separating
style and content with bilinear models. Neural computation ,
12(6):1247–1283, 2000. 7
[50] Antigoni Tsiami, Petros Koutras, and Petros Maragos.
Stavis: Spatio-temporal audiovisual saliency network. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 4766–4776, 2020. 1,2,
6,7,8
[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 5
[52] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong
Chen, Qifeng Chen, and Fang Wen. Pretraining is all
you need for image-to-image translation. arXiv preprint
arXiv:2205.12952 , 2022. 2
[53] Wenguan Wang, Jianbing Shen, Fang Guo, Ming-Ming
Cheng, and Ali Borji. Revisiting video saliency: A large-
scale benchmark and a new model. In Proceedings of the
IEEE Conference on computer vision and pattern recogni-
tion, pages 4894–4903, 2018. 6,1
[54] Wenguan Wang, Jianbing Shen, Jianwen Xie, Ming-Ming
Cheng, Haibin Ling, and Ali Borji. Revisiting video saliency
prediction in the deep learning era. IEEE transactions on
pattern analysis and machine intelligence , 43(1):220–237,
2019. 8
27282
[55] Ziqiang Wang, Zhi Liu, Gongyang Li, Yang Wang, Tian-
hong Zhang, Lihua Xu, and Jijun Wang. Spatio-temporal
self-attention network for video saliency prediction. IEEE
Transactions on Multimedia , 2021. 2,3
[56] Julia Wolleb, Robin Sandk ¨uhler, Florentin Bieder, and
Philippe C Cattin. The swiss army knife for image-to-image
translation: Multi-task diffusion models. arXiv preprint
arXiv:2204.02641 , 2022. 2
[57] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and
Kevin Murphy. Rethinking spatiotemporal feature learn-
ing: Speed-accuracy trade-offs in video classiﬁcation. In
Proceedings of the European conference on computer vision
(ECCV) , pages 305–321, 2018. 4
[58] Junwen Xiong, Ganglai Wang, Peng Zhang, Wei Huang,
Yufei Zha, and Guangtao Zhai. Casp-net: Rethinking video
saliency prediction from an audio-visual consistency percep-
tual perspective. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages
6441–6450, 2023. 1,2,6,7,8
[59] Hao Xue, Minghui Sun, and Yanhua Liang. Ecanet: Explicit
cyclic attention-based network for video saliency prediction.
Neurocomputing , 468:233–244, 2022. 3
[60] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. Dif-
fusion probabilistic modeling for video generation. arXiv
preprint arXiv:2203.09481 , 2022. 2
[61] Shunyu Yao, Xiongkuo Min, and Guangtao Zhai. Deep
audio-visual fusion neural network for saliency estimation.
In2021 IEEE International Conference on Image Process-
ing (ICIP) , pages 1604–1608. IEEE, 2021. 1
[62] Qinsheng Zhang and Yongxin Chen. Fast sampling of dif-
fusion models with exponential integrator. arXiv preprint
arXiv:2204.13902 , 2022. 2
[63] Min Zhao, Fan Bao, Chongxuan Li, and Jun Zhu. Egsde:
Unpaired image-to-image translation via energy-guided
stochastic differential equations. Advances in Neural Infor-
mation Processing Systems , 35:3609–3623, 2022. 2
[64] Xiaofei Zhou, Songhe Wu, Ran Shi, Bolun Zheng, Shuai
Wang, Haibing Yin, Jiyong Zhang, and Chenggang Yan.
Transformer-based multi-scale feature integration network
for video saliency prediction. IEEE Transactions on Circuits
and Systems for Video Technology , 2023. 2,3
[65] Shiping Zhu and Ziyao Xu. Spatiotemporal visual saliency
guided perceptual high efﬁciency video coding with neural
network. Neurocomputing , 275:511–522, 2018. 1
27283
