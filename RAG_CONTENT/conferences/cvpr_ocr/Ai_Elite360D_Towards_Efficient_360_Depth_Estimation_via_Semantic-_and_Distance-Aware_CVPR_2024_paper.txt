This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
9926
9927
9928
9929
9930
Datasets Backbone Method Pub’Year #Params (M) #FLOPs (G) Abs Rel↓Sq Rel↓RMSE↓δ1(%)↑δ2(%)↑δ3(%)↑
M3D [ 5]TransformerEGFormer [ 45] ICCV’23 15.39 66.21 0.1473 0.1517 0.6025 81.58 93.90 97.35
PanoFormer [ 32]ECCV’22 20.38 81.09 0.1051 0.0966 0.4929 89.08 96.23 98.31
ResNet-18 [ 14]BiFuse [ 37] CVPR’20 35.80 165.66 0.1360 0.1202 0.5488 83.27 95.12 98.10
UniFuse [ 16] RAL’21 30.26 62.60 0.1191 0.1030 0.5158 86.04 95.84 98.30
OmniFusion [ 41]CVPR’22 32.35 98.68 0.1209 0.1090 0.5055 86.58 95.81 98.36
HRDFuse†[2] CVPR’23 26.09 50.59 0.1414 0.1241 0.5507 81.48 94.89 98.20
Ours - 15.43 45.91 0.1272 0.1070 0.5270 85.28 95.28 98.49
ResNet-34 [ 14]BiFuse [ 37] CVPR’20 56.01 199.58 0.1126 0.0992 0.5027 88.00 96.13 98.47
BiFuse++ [ 38] TPAMI’22 52.49 87.48 0.1123 0.0915 0.4853 88.12 96.56 98.69
UniFuse [ 16] RAL’21 50.48 96.52 0.1144 0.0936 0.4835 87.85 96.59 98.73
OmniFusion [ 41]CVPR’22 42.46 142.29 0.1161 0.1007 0.4931 87.72 96.15 98.44
HRDFuse†[2] CVPR’23 46.31 80.87 0.1172 0.0971 0.5025 86.74 96.17 98.49
Ours - 25.54 65.29 0.1115 0.0914 0.4875 88.15 96.46 98.74
ResNet-50∗[14]BiFuse [ 37] CVPR’20 253.08 775.24 0.1179 0.0981 0.4970 86.74 96.27 98.66
UniFuse [ 16] RAL’21 131.30 222.30 0.1185 0.0984 0.5024 86.66 96.18 98.50
Ours - 42.99 170.11 0.1112 0.0980 0.4870 86.70 96.01 98.61
S2D3D [ 3]TransformerEGFormer [ 45] ICCV’23 15.39 66.21 0.1528 0.1408 0.4974 81.85 93.38 97.36
PanoFormer [ 32]ECCV’22 20.38 81.09 0.1122 0.0786 0.3945 88.74 95.84 98.59
ResNet-34 [ 14]OmniFusion [ 41]CVPR’22 42.46 142.29 0.1154 0.0775 0.3809 86.74 96.03 98.71
UniFuse [ 16] RAL’21 50.48 96.52 0.1124 0.0709 0.3555 87.06 97.04 98.99
Ours - 25.51 65.28 0.1182 0.0728 0.3756 88.72 96.84 98.92
Struct3D [ 48]TransformerEGFormer [ 45] ICCV’23 15.39 66.21 0.2205 0.4509 0.6841 79.79 90.71 94.55
PanoFormer [ 32]ECCV’22 20.38 81.09 0.2549 0.4949 0.7937 74.70 89.15 93.97
ResNet-34 [ 14]BiFuse [ 37] CVPR’20 56.01 199.58 0.1573 0.2455 0.5213 85.91 94.00 96.72
UniFuse [ 16] RAL’21 50.48 96.52 0.1506 0.2319 0.5016 85.42 93.99 96.76
Ours - 25.51 65.28 0.1480 0.2215 0.4961 87.41 94.34 96.66
Table 2. Quantitative comparison with the SOTA methods .†means that we modify the HRDFuse network structure for a fair compari-
son. Green represents the best performance under the given encoder backbone.
cross-attention mechanism [ 39]. Given the ERP pixel-wise
featureFE
i,jand ICOSAP point feature set [FI], we ﬁrstly
calculate the spatial distance embedding DisSPfrom the
spatial coordinates of ERP pixel and ICOSAP point set, i.e.
[xe
i,j,ye
i,j,ze
i,j]and{[xico
n,yico
n,zico
n]}N, as:
DisSP
i,j= [e−∆xi,j,e−∆yi,j,e−∆zi,j]WSP, (1)
where linear projection WSP∈R3×d,DisSP
i,j∈R1×N×d,
and[∆xi,j,∆yi,j,∆zi,j]∈R1×N×3is the distances be-
tween[xe
i,j,ye
i,j,ze
i,j]and{[xico
n,yico
n,zico
n]}N. In particu-
lar, the operation e−(·)allowsDisSPto pay more attention
to the close parts between ERP pixels and ICOSAP point
set. After that, we produce the query QD
i,jand keyKD
fromFE
i,jand[FI], respectively and calculate the semantic
distance embedding DisSE
i,j:
DisSE
i,j=e−/bardblQD
i,j−KD/bardbl, (2)
whereWD
Q,WD
K∈RC×dare linear projections, QD
i,j∈
R1×d,KD∈RN×d, andDisSE
i,j∈R1×N×d. Lastly, the
distance-aware attention weight AD
i,jis generated with spa-
tial and semantic distance embeddings, and the distance-
aware afﬁnity feature vector FDA
i,jis obtained from the at-
tention weight AD
i,jand the value VD:
AD
i,j=softmax (/summationtext(DisSP
i,j+DisSE
i,j)
√
d), (3)
VD=FIWD
V, FDA
i,j=AD
i,j∗VD, (4)
where/summationtextmeans the sum for the channel dimension, i.e./summationtext(DisSP
i,j+DisSE
i,j)∈R1×N. After querying all ERP
pixel-wise features, we obtain the distance-aware aggre-
gated feature FDA∈Rh×w×d,d=C.Gated fusion. Since direct average or concatenation may
compromise the original representation ability, inspired
by [9], we propose the gated fusion block to adaptively fuse
FSAandFDAand obtain the representations FGLfrom a
local-with-global perspective, formulated as:
FGL=gSA∗FSA+gDA∗FDA, (5)
gSA=σSA(WSA
g·[FSA;FDA]),
gDA=σDA(WDA
g·[FSA;FDA]),
whereWSA
gandWDA
gare linear projections, σSA(·)and
σDA(·)are the sigmoid functions.
3.4. Optimization
With the fused feature FGLand multi-scale ERP feature
maps in the ERP encoder backbone, we feed them into
a decoder [ 30] with several up-sampling blocks and skip-
connections to output the ﬁnal depth. For the depth super-
vision, we follow existing works [ 28,32] and employ the
combination of Berhu loss [ 20] and gradient loss [ 28]. (De-
tails of training loss can be found in the suppl. material .).
4. Experiments
4.1. Datasets, Metrics, and Implementation Details
Datasets and Metrics. We evaluate Elite360D on three
datasets: two real-world datasets, Matterport3D(M3D) [ 5],
Stanford2D3D(S2D3D) [ 3], and a recently proposed large-
scale synthetic dataset, Structure3D(Struct3D) [ 48]. For the
evaluation metrics, we follow previous works [ 16,32,45]
to employ some standard metrics, including absolute rela-
tive error (Abs Rel), squared relative error (Sq Rel), root
mean squared error (RMSE), and three threshold percent-
ageδ < αt(α= 1.25,t= 1,2,3), denoted as δt. Addi-
9931
Figure 7. Qualitative results (with ResNet-34 as the backbone) on Matterport3D (top), Stanford2D3D (middle) and Structure3D (bottom).
Backbone Pre-trained Abs Rel↓Sq Rel↓RMSE↓δ1↑
ResNet-34/enc-37 0.1596 0.1452 0.5856 81.36
/enc-33 0.1115 0.0914 0.4875 88.15
EfﬁcientNet-B5/enc-37 0.1211 0.1087 0.5131 86.80
/enc-33 0.1048 0.0805 0.4524 89.92
DilateFormer/enc-37 0.1515 0.1415 0.5694 80.95
/enc-33 0.1423 0.1251 0.5517 82.68
Table 3. The impact of pre-training on ERP encoder backbones.
tionally, we measure the number of parameters and FLOPS
to evaluate the efﬁciency of our method.
Training Details. We use diverse ERP encoder back-
bones, including CNNs (ResNet-18, 34, 50 [ 14], Efﬁcient-
Net B5 [ 35]), and transformers (Swin-B [ 24], DilateFormer-
T [17]). All backbones are pre-trained on ImageNet-
1K [ 11]. We set the default channel number Cto 64
and default subdivision level of ICOSAP as l= 4. For
the ICOSAP encoder, we employ the one of Point trans-
former [ 47] with three down-sample blocks. Follow-
ing [16], we use Adam optimizer [ 18] and a constant learn-
ing rate of 1e−4. Considering the unfair comparisons stem-
ming from variations in hyper-parameters and validation
procedures used across different methods, we re-train the
existing methods from scratch and validate them, follow-
ing the uniﬁed training and validation settings [ 16]. (Due to
page limit, detailed training and validation settings can be
found in suppl. mat. ).
4.2. Performance Comparison
Comparisons with ERP-based depth baselines. As
shown in Tab. 1, with an increase of only ∼1M parameters(C=64), our Elite360D demonstrates substantial advance-
ments over the ERP-based baselines across different ERP
encoder backbones on all three datasets. Speciﬁcally, for
the Matterport3D dataset, Elite360D achieves reductions
exceeding 10 %in Abs Rel error (ResNet-18, 34), along
with reductions of 4.00 %in Abs Rel error (Swin-B) and
4.02%in RMSE error (DilateFormer-T). Besides, with the
larger channel number C= 256 (ResNet-50), Elite360D
outperforms ERP baseline by 18.75 %(Abl Rel), 18.94 %
(Sq Rel). For the small-scale S2D3D dataset, Elite360D
outperforms ERP baseline by 9.21 %in Abs Rel error and
1.31%in accuracy δ1(ResNet-34), as well as 5.49 %in
Sq Rel error (EfﬁcientNet-B5). Remarkably, on the larger-
scale Structure3D, Elite360 performs favorably against the
baseline by a signiﬁcant margin, especially with ResNet-34.
Comparisons with prevalent methods. In Tab. 2, we con-
duct a comprehensive comparison with prevalent supervised
methods. From the results, we can observe that our ap-
proach achieves similar or even superior performance com-
pared to existing both bi-projection fusion methods and
single input methods at a signiﬁcantly lower cost, par-
ticularly on two large-scale datasets, Matterport3D and
Structure3D. Speciﬁcally, for the Matterport3D dataset, our
Elite360D with ResNet-34 outperforms UniFuse by 2.53 %
(Abs Rel) and with ResNet-50 outperforms BiFuse by
2.01%(RMSE). For the Structure3D dataset, our Elite360D
with ResNet-34 outperforms UniFuse by 4.48 %(Seq Rel),
1.99%(δ1). For performance on the Stanford2D3D dataset,
we suspect it might be related to the ICOSAP point encoder.
9932
Bi-projection feature fusion Abs Rel↓Sq Rel↓RMSE↓δ1↑
SFA [ 2] + Add 0.1276 0.1002 0.5150 84.27
SFA [ 2] + Concat 0.1191 0.1019 0.5143 86.52
Only SA 0.1204 0.1014 0.5121 86.26
Only DA 0.1184 0.0972 0.4944 87.06
Our B2F (SA + DA) 0.1115 0.0914 0.4875 88.15
Table 4. The ablation results for B2F module.
Final fusion Abs Rel↓Sq Rel↓RMSE↓δ1↑
Add 0.1685 0.1481 0.5809 74.60
Average 0.1198 0.0918 0.4893 86.65
Concatenation 0.1145 0.0937 0.4880 87.66
Adaptive fusion [ 2] 0.1244 0.0968 0.4891 86.08
Our gated fusion 0.1115 0.0914 0.4875 88.15
Table 5. The ablation results for the fusion of B2F module.
The limited data of Stanford2D3D dataset restricts the abil-
ity of the transformer-based point encoder to provide accu-
rate global perception. Moreover, in Fig. 7, we present the
qualitative comparisons. Our Elite360D can predict more
accurate depth values based on the local-with-global per-
ception capabilities (e.g., ﬂowers, shelves and doors). Ad-
ditional qualitative results and inference time comparisons
can be found in the suppl. material.
4.3. Ablation Study and Analyses
Most of ablation experiments are conducted on the Mat-
teport3D test dataset with ResNet34 as the backbone .
The Effect of pre-training. We verify the effectiveness
of ImageNet [ 11] pre-training with different encoder back-
bones. As observed from Tab. 3, the pre-training results in
a signiﬁcant improvement for all encoder backbones, e.g.,
6.79%improvement in accuracy δ1(ResNet-34). Notably,
pre-training has a relatively small impact on DilateFormer.
Combined with the results in Tab. 1, the explanation of
this phenomenon is that the default input resolution in pre-
trained models is different from actual input, thereby im-
pacting the resolution-related position embeddings. In gen-
eral, pre-training based on large-scale perspective images
can effectively enhance the performance of models based
on 360◦images and reduce the risk of overﬁtting.
The effectiveness of B2F module. In Tab. 4, we com-
pare four available bi-projection feature fusion modules. To
align the spatial dimensions between ICOSAP point fea-
ture set and ERP feature map, we introduce SFA module
from [ 2]. After that, we employ direct addition and concate-
nation to aggregate these two projections. We also achieve
the bi-projection feature fusion with semantic-aware afﬁn-
ity attention (SA) alone and distance-aware afﬁnity atten-
tion (DA) alone. Compared to the methods based solely on
semantic-aware feature similarities (The ﬁrst three rows),
single distance-aware afﬁnity attention can achieve better
performance, which indicates that the spatial positional re-
lationships boost the bi-projection feature fusion. Overall,
our B2F module achieves the best performance.
The superiority of ICOSAP. As only CP /TP’s patch cen-
ters lie on the sphere’s surface, we extract the feature em-Method #Param(M) #FLOPs(G) Abs Rel ↓RMSE↓δ1↑
ERP-CP 25.66 54.15 0.1369 0.5401 83.69
ERP-TP (N=18) 25.66 50.58 0.1328 0.5385 83.87
ERP-ICOSAP (Ours) 15.43 45.91 0.1272 0.5270 85.28
Table 6. The comparison of different projections on Matterport3D.
Nof{FI}#Params (M) #FLOPs (G) Abs Rel↓Sq Rel↓RMSE↓δ1↑
20 27.41 66.29 0.1157 0.0995 0.5024 87.12
80 25.54 65.29 0.1115 0.0914 0.4875 88.15
320 24.98 64.32 0.1153 0.0943 0.4905 87.85
Table 7. Impact of the ICOSAP point-wise feature number N.
LargerN, fewer down-sampling blocks in the point encoder.
bedding from each CP /TP patch and employs the patch
center coordinates and feature embedding as the input of
B2F module. In Tab. 6, we show the results with ResNet18
backbone. Our Elite360D, utilizing the ICOSAP point set,
marginally outperforms models with CP and TP patches,
while exhibiting fewer parameters and FLOPs.
The effectiveness of gated fusion. We conduct an abla-
tion study for the gated fusion block, outlined in Tab. 5.
With the feature maps FSAandFDA, We compare it with
the direct addition, average fusion, concatenation, and the
adaptive fusion in [ 2]. The gated fusion performs best.
ICOSAP point feature number N.We study the effect of
the ICOSAP feature point number (See Tab. 7). Too few
points (N=20) lead to the over-concentrated global contex-
tual information resulting from excessive down-sampling
blocks, while too many points (N=320) lead to under-
concentrated condition, resulting in insufﬁcient perception
of ERP pixel features. Best performance can be observed
when N=80 and we used N=80 as default in this paper.
5. Conclusion and Future Work
In this paper, we proposed a novel bi-projection fusion so-
lution for efﬁcient 360 depth estimation. To address the
limited local receptive ﬁeld of ERP pixel-wise features and
avoid expensive bi-projection fusion modules, we proposed
a compact yet effective B2F module to learn the represen-
tations with local-with-global perceptions from ERP and
ICOSAP. With an increase of 1M parameters, we signiﬁ-
cantly improved the performance of the ERP-based depth
estimation baseline. Remarkably, our approach achieved
performance on par with complex state-of-the-art methods.
Future Work: From the experimental results, we observed
that ERP-based depth baseline, with pre-trained Efﬁcient-
Net backbone, even outperforms existing speciﬁcally de-
signed methods. Therefore, in the future, we will explore
how to fully leverage different projections and successful
perspective models for 360◦community.
Acknowledgement
This paper is supported by the National Natural Sci-
ence Foundation of China (NSF) under Grant No.
NSFC22FYT45 and the Guangzhou City, University and
Enterprise Joint Fund under Grant No.SL2022A03J01278.
9933
References
[1] Hao Ai, Zidong Cao, Jin Zhu, Haotian Bai, Yucheng Chen,
and Ling Wang. Deep learning for omnidirectional vision: A
survey and new perspectives. ArXiv , abs/2205.10468, 2022.
1
[2] Hao Ai, Zidong Cao, Yan-Pei Cao, Ying Shan, and Lin
Wang. Hrdfuse: Monocular 360° depth estimation by collab-
oratively learning holistic-with-regional depth distributions.
2023 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , pages 13273–13282, 2023. 2,3,
6,8
[3] Iro Armeni, Sasha Sax, Amir Roshan Zamir, and Silvio
Savarese. Joint 2d-3d-semantic data for indoor scene under-
standing. CoRR , abs/1702.01105, 2017. 2,5,6
[4] Zidong Cao, Hao Ai, Yan Cao, Ying Shan, Xiaohu Qie,
and Lin Wang. Omnizoomer: Learning to move and zoom
in on sphere at high-resolution. 2023 IEEE/CVF Inter-
national Conference on Computer Vision (ICCV) , pages
12851–12861, 2023. 1
[5] Angel X. Chang, Angela Dai, Thomas A. Funkhouser, Ma-
ciej Halber, Matthias Nießner, Manolis Savva, Shuran Song,
Andy Zeng, and Yinda Zhang. Matterport3d: Learning from
RGB-D data in indoor environments. In 3DV, pages 667–
676. IEEE Computer Society, 2017. 1,2,5,6
[6] Zehui Chen, Zhenyu Li, Shiquan Zhang, Liangji Fang,
Qinghong Jiang, Feng Zhao, Bolei Zhou, and Hang Zhao.
Autoalign: Pixel-instance feature aggregation for multi-
modal 3d object detection. In International Joint Conference
on Artiﬁcial Intelligence , 2022. 3
[7] Hsien-Tzu Cheng, Chun-Hung Chao, Jin-Dong Dong, Hao-
Kai Wen, Tyng-Luh Liu, and Min Sun. Cube padding
for weakly-supervised saliency prediction in 360° videos.
2018 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 1420–1429, 2018. 3
[8] Xinjing Cheng, Peng Wang, Yanqi Zhou, Chenye Guan, and
Ruigang Yang. Omnidirectional depth extension networks.
In2020 IEEE International Conference on Robotics and Au-
tomation (ICRA) , pages 589–595, 2020. 2
[9] Yanhua Cheng, Rui Cai, Zhiwei Li, Xin Zhao, and Kaiqi
Huang. Locality-sensitive deconvolution networks with
gated fusion for rgb-d indoor semantic segmentation. 2017
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , pages 1475–1483, 2017. 6
[10] Taco Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max
Welling. Gauge equivariant convolutional networks and the
icosahedral cnn. In International Conference on Machine
Learning , 2019. 2
[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. computer vision and pattern recognition , 2009. 2,
4,7,8
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition atscale. In International Conference on Learning Representa-
tions , 2021. 3,5
[13] Marc Eder, Mykhailo Shvets, John Lim, and Jan-Michael
Frahm. Tangent images for mitigating spherical distortion.
2020 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , pages 12423–12431, 2019. 3
[14] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep
residual learning for image recognition. 2016 IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 770–778, 2015. 2,4,5,6,7
[15] ChiyuMax Jiang, Jingwei Huang, Karthik Kashinath, Prab-
hat Prabhat, Philip Marcus, and Matthias Niessner. Spher-
ical cnns on unstructured grids. International Conference
on Learning Representations,International Conference on
Learning Representations , 2019. 2
[16] Hualie Jiang, Zhe Sheng, Siyu Zhu, Zilong Dong, and Rui
Huang. Unifuse: Unidirectional fusion for 360° panorama
depth estimation. IEEE Robotics and Automation Letters , 6:
1519–1526, 2021. 2,3,4,6,7
[17] Jiayu Jiao, Yu-Ming Tang, Kun-Yu Lin, Yipeng Gao, Jin-
hua Ma, Yaowei Wang, and Wei-Shi Zheng. Dilateformer:
Multi-scale dilated transformer for visual recognition. IEEE
Transactions on Multimedia , pages 1–14, 2023. 5,7
[18] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. CoRR , abs/1412.6980, 2014. 7
[19] Varun Ravi Kumar, Senthil Kumar Yogamani, Hazem
Rashed, Ganesh Sitsu, Christian Witt, Isabelle Leang, Stefan
Milz, and Patrick M ¨ader. Omnidet: Surround view cameras
based multi-task visual perception network for autonomous
driving. IEEE Robotics and Automation Letters , 6:2830–
2837, 2021. 1
[20] Iro Laina, C. Rupprecht, Vasileios Belagiannis, Federico
Tombari, and Nassir Navab. Deeper depth prediction with
fully convolutional residual networks. 3DV 2016 , pages 239–
248, 2016. 6
[21] Yeonkun Lee, Jaeseok Jeong, Jong Seob Yun, Wonjune Cho,
and Kuk jin Yoon. Spherephd: Applying cnns on a spherical
polyhedron representation of 360° images. 2019 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 9173–9181, 2018. 2,3,4
[22] Meng Li, Senbo Wang, Weihao Yuan, Weichao Shen, Zhe
Sheng, and Zilong Dong. S2net: Accurate panorama depth
estimation on spherical surface. IEEE Robotics and Automa-
tion Letters , 8:1053–1060, 2023. 3
[23] Yunhao Li, Wei Shen, Zhongpai Gao, Yucheng Zhu, Guang-
tao Zhai, and Guodong Guo. Looking here or there? gaze
following in 360-degree images. 2021 IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV) , pages 3722–
3731, 2021. 1
[24] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. 2021
IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 9992–10002, 2021. 2,3,4,5,7
[25] Yunze Man, Liangyan Gui, and Yu-Xiong Wang. Bev-
guided multi-modality fusion for driving perception. 2023
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 21960–21969, 2023. 3
9934
[26] Daniel Martin, Ana Serrano, Alexander W. Bergman, Gor-
don Wetzstein, and Bel ´en Masi ´a. Scangan360: A generative
model of realistic scanpaths for 360° images. IEEE Trans-
actions on Visualization and Computer Graphics , 28:2003–
2013, 2021. 1
[27] Rafael Monroy, Sebastian Lutz, Tejo Chalasani, and
Aljoscha Smolic. Salnet360: Saliency maps for omni-
directional images with cnn. Signal Process. Image Com-
mun. , 69:26–34, 2017. 3
[28] Giovanni Pintore, Eva Almansa, and Jens Schneider.
Slicenet: deep dense depth estimation from a single in-
door panorama using a slice-based representation. 2021
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 11531–11540, 2021. 1,3,6
[29] Manuel Rey-Area, Mingze Yuan, and Christian Richardt.
360monodepth: High-resolution 360° monocular depth es-
timation. 2022 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 3752–3762, 2022. 3
[30] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
InMICCAI (3) , pages 234–241. Springer, 2015. 3,6
[31] Mehran Shakerinava and Siamak Ravanbakhsh. Equivari-
ant networks for pixelized spheres. Proceedings of the
38th International Conference on Machine Learning, ICML ,
abs/2106.06662, 2021. 3,4
[32] Zhijie Shen, Chunyu Lin, Kang Liao, Lang Nie, Zishuo
Zheng, and Yao Zhao. Panoformer: Panorama transformer
for indoor 360° depth estimation. In European Conference
on Computer Vision , 2022. 2,3,6
[33] Yu-Chuan Su and Kristen Grauman. Kernel transformer net-
works for compact spherical convolution. 2019 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 9434–9443, 2018. 1
[34] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Hohonet:
360 indoor holistic understanding with latent horizontal fea-
tures. 2021 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 2573–2582, 2020. 1,3
[35] Mingxing Tan and Quoc V . Le. Efﬁcientnet: Rethinking
model scaling for convolutional neural networks. In ICML ,
pages 6105–6114. PMLR, 2019. 4,5,7
[36] Keisuke Tateno, Nassir Navab, and Federico Tombari.
Distortion-aware convolutional ﬁlters for dense prediction in
panoramic images. In European Conference on Computer
Vision , 2018. 1,2
[37] Fu-En Wang, Yu-Hsuan Yeh, Min Sun, Wei-Chen Chiu, and
Yi-Hsuan Tsai. Bifuse: Monocular 360 depth estimation via
bi-projection fusion. 2020 IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 459–
468, 2020. 2,3,4,6
[38] Fu-En Wang, Yu-Hsuan Yeh, Yi-Hsuan Tsai, Wei-Chen
Chiu, and Min Sun. Bifuse++: Self-supervised and efﬁcient
bi-projection fusion for 360° depth estimation. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence , 45:
5448–5460, 2022. 2,3,6
[39] Sirui Wang, Di Liang, Jian Song, Yuntao Li, and Wei
Wu. DABERT: dual attention enhanced BERT for seman-
tic matching. In COLING , pages 1645–1654. International
Committee on Computational Linguistics, 2022. 6[40] X. Wang, Ross B. Girshick, Abhinav Kumar Gupta, and
Kaiming He. Non-local neural networks. 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 7794–7803, 2017. 4
[41] Yu yang Li, Yuliang Guo, Zhixin Yan, Xinyu Huang, Ye
Duan, and Liu Ren. Omnifusion: 360 monocular depth esti-
mation via geometry-aware fusion. 2022 IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 2791–2800, 2022. 2,3,6
[42] Xiaoqing Ye, Mao Shu, Hanyu Li, Yifeng Shi, Yingying
Li, Guan-Sheng Wang, Xiao Tan, and Errui Ding. Rope3d:
The roadside perception dataset for autonomous driving and
monocular 3d object detection task. 2022 IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 21309–21318, 2022. 1
[43] Youngho Yoon, Inchul Chung, Lin Wang, and Kuk-Jin
Yoon. Spheresr: 360◦image super-resolution with arbi-
trary projection via continuous spherical image representa-
tion. 2022 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 5667–5676, 2021. 1,2,
3
[44] Haozheng Yu, Lu He, Bing Jian, Weiwei Feng, and
Shanghua Liu. Panelnet: Understanding 360 indoor environ-
ment via panel representation. 2023 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
878–887, 2023. 1,3
[45] Ilwi Yun, Chan-Yong Shin, Hyunku Lee, Hyuk-Jae Lee,
and Chae-Eun Rhee. Egformer: Equirectangular geometry-
biased transformer for 360 depth estimation. ArXiv ,
abs/2304.07803, 2023. 2,3,4,6
[46] Chao Zhang, Stephan Liwicki, William Smith, and Roberto
Cipolla. Orientation-aware semantic segmentation on icosa-
hedron spheres. 2019 IEEE/CVF International Conference
on Computer Vision (ICCV) , pages 3532–3540, 2019. 2,4
[47] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip H. S. Torr, and
Vladlen Koltun. Point transformer. 2021 IEEE/CVF In-
ternational Conference on Computer Vision (ICCV) , pages
16239–16248, 2020. 4,7
[48] Jia Zheng, Junfei Zhang, Jing Li, Rui Tang, Shenghua Gao,
and Zihan Zhou. Structured3d: A large photo-realistic
dataset for structured 3d modeling. In European Conference
on Computer Vision , 2019. 5,6
[49] Xu Zheng, Jinjing Zhu, Ye-Peng Liu, Zidong Cao, Chong
Fu, and Lin Wang. Both style and distortion matter: Dual-
path unsupervised domain adaptation for panoramic seman-
tic segmentation. 2023 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 1285–1295,
2023. 1
[50] Chuanqing Zhuang, Zhengda Lu, Yiqun Wang, Jun Xiao,
and Ying Wang. Acdnet: Adaptively combined dilated con-
volution for monocular panorama depth estimation. CoRR ,
abs/2112.14440, 2021. 1,3
[51] Nikolaos Zioulis, Antonis Karakottas, Dimitrios Zarpalas,
and Petros Daras. Omnidepth: Dense depth estimation for
indoors spherical panoramas. In ECCV (6) , pages 453–471.
Springer, 2018. 1,2
9935
