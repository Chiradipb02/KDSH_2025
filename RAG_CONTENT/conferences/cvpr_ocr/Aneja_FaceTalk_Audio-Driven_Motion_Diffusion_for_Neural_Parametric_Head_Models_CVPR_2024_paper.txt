FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models
Shivangi Aneja1Justus Thies2,3Angela Dai1Matthias Nie√üner1
1Technical University of Munich2MPI-IS, T ¬®ubingen3TU Darmstadt
hello
Speech Signaleveryonecongratulationsclassandto
[silence]
Figure 1. Given an input speech signal, we propose a diffusion-based approach to synthesize high-quality and temporally consistent 3D
motion sequences of high-fidelity human heads as neural parametric head models. Our method can generate a diverse set of expressions
(including wrinkles and eye blinks) and the generated mouth motion is temporally synchronized with the given audio signal.
Abstract
We introduce FaceTalk1, a novel generative approach de-
signed for synthesizing high-fidelity 3D motion sequences
of talking human heads from input audio signal. To capture
the expressive, detailed nature of human heads, including
hair, ears, and finer-scale eye movements, we propose to
couple speech signal with the latent space of neural para-
metric head models to create high-fidelity, temporally co-
herent motion sequences. We propose a new latent diffu-
sion model for this task, operating in the expression space of
neural parametric head models, to synthesize audio-driven
realistic head sequences. In the absence of a dataset with
corresponding NPHM expressions to audio, we optimize for
these correspondences to produce a dataset of temporally-
optimized NPHM expressions fit to audio-video recordings
of people talking. To the best of our knowledge, this is
the first work to propose a generative approach for realis-
tic and high-quality motion synthesis of volumetric human
heads, representing a significant advancement in the field of
audio-driven 3D animation. Notably, our approach stands
out in its ability to generate plausible motion sequences that
can produce high-fidelity head animation coupled with the
1Project Page: https://shivangi- aneja.github.io/
projects/facetalkNPHM shape space. Our experimental results substantiate
the effectiveness of FaceTalk, consistently achieving supe-
rior and visually natural motion, encompassing diverse fa-
cial expressions and styles, outperforming existing methods
by 75% in perceptual user study evaluation.
1. Introduction
Modeling 3D animation of humans has a wide range of
applications in the realm of digital media, including an-
imated movies, computer games, and virtual assistants.
In recent years, there have been numerous works propos-
ing generative approaches for motion synthesis of human
bodies, enabling the animation of human skeletons condi-
tioned on various signals such as action [3, 25, 46], lan-
guage [1, 4, 36, 47, 64, 77] and music [2, 67]. While human
faces are critical to synthesis of humans, generative syn-
thesis of 3D faces in motion has focused on 3D morphable
models (3DMMs) leveraging linear blendshapes [7, 40] to
represent head motion and expression. Such models char-
acterize a disentangled space of head shape and motion,
but lack the capacity to comprehensively represent the com-
plexity and fine-grained details of human face geometry in
motion (e.g., hair, skin furrowing during motion, etc.).
Thus, we propose to represent animated head sequences
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
21263
with a volumetric 3D head representation, leveraging the
expressive representation space of neural parametric head
models (NPHMs) [23, 24]. NPHMs offer a flexible rep-
resentation capable of handling complex and irregular fa-
cial expressions (e.g., blinking, skin creasing), along with a
high-fidelity shape space including the head, hair, and ears,
making them a much more suitable choice for face anima-
tion. We address the challenging task of creating an audio-
conditional generative animation model for this volumetric
representation.
We design the first transformer-based latent diffusion
model for audio-driven head animation synthesis. Our dif-
fusion model operates in the latent NPHM expression space
to generate temporally coherent expressions consistent with
an input audio signal represented with Wave2Vec 2.0 [6].
In the absence of paired audio-NPHM data, we optimize for
corresponding NPHM expressions to fit to multi-view video
recordings of people speaking, generating train supervision
for our task. As NPHMs are designed for static (frame-by-
frame) expressions without temporal consistency, we em-
ploy both geometric and temporal priors to produce tem-
porally consistent optimized motion sequences. This en-
ables training our audio-head diffusion model to synthesize
realistic speech-conditioned 3D head sequences, capable
of capturing high-frequency details like wrinkles and eye
blinks present in the face region. Our method takes first step
towards simplifying the task of high-fidelity facial motion
generation of 3D faces for content creation applications.
In summary, we propose the first latent diffusion model
for the creation of audio-conditioned animations of volu-
metric avatars. By producing volumetric head animation,
our generative model is highly expressive yet efficient com-
pared to existing 3D methods. We also demonstrate control
over motion style, using classifier-free guidance to adjust
the strength of the stylistic expression.
2. Related Work
Facial Animation. Our proposed method is the first work
for audio-conditioned latent diffusion of volumetric head
avatars. There is a large corpus of research works in
the field of 2D audio-driven facial animation operating
on RGB videos, synthesizing 2D sequences directly [10‚Äì
13, 16, 27, 33, 34, 48, 56, 59, 61, 70, 71, 73, 79, 80]. How-
ever, these methods operate in pixel space and do not pro-
duce any geometric information. Another line of work also
operating on RGB videos but using intermediate 3D repre-
sentations are based on 3DMMs [19, 32, 57, 62, 66]. Al-
though these methods generate RGB videos in the end, they
use 3DMMs which produces very sparse geometric infor-
mation.
Recent works based on radiance fields [22, 26, 37, 41,
55, 76] have also gained popularity due to their capability to
model densities directly from images. These methods gen-erate impressive RGB videos but the underlying geometry
learned is highly imperfect (no segregation between back-
ground and facial region) as shown by Chan et al. [9]. Ad-
ditionally, these methods require identity-specific training,
and thus can not be used for content creation applications.
Learning animation of 3D meshes directly is much more
promising, but only a handful of methods exist [14, 15,
21, 35, 44, 51, 65, 75]. A vast majority of these works
model speech-conditioned animation for an artist-designed
template mesh. Although these methods can match the fa-
cial motion with the speech signal, one limitation of these
methods is their incapability to represent fine-scale details
present in faces. Another downside of these methods is
that these approaches learn a deterministic model produc-
ing no/muted motion in the upper region of the face, thus
limiting them from being able to produce realistic motion.
In this work, we solve these issues by proposing a gener-
ative model that can operate in the compact and detailed
latent space of neural parametric head models, thus capable
of representing fine-scale facial details and synthesizing a
diverse set of expressions and speaking styles.
Diffusion Models for Generative Synthesis. In re-
cent years, diffusion models have experienced a surge of
interest as highly expressive and efficient generative mod-
els. These models have demonstrated strong performances
as a generative model for a variety of domains such as
images [17, 29, 49, 52, 53, 78], videos [8, 18, 20, 69],
speech [30, 38, 39] and motion [2, 5, 36, 64, 67, 81].
2D facial animation has also seen some progress with
diffusion models [56, 59]. DiffusedHeads [59] operates in
pixel space, making the sampling process very slow. Al-
though DiffTalk [56] learns the diffusion process in the
latent space, the sampling is performed in an autoregres-
sive fashion, limiting sampling efficiency. Moreover, these
methods operate on RGB videos, and though they achieve
high-quality results on a per-frame basis, consistency and
coherence across different timesteps remain challenging
due to temporal jitter. Hence, these methods can not be
applied directly to the parametric head models. Concurrent
to our work, DiffPoseTalk [60] and FaceDiffuser [58] pro-
pose a diffusion-based approach for animating 3D meshes
from speech signal. However, these methods require addi-
tional conditions like style embedding, generate sequences
in an autoregressive fashion coupled with diffusion denois-
ing making them very slow and still they can not produce
fine-scale facial details due to the use of 3DMMs which
are limited in expressivity. Concurrent to ours, DPHM [63]
also utilizes diffusion to regularize NPHM expressions from
depth, operating on a frame-by-frame basis. In contrast,
we synthesize the entire sequence simultaneously making it
much faster and operate directly in the latent space of highly
expressive NPHMs, producing high-fidelity and temporally
consistent results.
21264
Expression  MLP‚Ä¶FaceTalkt       t-1Diffuse{Œ∏exp}1:NT‚àºùí©(0,I)
‚Ä¶‚Ä¶
Speech WaveformLinear MapperWave2Vec 2.0 (Frozen)a1a2aN
Frequency InterpolationTransformer EncoderTemporal Convolutional Network‚Ä¶
tDiffusion Timestamp
Linear Projection~PE
‚Ä¶
Self-AttentionNoisy ExpressionsFiLM MLPCross-AttentionFiLM MLPFeed ForwardFiLM MLPLinear Projection‚Ä¶‚Ä¶{Œ∏exp}1:N0Denoised Expressions‚Ä¶Expression  Audio Mask
INFERENCE√óMTRAININGSpeechExpressions
Smoothing
‚Ä¶‚Ä¶
1111100000000000000000000Expression Decoder Block
Timestamp EmbedderÃÇtIdentity MLPNPHM(Frozen)(Frozen){ÃÇŒ∏exp}1:NExtracted Sequence
{Œ∏exp}1:NtFigure 2. Pipeline Overview. FaceTalk uses frozen Wave2Vec 2.0 [6] to extract audio embeddings from a speech signal. The diffu-
sion timestamp is embedded using a timestamp embedder. The expression decoder employs a multi-head transformer decoder [68] with
FiLM [45] layers, interleaved between Self-Attention, Cross Attention, and FeedForward layers, to incorporate diffusion timestamp. Dur-
ing training, the model is trained to denoise the noisy expression sequences from timestamp t. At inference, FaceTalk denoises the gaussian
noise sequence
Œ∏exp	1:N
T‚àº N(0,I)iteratively until t= 0, yielding the estimated final sequenceÀÜŒ∏exp	1:N. These are then input to
the frozen NPHM model, utilizing facial smoothing, and mesh sequences are extracted using MC [42].
3. Preliminaries
NPHM. In contrast to traditional 3D morphable mod-
els, which remain limited in expressive detail, we employ
the Neural Parametric Head Model (NPHM) representa-
tion [23, 24]. Similar to traditional 3DMMs, NPHM disen-
tangles a human face into an identity and expression space;
however, the shape and expression space are volumetric, en-
abling expressive capture of details such as fine-scale eye
movements, hair, ears, etc. NPHM uses two auto-decoder
style neural networks (a) Identity Network
I	
to represent
the overall facial shape and (b) Expression Network
E	
to
represent the facial movements such as jaw pose, wrinkles,
eyeblinks, etc., jointly denoted as F=
I,E	
. The identity
and expressions latent codes
Œ∏id,Œ∏exp	
define the facial
shape and expression respectively. Unlike 3DMMs which
predict a fixed-topology mesh that can not model fine sur-
face details, NPHM can handle different hairstyles, wrin-
kles and complex facial expressions. NPHM represents
identities with a signed distance field (SDF) in canonical
space and models the expressions as deformations onto the
facial region. Mathematically, it can be defined as:
F(xi,Œ∏id,Œ∏exp)‚Üísi:R3√óRŒ∏id√óRŒ∏exp‚ÜíR,(1)
where xi‚ààR3represents the query points, Œ∏id‚ààR1344
represents identity latent code, Œ∏exp‚ààR200represents ex-
pression latent code and sidenotes the predicted SDF. The
mesh is then extracted as the zero-level iso-surface decision
boundary using Marching Cubes [42].
Diffusion. Diffusion models are a class of generative mod-els that consist of a forward and reverse process. The for-
ward process converts the original structured data distribu-
tion into Gaussian noise, modeled following a fixed Markov
chain as:
q(x1:T|x0) =TY
t=1q(xt|xt‚àí1), (2)
where q(xt|xt‚àí1)denotes the forward process adding
white noise to the original data distribution x0. Mathemati-
cally, the forward process can be written as:
q(xt|x0)‚àº N(‚àöŒ±tx0,(1‚àíŒ±t)I), (3)
where Œ±t‚àà(0,1)are constants following a decreasing co-
sine schedule such that when Œ±ttends to 0, we can approxi-
matexT‚àº N(0,I). The reverse diffusion process progres-
sively denoises samples from xT‚àº N(0,I)into samples
from a learned distribution x0. In our experiments, we learn
a generative model GŒ∏to reverse the forward diffusion pro-
cess by learning to estimate GŒ∏(xt, t,c) =ÀÜx‚âàx, where x
refers to predicting the cleaned samples directly, xtrefers
to noisy input at diffusion timestamp t. Given a conditional
signal c, we optimize model parameters Œ∏for all diffusion
timestamps tusing the following objective:
LŒ∏=Ex,t
‚à•x‚àí GŒ∏(xt, t,c)‚à•2
2
. (4)
4. Method
FaceTalk performs high-fidelity and temporally consistent
generative synthesis of motion sequences of heads, condi-
21265
tioned on audio signal. In order to characterize complex
face motions and fine-scale movements, we synthesize re-
alistic heads in the latent expression space of a paramet-
ric model for volumetric head representations, i.e., neu-
ral parametric head models (NPHMs) [23]. We, thus, de-
velop a speech-conditioned latent diffusion model to syn-
thesize temporally coherent head expression sequences that
are coupled with the NPHM shape space to produce com-
plex, realistic head animations of different identities. An
overview of our approach is illustrated in Fig. 2.
Audio Encoding. We employ a state-of-the-art pre-trained
speech model Wave2Vec 2.0 [6] to encode the audio sig-
nal. Specifically, we first use the audio feature extractor
made up of temporal convolution layers (TCN) to extract
audio feature vectors {ai}Na
i=1from the raw waveform. This
is followed by the Frequency Interpolation layer that aligns
the input audio signal {ai}Na
i=1(captured at frequency fa
= 16kHz) with our dataset {ai}Ne
i=1(captures at framerate
fe= 24Hz). Finally, a stacked multi-layer transformer en-
coder network processes these resampled features and out-
puts the processed and aligned audio feature vectors. The
audio encoder is initialized with the pre-trained wav2vec
2.0 weights, followed by a feedforward layer to project the
aligned audio features into the latent space of our expression
decoder model. These aligned audio features are denoted
as:
A1:N={ai}N
i=1. (5)
These audio features A1:Nare then fed to the
cross-attention layers of the expression decoder via the
expression-audio alignment mask Mto learn speech-
conditioned expression features during training.
Expression Encoding. We train our expression de-
coder network on optimized NPHM expression sequences
Œ∏exp	1:N(obtained in Section 5), where Nrefers to
number of frames in a sequence. We train a diffusion-
based stacked multi-layer transformer [68] decoder network
to synthesize facial expressions in the latent space of the
NPHM model. During training, following forward diffu-
sion (Eq. 3) we add noise for a randomly sampled diffusion
timestamp t‚àºUniform (0, T)to create noisy expression
codes
Œ∏exp	1:N
t. These noisy expression codes are then
projected to the latent space of our model via a linear layer,
followed by a stack of transformer decoder blocks, and then
projected back to the original NPHM space using another
linear layer. To embed diffusion timestamp in the latent
space of our model, we apply sinusoidal embedding and
process it through a three-layer MLP. Next, to fuse diffu-
sion timestamp into the model, we use a one-layer FiLM
(feature-wise linear modulation) network [45] between the
multi-head self-attention, multi-head cross-attention and
feedforward layers of the transformer decoder block, whichis critical for the model to produce high-quality expressions
as we show in the results. We leverage a look-ahead binary
target mask T ‚ààRN√óNin the multi-head self-attention
layers to prevent the model from peeking into the future ex-
pression codes. Mathematically, it can be written as:
Tij=(
True ifi‚â§j
False else(6)
where Tijrefers to the (i, j)thelement of the matrix and
1‚â§i, j‚â§N. The audio features A1:Nare fused into
the network via the multi-head cross-attention layers. We
leverage the expression-audio alignment mask Mto fuse
the audio features into the network to ensure that mouth
poses learned by the expression codes are consistent with
the speech signal. The binary mask M ‚àà RN√óNis Kro-
necker delta function Œ¥ijsuch that the audio features for
ithtimestamp attend to expression features at the jthtimes-
tamp if and only if i=j. Mathematically,
M=Œ¥ij=(
True ifi=j
False ifiÃ∏=j(7)
We demonstrate in the results (Section 6) that this align-
ment is crucial for learning audio-consistent expression
codes. As a by-product, this further enables the model to
generalize to arbitrary long audio signals during inference
which we also show in our results. Our model is trained
with the diffusion loss in Eq. 4, with input x=
Œ∏exp	1:N
and conditioning c=A1:N.
Expression Augmentation. Unlike other domains (like
text-to-motion) where the synthesized motion can vary dra-
matically, speech-driven 3D facial animation is more con-
strained, requiring precise mouth alignment with the audio
signal, making it prone to overfitting. To alleviate this, we
propose a novel augmentation strategy capable of producing
diverse expressions for a given audio signal. Our key insight
is to augment the dataset to generate different expression
codes for the same speech signal by randomly amplifying
and suppressing its magnitude. Specifically, we randomly
sample modulation factor rwithin the bounds [a, b]as:
r‚àºUniform (a, b), (8)
and scale the expression codes as
r√óŒ∏exp	1:N. We show
in the results this augmentation strategy helps the model
synthesize diverse expressions.
Sampling. FaceTalk uses a diffusion-based framework
to learn to synthesize NPHM expression sequences of N
frames
Œ∏exp	1:N‚ààRN√ó200. At each of the denois-
ing timestep t, FaceTalk predicts the denoised sample and
noises it back to timestamp t‚àí1:

Œ∏exp	1:N
t‚àí1=GŒ∏ 
Œ∏exp	1:N
t,A1:N, t‚àí1
, (9)
21266
terminating when it reaches t = 0. We train our model using
classifier-free guidance [28]. We implement classifier-free
guidance by randomly replacing the audio with null condi-
tioning A1:N= Œ¶ during training with 25% probability.
During inference, we can use the weighted combination of
conditional and unconditionally generated samples:

Œ∏c
exp	1:N
t=w.
Œ∏c
exp	1:N
t+ (1‚àíw).
Œ∏u
exp	1:N
t,(10)
where Œ∏c
expandŒ∏u
exprefers to conditionally and uncondi-
tionally generated samples. To amplify the audio condition-
ing, we can use guidance strength w >1.
Sequence Generation. The expression codes predicted
aboveÀÜŒ∏exp	1:Nare then passed to the pretrained NPHM
expression mapper
E	
to obtain expression deformations
Œ¥exp	1:N. We then apply smoothing based on a Gaussian
kernel to these deformations to remove the unwanted wiggle
in the head and neck regions and to ensure that generating
sequences are free from flickering artifacts. Specifically,
we first define a control center Con the mouth region in the
canonical space as:
C= [cx, cy, cz]. (11)
Next, using a 3D gaussian kernel with standard deviation
Œ£ = [ œÉx, œÉy, œÉz], centered at C, for each point sampled
from 3D grid P ‚ààR|X|√ó|Y|√ó|Z|asPxyz= [px, py, pz],
we compute its distance from Cas:
dxyz=s
(px‚àícx)2
œÉ2x+(py‚àícy)2
œÉ2y+(pz‚àícz)2
œÉ2z.(12)
Based on this distance, smoothing weights are obtained
from the Gaussian kernel with min-max normalization as:
wxyz=1
2œÄœÉxœÉyœÉz.e‚àí(1
2√ód2
xyz)(13)
wk
xyz=wk
xyz‚àímin 
w1:M
xyz
max 
w1:Mxyz
‚àímin 
w1:Mxyz, (14)
where M=|X|√ó|Y|√ó|Z|. The obtained expression defor-
mations
Œ¥exp	1:Nare then multiplied with these smooth-
ing weights
w	1:N=
(w1, w2,¬∑¬∑¬∑wM)	1:Nto obtain
smoothed expression deformations as:

Œ¥s
exp	1:N=
Œ¥exp	1:N√ó
w	1:N. (15)
These smoothed expression deformations
Œ¥s
exp	1:N,
along with identity Œ∏idand predicted expression codesÀÜŒ∏exp	1:Nare passed to the NPHM identity MLP
I	
to
obtain smoothed SDF, from which meshes are extracted us-
ing MC [42].
ExpressionMLPInput Point Cloud (Query Points)Expression Codes‚Ä¶{Œ∏iexp}Ni=1Identity CodeŒ∏id
‚Ñítotal
IdentityMLP{ùí´i}Ni=1Expression DeformationsNPHMMarching Cubes
+Extracted NPHM Sequence
FrozenLearnable
NeRSemble Dataset
COLMAP{Œ¥iexp}Ni=1Figure 3. Given the pointcloud sequence
Pi	N
i=1extracted
from multi-view sequences from NeRSemble dataset [37] (bot-
tom right), which also act as query points, we leverage the pre-
trained Expression MLP
E	
to extract the expression deforma-
tions
Œ¥i
exp	1:Nand add them back to the input points to get
the deformed points
P‚Ä≤
i	N
i=1. These points are then fed to the
Identity MLP
I	
which outputs the SDF. The expression codes
Œ∏i
exp	1:Nare optimized using overall loss Ltotal. Note that both
fixed identity code Œ∏idand learnable expression codes
Œ∏i
exp	1:N
are fed to both
I	
and
E	
. Once optimized, the meshes are
then extracted with Marching Cubes [42].
5. Dataset Creation
To train our latent diffusion model, we require temporally
consistent NPHM expression codes as well as their cor-
responding audio. To this end, we leverage multi-view
recordings of NeRSemble [37] captured with 16 cameras of
talking human faces paired with corresponding audio sig-
nals. Notably, these sequences are captured for faces al-
ready present in the identity space of the NPHM model.
Thus, for a given subject, only the expression codes corre-
sponding to the multi-view sequences have to be estimated.
Specifically, we first extract 3D pointclouds
Pi	N
i=1from
individual frames using COLMAP [54], where Nrefers to
the number of frames in a sequence, with each Pi‚ààRK√ó3
consisting of Kpoints. Given the pretrained NPHM model,
for the known facial shape Œ∏id, we then optimize the expres-
sion codes
Œ∏i
exp	N
i=1to match the pointclouds. In partic-
ular, at every iteration, we randomly sample a subset Siof
5000 points from PiasSi‚àºUniform (5000 ,Pi), and then
feed the sampled points Si, identity code Œ∏idand learnable
expression code Œ∏i
expinto the expression decoder
E	
to
obtain the expression deformation Œ¥ifor the sampled points
Si. The deformation Œ¥iis then added to Sito obtain de-
formed points Di=Si+Œ¥i. These deformed points Di
are finally fed to the identity decoder
I	
along with la-
tent codes Œ∏idandŒ∏i
expto obtain the SDF. The expression
21267
frownwheneventsturn
frownwheneventsturn[silence]VOCAMeshTalkFaceformerCodeTalker
ImitatorEmoTalkEmoteOurs[silence]Figure 4. Qualitative comparison for audio-driven face animation. Our approach maintains high fidelity while demonstrating rich mouth
and nasolabial movements. In particular, we demonstrate more accurate lip articulation, precisely synchronized to phonetic movements.
codes are then optimized using the loss Ltotal which is il-
lustrated in Fig. 3. Naively optimizing the expression codes
on a per-frame basis shows flickering artifacts. To mitigate
this, we optimize expressions in groups of n= 10 frames in
a sliding window fashion (with an overlap of 2 frames be-
tween adjacent windows) using an additional temporal reg-
ularization Ltemp. To further prevent the expression codes
Œ∏i
exp	N
i=1from deviating too much from the distribution
already learned by the NPHM model, we employ additional
L2 expression regularization Lexp. We minimize the objec-
tiveLtotal defined as:
Ltotal=Œªsd fLsd f+ŒªtempLtemp+ŒªregLreg. (16)
The SDF loss Lsd f, temporal regularization Ltemp, and ex-
pression regularization Lexpcan be defined as:
Lsd f=|n|X
i=11
|S||S|X
k=1SDF(Sk
i)
1(17)
Ltemp=|n|X
i=1Œ∏i+1
exp‚àíŒ∏i
exp
œµ,Lexp=|n|X
i=1Œ∏i
exp
2,
(18)
where ‚à•.‚à•1,‚à•.‚à•2and‚à•.‚à•œµrefers to L1, L2 and Huber loss
respectively, and nrefers to number of frames simultane-
ously optimized. More details regarding aligning the co-
ordinate system of NeRSemble dataset [37] with NPHMcan be found in the supplemental document. Once op-
timized, next to generate mesh sequences, we uniformly
sample points from a 3D grid P ‚àà R|X|√ó|Y|√ó|Z|such
thatPxyz= [px, py, pz]is a point in 3d space. These
points, along with the identity Œ∏idand expression codes
Œ∏i
exp	N
i=1are passed to the pretrained NPHM model out-
putting an SDF; meshes are then extracted with [42]. In
total, our FaceTalk dataset consists of 1000 sequences, an
order of magnitude larger than the existing datasets [14].
6. Results
We evaluate FaceTalk on the task audio-driven motion syn-
thesis and compare it against state-of-the-art methods.
Evaluation Metrics. Unlike template-based methods that
rely on LVE (Lip Vertex Error), our method produces SDFs
for each frame; extracted mesh topologies thus vary across
frames, making LVE inapplicable for evaluation. Thus, we
employ LSE-D (Lip Sync Error Distance) [48] for quanti-
tative evaluation of lip synchronization. For diversity, we
compute FID and KID, as well as diversity scores [50]. As
FID/KID may not reflect all quality considerations, when
samples are limited, we include two established quality
metrics: (1) FIQA (Face Image Quality Assessment) [43]
(2) VQA (Video Quality Assessment) [74]. FIQA quanti-
fies image quality for face recognition and similarity to in-
the-wild real faces and VQA measures overall video qual-
21268
portisstrongwine[silence]
[silence]crampnodangerswim
[silence]writefriendnotecherish
Different Identities
Diverse ExpressionsFigure 5. Left. Expressions generated by our method can easily be applied to diverse identities with complex geometry. Right. Given a
speech signal, our method can further generate diversity in expression for the same identity. Note the difference in speaking style (intensity
of mouth opening) as well as eyeblinks/frowning in the upper face area.
ity in terms of distortions/semantics/aesthetics. Finally, for
perceptual evaluation, we perform a user study with 40 par-
ticipants on a diverse set of 15 unseen audio clips. Most
baseline methods are trained using V ocaset [14]. Thus, for
a fair comparison, we construct a hybrid audio test set of
100 sequences, with 25 test audio clips each from V ocaset
(‚àº2-3 seconds) and ours ( ‚àº2-3 seconds), and 50 ( ‚àº5-
7 seconds) from LJSpeech [31]. Test audios were selected
from identities not seen during training of any method. The
metrics are reported on this hybrid test set.
Implementation Details. For dataset creation, we optimize
NPHM expressions for 500 iterations in a group of n= 10
frames with a step size of 0.001 for iterations ‚â§300, and
0.0001 otherwise. We use Œªsd f= 10 ,Œªtemp = 0.1and
Œªreg= 0.0025 for Eq. 16 during optimization. To train our
diffusion model, we randomly clip sequences to 2 seconds
(48 frames) for efficient minibatching. We train with Adam
optimizer with a learning rate of 0.0001. For diffusion, we
use 1000 noising/denoising timestamps, a cosine schedule
to add noise to input sequences. We apply data augmenta-
tion to the expression codes by uniformly modulating them.
Baseline Comparisons. This is the first work to per-
form speech-conditioned synthesis for volumetric head mo-
tion sequences. We compare with state-of-the-art template-
based methods in Fig 4 and Tab. 1. Specifically, we compare
against speech-driven animation methods [14, 21, 51, 75],Method LSE-D ‚ÜìFID‚Üì KID‚ÜìFIQA‚ÜëVQA‚Üë
VOCA [14] 13.6191 239.043 0.280 33.36 0.4251
MeshTalk [51] 12.9607 220.172 0.254 36.14 0.4855
FaceFormer [21] 11.9848 215.274 0.222 38.24 0.5227
CodeTalker [75] 11.8054 208.064 0.207 38.38 0.5274
Imitator [65] 11.6119 208.479 0.212 38.44 0.5384
EmoTalk [44] 11.7485 201.311 0.201 31.80 0.5037
EMOTE [15] 11.7192 227.924 0.247 39.44 0.5149
Ours 11.2737 40.692 0.009 45.75 0.6145
Table 1. In comparison to state of the art, FaceTalk more accu-
rately matches the audio, while producing high perceptual fidelity.
personalized methods [65], as well as recent emotion-based
methods [15, 44]. Our approach more accurately captures
subtle mouth movements compared to less expressive base-
lines, resulting in better FID/KID. These scores were eval-
uated only on mouth region crops to avoid bias towards rest
of facial geometry. Our method consistently achieves better
lip-audio synchronization while also representing fine-scale
facial details like creasing in the nasolabial folds with wider
mouth motions. This is confirmed by our perceptual user
study in Fig 6.
Generative Synthesis. Our generated expression codes
are identity-agnostic, and are easily transferred to different
identities, as shown in Fig. 5. Additionally, we can synthe-
size diverse expressions per-identity for a given audio.
21269
75.56
71.1173.89
12.4415.33
11.33
5.228.786.22
Overall Quality Lip Sync  Facial Realism01020304050607080
FaceT alk
Imitator
CodeT alkerFigure 6. User study comparison with baselines. We measure pref-
erence for (1) Overall Animation Quality, (2) Lip Synchronization
and (3) Facial Realism. FaceTalk results are overwhelmingly pre-
ferred over the best baseline methods on all these aspects.
Our Method LSE-D ‚ÜìDiversity ‚Üë
w/o expr. aug. 11.6229 1.61e-8
w/o facial smoothing 11.3488 0.34
w/o FiLM layer 12.029 0.006
w/o expr.-audio align. 13.720 5.17e-8
w/o diffusion 11.3217 0.000
Full (Ours) 11.2737 0.34
Table 2. Ablation over model design. Expression augmentation
improves diversity, while facial smoothing alleviates inter-frame
jitter. Using FiLM conditioning achieves accurate mouth pose
and increased diversity. Without expression-audio alignment, the
model ignores the audio signal. Without diffusion, the model fails
to generate diverse results. Our full model with all components
achieves the best results.
Architecture Ablations. We ablate our model design
choices in Tab 2, with visual results shown in Fig 7, and
refer to the supplemental for video demonstration. Our
proposed expression augmentation helps the model to syn-
thesize diverse expressions. Facial smoothing removes un-
wanted wobbliness in the head and neck region, thereby re-
ducing the inter-frame jitter. FiLM conditioning ensures
that expressions generated by our method are more pro-
nounced and that expressions accurately match the audio
signal. The expression-audio alignment mask is necessary
to match the synthesized expression codes with the audio
signal. Our diffusion training ensures that expressions gen-
erated by our method are high-quality and diverse.
Limitations. While FaceTalk effectively synthesizes high-
fidelity and audio-synchronized facial expressions, it still
has limitations. For instance, the use of a diffusion model
requires multiple denoising steps during inference, limiting
its real-time application. We believe that this could be im-
proved by investigating efficient sampling techniques [72].
Currently, our method specializes in synthesizing only the
expression codes. For holistic 3D facial animation, we need
to extend its capability to synthesize facial identities. In the
future, we would like to generate diverse identities aligned
w/o expression audio alignmentFull[silence]pluckbrightroseleavesw/o FiLM conditioning 
Figure 7. Without FiLM conditioning, the model fails to synthe-
size accurate lip synchronization and even generates uncanny ex-
pressions. Without expression-audio alignment, the model fully
ignores the audio signal synthesizing constant expression. Our full
diffusion model synthesizes accurate lip articulation while main-
taining temporal coherence which can be seen in the suppl. video.
with the gender inferred directly from the audio.
7. Conclusion
In this work, we have introduced FaceTalk, a new ap-
proach to synthesize animations of realistic volumetric hu-
man heads from audio. We introduced the first latent dif-
fusion model for audio-driven head animation, producing
significantly higher fidelity results than existing methods.
FaceTalk leverages a parametric head model producing vol-
umetric head representations to generate expressive, fine-
grained motions such as eye blinks, skin creasing, etc. We
also demonstrate the applicability of our method for other
conditioning signals such as facial landmarks. We believe
this is an important first step towards enabling the animation
of highly detailed 3D face models, which can enable many
new possibilities for content creation and digital avatars.
8. Acknowledgments
This work was supported by the ERC Starting Grant
Scan2CAD (804724), the Bavarian State Ministry of Sci-
ence and the Arts and coordinated by the Bavarian Research
Institute for Digital Transformation (bidt), the German Re-
search Foundation (DFG) Grant ‚ÄúMaking Machine Learn-
ing on Static and Dynamic 3D Data Practical,‚Äù the Ger-
man Research Foundation (DFG) Research Unit ‚ÄúLearning
and Simulation in Visual Computing,‚Äù and Sony Semicon-
ductor Solutions Corporation. We would like to thank Si-
mon Giebenhain and Tobias Kirschstein for their help with
dataset.
21270
References
[1] Chaitanya Ahuja and Louis-Philippe Morency. Lan-
guage2pose: Natural language grounded pose forecasting,
2019. 1
[2] Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and
Gustav Eje Henter. Listen, denoise, action! audio-driven mo-
tion synthesis with diffusion models. ACM Trans. Graph. , 42
(4):44:1‚Äì44:20, 2023. 1, 2
[3] Nikos Athanasiou, Mathis Petrovich, Michael J. Black, and
G¬®ul Varol. TEACH: Temporal Action Compositions for 3D
Humans. In International Conference on 3D Vision (3DV) ,
2022. 1
[4] Nikos Athanasiou, Mathis Petrovich, Michael J. Black, and
G¬®ul Varol. SINC: Spatial composition of 3D human motions
for simultaneous action generation. In International Confer-
ence on Computer Vision (ICCV) , 2023. 1
[5] Samaneh Azadi, Akbar Shah, Thomas Hayes, Devi Parikh,
and Sonal Gupta. Make-an-animation: Large-scale text-
conditional 3d human motion generation, 2023. 2
[6] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and
Michael Auli. wav2vec 2.0: A framework for self-supervised
learning of speech representations, 2020. 2, 3, 4
[7] V olker Blanz and Thomas Vetter. A morphable model for the
synthesis of 3d faces. In Seminal Graphics Papers: Pushing
the Boundaries, Volume 2 , pages 157‚Äì164. 2023. 1
[8] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-
horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.
Align your latents: High-resolution video synthesis with la-
tent diffusion models. In IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , 2023. 2
[9] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki
Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,
Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero
Karras, and Gordon Wetzstein. Efficient geometry-aware 3D
generative adversarial networks. In arXiv , 2021. 2
[10] Lele Chen, Zhiheng Li, Ross K. Maddox, Zhiyao Duan, and
Chenliang Xu. Lip movements generation at a glance, 2018.
2
[11] Lele Chen, Ross K Maddox, Zhiyao Duan, and Chenliang
Xu. Hierarchical cross-modal talking face generation with
dynamic pixel-wise loss. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
7832‚Äì7841, 2019.
[12] Lele Chen, Guofeng Cui, Celong Liu, Zhong Li, Ziyi Kou, Yi
Xu, and Chenliang Xu. Talking-head generation with rhyth-
mic head motion. arXiv preprint arXiv:2007.08547 , 2020.
[13] Joon Son Chung, Amir Jamaludin, and Andrew Zisserman.
You said that?, 2017. 2
[14] Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw, Anurag
Ranjan, and Michael Black. Capture, learning, and synthesis
of 3D speaking styles. In Proceedings IEEE Conf. on Com-
puter Vision and Pattern Recognition (CVPR) , pages 10101‚Äì
10111, 2019. 2, 6, 7
[15] Radek Dan ÀáeÀácek, Kiran Chhatre, Shashank Tripathi, Yandong
Wen, Michael Black, and Timo Bolkart. Emotional speech-
driven animation with content-emotion disentanglement. InSIGGRAPH Asia 2023 Conference Papers , New York, NY ,
USA, 2023. Association for Computing Machinery. 2, 7
[16] Dipanjan Das, Sandika Biswas, Sanjana Sinha, and Brojesh-
war Bhowmick. Speech-driven facial animation using cas-
caded gans for learning of motion and texture. In Computer
Vision ‚Äì ECCV 2020: 16th European Conference, Glas-
gow, UK, August 23‚Äì28, 2020, Proceedings, Part XXX , page
408‚Äì424, Berlin, Heidelberg, 2020. Springer-Verlag. 2
[17] Prafulla Dhariwal and Alex Nichol. Diffusion models beat
gans on image synthesis, 2021. 2
[18] Rohan Dhesikan and Vignesh Rajmohan. Sketching the fu-
ture (stf): Applying conditional control techniques to text-
to-video models, 2023. 2
[19] Michail Christos Doukas, Stefanos Zafeiriou, and Viktoriia
Sharmanska. Headgan: One-shot neural head synthesis and
editing. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision (ICCV) , pages 14398‚Äì14407,
2021. 2
[20] Fanda Fan, Chaoxu Guo, Litong Gong, Biao Wang, Tiezheng
Ge, Yuning Jiang, Chunjie Luo, and Jianfeng Zhan. Hier-
archical masked 3d diffusion model for video outpainting,
2023. 2
[21] Yingruo Fan, Zhaojiang Lin, Jun Saito, Wenping Wang, and
Taku Komura. Faceformer: Speech-driven 3d facial anima-
tion with transformers. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , 2022. 2, 7
[22] Guy Gafni, Justus Thies, Michael Zollh ¬®ofer, and Matthias
Nie√üner. Dynamic neural radiance fields for monocular 4d
facial avatar reconstruction. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 8649‚Äì8658, 2021. 2
[23] Simon Giebenhain, Tobias Kirschstein, Markos Georgopou-
los, Martin R ¬®unz, Lourdes Agapito, and Matthias Nie√üner.
Learning neural parametric head models. In Proc. IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR) ,
2023. 2, 3, 4
[24] Simon Giebenhain, Tobias Kirschstein, Markos Georgopou-
los, Martin R ¬®unz, Lourdes Agapito, and Matthias Nie√üner.
MonoNPHM: Dynamic head reconstruction from monocular
videos. In Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR) , 2024. 2, 3
[25] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao
Sun, Annan Deng, Minglun Gong, and Li Cheng. Ac-
tion2motion: Conditioned generation of 3d human motions.
InProceedings of the 28th ACM International Conference on
Multimedia , page 2021‚Äì2029, New York, NY , USA, 2020.
Association for Computing Machinery. 1
[26] Yudong Guo, Keyu Chen, Sen Liang, Yongjin Liu, Hujun
Bao, and Juyong Zhang. Ad-nerf: Audio driven neural radi-
ance fields for talking head synthesis. In IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV) , 2021. 2
[27] Siddharth Gururani, Arun Mallya, Ting-Chun Wang, Rafael
Valle, and Ming-Yu Liu. Space: Speech-driven portrait ani-
mation with controllable expression, 2022. 2
[28] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. In NeurIPS 2021 Workshop on Deep Generative
Models and Downstream Applications , 2021. 5
21271
[29] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. arXiv preprint arxiv:2006.11239 ,
2020. 2
[30] Rongjie Huang, Max W. Y . Lam, Jun Wang, Dan Su, Dong
Yu, Yi Ren, and Zhou Zhao. Fastdiff: A fast conditional
diffusion model for high-quality speech synthesis, 2022. 2
[31] Keith Ito and Linda Johnson. The lj speech dataset. https:
//keithito.com/LJ-Speech-Dataset/ , 2017. 7
[32] Xinya Ji, Hang Zhou, Kaisiyuan Wang, Wayne Wu,
Chen Change Loy, Xun Cao, and Feng Xu. Audio-driven
emotional video portraits. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR) , 2021. 2
[33] Xinya Ji, Hang Zhou, Kaisiyuan Wang, Qianyi Wu, Wayne
Wu, Feng Xu, and Xun Cao. Eamm: One-shot emotional
talking face via audio-based emotion-aware motion model.
InACM SIGGRAPH 2022 Conference Proceedings , 2022. 2
[34] Prajwal K R, Rudrabha Mukhopadhyay, Jerin Philip, Ab-
hishek Jha, Vinay Namboodiri, and C V Jawahar. Towards
automatic face-to-face translation. In Proceedings of the
27th ACM International Conference on Multimedia , page
1428‚Äì1436, New York, NY , USA, 2019. Association for
Computing Machinery. 2
[35] Tero Karras, Timo Aila, Samuli Laine, Antti Herva, and
Jaakko Lehtinen. Audio-driven facial animation by joint end-
to-end learning of pose and emotion. ACM Trans. Graph. , 36
(4), 2017. 2
[36] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free-
form language-based motion synthesis & editing. arXiv
preprint arXiv:2209.00349 , 2022. 1, 2
[37] Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim
Walter, and Matthias Nie√üner. Nersemble: Multi-view ra-
diance field reconstruction of human heads. ACM Trans.
Graph. , 42(4), 2023. 2, 5, 6
[38] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and
Bryan Catanzaro. Diffwave: A versatile diffusion model for
audio synthesis, 2021. 2
[39] Max WY Lam, Jun Wang, Dan Su, and Dong Yu. Bddm:
Bilateral denoising diffusion models for fast and high-quality
speech synthesis. In International Conference on Learning
Representations , 2022. 2
[40] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and
Javier Romero. Learning a model of facial shape and ex-
pression from 4D scans. ACM Transactions on Graphics,
(Proc. SIGGRAPH Asia) , 36(6):194:1‚Äì194:17, 2017. 1
[41] Xian Liu, Yinghao Xu, Qianyi Wu, Hang Zhou, Wayne
Wu, and Bolei Zhou. Semantic-aware implicit neural
audio-driven video portrait generation. arXiv preprint
arXiv:2201.07786 , 2022. 2
[42] William E. Lorensen and Harvey E. Cline. Marching cubes:
A high resolution 3d surface construction algorithm. In
Proceedings of the 14th Annual Conference on Computer
Graphics and Interactive Techniques , page 163‚Äì169, New
York, NY , USA, 1987. Association for Computing Machin-
ery. 3, 5, 6
[43] Fu-Zhao Ou, Xingyu Chen, Ruixin Zhang, Yuge Huang,
Shaoxin Li, Jilin Li, Yong Li, Liujuan Cao, and Yuan-GenWang. SDD-FIQA: Unsupervised face image quality assess-
ment with similarity distribution distance. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , 2021. 6
[44] Ziqiao Peng, Haoyu Wu, Zhenbo Song, Hao Xu, Xiangyu
Zhu, Jun He, Hongyan Liu, and Zhaoxin Fan. Emotalk:
Speech-driven emotional disentanglement for 3d face anima-
tion. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision (ICCV) , pages 20687‚Äì20697, 2023.
2, 7
[45] Ethan Perez, Florian Strub, Harm de Vries, Vincent Du-
moulin, and Aaron Courville. Film: Visual reasoning with a
general conditioning layer, 2017. 3, 4
[46] Mathis Petrovich, Michael J. Black, and G ¬®ul Varol. Action-
conditioned 3D human motion synthesis with transformer
V AE. In International Conference on Computer Vision
(ICCV) , 2021. 1
[47] Mathis Petrovich, Michael J. Black, and G ¬®ul Varol. TEMOS:
Generating diverse human motions from textual descriptions.
InEuropean Conference on Computer Vision (ECCV) , 2022.
1
[48] K R Prajwal, Rudrabha Mukhopadhyay, Vinay P. Nambood-
iri, and C.V . Jawahar. A lip sync expert is all you need
for speech to lip generation in the wild. In Proceedings of
the 28th ACM International Conference on Multimedia , page
484‚Äì492, New York, NY , USA, 2020. Association for Com-
puting Machinery. 2, 6
[49] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents, 2022. 2
[50] Zhiyuan Ren, Zhihong Pan, Xin Zhou, and Le Kang. Dif-
fusion motion: Generate text-guided 3d human motion by
diffusion model, 2023. 6
[51] Alexander Richard, Michael Zollh ¬®ofer, Yandong Wen, Fer-
nando de la Torre, and Yaser Sheikh. Meshtalk: 3d face an-
imation from speech using cross-modality disentanglement.
InProceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV) , pages 1173‚Äì1182, 2021. 2, 7
[52] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ÀúA¬∂rn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , 2022. 2
[53] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,
Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J
Fleet, and Mohammad Norouzi. Photorealistic text-to-image
diffusion models with deep language understanding, 2022. 2
[54] Johannes Lutz Sch ¬®onberger and Jan-Michael Frahm.
Structure-from-motion revisited. In Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2016. 5
[55] Shuai Shen, Wanhua Li, Zheng Zhu, Yueqi Duan, Jie Zhou,
and Jiwen Lu. Learning dynamic facial radiance fields for
few-shot talking head synthesis. In ECCV , 2022. 2
[56] Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li, Zheng
Zhu, Jie Zhou, and Jiwen Lu. Difftalk: Crafting diffusion
21272
models for generalized audio-driven portraits animation. In
CVPR , 2023. 2
[57] Linsen Song, Wayne Wu, Chen Qian, Ran He, and
Chen Change Loy. Everybody‚Äôs talkin‚Äô: Let me talk as you
want, 2020. 2
[58] Stefan Stan, Kazi Injamamul Haque, and Zerrin Yumak.
Facediffuser: Speech-driven 3d facial animation synthesis
using diffusion. In Proceedings of the 16th ACM SIGGRAPH
Conference on Motion, Interaction and Games , New York,
NY , USA, 2023. Association for Computing Machinery. 2
[59] Micha≈Ç Stypu≈Çkowski, Konstantinos V ougioukas, Sen He,
Maciej Zikeba, Stavros Petridis, and Maja Pantic. Diffused
Heads: Diffusion Models Beat GANs on Talking-Face Gen-
eration. In https://arxiv.org/abs/2301.03396 , 2023. 2
[60] Zhiyao Sun, Tian Lv, Sheng Ye, Matthieu Gaetan Lin, Jenny
Sheng, Yu-Hui Wen, Minjing Yu, and Yong jin Liu. Diff-
posetalk: Speech-driven stylistic 3d facial animation and
head pose generation via diffusion models, 2023. 2
[61] Supasorn Suwajanakorn, Steven M. Seitz, and Ira
Kemelmacher-Shlizerman. Synthesizing obama: Learning
lip sync from audio. ACM Trans. Graph. , 2017. 2
[62] Anni Tang, Tianyu He, Xu Tan, Jun Ling, Runnan Li, Sheng
Zhao, Li Song, and Jiang Bian. Memories are one-to-
many mapping alleviators in talking face generation. arXiv
preprint arXiv:2212.05005 , 2022. 2
[63] Jiapeng Tang, Angela Dai, Yinyu Nie, Lev Markhasin, Justus
Thies, and Matthias Niessner. Dphms: Diffusion parametric
head models for depth-based tracking. 2024. 2
[64] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel
Cohen-or, and Amit Haim Bermano. Human motion diffu-
sion model. In The Eleventh International Conference on
Learning Representations , 2023. 1, 2
[65] Balamurugan Thambiraja, Ikhsanul Habibie, Sadegh Aliak-
barian, Darren Cosker, Christian Theobalt, and Justus Thies.
Imitator: Personalized speech-driven 3d facial animation. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) , pages 20621‚Äì20631, 2023. 2, 7
[66] Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian
Theobalt, and Matthias Nie√üner. Neural voice puppetry:
Audio-driven facial reenactment. ECCV 2020 , 2020. 2
[67] Jonathan Tseng, Rodrigo Castellon, and C Karen Liu. Edge:
Editable dance generation from music. arXiv preprint
arXiv:2211.10658 , 2022. 1, 2
[68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need, 2023. 3, 4
[69] Vikram V oleti, Alexia Jolicoeur-Martineau, and Christopher
Pal. Mcvd: Masked conditional video diffusion for predic-
tion, generation, and interpolation. In (NeurIPS) Advances
in Neural Information Processing Systems , 2022. 2
[70] Konstantinos V ougioukas, Stavros Petridis, and Maja Pan-
tic. End-to-end speech-driven facial animation with temporal
gans, 2018. 2
[71] Konstantinos V ougioukas, Stavros Petridis, and Maja Pantic.
Realistic speech-driven facial animation with gans, 2019. 2
[72] Daniel Watson, Jonathan Ho, Mohammad Norouzi, and
William Chan. Learning to efficiently sample from diffusion
probabilistic models, 2021. 8[73] O. Wiles, A.S. Koepke, and A. Zisserman. X2face: A net-
work for controlling face generation by using images, audio,
and pose codes. In European Conference on Computer Vi-
sion, 2018. 2
[74] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jing-
wen Hou Hou, Annan Wang, Wenxiu Sun Sun, Qiong Yan,
and Weisi Lin. Exploring video quality assessment on user
generated contents from aesthetic and technical perspectives.
InInternational Conference on Computer Vision (ICCV) ,
2023. 6
[75] Jinbo Xing, Menghan Xia, Yuechen Zhang, Xiaodong Cun,
Jue Wang, and Tien-Tsin Wong. Codetalker: Speech-driven
3d facial animation with discrete motion prior. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 12780‚Äì12790, 2023. 2, 7
[76] Shunyu Yao, RuiZhe Zhong, Yichao Yan, Guangtao Zhai,
and Xiaokang Yang. Dfa-nerf: Personalized talking head
generation via disentangled face attributes neural rendering.
arXiv preprint arXiv:2201.00791 , 2022. 2
[77] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli
Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi
Shen. T2m-gpt: Generating human motion from textual de-
scriptions with discrete representations. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , 2023. 1
[78] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models, 2023.
2
[79] Hang Zhou, Yu Liu, Ziwei Liu, Ping Luo, and Xiaogang
Wang. Talking face generation by adversarially disentangled
audio-visual representation. In AAAI Conference on Artifi-
cial Intelligence (AAAI) , 2019. 2
[80] Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevar-
ria, Evangelos Kalogerakis, and Dingzeyu Li. Makelttalk:
Speaker-aware talking-head animation. ACM Trans. Graph. ,
39(6), 2020. 2
[81] Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei
Liu, and Lequan Yu. Taming diffusion models for audio-
driven co-speech gesture generation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10544‚Äì10553, 2023. 2
21273
