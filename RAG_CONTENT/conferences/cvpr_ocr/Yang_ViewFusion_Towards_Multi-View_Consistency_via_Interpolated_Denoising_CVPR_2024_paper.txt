ViewFusion: Towards Multi-View Consistency via Interpolated Denoising
Xianghui Yang1,2*, Yan Zuo1, Sameera Ramasinghe1, Loris Bazzani1, Gil Avraham1, Anton van den Hengel1,3
1Amazon,2The University of Sydney,3The University of Adelaide
Abstract
Novel-view synthesis through diffusion models has
demonstrated remarkable potential for generating diverse
and high-quality images. Yet, the independent process of
image generation in these prevailing methods leads to chal-
lenges in maintaining multiple view consistency. To ad-
dress this, we introduce ViewFusion, a novel, training-
free algorithm that can be seamlessly integrated into ex-
isting pre-trained diffusion models. Our approach adopts
an auto-regressive method that implicitly leverages previ-
ously generated views as context for next view generation,
ensuring robust multi-view consistency during the novel-
view generation process. Through a diffusion process that
fuses known-view information via interpolated denoising,
our framework successfully extends single-view conditioned
models to work in multiple-view conditional settings with-
out any additional fine-tuning. Extensive experimental re-
sults demonstrate the effectiveness of ViewFusion in gener-
ating consistent and detailed novel views.
1. Introduction
Humans have a remarkable capacity for visualizing un-
seen perspectives from just a single image view – an in-
tuitive process that remains complex to model. Such an
ability is known as Novel View Synthesis (NVS) and ne-
cessitates robust geometric priors to accurately infer three-
dimensional details from flat imagery; lifting from a two-
dimensional projection to a three-dimensional form in-
volves assumptions and knowledge about the nature of the
object and space. Recently, significant advancements in
NVS have been brought forward by neural networks [14,
15, 30, 41, 60, 64, 65, 72, 75, 76], where novel view gen-
eration for downstream reconstruction shows promising po-
tential [34, 69].
Specifically, diffusion models [19, 51] and their ability
to generate high-quality 2D images have garnered signif-
icant attention in the 3D domain, where pre-trained, text-
conditioned 2D diffusion models have been re-purposed for
3D applications via distillation [4, 31, 40, 47, 49, 57, 63,
*Work done during internship at Amazon
View 2View 12nd Denoise1st DenoiseCondition
(a) Inconsistent generation.
2nd Denoise1st Denoise
View 1View 2Condition
 (b) Consistent generation (ours).
Figure 1. The cause of multi-view inconsistency in diffusion-
based novel-view synthesis models. (a) Diffusion models incor-
porate randomness for diversity and better distribution modeling;
this independent generation process produces realistic views un-
der specific instances but may produce different plausible views
for various instances, lacking alignment across adjacent views. (b)
In contrast, ViewFusion incorporates an auto-regressive process
to reduce uncertainty and achieve multi-view consistency, by en-
suring a correlated denoising process that ends at the same high-
density area, fostering consistency across views.
68, 73]. Follow-up approaches [34, 69] remove the require-
ment of text conditioning and instead take an image and tar-
get pose as conditions for NVS. However, distillation [63]
is still required as the diffusion model cannot produce the
multi-view consistent outputs that are appropriate for cer-
tain downstream tasks ( e.g., optimizing Neural Radiance
Fields (NeRFs) [41]).
Under the single-view setting, maintaining multi-view
consistency remains particularly challenging since there
may exist several plausible outputs for a novel view that
are aligned with the given input image. For diffusion-based
approaches which generate novel views in an independent
manner [34, 69], this results in synthesized views contain-
ing artifacts of multi-view inconsistency (Fig. 1a). Previous
work [33, 36, 38, 53, 74, 77] focuses on improving the ro-
bustness of the downstream reconstruction to address the
inconsistency issue, including feature projection layers in
the NeRF [33] or utilising three-dimensional priors to con-
strain NeRF optimization [36, 77], yet these techniques re-
quire training or fine-tuning to align additional modules to
the original diffusion models.
In this work, we address the multi-view inconsistency
that arises during the process of view synthesis. Rather than
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
9870
independently synthesizing views conditioned only on the
initial reference image, we develop a novel approach where
each subsequently generated view is also conditioned on the
entire set of previously generated views. Specifically, our
method incorporates an auto-regressive process into the dif-
fusion process to model the joint distribution of views, guid-
ing our novel-view synthesis by maintaining the denoising
direction towards the same high density area of already gen-
erated views (Fig. 1b).
Our framework, named ViewFusion, relaxes the single-
view conditioning requirement of typical diffusion models
through an interpolated denoising process. ViewFusion of-
fers several additional advantages: 1) it can utilize all avail-
able views as guidance, thereby enhancing the quality of
generated images by incorporating more information; 2) it
does not require any additional fine-tuning, effortlessly con-
verting pre-trained single-view conditioned diffusion mod-
els into multi-view conditioned diffusion models; 3) it pro-
vides greater flexibility in setting adaptive weights for con-
dition images based on their relative view distance to the
target view.
The contributions of this paper are the following:
• We propose a training-free algorithm which can be di-
rectly applied to pre-trained diffusion models to improve
multi-view consistency of synthesized views and supports
multiple conditional inputs.
• Our method utilizes a novel, auto-regressive approach
which we call Interpolated Denoising , that implicitly ad-
dresses key limitations of previous auto-regressive ap-
proaches for view synthesis.
• Extensive empirical analysis on ABO [6] and GSO [9]
show that our method is able to achieve better 3D consis-
tency in image generation, leading to significant improve-
ments in novel view synthesis and 3D reconstruction of
shapes under single-view and multi-view image settings
over other baseline methods.
2. Related Work
2.1. 3D-adapted Diffusion Models
Diffusion models have excelled in image generation us-
ing conditional inputs [20, 46, 50, 84] and given this suc-
cess in the 2D domain, recent works have tried to extend
diffusion models to 3D content generation [1, 3, 5, 10,
16, 18, 22, 24, 25, 27, 37, 43–45, 66, 82, 83] – although
the scarcity of 3D data presents a significant challenge to
directly train these diffusion models. Nonetheless, pio-
neer works such as DreamFusion [47] and Score Jacobian
Chaining [63] leverage pre-trained text-conditioned diffu-
sion models to craft 3D models via distillation. Follow-up
approaches [4, 31, 57, 68] improve this distillation in terms
of speed, resolution and shape quality. Approaches such as
[40, 49, 57, 73] extend upon this to support image condi-tions through the use of captions with limited success due
to the non-trivial nature of textual inversion [13].
2.2. Novel View Synthesis Diffusion Models
Another line of research [2, 8, 17, 28, 35, 56, 58, 61,
62, 69, 71, 78, 80, 86] directly applies 2D diffusion mod-
els to generate multi-view images for shape reconstruc-
tion. To circumvent the weakness of text-conditioned diffu-
sion models, novel-view synthesis diffusion models [34, 69]
have also been explored, which take an image and tar-
get pose as conditions to generate novel views. However,
for these approaches, recovering a 3D consistent shape
is still a key challenge. To mitigate 3D inconsistency,
Liu et al. [33] suggests training a Neural Radiance Field
(NeRF) with feature projection layers. Concurrently, other
works [36, 38, 70, 74, 77] add modules to original diffusion
models for multi-view consistency, including epipolar at-
tention [77], synchronized multi-view noise predictor [36]
and cross-view attention [38, 70]; although these methods
require fine-tuning an already pre-trained model. We adopt
a different paradigm, instead of extending a single-view dif-
fusion model with additional trainable models that incor-
porate multi-view conditions, our training-free method en-
ables pre-trained diffusion models to incorporate previously
generated views via the denoising step and holistically ex-
tends these models into multi-view settings.
2.3. Other Single-view Reconstruction Methods
Before the prosperity of generative models used in 3D re-
construction, many works [11, 12, 14, 15, 26, 29, 30, 59,
64, 75] reconstructed 3D shapes from single-view images
using regression [14, 15, 29, 64, 75] or retrieval [59], both
of which face difficulties in generalizing to real data or
new categories. Methods based on Neural Radiance Fields
(NeRFs) [41] have found success in novel-view synthesis,
but these approaches typically depend on densely captured
images with accurately calibrated camera positions. Cur-
rently, several studies are investigating the adaptation of
NeRF to single-view settings [21, 32, 52, 79]; although,
reconstructing arbitrary objects from single-view images is
still a challenging problem.
3. Method
3.1. Denoising Diffusion Probabilistic Models
Denoising diffusion probabilistic models (DDPM) [19, 54]
are a class of generative models that model the real data dis-
tribution q(x0)with a tractable model distribution pθ(x0)
by learning to iteratively denoise samples. It learns a proba-
bility model pθ(x0) =R
pθ(x0:T)dx1:Tto convert unstruc-
tured noise xTto real samples x0in the form of a Markov
chain, with Gaussian transitions. The Gaussian transition is
9871
...
...Stage 3
Noise Interpolation
Module
Denoising U-NetNoise Interpolation
Module
Denoising U-NetNoise Interpolation
ModuleDenoising U-Net
2
1
2
1
2
1
...Stage 1
Denoising U-Net
Denoising U-Net Denoising U-Net
...Stage 2
Noise Interpolation
Module
Denoising U-NetNoise Interpolation
Module
Denoising U-NetNoise Interpolation
ModuleDenoising U-Net
1
1
1
Figure 2. Illustration of the Auto-Regressive Generation Process. In our approach, we extend a pre-trained diffusion model from single-
stage to multi-stage generation and we maintain a view set that contains all generated views. For each stage, we construct Nreverse
diffusion processes and sharing a common starting noise. At each time step within this generation stage, the diffusion model predicts N
noises individually. These Nnoises are then subjected to weighted interpolation through the Noise Interpolation Module , concluding the
denoising step with the a shared interpolated noise for subsequent denoising steps.
defined as:
q(xT|x0) =TY
t=1q(xt|xt−1) =TY
t=1N(xt;p
1−βtxt−1, βtI),
(1)
where βt, t∈ {1, ..., T}are the variance schedule parame-
ter and timestep in the denoising process respectively. The
reverse denoising process starts from a noise sampled from
a Gaussian distribution q(xT) =N(0,I)and is constructed
as:
pθ(x0|xT) =TY
t=1pθ(xt−1|xt) =TY
t=1N(xt−1;µθ(xt, t), σ2
tI),
(2)
where the variance σ2
tis a time-dependent constant [19],
andµθ(xt, t)is the mean from the learned noise predictor
ϵθ:
µθ(xt, t) =1√αt
xt−βt√1−¯αtϵθ(xt, t)
.(3)
Here, αtand¯αtare constants derived from βt. The objec-
tive of noise predictor ϵθis simplified to:
ℓ=Et,x0,ϵ
∥ϵ−ϵθ(√¯αtx0+√
1−¯αtϵ, t)∥2
,(4)
where ϵis a random variable sampled from N(0,I)[19].
3.2. Pose-Conditional Diffusion Models
Similar to other generative models [42, 55], diffusion mod-
els inherently possess the capability to model conditionaldistributions of the form pθ(xt−1|xt, y)where yis the con-
dition. We employ a conditional denoising autoencoder, de-
noted as ϵθ(xt, t, y)which enables controlling the synthe-
sis process through a variety of input modalities, including
textual descriptions [50], semantic maps [20, 46], or other
image-to-image translation tasks [20]. In the following, we
present a range of approaches to novel-view synthesis, ex-
ploring how various works, including our own, approach
the concept of a single reverse diffusion step. Through this
comparison, we clarify and establish the underlying rela-
tionships between these different methodologies. The no-
tation will follow that bottom subscript (·)tindicates the
diffusion step and upper subscript (·)irelates to the view
index. Subsequently, the i-th condition image and its rela-
tive pose to the target view are defined as yiandπi, respec-
tively, and the noisy image to be denoised at timestep tis
defined as xt.
Direct condition was applied by Zero 1-to-3 [34] to the
reverse process when given a single input image and target
posey1, π1:
p(xt−1|xt,y1, π1). (5)
Stochastic conditioning was formulated by [69] which
can leverage multiple views sampled from a collection of
views py,π(Y, π):
p(xt−1|xt,yi, πi),{yi, πi} ∼py,π(Y, π), (6)
9872
where the sampling of image and pose happens at each
diffusion step t.
Joint output distribution was shown in
SyncDreamer[36] which learns a joint distribution of
many views given an image condition y1:
p(x1:N
t−1|x1:N
t,y1, e1), (7)
where Nis the number of generated novel views and e
is the elevation condition (partial pose information). We
note that in this formulation the target poses are not fully
specified as part of the condition allowing for diverse pose
generation of outputs.
Auto-regressive distribution is an auto-regressive distri-
bution setting which can generate an arbitrary number of
views given a single or multiple condition images and poses
contained in the set of y1:N−1, π1:N−1:
p(xN
t−1|xN
t,y1:N−1, π1:N−1). (8)
Our approach falls in the auto-regressive category and for
the remainder of this section we detail the implementation
to achieve this sampling strategy.
3.3. Interpolated Denoising
The standard DDPM model has been adapted for novel-
view image synthesis by using an image and target pose
(i.e., rotation and translation offsets) as conditional in-
puts [69]. Following training on a large-scale dataset, this
approach has demonstrated the capability for zero-shot re-
construction [34]. To address the challenge of maintaining
multi-view consistency, we employ an auto-regressive ap-
proach for generating sequential frames (See Fig. 2). In-
stead of independently producing each frame from just the
input images – a process prone to significant variations be-
tween adjacent images – we integrate an auto-regressive
algorithm into the diffusion process. This integration en-
ables us to model a conditional joint distribution, ensuring
smoother and more consistent transitions between frames.
To guide the synthesis of novel views using images un-
der different views, we design an interpolated denoising
process. For the purpose of this derivation, we assume
access to an image set containing N−1images denoted
as{y1, ...,yN−1}. We want to model the distribution of
theN-th view image conditioned on these N−1views
q(xN
1:T|y1:N−1), where the relative pose offsets πi, i∈
{1, N−1}between the condition images {y1, ...,yN−1}
and target image xN
0are omitted for simplicity. The for-
ward process of the multi-view conditioned diffusion modelis a direct extension of the vanilla DDPM in Eq. 1, where
noises are added to every view independently by
q(xN
1:T|y1:N) =TY
t=1q(xN
t|xN
t−1,y1:N) (9)
where q(xN
t|xN
t−1,y1:N) = N(xN
t;√1−βtxN
t−1, βtI).
The initial is defined as xN
0:=yN. Similarly, following
Eq. 2, the logreverse process is constructed as
logpθ(xN
0|xN
T,y1:N−1) =TX
t=1logpθ(xN
t−1|xN
t,y1:N−1)
≈
(1)TX
t=1logN−1Y
n=1pθ(xN
t−1|xN
t,yn)
=TX
t=1N−1X
n=1logN(xN
t−1;µn
θ(xN
t,yn, t), σ2
tI)
=TX
t=1N 
xN
t−1;¯µθ(xN
t,y1:N−1, t),¯σt2I
.
(10)
Where ¯µθ,¯σt2are taken as the mean and variance of the
summation of N−1log-normal distributions. A note on
subscript (1)in Eq.10; to avoid cluttering the derivation, we
assume N−1independent inferences of the same random
variable xN
t−1using a different ynthat results in N−1
independent normal distributions, which would require an
additional subscript that we omitted for clarity.
3.4. Single and Multi-view Denoising
In practice , however, we may not have all N−1views but
a single view or a handful of views. For the reminder of
this section, we treat an estimated view as xn
0, to be the n-
th view ynafter a full reverse diffusion process. We use
¯µθ(xt,y1:N−1, t)as the weighted average of µn
θ(xt,yn, t).
For computing ¯µθusing both given views and estimated
views we adopt an approach where different views con-
tribute differently to the target view, and we assign the
weight ωnfor the n-th view in practice while satisfying the
constraintPN−1
n=1wn= 1. The Noise Interpolation Module
in Fig. 2 is modeled as:
¯µθ(xt,y1:N−1,t) =N−1X
n=1ωnµn
θ(xt,yn, t)
=N−1X
n=1ωn1√αt
xt−βt√1−¯αtϵθ(xt,yn, t)
=1√αt 
xt−βt√1−¯αtNX
n=1ωnϵθ(xt,yn, t)!
.
(11)
In our approach, as the full view set is not given to us,
we approximate this process by an auto-regressive way and
9873
grow the condition set during the generation. We define the
weight parameter ωnbased on the angle offset, i.e., azimuth
(∆n
a), elevation ( ∆n
e), and distance ( ∆n
d), between the target
view and the n−thcondition view. The core idea is to as-
sign higher importance to near-view images during the de-
noising process while ensuring that the weight for the initial
condition image does not diminish too rapidly, even when
the target view is positioned at a nearly opposite angle. We
use an exponential decay weight function for the initial con-
dition image, defined as ωn=e−∆n
τc. Here, τcis the tem-
perature parameter that regulates the decay speed, and ∆n
is the sum of the absolute relative azimuth ( ∆n
a), elevation
(∆n
e), and distance ( ∆n
d) between the target and condition
poses. We calculate ∆nas∆n=|∆n
a|/π+|∆n
e|/π+|∆n
d|.
For the weights of the remaining images denoted as
{x2
0, ..., xN
0}, all generated from the initial condition image
y1:=x1
0, we use a softmax function to define the weights
ωn:
ωn=Softmax (e−∆n
τg
PN
n=2e−∆n
τg), n= 2, ..., N (12)
Similarly, ∆nrepresents the relative pose offset between
target view and the n-th generated view, and τgrepresents
the temperature parameter for generated views. As an ex-
ample, in the single-view case, the weights are expressed as
follows,
ωn=

exp(−∆n
τc), n = 1
(1−ω1)Softmax (e−∆n
τg
PN
n=2e−∆n
τg), n̸= 1(13)
we apply the term 1−ω1on the generated image weights
to ensure the requirement ofPN−1
n=1wn= 1 will be met.
In practice, Eq.13 is generalised to allow the condition set
can be larger than 1,i.e., multi-view generation (see supple-
mentary).
3.5. Step-by-step Generation
Single image generation. When applying the auto-
regressive approach to image generation, we have devised
a generation trajectory, as illustrated in Fig. 3a. We uni-
formly sample views along this trajectory in sequence. Each
previously generated view image on this trajectory is incor-
porated into the condition set, providing guidance for the
subsequent denoising process via our interpolated denois-
ing method. To determine the number of steps, denoted as
S, needed for this trajectory, we use the following formula:
S=max
⌈∆N
a
δ⌉,⌈∆N
e
δ⌉
. (14)
!!!"!#$"!#!%!&"#!(a) Single image generation.
"#!!'!!!"!&!#!#$"!#$&!(…… (b) Spin video generation.
Figure 3. Illustration of Step-by-step Generation. (a) we uniformly
sample views along this trajectory in sequence to generate a novel-
view image; (b) we sample views from nearest to furthest views
according to to view distance to generate a 360◦spin video.
Here, we set the maximum offset per step δto determine
the step count S, also based on the target view offsets ∆N
a
and∆N
e. We then proceed to sample the n-th view using
the following equation:
(∆n
a,∆n
e,∆n
d) = (∆N
a
S∗n,∆N
e
S∗n,∆N
d
S∗n) (15)
Spin videos generation. In contrast to generating a single
target image, the process of spin video generation begins
from an initial image and concludes at the same position.
To achieve this, we need to modify the generation order to
leverage the broad range of rotation images, rather than sim-
ply following the rotation degree range of [0◦,360◦]in se-
quence. This is because, at ∆a=π, the view is opposite
to the conditioning view, marking the end of the generation
process. To establish the generation order for spin video
generation, we introduce the minimum azimuth offset, de-
noted as δ, and employ a skip trajectory with the following
order: {δ,−δ,2δ,−2δ..., Nδ }, shown in Fig. 3b. For sim-
plicity, we only consider rotation along the azimuth dimen-
sion in this context.
4. Experiments
Datasets. We evaluate our method and compare to base-
lines on the ABO [6] and GSO [9] datasets. These datasets
are out-of-the-distribution as all baselines are trained on the
Objaverse [7]. We also provide qualitative results on real
images to showcase performance of our method on in-the-
wild images in the supplementary. For additional results,
please refer to the videos contained in the supplementary.
Metrics. We assess our novel-view synthesis on three
main criteria:
1.Image Quality : LPIPS [85], PSNR, and SSIM [67] met-
rics to help gauge the similarity between synthesized and
ground-truth views.
9874
Dataset MethodFree Renderings SyncDreamer Renderings
SSIM↑PSNR↑LPIPS↓SSIM↑PSNR↑LPIPS↓
ABOZero123 [34] 0.8796 21.33 0.0961 0.7822 18.27 0.1999
SyncDre. [36] 0.7712 13.43 0.2182 0.8031 19.07 0.1816
Ours 0.8848 21.43 0.0923 0.7983 18.75 0.1985
GSOZero123 [34] 0.8710 20.33 0.1029 0.7925 18.06 0.1714
SyncDre. [36] 0.8023 14.42 0.1833 0.8024 18.20 0.1647
Ours 0.8820 20.73 0.0958 0.8076 18.40 0.1703
Table 1. Quantitative results on ABO and GSO datasets with ar-
bitrary (left) and discrete (right) rotation and translation. Free
renderings are a set of arbitrary rotation and translation as target
generation view, while SyncDreamer renderings are a fixed set
of 16 views with discrete azimuth, fixed elevation and distance,
i.e., azimuth ∈ {0◦,22.5◦,45◦, ...,315◦,337.5◦}, elevation= 30◦.
Note that SyncDreamer [36] cannot generate images under arbi-
trary views apart from the predefined 16 camera positions.
2.Multi-View Consistency : Using SIFT [39], LPIPS [85]
and CLIP [48], we measure the uniformity of images
across various perspectives.
3.3D Reconstruction : Chamfer distances and F-score be-
tween ground-truth and reconstructed shapes determine
geometrical consistency.
4.1. Novel-view synthesis
In Tab. 1, we show quantitative results for novel-view syn-
thesis under arbitrary and fixed-view settings. The fixed-
view setting uses the rendering set of [36] and ensures a
fair comparison to [36] which is limited to this fixed-view
generation setting. As shown in Tab. 1, our method can pro-
duce comparable results to [36], without any fine-tuning on
the rendering set of [36]. Under the arbitrary-view setting,
we sample the nearest view from the rendering set of [36]
to the designated target view for generation; [36] under-
performs both [34] and our approach. Overall, regardless
of the evaluation setting, our method consistently outper-
forms [34] and even outperforms [36] on the GSO dataset
under the favourable fixed-view setting for [36].
The task of novel view synthesis serves as a precur-
sor for enabling more significant downstream applications,
such as 3D reconstruction, by generating requisite input
views. Thus, in addition to image quality, thorough evalua-
tion needs to encompass the multi-view consistency of the
synthesized perspectives. This additional criterion ensures
that the generated imagery not only appears visually com-
pelling but also aligns geometrically across different view-
points. In the following sections, we evaluate against base-
lines on 3D consistency and show that the significant im-
provements our approach offers.
Multi-view Consistency. For measuring multi-view con-
sistency of synthesized views, quantitative results are shown
in Tab. 2, while qualitative comparisons are shown in Figs. 4
and 5. Here, we can see when compared to the baselines
of [34] and [36], our approach excels in generating imagesDataset Method View SIFT ↑LPIPS↓CLIP↑
ABOZero123 [34]
1613.38 0.1782 0.9604
SyncDreamer [36] 12.36 0.1895 0.9584
Ours 13.51 0.1602 0.9664
Zero123 [34]3617.03 0.1231 0.9725
Ours 18.01 0.0966 0.9812
GSOZero123 [34]
1612.58 0.1411 0.9482
SyncDreamer [36] 13.24 0.1315 0.9532
Ours 13.83 0.1187 0.9601
Zero123 [34]3615.20 0.1056 0.9592
Ours 17.95 0.0676 0.9773
Table 2. Quantitative results for multi-view consistency. We re-
port the SIFT matching point number, LPIPS and CLIP similarity
between adjacent frames to evaluate the multi-view consistency.
Note that SyncDreamer can only generate 16 view images for a
spin video due to the constraints imposed by its training.
Dataset Method SSIM ↑PSNR↑LPIPS↓
ABOZero123 [34] (1 view) 0.8820 21.51 0.0945
Ours (1 view) 0.8870 21.61 0.0904
Ours (2 views) 0.8913 21.92 0.0887
Ours (3 views) 0.8995 22.74 0.0815
GSOZero123 [34] (1 view) 0.8721 20.42 0.1017
Ours (1 view) 0.8830 20.87 0.0948
Ours (2 views) 0.8901 21.25 0.0893
Ours (3 views) 0.8979 21.95 0.0792
Table 3. Quantitative results for multi-conditioned generation.
Our approach outperforms the Zero-1-to-3 baseline [34] by ex-
tending its single-view reconstruction framework to a multi-view
reconstruction framework. Note that 3 condition views are re-
moved from test set, and thus the results are slightly different
to Tab. 1.
that are both semantically consistent with the input image
and maintains multi-view consistency in terms of colors and
geometry under arbitrary-view settings.
4.2. Multi-view conditional setting for NVS.
Additionally, our approach allows for the extension of
single-view conditioned models into multi-view condi-
tioned models which can take multiple conditional images
as input. The results presented in Tab. 3 showcase the ad-
vantages our method offers, as novel-view synthesis quality
improves with an increasing number of conditional input
views. This is a significant improvement over the Zero-1-
to-3 baseline and demonstrates the efficacy of our proposed
method in the multi-view setting.
4.3. 3D Reconstruction
For shape reconstruction, we present our results both quan-
titatively (Tab. 4) and qualitatively (Fig. 6). We compare
against baselines which synthesize novel views as well as
direct image-to-shape approaches [23, 44]. For the for-
mer approach, instead of using distillation [63], we train
NeuS [65] on synthesized images to recover a shape.
9875
Figure 4. Qualitative results for 360◦Spin Video Generation. Note the additional consistency in generated views our approach offers over
the competing baselines shown in the bounding boxes.
Figure 5. Qualitative comparison for Motion Smoothness. We
visualize the output videos using space-time Y-t slices through
frames of the generated spin video (along the scanline shown in
the condition).
Single-view Reconstruction. We first compare our pro-
posed method with several baselines under the single-
view reconstruction setting, including Point-E [44] and
Shap-E [23], Zero-1-to-3 [34], One-2-3-45 [33] and Sync-
Dreamer [36]. Qualitative results are shown in Fig. 6; Point-
E and Shap-E tend to generate incomplete shapes due to the
single-view setting. Multi-view alignment methods such as
Syncdreamer [36] and One-2-3-45 [33] capture the general
geometry but tend to lose fine details. In comparison, our
proposed method achieves the highest reconstruction qual-
ity amongst all approaches, where we can generate smooth
surfaces and capture detailed geometry with precision.
Multi-view Reconstruction Given our approach can also
synthesis novel views under the multi-view setting, we also
show results where we use 3 conditional input views to
synthesize 36 views for shape reconstruction. Compared
with existing multi-view reconstruction frameworks such asCond.
viewsMethodGen.
viewsABO GSO
CD↓ F-score ↑ CD↓ F-score ↑
1Point-E [44] N/A 0.0428 0.7144 0.0672 0.6340
Shap-E [23] N/A 0.0466 0.7364 0.0384 0.7313
One-2-3-45 [33] 32 0.0419 0.6665 0.0408 0.6490
SyncDreamer [36] 16 0.0160 0.8187 0.0229 0.7767
Zero123 [34] 16 0.0147 0.8226 0.0206 0.8045
Zero123 [34] 36 0.0139 0.8247 0.0207 0.8078
Ours 16 0.0133 0.8423 0.0177 0.8274
Ours 36 0.0126 0.8472 0.0164 0.8436
3MonoSDF [81] N/A 0.1020 0.3963 0.0830 0.4581
Ours 36 0.0115 0.8587 0.0124 0.8628
Table 4. Quantitative results on reconstructing 3D Shapes using
the generated images.
Dataset Method SSIM ↑ PSNR↑LPIPS↓
ABOZero123 (no interp.) 0.8796 21.33 0.0961
Standard auto-regression 0.8010 16.77 0.1854
Interpolated conditions 0.7243 13.26 0.3770
Interpolated outputs 0.8925 *21.95 * 0.1246
Stochastic conditioning [69] 0.8699 20.64 0.1106
Interpolated denoising 0.8848 21.43 0.0923
GSOZero123 (no interp.) 0.8710 20.33 0.1029
Standard auto-regression 0.8094 16.30 0.1801
Interpolated conditions 0.7661 14.38 0.3427
Interpolated outputs 0.8799 20.44 0.1659
Stochastic conditioning [69] 0.8658 19.97 0.1119
Interpolated denoising 0.8820 20.73 0.0958
Table 5. Quantitative comparison between our method to different
auto-regressive variants. Note that the Interpolated outputs variant
achieves the highest SSIM and PSNR values on the ABO dataset,
but generates blurred images.
MonoSDF [81], our approach capture more details and gen-
erates smoother surfaces (note that [81] also relies on addi-
tional depth and normal estimations for its reconstruction
whereas ours does not).
4.4. Ablations
Here, we study the effectiveness of our proposed in-
terpolated denoising process by exploring various auto-
regressive generation variants (in Tab. 5 and Fig. 7). Specif-
ically, we investigate four variants of denoising: stan-
9876
Figure 6. Qualitative results for single-view (1 condition image) and multi-view (3 condition images) 3D Reconstruction. Our method
offers the most consistent results across a variety of different reconstructed shapes.
Figure 7. Qualitative comparison for different auto-regressive gen-
erations.
dard auto-regression, interpolated conditions, interpolated
outputs, and stochastic conditioning which was proposed
in [69].
Standard Auto-regression. One initial approach to auto-
regression involves using the last generated view as the sub-
sequent conditioning. However, this method exhibits bad
generation quality, due to the accumulation of errors during
the sequential generation process. As each subsequent view
relies on the accuracy of the previous one, any inaccuracies
or imperfections in earlier stages can compound, leading to
a degradation in overall image quality. This limitation high-
lights the need for more sophisticated auto-regressive strate-
gies to address the issue of error propagation and enhance
the quality of generated views.
Interpolated Conditions and Interpolated Outputs. In-
terpolated Conditions and Interpolated Outputs are two
straightforward approaches to introduce auto-regressive
generation into an existing diffusion model. The for-
mer method involves interpolating feature embeddings from
condition images and poses, while the latter interpolates the
final image feature maps produced by the model. DespiteSSIM and PSNR metrics showing favorable results for In-
terpolated Outputs over others in Tab. 5, as illustrated in
visual comparisons in Fig. 7 shows that it leads to blurring
of the output views and this is corroborated by larger LPIPS
distance.
Stochastic Conditioning. We also explore the application
of the Stochastic Conditioning Sampler proposed by [69] to
the Zero-1-to-3 model. We observe that Stochastic Condi-
tioning fails to deliver the desired auto-regressive genera-
tion results; this may be attributed to the specific category
on which the diffusion model used by Watson et al. [69] was
trained, allowing it to handle plausible condition images
more effectively. By contrast, Zero-1-to-3 [34] was trained
on a cross-category dataset and designed for zero-shot re-
construction. Additionally, our evaluation data contains
out-of-the-distribution data ( i.e., ABO [6] and GSO [9]).
5. Conclusion
In this work, we have developed ViewFusion, a novel
algorithm that addresses the challenge of multi-view
consistency in novel-view synthesis with diffusion mod-
els. Our approach circumvents the need for fine-tuning
or additional modules by integrating an auto-regressive
mechanism that incrementally refines view synthesis,
utilizing the entire history of previously generated views.
Our proposed diffusion interpolation technique extends
the denoising process in pre-trained diffusion models
from a single-view setting to a multi-view setting without
training requirements. Empirical evidence underscores
ViewFusion’s capability to produce consistently high-
quality views, and we achieve significant steps forward in
novel view synthesis and 3D reconstruction applications.
9877
References
[1] Titas Anciukevi ˇcius, Zexiang Xu, Matthew Fisher, Paul Hen-
derson, Hakan Bilen, Niloy J Mitra, and Paul Guerrero. Ren-
derdiffusion: Image diffusion for 3d reconstruction, inpaint-
ing and generation. In CVPR , 2023. 2
[2] Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W
Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini
De Mello, Tero Karras, and Gordon Wetzstein. Generative
novel view synthesis with 3d-aware diffusion models. In
ICCV , 2023. 2
[3] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen
Tu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf:
A unified approach to 3d generation and reconstruction. In
ICCV , 2023. 2
[4] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.
Fantasia3d: Disentangling geometry and appearance for
high-quality text-to-3d content creation. arXiv preprint
arXiv:2303.13873 , 2023. 1, 2
[5] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexan-
der G Schwing, and Liang-Yan Gui. Sdfusion: Multimodal
3d shape completion, reconstruction, and generation. In
CVPR , 2023. 2
[6] Jasmine Collins, Shubham Goel, Kenan Deng, Achlesh-
war Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang,
Tomas F Yago Vicente, Thomas Dideriksen, Himanshu
Arora, Matthieu Guillaumin, and Jitendra Malik. Abo:
Dataset and benchmarks for real-world 3d object understand-
ing. CVPR , 2022. 2, 5, 8
[7] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,
Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana
Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:
A universe of annotated 3d objects. In CVPR , 2023. 5
[8] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan,
Yin Zhou, Leonidas Guibas, Dragomir Anguelov, et al.
Nerdi: Single-view nerf synthesis with language-guided dif-
fusion as general image priors. In CVPR , 2023. 2
[9] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kin-
man, Ryan Hickman, Krista Reymann, Thomas B McHugh,
and Vincent Vanhoucke. Google scanned objects: A high-
quality dataset of 3d scanned household items. In ICRA ,
2022. 2, 5, 8
[10] Ziya Erkoc ¸, Fangchang Ma, Qi Shan, Matthias Nießner,
and Angela Dai. Hyperdiffusion: Generating implicit
neural fields with weight-space diffusion. arXiv preprint
arXiv:2303.17015 , 2023. 2
[11] George Fahim, Khalid Amin, and Sameh Zarif. Single-view
3d reconstruction: A survey of deep learning methods. Com-
puters & Graphics , 94:164–190, 2021. 2
[12] Kui Fu, Jiansheng Peng, Qiwen He, and Hanxiao Zhang.
Single image 3d object reconstruction based on deep learn-
ing: A review. Multimedia Tools and Applications , 80:463–
498, 2021. 2
[13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-
nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-
Or. An image is worth one word: Personalizing text-to-
image generation using textual inversion. arXiv preprint
arXiv:2208.01618 , 2022. 2[14] Georgia Gkioxari, Justin Johnson, and Jitendra Malik. Mesh
r-cnn. 2019 IEEE/CVF International Conference on Com-
puter Vision (ICCV) , 2019. 1, 2
[15] Thibault Groueix, Matthew Fisher, Vladimir G. Kim,
Bryan C. Russell, and Mathieu Aubry. A papier-mache ap-
proach to learning 3d surface generation. 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
2018. 1, 2
[16] Jiatao Gu, Qingzhe Gao, Shuangfei Zhai, Baoquan Chen,
Lingjie Liu, and Josh Susskind. Learning controllable 3d
diffusion models from single-view images. arXiv preprint
arXiv:2304.06700 , 2023. 2
[17] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M Susskind,
Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi.
Nerfdiff: Single-image view synthesis with nerf-guided dis-
tillation from 3d-aware diffusion. In ICML , 2023. 2
[18] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Bar-
las O ˘guz. 3dgen: Triplane latent diffusion for textured mesh
generation. arXiv preprint arXiv:2303.05371 , 2023. 2
[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In NeurIPS , 2020. 1, 2, 3
[20] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adver-
sarial networks. CVPR , 2017. 2, 3
[21] Wonbong Jang and Lourdes Agapito. Codenerf: Disentan-
gled neural radiance fields for object categories. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 12949–12958, 2021. 2
[22] Heewoo Jun and Alex Nichol. Shap-e: Generat-
ing conditional 3d implicit functions. arXiv preprint
arXiv:2305.02463 , 2023. 2
[23] Heewoo Jun and Alex Nichol. Shap-e: Generating condi-
tional 3d implicit functions, 2023. 6, 7
[24] Animesh Karnewar, Niloy J Mitra, Andrea Vedaldi, and
David Novotny. Holofusion: Towards photo-realistic 3d gen-
erative modeling. In ICCV , 2023. 2
[25] Animesh Karnewar, Andrea Vedaldi, David Novotny, and
Niloy J Mitra. Holodiffusion: Training a 3d diffusion model
using 2d images. In CVPR , 2023. 2
[26] Hiroharu Kato and Tatsuya Harada. Learning view priors for
single-view 3d reconstruction. In CVPR , 2019. 2
[27] Seung Wook Kim, Bradley Brown, Kangxue Yin, Karsten
Kreis, Katja Schwarz, Daiqing Li, Robin Rombach, Antonio
Torralba, and Sanja Fidler. Neuralfield-ldm: Scene gener-
ation with hierarchical latent diffusion models. In CVPR ,
2023. 2
[28] Jiabao Lei, Jiapeng Tang, and Kui Jia. Generative scene syn-
thesis via incremental view inpainting using rgbd diffusion
models. In CVPR , 2022. 2
[29] Xueting Li, Sifei Liu, Kihwan Kim, Shalini De Mello, Varun
Jampani, Ming-Hsuan Yang, and Jan Kautz. Self-supervised
single-view 3d reconstruction via semantic consistency. In
ECCV , 2020. 2
[30] Chen-Hsuan Lin, Chaoyang Wang, and Simon Lucey. Sdf-
srn: Learning signed distance 3d object reconstruction from
static images. In Advances in Neural Information Processing
Systems (NeurIPS) , 2020. 1, 2
9878
[31] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,
Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution
text-to-3d content creation. In CVPR , 2023. 1, 2
[32] Kai-En Lin, Lin Yen-Chen, Wei-Sheng Lai, Tsung-Yi Lin,
Yi-Chang Shih, and Ravi Ramamoorthi. Vision transformer
for nerf-based view synthesis from a single input image. In
WACV , 2023. 2
[33] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen,
Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45:
Any single image to 3d mesh in 45 seconds without per-
shape optimization, 2023. 1, 2, 7
[34] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-
makov, Sergey Zakharov, and Carl V ondrick. Zero-1-to-3:
Zero-shot one image to 3d object. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 9298–9309, 2023. 1, 2, 3, 4, 6, 7, 8
[35] Xinhang Liu, Shiu-hong Kao, Jiaben Chen, Yu-Wing Tai,
and Chi-Keung Tang. Deceptive-nerf: Enhancing nerf recon-
struction using pseudo-observations from diffusion models.
arXiv preprint arXiv:2305.15171 , 2023. 2
[36] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie
Liu, Taku Komura, and Wenping Wang. Syncdreamer:
Learning to generate multiview-consistent images from a
single-view image. arXiv preprint arXiv:2309.03453 , 2023.
1, 2, 4, 6, 7
[37] Zhen Liu, Yao Feng, Michael J Black, Derek
Nowrouzezahrai, Liam Paull, and Weiyang Liu. Meshdiffu-
sion: Score-based generative 3d mesh modeling. In ICLR ,
2023. 2
[38] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,
Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,
Marc Habermann, Christian Theobalt, and Wenping Wang.
Wonder3d: Single image to 3d using cross-domain diffusion,
2023. 1, 2
[39] D.G. Lowe. Object recognition from local scale-invariant
features. In Proceedings of the Seventh IEEE International
Conference on Computer Vision , pages 1150–1157 vol.2,
1999. 6
[40] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and
Andrea Vedaldi. Realfusion: 360deg reconstruction of any
object from a single image. In CVPR , 2023. 1, 2
[41] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. In ECCV , 2020. 1, 2
[42] Mehdi Mirza and Simon Osindero. Conditional generative
adversarial nets. CoRR , abs/1411.1784, 2014. 3
[43] Norman M ¨uller, Yawar Siddiqui, Lorenzo Porzi,
Samuel Rota Bulo, Peter Kontschieder, and Matthias
Nießner. Diffrf: Rendering-guided 3d radiance field
diffusion. In CVPR , 2023. 2
[44] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela
Mishkin, and Mark Chen. Point-e: A system for generat-
ing 3d point clouds from complex prompts. arXiv preprint
arXiv:2212.08751 , 2022. 6, 7
[45] Evangelos Ntavelis, Aliaksandr Siarohin, Kyle Olszewski,
Chaoyang Wang, Luc Van Gool, and Sergey Tulyakov.Autodecoding latent 3d diffusion models. arXiv preprint
arXiv:2307.05445 , 2023. 2
[46] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan
Zhu. Semantic image synthesis with spatially-adaptive nor-
malization. 2019 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , 2019. 2, 3
[47] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR ,
2023. 1, 2
[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML , 2021. 6
[49] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,
Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aber-
man, Michael Rubinstein, Jonathan Barron, et al. Dream-
booth3d: Subject-driven text-to-3d generation. arXiv
preprint arXiv:2303.13508 , 2023. 1, 2
[50] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee. Generative ad-
versarial text to image synthesis. In International conference
on machine learning , pages 1060–1069. PMLR, 2016. 2, 3
[51] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , 2022. 1
[52] Kyle Sargent, Jing Yu Koh, Han Zhang, Huiwen Chang,
Charles Herrmann, Pratul Srinivasan, Jiajun Wu, and Deqing
Sun. Vq3d: Learning a 3d-aware generative model on ima-
genet. arXiv preprint arXiv:2302.06833 , 2023. 2
[53] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,
and Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-
eration, 2023. 1
[54] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In ICML , 2015. 2
[55] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning
structured output representation using deep conditional gen-
erative models. In Neural Information Processing Systems ,
2015. 3
[56] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea
Vedaldi. Viewset diffusion:(0-) image-conditioned 3d gener-
ative models from 2d data. arXiv preprint arXiv:2306.07881 ,
2023. 2
[57] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,
Lizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d
creation from a single image with diffusion prior. In ICCV ,
2023. 1, 2
[58] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and
Yasutaka Furukawa. Mvdiffusion: Enabling holistic multi-
view image generation with correspondence-aware diffusion.
arXiv preprint arXiv:2307.01097 , 2023. 2
[59] Maxim Tatarchenko, Stephan R Richter, Ren ´e Ranftl,
Zhuwen Li, Vladlen Koltun, and Thomas Brox. What do
single-view 3d reconstruction networks learn? In CVPR ,
2019. 2
9879
[60] A. Tewari, O. Fried, J. Thies, V . Sitzmann, S. Lombardi,
K. Sunkavalli, R. Martin-Brualla, T. Simon, J. Saragih, M.
Nießner, R. Pandey, S. Fanello, G. Wetzstein, J.-Y . Zhu, C.
Theobalt, M. Agrawala, E. Shechtman, D. B Goldman, and
M. Zollh ¨ofer. State of the art on neural rendering. Computer
Graphics Forum , 39(2):701–727, 2020. 1
[61] Ayush Tewari, Tianwei Yin, George Cazenavette, Semon
Rezchikov, Joshua B Tenenbaum, Fr ´edo Durand, William T
Freeman, and Vincent Sitzmann. Diffusion with forward
models: Solving stochastic inverse problems without direct
supervision. arXiv preprint arXiv:2306.11719 , 2023. 2
[62] Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, Jia-
Bin Huang, and Johannes Kopf. Consistent view synthesis
with pose-guided diffusion models. In CVPR , 2023. 2
[63] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,
and Greg Shakhnarovich. Score jacobian chaining: Lifting
pretrained 2d diffusion models for 3d generation. In CVPR ,
2023. 1, 2, 6
[64] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei
Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh
models from single rgb images. In ECCV , 2018. 1, 2
[65] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. Neus: Learning neural implicit
surfaces by volume rendering for multi-view reconstruction.
InNeurIPS , 2021. 1, 6
[66] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin
Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang
Wen, Qifeng Chen, et al. Rodin: A generative model for
sculpting 3d digital avatars using diffusion. In CVPR , 2023.
2
[67] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli.
Image quality assessment: from error visibility to structural
similarity. IEEE Transactions on Image Processing , 13(4):
600–612, 2004. 5
[68] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan
Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and
diverse text-to-3d generation with variational score distilla-
tion. arXiv preprint arXiv:2305.16213 , 2023. 1, 2
[69] Daniel Watson, William Chan, Ricardo Martin-Brualla,
Jonathan Ho, Andrea Tagliasacchi, and Mohammad
Norouzi. Novel view synthesis with diffusion models. arXiv
preprint arXiv:2210.04628 , 2022. 1, 2, 3, 4, 7, 8
[70] Haohan Weng, Tianyu Yang, Jianan Wang, Yu Li, Tong
Zhang, C. L. Philip Chen, and Lei Zhang. Consistent123:
Improve consistency for one image to 3d object synthesis,
2023. 2
[71] Jianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin
Tong. 3d-aware image generation using 2d diffusion mod-
els.arXiv preprint arXiv:2303.17905 , 2023. 2
[72] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany,
Shiqin Yan, Numair Khan, Federico Tombari, James Tomp-
kin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in
visual computing and beyond. In Computer Graphics Forum ,
2022. 1
[73] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,
and Zhangyang Wang. Neurallift-360: Lifting an in-the-wild
2d photo to a 3d object with 360 views. arXiv e-prints , pages
arXiv–2211, 2022. 1, 2[74] Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hong-
dong Li. Consistnet: Enforcing 3d consistency for multi-
view images diffusion, 2023. 1, 2
[75] Xianghui Yang, Guosheng Lin, and Luping Zhou. Single-
view 3d mesh reconstruction for seen and unseen categories.
IEEE Transactions on Image Processing , pages 1–1, 2023.
1, 2
[76] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan.
Mvsnet: Depth inference for unstructured multi-view stereo.
InECCV , 2018. 1
[77] Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, and Heng
Wang. Consistent-1-to-3: Consistent image to 3d view syn-
thesis via geometry-aware diffusion models, 2023. 1, 2
[78] Paul Yoo, Jiaxian Guo, Yutaka Matsuo, and Shixiang Shane
Gu. Dreamsparse: Escaping from plato’s cave with 2d frozen
diffusion model given sparse views. CoRR , 2023. 2
[79] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.
pixelnerf: Neural radiance fields from one or few images.
2021 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , 2021. 2
[80] Jason J. Yu, Fereshteh Forghani, Konstantinos G. Derpanis,
and Marcus A. Brubaker. Long-term photometric consistent
novel view synthesis with diffusion models. In ICCV , 2023.
2
[81] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sat-
tler, and Andreas Geiger. Monosdf: Exploring monocu-
lar geometric cues for neural implicit surface reconstruc-
tion. Advances in Neural Information Processing Systems
(NeurIPS) , 2022. 7
[82] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic,
Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent
point diffusion models for 3d shape generation. In NeurIPS ,
2022. 2
[83] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter
Wonka. 3dshape2vecset: A 3d shape representation for neu-
ral fields and generative diffusion models. In SIGGRAPH ,
2023. 2
[84] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models, 2023.
2
[85] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR , 2018. 5, 6
[86] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Dis-
tilling view-conditioned diffusion for 3d reconstruction. In
CVPR , 2023. 2
9880
