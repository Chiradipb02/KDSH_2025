From Activation to Initialization: Scaling Insights for Optimizing Neural Fields
Hemanth Saratchandran∗
Australian Institute of Machine Learning,
University of Adelaide, AustraliaSameera Ramasinghe
Amazon, Australia
Simon Lucey
Australian Institute of Machine Learning,
University of Adelaide, Australia
Abstract
In the realm of computer vision, Neural Fields have
gained prominence as a contemporary tool harnessing neu-
ral networks for signal representation. Despite the remark-
able progress in adapting these networks to solve a vari-
ety of problems, the field still lacks a comprehensive the-
oretical framework. This article aims to address this gap
by delving into the intricate interplay between initialization
and activation, providing a foundational basis for the ro-
bust optimization of Neural Fields. Our theoretical insights
reveal a deep-seated connection among network initializa-
tion, architectural choices, and the optimization process,
emphasizing the need for a holistic approach when design-
ing cutting-edge Neural Fields.
1. Introduction
Neural Fields have emerged as a compelling paradigm
leveraging coordinate-based neural networks to achieve
a concise and expressive encoding of intricate geometric
structures and visual phenomena [5, 6,17,19,28,33]. De-
spite the notable advancements in the application of neural
fields across various domains [7, 24,31,32], the prevalent
approach to understanding and designing these networks re-
mains primarily empirical. Additionally, in the era of big
data, practitioners often follow the trend of scaling networks
ad hoc with larger datasets, lacking a clear understanding of
how such architectures should proportionately adapt to data
size.
In this paper, we explore the scaling dynamics of neu-
ral fields in relation to data size. Specifically, when given a
neural field and a dataset, we inquire about the number of
parameters necessary for the neural architecture to facilitate
*hemanth.saratchandran@adelaide.edu.augradient descent convergence to a global minimum. Our
theoretical findings imply that the answer to this question
depends on the chosen activation and initialization scheme.
For shallow networks employing a sine [30], sinc [24],
Gaussian [23], or wavelet activation [26] and initialized
with standard schemes such as LeCun [16], Kaiming [13],
or Xavier [11], the network parameters must scale super-
linearly with the number of training samples for gradient de-
scent to converge effectively. In the case of deep networks
with the same activations and initializations, we prove that
the network parameters need to scale super-quadratically.
This contrasts with the work [20], demonstrating that net-
works employing a ReLU activation, with or without a posi-
tional embedding layer [33], scale quadratically in the shal-
low setting and cubically in the deep setting. Other studies
have explored analogous scaling laws [1–4, 21], yet in each
instance, the scaling demands were cubic or more and per-
tained to initializations not commonly employed by prac-
titioners in the field. While Nguyen’s work [20] was pre-
viously considered state-of-the-art, our theoretical insights
challenge this notion, demonstrating that significantly fewer
parameters are required when employing a different activa-
tion function. For further comparison of our results with the
literature we refer the reader to the related work sec. 2.
Theoretical results often find themselves in regimes that
may not align with practical applications, rendering the the-
ory insightful but lacking in predictiveness. To underscore
the predictive efficacy of our theoretical framework, we de-
sign a novel initialization scheme and demonstrate its supe-
rior optimality compared to standard practices in the liter-
ature. Our findings reveal that neural fields, equipped with
the activations sine, sinc, Gaussian, or wavelet and initial-
ized using our proposed scheme, require a linear scaling
with data in the shallow case and a quadratic scaling in the
deep case, for gradient descent to converge to a global op-
timum. When compared with standard practical initializa-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
413
Figure 1. We evaluate Gaussian-activated networks comprising
four hidden layers, initialized using four different methods, and
trained with full-batch gradient descent. The comparison is per-
formed on an image reconstruction task, with the final train PSNRs
displayed in parentheses in the legend.
tions such as LeCun [16], Xavier [11], and Kaiming [13],
our initialization proves to be significantly more parameter-
efficient. We turn the readers attention to fig. 1where
we compared our initialization with Kaiming normal [13],
Xavier normal [11] and Lecun Normal [16] on an image
reconstruction task. As predicted by our theory, our initial-
ization shows superior performance.
We extensively test our initialization scheme across var-
ious neural field applications including image regression,
super resolution, shape reconstruction, tomographic recon-
struction, novel view synthesis and physical modeling. Our
contributions include:
1. The first theoretical proof of scaling laws for neural fields
with the activations sine, sinc, Gaussian, and wavelets,
ensuring effective convergence with gradient descent.
The proof demonstrates that networks employing these
activations surpass state-of-the-art outcomes in terms of
parameter efficiency.
2. The development of a superior initialization scheme com-
pared to standard approaches in the literature.
3. Empirical validation of our theoretical predictions on var-
ious neural field applications
2. Related Work
Several works have considered the effect of overparameter-
ization on gradient descent convergence. The work [10]
considered convergence of gradient decent for smooth ac-
tivations using the Neural Tangent Kernel (NTK) parame-
terization [15] and showed that if all hidden layers satis-
fied the growth Ω(N4), then gradient decent converges to
a global minimum. In [14], using the Neural Tangent Hi-
erarchy it was shown that for a smooth activation, Ω(N3)
suffices for all the hidden layers to guarantee convergence
of gradient decent to a global minimum. Both these pa-pers used the standard normal distribution as initialization
N(0,1), which is rarely used in practise especially in neu-
ral fields applications. There have been several works that
have studied the convergence of gradient decent for ReLU
activated neural networks, [3, 37] prove convergence for
gradient decent in the setting where the input and output
layers are fixed, while the inner layers are only trained.
Their result requires polynomial overparameterization for a
large degree polynomial. For two layer ReLU networks, the
works [4, 10,21] study the convergence of gradient decent
essentially showing that the width scaling must be at the or-
der of Ω(N4). For deep ReLU networks, the state of the art
was proved in [20] showing that one could take one hidden
layer of order Ω(N3). In each case our results show that
much less overparamterization is needed and our analysis is
carried out with initializations used by practitioners.
3. Notation
We consider a depth Lneural network with layer widths
{n1, . . . , n L}. We let X∈RN×n0denote the training data,
withn0being the dimension of the input and Nbeing the
number of training samples. We let Y∈RN×nLdenote the
training labels. The output at layer kwill be denoted by Fk
and is defined by
Fk=

FL−1WL+bL, k =L
ϕ(Fk−1Wk+bk), k∈[L−1]
X, k= 0(3.1)
where the weights Wk∈Rnk−1×nkand the biases bk∈
Rnkandϕis an activation applied component wise. The
notation [m]is defined by [m] = {1, . . . , m }. For a weight
matrix Wkat layer k, the notation W0
kwill denote the
initialization of that weight matrix. These are the initial
weights of the network before training begins under a gra-
dient descent algorithm. The networks we use in the paper
will all be trained using the MSE loss function, which we
denote by L. Our theoretical results will be primarily for
the case where ϕis given by one of the activation sine, sinc,
Gaussian or wavelet. We remind the reader that the sinc
function is defined by: sinc(x) =sin(x)
xforx̸= 0 and
sinc(0) = 1. For more details on how these activations are
used within the context of neural fields we refer the reader
to [23, 24, 26, 29].
We will always assume the data set Xconsists of i.i.d
sub-gaussian vectors and for the theoretical proofs will as-
sume they are normalized to have norm ||Xi||2= 1. More
details on data assumptions can be found in sec. 1 of the
supp. material.
Our networks will be free to have a positional embedding
layer (PE), which is simply an embedding of the data into
a higher dimensional space. The reader who is unfamiliar
with PE should consult the standard references [19, 33].
414
A neural field is any such network that parameterizes a
continuous field. Examples of neural fields can be found in
sec. 5. All networks will be trained with the standard Mean
Square Error (MSE) loss.
We will use standard complexity notations, Ω(·),O(·),
Θ(·) throughout the paper. The reader who is unfamiliar
with this notation can consult sec. 1 of the supp. material.
Finally, we will use the notation ”w.h.p.” to denote with high
probability.
4. Theoretical Scaling Laws
In this section, we provide a theoretical understanding of
how much overparameterization a neural field needs in or-
der for gradient descent to converge to a global minimum
for a given data set. Furthermore, we prove a scaling law
that details how the network must scale as the dataset size
grows, in order to facilitate optimum convergence of gra-
dient descent. We will work with the activations sine, sinc
and Gaussian. For wavelets see sec. 1 of the supp material.
To accommodate space constraints and offer readers a more
expansive perspective, we have deferred the exhaustive de-
tails of the proofs to sec. 1 of the supp. material.
We begin with a definition of overparameterization that
we will use throughout the paper. Our definition is consis-
tent with several works in the literature [2, 8,20,37].
Definition 4.1. Given a data set XwithNsamples and a
neural network F(θ). We say the network F(θ)is overpa-
rameterized with respect to the size of Xif the number of
parameters θis greater than N.
In general, it is known via several works in the litera-
ture that under various assumptions, overparameterization
is necessary for gradient descent to converge to a global
minimum [1, 3,4,10,20,21]
4.1. A scaling law for shallow networks
In this section, we present our theorem, delineating a com-
plexity bound that dictates the extent of overparameteriza-
tion necessary for a neural field to enable gradient descent
convergence to a global minimum, particularly in the con-
text of shallow networks—those with only two layers.
Theorem 4.2. LetXbe a fixed data set with Nsamples.
LetFbe a shallow neural network of depth 2admitting one
of the following activation functions:
1.sin(ωx),
2.sinc(ωx ) =sin(ωx )
x,
3.e−x2/2ω2
where ω(1/ω2) is a fixed frequency hyperparameter. Let
the widths of the network satisfy
n1= Ω(N3/2)andn2= Θ( m) (4.1)where mis a fixed positive constant. Suppose the net-
work has been initialized according to LeCun’s initializa-
tion scheme
(W0
1)ij∼ N(0,1
n0)and(W0
2)ij∼ N(0,1
n1). (4.2)
Then for a small enough learning rate gradient descent con-
verges to a global minimum w.h.p. when trained on X.
Remark 4.3. Thm. 4.2has been expressed in the context
of LeCun’s initialization. However, a similar theorem can
be proved for the two other standard initializations used in
the literature, namely Xavier normal initialization [11] and
Kaiming normal initialization [13].
4.2. A scaling law for deep networks
In this section we present a generalization of thm. 4.2to the
setting of deep networks.
Theorem 4.4. LetXbe a fixed data set of size N. LetFbe
a deep neural network of depth L,L >2, admitting one of
the following activation functions:
1.sin(ωx),
2.sinc(ωx ) =sin(ωx )
x,
3.e−x2/2ω2
where ω > 0(1/ω2) is a fixed frequency hyperparameter.
Let the widths of the network satisfy
nl= Θ( m),∀l∈[L]withl̸=L−1, (4.3)
nL−1= Ω(N5/2), (4.4)
where mis a fixed constant that is allowed to depend lin-
early on N. Suppose the network is initialized according to
LeCun’s initialization scheme
(W0
l)ij∼ N(0,1/nl−1), for l∈[L]. (4.5)
Then for a small enough learning rate gradient descent con-
verges to a global minimum w.h.p. when trained on X.
Remark 4.5. A similar theorem can be proved for Xavier
and Kaiming normal initializations. See sec. 1 of the supp.
material for learning rate bounds.
Remark 4.6. Thms. 4.2and4.4reveal a significant connec-
tion between activation functions and initialization methods
in neural network training. These theorems underscore the
importance of carefully choosing the architecture (via acti-
vation functions) and initialization for crafting parameter-
efficient networks that converge to optimal solutions.
4.3. Analyzing the proof methodology
In sec. 4.1and4.2, we explored the impact of initializa-
tion and activation choices on overparameterization in or-
der for gradient descent to converge to a global minimum.
415
This naturally leads to the question of whether there are
initialization schemes for neural fields utilizing the activa-
tionssin(ωx),sinc(ωx ), and e−x2/2ω2that necessitate less
overparameterization compared to the results established in
thms. 4.2 and 4.4.
In order to answer this question we start by giving a
concise sketch of the main idea of the proof methodology
of thm. 4.2. We let Fbe a fixed 2-layer network. Let
σ0=σmin(F0
1), denote the smallest singular value of the
hidden layer of Fand let W0
2∈Rn1×n2denote the initial
weight matrix of the second layer of F. The key step in
proving thm. 4.2is to establish the lower bound
σ2
0≥16√
N√n1p
2L(θ 0)||W0
2||2 (4.6)
when n1= Ω(⌈N3/2⌉). Once this is achieved a routine use
of convergence theory leads to the proof of thm. 4.2. The
interested reader can consult sec. 1 of supp. material for
full details.
To obtain (4.6), the proof proceeds by first establishing
the following two complexity bounds:
σ0≥Ω(√n1)andp
2L(θ 0) =O(√
N). (4.7)
We pause to mention that the two inequalities in (4.7) are
activation dependent and is precisely the place where we
need to use that the activation is one of sine, Gaussian, sinc,
or wavelet.
Substituting (4.7) into (4.6) shows us that proving in-
equality (4.6) boils down to showing
n1≥CNn3/4
1||W0
2||2, (4.8)
where C > 0is a constant (coming from the complexity
bounds in (4.7)) which we won’t worry about for this dis-
cussion. In order to establish inequality (4.8) we need to
understand the 2-norm of the random matrix W0
2. This is
precisely where LeCun’s initialization is used. By appeal-
ing to thm. 2.13 of [9], it can be shown that if a random
n1×n2-matrix Whas entries sampled from a Normal dis-
tribution of the form N(0,1/n1)then
||W||2=O(√n2/√n1). (4.9)
Applying (4.9) to the random weight matrix W0
2, initialized
using LeCun’s initialization (4.2), we see that (4.8) holds if
n1= Ω(N3/2).
Inequality (4.8) illuminates the intrinsic connection be-
tween initialization and overparameterization. The reduc-
tion of the product n3/4
1|W0
2|2serves as a theoretical in-
dicator for the diminished necessity of overparameteriza-
tion. Achieving a small value for n3/4
1|W0
2|2entails mini-
mizing the norm |W0
2|2, and leveraging (4.9) (refer to thm.
2.13 in [9]), we ascertain that the norm |W0
2|2possesses
a complexity bound of O(√n2/√n1). Consequently, fora smaller norm |W0
2|2, theoretical sampling from a Gaus-
sian distribution with an exceedingly small variance is war-
ranted. Suppose the entries of W0
2are sampled from
N(0,1/np
1). Employing (4.9) once more, we deduce that
|W0
2|2=O(√n2/np/2
1). Substituting this result back into
(4.8), we observe that a larger value of pcorresponds to a
reduced complexity requirement for n1. In essence, sam-
pling the final layers weight matrix from a Normal distri-
bution with smaller variance necessitates less overparame-
terization for gradient descent to converge to a global min-
imum. However, it’s crucial to note that excessively small
variances in the Gaussian distribution pose the challenge of
vanishing gradients in the network.
While the preceding discussion focused on shallow net-
works, a parallel argument can be extended to deep net-
works, illustrating that the level of overparameterization re-
quired for the last hidden layer to achieve convergence to a
global minimum is linked to the variance of the Normal dis-
tribution from which we initialize the final layer’s weights.
Moreover, a smaller variance corresponds to a reduced need
for overparameterization.
4.4. Designing new initializations
The discussion in the previous section suggested a new ap-
proach to initializing weights for a neural network. The
main point was that by controlling the variance of the Nor-
mal distribution the final layer weights are sampled from,
we can use less parameters and still converge to a global
minimum under gradient descent.
Fix a deep neural network FwithLlayers. We define
the following initialization.
Initialization 1:
(W0
l)ij∼ N(0,1/nl−1)forl∈[L−1]. (4.10)
(W0
L)ij∼ N(0,2/(n3/2
l−1)). (4.11)
As suggested by the discussion in sec. 4.3the variance of
the Gaussian we sample from for the last layer is smaller
by a factor of 1/√nL−1. The biases of the network will be
initialized to 0or a very small number such as 0.01 as is the
standard practice for many common normal initializations.
Fig. 2, gives a diagrammatic rendition of initialization 1.
Theorem 4.7. LetXbe a fixed dataset with Ntraining
samples. Let Fbe a shallow neural network of depth 2
admitting one of the following activation functions:
1.sin(ωx),
2.sinc(ωx ) =sin(ωx )
x,
3.e−x2/2ω2.
Let the widths of the network satisfy
n1= Ω(N )andn2= Θ( m) (4.12)
416
Figure 2. Diagram showing how to initialize weight matrices ac-
cording to Initialization 1. The final output layer is initialized
with a Normal distribution of smaller variance than the previous
layers by a factor of 1/√fan in, where fan indenotes the input
dimension to the layer.
where mis a fixed positive constant. Suppose the network
has been initialized according to initialization 1, see (4.10),
(4.11). Then for a small enough learning rate gradient de-
scent converges to a global minimum w.h.p.
Remark 4.8. Comparing thm. 4.7with thm. 4.2, we observe
that utilizing Initialization 1 requires only linear overparam-
eterization in the final hidden layer, as opposed to superlin-
ear overparameterization.
Theorem 4.9. LetXbe a fixed dataset with Ntraining
samples. Let Fbe a deep neural network of depth L,L >2,
admitting one of the following activation functions:
1.sin(ωx),
2.sinc(ωx ) =sin(ωx )
x,
3.e−x2/2ω2.
Let the widths of the network satisfy
nl= Θ( m),∀l∈[L]withl̸=L−1, (4.13)
nL−1= Ω(N2), (4.14)
where mis a fixed constant that is allowed to depend lin-
early on N. Suppose the network is initialized by initializa-
tion 1, see (4.10), (4.11). Then for a small enough learning
rate gradient descent converges to a global minimum w.h.p.
Remark 4.10. Comparing thm. 4.7with thm. 4.4, we ob-
serve that utilizing Initialization 1 requires quadratic over-
parameterization in the final hidden layer, as opposed to su-
per quadratic overparameterization, for gradient descent to
converge to a global minimum.Many practitioners in machine learning often use the
Uniform distribution to sample the weight matrices of a
neural network at initialization. Motivated by this we de-
fine a second uniform initialization scheme as follows.
Fix a deep neural field FwithLlayers We define the
following uniform initialization.
Initialization 2
(W0
l)ij∼ U(−1/√nl−1,1/√nl−1)forl̸=L. (4.15)
(W0
L)ij∼ U(−1/(n3/4
l−1),1/(n3/4
l−1)) (4.16)
where U(a, b) denotes the Uniform distribution on the in-
terval [a, b]. The biases can be initialized to be 0or a very
small number. In sec. 1 of the supp. material we give an-
other way to initialize the biases.
Remark 4.11. In [29], Sitzmann et al. presented a uniform
initialization for networks using a sine activation. In sec.
1 of the supp. material, we demonstrate the combination
of our initialization (4.15), (4.16) with theirs and provide
comparisons in sec. 2 of the supp. material.
5. Experiments: Applications to Neural Fields
In this section, we empirically test the theory developed in
sec. 4, focusing exclusively on the widely used initializa-
tions LeCun normal [16], Xavier normal [11], Kaiming nor-
mal [13], and their uniform counterparts. If unfamiliar with
these initializations, please refer to sec. 2 of the supp. ma-
terial. Additionally, we employ four activations: sinc [24],
Gaussian [23], Gabor wavelet [26], each with a frequency
parameter ω(or1/ω2for the Gaussian) and ReLU-PE [33]
with a positional embedding layer. For details on tuning
this hyperparameter and the frequencies used in each exper-
iment, consult sec. 2 of the supp. material. Further experi-
ments can also be found in sec. 2 of the supp. material.
5.1. Practical Validation of the Theoretical Analysis
We conduct empirical testing to validate the theories pre-
sented in sections 4.1and4.2. In section 4.1, we derived
theorem 4.2, demonstrating that a shallow feedforward neu-
ral network utilizing activation functions such as sine, sinc,
Gaussian, or wavelet, and initialized according to LeCun,
Xavier, or Kaiming initialization, necessitates superlinear
growth in the width of the hidden layer as the dataset size
increases for gradient descent to converge to a global mini-
mum.
It is noteworthy that this growth requirement is less strin-
gent than what is established for ReLU activation (with
or without PE) in Ngyuen’s work [20], which asserts a
quadratic growth in the number of data samples. Conse-
quently, we anticipate observing that a ReLU (with or with-
out PE) network demands more parameters than its sinc-
activated counterpart when trained on the same dataset until
convergence.
417
Shallow Experiment: In our investigation, we performed
a 1-dimensional curve fitting experiment on the function
f(x) = sin(2πx ) + sin(6 πx) + sin(10 πx). We system-
atically sampled the curve at intervals of 10,20,50,75,
100,125,150,175, and 200points, creating nine datasets
of varying sizes for our training data.
Subsequently, we employed three networks with sinc ac-
tivation and one hidden layer, along with three networks
featuring ReLU-PE activation and one hidden layer. The
positional embedding layer had dimension 8and employed
a Random Fourier Feature (RFF) type embedding [33].
Each dataset underwent training using full batch gradient
descent until reaching a PSNR value of 35dB. We increased
the number of parameters with the growth in dataset size,
adjusting them until the respective networks achieved con-
vergence at the target PSNR.
Initialization for both sinc and ReLU-PE networks was
performed using LeCun, Xavier, and Kaiming initializa-
tions. The results of this experiment are illustrated in fig.
3 (left). As anticipated by thm. 4.2, the sinc-activated
networks exhibited a significantly lower parameter require-
ment for convergence as the dataset size increased. Further-
more, as fig. 3(left) shows the sinc networks had parameter
growth similar to O(N3/2), as predicited by thm. 4.2. The
ReLU-PE networks had parameter growth as O(N2)as pre-
dicted in [20].
Deep Experiment: For the case of deep networks we ran
a similar experiment to the above shallow networks except
that this time we used an image regression task.
The task was to reconstruct a 512×512 Lion image.
Given pixel coordinates x∈R2, the aim of the task is
to a network Nto regress the associated RGB values c∈
R3[23,29]. We sampled 1000, 5000, 10000, 25000, 50000,
100000, 150000 and200000 pixel coordinates, creating a
total of 8datasets of varying size as the training data.
We employed three networks with sinc activation and
four hidden layers, along with three networks featuring
ReLU-PE activation and four hidden layers. The PE-layer
was16dimensional and used Random Fourier Features
(RFF) as the positional embedding technique [33]. The first
three hidden layers of all networks had 64neurons each. We
increased the number of parameters by only increasing the
width of the last hidden layer as this was shown to suffice
from thm. 4.4.
Each dataset underwent training using full batch gradient
descent until reaching a PSNR value of 25dB. Initialization
for both sinc and ReLU-PE networks was performed using
LeCun, Xavier, and Kaiming initializations. The results of
this experiment are illustrated in fig. 3(right). As predicted
by thm. 4.2, the sinc-activated networks exhibited a signif-
icantly lower parameter requirement for convergence as the
dataset size increased. Nevertheless, it is essential to high-
025e3 50e3 75e3 100e3 125e3 150e3 175e3 200e3
Number of data samples40e3
20e360e380e3100e3120e3140e3160e3180e3Number of parameters
Number of data samplesNumber of parametersFigure 3. Comparing how many parameters are needed for a
ReLU-PE and sinc network to converge with different intializa-
tions and data set sizes. Left figure shows results for shallow net-
works on a 1-dim. curve fitting task. Right figure shows results for
deep networks on a image regression task. For all initializations,
the sinc activated networks require much less parameters to con-
verge than the ReLU-PE ones.
light that beyond a dataset size of 30,000, the sinc networks
achieved convergence using fewer parameters than the ac-
tual number of samples. This discrepancy arises from our
decision to set the cutoff point at 25dB. The rationale be-
hind this choice was rooted in practical considerations re-
lated to memory constraints. Given that we employed full-
batch training, higher dB values necessitated a substantially
greater number of parameters, leading to memory-related
issues.
The second experiment was to test thm. 4.9. In order
to do this we carried out a similar experiment to the above
deep network experiment, using the same datasets and data
instance. In this setting we considered four sinc networks,
each initialized with LeCun normal, Xavier normal, Kaim-
ing normal and initialization 1 (see (4.10)). We trained
each each network until it reached a PSNR of 25dB, in-
creasing the width of the final hidden layer as the dataset
size increased. Fig. 4(left) shows the results of the exper-
iment. As predicted by thm. 4.9, the network employing
our initialization 1 needs far less parameters to converge.
We repeated the experiments this time initializing the net-
works with the uniform distributions, initialization 2 (see
(4.11)), LeCun uniform, Xavier uniform, Kaiming uniform.
Fig. 4(right) shows that the network initialized with initial-
ization 2 required less parameters to converge than all other
networks.
5.2. Single Image Super Resolution
We test our initializations on an image super resolution task.
We take the approach considered in [26], where a 4×su-
per resolution is conducted on an image from the DIV2K
dataset [1, 34]. The problem is cast as solving y=Ax,
where the operator Aimplements a 4×downsampling (with
no aliasing). We then solve for xas the output of a neural
field.
We explored the impact of four normal initializations
and four uniform initializations on a Gaussian-activated net-
418
025e3 50e3 75e3 100e3 125e3 150e3 175e3 200e3
Number of data samples40e3
20e360e380e3100e3120e3140e3160e3180e3Number of parametersComparison for Deep Networks with Normal Initializations
40e3
20e360e380e3100e3120e3140e3160e3180e3Number of parameters
025e3 50e3 75e3 100e3 125e3 150e3 175e3 200e3
Number of data samplesComparison for Deep Networks with Uniform InitializationsFigure 4. Comparing the performance of deep networks with sinc
activation across image regression tasks, utilizing four distinct ini-
tialization schemes. Networks were trained until reaching a 25dB
PSNR. On the left, we observe the outcomes with four normal ini-
tializations, showcasing that our initialization demands the fewest
parameters for convergence. On the right, the comparison ex-
tends to four different uniform initializations, where our approach
emerges as the most effective.
Ground Truth
Kaiming Normal
Kaiming Uniform
LeCun Normal
LeCun Uniform
Xavier Normal
Xavier Uniform
Initialization 1 (ours)
Initialization 2 (ours)29 dB
0.5220 dB
0.41 29 dB
0.35 32 dB
29 dB0.68
0 .6028 dB 20 dB
0.48 34 dB
0.72 0.58
Figure 5. The figure shows the results for a 4×single image su-
per resolution with four normal initializations and four uniform
initializations. Networks initialized with initialization 1 (our) and
initialization 2 (our) produced the highest train dB and SSIM at
convergence. Zoom in for better viewing.
work with two hidden layers. The Gaussian activation was
characterized by a variance of 0.12, a choice validated as
optimal across all initializations and commonly adopted by
practitioners [7, 23, 24,27]. Subsequently, all networks
underwent training using the Adam optimizer. Figure 5
presents the outcomes of this investigation. Notably, among
the normal initializations, our first initialization demon-
strated superior performance, while among the uniform ini-
tializations, our second initialization excelled. In each case,
Structural Similarity (SSIM [36].) was computed, with both
our first and second initializations consistently producing
the highest SSIM values.
5.3. Occupancy Fields
We optimize a binary occupancy field, which represents a
3D shape as the decision boundary of a neural network
[12,35]. We use the thai statue instance obtained from XYZ
RGB Inc. We trained two groups of four networks, each
Figure 6. Top left; comparison of normal initializations. Top right;
comparison of uniform initializations. In both cases our initializa-
tion performs better reaching a higher PSNR. Bottom summary of
final train PSNR and IOU accuracy.
Train PSNR (dB) IOU
Initialization 1 (ours) 22.7 0.89
LeCun Normal 20.3 0.81
Xavier Normal 21.2 0.84
Kaiming Normal 19.9 0.80
Initialization 2 (ours) 24.5 0.91
LeCun Uniform 21.2 0.86
Xaiver Uniform 21.6 0.87
Kaiming Uniform 20.2 0.82
Table 1. Table showing results of each initialization on an Occu-
pancy task.
activated with the Gabor wavelet [26]. The first group of
four networks were trained with four normal initializations,
Lecun normal, Xavier normal, Kaiming normal and initial-
ization 1 (see (4.10), (4.11)). The second group of four net-
works were trained with LeCun uniform, Xavier uniform,
Kaiming Uniform and initialization 2 (see (4.15), (4.16)).
All networks were trained with the Adam optimizer. For
accuracy testing we used the performance metric given by
Intersection Over Union (IOU) . Fig. 6 and table 1 details
the results of the experiments. As can be seen from the
figure, both our initializations converge to a higher PSNR,
2−4dB higher than the others, and have the highest IOU.
Figures of reconstructed meshes can be found in the sec. 2
of supp. material.
5.4. Neural Radiance Fields (NeRF)
NeRF has recently emerged as a compelling method, lever-
aging a Multi-Layer Perceptron (MLP) to model 3D objects
and scenes based on multi-view 2D images. This innova-
tive approach exhibits promise in achieving high-fidelity re-
constructions for novel view synthesis tasks [7, 18,19,25].
Given 3D points x∈R3and viewing directions, NeRF is
designed to estimate the radiance field of a 3D scene. This
field maps each input 3D coordinate to its corresponding
volume density σ∈Rand directional emitted color c∈R3
[7,18,19].
NeRF is commonly trained using Kaiming uniform ini-
tialization for optimal outcomes. Our experiment involved
419
Kaiming Uniform Initialization 2 (ours)
26.1dB 30.2dB 26.3dB 29.1dB
26.3dB31.3dB 26.9dB 29.8dBGround TruthFigure 7. Training results comparison for two Gaussian-activated
NeRFs: one with Kaiming uniform initialization and the other
with Initialization 2 (see (4.15) and (4.16)). Top-left: Training
PSNR. Top-right: Four ground truth instances for testing. Bottom:
Test PSNRs. Initialization 2 consistently outperforms the Kaiming
uniform-initialized NeRF in both training and test accuracy.
two Gaussian-activated NeRFs: one initialized with Kaim-
ing uniform and the other with Initialization 2 (see (4.15),
(4.16)). We used the Lego instance from the NeRF real syn-
thetic data set. All networks were trained with the Adam
optimizer. Fig. 7 displays the results, indicating that Initial-
ization 2 achieved a higher training PSNR by 1.1dB. Testing
across 24 unseen views revealed that Initialization 2 consis-
tently outperformed Kaiming initialization, with a test dif-
ference ranging from 0.1 to 1.1dB across different scenes.
For more details please see sec. 2 of the supp. material.
5.5. Physics Informed Neural Networks (PINNs)
Physics informed neural networks are an innovative neural
architecture that parameterize a physics field arising as the
solution of a partial differential equation (PDE). For an in-
troduction to PINNs we refer the reader to [22].
We consider the 2D incompressible Navier-Stokes equa-
tions as considered in [22].
ut+uux+ 0.01u y=−px+ 0.01(u xx+uyy)(5.1)
vt+uvx+ 0.01v y=−py+ 0.01(v xx+vyy) (5.2)
where u(x, y, t )denotes the x-component of the velocity
field of the fluid, and v(x, y, t )denotes the y-component of
the velocity field. The term p(t, x, y )is the pressure. The
domain of the problem is [−15, 25]×[−8,8]×[0,20].
The fluid field solution to equations (5.1) and (5.2) can
be parameterized by a neural field that is a PINN. The two
PDE equations (5.1) and (5.2) are embedded into the loss
function enforcing a physical constraint on the network as
it trains.
Figure 8. Results from training eight different Gaussian activated
PINNs, each with a different initialization. Top row plots the total
loss PSNR (MSE loss + PDE loss) and bottom row plots PDE loss
PSNR. In both cases the PINNs initialized with our initialization
reach a higher dB in both total and PDE loss.
We trained eight Gaussian-activated PINNs with three
hidden layers and a width of 128. Four PINNs used LeCun,
Xavier, Kaiming, and Initialization 1 ((4.10), (4.11)) for
normal initialization, while the other four used correspond-
ing uniform initializations. Training employed an Adam op-
timizer with full batch. For detailed training setup, please
refer to sec. 3 in the supplementary material.
Fig. (8) shows the training PSNR’s of each of the net-
works. As can be seen from the figure, the PINNs initial-
ized with initialization 1 and 2 reach a higher dB in total
loss and PDE loss.
6. Conclusion
This paper established a theoretical framework for the op-
timal scaling of neural fields as dataset sizes expand, en-
suring optimal convergence during gradient descent. We
uncovered a link between this scalability challenge and the
activation and initialization of the network. Our theoretical
framework yielded state-of-the-art results for both shallow
and deep networks. Moreover, leveraging our theoretical
insights, we proposed a novel initialization scheme and val-
idated its efficacy across diverse neural field applications.
7. Limitations
This paper delves into the theory to identify the necessary
degree of overparameterization for gradient descent to reach
a global minimum. For practical scenarios, training typi-
cally uses mini-batches. Unfortunately, our theoretical re-
sults do not yet apply to mini-batch training and we suggest
that future research applying our findings to mini-batch sce-
narios could provide useful methodologies on how to scale
networks for such training.
420
References
[1] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge
on single image super-resolution: Dataset and study. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition workshops, pages 126–135, 2017. 1,3,6
[2] Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learn-
ing and generalization in overparameterized neural networks,
going beyond two layers. Advances in neural information
processing systems, 32, 2019. 3
[3] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A conver-
gence theory for deep learning via over-parameterization. In
International Conference on Machine Learning, pages 242–
252. PMLR, 2019. 2,3
[4] Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong
Wang. Fine-grained analysis of optimization and general-
ization for overparameterized two-layer neural networks. In
International Conference on Machine Learning, pages 322–
332. PMLR, 2019. 1,2,3
[5] Boyuan Chen, Robert Kwiatkowski, Carl V ondrick, and Hod
Lipson. Fully body visual self-modeling of robot morpholo-
gies. Science Robotics, 7(68):eabn1944, 2022. 1
[6] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning
continuous image representation with local implicit image
function. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, pages 8628–8638,
2021. 1
[7] Shin-Fang Chng, Sameera Ramasinghe, Jamie Sherrah, and
Simon Lucey. Gaussian activated neural radiance fields for
high fidelity reconstruction and pose estimation. In Com-
puter Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXIII,
pages 264–280. Springer, 2022. 1,7
[8] Yaim Cooper. The loss landscape of overparameterized neu-
ral networks. arXiv preprint arXiv:1804.10200, 2018. 3
[9] Kenneth R Davidson and Stanislaw J Szarek. Local operator
theory, random matrices and banach spaces. Handbook of
the geometry of Banach spaces, 1(317-366):131, 2001. 4
[10] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu
Zhai. Gradient descent finds global minima of deep neural
networks. In International conference on machine learning,
pages 1675–1685. PMLR, 2019. 2,3
[11] Xavier Glorot and Yoshua Bengio. Understanding the diffi-
culty of training deep feedforward neural networks. In Pro-
ceedings of the thirteenth international conference on artifi-
cial intelligence and statistics, pages 249–256. JMLR Work-
shop and Conference Proceedings, 2010. 1,2,3,5
[12] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and
Yaron Lipman. Implicit geometric regularization for learning
shapes. ICML, 2020. 7
[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectifiers: Surpassing human-level perfor-
mance on imagenet classification. In Proceedings of the
IEEE international conference on computer vision, pages
1026–1034, 2015. 1,2,3,5
[14] Jiaoyang Huang and Horng-Tzer Yau. Dynamics of deep
neural networks and neural tangent hierarchy. In Interna-tional conference on machine learning, pages 4542–4551.
PMLR, 2020. 2
[15] Arthur Jacot, Franck Gabriel, and Cl ´ement Hongler. Neu-
ral tangent kernel: Convergence and generalization in neural
networks. Advances in neural information processing sys-
tems, 31, 2018. 2
[16] Yann LeCun, L ´eon Bottou, Genevieve B Orr, and Klaus-
Robert M ¨uller. Efficient backprop. In Neural networks:
Tricks of the trade, pages 9–50. Springer, 2002. 1,2,5
[17] Yunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit Agrawal,
and Antonio Torralba. 3d neural scene representations for
visuomotor control. In Conference on Robot Learning, pages
112–123. PMLR, 2022. 1
[18] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Si-
mon Lucey. Barf: Bundle-adjusting neural radiance fields.
InProceedings of the IEEE/CVF International Conference
on Computer Vision, pages 5741–5751, 2021. 7
[19] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. Communications of the ACM, 65(1):99–106, 2021. 1,
2,7
[20] Quynh Nguyen. On the proof of global convergence of gradi-
ent descent for deep relu networks with linear widths. In In-
ternational Conference on Machine Learning, pages 8056–
8062. PMLR, 2021. 1,2,3,5,6
[21] Samet Oymak and Mahdi Soltanolkotabi. Toward moder-
ate overparameterization: Global convergence guarantees for
training shallow neural networks. IEEE Journal on Selected
Areas in Information Theory, 1(1):84–105, 2020. 1,2,3
[22] Maziar Raissi, Paris Perdikaris, and George E Karniadakis.
Physics-informed neural networks: A deep learning frame-
work for solving forward and inverse problems involving
nonlinear partial differential equations. Journal of Computa-
tional physics, 378:686–707, 2019. 8
[23] S. Ramasinghe and S. Lucey. Beyond Periodicity: Towards
a Unifying Framework for Activations in Coordinate-MLPs.
InECCV, 2022. 1,2,5,6,7
[24] Sameera Ramasinghe, Hemanth Saratchandran, Violetta
Shevchenko, and Simon Lucey. On the effectiveness of neu-
ral priors in modeling dynamical systems. arXiv preprint
arXiv:2303.05728, 2023. 1,2,5,7
[25] C. Reiser, S. Peng, Y . Liao, and A. Geiger. KiloNeRF:
Speeding Up Neural Radiance Fields With Thousands of
Tiny MLPs. In ICCV, 2021. 7
[26] Vishwanath Saragadam, Daniel LeJeune, Jasper Tan, Guha
Balakrishnan, Ashok Veeraraghavan, and Richard G Bara-
niuk. Wire: Wavelet implicit neural representations. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 18507–18516, 2023. 1,2,5,
6,7
[27] Hemanth Saratchandran, Shin-Fang Chng, Sameera Ramas-
inghe, Lachlan MacDonald, and Simon Lucey. Curvature-
aware training for coordinate networks. arXiv preprint
arXiv:2305.08552, 2023. 7
[28] Vincent Sitzmann, Michael Zollh ¨ofer, and Gordon Wet-
zstein. Scene representation networks: Continuous 3d-
421
structure-aware neural scene representations. Advances in
Neural Information Processing Systems, 32, 2019. 1
[29] V . Sitzmann, J. Martel, A. Bergman, D. Lindell, G., and Wet-
zstein. Implicit Neural Representations with Periodic Acti-
vation Functions. In NIPS, 2020. 2,5,6
[30] Vincent Sitzmann, Julien Martel, Alexander Bergman, David
Lindell, and Gordon Wetzstein. Implicit neural representa-
tions with periodic activation functions. Advances in neural
information processing systems, 33:7462–7473, 2020. 1
[31] Ivan Skorokhodov, Savva Ignatyev, and Mohamed Elho-
seiny. Adversarial generation of continuous images. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 10753–10764, 2021. 1
[32] Yu Sun, Jiaming Liu, Mingyang Xie, Brendt Wohlberg,
and Ulugbek S Kamilov. Coil: Coordinate-based inter-
nal learning for imaging inverse problems. arXiv preprint
arXiv:2102.05181, 2021. 1
[33] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara
Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-
mamoorthi, Jonathan Barron, and Ren Ng. Fourier features
let networks learn high frequency functions in low dimen-
sional domains. Advances in Neural Information Processing
Systems, 33:7537–7547, 2020. 1,2,5,6
[34] Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-
Hsuan Yang, and Lei Zhang. Ntire 2017 challenge on single
image super-resolution: Methods and results. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition workshops, pages 114–125, 2017. 6
[35] Peng-Shuai Wang, Yang Liu, Yu-Qi Yang, and Xin Tong.
Spline positional encoding for learning 3d implicit signed
distance fields. arXiv preprint arXiv:2106.01553, 2021. 7
[36] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE transactions on image processing,
13(4):600–612, 2004. 7
[37] Difan Zou and Quanquan Gu. An improved analysis of train-
ing over-parameterized deep neural networks. Advances in
neural information processing systems, 32, 2019. 2,3
422
