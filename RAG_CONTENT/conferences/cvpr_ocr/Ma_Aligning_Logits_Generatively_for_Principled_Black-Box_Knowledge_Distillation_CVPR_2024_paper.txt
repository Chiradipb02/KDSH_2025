Aligning Logits Generatively for Principled Black-Box Knowledge Distillation
Jing Ma*, Xiang Xiang∗†
School of Artificial Intelligence and Automation,
Huazhong University of Science and Tech, Wuhan, ChinaKe Wang, Yuchuan Wu, Yongbin Li
Alibaba Damo Academy
Hangzhou, China
Abstract
Black-Box Knowledge Distillation (B2KD) is a formu-
lated problem for cloud-to-edge model compression with in-
visible data and models hosted on the server. B2KD faces
challenges such as limited Internet exchange and edge-
cloud disparity of data distributions. In this paper, we for-
malize a two-step workflow consisting of deprivatization
and distillation, and theoretically provide a new optimiza-
tion direction from logits to cell boundary different from
direct logits alignment. With its guidance, we propose a
new method Mapping-Emulation KD (MEKD) that distills
a black-box cumbersome model into a lightweight one. Our
method does not differentiate between treating soft or hard
responses, and consists of: 1) deprivatization: emulating
the inverse mapping of the teacher function with a genera-
tor, and 2) distillation: aligning low-dimensional logits of
the teacher and student models by reducing the distance
of high-dimensional image points. For different teacher-
student pairs, our method yields inspiring distillation per-
formance on various benchmarks, and outperforms the pre-
vious state-of-the-art approaches.
1. Introduction
Knowledge Distillation (KD) is a widely accepted approach
to the problem of model compression and acceleration,
which has received sustained attention from both the aca-
demic and industrial research communities [14, 16, 35, 44].
The goal of KD is to extract knowledge from a cumbersome
model or an ensemble of models, known as the teacher, and
use it as supervision to guide the training of lightweight
models, known as the student [1, 5, 36]. In the application
of KD, privacy protection has always been a very concern-
ing issue for researchers and users, which not only refers to
the privacy of user data but also includes the model copy-
right of cloud service providers.
Black-Box Knowledge Distillation (B2KD) is a prob-
lem posed in the process of cloud-to-edge model compres-
*Equal contribution, co-first author; also with Nat. Key Lab of MSIIPT.
†Correspondence to xex@hust.edu.cn ; also with Peng Cheng Lab.
Figure 1. Schematic process of cloud-to-edge model compression.
A cumbersome black-box model is deployed on a cloud server,
trained with millions of samples and tags. The cloud server only
provides APIs to receive query data and return inference responses
of either soft or hard type. The edge device needs to distill a
lightweight model using unlabeled local data.
sion [34, 46, 49]. The cloud server hosts a teacher model
whose internal structure and composition, connections be-
tween layers, model parameters, and gradients used for
back-propagation are all invisible and unavailable to edge
devices, as shown in Fig. 1. Due to resource limitations, the
edge device can only host a lightweight student model. At
the same time, low-quality and unlabeled local data cannot
be used to train a reliable deep neural network. As a result,
it must rely on sending query samples to the APIs of cloud
servers for heavy inference [45].
In practice, B2KD faces some key challenges. (a) Cloud
servers and edge devices should maintain limited data ex-
change due to Internet latency and bandwidth constraints, as
well as charges for the amount of queried data or API usage
time. (b) In some cases, for query samples, these APIs only
provide indexes or semantic tags for the category with the
highest probability ( i.e., hard responses), rather than proba-
bility vectors for all possible classes ( i.e., soft responses).
(c) Because users refuse to send sensitive data to cloud
servers, the distribution gap between local and cloud data
is difficult to measure, making the distilled student model
inaccurate in the application.
Adversarial learning has been shown to be effective in
generating pseudo samples, which is widely used in data
augmentation and low-shot learning [8, 49]. A well-trained
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
23148
generator can overcome the mode collapse problem and
align real and synthetic data distribution. In particular, we
want to produce images relevant to training, whether or not
they resemble real data [30]. Meanwhile, images generated
to obtain high responses from the teacher model combine
different patterns with highly generalized features instead
of sample-specific idiosyncrasies [47]. Therefore, using a
well-trained generator to synthesize pseudo images can au-
tomatically filter out privacy-related high-frequency infor-
mation, this process is called deprivatization ,
In this paper, we propose an approach to solve B2KD
bymapping emulation . Our motivation is in accordance
with the fact that it can drive alignment between low-
dimensional logits by reducing the distance between two
generated images in the high-dimensional space. In addi-
tion, we argue that an image contains a lot of fine-grained
information, which can be treated as another type of knowl-
edge to provide different gradient directions for updating
the parameters of student model, as shown in Fig. 4. Com-
bining image-level loss with coarse-grained logit-level loss
can effectively improve the distillation effect. According
to the Kolmogorov theorem [6, 24], a sufficiently complex
neural network is capable of representing an arbitrary multi-
variate continuous function from any dimension to another.
Thus, a well-trained generator can not only emulate the in-
verse mapping of the teacher function (Thm. 1) but also help
update the logits of a student to converge to the logits of a
teacher (Thm. 2), with reasonable generalizability (Thm. 3).
In practice, we derive using a generative adversarial net-
work (GAN) for deprivatization and exploit it as an inverse
mapping of the teacher function. The generator uses ran-
dom variables as inputs that are sampled from a prior distri-
bution with the same dimensionality as the logits. The well-
trained generator is frozen and grafted behind the teacher
and student model, whose output logits of the same exam-
ples are used as the inputs of the generator, as shown in
Fig. 2. Experimental results show that MEKD can effec-
tively protect the privacy of local data and models in the
cloud, and it performs well under either soft or hard re-
sponses. At the same time, MEKD has robust results in
the case of limited query samples and out-of-domain data.
Overall, the contributions of this paper are: 1)We for-
malize the problem of B2KD and provide a two-step work-
flow of deprivatization and distillation. 2)We theoretically
provide a new optimization direction from logits to cell
boundary different from direct logits alignment. 3)We pro-
pose a new method of Mapping-Emulation Knowledge Dis-
tillation (MEKD). The improved experimental performance
has demonstrated the effectiveness of our approach.
2. Related Work
Knowledge Distillation (KD) . Hinton et al. [18] propose
an original teacher-student architecture that uses the logitsof the teacher model as the knowledge. Since then, some
KD methods regard knowledge as final responses to input
samples [3, 29, 53], some regard knowledge as features ex-
tracted from different layers of neural networks [22, 23, 38],
and some regard knowledge as relations between such lay-
ers [9, 37, 52]. The purpose of defining different types of
knowledge is to efficiently extract the underlying represen-
tation learned by the teacher model from the large-scale
data. If we consider a network as a mapping function of
input distribution to output, then different knowledge types
help to approximate such a function. Based on the type of
knowledge transferred, KD can be divided into response-
based, feature-based, and relation-based [14]. The first two
aim to derive the student to mimic the responses of the out-
put layer or the feature maps of the hidden layers of the
teacher, and the last approach uses the relationships be-
tween the teacher’s different layers to guide the training of
the student model. Feature-based and relation-based meth-
ods [23, 52], depending on the model utilized, may leak the
information of structures and parameters through the inter-
mediate layers’ data. For example, we can reconstruct a
ResNet [17] based on the feature dimensions of different
layers, and calculate each neuron’s parameter using specific
images and their responses in the feature maps.
Black-Box Knowledge Distillation (B2KD) . Response-
based KD methods [3, 18, 53] have the natural property of
hiding models. Hinton et al. [18] use Kullback-Leibler Di-
vergence (KLD) between the softened logits of teacher and
student models as the loss to align the output distribution,
and Zhao et al. [53] decouple the KLD into two uncor-
related losses and combine them by weighted summation.
These calculations do not take into account the details of
the teacher model, which is exactly a black box. The re-
cently proposed approaches for B2KD also address the is-
sue of hiding the teacher model deployed in the cloud server
[34, 46, 49]. Orekondy et al. [34] use a reinforcement learn-
ing approach to improve query sample efficiency. Wang et
al. [46] blend mixup and activate learning to augment the
few unlabeled images and choose hard examples for distil-
lation. And Wang [49] proposes a decision-based black-box
model and constructs the soft label for each training sample
by computing its distances to the decision boundaries of the
teacher model. These existing approaches partially address
the challenges of cloud-to-edge black-box model distilla-
tion, but none of them take into account the privacy leak of
user data when sending original local images to the cloud.
Generative Adversarial Networks (GANs) have the
capacity to handle sharp estimated density functions and
generate realistic-looking images efficiently. A typical
GAN [13] comprises a discriminator distinguishing real im-
ages and generated images, and a generator synthesizing
images to fool the discriminator. GANs are divided into
architecture-variant and loss-variant. The former focuses on
23149
Figure 2. The overall framework of MEKD. Lower left: two architectures of GAN-based KD. Upper right: the process of deprivatization.
GAN is used to synthetic high-response images to the teacher model within the distribution of data in edge devices. Lower right: the
process of distillation with the frozen generator. The synthetic privacy-free images are query samples sent to the teacher model through the
APIs of cloud servers. The student model is distilled by reducing the logit-level and image-level discrepancy.
network architectures [7, 39] or latent space [10, 31], e.g.,
some specific architectures are proposed for specific tasks
[21, 54]. The latter utilizes different loss types and regular-
ization tools [2, 15] to enable more stable learning.
Adversarial Distillation (AD) exploits adversarial ar-
chitecture to help the teacher and student model have a
better understanding of the real data distribution [4, 14,
41, 48, 50]. The methods of AD can be divided into two
types according to the generator-discriminator architecture,
as shown in Fig. 2: (a) the generator is used to synthetic im-
ages to obey a real distribution, and these images are used to
help distill models [8, 48]; (b) the teacher and student mod-
els are regarded as generators and another discriminator is
drafted behind them to judge whether the distribution of fea-
tures or logits is consistent [50, 51]. AD is also employed
for low-shot knowledge distillation and received inspiring
results [8]. Our method provides an alternative adversar-
ial architecture, which utilizes a well-trained generator to
guide the alignment between the outputs of models.
3. Theory for Mapping-Emulation KD
First, we propose two definitions. Def. 1 defines that two
functions that map the same data distribution µto the same
latent distribution υare equivalent. The ideal state of KD
is to obtain a student function fSthat is equivalent to the
teacher function fT. Def. 2 defines that the mapping func-
tion of a generator G, which can map a prior distribution p
to data manifold Σand guarantee that the generated image
distribution µ′is the same as the real image distribution µ,
is considered to be the inverse mapping of the teacher func-
Figure 3. Mapping relationships of fS, fT, fG. IffSandfTcan
mapµto the same distribution υ, then fS=fT, and if fGcan
map the prior distribution ptoµ, then fG=f−1
T.
tion, i.e.fG=f−1
T. And we call it a well-trained generator.
The mapping relationships are shown in Fig. 3.
Definition 1. (Function Equivalence) Giving the student
and teacher model fSandfT, for a data distribution µ∈ X
in image space which is mapped to PS∈ Y andPT∈ Y
in latent space. If the Wasserstein distance between PSand
PTequals zero,
W(PS,PT) = inf
γ∈Π(PS,PT)E(yS,yT)∼γ[∥yS−yT∥] = 0,
(1)
the student and teacher model are equivalent, i.e.,fS=
fT, where Π(PS,PT)is the set of all joint distributions
γ(yS, yT)whose marginals are PSandPT, respectively.
Definition 2. (Inverse Mapping) Giving a prior distribu-
tionp∈RC, for a data distribution µ∈Rn, if the Wasser-
stein distance between generated distribution µ′= (fG)#p
andµequals zero,
W(µ′, µ) = inf
γ∈Π(µ′,µ)E(x′,x)∼γ[∥x′−x∥] = 0,(2)
23150
then the generator fG:RC→Rnis the inverse mapping of
the teacher function fT:Rn→RC, denoted as fG=f−1
T,
where Π(µ′, µ)is the set of all joint distributions γ(x′, x)
whose marginals are respectively µ′andµ.
Fixing a decoding map fGfor a well-trained generator
G, the latent space Zis partitioned as
D(fG) :Z=[
αUα, (3)
where D(fG)is called the decomposition induced by the
decoding map fG[28], and {Uα}are called cells. As
shown in Fig. 4, fGmaps a cell decomposition in the latent
spaceD(fG)to a cell decomposition in the image space
1
nP
iδx(i). Each cell Uαis mapped to a sample δx(i)by
the decoding map fG[27]. In another word, fGpushes the
prior distribution pto the exact empirical distribution,
(fG)#p=1
nX
iδx(i). (4)
Theorem 1. (Empirical Approximation) For any 0< ϵ <
1/2and any integer m > 4, letg:RC→Rnbe the
mapping function of generator Gwithn≤20 log m
ϵ2. For
two sets VS={yS:yS∈PS}andVT={yT:yT∈
PT}, both of which have mpoints in RC, if the empirical
Wasserstein distance between g(VS)andg(VT)equals zero,
ˆW(g(VS), g(VT)) =1
mmX
i=1∥g(yi
S)−g(yi
T)∥= 0,(5)
thenW(PS,PT) = 0 .
Thm. 1 (see Appendix for proof) provides a method to
approximate the expected Wasserstein distance W(PS,PT)
using the empirical Wasserstein distance ˆW(g(VS), g(VT)).
By reducing the distance between points g(yi
S)andg(yi
T)
in high-dimensional space, an optimization direction ∇LF
different from ∇LKLis produced for logits yi
Sandyi
Tin
low-dimensional space. The gradient update causes yi
Sto
move towards the boundary of the cell in which yi
Tresides,
as shown in Fig. 4.
Theorem 2. (Optimization Direction) Letµ∈ X be any
distribution. fS, fT, fGare the mapping functions of the
student, teacher, and generator, respectively. fSis parame-
terized by θS∈ΘS. Then, when
min
θS∈ΘSEx∼µ[∥fG◦fS(x), fG◦fT(x)∥]→0,(6)
it holds that fS→fT, and we have
∇θSEx∼µ[fS(x)] =∇θSW(PS,PT)
=Ex∼µ[∇θS∥fG◦fS(x)−fG◦fT(x)∥].(7)
Thus, to achieve fS→fT, it is sufficient to optimize
Ex∼µ[∥fG◦fS(x), fG◦fT(x)∥]in the parameter space
ΘS. The global gradient of parameter θScan be replaced
by the gradient calculated on the empirical distance of high-
dimensional image points, refer to Appendix for proof.
Figure 4. Cell Uαin the latent space is mapped via fGto an exact
image x(i)of the same color. The move of point x′
Stox′
Tcauses
the logits ySto align with yTfrom a direction different from LKL.
Theorem 3. (Generalization Bound) LetH⊆RX×Ybe
a hypothesis set for C-way classification task. For any 0<
ϵ <1/2and a sample Sof size m > 4drawn according
toµ, letg:RC→Rnbe a mapping function of generator
Gwithn≤20 log m
ϵ2. Fix ρ >0, for any 1> δ > 0, with
probability at least 1−δ, the following holds for all h∈H,
R(h)≤ˆRρ(h) +2C2
ρ(1−ϵ)r
r2Λ2
m+s
log1
δ
2m. (8)
For any x∈ X, theΛ≥0and(PC
y=1∥h(x, y)∥p)1/p≤Λ
for any p≥1, and the r >0forK(x, x)≤r2where kernel
K:X × X → Ris positive definite symmetric.
Thm. 3 (see Appendix for proof) gives the generaliza-
tion bound of aligning low-dimensional logits by reducing
the distance of high-dimensional image points, which guar-
antees generalizability to the unseen samples.
4. Algorithm of Mapping-Emulation KD
Hinton et al. [18] propose a simple but effective KD method
that uses the softened logits of the teacher model as a super-
vision to guide student training. They use the Kullback-
Leibler Divergence (KLD) to measure the discrepancy be-
tween the logits of the two models, where the student model
is trained to minimize the gap in the hope of achieving the
same output. The loss is defined as
LKL=KL[p(c|xi;θT)||p(c|xi;θS)]
=1
NNX
iCX
cp(c|xi;θT) logp(c|xi;θT)
p(c|xi;θS)
,(9)
where iis the sample index and Nis the number of samples.
Regardless of the method used, the essence of KD is to learn
the mapping function of the teacher model from input to
output, i.e.,fT. However, it is hard to deduce the mapping
function from the existing parameters of the teacher model.
One can only guess the mapping process by using the re-
sponses to the input samples of different network layers or
the relations between features and treat them as knowledge
to guide the training of the student model [52]. However, in
the black-box KD problem, the internal responses or rela-
tions between layers of the teacher model are not available,
which makes effective distillation more challenging.
23151
Deprivatization. For a C-way classification problem,
we first train a GAN using the random noise variable zsam-
pled from the prior distribution pin latent space Yas input.
Note that the dimensionality of zis the same as the output
logits of the teacher model, i.e.|z|=C. The generator G
uses noise zto synthesize images, and the discriminator D
minimizes the Wasserstein distance between the generated
µ′and the real distribution µ. The synthetic privacy-free
images are simultaneously sent to the cloud server for in-
ference responses, which can be soft (probability vectors
for all possible classes) or hard (indexes or semantic tags
for the category with the highest probability). We expect the
synthetic images to match the high responses of the teacher
model so that they can maximize the containment of pat-
terns in real data. We adopt the information maximization
(IM) loss [20, 42], which is formulated as
LIM=−1
mmX
i=1ˆy(i)
tlog
D
G
z(i)
, (10)
where ˆy(i)
t= max c∈CT 
G 
z(i)
for0.0≤ˆy(i)
t≤1.0in
soft responses and ˆy(i)
t= 1.0in hard responses.
Suppose the discriminator is capable of completely blur-
ring the discrepancy between synthetic and genuine im-
ages. In this case, the resulting generator represents a func-
tion from the latent space to the image space, defined as
fG:Y → X , with an inverse mapping of the teacher
function. Note that the generator and the discriminator are
trained simultaneously: we adjust parameters for the gener-
ator to minimize log(1−D(G(z)))and adjust parameters
for the discriminator to minimize logD(x). And their loss
functions are
LD=−1
mmX
i=1h
logD
x(i)
+ log
1−D
G
z(i)i
,
(11)
LG=−1
mmX
i=1log
1−D
G
z(i)
. (12)
We introduce a trade-off hyperparameter αto balance
LGAN andLIM, and all the losses in the first step of de-
privatization constitute
LDp= (LG+LD) +αLIM, (13)
Distillation. The well-trained generator Gcontains the
knowledge that the teacher uses to make inferences. It is
equivalent to a teacher assistant transferring the teacher’s
knowledge to the student. Fig. 2 illustrates the architecture
of MEKD. We freeze the generator and graft it behind the
teacher and student model in the same way, using the soft-
ened logits of both models as the generator input. A batch
of synthetic images X′={x′(i)=fG(z(i))}m
i=1is fed into
the embedded network to output high-dimensional points
in the same image space, simultaneously. The distance be-
tween the output high-dimensional points from the logits ofthe teacher model X′′
T=fG◦fT(X′)and the others from
the student X′′
S=fG◦fS(X′)are measured by the distance
measurement formula LF= d(X′′
S, X′′
T). We minimize the
distance LFto drive the student model to mimic the output
logits of the teacher model, and use L1-norm ( F= 1) of
X′′
SandX′′
Tas the loss function to distill the student,
LDt=1
mmX
i=1G
S
x′(i)
/τ
−G
T
x′(i)
/τ
F
+β1
mmX
i=1T
x′(i)
log
T
x′(i)
S
x′(i)
, (14)
where query sample x′(i)=G(z(i))is generated from noise
z(i)and temperature τis used to soften the output logits.
Through the experiments, we found that L2-norm has a sim-
ilar effect with L1-norm, refer to Tab. 5.
We also add logit-level knowledge (Eqn. 9) to induce dis-
tillation and use a hyperparameter βto balance these two
losses. Unlike most KD methods, we do not use cross-
entropy loss with ground-truth labels, due to its unavailabil-
ity in edge devices. An algorithm is summarized in Alg. 1.
Algorithm 1 MEKD optimization algorithm.
Input: Pre-trained teacher T(x;θT)deployed in the cloud
server, random initialized student S(x;θS)and local dataset
Xhosted in the edge device.
Output: An optimized student S(x;θS)on dataset X.
1:▷Step 1: Deprivatization
2:Initialize a generator G(z;θG)and a discriminator
D(x;θD), and ensure the dimensionality of zequals
to the category count C.
3:repeat
4: Sample a batch of noises Zfrom a prior distribu-
5: tionpand synthetic images X′=G(Z).
6: TheX′is sent to Tin cloud to get soft or hard
7: inference responses ˆY′
t=T(X′).
8: Sample a batch of examples Xfrom dataset X.
9: Update the discriminator Dto distinguish X
10: andX′usingLDfrom Eqn. 11.
11: Update the generator Gto fool the discriminator D
12: usingLG+αLIMfrom Eqn. 10 and Eqn. 12.
13:until converge
14:▷Step 2: Distillation
15:Initialize the student Sand freeze the generator G.
16:repeat
17: Sample a batch of noises Zfrom a prior distribu-
18: tionpand synthetic images X′=G(Z).
19: TheX′is sent to Tin cloud to get soft or hard
20: inference responses ˆY′
t=T(X′).
21: Update the student SusingLDtfrom Eqn. 14.
22:until converge
23152
Method Data Size MNIST CIFAR-10 CIFAR-100 Tiny ImageNet
Teacher 50K ∼100KResNet32 VGG13 ResNet56 ResNet56 VGG13 ResNet56 ResNet110 ResNet110
99.50 99.52 94.15 94.15 74.68 72.06 60.71 60.71
Student 50K ∼100KResNet8 VGG11 ResNet8 VGG11 VGG11 VGG11 ResNet32 MobileNet
99.24 99.41 87.74 91.81 69.12 69.12 55.47 56.07
KD [18] 50K ∼100K 99.33 99.44 86.58 82.25 70.88 67.97 54.14 57.85
ML [3] 50K ∼100K 99.49 99.40 87.89 91.91 67.78 70.18 56.56 60.07
AL [48] 50K ∼100K 99.37 99.26 87.25 91.97 69.92 71.13 46.02 51.29
DKD [53] 50K ∼100K 99.33 99.43 86.61 92.42 67.32 70.10 55.99 59.43
DAFL [8] 0K 96.42 97.00 60.67 66.03 43.78 48.32 38.44 40.93
KN [34] 10K 98.61 98.81 80.62 82.41 57.83 55.64 48.92 50.22
AM [46] 10K 99.33 99.47 74.89 74.26 62.17 63.20 47.72 51.54
DB3KD [49] 10K 98.94 99.16 78.47 85.84 63.48 62.76 47.95 50.49
MEKD (soft) 10K 99.40 99.43 85.36 87.27 64.76 64.83 50.87 54.93
MEKD (hard) 10K 99.40 99.45 84.45 87.25 64.72 65.32 49.89 54.71
Table 1. Top-1 classification accuracy (%) of the student model on MNIST, CIFAR-10, CIFAR-100 and Tiny ImageNet.
5. Experiments
5.1. Experiment Setup
In this section, we compare our method with response-
based KD and black-box KD methods in an unsupervised
environment. Experimental results show that when the
cross-entropy loss based on ground-truth labels is removed,
the distillation performance of these methods decreases.
Datasets Setup. We conduct experiments on MNIST
[26], CIFAR [25], Tiny ImageNet [11], and ImageNet-1K
[11], all of which are widely used for image classification.
While training B2KD methods, we randomly select 10K
images ( 100Kfor ImageNet-1K) from the training set, and
all images in the test set (val set for ImageNet) are used as
the benchmark to calculate accuracy. For other approaches,
except DAFL [8] based on zero-shot learning, we use the
whole training set. We mainly use top-1 classification accu-
racy as an evaluation metric to assess the distillation effect.
To make a fair and intuitive comparison, we follow the same
setup as previous B2KD methods in our main experiments .
However, we find that the original settings in the B2KD ex-
periments do not represent the challenges raised in practical
applications, so we add extended experiments in Sec. 5.4 to
illustrate the practicability of our proposed method.
Implementation. See also the project page1. We use
ResNet [17], VGG [43] and MobileNet [19] as the back-
bone, and adopt standard data augmentation techniques
(random crop and horizontal flip) and an SGD optimizer
in all experiments. We consistently train the teacher and
student model for 350 epochs, except for 12epochs for
MNIST, and we adopt a multi-step LR scheduler following
the paper [22]. After training the teacher, we train a DC-
GAN [39] with Gaussian noise in the same dimension as the
1https://github.com/HAIV-Lab/MEKDcategory counts. The output logits of teacher or student for
samples in the same class follow a Gaussian distribution,
and the logits center is the mean of the Gaussian. Since
the conversion between different Gaussian distributions is
a linear process, using Gaussian as the prior distribution p
provides a smooth dual space for the student’s logits update.
Competing Methods. In order to verify the effec-
tiveness of our method, we compare several methods of
response-based KD and black-box KD. We select KD [18]
proposed by Hinton et al. and ML [3] proposed by Ba
and Caruana as the baselines, and we also compare the re-
cently published DKD [53] based on decoupled KLD. For
the two GAN-based KD frameworks summarized in Sec. 2,
we choose AL [48] and DAFL [8] as comparison methods.
Meanwhile, we compete with some black-box KD methods
such as KN [34], AM [46] and DB3KD [49]. Of these meth-
ods, DB3KD and MEKD(hard) only utilize hard responses,
while the other methods are based on soft responses.
5.2. Performance Evaluation
On MNIST, CIFAR, and Tiny ImageNet, we use
ResNet32/56/110 and VGG13 as the teacher model and use
ResNet8/32, VGG11, and MobileNet as the student model.
We compare the top-1 classification accuracy (ACC) of dif-
ferent teacher-student pairs, the results are shown in Tab. 1.
On relatively easy tasks, such as MNIST and CIFAR-10,
T - S KD AL AM DB3KD MEKD
Pairs (soft) (soft) (soft) (hard) (soft)
RN50 - RN34 52.08 53.50 56.92 58.61 59.89
RX101 - RX50 54.90 50.88 55.64 59.90 61.21
Table 2. Top-1 classification accuracy (%) of the student model
on ImageNet-1K with data size 100K. We use the pre-trained
ResNet50 (76.13%) and ResNeXt101 (79.32%) as teachers.
23153
our proposed method has a small gap compared to response-
baed KD methods that use the full training set. This makes
sense in the applications of cloud-to-edge model compres-
sion because edge devices do not have a lot of capacity to
store more than ten thousand pieces of data.
CIFAR-100 and Tiny ImageNet are more challenging.
These tasks contain far more patterns than MNIST and
CIFAR-10, and data distributions are so complex that it is
difficult for a generator to capture all the patterns. However,
as long as the mode collapse problem can be mitigated, it is
possible to synthesize complex samples beneficial to dis-
tillation, so we exploit DCGAN [39] as our generator. DC-
GAN has a more stable training process and is more suitable
for generating RGB images than a fully-connected GAN
[39]. Experimental results show that MEKD can obtain an
accuracy improvement of 5%∼10% compared to other
B2KD methods, and the accuracy of MEKD with soft or
hard responses is similar, with a difference of less than 1%.
We also conduct experiments on large-scale datasets and
sophisticated networks. On ImageNet-1K, we use two
teacher-student (T-S) pairs of ResNet50 (RN50) - ResNet34
(RN34) and ResNeXt101 (RX101) - ResNeXt50 (RX50).
All methods are trained using a subset of 100Ksamples.
The experimental results are shown in Tab. 2.
Uniformly, we set the number of query samples to 50K
on CIFAR and MNIST, 300Kon ImageNet, and discuss the
performance impact of limited query samples in Sec. 5.4.
5.3. Ablation Study
We choose an effective T-S pair [32] of ResNet56 - Mo-
bileNet for ablation studies unless otherwise stated.
Ablation Study of Data Size. We explore the perfor-
mance with different data sizes, the results are shown in
Tab. 3. In general, B2KD methods have higher robustness to
small data sizes than traditional KD methods, and in which
MEKD achieves the highest distillation performance.
Ablation Study of Deprivatization. Theαis a hyper-
parameter to balance LGAN andLIM. TheLIMis used to
maximize the responses of the teacher to the generated sam-
ples. Therefore, the training of the generator with or without
LIMwill affect the quality of synthetic images. Fig. 5 (a)
shows real images of CIFAR-10. Fig. 5 (b) shows synthetic
Data Size 0.1K 1K 10K 50K (full)
KD [18] 16.74 31.25 70.90 90.43
AL [48] 12.97 32.05 68.61 90.54
AM [46] 48.31 62.05 73.65 86.33
DB3KD [49] 43.05 64.28 81.67 92.46
MEKD (soft) 49.04 69.84 86.85 93.48
MEKD (hard) 47.12 68.66 86.53 93.09
Table 3. Ablation study of data size on CIFAR-10. We use the T-S
pair of ResNet56 - MobileNet, and the full training set is 50K.
(a)
 (b)
 (c)
Figure 5. Real images of CIFAR-10 (a) and synthetic images using
MEKD with LIM(b) and without LIM(c).
images with α= 0.5and Fig. 5 (c) shows synthetic images
without LIM(i.e.α= 0), both using the same noise vec-
tors. The teacher of ResNet56 responds from 0.72∼0.96
to the synthetic image in Fig. 5 (b) and from 0.41∼0.87
to the one in Fig. 5 (c). The effect of αis also reported in
Tab. 4, which reflects that the utilization of LIMcan im-
prove the performance of model distillation.
Ablation Study of Distillation. In Eqn. 14, the βis a
trade-off hyperparameter to balance LFandLKL, which
provide different gradient directions for θS. As shown in
Tab. 4, the distillation performance can be improved by in-
troducing KLD as an additional loss function.
The temperature τis another important hyperparame-
ter for MEKD since it softens the output logits of both the
teacher and student models. The results are shown in Fig. 6.
Its validity comes from the fact that softened logits can in-
crease the probability of being sampled in a standard normal
distribution. Since GANs use a standard Gaussian distri-
bution as input, samples generated from out-of-distribution
noises with low-sampling probability are usually fuzzy and
incorporate few patterns [40], which are meaningless for
distillation. Meanwhile, a high value of τreduces the dis-
crepancy between softened logits, and LF= 0 when they
locate in the same cell. It reduces the performance of distil-
lation, especially for challenging tasks, such as ImageNet.
1,0 3.0 5.0 7.0 9.0
Temperature 
586062646668T op-1 Accuracy
soft
hard
(a)
1,0 3.0 5.0 7.0 9.0
Temperature 
83.584.084.585.085.586.086.587.0T op-1 Accuracy
 (b)
Figure 6. Ablation study of temperature τon CIFAR-100 (a) and
CIFAR-10 (b). We use the T-S pair of ResNet56 - MobileNet.
αResponse Type
Soft Hard
0.0 60.87 61.31
0.1 66.06 66.86
0.5 67.07 67.36
1.0 67.01 67.11βResponse Type
Soft Hard
0.0 56.28 56.23
0.1 64.13 65.60
0.5 66.79 67.01
1.0 67.07 67.36
Table 4. Ablation study of hyperparamete αandβon CIFAR-100.
We use the T-S pair of ResNet56 - MobileNet.
23154
Dataset Method Model ACC (L1/L2)
CIFAR-10MEKD (soft) MobileNet 86.85 / 86.63
MEKD (hard) MobileNet 86.53 / 86.88
CIFAR-100MEKD (soft) MobileNet 67.07 / 66.95
MEKD (hard) MobileNet 67.36 / 66.94
Table 5. Ablation study of different LF. We use ResNet56 as the
teacher model. ACC: top-1 classification accuracy (%).
10K 20K 30K 40K 50K
Query Samples303540455055606570T op-1 Accuracy
(a)
10K 20K 30K 40K 50K
Query Samples303540455055606570T op-1 Accuracy
 (b)
10K 20K 30K 40K 50K
Query Samples405060708090T op-1 Accuracy
(c)
10K 20K 30K 40K 50K
Query Samples405060708090T op-1 Accuracy
  w  IM &   w  KL
w/o IM &   w  KL
  w  IM & w/o KL
w/o IM & w/o KL (d)
Figure 7. Curve of top-1 classification accuracy on the datasets
of CIFAR-100 (a,b) and CIFAR-10 (c,d). Using MEKD with soft
(a,c) or hard (b,d) responses with or without LIMandLKL. We
use ResNet56 as the teacher and use MobileNet as the student.
Ablation Study of Different LF.In Eqn. 14, we use LF
to calculate the distance between generated samples X′′
Sand
X′′
T. From the analysis of experimental results, as shown
in Tab. 5, we argue that the effect on distillation is similar
whether Fequals 1or2. The reason is that LFis used to
measure the distance between logits of the student and the
boundary of cells, in which logits of the teacher reside, and
different LFrepresent similar gradient directions.
5.4. Extended Experiments
In the real-world application of cloud-to-edge model com-
pression, there are some restrictions, such as the limitation
of Internet data exchange and the domain shift in practical
scenarios. We conduct additional experiments to explore
the effect of MEKD under these constraints.
MEKD with Limited Query Samples. We distill a stu-
dent MobileNet on CIFAR-10 and CIFAR-100 with a total
query sample size ranging from 10Kto50Kwith an inter-
val of 10K. We report the ACC of MEKD with or without
LIMandLKL. The curves in Fig. 7 show that with more
query samples sent to the cloud server, the student model
in the edge device can be trained more fully. We can also
analyze from the curves that LIMdoes not seem to be that
useful without using KLD as an additional distillation loss
function, and it gives a big boost to the overall MEKD due
to the extra gradient direction of the mapping emulation.TeacherResNet56 VGG13Data
Exchange74.37 79.86
Student MobileNet MobileNet
KD [18] 76.27 80.67 ∼175 MB
ML [3] 76.78 81.90 ∼175 MB
AL [48] 77.09 80.98 ∼175 MB
DKD [53] 75.47 80.64 ∼175 MB
DAFL [8] 69.20 67.07 ∼28.4 GB
KN [34] 79.65 83.37 ∼145 MB
AM [46] 84.05 86.70 ∼11.6 GB
DB3KD [49] 90.15 91.14 ∼20.8 GB
MEKD (soft) 86.45 88.65 ∼120 MB
MEKD (hard) 86.77 89.21 ∼120 MB
Table 6. Top-1 classification accuracy (%) of methods on SVHN.
The teacher models are trained on Syn. Digits with vanilla su-
pervised learning, and achieve the top-1 classification accuracy of
99.56% for ResNet56 and 99.52% for VGG13 on Syn.Digits.
MEKD with Out-of-Domain Data. We train a teacher
(ResNet56 or VGG13) with vanilla supervised learning on
Syn. Digits [12], which contains about 500Ksoftware-
synthesized images. We distill a student (MobileNet) on
SVHN [33] consisting only of real-shooting photographs.
Tab. 6 shows the ACC on the test set of SVHN. MEKD
outperforms most methods in the task of out-of-domain dis-
tillation, while DB3KD achieves higher performance due to
the use of robust labels [49]. However, DB3KD leads to a
very high data exchange cost between the server and client,
since it requires multiple queries to find a mixed image lo-
cated in the decision boundary to compute robust labels. In
contrast, the data exchange cost of MEKD is much lower.
6. Conclusion
In this paper, we provide a two-step workflow of depriva-
tization and distillation for B2KD. Different from aligning
logits directly, we theoretically provide a new optimization
direction from logits to cell boundaries, and propose a new
method of MEKD. Taking a generator as an inverse map-
ping of the teacher function does not leak information about
the internal structure or parameters of the teacher, because
it has a completely different network structure.
Limitation. A well-trained generator is critical in MEKD,
and GANs are known to suffer from mode collapse, espe-
cially for challenging tasks. We alleviate this problem with
DCGAN. Although the parameter size and structural limita-
tions of the model prevent the student from fully mimicking
the function of the teacher, MEKD can still improve distil-
lation performance compared with other B2KD methods.
Acknowledgement . This research was supported by Natu-
ral Science Fund of Hubei Province (Grant # 2022CFB823),
Alibaba Innovation Research program under Grant Contract
# CRAQ7WHZ11220001-20978282, and HUST Indepen-
dent Innovation Research Fund (Grant # 2021XXJS096).
23155
References
[1] Gustavo Aguilar, Yuan Ling, Yu Zhang, Benjamin Yao, Xing
Fan, and Chenlei Guo. Knowledge distillation from internal
representations. In Proceedings of the AAAI Conference on
Artificial Intelligence , pages 7350–7357, 2020.
[2] Martin Arjovsky, Soumith Chintala, and L ´eon Bottou.
Wasserstein generative adversarial networks. In Interna-
tional Conference on Machine Learning , pages 214–223.
PMLR, 2017.
[3] Jimmy Ba and Rich Caruana. Do deep nets really need to be
deep? Advances in Neural Information Processing Systems ,
27, 2014.
[4] Vasileios Belagiannis, Azade Farshad, and Fabio Galasso.
Adversarial network compression. In Proceedings of the Eu-
ropean Conference on Computer Vision Workshops , pages
0–0, 2018.
[5] Paul Bergmann, Michael Fauser, David Sattlegger, and
Carsten Steger. Uninformed students: Student-teacher
anomaly detection with discriminative latent embeddings. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 4183–4192, 2020.
[6] J ¨urgen Braun and Michael Griebel. On a constructive proof
of kolmogorov’s superposition theorem. Constructive Ap-
proximation , 30(3):653–675, 2009.
[7] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale gan training for high fidelity natural image synthesis.
InInternational Conference on Learning Representations ,
2018.
[8] Hanting Chen, Yunhe Wang, Chang Xu, Zhaohui Yang,
Chuanjian Liu, Boxin Shi, Chunjing Xu, Chao Xu, and Qi
Tian. Data-free learning of student networks. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 3514–3522, 2019.
[9] Hanting Chen, Yunhe Wang, Chang Xu, Chao Xu, and
Dacheng Tao. Learning student networks via feature embed-
ding. IEEE Transactions on Neural Networks and Learning
Systems , 32(1):25–35, 2020.
[10] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Infogan: Interpretable repre-
sentation learning by information maximizing generative ad-
versarial nets. Advances in Neural Information Processing
Systems , 29, 2016.
[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition , pages 248–255, 2009.
[12] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas-
cal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario
Marchand, and Victor Lempitsky. Domain-adversarial train-
ing of neural networks. The journal of machine learning
research , 17(1):2096–2030, 2016.
[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. Advances in
Neural Information Processing Systems , 27, 2014.
[14] Jianping Gou, Baosheng Yu, Stephen J Maybank, andDacheng Tao. Knowledge distillation: A survey. Interna-
tional Journal of Computer Vision , 129(6):1789–1819, 2021.
[15] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent
Dumoulin, and Aaron C Courville. Improved training of
wasserstein gans. Advances in Neural Information Process-
ing Systems , 30, 2017.
[16] Chaoyang He, Murali Annavaram, and Salman Avestimehr.
Group knowledge transfer: Federated learning of large cnns
at the edge. Advances in Neural Information Processing Sys-
tems, 33:14068–14080, 2020.
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 770–778, 2016.
[18] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Dis-
tilling the knowledge in a neural network. arXiv
preprint:1503.02531 , 2(7), 2015.
[19] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efficient convolu-
tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861 , 2017.
[20] Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto,
and Masashi Sugiyama. Learning discrete representations
via information maximizing self-augmented training. In In-
ternational conference on machine learning , pages 1558–
1567. PMLR, 2017.
[21] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 4401–4410, 2019.
[22] Jangho Kim, SeongUk Park, and Nojun Kwak. Paraphras-
ing complex network: Network compression via factor trans-
fer.Advances in Neural Information Processing Systems , 31,
2018.
[23] Nikos Komodakis and Sergey Zagoruyko. Paying more at-
tention to attention: improving the performance of convolu-
tional neural networks via attention transfer. In ICLR , 2017.
[24] Mario K ¨oppen. On the training of a kolmogorov network.
InInternational Conference on Artificial Neural Networks ,
pages 474–479. Springer, 2002.
[25] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. Unvieristy of Toronto:
Technical Report , 2009.
[26] Yann LeCun, L ´eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
[27] Na Lei, Kehua Su, Li Cui, Shing-Tung Yau, and Xianfeng
Gu. A geometric view of optimal transportation and gener-
ative model. Computer Aided Geometric Design , 68:1–21,
2019.
[28] Na Lei, Dongsheng An, Yang Guo, Kehua Su, Shixia Liu,
Zhongxuan Luo, Shing-Tung Yau, and Xianfeng Gu. A ge-
ometric understanding of deep learning. Engineering , 6(3):
361–374, 2020.
[29] Zhong Meng, Jinyu Li, Yong Zhao, and Yifan Gong. Condi-
tional teacher-student learning. In Proceedings of the IEEE
23156
International Conference on Acoustics, Speech and Signal
Processing , pages 6445–6449, 2019.
[30] Paul Micaelli and Amos J Storkey. Zero-shot knowledge
transfer via adversarial belief matching. Advances in Neu-
ral Information Processing Systems , 32, 2019.
[31] Mehdi Mirza and Simon Osindero. Conditional generative
adversarial nets. arXiv preprint:1411.1784 , 2014.
[32] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir
Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Im-
proved knowledge distillation via teacher assistant. In Pro-
ceedings of the AAAI conference on artificial intelligence ,
pages 5191–5198, 2020.
[33] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis-
sacco, Bo Wu, and Andrew Y Ng. Reading digits in natural
images with unsupervised feature learning. 2011.
[34] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz.
Knockoff nets: Stealing functionality of black-box models.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 4954–4963, 2019.
[35] Kaan Ozkara, Navjot Singh, Deepesh Data, and Suhas Dig-
gavi. Quped: Quantized personalization via distillation with
applications to federated learning. Advances in Neural Infor-
mation Processing Systems , 34, 2021.
[36] Boxiao Pan, Haoye Cai, De-An Huang, Kuan-Hui Lee,
Adrien Gaidon, Ehsan Adeli, and Juan Carlos Niebles.
Spatio-temporal graph for video captioning with knowledge
distillation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 10870–
10879, 2020.
[37] Nikolaos Passalis, Maria Tzelepi, and Anastasios Tefas. Het-
erogeneous knowledge distillation using information flow
modeling. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 2339–
2348, 2020.
[38] Peyman Passban, Yimeng Wu, Mehdi Rezagholizadeh, and
Qun Liu. Alp-kd: Attention-based layer projection for
knowledge distillation. In Proceedings of the AAAI Confer-
ence on Artificial Intelligence , pages 13657–13665, 2021.
[39] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper-
vised representation learning with deep convolutional gen-
erative adversarial networks. arXiv preprint:1511.06434 ,
2015.
[40] Thomas Schlegl, Philipp Seeb ¨ock, Sebastian M Waldstein,
Ursula Schmidt-Erfurth, and Georg Langs. Unsupervised
anomaly detection with generative adversarial networks to
guide marker discovery. In International conference on in-
formation processing in medical imaging , pages 146–157.
Springer, 2017.
[41] Zhiqiang Shen, Zhankui He, and Xiangyang Xue. Meal:
Multi-model ensemble via adversarial learning. In Proceed-
ings of the AAAI Conference on Artificial Intelligence , pages
4886–4893, 2019.
[42] Yuan Shi and Fei Sha. Information-theoretical learning of
discriminative clusters for unsupervised domain adaptation.
InICML , 2012.
[43] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 , 2014.[44] Samuel Stanton, Pavel Izmailov, Polina Kirichenko, Alexan-
der A Alemi, and Andrew G Wilson. Does knowledge dis-
tillation really work? Advances in Neural Information Pro-
cessing Systems , 34, 2021.
[45] Florian Tram `er, Fan Zhang, Ari Juels, Michael K Reiter,
and Thomas Ristenpart. Stealing machine learning models
via prediction {APIs}. In25th USENIX security symposium
(USENIX Security 16) , pages 601–618, 2016.
[46] Dongdong Wang, Yandong Li, Liqiang Wang, and Boqing
Gong. Neural networks are more productive teachers than
human raters: Active mixup for data-efficient knowledge
distillation from a blackbox model. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 1498–1507, 2020.
[47] Haohan Wang, Xindi Wu, Zeyi Huang, and Eric P Xing.
High-frequency component helps explain the generaliza-
tion of convolutional neural networks. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8684–8694, 2020.
[48] Yunhe Wang, Chang Xu, Chao Xu, and Dacheng Tao. Adver-
sarial learning of portable student networks. In Proceedings
of the AAAI Conference on Artificial Intelligence , 2018.
[49] Zi Wang. Zero-shot knowledge distillation from a decision-
based black-box model. In International Conference on Ma-
chine Learning , pages 10675–10685. PMLR, 2021.
[50] Zheng Xu, Yen-Chang Hsu, and Jiawei Huang. Training
shallow and thin networks for acceleration via knowledge
distillation with conditional adversarial networks. arXiv
preprint:1709.00513 , 2017.
[51] Jingwen Ye, Yixin Ji, Xinchao Wang, Xin Gao, and Min-
gli Song. Data-free knowledge amalgamation via group-
stack dual-gan. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 12516–
12525, 2020.
[52] Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A
gift from knowledge distillation: Fast optimization, network
minimization and transfer learning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 4133–4141, 2017.
[53] Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun
Liang. Decoupled knowledge distillation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 11953–11962, 2022.
[54] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In Proceedings of the IEEE
International Conference on Computer Vision , pages 2223–
2232, 2017.
23157
