WorDepth: Variational Language Prior for Monocular Depth Estimation
Ziyao Zeng1Daniel Wang1Fengyu Yang1Hyoungseob Park1
Stefano Soatto2Dong Lao2Alex Wong1
1Yale University2University of California, Los Angeles
1{ziyao.zeng, daniel.wang.dhw33, fengyu.yang, hyoungseob.park, alex.wong }@yale.edu
2{soatto,lao }@cs.ucla.edu
Abstract
Three-dimensional (3D) reconstruction from a single im-
age is an ill-posed problem with inherent ambiguities, i.e.
scale. Predicting a 3D scene from text description(s) is
similarly ill-posed, i.e. spatial arrangements of objects de-
scribed. We investigate the question of whether two inher-
ently ambiguous modalities can be used in conjunction to
produce metric-scaled reconstructions. To test this, we fo-
cus on monocular depth estimation, the problem of predict-
ing a dense depth map from a single image, but with an
additional text caption describing the scene. To this end,
we begin by encoding the text caption as a mean and stan-
dard deviation; using a variational framework, we learn the
distribution of the plausible metric reconstructions of 3D
scenes corresponding to the text captions as a prior. To
“select” a speciﬁc reconstruction or depth map, we encode
the given image through a conditional sampler that samples
from the latent space of the variational text encoder, which
is then decoded to the output depth map. Our approach is
trained alternatingly between the text and image branches:
in one optimization step, we predict the mean and standard
deviation from the text description and sample from a stan-
dard Gaussian, and in the other, we sample using a (im-
age) conditional sampler. Once trained, we directly predict
depth from the encoded text using the conditional sampler.
We demonstrate our approach on indoor (NYUv2) and out-
door (KITTI) scenarios, where we show that language can
consistently improve performance in both. Code: https:
//github.com/Adonis-galaxy/WorDepth .
1. Introduction
The process of imaging is a surjection from a 3D scene
to the 2D image domain, where inﬁnitely many 3D scenes
can map to the same image. Its inverse problem, estimating
the 3D scene structure from a single image, i.e., monocular
depth estimation, is therefore ill-posed with inherent ambi-
Metric-scale depth mapInfinitely many 3D scenes
Infinitely many 3D scenes Figure 1. Language as a prior for depth estimation . Depth
estimation from a single image is an ill-posed problem (i.e., scale),
and likewise from text captions (i.e., layout). Can two inherently
ambiguous modalities resolve metric-scaled depth estimates?
guity, such as the scale of the reconstruction. Consequently,
induction is necessary, and depth estimation becomes draw-
ing a scene with maximum likelihood from the distribu-
tion of all possible scenes, conditioned on the image. This
conditional scene distribution is learned by a deep neural
network on a chosen training set. While an ideal training
set should accurately reﬂect this distribution, practical chal-
lenges arise due to the scarcity of well-established large-
scale depth datasets. A crucial question arises: Can any
priors, other than the training set, be leveraged to calibrate
the learned scene distribution to true real-world statistics?
These priors may come in many forms, from generic pri-
ors such as local smoothness and connectivity [ 19,22,66,
98] or object orientation [ 15] that may be imposed as a part
of the training objective (regularizer) to speciﬁc inductive
biases realized as architectural designs (layers) [ 65] or a col-
lection object shapes [ 14]. While generic priors are suitable
for a wide variety of scenes, they typically lack speciﬁcity,
i.e., size or shape of objects within a speciﬁc 3D scene. On
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
9708
the other hand, speciﬁc network designs may backﬁre when
the assumption motivating the design does not hold, i.e.,
using speciﬁcs about camera parameters for reconstruction.
We consider a more ﬂexible source of priors – language –
that is closely tied to semantics, and often shape (and func-
tionality) [ 4,31,32]. Consider a text description of “A bed-
room with a bed and a table” as in Fig. 1: One can imagine
a probable 3D scene containing a bed and a table as the pri-
mary objects. In fact, there exist inﬁnitely many 3D scenes
compatible with the description, as there are ambiguities in
terms of the scene layout and the precise shape of the bed
and table. Yet, one may surmise that the scale of the scene
is closely related to the objects (and their typical sizes) pop-
ulating it. This lends to a prior that is speciﬁc for a given
scene, yet, generic enough without assumptions on the cam-
era used or the shapes within the imaged 3D scene.
Hence, the question at hand becomes whether two inher-
ently ambiguous modalities (camera image and text descrip-
tions) can be exploited for their complementary strengths:
In the image, one can observe the layout and object shapes
populating the 3D scene; in a text caption, one has strong
priors about the scale (and coarse shapes) of the scene. Our
work aims to resolve the respective ambiguities of the two
modalities by using language to reduce the solution space
to yield metric-scaled reconstructions as 2.5D depth maps.
To test the feasibility of this approach, we consider the
ill-posed inverse problem of monocular depth estimation,
where one predicts a depth map from a single image. In-
stead of using just an image, we also assume a text descrip-
tion or caption describing the 3D scene captured within the
image. Note that we do not make any assumption regard-
ing the source of the description, i.e., it can be dictated by
humans or generated by a model. But for practicality, we
use an image captioner (ExpansionNet v2 [ 25]) to generate
a brief, concise description of the image.
To exploit the inherent ambiguity of text captions, where
a single description can generate inﬁnitely many 3D scenes,
we choose to encode the caption using a variational auto-
encoder (V AE) as a mean and standard deviation of the
plausible scene layout distribution. By sampling a noise
vector from a standard Gaussian and using the reparame-
terization trick customary in V AEs, we can draw from the
latent distribution and decode it into a metric-scaled depth
map. Yet, to choose a particular depth map amongst the
many possible, one must rely on the image. This is facili-
tated by a conditional sampler that predicts the noise vector
from the given image in place of the one sampled from a
Gaussian to be used in the reparameterization step. Con-
sequently, this substitution enables one to sample the most
probable depth map, adhering to the scene arrangement and
object shapes observed in the image, from the learned dis-
tribution. This naturally lends to an alternating optimization
process between the (text-)V AE and conditional sampler.In one alternation, one would predict the mean and stan-
dard deviation from the text caption and optimize the text-
V AE branch for depth by minimizing a loss with respect
to ground truth on the depth map sampled using a standard
Gaussian (similar to traditional V AEs). In the other alter-
nation, one would still use the mean and standard deviation
predicted by the text-V AE, but instead, use the conditional
sampler to “select” a speciﬁc depth map compatible with
the image, and again, minimize a loss on the output depth.
Note: that depending on the alternation, either the text-V AE
or the conditional sampler is frozen. At test-time, one no
longer needs to sample from the Gaussian and may directly
predict depth using the text-V AE with the conditional sam-
pler (see Fig. 2). In another mode, one may use the text-
V AE alone to generate plausible scenes for a given caption.
Our contributions are as follows: (i) We propose a vari-
ational framework that leverages complementary strengths
of two inherently ambiguous modalities for monocular
depth estimation; we term our approach, WorDepth. (ii) We
introduce an image-based conditional sampler that models
the use of language as a conditional prior. (iii) We achieve
the state-of-the-art on indoor (NYU Depth V2 [ 58]) and
outdoor (KITTI [ 20]) benchmarks. (iv) To the best of our
knowledge, we are the ﬁrst to treat language as a variational
prior for monocular depth estimation.
2. Related Work
Monocular depth estimation trains by minimizing loss
between depth predictions and ground-truth depth maps
[2,7,17,35,46,52,54,61,77,79,83,85]. Speciﬁcally,
DORN [ 16] employs a spacing-increasing discretization
strategy for depth estimation as an ordinal regression prob-
lem. AdaBins [ 2] introduces a transformer block that seg-
ments the depth range into adaptive bins. ASTransformer
[7] incorporates an Attention-based Up-sample Block to
enhance detailed texture features. DepthFormer [ 40] em-
ploys hierarchical aggregation and heterogeneous interac-
tion modules for effective feature afﬁnity and modeling.
RPSF [ 47] presents a differentiable model of the aper-
ture mask. However, deriving semantics solely from vi-
sual cues is challenging because of scale ambiguity and the
limited size of fully annotated training datasets. We use
language as a prior to ground predictions to metric scale.
When ground-truth depth is not available, self-supervised
approaches [ 3,15,27,36,51,62–64,69,84,91,92,96] rely
on geometric constraints, often established via from various
modalities, including lidar [ 44,50,66–68,71,78] and radar
[59], or through deliberate design. Arising from training,
if done at a large scale, is a prior on the scene that can be
exploited for semantic tasks [ 33]. On the other hand, we
consider language as a semantic prior to enhance the effec-
tiveness of monocular depth estimation.
Variational and generative methods focus on the am-
9709
biguous nature of monocular depth estimation, many in-
volving Diffusion or V AE models for modeling this ambi-
guity [ 5,10,41,56,57,72,82]. DepthGen [ 56] uses a depth
pre-trained diffusion model, which generates depth estima-
tions conditioned on images, and shows that the model is
capable of generating multiple plausible depth maps when
depth is ambiguous. DDVM [ 57] uses a similar approach
and designed a training pipeline that can produce both depth
maps and optical ﬂow outputs with a diffusion model. [ 72]
trained a V AE model that outputs a probability distribution
over scene depth given an image, which can then be com-
bined with additional inputs for more accurate depth esti-
mations. VDN [ 10] models depth as a distribution with its
variance interpreted as uncertainty. The CodeSLAM model
[5] also employed a V AE conditioned on image intensities
for depth estimation. However, although these work ex-
plored the idea of uncertainty in depth estimation, and even
combined other modalities of inputs [ 72], none have ex-
perimented with language priors, and most V AE-based ap-
proaches use images to obtain the mean of the modeled dis-
tribution, which is fundamentally different from WorDepth.
Foundation models [6,21,23,37,38,48,49,53,76,94,
100] acquire a comprehensive understanding of languages,
images, and other data types through pre-training under
substantial and diverse datasets, thus forming an effective
baseline for downstream tasks [ 2,8,12,39,42,70,73–
75,80,81,88]. To leverage foundation models for monoc-
ular depth estimation, TADP [ 30] uses captions created
by AI to enhance the correlation between text and images
in diffusion-based vision models. VPD [ 93] leverages a
diffusion-based pipeline with cross-attention between text
and images. Dinov2 [ 48] trains a ViT [ 11] with 1B pa-
rameters using an automatically built image dataset under
contrastive learning objectives. Unlike methods that rely on
foundation models for feature extraction, WorDepth is po-
tentially more efﬁcient for industrial applications.
Vision-language models are designed to build connec-
tions between visual and language inputs. CLIP [ 53] con-
ducts contrastive learning between text-image pairs, em-
powering various tasks like few-shot image classiﬁcation
[18,86,87,97], image segmentation [ 55,95], object de-
tection [ 55,99], and 3D perception [ 26,89,90,101]. In
light of the powerful emerging ability brought by recent
vision-language models, some works have tried to apply
the vision-language model for monocular depth estimation.
DepthCLIP [ 90] leverages the semantic depth response of
CLIP [ 53] with a depth projection scheme to conduct zero-
shot adaptation from the semantic language response to
monocular depth estimation. Furthermore, [ 26] extends
DepthCLIP with learnable prompts and depth codebook to
narrow the depth domain gap among different scenes. Like-
wise, [ 1] modiﬁes DepthCLIP [ 90] using continuous learn-
able tokens in place of discrete human-language words. Ad-ditionally, VPD [ 93] exploits the high-ﬁdelity embedding of
a pre-trained text-to-image diffusion model in monocular
depth estimation. However, existing methods using vision-
language models rely on implicit modeling. Conversely,
WorDepth explicitly models language as a prior for depth
estimation and exploits strong priors regarding the size of
objects described in text captions to better ground monocu-
lar depth (often scaleless) to metric scale.
3. Method
Given an RGB image x:⌦⇢R2!R3, monocu-
lar depth estimation aims to infer a dense depth map y:
⌦⇢R2!R+using a parameterized function hrealized
as a neural network, i.e., y:=h(·). We consider a super-
vised dataset D={x(m),t(m),y⇤(m)}M
m=1with Msam-
ples, where y⇤:⌦⇢R2!R+denotes the ground-truth
depth map, and tthe text caption describing the image.
3.1. Text variational auto-encoder
To incorporate language priors to monocular depth es-
timation, we ﬁrst design a variational auto-encoder (V AE)
to learn the latent distribution of possible depth maps as
described by the text caption. This V AE is comprised of
the text encoder from a pre-trained vision-language model,
CLIP [ 53], which by default offers a shared latent space
between vision and text embeddings, followed by a multi-
layer perceptron (MLP) to estimate the mean ˆµ2Rdand
standard deviation ˆ 2Rdof the latent distribution of
plausible scenes based on the text encoding. Note that the
CLIP text encoder is frozen at all times and never updated
when training WorDepth. Speciﬁcally, given a text cap-
tiont={t1,t2,. . .}, we ﬁrst encode it using the CLIP text
encoder and estimate the mean and standard deviation as
(ˆµ,ˆ )= g (t)2R2⇥dusing a multi-layer perceptron
(MLP). To sample from the distribution parameterized by
ˆµandˆ , we ﬁrst draw a noise vector ✏2Rdfrom a stan-
dard Gaussian ✏⇠N(0,1). Then, we use ✏to sample from
the latent distribution via the reparameterization trick [ 29],
ˆz=ˆµ+✏·ˆ . We refer to this module as a text variational
auto-encoder (text-V AE). To generate a depth map ˆyfrom
the sample ˆz2Rd, we ﬁrst duplicate ˆzalong the horizontal
and vertical axes to yield a d⇥h⇥wlatent (choice of de-
sign to be discussed below in Sec. 3.2) and feed it through
a depth decoder to yield ˆy=h (ˆz)2RH⇥W
+ , where we
overload ˆzas the spatially duplicated latent, and HandW
denote the height and width of the depth map, preset as hy-
perparameters to match the desired image dimensions.
To train our text-V AE and depth decoder, we minimize
LV AE=LSI(y⇤,ˆy)+↵·LKL(ˆµ,ˆ ) (1)
with respect to  and , where LSIis the scale invariant loss
(Eq. ( 3)),LKLthe KL divergence loss (Eq. ( 4)) as detailed
in Section 3.3, and ↵the weight of the KL divergence term.
9710
TextDescriptionConditional Sampler̃"∈$!×#×$̃%=̂(+̃"*+,Predicted Depth
Ground TruthPixel-wise depth losstext-VAÊ(,+,	∈$! × $! Depth DecoderPrior Depth"∼N(0,1)“A living room with a couch and a ceiling fan.”
DetachedgradientKL-Divergencewith N(0,1)Depth Decoder
RGBImagê%=̂(+"*+,SharedweightsAlternating witha ratiop
Figure 2. Training WorDepth. We begin with optimizing text-V AE by predicting the mean and standard deviation of the latent distribution
of depth maps corresponding to a text caption. We then sample ˆzfrom the distribution using the reparameterization trick with ✏⇠N(0,1)
and decode it into a depth map for loss computation. We then optimize a conditional sampler by predicting patch-wise ˜✏from an image to
sample ˜zfrom the latent to yield output depth for the loss computation. The depth decoder is updated in both alternating steps.
3.2. Image-based conditional sampler
While our text-V AE can predict plausible metric-scaled
depth maps from text captions, we are interested in the
depth map corresponding to a speciﬁc image. To do so, we
treat text-V AE as the latent prior distribution of the plausi-
ble scene layouts. Predicting depth ˜yfor a speciﬁc image
xrequires sampling the latent corresponding to the depth
map of the 3D scene layout with the highest likelihood to be
compatible with the observed image, i.e., prior conditioned
on the image. To this end, we introduce an image-based
conditional sampler that will predict the sample ˜✏in place
of✏⇠N(0,1)drawn from the standard Gaussian. Using
the reparameterization trick as before, we will use ˜✏to select
the latent vector ˜zto be decoded by the depth decoder.
Speciﬁcally, our image-based conditional sampler uti-
lizes a Swin-L transformer backbone to encode an image
x2R3⇥H⇥W. We chose this design to exploit the local-
ity of the tokens produced by Swin-L. The tokens are then
encoded into h⇥wnumber of local samples ˜✏2Rd⇥h⇥w
to be used to sample from the latent distribution of our text-
V AE; in other words, we perform “patch-wise” selection
from latent distribution for more granular predictions. To
do so, we additionally include ˆµandˆ as part of its input.
We note that ˆµandˆ have been detached from the computa-
tional graph and treated as input. We refer to this module as
our conditional sampler ˜✏=f'(x,ˆµ,ˆ ), which aims to es-
timate the most probable latent variable of text-V AE. Thus,
the scene layout latent vector is now given by ˜z=ˆµ+˜✏·ˆ ,
and the predicted depth ˜y=h (˜z). As an implementation
detail, we note that skip connections from the encoder f'
are injected into h by concatenation; when training text-
V AE (Sec. 3.1), feature maps of skip connections are of the
same size, but populated with zeros instead.
To train the conditional sampler, we minimize the sameloss (Eq. ( 1)) as that of text-V AE:
LCS=LSI(y⇤,˜y)+ ·LKL(˜µ,˜ ) (2)
with respect to 'and . With a batch size of b, the number
of˜✏isb⇥h⇥w, while ˜µand˜ are the sample mean and
standard deviation of ˜✏over a batch. We impose a KL di-
vergence loss as regularization so that the estimated ˜✏does
not drift from the standard Gaussian, which also serves to
improve training stability.
3.3. Training Loss
Scale invariant loss. We minimize a supervised loss us-
ing ground truth y⇤. To improve training stability over di-
verse scenes, we use the scale-invariant depth loss [ 13]:
LSI(y,y⇤)=1
NeX
(i,j)2⌦e(i, j)2  
N2e(X
(i,j)2⌦e(i, j))2,
(3)
where e(i, j) = log y(i, j) logy⇤(i, j),⌦denotes the im-
age space, Nethe number of pixels, ythe predicted depth,
and the scaling factor to control the sensitivity of the loss.
Kullback-Leibler (KL) divergence loss. Following
[29], we employ the KL Divergence loss as a regularizer,
which biases the predicted latent distribution (parameter-
ized by mean µand standard deviation  ) towards a stan-
dard Gaussian distribution. We apply the Kullback-Leibler
divergence loss to µand as follows:
LKL(µ,  )= log( )+ 2+µ2
2 1
2. (4)
3.4. Optimizing Wordepth
Training Wordepth involves optimizing text-V AE with
our conditional sampler: One may choose to ﬁrst train
text-V AE until convergence (i.e., optimize for  ⇤, ⇤), then
9711
ImageTextOursOurs ErrorAdabinsAdabins ErrorGround Truth
A man standing in a kitchen next to a counter.
DepthValue (m)
Error(Abs Rel)
A bathroom with a toilet, a sink and a shower curtain.
A classroom with a group of desks.
An unmade bed in a bedroom with a window.
A store with chairs and tables in it.
Figure 3. Qualitative results on NYU Depth V2. We compare WorDepth with AdaBins [ 2]. Text descriptions are generated using
ExpansionNet v2 [ 25]. Overall, WorDepth improves uniformly across the image (darker in error map), implying better scale. WorDepth
also predicts more accurate depth in regions corresponding to “chairs”, “window”, “shower curtain”, “man”, and “desks”, which are objects
speciﬁed by text descriptions. Note: Zeros in the ground truth depth map indicate the absence of valid depth values.
freeze  ⇤, ⇤, and ﬁnally train the image-based conditional
sample (i.e., optimize for '⇤). However, we ﬁnd that do-
ing so often results in the conditional sampler being trapped
in a suboptimal local minimum. Moreover, this introduces
the inconvenience of an extra stage of training. Instead, we
propose an alternating optimization scheme to train the text-
V AE with conditional sampler. In one alternating step, we
freeze the conditional sampler and train the text-V AE and
depth decoder following the procedure in Sec. 3.1, i.e., pre-
dicting ˆµandˆ from text caption tand using the reparam-
eterization trick with an ✏drawn from a standard Gaussian
to sample the latent vector. In the next alternating step, we
freeze text-V AE and train the conditional sampler with the
depth decoder following Sec. 3.2, i.e., predicting ˆµandˆ 
using the frozen text-V AE and sample from the latent dis-
tribution using ˜✏predicted from the image. These alternat-
ing steps are repeated with a ratio of p(for optimizing text-
V AE) to 1 p(for optimizing the conditional sampler).
Inference. Once trained, we no longer require drawing
✏from a standard Gaussian. Instead, at test time, the infer-
ence step simply follows Sec. 3.2. In another mode, if one
wants to generate depth maps from text captions, one can
discard the conditional sampler branch and directly sample
from a standard Gaussian instead.4. Experiments
Datasets. We evaluate our method on indoor (NYU
Depth V2 [ 58]) and outdoor (KITTI [ 20]) scenarios. NYU
Depth V2 contains 480 ⇥640 images with depth values from
1⇥10 3to 10 meters. We follow [ 34] for the dataset par-
tition, which contains 24,231 train images and 654 test im-
ages. KITTI contains 352 ⇥1216 images where depth values
from 1⇥10 3to 80 meters. We adopt the Eigen Split [ 13]
consisting of 23,488 training images and 697 testing im-
ages. Following [ 2,85], after cleaning out samples without
valid ground truth, we have 652 valid images for testing.
Network Architecture. We use the ResNet-50 [ 24] ver-
sion of CLIP [ 53] text encoder to extract text features. We
use ExpansionNet-v2 [ 25] for captioning images for efﬁ-
ciency. We set the dimension dof the latent space of the
text-V AE and image-based conditional sampler to be 128.
As for the image-based conditional sampler, we use a Swin-
L Transformer backbone [ 45] pre-trained on ImageNet [ 9].
For the text-V AE, given CLIP features of size 1024, we use
a 3-layer MLP with hidden dimensions of 512, 256, and 128
to encode text features. For the depth decoder, there are 3
convolutional up-sampling and reﬁnement layers. For depth
prediction, we attach 3 skip connections from the condi-
tional sampler to the depth decoder between corresponding
9712
MethodBackbone <1.25" <1.252" <1.253"Abs Rel#log10#RMSE#DepthCLIP [90]CLIP (zero-shot)0.394 0.683 0.8510.388 0.156 1.167CLIPMDE [1]CLIP0.465 0.776 0.9220.319 0.139 0.970GeoNet [52]ResNet-500.834 0.960 0.9900.128 0.057 0.569DORN [16]ResNet-1010.828 0.965 0.9920.115 0.051 0.509Yin et al. [79]ResNeXt-1010.875 0.976 0.9940.108 0.048 0.416TransDepth [77]ViT-B0.900 0.983 0.9960.106 0.045 0.365ASN [46]HRNet-480.890 0.982 0.9960.101 0.044 0.377Big to Small [35]DenseNet-1610.885 0.978 0.9940.110 0.047 0.392DPT-Hybird [54]ViT-B0.904 0.9880.9980.110 0.045 0.357ASTransformer [7]ViT-B0.902 0.985 0.9970.103 0.044 0.374AdaBins [2]EffNet-B5 + ViT-mini0.903 0.984 0.9970.103 0.044 0.364NeWCRFs [85]Swin-L0.9220.992 0.9980.095 0.041 0.331Yu et al. [83]Swin-L0.921 0.9900.9980.093 0.040 0.331DepthFormer [40]Swin-L0.923 0.989 0.9970.094 0.040 0.329BaselineSwin-L0.910 0.9900.9980.098 0.043 0.351WorDepthSwin-L0.932 0.992 0.9980.088 0.038 0.317%Improvement-+2.42% +0.02% +0.00%-10.20% -11.63% -9.69%Table 1. Quantitative results on NYU Depth V2. The baseline method is to directly train a Swin-L image encoder and the depth decoder
without the help of language prior. Improvement refers to the performance enhancement relative to the Baseline.
layers. When optimizing for text-V AE by our alternating
optimization scheme (Sec. 3.4), we sample ✏⇠N (0,1)
from a standard Gaussian; as an implementation detail, all
values passed from the skip connections are set to be zero.
Hyperparameters. We use the Adam [ 28] optimizer
without weight decay. The learning rate is reduced from
3⇥10 5to1⇥10 5by a cosine learning rate sched-
uler. The model is trained for 50 epochs for both KITTI
and NYU Depth V2 under this scheduler.  for scale-
invariant loss is set to 0.85, and the weights ↵and for KL-
Divergence are set to 1⇥10 3. We set the probability pto
optimizing text-V AE branch to 1%. Data augmentation in-
cludes random gamma within [0.9,1.1], random brightness
within [0.75,1.25]for NYU Depth V2 [ 58] and [0.9,1.1]
for KITTI [ 20], random color intensity within [0.9,1.1]for
each channel, random horizontal ﬂipping with 50% proba-
bility, and random rotations within [ 2.5,2.5]degrees.
Evaluation metrics. Following [ 7,43], we evaluate
WorDepth and baseline methods quantitatively using mean
absolute relative error (Abs Rel), root mean square error
(RMSE), absolute error in log space (log10), logarithmic
root mean square error (RMSE log) and threshold accuracy
( i). The evaluation metrics are summarized in the Supp.
Mat. For qualitative results and comparisons, see Fig. 3and
4, where the error map shows the absolute relative error.
Quantitative results. We show results on NYU Depth
V2 in Tab. 1, where we improve over the baseline and
existing works across all evaluation metrics. We want to
highlight that WorDepth signiﬁcantly excels in terms of the
threshold accuracy  <1.25, which measures the propor-
tion of predictions deviating from the ground truth within a
speciﬁc range. We note that while existing methods oftenproduce high ﬁdelity shapes (i.e., ordinal relationships of
points) in depth maps, the scale tends to be off – leading to
lower  <1.25. Our gain in the  <1.25metric indicates
that a greater proportion of depth estimations align closely
with the ground truth, thanks to better scale estimated based
on objects that populate the scene, thereby yielding depth
values in ranges closer to that of ground truth.
Tab. 2shows the results on the KITTI dataset, using the
Eigen Split [ 13] partition. WorDepth also achieves state-
of-the-art performance. Like NYU Depth V2, WorDepth
improves the threshold accuracy  <1.25, however, the rel-
ative performance gain on this metric is not as pronounced
as on NYU Depth V2. This difference can be due to the
wider range of object sizes and shapes that may populate
an outdoor scene that are attributed to the same equivalence
class of an object. For example, the term “car” may refer
to a sedan, a coupe, or a hatchback – all exhibit different
sizes (coupes are smaller than sedans) and shapes (hatch-
backs have an elevated and connect trunk). While text cap-
tions give ﬂexibility between generality and speciﬁcity as a
prior, in cases where captions tend to be vague, the explicit
reliance (by modeling as a conditional prior) on them may
backﬁre, leading to incorrect shapes and sizes. Nonetheless,
conditioning on the image resolve such cases to a degree
and usage of the prior leads to more beneﬁts than harm.
Qualitative comparisons. We show representative vi-
sual examples comparing WorDepth with a baseline method
AdaBins [ 2] on the NYU Depth V2 dataset in Fig. 3, to
highlight the beneﬁt of the language prior. From the error
map where brighter regions indicate larger errors, it is evi-
dent that WorDepth predicts more accurate depth for objects
mentioned in the text description, like “chairs and tables”
9713
An empty street with trees and a wall.TextImage
AdaBinsOurs
Ground TruthA red truck is driving down a road with trees.A group of cars parked in front of a large building.OursErrorAdaBinsError
DepthValue (m)
Error(Abs Rel)
Figure 4. Visualization of depth estimations on KITTI. Compared with AdaBins [ 2], WorDepth improves uniformly across the image
(darker in error map), implying better scale. WorDepth also predicts more accurate depth in regions corresponding to “wall”, “trees”,
“building”, which are objects speciﬁed by text descriptions. Note: Zeros in ground truth depth indicate the absence of valid depth values.
in the ﬁrst row, “a window” in the second row, “a shower
curtain” in the third row, “a man” in the fourth row, and
“a group of desks” in the last row. Note that errors maps
of WorDepth shows improvement uniformly across the im-
age regions; this implies that our method estimates a better
scale than existing ones, thanks to priors about object size
(and coarse shapes) from text captions. Knowing that a cer-
tain object exists within an image reduces the problem to
“placing” the object in the 3D scene based on its shape and
location in the image. We showed that scale can be inferred
from language, which can narrow down the solution space
of depth prediction, leading to improved accuracy.
A similar pattern is also evident in KITTI (Fig. 4). Ex-
amples include improved accuracy for “a wall” shown in the
ﬁrst column, “trees” in the second column, and “a group of
cars” alongside “a large building” in the last column. This
observation is intriguing because, for example, the text “a
wall” is ambiguous by itself, especially in outdoor scenes,
where the wall could be any size or distance away from the
camera, 1 or 100 meters. However, the text description of
a scene, either from a human annotator or a deep neural
network, inherently carries biases that emphasize “a wall”
when its size (tall or wide enough) or depth falls within a
speciﬁc range while ignoring it when it falls within anotherrange. The resulting prior embedded in the text description
may convey more scale information than initially apparent.
Optimizing with different alternation ratios. As a
sensitivity study, we investigate how different ratios of alter-
nating optimization steps between text-V AE and conditional
sampler have an effect on the performance of WorDepth.
We ﬁnd that optimizing text-V AE with a lower ratio will
lead to a more deterministic model, which is anticipated.
On the other hand, optimizing text-V AE more frequently
enables the model to learn a better variational prior on the
depth maps corresponding to text captions, which facilitates
the generation of diverse prior depth maps. However, this
comes at the cost of training time as the conditional sam-
pler takes more steps to converge and, given a ﬁxed num-
ber of steps, results in more blurry predictions. We iden-
tify the ratio at 1% in updating text-V AE to be the best em-
pirically (Tab. 3). Ratios exceeding 10% notably degrades
performance given a ﬁxed training length because of fewer
updates to the sampler. Note that at 100%, where we do
not condition the image, caption to depth generation still
yields reasonable results as the text captions produce plausi-
ble statistics that match the ground truth depth. On the other
hand, without the modeling language as a variational prior
(at 0%, where we train both text-V AE and conditional opti-
9714
MethodBackbone <1.25" <1.252" <1.253"Abs Rel#RMSElog#RMSE#CLIPMDE [1]CLIP0.550 0.830 0.9380.303 0.119 6.322DORN [16]ResNet-1010.932 0.984 0.9950.072 0.120 2.727Yin et al. [79]ResNeXt-1010.938 0.990 0.9980.072 0.117 3.258TransDepth [77]ViT-B0.956 0.9940.9990.064 0.098 2.755Big to Small [35]DenseNet-1610.955 0.993 0.9980.060 0.096 2.798DPT-Hybird [54]ViT-B0.959 0.9950.9990.062 0.092 2.573ASTransformer [7]ViT-B0.963 0.9950.9990.058 0.089 2.685AdaBins [2]EffNet-B5+ViT-mini0.964 0.9950.9990.058 0.089 2.360NeWCRFs [85]Swin-L0.974 0.9970.9990.052 0.079 2.129Yu et al. [83]Swin-L0.972 0.9960.9990.054 0.081 2.134DepthFormer [40]Swin-L0.975 0.9970.9990.052 0.079 2.143BaselineSwin-L0.969 0.9960.9990.054 0.085 2.343WorDepthSwin-L0.979 0.998 0.9990.049 0.074 2.039% Improvement-+1.03% +0.20% +0.00%-9.26% -12.94% -12.97%Table 2. Quantitative results on KITTI Eigen Split. The baseline method is to directly train a Swin-L image encoder and the depth
decoder without the help of language prior. Improvement is the relative performance gain compared with the Baseline.
p  <1.25" <1.252" <1.253"AbsRel #log10#RMSE #
0% 0.929 0.990 0.998 0.091 0.039 0.323
1% 0.932 0.992 0.998 0.088 0.038 0.317
10% 0.930 0.991 0.998 0.090 0.039 0.322
50% 0.763 0.942 0.987 0.163 0.068 0.527
90% 0.642 0.906 0.975 0.211 0.089 0.687
100% 0.590 0.889 0.973 0.225 0.097 0.746
Table 3. Sensitivity to different ratios of alternating optimiza-
tion steps between text-VAE and conditional sampler on NYU
Depth V2. pdenotes probability of optimizing text-V AE. While
more steps spent on text-V AE will yield better generative results,
it comes at the cost of slower convergence for the sampler.
mizer jointly as a direct map from single image and caption
to depth), performance degrade to do the lack of the prior.
Zero-shot Generalization. Given the smaller domain
gap in language descriptions across different scenes com-
pared to images, we conduct a zero-shot transfer experi-
ment to highlight our improved generalization ability. We
train the model on the NYU Depth V2 [ 58] and test it
on the Sun-RGBD [ 60] without ﬁne-tuning. As shown in
Tab. 4, WorDepth outperforms baseline methods by a sub-
stantial margin, indicating the transferability of language
priors which underscores the robustness of text descriptions
in handling scene variability. This suggests that language
descriptions may offer a more stable basis for generaliza-
tion across diverse data domains than direct visual signals.
5. Discussion
In this study, we seek to answer the question of whether
language can be used to calibrate the learned scene distribu-
tion to true real-world statistics. The answer is yes, which
is valuable for circumventing the long-standing problem
of scale ambiguity in monocular depth or structure-from-Method  <1.25" <1.252" <1.253"AbsRel #log10#RMSE #
Adabins 0.771 0.944 0.983 0.159 0.068 0.476
DepthFormer 0.815 0.970 0.993 0.137 0.059 0.408
Baseline 0.803 0.965 0.990 0.141 0.062 0.427
WorDepth 0.833 0.976 0.994 0.123 0.054 0.376
Table 4. Zero-shot generalization to SUN-RGBD. The models
are trained on the NYU Depth V2 and testing on the Sun-RGBD
without any ﬁne-tuning.
motion problems. The approach is a ﬁrst in leveraging com-
plementary properties of two modalities with inherent am-
biguities for the 3D reconstruction, to address the deﬁcits in
one another. We show that by exploiting the layout/scene
ambiguity in language as a strength via our variational ap-
proach, we can ground predictions to metric scale. This
opens up new avenue in how one can address the issue of
scale in 3D reconstruction as well as provide a direct frame-
work to extending the many works that currently are limited
to relative or scaleless depth predictions.
Limitations. Generic regularizers typically yield little
gains, but do little harm; speciﬁc regularizers can provide
larger boosts but are limited in their applications. While
using language as a prior gives ﬂexibility between the two,
speciﬁcity in the caption controls the degree of regulariza-
tion imposed. Naturally, vague captions give little to no
information on object shape or size, so there is little to be
gained; speciﬁc, but incorrect captions may misﬁre, barring
any malicious intent. As WorDepth relies on the quality of
the caption, it is susceptible to inaccuracies stemming from
descriptions provided by the image captioner. Its ease of
use also opens up vulnerabilities from malicious users who
may choose captions to steer predictions incorrectly.
Acknowledgments. We thank Byung-Woo Hong and
Yangchao Wu for their insightful discussions and valuable
feedback. This work was supported by NSF 2112562.
9715
References
[1]Dylan Auty and Krystian Mikolajczyk. Learning to prompt
clip for monocular depth estimation: Exploring the limits of
human language. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 2039–2047,
2023. 3,6,8
[2]Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.
Adabins: Depth estimation using adaptive bins. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 4009–4018, 2021. 2,3,5,6,7,
8,1
[3]Jia-Wang Bian, Huangying Zhan, Naiyan Wang, Zhichao
Li, Le Zhang, Chunhua Shen, Ming-Ming Cheng, and Ian
Reid. Unsupervised scale-consistent depth learning from
video. International Journal of Computer Vision , 129(9):
2548–2564, 2021. 2
[4]Irving Biederman and Ginny Ju. Surface versus edge-based
determinants of visual recognition. Cognitive psychology ,
20(1):38–64, 1988. 2
[5]Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan
Leutenegger, and Andrew J Davison. Codeslam—learning
a compact, optimisable representation for dense visual
slam. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 2560–2568, 2018. 3
[6]Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´eJ´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin.
Emerging properties in self-supervised vision transformers.
InProceedings of the IEEE/CVF international conference
on computer vision , pages 9650–9660, 2021. 3
[7]Wenjie Chang, Yueyi Zhang, and Zhiwei Xiong.
Transformer-based monocular depth estimation with
attention supervision. In 32nd British Machine Vision
Conference (BMVC 2021) , 2021. 2,6,8,1
[8]Jiaben Chen, Renrui Zhang, Dongze Lian, Jiaqi Yang,
Ziyao Zeng, and Jianbo Shi. iquery: Instruments as queries
for audio-visual sound separation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 14675–14686, 2023. 3
[9]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical im-
age database. In 2009 IEEE conference on computer vision
and pattern recognition , pages 248–255. Ieee, 2009. 5
[10] Georgi Dikov and Joris van Vugt. Variational depth net-
works: Uncertainty-aware monocular self-supervised depth
estimation. In European Conference on Computer Vision ,
pages 43–60. Springer, 2022. 3
[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold,
Sylvain Gelly, et al. An image is worth 16x16 words:
Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 3
[12] Yiming Dou, Fengyu Yang, Yi Liu, Antonio Loquercio, and
Andrew Owens. Tactile-augmented radiance ﬁelds. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , 2024. 3[13] David Eigen, Christian Puhrsch, and Rob Fergus. Depth
map prediction from a single image using a multi-scale
deep network. Advances in neural information processing
systems , 27, 2014. 4,5,6,1
[14] Xiaohan Fei and Stefano Soatto. Visual-inertial object de-
tection and mapping. In Proceedings of the European con-
ference on computer vision (ECCV) , pages 301–317, 2018.
1
[15] Xiaohan Fei, Alex Wong, and Stefano Soatto. Geo-
supervised visual depth prediction. IEEE Robotics and Au-
tomation Letters , 4(2):1661–1668, 2019. 1,2
[16] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-
manghelich, and Dacheng Tao. Deep ordinal regression
network for monocular depth estimation. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 2002–2011, 2018. 2,6,8
[17] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-
manghelich, and Dacheng Tao. Deep ordinal regression
network for monocular depth estimation. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 2002–2011, 2018. 2
[18] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao
Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao.
Clip-adapter: Better vision-language models with feature
adapters. arXiv preprint arXiv:2110.04544 , 2021. 3
[19] Ravi Garg, Vijay Kumar Bg, Gustavo Carneiro, and Ian
Reid. Unsupervised cnn for single view depth estimation:
Geometry to the rescue. In Computer Vision–ECCV 2016:
14th European Conference, Amsterdam, The Netherlands,
October 11-14, 2016, Proceedings, Part VIII 14 , pages
740–756. Springer, 2016. 1
[20] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In 2012 IEEE conference on computer vision and
pattern recognition , pages 3354–3361. IEEE, 2012. 2,5,6,
1
[21] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat
Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan
Misra. Imagebind: One embedding space to bind them all.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 15180–15190, 2023.
3
[22] Cl´ement Godard, Oisin Mac Aodha, and Gabriel J Bros-
tow. Unsupervised monocular depth estimation with left-
right consistency. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 270–279,
2017. 1
[23] Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xi-
anzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi
Li, Hongsheng Li, et al. Point-bind & point-llm: Align-
ing point cloud with multi-modality for 3d understand-
ing, generation, and instruction following. arXiv preprint
arXiv:2309.00615 , 2023. 3
[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 5
9716
[25] Jia Cheng Hu, Roberto Cavicchioli, and Alessandro Capo-
tondi. Expansionnet v2: Block static expansion in fast
end to end training for image captioning. arXiv preprint
arXiv:2208.06551 , 2022. 2,5
[26] Xueting Hu, Ce Zhang, Yi Zhang, Bowen Hai, Ke Yu, and
Zhihai He. Learning to adapt clip for few-shot monocular
depth estimation. arXiv preprint arXiv:2311.01034 , 2023.
3
[27] Pan Ji, Runze Li, Bir Bhanu, and Yi Xu. Monoindoor: To-
wards good practice of self-supervised monocular depth es-
timation for indoor environments. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 12787–12796, 2021. 2
[28] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 6
[29] Diederik P Kingma and Max Welling. Auto-encoding vari-
ational bayes. arXiv preprint arXiv:1312.6114 , 2013. 3,
4
[30] Neehar Kondapaneni, Markus Marks, Manuel Knott,
Rog´erio Guimar ˜aes, and Pietro Perona. Text-image align-
ment for diffusion-based perception. arXiv preprint
arXiv:2310.00031 , 2023. 3
[31] Barbara Landau, Linda B Smith, and Susan S Jones. The
importance of shape in early lexical learning. Cognitive
development , 3(3):299–321, 1988. 2
[32] Barbara Landau, Linda Smith, and Susan Jones. Object
shape, object function, and object name. Journal of memory
and language , 38(1):1–27, 1998. 2
[33] Dong Lao, Fengyu Yang, Daniel Wang, Hyoungseob Park,
Samuel Lu, Alex Wong, and Stefano Soatto. On the viabil-
ity of monocular depth pre-training for semantic segmenta-
tion. arXiv preprint arXiv:2203.13987 , 2022. 2
[34] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and
Il Hong Suh. From big to small: Multi-scale local planar
guidance for monocular depth estimation. arXiv preprint
arXiv:1907.10326 , 2019. 5
[35] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and
Il Hong Suh. From big to small: Multi-scale local planar
guidance for monocular depth estimation. arXiv preprint
arXiv:1907.10326 , 2019. 2,6,8
[36] Boying Li, Yuan Huang, Zeyu Liu, Danping Zou, and
Wenxian Yu. Structdepth: Leveraging the structural reg-
ularities for self-supervised indoor depth estimation. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 12663–12673, 2021. 2
[37] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for uni-
ﬁed vision-language understanding and generation. In
International Conference on Machine Learning , pages
12888–12900. PMLR, 2022. 3
[38] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. arXiv
preprint arXiv:2301.12597 , 2023. 3
[39] Yong-Lu Li, Xiaoqian Wu, Xinpeng Liu, Yiming Dou,
Yikun Ji, Junyi Zhang, Yixing Li, Jingru Tan, Xudong Lu,and Cewu Lu. From isolated islands to pangea: Unify-
ing semantic space for human action understanding. arXiv
preprint arXiv:2304.00553 , 2023. 3
[40] Zhenyu Li, Zehui Chen, Xianming Liu, and Junjun Jiang.
Depthformer: Exploiting long-range correlation and local
information for accurate monocular depth estimation. arXiv
preprint arXiv:2203.14211 , 2022. 2,6,8
[41] Yiqing Liang, Numair Khan, Zhengqin Li, Thu Nguyen-
Phuoc, Douglas Lanman, James Tompkin, and Lei Xiao.
Gaufre: Gaussian deformation ﬁelds for real-time dynamic
novel view synthesis, 2023. 3
[42] Yiqing Liang, Eliot Laidlaw, Alexander Meyerowitz, Sri-
nath Sridhar, and James Tompkin. Semantic attention ﬂow
ﬁelds for monocular dynamic scene decomposition. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) , 2023. 3
[43] Ce Liu, Suryansh Kumar, Shuhang Gu, Radu Timofte,
and Luc Van Gool. Va-depthnet: A variational ap-
proach to single image depth prediction. arXiv preprint
arXiv:2302.06556 , 2023. 6,1
[44] Tian Yu Liu, Parth Agrawal, Allison Chen, Byung-Woo
Hong, and Alex Wong. Monitored distillation for positive
congruent depth completion. In European Conference on
Computer Vision , pages 35–53. Springer, 2022. 2
[45] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 10012–10022, 2021. 5
[46] Xiaoxiao Long, Cheng Lin, Lingjie Liu, Wei Li, Christian
Theobalt, Ruigang Yang, and Wenping Wang. Adaptive
surface normal constraint for depth estimation. In Proceed-
ings of the IEEE/CVF international conference on com-
puter vision , pages 12849–12858, 2021. 2,6
[47] Mazen Mel, Muhammad Siddiqui, and Pietro Zanut-
tigh. End-to-end learning for joint depth and image
reconstruction from diffracted rotation. arXiv preprint
arXiv:2204.07076 , 2022. 2
[48] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy
Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.
Dinov2: Learning robust visual features without supervi-
sion. arXiv preprint arXiv:2304.07193 , 2023. 3
[49] Zixuan Pan, Zihao Wei, and Andrew Owens. Efﬁcient
vision-language pre-training by cluster masking. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , 2024. 3
[50] Hyoungseob Park, Anjali Gupta, and Alex Wong. Test-
time adaptation for depth completion. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2024. 2
[51] Rui Peng, Ronggang Wang, Yawen Lai, Luyang Tang, and
Yangang Cai. Excavating the potential capacity of self-
supervised monocular depth estimation. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 15560–15569, 2021. 2
[52] Xiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasun,
and Jiaya Jia. Geonet: Geometric neural network for joint
9717
depth and surface normal estimation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition , pages 283–291, 2018. 2,6
[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In International conference on machine learning ,
pages 8748–8763. PMLR, 2021. 3,5
[54] Ren´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-
sion transformers for dense prediction. In Proceedings of
the IEEE/CVF international conference on computer vi-
sion, pages 12179–12188, 2021. 2,6,8
[55] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yan-
song Tang, Zheng Zhu, Guan Huang, Jie Zhou, and
Jiwen Lu. Denseclip: Language-guided dense pre-
diction with context-aware prompting. arXiv preprint
arXiv:2112.01518 , 2021. 3
[56] Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, and
David J Fleet. Monocular depth estimation using diffusion
models. arXiv preprint arXiv:2302.14816 , 2023. 3
[57] Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek
Kar, Mohammad Norouzi, Deqing Sun, and David J Fleet.
The surprising effectiveness of diffusion models for optical
ﬂow and monocular depth estimation. Advances in Neural
Information Processing Systems , 36, 2024. 3
[58] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
Fergus. Indoor segmentation and support inference from
rgbd images. In Computer Vision–ECCV 2012: 12th Euro-
pean Conference on Computer Vision, Florence, Italy, Oc-
tober 7-13, 2012, Proceedings, Part V 12 , pages 746–760.
Springer, 2012. 2,5,6,8,1
[59] Akash Deep Singh, Yunhao Ba, Ankur Sarker, Howard
Zhang, Achuta Kadambi, Stefano Soatto, Mani Srivastava,
and Alex Wong. Depth estimation from camera image
and mmwave radar point cloud. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 9275–9285, 2023. 2
[60] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao.
Sun rgb-d: A rgb-d scene understanding benchmark suite.
InProceedings of the IEEE conference on computer vision
and pattern recognition , pages 567–576, 2015. 8
[61] Rishi Upadhyay, Howard Zhang, Yunhao Ba, Ethan Yang,
Blake Gella, Sicheng Jiang, Alex Wong, and Achuta
Kadambi. Enhancing diffusion models with 3d perspec-
tive geometry constraints. ACM Transactions on Graphics
(TOG) , 42(6):1–15, 2023. 2
[62] Ruoyu Wang, Zehao Yu, and Shenghua Gao. Planedepth:
Self-supervised depth estimation via orthogonal planes. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 21425–21434, 2023.
2
[63] Youhong Wang, Yunji Liang, Hao Xu, Shaohui Jiao, and
Hongkai Yu. Sqldepth: Generalizable self-supervised ﬁne-
structured monocular depth estimation. arXiv preprint
arXiv:2309.00526 , 2023.
[64] Alex Wong and Stefano Soatto. Bilateral cyclic constraint
and adaptive regularization for unsupervised monoculardepth prediction. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages
5644–5653, 2019. 2
[65] Alex Wong and Stefano Soatto. Unsupervised depth com-
pletion with calibrated backprojection layers. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 12747–12756, 2021. 1
[66] Alex Wong, Xiaohan Fei, Stephanie Tsuei, and Stefano
Soatto. Unsupervised depth completion from visual iner-
tial odometry. IEEE Robotics and Automation Letters ,5
(2):1899–1906, 2020. 1,2
[67] Alex Wong, Safa Cicek, and Stefano Soatto. Learning
topology from synthetic data for unsupervised depth com-
pletion. IEEE Robotics and Automation Letters , 6(2):1495–
1502, 2021.
[68] Alex Wong, Xiaohan Fei, Byung-Woo Hong, and Stefano
Soatto. An adaptive framework for learning unsupervised
depth completion. IEEE Robotics and Automation Letters ,
6(2):3120–3127, 2021. 2
[69] Cho-Ying Wu, Jialiang Wang, Michael Hall, Ulrich Neu-
mann, and Shuochen Su. Toward practical monocular in-
door depth estimation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 3814–3824, 2022. 2
[70] Shaokai Wu and Fengyu Yang. Boosting detection in crowd
analysis via underutilized output features. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 15609–15618, 2023. 3
[71] Yangchao Wu, Tian Yu Liu, Hyoungseob Park, Stefano
Soatto, Dong Lao, and Alex Wong. Augundo: Scaling up
augmentations for unsupervised depth completion. arXiv
preprint arXiv:2310.09739 , 2023. 2
[72] Zhihao Xia, Patrick Sullivan, and Ayan Chakrabarti. Gen-
erating and exploiting probabilistic monocular depth esti-
mates. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 65–74,
2020. 3
[73] Fengyu Yang and Chenyang Ma. Sparse and complete la-
tent organization for geospatial semantic segmentation. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 1809–1818, 2022. 3
[74] Fengyu Yang, Chenyang Ma, Jiacheng Zhang, Jing Zhu,
Wenzhen Yuan, and Andrew Owens. Touch and go: Learn-
ing from human-collected vision and touch. Neural In-
formation Processing Systems (NeurIPS) - Datasets and
Benchmarks Track , 2022.
[75] Fengyu Yang, Jiacheng Zhang, and Andrew Owens. Gen-
erating visual scenes from touch. International Conference
on Computer Vision (ICCV) , 2023. 3
[76] Fengyu Yang, Chao Feng, Ziyang Chen, Hyoungseob Park,
Daniel Wang, Yiming Dou, Ziyao Zeng, Xien Chen, Rit
Gangopadhyay, Andrew Owens, and Alex Wong. Bind-
ing touch to everything: Learning uniﬁed multimodal tac-
tile representations. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , 2024.
3
[77] Guanglei Yang, Hao Tang, Mingli Ding, Nicu Sebe, and
Elisa Ricci. Transformer-based attention networks for
9718
continuous pixel-wise prediction. In Proceedings of the
IEEE/CVF International Conference on Computer vision ,
pages 16269–16279, 2021. 2,6,8
[78] Yanchao Yang, Alex Wong, and Stefano Soatto. Dense
depth posterior (ddp) from single image and sparse range.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 3353–3362, 2019. 2
[79] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. En-
forcing geometric constraints of virtual normal for depth
prediction. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 5684–5693, 2019.
2,6,8
[80] Chenyu You, Weicheng Dai, Fenglin Liu, Yifei Min, Hao-
ran Su, Xiaoran Zhang, Xiaoxiao Li, David A Clifton,
Lawrence Staib, and James S Duncan. Mine your own
anatomy: Revisiting medical image segmentation with ex-
tremely limited labels. arXiv preprint arXiv:2209.13476 ,
2022. 3
[81] Chenyu You, Weicheng Dai, Yifei Min, Lawrence Staib,
and James S Duncan. Implicit anatomical rendering for
medical image segmentation with stochastic experts. In In-
ternational Conference on Medical Image Computing and
Computer-Assisted Intervention , pages 561–571. Springer,
2023. 3
[82] Chenyu You, Weicheng Dai, Yifei Min, Fenglin Liu, David
Clifton, S Kevin Zhou, Lawrence Staib, and James Dun-
can. Rethinking semi-supervised medical image segmenta-
tion: A variance-reduction perspective. Advances in Neural
Information Processing Systems , 36, 2024. 3
[83] Shangbin Yu, Renyan Zhang, Shuaiye Ma, and Xinfang
Jiang. Monocular depth estimation network based on swin
transformer. In Journal of Physics: Conference Series , page
012019. IOP Publishing, 2023. 2,6,8
[84] Zehao Yu, Lei Jin, and Shenghua Gao. P 2 net: Patch-match
and plane-regularization for unsupervised indoor depth es-
timation. In European Conference on Computer Vision ,
pages 206–222. Springer, 2020. 2
[85] Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and
Ping Tan. Neural window fully-connected crfs for monocu-
lar depth estimation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
3916–3925, 2022. 2,5,6,8
[86] Renrui Zhang, Rongyao Fang, Peng Gao, Wei Zhang,
Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.
Tip-adapter: Training-free clip-adapter for better vision-
language modeling. arXiv preprint arXiv:2111.03930 ,
2021. 3
[87] Renrui Zhang, Longtian Qiu, Wei Zhang, and Ziyao Zeng.
Vt-clip: Enhancing vision-language models with visual-
guided texts. arXiv preprint arXiv:2112.02399 , 2021. 3
[88] Renrui Zhang, Ziyao Zeng, Ziyu Guo, Xinben Gao, Kexue
Fu, and Jianbo Shi. Dspoint: Dual-scale point cloud
recognition with high-frequency fusion. arXiv preprint
arXiv:2111.10332 , 2021. 3
[89] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xu-
peng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng
Li. Pointclip: Point cloud understanding by clip. In Pro-ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 8552–8562, 2022. 3
[90] Renrui Zhang, Ziyao Zeng, Ziyu Guo, and Yafeng Li.
Can language understand depth? In Proceedings of the
30th ACM International Conference on Multimedia , pages
6868–6874, 2022. 3,6
[91] Chaoqiang Zhao, Youmin Zhang, Matteo Poggi, Fabio Tosi,
Xianda Guo, Zheng Zhu, Guan Huang, Yang Tang, and
Stefano Mattoccia. Monovit: Self-supervised monocular
depth estimation with a vision transformer. In 2022 Inter-
national Conference on 3D Vision (3DV) , pages 668–678.
IEEE, 2022. 2
[92] Wang Zhao, Shaohui Liu, Yezhi Shu, and Yong-Jin Liu. To-
wards better generalization: Joint depth-pose learning with-
out posenet. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 9151–
9161, 2020. 2
[93] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu,
Jie Zhou, and Jiwen Lu. Unleashing text-to-image dif-
fusion models for visual perception. arXiv preprint
arXiv:2303.02153 , 2023. 3
[94] Chenhao Zheng, Jieyu Zhang, Aniruddha Kembhavi, and
Ranjay Krishna. Iterated learning improves composition-
ality in large vision-language models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2024. 3
[95] Chong Zhou, Chen Change Loy, and Bo Dai. Dense-
clip: Extract free dense labels from clip. arXiv preprint
arXiv:2112.01071 , 2021. 3
[96] Junsheng Zhou, Yuwang Wang, Kaihuai Qin, and Wenjun
Zeng. Moving indoor: Unsupervised video depth learn-
ing in challenging environments. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 8618–8627, 2019. 2
[97] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Zi-
wei Liu. Learning to prompt for vision-language models.
arXiv preprint arXiv:2109.01134 , 2021. 3
[98] Tinghui Zhou, Matthew Brown, Noah Snavely, and
David G Lowe. Unsupervised learning of depth and ego-
motion from video. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 1851–
1858, 2017. 1
[99] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Phillip
Kr¨ahenb ¨uhl, and Ishan Misra. Detecting twenty-thousand
classes using image-level supervision. arXiv preprint
arXiv:2201.02605 , 2022. 3
[100] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui,
HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang,
Zongwei Li, et al. Languagebind: Extending video-
language pretraining to n-modality by language-based se-
mantic alignment. arXiv preprint arXiv:2310.01852 , 2023.
3
[101] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao
Zeng, Zipeng Qin, Shanghang Zhang, and Peng Gao. Point-
clip v2: Prompting clip and gpt for powerful 3d open-world
learning. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 2639–2650, 2023.
3
9719
