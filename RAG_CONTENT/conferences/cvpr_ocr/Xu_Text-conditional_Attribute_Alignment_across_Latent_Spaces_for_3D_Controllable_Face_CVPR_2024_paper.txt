Text-conditional Attribute Alignment across Latent Spaces for 3D Controllable
Face Image Synthesis
Feifan Xu1, Rui Li2*, Si Wu1,3,4*, Yong Xu1,3,4and Hau San Wong5
1School of Computer Science and Engineering, South China University of Technology
2Department of Computer Science, Shantou University
3Peng Cheng Laboratory4PAZHOU LAB
5Department of Computer Science, City University of Hong Kong
csfeifan@mail.scut.edu.cn, ruili@stu.edu.cn, {cswusi, yxu}@scut.edu.cn, cshswong@cityu.edu.hk
+Melancholic +Hospitable +Displeased +Neutral +Disgust  +Angry +Happy
Lighting Control
Pose Control0 Lighting Control
Pose Control 0
Figure 1. The examples to demonstrate that the proposed method that can explicitly control face expression, pose, and illumination in a
fine-grained manner. The manipulated images are consistent with the 3D rendered images shown in the last row.
Abstract
With the advent of generative models and vision-
language pretraining, significant improvement has been
made in text-driven face manipulation. The text embedding
can be used as target supervision for expression control.
However, it is non-trivial to associate with its 3D attributes,
i.e., pose and illumination. To address these issues, we pro-
pose a Text-conditional Attribute aLignment approach for
3D controllable face image synthesis, and our model is re-
ferred to as TcALign. Specifically, since the 3D rendered
image can be precisely controlled with the 3D face repre-
*Co-corresponding author.sentation, we first propose a Text-conditional 3D Editor to
produce the target face representation to realize text-driven
manipulation in the 3D space. An attribute embedding
space spanned by the target-related attributes embeddings
is also introduced to infer the disentangled task-specific di-
rection. Next, we train a cross-modal latent mapping net-
work conditioned on the derived difference of 3D represen-
tation to infer a correct vector in the latent space of Style-
GAN. This correction vector learning design can accurately
transfer the attribute manipulation on 3D images to 2D im-
ages. We show that the proposed method delivers more pre-
cise text-driven multi-attribute manipulation for 3D con-
trollable face image synthesis. Extensive qualitative and
quantitative experiments verify the effectiveness and supe-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
9172
riority of our method over the other competing methods.
1. Introduction
The automatic manipulation of face characteristics e.g. ex-
pression, pose, and illumination has a wide range of appli-
cations in movie production and or game design. Among
various techniques, style-based generative models [22, 23]
learn a disentangled latent space (W ) and synthesize high-
fidelity images. Therefore, instead of training a generator
from scratch, many works [2, 39,40,49] explore the latent
space to discover semantic latent paths for the manipulation
of different attributes. However, existing methods typically
involve a well-trained attribute classifier or a large number
of annotated data for supervision, which are cumbersome
and only enable the manipulation of predefined attributes.
Recently, with the advent of the Contrastive Language-
Image Pre-training (CLIP) model [34], some works [30,
32,51] leveraged a semantic consistency constraint based
on CLIP to realize text-driven image manipulation, which
is more flexible and user-friendly. However, these meth-
ods, e.g. StyleCLIP [32] are mainly designed for appear-
ance control with a text prompt, which is not sufficient to
achieve fine-grained control, particularly on 3D physical at-
tributes. On the other hand, there are some works targeted
for 3D controllable face synthesis [10, 11,42]. They often
learn or construct a global 3D face representation as condi-
tion to synthesize a 2D face, so that the desired control can
be achieved by editing the 3D representation. However, the
quality of generated faces is often sub-optimal due to the
difficulty in directly mapping 2D and 3D spaces. There is
also no text-driven control which limits their applicability.
In this paper, we propose TcALign which incorporates
3D priors [4, 12] based on StyleCLIP to enable text- and
3D-control, simultaneously. The key is to ensure that the
synthesized image is consistent with its 3D face represen-
tation manipulated by a text prompt in all physical charac-
teristics as shown in Figure 1(last row). However, there
involve three different modalities, i.e., text, 2D, and 3D.
Learning precise attribute alignment among them within
one model is challenging, and we observe that naively us-
ing a target text embedding to infer the corresponding 3D or
latent transformation often results in inaccurate face control
or non-target attribute change.
To address the above issues, we first learn a text-
conditional 3D Editor to enable 3D editing with textual
prompts (shown in Figure 2) in 3D space. To precisely build
relationships between text and 3D modalities for accurate
control, we introduce an Attribute Embedding Space (AES)
to infer the disentangled target direction for 3D image ma-
nipulation, and the text-conditional 3D image embedding
can reflect the target attributes which will be used to pro-
duce the corresponding 3D face representation with 3D Ed-itor, thus enables text-conditional synthesis for 3D images.
To achieve high-quality 2D face synthesis, we further adopt
a cross-modal latent mapping network based on the pre-
vious 3D transformation information to infer a correction
vector within the Wspace. The final target image is gener-
ated from the latent code of the source image added by the
correction vector via the pretrained StyleGAN. Compared
to the global latent code prediction, this correction vector
learning design leads to more precise multi-attribute ma-
nipulation (Figure 1). We have performed both qualitative
and quantitative experiments to verify the effectiveness and
superiority of our method over recent competing methods.
To summarize, our contributions include the followings:
• The proposed approach enables both 3D-aware and text-
driven face control on expressions, poses, and illumina-
tions, which is important in many real-world face synthe-
sis applications.
• We incorporate AES in 3D Editor to achieve accurate 3D
image control with text prompts via the discovery of dis-
entangled manipulation direction.
• We propose a cross-modal latent mapping network con-
ditioned on the difference of 3D representation to infer
a correction vector within the latent space of StyleGAN,
which transfers the precise attribute rendering on 3D im-
ages to 2D images.
2. Related work
Generic face editing. Previous methods heavily adopted
Conditional-GAN [31] for face editing. Pix2Pix [20] is
one of the pioneering works for image translation which
requires paired data. To improve the data efficiency, cycle-
consistency regularization is proposed in CycleGAN [56]
which realizes unpaired image translation. Afterwards,
many works have been proposed for more diverse types
of translation, e.g., MUNIT [19] aims for multi-modal
translation by introducing content- and style-encoder; Star-
GAN [6] enables multi-domain translation with a single
generator, etc. Except for image translation, the genera-
tor can be conditioned on some predefined factors [16, 43]
for controllable editing. Recently, some studies used text
as the condition to guide face synthesis for flexible con-
trol. A pioneering work [37] trained a CGAN with the text
embedding from a language model. Meanwhile, various
techniques were proposed to improve the generation qual-
ity, such as the stacked architecture [52], visual-content dis-
entanglement [29], cross-modality fusion [27], contrastive
learning [53], etc. However, these methods trained the gen-
erator from scratch on a specific dataset which is less ef-
fective for real-world applications. As a result, some recent
works adopted a pretrained generator for high-quality and
generalizable synthesis [24, 51].
Generative prior-based face manipulation. The la-
tent spaces of a pretrained StyleGAN [22, 23] demon-
9173
strate promising disentangled properties. Therefore, many
works [1, 7,39,40,44,49] used StyleGAN as the backbone
and investigated its latent spaces for performing a wide va-
riety of image manipulations. One direction is to train a
network to discover the corresponding path in the latent
spaces for manipulating a desired attribute [15, 40,49]. An-
other direction is to encode a reference image into the latent
spaces of StyleGAN to realize exemplar-based manipula-
tion [1, 3,38,46]. Meanwhile, with the advent of CLIP [34]
with the semantic visual and textual embeddings, there
are increasing number of works combining StyleGAN and
CLIP for image manipulation with text descriptions [32].
For example, TediGAN [50] aligns the text and image em-
beddings in the latent spaces of StyleGAN for generation.
StyleCLIP [32] proposed a linguistic-visual semantic loss
based on CLIP to guide image synthesis with text input.
DeltaEdit [30] focused on improving the distribution align-
ment between the textual and visual embedding by identify-
ing a delta image and text space. For multiple-attribute ma-
nipulation, StyleFlow [2] used a conditional flow model to
learn sequential transformations in which each transforma-
tion corresponds to one attribute edit, while TUSLT [47] in-
troduced an auxiliary attribute classifier together with CLIP
to manipulate multiple attributes with a single pass. Despite
the significant progress, it is still difficult to enable precise
3D physical control, and undesired manipulation may be in-
curred due to the large gap between 3D and text spaces.
3D-aware face generation. Our method is related to
3D-aware manipulation methods which incorporate 3D pri-
ors for face generation. Shi et al. [41] directly disentan-
gled the latent space of StyleGAN into 3D components i.e.,
texture, shape, viewpoint, lighting. Kim et al. [25] lever-
aged 3DMM to generate faces with the 3D coefficients es-
timated from input images. DiscoFaceGAN [10] trained a
generator to imitate the rendered 3D faces via 3DMM. Dif-
fusionRig [11] used DECA [13] to extract 3D coefficients
from a single image which serve as the condition for a dif-
fusion model [18], these coefficients can be easily edited to
achieve the desired 3D control. However, most 3D-aware
face generation methods do not support text-driven manip-
ulation. In contrast, our method is the first to use an AES
to infer the disentangled direction for a 3D image, and pro-
pose a 3D Editor conditioned on the manipulated embed-
ding to realize text manipulation in 3D space. Next, the
resulting 3D difference information is used as the condition
for a cross-modal latent mapper, which enables 3D-aware
manipulations in the latent space. This framework allows
3D controllable face synthesis with text, and the manipu-
lated image is consistent with its 3D face representation.
3. Proposed Method
Given a 2D face image, our goal is to precisely control
its expression, pose, and illumination with text descrip-tions. Recent works explored the Wspace of StyleGAN
for high-quality generation, and leveraged CLIP linguistic-
visual semantic embeddings for supervisions. We further
consider incorporating 3D priors which provide a reference
3D space for fine-grained face control. However, discrep-
ancies among different modalities often result in incorrect
controls or undesired artifacts. Therefore, improving align-
ments among different spaces is essential for enhancing the
performance of fine-grained text-driven face manipulation.
Toward this end, we propose a two-stage training
pipeline. First, since a 3D face image has less variations and
can be easily edited with its 3D representation, we propose
a Text-conditional 3D Editor (Γ) to infer the target 3D face
representation θtarget based on the target text tand origi-
nal 3D face representation θ0, which realizes text manipu-
lation in 3D space. Second, to transfer the manipulation in
3D space to the latent space for high-quality 2D face image
synthesis, a cross-modal latent mapping network (Φ) condi-
tioned on the 3D difference information ∆θ=θtarget−θ0
aims to predict the correction vector ∆win the Wspace
of StyleGAN (G sty). Compared with global latent code
prediction, two residual transformations are easily aligned
for cross-modal mapping. As a result, text-driven 3D con-
trollable face image synthesis can be accurately performed.
The overall framework is shown in Figure 2.
3.1. 3D Face Representation
In this work, we incorporate 3D priors for fine-grained
face attribute control. Based on [9], 3D priors include the
3DMM, illumination model, and camera model.
Specifically, in 3DMM, the Basel Face Model [33] is
used for face shape and texture, and face expression is
based on [14]. Therefore, the coefficients in 3DMM in-
clude α∈R80,δ∈R80andβ∈R64for identity, tex-
ture and expression control, respectively. The illumina-
tion model is based on Spherical Harmonics (SH) [35, 36],
where γ∈R27is used for illumination control. The coeffi-
cients p∈R6for the perspective camera model control the
face poses, including pitch and yaw rotations.
Therefore, the 3D face representation is θ=
(α, δ, β, γ, p )∈R257, which can be used to render a 3D im-
age with differentiable mathematical operations Rvia the
3DMM database. The rendering process shown in Figure 2
includes θ=Rinv(x)andx′=R(θ).Rinvis a pretrained
3D Predictor [9], which can be viewed as an inverse render-
ingprocess. x′denotes the rendered 3D image.
3.2. Text-conditional 3D Editing
We consider using the rendered 3D images for CLIP image
embedding extraction which enhances 3D physical control,
and the corresponding embedding without irrelevant back-
ground is more disentangled for text control. Given the
original x′
0and a target attribute text t,Γaims to produce
9174
Input 𝑥0Encoder
Synthesis𝑤0
𝜃𝑡𝑎𝑟𝑔𝑒𝑡𝜃0Δ𝜃
Latent MapperΔ𝑤—
“a disgusted 10 
degree left face 
with front lighting”3D EditorGenerator3D 
PredictorGenerator
Element -wise AdditionElement -wise Subtraction Rendering Process Full ConnectionRendering Process 
𝜃
3DMM DatabaseModelerCLIP Image Encoder
CLIP Text EncoderAES, AImage Region
Image 
Embedding
Text Embedding Target Embedding
Inversion3D
Rendered
Image
Text
Language
Channel -wise ConcatenationCC
3D  Predictor
Image 3D
Rendered 
Image3D Editor Architecture
＋
C
＋—
… … …
𝑤𝑡𝑎𝑟𝑔𝑒𝑡𝑟Figure 2. Overview of TcALign which consists of two learnable modules: The Text-conditional 3D Editor infers the target 3D face repre-
sentation to realize text-driven control on 3D images. The Cross-modal Latent Mapping network predicts the latent transformation based
on 3D difference information to achieve 3D controllable face synthesis. Encoder (e4e), Generator (StyleGAN), 3D Predictor (Deep3DFR),
CLIP Encoders are fixed during training.
the corresponding target 3D face representation θtarget . In
this case, the text can precisely describe the manipulations
in 3D space.
In Figure 2(3D Editor module), CLIP contains an im-
age encoder Eimgand a text encoder Etxt, which map x′
0
andtinto a shared 512-d space, i.e.,ex′
0=Eimg(x′
0)and
et=Etxt(t). Although CLIP demonstrates strong seman-
tic representations, recent studies [30, 54] observed that the
corresponding image embedding and text embedding are
not well aligned owing to the modality gap. Therefore, in-
stead of using etas the target embedding for supervision,
we introduce an attribute embedding space (AES) Ato in-
corporate text information etby discovering the relevant
disentangled transformation for ex′
0. Specifically, we first
project ex′
0intoA, which is spanned with a set of basis at-
tribute embeddings related to t, and the residual vector rof
the projection is recalled as follows:
ep′
x=PA(ex′
0) r=ex′
0−ep′
x, (1)
wherePAis the projection operation on A. Then, ep′
xis in
the text embedding space which can be directly manipulated
withet, and the manipulated embedding is finally projected
back to the CLIP image embedding space by adding ras
shown in Figure 2. The final text-manipulated 3D image
embedding is:
etx′=M(e p′
x, et) +r, (2)
whereMis the manipulation operation guided by et, which
is realized by weakening the other irrelevant basis attributes
ofetas in [54]
In summary, since Ais constructed by basis attribute
embeddings, the projection ensures that image manipula-
tion operations are disentangled, which alleviates undesiredchanges. Therefore, etx′should respect the attributes de-
scribed by t. Toward this end, we train Γto generate θtarget
conditioned on etx′. The CLIP embedding of the 3D image
rendered by θtarget should be close to etx′. The semantic
correspondence constraint is formulated as follows:
Lcorr= 1−cos(E img(R(Γ(e tx′))), e tx′), (3)
where cos(·,·)denotes the cosine distance between two
image embeddings. The rendered target image x′
target =
R(G3d(ext))is precisely associated with θtarget = Γ(e xt).
As a result, we can use text to manipulate the 3D image in
a fine-grained manner.
3.3. Cross-modal Latent Mapping
StyleGAN enables high-quality 2D face generation, and
its latent space Wdemonstrates meaningful and disentan-
gled properties. A pretrained StyleGAN inversion Encoder
e4e [45] Estyextends to W+space for better generation
quality. In our experiment, we use e4e to derive the latent
codew0=Esty(x0)of the source image x0. Therefore, the
goal is to find a latent transformation which can respect the
target attributes and preserve the original attributes as well
in the latent space.
However, owing to the modality gap, it is challenging
to precisely infer the latent transformations for producing
physically meaningful 3D rigging with text. To address this
issue, we consider incorporating the above 3D difference in-
formation (∆θ =θtarget−θ0) into the latent mapping net-
work (Φ) to transfer the manipulation in 3D space to latent
space, where ∆θexactly represents the precise 3D rigging
for controlling the generation.
Specifically, Φis conditioned on both w0and∆θ, which
produces latent correction vector ∆w. Compared to global
9175
latent code prediction, the correction learning strategy is
input-aware and more faithful to the original image, and
can preserve the original ID and attributes better. In ad-
dition, ∆wand∆θcan be easily associated across two dif-
ferent modalities. Following [32], our Φconsists of 3 paral-
lel groups which correspond to coarse-, medium-, and fine-
level generation. The formulation is expressed as:
∆w= Φ(w 0,∆θ). (4)
As a result, the target latent code wtarget can be obtained
by adding the original latent code to the correction vector,
which will be decoded by the pretrained StyleGAN Gstyfor
generating the target image:
xtarget =Gsty(w0+ Φ(w 0,∆θ)). (5)
Due to the unique design of Φwhich is conditioned on
3D transformation to estimate ∆wfor 2D generation, we
can achieve 3D-W +alignment via the proposed 2D-3D
self-consistency constraints as follows.
3D consistency. Since xtarget represents the target text-
manipulated image, its 3D face representation should be the
same as θtarget . Therefore, we can use Rinvto inverse ren-
derxtarget , and the 3D consistency constraint is expressed
as follows:
L3d=||θtarget− R inv(xtarget )||2. (6)
By minimizing L3d,Φseeks suitable latent transformations
such that the attributes of the target image are well repre-
sented in 3D space.
2D landmark consistency. Although 3D attributes i.e.,
pose, expression, illumination are well represented with 3D
priors, 2D details associated with mouth, eyes, eyebrows,
etc. should be subtly processed. Instead of using pixel-wise
L2loss, we adopt a pretrained landmark detection model to
produce the key parts of a face in 2D space, and propose a
2D landmark consistency constraint as follows:
L2d=||R(θ target )−F(xtarget )||2, (7)
where Fis the pretrained landmark detection model [5],
and the rendering process Rcan automatically produce the
landmark with θtarget . By minimizing L2d,xtarget can pre-
serve 2D details to achieve high-quality image generation.
3.4. Model Optimization
By integrating the above 3D Editor and 3D-aware latent
mapper, the overall optimization process can be expressed
as follows:
min
ΓLcorr,
min
ΦL3d+L2d,(8)
We only train {Γ,Φ}and randomly specify the target
text for manipulation. During inference, we can use text to
manipulate face images in a fine-grained manner.4. Experiments
4.1. Experitmenal Setup
Training data. We use Flickr-Face-HQ (FFHQ) [23] dur-
ing training, which contains 70,000 1024×1024 face im-
ages crawled from Flickr. Images have no annotated at-
tributes, and we randomly sample the target text attributes
and pre-process them as in [32].
Implementation details. Our framework includes five
pretrained models: StyleGAN [22] (G sty) for generation
based on the latent codes, e4e encoder (E sty) forW+
codes extraction, 3D Predictor (R inv) for inverse render-
ing, CLIP text and image encoders (E txt,Eimg) for seman-
tic linguistic-visual embedding extraction. There are two
trainable models ΓandΦfor 3D Editing and latent correc-
tion vector prediction, respectively. We use Adam [26] to
optimize the trainable model with a learning rate of 0.5. We
observe that the resulting performance is robust.
Baseline models. We compare our model with recent
representative and state-of-the-art methods: StyleClip [32]
and DeltaEdit [30] are text-driven image manipulation mod-
els which use CLIP and improved CLIP delta similarity
to guide image editing with text semantics. DiscoFace-
GAN [10], GANControl [42] and DiffusionRig [11] are 3D-
aware face editing models which incorporate 3D priors into
GAN and Diffusion model training, respectively. We use of-
ficial implementations or source pretrained models of these
methods during evaluations.
Evaluation metrics. We use Fr ´echet Inception Dis-
tance (FID) [17] for evaluation of the image quality, which
is widely adopted in image generation tasks [48]. To as-
sess the performance of identity preservation, the IDen-
tity Distance (IDD) between original and manipulated im-
ages is computed with a well-trained face recognition net-
work [8]. We also adopt the Target Attribute Recognition
Rate (TARR) to measure the attribute correctness. Specif-
ically, we use the off-the-shelf expression classifier [28],
light spherical harmonics predictor [55] and face pose es-
timator [21] to compute similarity or distance scores for the
expression, light and pose attributes, respectively.
4.2. Ablation Study
We conduct several ablation studies to validate the effec-
tiveness of each proposed component. The qualitative and
quantitative results are presented in Figure 3and Figure 4,
respectively.
(a) w/o A.We first conduct an experiment to demon-
strate the effectiveness of text-conditional 3D Editing via
A. Specifically, we remove the alignment operations in
Afor disentangled text manipulation, and directly use the
target text embedding for supervision. As shown in Fig-
ure3(a), the resulting variant cannot control the 3D at-
tributes (i.e., a large score of TARR:Pose in Figure 4) due to
9176
(a) w/o A (b) w/o w 0 (c) w/o 3D (d) w/o Δθ TcALign 3D rendering
+ Disgust
+ Front lighting
+ -10o
+ Surprise
+ Top lighting
+ -20oFigure 3. Ablation studies of different components in our framework. (a) removes Aand directly uses the text embedding as the target
for supervision. (b) removes w0inΦ; (c) removes 3D priors and uses the pretrained networks for 3D supervision; (d) removes correction
vector learning and adopts global latent code prediction.
TARR: Expression ( ↑)TARR:Pose ( ↓) FID (↓) IDD (↓)w/o A w/o w 0 w/o 3D w/o Δθ TcALign
Figure 4. Comparison between TcALign and its variants in terms
of different metrics.
the discrepancy between different modalities. On the other
hand, our model can better infer the transformation with the
discovery of target-specific direction in Afor precise ma-
nipulation in 3D space.
(b) w/o w0.We remove w0inΦto verify the importance
of input-aware generation. As shown in Figure 3(b), there
exist some inaccurate controls on the pose and illumination
attributes and some non-target attributes are changed. Fig-
ure4also reports that the score of TARR:pose is largely
degraded. Input-aware generation indicates deriving a spe-
cific transformation for each input, thus more precise latent
correction transformation can be inferred by considering the
original information within w0.
(c) w/o 3D. We further remove the 3D priors in our
framework, and use pretrained models to realize 3D con-
trol. In Figures 3and4, we can observe that although the
TARR scores are not seriously degraded which indicates ef-
fective guidance with pretrained models, there exist obvi-
ous undesired controls and artifacts (large FID score), and
the IDD score is also increased. These results confirm the
effectiveness of 3D priors for better performance.
(d) w/o ∆θ.Instead of directly inferring the global latent
code wtarget withθtarget , our model adopts a correction
vector learning scheme which infers ∆wwith∆θ. This ex-
periment shows that global prediction across two modalities
is quite difficult, and the corresponding ∆transformations
in the two spaces can be easily aligned. As shown in Fig-
ure3(d), the variant results are significantly degraded, with
(a) Before alignment (b) After alignmentFigure 5. Visualization of CLIP image embeddings from different
modalities (a) before alignment and (b) after alignment. Different
colors denote different modalities (top) and attributes (bottom).
obvious changes on undesired attributes and ID, which is
also reported in Figure 4. This strategy enables wtarget to
be close to w0, which avoids degraded performance. As a
result, our results are more faithful to the original input and
demonstrate accurate control.
4.3. Modality Alignment Analysis
In this section, we conduct an experiment to validate the
capability of our method to align all involved modalities in
the CLIP image embedding space. Specifically, there ex-
ist three representations that correspond to the same tar-
get manipulated image in our framework, which are text-
manipulated image embedding etx′, target 3D face repre-
sentation θtarget , and target latent code wtarget . Therefore,
R(θtarget )andGsty(wtarget )should be aligned in CLIP
image embedding space together with etx′.
We randomly sample 100 face images, and manipulate
them with 3 expressions (angry, surprise, happy) to obtain
the text-manipulated image embedding etx′. Next, the cor-
responding CLIP image embedding of the generated target
9177
Method FID↓IDD↓TARR
Expression ↑Illumination ↑Pose↓
DiffusionRig [
11] 27.634 0.224 N/A 0.846 1.736
DiscoFaceGAN [10] 63.363 0.591 N/A 0.808 2.106
GANControl [42] 34.782 0.38 N/A 0.834 1.571
StyleCLIP [32] 7.595 0.124 0.728 N/A N/A
DeltaEdit [30] 7.658 0.119 0.547 N/A N/A
TcALign (Ours) 7.104 0.107 0.913 0.859 0.714
Table 1. Quantitative comparison of TcALign and competing
methods on the image quality, ID preservation and attribute cor-
rectness.
images extarget and the rendered target images eR(θ target )
are produced. As shown in Figure 5, we use PCA to project
all embeddings from three modalities into a 2D space. It
can be observed that before alignment, there exists a large
modality gap among different spaces, and embeddings with
different expressions are randomly scattered. On the other
hand, the three embedding spaces are well aligned in our
model after training, and three attributes are clearly clus-
tered. These observations suggest that TcALign can align
these three modalities and achieve accurate disentangled
image manipulation in both 3D and latent spaces.
4.4. Visualization Analysis
In this section, we present visualization results of our
method and competing methods. Figure 6shows the ma-
nipulated images with different expressions, illuminations
and poses from our model and competing methods. It is
clear that our results exhibit higher quality with the multi-
ple attributes precisely controlled. Meanwhile, Discoface-
GAN without pretrained generator is likely to change the
face ID, and GANControl cannot precisely control the ex-
pressions. Two representative text-driven methods Style-
CLIP and DeltaEdit fail to manipulate 3D attributes, i.e.,
pose and illumination. DiffusionRig [11] shows promis-
ing 3D-aware control. However, it has no text control for
expressions and requires carefully personalized finetuning
with around 20 images. In Figure 6, the corresponding gen-
eration quality is degraded with few finetuning samples. In
contrast, TcALign achieves high-quality generation without
further finetuning and precise control without changing the
other attributes. The poses and illuminations are also con-
sistent with the rendered 3D images.
4.5. Quantitative Comparison
We further perform the quantitative comparison between
our method and competing methods. We randomly sam-
ple 3,000-ID real face images and manipulate each of them
with 5 different attributes. All the metrics are computed un-
der the same setting for fair comparison.
Table 1shows the quantitative comparison results in
terms of FID, IDD and TARR between TcALign and com-peting methods. First, TcALign can significantly outper-
form previous 3D-aware generation methods (i.e., Diffu-
sionRig, DiscoFaceGAN and GANControl) in terms of FID
which are generally conditioned on the global 3D represen-
tation. In particular, DiscoFaceGAN delivers an unsatisfac-
tory FID score of 63.363, while the FID of our model is
significantly improved to 7.104 which implies high-quality
generation. In addition, DiffusionRig and GANControl
demonstrate improved performance on the manipulations
of pose and illumination with the adoption of a pretrained
generator. On the other hand, TcALign obtains much better
performance in terms of TARR than both of them, which
verifies its capability of performing fine-grained 3D control
with the correction vector learning scheme.
Second, we compare our approach with the SOTA
text-driven image manipulation methods (StyleCLIP and
DeltaEdit). Both methods are based on StyleGAN which
does not enable 3D control on poses and illuminations.
StyleCLIP shows better performance on expression con-
trols and DeltaEdit achieves better IDD score. TcAlign is
the only model that can manipulate all these attributes si-
multaneously, and achieves the significant improvement on
TARR:Expression score of 0.913 with the disentangled text
manipulation.
In addition, the ID preservation analysis results are pre-
sented in Figure 7. We can observe that DiscoFaceGAN
fails in preserving the original ID (higher IDD score) in
most cases in the absence of the pretrained generator, while
our model is the most powerful in preserving ID under dif-
ferent manipulation factors. This further verifies the effec-
tiveness of our 3D controllable synthesis which maintains
the original information as well.
4.6. Further Analysis
In this section, we further analyse our model by manipu-
lating images with infrequent open-vocabulary expressions.
The results are shown in Figure 8where StyleCLIP often
fails, and some infrequent expressions i.e. ‘claustrophobic’
are not understandable for DeltaEdit. In contrast, TcALign
can accurately understand these open-vocabulary expres-
sions via the disentangled text manipulation in 3D space.
We also present the generation results based on interpo-
lation with respect to θtarget . As shown in Figure 9, Dif-
fusionRig cannot precisely control the expressions and illu-
minations, and there exist generation artifacts. Meanwhile,
the attributes of our results are smoothly transferred with
high-quality generation, which demonstrates that TcAlign
successfully achieves fine-grained control in a 3D setting.
5. Conclusion
Text-conditional face manipulation is an important research
topic which allows more flexible controls of face attributes.
Due to the complex variations of 2D images and the domain
9178
+ Angry
+ Top lighting
+ 30o
+ Sad
+ Front lighting
+ 10o
+ Happy
+ Right lighting
+ -30oInput DiscofaceGAN DiffusionRig StyleCLIP DeltaEdit GANControl TcALign 3D renderingFigure 6. Multiple-attribute (including expression, lighting and pose) manipulation results of TcALign and competing methods.
Figure 7. ID preservation analysis with different attribute variations. 0 indicates no manipulation with the original ID.
Input
+ claustrophobic 
+ incredulous DeltaEdit
 StyleCLIP
 TcALign
 3D rendering
Figure 8. Comparison of our model and competing methods in
manipulating infrequent open-vocabulary expressions.
gap between texts and images, it is very challenging to in-
fer the disentangled manipulation direction to avoid incur-
ring undesired artifacts. We present a new Text-conditional
attribute alignment method for fine-grained face manipula-
tion, which is a new task and not addressed by previous
methods. Specifically, we use a 3D rendered image which
can be precisely manipulated with a 3D face representation,
and propose a 3D Editor to achieve text manipulation in 3D
space. We demonstrate that by leveraging the 3D difference
information as conditions, our cross-modal latent mapping
network can generate disentangled and controllable latent
transformations in the latent space of StyleGAN, which en-
ables 3D control with text. Extensive experiments demon-
strate the superiority of our model and its capability in per-
forming precise text-driven face manipulation, which is im-
portant for a broad range of real-world applications.
Happy: 0
Pose: -20o
Left lighting: 0Happy: 1.2
Pose: +20o
Left lighting: 1.2
TcALign DiffusionRig 3D RenderingFigure 9. Multi-attribute manipulation of our model with interpo-
lation on θtarget . Each row shares the same face, and the corre-
sponding θtarget is linearly interpolated between 0 and 1.2.
6. Acknowledgments
This work was supported in part by the National Natural
Science Foundation of China (Project No. 62072189), in
part by the Research Grants Council of the Hong Kong Spe-
cial Administration Region (Project No. CityU 11206622),
in part by the GuangDong Basic and Applied Basic
Research Foundation (Project No. 2020A1515010484,
2022A1515011160), and in part by TCL Science and Tech-
nology Innovation Fund (Project No. 20231752).
9179
References
[1] Rameen Abdal, Yipeng Qin, and Peter Wonka. Im-
age2stylegan++: How to edit the embedded images? In
CVPR, pages 8293–8302, 2020. 3
[2] Rameen Abdal, Peihao Zhu, Niloy J. Mitra, and Peter
Wonka. Styleflow: Attribute-conditioned exploration of
stylegan-generated images using conditional continuous nor-
malizing flows. ACM Trans. Graph., 40(3):21:1–21:21,
2021. 2,3
[3] Yuval Alaluf, Or Patashnik, and Daniel Cohen-Or. Restyle:
A residual-based stylegan encoder via iterative refinement.
InICCV, pages 6691–6700, 2021. 3
[4] Volker Blanz and Thomas Vetter. A morphable model for the
synthesis of 3d faces. In SIGGRAPH, pages 187–194. ACM,
1999. 2
[5] Sheng Chen, Yang Liu, Xiang Gao, and Zhen Han. Mobile-
facenets: Efficient cnns for accurate real-time face verifica-
tion on mobile devices. In CCBR, pages 428–438, 2018. 5
[6] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha.
Stargan v2: Diverse image synthesis for multiple domains.
InCVPR, pages 8185–8194, 2020. 2
[7] Edo Collins, Raja Bala, Bob Price, and Sabine S ¨usstrunk.
Editing in style: Uncovering the local semantics of gans. In
CVPR, pages 5770–5779, 2020. 3
[8] Jiankang Deng, Jia Guo, Jing Yang, Niannan Xue, Irene Kot-
sia, and Stefanos Zafeiriou. Arcface: Additive angular mar-
gin loss for deep face recognition. IEEE Trans. Pattern Anal.
Mach. Intell., 44(10):5962–5979, 2022. 5
[9] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde
Jia, and Xin Tong. Accurate 3d face reconstruction with
weakly-supervised learning: From single image to image set.
InCVPRW, pages 285–295, 2019. 3
[10] Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, and Xin
Tong. Disentangled and controllable face image generation
via 3d imitative-contrastive learning. In CVPR, pages 5153–
5162, 2020. 2,3,5,7
[11] Zheng Ding, Xuaner Zhang, Zhihao Xia, Lars Jebe,
Zhuowen Tu, and Xiuming Zhang. Diffusionrig: Learning
personalized priors for facial appearance editing. In CVPR,
pages 12736–12746, 2023. 2,3,5,7
[12] Bernhard Egger, William A. P. Smith, Ayush Tewari, Ste-
fanie Wuhrer, Michael Zollh ¨ofer, Thabo Beeler, Florian
Bernard, Timo Bolkart, Adam Kortylewski, Sami Romdhani,
Christian Theobalt, Volker Blanz, and Thomas Vetter. 3d
morphable face models - past, present, and future. ACM
Trans. Graph., 39(5):157:1–157:38, 2020. 2
[13] Yao Feng, Haiwen Feng, Michael J. Black, and Timo
Bolkart. Learning an animatable detailed 3d face model from
in-the-wild images. ACM Trans. Graph., 40(4):88:1–88:13,
2021. 3
[14] Yudong Guo, Juyong Zhang, Jianfei Cai, Boyi Jiang, and
Jianmin Zheng. Cnn-based real-time dense face reconstruc-
tion with inverse-rendered photo-realistic face images. IEEE
Trans. Pattern Anal. Mach. Intell., 41(6):1294–1307, 2019.
3
[15] Erik H ¨ark¨onen, Aaron Hertzmann, Jaakko Lehtinen, andSylvain Paris. Ganspace: Discovering interpretable GAN
controls. In NeurIPS, 2020. 3
[16] Zhenliang He, Wangmeng Zuo, Meina Kan, Shiguang Shan,
and Xilin Chen. Attgan: Facial attribute editing by only
changing what you want. IEEE Trans. Image Process., 28
(11):5464–5478, 2019. 2
[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In NeurIPS, pages 6626–6637, 2017. 5
[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In NeurIPS, 2020. 3
[19] Xun Huang, Ming-Yu Liu, Serge J. Belongie, and Jan Kautz.
Multimodal unsupervised image-to-image translation. In
ECCV, pages 179–196, 2018. 2
[20] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A.
Efros. Image-to-image translation with conditional adver-
sarial networks. In CVPR, pages 5967–5976, 2017. 2
[21] We Jie. Facial-expression-recognition.pytorch, 2018. 5
[22] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improving
the image quality of stylegan. In CVPR, pages 8107–8116,
2020. 2,5
[23] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
IEEE Trans. Pattern Anal. Mach. Intell., 43(12):4217–4228,
2021. 2,5
[24] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Dif-
fusionclip: Text-guided diffusion models for robust image
manipulation. In CVPR, pages 2416–2425, 2022. 2
[25] Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng
Xu, Justus Thies, Matthias Nießner, Patrick P ´erez, Christian
Richardt, Michael Zollh ¨ofer, and Christian Theobalt. Deep
video portraits. ACM Trans. Graph., 37(4):163, 2018. 3
[26] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In ICLR, 2015. 5
[27] Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip
H. S. Torr. Manigan: Text-guided image manipulation. In
CVPR, pages 7877–7886, 2020. 2
[28] Ming Liu, Yukang Ding, Min Xia, Xiao Liu, Errui Ding,
Wangmeng Zuo, and Shilei Wen. STGAN: A unified se-
lective transfer network for arbitrary image attribute editing.
InCVPR, pages 3673–3682, 2019. 5
[29] Yahui Liu, Marco De Nadai, Deng Cai, Huayang Li, Xavier
Alameda-Pineda, Nicu Sebe, and Bruno Lepri. Describe
what to change: A text-guided unsupervised image-to-image
translation approach. In ACMMM, pages 1357–1365, 2020.
2
[30] Yueming Lyu, Tianwei Lin, Fu Li, Dongliang He, Jing Dong,
and Tieniu Tan. Deltaedit: Exploring text-free training for
text-driven image manipulation. In CVPR, pages 6894–6903,
2023. 2,3,4,5,7
[31] Mehdi Mirza and Simon Osindero. Conditional generative
adversarial nets. CoRR, abs/1411.1784, 2014. 2
[32] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,
and Dani Lischinski. Styleclip: Text-driven manipulation of
stylegan imagery. In ICCV, pages 2065–2074, 2021. 2,3,5,
7
9180
[33] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami
Romdhani, and Thomas Vetter. A 3d face model for pose
and illumination invariant face recognition. In AVSS, pages
296–301, 2009. 3
[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In ICML, pages
8748–8763, 2021. 2,3
[35] Ravi Ramamoorthi and Pat Hanrahan. An efficient repre-
sentation for irradiance environment maps. In SIGGRAPH,
pages 497–500. ACM, 2001. 3
[36] Ravi Ramamoorthi and Pat Hanrahan. A signal-processing
framework for inverse rendering. In SIGGRAPH, pages 117–
128. ACM, 2001. 3
[37] Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee. Generative ad-
versarial text to image synthesis. In ICML, pages 1060–1069,
2016. 2
[38] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan,
Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding
in style: A stylegan encoder for image-to-image translation.
InCVPR, pages 2287–2296, 2021. 3
[39] Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Inter-
preting the latent space of gans for semantic face editing. In
CVPR, pages 9240–9249, 2020. 2,3
[40] Yujun Shen, Ceyuan Yang, Xiaoou Tang, and Bolei Zhou. In-
terfacegan: Interpreting the disentangled face representation
learned by gans. IEEE Trans. Pattern Anal. Mach. Intell., 44
(4):2004–2018, 2022. 2,3
[41] Yichun Shi, Divyansh Aggarwal, and Anil K. Jain. Lifting
2d stylegan for 3d-aware face generation. In CVPR, pages
6258–6266, 2021. 3
[42] Alon Shoshan, Nadav Bhonker, Igor Kviatkovsky, and
G´erard G. Medioni. Gan-control: Explicitly controllable
gans. In ICCV, pages 14063–14073, 2021. 2,5,7
[43] Quanpeng Song, Jiaxin Li, Si Wu, and Hau-San Wong. A
graph-based discriminator architecture for multi-attribute fa-
cial image editing. IEEE Transactions on Multimedia, pages
1–11, 2023. 2
[44] Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Flo-
rian Bernard, Hans-Peter Seidel, Patrick P ´erez, Michael
Zollh ¨ofer, and Christian Theobalt. Stylerig: Rigging style-
gan for 3d control over portrait images. In CVPR, pages
6141–6150, 2020. 3
[45] Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and
Daniel Cohen-Or. Designing an encoder for stylegan im-
age manipulation. ACM Trans. Graph., 40(4):133:1–133:14,
2021. 4
[46] Tengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, and
Qifeng Chen. High-fidelity GAN inversion for image at-
tribute editing. In CVPR, pages 11369–11378, 2022. 3
[47] Xiwen Wei, Zhen Xu, Cheng Liu, Si Wu, Zhiwen Yu, and
Hau-San Wong. Text-guided unsupervised latent transforma-
tion for multi-attribute image manipulation. In CVPR, pages
19285–19294, 2023. 3[48] Yue Wu, Sicheng Xu, Jianfeng Xiang, Fangyun Wei, Qifeng
Chen, Jiaolong Yang, and Xin Tong. Aniportraitgan: Ani-
matable 3d portrait generation from 2d image collections. In
SIGGRAPH, 2023. 5
[49] Zongze Wu, Dani Lischinski, and Eli Shechtman. Stylespace
analysis: Disentangled controls for stylegan image genera-
tion. In CVPR, pages 12863–12872, 2021. 2,3
[50] Weihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu.
Tedigan: Text-guided diverse face image generation and ma-
nipulation. In CVPR, pages 2256–2265, 2021. 3
[51] Zipeng Xu, Tianwei Lin, Hao Tang, Fu Li, Dongliang He,
Nicu Sebe, Radu Timofte, Luc Van Gool, and Errui Ding.
Predict, prevent, and evaluate: Disentangled text-driven im-
age manipulation empowered by pre-trained vision-language
model. In CVPR, pages 18208–18217, 2022. 2
[52] Han Zhang, Tao Xu, and Hongsheng Li. Stackgan: Text to
photo-realistic image synthesis with stacked generative ad-
versarial networks. In ICCV, pages 5908–5916, 2017. 2
[53] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and
Yinfei Yang. Cross-modal contrastive learning for text-to-
image generation. In CVPR, pages 833–842, 2021. 2
[54] Chenliang Zhou, Fangcheng Zhong, and Cengiz ¨Oztireli.
CLIP-PAE: projection-augmentation embedding to extract
relevant features for a disentangled, interpretable and con-
trollable text-guided face manipulation. In ACM SIG-
GRAPH, pages 57:1–57:9. ACM, 2023. 4
[55] Hao Zhou, Sunil Hadap, Kalyan Sunkavalli, and David Ja-
cobs. Deep single-image portrait relighting. In ICCV, pages
7193–7201, 2019. 5
[56] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A.
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In ICCV, pages 2242–2251,
2017. 2
9181
