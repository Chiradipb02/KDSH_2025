Seeing the Unseen: Visual Common Sense for Semantic Placement
Ram Ramrakhya1∗Aniruddha Kembhavi2Dhruv Batra1Zsolt Kira1Kuo-Hao Zeng2†Luca Weihs2†
1Georgia Institute of Technology2PRIOR @ Allen Institute of AI
1{ram.ramrakhya,dbatra,zk15}@gatech.edu2{anik,khzeng,lucaw}@allenai.org
Project Page: ram81.github.io/projects/seeing-unseen
Where can I put cushions?
(b)
Full of cushions
Cushion would look better 
on the couch.
(a)
Full of cushions
Cushion should go on 
armchair for comfort.
Embodied Semantic Placement
(c)
right side of the gray sofa 
leaning against the armrest 
also presents a tidy option 
for a cushion.in the center of the gray 
sofa's seat, which is 
currently unoccupied.
Figure 1. Semantic Placement . Consider asking an agent to place cushions in a living room. In (a), the couch on the right is already full
with cushions, and a natural human preference would be to place the cushion against the backrest of the armchair. In (b), a natural placement
preference would be center of the couch. We propose the problem of Semantic Placement (SP) – given an image and a name of an object, a
vision system must predict a semantic mask indicating a valid placement for the object in the image. For both (a) and (b) GPT4V gives
meaningful natural language responses but, as we show, struggles to localize regions precisely in pixel space. (c) Our SP predictions enable
a Stretch robot [ 1] from Hello Robot to perform Embodied Semantic Placement ( eSP) task within a photorealistic simulated environment.
Abstract
Computer vision tasks typically involve describing what is
present in an image ( e.g. classiﬁcation, detection, segmenta-
tion, and captioning). We study a visual common sense task
that requires understanding ‘what is not present’. Specif-
ically, given an image ( e.g. of a living room) and a name
of an object ("cushion"), a vision system is asked to predict
semantically-meaningful regions (masks or bounding boxes)
in the image where that object could be placed or is likely
be placed by humans ( e.g. on the sofa). We call this task: Se-
mantic Placement (SP) and believe that such common-sense
visual understanding is critical for assitive robots (tidying a
house), AR devices (automatically rendering an object in the
user’s space), and visually-grounded chatbots with common
sense. Studying the invisible is hard. Datasets for image
description are typically constructed by curating relevant
images (e.g. via image search with object names) and asking
humans to annotate the contents of the image; neither of
those two steps are straightforward for objects notpresent in
∗Work done as part of the internship at PRIOR @ AI2
†Equal advisingthe image. We overcome this challenge by operating in the
opposite direction: we start with an image of an object in
context, which is easy to ﬁnd online, and then remove that ob-
ject from the image via inpainting. This automated pipeline
converts unstructured web data into a dataset comprising
pairs of images with/without the object. With this proposed
data generation pipeline, we collect a novel dataset, contain-
ing∼1.3M images across 9object categories. We then train
aSPprediction model, called CLIP-UNet, on our dataset.
The CLIP-UNet outperforms existing VLMs and baselines
that combine semantic priors with object detectors, gener-
alizes well to real-world and simulated images and exhibits
semantics-aware reasoning for object placement. In our
user studies, we ﬁnd that the SPmasks predicted by CLIP-
UNet are favored 43.7%and31.3%times when comparing
against the 4SP baselines on real and simulated images. In
addition, leveraging SPmask predictions from CLIP-UNet
enables downstream applications like building tidying robots
in indoor environments.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
16273
1. Introduction
When tasked with putting away a cushion in a home, humans
quickly bring to bear extensive priors about how cushions
are used and where they are most frequently placed. For
instance, cushions are generally put on or near seating areas
(e.g., on a couch). However, these priors themselves are not
enough: consider an example living room shown in Fig. 1(a).
As shown in the ﬁgure, the couch already has cushions on
both armrests so, to avoid redundancy, one might place the
cushion against the back of the armchair for the comfort
of anyone who might later sit upon it. On the other hand,
given the same task with the image from Fig. 1(b), the an-
swer might change to placing the cushion in center of the
couch to give the room a more aesthetically pleasant feel
as the armchair already has a cushion on it. Notice that the
answer from humans about object placement changes based
on changes in the visual context. We call this task Semantic
Placement (SP), and believe that such common-sense visual
understanding is critical for assistive robots (tidying a house),
AR devices (automatically rendering an object in the user’s
space), and visually-grounded chatbots with common sense.
How can we build vision systems with SP prediction
abilities? Modern computer vision tasks have focused on
classifying, localizing, and describing what is visible in an
image ( e.g. classiﬁcation, object detection, segmentation,
and captioning). Most visual representation learning
approaches, e.g. CLIP [ 2–6], use losses that encourage the
learned representations to capture what is shown in the
image but are not designed to be used to answer queries
about the invisible in the image zero-shot ; the visual context
generated by these models is, however, extremely valueable
and we use CLIP as the visual backbone in this work. Recent
advances in vision-and-language (VLM) foundation models
has made some progress in this direction. We can ask
VLMs questions that require reasoning about the invisible,
conditioned on visual context to infer the answers to a ques-
tion. However, existing VLMs are still in early stages and
struggle to answer queries that require precise localization
in pixel space as shown in our experiments (see Sec. 5).
In this paper, we study the problem of Semantic Placement
(SP) of objects in images. In particular, given an image
(e.g. showing a living room) and name of an object ("cush-
ion"), a vision system is tasked to predict a pixel-level mask
highlighting semantically-meaningful regions (referred as
SPmasks) in an image where that object could be placed or
is likely to be placed by humans ( e.g. a couch). Learning to
predict SPmasks is hard, since the target object is typically
not visible in the given image. Datasets for image description
are typically constructed by curating relevant images (e.g.
via image search with object names) and asking humans to
annotate the contents of the image; neither of those two steps
are straightforward for objects notpresent in the image.
To overcome this challenge, we propose to operate in theopposite direction – speciﬁcally, we start with an image of an
object in context (which is easy to ﬁnd online) and remove
that object from the image via inpainting [ 7,8]. This auto-
mated pipline converts unstructured web data into a a dataset
comprising pairs of images with/without the object at scale
without expensive human annotation . However, inpainting
models are not perfect. We ﬁnd that SPprediction models,
when trained on inpainted images, tend to latch onto inpaint-
ing artifacts. This leads to high performance on inpainted
images, but lower performance on real images. To remedy
this, we propose a novel data augmentation method, com-
bining results from multiple inpainting models, diffusion
based augmentations, and common data augmentations (re-
fer Sec. 3.1section for more details). Using this automated
pipeline, we generate a large SP dataset using real world
images from LAION [ 9], including ∼1.3million images
across9object categories.
We propose a simple method for SPmask prediction
by using a frozen CLIP [ 2] backbone with a language
conditioned UNet [ 10] decoder inspired by LingUNet [ 11]
and CLIPort [ 12], in Sec. 3.3. First, we pretrain the
CLIP-UNet model on images from our SP dataset and then
ﬁnetune on a small high-quality image dataset of ∼80k
synthetic images collected from synthetic HSSD [ 13] scenes,
where inpainting is unnecessary as objects can be removed
programmatically from the underlying 3D scenes. We ﬁnd
ﬁnetuning on this small but high-quality dataset with ground
truth object placement annotations improves performance of
our CLIP-UNet baseline and enables better generalization
to both real and synthetic images.
For evaluation we use 400 real world images from
LAION [ 9] and∼18kfrom HSSD [ 13] scenes. We ﬁnd that
CLIP-UNet outperforms strong baselines leveraging VLMs,
including LLaVa-1.5 [ 14] and GPT4V [ 15], and methods
using open-vocabulary object detection and segmentation
models with placement priors coming from LLMs. In user
studies, we ﬁnd that the SPmask predicted by our method
are favored 43.7%times against the baselines on real images
and by31.3%times on images from HSSD scenes.
SP mask predictions hold potential for a variety of down-
stream applications, including assistive agents, real-time AR
rendering, and visually-grounded chatbots. In this paper, we
demonstrate that SP masks predicted by CLIP-UNet enable
embodied agents to perform Embodied Semantic Placement
(eSP) task in a photorealistic, physics-enabled simulated
environment, Habitat [ 16–18] using Hello Robot’s Stretch
robot [ 1]. IneSP, an agent is spawned at a random location in
an indoor environment and is tasked with placing an instance
of a target object category at a semantically meaningful lo-
cation with access to robot observations (RGB, Depth, and
pose) and SPmasks from a SPmodel. Using SPmasks
predicted by our CLIP-UNet model, agent achieves a 12.5%
success rate on 8categories when evaluated in 10unique
16274
indoor scenes over 106episodes. While the absolute success
is indeed low, we note that majority of failures ∼80% for
downstream eSPtask are due to imperfect control policy
for object placement and ﬁne-grained navigation, which is
orthogonal to the focus of this work. We show a qualitative
example of a placement prediction by our agent for object
category ‘cushion’ while performing the task in Fig. 1(c).
In summary, our contributions include: (1) a novel task
called Semantic Placement ( SP), (2) an automated data cu-
ration pipeline leveraging inpainting and object detection
models to supervise an end-to-end SPprediction model us-
ing real-world data, (3) a novel data augmentation method
to alleviate overﬁtting to inpainting artifacts, and (4) our
approach generates SPpredictions which generalize well to
the real-world and enable downstream robot execution.
2. Related Work
Object Affordance Prediction from Common-sense Rea-
soning. Object affordance [ 22–24] is deﬁned as a func-
tion that can map images of object to potential interactions
that are possible, like holdable, pushable, liftable, placeable,
etc. Learning such a function requires learning characteris-
tics of a object based on visual appearance, semantics, or
physical characteristics. In contrast, we are interested in
Semantic Placement task which requires reasoning about
placement of an object that is not present in the image using
the context and semantics of what is present in the image.
Prior works [ 25–27] have leveraged LLMs to extract ob-
ject affordances in the form of states like whether a object
is misplaced or is it a receptacle i.e. where you can place
another objects, to build agents to tidy up a indoor environ-
ment. LLMs [ 28–33] and VLMs [ 34–41] demonstrate strong
common-sense reasoning about object affordances based on
visual appearance or semantics, however they seldom output
SPmask/heatmap predictions with sufﬁcient granularity that
is required for downstream tasks of precise placement .
Learning Visual Affordances for Object Placement. Also
related to SPis prior work on object affordances [ 22–24] for
tasks such as tabletop manipulation [ 12,42–44], articulated
manipulation [ 45–49], dexterous grasping [ 50], and inter-
actions between embodied agents and environments [ 51].
These works focus on learning affordances for manipulation
about where to interact and how to interact with the object
by leveraging labelled simulation data, exocentric images,
and limited real world robot data. In contrast, our work fo-
cuses on predicting plausible locations for placing objects
which are not present in an image based on visual context
by leveraging automatically generated large scale labelled
data. The problem we explore is more closely aligned with
the concept of learning object-object affordances [ 52–54],
which includes the challenge of placing objects within/on
the receptacles. Perhaps the most similar to our work is
O2O [ 54] which predicts 3D affordances maps using pointcloud inputs. The O2O model was trained with data collected
through simulated interactions, resulting in more geometry-
aware affordance predictions, with limited generalization
ability. In comparison, we propose learning a SPmodel
using both images in the wild [ 9] and a high-quality simula-
tion environment [ 13] which leads to better generalization
ability. Similar to our method, recent approaches also pro-
pose learning visual affordances from natural images [ 55],
human-captured videos [ 56], or images paired with synthe-
sized interactions [ 57,58]. However, these works focus on
learning affordances for what is present in the image, in con-
trast, we study learning placement localization for objects
that are not present .
3. Approach
We introduce our dataset generation pipeline in Sec. 3.1,
synthetic ﬁnetuning dataset in Sec. 3.2and describe our SP
model and learning procedure in Sec. 3.3.
3.1. Dataset Generation
To collect paired data for training (referred as LAION-SP)
theSPmodel, we propose leveraging recent advances in
open-vocabulary object detectors, segmentation models, and
image inpainting models. With these powerful off-the-shelf
“foundation” models, we can generate paired training data at
scale using images in the wild. Fig. 2shows our automated
data generation pipeline, including ﬁve steps:
(A)Query Image. First, we gather 1M indoor images from
the LAION dataset by using text queries such as ‘living
room’, ‘bedroom’, and ‘kitchen’ to ﬁlter out irrelevant im-
ages i.e. images not from houses.
(B)Find Objects of Interest. Next, for each image we use
Detic [ 19], an open vocabulary object detector, to detect ob-
jects of interest for our task. We use 9target object categories
in this paper, speciﬁcally Plotted Plant ,Lamp,Cushion ,Vase,
Trash Can ,Toaster ,Table Lamp ,Alarm Clock , andLaptop .
For each detected instance, we generate a segmentation mask
using SAM [ 20]. We use SAM masks instead of Detic masks
as they are ﬁne-grained and result in better inpainting perfor-
mance. For information on how we prompt SAM and Detic
to generate segmentation masks, see Appendix A.
(C)Inpaint Objects of Interest. Using the detection results,
we pass the segmentation masks of instances of a sampled
object category and original image to one of the two inpaint-
ing models (each sampled with 50% probability), LAMA [ 7]
or Stable Diffusion [ 59], to generate an inpainted sample.
Speciﬁcally, we randomly sample a few instances of a target
object category and 1-4distractor objects of different cate-
gory for inpainting. We add distractor instances to make the
task of SPprediction more challenging as the model cannot
simply predict the, possibly only, free space. This also helps
prevent the model from overﬁtting to inpainting artifacts.
(D)Filter. Inpainting models are imperfect and we need
16275
LAION
Text Queries 
living room 
kitchen1M imagesDetic 
& 
SAM
Stable Diffusion
SDEdit
Discard
(A) Query ImagesSample a pair of objects
distractor objects
(B) Find Objects of Interest Inpainted Image
(C) Inpaint Objects of InterestDetic 
Filter
(D) Filter (E) Enhance Image Quality Pass
Fail
2x Augmented Images5% noisePrompt: 4k, HD
Stable 
DiffusionLAMA
1-pp
Figure 2. Automatic Training Dataset Generation Pipeline Utilizing Foundation Models and Web Data. Our pipeline consists of ﬁve
steps. (A) Query Images : we collect raw images from LAION [ 9] using sample text queries such as ‘living room’ shown in the leftmost
panel. (B) Find Objects of Interest : we employ Detic [ 19] and SAM [ 20] to identify the segmentation masks of objects of interest. (C) Inpaint
Objects of Interest : we use inpainting models to remove the objects of interest from the images. (D) Filter : we discard images where
impainting failed by attempting to detect inpainted objects. (E) Enhance Image Quality : we leverage Stable Diffusion img2img [ 8] and
SDEdit [ 21] to enhance the quality of the generated images, which is crucial for training our Semantic Placement model.
strict validation mechanisms to check if inpainting was suc-
cessful or not. To do so, we use 2D instance matching
between original and inpainted images using the detections
from Detic [ 19]. Speciﬁcally, if we ﬁnd an object instance
post-inpainting with IOU greater than 90% with an instance
from original image, the inpainting model failed and we dis-
card the generated result. All samples that pass the validation
check are kept as part of training dataset.
(E)Enhance Image Quality. In our initial experiments, we
found that training the SP model directly using the dataset
generated by the Filter step leads to overﬁtting.1The model
quickly latches onto the artifacts introduced from the in-
painting models. To mitigate this issue, we generate two
augmented versions of each inpainted image with the help
of diffusion models. To create the ﬁrst augmented variant,
we add5%Gaussian noise to the image and use SDEdit [ 21]
to denoise the image similar to Affordance Diffusion [ 57].
To create the second variant, we feed the inpainted image
to Stable Diffusion img2img [ 8] model and prompt it with
‘high resolution, 4k’ which, in practice, results in small ob-
ject texture changes. We ﬁnd this acts as regularization and
helps avoid overﬁtting on inpainting artifacts during training.
For each image processed in this way, we are left with two
augmented and inpainted images, both paired with SPan-
notations for an object category corresponding to the SAM
masks generated at the beginning of the processing to form
training samples. In total, we generate 1,329,186images
with an object category and its corresponding SAM mask
from48,728unique images queried from the LAION dataset.
In Tab. 1, we show the number of generated images per
object category. Fig. 3showcases three qualitative exam-
ples generated by our dataset generation pipeline, including
Cushion ,Laptop , andPlotted Plant . In addition to LAION-
SP training dataset of ∼1.3Mimages we also create a dataset
of400unseen original images from LAION for our evalua-
tion referred as LAION-SP Val dataset.
1The model trained on the inpainted images without quality enhance-
ment ( i.e., Step E) yields ∼0TP zero-shot evaluating on HSSD dataset.Category Potted Plant Lamp Cushion Vase Trash Can
# Images 207,366 320,922 323,541 417,591 13,353
Category Toaster Table Lamp Alarm Clock Laptop
# Images 23,928 5,559 14,496 2,430
Table 1. Number of Images per Category in LAION-SP Dataset.
Image from LAION Object of Interest Result Image
 Cushion
 Laptop
 Potted Plant
Figure 3. Qualitative Examples of Generated Images . We
present three examples of Cushion ,Laptop , andPotted Plants ,
which include raw images queried from LAION (left), identiﬁed
objects of interest and their segmentation masks obtained from
SAM (middle), and the result images after Inpainting ,Flitering , and
Quality Enhancement steps (right). For clarity, we have magniﬁed
the inpainted regions, highlighted in green dotted boxes.
3.2. Synthentic Images
For ﬁnetuning, we collect a small high-quality image dataset
from synthetic HSSD [ 13] scenes, a synthetic indoor en-
vironment dataset comprising 211high-quality 3D scenes,
containing 18,656models of real-world objects. To generate
the dataset using HSSD scenes inpainting is unnecessary as
the objects can be removed programatically from underly-
ing 3D scenes and the image can be re-rendered from the
same viewpoint using Habitat [ 16,17] simulator. Using 135
training scenes we generate ∼80ktraining images across
8object categories. Similarly, using 33unseen evaluation
scenes we generate a dataset of ∼18kimages for evaluation
16276
Skip Connections
7×7×2048
Target Query: Cushion
1024CLIP-ResNet50 
(Frozen)
CLIP-TextEncoder 
(Frozen) FC Downsample
Tile
Element-wise product1024
Frozen 
ResNet
Frozen 
TextEncoder
512
256Input Image I
f
ee(ℓ=1)e(ℓ=2)e(ℓ=3)Mask Prediction M
f(ℓ=1) f(ℓ=2) f(ℓ=3)
Figure 4. CLIP-UNet for the SP task . Inspired by CLIPort [ 12], we ﬁrst encode the input image Iinto a feature sensor f, and encode the
target object category qinto an embedding e. Further downsampling and tiling ensure that the target embedding matches the dimension of
the feature tensors f(ℓ)at the ﬁrst three decoder layers. We then use an element-wise product to combine the target embedding e(ℓ)and the
feature tensor f(ℓ)to achieve semantic conditioning. Similar to LingUNet [ 11], we add skip-connections for these three layers. Finally,
CLIPort outputs a mask prediction on the image, indicating the optimal region to place the given target object.
with the same 8object categories. Additional details on
image generation and viewpoint sampling is in App. A.2.
3.3. Learning Object Placement Affordance
To learn an SPmask prediction model, we use the dataset
generated from Sec. 3.1. The inputs to the SPmodel in-
clude an RGB image IinH×W×3size and a target ob-
ject category qin text. The model outputs an affordance
maskM, sizeH×W×1, conditioned on the target ob-
ject. Fig. 4shows the architecture of our proposed CLIP-
UNet model. Inspired by CLIPort [ 12], we use a frozen
ResNet50 [ 60], pre-trained by CLIP [ 2], to encode the input
imageIinto a feature tensor fup until the penulitmate layer
R7×7×2048. The decoder then upsamples the feature tensor
ftof(ℓ)∈RHℓ×Wℓ×Cℓat each layer ℓand, at the end,
produces a mask M∈RH×W×1, where0≤M[i,j]≤1.
To encode the target object category q, we use CLIP pre-
trained transformer based sentence encoder to construct a
target embedding e∈R1024. To condition the decoding
process with the target embedding, we ﬁrst downsample it to
¯e∈RCℓand then tile it to match the dimension of feature ten-
sorf(ℓ)at layerℓin the decoder: ¯e→¯e(ℓ)∈RHℓ×Wℓ×Cℓ,
whereCℓ={1024,512,256}andℓ∈ {1,2,3}. Then, we
use the tiled target embedding to condition the visual de-
coder layers through an element-wise product. As CLIP
utilizes contrastive loss on the dot-product aligned features
from pooled image features and language embeddings, the
element-wise product allows us to leverage this learned align-
ment while the tile operation preserves the original dimen-
sions of visual features. Inspired by LingUNet [ 11], we apply
this language conditioned operation to the ﬁrst three upsam-
pling layers right after the feature tensor fproduced by the
frozen ResNet. Moreover, following UNet [ 10], we add skip
connections to decoder layers from the corresponding layers
in ResNet encoder. In this way, the model preserves different
levels of semantic information from input image.
Training Details . We train our CLIP-UNet model in two
stages. First, we pretrain our model using the LAION-SP
dataset generated in Sec. 3.1, containing 1.3M images across
IoU( , )<0.5
IoU( , )<0.5
IoP( , )= 1IoP( , )= 1
Figure 5. I OUv.s. IOP.Top left: a hypothetical ground-truth
(GT) SPregion for objects of type “book”. Top right & bottom
left: two possible SPpredictions. Both predicted regions are high-
quality and should be considered true-positives. The IoU for these
predictions is, however, <0.5as the I OU normalizes by the large
GT region. The I OP, however, only normalizes by the predicted
mask’s size and thus is equal to 1 for both predicted regions.
9categories for 10epochs using dice loss [61]. During pre-
training, in addition to diffusion model augmented images,
we also use common data augmentations, such as gaussian
blurring, additive gaussian noise, horizontal ﬂipping, and
color jitter to mitigate inpainting artifacts. Next, we ﬁnetune
the LAION-SP pretrained model using a small, high-quality,
dataset generated using synthetic HSSD scenes [ 13,16,17]
mentioned in Sec. 3.2. As the HSSD image dataset is gener-
ated using a simulator we can manipulate the scene to render
images with/without object images without introducing any
artifacts that models can latch on to. This two-stage training
improves performance of our CLIP-UNet model and enables
better generalization to both real and synthetic images as
shown in Sec. 5.
4. Evaluation Metrics
In this section, we propose metrics for evaluating SPpredic-
tion performance. Before deﬁning these metrics, we will be-
gin by deﬁning what we mean by true/false positive (TP/FP)
and true/false negative (TN/FN) SP predictions.
16277
Preliminaries. Consider an image I, an object type query q,
and an (exhaustive) set of {0,1}-valued ground-truth dis-
joint regions r1,...,rK∈ {0,1}H×Wdescribing the lo-
cations where objects of type qcan be placed in the im-
ageI. Let the model produced region predictions be de-
noted by /hatwider1,...,/hatwiderL∈ {0,1}H×W. Intuitively, we would
like a predicted region /hatwiderjto be considered a true posi-
tive (TP), if it “overlaps sufﬁciently” with some GT region
ri. Measuring region overlap is commonly achieved, e.g.
in the semantic segmentation and object detection litera-
ture [ 62–64], using the intersection-over-union (I OU) metric,
IOU(r,r′) =r·r′/(r·r+r′·r′−r·r′)where·denotes the
usual dot product. The I OU works well when one wishes to
enforce that two regions overlap exactly ; for SPprediction,
however, requiring exact overlap is too restrictive as it nor-
malizes by too large of a region, see Fig. 5. Instead we use
the intersection-over-prediction IOP(r,r′) =r·r′/(r′·r′)
which normalizes only by the size of the predicted region r′.
That is, we say that /hatwiderjis a TP if there exists some risuch that
IOP(/hatwiderj,ri)≥TwhereT∈[0,1]is some threshold value
(for us,T= 0.5). We say that /hatwiderjis a FP if there is no riwith
IOP(/hatwiderj,ri)≥T. Importantly: TPs are counted with respect
to the ground truth region riwhile FPs are counted with
respect to the predicted region /hatwiderj. This means that that if the
model predicts multiple regions /hatwiderjwhich all correspond to a
singleri, then these multiple regions will be counted as only
a single TP. Additionally, number of FN equal to number of
GT regions rinot covered by any predicted region /hatwiderj.
Precision and recall. Given the above, we can now deﬁne
the usual recall and precision metrics for an image Ias
Precision(I) =#TP
#TP+#FPandRecall(I) =#TP
#TP+#FN.
When reporting metrics on our evaluation sets, we report the
average precision and recall over all images. If an image I
has no GT masks, then Recall(I)is not well-deﬁned and so
we do not include such images when computing the average.
We compute these metrics only on HSSD dataset as these
require access to accurate GT region annotations.
Receptacle priors. One important facet of SPprediction
is an understanding of the relationship between receptacle
types and the objects that are typically placed upon them.
For instance, you will almost always ﬁnd a plunger on the
ﬂoor and not on a dining table. Indeed, it is exactly these
types of receptacle relationships that some previous work,
e.g. [25–27], have focused upon. In order to measure the
model’s ability to encode such priors, we introduce the recep-
tacle surface precision (RSP) and receptacle surface recall
(RSR) metrics. To compute these metrics, we ﬁrst, for each
object type query q, curate a collection of receptacle types
that such an object is commonly found upon (see Sec Ap-
pendix Bfor more details). We then, for each image Iand
object type query q, assume we have access to segmentation
maskss1,...,sKof receptacles upon which qis commonly
found. Moreover, as large parts of each receptacle mask willcorrespond to unplaceable areas ( e.g. the legs of a couch)
we further assume that each sicorresponds only to the areas
of the receptacle that are “placeable”, i.e. have a surface
normal that is pointing (approximately) upward. In practice,
computing the receptacle masks can often be done automati-
cally by leveraging simulated environments in which object
categories and geometry are known (e.g. HSSD), or by using
open-vocabulary object detectors and depth maps for real
world images. As the results from open-vocabulary detec-
tors and depth maps are noisy we only report these metrics
on HSSD image dataset where we have access to ground
truth. We can then compute the RSR and RSP just as above
by replacing the GT regions riwith the surface grounded
receptacle segmentation masks si.
Target Precision (TrP) . To quantify precision of SPmodels
at localizing possible ground truth placements we compute
the Target Precision (TrP) metric. To compute TrP, we pro-
gramatically compute the GT placement masks for an object
category from HSSD scenes and use these as GT regions for
computing the precision metrics.
Human preference (HP). To understand how humans judge
our baselines outputs, we require human annotators to rank
each model’s SPpredictions from most preferred to least
preferred when shown predictions from 5models described
in Sec. 5. We then report the % of time that these annotators
rank each model’s predictions as the best, i.e. ranked above
all others, among 5SPpredictions. Further details in App. B.
5. Experiments
5.1. Semantic Placement Evaluation
Here we present evaluation results on two image datasets: 1.)
LAION-SP Val: 400real images collected from LAION [ 9],
2.) HSSD Val: 18kimages from unseen HSSD scenes [ 13].
First, we describe baselines used for evaluation:
LLM + Detector . In this baseline we leverage common-
sense priors from LLMs to ﬁnd target receptacles for a par-
ticular object and use a open-vocabulary detector, Detic [ 19],
to localize the receptacle in the image. First, for each of
the9object categories in the dataset we prompt an LLM for
common receptacle categories on which each object is found
in indoor environment. Next, during evaluation we use Detic
to localize the segmentation mask of all valid receptacles for
a object category in an image.
LLaV A . VLMs like LLaVa [ 14] connect vision encoders to
LLMs which exhibits general purpose vision-and-language
understanding. To evaluate LLaV A on SP, given the input
image we prompt it to output normalized bounding box co-
ordinates to localize a placement area. Next, we convert the
predicted normalized bounding box to a binary segmentation
mask to use as the SPmask for downstream applications.
Refer Appendix Cfor the prompt and sample predictions.
GPT4V [ 65]. Similar to LLaV A [ 14], GPT4V is a mul-
16278
Figure 6. Qualitative examples of SPmasks predicted by our CLIP-UNet model pretrained on LAION-SP dataset and ﬁnetuned on HSSD
images. (a) shows evaluation results on real image dataset from LAION [ 9], (b) shows results on images from HSSD dataset [ 13], and (c)
shows results of placement predicted while evaluating tidying robot on Embodied Semantic Placement ( eSP) task.
timodal LLM with remarkable vision-and-language under-
standing. To evaluate GPT4V , we pass the input image and
prompt it to output normalized bounding box coordinates to
localize a placement area which is then converted to a binary
segmentation mask to use as the SPmask. Refer Appendix C
for the GPT4V prompt and sample predictions.
Ours (HSSD) . Variant of our CLIP-UNet model described
in Sec. 3.3trained only on data collected from HSSD scenes
i.e. no pretraining on the LAION-SP dataset from Sec. 3.1.
Ours (LAION-SP →HSSD) . Our CLIP-UNet model
from Sec. 3.3: ﬁrst pretrained on the LAION-SP dataset
and then ﬁnetuned on a synthetic image dataset from HSSD.
LAION-SP V AL HSSD V AL
Method HP (↑) HP(↑)TrP(↑)RSP(↑)RSR(↑)
1) LLM + Detector 21.5 29 .8 10 .141.038.2
2) LLaV A 4.9 6 .8 0 .0 26 .343.4
3) GPT4V 9.4 8 .3 − − −
4) Ours (HSSD) 20.1 23 .0 16 .2 26 .6 36.5
5) Ours (SP →HSSD) 43.7 31 .3 18 .524.9 35.3
Table 2. SP evaluation on LAION-SP and HSSD valida-
tion splits. HP denotes Human Preference, TrP denotes Target
Precision, RSP denotes Receptacle Surface Precision, and RSR
denotes Receptacle Surface Recall. We use ↑to indicate that larger
values are preferred.
Results . Tab. 2reports results of evaluating methods on the
LAION-SP and HSSD evaluation datasets. In our human
preference study, our method (row 5) is favored the most
by a large margin on real world images, and modestly in
simulated images, when asked to rank predictions from all 5
baselines from Tab. 2. This demonstrates the effectiveness
of using web data for pretraining our CLIP-UNet model. In
addition to human preferences, we also conduct quantitative
evaluation using metrics from Sec. 4. Our method outper-
forms a strong baseline that uses an LLM prior and object
detector Detic (row 1) on target precision (TrP) by 8.4%, is
comparable in the RSR metric, and performs worse on RSP
metrics. This shows that the CLIP-UNet has higher preci-
sion at localizing high-quality target placements availablein the HSSD dataset, but has poor precision, compared to
the Prior+Detector baseline, when localizing all possible
visible receptacles in the image. We hypothesize that this
low precision is caused by false positive predictions in the
vicinity of receptacles not grounded to appropriate surfaces.
Our method (row 5) also outperforms both VLM baselines,
i.e. LLaV A (row 2), signiﬁcantly on TrP and achieves com-
parable performance on the RSP metric. In addition, our
method also outperforms GPT4V on human preference eval-
uation by 34.3%on real images and 23.0%on HSSD images.
Due to current GPT4V API limits, quantitative evaluation
on18kimages from HSSD val split would’ve taken 180
days so we could not show quantitative results in row 3. Af-
ter some preliminary analysis of results of the VLMs we
ﬁnd that, when tasked to output the placement location as
language in addition to bounding box coordinates, these
VLMs do a good job at giving reasonable responses but fail
to precisely localize the output in the image space. More
details in App. D. These results demonstrate the difﬁculty
of SP prediction and highlight that there’s still scope for
improvements in general-purpose VLMs. Finally, we com-
pare our method (row 5) against a CLIP-UNet trained only
on the HSSD dataset (row 4), and we ﬁnd that LAION-SP
pretraining helps signiﬁcantly in improving generalization
performance of CLIP-UNet baseline.
HSSD V AL
Method TrP (↑)RSP(↑)RSR(↑)
1) Ours (LAION-SP) 10.1 23.7 26 .3
2) Ours (HSSD) 16.226.6 36 .5
3) Ours (LAION-SP →HSSD) 18.524.9 35 .3
Table 3. LAION-SP pretraining ablations. We show the evalua-
tion results by training our CLIP-UNet on different datasets.
Effectiveness of Pretraining on the SP Dataset . Tab. 3
shows results varying the CLIP-UNet training dataset. First,
evaluating the model trained on the LAION-SP dataset zero-
shot on HSSD (row 1) results in 10.1%TrP,23.7%RSP, and
26.3%RSR. This suggests the LAION-SP pretrained model
16279
Baseline Success (↑)
1) LLM + Detector 10.5%
2) LLaV A 9.0%
3) Ours (LAION-SP →HSSD) 12.5%
Table 4. Embodied Semantic Place (eSP) evaluation perfor-
mance on HSSD VAL split. We evaluate each SP model from
Sec. 5.1using a modular eSP policy with same hyperparameters.
is, in general, good at identifying correct receptacle surfaces
in HSSD but does not, as shown by low TrP numbers, per-
form very well in precisely localizing one of the ground truth
object placements. In contrast to training on the LAION-SP
dataset, if we just train from scratch on HSSD images (row
2 vs 1) we achieve a +6.1absolute improvement on TrP,
+2.9%on RSP, and +10.2%on RSR. However, with small
amounts of ﬁnetuning of the LAION-SP pretrained model
on HSSD dataset (row 3 vs 2), we obtain our best perform-
ing model which obtains a further absolute improvement of
+2.3%on TrP with comparable performance on RSP and
RSR. In addition, as shown in human preference numbers
in Tab. 2(row 4 vs 5), pretraining on LAION-SP and ﬁnetun-
ing on HSSD leads to overall better generalization to both
sim and real images.These results effectively demonstrate
that pretraining on the LAION-SP dataset enables better
generalization. See Fig. 6for qualitative examples of our
HSSD-ﬁnetuned model’s predictions.
Open-Vocab Object Detector Ablation. We present ab-
lations of open vocabulary object detectors used in our
LLM+Detector baseline in Appendix C.1.
5.2. Embodied Evaluation
In this section, we present the results of using our CLIP-
UNet (LAION-SP →HSSD) model for the downstream
application of building a tidying robot. Speciﬁcally, in this
task, an agent is spawned at a random location in an in-
door environment and is tasked with placing an instance
of a target object category at a semantically meaningful lo-
cation. We call this task Embodied Semantic Placement
(eSP). For our experiments, we use Hello Robot’s Stretch
robot [ 1] with the full action space as deﬁned in [ 66]. Specif-
ically, the observation space, shown in the Fig. 7, includes
RGB+Depth images from the robot’s head camera, camera
pose, arm joint and gripper states, and robot’s pose relative
to the starting pose of an episode. The robot’s action space
comprises discrete navigation actions: MOVE _FORWARD
(0.25m),TURN _LEFT (30◦),TURN _RIGHT (30◦),LOOK _UP
(30◦), and LOOK _DOWN (30◦). For manipulation, we use
a continuous action space for ﬁne-grained control of the
gripper, arm extension and arm lift.
Evaluation Dataset . ForeSPevaluation, we create a dataset
consiting of 106episodes using HSSD scenes [ 13], each
speciﬁed by an agent’s starting pose and a target object
category. These episodes span 8object categories across 10indoor environments. An episode is successful if the agent
successfully places the object on one of the semantically
valid receptacle ( e.g. cushion on a bed or couch).
Embodied Semantic Placement Policy . To perform the task
with only robot observations and SP mask predictions from
the CLIP-UNet at each frame, we use a two-stage modular
policy consisting of “navigation” and “place” policies. The
navigation policy employs frontier exploration [ 67], building
a top-down semantic map using Active-Neural SLAM [ 68].
At each timestep, using the camera pose and depth we project
the predicted SPmasks onto a top-down placement affor-
dance map and explore the environment for 150steps. Fol-
lowing exploration, we utilize the placement affordance map
to navigate within 0.2mof a placement area. We then rerun
the CLIP-UNet while the agent performs a panoramic turn,
allowing for the identiﬁcation of a precise placement predic-
tion in the 2D image space. This prediction is then projected
to 3D to sample a placement location. Once a placement
location is identiﬁed, an inverse-kinematics-based planner is
used to place the object at the predicted location. The policy
is illustrated in Fig. 7, refer App. Efor more details.
Results .Tab. 4presents the results of evaluating the eSP
policy using SP mask predictions LLM +Detector, LLaVa
and our CLIP-UNet (LAION-SP →HSSD) model on HSSD
val split. We do not evaluate GPT4V on eSP task due to API
limitations, eSP policy evaluation requires running inference
using GPT4V after each robot action which amounts to a
total of∼53kframes for full evaluation. We ﬁnd our CLIP-
UNet eSP policy achieves a 12.5%success on the eSPtask
across10indoor environments, outperforming LLaVa and
LLM+Detector eSP baselines by 2−3.5%on task success.
We observe that our CLIP-UNet eSPagent can effectively
reason about appropriate object placements in these settings.
For example, in a living room scenario near a couch, the
agent determines that a book should be placed on the coffee
table, as shown in Fig. 1(c). For qualitative videos and
additional examples, please refer the supplementary.
Failure Modes . We present a detailed analysis of failure
modes of CLIP-UNet eSP policy in Appendix F.2.
6. Conclusion
We propose Semantic Placement ( SP), a novel task where,
given an image and object type, a vision system must predict
a binary mask highlighting semantically-meaningful regions
in an image where that object could be placed. Learning to
predict the invisible is hard. We address this challenge by
making visible objects invisible: we start with an image of
an object in context and remove that object from the image
via inpainting. This automated data curation pipeline, lever-
aging inpainting and object detection models, enables us to
supervise an end-to-end SPprediction model, CLIP-UNet,
using real-world data. Our CLIP-UNet produces SPpredic-
tions which generalize well to the real-world, are favored
more by humans, and enable downstream robot execution.
16280
References
[1]C. C. Kemp, A. Edsinger, H. M. Clever, and B. Matule-
vich, “The Design of Stretch: A Compact, Lightweight
Mobile Manipulator for Indoor Human Environments,”
in2022 International Conference on Robotics and Au-
tomation, ICRA 2022, Philadelphia, PA, USA, May
23-27, 2022 , pp. 3150–3157, IEEE, 2022. 1,2,8,14
[2]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,
S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark,
et al. , “Learning transferable visual models from natu-
ral language supervision,” in ICML , 2021. 2,5
[3]G. Ilharco, M. Wortsman, R. Wightman, C. Gor-
don, N. Carlini, R. Taori, A. Dave, V . Shankar,
H. Namkoong, J. Miller, H. Hajishirzi, A. Farhadi,
and L. Schmidt, “Openclip,” 2021. 2
[4]X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer, “Sig-
moid loss for language image pre-training,” in ICCV ,
2023. 2
[5]Q. Sun, Y . Fang, L. Y . Wu, X. Wang, and Y . Cao, “Eva-
clip: Improved training techniques for clip at scale,”
ArXiv , vol. abs/2303.15389, 2023. 2
[6]X. Li, Z. Wang, and C. Xie, “An inverse scaling law
for clip training,” ArXiv , vol. abs/2305.07017, 2023. 2
[7]R. Suvorov, E. Logacheva, A. Mashikhin, A. Remi-
zova, A. Ashukha, A. Silvestrov, N. Kong, H. Goka,
K. Park, and V . Lempitsky, “Resolution-robust large
mask inpainting with fourier convolutions,” in CVPR ,
2021. 2,3
[8]R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and
B. Ommer, “High-resolution image synthesis with la-
tent diffusion models,” 2021. 2,4
[9]C. Schuhmann, R. Beaumont, R. Vencu, C. Gor-
don, R. Wightman, M. Cherti, T. Coombes, A. Katta,
C. Mullis, M. Wortsman, et al. , “Laion-5b: An open
large-scale dataset for training next generation image-
text models,” in NeurIPS , 2022. 2,3,4,6,7,12
[10] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Con-
volutional networks for biomedical image segmenta-
tion,” in MICCAI , 2015. 2,5
[11] D. K. Misra, A. Bennett, V . Blukis, E. Niklasson,
M. Shatkhin, and Y . Artzi, “Mapping instructions to
actions in 3d environments with visual goal prediction,”
inCoRL , 2018. 2,5
[12] M. Shridhar, L. Manuelli, and D. Fox, “Cliport: What
and where pathways for robotic manipulation,” in Pro-
ceedings of the 5th Conference on Robot Learning
(CoRL) , 2021. 2,3,5[13] M. Khanna*, Y . Mao*, H. Jiang, S. Haresh, B. Shack-
lett, D. Batra, A. Clegg, E. Undersander, A. X. Chang,
and M. Savva, “Habitat Synthetic Scenes Dataset
(HSSD-200): An Analysis of 3D Scene Scale and
Realism Tradeoffs for ObjectGoal Navigation,” arXiv
preprint , 2023. 2,3,4,5,6,7,8,12,13
[14] H. Liu, C. Li, Y . Li, and Y . J. Lee, “Improved baselines
with visual instruction tuning,” 2023. 2,6,13
[15] OpenAI, “Gpt-4 technical report,” ArXiv , 2023. 2
[16] M. Savva, A. Kadian, O. Maksymets, Y . Zhao, E. Wi-
jmans, B. Jain, J. Straub, J. Liu, V . Koltun, J. Malik,
et al. , “Habitat: A platform for embodied AI research,”
inICCV , 2019. 2,4,5,12
[17] A. Szot, A. Clegg, E. Undersander, E. Wijmans,
Y . Zhao, J. Turner, N. Maestre, M. Mukadam, D. S.
Chaplot, O. Maksymets, A. Gokaslan, V . V ondrus,
S. Dharur, F. Meier, W. Galuba, A. Chang, Z. Kira,
V . Koltun, J. Malik, M. Savva, and D. Batra, “Habitat
2.0: Training home assistants to rearrange their habitat,”
inNeurIPS , 2021. 2,4,5,12
[18] X. Puig, E. Undersander, A. Szot, M. D. Cote, R. Part-
sey, J. Yang, R. Desai, A. W. Clegg, M. Hlavac,
T. Min, T. Gervet, V . V ondruš, V .-P. Berges, J. Turner,
O. Maksymets, Z. Kira, M. Kalakrishnan, J. Malik,
D. S. Chaplot, U. Jain, D. Batra, A. Rai, and R. Mot-
taghi, “Habitat 3.0: A co-habitat for humans, avatars
and robots,” 2023. 2
[19] X. Zhou, R. Girdhar, A. Joulin, P. Krähenbühl, and
I. Misra, “Detecting twenty-thousand classes using
image-level supervision,” in ECCV , 2022. 3,4,6,12,
13
[20] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland,
L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y .
Lo, P. Dollár, and R. Girshick, “Segment anything,” in
ICCV , 2023. 3,4,13
[21] C. Meng, Y . He, Y . Song, J. Song, J. Wu, J.-Y . Zhu,
and S. Ermon, “SDEdit: Guided image synthesis and
editing with stochastic differential equations,” in ICLR ,
2022. 4
[22] H. Luo, W. Zhai, J. Zhang, Y . Cao, and D. Tao, “Learn-
ing affordance grounding from exocentric images,” in
CVPR , 2022. 3
[23] Y .-W. Chao, Z. Wang, R. Mihalcea, and J. Deng, “Min-
ing semantic affordances of visual object categories,”
inCVPR , 2015. 3
16281
[24] D. Hadjivelichkov, S. Zwane, L. Agapito, M. P. Deisen-
roth, and D. Kanoulas, “One-shot transfer of affordance
regions? affcorrs!,” in CoRL , 2023. 3
[25] Y . Kant, A. Ramachandran, S. Yenamandra,
I. Gilitschenski, D. Batra, A. Szot, and H. Agrawal,
“Housekeep: Tidying virtual households using
commonsense reasoning,” in ECCV , 2022. 3,6
[26] G. Sarch, Z. Fang, A. W. Harley, P. Schydlo, M. J. Tarr,
S. Gupta, and K. Fragkiadaki, “Tidee: Tidying up novel
rooms using visuo-semantic commonsense priors,” in
European Conference on Computer Vision , 2022. 3,6
[27] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng,
S. Song, J. Bohg, S. Rusinkiewicz, and T. Funkhouser,
“Tidybot: Personalized robot assistance with large lan-
guage models,” Autonomous Robots , 2023. 3,6
[28] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Ka-
plan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell, et al. , “Language models are few-shot learn-
ers,” in NeurIPS , 2020. 3
[29] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,
“Bert: Pre-training of deep bidirectional transformers
for language understanding,” in ACL, 2019. 3
[30] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,
O. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov,
“Roberta: A robustly optimized bert pretraining ap-
proach,” in arXiv , 2019. 3
[31] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kul-
shreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y . Du,
et al. , “Lamda: Language models for dialog applica-
tions,” in arXiv preprint arXiv:2201.08239 , 2022. 3
[32] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,
M. Matena, Y . Zhou, W. Li, and P. J. Liu, “Exploring
the limits of transfer learning with a uniﬁed text-to-
text transformer,” in The Journal of Machine Learning
Research , 2020. 3
[33] J. Wei, M. Bosma, V . Y . Zhao, K. Guu, A. W. Yu,
B. Lester, N. Du, A. M. Dai, and Q. V . Le, “Finetuned
language models are zero-shot learners,” in ICLR , 2020.
3
[34] R. Zellers, J. Lu, X. Lu, Y . Yu, Y . Zhao, M. Salehi,
A. Kusupati, J. Hessel, A. Farhadi, and Y . Choi, “Mer-
lot reserve: Neural script knowledge through vision
and language and sound,” in CVPR , 2022. 3
[35] R. Zellers, X. Lu, J. Hessel, Y . Yu, J. S. Park, J. Cao,
A. Farhadi, and Y . Choi, “Merlot: Multimodal neural
script knowledge models,” in NeurIPS , 2021. 3[36] S. Zhang, P. Sun, S. Chen, M. Xiao, W. Shao, W. Zhang,
K. Chen, and P. Luo, “Gpt4roi: Instruction tuning
large language model on region-of-interest,” in arXiv
preprint arXiv:2307.03601 , 2023. 3
[37] W. Su, X. Zhu, Y . Cao, B. Li, L. Lu, F. Wei, and J. Dai,
“Vl-bert: Pre-training of generic visual-linguistic repre-
sentations,” in ICLR , 2020. 3
[38] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr,
Y . Hasson, K. Lenc, A. Mensch, K. Millican,
M. Reynolds, et al. , “Flamingo: a visual language
model for few-shot learning,” in NeurIPS , 2022. 3
[39] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrap-
ping language-image pre-training with frozen image
encoders and large language models,” in arXiv preprint
arXiv:2301.12597 , 2023. 3
[40] J. Li, D. Li, C. Xiong, and S. Hoi, “Blip: Bootstrap-
ping language-image pre-training for uniﬁed vision-
language understanding and generation,” in ICML ,
2022. 3
[41] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny,
“Minigpt-4: Enhancing vision-language understand-
ing with advanced large language models,” in arXiv
preprint arXiv:2304.10592 , 2023. 3
[42] F.-J. Chu, R. Xu, L. Seguin, and P. A. Vela, “Toward
affordance detection and ranking on novel objects for
real-world robotic manipulation,” in IEEE Robotics
and Automation Letters , 2019. 3
[43] Y .-C. Lin, P. Florence, A. Zeng, J. T. Barron, Y . Du,
W.-C. Ma, A. Simeonov, A. R. Garcia, and P. Isola,
“Mira: Mental imagery for robotic affordances,” in
CoRL , 2023. 3
[44] A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien,
M. Attarian, T. Armstrong, I. Krasin, D. Duong,
V . Sindhwani, et al. , “Transporter networks: Rearrang-
ing the visual world for robotic manipulation,” in CoRL ,
2021. 3
[45] Y . Wang, R. Wu, K. Mo, J. Ke, Q. Fan, L. J. Guibas,
and H. Dong, “Adaafford: Learning to adapt manipula-
tion affordance for 3d articulated objects via few-shot
interactions,” in ECCV , 2022. 3
[46] K. Mo, L. J. Guibas, M. Mukadam, A. Gupta, and
S. Tulsiani, “Where2act: From pixels to actions for
articulated 3d objects,” in ICCV , 2021. 3
[47] C. Ning, R. Wu, H. Lu, K. Mo, and H. Dong,
“Where2explore: Few-shot affordance learning for
unseen novel categories of articulated objects,” in
NeurIPS , 2023. 3
16282
[48] Y . Geng, B. An, H. Geng, Y . Chen, Y . Yang, and
H. Dong, “End-to-end affordance learning for robotic
manipulation,” in ICRA , 2023. 3
[49] Y . Zhao, R. Wu, Z. Chen, Y . Zhang, Q. Fan, K. Mo, and
H. Dong, “Dualafford: Learning collaborative visual
affordance for dual-gripper manipulation,” in ICLR ,
2022. 3
[50] P. Li, T. Liu, Y . Li, Y . Geng, Y . Zhu, Y . Yang, and
S. Huang, “Gendexgrasp: Generalizable dexterous
grasping,” in ICRA , 2023. 3
[51] T. Nagarajan and K. Grauman, “Learning affordance
landscapes for interaction exploration in 3d environ-
ments,” in NeurIPS , 2020. 3
[52] Y . Sun, S. Ren, and Y . Lin, “Object–object interaction
affordance learning,” in RSS, 2014. 3
[53] Y . Zhu, Y . Zhao, and S. Chun Zhu, “Understanding
tools: Task-oriented object modeling, learning and
recognition,” in CVPR , 2015. 3
[54] K. Mo, Y . Qin, F. Xiang, H. Su, and L. Guibas, “O2o-
afford: Annotation-free large-scale object-object affor-
dance learning,” in CoRL , 2022. 3
[55] H. Bharadhwaj, A. Gupta, and S. Tulsiani, “Visual
affordance prediction for guiding robot exploration,” in
ICRA , 2023. 3
[56] S. Bahl, R. Mendonca, L. Chen, U. Jain, and D. Pathak,
“Affordances from human videos as a versatile repre-
sentation for robotics,” in CVPR , 2023. 3
[57] Y . Ye, X. Li, A. Gupta, S. De Mello, S. Birchﬁeld,
J. Song, S. Tulsiani, and S. Liu, “Affordance diffusion:
Synthesizing hand-object interactions,” in CVPR , 2023.
3,4
[58] T. Yu, T. Xiao, A. Stone, J. Tompson, A. Brohan,
S. Wang, J. Singh, C. Tan, J. Peralta, B. Ichter, et al. ,
“Scaling robot learning with semantically imagined ex-
perience,” in RSS, 2023. 3
[59] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and
B. Ommer, “High-resolution image synthesis with la-
tent diffusion models,” in CVPR , 2022. 3
[60] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual
Learning for Image Recognition,” in CVPR , 2016. 5
[61] C. H. Sudre, W. Li, T. Vercauteren, S. Ourselin, and
M. Jorge Cardoso, “Generalised dice overlap as a deep
learning loss function for highly unbalanced segmenta-
tions,” in MICCAI Workshop , 2017. 5[62] R. Girshick, I. Radosavovic, G. Gkioxari, P. Dol-
lár, and K. He, “Detectron.” https://github.com/
facebookresearch/detectron , 2018. 6
[63] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona,
D. Ramanan, P. Dollár, and C. L. Zitnick, “Microsoft
coco: Common objects in context,” in ECCV , 2014. 6
[64] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland,
L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y .
Lo,et al. , “Segment anything,” in ICCV , 2023. 6,12
[65] OpenAI, “Gpt-4 technical report,” 2023. 6,13
[66] S. Yenamandra, A. Ramachandran, K. Yadav, A. Wang,
M. Khanna, T. Gervet, T.-Y . Yang, V . Jain, A. W. Clegg,
J. Turner, et al. , “Homerobot: Open-vocabulary mobile
manipulation,” in arXiv preprint arXiv:2306.11565 ,
2023. 8,14
[67] S. K. Ramakrishnan, D. Jayaraman, and K. Grauman,
“An exploration of embodied visual exploration,” arXiv
preprint arXiv:2001.02192 , 2020. 8
[68] D. S. Chaplot, D. Gandhi, A. Gupta, and R. Salakhut-
dinov, “Object goal navigation using goal-oriented se-
mantic exploration,” in NeurIPS , 2020. 8,14
[69] B. Yamauchi, “A frontier-based approach for au-
tonomous exploration,” in Proceedings 1997 IEEE In-
ternational Symposium on Computational Intelligence
in Robotics and Automation CIRA’97. ’Towards New
Computational Principles for Robotics and Automa-
tion’ , 1997. 12,14
[70] M. Deitke, W. Han, A. Herrasti, A. Kembhavi,
E. Kolve, R. Mottaghi, J. Salvador, D. Schwenk, E. Van-
derBilt, M. Wallingford, L. Weihs, M. Yatskar, and
A. Farhadi, “RoboTHOR: An Open Simulation-to-Real
Embodied AI Platform,” in CVPR , 2020. 12
[71] M. Deitke, R. Hendrix, A. Farhadi, K. Ehsani,
and A. Kembhavi, “Phone2proc: Bringing robust
robots into our chaotic world,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 9665–9675, 2023. 12
[72] M. Minderer, A. Gritsenko, A. Stone, M. Neumann,
D. Weissenborn, A. Dosovitskiy, A. Mahendran,
A. Arnab, M. Dehghani, Z. Shen, et al. , “Simple open-
vocabulary object detection,” in ECCV , 2022. 13
[73] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang,
C. Li, J. Yang, H. Su, J. Zhu, et al. , “Grounding dino:
Marrying dino with grounded pre-training for open-
set object detection,” arXiv preprint arXiv:2303.05499 ,
2023. 13
16283
