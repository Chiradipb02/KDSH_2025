Revisiting Sampson Approximations for Geometric Estimation Problems
Felix Rydell
KTH Royal Institute of Technology
felixry@kth.seAng´elica Torres
Max Planck Institute for Mathematics in the Sciences
angelica.torres@mis.mpg.de
Viktor Larsson
Lund University
viktor.larsson@math.lth.se
Abstract
Many problems in computer vision can be formulated
as geometric estimation problems, i.e. given a collection of
measurements (e.g. point correspondences) we wish to fit a
model (e.g. an essential matrix) that agrees with our obser-
vations. This necessitates some measure of how much an
observation “agrees” with a given model. A natural choice
is to consider the smallest perturbation that makes the ob-
servation exactly satisfy the constraints. However, for many
problems, this metric is expensive or otherwise intractable
to compute. The so-called Sampson error approximates this
geometric error through a linearization scheme. For epipo-
lar geometry, the Sampson error is a popular choice and
in practice known to yield very tight approximations of the
corresponding geometric residual (the reprojection error).
In this paper we revisit the Sampson approximation and
provide new theoretical insights as to why and when this
approximation works, as well as provide explicit bounds on
the tightness under some mild assumptions. Our theoretical
results are validated in several experiments on real data and
in the context of different geometric estimation tasks.
1. Introduction
Estimating a geometric model from a collection of mea-
surements is a common task in computer vision pipelines.
Prerequisite for any such estimation is the ability to check
whether a measurement is consistent with a given model.
This can be used both to filter outlier measurements or
as a loss for non-linear model refinement. For generative
models, i.e. models that can produce the idealized mea-
surements, it is straight-forward to check this consistency,
e.g. computing the difference between the projection and
observed 2D keypoint. However, in many cases the relation
between model and data is implicitly encoded through a set
of geometric constraints. One such example is the epipolar
z1
z2
z1z2C(z1)C(z2)
C(z1) +Jε
C(z2) +Jε
 z1
z2z1+εS
1
z2+εS
2Linearization z1 Linearization z2Figure 1. The model C(x, y) =x2+ 2y2−4 = 0 is the ellipse on the top
left, and the data points are z1andz2. On the top right, the gray surface is the
graph (x, y, C (x, y)), the orange and purple curves are the level sets C(x, y) =
C(z1)andC(x, y) =C(z2)respectively. In the bottom right the blue planes are
tangent to the surface at (z1, C(z1))and(z2, C(z2))respectively. The orange
and purple lines are the linearized constraints for z1andz2respectively. We keep
this color convention for the linearized constraints on the bottom left, and represent
their normals in red. The Sampson approximations for z1andz2are the red points
z1+εS
1andz2+εS
2, and the minimizers of the geometric error are depicted in
green. Since z1is in the gray region (obtained from Proposition 3.2), its Sampson
approximation is better than the approximation for z2.
constraint in two-view geometry,
C(x1,x2,E) = (x2; 1)⊺E(x1; 1) = 0 (1)
relating the essential (or fundamental) matrix Ewith the
point correspondence ( x1,x2)∈R2×R2. For a noisy cor-
respondence the constraint will not be satisfied exactly, so it
becomes natural to ask: How close is this match to agreeing
with the model? Mathematically, this can be formulated as
min
ˆx1,ˆx2∥ˆx1−x1∥2+∥ˆx2−x2∥2(2)
s.t.C(ˆx1,ˆx2,E) = 0 (3)
i.e., what is the smallest pertubation to the points such that
they exactly satisfy the constraint. This minimum distance
gives a measure of the consistency between the model (the
essential matrix E) and the measurements ( x1andx2.)
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
4990
For the specific case of the epipolar constraint, the op-
timal solution to (2) can be computed [8, 14] by finding
the roots to degree 6 univariate polynomial. As this pro-
cedure is relatively expensive, and difficult to integrate into
non-linear refinement methods, it is common in practice to
instead use some approximation of the error in (2). One
example of this is the Sampson error [15, 20], defined as
E2
S=((x2; 1)⊺E(x1; 1))2
∥E12(x1; 1)∥2+∥(E⊺)12(x2; 1)∥2. (4)
This expression is derived by linearizing the constraint in
(2), i.e. by replacing C(ˆx1,ˆx2, E) = 0 by the constant
and linear terms of its Taylor expansion around the data
point (x1,x2). It provides a good approximation of (2)
if the curvature of the constraint at the data point is small
enough in relation to the size of C(x1,x2, E), while being
cheap to compute and easy to optimize in non-linear refine-
ment. This approach was originally used by Sampson [20]
to approximate distances between conics and points, but has
since been applied to many other geometric models.
In this paper we revisit the Sampson approximation and
provide new theoretical insights into why (and when) the
approximation works well. Under relatively mild assump-
tions we derive explicit bounds on the tightness of the ap-
proximation. These bounds are experimentally validated on
real data and showcased in multiple applications from ge-
ometric computer vision (two- and three-view estimation,
vanishing point estimation and resectioning).
The paper is organized as follows: Section 1.1 discusses
the related work. In Section 2 we first present the classical
derivation of the Sampson error, along with some geomet-
rical interpretations of the approximation. In Section 3 we
present our bounds on the approximation. Finally, in Sec-
tion 4 we provide some experimental evaluation.
1.1. Related Work
The Geometric Error. In applied algebraic geometry, fit-
ting noisy data points to a mathematical model defined by
polynomials has recently seen a lot of interest [6], and more
specifically for 3D reconstruction [16, 19]. One of the main
contributions is the development and computation of the
so called Euclidean distance degree, which is the number
of critical points to the closest point optimization problem
given generic data. It expresses the algebraic complexity of
fitting data to a model; the higher the Euclidean distance de-
gree is, the more computationally expensive this optimiza-
tion is. It is used to implement efficient solvers in Homo-
topy Continuation [2] or for solving the associated polyno-
mials systems via specialized symbolic solvers [11].
The Sampson Error. The Sampson approximation was
first proposed in [20] to approximate the point-conic
distance. It was also later derived independently byTaubin [27]. Since then it has appeared in numerous pa-
pers for different problems. Luong and Faugeras [15] intro-
duced it for approximating the reprojection error in epipolar
geometry. The corresponding geometric error for homogra-
phies was first introduced by Sturm [24] and later revisited
by Chum et al. [4] who also dervied the Sampson approxi-
mation in this setting. Leonardos et al. [12] used the Samp-
son error for point-line-line constraint from the trifocal ten-
sor. Chojnacki et al. [3] considered how to integrate known
measurement covariances, and recently Terekhov et al. [28]
generalized the Sampson error in epipolar geometry to han-
dle arbitrary central camera model. In [26] it was used in an
optimization method for conic fitting that guarantees con-
vergence to an ellipse. This extensive use of the Sampson
approximation for geometric problems shows its versatility,
which motivates a deeper theoretical study in a more gen-
eral setting. This is precisely our goal in this work.
2. The Sampson Approximation
We consider geometric residuals analogous to (2) for
general models, i.e. problems of the form
E2
G(z,θ) = min
ε∥ε∥2(5)
s.t. C(z+ε,θ) = 0 (6)
where z∈Rnis our measurement, θour model param-
eters, and C(z,θ)our geometric constraint. In the special
case of epipolar geometry, we have that z= [x1,x2]con-
sists of the two matching image points, the model param-
eters are the entries of the essential matrix E, and the con-
straint is,
C(z,E) = (x2; 1)⊺E(x1; 1) = 0 . (7)
In the rest of the paper we consider a general polynomial
constraint C(z,θ)and will for notational convenience drop
the dependence on the model parameters θin most places.
Since (5) often does not admit a simple closed form
solution, the idea in [20] is to linearize the constraint,
C(z+ε) = 0 , at the original measurement z, i.e.
E2
S(z,θ) = min
ε∥ε∥2(8)
s.t. C(z) +Jε= 0 (9)
where J=∂C(z)/∂zis the Jacobian of the constraint,
evaluated at z. Introducing a Lagrangian for (8),1
L(ε, α) =1
2∥ε∥2+α(C(z) +Jε) (10)
we get the first-order constraints as
ε+αJ⊺= 0, C(z) +Jε= 0. (11)
1For convenience we introduce 1/2 here; it does not affect the optimum.
4991
Inserting the first equation into the second yields
C(z)−α∥J∥2= 0 = ⇒ε=−C(z)
∥J∥2J⊺. (12)
Thus the minimum in (8) is given by
E2
S(z,θ) =C(z)
∥J∥2J⊺2
=C(z)2
∥J∥2. (13)
In the special case of epipolar geometry, replacing C(z)
with the epipolar constraint, we arrive at the classical for-
mula for the Sampson error (4).
2.1. Multiple Constraints and Covariances
The Sampson error for a single constraint, discussed pre-
viously, can easily be generalized to the case where we mea-
sure the deviation in the Mahalanobis distance from a model
defined by multiple constraints. See [7, p128] for more de-
tails. Assume that the Nconstraints are given by
C(z) = (C1(z), C2(z), . . . , C N(z))⊺=0, (14)
and we want to measure the deviation εin Mahalanobis dis-
tance for a given positive definite covariance Σ. This can
be formulated as
E2
G(z,θ) = min
ε∥ε∥2
Σ (15)
s.t.C(z+ε) =0, (16)
where ∥ε∥2
Σ=ε⊺Σ−1ε. The Lagrangian of the linearized
constraint then becomes
L(ε,α) =1
2∥ε∥2
Σ+α⊺(C(z) +Jε), (17)
where J∈RN×nis the Jacobian of Cevaluated at z∈Rn.
First order conditions again yield
Σ−1ε+J⊺α=0,C(z) +Jε=0. (18)
We get ε=−ΣJ⊺α,allowing us to solve for αas
C(z)−JΣJ⊺α=0=⇒α= (JΣJ⊺)−1C(z),(19)
whenJΣJ⊺is invertible (in Section 2.2 we consider the
general case). Let εSdenote the argmin of the linearized
optimization problem. From this we get
εS=−ΣJT(JΣJ⊺)−1C(z), (20)
and finally, ∥εS∥2
Σ=εSΣ−1εS, which simplifies to
C(z)⊺(JΣJ⊺)−1C(z) =∥C(z)∥2
JΣJ⊺. (21)2.2. General Case
Let(·)†denote the Moore-Penrose psuedo-inverse of a
matrix. Assuming that the linearized equation C(z)+Jε=
0has at least one solution, meaning that C(z)∈ImJ=
ImJΣ1/2, then (JΣ1/2)(JΣ1/2)†C(z) =C(z). In this
case, all solutions to the linearized equation can be written
ε(µ) =−Σ1/2(JΣ1/2)†C(z) +Mµ,µ∈RN(22)
where the columns of Mis a basis for the nullspace of J.
In order to find εSwe consider
∥εS∥Σ= min
µ∥Σ−1/2
−Σ1/2(JΣ1/2)†C(z) +Mµ
∥
(23)
= min
µ∥ −(JΣ1/2)†C(z) +Σ−1/2Mµ∥.(24)
Here we note that M⊺Σ−1/2(JΣ1/2)†=0, which implies
that the optimal choice is µ=0. To be precise, we get
εS=−Σ1/2(JΣ1/2)†C(z). (25)
and thus
∥εS∥2
Σ=∥(JΣ1/2)†C(z)∥2. (26)
Remark 2.1. For the case of Σ =I, we note that ∥εS∥=
∥J†C(z0)∥is the length of the Gauss-Newton step,
z1=z0−(J⊺J)−1J⊺C(z0) (27)
applied to solving C(x) = 0 starting from the point z0.
Figure 1 visualizes this for one constraint in two variables.
3. Bounding the Approximation Error
In this section we investigate the Sampson approxima-
tion (8) in terms of how well it approximates the geometric
error (5). We do this by constructing explicit bounds. We
start by studying one quadratic constraint and then we gen-
eralize our methods by considering constraints of higher de-
grees. Finally, we discuss the case of multiple constraints,
but leave the details in the Supplementary Material.
3.1. One Quadratic Constraint
Consider the case where we have one quadratic con-
straint C(z). The classical Sampson error (4) falls into this
category as the epipolar constraint is a quadratic polynomial
in terms of the image points.
We write εGfor the argmin of (5) and εSfor the argmin
of (8). Given a fixed data point z∈Rn, we write Hfor the
Hessian of Catz. Since Cis a quadratic polynomial,
C(z+ε) =C(z) +Jε+1
2ε⊺Hε. (28)
4992
We use that any vector εsatisfies the inequality
|ε⊺Hε| ≤ρ∥ε∥2(29)
where ρis the spectral radius of the Hessian of C(z), that
is, the maximum of the absolute values of its eigenvalues.
First we present an upper bound on the Sampson approx-
imation ∥εS∥in terms of the geometric residual ∥εG∥.
Proposition 3.1. When the optimization problem (5)only
has one quadratic constraint and J̸= 0, then
∥εS∥ ≤ ∥εG∥+ρ
2∥J∥∥εG∥2. (30)
Under the assumption that ρ/2∥J∥is reasonably small
(i.e. function is approximately linear), we can interpret this
proposition as saying that if εSis big, then εGis also big.
The condition J̸= 0is reasonable, as the approximation
is undefined if the linearized constraint set is empty.
Proof. Since εGsatisfies the constraint (28), we have that
0 =C(z) +JεG+1
2(εG)⊺HεG(31)
=C(z) +J
εG+J⊺
∥J∥21
2(εG)⊺HεG
. (32)
In other words, εG+J⊺
∥J∥21
2(εG)⊺HεGsatisfies the lin-
earized constraint. Since εSis by definition the smallest
vector that satisfies the linearized constraint, we get that
∥εS∥ ≤ ∥εG∥+1
2∥J∥|(εG)⊺HεG|. (33)
Finally, using the inequality of the spectral radius ρmen-
tioned above, we get the upper bound.
Next we give an upper bound of ∥εG∥in terms of ∥εS∥.
Proposition 3.2. If the minimization problem (5)only has
one quadratic constraint,
J̸= 0and∥J∥4≥2|C(z)||JHJ⊺|, (34)
then
∥εG∥ ≤2∥εS∥. (35)
The assumption should be intuitively understood as the
model being close enough to linear in the direction of J
locally around the input z, relative to the size of C(z). Note
that from the Proposition, we also have
∥εS−εG∥ ≤ ∥εS∥+∥εG∥ ≤3∥εS∥. (36)Proof. Define ε(λ) :=λ
∥J∥J⊺forλ∈Rand consider
C(z+ε(λ)) =C(z) +∥J∥λ+JHJ⊺
2∥J∥2λ2, (37)
which is a quadratic polynomial in λ. Evaluating this poly-
nomial at λ=−2C(z)/∥J∥, we get
−C(z) + 2JHJ⊺
∥J∥4C(z)2. (38)
The absolute value of the second term can by assumption
be bounded from above by |C(z)|. This means that either
C(z)≥0andC(z+ε(−2C(z)/∥J∥))≤0or the other
way around. By continuity of polynomials, there must exist
a solution λ∗to (37) in the intervalh
0,−2|C(z)|/∥J∥i
.
SinceεGis the smallest vector satisfying this, we have
∥εG∥ ≤ ∥ε(λ∗)∥=|λ∗| ≤2|C(z)|/∥J∥= 2∥εS∥.(39)
In order to get a sharper bound in the case that JHJ⊺̸=
0, we may instead solve the quadratic equation (37) directly:
λ∗=−∥J∥3± ∥J∥p
∥J∥4−2C(z)JHJ⊺
JHJ⊺ .(40)
Letλ∗be the solution with smallest absolute value, then
∥εG∥ ≤ | λ∗|and, following from the proof of Proposi-
tion 3.2, |λ∗| ≤2∥εS∥. Note that if JHJ⊺= 0, then
λ∗=−C(z)/∥J∥and we directly get ∥εG∥ ≤ ∥εS∥.
To summarize we have the following inequalities
1
2∥εG∥ ≤1
τ∥εG∥ ≤ ∥εS∥ ≤ ∥εG∥+ρ
2∥J∥∥εG∥2,(41)
where τ=|λ∗/C(z)|∥J∥andλ∗is given by (40).
The hypothesis in Proposition 3.2 defines a region where
the geometric error is bounded linearly by the Sampson er-
ror. We highlight that this region contains the region defined
by
∥εS∥=|C(z)|
∥J∥≤∥J∥
2ρ. (42)
Example 3.3. Consider as a constraint the conic C(x) =
x2
1+2x2
2−4inR2. For fixed z= (z1, z2), the minimization
problem is
min
ε∥ε∥2(43)
s.t. (z1+ε1)2+ 2(z2+ε2)2−4 = 0 (44)
and the linearized constraint is
z2
1+ 2z2
2−4 +2z14z2ε1
ε2
= 0. (45)
4993
Figure 2. Regions obtained for Example 3.3. On the left, the constraint
x2
1+ 2x2
2−4 = 0 is depicted in blue, the purple region is obtained
from Equation (42) and it is contained in the orange region coming from
Equation (35). On the right, the level sets for the ratio ∥εG∥/∥εS∥, and
the constraint depicted in blue. Observe that, although the ratio changes, it
is bounded by 2 for every point in the colored regions.
From Proposition 3.2, we have that ∥εG∥ ≤2∥εS∥ifzis in
the colored region of Figure 2 (both the orange and purple
region). Finally, the relaxed condition from (42) gives a
smaller region, potentially easier to work with, where the
previous bounds also hold (purple region in Figure 2)
3.2. One Polynomial Constraint
To understand the Sampson error for a model defined by
a single polynomial constraint of any degree C:Rn→
R, we extend the method presented previously and consider
Taylor approximations of degree d:
C(z+ε) =C(z) +dX
i=11
i!ε× Ti, (46)
where Tiis a symmetric n× ··· × ntensor of order i, and
ε× Ti=X
j1,...,ji∈[n](Ti)j1,...,jiεj1···εji. (47)
For example, T1=Jis the Jacobian and T2=Hthe
Hessian.
Proposition 3.4. When the optimization problem (5)only
has one polynomial constraint of degree d,
J̸= 0and|C(z)| ≥dX
i=2(−1)i2iC(z)i
i!∥J∥iJ× Ti,(48)
then∥εG∥ ≤2∥εS∥.
Proof. The proof is exactly the same as for Proposition 3.2;
one checks that C(z)≤0andC(z+ε(λ))≥0(or the
other way around) for λ=−2C(z)/∥J∥.
3.3. Multiple Polynomial Constraints
For mathematical models defined by multiple con-
straints, the Sampson error and its relation to the geomet-
ric error are more involved. Here, we give an overview ofour approach to computing Sampson errors in practice and
studying its relation to the geometric error for polynomial
constraints, but leave mathematical statements and proofs
in the Supplementary Material.
Assume that C(z) = ( C1(z), . . . , C N(z))areNpoly-
nomial constraints. The model C(z) =0, forz∈Rn,
is an algebraic variety X(a zero set of a system of poly-
nomials) of some dimension m≤n, which depends on
the constraints. For generic constraints of fixed degrees
m=n−N, i.e. each constraint lowers the dimension of the
model by one, however, for specific systems of polynomi-
als this equality does not necessarily hold. In both cases, the
intuitive behaviour is that the more constraints we have, the
smaller mis and our data is harder to fit to the model. The
same phenomenon occurs with the linearized constraints
for the Sampson error: The more constraints we have, the
smaller is the affine linear space defined by C(z)+Jε= 0.
Therefore, if the number of constraints Nis much greater
than the codimension n−m, the Sampson approximation
is likely to be poor. To remedy this, we propose to use the
fact that locally around a generic point of X, the model is
described by precisely n−mpolynomials whose Jacobian
has full rank. This is a result coming from algebraic geome-
try. Our proposal is to perform Sampson approximation by
choosing a subset of n−mconstraints whose Jacobian is
full-rank and linearizing those constraints. In Section 4.2
we try this approach for the Three-View Sampson error and
the results suggest that it is also beneficial to choose these
constraints with smallest degree possible.
In the Supplementary Material, we generalize Proposi-
tion 3.1 and find an upper bound for ∥εS∥in terms of ∥εG∥
for multiple constraints of any degrees. We also generalize
Proposition 3.2 for quadratic constraints and data points z
such that the Jacobian Jatzhas linearly independent rows.
In this case, under appropriate conditions, we find a λ∗such
thatCi(z+∥J∥J†λ∗) = 0 fori= 1, . . . , n . We get
∥εG∥ ≤ ∥∥ J∥J†λ∗∥ ≤ ∥J∥∥J†∥∥λ∗∥, (49)
and by construction we get an upper bound for this ∥λ∗∥
expressed in terms of C, its Jacobian and its Hessian.
4. Experimental Evaluation
In the following sections we evaluate the Sampson ap-
proximation for different geometric estimation problems.
First, in Section 4.1 we evaluate the classical Sampson error
for two-view geometry. Next, in Section 4.2 we consider the
analogous error in the three-view setting. Section 4.3 con-
siders line segment to vanishing point errors. Finally, Sec-
tion 4.4 show an application from absolute pose estimation
with uncertainties applied in both 2D and 3D.
4994
4.1. Application: Two-view Relative Pose
We first consider the classical setting where the Samp-
son approximation is applied, two-view relative pose esti-
mation. For the experiments we use image pairs from the
IMC Phototourism 2021 [9] (SIFT), MegaDepth-1500 [25]
(SP+LG [13]) and ScanNet-1500 [22] (SP+SG [22]). For
each image pair we estimate an initial essential matrix us-
ing DLT [7] applied to the ground truth inliers (from [14]).
Results and Discussion. Figure 3 shows the distribution of
the difference between the Sampson approximation and the
true error, and in Table 1 shows the Area-Under-Curve2up
to 1 pixel, on the British Museum scene from IMC-PT. For
comparison we also include the symmetric epipolar error
(distances to the epipolar lines computed in both images)
which is another popular choice in practice. The Sampson
error provides a very accurate approximation of the true
reprojection error. As discussed in Section 3 the quality
of the approximation depends on the how close the initial
point is to the constraint set (small C(z)) and the curvature
(small Hessian). In Figure 4 we plot the approximation er-
ror|ES−EG|against ρ|C(z)|/∥J∥2, where ρis the spectral
norm of the Hessian. Consistent with the theory, the fig-
ure shows a clear trend where correspondences with smaller
ρ|C(z)|/∥J∥2have smaller errors.
Finally we also evaluate the error in the context of pose
refinement on all three datasets. Table 2 shows the result-
ing pose errors (max of rotation and translation error) after
non-linear refinement of the initial essential matrix using
different error functions. Algebraic is the squared epipo-
lar constraints. Cosine is the squared cosine of the angles
between the normals of the epipolar planes and the point
correspondences. The Sampson error provides the most ac-
curate camera poses after refinement.
AUC@10◦pose error IMC-PT MD1.5k SN1.5k
Initial estimate (DLT) 0.361 0.321 0.274↰Algebraic 0.580 0.515 0.485↰Cosine 0.654 0.689 0.600↰Sym. Epipolar 0.673 0.728 0.654↰Sampson 0.678 0.732 0.657
Table 2. Comparison of losses for two-view relative pose refine-
ment on IMC-PT, MegaDepth-1500 and ScanNet-1500. The initial pose
is found by linear estimate (DLT) applied to the inlier correspondences
(w.r.t. ground truth pose), followed by non-linear refinement.
4.2. Application: Three-view Sampson Error
In this section we evaluate different error formulations
for 3-view point matches. The naive baseline is to simply
average the two-view Sampson errors (4), Epair=
ES(x,x′, E12) +ES(x,x′′, E13) +ES(x′,x′′, E23)(50)
2Area under the CDF up to 1 px error as a ratio of the complete square.−15 −10 −5 000.10.20.30.4
log10|E − E G|Sampson error
Sym. epipolar error
Figure 3. Approximation gap for two-view relative pose. Comparison
with optimal triangulation [14]. The unit is in pixels. Here Erefers to
either the Sampson or the symmetric epipolar error.
|E − E G|, AUC @ τpx (↑)
Error τ= 0.1τ= 0.5τ= 1
Sampson 0.991 0.998 0.999
Symmetric Epipolar 0.620 0.839 0.902
Table 1. The table shows the Area-Under-Curve of the approximation
error|E − E G|up to different thresholds.
−8 −6 −4 −2 0−15−10−50
log10ρ|C(z)|/∥J∥2log10|ES− EG|
Figure 4. Approximation error against ρ|C(z)|/∥J∥2for two-view
relative pose. Figure shows a heatmap built from the inlier correspon-
dences of ∼5k image pairs from the British Museum scene. Points that are
either close to satisfying the epipolar constraint ( C(z)≈0) or have low
curvature ( ρ) have smaller errors.
where (x,x′,x′′)is the correspondence and Eijare the es-
sential matrices. For a given a trifocal tensor T ∈R3×3×3
with slices T1,T2,T3∈R3×3, a consistent three-view point
correspondence x,x′,x′′satisfies
C9(x,x′,x′′) = [ˆx′]× X
kˆxkTk!
[ˆx′′]×=0 (51)
where ˆx= [x; 1]is the homogenization of the 2D point x∈
R2. While we have nine equations, only four are linearly
independent. These can be obtained by multiplying with
two3×2matrices,
C4(x,x′,x′′) =S⊺
1[ˆx′]× X
kˆxkTk!
[ˆx′′]×S2(52)
where S1, S2∈R3×2contains a basis for the complement
of the left and right nullspace of C9respectively. Another
4995
alternative is to only consider the three pairwise constraints,
C3(x,x′,x′′) =
x′⊺E12x,x′′⊺E13x,x′′⊺E23x′
(53)
Note that applying the Sampson approximation to C3yields
a different error compared to Epair (50), as the constraints
are considered jointly.
In this section we evaluate the following errors
•Epair- averaging the pairwise Sampson errors (50)
•E9- applying Sampson approximation to C9(51)
•E4- applying Sampson approximation to C4(52)
•E3- applying Sampson approximation to C3(53)
We also include two combinations of the above. First taking
3 out of the 4 constraints from C4, denoted C4:3, and one
where we combine C3andC4by taking two quadratic and
one cubic constraint, denoted C4:1,3:2. We also consider a
set of psuedo-Sampson approximations which take the form
∥C∥2/∥J∥2, and thus avoid computing matrix inverses as
in Section 2.2. This can be seen as a naive extension of
the 1-dimensional Sampson approximation (8) to the multi-
dimensional case. We denote these as ˆE9,ˆE4,ˆE3.
Experiment setup. To compare the approximations
we generate synthetic camera triplets (70◦field-of-view,
1000x1000 pixel images) observing a 3D point. To the three
projections we add normally distributed noise with stan-
dard deviation σ∈ {1,5,10}px. We obtain the reference
ground-truth reprojection error by directly optimizing over
the 3D point. For each of the evaluated approximated error
functions we compute the difference to the ground-truth.
Results and Discussion. Table 3 shows the errors for 100k
synthetic instances. We compute the Area-Under-Curve up
to 1 pixel deviation from the reference error. The Sampson
approximation of the epipolar constraints C3(53) yields the
best approximation. In particular, we can see that the naive
approximation Epairthat averages the pairwise Sampson er-
rors, is significantly worse. Interestingly, we can also see
thatC4performs much worse compared to both C3and
the mixed variants C4:3andC4:1,3:2. This is consistent
with the discussion in Sec. 3.3.
4.3. Application: Vanishing Point Estimation
We now show another example with a 1-dimensional
quadratic constraint. Consider a line-segment (x1,x2)∈
R2×R2and a vanishing point v∈S2. Assuming we want
to refine v, it is reasonable to consider what is the smallest
pertubation of the line endpoints such that the line passes
through the vanishing point, i.e. satisfy the constraint
C(x1,x2) =v⊺(x1
1
×x2
1
) (54)|E − E gt|, AUC @ 1px (↑)
σ= 1px σ= 5px σ= 10 px
E3=∥J†
3C3∥ 0.998 0.961 0.882
E4:1,3:2=∥J†
4:1,3:2C4:1,3:2∥ 0.995 0.937 0.830
E4:3=∥J†
4:3C4:3∥ 0.994 0.912 0.768
E4=∥J†
4C4∥ 0.765 0.481 0.353
Epair=...(50) 0.764 0.312 0.172
ˆE3=∥C3∥/∥J3∥ 0.361 0.016 0.003
ˆE4=∥C4∥/∥J4∥ 0.356 0.009 0.001
ˆE9=∥C9∥/∥J9∥ 0.356 0.009 0.001
E9=∥J†
9C9∥ 0.348 0.075 0.034
Table 3. Comparison of three-view reprojection error approxima-
tions. Each error metric is compared to the ground truth error (found by
non-linear refinement of the 3D point). To each keypoint we add noise
with standard deviation σpixels.
Differentiating with respect to the image points we get,
J=x2
1
×v
S,
v×x1
1
S
(55)
where S=
I20⊺, and we can directly setup a Samp-
son approximation of the line-segment to vanishing point
distance as |C(x1,x2)|/∥J∥. For this problem the ground-
truth error EGcan be computed in closed form using SVD.
In Section 3.1 we derived bounds that relate the true ge-
ometric error εGand the Sampson approximation εS,
Bl:=1
τ≤∥εS∥
∥εG∥≤1 +ρ
2∥J∥∥εG∥=:Bu (56)
Next we evaluate how tight these bounds are on real data.
Experiment Setup. For the experiment we consider circa
350k pairs of line segments and vanishing points col-
lected from the YUB+ [5] and NYU VP [10, 17]. The
line segments are detected using DeepLSD [18] and using
Progressive-X [1] we estimate a set of vanishing points.
Results and Discussion. Figure 5 shows Bu, Blfor each
of line-vanishing point pairs and in Figure 6 we show the
distribution of the difference between the bounds. As can
be seen in the figures the approximation works extremely
well for this setting. We also experimented with refining the
vanishing points using the Sampson error but found that the
results are very similar to minimizing the mid-point error
(as was done in [18]). The full results and details can be
found in the Supplementary Material.
4.4. Application: 2D/3D Reprojection Error
Minimizing the square reprojection error, i.e. deviation
between the observed 2D point and the projection of the 3D
point, assumes a Gaussian noise model on the 2D obser-
vations. However, in many scenarios, we also have noise
in the 3D points. In this section we consider the case where
we have a known covariance for both the 2D and 3D-points.
4996
τ= 5px τ= 10 px τ= 20 px
AUC (↑)εR(↓)εt(↓) AUC (↑)εR(↓)εt(↓) AUC (↑)εR(↓)εt(↓) RT
Reprojection error (60) 0.358 1.03 3.20 0.350 1.06 3.24 0.311 1.21 3.63 0.7 ms
Reprojection error + Cov. (61) 0.367 1.01 3.10 0.379 0.99 2.99 0.378 0.99 3.00 10.1 ms↰Sampson approximation 0.367 1.01 3.10 0.379 0.98 2.99 0.375 1.00 3.02 2.4 ms
Table 4. Pose refinement on 7Scenes. Table shows the Area-Under-Curve (AUC) @ 5cm position errors and the median errors in rotation εR(deg.) and
translation εt(cm). Results are reported for different inlier thresholds τ. For large thresholds, more uncertain points are included and the improvement from
the covariance weighting is more significant. However, larger errors also make the linearization point (the original correspondence) worse, degrading the
Sampson approximation. For the two lower thresholds, the Sampson approximation of 2D-3D covariance weighted reprojection error gives almost identical
results as performing the full (expensive) optimization.
0 1·1052·1053·1050.9990.999511.00051.001
Bu= 1 +ρ
2∥J∥∥εG∥
Bl=τ−1
Figure 5. Evaluation of the bounds for VP-line error. The lower bound
Bland upper bound Bufor the ≈350k VP-line pairs in the combined
dataset. For illustration the pairs are sorted w.r.t. the tightness of the bound.
−10 −8 −6 −4 −200.20.4
log10(Bu−Bl)
Figure 6. Evaluation of the bounds for VP-line error. Distribution of
the log10differences between the upper and lower bounds.
Given a point correspondence (x,X)∈R2×R3, to-
gether with covariances Σ2D∈R2×2andΣ3D∈R3×3,
the maximum likelihood estimate is then given by
min
ε2,ε3∥ε2∥2
Σ2D+∥ε3∥2
Σ3D(57)
s.t.C(x+ε2,X+ε3) = 0 (58)
where C(x,X)encodes the reprojection equations, i.e.
C(x,X) = [I2×2,−x] (RX+t) = 0 . (59)
Experiment Setup. For the experiment we consider the vi-
sual localization benchmark setup on 7Scenes [23] dataset.
Using HLoc [21] we establish 2D-3D matches and estimate
an initial camera pose for each query image. For the exper-
iment we then refine this camera pose, including the uncer-
tainty in both the 2D and 3D points. The 2D covariances are
assumed to be unit gaussians and to obtain the 3D covari-
ances we propagate the 2D covariances from the mapping
images used to triangulate the 3D point.
Results and Discussion. Table 4 shows the average pose
error across all scenes (per-scene results are available in theSupplementary Material). We compare only minimizing the
reprojection error (only 2D noise)
min
R,tX
k∥xk−π(RXk+t)∥2
Σ2(60)
with optimizing over the 3D points as well (2D/3D noise),
min
R,t,{ˆXk}X
k∥xk−π(RˆXk+t)∥2
Σ2+∥ˆXk−Xk∥2
Σ3(61)
and applying the Sampson approximation to (59). As shown
in Table 4, including the uncertainty of the 3D point can
greatly improve the pose accuracy. Further, the Sampson
approximation works well in this setting and it is only when
we include matches with very large errors (20 pixels) that
performance degrades.
Note that the optimization problem in (61) requires pa-
rameterizing each individual 3D point, potentially leading
to hundreds or thousands of extra parameters compared to
(60) that only optimize over the 6-DoF in the camera pose.
Since the Sampson approximation eliminates the extra un-
knowns, it also allows us to only optimize over the cam-
era pose while modelling the 3D uncertainty. Table 4 also
shows the average runtime in milliseconds for the query im-
ages. Minimizing the Sampson approximation is signifi-
cantly faster compared to (61).
5. Conclusions
The Sampson approximation, originally applied to com-
pute conic-point distances, has shown itself to surprisingly
versatile in the context of robust model fitting. While it has
been known that it works extremely well in practice, we
provide the first theoretical bounds on the approximation
error. In multiple experiments on real data in different
application contexts we have validated our theory and
highlighted the usefulness of the approximation.
Acknowledgments: Viktor Larsson was supported by the strategic
research project ELLIIT and the Swedish Research Council (grant
no. 2023-05424). Felix Rydell was supported by the Knut and
Alice Wallenberg Foundation within their WASP (Wallenberg AI,
Autonomous Systems and Software Program) AI/Math initiative.
Ang´elica Torres was supported by DFG grant 464109215 within the
priority programme SPP 2298 “Theoretical Foundations of Deep
Learning”.
4997
References
[1] Daniel Barath and Jiri Matas. Progressive-X: Efficient, any-
time, multi-model fitting algorithm. In International Confer-
ence on Computer Vision (ICCV) , 2019. 7
[2] Paul Breiding and Sascha Timme. Homotopycontinuation.jl:
A package for homotopy continuation in julia. In James H.
Davenport, Manuel Kauers, George Labahn, and Josef Ur-
ban, editors, Mathematical Software – ICMS 2018 , pages
458–465, Cham, 2018. Springer International Publishing. 2
[3] Wojciech Chojnacki, Michael J. Brooks, Anton Van
Den Hengel, and Darren Gawley. On the fitting of surfaces
to data with covariances. IEEE Trans. Pattern Analysis and
Machine Intelligence (PAMI) , 2000. 2
[4] Ond ˇrej Chum, Tom ´aˇs Pajdla, and Peter Sturm. The geomet-
ric error for homographies. Computer Vision and Image Un-
derstanding (CVIU) , 2005. 2
[5] Patrick Denis, James H Elder, and Francisco J Estrada. Ef-
ficient edge-based methods for estimating manhattan frames
in urban imagery. In European Conference on Computer Vi-
sion (ECCV) , 2008. 7
[6] Jan Draisma, Emil Horobet ¸, Giorgio Ottaviani, Bernd Sturm-
fels, and Rekha R Thomas. The euclidean distance degree of
an algebraic variety. Foundations of computational mathe-
matics , 16:99–149, 2016. 2
[7] Richard Hartley and Andrew Zisserman. Multiple view ge-
ometry in computer vision . Cambridge university press,
2003. 3, 6
[8] Richard I Hartley and Peter Sturm. Triangulation. Computer
Vision and Image Understanding (CVIU) , 1997. 2
[9] Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas,
Pascal Fua, Kwang Moo Yi, and Eduard Trulls. Image
Matching Across Wide Baselines: From Paper to Prac-
tice. International Journal of Computer Vision (IJCV) ,
129(2):517–547, Feb. 2021. 6
[10] Florian Kluger, Eric Brachmann, Hanno Ackermann,
Carsten Rother, Michael Ying Yang, and Bodo Rosenhahn.
CONSAC: Robust multi-model fitting by conditional sam-
ple consensus. In Computer Vision and Pattern Recognition
(CVPR) , 2020. 7
[11] Viktor Larsson, Kalle Astrom, and Magnus Oskarsson. Effi-
cient solvers for minimal problems by syzygy-based reduc-
tion. In Computer Vision and Pattern Recognition (CVPR) ,
2017. 2
[12] Spyridon Leonardos, Roberto Tron, and Kostas Daniilidis. A
metric parametrization for trifocal tensors with non-colinear
pinholes. In Computer Vision and Pattern Recognition
(CVPR) , 2015. 2
[13] Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Polle-
feys. Lightglue: Local feature matching at light speed. In
International Conference on Computer Vision (ICCV) , 2023.
6
[14] Peter Lindstrom. Triangulation made easy. In Computer
Vision and Pattern Recognition (CVPR) , 2010. 2, 6
[15] Quan-Tuan Luong and Olivier D Faugeras. The fundamental
matrix: Theory, algorithms, and stability analysis. Interna-
tional Journal of Computer Vision (IJCV) , 1996. 2[16] Laurentiu G Maxim, Jose I Rodriguez, and Botong Wang.
Euclidean distance degree of the multiview variety. SIAM
Journal on Applied Algebra and Geometry , 4(1):28–48,
2020. 2
[17] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob
Fergus. Indoor segmentation and support inference from
rgbd images. In European Conference on Computer Vision
(ECCV) , 2012. 7
[18] R ´emi Pautrat, Daniel Barath, Viktor Larsson, Martin R Os-
wald, and Marc Pollefeys. Deeplsd: Line segment detection
and refinement with deep image gradients. In Computer Vi-
sion and Pattern Recognition (CVPR) , 2023. 7
[19] Felix Rydell, Elima Shehu, and Angelica Torres. Theoretical
and numerical analysis of 3d reconstruction using point and
line incidences. arXiv preprint arXiv:2303.13593 , 2023. 2
[20] Paul D Sampson. Fitting conic sections to “very scattered”
data: An iterative refinement of the bookstein algorithm.
Computer graphics and image processing , 1982. 2
[21] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and
Marcin Dymczyk. From coarse to fine: Robust hierarchical
localization at large scale. In Computer Vision and Pattern
Recognition (CVPR) , 2019. 8
[22] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,
and Andrew Rabinovich. Superglue: Learning feature
matching with graph neural networks. In Computer Vision
and Pattern Recognition (CVPR) , 2020. 6
[23] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram
Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene co-
ordinate regression forests for camera relocalization in RGB-
D images. In Computer Vision and Pattern Recognition
(CVPR) , 2013. 8
[24] Peter Sturm. Vision 3D non calibr ´ee: contributions `a la re-
construction projective et ´etude des mouvements critiques
pour l’auto-calibrage . PhD thesis, Institut National Poly-
technique de Grenoble-INPG, 1997. 2
[25] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and
Xiaowei Zhou. Loftr: Detector-free local feature matching
with transformers. In Computer Vision and Pattern Recogni-
tion (CVPR) , 2021. 6
[26] Zygmunt L Szpak, Wojciech Chojnacki, and Anton Van
Den Hengel. Guaranteed ellipse fitting with the sampson dis-
tance. In European Conference on Computer Vision (ECCV) ,
2012. 2
[27] Gabriel Taubin. Estimation of planar curves, surfaces, and
nonplanar space curves defined by implicit equations with
applications to edge and range image segmentation. IEEE
Trans. Pattern Analysis and Machine Intelligence (PAMI) ,
1991. 2
[28] Mikhail Terekhov and Viktor Larsson. Tangent sampson er-
ror: Fast approximate two-view reprojection error for central
camera models. In International Conference on Computer
Vision (ICCV) , 2023. 2
4998
