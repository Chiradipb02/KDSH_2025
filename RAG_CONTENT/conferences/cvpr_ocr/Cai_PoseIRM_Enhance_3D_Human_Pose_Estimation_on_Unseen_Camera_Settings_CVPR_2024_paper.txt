PoseIRM: Enhance 3D Human Pose Estimation on Unseen Camera Settings via
Invariant Risk Minimization
Yanlu Cai1, Weizhong Zhang1,*, Yuan Wu1, Cheng Jin1,2,*
1Fudan University, Shanghai, China2Haina Lab, Shanghai, China
{ylcai20, weizhongzhang, wuyuan, jc }@fudan.edu.cn
Abstract
Camera-parameter-free multi-view pose estimation is an
emerging technique for 3D human pose estimation (HPE).
They can infer the camera settings implicitly or explicitly
to mitigate the depth uncertainty impact, showcasing sig-
nificant potential in real applications. However, due to the
limited camera setting diversity in the available datasets,
the inferred camera parameters are always simply hard-
coded into the model during training and not adaptable to
the input in inference, making the learned models cannot
generalize well under unseen camera settings. A natural
solution is to artificially synthesize some samples, i.e., 2D-
3D pose pairs, under massive new camera settings. Un-
fortunately, to prevent over-fitting the existing camera set-
ting, the number of synthesized samples for each new cam-
era setting should be comparable with that for the existing
one, which multiplies the scale of training and even makes
it computationally prohibitive. In this paper, we propose a
novel HPE approach under the invariant risk minimization
(IRM) paradigm. Precisely, we first synthesize 2D poses
from myriad camera settings. We then train our model un-
der the IRM paradigm, which targets at learning a common
optimal model across all camera settings and thus enforces
the model to automatically learn the camera parameters
based on the input data. This allows the model to accurately
infer 3D poses on unseen data by training on only a hand-
ful of samples from each synthesized setting and thus avoid
the unbearable training cost increment. Another appealing
feature of our method is that benefited from the capability
of IRM in identifying the invariant features, its performance
on the seen camera settings is enhanced as well. Compre-
hensive experiments verify the superiority of our approach.
1. Introduction
3D multi-view Human Pose Estimation (HPE) leverages the
camera relationship between multiple viewpoint to mitigate
*Corresponding authorsthe impact of depth uncertainty. Existing methods primar-
ily rely on camera parameters to construct epipolar geomet-
ric constraints between camera viewpoints. Unlike camera-
parameter-required methods, camera-parameter-free meth-
ods can explicitly or implicitly recover camera parameters
during training, thus making them applicable in broader
scenarios where camera parameters are unavailable, such
as HPE in uncontrolled environments or dynamic HPE with
moving cameras. HPCP [21] leverages human pose prior
such as bone length to optimize potential camera parame-
ters. Flex [4] models viewpoint-consistent 3D poses by hi-
erarchical skeletal representation. MTF-Transformer [20]
leverages temporal information to obtain more accurate
camera parameters. Probabilistic Triangulation [11] adopts
Monte Carlo sampling to select the camera parameters.
These methods have achieved commendable results, closely
matching the performance of camera-parameter-free meth-
ods under seen camera setting.
However, when generalized to unseen camera settings,
camera-parameter-free methods exhibit a great performance
drop, in contrast the drop in the camera-parameter-required
methods is negligible [20]. We argue that this discrepancy
primarily stems from the reason that the inferred camera
parameters in the camer-parameter-free methods are always
simply hardcoded into the models during training, i.e., they
are not adaptable to the input in inference. To be precise,
the number of camera viewpoints within existing datasets
is rather limited (typically four or eight), whereas previous
mainstream models require at least four camera viewpoints
as input to address the challenges such as self-occlusion
of the human body or inaccuracies in 2D pose estimation,
making the number of available camera settings no larger
than two. With such extremely limited diversity of cam-
era settings, because of the training imbalance, camera-
parameter-free methods tend to memorize the specific cam-
era settings rather than to generalize to arbitrary settings,
thus significantly hindering their generalization capabilities
across varied camera settings. Our experiments have sub-
stantiated this conjecture. Please refer to Tab. 5 for details.
A natural solution to address the above challenge is to ar-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
2124
tificially synthesize some samples under massive new cam-
era settings and merge them into the training process. Un-
fortunately, to prevent over-fitting the existing camera set-
tings, it can be expected that the number of synthesized
samples for each new camera setting should be compara-
ble with that for the existing one. This would undoubtedly
lead to an unacceptable increment of training time and even
make the training computationally prohibitive.
In this paper, to enhance the estimation on unseen cam-
era settings, we propose an effective synthetic data aug-
mented HPE approach under the invariant risk minimization
(IRM) paradigm, namely PoseIRM1. To be precise, instead
of synthesizing images directly, which is almost infeasible
due to the high complexity and actually unnecessary, we
generate 2D poses from myriad new camera settings by ei-
ther injecting noise into the 2D poses projected from the 3D
ground truth poses or projecting noisy 3D poses into cam-
era planes. A specific proposal is developed to select the
camera settings close to the reality. We then train a 3D HPE
model under the IRM[1] paradigm, which targets at learn-
ing a common optimal model across all camera settings and
thus enforces the model to automatically learn the camera
parameters based on the input data. This framework al-
lows the model to accurately infer 3D poses on the unseen
camera settings by training on only a handful of samples
from each synthesized camera setting and thus avoid both
the training imbalance issue and the unbearable training
cost increment. Another appealing feature of our method
is that benefited from the enhanced capability of IRM in
feature learning, its performance on the seen camera set-
tings is enhanced as well. Moreover, our learning paradigm
can be integrated with general HPE methods flexibly. Com-
prehensive experiments on both Human3.6M and TotalCap-
ture datasets clearly attest to the superiority of our approach.
Specifically, our PoseIRM framework enhances the model’s
generalization capabilities in unseen camera settings. Addi-
tionally, by facilitating the model’s adaptation to varied and
diverse camera settings, the PoseIRM framework further
augments its representational capacity, thereby improving
performance in seen camera settings.
Our contributions can be summarized as follows.
• We propose an effective data augmentation paradigm,
which is comprised of two 2D pose synthesis algorithms
(PSA) and a camera setting selection proposal. It enables
us to generate high-quality 2D-3D pose under the camera
settings similar to reality.
• We propose an Invariant Risk Minimization (IRM) based
approach PoseIRM, which enhances the model’s repre-
sentational capability and generalization ability to adopt
to unseen camera settings by promoting consistent perfor-
mance across all camera settings.
1Code and Supplementary materials are available at:
https://github.com/DoUntilFalse/PoseIRM• Experiments demonstrate that our methods achieve state-
of-the-art (SOTA) results on the Human3.6M and To-
talCapture datasets. Moreover, PoseIRM significantly
reduce the performance drop of camera-parameter-free
methods in unseen camera settings.
2. Related Work
2.1. Monocular 3D HPE
Given that an infinite number of 3D postures can corre-
spond to a single 2D pose, single-view 3D posture esti-
mation is an ill-posed problem. Monocular methods usu-
ally leverage human structure prior [19] to constrain the
range of feasible 3D poses, thus reducing the depth un-
certainty. SemGCN [31] regards the human skeleton as a
graph and uses Graph Convolution Network(GCN) to fuse
features between keypoints. Therefore, the feature fusion is
performed along the human skeleton, and only the features
between adjacent keypoints on the skeleton can be directly
fused. SRNet [28] splits keypoints by limbs to enhance fea-
ture fusion between keypoints within the same limb, and
then recombine them. UGCN [23] proposes a Motion Loss,
which introduces explicit temporal motion constraints for
multi-frame 3D pose estimation to reduce depth uncertainty.
GAST-Net [14] proposes a network structure that combines
GCN and self-attention mechanism, allowing features be-
tween keypoints to be fused not only along the skeleton
graph, but also to adaptively extract connections and fuse
between keypoints.
Transformer has demonstrated its powerful capabilities
across multiple domains [25, 26]. PoseFormer [33] intro-
duce transformer to 3D HPE. Compared with previous GCN
methods, PoseFormer can obtain the attention of different
joints and fuse them flexibly rather than fuse them through
predefined graph, i.e. human skeleton. PoseFormer also
adopt temporal transformer to obtain temporal information.
However, due to the computational complexity of the Trans-
former, which increases quadratically with the number of
tokens, PoseFormer struggles to utilize information from
a greater number of frames. To address this issue, Pose-
FormerV2 [32] and MixSTE [29] have optimized the model
from two different aspects: low-pass filtering in the fre-
quency domain and the separation of individual keypoints,
respectively, making it to accommodate longer sequences.
DiffPose [3], Multi-hypothesis DiffPose [7] and D3DP [18]
leverage Diffusion to generate feasible 3D poses with 2D
input poses as condition.
In summary, monocular methods leverage several human
prior to reduce depth uncertainty. However, due to the lack
of depth information, the error remains significant.
2125
2.2. Multi-view 3D HPE
2.2.1 Multi-view 3D HPE with Camera Parameter
Camera-parameter-required multi-view methods can obtain
camera positions, orientation and the relationships between
cameras through camera parameters. Consequently, they
can utilize epipolar geometry constraints to recover depth
information accurately. For instance, a relative depth rela-
tionship in one viewpoint may manifest as a left-right rela-
tionship in another viewpoint. This implies that the primary
focus of these methods is on how to eliminate the impact
of inaccurate 2D keypoints. This is because, without error
in the 2D results, the 3D poses can be absolutely accurately
determinate using traditional methods (such as Triangula-
tion) when the camera parameters are known. Learnable
Triangulation [10] propose a non-parametric yet differen-
tiable triangulation method, which enables gradient propa-
gation back to the 2D posture estimator, thereby optimiz-
ing its performance. AdaFuse [30] fuses the heatmaps of
different views through epipolar lines to improve the ac-
curacy of 2D keypoint estimation and reduce the impact
of outliers. Canonical Fusion [17] first recovers consis-
tent camera-agnostic pose representations to reduce the key-
points error and then fuses them by an efficient triangula-
tion module DLT. Cross-view Fusion [16] proposes the re-
cursive pictorial structure model, which partitions the space
into several voxels recursively to obtain more accurate 3D
poses by progressively eliminating the influence of out-
liers. DeepFuse [8] leverage IMU-data to refine the pose
results. Epipolar Transformer [6] extends feature fusion
from the vicinity of keypoints to the vicinity of the epipolar
lines, in order to compensate for misalignments caused by
depth uncertainty. In summary, camera-parameter-required
multi-view methods can leverage camera parameter to de-
terminate camera positions and relationships, thus utilizing
epipolar geometry constraints to recover depth information.
2.2.2 Multi-view 3D HPE without Camera Parameter
Without camera parameter, the camera position, orientation,
and relationships need to be regressed by the model. The
error in regressing camera parameter makes 3D HPE even
more challenging.
HPCP [21] leverages human pose prior such as bone
length to optimize potential camera parameters and to
predict 3D poses more accurately. FLEX [4] leverage
viewpoint-independent skeleton representation(bones and
angles) to model a consistent human pose between differ-
ent views. However, due to error accumulation of hierarchi-
cal representation, large errors is presented at terminal key-
points like wrist and ankle. MTF-Transformer [20] leverage
transformation matrix regression to force model to learn the
camera parameters. Probabilistic Triangulation [11] main-tain camera distribution by computing the posterior proba-
bility of the camera through Monte Carlo sampling.
However, the number of cameras in the dataset is quite
limited (typically 4 or 8), with fixed positions and orienta-
tions. Moreover, multi-view methods often take four-view
2d poses as input to mitigate self-occlusion of the human
body. This results in the very limited diversity of camera
setting, typically restricted to one or two types of camera
setting. Under such camera setting with very limited diver-
sity, models tend to memorize the seen settings instead of
generalizing to arbitrary camera setting. Thus, we propose
a pose synthesis algorithm and a IRM-based pose estima-
tion method, namely PoseIRM, which demonstrate strong
generalization capabilities across various camera settings.
2.3. Invariant Risk Minimization
IRM [1] is an emerging machine learning paradigm to en-
able the learned model generalize on the unseen data with
distributional shift. The basic idea is to learn an invariant
feature representation on the datasets drawn from multiple
environments, in the sense that with this representation one
is able to learn a common classifier working well across all
these environments, and thus the learned model can be ex-
pected to generalize well on the data with unseen distribu-
tions. Based on this idea, IRM can be naturally phrased as a
bi-leveled optimization problem (please refer to Eqn.(8) for
details), however, it is challenging to optimize due to the
high computational complexity. Therefore, more practical
version of IRM are proposed in the recent studies [13, 34],
by relaxing the bi-leveled optimization problem into a reg-
ularized minimization task. Promising empirical results are
repeatedly reported in the literature [1, 2, 12, 13, 27]. We
notice that IRM have not been introduced to the area of pose
estimation. One of the main contribution of this paper is
that by proposing a novel synthetic data augmented HPE
approach under IRM paradigm, we show that IRM can ef-
fectively solve the challenges of HPE in generalization on
unseen camera setting and training with imbalanced data.
3. Method
In this section, we first introduce our data augmentation
paradigm, which is comprised of one camera setting selec-
tion proposal and two 2D pose synthesis algorithms. Then
we present our pose estimation approach PoseIRM.
3.1. Camera Setting Selection Proposal
In consideration of authenticity, simple randomizing of
camera parameters is not reasonable. Therefore, we adhere
to the following principles for selecting camera settings:
1. Reasonable roll angle. That is y-axis of the captured im-
ages should be nearly parallel to the ground.
2. Reasonable pitch angle. The angle between the camera’s
optical axis and the ground should not be too large. In
2126
Figure 1. The framework of our PoseIRM. We first generate multiple camera setting Si, then PoseIRM extract feature from each view and
then fuse them to obtain multi-view 3D poses and camera parameters. IRM regularizer is employed to force model to perform consistantly
across different camera settings. MPJPE loss and camera loss are employed to supervise the model in regressing 3D poses and camera
parameters, respectively.
other words, cameras should capture the subject from the
sides rather than from directly above or below.
3. Distinctiveness within the same camera setting: Differ-
ent viewpoints under the same camera setting should ex-
hibit significant differences.
4. Uniform sampling of camera settings. We uniformly
sample the camera settings from all the candidates sat-
isfying the above 3 principles.
Following the above principles, we present a formalized
description of the candidate camera setting S={Vi}V
i=1
withVviewpoints:
V= (R,I,T,D), (1)
where RandTrepresent the orientation and translation of
the camera, while the IandDare the intrinsic parameter
matrix and the camera distortion.
To prevent ambiguity, let’s first clarify the camera and
world coordinate systems. In the camera coordinate system
(Xc, Yc, Zc, Oc), the positive directions of the x-axis and
y-axis correspond to the image’s top-to-bottom and left-to-
right directions, respectively. The positive direction of the
z-axis points from the camera’s focal point along the prin-
cipal axis toward the camera plane. The origin Ocis the
focal point of the camera. In the world coordinate system
(Xw, Yw, Zw, Ow), the x-y plane is parallel to the ground,
and z-axis direction is perpendicular to the ground, pointing
upwards. The positive x-axis direction and the origin Owis
aligned with that of the given dataset.
The camera’s orientation Ris expressed using azimuth,
pitch, and roll angles, namely α, β, γ . The azimuth angleαis the angle between Xwand the projection of Zconto
theXw−Ywplane. The pitch angle βis the angle between
theZcandXw−Ywplanes. The roll angle γis the angle
between YcandXw−Ywplanes.
Adhering to the aforementioned principles 1, 2, and 4,
we can randomly sample the camera orientation as follows:
R=Euler zxy(γ, β, α ) (2)
where Euler zxyis a function which converts Euler angles
to rotation matrix, α∼ U[−π, π), β∼ U[−π/6, π/6], and
γ∼ U[−π/N, π/N ]withN= 36 andUbeing the uniform
distribution.
We generate the intrinsic matrix Iusing camera focal
lengths and centers similar to those in the given dataset, i.e.,
I=
Fx0Cx
0FyCy
0 0 1
, (3)
where FandCare sampled from the two dimensional nor-
mal distributions N(¯F, σf)andN(¯C, σc)separately. Their
mean values ¯Fand¯Care the averaged camera focal length
and centre of the given dataset, and σ2
fandσ2
care the sam-
ple variance. We also sample the distortion Daround the
average value of that in the given dataset.
Given that the camera orientation is already determined,
the translation Tcan be decomposed as the look-at point
PlookAt and the distance Dbetween camera and the look-at
point. The look-at point PlookAt is sampled around the cen-
troidMof all the keypoints, i.e., PlookAt ∼ N(M, σp)⊂
R3, where the standard deviation σpis set to be 0.1.
2127
We let the distance Dbe slightly greater than the mini-
mum distance Dminto keep all keypoints within the screen.
That is,
Dmin= max
Ji∈J(max( |FxJi
x|,|FyJi
y|)− Ji
z), (4)
D∼ U(1
1−λscaleDmin,1
1−λscaleDmin+λD),(5)
where Jiis the location of the i-th keypoint from the key-
points set Jin the given dataset, which is rotated into cam-
era view, λscale is the scaling factor to make sure that the
entire human body rather than the keypoint is within the
screen. For the detailed derivation of the formula, please
refer to the supplementary materials. Finally, the camera
translation can be expressed as,
T=
0
0
D
− RPlookAt . (6)
To satisfy principle 3, we let each two views in the same
camera setting has a significant difference in azimuth an-
gles, we stipulate that,
|αi−αj| ≥ϵ,∀i, j, i̸=j, (7)
where ϵpresents the minimum threshold.
3.2. Pose Synthesis Algorithm
Instead of synthesizing images directly, which is almost in-
feasible due to the high complexity and actually unneces-
sary, we aim to generate 2D poses from myriad camera set-
tings. Our basic idea is to project 3D ground-truth poses
into synthesized camera viewpoints to build the 2D ground-
truth poses (2DGT) in the new viewpoint via classic algo-
rithm. To make the generated poses close to the real data,
i.e., the poses detected by 2D pose estimators from real im-
ages, we propose the following two distinct Pose Synthesis
Algorithms (PSA).
Our first algorithm, known as Distribution-Aware PSA
(DAPSA), first collects all 2D detection errors ( E(j)) for
each joint (e.g., wrists and shoulders) and model these er-
rors using Student’s t-distribution. Subsequently, DAPSA
samples random biases from these t-distributions and apply
them to the 2D ground-truth pose obtained through projec-
tion from the respective viewpoint. The synthesized results
exhibit a consistent error distribution with the real data.
Our second algorithm, Multi-view Consistent PSA
(MVCPSA), first triangulates the 2D detection results into
3D synthesized poses. The discrepancies between these
synthesized 3D poses and the 3D ground truth arise from
the cumulative effects of deviations in multiple 2D poses.
Subsequently, we project these synthesized 3D poses onto a
new viewpoint to generate 2D poses with similar detection
errors in the real data.As a result, we obtain samples of (2D, 3D) paired poses
in the new viewpoint.
3.3. IRM framework
LetE:={S1,S2,···,SE}be the set of E camera setting.
Using the framework of IRM, we can naturally phrase our
training problem as follows:
min
ω:={v,Φ}R(ω) :=X
e∈ERe(v,Φ) (8)
s.t.v∈arg min
veRe(ve,Φ),∀e∈ E, (9)
where Reis the weighted sum of 3D poses error LMPJPE
and camera parameters error Lcam (See in supplementary
materials) of the data from the environment e,vandΦare
the parameters of the regression heads and the backbone,
i.e., feature extract and fusion modules. Intuitively, the in-
ner loop enforce the learned features would enable common
optimal pose regression heads for all camera setting, while
the outer loop optimize the overall accuracy.
Notice that direct solving the above bi-leveled optimiza-
tion problem is challenging, following the recent studies
[1, 13] in IRM, we relaxed it into the following one:
min
ω˜R(ω) :=X
e∈ERe(ω) +λJ(ω), (PoseIRM)
where the regular J(ω)takes the form of
J(ω) := Var[LMPJPE (ω)] + Var[LCam(ω)] (10)
with Var [·]being the variance promoting similar perfor-
mance for all environments.
Discussion. Our IRM based approach PoseIRM has the
following three appealing features. One is that it treats each
environment equally and enforce the model to achieve opti-
mal performance in all environments, thus naturally it natu-
rally addressed the training imbalance issue. To be precise,
it allows the model to accurately infer 3D poses by training
on only a handful of samples from each synthesized cam-
era setting and thus avoid the unbearable training cost in-
crement. The second is our PoseIRM framework aims at
learn a common optimal model across all camera settings,
enhancing the model’s generalization capabilities in unseen
camera settings. Additionally, by facilitating the model’s
adaptation to varied and diverse camera settings, PoseIRM
further augments its representational capability, thereby im-
proving performance in seen camera settings.
3.4. 2D-3D Lifting Network
We select a simple baseline as our 2D-3D lifting network
as follows. We adopt PoseFormer [33] as single-view fea-
ture extractor, which take 2D poses as input and obtain
single-view pose feature from multiple frame. Then we
2128
Figure 2. Visualization of the results regressed by the model with/without IRM in the unseen camera setting of TotalCapture dataset.
leverage a two-layer transformer decoder to fuse the four
single-view pose feature into multi-view pose feature. Af-
ter that, two heads are appended to the network. One is
for 3D pose regression and the other is to predict camera
viewpoint V. Each head adopt three-layer FCN to regress
the 3D poses/camera parameters. In consideration of the
degrees of freedom, we opt to represent Rusing a ro-
tation vector. Specifically, the rotation vector is denoted
as[x∗θ, y∗θ, z∗θ]T, where [x, y, z ]Tis a unit vec-
tor representing the rotation axis and θsignifies the ro-
tation angle. Regarding the camera intrinsic parameter,
we employ (fx, fy, cx, cy )to represent the intrinsic ma-
trixI. As for the lens distortion parameters, we utilize
(k1, k2, k3, p1, p2)to denote the distortion parameters D.
4. Experiment
We evaluate the performance of our method on the Hu-
man3.6M [9] and TotalCapture [22] datasets. We also pro-
vide comprehensive ablation studies to demonstrate the ef-
fectiveness of our PSA and our IRM architecture. Addition-
ally, we present a camera parameter regression experiment
to verify our hypothesis.
4.1. Experimental Settings
Datasets. We evaluate our PoseIRM on both Human3.6M
and TotalCapture datasets. Human3.6M dataset is the most
widely used 3D Human Pose Estimation dataset, contain-
ing over 3 million frames of images with 11 human sub-
jects performing 15 different types of actions captured from
four different camera viewpoints. The 3D poses are cap-
tured by a motion capture system. Following the settings of
previous methods [15, 20, 33], we use 17 keypoints to rep-
resent the human pose and use subjects S1, S5, S6, S7, and
S8 for training, and subjects S9 and S11 for test. The To-
talCapture dataset utilizes 8 completely synchronized cam-
eras to collect four types of actions (Rom, Acting, Walk-
ing, and Freestyle) from five subjects, with each action
repeated three times, totaling approximately one millionframes. We fully follow the experimental settings of the
MTF-Transformer [20], where cameras 1, 3, 5, and 7 are
used in both the training and test sets, while cameras 2, 4,
6, and 8 appear only in the test set.
Evaluation Metrics. Mean Per Joint Position Error
(MPJPE) and Procrustes-aligned MPJPE (P-MPJPE) [24]
are used as the evaluation metrics.
Data Augmentation. We generate 7500 different cam-
era settings for Human3.6M datasets and 8100 different
camera settings for TotalCapture. The 3D poses of each
action clip is processed using a different camera setting for
100 epochs. To verify our method can infer the cam-
era setting parameter implicitly in inference instead of
memorize them, the unseen camera settings in test time
are excluded from the generated settings for training.
4.2. Quantitative Analysis
We report the comparison between our method and other
methods with/without camera parameter. As shown in
Tab. 1, our method has achieved state-of-the-art (SOTA)
performance on Human3.6M. This underscores the effec-
tiveness of our IRM framework in enhancing the model’s
generalization across various camera settings. It forces the
model to extract superior features rather than memorizing a
specific camera setting, thereby improving the model’s rep-
resentational capacity. We also report the performance of
the methods with/without camera parameter on TotalCap-
ture in Tab. 2. The experimental results indicate that our
method exhibits a significant advantage in both seen and un-
seen camera settings. This further underscores the effective-
ness of our IRM framework in enhancing the model’s rep-
resentational capacity. Moreover, our method demonstrates
a lower performance drop in unseen camera setting com-
pared to seen camera setting (3.8 v.s. 11.9), approaching
the performance drop of camera-parameter-required meth-
ods (3.8 v.s. 1.9). Taking into account that the camera-
parameter-required methods obtain the camera parameters
for new viewpoints, essentially granting it prior exposure to
2129
Method Input Setting Dir. Disc. Eat. Greet Phone Photo Pose Purch. Sit. SitD. Smoke Wait WalkD. Walk WalkT. Avg.
Monocular methods
SRNet (CPN, T = 243) 46.6 47.1 43.9 41.6 45.8 49.6 46.5 40.0 53.4 61.1 46.1 42.6 43.1 31.5 32.6 44.8
UGCN (CPN, T = 96) 40.2 42.5 42.6 41.1 46.7 56.7 41.4 42.3 56.2 60.4 46.3 42.2 46.2 31.7 31.0 44.5
PoseFormer (CPN, T = 81) 41.5 44.8 39.8 42.5 46.5 51.6 42.1 42.0 53.3 60.7 45.5 43.3 46.1 31.8 32.2 44.3
MHFormer (CPN, T = 351) 39.2 43.1 40.1 40.9 44.9 51.2 40.6 41.3 53.5 60.3 43.7 41.1 43.8 29.8 30.6 43.0
Occlusion-aware Network (CPN, T = 128) 38.3 41.3 46.1 40.1 41.6 51.9 41.8 40.9 51.5 58.4 42.2 44.6 41.7 33.7 30.1 42.9
Multi-view Methods with Camera Parameter
DeepFuse (*, T = 1) 26.8 32.0 25.6 52.1 33.3 42.3 25.8 25.9 40.5 76.6 39.1 54.5 35.9 25.1 24.2 37.5
Canonical Fusion (*, T = 1) 27.3 32.1 25.0 26.5 29.3 35.4 28.8 31.6 36.4 31.7 31.2 29.9 26.9 33.7 30.4 30.2
Epipolar transformers (*, T = 1) 25.7 27.7 23.7 24.8 26.9 31.4 24.9 26.5 28.8 31.7 28.2 26.4 23.6 28.3 23.5 26.9
Crossview Fusion (*, T = 1) 24.0 26.7 23.2 24.3 24.8 22.8 24.1 28.6 32.1 26.9 31.0 25.6 25.0 28.0 24.4 26.2
TransFusion (*, T = 1) 24.4 26.4 23.4 21.1 25.2 23.2 24.7 33.8 29.8 26.4 26.8 24.2 23.2 26.1 23.3 25.8
Learnable Triangulation (*, T = 1) 19.9 20.0 18.9 18.5 20.5 19.4 18.4 22.1 22.5 28.7 21.2 20.8 19.7 22.1 20.2 20.8
AdaFuse (*, T = 1) 17.8 19.5 17.6 20.7 19.3 16.8 18.9 20.2 25.7 20.1 19.2 20.5 17.2 20.5 17.3 19.5
MvP (*, T = 1) - - - - - - - - - - - - - - - 18.6
MTF-Transformer+ (CPN, T = 27) 23.4 25.2 23.1 24.4 27.4 28.5 22.8 25.2 28.7 36.2 25.9 23.6 26.6 22.6 22.7 25.8
Multi-view Methods without Camera Parameter
FLEX (ResNet152, T = 27) 23.1 28.8 26.8 28.1 31.6 37.1 25.7 31.4 36.5 39.6 35.0 29.5 35.6 26.8 26.4 30.9
FLEX (CPN, T = 27) - - - - - - - - - - - - - - - 31.7
Probabilistic Triangulation (*, T = 1) 24.0 25.4 26.6 30.4 32.1 20.1 20.5 36.5 40.1 29.5 27.4 27.6 20.8 24.1 22.0 27.8
MTF-Transformer (CPN, T = 27) 23.1 25.4 24.7 24.5 27.9 28.3 23.9 24.6 30.7 35.7 25.8 24.2 28.4 22.8 23.1 26.2
Ours (CPN, T = 27) 22.2 24.6 22.9 23.2 26.0 27.0 22.2 23.7 29.3 33.6 25.6 22.8 25.8 22.5 22.5 25.1
Table 1. MPJPE on Human3.6M for both camera-parameter-required and camera-parameter-free methods. CPN is adopted as the 2D pose
estimator. * means it’s an image-to-3d method and no 2D pose estimator is used. T is the number of input frames.
these unseen camera settings, it achieves a very low perfor-
mance drop. However, our method without camera parame-
ter can also exhibit a comparatively low performance drop,
indicating that it has already achieved excellent generaliza-
tion across different camera settings.
4.3. Generalization
Given the constraint that the Human3.6M dataset only en-
compasses four camera viewpoints and that prevalent multi-
view methods typically accept four viewpoints as input, it
becomes challenging to solely rely on Human3.6M for val-
idating the model’s generalizability across different camera
setting as there are only one setting. To address this, we
treat Human3.6M dataset as one camera setting, viewpoint
1,3,5,7 and viewpoint 2,4,6,8 of the TotalCapture dataset as
two other camera settings. Thus, we train our model on the
Human3.6M dataset and subsequently freeze the model but
only fine-tune the pose regression head on viewpoints 1, 3,
5, and 7 of the TotalCapture dataset to adapt to the distinct
data initialization of TotalCapture. For instance, the axis
pointing vertically upwards from the ground in the Total-
Capture dataset differs from that in the Human3.6M dataset.
Moreover, to align the skeletal structure of the TotalCapture
dataset with that of Human3.6M, we introduced an addi-
tional virtual ’Thorax’ point to the TotalCapture skeleton.
This point is positioned at the midpoint between the Neck
and Spine. After fine-tuning the regression head on the To-
talCapture dataset, we evaluate the model’s generalization
capabilities on viewpoints 2, 4, 6, and 8 of TotalCapture. As
shown in Tab. 2 with the §mark, our method demonstrates
excellent generalizability, which strongly indicates its ca-
pability to extract valuable features, enabling the model to
swiftly adapt to new camera settings.
Figure 3. Histogram of the error distribution, which includes the
model’s performance in Seen/Unseen camera settings with and
without Invariant Risk Minimization framework. To eliminate the
effects of cross-subject and cross-action variations, we give the
results of all samples with seen subjects and seen actions (i.e., ex-
cluding freestyle).
4.4. Ablation Study
We performed all our ablation study in the Total Capture
dataset. We conducted a comparison of the two PSA meth-
ods on the unseen camera setting, and the results are pre-
sented in Tab. 3. We can observe that both of our PSA meth-
ods have a substantial impact (16.6 and 20.7 vs 27.2). Fur-
thermore, MVCPSA outperforms DAPSA (16.6 vs 20.7).
This supports our hypothesis that the errors of 2D pose esti-
mators between different viewpoints are not independently
and identically distributed but rather exhibit correlations.
We also conducted ablation experiments on the IRM
framework, and the results are presented in Tab. 4 and
Fig. 3. In Table 4, we provide the performance of our
method with and without IRM in both Seen and Unseen
camera settings. The results show that using IRM im-
proves the model’s performance in both Seen and Unseen
settings. However, we believe that the reasons behind the
improvements in these two settings are different. We at-
2130
MethodSeen Cameras(1,3,5,7) Unseen Cameras(2,4,6,8)
Mean Seen Subjects(S1,S2,S3) Unseen Subjects(S4,S5)MeanSeen Subjects(S1,S2) Unseen Subjects(S4,S5)MeanW2 FS3 A3 W2 FS3 A3 W2 FS3 A3 W2 FS3 A3
Crossview Fusion(*) † 19.0 28.0 21.0 32.0 54.0 33.0 29.0 - - - - - - - -
Canonical Fusion(*) † 10.6 30.4 16.3 27.0 65.0 34.2 27.5 22.4 47.1 27.8 39.1 75.7 43.1 38.2 32.9
MTF-Transformer+(Res101) †10.7 26.5 16.7 27.4 49.4 34.1 25.1 13.9 29.2 18.1 29.2 49.5 35.6 27.0 26.1
FLEX(Res101) 33.2 81.0 34.2 38.3 123.8 59.5 49.4 109.3 152.1 105.3 114.3 175.5 122.5 125.4 87.4
MTF-Transformer(Res101) § 13.2 31.8 17.1 26.6 51.8 33.0 30.5 34.0 68.0 39.6 47.0 90.7 55.1 56.1 43.3
MTF-Transformer(Res101) 9.3 26.5 14.5 26.7 53.1 33.8 24.7 23.7 40.3 27.4 37.0 61.8 42.9 36.6 30.7
Ours(Res101) § 6.65 19.9 9.47 16.6 29.6 20.1 17.8 10.2 38.3 11.6 16.7 41.9 21.1 23.6 20.7
Ours(Res101) 5.13 14.0 7.47 14.3 25.5 18.0 14.7 6.8 24.3 9.1 14.0 35.3 19.0 18.5 16.6
Table 2. MPJPE on TotalCapture [9] dataset. We adopt Res101 [5] as the 2D pose estimator for fair comparison. †marks camera-
parameter-required methods. §presents that we only train the regression head on TotalCapture.
Method MPJPE PMPJPE
W/o PSA 27.2 22.0
DAPSA 18.7 13.2
MVCPSA 16.6 12.6
Table 3. Ablation study of PSA methods. Best in Bold .
tribute the enhancement in the Unseen camera setting pri-
marily to IRM improving the model’s generalization ca-
pabilities, while the improvement in the Seen camera set-
ting is attributed to IRM enhancing the model’s feature ex-
traction capabilities. As demonstrated in Tab. 4, there is a
gradual enhancement in the model’s performance under un-
seen camera settings as the Penalty term is incrementally
increased. This trend underscores the efficacy of Invariant
Risk Minimization (IRM) in augmenting the model’s gen-
eralization capabilities to novel camera configurations. The
model achieves its best performance in seen camera set-
tings when the penalty term is optimally balanced, as this
facilitates the enhancement of the model’s feature repre-
sentation by forcing model to adapt to more complex and
diverse camera settings. However, when the penalty term
becomes sufficiently large, the model, in striving for strong
consistency across different camera settings, slightly sacri-
fices performance in settings where it previously excelled,
in order to align with the unseen camera settings.
4.5. Camera Parameter Regression
To validate our hypothesis that a limited numver of cam-
era settings leads models to memorize specific camera set-
tings rather than generalize to arbitrary settings, we con-
duct a camera parameter regression experiment. We con-
ducted comparative analyses with the single-view method
PoseFormer and the multi-view method MTF-Transformer.
Their method are augmented with an additional camera pa-
rameter regression head same as ours. Subsequently, we
train the entire model on the TotalCapture dataset. The per-
formance of these methods in regressing camera parameters
was then evaluated under both seen and unseen camera set-
tings, with the results detailed in Tab. 5. The results indicate
that in the absence of PSA, i.e., under limited camera set-
tings, there is a consistently higher regression error acrossthe models. Notably, under unseen camera settings, the er-
ror without PSA approaches half of the disparity observed
between seen and unseen camera settings, which is 30◦,7◦,
and 2m. It implies that their methods only memorize the
seen camera settings.
MethodSeen camera setting Unseen camera setting
MPJPE PMPJPE MPJPE PMPJPE
W/o IRM 16.8 13.2 22.4 16.8
IRM(P=1) 13.2 10.0 20.2 15.2
IRM(P=100) 14.7 11.2 18.4 13.8
Table 4. Ablation study of IRM framework. Best in Bold .
MethodsSeen Camera setting Unseen Camera setting
Axis Angle Trans. Axis Angle Trans.
PoseFormer 20◦11◦1.1m 28◦19◦1.5m
MTF-Transformer 6◦5◦0.3m 20◦15◦0.8m
Ours without PSA 4◦2◦0.2m 15◦10◦0.7m
Ours 1◦1◦0.04m 2◦3◦0.1m
Table 5. The results of camera parameter regression. Best in Bold .
Axis, Angle and Trans. represents the average error of rotation
axis, rotation angle and translation.
5. Conclusions
In this paper, to enhance the estimation on unseen camera
settings, we propose an effective synthetic data augmented
3D human pose estimation approach under the invariant risk
minimization paradigm. Our method PoseIRM allows the
model to accurately infer 3D poses on unseen data by train-
ing on only a handful of samples from each synthesized set-
ting. Benefited from the strong feature learning capability
of IRM, PoseIRM further improves its performance in seen
camera settings. Extensive experimental results demon-
strate the superiority of our method over the baselines.
6. Acknowledgments
This work was supported by National Natural Science Fund
of China (62176064). The computations in this research
were performed on the CFFF platform of Fudan University.
2131
References
[1] Martin Arjovsky, L ´eon Bottou, Ishaan Gulrajani, and David
Lopez-Paz. Invariant risk minimization. arXiv preprint
arXiv:1907.02893 , 2019. 2, 3, 5
[2] Shiyu Chang, Yang Zhang, Mo Yu, and Tommi Jaakkola. In-
variant rationalization. In International Conference on Ma-
chine Learning , pages 1448–1458. PMLR, 2020. 3
[3] Jia Gong, Lin Geng Foo, Zhipeng Fan, Qiuhong Ke, Hos-
sein Rahmani, and Jun Liu. Diffpose: Toward more reliable
3d pose estimation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
13041–13051, 2023. 2
[4] Brian Gordon, Sigal Raab, Guy Azov, Raja Giryes, and
Daniel Cohen-Or. Flex: Extrinsic parameters-free multi-
view 3d human motion reconstruction. In Computer Vision–
ECCV 2022: 17th European Conference, Tel Aviv, Israel,
October 23–27, 2022, Proceedings, Part XXXIII , pages 176–
196. Springer, 2022. 1, 3
[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. CoRR ,
abs/1512.03385, 2015. 8
[6] Yihui He, Rui Yan, Katerina Fragkiadaki, and Shoou-I Yu.
Epipolar transformers. In Proceedings of the ieee/cvf con-
ference on computer vision and pattern recognition , pages
7779–7788, 2020. 3
[7] Karl Holmquist and Bastian Wandt. Diffpose: Multi-
hypothesis human pose estimation using diffusion models.
InProceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV) , pages 15977–15987, 2023. 2
[8] Fuyang Huang, Ailing Zeng, Minhao Liu, Qiuxia Lai, and
Qiang Xu. Deepfuse: An imu-aware network for real-time
3d human pose estimation from multi-view image. In Pro-
ceedings of the IEEE/CVF Winter Conference on Applica-
tions of Computer Vision , pages 429–438, 2020. 3
[9] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian
Sminchisescu. Human3. 6m: Large scale datasets and pre-
dictive methods for 3d human sensing in natural environ-
ments. IEEE transactions on pattern analysis and machine
intelligence , 36:1325–1339, 2013. 6, 8
[10] Karim Iskakov, Egor Burkov, Victor Lempitsky, and Yury
Malkov. Learnable triangulation of human pose. In Proceed-
ings of the IEEE/CVF international conference on computer
vision , pages 7718–7727, 2019. 3
[11] Boyuan Jiang, Lei Hu, and Shihong Xia. Probabilistic tri-
angulation for uncalibrated multi-view 3d human pose esti-
mation. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 14850–14860, 2023. 1,
3
[12] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Do-
main extrapolation via regret minimization. arXiv preprint
arXiv:2006.03908 , 2020. 3
[13] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen,
Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi
Le Priol, and Aaron Courville. Out-of-distribution general-
ization via risk extrapolation (rex). In International Confer-
ence on Machine Learning , pages 5815–5826. PMLR, 2021.
3, 5[14] Junfa Liu, Juan Rojas, Yihui Li, Zhijun Liang, Yisheng
Guan, Ning Xi, and Haifei Zhu. A graph attention spatio-
temporal convolutional network for 3d human pose estima-
tion in video. In 2021 IEEE International Conference on
Robotics and Automation (ICRA) , pages 3374–3380. IEEE,
2021. 2
[15] Dario Pavllo, Christoph Feichtenhofer, David Grangier, and
Michael Auli. 3d human pose estimation in video with tem-
poral convolutions and semi-supervised training. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 7753–7762, 2019. 6
[16] Haibo Qiu, Chunyu Wang, Jingdong Wang, Naiyan Wang,
and Wenjun Zeng. Cross view fusion for 3d human pose
estimation. In Proceedings of the IEEE/CVF international
conference on computer vision , pages 4342–4351, 2019. 3
[17] Edoardo Remelli, Shangchen Han, Sina Honari, Pascal Fua,
and Robert Wang. Lightweight multi-view 3d pose estima-
tion through camera-disentangled representation. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 6040–6049, 2020. 3
[18] Wenkang Shan, Zhenhua Liu, Xinfeng Zhang, Zhao Wang,
Kai Han, Shanshe Wang, Siwei Ma, and Wen Gao.
Diffusion-based 3d human pose estimation with multi-
hypothesis aggregation. arXiv preprint arXiv:2303.11579 ,
2023. 2
[19] Mingyi Shi, Kfir Aberman, Andreas Aristidou, Taku Ko-
mura, Dani Lischinski, Daniel Cohen-Or, and Baoquan
Chen. Motionet: 3d human motion reconstruction from
monocular video with skeleton consistency. ACM Transac-
tions on Graphics (TOG) , 40:1–15, 2020. 2
[20] Hui Shuai, Lele Wu, and Qingshan Liu. Adaptive multi-view
and temporal fusing transformer for 3d human pose estima-
tion. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 2022. 1, 3, 6
[21] Kosuke Takahashi, Dan Mikami, Mariko Isogawa, and
Hideaki Kimata. Human pose as calibration pattern: 3d hu-
man pose estimation with multiple unsynchronized and un-
calibrated cameras. In 2018 IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition Workshops (CVPRW) ,
pages 1856–18567, 2018. 1, 3
[22] Matt Trumble, Andrew Gilbert, Charles Malleson, Adrian
Hilton, and John Collomosse. Total capture: 3d human pose
estimation fusing video and inertial sensors. In 2017 British
Machine Vision Conference (BMVC) , 2017. 6
[23] Jingbo Wang, Sijie Yan, Yuanjun Xiong, and Dahua Lin.
Motion guided 3d pose estimation from videos. In Computer
Vision–ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part XIII 16 , pages
764–780. Springer, 2020. 2
[24] Jinbao Wang, Shujie Tan, Xiantong Zhen, Shuo Xu, Feng
Zheng, Zhenyu He, and Ling Shao. Deep 3d human pose
estimation: A review. Computer Vision and Image Under-
standing , 210:103225, 2021. 6
[25] Yibin Wang, Yuchao Feng, Jie Wu, Honghui Xu, and Jian-
wei Zheng. Ca-gan: Object placement via coalescing at-
tention based generative adversarial network. In 2023 IEEE
International Conference on Multimedia and Expo (ICME) ,
pages 2375–2380. IEEE, 2023. 2
2132
[26] Yibin Wang, Haixia Long, Qianwei Zhou, Tao Bo, and Jian-
wei Zheng. Plsnet: Position-aware gcn-based autism spec-
trum disorder diagnosis via fc learning and rois sifting. Com-
puters in Biology and Medicine , 163:107184, 2023. 2
[27] Chuanlong Xie, Fei Chen, Yue Liu, and Zhenguo Li. Risk
variance penalization: From distributional robustness to
causality. arXiv e-prints , pages arXiv–2006, 2020. 3
[28] Ailing Zeng, Xiao Sun, Fuyang Huang, Minhao Liu, Qiang
Xu, and Stephen Lin. Srnet: Improving generalization in 3d
human pose estimation with a split-and-recombine approach.
InComputer Vision–ECCV 2020: 16th European Confer-
ence, Glasgow, UK, August 23–28, 2020, Proceedings, Part
XIV 16 , pages 507–523. Springer, 2020. 2
[29] Jinlu Zhang, Zhigang Tu, Jianyu Yang, Yujin Chen, and Jun-
song Yuan. Mixste: Seq2seq mixed spatio-temporal encoder
for 3d human pose estimation in video. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 13232–13242, 2022. 2
[30] Zhe Zhang, Chunyu Wang, Weichao Qiu, Wenhu Qin, and
Wenjun Zeng. Adafuse: Adaptive multiview fusion for accu-
rate human pose estimation in the wild. International Jour-
nal of Computer Vision , 129:703–718, 2021. 3
[31] Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, and
Dimitris N Metaxas. Semantic graph convolutional net-
works for 3d human pose regression. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 3425–3435, 2019. 2
[32] Qitao Zhao, Ce Zheng, Mengyuan Liu, Pichao Wang, and
Chen Chen. Poseformerv2: Exploring frequency domain for
efficient and robust 3d human pose estimation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 8877–8886, 2023. 2
[33] Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang,
Chen Chen, and Zhengming Ding. 3d human pose estima-
tion with spatial and temporal transformers. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 11656–11665, 2021. 2, 5, 6
[34] Xiao Zhou, Yong Lin, Weizhong Zhang, and Tong Zhang.
Sparse invariant risk minimization. In International Con-
ference on Machine Learning , pages 27222–27244. PMLR,
2022. 3
2133
