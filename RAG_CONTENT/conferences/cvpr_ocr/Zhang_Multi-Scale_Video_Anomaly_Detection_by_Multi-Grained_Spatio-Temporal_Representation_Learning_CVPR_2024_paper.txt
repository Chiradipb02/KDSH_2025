Multi-Scale Video Anomaly Detection by Multi-Grained Spatio-Temporal
Representation Learning
Menghao Zhang, Jingyu Wang*, Qi Qi, Haifeng Sun, Zirui Zhuang, Pengfei Ren, Ruilong Ma, Jianxin Liao
State Key Laboratory of Networking and Switching Technology,
Beijing University of Posts and Telecommunications
zhangmenghao,wangjingyu,qiqi8266,hfsun,zhuangzirui,rpf,maruilong@bupt.edu.cn; jxlbupt@gmail.com
Abstract
Recent progress in video anomaly detection suggests that
the features of appearance and motion play crucial roles in
distinguishing abnormal patterns from normal ones. How-
ever, we note that the effect of spatial scales of anomalies
is ignored. The fact that many abnormal events occur in
limited localized regions and severe background noise in-
terferes with the learning of anomalous changes. Mean-
while, most existing methods are limited by coarse-grained
modeling approaches, which are inadequate for learning
highly discriminative features to discriminate subtle differ-
ences between small-scale anomalies and normal patterns.
To this end, this paper address multi-scale video anomaly
detection by multi-grained spatio-temporal representation
learning. We utilize video continuity to design three proxy
tasks to perform feature learning at both coarse-grained
and fine-grained levels, i.e., continuity judgment, discon-
tinuity localization, and missing frame estimation. In par-
ticular, we formulate missing frame estimation as a con-
trastive learning task in feature space instead of a recon-
struction task in RGB space to learn highly discriminative
features. Experiments show that our proposed method out-
performs state-of-the-art methods on four datasets, espe-
cially in scenes with small-scale anomalies.
1. Introduction
Video Anomaly Detection (V AD) is dedicated to detecting
anomalous events in videos with wide applications in public
safety and intelligent surveillance. The primary challenge
of V AD is the sparsity of abnormal samples, limiting the di-
rect learning of abnormal patterns from the available data.
Therefore, most previous V AD studies can be divided into
the weakly supervised category [9, 29, 36, 40, 43, 50, 53]
that learns with video-level annotations, or the category
only learning from normal data [5, 23, 24, 39, 46]. Our work
**Corresponding author
濷 濷 濷
PMem
RoadMap
OursGT
Partial detectionSmall-scale
Partial detection
Detect all anomalies 
completelyLarge-scale
Video Sequence Anomaly Score Curve Anomaly-identified SectionFigure 1. Examples of small-scale anomalies and comparisons
of results from different methods. From top to bottom: video
sequence, ground truth (green regions are abnormal), result of
PMem [31] (reconstruction-based method), result of RoadMap
[41] (prediction-based method) and result of ours.
focuses on the latter, which typically models the spatio-
temporal features of the normal pattern, while samples that
do not conform to the model are labeled as anomalies.
In the same scene, the appearance and motion of fore-
ground objects are the main differences between normal
and abnormal patterns, and also the crucial features that the
model needs to learn. We note that the effect of the spatial
scale of the anomalies on feature learning is ignored. As il-
lustrated in Fig. 1, many abnormal events occur in a limited
region with only subtle differences from the normal pattern
while background occupying a larger portion of the frame
is the same. Therefore, severe background noise interferes
with the feature learning of anomalous changes. Apart from
small-scale anomalies, there are also abnormal events that
span almost the entire frame, such as traffic accidents. Such
changes in the scale of anomalies require the model to re-
main robust to anomalies of various scales.
Unfortunately, most previous methods exhibit a lower
accuracy in detecting small-scale anomalies compared to
larger-scale anomalies, as shown in Fig. 1. Reconstruction-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
17385
based [14, 16, 28, 30, 31, 45] and prediction-based [2,
5, 20, 23, 41] approaches are the mainstream modeling
paradigms. The two approaches learn spatio-temporal fea-
tures of normal patterns by reconstructing and predicting
frames, respectively. However, both approaches are inter-
fered by severe background noise since they perform re-
construction or prediction of frame in RGB space. Recent
object-centric works [11, 12, 17] separate and process fore-
grounds and backgrounds to address the challenge of back-
ground noise. Nevertheless, modeling approaches based on
prediction and binary classification struggle to learn highly
discriminative features to distinguish small-scale anoma-
lies. Some multi-scale V AD methods [41, 52] learn features
at different resolutions through pooling or feature pyramid-
ing but do not focus on appearance and motion features.
In this paper, we aim to explore a robust multi-scale
V AD method. On the one hand, the context dependence
of the anomalies necessitates the model to grasp global mo-
tion patterns and long-range features within the video. On
the other hand, detecting small-scale anomalies not only
demands that the model avoid the background noise inter-
ference but also requires it to discern highly discriminative
short-range spatio-temporal variances between frames, ow-
ing to their subtle differences from the normal pattern. Con-
sequently, the model needs to comprehensively learn the
spatio-temporal features (spatial appearance and temporal
motion features) of normal pattern in a multi-grained man-
ner to achieve robustness to multi-scale anomalies.
To this end, we take video continuity [19] as supervi-
sion to construct three self-supervised proxy tasks, enabling
V AD model to learn spatio-temporal features in both coarse-
grained and fine-grained manners during training. (i)To
determine whether a video sequence is continuous or not.
Continuity judgment requires the model to learn the over-
all long-range temporal features and global motion patterns
of the video in a coarse-grained manner. (ii)To locate dis-
continuities. Finding where the discontinuities occur drives
the model to capture changes in local motion in a fine-
grained manner rather than background noise. (iii)To es-
timate the missing frame in feature space instead of RGB
space. We formulate the estimation task as a contrastive
learning task to avoid the background noise interference and
learn highly discriminative features of motion and appear-
ances. By jointly solving three proxy tasks, the model can
maintain robustess to anomalies of various scales.
We conduct experiments on four challenging datasets
(Avenue [25], ShanghaiTech [27], UCF-Crime [36] and
Campus [2]). The experiments show that our proposed
method outperforms the State-Of-The-Art (SOTA) method,
especially in scenes with small-scale anomalies.
Our contributions can be summarized as follows:
• Three straightforward yet effective multi-grained spatio-
temporal representation proxy tasks are designed, en-abling the video anomaly detection model to maintain ro-
bustness to multi-scale anomalies.
• A reconstruction scheme for frames based on contrastive
learning is proposed, motivating the model to learn highly
discriminative features from both appearance and motion,
rather than simple RGB features and background noise.
• Our method achieves state-of-the-art performance on four
datasets, especially outperforming previous methods by a
large margin in scenes with small-scale anomalies.
2. Related Work
2.1. Video Anomaly Detection
Our work focuses on the video anomaly detection method
that only learning from normal data, which is mainly
grouped into the reconstruction-based methods [4, 16, 28,
33] and the prediction-based methods [20, 41, 48]. These
two methods typically use autoencoders (AEs) [15, 51],
memory-augmented AEs [13, 14, 23, 28, 31], or generative
models [20, 32, 41] to reconstruct current frames or predict
future frames so that frames with large reconstruction or
prediction errors are recognized as anomalous. While some
work attempts to introduce optical flow [23, 30], skele-
tal information [7, 10, 39], or utilize bi-directional predic-
tion [5, 8, 18, 52] to learn motion and temporal features,
these methods cannot be effective for small-scale anomalies
due to disturbance from background noise. Recent object-
centric methods [12, 17, 39] attempt to separate background
and objects to avoid the disturbance of background noise.
Although such approach can focus on the foreground ob-
jects, the way these methods model the object in terms
of classification or RGB prediction lacks the exploration
of fine-grained features. In addition, some efforts [1, 24]
to synthesize virtual anomaly data fail to work for such
anomalies owing to the lack of small-scale virtual data.
Different from these methods, we expect to enable the
model to learn different scales of variation and fine-grained
features of the normal pattern via self-supervised spatio-
temporal representation learning.
2.2. Self-Supervised Learning in V AD
Self-supervised learning V AD methods typically performe
contrastive learning or solve pretext tasks. For instance,
Wang et al. [42] performs contrastive learning with clus-
tered attention mechanism. However, it requires customized
data enhancement strategies. For pretext tasks, Yang et
al.[46] learn spatio-temporal representations by keyframe-
based event restoration, while the work [11] designs multi-
ple proxy tasks in an object-centric way to detect anomalies
in different aspects. However, the former [46] suffers from
severe background noise interference due to reconstructing
events in RGB space, while the latter [11] lacks the abil-
ity to learn fine-grained features as its tasks are primarily
17386
Discontinuous Video ܸ݀
Missing Frame ݉ܫ
Continuous Video ܸܿ
Backbone Self-Supervised Representation Learning
Feature Representation ݂Task 1: Continuity Judgment
Task 2: Discontinuity Localization
Task 3: Missing Frame EstimationFeature
Space
Projection
HeadAdjacent 
Frame of ImFrame from 
Other ScenesNon-Adjacent 
Frame of ImRepresentation of FramesRepresentation of Clips
Video Encoder ܨP1
P2
P3
ܨܸܿǡܸ݀
ܨሺܸ݀ሻ
ܨܸܿǡܸ݀ܨሺ݉ܫሻWhether is ܫ
What is ݉ܫWhere is ݉ܫ
ScoreNormal Video Clip ܸ
 Testing Video
V AD Inference
Video Sampling
Joint 
Loss ࡲࡸFigure 2. Method overview. The overall framework consists of a shared backbone network and three branches. The three branches perform
continuity judgment, discontinuity localization, and missing frame estimation tasks to learn the spatio-temporal features of normal patterns
at coarse- and fine-grained levels, respectively. The feature representation fgenerated after self-supervised training can effectively detect
anomalies of various scales. Blue arrows and grey arrows represent training and inference processes, respectively. Best viewed in color.
binary classifications.
In this work, we design proxy tasks based on video conti-
nuity [19] to learn spatio-temporal features of normal events
in a multi-grained manner to perceive anomalies of vari-
ous scales. In addition, we formulate the reconstruction of
frames as a contrastive learning task to learning highly dis-
criminative features instead of background noise.
2.3. Multi-Scale Learning
Multi-scale learning typically considers information from
different scales or granularities. Similar to our work, Xiao
et al. [44] hierarchically learns the spatio-temporal features
through event mask prediction task. However, event-level
features are not sufficient for detecting subtle variances be-
tween frames. There is also some multi-scale works [41, 52]
in V AD. For instance, Wang et al. [41] obtain image fea-
tures of different resolutions to perform prediction through
different pooling operations, while Zhong et al. [52] obtain
features at different scales through feature pyramid. Al-
though pooling and feature pyramids can provide feature
representations of different resolutions, they cannot specif-
ically capture the motion and appearance features which is
crucial for anomaly detection. In contrast, we design the
self-supervised proxy tasks to direct the model’s attention
towards changes in appearance and motion between frames,
enabling it to learn features in a multi-grained manner.3. Method
3.1. Overview
Our proposed multi-scale video anomaly detection method
attempts to learn spatio-temporal features of normal pat-
terns at both coarse-grained and fine-grained levels by
jointly solving multiple proxy tasks of spatio-temporal rep-
resentation learning. For a normal video clip V, our ap-
proach is devoted to learning a video encoder Fthat trans-
forms the video clip Vinto the scale-aware high-quality
spatio-temporal feature f. We define fas the high-quality
feature in terms of anomalous information of different
scales if it is able to easily address the following questions:
(1) Whether there are missing frames in video clips V, i.e.,
whether feature representation fremains sensitive to large-
scale anomalies. (2) If the video clip Vis not continuous,
where are the missing frames, i.e., whether feature repre-
sentation fremains sensitive to small-scale anomalies. (3)
If video clip Vis discontinuous, what are missing frames,
i.e., whether the feature representation fcontains high-level
motion feature and contextual feature.
The overall architecture of our proposed method is illus-
trated in Fig. 2. The network consists of three branches and
a video encoder F, all of the branches share the same back-
bone encoder F. Given two nonoverlapping video clips:
continuous clip Vc, discontinuous clip Vd, and the corre-
sponding missing frame Im, each of the three branches
solves a proxy task and operates together to optimize the
17387
backbone. The first branch is used to discriminate whether
a video clip is continuous or not. The second branch is used
to locate the position of the missing frame in the discontin-
uous clip Vd. The last branch is designed to learn high-level
features through contrastive learning mechanism, so that the
feature representation of the discontinuous clip Vdcan ef-
fectively approach the feature representation of the missing
frame Im. After joint optimization of self-supervised tasks,
the backbone Fcan generate scale-aware feature represen-
tations ffor anomaly inference.
3.2. Data Acquisition
To perform the proposed scale-aware proxy tasks, we select
continuous video clips and discontinuous clips containing
only one missing frame Im, respectively. The continuous
clip contains Tcontinuous frames, while the discontinuous
clip has a length of T−1. Given an initial continuous clip
of length T, we uniformly select a frame from the interval
[1, T−2]as the missing frame Imto form the breakpoint
(counting from 0). And the leading and alternate of the
missing frames are concatenated together to form discontin-
uous video clip Vd. The restriction of missing frame inside
the clip sequence ensures that the construction of missing
frames and discontinuous clips remains consistent.
As shown in Fig. 2, we formulate the continuity discrim-
ination task as a binary classification task while the missing
frame localization task is formulated as a T−2class classifi-
cation task labeled by Im. The estimation task is formulated
a contrastive learning task. Both initial and continuous clips
are formed by randomly sampling non-overlapping clips of
length T from the same video.
3.3. Network Architecture
We employ a video encoder (defaults a 3D-ConvNet) as the
backbone network Fand use it as a shared base for three
proxy tasks. Each task introduces into its top a projection
network, denoted P1,P2, andP3, which are used to process
the video clip embeddings fextracted from the backbone
network. All projection heads contain spatio-temporal aver-
aging pooling layers to ensure that the dimensionality of the
deep feature embeddings remains consistent, even though
the temporal lengths may be different.
In continuity judgment and missing frame localization,
we use fully connected layers as classifiers at the end of
P1andP2for performing classification operations. As for
the missing frame estimation task, we use P3to embed the
video features fextracted by the backbone network into the
low-dimensional features to get the representation, i.e., the
feature representation of the missing frame.
3.4. Proxy Tasks and Joint Learning
Task 1: Continuity Judgment. The continuity judgment
is intended to help the model learn spatio-temporal featuresand motion patterns in the video in a coarse-grained man-
ner to perceive large-scale anomalies. In this task, in order
for the model to learn by video features rather than other
cues such as context, the positive and negative samples in
each batch in our training come from the same video scene.
With N samples in a training batch, cross-entropy loss LCE
is used to optimize the model, and the loss for continuity
judgment is L1:
L1=1
NNX
i=1 
LCE 
P1 
F 
vi
d
1+P1 
F 
vi
c
0
,
(1)
where (·)represents the process of feature processing by
the network, FandP1represent the shared backbone net-
work and the continuity judgment projection head, respec-
tively, and vi
candvi
drepresent the ithcontinuous and dis-
continuous video clip, respectively.
Task 2: Discontinuity Localization. While the continu-
ity judgment task only learns spatio-temporal features in a
coarse-grained manner, the discontinuous localization task
expects to learn in a fine-grained manner in order to per-
ceive small-scale anomalies. Compared to binary labels,
labeling with missing frames Imin this task can drive the
model to learn changes in motion between adjacent frames
in a fine-grained manner. Similarly, the discontinuous lo-
calization loss L2can be expressed as an average of the
cross-entropy loss of Nsamples in each batch:
L2=1
NNX
i=1LCE
P2 
F 
vi
d
Ii
, (2)
where P2represents the discontinuity localization projec-
tion head and Iirepresents the missing frame position cor-
responding to discontinuous clip vi
d.
Task 3: Missing Frame Estimation. In order to further
learn the highly discriminative spatio-temporal features of
the video, we estimate missing frames in terms of both ap-
pearance and motion. Meanwhile, we choose to estimate
frames in feature space rather than RGB space to avoid
background noise interference. We design a contrastive
learning mechanism to perform this estimation task based
on the following facts. First, since adjacent frames and
missing frames are temporally connected, adjacent frames
and missing frames contain more similar motions compared
to non-adjacent frames from the same scene but farther
away. Second, video frames from the same scene share
more similar appearance features with missing frames than
video frames from other scenes.
Accordingly, we learn the motion information in the
video by triplet loss [34] taking the discontinuous video
clips vi
das the anchor point, the missing frames Imas the
positive samples, and the continuous video clips vi
cas the
negative samples. In addition, we take discontinuous clips
17388
vi
das anchor points, continuous clips vi
cfrom the same
scene as positive samples, and video clips
vj	
i̸=jfrom
different scenes as negative samples, to learn the appearance
information in the video by contrastive loss [19]. Defining
the cosine similarity computation operation as sim(·,·),
and the hyperparameter that balances the relative contribu-
tions of the triplet loss and the contrastive loss as ω∈[0,1],
the missing frame estimation loss is denoted as:
L3=1
NNX
i=1(ω×max(0 , γ−(p+
i−p−
i))
−(1−ω) log 
q+
i
q+
i+PN
j=1,j̸=iqi,j−!
),(3)
p+
i= sim 
P3(Vi
d, Ii)
, (4)
p−
i= sim 
P3(Vi
d, Vi
c)
, (5)
q+
i= exp 
sim 
P3(Vi
d, Vi
c)
/τ
, (6)
q−
i,j= exp
sim
P3(vi
d, vj
d)
/τ
+ exp 
sim 
P3(vi
d, vj
c)
/τ
,(7)
where p+
iandp−
idenote the similarity between positive
sample pairs and negative sample pairs in the triplet loss,
respectively, q+
idenotes a single positive sample pair in
the contrastive loss, qi,j−denotes the similarity of nega-
tive sample pairs in the contrastive loss, and τis the tem-
perature factor in the contrastive loss. Increasing ωallows
for more focus on motion information, while decreasing ω
allows for more focus on appearance information. Over-
all, the model is incentivized to learn fine-grained motion
changes and contextual features, including background in-
formation and object appearance.
Joint Optimization. The above three proxy tasks learn
spatio-temporal features in videos from different levels, and
we jointly optimize our shared video encoder Fwith the
multi-loss function.
LF=w1L1+w2L2+w3L3, (8)
where w1, w2, w3∈[0,1]are the individual weights on the
L1,L2andL3. Joint optimization helps the model to learn
video spatio-temporal features from both coarse- and fine-
grained scales to perceive anomalies of various scales.
3.5. V AD Inference
In the inference, we use Tframes as the processing unit.
Following the setup of the SOTA method [20, 41, 46], wechoose not to compute anomaly scores using the error be-
tween each estimated frame ˆIt(1≤t≤T)and the cor-
responding ground truth It. Instead, we opt to employ the
Peak Signal-to-Noise Ratio (PSNR) corresponding to the
frame with the largest Mean-Square Error (MSE) among
theTframes and the ground truth. This approach serves as
the detection metric for the video sequence, as follow:
MSE
It,ˆIt
=1
H·WX
i,jIt(i, j)−ˆIt(i, j)2
2,(9)
PSNR t= 10 log10MAX2
It
MSE
It,ˆIt, (10)
where HandWare the height and width of the frames,
respectively, It(i, j)/ˆIt(i, j)represents the RGB value of
the(i, j)-th pixel in It/ˆIt, and MAX Itrepresents the max-
imum value of the image pixel in It. Finally, we calculate
the anomaly score Stfor the t-th frame Itby normalizing
the PSNR values over all Tframes:
St= 1−PSNR t−mint′PSNR t′
max t′PSNR t′−mint′PSNR t′, (11)
where 1≤t′≤T. A higher score Stindicates that the
frame Itis more likely to be anomalous.
4. Experiment
4.1. Dataset and Evaluation Metrics
Cross-Scene Anomaly Dataset Avenue [25] contains 16
training videos and 21 test videos with 47 abnormal events,
including running and throwing. The scenes in this dataset
are single and the anomalies are mainly large-scale. Shang-
haiTech [27] has 13 scenes with complex lighting condi-
tions and different perspectives. In addition, the dataset
includes anomalies caused by sudden movements, such as
chasing and arguing. The different perspectives and the un-
fixed position of the camera lead to a large variation of both
the object scale and the anomaly scale in the scene. This
is a significant challenge to the robustness of anomaly de-
tection methods to scale variations. UCF-Crime [36] com-
prises 13 anomaly types, spanning a total of 128 hours of
video footage. This dataset is complex because of the un-
constrained backgrounds and anomalies of various scales.
The training set contains video-level annotations, whereas
the testing set includes frame-level annotations.
Scene-Dependent Anomaly Dataset. Campus [2] is cur-
rently the most challenging dataset in its field with 43
scenes, 28 classes of anomalous events and 16 hours of
videos. Especially, it contains scene-dependent anomalies,
which means an normal event may be abnormal in another
scene. Detecting scene-dependent anomalies requires the
model to understand the scene and learn highly discrimina-
tive features rather than overfit.
17389
Method Length TShanghaiTech Campus
AUC AUC
Micro Macro Micro Macro
Continuity
Judgment8 78.1 82.3 60.1 63.2
12 77.2 81.1 60.3 63.9
16 75.1 79.2 58.2 60.9
Discontinuity
Localization8 82.6 85.5 62.1 65.4
12 83.4 87.2 62.6 65.1
16 82.1 85.5 61.9 65.0
Missing Frame
Estimation8 79.8 83.2 63.5 66.8
12 81.2 84.9 65.2 68.9
16 80.5 84.0 64.9 68.0
Overall
Framework8 84.2 88.8 66.3 69.2
12 85.1 89.8 67.3 70.9
16 83.9 88.3 66.0 69.2
Table 1. Comparison of proposed framework and individual proxy
tasks between different lengths Tof input clips. We report Micro
and Macro AUC ( %) on ShanghaiTech and Campus datasets. The
best performing results are marked in bold.
Evaluation Metrics The area under the ROC curve (AUC)
serves as a commonly used metric for evaluation and com-
parison. A higher AUC score indicates a better anomaly
detection capability. Following previous research [12], we
evaluate both the Micro and Macro versions of AUC.
4.2. Implementation Details
We train and evaluate our method with an NVIDIA RTX
3090 GPU. In the training phase, we resize the resolution of
all input video clips to 256×256pixels, while the values
of the pixels in all frames are normalized to the range [0,
1]. For the pre-training of the three proxy tasks, we utilize
AdamW as the optimizer while the length of the continu-
ous video clips is set to T= 12 frames. The initial learn-
ing rate is set to 0.0003 and is gradually decayed following
the scheme of cosine annealing. For the hyperparameters,
we set ω= 0.5in Equation 3, w1=w2=w3= 1 in
Equation 8, and the temperature factor τ= 0.1in the con-
trastive loss. In our reported experimental performance, the
shared backbone network Fis implemented as I3D-RGB
[3]. In addition, we incorporate the designed proxy tasks
into the backbone of the SOTA methods [2, 11], the ex-
perimental results are also reported. Following the existing
method setup [11, 24, 33], we perform the proxy task at the
object-level with the multi-task backbone [11] and at the
frame-level with the other backbones.
4.3. Ablation Study
Considering the ShanghaiTech and Campus datasets con-
tain diverse perspectives and scenes, and large variations in
scales of anomaly, we conduct exhaustive ablation experi-
ments on the ShanghaiTech and Campus datasets.
Sensitivity to the Length of Video Clips. The length Tof
the input clips is an important setting in our proposed frame-
work. With a small T, both continuity judgments and dis-ID CJ DL MFE FPShanghaiTech Campus
AUC AUC
Micro Macro Micro Macro
1 - - - ✓ 70.2 71.2 58.1 58.6
2✓ - - - 77.2 81.1 60.3 63.9
3 -✓ - - 83.4 87.2 62.6 65.1
4 - - ✓ - 81.2 84.9 65.2 68.9
5✓ ✓ - - 84.1 87.8 62.8 65.4
6✓ -✓ - 82.3 85.6 65.3 69.2
7 -✓ ✓ - 84.5 89.5 66.2 70.1
8✓ ✓ ✓ - 85.1 89.8 67.3 70.9
9✓ ✓ -✓ 84.2 87.6 62.8 65.6
Table 2. Ablation experiments on the contributions of each proxy
task. We report the AUC ( %) scores on ShanghaiTech and Campus
datasets. ’CJ’, ’DL’ and ’MFE’ stand for the three proxy tasks of
continuity judgment, discontinuity localization and missing frame
estimation, respectively. In addition we add experiments on frame
prediction for comparison, ’FP’ represents frame prediction.
continuity localization will be so ambiguous as to be diffi-
cult. While with a larger T, the model may not need to learn
high-quality spatio-temporal features to solve the task due
to enough spatio-temporal information in the feed. Table 1
shows the sensitivity of each proxy task to T. For the dis-
continuous localization and missing frame estimation tasks,
T= 12 presents better performance. Although at T= 8,
the continuity judgment yields competitive results on the
ShanghaiTech dataset, the overall framework achieves the
best outcomes with jointly optimized at T= 12 .
Effect of Continuity Judgment. The results of the abla-
tion experiments for each proxy task are shown in Table 2,
where we take the frame prediction [20] performed on the
same backbone Fas the baseline and train it with gradi-
ent loss and intensity loss (ID 1). Performing the continuity
judgment task alone improves the performance of the model
compared to frame prediction (ID 2). This suggests that
continuity judgment task motivates the model to generate
higher quality representations than frame prediction.
Effect of Discontinuity Localization. Compared to other
proxy tasks and baseline, the performance of the discontin-
uous localization task alone can be significantly improved,
especially on ShanghaiTech which contains anomalies of
various scales (ID 3, ID 5 and ID 7). As this task sam-
ples discontinuous positions uniformly along the time axis,
it motivates the model to capture subtle variations between
frames, obtaining fine-grained representations.
Effect of Missing Frame Estimation. Performing miss-
ing frame estimation alone is effective in improving perfor-
mance on Campus compared to baseline and other proxy
tasks, which contains scene-dependent anomalies (ID 4).
This improvement illustrates that our proposed contrastive
learning scheme can help the model understand the scene.
Compared to performing the frame prediction task to learn
RGB features (ID 1 and ID 9), our proposed scheme learns
highly discriminative motion features and semantically ef-
17390
yearMethodAvenue ShanghaiTech
AUC AUC
Micro Macro Micro Macro2018Liuet al. [20] 85.1 - 72.8 -
Liuet al. [22] 84.4 - - -
Sultani et al. [36] - - 76.5 -2019Gong et al. [14] 83.8 - 71.2 -
Leeet al. [18] 90.0 - 76.2 -
Ionescu et al. [17] 87.4 90.4 78.7 84.92020Park et al. [31] 88.5 - 70.5 -
Sunet al. [37] 89.6 - 74.7 -
Luet al. [26] 85.8 - 75.9 -
Wang et al. [42] 87.0 - 79.3 -
Yuet al. [47] 89.6 - 74.8 -2021Liuet al. [23] 91.1 - 76.2 -
Lvet al. [28] 89.5 - 73.8 -
Georgescu et al. [11]∗91.5 91.9 82.4 89.3
Georgescu et al. [12]∗92.3 90.4 82.7 89.32022Wang et al. [41] 88.3 - 76.6 -
Zaheer et al. [49] 74.2 - 79.6 -
Chen et al. [5] 90.3 - 78.1 -
Zhong et al. [52] 89.0 - 74.5 -
Cho et al. [6] 88.0 - 76.3 -
Yang et al. [45] 89.9 - 74.7 -
Ristea et al. [33]∗ 92.9 91.9 83.6 89.5
Acsintoae et al. [1]▽∗93.0 93.2 83.7 90.52023Yang et al. [46] 89.9 - 73.8 -
Caoet al. [2] 86.8 - 79.2 -
Liuet al. [21] 92.8 - 78.8 -
Singh et al. [35] 86.0 - 76.6 -
Sunet al. [39] 92.4 - 83.0 -
Sunet al. [38] 91.5 - 78.6 -
Liuet al. [24]▽91.8 92.3 83.8 87.8
Liuet al. [24]▽∗93.6 93.9 85.0 91.4
Ours 92.4 92.9 85.1 89.8
Ours▽93.2 92.5 86.2 91.0
Ours∗(Estimation Task) 92.6 93.0 85.5 91.8
Ours∗(Three Tasks) 93.6 93.8 86.8 92.4
Ours▽∗(Three Tasks) 94.3 94.5 87.5 93.0
Table 3. Comparison with SOTA methods of the Micro and Macro
AUC (%) on Avenue and ShanghaiTech datasets. The best per-
forming results are marked in bold.
▽: Methods apply virtual dataset for training.
∗: Methods utilize multi-task model ( [11] or [12]) as backbone.
Method Reference Micro AUC Macro AUC
Park et al. [31] CVPR20 68.9 72.4
Georgescu et al. [11] CVPR21 74.6 78.2
Wang et al. [41] TNNLS22 72.9 76.8
Sunet al. [39] CVPR23 75.5 78.3
Ours - 80.6 83.9
Table 4. Results of Micro and Macro AUC( %) on UCF-Crime
dataset. The best performing results are marked in bold.
fective contexts, including the appearance of objects and
background (ID 4 and ID 8).
Effect of Joint Optimization. Although a single task may
be able to achieve competitive performance. Experiments
have shown that when proxy tasks are executed in pairs (IDMethod Reference Miceo AUC Macro AUC
Liuet al. [20] CVPR18 57.9 60.2
Gong et al. [14] CVPR19 61.9 62.5
Ionescu et al. [17] CVPR19 59.3 63.4
Park et al. [31] CVPR20 62.5 63.6
Liuet al. [23] ICCV21 63.7 -
Lvet al. [28] CVPR21 64.4 -
Wang et al. [41] TNNLS22 61.9 64.2
Caoet al. [2]△CVPR23 68.2 -
Ours - 67.3 70.9
Ours△- 70.1 72.2
Table 5. Comparison with SOTA methods of the Micro and Macro
AUC(%) on Campus dataset. The best performing results are
marked in bold.
△:Version that utilize scene-conditioned model [2] as backbone.
5, ID 6 and ID 7) and all tasks are executed together (ID 8),
the performance of the model continues to improve to an
optimum. This joint improvement is attributed to the fact
that different proxy tasks learn spatio-temporal features at
different levels, and they complement each other in motion
and contextual feature learning.
4.4. Comparisons with State-Of-The-Arts
We compare the proposed framework with SOTA methods
in terms of Micro and Macro AUC( %). It is noteworthy that
the current state-of-the-art methods [1, 24, 33] utilize the
multi-task framework proposed by Georgescu et al. ([11]
and [12], respectively) as the backbone, where [1] and [24]
employ virtual data for training. Therefore, we evaluate dif-
ferent configurations of the proposed method on the Avenue
and ShanghaiTech datasets, as shown in Table 3. With the
multi-task backbone [11] (marked with ∗in Table 3), we
assess two versions: one involves replacing only the predic-
tion component with our designed missing estimation task,
while keeping the other proxy tasks unchanged. The other
version replaces both proxy tasks in the motion branch with
continuity judgment and discontinuity localization tasks in
addition to incorporating the missing estimation task. In ad-
dition, we report the performance of applying virtual data
for training (marked with ▽in Table 3).
Results on Avenue. As shown in Table 3, our method
achieved the highest AUC scores, obtaining a Micro AUC
of 94.3 %and a Macro AUC of 94.5 %. Without the back-
bone of multi-task [11, 12] and virtual data [24], our method
still scored the best performance with 92.9 %Macro AUC.
Results on ShanghaiTech. Our proposed method yields
a Micro AUC of 87.5 %and a Macro AUC of 93.0 %on
the ShanghaiTech dataset. With the backbone of multi-task
[11, 12], we improve the Micro AUC by 3.1 %compared
to Georgescu et al. [11] in the setup with only the miss-
ing estimation task, and by 4.4 %when incorporating three
designed proxy tasks. Without the backbone of multi-task
17391
Small-Scale
 Small-Scale
 Large-Scale
Ground
Truth
ROAD
MAP
DEDD
net
USTN -
DSC
Ours
Anomaly Normal region but with large errorsFigure 3. Examples of detection for anomalies of different scales
from the ShanghaiTech dataset, including ground truth and predic-
tion or reconstruction error maps from different methods (from top
to bottom: ROADMAP[41], DEDDnet[52], USTN-DSC[46], and
our method). Best viewed in color.
[11, 12] and virtual data [24], our method still achieves
a Micro AUC score of 85.1 %,which is comparable to the
method [24] that employed the multi-task backbone and vir-
tual training data. In comparison with the multi-scale V AD
methods ([41] and [52]), our method achieves a significant
improvement of 8.5 %and 10.6 %, respectively.
Results on UCF-Crime. Due to the absence of published
results (methods only learning from normal data) on the
UCF-Crime dataset, we implement the code from the ex-
isting literature [11, 31, 39, 41]. As shown in Table 4, our
proposed method achieves a significant improvement com-
pared to the second best method by 5.1 %in terms of Micro
AUC and 5.6 %in terms of Macro AUC.
Results on Campus. We evaluate an additional version on
the Campus dataset with the scene-conditioned model (pro-
vided by Cao et al. [2]) as the backbone, as it is necessary
to incorporate scene features for detecting scene-dependent
anomalies. As shown in Table 5, our method achieves
the highest Micro AUC score of 70.1 %and Macro AUC
score of 72.2 %. Without the scene-conditioned model, our
method still obtain a Micro AUC of 67.3 %, surpassing other
RGB reconstruction-based [14, 28, 31] or RGB prediction-
based methods [20, 23, 41].
4.5. Qualitative Results
Fig. 1 shows the anomaly curves of the test video and the
comparison of our method with PMem [31] (reconstruction-
Figure 4. Visualization of salient regions of test frames by the
proposed method. Best viewed in color.
based method) and RoadMap [41] (prediction-based
method). When there are small-scale anomalies in the sam-
pled videos, the anomaly score curves of the other two
methods have obvious missed detections, while our method
can accurately perceive the small-scale anomalies. In addi-
tion, Fig. 3 illustrates more examples, including the com-
parison with the multi-scale V AD methods [41, 52] and the
comparison with self-supervised V AD method [46]. It can
be observed that for normal regions in video frames, our
method is able to estimate them well, while for abnormal
event regions large errors occur. While other methods gen-
erate large errors for some normal regions, especially for
frames with small-scale anomalies.
Visualizations of the salient regions of the frames by our
method are illustrated in Fig. 4. For both large-scale anoma-
lies (1-th row) and small-scale anomalies (2-th row), our
model can accurately focus on the anomalous regions.
5. Conclusion
In this paper, we observe that anomalous spatial scales af-
fect the feature learning of the model. Small-scale anoma-
lies require the model to learn highly discriminative fea-
tures in a fine-grained manner rather than background noise.
To detect multi-scale anomalies, we design proxy tasks su-
pervised by video continuity that motivate the model to
learn spatio-temporal features in both coarse-grained and
fine-grained manners. Furthermore, instead of performing
RGB reconstruction and prediction, we estimation frames
in feature space through contrastive learning to learn highly
discriminative features of appearance and motion. Exper-
iments conducted on four challenging benchmark datasets
validate the effectiveness of our proposed method.
Acknowledgement. This work was supported by the
National Natural Science Foundation of China under
Grants (62101064, 62171057, 62201072, U23B2001,
62001054, 62071067), the National Postdoctoral Pro-
gram for Innovative Talents under Grant BX20230052,
China Postdoctoral Science Foundation (2023TQ0039),
the Ministry of Education and China Mobile Joint Fund
(MCM20200202, MCM20180101), Beijing University of
Posts and Telecommunications-China Mobile Research In-
stitute Joint Innovation Center.
17392
References
[1] Andra Acsintoae, Andrei Florescu, Mariana-Iuliana
Georgescu, Tudor Mare, Paul Sumedrea, Radu Tudor
Ionescu, Fahad Shahbaz Khan, and Mubarak Shah. Ub-
normal: New benchmark for supervised open-set video
anomaly detection. In CVPR , pages 20111–20121, 2022. 2,
7
[2] Congqi Cao, Yue Lu, Peng Wang, and Yanning Zhang. A
new comprehensive benchmark for semi-supervised video
anomaly detection and anticipation. In CVPR , pages 20392–
20401, 2023. 2, 5, 6, 7, 8
[3] Jo ˜ao Carreira and Andrew Zisserman. Quo vadis, action
recognition? A new model and the kinetics dataset. In CVPR ,
pages 4724–4733, 2017. 6
[4] Yunpeng Chang, Zhigang Tu, Wei Xie, and Junsong Yuan.
Clustering driven deep autoencoder for video anomaly de-
tection. In ECCV , pages 329–345, 2020. 2
[5] Chengwei Chen, Yuan Xie, Shaohui Lin, Angela Yao, Guan-
nan Jiang, Wei Zhang, Yanyun Qu, Ruizhi Qiao, Bo Ren,
and Lizhuang Ma. Comprehensive regularization in a bi-
directional predictive network for video anomaly detection.
InAAAI , pages 230–238, 2022. 1, 2, 7
[6] MyeongAh Cho, Taeoh Kim, Woo Jin Kim, Suhwan Cho,
and Sangyoun Lee. Unsupervised video anomaly detection
via normalizing flows with implicit latent features. Pattern
Recognit. , 129:108703, 2022. 7
[7] Romero F. A. B. de Morais, Vuong Le, Truyen Tran, Budha-
ditya Saha, Moussa Reda Mansour, and Svetha Venkatesh.
Learning regularity in skeleton trajectories for anomaly de-
tection in videos. In CVPR , pages 11996–12004, 2019. 2
[8] Zhiwen Fang, Jiafei Liang, Joey Tianyi Zhou, Yang Xiao,
and Feng Yang. Anomaly detection with bidirectional con-
sistency in videos. IEEE Trans. Neural Networks Learn.
Syst., 33(3):1079–1092, 2022. 2
[9] Jia-Chang Feng, Fa-Ting Hong, and Wei-Shi Zheng. MIST:
multiple instance self-training framework for video anomaly
detection. In CVPR , pages 14009–14018, 2021. 1
[10] Alessandro Flaborea, Luca Collorone, Guido
Maria D’Amely di Melendugno, Stefano D’Arrigo, Bardh
Prenkaj, and Fabio Galasso. Multimodal motion conditioned
diffusion model for skeleton-based video anomaly detection.
In(ICCV) , pages 10318–10329, 2023. 2
[11] Mariana-Iuliana Georgescu, Antonio Barbalau, Radu Tu-
dor Ionescu, Fahad Shahbaz Khan, Marius Popescu, and
Mubarak Shah. Anomaly detection in video via self-
supervised and multi-task learning. In CVPR , pages 12742–
12752, 2021. 2, 6, 7, 8
[12] Mariana-Iuliana Georgescu, Radu Tudor Ionescu, Fa-
had Shahbaz Khan, Marius Popescu, and Mubarak Shah. A
background-agnostic framework with adversarial training for
abnormal event detection in video. IEEE Trans. Pattern Anal.
Mach. Intell. , 44(9):4505–4523, 2022. 2, 6, 7, 8
[13] Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha,
Moussa Reda Mansour, Svetha Venkatesh, and Anton
van den Hengel. Memorizing normality to detect anomaly:
Memory-augmented deep autoencoder for unsupervised
anomaly detection. In ICCV , pages 1705–1714, 2019. 2[14] Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha,
Moussa Reda Mansour, Svetha Venkatesh, and Anton
van den Hengel. Memorizing normality to detect anomaly:
Memory-augmented deep autoencoder for unsupervised
anomaly detection. In ICCV , pages 1705–1714, 2019. 2,
7, 8
[15] Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit K.
Roy-Chowdhury, and Larry S. Davis. Learning temporal reg-
ularity in video sequences. In CVPR , pages 733–742, 2016.
2
[16] Jinlei Hou, Yingying Zhang, Qiaoyong Zhong, Di Xie, Shil-
iang Pu, and Hong Zhou. Divide-and-assemble: Learning
block-wise memory for unsupervised anomaly detection. In
ICCV , pages 8771–8780, 2021. 2
[17] Radu Tudor Ionescu, Fahad Shahbaz Khan, Mariana-Iuliana
Georgescu, and Ling Shao. Object-centric auto-encoders and
dummy anomalies for abnormal event detection in video. In
CVPR , pages 7842–7851, 2019. 2, 7
[18] Sangmin Lee, Hak Gu Kim, and Yong Man Ro. BMAN:
bidirectional multi-scale aggregation networks for abnormal
event detection. IEEE Trans. Image Process. , 29:2395–2408,
2019. 2, 7
[19] Hanwen Liang, Niamul Quader, Zhixiang Chi, Lizhe Chen,
Peng Dai, Juwei Lu, and Yang Wang. Self-supervised spa-
tiotemporal representation learning by exploiting video con-
tinuity. In AAAI , pages 1564–1573, 2022. 2, 3, 5
[20] Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. Fu-
ture frame prediction for anomaly detection - A new base-
line. In CVPR , pages 6536–6545, 2018. 2, 5, 6, 7, 8
[21] Wenrui Liu, Hong Chang, Bingpeng Ma, Shiguang Shan,
and Xilin Chen. Diversity-measurable anomaly detection.
InCVPR , pages 12147–12156, 2023. 7
[22] Yusha Liu, Chun-Liang Li, and Barnab ´as P´oczos. Classifier
two sample test for video anomaly detections. In BMVC ,
page 71. BMV A Press, 2018. 7
[23] Zhian Liu, Yongwei Nie, Chengjiang Long, Qing Zhang, and
Guiqing Li. A hybrid video anomaly detection framework
via memory-augmented flow reconstruction and flow-guided
frame prediction. In ICCV , pages 13568–13577, 2021. 1, 2,
7, 8
[24] Zuhao Liu, Xiao-Ming Wu, Dian Zheng, Kun-Yu Lin, and
Wei-Shi Zheng. Generating anomalies for video anomaly de-
tection with prompt-based feature mapping. In CVPR , pages
24500–24510. IEEE, 2023. 1, 2, 6, 7, 8
[25] Cewu Lu, Jianping Shi, and Jiaya Jia. Abnormal event de-
tection at 150 FPS in MATLAB. In ICCV , pages 2720–2727,
2013. 2, 5
[26] Yiwei Lu, Frank Yu, Mahesh Kumar Krishna Reddy, and
Yang Wang. Few-shot scene-adaptive anomaly detection. In
ECCV , pages 125–141, 2020. 7
[27] Weixin Luo, Wen Liu, and Shenghua Gao. A revisit of sparse
coding based anomaly detection in stacked RNN framework.
InICCV , pages 341–349, 2017. 2, 5
[28] Hui Lv, Chen Chen, Zhen Cui, Chunyan Xu, Yong Li, and
Jian Yang. Learning normal dynamics in videos with meta
prototype network. In CVPR , pages 15425–15434, 2021. 2,
7, 8
17393
[29] Hui Lv, Zhongqi Yue, Qianru Sun, Bin Luo, Zhen Cui, and
Hanwang Zhang. Unbiased multiple instance learning for
weakly supervised video anomaly detection. In CVPR , pages
8022–8031, 2023. 1
[30] Trong-Nguyen Nguyen and Jean Meunier. Anomaly detec-
tion in video sequence with appearance-motion correspon-
dence. In ICCV , pages 1273–1283, 2019. 2
[31] Hyunjong Park, Jongyoun Noh, and Bumsub Ham. Learning
memory-guided normality for anomaly detection. In CVPR ,
pages 14360–14369, 2020. 1, 2, 7, 8
[32] Mahdyar Ravanbakhsh, Enver Sangineto, Moin Nabi, and
Nicu Sebe. Training adversarial discriminators for cross-
channel abnormal event detection in crowds. In WACV , pages
1896–1904, 2019. 2
[33] Nicolae-Catalin Ristea, Neelu Madan, Radu Tudor Ionescu,
Kamal Nasrollahi, Fahad Shahbaz Khan, Thomas B. Moes-
lund, and Mubarak Shah. Self-supervised predictive con-
volutional attentive block for anomaly detection. In CVPR ,
pages 13566–13576, 2022. 2, 6, 7
[34] Florian Schroff, Dmitry Kalenichenko, and James Philbin.
Facenet: A unified embedding for face recognition and clus-
tering. In CVPR , pages 815–823, 2015. 4
[35] Ashish Singh, Michael J Jones, and Erik G Learned-Miller.
Eval: Explainable video anomaly localization. In CVPR ,
pages 18717–18726, 2023. 7
[36] Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world
anomaly detection in surveillance videos. In CVPR , pages
6479–6488, 2018. 1, 2, 5, 7
[37] Che Sun, Yunde Jia, Yao Hu, and Yuwei Wu. Scene-aware
context reasoning for unsupervised abnormal event detection
in videos. In ACM Multimedia , pages 184–192, 2020. 7
[38] Che Sun, Chenrui Shi, Yunde Jia, and Yuwei Wu. Learning
event-relevant factors for video anomaly detection. In AAAI ,
pages 2384–2392, 2023. 7
[39] Shengyang Sun and Xiaojin Gong. Hierarchical semantic
contrast for scene-aware video anomaly detection. In CVPR ,
pages 22846–22856, 2023. 1, 2, 7, 8
[40] Yu Tian, Guansong Pang, Yuanhong Chen, Rajvinder Singh,
Johan W. Verjans, and Gustavo Carneiro. Weakly-supervised
video anomaly detection with robust temporal feature mag-
nitude learning. In ICCV , pages 4955–4966, 2021. 1
[41] Xuanzhao Wang, Zhengping Che, Bo Jiang, Ning Xiao, Ke
Yang, Jian Tang, Jieping Ye, Jingyu Wang, and Qi Qi. Robust
unsupervised video anomaly detection by multipath frame
prediction. IEEE Trans. Neural Networks Learn. Syst. , 33
(6):2301–2312, 2022. 1, 2, 3, 5, 7, 8
[42] Ziming Wang, Yuexian Zou, and Zeming Zhang. Cluster
attention contrast for video anomaly detection. In ACM Mul-
timedia , pages 2463–2471, 2020. 2, 7
[43] Jie Wu, Wei Zhang, Guanbin Li, Wenhao Wu, Xiao Tan,
Yingying Li, Errui Ding, and Liang Lin. Weakly-supervised
spatio-temporal anomaly detection in surveillance video. In
IJCAI , pages 1172–1178, 2021. 1
[44] Fanyi Xiao, Kaustav Kundu, Joseph Tighe, and Davide Mod-
olo. Hierarchical self-supervised representation learning for
movie understanding. In CVPR , pages 9717–9726, 2022. 3[45] Zhiwei Yang, Peng Wu, Jing Liu, and Xiaotao Liu. Dy-
namic local aggregation network with adaptive clusterer for
anomaly detection. In ECCV , pages 404–421, 2022. 2, 7
[46] Zhiwei Yang, Jing Liu, Zhaoyang Wu, Peng Wu, and Xiaotao
Liu. Video event restoration based on keyframes for video
anomaly detection. In CVPR , pages 14592–14601, 2023. 1,
2, 5, 7, 8
[47] Guang Yu, Siqi Wang, Zhiping Cai, En Zhu, Chuanfu Xu,
Jianping Yin, and Marius Kloft. Cloze test helps: Effec-
tive video anomaly detection via learning to complete video
events. In ACM Multimedia , pages 583–591, 2020. 7
[48] Jongmin Yu, Younkwan Lee, Kin Choong Yow, Moongu
Jeon, and Witold Pedrycz. Abnormal event detection and
localization via adversarial event prediction. IEEE Trans.
Neural Networks Learn. Syst. , 33(8):3572–3586, 2022. 2
[49] Muhammad Zaigham Zaheer, Arif Mahmood, Muham-
mad Haris Khan, Mattia Seg `u, Fisher Yu, and Seung-Ik
Lee. Generative cooperative learning for unsupervised video
anomaly detection. In CVPR , pages 14724–14734, 2022. 7
[50] Chen Zhang, Guorong Li, Yuankai Qi, Shuhui Wang, Laiyun
Qing, Qingming Huang, and Ming-Hsuan Yang. Exploiting
completeness and uncertainty of pseudo labels for weakly
supervised video anomaly detection. In CVPR , pages 16271–
16280, 2023. 1
[51] Yiru Zhao, Bing Deng, Chen Shen, Yao Liu, Hongtao Lu,
and Xian-Sheng Hua. Spatio-temporal autoencoder for video
anomaly detection. In ACM Multimedia , pages 1933–1941,
2017. 2
[52] Yuanhong Zhong, Xia Chen, Yongting Hu, Panliang Tang,
and Fan Ren. Bidirectional spatio-temporal feature learn-
ing with multiscale evaluation for video anomaly detection.
IEEE Trans. Circuits Syst. Video Technol. , 32(12):8285–
8296, 2022. 2, 3, 7, 8
[53] Hang Zhou, Junqing Yu, and Wei Yang. Dual memory units
with uncertainty regulation for weakly supervised video
anomaly detection. In AAAI , pages 3769–3777. AAAI, 2023.
1
17394
