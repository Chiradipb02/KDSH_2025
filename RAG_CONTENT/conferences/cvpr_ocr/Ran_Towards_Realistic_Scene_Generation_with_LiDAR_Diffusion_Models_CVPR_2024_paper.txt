Towards Realistic Scene Generation with LiDAR Diffusion Models
Haoxi Ran
Carnegie Mellon University
ranhaoxi@cmu.eduVitor Guizilini
Toyota Research Institute
vitor.guizilini@tri.globalYue Wang
University of Southern California
yue.w@usc.edu
Reference
Curve -based DMs ( Ours)
Throughput ↑: 1.603 samples/sec ( ×107) 
Patch -based DMs  (Latent Diffusion)  
Throughput ↑: 2.171 samples/sec ( ×145) 
Point-based DMs  (LiDARGen)  
Throughput ↑: 0.015 samples/sec (×1) 
Figure 1. Our method (LiDM) steps towards LiDAR-realistic scene generation by preserving curve-like structures and objects with greater
resemblance to real-world data (Reference), and marks a milestone for conditional LiDAR scene generation from different input modalities.
Abstract
Diffusion models (DMs) excel in photo-realistic image
synthesis, but their adaptation to LiDAR scene generation
poses a substantial hurdle. This is primarily because DMs
operating in the point space struggle to preserve the curve-
like patterns and 3D geometry of LiDAR scenes, which con-
sumes much of their representation power. In this paper,
we propose LiDAR Diffusion Models (LiDMs) to gener-
ate LiDAR-realistic scenes from a latent space tailored to
capture the realism of LiDAR scenes by incorporating ge-
ometric priors into the learning pipeline. Our method tar-
gets three major desiderata: pattern realism, geometry re-
alism, and object realism. Specifically, we introduce curve-
wise compression to simulate real-world LiDAR patterns,
point-wise coordinate supervision to learn scene geome-
try, and patch-wise encoding for a full 3D object context.
With these three core designs, our method achieves com-
petitive performance on unconditional LiDAR generation
in 64-beam scenario and state of the art on conditional
LiDAR generation, while maintaining high efficiency com-
pared to point-based DMs (up to 107 ×faster). Further-
more, by compressing LiDAR scenes into a latent space, we
enable the controllability of DMs with various conditions
such as semantic maps, camera views, and text prompts.Our code and pretrained weights are available at https:
//github.com/hancyran/LiDAR-Diffusion .
1. Introduction
Recent years have observed a surge of conditional genera-
tive models that are capable of generating visually appeal-
ing and highly realistic images. Among them, diffusion
models (DMs) have emerged as one of the most popular
methods, thanks to its unexceptionable performance. To en-
able generation with arbitrary conditions, Latent Diffusion
Models (LDMs) [44] combine the cross-attention mecha-
nism with a convolutional autoencoder to generate high-
resolution images. Its subsequent extensions ( e.g., Sta-
ble Diffusion [2], Midjourney [1], ControlNet [60]) further
boosted its potential for conditional image synthesis.
This success leads us to inquire: can we apply control-
lable DMs to LiDAR scene generation for autonomous driv-
ing and robotics? For instance, given a collection of bound-
ing boxes, can these models synthesize corresponding Li-
DAR scenes, thus turning these bounding boxes into high-
quality and expensive labeled data? Alternatively, is it pos-
sible to generate a 3D scene solely from a set of images?
Or even more ambitiously, can we design a language-driven
LiDAR generator for controllable simulation? To answer
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
14738
these interleaved questions, our goal is to design DMs that
incorporate a diverse set of conditions ( e.g., layouts, camera
views, text) to generate LiDAR-realistic scenes.
To that end, we glean insights from recent works of
DMs for autonomous driving. In [62], point-based DM
(i.e., LiDARGen) is introduced to unconditional LiDAR
scene generation. However, this model tends to produce
noisy backgrounds ( e.g., roads, walls) and ambiguous ob-
jects ( e.g., cars), leading to a failure in generating LiDAR-
realistic scenes ( cf. Fig. 1). In addition, applying diffusion
on points without any compression can computationally
slow down the inference process. Moreover, the direct ap-
plication of patch-based DMs ( i.e., Latent Diffusion [44]) to
LiDAR scene generation yields unsatisfactory performance
both qualitatively and quantitatively ( cf. Fig. 1).
To enable conditional LiDAR-realistic scene generation,
we thereby propose a curve-based generator, termed Li-
DAR Diffusion Models (LiDMs), to answer the aforemen-
tioned questions and tackle the shortcomings of recent
works. LiDMs are capable of processing arbitrary con-
ditions, such as bounding boxes, camera images, and se-
mantic maps. LiDMs leverage range images as the repre-
sentations of LiDAR scenes, which are prevalent in vari-
ous downstream tasks such as detection [28, 37], semantic
segmentation [38, 55], and generation [62]. This choice is
grounded in the reversible and lossless conversion between
range images and point clouds, along with the substantial
benefits from the highly optimized 2D convolutional op-
eration. To grasp the semantic and conceptual essence of
LiDAR scenes during the diffusion process, our approach
leverages encoded points of LiDAR scenes into a perceptu-
ally equivalent latent space before the diffusion process.
To further improve the realistic simulation of real-world
LiDAR data, we focus on three key components: pattern
realism, geometry realism, and object realism. First, We
leverage curve-wise compression in the auto-encoding pro-
cess to maintain the curve patterns of points, motivated
by [49]. Second, to achieve geometry realism, we intro-
duce point-wise coordinate supervision to imbue our auto-
encoder with the understanding of scene-level geometry.
Lastly, we enlarge the receptive field by incorporating an
additional patch-wise down-sampling strategy to capture
the full context of visually large objects. Augmented by
these proposed modules, the resulting perceptual space en-
ables DMs to efficiently synthesize high-quality LiDAR
scenes ( cf. Fig. 1), while also exhibiting superior perfor-
mance with a ×107 speedup compared to point-based DMs
(assessed on one NVIDIA RTX 3090 ), and supporting arbi-
trary types of image-based and token-based conditions. We
summarize our key contributions as follows:
• We propose a novel LiDAR Diffusion Model (LiDM),
a generative model that consumes arbitrary input condi-
tions for LiDAR-realistic scene generation. To the best ofour knowledge, this is the first method capable of LiDAR
scene generation from multi-modal conditions.
• We introduce curve-wise compression to maintain real-
istic LiDAR patterns, point-wise coordinate supervision
to regularize models for scene-level geometry, and patch-
wise encoding to fully capture the context of 3D objects.
• We introduce three metrics for thorough and quantitative
evaluation of the quality of generated LiDAR scenes in
the perceptual space, comparing representations includ-
ing range images, sparse volumes, and point clouds.
• Our method achieves state-of-the-art on conditional
scene synthesis under 64-beam scenario, while realizing
a×107 speedup compared to point-based DMs.
2. Related Work
Diffusion Models. Diffusion models (DMs) [48] shows
great success in image synthesis [9, 18, 26] in pixel space.
Instead of applying diffusion to pixel space, Latent Diffu-
sion Models (LDMs) [44] adopt a perceptually equivalent
latent space for DMs. Their larger-scale applications of
LDMs ( e.g., Stable Diffusion [2], Midjourney [1] further
boost the community of DMs. Recent applications of DMs,
including language-guided DMs ( e.g., Glide [39], DALL-
E2 [43]) and other controllable DMs ( e.g., ControlNet [60]),
also reveal the great potential of DMs.
3D Diffusion Models. 3D diffusion models represent a
crucial branch of DMs, offering the capability to generate
high-quality samples across various 3D modalities. This in-
cludes point clouds [33, 36, 52, 61], meshes [16, 32, 34],
and implicit fields [7, 11, 19, 29, 30, 47, 59]. Recently,
Point-E [40], a language-guided DM, has demonstrated ef-
ficient capabilities in generating high-quality hand-crafted
3D models based on a large-scale 3D dataset.
LiDAR Scene Generation. Given the larger-scale and
complex scenes, treating LiDAR point clouds as a point
cloud generation task akin to hand-crafted 3D models en-
counters difficulties. In [6], the authors explore the possi-
bilities of generative models in LiDAR scenes by providing
two solutions, LiDARV AE and LiDARGAN. [62] further
introduces a point-based DM, LiDARGen, to generate sat-
isfactory samples of range images as LiDAR scenes. How-
ever, these aforementioned methods still fail to generate
LiDAR-realistic scenes as they may overlook the curve-like
structures and geometric details inherent in LiDAR data.
LiDAR Scene Simulation. LiDAR simulation produces
LiDAR point clouds through physics-based simulators [10,
27] or data-driven simulators [4, 20, 35, 54]. Physics-based
LiDAR simulators ( e.g., CARLA [10]) use raycasting to
project rays from the sensor’s origin onto the environment’s
14739
geometry to simulate LiDAR by calculating intersections.
Benefiting from producing realistic LiDAR scenes, data-
driven LiDAR simulation gains great attention in the com-
munity. A pioneering work LiDARSim [35] adopts deep
learning models to produce deviations from physics-based
simulations to generate realistic LiDAR point clouds.
3. LiDAR Diffusion Models
In this section, we present LiDAR Diffusion Models
(LiDMs) with details. An overview is shown in Fig. 2. In
Sec. 3.1, we discuss our design choice of range images for
data representation. In Sec. 3.2, we formulate the task of
LiDAR scene generation with LiDMs. In Sec. 3.3, we ex-
plore the design of LiDAR compression in terms of both
pattern and scene geometry realism. In Sec. 3.4, we present
potential applications of LiDM based on multi-modal con-
ditions. In Sec. 3.5, we define the training objectives in the
stages of LiDAR compression and LiDAR Diffusion. Fi-
nally, in Sec. 3.6, we describe three novel perceptual met-
rics designed to quantitatively analyze the sample quality
for LiDAR scene generation.
3.1. Data Representation
LiDAR data can be represented by different modalities.
Since our goal is to simulate raw LiDAR output, we have to
choose a representation that is losslessly converted to and
from raw data, which eliminates optional voxelization. To
this end, we consider two choices, namely 3D point clouds
and 2D range images. Both modalities have been success-
fully explored in previous works, albeit in different settings:
point cloud generation [3, 57, 58] often focuses on human-
crafted objects due to the efficiency limitation, while range
image generation [6, 62] is tailored for larger-scale scenes
such as LiDAR scenes. This separation indicates a clear
preference towards range images for our settings.
3.2. Problem Formulation
Both the input and the output of our generators are repre-
sented as range images, denoted as xandˆx, respectively,
where x,ˆx∈RH×W, and HandWare the height and
width of the range image, respectively. For one pixel, de-
fined by its normalized 2D location and range value, we
can directly compute its depth, yaw, and pitch given pre-
defined parameters. Specifically, we define the 3D coordi-
natep= [α, β, γ ]of the pixel with:
α=cos(yaw) ×cos(pitch) ×depth , (1)
β=−sin(yaw) ×cos(pitch) ×depth , (2)
γ=sin(pitch) ×depth . (3)
Our LiDMs pipeline can be split into two parts: LiDAR
compression and diffusion process. For LiDAR compres-
sion, we use an encoder Eto compress range images intolatent code z=E(x), where z∈Rh×w×d, and a decoder
Dto decode it into ˆx=D(z) =D(E(x)). For the dif-
fusion process, following standard practice [44], we denote
an equally weighted sequence of unconditional denoising
auto-encoders as ϵθ(zt, t), with timestamp t= 1. . . T .
3.3. Towards LiDAR-Realistic Generation
Pattern Realism The presence of curves is a common
pattern in LiDAR scenes. A curve ci= [p1, ..., p ni]is
represented by a sequence of nipoints where consecutive
pairs are connected by a polyline. Furthermore, as defined
in [49], a LiDAR point cloud is equivalent to a set of curves,
namely a curve cloud C={c1, ..., c m}. While we cannot
directly apply the concept of curve cloud to range images,
we can still explore curve-like structures.
Given that each beam of a LiDAR sensor sequentially
captures 3D points by scanning the scene horizontally, we
can safely assume that each curve is stored in only one
row of a range image, and is represented as a continu-
ous segment of pixels. Thus, we design our auto-encoders
through curve-wise compression, which results in horizon-
tally downsampled range images, by a factor fc:= 2η,
where η∈N. To achieve curve-wise compression, we
implement with these details: 1) the kernel size of convo-
lutions inside each curve-wise residual block is 1×4, in-
stead of the traditional 3×3used on images; 2) each down-
sampling or upsampling layer is applied only horizontally;
3) the padding is circular considering the two sides of an
range image to be connected end to end. Through this im-
plementation, we effectively preserve curve-like structures
in a perceptually equivalent space.
Geometry Realism Preserving scene-level geometry is an-
other key aspect in LiDAR-realistic generation. To achieve
this, our model should possess the capability to clearly dis-
tinguish between objects and the background, demonstrat-
ing sensitivity to the contours of 3D geometry. However, the
conversion of point clouds to range images (as described in
Sec. 3.2) may lead to a loss of geometry. Thus, we introduce
a novel point-wise coordinate supervision to enhance the
understanding of autoencoders in 3D space. Point clouds
are informative to describe the geometry of LiDAR scenes
through coordinates, but due to irregularity, we cannot di-
rectly apply point cloud distance loss functions ( e.g., Cham-
fer Distance [13]) to the training of autoencoders. For this
purpose, we design a simple manner by supervising the con-
verted coordinate of each pixel between the input and out-
put range images. Note that, the converted coordinate-based
images are in the shape of H×W×3. Point-wise coordinate
supervision includes both a pixelwise 3D distance loss and
an adversarial objective on the coordinate-based images.
Object Realism Achieving object realism is challenging,
but an important aspect of recovering reasonable and com-
plete shapes. While curve compression effectively captures
14740
Encoder
Encoder
Encoder
MiddleEncoder
Decoder
Decoder
Decoder
Decoder
64×1024  
Output Range ImageMiddle
ℒ𝑟𝑒𝑐 
ℒ𝐺𝐴𝑁 
ℒ𝑉𝑄 Training
𝒟 
ℰ Multimodal
Conditioning
bboxkeypoint
camerastextsemantic map
𝑧𝑇 
𝑧 
×𝑇 
×1 
LiDAR
Compression LiDAR
DiffusionInput Range Image
𝑥  
𝑥 
𝑥  
𝑥 
𝜏𝜃 
CLIP-based conditionstoken-based conditions
… 
… image -based conditions
… 
timestamp
𝑡 
𝜖𝜃 
Curvewise
Residual BlockPatchwise
Residual BlockCurvewise
SamplingPatchwise
SamplingSkip
ConnectionDenoising
ProcessDiffusion
Process
64×1024  
64×1024  
64×512 
32×256 
16×128 
16×128 
16×128 
16×128 
32×256 
64×512 Figure 2. An overview of LiDMs on 64-beam data, which includes
three parts: LiDAR compression ( cf. Sec. 3.3 & 3.5), Multimodal
Conditioning ( cf. Sec. 3.4), and LiDAR Diffusion ( cf. Sec. 3.5).
patterns in LiDAR scenes, it suffers from a restricted recep-
tive field when capturing the complete context of 3D ob-
jects, particularly for visually larger objects in range images
(i.e., objects near the ego-center). Thus, we introduce patch-
wise blocks in the intermediate layers of autoencoders for
patch-wise encoding, which is qualitatively effective as a
way to improve the synthesis quality of objects. We define
the factor of downsampling during patch-wise encoding as
fp= 2µ,µ∈N, and thus h=H/f p,w=W/(fc×fp).
3.4. Multimodal Conditioning
Previous works [44, 60] have shown DMs’ capability to
model conditional distributions. Utilizing our LiDMs, we
further introduce multimodal conditioning to realize the sig-
nificant potential for downstream tasks within the domain
of autonomous driving. Typically, two types of conditions
can serve as inputs in LiDAR scenes: image-based con-
ditions ( e.g., semantic maps), and token-based conditions
(e.g., bounding boxes, keypoints). We approach the ap-
plications of image-based conditioning as image-to-image
translation tasks [21], while we employ the cross-attention
mechanism to handle token-based conditions, aligning with
a widely adopted practice [44]. To broaden the scope ofconditional LiDAR generation, we introduce Camera-to-
LiDAR task by extracting the global features of each view
using a pretrained latent space provided by CLIP [41]. Due
to the spatial mismatch between multiple camera views and
a LiDAR point cloud, we cannot directly treat Camera-to-
LiDAR generation as another image-to-image translation
task. Therefore, for a LiDAR scene, in contrast to tasks em-
ploying the entire image-based condition, we guide LiDMs
with a condition representation formed by concatenating
global features of all camera views.
Most recently, the paradigm of contrastive image-text
pretraining ( e.g., CLIP [41]) has demonstrated remarkable
progress in zero-shot learning [42]. CLIP enables zero-shot
understanding across diverse generation tasks of text-to-
image [43], text-to-video [56], etc. To explore the potential
applications of language-guided autonomous driving [22]
for LiDAR generation, we leverage the text-image latent
space of CLIP to encode descriptive prompts for LiDMs.
Consequently, we can seamlessly transition the Camera-to-
LiDAR task into a novel Text-to-LiDAR generation.
3.5. Training Objectives
LiDAR Compression Through curve-wise compression,
point-wise coordinate supervision and patch-wise encoding,
we design autoencoders to compress range images. To train
these autoencoders, we adopt a set of objectives, including
a pixelwise L1reconstruction objective Lrec, a curve-based
adversarial objective adapted from [21] LGAN and a vector
quantization regularization [53] LVQ. Specifically, we com-
pute both LrecandLGAN with both range and coordinate-
based images as input. We compute Lrecin the parts of
pixelwise L1loss with range images and pixelwise coordi-
nate distance loss with coordinate-based images:
Lrec(x) =Ex[∥x−ˆx∥+λ∥p−ˆp∥2
2], (4)
where λis a scale factor for the supervision of coordinate-
based images. Additionally, we compute LGAN by feed-
ing both range images and coordinate-based images into our
CurveGAN, a variant of PatchGAN [21] by applying curve-
wise compression in the first stage. We define the adversar-
ial objective as:
LGAN(x) =Ex[logD([x, p]) + log(1 − D([ˆx,ˆp]))],(5)
where [. . .]means concatenation operation. Besides, we
utilize the loss of vector quantization [53] LVQto learn a
codebook of range image constituents. We incorporate the
vector quantization layer in the decoder, following the im-
plementation of [12]. Overall, the complete training objec-
tive of autoencoders is:
LAE=Lrec+LGAN +LVQ. (6)
14741
LiDAR Diffusion As probabilistic models, DMs [48] aim
to comprehend a data distribution through a progressive de-
noising process on variables sampled from a Gaussian dis-
tribution. In image synthesis, previous works have em-
ployed DMs either in the pixel space [9, 18] or a latent
space [44]. In this paper, we enable DMs to leverage a low-
dimensional latent space created by our autoencoders. This
space adequately preserves LiDAR patterns and the geom-
etry of scenes and objects while maintaining computational
efficiency. Furthermore, it empowers DMs to focus on the
semantic information of large-scale LiDAR scenes. We de-
fine the objective of our unconditional LiDMs:
LLiDM =EE(x),ϵ∼N(0,1),th
∥ϵ−ϵθ(zt, t)∥2
2i
,(7)
where ϵθ(zt, t)is a UNet [45] backbone with timestamp
conditioning. For inference, we obtain sampled range im-
ages by decoding the denoised latent code zwithD. Simi-
larly, given an input condition y, we define the objective of
ourconditional LiDMs as:
LcLiDM =EE(x),y,ϵ∼N(0,1),th
∥ϵ−ϵθ(zt, t, τθ(y))∥2
2i
,
(8)
where τθis a condition encoder responsible for converting
yinto an accessible representation for the UNet backbone.
3.6. Evaluation Metrics
To evaluate LiDAR generative models, previous works on
LiDAR scene generation [6, 62] commonly utilize statisti-
cal metrics as proposed in [3]. Nonetheless, they may en-
counter difficulties to quantitatively measure the synthesis
quality at a perceptual level. Thus, we design several per-
ceptual level metrics for LiDAR generative models.
Perceptual evaluation ( e.g., FID [17]) stands as a preva-
lent manner to measure image synthesis quality across a
spectrum of popular image-based generators [9, 18, 23–
25, 44, 46]. Typically, these prior works compute the
Fr´echet distance [15] between the data distributions of
of real data and generated samples within a perceptual
space defined by a pretrained classifier ( e.g., Inception
model [50]). To enhance comprehension of the perfor-
mance of LiDAR generative models at a perceptual level,
we augment the perceptual evaluation with three Fr ´echet-
distance-based perceptual metrics: Fr ´echet Range Image
Distance (FRID), Fr ´echet Sparse V olume Distance (FSVD),
and Fr ´echet Point-based V olume Distance (FPVD).
In the absence of a pretrained classifier specifically tai-
lored for LiDAR scenes, we adopt segmentation-based pre-
trained models for our evaluations. Specifically, we cal-
culate FRID, FSVD, FPVD through three simple methods,
which include RangeNet++ [38], MinkowskiNet [8], and
SPVCNN [51], respectively. Different from FID, which is
computed relying on the global feature of each input imagePerceptual Statistical
Method FRID ↓ FSVD↓ FPVD↓ JSD↓MMD↓
(10−4)
Noise 3277 497.1 336.2 0.360 32.09
LiDARGAN [6] 1222 183.4 168.1 0.272 4.74
LiDARV AE [6] 199.1 129.9 105.8 0.237 7.07
LiDARGen [62] (1160s) 129.1 39.2 33.4 0.188 2.88
LiDARGen [62] (50s) 2051 480.6 400.7 0.506 9.91
LDM [44] (50s) 199.5 70.7 61.9 0.236 5.06
LiDM (ours, 50s) 158.8 53.7 42.7 0.213 4.46
∆Improv. 20.4% 24.0% 31.0% 9.7% 11.9%
Table 1. Comparison of unconditional LiDAR scene generation
with recent state-of-the-art methods. We conduct experiments on
64-beam ( i.e., KITTI-360 [31]) data.“ ↓” indicates that lower val-
ues are better. N-s refers to Nsampling steps during inference. ∆
Improv. is the relative improvement of our method compared to the
baseline of Latent Diffusion (LDM) [44], with the same number of
diffusion steps. Note that, the diffusion process of LiDARGen [62]
has 232 levels and 5 iterations in each level ( i.e., 1160 steps in to-
tal). denotes baseline results, while denotes our results.
in the final stage, our proposed metrics are founded on the
average of the output features of each LiDAR scene in the
intermediate stage. With the computed features of samples
and the real-world data, we enable the performance com-
parisons in perceptual space between previous LiDAR gen-
erators and our LiDMs. cf. the supplement for details.
4. Experiments
4.1. Experimental Settings
We train and evaluate our models in the LiDAR scenar-
ios of 32-beam data from nuScenes [14], gathered around
the suburbs in Germany, and 64-beam data from KITTI-
360 [31], collected inside the cities. For the 32-beam sce-
nario, we train autoencoders on the full training dataset,
containing 286,816 samples, and validate with 10,921 sam-
ples. For the 64-beam scenario, we train autoencoders on
63,315 samples of 9 sequences (including training and test
set) and evaluate on 1,031 samples of one sequence. Differ-
ent from autoencoders, we train and validate LiDMs on the
widely adopted subsets of both datasets, which provide var-
ious conditions, including bounding boxes, views of mul-
tiple cameras or perspective views. Specifically, we adopt
SemanticKITTI [5] for Semantic-Map-to-LiDAR task.
Our LiDMs process range images with dimensions of
32×1024 for 32-beam data and 64 ×1024 for 64-beam data.
The pixel values of the range images are computed through
binary logarithm followed by scaling. Training is conducted
on eight NVIDIA RTX 3090, each with 24GB of GPU
memory, and one of them is utilized for inference.
4.2. Unconditional LiDAR Diffusion
To verify the effectiveness of our method, we train un-
conditional LiDMs on 64-beam data from KITTI-360 [31].
14742
Figure 3. Samples from LiDARGen [62], Latent Diffusion [44], and our LiDMs on 64-beam scenario.
Figure 4. Samples from our LiDMs on 32-beam scenario.
We evaluate the performance of each method using FRID
(range image), FSVD (sparse volume), and FPVD (point-
based volume), as described in Sec. 3.6. Together, these
three metrics enable the perceptual evaluation to assess the
quality of LiDAR scenes generated with different methods.
As shown in Table 1, with a very limited number of sam-
pling steps ( i.e., 50), we establish a new state of the art for
almost all considered metrics. Specifically, within 50 sam-
pling steps, our approach outperforms the previous state-of-
the-art method ( i.e., LiDARGen [62]) by a large margin for
all considered metrics. Additionally, with only 50 evalua-
tion steps, our method performs competitive with LiDAR-
Gen with a longer diffusion process of 1160 steps. Simul-
taneously, LiDMs reports improvements between 9.7%and
31.0%relative to the baseline model LDM [44]. For a qual-
itative comparison, in Fig. 3 we provide examples generated
with each model, alongside reference point clouds. We fur-
Figure 5. Samples from our LiDM for Semantic-Map-to-LiDAR
generation on SemanticKITTI [5].
ther provide some 32-beam samples for example in Fig. 4.
4.3. Conditional LiDAR Diffusion
To further exploit the potential of LiDMs, we imple-
ment several variations of conditional LiDAR scene gener-
ation, including Semantic-Map-to-LiDAR and Camera-to-
LiDAR. For a quantitative analysis, we compare LiDMs to
LiDARGen [62] and to our baseline Latent Diffusion [44],
with results reported in Table 2.
14743
Semantic-Map-to-LiDAR [5] Camera-to-LiDAR [31]
Method FRID↓ FSVD↓ FPVD↓ JSD↓MMD↓
(10−4)FRID↓ FSVD↓ FPVD↓ JSD↓MMD↓
(10−4)
LiDARGen [62] 42.5 31.7 30.1 0.130 5.18 - - - - -
Latent Diffusion [44] 24.0 21.3 20.3 0.088 3.73 50.2 35.9 26.5 0.256 3.80
LiDAR Diffusion (ours) 22.9 20.2 17.7 0.072 3.16 44.9 32.5 25.8 0.205 3.69
Table 2. Comparison of conditional LiDAR scene generation with recent state-of-the-art methods. We conduct Semantic-Map-to-LiDAR
experiments on SemanticKITTI [5] and Camera-to-LiDAR on KITTI-360 [31]. “ ↓” indicates that lower values are better. We implement
Semantic-Map-to-LiDAR on LiDARGen through the concatenation operation. Camera-to-LiDAR on LiDARGen is not viable through
concatenation, and hence we do not report results in this setting.
Input
GT
Sample
3D View
(GT | Sample)
Input
GT
Sample
3D View
(GT | Sample)
Figure 6. LiDM samples for conditional Camera-to-LiDAR gen-
eration on KITTI-360 [31]. The orange box indicates the area cov-
ered by the input image. For each scene, KITTI-360 provides one
perspective, which cover only a part of the scene. Thus, LiDM per-
forms conditional generation for the camera-covered region and
unconditional generation for the remaining unobserved regions.
Semantic-Map-to-LiDAR Transforming semantic maps
into RGB images is a typical image-to-image translation
task [21]. However, it remains underexplored in the context
of LiDAR scene generation. As shown in our reported re-
sults, LiDM outperforms Latent Diffusion and LiDARGen
by a substantial margin. Additionally, conditioning LiDMs
with semantic maps leads to significant improvements rela-
tive to unconditional generation. We argue that having ac-
cess to such data enhances the understanding of LiDMs at a
semantic level, which facilitates the generation of LiDAR-
realistic scenes. Fig. 5 further illustrates the effectiveness
of semantic-map-based conditioning with LiDMs.
Camera-to-LiDAR Camera views are commonplace in
the context of autonomous driving. To explore the rela-tionship and complementarity between cameras and LiDAR
sensor data, we implement Camera-to-LiDAR generation
on KITTI-360 [31]. In this setting, LiDMs outperform Li-
DARGen [62] by over 36% among all metrics, while also
successfully capturing semantic information from camera
views. In Fig. 6, on the top, we see LiDM generating
smooth ground from an input image containing a road with-
out any objects. Similarly, on the bottom we see LiDM ex-
tracting the semantic information about the presence of a
car and generating it on the synthesized LiDAR scene, al-
though it still struggles with its precise scale in 3D space.
4.4. Zero-shot Text-to-LiDAR Generation
Text-to-image learning has become very popular recently
due to the introduction of contract language-vision pre-
training paradigm [41]. To facilitate LiDAR generation
with language-guided conditioning, we introduce zero-shot
Text-to-LiDAR generation based on a pretrained Camera-
to-LiDAR LiDM, which is transformable to the task of
Text-to-LiDAR. Through provided prompts, LiDM can hal-
lucinate possible scenes related to the input prompts. Fig. 7
shows some evidence to this argument. However, Text-to-
LiDAR LiDM still struggles to generate scenes when pre-
sented with complex prompts, primarily due to constraints
imposed by the limited amount of available training data.
4.5. Study on LiDAR-Realistic Generation
We explore our designed autoencoders for LiDAR compres-
sion and ablate on our proposed point-wise coordinate su-
pervision. To analyze the behavior of LiDAR compression
in terms of curve-wise and patch-wise encoding, we con-
duct experiments on various scale factors. The results in
Fig. 8 show that curve-wise encoding generally performs
better than patch-wise encoding. However, by introduc-
ing one stage of patch-wise encoding, we allow the autoen-
coders to further compress range images while maintaining
competitive performance. To balance between performance
and compression rate, we chose fc= 2 andfp= 4 as the
default settings for our experiments.
Additionally, we study the effectiveness of our proposed
point-wise coordinate supervision. The visualization in
14744
Figure 7. LiDM samples for zero-shot Text-to-LiDAR generation
on 64-beam scenario. The areas enclosed by orange dotted lines
indicate those influenced by the conditioning, and green boxes
highlight objects potentially associated with the prompts.
Figure 8. Overall scale factor ( fc×fp) vs sampling quality (FRID
& FSVD). We compare different scales of curve-wise encoding
(Curve), patch-wise encoding (Patch), and curve-wise encoding
with one (C+1P) or two (C+2P) stages of patch-wise encoding on
KITTI-360 [31].
Fig. 9 illustrates that point-wise coordinate supervision aids
autoencoders in preserving scene-level geometry by recon-
structing sharper boundaries in 3D space.
4.6. Efficiency Analysis
Efficiency is particularly important for LiDAR generative
models, specially when considering adoption to down-
stream tasks. To investigate this aspect of LiDMs, we pro-
vide an overhead comparison between LiDMs and the pre-
vious state-of-the-art point-based DM, LiDARGen [62], in
terms of throughput ( samples/sec ), inference speed ( step-
w/o pointwise supervision w/ pointwise supervisionFigure 9. Samples from LiDM with or without point-wise super-
vision, as proposed in Sec. 3.3.
Method Diffusion Size Throughput ↑ Infer.Speed ↑
LiDARGen [62] 64 ×1024 0.015 17.5
LiDM (ours) 16×128 1.603 80.2
Table 3. Efficiency of LiDARGen and our LiDMs on the 64-
beam scenario. We compute the number of generated samples per
second of each model as throughput. Infer. Speed is the number
of inference passes ( i.e., one pass representes a diffusion step) per
second. We test both models in one NVIDIA RTX 3090 and adjust
batch size to make full use of 24GB GPU memory.
s/sec ). As shown in Table 3, the throughput and inference
speed of LiDM is around ×107 and ×4.6 faster than Li-
DARGen [62], respectively, which shows the superiority of
our method in terms of efficiency.
5. Limitations
LiDM suffers from a visual gap relative to real-world Li-
DAR data. As shown in previous work [44], diffusion mod-
els should be powerful enough to capture the semantic in-
formation of input data, and therefore we argue that autoen-
coders are the key to failures when recovering most details
of scenes. Our contributions are a step towards LiDAR-
realistic autoencoders, however further work is still re-
quired. For example, autoencoders may reconstruct blurry
boundaries between objects and the background, which
though imperceptible on range images, may lead to visually
unreasonable objects in 3D space.
6. Conclusion
We propose LiDAR Diffusion Models (LiDMs), a general-
conditioning framework for LiDAR scene generation. With
a focus on preserving curve-like patterns as well as scene-
level and object-level geometry, we design an efficient la-
tent space for DMs to achieve LiDAR-realistic generation
This design empowers our LiDMs to achieve competitive
performance in unconditional generation and state of the art
in conditional generation under 64-beam scenario, and en-
ables the controllability of LiDMs with diverse conditions,
including semantic maps, camera views, and text prompts.
To the best of our knowledge, ours is the first method to
successfully introduce conditioning to LiDAR generation.
14745
References
[1] Midjourney. https://www.midjourney.com . 1, 2
[2] Stable diffusion. https://github.com/CompVis/
stable-diffusion . 1, 2
[3] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and
Leonidas Guibas. Learning representations and generative
models for 3d point clouds. In International conference on
machine learning , pages 40–49. PMLR, 2018. 3, 5
[4] Alexander Amini, Tsun-Hsuan Wang, Igor Gilitschenski,
Wilko Schwarting, Zhijian Liu, Song Han, Sertac Karaman,
and Daniela Rus. Vista 2.0: An open, data-driven simulator
for multimodal sensing and policy learning for autonomous
vehicles. In 2022 International Conference on Robotics and
Automation (ICRA) , pages 2419–2426. IEEE, 2022. 2
[5] Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-
zel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Se-
mantickitti: A dataset for semantic scene understanding of
lidar sequences. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 9297–9307,
2019. 5, 6, 7
[6] Lucas Caccia, Herke Van Hoof, Aaron Courville, and Joelle
Pineau. Deep generative modeling of lidar data. In 2019
IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS) , pages 5034–5040. IEEE, 2019. 2, 3,
5
[7] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tuyakov, Alex
Schwing, and Liangyan Gui. SDFusion: Multimodal 3d
shape completion, reconstruction, and generation. arXiv ,
2022. 2
[8] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d
spatio-temporal convnets: Minkowski convolutional neural
networks. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 3075–3084,
2019. 5
[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems , 34:8780–8794, 2021. 2, 5
[10] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Anto-
nio Lopez, and Vladlen Koltun. Carla: An open urban driv-
ing simulator. In Conference on robot learning , pages 1–16.
PMLR, 2017. 2
[11] Ziya Erkoc ¸, Fangchang Ma, Qi Shan, Matthias Nießner, and
Angela Dai. Hyperdiffusion: Generating implicit neural
fields with weight-space diffusion, 2023. 2
[12] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
transformers for high-resolution image synthesis. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 12873–12883, 2021. 4
[13] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set
generation network for 3d object reconstruction from a single
image. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 605–613, 2017. 3
[14] Whye Kit Fong, Rohit Mohan, Juana Valeria Hurtado, Lub-
ing Zhou, Holger Caesar, Oscar Beijbom, and Abhinav Val-
ada. Panoptic nuscenes: A large-scale benchmark for lidar
panoptic segmentation and tracking. IEEE Robotics and Au-
tomation Letters , 7(2):3795–3802, 2022. 5[15] Maurice Fr ´echet. Sur la distance de deux lois de probabilit ´e.
InAnnales de l’ISUP , pages 183–198, 1957. 5
[16] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Bar-
las O ˘guz. 3dgen: Triplane latent diffusion for textured mesh
generation. arXiv preprint arXiv:2303.05371 , 2023. 2
[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems ,
30, 2017. 5
[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840–6851, 2020. 2, 5
[19] Jingyu Hu, Ka-Hei Hui, Zhengzhe Liu, Ruihui Li, and Chi-
Wing Fu. Neural wavelet-domain diffusion for 3d shape
generation, inversion, and manipulation. arXiv preprint
arXiv:2302.00190 , 2023. 2
[20] Jordan SK Hu and Steven L Waslander. Pattern-aware data
augmentation for lidar 3d object detection. In 2021 IEEE
International Intelligent Transportation Systems Conference
(ITSC) , pages 2703–2710. IEEE, 2021. 2
[21] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adver-
sarial networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1125–1134,
2017. 4, 7
[22] Kanishk Jain, Varun Chhangani, Amogh Tiwari, K Mad-
hava Krishna, and Vineet Gandhi. Ground then navigate:
Language-guided navigation in dynamic scenes. In 2023
IEEE International Conference on Robotics and Automation
(ICRA) , pages 4113–4120. IEEE, 2023. 4
[23] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of gans for improved quality, stability,
and variation. arXiv preprint arXiv:1710.10196 , 2017. 5
[24] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 4401–4410, 2019.
[25] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improv-
ing the image quality of stylegan. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 8110–8119, 2020. 5
[26] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan
Ho. Variational diffusion models. Advances in neural infor-
mation processing systems , 34:21696–21707, 2021. 2
[27] Nathan Koenig and Andrew Howard. Design and use
paradigms for gazebo, an open-source multi-robot simula-
tor. In 2004 IEEE/RSJ international conference on intelli-
gent robots and systems (IROS)(IEEE Cat. No. 04CH37566) ,
pages 2149–2154. IEEE, 2004. 2
[28] Bo Li, Tianlei Zhang, and Tian Xia. Vehicle detection from
3d lidar using fully convolutional network. arXiv preprint
arXiv:1608.07916 , 2016. 2
[29] Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. Diffusion-
sdf: Text-to-shape via voxelized diffusion. arXiv preprint
arXiv:2212.03293 , 2022. 2
14746
[30] Yuhan Li, Yishun Dou, Xuanhong Chen, Bingbing Ni, Yilin
Sun, Yutian Liu, and Fuzhen Wang. 3dqd: Generalized deep
3d shape prior via part-discretized diffusion process. arXiv
preprint arXiv:2303.10406 , 2023. 2
[31] Yiyi Liao, Jun Xie, and Andreas Geiger. Kitti-360: A novel
dataset and benchmarks for urban scene understanding in 2d
and 3d. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 45(3):3292–3310, 2022. 5, 7, 8
[32] Zhen Liu, Yao Feng, Michael J. Black, Derek
Nowrouzezahrai, Liam Paull, and Weiyang Liu. Meshd-
iffusion: Score-based generative 3d mesh modeling. In
International Conference on Learning Representations ,
2023. 2
[33] Shitong Luo and Wei Hu. Diffusion probabilistic models for
3d point cloud generation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 2837–2845, 2021. 2
[34] Zhaoyang Lyu, Jinyi Wang, Yuwei An, Ya Zhang, Dahua
Lin, and Bo Dai. Controllable mesh generation through
sparse latent point diffusion models. arXiv preprint
arXiv:2303.07938 , 2023. 2
[35] Sivabalan Manivasagam, Shenlong Wang, Kelvin Wong,
Wenyuan Zeng, Mikita Sazanovich, Shuhan Tan, Bin Yang,
Wei-Chiu Ma, and Raquel Urtasun. Lidarsim: Realistic lidar
simulation by leveraging the real world. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 11167–11176, 2020. 2, 3
[36] Luke Melas-Kyriazi, Christian Rupprecht, and Andrea
Vedaldi. Pc2: Projection-conditioned point cloud diffusion
for single-image 3d reconstruction. In Arxiv , 2023. 2
[37] Gregory P Meyer, Ankit Laddha, Eric Kee, Carlos Vallespi-
Gonzalez, and Carl K Wellington. Lasernet: An efficient
probabilistic 3d object detector for autonomous driving. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 12677–12686, 2019. 2
[38] Andres Milioto, Ignacio Vizzo, Jens Behley, and Cyrill
Stachniss. Rangenet++: Fast and accurate lidar semantic
segmentation. In 2019 IEEE/RSJ international conference
on intelligent robots and systems (IROS) , pages 4213–4220.
IEEE, 2019. 2, 5
[39] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741 , 2021. 2
[40] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela
Mishkin, and Mark Chen. Point-e: A system for generat-
ing 3d point clouds from complex prompts. arXiv preprint
arXiv:2212.08751 , 2022. 2
[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 4, 7
[42] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.Zero-shot text-to-image generation. In International Confer-
ence on Machine Learning , pages 8821–8831. PMLR, 2021.
4
[43] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents. arXiv preprint arXiv:2204.06125 , 1
(2):3, 2022. 2, 4
[44] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684–10695, 2022. 1, 2, 3, 4, 5, 6, 7, 8
[45] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III
18, pages 234–241. Springer, 2015. 5
[46] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems , 35:36479–36494, 2022. 5
[47] J Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner,
Jiajun Wu, and Gordon Wetzstein. 3d neural field generation
using triplane diffusion. arXiv preprint arXiv:2211.16677 ,
2022. 2
[48] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International confer-
ence on machine learning , pages 2256–2265. PMLR, 2015.
2, 5
[49] Colton Stearns, Jiateng Liu, Davis Rempe, Despoina
Paschalidou, Jeong Joon Park, Sebastien Mascha, and
Leonidas J Guibas. Curvecloudnet: Processing point clouds
with 1d structure. arXiv preprint arXiv:2303.12050 , 2023.
2, 3
[50] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception archi-
tecture for computer vision. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
2818–2826, 2016. 5
[51] Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin,
Hanrui Wang, and Song Han. Searching efficient 3d architec-
tures with sparse point-voxel convolution. In European con-
ference on computer vision , pages 685–702. Springer, 2020.
5
[52] Michał J Tyszkiewicz, Pascal Fua, and Eduard Trulls. Gecco:
Geometrically-conditioned point diffusion models. arXiv
preprint arXiv:2303.05916 , 2023. 2
[53] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete
representation learning. Advances in neural information pro-
cessing systems , 30, 2017. 4
[54] Tsun-Hsuan Wang, Alexander Amini, Wilko Schwarting,
Igor Gilitschenski, Sertac Karaman, and Daniela Rus. Learn-
ing interactive driving policies via data-driven simulation. In
14747
2022 International Conference on Robotics and Automation
(ICRA) , pages 7745–7752. IEEE, 2022. 2
[55] Bichen Wu, Alvin Wan, Xiangyu Yue, and Kurt Keutzer.
Squeezeseg: Convolutional neural nets with recurrent crf for
real-time road-object segmentation from 3d lidar point cloud.
In2018 IEEE international conference on robotics and au-
tomation (ICRA) , pages 1887–1893. IEEE, 2018. 2
[56] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko,
Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and
Christoph Feichtenhofer. Videoclip: Contrastive pre-training
for zero-shot video-text understanding. arXiv preprint
arXiv:2109.14084 , 2021. 4
[57] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge
Belongie, and Bharath Hariharan. Pointflow: 3d point cloud
generation with continuous normalizing flows. In Proceed-
ings of the IEEE/CVF international conference on computer
vision , pages 4541–4550, 2019. 3
[58] Yaoqing Yang, Chen Feng, Yiru Shen, and Dong Tian. Fold-
ingnet: Point cloud auto-encoder via deep grid deformation.
InProceedings of the IEEE conference on computer vision
and pattern recognition , pages 206–215, 2018. 3
[59] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic,
Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent
point diffusion models for 3d shape generation. In Advances
in Neural Information Processing Systems (NeurIPS) , 2022.
2
[60] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3836–3847, 2023. 1, 2, 4
[61] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation
and completion through point-voxel diffusion. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision (ICCV) , pages 5826–5835, 2021. 2
[62] Vlas Zyrianov, Xiyue Zhu, and Shenlong Wang. Learning to
generate realistic lidar point clouds. In European Conference
on Computer Vision , pages 17–35. Springer, 2022. 2, 3, 5, 6,
7, 8
14748
