PRDP: Proximal Reward Difference Prediction
for Large-Scale Reward Finetuning of Diffusion Models
Fei Deng1,2*, Qifei Wang1, Wei Wei3†, Tingbo Hou1, Matthias Grundmann1
1Google,2Rutgers University,3Accenture
https://fdeng18.github.io/prdp
A painting of a girl standing 
on a mountain looking out 
at an approaching storm 
over the ocean, with wind 
blowing and ocean mist, 
surrounded by lightning. 
A night scene of a lavender 
ﬁeld with a town and 
church in the background, 
reminiscent of Vincent van 
Gogh's style. 
cinematic still of 
an adorable 
walking robot in 
the desert, at 
sunset 
a close up of a cat 
wearing a pikachu 
hat, reddit, gif, real 
life charmander, 
very aesthetic!!!!!!, 
soft!! 
rural house with 
a garden and a 
swimming pool 
An abandoned 
Segway in the forest Stable Diffusion v1.4 PRDP 
A corgi dressed as a 
bee costume. 
Figure 1. Generation samples on complex, unseen prompts. Our proposed method, PRDP, achieves stable black-box reward finetuning
for diffusion models for the first time on large-scale prompt datasets, leading to superior generation quality on complex, unseen prompts.
Here, PRDP is finetuned from Stable Diffusion v1.4 on the training set prompts of Pick-a-Pic v1 dataset, using a weighted combination of
rewards: PickScore = 10 , HPSv2 = 2, Aesthetic = 0.05. The images within each column are generated using the same random seed.
Abstract
Reward finetuning has emerged as a promising approach
to aligning foundation models with downstream objectives.
Remarkable success has been achieved in the language do-
main by using reinforcement learning (RL) to maximize re-
wards that reflect human preference. However, in the vi-
sion domain, existing RL-based reward finetuning methods
are limited by their instability in large-scale training, ren-
dering them incapable of generalizing to complex, unseen
prompts. In this paper, we propose Proximal Reward Dif-
ference Prediction (PRDP), enabling stable black-box re-
ward finetuning for diffusion models for the first time on
large-scale prompt datasets with over 100K prompts. Our
key innovation is the Reward Difference Prediction (RDP)
objective that has the same optimal solution as the RL ob-
*Work done during an internship at Google.
†Work done while working at Google.jective while enjoying better training stability. Specifically,
the RDP objective is a supervised regression objective that
tasks the diffusion model with predicting the reward differ-
ence of generated image pairs from their denoising trajec-
tories. We theoretically prove that the diffusion model that
obtains perfect reward difference prediction is exactly the
maximizer of the RL objective. We further develop an online
algorithm with proximal updates to stably optimize the RDP
objective. In experiments, we demonstrate that PRDP can
match the reward maximization ability of well-established
RL-based methods in small-scale training. Furthermore,
through large-scale training on text prompts from the Hu-
man Preference Dataset v2 and the Pick-a-Pic v1 dataset,
PRDP achieves superior generation quality on a diverse
set of complex, unseen prompts whereas RL-based methods
completely fail.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
7423
1. Introduction
Diffusion models have achieved remarkable success in gen-
erative modeling of continuous data, especially in photore-
alistic text-to-image synthesis [7, 15, 30, 36, 37, 40, 44, 46].
However, the maximum likelihood training objective of dif-
fusion models is often misaligned with their downstream
use cases, such as generating novel compositions of objects
unseen during training, and producing images that are aes-
thetically preferred by humans.
A similar misalignment problem exists in language mod-
els, where exactly matching the model output to the training
distribution tends to yield undesirable model behavior. For
example, the model may output biased, toxic, or harmful
content. A successful solution, called reinforcement learn-
ing from human feedback (RLHF) [2, 31, 47, 61], is to use
reinforcement learning (RL) to finetune the language model
such that it maximizes some reward function that reflects
human preference. Typically, the reward function is defined
by a reward model pretrained from human preference data.
Inspired by the success of RLHF in language models,
researchers have developed several reward models in the vi-
sion domain [22, 23, 53–55] that are similarly trained to be
aligned with human preference. Furthermore, two recent
works, DDPO [4] and DPOK [10], have explored using RL
to finetune diffusion models. They both view the denoising
process as a Markov decision process [9], and apply policy
gradient methods such as PPO [42] to maximize rewards.
However, policy gradients are notoriously prone to high
variance, causing training instability. To reduce variance, a
common approach is to normalize the rewards by subtract-
ing their expected value [48, 51]. DPOK fits a value func-
tion to estimate the expected reward, showing promising re-
sults when trained on ∼200prompts. Alternatively, DDPO
maintains a separate buffer for each prompt to track the
mean and variance of rewards, demonstrating stable train-
ing on ∼400prompts and better performance than DPOK.
Nevertheless, we find that DDPO still suffers from training
instability on larger numbers of prompts, depriving it of the
benefits offered by training on large-scale prompt datasets.
In this paper, we propose Proximal Reward Difference
Prediction (PRDP), a scalable reward maximization algo-
rithm that does not rely on policy gradients. To the best of
our knowledge, PRDP is the first method that achieves sta-
ble large-scale finetuning of diffusion models on more than
100K prompts for black-box reward functions.
Inspired by the recent success of DPO [35] that converts
the RLHF objective for language models into a supervised
classification objective, we derive for diffusion models a
new supervised regression objective, called Reward Differ-
ence Prediction (RDP), that has the same optimal solution
as the RLHF objective while enjoying better training sta-
bility. Specifically, our RDP objective tasks the diffusion
model with predicting the reward difference of generatedimage pairs from their denoising trajectories. We prove that
the diffusion model that obtains perfect reward difference
prediction is exactly the maximizer of the RLHF objective.
We further propose proximal updates and online optimiza-
tion to improve training stability and generation quality.
Our contributions are summarized as follows:
• We propose PRDP, a scalable reward finetuning method
for diffusion models, with a new reward difference pre-
diction objective and its stable optimization algorithm.
• PRDP achieves stable black-box reward maximization for
diffusion models for the first time on large-scale prompt
datasets with over 100K prompts.
• PRDP exhibits superior generation quality and general-
ization to unseen prompts through large-scale training.
2. Preliminaries
In this section, we briefly introduce the generative process
of denoising diffusion probabilistic models (DDPMs) [15,
44, 46]. Given a text prompt c, a text-to-image DDPM πθ
with parameters θdefines a text-conditioned image distri-
bution πθ(x0|c)as follows:
πθ(x0|c) =Z
πθ(x0:T|c) dx1:T
=Z
p(xT)TY
t=1πθ(xt−1|xt,c) dx1:T,(1)
where x0is the image, and x1:Tare latent variables of the
same dimension as x0. Typically, p(xT) =N(0,I), and
πθ(xt−1|xt,c) =N(xt−1;µθ(xt,c), σ2
tI) (2)
is a Gaussian distribution with learnable mean and fixed co-
variance. To generate an image x0∼πθ(x0|c), DDPM
uses ancestral sampling. That is, it samples the full de-
noising trajectory x0:T∼πθ(x0:T|c), by first sampling
xT∼p(xT), and then sampling xt−1∼πθ(xt−1|xt,c)
fort=T, . . . , 1. Conversely, given a denoising trajectory
x0:T, we can analytically compute its log-likelihood as
logπθ(x0:T|c) = log p(xT) +TX
t=1logπθ(xt−1|xt,c)(3)
=−1
2TX
t=1∥xt−1−µθ(xt,c)∥2
σ2
t+C,(4)
where Cis a constant independent of θ.
3. Method
3.1. Reward Difference Prediction for
KL-Regularized Reward Maximization
We start derivation from the typical RLHF objective [10]:
max
πθEx0,c[r(x0,c)−βKL[πθ(x0|c)||πref(x0|c)]].(5)
7424
<latexit sha1_base64="haqajY4GJXrL2mrJ8jNim4M/0sc=">AAACT3icbVHLSgNBEJyN7/hK9OhlMQiCEHbF1zHoxaOCUSEveie9OmRmZ5npVcOS//Cqf+TRL/EmTuIeNFowUFT1QHV1lEphKQjevdLM7Nz8wuJSeXlldW29Ut24tjozHJtcS21uI7AoRYJNEiTxNjUIKpJ4Ew3Oxv7NAxordHJFwxQ7Cu4SEQsO5KRuWwHdR3H+NOpddaFXqQX1YAL/LwkLUmMFLnpVb6/d1zxTmBCXYG0rDFLq5GBIcImjcjuzmAIfwB22HE1Aoe3kk9gjf8cpfT/Wxr2E/In680cOytqhitzkOKad9sbiv16k/pe1HhBE9lesXMEAOUo5pWaShNGPUytQfNLJRZJmhAn/3iDOpE/aH5fr94VBTnLoCHAjXAk+vwcDnNwJyq7ecLrMv+R6vx4e1Q8vD2qN06LoRbbFttkuC9kxa7BzdsGajDPDntkLe/XevA/vs1SMlryCbLJfKC19AbZztQI=</latexit>xaT<latexit sha1_base64="Fom46tFNC2axun2/lzxGH57YcN8=">AAACT3icbVHLSgNBEJyN7/iKevSyGARBCLvi6yh68ahgNJDE0DvpjUNmdpaZXjUs+Q+v+kce/RJv4iTuQRMLBoqqHqiujlIpLAXBh1eamZ2bX1hcKi+vrK6tVzY2b63ODMc611KbRgQWpUiwToIkNlKDoCKJd1H/YuTfPaKxQic3NEixraCXiFhwICfdtxTQQxTnz8NOcA+dSjWoBWP40yQsSJUVuOpsePutruaZwoS4BGubYZBSOwdDgksclluZxRR4H3rYdDQBhbadj2MP/V2ndP1YG/cS8sfq7x85KGsHKnKTo5h20huJ/3qR+l/Wuk8Q2T+xcgV95CjlhJpJEkY/TaxA8Wk7F0maESb8Z4M4kz5pf1Su3xUGOcmBI8CNcCX4/AEMcHInKLt6w8kyp8ntQS08rh1dH1bPzouiF9k222F7LGQn7IxdsitWZ5wZ9sJe2Zv37n16X6VitOQVZIv9QWnpG3LztN4=</latexit>xa0
<latexit sha1_base64="noMSQL7xl9siHhxPj1KOXGiIPf0=">AAACT3icbVHLSgMxFM3UV1tfVZduBosgCGVGfC1FNy4r2CrYB0l6R0OTyZDcUcvQ/3Crf+TSL3EnZuosbPVA4HDODZx7LkuksBgEH15pbn5hcalcqS6vrK6t1zY221anhkOLa6nNLaMWpIihhQIl3CYGqGISbtjwIvdvHsFYoeNrHCXQVfQ+FpHgFJ3U6yiKDyzKnsf9oMf6tXrQCCbw/5KwIHVSoNnf8PY7A81TBTFySa29C4MEuxk1KLiEcbWTWkgoH9J7uHM0pgpsN5vEHvu7Thn4kTbuxehP1N8/MqqsHSnmJvOYdtbLxX89pv6XtR4iZXYqVqboEDhIOaOmEoXRTzMrYHTazUScpAgx/9kgSqWP2s/L9QfCAEc5coRyI1wJPn+ghnJ0J6i6esPZMv+S9kEjPG4cXR3Wz86Lostkm+yQPRKSE3JGLkmTtAgnhryQV/LmvXuf3lepGC15BdkiUyhVvgF00bTf</latexit>xb0
<latexit sha1_base64="zt1wwPog588gd1FJF40rs/0Vv5s=">AAACT3icbVHLSgMxFM3UV1tfVZduBosgCGVGfC1FNy4r9AW2liS9o6HJZEjuqGXof7jVP3Lpl7gT0zoLrT0QOJxzA+eeyxIpLAbBh1dYWFxaXimWyqtr6xubla3tltWp4dDkWmrTYdSCFDE0UaCETmKAKiahzYZXE7/9CMYKHTdwlEBP0ftYRIJTdNJdV1F8YFH2PO437li/Ug1qwRT+fxLmpEpy1Ptb3mF3oHmqIEYuqbW3YZBgL6MGBZcwLndTCwnlQ3oPt47GVIHtZdPYY3/fKQM/0sa9GP2p+vtHRpW1I8Xc5CSmnfUm4lyPqfmy1kOkzP6JlSk6BA5SzqipRGH008wKGJ33MhEnKULMfzaIUumj9ifl+gNhgKMcOUK5Ea4Enz9QQzm6E5RdveFsmf9J66gWntZObo6rF5d50UWyS/bIAQnJGbkg16ROmoQTQ17IK3nz3r1P76uQjxa8nOyQPyiUvgG4UbUD</latexit>xbT
Denoising Trajectories
<latexit sha1_base64="WDjOdA0i6OLCz1lfUtQGSiN9mWY=">AAACXHicdVFdSxtBFJ2s2tqobbRQkL4MDQVBWHZDYuKb2Jc+RmhUcMNyd3JjhszsLDN3W8IS/DW+tr/Hl/6WTj4KNdgDA4dz7oV7zmSFko6i6KkWbG3vvHq9+6a+t3/w9l3j8OjamdIKHAijjL3NwKGSOQ5IksLbwiLoTOFNNv2y8G++o3XS5N9oVuBQw30ux1IAeSltHCeFTKuEJkiQJhpoYnVl1Gg+TxvNKOy047h7xqOwG3V6vZYn591Wp33O4zBaosnW6KeHtdNkZESpMSehwLm7OCpoWIElKRTO60npsAAxhXu88zQHjW5YLTPM+WevjPjYWP9y4kv1340KtHMznfnJxZVu01uIL3qZflk2ZkqQuWdnVRqmKFCpDbVUJK35sRGBxr1hJfOiJMzFKsG4VJwMXzTNR9KiIDXzBISVvgQuJmBBkP+Puq/3b4f8/+S6FcZnYeeq3by4XBe9yz6yT+yExazLLthX1mcDJtgDe2Q/2a/a72A72AsOVqNBbb3znj1D8OEPdJ+4xg==</latexit>⇡✓oldPrompt<latexit sha1_base64="DmLEuAjHFa6PUY3gX90++yDL/yE=">AAACS3icbVDLSgNBEJyNrxjfevSyGARBCLvi6xj04jGCScRkkdlJrw6ZxzLTq4Qlf+FV/8gP8Du8iQcnj4NZLWgoqrqhuuJUcItB8OGV5uYXFpfKy5WV1bX1jc2t7ZbVmWHQZFpocxtTC4IraCJHAbepASpjAe24fzny209gLNfqBgcpRJI+KJ5wRtFJd11J8TFOcja836wGtWAM/y8Jp6RKpmjcb3mH3Z5mmQSFTFBrO2GQYpRTg5wJGFa6mYWUsj59gI6jikqwUT6OPPT3ndLzE23cKPTH6u+LnEprBzJ2m6OItuiNxH+9WP4va91HGtuZWLmkfWAgREHNBHKjnwsvYHIe5VylGYJikw+STPio/VGxfo8bYCgGjlBmuCvBZ4/UUIau/oqrNyyW+Ze0jmrhae3k+rhav5gWXSa7ZI8ckJCckTq5Ig3SJIwo8kJeyZv37n16X973ZLXkTW92yAxKCz9UlrRS</latexit>c<latexit sha1_base64="Qh7uIRmJJadq4D6oD95GGWa9JgQ=">AAACS3icdZDfSltBEMb3pNXatFVjL3uzNAiFwmFPSWxyJ3rTSwuNiiaEOZuJWbJ/DrtzlHDwLby1b+QD9Dl6J150E1OowQ4s+/F9MzDzywutAgnxK6m9eLm2/mrjdf3N23ebW9uNnePgSi+xJ512/jSHgFpZ7JEijaeFRzC5xpN8ejjPTy7RB+XsD5oVODBwYdVYSaBonfULNezTBAmG202RZiLr7mVcpK226HZEFN353+JZKhbVZMs6GjaSz/2Rk6VBS1JDCOeZKGhQgSclNV7X+2XAAuQULvA8SgsGw6BarHzNd6Mz4mPn47PEF+6/ExWYEGYmj50GaBJWs7n5bJab523npgR5eLJWZWCKErVecUtNyrurlRNo3BlUyhYloZWPF4xLzcnxOVg+Uh4l6VkUIL2KELicgAdJEX894v3LkP9fHH9Js720/b3V3D9Ygt5gH9hH9oll7CvbZ9/YEesxySy7YbfsZ3KX/E7uk4fH1lqynHnPnlRt7Q+Xt7R5</latexit>⇡✓<latexit sha1_base64="Fz3HpDd467TApzeuRyaDA46DhA4=">AAACYHicbVHRShtBFJ2srU3T1iT6phSGBsHSEnZLW32U+uKjglHBrOHu5G4yZGZnmbnbNix58mt8tV/T135JJ3EFTbwwcO4598I9Z5JcSUdh+LcWrL14uf6q/rrx5u27jWarvXnuTGEF9oRRxl4m4FDJDHskSeFlbhF0ovAimRzN9YufaJ002RlNc4w1jDKZSgHkqUHrvd3ra6Bxkpa/Z4PwGj7zh17MPg5anbAbLoqvgqgCHVbVyaBd+9QfGlFozEgocO4qCnOKS7AkhcJZo184zEFMYIRXHmag0cXlwseM73pmyFNj/cuIL9jHGyVo56Y68ZPzG92yNief1RL9PG3MhCBxT84qNUxQoFJLbKFIWvNryQKlB3Eps7wgzMS9g7RQnAyfp82H0qIgNfUAhJU+BC7GYEGQ/5OGjzdaDnMVnH/pRt+7306/dg5/VEHX2Q77wPZYxPbZITtmJ6zHBLtht+yO/an9C+pBM2jfjwa1ameLPalg+z8SWrjf</latexit>r(xa0,c)
<latexit sha1_base64="Zf7ppOYqCpt6PSCW1Yj9/1rWgnA=">AAACYHicbVFNSwMxEE3Xr1q/Wr0pQrAIilJ2xa+j6MWjglXB1pKksxqabJZkVi1LT/4ar/prvPpLTGsFrQ4E3rw3A/NeeKqkwzB8LwRj4xOTU8Xp0szs3PxCubJ46UxmBdSFUcZec+ZAyQTqKFHBdWqBaa7gindO+vrVA1gnTXKB3RSamt0lMpaCoada5VW70dAM73mcP/Va4S3fpt+96G22ytWwFg6K/gXREFTJsM5alcJWo21EpiFBoZhzN1GYYjNnFqVQ0Cs1MgcpEx12BzceJkyDa+YDHz267pk2jY31L0E6YH9u5Ew719XcT/ZvdKNan/xX4/p/2pgOMu5+nZVr1gEBSo2wmUJpzeOIBYwPm7lM0gwhEV8O4kxRNLSfNm1LCwJV1wMmrPQhUHHPLBPo/6Tk441Gw/wLLndq0X5t73y3enQ8DLpIVsga2SAROSBH5JSckToR5Jm8kFfyVvgIisFCUPkaDQrDnSXyq4LlTxRFuOA=</latexit>r(xb0,c)
<latexit sha1_base64="WmDsP/2UCczt78wshG8Bq6vRg0g=">AAACcHicdZHbbtNAEIY35tSGUwo3lbhgIUIqJ2uN0kO4quCGyyI1baU6WOPNuFll7bV2x0Bk+Sn6NNzCU/AaPAHrNJVoVEYa7a9vZqSZf9NSK0dC/O4EN27eun1nbb179979Bw97G4+OnKmsxJE02tiTFBxqVeCIFGk8KS1Cnmo8Tmcf2/rxV7ROmeKQ5iWOczgrVKYkkEdJ7208Baptk8Q0RYKtOAeapln9vUlq8f6w+QJvLpFsXia9vggjEQ13Ii7CwbYY7gkvhu074FEoFtFnyzhINjqv44mRVY4FSQ3OnUaipHENlpTU2HTjymEJcgZneOplATm6cb24q+EvPJnwzFifBfEF/Xeihty5eZ76znZHt1pr4bW1NL8eGzMjSN2VteocZihR6xVaaVLWfFs5gbK9ca2KsiIs5MUFWaU5Gd66zyfKoiQ99wKkVd4ELqdgQZL/o66399JD/n9x9C6MdsLtz4P+/oel0WvsCXvOtljEdtk++8QO2IhJds5+sJ/sV+dPsBk8DZ5dtAad5cxjdiWCV38BjiXAkQ==</latexit>ˆr✓(xa0:T,c)
<latexit sha1_base64="a/BszQZympGvut+3FMgQb8PmSCc=">AAACcHicdZHbbhMxEIad5dSGUwo3lbjAECGV08qL0kO4quCGyyI1baVuWI0db2PFXq/sWSBa7VP0abiFp+A1eAK8aSrRqIw08q9vZqSZ37zUyiNjvzvRjZu3bt9ZW+/evXf/wcPexqMjbysn5EhYbd0JBy+1KuQIFWp5UjoJhmt5zGcf2/rxV+m8ssUhzks5NnBWqFwJwICy3tt0Cli7JktxKhG2UgM45Xn9vclq9v6w+cLfXCLRvMx6fRYnLBnuJJTFg2023GNBDNt3QJOYLaJPlnGQbXRepxMrKiMLFBq8P01YieMaHCqhZdNNKy9LEDM4k6dBFmCkH9eLuxr6IpAJza0LWSBd0H8najDezw0Pne2OfrXWwmtr3FyPrZ0hcH9lrdrATAqp9QqtNCpnv62cgPneuFZFWaEsxMUFeaUpWtq6TyfKSYF6HgQIp4IJVEzBgcDwR91g76WH9P/i6F2c7MTbnwf9/Q9Lo9fIE/KcbJGE7JJ98okckBER5Jz8ID/Jr86faDN6Gj27aI06y5nH5EpEr/4CkA/Akg==</latexit>ˆr✓(xb0:T,c)<latexit sha1_base64="x2lgAGoIGGQ7doiGumQJjqlzarI=">AAACWHicdVFNaxsxEB1vP5K4X05KT72ImkKhsGiDk8a30OSQYwp1EsgaMyuPY2FptUizLWbxj+m1/UXNr6nsuNCY9MGgx3szMPNUVEYHlvJ3K3n0+MnTre2d9rPnL16+6uzuXQRXe0UD5YzzVwUGMrqkAWs2dFV5QlsYuixmJ0v/8hv5oF35lecVDS3elHqiFXKURp03+SkZRpFPkRu/GOU8JcZRpyvTTGb9w0zItHcg+0cykv7y7YkslSt0YY3z0W7rYz52qrZUsjIYwnUmKx426FkrQ4t2XgeqUM3whq4jLdFSGDar/RfifVTGYuJ8rJLFSv13okEbwtwWsdMiT8OmtxQf9Ar7sOzcjLEI99ZqLM5IkTEbam1Ye/d94wSeHA0bXVY1U6nuLpjURrATy5TFWHtSbOaRoPI6hiDUFD0qjn/RjvH+zVD8n1zsp9lhevCl1z3+vA56G97CO/gAGXyCYziDcxiAggZ+wE/41bpNINlKdu5ak9Z65jXcQ7L3B2Qntk8=</latexit> ˆr✓
<latexit sha1_base64="f3WOpK6AHrhastrElrvaIovWjO0=">AAACSXicbZDJSgNBEIZ74h53PXppDIIghBlxO4p68BjBLJAEqelUtEn39NBdo4TBl/Cqb+QT+BjexJOd5aAxPxT8fFUFVX+cKukoDD+Cwszs3PzC4lJxeWV1bX1jc6vmTGYFVoVRxjZicKhkglWSpLCRWgQdK6zHvctBv/6I1kmT3FI/xbaG+0R2pQDyqNG6QkXA7d1GKSyHQ/H/JhqbEhurcrcZHLQ6RmQaExIKnGtGYUrtHCxJofC52MocpiB6cI9NbxPQ6Nr58OBnvudJh3eN9ZUQH9LfGzlo5/o69pMa6MFN9gZwai/W07ExPYLY/Tkr19BDgUpN0EyRtOZp4gXqnrVzmaQZYSJGH3QzxcnwQay8Iy0KUn1vQFjpQ+DiASwI8uEXfbzRZJj/Te2wHJ2Uj2+OSucX46AX2Q7bZfssYqfsnF2zCqsywRR7Ya/sLXgPPoOv4Hs0WgjGO9vsjwozP4x2su0=</latexit> r
PredictedReward DifferenceGroundtruthReward Difference
Model SnapshotA greencolored rabbitDiffusionModelDiffusionModelRewardModelMSELoss
<latexit sha1_base64="XFB6TA1A6/+MvqRlI9AEb0qra5E=">AAACVnicdZHPattAEMZXavPPbVKnPuay1BRSAkJK4ka9meTSQw8p1EkgNma0HseLd7Vid5RihJ8l1/SNmpcJXTkuNCYdWPj4zQx8821WKOkojh+C8NXrtfWNza3Gm7fbO++au+8vnCmtwJ4wytirDBwqmWOPJCm8KiyCzhReZtOzun95i9ZJk/+gWYEDDTe5HEsB5NGw2eproIkAVX2b7/dpggSfhs12HKWd+Esa8zhK0vTwqFOL4xrxJIoX1WbLOh/uBgf9kRGlxpyEAueuk7igQQWWpFA4b/RLhwWIKdzgtZc5aHSDauF+zj96MuJjY/3LiS/ovxsVaOdmOvOTtVe32qvhi71Mv4yNmRJk7pmtSsMUBSq1QktF0pqfKyfQOB1UMi9Kwlw8XTAuFSfD64z5SFoUpGZegLDSh8DFBCwI8j/R8PH+zZD/X1wcRsnnqPP9uN09XQa9yfbYB7bPEnbCuuwrO2c9JtiM3bF79iv4HTyGa+HG02gYLHda7FmFzT9sXbZH</latexit>L(✓)Figure 2. PRDP framework. PRDP mitigates the instability of policy gradient methods by converting the RLHF objective to an equivalent
supervised regression objective. Specifically, given a text prompt, PRDP samples two images, and tasks the diffusion model with predicting
the reward difference of these two images from their denoising trajectories. The diffusion model is updated by stochastic gradient descent
on the MSE loss that measures the prediction error. We prove that the MSE loss and the RLHF objective have the same optimal solution.
Here, we seek to finetune the diffusion model πθby maxi-
mizing a given reward function r(x0,c)with a KL regular-
ization, whose strength is controlled by a hyperparameter
β. The reward function can be a pretrained reward model
(e.g., HPSv2 [53], PickScore [22]) that measures the gen-
eration quality, and the KL regularization discourages πθ
from deviating too far from the pretrained diffusion model
πref(e.g., Stable Diffusion [37]). This helps πθto pre-
serve the overall generation capability of πref, and keeps
the generated images x0close to the distribution where the
reward model is accurate. The expectation is taken over text
prompts c∼p(c)and images x0∼πθ(x0|c), where p(c)
is a predefined prompt distribution, usually a uniform dis-
tribution over a set of training prompts.
In contrast to language models, the KL regularization
in Eq. (5) cannot be computed analytically, due to the in-
tractable integral defined in Eq. (1). Hence, we instead max-
imize a lower bound of the objective in Eq. (5):
max
πθEx0,c[r(x0,c)−βKL[πθ(¯x|c)||πref(¯x|c)]],(6)
where ¯x:=x0:Tis the full denoising trajectory. We provide
the proof of lower bound in Appendix A.1.
While it is possible to apply REINFORCE [51] or more
advanced policy gradient methods [4, 10, 42] to optimize
Eq. (6), we empirically find they are hard to scale to large
numbers of prompts due to training instability. Inspired by
DPO [35], we propose to reformulate Eq. (6) into a super-
vised learning objective, allowing stable training on more
than100K prompts.
First, we derive the optimal solution to Eq. (6) as:
πθ⋆(¯x|c) =1
Z(c)πref(¯x|c) exp1
βr(x0,c)
,(7)
where Z(c) =R
πref(¯x|c)exp( r(x0,c)/β)d¯xis the parti-
tion function. Proof can be found in Appendix A.2. SinceZ(c)is intractable, Eq. (7) cannot be directly used to com-
puteπθ⋆. However, it reveals that πθ⋆must satisfy
logπθ⋆(¯x|c)
πref(¯x|c)=1
βr(x0,c)−logZ(c) (8)
for all ¯xandc. This allows us to cancel the logZ(c)term
by considering two denoising trajectories ¯xaand¯xbthat
correspond to the same text prompt c:
logπθ⋆(¯xa|c)
πref(¯xa|c)−logπθ⋆(¯xb|c)
πref(¯xb|c)=r(xa
0,c)−r(xb
0,c)
β.
(9)
Define
ˆrθ(¯x,c):= logπθ(¯x|c)
πref(¯x|c), (10)
∆ˆrθ(¯xa,¯xb,c):= ˆrθ(¯xa,c)−ˆrθ(¯xb,c), (11)
∆r(xa
0,xb
0,c):=r(xa
0,c)−r(xb
0,c), (12)
then Eq. (9) becomes
∆ˆrθ⋆(¯xa,¯xb,c) = ∆ r(xa
0,xb
0,c)/β. (13)
This motivates us to optimize πθby minimizing the follow-
ing mean squared error (MSE) loss:
L(θ) =E¯xa,¯xb,c[lθ(¯xa,¯xb,c)] (14)
:=E¯xa,¯xb,c∆ˆrθ(¯xa,¯xb,c)−∆r(xa
0,xb
0,c)/β2.
We call L(θ)the Reward Difference Prediction (RDP) ob-
jective, since we learn πθby predicting the reward differ-
ence ∆r(xa
0,xb
0,c)instead of directly maximizing the re-
ward. An illustration is provided in Fig. 2. We further show
in Appendix A.3 that
πθ=πθ⋆⇐⇒ L (θ) = 0 . (15)
7425
Training w/o Proximal Updates 
 Training w/ Proximal Updates 
Figure 3. Effect of proximal updates. We show generation samples during the PRDP training process. Here, we use the small-scale setup
described in Sec. 4.1 and HPSv2 as the reward model. All samples use the same prompt “A painting of a deer” and the same random seed.
(Left) Without proximal updates, training is quite unstable, and the generation quickly becomes meaningless noise. (Right) With proximal
updates, the training stability is remarkably improved.
Algorithm 1 PRDP Training
Require: pretrained diffusion model πref, training prompt distri-
bution p(c), reward model r(x0,c), training epochs E, gradi-
ent updates Kper epoch, prompt batch size N, image batch
sizeBper prompt
1:πθ←πref ▷Initialization
2:forepoch e= 1, . . . , E do
3: πθold←πθ ▷Model snapshot
4:{cn}N
n=1iid∼p(c) ▷Sample text prompts
5: for each text prompt cndo
6: {¯xn,i}B
i=1iid∼πθold(¯x|cn)▷Denoising trajectories
7: end for
8: Obtain rewards r(xn,i
0,cn)for all n, i
9: forgradient step k= 1, . . . , K do
10: L(θ)←1
N(B
2)PN
n=1P
1≤i<j≤Blθ(¯xn,i,¯xn,j,cn)
11: Update model parameters θby gradient descent
12: end for
13:end for
3.2. Online Optimization
To estimate the expectation in L(θ), we need samples of de-
noising trajectories ¯xaand¯xbthat correspond to the same
prompt c. A straightforward approach, as similarly done in
DPO, is to sample ¯xa,¯xbiid∼πref(¯x|c). This can be im-
plemented as uniform sampling from a fixed offline dataset
generated by the pretrained model πref.
However, the offline dataset lacks sufficient coverage of
samples from πθ(¯x|c)that keeps updating, leading to sub-
optimal generation quality. Therefore, we propose an online
optimization procedure, inspired by online RL algorithms.
Specifically, we sample ¯xa,¯xbiid∼πθold(¯x|c), where θoldis
a snapshot of the diffusion model parameters θ, and we set
θold←θevery Kgradient updates. In practice, we use
πθoldto generate a batch of denoising trajectories, and then
use all pairs of denoising trajectories in the batch to com-
pute the loss L(θ). Details are provided in Algorithm 1. We
will show in Sec. 4.3 that online optimization significantly
improves generation quality.3.3. Proximal Updates for Stable Training
We find in our experiments that directly optimizing Eq. (14)
is prone to training instability, as illustrated in Fig. 3 (Left).
This is likely due to excessively large model updates during
training. To resolve this issue, we propose proximal updates
that remove the incentive for moving πθtoo far away from
πθold. Inspired by PPO [42], we achieve this by clipping the
log probability ratio log(πθ(¯x|c)/πθold(¯x|c))to be within
a small interval [−ϵ′, ϵ′]. This can be implemented by clip-
ping the ˆrθ(¯x,c)asˆrclip
θ(¯x,c):=
clip (ˆrθ(¯x,c),ˆrθold(¯x,c)−ϵ′,ˆrθold(¯x,c) +ϵ′),(16)
because log(πθ(¯x|c)/πθold(¯x|c)) = ˆ rθ(¯x,c)−ˆrθold(¯x,c).
We then use ˆrclip
θ(¯x,c)to compute the clipped MSE loss
lclip
θ(¯xa,¯xb,c):=
∆ˆrclip
θ(¯xa,¯xb,c)−∆r(xa
0,xb
0,c)/β2
, (17)
where ∆ˆrclip
θ(¯xa,¯xb,c):= ˆrclip
θ(¯xa,c)−ˆrclip
θ(¯xb,c). Sim-
ilar to PPO [42], our final loss is the maximum of the
clipped and unclipped MSE loss:
lθ(¯xa,¯xb,c)←max( lθ(¯xa,¯xb,c), lclip
θ(¯xa,¯xb,c)).(18)
This ensures that we minimize an upper bound of the origi-
nal loss, making the optimization problem well-defined.
In practice, the clipping in Eq. (16) is decomposed and
applied at each denoising step t. First, ˆrθ(¯x,c)can be de-
composed as ˆrθ(¯x,c) =PT
t=1ˆrθ,t(¯x,c), where
ˆrθ,t(¯x,c):= log( πθ(xt−1|xt,c)/πref(xt−1|xt,c)).(19)
We apply clipping to each ˆrθ,t(¯x,c)asˆrclip
θ,t(¯x,c):=
clip (ˆrθ,t(¯x,c),ˆrθold,t(¯x,c)−ϵ,ˆrθold,t(¯x,c) +ϵ),(20)
where ϵis the stepwise clipping range. Finally, we replace
Eq. (16) with
ˆrclip
θ(¯x,c):=TX
t=1ˆrclip
θ,t(¯x,c). (21)
As shown in Fig. 3 (Right), our proposed proximal updates
can remarkably improve optimization stability.
7426
snak e Reward Model: HPSv2 
whale horse duck monk e y goat Reward Model: PickScore Stable 
Diffusion DDPO PRDP Figure 4. Generation samples from small-scale training. DDPO and PRDP are finetuned from Stable Diffusion v1.4 on 45prompts
consisting of common animal names, with HPSv2 (Left) and PickScore (Right) as the reward model. Samples within each column use the
same random seed. The prompt template is “A painting of a ⟨animal⟩”, where the ⟨animal⟩is listed on top of each column. All prompts
are seen during training. Both DDPO and PRDP significantly improve the generation quality, with PRDP being slightly better.
4. Experiments
In our experiments, we first verify on a set of 45prompts
that PRDP can match the reward maximization ability of
DDPO [4], which is based on the well-established PPO [42]
algorithm. We then conduct a large-scale training on more
than100K prompts from the training set of HPDv2 [53],
showing that PRDP can successfully handle large-scale
training whereas DDPO fails. We further perform a large-
scale multi-reward finetuning on the training set prompts of
Pick-a-Pic v1 dataset [22], highlighting the superior genera-
tion quality of PRDP on complex, unseen prompts. Finally,
we showcase the advantages of our algorithm design, such
as online optimization and KL regularization.
4.1. Experimental Setup
To perform reward finetuning, we need a pretrained diffu-
sion model, a pretrained reward model, and a training set of
prompts. For all experiments, we use Stable Diffusion (SD)
v1.4 [37] as the pretrained diffusion model, and finetune the
full UNet weights. For sampling, during both training and
evaluation, we use the DDPM sampler [15] with 50denois-
ing steps and a classifier-free guidance [14] scale of 5.0.
Small-scale setup. We use a set of 45prompts, with the
template “A painting of a ⟨animal ⟩”, where the ⟨animal ⟩is
taken from the list of common animal names used in DDPO.Table 1. Reward score comparison on small-scale training.
SD v1.4 DDPO PRDP
HPSv2 0.2855 0 .3398 0.3471
PickScore 0.2179 0 .2664 0.2700
We conduct reward finetuning separately for two recently
proposed reward models, HPSv2 [53] and PickScore [22].
We train for 100epochs, where in each epoch, we sample
32prompts and 16images per prompt. The evaluation uses
the same set of prompts as training. We report reward scores
averaged over 256random samples per prompt.
Large-scale setup. Following DRaFT [6], we use more
than100K prompts from the training set of HPDv2, and
finetune for HPSv2 and PickScore separately. We train for
1000 epochs. In each epoch, we sample 64prompts and 8
images per prompt. We evaluate the finetuned model on 500
randomly sampled training prompts, as well as a variety of
unseen prompts, including 500prompts from the Pick-a-Pic
v1 test set, and 800prompts from each of the four bench-
mark categories of HPDv2, namely animation, concept art,
painting, and photo. We report reward scores averaged over
64random samples per prompt.
Large-scale multi-reward setup. We mostly follow the
7427
cinematic still of 
highly reﬂective 
stainless steel 
train in the desert, 
at sunset Reward Model: HPSv2 
The image is a 
wooden sculpture of 
a cute robot with cat 
ears, displayed in a 
contemporary art 
gallery. A chibi frog 
character surﬁng 
at the beach. An anthropomorphic 
frog wizard wearing a 
cape and holding a 
wand. Digital art of a cherry 
tree overlooking a 
valley with a waterfall 
at sunset. A monkey in a blue 
top hat painted in 
oil by Vincent van 
Gogh in the 1800s. Reward Model: PickScore Stable 
Diffusion DDPO PRDP Figure 5. Generation samples from large-scale training. DDPO and PRDP are finetuned from Stable Diffusion v1.4 on over 100K
prompts from the training set of HPDv2, with HPSv2 (Left) and PickScore (Right) as the reward model. Samples within each column
are generated from the prompt shown on top, using the same random seed. All prompts are unseen during training. PRDP significantly
improves the generation quality over Stable Diffusion, whereas DDPO fails to generate reasonable results.
large-scale setup, except that we use the training set prompts
of Pick-a-Pic v1 dataset, and a weighted combination of re-
wards: PickScore = 10 , HPSv2 = 2, Aesthetic = 0.05,
where Aesthetic is the LAION aesthetic score.
Baselines. DDPO [4] and DPOK [10] are the two most
recent RL finetuning methods for black-box rewards. Since
DDPO has demonstrated better performance than DPOK,
we mainly compare to DDPO. To ensure a fair comparison,
we train DDPO and PRDP for the same number of epochs,
with the same number of reward queries per epoch. We also
use the same random seeds to sample images for evaluation.
4.2. Main Results
Small-scale finetuning. We show generation samples from
small-scale finetuning in Fig. 4 and reward scores in Tab. 1.
Both DDPO and PRDP can significantly improve the gen-
eration quality over Stable Diffusion, with more vivid col-
ors and details. Quantitatively, PRDP achieves slightly bet-
ter reward scores than DDPO. This verifies that PRDP can
match the reward maximization ability of well-established
policy gradient methods.
Large-scale finetuning. We present generation samplesfrom large-scale finetuning in Fig. 5 and reward scores in
Tab. 2. We observe that Stable Diffusion generates images
with relevant content but low quality. Meanwhile, DDPO
fails to give reasonable results. It generates irrelevant, low
quality images or even meaningless noise, leading to lower
reward scores than Stable Diffusion. This is due to the in-
stability of DDPO in large-scale training, which we further
investigate in Appendix B. In contrast, PRDP maintains sta-
bility in the large-scale setup, and significantly improves the
generation quality on both seen and unseen prompts.
Large-scale multi-reward finetuning. We provide gen-
eration samples in Figs. 1 and 11 to 15, and reward scores
in Tab. 3, showing the superior generation quality of PRDP
on a diverse set of complex, unseen prompts.
4.3. Effect of Online Optimization
In this section, we show that online optimization has a great
advantage over offline optimization. To ensure a fair com-
parison, we use the same number of reward queries and gra-
dient updates for both methods. Specifically, following the
small-scale setup, for online training, we use 100epochs,
where each epoch makes 512queries to the reward model.
7428
Table 2. Reward score comparison on large-scale training.
Reward
ModelMethodSeen Prompts Unseen Prompts
HPD v2
Training SetPick-a-Pic v1
Test SetHPD v2
AnimationHPD v2
Concept ArtHPD v2
PaintingHPD v2
Photo
HPSv2SD v1.4 0.2685 0 .2665 0 .2737 0 .2656 0 .2654 0 .2750
DDPO 0.2464 0 .2501 0 .2673 0 .2558 0 .2570 0 .2093
PRDP 0.3175 0 .3050 0 .3223 0 .3175 0 .3172 0 .3159
PickScoreSD v1.4 0.2092 0 .2082 0 .2111 0 .2062 0 .2059 0 .2172
DDPO 0.2032 0 .1992 0 .2077 0 .2125 0 .2124 0 .1780
PRDP 0.2424 0 .2344 0 .2450 0 .2441 0 .2448 0 .2387
Online Optimization for PickScore 
 Online Optimization for HPSv2 
Figure 6. Effect of online optimization. We show generation samples during the PRDP training process, with HPSv2 (Left) and PickScore
(Right) as the reward model. We follow the small-scale training setup. The prompts for the first and the second rows are “A painting of
a squirrel” and “A painting of a bird”, respectively. Samples within each row use the same random seed. It can be observed that online
optimization continually improves the generation quality.
For offline training, we sample 51200 images from the pre-
trained Stable Diffusion, obtain their rewards, and then per-
form the same total number of gradient updates as in online
training. We show generation samples during the online op-
timization process in Fig. 6, and quantitative comparisons in
Fig. 7. We observe that online optimization continually im-
proves the generation quality, achieving significantly better
reward scores than offline optimization.
4.4. Effect of KL Regularization
A common limitation of reward finetuning is reward hack-
ing, where the finetuned diffusion model exploits inaccura-
cies in the reward model, and produces undesired images
with high reward scores. In this section, we show that the
KL regularization in our PRDP formulation can help allevi-
ate this issue. For this purpose, we use the LAION aesthetic
predictor as the reward model. It only takes images as input,
and can be exploited by disregarding text-image alignment.
We follow the small-scale setup, except that we train for
250epochs and directly use the 45common animal names
as prompts. As demonstrated in Fig. 8, DDPO, without KL
regularization, is prone to reward hacking. It completely ig-
/uni00000014 /uni00000016/uni00000014 /uni00000018/uni00000014 /uni0000001a/uni00000014 /uni0000001c/uni00000014 /uni00000015/uni00000014/uni00000014
/uni00000033/uni00000052/uni00000050/uni0000004d/uni00000052/uni00000049/uni00000004/uni00000038/uni00000056/uni00000045/uni0000004d/uni00000052/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000029/uni00000054/uni00000053/uni00000047/uni0000004c/uni00000057/uni00000014/uni00000012/uni00000016/uni0000001c/uni00000014/uni00000012/uni00000017/uni00000014/uni00000014/uni00000012/uni00000017/uni00000016/uni00000014/uni00000012/uni00000017/uni00000018/uni00000014/uni00000012/uni00000017/uni0000001a
/uni0000002c/uni00000034/uni00000037/uni0000005a/uni00000016
/uni00000033/uni0000004a/uni0000004a/uni00000050/uni0000004d/uni00000052/uni00000049
/uni00000033/uni00000052/uni00000050/uni0000004d/uni00000052/uni00000049
/uni00000014 /uni00000016/uni00000014 /uni00000018/uni00000014 /uni0000001a/uni00000014 /uni0000001c/uni00000014 /uni00000015/uni00000014/uni00000014
/uni00000033/uni00000052/uni00000050/uni0000004d/uni00000052/uni00000049/uni00000004/uni00000038/uni00000056/uni00000045/uni0000004d/uni00000052/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000029/uni00000054/uni00000053/uni00000047/uni0000004c/uni00000057/uni00000014/uni00000012/uni00000016/uni00000014/uni00000014/uni00000012/uni00000016/uni00000016/uni00000014/uni00000012/uni00000016/uni00000018/uni00000014/uni00000012/uni00000016/uni0000001a/uni00000014/uni00000012/uni00000016/uni0000001c
/uni00000034/uni0000004d/uni00000047/uni0000004f/uni00000037/uni00000047/uni00000053/uni00000056/uni00000049Figure 7. Comparison of online and offline optimization. We
evaluate the reward scores of model checkpoints during online op-
timization and the final model obtained by offline optimization.
We follow the small-scale training setup, and optimize the models
for HPSv2 and PickScore separately. Online optimization matches
the performance of offline optimization in ∼10epochs, and keeps
improving the reward score afterwards.
nores the text prompts and generates similar images for all
prompts. In contrast, PRDP with β= 10 can successfully
preserve the text-image alignment while improving the aes-
thetic quality. More analysis can be found in Appendix C.
7429
DDPO PRDP cat butterﬂy fr og Figure 8. Effect of KL regularization. We show generation sam-
ples from DDPO and PRDP when optimizing the LAION aesthetic
score. We use the small-scale training setup, except that we train
for250epochs. Samples within each column are generated from
the prompt shown on top, using the same random seed. DDPO,
without KL regularization, over-optimizes the reward, generating
similar images for all prompts. In contrast, PRDP, formulated with
KL regularization, successfully preserves text-image alignment.
5. Related Work
Diffusion models. As a new class of generative models,
diffusion models [15, 44, 46] have achieved remarkable suc-
cess in a wide variety of data modalities, including images
[7, 17, 30, 36, 37, 39–41], videos [16, 43], audios [25], 3D
shapes [13, 32, 57, 60], and robotic trajectories [1, 5, 18]. To
facilitate control over the content and style of generation, re-
cent works have investigated finetuning diffusion models on
various conditioning signals [11, 19, 20, 27, 28, 38, 45, 58].
However, it remains challenging to adapt diffusion models
to downstream use cases that are misaligned with the train-
ing objective, such as generating novel compositions of ob-
jects unseen during training, and producing images that are
aesthetically preferred by humans. Although classifier guid-
ance [7] can help mitigate this issue, the classifier requires
noisy images as input, making it hard to use off-the-shelf
classifiers such as object detectors and aesthetic predictors
for guidance. In contrast, we finetune the diffusion model to
maximize rewards that reflect downstream objectives. Our
method can work with generic off-the-shelf reward models
that take clean images as input.
Language model learning from human feedback. The
maximum likelihood training objective for language models
tends to yield undesirable model behavior, due to the poten-
tially biased, toxic, or harmful content in the training data.
Reinforcement learning from human feedback (RLHF) has
recently emerged as a successful remedy [2, 3, 12, 26, 29,
31, 47, 52, 61]. Typically, a reward model is first trained
from human preference data ( e.g., rankings of outputs froma pretrained language model). Then, the language model is
finetuned by online RL algorithms ( e.g., PPO [42]) to max-
imize the score given by the reward model. More recently,
DPO [35] proposes a supervised learning method that di-
rectly optimizes the language model from preference data,
skipping the reward model training and avoiding the insta-
bility of RL algorithms. Our method is inspired by DPO
and PPO, but designed specifically for diffusion models.
Reward finetuning for diffusion models. Inspired by
the success of RLHF in the language domain, researchers
have developed several reward models in the vision domain
[21–24, 34, 53–56]. Moreover, recent works have explored
using these reward models to improve the generation quality
of diffusion models. A simple approach, called supervised
finetuning [23, 54], is to finetune the diffusion model to-
ward high-reward samples from an offline dataset. Its major
drawback is that the generation quality is limited by the of-
fline dataset. For further improvement, RAFT [8] proposes
an online variant that iteratively re-generates the dataset. A
more direct method for online optimization is to backprop-
agate the reward function gradient through the denoising
process [6, 33, 49, 55]. However, this only works for dif-
ferentiable rewards. For generic rewards, DDPO [4] and
DPOK [10] propose RL finetuning. While they have shown
promising results on small prompt sets, they are unstable
in large-scale training. Our work addresses the training in-
stability issue, achieving stable reward finetuning on large-
scale prompt datasets for generic rewards. Concurrent with
our work, Diffusion-DPO [50] adapts DPO to efficiently
align diffusion models from large-scale offline preference
data, and [59] proposes to stabilize large-scale RL finetun-
ing by combining the diffusion model pretraining loss.
6. Conclusion
This paper presents PRDP, the first black-box reward fine-
tuning method for diffusion models that is stable on large-
scale prompt datasets with over 100K prompts. We achieve
this by converting the RLHF objective to an equivalent su-
pervised regression objective and developing its stable opti-
mization algorithm. Our large-scale experiments highlight
the superior generation quality of PRDP on complex, un-
seen prompts, which is beyond the capability of existing RL
finetuning methods. We also demonstrate that the KL reg-
ularization in the PRDP formulation can help alleviate the
common issue of reward hacking. We hope that our work
can inspire future research on large-scale reward finetuning
for diffusion models.
Acknowledgments
We thank authors of DRaFT [6] for sharing their training
prompts and reward models. We appreciate helpful discus-
sion with Ligong Han, Yanwu Xu, Yaxuan Zhu, Zhonghao
Wang, Yunzhi Zhang, Yang Zhao, and Zhisheng Xiao.
7430
References
[1] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B. Tenenbaum,
Tommi S. Jaakkola, and Pulkit Agrawal. Is conditional gen-
erative modeling all you need for decision making? In In-
ternational Conference on Learning Representations , 2023.
8
[2] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell,
Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort,
Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Ka-
davath, Jackson Kernion, Tom Conerly, Sheer El-Showk,
Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tris-
tan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt,
Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown,
Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and
Jared Kaplan. Training a helpful and harmless assistant with
reinforcement learning from human feedback. arXiv preprint
arXiv:2204.05862 , 2022. 2, 8
[3] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda
Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna
Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol
Chen, Catherine Olsson, Christopher Olah, Danny Her-
nandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-
Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey
Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite,
Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas
Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby,
Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec,
Sheer El Showk, Stanislav Fort, Tamera Lanham, Timo-
thy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan
Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann,
Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom
Brown, and Jared Kaplan. Constitutional AI: Harmlessness
from AI feedback. arXiv preprint arXiv:2212.08073 , 2022.
8
[4] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and
Sergey Levine. Training diffusion models with reinforce-
ment learning. In International Conference on Learning
Representations , 2024. 2, 3, 5, 6, 8, 15, 16, 18
[5] Chang Chen, Fei Deng, Kenji Kawaguchi, Caglar Gulcehre,
and Sungjin Ahn. Simple hierarchical planning with diffu-
sion. In International Conference on Learning Representa-
tions , 2024. 8
[6] Kevin Clark, Paul Vicol, Kevin Swersky, and David J. Fleet.
Directly fine-tuning diffusion models on differentiable re-
wards. In International Conference on Learning Represen-
tations , 2024. 5, 8, 17
[7] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat GANs on image synthesis. In Advances in Neural In-
formation Processing Systems , 2021. 2, 8
[8] Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang,
Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, KaShun
SHUM, and Tong Zhang. RAFT: Reward ranked finetuning
for generative foundation model alignment. Transactions on
Machine Learning Research , 2023. 8
[9] Ying Fan and Kangwook Lee. Optimizing DDPM sampling
with shortcut fine-tuning. In International Conference on
Machine Learning , 2023. 2[10] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu,
Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Moham-
mad Ghavamzadeh, Kangwook Lee, and Kimin Lee. DPOK:
Reinforcement learning for fine-tuning text-to-image diffu-
sion models. In Advances in Neural Information Processing
Systems , 2023. 2, 3, 6, 8
[11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,
Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An
image is worth one word: Personalizing text-to-image gen-
eration using textual inversion. In International Conference
on Learning Representations , 2023. 8
[12] Amelia Glaese, Nat McAleese, Maja Tr˛ ebacz, John
Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,
Laura Weidinger, Martin Chadwick, Phoebe Thacker,
Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen
Huang, Ramona Comanescu, Fan Yang, Abigail See,
Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz,
Jaume Sanchez Elias, Richard Green, So ˇna Mokrá, Nicholas
Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason
Gabriel, William Isaac, John Mellor, Demis Hassabis, Ko-
ray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irv-
ing. Improving alignment of dialogue agents via targeted
human judgements. arXiv preprint arXiv:2209.14375 , 2022.
8
[13] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M. Susskind,
Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi.
NerfDiff: Single-image view synthesis with NeRF-guided
distillation from 3D-aware diffusion. In International Con-
ference on Machine Learning , 2023. 8
[14] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. In NeurIPS 2021 Workshop on Deep Generative
Models and Downstream Applications , 2021. 5
[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. In Advances in Neural Informa-
tion Processing Systems , 2020. 2, 5, 8
[16] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben
Poole, Mohammad Norouzi, David J. Fleet, and Tim Sali-
mans. Imagen video: High definition video generation with
diffusion models. arXiv preprint arXiv:2210.02303 , 2022. 8
[17] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet,
Mohammad Norouzi, and Tim Salimans. Cascaded diffu-
sion models for high fidelity image generation. Journal of
Machine Learning Research , 23(47):1–33, 2022. 8
[18] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey
Levine. Planning with diffusion for flexible behavior synthe-
sis. In International Conference on Machine Learning , 2022.
8
[19] Jindong Jiang, Fei Deng, Gautam Singh, and Sungjin Ahn.
Object-centric slot diffusion. In Advances in Neural Infor-
mation Processing Systems , 2023. 8
[20] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen
Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:
Text-based real image editing with diffusion models. In
CVPR , 2023. 8
[21] Junjie Ke, Keren Ye, Jiahui Yu, Yonghui Wu, Peyman Milan-
far, and Feng Yang. VILA: Learning image aesthetics from
7431
user comments with vision-language pretraining. In CVPR ,
2023. 8
[22] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Ma-
tiana, Joe Penna, and Omer Levy. Pick-a-Pic: An open
dataset of user preferences for text-to-image generation. In
Advances in Neural Information Processing Systems , 2023.
2, 3, 5, 16, 17
[23] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins,
Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad
Ghavamzadeh, and Shixiang Shane Gu. Aligning text-
to-image models using human feedback. arXiv preprint
arXiv:2302.12192 , 2023. 2, 8
[24] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
BLIP: Bootstrapping language-image pre-training for unified
vision-language understanding and generation. In Interna-
tional Conference on Machine Learning , 2022. 8
[25] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu,
Danilo Mandic, Wenwu Wang, and Mark D Plumbley. Audi-
oLDM: Text-to-audio generation with latent diffusion mod-
els. In International Conference on Machine Learning , 2023.
8
[26] Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. Chain of
hindsight aligns language models with feedback. In Interna-
tional Conference on Learning Representations , 2024. 8
[27] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and
Daniel Cohen-Or. Null-text inversion for editing real images
using guided diffusion models. In CVPR , 2023. 8
[28] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-
gang Qi, Ying Shan, and Xiaohu Qie. T2I-adapter: Learning
adapters to dig out more controllable ability for text-to-image
diffusion models. arXiv preprint arXiv:2302.08453 , 2023. 8
[29] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
Long Ouyang, Christina Kim, Christopher Hesse, Shan-
tanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang,
Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin But-
ton, Matthew Knight, Benjamin Chess, and John Schulman.
WebGPT: Browser-assisted question-answering with human
feedback. arXiv preprint arXiv:2112.09332 , 2021. 8
[30] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,
Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya
Sutskever, and Mark Chen. GLIDE: Towards photorealis-
tic image generation and editing with text-guided diffusion
models. In International Conference on Machine Learning ,
2022. 2, 8
[31] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob
Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
Askell, Peter Welinder, Paul F Christiano, Jan Leike, and
Ryan Lowe. Training language models to follow instructions
with human feedback. In Advances in Neural Information
Processing Systems , 2022. 2, 8
[32] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-
hall. DreamFusion: Text-to-3D using 2D diffusion. In In-
ternational Conference on Learning Representations , 2023.
8
[33] Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and
Katerina Fragkiadaki. Aligning text-to-image diffusionmodels with reward backpropagation. arXiv preprint
arXiv:2310.03739 , 2023. 8
[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In International
Conference on Machine Learning , 2021. 8
[35] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-
pher D Manning, Stefano Ermon, and Chelsea Finn. Direct
preference optimization: Your language model is secretly a
reward model. In Advances in Neural Information Process-
ing Systems , 2023. 2, 3, 8, 13
[36] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with CLIP latents. arXiv preprint arXiv:2204.06125 ,
2022. 2, 8
[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , 2022. 2, 3, 5,
8, 16, 17
[38] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In CVPR , 2023. 8
[39] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,
Jonathan Ho, Tim Salimans, David Fleet, and Mohammad
Norouzi. Palette: Image-to-image diffusion models. In ACM
SIGGRAPH 2022 Conference Proceedings , 2022. 8
[40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Sali-
mans, Jonathan Ho, David J Fleet, and Mohammad Norouzi.
Photorealistic text-to-image diffusion models with deep lan-
guage understanding. In Advances in Neural Information
Processing Systems , 2022. 2
[41] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-
mans, David J. Fleet, and Mohammad Norouzi. Image
super-resolution via iterative refinement. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 45(4):4713–
4726, 2023. 8
[42] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-
ford, and Oleg Klimov. Proximal policy optimization algo-
rithms. arXiv preprint arXiv:1707.06347 , 2017. 2, 3, 4, 5, 8,
18
[43] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman.
Make-A-Video: Text-to-video generation without text-video
data. In International Conference on Learning Representa-
tions , 2023. 8
[44] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International Confer-
ence on Machine Learning , 2015. 2, 8
[45] Kihyuk Sohn, Lu Jiang, Jarred Barber, Kimin Lee, Nataniel
Ruiz, Dilip Krishnan, Huiwen Chang, Yuanzhen Li, Irfan
7432
Essa, Michael Rubinstein, Yuan Hao, Glenn Entis, Irina
Blok, and Daniel Castro Chin. StyleDrop: Text-to-image
synthesis of any style. In Advances in Neural Information
Processing Systems , 2023. 8
[46] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. In International Conference on Learning Represen-
tations , 2021. 2, 8
[47] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler,
Ryan Lowe, Chelsea V oss, Alec Radford, Dario Amodei, and
Paul F Christiano. Learning to summarize with human feed-
back. In Advances in Neural Information Processing Sys-
tems, 2020. 2, 8
[48] Richard S Sutton and Andrew G Barto. Reinforcement learn-
ing: An introduction . MIT press, 2018. 2
[49] Bram Wallace, Akash Gokul, Stefano Ermon, and Nikhil
Naik. End-to-end diffusion latent optimization improves
classifier guidance. In ICCV , 2023. 8
[50] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou,
Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming
Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model align-
ment using direct preference optimization. In CVPR , 2024.
8
[51] Ronald J Williams. Simple statistical gradient-following al-
gorithms for connectionist reinforcement learning. Machine
learning , 8:229–256, 1992. 2, 3
[52] Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon,
Ryan Lowe, Jan Leike, and Paul Christiano. Recursively
summarizing books with human feedback. arXiv preprint
arXiv:2109.10862 , 2021. 8
[53] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng
Zhu, Rui Zhao, and Hongsheng Li. Human preference score
v2: A solid benchmark for evaluating human preferences of
text-to-image synthesis. arXiv preprint arXiv:2306.09341 ,
2023. 2, 3, 5, 8, 15, 16, 17
[54] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hong-
sheng Li. Human preference score: Better aligning text-to-
image models with human preference. In ICCV , 2023. 8
[55] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai
Li, Ming Ding, Jie Tang, and Yuxiao Dong. ImageRe-
ward: Learning and evaluating human preferences for text-
to-image generation. In Advances in Neural Information
Processing Systems , 2023. 2, 8
[56] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-
jtaba Seyedhosseini, and Yonghui Wu. CoCa: Contrastive
captioners are image-text foundation models. Transactions
on Machine Learning Research , 2022. 8
[57] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic,
Or Litany, Sanja Fidler, and Karsten Kreis. LION: Latent
point diffusion models for 3D shape generation. In Advances
in Neural Information Processing Systems , 2022. 8
[58] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
ICCV , 2023. 8
[59] Yinan Zhang, Eric Tzeng, Yilun Du, and Dmitry Kislyuk.
Large-scale reinforcement learning for diffusion models.
arXiv preprint arXiv:2401.12244 , 2024. 8[60] Linqi Zhou, Yilun Du, and Jiajun Wu. 3D shape genera-
tion and completion through point-voxel diffusion. In ICCV ,
2021. 8
[61] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B
Brown, Alec Radford, Dario Amodei, Paul Christiano, and
Geoffrey Irving. Fine-tuning language models from human
preferences. arXiv preprint arXiv:1909.08593 , 2019. 2, 8
7433
