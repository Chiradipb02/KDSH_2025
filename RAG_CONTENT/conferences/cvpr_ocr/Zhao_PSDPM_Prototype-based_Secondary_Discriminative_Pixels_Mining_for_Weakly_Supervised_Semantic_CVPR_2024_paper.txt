PSDPM: Prototype-based Secondary Discriminative Pixels Mining for Weakly
Supervised Semantic Segmentation
Xinqiao Zhao1,2,∗Ziqian Yang1,2,∗Tianhong Dai3Bingfeng Zhang4Jimin Xiao1,†
1XJTLU2University of Liverpool3University of Aberdeen4China University of Petroleum
Abstract
Image-level WeaklySupervised Semantic Segmentation
(WSSS) has received increasing attention due to its low an-
notation cost. ClassActivation Mapping (CAM) generated
through classifier weights in WSSS inevitably ignores cer-
tain useful cues, while the CAM generated through class
prototypes can alleviate that. However, because of the dif-
ferent goals of image classification and semantic segmenta-
tion, the class prototypes still focus on activating primary
discriminative pixels learned from classification loss, lead-
ing to incomplete CAM. In this paper, we propose a plug-
and-play Prototype-based Secondary Discriminative Pixels
Mining (PSDPM) framework for enabling class prototypes
to activate more secondary discriminative pixels, thus gen-
erating a more complete CAM. Specifically, we introduce
aForeground PixelEstimation Module (FPEM) for esti-
mating potential foreground pixels based on the correla-
tions between primary and secondary discriminative pix-
els and the semantic segmentation results of baseline meth-
ods. Then, we enable WSSS model to learn discrimina-
tive features from secondary discriminative pixels through
a consistency loss calculated between FPEM result and
class-prototype CAM. Experimental results show that our
PSDPM improves various baseline methods significantly
and achieves new state-of-the-art performances on WSSS
benchmarks. Codes are available at https://github.
com/xinqiaozhao/PSDPM .
1. Introduction
Deep learning technology shows impressive performances
on image [3, 18, 57] and video segmentation [56, 60] tasks.
Among them, image-level weakly supervised semantic seg-
mentation has received increasing attention due to its low
annotation cost. Existing methods mainly rely on Class
Activation Mapping (CAM) and knowledge from CLIP [16]
to obtain pseudo labels for training semantic segmentation
models [1]. Considering the fact that CAM based on clas-
∗:Equal contributions †:Corresponding authorsifier weights only focuses on limited regions as the classi-
fier weights deviate from the center of foreground features
and can only activate foreground pixels around it [5], many
works intend to acquire a more complete CAM by leverag-
ing class prototypes. Chen et al. [7] use the cluster centers
of each class pixel-wise features as the class prototypes to
replace the classifier weights to generate CAMs. Similarly,
Chen et al. [5] propose an image-specific prototype CAM
generation method, which fuses class discriminative fea-
tures learned by classifier weight with other image-specific
information, mitigating the activation center shifting issues
and completing the CAM.
However, the aforementioned prototype-based methods
still remain shortcomings. Since the pixel-wise feature dis-
tribution of each class in one image is still very sparse ( e.g.,
t-SNE visualization results in Fig. 1(b2)), only foreground
features close to the class prototype can be activated with
high confidence, while those far away from the class pro-
totype cannot be activated. This observation explains the
reason that the class-prototype CAMs are still incomplete
for certain images ( e.g., SIPE CAMs in Fig. 1(b1)).
One reason behind the sparse distribution of foreground
pixel-wise features is that the discriminative features in cer-
tain foreground pixels are not learned by WSSS model, and
the extracted features from these pixels fail to exhibit close
proximity to the discriminative prototype which originates
from the classifier weights of WSSS model. Specifically,
current prototype-based works [5, 7] concentrate on fus-
ing features acquired from classifier weights with image-
specific features, and the learning of their WSSS models are
still driven by classification loss. However, WSSS model
trained through classification loss primarily captures and
learns from parts of foreground pixels which are most dis-
criminative and enough for reducing the classification loss
to the converge point, while ignoring the other foreground
pixels. In this paper, we refer to the discriminative pixels
captured and learned by WSSS model through classifica-
tion loss as primary discriminative pixels. In contrast, the
remaining discriminative pixels which are not captured by
WSSS model are referred to as secondary discriminative
pixels. Since current prototype-based approaches do not en-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
3437
(b1) SIPE CAMs SIPE CAMOur CAMOur CAMSIPE CAM
(d2) Feature distribution of ours 
(c) Our motivation 
(a)  Images(b2) Feature distribution of SIPE(d1) Our CAMs Figure 1. Image-specific prototype CAMs and pixel-wise foreground feature distribution comparisons. Prototype-based method SIPE [5]
is chosen as an example and the feature distribution is visualized through t-SNE [34]. (c) depicts our motivation that compelling features
extracted from estimated secondary discriminative pixels to be closely align with image-specific class prototype. Compared with the
SIPE result, our method achieves a more compact foreground feature distribution, with the image-specific prototype being the distribution
centroid. This makes the prototype from our method activate a more complete CAM.
able WSSS model to capture secondary discriminative pix-
els and do not make its classifier learn the discriminative
features in secondary discriminative pixels, the features ex-
tracted from secondary discriminative pixels are far away
from the class prototypes which originates from the classi-
fier of WSSS model. Consequently, the foreground pixel-
wise feature distribution becomes sparse, leading to incom-
plete prototype CAM. If WSSS model can learn discrimina-
tive features from both primary andsecondary discrimina-
tive pixels, the pixel-wise feature distribution of foreground
pixels will become more compact towards class prototype,
leading to a more complete CAM prediction.
Considering the challenges described above, constrain-
ing the features extracted from all foreground pixels to
closely align with discriminative class prototype could be
a solution, as shown in Fig. 1(c). This is because, to satisfy
the constraint, WSSS model has to learn discriminative fea-
tures not only from primary discriminative foreground pix-
els but also from secondary discriminative foreground pix-
els. However, it is infeasible to acquire all foreground pixels
for each image in advance during WSSS model training. To
address this, we introduce a plug-and-play Prototype-based
Secondary Discriminative Pixels Mining (PSDPM) frame-
work, which can estimate the potential foreground pixels
and optimize WSSS model to learn compact-distributed dis-
crimiantive features from both primary andsecondary dis-
criminative foreground pixels, improving various existing
WSSS methods significantly. Specifically, a Foreground
PixelEstimation Module (FPEM) is first proposed to es-
timate potential foreground pixels by leveraging the corre-
lations between primary andsecondary discriminative fea-
tures, based on the semantic segmentation results of base-
line WSSS methods. Then, we optimize the features within
potential foreground pixels to closely align with class proto-
type, driving WSSS model to learn discriminative features
from secondary discriminative pixels. As can be found in
Fig. 1(d1) and Fig. 1(d2), through our PSDPM, a compactforeground pixel-wise feature distribution can be achieved,
and the prototype can thus activate a complete CAM. The
contributions of this work include:
• We first argue that WSSS model only captures and
learns from primary discriminative pixels under current
prototype-based WSSS methods because of the different
goals of image classification and semantic segmentation.
This causes the features extracted from secondary dis-
criminative pixels far away from the discriminative pro-
totype in feature space, leading to incomplete CAMs.
• We propose a plug-and-play Prototype-based Secondary
Discriminative Pixels Mining (PSDPM) framework for
image-specific prototype CAM generation, which can ef-
fectively encourage WSSS model to learn from secondary
discriminative pixels and significantly improve the per-
formance of various existing WSSS methods.
• Our method achieves new state-of-the-art WSSS perfor-
mances with only image-level labels on two benchmarks,
reaching 74.1% and 74.9% mIoU on the validation and
test sets of Pascal VOC 2012, respectively, and 47.2%
mIoU on MS COCO 2014 validation set.
2. Related Work
2.1. Image-level Weakly Supervised Semantic Seg-
mentation
Image-level Weakly Supervised Semantic Segmentation
(WSSS) has received increasing attention due to its low
annotation cost [33, 36, 37, 55]. The key step of WSSS
is to generate high-quality CAMs for pseudo-labels [32,
36, 40, 48]. Attention mapping [30, 42, 51], heuristic ap-
proaches ( e.g., erasing and accumulation) [17, 48, 54] have
been proposed to encourage the network to explore novel re-
gions rather than solely focusing on limited discriminative
regions. Moreover, other strategies include self-supervised
learning [5, 42], contrastive learning [11, 59], and cross-
image information [27, 38, 47] have been used to generate
3438
ResNet50ImageSoft-prototypePrototype CAMClassifier weight CAMDeepLabV2
FPEMClassifierℒ!"#+Adding upℒ!$%
ResNet50ImageSoft-prototypePost-processingPrototype CAMClassifier weight CAMPseudo labelDeepLabV2
ClassifierWSSS model trainingSemantic Segmentation model trainingDeepLabV2Baseline method
Using the DeepLabtrained on the pseudo labels of baselinePseudo labelEstimated foreground pixelsForeground seed 
Figure 2. The training pipeline of our proposed PSDPM. For WSSS model training, we first train a DeepLab semantic segmentation model
using the pseudo labels provided by baseline WSSS method. Then, the DeepLab model is used for calculating a consistency loss Lcon
between our FPEM refined CAM and prototype CAM calculated from classifier weight CAM. IRN [1] or DenseCRF [20] is adopted as
post-processing methods during semantic segmentation model training.
accurate CAMs. To further improve the quality of the gen-
erated CAMs, CRF [20] and IRN [1] are employed to refine
the rough boundary of the CAMs to produce final enhanced
pseudo labels, which are then used for semantic segmenta-
tion model training.
2.2. Class Prototype in WSSS
A series of class prototype methods have been proposed
[7, 41]. From the view of optimization, Du et al. [11] pro-
pose a pixel-to-prototype contrast that estimates the proto-
types from pixel-wise feature embeddings across the train-
ing batch, obtaining a more suitable feature representation
for semantic segmentation. From the view of CAM cal-
culation, Chen et al. [7] perform clustering on all pixel-
wise features of an object class and selects the cluster cen-
ters as class prototypes. Then, similarity matrices are de-
rived by comparing all pixel-wise features to every proto-
type for CAM generation. Besides, Chen et al. [5] pro-
pose a prototype-based image-specific CAM. The prototype
is first calculated through masked average pooling of hier-
archical extracted features and then used to activate class-
specific regions. However, these prototype-based methods
still generate incomplete CAMs, as illustrated in Sec. 1.
This paper first analyzes the reason behind this issue and
then proposes a prototype-based secondary discriminative
pixels mining method that significantly improves the per-
formances of current state-of-the-art WSSS method.
2.3. Self-attention in WSSS
Self-attention is an efficient module to capture context in-
formation and refine pixel-wise prediction results [2, 35].
Its effectiveness in WSSS has been verified in [13]. The
general idea behind the self-attention mechanism is re-calculating each feature representation based on all other
features through the dot-product pixel affinity in an em-
bedding space [39]. It revises feature maps by capturing
context feature dependency, which also meets the ideas of
most WSSS methods using the similarity of pixels to refine
the original activation map. Inspired by the self-attention
mechanism, Wang et al. [42] first introduce a Pixel Corre-
lation Module (PCM) at the end of the network to integrate
the low-level feature of each pixel for WSSS. This method
calculates the CAM affinity only using the high-level fea-
ture and image RGB information. However, other differ-
ent level information ( e.g., feature position, various levels
of features) are not fully considered in PCM, restricting its
performance improvement in WSSS task. Different from
the previous work PCM [42] which is a pure context refine-
ment module and aims at refining final CAMs, in this work,
we adopt self-attention mechanism to continuously esti-
mate potential foreground pixels for driving WSSS model
to learn from secondary discriminative pixels. Additionally,
we also introduce a cascade fashion in our FPEM to lever-
age all kinds of low-level information and improve potential
foreground pixels estimation accuracy.
3. Methodology
For enabling WSSS model to learn discriminative features
from both primary and secondary discriminative pixels,
and simultaneously rectifying the CAM activation center
through class prototypes, our main idea is to first esti-
mate potential foreground regions which contain primary
and secondary discriminative pixels, and then ensure all
features from estimated foreground pixels are aligned as
closely as possible with the discriminative prototype. This
3439
Figure 3. The calculation flow of our Foreground PixelEstimation
Module (FPEM). The position matrix, RGB image, and extracted
features are first concatenated, and then embedded through three
1×1Convolutional Neural Networks (CNNs), respectively, fol-
lowing Eq. (1). Finally, three embedded results are processed in
turn through a cascade manner. The red arrow denotes the calcu-
lation formulated in Eq. (2).
W…21W…21…………W…21HH…H…………22…211…112…W12…W…………12…W11…122…2…………HH…H12…W12…W…………12…W⨁HH…H…………22…211…1W…21W…21…………W…21⨁⨁
11…122…2…………HH…H
Figure 4. The illustration of position matrix calculation. First, four
height- or width-based incremented matrixes are created. Then,
four matrixes are concatenated and ℓ2-normalization is applied on
the concatenated dimension for each position vector. In this way,
closer pixels have higher cosine similarity on our position matrix
and vice versa.
makes WSSS model have to learn discriminative features
from both primary and secondary discriminative pixels, re-
sulting in a more compact feature distribution of foreground
pixels and a more complete CAM. The pipeline of our
Prototype-based Secondary Discriminative Pixels Mining
(PSDPM) is depicted in Fig. 2. It involves a Foreground
PixelEstimation Module (FPEM) with a consistency loss
Lconcalculated between the prototype CAM and the FPEM
output. The following sub-sections will describe each com-
ponent in details.3.1. Foreground Pixel Estimation Module (FPEM)
Chen et al. [5] show that CAM refinement methods ( e.g.,
DenseCRF [20], IRN [1]) substantially improve the quality
of CAM which considers discriminative features only from
primary discriminative pixels. These methods activate par-
tial secondary discriminative pixels and make CAMs more
complete by leveraging the correlations in low-level infor-
mation among different pixels. This indicates that primary
and secondary discriminative pixels exhibit strong corre-
lations in low-level information, such as RGB values and
spatial positions. However, primary discrminative pixels
do not only correlate with secondary discrminative pixels,
they could also have low-level correlations with the non-
discriminative ones. On the other hand, the low-level cor-
relations become fewer when the encoder network goes
deeper, as the high-level information dominates the deeper
feature map [49]. In this case, the pixel correlations are
mainly retained through discriminative features related fea-
tures, making the remained correlations less related to non-
discriminative pixels, compared with low-level correlations.
Inspired from these findings, in order to estimate foreground
pixels (including primary and secondary discriminative pix-
els) more accurately only based on the learned primary dis-
criminative pixels, we leverage the correlations between
primary and secondary discriminative pixels with varying
confidence levels. Specifically, consider an input CAM
that has already activated primary discriminative pixels, we
adopt our Foreground PixelEstimation Module (FPEM)
to make CAM activate more potential secondary discrim-
inative pixels and regard the expanded CAM as potential
foreground pixels. This module introduces the most con-
fident correlations ( i.e., the correlations are not related to
non-discriminative pixels) in the deepest feature map first
to refine the input CAM in general and then gradually intro-
duces less confident correlations to refine the CAM details,
minimizing the total refinement error.
The detailed procedure of our FPEM is shown in Fig. 3.
Specifically, we select the feature maps of the last three
layers in the feature encoder ( i.e., ResNet50 in this paper)
and then concatenate each feature map with a position ma-
trix and a scaled input RGB image, providing three com-
bined representations, each with RGB, position, and fea-
ture information. For example, concerning the last layer
feature map F3(supposing its size is 32 ×32×1024 and
1024 is its feature map channel size), we interpolate the
original RGB image to the size of 32 ×32×3 through bi-
linear interpolation and create a position matrix whose size
is 32×32×2. The position matrix is created following the
illustration shown in Fig. 4. Each position is represented
by one four-dimensional vector and closer pixels will have
high cosine similarity, fully preserving relative position in-
formation when doing self-attention calculation. Then, the
feature map, interpolated image, and position matrix are
3440
concatenated on the third channel dimension, obtaining a
combined representation. After obtaining three combined
representations for F3,F2, andF1, we embed them through
three Convolutional Neural Networks (CNNs) ( i.e.,θ3,θ2,
andθ1) and interpolate the embedded results to the size of
input CAM through bilinear interpolation, obtaining three
correlation maps R3,R2andR1. Their formulations are
presented through Eq. (1):
Ri=θi(POS i⊕Fi⊕Ii), (1)
wherePOS idenotes the position matrix with the same size
as feature map Fi;Iidenotes the bilinear interpolated im-
age also with the same size as feature map Fi;θidenotes
individual 1×1convolutional layer as shown in Fig. 3.
After having three correlation maps, we first calculate a
revised CAM Mr3based on the input CAM and the most
confident correlation map R3(R3originates from the deep-
est feature map F3and is thus most confident), as follows:
Mi= 1
CiP
∀jReLU ((Rn)T
i(Rn)j
∥(Rn)i∥·∥(Rn)j∥)ˆMi
+ˆMi
/2,
(2)
where (Rn)idenotes the correlation map feature value
ofRnat position i, which is a vector; jdenotes any
position in the CAM; Cidenotes the coefficient sumP
∀jReLU ((Rn)T
i(Rn)j
∥(Rn)i∥·∥(Rn)j∥);ˆMidenotes the input CAM
value at position i;Midenotes the revised CAM value at
position i, which is the average of two terms: weighted sum
of input CAM ˆMiwith normalized similarities calculated
fromRnand ˆMi. We adopt a residual way when calculat-
ingMifrom ˆMito prevent the final result of our cascade-
style FPEM from being too wrong.
After that, the second revised CAM Mr2is calculated
through Eq. (2) based on the correlation map R2and the
first revised CAM Mr3as input CAM. Similarly, the third
revised CAM Mr1is calculated based on the correlation
mapR1and the second revised CAM Mr2as input CAM.
In this way, the most confident correlation map R3which
originates from the deepest feature map is first considered
to give a coarse CAM refinement result. Then, based on this
coarse CAM refinement result, lower-confident correlation
maps with higher resolutions ( i.e.,R2andR1) which origi-
nate from shallower feature maps are gradually included for
further adjusting the details of the coarse CAM refinement
result, and the third revised CAM Mr1is regarded as final
estimated potential foreground pixels.
3.2. Plug-and-play Prototype-based Training
On the one hand, for each training image, we calculate
image-specific prototype CAM based on classifier weight
CAM. Specifically, the soft prototype Pcof class cis cal-
culated through Eq. (3):
Pc=MAP 
(MW)c⊙L(F1,F2,F3)
, (3)whereMWdenotes the classifier weight CAM which is cal-
culated through 1×1convolutional kernels; MAP (·)de-
notes masked average pooling; L(·)denotes the linear pro-
jection; F1,F2,F3are the last three layer feature maps of
feature encoder as shown in Fig. 3. Then, we calculate pro-
totype CAM based on soft prototype Pcthrough Eq. (4):
(MP)c=ReLU 
cos⟨Pc, L(F1,F2,F3)⟩
,(4)
where MPdenotes the prototype CAM; cos⟨·,·⟩denotes
the cosine similarity between the two terms within it. With
our prototype CAM, as the discriminative classifier weights
are fused with image-specific information for CAM activa-
tion, the CAM activation center can be rectified based on
the characteristics of input images.
On the other hand, each training image is also input to
a DeepLab V2 model for calculating semantic segmenta-
tion probability maps. This DeepLab model is trained on
the pseudo-labels provided by the baseline WSSS method,
and the output semantic segmentation probability map is de-
noted as MDto represent the potential foreground seed pix-
els. The quality of seed pixels depends on the performance
of baseline WSSS method. The seed pixels are mainly
composed of primary discriminative pixels as the baseline
method is unable to learn discrminative features from sec-
ondary discriminative pixels. Then, the semantic segmenta-
tion probability map MDand our prototype CAM MPare
added as the input to our FPEM module for further expand-
ing CAM with more secondary discriminative pixels.
Finally, a consistency loss Lconis used to minimize the
difference between the FPEM output and prototype CAM
MPthrough Eq. (5).
Lcon=X
cϕ 
(MD)c+ (MP)c
/2
−(MP)c
1,
(5)
where (MD)cdenotes the DeepLab V2 output of class c,
which is the potential foreground seed pixels; ϕ(·)denotes
our FPEM module.
Here, FPEM aims to expand the seed pixels MDwhich
mainly contain primary discriminative pixels, to include
more additional secondary discriminative pixels, providing
potential foreground pixels ( i.e.,primary and secondary dis-
criminative pixels) to guide the learning of prototype CAM.
As the prototype CAM originates from discriminative clas-
sifier weights, when narrowing the gap between prototype
CAM and FPEM estimated foreground pixels through min-
imizing Lcon, WSSS model is compelled to learn all dis-
criminative features from FPEM estimated foreground pix-
els as much as possible to make class prototype activate the
same pixels as FPEM estimated. In this way, WSSS model
can learn discriminative features from both primary and sec-
ondary discriminative pixels.
As WSSS model continues to learn increasing numbers
3441
of discriminative features from the secondary discrimina-
tive pixels during training, the prototype CAM becomes
more and more complete. Considering this, the prototype
CAM is combined with the output of DeepLab V2 as the in-
put of FPEM, which encourages FPEM to continuously ex-
plore more secondary discriminative foreground pixels be-
yond the output of DeepLab V2.
In summary, the total WSSS model training loss Ltotalof
our PSDPM is formulated as follows:
Ltotal=Lcls+Lcon, (6)
where Lclsis the multi-label soft margin classification loss
[5] calculated from classifier output as shown in Fig. 2.
3.3. Semantic Segmentation Model Training
As WSSS model has learned sufficient discriminative fea-
tures from both primary and secondary discriminative pix-
els, our prototype CAM can cover complete foreground pix-
els, with CAM activation center being rectified. However,
the resolution of our prototype CAM is relative low com-
pared with the resolution of the original image. Therefore,
we adopt post-processing ( e.g., IRN [1] or DenseCRF [20])
on our prototype CAMs to refine the activation details, and
regard the post-processed prototype CAMs as pseudo labels
for semantic segmentation model training.
4. Experiments
4.1. Dataset and Implementation Details
Datasets and Evaluation Metrics. Experiments are con-
ducted on two benchmarks: PASCAL VOC 2012 [12] with
21 classes and MS COCO 2014 [25] with 81 classes. For
PASCAL VOC 2012, following [5, 24, 42, 50, 52], we use
the augmented SBD set [14] with 10,582 annotated images.
MeanIntersection overUnion (mIoU) [28] is used to eval-
uate segmentation results. To evaluate the quality of CAM
and pseudo label, CAM and pseudo label are generated for
every image in the train set and the mIoUs are calculated
based on the ground truth masks. To evaluate the seman-
tic segmentation model performance, we train DeepLab V2
and use it to predict masks for the images in val and test
sets, and calculate mIoU based on ground truth masks.
Implementation Details. For WSSS model training (as
shown in Fig. 2), we adopt the ImageNet [10] pretrained
ResNet50 [15]. SGD optimizer with a momentum of 0.9
and a weight decay of 10−4are adopted. The initial learn-
ing rate is set to 0.1 for backbone and 1 for other param-
eters. The learning rate is then adjusted by poly decay
with power of 0.9, following [1, 5]. The WSSS model is
trained with a batch size of 16 for Pascal VOC 2012 and
MS COCO 2014. The scale ratios of multi-scaled CAM
during inference are set to {0.5,1.0,1.5,2.0}following [5].
Random cropping size 512 ×512 is adopted for training dataTable 1. Evaluation results of pseudo-label performance (mIoU
(%)) on various WSSS methods on PASCAL VOC 2012.
Method Pseudo label
IRN [1] CVPR’19 66.3
SEAM [42] CVPR’20 63.6
IRN+CONTA [53] NeurIPS’20 67.9
AdvCAM [22] CVPR’21 69.9
RIB [21] NeurIPS’21 70.6
AMN [23] CVPR’22 72.2
SIPE [5] CVPR’22 68.0
ESOL [24] NeurIPS’22 68.7
CLIMS [44] CVPR’22 70.5
CLIMS + Ours 75.5 (+5.0)
IRN+LPCAM [7] CVPR’23 71.2
IRN+LPCAM + Ours 75.1 (+3.9)
CLIP-ES [26] CVPR’23 75.0
CLIP-ES + Ours 77.3 (+2.3)
augmentation. Our prototype CAM MPis further post-
processed by DenseCRF [20] or IRN [1] to generate the fi-
nal pseudo labels, which are used to train the segmentation
model. With regard to the semantic segmentation model,
we adopt ResNet101-based DeepLab V2 with weights pre-
trained on ImageNet dataset. For PASCAL VOC 2012,
we follow the default training settings of HSC [43]. In-
put images are randomly scaled to {0.5,1.5}and cropped
to 448×448 for training. The batch size is set to 10, and
iteration is 20k. Besides, for MS COCO 2014, we follow
the default training settings of CLIP-ES [26]. Input images
are randomly scaled to {0.5,0.75,1.0,1.25,1.5,1.75,2.0}
and 481 ×481 are cropped. The batch size and the number
of iterations are set to 5 and 100k, respectively.
4.2. Comparison of Pseudo-label Quality
To validate the effectiveness of our PSDPM, we evaluate
the quality of pseudo label quantitatively in Tab. 1. The
pseudo labels on Pascal VOC 2012 are generated through
IRN [1] based on the prototype CAMs of our PSDPM. Ex-
perimental results show that our PSDPM can consistently
improve various existing works on PASCAL VOC. Partic-
ularly, through applying our PSDPM on CLIP-ES [26], we
improve the pseudo-label performance of CLIP-ES by 2.3
%, achieving new state-of-the-art pseudo-label performance
77.3% on PASCAL VOC 2012.
In addition, we study the qualitative effects of our PS-
DPM on both CAMs and pseudo labels in Fig. 5. We
also compare the differences between our method and an-
other prototype-based method SIPE [5]. Firstly, it can be
seen that the prototype-based method SIPE can improve
the classifier weight CAM quality by activating more com-
plete foreground pixels. However, for certain images ( e.g.,
‘bird’ in the last column and ‘person’ in the third and fourth
3442
ImagesClassifierCAMsSIPECAMsPSDPMCAMsGroundTruthsPseudoLabels
Figure 5. CAM and pseudo-label visualization results on PASCAL VOC 2012.
Table 2. Evaluation results of DeepLab V2 on PASCAL VOC
2012. The best results are in bold.
MethodsPASCAL VOC
Val Test
SEAM [42] CVPR’20 64.5 65.7
AdvCAM [22] CVPR’21 68.1 68.0
SEAM+PPC [11] CVPR’22 67.7 67.4
ReCAM [8] CVPR’22 68.5 68.4
SIPE [5] CVPR’22 68.8 69.7
AMN [23] CVPR’22 70.7 70.6
MCTformer [45] CVPR’22 71.9 71.6
ESOL [24] NeurIPS’22 69.9 69.3
SAS [19] AAAI’23 69.5 70.1
SEAM+OCR [9] CVPR’23 67.8 68.4
AMN+LPCAM [7] CVPR’23 70.1 70.4
MMCST [46] CVPR’23 72.2 72.2
ToCo [31] CVPR’23 69.8 70.5
BECO [29] CVPR’23 72.1 71.8
AdvCAM+FPR [4] ICCV’23 70.3 70.1
SFC [58] AAAI’24 71.2 72.5
CLIMS [44] CVPR’22 69.3 68.7
CLIMS + Ours 73.1 (+3.8) 73.9 (+5.2)
IRN+LPCAM [7] CVPR’23 68.6 68.7
IRN+LPCAM + Ours 72.3 (+3.7) 73.2 (+4.5)
CLIP-ES [26] CVPR’23 71.1 71.4
CLIP-ES + Ours 74.1 (+3.0) 74.9 (+3.5)
columns), the CAM results of SIPE are still incomplete as
WSSS model does not capture and learn from secondary
discriminative pixels. Besides, for certain image ( e.g., bird
in the second column), SIPE has an over-activating issue
because it adopts a structure-aware seeds locating algorithmTable 3. Evaluation results of DeepLab V2 on MS COCO 2014.
The best results are in bold.
MethodsMS COCO
Val
SIPE [5] CVPR’22 40.6
AMN [23] CVPR’22 44.7
MDBA [6] TIP’23 37.8
SAS [19] AAAI’23 44.8
MMCST [46] CVPR’23 45.9
ToCo [31] CVPR’23 41.3
SFC [58] AAAI’24 46.8
CLIP-ES [26] 45.4
CLIP-ES + Ours 47.2 (+1.8)
for expanding the incomplete prototype CAM, which may
expand the CAM in an inaccurate way. Through our PS-
DPM, as the WSSS model can learn discriminative features
from both primary and secondary discriminative pixels, all
CAM results are consistently improved in terms of object
completeness and boundary fineness.
4.3. Comparison of Final Semantic Segmentation
Performance
In this study, IRN [1] (on Pascal VOC 2012) and DenseCRF
[20] (on MS COCO 2014) are utilized for post-processing
our PSDPM prototype CAMs. Their results are then treated
as pseudo labels to train semantic segmentation model
DeepLab V2 in a fully supervised manner. Tab. 2 reports
the mIoU scores of our method and recent WSSS methods
on the validation and test sets of PASCAL VOC 2012. On
3443
Table 4. Ablation study of our PSDPM in WSSS model training on
PASCAL VOC 2012 training set and mIoU (%) is used for eval-
uation. CLIMS [44] is adopted as baseline method for PSDPM.
‘D’, ‘A’ and ‘F’ denote different parts in our WSSS model train-
ing (Fig. 2). ‘D’ denotes using the output of DeepLab V2 which
is trained on the pseudo labels of baseline method; ‘A’ denotes
adding up DeepLab V2 output and prototype CAM; ‘FPEM’ de-
notes using our FPEM.
# D A FPEM Prototype CAM
1✓ 59.7
2✓ ✓ 61.3
3✓ ✓ 67.9
4✓ ✓ ✓ 68.5
this dataset, we achieve 74.1% and 74.9% mIoUs using an
ImageNet pre-trained backbone based on CLIP-ES, outper-
forming all other image-level WSSS methods. Tab. 3 re-
ports the performance comparison on MS COCO 2014. Our
method achieves 47.2% mIoU on the validation set, outper-
forming all other WSSS methods. Additionally, the consis-
tent performance improvements of our PSDPM on various
baseline methods and datasets demonstrate that these base-
line methods are originally limited to primary discrimina-
tive pixels and can benefit from our PSDPM.
4.4. Ablation Studies
In Tab. 4, we first verify the effectiveness of different com-
ponents of our PSDPM (Fig. 2). By regarding the output of
DeepLab trained on the pseudo labels of baseline method as
estimated foreground pixels, and calculating Lconbetween
the estimated foreground pixels and prototype CAM ( i.e.,
setting #1), we achieve 59.7% mIoU. Base on this, adding
up Deeplab output and prototype CAM, and regard the sum
as estimated foreground pixels for Lconcalculation ( i.e., set-
ting #2) can improve the mIoU to 61.3%. This is because
prototype CAM can be complementary to the DeepLab out-
put and enable WSSS model to learn new features from
Lcon. However, WSSS model learning is still limited to the
primary discriminative pixels. Further adopting FPEM to
the result of ‘A’ ( i.e., ‘setting #4’) can introduce more sec-
ondary discriminative pixels into consideration for our PS-
DPM, achieving the best performance 68.5%. Using FPEM
without adding prototype CAM to DeepLab output ( i.e.,
‘setting #3’) makes WSSS model being restrained to the
baseline method and cannot continuously mine more sec-
ondary discriminative pixels, only achieving 67.9% mIoU.
Tab. 5 studies the effectiveness of different steps in FPEM.
When using Mr3for foreground pixel estimation ( i.e., ‘w/
Mr3’), we can achieve 64.8%, which is better than ‘w/o
FPEM’ which regards the sum of DeepLab output and pro-
totype CAM as estimated foreground pixels. Using Mr2
(i.e., ‘w/Mr2’) can continue the performance improve-Table 5. Ablation study of FPEM on PASCAL VOC 2012 training
set and mIoU (%) is used for evaluation. CLIMS [44] is adopted
as baseline method for PSDPM. ‘w/o FPEM’ indicates our PS-
DPM without FPEM. ‘w/ Mr3’, ‘w/Mr2’ and ‘w/ Mr1’ indi-
cate regarding the results of FPEM different steps as estimated
foreground pixels. ‘w/o POS ’, ‘w/oF’ and ‘w/o I’ denote with-
out the corresponding component in Eq. (1).
# FPEM Prototype CAM
1 w/o FPEM 61.3
2 w/ Mr3 64.8
3 w/ Mr2 68.0
4 w/ Mr1 68.5
5 w/o POS 68.1
6 w/o F 64.2
7 w/o I 68.2
Table 6. The influence of post-processing method on baseline. The
result shows that the improvement brought by our PSDPM is not
due to post-processing ( e.g., IRN).
Methods Pseudo label
CLIP-ES [26] 75.0
CLIP-ES [26] + IRN [1] 75.1
CLIP-ES + Ours (w/ IRN) 77.3
ment significantly by 3.2%. Using Mr1(i.e., ‘w/Mr1’)
further improves the FPEM effectiveness to 68.5%. Set-
tings #5˜#7 show that POS ,F,Iin Eq. (1) are all help-
ful. Tab. 6 shows that simply adding post-processing can
not improve the baseline performance much, indicating the
improvements brought by our PSDPM on various baselines
are not because of post-processing.
5. Conclusion
In this paper, we demonstrate that WSSS model does not
learn discriminative features from secondary discriminative
pixels in current classification-loss-driven WSSS methods,
and this causes the incomplete CAM activation issue. For
solving this, we propose a plug-and-play Prototype-based
Secondary Discriminative Pixels Mining (PSDPM) frame-
work for enabling WSSS model to learn discrminative fea-
tures from secondary discriminative pixels, improving vari-
ous existing WSSS methods to new state-of-the-art perfor-
mances. Other possible solutions for secondary discrimina-
tive pixel issue will be investigated in our future works.
Acknowledgement This work was supported by the National
Key R&D Program of China (No.2022YFE0200300), the Na-
tional Natural Science Foundation of China (No. 61972323,
62331003), Suzhou Basic Research Program (SYG202316) and
XJTLU REF-22-01-010, XJTLU AI University Research Centre,
Jiangsu Province Engineering Research Centre of Data Science
and Cognitive Computation at XJTLU and SIP AI innovation plat-
form (YZCXPT2022103).
3444
References
[1] Jiwoon Ahn, Sunghyun Cho, and Suha Kwak. Weakly su-
pervised learning of instance segmentation with inter-pixel
relations. In CVPR , 2019. 1, 3, 4, 6, 7, 8
[2] Antoni Buades, Bartomeu Coll, and J-M Morel. A non-local
algorithm for image denoising. In CVPR , 2005. 3
[3] Jinpeng Chen, Runmin Cong, Yuxuan Luo, Horace Ip,
and Sam Kwong. Saving 100x storage: Prototype re-
play for reconstructing training sample distribution in class-
incremental semantic segmentation. Advances in Neural In-
formation Processing Systems , 36, 2024. 1
[4] Liyi Chen, Chenyang Lei, Ruihuang Li, Shuai Li, Zhaoxiang
Zhang, and Lei Zhang. Fpr: False positive rectification for
weakly supervised semantic segmentation. In ICCV , 2023. 7
[5] Qi Chen, Lingxiao Yang, Jian-Huang Lai, and Xiaohua
Xie. Self-supervised image-specific prototype exploration
for weakly supervised semantic segmentation. In CVPR ,
2022. 1, 2, 3, 4, 6, 7
[6] Tao Chen, Yazhou Yao, and Jinhui Tang. Multi-granularity
denoising and bidirectional alignment for weakly supervised
semantic segmentation. IEEE TIP , 2023. 7
[7] Zhaozheng Chen and Qianru Sun. Extracting class activation
maps from non-discriminative features as well. In CVPR ,
2023. 1, 3, 6, 7
[8] Zhaozheng Chen, Tan Wang, Xiongwei Wu, Xian-Sheng
Hua, Hanwang Zhang, and Qianru Sun. Class re-activation
maps for weakly-supervised semantic segmentation. In
CVPR , 2022. 7
[9] Zesen Cheng, Pengchong Qiao, Kehan Li, Siheng Li, Pengxu
Wei, Xiangyang Ji, Li Yuan, Chang Liu, and Jie Chen.
Out-of-candidate rectification for weakly supervised seman-
tic segmentation. In CVPR , 2023. 7
[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR , 2009. 6
[11] Ye Du, Zehua Fu, Qingjie Liu, and Yunhong Wang. Weakly
supervised semantic segmentation by pixel-to-prototype
contrast. In CVPR , 2022. 2, 3, 7
[12] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge. IJCV , 2010. 6
[13] Junsong Fan, Zhaoxiang Zhang, Tieniu Tan, Chunfeng Song,
and Jun Xiao. Cian: Cross-image affinity net for weakly
supervised semantic segmentation. In AAAI , 2020. 3
[14] Bharath Hariharan, Pablo Arbel ´aez, Lubomir Bourdev,
Subhransu Maji, and Jitendra Malik. Semantic contours from
inverse detectors. In ICCV , 2011. 6
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
2016. 6
[16] Tianyu Huang, Bowen Dong, Yunhan Yang, Xiaoshui
Huang, Rynson WH Lau, Wanli Ouyang, and Wangmeng
Zuo. Clip2point: Transfer clip to point cloud classifica-
tion with image-depth pre-training. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 22157–22167, 2023. 1[17] Peng-Tao Jiang, Ling-Hao Han, Qibin Hou, Ming-Ming
Cheng, and Yunchao Wei. Online attention accumulation
for weakly supervised semantic segmentation. IEEE TPAMI ,
2021. 2
[18] Siyu Jiao, Yunchao Wei, Yaowei Wang, Yao Zhao, and
Humphrey Shi. Learning mask-aware clip representations
for zero-shot segmentation. Advances in Neural Information
Processing Systems , 36:35631–35653, 2023. 1
[19] Sangtae Kim, Daeyoung Park, and Byonghyo Shim.
Semantic-aware superpixel for weakly supervised semantic
segmentation. In AAAI , 2023. 7
[20] Philipp Kr ¨ahenb ¨uhl and Vladlen Koltun. Efficient inference
in fully connected crfs with gaussian edge potentials. In
NeurIPS , 2011. 3, 4, 6, 7
[21] Jungbeom Lee, Jooyoung Choi, Jisoo Mok, and Sungroh
Yoon. Reducing information bottleneck for weakly super-
vised semantic segmentation. In NeurIPS , 2021. 6
[22] Jungbeom Lee, Eunji Kim, and Sungroh Yoon. Anti-
adversarially manipulated attributions for weakly and semi-
supervised semantic segmentation. In CVPR , 2021. 6, 7
[23] Minhyun Lee, Dongseob Kim, and Hyunjung Shim. Thresh-
old matters in wsss: manipulating the activation for the ro-
bust and accurate segmentation model against thresholds. In
CVPR , 2022. 6, 7
[24] Jinlong Li, Zequn Jie, Xu Wang, Xiaolin Wei, and Lin
Ma. Expansion and shrinkage of localization for weakly-
supervised semantic segmentation. In NeurIPS , 2022. 6, 7
[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
ECCV , 2014. 6
[26] Yuqi Lin, Minghao Chen, Wenxiao Wang, Boxi Wu, Ke Li,
Binbin Lin, Haifeng Liu, and Xiaofei He. Clip is also an
efficient segmenter: A text-driven approach for weakly su-
pervised semantic segmentation. In CVPR , 2023. 6, 7, 8
[27] Man Liu, Chunjie Zhang, Huihui Bai, Riquan Zhang, and
Yao Zhao. Cross-part learning for fine-grained image classi-
fication. IEEE TIP , 2021. 2
[28] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In
CVPR , 2015. 6
[29] Shenghai Rong, Bohai Tu, Zilei Wang, and Junjie Li.
Boundary-enhanced co-training for weakly supervised se-
mantic segmentation. In CVPR , 2023. 7
[30] Lixiang Ru, Yibing Zhan, Baosheng Yu, and Bo Du. Learn-
ing affinity from attention: End-to-end weakly-supervised
semantic segmentation with transformers. In CVPR , 2022.
2
[31] Lixiang Ru, Heliang Zheng, Yibing Zhan, and Bo Du. Token
contrast for weakly-supervised semantic segmentation. In
CVPR , 2023. 7
[32] Guolei Sun, Wenguan Wang, Jifeng Dai, and Luc Van Gool.
Mining cross-image semantics for weakly supervised seman-
tic segmentation. In ECCV , 2020. 2
[33] Feilong Tang, Zhongxing Xu, Zhaojun Qu, Wei Feng,
Xingjian Jiang, and Zongyuan Ge. Hunting attributes: Con-
text prototype-aware learning for weakly supervised seman-
tic segmentation, 2024. 2
3445
[34] Laurens Van der Maaten and Geoffrey Hinton. Visualizing
data using t-sne. JMLR , 2008. 2
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS , 2017. 3
[36] Changwei Wang, Rongtao Xu, Shibiao Xu, Weiliang Meng,
and Xiaopeng Zhang. Treating pseudo-labels generation as
image matting for weakly supervised semantic segmentation.
InICCV , 2023. 2
[37] Jian Wang, Siyue Yu, Bingfeng Zhang, Xinqiao Zhao, ´Angel
F. Garc ´ıa-Fern ´andez, Eng Gee Lim, and Jimin Xiao. Cross-
frame feature-saliency mutual reinforcing for weakly super-
vised video salient object detection. PR, 2024. 2
[38] Wenguan Wang, Guolei Sun, and Luc Van Gool. Looking
beyond single images for weakly supervised semantic seg-
mentation learning. IEEE TPAMI , 2022. 2
[39] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-
ing He. Non-local neural networks. In CVPR , 2018. 3
[40] Xiaoyang Wang, Jimin Xiao, Bingfeng Zhang, and Limin
Yu. Card: Semi-supervised semantic segmentation via class-
agnostic relation based denoising. In IJCAI , 2022. 2
[41] Xiaoyang Wang, Bingfeng Zhang, Limin Yu, and Jimin
Xiao. Hunting sparsity: Density-guided contrastive learn-
ing for semi-supervised semantic segmentation. In CVPR ,
2023. 3
[42] Yude Wang, Jie Zhang, Meina Kan, Shiguang Shan, and
Xilin Chen. Self-supervised equivariant attention mech-
anism for weakly supervised semantic segmentation. In
CVPR , 2020. 2, 3, 6, 7
[43] Yuanchen Wu, Xiaoqiang Li, Songmin Dai, Jide Li, Tong
Liu, and Shaorong Xie. Hierarchical semantic contrast for
weakly supervised semantic segmentation. In IJCAI , 2023.
6
[44] Jinheng Xie, Xianxu Hou, Kai Ye, and Linlin Shen. Clims:
cross language image matching for weakly supervised se-
mantic segmentation. In CVPR , 2022. 6, 7, 8
[45] Lian Xu, Wanli Ouyang, Mohammed Bennamoun, Farid
Boussaid, and Dan Xu. Multi-class token transformer for
weakly supervised semantic segmentation. In CVPR , 2022.
7
[46] Lian Xu, Wanli Ouyang, Mohammed Bennamoun, Farid
Boussaid, and Dan Xu. Learning multi-modal class-specific
tokens for weakly supervised dense object localization. In
CVPR , 2023. 7
[47] Rongtao Xu, Changwei Wang, Jiaxi Sun, Shibiao Xu, Weil-
iang Meng, and Xiaopeng Zhang. Self correspondence dis-
tillation for end-to-end weakly-supervised semantic segmen-
tation. In AAAI , 2023. 2
[48] Sung-Hoon Yoon, Hyeokjun Kweon, Jegyeong Cho, Shin-
jeong Kim, and Kuk-Jin Yoon. Adversarial erasing frame-
work via triplet with gated pyramid pooling layer for weakly
supervised semantic segmentation. In ECCV , 2022. 2
[49] Matthew D Zeiler and Rob Fergus. Visualizing and under-
standing convolutional networks. In ECCV , 2014. 4
[50] Bingfeng Zhang, Jimin Xiao, Yunchao Wei, Mingjie Sun,
and Kaizhu Huang. Reliability does matter: An end-to-
end weakly supervised semantic segmentation approach. In
AAAI , 2020. 6[51] Bingfeng Zhang, Jimin Xiao, Jianbo Jiao, Yunchao Wei,
and Yao Zhao. Affinity attention graph neural network for
weakly supervised semantic segmentation. IEEE TPAMI ,
2021. 2
[52] Bingfeng Zhang, Jimin Xiao, Yunchao Wei, and Yao Zhao.
Credible dual-expert learning for weakly supervised seman-
tic segmentation. IJCV , 2023. 6
[53] Dong Zhang, Hanwang Zhang, Jinhui Tang, Xian-Sheng
Hua, and Qianru Sun. Causal intervention for weakly-
supervised semantic segmentation. In NeurIPS , 2020. 6
[54] Fei Zhang, Chaochen Gu, Chenyue Zhang, and Yuchao Dai.
Complementary patch for weakly supervised semantic seg-
mentation. In ICCV , 2021. 2
[55] Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen,
and Yunchao Wei. Slca: Slow learner with classifier align-
ment for continual learning on a pre-trained model. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 19148–19158, 2023. 2
[56] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng
Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo:
Training-free controllable text-to-video generation. arXiv
preprint arXiv:2305.13077 , 2023. 1
[57] Zekang Zhang, Guangyu Gao, Jianbo Jiao, Chi Harold Liu,
and Yunchao Wei. Coinseg: Contrast inter-and intra-class
representations for incremental segmentation. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 843–853, 2023. 1
[58] Xinqiao Zhao, Feilong Tang, Xiaoyang Wang, and Jimin
Xiao. Sfc: Shared feature calibration in weakly supervised
semantic segmentation. arXiv preprint arXiv:2401.11719 ,
2024. 7
[59] Tianfei Zhou, Meijie Zhang, Fang Zhao, and Jianwu Li. Re-
gional semantic contrast and aggregation for weakly super-
vised semantic segmentation. In CVPR , 2022. 2
[60] Hongguang Zhu, Yunchao Wei, Xiaodan Liang, Chunjie
Zhang, and Yao Zhao. Ctp: Towards vision-language con-
tinual pretraining via compatible momentum contrast and
topology preservation. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 22257–
22267, 2023. 1
3446
